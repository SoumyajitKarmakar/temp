Not using distributed mode
[18:17:45.114381] job dir: /notebooks/CVPR2023
[18:17:45.114633] Namespace(batch_size=64,
epochs=100,
accum_iter=1,
model='mae_vit_tiny',
norm_pix_loss=False,
dataset='c10',
input_size=32,
patch_size=2,
mask_ratio=0.75,
lambda_weight=0.1,
drop_path=0.1,
clip_grad=None,
weight_decay=0.05,
lr=None,
blr=0.001,
layer_decay=0.75,
min_lr=1e-06,
warmup_epochs=5,
color_jitter=None,
aa='rand-m9-mstd0.5-inc1',
smoothing=0.1,
reprob=0.25,
remode='pixel',
recount=1,
resplit=False,
mixup=0,
cutmix=0,
cutmix_minmax=None,
mixup_prob=1.0,
mixup_switch_prob=0.5,
mixup_mode='batch',
finetune='',
global_pool=True,
data_path='/datasets01/imagenet_full_size/061417/',
nb_classes=10,
output_dir='./output_dir',
log_dir='./output_dir',
device='cuda',
seed=0,
resume='',
start_epoch=0,
eval=False,
dist_eval=False,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[18:17:46.173719] Files already downloaded and verified
/usr/local/lib/python3.9/dist-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
[18:17:46.978387] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCropAndInterpolation(size=(32, 32), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic)
               RandomHorizontalFlip(p=0.5)
               RandAugment(n=2, ops=
           	AugmentOp(name=AutoContrast, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Equalize, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Invert, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Rotate, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=PosterizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeAdd, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ColorIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ContrastIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=BrightnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SharpnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearX, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearY, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateXRel, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateYRel, p=0.5, m=9, mstd=0.5))
               ToTensor()
               Normalize(mean=tensor([0.4914, 0.4822, 0.4465]), std=tensor([0.2023, 0.1994, 0.2010]))
               RandomErasing(p=0.25, mode=pixel, count=(1, 1))
           )
[18:17:47.346577] Files already downloaded and verified
[18:17:47.745721] Dataset CIFAR10
    Number of datapoints: 10000
    Root location: ./data
    Split: Test
    StandardTransform
Transform: Compose(
               Resize(size=36, interpolation=bicubic, max_size=None, antialias=None)
               CenterCrop(size=(32, 32))
               ToTensor()
               Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.201))
           )
[18:17:47.746353] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f56ef48b8e0>
/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[18:17:58.208271] Model = MaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(2, 2), stride=(2, 2))
    (norm): Identity()
  )
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (7): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (8): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (9): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (10): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (11): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
  (decoder_embed): Linear(in_features=192, out_features=128, bias=True)
  (decoder_blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=128, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=128, out_features=128, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=512, out_features=128, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=128, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=128, out_features=128, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=512, out_features=128, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (decoder_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
  (decoder_pred): Linear(in_features=128, out_features=12, bias=True)
  (head): Linear(in_features=192, out_features=10, bias=True)
  (classifier_mask): Sequential(
    (0): Linear(in_features=192, out_features=5, bias=True)
    (1): LogSoftmax(dim=1)
  )
)
[18:17:58.209222] number of params (M): 5.77
[18:17:58.209779] base lr: 1.00e-03
[18:17:58.209972] actual lr: 2.50e-04
[18:17:58.210121] accumulate grad iterations: 1
[18:17:58.210262] effective batch size: 64
[18:17:58.212020] criterion = LabelSmoothingCrossEntropy()
[18:17:58.212593] Start training for 100 epochs
[18:17:58.215727] log_dir: ./output_dir
[18:18:01.336175] Epoch: [0]  [  0/781]  eta: 0:40:35  lr: 0.000000  training_loss: 2.0337 (2.0337)  classification_loss: 0.2656 (0.2656)  loss_mask: 1.7681 (1.7681)  time: 3.1187  data: 0.5060  max mem: 4070
[18:18:04.560657] Epoch: [0]  [ 20/781]  eta: 0:03:49  lr: 0.000001  training_loss: 1.9799 (2.0010)  classification_loss: 0.2621 (0.2612)  loss_mask: 1.7284 (1.7398)  time: 0.1611  data: 0.0002  max mem: 4130
[18:18:07.771856] Epoch: [0]  [ 40/781]  eta: 0:02:52  lr: 0.000003  training_loss: 1.8623 (1.9396)  classification_loss: 0.2557 (0.2587)  loss_mask: 1.6062 (1.6808)  time: 0.1605  data: 0.0002  max mem: 4130
[18:18:10.981100] Epoch: [0]  [ 60/781]  eta: 0:02:30  lr: 0.000004  training_loss: 1.7643 (1.8844)  classification_loss: 0.2518 (0.2561)  loss_mask: 1.5213 (1.6284)  time: 0.1604  data: 0.0002  max mem: 4130
[18:18:14.212945] Epoch: [0]  [ 80/781]  eta: 0:02:18  lr: 0.000005  training_loss: 1.7547 (1.8523)  classification_loss: 0.2456 (0.2537)  loss_mask: 1.5174 (1.5986)  time: 0.1615  data: 0.0002  max mem: 4130
[18:18:17.485254] Epoch: [0]  [100/781]  eta: 0:02:09  lr: 0.000006  training_loss: 1.7299 (1.8267)  classification_loss: 0.2383 (0.2509)  loss_mask: 1.4883 (1.5759)  time: 0.1635  data: 0.0002  max mem: 4130
[18:18:20.718894] Epoch: [0]  [120/781]  eta: 0:02:02  lr: 0.000008  training_loss: 1.6808 (1.8042)  classification_loss: 0.2359 (0.2483)  loss_mask: 1.4424 (1.5558)  time: 0.1616  data: 0.0002  max mem: 4130
[18:18:23.934045] Epoch: [0]  [140/781]  eta: 0:01:56  lr: 0.000009  training_loss: 1.6893 (1.7887)  classification_loss: 0.2335 (0.2462)  loss_mask: 1.4542 (1.5426)  time: 0.1607  data: 0.0003  max mem: 4130
[18:18:27.150288] Epoch: [0]  [160/781]  eta: 0:01:51  lr: 0.000010  training_loss: 1.6759 (1.7745)  classification_loss: 0.2321 (0.2444)  loss_mask: 1.4430 (1.5300)  time: 0.1607  data: 0.0002  max mem: 4130
[18:18:30.348505] Epoch: [0]  [180/781]  eta: 0:01:46  lr: 0.000012  training_loss: 1.6737 (1.7634)  classification_loss: 0.2312 (0.2430)  loss_mask: 1.4442 (1.5204)  time: 0.1598  data: 0.0002  max mem: 4130
[18:18:33.591562] Epoch: [0]  [200/781]  eta: 0:01:42  lr: 0.000013  training_loss: 1.6647 (1.7535)  classification_loss: 0.2290 (0.2417)  loss_mask: 1.4307 (1.5118)  time: 0.1620  data: 0.0002  max mem: 4130
[18:18:36.831717] Epoch: [0]  [220/781]  eta: 0:01:37  lr: 0.000014  training_loss: 1.6468 (1.7443)  classification_loss: 0.2291 (0.2405)  loss_mask: 1.4191 (1.5037)  time: 0.1619  data: 0.0002  max mem: 4130
[18:18:40.057105] Epoch: [0]  [240/781]  eta: 0:01:33  lr: 0.000015  training_loss: 1.6459 (1.7363)  classification_loss: 0.2294 (0.2397)  loss_mask: 1.4179 (1.4967)  time: 0.1612  data: 0.0002  max mem: 4130
[18:18:43.289423] Epoch: [0]  [260/781]  eta: 0:01:29  lr: 0.000017  training_loss: 1.6452 (1.7290)  classification_loss: 0.2284 (0.2388)  loss_mask: 1.4160 (1.4902)  time: 0.1615  data: 0.0002  max mem: 4130
[18:18:46.500490] Epoch: [0]  [280/781]  eta: 0:01:26  lr: 0.000018  training_loss: 1.6226 (1.7216)  classification_loss: 0.2283 (0.2381)  loss_mask: 1.3922 (1.4835)  time: 0.1605  data: 0.0004  max mem: 4130
[18:18:49.742175] Epoch: [0]  [300/781]  eta: 0:01:22  lr: 0.000019  training_loss: 1.6174 (1.7152)  classification_loss: 0.2292 (0.2375)  loss_mask: 1.3929 (1.4778)  time: 0.1620  data: 0.0005  max mem: 4130
[18:18:52.995947] Epoch: [0]  [320/781]  eta: 0:01:18  lr: 0.000020  training_loss: 1.6140 (1.7085)  classification_loss: 0.2277 (0.2369)  loss_mask: 1.3834 (1.4717)  time: 0.1626  data: 0.0002  max mem: 4130
[18:18:56.233878] Epoch: [0]  [340/781]  eta: 0:01:14  lr: 0.000022  training_loss: 1.6055 (1.7032)  classification_loss: 0.2276 (0.2363)  loss_mask: 1.3795 (1.4669)  time: 0.1618  data: 0.0002  max mem: 4130
[18:18:59.502905] Epoch: [0]  [360/781]  eta: 0:01:11  lr: 0.000023  training_loss: 1.5929 (1.6970)  classification_loss: 0.2278 (0.2358)  loss_mask: 1.3655 (1.4612)  time: 0.1634  data: 0.0003  max mem: 4130
[18:19:02.749933] Epoch: [0]  [380/781]  eta: 0:01:07  lr: 0.000024  training_loss: 1.5938 (1.6916)  classification_loss: 0.2263 (0.2353)  loss_mask: 1.3683 (1.4563)  time: 0.1623  data: 0.0003  max mem: 4130
[18:19:05.983853] Epoch: [0]  [400/781]  eta: 0:01:04  lr: 0.000026  training_loss: 1.5676 (1.6855)  classification_loss: 0.2252 (0.2348)  loss_mask: 1.3381 (1.4507)  time: 0.1616  data: 0.0002  max mem: 4130
[18:19:09.206754] Epoch: [0]  [420/781]  eta: 0:01:00  lr: 0.000027  training_loss: 1.5119 (1.6784)  classification_loss: 0.2279 (0.2345)  loss_mask: 1.2866 (1.4438)  time: 0.1611  data: 0.0002  max mem: 4130
[18:19:12.479866] Epoch: [0]  [440/781]  eta: 0:00:57  lr: 0.000028  training_loss: 1.4901 (1.6702)  classification_loss: 0.2252 (0.2341)  loss_mask: 1.2630 (1.4361)  time: 0.1636  data: 0.0002  max mem: 4130
[18:19:15.739022] Epoch: [0]  [460/781]  eta: 0:00:53  lr: 0.000029  training_loss: 1.4531 (1.6618)  classification_loss: 0.2224 (0.2336)  loss_mask: 1.2317 (1.4281)  time: 0.1629  data: 0.0002  max mem: 4130
[18:19:19.001311] Epoch: [0]  [480/781]  eta: 0:00:50  lr: 0.000031  training_loss: 1.4261 (1.6522)  classification_loss: 0.2249 (0.2333)  loss_mask: 1.2019 (1.4188)  time: 0.1630  data: 0.0003  max mem: 4130
[18:19:22.307453] Epoch: [0]  [500/781]  eta: 0:00:47  lr: 0.000032  training_loss: 1.4267 (1.6431)  classification_loss: 0.2250 (0.2330)  loss_mask: 1.1972 (1.4101)  time: 0.1652  data: 0.0003  max mem: 4130
[18:19:25.575427] Epoch: [0]  [520/781]  eta: 0:00:43  lr: 0.000033  training_loss: 1.3137 (1.6302)  classification_loss: 0.2268 (0.2328)  loss_mask: 1.0834 (1.3974)  time: 0.1633  data: 0.0003  max mem: 4130

[18:19:28.838807] Epoch: [0]  [540/781]  eta: 0:00:40  lr: 0.000035  training_loss: 1.3336 (1.6189)  classification_loss: 0.2258 (0.2325)  loss_mask: 1.1082 (1.3863)  time: 0.1631  data: 0.0003  max mem: 4130
[18:19:32.091436] Epoch: [0]  [560/781]  eta: 0:00:36  lr: 0.000036  training_loss: 1.2486 (1.6062)  classification_loss: 0.2250 (0.2323)  loss_mask: 1.0230 (1.3739)  time: 0.1626  data: 0.0002  max mem: 4130
[18:19:35.353965] Epoch: [0]  [580/781]  eta: 0:00:33  lr: 0.000037  training_loss: 1.2537 (1.5932)  classification_loss: 0.2243 (0.2320)  loss_mask: 1.0263 (1.3611)  time: 0.1630  data: 0.0002  max mem: 4130
[18:19:38.598549] Epoch: [0]  [600/781]  eta: 0:00:30  lr: 0.000038  training_loss: 1.1899 (1.5801)  classification_loss: 0.2244 (0.2318)  loss_mask: 0.9605 (1.3483)  time: 0.1622  data: 0.0002  max mem: 4130
[18:19:41.865745] Epoch: [0]  [620/781]  eta: 0:00:26  lr: 0.000040  training_loss: 1.2815 (1.5704)  classification_loss: 0.2242 (0.2316)  loss_mask: 1.0559 (1.3388)  time: 0.1633  data: 0.0003  max mem: 4130
[18:19:45.130387] Epoch: [0]  [640/781]  eta: 0:00:23  lr: 0.000041  training_loss: 1.1017 (1.5563)  classification_loss: 0.2249 (0.2314)  loss_mask: 0.8719 (1.3250)  time: 0.1632  data: 0.0002  max mem: 4130
[18:19:48.386087] Epoch: [0]  [660/781]  eta: 0:00:20  lr: 0.000042  training_loss: 1.0803 (1.5420)  classification_loss: 0.2257 (0.2312)  loss_mask: 0.8526 (1.3107)  time: 0.1627  data: 0.0003  max mem: 4130
[18:19:51.653859] Epoch: [0]  [680/781]  eta: 0:00:16  lr: 0.000044  training_loss: 1.1109 (1.5306)  classification_loss: 0.2255 (0.2311)  loss_mask: 0.8813 (1.2995)  time: 0.1633  data: 0.0002  max mem: 4130
[18:19:54.906381] Epoch: [0]  [700/781]  eta: 0:00:13  lr: 0.000045  training_loss: 1.1394 (1.5203)  classification_loss: 0.2250 (0.2309)  loss_mask: 0.9131 (1.2894)  time: 0.1626  data: 0.0002  max mem: 4130
[18:19:58.176745] Epoch: [0]  [720/781]  eta: 0:00:10  lr: 0.000046  training_loss: 1.0841 (1.5087)  classification_loss: 0.2255 (0.2308)  loss_mask: 0.8634 (1.2779)  time: 0.1634  data: 0.0002  max mem: 4130
[18:20:01.442682] Epoch: [0]  [740/781]  eta: 0:00:06  lr: 0.000047  training_loss: 1.0518 (1.4969)  classification_loss: 0.2249 (0.2306)  loss_mask: 0.8284 (1.2662)  time: 0.1632  data: 0.0003  max mem: 4130
[18:20:04.702981] Epoch: [0]  [760/781]  eta: 0:00:03  lr: 0.000049  training_loss: 0.9647 (1.4839)  classification_loss: 0.2246 (0.2305)  loss_mask: 0.7423 (1.2535)  time: 0.1629  data: 0.0002  max mem: 4130
[18:20:07.932019] Epoch: [0]  [780/781]  eta: 0:00:00  lr: 0.000050  training_loss: 0.9681 (1.4710)  classification_loss: 0.2249 (0.2303)  loss_mask: 0.7444 (1.2407)  time: 0.1614  data: 0.0002  max mem: 4130
[18:20:08.052363] Epoch: [0] Total time: 0:02:09 (0.1662 s / it)
[18:20:08.053273] Averaged stats: lr: 0.000050  training_loss: 0.9681 (1.4710)  classification_loss: 0.2249 (0.2303)  loss_mask: 0.7444 (1.2407)
[18:20:09.934784] Test:  [  0/157]  eta: 0:01:39  testing_loss: 2.1177 (2.1177)  acc1: 42.1875 (42.1875)  acc5: 84.3750 (84.3750)  time: 0.6353  data: 0.6025  max mem: 4130
[18:20:10.232841] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 2.1696 (2.1629)  acc1: 23.4375 (26.7045)  acc5: 78.1250 (78.2670)  time: 0.0847  data: 0.0549  max mem: 4130
[18:20:10.516727] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 2.1719 (2.1668)  acc1: 23.4375 (25.3720)  acc5: 76.5625 (76.4881)  time: 0.0290  data: 0.0002  max mem: 4130
[18:20:10.808392] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 2.1615 (2.1611)  acc1: 23.4375 (25.8569)  acc5: 78.1250 (77.1169)  time: 0.0287  data: 0.0002  max mem: 4130
[18:20:11.093427] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 2.1525 (2.1618)  acc1: 21.8750 (25.1905)  acc5: 78.1250 (77.0579)  time: 0.0287  data: 0.0002  max mem: 4130
[18:20:11.380558] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 2.1525 (2.1613)  acc1: 23.4375 (25.0000)  acc5: 78.1250 (77.3591)  time: 0.0285  data: 0.0002  max mem: 4130
[18:20:11.664521] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 2.1523 (2.1609)  acc1: 23.4375 (24.8975)  acc5: 78.1250 (77.6895)  time: 0.0284  data: 0.0002  max mem: 4130
[18:20:11.948641] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 2.1500 (2.1593)  acc1: 25.0000 (25.0440)  acc5: 79.6875 (78.0150)  time: 0.0282  data: 0.0002  max mem: 4130
[18:20:12.236764] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 2.1450 (2.1582)  acc1: 26.5625 (25.0965)  acc5: 81.2500 (78.5108)  time: 0.0284  data: 0.0002  max mem: 4130
[18:20:12.518801] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 2.1564 (2.1598)  acc1: 23.4375 (24.7768)  acc5: 81.2500 (78.4856)  time: 0.0284  data: 0.0002  max mem: 4130
[18:20:12.801161] Test:  [100/157]  eta: 0:00:01  testing_loss: 2.1620 (2.1591)  acc1: 25.0000 (25.1238)  acc5: 78.1250 (78.4808)  time: 0.0281  data: 0.0002  max mem: 4130
[18:20:13.082493] Test:  [110/157]  eta: 0:00:01  testing_loss: 2.1620 (2.1598)  acc1: 25.0000 (25.0704)  acc5: 76.5625 (78.4347)  time: 0.0281  data: 0.0002  max mem: 4130
[18:20:13.363882] Test:  [120/157]  eta: 0:00:01  testing_loss: 2.1614 (2.1586)  acc1: 23.4375 (25.1033)  acc5: 76.5625 (78.5640)  time: 0.0280  data: 0.0002  max mem: 4130
[18:20:13.645421] Test:  [130/157]  eta: 0:00:00  testing_loss: 2.1627 (2.1597)  acc1: 23.4375 (24.9046)  acc5: 78.1250 (78.4470)  time: 0.0280  data: 0.0002  max mem: 4130
[18:20:13.925770] Test:  [140/157]  eta: 0:00:00  testing_loss: 2.1627 (2.1589)  acc1: 23.4375 (25.0665)  acc5: 76.5625 (78.4574)  time: 0.0280  data: 0.0001  max mem: 4130
[18:20:14.205197] Test:  [150/157]  eta: 0:00:00  testing_loss: 2.1502 (2.1581)  acc1: 26.5625 (25.1449)  acc5: 78.1250 (78.5906)  time: 0.0279  data: 0.0001  max mem: 4130
[18:20:14.464990] Test:  [156/157]  eta: 0:00:00  testing_loss: 2.1480 (2.1578)  acc1: 25.0000 (25.0800)  acc5: 81.2500 (78.7000)  time: 0.0324  data: 0.0001  max mem: 4130
[18:20:14.621756] Test: Total time: 0:00:05 (0.0339 s / it)
[18:20:14.622197] * Acc@1 25.080 Acc@5 78.700 loss 2.158
[18:20:14.622482] Accuracy of the network on the 10000 test images: 25.1%
[18:20:14.622654] Max accuracy: 25.08%
[18:20:14.935330] log_dir: ./output_dir
[18:20:15.765697] Epoch: [1]  [  0/781]  eta: 0:10:47  lr: 0.000050  training_loss: 1.0907 (1.0907)  classification_loss: 0.2277 (0.2277)  loss_mask: 0.8629 (0.8629)  time: 0.8287  data: 0.6189  max mem: 4132
[18:20:19.041262] Epoch: [1]  [ 20/781]  eta: 0:02:28  lr: 0.000051  training_loss: 1.0349 (1.0359)  classification_loss: 0.2249 (0.2251)  loss_mask: 0.8052 (0.8108)  time: 0.1637  data: 0.0003  max mem: 4132
[18:20:22.286274] Epoch: [1]  [ 40/781]  eta: 0:02:12  lr: 0.000053  training_loss: 0.9742 (1.0204)  classification_loss: 0.2249 (0.2252)  loss_mask: 0.7494 (0.7952)  time: 0.1622  data: 0.0005  max mem: 4132
[18:20:25.573987] Epoch: [1]  [ 60/781]  eta: 0:02:05  lr: 0.000054  training_loss: 0.9762 (1.0125)  classification_loss: 0.2256 (0.2254)  loss_mask: 0.7507 (0.7871)  time: 0.1643  data: 0.0003  max mem: 4132
[18:20:28.813602] Epoch: [1]  [ 80/781]  eta: 0:02:00  lr: 0.000055  training_loss: 0.9783 (1.0187)  classification_loss: 0.2239 (0.2252)  loss_mask: 0.7544 (0.7935)  time: 0.1619  data: 0.0003  max mem: 4132
[18:20:32.044938] Epoch: [1]  [100/781]  eta: 0:01:55  lr: 0.000056  training_loss: 1.0391 (1.0265)  classification_loss: 0.2246 (0.2251)  loss_mask: 0.8135 (0.8013)  time: 0.1615  data: 0.0002  max mem: 4132
[18:20:35.289482] Epoch: [1]  [120/781]  eta: 0:01:51  lr: 0.000058  training_loss: 0.9590 (1.0214)  classification_loss: 0.2250 (0.2251)  loss_mask: 0.7354 (0.7962)  time: 0.1621  data: 0.0003  max mem: 4132
[18:20:38.546863] Epoch: [1]  [140/781]  eta: 0:01:47  lr: 0.000059  training_loss: 0.9356 (1.0086)  classification_loss: 0.2235 (0.2250)  loss_mask: 0.7082 (0.7836)  time: 0.1627  data: 0.0003  max mem: 4132
[18:20:41.778252] Epoch: [1]  [160/781]  eta: 0:01:43  lr: 0.000060  training_loss: 1.0679 (1.0144)  classification_loss: 0.2251 (0.2251)  loss_mask: 0.8441 (0.7893)  time: 0.1615  data: 0.0002  max mem: 4132
[18:20:45.026957] Epoch: [1]  [180/781]  eta: 0:01:39  lr: 0.000062  training_loss: 0.9835 (1.0120)  classification_loss: 0.2250 (0.2250)  loss_mask: 0.7557 (0.7870)  time: 0.1623  data: 0.0003  max mem: 4132
[18:20:48.276599] Epoch: [1]  [200/781]  eta: 0:01:36  lr: 0.000063  training_loss: 0.9477 (1.0050)  classification_loss: 0.2235 (0.2249)  loss_mask: 0.7258 (0.7801)  time: 0.1624  data: 0.0003  max mem: 4132
[18:20:51.526851] Epoch: [1]  [220/781]  eta: 0:01:32  lr: 0.000064  training_loss: 0.9240 (0.9989)  classification_loss: 0.2262 (0.2249)  loss_mask: 0.6966 (0.7739)  time: 0.1624  data: 0.0002  max mem: 4132
[18:20:54.753390] Epoch: [1]  [240/781]  eta: 0:01:29  lr: 0.000065  training_loss: 0.9678 (0.9981)  classification_loss: 0.2259 (0.2249)  loss_mask: 0.7392 (0.7731)  time: 0.1612  data: 0.0003  max mem: 4132
[18:20:57.997324] Epoch: [1]  [260/781]  eta: 0:01:25  lr: 0.000067  training_loss: 0.9277 (0.9943)  classification_loss: 0.2249 (0.2249)  loss_mask: 0.7023 (0.7694)  time: 0.1621  data: 0.0002  max mem: 4132
[18:21:01.276687] Epoch: [1]  [280/781]  eta: 0:01:22  lr: 0.000068  training_loss: 0.9111 (0.9904)  classification_loss: 0.2265 (0.2250)  loss_mask: 0.6870 (0.7654)  time: 0.1639  data: 0.0002  max mem: 4132
[18:21:04.535458] Epoch: [1]  [300/781]  eta: 0:01:19  lr: 0.000069  training_loss: 0.8630 (0.9827)  classification_loss: 0.2242 (0.2250)  loss_mask: 0.6408 (0.7577)  time: 0.1628  data: 0.0002  max mem: 4132
[18:21:07.777384] Epoch: [1]  [320/781]  eta: 0:01:15  lr: 0.000070  training_loss: 0.8737 (0.9771)  classification_loss: 0.2238 (0.2249)  loss_mask: 0.6498 (0.7522)  time: 0.1620  data: 0.0002  max mem: 4132
[18:21:11.021733] Epoch: [1]  [340/781]  eta: 0:01:12  lr: 0.000072  training_loss: 0.8803 (0.9727)  classification_loss: 0.2256 (0.2249)  loss_mask: 0.6514 (0.7478)  time: 0.1621  data: 0.0002  max mem: 4132
[18:21:14.278553] Epoch: [1]  [360/781]  eta: 0:01:09  lr: 0.000073  training_loss: 0.9029 (0.9706)  classification_loss: 0.2253 (0.2249)  loss_mask: 0.6777 (0.7457)  time: 0.1628  data: 0.0002  max mem: 4132
[18:21:17.555384] Epoch: [1]  [380/781]  eta: 0:01:05  lr: 0.000074  training_loss: 0.9831 (0.9709)  classification_loss: 0.2257 (0.2250)  loss_mask: 0.7552 (0.7459)  time: 0.1638  data: 0.0002  max mem: 4132
[18:21:20.809595] Epoch: [1]  [400/781]  eta: 0:01:02  lr: 0.000076  training_loss: 0.8938 (0.9673)  classification_loss: 0.2246 (0.2249)  loss_mask: 0.6701 (0.7424)  time: 0.1626  data: 0.0002  max mem: 4132
[18:21:24.056907] Epoch: [1]  [420/781]  eta: 0:00:59  lr: 0.000077  training_loss: 0.8758 (0.9641)  classification_loss: 0.2237 (0.2249)  loss_mask: 0.6534 (0.7392)  time: 0.1623  data: 0.0002  max mem: 4132
[18:21:27.308549] Epoch: [1]  [440/781]  eta: 0:00:55  lr: 0.000078  training_loss: 0.8658 (0.9599)  classification_loss: 0.2235 (0.2249)  loss_mask: 0.6423 (0.7351)  time: 0.1625  data: 0.0002  max mem: 4132
[18:21:30.545257] Epoch: [1]  [460/781]  eta: 0:00:52  lr: 0.000079  training_loss: 0.8537 (0.9556)  classification_loss: 0.2236 (0.2248)  loss_mask: 0.6294 (0.7307)  time: 0.1618  data: 0.0002  max mem: 4132
[18:21:33.778619] Epoch: [1]  [480/781]  eta: 0:00:49  lr: 0.000081  training_loss: 0.9254 (0.9562)  classification_loss: 0.2243 (0.2248)  loss_mask: 0.6984 (0.7314)  time: 0.1616  data: 0.0002  max mem: 4132
[18:21:37.080154] Epoch: [1]  [500/781]  eta: 0:00:46  lr: 0.000082  training_loss: 0.9848 (0.9574)  classification_loss: 0.2238 (0.2248)  loss_mask: 0.7651 (0.7326)  time: 0.1650  data: 0.0003  max mem: 4132
[18:21:40.305257] Epoch: [1]  [520/781]  eta: 0:00:42  lr: 0.000083  training_loss: 0.8353 (0.9534)  classification_loss: 0.2243 (0.2248)  loss_mask: 0.6121 (0.7287)  time: 0.1612  data: 0.0002  max mem: 4132
[18:21:43.601397] Epoch: [1]  [540/781]  eta: 0:00:39  lr: 0.000085  training_loss: 0.8341 (0.9502)  classification_loss: 0.2239 (0.2248)  loss_mask: 0.6153 (0.7255)  time: 0.1647  data: 0.0002  max mem: 4132
[18:21:46.844457] Epoch: [1]  [560/781]  eta: 0:00:36  lr: 0.000086  training_loss: 0.8625 (0.9472)  classification_loss: 0.2236 (0.2248)  loss_mask: 0.6406 (0.7224)  time: 0.1621  data: 0.0002  max mem: 4132
[18:21:50.104696] Epoch: [1]  [580/781]  eta: 0:00:32  lr: 0.000087  training_loss: 0.9209 (0.9468)  classification_loss: 0.2247 (0.2248)  loss_mask: 0.6962 (0.7220)  time: 0.1629  data: 0.0006  max mem: 4132
[18:21:53.356274] Epoch: [1]  [600/781]  eta: 0:00:29  lr: 0.000088  training_loss: 0.9842 (0.9479)  classification_loss: 0.2242 (0.2248)  loss_mask: 0.7598 (0.7231)  time: 0.1625  data: 0.0002  max mem: 4132
[18:21:56.601596] Epoch: [1]  [620/781]  eta: 0:00:26  lr: 0.000090  training_loss: 0.8902 (0.9466)  classification_loss: 0.2237 (0.2247)  loss_mask: 0.6665 (0.7219)  time: 0.1622  data: 0.0002  max mem: 4132
[18:21:59.853236] Epoch: [1]  [640/781]  eta: 0:00:23  lr: 0.000091  training_loss: 0.8791 (0.9445)  classification_loss: 0.2235 (0.2247)  loss_mask: 0.6555 (0.7198)  time: 0.1625  data: 0.0003  max mem: 4132
[18:22:03.102949] Epoch: [1]  [660/781]  eta: 0:00:19  lr: 0.000092  training_loss: 0.9261 (0.9438)  classification_loss: 0.2246 (0.2247)  loss_mask: 0.7065 (0.7191)  time: 0.1624  data: 0.0003  max mem: 4132
[18:22:06.388995] Epoch: [1]  [680/781]  eta: 0:00:16  lr: 0.000094  training_loss: 0.8654 (0.9419)  classification_loss: 0.2253 (0.2247)  loss_mask: 0.6451 (0.7172)  time: 0.1642  data: 0.0002  max mem: 4132
[18:22:09.637029] Epoch: [1]  [700/781]  eta: 0:00:13  lr: 0.000095  training_loss: 0.8868 (0.9406)  classification_loss: 0.2224 (0.2247)  loss_mask: 0.6661 (0.7159)  time: 0.1623  data: 0.0002  max mem: 4132
[18:22:12.878006] Epoch: [1]  [720/781]  eta: 0:00:09  lr: 0.000096  training_loss: 0.8451 (0.9384)  classification_loss: 0.2225 (0.2246)  loss_mask: 0.6191 (0.7138)  time: 0.1620  data: 0.0002  max mem: 4132
[18:22:16.113514] Epoch: [1]  [740/781]  eta: 0:00:06  lr: 0.000097  training_loss: 0.9214 (0.9386)  classification_loss: 0.2271 (0.2247)  loss_mask: 0.6943 (0.7139)  time: 0.1617  data: 0.0002  max mem: 4132
[18:22:19.347392] Epoch: [1]  [760/781]  eta: 0:00:03  lr: 0.000099  training_loss: 0.8999 (0.9377)  classification_loss: 0.2236 (0.2247)  loss_mask: 0.6782 (0.7130)  time: 0.1616  data: 0.0002  max mem: 4132
[18:22:22.570675] Epoch: [1]  [780/781]  eta: 0:00:00  lr: 0.000100  training_loss: 0.8144 (0.9351)  classification_loss: 0.2252 (0.2247)  loss_mask: 0.5892 (0.7104)  time: 0.1611  data: 0.0002  max mem: 4132
[18:22:22.721110] Epoch: [1] Total time: 0:02:07 (0.1636 s / it)
[18:22:22.721576] Averaged stats: lr: 0.000100  training_loss: 0.8144 (0.9351)  classification_loss: 0.2252 (0.2247)  loss_mask: 0.5892 (0.7104)
[18:22:23.350108] Test:  [  0/157]  eta: 0:01:37  testing_loss: 2.1031 (2.1031)  acc1: 34.3750 (34.3750)  acc5: 84.3750 (84.3750)  time: 0.6238  data: 0.5918  max mem: 4132
[18:22:23.638756] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 2.1483 (2.1445)  acc1: 25.0000 (25.4261)  acc5: 75.0000 (77.5568)  time: 0.0827  data: 0.0540  max mem: 4132
[18:22:23.925544] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 2.1503 (2.1462)  acc1: 23.4375 (24.4048)  acc5: 75.0000 (75.9673)  time: 0.0285  data: 0.0002  max mem: 4132
[18:22:24.212517] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 2.1363 (2.1419)  acc1: 23.4375 (24.8992)  acc5: 75.0000 (76.1089)  time: 0.0285  data: 0.0002  max mem: 4132
[18:22:24.497112] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 2.1377 (2.1430)  acc1: 25.0000 (24.6570)  acc5: 78.1250 (76.6006)  time: 0.0284  data: 0.0002  max mem: 4132
[18:22:24.784746] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 2.1377 (2.1412)  acc1: 25.0000 (24.6936)  acc5: 78.1250 (76.6544)  time: 0.0284  data: 0.0002  max mem: 4132
[18:22:25.066053] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 2.1334 (2.1411)  acc1: 25.0000 (24.9488)  acc5: 78.1250 (76.9980)  time: 0.0283  data: 0.0002  max mem: 4132
[18:22:25.350627] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 2.1334 (2.1397)  acc1: 25.0000 (24.8680)  acc5: 79.6875 (77.2007)  time: 0.0282  data: 0.0002  max mem: 4132
[18:22:25.635392] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 2.1217 (2.1396)  acc1: 23.4375 (24.8071)  acc5: 79.6875 (77.4306)  time: 0.0283  data: 0.0002  max mem: 4132
[18:22:25.917192] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 2.1510 (2.1423)  acc1: 21.8750 (24.3990)  acc5: 76.5625 (77.0261)  time: 0.0282  data: 0.0002  max mem: 4132
[18:22:26.199178] Test:  [100/157]  eta: 0:00:01  testing_loss: 2.1539 (2.1422)  acc1: 23.4375 (24.6132)  acc5: 75.0000 (76.9802)  time: 0.0281  data: 0.0002  max mem: 4132
[18:22:26.482623] Test:  [110/157]  eta: 0:00:01  testing_loss: 2.1440 (2.1433)  acc1: 23.4375 (24.3806)  acc5: 76.5625 (76.8863)  time: 0.0282  data: 0.0002  max mem: 4132
[18:22:26.765735] Test:  [120/157]  eta: 0:00:01  testing_loss: 2.1377 (2.1421)  acc1: 25.0000 (24.5351)  acc5: 76.5625 (76.8337)  time: 0.0282  data: 0.0002  max mem: 4132
[18:22:27.047819] Test:  [130/157]  eta: 0:00:00  testing_loss: 2.1375 (2.1426)  acc1: 26.5625 (24.5706)  acc5: 76.5625 (76.7533)  time: 0.0281  data: 0.0002  max mem: 4132
[18:22:27.329510] Test:  [140/157]  eta: 0:00:00  testing_loss: 2.1459 (2.1420)  acc1: 25.0000 (24.5567)  acc5: 76.5625 (76.6733)  time: 0.0281  data: 0.0001  max mem: 4132
[18:22:27.609816] Test:  [150/157]  eta: 0:00:00  testing_loss: 2.1321 (2.1413)  acc1: 25.0000 (24.6275)  acc5: 78.1250 (76.8108)  time: 0.0280  data: 0.0001  max mem: 4132
[18:22:27.760072] Test:  [156/157]  eta: 0:00:00  testing_loss: 2.1254 (2.1405)  acc1: 25.0000 (24.5900)  acc5: 78.1250 (76.9300)  time: 0.0270  data: 0.0001  max mem: 4132
[18:22:27.897728] Test: Total time: 0:00:05 (0.0329 s / it)
[18:22:27.898196] * Acc@1 24.590 Acc@5 76.930 loss 2.141
[18:22:27.898500] Accuracy of the network on the 10000 test images: 24.6%
[18:22:27.898708] Max accuracy: 25.08%
[18:22:28.181439] log_dir: ./output_dir
[18:22:29.010279] Epoch: [2]  [  0/781]  eta: 0:10:46  lr: 0.000100  training_loss: 0.7584 (0.7584)  classification_loss: 0.2256 (0.2256)  loss_mask: 0.5328 (0.5328)  time: 0.8272  data: 0.6400  max mem: 4132
[18:22:32.267166] Epoch: [2]  [ 20/781]  eta: 0:02:27  lr: 0.000101  training_loss: 0.8051 (0.8209)  classification_loss: 0.2241 (0.2242)  loss_mask: 0.5805 (0.5967)  time: 0.1627  data: 0.0002  max mem: 4132
[18:22:35.503792] Epoch: [2]  [ 40/781]  eta: 0:02:12  lr: 0.000103  training_loss: 0.8114 (0.8231)  classification_loss: 0.2239 (0.2242)  loss_mask: 0.5816 (0.5989)  time: 0.1618  data: 0.0003  max mem: 4132
[18:22:38.748352] Epoch: [2]  [ 60/781]  eta: 0:02:04  lr: 0.000104  training_loss: 0.8670 (0.8353)  classification_loss: 0.2249 (0.2244)  loss_mask: 0.6467 (0.6109)  time: 0.1621  data: 0.0002  max mem: 4132
[18:22:41.985936] Epoch: [2]  [ 80/781]  eta: 0:01:59  lr: 0.000105  training_loss: 0.8789 (0.8454)  classification_loss: 0.2239 (0.2244)  loss_mask: 0.6546 (0.6211)  time: 0.1618  data: 0.0002  max mem: 4132
[18:22:45.218235] Epoch: [2]  [100/781]  eta: 0:01:54  lr: 0.000106  training_loss: 0.7951 (0.8430)  classification_loss: 0.2231 (0.2242)  loss_mask: 0.5742 (0.6188)  time: 0.1615  data: 0.0003  max mem: 4132
[18:22:48.453565] Epoch: [2]  [120/781]  eta: 0:01:50  lr: 0.000108  training_loss: 0.8289 (0.8490)  classification_loss: 0.2243 (0.2243)  loss_mask: 0.6071 (0.6247)  time: 0.1617  data: 0.0002  max mem: 4132
[18:22:51.772059] Epoch: [2]  [140/781]  eta: 0:01:47  lr: 0.000109  training_loss: 0.8489 (0.8494)  classification_loss: 0.2220 (0.2242)  loss_mask: 0.6215 (0.6252)  time: 0.1658  data: 0.0004  max mem: 4132
[18:22:55.021918] Epoch: [2]  [160/781]  eta: 0:01:43  lr: 0.000110  training_loss: 0.9061 (0.8549)  classification_loss: 0.2239 (0.2242)  loss_mask: 0.6802 (0.6308)  time: 0.1624  data: 0.0002  max mem: 4132
[18:22:58.262186] Epoch: [2]  [180/781]  eta: 0:01:39  lr: 0.000112  training_loss: 0.8776 (0.8581)  classification_loss: 0.2231 (0.2242)  loss_mask: 0.6513 (0.6339)  time: 0.1619  data: 0.0002  max mem: 4132
[18:23:01.501560] Epoch: [2]  [200/781]  eta: 0:01:36  lr: 0.000113  training_loss: 0.8168 (0.8552)  classification_loss: 0.2230 (0.2240)  loss_mask: 0.5924 (0.6312)  time: 0.1619  data: 0.0002  max mem: 4132
[18:23:04.763230] Epoch: [2]  [220/781]  eta: 0:01:32  lr: 0.000114  training_loss: 0.8010 (0.8516)  classification_loss: 0.2232 (0.2239)  loss_mask: 0.5764 (0.6277)  time: 0.1630  data: 0.0003  max mem: 4132
[18:23:08.016355] Epoch: [2]  [240/781]  eta: 0:01:29  lr: 0.000115  training_loss: 0.8749 (0.8520)  classification_loss: 0.2239 (0.2239)  loss_mask: 0.6542 (0.6281)  time: 0.1626  data: 0.0002  max mem: 4132
[18:23:11.258792] Epoch: [2]  [260/781]  eta: 0:01:25  lr: 0.000117  training_loss: 0.8763 (0.8539)  classification_loss: 0.2238 (0.2240)  loss_mask: 0.6478 (0.6299)  time: 0.1621  data: 0.0002  max mem: 4132
[18:23:14.505037] Epoch: [2]  [280/781]  eta: 0:01:22  lr: 0.000118  training_loss: 0.8474 (0.8534)  classification_loss: 0.2250 (0.2240)  loss_mask: 0.6165 (0.6294)  time: 0.1622  data: 0.0002  max mem: 4132
[18:23:17.752713] Epoch: [2]  [300/781]  eta: 0:01:19  lr: 0.000119  training_loss: 0.8710 (0.8559)  classification_loss: 0.2231 (0.2240)  loss_mask: 0.6435 (0.6318)  time: 0.1623  data: 0.0002  max mem: 4132
[18:23:20.997281] Epoch: [2]  [320/781]  eta: 0:01:15  lr: 0.000120  training_loss: 0.8189 (0.8545)  classification_loss: 0.2223 (0.2239)  loss_mask: 0.5937 (0.6306)  time: 0.1622  data: 0.0002  max mem: 4132
[18:23:24.240564] Epoch: [2]  [340/781]  eta: 0:01:12  lr: 0.000122  training_loss: 0.8089 (0.8517)  classification_loss: 0.2244 (0.2239)  loss_mask: 0.5883 (0.6277)  time: 0.1621  data: 0.0002  max mem: 4132
[18:23:27.466644] Epoch: [2]  [360/781]  eta: 0:01:09  lr: 0.000123  training_loss: 0.7889 (0.8486)  classification_loss: 0.2234 (0.2239)  loss_mask: 0.5608 (0.6247)  time: 0.1612  data: 0.0002  max mem: 4132
[18:23:30.708673] Epoch: [2]  [380/781]  eta: 0:01:05  lr: 0.000124  training_loss: 0.8719 (0.8502)  classification_loss: 0.2232 (0.2239)  loss_mask: 0.6483 (0.6263)  time: 0.1620  data: 0.0002  max mem: 4132
[18:23:33.977485] Epoch: [2]  [400/781]  eta: 0:01:02  lr: 0.000126  training_loss: 0.8543 (0.8502)  classification_loss: 0.2223 (0.2239)  loss_mask: 0.6293 (0.6264)  time: 0.1634  data: 0.0002  max mem: 4132
[18:23:37.232184] Epoch: [2]  [420/781]  eta: 0:00:59  lr: 0.000127  training_loss: 0.7487 (0.8464)  classification_loss: 0.2235 (0.2239)  loss_mask: 0.5233 (0.6225)  time: 0.1627  data: 0.0002  max mem: 4132
[18:23:40.470600] Epoch: [2]  [440/781]  eta: 0:00:55  lr: 0.000128  training_loss: 0.8215 (0.8476)  classification_loss: 0.2236 (0.2239)  loss_mask: 0.5996 (0.6238)  time: 0.1618  data: 0.0002  max mem: 4132
[18:23:43.766484] Epoch: [2]  [460/781]  eta: 0:00:52  lr: 0.000129  training_loss: 0.7799 (0.8447)  classification_loss: 0.2226 (0.2238)  loss_mask: 0.5522 (0.6209)  time: 0.1647  data: 0.0002  max mem: 4132
[18:23:47.052444] Epoch: [2]  [480/781]  eta: 0:00:49  lr: 0.000131  training_loss: 0.7916 (0.8439)  classification_loss: 0.2235 (0.2238)  loss_mask: 0.5690 (0.6201)  time: 0.1642  data: 0.0002  max mem: 4132
[18:23:50.296601] Epoch: [2]  [500/781]  eta: 0:00:46  lr: 0.000132  training_loss: 0.8024 (0.8432)  classification_loss: 0.2220 (0.2238)  loss_mask: 0.5821 (0.6195)  time: 0.1621  data: 0.0002  max mem: 4132
[18:23:53.539483] Epoch: [2]  [520/781]  eta: 0:00:42  lr: 0.000133  training_loss: 0.8490 (0.8442)  classification_loss: 0.2230 (0.2238)  loss_mask: 0.6259 (0.6204)  time: 0.1621  data: 0.0003  max mem: 4132
[18:23:56.780569] Epoch: [2]  [540/781]  eta: 0:00:39  lr: 0.000135  training_loss: 0.8139 (0.8431)  classification_loss: 0.2229 (0.2238)  loss_mask: 0.5894 (0.6194)  time: 0.1620  data: 0.0002  max mem: 4132
[18:24:00.016514] Epoch: [2]  [560/781]  eta: 0:00:36  lr: 0.000136  training_loss: 0.8031 (0.8417)  classification_loss: 0.2236 (0.2237)  loss_mask: 0.5831 (0.6180)  time: 0.1617  data: 0.0002  max mem: 4132
[18:24:03.293673] Epoch: [2]  [580/781]  eta: 0:00:32  lr: 0.000137  training_loss: 0.7955 (0.8400)  classification_loss: 0.2225 (0.2237)  loss_mask: 0.5689 (0.6163)  time: 0.1638  data: 0.0003  max mem: 4132
[18:24:06.524733] Epoch: [2]  [600/781]  eta: 0:00:29  lr: 0.000138  training_loss: 0.7750 (0.8385)  classification_loss: 0.2235 (0.2237)  loss_mask: 0.5545 (0.6148)  time: 0.1615  data: 0.0002  max mem: 4132
[18:24:09.773497] Epoch: [2]  [620/781]  eta: 0:00:26  lr: 0.000140  training_loss: 0.7204 (0.8360)  classification_loss: 0.2221 (0.2237)  loss_mask: 0.4921 (0.6123)  time: 0.1623  data: 0.0003  max mem: 4132
[18:24:13.018739] Epoch: [2]  [640/781]  eta: 0:00:23  lr: 0.000141  training_loss: 0.8094 (0.8356)  classification_loss: 0.2223 (0.2237)  loss_mask: 0.5825 (0.6119)  time: 0.1622  data: 0.0002  max mem: 4132
[18:24:16.263058] Epoch: [2]  [660/781]  eta: 0:00:19  lr: 0.000142  training_loss: 0.7953 (0.8342)  classification_loss: 0.2231 (0.2237)  loss_mask: 0.5758 (0.6105)  time: 0.1621  data: 0.0003  max mem: 4132
[18:24:19.501428] Epoch: [2]  [680/781]  eta: 0:00:16  lr: 0.000144  training_loss: 0.8036 (0.8337)  classification_loss: 0.2235 (0.2237)  loss_mask: 0.5780 (0.6100)  time: 0.1618  data: 0.0003  max mem: 4132
[18:24:22.734994] Epoch: [2]  [700/781]  eta: 0:00:13  lr: 0.000145  training_loss: 0.8621 (0.8351)  classification_loss: 0.2221 (0.2236)  loss_mask: 0.6387 (0.6115)  time: 0.1616  data: 0.0002  max mem: 4132
[18:24:25.968284] Epoch: [2]  [720/781]  eta: 0:00:09  lr: 0.000146  training_loss: 0.8665 (0.8364)  classification_loss: 0.2232 (0.2237)  loss_mask: 0.6407 (0.6127)  time: 0.1616  data: 0.0003  max mem: 4132
[18:24:29.235234] Epoch: [2]  [740/781]  eta: 0:00:06  lr: 0.000147  training_loss: 0.7748 (0.8354)  classification_loss: 0.2238 (0.2237)  loss_mask: 0.5494 (0.6117)  time: 0.1632  data: 0.0002  max mem: 4132
[18:24:32.539573] Epoch: [2]  [760/781]  eta: 0:00:03  lr: 0.000149  training_loss: 0.8465 (0.8355)  classification_loss: 0.2250 (0.2237)  loss_mask: 0.6195 (0.6118)  time: 0.1651  data: 0.0003  max mem: 4132
[18:24:35.771847] Epoch: [2]  [780/781]  eta: 0:00:00  lr: 0.000150  training_loss: 0.8520 (0.8357)  classification_loss: 0.2237 (0.2237)  loss_mask: 0.6282 (0.6120)  time: 0.1615  data: 0.0002  max mem: 4132
[18:24:35.925225] Epoch: [2] Total time: 0:02:07 (0.1636 s / it)
[18:24:35.925686] Averaged stats: lr: 0.000150  training_loss: 0.8520 (0.8357)  classification_loss: 0.2237 (0.2237)  loss_mask: 0.6282 (0.6120)
[18:24:36.532611] Test:  [  0/157]  eta: 0:01:34  testing_loss: 2.0688 (2.0688)  acc1: 31.2500 (31.2500)  acc5: 81.2500 (81.2500)  time: 0.6026  data: 0.5671  max mem: 4132
[18:24:36.818454] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 2.1282 (2.1220)  acc1: 23.4375 (22.4432)  acc5: 75.0000 (75.4261)  time: 0.0805  data: 0.0517  max mem: 4132
[18:24:37.100015] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 2.1293 (2.1222)  acc1: 21.8750 (22.4702)  acc5: 71.8750 (74.4792)  time: 0.0282  data: 0.0002  max mem: 4132
[18:24:37.380666] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 2.1095 (2.1142)  acc1: 23.4375 (23.6895)  acc5: 71.8750 (74.4960)  time: 0.0280  data: 0.0002  max mem: 4132
[18:24:37.661918] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 2.0977 (2.1148)  acc1: 23.4375 (23.3232)  acc5: 75.0000 (74.2378)  time: 0.0280  data: 0.0002  max mem: 4132
[18:24:37.944381] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 2.1225 (2.1160)  acc1: 21.8750 (23.2537)  acc5: 73.4375 (74.3260)  time: 0.0281  data: 0.0002  max mem: 4132
[18:24:38.226241] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 2.1115 (2.1152)  acc1: 23.4375 (23.5400)  acc5: 75.0000 (74.5902)  time: 0.0281  data: 0.0002  max mem: 4132
[18:24:38.507187] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 2.1066 (2.1138)  acc1: 23.4375 (23.5915)  acc5: 75.0000 (74.6699)  time: 0.0280  data: 0.0002  max mem: 4132
[18:24:38.792106] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 2.1013 (2.1146)  acc1: 21.8750 (23.3989)  acc5: 75.0000 (74.5949)  time: 0.0282  data: 0.0002  max mem: 4132
[18:24:39.075429] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 2.1315 (2.1174)  acc1: 20.3125 (23.0426)  acc5: 73.4375 (74.4677)  time: 0.0283  data: 0.0002  max mem: 4132
[18:24:39.358564] Test:  [100/157]  eta: 0:00:01  testing_loss: 2.1288 (2.1170)  acc1: 21.8750 (23.0198)  acc5: 75.0000 (74.4740)  time: 0.0282  data: 0.0002  max mem: 4132
[18:24:39.641236] Test:  [110/157]  eta: 0:00:01  testing_loss: 2.1184 (2.1180)  acc1: 21.8750 (22.9307)  acc5: 73.4375 (74.4369)  time: 0.0282  data: 0.0002  max mem: 4132
[18:24:39.928054] Test:  [120/157]  eta: 0:00:01  testing_loss: 2.1094 (2.1164)  acc1: 23.4375 (23.1792)  acc5: 75.0000 (74.4447)  time: 0.0284  data: 0.0002  max mem: 4132
[18:24:40.210119] Test:  [130/157]  eta: 0:00:00  testing_loss: 2.1157 (2.1171)  acc1: 23.4375 (23.0916)  acc5: 76.5625 (74.5348)  time: 0.0283  data: 0.0002  max mem: 4132
[18:24:40.490844] Test:  [140/157]  eta: 0:00:00  testing_loss: 2.1147 (2.1157)  acc1: 23.4375 (23.1937)  acc5: 78.1250 (74.7230)  time: 0.0280  data: 0.0002  max mem: 4132
[18:24:40.771371] Test:  [150/157]  eta: 0:00:00  testing_loss: 2.1009 (2.1154)  acc1: 23.4375 (23.1581)  acc5: 76.5625 (74.7413)  time: 0.0279  data: 0.0001  max mem: 4132
[18:24:40.927696] Test:  [156/157]  eta: 0:00:00  testing_loss: 2.0997 (2.1150)  acc1: 21.8750 (23.1200)  acc5: 76.5625 (74.8600)  time: 0.0273  data: 0.0001  max mem: 4132
[18:24:41.079064] Test: Total time: 0:00:05 (0.0328 s / it)
[18:24:41.079711] * Acc@1 23.120 Acc@5 74.860 loss 2.115
[18:24:41.080061] Accuracy of the network on the 10000 test images: 23.1%
[18:24:41.080293] Max accuracy: 25.08%
[18:24:41.302162] log_dir: ./output_dir
[18:24:42.102904] Epoch: [3]  [  0/781]  eta: 0:10:23  lr: 0.000150  training_loss: 0.8486 (0.8486)  classification_loss: 0.2196 (0.2196)  loss_mask: 0.6290 (0.6290)  time: 0.7988  data: 0.5934  max mem: 4132
[18:24:45.346429] Epoch: [3]  [ 20/781]  eta: 0:02:26  lr: 0.000151  training_loss: 0.8201 (0.8235)  classification_loss: 0.2219 (0.2227)  loss_mask: 0.6002 (0.6008)  time: 0.1621  data: 0.0002  max mem: 4132
[18:24:48.580649] Epoch: [3]  [ 40/781]  eta: 0:02:11  lr: 0.000153  training_loss: 0.8373 (0.8296)  classification_loss: 0.2242 (0.2233)  loss_mask: 0.6240 (0.6063)  time: 0.1616  data: 0.0003  max mem: 4132
[18:24:51.821610] Epoch: [3]  [ 60/781]  eta: 0:02:04  lr: 0.000154  training_loss: 0.7684 (0.8139)  classification_loss: 0.2228 (0.2232)  loss_mask: 0.5440 (0.5908)  time: 0.1620  data: 0.0002  max mem: 4132
[18:24:55.077842] Epoch: [3]  [ 80/781]  eta: 0:01:59  lr: 0.000155  training_loss: 0.8337 (0.8207)  classification_loss: 0.2244 (0.2234)  loss_mask: 0.6081 (0.5973)  time: 0.1626  data: 0.0002  max mem: 4132
[18:24:58.326937] Epoch: [3]  [100/781]  eta: 0:01:54  lr: 0.000156  training_loss: 0.8305 (0.8211)  classification_loss: 0.2247 (0.2236)  loss_mask: 0.6058 (0.5976)  time: 0.1624  data: 0.0002  max mem: 4132
[18:25:01.563384] Epoch: [3]  [120/781]  eta: 0:01:50  lr: 0.000158  training_loss: 0.8241 (0.8256)  classification_loss: 0.2232 (0.2235)  loss_mask: 0.6009 (0.6021)  time: 0.1617  data: 0.0002  max mem: 4132
[18:25:04.801221] Epoch: [3]  [140/781]  eta: 0:01:46  lr: 0.000159  training_loss: 0.7466 (0.8164)  classification_loss: 0.2233 (0.2234)  loss_mask: 0.5188 (0.5930)  time: 0.1618  data: 0.0003  max mem: 4132
[18:25:08.104594] Epoch: [3]  [160/781]  eta: 0:01:43  lr: 0.000160  training_loss: 0.7234 (0.8065)  classification_loss: 0.2232 (0.2234)  loss_mask: 0.5027 (0.5831)  time: 0.1651  data: 0.0002  max mem: 4132
[18:25:11.491423] Epoch: [3]  [180/781]  eta: 0:01:40  lr: 0.000162  training_loss: 0.7832 (0.8055)  classification_loss: 0.2238 (0.2235)  loss_mask: 0.5643 (0.5820)  time: 0.1692  data: 0.0002  max mem: 4132
[18:25:14.772896] Epoch: [3]  [200/781]  eta: 0:01:36  lr: 0.000163  training_loss: 0.7619 (0.8044)  classification_loss: 0.2230 (0.2234)  loss_mask: 0.5405 (0.5810)  time: 0.1640  data: 0.0003  max mem: 4132
[18:25:18.031255] Epoch: [3]  [220/781]  eta: 0:01:33  lr: 0.000164  training_loss: 0.7769 (0.8043)  classification_loss: 0.2224 (0.2234)  loss_mask: 0.5575 (0.5809)  time: 0.1628  data: 0.0002  max mem: 4132
[18:25:21.307398] Epoch: [3]  [240/781]  eta: 0:01:29  lr: 0.000165  training_loss: 0.7503 (0.8010)  classification_loss: 0.2247 (0.2235)  loss_mask: 0.5252 (0.5775)  time: 0.1637  data: 0.0002  max mem: 4132
[18:25:24.552863] Epoch: [3]  [260/781]  eta: 0:01:26  lr: 0.000167  training_loss: 0.7546 (0.7976)  classification_loss: 0.2238 (0.2235)  loss_mask: 0.5252 (0.5741)  time: 0.1622  data: 0.0002  max mem: 4132
[18:25:27.784502] Epoch: [3]  [280/781]  eta: 0:01:22  lr: 0.000168  training_loss: 0.8339 (0.8035)  classification_loss: 0.2260 (0.2237)  loss_mask: 0.6119 (0.5798)  time: 0.1615  data: 0.0002  max mem: 4132
[18:25:31.037937] Epoch: [3]  [300/781]  eta: 0:01:19  lr: 0.000169  training_loss: 0.8294 (0.8057)  classification_loss: 0.2234 (0.2237)  loss_mask: 0.6050 (0.5821)  time: 0.1626  data: 0.0002  max mem: 4132
[18:25:34.310502] Epoch: [3]  [320/781]  eta: 0:01:16  lr: 0.000170  training_loss: 0.7882 (0.8060)  classification_loss: 0.2212 (0.2236)  loss_mask: 0.5598 (0.5824)  time: 0.1635  data: 0.0002  max mem: 4132
[18:25:37.561955] Epoch: [3]  [340/781]  eta: 0:01:12  lr: 0.000172  training_loss: 0.7836 (0.8051)  classification_loss: 0.2235 (0.2236)  loss_mask: 0.5623 (0.5815)  time: 0.1625  data: 0.0003  max mem: 4132
[18:25:40.796929] Epoch: [3]  [360/781]  eta: 0:01:09  lr: 0.000173  training_loss: 0.7332 (0.8035)  classification_loss: 0.2234 (0.2236)  loss_mask: 0.5082 (0.5798)  time: 0.1617  data: 0.0002  max mem: 4132
[18:25:44.045589] Epoch: [3]  [380/781]  eta: 0:01:05  lr: 0.000174  training_loss: 0.7877 (0.8045)  classification_loss: 0.2239 (0.2237)  loss_mask: 0.5675 (0.5808)  time: 0.1624  data: 0.0002  max mem: 4132
[18:25:47.298437] Epoch: [3]  [400/781]  eta: 0:01:02  lr: 0.000176  training_loss: 0.7655 (0.8034)  classification_loss: 0.2248 (0.2237)  loss_mask: 0.5372 (0.5797)  time: 0.1625  data: 0.0002  max mem: 4132
[18:25:50.521242] Epoch: [3]  [420/781]  eta: 0:00:59  lr: 0.000177  training_loss: 0.7979 (0.8034)  classification_loss: 0.2247 (0.2238)  loss_mask: 0.5734 (0.5796)  time: 0.1611  data: 0.0002  max mem: 4132
[18:25:53.739424] Epoch: [3]  [440/781]  eta: 0:00:55  lr: 0.000178  training_loss: 0.8201 (0.8035)  classification_loss: 0.2241 (0.2238)  loss_mask: 0.5937 (0.5798)  time: 0.1608  data: 0.0002  max mem: 4132
[18:25:56.968305] Epoch: [3]  [460/781]  eta: 0:00:52  lr: 0.000179  training_loss: 0.7484 (0.8011)  classification_loss: 0.2248 (0.2238)  loss_mask: 0.5267 (0.5773)  time: 0.1614  data: 0.0002  max mem: 4132
[18:26:00.192305] Epoch: [3]  [480/781]  eta: 0:00:49  lr: 0.000181  training_loss: 0.7601 (0.8005)  classification_loss: 0.2246 (0.2238)  loss_mask: 0.5317 (0.5767)  time: 0.1611  data: 0.0002  max mem: 4132
[18:26:03.412469] Epoch: [3]  [500/781]  eta: 0:00:46  lr: 0.000182  training_loss: 0.7574 (0.7991)  classification_loss: 0.2234 (0.2238)  loss_mask: 0.5296 (0.5753)  time: 0.1609  data: 0.0002  max mem: 4132
[18:26:06.629618] Epoch: [3]  [520/781]  eta: 0:00:42  lr: 0.000183  training_loss: 0.7331 (0.7972)  classification_loss: 0.2235 (0.2238)  loss_mask: 0.5120 (0.5734)  time: 0.1608  data: 0.0002  max mem: 4132
[18:26:09.857641] Epoch: [3]  [540/781]  eta: 0:00:39  lr: 0.000185  training_loss: 0.7396 (0.7953)  classification_loss: 0.2232 (0.2238)  loss_mask: 0.5173 (0.5715)  time: 0.1613  data: 0.0003  max mem: 4132
[18:26:13.098789] Epoch: [3]  [560/781]  eta: 0:00:36  lr: 0.000186  training_loss: 0.7195 (0.7931)  classification_loss: 0.2212 (0.2238)  loss_mask: 0.4945 (0.5694)  time: 0.1620  data: 0.0003  max mem: 4132
[18:26:16.355067] Epoch: [3]  [580/781]  eta: 0:00:32  lr: 0.000187  training_loss: 0.7340 (0.7913)  classification_loss: 0.2242 (0.2238)  loss_mask: 0.5174 (0.5675)  time: 0.1627  data: 0.0002  max mem: 4132
[18:26:19.601598] Epoch: [3]  [600/781]  eta: 0:00:29  lr: 0.000188  training_loss: 0.7413 (0.7902)  classification_loss: 0.2232 (0.2238)  loss_mask: 0.5124 (0.5664)  time: 0.1622  data: 0.0003  max mem: 4132
[18:26:22.849186] Epoch: [3]  [620/781]  eta: 0:00:26  lr: 0.000190  training_loss: 0.6877 (0.7874)  classification_loss: 0.2235 (0.2238)  loss_mask: 0.4635 (0.5636)  time: 0.1623  data: 0.0004  max mem: 4132
[18:26:26.102419] Epoch: [3]  [640/781]  eta: 0:00:23  lr: 0.000191  training_loss: 0.7672 (0.7871)  classification_loss: 0.2245 (0.2238)  loss_mask: 0.5435 (0.5633)  time: 0.1625  data: 0.0002  max mem: 4132
[18:26:29.366133] Epoch: [3]  [660/781]  eta: 0:00:19  lr: 0.000192  training_loss: 0.7452 (0.7856)  classification_loss: 0.2243 (0.2238)  loss_mask: 0.5233 (0.5618)  time: 0.1631  data: 0.0002  max mem: 4132
[18:26:32.592373] Epoch: [3]  [680/781]  eta: 0:00:16  lr: 0.000194  training_loss: 0.7383 (0.7853)  classification_loss: 0.2230 (0.2238)  loss_mask: 0.5152 (0.5615)  time: 0.1612  data: 0.0002  max mem: 4132
[18:26:35.819832] Epoch: [3]  [700/781]  eta: 0:00:13  lr: 0.000195  training_loss: 0.8472 (0.7876)  classification_loss: 0.2235 (0.2238)  loss_mask: 0.6270 (0.5638)  time: 0.1613  data: 0.0002  max mem: 4132
[18:26:39.059868] Epoch: [3]  [720/781]  eta: 0:00:09  lr: 0.000196  training_loss: 0.7950 (0.7880)  classification_loss: 0.2245 (0.2238)  loss_mask: 0.5699 (0.5642)  time: 0.1619  data: 0.0002  max mem: 4132
[18:26:42.305832] Epoch: [3]  [740/781]  eta: 0:00:06  lr: 0.000197  training_loss: 0.7424 (0.7868)  classification_loss: 0.2246 (0.2238)  loss_mask: 0.5223 (0.5630)  time: 0.1622  data: 0.0003  max mem: 4132
[18:26:45.547006] Epoch: [3]  [760/781]  eta: 0:00:03  lr: 0.000199  training_loss: 0.7166 (0.7850)  classification_loss: 0.2247 (0.2238)  loss_mask: 0.4923 (0.5612)  time: 0.1620  data: 0.0002  max mem: 4132
[18:26:48.762710] Epoch: [3]  [780/781]  eta: 0:00:00  lr: 0.000200  training_loss: 0.7193 (0.7836)  classification_loss: 0.2238 (0.2238)  loss_mask: 0.4913 (0.5598)  time: 0.1607  data: 0.0002  max mem: 4132
[18:26:48.906613] Epoch: [3] Total time: 0:02:07 (0.1634 s / it)
[18:26:48.907074] Averaged stats: lr: 0.000200  training_loss: 0.7193 (0.7836)  classification_loss: 0.2238 (0.2238)  loss_mask: 0.4913 (0.5598)
[18:26:49.542573] Test:  [  0/157]  eta: 0:01:38  testing_loss: 2.0707 (2.0707)  acc1: 23.4375 (23.4375)  acc5: 81.2500 (81.2500)  time: 0.6288  data: 0.5983  max mem: 4132
[18:26:49.836593] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 2.1373 (2.1200)  acc1: 23.4375 (22.8693)  acc5: 76.5625 (76.9886)  time: 0.0837  data: 0.0549  max mem: 4132
[18:26:50.122349] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 2.1373 (2.1211)  acc1: 23.4375 (22.5446)  acc5: 75.0000 (75.5952)  time: 0.0288  data: 0.0004  max mem: 4132
[18:26:50.414811] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 2.1144 (2.1152)  acc1: 23.4375 (23.5887)  acc5: 75.0000 (75.8569)  time: 0.0287  data: 0.0004  max mem: 4132
[18:26:50.704846] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 2.1123 (2.1160)  acc1: 25.0000 (23.8948)  acc5: 76.5625 (75.6479)  time: 0.0289  data: 0.0004  max mem: 4132
[18:26:50.993283] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 2.1245 (2.1177)  acc1: 21.8750 (23.4069)  acc5: 76.5625 (75.6740)  time: 0.0287  data: 0.0002  max mem: 4132
[18:26:51.280331] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 2.1169 (2.1174)  acc1: 21.8750 (23.6680)  acc5: 76.5625 (75.9221)  time: 0.0286  data: 0.0002  max mem: 4132
[18:26:51.566420] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 2.1092 (2.1160)  acc1: 23.4375 (23.5915)  acc5: 78.1250 (76.3424)  time: 0.0284  data: 0.0002  max mem: 4132
[18:26:51.856673] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 2.1092 (2.1169)  acc1: 21.8750 (23.3025)  acc5: 78.1250 (76.3503)  time: 0.0286  data: 0.0002  max mem: 4132
[18:26:52.142785] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 2.1386 (2.1195)  acc1: 20.3125 (23.1628)  acc5: 76.5625 (76.2191)  time: 0.0287  data: 0.0002  max mem: 4132
[18:26:52.433015] Test:  [100/157]  eta: 0:00:01  testing_loss: 2.1332 (2.1198)  acc1: 21.8750 (23.0353)  acc5: 76.5625 (76.2067)  time: 0.0287  data: 0.0002  max mem: 4132
[18:26:52.729099] Test:  [110/157]  eta: 0:00:01  testing_loss: 2.1175 (2.1205)  acc1: 21.8750 (23.0715)  acc5: 76.5625 (76.1824)  time: 0.0291  data: 0.0002  max mem: 4132
[18:26:53.012410] Test:  [120/157]  eta: 0:00:01  testing_loss: 2.1083 (2.1191)  acc1: 21.8750 (23.0243)  acc5: 76.5625 (76.1493)  time: 0.0288  data: 0.0002  max mem: 4132
[18:26:53.300510] Test:  [130/157]  eta: 0:00:00  testing_loss: 2.1150 (2.1194)  acc1: 23.4375 (23.1393)  acc5: 76.5625 (76.1927)  time: 0.0285  data: 0.0002  max mem: 4132
[18:26:53.589011] Test:  [140/157]  eta: 0:00:00  testing_loss: 2.1150 (2.1182)  acc1: 23.4375 (23.2380)  acc5: 78.1250 (76.2855)  time: 0.0287  data: 0.0002  max mem: 4132
[18:26:53.869079] Test:  [150/157]  eta: 0:00:00  testing_loss: 2.1048 (2.1179)  acc1: 21.8750 (23.1995)  acc5: 76.5625 (76.2521)  time: 0.0283  data: 0.0001  max mem: 4132
[18:26:54.020027] Test:  [156/157]  eta: 0:00:00  testing_loss: 2.1049 (2.1174)  acc1: 21.8750 (23.0800)  acc5: 76.5625 (76.3400)  time: 0.0270  data: 0.0001  max mem: 4132
[18:26:54.169997] Test: Total time: 0:00:05 (0.0335 s / it)
[18:26:54.170430] * Acc@1 23.080 Acc@5 76.340 loss 2.117
[18:26:54.170712] Accuracy of the network on the 10000 test images: 23.1%
[18:26:54.170881] Max accuracy: 25.08%
[18:26:54.397086] log_dir: ./output_dir
[18:26:55.212531] Epoch: [4]  [  0/781]  eta: 0:10:35  lr: 0.000200  training_loss: 0.6931 (0.6931)  classification_loss: 0.2227 (0.2227)  loss_mask: 0.4704 (0.4704)  time: 0.8136  data: 0.6322  max mem: 4132
[18:26:58.449577] Epoch: [4]  [ 20/781]  eta: 0:02:26  lr: 0.000201  training_loss: 0.7381 (0.7200)  classification_loss: 0.2242 (0.2243)  loss_mask: 0.5141 (0.4957)  time: 0.1618  data: 0.0002  max mem: 4132
[18:27:01.697874] Epoch: [4]  [ 40/781]  eta: 0:02:11  lr: 0.000203  training_loss: 0.7332 (0.7299)  classification_loss: 0.2234 (0.2239)  loss_mask: 0.5132 (0.5060)  time: 0.1623  data: 0.0004  max mem: 4132
[18:27:04.985371] Epoch: [4]  [ 60/781]  eta: 0:02:05  lr: 0.000204  training_loss: 0.7376 (0.7375)  classification_loss: 0.2236 (0.2240)  loss_mask: 0.5121 (0.5135)  time: 0.1643  data: 0.0002  max mem: 4132
[18:27:08.249884] Epoch: [4]  [ 80/781]  eta: 0:01:59  lr: 0.000205  training_loss: 0.6619 (0.7210)  classification_loss: 0.2267 (0.2243)  loss_mask: 0.4353 (0.4967)  time: 0.1632  data: 0.0003  max mem: 4132
[18:27:11.475097] Epoch: [4]  [100/781]  eta: 0:01:55  lr: 0.000206  training_loss: 0.7048 (0.7224)  classification_loss: 0.2248 (0.2242)  loss_mask: 0.4887 (0.4982)  time: 0.1612  data: 0.0002  max mem: 4132
[18:27:14.689625] Epoch: [4]  [120/781]  eta: 0:01:50  lr: 0.000208  training_loss: 0.8620 (0.7459)  classification_loss: 0.2236 (0.2242)  loss_mask: 0.6383 (0.5218)  time: 0.1606  data: 0.0002  max mem: 4132
[18:27:17.928968] Epoch: [4]  [140/781]  eta: 0:01:46  lr: 0.000209  training_loss: 0.7443 (0.7457)  classification_loss: 0.2230 (0.2240)  loss_mask: 0.5199 (0.5217)  time: 0.1619  data: 0.0003  max mem: 4132
[18:27:21.187884] Epoch: [4]  [160/781]  eta: 0:01:43  lr: 0.000210  training_loss: 0.7782 (0.7494)  classification_loss: 0.2215 (0.2238)  loss_mask: 0.5549 (0.5256)  time: 0.1629  data: 0.0003  max mem: 4132
[18:27:24.422153] Epoch: [4]  [180/781]  eta: 0:01:39  lr: 0.000212  training_loss: 0.8268 (0.7588)  classification_loss: 0.2252 (0.2239)  loss_mask: 0.6012 (0.5348)  time: 0.1616  data: 0.0003  max mem: 4132
[18:27:27.719330] Epoch: [4]  [200/781]  eta: 0:01:36  lr: 0.000213  training_loss: 0.8141 (0.7642)  classification_loss: 0.2228 (0.2238)  loss_mask: 0.5952 (0.5404)  time: 0.1648  data: 0.0002  max mem: 4132
[18:27:31.012205] Epoch: [4]  [220/781]  eta: 0:01:32  lr: 0.000214  training_loss: 0.7105 (0.7588)  classification_loss: 0.2246 (0.2239)  loss_mask: 0.4893 (0.5350)  time: 0.1646  data: 0.0003  max mem: 4132
[18:27:34.287221] Epoch: [4]  [240/781]  eta: 0:01:29  lr: 0.000215  training_loss: 0.7032 (0.7542)  classification_loss: 0.2239 (0.2239)  loss_mask: 0.4675 (0.5302)  time: 0.1637  data: 0.0002  max mem: 4132
[18:27:37.567544] Epoch: [4]  [260/781]  eta: 0:01:26  lr: 0.000217  training_loss: 0.8175 (0.7603)  classification_loss: 0.2249 (0.2240)  loss_mask: 0.5900 (0.5363)  time: 0.1639  data: 0.0002  max mem: 4132
[18:27:40.875076] Epoch: [4]  [280/781]  eta: 0:01:22  lr: 0.000218  training_loss: 0.7072 (0.7590)  classification_loss: 0.2226 (0.2240)  loss_mask: 0.4875 (0.5350)  time: 0.1653  data: 0.0002  max mem: 4132
[18:27:44.101956] Epoch: [4]  [300/781]  eta: 0:01:19  lr: 0.000219  training_loss: 0.6710 (0.7545)  classification_loss: 0.2249 (0.2241)  loss_mask: 0.4469 (0.5304)  time: 0.1613  data: 0.0002  max mem: 4132
[18:27:47.360508] Epoch: [4]  [320/781]  eta: 0:01:16  lr: 0.000220  training_loss: 0.6777 (0.7491)  classification_loss: 0.2234 (0.2241)  loss_mask: 0.4503 (0.5250)  time: 0.1628  data: 0.0003  max mem: 4132
[18:27:50.642572] Epoch: [4]  [340/781]  eta: 0:01:12  lr: 0.000222  training_loss: 0.6813 (0.7460)  classification_loss: 0.2251 (0.2241)  loss_mask: 0.4602 (0.5219)  time: 0.1640  data: 0.0002  max mem: 4132
[18:27:53.884300] Epoch: [4]  [360/781]  eta: 0:01:09  lr: 0.000223  training_loss: 0.7759 (0.7481)  classification_loss: 0.2245 (0.2242)  loss_mask: 0.5510 (0.5239)  time: 0.1620  data: 0.0002  max mem: 4132
[18:27:57.154231] Epoch: [4]  [380/781]  eta: 0:01:06  lr: 0.000224  training_loss: 0.7060 (0.7474)  classification_loss: 0.2252 (0.2242)  loss_mask: 0.4855 (0.5232)  time: 0.1634  data: 0.0002  max mem: 4132
[18:28:00.405075] Epoch: [4]  [400/781]  eta: 0:01:02  lr: 0.000226  training_loss: 0.6853 (0.7453)  classification_loss: 0.2255 (0.2242)  loss_mask: 0.4648 (0.5210)  time: 0.1625  data: 0.0002  max mem: 4132
[18:28:03.656661] Epoch: [4]  [420/781]  eta: 0:00:59  lr: 0.000227  training_loss: 0.7084 (0.7447)  classification_loss: 0.2240 (0.2242)  loss_mask: 0.4845 (0.5205)  time: 0.1625  data: 0.0002  max mem: 4132
[18:28:06.893512] Epoch: [4]  [440/781]  eta: 0:00:56  lr: 0.000228  training_loss: 0.6780 (0.7434)  classification_loss: 0.2244 (0.2242)  loss_mask: 0.4526 (0.5192)  time: 0.1618  data: 0.0002  max mem: 4132
[18:28:10.123287] Epoch: [4]  [460/781]  eta: 0:00:52  lr: 0.000229  training_loss: 0.6425 (0.7398)  classification_loss: 0.2228 (0.2242)  loss_mask: 0.4210 (0.5156)  time: 0.1614  data: 0.0002  max mem: 4132
[18:28:13.354803] Epoch: [4]  [480/781]  eta: 0:00:49  lr: 0.000231  training_loss: 0.7230 (0.7395)  classification_loss: 0.2261 (0.2242)  loss_mask: 0.4931 (0.5152)  time: 0.1615  data: 0.0003  max mem: 4132
[18:28:16.670115] Epoch: [4]  [500/781]  eta: 0:00:46  lr: 0.000232  training_loss: 0.8158 (0.7431)  classification_loss: 0.2227 (0.2242)  loss_mask: 0.5940 (0.5189)  time: 0.1657  data: 0.0004  max mem: 4132
[18:28:19.947818] Epoch: [4]  [520/781]  eta: 0:00:42  lr: 0.000233  training_loss: 0.7495 (0.7431)  classification_loss: 0.2243 (0.2242)  loss_mask: 0.5219 (0.5189)  time: 0.1638  data: 0.0002  max mem: 4132
[18:28:23.205817] Epoch: [4]  [540/781]  eta: 0:00:39  lr: 0.000235  training_loss: 0.6917 (0.7414)  classification_loss: 0.2247 (0.2243)  loss_mask: 0.4679 (0.5171)  time: 0.1628  data: 0.0002  max mem: 4132
[18:28:26.442703] Epoch: [4]  [560/781]  eta: 0:00:36  lr: 0.000236  training_loss: 0.6782 (0.7393)  classification_loss: 0.2227 (0.2242)  loss_mask: 0.4533 (0.5151)  time: 0.1618  data: 0.0002  max mem: 4132
[18:28:29.677895] Epoch: [4]  [580/781]  eta: 0:00:32  lr: 0.000237  training_loss: 0.6986 (0.7382)  classification_loss: 0.2228 (0.2241)  loss_mask: 0.4737 (0.5141)  time: 0.1617  data: 0.0002  max mem: 4132
[18:28:32.915131] Epoch: [4]  [600/781]  eta: 0:00:29  lr: 0.000238  training_loss: 0.7437 (0.7388)  classification_loss: 0.2242 (0.2241)  loss_mask: 0.5207 (0.5146)  time: 0.1618  data: 0.0003  max mem: 4132
[18:28:36.163939] Epoch: [4]  [620/781]  eta: 0:00:26  lr: 0.000240  training_loss: 0.7386 (0.7393)  classification_loss: 0.2240 (0.2242)  loss_mask: 0.5141 (0.5152)  time: 0.1624  data: 0.0002  max mem: 4132
[18:28:39.417995] Epoch: [4]  [640/781]  eta: 0:00:23  lr: 0.000241  training_loss: 0.7226 (0.7388)  classification_loss: 0.2238 (0.2241)  loss_mask: 0.4958 (0.5146)  time: 0.1626  data: 0.0003  max mem: 4132
[18:28:42.703875] Epoch: [4]  [660/781]  eta: 0:00:19  lr: 0.000242  training_loss: 0.6572 (0.7369)  classification_loss: 0.2236 (0.2241)  loss_mask: 0.4334 (0.5127)  time: 0.1642  data: 0.0002  max mem: 4132
[18:28:45.943572] Epoch: [4]  [680/781]  eta: 0:00:16  lr: 0.000244  training_loss: 0.7051 (0.7355)  classification_loss: 0.2246 (0.2241)  loss_mask: 0.4827 (0.5113)  time: 0.1619  data: 0.0002  max mem: 4132
[18:28:49.181451] Epoch: [4]  [700/781]  eta: 0:00:13  lr: 0.000245  training_loss: 0.8458 (0.7390)  classification_loss: 0.2222 (0.2242)  loss_mask: 0.6263 (0.5148)  time: 0.1618  data: 0.0002  max mem: 4132
[18:28:52.409419] Epoch: [4]  [720/781]  eta: 0:00:09  lr: 0.000246  training_loss: 0.6950 (0.7379)  classification_loss: 0.2250 (0.2242)  loss_mask: 0.4698 (0.5137)  time: 0.1613  data: 0.0005  max mem: 4132
[18:28:55.666964] Epoch: [4]  [740/781]  eta: 0:00:06  lr: 0.000247  training_loss: 0.6504 (0.7359)  classification_loss: 0.2255 (0.2243)  loss_mask: 0.4249 (0.5117)  time: 0.1628  data: 0.0002  max mem: 4132
[18:28:58.910758] Epoch: [4]  [760/781]  eta: 0:00:03  lr: 0.000249  training_loss: 0.6572 (0.7341)  classification_loss: 0.2237 (0.2243)  loss_mask: 0.4332 (0.5098)  time: 0.1621  data: 0.0002  max mem: 4132
[18:29:02.178585] Epoch: [4]  [780/781]  eta: 0:00:00  lr: 0.000250  training_loss: 0.6407 (0.7319)  classification_loss: 0.2240 (0.2243)  loss_mask: 0.4177 (0.5076)  time: 0.1633  data: 0.0002  max mem: 4132
[18:29:02.333991] Epoch: [4] Total time: 0:02:07 (0.1638 s / it)
[18:29:02.334709] Averaged stats: lr: 0.000250  training_loss: 0.6407 (0.7319)  classification_loss: 0.2240 (0.2243)  loss_mask: 0.4177 (0.5076)
[18:29:02.979646] Test:  [  0/157]  eta: 0:01:40  testing_loss: 2.1056 (2.1056)  acc1: 23.4375 (23.4375)  acc5: 78.1250 (78.1250)  time: 0.6408  data: 0.6104  max mem: 4132
[18:29:03.269057] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 2.1522 (2.1403)  acc1: 20.3125 (19.6023)  acc5: 75.0000 (75.5682)  time: 0.0844  data: 0.0556  max mem: 4132
[18:29:03.552186] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 2.1528 (2.1401)  acc1: 18.7500 (19.6429)  acc5: 73.4375 (74.7024)  time: 0.0284  data: 0.0001  max mem: 4132
[18:29:03.835394] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 2.1318 (2.1354)  acc1: 20.3125 (20.3125)  acc5: 73.4375 (74.7984)  time: 0.0281  data: 0.0002  max mem: 4132
[18:29:04.124250] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 2.1313 (2.1347)  acc1: 20.3125 (20.7698)  acc5: 73.4375 (74.2759)  time: 0.0284  data: 0.0002  max mem: 4132
[18:29:04.408274] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 2.1373 (2.1374)  acc1: 18.7500 (20.1287)  acc5: 73.4375 (74.2341)  time: 0.0285  data: 0.0002  max mem: 4132
[18:29:04.694054] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 2.1425 (2.1371)  acc1: 17.1875 (20.0564)  acc5: 75.0000 (74.3084)  time: 0.0284  data: 0.0002  max mem: 4132
[18:29:04.976127] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 2.1292 (2.1358)  acc1: 18.7500 (19.5863)  acc5: 75.0000 (74.6259)  time: 0.0283  data: 0.0002  max mem: 4132
[18:29:05.262326] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 2.1265 (2.1377)  acc1: 17.1875 (19.2323)  acc5: 76.5625 (74.4792)  time: 0.0283  data: 0.0001  max mem: 4132
[18:29:05.545360] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 2.1621 (2.1402)  acc1: 15.6250 (18.8359)  acc5: 75.0000 (74.4677)  time: 0.0283  data: 0.0002  max mem: 4132
[18:29:05.833651] Test:  [100/157]  eta: 0:00:01  testing_loss: 2.1401 (2.1405)  acc1: 17.1875 (18.9202)  acc5: 75.0000 (74.4740)  time: 0.0284  data: 0.0002  max mem: 4132
[18:29:06.118909] Test:  [110/157]  eta: 0:00:01  testing_loss: 2.1379 (2.1405)  acc1: 18.7500 (19.0034)  acc5: 75.0000 (74.6199)  time: 0.0285  data: 0.0002  max mem: 4132
[18:29:06.403163] Test:  [120/157]  eta: 0:00:01  testing_loss: 2.1317 (2.1395)  acc1: 18.7500 (19.0341)  acc5: 75.0000 (74.7546)  time: 0.0283  data: 0.0002  max mem: 4132
[18:29:06.688478] Test:  [130/157]  eta: 0:00:00  testing_loss: 2.1467 (2.1402)  acc1: 18.7500 (19.0959)  acc5: 75.0000 (74.7137)  time: 0.0284  data: 0.0002  max mem: 4132
[18:29:06.970261] Test:  [140/157]  eta: 0:00:00  testing_loss: 2.1398 (2.1389)  acc1: 20.3125 (19.2376)  acc5: 75.0000 (74.7673)  time: 0.0282  data: 0.0002  max mem: 4132
[18:29:07.249464] Test:  [150/157]  eta: 0:00:00  testing_loss: 2.1304 (2.1389)  acc1: 20.3125 (19.3088)  acc5: 75.0000 (74.8137)  time: 0.0279  data: 0.0001  max mem: 4132
[18:29:07.399821] Test:  [156/157]  eta: 0:00:00  testing_loss: 2.1297 (2.1383)  acc1: 21.8750 (19.3100)  acc5: 76.5625 (74.9900)  time: 0.0270  data: 0.0001  max mem: 4132
[18:29:07.538575] Test: Total time: 0:00:05 (0.0331 s / it)
[18:29:07.539030] * Acc@1 19.310 Acc@5 74.990 loss 2.138
[18:29:07.539332] Accuracy of the network on the 10000 test images: 19.3%
[18:29:07.539565] Max accuracy: 25.08%
[18:29:07.990333] log_dir: ./output_dir
[18:29:08.846899] Epoch: [5]  [  0/781]  eta: 0:11:06  lr: 0.000250  training_loss: 0.5910 (0.5910)  classification_loss: 0.2251 (0.2251)  loss_mask: 0.3659 (0.3659)  time: 0.8537  data: 0.6711  max mem: 4132
[18:29:12.107100] Epoch: [5]  [ 20/781]  eta: 0:02:29  lr: 0.000250  training_loss: 0.7067 (0.7126)  classification_loss: 0.2248 (0.2244)  loss_mask: 0.4798 (0.4882)  time: 0.1629  data: 0.0002  max mem: 4132
[18:29:15.382337] Epoch: [5]  [ 40/781]  eta: 0:02:13  lr: 0.000250  training_loss: 0.8401 (0.7893)  classification_loss: 0.2252 (0.2247)  loss_mask: 0.6162 (0.5646)  time: 0.1637  data: 0.0002  max mem: 4132
[18:29:18.632298] Epoch: [5]  [ 60/781]  eta: 0:02:05  lr: 0.000250  training_loss: 0.6834 (0.7621)  classification_loss: 0.2239 (0.2245)  loss_mask: 0.4602 (0.5376)  time: 0.1624  data: 0.0002  max mem: 4132
[18:29:21.879249] Epoch: [5]  [ 80/781]  eta: 0:02:00  lr: 0.000250  training_loss: 0.6416 (0.7366)  classification_loss: 0.2241 (0.2246)  loss_mask: 0.4175 (0.5120)  time: 0.1623  data: 0.0003  max mem: 4132
[18:29:25.126893] Epoch: [5]  [100/781]  eta: 0:01:55  lr: 0.000250  training_loss: 0.6950 (0.7263)  classification_loss: 0.2228 (0.2243)  loss_mask: 0.4653 (0.5021)  time: 0.1623  data: 0.0003  max mem: 4132
[18:29:28.368946] Epoch: [5]  [120/781]  eta: 0:01:51  lr: 0.000250  training_loss: 0.6126 (0.7126)  classification_loss: 0.2233 (0.2241)  loss_mask: 0.3866 (0.4885)  time: 0.1620  data: 0.0002  max mem: 4132
[18:29:31.614860] Epoch: [5]  [140/781]  eta: 0:01:47  lr: 0.000250  training_loss: 0.6025 (0.6976)  classification_loss: 0.2224 (0.2240)  loss_mask: 0.3797 (0.4736)  time: 0.1622  data: 0.0002  max mem: 4132
[18:29:34.862706] Epoch: [5]  [160/781]  eta: 0:01:43  lr: 0.000250  training_loss: 0.6149 (0.6874)  classification_loss: 0.2230 (0.2240)  loss_mask: 0.3939 (0.4635)  time: 0.1623  data: 0.0003  max mem: 4132
[18:29:38.140148] Epoch: [5]  [180/781]  eta: 0:01:40  lr: 0.000250  training_loss: 0.7344 (0.6988)  classification_loss: 0.2255 (0.2242)  loss_mask: 0.5118 (0.4747)  time: 0.1638  data: 0.0002  max mem: 4132
[18:29:41.401433] Epoch: [5]  [200/781]  eta: 0:01:36  lr: 0.000250  training_loss: 0.7583 (0.7077)  classification_loss: 0.2247 (0.2242)  loss_mask: 0.5335 (0.4836)  time: 0.1630  data: 0.0002  max mem: 4132
[18:29:44.705623] Epoch: [5]  [220/781]  eta: 0:01:33  lr: 0.000250  training_loss: 0.6311 (0.7026)  classification_loss: 0.2252 (0.2243)  loss_mask: 0.4052 (0.4783)  time: 0.1651  data: 0.0002  max mem: 4132
[18:29:47.949378] Epoch: [5]  [240/781]  eta: 0:01:29  lr: 0.000250  training_loss: 0.6378 (0.6999)  classification_loss: 0.2238 (0.2244)  loss_mask: 0.4119 (0.4755)  time: 0.1621  data: 0.0002  max mem: 4132
[18:29:51.202536] Epoch: [5]  [260/781]  eta: 0:01:26  lr: 0.000250  training_loss: 0.5933 (0.6951)  classification_loss: 0.2233 (0.2244)  loss_mask: 0.3693 (0.4708)  time: 0.1626  data: 0.0002  max mem: 4132
[18:29:54.464324] Epoch: [5]  [280/781]  eta: 0:01:22  lr: 0.000250  training_loss: 0.6271 (0.6920)  classification_loss: 0.2247 (0.2244)  loss_mask: 0.4018 (0.4676)  time: 0.1630  data: 0.0002  max mem: 4132
[18:29:57.682446] Epoch: [5]  [300/781]  eta: 0:01:19  lr: 0.000250  training_loss: 0.6568 (0.6906)  classification_loss: 0.2244 (0.2244)  loss_mask: 0.4356 (0.4662)  time: 0.1608  data: 0.0003  max mem: 4132
[18:30:00.927780] Epoch: [5]  [320/781]  eta: 0:01:15  lr: 0.000250  training_loss: 0.6201 (0.6866)  classification_loss: 0.2258 (0.2245)  loss_mask: 0.3999 (0.4621)  time: 0.1622  data: 0.0003  max mem: 4132
[18:30:04.169454] Epoch: [5]  [340/781]  eta: 0:01:12  lr: 0.000250  training_loss: 0.5722 (0.6821)  classification_loss: 0.2259 (0.2245)  loss_mask: 0.3417 (0.4575)  time: 0.1620  data: 0.0003  max mem: 4132
[18:30:07.448411] Epoch: [5]  [360/781]  eta: 0:01:09  lr: 0.000250  training_loss: 0.6501 (0.6813)  classification_loss: 0.2247 (0.2245)  loss_mask: 0.4249 (0.4567)  time: 0.1638  data: 0.0003  max mem: 4132
[18:30:10.680567] Epoch: [5]  [380/781]  eta: 0:01:05  lr: 0.000250  training_loss: 0.7057 (0.6852)  classification_loss: 0.2253 (0.2246)  loss_mask: 0.4821 (0.4606)  time: 0.1614  data: 0.0002  max mem: 4132
[18:30:13.942500] Epoch: [5]  [400/781]  eta: 0:01:02  lr: 0.000250  training_loss: 0.6877 (0.6868)  classification_loss: 0.2237 (0.2246)  loss_mask: 0.4675 (0.4622)  time: 0.1630  data: 0.0003  max mem: 4132
[18:30:17.195129] Epoch: [5]  [420/781]  eta: 0:00:59  lr: 0.000250  training_loss: 0.6507 (0.6856)  classification_loss: 0.2244 (0.2246)  loss_mask: 0.4249 (0.4610)  time: 0.1626  data: 0.0002  max mem: 4132
[18:30:20.425034] Epoch: [5]  [440/781]  eta: 0:00:55  lr: 0.000250  training_loss: 0.6362 (0.6836)  classification_loss: 0.2244 (0.2246)  loss_mask: 0.4128 (0.4590)  time: 0.1614  data: 0.0002  max mem: 4132
[18:30:23.653927] Epoch: [5]  [460/781]  eta: 0:00:52  lr: 0.000250  training_loss: 0.6007 (0.6795)  classification_loss: 0.2217 (0.2245)  loss_mask: 0.3793 (0.4550)  time: 0.1614  data: 0.0002  max mem: 4132
[18:30:26.898250] Epoch: [5]  [480/781]  eta: 0:00:49  lr: 0.000250  training_loss: 0.5593 (0.6755)  classification_loss: 0.2257 (0.2246)  loss_mask: 0.3295 (0.4510)  time: 0.1621  data: 0.0002  max mem: 4132
[18:30:30.152770] Epoch: [5]  [500/781]  eta: 0:00:46  lr: 0.000250  training_loss: 0.6703 (0.6767)  classification_loss: 0.2255 (0.2245)  loss_mask: 0.4451 (0.4521)  time: 0.1626  data: 0.0002  max mem: 4132
[18:30:33.393134] Epoch: [5]  [520/781]  eta: 0:00:42  lr: 0.000250  training_loss: 0.6465 (0.6764)  classification_loss: 0.2257 (0.2246)  loss_mask: 0.4184 (0.4518)  time: 0.1619  data: 0.0002  max mem: 4132
[18:30:36.611653] Epoch: [5]  [540/781]  eta: 0:00:39  lr: 0.000250  training_loss: 0.5767 (0.6734)  classification_loss: 0.2247 (0.2246)  loss_mask: 0.3522 (0.4488)  time: 0.1609  data: 0.0003  max mem: 4132
[18:30:39.838457] Epoch: [5]  [560/781]  eta: 0:00:36  lr: 0.000250  training_loss: 0.5776 (0.6716)  classification_loss: 0.2234 (0.2246)  loss_mask: 0.3541 (0.4470)  time: 0.1612  data: 0.0002  max mem: 4132
[18:30:43.134521] Epoch: [5]  [580/781]  eta: 0:00:32  lr: 0.000250  training_loss: 0.6176 (0.6713)  classification_loss: 0.2243 (0.2246)  loss_mask: 0.3926 (0.4467)  time: 0.1647  data: 0.0002  max mem: 4132
[18:30:46.460352] Epoch: [5]  [600/781]  eta: 0:00:29  lr: 0.000250  training_loss: 0.5711 (0.6693)  classification_loss: 0.2251 (0.2246)  loss_mask: 0.3445 (0.4447)  time: 0.1662  data: 0.0002  max mem: 4132
[18:30:49.700246] Epoch: [5]  [620/781]  eta: 0:00:26  lr: 0.000250  training_loss: 0.5059 (0.6645)  classification_loss: 0.2244 (0.2246)  loss_mask: 0.2837 (0.4399)  time: 0.1619  data: 0.0002  max mem: 4132
[18:30:52.936169] Epoch: [5]  [640/781]  eta: 0:00:23  lr: 0.000250  training_loss: 0.5964 (0.6636)  classification_loss: 0.2247 (0.2246)  loss_mask: 0.3714 (0.4390)  time: 0.1617  data: 0.0002  max mem: 4132
[18:30:56.181807] Epoch: [5]  [660/781]  eta: 0:00:19  lr: 0.000250  training_loss: 0.7232 (0.6660)  classification_loss: 0.2242 (0.2246)  loss_mask: 0.5016 (0.4414)  time: 0.1622  data: 0.0002  max mem: 4132
[18:30:59.415875] Epoch: [5]  [680/781]  eta: 0:00:16  lr: 0.000250  training_loss: 0.5987 (0.6650)  classification_loss: 0.2240 (0.2246)  loss_mask: 0.3718 (0.4405)  time: 0.1616  data: 0.0003  max mem: 4132
[18:31:02.643423] Epoch: [5]  [700/781]  eta: 0:00:13  lr: 0.000250  training_loss: 0.5777 (0.6631)  classification_loss: 0.2236 (0.2246)  loss_mask: 0.3567 (0.4385)  time: 0.1613  data: 0.0002  max mem: 4132
[18:31:05.903309] Epoch: [5]  [720/781]  eta: 0:00:09  lr: 0.000250  training_loss: 0.5630 (0.6607)  classification_loss: 0.2252 (0.2246)  loss_mask: 0.3368 (0.4362)  time: 0.1629  data: 0.0004  max mem: 4132
[18:31:09.118094] Epoch: [5]  [740/781]  eta: 0:00:06  lr: 0.000250  training_loss: 0.4875 (0.6565)  classification_loss: 0.2240 (0.2246)  loss_mask: 0.2587 (0.4319)  time: 0.1606  data: 0.0002  max mem: 4132
[18:31:12.331522] Epoch: [5]  [760/781]  eta: 0:00:03  lr: 0.000250  training_loss: 0.6132 (0.6551)  classification_loss: 0.2236 (0.2245)  loss_mask: 0.3896 (0.4306)  time: 0.1606  data: 0.0002  max mem: 4132
[18:31:15.547267] Epoch: [5]  [780/781]  eta: 0:00:00  lr: 0.000250  training_loss: 0.6199 (0.6559)  classification_loss: 0.2247 (0.2246)  loss_mask: 0.3999 (0.4314)  time: 0.1607  data: 0.0002  max mem: 4132
[18:31:15.700323] Epoch: [5] Total time: 0:02:07 (0.1635 s / it)
[18:31:15.700770] Averaged stats: lr: 0.000250  training_loss: 0.6199 (0.6559)  classification_loss: 0.2247 (0.2246)  loss_mask: 0.3999 (0.4314)
[18:31:16.333018] Test:  [  0/157]  eta: 0:01:38  testing_loss: 2.1199 (2.1199)  acc1: 25.0000 (25.0000)  acc5: 81.2500 (81.2500)  time: 0.6258  data: 0.5967  max mem: 4132
[18:31:16.622187] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 2.1645 (2.1507)  acc1: 21.8750 (22.0170)  acc5: 78.1250 (77.5568)  time: 0.0829  data: 0.0544  max mem: 4132
[18:31:16.904351] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 2.1598 (2.1504)  acc1: 20.3125 (22.0982)  acc5: 75.0000 (76.2649)  time: 0.0283  data: 0.0002  max mem: 4132
[18:31:17.186120] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 2.1365 (2.1469)  acc1: 23.4375 (22.7319)  acc5: 73.4375 (75.9073)  time: 0.0280  data: 0.0002  max mem: 4132
[18:31:17.467502] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 2.1424 (2.1467)  acc1: 25.0000 (23.5137)  acc5: 75.0000 (75.4954)  time: 0.0280  data: 0.0002  max mem: 4132
[18:31:17.749228] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 2.1546 (2.1492)  acc1: 23.4375 (22.6716)  acc5: 76.5625 (75.3983)  time: 0.0280  data: 0.0001  max mem: 4132
[18:31:18.031215] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 2.1538 (2.1495)  acc1: 18.7500 (22.4641)  acc5: 76.5625 (75.4867)  time: 0.0280  data: 0.0001  max mem: 4132
[18:31:18.312970] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 2.1452 (2.1479)  acc1: 18.7500 (22.0731)  acc5: 76.5625 (75.8363)  time: 0.0280  data: 0.0001  max mem: 4132
[18:31:18.597023] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 2.1380 (2.1488)  acc1: 18.7500 (21.8171)  acc5: 76.5625 (75.9838)  time: 0.0281  data: 0.0002  max mem: 4132
[18:31:18.884491] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 2.1618 (2.1508)  acc1: 18.7500 (21.3942)  acc5: 76.5625 (75.8757)  time: 0.0284  data: 0.0002  max mem: 4132
[18:31:19.167118] Test:  [100/157]  eta: 0:00:01  testing_loss: 2.1515 (2.1513)  acc1: 18.7500 (21.3800)  acc5: 73.4375 (75.5415)  time: 0.0283  data: 0.0002  max mem: 4132
[18:31:19.448424] Test:  [110/157]  eta: 0:00:01  testing_loss: 2.1485 (2.1516)  acc1: 20.3125 (21.3542)  acc5: 73.4375 (75.5631)  time: 0.0281  data: 0.0002  max mem: 4132
[18:31:19.731893] Test:  [120/157]  eta: 0:00:01  testing_loss: 2.1379 (2.1501)  acc1: 20.3125 (21.5651)  acc5: 76.5625 (75.5940)  time: 0.0281  data: 0.0002  max mem: 4132
[18:31:20.016491] Test:  [130/157]  eta: 0:00:00  testing_loss: 2.1502 (2.1504)  acc1: 25.0000 (21.8511)  acc5: 76.5625 (75.6441)  time: 0.0283  data: 0.0002  max mem: 4132
[18:31:20.298467] Test:  [140/157]  eta: 0:00:00  testing_loss: 2.1478 (2.1489)  acc1: 25.0000 (22.0191)  acc5: 76.5625 (75.7979)  time: 0.0282  data: 0.0002  max mem: 4132
[18:31:20.579287] Test:  [150/157]  eta: 0:00:00  testing_loss: 2.1315 (2.1484)  acc1: 23.4375 (22.1440)  acc5: 78.1250 (75.8485)  time: 0.0279  data: 0.0001  max mem: 4132
[18:31:20.729956] Test:  [156/157]  eta: 0:00:00  testing_loss: 2.1295 (2.1477)  acc1: 23.4375 (22.1300)  acc5: 76.5625 (75.9800)  time: 0.0270  data: 0.0001  max mem: 4132
[18:31:20.895522] Test: Total time: 0:00:05 (0.0331 s / it)
[18:31:20.896051] * Acc@1 22.130 Acc@5 75.980 loss 2.148
[18:31:20.896403] Accuracy of the network on the 10000 test images: 22.1%
[18:31:20.896616] Max accuracy: 25.08%
[18:31:21.279706] log_dir: ./output_dir
[18:31:22.100781] Epoch: [6]  [  0/781]  eta: 0:10:39  lr: 0.000250  training_loss: 0.5197 (0.5197)  classification_loss: 0.2270 (0.2270)  loss_mask: 0.2927 (0.2927)  time: 0.8194  data: 0.6347  max mem: 4132
[18:31:25.337291] Epoch: [6]  [ 20/781]  eta: 0:02:26  lr: 0.000250  training_loss: 0.5816 (0.5907)  classification_loss: 0.2252 (0.2256)  loss_mask: 0.3535 (0.3652)  time: 0.1617  data: 0.0003  max mem: 4132
[18:31:28.567415] Epoch: [6]  [ 40/781]  eta: 0:02:11  lr: 0.000250  training_loss: 0.5164 (0.5594)  classification_loss: 0.2238 (0.2248)  loss_mask: 0.2925 (0.3346)  time: 0.1614  data: 0.0002  max mem: 4132
[18:31:31.807810] Epoch: [6]  [ 60/781]  eta: 0:02:04  lr: 0.000250  training_loss: 0.5102 (0.5484)  classification_loss: 0.2245 (0.2246)  loss_mask: 0.2836 (0.3238)  time: 0.1619  data: 0.0002  max mem: 4132
[18:31:35.071131] Epoch: [6]  [ 80/781]  eta: 0:01:59  lr: 0.000250  training_loss: 0.5834 (0.5706)  classification_loss: 0.2249 (0.2247)  loss_mask: 0.3612 (0.3460)  time: 0.1631  data: 0.0002  max mem: 4132
[18:31:38.325027] Epoch: [6]  [100/781]  eta: 0:01:54  lr: 0.000250  training_loss: 0.6160 (0.5804)  classification_loss: 0.2243 (0.2247)  loss_mask: 0.3941 (0.3557)  time: 0.1626  data: 0.0002  max mem: 4132
[18:31:41.579488] Epoch: [6]  [120/781]  eta: 0:01:50  lr: 0.000250  training_loss: 0.5565 (0.5828)  classification_loss: 0.2253 (0.2247)  loss_mask: 0.3357 (0.3581)  time: 0.1626  data: 0.0002  max mem: 4132
[18:31:44.823359] Epoch: [6]  [140/781]  eta: 0:01:46  lr: 0.000250  training_loss: 0.5491 (0.5797)  classification_loss: 0.2263 (0.2247)  loss_mask: 0.3207 (0.3549)  time: 0.1621  data: 0.0002  max mem: 4132
[18:31:48.127030] Epoch: [6]  [160/781]  eta: 0:01:43  lr: 0.000250  training_loss: 0.5513 (0.5771)  classification_loss: 0.2244 (0.2247)  loss_mask: 0.3238 (0.3524)  time: 0.1651  data: 0.0002  max mem: 4132
[18:31:51.374521] Epoch: [6]  [180/781]  eta: 0:01:39  lr: 0.000250  training_loss: 0.5550 (0.5757)  classification_loss: 0.2236 (0.2246)  loss_mask: 0.3316 (0.3510)  time: 0.1623  data: 0.0002  max mem: 4132
[18:31:54.613076] Epoch: [6]  [200/781]  eta: 0:01:36  lr: 0.000250  training_loss: 0.5491 (0.5753)  classification_loss: 0.2242 (0.2246)  loss_mask: 0.3250 (0.3507)  time: 0.1619  data: 0.0002  max mem: 4132
[18:31:57.858888] Epoch: [6]  [220/781]  eta: 0:01:32  lr: 0.000250  training_loss: 0.5388 (0.5745)  classification_loss: 0.2255 (0.2248)  loss_mask: 0.3145 (0.3498)  time: 0.1622  data: 0.0002  max mem: 4132
[18:32:01.119048] Epoch: [6]  [240/781]  eta: 0:01:29  lr: 0.000250  training_loss: 0.4927 (0.5676)  classification_loss: 0.2260 (0.2249)  loss_mask: 0.2692 (0.3428)  time: 0.1629  data: 0.0004  max mem: 4132
[18:32:04.363540] Epoch: [6]  [260/781]  eta: 0:01:25  lr: 0.000250  training_loss: 0.4847 (0.5625)  classification_loss: 0.2236 (0.2248)  loss_mask: 0.2557 (0.3377)  time: 0.1622  data: 0.0002  max mem: 4132
[18:32:07.614236] Epoch: [6]  [280/781]  eta: 0:01:22  lr: 0.000250  training_loss: 0.4753 (0.5559)  classification_loss: 0.2249 (0.2249)  loss_mask: 0.2490 (0.3310)  time: 0.1624  data: 0.0002  max mem: 4132
[18:32:10.837638] Epoch: [6]  [300/781]  eta: 0:01:19  lr: 0.000250  training_loss: 0.5053 (0.5525)  classification_loss: 0.2248 (0.2249)  loss_mask: 0.2761 (0.3276)  time: 0.1611  data: 0.0003  max mem: 4132
[18:32:14.049489] Epoch: [6]  [320/781]  eta: 0:01:15  lr: 0.000250  training_loss: 0.4964 (0.5528)  classification_loss: 0.2243 (0.2249)  loss_mask: 0.2746 (0.3279)  time: 0.1605  data: 0.0002  max mem: 4132
[18:32:17.265963] Epoch: [6]  [340/781]  eta: 0:01:12  lr: 0.000250  training_loss: 0.4851 (0.5495)  classification_loss: 0.2247 (0.2249)  loss_mask: 0.2582 (0.3246)  time: 0.1607  data: 0.0003  max mem: 4132
[18:32:20.504722] Epoch: [6]  [360/781]  eta: 0:01:09  lr: 0.000250  training_loss: 0.5575 (0.5521)  classification_loss: 0.2249 (0.2249)  loss_mask: 0.3295 (0.3272)  time: 0.1618  data: 0.0002  max mem: 4132
[18:32:23.747537] Epoch: [6]  [380/781]  eta: 0:01:05  lr: 0.000250  training_loss: 0.4855 (0.5497)  classification_loss: 0.2267 (0.2250)  loss_mask: 0.2558 (0.3247)  time: 0.1621  data: 0.0003  max mem: 4132
[18:32:26.989324] Epoch: [6]  [400/781]  eta: 0:01:02  lr: 0.000250  training_loss: 0.4814 (0.5465)  classification_loss: 0.2251 (0.2250)  loss_mask: 0.2588 (0.3214)  time: 0.1620  data: 0.0003  max mem: 4132
[18:32:30.222172] Epoch: [6]  [420/781]  eta: 0:00:59  lr: 0.000250  training_loss: 0.4889 (0.5436)  classification_loss: 0.2254 (0.2251)  loss_mask: 0.2639 (0.3185)  time: 0.1616  data: 0.0003  max mem: 4132
[18:32:33.455947] Epoch: [6]  [440/781]  eta: 0:00:55  lr: 0.000250  training_loss: 0.4846 (0.5424)  classification_loss: 0.2247 (0.2251)  loss_mask: 0.2603 (0.3174)  time: 0.1616  data: 0.0002  max mem: 4132
[18:32:36.730967] Epoch: [6]  [460/781]  eta: 0:00:52  lr: 0.000250  training_loss: 0.5828 (0.5436)  classification_loss: 0.2234 (0.2250)  loss_mask: 0.3599 (0.3185)  time: 0.1637  data: 0.0002  max mem: 4132
[18:32:39.960437] Epoch: [6]  [480/781]  eta: 0:00:49  lr: 0.000250  training_loss: 0.4709 (0.5411)  classification_loss: 0.2255 (0.2250)  loss_mask: 0.2400 (0.3161)  time: 0.1614  data: 0.0002  max mem: 4132
[18:32:43.272422] Epoch: [6]  [500/781]  eta: 0:00:45  lr: 0.000250  training_loss: 0.5052 (0.5399)  classification_loss: 0.2239 (0.2250)  loss_mask: 0.2812 (0.3149)  time: 0.1655  data: 0.0002  max mem: 4132
[18:32:46.580845] Epoch: [6]  [520/781]  eta: 0:00:42  lr: 0.000250  training_loss: 0.4320 (0.5362)  classification_loss: 0.2243 (0.2250)  loss_mask: 0.2083 (0.3111)  time: 0.1653  data: 0.0003  max mem: 4132
[18:32:49.874005] Epoch: [6]  [540/781]  eta: 0:00:39  lr: 0.000250  training_loss: 0.4429 (0.5332)  classification_loss: 0.2244 (0.2250)  loss_mask: 0.2196 (0.3082)  time: 0.1646  data: 0.0002  max mem: 4132
[18:32:53.112184] Epoch: [6]  [560/781]  eta: 0:00:36  lr: 0.000250  training_loss: 0.4185 (0.5297)  classification_loss: 0.2247 (0.2250)  loss_mask: 0.1950 (0.3047)  time: 0.1618  data: 0.0003  max mem: 4132
[18:32:56.393761] Epoch: [6]  [580/781]  eta: 0:00:32  lr: 0.000250  training_loss: 0.4249 (0.5267)  classification_loss: 0.2239 (0.2250)  loss_mask: 0.1997 (0.3017)  time: 0.1640  data: 0.0002  max mem: 4132
[18:32:59.661740] Epoch: [6]  [600/781]  eta: 0:00:29  lr: 0.000250  training_loss: 0.4345 (0.5247)  classification_loss: 0.2238 (0.2249)  loss_mask: 0.2130 (0.2998)  time: 0.1633  data: 0.0003  max mem: 4132
[18:33:02.917893] Epoch: [6]  [620/781]  eta: 0:00:26  lr: 0.000250  training_loss: 0.4991 (0.5246)  classification_loss: 0.2251 (0.2250)  loss_mask: 0.2742 (0.2996)  time: 0.1627  data: 0.0002  max mem: 4132
[18:33:06.167416] Epoch: [6]  [640/781]  eta: 0:00:23  lr: 0.000250  training_loss: 0.4435 (0.5225)  classification_loss: 0.2268 (0.2250)  loss_mask: 0.2161 (0.2975)  time: 0.1624  data: 0.0002  max mem: 4132
[18:33:09.420204] Epoch: [6]  [660/781]  eta: 0:00:19  lr: 0.000250  training_loss: 0.4150 (0.5195)  classification_loss: 0.2249 (0.2250)  loss_mask: 0.1873 (0.2945)  time: 0.1626  data: 0.0003  max mem: 4132
[18:33:12.682945] Epoch: [6]  [680/781]  eta: 0:00:16  lr: 0.000250  training_loss: 0.4481 (0.5177)  classification_loss: 0.2238 (0.2250)  loss_mask: 0.2239 (0.2927)  time: 0.1630  data: 0.0003  max mem: 4132
[18:33:15.926848] Epoch: [6]  [700/781]  eta: 0:00:13  lr: 0.000250  training_loss: 0.4403 (0.5155)  classification_loss: 0.2241 (0.2250)  loss_mask: 0.2187 (0.2906)  time: 0.1621  data: 0.0003  max mem: 4132
[18:33:19.190877] Epoch: [6]  [720/781]  eta: 0:00:09  lr: 0.000250  training_loss: 0.4469 (0.5142)  classification_loss: 0.2249 (0.2250)  loss_mask: 0.2254 (0.2892)  time: 0.1631  data: 0.0002  max mem: 4132
[18:33:22.442108] Epoch: [6]  [740/781]  eta: 0:00:06  lr: 0.000250  training_loss: 0.4056 (0.5125)  classification_loss: 0.2274 (0.2251)  loss_mask: 0.1778 (0.2874)  time: 0.1625  data: 0.0002  max mem: 4132
[18:33:25.676758] Epoch: [6]  [760/781]  eta: 0:00:03  lr: 0.000250  training_loss: 0.4762 (0.5115)  classification_loss: 0.2244 (0.2250)  loss_mask: 0.2486 (0.2865)  time: 0.1617  data: 0.0002  max mem: 4132
[18:33:28.878688] Epoch: [6]  [780/781]  eta: 0:00:00  lr: 0.000250  training_loss: 0.4461 (0.5102)  classification_loss: 0.2249 (0.2251)  loss_mask: 0.2234 (0.2851)  time: 0.1600  data: 0.0002  max mem: 4132
[18:33:29.022314] Epoch: [6] Total time: 0:02:07 (0.1636 s / it)
[18:33:29.023159] Averaged stats: lr: 0.000250  training_loss: 0.4461 (0.5102)  classification_loss: 0.2249 (0.2251)  loss_mask: 0.2234 (0.2851)
[18:33:29.665673] Test:  [  0/157]  eta: 0:01:40  testing_loss: 2.1259 (2.1259)  acc1: 31.2500 (31.2500)  acc5: 78.1250 (78.1250)  time: 0.6376  data: 0.5992  max mem: 4132
[18:33:29.951651] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 2.1697 (2.1631)  acc1: 23.4375 (24.0057)  acc5: 73.4375 (75.4261)  time: 0.0838  data: 0.0548  max mem: 4132
[18:33:30.240876] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 2.1722 (2.1652)  acc1: 23.4375 (23.2887)  acc5: 73.4375 (73.9583)  time: 0.0286  data: 0.0002  max mem: 4132
[18:33:30.524855] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 2.1624 (2.1618)  acc1: 23.4375 (23.8407)  acc5: 73.4375 (73.7399)  time: 0.0285  data: 0.0002  max mem: 4132
[18:33:30.810642] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 2.1624 (2.1617)  acc1: 23.4375 (23.7805)  acc5: 71.8750 (73.4756)  time: 0.0284  data: 0.0002  max mem: 4132
[18:33:31.091988] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 2.1705 (2.1641)  acc1: 20.3125 (22.9779)  acc5: 71.8750 (73.3762)  time: 0.0282  data: 0.0002  max mem: 4132
[18:33:31.373790] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 2.1659 (2.1641)  acc1: 21.8750 (23.0533)  acc5: 71.8750 (73.4119)  time: 0.0280  data: 0.0001  max mem: 4132
[18:33:31.656681] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 2.1548 (2.1633)  acc1: 23.4375 (22.9974)  acc5: 73.4375 (73.6356)  time: 0.0281  data: 0.0001  max mem: 4132
[18:33:31.937678] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 2.1564 (2.1640)  acc1: 20.3125 (22.5887)  acc5: 75.0000 (73.6690)  time: 0.0281  data: 0.0001  max mem: 4132
[18:33:32.220403] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 2.1811 (2.1660)  acc1: 20.3125 (22.3043)  acc5: 75.0000 (73.4032)  time: 0.0281  data: 0.0001  max mem: 4132
[18:33:32.504468] Test:  [100/157]  eta: 0:00:01  testing_loss: 2.1665 (2.1663)  acc1: 21.8750 (22.4010)  acc5: 73.4375 (73.2673)  time: 0.0282  data: 0.0002  max mem: 4132
[18:33:32.788394] Test:  [110/157]  eta: 0:00:01  testing_loss: 2.1593 (2.1666)  acc1: 21.8750 (22.3818)  acc5: 71.8750 (73.3249)  time: 0.0283  data: 0.0002  max mem: 4132
[18:33:33.073774] Test:  [120/157]  eta: 0:00:01  testing_loss: 2.1575 (2.1657)  acc1: 21.8750 (22.5207)  acc5: 71.8750 (73.2955)  time: 0.0283  data: 0.0002  max mem: 4132
[18:33:33.360526] Test:  [130/157]  eta: 0:00:00  testing_loss: 2.1582 (2.1662)  acc1: 21.8750 (22.4475)  acc5: 75.0000 (73.3779)  time: 0.0285  data: 0.0002  max mem: 4132
[18:33:33.642892] Test:  [140/157]  eta: 0:00:00  testing_loss: 2.1582 (2.1650)  acc1: 21.8750 (22.6840)  acc5: 75.0000 (73.4707)  time: 0.0283  data: 0.0001  max mem: 4132
[18:33:33.921896] Test:  [150/157]  eta: 0:00:00  testing_loss: 2.1489 (2.1645)  acc1: 25.0000 (22.7339)  acc5: 75.0000 (73.6134)  time: 0.0279  data: 0.0001  max mem: 4132
[18:33:34.072863] Test:  [156/157]  eta: 0:00:00  testing_loss: 2.1495 (2.1639)  acc1: 23.4375 (22.6900)  acc5: 75.0000 (73.7500)  time: 0.0269  data: 0.0001  max mem: 4132
[18:33:34.227652] Test: Total time: 0:00:05 (0.0331 s / it)
[18:33:34.228132] * Acc@1 22.690 Acc@5 73.750 loss 2.164
[18:33:34.228427] Accuracy of the network on the 10000 test images: 22.7%
[18:33:34.228598] Max accuracy: 25.08%
[18:33:34.595095] log_dir: ./output_dir
[18:33:35.434773] Epoch: [7]  [  0/781]  eta: 0:10:54  lr: 0.000250  training_loss: 0.4004 (0.4004)  classification_loss: 0.2252 (0.2252)  loss_mask: 0.1752 (0.1752)  time: 0.8380  data: 0.6360  max mem: 4132
[18:33:38.776830] Epoch: [7]  [ 20/781]  eta: 0:02:31  lr: 0.000250  training_loss: 0.4293 (0.4459)  classification_loss: 0.2246 (0.2253)  loss_mask: 0.2066 (0.2206)  time: 0.1670  data: 0.0002  max mem: 4132
[18:33:42.005531] Epoch: [7]  [ 40/781]  eta: 0:02:13  lr: 0.000250  training_loss: 0.4015 (0.4314)  classification_loss: 0.2269 (0.2256)  loss_mask: 0.1726 (0.2058)  time: 0.1614  data: 0.0002  max mem: 4132
[18:33:45.244698] Epoch: [7]  [ 60/781]  eta: 0:02:05  lr: 0.000250  training_loss: 0.4028 (0.4294)  classification_loss: 0.2275 (0.2260)  loss_mask: 0.1734 (0.2034)  time: 0.1619  data: 0.0002  max mem: 4132
[18:33:48.486798] Epoch: [7]  [ 80/781]  eta: 0:02:00  lr: 0.000250  training_loss: 0.3738 (0.4197)  classification_loss: 0.2260 (0.2260)  loss_mask: 0.1503 (0.1937)  time: 0.1620  data: 0.0002  max mem: 4132
[18:33:51.720319] Epoch: [7]  [100/781]  eta: 0:01:55  lr: 0.000250  training_loss: 0.3964 (0.4194)  classification_loss: 0.2244 (0.2259)  loss_mask: 0.1712 (0.1936)  time: 0.1616  data: 0.0002  max mem: 4132
[18:33:54.974152] Epoch: [7]  [120/781]  eta: 0:01:51  lr: 0.000250  training_loss: 0.4702 (0.4319)  classification_loss: 0.2244 (0.2257)  loss_mask: 0.2458 (0.2062)  time: 0.1626  data: 0.0002  max mem: 4132
[18:33:58.195810] Epoch: [7]  [140/781]  eta: 0:01:47  lr: 0.000250  training_loss: 0.4937 (0.4438)  classification_loss: 0.2246 (0.2256)  loss_mask: 0.2649 (0.2183)  time: 0.1610  data: 0.0003  max mem: 4132
[18:34:01.463643] Epoch: [7]  [160/781]  eta: 0:01:43  lr: 0.000250  training_loss: 0.4237 (0.4454)  classification_loss: 0.2250 (0.2255)  loss_mask: 0.1978 (0.2199)  time: 0.1633  data: 0.0003  max mem: 4132
[18:34:04.723264] Epoch: [7]  [180/781]  eta: 0:01:39  lr: 0.000250  training_loss: 0.4259 (0.4445)  classification_loss: 0.2261 (0.2256)  loss_mask: 0.1954 (0.2189)  time: 0.1629  data: 0.0005  max mem: 4132
[18:34:08.020019] Epoch: [7]  [200/781]  eta: 0:01:36  lr: 0.000250  training_loss: 0.3918 (0.4413)  classification_loss: 0.2252 (0.2256)  loss_mask: 0.1671 (0.2157)  time: 0.1648  data: 0.0003  max mem: 4132
[18:34:11.312270] Epoch: [7]  [220/781]  eta: 0:01:33  lr: 0.000250  training_loss: 0.3717 (0.4354)  classification_loss: 0.2254 (0.2256)  loss_mask: 0.1468 (0.2097)  time: 0.1645  data: 0.0002  max mem: 4132
[18:34:14.554182] Epoch: [7]  [240/781]  eta: 0:01:29  lr: 0.000250  training_loss: 0.3642 (0.4307)  classification_loss: 0.2257 (0.2256)  loss_mask: 0.1385 (0.2050)  time: 0.1620  data: 0.0002  max mem: 4132
[18:34:17.827144] Epoch: [7]  [260/781]  eta: 0:01:26  lr: 0.000250  training_loss: 0.4312 (0.4361)  classification_loss: 0.2253 (0.2257)  loss_mask: 0.2094 (0.2104)  time: 0.1636  data: 0.0002  max mem: 4132
[18:34:21.074514] Epoch: [7]  [280/781]  eta: 0:01:22  lr: 0.000250  training_loss: 0.4957 (0.4414)  classification_loss: 0.2270 (0.2257)  loss_mask: 0.2648 (0.2157)  time: 0.1623  data: 0.0002  max mem: 4132
[18:34:24.342246] Epoch: [7]  [300/781]  eta: 0:01:19  lr: 0.000250  training_loss: 0.5002 (0.4470)  classification_loss: 0.2257 (0.2257)  loss_mask: 0.2775 (0.2213)  time: 0.1633  data: 0.0002  max mem: 4132
[18:34:27.585473] Epoch: [7]  [320/781]  eta: 0:01:16  lr: 0.000250  training_loss: 0.4201 (0.4460)  classification_loss: 0.2246 (0.2256)  loss_mask: 0.1986 (0.2203)  time: 0.1621  data: 0.0002  max mem: 4132
[18:34:30.826644] Epoch: [7]  [340/781]  eta: 0:01:12  lr: 0.000250  training_loss: 0.3745 (0.4425)  classification_loss: 0.2260 (0.2256)  loss_mask: 0.1539 (0.2169)  time: 0.1620  data: 0.0002  max mem: 4132
[18:34:34.072419] Epoch: [7]  [360/781]  eta: 0:01:09  lr: 0.000250  training_loss: 0.3851 (0.4392)  classification_loss: 0.2247 (0.2256)  loss_mask: 0.1612 (0.2136)  time: 0.1622  data: 0.0003  max mem: 4132
[18:34:37.315637] Epoch: [7]  [380/781]  eta: 0:01:05  lr: 0.000250  training_loss: 0.3740 (0.4362)  classification_loss: 0.2254 (0.2256)  loss_mask: 0.1472 (0.2106)  time: 0.1621  data: 0.0003  max mem: 4132
[18:34:40.569684] Epoch: [7]  [400/781]  eta: 0:01:02  lr: 0.000250  training_loss: 0.4268 (0.4367)  classification_loss: 0.2248 (0.2256)  loss_mask: 0.2041 (0.2111)  time: 0.1626  data: 0.0003  max mem: 4132
[18:34:43.800282] Epoch: [7]  [420/781]  eta: 0:00:59  lr: 0.000250  training_loss: 0.3518 (0.4331)  classification_loss: 0.2263 (0.2256)  loss_mask: 0.1241 (0.2075)  time: 0.1614  data: 0.0003  max mem: 4132
[18:34:47.040212] Epoch: [7]  [440/781]  eta: 0:00:55  lr: 0.000250  training_loss: 0.3548 (0.4295)  classification_loss: 0.2256 (0.2256)  loss_mask: 0.1267 (0.2039)  time: 0.1619  data: 0.0003  max mem: 4132
[18:34:50.288926] Epoch: [7]  [460/781]  eta: 0:00:52  lr: 0.000250  training_loss: 0.3645 (0.4272)  classification_loss: 0.2266 (0.2257)  loss_mask: 0.1356 (0.2015)  time: 0.1624  data: 0.0002  max mem: 4132
[18:34:53.529397] Epoch: [7]  [480/781]  eta: 0:00:49  lr: 0.000250  training_loss: 0.3928 (0.4275)  classification_loss: 0.2251 (0.2256)  loss_mask: 0.1677 (0.2018)  time: 0.1620  data: 0.0002  max mem: 4132
[18:34:56.768202] Epoch: [7]  [500/781]  eta: 0:00:46  lr: 0.000250  training_loss: 0.3931 (0.4263)  classification_loss: 0.2254 (0.2256)  loss_mask: 0.1673 (0.2007)  time: 0.1619  data: 0.0002  max mem: 4132
[18:35:00.040089] Epoch: [7]  [520/781]  eta: 0:00:42  lr: 0.000250  training_loss: 0.3429 (0.4234)  classification_loss: 0.2267 (0.2257)  loss_mask: 0.1195 (0.1977)  time: 0.1635  data: 0.0002  max mem: 4132
[18:35:03.289790] Epoch: [7]  [540/781]  eta: 0:00:39  lr: 0.000250  training_loss: 0.3731 (0.4225)  classification_loss: 0.2249 (0.2257)  loss_mask: 0.1482 (0.1968)  time: 0.1623  data: 0.0002  max mem: 4132
[18:35:06.538028] Epoch: [7]  [560/781]  eta: 0:00:36  lr: 0.000249  training_loss: 0.3650 (0.4213)  classification_loss: 0.2260 (0.2257)  loss_mask: 0.1403 (0.1956)  time: 0.1623  data: 0.0003  max mem: 4132
[18:35:09.812743] Epoch: [7]  [580/781]  eta: 0:00:32  lr: 0.000249  training_loss: 0.3348 (0.4185)  classification_loss: 0.2247 (0.2257)  loss_mask: 0.1093 (0.1928)  time: 0.1637  data: 0.0002  max mem: 4132
[18:35:13.048052] Epoch: [7]  [600/781]  eta: 0:00:29  lr: 0.000249  training_loss: 0.3640 (0.4169)  classification_loss: 0.2258 (0.2257)  loss_mask: 0.1357 (0.1912)  time: 0.1617  data: 0.0003  max mem: 4132
[18:35:16.289724] Epoch: [7]  [620/781]  eta: 0:00:26  lr: 0.000249  training_loss: 0.3744 (0.4161)  classification_loss: 0.2245 (0.2257)  loss_mask: 0.1479 (0.1904)  time: 0.1620  data: 0.0002  max mem: 4132
[18:35:19.542156] Epoch: [7]  [640/781]  eta: 0:00:23  lr: 0.000249  training_loss: 0.3177 (0.4132)  classification_loss: 0.2251 (0.2257)  loss_mask: 0.0891 (0.1875)  time: 0.1625  data: 0.0002  max mem: 4132
[18:35:22.785436] Epoch: [7]  [660/781]  eta: 0:00:19  lr: 0.000249  training_loss: 0.3465 (0.4119)  classification_loss: 0.2238 (0.2256)  loss_mask: 0.1264 (0.1863)  time: 0.1621  data: 0.0002  max mem: 4132
[18:35:26.030899] Epoch: [7]  [680/781]  eta: 0:00:16  lr: 0.000249  training_loss: 0.4531 (0.4137)  classification_loss: 0.2250 (0.2256)  loss_mask: 0.2277 (0.1881)  time: 0.1622  data: 0.0003  max mem: 4132
[18:35:29.275426] Epoch: [7]  [700/781]  eta: 0:00:13  lr: 0.000249  training_loss: 0.3413 (0.4120)  classification_loss: 0.2246 (0.2256)  loss_mask: 0.1163 (0.1864)  time: 0.1622  data: 0.0002  max mem: 4132
[18:35:32.502318] Epoch: [7]  [720/781]  eta: 0:00:09  lr: 0.000249  training_loss: 0.3556 (0.4112)  classification_loss: 0.2259 (0.2256)  loss_mask: 0.1329 (0.1856)  time: 0.1613  data: 0.0002  max mem: 4132
[18:35:35.778182] Epoch: [7]  [740/781]  eta: 0:00:06  lr: 0.000249  training_loss: 0.3763 (0.4106)  classification_loss: 0.2257 (0.2256)  loss_mask: 0.1513 (0.1850)  time: 0.1637  data: 0.0002  max mem: 4132
[18:35:39.014171] Epoch: [7]  [760/781]  eta: 0:00:03  lr: 0.000249  training_loss: 0.3926 (0.4101)  classification_loss: 0.2247 (0.2256)  loss_mask: 0.1642 (0.1845)  time: 0.1617  data: 0.0002  max mem: 4132
[18:35:42.250029] Epoch: [7]  [780/781]  eta: 0:00:00  lr: 0.000249  training_loss: 0.3331 (0.4081)  classification_loss: 0.2253 (0.2256)  loss_mask: 0.1100 (0.1825)  time: 0.1617  data: 0.0002  max mem: 4132
[18:35:42.408321] Epoch: [7] Total time: 0:02:07 (0.1637 s / it)
[18:35:42.408860] Averaged stats: lr: 0.000249  training_loss: 0.3331 (0.4081)  classification_loss: 0.2253 (0.2256)  loss_mask: 0.1100 (0.1825)
[18:35:43.037933] Test:  [  0/157]  eta: 0:01:38  testing_loss: 2.1114 (2.1114)  acc1: 37.5000 (37.5000)  acc5: 78.1250 (78.1250)  time: 0.6245  data: 0.5932  max mem: 4132
[18:35:43.326790] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 2.1664 (2.1576)  acc1: 23.4375 (24.4318)  acc5: 75.0000 (75.1420)  time: 0.0828  data: 0.0541  max mem: 4132
[18:35:43.616168] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 2.1717 (2.1611)  acc1: 23.4375 (24.0327)  acc5: 71.8750 (74.4792)  time: 0.0287  data: 0.0002  max mem: 4132
[18:35:43.901691] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 2.1552 (2.1573)  acc1: 25.0000 (23.9415)  acc5: 75.0000 (74.6976)  time: 0.0286  data: 0.0002  max mem: 4132
[18:35:44.194350] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 2.1578 (2.1577)  acc1: 21.8750 (23.2088)  acc5: 75.0000 (74.5427)  time: 0.0287  data: 0.0002  max mem: 4132
[18:35:44.481273] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 2.1651 (2.1596)  acc1: 20.3125 (22.7328)  acc5: 71.8750 (74.1728)  time: 0.0288  data: 0.0002  max mem: 4132
[18:35:44.771472] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 2.1635 (2.1600)  acc1: 20.3125 (22.5666)  acc5: 71.8750 (74.0266)  time: 0.0287  data: 0.0002  max mem: 4132
[18:35:45.060343] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 2.1550 (2.1595)  acc1: 20.3125 (22.3812)  acc5: 73.4375 (74.1637)  time: 0.0288  data: 0.0002  max mem: 4132
[18:35:45.343881] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 2.1571 (2.1596)  acc1: 20.3125 (22.2222)  acc5: 75.0000 (74.2863)  time: 0.0285  data: 0.0002  max mem: 4132
[18:35:45.625790] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 2.1654 (2.1610)  acc1: 20.3125 (21.8922)  acc5: 75.0000 (74.1071)  time: 0.0282  data: 0.0002  max mem: 4132
[18:35:45.912589] Test:  [100/157]  eta: 0:00:01  testing_loss: 2.1646 (2.1614)  acc1: 20.3125 (21.8595)  acc5: 73.4375 (74.0099)  time: 0.0283  data: 0.0002  max mem: 4132
[18:35:46.198841] Test:  [110/157]  eta: 0:00:01  testing_loss: 2.1627 (2.1617)  acc1: 20.3125 (21.6920)  acc5: 73.4375 (74.1132)  time: 0.0285  data: 0.0002  max mem: 4132
[18:35:46.496311] Test:  [120/157]  eta: 0:00:01  testing_loss: 2.1476 (2.1604)  acc1: 21.8750 (21.7975)  acc5: 75.0000 (74.0702)  time: 0.0290  data: 0.0002  max mem: 4132
[18:35:46.790234] Test:  [130/157]  eta: 0:00:00  testing_loss: 2.1552 (2.1607)  acc1: 23.4375 (21.9108)  acc5: 73.4375 (74.0219)  time: 0.0294  data: 0.0002  max mem: 4132
[18:35:47.073747] Test:  [140/157]  eta: 0:00:00  testing_loss: 2.1552 (2.1596)  acc1: 23.4375 (22.0745)  acc5: 73.4375 (74.0691)  time: 0.0287  data: 0.0002  max mem: 4132
[18:35:47.354648] Test:  [150/157]  eta: 0:00:00  testing_loss: 2.1492 (2.1590)  acc1: 21.8750 (22.0613)  acc5: 75.0000 (74.1929)  time: 0.0281  data: 0.0001  max mem: 4132
[18:35:47.505407] Test:  [156/157]  eta: 0:00:00  testing_loss: 2.1462 (2.1583)  acc1: 20.3125 (22.0400)  acc5: 76.5625 (74.3000)  time: 0.0270  data: 0.0001  max mem: 4132
[18:35:47.655769] Test: Total time: 0:00:05 (0.0334 s / it)
[18:35:47.657104] * Acc@1 22.040 Acc@5 74.300 loss 2.158
[18:35:47.657989] Accuracy of the network on the 10000 test images: 22.0%
[18:35:47.658661] Max accuracy: 25.08%
[18:35:47.842030] log_dir: ./output_dir
[18:35:48.647481] Epoch: [8]  [  0/781]  eta: 0:10:27  lr: 0.000249  training_loss: 0.3293 (0.3293)  classification_loss: 0.2238 (0.2238)  loss_mask: 0.1055 (0.1055)  time: 0.8030  data: 0.6369  max mem: 4132
[18:35:51.917174] Epoch: [8]  [ 20/781]  eta: 0:02:27  lr: 0.000249  training_loss: 0.3295 (0.3703)  classification_loss: 0.2251 (0.2258)  loss_mask: 0.1058 (0.1445)  time: 0.1634  data: 0.0002  max mem: 4132
[18:35:55.172042] Epoch: [8]  [ 40/781]  eta: 0:02:12  lr: 0.000249  training_loss: 0.3473 (0.3727)  classification_loss: 0.2249 (0.2254)  loss_mask: 0.1258 (0.1473)  time: 0.1627  data: 0.0002  max mem: 4132
[18:35:58.439081] Epoch: [8]  [ 60/781]  eta: 0:02:05  lr: 0.000249  training_loss: 0.3411 (0.3639)  classification_loss: 0.2242 (0.2252)  loss_mask: 0.1175 (0.1386)  time: 0.1633  data: 0.0003  max mem: 4132
[18:36:01.774126] Epoch: [8]  [ 80/781]  eta: 0:02:00  lr: 0.000249  training_loss: 0.3236 (0.3568)  classification_loss: 0.2246 (0.2252)  loss_mask: 0.0992 (0.1316)  time: 0.1667  data: 0.0002  max mem: 4132
[18:36:05.004163] Epoch: [8]  [100/781]  eta: 0:01:55  lr: 0.000249  training_loss: 0.3359 (0.3553)  classification_loss: 0.2257 (0.2251)  loss_mask: 0.1089 (0.1302)  time: 0.1614  data: 0.0003  max mem: 4132
[18:36:08.268568] Epoch: [8]  [120/781]  eta: 0:01:51  lr: 0.000249  training_loss: 0.3288 (0.3534)  classification_loss: 0.2247 (0.2251)  loss_mask: 0.1024 (0.1283)  time: 0.1631  data: 0.0003  max mem: 4132
[18:36:11.521537] Epoch: [8]  [140/781]  eta: 0:01:47  lr: 0.000249  training_loss: 0.3653 (0.3549)  classification_loss: 0.2255 (0.2250)  loss_mask: 0.1410 (0.1299)  time: 0.1626  data: 0.0003  max mem: 4132
[18:36:14.812505] Epoch: [8]  [160/781]  eta: 0:01:43  lr: 0.000249  training_loss: 0.3220 (0.3515)  classification_loss: 0.2253 (0.2250)  loss_mask: 0.1009 (0.1265)  time: 0.1645  data: 0.0002  max mem: 4132
[18:36:18.046849] Epoch: [8]  [180/781]  eta: 0:01:40  lr: 0.000249  training_loss: 0.3204 (0.3483)  classification_loss: 0.2253 (0.2250)  loss_mask: 0.0930 (0.1233)  time: 0.1616  data: 0.0003  max mem: 4132
[18:36:21.295596] Epoch: [8]  [200/781]  eta: 0:01:36  lr: 0.000249  training_loss: 0.3444 (0.3497)  classification_loss: 0.2226 (0.2249)  loss_mask: 0.1203 (0.1248)  time: 0.1624  data: 0.0005  max mem: 4132
[18:36:24.531355] Epoch: [8]  [220/781]  eta: 0:01:33  lr: 0.000249  training_loss: 0.5347 (0.3678)  classification_loss: 0.2249 (0.2249)  loss_mask: 0.3106 (0.1429)  time: 0.1617  data: 0.0002  max mem: 4132
[18:36:27.766857] Epoch: [8]  [240/781]  eta: 0:01:29  lr: 0.000249  training_loss: 0.4140 (0.3744)  classification_loss: 0.2266 (0.2250)  loss_mask: 0.1845 (0.1494)  time: 0.1617  data: 0.0002  max mem: 4132
[18:36:31.024522] Epoch: [8]  [260/781]  eta: 0:01:26  lr: 0.000249  training_loss: 0.3542 (0.3732)  classification_loss: 0.2258 (0.2251)  loss_mask: 0.1285 (0.1481)  time: 0.1628  data: 0.0003  max mem: 4132
[18:36:34.257927] Epoch: [8]  [280/781]  eta: 0:01:22  lr: 0.000249  training_loss: 0.3661 (0.3726)  classification_loss: 0.2250 (0.2251)  loss_mask: 0.1373 (0.1475)  time: 0.1616  data: 0.0002  max mem: 4132
[18:36:37.513849] Epoch: [8]  [300/781]  eta: 0:01:19  lr: 0.000249  training_loss: 0.3373 (0.3713)  classification_loss: 0.2245 (0.2251)  loss_mask: 0.1127 (0.1462)  time: 0.1627  data: 0.0002  max mem: 4132
[18:36:40.756479] Epoch: [8]  [320/781]  eta: 0:01:15  lr: 0.000249  training_loss: 0.3526 (0.3704)  classification_loss: 0.2245 (0.2251)  loss_mask: 0.1267 (0.1453)  time: 0.1621  data: 0.0002  max mem: 4132
[18:36:44.044856] Epoch: [8]  [340/781]  eta: 0:01:12  lr: 0.000249  training_loss: 0.3223 (0.3680)  classification_loss: 0.2247 (0.2251)  loss_mask: 0.0947 (0.1429)  time: 0.1643  data: 0.0002  max mem: 4132
[18:36:47.274826] Epoch: [8]  [360/781]  eta: 0:01:09  lr: 0.000249  training_loss: 0.3067 (0.3648)  classification_loss: 0.2245 (0.2251)  loss_mask: 0.0806 (0.1398)  time: 0.1614  data: 0.0002  max mem: 4132
[18:36:50.516542] Epoch: [8]  [380/781]  eta: 0:01:05  lr: 0.000249  training_loss: 0.3152 (0.3635)  classification_loss: 0.2256 (0.2251)  loss_mask: 0.0910 (0.1383)  time: 0.1620  data: 0.0002  max mem: 4132
[18:36:53.764172] Epoch: [8]  [400/781]  eta: 0:01:02  lr: 0.000249  training_loss: 0.3105 (0.3615)  classification_loss: 0.2253 (0.2252)  loss_mask: 0.0853 (0.1363)  time: 0.1623  data: 0.0003  max mem: 4132
[18:36:57.043056] Epoch: [8]  [420/781]  eta: 0:00:59  lr: 0.000249  training_loss: 0.3072 (0.3591)  classification_loss: 0.2253 (0.2252)  loss_mask: 0.0810 (0.1339)  time: 0.1639  data: 0.0002  max mem: 4132
[18:37:00.285905] Epoch: [8]  [440/781]  eta: 0:00:55  lr: 0.000249  training_loss: 0.3222 (0.3594)  classification_loss: 0.2234 (0.2252)  loss_mask: 0.0966 (0.1342)  time: 0.1621  data: 0.0003  max mem: 4132
[18:37:03.531842] Epoch: [8]  [460/781]  eta: 0:00:52  lr: 0.000249  training_loss: 0.3507 (0.3607)  classification_loss: 0.2243 (0.2251)  loss_mask: 0.1304 (0.1356)  time: 0.1622  data: 0.0003  max mem: 4132
[18:37:06.786796] Epoch: [8]  [480/781]  eta: 0:00:49  lr: 0.000249  training_loss: 0.3112 (0.3591)  classification_loss: 0.2242 (0.2251)  loss_mask: 0.0827 (0.1340)  time: 0.1627  data: 0.0002  max mem: 4132
[18:37:10.042984] Epoch: [8]  [500/781]  eta: 0:00:46  lr: 0.000249  training_loss: 0.3583 (0.3598)  classification_loss: 0.2252 (0.2251)  loss_mask: 0.1279 (0.1347)  time: 0.1627  data: 0.0003  max mem: 4132
[18:37:13.295806] Epoch: [8]  [520/781]  eta: 0:00:42  lr: 0.000249  training_loss: 0.4286 (0.3656)  classification_loss: 0.2271 (0.2252)  loss_mask: 0.2008 (0.1404)  time: 0.1626  data: 0.0002  max mem: 4132

[18:37:16.548899] Epoch: [8]  [540/781]  eta: 0:00:39  lr: 0.000249  training_loss: 0.7195 (0.3870)  classification_loss: 0.2274 (0.2252)  loss_mask: 0.4929 (0.1618)  time: 0.1626  data: 0.0002  max mem: 4132
[18:37:19.785668] Epoch: [8]  [560/781]  eta: 0:00:36  lr: 0.000249  training_loss: 0.4666 (0.3905)  classification_loss: 0.2268 (0.2253)  loss_mask: 0.2330 (0.1652)  time: 0.1618  data: 0.0002  max mem: 4132
[18:37:23.038620] Epoch: [8]  [580/781]  eta: 0:00:32  lr: 0.000249  training_loss: 0.3835 (0.3911)  classification_loss: 0.2256 (0.2254)  loss_mask: 0.1584 (0.1657)  time: 0.1626  data: 0.0002  max mem: 4132
[18:37:26.264009] Epoch: [8]  [600/781]  eta: 0:00:29  lr: 0.000249  training_loss: 0.3697 (0.3903)  classification_loss: 0.2271 (0.2254)  loss_mask: 0.1430 (0.1649)  time: 0.1612  data: 0.0002  max mem: 4132
[18:37:29.502407] Epoch: [8]  [620/781]  eta: 0:00:26  lr: 0.000249  training_loss: 0.3526 (0.3896)  classification_loss: 0.2263 (0.2254)  loss_mask: 0.1261 (0.1641)  time: 0.1618  data: 0.0002  max mem: 4132
[18:37:32.739608] Epoch: [8]  [640/781]  eta: 0:00:23  lr: 0.000249  training_loss: 0.3451 (0.3884)  classification_loss: 0.2254 (0.2254)  loss_mask: 0.1163 (0.1630)  time: 0.1617  data: 0.0003  max mem: 4132
[18:37:35.974752] Epoch: [8]  [660/781]  eta: 0:00:19  lr: 0.000249  training_loss: 0.3871 (0.3886)  classification_loss: 0.2246 (0.2254)  loss_mask: 0.1655 (0.1631)  time: 0.1617  data: 0.0004  max mem: 4132
[18:37:39.234386] Epoch: [8]  [680/781]  eta: 0:00:16  lr: 0.000249  training_loss: 0.3747 (0.3882)  classification_loss: 0.2247 (0.2254)  loss_mask: 0.1510 (0.1627)  time: 0.1629  data: 0.0002  max mem: 4132
[18:37:42.478608] Epoch: [8]  [700/781]  eta: 0:00:13  lr: 0.000249  training_loss: 0.3704 (0.3881)  classification_loss: 0.2223 (0.2254)  loss_mask: 0.1454 (0.1627)  time: 0.1621  data: 0.0003  max mem: 4132
[18:37:45.735640] Epoch: [8]  [720/781]  eta: 0:00:09  lr: 0.000249  training_loss: 0.3742 (0.3880)  classification_loss: 0.2260 (0.2254)  loss_mask: 0.1509 (0.1626)  time: 0.1628  data: 0.0002  max mem: 4132
[18:37:48.976029] Epoch: [8]  [740/781]  eta: 0:00:06  lr: 0.000249  training_loss: 0.3709 (0.3875)  classification_loss: 0.2252 (0.2254)  loss_mask: 0.1457 (0.1620)  time: 0.1619  data: 0.0002  max mem: 4132
[18:37:52.203459] Epoch: [8]  [760/781]  eta: 0:00:03  lr: 0.000249  training_loss: 0.3186 (0.3861)  classification_loss: 0.2255 (0.2254)  loss_mask: 0.0955 (0.1607)  time: 0.1613  data: 0.0002  max mem: 4132
[18:37:55.423186] Epoch: [8]  [780/781]  eta: 0:00:00  lr: 0.000249  training_loss: 0.3201 (0.3852)  classification_loss: 0.2262 (0.2255)  loss_mask: 0.0970 (0.1598)  time: 0.1609  data: 0.0002  max mem: 4132
[18:37:55.567604] Epoch: [8] Total time: 0:02:07 (0.1635 s / it)
[18:37:55.568977] Averaged stats: lr: 0.000249  training_loss: 0.3201 (0.3852)  classification_loss: 0.2262 (0.2255)  loss_mask: 0.0970 (0.1598)
[18:37:56.209945] Test:  [  0/157]  eta: 0:01:39  testing_loss: 2.1192 (2.1192)  acc1: 32.8125 (32.8125)  acc5: 78.1250 (78.1250)  time: 0.6365  data: 0.5898  max mem: 4132
[18:37:56.496523] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 2.1548 (2.1564)  acc1: 23.4375 (23.0114)  acc5: 75.0000 (75.4261)  time: 0.0837  data: 0.0538  max mem: 4132
[18:37:56.784183] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 2.1584 (2.1582)  acc1: 21.8750 (23.1399)  acc5: 73.4375 (74.3304)  time: 0.0285  data: 0.0002  max mem: 4132
[18:37:57.070382] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 2.1517 (2.1533)  acc1: 21.8750 (23.5383)  acc5: 73.4375 (74.3952)  time: 0.0285  data: 0.0003  max mem: 4132
[18:37:57.360447] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 2.1517 (2.1532)  acc1: 23.4375 (23.6280)  acc5: 75.0000 (74.5427)  time: 0.0287  data: 0.0002  max mem: 4132
[18:37:57.648282] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 2.1585 (2.1559)  acc1: 21.8750 (23.0699)  acc5: 73.4375 (74.4179)  time: 0.0288  data: 0.0002  max mem: 4132
[18:37:57.932300] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 2.1568 (2.1565)  acc1: 21.8750 (23.1045)  acc5: 71.8750 (74.3084)  time: 0.0285  data: 0.0002  max mem: 4132
[18:37:58.218444] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 2.1532 (2.1560)  acc1: 23.4375 (22.9974)  acc5: 73.4375 (74.5379)  time: 0.0284  data: 0.0002  max mem: 4132
[18:37:58.516656] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 2.1519 (2.1567)  acc1: 21.8750 (22.8974)  acc5: 73.4375 (74.6528)  time: 0.0290  data: 0.0004  max mem: 4132
[18:37:58.804207] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 2.1675 (2.1587)  acc1: 21.8750 (22.8022)  acc5: 73.4375 (74.3819)  time: 0.0291  data: 0.0004  max mem: 4132
[18:37:59.087232] Test:  [100/157]  eta: 0:00:01  testing_loss: 2.1553 (2.1589)  acc1: 21.8750 (22.6795)  acc5: 73.4375 (74.2420)  time: 0.0284  data: 0.0002  max mem: 4132
[18:37:59.376395] Test:  [110/157]  eta: 0:00:01  testing_loss: 2.1552 (2.1589)  acc1: 21.8750 (22.5929)  acc5: 73.4375 (74.3102)  time: 0.0285  data: 0.0004  max mem: 4132
[18:37:59.664527] Test:  [120/157]  eta: 0:00:01  testing_loss: 2.1472 (2.1577)  acc1: 21.8750 (22.6111)  acc5: 73.4375 (74.1606)  time: 0.0287  data: 0.0003  max mem: 4132
[18:37:59.952228] Test:  [130/157]  eta: 0:00:00  testing_loss: 2.1514 (2.1579)  acc1: 23.4375 (22.6980)  acc5: 73.4375 (74.0816)  time: 0.0287  data: 0.0002  max mem: 4132
[18:38:00.236367] Test:  [140/157]  eta: 0:00:00  testing_loss: 2.1509 (2.1568)  acc1: 25.0000 (22.8834)  acc5: 73.4375 (74.1800)  time: 0.0285  data: 0.0001  max mem: 4132
[18:38:00.515682] Test:  [150/157]  eta: 0:00:00  testing_loss: 2.1428 (2.1559)  acc1: 25.0000 (23.0029)  acc5: 75.0000 (74.2860)  time: 0.0281  data: 0.0001  max mem: 4132
[18:38:00.665932] Test:  [156/157]  eta: 0:00:00  testing_loss: 2.1363 (2.1552)  acc1: 25.0000 (22.9800)  acc5: 75.0000 (74.3600)  time: 0.0269  data: 0.0001  max mem: 4132
[18:38:00.804465] Test: Total time: 0:00:05 (0.0333 s / it)
[18:38:00.804894] * Acc@1 22.980 Acc@5 74.360 loss 2.155
[18:38:00.805197] Accuracy of the network on the 10000 test images: 23.0%
[18:38:00.805394] Max accuracy: 25.08%
[18:38:01.141919] log_dir: ./output_dir
[18:38:01.958926] Epoch: [9]  [  0/781]  eta: 0:10:36  lr: 0.000249  training_loss: 0.3052 (0.3052)  classification_loss: 0.2270 (0.2270)  loss_mask: 0.0782 (0.0782)  time: 0.8153  data: 0.6170  max mem: 4132
[18:38:05.214095] Epoch: [9]  [ 20/781]  eta: 0:02:27  lr: 0.000249  training_loss: 0.3359 (0.3519)  classification_loss: 0.2259 (0.2257)  loss_mask: 0.1147 (0.1261)  time: 0.1627  data: 0.0002  max mem: 4132
[18:38:08.448266] Epoch: [9]  [ 40/781]  eta: 0:02:11  lr: 0.000249  training_loss: 0.3267 (0.3399)  classification_loss: 0.2243 (0.2249)  loss_mask: 0.1035 (0.1149)  time: 0.1616  data: 0.0002  max mem: 4132
[18:38:11.689429] Epoch: [9]  [ 60/781]  eta: 0:02:04  lr: 0.000249  training_loss: 0.3158 (0.3348)  classification_loss: 0.2233 (0.2245)  loss_mask: 0.0945 (0.1102)  time: 0.1620  data: 0.0002  max mem: 4132
[18:38:14.936305] Epoch: [9]  [ 80/781]  eta: 0:01:59  lr: 0.000249  training_loss: 0.3392 (0.3419)  classification_loss: 0.2251 (0.2247)  loss_mask: 0.1155 (0.1171)  time: 0.1623  data: 0.0002  max mem: 4132
[18:38:18.198550] Epoch: [9]  [100/781]  eta: 0:01:54  lr: 0.000249  training_loss: 0.3255 (0.3405)  classification_loss: 0.2248 (0.2249)  loss_mask: 0.1007 (0.1156)  time: 0.1630  data: 0.0002  max mem: 4132
[18:38:21.434406] Epoch: [9]  [120/781]  eta: 0:01:50  lr: 0.000249  training_loss: 0.3124 (0.3367)  classification_loss: 0.2248 (0.2249)  loss_mask: 0.0889 (0.1118)  time: 0.1617  data: 0.0003  max mem: 4132
[18:38:24.684828] Epoch: [9]  [140/781]  eta: 0:01:46  lr: 0.000249  training_loss: 0.3094 (0.3318)  classification_loss: 0.2236 (0.2248)  loss_mask: 0.0789 (0.1071)  time: 0.1625  data: 0.0002  max mem: 4132
[18:38:27.926041] Epoch: [9]  [160/781]  eta: 0:01:43  lr: 0.000249  training_loss: 0.3147 (0.3298)  classification_loss: 0.2245 (0.2248)  loss_mask: 0.0902 (0.1049)  time: 0.1620  data: 0.0002  max mem: 4132
[18:38:31.192936] Epoch: [9]  [180/781]  eta: 0:01:39  lr: 0.000249  training_loss: 0.2932 (0.3273)  classification_loss: 0.2230 (0.2247)  loss_mask: 0.0667 (0.1026)  time: 0.1632  data: 0.0007  max mem: 4132
[18:38:34.444079] Epoch: [9]  [200/781]  eta: 0:01:36  lr: 0.000249  training_loss: 0.3368 (0.3301)  classification_loss: 0.2233 (0.2246)  loss_mask: 0.1151 (0.1054)  time: 0.1625  data: 0.0002  max mem: 4132
[18:38:37.702542] Epoch: [9]  [220/781]  eta: 0:01:32  lr: 0.000249  training_loss: 0.3119 (0.3292)  classification_loss: 0.2252 (0.2247)  loss_mask: 0.0862 (0.1045)  time: 0.1628  data: 0.0003  max mem: 4132
[18:38:40.950709] Epoch: [9]  [240/781]  eta: 0:01:29  lr: 0.000249  training_loss: 0.3347 (0.3311)  classification_loss: 0.2263 (0.2248)  loss_mask: 0.1108 (0.1063)  time: 0.1623  data: 0.0002  max mem: 4132
[18:38:44.239037] Epoch: [9]  [260/781]  eta: 0:01:25  lr: 0.000249  training_loss: 0.3408 (0.3340)  classification_loss: 0.2244 (0.2248)  loss_mask: 0.1237 (0.1092)  time: 0.1643  data: 0.0003  max mem: 4132
[18:38:47.504493] Epoch: [9]  [280/781]  eta: 0:01:22  lr: 0.000249  training_loss: 0.3408 (0.3366)  classification_loss: 0.2255 (0.2248)  loss_mask: 0.1153 (0.1118)  time: 0.1632  data: 0.0002  max mem: 4132
[18:38:50.777696] Epoch: [9]  [300/781]  eta: 0:01:19  lr: 0.000249  training_loss: 0.3209 (0.3359)  classification_loss: 0.2242 (0.2248)  loss_mask: 0.1008 (0.1112)  time: 0.1636  data: 0.0003  max mem: 4132
[18:38:54.032477] Epoch: [9]  [320/781]  eta: 0:01:15  lr: 0.000249  training_loss: 0.3124 (0.3355)  classification_loss: 0.2239 (0.2247)  loss_mask: 0.0898 (0.1109)  time: 0.1627  data: 0.0002  max mem: 4132
[18:38:57.288424] Epoch: [9]  [340/781]  eta: 0:01:12  lr: 0.000249  training_loss: 0.3124 (0.3350)  classification_loss: 0.2250 (0.2247)  loss_mask: 0.0870 (0.1103)  time: 0.1627  data: 0.0002  max mem: 4132
[18:39:00.518730] Epoch: [9]  [360/781]  eta: 0:01:09  lr: 0.000249  training_loss: 0.3072 (0.3346)  classification_loss: 0.2252 (0.2247)  loss_mask: 0.0808 (0.1098)  time: 0.1614  data: 0.0002  max mem: 4132
[18:39:03.743605] Epoch: [9]  [380/781]  eta: 0:01:05  lr: 0.000249  training_loss: 0.3291 (0.3360)  classification_loss: 0.2253 (0.2248)  loss_mask: 0.1049 (0.1112)  time: 0.1611  data: 0.0002  max mem: 4132
[18:39:07.011798] Epoch: [9]  [400/781]  eta: 0:01:02  lr: 0.000249  training_loss: 0.3085 (0.3356)  classification_loss: 0.2275 (0.2250)  loss_mask: 0.0805 (0.1106)  time: 0.1633  data: 0.0002  max mem: 4132
[18:39:10.282064] Epoch: [9]  [420/781]  eta: 0:00:59  lr: 0.000249  training_loss: 0.3125 (0.3344)  classification_loss: 0.2255 (0.2250)  loss_mask: 0.0871 (0.1095)  time: 0.1634  data: 0.0002  max mem: 4132
[18:39:13.559967] Epoch: [9]  [440/781]  eta: 0:00:55  lr: 0.000249  training_loss: 0.2810 (0.3333)  classification_loss: 0.2260 (0.2250)  loss_mask: 0.0561 (0.1083)  time: 0.1638  data: 0.0002  max mem: 4132
[18:39:16.867852] Epoch: [9]  [460/781]  eta: 0:00:52  lr: 0.000249  training_loss: 0.3188 (0.3329)  classification_loss: 0.2226 (0.2249)  loss_mask: 0.0952 (0.1079)  time: 0.1653  data: 0.0002  max mem: 4132
[18:39:20.114363] Epoch: [9]  [480/781]  eta: 0:00:49  lr: 0.000249  training_loss: 0.3402 (0.3342)  classification_loss: 0.2257 (0.2250)  loss_mask: 0.1179 (0.1092)  time: 0.1622  data: 0.0002  max mem: 4132
[18:39:23.365781] Epoch: [9]  [500/781]  eta: 0:00:46  lr: 0.000249  training_loss: 0.3365 (0.3350)  classification_loss: 0.2244 (0.2250)  loss_mask: 0.1107 (0.1100)  time: 0.1625  data: 0.0003  max mem: 4132
[18:39:26.608742] Epoch: [9]  [520/781]  eta: 0:00:42  lr: 0.000249  training_loss: 0.3088 (0.3344)  classification_loss: 0.2249 (0.2250)  loss_mask: 0.0846 (0.1094)  time: 0.1621  data: 0.0002  max mem: 4132
[18:39:29.856249] Epoch: [9]  [540/781]  eta: 0:00:39  lr: 0.000249  training_loss: 0.3067 (0.3342)  classification_loss: 0.2249 (0.2251)  loss_mask: 0.0827 (0.1091)  time: 0.1623  data: 0.0003  max mem: 4132
[18:39:33.102972] Epoch: [9]  [560/781]  eta: 0:00:36  lr: 0.000248  training_loss: 0.3129 (0.3337)  classification_loss: 0.2228 (0.2250)  loss_mask: 0.0950 (0.1087)  time: 0.1622  data: 0.0002  max mem: 4132
[18:39:36.352960] Epoch: [9]  [580/781]  eta: 0:00:32  lr: 0.000248  training_loss: 0.2934 (0.3329)  classification_loss: 0.2247 (0.2250)  loss_mask: 0.0708 (0.1079)  time: 0.1624  data: 0.0002  max mem: 4132
[18:39:39.596555] Epoch: [9]  [600/781]  eta: 0:00:29  lr: 0.000248  training_loss: 0.3184 (0.3325)  classification_loss: 0.2253 (0.2250)  loss_mask: 0.0941 (0.1075)  time: 0.1620  data: 0.0002  max mem: 4132
[18:39:42.859441] Epoch: [9]  [620/781]  eta: 0:00:26  lr: 0.000248  training_loss: 0.3096 (0.3326)  classification_loss: 0.2257 (0.2250)  loss_mask: 0.0825 (0.1076)  time: 0.1631  data: 0.0003  max mem: 4132
[18:39:46.097680] Epoch: [9]  [640/781]  eta: 0:00:23  lr: 0.000248  training_loss: 0.2892 (0.3313)  classification_loss: 0.2227 (0.2250)  loss_mask: 0.0669 (0.1064)  time: 0.1618  data: 0.0003  max mem: 4132
[18:39:49.341232] Epoch: [9]  [660/781]  eta: 0:00:19  lr: 0.000248  training_loss: 0.3219 (0.3315)  classification_loss: 0.2228 (0.2249)  loss_mask: 0.0961 (0.1066)  time: 0.1621  data: 0.0002  max mem: 4132
[18:39:52.583336] Epoch: [9]  [680/781]  eta: 0:00:16  lr: 0.000248  training_loss: 0.2931 (0.3307)  classification_loss: 0.2237 (0.2248)  loss_mask: 0.0656 (0.1058)  time: 0.1620  data: 0.0002  max mem: 4132
[18:39:55.838264] Epoch: [9]  [700/781]  eta: 0:00:13  lr: 0.000248  training_loss: 0.2772 (0.3295)  classification_loss: 0.2226 (0.2248)  loss_mask: 0.0613 (0.1047)  time: 0.1627  data: 0.0004  max mem: 4132
[18:39:59.074490] Epoch: [9]  [720/781]  eta: 0:00:09  lr: 0.000248  training_loss: 0.2936 (0.3287)  classification_loss: 0.2234 (0.2248)  loss_mask: 0.0703 (0.1039)  time: 0.1617  data: 0.0002  max mem: 4132
[18:40:02.318561] Epoch: [9]  [740/781]  eta: 0:00:06  lr: 0.000248  training_loss: 0.2766 (0.3275)  classification_loss: 0.2235 (0.2248)  loss_mask: 0.0527 (0.1027)  time: 0.1621  data: 0.0002  max mem: 4132
[18:40:05.589583] Epoch: [9]  [760/781]  eta: 0:00:03  lr: 0.000248  training_loss: 0.3216 (0.3287)  classification_loss: 0.2241 (0.2247)  loss_mask: 0.1022 (0.1040)  time: 0.1634  data: 0.0002  max mem: 4132
[18:40:08.887219] Epoch: [9]  [780/781]  eta: 0:00:00  lr: 0.000248  training_loss: 0.3520 (0.3306)  classification_loss: 0.2219 (0.2247)  loss_mask: 0.1299 (0.1059)  time: 0.1648  data: 0.0002  max mem: 4132
[18:40:09.029337] Epoch: [9] Total time: 0:02:07 (0.1637 s / it)
[18:40:09.029815] Averaged stats: lr: 0.000248  training_loss: 0.3520 (0.3306)  classification_loss: 0.2219 (0.2247)  loss_mask: 0.1299 (0.1059)
[18:40:09.699316] Test:  [  0/157]  eta: 0:01:44  testing_loss: 2.0504 (2.0504)  acc1: 39.0625 (39.0625)  acc5: 79.6875 (79.6875)  time: 0.6657  data: 0.6318  max mem: 4132
[18:40:09.990546] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 2.1227 (2.1227)  acc1: 25.0000 (25.2841)  acc5: 70.3125 (73.4375)  time: 0.0868  data: 0.0576  max mem: 4132
[18:40:10.280116] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 2.1371 (2.1291)  acc1: 25.0000 (25.0000)  acc5: 70.3125 (73.1399)  time: 0.0289  data: 0.0002  max mem: 4132
[18:40:10.564269] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 2.1121 (2.1226)  acc1: 23.4375 (24.5464)  acc5: 73.4375 (73.4375)  time: 0.0286  data: 0.0002  max mem: 4132
[18:40:10.849584] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 2.1121 (2.1231)  acc1: 21.8750 (24.1616)  acc5: 71.8750 (73.2088)  time: 0.0284  data: 0.0002  max mem: 4132
[18:40:11.138444] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 2.1328 (2.1269)  acc1: 21.8750 (23.3762)  acc5: 71.8750 (72.9167)  time: 0.0286  data: 0.0004  max mem: 4132
[18:40:11.433169] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 2.1302 (2.1269)  acc1: 20.3125 (23.2582)  acc5: 71.8750 (72.6947)  time: 0.0290  data: 0.0006  max mem: 4132
[18:40:11.724105] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 2.1165 (2.1270)  acc1: 20.3125 (22.9313)  acc5: 71.8750 (72.7333)  time: 0.0291  data: 0.0004  max mem: 4132
[18:40:12.009562] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 2.1342 (2.1266)  acc1: 21.8750 (22.9938)  acc5: 73.4375 (72.8395)  time: 0.0287  data: 0.0002  max mem: 4132
[18:40:12.296632] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 2.1410 (2.1283)  acc1: 21.8750 (22.7507)  acc5: 75.0000 (72.5790)  time: 0.0285  data: 0.0002  max mem: 4132
[18:40:12.588308] Test:  [100/157]  eta: 0:00:01  testing_loss: 2.1206 (2.1287)  acc1: 20.3125 (22.7723)  acc5: 73.4375 (72.5248)  time: 0.0288  data: 0.0002  max mem: 4132
[18:40:12.880159] Test:  [110/157]  eta: 0:00:01  testing_loss: 2.1215 (2.1292)  acc1: 20.3125 (22.5225)  acc5: 73.4375 (72.5788)  time: 0.0290  data: 0.0003  max mem: 4132
[18:40:13.169713] Test:  [120/157]  eta: 0:00:01  testing_loss: 2.1104 (2.1273)  acc1: 18.7500 (22.3399)  acc5: 73.4375 (72.6498)  time: 0.0290  data: 0.0004  max mem: 4132
[18:40:13.451798] Test:  [130/157]  eta: 0:00:00  testing_loss: 2.1203 (2.1280)  acc1: 20.3125 (22.3402)  acc5: 73.4375 (72.7219)  time: 0.0285  data: 0.0003  max mem: 4132
[18:40:13.732300] Test:  [140/157]  eta: 0:00:00  testing_loss: 2.1208 (2.1268)  acc1: 23.4375 (22.5288)  acc5: 73.4375 (72.9277)  time: 0.0280  data: 0.0001  max mem: 4132
[18:40:14.011833] Test:  [150/157]  eta: 0:00:00  testing_loss: 2.1100 (2.1259)  acc1: 23.4375 (22.6304)  acc5: 73.4375 (72.9822)  time: 0.0279  data: 0.0001  max mem: 4132
[18:40:14.163302] Test:  [156/157]  eta: 0:00:00  testing_loss: 2.1063 (2.1252)  acc1: 21.8750 (22.5500)  acc5: 75.0000 (73.1400)  time: 0.0270  data: 0.0001  max mem: 4132
[18:40:14.313941] Test: Total time: 0:00:05 (0.0336 s / it)
[18:40:14.314554] * Acc@1 22.550 Acc@5 73.140 loss 2.125
[18:40:14.315052] Accuracy of the network on the 10000 test images: 22.6%
[18:40:14.315383] Max accuracy: 25.08%
[18:40:14.714854] log_dir: ./output_dir
[18:40:15.498546] Epoch: [10]  [  0/781]  eta: 0:10:10  lr: 0.000248  training_loss: 0.3421 (0.3421)  classification_loss: 0.2242 (0.2242)  loss_mask: 0.1179 (0.1179)  time: 0.7821  data: 0.6062  max mem: 4132
[18:40:18.777733] Epoch: [10]  [ 20/781]  eta: 0:02:27  lr: 0.000248  training_loss: 0.3052 (0.3163)  classification_loss: 0.2233 (0.2241)  loss_mask: 0.0837 (0.0922)  time: 0.1639  data: 0.0002  max mem: 4132
[18:40:22.010711] Epoch: [10]  [ 40/781]  eta: 0:02:11  lr: 0.000248  training_loss: 0.2847 (0.3057)  classification_loss: 0.2221 (0.2236)  loss_mask: 0.0607 (0.0821)  time: 0.1616  data: 0.0002  max mem: 4132
[18:40:25.272611] Epoch: [10]  [ 60/781]  eta: 0:02:04  lr: 0.000248  training_loss: 0.2952 (0.3044)  classification_loss: 0.2255 (0.2243)  loss_mask: 0.0698 (0.0801)  time: 0.1630  data: 0.0002  max mem: 4132
[18:40:28.506695] Epoch: [10]  [ 80/781]  eta: 0:01:59  lr: 0.000248  training_loss: 0.2731 (0.2976)  classification_loss: 0.2240 (0.2244)  loss_mask: 0.0477 (0.0732)  time: 0.1616  data: 0.0002  max mem: 4132
[18:40:31.777662] Epoch: [10]  [100/781]  eta: 0:01:54  lr: 0.000248  training_loss: 0.2782 (0.2942)  classification_loss: 0.2231 (0.2244)  loss_mask: 0.0510 (0.0699)  time: 0.1634  data: 0.0003  max mem: 4132
[18:40:35.015015] Epoch: [10]  [120/781]  eta: 0:01:50  lr: 0.000248  training_loss: 0.3065 (0.2976)  classification_loss: 0.2231 (0.2242)  loss_mask: 0.0776 (0.0734)  time: 0.1618  data: 0.0003  max mem: 4132
[18:40:38.256225] Epoch: [10]  [140/781]  eta: 0:01:46  lr: 0.000248  training_loss: 0.3375 (0.3032)  classification_loss: 0.2207 (0.2239)  loss_mask: 0.1154 (0.0793)  time: 0.1620  data: 0.0002  max mem: 4132
[18:40:41.489091] Epoch: [10]  [160/781]  eta: 0:01:43  lr: 0.000248  training_loss: 0.2917 (0.3043)  classification_loss: 0.2222 (0.2238)  loss_mask: 0.0701 (0.0805)  time: 0.1616  data: 0.0002  max mem: 4132
[18:40:44.745457] Epoch: [10]  [180/781]  eta: 0:01:39  lr: 0.000248  training_loss: 0.2929 (0.3058)  classification_loss: 0.2238 (0.2239)  loss_mask: 0.0642 (0.0820)  time: 0.1627  data: 0.0002  max mem: 4132
[18:40:47.988593] Epoch: [10]  [200/781]  eta: 0:01:36  lr: 0.000248  training_loss: 0.3148 (0.3080)  classification_loss: 0.2223 (0.2239)  loss_mask: 0.0933 (0.0841)  time: 0.1621  data: 0.0002  max mem: 4132
[18:40:51.231059] Epoch: [10]  [220/781]  eta: 0:01:32  lr: 0.000248  training_loss: 0.2895 (0.3087)  classification_loss: 0.2256 (0.2241)  loss_mask: 0.0605 (0.0846)  time: 0.1620  data: 0.0002  max mem: 4132
[18:40:54.475280] Epoch: [10]  [240/781]  eta: 0:01:29  lr: 0.000248  training_loss: 0.3214 (0.3097)  classification_loss: 0.2248 (0.2241)  loss_mask: 0.0984 (0.0855)  time: 0.1621  data: 0.0002  max mem: 4132
[18:40:57.715155] Epoch: [10]  [260/781]  eta: 0:01:25  lr: 0.000248  training_loss: 0.3314 (0.3115)  classification_loss: 0.2244 (0.2241)  loss_mask: 0.1089 (0.0874)  time: 0.1619  data: 0.0002  max mem: 4132
[18:41:00.958935] Epoch: [10]  [280/781]  eta: 0:01:22  lr: 0.000248  training_loss: 0.2830 (0.3110)  classification_loss: 0.2253 (0.2242)  loss_mask: 0.0602 (0.0868)  time: 0.1621  data: 0.0002  max mem: 4132
[18:41:04.212574] Epoch: [10]  [300/781]  eta: 0:01:19  lr: 0.000248  training_loss: 0.2928 (0.3102)  classification_loss: 0.2232 (0.2242)  loss_mask: 0.0687 (0.0860)  time: 0.1626  data: 0.0002  max mem: 4132
[18:41:07.474423] Epoch: [10]  [320/781]  eta: 0:01:15  lr: 0.000248  training_loss: 0.2835 (0.3091)  classification_loss: 0.2229 (0.2241)  loss_mask: 0.0586 (0.0850)  time: 0.1630  data: 0.0003  max mem: 4132
[18:41:10.715304] Epoch: [10]  [340/781]  eta: 0:01:12  lr: 0.000248  training_loss: 0.3034 (0.3098)  classification_loss: 0.2226 (0.2241)  loss_mask: 0.0786 (0.0857)  time: 0.1620  data: 0.0003  max mem: 4132
[18:41:13.963668] Epoch: [10]  [360/781]  eta: 0:01:09  lr: 0.000248  training_loss: 0.2876 (0.3083)  classification_loss: 0.2249 (0.2241)  loss_mask: 0.0588 (0.0842)  time: 0.1623  data: 0.0005  max mem: 4132
[18:41:17.225302] Epoch: [10]  [380/781]  eta: 0:01:05  lr: 0.000248  training_loss: 0.2745 (0.3069)  classification_loss: 0.2231 (0.2241)  loss_mask: 0.0505 (0.0828)  time: 0.1630  data: 0.0002  max mem: 4132
[18:41:20.473539] Epoch: [10]  [400/781]  eta: 0:01:02  lr: 0.000248  training_loss: 0.3077 (0.3097)  classification_loss: 0.2227 (0.2241)  loss_mask: 0.0885 (0.0856)  time: 0.1623  data: 0.0002  max mem: 4132
[18:41:23.733227] Epoch: [10]  [420/781]  eta: 0:00:59  lr: 0.000248  training_loss: 0.3173 (0.3106)  classification_loss: 0.2223 (0.2241)  loss_mask: 0.0901 (0.0865)  time: 0.1629  data: 0.0002  max mem: 4132
[18:41:26.997417] Epoch: [10]  [440/781]  eta: 0:00:55  lr: 0.000248  training_loss: 0.3064 (0.3105)  classification_loss: 0.2246 (0.2242)  loss_mask: 0.0804 (0.0864)  time: 0.1631  data: 0.0002  max mem: 4132
[18:41:30.239583] Epoch: [10]  [460/781]  eta: 0:00:52  lr: 0.000248  training_loss: 0.3154 (0.3113)  classification_loss: 0.2236 (0.2241)  loss_mask: 0.0923 (0.0872)  time: 0.1620  data: 0.0002  max mem: 4132
[18:41:33.488073] Epoch: [10]  [480/781]  eta: 0:00:49  lr: 0.000248  training_loss: 0.2901 (0.3110)  classification_loss: 0.2234 (0.2241)  loss_mask: 0.0660 (0.0869)  time: 0.1623  data: 0.0002  max mem: 4132
[18:41:36.728440] Epoch: [10]  [500/781]  eta: 0:00:45  lr: 0.000248  training_loss: 0.2807 (0.3100)  classification_loss: 0.2236 (0.2241)  loss_mask: 0.0578 (0.0859)  time: 0.1619  data: 0.0002  max mem: 4132
[18:41:39.979011] Epoch: [10]  [520/781]  eta: 0:00:42  lr: 0.000248  training_loss: 0.3307 (0.3111)  classification_loss: 0.2236 (0.2241)  loss_mask: 0.1060 (0.0870)  time: 0.1625  data: 0.0002  max mem: 4132
[18:41:43.225564] Epoch: [10]  [540/781]  eta: 0:00:39  lr: 0.000248  training_loss: 0.3483 (0.3129)  classification_loss: 0.2239 (0.2241)  loss_mask: 0.1240 (0.0887)  time: 0.1621  data: 0.0002  max mem: 4132
[18:41:46.474026] Epoch: [10]  [560/781]  eta: 0:00:36  lr: 0.000248  training_loss: 0.3173 (0.3133)  classification_loss: 0.2224 (0.2241)  loss_mask: 0.0951 (0.0892)  time: 0.1624  data: 0.0002  max mem: 4132
[18:41:49.711545] Epoch: [10]  [580/781]  eta: 0:00:32  lr: 0.000248  training_loss: 0.3010 (0.3131)  classification_loss: 0.2236 (0.2241)  loss_mask: 0.0747 (0.0890)  time: 0.1618  data: 0.0002  max mem: 4132
[18:41:53.007516] Epoch: [10]  [600/781]  eta: 0:00:29  lr: 0.000248  training_loss: 0.2720 (0.3123)  classification_loss: 0.2222 (0.2241)  loss_mask: 0.0498 (0.0882)  time: 0.1647  data: 0.0002  max mem: 4132
[18:41:56.375094] Epoch: [10]  [620/781]  eta: 0:00:26  lr: 0.000248  training_loss: 0.3524 (0.3140)  classification_loss: 0.2252 (0.2241)  loss_mask: 0.1226 (0.0899)  time: 0.1683  data: 0.0002  max mem: 4132
[18:41:59.682280] Epoch: [10]  [640/781]  eta: 0:00:23  lr: 0.000248  training_loss: 0.2992 (0.3139)  classification_loss: 0.2264 (0.2242)  loss_mask: 0.0671 (0.0897)  time: 0.1653  data: 0.0002  max mem: 4132

[18:42:02.970023] Epoch: [10]  [660/781]  eta: 0:00:19  lr: 0.000248  training_loss: 0.2782 (0.3130)  classification_loss: 0.2238 (0.2242)  loss_mask: 0.0540 (0.0888)  time: 0.1643  data: 0.0003  max mem: 4132
[18:42:06.205681] Epoch: [10]  [680/781]  eta: 0:00:16  lr: 0.000248  training_loss: 0.2803 (0.3120)  classification_loss: 0.2217 (0.2242)  loss_mask: 0.0502 (0.0878)  time: 0.1617  data: 0.0002  max mem: 4132
[18:42:09.441532] Epoch: [10]  [700/781]  eta: 0:00:13  lr: 0.000248  training_loss: 0.2676 (0.3109)  classification_loss: 0.2216 (0.2241)  loss_mask: 0.0434 (0.0868)  time: 0.1617  data: 0.0002  max mem: 4132
[18:42:12.732839] Epoch: [10]  [720/781]  eta: 0:00:09  lr: 0.000248  training_loss: 0.3012 (0.3113)  classification_loss: 0.2220 (0.2241)  loss_mask: 0.0755 (0.0872)  time: 0.1645  data: 0.0002  max mem: 4132
[18:42:15.988390] Epoch: [10]  [740/781]  eta: 0:00:06  lr: 0.000248  training_loss: 0.3038 (0.3115)  classification_loss: 0.2246 (0.2241)  loss_mask: 0.0768 (0.0874)  time: 0.1627  data: 0.0002  max mem: 4132
[18:42:19.225551] Epoch: [10]  [760/781]  eta: 0:00:03  lr: 0.000248  training_loss: 0.3349 (0.3125)  classification_loss: 0.2232 (0.2241)  loss_mask: 0.1117 (0.0884)  time: 0.1617  data: 0.0003  max mem: 4132
[18:42:22.490432] Epoch: [10]  [780/781]  eta: 0:00:00  lr: 0.000248  training_loss: 0.2747 (0.3117)  classification_loss: 0.2237 (0.2241)  loss_mask: 0.0496 (0.0876)  time: 0.1632  data: 0.0001  max mem: 4132
[18:42:22.637991] Epoch: [10] Total time: 0:02:07 (0.1638 s / it)
[18:42:22.638454] Averaged stats: lr: 0.000248  training_loss: 0.2747 (0.3117)  classification_loss: 0.2237 (0.2241)  loss_mask: 0.0496 (0.0876)
[18:42:24.692881] Test:  [  0/157]  eta: 0:01:34  testing_loss: 2.0236 (2.0236)  acc1: 39.0625 (39.0625)  acc5: 81.2500 (81.2500)  time: 0.6000  data: 0.5569  max mem: 4132
[18:42:24.985023] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 2.1083 (2.0994)  acc1: 25.0000 (24.5739)  acc5: 75.0000 (74.8580)  time: 0.0810  data: 0.0508  max mem: 4132
[18:42:25.266439] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 2.1083 (2.1062)  acc1: 23.4375 (24.3304)  acc5: 71.8750 (74.2560)  time: 0.0285  data: 0.0001  max mem: 4132
[18:42:25.551493] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 2.1009 (2.0991)  acc1: 23.4375 (24.3952)  acc5: 73.4375 (74.8488)  time: 0.0282  data: 0.0002  max mem: 4132
[18:42:25.836920] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 2.1018 (2.1001)  acc1: 23.4375 (24.3140)  acc5: 75.0000 (74.7332)  time: 0.0284  data: 0.0002  max mem: 4132
[18:42:26.119709] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 2.1128 (2.1045)  acc1: 21.8750 (23.5907)  acc5: 73.4375 (74.5711)  time: 0.0283  data: 0.0002  max mem: 4132
[18:42:26.402087] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 2.1021 (2.1048)  acc1: 21.8750 (23.2838)  acc5: 73.4375 (74.4877)  time: 0.0281  data: 0.0002  max mem: 4132
[18:42:26.685641] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 2.0965 (2.1056)  acc1: 21.8750 (23.1734)  acc5: 75.0000 (74.5819)  time: 0.0282  data: 0.0002  max mem: 4132
[18:42:26.967347] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 2.1100 (2.1055)  acc1: 23.4375 (23.3025)  acc5: 76.5625 (74.7878)  time: 0.0281  data: 0.0002  max mem: 4132
[18:42:27.248015] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 2.1131 (2.1069)  acc1: 21.8750 (22.9739)  acc5: 76.5625 (74.7424)  time: 0.0280  data: 0.0001  max mem: 4132
[18:42:27.529007] Test:  [100/157]  eta: 0:00:01  testing_loss: 2.1066 (2.1074)  acc1: 21.8750 (23.0353)  acc5: 75.0000 (74.6751)  time: 0.0280  data: 0.0001  max mem: 4132
[18:42:27.809701] Test:  [110/157]  eta: 0:00:01  testing_loss: 2.1059 (2.1076)  acc1: 20.3125 (22.9730)  acc5: 75.0000 (74.7185)  time: 0.0280  data: 0.0001  max mem: 4132
[18:42:28.090785] Test:  [120/157]  eta: 0:00:01  testing_loss: 2.0848 (2.1053)  acc1: 21.8750 (22.9081)  acc5: 76.5625 (74.8063)  time: 0.0280  data: 0.0001  max mem: 4132
[18:42:28.371961] Test:  [130/157]  eta: 0:00:00  testing_loss: 2.0924 (2.1057)  acc1: 25.0000 (23.0200)  acc5: 75.0000 (74.8330)  time: 0.0280  data: 0.0001  max mem: 4132
[18:42:28.652470] Test:  [140/157]  eta: 0:00:00  testing_loss: 2.0967 (2.1042)  acc1: 25.0000 (23.2602)  acc5: 75.0000 (74.9113)  time: 0.0280  data: 0.0001  max mem: 4132
[18:42:28.930837] Test:  [150/157]  eta: 0:00:00  testing_loss: 2.0875 (2.1031)  acc1: 25.0000 (23.3133)  acc5: 76.5625 (74.9690)  time: 0.0278  data: 0.0001  max mem: 4132
[18:42:29.080871] Test:  [156/157]  eta: 0:00:00  testing_loss: 2.0869 (2.1025)  acc1: 25.0000 (23.3200)  acc5: 76.5625 (75.0500)  time: 0.0269  data: 0.0001  max mem: 4132
[18:42:29.227604] Test: Total time: 0:00:05 (0.0327 s / it)
[18:42:29.228112] * Acc@1 23.320 Acc@5 75.050 loss 2.102
[18:42:29.228463] Accuracy of the network on the 10000 test images: 23.3%
[18:42:29.228663] Max accuracy: 25.08%
[18:42:29.608029] log_dir: ./output_dir
[18:42:30.427248] Epoch: [11]  [  0/781]  eta: 0:10:38  lr: 0.000248  training_loss: 0.2500 (0.2500)  classification_loss: 0.2223 (0.2223)  loss_mask: 0.0277 (0.0277)  time: 0.8176  data: 0.6417  max mem: 4132
[18:42:33.678098] Epoch: [11]  [ 20/781]  eta: 0:02:27  lr: 0.000248  training_loss: 0.2851 (0.2964)  classification_loss: 0.2226 (0.2227)  loss_mask: 0.0610 (0.0737)  time: 0.1624  data: 0.0002  max mem: 4132
[18:42:36.953599] Epoch: [11]  [ 40/781]  eta: 0:02:12  lr: 0.000248  training_loss: 0.3418 (0.3275)  classification_loss: 0.2220 (0.2225)  loss_mask: 0.1189 (0.1050)  time: 0.1637  data: 0.0002  max mem: 4132
[18:42:40.191946] Epoch: [11]  [ 60/781]  eta: 0:02:05  lr: 0.000247  training_loss: 0.2853 (0.3178)  classification_loss: 0.2227 (0.2227)  loss_mask: 0.0659 (0.0950)  time: 0.1618  data: 0.0001  max mem: 4132
[18:42:43.440374] Epoch: [11]  [ 80/781]  eta: 0:01:59  lr: 0.000247  training_loss: 0.3047 (0.3174)  classification_loss: 0.2218 (0.2228)  loss_mask: 0.0801 (0.0946)  time: 0.1623  data: 0.0002  max mem: 4132
[18:42:46.693381] Epoch: [11]  [100/781]  eta: 0:01:55  lr: 0.000247  training_loss: 0.2833 (0.3121)  classification_loss: 0.2225 (0.2228)  loss_mask: 0.0612 (0.0893)  time: 0.1626  data: 0.0002  max mem: 4132
[18:42:49.929995] Epoch: [11]  [120/781]  eta: 0:01:50  lr: 0.000247  training_loss: 0.2766 (0.3060)  classification_loss: 0.2211 (0.2226)  loss_mask: 0.0542 (0.0834)  time: 0.1618  data: 0.0002  max mem: 4132
[18:42:53.169995] Epoch: [11]  [140/781]  eta: 0:01:47  lr: 0.000247  training_loss: 0.2700 (0.3046)  classification_loss: 0.2235 (0.2226)  loss_mask: 0.0485 (0.0820)  time: 0.1619  data: 0.0002  max mem: 4132
[18:42:56.463192] Epoch: [11]  [160/781]  eta: 0:01:43  lr: 0.000247  training_loss: 0.2636 (0.3000)  classification_loss: 0.2236 (0.2228)  loss_mask: 0.0384 (0.0773)  time: 0.1646  data: 0.0002  max mem: 4132
[18:42:59.702538] Epoch: [11]  [180/781]  eta: 0:01:39  lr: 0.000247  training_loss: 0.2677 (0.2985)  classification_loss: 0.2216 (0.2227)  loss_mask: 0.0459 (0.0758)  time: 0.1619  data: 0.0002  max mem: 4132
[18:43:02.966853] Epoch: [11]  [200/781]  eta: 0:01:36  lr: 0.000247  training_loss: 0.3259 (0.3073)  classification_loss: 0.2210 (0.2226)  loss_mask: 0.1009 (0.0846)  time: 0.1631  data: 0.0002  max mem: 4132
[18:43:06.225380] Epoch: [11]  [220/781]  eta: 0:01:32  lr: 0.000247  training_loss: 0.2851 (0.3052)  classification_loss: 0.2240 (0.2228)  loss_mask: 0.0601 (0.0824)  time: 0.1628  data: 0.0003  max mem: 4132
[18:43:09.501277] Epoch: [11]  [240/781]  eta: 0:01:29  lr: 0.000247  training_loss: 0.2731 (0.3027)  classification_loss: 0.2237 (0.2229)  loss_mask: 0.0487 (0.0799)  time: 0.1637  data: 0.0002  max mem: 4132
[18:43:12.756131] Epoch: [11]  [260/781]  eta: 0:01:26  lr: 0.000247  training_loss: 0.2654 (0.3007)  classification_loss: 0.2214 (0.2228)  loss_mask: 0.0410 (0.0779)  time: 0.1626  data: 0.0002  max mem: 4132
[18:43:16.005565] Epoch: [11]  [280/781]  eta: 0:01:22  lr: 0.000247  training_loss: 0.2519 (0.2976)  classification_loss: 0.2214 (0.2227)  loss_mask: 0.0326 (0.0748)  time: 0.1624  data: 0.0003  max mem: 4132
[18:43:19.247239] Epoch: [11]  [300/781]  eta: 0:01:19  lr: 0.000247  training_loss: 0.2870 (0.2989)  classification_loss: 0.2221 (0.2227)  loss_mask: 0.0660 (0.0762)  time: 0.1620  data: 0.0002  max mem: 4132
[18:43:22.492752] Epoch: [11]  [320/781]  eta: 0:01:15  lr: 0.000247  training_loss: 0.2903 (0.2986)  classification_loss: 0.2213 (0.2226)  loss_mask: 0.0691 (0.0760)  time: 0.1622  data: 0.0002  max mem: 4132
[18:43:25.726047] Epoch: [11]  [340/781]  eta: 0:01:12  lr: 0.000247  training_loss: 0.2753 (0.2973)  classification_loss: 0.2209 (0.2225)  loss_mask: 0.0546 (0.0747)  time: 0.1616  data: 0.0003  max mem: 4132
[18:43:28.979128] Epoch: [11]  [360/781]  eta: 0:01:09  lr: 0.000247  training_loss: 0.3460 (0.3005)  classification_loss: 0.2203 (0.2225)  loss_mask: 0.1267 (0.0780)  time: 0.1626  data: 0.0003  max mem: 4132
[18:43:32.247875] Epoch: [11]  [380/781]  eta: 0:01:05  lr: 0.000247  training_loss: 0.3025 (0.3011)  classification_loss: 0.2219 (0.2225)  loss_mask: 0.0820 (0.0786)  time: 0.1633  data: 0.0003  max mem: 4132
[18:43:35.573847] Epoch: [11]  [400/781]  eta: 0:01:02  lr: 0.000247  training_loss: 0.3167 (0.3019)  classification_loss: 0.2241 (0.2226)  loss_mask: 0.0906 (0.0793)  time: 0.1662  data: 0.0003  max mem: 4132
[18:43:38.850534] Epoch: [11]  [420/781]  eta: 0:00:59  lr: 0.000247  training_loss: 0.2816 (0.3011)  classification_loss: 0.2235 (0.2227)  loss_mask: 0.0535 (0.0784)  time: 0.1638  data: 0.0002  max mem: 4132
[18:43:42.067101] Epoch: [11]  [440/781]  eta: 0:00:55  lr: 0.000247  training_loss: 0.2754 (0.3006)  classification_loss: 0.2220 (0.2227)  loss_mask: 0.0523 (0.0780)  time: 0.1607  data: 0.0002  max mem: 4132
[18:43:45.318070] Epoch: [11]  [460/781]  eta: 0:00:52  lr: 0.000247  training_loss: 0.2615 (0.2997)  classification_loss: 0.2226 (0.2226)  loss_mask: 0.0454 (0.0771)  time: 0.1625  data: 0.0003  max mem: 4132
[18:43:48.555620] Epoch: [11]  [480/781]  eta: 0:00:49  lr: 0.000247  training_loss: 0.2861 (0.2996)  classification_loss: 0.2243 (0.2227)  loss_mask: 0.0558 (0.0769)  time: 0.1618  data: 0.0002  max mem: 4132
[18:43:51.793182] Epoch: [11]  [500/781]  eta: 0:00:46  lr: 0.000247  training_loss: 0.2709 (0.2993)  classification_loss: 0.2219 (0.2227)  loss_mask: 0.0458 (0.0766)  time: 0.1618  data: 0.0002  max mem: 4132
[18:43:55.043914] Epoch: [11]  [520/781]  eta: 0:00:42  lr: 0.000247  training_loss: 0.3071 (0.3016)  classification_loss: 0.2236 (0.2228)  loss_mask: 0.0835 (0.0788)  time: 0.1625  data: 0.0002  max mem: 4132
[18:43:58.297328] Epoch: [11]  [540/781]  eta: 0:00:39  lr: 0.000247  training_loss: 0.3153 (0.3023)  classification_loss: 0.2228 (0.2228)  loss_mask: 0.0921 (0.0796)  time: 0.1626  data: 0.0002  max mem: 4132
[18:44:01.542323] Epoch: [11]  [560/781]  eta: 0:00:36  lr: 0.000247  training_loss: 0.2885 (0.3024)  classification_loss: 0.2220 (0.2227)  loss_mask: 0.0676 (0.0797)  time: 0.1622  data: 0.0002  max mem: 4132
[18:44:04.798658] Epoch: [11]  [580/781]  eta: 0:00:32  lr: 0.000247  training_loss: 0.2919 (0.3029)  classification_loss: 0.2210 (0.2227)  loss_mask: 0.0720 (0.0802)  time: 0.1627  data: 0.0002  max mem: 4132
[18:44:08.034774] Epoch: [11]  [600/781]  eta: 0:00:29  lr: 0.000247  training_loss: 0.3023 (0.3039)  classification_loss: 0.2237 (0.2227)  loss_mask: 0.0780 (0.0812)  time: 0.1617  data: 0.0003  max mem: 4132
[18:44:11.255342] Epoch: [11]  [620/781]  eta: 0:00:26  lr: 0.000247  training_loss: 0.2782 (0.3034)  classification_loss: 0.2246 (0.2228)  loss_mask: 0.0547 (0.0806)  time: 0.1610  data: 0.0002  max mem: 4132
[18:44:14.505418] Epoch: [11]  [640/781]  eta: 0:00:23  lr: 0.000247  training_loss: 0.2720 (0.3027)  classification_loss: 0.2229 (0.2228)  loss_mask: 0.0476 (0.0799)  time: 0.1624  data: 0.0002  max mem: 4132
[18:44:17.738556] Epoch: [11]  [660/781]  eta: 0:00:19  lr: 0.000247  training_loss: 0.2904 (0.3026)  classification_loss: 0.2215 (0.2228)  loss_mask: 0.0663 (0.0798)  time: 0.1616  data: 0.0002  max mem: 4132
[18:44:20.973019] Epoch: [11]  [680/781]  eta: 0:00:16  lr: 0.000247  training_loss: 0.2846 (0.3025)  classification_loss: 0.2210 (0.2228)  loss_mask: 0.0610 (0.0797)  time: 0.1616  data: 0.0002  max mem: 4132
[18:44:24.221117] Epoch: [11]  [700/781]  eta: 0:00:13  lr: 0.000247  training_loss: 0.3217 (0.3035)  classification_loss: 0.2219 (0.2227)  loss_mask: 0.1009 (0.0807)  time: 0.1623  data: 0.0002  max mem: 4132
[18:44:27.458914] Epoch: [11]  [720/781]  eta: 0:00:09  lr: 0.000247  training_loss: 0.2851 (0.3036)  classification_loss: 0.2251 (0.2228)  loss_mask: 0.0632 (0.0808)  time: 0.1618  data: 0.0002  max mem: 4132
[18:44:30.706328] Epoch: [11]  [740/781]  eta: 0:00:06  lr: 0.000247  training_loss: 0.2788 (0.3031)  classification_loss: 0.2224 (0.2228)  loss_mask: 0.0553 (0.0803)  time: 0.1623  data: 0.0003  max mem: 4132
[18:44:33.949075] Epoch: [11]  [760/781]  eta: 0:00:03  lr: 0.000247  training_loss: 0.2833 (0.3028)  classification_loss: 0.2227 (0.2228)  loss_mask: 0.0606 (0.0800)  time: 0.1621  data: 0.0003  max mem: 4132
[18:44:37.176360] Epoch: [11]  [780/781]  eta: 0:00:00  lr: 0.000247  training_loss: 0.2743 (0.3022)  classification_loss: 0.2217 (0.2228)  loss_mask: 0.0516 (0.0794)  time: 0.1613  data: 0.0002  max mem: 4132
[18:44:37.317393] Epoch: [11] Total time: 0:02:07 (0.1635 s / it)
[18:44:37.317859] Averaged stats: lr: 0.000247  training_loss: 0.2743 (0.3022)  classification_loss: 0.2217 (0.2228)  loss_mask: 0.0516 (0.0794)
[18:44:37.909391] Test:  [  0/157]  eta: 0:01:31  testing_loss: 1.9915 (1.9915)  acc1: 35.9375 (35.9375)  acc5: 78.1250 (78.1250)  time: 0.5859  data: 0.5566  max mem: 4132
[18:44:38.224593] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 2.0820 (2.0692)  acc1: 28.1250 (27.9830)  acc5: 75.0000 (76.2784)  time: 0.0817  data: 0.0520  max mem: 4132
[18:44:38.512310] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 2.0804 (2.0748)  acc1: 26.5625 (27.1577)  acc5: 75.0000 (76.0417)  time: 0.0300  data: 0.0008  max mem: 4132
[18:44:38.796973] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 2.0684 (2.0699)  acc1: 26.5625 (26.7137)  acc5: 76.5625 (76.5625)  time: 0.0285  data: 0.0001  max mem: 4132
[18:44:39.080634] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 2.0798 (2.0704)  acc1: 26.5625 (26.3338)  acc5: 76.5625 (76.5625)  time: 0.0283  data: 0.0002  max mem: 4132
[18:44:39.368639] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 2.0840 (2.0760)  acc1: 25.0000 (25.8885)  acc5: 76.5625 (76.2561)  time: 0.0285  data: 0.0002  max mem: 4132
[18:44:39.653052] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 2.0822 (2.0766)  acc1: 23.4375 (25.6916)  acc5: 76.5625 (76.5113)  time: 0.0285  data: 0.0002  max mem: 4132
[18:44:39.941036] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 2.0705 (2.0769)  acc1: 23.4375 (25.3741)  acc5: 78.1250 (76.6725)  time: 0.0285  data: 0.0002  max mem: 4132
[18:44:40.228448] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 2.0681 (2.0780)  acc1: 23.4375 (25.0965)  acc5: 78.1250 (76.8133)  time: 0.0287  data: 0.0002  max mem: 4132
[18:44:40.512287] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 2.0958 (2.0806)  acc1: 21.8750 (24.9657)  acc5: 75.0000 (76.5797)  time: 0.0284  data: 0.0002  max mem: 4132
[18:44:40.798359] Test:  [100/157]  eta: 0:00:01  testing_loss: 2.0957 (2.0810)  acc1: 20.3125 (24.7525)  acc5: 75.0000 (76.4542)  time: 0.0284  data: 0.0002  max mem: 4132
[18:44:41.081838] Test:  [110/157]  eta: 0:00:01  testing_loss: 2.0818 (2.0812)  acc1: 23.4375 (24.7044)  acc5: 75.0000 (76.3936)  time: 0.0284  data: 0.0002  max mem: 4132
[18:44:41.367575] Test:  [120/157]  eta: 0:00:01  testing_loss: 2.0746 (2.0785)  acc1: 23.4375 (24.8580)  acc5: 76.5625 (76.5238)  time: 0.0283  data: 0.0002  max mem: 4132
[18:44:41.649511] Test:  [130/157]  eta: 0:00:00  testing_loss: 2.0618 (2.0792)  acc1: 25.0000 (24.8092)  acc5: 78.1250 (76.5267)  time: 0.0283  data: 0.0001  max mem: 4132
[18:44:41.932679] Test:  [140/157]  eta: 0:00:00  testing_loss: 2.0618 (2.0775)  acc1: 25.0000 (24.9778)  acc5: 78.1250 (76.5847)  time: 0.0281  data: 0.0002  max mem: 4132
[18:44:42.211946] Test:  [150/157]  eta: 0:00:00  testing_loss: 2.0581 (2.0767)  acc1: 26.5625 (25.0000)  acc5: 78.1250 (76.6867)  time: 0.0280  data: 0.0001  max mem: 4132
[18:44:42.363357] Test:  [156/157]  eta: 0:00:00  testing_loss: 2.0545 (2.0758)  acc1: 23.4375 (24.9700)  acc5: 78.1250 (76.7300)  time: 0.0270  data: 0.0001  max mem: 4132
[18:44:42.496438] Test: Total time: 0:00:05 (0.0330 s / it)
[18:44:42.496927] * Acc@1 24.970 Acc@5 76.730 loss 2.076
[18:44:42.497251] Accuracy of the network on the 10000 test images: 25.0%
[18:44:42.497440] Max accuracy: 25.08%
[18:44:42.673510] log_dir: ./output_dir
[18:44:43.496520] Epoch: [12]  [  0/781]  eta: 0:10:41  lr: 0.000247  training_loss: 0.2612 (0.2612)  classification_loss: 0.2228 (0.2228)  loss_mask: 0.0384 (0.0384)  time: 0.8212  data: 0.6218  max mem: 4132
[18:44:46.758520] Epoch: [12]  [ 20/781]  eta: 0:02:27  lr: 0.000247  training_loss: 0.2703 (0.2777)  classification_loss: 0.2205 (0.2215)  loss_mask: 0.0502 (0.0562)  time: 0.1630  data: 0.0002  max mem: 4132
[18:44:49.996532] Epoch: [12]  [ 40/781]  eta: 0:02:12  lr: 0.000247  training_loss: 0.2580 (0.2725)  classification_loss: 0.2212 (0.2221)  loss_mask: 0.0341 (0.0504)  time: 0.1618  data: 0.0003  max mem: 4132
[18:44:53.244739] Epoch: [12]  [ 60/781]  eta: 0:02:04  lr: 0.000247  training_loss: 0.2676 (0.2726)  classification_loss: 0.2201 (0.2220)  loss_mask: 0.0486 (0.0505)  time: 0.1623  data: 0.0003  max mem: 4132
[18:44:56.490105] Epoch: [12]  [ 80/781]  eta: 0:01:59  lr: 0.000247  training_loss: 0.2768 (0.2759)  classification_loss: 0.2230 (0.2225)  loss_mask: 0.0457 (0.0534)  time: 0.1622  data: 0.0003  max mem: 4132
[18:44:59.730072] Epoch: [12]  [100/781]  eta: 0:01:54  lr: 0.000247  training_loss: 0.2727 (0.2767)  classification_loss: 0.2225 (0.2226)  loss_mask: 0.0463 (0.0540)  time: 0.1619  data: 0.0003  max mem: 4132
[18:45:02.989585] Epoch: [12]  [120/781]  eta: 0:01:50  lr: 0.000247  training_loss: 0.3095 (0.2846)  classification_loss: 0.2204 (0.2223)  loss_mask: 0.0883 (0.0624)  time: 0.1629  data: 0.0002  max mem: 4132
[18:45:06.293321] Epoch: [12]  [140/781]  eta: 0:01:47  lr: 0.000247  training_loss: 0.2901 (0.2878)  classification_loss: 0.2213 (0.2222)  loss_mask: 0.0697 (0.0656)  time: 0.1651  data: 0.0003  max mem: 4132
[18:45:09.530600] Epoch: [12]  [160/781]  eta: 0:01:43  lr: 0.000246  training_loss: 0.3097 (0.2915)  classification_loss: 0.2232 (0.2223)  loss_mask: 0.0854 (0.0693)  time: 0.1618  data: 0.0002  max mem: 4132
[18:45:12.761175] Epoch: [12]  [180/781]  eta: 0:01:39  lr: 0.000246  training_loss: 0.2910 (0.2917)  classification_loss: 0.2189 (0.2220)  loss_mask: 0.0615 (0.0697)  time: 0.1614  data: 0.0003  max mem: 4132
[18:45:16.019324] Epoch: [12]  [200/781]  eta: 0:01:36  lr: 0.000246  training_loss: 0.2758 (0.2918)  classification_loss: 0.2217 (0.2220)  loss_mask: 0.0586 (0.0698)  time: 0.1628  data: 0.0004  max mem: 4132
[18:45:19.240585] Epoch: [12]  [220/781]  eta: 0:01:32  lr: 0.000246  training_loss: 0.2805 (0.2920)  classification_loss: 0.2211 (0.2221)  loss_mask: 0.0593 (0.0699)  time: 0.1609  data: 0.0002  max mem: 4132
[18:45:22.480357] Epoch: [12]  [240/781]  eta: 0:01:29  lr: 0.000246  training_loss: 0.2724 (0.2909)  classification_loss: 0.2226 (0.2221)  loss_mask: 0.0483 (0.0687)  time: 0.1619  data: 0.0002  max mem: 4132
[18:45:25.752508] Epoch: [12]  [260/781]  eta: 0:01:25  lr: 0.000246  training_loss: 0.2945 (0.2947)  classification_loss: 0.2199 (0.2221)  loss_mask: 0.0775 (0.0727)  time: 0.1635  data: 0.0002  max mem: 4132
[18:45:29.034593] Epoch: [12]  [280/781]  eta: 0:01:22  lr: 0.000246  training_loss: 0.3320 (0.2984)  classification_loss: 0.2221 (0.2221)  loss_mask: 0.1036 (0.0763)  time: 0.1640  data: 0.0002  max mem: 4132
[18:45:32.287686] Epoch: [12]  [300/781]  eta: 0:01:19  lr: 0.000246  training_loss: 0.2805 (0.2976)  classification_loss: 0.2222 (0.2221)  loss_mask: 0.0555 (0.0755)  time: 0.1626  data: 0.0002  max mem: 4132
[18:45:35.516621] Epoch: [12]  [320/781]  eta: 0:01:15  lr: 0.000246  training_loss: 0.2679 (0.2964)  classification_loss: 0.2195 (0.2220)  loss_mask: 0.0452 (0.0744)  time: 0.1613  data: 0.0002  max mem: 4132
[18:45:38.767572] Epoch: [12]  [340/781]  eta: 0:01:12  lr: 0.000246  training_loss: 0.2812 (0.2961)  classification_loss: 0.2214 (0.2219)  loss_mask: 0.0601 (0.0742)  time: 0.1625  data: 0.0003  max mem: 4132
[18:45:42.013067] Epoch: [12]  [360/781]  eta: 0:01:09  lr: 0.000246  training_loss: 0.5209 (0.4313)  classification_loss: 0.2510 (0.2238)  loss_mask: 0.2810 (0.2075)  time: 0.1622  data: 0.0003  max mem: 4132
[18:45:45.262583] Epoch: [12]  [380/781]  eta: 0:01:05  lr: 0.000246  training_loss: 3.2425 (0.6188)  classification_loss: 0.2583 (0.2261)  loss_mask: 2.9614 (0.3927)  time: 0.1624  data: 0.0002  max mem: 4132
[18:45:48.508255] Epoch: [12]  [400/781]  eta: 0:01:02  lr: 0.000246  training_loss: 1.8089 (0.6794)  classification_loss: 0.2318 (0.2265)  loss_mask: 1.5758 (0.4529)  time: 0.1622  data: 0.0003  max mem: 4132
[18:45:51.731909] Epoch: [12]  [420/781]  eta: 0:00:59  lr: 0.000246  training_loss: 1.6774 (0.7275)  classification_loss: 0.2305 (0.2267)  loss_mask: 1.4463 (0.5008)  time: 0.1611  data: 0.0002  max mem: 4132
[18:45:55.005798] Epoch: [12]  [440/781]  eta: 0:00:55  lr: 0.000246  training_loss: 1.6682 (0.7704)  classification_loss: 0.2305 (0.2269)  loss_mask: 1.4367 (0.5435)  time: 0.1636  data: 0.0002  max mem: 4132
[18:45:58.266484] Epoch: [12]  [460/781]  eta: 0:00:52  lr: 0.000246  training_loss: 1.6803 (0.8099)  classification_loss: 0.2302 (0.2270)  loss_mask: 1.4476 (0.5829)  time: 0.1629  data: 0.0003  max mem: 4132
[18:46:01.528700] Epoch: [12]  [480/781]  eta: 0:00:49  lr: 0.000246  training_loss: 1.6620 (0.8457)  classification_loss: 0.2301 (0.2272)  loss_mask: 1.4312 (0.6185)  time: 0.1630  data: 0.0003  max mem: 4132
[18:46:04.816932] Epoch: [12]  [500/781]  eta: 0:00:46  lr: 0.000246  training_loss: 1.6749 (0.8789)  classification_loss: 0.2299 (0.2273)  loss_mask: 1.4444 (0.6516)  time: 0.1643  data: 0.0002  max mem: 4132
[18:46:08.044829] Epoch: [12]  [520/781]  eta: 0:00:42  lr: 0.000246  training_loss: 1.6699 (0.9093)  classification_loss: 0.2302 (0.2274)  loss_mask: 1.4419 (0.6819)  time: 0.1613  data: 0.0003  max mem: 4132
[18:46:11.278837] Epoch: [12]  [540/781]  eta: 0:00:39  lr: 0.000246  training_loss: 1.6647 (0.9373)  classification_loss: 0.2299 (0.2275)  loss_mask: 1.4341 (0.7098)  time: 0.1616  data: 0.0003  max mem: 4132
[18:46:14.507283] Epoch: [12]  [560/781]  eta: 0:00:36  lr: 0.000246  training_loss: 1.6707 (0.9634)  classification_loss: 0.2298 (0.2276)  loss_mask: 1.4402 (0.7358)  time: 0.1614  data: 0.0003  max mem: 4132
[18:46:17.756070] Epoch: [12]  [580/781]  eta: 0:00:32  lr: 0.000246  training_loss: 1.6697 (0.9879)  classification_loss: 0.2305 (0.2277)  loss_mask: 1.4394 (0.7603)  time: 0.1623  data: 0.0002  max mem: 4132
[18:46:21.052859] Epoch: [12]  [600/781]  eta: 0:00:29  lr: 0.000246  training_loss: 1.6744 (1.0110)  classification_loss: 0.2299 (0.2278)  loss_mask: 1.4450 (0.7832)  time: 0.1648  data: 0.0002  max mem: 4132
[18:46:24.354671] Epoch: [12]  [620/781]  eta: 0:00:26  lr: 0.000246  training_loss: 1.6702 (1.0324)  classification_loss: 0.2299 (0.2278)  loss_mask: 1.4410 (0.8046)  time: 0.1650  data: 0.0002  max mem: 4132
[18:46:27.611083] Epoch: [12]  [640/781]  eta: 0:00:23  lr: 0.000246  training_loss: 1.6728 (1.0525)  classification_loss: 0.2297 (0.2279)  loss_mask: 1.4429 (0.8246)  time: 0.1627  data: 0.0002  max mem: 4132
[18:46:30.892696] Epoch: [12]  [660/781]  eta: 0:00:19  lr: 0.000246  training_loss: 1.6599 (1.0711)  classification_loss: 0.2297 (0.2279)  loss_mask: 1.4318 (0.8432)  time: 0.1640  data: 0.0002  max mem: 4132
[18:46:34.130150] Epoch: [12]  [680/781]  eta: 0:00:16  lr: 0.000246  training_loss: 1.6717 (1.0890)  classification_loss: 0.2296 (0.2280)  loss_mask: 1.4437 (0.8610)  time: 0.1618  data: 0.0002  max mem: 4132
[18:46:37.382864] Epoch: [12]  [700/781]  eta: 0:00:13  lr: 0.000246  training_loss: 1.6684 (1.1055)  classification_loss: 0.2296 (0.2280)  loss_mask: 1.4370 (0.8774)  time: 0.1626  data: 0.0002  max mem: 4132
[18:46:40.636524] Epoch: [12]  [720/781]  eta: 0:00:09  lr: 0.000246  training_loss: 1.6720 (1.1212)  classification_loss: 0.2297 (0.2281)  loss_mask: 1.4409 (0.8931)  time: 0.1626  data: 0.0002  max mem: 4132
[18:46:43.903462] Epoch: [12]  [740/781]  eta: 0:00:06  lr: 0.000246  training_loss: 1.6715 (1.1360)  classification_loss: 0.2302 (0.2282)  loss_mask: 1.4424 (0.9079)  time: 0.1633  data: 0.0002  max mem: 4132
[18:46:47.147532] Epoch: [12]  [760/781]  eta: 0:00:03  lr: 0.000246  training_loss: 1.6590 (1.1499)  classification_loss: 0.2288 (0.2282)  loss_mask: 1.4280 (0.9217)  time: 0.1621  data: 0.0003  max mem: 4132
[18:46:50.367492] Epoch: [12]  [780/781]  eta: 0:00:00  lr: 0.000246  training_loss: 1.6602 (1.1629)  classification_loss: 0.2299 (0.2282)  loss_mask: 1.4321 (0.9346)  time: 0.1609  data: 0.0002  max mem: 4132
[18:46:50.520983] Epoch: [12] Total time: 0:02:07 (0.1637 s / it)
[18:46:50.522352] Averaged stats: lr: 0.000246  training_loss: 1.6602 (1.1629)  classification_loss: 0.2299 (0.2282)  loss_mask: 1.4321 (0.9346)
[18:46:51.182003] Test:  [  0/157]  eta: 0:01:42  testing_loss: 2.2711 (2.2711)  acc1: 10.9375 (10.9375)  acc5: 65.6250 (65.6250)  time: 0.6530  data: 0.6235  max mem: 4132
[18:46:51.468672] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 2.2845 (2.2841)  acc1: 9.3750 (9.3750)  acc5: 60.9375 (61.6477)  time: 0.0853  data: 0.0568  max mem: 4132
[18:46:51.754162] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 2.2847 (2.2842)  acc1: 9.3750 (9.5982)  acc5: 60.9375 (61.8304)  time: 0.0285  data: 0.0003  max mem: 4132
[18:46:52.048107] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 2.2831 (2.2832)  acc1: 9.3750 (9.6774)  acc5: 60.9375 (61.9960)  time: 0.0288  data: 0.0005  max mem: 4132
[18:46:52.332491] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 2.2814 (2.2842)  acc1: 7.8125 (9.3369)  acc5: 60.9375 (61.6235)  time: 0.0288  data: 0.0004  max mem: 4132
[18:46:52.616123] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 2.2808 (2.2839)  acc1: 7.8125 (9.7426)  acc5: 59.3750 (61.7341)  time: 0.0283  data: 0.0001  max mem: 4132
[18:46:52.896880] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 2.2807 (2.2835)  acc1: 10.9375 (9.8617)  acc5: 64.0625 (62.1414)  time: 0.0281  data: 0.0001  max mem: 4132
[18:46:53.177755] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 2.2816 (2.2833)  acc1: 7.8125 (9.8592)  acc5: 64.0625 (62.2799)  time: 0.0280  data: 0.0001  max mem: 4132
[18:46:53.459118] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 2.2825 (2.2832)  acc1: 9.3750 (9.9537)  acc5: 62.5000 (62.3071)  time: 0.0280  data: 0.0001  max mem: 4132
[18:46:53.740656] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 2.2806 (2.2832)  acc1: 9.3750 (10.1648)  acc5: 62.5000 (62.2081)  time: 0.0280  data: 0.0001  max mem: 4132
[18:46:54.021895] Test:  [100/157]  eta: 0:00:01  testing_loss: 2.2773 (2.2831)  acc1: 10.9375 (10.1640)  acc5: 62.5000 (62.2989)  time: 0.0280  data: 0.0001  max mem: 4132
[18:46:54.302645] Test:  [110/157]  eta: 0:00:01  testing_loss: 2.2788 (2.2834)  acc1: 9.3750 (10.0648)  acc5: 64.0625 (62.4437)  time: 0.0280  data: 0.0001  max mem: 4132
[18:46:54.585355] Test:  [120/157]  eta: 0:00:01  testing_loss: 2.2868 (2.2835)  acc1: 7.8125 (10.0207)  acc5: 62.5000 (62.3709)  time: 0.0281  data: 0.0002  max mem: 4132
[18:46:54.871234] Test:  [130/157]  eta: 0:00:00  testing_loss: 2.2828 (2.2838)  acc1: 9.3750 (9.9714)  acc5: 60.9375 (62.1899)  time: 0.0282  data: 0.0002  max mem: 4132
[18:46:55.159520] Test:  [140/157]  eta: 0:00:00  testing_loss: 2.2828 (2.2836)  acc1: 9.3750 (9.9180)  acc5: 59.3750 (62.1676)  time: 0.0285  data: 0.0003  max mem: 4132
[18:46:55.437750] Test:  [150/157]  eta: 0:00:00  testing_loss: 2.2785 (2.2833)  acc1: 9.3750 (9.9752)  acc5: 59.3750 (62.2724)  time: 0.0282  data: 0.0003  max mem: 4132
[18:46:55.587291] Test:  [156/157]  eta: 0:00:00  testing_loss: 2.2794 (2.2833)  acc1: 9.3750 (10.0000)  acc5: 64.0625 (62.3700)  time: 0.0268  data: 0.0001  max mem: 4132
[18:46:55.725972] Test: Total time: 0:00:05 (0.0331 s / it)
[18:46:55.726438] * Acc@1 10.000 Acc@5 62.370 loss 2.283
[18:46:55.726765] Accuracy of the network on the 10000 test images: 10.0%
[18:46:55.726949] Max accuracy: 25.08%
[18:46:56.348117] log_dir: ./output_dir
[18:46:57.169272] Epoch: [13]  [  0/781]  eta: 0:10:40  lr: 0.000246  training_loss: 1.6897 (1.6897)  classification_loss: 0.2294 (0.2294)  loss_mask: 1.4603 (1.4603)  time: 0.8195  data: 0.6483  max mem: 4132
[18:47:00.395021] Epoch: [13]  [ 20/781]  eta: 0:02:26  lr: 0.000246  training_loss: 1.6580 (1.6692)  classification_loss: 0.2296 (0.2297)  loss_mask: 1.4287 (1.4395)  time: 0.1612  data: 0.0003  max mem: 4132
[18:47:03.684433] Epoch: [13]  [ 40/781]  eta: 0:02:12  lr: 0.000246  training_loss: 1.6671 (1.6674)  classification_loss: 0.2290 (0.2294)  loss_mask: 1.4373 (1.4380)  time: 0.1644  data: 0.0002  max mem: 4132
[18:47:06.921699] Epoch: [13]  [ 60/781]  eta: 0:02:04  lr: 0.000246  training_loss: 1.6588 (1.6659)  classification_loss: 0.2291 (0.2294)  loss_mask: 1.4290 (1.4365)  time: 0.1618  data: 0.0002  max mem: 4132
[18:47:10.166876] Epoch: [13]  [ 80/781]  eta: 0:01:59  lr: 0.000246  training_loss: 1.6599 (1.6653)  classification_loss: 0.2292 (0.2294)  loss_mask: 1.4310 (1.4360)  time: 0.1622  data: 0.0002  max mem: 4132
[18:47:13.412399] Epoch: [13]  [100/781]  eta: 0:01:54  lr: 0.000246  training_loss: 1.6391 (1.6619)  classification_loss: 0.2291 (0.2293)  loss_mask: 1.4111 (1.4326)  time: 0.1622  data: 0.0003  max mem: 4132
[18:47:16.654690] Epoch: [13]  [120/781]  eta: 0:01:50  lr: 0.000246  training_loss: 1.6604 (1.6618)  classification_loss: 0.2289 (0.2293)  loss_mask: 1.4328 (1.4326)  time: 0.1620  data: 0.0003  max mem: 4132
[18:47:19.866589] Epoch: [13]  [140/781]  eta: 0:01:46  lr: 0.000245  training_loss: 1.6484 (1.6604)  classification_loss: 0.2282 (0.2291)  loss_mask: 1.4212 (1.4312)  time: 0.1605  data: 0.0002  max mem: 4132
[18:47:23.093683] Epoch: [13]  [160/781]  eta: 0:01:43  lr: 0.000245  training_loss: 1.6291 (1.6573)  classification_loss: 0.2284 (0.2291)  loss_mask: 1.3998 (1.4282)  time: 0.1613  data: 0.0002  max mem: 4132
[18:47:26.326503] Epoch: [13]  [180/781]  eta: 0:01:39  lr: 0.000245  training_loss: 1.6203 (1.6534)  classification_loss: 0.2279 (0.2290)  loss_mask: 1.3912 (1.4244)  time: 0.1616  data: 0.0002  max mem: 4132
[18:47:29.564860] Epoch: [13]  [200/781]  eta: 0:01:35  lr: 0.000245  training_loss: 1.5945 (1.6473)  classification_loss: 0.2274 (0.2288)  loss_mask: 1.3661 (1.4185)  time: 0.1618  data: 0.0003  max mem: 4132
[18:47:32.812884] Epoch: [13]  [220/781]  eta: 0:01:32  lr: 0.000245  training_loss: 1.5278 (1.6372)  classification_loss: 0.2275 (0.2288)  loss_mask: 1.2972 (1.4084)  time: 0.1623  data: 0.0002  max mem: 4132
[18:47:36.068087] Epoch: [13]  [240/781]  eta: 0:01:29  lr: 0.000245  training_loss: 1.4606 (1.6241)  classification_loss: 0.2284 (0.2288)  loss_mask: 1.2298 (1.3953)  time: 0.1627  data: 0.0002  max mem: 4132
[18:47:39.301294] Epoch: [13]  [260/781]  eta: 0:01:25  lr: 0.000245  training_loss: 1.4276 (1.6089)  classification_loss: 0.2285 (0.2288)  loss_mask: 1.1980 (1.3801)  time: 0.1616  data: 0.0002  max mem: 4132
[18:47:42.557060] Epoch: [13]  [280/781]  eta: 0:01:22  lr: 0.000245  training_loss: 1.3375 (1.5937)  classification_loss: 0.2286 (0.2288)  loss_mask: 1.1112 (1.3650)  time: 0.1627  data: 0.0002  max mem: 4132
[18:47:45.805777] Epoch: [13]  [300/781]  eta: 0:01:18  lr: 0.000245  training_loss: 1.2986 (1.5758)  classification_loss: 0.2271 (0.2287)  loss_mask: 1.0715 (1.3471)  time: 0.1624  data: 0.0002  max mem: 4132
[18:47:49.057652] Epoch: [13]  [320/781]  eta: 0:01:15  lr: 0.000245  training_loss: 1.3308 (1.5616)  classification_loss: 0.2286 (0.2287)  loss_mask: 1.1048 (1.3329)  time: 0.1625  data: 0.0002  max mem: 4132
[18:47:52.353753] Epoch: [13]  [340/781]  eta: 0:01:12  lr: 0.000245  training_loss: 1.3186 (1.5476)  classification_loss: 0.2286 (0.2287)  loss_mask: 1.0899 (1.3189)  time: 0.1647  data: 0.0002  max mem: 4132
[18:47:55.600405] Epoch: [13]  [360/781]  eta: 0:01:09  lr: 0.000245  training_loss: 1.1962 (1.5285)  classification_loss: 0.2284 (0.2287)  loss_mask: 0.9662 (1.2999)  time: 0.1622  data: 0.0002  max mem: 4132
[18:47:58.854818] Epoch: [13]  [380/781]  eta: 0:01:05  lr: 0.000245  training_loss: 1.2068 (1.5133)  classification_loss: 0.2281 (0.2287)  loss_mask: 0.9802 (1.2846)  time: 0.1626  data: 0.0003  max mem: 4132
[18:48:02.135071] Epoch: [13]  [400/781]  eta: 0:01:02  lr: 0.000245  training_loss: 1.1156 (1.4953)  classification_loss: 0.2282 (0.2287)  loss_mask: 0.8888 (1.2666)  time: 0.1639  data: 0.0004  max mem: 4132
[18:48:05.403482] Epoch: [13]  [420/781]  eta: 0:00:59  lr: 0.000245  training_loss: 1.0950 (1.4763)  classification_loss: 0.2290 (0.2287)  loss_mask: 0.8636 (1.2477)  time: 0.1633  data: 0.0003  max mem: 4132
[18:48:08.644527] Epoch: [13]  [440/781]  eta: 0:00:55  lr: 0.000245  training_loss: 1.0819 (1.4587)  classification_loss: 0.2284 (0.2287)  loss_mask: 0.8530 (1.2300)  time: 0.1620  data: 0.0002  max mem: 4132
[18:48:11.876947] Epoch: [13]  [460/781]  eta: 0:00:52  lr: 0.000245  training_loss: 1.0420 (1.4413)  classification_loss: 0.2284 (0.2287)  loss_mask: 0.8136 (1.2126)  time: 0.1615  data: 0.0002  max mem: 4132
[18:48:15.126456] Epoch: [13]  [480/781]  eta: 0:00:49  lr: 0.000245  training_loss: 1.0629 (1.4285)  classification_loss: 0.2289 (0.2287)  loss_mask: 0.8335 (1.1998)  time: 0.1624  data: 0.0002  max mem: 4132
[18:48:18.347486] Epoch: [13]  [500/781]  eta: 0:00:45  lr: 0.000245  training_loss: 1.0148 (1.4136)  classification_loss: 0.2287 (0.2287)  loss_mask: 0.7858 (1.1849)  time: 0.1610  data: 0.0001  max mem: 4132
[18:48:21.572868] Epoch: [13]  [520/781]  eta: 0:00:42  lr: 0.000245  training_loss: 1.0880 (1.4021)  classification_loss: 0.2288 (0.2287)  loss_mask: 0.8591 (1.1734)  time: 0.1612  data: 0.0002  max mem: 4132
[18:48:24.798826] Epoch: [13]  [540/781]  eta: 0:00:39  lr: 0.000245  training_loss: 0.9839 (1.3881)  classification_loss: 0.2289 (0.2287)  loss_mask: 0.7548 (1.1594)  time: 0.1612  data: 0.0003  max mem: 4132
[18:48:28.031063] Epoch: [13]  [560/781]  eta: 0:00:36  lr: 0.000245  training_loss: 0.9654 (1.3735)  classification_loss: 0.2282 (0.2287)  loss_mask: 0.7373 (1.1448)  time: 0.1615  data: 0.0003  max mem: 4132
[18:48:31.277100] Epoch: [13]  [580/781]  eta: 0:00:32  lr: 0.000245  training_loss: 0.9342 (1.3594)  classification_loss: 0.2293 (0.2287)  loss_mask: 0.7067 (1.1306)  time: 0.1622  data: 0.0002  max mem: 4132
[18:48:34.554778] Epoch: [13]  [600/781]  eta: 0:00:29  lr: 0.000245  training_loss: 0.9275 (1.3455)  classification_loss: 0.2285 (0.2287)  loss_mask: 0.6996 (1.1168)  time: 0.1638  data: 0.0003  max mem: 4132
[18:48:37.802310] Epoch: [13]  [620/781]  eta: 0:00:26  lr: 0.000245  training_loss: 0.9171 (1.3322)  classification_loss: 0.2284 (0.2287)  loss_mask: 0.6865 (1.1034)  time: 0.1623  data: 0.0002  max mem: 4132
[18:48:41.037768] Epoch: [13]  [640/781]  eta: 0:00:23  lr: 0.000245  training_loss: 0.9143 (1.3195)  classification_loss: 0.2287 (0.2287)  loss_mask: 0.6852 (1.0908)  time: 0.1617  data: 0.0003  max mem: 4132
[18:48:44.319361] Epoch: [13]  [660/781]  eta: 0:00:19  lr: 0.000245  training_loss: 0.9143 (1.3074)  classification_loss: 0.2293 (0.2288)  loss_mask: 0.6818 (1.0786)  time: 0.1640  data: 0.0003  max mem: 4132
[18:48:47.629882] Epoch: [13]  [680/781]  eta: 0:00:16  lr: 0.000245  training_loss: 0.9377 (1.2967)  classification_loss: 0.2294 (0.2288)  loss_mask: 0.7076 (1.0679)  time: 0.1654  data: 0.0002  max mem: 4132
[18:48:50.875521] Epoch: [13]  [700/781]  eta: 0:00:13  lr: 0.000245  training_loss: 0.9573 (1.2865)  classification_loss: 0.2284 (0.2288)  loss_mask: 0.7269 (1.0577)  time: 0.1622  data: 0.0002  max mem: 4132
[18:48:54.112308] Epoch: [13]  [720/781]  eta: 0:00:09  lr: 0.000245  training_loss: 0.8213 (1.2742)  classification_loss: 0.2289 (0.2288)  loss_mask: 0.5921 (1.0454)  time: 0.1618  data: 0.0003  max mem: 4132
[18:48:57.420984] Epoch: [13]  [740/781]  eta: 0:00:06  lr: 0.000245  training_loss: 0.9158 (1.2665)  classification_loss: 0.2281 (0.2288)  loss_mask: 0.6885 (1.0377)  time: 0.1654  data: 0.0002  max mem: 4132
[18:49:00.651024] Epoch: [13]  [760/781]  eta: 0:00:03  lr: 0.000245  training_loss: 0.9158 (1.2591)  classification_loss: 0.2288 (0.2288)  loss_mask: 0.6885 (1.0304)  time: 0.1614  data: 0.0002  max mem: 4132
[18:49:03.863658] Epoch: [13]  [780/781]  eta: 0:00:00  lr: 0.000245  training_loss: 0.8593 (1.2496)  classification_loss: 0.2290 (0.2288)  loss_mask: 0.6303 (1.0208)  time: 0.1606  data: 0.0003  max mem: 4132
[18:49:03.995850] Epoch: [13] Total time: 0:02:07 (0.1634 s / it)
[18:49:03.996292] Averaged stats: lr: 0.000245  training_loss: 0.8593 (1.2496)  classification_loss: 0.2290 (0.2288)  loss_mask: 0.6303 (1.0208)
[18:49:04.645537] Test:  [  0/157]  eta: 0:01:41  testing_loss: 2.2442 (2.2442)  acc1: 18.7500 (18.7500)  acc5: 60.9375 (60.9375)  time: 0.6455  data: 0.6140  max mem: 4132
[18:49:04.939379] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 2.2757 (2.2724)  acc1: 14.0625 (14.3466)  acc5: 56.2500 (55.9659)  time: 0.0852  data: 0.0560  max mem: 4132
[18:49:05.224887] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 2.2711 (2.2749)  acc1: 12.5000 (13.3185)  acc5: 56.2500 (55.6548)  time: 0.0288  data: 0.0002  max mem: 4132
[18:49:05.510876] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 2.2656 (2.2720)  acc1: 12.5000 (13.7097)  acc5: 54.6875 (56.0484)  time: 0.0284  data: 0.0003  max mem: 4132
[18:49:05.797284] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 2.2698 (2.2730)  acc1: 12.5000 (13.4146)  acc5: 54.6875 (56.0595)  time: 0.0285  data: 0.0003  max mem: 4132
[18:49:06.083706] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 2.2698 (2.2733)  acc1: 12.5000 (13.2353)  acc5: 54.6875 (55.9436)  time: 0.0285  data: 0.0002  max mem: 4132
[18:49:06.374388] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 2.2706 (2.2728)  acc1: 14.0625 (13.3965)  acc5: 54.6875 (55.9426)  time: 0.0287  data: 0.0002  max mem: 4132
[18:49:06.657783] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 2.2706 (2.2733)  acc1: 12.5000 (13.2482)  acc5: 54.6875 (56.1180)  time: 0.0285  data: 0.0002  max mem: 4132
[18:49:06.941682] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 2.2689 (2.2725)  acc1: 12.5000 (13.3102)  acc5: 56.2500 (56.1921)  time: 0.0282  data: 0.0002  max mem: 4132
[18:49:07.222836] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 2.2672 (2.2719)  acc1: 12.5000 (13.4100)  acc5: 56.2500 (56.2328)  time: 0.0281  data: 0.0002  max mem: 4132
[18:49:07.505145] Test:  [100/157]  eta: 0:00:01  testing_loss: 2.2648 (2.2714)  acc1: 14.0625 (13.3973)  acc5: 59.3750 (56.2500)  time: 0.0281  data: 0.0002  max mem: 4132
[18:49:07.792188] Test:  [110/157]  eta: 0:00:01  testing_loss: 2.2651 (2.2721)  acc1: 10.9375 (13.2320)  acc5: 56.2500 (56.0389)  time: 0.0284  data: 0.0002  max mem: 4132
[18:49:08.074194] Test:  [120/157]  eta: 0:00:01  testing_loss: 2.2796 (2.2722)  acc1: 10.9375 (13.2490)  acc5: 53.1250 (55.9401)  time: 0.0283  data: 0.0002  max mem: 4132
[18:49:08.356434] Test:  [130/157]  eta: 0:00:00  testing_loss: 2.2802 (2.2728)  acc1: 12.5000 (13.1441)  acc5: 53.1250 (55.7371)  time: 0.0280  data: 0.0002  max mem: 4132
[18:49:08.638441] Test:  [140/157]  eta: 0:00:00  testing_loss: 2.2741 (2.2725)  acc1: 12.5000 (13.1649)  acc5: 53.1250 (55.9176)  time: 0.0280  data: 0.0002  max mem: 4132
[18:49:08.918405] Test:  [150/157]  eta: 0:00:00  testing_loss: 2.2662 (2.2722)  acc1: 14.0625 (13.2554)  acc5: 57.8125 (55.9810)  time: 0.0279  data: 0.0001  max mem: 4132
[18:49:09.072580] Test:  [156/157]  eta: 0:00:00  testing_loss: 2.2683 (2.2725)  acc1: 12.5000 (13.2300)  acc5: 56.2500 (55.9700)  time: 0.0271  data: 0.0001  max mem: 4132
[18:49:09.216178] Test: Total time: 0:00:05 (0.0332 s / it)
[18:49:09.216832] * Acc@1 13.230 Acc@5 55.970 loss 2.273
[18:49:09.217772] Accuracy of the network on the 10000 test images: 13.2%
[18:49:09.218307] Max accuracy: 25.08%
[18:49:09.349974] log_dir: ./output_dir
[18:49:10.190566] Epoch: [14]  [  0/781]  eta: 0:10:55  lr: 0.000245  training_loss: 0.7934 (0.7934)  classification_loss: 0.2280 (0.2280)  loss_mask: 0.5655 (0.5655)  time: 0.8388  data: 0.6409  max mem: 4132
[18:49:13.487844] Epoch: [14]  [ 20/781]  eta: 0:02:29  lr: 0.000244  training_loss: 0.8156 (0.8419)  classification_loss: 0.2290 (0.2295)  loss_mask: 0.5816 (0.6125)  time: 0.1648  data: 0.0002  max mem: 4132
[18:49:16.736289] Epoch: [14]  [ 40/781]  eta: 0:02:13  lr: 0.000244  training_loss: 0.9483 (0.9117)  classification_loss: 0.2284 (0.2290)  loss_mask: 0.7168 (0.6827)  time: 0.1623  data: 0.0002  max mem: 4132
[18:49:20.140402] Epoch: [14]  [ 60/781]  eta: 0:02:07  lr: 0.000244  training_loss: 0.8735 (0.9086)  classification_loss: 0.2288 (0.2289)  loss_mask: 0.6433 (0.6797)  time: 0.1701  data: 0.0002  max mem: 4132
[18:49:23.492911] Epoch: [14]  [ 80/781]  eta: 0:02:02  lr: 0.000244  training_loss: 0.8343 (0.8976)  classification_loss: 0.2289 (0.2290)  loss_mask: 0.6028 (0.6686)  time: 0.1675  data: 0.0002  max mem: 4132
[18:49:26.723531] Epoch: [14]  [100/781]  eta: 0:01:57  lr: 0.000244  training_loss: 0.9759 (0.9156)  classification_loss: 0.2284 (0.2290)  loss_mask: 0.7474 (0.6867)  time: 0.1615  data: 0.0003  max mem: 4132
[18:49:30.008640] Epoch: [14]  [120/781]  eta: 0:01:52  lr: 0.000244  training_loss: 0.8837 (0.9113)  classification_loss: 0.2289 (0.2290)  loss_mask: 0.6546 (0.6824)  time: 0.1642  data: 0.0002  max mem: 4132
[18:49:33.258676] Epoch: [14]  [140/781]  eta: 0:01:48  lr: 0.000244  training_loss: 0.9198 (0.9156)  classification_loss: 0.2272 (0.2288)  loss_mask: 0.6930 (0.6869)  time: 0.1624  data: 0.0002  max mem: 4132
[18:49:36.503258] Epoch: [14]  [160/781]  eta: 0:01:44  lr: 0.000244  training_loss: 0.9406 (0.9261)  classification_loss: 0.2290 (0.2289)  loss_mask: 0.7103 (0.6972)  time: 0.1622  data: 0.0002  max mem: 4132
[18:49:39.747255] Epoch: [14]  [180/781]  eta: 0:01:40  lr: 0.000244  training_loss: 0.8859 (0.9224)  classification_loss: 0.2283 (0.2289)  loss_mask: 0.6575 (0.6935)  time: 0.1621  data: 0.0002  max mem: 4132
[18:49:42.994552] Epoch: [14]  [200/781]  eta: 0:01:37  lr: 0.000244  training_loss: 0.8158 (0.9116)  classification_loss: 0.2276 (0.2287)  loss_mask: 0.5862 (0.6829)  time: 0.1623  data: 0.0002  max mem: 4132
[18:49:46.263689] Epoch: [14]  [220/781]  eta: 0:01:33  lr: 0.000244  training_loss: 0.8264 (0.9055)  classification_loss: 0.2283 (0.2287)  loss_mask: 0.5969 (0.6768)  time: 0.1634  data: 0.0002  max mem: 4132
[18:49:49.539191] Epoch: [14]  [240/781]  eta: 0:01:30  lr: 0.000244  training_loss: 0.9241 (0.9082)  classification_loss: 0.2283 (0.2287)  loss_mask: 0.6943 (0.6795)  time: 0.1637  data: 0.0003  max mem: 4132
[18:49:52.769062] Epoch: [14]  [260/781]  eta: 0:01:26  lr: 0.000244  training_loss: 0.8589 (0.9041)  classification_loss: 0.2298 (0.2289)  loss_mask: 0.6300 (0.6752)  time: 0.1614  data: 0.0003  max mem: 4132
[18:49:56.035546] Epoch: [14]  [280/781]  eta: 0:01:23  lr: 0.000244  training_loss: 0.7898 (0.8961)  classification_loss: 0.2299 (0.2290)  loss_mask: 0.5627 (0.6671)  time: 0.1633  data: 0.0003  max mem: 4132
[18:49:59.275165] Epoch: [14]  [300/781]  eta: 0:01:19  lr: 0.000244  training_loss: 0.8199 (0.8913)  classification_loss: 0.2279 (0.2289)  loss_mask: 0.5931 (0.6624)  time: 0.1619  data: 0.0002  max mem: 4132
[18:50:02.573732] Epoch: [14]  [320/781]  eta: 0:01:16  lr: 0.000244  training_loss: 0.8270 (0.8882)  classification_loss: 0.2285 (0.2289)  loss_mask: 0.5981 (0.6593)  time: 0.1649  data: 0.0002  max mem: 4132
[18:50:05.808889] Epoch: [14]  [340/781]  eta: 0:01:12  lr: 0.000244  training_loss: 0.7627 (0.8808)  classification_loss: 0.2292 (0.2289)  loss_mask: 0.5317 (0.6519)  time: 0.1617  data: 0.0002  max mem: 4132
[18:50:09.091280] Epoch: [14]  [360/781]  eta: 0:01:09  lr: 0.000244  training_loss: 0.7104 (0.8716)  classification_loss: 0.2281 (0.2289)  loss_mask: 0.4836 (0.6427)  time: 0.1640  data: 0.0004  max mem: 4132
[18:50:12.319904] Epoch: [14]  [380/781]  eta: 0:01:06  lr: 0.000244  training_loss: 0.7405 (0.8648)  classification_loss: 0.2276 (0.2288)  loss_mask: 0.5126 (0.6360)  time: 0.1613  data: 0.0003  max mem: 4132
[18:50:15.560741] Epoch: [14]  [400/781]  eta: 0:01:02  lr: 0.000244  training_loss: 0.7353 (0.8598)  classification_loss: 0.2295 (0.2289)  loss_mask: 0.5065 (0.6309)  time: 0.1620  data: 0.0003  max mem: 4132
[18:50:18.828352] Epoch: [14]  [420/781]  eta: 0:00:59  lr: 0.000244  training_loss: 0.6996 (0.8537)  classification_loss: 0.2313 (0.2291)  loss_mask: 0.4684 (0.6246)  time: 0.1633  data: 0.0002  max mem: 4132
[18:50:22.070560] Epoch: [14]  [440/781]  eta: 0:00:56  lr: 0.000244  training_loss: 0.7703 (0.8505)  classification_loss: 0.2293 (0.2291)  loss_mask: 0.5387 (0.6214)  time: 0.1620  data: 0.0004  max mem: 4132
[18:50:25.308527] Epoch: [14]  [460/781]  eta: 0:00:52  lr: 0.000244  training_loss: 0.7316 (0.8461)  classification_loss: 0.2277 (0.2291)  loss_mask: 0.5017 (0.6171)  time: 0.1618  data: 0.0002  max mem: 4132
[18:50:28.548573] Epoch: [14]  [480/781]  eta: 0:00:49  lr: 0.000244  training_loss: 0.8729 (0.8490)  classification_loss: 0.2286 (0.2291)  loss_mask: 0.6412 (0.6200)  time: 0.1619  data: 0.0001  max mem: 4132
[18:50:31.774744] Epoch: [14]  [500/781]  eta: 0:00:46  lr: 0.000244  training_loss: 0.7956 (0.8478)  classification_loss: 0.2284 (0.2290)  loss_mask: 0.5659 (0.6188)  time: 0.1612  data: 0.0002  max mem: 4132
[18:50:35.051939] Epoch: [14]  [520/781]  eta: 0:00:42  lr: 0.000244  training_loss: 0.7501 (0.8448)  classification_loss: 0.2292 (0.2290)  loss_mask: 0.5201 (0.6158)  time: 0.1638  data: 0.0002  max mem: 4132
[18:50:38.289934] Epoch: [14]  [540/781]  eta: 0:00:39  lr: 0.000244  training_loss: 0.7252 (0.8411)  classification_loss: 0.2279 (0.2290)  loss_mask: 0.4949 (0.6121)  time: 0.1618  data: 0.0003  max mem: 4132
[18:50:41.517294] Epoch: [14]  [560/781]  eta: 0:00:36  lr: 0.000244  training_loss: 0.7291 (0.8379)  classification_loss: 0.2272 (0.2289)  loss_mask: 0.5023 (0.6089)  time: 0.1613  data: 0.0002  max mem: 4132
[18:50:44.775086] Epoch: [14]  [580/781]  eta: 0:00:32  lr: 0.000244  training_loss: 0.8221 (0.8373)  classification_loss: 0.2285 (0.2289)  loss_mask: 0.5891 (0.6083)  time: 0.1628  data: 0.0002  max mem: 4132
[18:50:48.034195] Epoch: [14]  [600/781]  eta: 0:00:29  lr: 0.000244  training_loss: 0.8002 (0.8362)  classification_loss: 0.2285 (0.2289)  loss_mask: 0.5716 (0.6073)  time: 0.1629  data: 0.0002  max mem: 4132
[18:50:51.315238] Epoch: [14]  [620/781]  eta: 0:00:26  lr: 0.000244  training_loss: 0.7487 (0.8343)  classification_loss: 0.2277 (0.2289)  loss_mask: 0.5203 (0.6054)  time: 0.1640  data: 0.0002  max mem: 4132
[18:50:54.656310] Epoch: [14]  [640/781]  eta: 0:00:23  lr: 0.000243  training_loss: 0.7872 (0.8330)  classification_loss: 0.2280 (0.2289)  loss_mask: 0.5596 (0.6041)  time: 0.1670  data: 0.0002  max mem: 4132
[18:50:57.974275] Epoch: [14]  [660/781]  eta: 0:00:19  lr: 0.000243  training_loss: 0.7108 (0.8294)  classification_loss: 0.2284 (0.2289)  loss_mask: 0.4799 (0.6006)  time: 0.1658  data: 0.0002  max mem: 4132
[18:51:01.218536] Epoch: [14]  [680/781]  eta: 0:00:16  lr: 0.000243  training_loss: 0.6940 (0.8255)  classification_loss: 0.2286 (0.2288)  loss_mask: 0.4643 (0.5966)  time: 0.1621  data: 0.0002  max mem: 4132
[18:51:04.486222] Epoch: [14]  [700/781]  eta: 0:00:13  lr: 0.000243  training_loss: 0.7538 (0.8232)  classification_loss: 0.2285 (0.2288)  loss_mask: 0.5249 (0.5943)  time: 0.1633  data: 0.0004  max mem: 4132
[18:51:07.755912] Epoch: [14]  [720/781]  eta: 0:00:10  lr: 0.000243  training_loss: 0.6776 (0.8194)  classification_loss: 0.2287 (0.2288)  loss_mask: 0.4491 (0.5906)  time: 0.1634  data: 0.0003  max mem: 4132
[18:51:11.017838] Epoch: [14]  [740/781]  eta: 0:00:06  lr: 0.000243  training_loss: 0.6669 (0.8159)  classification_loss: 0.2284 (0.2288)  loss_mask: 0.4373 (0.5871)  time: 0.1630  data: 0.0004  max mem: 4132
[18:51:14.305422] Epoch: [14]  [760/781]  eta: 0:00:03  lr: 0.000243  training_loss: 0.7692 (0.8151)  classification_loss: 0.2278 (0.2288)  loss_mask: 0.5455 (0.5863)  time: 0.1643  data: 0.0002  max mem: 4132
[18:51:17.545842] Epoch: [14]  [780/781]  eta: 0:00:00  lr: 0.000243  training_loss: 0.7238 (0.8128)  classification_loss: 0.2283 (0.2288)  loss_mask: 0.4941 (0.5840)  time: 0.1619  data: 0.0003  max mem: 4132
[18:51:17.701696] Epoch: [14] Total time: 0:02:08 (0.1643 s / it)
[18:51:17.703092] Averaged stats: lr: 0.000243  training_loss: 0.7238 (0.8128)  classification_loss: 0.2283 (0.2288)  loss_mask: 0.4941 (0.5840)
[18:51:18.339681] Test:  [  0/157]  eta: 0:01:39  testing_loss: 2.2169 (2.2169)  acc1: 18.7500 (18.7500)  acc5: 68.7500 (68.7500)  time: 0.6309  data: 0.6006  max mem: 4132
[18:51:18.634539] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 2.2468 (2.2459)  acc1: 15.6250 (14.6307)  acc5: 60.9375 (60.9375)  time: 0.0839  data: 0.0556  max mem: 4132
[18:51:18.920673] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 2.2441 (2.2475)  acc1: 14.0625 (14.9554)  acc5: 60.9375 (60.7887)  time: 0.0288  data: 0.0008  max mem: 4132
[18:51:19.210721] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 2.2441 (2.2448)  acc1: 15.6250 (15.3226)  acc5: 62.5000 (60.6855)  time: 0.0287  data: 0.0003  max mem: 4132
[18:51:19.496367] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 2.2479 (2.2458)  acc1: 14.0625 (14.9771)  acc5: 59.3750 (60.8613)  time: 0.0287  data: 0.0002  max mem: 4132
[18:51:19.780568] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 2.2518 (2.2470)  acc1: 14.0625 (14.9203)  acc5: 59.3750 (60.9069)  time: 0.0284  data: 0.0003  max mem: 4132
[18:51:20.064371] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 2.2518 (2.2471)  acc1: 14.0625 (14.8309)  acc5: 59.3750 (60.9119)  time: 0.0283  data: 0.0003  max mem: 4132
[18:51:20.349945] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 2.2443 (2.2471)  acc1: 12.5000 (14.6567)  acc5: 60.9375 (61.1136)  time: 0.0283  data: 0.0002  max mem: 4132
[18:51:20.637469] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 2.2443 (2.2470)  acc1: 14.0625 (14.7955)  acc5: 60.9375 (61.0147)  time: 0.0285  data: 0.0002  max mem: 4132
[18:51:20.922688] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 2.2370 (2.2470)  acc1: 17.1875 (14.9554)  acc5: 60.9375 (60.8860)  time: 0.0285  data: 0.0002  max mem: 4132
[18:51:21.210006] Test:  [100/157]  eta: 0:00:01  testing_loss: 2.2346 (2.2464)  acc1: 17.1875 (15.0990)  acc5: 64.0625 (60.8756)  time: 0.0285  data: 0.0002  max mem: 4132
[18:51:21.494258] Test:  [110/157]  eta: 0:00:01  testing_loss: 2.2430 (2.2470)  acc1: 14.0625 (14.9212)  acc5: 60.9375 (60.8249)  time: 0.0284  data: 0.0002  max mem: 4132
[18:51:21.779365] Test:  [120/157]  eta: 0:00:01  testing_loss: 2.2526 (2.2469)  acc1: 14.0625 (15.0181)  acc5: 59.3750 (60.7180)  time: 0.0283  data: 0.0002  max mem: 4132
[18:51:22.066587] Test:  [130/157]  eta: 0:00:00  testing_loss: 2.2543 (2.2474)  acc1: 14.0625 (14.9571)  acc5: 59.3750 (60.6155)  time: 0.0284  data: 0.0002  max mem: 4132
[18:51:22.348120] Test:  [140/157]  eta: 0:00:00  testing_loss: 2.2491 (2.2468)  acc1: 15.6250 (15.1817)  acc5: 59.3750 (60.7824)  time: 0.0283  data: 0.0002  max mem: 4132
[18:51:22.628458] Test:  [150/157]  eta: 0:00:00  testing_loss: 2.2356 (2.2464)  acc1: 17.1875 (15.2628)  acc5: 62.5000 (60.8961)  time: 0.0279  data: 0.0001  max mem: 4132
[18:51:22.779762] Test:  [156/157]  eta: 0:00:00  testing_loss: 2.2356 (2.2463)  acc1: 15.6250 (15.1800)  acc5: 62.5000 (60.9900)  time: 0.0270  data: 0.0001  max mem: 4132
[18:51:22.935427] Test: Total time: 0:00:05 (0.0333 s / it)
[18:51:22.936000] * Acc@1 15.180 Acc@5 60.990 loss 2.246
[18:51:22.936308] Accuracy of the network on the 10000 test images: 15.2%
[18:51:22.936482] Max accuracy: 25.08%
[18:51:23.196478] log_dir: ./output_dir
[18:51:24.012209] Epoch: [15]  [  0/781]  eta: 0:10:35  lr: 0.000243  training_loss: 0.6884 (0.6884)  classification_loss: 0.2273 (0.2273)  loss_mask: 0.4610 (0.4610)  time: 0.8140  data: 0.6182  max mem: 4132
[18:51:27.268057] Epoch: [15]  [ 20/781]  eta: 0:02:27  lr: 0.000243  training_loss: 0.7032 (0.7108)  classification_loss: 0.2278 (0.2280)  loss_mask: 0.4761 (0.4828)  time: 0.1627  data: 0.0006  max mem: 4132
[18:51:30.517428] Epoch: [15]  [ 40/781]  eta: 0:02:12  lr: 0.000243  training_loss: 0.7311 (0.7312)  classification_loss: 0.2271 (0.2278)  loss_mask: 0.5014 (0.5034)  time: 0.1624  data: 0.0002  max mem: 4132
[18:51:33.783594] Epoch: [15]  [ 60/781]  eta: 0:02:05  lr: 0.000243  training_loss: 0.7105 (0.7356)  classification_loss: 0.2274 (0.2279)  loss_mask: 0.4818 (0.5078)  time: 0.1632  data: 0.0002  max mem: 4132
[18:51:37.126902] Epoch: [15]  [ 80/781]  eta: 0:02:00  lr: 0.000243  training_loss: 0.7498 (0.7464)  classification_loss: 0.2278 (0.2279)  loss_mask: 0.5186 (0.5185)  time: 0.1671  data: 0.0002  max mem: 4132
[18:51:40.377447] Epoch: [15]  [100/781]  eta: 0:01:55  lr: 0.000243  training_loss: 0.6834 (0.7343)  classification_loss: 0.2273 (0.2279)  loss_mask: 0.4578 (0.5064)  time: 0.1624  data: 0.0002  max mem: 4132
[18:51:43.647267] Epoch: [15]  [120/781]  eta: 0:01:51  lr: 0.000243  training_loss: 0.7203 (0.7322)  classification_loss: 0.2274 (0.2278)  loss_mask: 0.4945 (0.5045)  time: 0.1634  data: 0.0002  max mem: 4132
[18:51:46.907997] Epoch: [15]  [140/781]  eta: 0:01:47  lr: 0.000243  training_loss: 0.6902 (0.7284)  classification_loss: 0.2273 (0.2278)  loss_mask: 0.4630 (0.5006)  time: 0.1630  data: 0.0003  max mem: 4132
[18:51:50.141534] Epoch: [15]  [160/781]  eta: 0:01:43  lr: 0.000243  training_loss: 0.6735 (0.7245)  classification_loss: 0.2287 (0.2279)  loss_mask: 0.4448 (0.4966)  time: 0.1616  data: 0.0002  max mem: 4132
[18:51:53.412407] Epoch: [15]  [180/781]  eta: 0:01:40  lr: 0.000243  training_loss: 0.8332 (0.7390)  classification_loss: 0.2285 (0.2280)  loss_mask: 0.6025 (0.5109)  time: 0.1635  data: 0.0002  max mem: 4132
[18:51:56.683531] Epoch: [15]  [200/781]  eta: 0:01:36  lr: 0.000243  training_loss: 0.6876 (0.7345)  classification_loss: 0.2269 (0.2280)  loss_mask: 0.4612 (0.5065)  time: 0.1635  data: 0.0002  max mem: 4132
[18:51:59.953943] Epoch: [15]  [220/781]  eta: 0:01:33  lr: 0.000243  training_loss: 0.6233 (0.7267)  classification_loss: 0.2270 (0.2280)  loss_mask: 0.3940 (0.4987)  time: 0.1634  data: 0.0002  max mem: 4132
[18:52:03.239366] Epoch: [15]  [240/781]  eta: 0:01:29  lr: 0.000243  training_loss: 0.7054 (0.7250)  classification_loss: 0.2282 (0.2280)  loss_mask: 0.4723 (0.4970)  time: 0.1642  data: 0.0003  max mem: 4132
[18:52:06.485409] Epoch: [15]  [260/781]  eta: 0:01:26  lr: 0.000243  training_loss: 0.6586 (0.7197)  classification_loss: 0.2275 (0.2280)  loss_mask: 0.4314 (0.4917)  time: 0.1622  data: 0.0002  max mem: 4132
[18:52:09.727646] Epoch: [15]  [280/781]  eta: 0:01:22  lr: 0.000243  training_loss: 0.6477 (0.7157)  classification_loss: 0.2290 (0.2281)  loss_mask: 0.4169 (0.4877)  time: 0.1620  data: 0.0002  max mem: 4132
[18:52:12.962203] Epoch: [15]  [300/781]  eta: 0:01:19  lr: 0.000243  training_loss: 0.6618 (0.7112)  classification_loss: 0.2277 (0.2280)  loss_mask: 0.4323 (0.4832)  time: 0.1616  data: 0.0002  max mem: 4132
[18:52:16.262452] Epoch: [15]  [320/781]  eta: 0:01:16  lr: 0.000243  training_loss: 0.6928 (0.7103)  classification_loss: 0.2273 (0.2280)  loss_mask: 0.4629 (0.4823)  time: 0.1649  data: 0.0002  max mem: 4132
[18:52:19.500313] Epoch: [15]  [340/781]  eta: 0:01:12  lr: 0.000243  training_loss: 0.6955 (0.7090)  classification_loss: 0.2276 (0.2280)  loss_mask: 0.4691 (0.4811)  time: 0.1618  data: 0.0002  max mem: 4132
[18:52:22.740106] Epoch: [15]  [360/781]  eta: 0:01:09  lr: 0.000243  training_loss: 0.6301 (0.7047)  classification_loss: 0.2280 (0.2279)  loss_mask: 0.4064 (0.4768)  time: 0.1619  data: 0.0002  max mem: 4132
[18:52:25.971257] Epoch: [15]  [380/781]  eta: 0:01:06  lr: 0.000243  training_loss: 0.6254 (0.7011)  classification_loss: 0.2281 (0.2280)  loss_mask: 0.3908 (0.4731)  time: 0.1615  data: 0.0002  max mem: 4132
[18:52:29.195399] Epoch: [15]  [400/781]  eta: 0:01:02  lr: 0.000243  training_loss: 0.6904 (0.7013)  classification_loss: 0.2283 (0.2280)  loss_mask: 0.4612 (0.4733)  time: 0.1611  data: 0.0002  max mem: 4132
[18:52:32.432489] Epoch: [15]  [420/781]  eta: 0:00:59  lr: 0.000243  training_loss: 0.6358 (0.6987)  classification_loss: 0.2275 (0.2280)  loss_mask: 0.4078 (0.4707)  time: 0.1618  data: 0.0002  max mem: 4132
[18:52:35.692446] Epoch: [15]  [440/781]  eta: 0:00:56  lr: 0.000242  training_loss: 0.6227 (0.6955)  classification_loss: 0.2279 (0.2280)  loss_mask: 0.3921 (0.4674)  time: 0.1629  data: 0.0003  max mem: 4132
[18:52:38.953332] Epoch: [15]  [460/781]  eta: 0:00:52  lr: 0.000242  training_loss: 0.5696 (0.6901)  classification_loss: 0.2273 (0.2280)  loss_mask: 0.3413 (0.4620)  time: 0.1630  data: 0.0002  max mem: 4132
[18:52:42.187000] Epoch: [15]  [480/781]  eta: 0:00:49  lr: 0.000242  training_loss: 0.6430 (0.6889)  classification_loss: 0.2275 (0.2280)  loss_mask: 0.4151 (0.4609)  time: 0.1616  data: 0.0002  max mem: 4132
[18:52:45.474545] Epoch: [15]  [500/781]  eta: 0:00:46  lr: 0.000242  training_loss: 0.6039 (0.6862)  classification_loss: 0.2283 (0.2280)  loss_mask: 0.3776 (0.4582)  time: 0.1643  data: 0.0002  max mem: 4132
[18:52:48.738052] Epoch: [15]  [520/781]  eta: 0:00:42  lr: 0.000242  training_loss: 0.6162 (0.6846)  classification_loss: 0.2276 (0.2280)  loss_mask: 0.3899 (0.4566)  time: 0.1631  data: 0.0002  max mem: 4132
[18:52:51.986268] Epoch: [15]  [540/781]  eta: 0:00:39  lr: 0.000242  training_loss: 0.5985 (0.6819)  classification_loss: 0.2276 (0.2280)  loss_mask: 0.3723 (0.4539)  time: 0.1623  data: 0.0003  max mem: 4132
[18:52:55.230396] Epoch: [15]  [560/781]  eta: 0:00:36  lr: 0.000242  training_loss: 0.6476 (0.6815)  classification_loss: 0.2277 (0.2280)  loss_mask: 0.4198 (0.4535)  time: 0.1621  data: 0.0002  max mem: 4132
[18:52:58.471897] Epoch: [15]  [580/781]  eta: 0:00:32  lr: 0.000242  training_loss: 0.6110 (0.6798)  classification_loss: 0.2282 (0.2280)  loss_mask: 0.3810 (0.4518)  time: 0.1620  data: 0.0002  max mem: 4132
[18:53:01.733309] Epoch: [15]  [600/781]  eta: 0:00:29  lr: 0.000242  training_loss: 0.5559 (0.6762)  classification_loss: 0.2273 (0.2280)  loss_mask: 0.3237 (0.4482)  time: 0.1630  data: 0.0003  max mem: 4132
[18:53:04.948837] Epoch: [15]  [620/781]  eta: 0:00:26  lr: 0.000242  training_loss: 0.6391 (0.6748)  classification_loss: 0.2284 (0.2280)  loss_mask: 0.4028 (0.4467)  time: 0.1607  data: 0.0002  max mem: 4132
[18:53:08.223661] Epoch: [15]  [640/781]  eta: 0:00:23  lr: 0.000242  training_loss: 0.6032 (0.6747)  classification_loss: 0.2267 (0.2280)  loss_mask: 0.3755 (0.4467)  time: 0.1636  data: 0.0002  max mem: 4132
[18:53:11.485238] Epoch: [15]  [660/781]  eta: 0:00:19  lr: 0.000242  training_loss: 0.6376 (0.6734)  classification_loss: 0.2263 (0.2279)  loss_mask: 0.4143 (0.4455)  time: 0.1630  data: 0.0002  max mem: 4132
[18:53:14.748792] Epoch: [15]  [680/781]  eta: 0:00:16  lr: 0.000242  training_loss: 0.5979 (0.6727)  classification_loss: 0.2272 (0.2279)  loss_mask: 0.3712 (0.4448)  time: 0.1631  data: 0.0002  max mem: 4132
[18:53:17.999487] Epoch: [15]  [700/781]  eta: 0:00:13  lr: 0.000242  training_loss: 0.5864 (0.6706)  classification_loss: 0.2275 (0.2279)  loss_mask: 0.3582 (0.4427)  time: 0.1625  data: 0.0002  max mem: 4132
[18:53:21.236685] Epoch: [15]  [720/781]  eta: 0:00:09  lr: 0.000242  training_loss: 0.6082 (0.6695)  classification_loss: 0.2282 (0.2279)  loss_mask: 0.3784 (0.4416)  time: 0.1618  data: 0.0003  max mem: 4132
[18:53:24.469937] Epoch: [15]  [740/781]  eta: 0:00:06  lr: 0.000242  training_loss: 0.6030 (0.6684)  classification_loss: 0.2268 (0.2279)  loss_mask: 0.3781 (0.4404)  time: 0.1616  data: 0.0002  max mem: 4132
[18:53:27.717398] Epoch: [15]  [760/781]  eta: 0:00:03  lr: 0.000242  training_loss: 0.6236 (0.6675)  classification_loss: 0.2273 (0.2279)  loss_mask: 0.3893 (0.4396)  time: 0.1623  data: 0.0003  max mem: 4132
[18:53:30.957234] Epoch: [15]  [780/781]  eta: 0:00:00  lr: 0.000242  training_loss: 0.5734 (0.6658)  classification_loss: 0.2277 (0.2279)  loss_mask: 0.3434 (0.4378)  time: 0.1619  data: 0.0003  max mem: 4132
[18:53:31.096188] Epoch: [15] Total time: 0:02:07 (0.1638 s / it)
[18:53:31.096666] Averaged stats: lr: 0.000242  training_loss: 0.5734 (0.6658)  classification_loss: 0.2277 (0.2279)  loss_mask: 0.3434 (0.4378)
[18:53:31.703448] Test:  [  0/157]  eta: 0:01:34  testing_loss: 2.1899 (2.1899)  acc1: 26.5625 (26.5625)  acc5: 73.4375 (73.4375)  time: 0.6022  data: 0.5710  max mem: 4132
[18:53:32.011012] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 2.2208 (2.2256)  acc1: 18.7500 (17.8977)  acc5: 65.6250 (65.1989)  time: 0.0819  data: 0.0531  max mem: 4132
[18:53:32.292889] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 2.2208 (2.2264)  acc1: 17.1875 (17.7827)  acc5: 64.0625 (65.2530)  time: 0.0289  data: 0.0007  max mem: 4132
[18:53:32.575162] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 2.2173 (2.2234)  acc1: 17.1875 (17.2379)  acc5: 64.0625 (64.9698)  time: 0.0281  data: 0.0002  max mem: 4132
[18:53:32.856886] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 2.2239 (2.2247)  acc1: 15.6250 (17.0351)  acc5: 62.5000 (64.7104)  time: 0.0281  data: 0.0002  max mem: 4132
[18:53:33.138512] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 2.2333 (2.2265)  acc1: 15.6250 (16.5441)  acc5: 65.6250 (64.7059)  time: 0.0281  data: 0.0002  max mem: 4132
[18:53:33.420382] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 2.2333 (2.2269)  acc1: 14.0625 (16.4191)  acc5: 64.0625 (64.8309)  time: 0.0281  data: 0.0002  max mem: 4132
[18:53:33.701499] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 2.2267 (2.2267)  acc1: 15.6250 (16.4613)  acc5: 65.6250 (64.9868)  time: 0.0280  data: 0.0002  max mem: 4132
[18:53:33.981914] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 2.2265 (2.2268)  acc1: 15.6250 (16.6088)  acc5: 65.6250 (65.0463)  time: 0.0280  data: 0.0001  max mem: 4132
[18:53:34.265553] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 2.2172 (2.2270)  acc1: 17.1875 (16.7754)  acc5: 64.0625 (64.9210)  time: 0.0281  data: 0.0002  max mem: 4132
[18:53:34.548782] Test:  [100/157]  eta: 0:00:01  testing_loss: 2.2110 (2.2263)  acc1: 17.1875 (16.9400)  acc5: 65.6250 (64.9443)  time: 0.0282  data: 0.0002  max mem: 4132
[18:53:34.832204] Test:  [110/157]  eta: 0:00:01  testing_loss: 2.2152 (2.2267)  acc1: 17.1875 (16.9060)  acc5: 67.1875 (64.8508)  time: 0.0282  data: 0.0002  max mem: 4132
[18:53:35.115372] Test:  [120/157]  eta: 0:00:01  testing_loss: 2.2276 (2.2264)  acc1: 14.0625 (16.9163)  acc5: 64.0625 (64.7986)  time: 0.0282  data: 0.0002  max mem: 4132
[18:53:35.397483] Test:  [130/157]  eta: 0:00:00  testing_loss: 2.2289 (2.2270)  acc1: 17.1875 (16.9132)  acc5: 64.0625 (64.6469)  time: 0.0281  data: 0.0002  max mem: 4132
[18:53:35.677440] Test:  [140/157]  eta: 0:00:00  testing_loss: 2.2269 (2.2261)  acc1: 17.1875 (17.1210)  acc5: 64.0625 (64.8050)  time: 0.0280  data: 0.0001  max mem: 4132
[18:53:35.956500] Test:  [150/157]  eta: 0:00:00  testing_loss: 2.2146 (2.2256)  acc1: 17.1875 (17.1047)  acc5: 65.6250 (64.8282)  time: 0.0278  data: 0.0001  max mem: 4132
[18:53:36.111082] Test:  [156/157]  eta: 0:00:00  testing_loss: 2.2130 (2.2254)  acc1: 17.1875 (17.0600)  acc5: 67.1875 (64.8700)  time: 0.0271  data: 0.0001  max mem: 4132
[18:53:36.271660] Test: Total time: 0:00:05 (0.0329 s / it)
[18:53:36.272196] * Acc@1 17.060 Acc@5 64.870 loss 2.225
[18:53:36.272474] Accuracy of the network on the 10000 test images: 17.1%
[18:53:36.272646] Max accuracy: 25.08%
[18:53:36.500026] log_dir: ./output_dir
[18:53:37.360511] Epoch: [16]  [  0/781]  eta: 0:11:10  lr: 0.000242  training_loss: 0.6105 (0.6105)  classification_loss: 0.2267 (0.2267)  loss_mask: 0.3838 (0.3838)  time: 0.8587  data: 0.6840  max mem: 4132
[18:53:40.596968] Epoch: [16]  [ 20/781]  eta: 0:02:28  lr: 0.000242  training_loss: 0.5600 (0.5641)  classification_loss: 0.2281 (0.2278)  loss_mask: 0.3332 (0.3363)  time: 0.1617  data: 0.0002  max mem: 4132
[18:53:43.855452] Epoch: [16]  [ 40/781]  eta: 0:02:12  lr: 0.000242  training_loss: 0.5615 (0.5737)  classification_loss: 0.2264 (0.2273)  loss_mask: 0.3352 (0.3464)  time: 0.1628  data: 0.0002  max mem: 4132
[18:53:47.103994] Epoch: [16]  [ 60/781]  eta: 0:02:05  lr: 0.000242  training_loss: 0.5404 (0.5650)  classification_loss: 0.2292 (0.2278)  loss_mask: 0.3101 (0.3372)  time: 0.1624  data: 0.0002  max mem: 4132
[18:53:50.336298] Epoch: [16]  [ 80/781]  eta: 0:01:59  lr: 0.000242  training_loss: 0.5888 (0.5739)  classification_loss: 0.2273 (0.2280)  loss_mask: 0.3615 (0.3459)  time: 0.1615  data: 0.0002  max mem: 4132
[18:53:53.580430] Epoch: [16]  [100/781]  eta: 0:01:55  lr: 0.000242  training_loss: 0.5509 (0.5738)  classification_loss: 0.2261 (0.2278)  loss_mask: 0.3234 (0.3460)  time: 0.1621  data: 0.0003  max mem: 4132
[18:53:56.835255] Epoch: [16]  [120/781]  eta: 0:01:51  lr: 0.000242  training_loss: 0.5619 (0.5714)  classification_loss: 0.2265 (0.2276)  loss_mask: 0.3357 (0.3438)  time: 0.1627  data: 0.0002  max mem: 4132
[18:54:00.101155] Epoch: [16]  [140/781]  eta: 0:01:47  lr: 0.000242  training_loss: 0.5395 (0.5683)  classification_loss: 0.2261 (0.2275)  loss_mask: 0.3169 (0.3409)  time: 0.1632  data: 0.0003  max mem: 4132
[18:54:03.417435] Epoch: [16]  [160/781]  eta: 0:01:43  lr: 0.000242  training_loss: 0.4624 (0.5585)  classification_loss: 0.2267 (0.2275)  loss_mask: 0.2354 (0.3310)  time: 0.1657  data: 0.0001  max mem: 4132
[18:54:06.836382] Epoch: [16]  [180/781]  eta: 0:01:40  lr: 0.000242  training_loss: 0.6000 (0.5681)  classification_loss: 0.2285 (0.2276)  loss_mask: 0.3668 (0.3405)  time: 0.1709  data: 0.0002  max mem: 4132
[18:54:10.109751] Epoch: [16]  [200/781]  eta: 0:01:37  lr: 0.000241  training_loss: 0.6415 (0.5739)  classification_loss: 0.2262 (0.2274)  loss_mask: 0.4180 (0.3465)  time: 0.1636  data: 0.0002  max mem: 4132
[18:54:13.365954] Epoch: [16]  [220/781]  eta: 0:01:33  lr: 0.000241  training_loss: 0.5932 (0.5756)  classification_loss: 0.2274 (0.2274)  loss_mask: 0.3640 (0.3482)  time: 0.1627  data: 0.0002  max mem: 4132
[18:54:16.605490] Epoch: [16]  [240/781]  eta: 0:01:29  lr: 0.000241  training_loss: 0.5996 (0.5798)  classification_loss: 0.2275 (0.2274)  loss_mask: 0.3732 (0.3523)  time: 0.1619  data: 0.0002  max mem: 4132
[18:54:19.858089] Epoch: [16]  [260/781]  eta: 0:01:26  lr: 0.000241  training_loss: 0.5790 (0.5800)  classification_loss: 0.2265 (0.2274)  loss_mask: 0.3533 (0.3526)  time: 0.1625  data: 0.0002  max mem: 4132
[18:54:23.130789] Epoch: [16]  [280/781]  eta: 0:01:23  lr: 0.000241  training_loss: 0.5178 (0.5776)  classification_loss: 0.2286 (0.2275)  loss_mask: 0.2921 (0.3501)  time: 0.1636  data: 0.0002  max mem: 4132
[18:54:26.379911] Epoch: [16]  [300/781]  eta: 0:01:19  lr: 0.000241  training_loss: 0.5622 (0.5764)  classification_loss: 0.2267 (0.2275)  loss_mask: 0.3351 (0.3489)  time: 0.1624  data: 0.0002  max mem: 4132
[18:54:29.607238] Epoch: [16]  [320/781]  eta: 0:01:16  lr: 0.000241  training_loss: 0.4452 (0.5698)  classification_loss: 0.2264 (0.2275)  loss_mask: 0.2193 (0.3423)  time: 0.1613  data: 0.0002  max mem: 4132
[18:54:32.843379] Epoch: [16]  [340/781]  eta: 0:01:12  lr: 0.000241  training_loss: 0.5290 (0.5670)  classification_loss: 0.2269 (0.2274)  loss_mask: 0.2982 (0.3396)  time: 0.1617  data: 0.0002  max mem: 4132
[18:54:36.087311] Epoch: [16]  [360/781]  eta: 0:01:09  lr: 0.000241  training_loss: 0.5808 (0.5688)  classification_loss: 0.2266 (0.2274)  loss_mask: 0.3542 (0.3414)  time: 0.1621  data: 0.0002  max mem: 4132
[18:54:39.332988] Epoch: [16]  [380/781]  eta: 0:01:06  lr: 0.000241  training_loss: 0.5162 (0.5671)  classification_loss: 0.2271 (0.2275)  loss_mask: 0.2899 (0.3396)  time: 0.1622  data: 0.0002  max mem: 4132
[18:54:42.571217] Epoch: [16]  [400/781]  eta: 0:01:02  lr: 0.000241  training_loss: 0.5020 (0.5652)  classification_loss: 0.2305 (0.2276)  loss_mask: 0.2701 (0.3375)  time: 0.1618  data: 0.0002  max mem: 4132
[18:54:45.816524] Epoch: [16]  [420/781]  eta: 0:00:59  lr: 0.000241  training_loss: 0.5196 (0.5639)  classification_loss: 0.2293 (0.2277)  loss_mask: 0.2891 (0.3361)  time: 0.1622  data: 0.0002  max mem: 4132
[18:54:49.059284] Epoch: [16]  [440/781]  eta: 0:00:56  lr: 0.000241  training_loss: 0.5002 (0.5612)  classification_loss: 0.2275 (0.2277)  loss_mask: 0.2726 (0.3334)  time: 0.1620  data: 0.0004  max mem: 4132
[18:54:52.311379] Epoch: [16]  [460/781]  eta: 0:00:52  lr: 0.000241  training_loss: 0.5377 (0.5598)  classification_loss: 0.2267 (0.2277)  loss_mask: 0.3076 (0.3321)  time: 0.1625  data: 0.0002  max mem: 4132
[18:54:55.553505] Epoch: [16]  [480/781]  eta: 0:00:49  lr: 0.000241  training_loss: 0.4732 (0.5567)  classification_loss: 0.2276 (0.2277)  loss_mask: 0.2399 (0.3290)  time: 0.1620  data: 0.0002  max mem: 4132
[18:54:58.809527] Epoch: [16]  [500/781]  eta: 0:00:46  lr: 0.000241  training_loss: 0.5722 (0.5579)  classification_loss: 0.2265 (0.2277)  loss_mask: 0.3438 (0.3302)  time: 0.1627  data: 0.0002  max mem: 4132
[18:55:02.089325] Epoch: [16]  [520/781]  eta: 0:00:42  lr: 0.000241  training_loss: 0.4774 (0.5555)  classification_loss: 0.2266 (0.2277)  loss_mask: 0.2529 (0.3278)  time: 0.1639  data: 0.0002  max mem: 4132
[18:55:05.329123] Epoch: [16]  [540/781]  eta: 0:00:39  lr: 0.000241  training_loss: 0.4740 (0.5526)  classification_loss: 0.2275 (0.2277)  loss_mask: 0.2462 (0.3250)  time: 0.1619  data: 0.0003  max mem: 4132
[18:55:08.561740] Epoch: [16]  [560/781]  eta: 0:00:36  lr: 0.000241  training_loss: 0.5187 (0.5521)  classification_loss: 0.2278 (0.2277)  loss_mask: 0.2918 (0.3244)  time: 0.1615  data: 0.0003  max mem: 4132
[18:55:11.792751] Epoch: [16]  [580/781]  eta: 0:00:32  lr: 0.000241  training_loss: 0.4526 (0.5495)  classification_loss: 0.2260 (0.2277)  loss_mask: 0.2228 (0.3218)  time: 0.1615  data: 0.0002  max mem: 4132
[18:55:15.016884] Epoch: [16]  [600/781]  eta: 0:00:29  lr: 0.000241  training_loss: 0.4422 (0.5465)  classification_loss: 0.2277 (0.2277)  loss_mask: 0.2127 (0.3188)  time: 0.1611  data: 0.0002  max mem: 4132
[18:55:18.260054] Epoch: [16]  [620/781]  eta: 0:00:26  lr: 0.000241  training_loss: 0.4457 (0.5438)  classification_loss: 0.2266 (0.2277)  loss_mask: 0.2173 (0.3161)  time: 0.1621  data: 0.0002  max mem: 4132
[18:55:21.511118] Epoch: [16]  [640/781]  eta: 0:00:23  lr: 0.000241  training_loss: 0.4083 (0.5404)  classification_loss: 0.2266 (0.2276)  loss_mask: 0.1805 (0.3128)  time: 0.1625  data: 0.0002  max mem: 4132
[18:55:24.761589] Epoch: [16]  [660/781]  eta: 0:00:19  lr: 0.000241  training_loss: 0.5140 (0.5393)  classification_loss: 0.2268 (0.2276)  loss_mask: 0.2873 (0.3117)  time: 0.1624  data: 0.0002  max mem: 4132
[18:55:27.997226] Epoch: [16]  [680/781]  eta: 0:00:16  lr: 0.000241  training_loss: 0.4551 (0.5373)  classification_loss: 0.2275 (0.2276)  loss_mask: 0.2221 (0.3097)  time: 0.1617  data: 0.0002  max mem: 4132
[18:55:31.226891] Epoch: [16]  [700/781]  eta: 0:00:13  lr: 0.000240  training_loss: 0.5134 (0.5366)  classification_loss: 0.2263 (0.2276)  loss_mask: 0.2854 (0.3090)  time: 0.1614  data: 0.0003  max mem: 4132
[18:55:34.481475] Epoch: [16]  [720/781]  eta: 0:00:09  lr: 0.000240  training_loss: 0.4107 (0.5335)  classification_loss: 0.2270 (0.2276)  loss_mask: 0.1838 (0.3059)  time: 0.1626  data: 0.0002  max mem: 4132
[18:55:37.720846] Epoch: [16]  [740/781]  eta: 0:00:06  lr: 0.000240  training_loss: 0.4492 (0.5314)  classification_loss: 0.2275 (0.2276)  loss_mask: 0.2151 (0.3038)  time: 0.1619  data: 0.0003  max mem: 4132
[18:55:40.974469] Epoch: [16]  [760/781]  eta: 0:00:03  lr: 0.000240  training_loss: 0.4291 (0.5296)  classification_loss: 0.2275 (0.2276)  loss_mask: 0.2047 (0.3020)  time: 0.1625  data: 0.0002  max mem: 4132
[18:55:44.205610] Epoch: [16]  [780/781]  eta: 0:00:00  lr: 0.000240  training_loss: 0.4327 (0.5275)  classification_loss: 0.2266 (0.2276)  loss_mask: 0.2072 (0.2999)  time: 0.1614  data: 0.0002  max mem: 4132
[18:55:44.380195] Epoch: [16] Total time: 0:02:07 (0.1637 s / it)
[18:55:44.380898] Averaged stats: lr: 0.000240  training_loss: 0.4327 (0.5275)  classification_loss: 0.2266 (0.2276)  loss_mask: 0.2072 (0.2999)
[18:55:45.007398] Test:  [  0/157]  eta: 0:01:37  testing_loss: 2.1711 (2.1711)  acc1: 23.4375 (23.4375)  acc5: 73.4375 (73.4375)  time: 0.6222  data: 0.5892  max mem: 4132
[18:55:45.300677] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 2.2182 (2.2124)  acc1: 15.6250 (16.1932)  acc5: 67.1875 (66.9034)  time: 0.0830  data: 0.0537  max mem: 4132
[18:55:45.583862] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 2.2182 (2.2152)  acc1: 15.6250 (16.3690)  acc5: 67.1875 (67.1131)  time: 0.0287  data: 0.0002  max mem: 4132
[18:55:45.866019] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 2.2073 (2.2111)  acc1: 15.6250 (16.3306)  acc5: 67.1875 (66.7843)  time: 0.0281  data: 0.0002  max mem: 4132
[18:55:46.147716] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 2.2123 (2.2132)  acc1: 15.6250 (16.1585)  acc5: 65.6250 (66.3872)  time: 0.0281  data: 0.0002  max mem: 4132
[18:55:46.428944] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 2.2217 (2.2144)  acc1: 15.6250 (15.9620)  acc5: 64.0625 (66.2071)  time: 0.0280  data: 0.0002  max mem: 4132
[18:55:46.718308] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 2.2205 (2.2147)  acc1: 15.6250 (15.8299)  acc5: 64.0625 (66.0348)  time: 0.0284  data: 0.0002  max mem: 4132
[18:55:47.001235] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 2.2151 (2.2149)  acc1: 14.0625 (15.7570)  acc5: 65.6250 (66.3512)  time: 0.0285  data: 0.0002  max mem: 4132
[18:55:47.284330] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 2.2137 (2.2142)  acc1: 14.0625 (15.6250)  acc5: 68.7500 (66.3002)  time: 0.0282  data: 0.0002  max mem: 4132
[18:55:47.569964] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 2.2109 (2.2145)  acc1: 15.6250 (15.6078)  acc5: 64.0625 (66.1058)  time: 0.0283  data: 0.0002  max mem: 4132
[18:55:47.854937] Test:  [100/157]  eta: 0:00:01  testing_loss: 2.2065 (2.2141)  acc1: 15.6250 (15.6250)  acc5: 65.6250 (65.9344)  time: 0.0284  data: 0.0002  max mem: 4132
[18:55:48.141363] Test:  [110/157]  eta: 0:00:01  testing_loss: 2.2139 (2.2148)  acc1: 14.0625 (15.4561)  acc5: 65.6250 (65.8080)  time: 0.0284  data: 0.0002  max mem: 4132
[18:55:48.424887] Test:  [120/157]  eta: 0:00:01  testing_loss: 2.2181 (2.2142)  acc1: 14.0625 (15.5217)  acc5: 64.0625 (65.7025)  time: 0.0284  data: 0.0002  max mem: 4132
[18:55:48.706018] Test:  [130/157]  eta: 0:00:00  testing_loss: 2.2205 (2.2147)  acc1: 15.6250 (15.5773)  acc5: 65.6250 (65.7323)  time: 0.0281  data: 0.0001  max mem: 4132
[18:55:48.988077] Test:  [140/157]  eta: 0:00:00  testing_loss: 2.2158 (2.2138)  acc1: 17.1875 (15.6472)  acc5: 67.1875 (65.9464)  time: 0.0280  data: 0.0001  max mem: 4132
[18:55:49.268155] Test:  [150/157]  eta: 0:00:00  testing_loss: 2.2006 (2.2131)  acc1: 17.1875 (15.7699)  acc5: 67.1875 (66.0079)  time: 0.0280  data: 0.0001  max mem: 4132
[18:55:49.418453] Test:  [156/157]  eta: 0:00:00  testing_loss: 2.2006 (2.2128)  acc1: 17.1875 (15.7500)  acc5: 67.1875 (66.0800)  time: 0.0270  data: 0.0001  max mem: 4132
[18:55:49.570433] Test: Total time: 0:00:05 (0.0330 s / it)
[18:55:49.571461] * Acc@1 15.750 Acc@5 66.080 loss 2.213
[18:55:49.571777] Accuracy of the network on the 10000 test images: 15.8%
[18:55:49.571957] Max accuracy: 25.08%
[18:55:49.830093] log_dir: ./output_dir
[18:55:50.631788] Epoch: [17]  [  0/781]  eta: 0:10:24  lr: 0.000240  training_loss: 0.4174 (0.4174)  classification_loss: 0.2268 (0.2268)  loss_mask: 0.1906 (0.1906)  time: 0.7998  data: 0.6007  max mem: 4132
[18:55:53.896995] Epoch: [17]  [ 20/781]  eta: 0:02:27  lr: 0.000240  training_loss: 0.4840 (0.4747)  classification_loss: 0.2276 (0.2279)  loss_mask: 0.2543 (0.2468)  time: 0.1632  data: 0.0002  max mem: 4132
[18:55:57.178451] Epoch: [17]  [ 40/781]  eta: 0:02:12  lr: 0.000240  training_loss: 0.4979 (0.4875)  classification_loss: 0.2271 (0.2278)  loss_mask: 0.2716 (0.2598)  time: 0.1640  data: 0.0003  max mem: 4132
[18:56:00.417055] Epoch: [17]  [ 60/781]  eta: 0:02:05  lr: 0.000240  training_loss: 0.4301 (0.4721)  classification_loss: 0.2264 (0.2275)  loss_mask: 0.2064 (0.2446)  time: 0.1619  data: 0.0002  max mem: 4132
[18:56:03.645975] Epoch: [17]  [ 80/781]  eta: 0:01:59  lr: 0.000240  training_loss: 0.4257 (0.4635)  classification_loss: 0.2264 (0.2276)  loss_mask: 0.2030 (0.2358)  time: 0.1614  data: 0.0003  max mem: 4132
[18:56:06.875273] Epoch: [17]  [100/781]  eta: 0:01:54  lr: 0.000240  training_loss: 0.4129 (0.4659)  classification_loss: 0.2269 (0.2274)  loss_mask: 0.1898 (0.2385)  time: 0.1614  data: 0.0003  max mem: 4132
[18:56:10.157093] Epoch: [17]  [120/781]  eta: 0:01:50  lr: 0.000240  training_loss: 0.4667 (0.4692)  classification_loss: 0.2261 (0.2272)  loss_mask: 0.2437 (0.2419)  time: 0.1640  data: 0.0003  max mem: 4132
[18:56:13.391364] Epoch: [17]  [140/781]  eta: 0:01:47  lr: 0.000240  training_loss: 0.4433 (0.4667)  classification_loss: 0.2253 (0.2270)  loss_mask: 0.2138 (0.2397)  time: 0.1616  data: 0.0002  max mem: 4132
[18:56:16.621479] Epoch: [17]  [160/781]  eta: 0:01:43  lr: 0.000240  training_loss: 0.4421 (0.4648)  classification_loss: 0.2269 (0.2270)  loss_mask: 0.2161 (0.2378)  time: 0.1614  data: 0.0002  max mem: 4132
[18:56:19.861972] Epoch: [17]  [180/781]  eta: 0:01:39  lr: 0.000240  training_loss: 0.3974 (0.4583)  classification_loss: 0.2267 (0.2270)  loss_mask: 0.1707 (0.2313)  time: 0.1620  data: 0.0003  max mem: 4132
[18:56:23.088108] Epoch: [17]  [200/781]  eta: 0:01:36  lr: 0.000240  training_loss: 0.4038 (0.4530)  classification_loss: 0.2255 (0.2269)  loss_mask: 0.1766 (0.2261)  time: 0.1612  data: 0.0002  max mem: 4132
[18:56:26.331047] Epoch: [17]  [220/781]  eta: 0:01:32  lr: 0.000240  training_loss: 0.4735 (0.4583)  classification_loss: 0.2295 (0.2273)  loss_mask: 0.2427 (0.2311)  time: 0.1621  data: 0.0002  max mem: 4132
[18:56:29.571214] Epoch: [17]  [240/781]  eta: 0:01:29  lr: 0.000240  training_loss: 0.4974 (0.4638)  classification_loss: 0.2271 (0.2273)  loss_mask: 0.2736 (0.2365)  time: 0.1619  data: 0.0002  max mem: 4132
[18:56:32.828953] Epoch: [17]  [260/781]  eta: 0:01:25  lr: 0.000240  training_loss: 0.4147 (0.4617)  classification_loss: 0.2270 (0.2273)  loss_mask: 0.1875 (0.2344)  time: 0.1628  data: 0.0002  max mem: 4132
[18:56:36.080818] Epoch: [17]  [280/781]  eta: 0:01:22  lr: 0.000240  training_loss: 0.4420 (0.4612)  classification_loss: 0.2270 (0.2273)  loss_mask: 0.2123 (0.2339)  time: 0.1625  data: 0.0002  max mem: 4132
[18:56:39.347779] Epoch: [17]  [300/781]  eta: 0:01:19  lr: 0.000240  training_loss: 0.5798 (0.4697)  classification_loss: 0.2263 (0.2272)  loss_mask: 0.3538 (0.2425)  time: 0.1632  data: 0.0004  max mem: 4132
[18:56:42.598198] Epoch: [17]  [320/781]  eta: 0:01:15  lr: 0.000240  training_loss: 0.4482 (0.4681)  classification_loss: 0.2260 (0.2272)  loss_mask: 0.2204 (0.2409)  time: 0.1625  data: 0.0002  max mem: 4132
[18:56:45.825402] Epoch: [17]  [340/781]  eta: 0:01:12  lr: 0.000240  training_loss: 0.4100 (0.4653)  classification_loss: 0.2279 (0.2272)  loss_mask: 0.1786 (0.2381)  time: 0.1613  data: 0.0002  max mem: 4132
[18:56:49.070159] Epoch: [17]  [360/781]  eta: 0:01:09  lr: 0.000240  training_loss: 0.3935 (0.4629)  classification_loss: 0.2265 (0.2272)  loss_mask: 0.1701 (0.2357)  time: 0.1622  data: 0.0002  max mem: 4132
[18:56:52.318346] Epoch: [17]  [380/781]  eta: 0:01:05  lr: 0.000240  training_loss: 0.4643 (0.4646)  classification_loss: 0.2270 (0.2272)  loss_mask: 0.2390 (0.2374)  time: 0.1623  data: 0.0002  max mem: 4132
[18:56:55.563380] Epoch: [17]  [400/781]  eta: 0:01:02  lr: 0.000239  training_loss: 0.5245 (0.4695)  classification_loss: 0.2268 (0.2273)  loss_mask: 0.2996 (0.2422)  time: 0.1622  data: 0.0002  max mem: 4132
[18:56:58.803977] Epoch: [17]  [420/781]  eta: 0:00:59  lr: 0.000239  training_loss: 0.4830 (0.4721)  classification_loss: 0.2284 (0.2274)  loss_mask: 0.2566 (0.2447)  time: 0.1620  data: 0.0002  max mem: 4132
[18:57:02.041651] Epoch: [17]  [440/781]  eta: 0:00:55  lr: 0.000239  training_loss: 0.4122 (0.4697)  classification_loss: 0.2264 (0.2273)  loss_mask: 0.1832 (0.2423)  time: 0.1618  data: 0.0002  max mem: 4132
[18:57:05.280643] Epoch: [17]  [460/781]  eta: 0:00:52  lr: 0.000239  training_loss: 0.3803 (0.4667)  classification_loss: 0.2259 (0.2273)  loss_mask: 0.1544 (0.2394)  time: 0.1619  data: 0.0002  max mem: 4132
[18:57:08.527269] Epoch: [17]  [480/781]  eta: 0:00:49  lr: 0.000239  training_loss: 0.4108 (0.4657)  classification_loss: 0.2263 (0.2273)  loss_mask: 0.1807 (0.2384)  time: 0.1622  data: 0.0002  max mem: 4132
[18:57:11.792484] Epoch: [17]  [500/781]  eta: 0:00:45  lr: 0.000239  training_loss: 0.3689 (0.4622)  classification_loss: 0.2268 (0.2273)  loss_mask: 0.1402 (0.2349)  time: 0.1632  data: 0.0002  max mem: 4132
[18:57:15.046100] Epoch: [17]  [520/781]  eta: 0:00:42  lr: 0.000239  training_loss: 0.3943 (0.4595)  classification_loss: 0.2273 (0.2273)  loss_mask: 0.1670 (0.2323)  time: 0.1626  data: 0.0002  max mem: 4132
[18:57:18.282731] Epoch: [17]  [540/781]  eta: 0:00:39  lr: 0.000239  training_loss: 0.3427 (0.4559)  classification_loss: 0.2273 (0.2273)  loss_mask: 0.1131 (0.2286)  time: 0.1618  data: 0.0003  max mem: 4132
[18:57:21.505424] Epoch: [17]  [560/781]  eta: 0:00:36  lr: 0.000239  training_loss: 0.3897 (0.4536)  classification_loss: 0.2266 (0.2272)  loss_mask: 0.1613 (0.2263)  time: 0.1610  data: 0.0003  max mem: 4132
[18:57:24.756905] Epoch: [17]  [580/781]  eta: 0:00:32  lr: 0.000239  training_loss: 0.3709 (0.4509)  classification_loss: 0.2269 (0.2272)  loss_mask: 0.1456 (0.2237)  time: 0.1624  data: 0.0002  max mem: 4132
[18:57:28.061852] Epoch: [17]  [600/781]  eta: 0:00:29  lr: 0.000239  training_loss: 0.4124 (0.4503)  classification_loss: 0.2271 (0.2272)  loss_mask: 0.1883 (0.2231)  time: 0.1652  data: 0.0002  max mem: 4132
[18:57:31.305127] Epoch: [17]  [620/781]  eta: 0:00:26  lr: 0.000239  training_loss: 0.4023 (0.4492)  classification_loss: 0.2271 (0.2272)  loss_mask: 0.1757 (0.2220)  time: 0.1621  data: 0.0002  max mem: 4132
[18:57:34.575270] Epoch: [17]  [640/781]  eta: 0:00:23  lr: 0.000239  training_loss: 0.3423 (0.4460)  classification_loss: 0.2271 (0.2272)  loss_mask: 0.1169 (0.2188)  time: 0.1634  data: 0.0002  max mem: 4132
[18:57:37.847389] Epoch: [17]  [660/781]  eta: 0:00:19  lr: 0.000239  training_loss: 0.3788 (0.4458)  classification_loss: 0.2257 (0.2272)  loss_mask: 0.1509 (0.2186)  time: 0.1635  data: 0.0002  max mem: 4132
[18:57:41.166041] Epoch: [17]  [680/781]  eta: 0:00:16  lr: 0.000239  training_loss: 0.3916 (0.4449)  classification_loss: 0.2276 (0.2272)  loss_mask: 0.1658 (0.2178)  time: 0.1659  data: 0.0004  max mem: 4132
[18:57:44.399128] Epoch: [17]  [700/781]  eta: 0:00:13  lr: 0.000239  training_loss: 0.3502 (0.4425)  classification_loss: 0.2265 (0.2272)  loss_mask: 0.1237 (0.2153)  time: 0.1616  data: 0.0004  max mem: 4132
[18:57:47.657487] Epoch: [17]  [720/781]  eta: 0:00:09  lr: 0.000239  training_loss: 0.3779 (0.4407)  classification_loss: 0.2271 (0.2272)  loss_mask: 0.1503 (0.2136)  time: 0.1628  data: 0.0002  max mem: 4132
[18:57:50.892452] Epoch: [17]  [740/781]  eta: 0:00:06  lr: 0.000239  training_loss: 0.3683 (0.4394)  classification_loss: 0.2285 (0.2272)  loss_mask: 0.1393 (0.2122)  time: 0.1617  data: 0.0003  max mem: 4132
[18:57:54.158646] Epoch: [17]  [760/781]  eta: 0:00:03  lr: 0.000239  training_loss: 0.4920 (0.4417)  classification_loss: 0.2275 (0.2272)  loss_mask: 0.2668 (0.2145)  time: 0.1632  data: 0.0002  max mem: 4132
[18:57:57.460779] Epoch: [17]  [780/781]  eta: 0:00:00  lr: 0.000239  training_loss: 0.4054 (0.4410)  classification_loss: 0.2273 (0.2272)  loss_mask: 0.1761 (0.2138)  time: 0.1650  data: 0.0002  max mem: 4132
[18:57:57.593867] Epoch: [17] Total time: 0:02:07 (0.1636 s / it)
[18:57:57.594354] Averaged stats: lr: 0.000239  training_loss: 0.4054 (0.4410)  classification_loss: 0.2273 (0.2272)  loss_mask: 0.1761 (0.2138)
[18:57:58.236914] Test:  [  0/157]  eta: 0:01:40  testing_loss: 2.1599 (2.1599)  acc1: 26.5625 (26.5625)  acc5: 71.8750 (71.8750)  time: 0.6379  data: 0.6075  max mem: 4132
[18:57:58.524275] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 2.1897 (2.1930)  acc1: 17.1875 (18.3239)  acc5: 70.3125 (70.3125)  time: 0.0839  data: 0.0554  max mem: 4132
[18:57:58.806459] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 2.1897 (2.1935)  acc1: 17.1875 (18.4524)  acc5: 68.7500 (69.2708)  time: 0.0282  data: 0.0002  max mem: 4132
[18:57:59.089414] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 2.1857 (2.1895)  acc1: 15.6250 (17.7419)  acc5: 68.7500 (69.4052)  time: 0.0281  data: 0.0001  max mem: 4132
[18:57:59.377265] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 2.1917 (2.1907)  acc1: 17.1875 (17.8354)  acc5: 68.7500 (69.3216)  time: 0.0284  data: 0.0002  max mem: 4132
[18:57:59.662256] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 2.1967 (2.1930)  acc1: 17.1875 (17.9534)  acc5: 67.1875 (68.9951)  time: 0.0285  data: 0.0002  max mem: 4132
[18:57:59.953212] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 2.1977 (2.1941)  acc1: 17.1875 (17.9047)  acc5: 67.1875 (68.8012)  time: 0.0286  data: 0.0002  max mem: 4132
[18:58:00.236874] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 2.1917 (2.1934)  acc1: 17.1875 (17.9137)  acc5: 68.7500 (68.9921)  time: 0.0285  data: 0.0002  max mem: 4132
[18:58:00.518907] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 2.1917 (2.1940)  acc1: 15.6250 (17.8434)  acc5: 68.7500 (68.7693)  time: 0.0281  data: 0.0002  max mem: 4132
[18:58:00.802424] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 2.2014 (2.1951)  acc1: 15.6250 (17.9945)  acc5: 67.1875 (68.3894)  time: 0.0281  data: 0.0002  max mem: 4132
[18:58:01.086055] Test:  [100/157]  eta: 0:00:01  testing_loss: 2.1809 (2.1943)  acc1: 18.7500 (18.0229)  acc5: 67.1875 (68.3787)  time: 0.0282  data: 0.0002  max mem: 4132
[18:58:01.369698] Test:  [110/157]  eta: 0:00:01  testing_loss: 2.1805 (2.1946)  acc1: 18.7500 (17.9758)  acc5: 68.7500 (68.4966)  time: 0.0282  data: 0.0002  max mem: 4132
[18:58:01.653017] Test:  [120/157]  eta: 0:00:01  testing_loss: 2.1948 (2.1939)  acc1: 18.7500 (18.0269)  acc5: 67.1875 (68.3239)  time: 0.0282  data: 0.0002  max mem: 4132
[18:58:01.945365] Test:  [130/157]  eta: 0:00:00  testing_loss: 2.1986 (2.1943)  acc1: 18.7500 (18.0463)  acc5: 68.7500 (68.3445)  time: 0.0287  data: 0.0002  max mem: 4132
[18:58:02.225434] Test:  [140/157]  eta: 0:00:00  testing_loss: 2.1889 (2.1930)  acc1: 18.7500 (18.1627)  acc5: 70.3125 (68.5505)  time: 0.0285  data: 0.0001  max mem: 4132
[18:58:02.504292] Test:  [150/157]  eta: 0:00:00  testing_loss: 2.1796 (2.1923)  acc1: 17.1875 (18.2533)  acc5: 70.3125 (68.6155)  time: 0.0278  data: 0.0001  max mem: 4132
[18:58:02.654240] Test:  [156/157]  eta: 0:00:00  testing_loss: 2.1784 (2.1916)  acc1: 18.7500 (18.3100)  acc5: 70.3125 (68.7300)  time: 0.0269  data: 0.0001  max mem: 4132
[18:58:02.807887] Test: Total time: 0:00:05 (0.0332 s / it)
[18:58:02.808348] * Acc@1 18.310 Acc@5 68.730 loss 2.192
[18:58:02.808675] Accuracy of the network on the 10000 test images: 18.3%
[18:58:02.808848] Max accuracy: 25.08%
[18:58:03.268880] log_dir: ./output_dir
[18:58:04.091305] Epoch: [18]  [  0/781]  eta: 0:10:40  lr: 0.000239  training_loss: 0.3236 (0.3236)  classification_loss: 0.2262 (0.2262)  loss_mask: 0.0973 (0.0973)  time: 0.8202  data: 0.6217  max mem: 4132
[18:58:07.339932] Epoch: [18]  [ 20/781]  eta: 0:02:27  lr: 0.000239  training_loss: 0.3471 (0.3523)  classification_loss: 0.2273 (0.2267)  loss_mask: 0.1183 (0.1256)  time: 0.1623  data: 0.0002  max mem: 4132
[18:58:10.593718] Epoch: [18]  [ 40/781]  eta: 0:02:12  lr: 0.000239  training_loss: 0.4034 (0.3839)  classification_loss: 0.2260 (0.2264)  loss_mask: 0.1791 (0.1574)  time: 0.1626  data: 0.0003  max mem: 4132
[18:58:13.850655] Epoch: [18]  [ 60/781]  eta: 0:02:04  lr: 0.000239  training_loss: 0.3717 (0.3820)  classification_loss: 0.2265 (0.2266)  loss_mask: 0.1397 (0.1555)  time: 0.1628  data: 0.0003  max mem: 4132
[18:58:17.072953] Epoch: [18]  [ 80/781]  eta: 0:01:59  lr: 0.000238  training_loss: 0.3555 (0.3863)  classification_loss: 0.2274 (0.2267)  loss_mask: 0.1372 (0.1596)  time: 0.1610  data: 0.0002  max mem: 4132
[18:58:20.314490] Epoch: [18]  [100/781]  eta: 0:01:54  lr: 0.000238  training_loss: 0.3672 (0.3832)  classification_loss: 0.2278 (0.2267)  loss_mask: 0.1387 (0.1565)  time: 0.1620  data: 0.0005  max mem: 4132
[18:58:23.581711] Epoch: [18]  [120/781]  eta: 0:01:50  lr: 0.000238  training_loss: 0.3314 (0.3775)  classification_loss: 0.2249 (0.2265)  loss_mask: 0.1053 (0.1510)  time: 0.1633  data: 0.0002  max mem: 4132
[18:58:26.834567] Epoch: [18]  [140/781]  eta: 0:01:47  lr: 0.000238  training_loss: 0.3539 (0.3767)  classification_loss: 0.2256 (0.2264)  loss_mask: 0.1267 (0.1503)  time: 0.1626  data: 0.0002  max mem: 4132
[18:58:30.077154] Epoch: [18]  [160/781]  eta: 0:01:43  lr: 0.000238  training_loss: 0.3219 (0.3717)  classification_loss: 0.2266 (0.2265)  loss_mask: 0.0956 (0.1452)  time: 0.1621  data: 0.0002  max mem: 4132
[18:58:33.318019] Epoch: [18]  [180/781]  eta: 0:01:39  lr: 0.000238  training_loss: 0.3344 (0.3689)  classification_loss: 0.2267 (0.2265)  loss_mask: 0.1108 (0.1424)  time: 0.1620  data: 0.0002  max mem: 4132
[18:58:36.568834] Epoch: [18]  [200/781]  eta: 0:01:36  lr: 0.000238  training_loss: 0.4058 (0.3773)  classification_loss: 0.2253 (0.2265)  loss_mask: 0.1825 (0.1508)  time: 0.1625  data: 0.0003  max mem: 4132
[18:58:39.856489] Epoch: [18]  [220/781]  eta: 0:01:32  lr: 0.000238  training_loss: 0.3840 (0.3802)  classification_loss: 0.2273 (0.2266)  loss_mask: 0.1581 (0.1535)  time: 0.1643  data: 0.0002  max mem: 4132
[18:58:43.097657] Epoch: [18]  [240/781]  eta: 0:01:29  lr: 0.000238  training_loss: 0.3537 (0.3793)  classification_loss: 0.2247 (0.2266)  loss_mask: 0.1299 (0.1527)  time: 0.1620  data: 0.0002  max mem: 4132
[18:58:46.350196] Epoch: [18]  [260/781]  eta: 0:01:25  lr: 0.000238  training_loss: 0.3462 (0.3769)  classification_loss: 0.2264 (0.2266)  loss_mask: 0.1194 (0.1503)  time: 0.1625  data: 0.0002  max mem: 4132
[18:58:49.606005] Epoch: [18]  [280/781]  eta: 0:01:22  lr: 0.000238  training_loss: 0.3380 (0.3750)  classification_loss: 0.2267 (0.2267)  loss_mask: 0.1096 (0.1483)  time: 0.1627  data: 0.0003  max mem: 4132
[18:58:52.858604] Epoch: [18]  [300/781]  eta: 0:01:19  lr: 0.000238  training_loss: 0.3309 (0.3726)  classification_loss: 0.2263 (0.2267)  loss_mask: 0.1070 (0.1459)  time: 0.1625  data: 0.0002  max mem: 4132
[18:58:56.114333] Epoch: [18]  [320/781]  eta: 0:01:15  lr: 0.000238  training_loss: 0.3548 (0.3720)  classification_loss: 0.2275 (0.2267)  loss_mask: 0.1259 (0.1453)  time: 0.1627  data: 0.0002  max mem: 4132
[18:58:59.358413] Epoch: [18]  [340/781]  eta: 0:01:12  lr: 0.000238  training_loss: 0.3230 (0.3694)  classification_loss: 0.2255 (0.2267)  loss_mask: 0.0938 (0.1427)  time: 0.1621  data: 0.0002  max mem: 4132
[18:59:02.601086] Epoch: [18]  [360/781]  eta: 0:01:09  lr: 0.000238  training_loss: 0.3396 (0.3686)  classification_loss: 0.2255 (0.2267)  loss_mask: 0.1170 (0.1419)  time: 0.1620  data: 0.0002  max mem: 4132
[18:59:05.861354] Epoch: [18]  [380/781]  eta: 0:01:05  lr: 0.000238  training_loss: 0.3352 (0.3675)  classification_loss: 0.2257 (0.2267)  loss_mask: 0.1080 (0.1408)  time: 0.1629  data: 0.0002  max mem: 4132
[18:59:09.092984] Epoch: [18]  [400/781]  eta: 0:01:02  lr: 0.000238  training_loss: 0.4383 (0.3736)  classification_loss: 0.2255 (0.2267)  loss_mask: 0.2051 (0.1469)  time: 0.1615  data: 0.0002  max mem: 4132
[18:59:12.330204] Epoch: [18]  [420/781]  eta: 0:00:59  lr: 0.000238  training_loss: 0.4682 (0.3781)  classification_loss: 0.2268 (0.2267)  loss_mask: 0.2422 (0.1514)  time: 0.1618  data: 0.0002  max mem: 4132
[18:59:15.575510] Epoch: [18]  [440/781]  eta: 0:00:55  lr: 0.000238  training_loss: 0.3988 (0.3796)  classification_loss: 0.2268 (0.2267)  loss_mask: 0.1711 (0.1530)  time: 0.1621  data: 0.0002  max mem: 4132
[18:59:18.803169] Epoch: [18]  [460/781]  eta: 0:00:52  lr: 0.000238  training_loss: 0.4158 (0.3809)  classification_loss: 0.2260 (0.2267)  loss_mask: 0.1875 (0.1542)  time: 0.1613  data: 0.0002  max mem: 4132
[18:59:22.069133] Epoch: [18]  [480/781]  eta: 0:00:49  lr: 0.000238  training_loss: 0.3432 (0.3794)  classification_loss: 0.2263 (0.2267)  loss_mask: 0.1152 (0.1527)  time: 0.1632  data: 0.0003  max mem: 4132
[18:59:25.302708] Epoch: [18]  [500/781]  eta: 0:00:45  lr: 0.000238  training_loss: 0.3310 (0.3774)  classification_loss: 0.2254 (0.2267)  loss_mask: 0.1095 (0.1508)  time: 0.1616  data: 0.0003  max mem: 4132
[18:59:28.540317] Epoch: [18]  [520/781]  eta: 0:00:42  lr: 0.000238  training_loss: 0.3491 (0.3768)  classification_loss: 0.2274 (0.2267)  loss_mask: 0.1246 (0.1501)  time: 0.1618  data: 0.0002  max mem: 4132
[18:59:31.794991] Epoch: [18]  [540/781]  eta: 0:00:39  lr: 0.000237  training_loss: 0.3150 (0.3748)  classification_loss: 0.2271 (0.2268)  loss_mask: 0.0863 (0.1480)  time: 0.1626  data: 0.0002  max mem: 4132
[18:59:35.048824] Epoch: [18]  [560/781]  eta: 0:00:36  lr: 0.000237  training_loss: 0.3451 (0.3746)  classification_loss: 0.2269 (0.2267)  loss_mask: 0.1149 (0.1478)  time: 0.1626  data: 0.0003  max mem: 4132
[18:59:38.328986] Epoch: [18]  [580/781]  eta: 0:00:32  lr: 0.000237  training_loss: 0.3398 (0.3737)  classification_loss: 0.2270 (0.2268)  loss_mask: 0.1109 (0.1469)  time: 0.1639  data: 0.0003  max mem: 4132
[18:59:42.277148] Epoch: [18]  [600/781]  eta: 0:00:29  lr: 0.000237  training_loss: 0.3522 (0.3725)  classification_loss: 0.2268 (0.2268)  loss_mask: 0.1240 (0.1457)  time: 0.1973  data: 0.0005  max mem: 4132
[18:59:45.546783] Epoch: [18]  [620/781]  eta: 0:00:26  lr: 0.000237  training_loss: 0.3780 (0.3731)  classification_loss: 0.2279 (0.2269)  loss_mask: 0.1458 (0.1462)  time: 0.1634  data: 0.0003  max mem: 4132
[18:59:48.841935] Epoch: [18]  [640/781]  eta: 0:00:23  lr: 0.000237  training_loss: 0.3494 (0.3732)  classification_loss: 0.2263 (0.2268)  loss_mask: 0.1226 (0.1463)  time: 0.1647  data: 0.0003  max mem: 4132
[18:59:52.130522] Epoch: [18]  [660/781]  eta: 0:00:19  lr: 0.000237  training_loss: 0.3389 (0.3726)  classification_loss: 0.2247 (0.2268)  loss_mask: 0.1110 (0.1458)  time: 0.1643  data: 0.0002  max mem: 4132
[18:59:55.420723] Epoch: [18]  [680/781]  eta: 0:00:16  lr: 0.000237  training_loss: 0.3668 (0.3730)  classification_loss: 0.2262 (0.2268)  loss_mask: 0.1408 (0.1462)  time: 0.1644  data: 0.0002  max mem: 4132
[18:59:58.713775] Epoch: [18]  [700/781]  eta: 0:00:13  lr: 0.000237  training_loss: 0.3217 (0.3719)  classification_loss: 0.2253 (0.2268)  loss_mask: 0.0952 (0.1451)  time: 0.1645  data: 0.0003  max mem: 4132
[19:00:02.036060] Epoch: [18]  [720/781]  eta: 0:00:10  lr: 0.000237  training_loss: 0.2948 (0.3702)  classification_loss: 0.2254 (0.2268)  loss_mask: 0.0661 (0.1434)  time: 0.1660  data: 0.0002  max mem: 4132
[19:00:05.381415] Epoch: [18]  [740/781]  eta: 0:00:06  lr: 0.000237  training_loss: 0.3183 (0.3689)  classification_loss: 0.2247 (0.2268)  loss_mask: 0.0921 (0.1422)  time: 0.1672  data: 0.0004  max mem: 4132
[19:00:08.679099] Epoch: [18]  [760/781]  eta: 0:00:03  lr: 0.000237  training_loss: 0.3892 (0.3708)  classification_loss: 0.2257 (0.2267)  loss_mask: 0.1591 (0.1440)  time: 0.1648  data: 0.0003  max mem: 4132
[19:00:11.912665] Epoch: [18]  [780/781]  eta: 0:00:00  lr: 0.000237  training_loss: 0.3412 (0.3708)  classification_loss: 0.2255 (0.2267)  loss_mask: 0.1160 (0.1441)  time: 0.1616  data: 0.0002  max mem: 4132
[19:00:12.068437] Epoch: [18] Total time: 0:02:08 (0.1649 s / it)
[19:00:12.069077] Averaged stats: lr: 0.000237  training_loss: 0.3412 (0.3708)  classification_loss: 0.2255 (0.2267)  loss_mask: 0.1160 (0.1441)
[19:00:12.689131] Test:  [  0/157]  eta: 0:01:36  testing_loss: 2.1305 (2.1305)  acc1: 26.5625 (26.5625)  acc5: 70.3125 (70.3125)  time: 0.6157  data: 0.5849  max mem: 4132
[19:00:12.988562] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 2.1709 (2.1694)  acc1: 20.3125 (20.8807)  acc5: 68.7500 (70.0284)  time: 0.0822  data: 0.0535  max mem: 4132
[19:00:13.274975] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 2.1709 (2.1720)  acc1: 20.3125 (20.9821)  acc5: 68.7500 (69.4196)  time: 0.0286  data: 0.0003  max mem: 4132
[19:00:13.564921] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 2.1643 (2.1684)  acc1: 18.7500 (19.8589)  acc5: 68.7500 (69.8085)  time: 0.0287  data: 0.0002  max mem: 4132
[19:00:13.849456] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 2.1716 (2.1695)  acc1: 18.7500 (19.7790)  acc5: 70.3125 (70.3506)  time: 0.0286  data: 0.0002  max mem: 4132
[19:00:14.133482] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 2.1832 (2.1721)  acc1: 18.7500 (19.6385)  acc5: 68.7500 (70.0674)  time: 0.0283  data: 0.0002  max mem: 4132
[19:00:14.424604] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 2.1805 (2.1735)  acc1: 17.1875 (19.6977)  acc5: 68.7500 (70.0820)  time: 0.0286  data: 0.0002  max mem: 4132
[19:00:14.712283] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 2.1705 (2.1727)  acc1: 17.1875 (19.5202)  acc5: 70.3125 (70.2905)  time: 0.0288  data: 0.0002  max mem: 4132
[19:00:15.004470] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 2.1728 (2.1734)  acc1: 17.1875 (19.5023)  acc5: 70.3125 (70.1775)  time: 0.0288  data: 0.0002  max mem: 4132
[19:00:15.292439] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 2.1880 (2.1751)  acc1: 17.1875 (19.3853)  acc5: 67.1875 (69.7115)  time: 0.0288  data: 0.0002  max mem: 4132
[19:00:15.576954] Test:  [100/157]  eta: 0:00:01  testing_loss: 2.1601 (2.1744)  acc1: 18.7500 (19.3069)  acc5: 68.7500 (69.7092)  time: 0.0284  data: 0.0002  max mem: 4132
[19:00:15.860087] Test:  [110/157]  eta: 0:00:01  testing_loss: 2.1601 (2.1745)  acc1: 17.1875 (19.1160)  acc5: 70.3125 (69.8761)  time: 0.0282  data: 0.0002  max mem: 4132
[19:00:16.143142] Test:  [120/157]  eta: 0:00:01  testing_loss: 2.1768 (2.1735)  acc1: 17.1875 (19.1245)  acc5: 71.8750 (69.8735)  time: 0.0282  data: 0.0002  max mem: 4132
[19:00:16.428342] Test:  [130/157]  eta: 0:00:00  testing_loss: 2.1768 (2.1737)  acc1: 18.7500 (19.2032)  acc5: 70.3125 (69.8712)  time: 0.0283  data: 0.0002  max mem: 4132
[19:00:16.717143] Test:  [140/157]  eta: 0:00:00  testing_loss: 2.1667 (2.1723)  acc1: 20.3125 (19.2265)  acc5: 70.3125 (70.0022)  time: 0.0285  data: 0.0002  max mem: 4132
[19:00:17.000240] Test:  [150/157]  eta: 0:00:00  testing_loss: 2.1523 (2.1714)  acc1: 20.3125 (19.2260)  acc5: 71.8750 (70.1159)  time: 0.0284  data: 0.0002  max mem: 4132
[19:00:17.152328] Test:  [156/157]  eta: 0:00:00  testing_loss: 2.1523 (2.1705)  acc1: 20.3125 (19.2000)  acc5: 71.8750 (70.3000)  time: 0.0272  data: 0.0001  max mem: 4132
[19:00:17.302984] Test: Total time: 0:00:05 (0.0333 s / it)
[19:00:17.303458] * Acc@1 19.200 Acc@5 70.300 loss 2.170
[19:00:17.303857] Accuracy of the network on the 10000 test images: 19.2%
[19:00:17.304154] Max accuracy: 25.08%
[19:00:17.695874] log_dir: ./output_dir
[19:00:18.529854] Epoch: [19]  [  0/781]  eta: 0:10:49  lr: 0.000237  training_loss: 0.3084 (0.3084)  classification_loss: 0.2260 (0.2260)  loss_mask: 0.0824 (0.0824)  time: 0.8321  data: 0.6641  max mem: 4132
[19:00:21.794214] Epoch: [19]  [ 20/781]  eta: 0:02:28  lr: 0.000237  training_loss: 0.3228 (0.3266)  classification_loss: 0.2268 (0.2265)  loss_mask: 0.1022 (0.1001)  time: 0.1631  data: 0.0002  max mem: 4132
[19:00:25.068233] Epoch: [19]  [ 40/781]  eta: 0:02:13  lr: 0.000237  training_loss: 0.3105 (0.3257)  classification_loss: 0.2254 (0.2261)  loss_mask: 0.0852 (0.0997)  time: 0.1636  data: 0.0002  max mem: 4132
[19:00:28.323730] Epoch: [19]  [ 60/781]  eta: 0:02:05  lr: 0.000237  training_loss: 0.4845 (0.4173)  classification_loss: 0.2255 (0.2262)  loss_mask: 0.2592 (0.1912)  time: 0.1627  data: 0.0003  max mem: 4132
[19:00:31.552108] Epoch: [19]  [ 80/781]  eta: 0:01:59  lr: 0.000237  training_loss: 0.4916 (0.4418)  classification_loss: 0.2253 (0.2261)  loss_mask: 0.2643 (0.2157)  time: 0.1613  data: 0.0002  max mem: 4132
[19:00:34.826009] Epoch: [19]  [100/781]  eta: 0:01:55  lr: 0.000237  training_loss: 0.4142 (0.4391)  classification_loss: 0.2263 (0.2261)  loss_mask: 0.1857 (0.2130)  time: 0.1636  data: 0.0003  max mem: 4132
[19:00:38.103429] Epoch: [19]  [120/781]  eta: 0:01:51  lr: 0.000237  training_loss: 0.3540 (0.4294)  classification_loss: 0.2249 (0.2259)  loss_mask: 0.1289 (0.2035)  time: 0.1638  data: 0.0004  max mem: 4132
[19:00:41.363258] Epoch: [19]  [140/781]  eta: 0:01:47  lr: 0.000237  training_loss: 0.3429 (0.4193)  classification_loss: 0.2239 (0.2257)  loss_mask: 0.1186 (0.1936)  time: 0.1629  data: 0.0002  max mem: 4132
[19:00:44.599315] Epoch: [19]  [160/781]  eta: 0:01:43  lr: 0.000237  training_loss: 0.3555 (0.4115)  classification_loss: 0.2258 (0.2258)  loss_mask: 0.1273 (0.1857)  time: 0.1617  data: 0.0002  max mem: 4132
[19:00:47.842246] Epoch: [19]  [180/781]  eta: 0:01:40  lr: 0.000236  training_loss: 0.3407 (0.4056)  classification_loss: 0.2266 (0.2259)  loss_mask: 0.1174 (0.1797)  time: 0.1621  data: 0.0002  max mem: 4132
[19:00:51.078024] Epoch: [19]  [200/781]  eta: 0:01:36  lr: 0.000236  training_loss: 0.3501 (0.4032)  classification_loss: 0.2250 (0.2259)  loss_mask: 0.1307 (0.1773)  time: 0.1617  data: 0.0003  max mem: 4132
[19:00:54.349814] Epoch: [19]  [220/781]  eta: 0:01:32  lr: 0.000236  training_loss: 0.4164 (0.4058)  classification_loss: 0.2272 (0.2260)  loss_mask: 0.1966 (0.1797)  time: 0.1635  data: 0.0002  max mem: 4132
[19:00:57.604204] Epoch: [19]  [240/781]  eta: 0:01:29  lr: 0.000236  training_loss: 0.3478 (0.4031)  classification_loss: 0.2252 (0.2260)  loss_mask: 0.1212 (0.1771)  time: 0.1626  data: 0.0002  max mem: 4132
[19:01:00.889862] Epoch: [19]  [260/781]  eta: 0:01:26  lr: 0.000236  training_loss: 0.3352 (0.3983)  classification_loss: 0.2266 (0.2260)  loss_mask: 0.1131 (0.1723)  time: 0.1642  data: 0.0002  max mem: 4132
[19:01:04.183358] Epoch: [19]  [280/781]  eta: 0:01:22  lr: 0.000236  training_loss: 0.3169 (0.3929)  classification_loss: 0.2255 (0.2260)  loss_mask: 0.0932 (0.1669)  time: 0.1645  data: 0.0002  max mem: 4132
[19:01:07.470540] Epoch: [19]  [300/781]  eta: 0:01:19  lr: 0.000236  training_loss: 0.3205 (0.3887)  classification_loss: 0.2259 (0.2260)  loss_mask: 0.0947 (0.1626)  time: 0.1643  data: 0.0002  max mem: 4132
[19:01:10.727556] Epoch: [19]  [320/781]  eta: 0:01:16  lr: 0.000236  training_loss: 0.3039 (0.3841)  classification_loss: 0.2247 (0.2260)  loss_mask: 0.0799 (0.1581)  time: 0.1628  data: 0.0002  max mem: 4132
[19:01:13.983609] Epoch: [19]  [340/781]  eta: 0:01:12  lr: 0.000236  training_loss: 0.3289 (0.3809)  classification_loss: 0.2268 (0.2260)  loss_mask: 0.0991 (0.1548)  time: 0.1627  data: 0.0003  max mem: 4132
[19:01:17.244368] Epoch: [19]  [360/781]  eta: 0:01:09  lr: 0.000236  training_loss: 0.3062 (0.3775)  classification_loss: 0.2260 (0.2260)  loss_mask: 0.0790 (0.1514)  time: 0.1630  data: 0.0002  max mem: 4132
[19:01:20.519103] Epoch: [19]  [380/781]  eta: 0:01:06  lr: 0.000236  training_loss: 0.3316 (0.3766)  classification_loss: 0.2277 (0.2261)  loss_mask: 0.1038 (0.1505)  time: 0.1637  data: 0.0002  max mem: 4132
[19:01:23.810052] Epoch: [19]  [400/781]  eta: 0:01:02  lr: 0.000236  training_loss: 0.3455 (0.3754)  classification_loss: 0.2263 (0.2262)  loss_mask: 0.1161 (0.1492)  time: 0.1645  data: 0.0002  max mem: 4132
[19:01:27.075945] Epoch: [19]  [420/781]  eta: 0:00:59  lr: 0.000236  training_loss: 0.3075 (0.3726)  classification_loss: 0.2259 (0.2262)  loss_mask: 0.0834 (0.1464)  time: 0.1632  data: 0.0002  max mem: 4132
[19:01:30.337895] Epoch: [19]  [440/781]  eta: 0:00:56  lr: 0.000236  training_loss: 0.3094 (0.3704)  classification_loss: 0.2264 (0.2262)  loss_mask: 0.0768 (0.1442)  time: 0.1630  data: 0.0002  max mem: 4132
[19:01:33.575050] Epoch: [19]  [460/781]  eta: 0:00:52  lr: 0.000236  training_loss: 0.3202 (0.3689)  classification_loss: 0.2258 (0.2261)  loss_mask: 0.0939 (0.1428)  time: 0.1618  data: 0.0002  max mem: 4132
[19:01:36.818620] Epoch: [19]  [480/781]  eta: 0:00:49  lr: 0.000236  training_loss: 0.3111 (0.3671)  classification_loss: 0.2279 (0.2262)  loss_mask: 0.0833 (0.1408)  time: 0.1621  data: 0.0002  max mem: 4132
[19:01:40.031888] Epoch: [19]  [500/781]  eta: 0:00:46  lr: 0.000236  training_loss: 0.3033 (0.3650)  classification_loss: 0.2252 (0.2262)  loss_mask: 0.0741 (0.1388)  time: 0.1606  data: 0.0002  max mem: 4132
[19:01:43.260640] Epoch: [19]  [520/781]  eta: 0:00:42  lr: 0.000236  training_loss: 0.3811 (0.3662)  classification_loss: 0.2269 (0.2262)  loss_mask: 0.1525 (0.1399)  time: 0.1614  data: 0.0002  max mem: 4132
[19:01:46.517480] Epoch: [19]  [540/781]  eta: 0:00:39  lr: 0.000236  training_loss: 0.3669 (0.3673)  classification_loss: 0.2260 (0.2263)  loss_mask: 0.1447 (0.1410)  time: 0.1628  data: 0.0002  max mem: 4132
[19:01:49.784314] Epoch: [19]  [560/781]  eta: 0:00:36  lr: 0.000236  training_loss: 0.3312 (0.3664)  classification_loss: 0.2255 (0.2262)  loss_mask: 0.0985 (0.1402)  time: 0.1632  data: 0.0002  max mem: 4132
[19:01:53.032668] Epoch: [19]  [580/781]  eta: 0:00:32  lr: 0.000235  training_loss: 0.3137 (0.3650)  classification_loss: 0.2244 (0.2262)  loss_mask: 0.0896 (0.1387)  time: 0.1623  data: 0.0002  max mem: 4132
[19:01:56.298689] Epoch: [19]  [600/781]  eta: 0:00:29  lr: 0.000235  training_loss: 0.2804 (0.3624)  classification_loss: 0.2270 (0.2262)  loss_mask: 0.0545 (0.1362)  time: 0.1632  data: 0.0002  max mem: 4132
[19:01:59.624920] Epoch: [19]  [620/781]  eta: 0:00:26  lr: 0.000235  training_loss: 0.2992 (0.3606)  classification_loss: 0.2260 (0.2262)  loss_mask: 0.0758 (0.1343)  time: 0.1662  data: 0.0002  max mem: 4132
[19:02:02.878193] Epoch: [19]  [640/781]  eta: 0:00:23  lr: 0.000235  training_loss: 0.3297 (0.3603)  classification_loss: 0.2268 (0.2263)  loss_mask: 0.1048 (0.1340)  time: 0.1626  data: 0.0003  max mem: 4132
[19:02:06.162246] Epoch: [19]  [660/781]  eta: 0:00:19  lr: 0.000235  training_loss: 0.4045 (0.3623)  classification_loss: 0.2243 (0.2263)  loss_mask: 0.1813 (0.1361)  time: 0.1641  data: 0.0004  max mem: 4132
[19:02:09.418524] Epoch: [19]  [680/781]  eta: 0:00:16  lr: 0.000235  training_loss: 0.3338 (0.3615)  classification_loss: 0.2254 (0.2262)  loss_mask: 0.1066 (0.1352)  time: 0.1627  data: 0.0003  max mem: 4132
[19:02:12.702529] Epoch: [19]  [700/781]  eta: 0:00:13  lr: 0.000235  training_loss: 0.3032 (0.3604)  classification_loss: 0.2252 (0.2262)  loss_mask: 0.0783 (0.1342)  time: 0.1641  data: 0.0002  max mem: 4132
[19:02:15.999669] Epoch: [19]  [720/781]  eta: 0:00:10  lr: 0.000235  training_loss: 0.2931 (0.3585)  classification_loss: 0.2269 (0.2262)  loss_mask: 0.0648 (0.1323)  time: 0.1648  data: 0.0002  max mem: 4132
[19:02:19.250251] Epoch: [19]  [740/781]  eta: 0:00:06  lr: 0.000235  training_loss: 0.2813 (0.3566)  classification_loss: 0.2274 (0.2263)  loss_mask: 0.0516 (0.1303)  time: 0.1624  data: 0.0002  max mem: 4132
[19:02:22.520279] Epoch: [19]  [760/781]  eta: 0:00:03  lr: 0.000235  training_loss: 0.2898 (0.3549)  classification_loss: 0.2261 (0.2263)  loss_mask: 0.0651 (0.1286)  time: 0.1634  data: 0.0005  max mem: 4132
[19:02:25.738707] Epoch: [19]  [780/781]  eta: 0:00:00  lr: 0.000235  training_loss: 0.2887 (0.3536)  classification_loss: 0.2270 (0.2263)  loss_mask: 0.0646 (0.1273)  time: 0.1608  data: 0.0002  max mem: 4132
[19:02:25.895288] Epoch: [19] Total time: 0:02:08 (0.1641 s / it)
[19:02:25.896063] Averaged stats: lr: 0.000235  training_loss: 0.2887 (0.3536)  classification_loss: 0.2270 (0.2263)  loss_mask: 0.0646 (0.1273)
[19:02:26.530949] Test:  [  0/157]  eta: 0:01:39  testing_loss: 2.1384 (2.1384)  acc1: 34.3750 (34.3750)  acc5: 73.4375 (73.4375)  time: 0.6307  data: 0.6015  max mem: 4132
[19:02:26.819983] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 2.1906 (2.1877)  acc1: 21.8750 (20.5966)  acc5: 67.1875 (67.7557)  time: 0.0834  data: 0.0548  max mem: 4132
[19:02:27.103187] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 2.1906 (2.1907)  acc1: 20.3125 (20.5357)  acc5: 67.1875 (68.2292)  time: 0.0284  data: 0.0002  max mem: 4132
[19:02:27.385122] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 2.1881 (2.1847)  acc1: 20.3125 (19.8085)  acc5: 67.1875 (68.2460)  time: 0.0281  data: 0.0002  max mem: 4132
[19:02:27.668688] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 2.1810 (2.1863)  acc1: 17.1875 (19.4360)  acc5: 67.1875 (67.7973)  time: 0.0281  data: 0.0002  max mem: 4132
[19:02:27.951342] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 2.1922 (2.1887)  acc1: 17.1875 (18.7806)  acc5: 65.6250 (67.5245)  time: 0.0282  data: 0.0002  max mem: 4132
[19:02:28.238955] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 2.1918 (2.1890)  acc1: 15.6250 (18.6732)  acc5: 65.6250 (67.3156)  time: 0.0284  data: 0.0002  max mem: 4132
[19:02:28.524337] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 2.1896 (2.1894)  acc1: 15.6250 (18.4419)  acc5: 67.1875 (67.5616)  time: 0.0285  data: 0.0002  max mem: 4132
[19:02:28.809165] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 2.1896 (2.1886)  acc1: 15.6250 (18.3449)  acc5: 68.7500 (67.4576)  time: 0.0284  data: 0.0002  max mem: 4132
[19:02:29.092767] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 2.1861 (2.1891)  acc1: 15.6250 (18.2177)  acc5: 67.1875 (67.3249)  time: 0.0283  data: 0.0002  max mem: 4132
[19:02:29.376876] Test:  [100/157]  eta: 0:00:01  testing_loss: 2.1795 (2.1886)  acc1: 17.1875 (18.0229)  acc5: 68.7500 (67.2803)  time: 0.0283  data: 0.0002  max mem: 4132
[19:02:29.660821] Test:  [110/157]  eta: 0:00:01  testing_loss: 2.1853 (2.1891)  acc1: 18.7500 (18.0321)  acc5: 68.7500 (67.2157)  time: 0.0283  data: 0.0002  max mem: 4132
[19:02:29.942185] Test:  [120/157]  eta: 0:00:01  testing_loss: 2.1911 (2.1881)  acc1: 18.7500 (17.9494)  acc5: 65.6250 (67.1358)  time: 0.0281  data: 0.0002  max mem: 4132
[19:02:30.224023] Test:  [130/157]  eta: 0:00:00  testing_loss: 2.1933 (2.1887)  acc1: 17.1875 (17.9389)  acc5: 65.6250 (67.0682)  time: 0.0280  data: 0.0002  max mem: 4132
[19:02:30.505423] Test:  [140/157]  eta: 0:00:00  testing_loss: 2.1870 (2.1874)  acc1: 18.7500 (18.1738)  acc5: 67.1875 (67.2983)  time: 0.0280  data: 0.0002  max mem: 4132
[19:02:30.787186] Test:  [150/157]  eta: 0:00:00  testing_loss: 2.1762 (2.1866)  acc1: 18.7500 (18.1291)  acc5: 68.7500 (67.3738)  time: 0.0280  data: 0.0002  max mem: 4132
[19:02:30.938766] Test:  [156/157]  eta: 0:00:00  testing_loss: 2.1776 (2.1863)  acc1: 17.1875 (18.0800)  acc5: 68.7500 (67.4700)  time: 0.0271  data: 0.0001  max mem: 4132
[19:02:31.093717] Test: Total time: 0:00:05 (0.0331 s / it)
[19:02:31.094268] * Acc@1 18.080 Acc@5 67.470 loss 2.186
[19:02:31.094643] Accuracy of the network on the 10000 test images: 18.1%
[19:02:31.094904] Max accuracy: 25.08%
[19:02:31.454145] log_dir: ./output_dir
[19:02:32.221897] Epoch: [20]  [  0/781]  eta: 0:09:58  lr: 0.000235  training_loss: 0.3672 (0.3672)  classification_loss: 0.2254 (0.2254)  loss_mask: 0.1418 (0.1418)  time: 0.7658  data: 0.5932  max mem: 4132
[19:02:35.493976] Epoch: [20]  [ 20/781]  eta: 0:02:26  lr: 0.000235  training_loss: 0.3078 (0.3148)  classification_loss: 0.2261 (0.2267)  loss_mask: 0.0809 (0.0881)  time: 0.1635  data: 0.0002  max mem: 4132
[19:02:38.806904] Epoch: [20]  [ 40/781]  eta: 0:02:12  lr: 0.000235  training_loss: 0.3076 (0.3129)  classification_loss: 0.2248 (0.2258)  loss_mask: 0.0842 (0.0871)  time: 0.1656  data: 0.0005  max mem: 4132
[19:02:42.059262] Epoch: [20]  [ 60/781]  eta: 0:02:05  lr: 0.000235  training_loss: 0.2959 (0.3155)  classification_loss: 0.2261 (0.2260)  loss_mask: 0.0725 (0.0895)  time: 0.1625  data: 0.0002  max mem: 4132
[19:02:45.302831] Epoch: [20]  [ 80/781]  eta: 0:01:59  lr: 0.000235  training_loss: 0.2948 (0.3159)  classification_loss: 0.2245 (0.2262)  loss_mask: 0.0704 (0.0897)  time: 0.1621  data: 0.0002  max mem: 4132
[19:02:48.554969] Epoch: [20]  [100/781]  eta: 0:01:55  lr: 0.000235  training_loss: 0.3659 (0.3381)  classification_loss: 0.2259 (0.2262)  loss_mask: 0.1420 (0.1119)  time: 0.1625  data: 0.0006  max mem: 4132
[19:02:51.824979] Epoch: [20]  [120/781]  eta: 0:01:51  lr: 0.000235  training_loss: 0.3106 (0.3358)  classification_loss: 0.2247 (0.2259)  loss_mask: 0.0864 (0.1099)  time: 0.1634  data: 0.0002  max mem: 4132
[19:02:55.121567] Epoch: [20]  [140/781]  eta: 0:01:47  lr: 0.000235  training_loss: 0.2836 (0.3303)  classification_loss: 0.2246 (0.2259)  loss_mask: 0.0535 (0.1045)  time: 0.1647  data: 0.0002  max mem: 4132
[19:02:58.398008] Epoch: [20]  [160/781]  eta: 0:01:43  lr: 0.000235  training_loss: 0.2972 (0.3290)  classification_loss: 0.2260 (0.2260)  loss_mask: 0.0644 (0.1031)  time: 0.1637  data: 0.0003  max mem: 4132
[19:03:01.681397] Epoch: [20]  [180/781]  eta: 0:01:40  lr: 0.000235  training_loss: 0.2909 (0.3255)  classification_loss: 0.2250 (0.2260)  loss_mask: 0.0660 (0.0995)  time: 0.1641  data: 0.0002  max mem: 4132
[19:03:04.971103] Epoch: [20]  [200/781]  eta: 0:01:36  lr: 0.000234  training_loss: 0.3005 (0.3245)  classification_loss: 0.2249 (0.2259)  loss_mask: 0.0755 (0.0986)  time: 0.1644  data: 0.0005  max mem: 4132
[19:03:08.226646] Epoch: [20]  [220/781]  eta: 0:01:33  lr: 0.000234  training_loss: 0.2932 (0.3229)  classification_loss: 0.2258 (0.2259)  loss_mask: 0.0656 (0.0969)  time: 0.1626  data: 0.0002  max mem: 4132
[19:03:11.466259] Epoch: [20]  [240/781]  eta: 0:01:29  lr: 0.000234  training_loss: 0.2862 (0.3201)  classification_loss: 0.2254 (0.2259)  loss_mask: 0.0601 (0.0942)  time: 0.1619  data: 0.0003  max mem: 4132
[19:03:14.742149] Epoch: [20]  [260/781]  eta: 0:01:26  lr: 0.000234  training_loss: 0.2661 (0.3163)  classification_loss: 0.2265 (0.2259)  loss_mask: 0.0417 (0.0905)  time: 0.1637  data: 0.0002  max mem: 4132
[19:03:17.979885] Epoch: [20]  [280/781]  eta: 0:01:22  lr: 0.000234  training_loss: 0.2923 (0.3164)  classification_loss: 0.2266 (0.2259)  loss_mask: 0.0655 (0.0905)  time: 0.1618  data: 0.0002  max mem: 4132
[19:03:21.237329] Epoch: [20]  [300/781]  eta: 0:01:19  lr: 0.000234  training_loss: 0.3071 (0.3173)  classification_loss: 0.2249 (0.2259)  loss_mask: 0.0831 (0.0914)  time: 0.1628  data: 0.0002  max mem: 4132
[19:03:24.518661] Epoch: [20]  [320/781]  eta: 0:01:16  lr: 0.000234  training_loss: 0.3171 (0.3181)  classification_loss: 0.2254 (0.2259)  loss_mask: 0.0900 (0.0922)  time: 0.1640  data: 0.0002  max mem: 4132
[19:03:27.791859] Epoch: [20]  [340/781]  eta: 0:01:12  lr: 0.000234  training_loss: 0.3806 (0.3225)  classification_loss: 0.2247 (0.2259)  loss_mask: 0.1525 (0.0965)  time: 0.1636  data: 0.0005  max mem: 4132
[19:03:31.040830] Epoch: [20]  [360/781]  eta: 0:01:09  lr: 0.000234  training_loss: 0.4027 (0.3283)  classification_loss: 0.2260 (0.2260)  loss_mask: 0.1706 (0.1023)  time: 0.1624  data: 0.0002  max mem: 4132
[19:03:34.290692] Epoch: [20]  [380/781]  eta: 0:01:06  lr: 0.000234  training_loss: 0.3984 (0.3341)  classification_loss: 0.2263 (0.2260)  loss_mask: 0.1677 (0.1081)  time: 0.1624  data: 0.0002  max mem: 4132
[19:03:37.565621] Epoch: [20]  [400/781]  eta: 0:01:02  lr: 0.000234  training_loss: 0.4055 (0.3385)  classification_loss: 0.2284 (0.2261)  loss_mask: 0.1810 (0.1124)  time: 0.1637  data: 0.0002  max mem: 4132
[19:03:40.838865] Epoch: [20]  [420/781]  eta: 0:00:59  lr: 0.000234  training_loss: 0.3222 (0.3386)  classification_loss: 0.2272 (0.2261)  loss_mask: 0.0939 (0.1124)  time: 0.1636  data: 0.0003  max mem: 4132
[19:03:44.081348] Epoch: [20]  [440/781]  eta: 0:00:56  lr: 0.000234  training_loss: 0.3188 (0.3378)  classification_loss: 0.2256 (0.2261)  loss_mask: 0.0939 (0.1117)  time: 0.1620  data: 0.0002  max mem: 4132
[19:03:47.329649] Epoch: [20]  [460/781]  eta: 0:00:52  lr: 0.000234  training_loss: 0.3150 (0.3379)  classification_loss: 0.2266 (0.2261)  loss_mask: 0.0888 (0.1118)  time: 0.1623  data: 0.0002  max mem: 4132
[19:03:50.577210] Epoch: [20]  [480/781]  eta: 0:00:49  lr: 0.000234  training_loss: 0.2965 (0.3363)  classification_loss: 0.2270 (0.2261)  loss_mask: 0.0670 (0.1102)  time: 0.1623  data: 0.0002  max mem: 4132
[19:03:53.841383] Epoch: [20]  [500/781]  eta: 0:00:46  lr: 0.000234  training_loss: 0.2956 (0.3349)  classification_loss: 0.2261 (0.2261)  loss_mask: 0.0722 (0.1088)  time: 0.1631  data: 0.0002  max mem: 4132
[19:03:57.107948] Epoch: [20]  [520/781]  eta: 0:00:42  lr: 0.000234  training_loss: 0.3191 (0.3345)  classification_loss: 0.2268 (0.2261)  loss_mask: 0.0926 (0.1083)  time: 0.1633  data: 0.0002  max mem: 4132
[19:04:00.359109] Epoch: [20]  [540/781]  eta: 0:00:39  lr: 0.000234  training_loss: 0.3180 (0.3347)  classification_loss: 0.2260 (0.2262)  loss_mask: 0.0869 (0.1085)  time: 0.1625  data: 0.0002  max mem: 4132
[19:04:03.609119] Epoch: [20]  [560/781]  eta: 0:00:36  lr: 0.000234  training_loss: 0.3651 (0.3371)  classification_loss: 0.2241 (0.2261)  loss_mask: 0.1422 (0.1110)  time: 0.1624  data: 0.0002  max mem: 4132
[19:04:06.857975] Epoch: [20]  [580/781]  eta: 0:00:32  lr: 0.000234  training_loss: 0.3160 (0.3365)  classification_loss: 0.2258 (0.2261)  loss_mask: 0.0864 (0.1104)  time: 0.1623  data: 0.0002  max mem: 4132
[19:04:10.111021] Epoch: [20]  [600/781]  eta: 0:00:29  lr: 0.000233  training_loss: 0.3005 (0.3353)  classification_loss: 0.2265 (0.2261)  loss_mask: 0.0723 (0.1091)  time: 0.1625  data: 0.0002  max mem: 4132
[19:04:13.365780] Epoch: [20]  [620/781]  eta: 0:00:26  lr: 0.000233  training_loss: 0.2841 (0.3339)  classification_loss: 0.2268 (0.2261)  loss_mask: 0.0568 (0.1077)  time: 0.1627  data: 0.0004  max mem: 4132
[19:04:16.621420] Epoch: [20]  [640/781]  eta: 0:00:23  lr: 0.000233  training_loss: 0.2951 (0.3329)  classification_loss: 0.2270 (0.2261)  loss_mask: 0.0732 (0.1067)  time: 0.1627  data: 0.0002  max mem: 4132
[19:04:19.896041] Epoch: [20]  [660/781]  eta: 0:00:19  lr: 0.000233  training_loss: 0.2676 (0.3311)  classification_loss: 0.2242 (0.2261)  loss_mask: 0.0448 (0.1050)  time: 0.1636  data: 0.0002  max mem: 4132
[19:04:23.217008] Epoch: [20]  [680/781]  eta: 0:00:16  lr: 0.000233  training_loss: 0.2911 (0.3302)  classification_loss: 0.2253 (0.2261)  loss_mask: 0.0664 (0.1041)  time: 0.1660  data: 0.0002  max mem: 4132
[19:04:26.504955] Epoch: [20]  [700/781]  eta: 0:00:13  lr: 0.000233  training_loss: 0.2749 (0.3286)  classification_loss: 0.2249 (0.2261)  loss_mask: 0.0483 (0.1025)  time: 0.1643  data: 0.0002  max mem: 4132
[19:04:29.764201] Epoch: [20]  [720/781]  eta: 0:00:10  lr: 0.000233  training_loss: 0.2727 (0.3274)  classification_loss: 0.2272 (0.2261)  loss_mask: 0.0442 (0.1012)  time: 0.1629  data: 0.0002  max mem: 4132
[19:04:33.010936] Epoch: [20]  [740/781]  eta: 0:00:06  lr: 0.000233  training_loss: 0.2916 (0.3268)  classification_loss: 0.2263 (0.2261)  loss_mask: 0.0645 (0.1006)  time: 0.1623  data: 0.0002  max mem: 4132
[19:04:36.257099] Epoch: [20]  [760/781]  eta: 0:00:03  lr: 0.000233  training_loss: 0.2792 (0.3259)  classification_loss: 0.2238 (0.2261)  loss_mask: 0.0564 (0.0998)  time: 0.1622  data: 0.0002  max mem: 4132
[19:04:39.543958] Epoch: [20]  [780/781]  eta: 0:00:00  lr: 0.000233  training_loss: 0.2648 (0.3246)  classification_loss: 0.2261 (0.2261)  loss_mask: 0.0404 (0.0985)  time: 0.1643  data: 0.0002  max mem: 4132
[19:04:39.699107] Epoch: [20] Total time: 0:02:08 (0.1642 s / it)
[19:04:39.699846] Averaged stats: lr: 0.000233  training_loss: 0.2648 (0.3246)  classification_loss: 0.2261 (0.2261)  loss_mask: 0.0404 (0.0985)
[19:04:41.691854] Test:  [  0/157]  eta: 0:01:40  testing_loss: 2.1132 (2.1132)  acc1: 28.1250 (28.1250)  acc5: 73.4375 (73.4375)  time: 0.6421  data: 0.6094  max mem: 4132
[19:04:41.978840] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 2.1555 (2.1576)  acc1: 18.7500 (20.1705)  acc5: 71.8750 (71.5909)  time: 0.0843  data: 0.0556  max mem: 4132
[19:04:42.260348] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 2.1608 (2.1616)  acc1: 18.7500 (19.9405)  acc5: 70.3125 (70.9077)  time: 0.0283  data: 0.0002  max mem: 4132
[19:04:42.541937] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 2.1573 (2.1570)  acc1: 17.1875 (18.9012)  acc5: 70.3125 (71.2702)  time: 0.0280  data: 0.0002  max mem: 4132
[19:04:42.822947] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 2.1622 (2.1576)  acc1: 17.1875 (18.9024)  acc5: 71.8750 (71.3034)  time: 0.0280  data: 0.0002  max mem: 4132
[19:04:43.105586] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 2.1650 (2.1604)  acc1: 18.7500 (19.0257)  acc5: 70.3125 (71.0172)  time: 0.0280  data: 0.0002  max mem: 4132
[19:04:43.389115] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 2.1669 (2.1615)  acc1: 18.7500 (19.2111)  acc5: 70.3125 (71.0809)  time: 0.0282  data: 0.0002  max mem: 4132
[19:04:43.671669] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 2.1552 (2.1609)  acc1: 18.7500 (18.9921)  acc5: 71.8750 (71.3908)  time: 0.0282  data: 0.0002  max mem: 4132
[19:04:43.954657] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 2.1584 (2.1611)  acc1: 18.7500 (18.9622)  acc5: 73.4375 (71.4699)  time: 0.0281  data: 0.0002  max mem: 4132
[19:04:44.238390] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 2.1709 (2.1627)  acc1: 18.7500 (18.9217)  acc5: 71.8750 (71.1023)  time: 0.0282  data: 0.0002  max mem: 4132
[19:04:44.522165] Test:  [100/157]  eta: 0:00:01  testing_loss: 2.1469 (2.1620)  acc1: 18.7500 (18.8428)  acc5: 71.8750 (71.1479)  time: 0.0282  data: 0.0002  max mem: 4132
[19:04:44.806797] Test:  [110/157]  eta: 0:00:01  testing_loss: 2.1469 (2.1620)  acc1: 18.7500 (18.7641)  acc5: 71.8750 (71.2838)  time: 0.0283  data: 0.0002  max mem: 4132
[19:04:45.088417] Test:  [120/157]  eta: 0:00:01  testing_loss: 2.1629 (2.1609)  acc1: 17.1875 (18.7371)  acc5: 71.8750 (71.3456)  time: 0.0282  data: 0.0002  max mem: 4132
[19:04:45.370860] Test:  [130/157]  eta: 0:00:00  testing_loss: 2.1613 (2.1611)  acc1: 18.7500 (18.8335)  acc5: 71.8750 (71.4098)  time: 0.0281  data: 0.0002  max mem: 4132
[19:04:45.653628] Test:  [140/157]  eta: 0:00:00  testing_loss: 2.1537 (2.1597)  acc1: 18.7500 (18.9051)  acc5: 73.4375 (71.6534)  time: 0.0281  data: 0.0002  max mem: 4132
[19:04:45.933590] Test:  [150/157]  eta: 0:00:00  testing_loss: 2.1425 (2.1588)  acc1: 20.3125 (18.9673)  acc5: 75.0000 (71.7922)  time: 0.0280  data: 0.0001  max mem: 4132
[19:04:46.083993] Test:  [156/157]  eta: 0:00:00  testing_loss: 2.1387 (2.1579)  acc1: 20.3125 (19.0000)  acc5: 75.0000 (71.8900)  time: 0.0270  data: 0.0001  max mem: 4132
[19:04:46.223826] Test: Total time: 0:00:05 (0.0330 s / it)
[19:04:46.224302] * Acc@1 19.000 Acc@5 71.890 loss 2.158
[19:04:46.224664] Accuracy of the network on the 10000 test images: 19.0%
[19:04:46.224884] Max accuracy: 25.08%
[19:04:46.843373] log_dir: ./output_dir
[19:04:47.697401] Epoch: [21]  [  0/781]  eta: 0:11:05  lr: 0.000233  training_loss: 0.3340 (0.3340)  classification_loss: 0.2278 (0.2278)  loss_mask: 0.1062 (0.1062)  time: 0.8523  data: 0.6464  max mem: 4132
[19:04:50.947723] Epoch: [21]  [ 20/781]  eta: 0:02:28  lr: 0.000233  training_loss: 0.3413 (0.3479)  classification_loss: 0.2246 (0.2250)  loss_mask: 0.1143 (0.1229)  time: 0.1624  data: 0.0002  max mem: 4132
[19:04:54.209341] Epoch: [21]  [ 40/781]  eta: 0:02:13  lr: 0.000233  training_loss: 0.3226 (0.3458)  classification_loss: 0.2238 (0.2246)  loss_mask: 0.0967 (0.1213)  time: 0.1630  data: 0.0004  max mem: 4132
[19:04:57.459573] Epoch: [21]  [ 60/781]  eta: 0:02:05  lr: 0.000233  training_loss: 0.2880 (0.3282)  classification_loss: 0.2254 (0.2248)  loss_mask: 0.0658 (0.1034)  time: 0.1624  data: 0.0002  max mem: 4132
[19:05:00.701798] Epoch: [21]  [ 80/781]  eta: 0:01:59  lr: 0.000233  training_loss: 0.3095 (0.3270)  classification_loss: 0.2263 (0.2253)  loss_mask: 0.0818 (0.1017)  time: 0.1620  data: 0.0003  max mem: 4132
[19:05:03.946995] Epoch: [21]  [100/781]  eta: 0:01:55  lr: 0.000233  training_loss: 0.2786 (0.3202)  classification_loss: 0.2268 (0.2256)  loss_mask: 0.0551 (0.0946)  time: 0.1622  data: 0.0002  max mem: 4132
[19:05:07.188086] Epoch: [21]  [120/781]  eta: 0:01:51  lr: 0.000233  training_loss: 0.2798 (0.3142)  classification_loss: 0.2256 (0.2255)  loss_mask: 0.0589 (0.0888)  time: 0.1619  data: 0.0002  max mem: 4132
[19:05:10.414518] Epoch: [21]  [140/781]  eta: 0:01:47  lr: 0.000233  training_loss: 0.2955 (0.3126)  classification_loss: 0.2250 (0.2253)  loss_mask: 0.0709 (0.0873)  time: 0.1613  data: 0.0002  max mem: 4132
[19:05:13.644828] Epoch: [21]  [160/781]  eta: 0:01:43  lr: 0.000233  training_loss: 0.2742 (0.3097)  classification_loss: 0.2255 (0.2254)  loss_mask: 0.0508 (0.0843)  time: 0.1614  data: 0.0002  max mem: 4132
[19:05:14.110068] [19:05:14.110587] [19:05:14.110760] [19:05:14.110939] [19:05:14.111098] [19:05:14.111253] [19:05:14.111411] [19:05:14.111571] [19:05:14.111764]
Traceback (most recent call last):
  File "/notebooks/CVPR2023/main_two_branch_new.py", line 370, in <module>
    main(args)
  File "/notebooks/CVPR2023/main_two_branch_new.py", line 322, in main
    train_stats = train_one_epoch(
  File "/notebooks/CVPR2023/engine_two_branch.py", line 74, in train_one_epoch
    loss_scaler(loss, optimizer, clip_grad=max_norm,
  File "/notebooks/CVPR2023/util/misc.py", line 267, in __call__
    self._scaler.step(optimizer)
  File "/usr/local/lib/python3.9/dist-packages/torch/cuda/amp/grad_scaler.py", line 338, in step
    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
  File "/usr/local/lib/python3.9/dist-packages/torch/cuda/amp/grad_scaler.py", line 284, in _maybe_opt_step
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
  File "/usr/local/lib/python3.9/dist-packages/torch/cuda/amp/grad_scaler.py", line 284, in <genexpr>
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
KeyboardInterrupt