Not using distributed mode
[19:15:19.977988] job dir: /notebooks/CVPR2023
[19:15:19.978968] Namespace(batch_size=64,
epochs=100,
accum_iter=1,
model='mae_vit_tiny',
norm_pix_loss=False,
dataset='c10',
input_size=32,
patch_size=2,
mask_ratio=0.75,
lambda_weight=0.1,
drop_path=0.1,
clip_grad=None,
weight_decay=0.05,
lr=None,
blr=0.001,
layer_decay=0.75,
min_lr=1e-06,
warmup_epochs=5,
color_jitter=None,
aa='rand-m9-mstd0.5-inc1',
smoothing=0.1,
reprob=0.25,
remode='pixel',
recount=1,
resplit=False,
mixup=0,
cutmix=0,
cutmix_minmax=None,
mixup_prob=1.0,
mixup_switch_prob=0.5,
mixup_mode='batch',
finetune='',
global_pool=True,
data_path='/datasets01/imagenet_full_size/061417/',
nb_classes=10,
output_dir='./output_dir',
log_dir='./output_dir',
device='cuda',
seed=0,
resume='',
start_epoch=0,
eval=False,
dist_eval=False,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[19:15:20.785661] Files already downloaded and verified
/usr/local/lib/python3.9/dist-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
[19:15:21.606094] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCropAndInterpolation(size=(32, 32), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic)
               RandomHorizontalFlip(p=0.5)
               RandAugment(n=2, ops=
           	AugmentOp(name=AutoContrast, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Equalize, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Invert, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Rotate, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=PosterizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeAdd, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ColorIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ContrastIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=BrightnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SharpnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearX, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearY, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateXRel, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateYRel, p=0.5, m=9, mstd=0.5))
               ToTensor()
               Normalize(mean=tensor([0.4914, 0.4822, 0.4465]), std=tensor([0.2023, 0.1994, 0.2010]))
               RandomErasing(p=0.25, mode=pixel, count=(1, 1))
           )
[19:15:22.021512] Files already downloaded and verified
[19:15:22.452537] Dataset CIFAR10
    Number of datapoints: 10000
    Root location: ./data
    Split: Test
    StandardTransform
Transform: Compose(
               Resize(size=36, interpolation=bicubic, max_size=None, antialias=None)
               CenterCrop(size=(32, 32))
               ToTensor()
               Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.201))
           )
[19:15:22.453242] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7fa4612d6130>
/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[19:15:34.341057] Model = MaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(2, 2), stride=(2, 2))
    (norm): Identity()
  )
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (7): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (8): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (9): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (10): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (11): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
  (decoder_embed): Linear(in_features=192, out_features=128, bias=True)
  (decoder_blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=128, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=128, out_features=128, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=512, out_features=128, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=128, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=128, out_features=128, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=512, out_features=128, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (decoder_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
  (decoder_pred): Linear(in_features=128, out_features=12, bias=True)
  (head): Linear(in_features=192, out_features=10, bias=True)
  (classifier_mask): Sequential(
    (0): Linear(in_features=192, out_features=5, bias=True)
    (1): LogSoftmax(dim=1)
  )
)
[19:15:34.341908] number of params (M): 5.77
[19:15:34.342140] base lr: 1.00e-03
[19:15:34.342309] actual lr: 2.50e-04
[19:15:34.342483] accumulate grad iterations: 1
[19:15:34.342639] effective batch size: 64
[19:15:34.343893] criterion = LabelSmoothingCrossEntropy()
[19:15:34.344165] Start training for 100 epochs
[19:15:34.345307] log_dir: ./output_dir
[19:15:40.342383] Epoch: [0]  [  0/781]  eta: 1:18:01  lr: 0.000000  training_loss: 2.5864 (2.5864)  classification_loss: 2.3900 (2.3900)  loss_mask: 0.1965 (0.1965)  time: 5.9941  data: 0.4429  max mem: 4070
[19:15:43.579138] Epoch: [0]  [ 20/781]  eta: 0:05:34  lr: 0.000001  training_loss: 2.4948 (2.5184)  classification_loss: 2.3062 (2.3223)  loss_mask: 0.1967 (0.1961)  time: 0.1617  data: 0.0002  max mem: 4130
[19:15:46.787193] Epoch: [0]  [ 40/781]  eta: 0:03:44  lr: 0.000003  training_loss: 2.3667 (2.4487)  classification_loss: 2.1806 (2.2533)  loss_mask: 0.1940 (0.1954)  time: 0.1603  data: 0.0002  max mem: 4130
[19:15:50.000982] Epoch: [0]  [ 60/781]  eta: 0:03:04  lr: 0.000004  training_loss: 2.2858 (2.3949)  classification_loss: 2.1021 (2.2029)  loss_mask: 0.1867 (0.1921)  time: 0.1606  data: 0.0004  max mem: 4130
[19:15:53.207230] Epoch: [0]  [ 80/781]  eta: 0:02:43  lr: 0.000005  training_loss: 2.2404 (2.3601)  classification_loss: 2.0694 (2.1712)  loss_mask: 0.1799 (0.1889)  time: 0.1602  data: 0.0003  max mem: 4130
[19:15:56.410082] Epoch: [0]  [100/781]  eta: 0:02:28  lr: 0.000006  training_loss: 2.2341 (2.3350)  classification_loss: 2.0583 (2.1489)  loss_mask: 0.1743 (0.1861)  time: 0.1601  data: 0.0002  max mem: 4130
[19:15:59.610268] Epoch: [0]  [120/781]  eta: 0:02:17  lr: 0.000008  training_loss: 2.2161 (2.3160)  classification_loss: 2.0487 (2.1327)  loss_mask: 0.1690 (0.1834)  time: 0.1599  data: 0.0002  max mem: 4130
[19:16:02.819866] Epoch: [0]  [140/781]  eta: 0:02:09  lr: 0.000009  training_loss: 2.1957 (2.2990)  classification_loss: 2.0206 (2.1177)  loss_mask: 0.1691 (0.1814)  time: 0.1604  data: 0.0003  max mem: 4130
[19:16:06.027464] Epoch: [0]  [160/781]  eta: 0:02:02  lr: 0.000010  training_loss: 2.1815 (2.2851)  classification_loss: 2.0159 (2.1056)  loss_mask: 0.1663 (0.1794)  time: 0.1603  data: 0.0002  max mem: 4130
[19:16:09.236769] Epoch: [0]  [180/781]  eta: 0:01:55  lr: 0.000012  training_loss: 2.2157 (2.2760)  classification_loss: 2.0485 (2.0980)  loss_mask: 0.1660 (0.1780)  time: 0.1604  data: 0.0002  max mem: 4130
[19:16:12.482382] Epoch: [0]  [200/781]  eta: 0:01:50  lr: 0.000013  training_loss: 2.1805 (2.2666)  classification_loss: 2.0097 (2.0899)  loss_mask: 0.1648 (0.1767)  time: 0.1622  data: 0.0002  max mem: 4130
[19:16:15.724095] Epoch: [0]  [220/781]  eta: 0:01:44  lr: 0.000014  training_loss: 2.1721 (2.2591)  classification_loss: 2.0085 (2.0836)  loss_mask: 0.1618 (0.1755)  time: 0.1620  data: 0.0004  max mem: 4130
[19:16:18.947308] Epoch: [0]  [240/781]  eta: 0:01:40  lr: 0.000015  training_loss: 2.1996 (2.2544)  classification_loss: 2.0364 (2.0800)  loss_mask: 0.1632 (0.1744)  time: 0.1611  data: 0.0003  max mem: 4130
[19:16:22.162256] Epoch: [0]  [260/781]  eta: 0:01:35  lr: 0.000017  training_loss: 2.1946 (2.2492)  classification_loss: 2.0323 (2.0758)  loss_mask: 0.1600 (0.1734)  time: 0.1607  data: 0.0004  max mem: 4130
[19:16:25.383062] Epoch: [0]  [280/781]  eta: 0:01:30  lr: 0.000018  training_loss: 2.1843 (2.2446)  classification_loss: 2.0293 (2.0722)  loss_mask: 0.1610 (0.1725)  time: 0.1610  data: 0.0002  max mem: 4130
[19:16:28.614888] Epoch: [0]  [300/781]  eta: 0:01:26  lr: 0.000019  training_loss: 2.1975 (2.2416)  classification_loss: 2.0351 (2.0699)  loss_mask: 0.1608 (0.1718)  time: 0.1615  data: 0.0005  max mem: 4130
[19:16:31.841297] Epoch: [0]  [320/781]  eta: 0:01:22  lr: 0.000020  training_loss: 2.2030 (2.2385)  classification_loss: 2.0397 (2.0674)  loss_mask: 0.1597 (0.1711)  time: 0.1613  data: 0.0003  max mem: 4130
[19:16:35.046876] Epoch: [0]  [340/781]  eta: 0:01:18  lr: 0.000022  training_loss: 2.1800 (2.2350)  classification_loss: 2.0160 (2.0646)  loss_mask: 0.1604 (0.1704)  time: 0.1602  data: 0.0002  max mem: 4130
[19:16:38.264367] Epoch: [0]  [360/781]  eta: 0:01:14  lr: 0.000023  training_loss: 2.1801 (2.2315)  classification_loss: 2.0187 (2.0617)  loss_mask: 0.1594 (0.1698)  time: 0.1608  data: 0.0005  max mem: 4130
[19:16:41.509286] Epoch: [0]  [380/781]  eta: 0:01:10  lr: 0.000024  training_loss: 2.1892 (2.2291)  classification_loss: 2.0324 (2.0598)  loss_mask: 0.1600 (0.1693)  time: 0.1622  data: 0.0003  max mem: 4130
[19:16:44.732933] Epoch: [0]  [400/781]  eta: 0:01:06  lr: 0.000026  training_loss: 2.1571 (2.2257)  classification_loss: 2.0015 (2.0570)  loss_mask: 0.1565 (0.1687)  time: 0.1611  data: 0.0002  max mem: 4130
[19:16:47.950458] Epoch: [0]  [420/781]  eta: 0:01:03  lr: 0.000027  training_loss: 2.1847 (2.2240)  classification_loss: 2.0276 (2.0558)  loss_mask: 0.1597 (0.1682)  time: 0.1608  data: 0.0003  max mem: 4130
[19:16:51.289404] Epoch: [0]  [440/781]  eta: 0:00:59  lr: 0.000028  training_loss: 2.1695 (2.2218)  classification_loss: 2.0107 (2.0540)  loss_mask: 0.1590 (0.1679)  time: 0.1669  data: 0.0004  max mem: 4130
[19:16:54.505588] Epoch: [0]  [460/781]  eta: 0:00:55  lr: 0.000029  training_loss: 2.1345 (2.2183)  classification_loss: 1.9727 (2.0509)  loss_mask: 0.1590 (0.1675)  time: 0.1607  data: 0.0002  max mem: 4130
[19:16:57.762097] Epoch: [0]  [480/781]  eta: 0:00:52  lr: 0.000031  training_loss: 2.1683 (2.2169)  classification_loss: 2.0079 (2.0498)  loss_mask: 0.1580 (0.1671)  time: 0.1627  data: 0.0003  max mem: 4130
[19:17:00.969374] Epoch: [0]  [500/781]  eta: 0:00:48  lr: 0.000032  training_loss: 2.1686 (2.2149)  classification_loss: 2.0051 (2.0482)  loss_mask: 0.1563 (0.1667)  time: 0.1603  data: 0.0003  max mem: 4130
[19:17:04.192346] Epoch: [0]  [520/781]  eta: 0:00:44  lr: 0.000033  training_loss: 2.1563 (2.2129)  classification_loss: 2.0036 (2.0466)  loss_mask: 0.1563 (0.1663)  time: 0.1611  data: 0.0004  max mem: 4130
[19:17:07.421806] Epoch: [0]  [540/781]  eta: 0:00:41  lr: 0.000035  training_loss: 2.1675 (2.2118)  classification_loss: 2.0102 (2.0458)  loss_mask: 0.1576 (0.1660)  time: 0.1614  data: 0.0003  max mem: 4130
[19:17:10.660184] Epoch: [0]  [560/781]  eta: 0:00:37  lr: 0.000036  training_loss: 2.1588 (2.2102)  classification_loss: 2.0021 (2.0445)  loss_mask: 0.1569 (0.1657)  time: 0.1619  data: 0.0002  max mem: 4130
[19:17:13.893951] Epoch: [0]  [580/781]  eta: 0:00:34  lr: 0.000037  training_loss: 2.1536 (2.2083)  classification_loss: 1.9957 (2.0429)  loss_mask: 0.1564 (0.1654)  time: 0.1616  data: 0.0003  max mem: 4130
[19:17:17.137609] Epoch: [0]  [600/781]  eta: 0:00:30  lr: 0.000038  training_loss: 2.1490 (2.2063)  classification_loss: 1.9914 (2.0412)  loss_mask: 0.1572 (0.1651)  time: 0.1621  data: 0.0002  max mem: 4130
[19:17:20.367689] Epoch: [0]  [620/781]  eta: 0:00:27  lr: 0.000040  training_loss: 2.1641 (2.2051)  classification_loss: 2.0035 (2.0402)  loss_mask: 0.1570 (0.1649)  time: 0.1614  data: 0.0003  max mem: 4130
[19:17:23.638755] Epoch: [0]  [640/781]  eta: 0:00:24  lr: 0.000041  training_loss: 2.1545 (2.2040)  classification_loss: 1.9948 (2.0393)  loss_mask: 0.1575 (0.1646)  time: 0.1634  data: 0.0003  max mem: 4130
[19:17:26.867369] Epoch: [0]  [660/781]  eta: 0:00:20  lr: 0.000042  training_loss: 2.1497 (2.2026)  classification_loss: 1.9904 (2.0382)  loss_mask: 0.1556 (0.1643)  time: 0.1613  data: 0.0003  max mem: 4130
[19:17:30.083585] Epoch: [0]  [680/781]  eta: 0:00:17  lr: 0.000044  training_loss: 2.1409 (2.2009)  classification_loss: 1.9821 (2.0368)  loss_mask: 0.1559 (0.1641)  time: 0.1607  data: 0.0002  max mem: 4130
[19:17:33.331962] Epoch: [0]  [700/781]  eta: 0:00:13  lr: 0.000045  training_loss: 2.1483 (2.1993)  classification_loss: 1.9941 (2.0354)  loss_mask: 0.1563 (0.1639)  time: 0.1623  data: 0.0002  max mem: 4130
[19:17:36.579838] Epoch: [0]  [720/781]  eta: 0:00:10  lr: 0.000046  training_loss: 2.1765 (2.1986)  classification_loss: 2.0210 (2.0349)  loss_mask: 0.1549 (0.1637)  time: 0.1623  data: 0.0002  max mem: 4130
[19:17:39.813251] Epoch: [0]  [740/781]  eta: 0:00:06  lr: 0.000047  training_loss: 2.1617 (2.1976)  classification_loss: 2.0077 (2.0342)  loss_mask: 0.1549 (0.1635)  time: 0.1616  data: 0.0002  max mem: 4130
[19:17:43.032365] Epoch: [0]  [760/781]  eta: 0:00:03  lr: 0.000049  training_loss: 2.1442 (2.1965)  classification_loss: 1.9853 (2.0332)  loss_mask: 0.1560 (0.1633)  time: 0.1609  data: 0.0003  max mem: 4130
[19:17:46.243243] Epoch: [0]  [780/781]  eta: 0:00:00  lr: 0.000050  training_loss: 2.1391 (2.1954)  classification_loss: 1.9867 (2.0323)  loss_mask: 0.1549 (0.1631)  time: 0.1605  data: 0.0002  max mem: 4130
[19:17:46.357294] Epoch: [0] Total time: 0:02:12 (0.1690 s / it)
[19:17:46.357807] Averaged stats: lr: 0.000050  training_loss: 2.1391 (2.1954)  classification_loss: 1.9867 (2.0323)  loss_mask: 0.1549 (0.1631)
[19:17:48.375913] Test:  [  0/157]  eta: 0:01:33  testing_loss: 1.8723 (1.8723)  acc1: 39.0625 (39.0625)  acc5: 89.0625 (89.0625)  time: 0.5972  data: 0.5621  max mem: 4130
[19:17:48.663260] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 1.9985 (1.9838)  acc1: 34.3750 (32.6705)  acc5: 84.3750 (83.3807)  time: 0.0802  data: 0.0512  max mem: 4130
[19:17:48.944630] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.9945 (1.9891)  acc1: 31.2500 (31.0268)  acc5: 82.8125 (82.1429)  time: 0.0283  data: 0.0001  max mem: 4130
[19:17:49.224105] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 1.9819 (1.9808)  acc1: 31.2500 (31.6532)  acc5: 82.8125 (82.4597)  time: 0.0279  data: 0.0002  max mem: 4130
[19:17:49.502988] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 1.9733 (1.9818)  acc1: 31.2500 (31.8598)  acc5: 82.8125 (82.2790)  time: 0.0278  data: 0.0001  max mem: 4130
[19:17:49.785595] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.9852 (1.9824)  acc1: 31.2500 (31.4645)  acc5: 84.3750 (82.6900)  time: 0.0280  data: 0.0002  max mem: 4130
[19:17:50.067643] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.9815 (1.9819)  acc1: 31.2500 (31.6855)  acc5: 84.3750 (82.9662)  time: 0.0281  data: 0.0002  max mem: 4130
[19:17:50.346721] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.9736 (1.9826)  acc1: 29.6875 (31.4261)  acc5: 84.3750 (83.1426)  time: 0.0279  data: 0.0001  max mem: 4130
[19:17:50.625807] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.9612 (1.9806)  acc1: 29.6875 (31.5779)  acc5: 84.3750 (83.5455)  time: 0.0278  data: 0.0001  max mem: 4130
[19:17:50.904781] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.9835 (1.9822)  acc1: 29.6875 (31.3530)  acc5: 84.3750 (83.6195)  time: 0.0278  data: 0.0001  max mem: 4130
[19:17:51.183709] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.9982 (1.9833)  acc1: 31.2500 (31.4511)  acc5: 84.3750 (83.4932)  time: 0.0278  data: 0.0001  max mem: 4130
[19:17:51.463212] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.9969 (1.9840)  acc1: 32.8125 (31.5034)  acc5: 82.8125 (83.5445)  time: 0.0278  data: 0.0001  max mem: 4130
[19:17:51.743479] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.9600 (1.9811)  acc1: 32.8125 (31.6245)  acc5: 84.3750 (83.7939)  time: 0.0279  data: 0.0002  max mem: 4130
[19:17:52.028140] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.9758 (1.9826)  acc1: 32.8125 (31.6555)  acc5: 84.3750 (83.6474)  time: 0.0281  data: 0.0002  max mem: 4130
[19:17:52.308518] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.9967 (1.9827)  acc1: 29.6875 (31.6268)  acc5: 82.8125 (83.5771)  time: 0.0281  data: 0.0002  max mem: 4130
[19:17:52.586925] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.9743 (1.9810)  acc1: 32.8125 (31.7777)  acc5: 82.8125 (83.7645)  time: 0.0278  data: 0.0001  max mem: 4130
[19:17:52.850493] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.9650 (1.9802)  acc1: 32.8125 (31.6200)  acc5: 84.3750 (83.8200)  time: 0.0325  data: 0.0001  max mem: 4130
[19:17:53.020027] Test: Total time: 0:00:05 (0.0334 s / it)
[19:17:53.020575] * Acc@1 31.620 Acc@5 83.820 loss 1.980
[19:17:53.020896] Accuracy of the network on the 10000 test images: 31.6%
[19:17:53.021098] Max accuracy: 31.62%
[19:17:53.477852] log_dir: ./output_dir
[19:17:54.442047] Epoch: [1]  [  0/781]  eta: 0:12:31  lr: 0.000050  training_loss: 2.1313 (2.1313)  classification_loss: 1.9780 (1.9780)  loss_mask: 0.1533 (0.1533)  time: 0.9624  data: 0.7453  max mem: 4132
[19:17:57.677844] Epoch: [1]  [ 20/781]  eta: 0:02:32  lr: 0.000051  training_loss: 2.1209 (2.1416)  classification_loss: 1.9747 (1.9887)  loss_mask: 0.1534 (0.1529)  time: 0.1617  data: 0.0002  max mem: 4132
[19:18:00.919209] Epoch: [1]  [ 40/781]  eta: 0:02:14  lr: 0.000053  training_loss: 2.1356 (2.1495)  classification_loss: 1.9834 (1.9962)  loss_mask: 0.1534 (0.1533)  time: 0.1620  data: 0.0002  max mem: 4132
[19:18:04.150994] Epoch: [1]  [ 60/781]  eta: 0:02:06  lr: 0.000054  training_loss: 2.1546 (2.1494)  classification_loss: 2.0002 (1.9955)  loss_mask: 0.1542 (0.1539)  time: 0.1615  data: 0.0002  max mem: 4132
[19:18:07.385525] Epoch: [1]  [ 80/781]  eta: 0:02:00  lr: 0.000055  training_loss: 2.1492 (2.1511)  classification_loss: 1.9978 (1.9972)  loss_mask: 0.1522 (0.1539)  time: 0.1616  data: 0.0002  max mem: 4132
[19:18:10.644400] Epoch: [1]  [100/781]  eta: 0:01:55  lr: 0.000056  training_loss: 2.1293 (2.1493)  classification_loss: 1.9761 (1.9955)  loss_mask: 0.1532 (0.1538)  time: 0.1629  data: 0.0002  max mem: 4132
[19:18:13.872590] Epoch: [1]  [120/781]  eta: 0:01:51  lr: 0.000058  training_loss: 2.1316 (2.1468)  classification_loss: 1.9798 (1.9929)  loss_mask: 0.1539 (0.1538)  time: 0.1613  data: 0.0002  max mem: 4132
[19:18:17.147700] Epoch: [1]  [140/781]  eta: 0:01:47  lr: 0.000059  training_loss: 2.1079 (2.1417)  classification_loss: 1.9586 (1.9878)  loss_mask: 0.1545 (0.1539)  time: 0.1637  data: 0.0002  max mem: 4132
[19:18:20.423136] Epoch: [1]  [160/781]  eta: 0:01:43  lr: 0.000060  training_loss: 2.1319 (2.1400)  classification_loss: 1.9817 (1.9861)  loss_mask: 0.1530 (0.1539)  time: 0.1637  data: 0.0002  max mem: 4132
[19:18:23.658901] Epoch: [1]  [180/781]  eta: 0:01:40  lr: 0.000062  training_loss: 2.1375 (2.1395)  classification_loss: 1.9841 (1.9857)  loss_mask: 0.1526 (0.1538)  time: 0.1617  data: 0.0003  max mem: 4132
[19:18:26.874603] Epoch: [1]  [200/781]  eta: 0:01:36  lr: 0.000063  training_loss: 2.1127 (2.1376)  classification_loss: 1.9612 (1.9839)  loss_mask: 0.1523 (0.1537)  time: 0.1607  data: 0.0001  max mem: 4132
[19:18:30.118122] Epoch: [1]  [220/781]  eta: 0:01:32  lr: 0.000064  training_loss: 2.1201 (2.1355)  classification_loss: 1.9663 (1.9819)  loss_mask: 0.1526 (0.1537)  time: 0.1621  data: 0.0002  max mem: 4132
[19:18:33.354293] Epoch: [1]  [240/781]  eta: 0:01:29  lr: 0.000065  training_loss: 2.1154 (2.1336)  classification_loss: 1.9627 (1.9800)  loss_mask: 0.1528 (0.1536)  time: 0.1617  data: 0.0003  max mem: 4132
[19:18:36.583245] Epoch: [1]  [260/781]  eta: 0:01:25  lr: 0.000067  training_loss: 2.0868 (2.1315)  classification_loss: 1.9309 (1.9779)  loss_mask: 0.1535 (0.1536)  time: 0.1614  data: 0.0002  max mem: 4132
[19:18:39.811182] Epoch: [1]  [280/781]  eta: 0:01:22  lr: 0.000068  training_loss: 2.1360 (2.1311)  classification_loss: 1.9819 (1.9777)  loss_mask: 0.1502 (0.1534)  time: 0.1613  data: 0.0002  max mem: 4132
[19:18:43.032116] Epoch: [1]  [300/781]  eta: 0:01:19  lr: 0.000069  training_loss: 2.1065 (2.1300)  classification_loss: 1.9498 (1.9767)  loss_mask: 0.1502 (0.1533)  time: 0.1610  data: 0.0002  max mem: 4132
[19:18:46.269285] Epoch: [1]  [320/781]  eta: 0:01:15  lr: 0.000070  training_loss: 2.0823 (2.1281)  classification_loss: 1.9309 (1.9749)  loss_mask: 0.1514 (0.1532)  time: 0.1618  data: 0.0002  max mem: 4132
[19:18:49.504931] Epoch: [1]  [340/781]  eta: 0:01:12  lr: 0.000072  training_loss: 2.1204 (2.1270)  classification_loss: 1.9697 (1.9739)  loss_mask: 0.1507 (0.1531)  time: 0.1617  data: 0.0003  max mem: 4132
[19:18:52.747663] Epoch: [1]  [360/781]  eta: 0:01:09  lr: 0.000073  training_loss: 2.1269 (2.1267)  classification_loss: 1.9758 (1.9738)  loss_mask: 0.1494 (0.1529)  time: 0.1620  data: 0.0002  max mem: 4132
[19:18:55.998401] Epoch: [1]  [380/781]  eta: 0:01:05  lr: 0.000074  training_loss: 2.1080 (2.1265)  classification_loss: 1.9577 (1.9738)  loss_mask: 0.1485 (0.1527)  time: 0.1624  data: 0.0003  max mem: 4132
[19:18:59.262279] Epoch: [1]  [400/781]  eta: 0:01:02  lr: 0.000076  training_loss: 2.0897 (2.1245)  classification_loss: 1.9404 (1.9721)  loss_mask: 0.1472 (0.1524)  time: 0.1631  data: 0.0003  max mem: 4132
[19:19:02.544448] Epoch: [1]  [420/781]  eta: 0:00:59  lr: 0.000077  training_loss: 2.0938 (2.1225)  classification_loss: 1.9490 (1.9704)  loss_mask: 0.1452 (0.1521)  time: 0.1640  data: 0.0003  max mem: 4132
[19:19:05.781457] Epoch: [1]  [440/781]  eta: 0:00:55  lr: 0.000078  training_loss: 2.0694 (2.1208)  classification_loss: 1.9244 (1.9690)  loss_mask: 0.1457 (0.1519)  time: 0.1618  data: 0.0002  max mem: 4132
[19:19:09.014073] Epoch: [1]  [460/781]  eta: 0:00:52  lr: 0.000079  training_loss: 2.0652 (2.1184)  classification_loss: 1.9213 (1.9668)  loss_mask: 0.1461 (0.1516)  time: 0.1616  data: 0.0002  max mem: 4132
[19:19:12.250320] Epoch: [1]  [480/781]  eta: 0:00:49  lr: 0.000081  training_loss: 2.1082 (2.1176)  classification_loss: 1.9592 (1.9663)  loss_mask: 0.1424 (0.1513)  time: 0.1617  data: 0.0003  max mem: 4132
[19:19:15.482411] Epoch: [1]  [500/781]  eta: 0:00:45  lr: 0.000082  training_loss: 2.0940 (2.1162)  classification_loss: 1.9465 (1.9652)  loss_mask: 0.1444 (0.1510)  time: 0.1615  data: 0.0002  max mem: 4132
[19:19:18.747691] Epoch: [1]  [520/781]  eta: 0:00:42  lr: 0.000083  training_loss: 2.0403 (2.1139)  classification_loss: 1.9031 (1.9631)  loss_mask: 0.1428 (0.1508)  time: 0.1632  data: 0.0002  max mem: 4132

[19:19:22.036824] Epoch: [1]  [540/781]  eta: 0:00:39  lr: 0.000085  training_loss: 2.0751 (2.1129)  classification_loss: 1.9249 (1.9625)  loss_mask: 0.1429 (0.1505)  time: 0.1644  data: 0.0003  max mem: 4132
[19:19:25.274491] Epoch: [1]  [560/781]  eta: 0:00:36  lr: 0.000086  training_loss: 2.0765 (2.1116)  classification_loss: 1.9422 (1.9615)  loss_mask: 0.1393 (0.1501)  time: 0.1618  data: 0.0003  max mem: 4132
[19:19:28.511873] Epoch: [1]  [580/781]  eta: 0:00:32  lr: 0.000087  training_loss: 2.0776 (2.1103)  classification_loss: 1.9326 (1.9605)  loss_mask: 0.1400 (0.1498)  time: 0.1618  data: 0.0002  max mem: 4132
[19:19:31.756164] Epoch: [1]  [600/781]  eta: 0:00:29  lr: 0.000088  training_loss: 2.0518 (2.1087)  classification_loss: 1.9116 (1.9592)  loss_mask: 0.1403 (0.1495)  time: 0.1621  data: 0.0002  max mem: 4132
[19:19:34.987332] Epoch: [1]  [620/781]  eta: 0:00:26  lr: 0.000090  training_loss: 2.0377 (2.1064)  classification_loss: 1.9048 (1.9574)  loss_mask: 0.1388 (0.1491)  time: 0.1615  data: 0.0003  max mem: 4132
[19:19:38.240606] Epoch: [1]  [640/781]  eta: 0:00:23  lr: 0.000091  training_loss: 2.0470 (2.1047)  classification_loss: 1.9032 (1.9561)  loss_mask: 0.1319 (0.1486)  time: 0.1626  data: 0.0002  max mem: 4132
[19:19:41.465584] Epoch: [1]  [660/781]  eta: 0:00:19  lr: 0.000092  training_loss: 2.0400 (2.1030)  classification_loss: 1.9144 (1.9548)  loss_mask: 0.1335 (0.1482)  time: 0.1611  data: 0.0003  max mem: 4132
[19:19:44.709770] Epoch: [1]  [680/781]  eta: 0:00:16  lr: 0.000094  training_loss: 2.0524 (2.1015)  classification_loss: 1.9169 (1.9538)  loss_mask: 0.1290 (0.1477)  time: 0.1621  data: 0.0002  max mem: 4132
[19:19:47.936620] Epoch: [1]  [700/781]  eta: 0:00:13  lr: 0.000095  training_loss: 2.0544 (2.1001)  classification_loss: 1.9199 (1.9527)  loss_mask: 0.1342 (0.1474)  time: 0.1613  data: 0.0002  max mem: 4132
[19:19:51.170790] Epoch: [1]  [720/781]  eta: 0:00:09  lr: 0.000096  training_loss: 2.0442 (2.0989)  classification_loss: 1.9038 (1.9517)  loss_mask: 0.1373 (0.1471)  time: 0.1616  data: 0.0003  max mem: 4132
[19:19:54.414840] Epoch: [1]  [740/781]  eta: 0:00:06  lr: 0.000097  training_loss: 2.0501 (2.0977)  classification_loss: 1.9069 (1.9509)  loss_mask: 0.1325 (0.1468)  time: 0.1621  data: 0.0002  max mem: 4132
[19:19:57.692451] Epoch: [1]  [760/781]  eta: 0:00:03  lr: 0.000099  training_loss: 2.0223 (2.0959)  classification_loss: 1.8940 (1.9496)  loss_mask: 0.1275 (0.1463)  time: 0.1638  data: 0.0007  max mem: 4132
[19:20:00.910624] Epoch: [1]  [780/781]  eta: 0:00:00  lr: 0.000100  training_loss: 2.0823 (2.0949)  classification_loss: 1.9496 (1.9492)  loss_mask: 0.1264 (0.1457)  time: 0.1608  data: 0.0003  max mem: 4132
[19:20:01.086028] Epoch: [1] Total time: 0:02:07 (0.1634 s / it)
[19:20:01.086535] Averaged stats: lr: 0.000100  training_loss: 2.0823 (2.0949)  classification_loss: 1.9496 (1.9492)  loss_mask: 0.1264 (0.1457)
[19:20:01.782307] Test:  [  0/157]  eta: 0:01:48  testing_loss: 1.7018 (1.7018)  acc1: 43.7500 (43.7500)  acc5: 85.9375 (85.9375)  time: 0.6919  data: 0.6627  max mem: 4132
[19:20:02.077629] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.8195 (1.7961)  acc1: 42.1875 (38.4943)  acc5: 89.0625 (88.7784)  time: 0.0896  data: 0.0605  max mem: 4132
[19:20:02.359250] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.7726 (1.7828)  acc1: 42.1875 (40.9226)  acc5: 89.0625 (88.5417)  time: 0.0287  data: 0.0002  max mem: 4132
[19:20:02.642612] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.7726 (1.7773)  acc1: 42.1875 (40.9274)  acc5: 89.0625 (88.5081)  time: 0.0281  data: 0.0001  max mem: 4132
[19:20:02.931043] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.7879 (1.7860)  acc1: 37.5000 (40.3963)  acc5: 87.5000 (87.8811)  time: 0.0285  data: 0.0001  max mem: 4132
[19:20:03.220580] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.8019 (1.7878)  acc1: 37.5000 (39.9816)  acc5: 87.5000 (87.9596)  time: 0.0288  data: 0.0002  max mem: 4132
[19:20:03.505009] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.7704 (1.7826)  acc1: 40.6250 (40.7787)  acc5: 89.0625 (88.1404)  time: 0.0286  data: 0.0002  max mem: 4132
[19:20:03.795922] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.7607 (1.7845)  acc1: 42.1875 (40.7130)  acc5: 90.6250 (88.2923)  time: 0.0286  data: 0.0002  max mem: 4132
[19:20:04.083794] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.7755 (1.7831)  acc1: 42.1875 (41.0301)  acc5: 90.6250 (88.5224)  time: 0.0288  data: 0.0001  max mem: 4132
[19:20:04.368920] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.7789 (1.7850)  acc1: 39.0625 (40.7280)  acc5: 89.0625 (88.4959)  time: 0.0285  data: 0.0001  max mem: 4132
[19:20:04.656724] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.8429 (1.7901)  acc1: 37.5000 (40.3775)  acc5: 85.9375 (88.1498)  time: 0.0285  data: 0.0002  max mem: 4132
[19:20:04.943837] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.8261 (1.7920)  acc1: 37.5000 (40.2590)  acc5: 85.9375 (88.0490)  time: 0.0286  data: 0.0002  max mem: 4132
[19:20:05.225042] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.7967 (1.7895)  acc1: 39.0625 (40.3667)  acc5: 87.5000 (88.0682)  time: 0.0283  data: 0.0002  max mem: 4132
[19:20:05.511833] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.7869 (1.7916)  acc1: 39.0625 (40.1360)  acc5: 87.5000 (88.0248)  time: 0.0283  data: 0.0001  max mem: 4132
[19:20:05.801580] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.7901 (1.7905)  acc1: 37.5000 (40.2926)  acc5: 87.5000 (88.0430)  time: 0.0287  data: 0.0001  max mem: 4132
[19:20:06.079550] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.7794 (1.7892)  acc1: 39.0625 (40.3456)  acc5: 87.5000 (88.0691)  time: 0.0283  data: 0.0001  max mem: 4132
[19:20:06.229480] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.7536 (1.7895)  acc1: 37.5000 (40.1000)  acc5: 87.5000 (88.1000)  time: 0.0268  data: 0.0001  max mem: 4132
[19:20:06.419990] Test: Total time: 0:00:05 (0.0340 s / it)
[19:20:06.422008] * Acc@1 40.100 Acc@5 88.100 loss 1.790
[19:20:06.422506] Accuracy of the network on the 10000 test images: 40.1%
[19:20:06.422731] Max accuracy: 40.10%
[19:20:06.745469] log_dir: ./output_dir
[19:20:07.674441] Epoch: [2]  [  0/781]  eta: 0:12:03  lr: 0.000100  training_loss: 2.0564 (2.0564)  classification_loss: 1.9328 (1.9328)  loss_mask: 0.1236 (0.1236)  time: 0.9269  data: 0.7212  max mem: 4132
[19:20:10.896818] Epoch: [2]  [ 20/781]  eta: 0:02:30  lr: 0.000101  training_loss: 2.0229 (2.0220)  classification_loss: 1.8968 (1.8957)  loss_mask: 0.1270 (0.1263)  time: 0.1610  data: 0.0002  max mem: 4132

[19:20:14.121694] Epoch: [2]  [ 40/781]  eta: 0:02:13  lr: 0.000103  training_loss: 2.0285 (2.0260)  classification_loss: 1.9068 (1.9002)  loss_mask: 0.1236 (0.1258)  time: 0.1612  data: 0.0002  max mem: 4132
[19:20:17.360391] Epoch: [2]  [ 60/781]  eta: 0:02:05  lr: 0.000104  training_loss: 2.0081 (2.0254)  classification_loss: 1.8795 (1.8999)  loss_mask: 0.1236 (0.1254)  time: 0.1619  data: 0.0002  max mem: 4132
[19:20:20.598327] Epoch: [2]  [ 80/781]  eta: 0:01:59  lr: 0.000105  training_loss: 2.0339 (2.0271)  classification_loss: 1.9195 (1.9032)  loss_mask: 0.1178 (0.1239)  time: 0.1618  data: 0.0002  max mem: 4132
[19:20:23.832301] Epoch: [2]  [100/781]  eta: 0:01:55  lr: 0.000106  training_loss: 2.0294 (2.0268)  classification_loss: 1.8973 (1.9032)  loss_mask: 0.1203 (0.1236)  time: 0.1616  data: 0.0003  max mem: 4132
[19:20:27.086108] Epoch: [2]  [120/781]  eta: 0:01:51  lr: 0.000108  training_loss: 2.0247 (2.0267)  classification_loss: 1.9071 (1.9038)  loss_mask: 0.1191 (0.1229)  time: 0.1626  data: 0.0003  max mem: 4132
[19:20:30.360570] Epoch: [2]  [140/781]  eta: 0:01:47  lr: 0.000109  training_loss: 2.0184 (2.0251)  classification_loss: 1.8996 (1.9032)  loss_mask: 0.1149 (0.1219)  time: 0.1636  data: 0.0003  max mem: 4132
[19:20:33.588554] Epoch: [2]  [160/781]  eta: 0:01:43  lr: 0.000110  training_loss: 1.9868 (2.0221)  classification_loss: 1.8698 (1.9003)  loss_mask: 0.1216 (0.1217)  time: 0.1613  data: 0.0002  max mem: 4132
[19:20:36.844780] Epoch: [2]  [180/781]  eta: 0:01:39  lr: 0.000112  training_loss: 1.9917 (2.0201)  classification_loss: 1.8785 (1.8990)  loss_mask: 0.1134 (0.1212)  time: 0.1627  data: 0.0002  max mem: 4132
[19:20:40.081479] Epoch: [2]  [200/781]  eta: 0:01:36  lr: 0.000113  training_loss: 1.9774 (2.0182)  classification_loss: 1.8508 (1.8968)  loss_mask: 0.1223 (0.1214)  time: 0.1618  data: 0.0003  max mem: 4132
[19:20:43.314941] Epoch: [2]  [220/781]  eta: 0:01:32  lr: 0.000114  training_loss: 1.9825 (2.0141)  classification_loss: 1.8556 (1.8932)  loss_mask: 0.1130 (0.1209)  time: 0.1616  data: 0.0005  max mem: 4132
[19:20:46.541078] Epoch: [2]  [240/781]  eta: 0:01:29  lr: 0.000115  training_loss: 1.9998 (2.0128)  classification_loss: 1.8955 (1.8924)  loss_mask: 0.1132 (0.1204)  time: 0.1612  data: 0.0002  max mem: 4132
[19:20:49.793062] Epoch: [2]  [260/781]  eta: 0:01:25  lr: 0.000117  training_loss: 1.9848 (2.0102)  classification_loss: 1.8718 (1.8907)  loss_mask: 0.1105 (0.1195)  time: 0.1625  data: 0.0003  max mem: 4132
[19:20:53.049190] Epoch: [2]  [280/781]  eta: 0:01:22  lr: 0.000118  training_loss: 1.9731 (2.0090)  classification_loss: 1.8739 (1.8902)  loss_mask: 0.1079 (0.1187)  time: 0.1627  data: 0.0002  max mem: 4132
[19:20:56.284510] Epoch: [2]  [300/781]  eta: 0:01:19  lr: 0.000119  training_loss: 1.9815 (2.0081)  classification_loss: 1.8814 (1.8901)  loss_mask: 0.1086 (0.1180)  time: 0.1616  data: 0.0002  max mem: 4132
[19:20:59.501756] Epoch: [2]  [320/781]  eta: 0:01:15  lr: 0.000120  training_loss: 1.9717 (2.0061)  classification_loss: 1.8568 (1.8887)  loss_mask: 0.1083 (0.1174)  time: 0.1608  data: 0.0001  max mem: 4132
[19:21:02.755164] Epoch: [2]  [340/781]  eta: 0:01:12  lr: 0.000122  training_loss: 1.9844 (2.0053)  classification_loss: 1.8808 (1.8884)  loss_mask: 0.1082 (0.1169)  time: 0.1625  data: 0.0002  max mem: 4132
[19:21:06.011305] Epoch: [2]  [360/781]  eta: 0:01:09  lr: 0.000123  training_loss: 1.9405 (2.0031)  classification_loss: 1.8266 (1.8866)  loss_mask: 0.1107 (0.1165)  time: 0.1627  data: 0.0002  max mem: 4132
[19:21:09.249244] Epoch: [2]  [380/781]  eta: 0:01:05  lr: 0.000124  training_loss: 1.9647 (2.0025)  classification_loss: 1.8595 (1.8863)  loss_mask: 0.1092 (0.1161)  time: 0.1618  data: 0.0002  max mem: 4132
[19:21:12.500363] Epoch: [2]  [400/781]  eta: 0:01:02  lr: 0.000126  training_loss: 1.9435 (2.0004)  classification_loss: 1.8211 (1.8846)  loss_mask: 0.1083 (0.1158)  time: 0.1624  data: 0.0002  max mem: 4132
[19:21:15.740918] Epoch: [2]  [420/781]  eta: 0:00:59  lr: 0.000127  training_loss: 1.9575 (1.9981)  classification_loss: 1.8541 (1.8829)  loss_mask: 0.1050 (0.1152)  time: 0.1619  data: 0.0002  max mem: 4132
[19:21:19.001423] Epoch: [2]  [440/781]  eta: 0:00:55  lr: 0.000128  training_loss: 1.9664 (1.9972)  classification_loss: 1.8524 (1.8823)  loss_mask: 0.1058 (0.1149)  time: 0.1629  data: 0.0002  max mem: 4132
[19:21:22.231954] Epoch: [2]  [460/781]  eta: 0:00:52  lr: 0.000129  training_loss: 1.9421 (1.9956)  classification_loss: 1.8398 (1.8812)  loss_mask: 0.1048 (0.1144)  time: 0.1614  data: 0.0002  max mem: 4132
[19:21:25.459464] Epoch: [2]  [480/781]  eta: 0:00:49  lr: 0.000131  training_loss: 1.9700 (1.9945)  classification_loss: 1.8563 (1.8807)  loss_mask: 0.1004 (0.1138)  time: 0.1613  data: 0.0003  max mem: 4132
[19:21:28.681559] Epoch: [2]  [500/781]  eta: 0:00:45  lr: 0.000132  training_loss: 1.9577 (1.9931)  classification_loss: 1.8563 (1.8794)  loss_mask: 0.1074 (0.1136)  time: 0.1610  data: 0.0002  max mem: 4132
[19:21:31.925465] Epoch: [2]  [520/781]  eta: 0:00:42  lr: 0.000133  training_loss: 1.9378 (1.9919)  classification_loss: 1.8316 (1.8787)  loss_mask: 0.0997 (0.1132)  time: 0.1621  data: 0.0002  max mem: 4132
[19:21:35.153507] Epoch: [2]  [540/781]  eta: 0:00:39  lr: 0.000135  training_loss: 1.9584 (1.9913)  classification_loss: 1.8532 (1.8785)  loss_mask: 0.1040 (0.1128)  time: 0.1613  data: 0.0002  max mem: 4132
[19:21:38.393897] Epoch: [2]  [560/781]  eta: 0:00:36  lr: 0.000136  training_loss: 1.9379 (1.9895)  classification_loss: 1.8363 (1.8772)  loss_mask: 0.0984 (0.1123)  time: 0.1619  data: 0.0002  max mem: 4132
[19:21:41.620439] Epoch: [2]  [580/781]  eta: 0:00:32  lr: 0.000137  training_loss: 1.9319 (1.9878)  classification_loss: 1.8361 (1.8760)  loss_mask: 0.0968 (0.1119)  time: 0.1612  data: 0.0002  max mem: 4132
[19:21:44.881004] Epoch: [2]  [600/781]  eta: 0:00:29  lr: 0.000138  training_loss: 1.9412 (1.9867)  classification_loss: 1.8411 (1.8751)  loss_mask: 0.1063 (0.1117)  time: 0.1630  data: 0.0002  max mem: 4132
[19:21:48.116643] Epoch: [2]  [620/781]  eta: 0:00:26  lr: 0.000140  training_loss: 1.9280 (1.9850)  classification_loss: 1.8286 (1.8736)  loss_mask: 0.1004 (0.1113)  time: 0.1617  data: 0.0003  max mem: 4132
[19:21:51.380185] Epoch: [2]  [640/781]  eta: 0:00:23  lr: 0.000141  training_loss: 1.9435 (1.9832)  classification_loss: 1.8354 (1.8723)  loss_mask: 0.0952 (0.1109)  time: 0.1631  data: 0.0002  max mem: 4132
[19:21:54.626360] Epoch: [2]  [660/781]  eta: 0:00:19  lr: 0.000142  training_loss: 1.9282 (1.9816)  classification_loss: 1.8324 (1.8711)  loss_mask: 0.0935 (0.1105)  time: 0.1622  data: 0.0004  max mem: 4132
[19:21:57.851387] Epoch: [2]  [680/781]  eta: 0:00:16  lr: 0.000144  training_loss: 1.9285 (1.9800)  classification_loss: 1.8335 (1.8699)  loss_mask: 0.0955 (0.1101)  time: 0.1612  data: 0.0004  max mem: 4132
[19:22:01.099749] Epoch: [2]  [700/781]  eta: 0:00:13  lr: 0.000145  training_loss: 1.9306 (1.9786)  classification_loss: 1.8249 (1.8688)  loss_mask: 0.0999 (0.1098)  time: 0.1623  data: 0.0002  max mem: 4132
[19:22:04.322408] Epoch: [2]  [720/781]  eta: 0:00:09  lr: 0.000146  training_loss: 1.9271 (1.9773)  classification_loss: 1.8274 (1.8680)  loss_mask: 0.0923 (0.1093)  time: 0.1610  data: 0.0002  max mem: 4132
[19:22:07.563835] Epoch: [2]  [740/781]  eta: 0:00:06  lr: 0.000147  training_loss: 1.9497 (1.9764)  classification_loss: 1.8529 (1.8674)  loss_mask: 0.0976 (0.1091)  time: 0.1619  data: 0.0003  max mem: 4132
[19:22:10.791059] Epoch: [2]  [760/781]  eta: 0:00:03  lr: 0.000149  training_loss: 1.9222 (1.9754)  classification_loss: 1.8218 (1.8666)  loss_mask: 0.0991 (0.1088)  time: 0.1613  data: 0.0003  max mem: 4132
[19:22:14.010733] Epoch: [2]  [780/781]  eta: 0:00:00  lr: 0.000150  training_loss: 1.9253 (1.9740)  classification_loss: 1.8289 (1.8655)  loss_mask: 0.0986 (0.1085)  time: 0.1609  data: 0.0002  max mem: 4132
[19:22:14.177432] Epoch: [2] Total time: 0:02:07 (0.1632 s / it)
[19:22:14.177901] Averaged stats: lr: 0.000150  training_loss: 1.9253 (1.9740)  classification_loss: 1.8289 (1.8655)  loss_mask: 0.0986 (0.1085)
[19:22:14.776485] Test:  [  0/157]  eta: 0:01:33  testing_loss: 1.5183 (1.5183)  acc1: 42.1875 (42.1875)  acc5: 92.1875 (92.1875)  time: 0.5938  data: 0.5637  max mem: 4132
[19:22:15.062983] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 1.5374 (1.5790)  acc1: 43.7500 (44.6023)  acc5: 92.1875 (92.6136)  time: 0.0799  data: 0.0514  max mem: 4132
[19:22:15.347289] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.5504 (1.5584)  acc1: 48.4375 (47.6190)  acc5: 92.1875 (92.1131)  time: 0.0284  data: 0.0002  max mem: 4132
[19:22:15.631626] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 1.5507 (1.5562)  acc1: 50.0000 (48.4879)  acc5: 90.6250 (91.6835)  time: 0.0283  data: 0.0002  max mem: 4132
[19:22:15.923587] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 1.5620 (1.5655)  acc1: 46.8750 (47.8277)  acc5: 90.6250 (91.3872)  time: 0.0287  data: 0.0002  max mem: 4132
[19:22:16.210101] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.5644 (1.5639)  acc1: 46.8750 (48.0392)  acc5: 90.6250 (91.3603)  time: 0.0288  data: 0.0002  max mem: 4132
[19:22:16.493473] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.5186 (1.5557)  acc1: 51.5625 (48.5912)  acc5: 92.1875 (91.5727)  time: 0.0283  data: 0.0002  max mem: 4132
[19:22:16.775813] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.5056 (1.5528)  acc1: 51.5625 (48.9437)  acc5: 93.7500 (91.7914)  time: 0.0281  data: 0.0002  max mem: 4132
[19:22:17.063933] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.5339 (1.5540)  acc1: 50.0000 (48.9390)  acc5: 92.1875 (91.8596)  time: 0.0284  data: 0.0002  max mem: 4132
[19:22:17.345146] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.5777 (1.5579)  acc1: 48.4375 (48.9526)  acc5: 92.1875 (91.8098)  time: 0.0283  data: 0.0002  max mem: 4132
[19:22:17.635257] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.5959 (1.5621)  acc1: 48.4375 (48.7005)  acc5: 90.6250 (91.7543)  time: 0.0285  data: 0.0002  max mem: 4132
[19:22:17.916174] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.5902 (1.5641)  acc1: 46.8750 (48.6768)  acc5: 90.6250 (91.7652)  time: 0.0284  data: 0.0002  max mem: 4132
[19:22:18.199855] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.5442 (1.5617)  acc1: 50.0000 (48.9282)  acc5: 92.1875 (91.7743)  time: 0.0281  data: 0.0002  max mem: 4132
[19:22:18.488977] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.5329 (1.5633)  acc1: 51.5625 (48.8550)  acc5: 92.1875 (91.7104)  time: 0.0285  data: 0.0002  max mem: 4132
[19:22:18.770980] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.5625 (1.5629)  acc1: 50.0000 (49.0359)  acc5: 92.1875 (91.6888)  time: 0.0284  data: 0.0002  max mem: 4132
[19:22:19.049570] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.5527 (1.5618)  acc1: 51.5625 (49.2136)  acc5: 92.1875 (91.5977)  time: 0.0279  data: 0.0001  max mem: 4132
[19:22:19.199905] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.5289 (1.5622)  acc1: 50.0000 (49.1000)  acc5: 90.6250 (91.6200)  time: 0.0269  data: 0.0001  max mem: 4132
[19:22:19.373424] Test: Total time: 0:00:05 (0.0331 s / it)
[19:22:19.374101] * Acc@1 49.100 Acc@5 91.620 loss 1.562
[19:22:19.374410] Accuracy of the network on the 10000 test images: 49.1%
[19:22:19.374888] Max accuracy: 49.10%
[19:22:19.652568] log_dir: ./output_dir
[19:22:20.403046] Epoch: [3]  [  0/781]  eta: 0:09:44  lr: 0.000150  training_loss: 1.8218 (1.8218)  classification_loss: 1.7355 (1.7355)  loss_mask: 0.0862 (0.0862)  time: 0.7487  data: 0.5580  max mem: 4132
[19:22:23.624816] Epoch: [3]  [ 20/781]  eta: 0:02:23  lr: 0.000151  training_loss: 1.9011 (1.8956)  classification_loss: 1.7916 (1.8008)  loss_mask: 0.0893 (0.0948)  time: 0.1610  data: 0.0002  max mem: 4132
[19:22:26.859248] Epoch: [3]  [ 40/781]  eta: 0:02:10  lr: 0.000153  training_loss: 1.9683 (1.9299)  classification_loss: 1.8672 (1.8318)  loss_mask: 0.0980 (0.0981)  time: 0.1617  data: 0.0003  max mem: 4132
[19:22:30.099336] Epoch: [3]  [ 60/781]  eta: 0:02:03  lr: 0.000154  training_loss: 1.9228 (1.9304)  classification_loss: 1.8304 (1.8329)  loss_mask: 0.0935 (0.0975)  time: 0.1619  data: 0.0002  max mem: 4132
[19:22:33.468940] Epoch: [3]  [ 80/781]  eta: 0:01:59  lr: 0.000155  training_loss: 1.9319 (1.9291)  classification_loss: 1.8264 (1.8318)  loss_mask: 0.0939 (0.0973)  time: 0.1684  data: 0.0003  max mem: 4132
[19:22:36.727373] Epoch: [3]  [100/781]  eta: 0:01:55  lr: 0.000156  training_loss: 1.9349 (1.9308)  classification_loss: 1.8449 (1.8348)  loss_mask: 0.0888 (0.0960)  time: 0.1628  data: 0.0002  max mem: 4132
[19:22:39.951302] Epoch: [3]  [120/781]  eta: 0:01:50  lr: 0.000158  training_loss: 1.8725 (1.9258)  classification_loss: 1.7707 (1.8294)  loss_mask: 0.0972 (0.0964)  time: 0.1611  data: 0.0002  max mem: 4132
[19:22:43.197334] Epoch: [3]  [140/781]  eta: 0:01:46  lr: 0.000159  training_loss: 1.8731 (1.9224)  classification_loss: 1.7805 (1.8270)  loss_mask: 0.0899 (0.0954)  time: 0.1622  data: 0.0002  max mem: 4132
[19:22:46.412692] Epoch: [3]  [160/781]  eta: 0:01:43  lr: 0.000160  training_loss: 1.9302 (1.9219)  classification_loss: 1.8476 (1.8273)  loss_mask: 0.0854 (0.0946)  time: 0.1607  data: 0.0003  max mem: 4132
[19:22:49.638457] Epoch: [3]  [180/781]  eta: 0:01:39  lr: 0.000162  training_loss: 1.9172 (1.9197)  classification_loss: 1.8325 (1.8254)  loss_mask: 0.0895 (0.0943)  time: 0.1612  data: 0.0003  max mem: 4132
[19:22:52.876322] Epoch: [3]  [200/781]  eta: 0:01:35  lr: 0.000163  training_loss: 1.9048 (1.9179)  classification_loss: 1.8162 (1.8240)  loss_mask: 0.0880 (0.0939)  time: 0.1616  data: 0.0002  max mem: 4132
[19:22:56.115322] Epoch: [3]  [220/781]  eta: 0:01:32  lr: 0.000164  training_loss: 1.9138 (1.9172)  classification_loss: 1.8139 (1.8229)  loss_mask: 0.0972 (0.0943)  time: 0.1619  data: 0.0002  max mem: 4132
[19:22:59.362261] Epoch: [3]  [240/781]  eta: 0:01:29  lr: 0.000165  training_loss: 1.9116 (1.9172)  classification_loss: 1.8208 (1.8230)  loss_mask: 0.0897 (0.0942)  time: 0.1623  data: 0.0003  max mem: 4132
[19:23:02.674305] Epoch: [3]  [260/781]  eta: 0:01:25  lr: 0.000167  training_loss: 1.8614 (1.9135)  classification_loss: 1.7677 (1.8195)  loss_mask: 0.0909 (0.0940)  time: 0.1655  data: 0.0002  max mem: 4132
[19:23:05.890645] Epoch: [3]  [280/781]  eta: 0:01:22  lr: 0.000168  training_loss: 1.9040 (1.9146)  classification_loss: 1.8164 (1.8203)  loss_mask: 0.0972 (0.0943)  time: 0.1607  data: 0.0002  max mem: 4132
[19:23:09.106460] Epoch: [3]  [300/781]  eta: 0:01:18  lr: 0.000169  training_loss: 1.9039 (1.9145)  classification_loss: 1.8009 (1.8199)  loss_mask: 0.0992 (0.0947)  time: 0.1607  data: 0.0003  max mem: 4132
[19:23:12.326425] Epoch: [3]  [320/781]  eta: 0:01:15  lr: 0.000170  training_loss: 1.8642 (1.9124)  classification_loss: 1.7558 (1.8172)  loss_mask: 0.0984 (0.0951)  time: 0.1609  data: 0.0002  max mem: 4132
[19:23:15.545092] Epoch: [3]  [340/781]  eta: 0:01:12  lr: 0.000172  training_loss: 1.9030 (1.9115)  classification_loss: 1.7990 (1.8160)  loss_mask: 0.1035 (0.0955)  time: 0.1609  data: 0.0003  max mem: 4132
[19:23:18.857907] Epoch: [3]  [360/781]  eta: 0:01:09  lr: 0.000173  training_loss: 1.8550 (1.9094)  classification_loss: 1.7756 (1.8144)  loss_mask: 0.0872 (0.0950)  time: 0.1656  data: 0.0002  max mem: 4132
[19:23:22.087614] Epoch: [3]  [380/781]  eta: 0:01:05  lr: 0.000174  training_loss: 1.8744 (1.9083)  classification_loss: 1.7892 (1.8134)  loss_mask: 0.0932 (0.0949)  time: 0.1614  data: 0.0002  max mem: 4132
[19:23:25.319378] Epoch: [3]  [400/781]  eta: 0:01:02  lr: 0.000176  training_loss: 1.8974 (1.9073)  classification_loss: 1.8044 (1.8125)  loss_mask: 0.0911 (0.0948)  time: 0.1615  data: 0.0002  max mem: 4132
[19:23:28.539652] Epoch: [3]  [420/781]  eta: 0:00:59  lr: 0.000177  training_loss: 1.8459 (1.9051)  classification_loss: 1.7640 (1.8108)  loss_mask: 0.0833 (0.0943)  time: 0.1608  data: 0.0002  max mem: 4132
[19:23:31.777933] Epoch: [3]  [440/781]  eta: 0:00:55  lr: 0.000178  training_loss: 1.8603 (1.9024)  classification_loss: 1.7740 (1.8085)  loss_mask: 0.0863 (0.0940)  time: 0.1618  data: 0.0002  max mem: 4132
[19:23:34.999053] Epoch: [3]  [460/781]  eta: 0:00:52  lr: 0.000179  training_loss: 1.8661 (1.9012)  classification_loss: 1.7753 (1.8077)  loss_mask: 0.0827 (0.0935)  time: 0.1610  data: 0.0004  max mem: 4132
[19:23:38.255192] Epoch: [3]  [480/781]  eta: 0:00:49  lr: 0.000181  training_loss: 1.8724 (1.9001)  classification_loss: 1.7824 (1.8069)  loss_mask: 0.0853 (0.0932)  time: 0.1627  data: 0.0003  max mem: 4132
[19:23:41.535675] Epoch: [3]  [500/781]  eta: 0:00:45  lr: 0.000182  training_loss: 1.8573 (1.8985)  classification_loss: 1.7733 (1.8056)  loss_mask: 0.0836 (0.0929)  time: 0.1639  data: 0.0002  max mem: 4132
[19:23:44.831240] Epoch: [3]  [520/781]  eta: 0:00:42  lr: 0.000183  training_loss: 1.8597 (1.8974)  classification_loss: 1.7765 (1.8049)  loss_mask: 0.0830 (0.0926)  time: 0.1647  data: 0.0002  max mem: 4132
[19:23:48.068626] Epoch: [3]  [540/781]  eta: 0:00:39  lr: 0.000185  training_loss: 1.9011 (1.8973)  classification_loss: 1.8068 (1.8049)  loss_mask: 0.0891 (0.0925)  time: 0.1618  data: 0.0002  max mem: 4132
[19:23:51.296119] Epoch: [3]  [560/781]  eta: 0:00:36  lr: 0.000186  training_loss: 1.8543 (1.8961)  classification_loss: 1.7836 (1.8039)  loss_mask: 0.0847 (0.0922)  time: 0.1613  data: 0.0002  max mem: 4132
[19:23:54.594782] Epoch: [3]  [580/781]  eta: 0:00:32  lr: 0.000187  training_loss: 1.8846 (1.8956)  classification_loss: 1.7884 (1.8034)  loss_mask: 0.0880 (0.0921)  time: 0.1649  data: 0.0003  max mem: 4132
[19:23:57.820397] Epoch: [3]  [600/781]  eta: 0:00:29  lr: 0.000188  training_loss: 1.8655 (1.8947)  classification_loss: 1.7722 (1.8026)  loss_mask: 0.0866 (0.0920)  time: 0.1612  data: 0.0002  max mem: 4132
[19:24:01.110784] Epoch: [3]  [620/781]  eta: 0:00:26  lr: 0.000190  training_loss: 1.8304 (1.8931)  classification_loss: 1.7371 (1.8012)  loss_mask: 0.0843 (0.0918)  time: 0.1644  data: 0.0001  max mem: 4132
[19:24:04.343167] Epoch: [3]  [640/781]  eta: 0:00:23  lr: 0.000191  training_loss: 1.8525 (1.8920)  classification_loss: 1.7723 (1.8004)  loss_mask: 0.0828 (0.0916)  time: 0.1615  data: 0.0003  max mem: 4132
[19:24:07.567252] Epoch: [3]  [660/781]  eta: 0:00:19  lr: 0.000192  training_loss: 1.8450 (1.8909)  classification_loss: 1.7715 (1.7997)  loss_mask: 0.0791 (0.0912)  time: 0.1610  data: 0.0002  max mem: 4132
[19:24:10.846300] Epoch: [3]  [680/781]  eta: 0:00:16  lr: 0.000194  training_loss: 1.8181 (1.8892)  classification_loss: 1.7425 (1.7983)  loss_mask: 0.0806 (0.0909)  time: 0.1639  data: 0.0002  max mem: 4132
[19:24:14.102505] Epoch: [3]  [700/781]  eta: 0:00:13  lr: 0.000195  training_loss: 1.8406 (1.8876)  classification_loss: 1.7522 (1.7969)  loss_mask: 0.0840 (0.0907)  time: 0.1627  data: 0.0002  max mem: 4132
[19:24:17.331385] Epoch: [3]  [720/781]  eta: 0:00:09  lr: 0.000196  training_loss: 1.8414 (1.8868)  classification_loss: 1.7597 (1.7963)  loss_mask: 0.0814 (0.0906)  time: 0.1613  data: 0.0002  max mem: 4132
[19:24:20.557651] Epoch: [3]  [740/781]  eta: 0:00:06  lr: 0.000197  training_loss: 1.8492 (1.8855)  classification_loss: 1.7665 (1.7950)  loss_mask: 0.0868 (0.0905)  time: 0.1612  data: 0.0003  max mem: 4132
[19:24:23.783159] Epoch: [3]  [760/781]  eta: 0:00:03  lr: 0.000199  training_loss: 1.8639 (1.8858)  classification_loss: 1.7688 (1.7951)  loss_mask: 0.0931 (0.0907)  time: 0.1612  data: 0.0002  max mem: 4132
[19:24:27.006798] Epoch: [3]  [780/781]  eta: 0:00:00  lr: 0.000200  training_loss: 1.8705 (1.8850)  classification_loss: 1.7872 (1.7945)  loss_mask: 0.0838 (0.0906)  time: 0.1611  data: 0.0003  max mem: 4132
[19:24:27.161094] Epoch: [3] Total time: 0:02:07 (0.1633 s / it)
[19:24:27.161575] Averaged stats: lr: 0.000200  training_loss: 1.8705 (1.8850)  classification_loss: 1.7872 (1.7945)  loss_mask: 0.0838 (0.0906)
[19:24:27.772063] Test:  [  0/157]  eta: 0:01:35  testing_loss: 1.3663 (1.3663)  acc1: 59.3750 (59.3750)  acc5: 92.1875 (92.1875)  time: 0.6063  data: 0.5773  max mem: 4132
[19:24:28.054227] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 1.4714 (1.5048)  acc1: 48.4375 (47.3011)  acc5: 92.1875 (91.9034)  time: 0.0805  data: 0.0526  max mem: 4132
[19:24:28.340819] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.4518 (1.4696)  acc1: 50.0000 (49.5536)  acc5: 92.1875 (92.6339)  time: 0.0282  data: 0.0002  max mem: 4132
[19:24:28.625094] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 1.4518 (1.4679)  acc1: 53.1250 (50.6048)  acc5: 93.7500 (92.4395)  time: 0.0284  data: 0.0002  max mem: 4132
[19:24:28.908187] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 1.4920 (1.4788)  acc1: 51.5625 (50.3430)  acc5: 90.6250 (92.0732)  time: 0.0282  data: 0.0003  max mem: 4132
[19:24:29.188670] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.4954 (1.4815)  acc1: 48.4375 (49.9081)  acc5: 92.1875 (92.1569)  time: 0.0281  data: 0.0003  max mem: 4132
[19:24:29.468943] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.4410 (1.4746)  acc1: 50.0000 (50.3586)  acc5: 92.1875 (92.2387)  time: 0.0279  data: 0.0002  max mem: 4132
[19:24:29.753291] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.4391 (1.4691)  acc1: 51.5625 (50.4621)  acc5: 92.1875 (92.4956)  time: 0.0281  data: 0.0002  max mem: 4132
[19:24:30.035279] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.4415 (1.4702)  acc1: 48.4375 (50.3279)  acc5: 93.7500 (92.5926)  time: 0.0282  data: 0.0002  max mem: 4132
[19:24:30.316026] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.4736 (1.4715)  acc1: 50.0000 (50.1889)  acc5: 92.1875 (92.5996)  time: 0.0280  data: 0.0002  max mem: 4132
[19:24:30.595439] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.5117 (1.4767)  acc1: 43.7500 (49.5204)  acc5: 92.1875 (92.6052)  time: 0.0279  data: 0.0002  max mem: 4132
[19:24:30.875511] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.5168 (1.4794)  acc1: 43.7500 (49.3384)  acc5: 92.1875 (92.6380)  time: 0.0279  data: 0.0001  max mem: 4132
[19:24:31.155861] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.4767 (1.4785)  acc1: 48.4375 (49.4189)  acc5: 93.7500 (92.7944)  time: 0.0279  data: 0.0001  max mem: 4132
[19:24:31.437060] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.4741 (1.4800)  acc1: 50.0000 (49.3798)  acc5: 93.7500 (92.7958)  time: 0.0280  data: 0.0002  max mem: 4132
[19:24:31.717077] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.4737 (1.4791)  acc1: 48.4375 (49.5678)  acc5: 92.1875 (92.7748)  time: 0.0279  data: 0.0002  max mem: 4132
[19:24:31.994918] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.4411 (1.4771)  acc1: 50.0000 (49.7413)  acc5: 93.7500 (92.7359)  time: 0.0278  data: 0.0001  max mem: 4132
[19:24:32.145189] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.4240 (1.4764)  acc1: 50.0000 (49.6500)  acc5: 93.7500 (92.8100)  time: 0.0268  data: 0.0001  max mem: 4132
[19:24:32.306119] Test: Total time: 0:00:05 (0.0327 s / it)
[19:24:32.306708] * Acc@1 49.650 Acc@5 92.810 loss 1.476
[19:24:32.307019] Accuracy of the network on the 10000 test images: 49.6%
[19:24:32.307206] Max accuracy: 49.65%
[19:24:32.865662] log_dir: ./output_dir
[19:24:33.669051] Epoch: [4]  [  0/781]  eta: 0:10:26  lr: 0.000200  training_loss: 1.8002 (1.8002)  classification_loss: 1.7146 (1.7146)  loss_mask: 0.0856 (0.0856)  time: 0.8017  data: 0.6311  max mem: 4132
[19:24:36.914619] Epoch: [4]  [ 20/781]  eta: 0:02:26  lr: 0.000201  training_loss: 1.8164 (1.8356)  classification_loss: 1.7280 (1.7523)  loss_mask: 0.0834 (0.0833)  time: 0.1622  data: 0.0006  max mem: 4132
[19:24:40.139390] Epoch: [4]  [ 40/781]  eta: 0:02:11  lr: 0.000203  training_loss: 1.8428 (1.8387)  classification_loss: 1.7559 (1.7547)  loss_mask: 0.0845 (0.0839)  time: 0.1611  data: 0.0002  max mem: 4132
[19:24:43.382530] Epoch: [4]  [ 60/781]  eta: 0:02:04  lr: 0.000204  training_loss: 1.8549 (1.8436)  classification_loss: 1.7602 (1.7599)  loss_mask: 0.0834 (0.0837)  time: 0.1620  data: 0.0002  max mem: 4132
[19:24:46.615808] Epoch: [4]  [ 80/781]  eta: 0:01:58  lr: 0.000205  training_loss: 1.8729 (1.8513)  classification_loss: 1.7986 (1.7683)  loss_mask: 0.0772 (0.0829)  time: 0.1616  data: 0.0002  max mem: 4132
[19:24:49.835438] Epoch: [4]  [100/781]  eta: 0:01:54  lr: 0.000206  training_loss: 1.8605 (1.8542)  classification_loss: 1.7786 (1.7715)  loss_mask: 0.0819 (0.0828)  time: 0.1609  data: 0.0002  max mem: 4132
[19:24:53.076229] Epoch: [4]  [120/781]  eta: 0:01:50  lr: 0.000208  training_loss: 1.8156 (1.8483)  classification_loss: 1.7386 (1.7659)  loss_mask: 0.0798 (0.0823)  time: 0.1620  data: 0.0002  max mem: 4132
[19:24:56.334438] Epoch: [4]  [140/781]  eta: 0:01:46  lr: 0.000209  training_loss: 1.8020 (1.8428)  classification_loss: 1.7299 (1.7606)  loss_mask: 0.0808 (0.0822)  time: 0.1628  data: 0.0003  max mem: 4132
[19:24:59.552159] Epoch: [4]  [160/781]  eta: 0:01:42  lr: 0.000210  training_loss: 1.8217 (1.8383)  classification_loss: 1.7379 (1.7545)  loss_mask: 0.0928 (0.0838)  time: 0.1608  data: 0.0002  max mem: 4132
[19:25:02.778177] Epoch: [4]  [180/781]  eta: 0:01:39  lr: 0.000212  training_loss: 1.8378 (1.8394)  classification_loss: 1.7559 (1.7558)  loss_mask: 0.0805 (0.0837)  time: 0.1612  data: 0.0003  max mem: 4132
[19:25:06.032810] Epoch: [4]  [200/781]  eta: 0:01:35  lr: 0.000213  training_loss: 1.8391 (1.8413)  classification_loss: 1.7546 (1.7578)  loss_mask: 0.0812 (0.0834)  time: 0.1626  data: 0.0002  max mem: 4132
[19:25:09.260499] Epoch: [4]  [220/781]  eta: 0:01:32  lr: 0.000214  training_loss: 1.8234 (1.8399)  classification_loss: 1.7473 (1.7571)  loss_mask: 0.0772 (0.0828)  time: 0.1613  data: 0.0003  max mem: 4132
[19:25:12.509925] Epoch: [4]  [240/781]  eta: 0:01:28  lr: 0.000215  training_loss: 1.8671 (1.8415)  classification_loss: 1.7913 (1.7588)  loss_mask: 0.0805 (0.0827)  time: 0.1624  data: 0.0002  max mem: 4132
[19:25:15.726577] Epoch: [4]  [260/781]  eta: 0:01:25  lr: 0.000217  training_loss: 1.8208 (1.8395)  classification_loss: 1.7466 (1.7566)  loss_mask: 0.0843 (0.0828)  time: 0.1607  data: 0.0002  max mem: 4132
[19:25:18.964686] Epoch: [4]  [280/781]  eta: 0:01:22  lr: 0.000218  training_loss: 1.8212 (1.8395)  classification_loss: 1.7398 (1.7565)  loss_mask: 0.0837 (0.0830)  time: 0.1618  data: 0.0003  max mem: 4132
[19:25:22.208320] Epoch: [4]  [300/781]  eta: 0:01:18  lr: 0.000219  training_loss: 1.8403 (1.8399)  classification_loss: 1.7721 (1.7573)  loss_mask: 0.0745 (0.0826)  time: 0.1621  data: 0.0003  max mem: 4132
[19:25:25.444772] Epoch: [4]  [320/781]  eta: 0:01:15  lr: 0.000220  training_loss: 1.7942 (1.8372)  classification_loss: 1.7144 (1.7551)  loss_mask: 0.0742 (0.0821)  time: 0.1618  data: 0.0002  max mem: 4132
[19:25:28.661610] Epoch: [4]  [340/781]  eta: 0:01:12  lr: 0.000222  training_loss: 1.8221 (1.8364)  classification_loss: 1.7426 (1.7545)  loss_mask: 0.0747 (0.0819)  time: 0.1608  data: 0.0002  max mem: 4132
[19:25:31.891842] Epoch: [4]  [360/781]  eta: 0:01:08  lr: 0.000223  training_loss: 1.8630 (1.8378)  classification_loss: 1.7762 (1.7553)  loss_mask: 0.0907 (0.0826)  time: 0.1614  data: 0.0002  max mem: 4132
[19:25:35.133757] Epoch: [4]  [380/781]  eta: 0:01:05  lr: 0.000224  training_loss: 1.8403 (1.8380)  classification_loss: 1.7575 (1.7552)  loss_mask: 0.0819 (0.0828)  time: 0.1620  data: 0.0003  max mem: 4132
[19:25:38.419023] Epoch: [4]  [400/781]  eta: 0:01:02  lr: 0.000226  training_loss: 1.8035 (1.8372)  classification_loss: 1.7219 (1.7547)  loss_mask: 0.0784 (0.0825)  time: 0.1642  data: 0.0002  max mem: 4132
[19:25:41.649001] Epoch: [4]  [420/781]  eta: 0:00:58  lr: 0.000227  training_loss: 1.7843 (1.8352)  classification_loss: 1.7164 (1.7529)  loss_mask: 0.0770 (0.0823)  time: 0.1614  data: 0.0003  max mem: 4132
[19:25:44.869625] Epoch: [4]  [440/781]  eta: 0:00:55  lr: 0.000228  training_loss: 1.8234 (1.8346)  classification_loss: 1.7490 (1.7525)  loss_mask: 0.0768 (0.0821)  time: 0.1609  data: 0.0002  max mem: 4132
[19:25:48.081948] Epoch: [4]  [460/781]  eta: 0:00:52  lr: 0.000229  training_loss: 1.8337 (1.8338)  classification_loss: 1.7513 (1.7520)  loss_mask: 0.0740 (0.0818)  time: 0.1605  data: 0.0002  max mem: 4132
[19:25:51.298803] Epoch: [4]  [480/781]  eta: 0:00:49  lr: 0.000231  training_loss: 1.8660 (1.8352)  classification_loss: 1.7786 (1.7532)  loss_mask: 0.0849 (0.0820)  time: 0.1608  data: 0.0002  max mem: 4132
[19:25:54.525219] Epoch: [4]  [500/781]  eta: 0:00:45  lr: 0.000232  training_loss: 1.7670 (1.8337)  classification_loss: 1.6931 (1.7516)  loss_mask: 0.0851 (0.0821)  time: 0.1612  data: 0.0002  max mem: 4132
[19:25:57.769978] Epoch: [4]  [520/781]  eta: 0:00:42  lr: 0.000233  training_loss: 1.7720 (1.8319)  classification_loss: 1.6970 (1.7498)  loss_mask: 0.0792 (0.0821)  time: 0.1621  data: 0.0003  max mem: 4132
[19:26:01.007677] Epoch: [4]  [540/781]  eta: 0:00:39  lr: 0.000235  training_loss: 1.8296 (1.8320)  classification_loss: 1.7560 (1.7500)  loss_mask: 0.0798 (0.0820)  time: 0.1618  data: 0.0002  max mem: 4132
[19:26:04.252347] Epoch: [4]  [560/781]  eta: 0:00:35  lr: 0.000236  training_loss: 1.7901 (1.8308)  classification_loss: 1.7189 (1.7489)  loss_mask: 0.0787 (0.0819)  time: 0.1622  data: 0.0002  max mem: 4132
[19:26:07.479798] Epoch: [4]  [580/781]  eta: 0:00:32  lr: 0.000237  training_loss: 1.8337 (1.8306)  classification_loss: 1.7169 (1.7485)  loss_mask: 0.0809 (0.0820)  time: 0.1613  data: 0.0001  max mem: 4132
[19:26:10.733327] Epoch: [4]  [600/781]  eta: 0:00:29  lr: 0.000238  training_loss: 1.7934 (1.8300)  classification_loss: 1.7108 (1.7478)  loss_mask: 0.0862 (0.0822)  time: 0.1626  data: 0.0002  max mem: 4132
[19:26:13.952849] Epoch: [4]  [620/781]  eta: 0:00:26  lr: 0.000240  training_loss: 1.7711 (1.8278)  classification_loss: 1.6903 (1.7457)  loss_mask: 0.0781 (0.0821)  time: 0.1609  data: 0.0003  max mem: 4132
[19:26:17.205506] Epoch: [4]  [640/781]  eta: 0:00:22  lr: 0.000241  training_loss: 1.7810 (1.8273)  classification_loss: 1.7089 (1.7453)  loss_mask: 0.0768 (0.0819)  time: 0.1626  data: 0.0002  max mem: 4132
[19:26:20.435670] Epoch: [4]  [660/781]  eta: 0:00:19  lr: 0.000242  training_loss: 1.8124 (1.8270)  classification_loss: 1.7365 (1.7453)  loss_mask: 0.0716 (0.0816)  time: 0.1614  data: 0.0002  max mem: 4132
[19:26:23.666671] Epoch: [4]  [680/781]  eta: 0:00:16  lr: 0.000244  training_loss: 1.7990 (1.8267)  classification_loss: 1.7198 (1.7453)  loss_mask: 0.0728 (0.0815)  time: 0.1615  data: 0.0002  max mem: 4132
[19:26:26.899420] Epoch: [4]  [700/781]  eta: 0:00:13  lr: 0.000245  training_loss: 1.8063 (1.8275)  classification_loss: 1.7253 (1.7453)  loss_mask: 0.1105 (0.0822)  time: 0.1615  data: 0.0002  max mem: 4132
[19:26:30.124926] Epoch: [4]  [720/781]  eta: 0:00:09  lr: 0.000246  training_loss: 1.8088 (1.8273)  classification_loss: 1.7263 (1.7449)  loss_mask: 0.0885 (0.0825)  time: 0.1612  data: 0.0002  max mem: 4132
[19:26:33.340939] Epoch: [4]  [740/781]  eta: 0:00:06  lr: 0.000247  training_loss: 1.7806 (1.8262)  classification_loss: 1.6935 (1.7438)  loss_mask: 0.0804 (0.0824)  time: 0.1607  data: 0.0003  max mem: 4132
[19:26:36.559762] Epoch: [4]  [760/781]  eta: 0:00:03  lr: 0.000249  training_loss: 1.8038 (1.8256)  classification_loss: 1.7278 (1.7433)  loss_mask: 0.0792 (0.0823)  time: 0.1608  data: 0.0002  max mem: 4132
[19:26:39.770353] Epoch: [4]  [780/781]  eta: 0:00:00  lr: 0.000250  training_loss: 1.7504 (1.8242)  classification_loss: 1.6744 (1.7422)  loss_mask: 0.0732 (0.0820)  time: 0.1604  data: 0.0002  max mem: 4132
[19:26:39.941665] Epoch: [4] Total time: 0:02:07 (0.1627 s / it)
[19:26:39.942267] Averaged stats: lr: 0.000250  training_loss: 1.7504 (1.8242)  classification_loss: 1.6744 (1.7422)  loss_mask: 0.0732 (0.0820)
[19:26:40.662485] Test:  [  0/157]  eta: 0:01:52  testing_loss: 1.3649 (1.3649)  acc1: 46.8750 (46.8750)  acc5: 92.1875 (92.1875)  time: 0.7164  data: 0.6731  max mem: 4132
[19:26:40.955795] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.3812 (1.4110)  acc1: 46.8750 (48.7216)  acc5: 93.7500 (94.3182)  time: 0.0916  data: 0.0617  max mem: 4132
[19:26:41.244411] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.3704 (1.3770)  acc1: 50.0000 (51.2649)  acc5: 93.7500 (93.6012)  time: 0.0289  data: 0.0004  max mem: 4132
[19:26:41.528444] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.3521 (1.3764)  acc1: 53.1250 (52.3690)  acc5: 92.1875 (93.2964)  time: 0.0285  data: 0.0002  max mem: 4132
[19:26:41.817320] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.3798 (1.3800)  acc1: 53.1250 (52.5915)  acc5: 92.1875 (92.8735)  time: 0.0285  data: 0.0003  max mem: 4132
[19:26:42.103261] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.3520 (1.3756)  acc1: 53.1250 (52.8186)  acc5: 92.1875 (92.9841)  time: 0.0286  data: 0.0003  max mem: 4132
[19:26:42.388179] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.3520 (1.3706)  acc1: 53.1250 (53.1762)  acc5: 92.1875 (93.0840)  time: 0.0284  data: 0.0002  max mem: 4132
[19:26:42.671690] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.3426 (1.3651)  acc1: 56.2500 (54.0273)  acc5: 93.7500 (93.2658)  time: 0.0283  data: 0.0002  max mem: 4132
[19:26:42.960390] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.3461 (1.3675)  acc1: 56.2500 (53.6458)  acc5: 93.7500 (93.2870)  time: 0.0285  data: 0.0002  max mem: 4132
[19:26:43.247000] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.4021 (1.3707)  acc1: 54.6875 (53.6401)  acc5: 93.7500 (93.2692)  time: 0.0286  data: 0.0002  max mem: 4132
[19:26:43.535160] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.4018 (1.3760)  acc1: 51.5625 (53.2797)  acc5: 93.7500 (93.1776)  time: 0.0286  data: 0.0002  max mem: 4132
[19:26:43.819596] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.4199 (1.3790)  acc1: 51.5625 (53.2235)  acc5: 92.1875 (93.0602)  time: 0.0285  data: 0.0002  max mem: 4132
[19:26:44.100762] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.3764 (1.3768)  acc1: 53.1250 (53.3704)  acc5: 92.1875 (93.0398)  time: 0.0281  data: 0.0001  max mem: 4132
[19:26:44.396075] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.3546 (1.3779)  acc1: 54.6875 (53.2801)  acc5: 92.1875 (92.9270)  time: 0.0286  data: 0.0002  max mem: 4132
[19:26:44.676488] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.3856 (1.3779)  acc1: 53.1250 (53.3355)  acc5: 92.1875 (92.8967)  time: 0.0286  data: 0.0002  max mem: 4132
[19:26:44.956610] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.3571 (1.3752)  acc1: 56.2500 (53.5389)  acc5: 92.1875 (92.8394)  time: 0.0279  data: 0.0001  max mem: 4132
[19:26:45.107504] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.3387 (1.3753)  acc1: 56.2500 (53.4600)  acc5: 92.1875 (92.8800)  time: 0.0270  data: 0.0001  max mem: 4132
[19:26:45.275693] Test: Total time: 0:00:05 (0.0340 s / it)
[19:26:45.276443] * Acc@1 53.460 Acc@5 92.880 loss 1.375
[19:26:45.276739] Accuracy of the network on the 10000 test images: 53.5%
[19:26:45.276929] Max accuracy: 53.46%
[19:26:45.564489] log_dir: ./output_dir
[19:26:46.356405] Epoch: [5]  [  0/781]  eta: 0:10:17  lr: 0.000250  training_loss: 1.6699 (1.6699)  classification_loss: 1.6041 (1.6041)  loss_mask: 0.0658 (0.0658)  time: 0.7902  data: 0.6063  max mem: 4132
[19:26:49.600835] Epoch: [5]  [ 20/781]  eta: 0:02:26  lr: 0.000250  training_loss: 1.7595 (1.7657)  classification_loss: 1.6854 (1.6920)  loss_mask: 0.0748 (0.0737)  time: 0.1621  data: 0.0002  max mem: 4132
[19:26:52.826760] Epoch: [5]  [ 40/781]  eta: 0:02:11  lr: 0.000250  training_loss: 1.8065 (1.7997)  classification_loss: 1.7011 (1.7158)  loss_mask: 0.0925 (0.0839)  time: 0.1612  data: 0.0002  max mem: 4132
[19:26:56.085406] Epoch: [5]  [ 60/781]  eta: 0:02:04  lr: 0.000250  training_loss: 1.7936 (1.7935)  classification_loss: 1.7114 (1.7119)  loss_mask: 0.0776 (0.0816)  time: 0.1629  data: 0.0004  max mem: 4132
[19:26:59.314191] Epoch: [5]  [ 80/781]  eta: 0:01:58  lr: 0.000250  training_loss: 1.8080 (1.7982)  classification_loss: 1.7320 (1.7170)  loss_mask: 0.0771 (0.0812)  time: 0.1614  data: 0.0002  max mem: 4132
[19:27:02.549574] Epoch: [5]  [100/781]  eta: 0:01:54  lr: 0.000250  training_loss: 1.7900 (1.7972)  classification_loss: 1.7115 (1.7170)  loss_mask: 0.0720 (0.0802)  time: 0.1617  data: 0.0002  max mem: 4132
[19:27:05.784259] Epoch: [5]  [120/781]  eta: 0:01:50  lr: 0.000250  training_loss: 1.7552 (1.7922)  classification_loss: 1.6864 (1.7129)  loss_mask: 0.0728 (0.0793)  time: 0.1616  data: 0.0002  max mem: 4132
[19:27:09.008372] Epoch: [5]  [140/781]  eta: 0:01:46  lr: 0.000250  training_loss: 1.7270 (1.7858)  classification_loss: 1.6493 (1.7062)  loss_mask: 0.0792 (0.0796)  time: 0.1611  data: 0.0002  max mem: 4132
[19:27:12.235723] Epoch: [5]  [160/781]  eta: 0:01:42  lr: 0.000250  training_loss: 1.7461 (1.7834)  classification_loss: 1.6793 (1.7045)  loss_mask: 0.0736 (0.0789)  time: 0.1613  data: 0.0002  max mem: 4132
[19:27:15.461855] Epoch: [5]  [180/781]  eta: 0:01:39  lr: 0.000250  training_loss: 1.7672 (1.7848)  classification_loss: 1.6846 (1.7059)  loss_mask: 0.0796 (0.0788)  time: 0.1612  data: 0.0002  max mem: 4132
[19:27:18.692146] Epoch: [5]  [200/781]  eta: 0:01:35  lr: 0.000250  training_loss: 1.7890 (1.7861)  classification_loss: 1.6868 (1.7067)  loss_mask: 0.0822 (0.0794)  time: 0.1614  data: 0.0004  max mem: 4132
[19:27:21.924934] Epoch: [5]  [220/781]  eta: 0:01:32  lr: 0.000250  training_loss: 1.8055 (1.7865)  classification_loss: 1.7374 (1.7077)  loss_mask: 0.0712 (0.0789)  time: 0.1616  data: 0.0003  max mem: 4132
[19:27:25.172840] Epoch: [5]  [240/781]  eta: 0:01:28  lr: 0.000250  training_loss: 1.7842 (1.7887)  classification_loss: 1.6973 (1.7102)  loss_mask: 0.0683 (0.0785)  time: 0.1623  data: 0.0002  max mem: 4132
[19:27:28.408115] Epoch: [5]  [260/781]  eta: 0:01:25  lr: 0.000250  training_loss: 1.7705 (1.7868)  classification_loss: 1.6928 (1.7086)  loss_mask: 0.0725 (0.0782)  time: 0.1617  data: 0.0002  max mem: 4132
[19:27:31.657290] Epoch: [5]  [280/781]  eta: 0:01:22  lr: 0.000250  training_loss: 1.8094 (1.7884)  classification_loss: 1.7314 (1.7100)  loss_mask: 0.0759 (0.0785)  time: 0.1624  data: 0.0005  max mem: 4132
[19:27:34.899121] Epoch: [5]  [300/781]  eta: 0:01:18  lr: 0.000250  training_loss: 1.7961 (1.7904)  classification_loss: 1.7206 (1.7112)  loss_mask: 0.0843 (0.0792)  time: 0.1620  data: 0.0002  max mem: 4132
[19:27:38.132366] Epoch: [5]  [320/781]  eta: 0:01:15  lr: 0.000250  training_loss: 1.8016 (1.7909)  classification_loss: 1.7359 (1.7120)  loss_mask: 0.0728 (0.0789)  time: 0.1616  data: 0.0003  max mem: 4132
[19:27:41.350931] Epoch: [5]  [340/781]  eta: 0:01:12  lr: 0.000250  training_loss: 1.7667 (1.7892)  classification_loss: 1.6943 (1.7108)  loss_mask: 0.0722 (0.0785)  time: 0.1608  data: 0.0003  max mem: 4132
[19:27:44.573450] Epoch: [5]  [360/781]  eta: 0:01:08  lr: 0.000250  training_loss: 1.7852 (1.7890)  classification_loss: 1.7063 (1.7108)  loss_mask: 0.0727 (0.0782)  time: 0.1611  data: 0.0002  max mem: 4132
[19:27:47.790709] Epoch: [5]  [380/781]  eta: 0:01:05  lr: 0.000250  training_loss: 1.7488 (1.7880)  classification_loss: 1.6859 (1.7101)  loss_mask: 0.0712 (0.0779)  time: 0.1608  data: 0.0002  max mem: 4132
[19:27:51.007486] Epoch: [5]  [400/781]  eta: 0:01:02  lr: 0.000250  training_loss: 1.7689 (1.7879)  classification_loss: 1.7081 (1.7103)  loss_mask: 0.0721 (0.0776)  time: 0.1608  data: 0.0002  max mem: 4132
[19:27:54.226249] Epoch: [5]  [420/781]  eta: 0:00:58  lr: 0.000250  training_loss: 1.7690 (1.7879)  classification_loss: 1.7003 (1.7106)  loss_mask: 0.0715 (0.0773)  time: 0.1609  data: 0.0002  max mem: 4132
[19:27:57.450248] Epoch: [5]  [440/781]  eta: 0:00:55  lr: 0.000250  training_loss: 1.7735 (1.7873)  classification_loss: 1.7123 (1.7101)  loss_mask: 0.0713 (0.0772)  time: 0.1611  data: 0.0002  max mem: 4132
[19:28:00.688644] Epoch: [5]  [460/781]  eta: 0:00:52  lr: 0.000250  training_loss: 1.7300 (1.7848)  classification_loss: 1.6600 (1.7077)  loss_mask: 0.0701 (0.0770)  time: 0.1618  data: 0.0002  max mem: 4132
[19:28:03.933540] Epoch: [5]  [480/781]  eta: 0:00:49  lr: 0.000250  training_loss: 1.7856 (1.7844)  classification_loss: 1.7019 (1.7076)  loss_mask: 0.0683 (0.0767)  time: 0.1622  data: 0.0003  max mem: 4132
[19:28:07.172984] Epoch: [5]  [500/781]  eta: 0:00:45  lr: 0.000250  training_loss: 1.7531 (1.7835)  classification_loss: 1.6919 (1.7071)  loss_mask: 0.0680 (0.0764)  time: 0.1619  data: 0.0003  max mem: 4132
[19:28:10.398265] Epoch: [5]  [520/781]  eta: 0:00:42  lr: 0.000250  training_loss: 1.7539 (1.7824)  classification_loss: 1.6924 (1.7063)  loss_mask: 0.0678 (0.0762)  time: 0.1612  data: 0.0002  max mem: 4132
[19:28:13.621877] Epoch: [5]  [540/781]  eta: 0:00:39  lr: 0.000250  training_loss: 1.7746 (1.7826)  classification_loss: 1.7012 (1.7068)  loss_mask: 0.0655 (0.0758)  time: 0.1611  data: 0.0003  max mem: 4132

[19:28:16.858724] Epoch: [5]  [560/781]  eta: 0:00:35  lr: 0.000250  training_loss: 1.7537 (1.7818)  classification_loss: 1.6779 (1.7063)  loss_mask: 0.0682 (0.0755)  time: 0.1618  data: 0.0004  max mem: 4132
[19:28:20.091050] Epoch: [5]  [580/781]  eta: 0:00:32  lr: 0.000250  training_loss: 1.7726 (1.7814)  classification_loss: 1.7175 (1.7061)  loss_mask: 0.0702 (0.0753)  time: 0.1615  data: 0.0002  max mem: 4132
[19:28:23.307024] Epoch: [5]  [600/781]  eta: 0:00:29  lr: 0.000250  training_loss: 1.7781 (1.7816)  classification_loss: 1.7076 (1.7062)  loss_mask: 0.0773 (0.0754)  time: 0.1607  data: 0.0002  max mem: 4132
[19:28:26.537866] Epoch: [5]  [620/781]  eta: 0:00:26  lr: 0.000250  training_loss: 1.7307 (1.7808)  classification_loss: 1.6789 (1.7056)  loss_mask: 0.0681 (0.0752)  time: 0.1615  data: 0.0002  max mem: 4132
[19:28:29.804429] Epoch: [5]  [640/781]  eta: 0:00:22  lr: 0.000250  training_loss: 1.7504 (1.7805)  classification_loss: 1.6916 (1.7054)  loss_mask: 0.0710 (0.0751)  time: 0.1632  data: 0.0002  max mem: 4132
[19:28:33.058259] Epoch: [5]  [660/781]  eta: 0:00:19  lr: 0.000250  training_loss: 1.7754 (1.7805)  classification_loss: 1.6914 (1.7052)  loss_mask: 0.0825 (0.0753)  time: 0.1626  data: 0.0002  max mem: 4132
[19:28:36.309489] Epoch: [5]  [680/781]  eta: 0:00:16  lr: 0.000250  training_loss: 1.7352 (1.7795)  classification_loss: 1.6629 (1.7043)  loss_mask: 0.0649 (0.0752)  time: 0.1625  data: 0.0002  max mem: 4132
[19:28:39.521166] Epoch: [5]  [700/781]  eta: 0:00:13  lr: 0.000250  training_loss: 1.7594 (1.7788)  classification_loss: 1.6891 (1.7037)  loss_mask: 0.0706 (0.0751)  time: 0.1605  data: 0.0002  max mem: 4132
[19:28:42.741293] Epoch: [5]  [720/781]  eta: 0:00:09  lr: 0.000250  training_loss: 1.7818 (1.7790)  classification_loss: 1.7180 (1.7038)  loss_mask: 0.0758 (0.0751)  time: 0.1609  data: 0.0002  max mem: 4132
[19:28:45.996429] Epoch: [5]  [740/781]  eta: 0:00:06  lr: 0.000250  training_loss: 1.7560 (1.7785)  classification_loss: 1.6837 (1.7034)  loss_mask: 0.0707 (0.0751)  time: 0.1627  data: 0.0002  max mem: 4132
[19:28:49.257437] Epoch: [5]  [760/781]  eta: 0:00:03  lr: 0.000250  training_loss: 1.7167 (1.7776)  classification_loss: 1.6472 (1.7026)  loss_mask: 0.0689 (0.0749)  time: 0.1630  data: 0.0002  max mem: 4132
[19:28:52.511655] Epoch: [5]  [780/781]  eta: 0:00:00  lr: 0.000250  training_loss: 1.7556 (1.7773)  classification_loss: 1.6886 (1.7025)  loss_mask: 0.0673 (0.0748)  time: 0.1626  data: 0.0002  max mem: 4132
[19:28:52.666347] Epoch: [5] Total time: 0:02:07 (0.1627 s / it)
[19:28:52.666784] Averaged stats: lr: 0.000250  training_loss: 1.7556 (1.7773)  classification_loss: 1.6886 (1.7025)  loss_mask: 0.0673 (0.0748)
[19:28:53.363914] Test:  [  0/157]  eta: 0:01:48  testing_loss: 1.2580 (1.2580)  acc1: 51.5625 (51.5625)  acc5: 90.6250 (90.6250)  time: 0.6930  data: 0.6618  max mem: 4132
[19:28:53.649987] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.3271 (1.3453)  acc1: 51.5625 (52.9830)  acc5: 93.7500 (93.7500)  time: 0.0887  data: 0.0603  max mem: 4132
[19:28:53.932054] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.3120 (1.3168)  acc1: 53.1250 (54.0923)  acc5: 93.7500 (94.1964)  time: 0.0282  data: 0.0001  max mem: 4132
[19:28:54.216014] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.3035 (1.3195)  acc1: 54.6875 (54.3851)  acc5: 95.3125 (94.3548)  time: 0.0282  data: 0.0002  max mem: 4132
[19:28:54.499900] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.3057 (1.3205)  acc1: 53.1250 (54.3826)  acc5: 93.7500 (94.0549)  time: 0.0283  data: 0.0002  max mem: 4132
[19:28:54.786347] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.2940 (1.3148)  acc1: 54.6875 (54.5343)  acc5: 93.7500 (94.1789)  time: 0.0284  data: 0.0001  max mem: 4132
[19:28:55.067960] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.2710 (1.3055)  acc1: 56.2500 (55.1230)  acc5: 95.3125 (94.2623)  time: 0.0283  data: 0.0001  max mem: 4132
[19:28:55.352208] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.2500 (1.2952)  acc1: 59.3750 (55.9199)  acc5: 95.3125 (94.4762)  time: 0.0282  data: 0.0001  max mem: 4132
[19:28:55.641620] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.2500 (1.2973)  acc1: 57.8125 (55.8835)  acc5: 95.3125 (94.5409)  time: 0.0286  data: 0.0002  max mem: 4132
[19:28:55.921721] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.2944 (1.2981)  acc1: 56.2500 (55.8551)  acc5: 95.3125 (94.5742)  time: 0.0284  data: 0.0002  max mem: 4132
[19:28:56.201467] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.3044 (1.3033)  acc1: 54.6875 (55.5229)  acc5: 95.3125 (94.5545)  time: 0.0279  data: 0.0001  max mem: 4132
[19:28:56.481762] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.3486 (1.3056)  acc1: 53.1250 (55.3350)  acc5: 93.7500 (94.4679)  time: 0.0279  data: 0.0001  max mem: 4132
[19:28:56.771589] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.2920 (1.3036)  acc1: 54.6875 (55.4752)  acc5: 93.7500 (94.5377)  time: 0.0284  data: 0.0002  max mem: 4132
[19:28:57.052190] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.2702 (1.3045)  acc1: 56.2500 (55.3912)  acc5: 93.7500 (94.4776)  time: 0.0284  data: 0.0002  max mem: 4132
[19:28:57.337183] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.3162 (1.3055)  acc1: 54.6875 (55.3635)  acc5: 93.7500 (94.5368)  time: 0.0282  data: 0.0001  max mem: 4132
[19:28:57.616093] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.2740 (1.3017)  acc1: 54.6875 (55.5671)  acc5: 93.7500 (94.4743)  time: 0.0281  data: 0.0001  max mem: 4132
[19:28:57.766035] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.2609 (1.3025)  acc1: 54.6875 (55.4900)  acc5: 93.7500 (94.5100)  time: 0.0269  data: 0.0001  max mem: 4132
[19:28:57.941781] Test: Total time: 0:00:05 (0.0336 s / it)
[19:28:57.942234] * Acc@1 55.490 Acc@5 94.510 loss 1.303
[19:28:57.942527] Accuracy of the network on the 10000 test images: 55.5%
[19:28:57.942742] Max accuracy: 55.49%
[19:28:58.432473] log_dir: ./output_dir
[19:28:59.339268] Epoch: [6]  [  0/781]  eta: 0:11:46  lr: 0.000250  training_loss: 1.7309 (1.7309)  classification_loss: 1.6731 (1.6731)  loss_mask: 0.0578 (0.0578)  time: 0.9051  data: 0.7073  max mem: 4132
[19:29:02.578989] Epoch: [6]  [ 20/781]  eta: 0:02:30  lr: 0.000250  training_loss: 1.7058 (1.7111)  classification_loss: 1.6436 (1.6477)  loss_mask: 0.0640 (0.0634)  time: 0.1619  data: 0.0002  max mem: 4132
[19:29:05.824954] Epoch: [6]  [ 40/781]  eta: 0:02:13  lr: 0.000250  training_loss: 1.7450 (1.7249)  classification_loss: 1.6713 (1.6591)  loss_mask: 0.0676 (0.0658)  time: 0.1622  data: 0.0003  max mem: 4132
[19:29:09.050771] Epoch: [6]  [ 60/781]  eta: 0:02:05  lr: 0.000250  training_loss: 1.7632 (1.7363)  classification_loss: 1.6830 (1.6670)  loss_mask: 0.0773 (0.0693)  time: 0.1612  data: 0.0002  max mem: 4132
[19:29:12.297720] Epoch: [6]  [ 80/781]  eta: 0:01:59  lr: 0.000250  training_loss: 1.7567 (1.7435)  classification_loss: 1.6925 (1.6746)  loss_mask: 0.0667 (0.0690)  time: 0.1623  data: 0.0003  max mem: 4132
[19:29:15.531110] Epoch: [6]  [100/781]  eta: 0:01:55  lr: 0.000250  training_loss: 1.7658 (1.7485)  classification_loss: 1.6926 (1.6792)  loss_mask: 0.0703 (0.0693)  time: 0.1616  data: 0.0002  max mem: 4132
[19:29:18.760969] Epoch: [6]  [120/781]  eta: 0:01:50  lr: 0.000250  training_loss: 1.7199 (1.7423)  classification_loss: 1.6548 (1.6729)  loss_mask: 0.0659 (0.0694)  time: 0.1614  data: 0.0002  max mem: 4132
[19:29:22.048483] Epoch: [6]  [140/781]  eta: 0:01:47  lr: 0.000250  training_loss: 1.6941 (1.7382)  classification_loss: 1.6262 (1.6695)  loss_mask: 0.0638 (0.0687)  time: 0.1643  data: 0.0002  max mem: 4132
[19:29:25.323095] Epoch: [6]  [160/781]  eta: 0:01:43  lr: 0.000250  training_loss: 1.7243 (1.7364)  classification_loss: 1.6726 (1.6677)  loss_mask: 0.0683 (0.0687)  time: 0.1637  data: 0.0003  max mem: 4132
[19:29:28.560189] Epoch: [6]  [180/781]  eta: 0:01:39  lr: 0.000250  training_loss: 1.7127 (1.7322)  classification_loss: 1.6413 (1.6638)  loss_mask: 0.0665 (0.0684)  time: 0.1618  data: 0.0002  max mem: 4132
[19:29:31.788850] Epoch: [6]  [200/781]  eta: 0:01:36  lr: 0.000250  training_loss: 1.7273 (1.7333)  classification_loss: 1.6657 (1.6648)  loss_mask: 0.0653 (0.0685)  time: 0.1613  data: 0.0003  max mem: 4132
[19:29:35.034573] Epoch: [6]  [220/781]  eta: 0:01:32  lr: 0.000250  training_loss: 1.7565 (1.7338)  classification_loss: 1.6695 (1.6644)  loss_mask: 0.0779 (0.0695)  time: 0.1622  data: 0.0002  max mem: 4132
[19:29:38.269854] Epoch: [6]  [240/781]  eta: 0:01:29  lr: 0.000250  training_loss: 1.7431 (1.7363)  classification_loss: 1.6836 (1.6664)  loss_mask: 0.0769 (0.0699)  time: 0.1617  data: 0.0002  max mem: 4132
[19:29:41.500937] Epoch: [6]  [260/781]  eta: 0:01:25  lr: 0.000250  training_loss: 1.7146 (1.7353)  classification_loss: 1.6485 (1.6656)  loss_mask: 0.0668 (0.0697)  time: 0.1615  data: 0.0002  max mem: 4132
[19:29:44.769603] Epoch: [6]  [280/781]  eta: 0:01:22  lr: 0.000250  training_loss: 1.7360 (1.7343)  classification_loss: 1.6716 (1.6649)  loss_mask: 0.0637 (0.0694)  time: 0.1633  data: 0.0002  max mem: 4132
[19:29:48.011687] Epoch: [6]  [300/781]  eta: 0:01:19  lr: 0.000250  training_loss: 1.7602 (1.7363)  classification_loss: 1.6942 (1.6669)  loss_mask: 0.0691 (0.0694)  time: 0.1620  data: 0.0002  max mem: 4132
[19:29:51.236474] Epoch: [6]  [320/781]  eta: 0:01:15  lr: 0.000250  training_loss: 1.7070 (1.7357)  classification_loss: 1.6429 (1.6667)  loss_mask: 0.0641 (0.0691)  time: 0.1612  data: 0.0002  max mem: 4132
[19:29:54.463149] Epoch: [6]  [340/781]  eta: 0:01:12  lr: 0.000250  training_loss: 1.6510 (1.7324)  classification_loss: 1.5870 (1.6635)  loss_mask: 0.0644 (0.0689)  time: 0.1613  data: 0.0002  max mem: 4132
[19:29:57.674258] Epoch: [6]  [360/781]  eta: 0:01:09  lr: 0.000250  training_loss: 1.7522 (1.7336)  classification_loss: 1.6688 (1.6644)  loss_mask: 0.0726 (0.0692)  time: 0.1605  data: 0.0003  max mem: 4132
[19:30:00.911894] Epoch: [6]  [380/781]  eta: 0:01:05  lr: 0.000250  training_loss: 1.7362 (1.7337)  classification_loss: 1.6705 (1.6646)  loss_mask: 0.0697 (0.0691)  time: 0.1618  data: 0.0002  max mem: 4132
[19:30:04.198498] Epoch: [6]  [400/781]  eta: 0:01:02  lr: 0.000250  training_loss: 1.7643 (1.7349)  classification_loss: 1.6886 (1.6658)  loss_mask: 0.0694 (0.0691)  time: 0.1642  data: 0.0003  max mem: 4132
[19:30:07.423749] Epoch: [6]  [420/781]  eta: 0:00:59  lr: 0.000250  training_loss: 1.7633 (1.7359)  classification_loss: 1.6733 (1.6666)  loss_mask: 0.0683 (0.0693)  time: 0.1612  data: 0.0002  max mem: 4132
[19:30:10.659597] Epoch: [6]  [440/781]  eta: 0:00:55  lr: 0.000250  training_loss: 1.7104 (1.7353)  classification_loss: 1.6355 (1.6661)  loss_mask: 0.0646 (0.0692)  time: 0.1617  data: 0.0002  max mem: 4132
[19:30:13.872997] Epoch: [6]  [460/781]  eta: 0:00:52  lr: 0.000250  training_loss: 1.6911 (1.7336)  classification_loss: 1.6238 (1.6645)  loss_mask: 0.0653 (0.0691)  time: 0.1606  data: 0.0002  max mem: 4132
[19:30:17.093721] Epoch: [6]  [480/781]  eta: 0:00:49  lr: 0.000250  training_loss: 1.7104 (1.7330)  classification_loss: 1.6496 (1.6640)  loss_mask: 0.0646 (0.0689)  time: 0.1609  data: 0.0002  max mem: 4132
[19:30:20.328987] Epoch: [6]  [500/781]  eta: 0:00:45  lr: 0.000250  training_loss: 1.7436 (1.7343)  classification_loss: 1.6797 (1.6652)  loss_mask: 0.0707 (0.0691)  time: 0.1617  data: 0.0003  max mem: 4132
[19:30:23.587872] Epoch: [6]  [520/781]  eta: 0:00:42  lr: 0.000250  training_loss: 1.7212 (1.7339)  classification_loss: 1.6569 (1.6651)  loss_mask: 0.0617 (0.0688)  time: 0.1629  data: 0.0002  max mem: 4132
[19:30:26.817768] Epoch: [6]  [540/781]  eta: 0:00:39  lr: 0.000250  training_loss: 1.7377 (1.7342)  classification_loss: 1.6796 (1.6656)  loss_mask: 0.0631 (0.0686)  time: 0.1614  data: 0.0002  max mem: 4132
[19:30:30.040922] Epoch: [6]  [560/781]  eta: 0:00:36  lr: 0.000250  training_loss: 1.7202 (1.7333)  classification_loss: 1.6570 (1.6649)  loss_mask: 0.0632 (0.0684)  time: 0.1610  data: 0.0002  max mem: 4132
[19:30:33.264277] Epoch: [6]  [580/781]  eta: 0:00:32  lr: 0.000250  training_loss: 1.7246 (1.7328)  classification_loss: 1.6617 (1.6644)  loss_mask: 0.0676 (0.0684)  time: 0.1611  data: 0.0003  max mem: 4132
[19:30:36.530120] Epoch: [6]  [600/781]  eta: 0:00:29  lr: 0.000250  training_loss: 1.7154 (1.7324)  classification_loss: 1.6425 (1.6641)  loss_mask: 0.0634 (0.0683)  time: 0.1632  data: 0.0003  max mem: 4132
[19:30:39.759142] Epoch: [6]  [620/781]  eta: 0:00:26  lr: 0.000250  training_loss: 1.6662 (1.7307)  classification_loss: 1.6096 (1.6626)  loss_mask: 0.0594 (0.0681)  time: 0.1614  data: 0.0003  max mem: 4132
[19:30:42.991923] Epoch: [6]  [640/781]  eta: 0:00:22  lr: 0.000250  training_loss: 1.7683 (1.7316)  classification_loss: 1.6989 (1.6636)  loss_mask: 0.0672 (0.0680)  time: 0.1615  data: 0.0003  max mem: 4132
[19:30:46.209684] Epoch: [6]  [660/781]  eta: 0:00:19  lr: 0.000250  training_loss: 1.7318 (1.7322)  classification_loss: 1.6711 (1.6643)  loss_mask: 0.0598 (0.0678)  time: 0.1608  data: 0.0002  max mem: 4132
[19:30:49.459604] Epoch: [6]  [680/781]  eta: 0:00:16  lr: 0.000250  training_loss: 1.7219 (1.7320)  classification_loss: 1.6388 (1.6638)  loss_mask: 0.0728 (0.0681)  time: 0.1624  data: 0.0002  max mem: 4132
[19:30:52.697659] Epoch: [6]  [700/781]  eta: 0:00:13  lr: 0.000250  training_loss: 1.7096 (1.7316)  classification_loss: 1.6444 (1.6635)  loss_mask: 0.0664 (0.0681)  time: 0.1618  data: 0.0004  max mem: 4132
[19:30:55.938587] Epoch: [6]  [720/781]  eta: 0:00:09  lr: 0.000250  training_loss: 1.7346 (1.7317)  classification_loss: 1.6555 (1.6634)  loss_mask: 0.0708 (0.0683)  time: 0.1620  data: 0.0002  max mem: 4132
[19:30:59.166860] Epoch: [6]  [740/781]  eta: 0:00:06  lr: 0.000250  training_loss: 1.6993 (1.7307)  classification_loss: 1.6357 (1.6624)  loss_mask: 0.0683 (0.0683)  time: 0.1613  data: 0.0002  max mem: 4132
[19:31:02.429186] Epoch: [6]  [760/781]  eta: 0:00:03  lr: 0.000250  training_loss: 1.7147 (1.7306)  classification_loss: 1.6563 (1.6624)  loss_mask: 0.0621 (0.0682)  time: 0.1630  data: 0.0002  max mem: 4132
[19:31:05.638568] Epoch: [6]  [780/781]  eta: 0:00:00  lr: 0.000250  training_loss: 1.7248 (1.7306)  classification_loss: 1.6514 (1.6626)  loss_mask: 0.0669 (0.0681)  time: 0.1604  data: 0.0002  max mem: 4132
[19:31:05.813223] Epoch: [6] Total time: 0:02:07 (0.1631 s / it)
[19:31:05.813763] Averaged stats: lr: 0.000250  training_loss: 1.7248 (1.7306)  classification_loss: 1.6514 (1.6626)  loss_mask: 0.0669 (0.0681)
[19:31:06.398132] Test:  [  0/157]  eta: 0:01:30  testing_loss: 1.1518 (1.1518)  acc1: 60.9375 (60.9375)  acc5: 92.1875 (92.1875)  time: 0.5794  data: 0.5479  max mem: 4132
[19:31:06.692504] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 1.2895 (1.2996)  acc1: 50.0000 (53.1250)  acc5: 93.7500 (94.0341)  time: 0.0792  data: 0.0503  max mem: 4132
[19:31:06.976463] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.2688 (1.2665)  acc1: 56.2500 (55.0595)  acc5: 93.7500 (94.4196)  time: 0.0288  data: 0.0004  max mem: 4132
[19:31:07.258497] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 1.2123 (1.2633)  acc1: 56.2500 (55.5444)  acc5: 93.7500 (94.1532)  time: 0.0282  data: 0.0002  max mem: 4132
[19:31:07.540640] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 1.2687 (1.2696)  acc1: 56.2500 (55.6402)  acc5: 93.7500 (93.9024)  time: 0.0281  data: 0.0002  max mem: 4132
[19:31:07.824518] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.2687 (1.2649)  acc1: 56.2500 (55.9743)  acc5: 93.7500 (94.1176)  time: 0.0281  data: 0.0002  max mem: 4132
[19:31:08.105840] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.2198 (1.2574)  acc1: 57.8125 (56.4037)  acc5: 95.3125 (94.3391)  time: 0.0281  data: 0.0002  max mem: 4132
[19:31:08.387851] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.2085 (1.2528)  acc1: 59.3750 (56.7562)  acc5: 95.3125 (94.3882)  time: 0.0280  data: 0.0002  max mem: 4132
[19:31:08.669356] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.2220 (1.2535)  acc1: 59.3750 (56.8287)  acc5: 95.3125 (94.3866)  time: 0.0280  data: 0.0002  max mem: 4132
[19:31:08.951397] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.2357 (1.2540)  acc1: 57.8125 (56.8681)  acc5: 95.3125 (94.4883)  time: 0.0280  data: 0.0002  max mem: 4132
[19:31:09.233972] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.3102 (1.2614)  acc1: 53.1250 (56.4821)  acc5: 93.7500 (94.4307)  time: 0.0281  data: 0.0002  max mem: 4132
[19:31:09.516604] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.3253 (1.2641)  acc1: 50.0000 (56.2218)  acc5: 93.7500 (94.4398)  time: 0.0281  data: 0.0002  max mem: 4132
[19:31:09.800188] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.2839 (1.2613)  acc1: 56.2500 (56.3920)  acc5: 93.7500 (94.3698)  time: 0.0281  data: 0.0002  max mem: 4132
[19:31:10.082710] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.2776 (1.2648)  acc1: 57.8125 (56.2619)  acc5: 92.1875 (94.2867)  time: 0.0281  data: 0.0002  max mem: 4132
[19:31:10.362382] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.2656 (1.2654)  acc1: 57.8125 (56.4273)  acc5: 92.1875 (94.2043)  time: 0.0280  data: 0.0002  max mem: 4132
[19:31:10.640903] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.2379 (1.2615)  acc1: 60.9375 (56.7363)  acc5: 92.1875 (94.1536)  time: 0.0278  data: 0.0001  max mem: 4132
[19:31:10.790514] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.2501 (1.2627)  acc1: 59.3750 (56.6100)  acc5: 92.1875 (94.2100)  time: 0.0268  data: 0.0001  max mem: 4132
[19:31:10.928947] Test: Total time: 0:00:05 (0.0326 s / it)
[19:31:10.929397] * Acc@1 56.610 Acc@5 94.210 loss 1.263
[19:31:10.929691] Accuracy of the network on the 10000 test images: 56.6%
[19:31:10.929877] Max accuracy: 56.61%
[19:31:11.138223] log_dir: ./output_dir
[19:31:11.990981] Epoch: [7]  [  0/781]  eta: 0:11:04  lr: 0.000250  training_loss: 1.6257 (1.6257)  classification_loss: 1.5682 (1.5682)  loss_mask: 0.0575 (0.0575)  time: 0.8510  data: 0.6451  max mem: 4132
[19:31:15.207404] Epoch: [7]  [ 20/781]  eta: 0:02:27  lr: 0.000250  training_loss: 1.6682 (1.6903)  classification_loss: 1.6036 (1.6264)  loss_mask: 0.0636 (0.0639)  time: 0.1607  data: 0.0002  max mem: 4132
[19:31:18.428815] Epoch: [7]  [ 40/781]  eta: 0:02:11  lr: 0.000250  training_loss: 1.7482 (1.7180)  classification_loss: 1.6735 (1.6523)  loss_mask: 0.0691 (0.0658)  time: 0.1610  data: 0.0003  max mem: 4132
[19:31:21.653412] Epoch: [7]  [ 60/781]  eta: 0:02:04  lr: 0.000250  training_loss: 1.7402 (1.7214)  classification_loss: 1.6783 (1.6557)  loss_mask: 0.0632 (0.0657)  time: 0.1611  data: 0.0002  max mem: 4132
[19:31:24.910711] Epoch: [7]  [ 80/781]  eta: 0:01:59  lr: 0.000250  training_loss: 1.7105 (1.7214)  classification_loss: 1.6548 (1.6570)  loss_mask: 0.0613 (0.0644)  time: 0.1628  data: 0.0003  max mem: 4132
[19:31:28.158068] Epoch: [7]  [100/781]  eta: 0:01:54  lr: 0.000250  training_loss: 1.6845 (1.7170)  classification_loss: 1.6346 (1.6535)  loss_mask: 0.0602 (0.0634)  time: 0.1622  data: 0.0002  max mem: 4132
[19:31:31.451510] Epoch: [7]  [120/781]  eta: 0:01:50  lr: 0.000250  training_loss: 1.6614 (1.7095)  classification_loss: 1.6010 (1.6462)  loss_mask: 0.0620 (0.0633)  time: 0.1646  data: 0.0003  max mem: 4132
[19:31:34.667846] Epoch: [7]  [140/781]  eta: 0:01:46  lr: 0.000250  training_loss: 1.6505 (1.7038)  classification_loss: 1.5952 (1.6410)  loss_mask: 0.0590 (0.0628)  time: 0.1607  data: 0.0002  max mem: 4132
[19:31:37.882380] Epoch: [7]  [160/781]  eta: 0:01:43  lr: 0.000250  training_loss: 1.7199 (1.7060)  classification_loss: 1.6645 (1.6436)  loss_mask: 0.0590 (0.0624)  time: 0.1607  data: 0.0002  max mem: 4132
[19:31:41.115822] Epoch: [7]  [180/781]  eta: 0:01:39  lr: 0.000250  training_loss: 1.6857 (1.7041)  classification_loss: 1.6176 (1.6415)  loss_mask: 0.0615 (0.0626)  time: 0.1616  data: 0.0002  max mem: 4132
[19:31:44.372089] Epoch: [7]  [200/781]  eta: 0:01:36  lr: 0.000250  training_loss: 1.7422 (1.7081)  classification_loss: 1.6618 (1.6448)  loss_mask: 0.0690 (0.0633)  time: 0.1627  data: 0.0002  max mem: 4132
[19:31:47.595509] Epoch: [7]  [220/781]  eta: 0:01:32  lr: 0.000250  training_loss: 1.6928 (1.7085)  classification_loss: 1.6249 (1.6445)  loss_mask: 0.0684 (0.0640)  time: 0.1611  data: 0.0003  max mem: 4132
[19:31:50.832674] Epoch: [7]  [240/781]  eta: 0:01:29  lr: 0.000250  training_loss: 1.7090 (1.7105)  classification_loss: 1.6465 (1.6462)  loss_mask: 0.0677 (0.0642)  time: 0.1618  data: 0.0002  max mem: 4132
[19:31:54.061558] Epoch: [7]  [260/781]  eta: 0:01:25  lr: 0.000250  training_loss: 1.7048 (1.7096)  classification_loss: 1.6420 (1.6457)  loss_mask: 0.0602 (0.0639)  time: 0.1613  data: 0.0002  max mem: 4132
[19:31:57.293354] Epoch: [7]  [280/781]  eta: 0:01:22  lr: 0.000250  training_loss: 1.6875 (1.7084)  classification_loss: 1.6372 (1.6450)  loss_mask: 0.0549 (0.0635)  time: 0.1615  data: 0.0004  max mem: 4132
[19:32:00.529599] Epoch: [7]  [300/781]  eta: 0:01:18  lr: 0.000250  training_loss: 1.7365 (1.7110)  classification_loss: 1.6774 (1.6473)  loss_mask: 0.0680 (0.0637)  time: 0.1617  data: 0.0002  max mem: 4132
[19:32:03.756361] Epoch: [7]  [320/781]  eta: 0:01:15  lr: 0.000250  training_loss: 1.6868 (1.7089)  classification_loss: 1.6256 (1.6451)  loss_mask: 0.0589 (0.0637)  time: 0.1613  data: 0.0002  max mem: 4132
[19:32:06.985461] Epoch: [7]  [340/781]  eta: 0:01:12  lr: 0.000250  training_loss: 1.6641 (1.7074)  classification_loss: 1.5983 (1.6439)  loss_mask: 0.0591 (0.0634)  time: 0.1614  data: 0.0002  max mem: 4132
[19:32:10.212091] Epoch: [7]  [360/781]  eta: 0:01:08  lr: 0.000250  training_loss: 1.6943 (1.7057)  classification_loss: 1.6320 (1.6425)  loss_mask: 0.0580 (0.0632)  time: 0.1613  data: 0.0002  max mem: 4132
[19:32:13.468205] Epoch: [7]  [380/781]  eta: 0:01:05  lr: 0.000250  training_loss: 1.6982 (1.7056)  classification_loss: 1.6247 (1.6425)  loss_mask: 0.0591 (0.0631)  time: 0.1627  data: 0.0003  max mem: 4132
[19:32:16.700711] Epoch: [7]  [400/781]  eta: 0:01:02  lr: 0.000250  training_loss: 1.6822 (1.7043)  classification_loss: 1.6207 (1.6411)  loss_mask: 0.0661 (0.0633)  time: 0.1615  data: 0.0002  max mem: 4132
[19:32:19.927827] Epoch: [7]  [420/781]  eta: 0:00:58  lr: 0.000250  training_loss: 1.7124 (1.7037)  classification_loss: 1.6414 (1.6406)  loss_mask: 0.0596 (0.0631)  time: 0.1613  data: 0.0002  max mem: 4132
[19:32:23.190068] Epoch: [7]  [440/781]  eta: 0:00:55  lr: 0.000250  training_loss: 1.6865 (1.7034)  classification_loss: 1.6340 (1.6406)  loss_mask: 0.0545 (0.0628)  time: 0.1630  data: 0.0003  max mem: 4132
[19:32:26.439488] Epoch: [7]  [460/781]  eta: 0:00:52  lr: 0.000250  training_loss: 1.6663 (1.7028)  classification_loss: 1.6064 (1.6399)  loss_mask: 0.0657 (0.0629)  time: 0.1624  data: 0.0002  max mem: 4132
[19:32:29.684319] Epoch: [7]  [480/781]  eta: 0:00:49  lr: 0.000250  training_loss: 1.7391 (1.7033)  classification_loss: 1.6709 (1.6403)  loss_mask: 0.0616 (0.0631)  time: 0.1622  data: 0.0002  max mem: 4132
[19:32:32.945525] Epoch: [7]  [500/781]  eta: 0:00:45  lr: 0.000250  training_loss: 1.6722 (1.7027)  classification_loss: 1.6227 (1.6397)  loss_mask: 0.0604 (0.0630)  time: 0.1629  data: 0.0002  max mem: 4132
[19:32:36.201350] Epoch: [7]  [520/781]  eta: 0:00:42  lr: 0.000250  training_loss: 1.7060 (1.7023)  classification_loss: 1.6406 (1.6394)  loss_mask: 0.0592 (0.0629)  time: 0.1627  data: 0.0002  max mem: 4132
[19:32:39.455230] Epoch: [7]  [540/781]  eta: 0:00:39  lr: 0.000250  training_loss: 1.6866 (1.7020)  classification_loss: 1.6238 (1.6393)  loss_mask: 0.0542 (0.0627)  time: 0.1626  data: 0.0003  max mem: 4132
[19:32:42.687130] Epoch: [7]  [560/781]  eta: 0:00:36  lr: 0.000249  training_loss: 1.6796 (1.7018)  classification_loss: 1.6215 (1.6393)  loss_mask: 0.0549 (0.0625)  time: 0.1615  data: 0.0002  max mem: 4132
[19:32:46.002792] Epoch: [7]  [580/781]  eta: 0:00:32  lr: 0.000249  training_loss: 1.6833 (1.7015)  classification_loss: 1.6216 (1.6392)  loss_mask: 0.0551 (0.0623)  time: 0.1657  data: 0.0002  max mem: 4132
[19:32:49.237140] Epoch: [7]  [600/781]  eta: 0:00:29  lr: 0.000249  training_loss: 1.6586 (1.7008)  classification_loss: 1.5933 (1.6384)  loss_mask: 0.0605 (0.0624)  time: 0.1616  data: 0.0003  max mem: 4132
[19:32:52.467120] Epoch: [7]  [620/781]  eta: 0:00:26  lr: 0.000249  training_loss: 1.6515 (1.6999)  classification_loss: 1.5886 (1.6374)  loss_mask: 0.0646 (0.0625)  time: 0.1614  data: 0.0002  max mem: 4132
[19:32:55.726206] Epoch: [7]  [640/781]  eta: 0:00:22  lr: 0.000249  training_loss: 1.6759 (1.6993)  classification_loss: 1.6148 (1.6369)  loss_mask: 0.0593 (0.0625)  time: 0.1628  data: 0.0002  max mem: 4132
[19:32:58.955199] Epoch: [7]  [660/781]  eta: 0:00:19  lr: 0.000249  training_loss: 1.7173 (1.7000)  classification_loss: 1.6438 (1.6376)  loss_mask: 0.0593 (0.0624)  time: 0.1614  data: 0.0002  max mem: 4132
[19:33:02.184315] Epoch: [7]  [680/781]  eta: 0:00:16  lr: 0.000249  training_loss: 1.6992 (1.6999)  classification_loss: 1.6361 (1.6377)  loss_mask: 0.0531 (0.0622)  time: 0.1613  data: 0.0002  max mem: 4132
[19:33:05.404283] Epoch: [7]  [700/781]  eta: 0:00:13  lr: 0.000249  training_loss: 1.6835 (1.6996)  classification_loss: 1.6352 (1.6376)  loss_mask: 0.0578 (0.0620)  time: 0.1609  data: 0.0002  max mem: 4132
[19:33:08.644080] Epoch: [7]  [720/781]  eta: 0:00:09  lr: 0.000249  training_loss: 1.7108 (1.6999)  classification_loss: 1.6505 (1.6381)  loss_mask: 0.0522 (0.0619)  time: 0.1619  data: 0.0002  max mem: 4132
[19:33:11.868396] Epoch: [7]  [740/781]  eta: 0:00:06  lr: 0.000249  training_loss: 1.6843 (1.6991)  classification_loss: 1.6344 (1.6374)  loss_mask: 0.0524 (0.0617)  time: 0.1611  data: 0.0003  max mem: 4132
[19:33:15.120275] Epoch: [7]  [760/781]  eta: 0:00:03  lr: 0.000249  training_loss: 1.6944 (1.6989)  classification_loss: 1.6520 (1.6374)  loss_mask: 0.0541 (0.0615)  time: 0.1625  data: 0.0002  max mem: 4132
[19:33:18.480106] Epoch: [7]  [780/781]  eta: 0:00:00  lr: 0.000249  training_loss: 1.6841 (1.6991)  classification_loss: 1.6227 (1.6375)  loss_mask: 0.0637 (0.0617)  time: 0.1679  data: 0.0003  max mem: 4132
[19:33:18.663597] Epoch: [7] Total time: 0:02:07 (0.1633 s / it)
[19:33:18.664078] Averaged stats: lr: 0.000249  training_loss: 1.6841 (1.6991)  classification_loss: 1.6227 (1.6375)  loss_mask: 0.0637 (0.0617)
[19:33:19.220024] Test:  [  0/157]  eta: 0:01:26  testing_loss: 1.0850 (1.0850)  acc1: 62.5000 (62.5000)  acc5: 92.1875 (92.1875)  time: 0.5522  data: 0.5174  max mem: 4132
[19:33:19.515843] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 1.2460 (1.2486)  acc1: 57.8125 (55.8239)  acc5: 96.8750 (95.8807)  time: 0.0769  data: 0.0480  max mem: 4132
[19:33:19.798732] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.2091 (1.2116)  acc1: 57.8125 (58.0357)  acc5: 96.8750 (95.9821)  time: 0.0288  data: 0.0006  max mem: 4132
[19:33:20.092074] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 1.1797 (1.2163)  acc1: 59.3750 (58.6190)  acc5: 95.3125 (95.6653)  time: 0.0287  data: 0.0003  max mem: 4132
[19:33:20.377247] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 1.1887 (1.2124)  acc1: 59.3750 (58.4223)  acc5: 95.3125 (95.5412)  time: 0.0288  data: 0.0004  max mem: 4132
[19:33:20.661809] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.1887 (1.2126)  acc1: 57.8125 (58.6703)  acc5: 93.7500 (95.3125)  time: 0.0283  data: 0.0002  max mem: 4132
[19:33:20.944543] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.1965 (1.2042)  acc1: 59.3750 (58.8371)  acc5: 95.3125 (95.3125)  time: 0.0282  data: 0.0002  max mem: 4132
[19:33:21.229468] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.1494 (1.1952)  acc1: 60.9375 (59.4190)  acc5: 96.8750 (95.5326)  time: 0.0283  data: 0.0002  max mem: 4132
[19:33:21.512120] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.1689 (1.1968)  acc1: 60.9375 (59.5100)  acc5: 95.3125 (95.5054)  time: 0.0283  data: 0.0002  max mem: 4132
[19:33:21.792466] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.2069 (1.1993)  acc1: 59.3750 (59.5295)  acc5: 96.8750 (95.6731)  time: 0.0280  data: 0.0002  max mem: 4132
[19:33:22.073326] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.2384 (1.2053)  acc1: 54.6875 (59.1584)  acc5: 96.8750 (95.6219)  time: 0.0279  data: 0.0002  max mem: 4132
[19:33:22.354972] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.2446 (1.2076)  acc1: 53.1250 (58.8964)  acc5: 96.8750 (95.7207)  time: 0.0280  data: 0.0002  max mem: 4132
[19:33:22.636143] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.2024 (1.2042)  acc1: 59.3750 (59.1426)  acc5: 96.8750 (95.7515)  time: 0.0280  data: 0.0002  max mem: 4132
[19:33:22.917788] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.1982 (1.2069)  acc1: 60.9375 (58.9933)  acc5: 95.3125 (95.6465)  time: 0.0280  data: 0.0002  max mem: 4132
[19:33:23.197761] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.2047 (1.2091)  acc1: 56.2500 (58.9207)  acc5: 93.7500 (95.4787)  time: 0.0280  data: 0.0002  max mem: 4132
[19:33:23.475492] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.1712 (1.2043)  acc1: 59.3750 (59.1370)  acc5: 95.3125 (95.4988)  time: 0.0278  data: 0.0001  max mem: 4132
[19:33:23.625664] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.1712 (1.2051)  acc1: 59.3750 (58.9900)  acc5: 95.3125 (95.5400)  time: 0.0268  data: 0.0001  max mem: 4132
[19:33:23.771437] Test: Total time: 0:00:05 (0.0325 s / it)
[19:33:23.771932] * Acc@1 58.990 Acc@5 95.540 loss 1.205
[19:33:23.772273] Accuracy of the network on the 10000 test images: 59.0%
[19:33:23.772689] Max accuracy: 58.99%
[19:33:24.114479] log_dir: ./output_dir
[19:33:24.872263] Epoch: [8]  [  0/781]  eta: 0:09:50  lr: 0.000249  training_loss: 1.5164 (1.5164)  classification_loss: 1.4299 (1.4299)  loss_mask: 0.0865 (0.0865)  time: 0.7559  data: 0.5765  max mem: 4132
[19:33:28.159152] Epoch: [8]  [ 20/781]  eta: 0:02:26  lr: 0.000249  training_loss: 1.6557 (1.6955)  classification_loss: 1.5973 (1.6356)  loss_mask: 0.0584 (0.0598)  time: 0.1642  data: 0.0002  max mem: 4132
[19:33:31.383343] Epoch: [8]  [ 40/781]  eta: 0:02:11  lr: 0.000249  training_loss: 1.6907 (1.6945)  classification_loss: 1.6362 (1.6365)  loss_mask: 0.0535 (0.0581)  time: 0.1611  data: 0.0002  max mem: 4132
[19:33:34.617286] Epoch: [8]  [ 60/781]  eta: 0:02:04  lr: 0.000249  training_loss: 1.6581 (1.6841)  classification_loss: 1.5985 (1.6275)  loss_mask: 0.0525 (0.0567)  time: 0.1616  data: 0.0002  max mem: 4132
[19:33:37.851310] Epoch: [8]  [ 80/781]  eta: 0:01:58  lr: 0.000249  training_loss: 1.7105 (1.6939)  classification_loss: 1.6440 (1.6361)  loss_mask: 0.0578 (0.0578)  time: 0.1616  data: 0.0003  max mem: 4132
[19:33:41.090455] Epoch: [8]  [100/781]  eta: 0:01:54  lr: 0.000249  training_loss: 1.7026 (1.6980)  classification_loss: 1.6422 (1.6405)  loss_mask: 0.0566 (0.0575)  time: 0.1619  data: 0.0002  max mem: 4132
[19:33:44.339168] Epoch: [8]  [120/781]  eta: 0:01:50  lr: 0.000249  training_loss: 1.6630 (1.6926)  classification_loss: 1.6067 (1.6353)  loss_mask: 0.0516 (0.0572)  time: 0.1624  data: 0.0002  max mem: 4132
[19:33:47.572435] Epoch: [8]  [140/781]  eta: 0:01:46  lr: 0.000249  training_loss: 1.6431 (1.6843)  classification_loss: 1.5880 (1.6278)  loss_mask: 0.0524 (0.0565)  time: 0.1616  data: 0.0004  max mem: 4132
[19:33:50.838955] Epoch: [8]  [160/781]  eta: 0:01:43  lr: 0.000249  training_loss: 1.6808 (1.6830)  classification_loss: 1.6290 (1.6269)  loss_mask: 0.0530 (0.0561)  time: 0.1632  data: 0.0002  max mem: 4132
[19:33:54.081206] Epoch: [8]  [180/781]  eta: 0:01:39  lr: 0.000249  training_loss: 1.6278 (1.6799)  classification_loss: 1.5762 (1.6242)  loss_mask: 0.0515 (0.0558)  time: 0.1620  data: 0.0003  max mem: 4132
[19:33:57.310027] Epoch: [8]  [200/781]  eta: 0:01:35  lr: 0.000249  training_loss: 1.6717 (1.6796)  classification_loss: 1.6190 (1.6241)  loss_mask: 0.0505 (0.0555)  time: 0.1613  data: 0.0002  max mem: 4132
[19:34:00.539319] Epoch: [8]  [220/781]  eta: 0:01:32  lr: 0.000249  training_loss: 1.6817 (1.6795)  classification_loss: 1.6146 (1.6231)  loss_mask: 0.0645 (0.0564)  time: 0.1614  data: 0.0002  max mem: 4132
[19:34:03.776442] Epoch: [8]  [240/781]  eta: 0:01:28  lr: 0.000249  training_loss: 1.6712 (1.6802)  classification_loss: 1.6054 (1.6234)  loss_mask: 0.0540 (0.0568)  time: 0.1618  data: 0.0002  max mem: 4132
[19:34:07.008680] Epoch: [8]  [260/781]  eta: 0:01:25  lr: 0.000249  training_loss: 1.6571 (1.6800)  classification_loss: 1.6027 (1.6235)  loss_mask: 0.0499 (0.0565)  time: 0.1615  data: 0.0003  max mem: 4132
[19:34:10.230021] Epoch: [8]  [280/781]  eta: 0:01:22  lr: 0.000249  training_loss: 1.6628 (1.6805)  classification_loss: 1.6090 (1.6238)  loss_mask: 0.0560 (0.0567)  time: 0.1610  data: 0.0004  max mem: 4132
[19:34:13.467089] Epoch: [8]  [300/781]  eta: 0:01:18  lr: 0.000249  training_loss: 1.7116 (1.6820)  classification_loss: 1.6550 (1.6253)  loss_mask: 0.0560 (0.0568)  time: 0.1618  data: 0.0003  max mem: 4132
[19:34:16.699358] Epoch: [8]  [320/781]  eta: 0:01:15  lr: 0.000249  training_loss: 1.6436 (1.6799)  classification_loss: 1.5993 (1.6235)  loss_mask: 0.0478 (0.0563)  time: 0.1615  data: 0.0003  max mem: 4132
[19:34:19.957935] Epoch: [8]  [340/781]  eta: 0:01:12  lr: 0.000249  training_loss: 1.6654 (1.6788)  classification_loss: 1.6216 (1.6231)  loss_mask: 0.0457 (0.0557)  time: 0.1628  data: 0.0002  max mem: 4132
[19:34:23.199081] Epoch: [8]  [360/781]  eta: 0:01:08  lr: 0.000249  training_loss: 1.6716 (1.6794)  classification_loss: 1.6106 (1.6238)  loss_mask: 0.0484 (0.0556)  time: 0.1620  data: 0.0003  max mem: 4132
[19:34:26.437327] Epoch: [8]  [380/781]  eta: 0:01:05  lr: 0.000249  training_loss: 1.6691 (1.6785)  classification_loss: 1.6129 (1.6232)  loss_mask: 0.0499 (0.0553)  time: 0.1618  data: 0.0002  max mem: 4132
[19:34:29.670882] Epoch: [8]  [400/781]  eta: 0:01:02  lr: 0.000249  training_loss: 1.6976 (1.6793)  classification_loss: 1.6425 (1.6242)  loss_mask: 0.0483 (0.0551)  time: 0.1616  data: 0.0002  max mem: 4132
[19:34:32.922666] Epoch: [8]  [420/781]  eta: 0:00:58  lr: 0.000249  training_loss: 1.6201 (1.6774)  classification_loss: 1.5654 (1.6225)  loss_mask: 0.0506 (0.0549)  time: 0.1625  data: 0.0002  max mem: 4132
[19:34:36.163703] Epoch: [8]  [440/781]  eta: 0:00:55  lr: 0.000249  training_loss: 1.6285 (1.6756)  classification_loss: 1.5671 (1.6207)  loss_mask: 0.0534 (0.0549)  time: 0.1620  data: 0.0003  max mem: 4132

[19:34:39.393200] Epoch: [8]  [460/781]  eta: 0:00:52  lr: 0.000249  training_loss: 1.6723 (1.6761)  classification_loss: 1.6198 (1.6209)  loss_mask: 0.0591 (0.0551)  time: 0.1614  data: 0.0003  max mem: 4132
[19:34:42.623138] Epoch: [8]  [480/781]  eta: 0:00:49  lr: 0.000249  training_loss: 1.6302 (1.6746)  classification_loss: 1.5880 (1.6196)  loss_mask: 0.0469 (0.0550)  time: 0.1614  data: 0.0002  max mem: 4132
[19:34:45.889883] Epoch: [8]  [500/781]  eta: 0:00:45  lr: 0.000249  training_loss: 1.6875 (1.6749)  classification_loss: 1.6207 (1.6197)  loss_mask: 0.0581 (0.0552)  time: 0.1632  data: 0.0002  max mem: 4132
[19:34:49.120939] Epoch: [8]  [520/781]  eta: 0:00:42  lr: 0.000249  training_loss: 1.6486 (1.6738)  classification_loss: 1.5958 (1.6188)  loss_mask: 0.0506 (0.0550)  time: 0.1614  data: 0.0003  max mem: 4132
[19:34:52.341122] Epoch: [8]  [540/781]  eta: 0:00:39  lr: 0.000249  training_loss: 1.6813 (1.6731)  classification_loss: 1.6217 (1.6182)  loss_mask: 0.0520 (0.0549)  time: 0.1609  data: 0.0002  max mem: 4132
[19:34:55.597497] Epoch: [8]  [560/781]  eta: 0:00:36  lr: 0.000249  training_loss: 1.6416 (1.6720)  classification_loss: 1.5919 (1.6172)  loss_mask: 0.0502 (0.0548)  time: 0.1627  data: 0.0002  max mem: 4132
[19:34:58.837027] Epoch: [8]  [580/781]  eta: 0:00:32  lr: 0.000249  training_loss: 1.7048 (1.6725)  classification_loss: 1.6505 (1.6179)  loss_mask: 0.0457 (0.0546)  time: 0.1619  data: 0.0003  max mem: 4132
[19:35:02.059498] Epoch: [8]  [600/781]  eta: 0:00:29  lr: 0.000249  training_loss: 1.6825 (1.6723)  classification_loss: 1.6305 (1.6178)  loss_mask: 0.0515 (0.0545)  time: 0.1610  data: 0.0002  max mem: 4132
[19:35:05.284629] Epoch: [8]  [620/781]  eta: 0:00:26  lr: 0.000249  training_loss: 1.6435 (1.6713)  classification_loss: 1.5908 (1.6168)  loss_mask: 0.0523 (0.0545)  time: 0.1612  data: 0.0002  max mem: 4132
[19:35:08.611705] Epoch: [8]  [640/781]  eta: 0:00:22  lr: 0.000249  training_loss: 1.6543 (1.6707)  classification_loss: 1.6081 (1.6164)  loss_mask: 0.0451 (0.0542)  time: 0.1663  data: 0.0002  max mem: 4132
[19:35:11.879727] Epoch: [8]  [660/781]  eta: 0:00:19  lr: 0.000249  training_loss: 1.6217 (1.6700)  classification_loss: 1.5757 (1.6160)  loss_mask: 0.0463 (0.0540)  time: 0.1633  data: 0.0002  max mem: 4132
[19:35:15.112607] Epoch: [8]  [680/781]  eta: 0:00:16  lr: 0.000249  training_loss: 1.6337 (1.6690)  classification_loss: 1.5887 (1.6152)  loss_mask: 0.0449 (0.0538)  time: 0.1616  data: 0.0002  max mem: 4132
[19:35:18.327972] Epoch: [8]  [700/781]  eta: 0:00:13  lr: 0.000249  training_loss: 1.6492 (1.6683)  classification_loss: 1.6106 (1.6147)  loss_mask: 0.0424 (0.0536)  time: 0.1607  data: 0.0002  max mem: 4132
[19:35:21.551199] Epoch: [8]  [720/781]  eta: 0:00:09  lr: 0.000249  training_loss: 1.6727 (1.6681)  classification_loss: 1.6104 (1.6148)  loss_mask: 0.0427 (0.0533)  time: 0.1611  data: 0.0002  max mem: 4132
[19:35:24.797118] Epoch: [8]  [740/781]  eta: 0:00:06  lr: 0.000249  training_loss: 1.6350 (1.6676)  classification_loss: 1.5893 (1.6142)  loss_mask: 0.0550 (0.0534)  time: 0.1622  data: 0.0004  max mem: 4132
[19:35:28.136010] Epoch: [8]  [760/781]  eta: 0:00:03  lr: 0.000249  training_loss: 1.6732 (1.6677)  classification_loss: 1.6189 (1.6144)  loss_mask: 0.0446 (0.0533)  time: 0.1669  data: 0.0002  max mem: 4132
[19:35:31.350966] Epoch: [8]  [780/781]  eta: 0:00:00  lr: 0.000249  training_loss: 1.6717 (1.6676)  classification_loss: 1.6184 (1.6143)  loss_mask: 0.0502 (0.0533)  time: 0.1606  data: 0.0002  max mem: 4132
[19:35:31.512204] Epoch: [8] Total time: 0:02:07 (0.1631 s / it)
[19:35:31.512679] Averaged stats: lr: 0.000249  training_loss: 1.6717 (1.6676)  classification_loss: 1.6184 (1.6143)  loss_mask: 0.0502 (0.0533)
[19:35:32.235937] Test:  [  0/157]  eta: 0:01:52  testing_loss: 1.1226 (1.1226)  acc1: 57.8125 (57.8125)  acc5: 90.6250 (90.6250)  time: 0.7191  data: 0.6878  max mem: 4132
[19:35:32.527785] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.1799 (1.2167)  acc1: 56.2500 (55.8239)  acc5: 96.8750 (95.7386)  time: 0.0917  data: 0.0629  max mem: 4132
[19:35:32.810601] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.1773 (1.1778)  acc1: 57.8125 (58.0357)  acc5: 96.8750 (95.9821)  time: 0.0285  data: 0.0003  max mem: 4132
[19:35:33.096188] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.1676 (1.1778)  acc1: 59.3750 (58.2157)  acc5: 95.3125 (95.6653)  time: 0.0283  data: 0.0002  max mem: 4132
[19:35:33.380832] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.1738 (1.1757)  acc1: 57.8125 (58.7652)  acc5: 95.3125 (95.6555)  time: 0.0284  data: 0.0001  max mem: 4132
[19:35:33.682007] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.1517 (1.1718)  acc1: 62.5000 (59.2218)  acc5: 95.3125 (95.6495)  time: 0.0292  data: 0.0002  max mem: 4132
[19:35:33.968104] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.1517 (1.1636)  acc1: 62.5000 (59.6568)  acc5: 95.3125 (95.5686)  time: 0.0292  data: 0.0002  max mem: 4132
[19:35:34.252211] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.1165 (1.1553)  acc1: 62.5000 (60.2993)  acc5: 96.8750 (95.6646)  time: 0.0283  data: 0.0002  max mem: 4132
[19:35:34.545272] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.1165 (1.1587)  acc1: 62.5000 (60.3588)  acc5: 95.3125 (95.6211)  time: 0.0287  data: 0.0002  max mem: 4132
[19:35:34.832488] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.1642 (1.1611)  acc1: 60.9375 (60.3365)  acc5: 95.3125 (95.5701)  time: 0.0288  data: 0.0002  max mem: 4132
[19:35:35.115885] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.1767 (1.1676)  acc1: 57.8125 (59.8855)  acc5: 95.3125 (95.6683)  time: 0.0283  data: 0.0002  max mem: 4132
[19:35:35.401327] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.2142 (1.1690)  acc1: 57.8125 (59.8958)  acc5: 95.3125 (95.6503)  time: 0.0283  data: 0.0002  max mem: 4132
[19:35:35.683220] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.1410 (1.1644)  acc1: 59.3750 (59.9303)  acc5: 95.3125 (95.7386)  time: 0.0282  data: 0.0002  max mem: 4132
[19:35:35.967766] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.1617 (1.1676)  acc1: 59.3750 (59.7805)  acc5: 95.3125 (95.6465)  time: 0.0282  data: 0.0002  max mem: 4132
[19:35:36.254488] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.1703 (1.1690)  acc1: 57.8125 (59.8072)  acc5: 93.7500 (95.5895)  time: 0.0284  data: 0.0002  max mem: 4132
[19:35:36.533634] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.1522 (1.1654)  acc1: 62.5000 (60.0269)  acc5: 95.3125 (95.5712)  time: 0.0282  data: 0.0001  max mem: 4132
[19:35:36.683414] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.1375 (1.1679)  acc1: 59.3750 (59.8500)  acc5: 95.3125 (95.5800)  time: 0.0269  data: 0.0001  max mem: 4132
[19:35:36.841915] Test: Total time: 0:00:05 (0.0339 s / it)
[19:35:36.842403] * Acc@1 59.850 Acc@5 95.580 loss 1.168
[19:35:36.842712] Accuracy of the network on the 10000 test images: 59.9%
[19:35:36.842892] Max accuracy: 59.85%
[19:35:37.164395] log_dir: ./output_dir
[19:35:38.045235] Epoch: [9]  [  0/781]  eta: 0:11:26  lr: 0.000249  training_loss: 1.5691 (1.5691)  classification_loss: 1.5299 (1.5299)  loss_mask: 0.0391 (0.0391)  time: 0.8791  data: 0.6889  max mem: 4132
[19:35:41.322756] Epoch: [9]  [ 20/781]  eta: 0:02:30  lr: 0.000249  training_loss: 1.6393 (1.6588)  classification_loss: 1.5918 (1.6132)  loss_mask: 0.0448 (0.0456)  time: 0.1638  data: 0.0003  max mem: 4132
[19:35:44.551940] Epoch: [9]  [ 40/781]  eta: 0:02:13  lr: 0.000249  training_loss: 1.6295 (1.6501)  classification_loss: 1.5921 (1.6033)  loss_mask: 0.0456 (0.0468)  time: 0.1614  data: 0.0002  max mem: 4132
[19:35:47.801157] Epoch: [9]  [ 60/781]  eta: 0:02:05  lr: 0.000249  training_loss: 1.7058 (1.6678)  classification_loss: 1.6303 (1.6141)  loss_mask: 0.0631 (0.0537)  time: 0.1624  data: 0.0002  max mem: 4132
[19:35:51.027224] Epoch: [9]  [ 80/781]  eta: 0:01:59  lr: 0.000249  training_loss: 1.6305 (1.6646)  classification_loss: 1.5870 (1.6122)  loss_mask: 0.0454 (0.0524)  time: 0.1612  data: 0.0002  max mem: 4132
[19:35:54.249606] Epoch: [9]  [100/781]  eta: 0:01:55  lr: 0.000249  training_loss: 1.6116 (1.6619)  classification_loss: 1.5752 (1.6103)  loss_mask: 0.0484 (0.0517)  time: 0.1610  data: 0.0002  max mem: 4132
[19:35:57.491674] Epoch: [9]  [120/781]  eta: 0:01:50  lr: 0.000249  training_loss: 1.6236 (1.6552)  classification_loss: 1.5737 (1.6045)  loss_mask: 0.0442 (0.0508)  time: 0.1620  data: 0.0003  max mem: 4132
[19:36:00.722683] Epoch: [9]  [140/781]  eta: 0:01:47  lr: 0.000249  training_loss: 1.6408 (1.6543)  classification_loss: 1.5994 (1.6047)  loss_mask: 0.0414 (0.0496)  time: 0.1615  data: 0.0002  max mem: 4132
[19:36:03.949455] Epoch: [9]  [160/781]  eta: 0:01:43  lr: 0.000249  training_loss: 1.5970 (1.6493)  classification_loss: 1.5451 (1.6002)  loss_mask: 0.0453 (0.0491)  time: 0.1613  data: 0.0002  max mem: 4132
[19:36:07.170695] Epoch: [9]  [180/781]  eta: 0:01:39  lr: 0.000249  training_loss: 1.6396 (1.6474)  classification_loss: 1.5889 (1.5985)  loss_mask: 0.0439 (0.0489)  time: 0.1610  data: 0.0003  max mem: 4132
[19:36:10.414626] Epoch: [9]  [200/781]  eta: 0:01:36  lr: 0.000249  training_loss: 1.6213 (1.6471)  classification_loss: 1.5755 (1.5978)  loss_mask: 0.0511 (0.0493)  time: 0.1621  data: 0.0003  max mem: 4132
[19:36:13.668789] Epoch: [9]  [220/781]  eta: 0:01:32  lr: 0.000249  training_loss: 1.6407 (1.6473)  classification_loss: 1.5902 (1.5984)  loss_mask: 0.0444 (0.0489)  time: 0.1626  data: 0.0002  max mem: 4132
[19:36:16.892538] Epoch: [9]  [240/781]  eta: 0:01:29  lr: 0.000249  training_loss: 1.6345 (1.6468)  classification_loss: 1.5881 (1.5980)  loss_mask: 0.0447 (0.0488)  time: 0.1611  data: 0.0003  max mem: 4132
[19:36:20.130673] Epoch: [9]  [260/781]  eta: 0:01:25  lr: 0.000249  training_loss: 1.6441 (1.6476)  classification_loss: 1.5883 (1.5977)  loss_mask: 0.0567 (0.0499)  time: 0.1618  data: 0.0002  max mem: 4132
[19:36:23.365965] Epoch: [9]  [280/781]  eta: 0:01:22  lr: 0.000249  training_loss: 1.6537 (1.6494)  classification_loss: 1.5979 (1.5991)  loss_mask: 0.0561 (0.0503)  time: 0.1617  data: 0.0002  max mem: 4132
[19:36:26.607172] Epoch: [9]  [300/781]  eta: 0:01:18  lr: 0.000249  training_loss: 1.6379 (1.6498)  classification_loss: 1.6020 (1.5996)  loss_mask: 0.0483 (0.0502)  time: 0.1620  data: 0.0003  max mem: 4132
[19:36:29.829073] Epoch: [9]  [320/781]  eta: 0:01:15  lr: 0.000249  training_loss: 1.6611 (1.6490)  classification_loss: 1.6220 (1.5993)  loss_mask: 0.0415 (0.0497)  time: 0.1610  data: 0.0003  max mem: 4132
[19:36:33.068393] Epoch: [9]  [340/781]  eta: 0:01:12  lr: 0.000249  training_loss: 1.6377 (1.6478)  classification_loss: 1.6018 (1.5985)  loss_mask: 0.0422 (0.0493)  time: 0.1619  data: 0.0003  max mem: 4132
[19:36:36.315147] Epoch: [9]  [360/781]  eta: 0:01:08  lr: 0.000249  training_loss: 1.6348 (1.6474)  classification_loss: 1.5943 (1.5987)  loss_mask: 0.0364 (0.0487)  time: 0.1622  data: 0.0002  max mem: 4132
[19:36:39.535843] Epoch: [9]  [380/781]  eta: 0:01:05  lr: 0.000249  training_loss: 1.6358 (1.6477)  classification_loss: 1.5799 (1.5990)  loss_mask: 0.0463 (0.0486)  time: 0.1609  data: 0.0002  max mem: 4132
[19:36:42.771924] Epoch: [9]  [400/781]  eta: 0:01:02  lr: 0.000249  training_loss: 1.6298 (1.6465)  classification_loss: 1.5831 (1.5979)  loss_mask: 0.0431 (0.0485)  time: 0.1617  data: 0.0004  max mem: 4132
[19:36:45.981769] Epoch: [9]  [420/781]  eta: 0:00:58  lr: 0.000249  training_loss: 1.5877 (1.6438)  classification_loss: 1.5441 (1.5954)  loss_mask: 0.0435 (0.0484)  time: 0.1604  data: 0.0003  max mem: 4132
[19:36:49.268957] Epoch: [9]  [440/781]  eta: 0:00:55  lr: 0.000249  training_loss: 1.6189 (1.6436)  classification_loss: 1.5720 (1.5954)  loss_mask: 0.0409 (0.0482)  time: 0.1643  data: 0.0002  max mem: 4132
[19:36:52.500861] Epoch: [9]  [460/781]  eta: 0:00:52  lr: 0.000249  training_loss: 1.6056 (1.6419)  classification_loss: 1.5483 (1.5939)  loss_mask: 0.0429 (0.0480)  time: 0.1615  data: 0.0003  max mem: 4132
[19:36:55.726420] Epoch: [9]  [480/781]  eta: 0:00:49  lr: 0.000249  training_loss: 1.6260 (1.6421)  classification_loss: 1.5922 (1.5945)  loss_mask: 0.0379 (0.0476)  time: 0.1612  data: 0.0003  max mem: 4132
[19:36:58.957551] Epoch: [9]  [500/781]  eta: 0:00:45  lr: 0.000249  training_loss: 1.6246 (1.6415)  classification_loss: 1.5791 (1.5942)  loss_mask: 0.0369 (0.0473)  time: 0.1615  data: 0.0002  max mem: 4132
[19:37:02.189333] Epoch: [9]  [520/781]  eta: 0:00:42  lr: 0.000249  training_loss: 1.6576 (1.6418)  classification_loss: 1.6222 (1.5948)  loss_mask: 0.0394 (0.0470)  time: 0.1615  data: 0.0002  max mem: 4132
[19:37:05.440332] Epoch: [9]  [540/781]  eta: 0:00:39  lr: 0.000249  training_loss: 1.6673 (1.6425)  classification_loss: 1.6276 (1.5957)  loss_mask: 0.0401 (0.0468)  time: 0.1625  data: 0.0002  max mem: 4132
[19:37:08.716885] Epoch: [9]  [560/781]  eta: 0:00:36  lr: 0.000248  training_loss: 1.6191 (1.6412)  classification_loss: 1.5819 (1.5947)  loss_mask: 0.0377 (0.0465)  time: 0.1637  data: 0.0003  max mem: 4132
[19:37:11.952069] Epoch: [9]  [580/781]  eta: 0:00:32  lr: 0.000248  training_loss: 1.6216 (1.6403)  classification_loss: 1.5889 (1.5940)  loss_mask: 0.0393 (0.0462)  time: 0.1617  data: 0.0002  max mem: 4132
[19:37:15.183157] Epoch: [9]  [600/781]  eta: 0:00:29  lr: 0.000248  training_loss: 1.6363 (1.6397)  classification_loss: 1.5963 (1.5937)  loss_mask: 0.0383 (0.0460)  time: 0.1615  data: 0.0003  max mem: 4132
[19:37:18.425643] Epoch: [9]  [620/781]  eta: 0:00:26  lr: 0.000248  training_loss: 1.6047 (1.6391)  classification_loss: 1.5764 (1.5934)  loss_mask: 0.0384 (0.0457)  time: 0.1620  data: 0.0002  max mem: 4132
[19:37:21.667397] Epoch: [9]  [640/781]  eta: 0:00:22  lr: 0.000248  training_loss: 1.5874 (1.6383)  classification_loss: 1.5531 (1.5928)  loss_mask: 0.0369 (0.0455)  time: 0.1620  data: 0.0002  max mem: 4132
[19:37:24.905578] Epoch: [9]  [660/781]  eta: 0:00:19  lr: 0.000248  training_loss: 1.6078 (1.6381)  classification_loss: 1.5664 (1.5926)  loss_mask: 0.0414 (0.0456)  time: 0.1618  data: 0.0002  max mem: 4132
[19:37:28.136144] Epoch: [9]  [680/781]  eta: 0:00:16  lr: 0.000248  training_loss: 1.5893 (1.6372)  classification_loss: 1.5549 (1.5918)  loss_mask: 0.0400 (0.0454)  time: 0.1615  data: 0.0003  max mem: 4132
[19:37:31.370283] Epoch: [9]  [700/781]  eta: 0:00:13  lr: 0.000248  training_loss: 1.6040 (1.6363)  classification_loss: 1.5557 (1.5908)  loss_mask: 0.0454 (0.0455)  time: 0.1616  data: 0.0002  max mem: 4132
[19:37:34.602983] Epoch: [9]  [720/781]  eta: 0:00:09  lr: 0.000248  training_loss: 1.5510 (1.6346)  classification_loss: 1.5140 (1.5894)  loss_mask: 0.0380 (0.0452)  time: 0.1616  data: 0.0002  max mem: 4132
[19:37:37.832461] Epoch: [9]  [740/781]  eta: 0:00:06  lr: 0.000248  training_loss: 1.6040 (1.6340)  classification_loss: 1.5697 (1.5891)  loss_mask: 0.0343 (0.0449)  time: 0.1614  data: 0.0002  max mem: 4132
[19:37:41.123404] Epoch: [9]  [760/781]  eta: 0:00:03  lr: 0.000248  training_loss: 1.6464 (1.6343)  classification_loss: 1.6034 (1.5894)  loss_mask: 0.0421 (0.0449)  time: 0.1645  data: 0.0002  max mem: 4132
[19:37:44.339851] Epoch: [9]  [780/781]  eta: 0:00:00  lr: 0.000248  training_loss: 1.6111 (1.6341)  classification_loss: 1.5838 (1.5894)  loss_mask: 0.0419 (0.0448)  time: 0.1607  data: 0.0002  max mem: 4132
[19:37:44.523132] Epoch: [9] Total time: 0:02:07 (0.1631 s / it)
[19:37:44.523657] Averaged stats: lr: 0.000248  training_loss: 1.6111 (1.6341)  classification_loss: 1.5838 (1.5894)  loss_mask: 0.0419 (0.0448)
[19:37:45.234270] Test:  [  0/157]  eta: 0:01:50  testing_loss: 1.1230 (1.1230)  acc1: 57.8125 (57.8125)  acc5: 93.7500 (93.7500)  time: 0.7066  data: 0.6757  max mem: 4132
[19:37:45.521861] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.1552 (1.2016)  acc1: 57.8125 (57.6705)  acc5: 96.8750 (96.0227)  time: 0.0902  data: 0.0616  max mem: 4132
[19:37:45.812354] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.1381 (1.1623)  acc1: 62.5000 (59.9702)  acc5: 96.8750 (96.5030)  time: 0.0287  data: 0.0002  max mem: 4132
[19:37:46.094372] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.1442 (1.1601)  acc1: 62.5000 (61.0383)  acc5: 96.8750 (96.1190)  time: 0.0285  data: 0.0002  max mem: 4132
[19:37:46.387112] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.1538 (1.1612)  acc1: 60.9375 (60.8232)  acc5: 95.3125 (96.0366)  time: 0.0286  data: 0.0002  max mem: 4132
[19:37:46.672204] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.1158 (1.1543)  acc1: 60.9375 (61.4277)  acc5: 96.8750 (96.2010)  time: 0.0287  data: 0.0002  max mem: 4132
[19:37:46.955103] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.1051 (1.1481)  acc1: 64.0625 (61.4754)  acc5: 96.8750 (96.1322)  time: 0.0283  data: 0.0002  max mem: 4132
[19:37:47.245674] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.0750 (1.1395)  acc1: 64.0625 (61.8838)  acc5: 96.8750 (96.1708)  time: 0.0285  data: 0.0002  max mem: 4132
[19:37:47.532411] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.0985 (1.1411)  acc1: 64.0625 (61.9213)  acc5: 96.8750 (96.2577)  time: 0.0287  data: 0.0004  max mem: 4132
[19:37:47.819764] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.1488 (1.1416)  acc1: 62.5000 (61.8304)  acc5: 96.8750 (96.1882)  time: 0.0286  data: 0.0004  max mem: 4132
[19:37:48.104449] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.1703 (1.1484)  acc1: 60.9375 (61.6337)  acc5: 96.8750 (96.1943)  time: 0.0285  data: 0.0002  max mem: 4132
[19:37:48.388188] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.1888 (1.1491)  acc1: 60.9375 (61.5850)  acc5: 96.8750 (96.2697)  time: 0.0283  data: 0.0002  max mem: 4132
[19:37:48.674187] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.1622 (1.1461)  acc1: 60.9375 (61.5702)  acc5: 96.8750 (96.3326)  time: 0.0284  data: 0.0002  max mem: 4132
[19:37:48.956179] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.1474 (1.1492)  acc1: 64.0625 (61.4623)  acc5: 96.8750 (96.3263)  time: 0.0283  data: 0.0002  max mem: 4132
[19:37:49.239729] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.1649 (1.1506)  acc1: 60.9375 (61.4583)  acc5: 95.3125 (96.2212)  time: 0.0281  data: 0.0002  max mem: 4132
[19:37:49.519986] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.1243 (1.1483)  acc1: 62.5000 (61.5998)  acc5: 95.3125 (96.2024)  time: 0.0281  data: 0.0001  max mem: 4132
[19:37:49.673362] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.1213 (1.1496)  acc1: 62.5000 (61.6000)  acc5: 96.8750 (96.2500)  time: 0.0271  data: 0.0001  max mem: 4132
[19:37:49.823333] Test: Total time: 0:00:05 (0.0337 s / it)
[19:37:49.823835] * Acc@1 61.600 Acc@5 96.250 loss 1.150
[19:37:49.824161] Accuracy of the network on the 10000 test images: 61.6%
[19:37:49.824355] Max accuracy: 61.60%
[19:37:50.096271] log_dir: ./output_dir
[19:37:50.844275] Epoch: [10]  [  0/781]  eta: 0:09:42  lr: 0.000248  training_loss: 1.4893 (1.4893)  classification_loss: 1.4437 (1.4437)  loss_mask: 0.0456 (0.0456)  time: 0.7461  data: 0.5640  max mem: 4132
[19:37:54.080354] Epoch: [10]  [ 20/781]  eta: 0:02:24  lr: 0.000248  training_loss: 1.6000 (1.6164)  classification_loss: 1.5592 (1.5730)  loss_mask: 0.0407 (0.0434)  time: 0.1617  data: 0.0002  max mem: 4132
[19:37:57.373455] Epoch: [10]  [ 40/781]  eta: 0:02:11  lr: 0.000248  training_loss: 1.6228 (1.6238)  classification_loss: 1.5836 (1.5819)  loss_mask: 0.0392 (0.0419)  time: 0.1646  data: 0.0003  max mem: 4132
[19:38:00.602938] Epoch: [10]  [ 60/781]  eta: 0:02:04  lr: 0.000248  training_loss: 1.7046 (1.6382)  classification_loss: 1.6424 (1.5966)  loss_mask: 0.0364 (0.0416)  time: 0.1614  data: 0.0002  max mem: 4132
[19:38:03.835576] Epoch: [10]  [ 80/781]  eta: 0:01:58  lr: 0.000248  training_loss: 1.6394 (1.6356)  classification_loss: 1.6010 (1.5950)  loss_mask: 0.0334 (0.0406)  time: 0.1616  data: 0.0002  max mem: 4132
[19:38:07.074609] Epoch: [10]  [100/781]  eta: 0:01:54  lr: 0.000248  training_loss: 1.6127 (1.6308)  classification_loss: 1.5791 (1.5916)  loss_mask: 0.0332 (0.0392)  time: 0.1619  data: 0.0002  max mem: 4132
[19:38:10.312241] Epoch: [10]  [120/781]  eta: 0:01:50  lr: 0.000248  training_loss: 1.6096 (1.6293)  classification_loss: 1.5769 (1.5907)  loss_mask: 0.0327 (0.0386)  time: 0.1618  data: 0.0002  max mem: 4132
[19:38:13.526698] Epoch: [10]  [140/781]  eta: 0:01:46  lr: 0.000248  training_loss: 1.5732 (1.6205)  classification_loss: 1.5358 (1.5824)  loss_mask: 0.0341 (0.0381)  time: 0.1606  data: 0.0002  max mem: 4132
[19:38:16.753921] Epoch: [10]  [160/781]  eta: 0:01:42  lr: 0.000248  training_loss: 1.5732 (1.6172)  classification_loss: 1.5387 (1.5790)  loss_mask: 0.0377 (0.0383)  time: 0.1613  data: 0.0002  max mem: 4132
[19:38:19.972835] Epoch: [10]  [180/781]  eta: 0:01:39  lr: 0.000248  training_loss: 1.6150 (1.6173)  classification_loss: 1.5875 (1.5792)  loss_mask: 0.0331 (0.0381)  time: 0.1609  data: 0.0003  max mem: 4132
[19:38:23.189050] Epoch: [10]  [200/781]  eta: 0:01:35  lr: 0.000248  training_loss: 1.6087 (1.6176)  classification_loss: 1.5751 (1.5795)  loss_mask: 0.0392 (0.0381)  time: 0.1607  data: 0.0003  max mem: 4132
[19:38:26.461781] Epoch: [10]  [220/781]  eta: 0:01:32  lr: 0.000248  training_loss: 1.6587 (1.6205)  classification_loss: 1.5859 (1.5820)  loss_mask: 0.0362 (0.0384)  time: 0.1636  data: 0.0002  max mem: 4132
[19:38:29.735283] Epoch: [10]  [240/781]  eta: 0:01:28  lr: 0.000248  training_loss: 1.6145 (1.6189)  classification_loss: 1.5916 (1.5806)  loss_mask: 0.0354 (0.0383)  time: 0.1636  data: 0.0002  max mem: 4132
[19:38:32.970619] Epoch: [10]  [260/781]  eta: 0:01:25  lr: 0.000248  training_loss: 1.5791 (1.6160)  classification_loss: 1.5447 (1.5777)  loss_mask: 0.0354 (0.0383)  time: 0.1617  data: 0.0001  max mem: 4132
[19:38:36.197765] Epoch: [10]  [280/781]  eta: 0:01:22  lr: 0.000248  training_loss: 1.5807 (1.6143)  classification_loss: 1.5480 (1.5764)  loss_mask: 0.0323 (0.0379)  time: 0.1613  data: 0.0003  max mem: 4132
[19:38:39.420567] Epoch: [10]  [300/781]  eta: 0:01:18  lr: 0.000248  training_loss: 1.6132 (1.6143)  classification_loss: 1.5798 (1.5769)  loss_mask: 0.0295 (0.0374)  time: 0.1611  data: 0.0002  max mem: 4132
[19:38:42.648489] Epoch: [10]  [320/781]  eta: 0:01:15  lr: 0.000248  training_loss: 1.5895 (1.6135)  classification_loss: 1.5606 (1.5767)  loss_mask: 0.0280 (0.0369)  time: 0.1613  data: 0.0002  max mem: 4132
[19:38:45.877779] Epoch: [10]  [340/781]  eta: 0:01:12  lr: 0.000248  training_loss: 1.5813 (1.6136)  classification_loss: 1.5503 (1.5772)  loss_mask: 0.0284 (0.0365)  time: 0.1613  data: 0.0002  max mem: 4132
[19:38:49.144034] Epoch: [10]  [360/781]  eta: 0:01:08  lr: 0.000248  training_loss: 1.6236 (1.6151)  classification_loss: 1.5890 (1.5784)  loss_mask: 0.0337 (0.0366)  time: 0.1632  data: 0.0003  max mem: 4132
[19:38:52.482050] Epoch: [10]  [380/781]  eta: 0:01:05  lr: 0.000248  training_loss: 1.6054 (1.6149)  classification_loss: 1.5514 (1.5777)  loss_mask: 0.0446 (0.0372)  time: 0.1668  data: 0.0003  max mem: 4132
[19:38:55.719367] Epoch: [10]  [400/781]  eta: 0:01:02  lr: 0.000248  training_loss: 1.6391 (1.6152)  classification_loss: 1.5998 (1.5781)  loss_mask: 0.0346 (0.0372)  time: 0.1618  data: 0.0003  max mem: 4132
[19:38:58.955363] Epoch: [10]  [420/781]  eta: 0:00:59  lr: 0.000248  training_loss: 1.5574 (1.6130)  classification_loss: 1.5334 (1.5761)  loss_mask: 0.0296 (0.0369)  time: 0.1617  data: 0.0003  max mem: 4132
[19:39:02.180311] Epoch: [10]  [440/781]  eta: 0:00:55  lr: 0.000248  training_loss: 1.6074 (1.6135)  classification_loss: 1.5773 (1.5767)  loss_mask: 0.0310 (0.0367)  time: 0.1611  data: 0.0002  max mem: 4132
[19:39:05.406197] Epoch: [10]  [460/781]  eta: 0:00:52  lr: 0.000248  training_loss: 1.5619 (1.6119)  classification_loss: 1.5299 (1.5754)  loss_mask: 0.0303 (0.0365)  time: 0.1612  data: 0.0003  max mem: 4132
[19:39:08.630250] Epoch: [10]  [480/781]  eta: 0:00:49  lr: 0.000248  training_loss: 1.5891 (1.6120)  classification_loss: 1.5406 (1.5756)  loss_mask: 0.0321 (0.0365)  time: 0.1611  data: 0.0002  max mem: 4132
[19:39:11.885490] Epoch: [10]  [500/781]  eta: 0:00:45  lr: 0.000248  training_loss: 1.5846 (1.6115)  classification_loss: 1.5455 (1.5752)  loss_mask: 0.0302 (0.0363)  time: 0.1627  data: 0.0003  max mem: 4132
[19:39:15.129170] Epoch: [10]  [520/781]  eta: 0:00:42  lr: 0.000248  training_loss: 1.5556 (1.6108)  classification_loss: 1.5277 (1.5747)  loss_mask: 0.0274 (0.0361)  time: 0.1621  data: 0.0002  max mem: 4132
[19:39:18.368588] Epoch: [10]  [540/781]  eta: 0:00:39  lr: 0.000248  training_loss: 1.5945 (1.6116)  classification_loss: 1.5584 (1.5755)  loss_mask: 0.0316 (0.0360)  time: 0.1619  data: 0.0002  max mem: 4132
[19:39:21.621818] Epoch: [10]  [560/781]  eta: 0:00:36  lr: 0.000248  training_loss: 1.5932 (1.6113)  classification_loss: 1.5601 (1.5756)  loss_mask: 0.0268 (0.0357)  time: 0.1626  data: 0.0002  max mem: 4132
[19:39:24.880524] Epoch: [10]  [580/781]  eta: 0:00:32  lr: 0.000248  training_loss: 1.5913 (1.6105)  classification_loss: 1.5543 (1.5750)  loss_mask: 0.0268 (0.0354)  time: 0.1629  data: 0.0003  max mem: 4132
[19:39:28.112399] Epoch: [10]  [600/781]  eta: 0:00:29  lr: 0.000248  training_loss: 1.5767 (1.6096)  classification_loss: 1.5391 (1.5744)  loss_mask: 0.0272 (0.0352)  time: 0.1615  data: 0.0002  max mem: 4132
[19:39:31.344797] Epoch: [10]  [620/781]  eta: 0:00:26  lr: 0.000248  training_loss: 1.5502 (1.6085)  classification_loss: 1.5178 (1.5734)  loss_mask: 0.0315 (0.0351)  time: 0.1615  data: 0.0002  max mem: 4132
[19:39:34.580886] Epoch: [10]  [640/781]  eta: 0:00:22  lr: 0.000248  training_loss: 1.5732 (1.6075)  classification_loss: 1.5428 (1.5728)  loss_mask: 0.0253 (0.0348)  time: 0.1617  data: 0.0002  max mem: 4132
[19:39:37.852383] Epoch: [10]  [660/781]  eta: 0:00:19  lr: 0.000248  training_loss: 1.6101 (1.6080)  classification_loss: 1.5825 (1.5734)  loss_mask: 0.0273 (0.0346)  time: 0.1635  data: 0.0003  max mem: 4132
[19:39:41.097723] Epoch: [10]  [680/781]  eta: 0:00:16  lr: 0.000248  training_loss: 1.5657 (1.6069)  classification_loss: 1.5344 (1.5725)  loss_mask: 0.0309 (0.0345)  time: 0.1622  data: 0.0002  max mem: 4132
[19:39:44.330652] Epoch: [10]  [700/781]  eta: 0:00:13  lr: 0.000248  training_loss: 1.5627 (1.6062)  classification_loss: 1.5216 (1.5716)  loss_mask: 0.0348 (0.0345)  time: 0.1616  data: 0.0002  max mem: 4132
[19:39:47.556104] Epoch: [10]  [720/781]  eta: 0:00:09  lr: 0.000248  training_loss: 1.5758 (1.6059)  classification_loss: 1.5393 (1.5712)  loss_mask: 0.0400 (0.0347)  time: 0.1612  data: 0.0002  max mem: 4132
[19:39:50.788827] Epoch: [10]  [740/781]  eta: 0:00:06  lr: 0.000248  training_loss: 1.5766 (1.6051)  classification_loss: 1.5356 (1.5704)  loss_mask: 0.0324 (0.0347)  time: 0.1616  data: 0.0003  max mem: 4132
[19:39:54.042728] Epoch: [10]  [760/781]  eta: 0:00:03  lr: 0.000248  training_loss: 1.5746 (1.6048)  classification_loss: 1.5453 (1.5701)  loss_mask: 0.0290 (0.0346)  time: 0.1626  data: 0.0002  max mem: 4132
[19:39:57.252445] Epoch: [10]  [780/781]  eta: 0:00:00  lr: 0.000248  training_loss: 1.5749 (1.6047)  classification_loss: 1.5366 (1.5701)  loss_mask: 0.0309 (0.0346)  time: 0.1604  data: 0.0002  max mem: 4132
[19:39:57.414461] Epoch: [10] Total time: 0:02:07 (0.1630 s / it)
[19:39:57.414908] Averaged stats: lr: 0.000248  training_loss: 1.5749 (1.6047)  classification_loss: 1.5366 (1.5701)  loss_mask: 0.0309 (0.0346)
[19:39:59.526211] Test:  [  0/157]  eta: 0:01:53  testing_loss: 1.1665 (1.1665)  acc1: 57.8125 (57.8125)  acc5: 96.8750 (96.8750)  time: 0.7204  data: 0.6884  max mem: 4132
[19:39:59.829487] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.1665 (1.1844)  acc1: 57.8125 (58.3807)  acc5: 95.3125 (95.7386)  time: 0.0929  data: 0.0627  max mem: 4132
[19:40:00.113225] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.1137 (1.1369)  acc1: 62.5000 (61.3839)  acc5: 96.8750 (96.2054)  time: 0.0291  data: 0.0002  max mem: 4132
[19:40:00.395810] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.1144 (1.1346)  acc1: 65.6250 (62.1472)  acc5: 96.8750 (95.8165)  time: 0.0281  data: 0.0002  max mem: 4132
[19:40:00.681234] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.1144 (1.1332)  acc1: 62.5000 (62.4619)  acc5: 95.3125 (95.9604)  time: 0.0283  data: 0.0002  max mem: 4132
[19:40:00.961279] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.0947 (1.1298)  acc1: 62.5000 (62.6838)  acc5: 96.8750 (96.2010)  time: 0.0282  data: 0.0001  max mem: 4132
[19:40:01.242356] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.1218 (1.1274)  acc1: 60.9375 (62.5256)  acc5: 96.8750 (96.1322)  time: 0.0279  data: 0.0001  max mem: 4132
[19:40:01.525319] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.0876 (1.1182)  acc1: 62.5000 (63.0502)  acc5: 96.8750 (96.1708)  time: 0.0281  data: 0.0002  max mem: 4132
[19:40:01.805216] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.0876 (1.1233)  acc1: 62.5000 (62.7508)  acc5: 96.8750 (96.1420)  time: 0.0280  data: 0.0001  max mem: 4132
[19:40:02.087418] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.1632 (1.1273)  acc1: 62.5000 (62.5859)  acc5: 96.8750 (96.1367)  time: 0.0280  data: 0.0001  max mem: 4132
[19:40:02.372539] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.1644 (1.1324)  acc1: 59.3750 (62.2215)  acc5: 95.3125 (96.0860)  time: 0.0282  data: 0.0002  max mem: 4132
[19:40:02.657868] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.1858 (1.1348)  acc1: 59.3750 (62.3311)  acc5: 95.3125 (96.0867)  time: 0.0284  data: 0.0002  max mem: 4132
[19:40:02.944058] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.1040 (1.1291)  acc1: 62.5000 (62.4483)  acc5: 96.8750 (96.1519)  time: 0.0285  data: 0.0002  max mem: 4132
[19:40:03.230727] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.0925 (1.1309)  acc1: 60.9375 (62.2615)  acc5: 96.8750 (96.1594)  time: 0.0285  data: 0.0002  max mem: 4132
[19:40:03.513498] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.1403 (1.1327)  acc1: 59.3750 (62.1454)  acc5: 95.3125 (96.1104)  time: 0.0283  data: 0.0002  max mem: 4132
[19:40:03.792379] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.1341 (1.1298)  acc1: 62.5000 (62.4172)  acc5: 95.3125 (96.0886)  time: 0.0279  data: 0.0001  max mem: 4132
[19:40:03.944610] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.0914 (1.1311)  acc1: 62.5000 (62.3900)  acc5: 95.3125 (96.1200)  time: 0.0269  data: 0.0001  max mem: 4132
[19:40:04.102873] Test: Total time: 0:00:05 (0.0338 s / it)
[19:40:04.103341] * Acc@1 62.390 Acc@5 96.120 loss 1.131
[19:40:04.103924] Accuracy of the network on the 10000 test images: 62.4%
[19:40:04.104173] Max accuracy: 62.39%
[19:40:04.415564] log_dir: ./output_dir
[19:40:05.347905] Epoch: [11]  [  0/781]  eta: 0:12:06  lr: 0.000248  training_loss: 1.5085 (1.5085)  classification_loss: 1.4795 (1.4795)  loss_mask: 0.0290 (0.0290)  time: 0.9300  data: 0.7555  max mem: 4132
[19:40:08.626893] Epoch: [11]  [ 20/781]  eta: 0:02:32  lr: 0.000248  training_loss: 1.5723 (1.5786)  classification_loss: 1.5392 (1.5512)  loss_mask: 0.0272 (0.0274)  time: 0.1639  data: 0.0002  max mem: 4132
[19:40:11.859395] Epoch: [11]  [ 40/781]  eta: 0:02:14  lr: 0.000248  training_loss: 1.5916 (1.5853)  classification_loss: 1.5734 (1.5592)  loss_mask: 0.0249 (0.0261)  time: 0.1615  data: 0.0002  max mem: 4132
[19:40:15.092750] Epoch: [11]  [ 60/781]  eta: 0:02:06  lr: 0.000247  training_loss: 1.5955 (1.5929)  classification_loss: 1.5652 (1.5666)  loss_mask: 0.0255 (0.0263)  time: 0.1615  data: 0.0002  max mem: 4132
[19:40:18.353743] Epoch: [11]  [ 80/781]  eta: 0:02:00  lr: 0.000247  training_loss: 1.6040 (1.5961)  classification_loss: 1.5589 (1.5681)  loss_mask: 0.0285 (0.0280)  time: 0.1630  data: 0.0004  max mem: 4132
[19:40:21.589899] Epoch: [11]  [100/781]  eta: 0:01:55  lr: 0.000247  training_loss: 1.5836 (1.5950)  classification_loss: 1.5598 (1.5677)  loss_mask: 0.0243 (0.0273)  time: 0.1617  data: 0.0002  max mem: 4132
[19:40:24.854489] Epoch: [11]  [120/781]  eta: 0:01:51  lr: 0.000247  training_loss: 1.5607 (1.5907)  classification_loss: 1.5359 (1.5634)  loss_mask: 0.0273 (0.0273)  time: 0.1631  data: 0.0002  max mem: 4132
[19:40:28.073774] Epoch: [11]  [140/781]  eta: 0:01:47  lr: 0.000247  training_loss: 1.5540 (1.5857)  classification_loss: 1.5209 (1.5582)  loss_mask: 0.0249 (0.0275)  time: 0.1609  data: 0.0002  max mem: 4132
[19:40:31.325447] Epoch: [11]  [160/781]  eta: 0:01:43  lr: 0.000247  training_loss: 1.5767 (1.5850)  classification_loss: 1.5496 (1.5578)  loss_mask: 0.0234 (0.0272)  time: 0.1625  data: 0.0002  max mem: 4132
[19:40:34.563156] Epoch: [11]  [180/781]  eta: 0:01:40  lr: 0.000247  training_loss: 1.5507 (1.5826)  classification_loss: 1.5124 (1.5550)  loss_mask: 0.0294 (0.0276)  time: 0.1618  data: 0.0002  max mem: 4132
[19:40:37.810840] Epoch: [11]  [200/781]  eta: 0:01:36  lr: 0.000247  training_loss: 1.5717 (1.5819)  classification_loss: 1.5488 (1.5540)  loss_mask: 0.0295 (0.0279)  time: 0.1623  data: 0.0002  max mem: 4132
[19:40:41.045631] Epoch: [11]  [220/781]  eta: 0:01:32  lr: 0.000247  training_loss: 1.6019 (1.5836)  classification_loss: 1.5774 (1.5558)  loss_mask: 0.0248 (0.0278)  time: 0.1617  data: 0.0002  max mem: 4132
[19:40:44.289772] Epoch: [11]  [240/781]  eta: 0:01:29  lr: 0.000247  training_loss: 1.5990 (1.5848)  classification_loss: 1.5801 (1.5572)  loss_mask: 0.0220 (0.0275)  time: 0.1621  data: 0.0002  max mem: 4132
[19:40:47.532704] Epoch: [11]  [260/781]  eta: 0:01:26  lr: 0.000247  training_loss: 1.4898 (1.5788)  classification_loss: 1.4701 (1.5515)  loss_mask: 0.0242 (0.0273)  time: 0.1621  data: 0.0003  max mem: 4132
[19:40:50.787355] Epoch: [11]  [280/781]  eta: 0:01:22  lr: 0.000247  training_loss: 1.5779 (1.5784)  classification_loss: 1.5499 (1.5512)  loss_mask: 0.0247 (0.0272)  time: 0.1627  data: 0.0003  max mem: 4132
[19:40:54.008046] Epoch: [11]  [300/781]  eta: 0:01:19  lr: 0.000247  training_loss: 1.5900 (1.5791)  classification_loss: 1.5645 (1.5523)  loss_mask: 0.0219 (0.0268)  time: 0.1609  data: 0.0002  max mem: 4132
[19:40:57.238588] Epoch: [11]  [320/781]  eta: 0:01:15  lr: 0.000247  training_loss: 1.5583 (1.5781)  classification_loss: 1.5396 (1.5513)  loss_mask: 0.0267 (0.0268)  time: 0.1614  data: 0.0002  max mem: 4132
[19:41:00.482184] Epoch: [11]  [340/781]  eta: 0:01:12  lr: 0.000247  training_loss: 1.5679 (1.5776)  classification_loss: 1.5399 (1.5496)  loss_mask: 0.0416 (0.0280)  time: 0.1621  data: 0.0002  max mem: 4132
[19:41:03.719878] Epoch: [11]  [360/781]  eta: 0:01:09  lr: 0.000247  training_loss: 1.5912 (1.5790)  classification_loss: 1.5632 (1.5512)  loss_mask: 0.0243 (0.0278)  time: 0.1618  data: 0.0002  max mem: 4132
[19:41:07.000959] Epoch: [11]  [380/781]  eta: 0:01:05  lr: 0.000247  training_loss: 1.5977 (1.5800)  classification_loss: 1.5731 (1.5522)  loss_mask: 0.0246 (0.0278)  time: 0.1640  data: 0.0002  max mem: 4132
[19:41:10.247485] Epoch: [11]  [400/781]  eta: 0:01:02  lr: 0.000247  training_loss: 1.5194 (1.5784)  classification_loss: 1.4995 (1.5510)  loss_mask: 0.0203 (0.0275)  time: 0.1622  data: 0.0002  max mem: 4132
[19:41:13.495887] Epoch: [11]  [420/781]  eta: 0:00:59  lr: 0.000247  training_loss: 1.5100 (1.5763)  classification_loss: 1.4893 (1.5492)  loss_mask: 0.0194 (0.0271)  time: 0.1623  data: 0.0002  max mem: 4132
[19:41:16.757976] Epoch: [11]  [440/781]  eta: 0:00:55  lr: 0.000247  training_loss: 1.6124 (1.5773)  classification_loss: 1.5859 (1.5505)  loss_mask: 0.0210 (0.0269)  time: 0.1630  data: 0.0002  max mem: 4132
[19:41:19.980404] Epoch: [11]  [460/781]  eta: 0:00:52  lr: 0.000247  training_loss: 1.5523 (1.5761)  classification_loss: 1.5280 (1.5491)  loss_mask: 0.0258 (0.0270)  time: 0.1610  data: 0.0002  max mem: 4132
[19:41:23.219426] Epoch: [11]  [480/781]  eta: 0:00:49  lr: 0.000247  training_loss: 1.6013 (1.5777)  classification_loss: 1.5813 (1.5504)  loss_mask: 0.0319 (0.0274)  time: 0.1619  data: 0.0002  max mem: 4132
[19:41:26.445912] Epoch: [11]  [500/781]  eta: 0:00:45  lr: 0.000247  training_loss: 1.5895 (1.5789)  classification_loss: 1.5631 (1.5515)  loss_mask: 0.0264 (0.0274)  time: 0.1612  data: 0.0002  max mem: 4132
[19:41:29.672408] Epoch: [11]  [520/781]  eta: 0:00:42  lr: 0.000247  training_loss: 1.5447 (1.5789)  classification_loss: 1.5279 (1.5515)  loss_mask: 0.0244 (0.0274)  time: 0.1612  data: 0.0002  max mem: 4132
[19:41:32.934467] Epoch: [11]  [540/781]  eta: 0:00:39  lr: 0.000247  training_loss: 1.5466 (1.5787)  classification_loss: 1.5322 (1.5514)  loss_mask: 0.0219 (0.0273)  time: 0.1630  data: 0.0002  max mem: 4132
[19:41:36.157302] Epoch: [11]  [560/781]  eta: 0:00:36  lr: 0.000247  training_loss: 1.5369 (1.5776)  classification_loss: 1.5098 (1.5505)  loss_mask: 0.0204 (0.0271)  time: 0.1611  data: 0.0002  max mem: 4132
[19:41:39.387142] Epoch: [11]  [580/781]  eta: 0:00:32  lr: 0.000247  training_loss: 1.5431 (1.5763)  classification_loss: 1.5241 (1.5494)  loss_mask: 0.0208 (0.0269)  time: 0.1614  data: 0.0002  max mem: 4132
[19:41:42.605202] Epoch: [11]  [600/781]  eta: 0:00:29  lr: 0.000247  training_loss: 1.5484 (1.5749)  classification_loss: 1.5174 (1.5480)  loss_mask: 0.0220 (0.0268)  time: 0.1608  data: 0.0002  max mem: 4132
[19:41:45.852289] Epoch: [11]  [620/781]  eta: 0:00:26  lr: 0.000247  training_loss: 1.5578 (1.5744)  classification_loss: 1.5352 (1.5476)  loss_mask: 0.0226 (0.0267)  time: 0.1623  data: 0.0002  max mem: 4132
[19:41:49.080032] Epoch: [11]  [640/781]  eta: 0:00:23  lr: 0.000247  training_loss: 1.5418 (1.5739)  classification_loss: 1.5242 (1.5474)  loss_mask: 0.0190 (0.0265)  time: 0.1613  data: 0.0002  max mem: 4132
[19:41:52.318767] Epoch: [11]  [660/781]  eta: 0:00:19  lr: 0.000247  training_loss: 1.5717 (1.5741)  classification_loss: 1.5499 (1.5478)  loss_mask: 0.0192 (0.0263)  time: 0.1618  data: 0.0002  max mem: 4132
[19:41:55.561754] Epoch: [11]  [680/781]  eta: 0:00:16  lr: 0.000247  training_loss: 1.5449 (1.5742)  classification_loss: 1.5178 (1.5480)  loss_mask: 0.0239 (0.0263)  time: 0.1620  data: 0.0002  max mem: 4132
[19:41:58.786147] Epoch: [11]  [700/781]  eta: 0:00:13  lr: 0.000247  training_loss: 1.5602 (1.5742)  classification_loss: 1.5334 (1.5480)  loss_mask: 0.0195 (0.0262)  time: 0.1611  data: 0.0002  max mem: 4132
[19:42:02.019368] Epoch: [11]  [720/781]  eta: 0:00:09  lr: 0.000247  training_loss: 1.5685 (1.5743)  classification_loss: 1.5414 (1.5482)  loss_mask: 0.0214 (0.0261)  time: 0.1616  data: 0.0002  max mem: 4132
[19:42:05.241583] Epoch: [11]  [740/781]  eta: 0:00:06  lr: 0.000247  training_loss: 1.5987 (1.5740)  classification_loss: 1.5751 (1.5481)  loss_mask: 0.0198 (0.0260)  time: 0.1610  data: 0.0002  max mem: 4132
[19:42:08.478794] Epoch: [11]  [760/781]  eta: 0:00:03  lr: 0.000247  training_loss: 1.5746 (1.5742)  classification_loss: 1.5564 (1.5484)  loss_mask: 0.0217 (0.0259)  time: 0.1618  data: 0.0002  max mem: 4132
[19:42:11.692910] Epoch: [11]  [780/781]  eta: 0:00:00  lr: 0.000247  training_loss: 1.5766 (1.5744)  classification_loss: 1.5480 (1.5486)  loss_mask: 0.0221 (0.0258)  time: 0.1606  data: 0.0002  max mem: 4132
[19:42:11.851139] Epoch: [11] Total time: 0:02:07 (0.1632 s / it)
[19:42:11.851953] Averaged stats: lr: 0.000247  training_loss: 1.5766 (1.5744)  classification_loss: 1.5480 (1.5486)  loss_mask: 0.0221 (0.0258)
[19:42:12.426338] Test:  [  0/157]  eta: 0:01:29  testing_loss: 1.0844 (1.0844)  acc1: 59.3750 (59.3750)  acc5: 93.7500 (93.7500)  time: 0.5706  data: 0.5402  max mem: 4132
[19:42:12.709801] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 1.1430 (1.1228)  acc1: 60.9375 (60.7955)  acc5: 98.4375 (97.1591)  time: 0.0775  data: 0.0493  max mem: 4132
[19:42:12.992138] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.0650 (1.0720)  acc1: 62.5000 (62.9464)  acc5: 98.4375 (97.7679)  time: 0.0282  data: 0.0002  max mem: 4132
[19:42:13.271579] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 1.0502 (1.0668)  acc1: 65.6250 (63.6593)  acc5: 98.4375 (97.1270)  time: 0.0280  data: 0.0001  max mem: 4132
[19:42:13.553032] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 1.0826 (1.0705)  acc1: 64.0625 (63.4527)  acc5: 96.8750 (97.0274)  time: 0.0279  data: 0.0001  max mem: 4132
[19:42:13.833745] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.0365 (1.0668)  acc1: 64.0625 (64.0012)  acc5: 96.8750 (96.9056)  time: 0.0280  data: 0.0002  max mem: 4132
[19:42:14.114922] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.0372 (1.0616)  acc1: 64.0625 (64.0369)  acc5: 96.8750 (96.9518)  time: 0.0280  data: 0.0002  max mem: 4132
[19:42:14.396991] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.9950 (1.0503)  acc1: 67.1875 (64.7667)  acc5: 96.8750 (96.9410)  time: 0.0280  data: 0.0002  max mem: 4132
[19:42:14.678839] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.0082 (1.0522)  acc1: 67.1875 (64.6412)  acc5: 96.8750 (96.8364)  time: 0.0281  data: 0.0002  max mem: 4132
[19:42:14.959869] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.0524 (1.0551)  acc1: 62.5000 (64.3201)  acc5: 96.8750 (96.9437)  time: 0.0280  data: 0.0002  max mem: 4132
[19:42:15.240787] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.0963 (1.0608)  acc1: 59.3750 (63.9697)  acc5: 98.4375 (96.9369)  time: 0.0280  data: 0.0002  max mem: 4132
[19:42:15.521524] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.0999 (1.0618)  acc1: 62.5000 (64.2033)  acc5: 96.8750 (96.9313)  time: 0.0280  data: 0.0002  max mem: 4132
[19:42:15.803275] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.0362 (1.0586)  acc1: 65.6250 (64.3079)  acc5: 96.8750 (96.9654)  time: 0.0280  data: 0.0002  max mem: 4132
[19:42:16.083706] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.0328 (1.0620)  acc1: 62.5000 (63.9432)  acc5: 96.8750 (96.9585)  time: 0.0280  data: 0.0002  max mem: 4132
[19:42:16.363453] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.0554 (1.0624)  acc1: 60.9375 (63.8409)  acc5: 96.8750 (96.9526)  time: 0.0279  data: 0.0001  max mem: 4132
[19:42:16.641725] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.0338 (1.0593)  acc1: 64.0625 (64.0004)  acc5: 96.8750 (96.9060)  time: 0.0278  data: 0.0001  max mem: 4132
[19:42:16.791627] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.0338 (1.0599)  acc1: 65.6250 (63.9300)  acc5: 96.8750 (96.9200)  time: 0.0268  data: 0.0001  max mem: 4132
[19:42:16.934381] Test: Total time: 0:00:05 (0.0324 s / it)
[19:42:16.934852] * Acc@1 63.930 Acc@5 96.920 loss 1.060
[19:42:16.935138] Accuracy of the network on the 10000 test images: 63.9%
[19:42:16.935336] Max accuracy: 63.93%
[19:42:17.198463] log_dir: ./output_dir
[19:42:17.967588] Epoch: [12]  [  0/781]  eta: 0:09:59  lr: 0.000247  training_loss: 1.4395 (1.4395)  classification_loss: 1.4186 (1.4186)  loss_mask: 0.0208 (0.0208)  time: 0.7672  data: 0.5948  max mem: 4132
[19:42:21.177755] Epoch: [12]  [ 20/781]  eta: 0:02:24  lr: 0.000247  training_loss: 1.5363 (1.5474)  classification_loss: 1.5107 (1.5245)  loss_mask: 0.0233 (0.0229)  time: 0.1604  data: 0.0002  max mem: 4132
[19:42:24.399718] Epoch: [12]  [ 40/781]  eta: 0:02:10  lr: 0.000247  training_loss: 1.5464 (1.5601)  classification_loss: 1.5289 (1.5378)  loss_mask: 0.0190 (0.0223)  time: 0.1610  data: 0.0002  max mem: 4132
[19:42:27.629691] Epoch: [12]  [ 60/781]  eta: 0:02:03  lr: 0.000247  training_loss: 1.6023 (1.5676)  classification_loss: 1.5816 (1.5459)  loss_mask: 0.0209 (0.0217)  time: 0.1614  data: 0.0002  max mem: 4132
[19:42:30.869410] Epoch: [12]  [ 80/781]  eta: 0:01:58  lr: 0.000247  training_loss: 1.5602 (1.5673)  classification_loss: 1.5246 (1.5453)  loss_mask: 0.0211 (0.0220)  time: 0.1619  data: 0.0003  max mem: 4132
[19:42:34.116087] Epoch: [12]  [100/781]  eta: 0:01:53  lr: 0.000247  training_loss: 1.5388 (1.5666)  classification_loss: 1.5154 (1.5452)  loss_mask: 0.0187 (0.0214)  time: 0.1623  data: 0.0003  max mem: 4132
[19:42:37.384522] Epoch: [12]  [120/781]  eta: 0:01:50  lr: 0.000247  training_loss: 1.5192 (1.5612)  classification_loss: 1.4983 (1.5396)  loss_mask: 0.0219 (0.0216)  time: 0.1633  data: 0.0002  max mem: 4132
[19:42:40.622536] Epoch: [12]  [140/781]  eta: 0:01:46  lr: 0.000247  training_loss: 1.5664 (1.5596)  classification_loss: 1.5245 (1.5376)  loss_mask: 0.0216 (0.0220)  time: 0.1618  data: 0.0003  max mem: 4132
[19:42:43.868309] Epoch: [12]  [160/781]  eta: 0:01:42  lr: 0.000246  training_loss: 1.5364 (1.5581)  classification_loss: 1.5164 (1.5362)  loss_mask: 0.0201 (0.0219)  time: 0.1622  data: 0.0002  max mem: 4132
[19:42:47.097515] Epoch: [12]  [180/781]  eta: 0:01:39  lr: 0.000246  training_loss: 1.5415 (1.5582)  classification_loss: 1.5340 (1.5364)  loss_mask: 0.0187 (0.0218)  time: 0.1614  data: 0.0003  max mem: 4132
[19:42:50.331765] Epoch: [12]  [200/781]  eta: 0:01:35  lr: 0.000246  training_loss: 1.5714 (1.5594)  classification_loss: 1.5466 (1.5373)  loss_mask: 0.0239 (0.0222)  time: 0.1616  data: 0.0003  max mem: 4132
[19:42:53.587366] Epoch: [12]  [220/781]  eta: 0:01:32  lr: 0.000246  training_loss: 1.5699 (1.5594)  classification_loss: 1.5561 (1.5372)  loss_mask: 0.0226 (0.0222)  time: 0.1627  data: 0.0003  max mem: 4132
[19:42:56.821546] Epoch: [12]  [240/781]  eta: 0:01:28  lr: 0.000246  training_loss: 1.5356 (1.5580)  classification_loss: 1.5232 (1.5362)  loss_mask: 0.0167 (0.0218)  time: 0.1616  data: 0.0002  max mem: 4132
[19:43:00.101441] Epoch: [12]  [260/781]  eta: 0:01:25  lr: 0.000246  training_loss: 1.5683 (1.5580)  classification_loss: 1.5373 (1.5355)  loss_mask: 0.0270 (0.0225)  time: 0.1639  data: 0.0003  max mem: 4132
[19:43:03.368108] Epoch: [12]  [280/781]  eta: 0:01:22  lr: 0.000246  training_loss: 1.5804 (1.5588)  classification_loss: 1.5467 (1.5359)  loss_mask: 0.0260 (0.0229)  time: 0.1633  data: 0.0004  max mem: 4132
[19:43:06.607854] Epoch: [12]  [300/781]  eta: 0:01:18  lr: 0.000246  training_loss: 1.5724 (1.5602)  classification_loss: 1.5540 (1.5375)  loss_mask: 0.0195 (0.0228)  time: 0.1619  data: 0.0002  max mem: 4132
[19:43:09.850508] Epoch: [12]  [320/781]  eta: 0:01:15  lr: 0.000246  training_loss: 1.5091 (1.5592)  classification_loss: 1.4889 (1.5368)  loss_mask: 0.0170 (0.0225)  time: 0.1621  data: 0.0002  max mem: 4132
[19:43:13.099587] Epoch: [12]  [340/781]  eta: 0:01:12  lr: 0.000246  training_loss: 1.5320 (1.5578)  classification_loss: 1.5112 (1.5352)  loss_mask: 0.0192 (0.0225)  time: 0.1624  data: 0.0002  max mem: 4132
[19:43:16.373080] Epoch: [12]  [360/781]  eta: 0:01:08  lr: 0.000246  training_loss: 1.5529 (1.5576)  classification_loss: 1.5349 (1.5352)  loss_mask: 0.0183 (0.0223)  time: 0.1636  data: 0.0002  max mem: 4132
[19:43:19.593783] Epoch: [12]  [380/781]  eta: 0:01:05  lr: 0.000246  training_loss: 1.5331 (1.5583)  classification_loss: 1.5228 (1.5358)  loss_mask: 0.0254 (0.0225)  time: 0.1609  data: 0.0002  max mem: 4132
[19:43:22.826139] Epoch: [12]  [400/781]  eta: 0:01:02  lr: 0.000246  training_loss: 1.4854 (1.5570)  classification_loss: 1.4694 (1.5347)  loss_mask: 0.0195 (0.0223)  time: 0.1615  data: 0.0002  max mem: 4132
[19:43:26.058176] Epoch: [12]  [420/781]  eta: 0:00:59  lr: 0.000246  training_loss: 1.5599 (1.5572)  classification_loss: 1.5404 (1.5350)  loss_mask: 0.0184 (0.0222)  time: 0.1614  data: 0.0002  max mem: 4132
[19:43:29.309818] Epoch: [12]  [440/781]  eta: 0:00:55  lr: 0.000246  training_loss: 1.5503 (1.5573)  classification_loss: 1.5287 (1.5352)  loss_mask: 0.0198 (0.0221)  time: 0.1624  data: 0.0002  max mem: 4132
[19:43:32.545177] Epoch: [12]  [460/781]  eta: 0:00:52  lr: 0.000246  training_loss: 1.5009 (1.5552)  classification_loss: 1.4791 (1.5331)  loss_mask: 0.0203 (0.0221)  time: 0.1617  data: 0.0002  max mem: 4132
[19:43:35.779802] Epoch: [12]  [480/781]  eta: 0:00:49  lr: 0.000246  training_loss: 1.5328 (1.5547)  classification_loss: 1.5167 (1.5328)  loss_mask: 0.0173 (0.0220)  time: 0.1617  data: 0.0002  max mem: 4132
[19:43:39.055888] Epoch: [12]  [500/781]  eta: 0:00:45  lr: 0.000246  training_loss: 1.5076 (1.5532)  classification_loss: 1.4853 (1.5314)  loss_mask: 0.0169 (0.0218)  time: 0.1637  data: 0.0002  max mem: 4132
[19:43:42.296523] Epoch: [12]  [520/781]  eta: 0:00:42  lr: 0.000246  training_loss: 1.5440 (1.5527)  classification_loss: 1.5121 (1.5310)  loss_mask: 0.0180 (0.0218)  time: 0.1619  data: 0.0002  max mem: 4132
[19:43:45.531063] Epoch: [12]  [540/781]  eta: 0:00:39  lr: 0.000246  training_loss: 1.5161 (1.5521)  classification_loss: 1.4989 (1.5304)  loss_mask: 0.0207 (0.0217)  time: 0.1616  data: 0.0002  max mem: 4132
[19:43:48.759555] Epoch: [12]  [560/781]  eta: 0:00:36  lr: 0.000246  training_loss: 1.5742 (1.5526)  classification_loss: 1.5600 (1.5309)  loss_mask: 0.0174 (0.0217)  time: 0.1614  data: 0.0002  max mem: 4132
[19:43:51.981381] Epoch: [12]  [580/781]  eta: 0:00:32  lr: 0.000246  training_loss: 1.5155 (1.5514)  classification_loss: 1.4956 (1.5297)  loss_mask: 0.0215 (0.0217)  time: 0.1610  data: 0.0002  max mem: 4132
[19:43:55.234821] Epoch: [12]  [600/781]  eta: 0:00:29  lr: 0.000246  training_loss: 1.5110 (1.5506)  classification_loss: 1.4919 (1.5289)  loss_mask: 0.0189 (0.0217)  time: 0.1626  data: 0.0002  max mem: 4132
[19:43:58.471455] Epoch: [12]  [620/781]  eta: 0:00:26  lr: 0.000246  training_loss: 1.5134 (1.5492)  classification_loss: 1.4980 (1.5277)  loss_mask: 0.0162 (0.0215)  time: 0.1618  data: 0.0002  max mem: 4132
[19:44:01.726646] Epoch: [12]  [640/781]  eta: 0:00:22  lr: 0.000246  training_loss: 1.5571 (1.5487)  classification_loss: 1.5445 (1.5271)  loss_mask: 0.0237 (0.0216)  time: 0.1627  data: 0.0002  max mem: 4132
[19:44:04.963254] Epoch: [12]  [660/781]  eta: 0:00:19  lr: 0.000246  training_loss: 1.5443 (1.5484)  classification_loss: 1.5262 (1.5269)  loss_mask: 0.0185 (0.0216)  time: 0.1618  data: 0.0002  max mem: 4132
[19:44:08.211113] Epoch: [12]  [680/781]  eta: 0:00:16  lr: 0.000246  training_loss: 1.5545 (1.5491)  classification_loss: 1.5241 (1.5275)  loss_mask: 0.0198 (0.0216)  time: 0.1623  data: 0.0002  max mem: 4132
[19:44:11.454917] Epoch: [12]  [700/781]  eta: 0:00:13  lr: 0.000246  training_loss: 1.5460 (1.5488)  classification_loss: 1.5349 (1.5272)  loss_mask: 0.0182 (0.0215)  time: 0.1621  data: 0.0003  max mem: 4132
[19:44:14.671262] Epoch: [12]  [720/781]  eta: 0:00:09  lr: 0.000246  training_loss: 1.5416 (1.5496)  classification_loss: 1.5237 (1.5282)  loss_mask: 0.0163 (0.0214)  time: 0.1607  data: 0.0002  max mem: 4132
[19:44:17.946055] Epoch: [12]  [740/781]  eta: 0:00:06  lr: 0.000246  training_loss: 1.5288 (1.5491)  classification_loss: 1.5132 (1.5277)  loss_mask: 0.0189 (0.0214)  time: 0.1637  data: 0.0002  max mem: 4132
[19:44:21.176223] Epoch: [12]  [760/781]  eta: 0:00:03  lr: 0.000246  training_loss: 1.5254 (1.5491)  classification_loss: 1.5066 (1.5278)  loss_mask: 0.0188 (0.0213)  time: 0.1614  data: 0.0002  max mem: 4132
[19:44:24.382304] Epoch: [12]  [780/781]  eta: 0:00:00  lr: 0.000246  training_loss: 1.5503 (1.5492)  classification_loss: 1.5360 (1.5279)  loss_mask: 0.0174 (0.0213)  time: 0.1602  data: 0.0002  max mem: 4132
[19:44:24.544373] Epoch: [12] Total time: 0:02:07 (0.1631 s / it)
[19:44:24.544908] Averaged stats: lr: 0.000246  training_loss: 1.5503 (1.5492)  classification_loss: 1.5360 (1.5279)  loss_mask: 0.0174 (0.0213)
[19:44:25.282884] Test:  [  0/157]  eta: 0:01:55  testing_loss: 0.9722 (0.9722)  acc1: 64.0625 (64.0625)  acc5: 95.3125 (95.3125)  time: 0.7341  data: 0.6947  max mem: 4132
[19:44:25.569109] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.0364 (1.0706)  acc1: 64.0625 (64.4886)  acc5: 96.8750 (97.0170)  time: 0.0926  data: 0.0634  max mem: 4132
[19:44:25.856127] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.0364 (1.0455)  acc1: 65.6250 (65.5506)  acc5: 98.4375 (97.3958)  time: 0.0285  data: 0.0003  max mem: 4132
[19:44:26.141456] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.0459 (1.0486)  acc1: 65.6250 (65.6754)  acc5: 96.8750 (96.9254)  time: 0.0285  data: 0.0004  max mem: 4132
[19:44:26.441020] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.0552 (1.0503)  acc1: 65.6250 (65.3201)  acc5: 96.8750 (96.9131)  time: 0.0291  data: 0.0003  max mem: 4132
[19:44:26.725990] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.0336 (1.0448)  acc1: 65.6250 (65.9007)  acc5: 96.8750 (97.0282)  time: 0.0291  data: 0.0002  max mem: 4132
[19:44:27.011828] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.0141 (1.0408)  acc1: 65.6250 (65.8043)  acc5: 96.8750 (96.9775)  time: 0.0284  data: 0.0002  max mem: 4132
[19:44:27.300136] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.9922 (1.0345)  acc1: 65.6250 (65.7570)  acc5: 96.8750 (96.9190)  time: 0.0286  data: 0.0004  max mem: 4132
[19:44:27.587077] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.9922 (1.0351)  acc1: 67.1875 (65.6443)  acc5: 96.8750 (96.8750)  time: 0.0286  data: 0.0004  max mem: 4132
[19:44:27.868613] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.0558 (1.0386)  acc1: 64.0625 (65.4190)  acc5: 96.8750 (96.9265)  time: 0.0283  data: 0.0002  max mem: 4132
[19:44:28.161658] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.0720 (1.0442)  acc1: 62.5000 (65.1764)  acc5: 96.8750 (96.9524)  time: 0.0286  data: 0.0002  max mem: 4132
[19:44:28.442748] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.0954 (1.0458)  acc1: 62.5000 (65.2309)  acc5: 96.8750 (96.9172)  time: 0.0286  data: 0.0002  max mem: 4132
[19:44:28.724330] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.0317 (1.0411)  acc1: 65.6250 (65.3538)  acc5: 96.8750 (96.9267)  time: 0.0280  data: 0.0002  max mem: 4132
[19:44:29.008015] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.0317 (1.0433)  acc1: 64.0625 (65.0406)  acc5: 96.8750 (96.9346)  time: 0.0281  data: 0.0002  max mem: 4132
[19:44:29.291061] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.0706 (1.0445)  acc1: 60.9375 (64.9601)  acc5: 96.8750 (96.9193)  time: 0.0282  data: 0.0001  max mem: 4132
[19:44:29.570690] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.0267 (1.0405)  acc1: 65.6250 (65.2111)  acc5: 96.8750 (96.9060)  time: 0.0279  data: 0.0001  max mem: 4132
[19:44:29.720981] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.0196 (1.0429)  acc1: 65.6250 (65.0900)  acc5: 96.8750 (96.9200)  time: 0.0269  data: 0.0001  max mem: 4132
[19:44:29.876448] Test: Total time: 0:00:05 (0.0339 s / it)
[19:44:29.876996] * Acc@1 65.090 Acc@5 96.920 loss 1.043
[19:44:29.877307] Accuracy of the network on the 10000 test images: 65.1%
[19:44:29.877487] Max accuracy: 65.09%
[19:44:30.214826] log_dir: ./output_dir
[19:44:31.101076] Epoch: [13]  [  0/781]  eta: 0:11:30  lr: 0.000246  training_loss: 1.4668 (1.4668)  classification_loss: 1.4484 (1.4484)  loss_mask: 0.0184 (0.0184)  time: 0.8844  data: 0.6958  max mem: 4132
[19:44:34.352269] Epoch: [13]  [ 20/781]  eta: 0:02:29  lr: 0.000246  training_loss: 1.5166 (1.5019)  classification_loss: 1.5009 (1.4798)  loss_mask: 0.0209 (0.0221)  time: 0.1625  data: 0.0002  max mem: 4132
[19:44:37.647040] Epoch: [13]  [ 40/781]  eta: 0:02:14  lr: 0.000246  training_loss: 1.5554 (1.5303)  classification_loss: 1.5364 (1.5088)  loss_mask: 0.0194 (0.0215)  time: 0.1647  data: 0.0002  max mem: 4132
[19:44:40.873157] Epoch: [13]  [ 60/781]  eta: 0:02:05  lr: 0.000246  training_loss: 1.5541 (1.5414)  classification_loss: 1.5403 (1.5207)  loss_mask: 0.0196 (0.0207)  time: 0.1612  data: 0.0003  max mem: 4132
[19:44:44.152325] Epoch: [13]  [ 80/781]  eta: 0:02:00  lr: 0.000246  training_loss: 1.5820 (1.5475)  classification_loss: 1.5637 (1.5274)  loss_mask: 0.0171 (0.0200)  time: 0.1639  data: 0.0003  max mem: 4132
[19:44:47.397860] Epoch: [13]  [100/781]  eta: 0:01:55  lr: 0.000246  training_loss: 1.5215 (1.5444)  classification_loss: 1.5006 (1.5252)  loss_mask: 0.0160 (0.0193)  time: 0.1622  data: 0.0002  max mem: 4132
[19:44:50.697063] Epoch: [13]  [120/781]  eta: 0:01:51  lr: 0.000246  training_loss: 1.5254 (1.5434)  classification_loss: 1.5216 (1.5243)  loss_mask: 0.0178 (0.0191)  time: 0.1649  data: 0.0002  max mem: 4132
[19:44:53.967320] Epoch: [13]  [140/781]  eta: 0:01:47  lr: 0.000245  training_loss: 1.4895 (1.5384)  classification_loss: 1.4653 (1.5196)  loss_mask: 0.0146 (0.0188)  time: 0.1634  data: 0.0002  max mem: 4132
[19:44:57.209439] Epoch: [13]  [160/781]  eta: 0:01:44  lr: 0.000245  training_loss: 1.5216 (1.5388)  classification_loss: 1.4976 (1.5187)  loss_mask: 0.0231 (0.0201)  time: 0.1620  data: 0.0002  max mem: 4132
[19:45:00.468732] Epoch: [13]  [180/781]  eta: 0:01:40  lr: 0.000245  training_loss: 1.4871 (1.5358)  classification_loss: 1.4443 (1.5146)  loss_mask: 0.0312 (0.0212)  time: 0.1629  data: 0.0002  max mem: 4132
[19:45:03.699930] Epoch: [13]  [200/781]  eta: 0:01:36  lr: 0.000245  training_loss: 1.5302 (1.5355)  classification_loss: 1.5141 (1.5147)  loss_mask: 0.0160 (0.0208)  time: 0.1615  data: 0.0002  max mem: 4132
[19:45:06.934381] Epoch: [13]  [220/781]  eta: 0:01:33  lr: 0.000245  training_loss: 1.5696 (1.5363)  classification_loss: 1.5530 (1.5159)  loss_mask: 0.0164 (0.0203)  time: 0.1616  data: 0.0003  max mem: 4132
[19:45:10.154089] Epoch: [13]  [240/781]  eta: 0:01:29  lr: 0.000245  training_loss: 1.5619 (1.5383)  classification_loss: 1.5479 (1.5183)  loss_mask: 0.0147 (0.0200)  time: 0.1609  data: 0.0002  max mem: 4132
[19:45:13.369093] Epoch: [13]  [260/781]  eta: 0:01:26  lr: 0.000245  training_loss: 1.5335 (1.5370)  classification_loss: 1.5178 (1.5173)  loss_mask: 0.0156 (0.0198)  time: 0.1607  data: 0.0002  max mem: 4132
[19:45:16.598286] Epoch: [13]  [280/781]  eta: 0:01:22  lr: 0.000245  training_loss: 1.5430 (1.5372)  classification_loss: 1.5296 (1.5177)  loss_mask: 0.0157 (0.0194)  time: 0.1614  data: 0.0005  max mem: 4132
[19:45:19.866455] Epoch: [13]  [300/781]  eta: 0:01:19  lr: 0.000245  training_loss: 1.4870 (1.5347)  classification_loss: 1.4696 (1.5156)  loss_mask: 0.0136 (0.0191)  time: 0.1633  data: 0.0003  max mem: 4132
[19:45:23.106269] Epoch: [13]  [320/781]  eta: 0:01:15  lr: 0.000245  training_loss: 1.5373 (1.5366)  classification_loss: 1.5230 (1.5173)  loss_mask: 0.0185 (0.0192)  time: 0.1619  data: 0.0002  max mem: 4132
[19:45:26.338315] Epoch: [13]  [340/781]  eta: 0:01:12  lr: 0.000245  training_loss: 1.5134 (1.5358)  classification_loss: 1.4942 (1.5166)  loss_mask: 0.0163 (0.0192)  time: 0.1615  data: 0.0002  max mem: 4132
[19:45:29.586577] Epoch: [13]  [360/781]  eta: 0:01:09  lr: 0.000245  training_loss: 1.5512 (1.5357)  classification_loss: 1.5313 (1.5166)  loss_mask: 0.0164 (0.0191)  time: 0.1623  data: 0.0002  max mem: 4132
[19:45:32.828513] Epoch: [13]  [380/781]  eta: 0:01:05  lr: 0.000245  training_loss: 1.5564 (1.5358)  classification_loss: 1.5421 (1.5170)  loss_mask: 0.0146 (0.0189)  time: 0.1620  data: 0.0002  max mem: 4132
[19:45:36.117402] Epoch: [13]  [400/781]  eta: 0:01:02  lr: 0.000245  training_loss: 1.5348 (1.5357)  classification_loss: 1.5099 (1.5166)  loss_mask: 0.0230 (0.0192)  time: 0.1644  data: 0.0003  max mem: 4132
[19:45:39.337342] Epoch: [13]  [420/781]  eta: 0:00:59  lr: 0.000245  training_loss: 1.5293 (1.5349)  classification_loss: 1.5108 (1.5156)  loss_mask: 0.0214 (0.0193)  time: 0.1609  data: 0.0002  max mem: 4132
[19:45:42.577939] Epoch: [13]  [440/781]  eta: 0:00:55  lr: 0.000245  training_loss: 1.5041 (1.5330)  classification_loss: 1.4793 (1.5136)  loss_mask: 0.0184 (0.0194)  time: 0.1620  data: 0.0003  max mem: 4132
[19:45:45.841045] Epoch: [13]  [460/781]  eta: 0:00:52  lr: 0.000245  training_loss: 1.5339 (1.5325)  classification_loss: 1.5174 (1.5132)  loss_mask: 0.0154 (0.0193)  time: 0.1631  data: 0.0002  max mem: 4132
[19:45:49.104055] Epoch: [13]  [480/781]  eta: 0:00:49  lr: 0.000245  training_loss: 1.5383 (1.5325)  classification_loss: 1.5242 (1.5134)  loss_mask: 0.0145 (0.0191)  time: 0.1631  data: 0.0002  max mem: 4132
[19:45:52.355876] Epoch: [13]  [500/781]  eta: 0:00:46  lr: 0.000245  training_loss: 1.5355 (1.5327)  classification_loss: 1.5023 (1.5136)  loss_mask: 0.0171 (0.0191)  time: 0.1625  data: 0.0002  max mem: 4132
[19:45:55.610112] Epoch: [13]  [520/781]  eta: 0:00:42  lr: 0.000245  training_loss: 1.5177 (1.5326)  classification_loss: 1.5042 (1.5135)  loss_mask: 0.0171 (0.0191)  time: 0.1626  data: 0.0003  max mem: 4132
[19:45:58.832193] Epoch: [13]  [540/781]  eta: 0:00:39  lr: 0.000245  training_loss: 1.4961 (1.5323)  classification_loss: 1.4855 (1.5134)  loss_mask: 0.0154 (0.0190)  time: 0.1610  data: 0.0002  max mem: 4132
[19:46:02.064793] Epoch: [13]  [560/781]  eta: 0:00:36  lr: 0.000245  training_loss: 1.4606 (1.5310)  classification_loss: 1.4415 (1.5120)  loss_mask: 0.0158 (0.0189)  time: 0.1615  data: 0.0004  max mem: 4132
[19:46:05.329584] Epoch: [13]  [580/781]  eta: 0:00:32  lr: 0.000245  training_loss: 1.5131 (1.5308)  classification_loss: 1.4985 (1.5120)  loss_mask: 0.0145 (0.0188)  time: 0.1632  data: 0.0002  max mem: 4132
[19:46:08.555838] Epoch: [13]  [600/781]  eta: 0:00:29  lr: 0.000245  training_loss: 1.4702 (1.5296)  classification_loss: 1.4526 (1.5109)  loss_mask: 0.0169 (0.0187)  time: 0.1612  data: 0.0002  max mem: 4132
[19:46:11.785705] Epoch: [13]  [620/781]  eta: 0:00:26  lr: 0.000245  training_loss: 1.5431 (1.5288)  classification_loss: 1.5256 (1.5101)  loss_mask: 0.0171 (0.0187)  time: 0.1614  data: 0.0002  max mem: 4132
[19:46:15.018484] Epoch: [13]  [640/781]  eta: 0:00:23  lr: 0.000245  training_loss: 1.4953 (1.5282)  classification_loss: 1.4776 (1.5094)  loss_mask: 0.0189 (0.0188)  time: 0.1616  data: 0.0002  max mem: 4132
[19:46:18.273717] Epoch: [13]  [660/781]  eta: 0:00:19  lr: 0.000245  training_loss: 1.5189 (1.5283)  classification_loss: 1.4910 (1.5095)  loss_mask: 0.0171 (0.0189)  time: 0.1627  data: 0.0005  max mem: 4132
[19:46:21.497292] Epoch: [13]  [680/781]  eta: 0:00:16  lr: 0.000245  training_loss: 1.5287 (1.5286)  classification_loss: 1.5132 (1.5098)  loss_mask: 0.0135 (0.0188)  time: 0.1610  data: 0.0003  max mem: 4132
[19:46:24.742099] Epoch: [13]  [700/781]  eta: 0:00:13  lr: 0.000245  training_loss: 1.5275 (1.5284)  classification_loss: 1.5023 (1.5097)  loss_mask: 0.0139 (0.0187)  time: 0.1621  data: 0.0002  max mem: 4132
[19:46:27.982801] Epoch: [13]  [720/781]  eta: 0:00:09  lr: 0.000245  training_loss: 1.4632 (1.5276)  classification_loss: 1.4507 (1.5090)  loss_mask: 0.0130 (0.0186)  time: 0.1619  data: 0.0003  max mem: 4132
[19:46:31.267964] Epoch: [13]  [740/781]  eta: 0:00:06  lr: 0.000245  training_loss: 1.4972 (1.5269)  classification_loss: 1.4850 (1.5084)  loss_mask: 0.0140 (0.0185)  time: 0.1642  data: 0.0002  max mem: 4132
[19:46:34.492501] Epoch: [13]  [760/781]  eta: 0:00:03  lr: 0.000245  training_loss: 1.5090 (1.5273)  classification_loss: 1.4982 (1.5089)  loss_mask: 0.0143 (0.0184)  time: 0.1612  data: 0.0006  max mem: 4132
[19:46:37.724873] Epoch: [13]  [780/781]  eta: 0:00:00  lr: 0.000245  training_loss: 1.5257 (1.5273)  classification_loss: 1.5105 (1.5090)  loss_mask: 0.0128 (0.0183)  time: 0.1615  data: 0.0003  max mem: 4132
[19:46:37.881968] Epoch: [13] Total time: 0:02:07 (0.1635 s / it)
[19:46:37.882408] Averaged stats: lr: 0.000245  training_loss: 1.5257 (1.5273)  classification_loss: 1.5105 (1.5090)  loss_mask: 0.0128 (0.0183)
[19:46:38.602506] Test:  [  0/157]  eta: 0:01:52  testing_loss: 0.9726 (0.9726)  acc1: 64.0625 (64.0625)  acc5: 93.7500 (93.7500)  time: 0.7155  data: 0.6863  max mem: 4132
[19:46:38.884750] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.0112 (1.0633)  acc1: 64.0625 (62.3580)  acc5: 96.8750 (96.5909)  time: 0.0904  data: 0.0625  max mem: 4132
[19:46:39.167754] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.0112 (1.0192)  acc1: 64.0625 (64.3601)  acc5: 98.4375 (97.4702)  time: 0.0280  data: 0.0001  max mem: 4132
[19:46:39.462176] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.0211 (1.0257)  acc1: 65.6250 (64.6673)  acc5: 96.8750 (96.8750)  time: 0.0287  data: 0.0001  max mem: 4132
[19:46:39.754286] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.0107 (1.0257)  acc1: 64.0625 (64.8247)  acc5: 95.3125 (96.7226)  time: 0.0292  data: 0.0004  max mem: 4132
[19:46:40.034124] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.0060 (1.0218)  acc1: 64.0625 (65.0123)  acc5: 96.8750 (96.6912)  time: 0.0284  data: 0.0004  max mem: 4132
[19:46:40.313869] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.0063 (1.0168)  acc1: 64.0625 (65.1895)  acc5: 96.8750 (96.7469)  time: 0.0279  data: 0.0001  max mem: 4132
[19:46:40.595703] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.9459 (1.0098)  acc1: 67.1875 (65.5150)  acc5: 96.8750 (96.8090)  time: 0.0280  data: 0.0002  max mem: 4132
[19:46:40.881275] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.9768 (1.0153)  acc1: 67.1875 (65.6057)  acc5: 96.8750 (96.7785)  time: 0.0283  data: 0.0002  max mem: 4132
[19:46:41.167116] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.0441 (1.0179)  acc1: 64.0625 (65.4533)  acc5: 96.8750 (96.7548)  time: 0.0285  data: 0.0002  max mem: 4132
[19:46:41.452041] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.0638 (1.0230)  acc1: 62.5000 (65.2537)  acc5: 98.4375 (96.7667)  time: 0.0284  data: 0.0002  max mem: 4132
[19:46:41.752510] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.0701 (1.0249)  acc1: 64.0625 (65.2872)  acc5: 96.8750 (96.7765)  time: 0.0291  data: 0.0002  max mem: 4132
[19:46:42.036020] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.0099 (1.0216)  acc1: 64.0625 (65.1989)  acc5: 96.8750 (96.7588)  time: 0.0291  data: 0.0002  max mem: 4132
[19:46:42.319620] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.0303 (1.0258)  acc1: 62.5000 (64.9571)  acc5: 96.8750 (96.7438)  time: 0.0282  data: 0.0002  max mem: 4132
[19:46:42.600104] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.0478 (1.0251)  acc1: 62.5000 (65.1374)  acc5: 96.8750 (96.6866)  time: 0.0281  data: 0.0002  max mem: 4132
[19:46:42.878876] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.9931 (1.0217)  acc1: 67.1875 (65.3042)  acc5: 96.8750 (96.6991)  time: 0.0278  data: 0.0001  max mem: 4132
[19:46:43.029642] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.9931 (1.0238)  acc1: 65.6250 (65.1700)  acc5: 98.4375 (96.7400)  time: 0.0269  data: 0.0001  max mem: 4132
[19:46:43.200164] Test: Total time: 0:00:05 (0.0339 s / it)
[19:46:43.200705] * Acc@1 65.170 Acc@5 96.740 loss 1.024
[19:46:43.201073] Accuracy of the network on the 10000 test images: 65.2%
[19:46:43.201319] Max accuracy: 65.17%
[19:46:43.582762] log_dir: ./output_dir
[19:46:44.480871] Epoch: [14]  [  0/781]  eta: 0:11:39  lr: 0.000245  training_loss: 1.4066 (1.4066)  classification_loss: 1.3981 (1.3981)  loss_mask: 0.0085 (0.0085)  time: 0.8963  data: 0.7006  max mem: 4132
[19:46:47.716495] Epoch: [14]  [ 20/781]  eta: 0:02:29  lr: 0.000244  training_loss: 1.4586 (1.4745)  classification_loss: 1.4428 (1.4608)  loss_mask: 0.0127 (0.0138)  time: 0.1617  data: 0.0002  max mem: 4132
[19:46:50.991371] Epoch: [14]  [ 40/781]  eta: 0:02:13  lr: 0.000244  training_loss: 1.5131 (1.4991)  classification_loss: 1.4944 (1.4852)  loss_mask: 0.0117 (0.0139)  time: 0.1637  data: 0.0003  max mem: 4132
[19:46:54.214704] Epoch: [14]  [ 60/781]  eta: 0:02:05  lr: 0.000244  training_loss: 1.5118 (1.5053)  classification_loss: 1.5009 (1.4912)  loss_mask: 0.0124 (0.0141)  time: 0.1611  data: 0.0002  max mem: 4132
[19:46:57.452207] Epoch: [14]  [ 80/781]  eta: 0:01:59  lr: 0.000244  training_loss: 1.5341 (1.5136)  classification_loss: 1.5187 (1.4982)  loss_mask: 0.0171 (0.0154)  time: 0.1618  data: 0.0002  max mem: 4132
[19:47:00.685604] Epoch: [14]  [100/781]  eta: 0:01:55  lr: 0.000244  training_loss: 1.5313 (1.5223)  classification_loss: 1.5124 (1.5053)  loss_mask: 0.0189 (0.0170)  time: 0.1616  data: 0.0003  max mem: 4132
[19:47:03.938424] Epoch: [14]  [120/781]  eta: 0:01:51  lr: 0.000244  training_loss: 1.5075 (1.5201)  classification_loss: 1.4860 (1.5020)  loss_mask: 0.0233 (0.0180)  time: 0.1626  data: 0.0003  max mem: 4132
[19:47:07.164932] Epoch: [14]  [140/781]  eta: 0:01:47  lr: 0.000244  training_loss: 1.4861 (1.5141)  classification_loss: 1.4659 (1.4966)  loss_mask: 0.0130 (0.0175)  time: 0.1613  data: 0.0003  max mem: 4132
[19:47:10.403863] Epoch: [14]  [160/781]  eta: 0:01:43  lr: 0.000244  training_loss: 1.4692 (1.5116)  classification_loss: 1.4538 (1.4945)  loss_mask: 0.0127 (0.0171)  time: 0.1619  data: 0.0002  max mem: 4132
[19:47:13.675449] Epoch: [14]  [180/781]  eta: 0:01:39  lr: 0.000244  training_loss: 1.4863 (1.5103)  classification_loss: 1.4772 (1.4936)  loss_mask: 0.0121 (0.0167)  time: 0.1634  data: 0.0002  max mem: 4132
[19:47:16.916261] Epoch: [14]  [200/781]  eta: 0:01:36  lr: 0.000244  training_loss: 1.4864 (1.5096)  classification_loss: 1.4678 (1.4932)  loss_mask: 0.0143 (0.0164)  time: 0.1619  data: 0.0003  max mem: 4132
[19:47:20.146912] Epoch: [14]  [220/781]  eta: 0:01:32  lr: 0.000244  training_loss: 1.5397 (1.5100)  classification_loss: 1.5152 (1.4936)  loss_mask: 0.0159 (0.0165)  time: 0.1615  data: 0.0002  max mem: 4132
[19:47:23.439148] Epoch: [14]  [240/781]  eta: 0:01:29  lr: 0.000244  training_loss: 1.5108 (1.5114)  classification_loss: 1.4968 (1.4945)  loss_mask: 0.0192 (0.0169)  time: 0.1645  data: 0.0002  max mem: 4132
[19:47:26.680457] Epoch: [14]  [260/781]  eta: 0:01:25  lr: 0.000244  training_loss: 1.5422 (1.5120)  classification_loss: 1.5251 (1.4949)  loss_mask: 0.0171 (0.0171)  time: 0.1620  data: 0.0002  max mem: 4132
[19:47:29.934391] Epoch: [14]  [280/781]  eta: 0:01:22  lr: 0.000244  training_loss: 1.5287 (1.5127)  classification_loss: 1.5103 (1.4955)  loss_mask: 0.0173 (0.0172)  time: 0.1626  data: 0.0002  max mem: 4132
[19:47:33.229740] Epoch: [14]  [300/781]  eta: 0:01:19  lr: 0.000244  training_loss: 1.4951 (1.5117)  classification_loss: 1.4739 (1.4946)  loss_mask: 0.0159 (0.0171)  time: 0.1647  data: 0.0002  max mem: 4132
[19:47:36.463037] Epoch: [14]  [320/781]  eta: 0:01:15  lr: 0.000244  training_loss: 1.4754 (1.5110)  classification_loss: 1.4647 (1.4940)  loss_mask: 0.0148 (0.0170)  time: 0.1616  data: 0.0002  max mem: 4132
[19:47:39.689533] Epoch: [14]  [340/781]  eta: 0:01:12  lr: 0.000244  training_loss: 1.5282 (1.5114)  classification_loss: 1.5159 (1.4946)  loss_mask: 0.0137 (0.0168)  time: 0.1612  data: 0.0002  max mem: 4132
[19:47:42.936978] Epoch: [14]  [360/781]  eta: 0:01:09  lr: 0.000244  training_loss: 1.5037 (1.5120)  classification_loss: 1.4927 (1.4954)  loss_mask: 0.0125 (0.0166)  time: 0.1623  data: 0.0005  max mem: 4132
[19:47:46.165296] Epoch: [14]  [380/781]  eta: 0:01:05  lr: 0.000244  training_loss: 1.5340 (1.5125)  classification_loss: 1.5253 (1.4958)  loss_mask: 0.0175 (0.0167)  time: 0.1613  data: 0.0002  max mem: 4132
[19:47:49.431657] Epoch: [14]  [400/781]  eta: 0:01:02  lr: 0.000244  training_loss: 1.4730 (1.5123)  classification_loss: 1.4608 (1.4957)  loss_mask: 0.0138 (0.0166)  time: 0.1632  data: 0.0005  max mem: 4132
[19:47:52.673992] Epoch: [14]  [420/781]  eta: 0:00:59  lr: 0.000244  training_loss: 1.4833 (1.5119)  classification_loss: 1.4759 (1.4953)  loss_mask: 0.0126 (0.0166)  time: 0.1620  data: 0.0002  max mem: 4132
[19:47:55.919271] Epoch: [14]  [440/781]  eta: 0:00:55  lr: 0.000244  training_loss: 1.4524 (1.5105)  classification_loss: 1.4378 (1.4941)  loss_mask: 0.0117 (0.0163)  time: 0.1622  data: 0.0002  max mem: 4132
[19:47:59.195849] Epoch: [14]  [460/781]  eta: 0:00:52  lr: 0.000244  training_loss: 1.4903 (1.5098)  classification_loss: 1.4772 (1.4937)  loss_mask: 0.0105 (0.0162)  time: 0.1637  data: 0.0002  max mem: 4132
[19:48:02.439018] Epoch: [14]  [480/781]  eta: 0:00:49  lr: 0.000244  training_loss: 1.5020 (1.5097)  classification_loss: 1.4835 (1.4936)  loss_mask: 0.0133 (0.0162)  time: 0.1621  data: 0.0002  max mem: 4132
[19:48:05.679213] Epoch: [14]  [500/781]  eta: 0:00:46  lr: 0.000244  training_loss: 1.4585 (1.5096)  classification_loss: 1.4439 (1.4934)  loss_mask: 0.0157 (0.0162)  time: 0.1619  data: 0.0002  max mem: 4132
[19:48:08.928846] Epoch: [14]  [520/781]  eta: 0:00:42  lr: 0.000244  training_loss: 1.4817 (1.5093)  classification_loss: 1.4559 (1.4928)  loss_mask: 0.0206 (0.0165)  time: 0.1624  data: 0.0002  max mem: 4132
[19:48:12.182568] Epoch: [14]  [540/781]  eta: 0:00:39  lr: 0.000244  training_loss: 1.5337 (1.5108)  classification_loss: 1.5047 (1.4941)  loss_mask: 0.0209 (0.0167)  time: 0.1626  data: 0.0002  max mem: 4132
[19:48:15.453098] Epoch: [14]  [560/781]  eta: 0:00:36  lr: 0.000244  training_loss: 1.4623 (1.5097)  classification_loss: 1.4465 (1.4930)  loss_mask: 0.0139 (0.0167)  time: 0.1634  data: 0.0002  max mem: 4132
[19:48:18.713086] Epoch: [14]  [580/781]  eta: 0:00:32  lr: 0.000244  training_loss: 1.5346 (1.5104)  classification_loss: 1.5138 (1.4936)  loss_mask: 0.0169 (0.0168)  time: 0.1629  data: 0.0002  max mem: 4132
[19:48:21.955377] Epoch: [14]  [600/781]  eta: 0:00:29  lr: 0.000244  training_loss: 1.4660 (1.5092)  classification_loss: 1.4555 (1.4924)  loss_mask: 0.0141 (0.0167)  time: 0.1620  data: 0.0003  max mem: 4132
[19:48:25.188837] Epoch: [14]  [620/781]  eta: 0:00:26  lr: 0.000244  training_loss: 1.4860 (1.5093)  classification_loss: 1.4742 (1.4926)  loss_mask: 0.0133 (0.0166)  time: 0.1616  data: 0.0003  max mem: 4132
[19:48:28.442784] Epoch: [14]  [640/781]  eta: 0:00:23  lr: 0.000243  training_loss: 1.4680 (1.5082)  classification_loss: 1.4481 (1.4917)  loss_mask: 0.0107 (0.0165)  time: 0.1626  data: 0.0002  max mem: 4132
[19:48:31.691195] Epoch: [14]  [660/781]  eta: 0:00:19  lr: 0.000243  training_loss: 1.5192 (1.5088)  classification_loss: 1.4968 (1.4923)  loss_mask: 0.0143 (0.0165)  time: 0.1623  data: 0.0002  max mem: 4132
[19:48:34.992508] Epoch: [14]  [680/781]  eta: 0:00:16  lr: 0.000243  training_loss: 1.4944 (1.5086)  classification_loss: 1.4799 (1.4922)  loss_mask: 0.0125 (0.0164)  time: 0.1650  data: 0.0002  max mem: 4132
[19:48:36.490140] [19:48:36.490836] [19:48:36.490997] [19:48:36.491147] [19:48:36.491294] [19:48:36.491442] [19:48:36.491637]
Traceback (most recent call last):
  File "/notebooks/CVPR2023/main_two_branch_new.py", line 370, in <module>
    main(args)
  File "/notebooks/CVPR2023/main_two_branch_new.py", line 322, in main
    train_stats = train_one_epoch(
  File "/notebooks/CVPR2023/engine_two_branch.py", line 55, in train_one_epoch
    outputs , loss_mask= model(samples , args.mask_ratio)
  File "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/notebooks/CVPR2023/model_mae_image_loss.py", line 326, in forward
    ground_truth_label = ground_truth_label.to(mask_output.device)
KeyboardInterrupt