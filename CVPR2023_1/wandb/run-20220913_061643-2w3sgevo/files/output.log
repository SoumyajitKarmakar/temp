Not using distributed mode
[06:16:43.640046] job dir: /notebooks/CVPR2023
[06:16:43.640644] Namespace(batch_size=64,
epochs=100,
accum_iter=1,
model='mae_vit_tiny',
norm_pix_loss=False,
dataset='c10',
input_size=32,
patch_size=2,
mask_ratio=0.75,
lambda_weight=0.1,
drop_path=0.1,
clip_grad=None,
weight_decay=0.05,
lr=None,
blr=0.001,
layer_decay=0.75,
min_lr=1e-06,
warmup_epochs=5,
color_jitter=None,
aa='rand-m9-mstd0.5-inc1',
smoothing=0.1,
reprob=0.25,
remode='pixel',
recount=1,
resplit=False,
mixup=0,
cutmix=0,
cutmix_minmax=None,
mixup_prob=1.0,
mixup_switch_prob=0.5,
mixup_mode='batch',
finetune='',
global_pool=True,
data_path='/datasets01/imagenet_full_size/061417/',
nb_classes=10,
output_dir='./output_dir',
log_dir='./output_dir',
device='cuda',
seed=0,
resume='',
start_epoch=0,
eval=False,
dist_eval=False,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[06:16:44.017799] Files already downloaded and verified
[06:16:44.819459] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCropAndInterpolation(size=(32, 32), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic)
               RandomHorizontalFlip(p=0.5)
               RandAugment(n=2, ops=
           	AugmentOp(name=AutoContrast, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Equalize, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Invert, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Rotate, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=PosterizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeAdd, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ColorIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ContrastIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=BrightnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SharpnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearX, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearY, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateXRel, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateYRel, p=0.5, m=9, mstd=0.5))
               ToTensor()
               Normalize(mean=tensor([0.4914, 0.4822, 0.4465]), std=tensor([0.2023, 0.1994, 0.2010]))
               RandomErasing(p=0.25, mode=pixel, count=(1, 1))
           )
[06:16:45.197068] Files already downloaded and verified
/usr/local/lib/python3.9/dist-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
[06:16:45.627358] Dataset CIFAR10
    Number of datapoints: 10000
    Root location: ./data
    Split: Test
    StandardTransform
Transform: Compose(
               Resize(size=36, interpolation=bicubic, max_size=None, antialias=None)
               CenterCrop(size=(32, 32))
               ToTensor()
               Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.201))
           )
[06:16:45.628033] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f9691cea0d0>
/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[06:16:50.880339] Model = MaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(2, 2), stride=(2, 2))
    (norm): Identity()
  )
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (7): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (8): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (9): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (10): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (11): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
  (decoder_embed): Linear(in_features=192, out_features=128, bias=True)
  (decoder_blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=128, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=128, out_features=128, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=512, out_features=128, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=128, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=128, out_features=128, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=512, out_features=128, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (decoder_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
  (decoder_pred): Linear(in_features=128, out_features=12, bias=True)
  (head): Linear(in_features=192, out_features=10, bias=True)
  (classifier_mask): Sequential(
    (0): Linear(in_features=192, out_features=5, bias=True)
    (1): LogSoftmax(dim=1)
  )
)
[06:16:50.883825] number of params (M): 5.77
[06:16:50.884477] base lr: 1.00e-03
[06:16:50.885191] actual lr: 2.50e-04
[06:16:50.885450] accumulate grad iterations: 1
[06:16:50.885765] effective batch size: 64
[06:16:50.888002] criterion = LabelSmoothingCrossEntropy()
[06:16:50.888393] Start training for 100 epochs
[06:16:50.890721] log_dir: ./output_dir
[06:16:53.609382] Epoch: [0]  [  0/781]  eta: 0:35:21  lr: 0.000000  training_loss: 8.4337 (8.4337)  mae_loss: 3.8136 (3.8136)  classification_loss: 2.6555 (2.6555)  loss_mask: 1.9646 (1.9646)  time: 2.7168  data: 0.4532  max mem: 5446
[06:16:57.520109] Epoch: [0]  [ 20/781]  eta: 0:04:00  lr: 0.000001  training_loss: 8.4340 (8.4275)  mae_loss: 3.8678 (3.8710)  classification_loss: 2.6035 (2.6031)  loss_mask: 1.9500 (1.9534)  time: 0.1954  data: 0.0003  max mem: 5508
[06:17:01.488432] Epoch: [0]  [ 40/781]  eta: 0:03:11  lr: 0.000003  training_loss: 7.5238 (8.0337)  mae_loss: 3.1372 (3.5506)  classification_loss: 2.5074 (2.5580)  loss_mask: 1.8792 (1.9251)  time: 0.1983  data: 0.0002  max mem: 5508
[06:17:05.417285] Epoch: [0]  [ 60/781]  eta: 0:02:51  lr: 0.000004  training_loss: 6.7076 (7.5790)  mae_loss: 2.4835 (3.1973)  classification_loss: 2.4128 (2.5095)  loss_mask: 1.7817 (1.8721)  time: 0.1964  data: 0.0002  max mem: 5508
[06:17:09.391811] Epoch: [0]  [ 80/781]  eta: 0:02:40  lr: 0.000005  training_loss: 6.1073 (7.2155)  mae_loss: 2.0285 (2.9083)  classification_loss: 2.3435 (2.4690)  loss_mask: 1.7551 (1.8382)  time: 0.1986  data: 0.0002  max mem: 5508

[06:17:13.332443] Epoch: [0]  [100/781]  eta: 0:02:31  lr: 0.000006  training_loss: 5.7477 (6.9265)  mae_loss: 1.7172 (2.6801)  classification_loss: 2.3097 (2.4374)  loss_mask: 1.7043 (1.8090)  time: 0.1969  data: 0.0003  max mem: 5508
[06:17:17.268739] Epoch: [0]  [120/781]  eta: 0:02:24  lr: 0.000008  training_loss: 5.6100 (6.7126)  mae_loss: 1.6882 (2.5167)  classification_loss: 2.2938 (2.4143)  loss_mask: 1.6510 (1.7816)  time: 0.1967  data: 0.0003  max mem: 5508
[06:17:21.238035] Epoch: [0]  [140/781]  eta: 0:02:17  lr: 0.000009  training_loss: 5.4625 (6.5368)  mae_loss: 1.5344 (2.3798)  classification_loss: 2.2881 (2.3950)  loss_mask: 1.6305 (1.7620)  time: 0.1983  data: 0.0003  max mem: 5508
[06:17:25.183246] Epoch: [0]  [160/781]  eta: 0:02:12  lr: 0.000010  training_loss: 5.3307 (6.3931)  mae_loss: 1.4594 (2.2697)  classification_loss: 2.2731 (2.3797)  loss_mask: 1.6210 (1.7438)  time: 0.1972  data: 0.0002  max mem: 5508
[06:17:29.132488] Epoch: [0]  [180/781]  eta: 0:02:06  lr: 0.000012  training_loss: 5.3575 (6.2819)  mae_loss: 1.5039 (2.1842)  classification_loss: 2.2800 (2.3679)  loss_mask: 1.6096 (1.7298)  time: 0.1974  data: 0.0003  max mem: 5508
[06:17:33.082286] Epoch: [0]  [200/781]  eta: 0:02:01  lr: 0.000013  training_loss: 5.3090 (6.1847)  mae_loss: 1.4181 (2.1103)  classification_loss: 2.2456 (2.3564)  loss_mask: 1.6091 (1.7179)  time: 0.1974  data: 0.0002  max mem: 5508
[06:17:37.069443] Epoch: [0]  [220/781]  eta: 0:01:57  lr: 0.000014  training_loss: 5.2900 (6.1027)  mae_loss: 1.4226 (2.0485)  classification_loss: 2.2512 (2.3476)  loss_mask: 1.5966 (1.7066)  time: 0.1993  data: 0.0002  max mem: 5508
[06:17:41.040563] Epoch: [0]  [240/781]  eta: 0:01:52  lr: 0.000015  training_loss: 5.2827 (6.0371)  mae_loss: 1.4153 (1.9983)  classification_loss: 2.2645 (2.3413)  loss_mask: 1.6018 (1.6975)  time: 0.1984  data: 0.0003  max mem: 5508
[06:17:45.001722] Epoch: [0]  [260/781]  eta: 0:01:47  lr: 0.000017  training_loss: 5.1968 (5.9741)  mae_loss: 1.3829 (1.9510)  classification_loss: 2.2405 (2.3346)  loss_mask: 1.5874 (1.6886)  time: 0.1980  data: 0.0003  max mem: 5508
[06:17:49.003672] Epoch: [0]  [280/781]  eta: 0:01:43  lr: 0.000018  training_loss: 5.2160 (5.9215)  mae_loss: 1.4075 (1.9118)  classification_loss: 2.2521 (2.3292)  loss_mask: 1.5827 (1.6805)  time: 0.2000  data: 0.0002  max mem: 5508
[06:17:52.970488] Epoch: [0]  [300/781]  eta: 0:01:39  lr: 0.000019  training_loss: 5.1882 (5.8737)  mae_loss: 1.3256 (1.8750)  classification_loss: 2.2560 (2.3248)  loss_mask: 1.5786 (1.6739)  time: 0.1983  data: 0.0002  max mem: 5508
[06:17:56.918608] Epoch: [0]  [320/781]  eta: 0:01:34  lr: 0.000020  training_loss: 5.1717 (5.8295)  mae_loss: 1.3016 (1.8418)  classification_loss: 2.2502 (2.3201)  loss_mask: 1.5733 (1.6675)  time: 0.1973  data: 0.0002  max mem: 5508
[06:18:00.882597] Epoch: [0]  [340/781]  eta: 0:01:30  lr: 0.000022  training_loss: 5.1005 (5.7889)  mae_loss: 1.2961 (1.8107)  classification_loss: 2.2488 (2.3157)  loss_mask: 1.5766 (1.6625)  time: 0.1981  data: 0.0006  max mem: 5508
[06:18:04.866258] Epoch: [0]  [360/781]  eta: 0:01:26  lr: 0.000023  training_loss: 5.0724 (5.7489)  mae_loss: 1.2116 (1.7801)  classification_loss: 2.2544 (2.3117)  loss_mask: 1.5674 (1.6571)  time: 0.1991  data: 0.0003  max mem: 5508
[06:18:08.837511] Epoch: [0]  [380/781]  eta: 0:01:21  lr: 0.000024  training_loss: 5.0754 (5.7137)  mae_loss: 1.2188 (1.7526)  classification_loss: 2.2457 (2.3083)  loss_mask: 1.5810 (1.6528)  time: 0.1985  data: 0.0002  max mem: 5508
[06:18:12.815573] Epoch: [0]  [400/781]  eta: 0:01:17  lr: 0.000026  training_loss: 5.0133 (5.6804)  mae_loss: 1.2327 (1.7281)  classification_loss: 2.2270 (2.3044)  loss_mask: 1.5503 (1.6479)  time: 0.1988  data: 0.0002  max mem: 5508
[06:18:16.754682] Epoch: [0]  [420/781]  eta: 0:01:13  lr: 0.000027  training_loss: 5.0043 (5.6493)  mae_loss: 1.2091 (1.7043)  classification_loss: 2.2536 (2.3020)  loss_mask: 1.5485 (1.6431)  time: 0.1969  data: 0.0003  max mem: 5508
[06:18:20.784499] Epoch: [0]  [440/781]  eta: 0:01:09  lr: 0.000028  training_loss: 5.0563 (5.6224)  mae_loss: 1.2250 (1.6846)  classification_loss: 2.2367 (2.2988)  loss_mask: 1.5524 (1.6390)  time: 0.2014  data: 0.0002  max mem: 5508
[06:18:24.756233] Epoch: [0]  [460/781]  eta: 0:01:05  lr: 0.000029  training_loss: 4.9802 (5.5942)  mae_loss: 1.2355 (1.6651)  classification_loss: 2.1847 (2.2946)  loss_mask: 1.5258 (1.6346)  time: 0.1985  data: 0.0002  max mem: 5508
[06:18:28.707511] Epoch: [0]  [480/781]  eta: 0:01:01  lr: 0.000031  training_loss: 4.9663 (5.5691)  mae_loss: 1.1981 (1.6456)  classification_loss: 2.2279 (2.2929)  loss_mask: 1.5395 (1.6306)  time: 0.1975  data: 0.0002  max mem: 5508
[06:18:32.666995] Epoch: [0]  [500/781]  eta: 0:00:57  lr: 0.000032  training_loss: 4.9244 (5.5440)  mae_loss: 1.1637 (1.6268)  classification_loss: 2.2217 (2.2903)  loss_mask: 1.5386 (1.6270)  time: 0.1979  data: 0.0002  max mem: 5508
[06:18:36.608283] Epoch: [0]  [520/781]  eta: 0:00:52  lr: 0.000033  training_loss: 4.9569 (5.5214)  mae_loss: 1.2035 (1.6120)  classification_loss: 2.2276 (2.2877)  loss_mask: 1.4903 (1.6217)  time: 0.1969  data: 0.0002  max mem: 5508
[06:18:40.573232] Epoch: [0]  [540/781]  eta: 0:00:48  lr: 0.000035  training_loss: 4.9181 (5.4986)  mae_loss: 1.1646 (1.5957)  classification_loss: 2.2231 (2.2860)  loss_mask: 1.4911 (1.6169)  time: 0.1981  data: 0.0003  max mem: 5508
[06:18:44.532335] Epoch: [0]  [560/781]  eta: 0:00:44  lr: 0.000036  training_loss: 4.8816 (5.4771)  mae_loss: 1.1835 (1.5817)  classification_loss: 2.2204 (2.2844)  loss_mask: 1.4492 (1.6110)  time: 0.1979  data: 0.0002  max mem: 5508
[06:18:48.501146] Epoch: [0]  [580/781]  eta: 0:00:40  lr: 0.000037  training_loss: 4.8583 (5.4550)  mae_loss: 1.1640 (1.5673)  classification_loss: 2.2160 (2.2822)  loss_mask: 1.4324 (1.6055)  time: 0.1983  data: 0.0003  max mem: 5508
[06:18:52.431097] Epoch: [0]  [600/781]  eta: 0:00:36  lr: 0.000038  training_loss: 4.7926 (5.4335)  mae_loss: 1.1349 (1.5541)  classification_loss: 2.2169 (2.2801)  loss_mask: 1.4120 (1.5993)  time: 0.1964  data: 0.0002  max mem: 5508
[06:18:56.440549] Epoch: [0]  [620/781]  eta: 0:00:32  lr: 0.000040  training_loss: 4.7853 (5.4131)  mae_loss: 1.2002 (1.5422)  classification_loss: 2.2297 (2.2786)  loss_mask: 1.3625 (1.5923)  time: 0.2004  data: 0.0002  max mem: 5508
[06:19:00.391848] Epoch: [0]  [640/781]  eta: 0:00:28  lr: 0.000041  training_loss: 4.7080 (5.3916)  mae_loss: 1.1421 (1.5299)  classification_loss: 2.2208 (2.2774)  loss_mask: 1.3267 (1.5843)  time: 0.1974  data: 0.0002  max mem: 5508
[06:19:04.347315] Epoch: [0]  [660/781]  eta: 0:00:24  lr: 0.000042  training_loss: 4.7363 (5.3719)  mae_loss: 1.1857 (1.5196)  classification_loss: 2.2223 (2.2761)  loss_mask: 1.2937 (1.5762)  time: 0.1977  data: 0.0002  max mem: 5508
[06:19:08.357979] Epoch: [0]  [680/781]  eta: 0:00:20  lr: 0.000044  training_loss: 4.6443 (5.3502)  mae_loss: 1.1631 (1.5093)  classification_loss: 2.2033 (2.2740)  loss_mask: 1.2560 (1.5669)  time: 0.2005  data: 0.0003  max mem: 5508
[06:19:12.312165] Epoch: [0]  [700/781]  eta: 0:00:16  lr: 0.000045  training_loss: 4.5997 (5.3285)  mae_loss: 1.0994 (1.4980)  classification_loss: 2.2086 (2.2722)  loss_mask: 1.2692 (1.5583)  time: 0.1976  data: 0.0002  max mem: 5508
[06:19:16.272560] Epoch: [0]  [720/781]  eta: 0:00:12  lr: 0.000046  training_loss: 4.5326 (5.3070)  mae_loss: 1.1695 (1.4883)  classification_loss: 2.2430 (2.2713)  loss_mask: 1.1339 (1.5474)  time: 0.1979  data: 0.0004  max mem: 5508
[06:19:20.210627] Epoch: [0]  [740/781]  eta: 0:00:08  lr: 0.000047  training_loss: 4.4990 (5.2858)  mae_loss: 1.1072 (1.4786)  classification_loss: 2.2344 (2.2701)  loss_mask: 1.1694 (1.5371)  time: 0.1968  data: 0.0002  max mem: 5508
[06:19:24.163962] Epoch: [0]  [760/781]  eta: 0:00:04  lr: 0.000049  training_loss: 4.4813 (5.2643)  mae_loss: 1.0960 (1.4687)  classification_loss: 2.2149 (2.2690)  loss_mask: 1.1156 (1.5265)  time: 0.1976  data: 0.0003  max mem: 5508
[06:19:28.098518] Epoch: [0]  [780/781]  eta: 0:00:00  lr: 0.000050  training_loss: 4.4511 (5.2434)  mae_loss: 1.1375 (1.4603)  classification_loss: 2.2128 (2.2678)  loss_mask: 1.0786 (1.5153)  time: 0.1966  data: 0.0002  max mem: 5508
[06:19:28.225008] Epoch: [0] Total time: 0:02:37 (0.2015 s / it)
[06:19:28.225511] Averaged stats: lr: 0.000050  training_loss: 4.4511 (5.2434)  mae_loss: 1.1375 (1.4603)  classification_loss: 2.2128 (2.2678)  loss_mask: 1.0786 (1.5153)
[06:19:29.392301] Test:  [  0/157]  eta: 0:01:57  testing_loss: 1.9143 (1.9143)  acc1: 34.3750 (34.3750)  acc5: 92.1875 (92.1875)  time: 0.7495  data: 0.7093  max mem: 5508
[06:19:29.680711] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 2.0276 (2.0113)  acc1: 28.1250 (29.5455)  acc5: 82.8125 (83.2386)  time: 0.0942  data: 0.0648  max mem: 5508
[06:19:29.971718] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 2.0174 (2.0095)  acc1: 28.1250 (29.6875)  acc5: 81.2500 (82.2917)  time: 0.0288  data: 0.0004  max mem: 5508
[06:19:30.259015] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.9885 (2.0023)  acc1: 29.6875 (30.6956)  acc5: 81.2500 (81.9556)  time: 0.0287  data: 0.0003  max mem: 5508
[06:19:30.543403] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.9885 (2.0031)  acc1: 31.2500 (30.7546)  acc5: 81.2500 (81.7454)  time: 0.0284  data: 0.0002  max mem: 5508
[06:19:30.827919] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 2.0097 (2.0045)  acc1: 31.2500 (30.7292)  acc5: 81.2500 (81.9547)  time: 0.0283  data: 0.0002  max mem: 5508
[06:19:31.120247] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.9951 (2.0035)  acc1: 31.2500 (30.6865)  acc5: 84.3750 (82.2234)  time: 0.0287  data: 0.0003  max mem: 5508
[06:19:31.413339] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.9951 (2.0035)  acc1: 29.6875 (30.2817)  acc5: 84.3750 (82.3504)  time: 0.0291  data: 0.0004  max mem: 5508
[06:19:31.697029] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.9978 (2.0027)  acc1: 26.5625 (29.9961)  acc5: 84.3750 (82.7546)  time: 0.0287  data: 0.0002  max mem: 5508
[06:19:31.981088] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 2.0074 (2.0055)  acc1: 26.5625 (29.6360)  acc5: 84.3750 (82.7782)  time: 0.0283  data: 0.0002  max mem: 5508
[06:19:32.265504] Test:  [100/157]  eta: 0:00:02  testing_loss: 2.0232 (2.0065)  acc1: 28.1250 (29.6566)  acc5: 84.3750 (82.8125)  time: 0.0283  data: 0.0002  max mem: 5508
[06:19:32.549351] Test:  [110/157]  eta: 0:00:01  testing_loss: 2.0134 (2.0068)  acc1: 29.6875 (29.6593)  acc5: 84.3750 (82.8829)  time: 0.0283  data: 0.0002  max mem: 5508
[06:19:32.833419] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.9829 (2.0038)  acc1: 29.6875 (29.8683)  acc5: 84.3750 (83.1482)  time: 0.0283  data: 0.0002  max mem: 5508
[06:19:33.118039] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.9957 (2.0043)  acc1: 32.8125 (30.0692)  acc5: 84.3750 (83.0033)  time: 0.0283  data: 0.0002  max mem: 5508
[06:19:33.401781] Test:  [140/157]  eta: 0:00:00  testing_loss: 2.0060 (2.0035)  acc1: 31.2500 (30.0864)  acc5: 82.8125 (82.9787)  time: 0.0283  data: 0.0002  max mem: 5508
[06:19:33.683729] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.9856 (2.0022)  acc1: 29.6875 (30.2152)  acc5: 84.3750 (83.0815)  time: 0.0282  data: 0.0001  max mem: 5508
[06:19:33.973470] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.9828 (2.0013)  acc1: 28.1250 (30.0700)  acc5: 84.3750 (83.1600)  time: 0.0341  data: 0.0001  max mem: 5508
[06:19:34.154801] Test: Total time: 0:00:05 (0.0351 s / it)
[06:19:34.155273] * Acc@1 30.070 Acc@5 83.160 loss 2.001
[06:19:34.155579] Accuracy of the network on the 10000 test images: 30.1%
[06:19:34.155779] Max accuracy: 30.07%
[06:19:34.278969] log_dir: ./output_dir
[06:19:35.136323] Epoch: [1]  [  0/781]  eta: 0:11:08  lr: 0.000050  training_loss: 4.4021 (4.4021)  mae_loss: 1.1262 (1.1262)  classification_loss: 2.2028 (2.2028)  loss_mask: 1.0732 (1.0732)  time: 0.8558  data: 0.6224  max mem: 5511
[06:19:39.105493] Epoch: [1]  [ 20/781]  eta: 0:02:54  lr: 0.000051  training_loss: 4.4162 (4.4166)  mae_loss: 1.1499 (1.1662)  classification_loss: 2.1955 (2.2139)  loss_mask: 1.0273 (1.0365)  time: 0.1983  data: 0.0002  max mem: 5511
[06:19:43.063507] Epoch: [1]  [ 40/781]  eta: 0:02:38  lr: 0.000053  training_loss: 4.3687 (4.3888)  mae_loss: 1.0808 (1.1494)  classification_loss: 2.2177 (2.2243)  loss_mask: 0.9813 (1.0151)  time: 0.1978  data: 0.0002  max mem: 5511
[06:19:47.002808] Epoch: [1]  [ 60/781]  eta: 0:02:30  lr: 0.000054  training_loss: 4.3593 (4.3867)  mae_loss: 1.1346 (1.1385)  classification_loss: 2.2142 (2.2226)  loss_mask: 1.0066 (1.0255)  time: 0.1969  data: 0.0002  max mem: 5511
[06:19:50.950998] Epoch: [1]  [ 80/781]  eta: 0:02:24  lr: 0.000055  training_loss: 4.4440 (4.3970)  mae_loss: 1.1235 (1.1375)  classification_loss: 2.2135 (2.2234)  loss_mask: 1.0679 (1.0362)  time: 0.1973  data: 0.0002  max mem: 5511
[06:19:54.910711] Epoch: [1]  [100/781]  eta: 0:02:19  lr: 0.000056  training_loss: 4.3432 (4.3855)  mae_loss: 1.1230 (1.1350)  classification_loss: 2.2058 (2.2212)  loss_mask: 0.9899 (1.0293)  time: 0.1979  data: 0.0002  max mem: 5511
[06:19:58.882482] Epoch: [1]  [120/781]  eta: 0:02:14  lr: 0.000058  training_loss: 4.2534 (4.3665)  mae_loss: 1.0886 (1.1264)  classification_loss: 2.2307 (2.2200)  loss_mask: 0.9614 (1.0201)  time: 0.1985  data: 0.0002  max mem: 5511
[06:20:02.831645] Epoch: [1]  [140/781]  eta: 0:02:09  lr: 0.000059  training_loss: 4.1971 (4.3444)  mae_loss: 1.0447 (1.1188)  classification_loss: 2.1906 (2.2161)  loss_mask: 0.9276 (1.0095)  time: 0.1974  data: 0.0002  max mem: 5511
[06:20:06.763087] Epoch: [1]  [160/781]  eta: 0:02:05  lr: 0.000060  training_loss: 4.3020 (4.3473)  mae_loss: 1.1175 (1.1178)  classification_loss: 2.2153 (2.2149)  loss_mask: 0.9773 (1.0147)  time: 0.1964  data: 0.0002  max mem: 5511
[06:20:10.734328] Epoch: [1]  [180/781]  eta: 0:02:00  lr: 0.000062  training_loss: 4.2341 (4.3366)  mae_loss: 1.0948 (1.1171)  classification_loss: 2.1903 (2.2148)  loss_mask: 0.9028 (1.0047)  time: 0.1985  data: 0.0002  max mem: 5511
[06:20:14.677695] Epoch: [1]  [200/781]  eta: 0:01:56  lr: 0.000063  training_loss: 4.2311 (4.3251)  mae_loss: 1.1024 (1.1135)  classification_loss: 2.2126 (2.2138)  loss_mask: 0.9282 (0.9979)  time: 0.1971  data: 0.0002  max mem: 5511
[06:20:18.626373] Epoch: [1]  [220/781]  eta: 0:01:52  lr: 0.000064  training_loss: 4.1931 (4.3152)  mae_loss: 1.1159 (1.1135)  classification_loss: 2.1913 (2.2130)  loss_mask: 0.8850 (0.9887)  time: 0.1973  data: 0.0002  max mem: 5511
[06:20:22.588411] Epoch: [1]  [240/781]  eta: 0:01:48  lr: 0.000065  training_loss: 4.1914 (4.3054)  mae_loss: 1.1362 (1.1142)  classification_loss: 2.1951 (2.2124)  loss_mask: 0.8570 (0.9788)  time: 0.1980  data: 0.0002  max mem: 5511
[06:20:26.529397] Epoch: [1]  [260/781]  eta: 0:01:44  lr: 0.000067  training_loss: 4.1739 (4.2939)  mae_loss: 1.1143 (1.1133)  classification_loss: 2.2070 (2.2116)  loss_mask: 0.8220 (0.9689)  time: 0.1970  data: 0.0003  max mem: 5511
[06:20:30.479456] Epoch: [1]  [280/781]  eta: 0:01:40  lr: 0.000068  training_loss: 4.2035 (4.2890)  mae_loss: 1.1157 (1.1129)  classification_loss: 2.2318 (2.2125)  loss_mask: 0.8640 (0.9636)  time: 0.1974  data: 0.0003  max mem: 5511
[06:20:34.435107] Epoch: [1]  [300/781]  eta: 0:01:36  lr: 0.000069  training_loss: 4.1234 (4.2787)  mae_loss: 1.1091 (1.1123)  classification_loss: 2.1991 (2.2121)  loss_mask: 0.8240 (0.9543)  time: 0.1977  data: 0.0002  max mem: 5511
[06:20:38.387792] Epoch: [1]  [320/781]  eta: 0:01:32  lr: 0.000070  training_loss: 4.0325 (4.2652)  mae_loss: 1.0497 (1.1086)  classification_loss: 2.1916 (2.2113)  loss_mask: 0.8188 (0.9454)  time: 0.1975  data: 0.0002  max mem: 5511
[06:20:42.318331] Epoch: [1]  [340/781]  eta: 0:01:27  lr: 0.000072  training_loss: 4.0950 (4.2563)  mae_loss: 1.1187 (1.1086)  classification_loss: 2.2321 (2.2113)  loss_mask: 0.7912 (0.9364)  time: 0.1965  data: 0.0002  max mem: 5511
[06:20:46.251257] Epoch: [1]  [360/781]  eta: 0:01:23  lr: 0.000073  training_loss: 4.1732 (4.2491)  mae_loss: 1.1236 (1.1079)  classification_loss: 2.2247 (2.2119)  loss_mask: 0.8103 (0.9293)  time: 0.1966  data: 0.0002  max mem: 5511
[06:20:50.205815] Epoch: [1]  [380/781]  eta: 0:01:19  lr: 0.000074  training_loss: 4.1450 (4.2439)  mae_loss: 1.0701 (1.1071)  classification_loss: 2.2070 (2.2124)  loss_mask: 0.8269 (0.9244)  time: 0.1976  data: 0.0002  max mem: 5511
[06:20:54.149578] Epoch: [1]  [400/781]  eta: 0:01:15  lr: 0.000076  training_loss: 4.1062 (4.2383)  mae_loss: 1.1394 (1.1085)  classification_loss: 2.2097 (2.2121)  loss_mask: 0.7632 (0.9177)  time: 0.1971  data: 0.0003  max mem: 5511
[06:20:58.107817] Epoch: [1]  [420/781]  eta: 0:01:11  lr: 0.000077  training_loss: 4.1177 (4.2325)  mae_loss: 1.0807 (1.1080)  classification_loss: 2.1975 (2.2111)  loss_mask: 0.7931 (0.9135)  time: 0.1978  data: 0.0002  max mem: 5511
[06:21:02.046174] Epoch: [1]  [440/781]  eta: 0:01:07  lr: 0.000078  training_loss: 4.0715 (4.2261)  mae_loss: 1.0831 (1.1076)  classification_loss: 2.1802 (2.2104)  loss_mask: 0.8105 (0.9081)  time: 0.1968  data: 0.0002  max mem: 5511
[06:21:06.015511] Epoch: [1]  [460/781]  eta: 0:01:03  lr: 0.000079  training_loss: 4.0097 (4.2192)  mae_loss: 1.0788 (1.1065)  classification_loss: 2.1835 (2.2095)  loss_mask: 0.8126 (0.9032)  time: 0.1984  data: 0.0003  max mem: 5511
[06:21:09.964786] Epoch: [1]  [480/781]  eta: 0:00:59  lr: 0.000081  training_loss: 4.1265 (4.2159)  mae_loss: 1.0857 (1.1050)  classification_loss: 2.2263 (2.2102)  loss_mask: 0.8263 (0.9008)  time: 0.1974  data: 0.0003  max mem: 5511
[06:21:13.923302] Epoch: [1]  [500/781]  eta: 0:00:55  lr: 0.000082  training_loss: 4.2006 (4.2160)  mae_loss: 1.0874 (1.1052)  classification_loss: 2.1868 (2.2095)  loss_mask: 0.8920 (0.9013)  time: 0.1978  data: 0.0003  max mem: 5511
[06:21:17.903655] Epoch: [1]  [520/781]  eta: 0:00:51  lr: 0.000083  training_loss: 4.0134 (4.2089)  mae_loss: 1.0745 (1.1043)  classification_loss: 2.1963 (2.2092)  loss_mask: 0.7473 (0.8955)  time: 0.1989  data: 0.0002  max mem: 5511
[06:21:21.843159] Epoch: [1]  [540/781]  eta: 0:00:47  lr: 0.000085  training_loss: 4.0131 (4.2016)  mae_loss: 1.0471 (1.1027)  classification_loss: 2.2056 (2.2093)  loss_mask: 0.7358 (0.8897)  time: 0.1969  data: 0.0002  max mem: 5511
[06:21:25.792236] Epoch: [1]  [560/781]  eta: 0:00:43  lr: 0.000086  training_loss: 4.0485 (4.1962)  mae_loss: 1.0740 (1.1023)  classification_loss: 2.1978 (2.2088)  loss_mask: 0.7059 (0.8851)  time: 0.1974  data: 0.0002  max mem: 5511
[06:21:29.734847] Epoch: [1]  [580/781]  eta: 0:00:39  lr: 0.000087  training_loss: 4.1360 (4.1933)  mae_loss: 1.0739 (1.1022)  classification_loss: 2.1985 (2.2086)  loss_mask: 0.7900 (0.8826)  time: 0.1970  data: 0.0002  max mem: 5511
[06:21:33.682499] Epoch: [1]  [600/781]  eta: 0:00:35  lr: 0.000088  training_loss: 4.0579 (4.1901)  mae_loss: 1.0623 (1.1014)  classification_loss: 2.1832 (2.2082)  loss_mask: 0.7918 (0.8804)  time: 0.1973  data: 0.0003  max mem: 5511
[06:21:37.612706] Epoch: [1]  [620/781]  eta: 0:00:31  lr: 0.000090  training_loss: 4.0079 (4.1843)  mae_loss: 1.0546 (1.1001)  classification_loss: 2.1909 (2.2076)  loss_mask: 0.7550 (0.8766)  time: 0.1964  data: 0.0002  max mem: 5511
[06:21:41.553780] Epoch: [1]  [640/781]  eta: 0:00:27  lr: 0.000091  training_loss: 4.0675 (4.1799)  mae_loss: 1.0748 (1.1002)  classification_loss: 2.1738 (2.2070)  loss_mask: 0.7601 (0.8728)  time: 0.1970  data: 0.0002  max mem: 5511
[06:21:45.504962] Epoch: [1]  [660/781]  eta: 0:00:24  lr: 0.000092  training_loss: 4.0347 (4.1754)  mae_loss: 1.0510 (1.0988)  classification_loss: 2.1704 (2.2064)  loss_mask: 0.7482 (0.8701)  time: 0.1974  data: 0.0002  max mem: 5511
[06:21:49.446052] Epoch: [1]  [680/781]  eta: 0:00:20  lr: 0.000094  training_loss: 4.0171 (4.1699)  mae_loss: 1.0609 (1.0975)  classification_loss: 2.2039 (2.2065)  loss_mask: 0.7184 (0.8659)  time: 0.1970  data: 0.0002  max mem: 5511
[06:21:53.398746] Epoch: [1]  [700/781]  eta: 0:00:16  lr: 0.000095  training_loss: 3.9273 (4.1637)  mae_loss: 1.0344 (1.0955)  classification_loss: 2.1866 (2.2062)  loss_mask: 0.7075 (0.8620)  time: 0.1976  data: 0.0002  max mem: 5511
[06:21:57.370755] Epoch: [1]  [720/781]  eta: 0:00:12  lr: 0.000096  training_loss: 3.9676 (4.1590)  mae_loss: 0.9820 (1.0931)  classification_loss: 2.2100 (2.2063)  loss_mask: 0.7715 (0.8596)  time: 0.1985  data: 0.0002  max mem: 5511
[06:22:01.344565] Epoch: [1]  [740/781]  eta: 0:00:08  lr: 0.000097  training_loss: 4.0029 (4.1546)  mae_loss: 1.0076 (1.0910)  classification_loss: 2.2077 (2.2065)  loss_mask: 0.7592 (0.8571)  time: 0.1986  data: 0.0002  max mem: 5511
[06:22:05.293224] Epoch: [1]  [760/781]  eta: 0:00:04  lr: 0.000099  training_loss: 3.9631 (4.1493)  mae_loss: 0.9893 (1.0893)  classification_loss: 2.1924 (2.2064)  loss_mask: 0.7204 (0.8536)  time: 0.1972  data: 0.0002  max mem: 5511
[06:22:09.215169] Epoch: [1]  [780/781]  eta: 0:00:00  lr: 0.000100  training_loss: 3.9424 (4.1446)  mae_loss: 1.0498 (1.0876)  classification_loss: 2.2089 (2.2067)  loss_mask: 0.6827 (0.8503)  time: 0.1960  data: 0.0002  max mem: 5511
[06:22:09.392192] Epoch: [1] Total time: 0:02:35 (0.1986 s / it)
[06:22:09.392712] Averaged stats: lr: 0.000100  training_loss: 3.9424 (4.1446)  mae_loss: 1.0498 (1.0876)  classification_loss: 2.2089 (2.2067)  loss_mask: 0.6827 (0.8503)
[06:22:10.169919] Test:  [  0/157]  eta: 0:02:01  testing_loss: 1.8703 (1.8703)  acc1: 40.6250 (40.6250)  acc5: 85.9375 (85.9375)  time: 0.7725  data: 0.7405  max mem: 5511
[06:22:10.471841] Test:  [ 10/157]  eta: 0:00:14  testing_loss: 2.0105 (1.9811)  acc1: 31.2500 (31.1080)  acc5: 85.9375 (83.6648)  time: 0.0975  data: 0.0675  max mem: 5511
[06:22:10.760253] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.9812 (1.9808)  acc1: 31.2500 (32.3661)  acc5: 84.3750 (83.7054)  time: 0.0293  data: 0.0002  max mem: 5511
[06:22:11.049651] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.9703 (1.9731)  acc1: 32.8125 (32.9637)  acc5: 82.8125 (83.7198)  time: 0.0287  data: 0.0002  max mem: 5511
[06:22:11.339001] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.9656 (1.9747)  acc1: 32.8125 (33.1555)  acc5: 82.8125 (83.4223)  time: 0.0288  data: 0.0002  max mem: 5511
[06:22:11.629599] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.9838 (1.9741)  acc1: 32.8125 (33.2414)  acc5: 84.3750 (83.7623)  time: 0.0289  data: 0.0002  max mem: 5511
[06:22:11.917798] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.9703 (1.9732)  acc1: 34.3750 (33.2480)  acc5: 84.3750 (83.9395)  time: 0.0288  data: 0.0003  max mem: 5511
[06:22:12.205606] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.9656 (1.9741)  acc1: 32.8125 (33.0326)  acc5: 84.3750 (83.9569)  time: 0.0286  data: 0.0003  max mem: 5511
[06:22:12.499056] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.9585 (1.9722)  acc1: 35.9375 (33.4105)  acc5: 85.9375 (84.2785)  time: 0.0289  data: 0.0003  max mem: 5511
[06:22:12.786480] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.9766 (1.9748)  acc1: 31.2500 (33.1044)  acc5: 84.3750 (84.2891)  time: 0.0289  data: 0.0003  max mem: 5511
[06:22:13.075070] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.9988 (1.9764)  acc1: 29.6875 (32.9981)  acc5: 84.3750 (84.1584)  time: 0.0287  data: 0.0002  max mem: 5511
[06:22:13.361228] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.9934 (1.9773)  acc1: 31.2500 (32.9533)  acc5: 84.3750 (84.1779)  time: 0.0286  data: 0.0002  max mem: 5511
[06:22:13.647428] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.9673 (1.9738)  acc1: 34.3750 (33.0708)  acc5: 85.9375 (84.3621)  time: 0.0285  data: 0.0002  max mem: 5511
[06:22:13.938130] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.9723 (1.9752)  acc1: 34.3750 (33.1823)  acc5: 84.3750 (84.1603)  time: 0.0287  data: 0.0004  max mem: 5511
[06:22:14.223715] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.9947 (1.9746)  acc1: 34.3750 (33.3887)  acc5: 82.8125 (84.2088)  time: 0.0286  data: 0.0004  max mem: 5511
[06:22:14.504164] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.9771 (1.9734)  acc1: 34.3750 (33.3609)  acc5: 84.3750 (84.3026)  time: 0.0281  data: 0.0001  max mem: 5511
[06:22:14.656388] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.9593 (1.9723)  acc1: 34.3750 (33.1800)  acc5: 84.3750 (84.3900)  time: 0.0271  data: 0.0001  max mem: 5511
[06:22:14.831606] Test: Total time: 0:00:05 (0.0346 s / it)
[06:22:14.833138] * Acc@1 33.180 Acc@5 84.390 loss 1.972
[06:22:14.833446] Accuracy of the network on the 10000 test images: 33.2%
[06:22:14.833620] Max accuracy: 33.18%
[06:22:14.977084] log_dir: ./output_dir
[06:22:15.852968] Epoch: [2]  [  0/781]  eta: 0:11:22  lr: 0.000100  training_loss: 3.9803 (3.9803)  mae_loss: 1.0624 (1.0624)  classification_loss: 2.2199 (2.2199)  loss_mask: 0.6980 (0.6980)  time: 0.8742  data: 0.6557  max mem: 5511
[06:22:19.826212] Epoch: [2]  [ 20/781]  eta: 0:02:55  lr: 0.000101  training_loss: 3.8789 (3.9012)  mae_loss: 0.9790 (0.9899)  classification_loss: 2.1950 (2.1902)  loss_mask: 0.7107 (0.7212)  time: 0.1985  data: 0.0002  max mem: 5511
[06:22:23.817407] Epoch: [2]  [ 40/781]  eta: 0:02:39  lr: 0.000103  training_loss: 3.8662 (3.9111)  mae_loss: 1.0100 (0.9977)  classification_loss: 2.1956 (2.1982)  loss_mask: 0.6841 (0.7152)  time: 0.1995  data: 0.0003  max mem: 5511

[06:22:27.754345] Epoch: [2]  [ 60/781]  eta: 0:02:30  lr: 0.000104  training_loss: 3.9080 (3.9133)  mae_loss: 1.0101 (1.0033)  classification_loss: 2.2087 (2.2021)  loss_mask: 0.7017 (0.7079)  time: 0.1968  data: 0.0004  max mem: 5511
[06:22:31.719577] Epoch: [2]  [ 80/781]  eta: 0:02:24  lr: 0.000105  training_loss: 3.8877 (3.9107)  mae_loss: 0.9797 (0.9999)  classification_loss: 2.1949 (2.2067)  loss_mask: 0.6802 (0.7041)  time: 0.1982  data: 0.0004  max mem: 5511
[06:22:35.685458] Epoch: [2]  [100/781]  eta: 0:02:19  lr: 0.000106  training_loss: 3.8824 (3.9108)  mae_loss: 0.9569 (0.9959)  classification_loss: 2.1953 (2.2026)  loss_mask: 0.7432 (0.7124)  time: 0.1982  data: 0.0002  max mem: 5511
[06:22:39.629526] Epoch: [2]  [120/781]  eta: 0:02:14  lr: 0.000108  training_loss: 4.0076 (3.9313)  mae_loss: 0.9532 (0.9912)  classification_loss: 2.2077 (2.2049)  loss_mask: 0.8219 (0.7352)  time: 0.1971  data: 0.0002  max mem: 5511
[06:22:43.576369] Epoch: [2]  [140/781]  eta: 0:02:09  lr: 0.000109  training_loss: 3.8422 (3.9253)  mae_loss: 0.9696 (0.9885)  classification_loss: 2.1947 (2.2019)  loss_mask: 0.7388 (0.7348)  time: 0.1972  data: 0.0002  max mem: 5511
[06:22:47.535245] Epoch: [2]  [160/781]  eta: 0:02:05  lr: 0.000110  training_loss: 3.8797 (3.9211)  mae_loss: 0.9617 (0.9882)  classification_loss: 2.1890 (2.2007)  loss_mask: 0.7110 (0.7322)  time: 0.1979  data: 0.0002  max mem: 5511
[06:22:51.469493] Epoch: [2]  [180/781]  eta: 0:02:01  lr: 0.000112  training_loss: 3.8562 (3.9166)  mae_loss: 0.9995 (0.9895)  classification_loss: 2.1774 (2.2003)  loss_mask: 0.6626 (0.7268)  time: 0.1966  data: 0.0002  max mem: 5511
[06:22:55.409764] Epoch: [2]  [200/781]  eta: 0:01:56  lr: 0.000113  training_loss: 3.8236 (3.9074)  mae_loss: 0.9914 (0.9906)  classification_loss: 2.1682 (2.1976)  loss_mask: 0.6403 (0.7192)  time: 0.1969  data: 0.0002  max mem: 5511
[06:22:59.400353] Epoch: [2]  [220/781]  eta: 0:01:52  lr: 0.000114  training_loss: 3.7526 (3.8943)  mae_loss: 0.9306 (0.9855)  classification_loss: 2.1539 (2.1944)  loss_mask: 0.6413 (0.7144)  time: 0.1995  data: 0.0003  max mem: 5511
[06:23:03.336658] Epoch: [2]  [240/781]  eta: 0:01:48  lr: 0.000115  training_loss: 3.8816 (3.8957)  mae_loss: 0.9456 (0.9856)  classification_loss: 2.1963 (2.1938)  loss_mask: 0.7483 (0.7163)  time: 0.1967  data: 0.0002  max mem: 5511
[06:23:07.273433] Epoch: [2]  [260/781]  eta: 0:01:44  lr: 0.000117  training_loss: 3.8307 (3.8916)  mae_loss: 0.9420 (0.9823)  classification_loss: 2.1725 (2.1924)  loss_mask: 0.6695 (0.7168)  time: 0.1967  data: 0.0002  max mem: 5511
[06:23:11.214261] Epoch: [2]  [280/781]  eta: 0:01:40  lr: 0.000118  training_loss: 3.8732 (3.8893)  mae_loss: 0.9785 (0.9824)  classification_loss: 2.1794 (2.1922)  loss_mask: 0.6698 (0.7147)  time: 0.1970  data: 0.0004  max mem: 5511
[06:23:15.158603] Epoch: [2]  [300/781]  eta: 0:01:36  lr: 0.000119  training_loss: 3.8794 (3.8878)  mae_loss: 0.9323 (0.9805)  classification_loss: 2.1803 (2.1916)  loss_mask: 0.7361 (0.7156)  time: 0.1971  data: 0.0004  max mem: 5511
[06:23:19.116623] Epoch: [2]  [320/781]  eta: 0:01:32  lr: 0.000120  training_loss: 3.8726 (3.8850)  mae_loss: 0.9355 (0.9783)  classification_loss: 2.1707 (2.1907)  loss_mask: 0.7077 (0.7160)  time: 0.1978  data: 0.0003  max mem: 5511
[06:23:23.081029] Epoch: [2]  [340/781]  eta: 0:01:28  lr: 0.000122  training_loss: 3.8229 (3.8809)  mae_loss: 0.9289 (0.9754)  classification_loss: 2.1990 (2.1908)  loss_mask: 0.7106 (0.7147)  time: 0.1981  data: 0.0002  max mem: 5511
[06:23:27.002039] Epoch: [2]  [360/781]  eta: 0:01:23  lr: 0.000123  training_loss: 3.7762 (3.8762)  mae_loss: 0.9409 (0.9742)  classification_loss: 2.1792 (2.1903)  loss_mask: 0.6551 (0.7117)  time: 0.1960  data: 0.0002  max mem: 5511
[06:23:30.945832] Epoch: [2]  [380/781]  eta: 0:01:19  lr: 0.000124  training_loss: 3.8428 (3.8768)  mae_loss: 0.9513 (0.9723)  classification_loss: 2.1955 (2.1903)  loss_mask: 0.7089 (0.7142)  time: 0.1971  data: 0.0002  max mem: 5511
[06:23:34.898624] Epoch: [2]  [400/781]  eta: 0:01:15  lr: 0.000126  training_loss: 3.8399 (3.8762)  mae_loss: 0.9172 (0.9701)  classification_loss: 2.1722 (2.1889)  loss_mask: 0.7678 (0.7171)  time: 0.1975  data: 0.0003  max mem: 5511
[06:23:38.830092] Epoch: [2]  [420/781]  eta: 0:01:11  lr: 0.000127  training_loss: 3.7574 (3.8717)  mae_loss: 0.9554 (0.9699)  classification_loss: 2.1752 (2.1882)  loss_mask: 0.6182 (0.7136)  time: 0.1965  data: 0.0002  max mem: 5511
[06:23:42.761093] Epoch: [2]  [440/781]  eta: 0:01:07  lr: 0.000128  training_loss: 3.8401 (3.8696)  mae_loss: 0.9523 (0.9698)  classification_loss: 2.1559 (2.1872)  loss_mask: 0.6630 (0.7127)  time: 0.1964  data: 0.0002  max mem: 5511
[06:23:46.715423] Epoch: [2]  [460/781]  eta: 0:01:03  lr: 0.000129  training_loss: 3.6906 (3.8621)  mae_loss: 0.8924 (0.9666)  classification_loss: 2.1618 (2.1859)  loss_mask: 0.6460 (0.7096)  time: 0.1976  data: 0.0003  max mem: 5511
[06:23:50.695991] Epoch: [2]  [480/781]  eta: 0:00:59  lr: 0.000131  training_loss: 3.8223 (3.8608)  mae_loss: 0.9215 (0.9658)  classification_loss: 2.2049 (2.1866)  loss_mask: 0.6472 (0.7084)  time: 0.1989  data: 0.0003  max mem: 5511
[06:23:54.658777] Epoch: [2]  [500/781]  eta: 0:00:55  lr: 0.000132  training_loss: 3.8414 (3.8621)  mae_loss: 0.9123 (0.9645)  classification_loss: 2.1913 (2.1862)  loss_mask: 0.7502 (0.7114)  time: 0.1980  data: 0.0002  max mem: 5511
[06:23:58.602941] Epoch: [2]  [520/781]  eta: 0:00:51  lr: 0.000133  training_loss: 3.8386 (3.8614)  mae_loss: 0.9118 (0.9622)  classification_loss: 2.1596 (2.1855)  loss_mask: 0.7565 (0.7137)  time: 0.1971  data: 0.0002  max mem: 5511
[06:24:02.540331] Epoch: [2]  [540/781]  eta: 0:00:47  lr: 0.000135  training_loss: 3.7548 (3.8580)  mae_loss: 0.8772 (0.9598)  classification_loss: 2.1657 (2.1852)  loss_mask: 0.6729 (0.7130)  time: 0.1968  data: 0.0002  max mem: 5511
[06:24:06.482251] Epoch: [2]  [560/781]  eta: 0:00:43  lr: 0.000136  training_loss: 3.6918 (3.8525)  mae_loss: 0.9005 (0.9580)  classification_loss: 2.1729 (2.1843)  loss_mask: 0.6508 (0.7102)  time: 0.1970  data: 0.0002  max mem: 5511
[06:24:10.434636] Epoch: [2]  [580/781]  eta: 0:00:39  lr: 0.000137  training_loss: 3.6748 (3.8461)  mae_loss: 0.8708 (0.9551)  classification_loss: 2.1372 (2.1834)  loss_mask: 0.6021 (0.7076)  time: 0.1975  data: 0.0002  max mem: 5511
[06:24:14.371145] Epoch: [2]  [600/781]  eta: 0:00:35  lr: 0.000138  training_loss: 3.7311 (3.8430)  mae_loss: 0.8998 (0.9539)  classification_loss: 2.1742 (2.1832)  loss_mask: 0.6691 (0.7059)  time: 0.1967  data: 0.0002  max mem: 5511
[06:24:18.322500] Epoch: [2]  [620/781]  eta: 0:00:31  lr: 0.000140  training_loss: 3.7057 (3.8384)  mae_loss: 0.8550 (0.9510)  classification_loss: 2.1701 (2.1827)  loss_mask: 0.6780 (0.7048)  time: 0.1975  data: 0.0002  max mem: 5511
[06:24:22.248045] Epoch: [2]  [640/781]  eta: 0:00:27  lr: 0.000141  training_loss: 3.6823 (3.8338)  mae_loss: 0.8942 (0.9488)  classification_loss: 2.1604 (2.1819)  loss_mask: 0.6182 (0.7031)  time: 0.1962  data: 0.0002  max mem: 5511
[06:24:26.223177] Epoch: [2]  [660/781]  eta: 0:00:24  lr: 0.000142  training_loss: 3.6692 (3.8287)  mae_loss: 0.8276 (0.9460)  classification_loss: 2.1710 (2.1814)  loss_mask: 0.6317 (0.7013)  time: 0.1987  data: 0.0002  max mem: 5511
[06:24:30.198278] Epoch: [2]  [680/781]  eta: 0:00:20  lr: 0.000144  training_loss: 3.6981 (3.8257)  mae_loss: 0.8685 (0.9440)  classification_loss: 2.1634 (2.1809)  loss_mask: 0.6626 (0.7008)  time: 0.1987  data: 0.0003  max mem: 5511
[06:24:34.176157] Epoch: [2]  [700/781]  eta: 0:00:16  lr: 0.000145  training_loss: 3.6676 (3.8229)  mae_loss: 0.8488 (0.9415)  classification_loss: 2.1510 (2.1803)  loss_mask: 0.6888 (0.7011)  time: 0.1988  data: 0.0002  max mem: 5511
[06:24:38.114789] Epoch: [2]  [720/781]  eta: 0:00:12  lr: 0.000146  training_loss: 3.7015 (3.8202)  mae_loss: 0.8413 (0.9385)  classification_loss: 2.1669 (2.1801)  loss_mask: 0.7064 (0.7016)  time: 0.1968  data: 0.0002  max mem: 5511
[06:24:42.084186] Epoch: [2]  [740/781]  eta: 0:00:08  lr: 0.000147  training_loss: 3.6535 (3.8171)  mae_loss: 0.8262 (0.9358)  classification_loss: 2.1652 (2.1798)  loss_mask: 0.6391 (0.7014)  time: 0.1984  data: 0.0002  max mem: 5511
[06:24:46.055125] Epoch: [2]  [760/781]  eta: 0:00:04  lr: 0.000149  training_loss: 3.6910 (3.8143)  mae_loss: 0.8288 (0.9330)  classification_loss: 2.1688 (2.1798)  loss_mask: 0.7208 (0.7015)  time: 0.1985  data: 0.0002  max mem: 5511
[06:24:49.983812] Epoch: [2]  [780/781]  eta: 0:00:00  lr: 0.000150  training_loss: 3.6966 (3.8117)  mae_loss: 0.8510 (0.9309)  classification_loss: 2.1391 (2.1791)  loss_mask: 0.6795 (0.7017)  time: 0.1964  data: 0.0002  max mem: 5511
[06:24:50.171259] Epoch: [2] Total time: 0:02:35 (0.1987 s / it)
[06:24:50.171728] Averaged stats: lr: 0.000150  training_loss: 3.6966 (3.8117)  mae_loss: 0.8510 (0.9309)  classification_loss: 2.1391 (2.1791)  loss_mask: 0.6795 (0.7017)
[06:24:50.918240] Test:  [  0/157]  eta: 0:01:56  testing_loss: 1.7769 (1.7769)  acc1: 43.7500 (43.7500)  acc5: 87.5000 (87.5000)  time: 0.7415  data: 0.7097  max mem: 5511
[06:24:51.218639] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.8923 (1.8784)  acc1: 37.5000 (35.6534)  acc5: 85.9375 (85.7955)  time: 0.0945  data: 0.0649  max mem: 5511
[06:24:51.503214] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.8663 (1.8721)  acc1: 39.0625 (37.2024)  acc5: 85.9375 (85.4167)  time: 0.0291  data: 0.0003  max mem: 5511
[06:24:51.790914] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.8473 (1.8621)  acc1: 39.0625 (37.5504)  acc5: 84.3750 (85.5847)  time: 0.0285  data: 0.0003  max mem: 5511
[06:24:52.076480] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.8447 (1.8652)  acc1: 37.5000 (37.8049)  acc5: 84.3750 (84.8704)  time: 0.0284  data: 0.0003  max mem: 5511
[06:24:52.359850] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.8812 (1.8663)  acc1: 37.5000 (37.7757)  acc5: 84.3750 (85.1716)  time: 0.0282  data: 0.0002  max mem: 5511
[06:24:52.648333] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.8512 (1.8620)  acc1: 37.5000 (38.0379)  acc5: 84.3750 (85.3740)  time: 0.0284  data: 0.0002  max mem: 5511
[06:24:52.933755] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.8394 (1.8615)  acc1: 37.5000 (38.0062)  acc5: 84.3750 (85.5854)  time: 0.0285  data: 0.0002  max mem: 5511
[06:24:53.218449] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.8568 (1.8604)  acc1: 37.5000 (38.0015)  acc5: 87.5000 (85.8603)  time: 0.0284  data: 0.0003  max mem: 5511
[06:24:53.503748] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.8825 (1.8632)  acc1: 37.5000 (37.8091)  acc5: 85.9375 (85.8516)  time: 0.0283  data: 0.0002  max mem: 5511
[06:24:53.789124] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.8825 (1.8649)  acc1: 35.9375 (37.6392)  acc5: 84.3750 (85.8137)  time: 0.0284  data: 0.0002  max mem: 5511
[06:24:54.073536] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.8800 (1.8665)  acc1: 35.9375 (37.5704)  acc5: 84.3750 (85.7827)  time: 0.0283  data: 0.0002  max mem: 5511
[06:24:54.358585] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.8369 (1.8626)  acc1: 35.9375 (37.6033)  acc5: 87.5000 (86.0408)  time: 0.0283  data: 0.0002  max mem: 5511
[06:24:54.646837] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.8604 (1.8642)  acc1: 37.5000 (37.6551)  acc5: 89.0625 (86.0329)  time: 0.0285  data: 0.0002  max mem: 5511
[06:24:54.931057] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.8792 (1.8643)  acc1: 39.0625 (37.6995)  acc5: 85.9375 (86.0372)  time: 0.0285  data: 0.0002  max mem: 5511
[06:24:55.212310] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.8725 (1.8630)  acc1: 39.0625 (37.8518)  acc5: 84.3750 (86.0203)  time: 0.0282  data: 0.0001  max mem: 5511
[06:24:55.364139] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.8540 (1.8618)  acc1: 39.0625 (37.7300)  acc5: 85.9375 (86.0800)  time: 0.0272  data: 0.0001  max mem: 5511
[06:24:55.552907] Test: Total time: 0:00:05 (0.0343 s / it)
[06:24:55.553356] * Acc@1 37.730 Acc@5 86.080 loss 1.862
[06:24:55.553636] Accuracy of the network on the 10000 test images: 37.7%
[06:24:55.553849] Max accuracy: 37.73%
[06:24:55.672756] log_dir: ./output_dir
[06:24:56.616726] Epoch: [3]  [  0/781]  eta: 0:12:15  lr: 0.000150  training_loss: 3.5359 (3.5359)  mae_loss: 0.7667 (0.7667)  classification_loss: 2.0506 (2.0506)  loss_mask: 0.7186 (0.7186)  time: 0.9420  data: 0.7350  max mem: 5511
[06:25:00.562490] Epoch: [3]  [ 20/781]  eta: 0:02:57  lr: 0.000151  training_loss: 3.6300 (3.6156)  mae_loss: 0.8003 (0.8103)  classification_loss: 2.1294 (2.1375)  loss_mask: 0.6721 (0.6678)  time: 0.1972  data: 0.0003  max mem: 5511
[06:25:04.503153] Epoch: [3]  [ 40/781]  eta: 0:02:39  lr: 0.000153  training_loss: 3.7058 (3.6747)  mae_loss: 0.8181 (0.8132)  classification_loss: 2.2026 (2.1618)  loss_mask: 0.6913 (0.6997)  time: 0.1970  data: 0.0002  max mem: 5511
[06:25:08.452085] Epoch: [3]  [ 60/781]  eta: 0:02:30  lr: 0.000154  training_loss: 3.5864 (3.6452)  mae_loss: 0.7682 (0.8039)  classification_loss: 2.1441 (2.1612)  loss_mask: 0.6351 (0.6801)  time: 0.1974  data: 0.0002  max mem: 5511
[06:25:12.389950] Epoch: [3]  [ 80/781]  eta: 0:02:24  lr: 0.000155  training_loss: 3.6046 (3.6436)  mae_loss: 0.7640 (0.7982)  classification_loss: 2.1848 (2.1658)  loss_mask: 0.6593 (0.6795)  time: 0.1968  data: 0.0002  max mem: 5511
[06:25:16.341472] Epoch: [3]  [100/781]  eta: 0:02:19  lr: 0.000156  training_loss: 3.6398 (3.6412)  mae_loss: 0.7523 (0.7882)  classification_loss: 2.1828 (2.1704)  loss_mask: 0.6941 (0.6826)  time: 0.1975  data: 0.0002  max mem: 5511
[06:25:20.305398] Epoch: [3]  [120/781]  eta: 0:02:14  lr: 0.000158  training_loss: 3.5598 (3.6276)  mae_loss: 0.7338 (0.7796)  classification_loss: 2.1613 (2.1697)  loss_mask: 0.6556 (0.6782)  time: 0.1981  data: 0.0003  max mem: 5511
[06:25:24.244435] Epoch: [3]  [140/781]  eta: 0:02:09  lr: 0.000159  training_loss: 3.4159 (3.6035)  mae_loss: 0.7150 (0.7720)  classification_loss: 2.1351 (2.1686)  loss_mask: 0.5564 (0.6629)  time: 0.1969  data: 0.0002  max mem: 5511
[06:25:28.190404] Epoch: [3]  [160/781]  eta: 0:02:05  lr: 0.000160  training_loss: 3.4536 (3.5820)  mae_loss: 0.6940 (0.7633)  classification_loss: 2.1606 (2.1678)  loss_mask: 0.5689 (0.6509)  time: 0.1972  data: 0.0002  max mem: 5511
[06:25:32.125451] Epoch: [3]  [180/781]  eta: 0:02:00  lr: 0.000162  training_loss: 3.4567 (3.5710)  mae_loss: 0.6958 (0.7567)  classification_loss: 2.1466 (2.1663)  loss_mask: 0.5817 (0.6480)  time: 0.1967  data: 0.0002  max mem: 5511
[06:25:36.069674] Epoch: [3]  [200/781]  eta: 0:01:56  lr: 0.000163  training_loss: 3.4948 (3.5623)  mae_loss: 0.6822 (0.7504)  classification_loss: 2.1563 (2.1655)  loss_mask: 0.6347 (0.6464)  time: 0.1971  data: 0.0002  max mem: 5511
[06:25:40.019149] Epoch: [3]  [220/781]  eta: 0:01:52  lr: 0.000164  training_loss: 3.4749 (3.5574)  mae_loss: 0.6617 (0.7434)  classification_loss: 2.1476 (2.1634)  loss_mask: 0.6843 (0.6506)  time: 0.1974  data: 0.0002  max mem: 5511
[06:25:43.961624] Epoch: [3]  [240/781]  eta: 0:01:48  lr: 0.000165  training_loss: 3.3984 (3.5471)  mae_loss: 0.6290 (0.7355)  classification_loss: 2.1519 (2.1628)  loss_mask: 0.6028 (0.6487)  time: 0.1970  data: 0.0002  max mem: 5511
[06:25:47.921081] Epoch: [3]  [260/781]  eta: 0:01:44  lr: 0.000167  training_loss: 3.4087 (3.5368)  mae_loss: 0.6445 (0.7279)  classification_loss: 2.1409 (2.1606)  loss_mask: 0.6307 (0.6483)  time: 0.1979  data: 0.0002  max mem: 5511
[06:25:51.867880] Epoch: [3]  [280/781]  eta: 0:01:40  lr: 0.000168  training_loss: 3.5253 (3.5401)  mae_loss: 0.6200 (0.7204)  classification_loss: 2.1591 (2.1622)  loss_mask: 0.7182 (0.6575)  time: 0.1972  data: 0.0002  max mem: 5511
[06:25:55.825386] Epoch: [3]  [300/781]  eta: 0:01:36  lr: 0.000169  training_loss: 3.4226 (3.5339)  mae_loss: 0.6023 (0.7127)  classification_loss: 2.1781 (2.1628)  loss_mask: 0.6702 (0.6585)  time: 0.1978  data: 0.0002  max mem: 5511
[06:25:59.780874] Epoch: [3]  [320/781]  eta: 0:01:32  lr: 0.000170  training_loss: 3.3193 (3.5225)  mae_loss: 0.5852 (0.7044)  classification_loss: 2.1359 (2.1615)  loss_mask: 0.6020 (0.6566)  time: 0.1977  data: 0.0003  max mem: 5511
[06:26:03.769363] Epoch: [3]  [340/781]  eta: 0:01:28  lr: 0.000172  training_loss: 3.4031 (3.5155)  mae_loss: 0.5925 (0.6989)  classification_loss: 2.1325 (2.1604)  loss_mask: 0.6270 (0.6562)  time: 0.1993  data: 0.0004  max mem: 5511
[06:26:07.807165] Epoch: [3]  [360/781]  eta: 0:01:24  lr: 0.000173  training_loss: 3.3344 (3.5049)  mae_loss: 0.5534 (0.6914)  classification_loss: 2.1403 (2.1602)  loss_mask: 0.5831 (0.6533)  time: 0.2018  data: 0.0006  max mem: 5511
[06:26:11.796568] Epoch: [3]  [380/781]  eta: 0:01:20  lr: 0.000174  training_loss: 3.4482 (3.5037)  mae_loss: 0.5665 (0.6863)  classification_loss: 2.1556 (2.1596)  loss_mask: 0.6925 (0.6578)  time: 0.1994  data: 0.0003  max mem: 5511
[06:26:15.729942] Epoch: [3]  [400/781]  eta: 0:01:16  lr: 0.000176  training_loss: 3.4230 (3.4995)  mae_loss: 0.5554 (0.6807)  classification_loss: 2.1668 (2.1597)  loss_mask: 0.6861 (0.6591)  time: 0.1966  data: 0.0002  max mem: 5511
[06:26:19.711352] Epoch: [3]  [420/781]  eta: 0:01:12  lr: 0.000177  training_loss: 3.3344 (3.4929)  mae_loss: 0.5705 (0.6756)  classification_loss: 2.1207 (2.1588)  loss_mask: 0.6358 (0.6585)  time: 0.1989  data: 0.0002  max mem: 5511
[06:26:23.646475] Epoch: [3]  [440/781]  eta: 0:01:07  lr: 0.000178  training_loss: 3.4084 (3.4887)  mae_loss: 0.5702 (0.6709)  classification_loss: 2.1273 (2.1575)  loss_mask: 0.6870 (0.6603)  time: 0.1967  data: 0.0002  max mem: 5511
[06:26:27.584100] Epoch: [3]  [460/781]  eta: 0:01:03  lr: 0.000179  training_loss: 3.3045 (3.4797)  mae_loss: 0.5305 (0.6649)  classification_loss: 2.1354 (2.1571)  loss_mask: 0.5848 (0.6577)  time: 0.1968  data: 0.0002  max mem: 5511
[06:26:31.586391] Epoch: [3]  [480/781]  eta: 0:00:59  lr: 0.000181  training_loss: 3.3107 (3.4724)  mae_loss: 0.5477 (0.6599)  classification_loss: 2.1460 (2.1568)  loss_mask: 0.5950 (0.6557)  time: 0.2000  data: 0.0002  max mem: 5511
[06:26:35.547204] Epoch: [3]  [500/781]  eta: 0:00:55  lr: 0.000182  training_loss: 3.2799 (3.4653)  mae_loss: 0.5424 (0.6555)  classification_loss: 2.1446 (2.1561)  loss_mask: 0.5750 (0.6536)  time: 0.1979  data: 0.0003  max mem: 5511
[06:26:39.504217] Epoch: [3]  [520/781]  eta: 0:00:51  lr: 0.000183  training_loss: 3.3012 (3.4585)  mae_loss: 0.5054 (0.6505)  classification_loss: 2.1547 (2.1557)  loss_mask: 0.6124 (0.6523)  time: 0.1978  data: 0.0002  max mem: 5511
[06:26:43.476909] Epoch: [3]  [540/781]  eta: 0:00:47  lr: 0.000185  training_loss: 3.3145 (3.4549)  mae_loss: 0.5391 (0.6463)  classification_loss: 2.1653 (2.1564)  loss_mask: 0.6391 (0.6522)  time: 0.1985  data: 0.0003  max mem: 5511
[06:26:47.438825] Epoch: [3]  [560/781]  eta: 0:00:44  lr: 0.000186  training_loss: 3.2513 (3.4471)  mae_loss: 0.5181 (0.6421)  classification_loss: 2.1209 (2.1553)  loss_mask: 0.5896 (0.6497)  time: 0.1980  data: 0.0002  max mem: 5511
[06:26:51.401394] Epoch: [3]  [580/781]  eta: 0:00:40  lr: 0.000187  training_loss: 3.2233 (3.4403)  mae_loss: 0.5112 (0.6374)  classification_loss: 2.1478 (2.1553)  loss_mask: 0.5588 (0.6475)  time: 0.1981  data: 0.0002  max mem: 5511
[06:26:55.347034] Epoch: [3]  [600/781]  eta: 0:00:36  lr: 0.000188  training_loss: 3.2784 (3.4353)  mae_loss: 0.5179 (0.6336)  classification_loss: 2.1470 (2.1549)  loss_mask: 0.6030 (0.6468)  time: 0.1972  data: 0.0002  max mem: 5511
[06:26:59.280555] Epoch: [3]  [620/781]  eta: 0:00:32  lr: 0.000190  training_loss: 3.2013 (3.4288)  mae_loss: 0.5039 (0.6297)  classification_loss: 2.1234 (2.1541)  loss_mask: 0.5626 (0.6451)  time: 0.1966  data: 0.0003  max mem: 5511
[06:27:03.307674] Epoch: [3]  [640/781]  eta: 0:00:28  lr: 0.000191  training_loss: 3.1593 (3.4227)  mae_loss: 0.5055 (0.6261)  classification_loss: 2.1281 (2.1532)  loss_mask: 0.5705 (0.6434)  time: 0.2013  data: 0.0002  max mem: 5511
[06:27:07.253587] Epoch: [3]  [660/781]  eta: 0:00:24  lr: 0.000192  training_loss: 3.1713 (3.4166)  mae_loss: 0.4917 (0.6222)  classification_loss: 2.1457 (2.1531)  loss_mask: 0.5404 (0.6413)  time: 0.1972  data: 0.0003  max mem: 5511
[06:27:11.225354] Epoch: [3]  [680/781]  eta: 0:00:20  lr: 0.000194  training_loss: 3.2545 (3.4126)  mae_loss: 0.5042 (0.6189)  classification_loss: 2.1376 (2.1522)  loss_mask: 0.6200 (0.6415)  time: 0.1985  data: 0.0003  max mem: 5511
[06:27:15.198610] Epoch: [3]  [700/781]  eta: 0:00:16  lr: 0.000195  training_loss: 3.2690 (3.4105)  mae_loss: 0.5004 (0.6158)  classification_loss: 2.1099 (2.1511)  loss_mask: 0.7097 (0.6436)  time: 0.1986  data: 0.0002  max mem: 5511
[06:27:19.191888] Epoch: [3]  [720/781]  eta: 0:00:12  lr: 0.000196  training_loss: 3.2407 (3.4068)  mae_loss: 0.4799 (0.6123)  classification_loss: 2.1198 (2.1505)  loss_mask: 0.6384 (0.6440)  time: 0.1996  data: 0.0002  max mem: 5511
[06:27:23.157510] Epoch: [3]  [740/781]  eta: 0:00:08  lr: 0.000197  training_loss: 3.1993 (3.4019)  mae_loss: 0.4933 (0.6090)  classification_loss: 2.1102 (2.1497)  loss_mask: 0.6027 (0.6432)  time: 0.1982  data: 0.0002  max mem: 5511
[06:27:27.117991] Epoch: [3]  [760/781]  eta: 0:00:04  lr: 0.000199  training_loss: 3.2620 (3.3981)  mae_loss: 0.4962 (0.6063)  classification_loss: 2.1572 (2.1496)  loss_mask: 0.6166 (0.6422)  time: 0.1979  data: 0.0003  max mem: 5511
[06:27:31.040902] Epoch: [3]  [780/781]  eta: 0:00:00  lr: 0.000200  training_loss: 3.2218 (3.3932)  mae_loss: 0.5008 (0.6037)  classification_loss: 2.1135 (2.1489)  loss_mask: 0.5481 (0.6406)  time: 0.1960  data: 0.0001  max mem: 5511
[06:27:31.204600] Epoch: [3] Total time: 0:02:35 (0.1991 s / it)
[06:27:31.205094] Averaged stats: lr: 0.000200  training_loss: 3.2218 (3.3932)  mae_loss: 0.5008 (0.6037)  classification_loss: 2.1135 (2.1489)  loss_mask: 0.5481 (0.6406)
[06:27:31.910702] Test:  [  0/157]  eta: 0:01:50  testing_loss: 1.6968 (1.6968)  acc1: 43.7500 (43.7500)  acc5: 92.1875 (92.1875)  time: 0.7014  data: 0.6706  max mem: 5511
[06:27:32.205312] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.8170 (1.8057)  acc1: 37.5000 (37.9261)  acc5: 89.0625 (88.0682)  time: 0.0901  data: 0.0612  max mem: 5511
[06:27:32.489520] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.7789 (1.7833)  acc1: 39.0625 (40.6250)  acc5: 89.0625 (88.4673)  time: 0.0286  data: 0.0002  max mem: 5511
[06:27:32.782396] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.7569 (1.7755)  acc1: 39.0625 (40.9274)  acc5: 87.5000 (87.9032)  time: 0.0287  data: 0.0002  max mem: 5511
[06:27:33.069652] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.7693 (1.7819)  acc1: 40.6250 (40.8155)  acc5: 87.5000 (87.3095)  time: 0.0288  data: 0.0003  max mem: 5511
[06:27:33.361748] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.7789 (1.7811)  acc1: 42.1875 (41.1458)  acc5: 85.9375 (87.1936)  time: 0.0288  data: 0.0002  max mem: 5511
[06:27:33.649552] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.7470 (1.7783)  acc1: 42.1875 (41.1117)  acc5: 85.9375 (87.3719)  time: 0.0288  data: 0.0002  max mem: 5511
[06:27:33.935529] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.7411 (1.7774)  acc1: 39.0625 (41.0431)  acc5: 87.5000 (87.4780)  time: 0.0285  data: 0.0002  max mem: 5511
[06:27:34.223054] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.7734 (1.7767)  acc1: 39.0625 (40.8951)  acc5: 89.0625 (87.6157)  time: 0.0285  data: 0.0002  max mem: 5511
[06:27:34.509271] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.7892 (1.7784)  acc1: 40.6250 (40.8482)  acc5: 89.0625 (87.6545)  time: 0.0286  data: 0.0002  max mem: 5511
[06:27:34.797235] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.8047 (1.7809)  acc1: 40.6250 (40.7178)  acc5: 85.9375 (87.5464)  time: 0.0285  data: 0.0002  max mem: 5511
[06:27:35.083487] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.8047 (1.7834)  acc1: 39.0625 (40.5546)  acc5: 85.9375 (87.4859)  time: 0.0285  data: 0.0002  max mem: 5511
[06:27:35.368934] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.7642 (1.7809)  acc1: 39.0625 (40.5088)  acc5: 89.0625 (87.6808)  time: 0.0284  data: 0.0002  max mem: 5511
[06:27:35.661532] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.7607 (1.7820)  acc1: 40.6250 (40.4819)  acc5: 89.0625 (87.6908)  time: 0.0288  data: 0.0002  max mem: 5511
[06:27:35.944970] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.7821 (1.7824)  acc1: 40.6250 (40.5474)  acc5: 85.9375 (87.6330)  time: 0.0287  data: 0.0001  max mem: 5511
[06:27:36.228290] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.7793 (1.7813)  acc1: 42.1875 (40.7285)  acc5: 87.5000 (87.6552)  time: 0.0282  data: 0.0001  max mem: 5511
[06:27:36.382895] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.7652 (1.7806)  acc1: 42.1875 (40.5500)  acc5: 87.5000 (87.6700)  time: 0.0273  data: 0.0001  max mem: 5511
[06:27:36.580255] Test: Total time: 0:00:05 (0.0342 s / it)
[06:27:36.580910] * Acc@1 40.550 Acc@5 87.670 loss 1.781
[06:27:36.581363] Accuracy of the network on the 10000 test images: 40.5%
[06:27:36.581663] Max accuracy: 40.55%
[06:27:36.869745] log_dir: ./output_dir
[06:27:37.858588] Epoch: [4]  [  0/781]  eta: 0:12:50  lr: 0.000200  training_loss: 3.1481 (3.1481)  mae_loss: 0.5607 (0.5607)  classification_loss: 2.0708 (2.0708)  loss_mask: 0.5166 (0.5166)  time: 0.9867  data: 0.7729  max mem: 5511
[06:27:41.801311] Epoch: [4]  [ 20/781]  eta: 0:02:58  lr: 0.000201  training_loss: 3.2031 (3.1834)  mae_loss: 0.4714 (0.4893)  classification_loss: 2.1053 (2.1095)  loss_mask: 0.5976 (0.5847)  time: 0.1970  data: 0.0002  max mem: 5511
[06:27:45.751478] Epoch: [4]  [ 40/781]  eta: 0:02:40  lr: 0.000203  training_loss: 3.2302 (3.2016)  mae_loss: 0.5004 (0.4945)  classification_loss: 2.1295 (2.1201)  loss_mask: 0.5997 (0.5870)  time: 0.1974  data: 0.0003  max mem: 5511
[06:27:49.717821] Epoch: [4]  [ 60/781]  eta: 0:02:31  lr: 0.000204  training_loss: 3.2313 (3.2067)  mae_loss: 0.4683 (0.4895)  classification_loss: 2.1125 (2.1187)  loss_mask: 0.6096 (0.5985)  time: 0.1982  data: 0.0003  max mem: 5511
[06:27:53.680084] Epoch: [4]  [ 80/781]  eta: 0:02:25  lr: 0.000205  training_loss: 3.1845 (3.1946)  mae_loss: 0.4919 (0.4923)  classification_loss: 2.1420 (2.1227)  loss_mask: 0.5102 (0.5796)  time: 0.1980  data: 0.0003  max mem: 5511
[06:27:57.622538] Epoch: [4]  [100/781]  eta: 0:02:19  lr: 0.000206  training_loss: 3.2205 (3.2019)  mae_loss: 0.4613 (0.4898)  classification_loss: 2.1398 (2.1270)  loss_mask: 0.5833 (0.5851)  time: 0.1970  data: 0.0003  max mem: 5511
[06:28:01.558169] Epoch: [4]  [120/781]  eta: 0:02:14  lr: 0.000208  training_loss: 3.3927 (3.2323)  mae_loss: 0.4823 (0.4889)  classification_loss: 2.1066 (2.1265)  loss_mask: 0.7821 (0.6170)  time: 0.1967  data: 0.0002  max mem: 5511
[06:28:05.491049] Epoch: [4]  [140/781]  eta: 0:02:10  lr: 0.000209  training_loss: 3.2099 (3.2293)  mae_loss: 0.4845 (0.4866)  classification_loss: 2.0875 (2.1229)  loss_mask: 0.6090 (0.6198)  time: 0.1966  data: 0.0002  max mem: 5511
[06:28:09.422892] Epoch: [4]  [160/781]  eta: 0:02:05  lr: 0.000210  training_loss: 3.1913 (3.2255)  mae_loss: 0.4818 (0.4866)  classification_loss: 2.0980 (2.1187)  loss_mask: 0.6051 (0.6202)  time: 0.1965  data: 0.0002  max mem: 5511
[06:28:13.370241] Epoch: [4]  [180/781]  eta: 0:02:01  lr: 0.000212  training_loss: 3.3008 (3.2328)  mae_loss: 0.4597 (0.4859)  classification_loss: 2.1243 (2.1220)  loss_mask: 0.6176 (0.6249)  time: 0.1973  data: 0.0002  max mem: 5511
[06:28:17.308493] Epoch: [4]  [200/781]  eta: 0:01:56  lr: 0.000213  training_loss: 3.2678 (3.2359)  mae_loss: 0.4607 (0.4853)  classification_loss: 2.1333 (2.1239)  loss_mask: 0.6375 (0.6267)  time: 0.1968  data: 0.0002  max mem: 5511
[06:28:21.242304] Epoch: [4]  [220/781]  eta: 0:01:52  lr: 0.000214  training_loss: 3.1579 (3.2282)  mae_loss: 0.4815 (0.4846)  classification_loss: 2.0881 (2.1226)  loss_mask: 0.5637 (0.6210)  time: 0.1966  data: 0.0002  max mem: 5511
[06:28:25.216479] Epoch: [4]  [240/781]  eta: 0:01:48  lr: 0.000215  training_loss: 3.2025 (3.2260)  mae_loss: 0.5044 (0.4860)  classification_loss: 2.0961 (2.1214)  loss_mask: 0.5778 (0.6186)  time: 0.1986  data: 0.0002  max mem: 5511
[06:28:29.155768] Epoch: [4]  [260/781]  eta: 0:01:44  lr: 0.000217  training_loss: 3.2627 (3.2287)  mae_loss: 0.5001 (0.4861)  classification_loss: 2.1082 (2.1205)  loss_mask: 0.6239 (0.6221)  time: 0.1969  data: 0.0002  max mem: 5511
[06:28:33.100147] Epoch: [4]  [280/781]  eta: 0:01:40  lr: 0.000218  training_loss: 3.2145 (3.2286)  mae_loss: 0.4718 (0.4861)  classification_loss: 2.1145 (2.1209)  loss_mask: 0.5929 (0.6216)  time: 0.1971  data: 0.0002  max mem: 5511
[06:28:37.091216] Epoch: [4]  [300/781]  eta: 0:01:36  lr: 0.000219  training_loss: 3.2042 (3.2275)  mae_loss: 0.4672 (0.4858)  classification_loss: 2.1469 (2.1215)  loss_mask: 0.6060 (0.6202)  time: 0.1995  data: 0.0003  max mem: 5511
[06:28:41.027298] Epoch: [4]  [320/781]  eta: 0:01:32  lr: 0.000220  training_loss: 3.1077 (3.2211)  mae_loss: 0.4533 (0.4845)  classification_loss: 2.0892 (2.1195)  loss_mask: 0.5659 (0.6171)  time: 0.1967  data: 0.0002  max mem: 5511
[06:28:44.960142] Epoch: [4]  [340/781]  eta: 0:01:28  lr: 0.000222  training_loss: 3.2017 (3.2205)  mae_loss: 0.4726 (0.4844)  classification_loss: 2.1057 (2.1193)  loss_mask: 0.5731 (0.6169)  time: 0.1966  data: 0.0002  max mem: 5511
[06:28:48.889071] Epoch: [4]  [360/781]  eta: 0:01:23  lr: 0.000223  training_loss: 3.2266 (3.2192)  mae_loss: 0.4787 (0.4842)  classification_loss: 2.1139 (2.1196)  loss_mask: 0.6000 (0.6155)  time: 0.1963  data: 0.0003  max mem: 5511
[06:28:52.837436] Epoch: [4]  [380/781]  eta: 0:01:19  lr: 0.000224  training_loss: 3.1672 (3.2173)  mae_loss: 0.4722 (0.4841)  classification_loss: 2.1472 (2.1190)  loss_mask: 0.5850 (0.6141)  time: 0.1973  data: 0.0002  max mem: 5511
[06:28:56.782295] Epoch: [4]  [400/781]  eta: 0:01:15  lr: 0.000226  training_loss: 3.1263 (3.2142)  mae_loss: 0.4669 (0.4832)  classification_loss: 2.0719 (2.1176)  loss_mask: 0.5618 (0.6134)  time: 0.1972  data: 0.0002  max mem: 5511
[06:29:00.784039] Epoch: [4]  [420/781]  eta: 0:01:11  lr: 0.000227  training_loss: 3.1221 (3.2113)  mae_loss: 0.4538 (0.4820)  classification_loss: 2.0949 (2.1166)  loss_mask: 0.5982 (0.6127)  time: 0.2000  data: 0.0002  max mem: 5511
[06:29:04.759252] Epoch: [4]  [440/781]  eta: 0:01:07  lr: 0.000228  training_loss: 3.1968 (3.2106)  mae_loss: 0.4659 (0.4813)  classification_loss: 2.0791 (2.1156)  loss_mask: 0.6164 (0.6136)  time: 0.1987  data: 0.0002  max mem: 5511
[06:29:08.692031] Epoch: [4]  [460/781]  eta: 0:01:03  lr: 0.000229  training_loss: 3.1126 (3.2059)  mae_loss: 0.4463 (0.4806)  classification_loss: 2.0943 (2.1151)  loss_mask: 0.5262 (0.6101)  time: 0.1966  data: 0.0002  max mem: 5511
[06:29:12.638551] Epoch: [4]  [480/781]  eta: 0:00:59  lr: 0.000231  training_loss: 3.2321 (3.2062)  mae_loss: 0.4359 (0.4792)  classification_loss: 2.1426 (2.1160)  loss_mask: 0.6521 (0.6110)  time: 0.1972  data: 0.0002  max mem: 5511
[06:29:16.575042] Epoch: [4]  [500/781]  eta: 0:00:55  lr: 0.000232  training_loss: 3.2524 (3.2081)  mae_loss: 0.4561 (0.4787)  classification_loss: 2.0869 (2.1152)  loss_mask: 0.6585 (0.6143)  time: 0.1967  data: 0.0002  max mem: 5511
[06:29:20.520291] Epoch: [4]  [520/781]  eta: 0:00:51  lr: 0.000233  training_loss: 3.1784 (3.2065)  mae_loss: 0.4639 (0.4783)  classification_loss: 2.0593 (2.1138)  loss_mask: 0.5973 (0.6144)  time: 0.1972  data: 0.0002  max mem: 5511
[06:29:24.492655] Epoch: [4]  [540/781]  eta: 0:00:47  lr: 0.000235  training_loss: 3.1496 (3.2058)  mae_loss: 0.4817 (0.4787)  classification_loss: 2.1061 (2.1136)  loss_mask: 0.5936 (0.6135)  time: 0.1985  data: 0.0003  max mem: 5511
[06:29:28.433783] Epoch: [4]  [560/781]  eta: 0:00:43  lr: 0.000236  training_loss: 3.0554 (3.2019)  mae_loss: 0.4561 (0.4782)  classification_loss: 2.0709 (2.1121)  loss_mask: 0.5386 (0.6116)  time: 0.1970  data: 0.0002  max mem: 5511
[06:29:32.372239] Epoch: [4]  [580/781]  eta: 0:00:39  lr: 0.000237  training_loss: 3.1664 (3.2009)  mae_loss: 0.4703 (0.4776)  classification_loss: 2.0926 (2.1113)  loss_mask: 0.6362 (0.6120)  time: 0.1968  data: 0.0002  max mem: 5511
[06:29:36.329589] Epoch: [4]  [600/781]  eta: 0:00:35  lr: 0.000238  training_loss: 3.2383 (3.2033)  mae_loss: 0.4600 (0.4769)  classification_loss: 2.1053 (2.1112)  loss_mask: 0.6674 (0.6152)  time: 0.1978  data: 0.0002  max mem: 5511
[06:29:40.321871] Epoch: [4]  [620/781]  eta: 0:00:31  lr: 0.000240  training_loss: 3.1717 (3.2032)  mae_loss: 0.4756 (0.4768)  classification_loss: 2.0604 (2.1100)  loss_mask: 0.6350 (0.6164)  time: 0.1995  data: 0.0002  max mem: 5511
[06:29:44.282820] Epoch: [4]  [640/781]  eta: 0:00:28  lr: 0.000241  training_loss: 3.1242 (3.2003)  mae_loss: 0.4581 (0.4762)  classification_loss: 2.0626 (2.1090)  loss_mask: 0.5933 (0.6152)  time: 0.1980  data: 0.0002  max mem: 5511
[06:29:48.226736] Epoch: [4]  [660/781]  eta: 0:00:24  lr: 0.000242  training_loss: 3.0888 (3.1974)  mae_loss: 0.4795 (0.4759)  classification_loss: 2.1014 (2.1084)  loss_mask: 0.5675 (0.6132)  time: 0.1971  data: 0.0002  max mem: 5511
[06:29:52.159235] Epoch: [4]  [680/781]  eta: 0:00:20  lr: 0.000244  training_loss: 3.1096 (3.1948)  mae_loss: 0.4425 (0.4753)  classification_loss: 2.1183 (2.1083)  loss_mask: 0.5501 (0.6112)  time: 0.1965  data: 0.0002  max mem: 5511
[06:29:56.100995] Epoch: [4]  [700/781]  eta: 0:00:16  lr: 0.000245  training_loss: 3.2439 (3.1983)  mae_loss: 0.4662 (0.4755)  classification_loss: 2.0579 (2.1083)  loss_mask: 0.6811 (0.6145)  time: 0.1970  data: 0.0002  max mem: 5511
[06:30:00.045935] Epoch: [4]  [720/781]  eta: 0:00:12  lr: 0.000246  training_loss: 3.1520 (3.1972)  mae_loss: 0.4389 (0.4747)  classification_loss: 2.1224 (2.1090)  loss_mask: 0.5683 (0.6135)  time: 0.1971  data: 0.0003  max mem: 5511
[06:30:03.991333] Epoch: [4]  [740/781]  eta: 0:00:08  lr: 0.000247  training_loss: 3.1746 (3.1960)  mae_loss: 0.4635 (0.4745)  classification_loss: 2.1099 (2.1092)  loss_mask: 0.5532 (0.6123)  time: 0.1972  data: 0.0002  max mem: 5511
[06:30:07.957341] Epoch: [4]  [760/781]  eta: 0:00:04  lr: 0.000249  training_loss: 3.1356 (3.1945)  mae_loss: 0.4565 (0.4741)  classification_loss: 2.0957 (2.1092)  loss_mask: 0.5454 (0.6111)  time: 0.1982  data: 0.0002  max mem: 5511
[06:30:11.879136] Epoch: [4]  [780/781]  eta: 0:00:00  lr: 0.000250  training_loss: 3.0551 (3.1912)  mae_loss: 0.4598 (0.4740)  classification_loss: 2.0878 (2.1086)  loss_mask: 0.5161 (0.6087)  time: 0.1960  data: 0.0002  max mem: 5511
[06:30:12.019736] Epoch: [4] Total time: 0:02:35 (0.1987 s / it)
[06:30:12.020719] Averaged stats: lr: 0.000250  training_loss: 3.0551 (3.1912)  mae_loss: 0.4598 (0.4740)  classification_loss: 2.0878 (2.1086)  loss_mask: 0.5161 (0.6087)
[06:30:12.706720] Test:  [  0/157]  eta: 0:01:47  testing_loss: 1.7060 (1.7060)  acc1: 50.0000 (50.0000)  acc5: 85.9375 (85.9375)  time: 0.6822  data: 0.6362  max mem: 5511
[06:30:12.999099] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.7369 (1.7472)  acc1: 43.7500 (41.7614)  acc5: 89.0625 (87.9261)  time: 0.0884  data: 0.0580  max mem: 5511
[06:30:13.290045] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.7253 (1.7212)  acc1: 43.7500 (43.8244)  acc5: 89.0625 (88.6905)  time: 0.0290  data: 0.0002  max mem: 5511
[06:30:13.576139] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.7050 (1.7121)  acc1: 43.7500 (44.4052)  acc5: 87.5000 (88.3065)  time: 0.0287  data: 0.0002  max mem: 5511
[06:30:13.860406] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.7050 (1.7213)  acc1: 43.7500 (43.8643)  acc5: 84.3750 (87.6524)  time: 0.0284  data: 0.0002  max mem: 5511
[06:30:14.151576] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.7258 (1.7188)  acc1: 40.6250 (43.6275)  acc5: 87.5000 (88.0515)  time: 0.0286  data: 0.0002  max mem: 5511
[06:30:14.441725] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.7045 (1.7182)  acc1: 40.6250 (43.6475)  acc5: 89.0625 (88.2941)  time: 0.0289  data: 0.0002  max mem: 5511
[06:30:14.727128] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.6916 (1.7154)  acc1: 42.1875 (43.6620)  acc5: 89.0625 (88.3363)  time: 0.0287  data: 0.0002  max mem: 5511
[06:30:15.012186] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.7031 (1.7163)  acc1: 42.1875 (43.4221)  acc5: 89.0625 (88.4838)  time: 0.0284  data: 0.0002  max mem: 5511
[06:30:15.303102] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.7248 (1.7200)  acc1: 40.6250 (43.3551)  acc5: 89.0625 (88.4615)  time: 0.0287  data: 0.0002  max mem: 5511
[06:30:15.591060] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.7334 (1.7232)  acc1: 40.6250 (43.0384)  acc5: 87.5000 (88.3509)  time: 0.0288  data: 0.0002  max mem: 5511
[06:30:15.876098] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.7593 (1.7254)  acc1: 40.6250 (42.8632)  acc5: 87.5000 (88.3446)  time: 0.0285  data: 0.0002  max mem: 5511
[06:30:16.163388] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.7096 (1.7236)  acc1: 42.1875 (42.9365)  acc5: 89.0625 (88.5201)  time: 0.0285  data: 0.0002  max mem: 5511
[06:30:16.449439] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.7056 (1.7247)  acc1: 42.1875 (42.9986)  acc5: 89.0625 (88.4065)  time: 0.0285  data: 0.0002  max mem: 5511
[06:30:16.731395] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.7223 (1.7233)  acc1: 43.7500 (43.2957)  acc5: 89.0625 (88.4530)  time: 0.0282  data: 0.0002  max mem: 5511
[06:30:17.013632] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.7191 (1.7223)  acc1: 45.3125 (43.3775)  acc5: 89.0625 (88.4623)  time: 0.0281  data: 0.0001  max mem: 5511
[06:30:17.164222] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.7191 (1.7219)  acc1: 42.1875 (43.1700)  acc5: 87.5000 (88.4400)  time: 0.0271  data: 0.0001  max mem: 5511
[06:30:17.330522] Test: Total time: 0:00:05 (0.0338 s / it)
[06:30:17.331922] * Acc@1 43.170 Acc@5 88.440 loss 1.722
[06:30:17.332268] Accuracy of the network on the 10000 test images: 43.2%
[06:30:17.332446] Max accuracy: 43.17%
[06:30:17.435241] log_dir: ./output_dir
[06:30:18.267025] Epoch: [5]  [  0/781]  eta: 0:10:48  lr: 0.000250  training_loss: 2.9304 (2.9304)  mae_loss: 0.4810 (0.4810)  classification_loss: 1.9875 (1.9875)  loss_mask: 0.4620 (0.4620)  time: 0.8301  data: 0.6186  max mem: 5511
[06:30:22.240473] Epoch: [5]  [ 20/781]  eta: 0:02:53  lr: 0.000250  training_loss: 3.0845 (3.0786)  mae_loss: 0.4533 (0.4663)  classification_loss: 2.0617 (2.0547)  loss_mask: 0.5481 (0.5577)  time: 0.1986  data: 0.0002  max mem: 5511
[06:30:26.212495] Epoch: [5]  [ 40/781]  eta: 0:02:38  lr: 0.000250  training_loss: 3.2588 (3.1792)  mae_loss: 0.4855 (0.4740)  classification_loss: 2.1387 (2.0934)  loss_mask: 0.6234 (0.6118)  time: 0.1985  data: 0.0003  max mem: 5511
[06:30:30.195577] Epoch: [5]  [ 60/781]  eta: 0:02:30  lr: 0.000250  training_loss: 3.0807 (3.1496)  mae_loss: 0.4655 (0.4704)  classification_loss: 2.0682 (2.0873)  loss_mask: 0.5369 (0.5918)  time: 0.1991  data: 0.0002  max mem: 5511
[06:30:34.165062] Epoch: [5]  [ 80/781]  eta: 0:02:24  lr: 0.000250  training_loss: 3.0108 (3.1164)  mae_loss: 0.4450 (0.4668)  classification_loss: 2.0478 (2.0826)  loss_mask: 0.4726 (0.5670)  time: 0.1984  data: 0.0003  max mem: 5511
[06:30:38.130434] Epoch: [5]  [100/781]  eta: 0:02:19  lr: 0.000250  training_loss: 3.0643 (3.1103)  mae_loss: 0.4436 (0.4649)  classification_loss: 2.0786 (2.0829)  loss_mask: 0.5310 (0.5625)  time: 0.1982  data: 0.0002  max mem: 5511
[06:30:42.066342] Epoch: [5]  [120/781]  eta: 0:02:14  lr: 0.000250  training_loss: 3.0396 (3.0997)  mae_loss: 0.4413 (0.4606)  classification_loss: 2.0665 (2.0799)  loss_mask: 0.5261 (0.5592)  time: 0.1967  data: 0.0004  max mem: 5511
[06:30:46.024032] Epoch: [5]  [140/781]  eta: 0:02:09  lr: 0.000250  training_loss: 3.0217 (3.0854)  mae_loss: 0.4491 (0.4591)  classification_loss: 2.0178 (2.0752)  loss_mask: 0.4889 (0.5511)  time: 0.1978  data: 0.0002  max mem: 5511
[06:30:50.006185] Epoch: [5]  [160/781]  eta: 0:02:05  lr: 0.000250  training_loss: 3.0247 (3.0813)  mae_loss: 0.4489 (0.4577)  classification_loss: 2.0935 (2.0757)  loss_mask: 0.4967 (0.5480)  time: 0.1990  data: 0.0002  max mem: 5511
[06:30:53.962954] Epoch: [5]  [180/781]  eta: 0:02:01  lr: 0.000250  training_loss: 3.1472 (3.0912)  mae_loss: 0.4542 (0.4575)  classification_loss: 2.0825 (2.0763)  loss_mask: 0.6236 (0.5573)  time: 0.1978  data: 0.0002  max mem: 5511
[06:30:57.954562] Epoch: [5]  [200/781]  eta: 0:01:57  lr: 0.000250  training_loss: 3.1681 (3.1007)  mae_loss: 0.4576 (0.4585)  classification_loss: 2.1022 (2.0795)  loss_mask: 0.5813 (0.5627)  time: 0.1995  data: 0.0003  max mem: 5511
[06:31:01.908184] Epoch: [5]  [220/781]  eta: 0:01:52  lr: 0.000250  training_loss: 3.0366 (3.0959)  mae_loss: 0.4602 (0.4590)  classification_loss: 2.0927 (2.0799)  loss_mask: 0.4789 (0.5570)  time: 0.1976  data: 0.0004  max mem: 5511
[06:31:05.908098] Epoch: [5]  [240/781]  eta: 0:01:48  lr: 0.000250  training_loss: 3.0164 (3.0941)  mae_loss: 0.4682 (0.4593)  classification_loss: 2.0903 (2.0809)  loss_mask: 0.4879 (0.5539)  time: 0.1999  data: 0.0003  max mem: 5511
[06:31:09.871564] Epoch: [5]  [260/781]  eta: 0:01:44  lr: 0.000250  training_loss: 2.9720 (3.0895)  mae_loss: 0.4609 (0.4594)  classification_loss: 2.0293 (2.0787)  loss_mask: 0.4718 (0.5515)  time: 0.1981  data: 0.0002  max mem: 5511
[06:31:13.805634] Epoch: [5]  [280/781]  eta: 0:01:40  lr: 0.000250  training_loss: 3.0688 (3.0889)  mae_loss: 0.4667 (0.4599)  classification_loss: 2.0624 (2.0775)  loss_mask: 0.5217 (0.5514)  time: 0.1966  data: 0.0002  max mem: 5511
[06:31:17.752918] Epoch: [5]  [300/781]  eta: 0:01:36  lr: 0.000250  training_loss: 3.1810 (3.0953)  mae_loss: 0.4393 (0.4587)  classification_loss: 2.1158 (2.0795)  loss_mask: 0.5838 (0.5571)  time: 0.1973  data: 0.0003  max mem: 5511
[06:31:21.739807] Epoch: [5]  [320/781]  eta: 0:01:32  lr: 0.000250  training_loss: 3.1004 (3.0944)  mae_loss: 0.4732 (0.4594)  classification_loss: 2.0929 (2.0802)  loss_mask: 0.5404 (0.5548)  time: 0.1992  data: 0.0002  max mem: 5511
[06:31:25.681775] Epoch: [5]  [340/781]  eta: 0:01:28  lr: 0.000250  training_loss: 2.9218 (3.0851)  mae_loss: 0.4452 (0.4587)  classification_loss: 2.0518 (2.0793)  loss_mask: 0.4258 (0.5471)  time: 0.1970  data: 0.0003  max mem: 5511
[06:31:29.693390] Epoch: [5]  [360/781]  eta: 0:01:24  lr: 0.000250  training_loss: 3.0836 (3.0835)  mae_loss: 0.4676 (0.4592)  classification_loss: 2.0764 (2.0790)  loss_mask: 0.5165 (0.5453)  time: 0.2005  data: 0.0003  max mem: 5511
[06:31:33.648421] Epoch: [5]  [380/781]  eta: 0:01:20  lr: 0.000250  training_loss: 3.0941 (3.0848)  mae_loss: 0.4298 (0.4589)  classification_loss: 2.0613 (2.0782)  loss_mask: 0.5443 (0.5477)  time: 0.1977  data: 0.0002  max mem: 5511
[06:31:37.585169] Epoch: [5]  [400/781]  eta: 0:01:16  lr: 0.000250  training_loss: 2.9896 (3.0819)  mae_loss: 0.4511 (0.4583)  classification_loss: 2.0766 (2.0776)  loss_mask: 0.4970 (0.5459)  time: 0.1968  data: 0.0003  max mem: 5511
[06:31:41.555682] Epoch: [5]  [420/781]  eta: 0:01:12  lr: 0.000250  training_loss: 2.9991 (3.0784)  mae_loss: 0.4573 (0.4587)  classification_loss: 2.0570 (2.0774)  loss_mask: 0.4595 (0.5422)  time: 0.1985  data: 0.0003  max mem: 5511
[06:31:45.493875] Epoch: [5]  [440/781]  eta: 0:01:08  lr: 0.000250  training_loss: 3.0194 (3.0781)  mae_loss: 0.4648 (0.4587)  classification_loss: 2.0405 (2.0760)  loss_mask: 0.5540 (0.5435)  time: 0.1968  data: 0.0002  max mem: 5511
[06:31:49.455640] Epoch: [5]  [460/781]  eta: 0:01:04  lr: 0.000250  training_loss: 3.0477 (3.0769)  mae_loss: 0.4388 (0.4579)  classification_loss: 2.0596 (2.0751)  loss_mask: 0.5583 (0.5440)  time: 0.1980  data: 0.0002  max mem: 5511
[06:31:53.411130] Epoch: [5]  [480/781]  eta: 0:01:00  lr: 0.000250  training_loss: 3.0586 (3.0772)  mae_loss: 0.4493 (0.4576)  classification_loss: 2.0818 (2.0757)  loss_mask: 0.5190 (0.5438)  time: 0.1977  data: 0.0002  max mem: 5511
[06:31:57.354371] Epoch: [5]  [500/781]  eta: 0:00:56  lr: 0.000250  training_loss: 3.1104 (3.0781)  mae_loss: 0.4761 (0.4584)  classification_loss: 2.0378 (2.0750)  loss_mask: 0.5478 (0.5447)  time: 0.1971  data: 0.0002  max mem: 5511
[06:32:01.308910] Epoch: [5]  [520/781]  eta: 0:00:52  lr: 0.000250  training_loss: 3.0596 (3.0782)  mae_loss: 0.4443 (0.4578)  classification_loss: 2.0515 (2.0743)  loss_mask: 0.5871 (0.5460)  time: 0.1976  data: 0.0002  max mem: 5511
[06:32:05.240636] Epoch: [5]  [540/781]  eta: 0:00:48  lr: 0.000250  training_loss: 3.0526 (3.0773)  mae_loss: 0.4462 (0.4575)  classification_loss: 2.0452 (2.0743)  loss_mask: 0.5275 (0.5455)  time: 0.1965  data: 0.0002  max mem: 5511
[06:32:09.171248] Epoch: [5]  [560/781]  eta: 0:00:43  lr: 0.000250  training_loss: 3.0618 (3.0764)  mae_loss: 0.4683 (0.4577)  classification_loss: 2.0261 (2.0730)  loss_mask: 0.5283 (0.5458)  time: 0.1965  data: 0.0002  max mem: 5511
[06:32:13.100351] Epoch: [5]  [580/781]  eta: 0:00:39  lr: 0.000250  training_loss: 2.9266 (3.0719)  mae_loss: 0.4472 (0.4573)  classification_loss: 2.0443 (2.0725)  loss_mask: 0.4433 (0.5421)  time: 0.1964  data: 0.0002  max mem: 5511
[06:32:17.027530] Epoch: [5]  [600/781]  eta: 0:00:35  lr: 0.000250  training_loss: 3.0592 (3.0721)  mae_loss: 0.4618 (0.4575)  classification_loss: 2.0603 (2.0723)  loss_mask: 0.4812 (0.5422)  time: 0.1963  data: 0.0003  max mem: 5511
[06:32:20.992416] Epoch: [5]  [620/781]  eta: 0:00:32  lr: 0.000250  training_loss: 2.9774 (3.0695)  mae_loss: 0.4711 (0.4577)  classification_loss: 2.0352 (2.0714)  loss_mask: 0.4641 (0.5404)  time: 0.1982  data: 0.0002  max mem: 5511
[06:32:24.911568] Epoch: [5]  [640/781]  eta: 0:00:28  lr: 0.000250  training_loss: 2.9740 (3.0687)  mae_loss: 0.4603 (0.4578)  classification_loss: 2.0557 (2.0709)  loss_mask: 0.5015 (0.5400)  time: 0.1959  data: 0.0002  max mem: 5511
[06:32:28.861566] Epoch: [5]  [660/781]  eta: 0:00:24  lr: 0.000250  training_loss: 2.9844 (3.0671)  mae_loss: 0.4519 (0.4577)  classification_loss: 2.0518 (2.0703)  loss_mask: 0.4912 (0.5392)  time: 0.1974  data: 0.0002  max mem: 5511
[06:32:32.806921] Epoch: [5]  [680/781]  eta: 0:00:20  lr: 0.000250  training_loss: 3.0812 (3.0675)  mae_loss: 0.4730 (0.4582)  classification_loss: 2.0316 (2.0696)  loss_mask: 0.5085 (0.5397)  time: 0.1971  data: 0.0003  max mem: 5511
[06:32:36.730156] Epoch: [5]  [700/781]  eta: 0:00:16  lr: 0.000250  training_loss: 2.9891 (3.0658)  mae_loss: 0.4742 (0.4584)  classification_loss: 2.0560 (2.0690)  loss_mask: 0.4847 (0.5384)  time: 0.1961  data: 0.0003  max mem: 5511
[06:32:40.681739] Epoch: [5]  [720/781]  eta: 0:00:12  lr: 0.000250  training_loss: 2.9399 (3.0629)  mae_loss: 0.4373 (0.4579)  classification_loss: 2.0579 (2.0679)  loss_mask: 0.4780 (0.5371)  time: 0.1975  data: 0.0003  max mem: 5511
[06:32:44.658713] Epoch: [5]  [740/781]  eta: 0:00:08  lr: 0.000250  training_loss: 2.9489 (3.0603)  mae_loss: 0.4737 (0.4579)  classification_loss: 2.0616 (2.0677)  loss_mask: 0.4198 (0.5347)  time: 0.1988  data: 0.0002  max mem: 5511
[06:32:48.605638] Epoch: [5]  [760/781]  eta: 0:00:04  lr: 0.000250  training_loss: 3.0168 (3.0594)  mae_loss: 0.4581 (0.4580)  classification_loss: 2.0236 (2.0670)  loss_mask: 0.4866 (0.5344)  time: 0.1973  data: 0.0002  max mem: 5511
[06:32:52.528623] Epoch: [5]  [780/781]  eta: 0:00:00  lr: 0.000250  training_loss: 3.0125 (3.0584)  mae_loss: 0.4441 (0.4578)  classification_loss: 2.0391 (2.0668)  loss_mask: 0.4755 (0.5338)  time: 0.1961  data: 0.0002  max mem: 5511
[06:32:52.694362] Epoch: [5] Total time: 0:02:35 (0.1988 s / it)
[06:32:52.694823] Averaged stats: lr: 0.000250  training_loss: 3.0125 (3.0584)  mae_loss: 0.4441 (0.4578)  classification_loss: 2.0391 (2.0668)  loss_mask: 0.4755 (0.5338)
[06:32:53.414298] Test:  [  0/157]  eta: 0:01:52  testing_loss: 1.6031 (1.6031)  acc1: 51.5625 (51.5625)  acc5: 90.6250 (90.6250)  time: 0.7151  data: 0.6837  max mem: 5511
[06:32:53.699629] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.6031 (1.6271)  acc1: 45.3125 (43.0398)  acc5: 92.1875 (91.4773)  time: 0.0908  data: 0.0623  max mem: 5511
[06:32:53.983763] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.5910 (1.6044)  acc1: 45.3125 (46.7262)  acc5: 92.1875 (91.4435)  time: 0.0283  data: 0.0002  max mem: 5511
[06:32:54.267891] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.5815 (1.6021)  acc1: 48.4375 (47.5302)  acc5: 90.6250 (91.2298)  time: 0.0283  data: 0.0001  max mem: 5511
[06:32:54.553868] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.6096 (1.6131)  acc1: 46.8750 (46.9131)  acc5: 90.6250 (90.7393)  time: 0.0283  data: 0.0002  max mem: 5511
[06:32:54.841838] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.6218 (1.6131)  acc1: 43.7500 (47.0895)  acc5: 90.6250 (90.8701)  time: 0.0285  data: 0.0002  max mem: 5511
[06:32:55.134257] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.5980 (1.6106)  acc1: 48.4375 (47.1568)  acc5: 90.6250 (90.8811)  time: 0.0288  data: 0.0002  max mem: 5511
[06:32:55.418546] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.5712 (1.6068)  acc1: 48.4375 (47.5352)  acc5: 92.1875 (91.1532)  time: 0.0287  data: 0.0002  max mem: 5511
[06:32:55.701745] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.5712 (1.6074)  acc1: 48.4375 (47.6080)  acc5: 92.1875 (91.2230)  time: 0.0282  data: 0.0001  max mem: 5511
[06:32:55.986070] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.6185 (1.6098)  acc1: 45.3125 (47.5103)  acc5: 90.6250 (91.1229)  time: 0.0282  data: 0.0001  max mem: 5511
[06:32:56.273477] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.6601 (1.6145)  acc1: 43.7500 (46.9678)  acc5: 90.6250 (91.0118)  time: 0.0284  data: 0.0002  max mem: 5511
[06:32:56.563899] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.6542 (1.6167)  acc1: 43.7500 (46.8891)  acc5: 90.6250 (91.0191)  time: 0.0288  data: 0.0002  max mem: 5511
[06:32:56.856297] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.6113 (1.6155)  acc1: 46.8750 (47.0041)  acc5: 90.6250 (91.0899)  time: 0.0289  data: 0.0002  max mem: 5511
[06:32:57.145468] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.6209 (1.6185)  acc1: 48.4375 (46.9704)  acc5: 90.6250 (90.9948)  time: 0.0289  data: 0.0003  max mem: 5511
[06:32:57.430475] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.6251 (1.6195)  acc1: 50.0000 (47.2074)  acc5: 89.0625 (90.9353)  time: 0.0285  data: 0.0002  max mem: 5511
[06:32:57.712030] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.6117 (1.6182)  acc1: 50.0000 (47.3924)  acc5: 89.0625 (90.8940)  time: 0.0282  data: 0.0001  max mem: 5511
[06:32:57.863669] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.5788 (1.6174)  acc1: 46.8750 (47.3200)  acc5: 90.6250 (90.9000)  time: 0.0271  data: 0.0001  max mem: 5511
[06:32:58.033967] Test: Total time: 0:00:05 (0.0340 s / it)
[06:32:58.034399] * Acc@1 47.320 Acc@5 90.900 loss 1.617
[06:32:58.034711] Accuracy of the network on the 10000 test images: 47.3%
[06:32:58.034891] Max accuracy: 47.32%
[06:32:58.139510] log_dir: ./output_dir
[06:32:59.098794] Epoch: [6]  [  0/781]  eta: 0:12:27  lr: 0.000250  training_loss: 2.9875 (2.9875)  mae_loss: 0.4852 (0.4852)  classification_loss: 2.0917 (2.0917)  loss_mask: 0.4105 (0.4105)  time: 0.9575  data: 0.7148  max mem: 5511
[06:33:03.043883] Epoch: [6]  [ 20/781]  eta: 0:02:57  lr: 0.000250  training_loss: 2.9609 (2.9559)  mae_loss: 0.4509 (0.4525)  classification_loss: 2.0311 (2.0451)  loss_mask: 0.4744 (0.4584)  time: 0.1972  data: 0.0002  max mem: 5511
[06:33:06.983716] Epoch: [6]  [ 40/781]  eta: 0:02:39  lr: 0.000250  training_loss: 2.9178 (2.9367)  mae_loss: 0.4602 (0.4566)  classification_loss: 2.0144 (2.0362)  loss_mask: 0.4349 (0.4438)  time: 0.1969  data: 0.0002  max mem: 5511
[06:33:10.942569] Epoch: [6]  [ 60/781]  eta: 0:02:31  lr: 0.000250  training_loss: 2.9330 (2.9438)  mae_loss: 0.4625 (0.4596)  classification_loss: 2.0169 (2.0394)  loss_mask: 0.4232 (0.4447)  time: 0.1979  data: 0.0002  max mem: 5511
[06:33:14.874172] Epoch: [6]  [ 80/781]  eta: 0:02:24  lr: 0.000250  training_loss: 3.0797 (2.9808)  mae_loss: 0.4656 (0.4605)  classification_loss: 2.0544 (2.0445)  loss_mask: 0.5087 (0.4758)  time: 0.1965  data: 0.0003  max mem: 5511
[06:33:18.814982] Epoch: [6]  [100/781]  eta: 0:02:19  lr: 0.000250  training_loss: 3.0799 (3.0100)  mae_loss: 0.4463 (0.4603)  classification_loss: 2.0334 (2.0462)  loss_mask: 0.5674 (0.5034)  time: 0.1969  data: 0.0002  max mem: 5511

[06:33:22.744774] Epoch: [6]  [120/781]  eta: 0:02:14  lr: 0.000250  training_loss: 3.0423 (3.0165)  mae_loss: 0.4575 (0.4587)  classification_loss: 2.0240 (2.0448)  loss_mask: 0.5355 (0.5130)  time: 0.1964  data: 0.0002  max mem: 5511
[06:33:26.704195] Epoch: [6]  [140/781]  eta: 0:02:09  lr: 0.000250  training_loss: 2.9224 (3.0033)  mae_loss: 0.4346 (0.4552)  classification_loss: 2.0200 (2.0414)  loss_mask: 0.4367 (0.5067)  time: 0.1979  data: 0.0002  max mem: 5511
[06:33:30.679700] Epoch: [6]  [160/781]  eta: 0:02:05  lr: 0.000250  training_loss: 2.8919 (2.9951)  mae_loss: 0.4438 (0.4540)  classification_loss: 2.0719 (2.0444)  loss_mask: 0.4341 (0.4967)  time: 0.1986  data: 0.0002  max mem: 5511
[06:33:34.622294] Epoch: [6]  [180/781]  eta: 0:02:01  lr: 0.000250  training_loss: 2.9078 (2.9842)  mae_loss: 0.4253 (0.4525)  classification_loss: 2.0480 (2.0426)  loss_mask: 0.4348 (0.4891)  time: 0.1970  data: 0.0002  max mem: 5511
[06:33:38.575229] Epoch: [6]  [200/781]  eta: 0:01:56  lr: 0.000250  training_loss: 2.9179 (2.9829)  mae_loss: 0.4525 (0.4537)  classification_loss: 2.0446 (2.0422)  loss_mask: 0.4827 (0.4871)  time: 0.1975  data: 0.0002  max mem: 5511
[06:33:42.541971] Epoch: [6]  [220/781]  eta: 0:01:52  lr: 0.000250  training_loss: 3.0082 (2.9826)  mae_loss: 0.4363 (0.4532)  classification_loss: 2.0389 (2.0417)  loss_mask: 0.4774 (0.4877)  time: 0.1982  data: 0.0002  max mem: 5511
[06:33:46.501178] Epoch: [6]  [240/781]  eta: 0:01:48  lr: 0.000250  training_loss: 2.9654 (2.9818)  mae_loss: 0.4464 (0.4530)  classification_loss: 2.0589 (2.0433)  loss_mask: 0.4315 (0.4855)  time: 0.1979  data: 0.0002  max mem: 5511
[06:33:50.453407] Epoch: [6]  [260/781]  eta: 0:01:44  lr: 0.000250  training_loss: 2.9051 (2.9764)  mae_loss: 0.4359 (0.4523)  classification_loss: 2.0281 (2.0412)  loss_mask: 0.4541 (0.4829)  time: 0.1975  data: 0.0002  max mem: 5511
[06:33:54.408079] Epoch: [6]  [280/781]  eta: 0:01:40  lr: 0.000250  training_loss: 2.9571 (2.9736)  mae_loss: 0.4469 (0.4542)  classification_loss: 1.9925 (2.0395)  loss_mask: 0.4265 (0.4800)  time: 0.1976  data: 0.0002  max mem: 5511
[06:33:58.410119] Epoch: [6]  [300/781]  eta: 0:01:36  lr: 0.000250  training_loss: 3.0113 (2.9763)  mae_loss: 0.4521 (0.4544)  classification_loss: 2.0585 (2.0414)  loss_mask: 0.4495 (0.4805)  time: 0.2000  data: 0.0002  max mem: 5511
[06:34:02.377787] Epoch: [6]  [320/781]  eta: 0:01:32  lr: 0.000250  training_loss: 2.8284 (2.9698)  mae_loss: 0.4391 (0.4539)  classification_loss: 1.9965 (2.0394)  loss_mask: 0.3798 (0.4765)  time: 0.1983  data: 0.0002  max mem: 5511
[06:34:06.351193] Epoch: [6]  [340/781]  eta: 0:01:28  lr: 0.000250  training_loss: 2.9422 (2.9679)  mae_loss: 0.4675 (0.4547)  classification_loss: 1.9944 (2.0362)  loss_mask: 0.4649 (0.4770)  time: 0.1986  data: 0.0002  max mem: 5511
[06:34:10.303327] Epoch: [6]  [360/781]  eta: 0:01:24  lr: 0.000250  training_loss: 3.0357 (2.9732)  mae_loss: 0.4544 (0.4550)  classification_loss: 2.0100 (2.0363)  loss_mask: 0.5663 (0.4818)  time: 0.1975  data: 0.0002  max mem: 5511
[06:34:14.266277] Epoch: [6]  [380/781]  eta: 0:01:20  lr: 0.000250  training_loss: 2.9938 (2.9748)  mae_loss: 0.4590 (0.4553)  classification_loss: 2.0341 (2.0368)  loss_mask: 0.4906 (0.4827)  time: 0.1980  data: 0.0002  max mem: 5511
[06:34:18.211236] Epoch: [6]  [400/781]  eta: 0:01:16  lr: 0.000250  training_loss: 2.9070 (2.9720)  mae_loss: 0.4436 (0.4549)  classification_loss: 2.0297 (2.0372)  loss_mask: 0.4292 (0.4799)  time: 0.1972  data: 0.0002  max mem: 5511
[06:34:22.175039] Epoch: [6]  [420/781]  eta: 0:01:12  lr: 0.000250  training_loss: 2.8802 (2.9684)  mae_loss: 0.4541 (0.4549)  classification_loss: 2.0196 (2.0367)  loss_mask: 0.4020 (0.4768)  time: 0.1981  data: 0.0002  max mem: 5511
[06:34:26.137533] Epoch: [6]  [440/781]  eta: 0:01:08  lr: 0.000250  training_loss: 2.7655 (2.9605)  mae_loss: 0.4490 (0.4544)  classification_loss: 1.9846 (2.0344)  loss_mask: 0.3589 (0.4717)  time: 0.1980  data: 0.0002  max mem: 5511
[06:34:30.103237] Epoch: [6]  [460/781]  eta: 0:01:04  lr: 0.000250  training_loss: 2.8060 (2.9554)  mae_loss: 0.4298 (0.4534)  classification_loss: 1.9755 (2.0323)  loss_mask: 0.3919 (0.4697)  time: 0.1982  data: 0.0004  max mem: 5511
[06:34:34.044410] Epoch: [6]  [480/781]  eta: 0:00:59  lr: 0.000250  training_loss: 2.8956 (2.9518)  mae_loss: 0.4456 (0.4535)  classification_loss: 2.0497 (2.0318)  loss_mask: 0.3735 (0.4665)  time: 0.1970  data: 0.0002  max mem: 5511
[06:34:38.002917] Epoch: [6]  [500/781]  eta: 0:00:55  lr: 0.000250  training_loss: 2.8785 (2.9504)  mae_loss: 0.4505 (0.4537)  classification_loss: 2.0112 (2.0318)  loss_mask: 0.4126 (0.4649)  time: 0.1978  data: 0.0002  max mem: 5511
[06:34:41.942173] Epoch: [6]  [520/781]  eta: 0:00:51  lr: 0.000250  training_loss: 2.8039 (2.9464)  mae_loss: 0.4500 (0.4536)  classification_loss: 2.0005 (2.0314)  loss_mask: 0.3834 (0.4614)  time: 0.1969  data: 0.0002  max mem: 5511
[06:34:45.900367] Epoch: [6]  [540/781]  eta: 0:00:47  lr: 0.000250  training_loss: 2.9064 (2.9451)  mae_loss: 0.4754 (0.4544)  classification_loss: 2.0155 (2.0316)  loss_mask: 0.3756 (0.4592)  time: 0.1978  data: 0.0002  max mem: 5511
[06:34:49.859812] Epoch: [6]  [560/781]  eta: 0:00:43  lr: 0.000250  training_loss: 2.8528 (2.9424)  mae_loss: 0.4866 (0.4553)  classification_loss: 2.0154 (2.0308)  loss_mask: 0.3593 (0.4563)  time: 0.1979  data: 0.0002  max mem: 5511
[06:34:53.820179] Epoch: [6]  [580/781]  eta: 0:00:40  lr: 0.000250  training_loss: 2.8193 (2.9388)  mae_loss: 0.4798 (0.4561)  classification_loss: 2.0105 (2.0298)  loss_mask: 0.3510 (0.4528)  time: 0.1979  data: 0.0002  max mem: 5511
[06:34:57.766993] Epoch: [6]  [600/781]  eta: 0:00:36  lr: 0.000250  training_loss: 2.8037 (2.9353)  mae_loss: 0.4616 (0.4563)  classification_loss: 2.0157 (2.0291)  loss_mask: 0.3447 (0.4499)  time: 0.1973  data: 0.0002  max mem: 5511
[06:35:01.729694] Epoch: [6]  [620/781]  eta: 0:00:32  lr: 0.000250  training_loss: 2.8257 (2.9329)  mae_loss: 0.4572 (0.4562)  classification_loss: 1.9740 (2.0277)  loss_mask: 0.4100 (0.4489)  time: 0.1980  data: 0.0002  max mem: 5511
[06:35:05.680248] Epoch: [6]  [640/781]  eta: 0:00:28  lr: 0.000250  training_loss: 2.8765 (2.9320)  mae_loss: 0.4520 (0.4562)  classification_loss: 2.0449 (2.0279)  loss_mask: 0.3789 (0.4478)  time: 0.1974  data: 0.0002  max mem: 5511
[06:35:09.632973] Epoch: [6]  [660/781]  eta: 0:00:24  lr: 0.000250  training_loss: 2.8560 (2.9303)  mae_loss: 0.4474 (0.4561)  classification_loss: 2.0350 (2.0282)  loss_mask: 0.3704 (0.4461)  time: 0.1975  data: 0.0002  max mem: 5511
[06:35:13.611963] Epoch: [6]  [680/781]  eta: 0:00:20  lr: 0.000250  training_loss: 2.8980 (2.9293)  mae_loss: 0.4328 (0.4556)  classification_loss: 1.9873 (2.0271)  loss_mask: 0.4574 (0.4466)  time: 0.1988  data: 0.0003  max mem: 5511
[06:35:17.556009] Epoch: [6]  [700/781]  eta: 0:00:16  lr: 0.000250  training_loss: 2.7577 (2.9256)  mae_loss: 0.4454 (0.4554)  classification_loss: 1.9898 (2.0268)  loss_mask: 0.3173 (0.4434)  time: 0.1971  data: 0.0002  max mem: 5511
[06:35:21.500731] Epoch: [6]  [720/781]  eta: 0:00:12  lr: 0.000250  training_loss: 2.8298 (2.9228)  mae_loss: 0.4437 (0.4551)  classification_loss: 2.0344 (2.0271)  loss_mask: 0.3436 (0.4407)  time: 0.1972  data: 0.0002  max mem: 5511
[06:35:25.439773] Epoch: [6]  [740/781]  eta: 0:00:08  lr: 0.000250  training_loss: 2.7528 (2.9180)  mae_loss: 0.4438 (0.4547)  classification_loss: 1.9869 (2.0262)  loss_mask: 0.3212 (0.4372)  time: 0.1969  data: 0.0003  max mem: 5511
[06:35:29.384321] Epoch: [6]  [760/781]  eta: 0:00:04  lr: 0.000250  training_loss: 2.7936 (2.9160)  mae_loss: 0.4443 (0.4546)  classification_loss: 2.0049 (2.0260)  loss_mask: 0.3538 (0.4354)  time: 0.1971  data: 0.0002  max mem: 5511
[06:35:33.311984] Epoch: [6]  [780/781]  eta: 0:00:00  lr: 0.000250  training_loss: 2.7878 (2.9133)  mae_loss: 0.4512 (0.4546)  classification_loss: 2.0182 (2.0257)  loss_mask: 0.3364 (0.4330)  time: 0.1963  data: 0.0002  max mem: 5511
[06:35:33.455273] Epoch: [6] Total time: 0:02:35 (0.1989 s / it)
[06:35:33.455742] Averaged stats: lr: 0.000250  training_loss: 2.7878 (2.9133)  mae_loss: 0.4512 (0.4546)  classification_loss: 2.0182 (2.0257)  loss_mask: 0.3364 (0.4330)
[06:35:34.035365] Test:  [  0/157]  eta: 0:01:30  testing_loss: 1.5455 (1.5455)  acc1: 51.5625 (51.5625)  acc5: 92.1875 (92.1875)  time: 0.5745  data: 0.5421  max mem: 5511
[06:35:34.329149] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 1.5940 (1.6100)  acc1: 45.3125 (45.0284)  acc5: 90.6250 (90.4830)  time: 0.0787  data: 0.0495  max mem: 5511
[06:35:34.613875] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.5692 (1.5864)  acc1: 46.8750 (47.9911)  acc5: 90.6250 (91.1458)  time: 0.0288  data: 0.0002  max mem: 5511
[06:35:34.899362] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 1.5638 (1.5816)  acc1: 50.0000 (48.0343)  acc5: 92.1875 (91.2298)  time: 0.0283  data: 0.0002  max mem: 5511
[06:35:35.183227] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 1.5647 (1.5908)  acc1: 48.4375 (47.4085)  acc5: 90.6250 (91.0442)  time: 0.0283  data: 0.0002  max mem: 5511
[06:35:35.468651] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.5902 (1.5879)  acc1: 48.4375 (47.9779)  acc5: 90.6250 (91.0846)  time: 0.0283  data: 0.0002  max mem: 5511
[06:35:35.752943] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.5490 (1.5818)  acc1: 50.0000 (48.3350)  acc5: 92.1875 (91.1373)  time: 0.0283  data: 0.0002  max mem: 5511
[06:35:36.037704] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.5449 (1.5824)  acc1: 50.0000 (48.4155)  acc5: 92.1875 (91.0651)  time: 0.0283  data: 0.0002  max mem: 5511
[06:35:36.326471] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.5717 (1.5829)  acc1: 48.4375 (48.4375)  acc5: 92.1875 (91.2230)  time: 0.0285  data: 0.0002  max mem: 5511
[06:35:36.614822] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.5816 (1.5848)  acc1: 50.0000 (48.6264)  acc5: 90.6250 (91.0027)  time: 0.0287  data: 0.0002  max mem: 5511
[06:35:36.903349] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.6486 (1.5903)  acc1: 45.3125 (48.1281)  acc5: 90.6250 (90.9344)  time: 0.0287  data: 0.0002  max mem: 5511
[06:35:37.191257] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.6502 (1.5938)  acc1: 42.1875 (47.7759)  acc5: 90.6250 (90.9628)  time: 0.0287  data: 0.0002  max mem: 5511
[06:35:37.479993] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.5968 (1.5916)  acc1: 46.8750 (47.8693)  acc5: 92.1875 (91.0770)  time: 0.0287  data: 0.0002  max mem: 5511
[06:35:37.765730] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.5711 (1.5939)  acc1: 46.8750 (47.8173)  acc5: 92.1875 (91.0305)  time: 0.0286  data: 0.0002  max mem: 5511
[06:35:38.056256] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.5870 (1.5938)  acc1: 48.4375 (47.9388)  acc5: 90.6250 (90.9685)  time: 0.0286  data: 0.0002  max mem: 5511
[06:35:38.340671] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.5877 (1.5927)  acc1: 48.4375 (47.8787)  acc5: 89.0625 (90.9147)  time: 0.0286  data: 0.0002  max mem: 5511
[06:35:38.494639] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.5877 (1.5933)  acc1: 45.3125 (47.8300)  acc5: 90.6250 (90.9300)  time: 0.0275  data: 0.0002  max mem: 5511
[06:35:38.674984] Test: Total time: 0:00:05 (0.0332 s / it)
[06:35:38.675633] * Acc@1 47.830 Acc@5 90.930 loss 1.593
[06:35:38.675934] Accuracy of the network on the 10000 test images: 47.8%
[06:35:38.676115] Max accuracy: 47.83%
[06:35:38.822148] log_dir: ./output_dir
[06:35:39.823682] Epoch: [7]  [  0/781]  eta: 0:13:00  lr: 0.000250  training_loss: 2.6741 (2.6741)  mae_loss: 0.4497 (0.4497)  classification_loss: 1.9788 (1.9788)  loss_mask: 0.2456 (0.2456)  time: 0.9994  data: 0.7801  max mem: 5511
[06:35:43.777769] Epoch: [7]  [ 20/781]  eta: 0:02:59  lr: 0.000250  training_loss: 2.8133 (2.7714)  mae_loss: 0.4468 (0.4484)  classification_loss: 1.9705 (1.9906)  loss_mask: 0.3313 (0.3323)  time: 0.1976  data: 0.0002  max mem: 5511
[06:35:47.709028] Epoch: [7]  [ 40/781]  eta: 0:02:40  lr: 0.000250  training_loss: 2.7986 (2.7973)  mae_loss: 0.4605 (0.4550)  classification_loss: 2.0219 (2.0025)  loss_mask: 0.3231 (0.3398)  time: 0.1965  data: 0.0002  max mem: 5511
[06:35:51.684216] Epoch: [7]  [ 60/781]  eta: 0:02:31  lr: 0.000250  training_loss: 2.7864 (2.8097)  mae_loss: 0.4636 (0.4581)  classification_loss: 2.0361 (2.0161)  loss_mask: 0.3098 (0.3355)  time: 0.1987  data: 0.0003  max mem: 5511
[06:35:55.654544] Epoch: [7]  [ 80/781]  eta: 0:02:25  lr: 0.000250  training_loss: 2.7309 (2.7914)  mae_loss: 0.4586 (0.4590)  classification_loss: 1.9850 (2.0101)  loss_mask: 0.2647 (0.3224)  time: 0.1984  data: 0.0002  max mem: 5511
[06:35:59.615194] Epoch: [7]  [100/781]  eta: 0:02:20  lr: 0.000250  training_loss: 2.7607 (2.7863)  mae_loss: 0.4382 (0.4598)  classification_loss: 2.0089 (2.0115)  loss_mask: 0.2693 (0.3150)  time: 0.1979  data: 0.0002  max mem: 5511
[06:36:03.576700] Epoch: [7]  [120/781]  eta: 0:02:15  lr: 0.000250  training_loss: 2.8109 (2.7932)  mae_loss: 0.4429 (0.4590)  classification_loss: 1.9559 (2.0057)  loss_mask: 0.3805 (0.3285)  time: 0.1980  data: 0.0002  max mem: 5511
[06:36:07.536805] Epoch: [7]  [140/781]  eta: 0:02:10  lr: 0.000250  training_loss: 2.8760 (2.8046)  mae_loss: 0.4330 (0.4560)  classification_loss: 1.9865 (2.0026)  loss_mask: 0.3871 (0.3460)  time: 0.1979  data: 0.0002  max mem: 5511
[06:36:11.488135] Epoch: [7]  [160/781]  eta: 0:02:05  lr: 0.000250  training_loss: 2.7898 (2.8053)  mae_loss: 0.4605 (0.4560)  classification_loss: 1.9905 (2.0027)  loss_mask: 0.3434 (0.3466)  time: 0.1975  data: 0.0002  max mem: 5511
[06:36:15.417784] Epoch: [7]  [180/781]  eta: 0:02:01  lr: 0.000250  training_loss: 2.7299 (2.7970)  mae_loss: 0.4489 (0.4557)  classification_loss: 1.9947 (2.0013)  loss_mask: 0.2654 (0.3400)  time: 0.1964  data: 0.0002  max mem: 5511
[06:36:19.356412] Epoch: [7]  [200/781]  eta: 0:01:57  lr: 0.000250  training_loss: 2.6923 (2.7902)  mae_loss: 0.4458 (0.4546)  classification_loss: 2.0095 (2.0022)  loss_mask: 0.2595 (0.3334)  time: 0.1969  data: 0.0002  max mem: 5511
[06:36:23.318076] Epoch: [7]  [220/781]  eta: 0:01:52  lr: 0.000250  training_loss: 2.7297 (2.7839)  mae_loss: 0.4467 (0.4540)  classification_loss: 1.9937 (2.0021)  loss_mask: 0.2619 (0.3277)  time: 0.1980  data: 0.0002  max mem: 5511
[06:36:27.283376] Epoch: [7]  [240/781]  eta: 0:01:48  lr: 0.000250  training_loss: 2.7432 (2.7790)  mae_loss: 0.4609 (0.4550)  classification_loss: 2.0105 (2.0024)  loss_mask: 0.2366 (0.3216)  time: 0.1982  data: 0.0002  max mem: 5511
[06:36:31.285548] Epoch: [7]  [260/781]  eta: 0:01:44  lr: 0.000250  training_loss: 2.6877 (2.7792)  mae_loss: 0.4518 (0.4549)  classification_loss: 2.0165 (2.0030)  loss_mask: 0.2871 (0.3213)  time: 0.2000  data: 0.0002  max mem: 5511
[06:36:35.249632] Epoch: [7]  [280/781]  eta: 0:01:40  lr: 0.000250  training_loss: 2.9263 (2.7887)  mae_loss: 0.4487 (0.4552)  classification_loss: 1.9706 (2.0020)  loss_mask: 0.4918 (0.3316)  time: 0.1981  data: 0.0002  max mem: 5511
[06:36:39.206652] Epoch: [7]  [300/781]  eta: 0:01:36  lr: 0.000250  training_loss: 2.8620 (2.7969)  mae_loss: 0.4425 (0.4543)  classification_loss: 2.0059 (2.0027)  loss_mask: 0.4446 (0.3399)  time: 0.1978  data: 0.0002  max mem: 5511
[06:36:43.150872] Epoch: [7]  [320/781]  eta: 0:01:32  lr: 0.000250  training_loss: 2.7452 (2.7946)  mae_loss: 0.4503 (0.4545)  classification_loss: 1.9890 (2.0017)  loss_mask: 0.2839 (0.3384)  time: 0.1971  data: 0.0002  max mem: 5511
[06:36:47.127464] Epoch: [7]  [340/781]  eta: 0:01:28  lr: 0.000250  training_loss: 2.6235 (2.7863)  mae_loss: 0.4468 (0.4542)  classification_loss: 1.9429 (1.9994)  loss_mask: 0.2402 (0.3327)  time: 0.1987  data: 0.0003  max mem: 5511
[06:36:51.083814] Epoch: [7]  [360/781]  eta: 0:01:24  lr: 0.000250  training_loss: 2.7157 (2.7830)  mae_loss: 0.4477 (0.4544)  classification_loss: 1.9832 (1.9986)  loss_mask: 0.3020 (0.3300)  time: 0.1977  data: 0.0002  max mem: 5511
[06:36:55.053218] Epoch: [7]  [380/781]  eta: 0:01:20  lr: 0.000250  training_loss: 2.7167 (2.7823)  mae_loss: 0.4588 (0.4553)  classification_loss: 2.0033 (1.9985)  loss_mask: 0.2680 (0.3284)  time: 0.1984  data: 0.0002  max mem: 5511
[06:36:58.994912] Epoch: [7]  [400/781]  eta: 0:01:16  lr: 0.000250  training_loss: 2.7288 (2.7800)  mae_loss: 0.4374 (0.4546)  classification_loss: 1.9671 (1.9966)  loss_mask: 0.3201 (0.3288)  time: 0.1970  data: 0.0002  max mem: 5511
[06:37:02.966162] Epoch: [7]  [420/781]  eta: 0:01:12  lr: 0.000250  training_loss: 2.6901 (2.7755)  mae_loss: 0.4572 (0.4550)  classification_loss: 1.9839 (1.9957)  loss_mask: 0.2323 (0.3249)  time: 0.1985  data: 0.0003  max mem: 5511
[06:37:06.911375] Epoch: [7]  [440/781]  eta: 0:01:08  lr: 0.000250  training_loss: 2.6395 (2.7699)  mae_loss: 0.4328 (0.4548)  classification_loss: 1.9718 (1.9954)  loss_mask: 0.2181 (0.3197)  time: 0.1972  data: 0.0002  max mem: 5511
[06:37:10.871071] Epoch: [7]  [460/781]  eta: 0:01:04  lr: 0.000250  training_loss: 2.6943 (2.7659)  mae_loss: 0.4454 (0.4543)  classification_loss: 1.9925 (1.9954)  loss_mask: 0.2330 (0.3162)  time: 0.1979  data: 0.0002  max mem: 5511
[06:37:14.825060] Epoch: [7]  [480/781]  eta: 0:01:00  lr: 0.000250  training_loss: 2.6822 (2.7631)  mae_loss: 0.4454 (0.4541)  classification_loss: 2.0109 (1.9952)  loss_mask: 0.2314 (0.3138)  time: 0.1976  data: 0.0002  max mem: 5511
[06:37:18.774900] Epoch: [7]  [500/781]  eta: 0:00:56  lr: 0.000250  training_loss: 2.6368 (2.7581)  mae_loss: 0.4329 (0.4534)  classification_loss: 1.9511 (1.9943)  loss_mask: 0.2273 (0.3103)  time: 0.1974  data: 0.0002  max mem: 5511
[06:37:22.736571] Epoch: [7]  [520/781]  eta: 0:00:52  lr: 0.000250  training_loss: 2.6408 (2.7538)  mae_loss: 0.4508 (0.4533)  classification_loss: 1.9764 (1.9934)  loss_mask: 0.2308 (0.3071)  time: 0.1980  data: 0.0002  max mem: 5511
[06:37:26.700298] Epoch: [7]  [540/781]  eta: 0:00:48  lr: 0.000250  training_loss: 2.6774 (2.7520)  mae_loss: 0.4346 (0.4528)  classification_loss: 1.9750 (1.9935)  loss_mask: 0.2464 (0.3057)  time: 0.1980  data: 0.0002  max mem: 5511
[06:37:30.667802] Epoch: [7]  [560/781]  eta: 0:00:44  lr: 0.000249  training_loss: 2.6975 (2.7509)  mae_loss: 0.4517 (0.4528)  classification_loss: 1.9830 (1.9937)  loss_mask: 0.2684 (0.3044)  time: 0.1983  data: 0.0002  max mem: 5511
[06:37:34.707000] Epoch: [7]  [580/781]  eta: 0:00:40  lr: 0.000249  training_loss: 2.5847 (2.7457)  mae_loss: 0.4185 (0.4523)  classification_loss: 1.9523 (1.9931)  loss_mask: 0.1805 (0.3003)  time: 0.2019  data: 0.0005  max mem: 5511
[06:37:38.672310] Epoch: [7]  [600/781]  eta: 0:00:36  lr: 0.000249  training_loss: 2.6487 (2.7429)  mae_loss: 0.4303 (0.4520)  classification_loss: 1.9425 (1.9926)  loss_mask: 0.2279 (0.2983)  time: 0.1982  data: 0.0003  max mem: 5511
[06:37:42.628494] Epoch: [7]  [620/781]  eta: 0:00:32  lr: 0.000249  training_loss: 2.6460 (2.7401)  mae_loss: 0.4545 (0.4521)  classification_loss: 1.9559 (1.9919)  loss_mask: 0.2196 (0.2961)  time: 0.1977  data: 0.0002  max mem: 5511
[06:37:46.640294] Epoch: [7]  [640/781]  eta: 0:00:28  lr: 0.000249  training_loss: 2.5719 (2.7358)  mae_loss: 0.4789 (0.4527)  classification_loss: 1.9708 (1.9913)  loss_mask: 0.1592 (0.2918)  time: 0.2005  data: 0.0002  max mem: 5511
[06:37:50.630687] Epoch: [7]  [660/781]  eta: 0:00:24  lr: 0.000249  training_loss: 2.6176 (2.7325)  mae_loss: 0.4452 (0.4526)  classification_loss: 1.9759 (1.9911)  loss_mask: 0.1607 (0.2888)  time: 0.1994  data: 0.0002  max mem: 5511
[06:37:54.582088] Epoch: [7]  [680/781]  eta: 0:00:20  lr: 0.000249  training_loss: 2.7765 (2.7351)  mae_loss: 0.4663 (0.4532)  classification_loss: 1.9814 (1.9911)  loss_mask: 0.3045 (0.2908)  time: 0.1974  data: 0.0003  max mem: 5511
[06:37:58.547462] Epoch: [7]  [700/781]  eta: 0:00:16  lr: 0.000249  training_loss: 2.6412 (2.7318)  mae_loss: 0.4559 (0.4529)  classification_loss: 1.9577 (1.9907)  loss_mask: 0.1959 (0.2883)  time: 0.1982  data: 0.0002  max mem: 5511
[06:38:02.485778] Epoch: [7]  [720/781]  eta: 0:00:12  lr: 0.000249  training_loss: 2.6014 (2.7287)  mae_loss: 0.4296 (0.4523)  classification_loss: 1.9958 (1.9909)  loss_mask: 0.1824 (0.2855)  time: 0.1968  data: 0.0003  max mem: 5511
[06:38:06.402052] Epoch: [7]  [740/781]  eta: 0:00:08  lr: 0.000249  training_loss: 2.6219 (2.7254)  mae_loss: 0.4360 (0.4521)  classification_loss: 1.9765 (1.9902)  loss_mask: 0.1818 (0.2832)  time: 0.1957  data: 0.0002  max mem: 5511
[06:38:10.360449] Epoch: [7]  [760/781]  eta: 0:00:04  lr: 0.000249  training_loss: 2.6655 (2.7241)  mae_loss: 0.4397 (0.4520)  classification_loss: 2.0029 (1.9905)  loss_mask: 0.2032 (0.2816)  time: 0.1978  data: 0.0002  max mem: 5511
[06:38:14.286648] Epoch: [7]  [780/781]  eta: 0:00:00  lr: 0.000249  training_loss: 2.6199 (2.7215)  mae_loss: 0.4540 (0.4522)  classification_loss: 1.9327 (1.9898)  loss_mask: 0.1906 (0.2795)  time: 0.1962  data: 0.0002  max mem: 5511
[06:38:14.451890] Epoch: [7] Total time: 0:02:35 (0.1993 s / it)
[06:38:14.452775] Averaged stats: lr: 0.000249  training_loss: 2.6199 (2.7215)  mae_loss: 0.4540 (0.4522)  classification_loss: 1.9327 (1.9898)  loss_mask: 0.1906 (0.2795)
[06:38:15.187321] Test:  [  0/157]  eta: 0:01:54  testing_loss: 1.3586 (1.3586)  acc1: 59.3750 (59.3750)  acc5: 92.1875 (92.1875)  time: 0.7299  data: 0.6974  max mem: 5511
[06:38:15.482254] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.4399 (1.4670)  acc1: 51.5625 (49.1477)  acc5: 93.7500 (93.0398)  time: 0.0930  data: 0.0640  max mem: 5511
[06:38:15.767284] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.4141 (1.4297)  acc1: 51.5625 (51.4137)  acc5: 93.7500 (93.7500)  time: 0.0288  data: 0.0004  max mem: 5511
[06:38:16.052926] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.4233 (1.4369)  acc1: 53.1250 (51.1089)  acc5: 93.7500 (93.4476)  time: 0.0283  data: 0.0002  max mem: 5511
[06:38:16.338545] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.4436 (1.4465)  acc1: 53.1250 (50.9909)  acc5: 92.1875 (92.7210)  time: 0.0284  data: 0.0003  max mem: 5511
[06:38:16.623980] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.4476 (1.4444)  acc1: 51.5625 (51.1949)  acc5: 92.1875 (92.7390)  time: 0.0284  data: 0.0002  max mem: 5511
[06:38:16.910632] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.4156 (1.4385)  acc1: 51.5625 (51.3832)  acc5: 92.1875 (92.7254)  time: 0.0285  data: 0.0002  max mem: 5511
[06:38:17.197993] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.3820 (1.4348)  acc1: 54.6875 (51.7165)  acc5: 92.1875 (92.9577)  time: 0.0285  data: 0.0002  max mem: 5511
[06:38:17.482811] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.4090 (1.4364)  acc1: 53.1250 (51.6975)  acc5: 93.7500 (93.1134)  time: 0.0283  data: 0.0002  max mem: 5511
[06:38:17.768096] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.4399 (1.4400)  acc1: 51.5625 (51.4423)  acc5: 93.7500 (92.9945)  time: 0.0283  data: 0.0002  max mem: 5511
[06:38:18.057298] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.4775 (1.4448)  acc1: 48.4375 (50.9592)  acc5: 92.1875 (92.8837)  time: 0.0286  data: 0.0002  max mem: 5511
[06:38:18.347846] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.4919 (1.4490)  acc1: 46.8750 (50.7742)  acc5: 92.1875 (92.8209)  time: 0.0288  data: 0.0003  max mem: 5511
[06:38:18.632051] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.4567 (1.4462)  acc1: 50.0000 (50.9427)  acc5: 93.7500 (92.9881)  time: 0.0286  data: 0.0002  max mem: 5511
[06:38:18.916654] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.4481 (1.4483)  acc1: 53.1250 (50.9065)  acc5: 93.7500 (92.8674)  time: 0.0283  data: 0.0002  max mem: 5511
[06:38:19.200406] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.4516 (1.4484)  acc1: 51.5625 (51.0527)  acc5: 90.6250 (92.7748)  time: 0.0283  data: 0.0001  max mem: 5511
[06:38:19.482763] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.4297 (1.4463)  acc1: 51.5625 (51.2107)  acc5: 92.1875 (92.8187)  time: 0.0282  data: 0.0001  max mem: 5511
[06:38:19.639449] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.4252 (1.4470)  acc1: 50.0000 (51.0900)  acc5: 92.1875 (92.8100)  time: 0.0274  data: 0.0001  max mem: 5511
[06:38:19.813179] Test: Total time: 0:00:05 (0.0341 s / it)
[06:38:19.814369] * Acc@1 51.090 Acc@5 92.810 loss 1.447
[06:38:19.814689] Accuracy of the network on the 10000 test images: 51.1%
[06:38:19.814881] Max accuracy: 51.09%
[06:38:19.919385] log_dir: ./output_dir
[06:38:20.770814] Epoch: [8]  [  0/781]  eta: 0:11:03  lr: 0.000249  training_loss: 2.5649 (2.5649)  mae_loss: 0.4655 (0.4655)  classification_loss: 1.8296 (1.8296)  loss_mask: 0.2698 (0.2698)  time: 0.8496  data: 0.6233  max mem: 5511
[06:38:24.726407] Epoch: [8]  [ 20/781]  eta: 0:02:54  lr: 0.000249  training_loss: 2.6247 (2.6402)  mae_loss: 0.4352 (0.4432)  classification_loss: 1.9581 (1.9651)  loss_mask: 0.2063 (0.2319)  time: 0.1977  data: 0.0002  max mem: 5511
[06:38:28.677334] Epoch: [8]  [ 40/781]  eta: 0:02:38  lr: 0.000249  training_loss: 2.6151 (2.6422)  mae_loss: 0.4422 (0.4440)  classification_loss: 2.0048 (1.9750)  loss_mask: 0.1821 (0.2231)  time: 0.1975  data: 0.0002  max mem: 5511
[06:38:32.655198] Epoch: [8]  [ 60/781]  eta: 0:02:30  lr: 0.000249  training_loss: 2.5468 (2.6215)  mae_loss: 0.4331 (0.4432)  classification_loss: 1.9410 (1.9645)  loss_mask: 0.1919 (0.2138)  time: 0.1987  data: 0.0003  max mem: 5511
[06:38:36.605597] Epoch: [8]  [ 80/781]  eta: 0:02:24  lr: 0.000249  training_loss: 2.5415 (2.6149)  mae_loss: 0.4368 (0.4426)  classification_loss: 1.9407 (1.9650)  loss_mask: 0.1745 (0.2073)  time: 0.1974  data: 0.0002  max mem: 5511
[06:38:40.544388] Epoch: [8]  [100/781]  eta: 0:02:18  lr: 0.000249  training_loss: 2.6011 (2.6123)  mae_loss: 0.4229 (0.4394)  classification_loss: 1.9928 (1.9713)  loss_mask: 0.1591 (0.2016)  time: 0.1969  data: 0.0002  max mem: 5511
[06:38:44.481436] Epoch: [8]  [120/781]  eta: 0:02:14  lr: 0.000249  training_loss: 2.5621 (2.6064)  mae_loss: 0.4287 (0.4375)  classification_loss: 1.9181 (1.9667)  loss_mask: 0.1869 (0.2022)  time: 0.1967  data: 0.0002  max mem: 5511
[06:38:48.428810] Epoch: [8]  [140/781]  eta: 0:02:09  lr: 0.000249  training_loss: 2.5710 (2.6033)  mae_loss: 0.4279 (0.4367)  classification_loss: 1.9265 (1.9592)  loss_mask: 0.2005 (0.2073)  time: 0.1973  data: 0.0002  max mem: 5511
[06:38:52.375912] Epoch: [8]  [160/781]  eta: 0:02:05  lr: 0.000249  training_loss: 2.6034 (2.6072)  mae_loss: 0.4490 (0.4391)  classification_loss: 1.9656 (1.9603)  loss_mask: 0.1817 (0.2078)  time: 0.1973  data: 0.0002  max mem: 5511
[06:38:56.332036] Epoch: [8]  [180/781]  eta: 0:02:00  lr: 0.000249  training_loss: 2.5636 (2.6034)  mae_loss: 0.4528 (0.4408)  classification_loss: 1.9183 (1.9567)  loss_mask: 0.1818 (0.2058)  time: 0.1977  data: 0.0002  max mem: 5511
[06:39:00.290723] Epoch: [8]  [200/781]  eta: 0:01:56  lr: 0.000249  training_loss: 2.5518 (2.6002)  mae_loss: 0.4372 (0.4409)  classification_loss: 1.9430 (1.9565)  loss_mask: 0.1582 (0.2028)  time: 0.1978  data: 0.0002  max mem: 5511
[06:39:04.292490] Epoch: [8]  [220/781]  eta: 0:01:52  lr: 0.000249  training_loss: 2.5833 (2.5989)  mae_loss: 0.4240 (0.4410)  classification_loss: 1.9356 (1.9546)  loss_mask: 0.2144 (0.2033)  time: 0.2000  data: 0.0002  max mem: 5511
[06:39:08.251892] Epoch: [8]  [240/781]  eta: 0:01:48  lr: 0.000249  training_loss: 2.5295 (2.5969)  mae_loss: 0.4429 (0.4411)  classification_loss: 1.9436 (1.9553)  loss_mask: 0.1496 (0.2005)  time: 0.1979  data: 0.0003  max mem: 5511
[06:39:12.230937] Epoch: [8]  [260/781]  eta: 0:01:44  lr: 0.000249  training_loss: 2.5857 (2.5957)  mae_loss: 0.4288 (0.4402)  classification_loss: 1.9651 (1.9568)  loss_mask: 0.1785 (0.1987)  time: 0.1988  data: 0.0002  max mem: 5511
[06:39:16.180605] Epoch: [8]  [280/781]  eta: 0:01:40  lr: 0.000249  training_loss: 2.6069 (2.5981)  mae_loss: 0.4853 (0.4428)  classification_loss: 1.9505 (1.9563)  loss_mask: 0.1895 (0.1990)  time: 0.1974  data: 0.0002  max mem: 5511
[06:39:20.128565] Epoch: [8]  [300/781]  eta: 0:01:36  lr: 0.000249  training_loss: 2.5782 (2.5995)  mae_loss: 0.4279 (0.4421)  classification_loss: 1.9786 (1.9586)  loss_mask: 0.1909 (0.1988)  time: 0.1973  data: 0.0002  max mem: 5511
[06:39:24.074945] Epoch: [8]  [320/781]  eta: 0:01:32  lr: 0.000249  training_loss: 2.5446 (2.5957)  mae_loss: 0.4419 (0.4427)  classification_loss: 1.9457 (1.9577)  loss_mask: 0.1259 (0.1953)  time: 0.1972  data: 0.0002  max mem: 5511
[06:39:28.009606] Epoch: [8]  [340/781]  eta: 0:01:28  lr: 0.000249  training_loss: 2.6094 (2.5958)  mae_loss: 0.4531 (0.4447)  classification_loss: 1.9457 (1.9570)  loss_mask: 0.1524 (0.1940)  time: 0.1966  data: 0.0003  max mem: 5511
[06:39:31.974446] Epoch: [8]  [360/781]  eta: 0:01:23  lr: 0.000249  training_loss: 2.5951 (2.5965)  mae_loss: 0.4565 (0.4454)  classification_loss: 1.9409 (1.9573)  loss_mask: 0.1622 (0.1938)  time: 0.1982  data: 0.0002  max mem: 5511
[06:39:35.915879] Epoch: [8]  [380/781]  eta: 0:01:19  lr: 0.000249  training_loss: 2.5486 (2.5971)  mae_loss: 0.4495 (0.4456)  classification_loss: 1.9316 (1.9568)  loss_mask: 0.1788 (0.1946)  time: 0.1970  data: 0.0003  max mem: 5511
[06:39:39.869066] Epoch: [8]  [400/781]  eta: 0:01:15  lr: 0.000249  training_loss: 2.5577 (2.5965)  mae_loss: 0.4348 (0.4455)  classification_loss: 1.9726 (1.9584)  loss_mask: 0.1419 (0.1926)  time: 0.1976  data: 0.0003  max mem: 5511
[06:39:43.837335] Epoch: [8]  [420/781]  eta: 0:01:11  lr: 0.000249  training_loss: 2.6118 (2.5961)  mae_loss: 0.4606 (0.4466)  classification_loss: 1.9274 (1.9578)  loss_mask: 0.1637 (0.1917)  time: 0.1983  data: 0.0002  max mem: 5511
[06:39:47.777980] Epoch: [8]  [440/781]  eta: 0:01:07  lr: 0.000249  training_loss: 2.5687 (2.5945)  mae_loss: 0.4405 (0.4468)  classification_loss: 1.9613 (1.9575)  loss_mask: 0.1564 (0.1901)  time: 0.1970  data: 0.0002  max mem: 5511
[06:39:51.726992] Epoch: [8]  [460/781]  eta: 0:01:03  lr: 0.000249  training_loss: 2.5762 (2.5922)  mae_loss: 0.4174 (0.4464)  classification_loss: 1.9276 (1.9572)  loss_mask: 0.1414 (0.1887)  time: 0.1974  data: 0.0002  max mem: 5511
[06:39:55.666681] Epoch: [8]  [480/781]  eta: 0:00:59  lr: 0.000249  training_loss: 2.5519 (2.5909)  mae_loss: 0.4312 (0.4461)  classification_loss: 1.9386 (1.9564)  loss_mask: 0.1609 (0.1884)  time: 0.1969  data: 0.0004  max mem: 5511
[06:39:59.634375] Epoch: [8]  [500/781]  eta: 0:00:55  lr: 0.000249  training_loss: 2.5361 (2.5899)  mae_loss: 0.4367 (0.4460)  classification_loss: 1.9315 (1.9553)  loss_mask: 0.2036 (0.1885)  time: 0.1983  data: 0.0002  max mem: 5511
[06:40:03.569712] Epoch: [8]  [520/781]  eta: 0:00:51  lr: 0.000249  training_loss: 2.5734 (2.5897)  mae_loss: 0.4494 (0.4465)  classification_loss: 1.9402 (1.9548)  loss_mask: 0.1753 (0.1885)  time: 0.1967  data: 0.0002  max mem: 5511
[06:40:07.514655] Epoch: [8]  [540/781]  eta: 0:00:47  lr: 0.000249  training_loss: 2.6138 (2.5898)  mae_loss: 0.4443 (0.4463)  classification_loss: 1.9569 (1.9543)  loss_mask: 0.1826 (0.1892)  time: 0.1972  data: 0.0005  max mem: 5511
[06:40:11.463752] Epoch: [8]  [560/781]  eta: 0:00:43  lr: 0.000249  training_loss: 2.5488 (2.5876)  mae_loss: 0.4308 (0.4464)  classification_loss: 1.9460 (1.9535)  loss_mask: 0.1335 (0.1876)  time: 0.1974  data: 0.0002  max mem: 5511
[06:40:15.448248] Epoch: [8]  [580/781]  eta: 0:00:39  lr: 0.000249  training_loss: 2.5995 (2.5864)  mae_loss: 0.4558 (0.4467)  classification_loss: 1.9906 (1.9537)  loss_mask: 0.1315 (0.1860)  time: 0.1991  data: 0.0002  max mem: 5511
[06:40:19.391173] Epoch: [8]  [600/781]  eta: 0:00:35  lr: 0.000249  training_loss: 2.5406 (2.5850)  mae_loss: 0.4395 (0.4467)  classification_loss: 1.9109 (1.9532)  loss_mask: 0.1493 (0.1851)  time: 0.1971  data: 0.0003  max mem: 5511
[06:40:23.371688] Epoch: [8]  [620/781]  eta: 0:00:31  lr: 0.000249  training_loss: 2.4662 (2.5821)  mae_loss: 0.4387 (0.4465)  classification_loss: 1.8783 (1.9514)  loss_mask: 0.1424 (0.1842)  time: 0.1989  data: 0.0003  max mem: 5511
[06:40:27.336165] Epoch: [8]  [640/781]  eta: 0:00:28  lr: 0.000249  training_loss: 2.5070 (2.5797)  mae_loss: 0.4395 (0.4466)  classification_loss: 1.9293 (1.9510)  loss_mask: 0.1151 (0.1821)  time: 0.1981  data: 0.0003  max mem: 5511
[06:40:31.327332] Epoch: [8]  [660/781]  eta: 0:00:24  lr: 0.000249  training_loss: 2.5254 (2.5777)  mae_loss: 0.4510 (0.4469)  classification_loss: 1.9323 (1.9504)  loss_mask: 0.1154 (0.1804)  time: 0.1995  data: 0.0002  max mem: 5511
[06:40:35.309283] Epoch: [8]  [680/781]  eta: 0:00:20  lr: 0.000249  training_loss: 2.4976 (2.5752)  mae_loss: 0.4261 (0.4465)  classification_loss: 1.9308 (1.9497)  loss_mask: 0.1146 (0.1790)  time: 0.1990  data: 0.0002  max mem: 5511
[06:40:39.264687] Epoch: [8]  [700/781]  eta: 0:00:16  lr: 0.000249  training_loss: 2.4945 (2.5733)  mae_loss: 0.4326 (0.4461)  classification_loss: 1.9081 (1.9491)  loss_mask: 0.1264 (0.1781)  time: 0.1977  data: 0.0002  max mem: 5511
[06:40:43.242053] Epoch: [8]  [720/781]  eta: 0:00:12  lr: 0.000249  training_loss: 2.5379 (2.5724)  mae_loss: 0.4398 (0.4458)  classification_loss: 1.9752 (1.9498)  loss_mask: 0.1085 (0.1768)  time: 0.1988  data: 0.0003  max mem: 5511
[06:40:47.193853] Epoch: [8]  [740/781]  eta: 0:00:08  lr: 0.000249  training_loss: 2.4865 (2.5709)  mae_loss: 0.4042 (0.4452)  classification_loss: 1.9135 (1.9489)  loss_mask: 0.1472 (0.1767)  time: 0.1975  data: 0.0002  max mem: 5511
[06:40:51.159618] Epoch: [8]  [760/781]  eta: 0:00:04  lr: 0.000249  training_loss: 2.5769 (2.5710)  mae_loss: 0.4556 (0.4456)  classification_loss: 1.9466 (1.9490)  loss_mask: 0.1495 (0.1764)  time: 0.1982  data: 0.0002  max mem: 5511
[06:40:55.094708] Epoch: [8]  [780/781]  eta: 0:00:00  lr: 0.000249  training_loss: 2.5000 (2.5701)  mae_loss: 0.4342 (0.4453)  classification_loss: 1.9342 (1.9485)  loss_mask: 0.1701 (0.1764)  time: 0.1967  data: 0.0002  max mem: 5511
[06:40:55.251137] Epoch: [8] Total time: 0:02:35 (0.1989 s / it)
[06:40:55.251723] Averaged stats: lr: 0.000249  training_loss: 2.5000 (2.5701)  mae_loss: 0.4342 (0.4453)  classification_loss: 1.9342 (1.9485)  loss_mask: 0.1701 (0.1764)
[06:40:55.962778] Test:  [  0/157]  eta: 0:01:50  testing_loss: 1.3260 (1.3260)  acc1: 56.2500 (56.2500)  acc5: 92.1875 (92.1875)  time: 0.7056  data: 0.6752  max mem: 5511
[06:40:56.257630] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.3647 (1.4018)  acc1: 53.1250 (52.2727)  acc5: 93.7500 (93.3239)  time: 0.0908  data: 0.0615  max mem: 5511
[06:40:56.546230] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.3499 (1.3707)  acc1: 53.1250 (53.9435)  acc5: 93.7500 (93.3780)  time: 0.0290  data: 0.0002  max mem: 5511
[06:40:56.834845] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.3649 (1.3743)  acc1: 56.2500 (54.0323)  acc5: 93.7500 (93.1956)  time: 0.0287  data: 0.0002  max mem: 5511
[06:40:57.121692] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.4057 (1.3879)  acc1: 54.6875 (54.0015)  acc5: 92.1875 (92.7210)  time: 0.0286  data: 0.0002  max mem: 5511
[06:40:57.406348] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.3859 (1.3824)  acc1: 54.6875 (54.5343)  acc5: 92.1875 (92.8002)  time: 0.0284  data: 0.0002  max mem: 5511
[06:40:57.691211] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.3502 (1.3771)  acc1: 54.6875 (54.6619)  acc5: 93.7500 (92.9559)  time: 0.0283  data: 0.0002  max mem: 5511
[06:40:57.982264] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.3213 (1.3738)  acc1: 54.6875 (54.7095)  acc5: 93.7500 (93.0898)  time: 0.0286  data: 0.0003  max mem: 5511
[06:40:58.273948] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.3308 (1.3737)  acc1: 54.6875 (54.6103)  acc5: 93.7500 (93.1327)  time: 0.0290  data: 0.0004  max mem: 5511
[06:40:58.561938] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.3576 (1.3761)  acc1: 54.6875 (54.4471)  acc5: 93.7500 (93.2005)  time: 0.0288  data: 0.0002  max mem: 5511
[06:40:58.849343] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.4266 (1.3830)  acc1: 50.0000 (53.9759)  acc5: 93.7500 (93.2240)  time: 0.0286  data: 0.0002  max mem: 5511
[06:40:59.142119] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.4578 (1.3870)  acc1: 50.0000 (53.8288)  acc5: 93.7500 (93.2573)  time: 0.0288  data: 0.0002  max mem: 5511
[06:40:59.433507] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.3900 (1.3849)  acc1: 53.1250 (53.7707)  acc5: 93.7500 (93.3368)  time: 0.0291  data: 0.0002  max mem: 5511
[06:40:59.719212] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.3723 (1.3866)  acc1: 54.6875 (53.7572)  acc5: 93.7500 (93.2729)  time: 0.0287  data: 0.0002  max mem: 5511
[06:41:00.010142] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.3837 (1.3882)  acc1: 54.6875 (53.8675)  acc5: 93.7500 (93.2402)  time: 0.0287  data: 0.0002  max mem: 5511
[06:41:00.292429] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.3770 (1.3855)  acc1: 54.6875 (53.9839)  acc5: 93.7500 (93.2947)  time: 0.0285  data: 0.0001  max mem: 5511
[06:41:00.444987] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.3743 (1.3870)  acc1: 54.6875 (53.8900)  acc5: 93.7500 (93.3300)  time: 0.0272  data: 0.0001  max mem: 5511
[06:41:00.608576] Test: Total time: 0:00:05 (0.0341 s / it)
[06:41:00.609654] * Acc@1 53.890 Acc@5 93.330 loss 1.387
[06:41:00.609981] Accuracy of the network on the 10000 test images: 53.9%
[06:41:00.610180] Max accuracy: 53.89%
[06:41:00.803828] log_dir: ./output_dir
[06:41:01.678245] Epoch: [9]  [  0/781]  eta: 0:11:21  lr: 0.000249  training_loss: 2.3676 (2.3676)  mae_loss: 0.4453 (0.4453)  classification_loss: 1.8212 (1.8212)  loss_mask: 0.1010 (0.1010)  time: 0.8727  data: 0.6390  max mem: 5511
[06:41:05.617959] Epoch: [9]  [ 20/781]  eta: 0:02:54  lr: 0.000249  training_loss: 2.5138 (2.5019)  mae_loss: 0.4516 (0.4473)  classification_loss: 1.9361 (1.9363)  loss_mask: 0.1035 (0.1184)  time: 0.1969  data: 0.0002  max mem: 5511
[06:41:09.607744] Epoch: [9]  [ 40/781]  eta: 0:02:39  lr: 0.000249  training_loss: 2.4551 (2.4839)  mae_loss: 0.4499 (0.4483)  classification_loss: 1.9224 (1.9248)  loss_mask: 0.0973 (0.1108)  time: 0.1994  data: 0.0002  max mem: 5511
[06:41:13.554649] Epoch: [9]  [ 60/781]  eta: 0:02:30  lr: 0.000249  training_loss: 2.5777 (2.5044)  mae_loss: 0.4391 (0.4499)  classification_loss: 1.9627 (1.9383)  loss_mask: 0.1155 (0.1162)  time: 0.1973  data: 0.0002  max mem: 5511
[06:41:17.504172] Epoch: [9]  [ 80/781]  eta: 0:02:24  lr: 0.000249  training_loss: 2.5518 (2.5092)  mae_loss: 0.4356 (0.4494)  classification_loss: 1.9167 (1.9362)  loss_mask: 0.1311 (0.1236)  time: 0.1974  data: 0.0002  max mem: 5511
[06:41:21.439173] Epoch: [9]  [100/781]  eta: 0:02:19  lr: 0.000249  training_loss: 2.5465 (2.5298)  mae_loss: 0.4350 (0.4495)  classification_loss: 1.9122 (1.9309)  loss_mask: 0.1929 (0.1494)  time: 0.1967  data: 0.0002  max mem: 5511
[06:41:25.382689] Epoch: [9]  [120/781]  eta: 0:02:14  lr: 0.000249  training_loss: 2.4819 (2.5236)  mae_loss: 0.4433 (0.4487)  classification_loss: 1.8921 (1.9275)  loss_mask: 0.1238 (0.1474)  time: 0.1971  data: 0.0002  max mem: 5511
[06:41:29.343499] Epoch: [9]  [140/781]  eta: 0:02:09  lr: 0.000249  training_loss: 2.4319 (2.5124)  mae_loss: 0.4265 (0.4462)  classification_loss: 1.8457 (1.9221)  loss_mask: 0.1115 (0.1442)  time: 0.1980  data: 0.0003  max mem: 5511
[06:41:33.296012] Epoch: [9]  [160/781]  eta: 0:02:05  lr: 0.000249  training_loss: 2.4750 (2.5092)  mae_loss: 0.4429 (0.4459)  classification_loss: 1.8793 (1.9184)  loss_mask: 0.1442 (0.1449)  time: 0.1976  data: 0.0002  max mem: 5511
[06:41:37.262577] Epoch: [9]  [180/781]  eta: 0:02:00  lr: 0.000249  training_loss: 2.4390 (2.5019)  mae_loss: 0.4308 (0.4440)  classification_loss: 1.9113 (1.9167)  loss_mask: 0.1135 (0.1412)  time: 0.1982  data: 0.0004  max mem: 5511
[06:41:41.228673] Epoch: [9]  [200/781]  eta: 0:01:56  lr: 0.000249  training_loss: 2.5308 (2.5034)  mae_loss: 0.4340 (0.4428)  classification_loss: 1.9314 (1.9185)  loss_mask: 0.1196 (0.1421)  time: 0.1982  data: 0.0002  max mem: 5511
[06:41:45.168615] Epoch: [9]  [220/781]  eta: 0:01:52  lr: 0.000249  training_loss: 2.4385 (2.4964)  mae_loss: 0.4254 (0.4416)  classification_loss: 1.8801 (1.9167)  loss_mask: 0.0935 (0.1381)  time: 0.1969  data: 0.0002  max mem: 5511
[06:41:49.125235] Epoch: [9]  [240/781]  eta: 0:01:48  lr: 0.000249  training_loss: 2.4779 (2.4957)  mae_loss: 0.4383 (0.4415)  classification_loss: 1.9021 (1.9163)  loss_mask: 0.1366 (0.1379)  time: 0.1978  data: 0.0002  max mem: 5511
[06:41:53.064546] Epoch: [9]  [260/781]  eta: 0:01:44  lr: 0.000249  training_loss: 2.5033 (2.4961)  mae_loss: 0.4494 (0.4421)  classification_loss: 1.9151 (1.9156)  loss_mask: 0.1190 (0.1384)  time: 0.1969  data: 0.0002  max mem: 5511
[06:41:57.041398] Epoch: [9]  [280/781]  eta: 0:01:40  lr: 0.000249  training_loss: 2.5501 (2.4991)  mae_loss: 0.4601 (0.4434)  classification_loss: 1.9604 (1.9181)  loss_mask: 0.1119 (0.1376)  time: 0.1987  data: 0.0002  max mem: 5511
[06:42:01.015022] Epoch: [9]  [300/781]  eta: 0:01:36  lr: 0.000249  training_loss: 2.4288 (2.4960)  mae_loss: 0.4454 (0.4433)  classification_loss: 1.9031 (1.9174)  loss_mask: 0.0898 (0.1353)  time: 0.1986  data: 0.0002  max mem: 5511
[06:42:04.960650] Epoch: [9]  [320/781]  eta: 0:01:32  lr: 0.000249  training_loss: 2.4703 (2.4945)  mae_loss: 0.4050 (0.4420)  classification_loss: 1.8950 (1.9174)  loss_mask: 0.0978 (0.1350)  time: 0.1972  data: 0.0002  max mem: 5511
[06:42:08.940828] Epoch: [9]  [340/781]  eta: 0:01:28  lr: 0.000249  training_loss: 2.5032 (2.4941)  mae_loss: 0.4419 (0.4418)  classification_loss: 1.9173 (1.9174)  loss_mask: 0.1112 (0.1350)  time: 0.1989  data: 0.0002  max mem: 5511
[06:42:12.920054] Epoch: [9]  [360/781]  eta: 0:01:24  lr: 0.000249  training_loss: 2.4597 (2.4931)  mae_loss: 0.4185 (0.4410)  classification_loss: 1.9420 (1.9185)  loss_mask: 0.1004 (0.1337)  time: 0.1989  data: 0.0002  max mem: 5511
[06:42:16.898580] Epoch: [9]  [380/781]  eta: 0:01:20  lr: 0.000249  training_loss: 2.5978 (2.4998)  mae_loss: 0.4623 (0.4420)  classification_loss: 1.9092 (1.9187)  loss_mask: 0.2095 (0.1391)  time: 0.1988  data: 0.0002  max mem: 5511
[06:42:20.872495] Epoch: [9]  [400/781]  eta: 0:01:16  lr: 0.000249  training_loss: 2.4639 (2.5002)  mae_loss: 0.4203 (0.4414)  classification_loss: 1.9129 (1.9181)  loss_mask: 0.1629 (0.1407)  time: 0.1986  data: 0.0002  max mem: 5511
[06:42:24.833432] Epoch: [9]  [420/781]  eta: 0:01:12  lr: 0.000249  training_loss: 2.3838 (2.4960)  mae_loss: 0.4328 (0.4407)  classification_loss: 1.8615 (1.9158)  loss_mask: 0.1144 (0.1395)  time: 0.1979  data: 0.0002  max mem: 5511
[06:42:28.784094] Epoch: [9]  [440/781]  eta: 0:01:07  lr: 0.000249  training_loss: 2.4361 (2.4935)  mae_loss: 0.4432 (0.4406)  classification_loss: 1.9021 (1.9153)  loss_mask: 0.0873 (0.1376)  time: 0.1974  data: 0.0003  max mem: 5511
[06:42:32.732093] Epoch: [9]  [460/781]  eta: 0:01:03  lr: 0.000249  training_loss: 2.4544 (2.4925)  mae_loss: 0.4211 (0.4401)  classification_loss: 1.9168 (1.9153)  loss_mask: 0.1127 (0.1371)  time: 0.1973  data: 0.0002  max mem: 5511
[06:42:36.703586] Epoch: [9]  [480/781]  eta: 0:00:59  lr: 0.000249  training_loss: 2.5255 (2.4945)  mae_loss: 0.4381 (0.4400)  classification_loss: 1.8920 (1.9154)  loss_mask: 0.1714 (0.1391)  time: 0.1985  data: 0.0003  max mem: 5511
[06:42:40.688928] Epoch: [9]  [500/781]  eta: 0:00:55  lr: 0.000249  training_loss: 2.4826 (2.4941)  mae_loss: 0.4259 (0.4398)  classification_loss: 1.9066 (1.9147)  loss_mask: 0.1350 (0.1396)  time: 0.1992  data: 0.0002  max mem: 5511
[06:42:44.622596] Epoch: [9]  [520/781]  eta: 0:00:51  lr: 0.000249  training_loss: 2.4846 (2.4940)  mae_loss: 0.4602 (0.4403)  classification_loss: 1.9399 (1.9151)  loss_mask: 0.1153 (0.1386)  time: 0.1966  data: 0.0002  max mem: 5511
[06:42:48.566106] Epoch: [9]  [540/781]  eta: 0:00:47  lr: 0.000249  training_loss: 2.4197 (2.4923)  mae_loss: 0.4436 (0.4405)  classification_loss: 1.9172 (1.9149)  loss_mask: 0.0835 (0.1369)  time: 0.1971  data: 0.0002  max mem: 5511
[06:42:52.496557] Epoch: [9]  [560/781]  eta: 0:00:43  lr: 0.000248  training_loss: 2.4279 (2.4898)  mae_loss: 0.4368 (0.4404)  classification_loss: 1.8994 (1.9144)  loss_mask: 0.0800 (0.1350)  time: 0.1964  data: 0.0002  max mem: 5511
[06:42:56.427547] Epoch: [9]  [580/781]  eta: 0:00:39  lr: 0.000248  training_loss: 2.4742 (2.4903)  mae_loss: 0.4303 (0.4403)  classification_loss: 1.9195 (1.9142)  loss_mask: 0.1166 (0.1359)  time: 0.1965  data: 0.0002  max mem: 5511
[06:43:00.414712] Epoch: [9]  [600/781]  eta: 0:00:36  lr: 0.000248  training_loss: 2.4434 (2.4887)  mae_loss: 0.4326 (0.4405)  classification_loss: 1.8889 (1.9129)  loss_mask: 0.1236 (0.1353)  time: 0.1993  data: 0.0003  max mem: 5511
[06:43:04.374884] Epoch: [9]  [620/781]  eta: 0:00:32  lr: 0.000248  training_loss: 2.4963 (2.4896)  mae_loss: 0.4461 (0.4406)  classification_loss: 1.8874 (1.9127)  loss_mask: 0.1067 (0.1363)  time: 0.1979  data: 0.0003  max mem: 5511
[06:43:08.337840] Epoch: [9]  [640/781]  eta: 0:00:28  lr: 0.000248  training_loss: 2.4446 (2.4881)  mae_loss: 0.4408 (0.4405)  classification_loss: 1.8574 (1.9113)  loss_mask: 0.1402 (0.1363)  time: 0.1981  data: 0.0003  max mem: 5511
[06:43:12.301353] Epoch: [9]  [660/781]  eta: 0:00:24  lr: 0.000248  training_loss: 2.5363 (2.4897)  mae_loss: 0.4474 (0.4406)  classification_loss: 1.8886 (1.9112)  loss_mask: 0.1996 (0.1379)  time: 0.1981  data: 0.0003  max mem: 5511
[06:43:16.265268] Epoch: [9]  [680/781]  eta: 0:00:20  lr: 0.000248  training_loss: 2.4550 (2.4887)  mae_loss: 0.4311 (0.4407)  classification_loss: 1.8998 (1.9108)  loss_mask: 0.1023 (0.1372)  time: 0.1981  data: 0.0003  max mem: 5511
[06:43:20.201839] Epoch: [9]  [700/781]  eta: 0:00:16  lr: 0.000248  training_loss: 2.3840 (2.4865)  mae_loss: 0.4276 (0.4407)  classification_loss: 1.8764 (1.9098)  loss_mask: 0.0809 (0.1360)  time: 0.1967  data: 0.0003  max mem: 5511
[06:43:24.149619] Epoch: [9]  [720/781]  eta: 0:00:12  lr: 0.000248  training_loss: 2.3639 (2.4839)  mae_loss: 0.4036 (0.4396)  classification_loss: 1.8601 (1.9090)  loss_mask: 0.0833 (0.1353)  time: 0.1973  data: 0.0003  max mem: 5511
[06:43:28.121882] Epoch: [9]  [740/781]  eta: 0:00:08  lr: 0.000248  training_loss: 2.4255 (2.4829)  mae_loss: 0.4343 (0.4395)  classification_loss: 1.8748 (1.9083)  loss_mask: 0.1127 (0.1351)  time: 0.1984  data: 0.0002  max mem: 5511
[06:43:32.114159] Epoch: [9]  [760/781]  eta: 0:00:04  lr: 0.000248  training_loss: 2.4980 (2.4831)  mae_loss: 0.4376 (0.4394)  classification_loss: 1.9474 (1.9085)  loss_mask: 0.1230 (0.1352)  time: 0.1995  data: 0.0003  max mem: 5511
[06:43:36.082591] Epoch: [9]  [780/781]  eta: 0:00:00  lr: 0.000248  training_loss: 2.4899 (2.4841)  mae_loss: 0.4058 (0.4390)  classification_loss: 1.9171 (1.9083)  loss_mask: 0.1695 (0.1368)  time: 0.1983  data: 0.0002  max mem: 5511
[06:43:36.257883] Epoch: [9] Total time: 0:02:35 (0.1990 s / it)
[06:43:36.258396] Averaged stats: lr: 0.000248  training_loss: 2.4899 (2.4841)  mae_loss: 0.4058 (0.4390)  classification_loss: 1.9171 (1.9083)  loss_mask: 0.1695 (0.1368)
[06:43:37.027096] Test:  [  0/157]  eta: 0:01:59  testing_loss: 1.3294 (1.3294)  acc1: 59.3750 (59.3750)  acc5: 92.1875 (92.1875)  time: 0.7643  data: 0.7305  max mem: 5511
[06:43:37.324439] Test:  [ 10/157]  eta: 0:00:14  testing_loss: 1.3734 (1.3949)  acc1: 54.6875 (52.9830)  acc5: 93.7500 (94.6023)  time: 0.0962  data: 0.0667  max mem: 5511
[06:43:37.612027] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.3334 (1.3642)  acc1: 54.6875 (54.8363)  acc5: 93.7500 (94.4940)  time: 0.0290  data: 0.0003  max mem: 5511
[06:43:37.916226] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.3547 (1.3693)  acc1: 53.1250 (54.7883)  acc5: 93.7500 (94.4052)  time: 0.0294  data: 0.0002  max mem: 5511
[06:43:38.205869] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.3781 (1.3737)  acc1: 53.1250 (55.0305)  acc5: 93.7500 (93.7119)  time: 0.0296  data: 0.0002  max mem: 5511
[06:43:38.492273] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.3615 (1.3696)  acc1: 54.6875 (55.3309)  acc5: 92.1875 (93.6887)  time: 0.0287  data: 0.0002  max mem: 5511
[06:43:38.786313] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.3361 (1.3653)  acc1: 54.6875 (55.5584)  acc5: 95.3125 (93.8268)  time: 0.0289  data: 0.0002  max mem: 5511
[06:43:39.071870] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.3409 (1.3617)  acc1: 54.6875 (55.6778)  acc5: 95.3125 (94.0141)  time: 0.0288  data: 0.0002  max mem: 5511
[06:43:39.359868] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.3473 (1.3621)  acc1: 56.2500 (55.7292)  acc5: 95.3125 (94.0779)  time: 0.0285  data: 0.0002  max mem: 5511
[06:43:39.643667] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.3467 (1.3646)  acc1: 56.2500 (55.5460)  acc5: 95.3125 (94.1106)  time: 0.0285  data: 0.0002  max mem: 5511
[06:43:39.930626] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.3795 (1.3705)  acc1: 53.1250 (55.1207)  acc5: 93.7500 (94.1058)  time: 0.0284  data: 0.0002  max mem: 5511
[06:43:40.222259] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.3872 (1.3733)  acc1: 50.0000 (54.7860)  acc5: 93.7500 (94.1441)  time: 0.0288  data: 0.0002  max mem: 5511
[06:43:40.507412] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.3793 (1.3711)  acc1: 51.5625 (54.8037)  acc5: 95.3125 (94.3182)  time: 0.0287  data: 0.0002  max mem: 5511
[06:43:40.794011] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.3552 (1.3737)  acc1: 54.6875 (54.5086)  acc5: 93.7500 (94.2748)  time: 0.0284  data: 0.0002  max mem: 5511
[06:43:41.078983] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.4016 (1.3755)  acc1: 54.6875 (54.6543)  acc5: 93.7500 (94.2265)  time: 0.0284  data: 0.0002  max mem: 5511
[06:43:41.360274] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.3586 (1.3738)  acc1: 54.6875 (54.7806)  acc5: 93.7500 (94.1743)  time: 0.0281  data: 0.0001  max mem: 5511
[06:43:41.512576] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.3663 (1.3754)  acc1: 53.1250 (54.6700)  acc5: 93.7500 (94.2600)  time: 0.0272  data: 0.0001  max mem: 5511
[06:43:41.673500] Test: Total time: 0:00:05 (0.0345 s / it)
[06:43:41.675308] * Acc@1 54.670 Acc@5 94.260 loss 1.375
[06:43:41.675645] Accuracy of the network on the 10000 test images: 54.7%
[06:43:41.675823] Max accuracy: 54.67%
[06:43:41.754490] log_dir: ./output_dir
[06:43:42.673047] Epoch: [10]  [  0/781]  eta: 0:11:55  lr: 0.000248  training_loss: 2.3151 (2.3151)  mae_loss: 0.3892 (0.3892)  classification_loss: 1.8247 (1.8247)  loss_mask: 0.1012 (0.1012)  time: 0.9167  data: 0.6992  max mem: 5511
[06:43:46.634825] Epoch: [10]  [ 20/781]  eta: 0:02:56  lr: 0.000248  training_loss: 2.4399 (2.4605)  mae_loss: 0.4190 (0.4260)  classification_loss: 1.9024 (1.9045)  loss_mask: 0.1189 (0.1300)  time: 0.1980  data: 0.0002  max mem: 5511
[06:43:50.589804] Epoch: [10]  [ 40/781]  eta: 0:02:39  lr: 0.000248  training_loss: 2.3962 (2.4391)  mae_loss: 0.4320 (0.4310)  classification_loss: 1.8860 (1.8968)  loss_mask: 0.0769 (0.1113)  time: 0.1977  data: 0.0002  max mem: 5511
[06:43:54.517602] Epoch: [10]  [ 60/781]  eta: 0:02:30  lr: 0.000248  training_loss: 2.4852 (2.4533)  mae_loss: 0.4279 (0.4332)  classification_loss: 1.9404 (1.9128)  loss_mask: 0.1011 (0.1073)  time: 0.1963  data: 0.0002  max mem: 5511
[06:43:58.432792] Epoch: [10]  [ 80/781]  eta: 0:02:24  lr: 0.000248  training_loss: 2.5168 (2.4643)  mae_loss: 0.4441 (0.4343)  classification_loss: 1.8852 (1.9142)  loss_mask: 0.1312 (0.1158)  time: 0.1957  data: 0.0002  max mem: 5511
[06:44:02.376303] Epoch: [10]  [100/781]  eta: 0:02:18  lr: 0.000248  training_loss: 2.4681 (2.4676)  mae_loss: 0.4407 (0.4332)  classification_loss: 1.9015 (1.9110)  loss_mask: 0.1518 (0.1234)  time: 0.1971  data: 0.0002  max mem: 5511
[06:44:06.315530] Epoch: [10]  [120/781]  eta: 0:02:14  lr: 0.000248  training_loss: 2.4480 (2.4644)  mae_loss: 0.4285 (0.4328)  classification_loss: 1.8986 (1.9099)  loss_mask: 0.0956 (0.1217)  time: 0.1969  data: 0.0006  max mem: 5511
[06:44:10.247722] Epoch: [10]  [140/781]  eta: 0:02:09  lr: 0.000248  training_loss: 2.3763 (2.4516)  mae_loss: 0.4388 (0.4342)  classification_loss: 1.8716 (1.9011)  loss_mask: 0.0720 (0.1163)  time: 0.1965  data: 0.0002  max mem: 5511
[06:44:14.193844] Epoch: [10]  [160/781]  eta: 0:02:05  lr: 0.000248  training_loss: 2.4128 (2.4513)  mae_loss: 0.4326 (0.4356)  classification_loss: 1.8723 (1.8985)  loss_mask: 0.1110 (0.1172)  time: 0.1972  data: 0.0002  max mem: 5511
[06:44:18.143994] Epoch: [10]  [180/781]  eta: 0:02:00  lr: 0.000248  training_loss: 2.4421 (2.4552)  mae_loss: 0.4495 (0.4361)  classification_loss: 1.9162 (1.8992)  loss_mask: 0.1124 (0.1200)  time: 0.1974  data: 0.0003  max mem: 5511
[06:44:22.094677] Epoch: [10]  [200/781]  eta: 0:01:56  lr: 0.000248  training_loss: 2.5123 (2.4609)  mae_loss: 0.4350 (0.4358)  classification_loss: 1.9239 (1.8992)  loss_mask: 0.1703 (0.1260)  time: 0.1974  data: 0.0003  max mem: 5511
[06:44:26.038728] Epoch: [10]  [220/781]  eta: 0:01:52  lr: 0.000248  training_loss: 2.5848 (2.4692)  mae_loss: 0.4174 (0.4353)  classification_loss: 1.9192 (1.8995)  loss_mask: 0.1536 (0.1343)  time: 0.1971  data: 0.0002  max mem: 5511
[06:44:29.983601] Epoch: [10]  [240/781]  eta: 0:01:48  lr: 0.000248  training_loss: 2.4775 (2.4705)  mae_loss: 0.4320 (0.4360)  classification_loss: 1.9160 (1.8996)  loss_mask: 0.1199 (0.1349)  time: 0.1971  data: 0.0002  max mem: 5511
[06:44:33.931421] Epoch: [10]  [260/781]  eta: 0:01:44  lr: 0.000248  training_loss: 2.4580 (2.4673)  mae_loss: 0.4288 (0.4354)  classification_loss: 1.8779 (1.8978)  loss_mask: 0.1222 (0.1342)  time: 0.1973  data: 0.0002  max mem: 5511
[06:44:37.897482] Epoch: [10]  [280/781]  eta: 0:01:40  lr: 0.000248  training_loss: 2.4009 (2.4644)  mae_loss: 0.4451 (0.4355)  classification_loss: 1.8931 (1.8970)  loss_mask: 0.0884 (0.1318)  time: 0.1982  data: 0.0002  max mem: 5511
[06:44:41.879165] Epoch: [10]  [300/781]  eta: 0:01:36  lr: 0.000248  training_loss: 2.4558 (2.4643)  mae_loss: 0.4415 (0.4367)  classification_loss: 1.9032 (1.8972)  loss_mask: 0.0973 (0.1304)  time: 0.1990  data: 0.0002  max mem: 5511
[06:44:45.843593] Epoch: [10]  [320/781]  eta: 0:01:31  lr: 0.000248  training_loss: 2.3931 (2.4594)  mae_loss: 0.4281 (0.4362)  classification_loss: 1.8624 (1.8948)  loss_mask: 0.0879 (0.1284)  time: 0.1981  data: 0.0002  max mem: 5511
[06:44:49.825752] Epoch: [10]  [340/781]  eta: 0:01:27  lr: 0.000248  training_loss: 2.4229 (2.4588)  mae_loss: 0.4314 (0.4364)  classification_loss: 1.8760 (1.8941)  loss_mask: 0.1051 (0.1283)  time: 0.1990  data: 0.0002  max mem: 5511
[06:44:53.777035] Epoch: [10]  [360/781]  eta: 0:01:23  lr: 0.000248  training_loss: 2.4167 (2.4567)  mae_loss: 0.4307 (0.4358)  classification_loss: 1.8864 (1.8949)  loss_mask: 0.0819 (0.1260)  time: 0.1975  data: 0.0002  max mem: 5511
[06:44:57.755003] Epoch: [10]  [380/781]  eta: 0:01:19  lr: 0.000248  training_loss: 2.3770 (2.4535)  mae_loss: 0.4095 (0.4354)  classification_loss: 1.8570 (1.8942)  loss_mask: 0.0913 (0.1239)  time: 0.1988  data: 0.0003  max mem: 5511
[06:45:01.693133] Epoch: [10]  [400/781]  eta: 0:01:15  lr: 0.000248  training_loss: 2.4425 (2.4520)  mae_loss: 0.4202 (0.4348)  classification_loss: 1.8655 (1.8931)  loss_mask: 0.0926 (0.1241)  time: 0.1968  data: 0.0003  max mem: 5511
[06:45:05.632955] Epoch: [10]  [420/781]  eta: 0:01:11  lr: 0.000248  training_loss: 2.3974 (2.4489)  mae_loss: 0.4260 (0.4343)  classification_loss: 1.8496 (1.8918)  loss_mask: 0.0806 (0.1228)  time: 0.1969  data: 0.0004  max mem: 5511
[06:45:09.604082] Epoch: [10]  [440/781]  eta: 0:01:07  lr: 0.000248  training_loss: 2.3960 (2.4470)  mae_loss: 0.4324 (0.4341)  classification_loss: 1.8736 (1.8910)  loss_mask: 0.0879 (0.1218)  time: 0.1984  data: 0.0002  max mem: 5511
[06:45:13.569746] Epoch: [10]  [460/781]  eta: 0:01:03  lr: 0.000248  training_loss: 2.3735 (2.4448)  mae_loss: 0.3980 (0.4337)  classification_loss: 1.8442 (1.8891)  loss_mask: 0.0927 (0.1220)  time: 0.1982  data: 0.0002  max mem: 5511
[06:45:17.522805] Epoch: [10]  [480/781]  eta: 0:00:59  lr: 0.000248  training_loss: 2.4504 (2.4470)  mae_loss: 0.4471 (0.4343)  classification_loss: 1.8858 (1.8893)  loss_mask: 0.1043 (0.1234)  time: 0.1976  data: 0.0003  max mem: 5511
[06:45:21.481322] Epoch: [10]  [500/781]  eta: 0:00:55  lr: 0.000248  training_loss: 2.3557 (2.4455)  mae_loss: 0.4458 (0.4350)  classification_loss: 1.8415 (1.8884)  loss_mask: 0.0848 (0.1220)  time: 0.1978  data: 0.0002  max mem: 5511
[06:45:25.435867] Epoch: [10]  [520/781]  eta: 0:00:51  lr: 0.000248  training_loss: 2.3792 (2.4439)  mae_loss: 0.4461 (0.4354)  classification_loss: 1.8257 (1.8868)  loss_mask: 0.0957 (0.1217)  time: 0.1976  data: 0.0002  max mem: 5511
[06:45:29.397352] Epoch: [10]  [540/781]  eta: 0:00:47  lr: 0.000248  training_loss: 2.4764 (2.4461)  mae_loss: 0.4378 (0.4353)  classification_loss: 1.8896 (1.8877)  loss_mask: 0.1597 (0.1231)  time: 0.1980  data: 0.0003  max mem: 5511
[06:45:33.373774] Epoch: [10]  [560/781]  eta: 0:00:43  lr: 0.000248  training_loss: 2.4232 (2.4458)  mae_loss: 0.4459 (0.4356)  classification_loss: 1.8947 (1.8883)  loss_mask: 0.0905 (0.1219)  time: 0.1987  data: 0.0002  max mem: 5511
[06:45:37.345349] Epoch: [10]  [580/781]  eta: 0:00:39  lr: 0.000248  training_loss: 2.4140 (2.4443)  mae_loss: 0.4262 (0.4354)  classification_loss: 1.8635 (1.8882)  loss_mask: 0.0835 (0.1206)  time: 0.1985  data: 0.0002  max mem: 5511
[06:45:41.297110] Epoch: [10]  [600/781]  eta: 0:00:35  lr: 0.000248  training_loss: 2.3210 (2.4417)  mae_loss: 0.4310 (0.4353)  classification_loss: 1.8311 (1.8869)  loss_mask: 0.0808 (0.1195)  time: 0.1975  data: 0.0002  max mem: 5511
[06:45:45.250160] Epoch: [10]  [620/781]  eta: 0:00:32  lr: 0.000248  training_loss: 2.4642 (2.4430)  mae_loss: 0.4326 (0.4355)  classification_loss: 1.8059 (1.8853)  loss_mask: 0.1684 (0.1223)  time: 0.1976  data: 0.0002  max mem: 5511
[06:45:49.224347] Epoch: [10]  [640/781]  eta: 0:00:28  lr: 0.000248  training_loss: 2.3837 (2.4413)  mae_loss: 0.4234 (0.4353)  classification_loss: 1.8380 (1.8844)  loss_mask: 0.0889 (0.1216)  time: 0.1986  data: 0.0002  max mem: 5511
[06:45:53.166489] Epoch: [10]  [660/781]  eta: 0:00:24  lr: 0.000248  training_loss: 2.3634 (2.4398)  mae_loss: 0.4177 (0.4349)  classification_loss: 1.8529 (1.8845)  loss_mask: 0.0809 (0.1204)  time: 0.1970  data: 0.0003  max mem: 5511
[06:45:57.134037] Epoch: [10]  [680/781]  eta: 0:00:20  lr: 0.000248  training_loss: 2.3903 (2.4380)  mae_loss: 0.4206 (0.4346)  classification_loss: 1.8637 (1.8836)  loss_mask: 0.0880 (0.1199)  time: 0.1983  data: 0.0003  max mem: 5511
[06:46:01.102832] Epoch: [10]  [700/781]  eta: 0:00:16  lr: 0.000248  training_loss: 2.3366 (2.4358)  mae_loss: 0.4393 (0.4348)  classification_loss: 1.7847 (1.8820)  loss_mask: 0.0884 (0.1191)  time: 0.1983  data: 0.0002  max mem: 5511
[06:46:05.098748] Epoch: [10]  [720/781]  eta: 0:00:12  lr: 0.000248  training_loss: 2.4086 (2.4355)  mae_loss: 0.4229 (0.4343)  classification_loss: 1.8467 (1.8814)  loss_mask: 0.1110 (0.1198)  time: 0.1997  data: 0.0002  max mem: 5511
[06:46:09.075899] Epoch: [10]  [740/781]  eta: 0:00:08  lr: 0.000248  training_loss: 2.4231 (2.4351)  mae_loss: 0.4202 (0.4342)  classification_loss: 1.8234 (1.8807)  loss_mask: 0.1142 (0.1202)  time: 0.1988  data: 0.0003  max mem: 5511
[06:46:13.039851] Epoch: [10]  [760/781]  eta: 0:00:04  lr: 0.000248  training_loss: 2.3957 (2.4346)  mae_loss: 0.4025 (0.4335)  classification_loss: 1.8698 (1.8803)  loss_mask: 0.1124 (0.1208)  time: 0.1981  data: 0.0002  max mem: 5511
[06:46:16.988486] Epoch: [10]  [780/781]  eta: 0:00:00  lr: 0.000248  training_loss: 2.3788 (2.4333)  mae_loss: 0.4126 (0.4332)  classification_loss: 1.8819 (1.8801)  loss_mask: 0.0785 (0.1201)  time: 0.1974  data: 0.0002  max mem: 5511
[06:46:17.148703] Epoch: [10] Total time: 0:02:35 (0.1990 s / it)
[06:46:17.149218] Averaged stats: lr: 0.000248  training_loss: 2.3788 (2.4333)  mae_loss: 0.4126 (0.4332)  classification_loss: 1.8819 (1.8801)  loss_mask: 0.0785 (0.1201)
[06:46:18.096560] Test:  [  0/157]  eta: 0:01:32  testing_loss: 1.2661 (1.2661)  acc1: 64.0625 (64.0625)  acc5: 93.7500 (93.7500)  time: 0.5889  data: 0.5458  max mem: 5511
[06:46:18.403259] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 1.3437 (1.3532)  acc1: 56.2500 (53.5511)  acc5: 93.7500 (94.8864)  time: 0.0813  data: 0.0498  max mem: 5511
[06:46:18.696747] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.3065 (1.3114)  acc1: 56.2500 (56.8452)  acc5: 93.7500 (94.5685)  time: 0.0298  data: 0.0004  max mem: 5511
[06:46:18.982538] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.3073 (1.3135)  acc1: 56.2500 (56.3508)  acc5: 95.3125 (94.8589)  time: 0.0288  data: 0.0005  max mem: 5511
[06:46:19.266459] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.3251 (1.3209)  acc1: 56.2500 (56.4024)  acc5: 93.7500 (94.5122)  time: 0.0284  data: 0.0002  max mem: 5511
[06:46:19.550345] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.3085 (1.3155)  acc1: 57.8125 (56.6789)  acc5: 93.7500 (94.5159)  time: 0.0283  data: 0.0002  max mem: 5511
[06:46:19.833827] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.2744 (1.3117)  acc1: 56.2500 (56.6086)  acc5: 93.7500 (94.4672)  time: 0.0282  data: 0.0002  max mem: 5511
[06:46:20.117896] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.2697 (1.3078)  acc1: 56.2500 (56.9102)  acc5: 93.7500 (94.5863)  time: 0.0282  data: 0.0002  max mem: 5511
[06:46:20.406141] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.2966 (1.3091)  acc1: 56.2500 (56.6165)  acc5: 95.3125 (94.5988)  time: 0.0285  data: 0.0002  max mem: 5511
[06:46:20.690373] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.3170 (1.3127)  acc1: 54.6875 (56.3702)  acc5: 93.7500 (94.5570)  time: 0.0285  data: 0.0002  max mem: 5511
[06:46:20.981056] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.3920 (1.3186)  acc1: 53.1250 (55.9715)  acc5: 93.7500 (94.4926)  time: 0.0286  data: 0.0002  max mem: 5511
[06:46:21.270781] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.3726 (1.3217)  acc1: 53.1250 (55.9685)  acc5: 93.7500 (94.5242)  time: 0.0289  data: 0.0002  max mem: 5511
[06:46:21.555202] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.3404 (1.3204)  acc1: 53.1250 (55.7593)  acc5: 95.3125 (94.6152)  time: 0.0285  data: 0.0002  max mem: 5511
[06:46:21.839247] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.3404 (1.3221)  acc1: 53.1250 (55.6775)  acc5: 93.7500 (94.5134)  time: 0.0283  data: 0.0002  max mem: 5511
[06:46:22.121303] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.3521 (1.3231)  acc1: 53.1250 (55.6184)  acc5: 93.7500 (94.4814)  time: 0.0282  data: 0.0002  max mem: 5511
[06:46:22.403127] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.3420 (1.3205)  acc1: 56.2500 (55.8671)  acc5: 93.7500 (94.4536)  time: 0.0281  data: 0.0001  max mem: 5511
[06:46:22.556648] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.3102 (1.3217)  acc1: 56.2500 (55.8200)  acc5: 95.3125 (94.5200)  time: 0.0272  data: 0.0001  max mem: 5511
[06:46:22.737872] Test: Total time: 0:00:05 (0.0333 s / it)
[06:46:22.738332] * Acc@1 55.820 Acc@5 94.520 loss 1.322
[06:46:22.738626] Accuracy of the network on the 10000 test images: 55.8%
[06:46:22.738846] Max accuracy: 55.82%
[06:46:22.840967] log_dir: ./output_dir
[06:46:23.774707] Epoch: [11]  [  0/781]  eta: 0:12:07  lr: 0.000248  training_loss: 2.1807 (2.1807)  mae_loss: 0.4017 (0.4017)  classification_loss: 1.7489 (1.7489)  loss_mask: 0.0301 (0.0301)  time: 0.9319  data: 0.7045  max mem: 5511
[06:46:27.750947] Epoch: [11]  [ 20/781]  eta: 0:02:57  lr: 0.000248  training_loss: 2.3381 (2.3763)  mae_loss: 0.4213 (0.4289)  classification_loss: 1.8350 (1.8287)  loss_mask: 0.0859 (0.1188)  time: 0.1987  data: 0.0002  max mem: 5511
[06:46:31.714989] Epoch: [11]  [ 40/781]  eta: 0:02:40  lr: 0.000248  training_loss: 2.4987 (2.4249)  mae_loss: 0.4294 (0.4305)  classification_loss: 1.8732 (1.8480)  loss_mask: 0.1610 (0.1464)  time: 0.1981  data: 0.0002  max mem: 5511
[06:46:35.673319] Epoch: [11]  [ 60/781]  eta: 0:02:31  lr: 0.000247  training_loss: 2.4101 (2.4198)  mae_loss: 0.4240 (0.4289)  classification_loss: 1.8680 (1.8556)  loss_mask: 0.1051 (0.1353)  time: 0.1978  data: 0.0002  max mem: 5511
[06:46:39.660440] Epoch: [11]  [ 80/781]  eta: 0:02:25  lr: 0.000247  training_loss: 2.3744 (2.4218)  mae_loss: 0.4289 (0.4312)  classification_loss: 1.8583 (1.8554)  loss_mask: 0.1093 (0.1353)  time: 0.1992  data: 0.0002  max mem: 5511
[06:46:43.609102] Epoch: [11]  [100/781]  eta: 0:02:19  lr: 0.000247  training_loss: 2.3778 (2.4125)  mae_loss: 0.4121 (0.4291)  classification_loss: 1.8680 (1.8572)  loss_mask: 0.0908 (0.1262)  time: 0.1974  data: 0.0003  max mem: 5511
[06:46:47.549727] Epoch: [11]  [120/781]  eta: 0:02:14  lr: 0.000247  training_loss: 2.3037 (2.3983)  mae_loss: 0.4110 (0.4256)  classification_loss: 1.8324 (1.8545)  loss_mask: 0.0766 (0.1183)  time: 0.1969  data: 0.0002  max mem: 5511
[06:46:51.488072] Epoch: [11]  [140/781]  eta: 0:02:10  lr: 0.000247  training_loss: 2.3490 (2.3934)  mae_loss: 0.4198 (0.4255)  classification_loss: 1.8374 (1.8516)  loss_mask: 0.0905 (0.1163)  time: 0.1968  data: 0.0002  max mem: 5511
[06:46:55.439641] Epoch: [11]  [160/781]  eta: 0:02:05  lr: 0.000247  training_loss: 2.3588 (2.3908)  mae_loss: 0.4221 (0.4258)  classification_loss: 1.8609 (1.8546)  loss_mask: 0.0658 (0.1104)  time: 0.1975  data: 0.0002  max mem: 5511
[06:46:59.403872] Epoch: [11]  [180/781]  eta: 0:02:01  lr: 0.000247  training_loss: 2.3451 (2.3868)  mae_loss: 0.4261 (0.4260)  classification_loss: 1.8337 (1.8526)  loss_mask: 0.0828 (0.1082)  time: 0.1981  data: 0.0002  max mem: 5511
[06:47:03.358847] Epoch: [11]  [200/781]  eta: 0:01:57  lr: 0.000247  training_loss: 2.3864 (2.3864)  mae_loss: 0.4177 (0.4257)  classification_loss: 1.8543 (1.8531)  loss_mask: 0.0884 (0.1076)  time: 0.1977  data: 0.0002  max mem: 5511
[06:47:07.330943] Epoch: [11]  [220/781]  eta: 0:01:52  lr: 0.000247  training_loss: 2.3621 (2.3876)  mae_loss: 0.4040 (0.4245)  classification_loss: 1.8474 (1.8535)  loss_mask: 0.1181 (0.1097)  time: 0.1985  data: 0.0002  max mem: 5511
[06:47:11.282176] Epoch: [11]  [240/781]  eta: 0:01:48  lr: 0.000247  training_loss: 2.4239 (2.3924)  mae_loss: 0.4001 (0.4231)  classification_loss: 1.8773 (1.8559)  loss_mask: 0.1093 (0.1134)  time: 0.1975  data: 0.0002  max mem: 5511
[06:47:15.223261] Epoch: [11]  [260/781]  eta: 0:01:44  lr: 0.000247  training_loss: 2.3068 (2.3897)  mae_loss: 0.4084 (0.4222)  classification_loss: 1.7911 (1.8520)  loss_mask: 0.1034 (0.1156)  time: 0.1970  data: 0.0002  max mem: 5511
[06:47:19.160002] Epoch: [11]  [280/781]  eta: 0:01:40  lr: 0.000247  training_loss: 2.3716 (2.3878)  mae_loss: 0.4208 (0.4219)  classification_loss: 1.8729 (1.8522)  loss_mask: 0.0814 (0.1138)  time: 0.1968  data: 0.0003  max mem: 5511
[06:47:23.105749] Epoch: [11]  [300/781]  eta: 0:01:36  lr: 0.000247  training_loss: 2.3719 (2.3893)  mae_loss: 0.4148 (0.4220)  classification_loss: 1.8756 (1.8528)  loss_mask: 0.1069 (0.1146)  time: 0.1972  data: 0.0003  max mem: 5511
[06:47:27.087850] Epoch: [11]  [320/781]  eta: 0:01:32  lr: 0.000247  training_loss: 2.3928 (2.3913)  mae_loss: 0.4168 (0.4220)  classification_loss: 1.8455 (1.8536)  loss_mask: 0.1240 (0.1156)  time: 0.1990  data: 0.0003  max mem: 5511
[06:47:31.055304] Epoch: [11]  [340/781]  eta: 0:01:28  lr: 0.000247  training_loss: 2.4088 (2.3916)  mae_loss: 0.4002 (0.4213)  classification_loss: 1.8610 (1.8539)  loss_mask: 0.1033 (0.1164)  time: 0.1983  data: 0.0004  max mem: 5511
[06:47:34.987869] Epoch: [11]  [360/781]  eta: 0:01:24  lr: 0.000247  training_loss: 2.3704 (2.3924)  mae_loss: 0.4068 (0.4201)  classification_loss: 1.8825 (1.8552)  loss_mask: 0.1152 (0.1171)  time: 0.1966  data: 0.0002  max mem: 5511
[06:47:38.954668] Epoch: [11]  [380/781]  eta: 0:01:20  lr: 0.000247  training_loss: 2.3629 (2.3927)  mae_loss: 0.4372 (0.4207)  classification_loss: 1.8533 (1.8549)  loss_mask: 0.0969 (0.1172)  time: 0.1982  data: 0.0003  max mem: 5511
[06:47:42.894074] Epoch: [11]  [400/781]  eta: 0:01:16  lr: 0.000247  training_loss: 2.3721 (2.3920)  mae_loss: 0.4058 (0.4201)  classification_loss: 1.8056 (1.8548)  loss_mask: 0.1132 (0.1172)  time: 0.1969  data: 0.0002  max mem: 5511
[06:47:46.833029] Epoch: [11]  [420/781]  eta: 0:01:11  lr: 0.000247  training_loss: 2.3433 (2.3890)  mae_loss: 0.4154 (0.4199)  classification_loss: 1.8299 (1.8538)  loss_mask: 0.0728 (0.1153)  time: 0.1968  data: 0.0002  max mem: 5511
[06:47:50.815917] Epoch: [11]  [440/781]  eta: 0:01:07  lr: 0.000247  training_loss: 2.3353 (2.3877)  mae_loss: 0.4026 (0.4194)  classification_loss: 1.8427 (1.8536)  loss_mask: 0.0893 (0.1147)  time: 0.1991  data: 0.0002  max mem: 5511
[06:47:54.768279] Epoch: [11]  [460/781]  eta: 0:01:03  lr: 0.000247  training_loss: 2.3038 (2.3845)  mae_loss: 0.3938 (0.4187)  classification_loss: 1.8237 (1.8518)  loss_mask: 0.0855 (0.1140)  time: 0.1975  data: 0.0002  max mem: 5511
[06:47:58.726850] Epoch: [11]  [480/781]  eta: 0:00:59  lr: 0.000247  training_loss: 2.3818 (2.3847)  mae_loss: 0.3899 (0.4176)  classification_loss: 1.8701 (1.8526)  loss_mask: 0.0934 (0.1144)  time: 0.1979  data: 0.0002  max mem: 5511
[06:48:02.662143] Epoch: [11]  [500/781]  eta: 0:00:55  lr: 0.000247  training_loss: 2.3299 (2.3830)  mae_loss: 0.4014 (0.4170)  classification_loss: 1.8327 (1.8523)  loss_mask: 0.0868 (0.1136)  time: 0.1967  data: 0.0002  max mem: 5511
[06:48:06.617176] Epoch: [11]  [520/781]  eta: 0:00:51  lr: 0.000247  training_loss: 2.4145 (2.3826)  mae_loss: 0.4085 (0.4167)  classification_loss: 1.8166 (1.8521)  loss_mask: 0.0952 (0.1138)  time: 0.1976  data: 0.0002  max mem: 5511
[06:48:10.572349] Epoch: [11]  [540/781]  eta: 0:00:47  lr: 0.000247  training_loss: 2.3574 (2.3816)  mae_loss: 0.4088 (0.4164)  classification_loss: 1.8067 (1.8515)  loss_mask: 0.1058 (0.1137)  time: 0.1977  data: 0.0003  max mem: 5511
[06:48:14.475781] Epoch: [11]  [560/781]  eta: 0:00:43  lr: 0.000247  training_loss: 2.3751 (2.3822)  mae_loss: 0.3863 (0.4158)  classification_loss: 1.8147 (1.8513)  loss_mask: 0.1211 (0.1150)  time: 0.1950  data: 0.0003  max mem: 5511
[06:48:18.417906] Epoch: [11]  [580/781]  eta: 0:00:39  lr: 0.000247  training_loss: 2.3644 (2.3809)  mae_loss: 0.3923 (0.4152)  classification_loss: 1.7911 (1.8504)  loss_mask: 0.1103 (0.1153)  time: 0.1970  data: 0.0003  max mem: 5511
[06:48:22.351941] Epoch: [11]  [600/781]  eta: 0:00:35  lr: 0.000247  training_loss: 2.3564 (2.3805)  mae_loss: 0.3954 (0.4144)  classification_loss: 1.8273 (1.8495)  loss_mask: 0.1261 (0.1166)  time: 0.1966  data: 0.0002  max mem: 5511
[06:48:26.301062] Epoch: [11]  [620/781]  eta: 0:00:31  lr: 0.000247  training_loss: 2.3400 (2.3787)  mae_loss: 0.3967 (0.4139)  classification_loss: 1.8135 (1.8489)  loss_mask: 0.0863 (0.1159)  time: 0.1974  data: 0.0003  max mem: 5511
[06:48:30.249859] Epoch: [11]  [640/781]  eta: 0:00:28  lr: 0.000247  training_loss: 2.2571 (2.3757)  mae_loss: 0.3769 (0.4129)  classification_loss: 1.7956 (1.8481)  loss_mask: 0.0761 (0.1147)  time: 0.1973  data: 0.0003  max mem: 5511
[06:48:34.178568] Epoch: [11]  [660/781]  eta: 0:00:24  lr: 0.000247  training_loss: 2.3045 (2.3741)  mae_loss: 0.3841 (0.4123)  classification_loss: 1.8082 (1.8477)  loss_mask: 0.0697 (0.1142)  time: 0.1964  data: 0.0002  max mem: 5511
[06:48:38.121257] Epoch: [11]  [680/781]  eta: 0:00:20  lr: 0.000247  training_loss: 2.3460 (2.3738)  mae_loss: 0.4151 (0.4124)  classification_loss: 1.8412 (1.8479)  loss_mask: 0.0707 (0.1134)  time: 0.1971  data: 0.0002  max mem: 5511
[06:48:42.049669] Epoch: [11]  [700/781]  eta: 0:00:16  lr: 0.000247  training_loss: 2.3345 (2.3728)  mae_loss: 0.3867 (0.4121)  classification_loss: 1.8269 (1.8479)  loss_mask: 0.0957 (0.1128)  time: 0.1963  data: 0.0003  max mem: 5511
[06:48:45.976522] Epoch: [11]  [720/781]  eta: 0:00:12  lr: 0.000247  training_loss: 2.3180 (2.3715)  mae_loss: 0.3864 (0.4115)  classification_loss: 1.8455 (1.8482)  loss_mask: 0.0665 (0.1118)  time: 0.1963  data: 0.0002  max mem: 5511
[06:48:49.918637] Epoch: [11]  [740/781]  eta: 0:00:08  lr: 0.000247  training_loss: 2.3027 (2.3701)  mae_loss: 0.3747 (0.4108)  classification_loss: 1.8365 (1.8474)  loss_mask: 0.1107 (0.1119)  time: 0.1970  data: 0.0002  max mem: 5511
[06:48:53.856925] Epoch: [11]  [760/781]  eta: 0:00:04  lr: 0.000247  training_loss: 2.3425 (2.3695)  mae_loss: 0.3946 (0.4106)  classification_loss: 1.8565 (1.8478)  loss_mask: 0.0733 (0.1112)  time: 0.1968  data: 0.0003  max mem: 5511
[06:48:57.781403] Epoch: [11]  [780/781]  eta: 0:00:00  lr: 0.000247  training_loss: 2.3012 (2.3681)  mae_loss: 0.3763 (0.4101)  classification_loss: 1.8392 (1.8478)  loss_mask: 0.0609 (0.1102)  time: 0.1961  data: 0.0002  max mem: 5511
[06:48:57.957229] Epoch: [11] Total time: 0:02:35 (0.1986 s / it)
[06:48:57.957716] Averaged stats: lr: 0.000247  training_loss: 2.3012 (2.3681)  mae_loss: 0.3763 (0.4101)  classification_loss: 1.8392 (1.8478)  loss_mask: 0.0609 (0.1102)
[06:48:58.715999] Test:  [  0/157]  eta: 0:01:58  testing_loss: 1.1915 (1.1915)  acc1: 59.3750 (59.3750)  acc5: 92.1875 (92.1875)  time: 0.7539  data: 0.7228  max mem: 5511
[06:48:59.001911] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.3410 (1.3276)  acc1: 57.8125 (53.6932)  acc5: 93.7500 (93.4659)  time: 0.0943  data: 0.0659  max mem: 5511
[06:48:59.285205] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.3131 (1.2915)  acc1: 57.8125 (55.4315)  acc5: 95.3125 (94.4940)  time: 0.0282  data: 0.0002  max mem: 5511
[06:48:59.568433] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.2817 (1.2892)  acc1: 59.3750 (56.1492)  acc5: 95.3125 (94.5060)  time: 0.0282  data: 0.0002  max mem: 5511
[06:48:59.858348] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.2717 (1.2894)  acc1: 57.8125 (56.9741)  acc5: 95.3125 (94.5884)  time: 0.0285  data: 0.0002  max mem: 5511
[06:49:00.144312] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.2533 (1.2822)  acc1: 60.9375 (57.4755)  acc5: 93.7500 (94.4853)  time: 0.0286  data: 0.0002  max mem: 5511
[06:49:00.431318] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.2460 (1.2755)  acc1: 60.9375 (57.7357)  acc5: 93.7500 (94.4160)  time: 0.0285  data: 0.0002  max mem: 5511
[06:49:00.717536] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.2136 (1.2692)  acc1: 59.3750 (57.9445)  acc5: 95.3125 (94.6303)  time: 0.0285  data: 0.0002  max mem: 5511
[06:49:01.005966] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.2311 (1.2700)  acc1: 59.3750 (58.0440)  acc5: 95.3125 (94.6952)  time: 0.0285  data: 0.0002  max mem: 5511
[06:49:01.295491] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.2607 (1.2737)  acc1: 56.2500 (57.7266)  acc5: 93.7500 (94.5913)  time: 0.0287  data: 0.0003  max mem: 5511
[06:49:01.581004] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.3283 (1.2778)  acc1: 54.6875 (57.5804)  acc5: 93.7500 (94.5854)  time: 0.0286  data: 0.0003  max mem: 5511
[06:49:01.866344] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.3450 (1.2806)  acc1: 54.6875 (57.4043)  acc5: 95.3125 (94.6650)  time: 0.0284  data: 0.0002  max mem: 5511
[06:49:02.151200] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.2759 (1.2779)  acc1: 56.2500 (57.5026)  acc5: 96.8750 (94.8347)  time: 0.0284  data: 0.0002  max mem: 5511
[06:49:02.435471] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.2665 (1.2797)  acc1: 57.8125 (57.3235)  acc5: 96.8750 (94.8235)  time: 0.0283  data: 0.0002  max mem: 5511
[06:49:02.718636] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.2659 (1.2800)  acc1: 57.8125 (57.4136)  acc5: 93.7500 (94.8360)  time: 0.0282  data: 0.0002  max mem: 5511
[06:49:03.001760] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.2659 (1.2780)  acc1: 59.3750 (57.5124)  acc5: 95.3125 (94.8572)  time: 0.0282  data: 0.0002  max mem: 5511
[06:49:03.156061] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.2745 (1.2795)  acc1: 56.2500 (57.3700)  acc5: 95.3125 (94.9000)  time: 0.0274  data: 0.0001  max mem: 5511
[06:49:03.330571] Test: Total time: 0:00:05 (0.0342 s / it)
[06:49:03.331047] * Acc@1 57.370 Acc@5 94.900 loss 1.279
[06:49:03.331411] Accuracy of the network on the 10000 test images: 57.4%
[06:49:03.331656] Max accuracy: 57.37%
[06:49:03.498621] log_dir: ./output_dir
[06:49:04.513897] Epoch: [12]  [  0/781]  eta: 0:13:11  lr: 0.000247  training_loss: 2.1859 (2.1859)  mae_loss: 0.4197 (0.4197)  classification_loss: 1.7126 (1.7126)  loss_mask: 0.0536 (0.0536)  time: 1.0135  data: 0.7983  max mem: 5511
[06:49:08.455714] Epoch: [12]  [ 20/781]  eta: 0:02:59  lr: 0.000247  training_loss: 2.3012 (2.3057)  mae_loss: 0.3935 (0.3896)  classification_loss: 1.7978 (1.8242)  loss_mask: 0.0839 (0.0919)  time: 0.1970  data: 0.0002  max mem: 5511
[06:49:12.374980] Epoch: [12]  [ 40/781]  eta: 0:02:40  lr: 0.000247  training_loss: 2.3069 (2.3011)  mae_loss: 0.3722 (0.3837)  classification_loss: 1.8609 (1.8379)  loss_mask: 0.0591 (0.0795)  time: 0.1959  data: 0.0003  max mem: 5511
[06:49:16.319714] Epoch: [12]  [ 60/781]  eta: 0:02:31  lr: 0.000247  training_loss: 2.3229 (2.3120)  mae_loss: 0.3850 (0.3874)  classification_loss: 1.8437 (1.8410)  loss_mask: 0.0698 (0.0836)  time: 0.1972  data: 0.0003  max mem: 5511
[06:49:20.317808] Epoch: [12]  [ 80/781]  eta: 0:02:25  lr: 0.000247  training_loss: 2.3018 (2.3163)  mae_loss: 0.3900 (0.3867)  classification_loss: 1.8413 (1.8456)  loss_mask: 0.0841 (0.0841)  time: 0.1998  data: 0.0002  max mem: 5511
[06:49:24.281287] Epoch: [12]  [100/781]  eta: 0:02:20  lr: 0.000247  training_loss: 2.2872 (2.3140)  mae_loss: 0.3695 (0.3844)  classification_loss: 1.8314 (1.8468)  loss_mask: 0.0671 (0.0829)  time: 0.1981  data: 0.0003  max mem: 5511
[06:49:28.243833] Epoch: [12]  [120/781]  eta: 0:02:15  lr: 0.000247  training_loss: 2.2831 (2.3109)  mae_loss: 0.3694 (0.3824)  classification_loss: 1.8160 (1.8408)  loss_mask: 0.0996 (0.0876)  time: 0.1981  data: 0.0002  max mem: 5511
[06:49:32.197891] Epoch: [12]  [140/781]  eta: 0:02:10  lr: 0.000247  training_loss: 2.3121 (2.3100)  mae_loss: 0.3838 (0.3831)  classification_loss: 1.8050 (1.8373)  loss_mask: 0.0945 (0.0897)  time: 0.1976  data: 0.0002  max mem: 5511
[06:49:36.156286] Epoch: [12]  [160/781]  eta: 0:02:05  lr: 0.000246  training_loss: 2.2968 (2.3144)  mae_loss: 0.3752 (0.3836)  classification_loss: 1.8241 (1.8375)  loss_mask: 0.1024 (0.0933)  time: 0.1978  data: 0.0003  max mem: 5511
[06:49:40.113169] Epoch: [12]  [180/781]  eta: 0:02:01  lr: 0.000246  training_loss: 2.2994 (2.3154)  mae_loss: 0.4144 (0.3870)  classification_loss: 1.7854 (1.8345)  loss_mask: 0.0911 (0.0939)  time: 0.1977  data: 0.0002  max mem: 5511
[06:49:44.071790] Epoch: [12]  [200/781]  eta: 0:01:57  lr: 0.000246  training_loss: 2.2724 (2.3151)  mae_loss: 0.3738 (0.3862)  classification_loss: 1.8314 (1.8336)  loss_mask: 0.1017 (0.0954)  time: 0.1978  data: 0.0003  max mem: 5511
[06:49:48.015777] Epoch: [12]  [220/781]  eta: 0:01:52  lr: 0.000246  training_loss: 2.3638 (2.3174)  mae_loss: 0.3734 (0.3857)  classification_loss: 1.8702 (1.8325)  loss_mask: 0.0903 (0.0992)  time: 0.1971  data: 0.0002  max mem: 5511
[06:49:51.965691] Epoch: [12]  [240/781]  eta: 0:01:48  lr: 0.000246  training_loss: 2.2897 (2.3149)  mae_loss: 0.3813 (0.3853)  classification_loss: 1.8011 (1.8306)  loss_mask: 0.0856 (0.0990)  time: 0.1974  data: 0.0003  max mem: 5511
[06:49:55.920009] Epoch: [12]  [260/781]  eta: 0:01:44  lr: 0.000246  training_loss: 2.2768 (2.3113)  mae_loss: 0.3691 (0.3841)  classification_loss: 1.8056 (1.8298)  loss_mask: 0.0712 (0.0974)  time: 0.1976  data: 0.0002  max mem: 5511
[06:49:59.885317] Epoch: [12]  [280/781]  eta: 0:01:40  lr: 0.000246  training_loss: 2.3199 (2.3108)  mae_loss: 0.3885 (0.3853)  classification_loss: 1.8124 (1.8284)  loss_mask: 0.0782 (0.0971)  time: 0.1982  data: 0.0002  max mem: 5511
[06:50:03.849976] Epoch: [12]  [300/781]  eta: 0:01:36  lr: 0.000246  training_loss: 2.2700 (2.3080)  mae_loss: 0.3653 (0.3851)  classification_loss: 1.8035 (1.8284)  loss_mask: 0.0461 (0.0945)  time: 0.1981  data: 0.0003  max mem: 5511
[06:50:07.771560] Epoch: [12]  [320/781]  eta: 0:01:32  lr: 0.000246  training_loss: 2.2241 (2.3045)  mae_loss: 0.3652 (0.3849)  classification_loss: 1.7897 (1.8272)  loss_mask: 0.0538 (0.0924)  time: 0.1960  data: 0.0002  max mem: 5511
[06:50:11.720682] Epoch: [12]  [340/781]  eta: 0:01:28  lr: 0.000246  training_loss: 2.3425 (2.3105)  mae_loss: 0.3849 (0.3848)  classification_loss: 1.7965 (1.8251)  loss_mask: 0.1905 (0.1007)  time: 0.1973  data: 0.0002  max mem: 5511
[06:50:15.662621] Epoch: [12]  [360/781]  eta: 0:01:24  lr: 0.000246  training_loss: 2.3452 (2.3132)  mae_loss: 0.3720 (0.3847)  classification_loss: 1.8359 (1.8258)  loss_mask: 0.1038 (0.1027)  time: 0.1970  data: 0.0002  max mem: 5511
[06:50:19.579856] Epoch: [12]  [380/781]  eta: 0:01:20  lr: 0.000246  training_loss: 2.2896 (2.3139)  mae_loss: 0.3991 (0.3856)  classification_loss: 1.8102 (1.8255)  loss_mask: 0.0860 (0.1029)  time: 0.1958  data: 0.0002  max mem: 5511
[06:50:23.537518] Epoch: [12]  [400/781]  eta: 0:01:16  lr: 0.000246  training_loss: 2.2890 (2.3142)  mae_loss: 0.3976 (0.3864)  classification_loss: 1.8272 (1.8254)  loss_mask: 0.0874 (0.1024)  time: 0.1978  data: 0.0002  max mem: 5511
[06:50:27.481635] Epoch: [12]  [420/781]  eta: 0:01:11  lr: 0.000246  training_loss: 2.2703 (2.3133)  mae_loss: 0.3686 (0.3862)  classification_loss: 1.8196 (1.8253)  loss_mask: 0.0904 (0.1018)  time: 0.1971  data: 0.0002  max mem: 5511
[06:50:31.438550] Epoch: [12]  [440/781]  eta: 0:01:07  lr: 0.000246  training_loss: 2.2847 (2.3118)  mae_loss: 0.3859 (0.3860)  classification_loss: 1.8163 (1.8253)  loss_mask: 0.0628 (0.1005)  time: 0.1978  data: 0.0002  max mem: 5511
[06:50:35.358431] Epoch: [12]  [460/781]  eta: 0:01:03  lr: 0.000246  training_loss: 2.2218 (2.3082)  mae_loss: 0.3615 (0.3851)  classification_loss: 1.7863 (1.8234)  loss_mask: 0.0786 (0.0997)  time: 0.1959  data: 0.0002  max mem: 5511
[06:50:39.305784] Epoch: [12]  [480/781]  eta: 0:00:59  lr: 0.000246  training_loss: 2.2686 (2.3069)  mae_loss: 0.3662 (0.3845)  classification_loss: 1.7661 (1.8230)  loss_mask: 0.0799 (0.0993)  time: 0.1973  data: 0.0002  max mem: 5511
[06:50:43.295003] Epoch: [12]  [500/781]  eta: 0:00:55  lr: 0.000246  training_loss: 2.2177 (2.3036)  mae_loss: 0.3609 (0.3837)  classification_loss: 1.7656 (1.8211)  loss_mask: 0.0716 (0.0987)  time: 0.1994  data: 0.0004  max mem: 5511
[06:50:47.259057] Epoch: [12]  [520/781]  eta: 0:00:51  lr: 0.000246  training_loss: 2.2587 (2.3025)  mae_loss: 0.3714 (0.3834)  classification_loss: 1.7515 (1.8200)  loss_mask: 0.0967 (0.0991)  time: 0.1981  data: 0.0002  max mem: 5511
[06:50:51.216272] Epoch: [12]  [540/781]  eta: 0:00:47  lr: 0.000246  training_loss: 2.3945 (2.3055)  mae_loss: 0.3642 (0.3833)  classification_loss: 1.7592 (1.8190)  loss_mask: 0.1743 (0.1032)  time: 0.1978  data: 0.0002  max mem: 5511
[06:50:55.168724] Epoch: [12]  [560/781]  eta: 0:00:43  lr: 0.000246  training_loss: 2.2942 (2.3056)  mae_loss: 0.3728 (0.3830)  classification_loss: 1.8068 (1.8187)  loss_mask: 0.0881 (0.1039)  time: 0.1975  data: 0.0004  max mem: 5511
[06:50:59.120330] Epoch: [12]  [580/781]  eta: 0:00:39  lr: 0.000246  training_loss: 2.2260 (2.3036)  mae_loss: 0.3672 (0.3828)  classification_loss: 1.7953 (1.8182)  loss_mask: 0.0577 (0.1026)  time: 0.1975  data: 0.0002  max mem: 5511
[06:51:03.086982] Epoch: [12]  [600/781]  eta: 0:00:35  lr: 0.000246  training_loss: 2.1842 (2.3015)  mae_loss: 0.3859 (0.3825)  classification_loss: 1.7584 (1.8173)  loss_mask: 0.0663 (0.1017)  time: 0.1982  data: 0.0002  max mem: 5511
[06:51:07.026755] Epoch: [12]  [620/781]  eta: 0:00:32  lr: 0.000246  training_loss: 2.1704 (2.2982)  mae_loss: 0.3608 (0.3823)  classification_loss: 1.7446 (1.8155)  loss_mask: 0.0535 (0.1004)  time: 0.1969  data: 0.0002  max mem: 5511
[06:51:10.980904] Epoch: [12]  [640/781]  eta: 0:00:28  lr: 0.000246  training_loss: 2.2289 (2.2962)  mae_loss: 0.3547 (0.3819)  classification_loss: 1.7942 (1.8144)  loss_mask: 0.0790 (0.1000)  time: 0.1976  data: 0.0002  max mem: 5511
[06:51:14.931921] Epoch: [12]  [660/781]  eta: 0:00:24  lr: 0.000246  training_loss: 2.2756 (2.2950)  mae_loss: 0.3633 (0.3815)  classification_loss: 1.8575 (1.8149)  loss_mask: 0.0535 (0.0986)  time: 0.1975  data: 0.0002  max mem: 5511
[06:51:18.881990] Epoch: [12]  [680/781]  eta: 0:00:20  lr: 0.000246  training_loss: 2.2482 (2.2942)  mae_loss: 0.3679 (0.3811)  classification_loss: 1.8043 (1.8149)  loss_mask: 0.0681 (0.0981)  time: 0.1974  data: 0.0002  max mem: 5511
[06:51:22.871544] Epoch: [12]  [700/781]  eta: 0:00:16  lr: 0.000246  training_loss: 2.2492 (2.2936)  mae_loss: 0.3578 (0.3808)  classification_loss: 1.8228 (1.8143)  loss_mask: 0.0942 (0.0986)  time: 0.1994  data: 0.0002  max mem: 5511
[06:51:26.816320] Epoch: [12]  [720/781]  eta: 0:00:12  lr: 0.000246  training_loss: 2.2626 (2.2932)  mae_loss: 0.3715 (0.3806)  classification_loss: 1.7993 (1.8150)  loss_mask: 0.0655 (0.0977)  time: 0.1972  data: 0.0002  max mem: 5511
[06:51:30.759309] Epoch: [12]  [740/781]  eta: 0:00:08  lr: 0.000246  training_loss: 2.2352 (2.2917)  mae_loss: 0.3567 (0.3799)  classification_loss: 1.7864 (1.8147)  loss_mask: 0.0604 (0.0971)  time: 0.1971  data: 0.0002  max mem: 5511
[06:51:34.698646] Epoch: [12]  [760/781]  eta: 0:00:04  lr: 0.000246  training_loss: 2.2381 (2.2908)  mae_loss: 0.3648 (0.3793)  classification_loss: 1.7916 (1.8147)  loss_mask: 0.0743 (0.0967)  time: 0.1969  data: 0.0002  max mem: 5511
[06:51:38.623596] Epoch: [12]  [780/781]  eta: 0:00:00  lr: 0.000246  training_loss: 2.2777 (2.2906)  mae_loss: 0.3550 (0.3789)  classification_loss: 1.7950 (1.8144)  loss_mask: 0.1123 (0.0973)  time: 0.1962  data: 0.0002  max mem: 5511
[06:51:38.787828] Epoch: [12] Total time: 0:02:35 (0.1988 s / it)
[06:51:38.788453] Averaged stats: lr: 0.000246  training_loss: 2.2777 (2.2906)  mae_loss: 0.3550 (0.3789)  classification_loss: 1.7950 (1.8144)  loss_mask: 0.1123 (0.0973)
[06:51:39.535261] Test:  [  0/157]  eta: 0:01:56  testing_loss: 1.0506 (1.0506)  acc1: 67.1875 (67.1875)  acc5: 96.8750 (96.8750)  time: 0.7422  data: 0.7065  max mem: 5511
[06:51:39.821512] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.2204 (1.2084)  acc1: 59.3750 (57.6705)  acc5: 96.8750 (96.0227)  time: 0.0932  data: 0.0645  max mem: 5511
[06:51:40.107427] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.1470 (1.1665)  acc1: 60.9375 (60.0446)  acc5: 96.8750 (96.8750)  time: 0.0284  data: 0.0002  max mem: 5511
[06:51:40.394481] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.1521 (1.1764)  acc1: 60.9375 (60.3831)  acc5: 96.8750 (96.3206)  time: 0.0285  data: 0.0003  max mem: 5511
[06:51:40.680688] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.1780 (1.1746)  acc1: 60.9375 (60.6707)  acc5: 95.3125 (96.2652)  time: 0.0285  data: 0.0003  max mem: 5511
[06:51:40.967364] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.1438 (1.1681)  acc1: 62.5000 (61.3664)  acc5: 96.8750 (96.3235)  time: 0.0285  data: 0.0002  max mem: 5511
[06:51:41.253025] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.1309 (1.1615)  acc1: 64.0625 (61.3986)  acc5: 96.8750 (96.3115)  time: 0.0285  data: 0.0002  max mem: 5511
[06:51:41.540112] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.1251 (1.1556)  acc1: 62.5000 (61.5097)  acc5: 96.8750 (96.4569)  time: 0.0285  data: 0.0002  max mem: 5511
[06:51:41.826498] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.1446 (1.1579)  acc1: 60.9375 (61.1883)  acc5: 96.8750 (96.3156)  time: 0.0285  data: 0.0002  max mem: 5511
[06:51:42.113302] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.1935 (1.1613)  acc1: 57.8125 (60.8345)  acc5: 95.3125 (96.1882)  time: 0.0285  data: 0.0004  max mem: 5511
[06:51:42.403531] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.1979 (1.1667)  acc1: 57.8125 (60.4734)  acc5: 95.3125 (96.2407)  time: 0.0287  data: 0.0003  max mem: 5511
[06:51:42.692048] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.2270 (1.1705)  acc1: 54.6875 (60.2055)  acc5: 95.3125 (96.2134)  time: 0.0288  data: 0.0002  max mem: 5511
[06:51:42.977300] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.1799 (1.1675)  acc1: 60.9375 (60.4597)  acc5: 95.3125 (96.2035)  time: 0.0286  data: 0.0002  max mem: 5511
[06:51:43.261481] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.1616 (1.1707)  acc1: 60.9375 (60.2457)  acc5: 95.3125 (96.1832)  time: 0.0283  data: 0.0002  max mem: 5511
[06:51:43.545088] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.1911 (1.1705)  acc1: 60.9375 (60.4388)  acc5: 95.3125 (96.1769)  time: 0.0283  data: 0.0002  max mem: 5511
[06:51:43.827278] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.1514 (1.1683)  acc1: 62.5000 (60.6374)  acc5: 95.3125 (96.1921)  time: 0.0282  data: 0.0001  max mem: 5511
[06:51:43.977246] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.1565 (1.1694)  acc1: 62.5000 (60.5500)  acc5: 95.3125 (96.1500)  time: 0.0271  data: 0.0001  max mem: 5511
[06:51:44.156390] Test: Total time: 0:00:05 (0.0342 s / it)
[06:51:44.157104] * Acc@1 60.550 Acc@5 96.150 loss 1.169
[06:51:44.157572] Accuracy of the network on the 10000 test images: 60.5%
[06:51:44.157894] Max accuracy: 60.55%
[06:51:44.318720] log_dir: ./output_dir
[06:51:45.230670] Epoch: [13]  [  0/781]  eta: 0:11:50  lr: 0.000246  training_loss: 2.2446 (2.2446)  mae_loss: 0.4099 (0.4099)  classification_loss: 1.7377 (1.7377)  loss_mask: 0.0970 (0.0970)  time: 0.9103  data: 0.7046  max mem: 5511
[06:51:49.205648] Epoch: [13]  [ 20/781]  eta: 0:02:56  lr: 0.000246  training_loss: 2.2187 (2.2216)  mae_loss: 0.3495 (0.3592)  classification_loss: 1.7549 (1.7598)  loss_mask: 0.1031 (0.1027)  time: 0.1986  data: 0.0002  max mem: 5511
[06:51:53.195635] Epoch: [13]  [ 40/781]  eta: 0:02:40  lr: 0.000246  training_loss: 2.2979 (2.2650)  mae_loss: 0.3715 (0.3637)  classification_loss: 1.8240 (1.8031)  loss_mask: 0.0897 (0.0982)  time: 0.1994  data: 0.0002  max mem: 5511
[06:51:57.121523] Epoch: [13]  [ 60/781]  eta: 0:02:31  lr: 0.000246  training_loss: 2.2550 (2.2674)  mae_loss: 0.3656 (0.3674)  classification_loss: 1.8304 (1.8134)  loss_mask: 0.0500 (0.0867)  time: 0.1962  data: 0.0002  max mem: 5511
[06:52:01.099792] Epoch: [13]  [ 80/781]  eta: 0:02:25  lr: 0.000246  training_loss: 2.2789 (2.2708)  mae_loss: 0.3570 (0.3683)  classification_loss: 1.7959 (1.8131)  loss_mask: 0.0951 (0.0894)  time: 0.1988  data: 0.0002  max mem: 5511
[06:52:05.049407] Epoch: [13]  [100/781]  eta: 0:02:19  lr: 0.000246  training_loss: 2.2461 (2.2707)  mae_loss: 0.3632 (0.3671)  classification_loss: 1.8293 (1.8150)  loss_mask: 0.0924 (0.0886)  time: 0.1974  data: 0.0002  max mem: 5511
[06:52:09.011963] Epoch: [13]  [120/781]  eta: 0:02:14  lr: 0.000246  training_loss: 2.2543 (2.2646)  mae_loss: 0.3498 (0.3654)  classification_loss: 1.8089 (1.8119)  loss_mask: 0.0646 (0.0874)  time: 0.1980  data: 0.0002  max mem: 5511
[06:52:12.934522] Epoch: [13]  [140/781]  eta: 0:02:10  lr: 0.000245  training_loss: 2.2083 (2.2599)  mae_loss: 0.3657 (0.3646)  classification_loss: 1.7432 (1.8069)  loss_mask: 0.0756 (0.0885)  time: 0.1960  data: 0.0002  max mem: 5511
[06:52:16.863455] Epoch: [13]  [160/781]  eta: 0:02:05  lr: 0.000245  training_loss: 2.2331 (2.2600)  mae_loss: 0.3430 (0.3624)  classification_loss: 1.7813 (1.8066)  loss_mask: 0.0800 (0.0910)  time: 0.1964  data: 0.0002  max mem: 5511
[06:52:20.803963] Epoch: [13]  [180/781]  eta: 0:02:01  lr: 0.000245  training_loss: 2.2154 (2.2554)  mae_loss: 0.3709 (0.3629)  classification_loss: 1.7594 (1.8015)  loss_mask: 0.0851 (0.0910)  time: 0.1969  data: 0.0003  max mem: 5511
[06:52:24.743722] Epoch: [13]  [200/781]  eta: 0:01:56  lr: 0.000245  training_loss: 2.2074 (2.2534)  mae_loss: 0.3667 (0.3632)  classification_loss: 1.7725 (1.8001)  loss_mask: 0.0702 (0.0900)  time: 0.1969  data: 0.0003  max mem: 5511
[06:52:28.704690] Epoch: [13]  [220/781]  eta: 0:01:52  lr: 0.000245  training_loss: 2.2603 (2.2524)  mae_loss: 0.3396 (0.3619)  classification_loss: 1.8332 (1.8022)  loss_mask: 0.0574 (0.0883)  time: 0.1979  data: 0.0002  max mem: 5511
[06:52:32.628830] Epoch: [13]  [240/781]  eta: 0:01:48  lr: 0.000245  training_loss: 2.2561 (2.2538)  mae_loss: 0.3598 (0.3614)  classification_loss: 1.8231 (1.8041)  loss_mask: 0.0874 (0.0883)  time: 0.1961  data: 0.0002  max mem: 5511
[06:52:36.554969] Epoch: [13]  [260/781]  eta: 0:01:44  lr: 0.000245  training_loss: 2.2345 (2.2520)  mae_loss: 0.3454 (0.3603)  classification_loss: 1.7923 (1.8029)  loss_mask: 0.0793 (0.0888)  time: 0.1962  data: 0.0003  max mem: 5511
[06:52:40.502719] Epoch: [13]  [280/781]  eta: 0:01:40  lr: 0.000245  training_loss: 2.1782 (2.2482)  mae_loss: 0.3563 (0.3602)  classification_loss: 1.7725 (1.8012)  loss_mask: 0.0442 (0.0868)  time: 0.1972  data: 0.0002  max mem: 5511
[06:52:44.451219] Epoch: [13]  [300/781]  eta: 0:01:36  lr: 0.000245  training_loss: 2.2516 (2.2480)  mae_loss: 0.3598 (0.3605)  classification_loss: 1.7929 (1.7999)  loss_mask: 0.0746 (0.0876)  time: 0.1973  data: 0.0003  max mem: 5511
[06:52:48.372115] Epoch: [13]  [320/781]  eta: 0:01:31  lr: 0.000245  training_loss: 2.3162 (2.2542)  mae_loss: 0.3503 (0.3602)  classification_loss: 1.7926 (1.8007)  loss_mask: 0.1567 (0.0933)  time: 0.1960  data: 0.0002  max mem: 5511
[06:52:52.334519] Epoch: [13]  [340/781]  eta: 0:01:27  lr: 0.000245  training_loss: 2.1822 (2.2515)  mae_loss: 0.3447 (0.3595)  classification_loss: 1.7659 (1.7991)  loss_mask: 0.0747 (0.0929)  time: 0.1981  data: 0.0002  max mem: 5511
[06:52:56.262948] Epoch: [13]  [360/781]  eta: 0:01:23  lr: 0.000245  training_loss: 2.1729 (2.2483)  mae_loss: 0.3480 (0.3590)  classification_loss: 1.7615 (1.7975)  loss_mask: 0.0600 (0.0918)  time: 0.1963  data: 0.0002  max mem: 5511
[06:53:00.206858] Epoch: [13]  [380/781]  eta: 0:01:19  lr: 0.000245  training_loss: 2.1879 (2.2462)  mae_loss: 0.3456 (0.3589)  classification_loss: 1.7882 (1.7974)  loss_mask: 0.0491 (0.0899)  time: 0.1971  data: 0.0002  max mem: 5511
[06:53:04.231247] Epoch: [13]  [400/781]  eta: 0:01:15  lr: 0.000245  training_loss: 2.2102 (2.2442)  mae_loss: 0.3426 (0.3585)  classification_loss: 1.7734 (1.7967)  loss_mask: 0.0581 (0.0890)  time: 0.2011  data: 0.0003  max mem: 5511
[06:53:08.177246] Epoch: [13]  [420/781]  eta: 0:01:11  lr: 0.000245  training_loss: 2.2104 (2.2423)  mae_loss: 0.3439 (0.3581)  classification_loss: 1.7462 (1.7956)  loss_mask: 0.0673 (0.0887)  time: 0.1972  data: 0.0002  max mem: 5511
[06:53:12.134720] Epoch: [13]  [440/781]  eta: 0:01:07  lr: 0.000245  training_loss: 2.1787 (2.2405)  mae_loss: 0.3518 (0.3579)  classification_loss: 1.7578 (1.7944)  loss_mask: 0.0658 (0.0882)  time: 0.1977  data: 0.0002  max mem: 5511
[06:53:16.080496] Epoch: [13]  [460/781]  eta: 0:01:03  lr: 0.000245  training_loss: 2.1749 (2.2394)  mae_loss: 0.3242 (0.3571)  classification_loss: 1.7623 (1.7932)  loss_mask: 0.0898 (0.0892)  time: 0.1972  data: 0.0002  max mem: 5511
[06:53:20.067265] Epoch: [13]  [480/781]  eta: 0:00:59  lr: 0.000245  training_loss: 2.1964 (2.2385)  mae_loss: 0.3472 (0.3569)  classification_loss: 1.7477 (1.7923)  loss_mask: 0.0865 (0.0894)  time: 0.1993  data: 0.0002  max mem: 5511
[06:53:24.018268] Epoch: [13]  [500/781]  eta: 0:00:55  lr: 0.000245  training_loss: 2.2047 (2.2380)  mae_loss: 0.3388 (0.3563)  classification_loss: 1.7774 (1.7915)  loss_mask: 0.1085 (0.0902)  time: 0.1974  data: 0.0002  max mem: 5511
[06:53:27.968707] Epoch: [13]  [520/781]  eta: 0:00:51  lr: 0.000245  training_loss: 2.2190 (2.2380)  mae_loss: 0.3457 (0.3560)  classification_loss: 1.7615 (1.7910)  loss_mask: 0.0819 (0.0911)  time: 0.1974  data: 0.0002  max mem: 5511
[06:53:31.977971] Epoch: [13]  [540/781]  eta: 0:00:47  lr: 0.000245  training_loss: 2.2577 (2.2394)  mae_loss: 0.3633 (0.3564)  classification_loss: 1.8211 (1.7924)  loss_mask: 0.0770 (0.0907)  time: 0.2004  data: 0.0002  max mem: 5511
[06:53:35.949518] Epoch: [13]  [560/781]  eta: 0:00:43  lr: 0.000245  training_loss: 2.1896 (2.2377)  mae_loss: 0.3530 (0.3566)  classification_loss: 1.7662 (1.7914)  loss_mask: 0.0667 (0.0897)  time: 0.1985  data: 0.0002  max mem: 5511
[06:53:39.925250] Epoch: [13]  [580/781]  eta: 0:00:39  lr: 0.000245  training_loss: 2.2494 (2.2397)  mae_loss: 0.3467 (0.3562)  classification_loss: 1.8341 (1.7922)  loss_mask: 0.1072 (0.0913)  time: 0.1987  data: 0.0002  max mem: 5511
[06:53:43.893945] Epoch: [13]  [600/781]  eta: 0:00:35  lr: 0.000245  training_loss: 2.2533 (2.2396)  mae_loss: 0.3455 (0.3560)  classification_loss: 1.7685 (1.7918)  loss_mask: 0.0952 (0.0918)  time: 0.1983  data: 0.0003  max mem: 5511
[06:53:47.843212] Epoch: [13]  [620/781]  eta: 0:00:32  lr: 0.000245  training_loss: 2.1483 (2.2378)  mae_loss: 0.3451 (0.3558)  classification_loss: 1.7311 (1.7907)  loss_mask: 0.0693 (0.0913)  time: 0.1974  data: 0.0002  max mem: 5511
[06:53:51.789876] Epoch: [13]  [640/781]  eta: 0:00:28  lr: 0.000245  training_loss: 2.2326 (2.2383)  mae_loss: 0.3550 (0.3558)  classification_loss: 1.7799 (1.7898)  loss_mask: 0.1122 (0.0928)  time: 0.1973  data: 0.0002  max mem: 5511
[06:53:55.778162] Epoch: [13]  [660/781]  eta: 0:00:24  lr: 0.000245  training_loss: 2.5702 (2.2553)  mae_loss: 0.3797 (0.3564)  classification_loss: 1.8498 (1.7930)  loss_mask: 0.2537 (0.1059)  time: 0.1993  data: 0.0002  max mem: 5511
[06:53:59.728011] Epoch: [13]  [680/781]  eta: 0:00:20  lr: 0.000245  training_loss: 2.5683 (2.2657)  mae_loss: 0.3894 (0.3575)  classification_loss: 1.9951 (1.7986)  loss_mask: 0.2015 (0.1095)  time: 0.1974  data: 0.0002  max mem: 5511
[06:54:03.692364] Epoch: [13]  [700/781]  eta: 0:00:16  lr: 0.000245  training_loss: 2.5365 (2.2734)  mae_loss: 0.3846 (0.3584)  classification_loss: 1.8856 (1.8015)  loss_mask: 0.2168 (0.1135)  time: 0.1981  data: 0.0002  max mem: 5511
[06:54:07.671528] Epoch: [13]  [720/781]  eta: 0:00:12  lr: 0.000245  training_loss: 2.5129 (2.2809)  mae_loss: 0.3690 (0.3585)  classification_loss: 1.9096 (1.8053)  loss_mask: 0.2184 (0.1171)  time: 0.1989  data: 0.0002  max mem: 5511
[06:54:11.636318] Epoch: [13]  [740/781]  eta: 0:00:08  lr: 0.000245  training_loss: 2.5195 (2.2875)  mae_loss: 0.3634 (0.3586)  classification_loss: 1.9324 (1.8088)  loss_mask: 0.2486 (0.1202)  time: 0.1982  data: 0.0002  max mem: 5511
[06:54:15.611889] Epoch: [13]  [760/781]  eta: 0:00:04  lr: 0.000245  training_loss: 2.4194 (2.2911)  mae_loss: 0.3678 (0.3588)  classification_loss: 1.8944 (1.8114)  loss_mask: 0.1339 (0.1209)  time: 0.1987  data: 0.0002  max mem: 5511
[06:54:19.558827] Epoch: [13]  [780/781]  eta: 0:00:00  lr: 0.000245  training_loss: 2.4851 (2.2991)  mae_loss: 0.3657 (0.3592)  classification_loss: 1.9376 (1.8135)  loss_mask: 0.2651 (0.1264)  time: 0.1973  data: 0.0002  max mem: 5511
[06:54:19.731130] Epoch: [13] Total time: 0:02:35 (0.1990 s / it)
[06:54:19.731623] Averaged stats: lr: 0.000245  training_loss: 2.4851 (2.2991)  mae_loss: 0.3657 (0.3592)  classification_loss: 1.9376 (1.8135)  loss_mask: 0.2651 (0.1264)
[06:54:20.468121] Test:  [  0/157]  eta: 0:01:54  testing_loss: 1.4123 (1.4123)  acc1: 54.6875 (54.6875)  acc5: 92.1875 (92.1875)  time: 0.7319  data: 0.6978  max mem: 5511
[06:54:20.757962] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.4784 (1.4876)  acc1: 51.5625 (52.6989)  acc5: 92.1875 (92.8977)  time: 0.0927  data: 0.0636  max mem: 5511
[06:54:21.048240] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.4784 (1.4726)  acc1: 51.5625 (52.9762)  acc5: 92.1875 (93.6012)  time: 0.0288  data: 0.0002  max mem: 5511
[06:54:21.343099] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.4832 (1.4813)  acc1: 51.5625 (52.7218)  acc5: 92.1875 (93.0444)  time: 0.0291  data: 0.0002  max mem: 5511
[06:54:21.627214] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.4943 (1.4852)  acc1: 51.5625 (52.7058)  acc5: 92.1875 (92.7210)  time: 0.0288  data: 0.0002  max mem: 5511
[06:54:21.913801] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.4842 (1.4839)  acc1: 51.5625 (52.6348)  acc5: 92.1875 (92.9228)  time: 0.0284  data: 0.0002  max mem: 5511
[06:54:22.213477] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.4771 (1.4822)  acc1: 51.5625 (52.4590)  acc5: 93.7500 (93.0584)  time: 0.0291  data: 0.0002  max mem: 5511
[06:54:22.497131] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.4477 (1.4786)  acc1: 51.5625 (52.6849)  acc5: 93.7500 (93.1338)  time: 0.0290  data: 0.0002  max mem: 5511
[06:54:22.784076] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.4629 (1.4795)  acc1: 53.1250 (52.7006)  acc5: 93.7500 (93.2485)  time: 0.0284  data: 0.0002  max mem: 5511
[06:54:23.071709] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.4966 (1.4816)  acc1: 51.5625 (52.4725)  acc5: 93.7500 (93.2864)  time: 0.0286  data: 0.0002  max mem: 5511
[06:54:23.359854] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.4966 (1.4850)  acc1: 50.0000 (52.3979)  acc5: 92.1875 (93.0693)  time: 0.0286  data: 0.0002  max mem: 5511
[06:54:23.649750] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.5210 (1.4887)  acc1: 50.0000 (52.3649)  acc5: 90.6250 (92.9336)  time: 0.0287  data: 0.0002  max mem: 5511
[06:54:23.935940] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.4913 (1.4865)  acc1: 53.1250 (52.5310)  acc5: 92.1875 (92.9881)  time: 0.0287  data: 0.0002  max mem: 5511
[06:54:24.224286] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.4878 (1.4881)  acc1: 54.6875 (52.5048)  acc5: 92.1875 (92.9389)  time: 0.0286  data: 0.0003  max mem: 5511
[06:54:24.513233] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.5181 (1.4892)  acc1: 53.1250 (52.5598)  acc5: 92.1875 (92.8302)  time: 0.0287  data: 0.0003  max mem: 5511
[06:54:24.793353] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.4691 (1.4873)  acc1: 53.1250 (52.6800)  acc5: 92.1875 (92.8394)  time: 0.0283  data: 0.0001  max mem: 5511
[06:54:24.945210] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.4904 (1.4890)  acc1: 56.2500 (52.6800)  acc5: 92.1875 (92.7500)  time: 0.0271  data: 0.0001  max mem: 5511
[06:54:25.123102] Test: Total time: 0:00:05 (0.0343 s / it)
[06:54:25.123560] * Acc@1 52.680 Acc@5 92.750 loss 1.489
[06:54:25.123969] Accuracy of the network on the 10000 test images: 52.7%
[06:54:25.124285] Max accuracy: 60.55%
[06:54:25.229399] log_dir: ./output_dir
[06:54:26.205324] Epoch: [14]  [  0/781]  eta: 0:12:40  lr: 0.000245  training_loss: 2.3844 (2.3844)  mae_loss: 0.3342 (0.3342)  classification_loss: 1.8685 (1.8685)  loss_mask: 0.1818 (0.1818)  time: 0.9740  data: 0.7655  max mem: 5511
[06:54:30.141247] Epoch: [14]  [ 20/781]  eta: 0:02:57  lr: 0.000244  training_loss: 2.3924 (2.4379)  mae_loss: 0.3602 (0.3615)  classification_loss: 1.8745 (1.8868)  loss_mask: 0.2007 (0.1896)  time: 0.1967  data: 0.0002  max mem: 5511
[06:54:34.099569] Epoch: [14]  [ 40/781]  eta: 0:02:40  lr: 0.000244  training_loss: 2.4120 (2.4165)  mae_loss: 0.3563 (0.3633)  classification_loss: 1.8474 (1.8684)  loss_mask: 0.1622 (0.1848)  time: 0.1978  data: 0.0003  max mem: 5511
[06:54:38.014973] Epoch: [14]  [ 60/781]  eta: 0:02:31  lr: 0.000244  training_loss: 2.4280 (2.4200)  mae_loss: 0.3687 (0.3649)  classification_loss: 1.8802 (1.8678)  loss_mask: 0.1761 (0.1873)  time: 0.1956  data: 0.0002  max mem: 5511
[06:54:41.958843] Epoch: [14]  [ 80/781]  eta: 0:02:24  lr: 0.000244  training_loss: 2.3587 (2.4084)  mae_loss: 0.3463 (0.3620)  classification_loss: 1.8628 (1.8698)  loss_mask: 0.1380 (0.1766)  time: 0.1971  data: 0.0003  max mem: 5511
[06:54:45.892557] Epoch: [14]  [100/781]  eta: 0:02:19  lr: 0.000244  training_loss: 2.3410 (2.3946)  mae_loss: 0.3478 (0.3586)  classification_loss: 1.8521 (1.8672)  loss_mask: 0.1336 (0.1688)  time: 0.1966  data: 0.0002  max mem: 5511
[06:54:49.824506] Epoch: [14]  [120/781]  eta: 0:02:14  lr: 0.000244  training_loss: 2.2937 (2.3857)  mae_loss: 0.3404 (0.3576)  classification_loss: 1.8320 (1.8584)  loss_mask: 0.1267 (0.1696)  time: 0.1965  data: 0.0003  max mem: 5511
[06:54:53.760002] Epoch: [14]  [140/781]  eta: 0:02:09  lr: 0.000244  training_loss: 2.2768 (2.3684)  mae_loss: 0.3533 (0.3567)  classification_loss: 1.7741 (1.8455)  loss_mask: 0.1278 (0.1662)  time: 0.1967  data: 0.0003  max mem: 5511
[06:54:57.694344] Epoch: [14]  [160/781]  eta: 0:02:05  lr: 0.000244  training_loss: 2.1749 (2.3478)  mae_loss: 0.3311 (0.3538)  classification_loss: 1.7379 (1.8361)  loss_mask: 0.0844 (0.1579)  time: 0.1966  data: 0.0003  max mem: 5511
[06:55:01.618699] Epoch: [14]  [180/781]  eta: 0:02:00  lr: 0.000244  training_loss: 2.2443 (2.3366)  mae_loss: 0.3574 (0.3543)  classification_loss: 1.7783 (1.8308)  loss_mask: 0.1028 (0.1515)  time: 0.1961  data: 0.0003  max mem: 5511
[06:55:05.542110] Epoch: [14]  [200/781]  eta: 0:01:56  lr: 0.000244  training_loss: 2.2029 (2.3250)  mae_loss: 0.3334 (0.3531)  classification_loss: 1.7609 (1.8272)  loss_mask: 0.0810 (0.1448)  time: 0.1961  data: 0.0003  max mem: 5511
[06:55:09.466963] Epoch: [14]  [220/781]  eta: 0:01:52  lr: 0.000244  training_loss: 2.1879 (2.3145)  mae_loss: 0.3445 (0.3521)  classification_loss: 1.7956 (1.8243)  loss_mask: 0.0643 (0.1381)  time: 0.1962  data: 0.0003  max mem: 5511
[06:55:13.420942] Epoch: [14]  [240/781]  eta: 0:01:48  lr: 0.000244  training_loss: 2.2384 (2.3102)  mae_loss: 0.3495 (0.3523)  classification_loss: 1.7873 (1.8224)  loss_mask: 0.0832 (0.1355)  time: 0.1976  data: 0.0003  max mem: 5511
[06:55:17.406866] Epoch: [14]  [260/781]  eta: 0:01:44  lr: 0.000244  training_loss: 2.2622 (2.3058)  mae_loss: 0.3315 (0.3513)  classification_loss: 1.7950 (1.8210)  loss_mask: 0.0989 (0.1334)  time: 0.1992  data: 0.0002  max mem: 5511
[06:55:21.349279] Epoch: [14]  [280/781]  eta: 0:01:40  lr: 0.000244  training_loss: 2.2488 (2.3024)  mae_loss: 0.3533 (0.3515)  classification_loss: 1.7818 (1.8199)  loss_mask: 0.0804 (0.1310)  time: 0.1970  data: 0.0002  max mem: 5511
[06:55:25.309640] Epoch: [14]  [300/781]  eta: 0:01:35  lr: 0.000244  training_loss: 2.1780 (2.2956)  mae_loss: 0.3525 (0.3512)  classification_loss: 1.7574 (1.8174)  loss_mask: 0.0661 (0.1270)  time: 0.1979  data: 0.0002  max mem: 5511
[06:55:29.257646] Epoch: [14]  [320/781]  eta: 0:01:31  lr: 0.000244  training_loss: 2.2255 (2.2902)  mae_loss: 0.3289 (0.3502)  classification_loss: 1.8181 (1.8156)  loss_mask: 0.0829 (0.1244)  time: 0.1973  data: 0.0003  max mem: 5511
[06:55:33.204317] Epoch: [14]  [340/781]  eta: 0:01:27  lr: 0.000244  training_loss: 2.2269 (2.2866)  mae_loss: 0.3481 (0.3502)  classification_loss: 1.7760 (1.8144)  loss_mask: 0.0751 (0.1220)  time: 0.1973  data: 0.0002  max mem: 5511
[06:55:37.159857] Epoch: [14]  [360/781]  eta: 0:01:23  lr: 0.000244  training_loss: 2.2085 (2.2830)  mae_loss: 0.3356 (0.3498)  classification_loss: 1.7867 (1.8144)  loss_mask: 0.0717 (0.1189)  time: 0.1977  data: 0.0003  max mem: 5511
[06:55:41.101631] Epoch: [14]  [380/781]  eta: 0:01:19  lr: 0.000244  training_loss: 2.2307 (2.2812)  mae_loss: 0.3309 (0.3497)  classification_loss: 1.8014 (1.8139)  loss_mask: 0.0817 (0.1177)  time: 0.1970  data: 0.0002  max mem: 5511
[06:55:45.057802] Epoch: [14]  [400/781]  eta: 0:01:15  lr: 0.000244  training_loss: 2.1994 (2.2776)  mae_loss: 0.3354 (0.3493)  classification_loss: 1.7553 (1.8119)  loss_mask: 0.0746 (0.1164)  time: 0.1977  data: 0.0002  max mem: 5511
[06:55:49.006915] Epoch: [14]  [420/781]  eta: 0:01:11  lr: 0.000244  training_loss: 2.2381 (2.2779)  mae_loss: 0.3532 (0.3494)  classification_loss: 1.7604 (1.8096)  loss_mask: 0.1571 (0.1190)  time: 0.1974  data: 0.0003  max mem: 5511
[06:55:52.953082] Epoch: [14]  [440/781]  eta: 0:01:07  lr: 0.000244  training_loss: 2.2165 (2.2752)  mae_loss: 0.3351 (0.3490)  classification_loss: 1.7304 (1.8073)  loss_mask: 0.1138 (0.1189)  time: 0.1972  data: 0.0002  max mem: 5511
[06:55:56.914187] Epoch: [14]  [460/781]  eta: 0:01:03  lr: 0.000244  training_loss: 2.2615 (2.2770)  mae_loss: 0.3482 (0.3490)  classification_loss: 1.8108 (1.8090)  loss_mask: 0.1144 (0.1190)  time: 0.1980  data: 0.0002  max mem: 5511
[06:56:00.916180] Epoch: [14]  [480/781]  eta: 0:00:59  lr: 0.000244  training_loss: 2.2827 (2.2770)  mae_loss: 0.3363 (0.3492)  classification_loss: 1.8434 (1.8097)  loss_mask: 0.0832 (0.1181)  time: 0.2000  data: 0.0003  max mem: 5511
[06:56:04.893502] Epoch: [14]  [500/781]  eta: 0:00:55  lr: 0.000244  training_loss: 2.1715 (2.2738)  mae_loss: 0.3366 (0.3484)  classification_loss: 1.7504 (1.8087)  loss_mask: 0.0747 (0.1168)  time: 0.1987  data: 0.0004  max mem: 5511
[06:56:08.857882] Epoch: [14]  [520/781]  eta: 0:00:51  lr: 0.000244  training_loss: 2.1684 (2.2718)  mae_loss: 0.3569 (0.3488)  classification_loss: 1.7680 (1.8070)  loss_mask: 0.0916 (0.1160)  time: 0.1981  data: 0.0002  max mem: 5511
[06:56:12.829150] Epoch: [14]  [540/781]  eta: 0:00:47  lr: 0.000244  training_loss: 2.3041 (2.2752)  mae_loss: 0.3488 (0.3490)  classification_loss: 1.7705 (1.8072)  loss_mask: 0.1570 (0.1190)  time: 0.1984  data: 0.0003  max mem: 5511
[06:56:16.772191] Epoch: [14]  [560/781]  eta: 0:00:43  lr: 0.000244  training_loss: 2.1736 (2.2718)  mae_loss: 0.3193 (0.3481)  classification_loss: 1.7631 (1.8053)  loss_mask: 0.0859 (0.1184)  time: 0.1971  data: 0.0003  max mem: 5511
[06:56:20.718562] Epoch: [14]  [580/781]  eta: 0:00:39  lr: 0.000244  training_loss: 2.2217 (2.2715)  mae_loss: 0.3410 (0.3479)  classification_loss: 1.7900 (1.8056)  loss_mask: 0.1012 (0.1180)  time: 0.1972  data: 0.0003  max mem: 5511
[06:56:24.656011] Epoch: [14]  [600/781]  eta: 0:00:35  lr: 0.000244  training_loss: 2.1863 (2.2689)  mae_loss: 0.3614 (0.3483)  classification_loss: 1.7297 (1.8033)  loss_mask: 0.0608 (0.1172)  time: 0.1968  data: 0.0002  max mem: 5511
[06:56:28.608354] Epoch: [14]  [620/781]  eta: 0:00:31  lr: 0.000244  training_loss: 2.1826 (2.2667)  mae_loss: 0.3465 (0.3483)  classification_loss: 1.7468 (1.8018)  loss_mask: 0.0954 (0.1165)  time: 0.1975  data: 0.0002  max mem: 5511
[06:56:32.555491] Epoch: [14]  [640/781]  eta: 0:00:27  lr: 0.000243  training_loss: 2.1588 (2.2635)  mae_loss: 0.3283 (0.3480)  classification_loss: 1.7201 (1.7997)  loss_mask: 0.0758 (0.1159)  time: 0.1973  data: 0.0002  max mem: 5511
[06:56:36.513154] Epoch: [14]  [660/781]  eta: 0:00:24  lr: 0.000243  training_loss: 2.2530 (2.2631)  mae_loss: 0.3287 (0.3476)  classification_loss: 1.8041 (1.8002)  loss_mask: 0.0917 (0.1153)  time: 0.1978  data: 0.0002  max mem: 5511
[06:56:40.453493] Epoch: [14]  [680/781]  eta: 0:00:20  lr: 0.000243  training_loss: 2.1865 (2.2611)  mae_loss: 0.3396 (0.3473)  classification_loss: 1.7881 (1.7997)  loss_mask: 0.0497 (0.1140)  time: 0.1969  data: 0.0002  max mem: 5511
[06:56:44.389006] Epoch: [14]  [700/781]  eta: 0:00:16  lr: 0.000243  training_loss: 2.1644 (2.2584)  mae_loss: 0.3314 (0.3470)  classification_loss: 1.7571 (1.7986)  loss_mask: 0.0700 (0.1129)  time: 0.1967  data: 0.0002  max mem: 5511
[06:56:48.326000] Epoch: [14]  [720/781]  eta: 0:00:12  lr: 0.000243  training_loss: 2.1593 (2.2568)  mae_loss: 0.3200 (0.3465)  classification_loss: 1.7873 (1.7990)  loss_mask: 0.0501 (0.1113)  time: 0.1968  data: 0.0005  max mem: 5511
[06:56:52.268268] Epoch: [14]  [740/781]  eta: 0:00:08  lr: 0.000243  training_loss: 2.1705 (2.2547)  mae_loss: 0.3053 (0.3460)  classification_loss: 1.7370 (1.7980)  loss_mask: 0.0785 (0.1106)  time: 0.1970  data: 0.0003  max mem: 5511
[06:56:56.214723] Epoch: [14]  [760/781]  eta: 0:00:04  lr: 0.000243  training_loss: 2.2507 (2.2544)  mae_loss: 0.3298 (0.3459)  classification_loss: 1.7843 (1.7974)  loss_mask: 0.0994 (0.1110)  time: 0.1972  data: 0.0003  max mem: 5511
[06:57:00.136515] Epoch: [14]  [780/781]  eta: 0:00:00  lr: 0.000243  training_loss: 2.1567 (2.2523)  mae_loss: 0.3357 (0.3457)  classification_loss: 1.7457 (1.7964)  loss_mask: 0.0739 (0.1102)  time: 0.1957  data: 0.0002  max mem: 5511
[06:57:00.303880] Epoch: [14] Total time: 0:02:35 (0.1986 s / it)
[06:57:00.304648] Averaged stats: lr: 0.000243  training_loss: 2.1567 (2.2523)  mae_loss: 0.3357 (0.3457)  classification_loss: 1.7457 (1.7964)  loss_mask: 0.0739 (0.1102)
[06:57:00.983024] Test:  [  0/157]  eta: 0:01:45  testing_loss: 1.0480 (1.0480)  acc1: 62.5000 (62.5000)  acc5: 96.8750 (96.8750)  time: 0.6737  data: 0.6422  max mem: 5511
[06:57:01.274384] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.1946 (1.1789)  acc1: 60.9375 (59.2330)  acc5: 96.8750 (97.0170)  time: 0.0875  data: 0.0586  max mem: 5511
[06:57:01.561220] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.1576 (1.1496)  acc1: 60.9375 (60.3423)  acc5: 96.8750 (96.9494)  time: 0.0287  data: 0.0002  max mem: 5511
[06:57:01.844609] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.1472 (1.1617)  acc1: 62.5000 (61.0383)  acc5: 95.3125 (96.3710)  time: 0.0284  data: 0.0002  max mem: 5511
[06:57:02.133902] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.1524 (1.1593)  acc1: 64.0625 (61.8521)  acc5: 95.3125 (96.3415)  time: 0.0285  data: 0.0002  max mem: 5511
[06:57:02.433134] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.1268 (1.1540)  acc1: 64.0625 (62.1630)  acc5: 96.8750 (96.4154)  time: 0.0293  data: 0.0002  max mem: 5511
[06:57:02.717540] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.1328 (1.1477)  acc1: 62.5000 (62.1926)  acc5: 96.8750 (96.3883)  time: 0.0290  data: 0.0002  max mem: 5511
[06:57:03.004056] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.1080 (1.1405)  acc1: 65.6250 (62.7641)  acc5: 96.8750 (96.3688)  time: 0.0284  data: 0.0002  max mem: 5511
[06:57:03.288179] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.1338 (1.1441)  acc1: 64.0625 (62.4807)  acc5: 95.3125 (96.2577)  time: 0.0284  data: 0.0002  max mem: 5511
[06:57:03.572868] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.1701 (1.1475)  acc1: 60.9375 (62.3283)  acc5: 95.3125 (96.3084)  time: 0.0283  data: 0.0002  max mem: 5511
[06:57:03.857632] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.1715 (1.1503)  acc1: 57.8125 (62.0668)  acc5: 96.8750 (96.3335)  time: 0.0283  data: 0.0002  max mem: 5511
[06:57:04.143850] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.1715 (1.1509)  acc1: 56.2500 (61.9088)  acc5: 96.8750 (96.3260)  time: 0.0284  data: 0.0002  max mem: 5511
[06:57:04.428704] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.1519 (1.1478)  acc1: 60.9375 (61.9835)  acc5: 96.8750 (96.3714)  time: 0.0284  data: 0.0002  max mem: 5511
[06:57:04.715377] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.1523 (1.1502)  acc1: 60.9375 (61.8559)  acc5: 95.3125 (96.3144)  time: 0.0284  data: 0.0002  max mem: 5511
[06:57:05.004456] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.1451 (1.1496)  acc1: 60.9375 (61.9348)  acc5: 95.3125 (96.2434)  time: 0.0286  data: 0.0002  max mem: 5511
[06:57:05.290108] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.1230 (1.1477)  acc1: 60.9375 (61.9309)  acc5: 95.3125 (96.2334)  time: 0.0285  data: 0.0001  max mem: 5511
[06:57:05.442294] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.1341 (1.1500)  acc1: 60.9375 (61.8300)  acc5: 96.8750 (96.2700)  time: 0.0274  data: 0.0001  max mem: 5511
[06:57:05.600348] Test: Total time: 0:00:05 (0.0337 s / it)
[06:57:05.600889] * Acc@1 61.830 Acc@5 96.270 loss 1.150
[06:57:05.601258] Accuracy of the network on the 10000 test images: 61.8%
[06:57:05.601506] Max accuracy: 61.83%
[06:57:05.767597] log_dir: ./output_dir
[06:57:06.639270] Epoch: [15]  [  0/781]  eta: 0:11:19  lr: 0.000243  training_loss: 2.1218 (2.1218)  mae_loss: 0.2862 (0.2862)  classification_loss: 1.8156 (1.8156)  loss_mask: 0.0200 (0.0200)  time: 0.8697  data: 0.6466  max mem: 5511
[06:57:10.608453] Epoch: [15]  [ 20/781]  eta: 0:02:55  lr: 0.000243  training_loss: 2.2174 (2.1838)  mae_loss: 0.3553 (0.3485)  classification_loss: 1.7336 (1.7474)  loss_mask: 0.0793 (0.0879)  time: 0.1984  data: 0.0002  max mem: 5511
[06:57:14.528575] Epoch: [15]  [ 40/781]  eta: 0:02:38  lr: 0.000243  training_loss: 2.1698 (2.1745)  mae_loss: 0.3385 (0.3487)  classification_loss: 1.7230 (1.7496)  loss_mask: 0.0566 (0.0763)  time: 0.1959  data: 0.0002  max mem: 5511
[06:57:18.482374] Epoch: [15]  [ 60/781]  eta: 0:02:30  lr: 0.000243  training_loss: 2.1990 (2.1839)  mae_loss: 0.3287 (0.3449)  classification_loss: 1.7594 (1.7605)  loss_mask: 0.0687 (0.0785)  time: 0.1976  data: 0.0004  max mem: 5511
[06:57:22.402848] Epoch: [15]  [ 80/781]  eta: 0:02:23  lr: 0.000243  training_loss: 2.1767 (2.1810)  mae_loss: 0.3380 (0.3441)  classification_loss: 1.7687 (1.7619)  loss_mask: 0.0552 (0.0750)  time: 0.1959  data: 0.0002  max mem: 5511
[06:57:26.344565] Epoch: [15]  [100/781]  eta: 0:02:18  lr: 0.000243  training_loss: 2.1737 (2.1894)  mae_loss: 0.3351 (0.3436)  classification_loss: 1.7570 (1.7623)  loss_mask: 0.0771 (0.0835)  time: 0.1970  data: 0.0002  max mem: 5511
[06:57:30.300051] Epoch: [15]  [120/781]  eta: 0:02:13  lr: 0.000243  training_loss: 2.2057 (2.1927)  mae_loss: 0.3086 (0.3394)  classification_loss: 1.7609 (1.7605)  loss_mask: 0.1280 (0.0929)  time: 0.1977  data: 0.0003  max mem: 5511
[06:57:34.261003] Epoch: [15]  [140/781]  eta: 0:02:09  lr: 0.000243  training_loss: 2.1578 (2.1859)  mae_loss: 0.3217 (0.3380)  classification_loss: 1.7504 (1.7586)  loss_mask: 0.0586 (0.0893)  time: 0.1980  data: 0.0003  max mem: 5511
[06:57:38.223226] Epoch: [15]  [160/781]  eta: 0:02:05  lr: 0.000243  training_loss: 2.2199 (2.1929)  mae_loss: 0.3439 (0.3393)  classification_loss: 1.7367 (1.7594)  loss_mask: 0.0921 (0.0942)  time: 0.1980  data: 0.0002  max mem: 5511
[06:57:42.169792] Epoch: [15]  [180/781]  eta: 0:02:00  lr: 0.000243  training_loss: 2.1692 (2.1915)  mae_loss: 0.3309 (0.3386)  classification_loss: 1.7258 (1.7594)  loss_mask: 0.0718 (0.0935)  time: 0.1972  data: 0.0002  max mem: 5511
[06:57:46.128073] Epoch: [15]  [200/781]  eta: 0:01:56  lr: 0.000243  training_loss: 2.1465 (2.1876)  mae_loss: 0.3305 (0.3384)  classification_loss: 1.7355 (1.7589)  loss_mask: 0.0650 (0.0903)  time: 0.1978  data: 0.0002  max mem: 5511
[06:57:50.083626] Epoch: [15]  [220/781]  eta: 0:01:52  lr: 0.000243  training_loss: 2.1110 (2.1843)  mae_loss: 0.3340 (0.3377)  classification_loss: 1.7643 (1.7590)  loss_mask: 0.0607 (0.0876)  time: 0.1977  data: 0.0002  max mem: 5511
[06:57:54.025454] Epoch: [15]  [240/781]  eta: 0:01:48  lr: 0.000243  training_loss: 2.2046 (2.1855)  mae_loss: 0.3309 (0.3382)  classification_loss: 1.7722 (1.7612)  loss_mask: 0.0688 (0.0862)  time: 0.1970  data: 0.0002  max mem: 5511
[06:57:57.977424] Epoch: [15]  [260/781]  eta: 0:01:44  lr: 0.000243  training_loss: 2.1709 (2.1848)  mae_loss: 0.3534 (0.3393)  classification_loss: 1.7092 (1.7582)  loss_mask: 0.0992 (0.0874)  time: 0.1975  data: 0.0003  max mem: 5511
[06:58:01.923057] Epoch: [15]  [280/781]  eta: 0:01:40  lr: 0.000243  training_loss: 2.1762 (2.1853)  mae_loss: 0.3428 (0.3399)  classification_loss: 1.7999 (1.7592)  loss_mask: 0.0741 (0.0862)  time: 0.1972  data: 0.0002  max mem: 5511
[06:58:05.889953] Epoch: [15]  [300/781]  eta: 0:01:36  lr: 0.000243  training_loss: 2.1880 (2.1862)  mae_loss: 0.3392 (0.3398)  classification_loss: 1.7411 (1.7585)  loss_mask: 0.1012 (0.0878)  time: 0.1982  data: 0.0003  max mem: 5511
[06:58:09.834158] Epoch: [15]  [320/781]  eta: 0:01:31  lr: 0.000243  training_loss: 2.1743 (2.1865)  mae_loss: 0.3335 (0.3392)  classification_loss: 1.7792 (1.7594)  loss_mask: 0.0733 (0.0880)  time: 0.1971  data: 0.0002  max mem: 5511
[06:58:13.792598] Epoch: [15]  [340/781]  eta: 0:01:27  lr: 0.000243  training_loss: 2.1863 (2.1875)  mae_loss: 0.3371 (0.3391)  classification_loss: 1.7337 (1.7588)  loss_mask: 0.1092 (0.0895)  time: 0.1978  data: 0.0002  max mem: 5511
[06:58:17.722874] Epoch: [15]  [360/781]  eta: 0:01:23  lr: 0.000243  training_loss: 2.1941 (2.1896)  mae_loss: 0.3363 (0.3390)  classification_loss: 1.7834 (1.7603)  loss_mask: 0.0782 (0.0902)  time: 0.1964  data: 0.0002  max mem: 5511
[06:58:21.651831] Epoch: [15]  [380/781]  eta: 0:01:19  lr: 0.000243  training_loss: 2.1790 (2.1890)  mae_loss: 0.3465 (0.3394)  classification_loss: 1.7487 (1.7605)  loss_mask: 0.0571 (0.0891)  time: 0.1964  data: 0.0002  max mem: 5511
[06:58:25.629054] Epoch: [15]  [400/781]  eta: 0:01:15  lr: 0.000243  training_loss: 2.1516 (2.1871)  mae_loss: 0.3280 (0.3387)  classification_loss: 1.7372 (1.7611)  loss_mask: 0.0507 (0.0874)  time: 0.1988  data: 0.0002  max mem: 5511
[06:58:29.577346] Epoch: [15]  [420/781]  eta: 0:01:11  lr: 0.000243  training_loss: 2.1143 (2.1858)  mae_loss: 0.3502 (0.3390)  classification_loss: 1.7239 (1.7602)  loss_mask: 0.0587 (0.0865)  time: 0.1973  data: 0.0002  max mem: 5511
[06:58:33.535405] Epoch: [15]  [440/781]  eta: 0:01:07  lr: 0.000242  training_loss: 2.1775 (2.1866)  mae_loss: 0.3413 (0.3393)  classification_loss: 1.7688 (1.7612)  loss_mask: 0.0822 (0.0861)  time: 0.1978  data: 0.0003  max mem: 5511
[06:58:37.457897] Epoch: [15]  [460/781]  eta: 0:01:03  lr: 0.000242  training_loss: 2.1134 (2.1846)  mae_loss: 0.3329 (0.3391)  classification_loss: 1.7045 (1.7598)  loss_mask: 0.0656 (0.0857)  time: 0.1960  data: 0.0002  max mem: 5511
[06:58:41.417528] Epoch: [15]  [480/781]  eta: 0:00:59  lr: 0.000242  training_loss: 2.2004 (2.1844)  mae_loss: 0.3320 (0.3389)  classification_loss: 1.7805 (1.7599)  loss_mask: 0.0685 (0.0856)  time: 0.1979  data: 0.0002  max mem: 5511
[06:58:45.366450] Epoch: [15]  [500/781]  eta: 0:00:55  lr: 0.000242  training_loss: 2.1932 (2.1853)  mae_loss: 0.3223 (0.3385)  classification_loss: 1.7661 (1.7601)  loss_mask: 0.0910 (0.0867)  time: 0.1973  data: 0.0003  max mem: 5511
[06:58:49.302862] Epoch: [15]  [520/781]  eta: 0:00:51  lr: 0.000242  training_loss: 2.1485 (2.1852)  mae_loss: 0.3327 (0.3383)  classification_loss: 1.7530 (1.7607)  loss_mask: 0.0592 (0.0861)  time: 0.1967  data: 0.0003  max mem: 5511
[06:58:53.249376] Epoch: [15]  [540/781]  eta: 0:00:47  lr: 0.000242  training_loss: 2.2242 (2.1873)  mae_loss: 0.3368 (0.3382)  classification_loss: 1.7935 (1.7620)  loss_mask: 0.0877 (0.0871)  time: 0.1973  data: 0.0002  max mem: 5511
[06:58:57.197066] Epoch: [15]  [560/781]  eta: 0:00:43  lr: 0.000242  training_loss: 2.1845 (2.1877)  mae_loss: 0.3476 (0.3383)  classification_loss: 1.7706 (1.7626)  loss_mask: 0.0660 (0.0868)  time: 0.1973  data: 0.0002  max mem: 5511
[06:59:01.145524] Epoch: [15]  [580/781]  eta: 0:00:39  lr: 0.000242  training_loss: 2.1775 (2.1877)  mae_loss: 0.3334 (0.3382)  classification_loss: 1.7421 (1.7617)  loss_mask: 0.1042 (0.0878)  time: 0.1973  data: 0.0002  max mem: 5511
[06:59:05.083929] Epoch: [15]  [600/781]  eta: 0:00:35  lr: 0.000242  training_loss: 2.1193 (2.1855)  mae_loss: 0.3159 (0.3379)  classification_loss: 1.7513 (1.7608)  loss_mask: 0.0496 (0.0868)  time: 0.1968  data: 0.0002  max mem: 5511
[06:59:09.024440] Epoch: [15]  [620/781]  eta: 0:00:31  lr: 0.000242  training_loss: 2.1281 (2.1837)  mae_loss: 0.3290 (0.3377)  classification_loss: 1.7342 (1.7600)  loss_mask: 0.0556 (0.0860)  time: 0.1969  data: 0.0002  max mem: 5511
[06:59:12.977192] Epoch: [15]  [640/781]  eta: 0:00:27  lr: 0.000242  training_loss: 2.2135 (2.1839)  mae_loss: 0.3355 (0.3378)  classification_loss: 1.7679 (1.7601)  loss_mask: 0.0731 (0.0860)  time: 0.1975  data: 0.0002  max mem: 5511
[06:59:16.920282] Epoch: [15]  [660/781]  eta: 0:00:23  lr: 0.000242  training_loss: 2.1387 (2.1832)  mae_loss: 0.3206 (0.3375)  classification_loss: 1.7612 (1.7601)  loss_mask: 0.0663 (0.0856)  time: 0.1971  data: 0.0002  max mem: 5511
[06:59:20.872632] Epoch: [15]  [680/781]  eta: 0:00:20  lr: 0.000242  training_loss: 2.1720 (2.1840)  mae_loss: 0.3201 (0.3373)  classification_loss: 1.7555 (1.7596)  loss_mask: 0.1384 (0.0872)  time: 0.1975  data: 0.0002  max mem: 5511
[06:59:24.815248] Epoch: [15]  [700/781]  eta: 0:00:16  lr: 0.000242  training_loss: 2.2003 (2.1842)  mae_loss: 0.3226 (0.3369)  classification_loss: 1.7293 (1.7588)  loss_mask: 0.1176 (0.0884)  time: 0.1970  data: 0.0002  max mem: 5511
[06:59:28.766694] Epoch: [15]  [720/781]  eta: 0:00:12  lr: 0.000242  training_loss: 2.1792 (2.1836)  mae_loss: 0.3286 (0.3368)  classification_loss: 1.7305 (1.7581)  loss_mask: 0.0900 (0.0887)  time: 0.1975  data: 0.0003  max mem: 5511
[06:59:32.730695] Epoch: [15]  [740/781]  eta: 0:00:08  lr: 0.000242  training_loss: 2.2019 (2.1826)  mae_loss: 0.3337 (0.3369)  classification_loss: 1.7192 (1.7576)  loss_mask: 0.0664 (0.0881)  time: 0.1981  data: 0.0002  max mem: 5511
[06:59:36.685378] Epoch: [15]  [760/781]  eta: 0:00:04  lr: 0.000242  training_loss: 2.1305 (2.1816)  mae_loss: 0.3188 (0.3365)  classification_loss: 1.7705 (1.7576)  loss_mask: 0.0595 (0.0876)  time: 0.1976  data: 0.0002  max mem: 5511
[06:59:40.603787] Epoch: [15]  [780/781]  eta: 0:00:00  lr: 0.000242  training_loss: 2.1273 (2.1803)  mae_loss: 0.3079 (0.3360)  classification_loss: 1.7219 (1.7575)  loss_mask: 0.0506 (0.0868)  time: 0.1958  data: 0.0002  max mem: 5511
[06:59:40.763234] Epoch: [15] Total time: 0:02:34 (0.1985 s / it)
[06:59:40.763701] Averaged stats: lr: 0.000242  training_loss: 2.1273 (2.1803)  mae_loss: 0.3079 (0.3360)  classification_loss: 1.7219 (1.7575)  loss_mask: 0.0506 (0.0868)
[06:59:41.336884] Test:  [  0/157]  eta: 0:01:29  testing_loss: 0.9551 (0.9551)  acc1: 71.8750 (71.8750)  acc5: 96.8750 (96.8750)  time: 0.5686  data: 0.5362  max mem: 5511
[06:59:41.624481] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 1.2039 (1.1714)  acc1: 59.3750 (58.3807)  acc5: 95.3125 (95.8807)  time: 0.0776  data: 0.0489  max mem: 5511
[06:59:41.908081] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.1537 (1.1307)  acc1: 59.3750 (61.0863)  acc5: 96.8750 (96.5774)  time: 0.0284  data: 0.0002  max mem: 5511
[06:59:42.193166] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 1.0999 (1.1337)  acc1: 60.9375 (60.7863)  acc5: 96.8750 (96.5222)  time: 0.0283  data: 0.0002  max mem: 5511
[06:59:42.479609] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 1.1105 (1.1321)  acc1: 64.0625 (61.7378)  acc5: 96.8750 (96.2271)  time: 0.0284  data: 0.0002  max mem: 5511
[06:59:42.763511] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.1105 (1.1293)  acc1: 62.5000 (61.9485)  acc5: 96.8750 (96.3235)  time: 0.0284  data: 0.0002  max mem: 5511
[06:59:43.047397] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.1175 (1.1261)  acc1: 62.5000 (62.1158)  acc5: 95.3125 (96.2090)  time: 0.0282  data: 0.0002  max mem: 5511
[06:59:43.331571] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.0871 (1.1178)  acc1: 62.5000 (62.2579)  acc5: 96.8750 (96.4129)  time: 0.0283  data: 0.0002  max mem: 5511
[06:59:43.614140] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.0974 (1.1185)  acc1: 60.9375 (62.3457)  acc5: 96.8750 (96.3156)  time: 0.0282  data: 0.0002  max mem: 5511
[06:59:43.896338] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.1139 (1.1199)  acc1: 60.9375 (62.4141)  acc5: 95.3125 (96.3599)  time: 0.0281  data: 0.0001  max mem: 5511
[06:59:44.178190] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.1597 (1.1238)  acc1: 59.3750 (62.0204)  acc5: 96.8750 (96.4573)  time: 0.0281  data: 0.0001  max mem: 5511
[06:59:44.465255] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.1620 (1.1251)  acc1: 59.3750 (62.0073)  acc5: 96.8750 (96.4386)  time: 0.0283  data: 0.0002  max mem: 5511
[06:59:44.753555] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.1031 (1.1215)  acc1: 62.5000 (62.2417)  acc5: 96.8750 (96.4101)  time: 0.0286  data: 0.0002  max mem: 5511
[06:59:45.036625] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.1160 (1.1235)  acc1: 62.5000 (62.0348)  acc5: 96.8750 (96.4695)  time: 0.0284  data: 0.0002  max mem: 5511
[06:59:45.319033] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.1160 (1.1223)  acc1: 60.9375 (62.1232)  acc5: 96.8750 (96.4428)  time: 0.0281  data: 0.0001  max mem: 5511
[06:59:45.601150] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.0872 (1.1197)  acc1: 62.5000 (62.1792)  acc5: 96.8750 (96.4921)  time: 0.0281  data: 0.0001  max mem: 5511
[06:59:45.755154] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.0872 (1.1220)  acc1: 60.9375 (62.0800)  acc5: 96.8750 (96.5400)  time: 0.0273  data: 0.0001  max mem: 5511
[06:59:45.923878] Test: Total time: 0:00:05 (0.0328 s / it)
[06:59:45.924594] * Acc@1 62.080 Acc@5 96.540 loss 1.122
[06:59:45.924910] Accuracy of the network on the 10000 test images: 62.1%
[06:59:45.925098] Max accuracy: 62.08%
[06:59:46.118254] log_dir: ./output_dir
[06:59:47.072456] Epoch: [16]  [  0/781]  eta: 0:12:23  lr: 0.000242  training_loss: 2.1838 (2.1838)  mae_loss: 0.3277 (0.3277)  classification_loss: 1.7771 (1.7771)  loss_mask: 0.0790 (0.0790)  time: 0.9524  data: 0.7305  max mem: 5511
[06:59:51.041654] Epoch: [16]  [ 20/781]  eta: 0:02:58  lr: 0.000242  training_loss: 2.1057 (2.1194)  mae_loss: 0.3306 (0.3288)  classification_loss: 1.7185 (1.7305)  loss_mask: 0.0539 (0.0600)  time: 0.1983  data: 0.0002  max mem: 5511
[06:59:55.031402] Epoch: [16]  [ 40/781]  eta: 0:02:40  lr: 0.000242  training_loss: 2.1264 (2.1230)  mae_loss: 0.3332 (0.3286)  classification_loss: 1.7431 (1.7331)  loss_mask: 0.0387 (0.0612)  time: 0.1994  data: 0.0002  max mem: 5511
[06:59:58.971655] Epoch: [16]  [ 60/781]  eta: 0:02:31  lr: 0.000242  training_loss: 2.1116 (2.1409)  mae_loss: 0.3136 (0.3298)  classification_loss: 1.7381 (1.7429)  loss_mask: 0.0501 (0.0682)  time: 0.1969  data: 0.0002  max mem: 5511
[07:00:02.934234] Epoch: [16]  [ 80/781]  eta: 0:02:25  lr: 0.000242  training_loss: 2.2025 (2.1522)  mae_loss: 0.3345 (0.3299)  classification_loss: 1.7440 (1.7425)  loss_mask: 0.1093 (0.0798)  time: 0.1980  data: 0.0002  max mem: 5511
[07:00:06.926362] Epoch: [16]  [100/781]  eta: 0:02:20  lr: 0.000242  training_loss: 2.1185 (2.1500)  mae_loss: 0.3129 (0.3267)  classification_loss: 1.7305 (1.7405)  loss_mask: 0.1013 (0.0828)  time: 0.1995  data: 0.0003  max mem: 5511
[07:00:10.897234] Epoch: [16]  [120/781]  eta: 0:02:15  lr: 0.000242  training_loss: 2.1927 (2.1564)  mae_loss: 0.3344 (0.3281)  classification_loss: 1.7451 (1.7410)  loss_mask: 0.0900 (0.0873)  time: 0.1984  data: 0.0002  max mem: 5511
[07:00:14.817865] Epoch: [16]  [140/781]  eta: 0:02:10  lr: 0.000242  training_loss: 2.1762 (2.1569)  mae_loss: 0.3535 (0.3319)  classification_loss: 1.7181 (1.7379)  loss_mask: 0.0797 (0.0872)  time: 0.1960  data: 0.0002  max mem: 5511
[07:00:18.771291] Epoch: [16]  [160/781]  eta: 0:02:05  lr: 0.000242  training_loss: 2.1497 (2.1569)  mae_loss: 0.3104 (0.3304)  classification_loss: 1.7761 (1.7417)  loss_mask: 0.0600 (0.0847)  time: 0.1976  data: 0.0002  max mem: 5511
[07:00:22.710469] Epoch: [16]  [180/781]  eta: 0:02:01  lr: 0.000242  training_loss: 2.1418 (2.1517)  mae_loss: 0.3298 (0.3308)  classification_loss: 1.7272 (1.7390)  loss_mask: 0.0508 (0.0818)  time: 0.1969  data: 0.0002  max mem: 5511
[07:00:26.655907] Epoch: [16]  [200/781]  eta: 0:01:57  lr: 0.000241  training_loss: 2.1234 (2.1501)  mae_loss: 0.3300 (0.3309)  classification_loss: 1.7329 (1.7407)  loss_mask: 0.0467 (0.0785)  time: 0.1972  data: 0.0002  max mem: 5511
[07:00:30.582095] Epoch: [16]  [220/781]  eta: 0:01:52  lr: 0.000241  training_loss: 2.1226 (2.1491)  mae_loss: 0.3316 (0.3308)  classification_loss: 1.7535 (1.7416)  loss_mask: 0.0490 (0.0767)  time: 0.1962  data: 0.0002  max mem: 5511
[07:00:34.542861] Epoch: [16]  [240/781]  eta: 0:01:48  lr: 0.000241  training_loss: 2.0977 (2.1460)  mae_loss: 0.3163 (0.3303)  classification_loss: 1.7067 (1.7393)  loss_mask: 0.0619 (0.0764)  time: 0.1980  data: 0.0003  max mem: 5511
[07:00:38.476473] Epoch: [16]  [260/781]  eta: 0:01:44  lr: 0.000241  training_loss: 2.2291 (2.1549)  mae_loss: 0.3116 (0.3293)  classification_loss: 1.6961 (1.7377)  loss_mask: 0.2463 (0.0880)  time: 0.1966  data: 0.0003  max mem: 5511
[07:00:42.452903] Epoch: [16]  [280/781]  eta: 0:01:40  lr: 0.000241  training_loss: 2.2156 (2.1599)  mae_loss: 0.3307 (0.3302)  classification_loss: 1.7416 (1.7381)  loss_mask: 0.1129 (0.0916)  time: 0.1987  data: 0.0002  max mem: 5511
[07:00:46.401552] Epoch: [16]  [300/781]  eta: 0:01:36  lr: 0.000241  training_loss: 2.1455 (2.1605)  mae_loss: 0.3220 (0.3299)  classification_loss: 1.7392 (1.7397)  loss_mask: 0.0706 (0.0909)  time: 0.1973  data: 0.0003  max mem: 5511
[07:00:50.343611] Epoch: [16]  [320/781]  eta: 0:01:32  lr: 0.000241  training_loss: 2.1720 (2.1595)  mae_loss: 0.3232 (0.3291)  classification_loss: 1.7740 (1.7411)  loss_mask: 0.0557 (0.0893)  time: 0.1970  data: 0.0004  max mem: 5511
[07:00:54.310612] Epoch: [16]  [340/781]  eta: 0:01:28  lr: 0.000241  training_loss: 2.0659 (2.1558)  mae_loss: 0.3406 (0.3298)  classification_loss: 1.6815 (1.7376)  loss_mask: 0.0531 (0.0884)  time: 0.1982  data: 0.0003  max mem: 5511
[07:00:58.273955] Epoch: [16]  [360/781]  eta: 0:01:24  lr: 0.000241  training_loss: 2.1209 (2.1558)  mae_loss: 0.3229 (0.3294)  classification_loss: 1.7116 (1.7370)  loss_mask: 0.0884 (0.0894)  time: 0.1981  data: 0.0002  max mem: 5511
[07:01:02.218083] Epoch: [16]  [380/781]  eta: 0:01:20  lr: 0.000241  training_loss: 2.1506 (2.1556)  mae_loss: 0.3530 (0.3303)  classification_loss: 1.7090 (1.7371)  loss_mask: 0.0505 (0.0882)  time: 0.1971  data: 0.0003  max mem: 5511
[07:01:06.163431] Epoch: [16]  [400/781]  eta: 0:01:16  lr: 0.000241  training_loss: 2.1267 (2.1544)  mae_loss: 0.3266 (0.3304)  classification_loss: 1.6894 (1.7360)  loss_mask: 0.0705 (0.0879)  time: 0.1972  data: 0.0002  max mem: 5511
[07:01:10.144162] Epoch: [16]  [420/781]  eta: 0:01:12  lr: 0.000241  training_loss: 2.1604 (2.1551)  mae_loss: 0.3166 (0.3303)  classification_loss: 1.7763 (1.7369)  loss_mask: 0.0753 (0.0878)  time: 0.1989  data: 0.0002  max mem: 5511
[07:01:14.097843] Epoch: [16]  [440/781]  eta: 0:01:07  lr: 0.000241  training_loss: 2.1406 (2.1550)  mae_loss: 0.3192 (0.3299)  classification_loss: 1.6986 (1.7360)  loss_mask: 0.0736 (0.0891)  time: 0.1976  data: 0.0002  max mem: 5511
[07:01:18.051123] Epoch: [16]  [460/781]  eta: 0:01:03  lr: 0.000241  training_loss: 2.1204 (2.1541)  mae_loss: 0.3194 (0.3293)  classification_loss: 1.6966 (1.7342)  loss_mask: 0.0880 (0.0905)  time: 0.1976  data: 0.0002  max mem: 5511
[07:01:22.016902] Epoch: [16]  [480/781]  eta: 0:00:59  lr: 0.000241  training_loss: 2.1701 (2.1540)  mae_loss: 0.3191 (0.3294)  classification_loss: 1.7394 (1.7346)  loss_mask: 0.0693 (0.0901)  time: 0.1982  data: 0.0002  max mem: 5511
[07:01:25.961220] Epoch: [16]  [500/781]  eta: 0:00:55  lr: 0.000241  training_loss: 2.1117 (2.1539)  mae_loss: 0.3248 (0.3299)  classification_loss: 1.7020 (1.7337)  loss_mask: 0.0719 (0.0903)  time: 0.1971  data: 0.0002  max mem: 5511
[07:01:29.925976] Epoch: [16]  [520/781]  eta: 0:00:51  lr: 0.000241  training_loss: 2.1250 (2.1532)  mae_loss: 0.3321 (0.3300)  classification_loss: 1.7211 (1.7336)  loss_mask: 0.0726 (0.0896)  time: 0.1982  data: 0.0003  max mem: 5511
[07:01:33.872705] Epoch: [16]  [540/781]  eta: 0:00:47  lr: 0.000241  training_loss: 2.1402 (2.1529)  mae_loss: 0.3244 (0.3301)  classification_loss: 1.7579 (1.7343)  loss_mask: 0.0601 (0.0885)  time: 0.1973  data: 0.0002  max mem: 5511
[07:01:37.812721] Epoch: [16]  [560/781]  eta: 0:00:43  lr: 0.000241  training_loss: 2.1471 (2.1539)  mae_loss: 0.3295 (0.3304)  classification_loss: 1.7404 (1.7354)  loss_mask: 0.0731 (0.0880)  time: 0.1969  data: 0.0002  max mem: 5511
[07:01:41.759115] Epoch: [16]  [580/781]  eta: 0:00:39  lr: 0.000241  training_loss: 2.2147 (2.1552)  mae_loss: 0.3150 (0.3301)  classification_loss: 1.7530 (1.7366)  loss_mask: 0.0970 (0.0886)  time: 0.1972  data: 0.0002  max mem: 5511
[07:01:45.701555] Epoch: [16]  [600/781]  eta: 0:00:35  lr: 0.000241  training_loss: 2.4774 (2.1659)  mae_loss: 0.3603 (0.3309)  classification_loss: 1.9232 (1.7427)  loss_mask: 0.1219 (0.0922)  time: 0.1970  data: 0.0002  max mem: 5511
[07:01:49.661767] Epoch: [16]  [620/781]  eta: 0:00:32  lr: 0.000241  training_loss: 3.0286 (2.1947)  mae_loss: 0.4010 (0.3332)  classification_loss: 1.9716 (1.7503)  loss_mask: 0.6327 (0.1112)  time: 0.1979  data: 0.0002  max mem: 5511
[07:01:53.607143] Epoch: [16]  [640/781]  eta: 0:00:28  lr: 0.000241  training_loss: 2.6853 (2.2157)  mae_loss: 0.3905 (0.3352)  classification_loss: 1.9575 (1.7571)  loss_mask: 0.3734 (0.1234)  time: 0.1972  data: 0.0002  max mem: 5511
[07:01:57.567791] Epoch: [16]  [660/781]  eta: 0:00:24  lr: 0.000241  training_loss: 2.4601 (2.2228)  mae_loss: 0.3661 (0.3361)  classification_loss: 1.9032 (1.7609)  loss_mask: 0.2017 (0.1258)  time: 0.1979  data: 0.0002  max mem: 5511
[07:02:01.535745] Epoch: [16]  [680/781]  eta: 0:00:20  lr: 0.000241  training_loss: 2.4656 (2.2337)  mae_loss: 0.3655 (0.3371)  classification_loss: 1.8820 (1.7652)  loss_mask: 0.2009 (0.1314)  time: 0.1983  data: 0.0002  max mem: 5511
[07:02:05.511044] Epoch: [16]  [700/781]  eta: 0:00:16  lr: 0.000240  training_loss: 2.6367 (2.2448)  mae_loss: 0.3683 (0.3381)  classification_loss: 1.9272 (1.7700)  loss_mask: 0.2977 (0.1367)  time: 0.1987  data: 0.0002  max mem: 5511
[07:02:09.501050] Epoch: [16]  [720/781]  eta: 0:00:12  lr: 0.000240  training_loss: 2.5346 (2.2528)  mae_loss: 0.3729 (0.3390)  classification_loss: 1.8932 (1.7736)  loss_mask: 0.2284 (0.1402)  time: 0.1994  data: 0.0002  max mem: 5511
[07:02:13.439201] Epoch: [16]  [740/781]  eta: 0:00:08  lr: 0.000240  training_loss: 2.2989 (2.2544)  mae_loss: 0.3405 (0.3392)  classification_loss: 1.8293 (1.7753)  loss_mask: 0.1098 (0.1400)  time: 0.1968  data: 0.0002  max mem: 5511
[07:02:17.386537] Epoch: [16]  [760/781]  eta: 0:00:04  lr: 0.000240  training_loss: 2.3664 (2.2574)  mae_loss: 0.3551 (0.3397)  classification_loss: 1.8400 (1.7770)  loss_mask: 0.1432 (0.1406)  time: 0.1973  data: 0.0002  max mem: 5511
[07:02:21.325120] Epoch: [16]  [780/781]  eta: 0:00:00  lr: 0.000240  training_loss: 2.3279 (2.2595)  mae_loss: 0.3345 (0.3397)  classification_loss: 1.7755 (1.7779)  loss_mask: 0.1670 (0.1419)  time: 0.1969  data: 0.0001  max mem: 5511
[07:02:21.504695] Epoch: [16] Total time: 0:02:35 (0.1990 s / it)
[07:02:21.505302] Averaged stats: lr: 0.000240  training_loss: 2.3279 (2.2595)  mae_loss: 0.3345 (0.3397)  classification_loss: 1.7755 (1.7779)  loss_mask: 0.1670 (0.1419)
[07:02:22.225547] Test:  [  0/157]  eta: 0:01:52  testing_loss: 1.0476 (1.0476)  acc1: 65.6250 (65.6250)  acc5: 92.1875 (92.1875)  time: 0.7163  data: 0.6867  max mem: 5511
[07:02:22.512560] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.1745 (1.1997)  acc1: 59.3750 (56.9602)  acc5: 95.3125 (95.0284)  time: 0.0910  data: 0.0626  max mem: 5511
[07:02:22.796270] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.1592 (1.1652)  acc1: 59.3750 (59.2262)  acc5: 95.3125 (95.9077)  time: 0.0283  data: 0.0002  max mem: 5511
[07:02:23.081888] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.1318 (1.1674)  acc1: 62.5000 (59.7278)  acc5: 96.8750 (95.9677)  time: 0.0283  data: 0.0002  max mem: 5511
[07:02:23.371206] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.1508 (1.1711)  acc1: 62.5000 (60.2134)  acc5: 95.3125 (95.7317)  time: 0.0286  data: 0.0003  max mem: 5511
[07:02:23.654412] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.1508 (1.1652)  acc1: 62.5000 (60.8762)  acc5: 95.3125 (95.6801)  time: 0.0285  data: 0.0003  max mem: 5511
[07:02:23.941059] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.1355 (1.1609)  acc1: 64.0625 (61.2961)  acc5: 95.3125 (95.6455)  time: 0.0284  data: 0.0002  max mem: 5511
[07:02:24.224234] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.1189 (1.1563)  acc1: 60.9375 (61.3116)  acc5: 96.8750 (95.7746)  time: 0.0284  data: 0.0002  max mem: 5511
[07:02:24.507627] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.1034 (1.1582)  acc1: 60.9375 (61.1304)  acc5: 96.8750 (95.7562)  time: 0.0282  data: 0.0002  max mem: 5511
[07:02:24.794104] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.1457 (1.1609)  acc1: 60.9375 (61.0062)  acc5: 96.8750 (95.9306)  time: 0.0284  data: 0.0002  max mem: 5511
[07:02:25.079381] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.1789 (1.1644)  acc1: 56.2500 (60.6281)  acc5: 96.8750 (95.9468)  time: 0.0285  data: 0.0002  max mem: 5511
[07:02:25.368092] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.2238 (1.1660)  acc1: 57.8125 (60.4589)  acc5: 95.3125 (95.9037)  time: 0.0286  data: 0.0002  max mem: 5511
[07:02:25.653906] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.1719 (1.1615)  acc1: 59.3750 (60.5888)  acc5: 96.8750 (95.9582)  time: 0.0286  data: 0.0002  max mem: 5511
[07:02:25.937202] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.1312 (1.1625)  acc1: 62.5000 (60.4962)  acc5: 96.8750 (95.9327)  time: 0.0283  data: 0.0002  max mem: 5511
[07:02:26.218944] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.1752 (1.1623)  acc1: 62.5000 (60.6937)  acc5: 95.3125 (95.9663)  time: 0.0281  data: 0.0002  max mem: 5511
[07:02:26.499802] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.1298 (1.1588)  acc1: 62.5000 (60.7512)  acc5: 95.3125 (95.9644)  time: 0.0280  data: 0.0001  max mem: 5511
[07:02:26.656205] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.1192 (1.1604)  acc1: 59.3750 (60.5700)  acc5: 96.8750 (95.9800)  time: 0.0274  data: 0.0001  max mem: 5511
[07:02:26.829370] Test: Total time: 0:00:05 (0.0339 s / it)
[07:02:26.829825] * Acc@1 60.570 Acc@5 95.980 loss 1.160
[07:02:26.830168] Accuracy of the network on the 10000 test images: 60.6%
[07:02:26.830373] Max accuracy: 62.08%
[07:02:26.950594] log_dir: ./output_dir
[07:02:27.916741] Epoch: [17]  [  0/781]  eta: 0:12:33  lr: 0.000240  training_loss: 2.0564 (2.0564)  mae_loss: 0.2753 (0.2753)  classification_loss: 1.6284 (1.6284)  loss_mask: 0.1527 (0.1527)  time: 0.9644  data: 0.7503  max mem: 5511
[07:02:31.866965] Epoch: [17]  [ 20/781]  eta: 0:02:58  lr: 0.000240  training_loss: 2.4514 (2.4589)  mae_loss: 0.3464 (0.3384)  classification_loss: 1.8247 (1.8158)  loss_mask: 0.2790 (0.3047)  time: 0.1974  data: 0.0002  max mem: 5511
[07:02:35.832913] Epoch: [17]  [ 40/781]  eta: 0:02:40  lr: 0.000240  training_loss: 2.3954 (2.4441)  mae_loss: 0.3523 (0.3488)  classification_loss: 1.8873 (1.8508)  loss_mask: 0.1710 (0.2445)  time: 0.1982  data: 0.0003  max mem: 5511
[07:02:39.811801] Epoch: [17]  [ 60/781]  eta: 0:02:31  lr: 0.000240  training_loss: 2.3897 (2.4338)  mae_loss: 0.3315 (0.3499)  classification_loss: 1.9157 (1.8680)  loss_mask: 0.1434 (0.2159)  time: 0.1988  data: 0.0002  max mem: 5511
[07:02:43.789056] Epoch: [17]  [ 80/781]  eta: 0:02:25  lr: 0.000240  training_loss: 2.3437 (2.4131)  mae_loss: 0.3222 (0.3460)  classification_loss: 1.8755 (1.8727)  loss_mask: 0.1102 (0.1945)  time: 0.1988  data: 0.0004  max mem: 5511
[07:02:47.752746] Epoch: [17]  [100/781]  eta: 0:02:20  lr: 0.000240  training_loss: 2.2765 (2.3839)  mae_loss: 0.3276 (0.3429)  classification_loss: 1.8375 (1.8627)  loss_mask: 0.0979 (0.1783)  time: 0.1980  data: 0.0002  max mem: 5511
[07:02:51.717184] Epoch: [17]  [120/781]  eta: 0:02:15  lr: 0.000240  training_loss: 2.2322 (2.3575)  mae_loss: 0.3228 (0.3415)  classification_loss: 1.7695 (1.8505)  loss_mask: 0.0952 (0.1655)  time: 0.1981  data: 0.0002  max mem: 5511
[07:02:55.646135] Epoch: [17]  [140/781]  eta: 0:02:10  lr: 0.000240  training_loss: 2.3541 (2.3516)  mae_loss: 0.3283 (0.3396)  classification_loss: 1.8538 (1.8469)  loss_mask: 0.1427 (0.1651)  time: 0.1964  data: 0.0002  max mem: 5511
[07:02:59.638856] Epoch: [17]  [160/781]  eta: 0:02:06  lr: 0.000240  training_loss: 2.3034 (2.3460)  mae_loss: 0.3299 (0.3392)  classification_loss: 1.8393 (1.8455)  loss_mask: 0.1202 (0.1614)  time: 0.1996  data: 0.0002  max mem: 5511
[07:03:03.609608] Epoch: [17]  [180/781]  eta: 0:02:01  lr: 0.000240  training_loss: 2.1789 (2.3297)  mae_loss: 0.3302 (0.3387)  classification_loss: 1.7250 (1.8346)  loss_mask: 0.1093 (0.1563)  time: 0.1984  data: 0.0002  max mem: 5511
[07:03:07.581638] Epoch: [17]  [200/781]  eta: 0:01:57  lr: 0.000240  training_loss: 2.2077 (2.3185)  mae_loss: 0.3357 (0.3380)  classification_loss: 1.7706 (1.8289)  loss_mask: 0.0863 (0.1516)  time: 0.1985  data: 0.0003  max mem: 5511
[07:03:11.550808] Epoch: [17]  [220/781]  eta: 0:01:53  lr: 0.000240  training_loss: 2.2720 (2.3125)  mae_loss: 0.3336 (0.3375)  classification_loss: 1.8001 (1.8273)  loss_mask: 0.1015 (0.1477)  time: 0.1984  data: 0.0002  max mem: 5511
[07:03:15.492938] Epoch: [17]  [240/781]  eta: 0:01:48  lr: 0.000240  training_loss: 2.1979 (2.3062)  mae_loss: 0.3477 (0.3389)  classification_loss: 1.7879 (1.8249)  loss_mask: 0.0697 (0.1425)  time: 0.1970  data: 0.0002  max mem: 5511
[07:03:19.443077] Epoch: [17]  [260/781]  eta: 0:01:44  lr: 0.000240  training_loss: 2.1503 (2.2944)  mae_loss: 0.3289 (0.3385)  classification_loss: 1.7207 (1.8171)  loss_mask: 0.0880 (0.1387)  time: 0.1974  data: 0.0002  max mem: 5511
[07:03:23.376033] Epoch: [17]  [280/781]  eta: 0:01:40  lr: 0.000240  training_loss: 2.1536 (2.2871)  mae_loss: 0.3338 (0.3381)  classification_loss: 1.7606 (1.8140)  loss_mask: 0.0797 (0.1349)  time: 0.1966  data: 0.0002  max mem: 5511
[07:03:27.349791] Epoch: [17]  [300/781]  eta: 0:01:36  lr: 0.000240  training_loss: 2.1653 (2.2795)  mae_loss: 0.3058 (0.3362)  classification_loss: 1.7721 (1.8115)  loss_mask: 0.0611 (0.1318)  time: 0.1985  data: 0.0005  max mem: 5511
[07:03:31.286817] Epoch: [17]  [320/781]  eta: 0:01:32  lr: 0.000240  training_loss: 2.1914 (2.2756)  mae_loss: 0.3163 (0.3355)  classification_loss: 1.7321 (1.8079)  loss_mask: 0.0941 (0.1321)  time: 0.1968  data: 0.0002  max mem: 5511
[07:03:35.232212] Epoch: [17]  [340/781]  eta: 0:01:28  lr: 0.000240  training_loss: 2.1649 (2.2702)  mae_loss: 0.3280 (0.3351)  classification_loss: 1.7480 (1.8042)  loss_mask: 0.0897 (0.1309)  time: 0.1972  data: 0.0002  max mem: 5511
[07:03:39.215527] Epoch: [17]  [360/781]  eta: 0:01:24  lr: 0.000240  training_loss: 2.1878 (2.2652)  mae_loss: 0.3158 (0.3342)  classification_loss: 1.7447 (1.8018)  loss_mask: 0.0840 (0.1292)  time: 0.1991  data: 0.0002  max mem: 5511
[07:03:43.185104] Epoch: [17]  [380/781]  eta: 0:01:20  lr: 0.000240  training_loss: 2.1661 (2.2601)  mae_loss: 0.3278 (0.3340)  classification_loss: 1.7630 (1.8000)  loss_mask: 0.0579 (0.1261)  time: 0.1984  data: 0.0003  max mem: 5511
[07:03:47.150692] Epoch: [17]  [400/781]  eta: 0:01:16  lr: 0.000239  training_loss: 2.0959 (2.2529)  mae_loss: 0.3245 (0.3332)  classification_loss: 1.7193 (1.7960)  loss_mask: 0.0692 (0.1236)  time: 0.1982  data: 0.0002  max mem: 5511
[07:03:51.096977] Epoch: [17]  [420/781]  eta: 0:01:12  lr: 0.000239  training_loss: 2.1428 (2.2480)  mae_loss: 0.3373 (0.3331)  classification_loss: 1.7459 (1.7934)  loss_mask: 0.0528 (0.1215)  time: 0.1972  data: 0.0003  max mem: 5511
[07:03:55.031682] Epoch: [17]  [440/781]  eta: 0:01:08  lr: 0.000239  training_loss: 2.1875 (2.2450)  mae_loss: 0.3086 (0.3325)  classification_loss: 1.7711 (1.7911)  loss_mask: 0.0890 (0.1214)  time: 0.1966  data: 0.0002  max mem: 5511
[07:03:58.975201] Epoch: [17]  [460/781]  eta: 0:01:04  lr: 0.000239  training_loss: 2.2218 (2.2450)  mae_loss: 0.3147 (0.3323)  classification_loss: 1.7190 (1.7887)  loss_mask: 0.1190 (0.1241)  time: 0.1971  data: 0.0002  max mem: 5511
[07:04:02.953370] Epoch: [17]  [480/781]  eta: 0:01:00  lr: 0.000239  training_loss: 2.2790 (2.2464)  mae_loss: 0.3062 (0.3317)  classification_loss: 1.7147 (1.7864)  loss_mask: 0.1722 (0.1283)  time: 0.1988  data: 0.0003  max mem: 5511
[07:04:06.905719] Epoch: [17]  [500/781]  eta: 0:00:56  lr: 0.000239  training_loss: 2.1567 (2.2438)  mae_loss: 0.3246 (0.3316)  classification_loss: 1.6986 (1.7842)  loss_mask: 0.1238 (0.1281)  time: 0.1975  data: 0.0003  max mem: 5511
[07:04:10.856122] Epoch: [17]  [520/781]  eta: 0:00:52  lr: 0.000239  training_loss: 2.0968 (2.2390)  mae_loss: 0.3206 (0.3313)  classification_loss: 1.7229 (1.7815)  loss_mask: 0.0764 (0.1262)  time: 0.1974  data: 0.0002  max mem: 5511
[07:04:14.806075] Epoch: [17]  [540/781]  eta: 0:00:48  lr: 0.000239  training_loss: 2.1549 (2.2369)  mae_loss: 0.3310 (0.3309)  classification_loss: 1.7079 (1.7805)  loss_mask: 0.0885 (0.1254)  time: 0.1974  data: 0.0002  max mem: 5511
[07:04:18.772421] Epoch: [17]  [560/781]  eta: 0:00:44  lr: 0.000239  training_loss: 2.1914 (2.2365)  mae_loss: 0.3250 (0.3309)  classification_loss: 1.7230 (1.7795)  loss_mask: 0.1310 (0.1261)  time: 0.1982  data: 0.0003  max mem: 5511
[07:04:22.717961] Epoch: [17]  [580/781]  eta: 0:00:40  lr: 0.000239  training_loss: 2.1909 (2.2352)  mae_loss: 0.3077 (0.3304)  classification_loss: 1.7449 (1.7792)  loss_mask: 0.0940 (0.1256)  time: 0.1972  data: 0.0003  max mem: 5511
[07:04:26.639765] Epoch: [17]  [600/781]  eta: 0:00:36  lr: 0.000239  training_loss: 2.1411 (2.2319)  mae_loss: 0.3311 (0.3305)  classification_loss: 1.7388 (1.7773)  loss_mask: 0.0727 (0.1241)  time: 0.1960  data: 0.0003  max mem: 5511
[07:04:30.618648] Epoch: [17]  [620/781]  eta: 0:00:32  lr: 0.000239  training_loss: 2.1251 (2.2285)  mae_loss: 0.3235 (0.3303)  classification_loss: 1.7117 (1.7757)  loss_mask: 0.0657 (0.1224)  time: 0.1989  data: 0.0002  max mem: 5511
[07:04:34.547291] Epoch: [17]  [640/781]  eta: 0:00:28  lr: 0.000239  training_loss: 2.1809 (2.2279)  mae_loss: 0.3226 (0.3303)  classification_loss: 1.7817 (1.7759)  loss_mask: 0.0889 (0.1217)  time: 0.1963  data: 0.0003  max mem: 5511
[07:04:38.502645] Epoch: [17]  [660/781]  eta: 0:00:24  lr: 0.000239  training_loss: 2.1982 (2.2269)  mae_loss: 0.3353 (0.3306)  classification_loss: 1.7793 (1.7757)  loss_mask: 0.0762 (0.1206)  time: 0.1976  data: 0.0002  max mem: 5511
[07:04:42.487423] Epoch: [17]  [680/781]  eta: 0:00:20  lr: 0.000239  training_loss: 2.1645 (2.2252)  mae_loss: 0.3289 (0.3306)  classification_loss: 1.7435 (1.7745)  loss_mask: 0.1090 (0.1201)  time: 0.1991  data: 0.0003  max mem: 5511
[07:04:46.444671] Epoch: [17]  [700/781]  eta: 0:00:16  lr: 0.000239  training_loss: 2.1187 (2.2227)  mae_loss: 0.3153 (0.3305)  classification_loss: 1.7050 (1.7730)  loss_mask: 0.0840 (0.1192)  time: 0.1978  data: 0.0002  max mem: 5511
[07:04:50.385627] Epoch: [17]  [720/781]  eta: 0:00:12  lr: 0.000239  training_loss: 2.0927 (2.2198)  mae_loss: 0.3186 (0.3302)  classification_loss: 1.6920 (1.7715)  loss_mask: 0.0732 (0.1182)  time: 0.1970  data: 0.0002  max mem: 5511
[07:04:54.337550] Epoch: [17]  [740/781]  eta: 0:00:08  lr: 0.000239  training_loss: 2.0517 (2.2171)  mae_loss: 0.3204 (0.3298)  classification_loss: 1.6694 (1.7693)  loss_mask: 0.1030 (0.1180)  time: 0.1975  data: 0.0003  max mem: 5511
[07:04:58.275557] Epoch: [17]  [760/781]  eta: 0:00:04  lr: 0.000239  training_loss: 2.1414 (2.2153)  mae_loss: 0.3201 (0.3297)  classification_loss: 1.7141 (1.7683)  loss_mask: 0.0956 (0.1173)  time: 0.1968  data: 0.0002  max mem: 5511
[07:05:02.214514] Epoch: [17]  [780/781]  eta: 0:00:00  lr: 0.000239  training_loss: 2.0951 (2.2129)  mae_loss: 0.3156 (0.3294)  classification_loss: 1.7144 (1.7672)  loss_mask: 0.0690 (0.1163)  time: 0.1968  data: 0.0002  max mem: 5511
[07:05:02.392023] Epoch: [17] Total time: 0:02:35 (0.1990 s / it)
[07:05:02.392506] Averaged stats: lr: 0.000239  training_loss: 2.0951 (2.2129)  mae_loss: 0.3156 (0.3294)  classification_loss: 1.7144 (1.7672)  loss_mask: 0.0690 (0.1163)
[07:05:02.987544] Test:  [  0/157]  eta: 0:01:32  testing_loss: 0.9537 (0.9537)  acc1: 70.3125 (70.3125)  acc5: 93.7500 (93.7500)  time: 0.5890  data: 0.5460  max mem: 5511
[07:05:03.276716] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 1.1075 (1.1006)  acc1: 62.5000 (61.5057)  acc5: 96.8750 (96.5909)  time: 0.0797  data: 0.0498  max mem: 5511
[07:05:03.559993] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.0810 (1.0634)  acc1: 62.5000 (62.2768)  acc5: 96.8750 (96.9494)  time: 0.0285  data: 0.0001  max mem: 5511
[07:05:03.843876] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 1.0430 (1.0737)  acc1: 62.5000 (62.3992)  acc5: 96.8750 (96.7238)  time: 0.0282  data: 0.0002  max mem: 5511
[07:05:04.129725] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 1.0508 (1.0728)  acc1: 65.6250 (63.1860)  acc5: 96.8750 (96.9512)  time: 0.0283  data: 0.0002  max mem: 5511
[07:05:04.414428] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.0508 (1.0675)  acc1: 65.6250 (63.7868)  acc5: 96.8750 (96.9669)  time: 0.0283  data: 0.0002  max mem: 5511
[07:05:04.704138] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.0595 (1.0648)  acc1: 65.6250 (63.8064)  acc5: 96.8750 (96.9006)  time: 0.0285  data: 0.0002  max mem: 5511
[07:05:04.988884] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.9981 (1.0551)  acc1: 65.6250 (64.1285)  acc5: 96.8750 (96.9410)  time: 0.0286  data: 0.0002  max mem: 5511
[07:05:05.272094] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.0002 (1.0598)  acc1: 65.6250 (64.1782)  acc5: 98.4375 (96.9715)  time: 0.0283  data: 0.0002  max mem: 5511
[07:05:05.555136] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.0807 (1.0623)  acc1: 65.6250 (64.2857)  acc5: 96.8750 (96.9437)  time: 0.0282  data: 0.0002  max mem: 5511
[07:05:05.838631] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.0938 (1.0666)  acc1: 62.5000 (64.1089)  acc5: 96.8750 (96.9988)  time: 0.0282  data: 0.0002  max mem: 5511
[07:05:06.129085] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.1049 (1.0683)  acc1: 60.9375 (63.8654)  acc5: 96.8750 (96.9454)  time: 0.0286  data: 0.0004  max mem: 5511
[07:05:06.413697] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.0621 (1.0651)  acc1: 64.0625 (63.9334)  acc5: 96.8750 (96.9912)  time: 0.0286  data: 0.0004  max mem: 5511
[07:05:06.699307] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.0624 (1.0677)  acc1: 64.0625 (63.7405)  acc5: 96.8750 (97.0062)  time: 0.0283  data: 0.0002  max mem: 5511
[07:05:06.983484] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.0643 (1.0670)  acc1: 64.0625 (63.9738)  acc5: 96.8750 (97.0301)  time: 0.0284  data: 0.0002  max mem: 5511
[07:05:07.267923] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.0253 (1.0636)  acc1: 65.6250 (64.1556)  acc5: 96.8750 (97.0406)  time: 0.0283  data: 0.0002  max mem: 5511
[07:05:07.419958] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.0123 (1.0675)  acc1: 65.6250 (64.0600)  acc5: 96.8750 (97.0600)  time: 0.0273  data: 0.0001  max mem: 5511
[07:05:07.565724] Test: Total time: 0:00:05 (0.0329 s / it)
[07:05:07.566171] * Acc@1 64.060 Acc@5 97.060 loss 1.068
[07:05:07.566455] Accuracy of the network on the 10000 test images: 64.1%
[07:05:07.566656] Max accuracy: 64.06%
[07:05:07.754718] log_dir: ./output_dir
[07:05:08.513871] Epoch: [18]  [  0/781]  eta: 0:09:50  lr: 0.000239  training_loss: 1.8856 (1.8856)  mae_loss: 0.2659 (0.2659)  classification_loss: 1.6009 (1.6009)  loss_mask: 0.0187 (0.0187)  time: 0.7567  data: 0.5471  max mem: 5511
[07:05:12.474393] Epoch: [18]  [ 20/781]  eta: 0:02:50  lr: 0.000239  training_loss: 2.0139 (2.0390)  mae_loss: 0.3141 (0.3179)  classification_loss: 1.6212 (1.6638)  loss_mask: 0.0564 (0.0573)  time: 0.1979  data: 0.0002  max mem: 5511
[07:05:16.473798] Epoch: [18]  [ 40/781]  eta: 0:02:37  lr: 0.000239  training_loss: 2.1424 (2.0903)  mae_loss: 0.3329 (0.3252)  classification_loss: 1.7595 (1.7007)  loss_mask: 0.0619 (0.0643)  time: 0.1999  data: 0.0003  max mem: 5511
[07:05:20.434902] Epoch: [18]  [ 60/781]  eta: 0:02:29  lr: 0.000239  training_loss: 2.0858 (2.0947)  mae_loss: 0.3289 (0.3279)  classification_loss: 1.7073 (1.7029)  loss_mask: 0.0578 (0.0638)  time: 0.1979  data: 0.0002  max mem: 5511
[07:05:24.393570] Epoch: [18]  [ 80/781]  eta: 0:02:23  lr: 0.000238  training_loss: 2.1749 (2.1183)  mae_loss: 0.3242 (0.3293)  classification_loss: 1.7473 (1.7133)  loss_mask: 0.0758 (0.0758)  time: 0.1979  data: 0.0002  max mem: 5511
[07:05:28.353136] Epoch: [18]  [100/781]  eta: 0:02:18  lr: 0.000238  training_loss: 2.2472 (2.1520)  mae_loss: 0.3352 (0.3327)  classification_loss: 1.7355 (1.7228)  loss_mask: 0.1978 (0.0965)  time: 0.1979  data: 0.0002  max mem: 5511
[07:05:32.310526] Epoch: [18]  [120/781]  eta: 0:02:14  lr: 0.000238  training_loss: 2.1447 (2.1537)  mae_loss: 0.3302 (0.3315)  classification_loss: 1.7298 (1.7267)  loss_mask: 0.0778 (0.0955)  time: 0.1978  data: 0.0003  max mem: 5511
[07:05:36.266193] Epoch: [18]  [140/781]  eta: 0:02:09  lr: 0.000238  training_loss: 2.2434 (2.1687)  mae_loss: 0.3302 (0.3317)  classification_loss: 1.7322 (1.7293)  loss_mask: 0.1462 (0.1076)  time: 0.1977  data: 0.0002  max mem: 5511
[07:05:40.217638] Epoch: [18]  [160/781]  eta: 0:02:05  lr: 0.000238  training_loss: 2.1595 (2.1706)  mae_loss: 0.3199 (0.3316)  classification_loss: 1.7380 (1.7329)  loss_mask: 0.0814 (0.1061)  time: 0.1975  data: 0.0002  max mem: 5511
[07:05:44.148331] Epoch: [18]  [180/781]  eta: 0:02:00  lr: 0.000238  training_loss: 2.1797 (2.1731)  mae_loss: 0.3303 (0.3317)  classification_loss: 1.7278 (1.7346)  loss_mask: 0.1055 (0.1067)  time: 0.1965  data: 0.0002  max mem: 5511
[07:05:48.127999] Epoch: [18]  [200/781]  eta: 0:01:56  lr: 0.000238  training_loss: 2.2614 (2.1826)  mae_loss: 0.3425 (0.3323)  classification_loss: 1.7461 (1.7363)  loss_mask: 0.1483 (0.1141)  time: 0.1989  data: 0.0002  max mem: 5511
[07:05:52.094541] Epoch: [18]  [220/781]  eta: 0:01:52  lr: 0.000238  training_loss: 2.2494 (2.1890)  mae_loss: 0.3113 (0.3310)  classification_loss: 1.7542 (1.7384)  loss_mask: 0.1429 (0.1196)  time: 0.1982  data: 0.0002  max mem: 5511
[07:05:56.051990] Epoch: [18]  [240/781]  eta: 0:01:48  lr: 0.000238  training_loss: 2.1896 (2.1886)  mae_loss: 0.3135 (0.3304)  classification_loss: 1.7425 (1.7392)  loss_mask: 0.1027 (0.1190)  time: 0.1978  data: 0.0002  max mem: 5511
[07:06:00.021335] Epoch: [18]  [260/781]  eta: 0:01:44  lr: 0.000238  training_loss: 2.1389 (2.1841)  mae_loss: 0.3093 (0.3291)  classification_loss: 1.7072 (1.7370)  loss_mask: 0.0929 (0.1180)  time: 0.1984  data: 0.0002  max mem: 5511
[07:06:03.986544] Epoch: [18]  [280/781]  eta: 0:01:40  lr: 0.000238  training_loss: 2.1497 (2.1843)  mae_loss: 0.3258 (0.3291)  classification_loss: 1.7174 (1.7375)  loss_mask: 0.0992 (0.1177)  time: 0.1982  data: 0.0002  max mem: 5511
[07:06:07.945186] Epoch: [18]  [300/781]  eta: 0:01:36  lr: 0.000238  training_loss: 2.1342 (2.1817)  mae_loss: 0.3241 (0.3289)  classification_loss: 1.7152 (1.7375)  loss_mask: 0.0733 (0.1154)  time: 0.1978  data: 0.0004  max mem: 5511
[07:06:11.890908] Epoch: [18]  [320/781]  eta: 0:01:32  lr: 0.000238  training_loss: 2.1444 (2.1800)  mae_loss: 0.3262 (0.3294)  classification_loss: 1.7233 (1.7370)  loss_mask: 0.0711 (0.1136)  time: 0.1972  data: 0.0002  max mem: 5511
[07:06:15.834162] Epoch: [18]  [340/781]  eta: 0:01:27  lr: 0.000238  training_loss: 2.0924 (2.1743)  mae_loss: 0.3360 (0.3295)  classification_loss: 1.6643 (1.7330)  loss_mask: 0.0756 (0.1118)  time: 0.1970  data: 0.0002  max mem: 5511

[07:06:19.804763] Epoch: [18]  [360/781]  eta: 0:01:23  lr: 0.000238  training_loss: 2.1110 (2.1718)  mae_loss: 0.3227 (0.3293)  classification_loss: 1.6987 (1.7317)  loss_mask: 0.0730 (0.1108)  time: 0.1984  data: 0.0002  max mem: 5511
[07:06:23.741644] Epoch: [18]  [380/781]  eta: 0:01:19  lr: 0.000238  training_loss: 2.0989 (2.1696)  mae_loss: 0.3205 (0.3294)  classification_loss: 1.6813 (1.7319)  loss_mask: 0.0517 (0.1083)  time: 0.1968  data: 0.0002  max mem: 5511
[07:06:27.689188] Epoch: [18]  [400/781]  eta: 0:01:15  lr: 0.000238  training_loss: 2.0590 (2.1652)  mae_loss: 0.3245 (0.3292)  classification_loss: 1.7088 (1.7304)  loss_mask: 0.0503 (0.1057)  time: 0.1973  data: 0.0002  max mem: 5511
[07:06:31.654945] Epoch: [18]  [420/781]  eta: 0:01:11  lr: 0.000238  training_loss: 2.0870 (2.1598)  mae_loss: 0.3143 (0.3286)  classification_loss: 1.7061 (1.7280)  loss_mask: 0.0475 (0.1032)  time: 0.1982  data: 0.0002  max mem: 5511
[07:06:35.604466] Epoch: [18]  [440/781]  eta: 0:01:07  lr: 0.000238  training_loss: 2.0865 (2.1569)  mae_loss: 0.3267 (0.3285)  classification_loss: 1.6903 (1.7264)  loss_mask: 0.0562 (0.1020)  time: 0.1974  data: 0.0002  max mem: 5511
[07:06:39.549929] Epoch: [18]  [460/781]  eta: 0:01:03  lr: 0.000238  training_loss: 2.0359 (2.1531)  mae_loss: 0.2982 (0.3278)  classification_loss: 1.6430 (1.7241)  loss_mask: 0.0734 (0.1012)  time: 0.1972  data: 0.0002  max mem: 5511
[07:06:43.494511] Epoch: [18]  [480/781]  eta: 0:00:59  lr: 0.000238  training_loss: 2.0943 (2.1527)  mae_loss: 0.3205 (0.3277)  classification_loss: 1.6927 (1.7235)  loss_mask: 0.1020 (0.1015)  time: 0.1971  data: 0.0002  max mem: 5511
[07:06:47.441871] Epoch: [18]  [500/781]  eta: 0:00:55  lr: 0.000238  training_loss: 2.1807 (2.1550)  mae_loss: 0.3193 (0.3277)  classification_loss: 1.7393 (1.7239)  loss_mask: 0.0761 (0.1034)  time: 0.1973  data: 0.0002  max mem: 5511
[07:06:51.402094] Epoch: [18]  [520/781]  eta: 0:00:51  lr: 0.000238  training_loss: 2.1255 (2.1547)  mae_loss: 0.3280 (0.3278)  classification_loss: 1.7162 (1.7237)  loss_mask: 0.0846 (0.1032)  time: 0.1979  data: 0.0002  max mem: 5511
[07:06:55.341879] Epoch: [18]  [540/781]  eta: 0:00:47  lr: 0.000237  training_loss: 2.1368 (2.1541)  mae_loss: 0.3137 (0.3277)  classification_loss: 1.7022 (1.7232)  loss_mask: 0.0724 (0.1033)  time: 0.1969  data: 0.0002  max mem: 5511
[07:06:59.284081] Epoch: [18]  [560/781]  eta: 0:00:43  lr: 0.000237  training_loss: 2.2138 (2.1569)  mae_loss: 0.3299 (0.3280)  classification_loss: 1.7637 (1.7249)  loss_mask: 0.1044 (0.1040)  time: 0.1970  data: 0.0003  max mem: 5511
[07:07:03.224502] Epoch: [18]  [580/781]  eta: 0:00:39  lr: 0.000237  training_loss: 2.0999 (2.1548)  mae_loss: 0.3123 (0.3277)  classification_loss: 1.6671 (1.7234)  loss_mask: 0.0927 (0.1036)  time: 0.1969  data: 0.0003  max mem: 5511
[07:07:07.151513] Epoch: [18]  [600/781]  eta: 0:00:35  lr: 0.000237  training_loss: 2.0833 (2.1527)  mae_loss: 0.3189 (0.3274)  classification_loss: 1.7048 (1.7229)  loss_mask: 0.0538 (0.1023)  time: 0.1963  data: 0.0002  max mem: 5511
[07:07:11.118602] Epoch: [18]  [620/781]  eta: 0:00:31  lr: 0.000237  training_loss: 2.0749 (2.1508)  mae_loss: 0.3193 (0.3269)  classification_loss: 1.7037 (1.7229)  loss_mask: 0.0510 (0.1010)  time: 0.1983  data: 0.0002  max mem: 5511
[07:07:15.074547] Epoch: [18]  [640/781]  eta: 0:00:27  lr: 0.000237  training_loss: 2.1025 (2.1502)  mae_loss: 0.3204 (0.3268)  classification_loss: 1.7001 (1.7224)  loss_mask: 0.0681 (0.1009)  time: 0.1977  data: 0.0002  max mem: 5511
[07:07:19.027023] Epoch: [18]  [660/781]  eta: 0:00:24  lr: 0.000237  training_loss: 2.0457 (2.1479)  mae_loss: 0.3206 (0.3267)  classification_loss: 1.6706 (1.7214)  loss_mask: 0.0616 (0.0998)  time: 0.1975  data: 0.0003  max mem: 5511
[07:07:22.964474] Epoch: [18]  [680/781]  eta: 0:00:20  lr: 0.000237  training_loss: 2.0852 (2.1469)  mae_loss: 0.3187 (0.3265)  classification_loss: 1.7093 (1.7215)  loss_mask: 0.0741 (0.0989)  time: 0.1968  data: 0.0003  max mem: 5511
[07:07:26.934949] Epoch: [18]  [700/781]  eta: 0:00:16  lr: 0.000237  training_loss: 2.0824 (2.1465)  mae_loss: 0.3322 (0.3267)  classification_loss: 1.6752 (1.7214)  loss_mask: 0.0537 (0.0985)  time: 0.1984  data: 0.0006  max mem: 5511
[07:07:30.869736] Epoch: [18]  [720/781]  eta: 0:00:12  lr: 0.000237  training_loss: 2.1343 (2.1467)  mae_loss: 0.3227 (0.3266)  classification_loss: 1.6829 (1.7220)  loss_mask: 0.0926 (0.0982)  time: 0.1966  data: 0.0002  max mem: 5511
[07:07:34.833427] Epoch: [18]  [740/781]  eta: 0:00:08  lr: 0.000237  training_loss: 2.0929 (2.1458)  mae_loss: 0.3070 (0.3263)  classification_loss: 1.7071 (1.7213)  loss_mask: 0.0740 (0.0982)  time: 0.1981  data: 0.0003  max mem: 5511
[07:07:38.801305] Epoch: [18]  [760/781]  eta: 0:00:04  lr: 0.000237  training_loss: 2.1472 (2.1457)  mae_loss: 0.3239 (0.3262)  classification_loss: 1.7071 (1.7214)  loss_mask: 0.0731 (0.0981)  time: 0.1983  data: 0.0002  max mem: 5511
[07:07:42.726385] Epoch: [18]  [780/781]  eta: 0:00:00  lr: 0.000237  training_loss: 2.0751 (2.1447)  mae_loss: 0.3126 (0.3259)  classification_loss: 1.6549 (1.7216)  loss_mask: 0.0575 (0.0972)  time: 0.1962  data: 0.0002  max mem: 5511
[07:07:42.892163] Epoch: [18] Total time: 0:02:35 (0.1986 s / it)
[07:07:42.892674] Averaged stats: lr: 0.000237  training_loss: 2.0751 (2.1447)  mae_loss: 0.3126 (0.3259)  classification_loss: 1.6549 (1.7216)  loss_mask: 0.0575 (0.0972)
[07:07:43.623474] Test:  [  0/157]  eta: 0:01:54  testing_loss: 0.9660 (0.9660)  acc1: 71.8750 (71.8750)  acc5: 96.8750 (96.8750)  time: 0.7269  data: 0.6964  max mem: 5511
[07:07:43.914628] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.1596 (1.1044)  acc1: 60.9375 (62.0739)  acc5: 96.8750 (96.8750)  time: 0.0923  data: 0.0638  max mem: 5511
[07:07:44.206377] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.0900 (1.0775)  acc1: 60.9375 (62.7976)  acc5: 96.8750 (96.9494)  time: 0.0289  data: 0.0003  max mem: 5511
[07:07:44.494807] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.0762 (1.0852)  acc1: 62.5000 (62.9032)  acc5: 96.8750 (96.4718)  time: 0.0289  data: 0.0002  max mem: 5511
[07:07:44.783121] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.0762 (1.0893)  acc1: 62.5000 (62.6524)  acc5: 95.3125 (96.1890)  time: 0.0287  data: 0.0002  max mem: 5511
[07:07:45.067368] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.0631 (1.0834)  acc1: 64.0625 (63.3578)  acc5: 95.3125 (96.2316)  time: 0.0285  data: 0.0002  max mem: 5511
[07:07:45.354493] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.0631 (1.0799)  acc1: 65.6250 (63.3197)  acc5: 96.8750 (96.2602)  time: 0.0284  data: 0.0002  max mem: 5511
[07:07:45.639578] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.0153 (1.0711)  acc1: 65.6250 (63.6884)  acc5: 96.8750 (96.3908)  time: 0.0285  data: 0.0002  max mem: 5511
[07:07:45.926134] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.0255 (1.0722)  acc1: 65.6250 (63.7346)  acc5: 96.8750 (96.3735)  time: 0.0285  data: 0.0002  max mem: 5511
[07:07:46.221954] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.0906 (1.0736)  acc1: 64.0625 (63.6848)  acc5: 96.8750 (96.4286)  time: 0.0290  data: 0.0002  max mem: 5511
[07:07:46.509893] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.0986 (1.0762)  acc1: 60.9375 (63.4282)  acc5: 96.8750 (96.5037)  time: 0.0291  data: 0.0002  max mem: 5511
[07:07:46.794699] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.1235 (1.0773)  acc1: 60.9375 (63.2742)  acc5: 96.8750 (96.5372)  time: 0.0285  data: 0.0002  max mem: 5511
[07:07:47.087867] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.0370 (1.0731)  acc1: 64.0625 (63.5589)  acc5: 96.8750 (96.5651)  time: 0.0288  data: 0.0002  max mem: 5511
[07:07:47.373144] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.0757 (1.0762)  acc1: 60.9375 (63.3469)  acc5: 96.8750 (96.5052)  time: 0.0288  data: 0.0002  max mem: 5511
[07:07:47.656954] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.1019 (1.0761)  acc1: 60.9375 (63.4087)  acc5: 96.8750 (96.5647)  time: 0.0283  data: 0.0002  max mem: 5511
[07:07:47.938662] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.0448 (1.0718)  acc1: 65.6250 (63.5658)  acc5: 96.8750 (96.5335)  time: 0.0281  data: 0.0002  max mem: 5511
[07:07:48.089277] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.0448 (1.0736)  acc1: 62.5000 (63.4000)  acc5: 96.8750 (96.5800)  time: 0.0271  data: 0.0001  max mem: 5511
[07:07:48.272141] Test: Total time: 0:00:05 (0.0342 s / it)
[07:07:48.272612] * Acc@1 63.400 Acc@5 96.580 loss 1.074
[07:07:48.272937] Accuracy of the network on the 10000 test images: 63.4%
[07:07:48.273126] Max accuracy: 64.06%
[07:07:48.382634] log_dir: ./output_dir
[07:07:49.372960] Epoch: [19]  [  0/781]  eta: 0:12:52  lr: 0.000237  training_loss: 2.1207 (2.1207)  mae_loss: 0.3404 (0.3404)  classification_loss: 1.7303 (1.7303)  loss_mask: 0.0500 (0.0500)  time: 0.9886  data: 0.7487  max mem: 5511
[07:07:53.325586] Epoch: [19]  [ 20/781]  eta: 0:02:58  lr: 0.000237  training_loss: 2.1101 (2.1042)  mae_loss: 0.3241 (0.3194)  classification_loss: 1.6714 (1.7046)  loss_mask: 0.0580 (0.0801)  time: 0.1975  data: 0.0002  max mem: 5511
[07:07:57.269747] Epoch: [19]  [ 40/781]  eta: 0:02:40  lr: 0.000237  training_loss: 2.0681 (2.0928)  mae_loss: 0.3126 (0.3189)  classification_loss: 1.6819 (1.6876)  loss_mask: 0.0745 (0.0863)  time: 0.1971  data: 0.0003  max mem: 5511
[07:08:01.211166] Epoch: [19]  [ 60/781]  eta: 0:02:31  lr: 0.000237  training_loss: 2.1379 (2.1091)  mae_loss: 0.2865 (0.3152)  classification_loss: 1.7224 (1.7058)  loss_mask: 0.0860 (0.0880)  time: 0.1970  data: 0.0002  max mem: 5511
[07:08:05.156264] Epoch: [19]  [ 80/781]  eta: 0:02:25  lr: 0.000237  training_loss: 2.1352 (2.1143)  mae_loss: 0.3065 (0.3124)  classification_loss: 1.7090 (1.7079)  loss_mask: 0.0912 (0.0941)  time: 0.1972  data: 0.0002  max mem: 5511
[07:08:09.076758] Epoch: [19]  [100/781]  eta: 0:02:19  lr: 0.000237  training_loss: 2.1015 (2.1210)  mae_loss: 0.3149 (0.3141)  classification_loss: 1.7295 (1.7129)  loss_mask: 0.0810 (0.0940)  time: 0.1959  data: 0.0002  max mem: 5511
[07:08:13.024133] Epoch: [19]  [120/781]  eta: 0:02:14  lr: 0.000237  training_loss: 2.0632 (2.1128)  mae_loss: 0.2928 (0.3120)  classification_loss: 1.7131 (1.7104)  loss_mask: 0.0667 (0.0904)  time: 0.1973  data: 0.0003  max mem: 5511
[07:08:16.950377] Epoch: [19]  [140/781]  eta: 0:02:09  lr: 0.000237  training_loss: 2.0901 (2.1145)  mae_loss: 0.3287 (0.3135)  classification_loss: 1.6515 (1.7023)  loss_mask: 0.1415 (0.0987)  time: 0.1962  data: 0.0002  max mem: 5511
[07:08:20.922481] Epoch: [19]  [160/781]  eta: 0:02:05  lr: 0.000237  training_loss: 2.2858 (2.1356)  mae_loss: 0.3223 (0.3146)  classification_loss: 1.6976 (1.7031)  loss_mask: 0.2587 (0.1180)  time: 0.1985  data: 0.0002  max mem: 5511
[07:08:24.851399] Epoch: [19]  [180/781]  eta: 0:02:01  lr: 0.000236  training_loss: 2.2071 (2.1444)  mae_loss: 0.3286 (0.3166)  classification_loss: 1.7404 (1.7087)  loss_mask: 0.1269 (0.1192)  time: 0.1964  data: 0.0002  max mem: 5511
[07:08:28.780029] Epoch: [19]  [200/781]  eta: 0:01:56  lr: 0.000236  training_loss: 2.1563 (2.1466)  mae_loss: 0.3289 (0.3181)  classification_loss: 1.7723 (1.7145)  loss_mask: 0.0640 (0.1140)  time: 0.1964  data: 0.0002  max mem: 5511
[07:08:32.702300] Epoch: [19]  [220/781]  eta: 0:01:52  lr: 0.000236  training_loss: 2.1325 (2.1459)  mae_loss: 0.3216 (0.3197)  classification_loss: 1.7119 (1.7156)  loss_mask: 0.0672 (0.1106)  time: 0.1960  data: 0.0003  max mem: 5511
[07:08:36.668520] Epoch: [19]  [240/781]  eta: 0:01:48  lr: 0.000236  training_loss: 2.1078 (2.1448)  mae_loss: 0.3208 (0.3196)  classification_loss: 1.6964 (1.7170)  loss_mask: 0.0654 (0.1082)  time: 0.1982  data: 0.0002  max mem: 5511
[07:08:40.645008] Epoch: [19]  [260/781]  eta: 0:01:44  lr: 0.000236  training_loss: 2.0625 (2.1400)  mae_loss: 0.3126 (0.3194)  classification_loss: 1.7091 (1.7164)  loss_mask: 0.0520 (0.1042)  time: 0.1987  data: 0.0002  max mem: 5511
[07:08:44.631522] Epoch: [19]  [280/781]  eta: 0:01:40  lr: 0.000236  training_loss: 2.0833 (2.1375)  mae_loss: 0.3305 (0.3202)  classification_loss: 1.7037 (1.7171)  loss_mask: 0.0416 (0.1001)  time: 0.1992  data: 0.0004  max mem: 5511
[07:08:48.577627] Epoch: [19]  [300/781]  eta: 0:01:36  lr: 0.000236  training_loss: 2.1210 (2.1360)  mae_loss: 0.3213 (0.3212)  classification_loss: 1.7385 (1.7182)  loss_mask: 0.0423 (0.0967)  time: 0.1972  data: 0.0002  max mem: 5511
[07:08:52.529457] Epoch: [19]  [320/781]  eta: 0:01:32  lr: 0.000236  training_loss: 2.0509 (2.1331)  mae_loss: 0.3375 (0.3222)  classification_loss: 1.6721 (1.7166)  loss_mask: 0.0426 (0.0943)  time: 0.1975  data: 0.0002  max mem: 5511
[07:08:56.451816] Epoch: [19]  [340/781]  eta: 0:01:27  lr: 0.000236  training_loss: 2.0314 (2.1275)  mae_loss: 0.3173 (0.3218)  classification_loss: 1.6520 (1.7142)  loss_mask: 0.0406 (0.0915)  time: 0.1960  data: 0.0003  max mem: 5511
[07:09:00.433719] Epoch: [19]  [360/781]  eta: 0:01:23  lr: 0.000236  training_loss: 2.0636 (2.1255)  mae_loss: 0.3332 (0.3226)  classification_loss: 1.7052 (1.7136)  loss_mask: 0.0403 (0.0894)  time: 0.1990  data: 0.0003  max mem: 5511
[07:09:04.362343] Epoch: [19]  [380/781]  eta: 0:01:19  lr: 0.000236  training_loss: 2.0084 (2.1207)  mae_loss: 0.3160 (0.3221)  classification_loss: 1.6570 (1.7117)  loss_mask: 0.0345 (0.0869)  time: 0.1964  data: 0.0003  max mem: 5511
[07:09:08.312409] Epoch: [19]  [400/781]  eta: 0:01:15  lr: 0.000236  training_loss: 2.0680 (2.1172)  mae_loss: 0.3207 (0.3222)  classification_loss: 1.6692 (1.7099)  loss_mask: 0.0497 (0.0850)  time: 0.1974  data: 0.0003  max mem: 5511
[07:09:12.255388] Epoch: [19]  [420/781]  eta: 0:01:11  lr: 0.000236  training_loss: 2.0778 (2.1153)  mae_loss: 0.3247 (0.3225)  classification_loss: 1.7213 (1.7095)  loss_mask: 0.0383 (0.0834)  time: 0.1971  data: 0.0002  max mem: 5511
[07:09:16.182029] Epoch: [19]  [440/781]  eta: 0:01:07  lr: 0.000236  training_loss: 2.0627 (2.1136)  mae_loss: 0.3296 (0.3230)  classification_loss: 1.6721 (1.7085)  loss_mask: 0.0503 (0.0821)  time: 0.1962  data: 0.0002  max mem: 5511
[07:09:20.146242] Epoch: [19]  [460/781]  eta: 0:01:03  lr: 0.000236  training_loss: 2.0661 (2.1111)  mae_loss: 0.3044 (0.3227)  classification_loss: 1.6557 (1.7060)  loss_mask: 0.0772 (0.0824)  time: 0.1981  data: 0.0002  max mem: 5511
[07:09:24.082740] Epoch: [19]  [480/781]  eta: 0:00:59  lr: 0.000236  training_loss: 2.1581 (2.1129)  mae_loss: 0.3026 (0.3220)  classification_loss: 1.7592 (1.7063)  loss_mask: 0.1026 (0.0846)  time: 0.1967  data: 0.0003  max mem: 5511
[07:09:28.034816] Epoch: [19]  [500/781]  eta: 0:00:55  lr: 0.000236  training_loss: 2.0879 (2.1116)  mae_loss: 0.3038 (0.3221)  classification_loss: 1.6809 (1.7056)  loss_mask: 0.0641 (0.0840)  time: 0.1975  data: 0.0002  max mem: 5511
[07:09:31.987502] Epoch: [19]  [520/781]  eta: 0:00:51  lr: 0.000236  training_loss: 2.0838 (2.1106)  mae_loss: 0.3202 (0.3222)  classification_loss: 1.6741 (1.7050)  loss_mask: 0.0544 (0.0834)  time: 0.1976  data: 0.0002  max mem: 5511
[07:09:35.927807] Epoch: [19]  [540/781]  eta: 0:00:47  lr: 0.000236  training_loss: 2.0832 (2.1101)  mae_loss: 0.3184 (0.3221)  classification_loss: 1.6681 (1.7045)  loss_mask: 0.0639 (0.0834)  time: 0.1969  data: 0.0002  max mem: 5511
[07:09:39.867323] Epoch: [19]  [560/781]  eta: 0:00:43  lr: 0.000236  training_loss: 2.0759 (2.1088)  mae_loss: 0.3159 (0.3220)  classification_loss: 1.6708 (1.7033)  loss_mask: 0.0742 (0.0835)  time: 0.1969  data: 0.0002  max mem: 5511
[07:09:43.806235] Epoch: [19]  [580/781]  eta: 0:00:39  lr: 0.000235  training_loss: 1.9981 (2.1064)  mae_loss: 0.3020 (0.3212)  classification_loss: 1.6647 (1.7026)  loss_mask: 0.0407 (0.0826)  time: 0.1969  data: 0.0002  max mem: 5511
[07:09:47.759370] Epoch: [19]  [600/781]  eta: 0:00:35  lr: 0.000235  training_loss: 1.9970 (2.1030)  mae_loss: 0.2984 (0.3208)  classification_loss: 1.6586 (1.7011)  loss_mask: 0.0271 (0.0811)  time: 0.1976  data: 0.0002  max mem: 5511
[07:09:51.717473] Epoch: [19]  [620/781]  eta: 0:00:31  lr: 0.000235  training_loss: 2.0146 (2.1006)  mae_loss: 0.3184 (0.3207)  classification_loss: 1.6585 (1.6999)  loss_mask: 0.0344 (0.0800)  time: 0.1978  data: 0.0002  max mem: 5511
[07:09:55.694338] Epoch: [19]  [640/781]  eta: 0:00:27  lr: 0.000235  training_loss: 2.0352 (2.0998)  mae_loss: 0.3161 (0.3205)  classification_loss: 1.6975 (1.7001)  loss_mask: 0.0356 (0.0791)  time: 0.1988  data: 0.0003  max mem: 5511
[07:09:59.643665] Epoch: [19]  [660/781]  eta: 0:00:24  lr: 0.000235  training_loss: 2.0558 (2.0995)  mae_loss: 0.2919 (0.3197)  classification_loss: 1.7115 (1.6998)  loss_mask: 0.1047 (0.0799)  time: 0.1974  data: 0.0002  max mem: 5511
[07:10:03.603627] Epoch: [19]  [680/781]  eta: 0:00:20  lr: 0.000235  training_loss: 2.0192 (2.0981)  mae_loss: 0.3127 (0.3196)  classification_loss: 1.6649 (1.6994)  loss_mask: 0.0496 (0.0791)  time: 0.1979  data: 0.0002  max mem: 5511
[07:10:07.585312] Epoch: [19]  [700/781]  eta: 0:00:16  lr: 0.000235  training_loss: 2.0620 (2.0976)  mae_loss: 0.3145 (0.3196)  classification_loss: 1.6479 (1.6991)  loss_mask: 0.0422 (0.0789)  time: 0.1990  data: 0.0003  max mem: 5511
[07:10:11.580980] Epoch: [19]  [720/781]  eta: 0:00:12  lr: 0.000235  training_loss: 2.0873 (2.0977)  mae_loss: 0.3204 (0.3195)  classification_loss: 1.6743 (1.6987)  loss_mask: 0.0779 (0.0794)  time: 0.1997  data: 0.0002  max mem: 5511
[07:10:15.520581] Epoch: [19]  [740/781]  eta: 0:00:08  lr: 0.000235  training_loss: 2.0247 (2.0960)  mae_loss: 0.3066 (0.3191)  classification_loss: 1.6968 (1.6984)  loss_mask: 0.0394 (0.0785)  time: 0.1969  data: 0.0002  max mem: 5511
[07:10:19.459337] Epoch: [19]  [760/781]  eta: 0:00:04  lr: 0.000235  training_loss: 2.0968 (2.0959)  mae_loss: 0.3052 (0.3189)  classification_loss: 1.7458 (1.6997)  loss_mask: 0.0302 (0.0773)  time: 0.1968  data: 0.0002  max mem: 5511
[07:10:23.415598] Epoch: [19]  [780/781]  eta: 0:00:00  lr: 0.000235  training_loss: 2.0807 (2.0956)  mae_loss: 0.3059 (0.3188)  classification_loss: 1.6845 (1.6996)  loss_mask: 0.0731 (0.0772)  time: 0.1977  data: 0.0001  max mem: 5511
[07:10:23.576100] Epoch: [19] Total time: 0:02:35 (0.1987 s / it)
[07:10:23.576703] Averaged stats: lr: 0.000235  training_loss: 2.0807 (2.0956)  mae_loss: 0.3059 (0.3188)  classification_loss: 1.6845 (1.6996)  loss_mask: 0.0731 (0.0772)
[07:10:24.135656] Test:  [  0/157]  eta: 0:01:26  testing_loss: 0.9384 (0.9384)  acc1: 68.7500 (68.7500)  acc5: 90.6250 (90.6250)  time: 0.5503  data: 0.5208  max mem: 5511
[07:10:24.449293] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 1.0716 (1.0689)  acc1: 64.0625 (63.2102)  acc5: 96.8750 (96.8750)  time: 0.0783  data: 0.0491  max mem: 5511
[07:10:24.740727] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.0338 (1.0376)  acc1: 64.0625 (64.5833)  acc5: 96.8750 (97.5446)  time: 0.0300  data: 0.0011  max mem: 5511
[07:10:25.030909] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 1.0228 (1.0415)  acc1: 68.7500 (65.1210)  acc5: 96.8750 (97.0766)  time: 0.0289  data: 0.0003  max mem: 5511
[07:10:25.320317] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 1.0228 (1.0371)  acc1: 68.7500 (65.4345)  acc5: 96.8750 (97.0274)  time: 0.0288  data: 0.0003  max mem: 5511
[07:10:25.606674] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.0077 (1.0327)  acc1: 67.1875 (65.4718)  acc5: 98.4375 (97.3958)  time: 0.0286  data: 0.0002  max mem: 5511
[07:10:25.893451] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.0176 (1.0317)  acc1: 65.6250 (65.3945)  acc5: 98.4375 (97.3105)  time: 0.0285  data: 0.0002  max mem: 5511
[07:10:26.184686] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.9810 (1.0227)  acc1: 67.1875 (65.8231)  acc5: 98.4375 (97.4032)  time: 0.0287  data: 0.0003  max mem: 5511
[07:10:26.483796] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.9933 (1.0260)  acc1: 67.1875 (65.7215)  acc5: 98.4375 (97.2801)  time: 0.0293  data: 0.0003  max mem: 5511
[07:10:26.774159] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.0305 (1.0278)  acc1: 65.6250 (65.5907)  acc5: 96.8750 (97.2871)  time: 0.0293  data: 0.0004  max mem: 5511
[07:10:27.063448] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.0425 (1.0336)  acc1: 64.0625 (65.4239)  acc5: 98.4375 (97.2927)  time: 0.0288  data: 0.0004  max mem: 5511
[07:10:27.351893] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.0792 (1.0353)  acc1: 64.0625 (65.2872)  acc5: 96.8750 (97.2973)  time: 0.0287  data: 0.0002  max mem: 5511
[07:10:27.639671] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.0276 (1.0308)  acc1: 65.6250 (65.4959)  acc5: 96.8750 (97.2624)  time: 0.0286  data: 0.0002  max mem: 5511
[07:10:27.924193] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.0279 (1.0326)  acc1: 65.6250 (65.4222)  acc5: 96.8750 (97.2090)  time: 0.0285  data: 0.0002  max mem: 5511
[07:10:28.210633] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.0480 (1.0319)  acc1: 64.0625 (65.4034)  acc5: 96.8750 (97.1964)  time: 0.0284  data: 0.0002  max mem: 5511
[07:10:28.493539] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.0098 (1.0288)  acc1: 64.0625 (65.4180)  acc5: 96.8750 (97.1958)  time: 0.0283  data: 0.0002  max mem: 5511
[07:10:28.646580] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.0242 (1.0309)  acc1: 64.0625 (65.2800)  acc5: 96.8750 (97.2000)  time: 0.0273  data: 0.0001  max mem: 5511
[07:10:28.797566] Test: Total time: 0:00:05 (0.0332 s / it)
[07:10:28.798024] * Acc@1 65.280 Acc@5 97.200 loss 1.031
[07:10:28.798318] Accuracy of the network on the 10000 test images: 65.3%
[07:10:28.798510] Max accuracy: 65.28%
[07:10:29.095645] log_dir: ./output_dir
[07:10:29.918915] Epoch: [20]  [  0/781]  eta: 0:10:40  lr: 0.000235  training_loss: 2.1191 (2.1191)  mae_loss: 0.3199 (0.3199)  classification_loss: 1.6674 (1.6674)  loss_mask: 0.1318 (0.1318)  time: 0.8207  data: 0.5995  max mem: 5511
[07:10:33.887162] Epoch: [20]  [ 20/781]  eta: 0:02:53  lr: 0.000235  training_loss: 2.0688 (2.1001)  mae_loss: 0.3074 (0.3142)  classification_loss: 1.6533 (1.6733)  loss_mask: 0.1083 (0.1127)  time: 0.1983  data: 0.0002  max mem: 5511
[07:10:37.833102] Epoch: [20]  [ 40/781]  eta: 0:02:37  lr: 0.000235  training_loss: 2.0835 (2.1084)  mae_loss: 0.3132 (0.3146)  classification_loss: 1.6773 (1.6822)  loss_mask: 0.0783 (0.1116)  time: 0.1972  data: 0.0002  max mem: 5511
[07:10:41.765984] Epoch: [20]  [ 60/781]  eta: 0:02:29  lr: 0.000235  training_loss: 2.1025 (2.1048)  mae_loss: 0.3059 (0.3144)  classification_loss: 1.7014 (1.6860)  loss_mask: 0.0881 (0.1044)  time: 0.1966  data: 0.0002  max mem: 5511
[07:10:45.699887] Epoch: [20]  [ 80/781]  eta: 0:02:23  lr: 0.000235  training_loss: 2.0398 (2.0892)  mae_loss: 0.3019 (0.3136)  classification_loss: 1.6695 (1.6805)  loss_mask: 0.0644 (0.0951)  time: 0.1966  data: 0.0002  max mem: 5511
[07:10:49.634803] Epoch: [20]  [100/781]  eta: 0:02:18  lr: 0.000235  training_loss: 2.0747 (2.0843)  mae_loss: 0.3015 (0.3129)  classification_loss: 1.6912 (1.6812)  loss_mask: 0.0586 (0.0902)  time: 0.1966  data: 0.0002  max mem: 5511
[07:10:53.578036] Epoch: [20]  [120/781]  eta: 0:02:13  lr: 0.000235  training_loss: 2.0390 (2.0752)  mae_loss: 0.3122 (0.3128)  classification_loss: 1.6425 (1.6756)  loss_mask: 0.0502 (0.0868)  time: 0.1970  data: 0.0002  max mem: 5511
[07:10:57.520287] Epoch: [20]  [140/781]  eta: 0:02:09  lr: 0.000235  training_loss: 2.0982 (2.0805)  mae_loss: 0.3061 (0.3125)  classification_loss: 1.7272 (1.6780)  loss_mask: 0.0617 (0.0900)  time: 0.1970  data: 0.0003  max mem: 5511
[07:11:01.468417] Epoch: [20]  [160/781]  eta: 0:02:04  lr: 0.000235  training_loss: 2.1400 (2.0863)  mae_loss: 0.3191 (0.3136)  classification_loss: 1.7210 (1.6821)  loss_mask: 0.0746 (0.0906)  time: 0.1973  data: 0.0002  max mem: 5511
[07:11:05.420244] Epoch: [20]  [180/781]  eta: 0:02:00  lr: 0.000235  training_loss: 2.0366 (2.0821)  mae_loss: 0.3069 (0.3133)  classification_loss: 1.7074 (1.6811)  loss_mask: 0.0587 (0.0877)  time: 0.1975  data: 0.0002  max mem: 5511
[07:11:09.361011] Epoch: [20]  [200/781]  eta: 0:01:56  lr: 0.000234  training_loss: 2.0464 (2.0819)  mae_loss: 0.2974 (0.3121)  classification_loss: 1.7154 (1.6835)  loss_mask: 0.0532 (0.0862)  time: 0.1970  data: 0.0002  max mem: 5511
[07:11:13.315166] Epoch: [20]  [220/781]  eta: 0:01:52  lr: 0.000234  training_loss: 2.0701 (2.0820)  mae_loss: 0.3130 (0.3123)  classification_loss: 1.7314 (1.6867)  loss_mask: 0.0436 (0.0830)  time: 0.1976  data: 0.0002  max mem: 5511
[07:11:17.291375] Epoch: [20]  [240/781]  eta: 0:01:48  lr: 0.000234  training_loss: 2.0756 (2.0805)  mae_loss: 0.3011 (0.3115)  classification_loss: 1.7184 (1.6890)  loss_mask: 0.0385 (0.0800)  time: 0.1987  data: 0.0002  max mem: 5511
[07:11:21.259718] Epoch: [20]  [260/781]  eta: 0:01:44  lr: 0.000234  training_loss: 2.0406 (2.0791)  mae_loss: 0.3074 (0.3119)  classification_loss: 1.6772 (1.6886)  loss_mask: 0.0410 (0.0786)  time: 0.1983  data: 0.0002  max mem: 5511
[07:11:25.210409] Epoch: [20]  [280/781]  eta: 0:01:39  lr: 0.000234  training_loss: 2.2076 (2.0909)  mae_loss: 0.3075 (0.3122)  classification_loss: 1.7722 (1.6956)  loss_mask: 0.1121 (0.0831)  time: 0.1974  data: 0.0002  max mem: 5511
[07:11:29.151683] Epoch: [20]  [300/781]  eta: 0:01:35  lr: 0.000234  training_loss: 2.1210 (2.0923)  mae_loss: 0.3047 (0.3119)  classification_loss: 1.7534 (1.6982)  loss_mask: 0.0514 (0.0823)  time: 0.1970  data: 0.0002  max mem: 5511
[07:11:33.088269] Epoch: [20]  [320/781]  eta: 0:01:31  lr: 0.000234  training_loss: 2.0873 (2.0906)  mae_loss: 0.3097 (0.3117)  classification_loss: 1.7013 (1.6976)  loss_mask: 0.0569 (0.0813)  time: 0.1968  data: 0.0002  max mem: 5511
[07:11:37.024305] Epoch: [20]  [340/781]  eta: 0:01:27  lr: 0.000234  training_loss: 2.0250 (2.0873)  mae_loss: 0.3074 (0.3117)  classification_loss: 1.6362 (1.6956)  loss_mask: 0.0439 (0.0800)  time: 0.1967  data: 0.0003  max mem: 5511
[07:11:40.992022] Epoch: [20]  [360/781]  eta: 0:01:23  lr: 0.000234  training_loss: 2.0357 (2.0846)  mae_loss: 0.3122 (0.3115)  classification_loss: 1.6906 (1.6953)  loss_mask: 0.0341 (0.0779)  time: 0.1983  data: 0.0002  max mem: 5511
[07:11:44.932927] Epoch: [20]  [380/781]  eta: 0:01:19  lr: 0.000234  training_loss: 2.0758 (2.0843)  mae_loss: 0.3080 (0.3121)  classification_loss: 1.6703 (1.6948)  loss_mask: 0.0569 (0.0773)  time: 0.1970  data: 0.0002  max mem: 5511
[07:11:48.885908] Epoch: [20]  [400/781]  eta: 0:01:15  lr: 0.000234  training_loss: 2.0388 (2.0829)  mae_loss: 0.3123 (0.3124)  classification_loss: 1.6551 (1.6942)  loss_mask: 0.0371 (0.0763)  time: 0.1976  data: 0.0004  max mem: 5511
[07:11:52.829093] Epoch: [20]  [420/781]  eta: 0:01:11  lr: 0.000234  training_loss: 2.0610 (2.0828)  mae_loss: 0.3097 (0.3124)  classification_loss: 1.7072 (1.6942)  loss_mask: 0.0627 (0.0762)  time: 0.1970  data: 0.0002  max mem: 5511
[07:11:56.802780] Epoch: [20]  [440/781]  eta: 0:01:07  lr: 0.000234  training_loss: 2.0169 (2.0802)  mae_loss: 0.3089 (0.3118)  classification_loss: 1.6617 (1.6924)  loss_mask: 0.0546 (0.0760)  time: 0.1986  data: 0.0002  max mem: 5511
[07:12:00.762632] Epoch: [20]  [460/781]  eta: 0:01:03  lr: 0.000234  training_loss: 2.0250 (2.0776)  mae_loss: 0.2969 (0.3114)  classification_loss: 1.6504 (1.6905)  loss_mask: 0.0543 (0.0756)  time: 0.1979  data: 0.0002  max mem: 5511
[07:12:04.703519] Epoch: [20]  [480/781]  eta: 0:00:59  lr: 0.000234  training_loss: 2.0333 (2.0746)  mae_loss: 0.2909 (0.3108)  classification_loss: 1.6972 (1.6899)  loss_mask: 0.0330 (0.0739)  time: 0.1969  data: 0.0002  max mem: 5511
[07:12:08.645739] Epoch: [20]  [500/781]  eta: 0:00:55  lr: 0.000234  training_loss: 2.3032 (2.0854)  mae_loss: 0.3522 (0.3121)  classification_loss: 1.7211 (1.6919)  loss_mask: 0.2099 (0.0813)  time: 0.1970  data: 0.0002  max mem: 5511
[07:12:12.592211] Epoch: [20]  [520/781]  eta: 0:00:51  lr: 0.000234  training_loss: 2.3993 (2.0964)  mae_loss: 0.3357 (0.3133)  classification_loss: 1.8214 (1.6975)  loss_mask: 0.1720 (0.0855)  time: 0.1972  data: 0.0002  max mem: 5511
[07:12:16.531005] Epoch: [20]  [540/781]  eta: 0:00:47  lr: 0.000234  training_loss: 2.5588 (2.1134)  mae_loss: 0.3396 (0.3144)  classification_loss: 1.8776 (1.7063)  loss_mask: 0.2799 (0.0927)  time: 0.1968  data: 0.0003  max mem: 5511
[07:12:20.507804] Epoch: [20]  [560/781]  eta: 0:00:43  lr: 0.000234  training_loss: 2.3836 (2.1246)  mae_loss: 0.3631 (0.3161)  classification_loss: 1.8179 (1.7111)  loss_mask: 0.1778 (0.0975)  time: 0.1988  data: 0.0002  max mem: 5511
[07:12:24.454941] Epoch: [20]  [580/781]  eta: 0:00:39  lr: 0.000234  training_loss: 2.3078 (2.1314)  mae_loss: 0.3344 (0.3170)  classification_loss: 1.8506 (1.7163)  loss_mask: 0.1155 (0.0981)  time: 0.1973  data: 0.0002  max mem: 5511
[07:12:28.409214] Epoch: [20]  [600/781]  eta: 0:00:35  lr: 0.000233  training_loss: 2.2441 (2.1360)  mae_loss: 0.3299 (0.3177)  classification_loss: 1.8595 (1.7201)  loss_mask: 0.1007 (0.0982)  time: 0.1976  data: 0.0002  max mem: 5511
[07:12:32.361011] Epoch: [20]  [620/781]  eta: 0:00:31  lr: 0.000233  training_loss: 2.2406 (2.1397)  mae_loss: 0.3358 (0.3182)  classification_loss: 1.7468 (1.7224)  loss_mask: 0.1317 (0.0991)  time: 0.1975  data: 0.0002  max mem: 5511
[07:12:36.306982] Epoch: [20]  [640/781]  eta: 0:00:27  lr: 0.000233  training_loss: 2.1411 (2.1405)  mae_loss: 0.3184 (0.3182)  classification_loss: 1.7674 (1.7240)  loss_mask: 0.0693 (0.0983)  time: 0.1972  data: 0.0002  max mem: 5511
[07:12:40.262440] Epoch: [20]  [660/781]  eta: 0:00:23  lr: 0.000233  training_loss: 2.1237 (2.1401)  mae_loss: 0.3132 (0.3184)  classification_loss: 1.7183 (1.7240)  loss_mask: 0.0657 (0.0977)  time: 0.1976  data: 0.0002  max mem: 5511
[07:12:44.197613] Epoch: [20]  [680/781]  eta: 0:00:20  lr: 0.000233  training_loss: 2.1732 (2.1411)  mae_loss: 0.3156 (0.3184)  classification_loss: 1.7332 (1.7242)  loss_mask: 0.1014 (0.0985)  time: 0.1967  data: 0.0003  max mem: 5511
[07:12:48.184931] Epoch: [20]  [700/781]  eta: 0:00:16  lr: 0.000233  training_loss: 2.1819 (2.1417)  mae_loss: 0.2980 (0.3180)  classification_loss: 1.7382 (1.7250)  loss_mask: 0.0818 (0.0987)  time: 0.1993  data: 0.0002  max mem: 5511
[07:12:52.188180] Epoch: [20]  [720/781]  eta: 0:00:12  lr: 0.000233  training_loss: 2.0974 (2.1408)  mae_loss: 0.2998 (0.3178)  classification_loss: 1.7125 (1.7248)  loss_mask: 0.0708 (0.0981)  time: 0.2001  data: 0.0003  max mem: 5511
[07:12:56.147552] Epoch: [20]  [740/781]  eta: 0:00:08  lr: 0.000233  training_loss: 2.1202 (2.1398)  mae_loss: 0.3161 (0.3179)  classification_loss: 1.6930 (1.7242)  loss_mask: 0.0810 (0.0977)  time: 0.1978  data: 0.0002  max mem: 5511
[07:13:00.156082] Epoch: [20]  [760/781]  eta: 0:00:04  lr: 0.000233  training_loss: 2.0479 (2.1380)  mae_loss: 0.3084 (0.3178)  classification_loss: 1.6950 (1.7237)  loss_mask: 0.0445 (0.0965)  time: 0.2003  data: 0.0002  max mem: 5511
[07:13:04.084227] Epoch: [20]  [780/781]  eta: 0:00:00  lr: 0.000233  training_loss: 2.0694 (2.1362)  mae_loss: 0.3142 (0.3177)  classification_loss: 1.6768 (1.7231)  loss_mask: 0.0456 (0.0954)  time: 0.1963  data: 0.0002  max mem: 5511
[07:13:04.257000] Epoch: [20] Total time: 0:02:35 (0.1987 s / it)
[07:13:04.257923] Averaged stats: lr: 0.000233  training_loss: 2.0694 (2.1362)  mae_loss: 0.3142 (0.3177)  classification_loss: 1.6768 (1.7231)  loss_mask: 0.0456 (0.0954)
[07:13:05.387775] Test:  [  0/157]  eta: 0:01:49  testing_loss: 0.9569 (0.9569)  acc1: 67.1875 (67.1875)  acc5: 95.3125 (95.3125)  time: 0.7004  data: 0.6685  max mem: 5511
[07:13:05.675851] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.0924 (1.1145)  acc1: 60.9375 (59.9432)  acc5: 96.8750 (97.0170)  time: 0.0897  data: 0.0610  max mem: 5511
[07:13:05.963285] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.0612 (1.0666)  acc1: 60.9375 (62.5000)  acc5: 96.8750 (97.1726)  time: 0.0286  data: 0.0002  max mem: 5511
[07:13:06.253680] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.0507 (1.0716)  acc1: 64.0625 (62.7520)  acc5: 96.8750 (97.1270)  time: 0.0287  data: 0.0002  max mem: 5511
[07:13:06.542489] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.0558 (1.0689)  acc1: 65.6250 (63.5671)  acc5: 96.8750 (97.1037)  time: 0.0288  data: 0.0002  max mem: 5511
[07:13:06.829822] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.0615 (1.0686)  acc1: 65.6250 (63.8787)  acc5: 96.8750 (97.1201)  time: 0.0286  data: 0.0002  max mem: 5511
[07:13:07.116678] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.0615 (1.0636)  acc1: 64.0625 (63.8576)  acc5: 96.8750 (97.0799)  time: 0.0285  data: 0.0002  max mem: 5511
[07:13:07.402753] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.0075 (1.0574)  acc1: 64.0625 (63.8644)  acc5: 96.8750 (97.1611)  time: 0.0285  data: 0.0002  max mem: 5511
[07:13:07.690003] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.0075 (1.0592)  acc1: 64.0625 (63.9468)  acc5: 96.8750 (97.1065)  time: 0.0285  data: 0.0002  max mem: 5511
[07:13:07.977556] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.0925 (1.0624)  acc1: 64.0625 (63.8565)  acc5: 96.8750 (97.1326)  time: 0.0286  data: 0.0002  max mem: 5511
[07:13:08.263447] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.0955 (1.0677)  acc1: 62.5000 (63.6293)  acc5: 96.8750 (97.1380)  time: 0.0285  data: 0.0002  max mem: 5511
[07:13:08.548686] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.1411 (1.0714)  acc1: 60.9375 (63.3868)  acc5: 96.8750 (97.0580)  time: 0.0284  data: 0.0002  max mem: 5511
[07:13:08.837638] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.0622 (1.0666)  acc1: 64.0625 (63.6622)  acc5: 96.8750 (97.0687)  time: 0.0286  data: 0.0002  max mem: 5511
[07:13:09.124861] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.0551 (1.0675)  acc1: 64.0625 (63.6331)  acc5: 96.8750 (97.0062)  time: 0.0286  data: 0.0002  max mem: 5511
[07:13:09.415656] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.0573 (1.0656)  acc1: 64.0625 (63.8520)  acc5: 95.3125 (97.0191)  time: 0.0287  data: 0.0002  max mem: 5511
[07:13:09.698150] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.0476 (1.0625)  acc1: 67.1875 (64.0315)  acc5: 96.8750 (97.0509)  time: 0.0285  data: 0.0001  max mem: 5511
[07:13:09.850279] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.0498 (1.0651)  acc1: 65.6250 (63.9500)  acc5: 96.8750 (97.0700)  time: 0.0272  data: 0.0001  max mem: 5511
[07:13:10.030275] Test: Total time: 0:00:05 (0.0340 s / it)
[07:13:10.031525] * Acc@1 63.950 Acc@5 97.070 loss 1.065
[07:13:10.031962] Accuracy of the network on the 10000 test images: 64.0%
[07:13:10.032182] Max accuracy: 65.28%
[07:13:10.162265] log_dir: ./output_dir
[07:13:11.044279] Epoch: [21]  [  0/781]  eta: 0:11:27  lr: 0.000233  training_loss: 2.1002 (2.1002)  mae_loss: 0.3338 (0.3338)  classification_loss: 1.5866 (1.5866)  loss_mask: 0.1798 (0.1798)  time: 0.8802  data: 0.6505  max mem: 5511
[07:13:14.990801] Epoch: [21]  [ 20/781]  eta: 0:02:54  lr: 0.000233  training_loss: 2.1316 (2.1236)  mae_loss: 0.3119 (0.3155)  classification_loss: 1.6328 (1.6440)  loss_mask: 0.1721 (0.1641)  time: 0.1972  data: 0.0003  max mem: 5511
[07:13:18.932167] Epoch: [21]  [ 40/781]  eta: 0:02:38  lr: 0.000233  training_loss: 2.1192 (2.1328)  mae_loss: 0.2964 (0.3115)  classification_loss: 1.6701 (1.6600)  loss_mask: 0.1376 (0.1614)  time: 0.1970  data: 0.0003  max mem: 5511
[07:13:22.872304] Epoch: [21]  [ 60/781]  eta: 0:02:30  lr: 0.000233  training_loss: 2.1058 (2.1354)  mae_loss: 0.3126 (0.3141)  classification_loss: 1.6943 (1.6782)  loss_mask: 0.0794 (0.1430)  time: 0.1969  data: 0.0002  max mem: 5511
[07:13:26.827426] Epoch: [21]  [ 80/781]  eta: 0:02:24  lr: 0.000233  training_loss: 2.0899 (2.1235)  mae_loss: 0.3166 (0.3149)  classification_loss: 1.7110 (1.6837)  loss_mask: 0.0603 (0.1249)  time: 0.1977  data: 0.0002  max mem: 5511
[07:13:30.782252] Epoch: [21]  [100/781]  eta: 0:02:18  lr: 0.000233  training_loss: 2.0584 (2.1105)  mae_loss: 0.3057 (0.3136)  classification_loss: 1.7173 (1.6869)  loss_mask: 0.0341 (0.1100)  time: 0.1977  data: 0.0002  max mem: 5511
[07:13:34.763146] Epoch: [21]  [120/781]  eta: 0:02:14  lr: 0.000233  training_loss: 2.0631 (2.1031)  mae_loss: 0.2997 (0.3128)  classification_loss: 1.6649 (1.6838)  loss_mask: 0.0869 (0.1064)  time: 0.1990  data: 0.0002  max mem: 5511
[07:13:38.710951] Epoch: [21]  [140/781]  eta: 0:02:09  lr: 0.000233  training_loss: 2.0540 (2.1013)  mae_loss: 0.2953 (0.3120)  classification_loss: 1.6474 (1.6795)  loss_mask: 0.1158 (0.1098)  time: 0.1973  data: 0.0002  max mem: 5511
[07:13:42.647318] Epoch: [21]  [160/781]  eta: 0:02:05  lr: 0.000233  training_loss: 2.0585 (2.0979)  mae_loss: 0.2946 (0.3107)  classification_loss: 1.6881 (1.6800)  loss_mask: 0.0825 (0.1072)  time: 0.1967  data: 0.0002  max mem: 5511
[07:13:46.613496] Epoch: [21]  [180/781]  eta: 0:02:00  lr: 0.000232  training_loss: 2.0214 (2.0912)  mae_loss: 0.3084 (0.3096)  classification_loss: 1.6382 (1.6771)  loss_mask: 0.0684 (0.1045)  time: 0.1982  data: 0.0002  max mem: 5511
[07:13:50.559757] Epoch: [21]  [200/781]  eta: 0:01:56  lr: 0.000232  training_loss: 2.1020 (2.0917)  mae_loss: 0.3058 (0.3095)  classification_loss: 1.6926 (1.6783)  loss_mask: 0.0675 (0.1039)  time: 0.1972  data: 0.0002  max mem: 5511
[07:13:54.567486] Epoch: [21]  [220/781]  eta: 0:01:52  lr: 0.000232  training_loss: 2.0262 (2.0875)  mae_loss: 0.3075 (0.3092)  classification_loss: 1.6773 (1.6783)  loss_mask: 0.0580 (0.1000)  time: 0.2002  data: 0.0002  max mem: 5511
[07:13:58.513919] Epoch: [21]  [240/781]  eta: 0:01:48  lr: 0.000232  training_loss: 1.9324 (2.0802)  mae_loss: 0.2988 (0.3088)  classification_loss: 1.5900 (1.6749)  loss_mask: 0.0446 (0.0965)  time: 0.1972  data: 0.0002  max mem: 5511
[07:14:02.458803] Epoch: [21]  [260/781]  eta: 0:01:44  lr: 0.000232  training_loss: 2.0421 (2.0791)  mae_loss: 0.3210 (0.3096)  classification_loss: 1.6424 (1.6737)  loss_mask: 0.0857 (0.0958)  time: 0.1972  data: 0.0002  max mem: 5511
[07:14:06.403147] Epoch: [21]  [280/781]  eta: 0:01:40  lr: 0.000232  training_loss: 2.1380 (2.0828)  mae_loss: 0.3211 (0.3112)  classification_loss: 1.7016 (1.6769)  loss_mask: 0.0866 (0.0948)  time: 0.1971  data: 0.0002  max mem: 5511
[07:14:10.347536] Epoch: [21]  [300/781]  eta: 0:01:36  lr: 0.000232  training_loss: 2.0697 (2.0822)  mae_loss: 0.3160 (0.3113)  classification_loss: 1.6830 (1.6792)  loss_mask: 0.0488 (0.0917)  time: 0.1971  data: 0.0002  max mem: 5511
[07:14:14.310908] Epoch: [21]  [320/781]  eta: 0:01:32  lr: 0.000232  training_loss: 2.1053 (2.0820)  mae_loss: 0.3077 (0.3111)  classification_loss: 1.7348 (1.6811)  loss_mask: 0.0356 (0.0897)  time: 0.1981  data: 0.0002  max mem: 5511
[07:14:18.271464] Epoch: [21]  [340/781]  eta: 0:01:28  lr: 0.000232  training_loss: 2.0608 (2.0802)  mae_loss: 0.3194 (0.3120)  classification_loss: 1.6596 (1.6789)  loss_mask: 0.0727 (0.0893)  time: 0.1979  data: 0.0002  max mem: 5511
[07:14:22.251175] Epoch: [21]  [360/781]  eta: 0:01:24  lr: 0.000232  training_loss: 2.1683 (2.0834)  mae_loss: 0.3097 (0.3123)  classification_loss: 1.6800 (1.6796)  loss_mask: 0.1106 (0.0915)  time: 0.1989  data: 0.0002  max mem: 5511
[07:14:26.205430] Epoch: [21]  [380/781]  eta: 0:01:19  lr: 0.000232  training_loss: 2.0764 (2.0835)  mae_loss: 0.3113 (0.3121)  classification_loss: 1.7254 (1.6812)  loss_mask: 0.0613 (0.0902)  time: 0.1976  data: 0.0003  max mem: 5511
[07:14:30.187227] Epoch: [21]  [400/781]  eta: 0:01:15  lr: 0.000232  training_loss: 2.0122 (2.0828)  mae_loss: 0.3257 (0.3125)  classification_loss: 1.6494 (1.6813)  loss_mask: 0.0366 (0.0890)  time: 0.1990  data: 0.0003  max mem: 5511
[07:14:34.116531] Epoch: [21]  [420/781]  eta: 0:01:11  lr: 0.000232  training_loss: 2.0300 (2.0815)  mae_loss: 0.3181 (0.3129)  classification_loss: 1.6844 (1.6811)  loss_mask: 0.0503 (0.0875)  time: 0.1964  data: 0.0003  max mem: 5511
[07:14:38.051440] Epoch: [21]  [440/781]  eta: 0:01:07  lr: 0.000232  training_loss: 1.9456 (2.0769)  mae_loss: 0.2971 (0.3128)  classification_loss: 1.5834 (1.6787)  loss_mask: 0.0346 (0.0854)  time: 0.1966  data: 0.0003  max mem: 5511
[07:14:41.997355] Epoch: [21]  [460/781]  eta: 0:01:03  lr: 0.000232  training_loss: 2.0285 (2.0771)  mae_loss: 0.3113 (0.3125)  classification_loss: 1.6676 (1.6780)  loss_mask: 0.0681 (0.0865)  time: 0.1972  data: 0.0002  max mem: 5511
[07:14:45.953987] Epoch: [21]  [480/781]  eta: 0:00:59  lr: 0.000232  training_loss: 2.0546 (2.0786)  mae_loss: 0.2971 (0.3119)  classification_loss: 1.6855 (1.6789)  loss_mask: 0.0833 (0.0878)  time: 0.1978  data: 0.0002  max mem: 5511
[07:14:49.913956] Epoch: [21]  [500/781]  eta: 0:00:55  lr: 0.000232  training_loss: 2.0724 (2.0775)  mae_loss: 0.3126 (0.3117)  classification_loss: 1.6995 (1.6789)  loss_mask: 0.0695 (0.0869)  time: 0.1979  data: 0.0002  max mem: 5511
[07:14:53.843249] Epoch: [21]  [520/781]  eta: 0:00:51  lr: 0.000232  training_loss: 2.1054 (2.0783)  mae_loss: 0.3357 (0.3126)  classification_loss: 1.6581 (1.6782)  loss_mask: 0.0764 (0.0875)  time: 0.1964  data: 0.0002  max mem: 5511
[07:14:57.800179] Epoch: [21]  [540/781]  eta: 0:00:47  lr: 0.000232  training_loss: 2.1931 (2.0831)  mae_loss: 0.3178 (0.3129)  classification_loss: 1.7056 (1.6798)  loss_mask: 0.1234 (0.0904)  time: 0.1978  data: 0.0002  max mem: 5511
[07:15:01.765046] Epoch: [21]  [560/781]  eta: 0:00:43  lr: 0.000231  training_loss: 2.0890 (2.0827)  mae_loss: 0.3192 (0.3132)  classification_loss: 1.6865 (1.6795)  loss_mask: 0.0650 (0.0900)  time: 0.1982  data: 0.0005  max mem: 5511
[07:15:05.713060] Epoch: [21]  [580/781]  eta: 0:00:39  lr: 0.000231  training_loss: 2.0520 (2.0821)  mae_loss: 0.3091 (0.3131)  classification_loss: 1.6554 (1.6795)  loss_mask: 0.0654 (0.0895)  time: 0.1973  data: 0.0003  max mem: 5511
[07:15:09.677677] Epoch: [21]  [600/781]  eta: 0:00:35  lr: 0.000231  training_loss: 2.0173 (2.0803)  mae_loss: 0.3046 (0.3128)  classification_loss: 1.6630 (1.6794)  loss_mask: 0.0494 (0.0882)  time: 0.1982  data: 0.0002  max mem: 5511
[07:15:13.651392] Epoch: [21]  [620/781]  eta: 0:00:31  lr: 0.000231  training_loss: 2.0126 (2.0782)  mae_loss: 0.3101 (0.3127)  classification_loss: 1.6207 (1.6784)  loss_mask: 0.0473 (0.0871)  time: 0.1986  data: 0.0002  max mem: 5511
[07:15:17.608769] Epoch: [21]  [640/781]  eta: 0:00:28  lr: 0.000231  training_loss: 1.9725 (2.0748)  mae_loss: 0.2984 (0.3125)  classification_loss: 1.6152 (1.6769)  loss_mask: 0.0285 (0.0854)  time: 0.1978  data: 0.0002  max mem: 5511
[07:15:21.583025] Epoch: [21]  [660/781]  eta: 0:00:24  lr: 0.000231  training_loss: 2.3029 (2.0809)  mae_loss: 0.3143 (0.3127)  classification_loss: 1.7854 (1.6806)  loss_mask: 0.1020 (0.0876)  time: 0.1986  data: 0.0002  max mem: 5511
[07:15:25.555624] Epoch: [21]  [680/781]  eta: 0:00:20  lr: 0.000231  training_loss: 2.2154 (2.0858)  mae_loss: 0.3274 (0.3133)  classification_loss: 1.7451 (1.6832)  loss_mask: 0.0991 (0.0893)  time: 0.1986  data: 0.0002  max mem: 5511
[07:15:29.494579] Epoch: [21]  [700/781]  eta: 0:00:16  lr: 0.000231  training_loss: 2.0818 (2.0876)  mae_loss: 0.3210 (0.3138)  classification_loss: 1.6936 (1.6843)  loss_mask: 0.0875 (0.0896)  time: 0.1969  data: 0.0002  max mem: 5511
[07:15:33.463580] Epoch: [21]  [720/781]  eta: 0:00:12  lr: 0.000231  training_loss: 2.1046 (2.0893)  mae_loss: 0.3248 (0.3140)  classification_loss: 1.6982 (1.6851)  loss_mask: 0.0740 (0.0903)  time: 0.1984  data: 0.0002  max mem: 5511
[07:15:37.407718] Epoch: [21]  [740/781]  eta: 0:00:08  lr: 0.000231  training_loss: 2.1044 (2.0898)  mae_loss: 0.3174 (0.3141)  classification_loss: 1.6953 (1.6857)  loss_mask: 0.0624 (0.0899)  time: 0.1971  data: 0.0002  max mem: 5511
[07:15:41.365887] Epoch: [21]  [760/781]  eta: 0:00:04  lr: 0.000231  training_loss: 2.1699 (2.0919)  mae_loss: 0.3191 (0.3144)  classification_loss: 1.7173 (1.6866)  loss_mask: 0.1060 (0.0909)  time: 0.1978  data: 0.0003  max mem: 5511
[07:15:45.303385] Epoch: [21]  [780/781]  eta: 0:00:00  lr: 0.000231  training_loss: 2.1812 (2.0940)  mae_loss: 0.3121 (0.3147)  classification_loss: 1.7146 (1.6875)  loss_mask: 0.1318 (0.0918)  time: 0.1968  data: 0.0002  max mem: 5511
[07:15:45.462625] Epoch: [21] Total time: 0:02:35 (0.1988 s / it)
[07:15:45.463872] Averaged stats: lr: 0.000231  training_loss: 2.1812 (2.0940)  mae_loss: 0.3121 (0.3147)  classification_loss: 1.7146 (1.6875)  loss_mask: 0.1318 (0.0918)
[07:15:46.184924] Test:  [  0/157]  eta: 0:01:52  testing_loss: 0.9977 (0.9977)  acc1: 65.6250 (65.6250)  acc5: 90.6250 (90.6250)  time: 0.7155  data: 0.6859  max mem: 5511
[07:15:46.479688] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.1115 (1.0902)  acc1: 62.5000 (61.9318)  acc5: 96.8750 (96.5909)  time: 0.0916  data: 0.0630  max mem: 5511
[07:15:46.765095] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.0359 (1.0598)  acc1: 62.5000 (63.8393)  acc5: 96.8750 (96.9494)  time: 0.0288  data: 0.0005  max mem: 5511
[07:15:47.057644] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.0522 (1.0677)  acc1: 65.6250 (63.8105)  acc5: 96.8750 (96.7742)  time: 0.0285  data: 0.0002  max mem: 5511
[07:15:47.343492] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.0381 (1.0643)  acc1: 65.6250 (64.1387)  acc5: 96.8750 (96.6845)  time: 0.0286  data: 0.0003  max mem: 5511
[07:15:47.637225] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.0293 (1.0581)  acc1: 65.6250 (64.7365)  acc5: 96.8750 (96.6912)  time: 0.0288  data: 0.0003  max mem: 5511
[07:15:47.926553] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.0365 (1.0541)  acc1: 65.6250 (64.7285)  acc5: 96.8750 (96.5932)  time: 0.0290  data: 0.0004  max mem: 5511
[07:15:48.214538] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.0179 (1.0483)  acc1: 65.6250 (64.8548)  acc5: 96.8750 (96.6769)  time: 0.0287  data: 0.0004  max mem: 5511
[07:15:48.501937] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.0092 (1.0509)  acc1: 64.0625 (64.9113)  acc5: 96.8750 (96.6242)  time: 0.0286  data: 0.0002  max mem: 5511
[07:15:48.789584] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.0701 (1.0538)  acc1: 64.0625 (64.8180)  acc5: 96.8750 (96.7720)  time: 0.0285  data: 0.0002  max mem: 5511
[07:15:49.074411] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.0701 (1.0546)  acc1: 64.0625 (64.6658)  acc5: 98.4375 (96.8595)  time: 0.0284  data: 0.0002  max mem: 5511
[07:15:49.357783] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.0761 (1.0581)  acc1: 62.5000 (64.4426)  acc5: 96.8750 (96.9172)  time: 0.0283  data: 0.0002  max mem: 5511
[07:15:49.641235] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.0663 (1.0553)  acc1: 65.6250 (64.6178)  acc5: 96.8750 (96.8750)  time: 0.0282  data: 0.0002  max mem: 5511
[07:15:49.925696] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.0536 (1.0562)  acc1: 65.6250 (64.4680)  acc5: 96.8750 (96.8392)  time: 0.0283  data: 0.0002  max mem: 5511
[07:15:50.208242] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.0314 (1.0545)  acc1: 64.0625 (64.6387)  acc5: 96.8750 (96.8196)  time: 0.0282  data: 0.0002  max mem: 5511
[07:15:50.489512] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.0083 (1.0518)  acc1: 65.6250 (64.7454)  acc5: 96.8750 (96.8233)  time: 0.0281  data: 0.0001  max mem: 5511
[07:15:50.641920] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.9944 (1.0546)  acc1: 64.0625 (64.7200)  acc5: 96.8750 (96.7900)  time: 0.0272  data: 0.0001  max mem: 5511
[07:15:50.769961] Test: Total time: 0:00:05 (0.0338 s / it)
[07:15:50.770500] * Acc@1 64.720 Acc@5 96.790 loss 1.055
[07:15:50.770786] Accuracy of the network on the 10000 test images: 64.7%
[07:15:50.770962] Max accuracy: 65.28%
[07:15:50.889918] log_dir: ./output_dir
[07:15:51.724571] Epoch: [22]  [  0/781]  eta: 0:10:50  lr: 0.000231  training_loss: 2.0326 (2.0326)  mae_loss: 0.3261 (0.3261)  classification_loss: 1.5925 (1.5925)  loss_mask: 0.1140 (0.1140)  time: 0.8327  data: 0.6144  max mem: 5511
[07:15:55.673570] Epoch: [22]  [ 20/781]  eta: 0:02:53  lr: 0.000231  training_loss: 2.0438 (2.0437)  mae_loss: 0.3137 (0.3153)  classification_loss: 1.6432 (1.6490)  loss_mask: 0.0638 (0.0793)  time: 0.1973  data: 0.0002  max mem: 5511
[07:15:59.651314] Epoch: [22]  [ 40/781]  eta: 0:02:38  lr: 0.000231  training_loss: 2.1106 (2.0684)  mae_loss: 0.3088 (0.3149)  classification_loss: 1.6917 (1.6691)  loss_mask: 0.0787 (0.0844)  time: 0.1988  data: 0.0003  max mem: 5511
[07:16:03.584836] Epoch: [22]  [ 60/781]  eta: 0:02:29  lr: 0.000231  training_loss: 2.1879 (2.1093)  mae_loss: 0.3211 (0.3154)  classification_loss: 1.7279 (1.6854)  loss_mask: 0.1294 (0.1085)  time: 0.1966  data: 0.0002  max mem: 5511
[07:16:07.534106] Epoch: [22]  [ 80/781]  eta: 0:02:23  lr: 0.000231  training_loss: 2.1764 (2.1283)  mae_loss: 0.3204 (0.3183)  classification_loss: 1.7022 (1.6952)  loss_mask: 0.1188 (0.1148)  time: 0.1974  data: 0.0002  max mem: 5511
[07:16:11.471150] Epoch: [22]  [100/781]  eta: 0:02:18  lr: 0.000231  training_loss: 2.2205 (2.1584)  mae_loss: 0.3408 (0.3213)  classification_loss: 1.7200 (1.7030)  loss_mask: 0.1773 (0.1341)  time: 0.1968  data: 0.0002  max mem: 5511
[07:16:15.429524] Epoch: [22]  [120/781]  eta: 0:02:13  lr: 0.000231  training_loss: 2.2927 (2.1819)  mae_loss: 0.3240 (0.3215)  classification_loss: 1.7704 (1.7157)  loss_mask: 0.1866 (0.1447)  time: 0.1978  data: 0.0002  max mem: 5511
[07:16:19.363888] Epoch: [22]  [140/781]  eta: 0:02:09  lr: 0.000230  training_loss: 2.2805 (2.1916)  mae_loss: 0.3182 (0.3217)  classification_loss: 1.7793 (1.7241)  loss_mask: 0.1396 (0.1457)  time: 0.1966  data: 0.0003  max mem: 5511
[07:16:23.345466] Epoch: [22]  [160/781]  eta: 0:02:05  lr: 0.000230  training_loss: 2.2353 (2.1939)  mae_loss: 0.3313 (0.3219)  classification_loss: 1.7765 (1.7305)  loss_mask: 0.0879 (0.1416)  time: 0.1990  data: 0.0002  max mem: 5511
[07:16:27.300971] Epoch: [22]  [180/781]  eta: 0:02:00  lr: 0.000230  training_loss: 2.1468 (2.1894)  mae_loss: 0.3171 (0.3219)  classification_loss: 1.7119 (1.7313)  loss_mask: 0.0907 (0.1362)  time: 0.1977  data: 0.0004  max mem: 5511
[07:16:31.262072] Epoch: [22]  [200/781]  eta: 0:01:56  lr: 0.000230  training_loss: 2.0650 (2.1753)  mae_loss: 0.2961 (0.3199)  classification_loss: 1.6575 (1.7257)  loss_mask: 0.0751 (0.1297)  time: 0.1980  data: 0.0002  max mem: 5511
[07:16:35.206100] Epoch: [22]  [220/781]  eta: 0:01:52  lr: 0.000230  training_loss: 2.1586 (2.1723)  mae_loss: 0.3055 (0.3195)  classification_loss: 1.7217 (1.7248)  loss_mask: 0.1145 (0.1279)  time: 0.1971  data: 0.0002  max mem: 5511
[07:16:39.142382] Epoch: [22]  [240/781]  eta: 0:01:48  lr: 0.000230  training_loss: 2.0880 (2.1664)  mae_loss: 0.3115 (0.3189)  classification_loss: 1.6591 (1.7217)  loss_mask: 0.0942 (0.1258)  time: 0.1967  data: 0.0002  max mem: 5511
[07:16:43.114657] Epoch: [22]  [260/781]  eta: 0:01:44  lr: 0.000230  training_loss: 2.0638 (2.1581)  mae_loss: 0.3151 (0.3186)  classification_loss: 1.6598 (1.7176)  loss_mask: 0.0652 (0.1219)  time: 0.1985  data: 0.0002  max mem: 5511
[07:16:47.071662] Epoch: [22]  [280/781]  eta: 0:01:40  lr: 0.000230  training_loss: 2.0499 (2.1527)  mae_loss: 0.3159 (0.3188)  classification_loss: 1.6690 (1.7155)  loss_mask: 0.0645 (0.1185)  time: 0.1977  data: 0.0004  max mem: 5511
[07:16:51.030459] Epoch: [22]  [300/781]  eta: 0:01:36  lr: 0.000230  training_loss: 2.0886 (2.1479)  mae_loss: 0.3052 (0.3183)  classification_loss: 1.7067 (1.7155)  loss_mask: 0.0450 (0.1141)  time: 0.1979  data: 0.0002  max mem: 5511
[07:16:54.993218] Epoch: [22]  [320/781]  eta: 0:01:32  lr: 0.000230  training_loss: 2.0740 (2.1421)  mae_loss: 0.2962 (0.3171)  classification_loss: 1.6824 (1.7134)  loss_mask: 0.0603 (0.1115)  time: 0.1980  data: 0.0002  max mem: 5511
[07:16:58.936244] Epoch: [22]  [340/781]  eta: 0:01:27  lr: 0.000230  training_loss: 2.0507 (2.1360)  mae_loss: 0.3110 (0.3168)  classification_loss: 1.6598 (1.7107)  loss_mask: 0.0466 (0.1085)  time: 0.1971  data: 0.0002  max mem: 5511
[07:17:02.886585] Epoch: [22]  [360/781]  eta: 0:01:23  lr: 0.000230  training_loss: 2.0205 (2.1301)  mae_loss: 0.2931 (0.3157)  classification_loss: 1.6752 (1.7082)  loss_mask: 0.0610 (0.1062)  time: 0.1974  data: 0.0002  max mem: 5511
[07:17:06.856926] Epoch: [22]  [380/781]  eta: 0:01:19  lr: 0.000230  training_loss: 1.9912 (2.1255)  mae_loss: 0.3014 (0.3157)  classification_loss: 1.6372 (1.7059)  loss_mask: 0.0459 (0.1039)  time: 0.1984  data: 0.0002  max mem: 5511
[07:17:10.801812] Epoch: [22]  [400/781]  eta: 0:01:15  lr: 0.000230  training_loss: 2.0932 (2.1258)  mae_loss: 0.3124 (0.3158)  classification_loss: 1.6593 (1.7035)  loss_mask: 0.0883 (0.1065)  time: 0.1971  data: 0.0002  max mem: 5511
[07:17:14.741241] Epoch: [22]  [420/781]  eta: 0:01:11  lr: 0.000230  training_loss: 2.0662 (2.1231)  mae_loss: 0.3181 (0.3160)  classification_loss: 1.6745 (1.7016)  loss_mask: 0.0775 (0.1054)  time: 0.1969  data: 0.0003  max mem: 5511
[07:17:18.680043] Epoch: [22]  [440/781]  eta: 0:01:07  lr: 0.000230  training_loss: 2.0405 (2.1199)  mae_loss: 0.3157 (0.3162)  classification_loss: 1.6638 (1.6991)  loss_mask: 0.0691 (0.1046)  time: 0.1968  data: 0.0003  max mem: 5511
[07:17:22.616337] Epoch: [22]  [460/781]  eta: 0:01:03  lr: 0.000230  training_loss: 2.0179 (2.1163)  mae_loss: 0.3140 (0.3163)  classification_loss: 1.6554 (1.6975)  loss_mask: 0.0369 (0.1025)  time: 0.1967  data: 0.0002  max mem: 5511
[07:17:26.553215] Epoch: [22]  [480/781]  eta: 0:00:59  lr: 0.000229  training_loss: 1.9815 (2.1119)  mae_loss: 0.3022 (0.3155)  classification_loss: 1.6748 (1.6966)  loss_mask: 0.0289 (0.0998)  time: 0.1968  data: 0.0002  max mem: 5511
[07:17:30.499995] Epoch: [22]  [500/781]  eta: 0:00:55  lr: 0.000229  training_loss: 1.9668 (2.1077)  mae_loss: 0.2964 (0.3147)  classification_loss: 1.6127 (1.6946)  loss_mask: 0.0434 (0.0984)  time: 0.1973  data: 0.0002  max mem: 5511
[07:17:34.462448] Epoch: [22]  [520/781]  eta: 0:00:51  lr: 0.000229  training_loss: 1.9911 (2.1040)  mae_loss: 0.3242 (0.3148)  classification_loss: 1.6280 (1.6924)  loss_mask: 0.0408 (0.0968)  time: 0.1980  data: 0.0004  max mem: 5511
[07:17:38.412424] Epoch: [22]  [540/781]  eta: 0:00:47  lr: 0.000229  training_loss: 2.0819 (2.1021)  mae_loss: 0.3038 (0.3144)  classification_loss: 1.6857 (1.6916)  loss_mask: 0.0635 (0.0961)  time: 0.1974  data: 0.0002  max mem: 5511
[07:17:42.403222] Epoch: [22]  [560/781]  eta: 0:00:43  lr: 0.000229  training_loss: 2.0384 (2.1007)  mae_loss: 0.3186 (0.3146)  classification_loss: 1.6338 (1.6903)  loss_mask: 0.0598 (0.0959)  time: 0.1994  data: 0.0003  max mem: 5511
[07:17:46.349808] Epoch: [22]  [580/781]  eta: 0:00:39  lr: 0.000229  training_loss: 2.0192 (2.0976)  mae_loss: 0.2830 (0.3136)  classification_loss: 1.6441 (1.6889)  loss_mask: 0.0609 (0.0950)  time: 0.1972  data: 0.0002  max mem: 5511
[07:17:50.298940] Epoch: [22]  [600/781]  eta: 0:00:35  lr: 0.000229  training_loss: 2.0488 (2.0963)  mae_loss: 0.3137 (0.3136)  classification_loss: 1.6750 (1.6883)  loss_mask: 0.0617 (0.0944)  time: 0.1974  data: 0.0002  max mem: 5511
[07:17:54.290334] Epoch: [22]  [620/781]  eta: 0:00:31  lr: 0.000229  training_loss: 1.9795 (2.0935)  mae_loss: 0.3021 (0.3133)  classification_loss: 1.6273 (1.6869)  loss_mask: 0.0578 (0.0933)  time: 0.1995  data: 0.0002  max mem: 5511
[07:17:58.271648] Epoch: [22]  [640/781]  eta: 0:00:28  lr: 0.000229  training_loss: 2.0352 (2.0926)  mae_loss: 0.2983 (0.3130)  classification_loss: 1.6612 (1.6857)  loss_mask: 0.0840 (0.0938)  time: 0.1990  data: 0.0002  max mem: 5511
[07:18:02.203369] Epoch: [22]  [660/781]  eta: 0:00:24  lr: 0.000229  training_loss: 2.0890 (2.0927)  mae_loss: 0.3131 (0.3129)  classification_loss: 1.6437 (1.6852)  loss_mask: 0.0960 (0.0947)  time: 0.1965  data: 0.0002  max mem: 5511
[07:18:06.157790] Epoch: [22]  [680/781]  eta: 0:00:20  lr: 0.000229  training_loss: 2.0339 (2.0905)  mae_loss: 0.3053 (0.3126)  classification_loss: 1.6751 (1.6840)  loss_mask: 0.0572 (0.0939)  time: 0.1976  data: 0.0002  max mem: 5511
[07:18:10.109993] Epoch: [22]  [700/781]  eta: 0:00:16  lr: 0.000229  training_loss: 2.0295 (2.0887)  mae_loss: 0.3119 (0.3125)  classification_loss: 1.6683 (1.6832)  loss_mask: 0.0643 (0.0930)  time: 0.1975  data: 0.0003  max mem: 5511
[07:18:14.061318] Epoch: [22]  [720/781]  eta: 0:00:12  lr: 0.000229  training_loss: 1.9700 (2.0857)  mae_loss: 0.3023 (0.3124)  classification_loss: 1.6218 (1.6819)  loss_mask: 0.0291 (0.0914)  time: 0.1975  data: 0.0003  max mem: 5511
[07:18:17.976988] Epoch: [22]  [740/781]  eta: 0:00:08  lr: 0.000229  training_loss: 1.9951 (2.0834)  mae_loss: 0.2820 (0.3119)  classification_loss: 1.6505 (1.6811)  loss_mask: 0.0394 (0.0904)  time: 0.1957  data: 0.0002  max mem: 5511
[07:18:21.918664] Epoch: [22]  [760/781]  eta: 0:00:04  lr: 0.000229  training_loss: 2.0287 (2.0815)  mae_loss: 0.3106 (0.3118)  classification_loss: 1.6324 (1.6804)  loss_mask: 0.0463 (0.0893)  time: 0.1970  data: 0.0003  max mem: 5511
[07:18:25.853135] Epoch: [22]  [780/781]  eta: 0:00:00  lr: 0.000229  training_loss: 2.1832 (2.0844)  mae_loss: 0.3145 (0.3121)  classification_loss: 1.6947 (1.6812)  loss_mask: 0.1171 (0.0911)  time: 0.1966  data: 0.0002  max mem: 5511
[07:18:26.036056] Epoch: [22] Total time: 0:02:35 (0.1986 s / it)
[07:18:26.036519] Averaged stats: lr: 0.000229  training_loss: 2.1832 (2.0844)  mae_loss: 0.3145 (0.3121)  classification_loss: 1.6947 (1.6812)  loss_mask: 0.1171 (0.0911)
[07:18:26.625327] Test:  [  0/157]  eta: 0:01:31  testing_loss: 0.9240 (0.9240)  acc1: 65.6250 (65.6250)  acc5: 98.4375 (98.4375)  time: 0.5843  data: 0.5547  max mem: 5511
[07:18:26.921724] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 1.0448 (1.0663)  acc1: 62.5000 (61.7898)  acc5: 98.4375 (97.8693)  time: 0.0799  data: 0.0508  max mem: 5511
[07:18:27.211081] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.0164 (1.0259)  acc1: 64.0625 (63.9881)  acc5: 98.4375 (97.5446)  time: 0.0291  data: 0.0004  max mem: 5511
[07:18:27.494432] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 1.0261 (1.0384)  acc1: 65.6250 (64.2137)  acc5: 96.8750 (97.2782)  time: 0.0285  data: 0.0003  max mem: 5511
[07:18:27.779074] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 1.0228 (1.0338)  acc1: 64.0625 (64.4436)  acc5: 96.8750 (97.1037)  time: 0.0283  data: 0.0002  max mem: 5511
[07:18:28.068246] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.0002 (1.0298)  acc1: 65.6250 (65.1348)  acc5: 96.8750 (97.0895)  time: 0.0286  data: 0.0002  max mem: 5511
[07:18:28.357762] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.0098 (1.0263)  acc1: 67.1875 (65.4457)  acc5: 96.8750 (97.0543)  time: 0.0288  data: 0.0002  max mem: 5511
[07:18:28.643560] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.9493 (1.0155)  acc1: 67.1875 (65.8891)  acc5: 96.8750 (97.1171)  time: 0.0286  data: 0.0002  max mem: 5511
[07:18:28.928477] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.9487 (1.0160)  acc1: 67.1875 (65.9144)  acc5: 96.8750 (96.9522)  time: 0.0284  data: 0.0002  max mem: 5511
[07:18:29.215646] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.0401 (1.0205)  acc1: 64.0625 (65.6937)  acc5: 96.8750 (96.9952)  time: 0.0285  data: 0.0002  max mem: 5511
[07:18:29.500744] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.0648 (1.0271)  acc1: 62.5000 (65.3001)  acc5: 96.8750 (96.9833)  time: 0.0285  data: 0.0002  max mem: 5511
[07:18:29.784585] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.0654 (1.0293)  acc1: 62.5000 (65.1745)  acc5: 96.8750 (96.9595)  time: 0.0283  data: 0.0002  max mem: 5511
[07:18:30.071175] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.9917 (1.0230)  acc1: 65.6250 (65.4571)  acc5: 96.8750 (97.0558)  time: 0.0283  data: 0.0002  max mem: 5511
[07:18:30.355499] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.9893 (1.0245)  acc1: 67.1875 (65.3745)  acc5: 98.4375 (97.0897)  time: 0.0284  data: 0.0002  max mem: 5511
[07:18:30.641258] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.0364 (1.0226)  acc1: 64.0625 (65.5918)  acc5: 98.4375 (97.1520)  time: 0.0283  data: 0.0002  max mem: 5511
[07:18:30.924489] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.9907 (1.0197)  acc1: 67.1875 (65.7078)  acc5: 98.4375 (97.1544)  time: 0.0282  data: 0.0001  max mem: 5511
[07:18:31.075644] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.9907 (1.0233)  acc1: 67.1875 (65.6200)  acc5: 96.8750 (97.1400)  time: 0.0273  data: 0.0001  max mem: 5511
[07:18:31.250504] Test: Total time: 0:00:05 (0.0332 s / it)
[07:18:31.250972] * Acc@1 65.620 Acc@5 97.140 loss 1.023
[07:18:31.251280] Accuracy of the network on the 10000 test images: 65.6%
[07:18:31.251608] Max accuracy: 65.62%
[07:18:31.461047] log_dir: ./output_dir
[07:18:32.292473] Epoch: [23]  [  0/781]  eta: 0:10:47  lr: 0.000229  training_loss: 2.2010 (2.2010)  mae_loss: 0.3454 (0.3454)  classification_loss: 1.6818 (1.6818)  loss_mask: 0.1739 (0.1739)  time: 0.8296  data: 0.5701  max mem: 5511
[07:18:36.237038] Epoch: [23]  [ 20/781]  eta: 0:02:52  lr: 0.000229  training_loss: 2.1240 (2.1310)  mae_loss: 0.3280 (0.3210)  classification_loss: 1.6720 (1.6912)  loss_mask: 0.0962 (0.1188)  time: 0.1971  data: 0.0002  max mem: 5511
[07:18:40.279030] Epoch: [23]  [ 40/781]  eta: 0:02:39  lr: 0.000228  training_loss: 2.1269 (2.1249)  mae_loss: 0.3135 (0.3200)  classification_loss: 1.6976 (1.6894)  loss_mask: 0.0813 (0.1155)  time: 0.2020  data: 0.0002  max mem: 5511
[07:18:44.223939] Epoch: [23]  [ 60/781]  eta: 0:02:30  lr: 0.000228  training_loss: 2.1049 (2.1208)  mae_loss: 0.3207 (0.3202)  classification_loss: 1.7291 (1.6998)  loss_mask: 0.0470 (0.1009)  time: 0.1972  data: 0.0002  max mem: 5511
[07:18:48.153597] Epoch: [23]  [ 80/781]  eta: 0:02:24  lr: 0.000228  training_loss: 2.0905 (2.1164)  mae_loss: 0.3173 (0.3204)  classification_loss: 1.6556 (1.6942)  loss_mask: 0.0824 (0.1018)  time: 0.1964  data: 0.0002  max mem: 5511
[07:18:52.112167] Epoch: [23]  [100/781]  eta: 0:02:19  lr: 0.000228  training_loss: 2.1347 (2.1158)  mae_loss: 0.3022 (0.3183)  classification_loss: 1.6544 (1.6931)  loss_mask: 0.1010 (0.1045)  time: 0.1978  data: 0.0002  max mem: 5511
[07:18:56.072065] Epoch: [23]  [120/781]  eta: 0:02:14  lr: 0.000228  training_loss: 2.0323 (2.1018)  mae_loss: 0.2855 (0.3136)  classification_loss: 1.6334 (1.6884)  loss_mask: 0.0722 (0.0998)  time: 0.1979  data: 0.0002  max mem: 5511
[07:19:00.013400] Epoch: [23]  [140/781]  eta: 0:02:09  lr: 0.000228  training_loss: 2.0541 (2.0919)  mae_loss: 0.3136 (0.3131)  classification_loss: 1.6262 (1.6808)  loss_mask: 0.0761 (0.0980)  time: 0.1970  data: 0.0002  max mem: 5511
[07:19:03.968816] Epoch: [23]  [160/781]  eta: 0:02:05  lr: 0.000228  training_loss: 2.0553 (2.0873)  mae_loss: 0.2966 (0.3121)  classification_loss: 1.6621 (1.6822)  loss_mask: 0.0483 (0.0931)  time: 0.1977  data: 0.0002  max mem: 5511
[07:19:07.922392] Epoch: [23]  [180/781]  eta: 0:02:01  lr: 0.000228  training_loss: 2.0141 (2.0812)  mae_loss: 0.3088 (0.3123)  classification_loss: 1.6834 (1.6812)  loss_mask: 0.0333 (0.0877)  time: 0.1976  data: 0.0002  max mem: 5511
[07:19:11.882150] Epoch: [23]  [200/781]  eta: 0:01:56  lr: 0.000228  training_loss: 1.9994 (2.0728)  mae_loss: 0.2832 (0.3105)  classification_loss: 1.6490 (1.6776)  loss_mask: 0.0462 (0.0847)  time: 0.1979  data: 0.0003  max mem: 5511
[07:19:15.833497] Epoch: [23]  [220/781]  eta: 0:01:52  lr: 0.000228  training_loss: 1.9947 (2.0674)  mae_loss: 0.2984 (0.3093)  classification_loss: 1.6508 (1.6759)  loss_mask: 0.0456 (0.0822)  time: 0.1974  data: 0.0002  max mem: 5511
[07:19:19.794713] Epoch: [23]  [240/781]  eta: 0:01:48  lr: 0.000228  training_loss: 2.0250 (2.0647)  mae_loss: 0.3091 (0.3088)  classification_loss: 1.6342 (1.6755)  loss_mask: 0.0400 (0.0803)  time: 0.1979  data: 0.0003  max mem: 5511
[07:19:23.737488] Epoch: [23]  [260/781]  eta: 0:01:44  lr: 0.000228  training_loss: 2.0036 (2.0583)  mae_loss: 0.2912 (0.3082)  classification_loss: 1.6111 (1.6725)  loss_mask: 0.0398 (0.0775)  time: 0.1971  data: 0.0004  max mem: 5511
[07:19:27.702735] Epoch: [23]  [280/781]  eta: 0:01:40  lr: 0.000228  training_loss: 2.0078 (2.0577)  mae_loss: 0.3051 (0.3085)  classification_loss: 1.6644 (1.6708)  loss_mask: 0.0699 (0.0784)  time: 0.1982  data: 0.0002  max mem: 5511
[07:19:31.636464] Epoch: [23]  [300/781]  eta: 0:01:36  lr: 0.000228  training_loss: 2.0669 (2.0591)  mae_loss: 0.2993 (0.3086)  classification_loss: 1.6708 (1.6720)  loss_mask: 0.0564 (0.0785)  time: 0.1966  data: 0.0002  max mem: 5511
[07:19:35.607578] Epoch: [23]  [320/781]  eta: 0:01:32  lr: 0.000228  training_loss: 2.0500 (2.0584)  mae_loss: 0.3116 (0.3090)  classification_loss: 1.6808 (1.6721)  loss_mask: 0.0471 (0.0773)  time: 0.1984  data: 0.0004  max mem: 5511
[07:19:39.568818] Epoch: [23]  [340/781]  eta: 0:01:28  lr: 0.000228  training_loss: 1.9965 (2.0551)  mae_loss: 0.3047 (0.3087)  classification_loss: 1.6339 (1.6708)  loss_mask: 0.0418 (0.0757)  time: 0.1980  data: 0.0002  max mem: 5511
[07:19:43.532230] Epoch: [23]  [360/781]  eta: 0:01:24  lr: 0.000228  training_loss: 2.0679 (2.0551)  mae_loss: 0.3245 (0.3096)  classification_loss: 1.6404 (1.6704)  loss_mask: 0.0391 (0.0751)  time: 0.1981  data: 0.0002  max mem: 5511
[07:19:47.519109] Epoch: [23]  [380/781]  eta: 0:01:20  lr: 0.000227  training_loss: 2.0522 (2.0562)  mae_loss: 0.3243 (0.3102)  classification_loss: 1.6285 (1.6701)  loss_mask: 0.0866 (0.0760)  time: 0.1993  data: 0.0003  max mem: 5511
[07:19:51.485572] Epoch: [23]  [400/781]  eta: 0:01:15  lr: 0.000227  training_loss: 1.9808 (2.0548)  mae_loss: 0.3059 (0.3099)  classification_loss: 1.6076 (1.6687)  loss_mask: 0.0719 (0.0761)  time: 0.1982  data: 0.0003  max mem: 5511
[07:19:55.447127] Epoch: [23]  [420/781]  eta: 0:01:11  lr: 0.000227  training_loss: 2.0741 (2.0570)  mae_loss: 0.3008 (0.3097)  classification_loss: 1.6937 (1.6696)  loss_mask: 0.0873 (0.0778)  time: 0.1980  data: 0.0003  max mem: 5511
[07:19:59.374527] Epoch: [23]  [440/781]  eta: 0:01:07  lr: 0.000227  training_loss: 2.1430 (2.0621)  mae_loss: 0.3041 (0.3093)  classification_loss: 1.6888 (1.6711)  loss_mask: 0.1501 (0.0817)  time: 0.1963  data: 0.0002  max mem: 5511
[07:20:03.382869] Epoch: [23]  [460/781]  eta: 0:01:03  lr: 0.000227  training_loss: 2.1163 (2.0650)  mae_loss: 0.3104 (0.3091)  classification_loss: 1.7201 (1.6724)  loss_mask: 0.1010 (0.0835)  time: 0.2003  data: 0.0003  max mem: 5511
[07:20:07.349449] Epoch: [23]  [480/781]  eta: 0:00:59  lr: 0.000227  training_loss: 2.1152 (2.0662)  mae_loss: 0.3119 (0.3091)  classification_loss: 1.6909 (1.6725)  loss_mask: 0.1033 (0.0846)  time: 0.1982  data: 0.0005  max mem: 5511
[07:20:11.309480] Epoch: [23]  [500/781]  eta: 0:00:55  lr: 0.000227  training_loss: 2.1369 (2.0701)  mae_loss: 0.2959 (0.3090)  classification_loss: 1.7394 (1.6750)  loss_mask: 0.0956 (0.0862)  time: 0.1979  data: 0.0002  max mem: 5511
[07:20:15.247484] Epoch: [23]  [520/781]  eta: 0:00:51  lr: 0.000227  training_loss: 2.1740 (2.0747)  mae_loss: 0.3141 (0.3090)  classification_loss: 1.7152 (1.6767)  loss_mask: 0.1524 (0.0890)  time: 0.1968  data: 0.0003  max mem: 5511
[07:20:19.185956] Epoch: [23]  [540/781]  eta: 0:00:47  lr: 0.000227  training_loss: 2.0865 (2.0759)  mae_loss: 0.3147 (0.3093)  classification_loss: 1.6743 (1.6770)  loss_mask: 0.0989 (0.0896)  time: 0.1968  data: 0.0003  max mem: 5511
[07:20:23.144285] Epoch: [23]  [560/781]  eta: 0:00:43  lr: 0.000227  training_loss: 2.0743 (2.0769)  mae_loss: 0.2974 (0.3095)  classification_loss: 1.6845 (1.6777)  loss_mask: 0.0882 (0.0898)  time: 0.1978  data: 0.0002  max mem: 5511
[07:20:27.140983] Epoch: [23]  [580/781]  eta: 0:00:40  lr: 0.000227  training_loss: 2.0579 (2.0762)  mae_loss: 0.3086 (0.3094)  classification_loss: 1.6790 (1.6770)  loss_mask: 0.0736 (0.0898)  time: 0.1998  data: 0.0003  max mem: 5511
[07:20:31.075372] Epoch: [23]  [600/781]  eta: 0:00:36  lr: 0.000227  training_loss: 2.0266 (2.0760)  mae_loss: 0.3028 (0.3092)  classification_loss: 1.6299 (1.6762)  loss_mask: 0.0762 (0.0906)  time: 0.1966  data: 0.0002  max mem: 5511
[07:20:35.032863] Epoch: [23]  [620/781]  eta: 0:00:32  lr: 0.000227  training_loss: 2.2292 (2.0805)  mae_loss: 0.3316 (0.3098)  classification_loss: 1.6987 (1.6771)  loss_mask: 0.1817 (0.0936)  time: 0.1978  data: 0.0002  max mem: 5511
[07:20:38.985955] Epoch: [23]  [640/781]  eta: 0:00:28  lr: 0.000227  training_loss: 2.1299 (2.0828)  mae_loss: 0.3191 (0.3097)  classification_loss: 1.6986 (1.6777)  loss_mask: 0.1370 (0.0953)  time: 0.1976  data: 0.0002  max mem: 5511
[07:20:42.928310] Epoch: [23]  [660/781]  eta: 0:00:24  lr: 0.000227  training_loss: 2.1964 (2.0867)  mae_loss: 0.3164 (0.3099)  classification_loss: 1.7413 (1.6794)  loss_mask: 0.1520 (0.0974)  time: 0.1970  data: 0.0002  max mem: 5511
[07:20:46.865417] Epoch: [23]  [680/781]  eta: 0:00:20  lr: 0.000227  training_loss: 2.1396 (2.0890)  mae_loss: 0.3159 (0.3100)  classification_loss: 1.7211 (1.6807)  loss_mask: 0.1047 (0.0983)  time: 0.1968  data: 0.0003  max mem: 5511
[07:20:50.797682] Epoch: [23]  [700/781]  eta: 0:00:16  lr: 0.000226  training_loss: 2.1690 (2.0913)  mae_loss: 0.3275 (0.3106)  classification_loss: 1.6460 (1.6809)  loss_mask: 0.1284 (0.0999)  time: 0.1965  data: 0.0002  max mem: 5511
[07:20:54.758528] Epoch: [23]  [720/781]  eta: 0:00:12  lr: 0.000226  training_loss: 2.0941 (2.0914)  mae_loss: 0.3030 (0.3104)  classification_loss: 1.6916 (1.6811)  loss_mask: 0.0809 (0.0999)  time: 0.1980  data: 0.0002  max mem: 5511
[07:20:58.715929] Epoch: [23]  [740/781]  eta: 0:00:08  lr: 0.000226  training_loss: 2.0287 (2.0900)  mae_loss: 0.3025 (0.3102)  classification_loss: 1.6503 (1.6804)  loss_mask: 0.0730 (0.0995)  time: 0.1977  data: 0.0002  max mem: 5511
[07:21:02.662382] Epoch: [23]  [760/781]  eta: 0:00:04  lr: 0.000226  training_loss: 2.0305 (2.0890)  mae_loss: 0.3098 (0.3101)  classification_loss: 1.6478 (1.6802)  loss_mask: 0.0632 (0.0987)  time: 0.1971  data: 0.0002  max mem: 5511
[07:21:06.587912] Epoch: [23]  [780/781]  eta: 0:00:00  lr: 0.000226  training_loss: 2.0517 (2.0886)  mae_loss: 0.3241 (0.3104)  classification_loss: 1.6853 (1.6803)  loss_mask: 0.0614 (0.0979)  time: 0.1962  data: 0.0002  max mem: 5511
[07:21:06.752812] Epoch: [23] Total time: 0:02:35 (0.1988 s / it)
[07:21:06.753710] Averaged stats: lr: 0.000226  training_loss: 2.0517 (2.0886)  mae_loss: 0.3241 (0.3104)  classification_loss: 1.6853 (1.6803)  loss_mask: 0.0614 (0.0979)
[07:21:07.340790] Test:  [  0/157]  eta: 0:01:31  testing_loss: 0.9734 (0.9734)  acc1: 73.4375 (73.4375)  acc5: 93.7500 (93.7500)  time: 0.5812  data: 0.5517  max mem: 5511
[07:21:07.627561] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 1.0216 (1.0522)  acc1: 65.6250 (64.2045)  acc5: 96.8750 (96.8750)  time: 0.0787  data: 0.0503  max mem: 5511
[07:21:07.911364] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.0259 (1.0244)  acc1: 64.0625 (65.9226)  acc5: 96.8750 (97.3214)  time: 0.0283  data: 0.0002  max mem: 5511
[07:21:08.194077] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 1.0255 (1.0314)  acc1: 64.0625 (65.5242)  acc5: 96.8750 (97.1270)  time: 0.0282  data: 0.0002  max mem: 5511
[07:21:08.477157] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 1.0115 (1.0280)  acc1: 65.6250 (65.6250)  acc5: 96.8750 (97.0274)  time: 0.0282  data: 0.0002  max mem: 5511
[07:21:08.762440] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.0008 (1.0207)  acc1: 67.1875 (66.2684)  acc5: 96.8750 (96.9363)  time: 0.0283  data: 0.0002  max mem: 5511
[07:21:09.047310] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.0023 (1.0170)  acc1: 67.1875 (66.0605)  acc5: 96.8750 (96.9775)  time: 0.0284  data: 0.0002  max mem: 5511
[07:21:09.334629] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.9541 (1.0075)  acc1: 67.1875 (66.3732)  acc5: 96.8750 (97.0070)  time: 0.0285  data: 0.0002  max mem: 5511
[07:21:09.620311] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.9513 (1.0118)  acc1: 68.7500 (66.1073)  acc5: 96.8750 (97.0486)  time: 0.0285  data: 0.0002  max mem: 5511
[07:21:09.904531] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.0200 (1.0133)  acc1: 67.1875 (66.2260)  acc5: 96.8750 (97.1497)  time: 0.0283  data: 0.0002  max mem: 5511
[07:21:10.188545] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.0339 (1.0174)  acc1: 64.0625 (65.9344)  acc5: 96.8750 (97.1380)  time: 0.0283  data: 0.0002  max mem: 5511
[07:21:10.471930] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.0364 (1.0188)  acc1: 64.0625 (65.8643)  acc5: 95.3125 (97.1002)  time: 0.0282  data: 0.0002  max mem: 5511
[07:21:10.755919] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.9886 (1.0137)  acc1: 67.1875 (66.1028)  acc5: 98.4375 (97.2237)  time: 0.0282  data: 0.0002  max mem: 5511
[07:21:11.040761] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.9886 (1.0153)  acc1: 67.1875 (65.9828)  acc5: 98.4375 (97.2328)  time: 0.0283  data: 0.0002  max mem: 5511
[07:21:11.329620] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.0603 (1.0167)  acc1: 65.6250 (66.0239)  acc5: 96.8750 (97.1742)  time: 0.0286  data: 0.0003  max mem: 5511
[07:21:11.613008] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.9985 (1.0135)  acc1: 65.6250 (66.1010)  acc5: 96.8750 (97.1440)  time: 0.0285  data: 0.0002  max mem: 5511
[07:21:11.763692] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.9838 (1.0165)  acc1: 67.1875 (66.1100)  acc5: 96.8750 (97.1500)  time: 0.0272  data: 0.0001  max mem: 5511
[07:21:11.946216] Test: Total time: 0:00:05 (0.0330 s / it)
[07:21:11.946794] * Acc@1 66.110 Acc@5 97.150 loss 1.017
[07:21:11.947133] Accuracy of the network on the 10000 test images: 66.1%
[07:21:11.947416] Max accuracy: 66.11%
[07:21:12.221677] log_dir: ./output_dir
[07:21:13.044648] Epoch: [24]  [  0/781]  eta: 0:10:41  lr: 0.000226  training_loss: 1.9302 (1.9302)  mae_loss: 0.3293 (0.3293)  classification_loss: 1.5764 (1.5764)  loss_mask: 0.0245 (0.0245)  time: 0.8211  data: 0.6084  max mem: 5511
[07:21:16.997555] Epoch: [24]  [ 20/781]  eta: 0:02:52  lr: 0.000226  training_loss: 2.0048 (2.0359)  mae_loss: 0.2910 (0.3000)  classification_loss: 1.6065 (1.6415)  loss_mask: 0.0784 (0.0945)  time: 0.1975  data: 0.0002  max mem: 5511
[07:21:20.923652] Epoch: [24]  [ 40/781]  eta: 0:02:37  lr: 0.000226  training_loss: 2.1270 (2.0659)  mae_loss: 0.3007 (0.3028)  classification_loss: 1.6535 (1.6524)  loss_mask: 0.1075 (0.1107)  time: 0.1962  data: 0.0002  max mem: 5511
[07:21:24.871492] Epoch: [24]  [ 60/781]  eta: 0:02:29  lr: 0.000226  training_loss: 2.0280 (2.0604)  mae_loss: 0.2934 (0.3015)  classification_loss: 1.6375 (1.6552)  loss_mask: 0.0813 (0.1037)  time: 0.1973  data: 0.0002  max mem: 5511
[07:21:28.824769] Epoch: [24]  [ 80/781]  eta: 0:02:23  lr: 0.000226  training_loss: 2.0161 (2.0534)  mae_loss: 0.3132 (0.3052)  classification_loss: 1.6458 (1.6554)  loss_mask: 0.0538 (0.0928)  time: 0.1976  data: 0.0002  max mem: 5511
[07:21:32.761918] Epoch: [24]  [100/781]  eta: 0:02:18  lr: 0.000226  training_loss: 2.0499 (2.0537)  mae_loss: 0.2989 (0.3043)  classification_loss: 1.6408 (1.6598)  loss_mask: 0.0595 (0.0896)  time: 0.1967  data: 0.0002  max mem: 5511
[07:21:36.708236] Epoch: [24]  [120/781]  eta: 0:02:13  lr: 0.000226  training_loss: 2.0350 (2.0493)  mae_loss: 0.2971 (0.3033)  classification_loss: 1.6477 (1.6582)  loss_mask: 0.0592 (0.0878)  time: 0.1972  data: 0.0002  max mem: 5511
[07:21:40.675458] Epoch: [24]  [140/781]  eta: 0:02:09  lr: 0.000226  training_loss: 2.0353 (2.0493)  mae_loss: 0.3089 (0.3042)  classification_loss: 1.6753 (1.6579)  loss_mask: 0.0650 (0.0872)  time: 0.1982  data: 0.0002  max mem: 5511
[07:21:44.625338] Epoch: [24]  [160/781]  eta: 0:02:04  lr: 0.000226  training_loss: 1.9876 (2.0424)  mae_loss: 0.2930 (0.3042)  classification_loss: 1.6133 (1.6541)  loss_mask: 0.0525 (0.0840)  time: 0.1974  data: 0.0002  max mem: 5511
[07:21:48.584033] Epoch: [24]  [180/781]  eta: 0:02:00  lr: 0.000226  training_loss: 1.9477 (2.0376)  mae_loss: 0.3084 (0.3060)  classification_loss: 1.6091 (1.6506)  loss_mask: 0.0433 (0.0810)  time: 0.1978  data: 0.0005  max mem: 5511
[07:21:52.549492] Epoch: [24]  [200/781]  eta: 0:01:56  lr: 0.000226  training_loss: 1.9384 (2.0308)  mae_loss: 0.2900 (0.3043)  classification_loss: 1.6158 (1.6485)  loss_mask: 0.0291 (0.0780)  time: 0.1982  data: 0.0002  max mem: 5511
[07:21:56.481554] Epoch: [24]  [220/781]  eta: 0:01:52  lr: 0.000226  training_loss: 2.0827 (2.0376)  mae_loss: 0.3139 (0.3047)  classification_loss: 1.7113 (1.6539)  loss_mask: 0.0633 (0.0789)  time: 0.1965  data: 0.0003  max mem: 5511
[07:22:00.434161] Epoch: [24]  [240/781]  eta: 0:01:48  lr: 0.000225  training_loss: 2.0464 (2.0394)  mae_loss: 0.2935 (0.3043)  classification_loss: 1.6398 (1.6548)  loss_mask: 0.0817 (0.0802)  time: 0.1976  data: 0.0002  max mem: 5511
[07:22:04.401680] Epoch: [24]  [260/781]  eta: 0:01:44  lr: 0.000225  training_loss: 2.0768 (2.0421)  mae_loss: 0.2981 (0.3038)  classification_loss: 1.6529 (1.6536)  loss_mask: 0.1087 (0.0848)  time: 0.1983  data: 0.0002  max mem: 5511
[07:22:08.342470] Epoch: [24]  [280/781]  eta: 0:01:40  lr: 0.000225  training_loss: 2.0354 (2.0431)  mae_loss: 0.3055 (0.3041)  classification_loss: 1.6038 (1.6517)  loss_mask: 0.1097 (0.0873)  time: 0.1970  data: 0.0002  max mem: 5511
[07:22:12.276223] Epoch: [24]  [300/781]  eta: 0:01:35  lr: 0.000225  training_loss: 2.0430 (2.0432)  mae_loss: 0.3051 (0.3042)  classification_loss: 1.6705 (1.6526)  loss_mask: 0.0672 (0.0863)  time: 0.1966  data: 0.0003  max mem: 5511
[07:22:16.241464] Epoch: [24]  [320/781]  eta: 0:01:31  lr: 0.000225  training_loss: 2.0334 (2.0423)  mae_loss: 0.3022 (0.3039)  classification_loss: 1.6676 (1.6544)  loss_mask: 0.0459 (0.0840)  time: 0.1982  data: 0.0002  max mem: 5511
[07:22:20.190753] Epoch: [24]  [340/781]  eta: 0:01:27  lr: 0.000225  training_loss: 2.0425 (2.0436)  mae_loss: 0.3154 (0.3044)  classification_loss: 1.6730 (1.6548)  loss_mask: 0.0728 (0.0845)  time: 0.1974  data: 0.0002  max mem: 5511
[07:22:24.166198] Epoch: [24]  [360/781]  eta: 0:01:23  lr: 0.000225  training_loss: 2.1038 (2.0483)  mae_loss: 0.3020 (0.3044)  classification_loss: 1.6342 (1.6540)  loss_mask: 0.1362 (0.0900)  time: 0.1987  data: 0.0002  max mem: 5511
[07:22:28.127842] Epoch: [24]  [380/781]  eta: 0:01:19  lr: 0.000225  training_loss: 2.1353 (2.0556)  mae_loss: 0.3157 (0.3047)  classification_loss: 1.6694 (1.6552)  loss_mask: 0.1721 (0.0956)  time: 0.1980  data: 0.0002  max mem: 5511
[07:22:32.080388] Epoch: [24]  [400/781]  eta: 0:01:15  lr: 0.000225  training_loss: 2.0182 (2.0545)  mae_loss: 0.3036 (0.3046)  classification_loss: 1.6333 (1.6557)  loss_mask: 0.0561 (0.0942)  time: 0.1975  data: 0.0002  max mem: 5511
[07:22:36.035147] Epoch: [24]  [420/781]  eta: 0:01:11  lr: 0.000225  training_loss: 1.9894 (2.0527)  mae_loss: 0.3036 (0.3046)  classification_loss: 1.6447 (1.6555)  loss_mask: 0.0435 (0.0926)  time: 0.1976  data: 0.0002  max mem: 5511
[07:22:39.998639] Epoch: [24]  [440/781]  eta: 0:01:07  lr: 0.000225  training_loss: 2.0168 (2.0511)  mae_loss: 0.2884 (0.3041)  classification_loss: 1.6386 (1.6547)  loss_mask: 0.0696 (0.0923)  time: 0.1981  data: 0.0002  max mem: 5511
[07:22:43.960977] Epoch: [24]  [460/781]  eta: 0:01:03  lr: 0.000225  training_loss: 2.0021 (2.0498)  mae_loss: 0.2908 (0.3039)  classification_loss: 1.6260 (1.6541)  loss_mask: 0.0622 (0.0918)  time: 0.1980  data: 0.0002  max mem: 5511
[07:22:47.901413] Epoch: [24]  [480/781]  eta: 0:00:59  lr: 0.000225  training_loss: 2.0245 (2.0478)  mae_loss: 0.2925 (0.3034)  classification_loss: 1.6566 (1.6537)  loss_mask: 0.0610 (0.0908)  time: 0.1969  data: 0.0002  max mem: 5511
[07:22:51.852994] Epoch: [24]  [500/781]  eta: 0:00:55  lr: 0.000225  training_loss: 1.9719 (2.0449)  mae_loss: 0.3001 (0.3033)  classification_loss: 1.6435 (1.6525)  loss_mask: 0.0437 (0.0892)  time: 0.1975  data: 0.0002  max mem: 5511
[07:22:55.792894] Epoch: [24]  [520/781]  eta: 0:00:51  lr: 0.000225  training_loss: 1.9721 (2.0430)  mae_loss: 0.3019 (0.3032)  classification_loss: 1.6170 (1.6517)  loss_mask: 0.0462 (0.0881)  time: 0.1969  data: 0.0003  max mem: 5511
[07:22:59.784926] Epoch: [24]  [540/781]  eta: 0:00:47  lr: 0.000225  training_loss: 1.9639 (2.0410)  mae_loss: 0.2979 (0.3030)  classification_loss: 1.6148 (1.6509)  loss_mask: 0.0456 (0.0872)  time: 0.1995  data: 0.0002  max mem: 5511
[07:23:03.772174] Epoch: [24]  [560/781]  eta: 0:00:43  lr: 0.000224  training_loss: 1.9992 (2.0388)  mae_loss: 0.3020 (0.3026)  classification_loss: 1.5885 (1.6490)  loss_mask: 0.0685 (0.0872)  time: 0.1992  data: 0.0002  max mem: 5511
[07:23:07.703055] Epoch: [24]  [580/781]  eta: 0:00:39  lr: 0.000224  training_loss: 2.0109 (2.0378)  mae_loss: 0.3032 (0.3029)  classification_loss: 1.6301 (1.6488)  loss_mask: 0.0503 (0.0861)  time: 0.1965  data: 0.0003  max mem: 5511
[07:23:11.657142] Epoch: [24]  [600/781]  eta: 0:00:35  lr: 0.000224  training_loss: 1.9668 (2.0355)  mae_loss: 0.2855 (0.3027)  classification_loss: 1.6005 (1.6474)  loss_mask: 0.0574 (0.0853)  time: 0.1976  data: 0.0002  max mem: 5511
[07:23:15.604283] Epoch: [24]  [620/781]  eta: 0:00:31  lr: 0.000224  training_loss: 1.9993 (2.0348)  mae_loss: 0.3072 (0.3025)  classification_loss: 1.6343 (1.6475)  loss_mask: 0.0435 (0.0847)  time: 0.1973  data: 0.0002  max mem: 5511
[07:23:19.558054] Epoch: [24]  [640/781]  eta: 0:00:27  lr: 0.000224  training_loss: 2.0128 (2.0339)  mae_loss: 0.2915 (0.3025)  classification_loss: 1.5971 (1.6462)  loss_mask: 0.0765 (0.0853)  time: 0.1976  data: 0.0002  max mem: 5511
[07:23:23.494187] Epoch: [24]  [660/781]  eta: 0:00:24  lr: 0.000224  training_loss: 1.9961 (2.0334)  mae_loss: 0.2978 (0.3021)  classification_loss: 1.6410 (1.6462)  loss_mask: 0.0707 (0.0851)  time: 0.1967  data: 0.0002  max mem: 5511
[07:23:27.435647] Epoch: [24]  [680/781]  eta: 0:00:20  lr: 0.000224  training_loss: 1.9967 (2.0327)  mae_loss: 0.3002 (0.3022)  classification_loss: 1.6303 (1.6464)  loss_mask: 0.0348 (0.0841)  time: 0.1970  data: 0.0002  max mem: 5511
[07:23:31.346606] Epoch: [24]  [700/781]  eta: 0:00:16  lr: 0.000224  training_loss: 1.9664 (2.0310)  mae_loss: 0.2935 (0.3020)  classification_loss: 1.6202 (1.6461)  loss_mask: 0.0332 (0.0829)  time: 0.1955  data: 0.0002  max mem: 5511
[07:23:35.287841] Epoch: [24]  [720/781]  eta: 0:00:12  lr: 0.000224  training_loss: 1.9796 (2.0293)  mae_loss: 0.2977 (0.3019)  classification_loss: 1.6054 (1.6455)  loss_mask: 0.0374 (0.0819)  time: 0.1970  data: 0.0002  max mem: 5511
[07:23:39.218430] Epoch: [24]  [740/781]  eta: 0:00:08  lr: 0.000224  training_loss: 2.0352 (2.0297)  mae_loss: 0.2894 (0.3019)  classification_loss: 1.6113 (1.6449)  loss_mask: 0.0972 (0.0830)  time: 0.1964  data: 0.0002  max mem: 5511
[07:23:43.186127] Epoch: [24]  [760/781]  eta: 0:00:04  lr: 0.000224  training_loss: 2.0830 (2.0317)  mae_loss: 0.3083 (0.3022)  classification_loss: 1.6507 (1.6449)  loss_mask: 0.1185 (0.0846)  time: 0.1983  data: 0.0002  max mem: 5511
[07:23:47.146195] Epoch: [24]  [780/781]  eta: 0:00:00  lr: 0.000224  training_loss: 2.0174 (2.0309)  mae_loss: 0.3114 (0.3022)  classification_loss: 1.6394 (1.6446)  loss_mask: 0.0484 (0.0841)  time: 0.1979  data: 0.0002  max mem: 5511
[07:23:47.317741] Epoch: [24] Total time: 0:02:35 (0.1986 s / it)
[07:23:47.318233] Averaged stats: lr: 0.000224  training_loss: 2.0174 (2.0309)  mae_loss: 0.3114 (0.3022)  classification_loss: 1.6394 (1.6446)  loss_mask: 0.0484 (0.0841)
[07:23:47.953562] Test:  [  0/157]  eta: 0:01:39  testing_loss: 0.8611 (0.8611)  acc1: 71.8750 (71.8750)  acc5: 95.3125 (95.3125)  time: 0.6316  data: 0.6019  max mem: 5511
[07:23:48.245942] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.0108 (1.0224)  acc1: 64.0625 (63.3523)  acc5: 98.4375 (97.7273)  time: 0.0837  data: 0.0549  max mem: 5511
[07:23:48.532073] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.9971 (0.9799)  acc1: 65.6250 (66.3690)  acc5: 96.8750 (97.6190)  time: 0.0287  data: 0.0003  max mem: 5511
[07:23:48.821623] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.9576 (0.9823)  acc1: 68.7500 (66.5827)  acc5: 96.8750 (97.4798)  time: 0.0286  data: 0.0004  max mem: 5511
[07:23:49.110010] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.9333 (0.9766)  acc1: 68.7500 (66.9588)  acc5: 96.8750 (97.5229)  time: 0.0288  data: 0.0004  max mem: 5511
[07:23:49.397539] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.9286 (0.9690)  acc1: 68.7500 (67.6471)  acc5: 98.4375 (97.5490)  time: 0.0287  data: 0.0003  max mem: 5511
[07:23:49.691534] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.9482 (0.9677)  acc1: 68.7500 (67.5717)  acc5: 96.8750 (97.4641)  time: 0.0289  data: 0.0003  max mem: 5511
[07:23:49.982687] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.9041 (0.9573)  acc1: 68.7500 (67.9357)  acc5: 96.8750 (97.5132)  time: 0.0291  data: 0.0003  max mem: 5511
[07:23:50.269791] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.9041 (0.9630)  acc1: 67.1875 (67.6119)  acc5: 96.8750 (97.3380)  time: 0.0287  data: 0.0002  max mem: 5511
[07:23:50.558656] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.9823 (0.9662)  acc1: 65.6250 (67.3592)  acc5: 98.4375 (97.4245)  time: 0.0285  data: 0.0002  max mem: 5511
[07:23:50.843634] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.9823 (0.9692)  acc1: 64.0625 (67.3113)  acc5: 98.4375 (97.3700)  time: 0.0285  data: 0.0002  max mem: 5511
[07:23:51.129789] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.9730 (0.9708)  acc1: 65.6250 (67.1875)  acc5: 96.8750 (97.3395)  time: 0.0284  data: 0.0002  max mem: 5511
[07:23:51.413903] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.9350 (0.9658)  acc1: 68.7500 (67.4070)  acc5: 98.4375 (97.3915)  time: 0.0283  data: 0.0002  max mem: 5511
[07:23:51.699699] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.9350 (0.9654)  acc1: 67.1875 (67.1756)  acc5: 98.4375 (97.4237)  time: 0.0283  data: 0.0002  max mem: 5511
[07:23:51.985039] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.9853 (0.9653)  acc1: 67.1875 (67.3759)  acc5: 96.8750 (97.3293)  time: 0.0284  data: 0.0002  max mem: 5511
[07:23:52.265772] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.9296 (0.9615)  acc1: 70.3125 (67.4876)  acc5: 96.8750 (97.3510)  time: 0.0282  data: 0.0001  max mem: 5511
[07:23:52.416426] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.9272 (0.9636)  acc1: 68.7500 (67.4300)  acc5: 96.8750 (97.3900)  time: 0.0270  data: 0.0001  max mem: 5511
[07:23:52.570534] Test: Total time: 0:00:05 (0.0334 s / it)
[07:23:52.571087] * Acc@1 67.430 Acc@5 97.390 loss 0.964
[07:23:52.571482] Accuracy of the network on the 10000 test images: 67.4%
[07:23:52.571832] Max accuracy: 67.43%
[07:23:52.836688] log_dir: ./output_dir
[07:23:53.816915] Epoch: [25]  [  0/781]  eta: 0:12:44  lr: 0.000224  training_loss: 1.9818 (1.9818)  mae_loss: 0.3308 (0.3308)  classification_loss: 1.6111 (1.6111)  loss_mask: 0.0399 (0.0399)  time: 0.9783  data: 0.7691  max mem: 5511
[07:23:57.767115] Epoch: [25]  [ 20/781]  eta: 0:02:58  lr: 0.000224  training_loss: 1.9403 (1.9561)  mae_loss: 0.3152 (0.3123)  classification_loss: 1.5937 (1.5998)  loss_mask: 0.0354 (0.0440)  time: 0.1974  data: 0.0002  max mem: 5511
[07:24:01.692928] Epoch: [25]  [ 40/781]  eta: 0:02:39  lr: 0.000224  training_loss: 1.9958 (1.9782)  mae_loss: 0.2759 (0.3055)  classification_loss: 1.6583 (1.6225)  loss_mask: 0.0373 (0.0502)  time: 0.1962  data: 0.0003  max mem: 5511
[07:24:05.660969] Epoch: [25]  [ 60/781]  eta: 0:02:31  lr: 0.000224  training_loss: 2.0031 (1.9937)  mae_loss: 0.3025 (0.3068)  classification_loss: 1.6137 (1.6253)  loss_mask: 0.0615 (0.0616)  time: 0.1983  data: 0.0003  max mem: 5511
[07:24:09.608293] Epoch: [25]  [ 80/781]  eta: 0:02:25  lr: 0.000223  training_loss: 1.9948 (1.9972)  mae_loss: 0.2952 (0.3037)  classification_loss: 1.6315 (1.6326)  loss_mask: 0.0534 (0.0608)  time: 0.1973  data: 0.0003  max mem: 5511
[07:24:13.561260] Epoch: [25]  [100/781]  eta: 0:02:19  lr: 0.000223  training_loss: 1.9587 (1.9922)  mae_loss: 0.2905 (0.3023)  classification_loss: 1.5964 (1.6295)  loss_mask: 0.0515 (0.0604)  time: 0.1976  data: 0.0002  max mem: 5511
[07:24:17.520011] Epoch: [25]  [120/781]  eta: 0:02:14  lr: 0.000223  training_loss: 1.9825 (1.9905)  mae_loss: 0.2830 (0.2995)  classification_loss: 1.5726 (1.6257)  loss_mask: 0.0755 (0.0653)  time: 0.1979  data: 0.0003  max mem: 5511
[07:24:21.485665] Epoch: [25]  [140/781]  eta: 0:02:10  lr: 0.000223  training_loss: 1.9556 (1.9837)  mae_loss: 0.3008 (0.3002)  classification_loss: 1.5936 (1.6196)  loss_mask: 0.0451 (0.0639)  time: 0.1981  data: 0.0008  max mem: 5511
[07:24:25.415828] Epoch: [25]  [160/781]  eta: 0:02:05  lr: 0.000223  training_loss: 1.9567 (1.9783)  mae_loss: 0.2811 (0.2985)  classification_loss: 1.6130 (1.6186)  loss_mask: 0.0340 (0.0611)  time: 0.1964  data: 0.0002  max mem: 5511
[07:24:29.354731] Epoch: [25]  [180/781]  eta: 0:02:01  lr: 0.000223  training_loss: 1.9699 (1.9774)  mae_loss: 0.3219 (0.3005)  classification_loss: 1.6197 (1.6180)  loss_mask: 0.0290 (0.0589)  time: 0.1969  data: 0.0003  max mem: 5511
[07:24:33.296994] Epoch: [25]  [200/781]  eta: 0:01:56  lr: 0.000223  training_loss: 1.9242 (1.9742)  mae_loss: 0.2916 (0.3002)  classification_loss: 1.5491 (1.6129)  loss_mask: 0.0595 (0.0611)  time: 0.1970  data: 0.0002  max mem: 5511
[07:24:37.232325] Epoch: [25]  [220/781]  eta: 0:01:52  lr: 0.000223  training_loss: 2.1517 (1.9892)  mae_loss: 0.2942 (0.2996)  classification_loss: 1.6216 (1.6142)  loss_mask: 0.1705 (0.0754)  time: 0.1967  data: 0.0002  max mem: 5511
[07:24:41.206187] Epoch: [25]  [240/781]  eta: 0:01:48  lr: 0.000223  training_loss: 2.0031 (1.9899)  mae_loss: 0.3001 (0.2998)  classification_loss: 1.6055 (1.6122)  loss_mask: 0.0995 (0.0779)  time: 0.1986  data: 0.0002  max mem: 5511
[07:24:45.150036] Epoch: [25]  [260/781]  eta: 0:01:44  lr: 0.000223  training_loss: 1.9660 (1.9891)  mae_loss: 0.2849 (0.2993)  classification_loss: 1.6138 (1.6135)  loss_mask: 0.0454 (0.0763)  time: 0.1971  data: 0.0002  max mem: 5511
[07:24:49.109122] Epoch: [25]  [280/781]  eta: 0:01:40  lr: 0.000223  training_loss: 1.9739 (1.9881)  mae_loss: 0.3191 (0.3003)  classification_loss: 1.5879 (1.6130)  loss_mask: 0.0479 (0.0748)  time: 0.1979  data: 0.0002  max mem: 5511
[07:24:53.072551] Epoch: [25]  [300/781]  eta: 0:01:36  lr: 0.000223  training_loss: 1.9770 (1.9894)  mae_loss: 0.2946 (0.3001)  classification_loss: 1.6342 (1.6151)  loss_mask: 0.0554 (0.0742)  time: 0.1981  data: 0.0002  max mem: 5511

[07:24:57.037046] Epoch: [25]  [320/781]  eta: 0:01:32  lr: 0.000223  training_loss: 2.0160 (1.9920)  mae_loss: 0.3151 (0.3002)  classification_loss: 1.6354 (1.6157)  loss_mask: 0.0855 (0.0761)  time: 0.1981  data: 0.0002  max mem: 5511
[07:25:01.001800] Epoch: [25]  [340/781]  eta: 0:01:28  lr: 0.000223  training_loss: 1.9734 (1.9905)  mae_loss: 0.2999 (0.3003)  classification_loss: 1.5654 (1.6135)  loss_mask: 0.0798 (0.0767)  time: 0.1982  data: 0.0002  max mem: 5511
[07:25:04.944130] Epoch: [25]  [360/781]  eta: 0:01:24  lr: 0.000223  training_loss: 2.0202 (1.9936)  mae_loss: 0.2957 (0.3005)  classification_loss: 1.6726 (1.6169)  loss_mask: 0.0461 (0.0762)  time: 0.1970  data: 0.0002  max mem: 5511
[07:25:08.895371] Epoch: [25]  [380/781]  eta: 0:01:20  lr: 0.000223  training_loss: 2.0869 (1.9987)  mae_loss: 0.3137 (0.3011)  classification_loss: 1.7133 (1.6223)  loss_mask: 0.0373 (0.0753)  time: 0.1975  data: 0.0002  max mem: 5511
[07:25:12.851714] Epoch: [25]  [400/781]  eta: 0:01:15  lr: 0.000222  training_loss: 2.0660 (2.0017)  mae_loss: 0.3164 (0.3018)  classification_loss: 1.6778 (1.6248)  loss_mask: 0.0513 (0.0750)  time: 0.1977  data: 0.0002  max mem: 5511
[07:25:16.812668] Epoch: [25]  [420/781]  eta: 0:01:11  lr: 0.000222  training_loss: 2.0291 (2.0038)  mae_loss: 0.3008 (0.3020)  classification_loss: 1.6403 (1.6257)  loss_mask: 0.0835 (0.0761)  time: 0.1980  data: 0.0002  max mem: 5511
[07:25:20.762055] Epoch: [25]  [440/781]  eta: 0:01:07  lr: 0.000222  training_loss: 2.0244 (2.0032)  mae_loss: 0.2848 (0.3014)  classification_loss: 1.6328 (1.6264)  loss_mask: 0.0498 (0.0753)  time: 0.1974  data: 0.0002  max mem: 5511
[07:25:24.700832] Epoch: [25]  [460/781]  eta: 0:01:03  lr: 0.000222  training_loss: 2.0196 (2.0040)  mae_loss: 0.3032 (0.3018)  classification_loss: 1.6097 (1.6268)  loss_mask: 0.0605 (0.0754)  time: 0.1968  data: 0.0003  max mem: 5511
[07:25:28.645676] Epoch: [25]  [480/781]  eta: 0:00:59  lr: 0.000222  training_loss: 1.9928 (2.0039)  mae_loss: 0.3019 (0.3020)  classification_loss: 1.6333 (1.6269)  loss_mask: 0.0500 (0.0751)  time: 0.1971  data: 0.0002  max mem: 5511
[07:25:32.631515] Epoch: [25]  [500/781]  eta: 0:00:55  lr: 0.000222  training_loss: 2.0264 (2.0042)  mae_loss: 0.2937 (0.3018)  classification_loss: 1.6140 (1.6269)  loss_mask: 0.0794 (0.0755)  time: 0.1992  data: 0.0002  max mem: 5511
[07:25:36.565770] Epoch: [25]  [520/781]  eta: 0:00:51  lr: 0.000222  training_loss: 2.0027 (2.0039)  mae_loss: 0.3004 (0.3019)  classification_loss: 1.6149 (1.6266)  loss_mask: 0.0683 (0.0754)  time: 0.1966  data: 0.0002  max mem: 5511
[07:25:40.547534] Epoch: [25]  [540/781]  eta: 0:00:47  lr: 0.000222  training_loss: 1.9495 (2.0022)  mae_loss: 0.2900 (0.3019)  classification_loss: 1.5832 (1.6262)  loss_mask: 0.0358 (0.0741)  time: 0.1989  data: 0.0003  max mem: 5511
[07:25:44.480112] Epoch: [25]  [560/781]  eta: 0:00:43  lr: 0.000222  training_loss: 1.9417 (2.0010)  mae_loss: 0.2951 (0.3020)  classification_loss: 1.5998 (1.6258)  loss_mask: 0.0465 (0.0732)  time: 0.1965  data: 0.0002  max mem: 5511
[07:25:48.453098] Epoch: [25]  [580/781]  eta: 0:00:39  lr: 0.000222  training_loss: 1.9379 (1.9995)  mae_loss: 0.3000 (0.3018)  classification_loss: 1.6154 (1.6258)  loss_mask: 0.0284 (0.0719)  time: 0.1986  data: 0.0003  max mem: 5511
[07:25:52.392031] Epoch: [25]  [600/781]  eta: 0:00:35  lr: 0.000222  training_loss: 1.9264 (1.9991)  mae_loss: 0.2936 (0.3018)  classification_loss: 1.5623 (1.6249)  loss_mask: 0.0332 (0.0724)  time: 0.1969  data: 0.0003  max mem: 5511
[07:25:56.341056] Epoch: [25]  [620/781]  eta: 0:00:32  lr: 0.000222  training_loss: 2.0269 (2.0005)  mae_loss: 0.3104 (0.3023)  classification_loss: 1.6073 (1.6249)  loss_mask: 0.0790 (0.0734)  time: 0.1974  data: 0.0002  max mem: 5511
[07:26:00.284991] Epoch: [25]  [640/781]  eta: 0:00:28  lr: 0.000222  training_loss: 1.9657 (1.9993)  mae_loss: 0.2838 (0.3023)  classification_loss: 1.6138 (1.6237)  loss_mask: 0.0411 (0.0732)  time: 0.1971  data: 0.0002  max mem: 5511
[07:26:04.226371] Epoch: [25]  [660/781]  eta: 0:00:24  lr: 0.000222  training_loss: 2.0135 (2.0000)  mae_loss: 0.2942 (0.3022)  classification_loss: 1.6415 (1.6242)  loss_mask: 0.0653 (0.0737)  time: 0.1970  data: 0.0002  max mem: 5511
[07:26:08.157007] Epoch: [25]  [680/781]  eta: 0:00:20  lr: 0.000222  training_loss: 2.0288 (2.0002)  mae_loss: 0.2940 (0.3020)  classification_loss: 1.6360 (1.6250)  loss_mask: 0.0497 (0.0732)  time: 0.1965  data: 0.0003  max mem: 5511
[07:26:12.097583] Epoch: [25]  [700/781]  eta: 0:00:16  lr: 0.000221  training_loss: 1.9281 (1.9981)  mae_loss: 0.2798 (0.3016)  classification_loss: 1.5990 (1.6245)  loss_mask: 0.0240 (0.0720)  time: 0.1969  data: 0.0002  max mem: 5511
[07:26:16.065334] Epoch: [25]  [720/781]  eta: 0:00:12  lr: 0.000221  training_loss: 1.9397 (1.9961)  mae_loss: 0.2773 (0.3011)  classification_loss: 1.6222 (1.6241)  loss_mask: 0.0248 (0.0709)  time: 0.1983  data: 0.0002  max mem: 5511
[07:26:20.011814] Epoch: [25]  [740/781]  eta: 0:00:08  lr: 0.000221  training_loss: 1.9430 (1.9948)  mae_loss: 0.2980 (0.3009)  classification_loss: 1.6034 (1.6234)  loss_mask: 0.0372 (0.0705)  time: 0.1972  data: 0.0002  max mem: 5511
[07:26:23.971344] Epoch: [25]  [760/781]  eta: 0:00:04  lr: 0.000221  training_loss: 2.0239 (1.9952)  mae_loss: 0.2956 (0.3007)  classification_loss: 1.5983 (1.6228)  loss_mask: 0.0925 (0.0718)  time: 0.1979  data: 0.0002  max mem: 5511
[07:26:27.934425] Epoch: [25]  [780/781]  eta: 0:00:00  lr: 0.000221  training_loss: 1.9380 (1.9942)  mae_loss: 0.2873 (0.3006)  classification_loss: 1.6109 (1.6218)  loss_mask: 0.0631 (0.0718)  time: 0.1981  data: 0.0001  max mem: 5511
[07:26:28.098331] Epoch: [25] Total time: 0:02:35 (0.1988 s / it)
[07:26:28.099037] Averaged stats: lr: 0.000221  training_loss: 1.9380 (1.9942)  mae_loss: 0.2873 (0.3006)  classification_loss: 1.6109 (1.6218)  loss_mask: 0.0631 (0.0718)
[07:26:28.842860] Test:  [  0/157]  eta: 0:01:56  testing_loss: 0.8734 (0.8734)  acc1: 76.5625 (76.5625)  acc5: 96.8750 (96.8750)  time: 0.7392  data: 0.7092  max mem: 5511
[07:26:29.129314] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.9362 (0.9616)  acc1: 67.1875 (66.4773)  acc5: 98.4375 (98.1534)  time: 0.0930  data: 0.0646  max mem: 5511
[07:26:29.414268] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.9252 (0.9296)  acc1: 68.7500 (68.1548)  acc5: 98.4375 (98.1399)  time: 0.0284  data: 0.0002  max mem: 5511
[07:26:29.696889] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.9036 (0.9313)  acc1: 70.3125 (68.4980)  acc5: 98.4375 (97.7319)  time: 0.0283  data: 0.0002  max mem: 5511
[07:26:29.980977] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.9036 (0.9299)  acc1: 70.3125 (69.0549)  acc5: 96.8750 (97.7134)  time: 0.0282  data: 0.0002  max mem: 5511
[07:26:30.263447] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.9072 (0.9256)  acc1: 70.3125 (69.4547)  acc5: 98.4375 (97.7635)  time: 0.0282  data: 0.0002  max mem: 5511
[07:26:30.545892] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.9072 (0.9245)  acc1: 70.3125 (69.1342)  acc5: 98.4375 (97.7203)  time: 0.0281  data: 0.0002  max mem: 5511
[07:26:30.829725] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.8730 (0.9191)  acc1: 68.7500 (69.1901)  acc5: 96.8750 (97.6893)  time: 0.0282  data: 0.0002  max mem: 5511
[07:26:31.113402] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.9062 (0.9244)  acc1: 67.1875 (68.9236)  acc5: 98.4375 (97.6466)  time: 0.0283  data: 0.0002  max mem: 5511
[07:26:31.396487] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.9561 (0.9264)  acc1: 67.1875 (68.8187)  acc5: 98.4375 (97.7679)  time: 0.0282  data: 0.0002  max mem: 5511
[07:26:31.680538] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.9540 (0.9291)  acc1: 65.6250 (68.6262)  acc5: 98.4375 (97.7568)  time: 0.0282  data: 0.0002  max mem: 5511
[07:26:31.964450] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.9563 (0.9325)  acc1: 65.6250 (68.3840)  acc5: 96.8750 (97.6914)  time: 0.0283  data: 0.0002  max mem: 5511
[07:26:32.249899] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.8997 (0.9273)  acc1: 67.1875 (68.5821)  acc5: 96.8750 (97.7014)  time: 0.0283  data: 0.0002  max mem: 5511
[07:26:32.532597] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.8935 (0.9289)  acc1: 68.7500 (68.5234)  acc5: 98.4375 (97.7219)  time: 0.0283  data: 0.0002  max mem: 5511
[07:26:32.814537] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.9215 (0.9282)  acc1: 67.1875 (68.6281)  acc5: 98.4375 (97.7283)  time: 0.0281  data: 0.0001  max mem: 5511
[07:26:33.097180] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.9148 (0.9268)  acc1: 70.3125 (68.7500)  acc5: 98.4375 (97.7235)  time: 0.0281  data: 0.0001  max mem: 5511
[07:26:33.249130] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.9453 (0.9281)  acc1: 70.3125 (68.6200)  acc5: 98.4375 (97.7200)  time: 0.0272  data: 0.0001  max mem: 5511
[07:26:33.424084] Test: Total time: 0:00:05 (0.0339 s / it)
[07:26:33.424652] * Acc@1 68.620 Acc@5 97.720 loss 0.928
[07:26:33.424982] Accuracy of the network on the 10000 test images: 68.6%
[07:26:33.425241] Max accuracy: 68.62%
[07:26:33.675110] log_dir: ./output_dir
[07:26:34.630303] Epoch: [26]  [  0/781]  eta: 0:12:24  lr: 0.000221  training_loss: 1.9289 (1.9289)  mae_loss: 0.3173 (0.3173)  classification_loss: 1.5198 (1.5198)  loss_mask: 0.0919 (0.0919)  time: 0.9529  data: 0.7442  max mem: 5511
[07:26:38.600153] Epoch: [26]  [ 20/781]  eta: 0:02:58  lr: 0.000221  training_loss: 1.9389 (1.9384)  mae_loss: 0.3141 (0.3122)  classification_loss: 1.5712 (1.5619)  loss_mask: 0.0509 (0.0642)  time: 0.1984  data: 0.0002  max mem: 5511
[07:26:42.587996] Epoch: [26]  [ 40/781]  eta: 0:02:40  lr: 0.000221  training_loss: 1.9291 (1.9309)  mae_loss: 0.2891 (0.3046)  classification_loss: 1.5565 (1.5623)  loss_mask: 0.0466 (0.0640)  time: 0.1993  data: 0.0003  max mem: 5511
[07:26:46.521944] Epoch: [26]  [ 60/781]  eta: 0:02:31  lr: 0.000221  training_loss: 2.0005 (1.9617)  mae_loss: 0.3050 (0.3057)  classification_loss: 1.6381 (1.5971)  loss_mask: 0.0327 (0.0589)  time: 0.1966  data: 0.0002  max mem: 5511
[07:26:50.479588] Epoch: [26]  [ 80/781]  eta: 0:02:25  lr: 0.000221  training_loss: 1.9414 (1.9574)  mae_loss: 0.2942 (0.3033)  classification_loss: 1.5813 (1.5992)  loss_mask: 0.0350 (0.0549)  time: 0.1978  data: 0.0002  max mem: 5511
[07:26:54.447899] Epoch: [26]  [100/781]  eta: 0:02:19  lr: 0.000221  training_loss: 1.9509 (1.9528)  mae_loss: 0.2802 (0.3005)  classification_loss: 1.6340 (1.6024)  loss_mask: 0.0286 (0.0499)  time: 0.1983  data: 0.0002  max mem: 5511
[07:26:58.423742] Epoch: [26]  [120/781]  eta: 0:02:15  lr: 0.000221  training_loss: 1.9266 (1.9468)  mae_loss: 0.2918 (0.2981)  classification_loss: 1.6123 (1.5995)  loss_mask: 0.0328 (0.0492)  time: 0.1987  data: 0.0002  max mem: 5511
[07:27:02.416766] Epoch: [26]  [140/781]  eta: 0:02:10  lr: 0.000221  training_loss: 1.9575 (1.9460)  mae_loss: 0.2918 (0.2980)  classification_loss: 1.5824 (1.5940)  loss_mask: 0.0450 (0.0540)  time: 0.1996  data: 0.0002  max mem: 5511
[07:27:06.358490] Epoch: [26]  [160/781]  eta: 0:02:05  lr: 0.000221  training_loss: 1.9838 (1.9511)  mae_loss: 0.2942 (0.2980)  classification_loss: 1.6177 (1.5957)  loss_mask: 0.0726 (0.0574)  time: 0.1970  data: 0.0002  max mem: 5511
[07:27:10.309514] Epoch: [26]  [180/781]  eta: 0:02:01  lr: 0.000221  training_loss: 1.9778 (1.9558)  mae_loss: 0.2946 (0.2980)  classification_loss: 1.6525 (1.5999)  loss_mask: 0.0431 (0.0578)  time: 0.1975  data: 0.0002  max mem: 5511
[07:27:14.261878] Epoch: [26]  [200/781]  eta: 0:01:57  lr: 0.000220  training_loss: 1.9272 (1.9549)  mae_loss: 0.2945 (0.2977)  classification_loss: 1.5922 (1.6003)  loss_mask: 0.0371 (0.0569)  time: 0.1975  data: 0.0002  max mem: 5511
[07:27:18.202406] Epoch: [26]  [220/781]  eta: 0:01:52  lr: 0.000220  training_loss: 1.9265 (1.9528)  mae_loss: 0.2696 (0.2956)  classification_loss: 1.5857 (1.6028)  loss_mask: 0.0230 (0.0544)  time: 0.1969  data: 0.0003  max mem: 5511
[07:27:22.177468] Epoch: [26]  [240/781]  eta: 0:01:48  lr: 0.000220  training_loss: 1.9258 (1.9522)  mae_loss: 0.2869 (0.2959)  classification_loss: 1.5944 (1.6035)  loss_mask: 0.0250 (0.0528)  time: 0.1986  data: 0.0003  max mem: 5511
[07:27:26.159667] Epoch: [26]  [260/781]  eta: 0:01:44  lr: 0.000220  training_loss: 1.9059 (1.9480)  mae_loss: 0.2951 (0.2963)  classification_loss: 1.5852 (1.6009)  loss_mask: 0.0208 (0.0508)  time: 0.1990  data: 0.0002  max mem: 5511
[07:27:30.112934] Epoch: [26]  [280/781]  eta: 0:01:40  lr: 0.000220  training_loss: 1.9209 (1.9457)  mae_loss: 0.2777 (0.2954)  classification_loss: 1.5907 (1.6017)  loss_mask: 0.0156 (0.0486)  time: 0.1976  data: 0.0003  max mem: 5511
[07:27:34.063010] Epoch: [26]  [300/781]  eta: 0:01:36  lr: 0.000220  training_loss: 1.9104 (1.9450)  mae_loss: 0.3100 (0.2965)  classification_loss: 1.5956 (1.6017)  loss_mask: 0.0170 (0.0468)  time: 0.1974  data: 0.0002  max mem: 5511
[07:27:38.018266] Epoch: [26]  [320/781]  eta: 0:01:32  lr: 0.000220  training_loss: 1.9930 (1.9490)  mae_loss: 0.2832 (0.2962)  classification_loss: 1.5913 (1.6018)  loss_mask: 0.0647 (0.0510)  time: 0.1976  data: 0.0003  max mem: 5511
[07:27:41.964522] Epoch: [26]  [340/781]  eta: 0:01:28  lr: 0.000220  training_loss: 2.0056 (1.9540)  mae_loss: 0.2972 (0.2964)  classification_loss: 1.5529 (1.5998)  loss_mask: 0.1043 (0.0578)  time: 0.1972  data: 0.0002  max mem: 5511
[07:27:45.927845] Epoch: [26]  [360/781]  eta: 0:01:24  lr: 0.000220  training_loss: 1.9590 (1.9543)  mae_loss: 0.2939 (0.2966)  classification_loss: 1.6045 (1.6001)  loss_mask: 0.0445 (0.0576)  time: 0.1981  data: 0.0002  max mem: 5511
[07:27:49.878616] Epoch: [26]  [380/781]  eta: 0:01:20  lr: 0.000220  training_loss: 1.9108 (1.9516)  mae_loss: 0.2970 (0.2968)  classification_loss: 1.5895 (1.5988)  loss_mask: 0.0198 (0.0560)  time: 0.1974  data: 0.0002  max mem: 5511
[07:27:53.808905] Epoch: [26]  [400/781]  eta: 0:01:16  lr: 0.000220  training_loss: 1.8890 (1.9486)  mae_loss: 0.2829 (0.2965)  classification_loss: 1.5615 (1.5973)  loss_mask: 0.0305 (0.0548)  time: 0.1964  data: 0.0002  max mem: 5511
[07:27:57.747334] Epoch: [26]  [420/781]  eta: 0:01:12  lr: 0.000220  training_loss: 1.9255 (1.9473)  mae_loss: 0.2820 (0.2962)  classification_loss: 1.5903 (1.5968)  loss_mask: 0.0302 (0.0543)  time: 0.1968  data: 0.0002  max mem: 5511
[07:28:01.727552] Epoch: [26]  [440/781]  eta: 0:01:08  lr: 0.000220  training_loss: 1.9081 (1.9460)  mae_loss: 0.2881 (0.2959)  classification_loss: 1.5845 (1.5965)  loss_mask: 0.0273 (0.0536)  time: 0.1989  data: 0.0004  max mem: 5511
[07:28:05.672035] Epoch: [26]  [460/781]  eta: 0:01:04  lr: 0.000220  training_loss: 1.9492 (1.9460)  mae_loss: 0.2883 (0.2956)  classification_loss: 1.5902 (1.5952)  loss_mask: 0.0667 (0.0552)  time: 0.1971  data: 0.0002  max mem: 5511
[07:28:09.618934] Epoch: [26]  [480/781]  eta: 0:01:00  lr: 0.000220  training_loss: 1.8936 (1.9449)  mae_loss: 0.2669 (0.2947)  classification_loss: 1.5671 (1.5949)  loss_mask: 0.0497 (0.0553)  time: 0.1972  data: 0.0002  max mem: 5511
[07:28:13.547900] Epoch: [26]  [500/781]  eta: 0:00:55  lr: 0.000219  training_loss: 1.9335 (1.9444)  mae_loss: 0.2853 (0.2945)  classification_loss: 1.5914 (1.5948)  loss_mask: 0.0280 (0.0550)  time: 0.1963  data: 0.0003  max mem: 5511
[07:28:17.496744] Epoch: [26]  [520/781]  eta: 0:00:51  lr: 0.000219  training_loss: 1.8880 (1.9413)  mae_loss: 0.2801 (0.2942)  classification_loss: 1.5770 (1.5934)  loss_mask: 0.0191 (0.0537)  time: 0.1973  data: 0.0002  max mem: 5511
[07:28:21.499086] Epoch: [26]  [540/781]  eta: 0:00:48  lr: 0.000219  training_loss: 1.9174 (1.9420)  mae_loss: 0.2857 (0.2940)  classification_loss: 1.5997 (1.5945)  loss_mask: 0.0254 (0.0535)  time: 0.2000  data: 0.0004  max mem: 5511
[07:28:25.481551] Epoch: [26]  [560/781]  eta: 0:00:44  lr: 0.000219  training_loss: 1.9677 (1.9445)  mae_loss: 0.2913 (0.2943)  classification_loss: 1.5603 (1.5939)  loss_mask: 0.0927 (0.0564)  time: 0.1990  data: 0.0002  max mem: 5511
[07:28:29.423103] Epoch: [26]  [580/781]  eta: 0:00:40  lr: 0.000219  training_loss: 1.9580 (1.9456)  mae_loss: 0.2991 (0.2943)  classification_loss: 1.5927 (1.5946)  loss_mask: 0.0581 (0.0567)  time: 0.1970  data: 0.0002  max mem: 5511
[07:28:33.392593] Epoch: [26]  [600/781]  eta: 0:00:36  lr: 0.000219  training_loss: 1.8870 (1.9452)  mae_loss: 0.2908 (0.2944)  classification_loss: 1.5850 (1.5948)  loss_mask: 0.0282 (0.0561)  time: 0.1984  data: 0.0002  max mem: 5511
[07:28:37.328329] Epoch: [26]  [620/781]  eta: 0:00:32  lr: 0.000219  training_loss: 1.8692 (1.9442)  mae_loss: 0.2883 (0.2943)  classification_loss: 1.5643 (1.5949)  loss_mask: 0.0201 (0.0550)  time: 0.1967  data: 0.0002  max mem: 5511
[07:28:41.327799] Epoch: [26]  [640/781]  eta: 0:00:28  lr: 0.000219  training_loss: 1.8948 (1.9435)  mae_loss: 0.2905 (0.2944)  classification_loss: 1.5717 (1.5951)  loss_mask: 0.0189 (0.0541)  time: 0.1998  data: 0.0002  max mem: 5511
[07:28:45.269793] Epoch: [26]  [660/781]  eta: 0:00:24  lr: 0.000219  training_loss: 1.9012 (1.9420)  mae_loss: 0.3018 (0.2946)  classification_loss: 1.5719 (1.5942)  loss_mask: 0.0184 (0.0533)  time: 0.1970  data: 0.0002  max mem: 5511
[07:28:49.230645] Epoch: [26]  [680/781]  eta: 0:00:20  lr: 0.000219  training_loss: 1.9352 (1.9417)  mae_loss: 0.2981 (0.2946)  classification_loss: 1.6297 (1.5949)  loss_mask: 0.0110 (0.0522)  time: 0.1979  data: 0.0002  max mem: 5511
[07:28:53.165562] Epoch: [26]  [700/781]  eta: 0:00:16  lr: 0.000219  training_loss: 1.9308 (1.9412)  mae_loss: 0.2878 (0.2946)  classification_loss: 1.5905 (1.5952)  loss_mask: 0.0122 (0.0514)  time: 0.1967  data: 0.0002  max mem: 5511
[07:28:57.100800] Epoch: [26]  [720/781]  eta: 0:00:12  lr: 0.000219  training_loss: 1.8759 (1.9401)  mae_loss: 0.2953 (0.2947)  classification_loss: 1.5324 (1.5943)  loss_mask: 0.0195 (0.0510)  time: 0.1967  data: 0.0002  max mem: 5511
[07:29:01.035244] Epoch: [26]  [740/781]  eta: 0:00:08  lr: 0.000219  training_loss: 1.9012 (1.9386)  mae_loss: 0.2835 (0.2947)  classification_loss: 1.5846 (1.5935)  loss_mask: 0.0237 (0.0504)  time: 0.1966  data: 0.0002  max mem: 5511
[07:29:04.980662] Epoch: [26]  [760/781]  eta: 0:00:04  lr: 0.000219  training_loss: 1.9668 (1.9388)  mae_loss: 0.2958 (0.2950)  classification_loss: 1.5922 (1.5936)  loss_mask: 0.0374 (0.0503)  time: 0.1972  data: 0.0002  max mem: 5511
[07:29:08.952154] Epoch: [26]  [780/781]  eta: 0:00:00  lr: 0.000218  training_loss: 1.9629 (1.9394)  mae_loss: 0.2883 (0.2950)  classification_loss: 1.5719 (1.5939)  loss_mask: 0.0341 (0.0505)  time: 0.1984  data: 0.0002  max mem: 5511
[07:29:09.125064] Epoch: [26] Total time: 0:02:35 (0.1990 s / it)
[07:29:09.125574] Averaged stats: lr: 0.000218  training_loss: 1.9629 (1.9394)  mae_loss: 0.2883 (0.2950)  classification_loss: 1.5719 (1.5939)  loss_mask: 0.0341 (0.0505)
[07:29:09.786494] Test:  [  0/157]  eta: 0:01:42  testing_loss: 0.8359 (0.8359)  acc1: 73.4375 (73.4375)  acc5: 98.4375 (98.4375)  time: 0.6554  data: 0.6255  max mem: 5511
[07:29:10.079858] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.8897 (0.9421)  acc1: 70.3125 (68.0398)  acc5: 98.4375 (98.4375)  time: 0.0859  data: 0.0570  max mem: 5511
[07:29:10.371757] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.8883 (0.9130)  acc1: 70.3125 (69.9405)  acc5: 98.4375 (98.0655)  time: 0.0290  data: 0.0003  max mem: 5511
[07:29:10.660464] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.8883 (0.9234)  acc1: 70.3125 (69.4052)  acc5: 96.8750 (97.4798)  time: 0.0288  data: 0.0003  max mem: 5511
[07:29:10.948528] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.8857 (0.9188)  acc1: 68.7500 (69.6646)  acc5: 96.8750 (97.5610)  time: 0.0287  data: 0.0004  max mem: 5511
[07:29:11.235933] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.8742 (0.9132)  acc1: 70.3125 (70.0061)  acc5: 98.4375 (97.6409)  time: 0.0286  data: 0.0005  max mem: 5511
[07:29:11.525841] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.9038 (0.9122)  acc1: 70.3125 (69.7234)  acc5: 98.4375 (97.6434)  time: 0.0287  data: 0.0003  max mem: 5511
[07:29:11.810015] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.8761 (0.9026)  acc1: 70.3125 (70.0704)  acc5: 96.8750 (97.6452)  time: 0.0285  data: 0.0002  max mem: 5511
[07:29:12.096367] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.8678 (0.9067)  acc1: 70.3125 (69.9846)  acc5: 96.8750 (97.5694)  time: 0.0284  data: 0.0002  max mem: 5511
[07:29:12.381638] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.9079 (0.9087)  acc1: 70.3125 (69.9691)  acc5: 96.8750 (97.6477)  time: 0.0284  data: 0.0002  max mem: 5511
[07:29:12.665716] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.9142 (0.9116)  acc1: 70.3125 (69.8639)  acc5: 98.4375 (97.6795)  time: 0.0283  data: 0.0002  max mem: 5511
[07:29:12.951589] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.9382 (0.9135)  acc1: 68.7500 (69.8198)  acc5: 98.4375 (97.7055)  time: 0.0284  data: 0.0002  max mem: 5511
[07:29:13.235445] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.8851 (0.9097)  acc1: 70.3125 (69.9768)  acc5: 98.4375 (97.7273)  time: 0.0283  data: 0.0002  max mem: 5511
[07:29:13.526917] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.8929 (0.9118)  acc1: 68.7500 (69.7996)  acc5: 98.4375 (97.7576)  time: 0.0286  data: 0.0002  max mem: 5511
[07:29:13.810879] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.9100 (0.9098)  acc1: 68.7500 (69.9579)  acc5: 98.4375 (97.7394)  time: 0.0286  data: 0.0002  max mem: 5511
[07:29:14.092351] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.9087 (0.9073)  acc1: 70.3125 (70.0228)  acc5: 98.4375 (97.7235)  time: 0.0281  data: 0.0001  max mem: 5511
[07:29:14.243833] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.9087 (0.9103)  acc1: 70.3125 (69.8600)  acc5: 98.4375 (97.7400)  time: 0.0271  data: 0.0001  max mem: 5511
[07:29:14.414765] Test: Total time: 0:00:05 (0.0337 s / it)
[07:29:14.415817] * Acc@1 69.860 Acc@5 97.740 loss 0.910
[07:29:14.416148] Accuracy of the network on the 10000 test images: 69.9%
[07:29:14.416420] Max accuracy: 69.86%
[07:29:14.626512] log_dir: ./output_dir
[07:29:15.616676] Epoch: [27]  [  0/781]  eta: 0:12:51  lr: 0.000218  training_loss: 2.0850 (2.0850)  mae_loss: 0.3403 (0.3403)  classification_loss: 1.6612 (1.6612)  loss_mask: 0.0835 (0.0835)  time: 0.9885  data: 0.7746  max mem: 5511
[07:29:19.585300] Epoch: [27]  [ 20/781]  eta: 0:02:59  lr: 0.000218  training_loss: 1.9286 (1.9445)  mae_loss: 0.2888 (0.2951)  classification_loss: 1.5968 (1.5921)  loss_mask: 0.0497 (0.0574)  time: 0.1983  data: 0.0004  max mem: 5511
[07:29:23.550938] Epoch: [27]  [ 40/781]  eta: 0:02:41  lr: 0.000218  training_loss: 1.9371 (1.9438)  mae_loss: 0.2861 (0.2951)  classification_loss: 1.5958 (1.6028)  loss_mask: 0.0249 (0.0460)  time: 0.1982  data: 0.0003  max mem: 5511
[07:29:27.482297] Epoch: [27]  [ 60/781]  eta: 0:02:31  lr: 0.000218  training_loss: 1.9224 (1.9418)  mae_loss: 0.2955 (0.2957)  classification_loss: 1.5706 (1.5896)  loss_mask: 0.0357 (0.0566)  time: 0.1965  data: 0.0003  max mem: 5511
[07:29:31.420618] Epoch: [27]  [ 80/781]  eta: 0:02:25  lr: 0.000218  training_loss: 1.9722 (1.9506)  mae_loss: 0.2954 (0.2952)  classification_loss: 1.6159 (1.6005)  loss_mask: 0.0463 (0.0549)  time: 0.1968  data: 0.0002  max mem: 5511
[07:29:35.347449] Epoch: [27]  [100/781]  eta: 0:02:19  lr: 0.000218  training_loss: 1.8682 (1.9355)  mae_loss: 0.2850 (0.2928)  classification_loss: 1.5628 (1.5903)  loss_mask: 0.0322 (0.0523)  time: 0.1963  data: 0.0002  max mem: 5511
[07:29:39.289495] Epoch: [27]  [120/781]  eta: 0:02:14  lr: 0.000218  training_loss: 1.8868 (1.9282)  mae_loss: 0.2938 (0.2929)  classification_loss: 1.5614 (1.5867)  loss_mask: 0.0259 (0.0486)  time: 0.1970  data: 0.0002  max mem: 5511
[07:29:43.232200] Epoch: [27]  [140/781]  eta: 0:02:09  lr: 0.000218  training_loss: 1.8891 (1.9296)  mae_loss: 0.2910 (0.2935)  classification_loss: 1.5828 (1.5890)  loss_mask: 0.0201 (0.0471)  time: 0.1971  data: 0.0003  max mem: 5511
[07:29:47.177770] Epoch: [27]  [160/781]  eta: 0:02:05  lr: 0.000218  training_loss: 1.8803 (1.9293)  mae_loss: 0.3038 (0.2942)  classification_loss: 1.5362 (1.5831)  loss_mask: 0.0689 (0.0520)  time: 0.1972  data: 0.0002  max mem: 5511
[07:29:51.147236] Epoch: [27]  [180/781]  eta: 0:02:01  lr: 0.000218  training_loss: 1.9324 (1.9302)  mae_loss: 0.2956 (0.2938)  classification_loss: 1.5464 (1.5827)  loss_mask: 0.0544 (0.0537)  time: 0.1984  data: 0.0002  max mem: 5511
[07:29:55.091375] Epoch: [27]  [200/781]  eta: 0:01:56  lr: 0.000218  training_loss: 1.9188 (1.9279)  mae_loss: 0.2821 (0.2925)  classification_loss: 1.6328 (1.5840)  loss_mask: 0.0218 (0.0514)  time: 0.1971  data: 0.0002  max mem: 5511
[07:29:59.054050] Epoch: [27]  [220/781]  eta: 0:01:52  lr: 0.000218  training_loss: 1.9650 (1.9269)  mae_loss: 0.2924 (0.2930)  classification_loss: 1.6003 (1.5832)  loss_mask: 0.0329 (0.0506)  time: 0.1981  data: 0.0004  max mem: 5511
[07:30:02.993483] Epoch: [27]  [240/781]  eta: 0:01:48  lr: 0.000218  training_loss: 1.9180 (1.9274)  mae_loss: 0.2928 (0.2931)  classification_loss: 1.6050 (1.5854)  loss_mask: 0.0216 (0.0489)  time: 0.1968  data: 0.0003  max mem: 5511
[07:30:06.926437] Epoch: [27]  [260/781]  eta: 0:01:44  lr: 0.000218  training_loss: 1.8269 (1.9232)  mae_loss: 0.2766 (0.2925)  classification_loss: 1.5507 (1.5845)  loss_mask: 0.0110 (0.0462)  time: 0.1966  data: 0.0002  max mem: 5511
[07:30:10.917365] Epoch: [27]  [280/781]  eta: 0:01:40  lr: 0.000217  training_loss: 1.8934 (1.9211)  mae_loss: 0.2954 (0.2923)  classification_loss: 1.6017 (1.5847)  loss_mask: 0.0113 (0.0440)  time: 0.1995  data: 0.0002  max mem: 5511
[07:30:14.860016] Epoch: [27]  [300/781]  eta: 0:01:36  lr: 0.000217  training_loss: 1.9577 (1.9235)  mae_loss: 0.2814 (0.2928)  classification_loss: 1.6189 (1.5882)  loss_mask: 0.0129 (0.0425)  time: 0.1970  data: 0.0002  max mem: 5511
[07:30:18.835077] Epoch: [27]  [320/781]  eta: 0:01:32  lr: 0.000217  training_loss: 1.9148 (1.9247)  mae_loss: 0.2874 (0.2923)  classification_loss: 1.5990 (1.5892)  loss_mask: 0.0336 (0.0432)  time: 0.1987  data: 0.0002  max mem: 5511
[07:30:22.771747] Epoch: [27]  [340/781]  eta: 0:01:28  lr: 0.000217  training_loss: 1.9077 (1.9249)  mae_loss: 0.2763 (0.2917)  classification_loss: 1.5170 (1.5864)  loss_mask: 0.1011 (0.0468)  time: 0.1968  data: 0.0002  max mem: 5511
[07:30:26.732215] Epoch: [27]  [360/781]  eta: 0:01:24  lr: 0.000217  training_loss: 1.9691 (1.9276)  mae_loss: 0.2887 (0.2917)  classification_loss: 1.6166 (1.5882)  loss_mask: 0.0345 (0.0476)  time: 0.1979  data: 0.0002  max mem: 5511
[07:30:30.708462] Epoch: [27]  [380/781]  eta: 0:01:20  lr: 0.000217  training_loss: 1.8668 (1.9267)  mae_loss: 0.2903 (0.2919)  classification_loss: 1.5582 (1.5885)  loss_mask: 0.0209 (0.0463)  time: 0.1987  data: 0.0002  max mem: 5511
[07:30:34.661707] Epoch: [27]  [400/781]  eta: 0:01:16  lr: 0.000217  training_loss: 1.9281 (1.9268)  mae_loss: 0.3068 (0.2926)  classification_loss: 1.5565 (1.5868)  loss_mask: 0.0429 (0.0474)  time: 0.1976  data: 0.0002  max mem: 5511
[07:30:38.582246] Epoch: [27]  [420/781]  eta: 0:01:11  lr: 0.000217  training_loss: 1.9267 (1.9283)  mae_loss: 0.3029 (0.2933)  classification_loss: 1.5952 (1.5869)  loss_mask: 0.0340 (0.0480)  time: 0.1959  data: 0.0003  max mem: 5511
[07:30:42.532739] Epoch: [27]  [440/781]  eta: 0:01:07  lr: 0.000217  training_loss: 1.9543 (1.9308)  mae_loss: 0.2782 (0.2934)  classification_loss: 1.5754 (1.5880)  loss_mask: 0.0559 (0.0493)  time: 0.1974  data: 0.0002  max mem: 5511
[07:30:46.466744] Epoch: [27]  [460/781]  eta: 0:01:03  lr: 0.000217  training_loss: 1.9552 (1.9306)  mae_loss: 0.2919 (0.2933)  classification_loss: 1.5770 (1.5878)  loss_mask: 0.0367 (0.0494)  time: 0.1966  data: 0.0003  max mem: 5511
[07:30:50.394802] Epoch: [27]  [480/781]  eta: 0:00:59  lr: 0.000217  training_loss: 1.9144 (1.9302)  mae_loss: 0.2919 (0.2931)  classification_loss: 1.5887 (1.5887)  loss_mask: 0.0177 (0.0484)  time: 0.1963  data: 0.0002  max mem: 5511
[07:30:54.358297] Epoch: [27]  [500/781]  eta: 0:00:55  lr: 0.000217  training_loss: 1.9024 (1.9293)  mae_loss: 0.3068 (0.2933)  classification_loss: 1.5756 (1.5888)  loss_mask: 0.0169 (0.0472)  time: 0.1981  data: 0.0002  max mem: 5511
[07:30:58.288279] Epoch: [27]  [520/781]  eta: 0:00:51  lr: 0.000217  training_loss: 1.9123 (1.9292)  mae_loss: 0.3043 (0.2936)  classification_loss: 1.5935 (1.5897)  loss_mask: 0.0087 (0.0459)  time: 0.1963  data: 0.0003  max mem: 5511
[07:31:02.205948] Epoch: [27]  [540/781]  eta: 0:00:47  lr: 0.000217  training_loss: 1.8624 (1.9277)  mae_loss: 0.2768 (0.2934)  classification_loss: 1.5776 (1.5896)  loss_mask: 0.0094 (0.0447)  time: 0.1958  data: 0.0002  max mem: 5511
[07:31:06.140450] Epoch: [27]  [560/781]  eta: 0:00:43  lr: 0.000216  training_loss: 1.8462 (1.9267)  mae_loss: 0.2877 (0.2932)  classification_loss: 1.5123 (1.5884)  loss_mask: 0.0306 (0.0451)  time: 0.1966  data: 0.0002  max mem: 5511
[07:31:10.078462] Epoch: [27]  [580/781]  eta: 0:00:39  lr: 0.000216  training_loss: 1.9549 (1.9264)  mae_loss: 0.2862 (0.2930)  classification_loss: 1.6188 (1.5882)  loss_mask: 0.0491 (0.0452)  time: 0.1968  data: 0.0003  max mem: 5511
[07:31:14.033255] Epoch: [27]  [600/781]  eta: 0:00:35  lr: 0.000216  training_loss: 1.8734 (1.9248)  mae_loss: 0.2776 (0.2929)  classification_loss: 1.5484 (1.5871)  loss_mask: 0.0238 (0.0448)  time: 0.1976  data: 0.0002  max mem: 5511
[07:31:17.991336] Epoch: [27]  [620/781]  eta: 0:00:31  lr: 0.000216  training_loss: 1.9036 (1.9257)  mae_loss: 0.2905 (0.2930)  classification_loss: 1.5823 (1.5870)  loss_mask: 0.0340 (0.0457)  time: 0.1978  data: 0.0003  max mem: 5511
[07:31:21.914388] Epoch: [27]  [640/781]  eta: 0:00:27  lr: 0.000216  training_loss: 1.9430 (1.9274)  mae_loss: 0.2875 (0.2929)  classification_loss: 1.5305 (1.5854)  loss_mask: 0.0918 (0.0491)  time: 0.1961  data: 0.0002  max mem: 5511
[07:31:25.867824] Epoch: [27]  [660/781]  eta: 0:00:24  lr: 0.000216  training_loss: 2.0275 (1.9306)  mae_loss: 0.3023 (0.2932)  classification_loss: 1.5926 (1.5855)  loss_mask: 0.1153 (0.0519)  time: 0.1976  data: 0.0002  max mem: 5511
[07:31:29.814194] Epoch: [27]  [680/781]  eta: 0:00:20  lr: 0.000216  training_loss: 1.9877 (1.9321)  mae_loss: 0.2942 (0.2933)  classification_loss: 1.6297 (1.5862)  loss_mask: 0.0638 (0.0525)  time: 0.1972  data: 0.0002  max mem: 5511
[07:31:33.758241] Epoch: [27]  [700/781]  eta: 0:00:16  lr: 0.000216  training_loss: 1.9320 (1.9323)  mae_loss: 0.2997 (0.2935)  classification_loss: 1.6247 (1.5871)  loss_mask: 0.0225 (0.0517)  time: 0.1971  data: 0.0002  max mem: 5511
[07:31:37.733659] Epoch: [27]  [720/781]  eta: 0:00:12  lr: 0.000216  training_loss: 1.8545 (1.9310)  mae_loss: 0.2804 (0.2933)  classification_loss: 1.5320 (1.5866)  loss_mask: 0.0216 (0.0511)  time: 0.1987  data: 0.0002  max mem: 5511
[07:31:41.694272] Epoch: [27]  [740/781]  eta: 0:00:08  lr: 0.000216  training_loss: 1.8957 (1.9298)  mae_loss: 0.2960 (0.2936)  classification_loss: 1.5543 (1.5855)  loss_mask: 0.0271 (0.0507)  time: 0.1979  data: 0.0002  max mem: 5511
[07:31:45.690415] Epoch: [27]  [760/781]  eta: 0:00:04  lr: 0.000216  training_loss: 1.8938 (1.9290)  mae_loss: 0.2804 (0.2934)  classification_loss: 1.5659 (1.5853)  loss_mask: 0.0290 (0.0504)  time: 0.1997  data: 0.0003  max mem: 5511
[07:31:49.656685] Epoch: [27]  [780/781]  eta: 0:00:00  lr: 0.000216  training_loss: 1.9381 (1.9291)  mae_loss: 0.2814 (0.2933)  classification_loss: 1.6126 (1.5857)  loss_mask: 0.0270 (0.0502)  time: 0.1982  data: 0.0002  max mem: 5511
[07:31:49.792351] Epoch: [27] Total time: 0:02:35 (0.1987 s / it)
[07:31:49.792822] Averaged stats: lr: 0.000216  training_loss: 1.9381 (1.9291)  mae_loss: 0.2814 (0.2933)  classification_loss: 1.6126 (1.5857)  loss_mask: 0.0270 (0.0502)
[07:31:50.550955] Test:  [  0/157]  eta: 0:01:58  testing_loss: 0.7994 (0.7994)  acc1: 73.4375 (73.4375)  acc5: 96.8750 (96.8750)  time: 0.7529  data: 0.7234  max mem: 5511
[07:31:50.836676] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.9350 (0.9532)  acc1: 67.1875 (67.3295)  acc5: 96.8750 (97.7273)  time: 0.0943  data: 0.0659  max mem: 5511
[07:31:51.125128] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.9350 (0.9262)  acc1: 68.7500 (68.4524)  acc5: 96.8750 (97.9167)  time: 0.0286  data: 0.0003  max mem: 5511
[07:31:51.412428] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.9061 (0.9323)  acc1: 68.7500 (68.3972)  acc5: 96.8750 (97.6310)  time: 0.0286  data: 0.0004  max mem: 5511
[07:31:51.700128] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.9036 (0.9326)  acc1: 68.7500 (68.4832)  acc5: 96.8750 (97.5610)  time: 0.0286  data: 0.0003  max mem: 5511
[07:31:51.986120] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.8926 (0.9303)  acc1: 68.7500 (68.6581)  acc5: 96.8750 (97.5184)  time: 0.0285  data: 0.0002  max mem: 5511
[07:31:52.276322] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.9143 (0.9290)  acc1: 67.1875 (68.4682)  acc5: 96.8750 (97.5154)  time: 0.0286  data: 0.0003  max mem: 5511
[07:31:52.569840] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.8888 (0.9229)  acc1: 70.3125 (68.7720)  acc5: 98.4375 (97.5132)  time: 0.0290  data: 0.0003  max mem: 5511
[07:31:52.862624] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.8821 (0.9269)  acc1: 70.3125 (68.7693)  acc5: 98.4375 (97.4923)  time: 0.0291  data: 0.0002  max mem: 5511
[07:31:53.149607] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.9078 (0.9271)  acc1: 68.7500 (68.7672)  acc5: 98.4375 (97.5962)  time: 0.0289  data: 0.0002  max mem: 5511
[07:31:53.447802] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.9129 (0.9300)  acc1: 67.1875 (68.7191)  acc5: 98.4375 (97.6485)  time: 0.0291  data: 0.0004  max mem: 5511
[07:31:53.736961] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.9530 (0.9305)  acc1: 68.7500 (68.8063)  acc5: 98.4375 (97.6351)  time: 0.0292  data: 0.0004  max mem: 5511
[07:31:54.028025] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.8858 (0.9223)  acc1: 70.3125 (69.1374)  acc5: 98.4375 (97.6498)  time: 0.0288  data: 0.0002  max mem: 5511
[07:31:54.314189] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.8858 (0.9248)  acc1: 70.3125 (68.9528)  acc5: 98.4375 (97.7099)  time: 0.0287  data: 0.0002  max mem: 5511
[07:31:54.597747] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.9325 (0.9239)  acc1: 67.1875 (69.0492)  acc5: 98.4375 (97.6840)  time: 0.0283  data: 0.0001  max mem: 5511
[07:31:54.878956] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.9137 (0.9216)  acc1: 68.7500 (69.1225)  acc5: 98.4375 (97.7028)  time: 0.0281  data: 0.0001  max mem: 5511
[07:31:55.029432] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.9113 (0.9233)  acc1: 70.3125 (69.0900)  acc5: 98.4375 (97.7000)  time: 0.0270  data: 0.0001  max mem: 5511
[07:31:55.226034] Test: Total time: 0:00:05 (0.0346 s / it)
[07:31:55.227836] * Acc@1 69.090 Acc@5 97.700 loss 0.923
[07:31:55.228216] Accuracy of the network on the 10000 test images: 69.1%
[07:31:55.228413] Max accuracy: 69.86%
[07:31:55.711039] log_dir: ./output_dir
[07:31:56.685926] Epoch: [28]  [  0/781]  eta: 0:12:39  lr: 0.000216  training_loss: 2.0040 (2.0040)  mae_loss: 0.3154 (0.3154)  classification_loss: 1.6307 (1.6307)  loss_mask: 0.0578 (0.0578)  time: 0.9728  data: 0.7485  max mem: 5511
[07:32:00.704520] Epoch: [28]  [ 20/781]  eta: 0:03:00  lr: 0.000216  training_loss: 1.9870 (1.9473)  mae_loss: 0.3041 (0.3049)  classification_loss: 1.5824 (1.5928)  loss_mask: 0.0419 (0.0496)  time: 0.2008  data: 0.0002  max mem: 5511
[07:32:04.651292] Epoch: [28]  [ 40/781]  eta: 0:02:41  lr: 0.000216  training_loss: 2.0517 (1.9873)  mae_loss: 0.2931 (0.3013)  classification_loss: 1.5840 (1.6053)  loss_mask: 0.0604 (0.0807)  time: 0.1973  data: 0.0002  max mem: 5511
[07:32:08.626283] Epoch: [28]  [ 60/781]  eta: 0:02:32  lr: 0.000215  training_loss: 2.0671 (2.0355)  mae_loss: 0.2952 (0.3028)  classification_loss: 1.7130 (1.6318)  loss_mask: 0.1331 (0.1009)  time: 0.1987  data: 0.0002  max mem: 5511
[07:32:12.587071] Epoch: [28]  [ 80/781]  eta: 0:02:25  lr: 0.000215  training_loss: 2.0071 (2.0284)  mae_loss: 0.3000 (0.3026)  classification_loss: 1.6159 (1.6289)  loss_mask: 0.0775 (0.0969)  time: 0.1980  data: 0.0002  max mem: 5511
[07:32:16.509682] Epoch: [28]  [100/781]  eta: 0:02:20  lr: 0.000215  training_loss: 2.0407 (2.0281)  mae_loss: 0.3046 (0.3025)  classification_loss: 1.6099 (1.6263)  loss_mask: 0.0798 (0.0992)  time: 0.1960  data: 0.0002  max mem: 5511
[07:32:20.463902] Epoch: [28]  [120/781]  eta: 0:02:15  lr: 0.000215  training_loss: 1.9693 (2.0121)  mae_loss: 0.2905 (0.3004)  classification_loss: 1.5920 (1.6207)  loss_mask: 0.0438 (0.0911)  time: 0.1976  data: 0.0003  max mem: 5511
[07:32:24.429928] Epoch: [28]  [140/781]  eta: 0:02:10  lr: 0.000215  training_loss: 1.9923 (2.0085)  mae_loss: 0.2958 (0.2998)  classification_loss: 1.6279 (1.6211)  loss_mask: 0.0533 (0.0876)  time: 0.1982  data: 0.0003  max mem: 5511
[07:32:28.370421] Epoch: [28]  [160/781]  eta: 0:02:05  lr: 0.000215  training_loss: 1.9721 (2.0025)  mae_loss: 0.2882 (0.2991)  classification_loss: 1.6040 (1.6200)  loss_mask: 0.0491 (0.0834)  time: 0.1969  data: 0.0002  max mem: 5511
[07:32:32.300065] Epoch: [28]  [180/781]  eta: 0:02:01  lr: 0.000215  training_loss: 1.9370 (1.9964)  mae_loss: 0.2971 (0.3001)  classification_loss: 1.6051 (1.6179)  loss_mask: 0.0296 (0.0784)  time: 0.1964  data: 0.0003  max mem: 5511
[07:32:36.314593] Epoch: [28]  [200/781]  eta: 0:01:57  lr: 0.000215  training_loss: 1.9419 (1.9909)  mae_loss: 0.2911 (0.2990)  classification_loss: 1.6008 (1.6160)  loss_mask: 0.0436 (0.0759)  time: 0.2006  data: 0.0002  max mem: 5511
[07:32:40.267142] Epoch: [28]  [220/781]  eta: 0:01:53  lr: 0.000215  training_loss: 1.9469 (1.9873)  mae_loss: 0.2959 (0.2984)  classification_loss: 1.5734 (1.6131)  loss_mask: 0.0775 (0.0758)  time: 0.1975  data: 0.0003  max mem: 5511
[07:32:44.207685] Epoch: [28]  [240/781]  eta: 0:01:48  lr: 0.000215  training_loss: 1.9589 (1.9884)  mae_loss: 0.2946 (0.2987)  classification_loss: 1.6434 (1.6165)  loss_mask: 0.0415 (0.0733)  time: 0.1969  data: 0.0002  max mem: 5511
[07:32:48.165670] Epoch: [28]  [260/781]  eta: 0:01:44  lr: 0.000215  training_loss: 1.9191 (1.9855)  mae_loss: 0.2864 (0.2981)  classification_loss: 1.5972 (1.6149)  loss_mask: 0.0258 (0.0724)  time: 0.1978  data: 0.0003  max mem: 5511
[07:32:52.119699] Epoch: [28]  [280/781]  eta: 0:01:40  lr: 0.000215  training_loss: 2.0483 (1.9897)  mae_loss: 0.3047 (0.2982)  classification_loss: 1.6102 (1.6148)  loss_mask: 0.1063 (0.0767)  time: 0.1976  data: 0.0002  max mem: 5511
[07:32:56.074338] Epoch: [28]  [300/781]  eta: 0:01:36  lr: 0.000215  training_loss: 1.9677 (1.9875)  mae_loss: 0.2922 (0.2976)  classification_loss: 1.5826 (1.6135)  loss_mask: 0.0674 (0.0764)  time: 0.1977  data: 0.0002  max mem: 5511
[07:32:59.998751] Epoch: [28]  [320/781]  eta: 0:01:32  lr: 0.000215  training_loss: 1.8990 (1.9816)  mae_loss: 0.2795 (0.2972)  classification_loss: 1.5476 (1.6099)  loss_mask: 0.0407 (0.0745)  time: 0.1961  data: 0.0002  max mem: 5511
[07:33:03.962583] Epoch: [28]  [340/781]  eta: 0:01:28  lr: 0.000214  training_loss: 1.9076 (1.9779)  mae_loss: 0.3063 (0.2978)  classification_loss: 1.5515 (1.6073)  loss_mask: 0.0364 (0.0728)  time: 0.1981  data: 0.0002  max mem: 5511
[07:33:07.888318] Epoch: [28]  [360/781]  eta: 0:01:24  lr: 0.000214  training_loss: 1.9150 (1.9742)  mae_loss: 0.2915 (0.2972)  classification_loss: 1.5613 (1.6064)  loss_mask: 0.0240 (0.0706)  time: 0.1962  data: 0.0002  max mem: 5511
[07:33:11.821154] Epoch: [28]  [380/781]  eta: 0:01:20  lr: 0.000214  training_loss: 1.9433 (1.9719)  mae_loss: 0.2922 (0.2972)  classification_loss: 1.5862 (1.6059)  loss_mask: 0.0257 (0.0688)  time: 0.1966  data: 0.0002  max mem: 5511
[07:33:15.766578] Epoch: [28]  [400/781]  eta: 0:01:16  lr: 0.000214  training_loss: 1.8976 (1.9683)  mae_loss: 0.3021 (0.2977)  classification_loss: 1.5406 (1.6028)  loss_mask: 0.0518 (0.0679)  time: 0.1972  data: 0.0002  max mem: 5511
[07:33:19.756238] Epoch: [28]  [420/781]  eta: 0:01:12  lr: 0.000214  training_loss: 1.9772 (1.9688)  mae_loss: 0.2873 (0.2976)  classification_loss: 1.5734 (1.6025)  loss_mask: 0.0654 (0.0687)  time: 0.1994  data: 0.0002  max mem: 5511
[07:33:23.705922] Epoch: [28]  [440/781]  eta: 0:01:08  lr: 0.000214  training_loss: 1.9789 (1.9694)  mae_loss: 0.2920 (0.2972)  classification_loss: 1.6343 (1.6046)  loss_mask: 0.0386 (0.0676)  time: 0.1974  data: 0.0009  max mem: 5511
[07:33:27.669328] Epoch: [28]  [460/781]  eta: 0:01:04  lr: 0.000214  training_loss: 1.9413 (1.9672)  mae_loss: 0.3057 (0.2975)  classification_loss: 1.5860 (1.6033)  loss_mask: 0.0386 (0.0665)  time: 0.1981  data: 0.0003  max mem: 5511
[07:33:31.635125] Epoch: [28]  [480/781]  eta: 0:00:59  lr: 0.000214  training_loss: 1.9401 (1.9660)  mae_loss: 0.2909 (0.2972)  classification_loss: 1.5995 (1.6028)  loss_mask: 0.0341 (0.0661)  time: 0.1982  data: 0.0003  max mem: 5511
[07:33:35.579001] Epoch: [28]  [500/781]  eta: 0:00:55  lr: 0.000214  training_loss: 1.9287 (1.9654)  mae_loss: 0.2826 (0.2968)  classification_loss: 1.6100 (1.6035)  loss_mask: 0.0347 (0.0652)  time: 0.1971  data: 0.0002  max mem: 5511
[07:33:39.547497] Epoch: [28]  [520/781]  eta: 0:00:51  lr: 0.000214  training_loss: 1.9386 (1.9659)  mae_loss: 0.2857 (0.2969)  classification_loss: 1.5857 (1.6034)  loss_mask: 0.0387 (0.0656)  time: 0.1983  data: 0.0002  max mem: 5511
[07:33:43.491002] Epoch: [28]  [540/781]  eta: 0:00:47  lr: 0.000214  training_loss: 1.9709 (1.9672)  mae_loss: 0.2795 (0.2963)  classification_loss: 1.5887 (1.6034)  loss_mask: 0.0982 (0.0674)  time: 0.1971  data: 0.0003  max mem: 5511
[07:33:47.437996] Epoch: [28]  [560/781]  eta: 0:00:43  lr: 0.000214  training_loss: 1.9157 (1.9654)  mae_loss: 0.2814 (0.2961)  classification_loss: 1.5374 (1.6015)  loss_mask: 0.0714 (0.0678)  time: 0.1973  data: 0.0002  max mem: 5511
[07:33:51.441834] Epoch: [28]  [580/781]  eta: 0:00:40  lr: 0.000214  training_loss: 1.9463 (1.9650)  mae_loss: 0.2815 (0.2960)  classification_loss: 1.5799 (1.6008)  loss_mask: 0.0569 (0.0683)  time: 0.2001  data: 0.0002  max mem: 5511
[07:33:55.401221] Epoch: [28]  [600/781]  eta: 0:00:36  lr: 0.000213  training_loss: 1.9731 (1.9659)  mae_loss: 0.2822 (0.2958)  classification_loss: 1.6088 (1.6011)  loss_mask: 0.0760 (0.0690)  time: 0.1979  data: 0.0002  max mem: 5511
[07:33:59.351269] Epoch: [28]  [620/781]  eta: 0:00:32  lr: 0.000213  training_loss: 1.8688 (1.9632)  mae_loss: 0.2966 (0.2958)  classification_loss: 1.5352 (1.5994)  loss_mask: 0.0320 (0.0680)  time: 0.1974  data: 0.0002  max mem: 5511
[07:34:03.343829] Epoch: [28]  [640/781]  eta: 0:00:28  lr: 0.000213  training_loss: 1.9101 (1.9619)  mae_loss: 0.2894 (0.2958)  classification_loss: 1.5761 (1.5988)  loss_mask: 0.0267 (0.0673)  time: 0.1995  data: 0.0003  max mem: 5511
[07:34:07.284003] Epoch: [28]  [660/781]  eta: 0:00:24  lr: 0.000213  training_loss: 1.9406 (1.9607)  mae_loss: 0.2863 (0.2957)  classification_loss: 1.6008 (1.5989)  loss_mask: 0.0285 (0.0662)  time: 0.1969  data: 0.0003  max mem: 5511
[07:34:11.220406] Epoch: [28]  [680/781]  eta: 0:00:20  lr: 0.000213  training_loss: 1.9099 (1.9601)  mae_loss: 0.3059 (0.2960)  classification_loss: 1.5875 (1.5990)  loss_mask: 0.0176 (0.0651)  time: 0.1967  data: 0.0003  max mem: 5511
[07:34:15.160985] Epoch: [28]  [700/781]  eta: 0:00:16  lr: 0.000213  training_loss: 1.9035 (1.9597)  mae_loss: 0.2961 (0.2959)  classification_loss: 1.5905 (1.5993)  loss_mask: 0.0271 (0.0645)  time: 0.1969  data: 0.0002  max mem: 5511
[07:34:19.137889] Epoch: [28]  [720/781]  eta: 0:00:12  lr: 0.000213  training_loss: 1.8815 (1.9583)  mae_loss: 0.2825 (0.2956)  classification_loss: 1.5509 (1.5991)  loss_mask: 0.0241 (0.0635)  time: 0.1988  data: 0.0002  max mem: 5511
[07:34:23.096953] Epoch: [28]  [740/781]  eta: 0:00:08  lr: 0.000213  training_loss: 1.8595 (1.9557)  mae_loss: 0.2825 (0.2953)  classification_loss: 1.5688 (1.5982)  loss_mask: 0.0111 (0.0623)  time: 0.1978  data: 0.0003  max mem: 5511
[07:34:27.048053] Epoch: [28]  [760/781]  eta: 0:00:04  lr: 0.000213  training_loss: 1.9168 (1.9553)  mae_loss: 0.2944 (0.2954)  classification_loss: 1.5868 (1.5984)  loss_mask: 0.0249 (0.0614)  time: 0.1975  data: 0.0003  max mem: 5511
[07:34:30.995172] Epoch: [28]  [780/781]  eta: 0:00:00  lr: 0.000213  training_loss: 1.8798 (1.9537)  mae_loss: 0.2768 (0.2951)  classification_loss: 1.5471 (1.5976)  loss_mask: 0.0275 (0.0610)  time: 0.1973  data: 0.0002  max mem: 5511
[07:34:31.167388] Epoch: [28] Total time: 0:02:35 (0.1990 s / it)
[07:34:31.169482] Averaged stats: lr: 0.000213  training_loss: 1.8798 (1.9537)  mae_loss: 0.2768 (0.2951)  classification_loss: 1.5471 (1.5976)  loss_mask: 0.0275 (0.0610)
[07:34:31.853212] Test:  [  0/157]  eta: 0:01:46  testing_loss: 0.8812 (0.8812)  acc1: 71.8750 (71.8750)  acc5: 100.0000 (100.0000)  time: 0.6789  data: 0.6478  max mem: 5511
[07:34:32.150548] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.9612 (0.9526)  acc1: 67.1875 (66.9034)  acc5: 100.0000 (98.5795)  time: 0.0886  data: 0.0591  max mem: 5511
[07:34:32.435630] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.9402 (0.9266)  acc1: 67.1875 (68.8988)  acc5: 98.4375 (98.0655)  time: 0.0290  data: 0.0002  max mem: 5511
[07:34:32.719345] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.9296 (0.9343)  acc1: 70.3125 (68.9516)  acc5: 96.8750 (97.5806)  time: 0.0283  data: 0.0002  max mem: 5511
[07:34:33.003689] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.8808 (0.9245)  acc1: 71.8750 (69.7409)  acc5: 96.8750 (97.4466)  time: 0.0283  data: 0.0002  max mem: 5511
[07:34:33.286638] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.8603 (0.9152)  acc1: 71.8750 (70.0674)  acc5: 96.8750 (97.5184)  time: 0.0282  data: 0.0002  max mem: 5511
[07:34:33.575693] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.8942 (0.9150)  acc1: 70.3125 (69.6977)  acc5: 98.4375 (97.5410)  time: 0.0285  data: 0.0002  max mem: 5511
[07:34:33.858747] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.8732 (0.9074)  acc1: 68.7500 (69.9824)  acc5: 98.4375 (97.6012)  time: 0.0285  data: 0.0002  max mem: 5511
[07:34:34.142526] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.8732 (0.9137)  acc1: 68.7500 (69.5795)  acc5: 98.4375 (97.4730)  time: 0.0282  data: 0.0002  max mem: 5511
[07:34:34.431705] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.9298 (0.9154)  acc1: 67.1875 (69.3853)  acc5: 98.4375 (97.5790)  time: 0.0285  data: 0.0002  max mem: 5511
[07:34:34.721598] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.9173 (0.9179)  acc1: 67.1875 (69.2296)  acc5: 98.4375 (97.5866)  time: 0.0288  data: 0.0002  max mem: 5511
[07:34:35.010765] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.9535 (0.9196)  acc1: 67.1875 (69.1582)  acc5: 96.8750 (97.5648)  time: 0.0288  data: 0.0003  max mem: 5511
[07:34:35.301813] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.9200 (0.9147)  acc1: 70.3125 (69.3182)  acc5: 96.8750 (97.6111)  time: 0.0289  data: 0.0003  max mem: 5511
[07:34:35.585463] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.8915 (0.9171)  acc1: 67.1875 (69.0959)  acc5: 98.4375 (97.6503)  time: 0.0286  data: 0.0002  max mem: 5511
[07:34:35.867460] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.9214 (0.9160)  acc1: 67.1875 (69.1933)  acc5: 96.8750 (97.6507)  time: 0.0282  data: 0.0001  max mem: 5511
[07:34:36.151992] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.9025 (0.9139)  acc1: 68.7500 (69.2881)  acc5: 96.8750 (97.6304)  time: 0.0282  data: 0.0001  max mem: 5511
[07:34:36.305796] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.9217 (0.9161)  acc1: 68.7500 (69.1900)  acc5: 96.8750 (97.6200)  time: 0.0274  data: 0.0001  max mem: 5511
[07:34:36.468298] Test: Total time: 0:00:05 (0.0337 s / it)
[07:34:36.468751] * Acc@1 69.190 Acc@5 97.620 loss 0.916
[07:34:36.469070] Accuracy of the network on the 10000 test images: 69.2%
[07:34:36.469252] Max accuracy: 69.86%
[07:34:36.597908] log_dir: ./output_dir
[07:34:37.595399] Epoch: [29]  [  0/781]  eta: 0:12:57  lr: 0.000213  training_loss: 1.8757 (1.8757)  mae_loss: 0.2991 (0.2991)  classification_loss: 1.4381 (1.4381)  loss_mask: 0.1386 (0.1386)  time: 0.9959  data: 0.7670  max mem: 5511
[07:34:41.543924] Epoch: [29]  [ 20/781]  eta: 0:02:59  lr: 0.000213  training_loss: 1.9295 (2.0497)  mae_loss: 0.2966 (0.3006)  classification_loss: 1.5384 (1.5335)  loss_mask: 0.1009 (0.2156)  time: 0.1973  data: 0.0002  max mem: 5511
[07:34:45.487622] Epoch: [29]  [ 40/781]  eta: 0:02:40  lr: 0.000213  training_loss: 1.9620 (2.0111)  mae_loss: 0.2955 (0.2995)  classification_loss: 1.6127 (1.5670)  loss_mask: 0.0587 (0.1447)  time: 0.1971  data: 0.0003  max mem: 5511
[07:34:49.422430] Epoch: [29]  [ 60/781]  eta: 0:02:31  lr: 0.000213  training_loss: 1.8753 (1.9753)  mae_loss: 0.2883 (0.2984)  classification_loss: 1.5397 (1.5689)  loss_mask: 0.0289 (0.1081)  time: 0.1967  data: 0.0002  max mem: 5511
[07:34:53.360598] Epoch: [29]  [ 80/781]  eta: 0:02:24  lr: 0.000213  training_loss: 1.8970 (1.9633)  mae_loss: 0.2937 (0.2985)  classification_loss: 1.5429 (1.5690)  loss_mask: 0.0468 (0.0958)  time: 0.1968  data: 0.0002  max mem: 5511
[07:34:57.284930] Epoch: [29]  [100/781]  eta: 0:02:19  lr: 0.000212  training_loss: 1.9045 (1.9550)  mae_loss: 0.2792 (0.2960)  classification_loss: 1.5265 (1.5664)  loss_mask: 0.0612 (0.0925)  time: 0.1961  data: 0.0002  max mem: 5511
[07:35:01.219903] Epoch: [29]  [120/781]  eta: 0:02:14  lr: 0.000212  training_loss: 1.9676 (1.9574)  mae_loss: 0.2891 (0.2943)  classification_loss: 1.5763 (1.5702)  loss_mask: 0.0786 (0.0930)  time: 0.1967  data: 0.0004  max mem: 5511
[07:35:05.190887] Epoch: [29]  [140/781]  eta: 0:02:09  lr: 0.000212  training_loss: 1.8058 (1.9392)  mae_loss: 0.2785 (0.2928)  classification_loss: 1.4912 (1.5625)  loss_mask: 0.0271 (0.0840)  time: 0.1985  data: 0.0004  max mem: 5511
[07:35:09.143286] Epoch: [29]  [160/781]  eta: 0:02:05  lr: 0.000212  training_loss: 1.8393 (1.9259)  mae_loss: 0.2761 (0.2907)  classification_loss: 1.5516 (1.5587)  loss_mask: 0.0179 (0.0765)  time: 0.1975  data: 0.0002  max mem: 5511
[07:35:13.082677] Epoch: [29]  [180/781]  eta: 0:02:01  lr: 0.000212  training_loss: 1.9076 (1.9217)  mae_loss: 0.2895 (0.2916)  classification_loss: 1.5660 (1.5593)  loss_mask: 0.0172 (0.0709)  time: 0.1969  data: 0.0002  max mem: 5511
[07:35:17.054528] Epoch: [29]  [200/781]  eta: 0:01:56  lr: 0.000212  training_loss: 1.8696 (1.9200)  mae_loss: 0.3086 (0.2932)  classification_loss: 1.5745 (1.5607)  loss_mask: 0.0176 (0.0661)  time: 0.1985  data: 0.0003  max mem: 5511
[07:35:21.015155] Epoch: [29]  [220/781]  eta: 0:01:52  lr: 0.000212  training_loss: 1.8296 (1.9111)  mae_loss: 0.2607 (0.2908)  classification_loss: 1.5653 (1.5587)  loss_mask: 0.0108 (0.0616)  time: 0.1979  data: 0.0002  max mem: 5511
[07:35:24.974122] Epoch: [29]  [240/781]  eta: 0:01:48  lr: 0.000212  training_loss: 1.8752 (1.9093)  mae_loss: 0.3048 (0.2916)  classification_loss: 1.5385 (1.5584)  loss_mask: 0.0147 (0.0593)  time: 0.1979  data: 0.0002  max mem: 5511
[07:35:28.901168] Epoch: [29]  [260/781]  eta: 0:01:44  lr: 0.000212  training_loss: 1.8594 (1.9060)  mae_loss: 0.2920 (0.2916)  classification_loss: 1.5469 (1.5580)  loss_mask: 0.0133 (0.0564)  time: 0.1963  data: 0.0002  max mem: 5511
[07:35:32.839312] Epoch: [29]  [280/781]  eta: 0:01:40  lr: 0.000212  training_loss: 1.9112 (1.9053)  mae_loss: 0.2939 (0.2917)  classification_loss: 1.5792 (1.5591)  loss_mask: 0.0139 (0.0546)  time: 0.1968  data: 0.0002  max mem: 5511
[07:35:36.795506] Epoch: [29]  [300/781]  eta: 0:01:36  lr: 0.000212  training_loss: 1.9587 (1.9105)  mae_loss: 0.2913 (0.2923)  classification_loss: 1.5781 (1.5614)  loss_mask: 0.0449 (0.0568)  time: 0.1977  data: 0.0002  max mem: 5511
[07:35:40.739708] Epoch: [29]  [320/781]  eta: 0:01:32  lr: 0.000212  training_loss: 1.8932 (1.9105)  mae_loss: 0.2829 (0.2918)  classification_loss: 1.5202 (1.5607)  loss_mask: 0.0589 (0.0581)  time: 0.1971  data: 0.0002  max mem: 5511
[07:35:44.710819] Epoch: [29]  [340/781]  eta: 0:01:28  lr: 0.000212  training_loss: 1.8913 (1.9095)  mae_loss: 0.2980 (0.2924)  classification_loss: 1.5218 (1.5593)  loss_mask: 0.0491 (0.0578)  time: 0.1985  data: 0.0003  max mem: 5511
[07:35:48.658497] Epoch: [29]  [360/781]  eta: 0:01:23  lr: 0.000211  training_loss: 1.9312 (1.9109)  mae_loss: 0.2977 (0.2923)  classification_loss: 1.5655 (1.5612)  loss_mask: 0.0404 (0.0574)  time: 0.1973  data: 0.0002  max mem: 5511
[07:35:52.591011] Epoch: [29]  [380/781]  eta: 0:01:19  lr: 0.000211  training_loss: 1.9199 (1.9103)  mae_loss: 0.2885 (0.2920)  classification_loss: 1.5766 (1.5615)  loss_mask: 0.0264 (0.0567)  time: 0.1965  data: 0.0002  max mem: 5511
[07:35:56.525349] Epoch: [29]  [400/781]  eta: 0:01:15  lr: 0.000211  training_loss: 1.8966 (1.9094)  mae_loss: 0.2654 (0.2912)  classification_loss: 1.5652 (1.5623)  loss_mask: 0.0407 (0.0559)  time: 0.1966  data: 0.0002  max mem: 5511
[07:36:00.485833] Epoch: [29]  [420/781]  eta: 0:01:11  lr: 0.000211  training_loss: 1.8411 (1.9078)  mae_loss: 0.2810 (0.2909)  classification_loss: 1.5526 (1.5622)  loss_mask: 0.0240 (0.0548)  time: 0.1979  data: 0.0002  max mem: 5511
[07:36:04.455394] Epoch: [29]  [440/781]  eta: 0:01:07  lr: 0.000211  training_loss: 1.8635 (1.9060)  mae_loss: 0.2901 (0.2910)  classification_loss: 1.5547 (1.5618)  loss_mask: 0.0156 (0.0531)  time: 0.1984  data: 0.0003  max mem: 5511
[07:36:08.423278] Epoch: [29]  [460/781]  eta: 0:01:03  lr: 0.000211  training_loss: 1.7872 (1.9019)  mae_loss: 0.2698 (0.2908)  classification_loss: 1.4927 (1.5596)  loss_mask: 0.0101 (0.0515)  time: 0.1983  data: 0.0003  max mem: 5511
[07:36:12.396065] Epoch: [29]  [480/781]  eta: 0:00:59  lr: 0.000211  training_loss: 1.9287 (1.9032)  mae_loss: 0.2717 (0.2908)  classification_loss: 1.5898 (1.5610)  loss_mask: 0.0465 (0.0514)  time: 0.1986  data: 0.0003  max mem: 5511
[07:36:16.349204] Epoch: [29]  [500/781]  eta: 0:00:55  lr: 0.000211  training_loss: 1.9448 (1.9055)  mae_loss: 0.2834 (0.2906)  classification_loss: 1.5282 (1.5607)  loss_mask: 0.0993 (0.0542)  time: 0.1976  data: 0.0005  max mem: 5511
[07:36:20.302375] Epoch: [29]  [520/781]  eta: 0:00:51  lr: 0.000211  training_loss: 1.9357 (1.9067)  mae_loss: 0.2781 (0.2904)  classification_loss: 1.5307 (1.5600)  loss_mask: 0.0918 (0.0563)  time: 0.1976  data: 0.0002  max mem: 5511
[07:36:24.286603] Epoch: [29]  [540/781]  eta: 0:00:47  lr: 0.000211  training_loss: 1.8563 (1.9055)  mae_loss: 0.2885 (0.2905)  classification_loss: 1.5107 (1.5592)  loss_mask: 0.0410 (0.0559)  time: 0.1991  data: 0.0002  max mem: 5511
[07:36:28.251478] Epoch: [29]  [560/781]  eta: 0:00:43  lr: 0.000211  training_loss: 1.8672 (1.9040)  mae_loss: 0.2854 (0.2904)  classification_loss: 1.5300 (1.5585)  loss_mask: 0.0257 (0.0551)  time: 0.1982  data: 0.0002  max mem: 5511
[07:36:32.182879] Epoch: [29]  [580/781]  eta: 0:00:39  lr: 0.000211  training_loss: 1.8703 (1.9034)  mae_loss: 0.2984 (0.2906)  classification_loss: 1.5618 (1.5585)  loss_mask: 0.0225 (0.0543)  time: 0.1965  data: 0.0002  max mem: 5511
[07:36:36.129880] Epoch: [29]  [600/781]  eta: 0:00:35  lr: 0.000211  training_loss: 1.8424 (1.9028)  mae_loss: 0.2838 (0.2906)  classification_loss: 1.5279 (1.5587)  loss_mask: 0.0234 (0.0535)  time: 0.1972  data: 0.0002  max mem: 5511

[07:36:40.102826] Epoch: [29]  [620/781]  eta: 0:00:32  lr: 0.000210  training_loss: 1.8498 (1.9013)  mae_loss: 0.2882 (0.2905)  classification_loss: 1.5520 (1.5583)  loss_mask: 0.0135 (0.0524)  time: 0.1986  data: 0.0005  max mem: 5511
[07:36:44.083350] Epoch: [29]  [640/781]  eta: 0:00:28  lr: 0.000210  training_loss: 1.8273 (1.9004)  mae_loss: 0.2982 (0.2908)  classification_loss: 1.5261 (1.5577)  loss_mask: 0.0225 (0.0519)  time: 0.1989  data: 0.0002  max mem: 5511
[07:36:48.023070] Epoch: [29]  [660/781]  eta: 0:00:24  lr: 0.000210  training_loss: 1.9473 (1.9020)  mae_loss: 0.2851 (0.2907)  classification_loss: 1.5678 (1.5578)  loss_mask: 0.0633 (0.0535)  time: 0.1969  data: 0.0002  max mem: 5511
[07:36:52.000235] Epoch: [29]  [680/781]  eta: 0:00:20  lr: 0.000210  training_loss: 1.9605 (1.9045)  mae_loss: 0.2809 (0.2905)  classification_loss: 1.5770 (1.5586)  loss_mask: 0.0776 (0.0553)  time: 0.1988  data: 0.0002  max mem: 5511
[07:36:55.944222] Epoch: [29]  [700/781]  eta: 0:00:16  lr: 0.000210  training_loss: 1.8681 (1.9038)  mae_loss: 0.2899 (0.2905)  classification_loss: 1.5273 (1.5586)  loss_mask: 0.0291 (0.0547)  time: 0.1971  data: 0.0003  max mem: 5511
[07:36:59.881053] Epoch: [29]  [720/781]  eta: 0:00:12  lr: 0.000210  training_loss: 1.8276 (1.9022)  mae_loss: 0.2829 (0.2902)  classification_loss: 1.5320 (1.5581)  loss_mask: 0.0182 (0.0539)  time: 0.1967  data: 0.0002  max mem: 5511
[07:37:03.819833] Epoch: [29]  [740/781]  eta: 0:00:08  lr: 0.000210  training_loss: 1.9162 (1.9039)  mae_loss: 0.2922 (0.2906)  classification_loss: 1.5093 (1.5575)  loss_mask: 0.0777 (0.0558)  time: 0.1969  data: 0.0002  max mem: 5511
[07:37:07.776011] Epoch: [29]  [760/781]  eta: 0:00:04  lr: 0.000210  training_loss: 1.9333 (1.9060)  mae_loss: 0.2933 (0.2906)  classification_loss: 1.5507 (1.5579)  loss_mask: 0.0875 (0.0575)  time: 0.1977  data: 0.0002  max mem: 5511
[07:37:11.735231] Epoch: [29]  [780/781]  eta: 0:00:00  lr: 0.000210  training_loss: 1.9254 (1.9061)  mae_loss: 0.2812 (0.2905)  classification_loss: 1.5787 (1.5580)  loss_mask: 0.0578 (0.0576)  time: 0.1979  data: 0.0002  max mem: 5511
[07:37:11.903110] Epoch: [29] Total time: 0:02:35 (0.1989 s / it)
[07:37:11.904032] Averaged stats: lr: 0.000210  training_loss: 1.9254 (1.9061)  mae_loss: 0.2812 (0.2905)  classification_loss: 1.5787 (1.5580)  loss_mask: 0.0578 (0.0576)
[07:37:12.566123] Test:  [  0/157]  eta: 0:01:42  testing_loss: 0.8418 (0.8418)  acc1: 73.4375 (73.4375)  acc5: 96.8750 (96.8750)  time: 0.6500  data: 0.6186  max mem: 5511
[07:37:12.854697] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.9125 (0.9281)  acc1: 68.7500 (67.8977)  acc5: 98.4375 (98.1534)  time: 0.0852  data: 0.0564  max mem: 5511
[07:37:13.140959] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.9125 (0.8956)  acc1: 68.7500 (69.7173)  acc5: 98.4375 (98.2143)  time: 0.0286  data: 0.0002  max mem: 5511
[07:37:13.434069] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.9163 (0.9028)  acc1: 70.3125 (69.6573)  acc5: 96.8750 (97.6815)  time: 0.0288  data: 0.0002  max mem: 5511
[07:37:13.721951] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.8938 (0.9009)  acc1: 70.3125 (69.8552)  acc5: 96.8750 (97.5229)  time: 0.0289  data: 0.0002  max mem: 5511
[07:37:14.009422] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.8633 (0.8944)  acc1: 71.8750 (70.5882)  acc5: 96.8750 (97.4877)  time: 0.0286  data: 0.0002  max mem: 5511
[07:37:14.296290] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.8828 (0.8927)  acc1: 68.7500 (70.2613)  acc5: 96.8750 (97.4129)  time: 0.0286  data: 0.0002  max mem: 5511
[07:37:14.584644] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.8703 (0.8867)  acc1: 68.7500 (70.4445)  acc5: 96.8750 (97.4912)  time: 0.0286  data: 0.0002  max mem: 5511
[07:37:14.873453] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.8755 (0.8911)  acc1: 70.3125 (70.2546)  acc5: 96.8750 (97.3958)  time: 0.0287  data: 0.0002  max mem: 5511
[07:37:15.163897] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.8825 (0.8909)  acc1: 70.3125 (70.4499)  acc5: 96.8750 (97.5103)  time: 0.0288  data: 0.0002  max mem: 5511
[07:37:15.447601] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.9082 (0.8941)  acc1: 68.7500 (70.1114)  acc5: 98.4375 (97.5248)  time: 0.0285  data: 0.0002  max mem: 5511
[07:37:15.734900] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.9352 (0.8970)  acc1: 67.1875 (70.1436)  acc5: 96.8750 (97.4803)  time: 0.0284  data: 0.0002  max mem: 5511
[07:37:16.029847] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.8796 (0.8907)  acc1: 71.8750 (70.3254)  acc5: 96.8750 (97.4561)  time: 0.0290  data: 0.0005  max mem: 5511
[07:37:16.318045] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.8487 (0.8903)  acc1: 71.8750 (70.3841)  acc5: 96.8750 (97.4714)  time: 0.0290  data: 0.0005  max mem: 5511
[07:37:16.604651] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.9006 (0.8903)  acc1: 70.3125 (70.5452)  acc5: 96.8750 (97.4402)  time: 0.0286  data: 0.0002  max mem: 5511
[07:37:16.887632] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.8918 (0.8874)  acc1: 71.8750 (70.6747)  acc5: 98.4375 (97.4855)  time: 0.0283  data: 0.0001  max mem: 5511
[07:37:17.040397] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.8478 (0.8891)  acc1: 71.8750 (70.7300)  acc5: 98.4375 (97.5000)  time: 0.0273  data: 0.0001  max mem: 5511
[07:37:17.214996] Test: Total time: 0:00:05 (0.0338 s / it)
[07:37:17.215467] * Acc@1 70.730 Acc@5 97.500 loss 0.889
[07:37:17.215806] Accuracy of the network on the 10000 test images: 70.7%
[07:37:17.216020] Max accuracy: 70.73%
[07:37:17.346337] log_dir: ./output_dir
[07:37:18.182187] Epoch: [30]  [  0/781]  eta: 0:10:51  lr: 0.000210  training_loss: 1.7720 (1.7720)  mae_loss: 0.2672 (0.2672)  classification_loss: 1.4788 (1.4788)  loss_mask: 0.0259 (0.0259)  time: 0.8341  data: 0.6064  max mem: 5511
[07:37:22.125983] Epoch: [30]  [ 20/781]  eta: 0:02:53  lr: 0.000210  training_loss: 1.8983 (1.9028)  mae_loss: 0.2840 (0.2841)  classification_loss: 1.5881 (1.5808)  loss_mask: 0.0288 (0.0379)  time: 0.1971  data: 0.0003  max mem: 5511
[07:37:26.070529] Epoch: [30]  [ 40/781]  eta: 0:02:37  lr: 0.000210  training_loss: 1.9164 (1.9173)  mae_loss: 0.2967 (0.2908)  classification_loss: 1.5959 (1.5945)  loss_mask: 0.0179 (0.0320)  time: 0.1971  data: 0.0002  max mem: 5511
[07:37:30.052228] Epoch: [30]  [ 60/781]  eta: 0:02:30  lr: 0.000210  training_loss: 1.8973 (1.9068)  mae_loss: 0.2807 (0.2901)  classification_loss: 1.5705 (1.5895)  loss_mask: 0.0144 (0.0271)  time: 0.1990  data: 0.0002  max mem: 5511
[07:37:33.988332] Epoch: [30]  [ 80/781]  eta: 0:02:23  lr: 0.000210  training_loss: 1.8422 (1.8937)  mae_loss: 0.2833 (0.2907)  classification_loss: 1.5425 (1.5785)  loss_mask: 0.0103 (0.0245)  time: 0.1967  data: 0.0002  max mem: 5511
[07:37:37.931704] Epoch: [30]  [100/781]  eta: 0:02:18  lr: 0.000209  training_loss: 1.8594 (1.8922)  mae_loss: 0.2840 (0.2905)  classification_loss: 1.5314 (1.5733)  loss_mask: 0.0302 (0.0284)  time: 0.1971  data: 0.0002  max mem: 5511
[07:37:41.857237] Epoch: [30]  [120/781]  eta: 0:02:13  lr: 0.000209  training_loss: 1.8561 (1.8858)  mae_loss: 0.2921 (0.2923)  classification_loss: 1.4707 (1.5589)  loss_mask: 0.0283 (0.0346)  time: 0.1962  data: 0.0002  max mem: 5511
[07:37:45.815900] Epoch: [30]  [140/781]  eta: 0:02:09  lr: 0.000209  training_loss: 1.9428 (1.8917)  mae_loss: 0.2919 (0.2932)  classification_loss: 1.5238 (1.5532)  loss_mask: 0.0802 (0.0452)  time: 0.1978  data: 0.0002  max mem: 5511
[07:37:49.785396] Epoch: [30]  [160/781]  eta: 0:02:05  lr: 0.000209  training_loss: 1.8927 (1.8922)  mae_loss: 0.2871 (0.2930)  classification_loss: 1.5413 (1.5542)  loss_mask: 0.0333 (0.0450)  time: 0.1983  data: 0.0002  max mem: 5511
[07:37:53.736008] Epoch: [30]  [180/781]  eta: 0:02:00  lr: 0.000209  training_loss: 1.8558 (1.8895)  mae_loss: 0.2759 (0.2923)  classification_loss: 1.5437 (1.5537)  loss_mask: 0.0200 (0.0436)  time: 0.1974  data: 0.0003  max mem: 5511
[07:37:57.677403] Epoch: [30]  [200/781]  eta: 0:01:56  lr: 0.000209  training_loss: 1.8093 (1.8838)  mae_loss: 0.2769 (0.2907)  classification_loss: 1.5336 (1.5525)  loss_mask: 0.0084 (0.0405)  time: 0.1970  data: 0.0003  max mem: 5511
[07:38:01.632539] Epoch: [30]  [220/781]  eta: 0:01:52  lr: 0.000209  training_loss: 1.8585 (1.8830)  mae_loss: 0.3002 (0.2912)  classification_loss: 1.5649 (1.5532)  loss_mask: 0.0089 (0.0386)  time: 0.1977  data: 0.0002  max mem: 5511
[07:38:05.548327] Epoch: [30]  [240/781]  eta: 0:01:48  lr: 0.000209  training_loss: 1.9005 (1.8839)  mae_loss: 0.2949 (0.2917)  classification_loss: 1.5648 (1.5548)  loss_mask: 0.0196 (0.0374)  time: 0.1957  data: 0.0002  max mem: 5511
[07:38:09.469662] Epoch: [30]  [260/781]  eta: 0:01:43  lr: 0.000209  training_loss: 1.9063 (1.8886)  mae_loss: 0.2820 (0.2914)  classification_loss: 1.5838 (1.5561)  loss_mask: 0.0650 (0.0411)  time: 0.1960  data: 0.0003  max mem: 5511
[07:38:13.404182] Epoch: [30]  [280/781]  eta: 0:01:39  lr: 0.000209  training_loss: 1.9736 (1.8918)  mae_loss: 0.2759 (0.2912)  classification_loss: 1.5441 (1.5562)  loss_mask: 0.0684 (0.0444)  time: 0.1966  data: 0.0002  max mem: 5511
[07:38:17.356320] Epoch: [30]  [300/781]  eta: 0:01:35  lr: 0.000209  training_loss: 1.8906 (1.8931)  mae_loss: 0.2991 (0.2913)  classification_loss: 1.5702 (1.5579)  loss_mask: 0.0344 (0.0440)  time: 0.1975  data: 0.0002  max mem: 5511
[07:38:21.331072] Epoch: [30]  [320/781]  eta: 0:01:31  lr: 0.000209  training_loss: 1.8850 (1.8925)  mae_loss: 0.2847 (0.2908)  classification_loss: 1.5492 (1.5594)  loss_mask: 0.0165 (0.0423)  time: 0.1986  data: 0.0002  max mem: 5511
[07:38:25.257333] Epoch: [30]  [340/781]  eta: 0:01:27  lr: 0.000208  training_loss: 1.8121 (1.8907)  mae_loss: 0.2981 (0.2908)  classification_loss: 1.5080 (1.5581)  loss_mask: 0.0228 (0.0418)  time: 0.1962  data: 0.0002  max mem: 5511
[07:38:29.213314] Epoch: [30]  [360/781]  eta: 0:01:23  lr: 0.000208  training_loss: 1.9310 (1.8926)  mae_loss: 0.2957 (0.2910)  classification_loss: 1.5660 (1.5591)  loss_mask: 0.0417 (0.0424)  time: 0.1977  data: 0.0002  max mem: 5511
[07:38:33.162239] Epoch: [30]  [380/781]  eta: 0:01:19  lr: 0.000208  training_loss: 1.8945 (1.8945)  mae_loss: 0.2884 (0.2908)  classification_loss: 1.5602 (1.5603)  loss_mask: 0.0337 (0.0433)  time: 0.1974  data: 0.0002  max mem: 5511
[07:38:37.138029] Epoch: [30]  [400/781]  eta: 0:01:15  lr: 0.000208  training_loss: 1.9593 (1.8997)  mae_loss: 0.2973 (0.2911)  classification_loss: 1.6156 (1.5615)  loss_mask: 0.0779 (0.0470)  time: 0.1987  data: 0.0002  max mem: 5511
[07:38:41.098316] Epoch: [30]  [420/781]  eta: 0:01:11  lr: 0.000208  training_loss: 1.9611 (1.9031)  mae_loss: 0.2860 (0.2911)  classification_loss: 1.5758 (1.5633)  loss_mask: 0.0661 (0.0486)  time: 0.1979  data: 0.0003  max mem: 5511
[07:38:45.090195] Epoch: [30]  [440/781]  eta: 0:01:07  lr: 0.000208  training_loss: 1.9143 (1.9046)  mae_loss: 0.2907 (0.2908)  classification_loss: 1.5552 (1.5646)  loss_mask: 0.0515 (0.0492)  time: 0.1995  data: 0.0002  max mem: 5511
[07:38:49.030297] Epoch: [30]  [460/781]  eta: 0:01:03  lr: 0.000208  training_loss: 1.8565 (1.9029)  mae_loss: 0.2894 (0.2912)  classification_loss: 1.5512 (1.5630)  loss_mask: 0.0305 (0.0487)  time: 0.1969  data: 0.0002  max mem: 5511
[07:38:52.975564] Epoch: [30]  [480/781]  eta: 0:00:59  lr: 0.000208  training_loss: 1.9467 (1.9059)  mae_loss: 0.2907 (0.2912)  classification_loss: 1.6189 (1.5659)  loss_mask: 0.0389 (0.0487)  time: 0.1972  data: 0.0002  max mem: 5511
[07:38:56.942523] Epoch: [30]  [500/781]  eta: 0:00:55  lr: 0.000208  training_loss: 1.9482 (1.9075)  mae_loss: 0.2851 (0.2910)  classification_loss: 1.5798 (1.5669)  loss_mask: 0.0535 (0.0496)  time: 0.1983  data: 0.0002  max mem: 5511
[07:39:00.905979] Epoch: [30]  [520/781]  eta: 0:00:51  lr: 0.000208  training_loss: 1.9003 (1.9072)  mae_loss: 0.2862 (0.2911)  classification_loss: 1.5265 (1.5654)  loss_mask: 0.0606 (0.0506)  time: 0.1981  data: 0.0003  max mem: 5511
[07:39:04.855572] Epoch: [30]  [540/781]  eta: 0:00:47  lr: 0.000208  training_loss: 1.9395 (1.9088)  mae_loss: 0.2877 (0.2912)  classification_loss: 1.5597 (1.5661)  loss_mask: 0.0549 (0.0515)  time: 0.1974  data: 0.0002  max mem: 5511
[07:39:08.826890] Epoch: [30]  [560/781]  eta: 0:00:43  lr: 0.000208  training_loss: 1.8874 (1.9089)  mae_loss: 0.3103 (0.2915)  classification_loss: 1.5498 (1.5663)  loss_mask: 0.0236 (0.0510)  time: 0.1985  data: 0.0002  max mem: 5511
[07:39:12.762570] Epoch: [30]  [580/781]  eta: 0:00:39  lr: 0.000208  training_loss: 1.8941 (1.9085)  mae_loss: 0.2829 (0.2917)  classification_loss: 1.5463 (1.5662)  loss_mask: 0.0293 (0.0506)  time: 0.1967  data: 0.0002  max mem: 5511
[07:39:16.689262] Epoch: [30]  [600/781]  eta: 0:00:35  lr: 0.000207  training_loss: 1.8672 (1.9077)  mae_loss: 0.2851 (0.2915)  classification_loss: 1.5366 (1.5658)  loss_mask: 0.0258 (0.0504)  time: 0.1963  data: 0.0003  max mem: 5511
[07:39:20.658242] Epoch: [30]  [620/781]  eta: 0:00:31  lr: 0.000207  training_loss: 1.8531 (1.9065)  mae_loss: 0.2769 (0.2912)  classification_loss: 1.5554 (1.5654)  loss_mask: 0.0246 (0.0499)  time: 0.1984  data: 0.0002  max mem: 5511
[07:39:24.623327] Epoch: [30]  [640/781]  eta: 0:00:27  lr: 0.000207  training_loss: 1.8515 (1.9050)  mae_loss: 0.2867 (0.2911)  classification_loss: 1.5432 (1.5650)  loss_mask: 0.0137 (0.0489)  time: 0.1982  data: 0.0002  max mem: 5511
[07:39:28.600131] Epoch: [30]  [660/781]  eta: 0:00:24  lr: 0.000207  training_loss: 1.9017 (1.9045)  mae_loss: 0.2862 (0.2910)  classification_loss: 1.5859 (1.5653)  loss_mask: 0.0163 (0.0482)  time: 0.1988  data: 0.0002  max mem: 5511
[07:39:32.558001] Epoch: [30]  [680/781]  eta: 0:00:20  lr: 0.000207  training_loss: 1.9303 (1.9051)  mae_loss: 0.2998 (0.2915)  classification_loss: 1.5664 (1.5648)  loss_mask: 0.0635 (0.0488)  time: 0.1978  data: 0.0002  max mem: 5511
[07:39:36.516869] Epoch: [30]  [700/781]  eta: 0:00:16  lr: 0.000207  training_loss: 1.9764 (1.9070)  mae_loss: 0.3011 (0.2918)  classification_loss: 1.6307 (1.5659)  loss_mask: 0.0410 (0.0493)  time: 0.1979  data: 0.0002  max mem: 5511
[07:39:40.465082] Epoch: [30]  [720/781]  eta: 0:00:12  lr: 0.000207  training_loss: 1.9561 (1.9087)  mae_loss: 0.2866 (0.2919)  classification_loss: 1.5971 (1.5672)  loss_mask: 0.0397 (0.0496)  time: 0.1973  data: 0.0002  max mem: 5511
[07:39:44.430174] Epoch: [30]  [740/781]  eta: 0:00:08  lr: 0.000207  training_loss: 1.8618 (1.9076)  mae_loss: 0.2835 (0.2916)  classification_loss: 1.5361 (1.5666)  loss_mask: 0.0328 (0.0494)  time: 0.1982  data: 0.0002  max mem: 5511
[07:39:48.368169] Epoch: [30]  [760/781]  eta: 0:00:04  lr: 0.000207  training_loss: 1.9577 (1.9081)  mae_loss: 0.2931 (0.2915)  classification_loss: 1.5725 (1.5670)  loss_mask: 0.0403 (0.0496)  time: 0.1968  data: 0.0002  max mem: 5511
[07:39:52.316536] Epoch: [30]  [780/781]  eta: 0:00:00  lr: 0.000207  training_loss: 1.8779 (1.9086)  mae_loss: 0.3006 (0.2918)  classification_loss: 1.5490 (1.5674)  loss_mask: 0.0377 (0.0494)  time: 0.1973  data: 0.0002  max mem: 5511
[07:39:52.475171] Epoch: [30] Total time: 0:02:35 (0.1986 s / it)
[07:39:52.475674] Averaged stats: lr: 0.000207  training_loss: 1.8779 (1.9086)  mae_loss: 0.3006 (0.2918)  classification_loss: 1.5490 (1.5674)  loss_mask: 0.0377 (0.0494)
[07:39:53.502826] Test:  [  0/157]  eta: 0:01:25  testing_loss: 0.8998 (0.8998)  acc1: 73.4375 (73.4375)  acc5: 93.7500 (93.7500)  time: 0.5455  data: 0.5157  max mem: 5511
[07:39:53.802143] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.8998 (0.9099)  acc1: 68.7500 (69.0341)  acc5: 98.4375 (98.2955)  time: 0.0765  data: 0.0471  max mem: 5511
[07:39:54.093849] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.9022 (0.8941)  acc1: 68.7500 (69.5685)  acc5: 98.4375 (98.1399)  time: 0.0293  data: 0.0002  max mem: 5511
[07:39:54.386610] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.9022 (0.9019)  acc1: 70.3125 (70.0101)  acc5: 96.8750 (97.6310)  time: 0.0291  data: 0.0002  max mem: 5511
[07:39:54.670317] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.8779 (0.9030)  acc1: 70.3125 (70.4268)  acc5: 96.8750 (97.6753)  time: 0.0287  data: 0.0002  max mem: 5511
[07:39:54.953325] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.8723 (0.8965)  acc1: 71.8750 (70.7414)  acc5: 98.4375 (97.8248)  time: 0.0282  data: 0.0002  max mem: 5511
[07:39:55.238571] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.8723 (0.8950)  acc1: 70.3125 (70.6199)  acc5: 98.4375 (97.9252)  time: 0.0283  data: 0.0002  max mem: 5511
[07:39:55.523210] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.8435 (0.8895)  acc1: 70.3125 (70.6206)  acc5: 98.4375 (97.9093)  time: 0.0284  data: 0.0002  max mem: 5511
[07:39:55.815838] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.8495 (0.8926)  acc1: 68.7500 (70.2932)  acc5: 98.4375 (97.8974)  time: 0.0287  data: 0.0002  max mem: 5511
[07:39:56.111062] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.8596 (0.8919)  acc1: 70.3125 (70.3640)  acc5: 96.8750 (97.9052)  time: 0.0292  data: 0.0002  max mem: 5511
[07:39:56.395094] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.8925 (0.8952)  acc1: 70.3125 (70.2351)  acc5: 96.8750 (97.8651)  time: 0.0288  data: 0.0002  max mem: 5511
[07:39:56.680502] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.9282 (0.8957)  acc1: 68.7500 (70.3125)  acc5: 96.8750 (97.7759)  time: 0.0283  data: 0.0002  max mem: 5511
[07:39:56.962180] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.8622 (0.8926)  acc1: 70.3125 (70.4416)  acc5: 96.8750 (97.7402)  time: 0.0282  data: 0.0001  max mem: 5511
[07:39:57.246962] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.8622 (0.8936)  acc1: 71.8750 (70.4198)  acc5: 96.8750 (97.7457)  time: 0.0282  data: 0.0002  max mem: 5511
[07:39:57.533652] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.9217 (0.8942)  acc1: 71.8750 (70.3901)  acc5: 96.8750 (97.6950)  time: 0.0284  data: 0.0002  max mem: 5511
[07:39:57.814737] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.8977 (0.8909)  acc1: 70.3125 (70.4677)  acc5: 98.4375 (97.6821)  time: 0.0283  data: 0.0001  max mem: 5511
[07:39:57.965945] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.8248 (0.8924)  acc1: 70.3125 (70.4400)  acc5: 98.4375 (97.7000)  time: 0.0273  data: 0.0001  max mem: 5511
[07:39:58.144960] Test: Total time: 0:00:05 (0.0330 s / it)
[07:39:58.145838] * Acc@1 70.440 Acc@5 97.700 loss 0.892
[07:39:58.146167] Accuracy of the network on the 10000 test images: 70.4%
[07:39:58.146385] Max accuracy: 70.73%
[07:39:58.265607] log_dir: ./output_dir
[07:39:59.094290] Epoch: [31]  [  0/781]  eta: 0:10:45  lr: 0.000207  training_loss: 1.9056 (1.9056)  mae_loss: 0.3160 (0.3160)  classification_loss: 1.4979 (1.4979)  loss_mask: 0.0918 (0.0918)  time: 0.8264  data: 0.5904  max mem: 5511
[07:40:03.032682] Epoch: [31]  [ 20/781]  eta: 0:02:52  lr: 0.000207  training_loss: 1.9153 (1.9220)  mae_loss: 0.2956 (0.2978)  classification_loss: 1.5243 (1.5493)  loss_mask: 0.0577 (0.0750)  time: 0.1968  data: 0.0002  max mem: 5511
[07:40:06.967116] Epoch: [31]  [ 40/781]  eta: 0:02:37  lr: 0.000207  training_loss: 1.8682 (1.9100)  mae_loss: 0.2997 (0.2922)  classification_loss: 1.5433 (1.5632)  loss_mask: 0.0283 (0.0545)  time: 0.1966  data: 0.0002  max mem: 5511
[07:40:10.910812] Epoch: [31]  [ 60/781]  eta: 0:02:29  lr: 0.000207  training_loss: 1.8863 (1.9063)  mae_loss: 0.2971 (0.2949)  classification_loss: 1.5494 (1.5696)  loss_mask: 0.0123 (0.0419)  time: 0.1971  data: 0.0002  max mem: 5511
[07:40:14.855813] Epoch: [31]  [ 80/781]  eta: 0:02:23  lr: 0.000206  training_loss: 1.8688 (1.9005)  mae_loss: 0.2832 (0.2933)  classification_loss: 1.5850 (1.5715)  loss_mask: 0.0124 (0.0357)  time: 0.1972  data: 0.0002  max mem: 5511
[07:40:18.793798] Epoch: [31]  [100/781]  eta: 0:02:18  lr: 0.000206  training_loss: 1.9080 (1.9080)  mae_loss: 0.2801 (0.2929)  classification_loss: 1.5870 (1.5735)  loss_mask: 0.0470 (0.0416)  time: 0.1968  data: 0.0002  max mem: 5511
[07:40:22.723974] Epoch: [31]  [120/781]  eta: 0:02:13  lr: 0.000206  training_loss: 1.8882 (1.9094)  mae_loss: 0.2785 (0.2925)  classification_loss: 1.5320 (1.5694)  loss_mask: 0.0592 (0.0475)  time: 0.1964  data: 0.0002  max mem: 5511
[07:40:26.698991] Epoch: [31]  [140/781]  eta: 0:02:09  lr: 0.000206  training_loss: 1.8355 (1.9011)  mae_loss: 0.2866 (0.2919)  classification_loss: 1.5265 (1.5637)  loss_mask: 0.0292 (0.0455)  time: 0.1987  data: 0.0002  max mem: 5511
[07:40:30.631627] Epoch: [31]  [160/781]  eta: 0:02:04  lr: 0.000206  training_loss: 1.8690 (1.8973)  mae_loss: 0.2737 (0.2911)  classification_loss: 1.5767 (1.5640)  loss_mask: 0.0149 (0.0422)  time: 0.1965  data: 0.0002  max mem: 5511
[07:40:34.579130] Epoch: [31]  [180/781]  eta: 0:02:00  lr: 0.000206  training_loss: 1.8670 (1.8958)  mae_loss: 0.2835 (0.2908)  classification_loss: 1.5224 (1.5617)  loss_mask: 0.0375 (0.0433)  time: 0.1973  data: 0.0002  max mem: 5511
[07:40:38.510435] Epoch: [31]  [200/781]  eta: 0:01:56  lr: 0.000206  training_loss: 1.8552 (1.8938)  mae_loss: 0.2843 (0.2901)  classification_loss: 1.5878 (1.5626)  loss_mask: 0.0137 (0.0411)  time: 0.1965  data: 0.0002  max mem: 5511
[07:40:42.471159] Epoch: [31]  [220/781]  eta: 0:01:52  lr: 0.000206  training_loss: 1.9128 (1.8956)  mae_loss: 0.2995 (0.2903)  classification_loss: 1.6066 (1.5645)  loss_mask: 0.0186 (0.0408)  time: 0.1980  data: 0.0002  max mem: 5511
[07:40:46.414216] Epoch: [31]  [240/781]  eta: 0:01:48  lr: 0.000206  training_loss: 1.8248 (1.8931)  mae_loss: 0.3141 (0.2919)  classification_loss: 1.4810 (1.5619)  loss_mask: 0.0128 (0.0393)  time: 0.1971  data: 0.0002  max mem: 5511
[07:40:50.375115] Epoch: [31]  [260/781]  eta: 0:01:43  lr: 0.000206  training_loss: 1.8386 (1.8886)  mae_loss: 0.2877 (0.2916)  classification_loss: 1.5338 (1.5599)  loss_mask: 0.0065 (0.0372)  time: 0.1980  data: 0.0003  max mem: 5511
[07:40:54.356807] Epoch: [31]  [280/781]  eta: 0:01:39  lr: 0.000206  training_loss: 1.8038 (1.8837)  mae_loss: 0.2835 (0.2913)  classification_loss: 1.5136 (1.5571)  loss_mask: 0.0076 (0.0353)  time: 0.1990  data: 0.0002  max mem: 5511
[07:40:58.305603] Epoch: [31]  [300/781]  eta: 0:01:35  lr: 0.000206  training_loss: 1.8828 (1.8842)  mae_loss: 0.2797 (0.2915)  classification_loss: 1.5501 (1.5584)  loss_mask: 0.0071 (0.0343)  time: 0.1974  data: 0.0002  max mem: 5511
[07:41:02.249144] Epoch: [31]  [320/781]  eta: 0:01:31  lr: 0.000205  training_loss: 1.8048 (1.8816)  mae_loss: 0.2782 (0.2917)  classification_loss: 1.5113 (1.5564)  loss_mask: 0.0190 (0.0335)  time: 0.1971  data: 0.0002  max mem: 5511
[07:41:06.177190] Epoch: [31]  [340/781]  eta: 0:01:27  lr: 0.000205  training_loss: 1.8553 (1.8811)  mae_loss: 0.2687 (0.2913)  classification_loss: 1.4675 (1.5543)  loss_mask: 0.0482 (0.0355)  time: 0.1963  data: 0.0002  max mem: 5511
[07:41:10.109125] Epoch: [31]  [360/781]  eta: 0:01:23  lr: 0.000205  training_loss: 1.8521 (1.8801)  mae_loss: 0.2871 (0.2912)  classification_loss: 1.5468 (1.5532)  loss_mask: 0.0281 (0.0356)  time: 0.1965  data: 0.0003  max mem: 5511
[07:41:14.067840] Epoch: [31]  [380/781]  eta: 0:01:19  lr: 0.000205  training_loss: 1.8869 (1.8792)  mae_loss: 0.2996 (0.2912)  classification_loss: 1.5521 (1.5530)  loss_mask: 0.0226 (0.0350)  time: 0.1979  data: 0.0003  max mem: 5511
[07:41:18.015664] Epoch: [31]  [400/781]  eta: 0:01:15  lr: 0.000205  training_loss: 1.8108 (1.8803)  mae_loss: 0.2815 (0.2910)  classification_loss: 1.4697 (1.5496)  loss_mask: 0.0268 (0.0398)  time: 0.1973  data: 0.0002  max mem: 5511
[07:41:22.009740] Epoch: [31]  [420/781]  eta: 0:01:11  lr: 0.000205  training_loss: 2.0019 (1.8866)  mae_loss: 0.2707 (0.2902)  classification_loss: 1.5639 (1.5498)  loss_mask: 0.1401 (0.0465)  time: 0.1996  data: 0.0002  max mem: 5511
[07:41:25.953638] Epoch: [31]  [440/781]  eta: 0:01:07  lr: 0.000205  training_loss: 1.8834 (1.8861)  mae_loss: 0.2933 (0.2905)  classification_loss: 1.5059 (1.5473)  loss_mask: 0.0803 (0.0483)  time: 0.1971  data: 0.0002  max mem: 5511
[07:41:29.894122] Epoch: [31]  [460/781]  eta: 0:01:03  lr: 0.000205  training_loss: 1.8428 (1.8845)  mae_loss: 0.2912 (0.2909)  classification_loss: 1.4983 (1.5449)  loss_mask: 0.0505 (0.0486)  time: 0.1969  data: 0.0002  max mem: 5511
[07:41:33.871606] Epoch: [31]  [480/781]  eta: 0:00:59  lr: 0.000205  training_loss: 1.8825 (1.8851)  mae_loss: 0.2896 (0.2910)  classification_loss: 1.5719 (1.5463)  loss_mask: 0.0267 (0.0479)  time: 0.1988  data: 0.0002  max mem: 5511
[07:41:37.811511] Epoch: [31]  [500/781]  eta: 0:00:55  lr: 0.000205  training_loss: 1.8888 (1.8840)  mae_loss: 0.2840 (0.2910)  classification_loss: 1.5387 (1.5463)  loss_mask: 0.0145 (0.0468)  time: 0.1969  data: 0.0002  max mem: 5511
[07:41:41.746976] Epoch: [31]  [520/781]  eta: 0:00:51  lr: 0.000205  training_loss: 1.8787 (1.8852)  mae_loss: 0.2848 (0.2907)  classification_loss: 1.5229 (1.5470)  loss_mask: 0.0623 (0.0475)  time: 0.1967  data: 0.0002  max mem: 5511
[07:41:45.711624] Epoch: [31]  [540/781]  eta: 0:00:47  lr: 0.000205  training_loss: 1.8685 (1.8842)  mae_loss: 0.2865 (0.2908)  classification_loss: 1.5437 (1.5470)  loss_mask: 0.0166 (0.0465)  time: 0.1981  data: 0.0003  max mem: 5511
[07:41:49.658236] Epoch: [31]  [560/781]  eta: 0:00:43  lr: 0.000204  training_loss: 1.8205 (1.8820)  mae_loss: 0.2896 (0.2909)  classification_loss: 1.5029 (1.5456)  loss_mask: 0.0129 (0.0454)  time: 0.1972  data: 0.0002  max mem: 5511
[07:41:53.583960] Epoch: [31]  [580/781]  eta: 0:00:39  lr: 0.000204  training_loss: 1.8659 (1.8813)  mae_loss: 0.2897 (0.2907)  classification_loss: 1.5240 (1.5450)  loss_mask: 0.0296 (0.0455)  time: 0.1962  data: 0.0002  max mem: 5511
[07:41:57.558023] Epoch: [31]  [600/781]  eta: 0:00:35  lr: 0.000204  training_loss: 1.8107 (1.8810)  mae_loss: 0.2770 (0.2903)  classification_loss: 1.4653 (1.5439)  loss_mask: 0.0636 (0.0467)  time: 0.1986  data: 0.0003  max mem: 5511
[07:42:01.495190] Epoch: [31]  [620/781]  eta: 0:00:31  lr: 0.000204  training_loss: 1.8652 (1.8798)  mae_loss: 0.2938 (0.2904)  classification_loss: 1.5063 (1.5429)  loss_mask: 0.0343 (0.0465)  time: 0.1968  data: 0.0003  max mem: 5511
[07:42:05.447834] Epoch: [31]  [640/781]  eta: 0:00:27  lr: 0.000204  training_loss: 1.8447 (1.8786)  mae_loss: 0.2769 (0.2904)  classification_loss: 1.5067 (1.5421)  loss_mask: 0.0295 (0.0461)  time: 0.1975  data: 0.0003  max mem: 5511
[07:42:09.395893] Epoch: [31]  [660/781]  eta: 0:00:23  lr: 0.000204  training_loss: 1.9176 (1.8797)  mae_loss: 0.2918 (0.2903)  classification_loss: 1.5751 (1.5430)  loss_mask: 0.0277 (0.0464)  time: 0.1973  data: 0.0002  max mem: 5511
[07:42:13.356258] Epoch: [31]  [680/781]  eta: 0:00:20  lr: 0.000204  training_loss: 1.8592 (1.8789)  mae_loss: 0.2910 (0.2904)  classification_loss: 1.5483 (1.5427)  loss_mask: 0.0184 (0.0458)  time: 0.1979  data: 0.0002  max mem: 5511
[07:42:17.278566] Epoch: [31]  [700/781]  eta: 0:00:16  lr: 0.000204  training_loss: 1.8204 (1.8776)  mae_loss: 0.2731 (0.2901)  classification_loss: 1.5117 (1.5420)  loss_mask: 0.0128 (0.0454)  time: 0.1960  data: 0.0002  max mem: 5511
[07:42:21.232810] Epoch: [31]  [720/781]  eta: 0:00:12  lr: 0.000204  training_loss: 1.8867 (1.8777)  mae_loss: 0.2864 (0.2902)  classification_loss: 1.5258 (1.5419)  loss_mask: 0.0409 (0.0456)  time: 0.1976  data: 0.0002  max mem: 5511
[07:42:25.227752] Epoch: [31]  [740/781]  eta: 0:00:08  lr: 0.000204  training_loss: 1.8357 (1.8761)  mae_loss: 0.2817 (0.2901)  classification_loss: 1.5076 (1.5410)  loss_mask: 0.0178 (0.0450)  time: 0.1997  data: 0.0004  max mem: 5511
[07:42:29.176798] Epoch: [31]  [760/781]  eta: 0:00:04  lr: 0.000204  training_loss: 1.8771 (1.8764)  mae_loss: 0.2844 (0.2900)  classification_loss: 1.5939 (1.5418)  loss_mask: 0.0117 (0.0446)  time: 0.1974  data: 0.0002  max mem: 5511
[07:42:33.131431] Epoch: [31]  [780/781]  eta: 0:00:00  lr: 0.000204  training_loss: 1.9119 (1.8769)  mae_loss: 0.2731 (0.2897)  classification_loss: 1.5419 (1.5419)  loss_mask: 0.0650 (0.0453)  time: 0.1976  data: 0.0002  max mem: 5511
[07:42:33.303806] Epoch: [31] Total time: 0:02:35 (0.1985 s / it)
[07:42:33.304525] Averaged stats: lr: 0.000204  training_loss: 1.9119 (1.8769)  mae_loss: 0.2731 (0.2897)  classification_loss: 1.5419 (1.5419)  loss_mask: 0.0650 (0.0453)
[07:42:34.030226] Test:  [  0/157]  eta: 0:01:53  testing_loss: 0.8205 (0.8205)  acc1: 75.0000 (75.0000)  acc5: 95.3125 (95.3125)  time: 0.7207  data: 0.6875  max mem: 5511
[07:42:34.322741] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.8499 (0.8769)  acc1: 71.8750 (71.8750)  acc5: 96.8750 (97.4432)  time: 0.0918  data: 0.0627  max mem: 5511
[07:42:34.621485] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.8184 (0.8466)  acc1: 71.8750 (72.7679)  acc5: 96.8750 (97.6935)  time: 0.0293  data: 0.0002  max mem: 5511
[07:42:34.913545] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.8198 (0.8485)  acc1: 73.4375 (71.9254)  acc5: 98.4375 (97.6310)  time: 0.0294  data: 0.0002  max mem: 5511
[07:42:35.201076] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.8258 (0.8543)  acc1: 73.4375 (72.2561)  acc5: 96.8750 (97.5229)  time: 0.0289  data: 0.0002  max mem: 5511
[07:42:35.498356] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.8147 (0.8466)  acc1: 73.4375 (72.5184)  acc5: 98.4375 (97.7022)  time: 0.0291  data: 0.0002  max mem: 5511
[07:42:35.790508] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.8218 (0.8458)  acc1: 71.8750 (72.2848)  acc5: 96.8750 (97.6178)  time: 0.0293  data: 0.0002  max mem: 5511
[07:42:36.073958] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7865 (0.8353)  acc1: 75.0000 (72.8653)  acc5: 98.4375 (97.7773)  time: 0.0287  data: 0.0002  max mem: 5511
[07:42:36.361267] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7817 (0.8450)  acc1: 75.0000 (72.5116)  acc5: 98.4375 (97.6659)  time: 0.0284  data: 0.0002  max mem: 5511
[07:42:36.649938] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.8517 (0.8443)  acc1: 70.3125 (72.4931)  acc5: 96.8750 (97.6820)  time: 0.0287  data: 0.0002  max mem: 5511
[07:42:36.935638] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.8517 (0.8469)  acc1: 70.3125 (72.4165)  acc5: 98.4375 (97.7568)  time: 0.0286  data: 0.0002  max mem: 5511
[07:42:37.218363] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.8774 (0.8512)  acc1: 70.3125 (72.2832)  acc5: 98.4375 (97.7055)  time: 0.0283  data: 0.0002  max mem: 5511
[07:42:37.501794] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.8304 (0.8454)  acc1: 71.8750 (72.4174)  acc5: 98.4375 (97.7144)  time: 0.0282  data: 0.0003  max mem: 5511
[07:42:37.785796] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.8301 (0.8451)  acc1: 71.8750 (72.3640)  acc5: 98.4375 (97.7219)  time: 0.0282  data: 0.0003  max mem: 5511
[07:42:38.067913] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.8390 (0.8454)  acc1: 71.8750 (72.4512)  acc5: 98.4375 (97.7394)  time: 0.0282  data: 0.0003  max mem: 5511
[07:42:38.348382] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.8343 (0.8436)  acc1: 71.8750 (72.4338)  acc5: 98.4375 (97.8063)  time: 0.0280  data: 0.0001  max mem: 5511
[07:42:38.500528] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.8300 (0.8445)  acc1: 73.4375 (72.4000)  acc5: 98.4375 (97.8300)  time: 0.0271  data: 0.0001  max mem: 5511
[07:42:38.652633] Test: Total time: 0:00:05 (0.0340 s / it)
[07:42:38.653501] * Acc@1 72.400 Acc@5 97.830 loss 0.844
[07:42:38.653845] Accuracy of the network on the 10000 test images: 72.4%
[07:42:38.654098] Max accuracy: 72.40%
[07:42:38.803233] log_dir: ./output_dir
[07:42:39.716023] Epoch: [32]  [  0/781]  eta: 0:11:51  lr: 0.000204  training_loss: 1.7678 (1.7678)  mae_loss: 0.2916 (0.2916)  classification_loss: 1.4018 (1.4018)  loss_mask: 0.0743 (0.0743)  time: 0.9112  data: 0.6855  max mem: 5511
[07:42:43.694032] Epoch: [32]  [ 20/781]  eta: 0:02:57  lr: 0.000204  training_loss: 1.8273 (1.8329)  mae_loss: 0.2774 (0.2895)  classification_loss: 1.5015 (1.5209)  loss_mask: 0.0162 (0.0224)  time: 0.1988  data: 0.0002  max mem: 5511
[07:42:47.657033] Epoch: [32]  [ 40/781]  eta: 0:02:39  lr: 0.000203  training_loss: 1.8022 (1.8153)  mae_loss: 0.2744 (0.2804)  classification_loss: 1.5394 (1.5183)  loss_mask: 0.0093 (0.0166)  time: 0.1980  data: 0.0002  max mem: 5511
[07:42:51.642328] Epoch: [32]  [ 60/781]  eta: 0:02:31  lr: 0.000203  training_loss: 1.8543 (1.8274)  mae_loss: 0.2810 (0.2849)  classification_loss: 1.5412 (1.5293)  loss_mask: 0.0050 (0.0132)  time: 0.1992  data: 0.0003  max mem: 5511
[07:42:55.569664] Epoch: [32]  [ 80/781]  eta: 0:02:25  lr: 0.000203  training_loss: 1.8511 (1.8287)  mae_loss: 0.2816 (0.2858)  classification_loss: 1.5444 (1.5318)  loss_mask: 0.0035 (0.0111)  time: 0.1963  data: 0.0002  max mem: 5511
[07:42:59.517713] Epoch: [32]  [100/781]  eta: 0:02:19  lr: 0.000203  training_loss: 1.8181 (1.8289)  mae_loss: 0.2705 (0.2843)  classification_loss: 1.5443 (1.5350)  loss_mask: 0.0026 (0.0096)  time: 0.1973  data: 0.0002  max mem: 5511
[07:43:03.485933] Epoch: [32]  [120/781]  eta: 0:02:14  lr: 0.000203  training_loss: 1.7776 (1.8261)  mae_loss: 0.2764 (0.2840)  classification_loss: 1.4837 (1.5335)  loss_mask: 0.0033 (0.0086)  time: 0.1983  data: 0.0003  max mem: 5511
[07:43:07.462236] Epoch: [32]  [140/781]  eta: 0:02:10  lr: 0.000203  training_loss: 1.7602 (1.8242)  mae_loss: 0.2730 (0.2839)  classification_loss: 1.4993 (1.5317)  loss_mask: 0.0031 (0.0086)  time: 0.1987  data: 0.0002  max mem: 5511
[07:43:11.406140] Epoch: [32]  [160/781]  eta: 0:02:05  lr: 0.000203  training_loss: 1.7750 (1.8181)  mae_loss: 0.2754 (0.2824)  classification_loss: 1.4678 (1.5271)  loss_mask: 0.0037 (0.0085)  time: 0.1971  data: 0.0002  max mem: 5511
[07:43:15.418435] Epoch: [32]  [180/781]  eta: 0:02:01  lr: 0.000203  training_loss: 1.7664 (1.8180)  mae_loss: 0.2814 (0.2830)  classification_loss: 1.4533 (1.5267)  loss_mask: 0.0041 (0.0082)  time: 0.2005  data: 0.0002  max mem: 5511
[07:43:19.371585] Epoch: [32]  [200/781]  eta: 0:01:57  lr: 0.000203  training_loss: 1.8057 (1.8193)  mae_loss: 0.2875 (0.2839)  classification_loss: 1.5241 (1.5261)  loss_mask: 0.0120 (0.0093)  time: 0.1976  data: 0.0002  max mem: 5511
[07:43:23.312693] Epoch: [32]  [220/781]  eta: 0:01:52  lr: 0.000203  training_loss: 1.9111 (1.8304)  mae_loss: 0.2724 (0.2833)  classification_loss: 1.5272 (1.5271)  loss_mask: 0.0921 (0.0200)  time: 0.1970  data: 0.0002  max mem: 5511
[07:43:27.285075] Epoch: [32]  [240/781]  eta: 0:01:48  lr: 0.000203  training_loss: 1.9214 (1.8342)  mae_loss: 0.2741 (0.2828)  classification_loss: 1.5619 (1.5279)  loss_mask: 0.0465 (0.0236)  time: 0.1985  data: 0.0002  max mem: 5511
[07:43:31.244433] Epoch: [32]  [260/781]  eta: 0:01:44  lr: 0.000203  training_loss: 1.7926 (1.8330)  mae_loss: 0.2804 (0.2824)  classification_loss: 1.5091 (1.5265)  loss_mask: 0.0231 (0.0241)  time: 0.1979  data: 0.0002  max mem: 5511
[07:43:35.185529] Epoch: [32]  [280/781]  eta: 0:01:40  lr: 0.000202  training_loss: 1.8726 (1.8348)  mae_loss: 0.2859 (0.2829)  classification_loss: 1.5624 (1.5287)  loss_mask: 0.0080 (0.0232)  time: 0.1969  data: 0.0002  max mem: 5511
[07:43:39.141728] Epoch: [32]  [300/781]  eta: 0:01:36  lr: 0.000202  training_loss: 1.8413 (1.8365)  mae_loss: 0.2958 (0.2838)  classification_loss: 1.5425 (1.5300)  loss_mask: 0.0122 (0.0227)  time: 0.1977  data: 0.0002  max mem: 5511
[07:43:43.099249] Epoch: [32]  [320/781]  eta: 0:01:32  lr: 0.000202  training_loss: 1.8062 (1.8346)  mae_loss: 0.2753 (0.2837)  classification_loss: 1.4823 (1.5274)  loss_mask: 0.0287 (0.0235)  time: 0.1978  data: 0.0002  max mem: 5511
[07:43:47.046360] Epoch: [32]  [340/781]  eta: 0:01:28  lr: 0.000202  training_loss: 1.8518 (1.8357)  mae_loss: 0.2825 (0.2838)  classification_loss: 1.5297 (1.5283)  loss_mask: 0.0154 (0.0236)  time: 0.1973  data: 0.0002  max mem: 5511
[07:43:51.002668] Epoch: [32]  [360/781]  eta: 0:01:24  lr: 0.000202  training_loss: 1.8130 (1.8331)  mae_loss: 0.2736 (0.2833)  classification_loss: 1.5037 (1.5269)  loss_mask: 0.0073 (0.0229)  time: 0.1977  data: 0.0002  max mem: 5511
[07:43:54.947652] Epoch: [32]  [380/781]  eta: 0:01:20  lr: 0.000202  training_loss: 1.8446 (1.8351)  mae_loss: 0.2763 (0.2833)  classification_loss: 1.5226 (1.5268)  loss_mask: 0.0270 (0.0250)  time: 0.1972  data: 0.0002  max mem: 5511
[07:43:58.922650] Epoch: [32]  [400/781]  eta: 0:01:16  lr: 0.000202  training_loss: 1.8544 (1.8352)  mae_loss: 0.2571 (0.2830)  classification_loss: 1.5081 (1.5270)  loss_mask: 0.0258 (0.0251)  time: 0.1987  data: 0.0003  max mem: 5511
[07:44:02.874947] Epoch: [32]  [420/781]  eta: 0:01:12  lr: 0.000202  training_loss: 1.9067 (1.8384)  mae_loss: 0.2966 (0.2835)  classification_loss: 1.5757 (1.5283)  loss_mask: 0.0370 (0.0266)  time: 0.1975  data: 0.0003  max mem: 5511
[07:44:06.833731] Epoch: [32]  [440/781]  eta: 0:01:08  lr: 0.000202  training_loss: 1.8154 (1.8387)  mae_loss: 0.2965 (0.2839)  classification_loss: 1.5114 (1.5284)  loss_mask: 0.0183 (0.0263)  time: 0.1978  data: 0.0002  max mem: 5511
[07:44:10.772199] Epoch: [32]  [460/781]  eta: 0:01:04  lr: 0.000202  training_loss: 1.7533 (1.8355)  mae_loss: 0.2729 (0.2834)  classification_loss: 1.4621 (1.5259)  loss_mask: 0.0170 (0.0263)  time: 0.1968  data: 0.0002  max mem: 5511
[07:44:14.719732] Epoch: [32]  [480/781]  eta: 0:00:59  lr: 0.000202  training_loss: 1.8653 (1.8381)  mae_loss: 0.2813 (0.2832)  classification_loss: 1.5551 (1.5265)  loss_mask: 0.0402 (0.0284)  time: 0.1973  data: 0.0002  max mem: 5511
[07:44:18.655664] Epoch: [32]  [500/781]  eta: 0:00:55  lr: 0.000202  training_loss: 1.8746 (1.8396)  mae_loss: 0.2697 (0.2827)  classification_loss: 1.5100 (1.5271)  loss_mask: 0.0438 (0.0299)  time: 0.1967  data: 0.0002  max mem: 5511
[07:44:22.584190] Epoch: [32]  [520/781]  eta: 0:00:51  lr: 0.000201  training_loss: 1.8068 (1.8401)  mae_loss: 0.2854 (0.2828)  classification_loss: 1.5199 (1.5277)  loss_mask: 0.0183 (0.0296)  time: 0.1963  data: 0.0002  max mem: 5511
[07:44:26.526924] Epoch: [32]  [540/781]  eta: 0:00:47  lr: 0.000201  training_loss: 1.8217 (1.8402)  mae_loss: 0.2848 (0.2831)  classification_loss: 1.5394 (1.5281)  loss_mask: 0.0088 (0.0290)  time: 0.1971  data: 0.0002  max mem: 5511
[07:44:30.467752] Epoch: [32]  [560/781]  eta: 0:00:43  lr: 0.000201  training_loss: 1.7880 (1.8384)  mae_loss: 0.2738 (0.2829)  classification_loss: 1.4980 (1.5272)  loss_mask: 0.0086 (0.0283)  time: 0.1970  data: 0.0003  max mem: 5511
[07:44:34.379364] Epoch: [32]  [580/781]  eta: 0:00:39  lr: 0.000201  training_loss: 1.7651 (1.8371)  mae_loss: 0.2776 (0.2828)  classification_loss: 1.5034 (1.5266)  loss_mask: 0.0061 (0.0276)  time: 0.1955  data: 0.0003  max mem: 5511
[07:44:38.315417] Epoch: [32]  [600/781]  eta: 0:00:35  lr: 0.000201  training_loss: 1.8086 (1.8365)  mae_loss: 0.2842 (0.2830)  classification_loss: 1.5165 (1.5267)  loss_mask: 0.0034 (0.0268)  time: 0.1967  data: 0.0002  max mem: 5511
[07:44:42.248078] Epoch: [32]  [620/781]  eta: 0:00:31  lr: 0.000201  training_loss: 1.7939 (1.8354)  mae_loss: 0.2923 (0.2834)  classification_loss: 1.4959 (1.5259)  loss_mask: 0.0024 (0.0261)  time: 0.1966  data: 0.0002  max mem: 5511
[07:44:46.195459] Epoch: [32]  [640/781]  eta: 0:00:28  lr: 0.000201  training_loss: 1.7805 (1.8342)  mae_loss: 0.2757 (0.2832)  classification_loss: 1.4992 (1.5255)  loss_mask: 0.0024 (0.0254)  time: 0.1973  data: 0.0003  max mem: 5511
[07:44:50.129172] Epoch: [32]  [660/781]  eta: 0:00:24  lr: 0.000201  training_loss: 1.8536 (1.8355)  mae_loss: 0.2853 (0.2835)  classification_loss: 1.5536 (1.5271)  loss_mask: 0.0045 (0.0248)  time: 0.1966  data: 0.0002  max mem: 5511
[07:44:54.051152] Epoch: [32]  [680/781]  eta: 0:00:20  lr: 0.000201  training_loss: 1.8012 (1.8342)  mae_loss: 0.2915 (0.2838)  classification_loss: 1.4996 (1.5263)  loss_mask: 0.0024 (0.0242)  time: 0.1960  data: 0.0002  max mem: 5511
[07:44:58.024139] Epoch: [32]  [700/781]  eta: 0:00:16  lr: 0.000201  training_loss: 1.8272 (1.8343)  mae_loss: 0.2831 (0.2838)  classification_loss: 1.5331 (1.5269)  loss_mask: 0.0014 (0.0235)  time: 0.1986  data: 0.0003  max mem: 5511
[07:45:01.976283] Epoch: [32]  [720/781]  eta: 0:00:12  lr: 0.000201  training_loss: 1.7792 (1.8336)  mae_loss: 0.2808 (0.2838)  classification_loss: 1.5343 (1.5268)  loss_mask: 0.0016 (0.0230)  time: 0.1975  data: 0.0002  max mem: 5511
[07:45:05.920643] Epoch: [32]  [740/781]  eta: 0:00:08  lr: 0.000201  training_loss: 1.7999 (1.8321)  mae_loss: 0.2796 (0.2837)  classification_loss: 1.4993 (1.5255)  loss_mask: 0.0029 (0.0229)  time: 0.1971  data: 0.0002  max mem: 5511
[07:45:09.861789] Epoch: [32]  [760/781]  eta: 0:00:04  lr: 0.000200  training_loss: 1.8726 (1.8325)  mae_loss: 0.2713 (0.2836)  classification_loss: 1.5575 (1.5262)  loss_mask: 0.0076 (0.0227)  time: 0.1969  data: 0.0002  max mem: 5511
[07:45:13.781676] Epoch: [32]  [780/781]  eta: 0:00:00  lr: 0.000200  training_loss: 1.8906 (1.8363)  mae_loss: 0.2760 (0.2836)  classification_loss: 1.5458 (1.5272)  loss_mask: 0.0339 (0.0255)  time: 0.1959  data: 0.0002  max mem: 5511
[07:45:13.968777] Epoch: [32] Total time: 0:02:35 (0.1987 s / it)
[07:45:13.969318] Averaged stats: lr: 0.000200  training_loss: 1.8906 (1.8363)  mae_loss: 0.2760 (0.2836)  classification_loss: 1.5458 (1.5272)  loss_mask: 0.0339 (0.0255)
[07:45:14.651538] Test:  [  0/157]  eta: 0:01:46  testing_loss: 0.8952 (0.8952)  acc1: 68.7500 (68.7500)  acc5: 98.4375 (98.4375)  time: 0.6763  data: 0.6366  max mem: 5511
[07:45:14.941008] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.9216 (0.9465)  acc1: 68.7500 (69.0341)  acc5: 98.4375 (97.7273)  time: 0.0875  data: 0.0580  max mem: 5511
[07:45:15.229773] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.9197 (0.9190)  acc1: 68.7500 (70.0149)  acc5: 98.4375 (97.7679)  time: 0.0287  data: 0.0002  max mem: 5511
[07:45:15.516384] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.9161 (0.9249)  acc1: 68.7500 (69.5565)  acc5: 98.4375 (97.5302)  time: 0.0286  data: 0.0002  max mem: 5511
[07:45:15.799784] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.9083 (0.9227)  acc1: 70.3125 (69.1692)  acc5: 96.8750 (97.3323)  time: 0.0284  data: 0.0002  max mem: 5511
[07:45:16.084799] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.8945 (0.9107)  acc1: 70.3125 (69.6691)  acc5: 96.8750 (97.3958)  time: 0.0283  data: 0.0002  max mem: 5511
[07:45:16.369214] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.8734 (0.9092)  acc1: 70.3125 (69.6977)  acc5: 96.8750 (97.3105)  time: 0.0283  data: 0.0002  max mem: 5511
[07:45:16.655407] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.8608 (0.9006)  acc1: 71.8750 (70.0924)  acc5: 98.4375 (97.4252)  time: 0.0284  data: 0.0002  max mem: 5511
[07:45:16.938384] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.8898 (0.9044)  acc1: 70.3125 (70.0424)  acc5: 98.4375 (97.3765)  time: 0.0283  data: 0.0002  max mem: 5511
[07:45:17.226825] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.9106 (0.9046)  acc1: 70.3125 (70.1065)  acc5: 96.8750 (97.4245)  time: 0.0285  data: 0.0003  max mem: 5511
[07:45:17.513254] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.8979 (0.9070)  acc1: 70.3125 (70.0340)  acc5: 98.4375 (97.4783)  time: 0.0286  data: 0.0003  max mem: 5511
[07:45:17.801343] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.9026 (0.9083)  acc1: 68.7500 (69.9606)  acc5: 96.8750 (97.4099)  time: 0.0286  data: 0.0002  max mem: 5511
[07:45:18.093723] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.8977 (0.9032)  acc1: 70.3125 (70.0155)  acc5: 96.8750 (97.4044)  time: 0.0289  data: 0.0004  max mem: 5511
[07:45:18.396998] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.8831 (0.9062)  acc1: 68.7500 (69.9189)  acc5: 96.8750 (97.3760)  time: 0.0296  data: 0.0008  max mem: 5511
[07:45:18.682631] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.9186 (0.9054)  acc1: 68.7500 (70.0022)  acc5: 96.8750 (97.4291)  time: 0.0293  data: 0.0006  max mem: 5511
[07:45:18.963936] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.8980 (0.9029)  acc1: 71.8750 (70.0952)  acc5: 98.4375 (97.3924)  time: 0.0282  data: 0.0002  max mem: 5511
[07:45:19.115339] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.8861 (0.9041)  acc1: 71.8750 (70.0400)  acc5: 96.8750 (97.3800)  time: 0.0272  data: 0.0001  max mem: 5511
[07:45:19.285821] Test: Total time: 0:00:05 (0.0338 s / it)
[07:45:19.286260] * Acc@1 70.040 Acc@5 97.380 loss 0.904
[07:45:19.286544] Accuracy of the network on the 10000 test images: 70.0%
[07:45:19.286740] Max accuracy: 72.40%
[07:45:19.508655] log_dir: ./output_dir
[07:45:20.483341] Epoch: [33]  [  0/781]  eta: 0:12:39  lr: 0.000200  training_loss: 1.9039 (1.9039)  mae_loss: 0.2785 (0.2785)  classification_loss: 1.4547 (1.4547)  loss_mask: 0.1708 (0.1708)  time: 0.9729  data: 0.7523  max mem: 5511
[07:45:24.424534] Epoch: [33]  [ 20/781]  eta: 0:02:58  lr: 0.000200  training_loss: 1.9408 (1.9351)  mae_loss: 0.2728 (0.2793)  classification_loss: 1.5188 (1.5320)  loss_mask: 0.1095 (0.1238)  time: 0.1970  data: 0.0002  max mem: 5511
[07:45:28.388829] Epoch: [33]  [ 40/781]  eta: 0:02:40  lr: 0.000200  training_loss: 1.8823 (1.9222)  mae_loss: 0.2853 (0.2831)  classification_loss: 1.5256 (1.5356)  loss_mask: 0.0710 (0.1035)  time: 0.1981  data: 0.0003  max mem: 5511
[07:45:32.331388] Epoch: [33]  [ 60/781]  eta: 0:02:31  lr: 0.000200  training_loss: 1.9252 (1.9251)  mae_loss: 0.2915 (0.2834)  classification_loss: 1.5396 (1.5395)  loss_mask: 0.0821 (0.1022)  time: 0.1971  data: 0.0002  max mem: 5511
[07:45:36.318250] Epoch: [33]  [ 80/781]  eta: 0:02:25  lr: 0.000200  training_loss: 1.8950 (1.9175)  mae_loss: 0.2601 (0.2804)  classification_loss: 1.5828 (1.5456)  loss_mask: 0.0475 (0.0916)  time: 0.1993  data: 0.0002  max mem: 5511
[07:45:40.289785] Epoch: [33]  [100/781]  eta: 0:02:20  lr: 0.000200  training_loss: 1.8573 (1.9037)  mae_loss: 0.2590 (0.2777)  classification_loss: 1.5176 (1.5401)  loss_mask: 0.0495 (0.0859)  time: 0.1985  data: 0.0002  max mem: 5511
[07:45:44.224959] Epoch: [33]  [120/781]  eta: 0:02:14  lr: 0.000200  training_loss: 1.7958 (1.8856)  mae_loss: 0.2642 (0.2770)  classification_loss: 1.5068 (1.5331)  loss_mask: 0.0203 (0.0754)  time: 0.1967  data: 0.0002  max mem: 5511
[07:45:48.161809] Epoch: [33]  [140/781]  eta: 0:02:10  lr: 0.000200  training_loss: 1.8210 (1.8818)  mae_loss: 0.2786 (0.2779)  classification_loss: 1.5159 (1.5341)  loss_mask: 0.0228 (0.0699)  time: 0.1967  data: 0.0002  max mem: 5511
[07:45:52.110800] Epoch: [33]  [160/781]  eta: 0:02:05  lr: 0.000200  training_loss: 1.8492 (1.8760)  mae_loss: 0.2657 (0.2770)  classification_loss: 1.5028 (1.5300)  loss_mask: 0.0399 (0.0690)  time: 0.1974  data: 0.0002  max mem: 5511
[07:45:56.053404] Epoch: [33]  [180/781]  eta: 0:02:01  lr: 0.000200  training_loss: 1.8024 (1.8695)  mae_loss: 0.2771 (0.2777)  classification_loss: 1.4617 (1.5253)  loss_mask: 0.0347 (0.0665)  time: 0.1971  data: 0.0002  max mem: 5511
[07:46:00.000325] Epoch: [33]  [200/781]  eta: 0:01:56  lr: 0.000199  training_loss: 1.8142 (1.8645)  mae_loss: 0.2611 (0.2780)  classification_loss: 1.4679 (1.5251)  loss_mask: 0.0155 (0.0615)  time: 0.1973  data: 0.0003  max mem: 5511
[07:46:03.943501] Epoch: [33]  [220/781]  eta: 0:01:52  lr: 0.000199  training_loss: 1.7962 (1.8594)  mae_loss: 0.2603 (0.2771)  classification_loss: 1.5185 (1.5252)  loss_mask: 0.0069 (0.0571)  time: 0.1971  data: 0.0003  max mem: 5511
[07:46:07.890920] Epoch: [33]  [240/781]  eta: 0:01:48  lr: 0.000199  training_loss: 1.8007 (1.8560)  mae_loss: 0.2836 (0.2778)  classification_loss: 1.5024 (1.5247)  loss_mask: 0.0089 (0.0534)  time: 0.1973  data: 0.0002  max mem: 5511
[07:46:11.857419] Epoch: [33]  [260/781]  eta: 0:01:44  lr: 0.000199  training_loss: 1.8334 (1.8550)  mae_loss: 0.2887 (0.2791)  classification_loss: 1.5069 (1.5253)  loss_mask: 0.0090 (0.0506)  time: 0.1982  data: 0.0003  max mem: 5511
[07:46:15.798799] Epoch: [33]  [280/781]  eta: 0:01:40  lr: 0.000199  training_loss: 1.9322 (1.8576)  mae_loss: 0.2860 (0.2800)  classification_loss: 1.5289 (1.5255)  loss_mask: 0.0533 (0.0521)  time: 0.1970  data: 0.0002  max mem: 5511
[07:46:19.726525] Epoch: [33]  [300/781]  eta: 0:01:36  lr: 0.000199  training_loss: 1.8819 (1.8595)  mae_loss: 0.2794 (0.2801)  classification_loss: 1.5238 (1.5256)  loss_mask: 0.0670 (0.0538)  time: 0.1963  data: 0.0002  max mem: 5511
[07:46:23.671778] Epoch: [33]  [320/781]  eta: 0:01:32  lr: 0.000199  training_loss: 1.9348 (1.8619)  mae_loss: 0.2760 (0.2800)  classification_loss: 1.5445 (1.5264)  loss_mask: 0.0633 (0.0555)  time: 0.1972  data: 0.0002  max mem: 5511
[07:46:27.648118] Epoch: [33]  [340/781]  eta: 0:01:28  lr: 0.000199  training_loss: 1.8436 (1.8619)  mae_loss: 0.2716 (0.2797)  classification_loss: 1.4928 (1.5255)  loss_mask: 0.0687 (0.0567)  time: 0.1987  data: 0.0002  max mem: 5511
[07:46:31.619688] Epoch: [33]  [360/781]  eta: 0:01:24  lr: 0.000199  training_loss: 1.9121 (1.8638)  mae_loss: 0.2843 (0.2802)  classification_loss: 1.5777 (1.5275)  loss_mask: 0.0355 (0.0560)  time: 0.1985  data: 0.0003  max mem: 5511
[07:46:35.581876] Epoch: [33]  [380/781]  eta: 0:01:20  lr: 0.000199  training_loss: 1.8390 (1.8624)  mae_loss: 0.2650 (0.2796)  classification_loss: 1.5424 (1.5273)  loss_mask: 0.0391 (0.0555)  time: 0.1979  data: 0.0002  max mem: 5511
[07:46:39.527489] Epoch: [33]  [400/781]  eta: 0:01:15  lr: 0.000199  training_loss: 1.8101 (1.8605)  mae_loss: 0.2900 (0.2802)  classification_loss: 1.5072 (1.5260)  loss_mask: 0.0154 (0.0543)  time: 0.1972  data: 0.0003  max mem: 5511
[07:46:43.492416] Epoch: [33]  [420/781]  eta: 0:01:11  lr: 0.000199  training_loss: 1.8492 (1.8604)  mae_loss: 0.2964 (0.2806)  classification_loss: 1.5332 (1.5257)  loss_mask: 0.0406 (0.0541)  time: 0.1982  data: 0.0002  max mem: 5511
[07:46:47.431044] Epoch: [33]  [440/781]  eta: 0:01:07  lr: 0.000198  training_loss: 1.8112 (1.8586)  mae_loss: 0.2782 (0.2806)  classification_loss: 1.5332 (1.5256)  loss_mask: 0.0116 (0.0523)  time: 0.1969  data: 0.0002  max mem: 5511
[07:46:51.356536] Epoch: [33]  [460/781]  eta: 0:01:03  lr: 0.000198  training_loss: 1.7898 (1.8554)  mae_loss: 0.2735 (0.2803)  classification_loss: 1.4936 (1.5247)  loss_mask: 0.0073 (0.0504)  time: 0.1962  data: 0.0002  max mem: 5511
[07:46:55.304738] Epoch: [33]  [480/781]  eta: 0:00:59  lr: 0.000198  training_loss: 1.8157 (1.8546)  mae_loss: 0.2663 (0.2800)  classification_loss: 1.5395 (1.5262)  loss_mask: 0.0046 (0.0485)  time: 0.1973  data: 0.0002  max mem: 5511
[07:46:59.251141] Epoch: [33]  [500/781]  eta: 0:00:55  lr: 0.000198  training_loss: 1.7908 (1.8519)  mae_loss: 0.2793 (0.2801)  classification_loss: 1.4993 (1.5251)  loss_mask: 0.0035 (0.0467)  time: 0.1972  data: 0.0002  max mem: 5511
[07:47:03.176336] Epoch: [33]  [520/781]  eta: 0:00:51  lr: 0.000198  training_loss: 1.7920 (1.8511)  mae_loss: 0.2895 (0.2805)  classification_loss: 1.5235 (1.5251)  loss_mask: 0.0046 (0.0455)  time: 0.1962  data: 0.0003  max mem: 5511
[07:47:07.153616] Epoch: [33]  [540/781]  eta: 0:00:47  lr: 0.000198  training_loss: 1.8445 (1.8521)  mae_loss: 0.2911 (0.2809)  classification_loss: 1.5303 (1.5262)  loss_mask: 0.0219 (0.0450)  time: 0.1988  data: 0.0004  max mem: 5511
[07:47:11.148985] Epoch: [33]  [560/781]  eta: 0:00:43  lr: 0.000198  training_loss: 1.7983 (1.8504)  mae_loss: 0.2680 (0.2807)  classification_loss: 1.5009 (1.5254)  loss_mask: 0.0166 (0.0444)  time: 0.1997  data: 0.0002  max mem: 5511
[07:47:15.094024] Epoch: [33]  [580/781]  eta: 0:00:39  lr: 0.000198  training_loss: 1.8324 (1.8503)  mae_loss: 0.2831 (0.2808)  classification_loss: 1.5316 (1.5258)  loss_mask: 0.0141 (0.0438)  time: 0.1972  data: 0.0004  max mem: 5511

[07:47:19.039674] Epoch: [33]  [600/781]  eta: 0:00:35  lr: 0.000198  training_loss: 1.9077 (1.8520)  mae_loss: 0.2840 (0.2811)  classification_loss: 1.5235 (1.5270)  loss_mask: 0.0359 (0.0439)  time: 0.1972  data: 0.0002  max mem: 5511
[07:47:22.973560] Epoch: [33]  [620/781]  eta: 0:00:31  lr: 0.000198  training_loss: 1.9577 (1.8560)  mae_loss: 0.2937 (0.2816)  classification_loss: 1.5295 (1.5277)  loss_mask: 0.1073 (0.0466)  time: 0.1966  data: 0.0002  max mem: 5511
[07:47:26.910555] Epoch: [33]  [640/781]  eta: 0:00:28  lr: 0.000198  training_loss: 1.8483 (1.8560)  mae_loss: 0.2694 (0.2815)  classification_loss: 1.4867 (1.5265)  loss_mask: 0.0718 (0.0480)  time: 0.1968  data: 0.0003  max mem: 5511
[07:47:30.862091] Epoch: [33]  [660/781]  eta: 0:00:24  lr: 0.000198  training_loss: 1.9168 (1.8577)  mae_loss: 0.2894 (0.2818)  classification_loss: 1.5353 (1.5270)  loss_mask: 0.0494 (0.0488)  time: 0.1975  data: 0.0002  max mem: 5511
[07:47:34.807774] Epoch: [33]  [680/781]  eta: 0:00:20  lr: 0.000197  training_loss: 1.9323 (1.8616)  mae_loss: 0.2849 (0.2820)  classification_loss: 1.5277 (1.5274)  loss_mask: 0.1250 (0.0523)  time: 0.1972  data: 0.0002  max mem: 5511
[07:47:38.753031] Epoch: [33]  [700/781]  eta: 0:00:16  lr: 0.000197  training_loss: 1.8599 (1.8621)  mae_loss: 0.2790 (0.2823)  classification_loss: 1.4933 (1.5274)  loss_mask: 0.0516 (0.0524)  time: 0.1972  data: 0.0002  max mem: 5511
[07:47:42.686041] Epoch: [33]  [720/781]  eta: 0:00:12  lr: 0.000197  training_loss: 1.8709 (1.8630)  mae_loss: 0.2858 (0.2826)  classification_loss: 1.5412 (1.5284)  loss_mask: 0.0255 (0.0519)  time: 0.1965  data: 0.0002  max mem: 5511
[07:47:46.615469] Epoch: [33]  [740/781]  eta: 0:00:08  lr: 0.000197  training_loss: 1.7846 (1.8609)  mae_loss: 0.2765 (0.2825)  classification_loss: 1.4923 (1.5272)  loss_mask: 0.0161 (0.0512)  time: 0.1964  data: 0.0002  max mem: 5511
[07:47:50.579853] Epoch: [33]  [760/781]  eta: 0:00:04  lr: 0.000197  training_loss: 1.8674 (1.8617)  mae_loss: 0.2967 (0.2826)  classification_loss: 1.5565 (1.5284)  loss_mask: 0.0257 (0.0507)  time: 0.1981  data: 0.0002  max mem: 5511
[07:47:54.531200] Epoch: [33]  [780/781]  eta: 0:00:00  lr: 0.000197  training_loss: 1.8310 (1.8615)  mae_loss: 0.2851 (0.2827)  classification_loss: 1.5268 (1.5286)  loss_mask: 0.0165 (0.0501)  time: 0.1975  data: 0.0002  max mem: 5511
[07:47:54.706540] Epoch: [33] Total time: 0:02:35 (0.1987 s / it)
[07:47:54.706998] Averaged stats: lr: 0.000197  training_loss: 1.8310 (1.8615)  mae_loss: 0.2851 (0.2827)  classification_loss: 1.5268 (1.5286)  loss_mask: 0.0165 (0.0501)
[07:47:55.377036] Test:  [  0/157]  eta: 0:01:44  testing_loss: 0.8137 (0.8137)  acc1: 78.1250 (78.1250)  acc5: 96.8750 (96.8750)  time: 0.6651  data: 0.6312  max mem: 5511
[07:47:55.667588] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.8224 (0.8846)  acc1: 71.8750 (70.7386)  acc5: 98.4375 (97.8693)  time: 0.0865  data: 0.0576  max mem: 5511
[07:47:55.954564] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.7922 (0.8448)  acc1: 73.4375 (72.3958)  acc5: 98.4375 (98.0655)  time: 0.0286  data: 0.0002  max mem: 5511
[07:47:56.247982] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.8033 (0.8513)  acc1: 75.0000 (72.0766)  acc5: 98.4375 (97.8327)  time: 0.0289  data: 0.0002  max mem: 5511
[07:47:56.536691] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.8511 (0.8560)  acc1: 70.3125 (71.7607)  acc5: 98.4375 (97.8659)  time: 0.0290  data: 0.0003  max mem: 5511
[07:47:56.825017] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.8412 (0.8481)  acc1: 70.3125 (72.0282)  acc5: 98.4375 (97.8554)  time: 0.0287  data: 0.0002  max mem: 5511
[07:47:57.113218] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.8333 (0.8498)  acc1: 73.4375 (71.7982)  acc5: 98.4375 (97.8740)  time: 0.0287  data: 0.0002  max mem: 5511
[07:47:57.408192] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.8086 (0.8400)  acc1: 73.4375 (72.1171)  acc5: 98.4375 (97.9093)  time: 0.0290  data: 0.0006  max mem: 5511
[07:47:57.696131] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.8157 (0.8470)  acc1: 71.8750 (71.8557)  acc5: 98.4375 (97.7816)  time: 0.0290  data: 0.0006  max mem: 5511
[07:47:57.986294] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.8737 (0.8482)  acc1: 71.8750 (72.0467)  acc5: 98.4375 (97.8709)  time: 0.0288  data: 0.0002  max mem: 5511
[07:47:58.273820] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.8384 (0.8516)  acc1: 71.8750 (71.9214)  acc5: 98.4375 (97.9425)  time: 0.0287  data: 0.0002  max mem: 5511
[07:47:58.556519] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.8728 (0.8528)  acc1: 70.3125 (71.8750)  acc5: 98.4375 (98.0011)  time: 0.0284  data: 0.0002  max mem: 5511
[07:47:58.842141] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.8172 (0.8478)  acc1: 71.8750 (71.9783)  acc5: 98.4375 (98.0630)  time: 0.0283  data: 0.0002  max mem: 5511
[07:47:59.125699] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.8172 (0.8483)  acc1: 70.3125 (71.8511)  acc5: 98.4375 (98.0677)  time: 0.0283  data: 0.0002  max mem: 5511
[07:47:59.407564] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.8653 (0.8477)  acc1: 71.8750 (71.9747)  acc5: 98.4375 (98.0607)  time: 0.0282  data: 0.0001  max mem: 5511
[07:47:59.687024] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.8221 (0.8446)  acc1: 71.8750 (72.0095)  acc5: 98.4375 (98.0753)  time: 0.0280  data: 0.0001  max mem: 5511
[07:47:59.837179] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.8221 (0.8468)  acc1: 70.3125 (71.8800)  acc5: 98.4375 (98.0500)  time: 0.0270  data: 0.0001  max mem: 5511
[07:47:59.963088] Test: Total time: 0:00:05 (0.0335 s / it)
[07:47:59.963793] * Acc@1 71.880 Acc@5 98.050 loss 0.847
[07:47:59.964195] Accuracy of the network on the 10000 test images: 71.9%
[07:47:59.964374] Max accuracy: 72.40%
[07:48:00.147365] log_dir: ./output_dir
[07:48:00.995196] Epoch: [34]  [  0/781]  eta: 0:11:00  lr: 0.000197  training_loss: 1.7268 (1.7268)  mae_loss: 0.2572 (0.2572)  classification_loss: 1.4279 (1.4279)  loss_mask: 0.0417 (0.0417)  time: 0.8460  data: 0.6079  max mem: 5511
[07:48:04.954689] Epoch: [34]  [ 20/781]  eta: 0:02:54  lr: 0.000197  training_loss: 1.7949 (1.8151)  mae_loss: 0.2757 (0.2801)  classification_loss: 1.4927 (1.4878)  loss_mask: 0.0265 (0.0471)  time: 0.1978  data: 0.0002  max mem: 5511
[07:48:08.920817] Epoch: [34]  [ 40/781]  eta: 0:02:38  lr: 0.000197  training_loss: 1.7985 (1.8137)  mae_loss: 0.2968 (0.2879)  classification_loss: 1.5058 (1.4942)  loss_mask: 0.0122 (0.0316)  time: 0.1982  data: 0.0002  max mem: 5511
[07:48:12.887950] Epoch: [34]  [ 60/781]  eta: 0:02:30  lr: 0.000197  training_loss: 1.7963 (1.8111)  mae_loss: 0.2829 (0.2876)  classification_loss: 1.5135 (1.4990)  loss_mask: 0.0086 (0.0245)  time: 0.1983  data: 0.0002  max mem: 5511
[07:48:16.851445] Epoch: [34]  [ 80/781]  eta: 0:02:24  lr: 0.000197  training_loss: 1.8011 (1.8097)  mae_loss: 0.2750 (0.2856)  classification_loss: 1.4925 (1.5036)  loss_mask: 0.0049 (0.0206)  time: 0.1980  data: 0.0002  max mem: 5511
[07:48:20.788129] Epoch: [34]  [100/781]  eta: 0:02:19  lr: 0.000197  training_loss: 1.8286 (1.8165)  mae_loss: 0.2742 (0.2860)  classification_loss: 1.5219 (1.5079)  loss_mask: 0.0063 (0.0226)  time: 0.1968  data: 0.0003  max mem: 5511
[07:48:24.732875] Epoch: [34]  [120/781]  eta: 0:02:14  lr: 0.000196  training_loss: 1.9155 (1.8318)  mae_loss: 0.2754 (0.2849)  classification_loss: 1.5139 (1.5128)  loss_mask: 0.0766 (0.0341)  time: 0.1972  data: 0.0003  max mem: 5511
[07:48:28.680771] Epoch: [34]  [140/781]  eta: 0:02:09  lr: 0.000196  training_loss: 1.7913 (1.8298)  mae_loss: 0.2928 (0.2853)  classification_loss: 1.4688 (1.5086)  loss_mask: 0.0373 (0.0358)  time: 0.1973  data: 0.0002  max mem: 5511
[07:48:32.611580] Epoch: [34]  [160/781]  eta: 0:02:05  lr: 0.000196  training_loss: 1.8859 (1.8352)  mae_loss: 0.2803 (0.2851)  classification_loss: 1.5245 (1.5105)  loss_mask: 0.0560 (0.0396)  time: 0.1964  data: 0.0002  max mem: 5511
[07:48:36.561513] Epoch: [34]  [180/781]  eta: 0:02:00  lr: 0.000196  training_loss: 1.8921 (1.8426)  mae_loss: 0.2837 (0.2862)  classification_loss: 1.5095 (1.5098)  loss_mask: 0.0745 (0.0466)  time: 0.1973  data: 0.0002  max mem: 5511
[07:48:40.505853] Epoch: [34]  [200/781]  eta: 0:01:56  lr: 0.000196  training_loss: 1.8825 (1.8486)  mae_loss: 0.2765 (0.2859)  classification_loss: 1.5495 (1.5137)  loss_mask: 0.0453 (0.0489)  time: 0.1971  data: 0.0002  max mem: 5511
[07:48:44.451583] Epoch: [34]  [220/781]  eta: 0:01:52  lr: 0.000196  training_loss: 1.8425 (1.8484)  mae_loss: 0.2856 (0.2851)  classification_loss: 1.5373 (1.5156)  loss_mask: 0.0296 (0.0476)  time: 0.1972  data: 0.0003  max mem: 5511
[07:48:48.410505] Epoch: [34]  [240/781]  eta: 0:01:48  lr: 0.000196  training_loss: 1.8052 (1.8477)  mae_loss: 0.2862 (0.2850)  classification_loss: 1.4978 (1.5163)  loss_mask: 0.0200 (0.0464)  time: 0.1979  data: 0.0003  max mem: 5511
[07:48:52.355471] Epoch: [34]  [260/781]  eta: 0:01:44  lr: 0.000196  training_loss: 1.7915 (1.8437)  mae_loss: 0.2825 (0.2848)  classification_loss: 1.4865 (1.5128)  loss_mask: 0.0307 (0.0460)  time: 0.1972  data: 0.0003  max mem: 5511
[07:48:56.292049] Epoch: [34]  [280/781]  eta: 0:01:40  lr: 0.000196  training_loss: 1.8598 (1.8439)  mae_loss: 0.2928 (0.2855)  classification_loss: 1.5232 (1.5139)  loss_mask: 0.0109 (0.0445)  time: 0.1967  data: 0.0003  max mem: 5511
[07:49:00.231495] Epoch: [34]  [300/781]  eta: 0:01:35  lr: 0.000196  training_loss: 1.8508 (1.8436)  mae_loss: 0.2839 (0.2851)  classification_loss: 1.5543 (1.5146)  loss_mask: 0.0151 (0.0440)  time: 0.1969  data: 0.0002  max mem: 5511
[07:49:04.180787] Epoch: [34]  [320/781]  eta: 0:01:31  lr: 0.000196  training_loss: 1.8202 (1.8417)  mae_loss: 0.2760 (0.2846)  classification_loss: 1.5088 (1.5147)  loss_mask: 0.0185 (0.0424)  time: 0.1974  data: 0.0002  max mem: 5511
[07:49:08.144474] Epoch: [34]  [340/781]  eta: 0:01:27  lr: 0.000196  training_loss: 1.7744 (1.8365)  mae_loss: 0.2718 (0.2844)  classification_loss: 1.4893 (1.5118)  loss_mask: 0.0065 (0.0403)  time: 0.1981  data: 0.0002  max mem: 5511
[07:49:12.082902] Epoch: [34]  [360/781]  eta: 0:01:23  lr: 0.000195  training_loss: 1.7950 (1.8355)  mae_loss: 0.2763 (0.2842)  classification_loss: 1.4993 (1.5127)  loss_mask: 0.0050 (0.0386)  time: 0.1968  data: 0.0002  max mem: 5511
[07:49:16.038247] Epoch: [34]  [380/781]  eta: 0:01:19  lr: 0.000195  training_loss: 1.8187 (1.8351)  mae_loss: 0.2821 (0.2842)  classification_loss: 1.5306 (1.5133)  loss_mask: 0.0067 (0.0376)  time: 0.1977  data: 0.0002  max mem: 5511
[07:49:19.972323] Epoch: [34]  [400/781]  eta: 0:01:15  lr: 0.000195  training_loss: 1.8330 (1.8364)  mae_loss: 0.2853 (0.2846)  classification_loss: 1.5386 (1.5147)  loss_mask: 0.0117 (0.0372)  time: 0.1966  data: 0.0002  max mem: 5511
[07:49:23.951629] Epoch: [34]  [420/781]  eta: 0:01:11  lr: 0.000195  training_loss: 1.8678 (1.8373)  mae_loss: 0.2747 (0.2843)  classification_loss: 1.5079 (1.5146)  loss_mask: 0.0345 (0.0385)  time: 0.1989  data: 0.0003  max mem: 5511
[07:49:27.910371] Epoch: [34]  [440/781]  eta: 0:01:07  lr: 0.000195  training_loss: 1.8530 (1.8389)  mae_loss: 0.2830 (0.2842)  classification_loss: 1.4762 (1.5147)  loss_mask: 0.0509 (0.0400)  time: 0.1978  data: 0.0002  max mem: 5511
[07:49:31.855620] Epoch: [34]  [460/781]  eta: 0:01:03  lr: 0.000195  training_loss: 1.8958 (1.8407)  mae_loss: 0.2827 (0.2839)  classification_loss: 1.5466 (1.5153)  loss_mask: 0.0454 (0.0415)  time: 0.1972  data: 0.0002  max mem: 5511
[07:49:35.802272] Epoch: [34]  [480/781]  eta: 0:00:59  lr: 0.000195  training_loss: 2.0439 (1.8495)  mae_loss: 0.2996 (0.2844)  classification_loss: 1.5533 (1.5179)  loss_mask: 0.1633 (0.0471)  time: 0.1972  data: 0.0003  max mem: 5511
[07:49:39.747991] Epoch: [34]  [500/781]  eta: 0:00:55  lr: 0.000195  training_loss: 2.0233 (1.8561)  mae_loss: 0.3025 (0.2853)  classification_loss: 1.5637 (1.5212)  loss_mask: 0.0944 (0.0495)  time: 0.1972  data: 0.0003  max mem: 5511
[07:49:43.731423] Epoch: [34]  [520/781]  eta: 0:00:51  lr: 0.000195  training_loss: 2.0393 (1.8621)  mae_loss: 0.2818 (0.2853)  classification_loss: 1.6052 (1.5244)  loss_mask: 0.1157 (0.0523)  time: 0.1991  data: 0.0003  max mem: 5511
[07:49:47.704522] Epoch: [34]  [540/781]  eta: 0:00:47  lr: 0.000195  training_loss: 1.9132 (1.8653)  mae_loss: 0.2956 (0.2858)  classification_loss: 1.5359 (1.5263)  loss_mask: 0.0596 (0.0532)  time: 0.1986  data: 0.0003  max mem: 5511
[07:49:51.668705] Epoch: [34]  [560/781]  eta: 0:00:43  lr: 0.000195  training_loss: 1.9057 (1.8666)  mae_loss: 0.2970 (0.2862)  classification_loss: 1.5284 (1.5274)  loss_mask: 0.0372 (0.0530)  time: 0.1981  data: 0.0002  max mem: 5511
[07:49:55.597401] Epoch: [34]  [580/781]  eta: 0:00:39  lr: 0.000194  training_loss: 1.8186 (1.8650)  mae_loss: 0.2801 (0.2860)  classification_loss: 1.4963 (1.5265)  loss_mask: 0.0238 (0.0524)  time: 0.1963  data: 0.0002  max mem: 5511
[07:49:59.570655] Epoch: [34]  [600/781]  eta: 0:00:35  lr: 0.000194  training_loss: 1.8522 (1.8646)  mae_loss: 0.3042 (0.2867)  classification_loss: 1.5122 (1.5264)  loss_mask: 0.0179 (0.0515)  time: 0.1986  data: 0.0002  max mem: 5511
[07:50:03.495432] Epoch: [34]  [620/781]  eta: 0:00:31  lr: 0.000194  training_loss: 1.9350 (1.8673)  mae_loss: 0.2965 (0.2869)  classification_loss: 1.5679 (1.5280)  loss_mask: 0.0499 (0.0524)  time: 0.1961  data: 0.0002  max mem: 5511
[07:50:07.444524] Epoch: [34]  [640/781]  eta: 0:00:27  lr: 0.000194  training_loss: 2.1040 (1.8753)  mae_loss: 0.3228 (0.2879)  classification_loss: 1.6238 (1.5313)  loss_mask: 0.1640 (0.0561)  time: 0.1974  data: 0.0002  max mem: 5511
[07:50:11.401598] Epoch: [34]  [660/781]  eta: 0:00:24  lr: 0.000194  training_loss: 2.0599 (1.8811)  mae_loss: 0.2996 (0.2883)  classification_loss: 1.6451 (1.5345)  loss_mask: 0.1015 (0.0583)  time: 0.1978  data: 0.0002  max mem: 5511
[07:50:15.346629] Epoch: [34]  [680/781]  eta: 0:00:20  lr: 0.000194  training_loss: 2.0032 (1.8858)  mae_loss: 0.2928 (0.2885)  classification_loss: 1.6183 (1.5374)  loss_mask: 0.0969 (0.0599)  time: 0.1972  data: 0.0002  max mem: 5511

[07:50:19.298855] Epoch: [34]  [700/781]  eta: 0:00:16  lr: 0.000194  training_loss: 1.9635 (1.8883)  mae_loss: 0.2952 (0.2887)  classification_loss: 1.6197 (1.5396)  loss_mask: 0.0703 (0.0601)  time: 0.1975  data: 0.0002  max mem: 5511
[07:50:23.224369] Epoch: [34]  [720/781]  eta: 0:00:12  lr: 0.000194  training_loss: 1.9415 (1.8903)  mae_loss: 0.2789 (0.2885)  classification_loss: 1.5792 (1.5406)  loss_mask: 0.0810 (0.0611)  time: 0.1962  data: 0.0003  max mem: 5511
[07:50:27.152012] Epoch: [34]  [740/781]  eta: 0:00:08  lr: 0.000194  training_loss: 1.8842 (1.8910)  mae_loss: 0.2947 (0.2888)  classification_loss: 1.5522 (1.5412)  loss_mask: 0.0444 (0.0609)  time: 0.1963  data: 0.0003  max mem: 5511
[07:50:31.145905] Epoch: [34]  [760/781]  eta: 0:00:04  lr: 0.000194  training_loss: 1.8958 (1.8915)  mae_loss: 0.2946 (0.2891)  classification_loss: 1.5526 (1.5420)  loss_mask: 0.0283 (0.0604)  time: 0.1996  data: 0.0002  max mem: 5511
[07:50:35.088746] Epoch: [34]  [780/781]  eta: 0:00:00  lr: 0.000194  training_loss: 1.8425 (1.8909)  mae_loss: 0.2851 (0.2890)  classification_loss: 1.5263 (1.5424)  loss_mask: 0.0173 (0.0595)  time: 0.1971  data: 0.0002  max mem: 5511
[07:50:35.255817] Epoch: [34] Total time: 0:02:35 (0.1986 s / it)
[07:50:35.256501] Averaged stats: lr: 0.000194  training_loss: 1.8425 (1.8909)  mae_loss: 0.2851 (0.2890)  classification_loss: 1.5263 (1.5424)  loss_mask: 0.0173 (0.0595)
[07:50:35.914097] Test:  [  0/157]  eta: 0:01:42  testing_loss: 0.8572 (0.8572)  acc1: 78.1250 (78.1250)  acc5: 96.8750 (96.8750)  time: 0.6530  data: 0.6206  max mem: 5511
[07:50:36.201235] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.8572 (0.8749)  acc1: 73.4375 (72.3011)  acc5: 100.0000 (99.0057)  time: 0.0850  data: 0.0566  max mem: 5511
[07:50:36.484371] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.8354 (0.8411)  acc1: 71.8750 (72.8423)  acc5: 98.4375 (98.6607)  time: 0.0282  data: 0.0002  max mem: 5511
[07:50:36.771797] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.8291 (0.8484)  acc1: 73.4375 (72.4294)  acc5: 98.4375 (98.5383)  time: 0.0283  data: 0.0002  max mem: 5511
[07:50:37.059033] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.8283 (0.8501)  acc1: 71.8750 (72.1418)  acc5: 98.4375 (98.4756)  time: 0.0285  data: 0.0003  max mem: 5511
[07:50:37.347545] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.8261 (0.8438)  acc1: 73.4375 (72.6103)  acc5: 98.4375 (98.3150)  time: 0.0285  data: 0.0002  max mem: 5511
[07:50:37.635588] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.8032 (0.8397)  acc1: 75.0000 (72.7459)  acc5: 98.4375 (98.3094)  time: 0.0286  data: 0.0002  max mem: 5511
[07:50:37.927908] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7702 (0.8318)  acc1: 75.0000 (73.3495)  acc5: 98.4375 (98.3055)  time: 0.0289  data: 0.0002  max mem: 5511
[07:50:38.214556] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.8003 (0.8371)  acc1: 75.0000 (73.1289)  acc5: 96.8750 (98.1867)  time: 0.0288  data: 0.0002  max mem: 5511
[07:50:38.498267] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.8347 (0.8372)  acc1: 75.0000 (73.2315)  acc5: 98.4375 (98.2830)  time: 0.0284  data: 0.0002  max mem: 5511
[07:50:38.785416] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.8546 (0.8422)  acc1: 71.8750 (72.9579)  acc5: 98.4375 (98.1900)  time: 0.0283  data: 0.0002  max mem: 5511
[07:50:39.073822] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.8768 (0.8456)  acc1: 70.3125 (72.7759)  acc5: 98.4375 (98.1560)  time: 0.0286  data: 0.0002  max mem: 5511
[07:50:39.361965] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.8462 (0.8417)  acc1: 71.8750 (72.9210)  acc5: 98.4375 (98.1921)  time: 0.0287  data: 0.0002  max mem: 5511
[07:50:39.646974] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.8495 (0.8425)  acc1: 71.8750 (72.8769)  acc5: 98.4375 (98.2228)  time: 0.0285  data: 0.0002  max mem: 5511
[07:50:39.929563] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.8592 (0.8415)  acc1: 71.8750 (72.7948)  acc5: 98.4375 (98.2602)  time: 0.0283  data: 0.0002  max mem: 5511
[07:50:40.210461] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.8579 (0.8404)  acc1: 73.4375 (72.8063)  acc5: 98.4375 (98.2409)  time: 0.0281  data: 0.0001  max mem: 5511
[07:50:40.360240] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.8579 (0.8421)  acc1: 70.3125 (72.6400)  acc5: 98.4375 (98.2300)  time: 0.0270  data: 0.0001  max mem: 5511
[07:50:40.542666] Test: Total time: 0:00:05 (0.0336 s / it)
[07:50:40.543087] * Acc@1 72.640 Acc@5 98.230 loss 0.842
[07:50:40.543361] Accuracy of the network on the 10000 test images: 72.6%
[07:50:40.543559] Max accuracy: 72.64%
[07:50:40.814586] log_dir: ./output_dir
[07:50:41.604230] Epoch: [35]  [  0/781]  eta: 0:10:15  lr: 0.000194  training_loss: 1.7414 (1.7414)  mae_loss: 0.3165 (0.3165)  classification_loss: 1.3832 (1.3832)  loss_mask: 0.0417 (0.0417)  time: 0.7880  data: 0.5788  max mem: 5511
[07:50:45.539098] Epoch: [35]  [ 20/781]  eta: 0:02:51  lr: 0.000194  training_loss: 1.7514 (1.7872)  mae_loss: 0.2766 (0.2849)  classification_loss: 1.4607 (1.4800)  loss_mask: 0.0098 (0.0223)  time: 0.1966  data: 0.0002  max mem: 5511
[07:50:49.484533] Epoch: [35]  [ 40/781]  eta: 0:02:36  lr: 0.000193  training_loss: 1.8463 (1.8108)  mae_loss: 0.2719 (0.2781)  classification_loss: 1.5306 (1.4986)  loss_mask: 0.0328 (0.0341)  time: 0.1972  data: 0.0002  max mem: 5511
[07:50:53.459468] Epoch: [35]  [ 60/781]  eta: 0:02:29  lr: 0.000193  training_loss: 1.8786 (1.8373)  mae_loss: 0.2813 (0.2792)  classification_loss: 1.5330 (1.5157)  loss_mask: 0.0406 (0.0424)  time: 0.1987  data: 0.0002  max mem: 5511
[07:50:57.413130] Epoch: [35]  [ 80/781]  eta: 0:02:23  lr: 0.000193  training_loss: 1.8706 (1.8535)  mae_loss: 0.2872 (0.2827)  classification_loss: 1.5332 (1.5210)  loss_mask: 0.0398 (0.0498)  time: 0.1976  data: 0.0002  max mem: 5511
[07:51:01.397468] Epoch: [35]  [100/781]  eta: 0:02:18  lr: 0.000193  training_loss: 1.8615 (1.8575)  mae_loss: 0.2722 (0.2815)  classification_loss: 1.4917 (1.5179)  loss_mask: 0.0715 (0.0581)  time: 0.1991  data: 0.0004  max mem: 5511
[07:51:05.326844] Epoch: [35]  [120/781]  eta: 0:02:13  lr: 0.000193  training_loss: 1.7634 (1.8518)  mae_loss: 0.2676 (0.2814)  classification_loss: 1.4914 (1.5151)  loss_mask: 0.0332 (0.0553)  time: 0.1964  data: 0.0002  max mem: 5511
[07:51:09.362914] Epoch: [35]  [140/781]  eta: 0:02:09  lr: 0.000193  training_loss: 1.7610 (1.8418)  mae_loss: 0.2847 (0.2808)  classification_loss: 1.4802 (1.5097)  loss_mask: 0.0173 (0.0512)  time: 0.2017  data: 0.0002  max mem: 5511
[07:51:13.291478] Epoch: [35]  [160/781]  eta: 0:02:05  lr: 0.000193  training_loss: 1.7440 (1.8317)  mae_loss: 0.2633 (0.2790)  classification_loss: 1.4884 (1.5065)  loss_mask: 0.0063 (0.0462)  time: 0.1964  data: 0.0002  max mem: 5511
[07:51:17.264292] Epoch: [35]  [180/781]  eta: 0:02:00  lr: 0.000193  training_loss: 1.7775 (1.8268)  mae_loss: 0.2744 (0.2788)  classification_loss: 1.4919 (1.5059)  loss_mask: 0.0060 (0.0420)  time: 0.1986  data: 0.0003  max mem: 5511
[07:51:21.187536] Epoch: [35]  [200/781]  eta: 0:01:56  lr: 0.000193  training_loss: 1.8223 (1.8231)  mae_loss: 0.2826 (0.2791)  classification_loss: 1.5154 (1.5052)  loss_mask: 0.0072 (0.0389)  time: 0.1961  data: 0.0002  max mem: 5511
[07:51:25.123491] Epoch: [35]  [220/781]  eta: 0:01:52  lr: 0.000193  training_loss: 1.7782 (1.8194)  mae_loss: 0.2735 (0.2784)  classification_loss: 1.4718 (1.5028)  loss_mask: 0.0122 (0.0382)  time: 0.1967  data: 0.0003  max mem: 5511
[07:51:29.048392] Epoch: [35]  [240/781]  eta: 0:01:48  lr: 0.000193  training_loss: 1.8803 (1.8242)  mae_loss: 0.2660 (0.2780)  classification_loss: 1.5086 (1.5037)  loss_mask: 0.0683 (0.0425)  time: 0.1962  data: 0.0003  max mem: 5511
[07:51:32.984812] Epoch: [35]  [260/781]  eta: 0:01:44  lr: 0.000192  training_loss: 1.8798 (1.8276)  mae_loss: 0.2790 (0.2784)  classification_loss: 1.5330 (1.5051)  loss_mask: 0.0427 (0.0441)  time: 0.1968  data: 0.0003  max mem: 5511
[07:51:36.916529] Epoch: [35]  [280/781]  eta: 0:01:39  lr: 0.000192  training_loss: 1.8784 (1.8307)  mae_loss: 0.2990 (0.2801)  classification_loss: 1.5189 (1.5036)  loss_mask: 0.0677 (0.0471)  time: 0.1965  data: 0.0003  max mem: 5511
[07:51:40.854914] Epoch: [35]  [300/781]  eta: 0:01:35  lr: 0.000192  training_loss: 1.7571 (1.8304)  mae_loss: 0.2634 (0.2798)  classification_loss: 1.4931 (1.5040)  loss_mask: 0.0365 (0.0466)  time: 0.1968  data: 0.0002  max mem: 5511
[07:51:44.809949] Epoch: [35]  [320/781]  eta: 0:01:31  lr: 0.000192  training_loss: 1.7981 (1.8283)  mae_loss: 0.2770 (0.2801)  classification_loss: 1.4847 (1.5030)  loss_mask: 0.0206 (0.0452)  time: 0.1977  data: 0.0003  max mem: 5511
[07:51:48.853667] Epoch: [35]  [340/781]  eta: 0:01:27  lr: 0.000192  training_loss: 1.7631 (1.8254)  mae_loss: 0.2817 (0.2805)  classification_loss: 1.4769 (1.5017)  loss_mask: 0.0105 (0.0432)  time: 0.2021  data: 0.0004  max mem: 5511
[07:51:52.803257] Epoch: [35]  [360/781]  eta: 0:01:23  lr: 0.000192  training_loss: 1.8150 (1.8255)  mae_loss: 0.2912 (0.2809)  classification_loss: 1.5258 (1.5029)  loss_mask: 0.0099 (0.0417)  time: 0.1974  data: 0.0002  max mem: 5511
[07:51:56.728184] Epoch: [35]  [380/781]  eta: 0:01:19  lr: 0.000192  training_loss: 1.8191 (1.8241)  mae_loss: 0.2821 (0.2810)  classification_loss: 1.5115 (1.5029)  loss_mask: 0.0083 (0.0402)  time: 0.1961  data: 0.0002  max mem: 5511
[07:52:00.679328] Epoch: [35]  [400/781]  eta: 0:01:15  lr: 0.000192  training_loss: 1.7331 (1.8199)  mae_loss: 0.2652 (0.2810)  classification_loss: 1.4354 (1.5001)  loss_mask: 0.0117 (0.0389)  time: 0.1975  data: 0.0003  max mem: 5511
[07:52:04.632545] Epoch: [35]  [420/781]  eta: 0:01:11  lr: 0.000192  training_loss: 1.8002 (1.8203)  mae_loss: 0.2718 (0.2809)  classification_loss: 1.4919 (1.5004)  loss_mask: 0.0270 (0.0390)  time: 0.1976  data: 0.0002  max mem: 5511
[07:52:08.561797] Epoch: [35]  [440/781]  eta: 0:01:07  lr: 0.000192  training_loss: 1.7711 (1.8185)  mae_loss: 0.2778 (0.2808)  classification_loss: 1.4576 (1.4985)  loss_mask: 0.0188 (0.0391)  time: 0.1963  data: 0.0003  max mem: 5511
[07:52:12.560070] Epoch: [35]  [460/781]  eta: 0:01:03  lr: 0.000192  training_loss: 1.8480 (1.8191)  mae_loss: 0.2543 (0.2803)  classification_loss: 1.4854 (1.4977)  loss_mask: 0.0727 (0.0412)  time: 0.1998  data: 0.0004  max mem: 5511
[07:52:16.561910] Epoch: [35]  [480/781]  eta: 0:00:59  lr: 0.000191  training_loss: 1.8050 (1.8201)  mae_loss: 0.2804 (0.2803)  classification_loss: 1.4981 (1.4968)  loss_mask: 0.0788 (0.0430)  time: 0.2000  data: 0.0003  max mem: 5511
[07:52:20.497585] Epoch: [35]  [500/781]  eta: 0:00:55  lr: 0.000191  training_loss: 1.9147 (1.8228)  mae_loss: 0.2810 (0.2806)  classification_loss: 1.5501 (1.4983)  loss_mask: 0.0492 (0.0439)  time: 0.1967  data: 0.0002  max mem: 5511
[07:52:24.456681] Epoch: [35]  [520/781]  eta: 0:00:51  lr: 0.000191  training_loss: 1.8234 (1.8225)  mae_loss: 0.2959 (0.2812)  classification_loss: 1.4905 (1.4982)  loss_mask: 0.0188 (0.0431)  time: 0.1979  data: 0.0003  max mem: 5511
[07:52:28.387432] Epoch: [35]  [540/781]  eta: 0:00:47  lr: 0.000191  training_loss: 1.8135 (1.8224)  mae_loss: 0.2825 (0.2815)  classification_loss: 1.5078 (1.4990)  loss_mask: 0.0087 (0.0419)  time: 0.1964  data: 0.0002  max mem: 5511
[07:52:32.331680] Epoch: [35]  [560/781]  eta: 0:00:43  lr: 0.000191  training_loss: 1.7569 (1.8210)  mae_loss: 0.2724 (0.2814)  classification_loss: 1.4909 (1.4991)  loss_mask: 0.0052 (0.0406)  time: 0.1971  data: 0.0002  max mem: 5511
[07:52:36.279491] Epoch: [35]  [580/781]  eta: 0:00:39  lr: 0.000191  training_loss: 1.7847 (1.8204)  mae_loss: 0.2841 (0.2816)  classification_loss: 1.4973 (1.4994)  loss_mask: 0.0031 (0.0394)  time: 0.1973  data: 0.0002  max mem: 5511
[07:52:40.197439] Epoch: [35]  [600/781]  eta: 0:00:35  lr: 0.000191  training_loss: 1.8627 (1.8227)  mae_loss: 0.2828 (0.2820)  classification_loss: 1.5271 (1.5003)  loss_mask: 0.0161 (0.0405)  time: 0.1958  data: 0.0002  max mem: 5511
[07:52:44.140886] Epoch: [35]  [620/781]  eta: 0:00:31  lr: 0.000191  training_loss: 1.9107 (1.8266)  mae_loss: 0.2825 (0.2820)  classification_loss: 1.4771 (1.4992)  loss_mask: 0.1517 (0.0454)  time: 0.1971  data: 0.0002  max mem: 5511
[07:52:48.088229] Epoch: [35]  [640/781]  eta: 0:00:27  lr: 0.000191  training_loss: 1.7939 (1.8272)  mae_loss: 0.2803 (0.2819)  classification_loss: 1.5053 (1.4992)  loss_mask: 0.0610 (0.0461)  time: 0.1973  data: 0.0002  max mem: 5511
[07:52:52.021561] Epoch: [35]  [660/781]  eta: 0:00:24  lr: 0.000191  training_loss: 1.8201 (1.8275)  mae_loss: 0.2766 (0.2819)  classification_loss: 1.5051 (1.4996)  loss_mask: 0.0355 (0.0460)  time: 0.1966  data: 0.0002  max mem: 5511
[07:52:55.942349] Epoch: [35]  [680/781]  eta: 0:00:20  lr: 0.000191  training_loss: 1.8311 (1.8276)  mae_loss: 0.2822 (0.2820)  classification_loss: 1.5051 (1.5004)  loss_mask: 0.0172 (0.0452)  time: 0.1959  data: 0.0002  max mem: 5511
[07:52:59.871447] Epoch: [35]  [700/781]  eta: 0:00:16  lr: 0.000190  training_loss: 1.7684 (1.8266)  mae_loss: 0.2778 (0.2819)  classification_loss: 1.5018 (1.5005)  loss_mask: 0.0081 (0.0441)  time: 0.1964  data: 0.0002  max mem: 5511
[07:53:03.803417] Epoch: [35]  [720/781]  eta: 0:00:12  lr: 0.000190  training_loss: 1.7789 (1.8258)  mae_loss: 0.2810 (0.2820)  classification_loss: 1.4932 (1.5007)  loss_mask: 0.0039 (0.0431)  time: 0.1965  data: 0.0002  max mem: 5511
[07:53:07.778580] Epoch: [35]  [740/781]  eta: 0:00:08  lr: 0.000190  training_loss: 1.7112 (1.8230)  mae_loss: 0.2740 (0.2818)  classification_loss: 1.4524 (1.4991)  loss_mask: 0.0041 (0.0421)  time: 0.1987  data: 0.0002  max mem: 5511
[07:53:11.718131] Epoch: [35]  [760/781]  eta: 0:00:04  lr: 0.000190  training_loss: 1.8098 (1.8223)  mae_loss: 0.2687 (0.2815)  classification_loss: 1.5232 (1.4997)  loss_mask: 0.0023 (0.0410)  time: 0.1969  data: 0.0003  max mem: 5511
[07:53:15.662606] Epoch: [35]  [780/781]  eta: 0:00:00  lr: 0.000190  training_loss: 1.7507 (1.8209)  mae_loss: 0.2747 (0.2814)  classification_loss: 1.4754 (1.4995)  loss_mask: 0.0019 (0.0400)  time: 0.1971  data: 0.0002  max mem: 5511
[07:53:15.834179] Epoch: [35] Total time: 0:02:35 (0.1985 s / it)
[07:53:15.834671] Averaged stats: lr: 0.000190  training_loss: 1.7507 (1.8209)  mae_loss: 0.2747 (0.2814)  classification_loss: 1.4754 (1.4995)  loss_mask: 0.0019 (0.0400)
[07:53:16.482075] Test:  [  0/157]  eta: 0:01:41  testing_loss: 0.7643 (0.7643)  acc1: 81.2500 (81.2500)  acc5: 98.4375 (98.4375)  time: 0.6433  data: 0.6094  max mem: 5511
[07:53:16.781453] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.7929 (0.8110)  acc1: 71.8750 (73.5795)  acc5: 98.4375 (99.1477)  time: 0.0850  data: 0.0557  max mem: 5511
[07:53:17.070650] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.7664 (0.7773)  acc1: 73.4375 (74.4792)  acc5: 98.4375 (98.6607)  time: 0.0289  data: 0.0002  max mem: 5511
[07:53:17.361628] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.7598 (0.7793)  acc1: 75.0000 (74.5968)  acc5: 98.4375 (98.4879)  time: 0.0288  data: 0.0002  max mem: 5511
[07:53:17.648505] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.7779 (0.7829)  acc1: 76.5625 (74.6570)  acc5: 98.4375 (98.5137)  time: 0.0287  data: 0.0002  max mem: 5511
[07:53:17.933349] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.7635 (0.7745)  acc1: 76.5625 (74.9081)  acc5: 98.4375 (98.6213)  time: 0.0285  data: 0.0003  max mem: 5511
[07:53:18.216725] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.7635 (0.7766)  acc1: 75.0000 (74.8719)  acc5: 98.4375 (98.5400)  time: 0.0283  data: 0.0003  max mem: 5511
[07:53:18.500428] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7172 (0.7677)  acc1: 76.5625 (75.1981)  acc5: 98.4375 (98.5475)  time: 0.0282  data: 0.0002  max mem: 5511
[07:53:18.784211] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7205 (0.7748)  acc1: 76.5625 (75.1350)  acc5: 98.4375 (98.4761)  time: 0.0282  data: 0.0002  max mem: 5511
[07:53:19.068190] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7982 (0.7769)  acc1: 73.4375 (75.0000)  acc5: 98.4375 (98.4718)  time: 0.0283  data: 0.0002  max mem: 5511
[07:53:19.353020] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.7982 (0.7808)  acc1: 73.4375 (74.8917)  acc5: 98.4375 (98.4839)  time: 0.0283  data: 0.0002  max mem: 5511
[07:53:19.639135] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.8220 (0.7851)  acc1: 71.8750 (74.7748)  acc5: 98.4375 (98.4516)  time: 0.0284  data: 0.0002  max mem: 5511
[07:53:19.929637] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.7843 (0.7831)  acc1: 73.4375 (74.8192)  acc5: 98.4375 (98.4633)  time: 0.0287  data: 0.0002  max mem: 5511
[07:53:20.221384] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.7717 (0.7830)  acc1: 75.0000 (74.8927)  acc5: 98.4375 (98.4971)  time: 0.0290  data: 0.0002  max mem: 5511
[07:53:20.506334] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7922 (0.7833)  acc1: 73.4375 (74.8781)  acc5: 98.4375 (98.4707)  time: 0.0287  data: 0.0002  max mem: 5511
[07:53:20.787373] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7802 (0.7797)  acc1: 76.5625 (74.9690)  acc5: 98.4375 (98.4685)  time: 0.0282  data: 0.0001  max mem: 5511
[07:53:20.940171] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7802 (0.7817)  acc1: 76.5625 (74.8900)  acc5: 98.4375 (98.4700)  time: 0.0272  data: 0.0001  max mem: 5511
[07:53:21.109358] Test: Total time: 0:00:05 (0.0336 s / it)
[07:53:21.109809] * Acc@1 74.890 Acc@5 98.470 loss 0.782
[07:53:21.110125] Accuracy of the network on the 10000 test images: 74.9%
[07:53:21.110312] Max accuracy: 74.89%
[07:53:21.292410] log_dir: ./output_dir
[07:53:22.240899] Epoch: [36]  [  0/781]  eta: 0:12:19  lr: 0.000190  training_loss: 1.8436 (1.8436)  mae_loss: 0.2534 (0.2534)  classification_loss: 1.5890 (1.5890)  loss_mask: 0.0012 (0.0012)  time: 0.9465  data: 0.7323  max mem: 5511
[07:53:26.171985] Epoch: [36]  [ 20/781]  eta: 0:02:56  lr: 0.000190  training_loss: 1.7325 (1.7526)  mae_loss: 0.2821 (0.2840)  classification_loss: 1.4727 (1.4662)  loss_mask: 0.0016 (0.0023)  time: 0.1964  data: 0.0002  max mem: 5511
[07:53:30.151736] Epoch: [36]  [ 40/781]  eta: 0:02:39  lr: 0.000190  training_loss: 1.8018 (1.7758)  mae_loss: 0.2849 (0.2849)  classification_loss: 1.5450 (1.4889)  loss_mask: 0.0016 (0.0020)  time: 0.1989  data: 0.0002  max mem: 5511
[07:53:34.087611] Epoch: [36]  [ 60/781]  eta: 0:02:31  lr: 0.000190  training_loss: 1.7983 (1.7797)  mae_loss: 0.2895 (0.2876)  classification_loss: 1.4769 (1.4899)  loss_mask: 0.0017 (0.0021)  time: 0.1967  data: 0.0002  max mem: 5511
[07:53:38.050952] Epoch: [36]  [ 80/781]  eta: 0:02:24  lr: 0.000190  training_loss: 1.7964 (1.7830)  mae_loss: 0.2834 (0.2857)  classification_loss: 1.5164 (1.4951)  loss_mask: 0.0015 (0.0021)  time: 0.1981  data: 0.0002  max mem: 5511
[07:53:42.013417] Epoch: [36]  [100/781]  eta: 0:02:19  lr: 0.000190  training_loss: 1.7464 (1.7756)  mae_loss: 0.2835 (0.2849)  classification_loss: 1.4768 (1.4886)  loss_mask: 0.0015 (0.0022)  time: 0.1980  data: 0.0002  max mem: 5511
[07:53:45.991700] Epoch: [36]  [120/781]  eta: 0:02:14  lr: 0.000190  training_loss: 1.7426 (1.7681)  mae_loss: 0.2692 (0.2839)  classification_loss: 1.4528 (1.4821)  loss_mask: 0.0012 (0.0021)  time: 0.1988  data: 0.0003  max mem: 5511
[07:53:49.953402] Epoch: [36]  [140/781]  eta: 0:02:10  lr: 0.000189  training_loss: 1.7602 (1.7656)  mae_loss: 0.2792 (0.2827)  classification_loss: 1.4784 (1.4809)  loss_mask: 0.0011 (0.0020)  time: 0.1980  data: 0.0004  max mem: 5511
[07:53:53.901088] Epoch: [36]  [160/781]  eta: 0:02:05  lr: 0.000189  training_loss: 1.7235 (1.7612)  mae_loss: 0.2736 (0.2816)  classification_loss: 1.4528 (1.4778)  loss_mask: 0.0009 (0.0019)  time: 0.1973  data: 0.0003  max mem: 5511
[07:53:57.861462] Epoch: [36]  [180/781]  eta: 0:02:01  lr: 0.000189  training_loss: 1.7374 (1.7601)  mae_loss: 0.2790 (0.2817)  classification_loss: 1.4297 (1.4766)  loss_mask: 0.0006 (0.0018)  time: 0.1979  data: 0.0002  max mem: 5511
[07:54:01.819998] Epoch: [36]  [200/781]  eta: 0:01:57  lr: 0.000189  training_loss: 1.7313 (1.7586)  mae_loss: 0.2849 (0.2817)  classification_loss: 1.4759 (1.4752)  loss_mask: 0.0009 (0.0018)  time: 0.1978  data: 0.0002  max mem: 5511
[07:54:05.771858] Epoch: [36]  [220/781]  eta: 0:01:52  lr: 0.000189  training_loss: 1.8020 (1.7632)  mae_loss: 0.2868 (0.2823)  classification_loss: 1.5118 (1.4791)  loss_mask: 0.0009 (0.0017)  time: 0.1975  data: 0.0002  max mem: 5511
[07:54:09.734715] Epoch: [36]  [240/781]  eta: 0:01:48  lr: 0.000189  training_loss: 1.7159 (1.7606)  mae_loss: 0.2784 (0.2821)  classification_loss: 1.4283 (1.4768)  loss_mask: 0.0006 (0.0016)  time: 0.1981  data: 0.0002  max mem: 5511
[07:54:13.719071] Epoch: [36]  [260/781]  eta: 0:01:44  lr: 0.000189  training_loss: 1.7772 (1.7612)  mae_loss: 0.2701 (0.2817)  classification_loss: 1.4987 (1.4780)  loss_mask: 0.0005 (0.0016)  time: 0.1991  data: 0.0003  max mem: 5511
[07:54:17.687033] Epoch: [36]  [280/781]  eta: 0:01:40  lr: 0.000189  training_loss: 1.7314 (1.7586)  mae_loss: 0.2689 (0.2805)  classification_loss: 1.4401 (1.4766)  loss_mask: 0.0006 (0.0015)  time: 0.1983  data: 0.0002  max mem: 5511
[07:54:21.623086] Epoch: [36]  [300/781]  eta: 0:01:36  lr: 0.000189  training_loss: 1.8079 (1.7606)  mae_loss: 0.2780 (0.2801)  classification_loss: 1.4958 (1.4790)  loss_mask: 0.0007 (0.0015)  time: 0.1967  data: 0.0002  max mem: 5511
[07:54:25.583925] Epoch: [36]  [320/781]  eta: 0:01:32  lr: 0.000189  training_loss: 1.7365 (1.7595)  mae_loss: 0.2630 (0.2798)  classification_loss: 1.4561 (1.4783)  loss_mask: 0.0007 (0.0015)  time: 0.1979  data: 0.0002  max mem: 5511
[07:54:29.554767] Epoch: [36]  [340/781]  eta: 0:01:28  lr: 0.000189  training_loss: 1.7877 (1.7614)  mae_loss: 0.2890 (0.2801)  classification_loss: 1.5076 (1.4797)  loss_mask: 0.0017 (0.0016)  time: 0.1985  data: 0.0003  max mem: 5511
[07:54:33.497921] Epoch: [36]  [360/781]  eta: 0:01:24  lr: 0.000188  training_loss: 1.7465 (1.7628)  mae_loss: 0.2689 (0.2798)  classification_loss: 1.4786 (1.4815)  loss_mask: 0.0008 (0.0015)  time: 0.1971  data: 0.0002  max mem: 5511
[07:54:37.428891] Epoch: [36]  [380/781]  eta: 0:01:20  lr: 0.000188  training_loss: 1.9443 (1.7763)  mae_loss: 0.2844 (0.2804)  classification_loss: 1.4896 (1.4828)  loss_mask: 0.0893 (0.0131)  time: 0.1965  data: 0.0002  max mem: 5511
[07:54:41.373833] Epoch: [36]  [400/781]  eta: 0:01:16  lr: 0.000188  training_loss: 1.8973 (1.7828)  mae_loss: 0.2791 (0.2804)  classification_loss: 1.4699 (1.4828)  loss_mask: 0.1172 (0.0196)  time: 0.1972  data: 0.0003  max mem: 5511
[07:54:45.314315] Epoch: [36]  [420/781]  eta: 0:01:12  lr: 0.000188  training_loss: 1.8016 (1.7859)  mae_loss: 0.2864 (0.2809)  classification_loss: 1.4766 (1.4830)  loss_mask: 0.0624 (0.0220)  time: 0.1969  data: 0.0002  max mem: 5511
[07:54:49.243069] Epoch: [36]  [440/781]  eta: 0:01:07  lr: 0.000188  training_loss: 1.8225 (1.7874)  mae_loss: 0.2722 (0.2804)  classification_loss: 1.4871 (1.4824)  loss_mask: 0.0659 (0.0246)  time: 0.1963  data: 0.0003  max mem: 5511
[07:54:53.198588] Epoch: [36]  [460/781]  eta: 0:01:03  lr: 0.000188  training_loss: 1.7167 (1.7857)  mae_loss: 0.2550 (0.2797)  classification_loss: 1.4098 (1.4799)  loss_mask: 0.0353 (0.0260)  time: 0.1977  data: 0.0002  max mem: 5511
[07:54:57.137073] Epoch: [36]  [480/781]  eta: 0:00:59  lr: 0.000188  training_loss: 1.7812 (1.7867)  mae_loss: 0.2764 (0.2801)  classification_loss: 1.4830 (1.4806)  loss_mask: 0.0217 (0.0260)  time: 0.1968  data: 0.0002  max mem: 5511
[07:55:01.075570] Epoch: [36]  [500/781]  eta: 0:00:55  lr: 0.000188  training_loss: 1.8390 (1.7884)  mae_loss: 0.2648 (0.2799)  classification_loss: 1.4840 (1.4821)  loss_mask: 0.0146 (0.0265)  time: 0.1968  data: 0.0002  max mem: 5511
[07:55:05.036795] Epoch: [36]  [520/781]  eta: 0:00:51  lr: 0.000188  training_loss: 1.8086 (1.7903)  mae_loss: 0.2867 (0.2800)  classification_loss: 1.5093 (1.4822)  loss_mask: 0.0464 (0.0282)  time: 0.1980  data: 0.0003  max mem: 5511
[07:55:08.994458] Epoch: [36]  [540/781]  eta: 0:00:47  lr: 0.000188  training_loss: 1.8322 (1.7917)  mae_loss: 0.2773 (0.2799)  classification_loss: 1.4816 (1.4824)  loss_mask: 0.0417 (0.0294)  time: 0.1978  data: 0.0002  max mem: 5511
[07:55:12.933669] Epoch: [36]  [560/781]  eta: 0:00:43  lr: 0.000188  training_loss: 1.7963 (1.7923)  mae_loss: 0.2852 (0.2803)  classification_loss: 1.4980 (1.4822)  loss_mask: 0.0347 (0.0298)  time: 0.1969  data: 0.0002  max mem: 5511
[07:55:16.868186] Epoch: [36]  [580/781]  eta: 0:00:39  lr: 0.000187  training_loss: 1.8167 (1.7922)  mae_loss: 0.2873 (0.2804)  classification_loss: 1.4719 (1.4822)  loss_mask: 0.0164 (0.0296)  time: 0.1966  data: 0.0002  max mem: 5511
[07:55:20.824411] Epoch: [36]  [600/781]  eta: 0:00:35  lr: 0.000187  training_loss: 1.8487 (1.7938)  mae_loss: 0.2828 (0.2806)  classification_loss: 1.5100 (1.4830)  loss_mask: 0.0228 (0.0302)  time: 0.1977  data: 0.0002  max mem: 5511
[07:55:24.777911] Epoch: [36]  [620/781]  eta: 0:00:31  lr: 0.000187  training_loss: 1.8205 (1.7949)  mae_loss: 0.2763 (0.2805)  classification_loss: 1.4865 (1.4837)  loss_mask: 0.0362 (0.0307)  time: 0.1976  data: 0.0002  max mem: 5511
[07:55:28.727380] Epoch: [36]  [640/781]  eta: 0:00:28  lr: 0.000187  training_loss: 1.8050 (1.7955)  mae_loss: 0.2718 (0.2802)  classification_loss: 1.4790 (1.4833)  loss_mask: 0.0568 (0.0319)  time: 0.1974  data: 0.0003  max mem: 5511
[07:55:32.672135] Epoch: [36]  [660/781]  eta: 0:00:24  lr: 0.000187  training_loss: 1.8234 (1.7961)  mae_loss: 0.2935 (0.2806)  classification_loss: 1.4862 (1.4836)  loss_mask: 0.0266 (0.0318)  time: 0.1971  data: 0.0002  max mem: 5511
[07:55:36.605720] Epoch: [36]  [680/781]  eta: 0:00:20  lr: 0.000187  training_loss: 1.7516 (1.7956)  mae_loss: 0.2905 (0.2809)  classification_loss: 1.4417 (1.4833)  loss_mask: 0.0198 (0.0315)  time: 0.1966  data: 0.0002  max mem: 5511
[07:55:40.535773] Epoch: [36]  [700/781]  eta: 0:00:16  lr: 0.000187  training_loss: 1.7910 (1.7969)  mae_loss: 0.2680 (0.2810)  classification_loss: 1.4664 (1.4840)  loss_mask: 0.0219 (0.0319)  time: 0.1964  data: 0.0002  max mem: 5511
[07:55:44.483114] Epoch: [36]  [720/781]  eta: 0:00:12  lr: 0.000187  training_loss: 1.8254 (1.7978)  mae_loss: 0.2770 (0.2810)  classification_loss: 1.4722 (1.4840)  loss_mask: 0.0384 (0.0328)  time: 0.1972  data: 0.0002  max mem: 5511
[07:55:48.423029] Epoch: [36]  [740/781]  eta: 0:00:08  lr: 0.000187  training_loss: 1.7752 (1.7977)  mae_loss: 0.2788 (0.2810)  classification_loss: 1.4788 (1.4839)  loss_mask: 0.0148 (0.0327)  time: 0.1969  data: 0.0003  max mem: 5511
[07:55:52.363221] Epoch: [36]  [760/781]  eta: 0:00:04  lr: 0.000187  training_loss: 1.8119 (1.7983)  mae_loss: 0.2755 (0.2809)  classification_loss: 1.5373 (1.4851)  loss_mask: 0.0110 (0.0323)  time: 0.1969  data: 0.0002  max mem: 5511
[07:55:56.290578] Epoch: [36]  [780/781]  eta: 0:00:00  lr: 0.000187  training_loss: 1.7285 (1.7973)  mae_loss: 0.2749 (0.2809)  classification_loss: 1.4272 (1.4845)  loss_mask: 0.0102 (0.0319)  time: 0.1963  data: 0.0001  max mem: 5511
[07:55:56.450896] Epoch: [36] Total time: 0:02:35 (0.1987 s / it)
[07:55:56.451393] Averaged stats: lr: 0.000187  training_loss: 1.7285 (1.7973)  mae_loss: 0.2749 (0.2809)  classification_loss: 1.4272 (1.4845)  loss_mask: 0.0102 (0.0319)
[07:55:57.129503] Test:  [  0/157]  eta: 0:01:45  testing_loss: 0.8175 (0.8175)  acc1: 81.2500 (81.2500)  acc5: 92.1875 (92.1875)  time: 0.6736  data: 0.6441  max mem: 5511
[07:55:57.418730] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.8175 (0.8171)  acc1: 73.4375 (72.8693)  acc5: 98.4375 (97.7273)  time: 0.0874  data: 0.0588  max mem: 5511
[07:55:57.707228] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.7508 (0.7808)  acc1: 73.4375 (73.8095)  acc5: 98.4375 (98.1399)  time: 0.0287  data: 0.0002  max mem: 5511
[07:55:57.991406] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.7530 (0.7869)  acc1: 73.4375 (73.8911)  acc5: 98.4375 (98.1351)  time: 0.0285  data: 0.0002  max mem: 5511
[07:55:58.275045] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.7905 (0.7942)  acc1: 71.8750 (73.5137)  acc5: 98.4375 (98.1707)  time: 0.0282  data: 0.0002  max mem: 5511
[07:55:58.559028] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.7554 (0.7836)  acc1: 73.4375 (74.0196)  acc5: 98.4375 (98.2843)  time: 0.0282  data: 0.0002  max mem: 5511
[07:55:58.845923] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.7524 (0.7845)  acc1: 73.4375 (73.8217)  acc5: 98.4375 (98.2326)  time: 0.0284  data: 0.0002  max mem: 5511
[07:55:59.130504] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7453 (0.7742)  acc1: 75.0000 (74.3178)  acc5: 98.4375 (98.2394)  time: 0.0285  data: 0.0002  max mem: 5511
[07:55:59.415307] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7336 (0.7831)  acc1: 75.0000 (74.1319)  acc5: 98.4375 (98.1674)  time: 0.0283  data: 0.0002  max mem: 5511
[07:55:59.698149] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.8008 (0.7825)  acc1: 75.0000 (74.3132)  acc5: 98.4375 (98.1456)  time: 0.0283  data: 0.0002  max mem: 5511
[07:55:59.981057] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.7911 (0.7865)  acc1: 73.4375 (74.1182)  acc5: 98.4375 (98.1590)  time: 0.0282  data: 0.0002  max mem: 5511
[07:56:00.265506] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.7911 (0.7868)  acc1: 73.4375 (74.2117)  acc5: 98.4375 (98.1560)  time: 0.0282  data: 0.0002  max mem: 5511
[07:56:00.550362] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.7379 (0.7804)  acc1: 75.0000 (74.4189)  acc5: 98.4375 (98.1792)  time: 0.0283  data: 0.0002  max mem: 5511
[07:56:00.844614] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.7379 (0.7804)  acc1: 75.0000 (74.4156)  acc5: 98.4375 (98.2109)  time: 0.0288  data: 0.0002  max mem: 5511
[07:56:01.131667] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.8051 (0.7809)  acc1: 75.0000 (74.4570)  acc5: 98.4375 (98.1937)  time: 0.0289  data: 0.0002  max mem: 5511
[07:56:01.412403] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7876 (0.7779)  acc1: 75.0000 (74.5550)  acc5: 98.4375 (98.1581)  time: 0.0283  data: 0.0001  max mem: 5511
[07:56:01.561951] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7876 (0.7802)  acc1: 75.0000 (74.5000)  acc5: 98.4375 (98.1800)  time: 0.0270  data: 0.0001  max mem: 5511
[07:56:01.743601] Test: Total time: 0:00:05 (0.0337 s / it)
[07:56:01.744067] * Acc@1 74.500 Acc@5 98.180 loss 0.780
[07:56:01.744356] Accuracy of the network on the 10000 test images: 74.5%
[07:56:01.744523] Max accuracy: 74.89%
[07:56:01.902517] log_dir: ./output_dir
[07:56:02.812057] Epoch: [37]  [  0/781]  eta: 0:11:48  lr: 0.000187  training_loss: 1.6627 (1.6627)  mae_loss: 0.2954 (0.2954)  classification_loss: 1.3601 (1.3601)  loss_mask: 0.0072 (0.0072)  time: 0.9078  data: 0.6622  max mem: 5511
[07:56:06.757989] Epoch: [37]  [ 20/781]  eta: 0:02:55  lr: 0.000186  training_loss: 1.6908 (1.7206)  mae_loss: 0.2797 (0.2706)  classification_loss: 1.3981 (1.4380)  loss_mask: 0.0082 (0.0120)  time: 0.1972  data: 0.0002  max mem: 5511
[07:56:10.721951] Epoch: [37]  [ 40/781]  eta: 0:02:39  lr: 0.000186  training_loss: 1.8300 (1.7829)  mae_loss: 0.2828 (0.2790)  classification_loss: 1.4740 (1.4606)  loss_mask: 0.0264 (0.0433)  time: 0.1981  data: 0.0002  max mem: 5511
[07:56:14.702483] Epoch: [37]  [ 60/781]  eta: 0:02:31  lr: 0.000186  training_loss: 1.8194 (1.7987)  mae_loss: 0.2655 (0.2786)  classification_loss: 1.5187 (1.4767)  loss_mask: 0.0393 (0.0434)  time: 0.1989  data: 0.0002  max mem: 5511
[07:56:18.650715] Epoch: [37]  [ 80/781]  eta: 0:02:24  lr: 0.000186  training_loss: 1.8209 (1.8001)  mae_loss: 0.2808 (0.2792)  classification_loss: 1.4958 (1.4809)  loss_mask: 0.0205 (0.0400)  time: 0.1973  data: 0.0002  max mem: 5511
[07:56:22.600232] Epoch: [37]  [100/781]  eta: 0:02:19  lr: 0.000186  training_loss: 1.7715 (1.8013)  mae_loss: 0.2779 (0.2801)  classification_loss: 1.4727 (1.4818)  loss_mask: 0.0230 (0.0394)  time: 0.1974  data: 0.0003  max mem: 5511
[07:56:26.579665] Epoch: [37]  [120/781]  eta: 0:02:14  lr: 0.000186  training_loss: 1.7390 (1.7901)  mae_loss: 0.2622 (0.2779)  classification_loss: 1.4298 (1.4769)  loss_mask: 0.0091 (0.0353)  time: 0.1989  data: 0.0003  max mem: 5511
[07:56:30.526249] Epoch: [37]  [140/781]  eta: 0:02:10  lr: 0.000186  training_loss: 1.7939 (1.7901)  mae_loss: 0.2778 (0.2788)  classification_loss: 1.4995 (1.4785)  loss_mask: 0.0067 (0.0328)  time: 0.1972  data: 0.0002  max mem: 5511
[07:56:34.462733] Epoch: [37]  [160/781]  eta: 0:02:05  lr: 0.000186  training_loss: 1.7336 (1.7861)  mae_loss: 0.2835 (0.2792)  classification_loss: 1.4541 (1.4769)  loss_mask: 0.0068 (0.0300)  time: 0.1967  data: 0.0002  max mem: 5511
[07:56:38.411597] Epoch: [37]  [180/781]  eta: 0:02:01  lr: 0.000186  training_loss: 1.7312 (1.7822)  mae_loss: 0.2859 (0.2792)  classification_loss: 1.4402 (1.4750)  loss_mask: 0.0110 (0.0280)  time: 0.1974  data: 0.0002  max mem: 5511
[07:56:42.346310] Epoch: [37]  [200/781]  eta: 0:01:56  lr: 0.000186  training_loss: 1.8408 (1.7925)  mae_loss: 0.2651 (0.2794)  classification_loss: 1.4948 (1.4793)  loss_mask: 0.0327 (0.0338)  time: 0.1967  data: 0.0002  max mem: 5511
[07:56:46.279243] Epoch: [37]  [220/781]  eta: 0:01:52  lr: 0.000186  training_loss: 1.8592 (1.8001)  mae_loss: 0.2831 (0.2808)  classification_loss: 1.4943 (1.4798)  loss_mask: 0.0898 (0.0395)  time: 0.1965  data: 0.0003  max mem: 5511
[07:56:50.276737] Epoch: [37]  [240/781]  eta: 0:01:48  lr: 0.000185  training_loss: 1.8040 (1.8020)  mae_loss: 0.2799 (0.2811)  classification_loss: 1.4492 (1.4786)  loss_mask: 0.0536 (0.0423)  time: 0.1998  data: 0.0002  max mem: 5511
[07:56:54.247544] Epoch: [37]  [260/781]  eta: 0:01:44  lr: 0.000185  training_loss: 1.7394 (1.7990)  mae_loss: 0.2588 (0.2798)  classification_loss: 1.4541 (1.4785)  loss_mask: 0.0150 (0.0407)  time: 0.1985  data: 0.0002  max mem: 5511
[07:56:58.198000] Epoch: [37]  [280/781]  eta: 0:01:40  lr: 0.000185  training_loss: 1.7424 (1.7956)  mae_loss: 0.2691 (0.2792)  classification_loss: 1.4467 (1.4777)  loss_mask: 0.0116 (0.0387)  time: 0.1974  data: 0.0002  max mem: 5511
[07:57:02.167282] Epoch: [37]  [300/781]  eta: 0:01:36  lr: 0.000185  training_loss: 1.7609 (1.7946)  mae_loss: 0.2722 (0.2793)  classification_loss: 1.4681 (1.4785)  loss_mask: 0.0079 (0.0368)  time: 0.1984  data: 0.0002  max mem: 5511
[07:57:06.112963] Epoch: [37]  [320/781]  eta: 0:01:32  lr: 0.000185  training_loss: 1.7498 (1.7941)  mae_loss: 0.2845 (0.2795)  classification_loss: 1.4801 (1.4790)  loss_mask: 0.0084 (0.0357)  time: 0.1972  data: 0.0002  max mem: 5511
[07:57:10.094277] Epoch: [37]  [340/781]  eta: 0:01:28  lr: 0.000185  training_loss: 1.7665 (1.7923)  mae_loss: 0.2786 (0.2792)  classification_loss: 1.4601 (1.4788)  loss_mask: 0.0077 (0.0343)  time: 0.1990  data: 0.0004  max mem: 5511
[07:57:14.036554] Epoch: [37]  [360/781]  eta: 0:01:24  lr: 0.000185  training_loss: 1.8023 (1.7926)  mae_loss: 0.2856 (0.2800)  classification_loss: 1.4784 (1.4793)  loss_mask: 0.0110 (0.0333)  time: 0.1970  data: 0.0003  max mem: 5511
[07:57:17.972151] Epoch: [37]  [380/781]  eta: 0:01:20  lr: 0.000185  training_loss: 1.7705 (1.7920)  mae_loss: 0.2996 (0.2809)  classification_loss: 1.4550 (1.4785)  loss_mask: 0.0103 (0.0327)  time: 0.1967  data: 0.0002  max mem: 5511
[07:57:21.911451] Epoch: [37]  [400/781]  eta: 0:01:15  lr: 0.000185  training_loss: 1.7722 (1.7920)  mae_loss: 0.2790 (0.2809)  classification_loss: 1.4860 (1.4795)  loss_mask: 0.0065 (0.0316)  time: 0.1969  data: 0.0002  max mem: 5511
[07:57:25.843261] Epoch: [37]  [420/781]  eta: 0:01:11  lr: 0.000185  training_loss: 1.7623 (1.7909)  mae_loss: 0.2740 (0.2809)  classification_loss: 1.4547 (1.4793)  loss_mask: 0.0063 (0.0307)  time: 0.1965  data: 0.0002  max mem: 5511
[07:57:29.779168] Epoch: [37]  [440/781]  eta: 0:01:07  lr: 0.000185  training_loss: 1.7287 (1.7917)  mae_loss: 0.2713 (0.2802)  classification_loss: 1.4632 (1.4790)  loss_mask: 0.0029 (0.0324)  time: 0.1967  data: 0.0002  max mem: 5511
[07:57:33.712596] Epoch: [37]  [460/781]  eta: 0:01:03  lr: 0.000184  training_loss: 1.9179 (1.7987)  mae_loss: 0.2694 (0.2801)  classification_loss: 1.4330 (1.4772)  loss_mask: 0.1604 (0.0414)  time: 0.1966  data: 0.0002  max mem: 5511
[07:57:37.647536] Epoch: [37]  [480/781]  eta: 0:00:59  lr: 0.000184  training_loss: 1.9333 (1.8019)  mae_loss: 0.2838 (0.2802)  classification_loss: 1.4659 (1.4770)  loss_mask: 0.0942 (0.0447)  time: 0.1966  data: 0.0002  max mem: 5511
[07:57:41.589477] Epoch: [37]  [500/781]  eta: 0:00:55  lr: 0.000184  training_loss: 1.8046 (1.8020)  mae_loss: 0.2645 (0.2799)  classification_loss: 1.4861 (1.4774)  loss_mask: 0.0385 (0.0447)  time: 0.1970  data: 0.0005  max mem: 5511
[07:57:45.507369] Epoch: [37]  [520/781]  eta: 0:00:51  lr: 0.000184  training_loss: 1.7527 (1.8012)  mae_loss: 0.2941 (0.2802)  classification_loss: 1.4462 (1.4773)  loss_mask: 0.0131 (0.0437)  time: 0.1958  data: 0.0002  max mem: 5511
[07:57:49.438739] Epoch: [37]  [540/781]  eta: 0:00:47  lr: 0.000184  training_loss: 1.8103 (1.8013)  mae_loss: 0.2727 (0.2801)  classification_loss: 1.5160 (1.4786)  loss_mask: 0.0121 (0.0426)  time: 0.1965  data: 0.0002  max mem: 5511
[07:57:53.395180] Epoch: [37]  [560/781]  eta: 0:00:43  lr: 0.000184  training_loss: 1.7579 (1.7995)  mae_loss: 0.2769 (0.2802)  classification_loss: 1.4383 (1.4774)  loss_mask: 0.0112 (0.0418)  time: 0.1977  data: 0.0002  max mem: 5511
[07:57:57.364819] Epoch: [37]  [580/781]  eta: 0:00:39  lr: 0.000184  training_loss: 1.7238 (1.7977)  mae_loss: 0.2670 (0.2798)  classification_loss: 1.4437 (1.4770)  loss_mask: 0.0124 (0.0410)  time: 0.1984  data: 0.0002  max mem: 5511
[07:58:01.322973] Epoch: [37]  [600/781]  eta: 0:00:35  lr: 0.000184  training_loss: 1.7543 (1.7963)  mae_loss: 0.2671 (0.2795)  classification_loss: 1.4438 (1.4770)  loss_mask: 0.0050 (0.0399)  time: 0.1978  data: 0.0002  max mem: 5511
[07:58:05.285925] Epoch: [37]  [620/781]  eta: 0:00:31  lr: 0.000184  training_loss: 1.7434 (1.7963)  mae_loss: 0.2785 (0.2797)  classification_loss: 1.4505 (1.4769)  loss_mask: 0.0293 (0.0396)  time: 0.1981  data: 0.0002  max mem: 5511
[07:58:09.240516] Epoch: [37]  [640/781]  eta: 0:00:27  lr: 0.000184  training_loss: 1.8274 (1.7970)  mae_loss: 0.2918 (0.2801)  classification_loss: 1.5148 (1.4777)  loss_mask: 0.0141 (0.0391)  time: 0.1977  data: 0.0002  max mem: 5511
[07:58:13.237780] Epoch: [37]  [660/781]  eta: 0:00:24  lr: 0.000184  training_loss: 1.7567 (1.7964)  mae_loss: 0.2844 (0.2804)  classification_loss: 1.4939 (1.4778)  loss_mask: 0.0049 (0.0382)  time: 0.1998  data: 0.0003  max mem: 5511
[07:58:17.221636] Epoch: [37]  [680/781]  eta: 0:00:20  lr: 0.000183  training_loss: 1.7527 (1.7963)  mae_loss: 0.2641 (0.2802)  classification_loss: 1.5016 (1.4787)  loss_mask: 0.0045 (0.0374)  time: 0.1991  data: 0.0003  max mem: 5511
[07:58:21.167706] Epoch: [37]  [700/781]  eta: 0:00:16  lr: 0.000183  training_loss: 1.7940 (1.7961)  mae_loss: 0.2815 (0.2802)  classification_loss: 1.5037 (1.4791)  loss_mask: 0.0090 (0.0369)  time: 0.1972  data: 0.0004  max mem: 5511
[07:58:25.101146] Epoch: [37]  [720/781]  eta: 0:00:12  lr: 0.000183  training_loss: 1.8009 (1.7962)  mae_loss: 0.2744 (0.2800)  classification_loss: 1.4956 (1.4793)  loss_mask: 0.0217 (0.0369)  time: 0.1966  data: 0.0002  max mem: 5511
[07:58:29.052305] Epoch: [37]  [740/781]  eta: 0:00:08  lr: 0.000183  training_loss: 1.7444 (1.7957)  mae_loss: 0.2628 (0.2799)  classification_loss: 1.4662 (1.4793)  loss_mask: 0.0151 (0.0365)  time: 0.1975  data: 0.0005  max mem: 5511
[07:58:32.993403] Epoch: [37]  [760/781]  eta: 0:00:04  lr: 0.000183  training_loss: 1.7800 (1.7950)  mae_loss: 0.2724 (0.2796)  classification_loss: 1.4884 (1.4795)  loss_mask: 0.0094 (0.0360)  time: 0.1970  data: 0.0002  max mem: 5511
[07:58:36.919945] Epoch: [37]  [780/781]  eta: 0:00:00  lr: 0.000183  training_loss: 1.7630 (1.7937)  mae_loss: 0.2728 (0.2793)  classification_loss: 1.4410 (1.4785)  loss_mask: 0.0160 (0.0358)  time: 0.1962  data: 0.0002  max mem: 5511
[07:58:37.075303] Epoch: [37] Total time: 0:02:35 (0.1987 s / it)
[07:58:37.075970] Averaged stats: lr: 0.000183  training_loss: 1.7630 (1.7937)  mae_loss: 0.2728 (0.2793)  classification_loss: 1.4410 (1.4785)  loss_mask: 0.0160 (0.0358)
[07:58:37.658869] Test:  [  0/157]  eta: 0:01:30  testing_loss: 0.7321 (0.7321)  acc1: 79.6875 (79.6875)  acc5: 98.4375 (98.4375)  time: 0.5780  data: 0.5484  max mem: 5511
[07:58:37.948713] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.7321 (0.7598)  acc1: 75.0000 (75.5682)  acc5: 98.4375 (99.0057)  time: 0.0785  data: 0.0500  max mem: 5511
[07:58:38.237967] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.7164 (0.7326)  acc1: 76.5625 (76.8601)  acc5: 98.4375 (98.8095)  time: 0.0287  data: 0.0002  max mem: 5511
[07:58:38.521833] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.7164 (0.7483)  acc1: 76.5625 (76.4113)  acc5: 98.4375 (98.3871)  time: 0.0285  data: 0.0002  max mem: 5511
[07:58:38.806887] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.7420 (0.7569)  acc1: 75.0000 (76.0290)  acc5: 98.4375 (98.4756)  time: 0.0283  data: 0.0002  max mem: 5511
[07:58:39.089703] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.7354 (0.7474)  acc1: 76.5625 (76.4093)  acc5: 98.4375 (98.4681)  time: 0.0282  data: 0.0002  max mem: 5511
[07:58:39.373564] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.7219 (0.7439)  acc1: 76.5625 (76.2551)  acc5: 98.4375 (98.5143)  time: 0.0282  data: 0.0002  max mem: 5511
[07:58:39.657772] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7216 (0.7386)  acc1: 75.0000 (76.4525)  acc5: 98.4375 (98.5035)  time: 0.0283  data: 0.0002  max mem: 5511
[07:58:39.942814] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7358 (0.7489)  acc1: 75.0000 (76.1381)  acc5: 98.4375 (98.2832)  time: 0.0283  data: 0.0002  max mem: 5511
[07:58:40.227128] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7894 (0.7502)  acc1: 75.0000 (76.0989)  acc5: 98.4375 (98.3173)  time: 0.0283  data: 0.0002  max mem: 5511
[07:58:40.513944] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.7587 (0.7562)  acc1: 71.8750 (75.6652)  acc5: 98.4375 (98.3292)  time: 0.0284  data: 0.0002  max mem: 5511
[07:58:40.796363] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.8249 (0.7597)  acc1: 71.8750 (75.5490)  acc5: 98.4375 (98.2967)  time: 0.0283  data: 0.0002  max mem: 5511
[07:58:41.084000] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.7384 (0.7530)  acc1: 75.0000 (75.8264)  acc5: 98.4375 (98.3213)  time: 0.0284  data: 0.0002  max mem: 5511
[07:58:41.369063] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6829 (0.7526)  acc1: 78.1250 (75.8588)  acc5: 98.4375 (98.3659)  time: 0.0285  data: 0.0002  max mem: 5511
[07:58:41.652907] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7540 (0.7523)  acc1: 78.1250 (75.8644)  acc5: 98.4375 (98.3932)  time: 0.0282  data: 0.0002  max mem: 5511
[07:58:41.935045] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7556 (0.7504)  acc1: 75.0000 (75.9209)  acc5: 98.4375 (98.3754)  time: 0.0281  data: 0.0002  max mem: 5511
[07:58:42.086771] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7664 (0.7526)  acc1: 75.0000 (75.8000)  acc5: 98.4375 (98.3800)  time: 0.0272  data: 0.0001  max mem: 5511
[07:58:42.250545] Test: Total time: 0:00:05 (0.0329 s / it)
[07:58:42.251181] * Acc@1 75.800 Acc@5 98.380 loss 0.753
[07:58:42.251490] Accuracy of the network on the 10000 test images: 75.8%
[07:58:42.251694] Max accuracy: 75.80%
[07:58:42.331162] log_dir: ./output_dir
[07:58:43.256524] Epoch: [38]  [  0/781]  eta: 0:12:01  lr: 0.000183  training_loss: 1.3598 (1.3598)  mae_loss: 0.2297 (0.2297)  classification_loss: 1.1032 (1.1032)  loss_mask: 0.0269 (0.0269)  time: 0.9238  data: 0.7098  max mem: 5511
[07:58:47.175093] Epoch: [38]  [ 20/781]  eta: 0:02:55  lr: 0.000183  training_loss: 1.8062 (1.8133)  mae_loss: 0.2877 (0.2908)  classification_loss: 1.4398 (1.4391)  loss_mask: 0.0710 (0.0833)  time: 0.1958  data: 0.0003  max mem: 5511
[07:58:51.115297] Epoch: [38]  [ 40/781]  eta: 0:02:38  lr: 0.000183  training_loss: 1.7750 (1.8000)  mae_loss: 0.2672 (0.2797)  classification_loss: 1.4725 (1.4504)  loss_mask: 0.0435 (0.0700)  time: 0.1969  data: 0.0002  max mem: 5511
[07:58:55.051410] Epoch: [38]  [ 60/781]  eta: 0:02:30  lr: 0.000183  training_loss: 1.7794 (1.8103)  mae_loss: 0.2833 (0.2822)  classification_loss: 1.4761 (1.4647)  loss_mask: 0.0347 (0.0634)  time: 0.1967  data: 0.0002  max mem: 5511
[07:58:58.995733] Epoch: [38]  [ 80/781]  eta: 0:02:24  lr: 0.000183  training_loss: 1.7186 (1.8013)  mae_loss: 0.2654 (0.2802)  classification_loss: 1.4447 (1.4692)  loss_mask: 0.0132 (0.0518)  time: 0.1971  data: 0.0002  max mem: 5511
[07:59:02.960435] Epoch: [38]  [100/781]  eta: 0:02:19  lr: 0.000182  training_loss: 1.7617 (1.7912)  mae_loss: 0.2676 (0.2790)  classification_loss: 1.4617 (1.4695)  loss_mask: 0.0039 (0.0427)  time: 0.1982  data: 0.0003  max mem: 5511
[07:59:06.889394] Epoch: [38]  [120/781]  eta: 0:02:14  lr: 0.000182  training_loss: 1.6869 (1.7795)  mae_loss: 0.2625 (0.2771)  classification_loss: 1.4533 (1.4660)  loss_mask: 0.0032 (0.0364)  time: 0.1964  data: 0.0002  max mem: 5511
[07:59:10.838027] Epoch: [38]  [140/781]  eta: 0:02:09  lr: 0.000182  training_loss: 1.7414 (1.7723)  mae_loss: 0.2666 (0.2761)  classification_loss: 1.4388 (1.4639)  loss_mask: 0.0031 (0.0323)  time: 0.1973  data: 0.0002  max mem: 5511
[07:59:14.770295] Epoch: [38]  [160/781]  eta: 0:02:05  lr: 0.000182  training_loss: 1.7408 (1.7692)  mae_loss: 0.2759 (0.2760)  classification_loss: 1.4222 (1.4638)  loss_mask: 0.0016 (0.0294)  time: 0.1965  data: 0.0002  max mem: 5511
[07:59:18.743382] Epoch: [38]  [180/781]  eta: 0:02:00  lr: 0.000182  training_loss: 1.7348 (1.7682)  mae_loss: 0.2670 (0.2760)  classification_loss: 1.4379 (1.4634)  loss_mask: 0.0132 (0.0288)  time: 0.1986  data: 0.0002  max mem: 5511
[07:59:22.688410] Epoch: [38]  [200/781]  eta: 0:01:56  lr: 0.000182  training_loss: 1.8113 (1.7734)  mae_loss: 0.2745 (0.2758)  classification_loss: 1.4676 (1.4649)  loss_mask: 0.0204 (0.0327)  time: 0.1972  data: 0.0002  max mem: 5511
[07:59:26.617830] Epoch: [38]  [220/781]  eta: 0:01:52  lr: 0.000182  training_loss: 1.7614 (1.7744)  mae_loss: 0.2689 (0.2753)  classification_loss: 1.4460 (1.4636)  loss_mask: 0.0441 (0.0354)  time: 0.1964  data: 0.0002  max mem: 5511
[07:59:30.646857] Epoch: [38]  [240/781]  eta: 0:01:48  lr: 0.000182  training_loss: 1.7470 (1.7754)  mae_loss: 0.2610 (0.2748)  classification_loss: 1.4702 (1.4663)  loss_mask: 0.0169 (0.0343)  time: 0.2014  data: 0.0003  max mem: 5511
[07:59:34.593610] Epoch: [38]  [260/781]  eta: 0:01:44  lr: 0.000182  training_loss: 1.7108 (1.7694)  mae_loss: 0.2723 (0.2749)  classification_loss: 1.4348 (1.4623)  loss_mask: 0.0049 (0.0321)  time: 0.1972  data: 0.0002  max mem: 5511
[07:59:38.524474] Epoch: [38]  [280/781]  eta: 0:01:40  lr: 0.000182  training_loss: 1.7693 (1.7685)  mae_loss: 0.2560 (0.2742)  classification_loss: 1.4879 (1.4642)  loss_mask: 0.0028 (0.0301)  time: 0.1964  data: 0.0003  max mem: 5511
[07:59:42.456360] Epoch: [38]  [300/781]  eta: 0:01:36  lr: 0.000182  training_loss: 1.7527 (1.7687)  mae_loss: 0.2652 (0.2739)  classification_loss: 1.4843 (1.4665)  loss_mask: 0.0022 (0.0283)  time: 0.1965  data: 0.0002  max mem: 5511
[07:59:46.392376] Epoch: [38]  [320/781]  eta: 0:01:31  lr: 0.000181  training_loss: 1.7464 (1.7677)  mae_loss: 0.2726 (0.2745)  classification_loss: 1.4694 (1.4664)  loss_mask: 0.0018 (0.0268)  time: 0.1967  data: 0.0002  max mem: 5511
[07:59:50.323941] Epoch: [38]  [340/781]  eta: 0:01:27  lr: 0.000181  training_loss: 1.7669 (1.7669)  mae_loss: 0.2877 (0.2752)  classification_loss: 1.4494 (1.4664)  loss_mask: 0.0017 (0.0253)  time: 0.1965  data: 0.0002  max mem: 5511
[07:59:54.297625] Epoch: [38]  [360/781]  eta: 0:01:23  lr: 0.000181  training_loss: 1.7768 (1.7671)  mae_loss: 0.2644 (0.2751)  classification_loss: 1.5099 (1.4680)  loss_mask: 0.0016 (0.0240)  time: 0.1986  data: 0.0002  max mem: 5511
[07:59:58.256723] Epoch: [38]  [380/781]  eta: 0:01:19  lr: 0.000181  training_loss: 1.6838 (1.7639)  mae_loss: 0.2707 (0.2754)  classification_loss: 1.4011 (1.4656)  loss_mask: 0.0012 (0.0229)  time: 0.1979  data: 0.0002  max mem: 5511

[08:00:02.197101] Epoch: [38]  [400/781]  eta: 0:01:15  lr: 0.000181  training_loss: 1.7090 (1.7616)  mae_loss: 0.2784 (0.2758)  classification_loss: 1.4209 (1.4640)  loss_mask: 0.0008 (0.0218)  time: 0.1969  data: 0.0002  max mem: 5511
[08:00:06.132694] Epoch: [38]  [420/781]  eta: 0:01:11  lr: 0.000181  training_loss: 1.6903 (1.7591)  mae_loss: 0.2741 (0.2760)  classification_loss: 1.4232 (1.4623)  loss_mask: 0.0009 (0.0208)  time: 0.1967  data: 0.0002  max mem: 5511
[08:00:10.083417] Epoch: [38]  [440/781]  eta: 0:01:07  lr: 0.000181  training_loss: 1.7429 (1.7585)  mae_loss: 0.2673 (0.2760)  classification_loss: 1.4695 (1.4625)  loss_mask: 0.0010 (0.0199)  time: 0.1975  data: 0.0002  max mem: 5511
[08:00:14.031459] Epoch: [38]  [460/781]  eta: 0:01:03  lr: 0.000181  training_loss: 1.6995 (1.7563)  mae_loss: 0.2665 (0.2755)  classification_loss: 1.4259 (1.4616)  loss_mask: 0.0009 (0.0192)  time: 0.1973  data: 0.0003  max mem: 5511
[08:00:17.962602] Epoch: [38]  [480/781]  eta: 0:00:59  lr: 0.000181  training_loss: 1.7946 (1.7601)  mae_loss: 0.2716 (0.2757)  classification_loss: 1.4508 (1.4617)  loss_mask: 0.0615 (0.0227)  time: 0.1965  data: 0.0002  max mem: 5511
[08:00:21.953484] Epoch: [38]  [500/781]  eta: 0:00:55  lr: 0.000181  training_loss: 1.7752 (1.7610)  mae_loss: 0.2796 (0.2759)  classification_loss: 1.4571 (1.4621)  loss_mask: 0.0267 (0.0230)  time: 0.1994  data: 0.0002  max mem: 5511
[08:00:25.917288] Epoch: [38]  [520/781]  eta: 0:00:51  lr: 0.000180  training_loss: 1.7279 (1.7604)  mae_loss: 0.2782 (0.2762)  classification_loss: 1.3859 (1.4609)  loss_mask: 0.0149 (0.0233)  time: 0.1981  data: 0.0002  max mem: 5511
[08:00:29.915685] Epoch: [38]  [540/781]  eta: 0:00:47  lr: 0.000180  training_loss: 1.8120 (1.7635)  mae_loss: 0.2705 (0.2762)  classification_loss: 1.5034 (1.4630)  loss_mask: 0.0400 (0.0243)  time: 0.1998  data: 0.0002  max mem: 5511
[08:00:33.851029] Epoch: [38]  [560/781]  eta: 0:00:43  lr: 0.000180  training_loss: 1.7795 (1.7635)  mae_loss: 0.2703 (0.2762)  classification_loss: 1.4427 (1.4620)  loss_mask: 0.0363 (0.0252)  time: 0.1967  data: 0.0002  max mem: 5511
[08:00:37.792948] Epoch: [38]  [580/781]  eta: 0:00:39  lr: 0.000180  training_loss: 1.8450 (1.7665)  mae_loss: 0.2868 (0.2769)  classification_loss: 1.4644 (1.4622)  loss_mask: 0.0668 (0.0273)  time: 0.1970  data: 0.0002  max mem: 5511
[08:00:41.740284] Epoch: [38]  [600/781]  eta: 0:00:35  lr: 0.000180  training_loss: 1.8744 (1.7703)  mae_loss: 0.2636 (0.2767)  classification_loss: 1.4966 (1.4631)  loss_mask: 0.0794 (0.0305)  time: 0.1973  data: 0.0002  max mem: 5511
[08:00:45.729891] Epoch: [38]  [620/781]  eta: 0:00:31  lr: 0.000180  training_loss: 1.7308 (1.7701)  mae_loss: 0.2843 (0.2770)  classification_loss: 1.4184 (1.4621)  loss_mask: 0.0441 (0.0310)  time: 0.1994  data: 0.0002  max mem: 5511
[08:00:49.656788] Epoch: [38]  [640/781]  eta: 0:00:27  lr: 0.000180  training_loss: 1.6931 (1.7690)  mae_loss: 0.2539 (0.2768)  classification_loss: 1.4216 (1.4614)  loss_mask: 0.0163 (0.0308)  time: 0.1963  data: 0.0002  max mem: 5511
[08:00:53.588894] Epoch: [38]  [660/781]  eta: 0:00:24  lr: 0.000180  training_loss: 1.7519 (1.7687)  mae_loss: 0.2729 (0.2768)  classification_loss: 1.4506 (1.4617)  loss_mask: 0.0088 (0.0302)  time: 0.1965  data: 0.0002  max mem: 5511
[08:00:57.553473] Epoch: [38]  [680/781]  eta: 0:00:20  lr: 0.000180  training_loss: 1.7661 (1.7688)  mae_loss: 0.2861 (0.2772)  classification_loss: 1.4841 (1.4621)  loss_mask: 0.0042 (0.0295)  time: 0.1981  data: 0.0002  max mem: 5511
[08:01:01.515835] Epoch: [38]  [700/781]  eta: 0:00:16  lr: 0.000180  training_loss: 1.7873 (1.7692)  mae_loss: 0.2888 (0.2774)  classification_loss: 1.4995 (1.4628)  loss_mask: 0.0038 (0.0290)  time: 0.1980  data: 0.0002  max mem: 5511
[08:01:05.463849] Epoch: [38]  [720/781]  eta: 0:00:12  lr: 0.000180  training_loss: 1.7743 (1.7699)  mae_loss: 0.2674 (0.2773)  classification_loss: 1.4869 (1.4641)  loss_mask: 0.0072 (0.0285)  time: 0.1973  data: 0.0002  max mem: 5511
[08:01:09.400075] Epoch: [38]  [740/781]  eta: 0:00:08  lr: 0.000179  training_loss: 1.7869 (1.7715)  mae_loss: 0.2714 (0.2774)  classification_loss: 1.4668 (1.4644)  loss_mask: 0.0253 (0.0297)  time: 0.1967  data: 0.0003  max mem: 5511
[08:01:13.329077] Epoch: [38]  [760/781]  eta: 0:00:04  lr: 0.000179  training_loss: 1.9074 (1.7746)  mae_loss: 0.2747 (0.2775)  classification_loss: 1.5361 (1.4663)  loss_mask: 0.0401 (0.0308)  time: 0.1964  data: 0.0003  max mem: 5511
[08:01:17.259114] Epoch: [38]  [780/781]  eta: 0:00:00  lr: 0.000179  training_loss: 1.7644 (1.7750)  mae_loss: 0.2813 (0.2773)  classification_loss: 1.4666 (1.4668)  loss_mask: 0.0251 (0.0309)  time: 0.1964  data: 0.0003  max mem: 5511
[08:01:17.429162] Epoch: [38] Total time: 0:02:35 (0.1986 s / it)
[08:01:17.429743] Averaged stats: lr: 0.000179  training_loss: 1.7644 (1.7750)  mae_loss: 0.2813 (0.2773)  classification_loss: 1.4666 (1.4668)  loss_mask: 0.0251 (0.0309)
[08:01:17.995007] Test:  [  0/157]  eta: 0:01:27  testing_loss: 0.7254 (0.7254)  acc1: 78.1250 (78.1250)  acc5: 96.8750 (96.8750)  time: 0.5577  data: 0.5281  max mem: 5511
[08:01:18.281115] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.7852 (0.7772)  acc1: 76.5625 (74.8580)  acc5: 98.4375 (98.5795)  time: 0.0766  data: 0.0483  max mem: 5511
[08:01:18.566609] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.7493 (0.7511)  acc1: 78.1250 (77.0833)  acc5: 98.4375 (98.5863)  time: 0.0284  data: 0.0003  max mem: 5511
[08:01:18.850550] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.7493 (0.7617)  acc1: 76.5625 (76.0081)  acc5: 98.4375 (98.3367)  time: 0.0283  data: 0.0002  max mem: 5511
[08:01:19.132369] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.7939 (0.7758)  acc1: 71.8750 (75.2287)  acc5: 98.4375 (98.2470)  time: 0.0282  data: 0.0002  max mem: 5511
[08:01:19.414900] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.7637 (0.7659)  acc1: 73.4375 (75.7047)  acc5: 98.4375 (98.3150)  time: 0.0281  data: 0.0002  max mem: 5511
[08:01:19.709155] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.7512 (0.7650)  acc1: 73.4375 (75.4098)  acc5: 98.4375 (98.2582)  time: 0.0287  data: 0.0003  max mem: 5511
[08:01:19.993920] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7456 (0.7587)  acc1: 76.5625 (75.5502)  acc5: 98.4375 (98.2835)  time: 0.0288  data: 0.0003  max mem: 5511
[08:01:20.279188] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7472 (0.7674)  acc1: 75.0000 (75.2315)  acc5: 98.4375 (98.2060)  time: 0.0284  data: 0.0002  max mem: 5511
[08:01:20.565464] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.8067 (0.7701)  acc1: 75.0000 (75.3091)  acc5: 98.4375 (98.1971)  time: 0.0285  data: 0.0002  max mem: 5511
[08:01:20.854396] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.7730 (0.7730)  acc1: 73.4375 (75.0619)  acc5: 98.4375 (98.1900)  time: 0.0286  data: 0.0002  max mem: 5511
[08:01:21.139170] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.8080 (0.7758)  acc1: 71.8750 (74.9015)  acc5: 98.4375 (98.1982)  time: 0.0285  data: 0.0002  max mem: 5511
[08:01:21.423706] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.7561 (0.7701)  acc1: 73.4375 (75.0129)  acc5: 100.0000 (98.2825)  time: 0.0283  data: 0.0002  max mem: 5511
[08:01:21.711201] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.7188 (0.7703)  acc1: 75.0000 (74.8807)  acc5: 100.0000 (98.3182)  time: 0.0285  data: 0.0002  max mem: 5511
[08:01:21.997006] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.8197 (0.7708)  acc1: 73.4375 (74.7562)  acc5: 98.4375 (98.3267)  time: 0.0285  data: 0.0002  max mem: 5511
[08:01:22.277719] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.8010 (0.7698)  acc1: 73.4375 (74.8344)  acc5: 98.4375 (98.2926)  time: 0.0282  data: 0.0001  max mem: 5511
[08:01:22.427954] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7445 (0.7703)  acc1: 75.0000 (74.8100)  acc5: 98.4375 (98.2900)  time: 0.0271  data: 0.0001  max mem: 5511
[08:01:22.583511] Test: Total time: 0:00:05 (0.0328 s / it)
[08:01:22.584586] * Acc@1 74.810 Acc@5 98.290 loss 0.770
[08:01:22.584935] Accuracy of the network on the 10000 test images: 74.8%
[08:01:22.585146] Max accuracy: 75.80%
[08:01:22.767243] log_dir: ./output_dir
[08:01:23.767752] Epoch: [39]  [  0/781]  eta: 0:13:00  lr: 0.000179  training_loss: 1.4492 (1.4492)  mae_loss: 0.2441 (0.2441)  classification_loss: 1.1983 (1.1983)  loss_mask: 0.0068 (0.0068)  time: 0.9987  data: 0.7679  max mem: 5511
[08:01:27.717709] Epoch: [39]  [ 20/781]  eta: 0:02:59  lr: 0.000179  training_loss: 1.7231 (1.7335)  mae_loss: 0.2725 (0.2804)  classification_loss: 1.4429 (1.4410)  loss_mask: 0.0092 (0.0120)  time: 0.1974  data: 0.0002  max mem: 5511
[08:01:31.642118] Epoch: [39]  [ 40/781]  eta: 0:02:40  lr: 0.000179  training_loss: 1.7650 (1.7458)  mae_loss: 0.2748 (0.2763)  classification_loss: 1.4679 (1.4576)  loss_mask: 0.0064 (0.0119)  time: 0.1961  data: 0.0003  max mem: 5511
[08:01:35.580039] Epoch: [39]  [ 60/781]  eta: 0:02:31  lr: 0.000179  training_loss: 1.7212 (1.7566)  mae_loss: 0.2669 (0.2747)  classification_loss: 1.4629 (1.4670)  loss_mask: 0.0107 (0.0149)  time: 0.1968  data: 0.0003  max mem: 5511
[08:01:39.506194] Epoch: [39]  [ 80/781]  eta: 0:02:24  lr: 0.000179  training_loss: 1.7666 (1.7539)  mae_loss: 0.2609 (0.2726)  classification_loss: 1.4971 (1.4663)  loss_mask: 0.0110 (0.0149)  time: 0.1962  data: 0.0003  max mem: 5511
[08:01:43.458794] Epoch: [39]  [100/781]  eta: 0:02:19  lr: 0.000179  training_loss: 1.7291 (1.7543)  mae_loss: 0.2782 (0.2731)  classification_loss: 1.4273 (1.4636)  loss_mask: 0.0164 (0.0176)  time: 0.1975  data: 0.0002  max mem: 5511
[08:01:47.402060] Epoch: [39]  [120/781]  eta: 0:02:14  lr: 0.000179  training_loss: 1.7726 (1.7545)  mae_loss: 0.2586 (0.2727)  classification_loss: 1.4278 (1.4577)  loss_mask: 0.0573 (0.0241)  time: 0.1970  data: 0.0003  max mem: 5511
[08:01:51.354340] Epoch: [39]  [140/781]  eta: 0:02:09  lr: 0.000179  training_loss: 1.7456 (1.7522)  mae_loss: 0.2669 (0.2719)  classification_loss: 1.4277 (1.4527)  loss_mask: 0.0316 (0.0276)  time: 0.1975  data: 0.0002  max mem: 5511
[08:01:55.305536] Epoch: [39]  [160/781]  eta: 0:02:05  lr: 0.000178  training_loss: 1.7166 (1.7469)  mae_loss: 0.2569 (0.2703)  classification_loss: 1.4336 (1.4494)  loss_mask: 0.0197 (0.0272)  time: 0.1975  data: 0.0002  max mem: 5511
[08:01:59.251002] Epoch: [39]  [180/781]  eta: 0:02:01  lr: 0.000178  training_loss: 1.7636 (1.7472)  mae_loss: 0.2767 (0.2710)  classification_loss: 1.4651 (1.4500)  loss_mask: 0.0098 (0.0262)  time: 0.1972  data: 0.0003  max mem: 5511
[08:02:03.188540] Epoch: [39]  [200/781]  eta: 0:01:56  lr: 0.000178  training_loss: 1.7421 (1.7454)  mae_loss: 0.2544 (0.2700)  classification_loss: 1.4383 (1.4512)  loss_mask: 0.0052 (0.0242)  time: 0.1968  data: 0.0003  max mem: 5511
[08:02:07.138569] Epoch: [39]  [220/781]  eta: 0:01:52  lr: 0.000178  training_loss: 1.7301 (1.7443)  mae_loss: 0.2717 (0.2708)  classification_loss: 1.4239 (1.4512)  loss_mask: 0.0021 (0.0223)  time: 0.1974  data: 0.0002  max mem: 5511
[08:02:11.095895] Epoch: [39]  [240/781]  eta: 0:01:48  lr: 0.000178  training_loss: 1.7724 (1.7467)  mae_loss: 0.2732 (0.2714)  classification_loss: 1.5006 (1.4548)  loss_mask: 0.0015 (0.0206)  time: 0.1978  data: 0.0002  max mem: 5511
[08:02:15.011391] Epoch: [39]  [260/781]  eta: 0:01:44  lr: 0.000178  training_loss: 1.6931 (1.7465)  mae_loss: 0.2626 (0.2714)  classification_loss: 1.4336 (1.4559)  loss_mask: 0.0014 (0.0191)  time: 0.1957  data: 0.0003  max mem: 5511
[08:02:18.943062] Epoch: [39]  [280/781]  eta: 0:01:40  lr: 0.000178  training_loss: 1.7713 (1.7475)  mae_loss: 0.2703 (0.2716)  classification_loss: 1.5119 (1.4580)  loss_mask: 0.0010 (0.0179)  time: 0.1965  data: 0.0002  max mem: 5511
[08:02:22.879960] Epoch: [39]  [300/781]  eta: 0:01:36  lr: 0.000178  training_loss: 1.7070 (1.7451)  mae_loss: 0.2636 (0.2711)  classification_loss: 1.4111 (1.4570)  loss_mask: 0.0008 (0.0169)  time: 0.1967  data: 0.0002  max mem: 5511
[08:02:26.855152] Epoch: [39]  [320/781]  eta: 0:01:31  lr: 0.000178  training_loss: 1.8089 (1.7499)  mae_loss: 0.2661 (0.2710)  classification_loss: 1.4231 (1.4565)  loss_mask: 0.0625 (0.0224)  time: 0.1987  data: 0.0002  max mem: 5511
[08:02:30.827139] Epoch: [39]  [340/781]  eta: 0:01:27  lr: 0.000178  training_loss: 1.8115 (1.7532)  mae_loss: 0.2779 (0.2716)  classification_loss: 1.4574 (1.4572)  loss_mask: 0.0394 (0.0243)  time: 0.1985  data: 0.0002  max mem: 5511
[08:02:34.787608] Epoch: [39]  [360/781]  eta: 0:01:23  lr: 0.000178  training_loss: 1.7456 (1.7521)  mae_loss: 0.2525 (0.2709)  classification_loss: 1.4540 (1.4573)  loss_mask: 0.0135 (0.0239)  time: 0.1979  data: 0.0002  max mem: 5511
[08:02:38.731006] Epoch: [39]  [380/781]  eta: 0:01:19  lr: 0.000177  training_loss: 1.7159 (1.7514)  mae_loss: 0.2799 (0.2711)  classification_loss: 1.4326 (1.4573)  loss_mask: 0.0048 (0.0229)  time: 0.1971  data: 0.0002  max mem: 5511
[08:02:42.672225] Epoch: [39]  [400/781]  eta: 0:01:15  lr: 0.000177  training_loss: 1.7523 (1.7525)  mae_loss: 0.2790 (0.2716)  classification_loss: 1.4980 (1.4589)  loss_mask: 0.0026 (0.0220)  time: 0.1970  data: 0.0002  max mem: 5511
[08:02:46.605263] Epoch: [39]  [420/781]  eta: 0:01:11  lr: 0.000177  training_loss: 1.7146 (1.7511)  mae_loss: 0.2633 (0.2716)  classification_loss: 1.4536 (1.4584)  loss_mask: 0.0026 (0.0211)  time: 0.1966  data: 0.0002  max mem: 5511
[08:02:50.560483] Epoch: [39]  [440/781]  eta: 0:01:07  lr: 0.000177  training_loss: 1.7128 (1.7492)  mae_loss: 0.2662 (0.2718)  classification_loss: 1.4290 (1.4571)  loss_mask: 0.0021 (0.0203)  time: 0.1977  data: 0.0002  max mem: 5511
[08:02:54.512045] Epoch: [39]  [460/781]  eta: 0:01:03  lr: 0.000177  training_loss: 1.8193 (1.7517)  mae_loss: 0.2689 (0.2718)  classification_loss: 1.4459 (1.4566)  loss_mask: 0.0190 (0.0233)  time: 0.1975  data: 0.0002  max mem: 5511
[08:02:58.441033] Epoch: [39]  [480/781]  eta: 0:00:59  lr: 0.000177  training_loss: 1.8378 (1.7559)  mae_loss: 0.2677 (0.2718)  classification_loss: 1.4590 (1.4579)  loss_mask: 0.0754 (0.0262)  time: 0.1964  data: 0.0002  max mem: 5511
[08:03:02.398999] Epoch: [39]  [500/781]  eta: 0:00:55  lr: 0.000177  training_loss: 1.7629 (1.7570)  mae_loss: 0.2749 (0.2721)  classification_loss: 1.4301 (1.4575)  loss_mask: 0.0480 (0.0274)  time: 0.1978  data: 0.0003  max mem: 5511
[08:03:06.328778] Epoch: [39]  [520/781]  eta: 0:00:51  lr: 0.000177  training_loss: 1.7740 (1.7573)  mae_loss: 0.2780 (0.2726)  classification_loss: 1.4628 (1.4577)  loss_mask: 0.0121 (0.0270)  time: 0.1964  data: 0.0003  max mem: 5511
[08:03:10.294024] Epoch: [39]  [540/781]  eta: 0:00:47  lr: 0.000177  training_loss: 1.7380 (1.7570)  mae_loss: 0.2672 (0.2727)  classification_loss: 1.4421 (1.4581)  loss_mask: 0.0057 (0.0262)  time: 0.1982  data: 0.0003  max mem: 5511
[08:03:14.233763] Epoch: [39]  [560/781]  eta: 0:00:43  lr: 0.000177  training_loss: 1.7646 (1.7566)  mae_loss: 0.2758 (0.2726)  classification_loss: 1.4737 (1.4583)  loss_mask: 0.0047 (0.0256)  time: 0.1969  data: 0.0002  max mem: 5511
[08:03:18.177462] Epoch: [39]  [580/781]  eta: 0:00:39  lr: 0.000176  training_loss: 1.7685 (1.7572)  mae_loss: 0.2655 (0.2726)  classification_loss: 1.4784 (1.4590)  loss_mask: 0.0105 (0.0256)  time: 0.1971  data: 0.0003  max mem: 5511
[08:03:22.132022] Epoch: [39]  [600/781]  eta: 0:00:35  lr: 0.000176  training_loss: 1.6886 (1.7554)  mae_loss: 0.2786 (0.2728)  classification_loss: 1.4145 (1.4570)  loss_mask: 0.0157 (0.0256)  time: 0.1976  data: 0.0002  max mem: 5511
[08:03:26.144134] Epoch: [39]  [620/781]  eta: 0:00:31  lr: 0.000176  training_loss: 1.7713 (1.7560)  mae_loss: 0.2803 (0.2733)  classification_loss: 1.4488 (1.4571)  loss_mask: 0.0140 (0.0256)  time: 0.2005  data: 0.0003  max mem: 5511
[08:03:30.091160] Epoch: [39]  [640/781]  eta: 0:00:27  lr: 0.000176  training_loss: 1.7981 (1.7578)  mae_loss: 0.2772 (0.2735)  classification_loss: 1.4643 (1.4569)  loss_mask: 0.0335 (0.0274)  time: 0.1973  data: 0.0002  max mem: 5511
[08:03:34.025872] Epoch: [39]  [660/781]  eta: 0:00:24  lr: 0.000176  training_loss: 1.8086 (1.7598)  mae_loss: 0.2857 (0.2741)  classification_loss: 1.4583 (1.4574)  loss_mask: 0.0356 (0.0283)  time: 0.1966  data: 0.0002  max mem: 5511
[08:03:37.964572] Epoch: [39]  [680/781]  eta: 0:00:20  lr: 0.000176  training_loss: 1.8920 (1.7653)  mae_loss: 0.2857 (0.2745)  classification_loss: 1.4693 (1.4586)  loss_mask: 0.1045 (0.0321)  time: 0.1968  data: 0.0004  max mem: 5511
[08:03:41.902162] Epoch: [39]  [700/781]  eta: 0:00:16  lr: 0.000176  training_loss: 1.8558 (1.7679)  mae_loss: 0.2798 (0.2748)  classification_loss: 1.4769 (1.4596)  loss_mask: 0.0658 (0.0335)  time: 0.1968  data: 0.0002  max mem: 5511
[08:03:45.864497] Epoch: [39]  [720/781]  eta: 0:00:12  lr: 0.000176  training_loss: 1.7754 (1.7686)  mae_loss: 0.2743 (0.2748)  classification_loss: 1.4249 (1.4594)  loss_mask: 0.0628 (0.0345)  time: 0.1980  data: 0.0002  max mem: 5511
[08:03:49.795690] Epoch: [39]  [740/781]  eta: 0:00:08  lr: 0.000176  training_loss: 1.6940 (1.7670)  mae_loss: 0.2601 (0.2746)  classification_loss: 1.4247 (1.4583)  loss_mask: 0.0175 (0.0341)  time: 0.1964  data: 0.0002  max mem: 5511
[08:03:53.738853] Epoch: [39]  [760/781]  eta: 0:00:04  lr: 0.000176  training_loss: 1.7644 (1.7668)  mae_loss: 0.2778 (0.2748)  classification_loss: 1.4655 (1.4585)  loss_mask: 0.0067 (0.0335)  time: 0.1971  data: 0.0002  max mem: 5511
[08:03:57.698999] Epoch: [39]  [780/781]  eta: 0:00:00  lr: 0.000176  training_loss: 1.7621 (1.7667)  mae_loss: 0.2766 (0.2748)  classification_loss: 1.4776 (1.4589)  loss_mask: 0.0065 (0.0329)  time: 0.1979  data: 0.0002  max mem: 5511
[08:03:57.859093] Epoch: [39] Total time: 0:02:35 (0.1986 s / it)
[08:03:57.859572] Averaged stats: lr: 0.000176  training_loss: 1.7621 (1.7667)  mae_loss: 0.2766 (0.2748)  classification_loss: 1.4776 (1.4589)  loss_mask: 0.0065 (0.0329)
[08:03:58.463648] Test:  [  0/157]  eta: 0:01:33  testing_loss: 0.7299 (0.7299)  acc1: 79.6875 (79.6875)  acc5: 95.3125 (95.3125)  time: 0.5983  data: 0.5669  max mem: 5511
[08:03:58.760278] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.7512 (0.7820)  acc1: 75.0000 (73.4375)  acc5: 98.4375 (98.1534)  time: 0.0811  data: 0.0525  max mem: 5511
[08:03:59.045350] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.7603 (0.7491)  acc1: 75.0000 (75.2232)  acc5: 98.4375 (98.3631)  time: 0.0289  data: 0.0007  max mem: 5511
[08:03:59.329062] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.7603 (0.7627)  acc1: 75.0000 (74.7480)  acc5: 98.4375 (97.9335)  time: 0.0283  data: 0.0002  max mem: 5511
[08:03:59.613012] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.7653 (0.7685)  acc1: 75.0000 (74.6951)  acc5: 98.4375 (97.9421)  time: 0.0282  data: 0.0002  max mem: 5511
[08:03:59.894978] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.7102 (0.7558)  acc1: 76.5625 (75.3370)  acc5: 98.4375 (98.0086)  time: 0.0282  data: 0.0002  max mem: 5511
[08:04:00.176625] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6982 (0.7522)  acc1: 78.1250 (75.4098)  acc5: 98.4375 (98.1557)  time: 0.0281  data: 0.0002  max mem: 5511
[08:04:00.458855] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6982 (0.7467)  acc1: 76.5625 (75.5942)  acc5: 98.4375 (98.1294)  time: 0.0281  data: 0.0002  max mem: 5511
[08:04:00.741662] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7443 (0.7570)  acc1: 73.4375 (75.1929)  acc5: 96.8750 (98.0517)  time: 0.0281  data: 0.0002  max mem: 5511
[08:04:01.024381] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7597 (0.7553)  acc1: 75.0000 (75.4636)  acc5: 98.4375 (98.0941)  time: 0.0282  data: 0.0002  max mem: 5511
[08:04:01.312814] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.7550 (0.7597)  acc1: 76.5625 (75.3868)  acc5: 98.4375 (98.0972)  time: 0.0284  data: 0.0002  max mem: 5511
[08:04:01.608131] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.7645 (0.7617)  acc1: 75.0000 (75.4223)  acc5: 98.4375 (98.1419)  time: 0.0290  data: 0.0002  max mem: 5511
[08:04:01.891237] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.7261 (0.7572)  acc1: 76.5625 (75.5682)  acc5: 98.4375 (98.2051)  time: 0.0288  data: 0.0002  max mem: 5511
[08:04:02.174462] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.7019 (0.7565)  acc1: 75.0000 (75.4652)  acc5: 100.0000 (98.2705)  time: 0.0282  data: 0.0002  max mem: 5511
[08:04:02.456175] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7324 (0.7568)  acc1: 73.4375 (75.3989)  acc5: 98.4375 (98.2934)  time: 0.0281  data: 0.0002  max mem: 5511
[08:04:02.740461] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7649 (0.7542)  acc1: 75.0000 (75.4656)  acc5: 98.4375 (98.3133)  time: 0.0282  data: 0.0002  max mem: 5511
[08:04:02.895424] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7463 (0.7542)  acc1: 75.0000 (75.3700)  acc5: 98.4375 (98.3600)  time: 0.0274  data: 0.0002  max mem: 5511
[08:04:03.050907] Test: Total time: 0:00:05 (0.0330 s / it)
[08:04:03.051939] * Acc@1 75.370 Acc@5 98.360 loss 0.754
[08:04:03.052272] Accuracy of the network on the 10000 test images: 75.4%
[08:04:03.052569] Max accuracy: 75.80%
[08:04:03.224613] log_dir: ./output_dir
[08:04:04.203768] Epoch: [40]  [  0/781]  eta: 0:12:43  lr: 0.000176  training_loss: 1.6484 (1.6484)  mae_loss: 0.2290 (0.2290)  classification_loss: 1.4176 (1.4176)  loss_mask: 0.0018 (0.0018)  time: 0.9772  data: 0.7394  max mem: 5511
[08:04:08.177263] Epoch: [40]  [ 20/781]  eta: 0:02:59  lr: 0.000175  training_loss: 1.7291 (1.6953)  mae_loss: 0.3007 (0.2898)  classification_loss: 1.4180 (1.4009)  loss_mask: 0.0034 (0.0047)  time: 0.1986  data: 0.0002  max mem: 5511
[08:04:12.110735] Epoch: [40]  [ 40/781]  eta: 0:02:40  lr: 0.000175  training_loss: 1.7393 (1.7267)  mae_loss: 0.2660 (0.2830)  classification_loss: 1.4615 (1.4403)  loss_mask: 0.0013 (0.0034)  time: 0.1966  data: 0.0002  max mem: 5511
[08:04:16.064240] Epoch: [40]  [ 60/781]  eta: 0:02:31  lr: 0.000175  training_loss: 1.7377 (1.7329)  mae_loss: 0.2692 (0.2782)  classification_loss: 1.4589 (1.4516)  loss_mask: 0.0020 (0.0030)  time: 0.1976  data: 0.0002  max mem: 5511
[08:04:20.002964] Epoch: [40]  [ 80/781]  eta: 0:02:25  lr: 0.000175  training_loss: 1.6374 (1.7207)  mae_loss: 0.2482 (0.2754)  classification_loss: 1.3852 (1.4422)  loss_mask: 0.0014 (0.0030)  time: 0.1969  data: 0.0002  max mem: 5511
[08:04:23.948809] Epoch: [40]  [100/781]  eta: 0:02:19  lr: 0.000175  training_loss: 1.7671 (1.7312)  mae_loss: 0.2715 (0.2759)  classification_loss: 1.5041 (1.4526)  loss_mask: 0.0010 (0.0027)  time: 0.1972  data: 0.0002  max mem: 5511
[08:04:27.872957] Epoch: [40]  [120/781]  eta: 0:02:14  lr: 0.000175  training_loss: 1.6783 (1.7248)  mae_loss: 0.2671 (0.2750)  classification_loss: 1.4045 (1.4474)  loss_mask: 0.0009 (0.0024)  time: 0.1961  data: 0.0003  max mem: 5511
[08:04:31.788341] Epoch: [40]  [140/781]  eta: 0:02:09  lr: 0.000175  training_loss: 1.7092 (1.7248)  mae_loss: 0.2694 (0.2762)  classification_loss: 1.4185 (1.4456)  loss_mask: 0.0009 (0.0030)  time: 0.1957  data: 0.0002  max mem: 5511
[08:04:35.764130] Epoch: [40]  [160/781]  eta: 0:02:05  lr: 0.000175  training_loss: 1.6935 (1.7205)  mae_loss: 0.2680 (0.2759)  classification_loss: 1.3870 (1.4418)  loss_mask: 0.0006 (0.0027)  time: 0.1987  data: 0.0002  max mem: 5511
[08:04:39.692573] Epoch: [40]  [180/781]  eta: 0:02:01  lr: 0.000175  training_loss: 1.7115 (1.7193)  mae_loss: 0.2663 (0.2755)  classification_loss: 1.4349 (1.4414)  loss_mask: 0.0006 (0.0025)  time: 0.1963  data: 0.0002  max mem: 5511
[08:04:43.638004] Epoch: [40]  [200/781]  eta: 0:01:56  lr: 0.000175  training_loss: 1.7024 (1.7212)  mae_loss: 0.2656 (0.2750)  classification_loss: 1.4225 (1.4439)  loss_mask: 0.0006 (0.0023)  time: 0.1972  data: 0.0002  max mem: 5511
[08:04:47.595522] Epoch: [40]  [220/781]  eta: 0:01:52  lr: 0.000174  training_loss: 1.7004 (1.7186)  mae_loss: 0.2865 (0.2757)  classification_loss: 1.4061 (1.4407)  loss_mask: 0.0005 (0.0022)  time: 0.1978  data: 0.0004  max mem: 5511
[08:04:51.530428] Epoch: [40]  [240/781]  eta: 0:01:48  lr: 0.000174  training_loss: 1.7030 (1.7196)  mae_loss: 0.2683 (0.2752)  classification_loss: 1.4493 (1.4424)  loss_mask: 0.0005 (0.0020)  time: 0.1967  data: 0.0002  max mem: 5511
[08:04:55.482121] Epoch: [40]  [260/781]  eta: 0:01:44  lr: 0.000174  training_loss: 1.7869 (1.7242)  mae_loss: 0.2813 (0.2761)  classification_loss: 1.4614 (1.4457)  loss_mask: 0.0010 (0.0024)  time: 0.1975  data: 0.0002  max mem: 5511
[08:04:59.399427] Epoch: [40]  [280/781]  eta: 0:01:40  lr: 0.000174  training_loss: 1.7728 (1.7291)  mae_loss: 0.2822 (0.2771)  classification_loss: 1.4237 (1.4449)  loss_mask: 0.0400 (0.0070)  time: 0.1958  data: 0.0002  max mem: 5511
[08:05:03.352314] Epoch: [40]  [300/781]  eta: 0:01:36  lr: 0.000174  training_loss: 1.8354 (1.7378)  mae_loss: 0.2650 (0.2765)  classification_loss: 1.4819 (1.4482)  loss_mask: 0.0739 (0.0130)  time: 0.1976  data: 0.0002  max mem: 5511
[08:05:07.280471] Epoch: [40]  [320/781]  eta: 0:01:31  lr: 0.000174  training_loss: 1.7803 (1.7417)  mae_loss: 0.2947 (0.2775)  classification_loss: 1.4421 (1.4483)  loss_mask: 0.0464 (0.0159)  time: 0.1963  data: 0.0002  max mem: 5511
[08:05:11.227894] Epoch: [40]  [340/781]  eta: 0:01:27  lr: 0.000174  training_loss: 1.7062 (1.7413)  mae_loss: 0.2927 (0.2782)  classification_loss: 1.4037 (1.4471)  loss_mask: 0.0123 (0.0160)  time: 0.1973  data: 0.0002  max mem: 5511
[08:05:15.166016] Epoch: [40]  [360/781]  eta: 0:01:23  lr: 0.000174  training_loss: 1.7298 (1.7407)  mae_loss: 0.2700 (0.2781)  classification_loss: 1.4200 (1.4471)  loss_mask: 0.0051 (0.0155)  time: 0.1968  data: 0.0003  max mem: 5511
[08:05:19.096031] Epoch: [40]  [380/781]  eta: 0:01:19  lr: 0.000174  training_loss: 1.7235 (1.7400)  mae_loss: 0.2711 (0.2781)  classification_loss: 1.4109 (1.4471)  loss_mask: 0.0033 (0.0149)  time: 0.1964  data: 0.0002  max mem: 5511
[08:05:23.047462] Epoch: [40]  [400/781]  eta: 0:01:15  lr: 0.000174  training_loss: 1.6928 (1.7389)  mae_loss: 0.2554 (0.2773)  classification_loss: 1.4455 (1.4471)  loss_mask: 0.0028 (0.0145)  time: 0.1975  data: 0.0002  max mem: 5511
[08:05:27.010274] Epoch: [40]  [420/781]  eta: 0:01:11  lr: 0.000173  training_loss: 1.6672 (1.7377)  mae_loss: 0.2692 (0.2773)  classification_loss: 1.3916 (1.4456)  loss_mask: 0.0029 (0.0147)  time: 0.1981  data: 0.0003  max mem: 5511
[08:05:30.936702] Epoch: [40]  [440/781]  eta: 0:01:07  lr: 0.000173  training_loss: 1.7718 (1.7400)  mae_loss: 0.2675 (0.2771)  classification_loss: 1.4302 (1.4453)  loss_mask: 0.0592 (0.0176)  time: 0.1962  data: 0.0002  max mem: 5511
[08:05:34.912634] Epoch: [40]  [460/781]  eta: 0:01:03  lr: 0.000173  training_loss: 1.7571 (1.7408)  mae_loss: 0.2665 (0.2769)  classification_loss: 1.4486 (1.4446)  loss_mask: 0.0526 (0.0193)  time: 0.1987  data: 0.0002  max mem: 5511
[08:05:38.840049] Epoch: [40]  [480/781]  eta: 0:00:59  lr: 0.000173  training_loss: 1.7341 (1.7398)  mae_loss: 0.2596 (0.2765)  classification_loss: 1.4373 (1.4441)  loss_mask: 0.0133 (0.0192)  time: 0.1963  data: 0.0003  max mem: 5511
[08:05:42.793379] Epoch: [40]  [500/781]  eta: 0:00:55  lr: 0.000173  training_loss: 1.7299 (1.7398)  mae_loss: 0.2824 (0.2767)  classification_loss: 1.4547 (1.4442)  loss_mask: 0.0057 (0.0188)  time: 0.1976  data: 0.0002  max mem: 5511

[08:05:46.717299] Epoch: [40]  [520/781]  eta: 0:00:51  lr: 0.000173  training_loss: 1.7628 (1.7410)  mae_loss: 0.2892 (0.2771)  classification_loss: 1.4447 (1.4446)  loss_mask: 0.0212 (0.0193)  time: 0.1961  data: 0.0005  max mem: 5511
[08:05:50.646926] Epoch: [40]  [540/781]  eta: 0:00:47  lr: 0.000173  training_loss: 1.8072 (1.7459)  mae_loss: 0.2753 (0.2771)  classification_loss: 1.4796 (1.4460)  loss_mask: 0.0906 (0.0228)  time: 0.1964  data: 0.0002  max mem: 5511
[08:05:54.581072] Epoch: [40]  [560/781]  eta: 0:00:43  lr: 0.000173  training_loss: 1.7337 (1.7458)  mae_loss: 0.2871 (0.2771)  classification_loss: 1.4122 (1.4447)  loss_mask: 0.0470 (0.0239)  time: 0.1966  data: 0.0002  max mem: 5511
[08:05:58.529774] Epoch: [40]  [580/781]  eta: 0:00:39  lr: 0.000173  training_loss: 1.7416 (1.7453)  mae_loss: 0.2648 (0.2767)  classification_loss: 1.4559 (1.4445)  loss_mask: 0.0181 (0.0241)  time: 0.1973  data: 0.0002  max mem: 5511
[08:06:02.475028] Epoch: [40]  [600/781]  eta: 0:00:35  lr: 0.000173  training_loss: 1.7464 (1.7453)  mae_loss: 0.2619 (0.2767)  classification_loss: 1.4386 (1.4447)  loss_mask: 0.0117 (0.0239)  time: 0.1972  data: 0.0005  max mem: 5511
[08:06:06.424838] Epoch: [40]  [620/781]  eta: 0:00:31  lr: 0.000173  training_loss: 1.7472 (1.7455)  mae_loss: 0.2784 (0.2767)  classification_loss: 1.4520 (1.4452)  loss_mask: 0.0074 (0.0236)  time: 0.1974  data: 0.0002  max mem: 5511
[08:06:10.368109] Epoch: [40]  [640/781]  eta: 0:00:27  lr: 0.000172  training_loss: 1.7871 (1.7459)  mae_loss: 0.2738 (0.2768)  classification_loss: 1.4631 (1.4458)  loss_mask: 0.0071 (0.0233)  time: 0.1971  data: 0.0002  max mem: 5511
[08:06:14.307858] Epoch: [40]  [660/781]  eta: 0:00:23  lr: 0.000172  training_loss: 1.7565 (1.7465)  mae_loss: 0.2750 (0.2767)  classification_loss: 1.4538 (1.4468)  loss_mask: 0.0082 (0.0230)  time: 0.1969  data: 0.0002  max mem: 5511
[08:06:18.267868] Epoch: [40]  [680/781]  eta: 0:00:20  lr: 0.000172  training_loss: 1.8038 (1.7480)  mae_loss: 0.2675 (0.2766)  classification_loss: 1.5186 (1.4488)  loss_mask: 0.0063 (0.0225)  time: 0.1979  data: 0.0002  max mem: 5511
[08:06:22.256694] Epoch: [40]  [700/781]  eta: 0:00:16  lr: 0.000172  training_loss: 1.7537 (1.7491)  mae_loss: 0.2647 (0.2765)  classification_loss: 1.4565 (1.4499)  loss_mask: 0.0052 (0.0226)  time: 0.1993  data: 0.0003  max mem: 5511
[08:06:26.219073] Epoch: [40]  [720/781]  eta: 0:00:12  lr: 0.000172  training_loss: 1.8401 (1.7520)  mae_loss: 0.2707 (0.2766)  classification_loss: 1.4366 (1.4499)  loss_mask: 0.0710 (0.0255)  time: 0.1980  data: 0.0002  max mem: 5511
[08:06:30.161344] Epoch: [40]  [740/781]  eta: 0:00:08  lr: 0.000172  training_loss: 1.7552 (1.7527)  mae_loss: 0.2617 (0.2763)  classification_loss: 1.4156 (1.4491)  loss_mask: 0.0776 (0.0273)  time: 0.1970  data: 0.0002  max mem: 5511
[08:06:34.083956] Epoch: [40]  [760/781]  eta: 0:00:04  lr: 0.000172  training_loss: 1.7512 (1.7532)  mae_loss: 0.2816 (0.2763)  classification_loss: 1.4253 (1.4493)  loss_mask: 0.0351 (0.0276)  time: 0.1961  data: 0.0002  max mem: 5511
[08:06:38.019490] Epoch: [40]  [780/781]  eta: 0:00:00  lr: 0.000172  training_loss: 1.7192 (1.7519)  mae_loss: 0.2733 (0.2760)  classification_loss: 1.4244 (1.4487)  loss_mask: 0.0110 (0.0272)  time: 0.1967  data: 0.0002  max mem: 5511
[08:06:38.188455] Epoch: [40] Total time: 0:02:34 (0.1984 s / it)
[08:06:38.188969] Averaged stats: lr: 0.000172  training_loss: 1.7192 (1.7519)  mae_loss: 0.2733 (0.2760)  classification_loss: 1.4244 (1.4487)  loss_mask: 0.0110 (0.0272)
[08:06:39.332146] Test:  [  0/157]  eta: 0:01:51  testing_loss: 0.6857 (0.6857)  acc1: 79.6875 (79.6875)  acc5: 96.8750 (96.8750)  time: 0.7101  data: 0.6641  max mem: 5511
[08:06:39.622286] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.7285 (0.7407)  acc1: 75.0000 (75.4261)  acc5: 98.4375 (98.2955)  time: 0.0907  data: 0.0608  max mem: 5511
[08:06:39.910406] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.7089 (0.7174)  acc1: 75.0000 (75.8929)  acc5: 98.4375 (98.3631)  time: 0.0287  data: 0.0003  max mem: 5511
[08:06:40.205107] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.7245 (0.7376)  acc1: 76.5625 (75.6048)  acc5: 98.4375 (98.0847)  time: 0.0290  data: 0.0003  max mem: 5511
[08:06:40.489820] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.7600 (0.7476)  acc1: 73.4375 (74.8476)  acc5: 98.4375 (98.1326)  time: 0.0288  data: 0.0004  max mem: 5511
[08:06:40.778894] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.7299 (0.7427)  acc1: 73.4375 (75.4902)  acc5: 98.4375 (98.1005)  time: 0.0286  data: 0.0002  max mem: 5511
[08:06:41.064335] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.7548 (0.7418)  acc1: 76.5625 (75.4355)  acc5: 98.4375 (98.1557)  time: 0.0286  data: 0.0002  max mem: 5511
[08:06:41.353686] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7288 (0.7354)  acc1: 75.0000 (75.7702)  acc5: 98.4375 (98.1954)  time: 0.0286  data: 0.0002  max mem: 5511
[08:06:41.643175] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7409 (0.7435)  acc1: 75.0000 (75.5401)  acc5: 98.4375 (98.1867)  time: 0.0288  data: 0.0002  max mem: 5511
[08:06:41.934571] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7557 (0.7420)  acc1: 76.5625 (75.7898)  acc5: 98.4375 (98.2143)  time: 0.0289  data: 0.0002  max mem: 5511
[08:06:42.236669] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.7261 (0.7459)  acc1: 76.5625 (75.5105)  acc5: 98.4375 (98.2828)  time: 0.0295  data: 0.0008  max mem: 5511
[08:06:42.524520] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.7713 (0.7477)  acc1: 73.4375 (75.5349)  acc5: 98.4375 (98.2686)  time: 0.0293  data: 0.0008  max mem: 5511
[08:06:42.818325] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.7102 (0.7435)  acc1: 76.5625 (75.6844)  acc5: 98.4375 (98.2955)  time: 0.0289  data: 0.0005  max mem: 5511
[08:06:43.107365] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.7155 (0.7426)  acc1: 78.1250 (75.9423)  acc5: 98.4375 (98.3063)  time: 0.0290  data: 0.0005  max mem: 5511
[08:06:43.393423] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7642 (0.7427)  acc1: 76.5625 (75.9198)  acc5: 98.4375 (98.2934)  time: 0.0286  data: 0.0002  max mem: 5511
[08:06:43.673980] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7642 (0.7415)  acc1: 75.0000 (75.9416)  acc5: 98.4375 (98.2823)  time: 0.0282  data: 0.0001  max mem: 5511
[08:06:43.825821] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7515 (0.7425)  acc1: 75.0000 (75.9200)  acc5: 98.4375 (98.2700)  time: 0.0271  data: 0.0001  max mem: 5511
[08:06:43.995402] Test: Total time: 0:00:05 (0.0342 s / it)
[08:06:43.996186] * Acc@1 75.920 Acc@5 98.270 loss 0.742
[08:06:43.996528] Accuracy of the network on the 10000 test images: 75.9%
[08:06:43.996729] Max accuracy: 75.92%
[08:06:44.132600] log_dir: ./output_dir
[08:06:44.940010] Epoch: [41]  [  0/781]  eta: 0:10:29  lr: 0.000172  training_loss: 1.6263 (1.6263)  mae_loss: 0.2305 (0.2305)  classification_loss: 1.3900 (1.3900)  loss_mask: 0.0058 (0.0058)  time: 0.8054  data: 0.5707  max mem: 5511
[08:06:48.894000] Epoch: [41]  [ 20/781]  eta: 0:02:52  lr: 0.000172  training_loss: 1.6697 (1.7030)  mae_loss: 0.2721 (0.2694)  classification_loss: 1.3892 (1.4213)  loss_mask: 0.0065 (0.0123)  time: 0.1976  data: 0.0001  max mem: 5511
[08:06:52.829324] Epoch: [41]  [ 40/781]  eta: 0:02:37  lr: 0.000172  training_loss: 1.6775 (1.7104)  mae_loss: 0.2674 (0.2699)  classification_loss: 1.4105 (1.4317)  loss_mask: 0.0040 (0.0088)  time: 0.1966  data: 0.0002  max mem: 5511
[08:06:56.782631] Epoch: [41]  [ 60/781]  eta: 0:02:29  lr: 0.000171  training_loss: 1.7059 (1.7155)  mae_loss: 0.2664 (0.2701)  classification_loss: 1.4334 (1.4381)  loss_mask: 0.0025 (0.0073)  time: 0.1976  data: 0.0002  max mem: 5511
[08:07:00.718847] Epoch: [41]  [ 80/781]  eta: 0:02:23  lr: 0.000171  training_loss: 1.7075 (1.7219)  mae_loss: 0.2766 (0.2748)  classification_loss: 1.4242 (1.4375)  loss_mask: 0.0091 (0.0097)  time: 0.1967  data: 0.0002  max mem: 5511
[08:07:04.677128] Epoch: [41]  [100/781]  eta: 0:02:18  lr: 0.000171  training_loss: 1.7815 (1.7375)  mae_loss: 0.2745 (0.2779)  classification_loss: 1.4541 (1.4446)  loss_mask: 0.0208 (0.0149)  time: 0.1978  data: 0.0002  max mem: 5511
[08:07:08.625272] Epoch: [41]  [120/781]  eta: 0:02:13  lr: 0.000171  training_loss: 1.8568 (1.7580)  mae_loss: 0.2678 (0.2782)  classification_loss: 1.4214 (1.4454)  loss_mask: 0.1282 (0.0344)  time: 0.1973  data: 0.0002  max mem: 5511
[08:07:12.585850] Epoch: [41]  [140/781]  eta: 0:02:09  lr: 0.000171  training_loss: 1.7865 (1.7641)  mae_loss: 0.2882 (0.2790)  classification_loss: 1.4565 (1.4467)  loss_mask: 0.0438 (0.0384)  time: 0.1979  data: 0.0003  max mem: 5511
[08:07:16.536013] Epoch: [41]  [160/781]  eta: 0:02:04  lr: 0.000171  training_loss: 1.7871 (1.7633)  mae_loss: 0.2702 (0.2775)  classification_loss: 1.4896 (1.4496)  loss_mask: 0.0206 (0.0363)  time: 0.1974  data: 0.0003  max mem: 5511
[08:07:20.534306] Epoch: [41]  [180/781]  eta: 0:02:00  lr: 0.000171  training_loss: 1.6782 (1.7532)  mae_loss: 0.2618 (0.2765)  classification_loss: 1.3944 (1.4432)  loss_mask: 0.0093 (0.0336)  time: 0.1998  data: 0.0003  max mem: 5511
[08:07:24.504120] Epoch: [41]  [200/781]  eta: 0:01:56  lr: 0.000171  training_loss: 1.7384 (1.7513)  mae_loss: 0.2689 (0.2763)  classification_loss: 1.4437 (1.4431)  loss_mask: 0.0085 (0.0320)  time: 0.1984  data: 0.0002  max mem: 5511
[08:07:28.468318] Epoch: [41]  [220/781]  eta: 0:01:52  lr: 0.000171  training_loss: 1.7686 (1.7505)  mae_loss: 0.2811 (0.2768)  classification_loss: 1.4668 (1.4432)  loss_mask: 0.0090 (0.0305)  time: 0.1981  data: 0.0002  max mem: 5511
[08:07:32.398783] Epoch: [41]  [240/781]  eta: 0:01:48  lr: 0.000171  training_loss: 1.7053 (1.7483)  mae_loss: 0.2548 (0.2756)  classification_loss: 1.3754 (1.4426)  loss_mask: 0.0119 (0.0301)  time: 0.1964  data: 0.0002  max mem: 5511
[08:07:36.357118] Epoch: [41]  [260/781]  eta: 0:01:44  lr: 0.000170  training_loss: 1.7541 (1.7466)  mae_loss: 0.2716 (0.2751)  classification_loss: 1.4531 (1.4411)  loss_mask: 0.0181 (0.0304)  time: 0.1978  data: 0.0002  max mem: 5511
[08:07:40.308203] Epoch: [41]  [280/781]  eta: 0:01:40  lr: 0.000170  training_loss: 1.6870 (1.7434)  mae_loss: 0.2708 (0.2752)  classification_loss: 1.4006 (1.4395)  loss_mask: 0.0065 (0.0288)  time: 0.1975  data: 0.0002  max mem: 5511
[08:07:44.242950] Epoch: [41]  [300/781]  eta: 0:01:36  lr: 0.000170  training_loss: 1.6891 (1.7425)  mae_loss: 0.2525 (0.2744)  classification_loss: 1.3982 (1.4379)  loss_mask: 0.0259 (0.0302)  time: 0.1966  data: 0.0002  max mem: 5511
[08:07:48.208442] Epoch: [41]  [320/781]  eta: 0:01:31  lr: 0.000170  training_loss: 1.7577 (1.7447)  mae_loss: 0.2638 (0.2739)  classification_loss: 1.4147 (1.4380)  loss_mask: 0.0551 (0.0328)  time: 0.1982  data: 0.0003  max mem: 5511
[08:07:52.165192] Epoch: [41]  [340/781]  eta: 0:01:27  lr: 0.000170  training_loss: 1.8139 (1.7494)  mae_loss: 0.2731 (0.2742)  classification_loss: 1.4271 (1.4390)  loss_mask: 0.0663 (0.0362)  time: 0.1977  data: 0.0005  max mem: 5511
[08:07:56.128340] Epoch: [41]  [360/781]  eta: 0:01:23  lr: 0.000170  training_loss: 1.7317 (1.7506)  mae_loss: 0.2667 (0.2741)  classification_loss: 1.4586 (1.4401)  loss_mask: 0.0380 (0.0364)  time: 0.1980  data: 0.0002  max mem: 5511
[08:08:00.072990] Epoch: [41]  [380/781]  eta: 0:01:19  lr: 0.000170  training_loss: 1.7361 (1.7501)  mae_loss: 0.2686 (0.2742)  classification_loss: 1.4350 (1.4406)  loss_mask: 0.0157 (0.0354)  time: 0.1971  data: 0.0002  max mem: 5511
[08:08:04.038793] Epoch: [41]  [400/781]  eta: 0:01:15  lr: 0.000170  training_loss: 1.7253 (1.7487)  mae_loss: 0.2579 (0.2738)  classification_loss: 1.4316 (1.4410)  loss_mask: 0.0046 (0.0339)  time: 0.1982  data: 0.0003  max mem: 5511
[08:08:07.989118] Epoch: [41]  [420/781]  eta: 0:01:11  lr: 0.000170  training_loss: 1.6858 (1.7466)  mae_loss: 0.2743 (0.2739)  classification_loss: 1.3934 (1.4403)  loss_mask: 0.0024 (0.0324)  time: 0.1974  data: 0.0003  max mem: 5511
[08:08:11.938548] Epoch: [41]  [440/781]  eta: 0:01:07  lr: 0.000170  training_loss: 1.6752 (1.7443)  mae_loss: 0.2662 (0.2738)  classification_loss: 1.4102 (1.4394)  loss_mask: 0.0024 (0.0311)  time: 0.1974  data: 0.0002  max mem: 5511
[08:08:15.897047] Epoch: [41]  [460/781]  eta: 0:01:03  lr: 0.000169  training_loss: 1.6861 (1.7418)  mae_loss: 0.2756 (0.2737)  classification_loss: 1.4125 (1.4382)  loss_mask: 0.0014 (0.0299)  time: 0.1978  data: 0.0002  max mem: 5511
[08:08:19.881892] Epoch: [41]  [480/781]  eta: 0:00:59  lr: 0.000169  training_loss: 1.7337 (1.7405)  mae_loss: 0.2520 (0.2732)  classification_loss: 1.4570 (1.4386)  loss_mask: 0.0012 (0.0287)  time: 0.1992  data: 0.0002  max mem: 5511
[08:08:23.839600] Epoch: [41]  [500/781]  eta: 0:00:55  lr: 0.000169  training_loss: 1.6855 (1.7394)  mae_loss: 0.2714 (0.2732)  classification_loss: 1.4041 (1.4385)  loss_mask: 0.0013 (0.0277)  time: 0.1978  data: 0.0002  max mem: 5511
[08:08:27.791169] Epoch: [41]  [520/781]  eta: 0:00:51  lr: 0.000169  training_loss: 1.6783 (1.7383)  mae_loss: 0.2805 (0.2735)  classification_loss: 1.3946 (1.4380)  loss_mask: 0.0022 (0.0268)  time: 0.1975  data: 0.0002  max mem: 5511
[08:08:31.743347] Epoch: [41]  [540/781]  eta: 0:00:47  lr: 0.000169  training_loss: 1.7337 (1.7397)  mae_loss: 0.2670 (0.2736)  classification_loss: 1.4477 (1.4388)  loss_mask: 0.0169 (0.0273)  time: 0.1975  data: 0.0002  max mem: 5511
[08:08:35.701626] Epoch: [41]  [560/781]  eta: 0:00:43  lr: 0.000169  training_loss: 1.7401 (1.7403)  mae_loss: 0.2875 (0.2742)  classification_loss: 1.4440 (1.4392)  loss_mask: 0.0075 (0.0268)  time: 0.1978  data: 0.0002  max mem: 5511
[08:08:39.683846] Epoch: [41]  [580/781]  eta: 0:00:39  lr: 0.000169  training_loss: 1.7616 (1.7406)  mae_loss: 0.2781 (0.2745)  classification_loss: 1.4423 (1.4391)  loss_mask: 0.0057 (0.0270)  time: 0.1990  data: 0.0002  max mem: 5511
[08:08:43.629263] Epoch: [41]  [600/781]  eta: 0:00:35  lr: 0.000169  training_loss: 1.8110 (1.7437)  mae_loss: 0.2628 (0.2745)  classification_loss: 1.4415 (1.4389)  loss_mask: 0.0499 (0.0304)  time: 0.1972  data: 0.0002  max mem: 5511
[08:08:47.587748] Epoch: [41]  [620/781]  eta: 0:00:31  lr: 0.000169  training_loss: 1.7476 (1.7435)  mae_loss: 0.2709 (0.2743)  classification_loss: 1.4329 (1.4383)  loss_mask: 0.0402 (0.0309)  time: 0.1978  data: 0.0002  max mem: 5511
[08:08:51.532087] Epoch: [41]  [640/781]  eta: 0:00:28  lr: 0.000169  training_loss: 1.6996 (1.7427)  mae_loss: 0.2618 (0.2744)  classification_loss: 1.3991 (1.4374)  loss_mask: 0.0268 (0.0309)  time: 0.1971  data: 0.0003  max mem: 5511
[08:08:55.479683] Epoch: [41]  [660/781]  eta: 0:00:24  lr: 0.000168  training_loss: 1.7529 (1.7429)  mae_loss: 0.2671 (0.2746)  classification_loss: 1.4496 (1.4377)  loss_mask: 0.0138 (0.0306)  time: 0.1972  data: 0.0002  max mem: 5511
[08:08:59.427870] Epoch: [41]  [680/781]  eta: 0:00:20  lr: 0.000168  training_loss: 1.7370 (1.7434)  mae_loss: 0.2631 (0.2743)  classification_loss: 1.4487 (1.4386)  loss_mask: 0.0111 (0.0304)  time: 0.1973  data: 0.0002  max mem: 5511
[08:09:03.395494] Epoch: [41]  [700/781]  eta: 0:00:16  lr: 0.000168  training_loss: 1.7840 (1.7450)  mae_loss: 0.2767 (0.2745)  classification_loss: 1.4235 (1.4390)  loss_mask: 0.0228 (0.0315)  time: 0.1983  data: 0.0002  max mem: 5511
[08:09:07.351887] Epoch: [41]  [720/781]  eta: 0:00:12  lr: 0.000168  training_loss: 1.9930 (1.7526)  mae_loss: 0.2957 (0.2749)  classification_loss: 1.6101 (1.4440)  loss_mask: 0.1058 (0.0337)  time: 0.1977  data: 0.0002  max mem: 5511
[08:09:11.354873] Epoch: [41]  [740/781]  eta: 0:00:08  lr: 0.000168  training_loss: 1.8570 (1.7555)  mae_loss: 0.2923 (0.2751)  classification_loss: 1.5087 (1.4460)  loss_mask: 0.0536 (0.0344)  time: 0.2001  data: 0.0003  max mem: 5511
[08:09:15.309952] Epoch: [41]  [760/781]  eta: 0:00:04  lr: 0.000168  training_loss: 1.8494 (1.7579)  mae_loss: 0.2652 (0.2751)  classification_loss: 1.5546 (1.4484)  loss_mask: 0.0236 (0.0343)  time: 0.1976  data: 0.0003  max mem: 5511
[08:09:19.236803] Epoch: [41]  [780/781]  eta: 0:00:00  lr: 0.000168  training_loss: 1.8525 (1.7595)  mae_loss: 0.2853 (0.2755)  classification_loss: 1.5091 (1.4498)  loss_mask: 0.0202 (0.0342)  time: 0.1962  data: 0.0002  max mem: 5511
[08:09:19.406948] Epoch: [41] Total time: 0:02:35 (0.1988 s / it)
[08:09:19.407440] Averaged stats: lr: 0.000168  training_loss: 1.8525 (1.7595)  mae_loss: 0.2853 (0.2755)  classification_loss: 1.5091 (1.4498)  loss_mask: 0.0202 (0.0342)
[08:09:20.099739] Test:  [  0/157]  eta: 0:01:48  testing_loss: 0.7513 (0.7513)  acc1: 79.6875 (79.6875)  acc5: 96.8750 (96.8750)  time: 0.6882  data: 0.6585  max mem: 5511
[08:09:20.404489] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.7513 (0.7821)  acc1: 76.5625 (74.7159)  acc5: 98.4375 (98.0114)  time: 0.0901  data: 0.0616  max mem: 5511
[08:09:20.693795] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.7446 (0.7625)  acc1: 75.0000 (75.2976)  acc5: 98.4375 (98.3631)  time: 0.0296  data: 0.0010  max mem: 5511
[08:09:20.979704] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.7446 (0.7678)  acc1: 73.4375 (75.0504)  acc5: 98.4375 (98.3871)  time: 0.0286  data: 0.0002  max mem: 5511
[08:09:21.261724] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.7692 (0.7720)  acc1: 73.4375 (75.0762)  acc5: 98.4375 (98.4756)  time: 0.0283  data: 0.0002  max mem: 5511
[08:09:21.545665] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.7713 (0.7638)  acc1: 76.5625 (75.6434)  acc5: 98.4375 (98.5294)  time: 0.0282  data: 0.0002  max mem: 5511
[08:09:21.830260] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.7433 (0.7622)  acc1: 76.5625 (75.2305)  acc5: 100.0000 (98.6168)  time: 0.0283  data: 0.0002  max mem: 5511
[08:09:22.123382] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7223 (0.7530)  acc1: 75.0000 (75.6162)  acc5: 98.4375 (98.6136)  time: 0.0288  data: 0.0002  max mem: 5511
[08:09:22.406003] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7431 (0.7617)  acc1: 75.0000 (75.4823)  acc5: 98.4375 (98.4375)  time: 0.0287  data: 0.0002  max mem: 5511
[08:09:22.688322] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7800 (0.7646)  acc1: 73.4375 (75.3949)  acc5: 98.4375 (98.4203)  time: 0.0281  data: 0.0002  max mem: 5511
[08:09:22.971420] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.7887 (0.7691)  acc1: 73.4375 (75.1856)  acc5: 98.4375 (98.4066)  time: 0.0282  data: 0.0002  max mem: 5511
[08:09:23.254994] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.8056 (0.7707)  acc1: 73.4375 (75.1267)  acc5: 98.4375 (98.3249)  time: 0.0282  data: 0.0002  max mem: 5511
[08:09:23.540804] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.7652 (0.7663)  acc1: 75.0000 (75.1679)  acc5: 98.4375 (98.3988)  time: 0.0283  data: 0.0003  max mem: 5511
[08:09:23.824540] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.7631 (0.7659)  acc1: 75.0000 (75.1789)  acc5: 98.4375 (98.4375)  time: 0.0283  data: 0.0003  max mem: 5511
[08:09:24.107675] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7903 (0.7674)  acc1: 75.0000 (75.1108)  acc5: 98.4375 (98.4375)  time: 0.0282  data: 0.0002  max mem: 5511
[08:09:24.389651] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7784 (0.7652)  acc1: 75.0000 (75.2794)  acc5: 98.4375 (98.4272)  time: 0.0281  data: 0.0002  max mem: 5511
[08:09:24.541007] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7483 (0.7668)  acc1: 76.5625 (75.2100)  acc5: 98.4375 (98.4300)  time: 0.0272  data: 0.0001  max mem: 5511
[08:09:24.723303] Test: Total time: 0:00:05 (0.0338 s / it)
[08:09:24.724353] * Acc@1 75.210 Acc@5 98.430 loss 0.767
[08:09:24.725034] Accuracy of the network on the 10000 test images: 75.2%
[08:09:24.725440] Max accuracy: 75.92%
[08:09:24.945080] log_dir: ./output_dir
[08:09:25.896601] Epoch: [42]  [  0/781]  eta: 0:12:21  lr: 0.000168  training_loss: 1.7176 (1.7176)  mae_loss: 0.2747 (0.2747)  classification_loss: 1.4331 (1.4331)  loss_mask: 0.0098 (0.0098)  time: 0.9496  data: 0.7018  max mem: 5511
[08:09:29.842261] Epoch: [42]  [ 20/781]  eta: 0:02:57  lr: 0.000168  training_loss: 1.7522 (1.7669)  mae_loss: 0.2842 (0.2863)  classification_loss: 1.4392 (1.4700)  loss_mask: 0.0060 (0.0107)  time: 0.1972  data: 0.0002  max mem: 5511
[08:09:33.789155] Epoch: [42]  [ 40/781]  eta: 0:02:39  lr: 0.000168  training_loss: 1.7630 (1.7581)  mae_loss: 0.2708 (0.2814)  classification_loss: 1.4816 (1.4684)  loss_mask: 0.0033 (0.0083)  time: 0.1973  data: 0.0002  max mem: 5511
[08:09:37.729712] Epoch: [42]  [ 60/781]  eta: 0:02:31  lr: 0.000168  training_loss: 1.7579 (1.7616)  mae_loss: 0.2691 (0.2802)  classification_loss: 1.4668 (1.4724)  loss_mask: 0.0060 (0.0090)  time: 0.1969  data: 0.0002  max mem: 5511
[08:09:41.682370] Epoch: [42]  [ 80/781]  eta: 0:02:24  lr: 0.000167  training_loss: 1.7147 (1.7471)  mae_loss: 0.2810 (0.2809)  classification_loss: 1.3806 (1.4559)  loss_mask: 0.0053 (0.0103)  time: 0.1976  data: 0.0002  max mem: 5511
[08:09:45.651504] Epoch: [42]  [100/781]  eta: 0:02:19  lr: 0.000167  training_loss: 1.7862 (1.7571)  mae_loss: 0.2792 (0.2816)  classification_loss: 1.5002 (1.4654)  loss_mask: 0.0038 (0.0101)  time: 0.1984  data: 0.0002  max mem: 5511
[08:09:49.586076] Epoch: [42]  [120/781]  eta: 0:02:14  lr: 0.000167  training_loss: 1.6938 (1.7489)  mae_loss: 0.2796 (0.2808)  classification_loss: 1.4246 (1.4579)  loss_mask: 0.0043 (0.0101)  time: 0.1966  data: 0.0002  max mem: 5511
[08:09:53.560094] Epoch: [42]  [140/781]  eta: 0:02:10  lr: 0.000167  training_loss: 1.6993 (1.7430)  mae_loss: 0.2667 (0.2796)  classification_loss: 1.4183 (1.4533)  loss_mask: 0.0054 (0.0101)  time: 0.1986  data: 0.0002  max mem: 5511
[08:09:57.497551] Epoch: [42]  [160/781]  eta: 0:02:05  lr: 0.000167  training_loss: 1.6880 (1.7388)  mae_loss: 0.2634 (0.2778)  classification_loss: 1.3648 (1.4481)  loss_mask: 0.0130 (0.0129)  time: 0.1968  data: 0.0002  max mem: 5511
[08:10:01.457862] Epoch: [42]  [180/781]  eta: 0:02:01  lr: 0.000167  training_loss: 1.8827 (1.7492)  mae_loss: 0.2649 (0.2779)  classification_loss: 1.4087 (1.4443)  loss_mask: 0.1182 (0.0270)  time: 0.1979  data: 0.0002  max mem: 5511
[08:10:05.398746] Epoch: [42]  [200/781]  eta: 0:01:56  lr: 0.000167  training_loss: 1.7654 (1.7532)  mae_loss: 0.2814 (0.2777)  classification_loss: 1.4154 (1.4418)  loss_mask: 0.0692 (0.0337)  time: 0.1970  data: 0.0002  max mem: 5511
[08:10:09.342094] Epoch: [42]  [220/781]  eta: 0:01:52  lr: 0.000167  training_loss: 1.8381 (1.7554)  mae_loss: 0.2639 (0.2771)  classification_loss: 1.4230 (1.4409)  loss_mask: 0.0555 (0.0374)  time: 0.1971  data: 0.0003  max mem: 5511
[08:10:13.306903] Epoch: [42]  [240/781]  eta: 0:01:48  lr: 0.000167  training_loss: 1.7061 (1.7535)  mae_loss: 0.2645 (0.2767)  classification_loss: 1.3941 (1.4405)  loss_mask: 0.0206 (0.0363)  time: 0.1982  data: 0.0002  max mem: 5511
[08:10:17.245049] Epoch: [42]  [260/781]  eta: 0:01:44  lr: 0.000167  training_loss: 1.6722 (1.7491)  mae_loss: 0.2505 (0.2757)  classification_loss: 1.3936 (1.4391)  loss_mask: 0.0101 (0.0344)  time: 0.1967  data: 0.0002  max mem: 5511
[08:10:21.204212] Epoch: [42]  [280/781]  eta: 0:01:40  lr: 0.000166  training_loss: 1.6867 (1.7456)  mae_loss: 0.2823 (0.2755)  classification_loss: 1.4365 (1.4376)  loss_mask: 0.0055 (0.0325)  time: 0.1978  data: 0.0002  max mem: 5511
[08:10:25.141468] Epoch: [42]  [300/781]  eta: 0:01:36  lr: 0.000166  training_loss: 1.7400 (1.7450)  mae_loss: 0.2643 (0.2752)  classification_loss: 1.4449 (1.4384)  loss_mask: 0.0070 (0.0313)  time: 0.1968  data: 0.0002  max mem: 5511
[08:10:29.087985] Epoch: [42]  [320/781]  eta: 0:01:32  lr: 0.000166  training_loss: 1.6895 (1.7420)  mae_loss: 0.2528 (0.2744)  classification_loss: 1.4279 (1.4375)  loss_mask: 0.0072 (0.0301)  time: 0.1972  data: 0.0002  max mem: 5511
[08:10:33.069726] Epoch: [42]  [340/781]  eta: 0:01:28  lr: 0.000166  training_loss: 1.7060 (1.7402)  mae_loss: 0.2738 (0.2743)  classification_loss: 1.4305 (1.4373)  loss_mask: 0.0028 (0.0286)  time: 0.1990  data: 0.0002  max mem: 5511
[08:10:37.016412] Epoch: [42]  [360/781]  eta: 0:01:24  lr: 0.000166  training_loss: 1.6817 (1.7393)  mae_loss: 0.2684 (0.2740)  classification_loss: 1.4559 (1.4382)  loss_mask: 0.0012 (0.0272)  time: 0.1972  data: 0.0003  max mem: 5511
[08:10:40.969467] Epoch: [42]  [380/781]  eta: 0:01:19  lr: 0.000166  training_loss: 1.7085 (1.7387)  mae_loss: 0.2707 (0.2745)  classification_loss: 1.4205 (1.4384)  loss_mask: 0.0014 (0.0258)  time: 0.1976  data: 0.0003  max mem: 5511
[08:10:44.929611] Epoch: [42]  [400/781]  eta: 0:01:15  lr: 0.000166  training_loss: 1.7326 (1.7372)  mae_loss: 0.2599 (0.2742)  classification_loss: 1.4834 (1.4383)  loss_mask: 0.0011 (0.0246)  time: 0.1979  data: 0.0003  max mem: 5511
[08:10:48.894819] Epoch: [42]  [420/781]  eta: 0:01:11  lr: 0.000166  training_loss: 1.7045 (1.7364)  mae_loss: 0.2822 (0.2750)  classification_loss: 1.4396 (1.4379)  loss_mask: 0.0006 (0.0235)  time: 0.1982  data: 0.0002  max mem: 5511
[08:10:52.821375] Epoch: [42]  [440/781]  eta: 0:01:07  lr: 0.000166  training_loss: 1.7436 (1.7368)  mae_loss: 0.2749 (0.2752)  classification_loss: 1.4520 (1.4391)  loss_mask: 0.0007 (0.0224)  time: 0.1962  data: 0.0003  max mem: 5511
[08:10:56.760562] Epoch: [42]  [460/781]  eta: 0:01:03  lr: 0.000166  training_loss: 1.6423 (1.7328)  mae_loss: 0.2615 (0.2748)  classification_loss: 1.3822 (1.4364)  loss_mask: 0.0007 (0.0215)  time: 0.1968  data: 0.0002  max mem: 5511
[08:11:00.712237] Epoch: [42]  [480/781]  eta: 0:00:59  lr: 0.000165  training_loss: 1.7203 (1.7314)  mae_loss: 0.2625 (0.2745)  classification_loss: 1.4379 (1.4363)  loss_mask: 0.0004 (0.0206)  time: 0.1975  data: 0.0002  max mem: 5511
[08:11:04.634507] Epoch: [42]  [500/781]  eta: 0:00:55  lr: 0.000165  training_loss: 1.7516 (1.7316)  mae_loss: 0.2846 (0.2747)  classification_loss: 1.4590 (1.4370)  loss_mask: 0.0006 (0.0199)  time: 0.1959  data: 0.0002  max mem: 5511
[08:11:08.603507] Epoch: [42]  [520/781]  eta: 0:00:51  lr: 0.000165  training_loss: 1.6956 (1.7318)  mae_loss: 0.2773 (0.2749)  classification_loss: 1.4205 (1.4378)  loss_mask: 0.0007 (0.0192)  time: 0.1984  data: 0.0002  max mem: 5511
[08:11:12.532417] Epoch: [42]  [540/781]  eta: 0:00:47  lr: 0.000165  training_loss: 1.7446 (1.7325)  mae_loss: 0.2812 (0.2751)  classification_loss: 1.4629 (1.4386)  loss_mask: 0.0012 (0.0188)  time: 0.1963  data: 0.0003  max mem: 5511
[08:11:16.472306] Epoch: [42]  [560/781]  eta: 0:00:43  lr: 0.000165  training_loss: 1.7141 (1.7316)  mae_loss: 0.2762 (0.2752)  classification_loss: 1.4405 (1.4381)  loss_mask: 0.0009 (0.0182)  time: 0.1969  data: 0.0002  max mem: 5511
[08:11:20.418214] Epoch: [42]  [580/781]  eta: 0:00:39  lr: 0.000165  training_loss: 1.7222 (1.7310)  mae_loss: 0.2659 (0.2751)  classification_loss: 1.4309 (1.4381)  loss_mask: 0.0019 (0.0178)  time: 0.1972  data: 0.0002  max mem: 5511
[08:11:24.371381] Epoch: [42]  [600/781]  eta: 0:00:35  lr: 0.000165  training_loss: 1.7104 (1.7305)  mae_loss: 0.2769 (0.2752)  classification_loss: 1.4061 (1.4379)  loss_mask: 0.0024 (0.0174)  time: 0.1976  data: 0.0003  max mem: 5511
[08:11:28.354412] Epoch: [42]  [620/781]  eta: 0:00:31  lr: 0.000165  training_loss: 1.8012 (1.7341)  mae_loss: 0.2895 (0.2757)  classification_loss: 1.4399 (1.4383)  loss_mask: 0.0246 (0.0201)  time: 0.1991  data: 0.0003  max mem: 5511
[08:11:32.317274] Epoch: [42]  [640/781]  eta: 0:00:28  lr: 0.000165  training_loss: 1.7590 (1.7354)  mae_loss: 0.2724 (0.2756)  classification_loss: 1.4177 (1.4382)  loss_mask: 0.0590 (0.0216)  time: 0.1981  data: 0.0002  max mem: 5511
[08:11:36.248238] Epoch: [42]  [660/781]  eta: 0:00:24  lr: 0.000165  training_loss: 1.7190 (1.7351)  mae_loss: 0.2640 (0.2753)  classification_loss: 1.4439 (1.4381)  loss_mask: 0.0272 (0.0218)  time: 0.1965  data: 0.0002  max mem: 5511
[08:11:40.222459] Epoch: [42]  [680/781]  eta: 0:00:20  lr: 0.000164  training_loss: 1.7142 (1.7343)  mae_loss: 0.2733 (0.2753)  classification_loss: 1.4149 (1.4376)  loss_mask: 0.0088 (0.0214)  time: 0.1986  data: 0.0002  max mem: 5511
[08:11:44.184559] Epoch: [42]  [700/781]  eta: 0:00:16  lr: 0.000164  training_loss: 1.7428 (1.7347)  mae_loss: 0.2843 (0.2755)  classification_loss: 1.4339 (1.4380)  loss_mask: 0.0064 (0.0213)  time: 0.1980  data: 0.0002  max mem: 5511
[08:11:48.152497] Epoch: [42]  [720/781]  eta: 0:00:12  lr: 0.000164  training_loss: 1.7267 (1.7343)  mae_loss: 0.2575 (0.2752)  classification_loss: 1.4514 (1.4376)  loss_mask: 0.0180 (0.0215)  time: 0.1983  data: 0.0003  max mem: 5511
[08:11:52.124067] Epoch: [42]  [740/781]  eta: 0:00:08  lr: 0.000164  training_loss: 1.7072 (1.7335)  mae_loss: 0.2645 (0.2749)  classification_loss: 1.4139 (1.4373)  loss_mask: 0.0080 (0.0213)  time: 0.1984  data: 0.0002  max mem: 5511
[08:11:56.084424] Epoch: [42]  [760/781]  eta: 0:00:04  lr: 0.000164  training_loss: 1.7311 (1.7338)  mae_loss: 0.2762 (0.2750)  classification_loss: 1.4732 (1.4379)  loss_mask: 0.0060 (0.0209)  time: 0.1979  data: 0.0002  max mem: 5511
[08:12:00.025766] Epoch: [42]  [780/781]  eta: 0:00:00  lr: 0.000164  training_loss: 1.6614 (1.7326)  mae_loss: 0.2706 (0.2749)  classification_loss: 1.3950 (1.4370)  loss_mask: 0.0033 (0.0207)  time: 0.1970  data: 0.0002  max mem: 5511
[08:12:00.214338] Epoch: [42] Total time: 0:02:35 (0.1988 s / it)
[08:12:00.214982] Averaged stats: lr: 0.000164  training_loss: 1.6614 (1.7326)  mae_loss: 0.2706 (0.2749)  classification_loss: 1.3950 (1.4370)  loss_mask: 0.0033 (0.0207)
[08:12:00.829849] Test:  [  0/157]  eta: 0:01:35  testing_loss: 0.6685 (0.6685)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.6100  data: 0.5801  max mem: 5511
[08:12:01.124325] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.7156 (0.7354)  acc1: 75.0000 (75.9943)  acc5: 98.4375 (99.1477)  time: 0.0819  data: 0.0529  max mem: 5511
[08:12:01.409106] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.7156 (0.7025)  acc1: 76.5625 (77.4554)  acc5: 98.4375 (98.8839)  time: 0.0287  data: 0.0002  max mem: 5511
[08:12:01.699830] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6878 (0.7092)  acc1: 76.5625 (77.0161)  acc5: 98.4375 (98.5383)  time: 0.0287  data: 0.0002  max mem: 5511
[08:12:01.987530] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.7071 (0.7242)  acc1: 75.0000 (76.3720)  acc5: 98.4375 (98.4756)  time: 0.0288  data: 0.0002  max mem: 5511
[08:12:02.276572] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.7113 (0.7163)  acc1: 76.5625 (77.1446)  acc5: 98.4375 (98.3762)  time: 0.0287  data: 0.0002  max mem: 5511
[08:12:02.558763] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6678 (0.7132)  acc1: 79.6875 (77.1516)  acc5: 98.4375 (98.4887)  time: 0.0284  data: 0.0002  max mem: 5511
[08:12:02.844557] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6492 (0.7010)  acc1: 78.1250 (77.6188)  acc5: 98.4375 (98.5035)  time: 0.0283  data: 0.0002  max mem: 5511
[08:12:03.127365] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6845 (0.7144)  acc1: 78.1250 (77.1412)  acc5: 98.4375 (98.3410)  time: 0.0283  data: 0.0002  max mem: 5511
[08:12:03.409772] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7571 (0.7143)  acc1: 75.0000 (77.2150)  acc5: 98.4375 (98.3345)  time: 0.0281  data: 0.0002  max mem: 5511
[08:12:03.693058] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.7590 (0.7190)  acc1: 76.5625 (77.1504)  acc5: 98.4375 (98.3601)  time: 0.0282  data: 0.0002  max mem: 5511
[08:12:03.990585] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.7668 (0.7200)  acc1: 76.5625 (77.1396)  acc5: 98.4375 (98.3249)  time: 0.0288  data: 0.0002  max mem: 5511
[08:12:04.274349] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6679 (0.7145)  acc1: 78.1250 (77.2598)  acc5: 98.4375 (98.3342)  time: 0.0289  data: 0.0002  max mem: 5511
[08:12:04.556980] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6518 (0.7144)  acc1: 79.6875 (77.4094)  acc5: 98.4375 (98.3421)  time: 0.0282  data: 0.0002  max mem: 5511
[08:12:04.839961] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7262 (0.7150)  acc1: 78.1250 (77.4490)  acc5: 98.4375 (98.3156)  time: 0.0282  data: 0.0002  max mem: 5511
[08:12:05.122504] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7079 (0.7119)  acc1: 78.1250 (77.5352)  acc5: 98.4375 (98.3237)  time: 0.0281  data: 0.0001  max mem: 5511
[08:12:05.273837] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7079 (0.7124)  acc1: 78.1250 (77.5000)  acc5: 98.4375 (98.3500)  time: 0.0272  data: 0.0001  max mem: 5511
[08:12:05.436869] Test: Total time: 0:00:05 (0.0332 s / it)
[08:12:05.437350] * Acc@1 77.500 Acc@5 98.350 loss 0.712
[08:12:05.437680] Accuracy of the network on the 10000 test images: 77.5%
[08:12:05.437869] Max accuracy: 77.50%
[08:12:05.717987] log_dir: ./output_dir
[08:12:06.562489] Epoch: [43]  [  0/781]  eta: 0:10:58  lr: 0.000164  training_loss: 1.5747 (1.5747)  mae_loss: 0.2917 (0.2917)  classification_loss: 1.2769 (1.2769)  loss_mask: 0.0060 (0.0060)  time: 0.8428  data: 0.6098  max mem: 5511
[08:12:10.509331] Epoch: [43]  [ 20/781]  eta: 0:02:53  lr: 0.000164  training_loss: 1.6376 (1.6489)  mae_loss: 0.2768 (0.2755)  classification_loss: 1.3591 (1.3678)  loss_mask: 0.0035 (0.0056)  time: 0.1972  data: 0.0002  max mem: 5511
[08:12:14.444917] Epoch: [43]  [ 40/781]  eta: 0:02:37  lr: 0.000164  training_loss: 1.7317 (1.6850)  mae_loss: 0.2873 (0.2779)  classification_loss: 1.4466 (1.4033)  loss_mask: 0.0014 (0.0039)  time: 0.1967  data: 0.0002  max mem: 5511
[08:12:18.406639] Epoch: [43]  [ 60/781]  eta: 0:02:29  lr: 0.000164  training_loss: 1.7661 (1.7100)  mae_loss: 0.2821 (0.2791)  classification_loss: 1.5034 (1.4278)  loss_mask: 0.0013 (0.0031)  time: 0.1980  data: 0.0002  max mem: 5511
[08:12:22.377529] Epoch: [43]  [ 80/781]  eta: 0:02:24  lr: 0.000164  training_loss: 1.6477 (1.6966)  mae_loss: 0.2704 (0.2777)  classification_loss: 1.3931 (1.4163)  loss_mask: 0.0006 (0.0026)  time: 0.1985  data: 0.0002  max mem: 5511
[08:12:26.327020] Epoch: [43]  [100/781]  eta: 0:02:18  lr: 0.000163  training_loss: 1.7320 (1.7024)  mae_loss: 0.2543 (0.2746)  classification_loss: 1.4723 (1.4256)  loss_mask: 0.0006 (0.0022)  time: 0.1974  data: 0.0003  max mem: 5511
[08:12:30.281037] Epoch: [43]  [120/781]  eta: 0:02:14  lr: 0.000163  training_loss: 1.7098 (1.7033)  mae_loss: 0.2707 (0.2740)  classification_loss: 1.4366 (1.4274)  loss_mask: 0.0005 (0.0019)  time: 0.1976  data: 0.0002  max mem: 5511
[08:12:34.246417] Epoch: [43]  [140/781]  eta: 0:02:09  lr: 0.000163  training_loss: 1.6701 (1.7011)  mae_loss: 0.2650 (0.2742)  classification_loss: 1.4161 (1.4251)  loss_mask: 0.0005 (0.0017)  time: 0.1982  data: 0.0003  max mem: 5511
[08:12:38.190828] Epoch: [43]  [160/781]  eta: 0:02:05  lr: 0.000163  training_loss: 1.6574 (1.6958)  mae_loss: 0.2754 (0.2746)  classification_loss: 1.3762 (1.4194)  loss_mask: 0.0007 (0.0017)  time: 0.1971  data: 0.0003  max mem: 5511
[08:12:42.164535] Epoch: [43]  [180/781]  eta: 0:02:00  lr: 0.000163  training_loss: 1.7019 (1.6946)  mae_loss: 0.2713 (0.2744)  classification_loss: 1.4133 (1.4183)  loss_mask: 0.0010 (0.0018)  time: 0.1986  data: 0.0002  max mem: 5511
[08:12:46.123861] Epoch: [43]  [200/781]  eta: 0:01:56  lr: 0.000163  training_loss: 1.6950 (1.6929)  mae_loss: 0.2650 (0.2738)  classification_loss: 1.4257 (1.4171)  loss_mask: 0.0013 (0.0020)  time: 0.1979  data: 0.0002  max mem: 5511
[08:12:50.083417] Epoch: [43]  [220/781]  eta: 0:01:52  lr: 0.000163  training_loss: 1.7759 (1.7025)  mae_loss: 0.2572 (0.2733)  classification_loss: 1.4589 (1.4191)  loss_mask: 0.0193 (0.0101)  time: 0.1979  data: 0.0002  max mem: 5511
[08:12:54.035422] Epoch: [43]  [240/781]  eta: 0:01:48  lr: 0.000163  training_loss: 1.8126 (1.7116)  mae_loss: 0.2831 (0.2739)  classification_loss: 1.3538 (1.4177)  loss_mask: 0.1095 (0.0201)  time: 0.1975  data: 0.0002  max mem: 5511
[08:12:57.983081] Epoch: [43]  [260/781]  eta: 0:01:44  lr: 0.000163  training_loss: 1.7289 (1.7138)  mae_loss: 0.2706 (0.2740)  classification_loss: 1.4203 (1.4178)  loss_mask: 0.0306 (0.0219)  time: 0.1973  data: 0.0002  max mem: 5511
[08:13:01.919049] Epoch: [43]  [280/781]  eta: 0:01:40  lr: 0.000163  training_loss: 1.6860 (1.7120)  mae_loss: 0.2514 (0.2731)  classification_loss: 1.3878 (1.4171)  loss_mask: 0.0174 (0.0218)  time: 0.1967  data: 0.0002  max mem: 5511
[08:13:05.886923] Epoch: [43]  [300/781]  eta: 0:01:36  lr: 0.000162  training_loss: 1.7083 (1.7119)  mae_loss: 0.2580 (0.2727)  classification_loss: 1.4259 (1.4183)  loss_mask: 0.0065 (0.0209)  time: 0.1983  data: 0.0002  max mem: 5511
[08:13:09.878568] Epoch: [43]  [320/781]  eta: 0:01:32  lr: 0.000162  training_loss: 1.6480 (1.7104)  mae_loss: 0.2578 (0.2723)  classification_loss: 1.3764 (1.4183)  loss_mask: 0.0021 (0.0198)  time: 0.1995  data: 0.0002  max mem: 5511
[08:13:13.819696] Epoch: [43]  [340/781]  eta: 0:01:28  lr: 0.000162  training_loss: 1.6391 (1.7092)  mae_loss: 0.2678 (0.2724)  classification_loss: 1.3650 (1.4181)  loss_mask: 0.0013 (0.0188)  time: 0.1970  data: 0.0003  max mem: 5511
[08:13:17.777452] Epoch: [43]  [360/781]  eta: 0:01:23  lr: 0.000162  training_loss: 1.6673 (1.7086)  mae_loss: 0.2653 (0.2721)  classification_loss: 1.4058 (1.4188)  loss_mask: 0.0010 (0.0178)  time: 0.1978  data: 0.0002  max mem: 5511
[08:13:21.717104] Epoch: [43]  [380/781]  eta: 0:01:19  lr: 0.000162  training_loss: 1.6413 (1.7075)  mae_loss: 0.2645 (0.2726)  classification_loss: 1.3642 (1.4178)  loss_mask: 0.0010 (0.0171)  time: 0.1969  data: 0.0003  max mem: 5511
[08:13:25.670586] Epoch: [43]  [400/781]  eta: 0:01:15  lr: 0.000162  training_loss: 1.7268 (1.7085)  mae_loss: 0.2798 (0.2727)  classification_loss: 1.4283 (1.4184)  loss_mask: 0.0036 (0.0173)  time: 0.1976  data: 0.0002  max mem: 5511
[08:13:29.606492] Epoch: [43]  [420/781]  eta: 0:01:11  lr: 0.000162  training_loss: 1.7026 (1.7082)  mae_loss: 0.2742 (0.2727)  classification_loss: 1.3950 (1.4180)  loss_mask: 0.0133 (0.0175)  time: 0.1967  data: 0.0002  max mem: 5511
[08:13:33.554593] Epoch: [43]  [440/781]  eta: 0:01:07  lr: 0.000162  training_loss: 1.6829 (1.7070)  mae_loss: 0.2776 (0.2728)  classification_loss: 1.4000 (1.4166)  loss_mask: 0.0099 (0.0175)  time: 0.1973  data: 0.0002  max mem: 5511
[08:13:37.541399] Epoch: [43]  [460/781]  eta: 0:01:03  lr: 0.000162  training_loss: 1.6484 (1.7064)  mae_loss: 0.2507 (0.2719)  classification_loss: 1.3695 (1.4156)  loss_mask: 0.0137 (0.0188)  time: 0.1993  data: 0.0002  max mem: 5511
[08:13:41.486616] Epoch: [43]  [480/781]  eta: 0:00:59  lr: 0.000162  training_loss: 1.8710 (1.7136)  mae_loss: 0.2806 (0.2723)  classification_loss: 1.4651 (1.4178)  loss_mask: 0.0908 (0.0235)  time: 0.1972  data: 0.0002  max mem: 5511
[08:13:45.490522] Epoch: [43]  [500/781]  eta: 0:00:55  lr: 0.000161  training_loss: 1.7580 (1.7147)  mae_loss: 0.2595 (0.2720)  classification_loss: 1.4264 (1.4181)  loss_mask: 0.0438 (0.0246)  time: 0.2001  data: 0.0003  max mem: 5511

[08:13:49.422251] Epoch: [43]  [520/781]  eta: 0:00:51  lr: 0.000161  training_loss: 1.7214 (1.7153)  mae_loss: 0.2608 (0.2718)  classification_loss: 1.4330 (1.4179)  loss_mask: 0.0280 (0.0255)  time: 0.1965  data: 0.0003  max mem: 5511
[08:13:53.355079] Epoch: [43]  [540/781]  eta: 0:00:47  lr: 0.000161  training_loss: 1.6665 (1.7136)  mae_loss: 0.2601 (0.2714)  classification_loss: 1.3924 (1.4169)  loss_mask: 0.0133 (0.0253)  time: 0.1966  data: 0.0002  max mem: 5511
[08:13:57.305518] Epoch: [43]  [560/781]  eta: 0:00:43  lr: 0.000161  training_loss: 1.6931 (1.7134)  mae_loss: 0.2598 (0.2711)  classification_loss: 1.4057 (1.4173)  loss_mask: 0.0066 (0.0249)  time: 0.1974  data: 0.0002  max mem: 5511
[08:14:01.247129] Epoch: [43]  [580/781]  eta: 0:00:39  lr: 0.000161  training_loss: 1.7212 (1.7143)  mae_loss: 0.2677 (0.2711)  classification_loss: 1.4499 (1.4187)  loss_mask: 0.0058 (0.0244)  time: 0.1970  data: 0.0003  max mem: 5511
[08:14:05.199364] Epoch: [43]  [600/781]  eta: 0:00:35  lr: 0.000161  training_loss: 1.6954 (1.7137)  mae_loss: 0.2705 (0.2709)  classification_loss: 1.4198 (1.4190)  loss_mask: 0.0039 (0.0238)  time: 0.1975  data: 0.0008  max mem: 5511
[08:14:09.220707] Epoch: [43]  [620/781]  eta: 0:00:32  lr: 0.000161  training_loss: 1.6728 (1.7136)  mae_loss: 0.2756 (0.2710)  classification_loss: 1.4204 (1.4195)  loss_mask: 0.0020 (0.0231)  time: 0.2009  data: 0.0002  max mem: 5511
[08:14:13.195733] Epoch: [43]  [640/781]  eta: 0:00:28  lr: 0.000161  training_loss: 1.6419 (1.7122)  mae_loss: 0.2554 (0.2707)  classification_loss: 1.3832 (1.4187)  loss_mask: 0.0013 (0.0227)  time: 0.1987  data: 0.0002  max mem: 5511
[08:14:17.134014] Epoch: [43]  [660/781]  eta: 0:00:24  lr: 0.000161  training_loss: 1.7070 (1.7123)  mae_loss: 0.2704 (0.2707)  classification_loss: 1.4204 (1.4190)  loss_mask: 0.0092 (0.0226)  time: 0.1968  data: 0.0002  max mem: 5511
[08:14:21.068331] Epoch: [43]  [680/781]  eta: 0:00:20  lr: 0.000161  training_loss: 1.7129 (1.7133)  mae_loss: 0.2624 (0.2706)  classification_loss: 1.4070 (1.4180)  loss_mask: 0.0625 (0.0247)  time: 0.1966  data: 0.0002  max mem: 5511
[08:14:25.016480] Epoch: [43]  [700/781]  eta: 0:00:16  lr: 0.000160  training_loss: 1.7414 (1.7144)  mae_loss: 0.2687 (0.2708)  classification_loss: 1.4462 (1.4181)  loss_mask: 0.0414 (0.0256)  time: 0.1973  data: 0.0002  max mem: 5511
[08:14:28.993666] Epoch: [43]  [720/781]  eta: 0:00:12  lr: 0.000160  training_loss: 1.6885 (1.7138)  mae_loss: 0.2527 (0.2704)  classification_loss: 1.4111 (1.4178)  loss_mask: 0.0228 (0.0256)  time: 0.1987  data: 0.0002  max mem: 5511
[08:14:32.930425] Epoch: [43]  [740/781]  eta: 0:00:08  lr: 0.000160  training_loss: 1.6662 (1.7128)  mae_loss: 0.2657 (0.2705)  classification_loss: 1.3842 (1.4171)  loss_mask: 0.0090 (0.0252)  time: 0.1967  data: 0.0003  max mem: 5511
[08:14:36.899164] Epoch: [43]  [760/781]  eta: 0:00:04  lr: 0.000160  training_loss: 1.7289 (1.7134)  mae_loss: 0.2577 (0.2704)  classification_loss: 1.4632 (1.4183)  loss_mask: 0.0039 (0.0247)  time: 0.1984  data: 0.0004  max mem: 5511
[08:14:40.837718] Epoch: [43]  [780/781]  eta: 0:00:00  lr: 0.000160  training_loss: 1.6566 (1.7122)  mae_loss: 0.2552 (0.2703)  classification_loss: 1.3901 (1.4176)  loss_mask: 0.0063 (0.0244)  time: 0.1968  data: 0.0002  max mem: 5511
[08:14:41.016252] Epoch: [43] Total time: 0:02:35 (0.1988 s / it)
[08:14:41.016701] Averaged stats: lr: 0.000160  training_loss: 1.6566 (1.7122)  mae_loss: 0.2552 (0.2703)  classification_loss: 1.3901 (1.4176)  loss_mask: 0.0063 (0.0244)
[08:14:41.628810] Test:  [  0/157]  eta: 0:01:35  testing_loss: 0.7132 (0.7132)  acc1: 75.0000 (75.0000)  acc5: 96.8750 (96.8750)  time: 0.6082  data: 0.5776  max mem: 5511
[08:14:41.917513] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.7132 (0.7315)  acc1: 76.5625 (76.7045)  acc5: 98.4375 (98.5795)  time: 0.0813  data: 0.0527  max mem: 5511
[08:14:42.201995] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.6941 (0.7025)  acc1: 78.1250 (77.9762)  acc5: 98.4375 (98.7351)  time: 0.0285  data: 0.0002  max mem: 5511
[08:14:42.491704] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.7221 (0.7148)  acc1: 78.1250 (77.2681)  acc5: 98.4375 (98.4879)  time: 0.0286  data: 0.0002  max mem: 5511
[08:14:42.775136] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.7265 (0.7235)  acc1: 76.5625 (76.8293)  acc5: 98.4375 (98.4375)  time: 0.0285  data: 0.0002  max mem: 5511
[08:14:43.058178] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.7128 (0.7193)  acc1: 76.5625 (77.1446)  acc5: 98.4375 (98.4069)  time: 0.0282  data: 0.0002  max mem: 5511
[08:14:43.342702] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6936 (0.7193)  acc1: 76.5625 (76.7418)  acc5: 98.4375 (98.4887)  time: 0.0283  data: 0.0002  max mem: 5511
[08:14:43.629789] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6774 (0.7120)  acc1: 76.5625 (77.0026)  acc5: 100.0000 (98.5695)  time: 0.0285  data: 0.0002  max mem: 5511
[08:14:43.914175] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7187 (0.7208)  acc1: 76.5625 (76.6590)  acc5: 98.4375 (98.5725)  time: 0.0285  data: 0.0002  max mem: 5511
[08:14:44.200597] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7336 (0.7195)  acc1: 76.5625 (76.8372)  acc5: 98.4375 (98.5577)  time: 0.0284  data: 0.0002  max mem: 5511
[08:14:44.487497] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.7467 (0.7230)  acc1: 76.5625 (76.6244)  acc5: 98.4375 (98.4375)  time: 0.0285  data: 0.0002  max mem: 5511
[08:14:44.777511] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.7567 (0.7259)  acc1: 73.4375 (76.3654)  acc5: 96.8750 (98.4093)  time: 0.0287  data: 0.0002  max mem: 5511
[08:14:45.066295] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6920 (0.7206)  acc1: 76.5625 (76.6271)  acc5: 98.4375 (98.3729)  time: 0.0288  data: 0.0002  max mem: 5511
[08:14:45.350955] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6905 (0.7205)  acc1: 76.5625 (76.5386)  acc5: 98.4375 (98.4136)  time: 0.0285  data: 0.0002  max mem: 5511
[08:14:45.634250] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7265 (0.7216)  acc1: 76.5625 (76.6844)  acc5: 98.4375 (98.4264)  time: 0.0283  data: 0.0002  max mem: 5511
[08:14:45.917249] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7166 (0.7194)  acc1: 79.6875 (76.8315)  acc5: 98.4375 (98.4375)  time: 0.0282  data: 0.0002  max mem: 5511
[08:14:46.067909] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7224 (0.7202)  acc1: 78.1250 (76.8000)  acc5: 98.4375 (98.4600)  time: 0.0272  data: 0.0001  max mem: 5511
[08:14:46.249792] Test: Total time: 0:00:05 (0.0333 s / it)
[08:14:46.250253] * Acc@1 76.800 Acc@5 98.460 loss 0.720
[08:14:46.250559] Accuracy of the network on the 10000 test images: 76.8%
[08:14:46.250738] Max accuracy: 77.50%
[08:14:46.401025] log_dir: ./output_dir
[08:14:47.366506] Epoch: [44]  [  0/781]  eta: 0:12:32  lr: 0.000160  training_loss: 1.5192 (1.5192)  mae_loss: 0.2999 (0.2999)  classification_loss: 1.1682 (1.1682)  loss_mask: 0.0511 (0.0511)  time: 0.9637  data: 0.7293  max mem: 5511
[08:14:51.307527] Epoch: [44]  [ 20/781]  eta: 0:02:57  lr: 0.000160  training_loss: 1.6711 (1.6999)  mae_loss: 0.2630 (0.2724)  classification_loss: 1.3931 (1.3683)  loss_mask: 0.0205 (0.0591)  time: 0.1970  data: 0.0002  max mem: 5511
[08:14:55.240622] Epoch: [44]  [ 40/781]  eta: 0:02:39  lr: 0.000160  training_loss: 1.6761 (1.6871)  mae_loss: 0.2591 (0.2674)  classification_loss: 1.4014 (1.3806)  loss_mask: 0.0131 (0.0390)  time: 0.1966  data: 0.0002  max mem: 5511
[08:14:59.197984] Epoch: [44]  [ 60/781]  eta: 0:02:31  lr: 0.000160  training_loss: 1.7062 (1.6956)  mae_loss: 0.2645 (0.2687)  classification_loss: 1.4302 (1.3980)  loss_mask: 0.0066 (0.0289)  time: 0.1978  data: 0.0002  max mem: 5511
[08:15:03.136826] Epoch: [44]  [ 80/781]  eta: 0:02:24  lr: 0.000160  training_loss: 1.6945 (1.6955)  mae_loss: 0.2698 (0.2712)  classification_loss: 1.4333 (1.4016)  loss_mask: 0.0020 (0.0226)  time: 0.1969  data: 0.0002  max mem: 5511
[08:15:07.065366] Epoch: [44]  [100/781]  eta: 0:02:19  lr: 0.000160  training_loss: 1.6320 (1.6858)  mae_loss: 0.2700 (0.2714)  classification_loss: 1.3586 (1.3956)  loss_mask: 0.0022 (0.0187)  time: 0.1963  data: 0.0002  max mem: 5511
[08:15:11.013865] Epoch: [44]  [120/781]  eta: 0:02:14  lr: 0.000159  training_loss: 1.6581 (1.6850)  mae_loss: 0.2604 (0.2701)  classification_loss: 1.4041 (1.3991)  loss_mask: 0.0010 (0.0158)  time: 0.1973  data: 0.0003  max mem: 5511
[08:15:14.965798] Epoch: [44]  [140/781]  eta: 0:02:09  lr: 0.000159  training_loss: 1.7149 (1.6994)  mae_loss: 0.2828 (0.2726)  classification_loss: 1.4197 (1.4026)  loss_mask: 0.0525 (0.0242)  time: 0.1975  data: 0.0002  max mem: 5511
[08:15:18.951310] Epoch: [44]  [160/781]  eta: 0:02:05  lr: 0.000159  training_loss: 1.8899 (1.7292)  mae_loss: 0.2651 (0.2732)  classification_loss: 1.5340 (1.4162)  loss_mask: 0.1211 (0.0398)  time: 0.1992  data: 0.0002  max mem: 5511
[08:15:22.872913] Epoch: [44]  [180/781]  eta: 0:02:01  lr: 0.000159  training_loss: 1.7622 (1.7346)  mae_loss: 0.2687 (0.2731)  classification_loss: 1.4344 (1.4175)  loss_mask: 0.0579 (0.0440)  time: 0.1960  data: 0.0002  max mem: 5511
[08:15:26.808455] Epoch: [44]  [200/781]  eta: 0:01:56  lr: 0.000159  training_loss: 1.7700 (1.7375)  mae_loss: 0.2698 (0.2733)  classification_loss: 1.4154 (1.4187)  loss_mask: 0.0572 (0.0454)  time: 0.1967  data: 0.0002  max mem: 5511
[08:15:30.731846] Epoch: [44]  [220/781]  eta: 0:01:52  lr: 0.000159  training_loss: 1.7103 (1.7348)  mae_loss: 0.2620 (0.2727)  classification_loss: 1.4280 (1.4192)  loss_mask: 0.0116 (0.0428)  time: 0.1961  data: 0.0002  max mem: 5511
[08:15:34.672768] Epoch: [44]  [240/781]  eta: 0:01:48  lr: 0.000159  training_loss: 1.6780 (1.7318)  mae_loss: 0.2673 (0.2725)  classification_loss: 1.3782 (1.4179)  loss_mask: 0.0067 (0.0414)  time: 0.1969  data: 0.0002  max mem: 5511
[08:15:38.631321] Epoch: [44]  [260/781]  eta: 0:01:44  lr: 0.000159  training_loss: 1.6857 (1.7282)  mae_loss: 0.2653 (0.2716)  classification_loss: 1.4083 (1.4172)  loss_mask: 0.0073 (0.0394)  time: 0.1978  data: 0.0002  max mem: 5511
[08:15:42.588772] Epoch: [44]  [280/781]  eta: 0:01:40  lr: 0.000159  training_loss: 1.6825 (1.7272)  mae_loss: 0.2618 (0.2719)  classification_loss: 1.3813 (1.4167)  loss_mask: 0.0142 (0.0387)  time: 0.1978  data: 0.0003  max mem: 5511
[08:15:46.539920] Epoch: [44]  [300/781]  eta: 0:01:36  lr: 0.000159  training_loss: 1.6823 (1.7261)  mae_loss: 0.2472 (0.2706)  classification_loss: 1.4176 (1.4176)  loss_mask: 0.0118 (0.0380)  time: 0.1975  data: 0.0002  max mem: 5511
[08:15:50.494452] Epoch: [44]  [320/781]  eta: 0:01:32  lr: 0.000158  training_loss: 1.7145 (1.7268)  mae_loss: 0.2742 (0.2710)  classification_loss: 1.4482 (1.4196)  loss_mask: 0.0056 (0.0362)  time: 0.1976  data: 0.0002  max mem: 5511
[08:15:54.449476] Epoch: [44]  [340/781]  eta: 0:01:27  lr: 0.000158  training_loss: 1.7163 (1.7258)  mae_loss: 0.2699 (0.2717)  classification_loss: 1.4147 (1.4195)  loss_mask: 0.0064 (0.0346)  time: 0.1976  data: 0.0002  max mem: 5511
[08:15:58.389452] Epoch: [44]  [360/781]  eta: 0:01:23  lr: 0.000158  training_loss: 1.7247 (1.7265)  mae_loss: 0.2721 (0.2715)  classification_loss: 1.4507 (1.4219)  loss_mask: 0.0028 (0.0331)  time: 0.1969  data: 0.0002  max mem: 5511
[08:16:02.327180] Epoch: [44]  [380/781]  eta: 0:01:19  lr: 0.000158  training_loss: 1.6600 (1.7252)  mae_loss: 0.2761 (0.2721)  classification_loss: 1.3812 (1.4216)  loss_mask: 0.0022 (0.0315)  time: 0.1968  data: 0.0002  max mem: 5511
[08:16:06.268065] Epoch: [44]  [400/781]  eta: 0:01:15  lr: 0.000158  training_loss: 1.7091 (1.7236)  mae_loss: 0.2692 (0.2720)  classification_loss: 1.4245 (1.4214)  loss_mask: 0.0015 (0.0301)  time: 0.1970  data: 0.0002  max mem: 5511
[08:16:10.218801] Epoch: [44]  [420/781]  eta: 0:01:11  lr: 0.000158  training_loss: 1.6794 (1.7213)  mae_loss: 0.2704 (0.2718)  classification_loss: 1.4080 (1.4204)  loss_mask: 0.0021 (0.0290)  time: 0.1974  data: 0.0002  max mem: 5511
[08:16:14.143691] Epoch: [44]  [440/781]  eta: 0:01:07  lr: 0.000158  training_loss: 1.6866 (1.7197)  mae_loss: 0.2775 (0.2719)  classification_loss: 1.3934 (1.4190)  loss_mask: 0.0110 (0.0288)  time: 0.1962  data: 0.0002  max mem: 5511
[08:16:18.151312] Epoch: [44]  [460/781]  eta: 0:01:03  lr: 0.000158  training_loss: 1.6715 (1.7178)  mae_loss: 0.2675 (0.2717)  classification_loss: 1.3623 (1.4174)  loss_mask: 0.0079 (0.0287)  time: 0.2003  data: 0.0002  max mem: 5511
[08:16:22.107717] Epoch: [44]  [480/781]  eta: 0:00:59  lr: 0.000158  training_loss: 1.7190 (1.7176)  mae_loss: 0.2699 (0.2715)  classification_loss: 1.4355 (1.4183)  loss_mask: 0.0033 (0.0279)  time: 0.1977  data: 0.0002  max mem: 5511
[08:16:26.048637] Epoch: [44]  [500/781]  eta: 0:00:55  lr: 0.000157  training_loss: 1.6942 (1.7175)  mae_loss: 0.2592 (0.2712)  classification_loss: 1.4065 (1.4187)  loss_mask: 0.0067 (0.0276)  time: 0.1970  data: 0.0002  max mem: 5511
[08:16:29.977853] Epoch: [44]  [520/781]  eta: 0:00:51  lr: 0.000157  training_loss: 1.6788 (1.7176)  mae_loss: 0.2748 (0.2712)  classification_loss: 1.3605 (1.4180)  loss_mask: 0.0191 (0.0284)  time: 0.1964  data: 0.0002  max mem: 5511
[08:16:33.934493] Epoch: [44]  [540/781]  eta: 0:00:47  lr: 0.000157  training_loss: 1.7404 (1.7186)  mae_loss: 0.2633 (0.2712)  classification_loss: 1.4049 (1.4177)  loss_mask: 0.0591 (0.0298)  time: 0.1977  data: 0.0003  max mem: 5511
[08:16:37.871292] Epoch: [44]  [560/781]  eta: 0:00:43  lr: 0.000157  training_loss: 1.7542 (1.7202)  mae_loss: 0.2630 (0.2710)  classification_loss: 1.4429 (1.4186)  loss_mask: 0.0378 (0.0307)  time: 0.1968  data: 0.0003  max mem: 5511
[08:16:41.812619] Epoch: [44]  [580/781]  eta: 0:00:39  lr: 0.000157  training_loss: 1.7162 (1.7196)  mae_loss: 0.2760 (0.2712)  classification_loss: 1.4058 (1.4182)  loss_mask: 0.0114 (0.0302)  time: 0.1970  data: 0.0003  max mem: 5511
[08:16:45.768175] Epoch: [44]  [600/781]  eta: 0:00:35  lr: 0.000157  training_loss: 1.6216 (1.7173)  mae_loss: 0.2759 (0.2714)  classification_loss: 1.3538 (1.4165)  loss_mask: 0.0048 (0.0294)  time: 0.1977  data: 0.0002  max mem: 5511
[08:16:49.732379] Epoch: [44]  [620/781]  eta: 0:00:31  lr: 0.000157  training_loss: 1.6609 (1.7160)  mae_loss: 0.2814 (0.2718)  classification_loss: 1.3692 (1.4156)  loss_mask: 0.0037 (0.0286)  time: 0.1981  data: 0.0002  max mem: 5511
[08:16:53.716063] Epoch: [44]  [640/781]  eta: 0:00:27  lr: 0.000157  training_loss: 1.6834 (1.7147)  mae_loss: 0.2578 (0.2715)  classification_loss: 1.3909 (1.4153)  loss_mask: 0.0024 (0.0278)  time: 0.1991  data: 0.0003  max mem: 5511
[08:16:57.681879] Epoch: [44]  [660/781]  eta: 0:00:24  lr: 0.000157  training_loss: 1.6649 (1.7136)  mae_loss: 0.2627 (0.2716)  classification_loss: 1.3741 (1.4149)  loss_mask: 0.0012 (0.0272)  time: 0.1982  data: 0.0002  max mem: 5511
[08:17:01.627430] Epoch: [44]  [680/781]  eta: 0:00:20  lr: 0.000157  training_loss: 1.6566 (1.7132)  mae_loss: 0.2783 (0.2717)  classification_loss: 1.4157 (1.4150)  loss_mask: 0.0012 (0.0265)  time: 0.1971  data: 0.0003  max mem: 5511
[08:17:05.555827] Epoch: [44]  [700/781]  eta: 0:00:16  lr: 0.000156  training_loss: 1.6847 (1.7124)  mae_loss: 0.2611 (0.2717)  classification_loss: 1.3879 (1.4147)  loss_mask: 0.0008 (0.0259)  time: 0.1963  data: 0.0003  max mem: 5511
[08:17:09.495204] Epoch: [44]  [720/781]  eta: 0:00:12  lr: 0.000156  training_loss: 1.6841 (1.7113)  mae_loss: 0.2576 (0.2715)  classification_loss: 1.4132 (1.4146)  loss_mask: 0.0006 (0.0252)  time: 0.1969  data: 0.0003  max mem: 5511
[08:17:13.446934] Epoch: [44]  [740/781]  eta: 0:00:08  lr: 0.000156  training_loss: 1.6051 (1.7094)  mae_loss: 0.2731 (0.2715)  classification_loss: 1.3424 (1.4134)  loss_mask: 0.0008 (0.0245)  time: 0.1975  data: 0.0002  max mem: 5511
[08:17:17.421411] Epoch: [44]  [760/781]  eta: 0:00:04  lr: 0.000156  training_loss: 1.6907 (1.7087)  mae_loss: 0.2632 (0.2715)  classification_loss: 1.4212 (1.4133)  loss_mask: 0.0006 (0.0239)  time: 0.1986  data: 0.0002  max mem: 5511
[08:17:21.367744] Epoch: [44]  [780/781]  eta: 0:00:00  lr: 0.000156  training_loss: 1.6840 (1.7084)  mae_loss: 0.2764 (0.2717)  classification_loss: 1.3990 (1.4131)  loss_mask: 0.0010 (0.0235)  time: 0.1972  data: 0.0002  max mem: 5511
[08:17:21.536877] Epoch: [44] Total time: 0:02:35 (0.1986 s / it)
[08:17:21.537623] Averaged stats: lr: 0.000156  training_loss: 1.6840 (1.7084)  mae_loss: 0.2764 (0.2717)  classification_loss: 1.3990 (1.4131)  loss_mask: 0.0010 (0.0235)
[08:17:22.274973] Test:  [  0/157]  eta: 0:01:54  testing_loss: 0.7891 (0.7891)  acc1: 78.1250 (78.1250)  acc5: 98.4375 (98.4375)  time: 0.7322  data: 0.6952  max mem: 5511
[08:17:22.559161] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.7561 (0.7509)  acc1: 75.0000 (75.5682)  acc5: 98.4375 (98.7216)  time: 0.0922  data: 0.0634  max mem: 5511
[08:17:22.843750] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.7033 (0.7223)  acc1: 76.5625 (77.2321)  acc5: 98.4375 (98.8839)  time: 0.0282  data: 0.0002  max mem: 5511
[08:17:23.131429] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6923 (0.7288)  acc1: 78.1250 (77.1673)  acc5: 98.4375 (98.5383)  time: 0.0284  data: 0.0002  max mem: 5511
[08:17:23.419947] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.7282 (0.7347)  acc1: 76.5625 (76.9055)  acc5: 98.4375 (98.4756)  time: 0.0286  data: 0.0002  max mem: 5511
[08:17:23.709927] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6836 (0.7263)  acc1: 76.5625 (77.1140)  acc5: 98.4375 (98.5294)  time: 0.0288  data: 0.0002  max mem: 5511
[08:17:23.999603] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6816 (0.7211)  acc1: 76.5625 (77.1260)  acc5: 98.4375 (98.6168)  time: 0.0288  data: 0.0002  max mem: 5511
[08:17:24.285759] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6823 (0.7142)  acc1: 76.5625 (77.2007)  acc5: 98.4375 (98.5695)  time: 0.0286  data: 0.0002  max mem: 5511
[08:17:24.571866] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6924 (0.7224)  acc1: 76.5625 (77.1026)  acc5: 98.4375 (98.5147)  time: 0.0285  data: 0.0002  max mem: 5511
[08:17:24.858160] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7306 (0.7213)  acc1: 76.5625 (77.2321)  acc5: 98.4375 (98.5062)  time: 0.0285  data: 0.0002  max mem: 5511
[08:17:25.146083] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.7090 (0.7251)  acc1: 76.5625 (77.0885)  acc5: 98.4375 (98.4684)  time: 0.0286  data: 0.0002  max mem: 5511
[08:17:25.434644] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.7582 (0.7252)  acc1: 76.5625 (77.2241)  acc5: 98.4375 (98.4234)  time: 0.0287  data: 0.0002  max mem: 5511
[08:17:25.730949] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.7040 (0.7202)  acc1: 78.1250 (77.4148)  acc5: 98.4375 (98.4504)  time: 0.0291  data: 0.0002  max mem: 5511
[08:17:26.017523] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.7014 (0.7199)  acc1: 78.1250 (77.3855)  acc5: 98.4375 (98.4614)  time: 0.0290  data: 0.0002  max mem: 5511
[08:17:26.306762] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7367 (0.7203)  acc1: 78.1250 (77.4047)  acc5: 98.4375 (98.4597)  time: 0.0286  data: 0.0002  max mem: 5511
[08:17:26.589129] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7219 (0.7171)  acc1: 78.1250 (77.5145)  acc5: 98.4375 (98.4375)  time: 0.0284  data: 0.0002  max mem: 5511
[08:17:26.744291] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7012 (0.7175)  acc1: 78.1250 (77.4300)  acc5: 98.4375 (98.4500)  time: 0.0273  data: 0.0001  max mem: 5511
[08:17:26.929425] Test: Total time: 0:00:05 (0.0343 s / it)
[08:17:26.929938] * Acc@1 77.430 Acc@5 98.450 loss 0.717
[08:17:26.930247] Accuracy of the network on the 10000 test images: 77.4%
[08:17:26.930438] Max accuracy: 77.50%
[08:17:27.227870] log_dir: ./output_dir
[08:17:28.028279] Epoch: [45]  [  0/781]  eta: 0:10:23  lr: 0.000156  training_loss: 1.6014 (1.6014)  mae_loss: 0.2231 (0.2231)  classification_loss: 1.3650 (1.3650)  loss_mask: 0.0134 (0.0134)  time: 0.7986  data: 0.5783  max mem: 5511
[08:17:31.962436] Epoch: [45]  [ 20/781]  eta: 0:02:51  lr: 0.000156  training_loss: 1.7132 (1.7017)  mae_loss: 0.2768 (0.2775)  classification_loss: 1.4182 (1.3966)  loss_mask: 0.0131 (0.0276)  time: 0.1966  data: 0.0002  max mem: 5511
[08:17:35.902935] Epoch: [45]  [ 40/781]  eta: 0:02:36  lr: 0.000156  training_loss: 1.6962 (1.6959)  mae_loss: 0.2690 (0.2770)  classification_loss: 1.3880 (1.3954)  loss_mask: 0.0153 (0.0235)  time: 0.1969  data: 0.0002  max mem: 5511
[08:17:39.836183] Epoch: [45]  [ 60/781]  eta: 0:02:28  lr: 0.000156  training_loss: 1.6809 (1.7013)  mae_loss: 0.2534 (0.2746)  classification_loss: 1.4018 (1.3981)  loss_mask: 0.0096 (0.0287)  time: 0.1966  data: 0.0003  max mem: 5511
[08:17:43.762680] Epoch: [45]  [ 80/781]  eta: 0:02:23  lr: 0.000156  training_loss: 1.6539 (1.6951)  mae_loss: 0.2795 (0.2752)  classification_loss: 1.3753 (1.3962)  loss_mask: 0.0066 (0.0238)  time: 0.1962  data: 0.0002  max mem: 5511
[08:17:47.714400] Epoch: [45]  [100/781]  eta: 0:02:18  lr: 0.000156  training_loss: 1.6639 (1.6934)  mae_loss: 0.2578 (0.2731)  classification_loss: 1.4002 (1.4002)  loss_mask: 0.0020 (0.0200)  time: 0.1975  data: 0.0002  max mem: 5511
[08:17:51.663372] Epoch: [45]  [120/781]  eta: 0:02:13  lr: 0.000155  training_loss: 1.6060 (1.6816)  mae_loss: 0.2646 (0.2721)  classification_loss: 1.3173 (1.3921)  loss_mask: 0.0017 (0.0174)  time: 0.1973  data: 0.0003  max mem: 5511
[08:17:55.640617] Epoch: [45]  [140/781]  eta: 0:02:09  lr: 0.000155  training_loss: 1.6334 (1.6785)  mae_loss: 0.2685 (0.2722)  classification_loss: 1.3502 (1.3903)  loss_mask: 0.0024 (0.0161)  time: 0.1988  data: 0.0003  max mem: 5511
[08:17:59.594307] Epoch: [45]  [160/781]  eta: 0:02:04  lr: 0.000155  training_loss: 1.6752 (1.6809)  mae_loss: 0.2656 (0.2716)  classification_loss: 1.4041 (1.3936)  loss_mask: 0.0020 (0.0157)  time: 0.1976  data: 0.0003  max mem: 5511
[08:18:03.528029] Epoch: [45]  [180/781]  eta: 0:02:00  lr: 0.000155  training_loss: 1.7481 (1.6900)  mae_loss: 0.2749 (0.2723)  classification_loss: 1.4142 (1.3936)  loss_mask: 0.0490 (0.0241)  time: 0.1966  data: 0.0003  max mem: 5511
[08:18:07.469659] Epoch: [45]  [200/781]  eta: 0:01:56  lr: 0.000155  training_loss: 1.7359 (1.6939)  mae_loss: 0.2756 (0.2726)  classification_loss: 1.3915 (1.3936)  loss_mask: 0.0562 (0.0277)  time: 0.1970  data: 0.0002  max mem: 5511
[08:18:11.411878] Epoch: [45]  [220/781]  eta: 0:01:52  lr: 0.000155  training_loss: 1.6808 (1.6935)  mae_loss: 0.2550 (0.2720)  classification_loss: 1.4247 (1.3950)  loss_mask: 0.0091 (0.0265)  time: 0.1969  data: 0.0002  max mem: 5511
[08:18:15.344195] Epoch: [45]  [240/781]  eta: 0:01:47  lr: 0.000155  training_loss: 1.6439 (1.6931)  mae_loss: 0.2682 (0.2718)  classification_loss: 1.3898 (1.3959)  loss_mask: 0.0044 (0.0254)  time: 0.1965  data: 0.0002  max mem: 5511
[08:18:19.285806] Epoch: [45]  [260/781]  eta: 0:01:43  lr: 0.000155  training_loss: 1.6890 (1.6937)  mae_loss: 0.2682 (0.2714)  classification_loss: 1.4322 (1.3962)  loss_mask: 0.0092 (0.0261)  time: 0.1970  data: 0.0002  max mem: 5511
[08:18:23.238325] Epoch: [45]  [280/781]  eta: 0:01:39  lr: 0.000155  training_loss: 1.6512 (1.6901)  mae_loss: 0.2735 (0.2715)  classification_loss: 1.3660 (1.3932)  loss_mask: 0.0090 (0.0254)  time: 0.1975  data: 0.0002  max mem: 5511
[08:18:27.175608] Epoch: [45]  [300/781]  eta: 0:01:35  lr: 0.000155  training_loss: 1.7070 (1.6916)  mae_loss: 0.2776 (0.2717)  classification_loss: 1.4149 (1.3942)  loss_mask: 0.0106 (0.0258)  time: 0.1968  data: 0.0003  max mem: 5511
[08:18:31.153782] Epoch: [45]  [320/781]  eta: 0:01:31  lr: 0.000154  training_loss: 1.7127 (1.6921)  mae_loss: 0.2789 (0.2721)  classification_loss: 1.4163 (1.3953)  loss_mask: 0.0054 (0.0247)  time: 0.1988  data: 0.0002  max mem: 5511
[08:18:35.110028] Epoch: [45]  [340/781]  eta: 0:01:27  lr: 0.000154  training_loss: 1.6387 (1.6916)  mae_loss: 0.2708 (0.2726)  classification_loss: 1.3243 (1.3945)  loss_mask: 0.0121 (0.0245)  time: 0.1977  data: 0.0002  max mem: 5511
[08:18:39.070900] Epoch: [45]  [360/781]  eta: 0:01:23  lr: 0.000154  training_loss: 1.7086 (1.6952)  mae_loss: 0.2794 (0.2732)  classification_loss: 1.3870 (1.3938)  loss_mask: 0.0243 (0.0282)  time: 0.1979  data: 0.0003  max mem: 5511
[08:18:43.019841] Epoch: [45]  [380/781]  eta: 0:01:19  lr: 0.000154  training_loss: 1.6832 (1.6956)  mae_loss: 0.2716 (0.2734)  classification_loss: 1.3269 (1.3926)  loss_mask: 0.0381 (0.0297)  time: 0.1974  data: 0.0002  max mem: 5511
[08:18:46.951570] Epoch: [45]  [400/781]  eta: 0:01:15  lr: 0.000154  training_loss: 1.6434 (1.6940)  mae_loss: 0.2801 (0.2735)  classification_loss: 1.3512 (1.3908)  loss_mask: 0.0212 (0.0297)  time: 0.1965  data: 0.0002  max mem: 5511
[08:18:50.907009] Epoch: [45]  [420/781]  eta: 0:01:11  lr: 0.000154  training_loss: 1.7250 (1.6955)  mae_loss: 0.2829 (0.2738)  classification_loss: 1.3808 (1.3915)  loss_mask: 0.0301 (0.0303)  time: 0.1977  data: 0.0002  max mem: 5511
[08:18:54.852181] Epoch: [45]  [440/781]  eta: 0:01:07  lr: 0.000154  training_loss: 1.7310 (1.6970)  mae_loss: 0.2582 (0.2737)  classification_loss: 1.3949 (1.3924)  loss_mask: 0.0186 (0.0309)  time: 0.1972  data: 0.0002  max mem: 5511
[08:18:58.806897] Epoch: [45]  [460/781]  eta: 0:01:03  lr: 0.000154  training_loss: 1.6566 (1.6948)  mae_loss: 0.2559 (0.2730)  classification_loss: 1.3680 (1.3918)  loss_mask: 0.0069 (0.0301)  time: 0.1976  data: 0.0002  max mem: 5511
[08:19:02.761609] Epoch: [45]  [480/781]  eta: 0:00:59  lr: 0.000154  training_loss: 1.6426 (1.6929)  mae_loss: 0.2610 (0.2727)  classification_loss: 1.3768 (1.3911)  loss_mask: 0.0038 (0.0290)  time: 0.1976  data: 0.0002  max mem: 5511
[08:19:06.702414] Epoch: [45]  [500/781]  eta: 0:00:55  lr: 0.000154  training_loss: 1.6430 (1.6921)  mae_loss: 0.2730 (0.2726)  classification_loss: 1.3736 (1.3915)  loss_mask: 0.0026 (0.0280)  time: 0.1969  data: 0.0002  max mem: 5511
[08:19:10.640028] Epoch: [45]  [520/781]  eta: 0:00:51  lr: 0.000153  training_loss: 1.6469 (1.6906)  mae_loss: 0.2790 (0.2730)  classification_loss: 1.3725 (1.3905)  loss_mask: 0.0025 (0.0271)  time: 0.1968  data: 0.0003  max mem: 5511
[08:19:14.601874] Epoch: [45]  [540/781]  eta: 0:00:47  lr: 0.000153  training_loss: 1.6957 (1.6906)  mae_loss: 0.2699 (0.2729)  classification_loss: 1.4254 (1.3913)  loss_mask: 0.0042 (0.0264)  time: 0.1980  data: 0.0003  max mem: 5511
[08:19:18.537993] Epoch: [45]  [560/781]  eta: 0:00:43  lr: 0.000153  training_loss: 1.6663 (1.6902)  mae_loss: 0.2558 (0.2725)  classification_loss: 1.4163 (1.3919)  loss_mask: 0.0015 (0.0257)  time: 0.1967  data: 0.0003  max mem: 5511
[08:19:22.471253] Epoch: [45]  [580/781]  eta: 0:00:39  lr: 0.000153  training_loss: 1.6409 (1.6892)  mae_loss: 0.2682 (0.2724)  classification_loss: 1.3678 (1.3917)  loss_mask: 0.0030 (0.0251)  time: 0.1966  data: 0.0002  max mem: 5511
[08:19:26.377017] Epoch: [45]  [600/781]  eta: 0:00:35  lr: 0.000153  training_loss: 1.6922 (1.6895)  mae_loss: 0.2705 (0.2722)  classification_loss: 1.3969 (1.3912)  loss_mask: 0.0162 (0.0261)  time: 0.1952  data: 0.0002  max mem: 5511
[08:19:30.333754] Epoch: [45]  [620/781]  eta: 0:00:31  lr: 0.000153  training_loss: 1.7497 (1.6914)  mae_loss: 0.2950 (0.2727)  classification_loss: 1.4147 (1.3924)  loss_mask: 0.0207 (0.0262)  time: 0.1978  data: 0.0002  max mem: 5511
[08:19:34.284380] Epoch: [45]  [640/781]  eta: 0:00:27  lr: 0.000153  training_loss: 1.6978 (1.6908)  mae_loss: 0.2601 (0.2725)  classification_loss: 1.3807 (1.3923)  loss_mask: 0.0142 (0.0260)  time: 0.1975  data: 0.0002  max mem: 5511
[08:19:38.228523] Epoch: [45]  [660/781]  eta: 0:00:23  lr: 0.000153  training_loss: 1.7117 (1.6907)  mae_loss: 0.2686 (0.2725)  classification_loss: 1.4243 (1.3927)  loss_mask: 0.0062 (0.0255)  time: 0.1971  data: 0.0002  max mem: 5511
[08:19:42.204056] Epoch: [45]  [680/781]  eta: 0:00:20  lr: 0.000153  training_loss: 1.6857 (1.6910)  mae_loss: 0.2720 (0.2728)  classification_loss: 1.3983 (1.3930)  loss_mask: 0.0077 (0.0252)  time: 0.1987  data: 0.0002  max mem: 5511
[08:19:46.151235] Epoch: [45]  [700/781]  eta: 0:00:16  lr: 0.000152  training_loss: 1.7678 (1.6926)  mae_loss: 0.2594 (0.2726)  classification_loss: 1.4695 (1.3944)  loss_mask: 0.0342 (0.0257)  time: 0.1973  data: 0.0004  max mem: 5511
[08:19:50.120628] Epoch: [45]  [720/781]  eta: 0:00:12  lr: 0.000152  training_loss: 1.7166 (1.6938)  mae_loss: 0.2753 (0.2727)  classification_loss: 1.4274 (1.3949)  loss_mask: 0.0200 (0.0262)  time: 0.1984  data: 0.0002  max mem: 5511
[08:19:54.077687] Epoch: [45]  [740/781]  eta: 0:00:08  lr: 0.000152  training_loss: 1.6952 (1.6941)  mae_loss: 0.2543 (0.2722)  classification_loss: 1.3742 (1.3950)  loss_mask: 0.0321 (0.0270)  time: 0.1978  data: 0.0002  max mem: 5511
[08:19:58.048867] Epoch: [45]  [760/781]  eta: 0:00:04  lr: 0.000152  training_loss: 1.7174 (1.6957)  mae_loss: 0.2582 (0.2721)  classification_loss: 1.4136 (1.3958)  loss_mask: 0.0353 (0.0278)  time: 0.1985  data: 0.0002  max mem: 5511
[08:20:02.004966] Epoch: [45]  [780/781]  eta: 0:00:00  lr: 0.000152  training_loss: 1.6635 (1.6951)  mae_loss: 0.2475 (0.2718)  classification_loss: 1.3841 (1.3958)  loss_mask: 0.0113 (0.0275)  time: 0.1977  data: 0.0002  max mem: 5511
[08:20:02.192331] Epoch: [45] Total time: 0:02:34 (0.1984 s / it)
[08:20:02.192795] Averaged stats: lr: 0.000152  training_loss: 1.6635 (1.6951)  mae_loss: 0.2475 (0.2718)  classification_loss: 1.3841 (1.3958)  loss_mask: 0.0113 (0.0275)
[08:20:02.769885] Test:  [  0/157]  eta: 0:01:29  testing_loss: 0.6478 (0.6478)  acc1: 82.8125 (82.8125)  acc5: 96.8750 (96.8750)  time: 0.5725  data: 0.5415  max mem: 5511
[08:20:03.064197] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.7256 (0.7065)  acc1: 78.1250 (77.1307)  acc5: 98.4375 (98.7216)  time: 0.0785  data: 0.0494  max mem: 5511
[08:20:03.348906] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.6921 (0.6797)  acc1: 78.1250 (77.7530)  acc5: 98.4375 (98.7351)  time: 0.0287  data: 0.0002  max mem: 5511
[08:20:03.634371] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.6575 (0.6901)  acc1: 76.5625 (77.4194)  acc5: 98.4375 (98.5887)  time: 0.0284  data: 0.0001  max mem: 5511
[08:20:03.922332] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.7098 (0.7006)  acc1: 76.5625 (77.1341)  acc5: 98.4375 (98.5137)  time: 0.0285  data: 0.0002  max mem: 5511
[08:20:04.208166] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.7098 (0.6902)  acc1: 78.1250 (77.9412)  acc5: 98.4375 (98.6213)  time: 0.0286  data: 0.0002  max mem: 5511
[08:20:04.501214] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6395 (0.6854)  acc1: 79.6875 (77.9201)  acc5: 98.4375 (98.6680)  time: 0.0288  data: 0.0002  max mem: 5511
[08:20:04.785258] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6361 (0.6796)  acc1: 78.1250 (78.0810)  acc5: 98.4375 (98.7016)  time: 0.0287  data: 0.0002  max mem: 5511
[08:20:05.069506] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6873 (0.6886)  acc1: 76.5625 (77.8164)  acc5: 98.4375 (98.6690)  time: 0.0282  data: 0.0002  max mem: 5511
[08:20:05.354656] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6873 (0.6876)  acc1: 76.5625 (77.9876)  acc5: 98.4375 (98.6435)  time: 0.0283  data: 0.0002  max mem: 5511
[08:20:05.639534] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.6731 (0.6922)  acc1: 78.1250 (77.8775)  acc5: 98.4375 (98.5613)  time: 0.0283  data: 0.0002  max mem: 5511
[08:20:05.928913] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.7273 (0.6933)  acc1: 76.5625 (77.7872)  acc5: 98.4375 (98.5360)  time: 0.0285  data: 0.0002  max mem: 5511
[08:20:06.221453] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6839 (0.6912)  acc1: 76.5625 (77.8796)  acc5: 98.4375 (98.5666)  time: 0.0290  data: 0.0002  max mem: 5511
[08:20:06.509121] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6839 (0.6912)  acc1: 76.5625 (77.8507)  acc5: 98.4375 (98.5926)  time: 0.0289  data: 0.0002  max mem: 5511
[08:20:06.797373] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6838 (0.6905)  acc1: 78.1250 (77.9366)  acc5: 98.4375 (98.5483)  time: 0.0286  data: 0.0002  max mem: 5511
[08:20:07.080470] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6688 (0.6872)  acc1: 78.1250 (77.9491)  acc5: 98.4375 (98.5617)  time: 0.0284  data: 0.0002  max mem: 5511
[08:20:07.231657] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6661 (0.6874)  acc1: 78.1250 (77.9500)  acc5: 98.4375 (98.5500)  time: 0.0273  data: 0.0001  max mem: 5511
[08:20:07.400443] Test: Total time: 0:00:05 (0.0331 s / it)
[08:20:07.400949] * Acc@1 77.950 Acc@5 98.550 loss 0.687
[08:20:07.401290] Accuracy of the network on the 10000 test images: 78.0%
[08:20:07.401508] Max accuracy: 77.95%
[08:20:07.713300] log_dir: ./output_dir
[08:20:08.557247] Epoch: [46]  [  0/781]  eta: 0:10:57  lr: 0.000152  training_loss: 1.4136 (1.4136)  mae_loss: 0.2300 (0.2300)  classification_loss: 1.1786 (1.1786)  loss_mask: 0.0050 (0.0050)  time: 0.8419  data: 0.6115  max mem: 5511
[08:20:12.518275] Epoch: [46]  [ 20/781]  eta: 0:02:53  lr: 0.000152  training_loss: 1.6125 (1.6113)  mae_loss: 0.2669 (0.2684)  classification_loss: 1.3177 (1.3336)  loss_mask: 0.0063 (0.0093)  time: 0.1980  data: 0.0002  max mem: 5511
[08:20:16.470369] Epoch: [46]  [ 40/781]  eta: 0:02:38  lr: 0.000152  training_loss: 1.6399 (1.6329)  mae_loss: 0.2772 (0.2707)  classification_loss: 1.3557 (1.3560)  loss_mask: 0.0030 (0.0062)  time: 0.1975  data: 0.0002  max mem: 5511
[08:20:20.446586] Epoch: [46]  [ 60/781]  eta: 0:02:30  lr: 0.000152  training_loss: 1.6633 (1.6466)  mae_loss: 0.2598 (0.2680)  classification_loss: 1.3917 (1.3738)  loss_mask: 0.0017 (0.0048)  time: 0.1987  data: 0.0002  max mem: 5511
[08:20:24.382531] Epoch: [46]  [ 80/781]  eta: 0:02:24  lr: 0.000152  training_loss: 1.6403 (1.6478)  mae_loss: 0.2684 (0.2672)  classification_loss: 1.3558 (1.3759)  loss_mask: 0.0013 (0.0047)  time: 0.1967  data: 0.0003  max mem: 5511
[08:20:28.312504] Epoch: [46]  [100/781]  eta: 0:02:18  lr: 0.000152  training_loss: 1.6558 (1.6508)  mae_loss: 0.2706 (0.2667)  classification_loss: 1.4063 (1.3796)  loss_mask: 0.0021 (0.0045)  time: 0.1964  data: 0.0003  max mem: 5511
[08:20:32.259848] Epoch: [46]  [120/781]  eta: 0:02:14  lr: 0.000151  training_loss: 1.6528 (1.6493)  mae_loss: 0.2642 (0.2675)  classification_loss: 1.4075 (1.3779)  loss_mask: 0.0007 (0.0039)  time: 0.1973  data: 0.0002  max mem: 5511
[08:20:36.203840] Epoch: [46]  [140/781]  eta: 0:02:09  lr: 0.000151  training_loss: 1.6706 (1.6531)  mae_loss: 0.2731 (0.2689)  classification_loss: 1.3906 (1.3805)  loss_mask: 0.0010 (0.0037)  time: 0.1971  data: 0.0002  max mem: 5511
[08:20:40.137565] Epoch: [46]  [160/781]  eta: 0:02:04  lr: 0.000151  training_loss: 1.6544 (1.6520)  mae_loss: 0.2586 (0.2680)  classification_loss: 1.3824 (1.3807)  loss_mask: 0.0013 (0.0034)  time: 0.1966  data: 0.0003  max mem: 5511
[08:20:44.190225] Epoch: [46]  [180/781]  eta: 0:02:01  lr: 0.000151  training_loss: 1.6335 (1.6536)  mae_loss: 0.2705 (0.2687)  classification_loss: 1.3790 (1.3817)  loss_mask: 0.0009 (0.0032)  time: 0.2026  data: 0.0002  max mem: 5511
[08:20:48.151588] Epoch: [46]  [200/781]  eta: 0:01:56  lr: 0.000151  training_loss: 1.6678 (1.6553)  mae_loss: 0.2597 (0.2679)  classification_loss: 1.4157 (1.3839)  loss_mask: 0.0006 (0.0035)  time: 0.1980  data: 0.0003  max mem: 5511
[08:20:52.105296] Epoch: [46]  [220/781]  eta: 0:01:52  lr: 0.000151  training_loss: 1.6246 (1.6555)  mae_loss: 0.2616 (0.2674)  classification_loss: 1.3656 (1.3846)  loss_mask: 0.0007 (0.0035)  time: 0.1976  data: 0.0002  max mem: 5511
[08:20:56.037858] Epoch: [46]  [240/781]  eta: 0:01:48  lr: 0.000151  training_loss: 1.6803 (1.6573)  mae_loss: 0.2705 (0.2677)  classification_loss: 1.4080 (1.3862)  loss_mask: 0.0010 (0.0034)  time: 0.1965  data: 0.0002  max mem: 5511
[08:20:59.986044] Epoch: [46]  [260/781]  eta: 0:01:44  lr: 0.000151  training_loss: 1.6770 (1.6571)  mae_loss: 0.2702 (0.2677)  classification_loss: 1.3990 (1.3861)  loss_mask: 0.0006 (0.0032)  time: 0.1972  data: 0.0002  max mem: 5511
[08:21:03.908622] Epoch: [46]  [280/781]  eta: 0:01:40  lr: 0.000151  training_loss: 1.6445 (1.6550)  mae_loss: 0.2630 (0.2679)  classification_loss: 1.3804 (1.3841)  loss_mask: 0.0009 (0.0030)  time: 0.1960  data: 0.0002  max mem: 5511
[08:21:07.862654] Epoch: [46]  [300/781]  eta: 0:01:36  lr: 0.000151  training_loss: 1.5984 (1.6532)  mae_loss: 0.2749 (0.2688)  classification_loss: 1.3255 (1.3812)  loss_mask: 0.0007 (0.0032)  time: 0.1976  data: 0.0003  max mem: 5511
[08:21:11.801585] Epoch: [46]  [320/781]  eta: 0:01:31  lr: 0.000150  training_loss: 1.6465 (1.6536)  mae_loss: 0.2760 (0.2688)  classification_loss: 1.3648 (1.3817)  loss_mask: 0.0017 (0.0031)  time: 0.1969  data: 0.0002  max mem: 5511
[08:21:15.736640] Epoch: [46]  [340/781]  eta: 0:01:27  lr: 0.000150  training_loss: 1.6511 (1.6536)  mae_loss: 0.2738 (0.2694)  classification_loss: 1.3658 (1.3812)  loss_mask: 0.0005 (0.0030)  time: 0.1967  data: 0.0002  max mem: 5511
[08:21:19.678851] Epoch: [46]  [360/781]  eta: 0:01:23  lr: 0.000150  training_loss: 1.7020 (1.6562)  mae_loss: 0.2615 (0.2692)  classification_loss: 1.4143 (1.3842)  loss_mask: 0.0004 (0.0029)  time: 0.1970  data: 0.0002  max mem: 5511
[08:21:23.630460] Epoch: [46]  [380/781]  eta: 0:01:19  lr: 0.000150  training_loss: 1.6297 (1.6569)  mae_loss: 0.2613 (0.2695)  classification_loss: 1.3535 (1.3846)  loss_mask: 0.0006 (0.0028)  time: 0.1975  data: 0.0003  max mem: 5511
[08:21:27.578701] Epoch: [46]  [400/781]  eta: 0:01:15  lr: 0.000150  training_loss: 1.6750 (1.6569)  mae_loss: 0.2721 (0.2695)  classification_loss: 1.3839 (1.3845)  loss_mask: 0.0006 (0.0029)  time: 0.1973  data: 0.0003  max mem: 5511
[08:21:31.547188] Epoch: [46]  [420/781]  eta: 0:01:11  lr: 0.000150  training_loss: 1.6595 (1.6591)  mae_loss: 0.2726 (0.2696)  classification_loss: 1.3742 (1.3839)  loss_mask: 0.0195 (0.0056)  time: 0.1984  data: 0.0002  max mem: 5511
[08:21:35.507080] Epoch: [46]  [440/781]  eta: 0:01:07  lr: 0.000150  training_loss: 1.7469 (1.6620)  mae_loss: 0.2710 (0.2699)  classification_loss: 1.3958 (1.3839)  loss_mask: 0.0436 (0.0081)  time: 0.1979  data: 0.0003  max mem: 5511
[08:21:39.462784] Epoch: [46]  [460/781]  eta: 0:01:03  lr: 0.000150  training_loss: 1.6417 (1.6621)  mae_loss: 0.2586 (0.2696)  classification_loss: 1.3656 (1.3836)  loss_mask: 0.0230 (0.0089)  time: 0.1976  data: 0.0002  max mem: 5511
[08:21:43.406975] Epoch: [46]  [480/781]  eta: 0:00:59  lr: 0.000150  training_loss: 1.6923 (1.6642)  mae_loss: 0.2639 (0.2694)  classification_loss: 1.4291 (1.3855)  loss_mask: 0.0133 (0.0093)  time: 0.1971  data: 0.0002  max mem: 5511
[08:21:47.370974] Epoch: [46]  [500/781]  eta: 0:00:55  lr: 0.000149  training_loss: 1.7455 (1.6719)  mae_loss: 0.2647 (0.2690)  classification_loss: 1.3586 (1.3860)  loss_mask: 0.0384 (0.0170)  time: 0.1981  data: 0.0003  max mem: 5511
[08:21:51.321280] Epoch: [46]  [520/781]  eta: 0:00:51  lr: 0.000149  training_loss: 1.8074 (1.6778)  mae_loss: 0.2824 (0.2695)  classification_loss: 1.3746 (1.3863)  loss_mask: 0.1039 (0.0221)  time: 0.1974  data: 0.0002  max mem: 5511
[08:21:55.279084] Epoch: [46]  [540/781]  eta: 0:00:47  lr: 0.000149  training_loss: 1.7541 (1.6803)  mae_loss: 0.2699 (0.2698)  classification_loss: 1.4241 (1.3874)  loss_mask: 0.0384 (0.0231)  time: 0.1978  data: 0.0002  max mem: 5511
[08:21:59.260249] Epoch: [46]  [560/781]  eta: 0:00:43  lr: 0.000149  training_loss: 1.7158 (1.6813)  mae_loss: 0.2610 (0.2697)  classification_loss: 1.3557 (1.3873)  loss_mask: 0.0351 (0.0243)  time: 0.1990  data: 0.0003  max mem: 5511
[08:22:03.197738] Epoch: [46]  [580/781]  eta: 0:00:39  lr: 0.000149  training_loss: 1.7410 (1.6838)  mae_loss: 0.2734 (0.2700)  classification_loss: 1.4327 (1.3890)  loss_mask: 0.0314 (0.0248)  time: 0.1968  data: 0.0002  max mem: 5511
[08:22:07.135855] Epoch: [46]  [600/781]  eta: 0:00:35  lr: 0.000149  training_loss: 1.7657 (1.6864)  mae_loss: 0.2661 (0.2700)  classification_loss: 1.4575 (1.3911)  loss_mask: 0.0252 (0.0254)  time: 0.1968  data: 0.0002  max mem: 5511
[08:22:11.127586] Epoch: [46]  [620/781]  eta: 0:00:31  lr: 0.000149  training_loss: 1.8095 (1.6911)  mae_loss: 0.2587 (0.2702)  classification_loss: 1.4689 (1.3932)  loss_mask: 0.0637 (0.0277)  time: 0.1995  data: 0.0003  max mem: 5511
[08:22:15.099508] Epoch: [46]  [640/781]  eta: 0:00:28  lr: 0.000149  training_loss: 1.8045 (1.6945)  mae_loss: 0.2654 (0.2701)  classification_loss: 1.4357 (1.3948)  loss_mask: 0.0564 (0.0296)  time: 0.1985  data: 0.0002  max mem: 5511
[08:22:19.077378] Epoch: [46]  [660/781]  eta: 0:00:24  lr: 0.000149  training_loss: 1.7476 (1.6965)  mae_loss: 0.2688 (0.2702)  classification_loss: 1.4098 (1.3958)  loss_mask: 0.0345 (0.0305)  time: 0.1988  data: 0.0003  max mem: 5511
[08:22:23.059881] Epoch: [46]  [680/781]  eta: 0:00:20  lr: 0.000149  training_loss: 1.7317 (1.6987)  mae_loss: 0.2601 (0.2703)  classification_loss: 1.4394 (1.3974)  loss_mask: 0.0326 (0.0311)  time: 0.1990  data: 0.0003  max mem: 5511
[08:22:27.019679] Epoch: [46]  [700/781]  eta: 0:00:16  lr: 0.000148  training_loss: 1.6806 (1.6986)  mae_loss: 0.2640 (0.2703)  classification_loss: 1.3889 (1.3977)  loss_mask: 0.0161 (0.0307)  time: 0.1979  data: 0.0002  max mem: 5511
[08:22:30.980604] Epoch: [46]  [720/781]  eta: 0:00:12  lr: 0.000148  training_loss: 1.7197 (1.6995)  mae_loss: 0.2689 (0.2704)  classification_loss: 1.4541 (1.3989)  loss_mask: 0.0074 (0.0302)  time: 0.1980  data: 0.0002  max mem: 5511
[08:22:34.914547] Epoch: [46]  [740/781]  eta: 0:00:08  lr: 0.000148  training_loss: 1.6559 (1.6989)  mae_loss: 0.2642 (0.2703)  classification_loss: 1.4002 (1.3989)  loss_mask: 0.0072 (0.0297)  time: 0.1966  data: 0.0002  max mem: 5511
[08:22:38.863824] Epoch: [46]  [760/781]  eta: 0:00:04  lr: 0.000148  training_loss: 1.7120 (1.6998)  mae_loss: 0.2563 (0.2701)  classification_loss: 1.4724 (1.4004)  loss_mask: 0.0082 (0.0293)  time: 0.1974  data: 0.0002  max mem: 5511
[08:22:42.796775] Epoch: [46]  [780/781]  eta: 0:00:00  lr: 0.000148  training_loss: 1.6874 (1.7006)  mae_loss: 0.2664 (0.2700)  classification_loss: 1.3975 (1.4013)  loss_mask: 0.0229 (0.0293)  time: 0.1966  data: 0.0002  max mem: 5511
[08:22:42.949830] Epoch: [46] Total time: 0:02:35 (0.1988 s / it)
[08:22:42.950366] Averaged stats: lr: 0.000148  training_loss: 1.6874 (1.7006)  mae_loss: 0.2664 (0.2700)  classification_loss: 1.3975 (1.4013)  loss_mask: 0.0229 (0.0293)
[08:22:43.573785] Test:  [  0/157]  eta: 0:01:37  testing_loss: 0.6558 (0.6558)  acc1: 82.8125 (82.8125)  acc5: 96.8750 (96.8750)  time: 0.6187  data: 0.5567  max mem: 5511
[08:22:43.870436] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.7105 (0.7154)  acc1: 76.5625 (75.8523)  acc5: 98.4375 (98.7216)  time: 0.0830  data: 0.0508  max mem: 5511
[08:22:44.161758] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.6707 (0.6835)  acc1: 76.5625 (78.1994)  acc5: 98.4375 (98.7351)  time: 0.0292  data: 0.0003  max mem: 5511
[08:22:44.467923] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6725 (0.6955)  acc1: 79.6875 (77.9738)  acc5: 98.4375 (98.6391)  time: 0.0297  data: 0.0003  max mem: 5511
[08:22:44.761985] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6967 (0.7018)  acc1: 78.1250 (77.4771)  acc5: 98.4375 (98.5518)  time: 0.0299  data: 0.0002  max mem: 5511
[08:22:45.053667] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6920 (0.6953)  acc1: 76.5625 (77.9105)  acc5: 98.4375 (98.4988)  time: 0.0291  data: 0.0003  max mem: 5511
[08:22:45.350646] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6522 (0.6900)  acc1: 79.6875 (78.2275)  acc5: 100.0000 (98.5143)  time: 0.0293  data: 0.0003  max mem: 5511
[08:22:45.633915] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6366 (0.6845)  acc1: 79.6875 (78.4331)  acc5: 98.4375 (98.5695)  time: 0.0289  data: 0.0002  max mem: 5511
[08:22:45.921806] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7063 (0.6934)  acc1: 78.1250 (78.2407)  acc5: 98.4375 (98.4954)  time: 0.0284  data: 0.0002  max mem: 5511
[08:22:46.213614] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7475 (0.6943)  acc1: 78.1250 (78.2624)  acc5: 98.4375 (98.4718)  time: 0.0289  data: 0.0002  max mem: 5511
[08:22:46.495880] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.7257 (0.7001)  acc1: 78.1250 (78.0322)  acc5: 98.4375 (98.4530)  time: 0.0286  data: 0.0002  max mem: 5511
[08:22:46.786228] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.7287 (0.7030)  acc1: 75.0000 (77.8575)  acc5: 98.4375 (98.4234)  time: 0.0285  data: 0.0002  max mem: 5511
[08:22:47.076567] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6704 (0.6978)  acc1: 78.1250 (78.0346)  acc5: 98.4375 (98.4892)  time: 0.0289  data: 0.0002  max mem: 5511
[08:22:47.364380] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6641 (0.6997)  acc1: 78.1250 (77.9222)  acc5: 98.4375 (98.5448)  time: 0.0288  data: 0.0002  max mem: 5511
[08:22:47.651287] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6900 (0.6980)  acc1: 78.1250 (78.0363)  acc5: 98.4375 (98.5483)  time: 0.0286  data: 0.0002  max mem: 5511
[08:22:47.937913] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6799 (0.6964)  acc1: 79.6875 (78.1043)  acc5: 98.4375 (98.5824)  time: 0.0285  data: 0.0002  max mem: 5511
[08:22:48.092082] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6624 (0.6963)  acc1: 79.6875 (78.0600)  acc5: 98.4375 (98.5700)  time: 0.0276  data: 0.0002  max mem: 5511
[08:22:48.274172] Test: Total time: 0:00:05 (0.0339 s / it)
[08:22:48.274637] * Acc@1 78.060 Acc@5 98.570 loss 0.696
[08:22:48.274932] Accuracy of the network on the 10000 test images: 78.1%
[08:22:48.275188] Max accuracy: 78.06%
[08:22:48.556570] log_dir: ./output_dir
[08:22:49.341096] Epoch: [47]  [  0/781]  eta: 0:10:11  lr: 0.000148  training_loss: 1.6302 (1.6302)  mae_loss: 0.2699 (0.2699)  classification_loss: 1.3514 (1.3514)  loss_mask: 0.0089 (0.0089)  time: 0.7826  data: 0.5676  max mem: 5511
[08:22:53.283419] Epoch: [47]  [ 20/781]  eta: 0:02:51  lr: 0.000148  training_loss: 1.6454 (1.6697)  mae_loss: 0.2590 (0.2633)  classification_loss: 1.3596 (1.3790)  loss_mask: 0.0158 (0.0274)  time: 0.1970  data: 0.0002  max mem: 5511
[08:22:57.223960] Epoch: [47]  [ 40/781]  eta: 0:02:36  lr: 0.000148  training_loss: 1.6917 (1.6905)  mae_loss: 0.2677 (0.2652)  classification_loss: 1.4053 (1.3973)  loss_mask: 0.0187 (0.0280)  time: 0.1969  data: 0.0003  max mem: 5511
[08:23:01.219257] Epoch: [47]  [ 60/781]  eta: 0:02:29  lr: 0.000148  training_loss: 1.6790 (1.6966)  mae_loss: 0.2781 (0.2697)  classification_loss: 1.3819 (1.4063)  loss_mask: 0.0042 (0.0206)  time: 0.1997  data: 0.0003  max mem: 5511
[08:23:05.187549] Epoch: [47]  [ 80/781]  eta: 0:02:23  lr: 0.000148  training_loss: 1.6555 (1.6852)  mae_loss: 0.2628 (0.2682)  classification_loss: 1.3884 (1.3992)  loss_mask: 0.0059 (0.0178)  time: 0.1983  data: 0.0002  max mem: 5511
[08:23:09.131783] Epoch: [47]  [100/781]  eta: 0:02:18  lr: 0.000148  training_loss: 1.8206 (1.7232)  mae_loss: 0.2608 (0.2688)  classification_loss: 1.4144 (1.4015)  loss_mask: 0.1189 (0.0528)  time: 0.1971  data: 0.0003  max mem: 5511
[08:23:13.117315] Epoch: [47]  [120/781]  eta: 0:02:14  lr: 0.000147  training_loss: 1.7285 (1.7256)  mae_loss: 0.2633 (0.2690)  classification_loss: 1.3842 (1.3950)  loss_mask: 0.0900 (0.0616)  time: 0.1992  data: 0.0002  max mem: 5511
[08:23:17.062580] Epoch: [47]  [140/781]  eta: 0:02:09  lr: 0.000147  training_loss: 1.6638 (1.7176)  mae_loss: 0.2690 (0.2697)  classification_loss: 1.3624 (1.3890)  loss_mask: 0.0329 (0.0589)  time: 0.1972  data: 0.0002  max mem: 5511
[08:23:21.004207] Epoch: [47]  [160/781]  eta: 0:02:05  lr: 0.000147  training_loss: 1.6683 (1.7140)  mae_loss: 0.2651 (0.2686)  classification_loss: 1.3906 (1.3915)  loss_mask: 0.0150 (0.0539)  time: 0.1970  data: 0.0002  max mem: 5511
[08:23:24.937525] Epoch: [47]  [180/781]  eta: 0:02:00  lr: 0.000147  training_loss: 1.6658 (1.7129)  mae_loss: 0.2697 (0.2685)  classification_loss: 1.3647 (1.3948)  loss_mask: 0.0119 (0.0496)  time: 0.1966  data: 0.0002  max mem: 5511
[08:23:28.882881] Epoch: [47]  [200/781]  eta: 0:01:56  lr: 0.000147  training_loss: 1.6471 (1.7076)  mae_loss: 0.2611 (0.2681)  classification_loss: 1.3426 (1.3929)  loss_mask: 0.0116 (0.0466)  time: 0.1971  data: 0.0002  max mem: 5511
[08:23:32.825362] Epoch: [47]  [220/781]  eta: 0:01:52  lr: 0.000147  training_loss: 1.7105 (1.7072)  mae_loss: 0.2602 (0.2681)  classification_loss: 1.4330 (1.3951)  loss_mask: 0.0145 (0.0440)  time: 0.1970  data: 0.0002  max mem: 5511
[08:23:36.797661] Epoch: [47]  [240/781]  eta: 0:01:48  lr: 0.000147  training_loss: 1.6757 (1.7071)  mae_loss: 0.2718 (0.2682)  classification_loss: 1.3745 (1.3968)  loss_mask: 0.0137 (0.0421)  time: 0.1985  data: 0.0002  max mem: 5511
[08:23:40.770782] Epoch: [47]  [260/781]  eta: 0:01:44  lr: 0.000147  training_loss: 1.6937 (1.7071)  mae_loss: 0.2746 (0.2688)  classification_loss: 1.3848 (1.3968)  loss_mask: 0.0269 (0.0414)  time: 0.1986  data: 0.0003  max mem: 5511
[08:23:44.730267] Epoch: [47]  [280/781]  eta: 0:01:40  lr: 0.000147  training_loss: 1.6793 (1.7047)  mae_loss: 0.2724 (0.2696)  classification_loss: 1.3930 (1.3949)  loss_mask: 0.0114 (0.0403)  time: 0.1979  data: 0.0003  max mem: 5511
[08:23:48.688708] Epoch: [47]  [300/781]  eta: 0:01:36  lr: 0.000146  training_loss: 1.6998 (1.7067)  mae_loss: 0.2810 (0.2699)  classification_loss: 1.4378 (1.3971)  loss_mask: 0.0145 (0.0397)  time: 0.1978  data: 0.0004  max mem: 5511
[08:23:52.649696] Epoch: [47]  [320/781]  eta: 0:01:32  lr: 0.000146  training_loss: 1.6564 (1.7037)  mae_loss: 0.2666 (0.2704)  classification_loss: 1.3364 (1.3942)  loss_mask: 0.0183 (0.0390)  time: 0.1980  data: 0.0003  max mem: 5511
[08:23:56.590218] Epoch: [47]  [340/781]  eta: 0:01:27  lr: 0.000146  training_loss: 1.6615 (1.7020)  mae_loss: 0.2672 (0.2701)  classification_loss: 1.3911 (1.3932)  loss_mask: 0.0280 (0.0387)  time: 0.1969  data: 0.0002  max mem: 5511
[08:24:00.529598] Epoch: [47]  [360/781]  eta: 0:01:23  lr: 0.000146  training_loss: 1.7360 (1.7036)  mae_loss: 0.2665 (0.2697)  classification_loss: 1.4447 (1.3949)  loss_mask: 0.0341 (0.0390)  time: 0.1969  data: 0.0003  max mem: 5511
[08:24:04.471268] Epoch: [47]  [380/781]  eta: 0:01:19  lr: 0.000146  training_loss: 1.6949 (1.7039)  mae_loss: 0.2813 (0.2703)  classification_loss: 1.3471 (1.3938)  loss_mask: 0.0292 (0.0398)  time: 0.1970  data: 0.0004  max mem: 5511
[08:24:08.398992] Epoch: [47]  [400/781]  eta: 0:01:15  lr: 0.000146  training_loss: 1.6155 (1.7001)  mae_loss: 0.2698 (0.2702)  classification_loss: 1.3311 (1.3913)  loss_mask: 0.0103 (0.0386)  time: 0.1962  data: 0.0002  max mem: 5511
[08:24:12.346136] Epoch: [47]  [420/781]  eta: 0:01:11  lr: 0.000146  training_loss: 1.6759 (1.6986)  mae_loss: 0.2663 (0.2704)  classification_loss: 1.3928 (1.3912)  loss_mask: 0.0030 (0.0370)  time: 0.1972  data: 0.0002  max mem: 5511
[08:24:16.288883] Epoch: [47]  [440/781]  eta: 0:01:07  lr: 0.000146  training_loss: 1.6339 (1.6948)  mae_loss: 0.2610 (0.2700)  classification_loss: 1.3699 (1.3894)  loss_mask: 0.0030 (0.0355)  time: 0.1970  data: 0.0002  max mem: 5511
[08:24:20.222273] Epoch: [47]  [460/781]  eta: 0:01:03  lr: 0.000146  training_loss: 1.6501 (1.6926)  mae_loss: 0.2550 (0.2699)  classification_loss: 1.3869 (1.3887)  loss_mask: 0.0019 (0.0340)  time: 0.1966  data: 0.0001  max mem: 5511
[08:24:24.174170] Epoch: [47]  [480/781]  eta: 0:00:59  lr: 0.000146  training_loss: 1.6674 (1.6923)  mae_loss: 0.2597 (0.2697)  classification_loss: 1.3951 (1.3900)  loss_mask: 0.0014 (0.0327)  time: 0.1975  data: 0.0002  max mem: 5511
[08:24:28.126453] Epoch: [47]  [500/781]  eta: 0:00:55  lr: 0.000145  training_loss: 1.6461 (1.6915)  mae_loss: 0.2633 (0.2697)  classification_loss: 1.3545 (1.3903)  loss_mask: 0.0015 (0.0316)  time: 0.1975  data: 0.0002  max mem: 5511
[08:24:32.077711] Epoch: [47]  [520/781]  eta: 0:00:51  lr: 0.000145  training_loss: 1.6366 (1.6900)  mae_loss: 0.2591 (0.2696)  classification_loss: 1.3471 (1.3900)  loss_mask: 0.0020 (0.0305)  time: 0.1975  data: 0.0003  max mem: 5511
[08:24:36.016215] Epoch: [47]  [540/781]  eta: 0:00:47  lr: 0.000145  training_loss: 1.6708 (1.6894)  mae_loss: 0.2753 (0.2699)  classification_loss: 1.3654 (1.3902)  loss_mask: 0.0011 (0.0294)  time: 0.1968  data: 0.0002  max mem: 5511
[08:24:39.954837] Epoch: [47]  [560/781]  eta: 0:00:43  lr: 0.000145  training_loss: 1.6249 (1.6892)  mae_loss: 0.2529 (0.2695)  classification_loss: 1.3710 (1.3912)  loss_mask: 0.0009 (0.0285)  time: 0.1968  data: 0.0002  max mem: 5511
[08:24:43.906671] Epoch: [47]  [580/781]  eta: 0:00:39  lr: 0.000145  training_loss: 1.6687 (1.6891)  mae_loss: 0.2678 (0.2696)  classification_loss: 1.3960 (1.3920)  loss_mask: 0.0011 (0.0275)  time: 0.1975  data: 0.0002  max mem: 5511
[08:24:47.849502] Epoch: [47]  [600/781]  eta: 0:00:35  lr: 0.000145  training_loss: 1.6491 (1.6876)  mae_loss: 0.2610 (0.2696)  classification_loss: 1.3602 (1.3913)  loss_mask: 0.0008 (0.0267)  time: 0.1971  data: 0.0002  max mem: 5511
[08:24:51.770670] Epoch: [47]  [620/781]  eta: 0:00:31  lr: 0.000145  training_loss: 1.6358 (1.6860)  mae_loss: 0.2834 (0.2701)  classification_loss: 1.3582 (1.3900)  loss_mask: 0.0007 (0.0258)  time: 0.1960  data: 0.0002  max mem: 5511
[08:24:55.723237] Epoch: [47]  [640/781]  eta: 0:00:27  lr: 0.000145  training_loss: 1.6900 (1.6859)  mae_loss: 0.2530 (0.2698)  classification_loss: 1.4313 (1.3908)  loss_mask: 0.0007 (0.0253)  time: 0.1976  data: 0.0002  max mem: 5511
[08:24:59.658401] Epoch: [47]  [660/781]  eta: 0:00:23  lr: 0.000145  training_loss: 1.6309 (1.6849)  mae_loss: 0.2654 (0.2696)  classification_loss: 1.3786 (1.3904)  loss_mask: 0.0029 (0.0248)  time: 0.1967  data: 0.0003  max mem: 5511
[08:25:03.645191] Epoch: [47]  [680/781]  eta: 0:00:20  lr: 0.000144  training_loss: 1.6203 (1.6848)  mae_loss: 0.2622 (0.2696)  classification_loss: 1.3539 (1.3904)  loss_mask: 0.0104 (0.0248)  time: 0.1993  data: 0.0002  max mem: 5511
[08:25:07.593843] Epoch: [47]  [700/781]  eta: 0:00:16  lr: 0.000144  training_loss: 1.6830 (1.6855)  mae_loss: 0.2791 (0.2698)  classification_loss: 1.4076 (1.3915)  loss_mask: 0.0030 (0.0243)  time: 0.1973  data: 0.0002  max mem: 5511
[08:25:11.547398] Epoch: [47]  [720/781]  eta: 0:00:12  lr: 0.000144  training_loss: 1.6550 (1.6844)  mae_loss: 0.2842 (0.2702)  classification_loss: 1.3565 (1.3904)  loss_mask: 0.0015 (0.0237)  time: 0.1976  data: 0.0002  max mem: 5511
[08:25:15.468506] Epoch: [47]  [740/781]  eta: 0:00:08  lr: 0.000144  training_loss: 1.6245 (1.6832)  mae_loss: 0.2679 (0.2702)  classification_loss: 1.3436 (1.3899)  loss_mask: 0.0007 (0.0231)  time: 0.1960  data: 0.0002  max mem: 5511
[08:25:19.426606] Epoch: [47]  [760/781]  eta: 0:00:04  lr: 0.000144  training_loss: 1.6394 (1.6824)  mae_loss: 0.2581 (0.2699)  classification_loss: 1.3876 (1.3900)  loss_mask: 0.0006 (0.0225)  time: 0.1978  data: 0.0004  max mem: 5511
[08:25:23.357997] Epoch: [47]  [780/781]  eta: 0:00:00  lr: 0.000144  training_loss: 1.6737 (1.6812)  mae_loss: 0.2622 (0.2697)  classification_loss: 1.4012 (1.3895)  loss_mask: 0.0005 (0.0219)  time: 0.1965  data: 0.0003  max mem: 5511
[08:25:23.518322] Epoch: [47] Total time: 0:02:34 (0.1984 s / it)
[08:25:23.519024] Averaged stats: lr: 0.000144  training_loss: 1.6737 (1.6812)  mae_loss: 0.2622 (0.2697)  classification_loss: 1.4012 (1.3895)  loss_mask: 0.0005 (0.0219)
[08:25:24.130835] Test:  [  0/157]  eta: 0:01:35  testing_loss: 0.5693 (0.5693)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.6067  data: 0.5726  max mem: 5511
[08:25:24.428601] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.6438 (0.6855)  acc1: 78.1250 (78.6932)  acc5: 98.4375 (98.7216)  time: 0.0820  data: 0.0522  max mem: 5511
[08:25:24.716043] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.6349 (0.6535)  acc1: 79.6875 (79.7619)  acc5: 98.4375 (99.0327)  time: 0.0291  data: 0.0002  max mem: 5511
[08:25:25.003398] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6486 (0.6713)  acc1: 79.6875 (78.5786)  acc5: 98.4375 (98.8407)  time: 0.0286  data: 0.0002  max mem: 5511
[08:25:25.287529] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6596 (0.6693)  acc1: 78.1250 (78.6966)  acc5: 98.4375 (98.8567)  time: 0.0284  data: 0.0002  max mem: 5511
[08:25:25.573987] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6195 (0.6576)  acc1: 79.6875 (79.1667)  acc5: 98.4375 (98.7439)  time: 0.0284  data: 0.0002  max mem: 5511
[08:25:25.857477] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6149 (0.6503)  acc1: 79.6875 (79.3545)  acc5: 100.0000 (98.7705)  time: 0.0284  data: 0.0002  max mem: 5511
[08:25:26.144543] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5955 (0.6440)  acc1: 79.6875 (79.6875)  acc5: 100.0000 (98.7456)  time: 0.0284  data: 0.0002  max mem: 5511
[08:25:26.430250] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6502 (0.6549)  acc1: 79.6875 (79.5139)  acc5: 98.4375 (98.6883)  time: 0.0285  data: 0.0002  max mem: 5511
[08:25:26.722336] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6657 (0.6529)  acc1: 79.6875 (79.4986)  acc5: 98.4375 (98.7122)  time: 0.0287  data: 0.0002  max mem: 5511
[08:25:27.008365] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.6610 (0.6560)  acc1: 79.6875 (79.3472)  acc5: 98.4375 (98.6696)  time: 0.0287  data: 0.0002  max mem: 5511
[08:25:27.293935] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6648 (0.6589)  acc1: 79.6875 (79.3356)  acc5: 98.4375 (98.5783)  time: 0.0284  data: 0.0002  max mem: 5511
[08:25:27.577883] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6081 (0.6535)  acc1: 79.6875 (79.5455)  acc5: 98.4375 (98.6054)  time: 0.0283  data: 0.0002  max mem: 5511
[08:25:27.866056] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6081 (0.6535)  acc1: 79.6875 (79.4728)  acc5: 98.4375 (98.6522)  time: 0.0284  data: 0.0002  max mem: 5511
[08:25:28.150358] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6426 (0.6517)  acc1: 79.6875 (79.5988)  acc5: 100.0000 (98.6924)  time: 0.0284  data: 0.0002  max mem: 5511
[08:25:28.431813] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6500 (0.6504)  acc1: 79.6875 (79.6151)  acc5: 100.0000 (98.7169)  time: 0.0281  data: 0.0001  max mem: 5511
[08:25:28.586248] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6431 (0.6504)  acc1: 81.2500 (79.6000)  acc5: 100.0000 (98.7300)  time: 0.0272  data: 0.0001  max mem: 5511
[08:25:28.759014] Test: Total time: 0:00:05 (0.0334 s / it)
[08:25:28.759452] * Acc@1 79.600 Acc@5 98.730 loss 0.650
[08:25:28.759733] Accuracy of the network on the 10000 test images: 79.6%
[08:25:28.759896] Max accuracy: 79.60%
[08:25:29.031850] log_dir: ./output_dir
[08:25:29.962260] Epoch: [48]  [  0/781]  eta: 0:12:05  lr: 0.000144  training_loss: 1.6489 (1.6489)  mae_loss: 0.2547 (0.2547)  classification_loss: 1.3936 (1.3936)  loss_mask: 0.0006 (0.0006)  time: 0.9285  data: 0.6977  max mem: 5511
[08:25:33.914040] Epoch: [48]  [ 20/781]  eta: 0:02:56  lr: 0.000144  training_loss: 1.6288 (1.6568)  mae_loss: 0.2835 (0.2800)  classification_loss: 1.3333 (1.3730)  loss_mask: 0.0005 (0.0037)  time: 0.1975  data: 0.0002  max mem: 5511
[08:25:37.884980] Epoch: [48]  [ 40/781]  eta: 0:02:39  lr: 0.000144  training_loss: 1.6804 (1.6586)  mae_loss: 0.2594 (0.2705)  classification_loss: 1.4223 (1.3851)  loss_mask: 0.0008 (0.0030)  time: 0.1985  data: 0.0003  max mem: 5511
[08:25:41.883227] Epoch: [48]  [ 60/781]  eta: 0:02:31  lr: 0.000144  training_loss: 1.6738 (1.6611)  mae_loss: 0.2613 (0.2687)  classification_loss: 1.3786 (1.3893)  loss_mask: 0.0016 (0.0030)  time: 0.1998  data: 0.0003  max mem: 5511
[08:25:45.863289] Epoch: [48]  [ 80/781]  eta: 0:02:25  lr: 0.000144  training_loss: 1.6424 (1.6579)  mae_loss: 0.2548 (0.2665)  classification_loss: 1.3588 (1.3863)  loss_mask: 0.0035 (0.0051)  time: 0.1989  data: 0.0003  max mem: 5511
[08:25:49.839395] Epoch: [48]  [100/781]  eta: 0:02:20  lr: 0.000143  training_loss: 1.6294 (1.6564)  mae_loss: 0.2694 (0.2677)  classification_loss: 1.3510 (1.3821)  loss_mask: 0.0027 (0.0066)  time: 0.1987  data: 0.0002  max mem: 5511
[08:25:53.835390] Epoch: [48]  [120/781]  eta: 0:02:15  lr: 0.000143  training_loss: 1.7151 (1.6734)  mae_loss: 0.2698 (0.2680)  classification_loss: 1.3463 (1.3756)  loss_mask: 0.0943 (0.0298)  time: 0.1997  data: 0.0003  max mem: 5511
[08:25:57.840986] Epoch: [48]  [140/781]  eta: 0:02:10  lr: 0.000143  training_loss: 1.6695 (1.6751)  mae_loss: 0.2631 (0.2681)  classification_loss: 1.3343 (1.3707)  loss_mask: 0.0494 (0.0364)  time: 0.2002  data: 0.0002  max mem: 5511
[08:26:01.803343] Epoch: [48]  [160/781]  eta: 0:02:06  lr: 0.000143  training_loss: 1.6304 (1.6704)  mae_loss: 0.2538 (0.2678)  classification_loss: 1.3431 (1.3676)  loss_mask: 0.0175 (0.0350)  time: 0.1980  data: 0.0002  max mem: 5511
[08:26:05.787488] Epoch: [48]  [180/781]  eta: 0:02:01  lr: 0.000143  training_loss: 1.6581 (1.6719)  mae_loss: 0.2606 (0.2669)  classification_loss: 1.3623 (1.3719)  loss_mask: 0.0181 (0.0331)  time: 0.1991  data: 0.0002  max mem: 5511
[08:26:09.762562] Epoch: [48]  [200/781]  eta: 0:01:57  lr: 0.000143  training_loss: 1.6144 (1.6694)  mae_loss: 0.2598 (0.2672)  classification_loss: 1.3753 (1.3715)  loss_mask: 0.0064 (0.0306)  time: 0.1987  data: 0.0003  max mem: 5511
[08:26:13.726270] Epoch: [48]  [220/781]  eta: 0:01:53  lr: 0.000143  training_loss: 1.6596 (1.6681)  mae_loss: 0.2478 (0.2658)  classification_loss: 1.3796 (1.3741)  loss_mask: 0.0019 (0.0282)  time: 0.1981  data: 0.0002  max mem: 5511
[08:26:17.706925] Epoch: [48]  [240/781]  eta: 0:01:49  lr: 0.000143  training_loss: 1.5904 (1.6632)  mae_loss: 0.2619 (0.2653)  classification_loss: 1.3527 (1.3718)  loss_mask: 0.0022 (0.0261)  time: 0.1989  data: 0.0002  max mem: 5511
[08:26:21.641019] Epoch: [48]  [260/781]  eta: 0:01:44  lr: 0.000143  training_loss: 1.6037 (1.6601)  mae_loss: 0.2536 (0.2650)  classification_loss: 1.3569 (1.3709)  loss_mask: 0.0017 (0.0243)  time: 0.1966  data: 0.0003  max mem: 5511
[08:26:25.609300] Epoch: [48]  [280/781]  eta: 0:01:40  lr: 0.000142  training_loss: 1.6766 (1.6598)  mae_loss: 0.2887 (0.2660)  classification_loss: 1.3812 (1.3711)  loss_mask: 0.0017 (0.0227)  time: 0.1983  data: 0.0003  max mem: 5511
[08:26:29.560677] Epoch: [48]  [300/781]  eta: 0:01:36  lr: 0.000142  training_loss: 1.6652 (1.6608)  mae_loss: 0.2614 (0.2660)  classification_loss: 1.3538 (1.3711)  loss_mask: 0.0027 (0.0237)  time: 0.1975  data: 0.0002  max mem: 5511
[08:26:33.500330] Epoch: [48]  [320/781]  eta: 0:01:32  lr: 0.000142  training_loss: 1.6074 (1.6612)  mae_loss: 0.2592 (0.2656)  classification_loss: 1.3433 (1.3712)  loss_mask: 0.0266 (0.0244)  time: 0.1969  data: 0.0002  max mem: 5511
[08:26:37.476959] Epoch: [48]  [340/781]  eta: 0:01:28  lr: 0.000142  training_loss: 1.6669 (1.6614)  mae_loss: 0.2682 (0.2658)  classification_loss: 1.3694 (1.3721)  loss_mask: 0.0073 (0.0235)  time: 0.1987  data: 0.0002  max mem: 5511
[08:26:41.449245] Epoch: [48]  [360/781]  eta: 0:01:24  lr: 0.000142  training_loss: 1.6648 (1.6625)  mae_loss: 0.2588 (0.2659)  classification_loss: 1.4003 (1.3739)  loss_mask: 0.0032 (0.0226)  time: 0.1985  data: 0.0002  max mem: 5511
[08:26:45.377622] Epoch: [48]  [380/781]  eta: 0:01:20  lr: 0.000142  training_loss: 1.6741 (1.6626)  mae_loss: 0.2698 (0.2664)  classification_loss: 1.3777 (1.3737)  loss_mask: 0.0062 (0.0225)  time: 0.1963  data: 0.0002  max mem: 5511
[08:26:49.332028] Epoch: [48]  [400/781]  eta: 0:01:16  lr: 0.000142  training_loss: 1.6747 (1.6644)  mae_loss: 0.2745 (0.2667)  classification_loss: 1.3822 (1.3744)  loss_mask: 0.0265 (0.0233)  time: 0.1976  data: 0.0003  max mem: 5511
[08:26:53.277463] Epoch: [48]  [420/781]  eta: 0:01:12  lr: 0.000142  training_loss: 1.7586 (1.6683)  mae_loss: 0.2641 (0.2667)  classification_loss: 1.4289 (1.3769)  loss_mask: 0.0380 (0.0247)  time: 0.1972  data: 0.0002  max mem: 5511
[08:26:57.205914] Epoch: [48]  [440/781]  eta: 0:01:08  lr: 0.000142  training_loss: 1.6784 (1.6684)  mae_loss: 0.2668 (0.2667)  classification_loss: 1.3878 (1.3773)  loss_mask: 0.0152 (0.0244)  time: 0.1963  data: 0.0003  max mem: 5511
[08:27:01.147253] Epoch: [48]  [460/781]  eta: 0:01:04  lr: 0.000142  training_loss: 1.6076 (1.6657)  mae_loss: 0.2759 (0.2669)  classification_loss: 1.3310 (1.3747)  loss_mask: 0.0086 (0.0242)  time: 0.1970  data: 0.0003  max mem: 5511
[08:27:05.082955] Epoch: [48]  [480/781]  eta: 0:01:00  lr: 0.000141  training_loss: 1.6402 (1.6659)  mae_loss: 0.2507 (0.2664)  classification_loss: 1.3789 (1.3753)  loss_mask: 0.0115 (0.0242)  time: 0.1967  data: 0.0003  max mem: 5511
[08:27:09.037499] Epoch: [48]  [500/781]  eta: 0:00:56  lr: 0.000141  training_loss: 1.6541 (1.6662)  mae_loss: 0.2661 (0.2664)  classification_loss: 1.3623 (1.3761)  loss_mask: 0.0061 (0.0236)  time: 0.1976  data: 0.0003  max mem: 5511
[08:27:12.986361] Epoch: [48]  [520/781]  eta: 0:00:52  lr: 0.000141  training_loss: 1.6879 (1.6678)  mae_loss: 0.2785 (0.2671)  classification_loss: 1.3848 (1.3777)  loss_mask: 0.0019 (0.0230)  time: 0.1974  data: 0.0002  max mem: 5511
[08:27:16.911226] Epoch: [48]  [540/781]  eta: 0:00:48  lr: 0.000141  training_loss: 1.6921 (1.6679)  mae_loss: 0.2808 (0.2678)  classification_loss: 1.3528 (1.3778)  loss_mask: 0.0034 (0.0223)  time: 0.1961  data: 0.0002  max mem: 5511
[08:27:20.837341] Epoch: [48]  [560/781]  eta: 0:00:44  lr: 0.000141  training_loss: 1.6557 (1.6667)  mae_loss: 0.2612 (0.2679)  classification_loss: 1.3843 (1.3769)  loss_mask: 0.0031 (0.0219)  time: 0.1962  data: 0.0003  max mem: 5511
[08:27:24.762739] Epoch: [48]  [580/781]  eta: 0:00:40  lr: 0.000141  training_loss: 1.6403 (1.6671)  mae_loss: 0.2722 (0.2680)  classification_loss: 1.3494 (1.3771)  loss_mask: 0.0030 (0.0220)  time: 0.1962  data: 0.0002  max mem: 5511
[08:27:28.706978] Epoch: [48]  [600/781]  eta: 0:00:36  lr: 0.000141  training_loss: 1.6160 (1.6658)  mae_loss: 0.2440 (0.2674)  classification_loss: 1.3389 (1.3764)  loss_mask: 0.0104 (0.0220)  time: 0.1971  data: 0.0002  max mem: 5511
[08:27:32.659868] Epoch: [48]  [620/781]  eta: 0:00:32  lr: 0.000141  training_loss: 1.6382 (1.6657)  mae_loss: 0.2622 (0.2675)  classification_loss: 1.3654 (1.3767)  loss_mask: 0.0052 (0.0215)  time: 0.1976  data: 0.0002  max mem: 5511
[08:27:36.603586] Epoch: [48]  [640/781]  eta: 0:00:28  lr: 0.000141  training_loss: 1.6075 (1.6647)  mae_loss: 0.2568 (0.2673)  classification_loss: 1.3452 (1.3762)  loss_mask: 0.0031 (0.0211)  time: 0.1971  data: 0.0003  max mem: 5511
[08:27:40.542003] Epoch: [48]  [660/781]  eta: 0:00:24  lr: 0.000141  training_loss: 1.6213 (1.6639)  mae_loss: 0.2560 (0.2670)  classification_loss: 1.3792 (1.3762)  loss_mask: 0.0023 (0.0206)  time: 0.1968  data: 0.0002  max mem: 5511
[08:27:44.524189] Epoch: [48]  [680/781]  eta: 0:00:20  lr: 0.000140  training_loss: 1.6373 (1.6632)  mae_loss: 0.2652 (0.2669)  classification_loss: 1.3766 (1.3763)  loss_mask: 0.0016 (0.0201)  time: 0.1990  data: 0.0002  max mem: 5511
[08:27:48.539141] Epoch: [48]  [700/781]  eta: 0:00:16  lr: 0.000140  training_loss: 1.6263 (1.6631)  mae_loss: 0.2611 (0.2669)  classification_loss: 1.3647 (1.3767)  loss_mask: 0.0007 (0.0196)  time: 0.2007  data: 0.0004  max mem: 5511
[08:27:52.527037] Epoch: [48]  [720/781]  eta: 0:00:12  lr: 0.000140  training_loss: 1.6500 (1.6638)  mae_loss: 0.2631 (0.2669)  classification_loss: 1.3789 (1.3778)  loss_mask: 0.0008 (0.0190)  time: 0.1993  data: 0.0002  max mem: 5511
[08:27:56.488222] Epoch: [48]  [740/781]  eta: 0:00:08  lr: 0.000140  training_loss: 1.6171 (1.6629)  mae_loss: 0.2580 (0.2668)  classification_loss: 1.3595 (1.3776)  loss_mask: 0.0004 (0.0186)  time: 0.1980  data: 0.0002  max mem: 5511
[08:28:00.448675] Epoch: [48]  [760/781]  eta: 0:00:04  lr: 0.000140  training_loss: 1.6685 (1.6626)  mae_loss: 0.2509 (0.2665)  classification_loss: 1.4029 (1.3780)  loss_mask: 0.0005 (0.0181)  time: 0.1979  data: 0.0003  max mem: 5511
[08:28:04.367630] Epoch: [48]  [780/781]  eta: 0:00:00  lr: 0.000140  training_loss: 1.6324 (1.6625)  mae_loss: 0.2607 (0.2663)  classification_loss: 1.3773 (1.3785)  loss_mask: 0.0011 (0.0177)  time: 0.1959  data: 0.0003  max mem: 5511
[08:28:04.543088] Epoch: [48] Total time: 0:02:35 (0.1991 s / it)
[08:28:04.543596] Averaged stats: lr: 0.000140  training_loss: 1.6324 (1.6625)  mae_loss: 0.2607 (0.2663)  classification_loss: 1.3773 (1.3785)  loss_mask: 0.0011 (0.0177)
[08:28:05.160066] Test:  [  0/157]  eta: 0:01:36  testing_loss: 0.6307 (0.6307)  acc1: 85.9375 (85.9375)  acc5: 96.8750 (96.8750)  time: 0.6120  data: 0.5803  max mem: 5511
[08:28:05.451465] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.6458 (0.6591)  acc1: 78.1250 (78.8352)  acc5: 100.0000 (99.0057)  time: 0.0819  data: 0.0529  max mem: 5511
[08:28:05.742792] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.6458 (0.6447)  acc1: 79.6875 (79.3155)  acc5: 100.0000 (99.0327)  time: 0.0289  data: 0.0004  max mem: 5511
[08:28:06.026097] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6452 (0.6578)  acc1: 79.6875 (78.7802)  acc5: 98.4375 (98.7903)  time: 0.0285  data: 0.0004  max mem: 5511
[08:28:06.311260] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6465 (0.6633)  acc1: 78.1250 (78.5061)  acc5: 98.4375 (98.8948)  time: 0.0283  data: 0.0002  max mem: 5511
[08:28:06.593603] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6455 (0.6545)  acc1: 79.6875 (79.1667)  acc5: 100.0000 (98.8358)  time: 0.0283  data: 0.0002  max mem: 5511
[08:28:06.879586] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6128 (0.6499)  acc1: 79.6875 (79.2520)  acc5: 98.4375 (98.8730)  time: 0.0283  data: 0.0002  max mem: 5511
[08:28:07.172098] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6070 (0.6443)  acc1: 79.6875 (79.5995)  acc5: 98.4375 (98.8996)  time: 0.0288  data: 0.0002  max mem: 5511
[08:28:07.463294] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6423 (0.6547)  acc1: 79.6875 (79.2438)  acc5: 98.4375 (98.8426)  time: 0.0290  data: 0.0003  max mem: 5511
[08:28:07.753877] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6743 (0.6518)  acc1: 78.1250 (79.4128)  acc5: 98.4375 (98.9011)  time: 0.0289  data: 0.0002  max mem: 5511
[08:28:08.044172] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.6743 (0.6552)  acc1: 79.6875 (79.3317)  acc5: 100.0000 (98.9171)  time: 0.0289  data: 0.0003  max mem: 5511
[08:28:08.331296] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6905 (0.6578)  acc1: 76.5625 (79.1244)  acc5: 100.0000 (98.9443)  time: 0.0287  data: 0.0002  max mem: 5511
[08:28:08.615653] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6216 (0.6527)  acc1: 78.1250 (79.3259)  acc5: 100.0000 (98.9928)  time: 0.0285  data: 0.0002  max mem: 5511
[08:28:08.900113] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6118 (0.6533)  acc1: 79.6875 (79.3655)  acc5: 100.0000 (99.0219)  time: 0.0283  data: 0.0002  max mem: 5511
[08:28:09.183511] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6487 (0.6520)  acc1: 79.6875 (79.3883)  acc5: 98.4375 (99.0027)  time: 0.0282  data: 0.0002  max mem: 5511
[08:28:09.465307] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6550 (0.6501)  acc1: 79.6875 (79.4185)  acc5: 98.4375 (98.9859)  time: 0.0281  data: 0.0002  max mem: 5511
[08:28:09.616399] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6487 (0.6490)  acc1: 79.6875 (79.3600)  acc5: 98.4375 (98.9900)  time: 0.0271  data: 0.0001  max mem: 5511
[08:28:09.783058] Test: Total time: 0:00:05 (0.0333 s / it)
[08:28:09.783766] * Acc@1 79.360 Acc@5 98.990 loss 0.649
[08:28:09.784060] Accuracy of the network on the 10000 test images: 79.4%
[08:28:09.784248] Max accuracy: 79.60%
[08:28:09.955542] log_dir: ./output_dir
[08:28:10.887627] Epoch: [49]  [  0/781]  eta: 0:12:06  lr: 0.000140  training_loss: 1.6698 (1.6698)  mae_loss: 0.2605 (0.2605)  classification_loss: 1.4089 (1.4089)  loss_mask: 0.0004 (0.0004)  time: 0.9303  data: 0.7162  max mem: 5511
[08:28:14.836373] Epoch: [49]  [ 20/781]  eta: 0:02:56  lr: 0.000140  training_loss: 1.6344 (1.6444)  mae_loss: 0.2497 (0.2649)  classification_loss: 1.3585 (1.3787)  loss_mask: 0.0006 (0.0009)  time: 0.1973  data: 0.0003  max mem: 5511
[08:28:18.778629] Epoch: [49]  [ 40/781]  eta: 0:02:39  lr: 0.000140  training_loss: 1.6804 (1.6847)  mae_loss: 0.2670 (0.2683)  classification_loss: 1.3922 (1.3950)  loss_mask: 0.0019 (0.0213)  time: 0.1970  data: 0.0003  max mem: 5511
[08:28:22.723927] Epoch: [49]  [ 60/781]  eta: 0:02:30  lr: 0.000140  training_loss: 1.7617 (1.7184)  mae_loss: 0.2886 (0.2759)  classification_loss: 1.4231 (1.4072)  loss_mask: 0.0500 (0.0354)  time: 0.1972  data: 0.0003  max mem: 5511
[08:28:26.658833] Epoch: [49]  [ 80/781]  eta: 0:02:24  lr: 0.000139  training_loss: 1.7831 (1.7417)  mae_loss: 0.2884 (0.2799)  classification_loss: 1.3814 (1.4042)  loss_mask: 0.1002 (0.0576)  time: 0.1967  data: 0.0002  max mem: 5511
[08:28:30.612463] Epoch: [49]  [100/781]  eta: 0:02:19  lr: 0.000139  training_loss: 1.7398 (1.7424)  mae_loss: 0.2666 (0.2782)  classification_loss: 1.3913 (1.4021)  loss_mask: 0.0631 (0.0621)  time: 0.1976  data: 0.0002  max mem: 5511
[08:28:34.564814] Epoch: [49]  [120/781]  eta: 0:02:14  lr: 0.000139  training_loss: 1.8102 (1.7509)  mae_loss: 0.2722 (0.2778)  classification_loss: 1.3903 (1.4012)  loss_mask: 0.0998 (0.0719)  time: 0.1975  data: 0.0003  max mem: 5511
[08:28:38.515728] Epoch: [49]  [140/781]  eta: 0:02:09  lr: 0.000139  training_loss: 1.7026 (1.7450)  mae_loss: 0.2672 (0.2761)  classification_loss: 1.4080 (1.4022)  loss_mask: 0.0295 (0.0667)  time: 0.1975  data: 0.0002  max mem: 5511
[08:28:42.456643] Epoch: [49]  [160/781]  eta: 0:02:05  lr: 0.000139  training_loss: 1.6588 (1.7361)  mae_loss: 0.2631 (0.2746)  classification_loss: 1.3798 (1.4009)  loss_mask: 0.0127 (0.0606)  time: 0.1970  data: 0.0002  max mem: 5511
[08:28:46.433474] Epoch: [49]  [180/781]  eta: 0:02:01  lr: 0.000139  training_loss: 1.7194 (1.7356)  mae_loss: 0.2665 (0.2744)  classification_loss: 1.3936 (1.4017)  loss_mask: 0.0243 (0.0595)  time: 0.1988  data: 0.0002  max mem: 5511
[08:28:50.386200] Epoch: [49]  [200/781]  eta: 0:01:56  lr: 0.000139  training_loss: 1.7172 (1.7373)  mae_loss: 0.2576 (0.2728)  classification_loss: 1.3746 (1.4012)  loss_mask: 0.0484 (0.0632)  time: 0.1976  data: 0.0002  max mem: 5511
[08:28:54.326691] Epoch: [49]  [220/781]  eta: 0:01:52  lr: 0.000139  training_loss: 1.6674 (1.7361)  mae_loss: 0.2622 (0.2723)  classification_loss: 1.3640 (1.4016)  loss_mask: 0.0457 (0.0623)  time: 0.1970  data: 0.0002  max mem: 5511
[08:28:58.280651] Epoch: [49]  [240/781]  eta: 0:01:48  lr: 0.000139  training_loss: 1.7119 (1.7319)  mae_loss: 0.2757 (0.2723)  classification_loss: 1.4044 (1.4002)  loss_mask: 0.0236 (0.0593)  time: 0.1976  data: 0.0003  max mem: 5511
[08:29:02.222870] Epoch: [49]  [260/781]  eta: 0:01:44  lr: 0.000139  training_loss: 1.6768 (1.7290)  mae_loss: 0.2592 (0.2714)  classification_loss: 1.3834 (1.4010)  loss_mask: 0.0136 (0.0566)  time: 0.1970  data: 0.0003  max mem: 5511
[08:29:06.204446] Epoch: [49]  [280/781]  eta: 0:01:40  lr: 0.000138  training_loss: 1.6791 (1.7250)  mae_loss: 0.2581 (0.2712)  classification_loss: 1.3864 (1.4001)  loss_mask: 0.0057 (0.0537)  time: 0.1990  data: 0.0002  max mem: 5511
[08:29:10.154943] Epoch: [49]  [300/781]  eta: 0:01:36  lr: 0.000138  training_loss: 1.6597 (1.7232)  mae_loss: 0.2805 (0.2716)  classification_loss: 1.3847 (1.4005)  loss_mask: 0.0061 (0.0511)  time: 0.1974  data: 0.0003  max mem: 5511
[08:29:14.100442] Epoch: [49]  [320/781]  eta: 0:01:32  lr: 0.000138  training_loss: 1.6488 (1.7172)  mae_loss: 0.2566 (0.2710)  classification_loss: 1.3458 (1.3973)  loss_mask: 0.0030 (0.0489)  time: 0.1972  data: 0.0002  max mem: 5511
[08:29:18.074222] Epoch: [49]  [340/781]  eta: 0:01:28  lr: 0.000138  training_loss: 1.6088 (1.7114)  mae_loss: 0.2594 (0.2707)  classification_loss: 1.3254 (1.3941)  loss_mask: 0.0032 (0.0466)  time: 0.1986  data: 0.0002  max mem: 5511
[08:29:22.043999] Epoch: [49]  [360/781]  eta: 0:01:24  lr: 0.000138  training_loss: 1.6712 (1.7105)  mae_loss: 0.2573 (0.2703)  classification_loss: 1.3616 (1.3935)  loss_mask: 0.0171 (0.0468)  time: 0.1984  data: 0.0002  max mem: 5511
[08:29:26.023837] Epoch: [49]  [380/781]  eta: 0:01:20  lr: 0.000138  training_loss: 1.6439 (1.7081)  mae_loss: 0.2656 (0.2706)  classification_loss: 1.3385 (1.3921)  loss_mask: 0.0134 (0.0454)  time: 0.1989  data: 0.0003  max mem: 5511
[08:29:29.960964] Epoch: [49]  [400/781]  eta: 0:01:15  lr: 0.000138  training_loss: 1.6157 (1.7038)  mae_loss: 0.2712 (0.2703)  classification_loss: 1.3536 (1.3901)  loss_mask: 0.0042 (0.0435)  time: 0.1968  data: 0.0002  max mem: 5511
[08:29:33.925108] Epoch: [49]  [420/781]  eta: 0:01:11  lr: 0.000138  training_loss: 1.6325 (1.7009)  mae_loss: 0.2758 (0.2703)  classification_loss: 1.3673 (1.3891)  loss_mask: 0.0026 (0.0415)  time: 0.1981  data: 0.0002  max mem: 5511
[08:29:37.863096] Epoch: [49]  [440/781]  eta: 0:01:07  lr: 0.000138  training_loss: 1.6312 (1.6977)  mae_loss: 0.2611 (0.2698)  classification_loss: 1.3709 (1.3882)  loss_mask: 0.0013 (0.0397)  time: 0.1968  data: 0.0003  max mem: 5511
[08:29:41.806068] Epoch: [49]  [460/781]  eta: 0:01:03  lr: 0.000137  training_loss: 1.5866 (1.6942)  mae_loss: 0.2625 (0.2699)  classification_loss: 1.3286 (1.3863)  loss_mask: 0.0010 (0.0380)  time: 0.1971  data: 0.0003  max mem: 5511
[08:29:45.749908] Epoch: [49]  [480/781]  eta: 0:00:59  lr: 0.000137  training_loss: 1.6627 (1.6923)  mae_loss: 0.2620 (0.2694)  classification_loss: 1.3755 (1.3860)  loss_mask: 0.0012 (0.0369)  time: 0.1971  data: 0.0002  max mem: 5511
[08:29:49.736456] Epoch: [49]  [500/781]  eta: 0:00:55  lr: 0.000137  training_loss: 1.6539 (1.6905)  mae_loss: 0.2676 (0.2696)  classification_loss: 1.3589 (1.3850)  loss_mask: 0.0113 (0.0359)  time: 0.1992  data: 0.0007  max mem: 5511
[08:29:53.675057] Epoch: [49]  [520/781]  eta: 0:00:51  lr: 0.000137  training_loss: 1.6440 (1.6889)  mae_loss: 0.2403 (0.2687)  classification_loss: 1.3996 (1.3856)  loss_mask: 0.0016 (0.0346)  time: 0.1968  data: 0.0002  max mem: 5511
[08:29:57.600089] Epoch: [49]  [540/781]  eta: 0:00:47  lr: 0.000137  training_loss: 1.6605 (1.6879)  mae_loss: 0.2498 (0.2682)  classification_loss: 1.4170 (1.3862)  loss_mask: 0.0013 (0.0334)  time: 0.1962  data: 0.0003  max mem: 5511
[08:30:01.573287] Epoch: [49]  [560/781]  eta: 0:00:43  lr: 0.000137  training_loss: 1.6480 (1.6870)  mae_loss: 0.2710 (0.2684)  classification_loss: 1.3794 (1.3863)  loss_mask: 0.0007 (0.0323)  time: 0.1986  data: 0.0004  max mem: 5511
[08:30:05.569304] Epoch: [49]  [580/781]  eta: 0:00:39  lr: 0.000137  training_loss: 1.5819 (1.6844)  mae_loss: 0.2584 (0.2681)  classification_loss: 1.3251 (1.3851)  loss_mask: 0.0006 (0.0312)  time: 0.1997  data: 0.0003  max mem: 5511
[08:30:09.518998] Epoch: [49]  [600/781]  eta: 0:00:35  lr: 0.000137  training_loss: 1.6005 (1.6824)  mae_loss: 0.2582 (0.2683)  classification_loss: 1.3524 (1.3840)  loss_mask: 0.0004 (0.0302)  time: 0.1974  data: 0.0002  max mem: 5511
[08:30:13.454021] Epoch: [49]  [620/781]  eta: 0:00:32  lr: 0.000137  training_loss: 1.6004 (1.6807)  mae_loss: 0.2751 (0.2686)  classification_loss: 1.3337 (1.3825)  loss_mask: 0.0008 (0.0295)  time: 0.1966  data: 0.0003  max mem: 5511
[08:30:17.421774] Epoch: [49]  [640/781]  eta: 0:00:28  lr: 0.000137  training_loss: 1.6595 (1.6798)  mae_loss: 0.2689 (0.2688)  classification_loss: 1.3645 (1.3822)  loss_mask: 0.0020 (0.0289)  time: 0.1983  data: 0.0002  max mem: 5511
[08:30:21.349632] Epoch: [49]  [660/781]  eta: 0:00:24  lr: 0.000136  training_loss: 1.6800 (1.6800)  mae_loss: 0.2585 (0.2688)  classification_loss: 1.3874 (1.3829)  loss_mask: 0.0013 (0.0282)  time: 0.1963  data: 0.0002  max mem: 5511
[08:30:25.303354] Epoch: [49]  [680/781]  eta: 0:00:20  lr: 0.000136  training_loss: 1.6374 (1.6788)  mae_loss: 0.2645 (0.2689)  classification_loss: 1.3521 (1.3823)  loss_mask: 0.0021 (0.0275)  time: 0.1976  data: 0.0003  max mem: 5511
[08:30:29.253224] Epoch: [49]  [700/781]  eta: 0:00:16  lr: 0.000136  training_loss: 1.6215 (1.6781)  mae_loss: 0.2555 (0.2686)  classification_loss: 1.3857 (1.3827)  loss_mask: 0.0006 (0.0268)  time: 0.1974  data: 0.0003  max mem: 5511
[08:30:33.229153] Epoch: [49]  [720/781]  eta: 0:00:12  lr: 0.000136  training_loss: 1.6151 (1.6775)  mae_loss: 0.2508 (0.2684)  classification_loss: 1.3574 (1.3829)  loss_mask: 0.0006 (0.0262)  time: 0.1987  data: 0.0002  max mem: 5511
[08:30:37.182875] Epoch: [49]  [740/781]  eta: 0:00:08  lr: 0.000136  training_loss: 1.5905 (1.6754)  mae_loss: 0.2501 (0.2681)  classification_loss: 1.3446 (1.3818)  loss_mask: 0.0006 (0.0255)  time: 0.1976  data: 0.0005  max mem: 5511
[08:30:41.152669] Epoch: [49]  [760/781]  eta: 0:00:04  lr: 0.000136  training_loss: 1.6440 (1.6754)  mae_loss: 0.2722 (0.2683)  classification_loss: 1.3911 (1.3822)  loss_mask: 0.0004 (0.0249)  time: 0.1984  data: 0.0002  max mem: 5511
[08:30:45.162847] Epoch: [49]  [780/781]  eta: 0:00:00  lr: 0.000136  training_loss: 1.6527 (1.6746)  mae_loss: 0.2597 (0.2682)  classification_loss: 1.3733 (1.3822)  loss_mask: 0.0005 (0.0242)  time: 0.2004  data: 0.0002  max mem: 5511
[08:30:45.349680] Epoch: [49] Total time: 0:02:35 (0.1990 s / it)
[08:30:45.350212] Averaged stats: lr: 0.000136  training_loss: 1.6527 (1.6746)  mae_loss: 0.2597 (0.2682)  classification_loss: 1.3733 (1.3822)  loss_mask: 0.0005 (0.0242)
[08:30:45.961249] Test:  [  0/157]  eta: 0:01:35  testing_loss: 0.6222 (0.6222)  acc1: 81.2500 (81.2500)  acc5: 98.4375 (98.4375)  time: 0.6063  data: 0.5752  max mem: 5511
[08:30:46.270589] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.6569 (0.6742)  acc1: 78.1250 (77.6989)  acc5: 100.0000 (99.4318)  time: 0.0827  data: 0.0525  max mem: 5511
[08:30:46.560290] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.6451 (0.6510)  acc1: 78.1250 (78.6458)  acc5: 100.0000 (99.4048)  time: 0.0296  data: 0.0002  max mem: 5511
[08:30:46.847522] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6535 (0.6601)  acc1: 79.6875 (78.5786)  acc5: 100.0000 (99.1431)  time: 0.0287  data: 0.0002  max mem: 5511
[08:30:47.136233] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6700 (0.6653)  acc1: 78.1250 (78.3537)  acc5: 98.4375 (99.1235)  time: 0.0286  data: 0.0003  max mem: 5511
[08:30:47.426545] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6521 (0.6580)  acc1: 78.1250 (78.8603)  acc5: 98.4375 (99.0196)  time: 0.0287  data: 0.0004  max mem: 5511
[08:30:47.711809] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6259 (0.6507)  acc1: 81.2500 (78.9959)  acc5: 98.4375 (99.0266)  time: 0.0286  data: 0.0004  max mem: 5511
[08:30:47.998466] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6171 (0.6467)  acc1: 78.1250 (79.1813)  acc5: 100.0000 (99.0097)  time: 0.0284  data: 0.0002  max mem: 5511
[08:30:48.281001] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6498 (0.6556)  acc1: 78.1250 (78.7809)  acc5: 98.4375 (98.9776)  time: 0.0283  data: 0.0001  max mem: 5511
[08:30:48.571354] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7084 (0.6549)  acc1: 76.5625 (78.8462)  acc5: 98.4375 (98.9354)  time: 0.0285  data: 0.0002  max mem: 5511
[08:30:48.858531] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.6892 (0.6593)  acc1: 78.1250 (78.7438)  acc5: 98.4375 (98.9171)  time: 0.0287  data: 0.0002  max mem: 5511
[08:30:49.149915] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6892 (0.6619)  acc1: 78.1250 (78.6318)  acc5: 98.4375 (98.8880)  time: 0.0287  data: 0.0002  max mem: 5511
[08:30:49.437759] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6492 (0.6581)  acc1: 79.6875 (78.8611)  acc5: 98.4375 (98.8895)  time: 0.0288  data: 0.0002  max mem: 5511
[08:30:49.725874] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6314 (0.6579)  acc1: 79.6875 (78.8764)  acc5: 98.4375 (98.8669)  time: 0.0287  data: 0.0002  max mem: 5511
[08:30:50.012168] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6569 (0.6564)  acc1: 79.6875 (79.0669)  acc5: 100.0000 (98.9140)  time: 0.0285  data: 0.0002  max mem: 5511
[08:30:50.296021] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6569 (0.6549)  acc1: 79.6875 (79.0977)  acc5: 98.4375 (98.9031)  time: 0.0283  data: 0.0002  max mem: 5511
[08:30:50.446414] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6132 (0.6558)  acc1: 79.6875 (79.0600)  acc5: 98.4375 (98.9200)  time: 0.0272  data: 0.0001  max mem: 5511
[08:30:50.622736] Test: Total time: 0:00:05 (0.0336 s / it)
[08:30:50.624256] * Acc@1 79.060 Acc@5 98.920 loss 0.656
[08:30:50.624585] Accuracy of the network on the 10000 test images: 79.1%
[08:30:50.624778] Max accuracy: 79.60%
[08:30:51.050498] log_dir: ./output_dir
[08:30:51.866964] Epoch: [50]  [  0/781]  eta: 0:10:36  lr: 0.000136  training_loss: 1.4358 (1.4358)  mae_loss: 0.2966 (0.2966)  classification_loss: 1.1387 (1.1387)  loss_mask: 0.0004 (0.0004)  time: 0.8147  data: 0.5744  max mem: 5511
[08:30:55.833306] Epoch: [50]  [ 20/781]  eta: 0:02:53  lr: 0.000136  training_loss: 1.5591 (1.5826)  mae_loss: 0.2523 (0.2694)  classification_loss: 1.2909 (1.3126)  loss_mask: 0.0005 (0.0006)  time: 0.1982  data: 0.0002  max mem: 5511
[08:30:59.787586] Epoch: [50]  [ 40/781]  eta: 0:02:37  lr: 0.000136  training_loss: 1.6132 (1.6067)  mae_loss: 0.2627 (0.2647)  classification_loss: 1.3316 (1.3406)  loss_mask: 0.0008 (0.0015)  time: 0.1976  data: 0.0002  max mem: 5511
[08:31:03.732071] Epoch: [50]  [ 60/781]  eta: 0:02:29  lr: 0.000135  training_loss: 1.6189 (1.6181)  mae_loss: 0.2626 (0.2629)  classification_loss: 1.3473 (1.3523)  loss_mask: 0.0010 (0.0029)  time: 0.1971  data: 0.0002  max mem: 5511
[08:31:07.701287] Epoch: [50]  [ 80/781]  eta: 0:02:24  lr: 0.000135  training_loss: 1.6530 (1.6324)  mae_loss: 0.2735 (0.2667)  classification_loss: 1.3987 (1.3634)  loss_mask: 0.0005 (0.0023)  time: 0.1984  data: 0.0002  max mem: 5511
[08:31:11.649190] Epoch: [50]  [100/781]  eta: 0:02:18  lr: 0.000135  training_loss: 1.7189 (1.6531)  mae_loss: 0.2549 (0.2664)  classification_loss: 1.4186 (1.3716)  loss_mask: 0.0082 (0.0152)  time: 0.1973  data: 0.0002  max mem: 5511
[08:31:15.630835] Epoch: [50]  [120/781]  eta: 0:02:14  lr: 0.000135  training_loss: 1.6578 (1.6576)  mae_loss: 0.2710 (0.2667)  classification_loss: 1.3345 (1.3711)  loss_mask: 0.0258 (0.0199)  time: 0.1990  data: 0.0003  max mem: 5511
[08:31:19.582469] Epoch: [50]  [140/781]  eta: 0:02:09  lr: 0.000135  training_loss: 1.5914 (1.6496)  mae_loss: 0.2486 (0.2650)  classification_loss: 1.3312 (1.3656)  loss_mask: 0.0130 (0.0190)  time: 0.1974  data: 0.0002  max mem: 5511
[08:31:23.531319] Epoch: [50]  [160/781]  eta: 0:02:05  lr: 0.000135  training_loss: 1.6227 (1.6472)  mae_loss: 0.2677 (0.2664)  classification_loss: 1.3444 (1.3636)  loss_mask: 0.0023 (0.0172)  time: 0.1974  data: 0.0003  max mem: 5511
[08:31:27.488208] Epoch: [50]  [180/781]  eta: 0:02:00  lr: 0.000135  training_loss: 1.6496 (1.6444)  mae_loss: 0.2796 (0.2672)  classification_loss: 1.3625 (1.3616)  loss_mask: 0.0015 (0.0157)  time: 0.1978  data: 0.0003  max mem: 5511
[08:31:31.437131] Epoch: [50]  [200/781]  eta: 0:01:56  lr: 0.000135  training_loss: 1.5821 (1.6418)  mae_loss: 0.2521 (0.2662)  classification_loss: 1.3507 (1.3605)  loss_mask: 0.0036 (0.0150)  time: 0.1974  data: 0.0002  max mem: 5511
[08:31:35.385374] Epoch: [50]  [220/781]  eta: 0:01:52  lr: 0.000135  training_loss: 1.6292 (1.6392)  mae_loss: 0.2600 (0.2664)  classification_loss: 1.3199 (1.3581)  loss_mask: 0.0061 (0.0147)  time: 0.1973  data: 0.0002  max mem: 5511
[08:31:39.374010] Epoch: [50]  [240/781]  eta: 0:01:48  lr: 0.000135  training_loss: 1.6013 (1.6380)  mae_loss: 0.2516 (0.2658)  classification_loss: 1.3276 (1.3586)  loss_mask: 0.0017 (0.0136)  time: 0.1993  data: 0.0003  max mem: 5511
[08:31:43.375894] Epoch: [50]  [260/781]  eta: 0:01:44  lr: 0.000134  training_loss: 1.6200 (1.6378)  mae_loss: 0.2347 (0.2646)  classification_loss: 1.3705 (1.3603)  loss_mask: 0.0018 (0.0130)  time: 0.2000  data: 0.0002  max mem: 5511
[08:31:47.365242] Epoch: [50]  [280/781]  eta: 0:01:40  lr: 0.000134  training_loss: 1.6846 (1.6466)  mae_loss: 0.2640 (0.2650)  classification_loss: 1.3665 (1.3607)  loss_mask: 0.0565 (0.0209)  time: 0.1994  data: 0.0005  max mem: 5511
[08:31:51.345268] Epoch: [50]  [300/781]  eta: 0:01:36  lr: 0.000134  training_loss: 1.6934 (1.6525)  mae_loss: 0.2507 (0.2648)  classification_loss: 1.3630 (1.3627)  loss_mask: 0.0563 (0.0250)  time: 0.1989  data: 0.0003  max mem: 5511
[08:31:55.291487] Epoch: [50]  [320/781]  eta: 0:01:32  lr: 0.000134  training_loss: 1.6960 (1.6535)  mae_loss: 0.2672 (0.2652)  classification_loss: 1.3895 (1.3632)  loss_mask: 0.0230 (0.0251)  time: 0.1972  data: 0.0005  max mem: 5511
[08:31:59.260378] Epoch: [50]  [340/781]  eta: 0:01:28  lr: 0.000134  training_loss: 1.5901 (1.6519)  mae_loss: 0.2725 (0.2656)  classification_loss: 1.3196 (1.3622)  loss_mask: 0.0078 (0.0241)  time: 0.1984  data: 0.0002  max mem: 5511
[08:32:03.199299] Epoch: [50]  [360/781]  eta: 0:01:24  lr: 0.000134  training_loss: 1.5714 (1.6489)  mae_loss: 0.2614 (0.2655)  classification_loss: 1.3281 (1.3603)  loss_mask: 0.0041 (0.0231)  time: 0.1969  data: 0.0002  max mem: 5511
[08:32:07.155187] Epoch: [50]  [380/781]  eta: 0:01:20  lr: 0.000134  training_loss: 1.5986 (1.6486)  mae_loss: 0.2710 (0.2663)  classification_loss: 1.3388 (1.3602)  loss_mask: 0.0028 (0.0221)  time: 0.1977  data: 0.0002  max mem: 5511
[08:32:11.083996] Epoch: [50]  [400/781]  eta: 0:01:16  lr: 0.000134  training_loss: 1.6633 (1.6493)  mae_loss: 0.2583 (0.2662)  classification_loss: 1.3931 (1.3621)  loss_mask: 0.0018 (0.0211)  time: 0.1963  data: 0.0002  max mem: 5511
[08:32:15.051364] Epoch: [50]  [420/781]  eta: 0:01:11  lr: 0.000134  training_loss: 1.6299 (1.6475)  mae_loss: 0.2633 (0.2660)  classification_loss: 1.3558 (1.3609)  loss_mask: 0.0054 (0.0206)  time: 0.1983  data: 0.0002  max mem: 5511
[08:32:18.980500] Epoch: [50]  [440/781]  eta: 0:01:07  lr: 0.000133  training_loss: 1.6064 (1.6464)  mae_loss: 0.2567 (0.2660)  classification_loss: 1.3250 (1.3602)  loss_mask: 0.0020 (0.0202)  time: 0.1964  data: 0.0002  max mem: 5511
[08:32:22.959119] Epoch: [50]  [460/781]  eta: 0:01:03  lr: 0.000133  training_loss: 1.6023 (1.6449)  mae_loss: 0.2639 (0.2660)  classification_loss: 1.2990 (1.3585)  loss_mask: 0.0115 (0.0203)  time: 0.1988  data: 0.0002  max mem: 5511
[08:32:26.926974] Epoch: [50]  [480/781]  eta: 0:00:59  lr: 0.000133  training_loss: 1.6805 (1.6454)  mae_loss: 0.2664 (0.2660)  classification_loss: 1.3957 (1.3591)  loss_mask: 0.0106 (0.0203)  time: 0.1983  data: 0.0002  max mem: 5511
[08:32:30.862935] Epoch: [50]  [500/781]  eta: 0:00:55  lr: 0.000133  training_loss: 1.6613 (1.6468)  mae_loss: 0.2758 (0.2663)  classification_loss: 1.3858 (1.3605)  loss_mask: 0.0069 (0.0200)  time: 0.1967  data: 0.0002  max mem: 5511
[08:32:34.827369] Epoch: [50]  [520/781]  eta: 0:00:51  lr: 0.000133  training_loss: 1.6717 (1.6466)  mae_loss: 0.2622 (0.2662)  classification_loss: 1.3646 (1.3601)  loss_mask: 0.0202 (0.0204)  time: 0.1981  data: 0.0002  max mem: 5511
[08:32:38.773275] Epoch: [50]  [540/781]  eta: 0:00:47  lr: 0.000133  training_loss: 1.7708 (1.6531)  mae_loss: 0.2727 (0.2663)  classification_loss: 1.4198 (1.3618)  loss_mask: 0.0951 (0.0251)  time: 0.1972  data: 0.0002  max mem: 5511
[08:32:42.713090] Epoch: [50]  [560/781]  eta: 0:00:43  lr: 0.000133  training_loss: 1.6729 (1.6547)  mae_loss: 0.2599 (0.2661)  classification_loss: 1.3792 (1.3632)  loss_mask: 0.0246 (0.0253)  time: 0.1969  data: 0.0002  max mem: 5511
[08:32:46.656524] Epoch: [50]  [580/781]  eta: 0:00:39  lr: 0.000133  training_loss: 1.6248 (1.6542)  mae_loss: 0.2608 (0.2661)  classification_loss: 1.3277 (1.3632)  loss_mask: 0.0075 (0.0249)  time: 0.1971  data: 0.0002  max mem: 5511
[08:32:50.613477] Epoch: [50]  [600/781]  eta: 0:00:35  lr: 0.000133  training_loss: 1.5492 (1.6525)  mae_loss: 0.2689 (0.2660)  classification_loss: 1.3035 (1.3621)  loss_mask: 0.0043 (0.0244)  time: 0.1977  data: 0.0006  max mem: 5511
[08:32:54.559289] Epoch: [50]  [620/781]  eta: 0:00:32  lr: 0.000133  training_loss: 1.6418 (1.6529)  mae_loss: 0.2934 (0.2671)  classification_loss: 1.3401 (1.3621)  loss_mask: 0.0029 (0.0237)  time: 0.1972  data: 0.0003  max mem: 5511
[08:32:58.508318] Epoch: [50]  [640/781]  eta: 0:00:28  lr: 0.000132  training_loss: 1.6402 (1.6524)  mae_loss: 0.2558 (0.2668)  classification_loss: 1.3869 (1.3625)  loss_mask: 0.0030 (0.0231)  time: 0.1974  data: 0.0002  max mem: 5511
[08:33:02.443287] Epoch: [50]  [660/781]  eta: 0:00:24  lr: 0.000132  training_loss: 1.6465 (1.6527)  mae_loss: 0.2703 (0.2672)  classification_loss: 1.3737 (1.3631)  loss_mask: 0.0013 (0.0224)  time: 0.1966  data: 0.0003  max mem: 5511
[08:33:06.361658] Epoch: [50]  [680/781]  eta: 0:00:20  lr: 0.000132  training_loss: 1.6036 (1.6525)  mae_loss: 0.2594 (0.2670)  classification_loss: 1.3373 (1.3636)  loss_mask: 0.0009 (0.0218)  time: 0.1958  data: 0.0002  max mem: 5511
[08:33:10.300555] Epoch: [50]  [700/781]  eta: 0:00:16  lr: 0.000132  training_loss: 1.6033 (1.6524)  mae_loss: 0.2713 (0.2674)  classification_loss: 1.3363 (1.3637)  loss_mask: 0.0007 (0.0212)  time: 0.1969  data: 0.0004  max mem: 5511
[08:33:14.228598] Epoch: [50]  [720/781]  eta: 0:00:12  lr: 0.000132  training_loss: 1.7042 (1.6531)  mae_loss: 0.2758 (0.2676)  classification_loss: 1.4018 (1.3648)  loss_mask: 0.0012 (0.0207)  time: 0.1963  data: 0.0002  max mem: 5511
[08:33:18.160177] Epoch: [50]  [740/781]  eta: 0:00:08  lr: 0.000132  training_loss: 1.6055 (1.6524)  mae_loss: 0.2709 (0.2679)  classification_loss: 1.3064 (1.3642)  loss_mask: 0.0008 (0.0203)  time: 0.1965  data: 0.0002  max mem: 5511
[08:33:22.096934] Epoch: [50]  [760/781]  eta: 0:00:04  lr: 0.000132  training_loss: 1.6707 (1.6532)  mae_loss: 0.2687 (0.2681)  classification_loss: 1.3732 (1.3652)  loss_mask: 0.0022 (0.0200)  time: 0.1968  data: 0.0002  max mem: 5511
[08:33:26.035608] Epoch: [50]  [780/781]  eta: 0:00:00  lr: 0.000132  training_loss: 1.6733 (1.6540)  mae_loss: 0.2724 (0.2682)  classification_loss: 1.4059 (1.3661)  loss_mask: 0.0020 (0.0196)  time: 0.1968  data: 0.0003  max mem: 5511
[08:33:26.210412] Epoch: [50] Total time: 0:02:35 (0.1987 s / it)
[08:33:26.210941] Averaged stats: lr: 0.000132  training_loss: 1.6733 (1.6540)  mae_loss: 0.2724 (0.2682)  classification_loss: 1.4059 (1.3661)  loss_mask: 0.0020 (0.0196)
[08:33:27.364190] Test:  [  0/157]  eta: 0:01:39  testing_loss: 0.6523 (0.6523)  acc1: 84.3750 (84.3750)  acc5: 100.0000 (100.0000)  time: 0.6342  data: 0.6036  max mem: 5511
[08:33:27.652689] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.6870 (0.7092)  acc1: 78.1250 (78.1250)  acc5: 98.4375 (98.7216)  time: 0.0837  data: 0.0551  max mem: 5511
[08:33:27.939975] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.6725 (0.6851)  acc1: 78.1250 (78.4970)  acc5: 98.4375 (98.6607)  time: 0.0286  data: 0.0002  max mem: 5511
[08:33:28.229776] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6725 (0.7000)  acc1: 78.1250 (78.2762)  acc5: 98.4375 (98.3871)  time: 0.0287  data: 0.0002  max mem: 5511
[08:33:28.518497] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6897 (0.7029)  acc1: 76.5625 (77.9726)  acc5: 98.4375 (98.5137)  time: 0.0287  data: 0.0003  max mem: 5511
[08:33:28.814534] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6501 (0.6933)  acc1: 78.1250 (78.1863)  acc5: 98.4375 (98.5294)  time: 0.0290  data: 0.0003  max mem: 5511
[08:33:29.100238] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6564 (0.6879)  acc1: 78.1250 (78.1506)  acc5: 98.4375 (98.5400)  time: 0.0289  data: 0.0002  max mem: 5511
[08:33:29.399778] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6234 (0.6804)  acc1: 79.6875 (78.3891)  acc5: 98.4375 (98.5695)  time: 0.0291  data: 0.0002  max mem: 5511
[08:33:29.684483] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6627 (0.6902)  acc1: 78.1250 (78.0671)  acc5: 98.4375 (98.5147)  time: 0.0291  data: 0.0002  max mem: 5511
[08:33:29.967663] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6731 (0.6888)  acc1: 78.1250 (78.1937)  acc5: 98.4375 (98.5920)  time: 0.0283  data: 0.0002  max mem: 5511
[08:33:30.251668] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.7015 (0.6929)  acc1: 76.5625 (77.8929)  acc5: 98.4375 (98.5922)  time: 0.0282  data: 0.0002  max mem: 5511
[08:33:30.534253] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.7182 (0.6952)  acc1: 73.4375 (77.5619)  acc5: 98.4375 (98.5923)  time: 0.0282  data: 0.0002  max mem: 5511
[08:33:30.818194] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6585 (0.6910)  acc1: 75.0000 (77.7376)  acc5: 98.4375 (98.5925)  time: 0.0282  data: 0.0002  max mem: 5511
[08:33:31.102540] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6460 (0.6928)  acc1: 76.5625 (77.6718)  acc5: 98.4375 (98.6164)  time: 0.0283  data: 0.0002  max mem: 5511
[08:33:31.384618] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7474 (0.6949)  acc1: 76.5625 (77.6817)  acc5: 98.4375 (98.5926)  time: 0.0282  data: 0.0001  max mem: 5511
[08:33:31.665988] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7278 (0.6946)  acc1: 76.5625 (77.7732)  acc5: 98.4375 (98.5720)  time: 0.0281  data: 0.0001  max mem: 5511
[08:33:31.817190] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6563 (0.6941)  acc1: 79.6875 (77.7900)  acc5: 98.4375 (98.6100)  time: 0.0271  data: 0.0001  max mem: 5511
[08:33:31.981252] Test: Total time: 0:00:05 (0.0335 s / it)
[08:33:31.981986] * Acc@1 77.790 Acc@5 98.610 loss 0.694
[08:33:31.982303] Accuracy of the network on the 10000 test images: 77.8%
[08:33:31.982484] Max accuracy: 79.60%
[08:33:32.167200] log_dir: ./output_dir
[08:33:33.139359] Epoch: [51]  [  0/781]  eta: 0:12:37  lr: 0.000132  training_loss: 1.5727 (1.5727)  mae_loss: 0.2660 (0.2660)  classification_loss: 1.3061 (1.3061)  loss_mask: 0.0006 (0.0006)  time: 0.9704  data: 0.7476  max mem: 5511
[08:33:37.119168] Epoch: [51]  [ 20/781]  eta: 0:02:59  lr: 0.000132  training_loss: 1.5759 (1.5829)  mae_loss: 0.2715 (0.2751)  classification_loss: 1.3109 (1.3069)  loss_mask: 0.0007 (0.0009)  time: 0.1989  data: 0.0003  max mem: 5511
[08:33:41.049463] Epoch: [51]  [ 40/781]  eta: 0:02:40  lr: 0.000131  training_loss: 1.6897 (1.6330)  mae_loss: 0.2825 (0.2753)  classification_loss: 1.3706 (1.3543)  loss_mask: 0.0014 (0.0033)  time: 0.1964  data: 0.0003  max mem: 5511
[08:33:45.015608] Epoch: [51]  [ 60/781]  eta: 0:02:31  lr: 0.000131  training_loss: 1.5991 (1.6268)  mae_loss: 0.2613 (0.2707)  classification_loss: 1.3440 (1.3535)  loss_mask: 0.0010 (0.0027)  time: 0.1982  data: 0.0002  max mem: 5511
[08:33:48.979155] Epoch: [51]  [ 80/781]  eta: 0:02:25  lr: 0.000131  training_loss: 1.6261 (1.6241)  mae_loss: 0.2424 (0.2677)  classification_loss: 1.3245 (1.3542)  loss_mask: 0.0006 (0.0022)  time: 0.1981  data: 0.0003  max mem: 5511
[08:33:52.922379] Epoch: [51]  [100/781]  eta: 0:02:19  lr: 0.000131  training_loss: 1.6538 (1.6249)  mae_loss: 0.2634 (0.2682)  classification_loss: 1.3516 (1.3544)  loss_mask: 0.0005 (0.0022)  time: 0.1971  data: 0.0002  max mem: 5511
[08:33:56.892074] Epoch: [51]  [120/781]  eta: 0:02:15  lr: 0.000131  training_loss: 1.5643 (1.6191)  mae_loss: 0.2689 (0.2681)  classification_loss: 1.3032 (1.3490)  loss_mask: 0.0006 (0.0020)  time: 0.1984  data: 0.0002  max mem: 5511
[08:34:00.896709] Epoch: [51]  [140/781]  eta: 0:02:10  lr: 0.000131  training_loss: 1.5760 (1.6172)  mae_loss: 0.2550 (0.2677)  classification_loss: 1.3228 (1.3477)  loss_mask: 0.0004 (0.0018)  time: 0.2001  data: 0.0003  max mem: 5511
[08:34:04.853511] Epoch: [51]  [160/781]  eta: 0:02:06  lr: 0.000131  training_loss: 1.6088 (1.6155)  mae_loss: 0.2436 (0.2653)  classification_loss: 1.3600 (1.3486)  loss_mask: 0.0005 (0.0016)  time: 0.1978  data: 0.0002  max mem: 5511
[08:34:08.803334] Epoch: [51]  [180/781]  eta: 0:02:01  lr: 0.000131  training_loss: 1.6273 (1.6187)  mae_loss: 0.2529 (0.2655)  classification_loss: 1.3604 (1.3517)  loss_mask: 0.0004 (0.0015)  time: 0.1974  data: 0.0002  max mem: 5511
[08:34:12.752624] Epoch: [51]  [200/781]  eta: 0:01:57  lr: 0.000131  training_loss: 1.5837 (1.6180)  mae_loss: 0.2779 (0.2660)  classification_loss: 1.3074 (1.3506)  loss_mask: 0.0004 (0.0014)  time: 0.1974  data: 0.0002  max mem: 5511
[08:34:16.712718] Epoch: [51]  [220/781]  eta: 0:01:53  lr: 0.000131  training_loss: 1.6362 (1.6201)  mae_loss: 0.2549 (0.2658)  classification_loss: 1.3750 (1.3530)  loss_mask: 0.0003 (0.0013)  time: 0.1979  data: 0.0002  max mem: 5511
[08:34:20.640723] Epoch: [51]  [240/781]  eta: 0:01:48  lr: 0.000130  training_loss: 1.6602 (1.6231)  mae_loss: 0.2673 (0.2660)  classification_loss: 1.3991 (1.3552)  loss_mask: 0.0006 (0.0019)  time: 0.1963  data: 0.0002  max mem: 5511
[08:34:24.608031] Epoch: [51]  [260/781]  eta: 0:01:44  lr: 0.000130  training_loss: 1.6148 (1.6225)  mae_loss: 0.2646 (0.2661)  classification_loss: 1.3278 (1.3540)  loss_mask: 0.0009 (0.0024)  time: 0.1983  data: 0.0002  max mem: 5511
[08:34:28.556902] Epoch: [51]  [280/781]  eta: 0:01:40  lr: 0.000130  training_loss: 1.6908 (1.6318)  mae_loss: 0.2535 (0.2658)  classification_loss: 1.3485 (1.3535)  loss_mask: 0.0295 (0.0125)  time: 0.1973  data: 0.0002  max mem: 5511
[08:34:32.530481] Epoch: [51]  [300/781]  eta: 0:01:36  lr: 0.000130  training_loss: 1.7068 (1.6383)  mae_loss: 0.2617 (0.2655)  classification_loss: 1.3100 (1.3532)  loss_mask: 0.1204 (0.0196)  time: 0.1986  data: 0.0002  max mem: 5511
[08:34:36.473941] Epoch: [51]  [320/781]  eta: 0:01:32  lr: 0.000130  training_loss: 1.6093 (1.6374)  mae_loss: 0.2499 (0.2652)  classification_loss: 1.3123 (1.3509)  loss_mask: 0.0358 (0.0213)  time: 0.1971  data: 0.0002  max mem: 5511
[08:34:40.423915] Epoch: [51]  [340/781]  eta: 0:01:28  lr: 0.000130  training_loss: 1.5829 (1.6348)  mae_loss: 0.2701 (0.2654)  classification_loss: 1.3007 (1.3482)  loss_mask: 0.0188 (0.0211)  time: 0.1974  data: 0.0002  max mem: 5511
[08:34:44.372682] Epoch: [51]  [360/781]  eta: 0:01:24  lr: 0.000130  training_loss: 1.6116 (1.6350)  mae_loss: 0.2652 (0.2657)  classification_loss: 1.3172 (1.3490)  loss_mask: 0.0055 (0.0204)  time: 0.1973  data: 0.0002  max mem: 5511
[08:34:48.391585] Epoch: [51]  [380/781]  eta: 0:01:20  lr: 0.000130  training_loss: 1.6333 (1.6351)  mae_loss: 0.2742 (0.2661)  classification_loss: 1.3650 (1.3496)  loss_mask: 0.0024 (0.0195)  time: 0.2009  data: 0.0002  max mem: 5511
[08:34:52.356206] Epoch: [51]  [400/781]  eta: 0:01:16  lr: 0.000130  training_loss: 1.6247 (1.6344)  mae_loss: 0.2631 (0.2663)  classification_loss: 1.3435 (1.3495)  loss_mask: 0.0015 (0.0186)  time: 0.1981  data: 0.0002  max mem: 5511
[08:34:56.286610] Epoch: [51]  [420/781]  eta: 0:01:12  lr: 0.000129  training_loss: 1.5672 (1.6327)  mae_loss: 0.2622 (0.2660)  classification_loss: 1.3279 (1.3489)  loss_mask: 0.0009 (0.0178)  time: 0.1964  data: 0.0005  max mem: 5511
[08:35:00.221279] Epoch: [51]  [440/781]  eta: 0:01:08  lr: 0.000129  training_loss: 1.6165 (1.6312)  mae_loss: 0.2619 (0.2658)  classification_loss: 1.3298 (1.3484)  loss_mask: 0.0010 (0.0170)  time: 0.1966  data: 0.0002  max mem: 5511
[08:35:04.170006] Epoch: [51]  [460/781]  eta: 0:01:04  lr: 0.000129  training_loss: 1.5922 (1.6302)  mae_loss: 0.2639 (0.2656)  classification_loss: 1.3244 (1.3482)  loss_mask: 0.0011 (0.0164)  time: 0.1974  data: 0.0002  max mem: 5511
[08:35:08.104607] Epoch: [51]  [480/781]  eta: 0:01:00  lr: 0.000129  training_loss: 1.6308 (1.6305)  mae_loss: 0.2633 (0.2655)  classification_loss: 1.3838 (1.3493)  loss_mask: 0.0008 (0.0158)  time: 0.1967  data: 0.0003  max mem: 5511
[08:35:12.086596] Epoch: [51]  [500/781]  eta: 0:00:56  lr: 0.000129  training_loss: 1.6244 (1.6304)  mae_loss: 0.2580 (0.2653)  classification_loss: 1.3560 (1.3499)  loss_mask: 0.0004 (0.0152)  time: 0.1990  data: 0.0003  max mem: 5511
[08:35:16.055964] Epoch: [51]  [520/781]  eta: 0:00:52  lr: 0.000129  training_loss: 1.6247 (1.6306)  mae_loss: 0.2624 (0.2652)  classification_loss: 1.3588 (1.3508)  loss_mask: 0.0006 (0.0146)  time: 0.1984  data: 0.0003  max mem: 5511
[08:35:20.015944] Epoch: [51]  [540/781]  eta: 0:00:48  lr: 0.000129  training_loss: 1.6153 (1.6298)  mae_loss: 0.2650 (0.2650)  classification_loss: 1.3466 (1.3507)  loss_mask: 0.0005 (0.0141)  time: 0.1979  data: 0.0002  max mem: 5511
[08:35:23.969150] Epoch: [51]  [560/781]  eta: 0:00:44  lr: 0.000129  training_loss: 1.5993 (1.6289)  mae_loss: 0.2904 (0.2656)  classification_loss: 1.3055 (1.3497)  loss_mask: 0.0004 (0.0136)  time: 0.1976  data: 0.0002  max mem: 5511
[08:35:27.930515] Epoch: [51]  [580/781]  eta: 0:00:40  lr: 0.000129  training_loss: 1.5938 (1.6283)  mae_loss: 0.2640 (0.2656)  classification_loss: 1.3047 (1.3495)  loss_mask: 0.0004 (0.0131)  time: 0.1979  data: 0.0002  max mem: 5511
[08:35:31.870993] Epoch: [51]  [600/781]  eta: 0:00:36  lr: 0.000129  training_loss: 1.5967 (1.6278)  mae_loss: 0.2677 (0.2659)  classification_loss: 1.3278 (1.3491)  loss_mask: 0.0014 (0.0128)  time: 0.1969  data: 0.0002  max mem: 5511
[08:35:35.828100] Epoch: [51]  [620/781]  eta: 0:00:32  lr: 0.000128  training_loss: 1.6073 (1.6283)  mae_loss: 0.2580 (0.2659)  classification_loss: 1.3475 (1.3493)  loss_mask: 0.0021 (0.0131)  time: 0.1977  data: 0.0002  max mem: 5511
[08:35:39.822349] Epoch: [51]  [640/781]  eta: 0:00:28  lr: 0.000128  training_loss: 1.6977 (1.6293)  mae_loss: 0.2583 (0.2659)  classification_loss: 1.3549 (1.3495)  loss_mask: 0.0198 (0.0139)  time: 0.1996  data: 0.0002  max mem: 5511
[08:35:43.751691] Epoch: [51]  [660/781]  eta: 0:00:24  lr: 0.000128  training_loss: 1.6430 (1.6296)  mae_loss: 0.2659 (0.2658)  classification_loss: 1.3848 (1.3500)  loss_mask: 0.0103 (0.0138)  time: 0.1964  data: 0.0003  max mem: 5511
[08:35:47.699519] Epoch: [51]  [680/781]  eta: 0:00:20  lr: 0.000128  training_loss: 1.6255 (1.6294)  mae_loss: 0.2668 (0.2659)  classification_loss: 1.3187 (1.3501)  loss_mask: 0.0015 (0.0134)  time: 0.1973  data: 0.0003  max mem: 5511
[08:35:51.677877] Epoch: [51]  [700/781]  eta: 0:00:16  lr: 0.000128  training_loss: 1.6197 (1.6299)  mae_loss: 0.2611 (0.2658)  classification_loss: 1.3676 (1.3508)  loss_mask: 0.0009 (0.0132)  time: 0.1988  data: 0.0002  max mem: 5511
[08:35:55.679460] Epoch: [51]  [720/781]  eta: 0:00:12  lr: 0.000128  training_loss: 1.6486 (1.6317)  mae_loss: 0.2556 (0.2659)  classification_loss: 1.3951 (1.3525)  loss_mask: 0.0051 (0.0133)  time: 0.2000  data: 0.0002  max mem: 5511
[08:35:59.649437] Epoch: [51]  [740/781]  eta: 0:00:08  lr: 0.000128  training_loss: 1.6334 (1.6318)  mae_loss: 0.2833 (0.2663)  classification_loss: 1.3444 (1.3522)  loss_mask: 0.0128 (0.0134)  time: 0.1984  data: 0.0002  max mem: 5511
[08:36:03.599936] Epoch: [51]  [760/781]  eta: 0:00:04  lr: 0.000128  training_loss: 1.6968 (1.6349)  mae_loss: 0.2740 (0.2663)  classification_loss: 1.3901 (1.3536)  loss_mask: 0.0475 (0.0150)  time: 0.1974  data: 0.0005  max mem: 5511
[08:36:07.509689] Epoch: [51]  [780/781]  eta: 0:00:00  lr: 0.000128  training_loss: 1.6965 (1.6371)  mae_loss: 0.2679 (0.2664)  classification_loss: 1.3611 (1.3543)  loss_mask: 0.0623 (0.0163)  time: 0.1954  data: 0.0002  max mem: 5511
[08:36:07.671047] Epoch: [51] Total time: 0:02:35 (0.1991 s / it)
[08:36:07.671523] Averaged stats: lr: 0.000128  training_loss: 1.6965 (1.6371)  mae_loss: 0.2679 (0.2664)  classification_loss: 1.3611 (1.3543)  loss_mask: 0.0623 (0.0163)
[08:36:08.261668] Test:  [  0/157]  eta: 0:01:31  testing_loss: 0.5917 (0.5917)  acc1: 84.3750 (84.3750)  acc5: 98.4375 (98.4375)  time: 0.5853  data: 0.5538  max mem: 5511
[08:36:08.549346] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.6829 (0.6546)  acc1: 79.6875 (79.5455)  acc5: 98.4375 (99.0057)  time: 0.0791  data: 0.0505  max mem: 5511
[08:36:08.836088] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.6588 (0.6424)  acc1: 79.6875 (80.4315)  acc5: 98.4375 (98.8839)  time: 0.0285  data: 0.0002  max mem: 5511
[08:36:09.122330] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.6437 (0.6521)  acc1: 79.6875 (79.8891)  acc5: 98.4375 (98.6895)  time: 0.0285  data: 0.0002  max mem: 5511
[08:36:09.407611] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.6437 (0.6573)  acc1: 79.6875 (79.6113)  acc5: 98.4375 (98.8186)  time: 0.0285  data: 0.0002  max mem: 5511
[08:36:09.691715] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6337 (0.6491)  acc1: 81.2500 (80.3309)  acc5: 98.4375 (98.8358)  time: 0.0284  data: 0.0002  max mem: 5511
[08:36:09.974645] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6243 (0.6471)  acc1: 82.8125 (80.4047)  acc5: 98.4375 (98.8986)  time: 0.0282  data: 0.0002  max mem: 5511
[08:36:10.258526] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5959 (0.6414)  acc1: 81.2500 (80.5238)  acc5: 100.0000 (98.9217)  time: 0.0282  data: 0.0002  max mem: 5511
[08:36:10.541697] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6322 (0.6519)  acc1: 79.6875 (80.0154)  acc5: 100.0000 (98.8812)  time: 0.0282  data: 0.0002  max mem: 5511
[08:36:10.827514] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6450 (0.6488)  acc1: 79.6875 (80.2198)  acc5: 98.4375 (98.8668)  time: 0.0283  data: 0.0002  max mem: 5511
[08:36:11.115460] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.6484 (0.6531)  acc1: 79.6875 (80.0588)  acc5: 98.4375 (98.8707)  time: 0.0285  data: 0.0003  max mem: 5511
[08:36:11.399252] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6758 (0.6553)  acc1: 79.6875 (80.0535)  acc5: 100.0000 (98.8457)  time: 0.0284  data: 0.0002  max mem: 5511
[08:36:11.684369] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6039 (0.6506)  acc1: 79.6875 (80.1782)  acc5: 98.4375 (98.8378)  time: 0.0283  data: 0.0002  max mem: 5511
[08:36:11.969531] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6103 (0.6511)  acc1: 79.6875 (80.1288)  acc5: 98.4375 (98.8430)  time: 0.0284  data: 0.0002  max mem: 5511
[08:36:12.258351] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6299 (0.6496)  acc1: 81.2500 (80.2416)  acc5: 98.4375 (98.8364)  time: 0.0284  data: 0.0002  max mem: 5511
[08:36:12.540411] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6306 (0.6482)  acc1: 81.2500 (80.2359)  acc5: 98.4375 (98.8204)  time: 0.0283  data: 0.0001  max mem: 5511
[08:36:12.693320] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6113 (0.6493)  acc1: 81.2500 (80.1900)  acc5: 98.4375 (98.8400)  time: 0.0273  data: 0.0001  max mem: 5511
[08:36:12.843634] Test: Total time: 0:00:05 (0.0329 s / it)
[08:36:12.844126] * Acc@1 80.190 Acc@5 98.840 loss 0.649
[08:36:12.844437] Accuracy of the network on the 10000 test images: 80.2%
[08:36:12.844665] Max accuracy: 80.19%
[08:36:13.045060] log_dir: ./output_dir
[08:36:13.993238] Epoch: [52]  [  0/781]  eta: 0:12:19  lr: 0.000128  training_loss: 1.5352 (1.5352)  mae_loss: 0.2496 (0.2496)  classification_loss: 1.2790 (1.2790)  loss_mask: 0.0065 (0.0065)  time: 0.9464  data: 0.7023  max mem: 5511
[08:36:17.934721] Epoch: [52]  [ 20/781]  eta: 0:02:57  lr: 0.000127  training_loss: 1.6049 (1.6202)  mae_loss: 0.2702 (0.2664)  classification_loss: 1.3292 (1.3320)  loss_mask: 0.0220 (0.0217)  time: 0.1970  data: 0.0002  max mem: 5511
[08:36:21.880271] Epoch: [52]  [ 40/781]  eta: 0:02:39  lr: 0.000127  training_loss: 1.6144 (1.6320)  mae_loss: 0.2554 (0.2639)  classification_loss: 1.3459 (1.3515)  loss_mask: 0.0087 (0.0166)  time: 0.1972  data: 0.0004  max mem: 5511
[08:36:25.839823] Epoch: [52]  [ 60/781]  eta: 0:02:31  lr: 0.000127  training_loss: 1.6133 (1.6304)  mae_loss: 0.2662 (0.2635)  classification_loss: 1.3493 (1.3475)  loss_mask: 0.0126 (0.0195)  time: 0.1978  data: 0.0002  max mem: 5511
[08:36:29.778765] Epoch: [52]  [ 80/781]  eta: 0:02:24  lr: 0.000127  training_loss: 1.6304 (1.6282)  mae_loss: 0.2671 (0.2641)  classification_loss: 1.3288 (1.3431)  loss_mask: 0.0154 (0.0210)  time: 0.1969  data: 0.0002  max mem: 5511
[08:36:33.738300] Epoch: [52]  [100/781]  eta: 0:02:19  lr: 0.000127  training_loss: 1.6517 (1.6315)  mae_loss: 0.2704 (0.2642)  classification_loss: 1.3639 (1.3490)  loss_mask: 0.0053 (0.0183)  time: 0.1979  data: 0.0007  max mem: 5511
[08:36:37.677462] Epoch: [52]  [120/781]  eta: 0:02:14  lr: 0.000127  training_loss: 1.5799 (1.6239)  mae_loss: 0.2507 (0.2624)  classification_loss: 1.3296 (1.3459)  loss_mask: 0.0014 (0.0156)  time: 0.1969  data: 0.0002  max mem: 5511
[08:36:41.643260] Epoch: [52]  [140/781]  eta: 0:02:09  lr: 0.000127  training_loss: 1.5717 (1.6173)  mae_loss: 0.2547 (0.2617)  classification_loss: 1.3080 (1.3420)  loss_mask: 0.0009 (0.0135)  time: 0.1982  data: 0.0002  max mem: 5511
[08:36:45.604336] Epoch: [52]  [160/781]  eta: 0:02:05  lr: 0.000127  training_loss: 1.5944 (1.6153)  mae_loss: 0.2664 (0.2616)  classification_loss: 1.3474 (1.3417)  loss_mask: 0.0006 (0.0119)  time: 0.1980  data: 0.0004  max mem: 5511
[08:36:49.537176] Epoch: [52]  [180/781]  eta: 0:02:01  lr: 0.000127  training_loss: 1.6574 (1.6187)  mae_loss: 0.2557 (0.2614)  classification_loss: 1.3756 (1.3465)  loss_mask: 0.0007 (0.0107)  time: 0.1966  data: 0.0002  max mem: 5511
[08:36:53.519423] Epoch: [52]  [200/781]  eta: 0:01:56  lr: 0.000127  training_loss: 1.5731 (1.6160)  mae_loss: 0.2527 (0.2606)  classification_loss: 1.3281 (1.3456)  loss_mask: 0.0007 (0.0097)  time: 0.1990  data: 0.0002  max mem: 5511
[08:36:57.517757] Epoch: [52]  [220/781]  eta: 0:01:52  lr: 0.000126  training_loss: 1.6167 (1.6152)  mae_loss: 0.2656 (0.2606)  classification_loss: 1.3656 (1.3457)  loss_mask: 0.0004 (0.0089)  time: 0.1998  data: 0.0003  max mem: 5511
[08:37:01.479025] Epoch: [52]  [240/781]  eta: 0:01:48  lr: 0.000126  training_loss: 1.6184 (1.6143)  mae_loss: 0.2518 (0.2605)  classification_loss: 1.3407 (1.3456)  loss_mask: 0.0004 (0.0082)  time: 0.1980  data: 0.0002  max mem: 5511
[08:37:05.439458] Epoch: [52]  [260/781]  eta: 0:01:44  lr: 0.000126  training_loss: 1.5889 (1.6137)  mae_loss: 0.2473 (0.2605)  classification_loss: 1.3547 (1.3457)  loss_mask: 0.0004 (0.0076)  time: 0.1979  data: 0.0002  max mem: 5511
[08:37:09.410229] Epoch: [52]  [280/781]  eta: 0:01:40  lr: 0.000126  training_loss: 1.6181 (1.6135)  mae_loss: 0.2724 (0.2612)  classification_loss: 1.3411 (1.3452)  loss_mask: 0.0003 (0.0071)  time: 0.1985  data: 0.0002  max mem: 5511
[08:37:13.344370] Epoch: [52]  [300/781]  eta: 0:01:36  lr: 0.000126  training_loss: 1.5687 (1.6134)  mae_loss: 0.2640 (0.2614)  classification_loss: 1.3225 (1.3453)  loss_mask: 0.0003 (0.0066)  time: 0.1966  data: 0.0003  max mem: 5511
[08:37:17.265635] Epoch: [52]  [320/781]  eta: 0:01:32  lr: 0.000126  training_loss: 1.5826 (1.6116)  mae_loss: 0.2603 (0.2618)  classification_loss: 1.3020 (1.3436)  loss_mask: 0.0004 (0.0063)  time: 0.1960  data: 0.0001  max mem: 5511
[08:37:21.242710] Epoch: [52]  [340/781]  eta: 0:01:28  lr: 0.000126  training_loss: 1.6152 (1.6125)  mae_loss: 0.2630 (0.2626)  classification_loss: 1.3598 (1.3439)  loss_mask: 0.0003 (0.0059)  time: 0.1988  data: 0.0002  max mem: 5511
[08:37:25.207322] Epoch: [52]  [360/781]  eta: 0:01:24  lr: 0.000126  training_loss: 1.6377 (1.6138)  mae_loss: 0.2515 (0.2627)  classification_loss: 1.3749 (1.3454)  loss_mask: 0.0003 (0.0056)  time: 0.1981  data: 0.0002  max mem: 5511
[08:37:29.201700] Epoch: [52]  [380/781]  eta: 0:01:20  lr: 0.000126  training_loss: 1.5884 (1.6123)  mae_loss: 0.2633 (0.2628)  classification_loss: 1.3427 (1.3442)  loss_mask: 0.0002 (0.0053)  time: 0.1996  data: 0.0002  max mem: 5511
[08:37:33.181103] Epoch: [52]  [400/781]  eta: 0:01:16  lr: 0.000125  training_loss: 1.6351 (1.6133)  mae_loss: 0.2538 (0.2626)  classification_loss: 1.3547 (1.3456)  loss_mask: 0.0003 (0.0051)  time: 0.1989  data: 0.0002  max mem: 5511
[08:37:37.131063] Epoch: [52]  [420/781]  eta: 0:01:12  lr: 0.000125  training_loss: 1.6107 (1.6123)  mae_loss: 0.2474 (0.2622)  classification_loss: 1.3516 (1.3447)  loss_mask: 0.0003 (0.0055)  time: 0.1974  data: 0.0002  max mem: 5511
[08:37:41.062349] Epoch: [52]  [440/781]  eta: 0:01:08  lr: 0.000125  training_loss: 1.6921 (1.6179)  mae_loss: 0.2546 (0.2619)  classification_loss: 1.2558 (1.3416)  loss_mask: 0.1945 (0.0144)  time: 0.1965  data: 0.0002  max mem: 5511
[08:37:45.049788] Epoch: [52]  [460/781]  eta: 0:01:04  lr: 0.000125  training_loss: 1.6794 (1.6233)  mae_loss: 0.2577 (0.2619)  classification_loss: 1.3341 (1.3427)  loss_mask: 0.0881 (0.0187)  time: 0.1992  data: 0.0001  max mem: 5511
[08:37:48.998423] Epoch: [52]  [480/781]  eta: 0:01:00  lr: 0.000125  training_loss: 1.5962 (1.6231)  mae_loss: 0.2319 (0.2611)  classification_loss: 1.3417 (1.3423)  loss_mask: 0.0364 (0.0196)  time: 0.1973  data: 0.0002  max mem: 5511
[08:37:52.952065] Epoch: [52]  [500/781]  eta: 0:00:56  lr: 0.000125  training_loss: 1.5927 (1.6223)  mae_loss: 0.2623 (0.2615)  classification_loss: 1.3119 (1.3413)  loss_mask: 0.0148 (0.0195)  time: 0.1975  data: 0.0003  max mem: 5511
[08:37:56.911616] Epoch: [52]  [520/781]  eta: 0:00:52  lr: 0.000125  training_loss: 1.6020 (1.6213)  mae_loss: 0.2519 (0.2612)  classification_loss: 1.3373 (1.3412)  loss_mask: 0.0059 (0.0190)  time: 0.1979  data: 0.0002  max mem: 5511
[08:38:00.867169] Epoch: [52]  [540/781]  eta: 0:00:48  lr: 0.000125  training_loss: 1.6634 (1.6224)  mae_loss: 0.2709 (0.2617)  classification_loss: 1.3854 (1.3421)  loss_mask: 0.0057 (0.0186)  time: 0.1977  data: 0.0002  max mem: 5511
[08:38:04.799185] Epoch: [52]  [560/781]  eta: 0:00:44  lr: 0.000125  training_loss: 1.6109 (1.6224)  mae_loss: 0.2716 (0.2621)  classification_loss: 1.3476 (1.3421)  loss_mask: 0.0038 (0.0182)  time: 0.1965  data: 0.0002  max mem: 5511
[08:38:08.742281] Epoch: [52]  [580/781]  eta: 0:00:40  lr: 0.000125  training_loss: 1.6346 (1.6227)  mae_loss: 0.2640 (0.2625)  classification_loss: 1.3457 (1.3425)  loss_mask: 0.0030 (0.0178)  time: 0.1971  data: 0.0002  max mem: 5511
[08:38:12.734150] Epoch: [52]  [600/781]  eta: 0:00:36  lr: 0.000124  training_loss: 1.5909 (1.6224)  mae_loss: 0.2556 (0.2625)  classification_loss: 1.3321 (1.3426)  loss_mask: 0.0016 (0.0173)  time: 0.1995  data: 0.0002  max mem: 5511
[08:38:16.710722] Epoch: [52]  [620/781]  eta: 0:00:32  lr: 0.000124  training_loss: 1.6236 (1.6221)  mae_loss: 0.2597 (0.2625)  classification_loss: 1.3217 (1.3429)  loss_mask: 0.0013 (0.0168)  time: 0.1987  data: 0.0002  max mem: 5511
[08:38:20.664176] Epoch: [52]  [640/781]  eta: 0:00:28  lr: 0.000124  training_loss: 1.5961 (1.6211)  mae_loss: 0.2602 (0.2624)  classification_loss: 1.3187 (1.3423)  loss_mask: 0.0009 (0.0163)  time: 0.1976  data: 0.0002  max mem: 5511
[08:38:24.618858] Epoch: [52]  [660/781]  eta: 0:00:24  lr: 0.000124  training_loss: 1.6813 (1.6223)  mae_loss: 0.2678 (0.2627)  classification_loss: 1.3952 (1.3437)  loss_mask: 0.0012 (0.0159)  time: 0.1976  data: 0.0003  max mem: 5511
[08:38:28.576936] Epoch: [52]  [680/781]  eta: 0:00:20  lr: 0.000124  training_loss: 1.5874 (1.6215)  mae_loss: 0.2467 (0.2626)  classification_loss: 1.3349 (1.3435)  loss_mask: 0.0005 (0.0154)  time: 0.1978  data: 0.0002  max mem: 5511
[08:38:32.531987] Epoch: [52]  [700/781]  eta: 0:00:16  lr: 0.000124  training_loss: 1.6142 (1.6214)  mae_loss: 0.2549 (0.2624)  classification_loss: 1.3483 (1.3440)  loss_mask: 0.0005 (0.0150)  time: 0.1976  data: 0.0003  max mem: 5511
[08:38:36.513450] Epoch: [52]  [720/781]  eta: 0:00:12  lr: 0.000124  training_loss: 1.6648 (1.6219)  mae_loss: 0.2687 (0.2626)  classification_loss: 1.3821 (1.3447)  loss_mask: 0.0004 (0.0146)  time: 0.1990  data: 0.0003  max mem: 5511
[08:38:40.483867] Epoch: [52]  [740/781]  eta: 0:00:08  lr: 0.000124  training_loss: 1.5785 (1.6204)  mae_loss: 0.2525 (0.2625)  classification_loss: 1.2947 (1.3437)  loss_mask: 0.0005 (0.0143)  time: 0.1985  data: 0.0002  max mem: 5511
[08:38:44.441518] Epoch: [52]  [760/781]  eta: 0:00:04  lr: 0.000124  training_loss: 1.6306 (1.6209)  mae_loss: 0.2608 (0.2626)  classification_loss: 1.3558 (1.3444)  loss_mask: 0.0012 (0.0139)  time: 0.1977  data: 0.0002  max mem: 5511
[08:38:48.364036] Epoch: [52]  [780/781]  eta: 0:00:00  lr: 0.000123  training_loss: 1.5870 (1.6211)  mae_loss: 0.2469 (0.2625)  classification_loss: 1.3459 (1.3450)  loss_mask: 0.0008 (0.0137)  time: 0.1961  data: 0.0002  max mem: 5511
[08:38:48.572291] Epoch: [52] Total time: 0:02:35 (0.1991 s / it)
[08:38:48.574099] Averaged stats: lr: 0.000123  training_loss: 1.5870 (1.6211)  mae_loss: 0.2469 (0.2625)  classification_loss: 1.3459 (1.3450)  loss_mask: 0.0008 (0.0137)
[08:38:49.178937] Test:  [  0/157]  eta: 0:01:34  testing_loss: 0.6584 (0.6584)  acc1: 79.6875 (79.6875)  acc5: 96.8750 (96.8750)  time: 0.5995  data: 0.5697  max mem: 5511
[08:38:49.464068] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.6584 (0.6413)  acc1: 79.6875 (80.3977)  acc5: 98.4375 (99.0057)  time: 0.0803  data: 0.0519  max mem: 5511
[08:38:49.746515] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.6423 (0.6274)  acc1: 81.2500 (81.1756)  acc5: 98.4375 (99.0327)  time: 0.0282  data: 0.0002  max mem: 5511
[08:38:50.030616] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.6218 (0.6366)  acc1: 79.6875 (80.5948)  acc5: 98.4375 (98.8407)  time: 0.0282  data: 0.0002  max mem: 5511
[08:38:50.321061] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.6129 (0.6399)  acc1: 79.6875 (80.2210)  acc5: 98.4375 (98.8948)  time: 0.0286  data: 0.0002  max mem: 5511
[08:38:50.608535] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6127 (0.6318)  acc1: 81.2500 (80.9436)  acc5: 98.4375 (98.8664)  time: 0.0288  data: 0.0002  max mem: 5511
[08:38:50.891165] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5840 (0.6267)  acc1: 82.8125 (80.9682)  acc5: 100.0000 (98.8986)  time: 0.0284  data: 0.0002  max mem: 5511
[08:38:51.174530] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6018 (0.6235)  acc1: 81.2500 (81.0299)  acc5: 100.0000 (98.8556)  time: 0.0282  data: 0.0002  max mem: 5511
[08:38:51.457292] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6303 (0.6377)  acc1: 79.6875 (80.4012)  acc5: 98.4375 (98.8426)  time: 0.0282  data: 0.0002  max mem: 5511
[08:38:51.742248] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6634 (0.6346)  acc1: 79.6875 (80.5632)  acc5: 100.0000 (98.9183)  time: 0.0283  data: 0.0002  max mem: 5511
[08:38:52.027235] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.6305 (0.6390)  acc1: 81.2500 (80.4146)  acc5: 100.0000 (98.9325)  time: 0.0284  data: 0.0002  max mem: 5511
[08:38:52.310370] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6860 (0.6419)  acc1: 79.6875 (80.2365)  acc5: 100.0000 (98.9302)  time: 0.0283  data: 0.0002  max mem: 5511
[08:38:52.595021] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6361 (0.6396)  acc1: 78.1250 (80.0749)  acc5: 98.4375 (98.9282)  time: 0.0283  data: 0.0002  max mem: 5511
[08:38:52.880024] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6361 (0.6420)  acc1: 76.5625 (79.8426)  acc5: 98.4375 (98.9623)  time: 0.0284  data: 0.0002  max mem: 5511
[08:38:53.167984] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6459 (0.6414)  acc1: 78.1250 (79.8759)  acc5: 100.0000 (98.9916)  time: 0.0284  data: 0.0002  max mem: 5511
[08:38:53.451187] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6351 (0.6396)  acc1: 79.6875 (79.9048)  acc5: 100.0000 (99.0066)  time: 0.0283  data: 0.0002  max mem: 5511
[08:38:53.601938] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6059 (0.6390)  acc1: 79.6875 (79.9000)  acc5: 100.0000 (99.0100)  time: 0.0273  data: 0.0001  max mem: 5511
[08:38:53.760920] Test: Total time: 0:00:05 (0.0330 s / it)
[08:38:53.762566] * Acc@1 79.900 Acc@5 99.010 loss 0.639
[08:38:53.763129] Accuracy of the network on the 10000 test images: 79.9%
[08:38:53.763335] Max accuracy: 80.19%
[08:38:53.885314] log_dir: ./output_dir
[08:38:54.876746] Epoch: [53]  [  0/781]  eta: 0:12:52  lr: 0.000123  training_loss: 1.6611 (1.6611)  mae_loss: 0.2692 (0.2692)  classification_loss: 1.3916 (1.3916)  loss_mask: 0.0002 (0.0002)  time: 0.9895  data: 0.7708  max mem: 5511
[08:38:58.811564] Epoch: [53]  [ 20/781]  eta: 0:02:58  lr: 0.000123  training_loss: 1.5608 (1.5806)  mae_loss: 0.2526 (0.2638)  classification_loss: 1.2673 (1.3121)  loss_mask: 0.0013 (0.0047)  time: 0.1966  data: 0.0001  max mem: 5511
[08:39:02.774751] Epoch: [53]  [ 40/781]  eta: 0:02:40  lr: 0.000123  training_loss: 1.5967 (1.5970)  mae_loss: 0.2481 (0.2598)  classification_loss: 1.3426 (1.3337)  loss_mask: 0.0011 (0.0035)  time: 0.1981  data: 0.0002  max mem: 5511
[08:39:06.711695] Epoch: [53]  [ 60/781]  eta: 0:02:31  lr: 0.000123  training_loss: 1.6148 (1.6017)  mae_loss: 0.2604 (0.2625)  classification_loss: 1.3403 (1.3363)  loss_mask: 0.0009 (0.0028)  time: 0.1967  data: 0.0002  max mem: 5511
[08:39:10.667320] Epoch: [53]  [ 80/781]  eta: 0:02:25  lr: 0.000123  training_loss: 1.6079 (1.5995)  mae_loss: 0.2620 (0.2622)  classification_loss: 1.3365 (1.3351)  loss_mask: 0.0003 (0.0022)  time: 0.1977  data: 0.0002  max mem: 5511
[08:39:14.639222] Epoch: [53]  [100/781]  eta: 0:02:19  lr: 0.000123  training_loss: 1.6427 (1.6058)  mae_loss: 0.2565 (0.2627)  classification_loss: 1.3746 (1.3410)  loss_mask: 0.0005 (0.0021)  time: 0.1985  data: 0.0002  max mem: 5511
[08:39:18.572610] Epoch: [53]  [120/781]  eta: 0:02:14  lr: 0.000123  training_loss: 1.5772 (1.6021)  mae_loss: 0.2455 (0.2599)  classification_loss: 1.3109 (1.3404)  loss_mask: 0.0003 (0.0018)  time: 0.1966  data: 0.0002  max mem: 5511
[08:39:22.516215] Epoch: [53]  [140/781]  eta: 0:02:10  lr: 0.000123  training_loss: 1.5653 (1.5968)  mae_loss: 0.2682 (0.2604)  classification_loss: 1.3030 (1.3342)  loss_mask: 0.0005 (0.0022)  time: 0.1971  data: 0.0002  max mem: 5511
[08:39:26.465721] Epoch: [53]  [160/781]  eta: 0:02:05  lr: 0.000123  training_loss: 1.5883 (1.5965)  mae_loss: 0.2484 (0.2606)  classification_loss: 1.3136 (1.3337)  loss_mask: 0.0007 (0.0022)  time: 0.1974  data: 0.0002  max mem: 5511
[08:39:30.401456] Epoch: [53]  [180/781]  eta: 0:02:01  lr: 0.000122  training_loss: 1.5919 (1.5977)  mae_loss: 0.2614 (0.2607)  classification_loss: 1.3444 (1.3350)  loss_mask: 0.0004 (0.0020)  time: 0.1967  data: 0.0002  max mem: 5511
[08:39:34.408504] Epoch: [53]  [200/781]  eta: 0:01:57  lr: 0.000122  training_loss: 1.5964 (1.5981)  mae_loss: 0.2574 (0.2612)  classification_loss: 1.3162 (1.3337)  loss_mask: 0.0004 (0.0033)  time: 0.2003  data: 0.0003  max mem: 5511
[08:39:38.352649] Epoch: [53]  [220/781]  eta: 0:01:52  lr: 0.000122  training_loss: 1.6739 (1.6053)  mae_loss: 0.2513 (0.2608)  classification_loss: 1.3467 (1.3329)  loss_mask: 0.0632 (0.0115)  time: 0.1971  data: 0.0003  max mem: 5511
[08:39:42.371355] Epoch: [53]  [240/781]  eta: 0:01:48  lr: 0.000122  training_loss: 1.5628 (1.6033)  mae_loss: 0.2565 (0.2606)  classification_loss: 1.2853 (1.3296)  loss_mask: 0.0244 (0.0131)  time: 0.2009  data: 0.0002  max mem: 5511
[08:39:46.351836] Epoch: [53]  [260/781]  eta: 0:01:44  lr: 0.000122  training_loss: 1.5865 (1.6035)  mae_loss: 0.2634 (0.2611)  classification_loss: 1.3256 (1.3295)  loss_mask: 0.0086 (0.0130)  time: 0.1989  data: 0.0003  max mem: 5511
[08:39:50.315870] Epoch: [53]  [280/781]  eta: 0:01:40  lr: 0.000122  training_loss: 1.6133 (1.6036)  mae_loss: 0.2722 (0.2616)  classification_loss: 1.3396 (1.3296)  loss_mask: 0.0042 (0.0125)  time: 0.1981  data: 0.0003  max mem: 5511
[08:39:54.280777] Epoch: [53]  [300/781]  eta: 0:01:36  lr: 0.000122  training_loss: 1.6094 (1.6044)  mae_loss: 0.2726 (0.2624)  classification_loss: 1.3307 (1.3302)  loss_mask: 0.0019 (0.0118)  time: 0.1982  data: 0.0002  max mem: 5511
[08:39:58.226124] Epoch: [53]  [320/781]  eta: 0:01:32  lr: 0.000122  training_loss: 1.5471 (1.6025)  mae_loss: 0.2505 (0.2619)  classification_loss: 1.3075 (1.3293)  loss_mask: 0.0011 (0.0112)  time: 0.1972  data: 0.0003  max mem: 5511
[08:40:02.230384] Epoch: [53]  [340/781]  eta: 0:01:28  lr: 0.000122  training_loss: 1.5683 (1.6000)  mae_loss: 0.2612 (0.2618)  classification_loss: 1.3079 (1.3272)  loss_mask: 0.0019 (0.0110)  time: 0.2001  data: 0.0002  max mem: 5511
[08:40:06.171919] Epoch: [53]  [360/781]  eta: 0:01:24  lr: 0.000122  training_loss: 1.6013 (1.6015)  mae_loss: 0.2551 (0.2619)  classification_loss: 1.3255 (1.3292)  loss_mask: 0.0011 (0.0105)  time: 0.1970  data: 0.0003  max mem: 5511
[08:40:10.121468] Epoch: [53]  [380/781]  eta: 0:01:20  lr: 0.000121  training_loss: 1.5888 (1.6010)  mae_loss: 0.2555 (0.2614)  classification_loss: 1.3236 (1.3297)  loss_mask: 0.0008 (0.0100)  time: 0.1974  data: 0.0004  max mem: 5511
[08:40:14.104370] Epoch: [53]  [400/781]  eta: 0:01:16  lr: 0.000121  training_loss: 1.5711 (1.6010)  mae_loss: 0.2577 (0.2612)  classification_loss: 1.3026 (1.3303)  loss_mask: 0.0006 (0.0095)  time: 0.1991  data: 0.0005  max mem: 5511
[08:40:18.075321] Epoch: [53]  [420/781]  eta: 0:01:12  lr: 0.000121  training_loss: 1.5659 (1.5993)  mae_loss: 0.2607 (0.2609)  classification_loss: 1.3266 (1.3293)  loss_mask: 0.0006 (0.0091)  time: 0.1985  data: 0.0002  max mem: 5511
[08:40:22.005829] Epoch: [53]  [440/781]  eta: 0:01:08  lr: 0.000121  training_loss: 1.5599 (1.5985)  mae_loss: 0.2605 (0.2611)  classification_loss: 1.2878 (1.3287)  loss_mask: 0.0004 (0.0087)  time: 0.1964  data: 0.0003  max mem: 5511
[08:40:25.960896] Epoch: [53]  [460/781]  eta: 0:01:04  lr: 0.000121  training_loss: 1.5897 (1.5981)  mae_loss: 0.2597 (0.2611)  classification_loss: 1.3329 (1.3287)  loss_mask: 0.0003 (0.0083)  time: 0.1977  data: 0.0002  max mem: 5511
[08:40:29.949093] Epoch: [53]  [480/781]  eta: 0:01:00  lr: 0.000121  training_loss: 1.5849 (1.5988)  mae_loss: 0.2634 (0.2613)  classification_loss: 1.3190 (1.3296)  loss_mask: 0.0004 (0.0080)  time: 0.1993  data: 0.0002  max mem: 5511
[08:40:33.900910] Epoch: [53]  [500/781]  eta: 0:00:56  lr: 0.000121  training_loss: 1.5812 (1.5997)  mae_loss: 0.2562 (0.2614)  classification_loss: 1.3297 (1.3306)  loss_mask: 0.0004 (0.0077)  time: 0.1975  data: 0.0002  max mem: 5511
[08:40:37.845273] Epoch: [53]  [520/781]  eta: 0:00:52  lr: 0.000121  training_loss: 1.6082 (1.6004)  mae_loss: 0.2676 (0.2616)  classification_loss: 1.3266 (1.3313)  loss_mask: 0.0003 (0.0074)  time: 0.1971  data: 0.0002  max mem: 5511
[08:40:41.789242] Epoch: [53]  [540/781]  eta: 0:00:48  lr: 0.000121  training_loss: 1.5986 (1.6008)  mae_loss: 0.2570 (0.2617)  classification_loss: 1.3391 (1.3320)  loss_mask: 0.0003 (0.0072)  time: 0.1971  data: 0.0002  max mem: 5511
[08:40:45.779054] Epoch: [53]  [560/781]  eta: 0:00:44  lr: 0.000120  training_loss: 1.5582 (1.6004)  mae_loss: 0.2548 (0.2616)  classification_loss: 1.3040 (1.3319)  loss_mask: 0.0003 (0.0069)  time: 0.1994  data: 0.0003  max mem: 5511
[08:40:49.746145] Epoch: [53]  [580/781]  eta: 0:00:40  lr: 0.000120  training_loss: 1.5953 (1.6004)  mae_loss: 0.2804 (0.2622)  classification_loss: 1.3229 (1.3316)  loss_mask: 0.0003 (0.0067)  time: 0.1983  data: 0.0002  max mem: 5511
[08:40:53.678607] Epoch: [53]  [600/781]  eta: 0:00:36  lr: 0.000120  training_loss: 1.5660 (1.5997)  mae_loss: 0.2658 (0.2623)  classification_loss: 1.3043 (1.3309)  loss_mask: 0.0002 (0.0065)  time: 0.1965  data: 0.0002  max mem: 5511
[08:40:57.632728] Epoch: [53]  [620/781]  eta: 0:00:32  lr: 0.000120  training_loss: 1.5564 (1.5993)  mae_loss: 0.2595 (0.2624)  classification_loss: 1.3034 (1.3306)  loss_mask: 0.0003 (0.0063)  time: 0.1976  data: 0.0002  max mem: 5511
[08:41:01.581455] Epoch: [53]  [640/781]  eta: 0:00:28  lr: 0.000120  training_loss: 1.5488 (1.5979)  mae_loss: 0.2620 (0.2622)  classification_loss: 1.2979 (1.3296)  loss_mask: 0.0002 (0.0061)  time: 0.1973  data: 0.0002  max mem: 5511
[08:41:05.555966] Epoch: [53]  [660/781]  eta: 0:00:24  lr: 0.000120  training_loss: 1.5738 (1.5973)  mae_loss: 0.2494 (0.2618)  classification_loss: 1.3085 (1.3296)  loss_mask: 0.0002 (0.0059)  time: 0.1986  data: 0.0002  max mem: 5511
[08:41:09.504008] Epoch: [53]  [680/781]  eta: 0:00:20  lr: 0.000120  training_loss: 1.6299 (1.5984)  mae_loss: 0.2632 (0.2620)  classification_loss: 1.3639 (1.3307)  loss_mask: 0.0002 (0.0057)  time: 0.1973  data: 0.0002  max mem: 5511
[08:41:13.464675] Epoch: [53]  [700/781]  eta: 0:00:16  lr: 0.000120  training_loss: 1.6936 (1.6001)  mae_loss: 0.2614 (0.2622)  classification_loss: 1.3923 (1.3323)  loss_mask: 0.0003 (0.0056)  time: 0.1979  data: 0.0003  max mem: 5511
[08:41:17.427932] Epoch: [53]  [720/781]  eta: 0:00:12  lr: 0.000120  training_loss: 1.6342 (1.6015)  mae_loss: 0.2632 (0.2622)  classification_loss: 1.3845 (1.3339)  loss_mask: 0.0002 (0.0054)  time: 0.1981  data: 0.0002  max mem: 5511
[08:41:21.364074] Epoch: [53]  [740/781]  eta: 0:00:08  lr: 0.000120  training_loss: 1.5656 (1.6004)  mae_loss: 0.2536 (0.2622)  classification_loss: 1.2969 (1.3329)  loss_mask: 0.0002 (0.0053)  time: 0.1967  data: 0.0002  max mem: 5511
[08:41:25.287911] Epoch: [53]  [760/781]  eta: 0:00:04  lr: 0.000119  training_loss: 1.5898 (1.6005)  mae_loss: 0.2484 (0.2619)  classification_loss: 1.3397 (1.3335)  loss_mask: 0.0002 (0.0052)  time: 0.1961  data: 0.0002  max mem: 5511
[08:41:29.193889] Epoch: [53]  [780/781]  eta: 0:00:00  lr: 0.000119  training_loss: 1.5885 (1.6007)  mae_loss: 0.2600 (0.2619)  classification_loss: 1.3131 (1.3337)  loss_mask: 0.0002 (0.0050)  time: 0.1952  data: 0.0002  max mem: 5511
[08:41:29.369620] Epoch: [53] Total time: 0:02:35 (0.1991 s / it)
[08:41:29.370175] Averaged stats: lr: 0.000119  training_loss: 1.5885 (1.6007)  mae_loss: 0.2600 (0.2619)  classification_loss: 1.3131 (1.3337)  loss_mask: 0.0002 (0.0050)
[08:41:30.017356] Test:  [  0/157]  eta: 0:01:40  testing_loss: 0.5711 (0.5711)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.6429  data: 0.6111  max mem: 5511
[08:41:30.305660] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.6291 (0.6327)  acc1: 81.2500 (80.9659)  acc5: 98.4375 (98.8636)  time: 0.0845  data: 0.0557  max mem: 5511
[08:41:30.587857] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.6289 (0.6175)  acc1: 81.2500 (80.6548)  acc5: 100.0000 (99.0327)  time: 0.0284  data: 0.0002  max mem: 5511
[08:41:30.870543] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5918 (0.6244)  acc1: 79.6875 (79.8387)  acc5: 100.0000 (98.8911)  time: 0.0281  data: 0.0001  max mem: 5511
[08:41:31.157494] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6074 (0.6281)  acc1: 81.2500 (80.1448)  acc5: 100.0000 (98.8948)  time: 0.0284  data: 0.0002  max mem: 5511
[08:41:31.440344] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5766 (0.6155)  acc1: 84.3750 (81.1275)  acc5: 98.4375 (98.8971)  time: 0.0284  data: 0.0002  max mem: 5511
[08:41:31.723311] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5631 (0.6091)  acc1: 82.8125 (80.9682)  acc5: 100.0000 (98.9754)  time: 0.0282  data: 0.0002  max mem: 5511
[08:41:32.005938] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5692 (0.6060)  acc1: 81.2500 (80.8979)  acc5: 100.0000 (98.9877)  time: 0.0282  data: 0.0002  max mem: 5511
[08:41:32.299223] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6141 (0.6188)  acc1: 78.1250 (80.3048)  acc5: 98.4375 (98.9776)  time: 0.0287  data: 0.0002  max mem: 5511
[08:41:32.585847] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6311 (0.6146)  acc1: 78.1250 (80.5288)  acc5: 98.4375 (98.9011)  time: 0.0289  data: 0.0002  max mem: 5511
[08:41:32.874526] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5750 (0.6166)  acc1: 82.8125 (80.6002)  acc5: 98.4375 (98.9325)  time: 0.0286  data: 0.0002  max mem: 5511
[08:41:33.161602] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6706 (0.6195)  acc1: 81.2500 (80.5039)  acc5: 100.0000 (98.9302)  time: 0.0286  data: 0.0002  max mem: 5511
[08:41:33.447673] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6198 (0.6162)  acc1: 79.6875 (80.6302)  acc5: 100.0000 (98.9540)  time: 0.0284  data: 0.0002  max mem: 5511
[08:41:33.732375] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6127 (0.6178)  acc1: 81.2500 (80.5940)  acc5: 100.0000 (98.9862)  time: 0.0284  data: 0.0002  max mem: 5511
[08:41:34.017352] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6204 (0.6174)  acc1: 81.2500 (80.5629)  acc5: 100.0000 (99.0027)  time: 0.0283  data: 0.0002  max mem: 5511
[08:41:34.297561] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6204 (0.6159)  acc1: 81.2500 (80.6084)  acc5: 98.4375 (98.9859)  time: 0.0281  data: 0.0001  max mem: 5511
[08:41:34.447928] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5875 (0.6155)  acc1: 81.2500 (80.5800)  acc5: 98.4375 (98.9900)  time: 0.0270  data: 0.0001  max mem: 5511
[08:41:34.633267] Test: Total time: 0:00:05 (0.0335 s / it)
[08:41:34.633819] * Acc@1 80.580 Acc@5 98.990 loss 0.615
[08:41:34.634314] Accuracy of the network on the 10000 test images: 80.6%
[08:41:34.634606] Max accuracy: 80.58%
[08:41:34.736925] log_dir: ./output_dir
[08:41:35.600822] Epoch: [54]  [  0/781]  eta: 0:11:13  lr: 0.000119  training_loss: 1.5721 (1.5721)  mae_loss: 0.2925 (0.2925)  classification_loss: 1.2794 (1.2794)  loss_mask: 0.0002 (0.0002)  time: 0.8622  data: 0.6540  max mem: 5511
[08:41:39.546105] Epoch: [54]  [ 20/781]  eta: 0:02:54  lr: 0.000119  training_loss: 1.5199 (1.5571)  mae_loss: 0.2446 (0.2484)  classification_loss: 1.2936 (1.3084)  loss_mask: 0.0002 (0.0003)  time: 0.1972  data: 0.0004  max mem: 5511
[08:41:43.485202] Epoch: [54]  [ 40/781]  eta: 0:02:38  lr: 0.000119  training_loss: 1.6229 (1.5874)  mae_loss: 0.2658 (0.2594)  classification_loss: 1.3597 (1.3277)  loss_mask: 0.0003 (0.0003)  time: 0.1969  data: 0.0002  max mem: 5511
[08:41:47.428034] Epoch: [54]  [ 60/781]  eta: 0:02:29  lr: 0.000119  training_loss: 1.6574 (1.5995)  mae_loss: 0.2618 (0.2635)  classification_loss: 1.3966 (1.3357)  loss_mask: 0.0002 (0.0003)  time: 0.1970  data: 0.0002  max mem: 5511
[08:41:51.376011] Epoch: [54]  [ 80/781]  eta: 0:02:23  lr: 0.000119  training_loss: 1.5945 (1.5978)  mae_loss: 0.2611 (0.2638)  classification_loss: 1.3194 (1.3337)  loss_mask: 0.0002 (0.0003)  time: 0.1973  data: 0.0002  max mem: 5511
[08:41:55.310984] Epoch: [54]  [100/781]  eta: 0:02:18  lr: 0.000119  training_loss: 1.5663 (1.5963)  mae_loss: 0.2503 (0.2646)  classification_loss: 1.3083 (1.3314)  loss_mask: 0.0002 (0.0003)  time: 0.1966  data: 0.0002  max mem: 5511
[08:41:59.249927] Epoch: [54]  [120/781]  eta: 0:02:13  lr: 0.000119  training_loss: 1.5557 (1.5926)  mae_loss: 0.2417 (0.2628)  classification_loss: 1.2901 (1.3296)  loss_mask: 0.0002 (0.0002)  time: 0.1969  data: 0.0002  max mem: 5511
[08:42:03.225395] Epoch: [54]  [140/781]  eta: 0:02:09  lr: 0.000119  training_loss: 1.5243 (1.5882)  mae_loss: 0.2589 (0.2620)  classification_loss: 1.2730 (1.3259)  loss_mask: 0.0002 (0.0002)  time: 0.1987  data: 0.0003  max mem: 5511
[08:42:07.186392] Epoch: [54]  [160/781]  eta: 0:02:05  lr: 0.000118  training_loss: 1.5797 (1.5885)  mae_loss: 0.2705 (0.2629)  classification_loss: 1.3312 (1.3254)  loss_mask: 0.0002 (0.0002)  time: 0.1980  data: 0.0002  max mem: 5511
[08:42:11.154680] Epoch: [54]  [180/781]  eta: 0:02:00  lr: 0.000118  training_loss: 1.6039 (1.5899)  mae_loss: 0.2677 (0.2643)  classification_loss: 1.3248 (1.3254)  loss_mask: 0.0002 (0.0002)  time: 0.1983  data: 0.0010  max mem: 5511
[08:42:15.086641] Epoch: [54]  [200/781]  eta: 0:01:56  lr: 0.000118  training_loss: 1.5919 (1.5901)  mae_loss: 0.2394 (0.2627)  classification_loss: 1.3366 (1.3272)  loss_mask: 0.0002 (0.0002)  time: 0.1965  data: 0.0002  max mem: 5511
[08:42:19.073757] Epoch: [54]  [220/781]  eta: 0:01:52  lr: 0.000118  training_loss: 1.6843 (1.5968)  mae_loss: 0.2555 (0.2630)  classification_loss: 1.4367 (1.3336)  loss_mask: 0.0002 (0.0002)  time: 0.1993  data: 0.0002  max mem: 5511
[08:42:23.034541] Epoch: [54]  [240/781]  eta: 0:01:48  lr: 0.000118  training_loss: 1.5736 (1.5973)  mae_loss: 0.2524 (0.2622)  classification_loss: 1.3199 (1.3348)  loss_mask: 0.0002 (0.0002)  time: 0.1979  data: 0.0002  max mem: 5511
[08:42:26.991464] Epoch: [54]  [260/781]  eta: 0:01:44  lr: 0.000118  training_loss: 1.6208 (1.5988)  mae_loss: 0.2592 (0.2629)  classification_loss: 1.3543 (1.3357)  loss_mask: 0.0002 (0.0002)  time: 0.1977  data: 0.0002  max mem: 5511
[08:42:30.956333] Epoch: [54]  [280/781]  eta: 0:01:40  lr: 0.000118  training_loss: 1.6222 (1.5993)  mae_loss: 0.2605 (0.2634)  classification_loss: 1.3470 (1.3357)  loss_mask: 0.0002 (0.0002)  time: 0.1981  data: 0.0002  max mem: 5511
[08:42:34.897553] Epoch: [54]  [300/781]  eta: 0:01:36  lr: 0.000118  training_loss: 1.6068 (1.6012)  mae_loss: 0.2658 (0.2642)  classification_loss: 1.3213 (1.3368)  loss_mask: 0.0002 (0.0002)  time: 0.1970  data: 0.0002  max mem: 5511
[08:42:38.851331] Epoch: [54]  [320/781]  eta: 0:01:32  lr: 0.000118  training_loss: 1.6170 (1.6010)  mae_loss: 0.2549 (0.2637)  classification_loss: 1.3421 (1.3371)  loss_mask: 0.0002 (0.0002)  time: 0.1976  data: 0.0002  max mem: 5511
[08:42:42.821536] Epoch: [54]  [340/781]  eta: 0:01:28  lr: 0.000118  training_loss: 1.5842 (1.6013)  mae_loss: 0.2807 (0.2649)  classification_loss: 1.3188 (1.3362)  loss_mask: 0.0001 (0.0002)  time: 0.1984  data: 0.0003  max mem: 5511
[08:42:46.756155] Epoch: [54]  [360/781]  eta: 0:01:23  lr: 0.000117  training_loss: 1.5706 (1.6017)  mae_loss: 0.2691 (0.2647)  classification_loss: 1.3330 (1.3368)  loss_mask: 0.0002 (0.0002)  time: 0.1966  data: 0.0002  max mem: 5511
[08:42:50.700688] Epoch: [54]  [380/781]  eta: 0:01:19  lr: 0.000117  training_loss: 1.5716 (1.6016)  mae_loss: 0.2599 (0.2648)  classification_loss: 1.2960 (1.3366)  loss_mask: 0.0002 (0.0002)  time: 0.1971  data: 0.0005  max mem: 5511
[08:42:54.641548] Epoch: [54]  [400/781]  eta: 0:01:15  lr: 0.000117  training_loss: 1.5804 (1.6002)  mae_loss: 0.2437 (0.2639)  classification_loss: 1.3106 (1.3361)  loss_mask: 0.0002 (0.0002)  time: 0.1969  data: 0.0002  max mem: 5511
[08:42:58.574611] Epoch: [54]  [420/781]  eta: 0:01:11  lr: 0.000117  training_loss: 1.5828 (1.5996)  mae_loss: 0.2581 (0.2638)  classification_loss: 1.3075 (1.3356)  loss_mask: 0.0002 (0.0002)  time: 0.1965  data: 0.0002  max mem: 5511
[08:43:02.504529] Epoch: [54]  [440/781]  eta: 0:01:07  lr: 0.000117  training_loss: 1.5802 (1.5995)  mae_loss: 0.2863 (0.2641)  classification_loss: 1.3360 (1.3352)  loss_mask: 0.0002 (0.0002)  time: 0.1964  data: 0.0002  max mem: 5511
[08:43:06.425740] Epoch: [54]  [460/781]  eta: 0:01:03  lr: 0.000117  training_loss: 1.5529 (1.5964)  mae_loss: 0.2593 (0.2635)  classification_loss: 1.2818 (1.3324)  loss_mask: 0.0001 (0.0005)  time: 0.1960  data: 0.0002  max mem: 5511
[08:43:10.339752] Epoch: [54]  [480/781]  eta: 0:00:59  lr: 0.000117  training_loss: 1.5803 (1.5972)  mae_loss: 0.2483 (0.2631)  classification_loss: 1.3328 (1.3334)  loss_mask: 0.0016 (0.0007)  time: 0.1956  data: 0.0002  max mem: 5511
[08:43:14.311236] Epoch: [54]  [500/781]  eta: 0:00:55  lr: 0.000117  training_loss: 1.6617 (1.6029)  mae_loss: 0.2698 (0.2636)  classification_loss: 1.3685 (1.3345)  loss_mask: 0.0396 (0.0048)  time: 0.1985  data: 0.0002  max mem: 5511
[08:43:18.244369] Epoch: [54]  [520/781]  eta: 0:00:51  lr: 0.000117  training_loss: 1.6539 (1.6050)  mae_loss: 0.2669 (0.2638)  classification_loss: 1.3085 (1.3332)  loss_mask: 0.0633 (0.0080)  time: 0.1966  data: 0.0002  max mem: 5511
[08:43:22.210002] Epoch: [54]  [540/781]  eta: 0:00:47  lr: 0.000116  training_loss: 1.6367 (1.6071)  mae_loss: 0.2730 (0.2641)  classification_loss: 1.3283 (1.3340)  loss_mask: 0.0260 (0.0090)  time: 0.1982  data: 0.0002  max mem: 5511
[08:43:26.143273] Epoch: [54]  [560/781]  eta: 0:00:43  lr: 0.000116  training_loss: 1.5976 (1.6065)  mae_loss: 0.2513 (0.2638)  classification_loss: 1.3031 (1.3330)  loss_mask: 0.0184 (0.0096)  time: 0.1966  data: 0.0002  max mem: 5511
[08:43:30.116681] Epoch: [54]  [580/781]  eta: 0:00:39  lr: 0.000116  training_loss: 1.6348 (1.6075)  mae_loss: 0.2616 (0.2637)  classification_loss: 1.3686 (1.3336)  loss_mask: 0.0124 (0.0102)  time: 0.1986  data: 0.0002  max mem: 5511
[08:43:34.044008] Epoch: [54]  [600/781]  eta: 0:00:35  lr: 0.000116  training_loss: 1.6178 (1.6085)  mae_loss: 0.2638 (0.2635)  classification_loss: 1.2842 (1.3332)  loss_mask: 0.0299 (0.0118)  time: 0.1963  data: 0.0003  max mem: 5511
[08:43:37.999174] Epoch: [54]  [620/781]  eta: 0:00:31  lr: 0.000116  training_loss: 1.5580 (1.6074)  mae_loss: 0.2531 (0.2634)  classification_loss: 1.2623 (1.3320)  loss_mask: 0.0128 (0.0120)  time: 0.1976  data: 0.0002  max mem: 5511
[08:43:41.994818] Epoch: [54]  [640/781]  eta: 0:00:27  lr: 0.000116  training_loss: 1.5508 (1.6066)  mae_loss: 0.2594 (0.2635)  classification_loss: 1.2734 (1.3314)  loss_mask: 0.0043 (0.0117)  time: 0.1997  data: 0.0002  max mem: 5511
[08:43:45.918495] Epoch: [54]  [660/781]  eta: 0:00:24  lr: 0.000116  training_loss: 1.5830 (1.6071)  mae_loss: 0.2642 (0.2638)  classification_loss: 1.3353 (1.3318)  loss_mask: 0.0015 (0.0115)  time: 0.1961  data: 0.0002  max mem: 5511
[08:43:49.877104] Epoch: [54]  [680/781]  eta: 0:00:20  lr: 0.000116  training_loss: 1.6439 (1.6083)  mae_loss: 0.2491 (0.2639)  classification_loss: 1.3783 (1.3331)  loss_mask: 0.0022 (0.0112)  time: 0.1978  data: 0.0002  max mem: 5511
[08:43:53.844367] Epoch: [54]  [700/781]  eta: 0:00:16  lr: 0.000116  training_loss: 1.5418 (1.6067)  mae_loss: 0.2585 (0.2637)  classification_loss: 1.2737 (1.3319)  loss_mask: 0.0027 (0.0111)  time: 0.1983  data: 0.0002  max mem: 5511
[08:43:57.815862] Epoch: [54]  [720/781]  eta: 0:00:12  lr: 0.000116  training_loss: 1.6103 (1.6079)  mae_loss: 0.2597 (0.2636)  classification_loss: 1.3512 (1.3330)  loss_mask: 0.0053 (0.0112)  time: 0.1985  data: 0.0002  max mem: 5511
[08:44:01.772068] Epoch: [54]  [740/781]  eta: 0:00:08  lr: 0.000115  training_loss: 1.6687 (1.6098)  mae_loss: 0.2664 (0.2638)  classification_loss: 1.3315 (1.3330)  loss_mask: 0.0371 (0.0130)  time: 0.1977  data: 0.0002  max mem: 5511
[08:44:05.729436] Epoch: [54]  [760/781]  eta: 0:00:04  lr: 0.000115  training_loss: 1.6150 (1.6105)  mae_loss: 0.2486 (0.2635)  classification_loss: 1.3440 (1.3340)  loss_mask: 0.0115 (0.0130)  time: 0.1978  data: 0.0002  max mem: 5511
[08:44:09.716370] Epoch: [54]  [780/781]  eta: 0:00:00  lr: 0.000115  training_loss: 1.6121 (1.6103)  mae_loss: 0.2623 (0.2635)  classification_loss: 1.3185 (1.3338)  loss_mask: 0.0058 (0.0129)  time: 0.1993  data: 0.0002  max mem: 5511
[08:44:09.895231] Epoch: [54] Total time: 0:02:35 (0.1987 s / it)
[08:44:09.895780] Averaged stats: lr: 0.000115  training_loss: 1.6121 (1.6103)  mae_loss: 0.2623 (0.2635)  classification_loss: 1.3185 (1.3338)  loss_mask: 0.0058 (0.0129)
[08:44:10.618444] Test:  [  0/157]  eta: 0:01:52  testing_loss: 0.5545 (0.5545)  acc1: 84.3750 (84.3750)  acc5: 96.8750 (96.8750)  time: 0.7180  data: 0.6848  max mem: 5511
[08:44:10.909621] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.6501 (0.6210)  acc1: 81.2500 (80.3977)  acc5: 98.4375 (99.0057)  time: 0.0916  data: 0.0624  max mem: 5511
[08:44:11.201231] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5961 (0.5969)  acc1: 81.2500 (81.9196)  acc5: 100.0000 (99.1815)  time: 0.0289  data: 0.0002  max mem: 5511
[08:44:11.487375] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5961 (0.6074)  acc1: 82.8125 (81.5020)  acc5: 100.0000 (98.8911)  time: 0.0287  data: 0.0002  max mem: 5511
[08:44:11.773877] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6135 (0.6153)  acc1: 79.6875 (81.0213)  acc5: 98.4375 (98.9710)  time: 0.0285  data: 0.0002  max mem: 5511
[08:44:12.061964] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5851 (0.6048)  acc1: 81.2500 (81.6176)  acc5: 100.0000 (98.9890)  time: 0.0286  data: 0.0002  max mem: 5511
[08:44:12.343499] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5643 (0.6015)  acc1: 82.8125 (81.6598)  acc5: 98.4375 (98.9242)  time: 0.0284  data: 0.0002  max mem: 5511
[08:44:12.625663] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5844 (0.5994)  acc1: 82.8125 (81.6241)  acc5: 98.4375 (98.8776)  time: 0.0280  data: 0.0002  max mem: 5511
[08:44:12.909100] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6090 (0.6086)  acc1: 79.6875 (81.2886)  acc5: 98.4375 (98.9005)  time: 0.0281  data: 0.0002  max mem: 5511
[08:44:13.191363] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6121 (0.6076)  acc1: 81.2500 (81.4732)  acc5: 98.4375 (98.9183)  time: 0.0282  data: 0.0002  max mem: 5511
[08:44:13.475030] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.6039 (0.6110)  acc1: 82.8125 (81.3892)  acc5: 100.0000 (98.9171)  time: 0.0282  data: 0.0002  max mem: 5511
[08:44:13.761688] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6467 (0.6137)  acc1: 78.1250 (81.1655)  acc5: 100.0000 (98.9020)  time: 0.0284  data: 0.0004  max mem: 5511
[08:44:14.053919] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5588 (0.6097)  acc1: 81.2500 (81.2887)  acc5: 100.0000 (98.9411)  time: 0.0288  data: 0.0003  max mem: 5511
[08:44:14.343364] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6050 (0.6104)  acc1: 81.2500 (81.1546)  acc5: 100.0000 (98.9981)  time: 0.0289  data: 0.0002  max mem: 5511
[08:44:14.630675] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5990 (0.6076)  acc1: 81.2500 (81.3830)  acc5: 100.0000 (99.0137)  time: 0.0287  data: 0.0002  max mem: 5511
[08:44:14.913867] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5946 (0.6046)  acc1: 82.8125 (81.3949)  acc5: 98.4375 (99.0066)  time: 0.0284  data: 0.0001  max mem: 5511
[08:44:15.067973] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5634 (0.6042)  acc1: 81.2500 (81.3900)  acc5: 98.4375 (99.0300)  time: 0.0273  data: 0.0001  max mem: 5511
[08:44:15.254658] Test: Total time: 0:00:05 (0.0341 s / it)
[08:44:15.255109] * Acc@1 81.390 Acc@5 99.030 loss 0.604
[08:44:15.255399] Accuracy of the network on the 10000 test images: 81.4%
[08:44:15.255601] Max accuracy: 81.39%
[08:44:15.438410] log_dir: ./output_dir
[08:44:16.415583] Epoch: [55]  [  0/781]  eta: 0:12:41  lr: 0.000115  training_loss: 1.4923 (1.4923)  mae_loss: 0.2364 (0.2364)  classification_loss: 1.2520 (1.2520)  loss_mask: 0.0039 (0.0039)  time: 0.9754  data: 0.7724  max mem: 5511
[08:44:20.353948] Epoch: [55]  [ 20/781]  eta: 0:02:57  lr: 0.000115  training_loss: 1.5552 (1.5934)  mae_loss: 0.2773 (0.2737)  classification_loss: 1.2842 (1.3159)  loss_mask: 0.0021 (0.0038)  time: 0.1968  data: 0.0003  max mem: 5511
[08:44:24.314603] Epoch: [55]  [ 40/781]  eta: 0:02:40  lr: 0.000115  training_loss: 1.5822 (1.5888)  mae_loss: 0.2591 (0.2692)  classification_loss: 1.3104 (1.3166)  loss_mask: 0.0018 (0.0030)  time: 0.1979  data: 0.0002  max mem: 5511
[08:44:28.247751] Epoch: [55]  [ 60/781]  eta: 0:02:31  lr: 0.000115  training_loss: 1.5706 (1.5911)  mae_loss: 0.2574 (0.2671)  classification_loss: 1.3282 (1.3216)  loss_mask: 0.0010 (0.0024)  time: 0.1966  data: 0.0002  max mem: 5511
[08:44:32.201125] Epoch: [55]  [ 80/781]  eta: 0:02:24  lr: 0.000115  training_loss: 1.5655 (1.5887)  mae_loss: 0.2493 (0.2649)  classification_loss: 1.3117 (1.3218)  loss_mask: 0.0008 (0.0020)  time: 0.1976  data: 0.0004  max mem: 5511
[08:44:36.172965] Epoch: [55]  [100/781]  eta: 0:02:19  lr: 0.000115  training_loss: 1.5770 (1.5884)  mae_loss: 0.2570 (0.2640)  classification_loss: 1.3302 (1.3226)  loss_mask: 0.0006 (0.0018)  time: 0.1985  data: 0.0003  max mem: 5511
[08:44:40.116369] Epoch: [55]  [120/781]  eta: 0:02:14  lr: 0.000115  training_loss: 1.5736 (1.5904)  mae_loss: 0.2526 (0.2631)  classification_loss: 1.3266 (1.3258)  loss_mask: 0.0004 (0.0015)  time: 0.1971  data: 0.0002  max mem: 5511
[08:44:44.045012] Epoch: [55]  [140/781]  eta: 0:02:09  lr: 0.000114  training_loss: 1.5627 (1.5849)  mae_loss: 0.2521 (0.2627)  classification_loss: 1.2947 (1.3208)  loss_mask: 0.0003 (0.0014)  time: 0.1963  data: 0.0002  max mem: 5511
[08:44:48.030378] Epoch: [55]  [160/781]  eta: 0:02:05  lr: 0.000114  training_loss: 1.5304 (1.5803)  mae_loss: 0.2558 (0.2616)  classification_loss: 1.2811 (1.3174)  loss_mask: 0.0003 (0.0013)  time: 0.1992  data: 0.0002  max mem: 5511
[08:44:51.990221] Epoch: [55]  [180/781]  eta: 0:02:01  lr: 0.000114  training_loss: 1.5726 (1.5805)  mae_loss: 0.2645 (0.2624)  classification_loss: 1.2825 (1.3169)  loss_mask: 0.0003 (0.0012)  time: 0.1979  data: 0.0002  max mem: 5511
[08:44:55.974799] Epoch: [55]  [200/781]  eta: 0:01:57  lr: 0.000114  training_loss: 1.5705 (1.5799)  mae_loss: 0.2575 (0.2621)  classification_loss: 1.3146 (1.3167)  loss_mask: 0.0004 (0.0011)  time: 0.1991  data: 0.0003  max mem: 5511
[08:44:59.952874] Epoch: [55]  [220/781]  eta: 0:01:52  lr: 0.000114  training_loss: 1.6058 (1.5824)  mae_loss: 0.2564 (0.2619)  classification_loss: 1.3525 (1.3195)  loss_mask: 0.0003 (0.0010)  time: 0.1988  data: 0.0003  max mem: 5511
[08:45:03.949695] Epoch: [55]  [240/781]  eta: 0:01:48  lr: 0.000114  training_loss: 1.5966 (1.5820)  mae_loss: 0.2711 (0.2623)  classification_loss: 1.3296 (1.3188)  loss_mask: 0.0002 (0.0010)  time: 0.1998  data: 0.0002  max mem: 5511
[08:45:07.921441] Epoch: [55]  [260/781]  eta: 0:01:44  lr: 0.000114  training_loss: 1.5908 (1.5828)  mae_loss: 0.2617 (0.2620)  classification_loss: 1.3135 (1.3199)  loss_mask: 0.0003 (0.0009)  time: 0.1985  data: 0.0002  max mem: 5511
[08:45:11.899556] Epoch: [55]  [280/781]  eta: 0:01:40  lr: 0.000114  training_loss: 1.5531 (1.5823)  mae_loss: 0.2644 (0.2626)  classification_loss: 1.2983 (1.3187)  loss_mask: 0.0004 (0.0009)  time: 0.1988  data: 0.0003  max mem: 5511
[08:45:15.888821] Epoch: [55]  [300/781]  eta: 0:01:36  lr: 0.000114  training_loss: 1.5941 (1.5826)  mae_loss: 0.2503 (0.2623)  classification_loss: 1.3313 (1.3194)  loss_mask: 0.0003 (0.0009)  time: 0.1994  data: 0.0003  max mem: 5511
[08:45:19.849061] Epoch: [55]  [320/781]  eta: 0:01:32  lr: 0.000114  training_loss: 1.6124 (1.5857)  mae_loss: 0.2745 (0.2633)  classification_loss: 1.3408 (1.3215)  loss_mask: 0.0002 (0.0009)  time: 0.1979  data: 0.0002  max mem: 5511
[08:45:23.872088] Epoch: [55]  [340/781]  eta: 0:01:28  lr: 0.000113  training_loss: 1.5614 (1.5846)  mae_loss: 0.2575 (0.2633)  classification_loss: 1.2875 (1.3205)  loss_mask: 0.0002 (0.0008)  time: 0.2011  data: 0.0002  max mem: 5511
[08:45:27.817615] Epoch: [55]  [360/781]  eta: 0:01:24  lr: 0.000113  training_loss: 1.6070 (1.5853)  mae_loss: 0.2565 (0.2631)  classification_loss: 1.3446 (1.3214)  loss_mask: 0.0002 (0.0008)  time: 0.1972  data: 0.0002  max mem: 5511
[08:45:31.745164] Epoch: [55]  [380/781]  eta: 0:01:20  lr: 0.000113  training_loss: 1.5799 (1.5859)  mae_loss: 0.2584 (0.2632)  classification_loss: 1.3443 (1.3218)  loss_mask: 0.0002 (0.0008)  time: 0.1963  data: 0.0002  max mem: 5511
[08:45:35.681755] Epoch: [55]  [400/781]  eta: 0:01:16  lr: 0.000113  training_loss: 1.5355 (1.5845)  mae_loss: 0.2652 (0.2634)  classification_loss: 1.2587 (1.3203)  loss_mask: 0.0002 (0.0008)  time: 0.1968  data: 0.0002  max mem: 5511
[08:45:39.633017] Epoch: [55]  [420/781]  eta: 0:01:12  lr: 0.000113  training_loss: 1.5419 (1.5843)  mae_loss: 0.2562 (0.2634)  classification_loss: 1.2682 (1.3202)  loss_mask: 0.0002 (0.0007)  time: 0.1975  data: 0.0002  max mem: 5511
[08:45:43.608915] Epoch: [55]  [440/781]  eta: 0:01:08  lr: 0.000113  training_loss: 1.5628 (1.5843)  mae_loss: 0.2567 (0.2631)  classification_loss: 1.2844 (1.3198)  loss_mask: 0.0009 (0.0013)  time: 0.1987  data: 0.0002  max mem: 5511
[08:45:47.544568] Epoch: [55]  [460/781]  eta: 0:01:04  lr: 0.000113  training_loss: 1.5616 (1.5836)  mae_loss: 0.2465 (0.2629)  classification_loss: 1.2965 (1.3187)  loss_mask: 0.0033 (0.0020)  time: 0.1967  data: 0.0004  max mem: 5511
[08:45:51.524706] Epoch: [55]  [480/781]  eta: 0:01:00  lr: 0.000113  training_loss: 1.8073 (1.5948)  mae_loss: 0.2527 (0.2628)  classification_loss: 1.3008 (1.3203)  loss_mask: 0.2042 (0.0118)  time: 0.1989  data: 0.0002  max mem: 5511
[08:45:55.472520] Epoch: [55]  [500/781]  eta: 0:00:56  lr: 0.000113  training_loss: 1.7134 (1.5994)  mae_loss: 0.2642 (0.2629)  classification_loss: 1.3064 (1.3202)  loss_mask: 0.0942 (0.0163)  time: 0.1973  data: 0.0002  max mem: 5511
[08:45:59.397647] Epoch: [55]  [520/781]  eta: 0:00:52  lr: 0.000112  training_loss: 1.6231 (1.6003)  mae_loss: 0.2556 (0.2630)  classification_loss: 1.3052 (1.3196)  loss_mask: 0.0485 (0.0177)  time: 0.1962  data: 0.0002  max mem: 5511
[08:46:03.344264] Epoch: [55]  [540/781]  eta: 0:00:48  lr: 0.000112  training_loss: 1.6307 (1.6020)  mae_loss: 0.2650 (0.2634)  classification_loss: 1.3411 (1.3205)  loss_mask: 0.0227 (0.0181)  time: 0.1972  data: 0.0004  max mem: 5511
[08:46:07.297804] Epoch: [55]  [560/781]  eta: 0:00:44  lr: 0.000112  training_loss: 1.6094 (1.6019)  mae_loss: 0.2683 (0.2637)  classification_loss: 1.3139 (1.3200)  loss_mask: 0.0169 (0.0181)  time: 0.1976  data: 0.0005  max mem: 5511
[08:46:11.279526] Epoch: [55]  [580/781]  eta: 0:00:40  lr: 0.000112  training_loss: 1.5947 (1.6024)  mae_loss: 0.2594 (0.2637)  classification_loss: 1.3292 (1.3209)  loss_mask: 0.0061 (0.0178)  time: 0.1990  data: 0.0002  max mem: 5511
[08:46:15.285067] Epoch: [55]  [600/781]  eta: 0:00:36  lr: 0.000112  training_loss: 1.5536 (1.6008)  mae_loss: 0.2637 (0.2638)  classification_loss: 1.2777 (1.3197)  loss_mask: 0.0033 (0.0173)  time: 0.2002  data: 0.0002  max mem: 5511
[08:46:19.198029] Epoch: [55]  [620/781]  eta: 0:00:32  lr: 0.000112  training_loss: 1.5778 (1.6004)  mae_loss: 0.2486 (0.2633)  classification_loss: 1.3120 (1.3202)  loss_mask: 0.0017 (0.0169)  time: 0.1956  data: 0.0002  max mem: 5511
[08:46:23.173594] Epoch: [55]  [640/781]  eta: 0:00:28  lr: 0.000112  training_loss: 1.5880 (1.6008)  mae_loss: 0.2554 (0.2632)  classification_loss: 1.3105 (1.3212)  loss_mask: 0.0012 (0.0164)  time: 0.1987  data: 0.0002  max mem: 5511
[08:46:27.109730] Epoch: [55]  [660/781]  eta: 0:00:24  lr: 0.000112  training_loss: 1.5980 (1.6004)  mae_loss: 0.2499 (0.2628)  classification_loss: 1.3207 (1.3217)  loss_mask: 0.0009 (0.0159)  time: 0.1967  data: 0.0002  max mem: 5511
[08:46:31.035976] Epoch: [55]  [680/781]  eta: 0:00:20  lr: 0.000112  training_loss: 1.6374 (1.6013)  mae_loss: 0.2852 (0.2633)  classification_loss: 1.3770 (1.3225)  loss_mask: 0.0007 (0.0155)  time: 0.1962  data: 0.0002  max mem: 5511
[08:46:34.977092] Epoch: [55]  [700/781]  eta: 0:00:16  lr: 0.000112  training_loss: 1.5619 (1.6007)  mae_loss: 0.2582 (0.2632)  classification_loss: 1.2829 (1.3224)  loss_mask: 0.0006 (0.0151)  time: 0.1970  data: 0.0002  max mem: 5511
[08:46:38.953850] Epoch: [55]  [720/781]  eta: 0:00:12  lr: 0.000111  training_loss: 1.6280 (1.6019)  mae_loss: 0.2671 (0.2634)  classification_loss: 1.3735 (1.3238)  loss_mask: 0.0005 (0.0147)  time: 0.1987  data: 0.0002  max mem: 5511
[08:46:42.915879] Epoch: [55]  [740/781]  eta: 0:00:08  lr: 0.000111  training_loss: 1.5301 (1.5996)  mae_loss: 0.2454 (0.2632)  classification_loss: 1.2400 (1.3221)  loss_mask: 0.0010 (0.0143)  time: 0.1979  data: 0.0004  max mem: 5511
[08:46:46.855305] Epoch: [55]  [760/781]  eta: 0:00:04  lr: 0.000111  training_loss: 1.5362 (1.5989)  mae_loss: 0.2447 (0.2630)  classification_loss: 1.2813 (1.3219)  loss_mask: 0.0012 (0.0141)  time: 0.1969  data: 0.0003  max mem: 5511
[08:46:50.800072] Epoch: [55]  [780/781]  eta: 0:00:00  lr: 0.000111  training_loss: 1.5894 (1.5981)  mae_loss: 0.2544 (0.2627)  classification_loss: 1.2611 (1.3212)  loss_mask: 0.0021 (0.0141)  time: 0.1972  data: 0.0002  max mem: 5511
[08:46:50.995155] Epoch: [55] Total time: 0:02:35 (0.1992 s / it)
[08:46:50.995947] Averaged stats: lr: 0.000111  training_loss: 1.5894 (1.5981)  mae_loss: 0.2544 (0.2627)  classification_loss: 1.2611 (1.3212)  loss_mask: 0.0021 (0.0141)
[08:46:51.721600] Test:  [  0/157]  eta: 0:01:52  testing_loss: 0.6214 (0.6214)  acc1: 79.6875 (79.6875)  acc5: 96.8750 (96.8750)  time: 0.7189  data: 0.6840  max mem: 5511
[08:46:52.007907] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.6214 (0.6269)  acc1: 79.6875 (80.2557)  acc5: 98.4375 (98.5795)  time: 0.0912  data: 0.0625  max mem: 5511
[08:46:52.294479] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5699 (0.6010)  acc1: 81.2500 (81.2500)  acc5: 100.0000 (98.8839)  time: 0.0285  data: 0.0003  max mem: 5511
[08:46:52.582619] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5917 (0.6107)  acc1: 81.2500 (80.6956)  acc5: 100.0000 (98.7903)  time: 0.0286  data: 0.0002  max mem: 5511
[08:46:52.865421] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6062 (0.6189)  acc1: 79.6875 (80.2210)  acc5: 98.4375 (98.7805)  time: 0.0284  data: 0.0002  max mem: 5511
[08:46:53.148907] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5921 (0.6072)  acc1: 81.2500 (80.9743)  acc5: 98.4375 (98.7745)  time: 0.0282  data: 0.0003  max mem: 5511
[08:46:53.437142] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5745 (0.6036)  acc1: 81.2500 (80.9939)  acc5: 100.0000 (98.8730)  time: 0.0284  data: 0.0002  max mem: 5511
[08:46:53.720904] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5736 (0.5969)  acc1: 82.8125 (81.3820)  acc5: 100.0000 (98.9437)  time: 0.0285  data: 0.0002  max mem: 5511
[08:46:54.004566] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5857 (0.6068)  acc1: 81.2500 (80.9799)  acc5: 98.4375 (98.8619)  time: 0.0283  data: 0.0002  max mem: 5511
[08:46:54.294063] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6352 (0.6048)  acc1: 81.2500 (81.0783)  acc5: 98.4375 (98.9011)  time: 0.0285  data: 0.0003  max mem: 5511
[08:46:54.594457] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.6002 (0.6091)  acc1: 81.2500 (80.9097)  acc5: 100.0000 (98.9016)  time: 0.0293  data: 0.0003  max mem: 5511
[08:46:54.879667] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6304 (0.6115)  acc1: 78.1250 (80.8277)  acc5: 100.0000 (98.9020)  time: 0.0291  data: 0.0003  max mem: 5511
[08:46:55.167278] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5971 (0.6084)  acc1: 79.6875 (80.8110)  acc5: 100.0000 (98.9282)  time: 0.0285  data: 0.0002  max mem: 5511
[08:46:55.462601] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5971 (0.6095)  acc1: 79.6875 (80.7848)  acc5: 100.0000 (98.9862)  time: 0.0289  data: 0.0002  max mem: 5511
[08:46:55.753688] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5829 (0.6080)  acc1: 81.2500 (80.8067)  acc5: 100.0000 (99.0359)  time: 0.0291  data: 0.0002  max mem: 5511
[08:46:56.035738] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5829 (0.6059)  acc1: 81.2500 (80.8671)  acc5: 100.0000 (99.0584)  time: 0.0285  data: 0.0001  max mem: 5511
[08:46:56.187435] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5829 (0.6062)  acc1: 82.8125 (80.8500)  acc5: 100.0000 (99.0900)  time: 0.0272  data: 0.0001  max mem: 5511
[08:46:56.371777] Test: Total time: 0:00:05 (0.0342 s / it)
[08:46:56.372244] * Acc@1 80.850 Acc@5 99.090 loss 0.606
[08:46:56.372654] Accuracy of the network on the 10000 test images: 80.8%
[08:46:56.373067] Max accuracy: 81.39%
[08:46:56.578588] log_dir: ./output_dir
[08:46:57.592478] Epoch: [56]  [  0/781]  eta: 0:13:10  lr: 0.000111  training_loss: 1.4803 (1.4803)  mae_loss: 0.3073 (0.3073)  classification_loss: 1.1677 (1.1677)  loss_mask: 0.0054 (0.0054)  time: 1.0120  data: 0.7904  max mem: 5511
[08:47:01.530959] Epoch: [56]  [ 20/781]  eta: 0:02:59  lr: 0.000111  training_loss: 1.5731 (1.5922)  mae_loss: 0.2577 (0.2630)  classification_loss: 1.2596 (1.2733)  loss_mask: 0.0235 (0.0559)  time: 0.1968  data: 0.0002  max mem: 5511
[08:47:05.500350] Epoch: [56]  [ 40/781]  eta: 0:02:41  lr: 0.000111  training_loss: 1.5869 (1.5894)  mae_loss: 0.2403 (0.2572)  classification_loss: 1.2862 (1.2868)  loss_mask: 0.0318 (0.0454)  time: 0.1984  data: 0.0002  max mem: 5511
[08:47:09.478461] Epoch: [56]  [ 60/781]  eta: 0:02:32  lr: 0.000111  training_loss: 1.5911 (1.5862)  mae_loss: 0.2493 (0.2549)  classification_loss: 1.3342 (1.2967)  loss_mask: 0.0076 (0.0347)  time: 0.1988  data: 0.0003  max mem: 5511
[08:47:13.482069] Epoch: [56]  [ 80/781]  eta: 0:02:26  lr: 0.000111  training_loss: 1.5604 (1.5827)  mae_loss: 0.2598 (0.2579)  classification_loss: 1.3007 (1.2972)  loss_mask: 0.0048 (0.0276)  time: 0.2001  data: 0.0003  max mem: 5511
[08:47:17.436800] Epoch: [56]  [100/781]  eta: 0:02:20  lr: 0.000111  training_loss: 1.5764 (1.5860)  mae_loss: 0.2697 (0.2599)  classification_loss: 1.3119 (1.3030)  loss_mask: 0.0031 (0.0231)  time: 0.1976  data: 0.0002  max mem: 5511
[08:47:21.382061] Epoch: [56]  [120/781]  eta: 0:02:15  lr: 0.000110  training_loss: 1.5733 (1.5809)  mae_loss: 0.2417 (0.2577)  classification_loss: 1.3296 (1.3037)  loss_mask: 0.0014 (0.0195)  time: 0.1972  data: 0.0002  max mem: 5511
[08:47:25.326352] Epoch: [56]  [140/781]  eta: 0:02:10  lr: 0.000110  training_loss: 1.5315 (1.5738)  mae_loss: 0.2651 (0.2579)  classification_loss: 1.2771 (1.2990)  loss_mask: 0.0011 (0.0169)  time: 0.1971  data: 0.0002  max mem: 5511
[08:47:29.294621] Epoch: [56]  [160/781]  eta: 0:02:06  lr: 0.000110  training_loss: 1.5292 (1.5694)  mae_loss: 0.2623 (0.2578)  classification_loss: 1.2866 (1.2967)  loss_mask: 0.0008 (0.0149)  time: 0.1983  data: 0.0002  max mem: 5511
[08:47:33.256824] Epoch: [56]  [180/781]  eta: 0:02:01  lr: 0.000110  training_loss: 1.5119 (1.5665)  mae_loss: 0.2566 (0.2576)  classification_loss: 1.2249 (1.2946)  loss_mask: 0.0009 (0.0143)  time: 0.1980  data: 0.0002  max mem: 5511
[08:47:37.229425] Epoch: [56]  [200/781]  eta: 0:01:57  lr: 0.000110  training_loss: 1.5954 (1.5708)  mae_loss: 0.2485 (0.2572)  classification_loss: 1.3342 (1.2985)  loss_mask: 0.0095 (0.0151)  time: 0.1985  data: 0.0002  max mem: 5511
[08:47:41.184346] Epoch: [56]  [220/781]  eta: 0:01:53  lr: 0.000110  training_loss: 1.5969 (1.5734)  mae_loss: 0.2507 (0.2569)  classification_loss: 1.3180 (1.2996)  loss_mask: 0.0122 (0.0168)  time: 0.1976  data: 0.0003  max mem: 5511
[08:47:45.136209] Epoch: [56]  [240/781]  eta: 0:01:48  lr: 0.000110  training_loss: 1.5511 (1.5722)  mae_loss: 0.2397 (0.2566)  classification_loss: 1.2703 (1.2984)  loss_mask: 0.0119 (0.0172)  time: 0.1975  data: 0.0002  max mem: 5511
[08:47:49.104158] Epoch: [56]  [260/781]  eta: 0:01:44  lr: 0.000110  training_loss: 1.5748 (1.5718)  mae_loss: 0.2571 (0.2572)  classification_loss: 1.2857 (1.2985)  loss_mask: 0.0030 (0.0161)  time: 0.1983  data: 0.0002  max mem: 5511
[08:47:53.092116] Epoch: [56]  [280/781]  eta: 0:01:40  lr: 0.000110  training_loss: 1.5790 (1.5743)  mae_loss: 0.2667 (0.2582)  classification_loss: 1.2975 (1.3010)  loss_mask: 0.0011 (0.0151)  time: 0.1993  data: 0.0003  max mem: 5511
[08:47:57.063028] Epoch: [56]  [300/781]  eta: 0:01:36  lr: 0.000110  training_loss: 1.5273 (1.5711)  mae_loss: 0.2559 (0.2577)  classification_loss: 1.2571 (1.2989)  loss_mask: 0.0010 (0.0145)  time: 0.1985  data: 0.0002  max mem: 5511
[08:48:01.020415] Epoch: [56]  [320/781]  eta: 0:01:32  lr: 0.000109  training_loss: 1.4966 (1.5699)  mae_loss: 0.2590 (0.2581)  classification_loss: 1.2584 (1.2980)  loss_mask: 0.0020 (0.0138)  time: 0.1978  data: 0.0002  max mem: 5511
[08:48:04.945275] Epoch: [56]  [340/781]  eta: 0:01:28  lr: 0.000109  training_loss: 1.5804 (1.5688)  mae_loss: 0.2678 (0.2588)  classification_loss: 1.2846 (1.2969)  loss_mask: 0.0010 (0.0130)  time: 0.1961  data: 0.0003  max mem: 5511
[08:48:08.889807] Epoch: [56]  [360/781]  eta: 0:01:24  lr: 0.000109  training_loss: 1.5965 (1.5696)  mae_loss: 0.2589 (0.2590)  classification_loss: 1.3421 (1.2983)  loss_mask: 0.0004 (0.0123)  time: 0.1971  data: 0.0002  max mem: 5511
[08:48:12.841689] Epoch: [56]  [380/781]  eta: 0:01:20  lr: 0.000109  training_loss: 1.5450 (1.5691)  mae_loss: 0.2643 (0.2595)  classification_loss: 1.2753 (1.2978)  loss_mask: 0.0004 (0.0117)  time: 0.1975  data: 0.0004  max mem: 5511
[08:48:16.772554] Epoch: [56]  [400/781]  eta: 0:01:16  lr: 0.000109  training_loss: 1.5631 (1.5694)  mae_loss: 0.2561 (0.2593)  classification_loss: 1.2917 (1.2989)  loss_mask: 0.0003 (0.0112)  time: 0.1965  data: 0.0002  max mem: 5511
[08:48:20.729863] Epoch: [56]  [420/781]  eta: 0:01:12  lr: 0.000109  training_loss: 1.5275 (1.5685)  mae_loss: 0.2458 (0.2590)  classification_loss: 1.2809 (1.2989)  loss_mask: 0.0004 (0.0106)  time: 0.1977  data: 0.0002  max mem: 5511
[08:48:24.693142] Epoch: [56]  [440/781]  eta: 0:01:08  lr: 0.000109  training_loss: 1.5406 (1.5690)  mae_loss: 0.2589 (0.2590)  classification_loss: 1.2898 (1.2996)  loss_mask: 0.0004 (0.0104)  time: 0.1981  data: 0.0002  max mem: 5511
[08:48:28.648083] Epoch: [56]  [460/781]  eta: 0:01:04  lr: 0.000109  training_loss: 1.5700 (1.5688)  mae_loss: 0.2597 (0.2589)  classification_loss: 1.2979 (1.2999)  loss_mask: 0.0006 (0.0100)  time: 0.1977  data: 0.0002  max mem: 5511
[08:48:32.639434] Epoch: [56]  [480/781]  eta: 0:01:00  lr: 0.000109  training_loss: 1.5579 (1.5693)  mae_loss: 0.2566 (0.2592)  classification_loss: 1.2912 (1.3005)  loss_mask: 0.0004 (0.0096)  time: 0.1995  data: 0.0002  max mem: 5511
[08:48:36.570880] Epoch: [56]  [500/781]  eta: 0:00:56  lr: 0.000109  training_loss: 1.6196 (1.5717)  mae_loss: 0.2595 (0.2592)  classification_loss: 1.3556 (1.3032)  loss_mask: 0.0003 (0.0092)  time: 0.1965  data: 0.0002  max mem: 5511
[08:48:40.517716] Epoch: [56]  [520/781]  eta: 0:00:52  lr: 0.000108  training_loss: 1.5763 (1.5723)  mae_loss: 0.2610 (0.2592)  classification_loss: 1.2941 (1.3042)  loss_mask: 0.0003 (0.0089)  time: 0.1972  data: 0.0002  max mem: 5511
[08:48:44.473223] Epoch: [56]  [540/781]  eta: 0:00:48  lr: 0.000108  training_loss: 1.6212 (1.5736)  mae_loss: 0.2656 (0.2596)  classification_loss: 1.3371 (1.3054)  loss_mask: 0.0003 (0.0086)  time: 0.1977  data: 0.0003  max mem: 5511
[08:48:48.414489] Epoch: [56]  [560/781]  eta: 0:00:44  lr: 0.000108  training_loss: 1.5222 (1.5721)  mae_loss: 0.2634 (0.2598)  classification_loss: 1.2648 (1.3040)  loss_mask: 0.0002 (0.0083)  time: 0.1970  data: 0.0002  max mem: 5511
[08:48:52.377200] Epoch: [56]  [580/781]  eta: 0:00:40  lr: 0.000108  training_loss: 1.5279 (1.5716)  mae_loss: 0.2593 (0.2599)  classification_loss: 1.2684 (1.3037)  loss_mask: 0.0002 (0.0080)  time: 0.1981  data: 0.0002  max mem: 5511
[08:48:56.328138] Epoch: [56]  [600/781]  eta: 0:00:36  lr: 0.000108  training_loss: 1.6037 (1.5724)  mae_loss: 0.2635 (0.2599)  classification_loss: 1.3199 (1.3048)  loss_mask: 0.0003 (0.0078)  time: 0.1975  data: 0.0003  max mem: 5511
[08:49:00.290316] Epoch: [56]  [620/781]  eta: 0:00:32  lr: 0.000108  training_loss: 1.5901 (1.5728)  mae_loss: 0.2667 (0.2599)  classification_loss: 1.3493 (1.3053)  loss_mask: 0.0003 (0.0075)  time: 0.1980  data: 0.0002  max mem: 5511
[08:49:04.241960] Epoch: [56]  [640/781]  eta: 0:00:28  lr: 0.000108  training_loss: 1.5549 (1.5724)  mae_loss: 0.2467 (0.2597)  classification_loss: 1.2969 (1.3055)  loss_mask: 0.0002 (0.0073)  time: 0.1975  data: 0.0002  max mem: 5511
[08:49:08.206524] Epoch: [56]  [660/781]  eta: 0:00:24  lr: 0.000108  training_loss: 1.5399 (1.5720)  mae_loss: 0.2636 (0.2598)  classification_loss: 1.2980 (1.3051)  loss_mask: 0.0002 (0.0071)  time: 0.1981  data: 0.0002  max mem: 5511
[08:49:12.153378] Epoch: [56]  [680/781]  eta: 0:00:20  lr: 0.000108  training_loss: 1.5784 (1.5720)  mae_loss: 0.2497 (0.2596)  classification_loss: 1.3180 (1.3055)  loss_mask: 0.0002 (0.0069)  time: 0.1973  data: 0.0002  max mem: 5511
[08:49:16.108957] Epoch: [56]  [700/781]  eta: 0:00:16  lr: 0.000107  training_loss: 1.5824 (1.5721)  mae_loss: 0.2600 (0.2598)  classification_loss: 1.2878 (1.3056)  loss_mask: 0.0001 (0.0067)  time: 0.1977  data: 0.0002  max mem: 5511
[08:49:20.075935] Epoch: [56]  [720/781]  eta: 0:00:12  lr: 0.000107  training_loss: 1.6232 (1.5735)  mae_loss: 0.2577 (0.2599)  classification_loss: 1.3759 (1.3071)  loss_mask: 0.0002 (0.0065)  time: 0.1983  data: 0.0002  max mem: 5511
[08:49:24.024420] Epoch: [56]  [740/781]  eta: 0:00:08  lr: 0.000107  training_loss: 1.5568 (1.5732)  mae_loss: 0.2554 (0.2597)  classification_loss: 1.3038 (1.3072)  loss_mask: 0.0001 (0.0063)  time: 0.1973  data: 0.0002  max mem: 5511
[08:49:28.006288] Epoch: [56]  [760/781]  eta: 0:00:04  lr: 0.000107  training_loss: 1.6382 (1.5741)  mae_loss: 0.2568 (0.2596)  classification_loss: 1.3479 (1.3081)  loss_mask: 0.0002 (0.0064)  time: 0.1990  data: 0.0002  max mem: 5511
[08:49:31.937047] Epoch: [56]  [780/781]  eta: 0:00:00  lr: 0.000107  training_loss: 1.5870 (1.5745)  mae_loss: 0.2568 (0.2596)  classification_loss: 1.3234 (1.3087)  loss_mask: 0.0003 (0.0062)  time: 0.1965  data: 0.0002  max mem: 5511
[08:49:32.100677] Epoch: [56] Total time: 0:02:35 (0.1991 s / it)
[08:49:32.102061] Averaged stats: lr: 0.000107  training_loss: 1.5870 (1.5745)  mae_loss: 0.2568 (0.2596)  classification_loss: 1.3234 (1.3087)  loss_mask: 0.0003 (0.0062)
[08:49:32.816057] Test:  [  0/157]  eta: 0:01:51  testing_loss: 0.5557 (0.5557)  acc1: 84.3750 (84.3750)  acc5: 98.4375 (98.4375)  time: 0.7095  data: 0.6799  max mem: 5511
[08:49:33.116404] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.6008 (0.6113)  acc1: 78.1250 (80.3977)  acc5: 100.0000 (99.2898)  time: 0.0915  data: 0.0624  max mem: 5511
[08:49:33.405865] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5748 (0.5837)  acc1: 81.2500 (81.3988)  acc5: 100.0000 (99.4048)  time: 0.0292  data: 0.0004  max mem: 5511
[08:49:33.694068] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5654 (0.5936)  acc1: 81.2500 (80.8468)  acc5: 100.0000 (99.1935)  time: 0.0287  data: 0.0002  max mem: 5511
[08:49:33.984127] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5989 (0.6030)  acc1: 81.2500 (80.7165)  acc5: 98.4375 (99.1235)  time: 0.0287  data: 0.0002  max mem: 5511
[08:49:34.282676] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5981 (0.5902)  acc1: 82.8125 (81.4951)  acc5: 98.4375 (99.1422)  time: 0.0292  data: 0.0002  max mem: 5511
[08:49:34.568562] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5406 (0.5862)  acc1: 84.3750 (81.6855)  acc5: 98.4375 (99.1291)  time: 0.0290  data: 0.0002  max mem: 5511
[08:49:34.854929] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5406 (0.5817)  acc1: 81.2500 (81.7562)  acc5: 98.4375 (99.0757)  time: 0.0285  data: 0.0002  max mem: 5511
[08:49:35.143817] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5962 (0.5904)  acc1: 81.2500 (81.4622)  acc5: 98.4375 (99.0741)  time: 0.0286  data: 0.0003  max mem: 5511
[08:49:35.437533] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6073 (0.5886)  acc1: 81.2500 (81.6106)  acc5: 98.4375 (99.0556)  time: 0.0289  data: 0.0003  max mem: 5511
[08:49:35.731407] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.5785 (0.5919)  acc1: 82.8125 (81.5749)  acc5: 98.4375 (99.0408)  time: 0.0292  data: 0.0003  max mem: 5511
[08:49:36.018251] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6057 (0.5945)  acc1: 81.2500 (81.4189)  acc5: 100.0000 (99.0428)  time: 0.0288  data: 0.0003  max mem: 5511
[08:49:36.309729] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5847 (0.5917)  acc1: 81.2500 (81.4179)  acc5: 100.0000 (99.0702)  time: 0.0287  data: 0.0003  max mem: 5511
[08:49:36.598905] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5847 (0.5933)  acc1: 79.6875 (81.3573)  acc5: 100.0000 (99.0816)  time: 0.0289  data: 0.0002  max mem: 5511
[08:49:36.884171] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5926 (0.5939)  acc1: 81.2500 (81.3941)  acc5: 100.0000 (99.1135)  time: 0.0286  data: 0.0002  max mem: 5511
[08:49:37.165868] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5926 (0.5924)  acc1: 82.8125 (81.4673)  acc5: 98.4375 (99.0791)  time: 0.0282  data: 0.0001  max mem: 5511
[08:49:37.318070] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5950 (0.5936)  acc1: 81.2500 (81.4200)  acc5: 98.4375 (99.0800)  time: 0.0272  data: 0.0001  max mem: 5511
[08:49:37.497158] Test: Total time: 0:00:05 (0.0343 s / it)
[08:49:37.497642] * Acc@1 81.420 Acc@5 99.080 loss 0.594
[08:49:37.497982] Accuracy of the network on the 10000 test images: 81.4%
[08:49:37.498176] Max accuracy: 81.42%
[08:49:37.688503] log_dir: ./output_dir
[08:49:38.578698] Epoch: [57]  [  0/781]  eta: 0:11:33  lr: 0.000107  training_loss: 1.6374 (1.6374)  mae_loss: 0.2499 (0.2499)  classification_loss: 1.3871 (1.3871)  loss_mask: 0.0004 (0.0004)  time: 0.8882  data: 0.6667  max mem: 5511
[08:49:42.519674] Epoch: [57]  [ 20/781]  eta: 0:02:54  lr: 0.000107  training_loss: 1.5558 (1.5693)  mae_loss: 0.2593 (0.2595)  classification_loss: 1.3146 (1.3096)  loss_mask: 0.0002 (0.0002)  time: 0.1969  data: 0.0003  max mem: 5511
[08:49:46.473469] Epoch: [57]  [ 40/781]  eta: 0:02:38  lr: 0.000107  training_loss: 1.5799 (1.5722)  mae_loss: 0.2529 (0.2577)  classification_loss: 1.3260 (1.3143)  loss_mask: 0.0001 (0.0002)  time: 0.1976  data: 0.0003  max mem: 5511
[08:49:50.412107] Epoch: [57]  [ 60/781]  eta: 0:02:30  lr: 0.000107  training_loss: 1.6358 (1.5860)  mae_loss: 0.2527 (0.2569)  classification_loss: 1.3842 (1.3289)  loss_mask: 0.0002 (0.0002)  time: 0.1969  data: 0.0003  max mem: 5511
[08:49:54.395387] Epoch: [57]  [ 80/781]  eta: 0:02:24  lr: 0.000107  training_loss: 1.5518 (1.5781)  mae_loss: 0.2434 (0.2571)  classification_loss: 1.2845 (1.3208)  loss_mask: 0.0002 (0.0002)  time: 0.1990  data: 0.0003  max mem: 5511
[08:49:58.353542] Epoch: [57]  [100/781]  eta: 0:02:19  lr: 0.000107  training_loss: 1.5574 (1.5782)  mae_loss: 0.2645 (0.2591)  classification_loss: 1.2825 (1.3188)  loss_mask: 0.0002 (0.0003)  time: 0.1978  data: 0.0003  max mem: 5511
[08:50:02.290123] Epoch: [57]  [120/781]  eta: 0:02:14  lr: 0.000106  training_loss: 1.5379 (1.5734)  mae_loss: 0.2405 (0.2580)  classification_loss: 1.2944 (1.3151)  loss_mask: 0.0002 (0.0003)  time: 0.1967  data: 0.0003  max mem: 5511
[08:50:06.228729] Epoch: [57]  [140/781]  eta: 0:02:09  lr: 0.000106  training_loss: 1.5238 (1.5678)  mae_loss: 0.2491 (0.2580)  classification_loss: 1.2830 (1.3095)  loss_mask: 0.0002 (0.0003)  time: 0.1968  data: 0.0003  max mem: 5511
[08:50:10.187213] Epoch: [57]  [160/781]  eta: 0:02:05  lr: 0.000106  training_loss: 1.5945 (1.5684)  mae_loss: 0.2428 (0.2572)  classification_loss: 1.3608 (1.3109)  loss_mask: 0.0001 (0.0002)  time: 0.1978  data: 0.0002  max mem: 5511
[08:50:14.147772] Epoch: [57]  [180/781]  eta: 0:02:00  lr: 0.000106  training_loss: 1.5770 (1.5711)  mae_loss: 0.2654 (0.2583)  classification_loss: 1.3070 (1.3126)  loss_mask: 0.0002 (0.0002)  time: 0.1979  data: 0.0002  max mem: 5511
[08:50:18.113362] Epoch: [57]  [200/781]  eta: 0:01:56  lr: 0.000106  training_loss: 1.5683 (1.5699)  mae_loss: 0.2559 (0.2585)  classification_loss: 1.3074 (1.3112)  loss_mask: 0.0001 (0.0002)  time: 0.1982  data: 0.0002  max mem: 5511
[08:50:22.052524] Epoch: [57]  [220/781]  eta: 0:01:52  lr: 0.000106  training_loss: 1.5301 (1.5673)  mae_loss: 0.2599 (0.2586)  classification_loss: 1.2757 (1.3084)  loss_mask: 0.0001 (0.0002)  time: 0.1969  data: 0.0002  max mem: 5511
[08:50:25.992900] Epoch: [57]  [240/781]  eta: 0:01:48  lr: 0.000106  training_loss: 1.5528 (1.5657)  mae_loss: 0.2594 (0.2590)  classification_loss: 1.2975 (1.3064)  loss_mask: 0.0001 (0.0002)  time: 0.1969  data: 0.0002  max mem: 5511
[08:50:29.970900] Epoch: [57]  [260/781]  eta: 0:01:44  lr: 0.000106  training_loss: 1.5630 (1.5656)  mae_loss: 0.2700 (0.2597)  classification_loss: 1.2868 (1.3056)  loss_mask: 0.0001 (0.0002)  time: 0.1988  data: 0.0003  max mem: 5511
[08:50:33.968200] Epoch: [57]  [280/781]  eta: 0:01:40  lr: 0.000106  training_loss: 1.5026 (1.5648)  mae_loss: 0.2506 (0.2589)  classification_loss: 1.2589 (1.3056)  loss_mask: 0.0001 (0.0002)  time: 0.1998  data: 0.0002  max mem: 5511
[08:50:37.933439] Epoch: [57]  [300/781]  eta: 0:01:36  lr: 0.000105  training_loss: 1.5970 (1.5679)  mae_loss: 0.2632 (0.2595)  classification_loss: 1.2959 (1.3082)  loss_mask: 0.0001 (0.0002)  time: 0.1981  data: 0.0003  max mem: 5511
[08:50:41.879078] Epoch: [57]  [320/781]  eta: 0:01:32  lr: 0.000105  training_loss: 1.5336 (1.5674)  mae_loss: 0.2486 (0.2589)  classification_loss: 1.2975 (1.3082)  loss_mask: 0.0001 (0.0002)  time: 0.1972  data: 0.0003  max mem: 5511
[08:50:45.864130] Epoch: [57]  [340/781]  eta: 0:01:28  lr: 0.000105  training_loss: 1.5411 (1.5674)  mae_loss: 0.2538 (0.2589)  classification_loss: 1.2863 (1.3083)  loss_mask: 0.0001 (0.0002)  time: 0.1992  data: 0.0002  max mem: 5511
[08:50:49.810982] Epoch: [57]  [360/781]  eta: 0:01:24  lr: 0.000105  training_loss: 1.6117 (1.5689)  mae_loss: 0.2545 (0.2588)  classification_loss: 1.3408 (1.3097)  loss_mask: 0.0002 (0.0005)  time: 0.1973  data: 0.0003  max mem: 5511
[08:50:53.753394] Epoch: [57]  [380/781]  eta: 0:01:20  lr: 0.000105  training_loss: 1.6013 (1.5712)  mae_loss: 0.2701 (0.2593)  classification_loss: 1.3460 (1.3111)  loss_mask: 0.0032 (0.0008)  time: 0.1970  data: 0.0002  max mem: 5511
[08:50:57.689050] Epoch: [57]  [400/781]  eta: 0:01:15  lr: 0.000105  training_loss: 1.6896 (1.5788)  mae_loss: 0.2541 (0.2590)  classification_loss: 1.3106 (1.3115)  loss_mask: 0.0801 (0.0082)  time: 0.1967  data: 0.0002  max mem: 5511
[08:51:01.618852] Epoch: [57]  [420/781]  eta: 0:01:11  lr: 0.000105  training_loss: 1.6445 (1.5832)  mae_loss: 0.2506 (0.2592)  classification_loss: 1.3032 (1.3117)  loss_mask: 0.0583 (0.0123)  time: 0.1964  data: 0.0002  max mem: 5511
[08:51:05.592122] Epoch: [57]  [440/781]  eta: 0:01:07  lr: 0.000105  training_loss: 1.5820 (1.5840)  mae_loss: 0.2563 (0.2595)  classification_loss: 1.3041 (1.3113)  loss_mask: 0.0321 (0.0132)  time: 0.1985  data: 0.0002  max mem: 5511
[08:51:09.547976] Epoch: [57]  [460/781]  eta: 0:01:03  lr: 0.000105  training_loss: 1.4863 (1.5814)  mae_loss: 0.2543 (0.2594)  classification_loss: 1.2167 (1.3089)  loss_mask: 0.0118 (0.0132)  time: 0.1977  data: 0.0002  max mem: 5511
[08:51:13.513090] Epoch: [57]  [480/781]  eta: 0:00:59  lr: 0.000105  training_loss: 1.5550 (1.5811)  mae_loss: 0.2486 (0.2591)  classification_loss: 1.3171 (1.3090)  loss_mask: 0.0071 (0.0130)  time: 0.1982  data: 0.0003  max mem: 5511
[08:51:17.459225] Epoch: [57]  [500/781]  eta: 0:00:55  lr: 0.000104  training_loss: 1.6169 (1.5829)  mae_loss: 0.2579 (0.2596)  classification_loss: 1.3372 (1.3107)  loss_mask: 0.0040 (0.0127)  time: 0.1972  data: 0.0003  max mem: 5511
[08:51:21.406332] Epoch: [57]  [520/781]  eta: 0:00:51  lr: 0.000104  training_loss: 1.6139 (1.5836)  mae_loss: 0.2703 (0.2600)  classification_loss: 1.3159 (1.3111)  loss_mask: 0.0022 (0.0124)  time: 0.1973  data: 0.0003  max mem: 5511
[08:51:25.337043] Epoch: [57]  [540/781]  eta: 0:00:47  lr: 0.000104  training_loss: 1.6112 (1.5850)  mae_loss: 0.2478 (0.2596)  classification_loss: 1.3298 (1.3116)  loss_mask: 0.0092 (0.0139)  time: 0.1965  data: 0.0002  max mem: 5511
[08:51:29.298522] Epoch: [57]  [560/781]  eta: 0:00:43  lr: 0.000104  training_loss: 1.6153 (1.5863)  mae_loss: 0.2580 (0.2596)  classification_loss: 1.3036 (1.3108)  loss_mask: 0.0408 (0.0158)  time: 0.1980  data: 0.0002  max mem: 5511
[08:51:33.220394] Epoch: [57]  [580/781]  eta: 0:00:39  lr: 0.000104  training_loss: 1.5225 (1.5851)  mae_loss: 0.2364 (0.2591)  classification_loss: 1.2726 (1.3099)  loss_mask: 0.0223 (0.0161)  time: 0.1960  data: 0.0003  max mem: 5511
[08:51:37.180508] Epoch: [57]  [600/781]  eta: 0:00:35  lr: 0.000104  training_loss: 1.5291 (1.5836)  mae_loss: 0.2568 (0.2591)  classification_loss: 1.2632 (1.3084)  loss_mask: 0.0107 (0.0161)  time: 0.1979  data: 0.0003  max mem: 5511
[08:51:41.121752] Epoch: [57]  [620/781]  eta: 0:00:31  lr: 0.000104  training_loss: 1.5846 (1.5838)  mae_loss: 0.2716 (0.2595)  classification_loss: 1.3156 (1.3085)  loss_mask: 0.0070 (0.0158)  time: 0.1970  data: 0.0002  max mem: 5511
[08:51:45.065272] Epoch: [57]  [640/781]  eta: 0:00:28  lr: 0.000104  training_loss: 1.5831 (1.5843)  mae_loss: 0.2589 (0.2597)  classification_loss: 1.3163 (1.3091)  loss_mask: 0.0054 (0.0155)  time: 0.1971  data: 0.0003  max mem: 5511
[08:51:49.026112] Epoch: [57]  [660/781]  eta: 0:00:24  lr: 0.000104  training_loss: 1.5634 (1.5837)  mae_loss: 0.2453 (0.2595)  classification_loss: 1.2870 (1.3091)  loss_mask: 0.0025 (0.0151)  time: 0.1980  data: 0.0003  max mem: 5511
[08:51:52.971116] Epoch: [57]  [680/781]  eta: 0:00:20  lr: 0.000104  training_loss: 1.5927 (1.5840)  mae_loss: 0.2537 (0.2595)  classification_loss: 1.3487 (1.3098)  loss_mask: 0.0013 (0.0147)  time: 0.1972  data: 0.0002  max mem: 5511
[08:51:56.970802] Epoch: [57]  [700/781]  eta: 0:00:16  lr: 0.000103  training_loss: 1.5291 (1.5829)  mae_loss: 0.2534 (0.2594)  classification_loss: 1.2694 (1.3091)  loss_mask: 0.0008 (0.0143)  time: 0.1999  data: 0.0002  max mem: 5511
[08:52:00.915186] Epoch: [57]  [720/781]  eta: 0:00:12  lr: 0.000103  training_loss: 1.5849 (1.5829)  mae_loss: 0.2678 (0.2596)  classification_loss: 1.3261 (1.3093)  loss_mask: 0.0010 (0.0140)  time: 0.1971  data: 0.0004  max mem: 5511
[08:52:04.842325] Epoch: [57]  [740/781]  eta: 0:00:08  lr: 0.000103  training_loss: 1.5166 (1.5813)  mae_loss: 0.2549 (0.2598)  classification_loss: 1.2423 (1.3077)  loss_mask: 0.0010 (0.0137)  time: 0.1963  data: 0.0003  max mem: 5511
[08:52:08.793766] Epoch: [57]  [760/781]  eta: 0:00:04  lr: 0.000103  training_loss: 1.8320 (1.5889)  mae_loss: 0.2530 (0.2600)  classification_loss: 1.4798 (1.3121)  loss_mask: 0.1005 (0.0168)  time: 0.1975  data: 0.0003  max mem: 5511
[08:52:12.728143] Epoch: [57]  [780/781]  eta: 0:00:00  lr: 0.000103  training_loss: 1.7758 (1.5939)  mae_loss: 0.2700 (0.2605)  classification_loss: 1.3655 (1.3139)  loss_mask: 0.0805 (0.0196)  time: 0.1966  data: 0.0002  max mem: 5511
[08:52:12.855489] Epoch: [57] Total time: 0:02:35 (0.1987 s / it)
[08:52:12.855979] Averaged stats: lr: 0.000103  training_loss: 1.7758 (1.5939)  mae_loss: 0.2700 (0.2605)  classification_loss: 1.3655 (1.3139)  loss_mask: 0.0805 (0.0196)
[08:52:13.574968] Test:  [  0/157]  eta: 0:01:52  testing_loss: 0.5571 (0.5571)  acc1: 84.3750 (84.3750)  acc5: 98.4375 (98.4375)  time: 0.7137  data: 0.6834  max mem: 5511
[08:52:13.865974] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.6301 (0.6387)  acc1: 81.2500 (79.9716)  acc5: 98.4375 (98.4375)  time: 0.0911  data: 0.0623  max mem: 5511
[08:52:14.151827] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.6126 (0.6183)  acc1: 81.2500 (80.8780)  acc5: 100.0000 (99.1071)  time: 0.0287  data: 0.0002  max mem: 5511
[08:52:14.445974] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6126 (0.6312)  acc1: 81.2500 (80.2923)  acc5: 100.0000 (98.7903)  time: 0.0288  data: 0.0002  max mem: 5511
[08:52:14.733137] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6226 (0.6412)  acc1: 79.6875 (79.9543)  acc5: 98.4375 (98.8186)  time: 0.0288  data: 0.0002  max mem: 5511
[08:52:15.021458] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6154 (0.6309)  acc1: 81.2500 (80.4228)  acc5: 98.4375 (98.7745)  time: 0.0286  data: 0.0002  max mem: 5511
[08:52:15.313599] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5946 (0.6254)  acc1: 81.2500 (80.3279)  acc5: 98.4375 (98.7961)  time: 0.0289  data: 0.0002  max mem: 5511
[08:52:15.605263] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5972 (0.6225)  acc1: 81.2500 (80.3257)  acc5: 100.0000 (98.8996)  time: 0.0290  data: 0.0002  max mem: 5511
[08:52:15.893687] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6267 (0.6333)  acc1: 79.6875 (80.0154)  acc5: 98.4375 (98.8040)  time: 0.0288  data: 0.0002  max mem: 5511
[08:52:16.183727] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6216 (0.6312)  acc1: 79.6875 (80.2885)  acc5: 98.4375 (98.7809)  time: 0.0287  data: 0.0002  max mem: 5511
[08:52:16.469801] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.6306 (0.6365)  acc1: 79.6875 (79.9660)  acc5: 98.4375 (98.7624)  time: 0.0287  data: 0.0002  max mem: 5511
[08:52:16.754559] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6504 (0.6378)  acc1: 78.1250 (79.9690)  acc5: 98.4375 (98.7472)  time: 0.0284  data: 0.0002  max mem: 5511
[08:52:17.042007] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6350 (0.6347)  acc1: 79.6875 (80.0620)  acc5: 98.4375 (98.7603)  time: 0.0284  data: 0.0002  max mem: 5511
[08:52:17.329654] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6402 (0.6374)  acc1: 79.6875 (79.9380)  acc5: 98.4375 (98.7953)  time: 0.0286  data: 0.0002  max mem: 5511
[08:52:17.617004] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6385 (0.6369)  acc1: 78.1250 (79.8870)  acc5: 100.0000 (98.8254)  time: 0.0286  data: 0.0002  max mem: 5511
[08:52:17.900031] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6154 (0.6339)  acc1: 78.1250 (79.9669)  acc5: 100.0000 (98.8307)  time: 0.0284  data: 0.0001  max mem: 5511
[08:52:18.053008] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5807 (0.6341)  acc1: 79.6875 (79.8900)  acc5: 100.0000 (98.8500)  time: 0.0273  data: 0.0001  max mem: 5511
[08:52:18.222204] Test: Total time: 0:00:05 (0.0342 s / it)
[08:52:18.222938] * Acc@1 79.890 Acc@5 98.850 loss 0.634
[08:52:18.223276] Accuracy of the network on the 10000 test images: 79.9%
[08:52:18.223493] Max accuracy: 81.42%
[08:52:18.335368] log_dir: ./output_dir
[08:52:19.311277] Epoch: [58]  [  0/781]  eta: 0:12:40  lr: 0.000103  training_loss: 1.5934 (1.5934)  mae_loss: 0.2815 (0.2815)  classification_loss: 1.2794 (1.2794)  loss_mask: 0.0325 (0.0325)  time: 0.9741  data: 0.7325  max mem: 5511
[08:52:23.284412] Epoch: [58]  [ 20/781]  eta: 0:02:59  lr: 0.000103  training_loss: 1.6285 (1.6286)  mae_loss: 0.2565 (0.2640)  classification_loss: 1.3008 (1.3223)  loss_mask: 0.0360 (0.0423)  time: 0.1986  data: 0.0002  max mem: 5511
[08:52:27.238687] Epoch: [58]  [ 40/781]  eta: 0:02:40  lr: 0.000103  training_loss: 1.7021 (1.6509)  mae_loss: 0.2439 (0.2587)  classification_loss: 1.3958 (1.3534)  loss_mask: 0.0203 (0.0388)  time: 0.1976  data: 0.0002  max mem: 5511
[08:52:31.202708] Epoch: [58]  [ 60/781]  eta: 0:02:31  lr: 0.000103  training_loss: 1.6131 (1.6489)  mae_loss: 0.2705 (0.2637)  classification_loss: 1.3303 (1.3538)  loss_mask: 0.0116 (0.0314)  time: 0.1981  data: 0.0002  max mem: 5511
[08:52:35.131442] Epoch: [58]  [ 80/781]  eta: 0:02:25  lr: 0.000103  training_loss: 1.5499 (1.6285)  mae_loss: 0.2531 (0.2630)  classification_loss: 1.2730 (1.3366)  loss_mask: 0.0148 (0.0289)  time: 0.1964  data: 0.0002  max mem: 5511
[08:52:39.067764] Epoch: [58]  [100/781]  eta: 0:02:19  lr: 0.000102  training_loss: 1.5731 (1.6218)  mae_loss: 0.2401 (0.2617)  classification_loss: 1.3083 (1.3353)  loss_mask: 0.0052 (0.0248)  time: 0.1967  data: 0.0002  max mem: 5511
[08:52:42.992761] Epoch: [58]  [120/781]  eta: 0:02:14  lr: 0.000102  training_loss: 1.5151 (1.6122)  mae_loss: 0.2461 (0.2607)  classification_loss: 1.2658 (1.3290)  loss_mask: 0.0048 (0.0225)  time: 0.1962  data: 0.0002  max mem: 5511
[08:52:46.962066] Epoch: [58]  [140/781]  eta: 0:02:10  lr: 0.000102  training_loss: 1.5847 (1.6088)  mae_loss: 0.2554 (0.2608)  classification_loss: 1.3152 (1.3279)  loss_mask: 0.0032 (0.0201)  time: 0.1984  data: 0.0002  max mem: 5511
[08:52:50.921753] Epoch: [58]  [160/781]  eta: 0:02:05  lr: 0.000102  training_loss: 1.5342 (1.6001)  mae_loss: 0.2475 (0.2601)  classification_loss: 1.2776 (1.3215)  loss_mask: 0.0035 (0.0185)  time: 0.1978  data: 0.0002  max mem: 5511
[08:52:54.851931] Epoch: [58]  [180/781]  eta: 0:02:01  lr: 0.000102  training_loss: 1.5984 (1.5995)  mae_loss: 0.2625 (0.2608)  classification_loss: 1.3059 (1.3202)  loss_mask: 0.0061 (0.0185)  time: 0.1964  data: 0.0002  max mem: 5511
[08:52:58.780675] Epoch: [58]  [200/781]  eta: 0:01:56  lr: 0.000102  training_loss: 1.6459 (1.6048)  mae_loss: 0.2593 (0.2613)  classification_loss: 1.3112 (1.3184)  loss_mask: 0.0423 (0.0251)  time: 0.1964  data: 0.0003  max mem: 5511
[08:53:02.734425] Epoch: [58]  [220/781]  eta: 0:01:52  lr: 0.000102  training_loss: 1.5849 (1.6036)  mae_loss: 0.2451 (0.2609)  classification_loss: 1.3176 (1.3178)  loss_mask: 0.0214 (0.0249)  time: 0.1976  data: 0.0002  max mem: 5511
[08:53:06.695753] Epoch: [58]  [240/781]  eta: 0:01:48  lr: 0.000102  training_loss: 1.5292 (1.6019)  mae_loss: 0.2618 (0.2609)  classification_loss: 1.2920 (1.3171)  loss_mask: 0.0081 (0.0239)  time: 0.1980  data: 0.0003  max mem: 5511
[08:53:10.678091] Epoch: [58]  [260/781]  eta: 0:01:44  lr: 0.000102  training_loss: 1.5830 (1.6018)  mae_loss: 0.2692 (0.2619)  classification_loss: 1.2976 (1.3169)  loss_mask: 0.0116 (0.0230)  time: 0.1990  data: 0.0002  max mem: 5511
[08:53:14.705059] Epoch: [58]  [280/781]  eta: 0:01:40  lr: 0.000102  training_loss: 1.5512 (1.5981)  mae_loss: 0.2547 (0.2617)  classification_loss: 1.2747 (1.3144)  loss_mask: 0.0039 (0.0220)  time: 0.2012  data: 0.0002  max mem: 5511
[08:53:18.661478] Epoch: [58]  [300/781]  eta: 0:01:36  lr: 0.000101  training_loss: 1.5491 (1.5952)  mae_loss: 0.2510 (0.2613)  classification_loss: 1.2840 (1.3132)  loss_mask: 0.0024 (0.0208)  time: 0.1977  data: 0.0002  max mem: 5511
[08:53:22.657500] Epoch: [58]  [320/781]  eta: 0:01:32  lr: 0.000101  training_loss: 1.5467 (1.5928)  mae_loss: 0.2512 (0.2609)  classification_loss: 1.2892 (1.3123)  loss_mask: 0.0017 (0.0196)  time: 0.1997  data: 0.0003  max mem: 5511
[08:53:26.618063] Epoch: [58]  [340/781]  eta: 0:01:28  lr: 0.000101  training_loss: 1.5560 (1.5908)  mae_loss: 0.2591 (0.2617)  classification_loss: 1.2638 (1.3103)  loss_mask: 0.0015 (0.0188)  time: 0.1979  data: 0.0002  max mem: 5511

[08:53:30.547803] Epoch: [58]  [360/781]  eta: 0:01:24  lr: 0.000101  training_loss: 1.5934 (1.5915)  mae_loss: 0.2591 (0.2618)  classification_loss: 1.3453 (1.3118)  loss_mask: 0.0018 (0.0179)  time: 0.1964  data: 0.0002  max mem: 5511
[08:53:34.484658] Epoch: [58]  [380/781]  eta: 0:01:20  lr: 0.000101  training_loss: 1.5649 (1.5904)  mae_loss: 0.2577 (0.2617)  classification_loss: 1.3112 (1.3117)  loss_mask: 0.0007 (0.0170)  time: 0.1968  data: 0.0002  max mem: 5511
[08:53:38.416094] Epoch: [58]  [400/781]  eta: 0:01:16  lr: 0.000101  training_loss: 1.5437 (1.5881)  mae_loss: 0.2515 (0.2612)  classification_loss: 1.2758 (1.3108)  loss_mask: 0.0005 (0.0162)  time: 0.1965  data: 0.0002  max mem: 5511
[08:53:42.353281] Epoch: [58]  [420/781]  eta: 0:01:12  lr: 0.000101  training_loss: 1.5302 (1.5855)  mae_loss: 0.2540 (0.2612)  classification_loss: 1.2480 (1.3088)  loss_mask: 0.0004 (0.0154)  time: 0.1968  data: 0.0002  max mem: 5511
[08:53:46.306199] Epoch: [58]  [440/781]  eta: 0:01:07  lr: 0.000101  training_loss: 1.5524 (1.5836)  mae_loss: 0.2526 (0.2609)  classification_loss: 1.2907 (1.3078)  loss_mask: 0.0005 (0.0149)  time: 0.1976  data: 0.0003  max mem: 5511
[08:53:50.284404] Epoch: [58]  [460/781]  eta: 0:01:03  lr: 0.000101  training_loss: 1.5648 (1.5837)  mae_loss: 0.2552 (0.2609)  classification_loss: 1.2988 (1.3085)  loss_mask: 0.0006 (0.0143)  time: 0.1988  data: 0.0002  max mem: 5511
[08:53:54.240562] Epoch: [58]  [480/781]  eta: 0:00:59  lr: 0.000100  training_loss: 1.5569 (1.5833)  mae_loss: 0.2688 (0.2615)  classification_loss: 1.2938 (1.3080)  loss_mask: 0.0011 (0.0138)  time: 0.1977  data: 0.0002  max mem: 5511
[08:53:58.202440] Epoch: [58]  [500/781]  eta: 0:00:55  lr: 0.000100  training_loss: 1.5646 (1.5831)  mae_loss: 0.2652 (0.2615)  classification_loss: 1.3164 (1.3083)  loss_mask: 0.0005 (0.0133)  time: 0.1980  data: 0.0003  max mem: 5511
[08:54:02.143371] Epoch: [58]  [520/781]  eta: 0:00:51  lr: 0.000100  training_loss: 1.5340 (1.5820)  mae_loss: 0.2424 (0.2613)  classification_loss: 1.2919 (1.3079)  loss_mask: 0.0005 (0.0128)  time: 0.1969  data: 0.0002  max mem: 5511
[08:54:06.129933] Epoch: [58]  [540/781]  eta: 0:00:47  lr: 0.000100  training_loss: 1.5572 (1.5817)  mae_loss: 0.2499 (0.2612)  classification_loss: 1.2945 (1.3076)  loss_mask: 0.0013 (0.0129)  time: 0.1992  data: 0.0002  max mem: 5511
[08:54:10.066244] Epoch: [58]  [560/781]  eta: 0:00:43  lr: 0.000100  training_loss: 1.6678 (1.5865)  mae_loss: 0.2530 (0.2613)  classification_loss: 1.2521 (1.3060)  loss_mask: 0.1367 (0.0192)  time: 0.1967  data: 0.0002  max mem: 5511
[08:54:14.023404] Epoch: [58]  [580/781]  eta: 0:00:40  lr: 0.000100  training_loss: 1.6245 (1.5874)  mae_loss: 0.2651 (0.2615)  classification_loss: 1.3056 (1.3054)  loss_mask: 0.0438 (0.0206)  time: 0.1978  data: 0.0003  max mem: 5511
[08:54:17.985659] Epoch: [58]  [600/781]  eta: 0:00:36  lr: 0.000100  training_loss: 1.5132 (1.5861)  mae_loss: 0.2515 (0.2614)  classification_loss: 1.2328 (1.3040)  loss_mask: 0.0142 (0.0207)  time: 0.1980  data: 0.0002  max mem: 5511
[08:54:21.912072] Epoch: [58]  [620/781]  eta: 0:00:32  lr: 0.000100  training_loss: 1.5093 (1.5841)  mae_loss: 0.2387 (0.2610)  classification_loss: 1.2411 (1.3025)  loss_mask: 0.0155 (0.0206)  time: 0.1962  data: 0.0002  max mem: 5511
[08:54:25.854572] Epoch: [58]  [640/781]  eta: 0:00:28  lr: 0.000100  training_loss: 1.5375 (1.5831)  mae_loss: 0.2649 (0.2611)  classification_loss: 1.2795 (1.3018)  loss_mask: 0.0067 (0.0202)  time: 0.1970  data: 0.0002  max mem: 5511
[08:54:29.814208] Epoch: [58]  [660/781]  eta: 0:00:24  lr: 0.000100  training_loss: 1.5478 (1.5822)  mae_loss: 0.2432 (0.2606)  classification_loss: 1.3262 (1.3017)  loss_mask: 0.0046 (0.0198)  time: 0.1978  data: 0.0002  max mem: 5511
[08:54:33.755159] Epoch: [58]  [680/781]  eta: 0:00:20  lr: 0.000099  training_loss: 1.5663 (1.5825)  mae_loss: 0.2700 (0.2608)  classification_loss: 1.3003 (1.3023)  loss_mask: 0.0017 (0.0194)  time: 0.1970  data: 0.0003  max mem: 5511
[08:54:37.721135] Epoch: [58]  [700/781]  eta: 0:00:16  lr: 0.000099  training_loss: 1.5778 (1.5832)  mae_loss: 0.2616 (0.2610)  classification_loss: 1.3124 (1.3033)  loss_mask: 0.0026 (0.0189)  time: 0.1982  data: 0.0002  max mem: 5511
[08:54:41.694286] Epoch: [58]  [720/781]  eta: 0:00:12  lr: 0.000099  training_loss: 1.6147 (1.5838)  mae_loss: 0.2496 (0.2607)  classification_loss: 1.3430 (1.3047)  loss_mask: 0.0013 (0.0184)  time: 0.1986  data: 0.0002  max mem: 5511
[08:54:45.657512] Epoch: [58]  [740/781]  eta: 0:00:08  lr: 0.000099  training_loss: 1.4807 (1.5818)  mae_loss: 0.2481 (0.2605)  classification_loss: 1.2305 (1.3034)  loss_mask: 0.0011 (0.0179)  time: 0.1981  data: 0.0003  max mem: 5511
[08:54:49.600401] Epoch: [58]  [760/781]  eta: 0:00:04  lr: 0.000099  training_loss: 1.5469 (1.5811)  mae_loss: 0.2460 (0.2604)  classification_loss: 1.2657 (1.3032)  loss_mask: 0.0007 (0.0175)  time: 0.1970  data: 0.0002  max mem: 5511
[08:54:53.529981] Epoch: [58]  [780/781]  eta: 0:00:00  lr: 0.000099  training_loss: 1.5684 (1.5812)  mae_loss: 0.2568 (0.2603)  classification_loss: 1.3083 (1.3039)  loss_mask: 0.0007 (0.0171)  time: 0.1964  data: 0.0003  max mem: 5511
[08:54:53.696526] Epoch: [58] Total time: 0:02:35 (0.1989 s / it)
[08:54:53.697112] Averaged stats: lr: 0.000099  training_loss: 1.5684 (1.5812)  mae_loss: 0.2568 (0.2603)  classification_loss: 1.3083 (1.3039)  loss_mask: 0.0007 (0.0171)
[08:54:54.418807] Test:  [  0/157]  eta: 0:01:52  testing_loss: 0.5814 (0.5814)  acc1: 82.8125 (82.8125)  acc5: 96.8750 (96.8750)  time: 0.7171  data: 0.6813  max mem: 5511
[08:54:54.710651] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.6193 (0.6190)  acc1: 81.2500 (80.8239)  acc5: 100.0000 (99.2898)  time: 0.0915  data: 0.0621  max mem: 5511
[08:54:54.995046] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5472 (0.5832)  acc1: 81.2500 (81.8452)  acc5: 100.0000 (99.3304)  time: 0.0286  data: 0.0002  max mem: 5511
[08:54:55.281848] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5472 (0.5950)  acc1: 82.8125 (81.7540)  acc5: 100.0000 (99.0927)  time: 0.0284  data: 0.0001  max mem: 5511
[08:54:55.566977] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6100 (0.6064)  acc1: 81.2500 (81.3643)  acc5: 98.4375 (99.0854)  time: 0.0285  data: 0.0002  max mem: 5511
[08:54:55.851252] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5923 (0.5997)  acc1: 82.8125 (81.8934)  acc5: 98.4375 (99.0196)  time: 0.0283  data: 0.0002  max mem: 5511
[08:54:56.133915] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5648 (0.5949)  acc1: 82.8125 (81.7623)  acc5: 100.0000 (99.0779)  time: 0.0282  data: 0.0002  max mem: 5511
[08:54:56.417683] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5708 (0.5919)  acc1: 81.2500 (81.8442)  acc5: 100.0000 (99.1197)  time: 0.0282  data: 0.0002  max mem: 5511
[08:54:56.702425] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6180 (0.6023)  acc1: 79.6875 (81.4429)  acc5: 100.0000 (99.1127)  time: 0.0283  data: 0.0002  max mem: 5511
[08:54:56.986400] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6217 (0.6005)  acc1: 79.6875 (81.4732)  acc5: 100.0000 (99.1587)  time: 0.0283  data: 0.0002  max mem: 5511
[08:54:57.269258] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.6122 (0.6047)  acc1: 81.2500 (81.3738)  acc5: 100.0000 (99.1337)  time: 0.0282  data: 0.0002  max mem: 5511
[08:54:57.551767] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6309 (0.6084)  acc1: 79.6875 (81.2641)  acc5: 98.4375 (99.0850)  time: 0.0281  data: 0.0002  max mem: 5511
[08:54:57.839122] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6157 (0.6064)  acc1: 79.6875 (81.2500)  acc5: 98.4375 (99.0961)  time: 0.0284  data: 0.0002  max mem: 5511
[08:54:58.129270] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6086 (0.6074)  acc1: 79.6875 (81.2261)  acc5: 100.0000 (99.1412)  time: 0.0287  data: 0.0002  max mem: 5511
[08:54:58.417826] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5772 (0.6048)  acc1: 81.2500 (81.4162)  acc5: 100.0000 (99.1578)  time: 0.0288  data: 0.0002  max mem: 5511
[08:54:58.700421] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5738 (0.6022)  acc1: 82.8125 (81.4259)  acc5: 98.4375 (99.1308)  time: 0.0284  data: 0.0001  max mem: 5511
[08:54:58.852060] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5738 (0.6023)  acc1: 82.8125 (81.4000)  acc5: 98.4375 (99.1300)  time: 0.0272  data: 0.0001  max mem: 5511
[08:54:59.021325] Test: Total time: 0:00:05 (0.0339 s / it)
[08:54:59.021763] * Acc@1 81.400 Acc@5 99.130 loss 0.602
[08:54:59.022051] Accuracy of the network on the 10000 test images: 81.4%
[08:54:59.022226] Max accuracy: 81.42%
[08:54:59.313361] log_dir: ./output_dir
[08:55:00.275497] Epoch: [59]  [  0/781]  eta: 0:12:30  lr: 0.000099  training_loss: 1.2753 (1.2753)  mae_loss: 0.2300 (0.2300)  classification_loss: 1.0445 (1.0445)  loss_mask: 0.0009 (0.0009)  time: 0.9604  data: 0.7370  max mem: 5511
[08:55:04.224825] Epoch: [59]  [ 20/781]  eta: 0:02:57  lr: 0.000099  training_loss: 1.5263 (1.5230)  mae_loss: 0.2759 (0.2720)  classification_loss: 1.2495 (1.2500)  loss_mask: 0.0006 (0.0010)  time: 0.1974  data: 0.0002  max mem: 5511
[08:55:08.197328] Epoch: [59]  [ 40/781]  eta: 0:02:40  lr: 0.000099  training_loss: 1.5669 (1.5468)  mae_loss: 0.2586 (0.2674)  classification_loss: 1.3104 (1.2785)  loss_mask: 0.0006 (0.0010)  time: 0.1985  data: 0.0006  max mem: 5511
[08:55:12.149430] Epoch: [59]  [ 60/781]  eta: 0:02:31  lr: 0.000099  training_loss: 1.5290 (1.5546)  mae_loss: 0.2641 (0.2662)  classification_loss: 1.2590 (1.2865)  loss_mask: 0.0006 (0.0020)  time: 0.1975  data: 0.0003  max mem: 5511
[08:55:16.085388] Epoch: [59]  [ 80/781]  eta: 0:02:25  lr: 0.000099  training_loss: 1.5482 (1.5531)  mae_loss: 0.2577 (0.2662)  classification_loss: 1.2680 (1.2841)  loss_mask: 0.0016 (0.0028)  time: 0.1967  data: 0.0002  max mem: 5511
[08:55:20.014970] Epoch: [59]  [100/781]  eta: 0:02:19  lr: 0.000098  training_loss: 1.5781 (1.5556)  mae_loss: 0.2609 (0.2650)  classification_loss: 1.3040 (1.2879)  loss_mask: 0.0018 (0.0027)  time: 0.1964  data: 0.0002  max mem: 5511
[08:55:23.947155] Epoch: [59]  [120/781]  eta: 0:02:14  lr: 0.000098  training_loss: 1.5266 (1.5497)  mae_loss: 0.2495 (0.2628)  classification_loss: 1.2704 (1.2845)  loss_mask: 0.0004 (0.0024)  time: 0.1965  data: 0.0002  max mem: 5511
[08:55:27.880091] Epoch: [59]  [140/781]  eta: 0:02:09  lr: 0.000098  training_loss: 1.5296 (1.5458)  mae_loss: 0.2662 (0.2623)  classification_loss: 1.2667 (1.2815)  loss_mask: 0.0003 (0.0021)  time: 0.1966  data: 0.0002  max mem: 5511
[08:55:31.824506] Epoch: [59]  [160/781]  eta: 0:02:05  lr: 0.000098  training_loss: 1.4934 (1.5423)  mae_loss: 0.2523 (0.2613)  classification_loss: 1.2581 (1.2791)  loss_mask: 0.0004 (0.0019)  time: 0.1971  data: 0.0004  max mem: 5511
[08:55:35.756063] Epoch: [59]  [180/781]  eta: 0:02:00  lr: 0.000098  training_loss: 1.5681 (1.5444)  mae_loss: 0.2588 (0.2610)  classification_loss: 1.3098 (1.2818)  loss_mask: 0.0003 (0.0017)  time: 0.1965  data: 0.0002  max mem: 5511
[08:55:39.751068] Epoch: [59]  [200/781]  eta: 0:01:56  lr: 0.000098  training_loss: 1.5223 (1.5412)  mae_loss: 0.2537 (0.2609)  classification_loss: 1.2692 (1.2787)  loss_mask: 0.0003 (0.0016)  time: 0.1997  data: 0.0003  max mem: 5511
[08:55:43.723800] Epoch: [59]  [220/781]  eta: 0:01:52  lr: 0.000098  training_loss: 1.5949 (1.5448)  mae_loss: 0.2584 (0.2608)  classification_loss: 1.3148 (1.2825)  loss_mask: 0.0003 (0.0015)  time: 0.1986  data: 0.0002  max mem: 5511
[08:55:47.713744] Epoch: [59]  [240/781]  eta: 0:01:48  lr: 0.000098  training_loss: 1.5486 (1.5451)  mae_loss: 0.2384 (0.2598)  classification_loss: 1.2869 (1.2840)  loss_mask: 0.0003 (0.0014)  time: 0.1994  data: 0.0002  max mem: 5511
[08:55:51.708308] Epoch: [59]  [260/781]  eta: 0:01:44  lr: 0.000098  training_loss: 1.5370 (1.5449)  mae_loss: 0.2635 (0.2598)  classification_loss: 1.2760 (1.2838)  loss_mask: 0.0002 (0.0013)  time: 0.1996  data: 0.0003  max mem: 5511
[08:55:55.674901] Epoch: [59]  [280/781]  eta: 0:01:40  lr: 0.000098  training_loss: 1.5259 (1.5461)  mae_loss: 0.2591 (0.2599)  classification_loss: 1.2782 (1.2850)  loss_mask: 0.0002 (0.0012)  time: 0.1982  data: 0.0002  max mem: 5511
[08:55:59.617265] Epoch: [59]  [300/781]  eta: 0:01:36  lr: 0.000097  training_loss: 1.5541 (1.5478)  mae_loss: 0.2551 (0.2594)  classification_loss: 1.3329 (1.2873)  loss_mask: 0.0002 (0.0011)  time: 0.1970  data: 0.0002  max mem: 5511
[08:56:03.544090] Epoch: [59]  [320/781]  eta: 0:01:32  lr: 0.000097  training_loss: 1.5222 (1.5470)  mae_loss: 0.2417 (0.2588)  classification_loss: 1.2767 (1.2871)  loss_mask: 0.0002 (0.0011)  time: 0.1963  data: 0.0002  max mem: 5511
[08:56:07.529204] Epoch: [59]  [340/781]  eta: 0:01:28  lr: 0.000097  training_loss: 1.5287 (1.5450)  mae_loss: 0.2661 (0.2590)  classification_loss: 1.2397 (1.2850)  loss_mask: 0.0002 (0.0011)  time: 0.1992  data: 0.0002  max mem: 5511
[08:56:11.470469] Epoch: [59]  [360/781]  eta: 0:01:24  lr: 0.000097  training_loss: 1.5479 (1.5459)  mae_loss: 0.2541 (0.2589)  classification_loss: 1.2970 (1.2856)  loss_mask: 0.0004 (0.0014)  time: 0.1970  data: 0.0003  max mem: 5511
[08:56:15.423757] Epoch: [59]  [380/781]  eta: 0:01:20  lr: 0.000097  training_loss: 1.5582 (1.5466)  mae_loss: 0.2609 (0.2592)  classification_loss: 1.2830 (1.2857)  loss_mask: 0.0011 (0.0017)  time: 0.1976  data: 0.0002  max mem: 5511
[08:56:19.397647] Epoch: [59]  [400/781]  eta: 0:01:16  lr: 0.000097  training_loss: 1.5618 (1.5481)  mae_loss: 0.2492 (0.2589)  classification_loss: 1.3013 (1.2874)  loss_mask: 0.0019 (0.0018)  time: 0.1986  data: 0.0003  max mem: 5511
[08:56:23.370416] Epoch: [59]  [420/781]  eta: 0:01:12  lr: 0.000097  training_loss: 1.5362 (1.5473)  mae_loss: 0.2448 (0.2585)  classification_loss: 1.2933 (1.2870)  loss_mask: 0.0008 (0.0018)  time: 0.1985  data: 0.0002  max mem: 5511
[08:56:27.358252] Epoch: [59]  [440/781]  eta: 0:01:08  lr: 0.000097  training_loss: 1.5094 (1.5461)  mae_loss: 0.2591 (0.2585)  classification_loss: 1.2920 (1.2859)  loss_mask: 0.0005 (0.0018)  time: 0.1993  data: 0.0003  max mem: 5511
[08:56:31.291831] Epoch: [59]  [460/781]  eta: 0:01:04  lr: 0.000097  training_loss: 1.5379 (1.5450)  mae_loss: 0.2588 (0.2584)  classification_loss: 1.2824 (1.2844)  loss_mask: 0.0009 (0.0021)  time: 0.1966  data: 0.0002  max mem: 5511
[08:56:35.258567] Epoch: [59]  [480/781]  eta: 0:01:00  lr: 0.000096  training_loss: 1.5393 (1.5439)  mae_loss: 0.2527 (0.2582)  classification_loss: 1.2848 (1.2835)  loss_mask: 0.0004 (0.0021)  time: 0.1983  data: 0.0002  max mem: 5511
[08:56:39.222060] Epoch: [59]  [500/781]  eta: 0:00:56  lr: 0.000096  training_loss: 1.6297 (1.5478)  mae_loss: 0.2555 (0.2584)  classification_loss: 1.3182 (1.2851)  loss_mask: 0.0156 (0.0043)  time: 0.1981  data: 0.0005  max mem: 5511
[08:56:43.174529] Epoch: [59]  [520/781]  eta: 0:00:52  lr: 0.000096  training_loss: 1.6098 (1.5516)  mae_loss: 0.2647 (0.2590)  classification_loss: 1.2928 (1.2866)  loss_mask: 0.0189 (0.0060)  time: 0.1975  data: 0.0003  max mem: 5511
[08:56:47.126674] Epoch: [59]  [540/781]  eta: 0:00:48  lr: 0.000096  training_loss: 1.6703 (1.5555)  mae_loss: 0.2713 (0.2594)  classification_loss: 1.3445 (1.2890)  loss_mask: 0.0149 (0.0071)  time: 0.1975  data: 0.0003  max mem: 5511
[08:56:51.094086] Epoch: [59]  [560/781]  eta: 0:00:44  lr: 0.000096  training_loss: 1.6625 (1.5590)  mae_loss: 0.2657 (0.2596)  classification_loss: 1.3654 (1.2917)  loss_mask: 0.0107 (0.0077)  time: 0.1983  data: 0.0003  max mem: 5511
[08:56:55.041030] Epoch: [59]  [580/781]  eta: 0:00:40  lr: 0.000096  training_loss: 1.6622 (1.5635)  mae_loss: 0.2532 (0.2598)  classification_loss: 1.3211 (1.2923)  loss_mask: 0.0560 (0.0113)  time: 0.1973  data: 0.0002  max mem: 5511
[08:56:58.984085] Epoch: [59]  [600/781]  eta: 0:00:36  lr: 0.000096  training_loss: 1.6346 (1.5658)  mae_loss: 0.2603 (0.2601)  classification_loss: 1.3110 (1.2930)  loss_mask: 0.0442 (0.0128)  time: 0.1971  data: 0.0004  max mem: 5511
[08:57:02.943293] Epoch: [59]  [620/781]  eta: 0:00:32  lr: 0.000096  training_loss: 1.5306 (1.5655)  mae_loss: 0.2666 (0.2603)  classification_loss: 1.2390 (1.2919)  loss_mask: 0.0250 (0.0132)  time: 0.1979  data: 0.0002  max mem: 5511
[08:57:06.905124] Epoch: [59]  [640/781]  eta: 0:00:28  lr: 0.000096  training_loss: 1.5882 (1.5665)  mae_loss: 0.2515 (0.2602)  classification_loss: 1.3292 (1.2930)  loss_mask: 0.0119 (0.0132)  time: 0.1980  data: 0.0003  max mem: 5511
[08:57:10.831008] Epoch: [59]  [660/781]  eta: 0:00:24  lr: 0.000096  training_loss: 1.5601 (1.5659)  mae_loss: 0.2443 (0.2598)  classification_loss: 1.2871 (1.2931)  loss_mask: 0.0034 (0.0130)  time: 0.1962  data: 0.0002  max mem: 5511
[08:57:14.790298] Epoch: [59]  [680/781]  eta: 0:00:20  lr: 0.000095  training_loss: 1.5254 (1.5653)  mae_loss: 0.2674 (0.2601)  classification_loss: 1.2598 (1.2925)  loss_mask: 0.0021 (0.0127)  time: 0.1979  data: 0.0002  max mem: 5511
[08:57:18.728901] Epoch: [59]  [700/781]  eta: 0:00:16  lr: 0.000095  training_loss: 1.5652 (1.5655)  mae_loss: 0.2666 (0.2606)  classification_loss: 1.2894 (1.2925)  loss_mask: 0.0012 (0.0124)  time: 0.1968  data: 0.0002  max mem: 5511
[08:57:22.655707] Epoch: [59]  [720/781]  eta: 0:00:12  lr: 0.000095  training_loss: 1.5877 (1.5655)  mae_loss: 0.2421 (0.2603)  classification_loss: 1.3031 (1.2930)  loss_mask: 0.0014 (0.0121)  time: 0.1963  data: 0.0002  max mem: 5511
[08:57:26.608502] Epoch: [59]  [740/781]  eta: 0:00:08  lr: 0.000095  training_loss: 1.4808 (1.5643)  mae_loss: 0.2531 (0.2602)  classification_loss: 1.2310 (1.2922)  loss_mask: 0.0013 (0.0119)  time: 0.1975  data: 0.0002  max mem: 5511
[08:57:30.613113] Epoch: [59]  [760/781]  eta: 0:00:04  lr: 0.000095  training_loss: 1.5237 (1.5639)  mae_loss: 0.2721 (0.2605)  classification_loss: 1.2409 (1.2918)  loss_mask: 0.0006 (0.0116)  time: 0.2001  data: 0.0002  max mem: 5511
[08:57:34.546643] Epoch: [59]  [780/781]  eta: 0:00:00  lr: 0.000095  training_loss: 1.5020 (1.5627)  mae_loss: 0.2614 (0.2605)  classification_loss: 1.2325 (1.2909)  loss_mask: 0.0008 (0.0113)  time: 0.1966  data: 0.0002  max mem: 5511
[08:57:34.718957] Epoch: [59] Total time: 0:02:35 (0.1990 s / it)
[08:57:34.719555] Averaged stats: lr: 0.000095  training_loss: 1.5020 (1.5627)  mae_loss: 0.2614 (0.2605)  classification_loss: 1.2325 (1.2909)  loss_mask: 0.0008 (0.0113)
[08:57:35.309100] Test:  [  0/157]  eta: 0:01:31  testing_loss: 0.5548 (0.5548)  acc1: 84.3750 (84.3750)  acc5: 96.8750 (96.8750)  time: 0.5847  data: 0.5550  max mem: 5511
[08:57:35.597763] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.5453 (0.5725)  acc1: 84.3750 (82.1023)  acc5: 100.0000 (99.2898)  time: 0.0792  data: 0.0508  max mem: 5511
[08:57:35.881150] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.5451 (0.5506)  acc1: 82.8125 (82.5893)  acc5: 100.0000 (99.4792)  time: 0.0284  data: 0.0003  max mem: 5511
[08:57:36.164823] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.5542 (0.5608)  acc1: 82.8125 (82.3589)  acc5: 100.0000 (99.3448)  time: 0.0282  data: 0.0002  max mem: 5511
[08:57:36.447804] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.5805 (0.5735)  acc1: 81.2500 (81.9741)  acc5: 100.0000 (99.3140)  time: 0.0282  data: 0.0002  max mem: 5511
[08:57:36.735337] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5467 (0.5639)  acc1: 82.8125 (82.5061)  acc5: 98.4375 (99.2953)  time: 0.0284  data: 0.0002  max mem: 5511
[08:57:37.019377] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5008 (0.5562)  acc1: 84.3750 (82.6588)  acc5: 100.0000 (99.3084)  time: 0.0285  data: 0.0002  max mem: 5511
[08:57:37.303725] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5008 (0.5532)  acc1: 84.3750 (82.8565)  acc5: 100.0000 (99.3178)  time: 0.0283  data: 0.0002  max mem: 5511
[08:57:37.591167] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5710 (0.5622)  acc1: 81.2500 (82.3881)  acc5: 100.0000 (99.3056)  time: 0.0285  data: 0.0002  max mem: 5511
[08:57:37.878456] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5747 (0.5612)  acc1: 79.6875 (82.4863)  acc5: 98.4375 (99.2273)  time: 0.0286  data: 0.0002  max mem: 5511
[08:57:38.163546] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5639 (0.5650)  acc1: 81.2500 (82.3484)  acc5: 98.4375 (99.2110)  time: 0.0285  data: 0.0002  max mem: 5511
[08:57:38.452501] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5870 (0.5666)  acc1: 81.2500 (82.3339)  acc5: 100.0000 (99.2117)  time: 0.0286  data: 0.0002  max mem: 5511
[08:57:38.740446] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5400 (0.5613)  acc1: 81.2500 (82.4768)  acc5: 100.0000 (99.2252)  time: 0.0287  data: 0.0002  max mem: 5511
[08:57:39.025382] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5288 (0.5618)  acc1: 81.2500 (82.4070)  acc5: 100.0000 (99.2486)  time: 0.0285  data: 0.0002  max mem: 5511
[08:57:39.311759] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5636 (0.5617)  acc1: 81.2500 (82.4136)  acc5: 100.0000 (99.2686)  time: 0.0284  data: 0.0002  max mem: 5511
[08:57:39.594508] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5614 (0.5605)  acc1: 81.2500 (82.4503)  acc5: 100.0000 (99.2757)  time: 0.0283  data: 0.0002  max mem: 5511
[08:57:39.747104] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5436 (0.5608)  acc1: 81.2500 (82.4700)  acc5: 100.0000 (99.2800)  time: 0.0273  data: 0.0001  max mem: 5511
[08:57:39.936534] Test: Total time: 0:00:05 (0.0332 s / it)
[08:57:39.937087] * Acc@1 82.470 Acc@5 99.280 loss 0.561
[08:57:39.937434] Accuracy of the network on the 10000 test images: 82.5%
[08:57:39.937705] Max accuracy: 82.47%
[08:57:40.240347] log_dir: ./output_dir
[08:57:41.078488] Epoch: [60]  [  0/781]  eta: 0:10:52  lr: 0.000095  training_loss: 1.3410 (1.3410)  mae_loss: 0.2174 (0.2174)  classification_loss: 1.1234 (1.1234)  loss_mask: 0.0003 (0.0003)  time: 0.8360  data: 0.6191  max mem: 5511
[08:57:45.017911] Epoch: [60]  [ 20/781]  eta: 0:02:52  lr: 0.000095  training_loss: 1.4524 (1.4959)  mae_loss: 0.2441 (0.2513)  classification_loss: 1.2278 (1.2438)  loss_mask: 0.0006 (0.0008)  time: 0.1969  data: 0.0002  max mem: 5511

[08:57:48.989169] Epoch: [60]  [ 40/781]  eta: 0:02:37  lr: 0.000095  training_loss: 1.5272 (1.5265)  mae_loss: 0.2586 (0.2553)  classification_loss: 1.2800 (1.2705)  loss_mask: 0.0003 (0.0007)  time: 0.1984  data: 0.0002  max mem: 5511
[08:57:52.923825] Epoch: [60]  [ 60/781]  eta: 0:02:29  lr: 0.000095  training_loss: 1.5233 (1.5353)  mae_loss: 0.2553 (0.2570)  classification_loss: 1.2834 (1.2777)  loss_mask: 0.0004 (0.0007)  time: 0.1967  data: 0.0002  max mem: 5511
[08:57:56.886125] Epoch: [60]  [ 80/781]  eta: 0:02:23  lr: 0.000095  training_loss: 1.5099 (1.5305)  mae_loss: 0.2561 (0.2579)  classification_loss: 1.2406 (1.2720)  loss_mask: 0.0003 (0.0007)  time: 0.1980  data: 0.0002  max mem: 5511
[08:58:00.855502] Epoch: [60]  [100/781]  eta: 0:02:18  lr: 0.000094  training_loss: 1.5427 (1.5367)  mae_loss: 0.2687 (0.2588)  classification_loss: 1.2788 (1.2772)  loss_mask: 0.0004 (0.0006)  time: 0.1984  data: 0.0002  max mem: 5511
[08:58:04.801470] Epoch: [60]  [120/781]  eta: 0:02:14  lr: 0.000094  training_loss: 1.5486 (1.5356)  mae_loss: 0.2494 (0.2586)  classification_loss: 1.2603 (1.2764)  loss_mask: 0.0003 (0.0006)  time: 0.1972  data: 0.0002  max mem: 5511
[08:58:08.765457] Epoch: [60]  [140/781]  eta: 0:02:09  lr: 0.000094  training_loss: 1.5547 (1.5376)  mae_loss: 0.2580 (0.2583)  classification_loss: 1.2785 (1.2788)  loss_mask: 0.0003 (0.0005)  time: 0.1981  data: 0.0002  max mem: 5511
[08:58:12.718865] Epoch: [60]  [160/781]  eta: 0:02:05  lr: 0.000094  training_loss: 1.5102 (1.5359)  mae_loss: 0.2595 (0.2586)  classification_loss: 1.2548 (1.2768)  loss_mask: 0.0002 (0.0005)  time: 0.1976  data: 0.0004  max mem: 5511
[08:58:16.665663] Epoch: [60]  [180/781]  eta: 0:02:00  lr: 0.000094  training_loss: 1.5139 (1.5385)  mae_loss: 0.2542 (0.2581)  classification_loss: 1.2826 (1.2798)  loss_mask: 0.0002 (0.0005)  time: 0.1972  data: 0.0003  max mem: 5511
[08:58:20.601748] Epoch: [60]  [200/781]  eta: 0:01:56  lr: 0.000094  training_loss: 1.5240 (1.5366)  mae_loss: 0.2477 (0.2572)  classification_loss: 1.2532 (1.2789)  loss_mask: 0.0003 (0.0005)  time: 0.1967  data: 0.0002  max mem: 5511
[08:58:24.541922] Epoch: [60]  [220/781]  eta: 0:01:52  lr: 0.000094  training_loss: 1.5377 (1.5383)  mae_loss: 0.2484 (0.2567)  classification_loss: 1.2588 (1.2812)  loss_mask: 0.0003 (0.0005)  time: 0.1969  data: 0.0002  max mem: 5511
[08:58:28.510418] Epoch: [60]  [240/781]  eta: 0:01:48  lr: 0.000094  training_loss: 1.6062 (1.5436)  mae_loss: 0.2682 (0.2579)  classification_loss: 1.3511 (1.2851)  loss_mask: 0.0005 (0.0007)  time: 0.1983  data: 0.0003  max mem: 5511
[08:58:32.491482] Epoch: [60]  [260/781]  eta: 0:01:44  lr: 0.000094  training_loss: 1.5581 (1.5443)  mae_loss: 0.2516 (0.2579)  classification_loss: 1.2979 (1.2857)  loss_mask: 0.0005 (0.0007)  time: 0.1989  data: 0.0002  max mem: 5511
[08:58:36.447018] Epoch: [60]  [280/781]  eta: 0:01:40  lr: 0.000094  training_loss: 1.5739 (1.5452)  mae_loss: 0.2646 (0.2586)  classification_loss: 1.2994 (1.2859)  loss_mask: 0.0004 (0.0007)  time: 0.1977  data: 0.0004  max mem: 5511
[08:58:40.399611] Epoch: [60]  [300/781]  eta: 0:01:36  lr: 0.000093  training_loss: 1.5232 (1.5449)  mae_loss: 0.2482 (0.2581)  classification_loss: 1.2867 (1.2861)  loss_mask: 0.0002 (0.0007)  time: 0.1975  data: 0.0003  max mem: 5511
[08:58:44.332164] Epoch: [60]  [320/781]  eta: 0:01:31  lr: 0.000093  training_loss: 1.5496 (1.5448)  mae_loss: 0.2681 (0.2588)  classification_loss: 1.2571 (1.2854)  loss_mask: 0.0002 (0.0006)  time: 0.1965  data: 0.0002  max mem: 5511
[08:58:48.315946] Epoch: [60]  [340/781]  eta: 0:01:27  lr: 0.000093  training_loss: 1.4797 (1.5419)  mae_loss: 0.2633 (0.2588)  classification_loss: 1.2233 (1.2826)  loss_mask: 0.0002 (0.0006)  time: 0.1991  data: 0.0002  max mem: 5511
[08:58:52.287527] Epoch: [60]  [360/781]  eta: 0:01:23  lr: 0.000093  training_loss: 1.5208 (1.5407)  mae_loss: 0.2601 (0.2586)  classification_loss: 1.2472 (1.2813)  loss_mask: 0.0004 (0.0007)  time: 0.1985  data: 0.0002  max mem: 5511
[08:58:56.294214] Epoch: [60]  [380/781]  eta: 0:01:20  lr: 0.000093  training_loss: 1.5710 (1.5427)  mae_loss: 0.2609 (0.2591)  classification_loss: 1.3107 (1.2829)  loss_mask: 0.0003 (0.0007)  time: 0.2002  data: 0.0005  max mem: 5511
[08:59:00.225181] Epoch: [60]  [400/781]  eta: 0:01:15  lr: 0.000093  training_loss: 1.5262 (1.5420)  mae_loss: 0.2573 (0.2593)  classification_loss: 1.2464 (1.2820)  loss_mask: 0.0003 (0.0007)  time: 0.1965  data: 0.0002  max mem: 5511
[08:59:04.166928] Epoch: [60]  [420/781]  eta: 0:01:11  lr: 0.000093  training_loss: 1.5481 (1.5426)  mae_loss: 0.2603 (0.2593)  classification_loss: 1.2868 (1.2827)  loss_mask: 0.0002 (0.0007)  time: 0.1970  data: 0.0002  max mem: 5511
[08:59:08.117830] Epoch: [60]  [440/781]  eta: 0:01:07  lr: 0.000093  training_loss: 1.5293 (1.5439)  mae_loss: 0.2613 (0.2596)  classification_loss: 1.2772 (1.2836)  loss_mask: 0.0002 (0.0006)  time: 0.1974  data: 0.0005  max mem: 5511
[08:59:12.065100] Epoch: [60]  [460/781]  eta: 0:01:03  lr: 0.000093  training_loss: 1.5456 (1.5437)  mae_loss: 0.2652 (0.2600)  classification_loss: 1.2752 (1.2831)  loss_mask: 0.0001 (0.0006)  time: 0.1973  data: 0.0002  max mem: 5511
[08:59:16.007365] Epoch: [60]  [480/781]  eta: 0:00:59  lr: 0.000092  training_loss: 1.5339 (1.5436)  mae_loss: 0.2295 (0.2592)  classification_loss: 1.2995 (1.2838)  loss_mask: 0.0002 (0.0006)  time: 0.1970  data: 0.0002  max mem: 5511
[08:59:19.967122] Epoch: [60]  [500/781]  eta: 0:00:55  lr: 0.000092  training_loss: 1.5450 (1.5431)  mae_loss: 0.2595 (0.2593)  classification_loss: 1.2828 (1.2831)  loss_mask: 0.0002 (0.0007)  time: 0.1979  data: 0.0003  max mem: 5511
[08:59:23.909715] Epoch: [60]  [520/781]  eta: 0:00:51  lr: 0.000092  training_loss: 1.5293 (1.5421)  mae_loss: 0.2530 (0.2593)  classification_loss: 1.2404 (1.2820)  loss_mask: 0.0006 (0.0007)  time: 0.1970  data: 0.0003  max mem: 5511
[08:59:27.841398] Epoch: [60]  [540/781]  eta: 0:00:47  lr: 0.000092  training_loss: 1.5220 (1.5421)  mae_loss: 0.2501 (0.2592)  classification_loss: 1.2807 (1.2822)  loss_mask: 0.0002 (0.0007)  time: 0.1965  data: 0.0002  max mem: 5511
[08:59:31.791767] Epoch: [60]  [560/781]  eta: 0:00:43  lr: 0.000092  training_loss: 1.5221 (1.5408)  mae_loss: 0.2630 (0.2594)  classification_loss: 1.2602 (1.2807)  loss_mask: 0.0001 (0.0007)  time: 0.1974  data: 0.0002  max mem: 5511
[08:59:35.734851] Epoch: [60]  [580/781]  eta: 0:00:39  lr: 0.000092  training_loss: 1.6104 (1.5429)  mae_loss: 0.2585 (0.2596)  classification_loss: 1.3276 (1.2823)  loss_mask: 0.0012 (0.0009)  time: 0.1971  data: 0.0002  max mem: 5511
[08:59:39.669380] Epoch: [60]  [600/781]  eta: 0:00:35  lr: 0.000092  training_loss: 1.5626 (1.5454)  mae_loss: 0.2609 (0.2597)  classification_loss: 1.2586 (1.2820)  loss_mask: 0.0312 (0.0037)  time: 0.1966  data: 0.0002  max mem: 5511
[08:59:43.649814] Epoch: [60]  [620/781]  eta: 0:00:31  lr: 0.000092  training_loss: 1.6349 (1.5482)  mae_loss: 0.2517 (0.2596)  classification_loss: 1.2978 (1.2823)  loss_mask: 0.0517 (0.0063)  time: 0.1989  data: 0.0002  max mem: 5511
[08:59:47.603764] Epoch: [60]  [640/781]  eta: 0:00:28  lr: 0.000092  training_loss: 1.5902 (1.5505)  mae_loss: 0.2594 (0.2596)  classification_loss: 1.2959 (1.2831)  loss_mask: 0.0528 (0.0079)  time: 0.1976  data: 0.0003  max mem: 5511
[08:59:51.554393] Epoch: [60]  [660/781]  eta: 0:00:24  lr: 0.000092  training_loss: 1.5729 (1.5514)  mae_loss: 0.2643 (0.2597)  classification_loss: 1.2774 (1.2833)  loss_mask: 0.0147 (0.0083)  time: 0.1974  data: 0.0002  max mem: 5511
[08:59:55.481410] Epoch: [60]  [680/781]  eta: 0:00:20  lr: 0.000091  training_loss: 1.5398 (1.5511)  mae_loss: 0.2523 (0.2597)  classification_loss: 1.2742 (1.2832)  loss_mask: 0.0062 (0.0083)  time: 0.1963  data: 0.0002  max mem: 5511
[08:59:59.467408] Epoch: [60]  [700/781]  eta: 0:00:16  lr: 0.000091  training_loss: 1.5511 (1.5516)  mae_loss: 0.2543 (0.2597)  classification_loss: 1.2893 (1.2838)  loss_mask: 0.0020 (0.0081)  time: 0.1992  data: 0.0002  max mem: 5511
[09:00:03.393299] Epoch: [60]  [720/781]  eta: 0:00:12  lr: 0.000091  training_loss: 1.5854 (1.5525)  mae_loss: 0.2439 (0.2594)  classification_loss: 1.3121 (1.2851)  loss_mask: 0.0014 (0.0080)  time: 0.1962  data: 0.0002  max mem: 5511
[09:00:07.365297] Epoch: [60]  [740/781]  eta: 0:00:08  lr: 0.000091  training_loss: 1.5051 (1.5524)  mae_loss: 0.2473 (0.2594)  classification_loss: 1.2561 (1.2852)  loss_mask: 0.0009 (0.0078)  time: 0.1985  data: 0.0002  max mem: 5511
[09:00:11.316375] Epoch: [60]  [760/781]  eta: 0:00:04  lr: 0.000091  training_loss: 1.5703 (1.5528)  mae_loss: 0.2482 (0.2592)  classification_loss: 1.3084 (1.2859)  loss_mask: 0.0019 (0.0077)  time: 0.1975  data: 0.0002  max mem: 5511
[09:00:15.261925] Epoch: [60]  [780/781]  eta: 0:00:00  lr: 0.000091  training_loss: 1.5343 (1.5525)  mae_loss: 0.2527 (0.2593)  classification_loss: 1.2604 (1.2857)  loss_mask: 0.0011 (0.0075)  time: 0.1972  data: 0.0002  max mem: 5511
[09:00:15.435753] Epoch: [60] Total time: 0:02:35 (0.1987 s / it)
[09:00:15.436344] Averaged stats: lr: 0.000091  training_loss: 1.5343 (1.5525)  mae_loss: 0.2527 (0.2593)  classification_loss: 1.2604 (1.2857)  loss_mask: 0.0011 (0.0075)
[09:00:16.703559] Test:  [  0/157]  eta: 0:01:55  testing_loss: 0.5217 (0.5217)  acc1: 85.9375 (85.9375)  acc5: 100.0000 (100.0000)  time: 0.7387  data: 0.7061  max mem: 5511
[09:00:16.989974] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.5464 (0.5744)  acc1: 81.2500 (81.9602)  acc5: 100.0000 (99.4318)  time: 0.0930  data: 0.0643  max mem: 5511
[09:00:17.277693] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5194 (0.5498)  acc1: 81.2500 (82.5893)  acc5: 100.0000 (99.4048)  time: 0.0285  data: 0.0002  max mem: 5511
[09:00:17.560480] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5171 (0.5587)  acc1: 84.3750 (82.3085)  acc5: 100.0000 (99.2440)  time: 0.0284  data: 0.0002  max mem: 5511
[09:00:17.844838] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5728 (0.5714)  acc1: 81.2500 (81.7454)  acc5: 98.4375 (99.1235)  time: 0.0282  data: 0.0002  max mem: 5511
[09:00:18.130327] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5402 (0.5631)  acc1: 82.8125 (82.1691)  acc5: 100.0000 (99.2034)  time: 0.0283  data: 0.0002  max mem: 5511
[09:00:18.414675] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5060 (0.5571)  acc1: 84.3750 (82.3514)  acc5: 100.0000 (99.2572)  time: 0.0283  data: 0.0002  max mem: 5511
[09:00:18.699332] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5060 (0.5520)  acc1: 84.3750 (82.5044)  acc5: 100.0000 (99.2958)  time: 0.0283  data: 0.0002  max mem: 5511
[09:00:18.982865] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5279 (0.5615)  acc1: 81.2500 (82.2145)  acc5: 100.0000 (99.2477)  time: 0.0282  data: 0.0002  max mem: 5511
[09:00:19.265733] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5672 (0.5588)  acc1: 81.2500 (82.3146)  acc5: 100.0000 (99.2617)  time: 0.0281  data: 0.0002  max mem: 5511
[09:00:19.551084] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.5402 (0.5615)  acc1: 84.3750 (82.3329)  acc5: 100.0000 (99.2729)  time: 0.0283  data: 0.0002  max mem: 5511
[09:00:19.835128] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6024 (0.5663)  acc1: 81.2500 (82.2776)  acc5: 100.0000 (99.1695)  time: 0.0283  data: 0.0002  max mem: 5511
[09:00:20.119432] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5383 (0.5616)  acc1: 82.8125 (82.4380)  acc5: 100.0000 (99.1994)  time: 0.0282  data: 0.0002  max mem: 5511
[09:00:20.404360] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5317 (0.5611)  acc1: 82.8125 (82.4070)  acc5: 100.0000 (99.2247)  time: 0.0282  data: 0.0002  max mem: 5511
[09:00:20.688319] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5513 (0.5598)  acc1: 82.8125 (82.4690)  acc5: 100.0000 (99.2132)  time: 0.0283  data: 0.0002  max mem: 5511
[09:00:20.969960] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5277 (0.5576)  acc1: 82.8125 (82.5435)  acc5: 98.4375 (99.1825)  time: 0.0281  data: 0.0001  max mem: 5511
[09:00:21.123296] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5124 (0.5574)  acc1: 82.8125 (82.5800)  acc5: 98.4375 (99.1900)  time: 0.0272  data: 0.0001  max mem: 5511
[09:00:21.292041] Test: Total time: 0:00:05 (0.0339 s / it)
[09:00:21.292465] * Acc@1 82.580 Acc@5 99.190 loss 0.557
[09:00:21.292764] Accuracy of the network on the 10000 test images: 82.6%
[09:00:21.292994] Max accuracy: 82.58%
[09:00:21.680655] log_dir: ./output_dir
[09:00:22.669355] Epoch: [61]  [  0/781]  eta: 0:12:50  lr: 0.000091  training_loss: 1.3842 (1.3842)  mae_loss: 0.2648 (0.2648)  classification_loss: 1.1163 (1.1163)  loss_mask: 0.0031 (0.0031)  time: 0.9866  data: 0.7782  max mem: 5511
[09:00:26.618348] Epoch: [61]  [ 20/781]  eta: 0:02:58  lr: 0.000091  training_loss: 1.5481 (1.5596)  mae_loss: 0.2731 (0.2731)  classification_loss: 1.2913 (1.2856)  loss_mask: 0.0006 (0.0009)  time: 0.1973  data: 0.0001  max mem: 5511
[09:00:30.569396] Epoch: [61]  [ 40/781]  eta: 0:02:40  lr: 0.000091  training_loss: 1.5605 (1.5660)  mae_loss: 0.2646 (0.2689)  classification_loss: 1.3079 (1.2963)  loss_mask: 0.0005 (0.0008)  time: 0.1975  data: 0.0002  max mem: 5511
[09:00:34.547211] Epoch: [61]  [ 60/781]  eta: 0:02:31  lr: 0.000091  training_loss: 1.5639 (1.5722)  mae_loss: 0.2683 (0.2680)  classification_loss: 1.3045 (1.3035)  loss_mask: 0.0004 (0.0007)  time: 0.1988  data: 0.0006  max mem: 5511
[09:00:38.474464] Epoch: [61]  [ 80/781]  eta: 0:02:25  lr: 0.000091  training_loss: 1.4974 (1.5588)  mae_loss: 0.2610 (0.2652)  classification_loss: 1.2234 (1.2930)  loss_mask: 0.0003 (0.0006)  time: 0.1963  data: 0.0003  max mem: 5511
[09:00:42.426494] Epoch: [61]  [100/781]  eta: 0:02:19  lr: 0.000090  training_loss: 1.5729 (1.5629)  mae_loss: 0.2549 (0.2641)  classification_loss: 1.3116 (1.2972)  loss_mask: 0.0004 (0.0015)  time: 0.1975  data: 0.0002  max mem: 5511
[09:00:46.379393] Epoch: [61]  [120/781]  eta: 0:02:14  lr: 0.000090  training_loss: 1.5465 (1.5586)  mae_loss: 0.2528 (0.2629)  classification_loss: 1.2614 (1.2935)  loss_mask: 0.0012 (0.0023)  time: 0.1976  data: 0.0002  max mem: 5511
[09:00:50.334241] Epoch: [61]  [140/781]  eta: 0:02:10  lr: 0.000090  training_loss: 1.5168 (1.5564)  mae_loss: 0.2598 (0.2631)  classification_loss: 1.2434 (1.2909)  loss_mask: 0.0012 (0.0024)  time: 0.1977  data: 0.0002  max mem: 5511
[09:00:54.338783] Epoch: [61]  [160/781]  eta: 0:02:05  lr: 0.000090  training_loss: 1.4422 (1.5467)  mae_loss: 0.2453 (0.2618)  classification_loss: 1.1797 (1.2827)  loss_mask: 0.0005 (0.0022)  time: 0.2001  data: 0.0002  max mem: 5511
[09:00:58.311335] Epoch: [61]  [180/781]  eta: 0:02:01  lr: 0.000090  training_loss: 1.5192 (1.5437)  mae_loss: 0.2598 (0.2623)  classification_loss: 1.2529 (1.2794)  loss_mask: 0.0003 (0.0020)  time: 0.1985  data: 0.0002  max mem: 5511
[09:01:02.271522] Epoch: [61]  [200/781]  eta: 0:01:57  lr: 0.000090  training_loss: 1.5164 (1.5431)  mae_loss: 0.2451 (0.2609)  classification_loss: 1.2686 (1.2798)  loss_mask: 0.0009 (0.0024)  time: 0.1979  data: 0.0002  max mem: 5511
[09:01:06.229698] Epoch: [61]  [220/781]  eta: 0:01:53  lr: 0.000090  training_loss: 1.4967 (1.5390)  mae_loss: 0.2435 (0.2601)  classification_loss: 1.2481 (1.2766)  loss_mask: 0.0010 (0.0023)  time: 0.1978  data: 0.0002  max mem: 5511
[09:01:10.209748] Epoch: [61]  [240/781]  eta: 0:01:48  lr: 0.000090  training_loss: 1.4501 (1.5358)  mae_loss: 0.2523 (0.2596)  classification_loss: 1.2047 (1.2740)  loss_mask: 0.0003 (0.0022)  time: 0.1989  data: 0.0002  max mem: 5511
[09:01:14.176555] Epoch: [61]  [260/781]  eta: 0:01:44  lr: 0.000090  training_loss: 1.5243 (1.5348)  mae_loss: 0.2570 (0.2594)  classification_loss: 1.2434 (1.2734)  loss_mask: 0.0003 (0.0021)  time: 0.1983  data: 0.0003  max mem: 5511
[09:01:18.174139] Epoch: [61]  [280/781]  eta: 0:01:40  lr: 0.000090  training_loss: 1.4848 (1.5334)  mae_loss: 0.2530 (0.2596)  classification_loss: 1.2243 (1.2719)  loss_mask: 0.0002 (0.0019)  time: 0.1998  data: 0.0002  max mem: 5511
[09:01:22.131369] Epoch: [61]  [300/781]  eta: 0:01:36  lr: 0.000089  training_loss: 1.5429 (1.5341)  mae_loss: 0.2615 (0.2595)  classification_loss: 1.2897 (1.2728)  loss_mask: 0.0002 (0.0018)  time: 0.1978  data: 0.0002  max mem: 5511
[09:01:26.100331] Epoch: [61]  [320/781]  eta: 0:01:32  lr: 0.000089  training_loss: 1.4978 (1.5338)  mae_loss: 0.2504 (0.2595)  classification_loss: 1.2740 (1.2726)  loss_mask: 0.0002 (0.0017)  time: 0.1984  data: 0.0002  max mem: 5511
[09:01:30.049648] Epoch: [61]  [340/781]  eta: 0:01:28  lr: 0.000089  training_loss: 1.5582 (1.5356)  mae_loss: 0.2710 (0.2604)  classification_loss: 1.2698 (1.2736)  loss_mask: 0.0002 (0.0016)  time: 0.1973  data: 0.0003  max mem: 5511
[09:01:34.013128] Epoch: [61]  [360/781]  eta: 0:01:24  lr: 0.000089  training_loss: 1.4959 (1.5328)  mae_loss: 0.2446 (0.2597)  classification_loss: 1.2162 (1.2716)  loss_mask: 0.0002 (0.0016)  time: 0.1981  data: 0.0003  max mem: 5511
[09:01:37.976571] Epoch: [61]  [380/781]  eta: 0:01:20  lr: 0.000089  training_loss: 1.4874 (1.5319)  mae_loss: 0.2411 (0.2593)  classification_loss: 1.2226 (1.2711)  loss_mask: 0.0002 (0.0015)  time: 0.1980  data: 0.0002  max mem: 5511
[09:01:41.909640] Epoch: [61]  [400/781]  eta: 0:01:16  lr: 0.000089  training_loss: 1.5643 (1.5334)  mae_loss: 0.2597 (0.2594)  classification_loss: 1.3094 (1.2725)  loss_mask: 0.0002 (0.0014)  time: 0.1966  data: 0.0003  max mem: 5511
[09:01:45.852114] Epoch: [61]  [420/781]  eta: 0:01:12  lr: 0.000089  training_loss: 1.5184 (1.5335)  mae_loss: 0.2524 (0.2589)  classification_loss: 1.2541 (1.2732)  loss_mask: 0.0002 (0.0014)  time: 0.1970  data: 0.0002  max mem: 5511
[09:01:49.809658] Epoch: [61]  [440/781]  eta: 0:01:08  lr: 0.000089  training_loss: 1.5372 (1.5337)  mae_loss: 0.2626 (0.2593)  classification_loss: 1.2713 (1.2731)  loss_mask: 0.0002 (0.0013)  time: 0.1977  data: 0.0003  max mem: 5511
[09:01:53.791134] Epoch: [61]  [460/781]  eta: 0:01:04  lr: 0.000089  training_loss: 1.4911 (1.5317)  mae_loss: 0.2545 (0.2595)  classification_loss: 1.2212 (1.2709)  loss_mask: 0.0002 (0.0013)  time: 0.1989  data: 0.0002  max mem: 5511
[09:01:57.734795] Epoch: [61]  [480/781]  eta: 0:01:00  lr: 0.000089  training_loss: 1.5040 (1.5315)  mae_loss: 0.2606 (0.2594)  classification_loss: 1.2501 (1.2709)  loss_mask: 0.0002 (0.0012)  time: 0.1971  data: 0.0002  max mem: 5511
[09:02:01.685013] Epoch: [61]  [500/781]  eta: 0:00:56  lr: 0.000088  training_loss: 1.5071 (1.5312)  mae_loss: 0.2391 (0.2593)  classification_loss: 1.2676 (1.2707)  loss_mask: 0.0001 (0.0012)  time: 0.1974  data: 0.0002  max mem: 5511
[09:02:05.631345] Epoch: [61]  [520/781]  eta: 0:00:52  lr: 0.000088  training_loss: 1.4974 (1.5300)  mae_loss: 0.2569 (0.2592)  classification_loss: 1.2441 (1.2697)  loss_mask: 0.0001 (0.0011)  time: 0.1972  data: 0.0002  max mem: 5511
[09:02:09.586493] Epoch: [61]  [540/781]  eta: 0:00:48  lr: 0.000088  training_loss: 1.5180 (1.5292)  mae_loss: 0.2522 (0.2592)  classification_loss: 1.2333 (1.2689)  loss_mask: 0.0001 (0.0011)  time: 0.1977  data: 0.0002  max mem: 5511
[09:02:13.535413] Epoch: [61]  [560/781]  eta: 0:00:44  lr: 0.000088  training_loss: 1.5090 (1.5290)  mae_loss: 0.2374 (0.2587)  classification_loss: 1.2534 (1.2692)  loss_mask: 0.0001 (0.0011)  time: 0.1974  data: 0.0003  max mem: 5511
[09:02:17.488714] Epoch: [61]  [580/781]  eta: 0:00:40  lr: 0.000088  training_loss: 1.4821 (1.5271)  mae_loss: 0.2640 (0.2587)  classification_loss: 1.2158 (1.2673)  loss_mask: 0.0002 (0.0010)  time: 0.1976  data: 0.0002  max mem: 5511
[09:02:21.488025] Epoch: [61]  [600/781]  eta: 0:00:36  lr: 0.000088  training_loss: 1.4650 (1.5253)  mae_loss: 0.2571 (0.2587)  classification_loss: 1.2117 (1.2656)  loss_mask: 0.0001 (0.0010)  time: 0.1999  data: 0.0002  max mem: 5511
[09:02:25.414672] Epoch: [61]  [620/781]  eta: 0:00:32  lr: 0.000088  training_loss: 1.5239 (1.5260)  mae_loss: 0.2413 (0.2582)  classification_loss: 1.2935 (1.2667)  loss_mask: 0.0001 (0.0010)  time: 0.1962  data: 0.0002  max mem: 5511
[09:02:29.362491] Epoch: [61]  [640/781]  eta: 0:00:28  lr: 0.000088  training_loss: 1.5489 (1.5263)  mae_loss: 0.2798 (0.2587)  classification_loss: 1.2779 (1.2667)  loss_mask: 0.0001 (0.0010)  time: 0.1973  data: 0.0002  max mem: 5511
[09:02:33.308730] Epoch: [61]  [660/781]  eta: 0:00:24  lr: 0.000088  training_loss: 1.5357 (1.5269)  mae_loss: 0.2505 (0.2586)  classification_loss: 1.2702 (1.2673)  loss_mask: 0.0001 (0.0009)  time: 0.1972  data: 0.0003  max mem: 5511
[09:02:37.240676] Epoch: [61]  [680/781]  eta: 0:00:20  lr: 0.000088  training_loss: 1.5024 (1.5263)  mae_loss: 0.2447 (0.2584)  classification_loss: 1.2653 (1.2670)  loss_mask: 0.0001 (0.0009)  time: 0.1965  data: 0.0002  max mem: 5511
[09:02:41.193739] Epoch: [61]  [700/781]  eta: 0:00:16  lr: 0.000087  training_loss: 1.4991 (1.5261)  mae_loss: 0.2339 (0.2577)  classification_loss: 1.2544 (1.2675)  loss_mask: 0.0002 (0.0009)  time: 0.1975  data: 0.0002  max mem: 5511
[09:02:45.146256] Epoch: [61]  [720/781]  eta: 0:00:12  lr: 0.000087  training_loss: 1.5685 (1.5279)  mae_loss: 0.2688 (0.2581)  classification_loss: 1.2989 (1.2690)  loss_mask: 0.0002 (0.0009)  time: 0.1975  data: 0.0003  max mem: 5511
[09:02:49.070096] Epoch: [61]  [740/781]  eta: 0:00:08  lr: 0.000087  training_loss: 1.4887 (1.5280)  mae_loss: 0.2498 (0.2580)  classification_loss: 1.2545 (1.2691)  loss_mask: 0.0001 (0.0009)  time: 0.1961  data: 0.0002  max mem: 5511
[09:02:53.003155] Epoch: [61]  [760/781]  eta: 0:00:04  lr: 0.000087  training_loss: 1.5697 (1.5284)  mae_loss: 0.2500 (0.2581)  classification_loss: 1.3104 (1.2695)  loss_mask: 0.0001 (0.0009)  time: 0.1965  data: 0.0002  max mem: 5511
[09:02:56.941120] Epoch: [61]  [780/781]  eta: 0:00:00  lr: 0.000087  training_loss: 1.5004 (1.5281)  mae_loss: 0.2568 (0.2581)  classification_loss: 1.2274 (1.2688)  loss_mask: 0.0012 (0.0011)  time: 0.1968  data: 0.0002  max mem: 5511
[09:02:57.095519] Epoch: [61] Total time: 0:02:35 (0.1990 s / it)
[09:02:57.096105] Averaged stats: lr: 0.000087  training_loss: 1.5004 (1.5281)  mae_loss: 0.2568 (0.2581)  classification_loss: 1.2274 (1.2688)  loss_mask: 0.0012 (0.0011)
[09:02:57.670660] Test:  [  0/157]  eta: 0:01:29  testing_loss: 0.5500 (0.5500)  acc1: 82.8125 (82.8125)  acc5: 98.4375 (98.4375)  time: 0.5705  data: 0.5408  max mem: 5511
[09:02:57.961424] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.5853 (0.5966)  acc1: 82.8125 (80.8239)  acc5: 100.0000 (99.4318)  time: 0.0781  data: 0.0494  max mem: 5511
[09:02:58.249277] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.5349 (0.5691)  acc1: 82.8125 (81.8452)  acc5: 100.0000 (99.5536)  time: 0.0288  data: 0.0004  max mem: 5511
[09:02:58.533277] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.5422 (0.5896)  acc1: 81.2500 (80.8468)  acc5: 100.0000 (99.3952)  time: 0.0285  data: 0.0003  max mem: 5511
[09:02:58.828558] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.5564 (0.5891)  acc1: 81.2500 (81.0595)  acc5: 98.4375 (99.2759)  time: 0.0288  data: 0.0002  max mem: 5511
[09:02:59.113494] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5564 (0.5795)  acc1: 82.8125 (81.4951)  acc5: 98.4375 (99.1728)  time: 0.0288  data: 0.0002  max mem: 5511
[09:02:59.401276] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5060 (0.5742)  acc1: 84.3750 (81.7111)  acc5: 98.4375 (99.1547)  time: 0.0284  data: 0.0002  max mem: 5511
[09:02:59.684881] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5700 (0.5761)  acc1: 82.8125 (81.5801)  acc5: 100.0000 (99.1417)  time: 0.0284  data: 0.0002  max mem: 5511
[09:02:59.968046] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6093 (0.5832)  acc1: 81.2500 (81.4236)  acc5: 100.0000 (99.0934)  time: 0.0282  data: 0.0002  max mem: 5511
[09:03:00.252662] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6049 (0.5804)  acc1: 82.8125 (81.6449)  acc5: 98.4375 (99.0728)  time: 0.0282  data: 0.0002  max mem: 5511
[09:03:00.536100] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5829 (0.5834)  acc1: 82.8125 (81.6213)  acc5: 98.4375 (99.0563)  time: 0.0282  data: 0.0002  max mem: 5511
[09:03:00.820636] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6129 (0.5854)  acc1: 81.2500 (81.6582)  acc5: 98.4375 (99.0006)  time: 0.0282  data: 0.0002  max mem: 5511
[09:03:01.103818] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5504 (0.5829)  acc1: 82.8125 (81.7794)  acc5: 100.0000 (99.0315)  time: 0.0282  data: 0.0002  max mem: 5511
[09:03:01.390134] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5674 (0.5842)  acc1: 79.6875 (81.6436)  acc5: 100.0000 (99.0697)  time: 0.0283  data: 0.0002  max mem: 5511
[09:03:01.671962] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5804 (0.5822)  acc1: 79.6875 (81.7819)  acc5: 100.0000 (99.1135)  time: 0.0282  data: 0.0003  max mem: 5511
[09:03:01.953366] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5763 (0.5804)  acc1: 84.3750 (81.9226)  acc5: 100.0000 (99.1101)  time: 0.0280  data: 0.0002  max mem: 5511
[09:03:02.104738] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5766 (0.5814)  acc1: 81.2500 (81.8700)  acc5: 100.0000 (99.1100)  time: 0.0271  data: 0.0001  max mem: 5511
[09:03:02.261477] Test: Total time: 0:00:05 (0.0329 s / it)
[09:03:02.261958] * Acc@1 81.870 Acc@5 99.110 loss 0.581
[09:03:02.262252] Accuracy of the network on the 10000 test images: 81.9%
[09:03:02.262437] Max accuracy: 82.58%
[09:03:02.512266] log_dir: ./output_dir
[09:03:03.531984] Epoch: [62]  [  0/781]  eta: 0:13:14  lr: 0.000087  training_loss: 1.5031 (1.5031)  mae_loss: 0.2237 (0.2237)  classification_loss: 1.2739 (1.2739)  loss_mask: 0.0055 (0.0055)  time: 1.0177  data: 0.7836  max mem: 5511
[09:03:07.473929] Epoch: [62]  [ 20/781]  eta: 0:02:59  lr: 0.000087  training_loss: 1.5063 (1.5666)  mae_loss: 0.2527 (0.2573)  classification_loss: 1.2426 (1.2456)  loss_mask: 0.0029 (0.0637)  time: 0.1970  data: 0.0002  max mem: 5511
[09:03:11.428190] Epoch: [62]  [ 40/781]  eta: 0:02:41  lr: 0.000087  training_loss: 1.6579 (1.6334)  mae_loss: 0.2615 (0.2598)  classification_loss: 1.2528 (1.2544)  loss_mask: 0.1154 (0.1192)  time: 0.1976  data: 0.0002  max mem: 5511
[09:03:15.420468] Epoch: [62]  [ 60/781]  eta: 0:02:32  lr: 0.000087  training_loss: 1.6198 (1.6237)  mae_loss: 0.2574 (0.2600)  classification_loss: 1.2579 (1.2620)  loss_mask: 0.0535 (0.1017)  time: 0.1995  data: 0.0003  max mem: 5511
[09:03:19.382973] Epoch: [62]  [ 80/781]  eta: 0:02:25  lr: 0.000087  training_loss: 1.5473 (1.6055)  mae_loss: 0.2679 (0.2616)  classification_loss: 1.2488 (1.2628)  loss_mask: 0.0126 (0.0812)  time: 0.1980  data: 0.0004  max mem: 5511
[09:03:23.364969] Epoch: [62]  [100/781]  eta: 0:02:20  lr: 0.000087  training_loss: 1.5102 (1.5907)  mae_loss: 0.2673 (0.2636)  classification_loss: 1.2426 (1.2608)  loss_mask: 0.0050 (0.0663)  time: 0.1990  data: 0.0003  max mem: 5511
[09:03:27.305670] Epoch: [62]  [120/781]  eta: 0:02:15  lr: 0.000086  training_loss: 1.5567 (1.5853)  mae_loss: 0.2481 (0.2618)  classification_loss: 1.3005 (1.2678)  loss_mask: 0.0019 (0.0557)  time: 0.1969  data: 0.0003  max mem: 5511
[09:03:31.250767] Epoch: [62]  [140/781]  eta: 0:02:10  lr: 0.000086  training_loss: 1.5020 (1.5770)  mae_loss: 0.2732 (0.2642)  classification_loss: 1.2141 (1.2649)  loss_mask: 0.0009 (0.0479)  time: 0.1972  data: 0.0003  max mem: 5511
[09:03:35.211668] Epoch: [62]  [160/781]  eta: 0:02:06  lr: 0.000086  training_loss: 1.5630 (1.5709)  mae_loss: 0.2517 (0.2627)  classification_loss: 1.2928 (1.2661)  loss_mask: 0.0007 (0.0421)  time: 0.1980  data: 0.0003  max mem: 5511
[09:03:39.157238] Epoch: [62]  [180/781]  eta: 0:02:01  lr: 0.000086  training_loss: 1.5088 (1.5670)  mae_loss: 0.2576 (0.2632)  classification_loss: 1.2505 (1.2663)  loss_mask: 0.0006 (0.0375)  time: 0.1972  data: 0.0002  max mem: 5511
[09:03:43.092076] Epoch: [62]  [200/781]  eta: 0:01:57  lr: 0.000086  training_loss: 1.5614 (1.5674)  mae_loss: 0.2700 (0.2637)  classification_loss: 1.3122 (1.2699)  loss_mask: 0.0007 (0.0339)  time: 0.1966  data: 0.0002  max mem: 5511
[09:03:47.035049] Epoch: [62]  [220/781]  eta: 0:01:52  lr: 0.000086  training_loss: 1.5494 (1.5654)  mae_loss: 0.2595 (0.2626)  classification_loss: 1.2952 (1.2719)  loss_mask: 0.0005 (0.0309)  time: 0.1971  data: 0.0002  max mem: 5511
[09:03:50.989245] Epoch: [62]  [240/781]  eta: 0:01:48  lr: 0.000086  training_loss: 1.5461 (1.5639)  mae_loss: 0.2533 (0.2627)  classification_loss: 1.2709 (1.2728)  loss_mask: 0.0004 (0.0284)  time: 0.1976  data: 0.0002  max mem: 5511
[09:03:54.942442] Epoch: [62]  [260/781]  eta: 0:01:44  lr: 0.000086  training_loss: 1.6005 (1.5648)  mae_loss: 0.2561 (0.2627)  classification_loss: 1.3120 (1.2759)  loss_mask: 0.0004 (0.0262)  time: 0.1976  data: 0.0002  max mem: 5511
[09:03:58.905570] Epoch: [62]  [280/781]  eta: 0:01:40  lr: 0.000086  training_loss: 1.5461 (1.5638)  mae_loss: 0.2671 (0.2634)  classification_loss: 1.2788 (1.2760)  loss_mask: 0.0003 (0.0244)  time: 0.1981  data: 0.0003  max mem: 5511
[09:04:02.862372] Epoch: [62]  [300/781]  eta: 0:01:36  lr: 0.000086  training_loss: 1.5786 (1.5638)  mae_loss: 0.2441 (0.2623)  classification_loss: 1.3319 (1.2786)  loss_mask: 0.0003 (0.0228)  time: 0.1977  data: 0.0005  max mem: 5511
[09:04:06.850160] Epoch: [62]  [320/781]  eta: 0:01:32  lr: 0.000085  training_loss: 1.5154 (1.5601)  mae_loss: 0.2456 (0.2614)  classification_loss: 1.2725 (1.2772)  loss_mask: 0.0003 (0.0214)  time: 0.1993  data: 0.0002  max mem: 5511
[09:04:10.843961] Epoch: [62]  [340/781]  eta: 0:01:28  lr: 0.000085  training_loss: 1.5375 (1.5573)  mae_loss: 0.2590 (0.2618)  classification_loss: 1.2262 (1.2750)  loss_mask: 0.0004 (0.0205)  time: 0.1996  data: 0.0002  max mem: 5511
[09:04:14.802091] Epoch: [62]  [360/781]  eta: 0:01:24  lr: 0.000085  training_loss: 1.4881 (1.5552)  mae_loss: 0.2596 (0.2614)  classification_loss: 1.2242 (1.2744)  loss_mask: 0.0008 (0.0195)  time: 0.1978  data: 0.0002  max mem: 5511
[09:04:18.762653] Epoch: [62]  [380/781]  eta: 0:01:20  lr: 0.000085  training_loss: 1.5353 (1.5558)  mae_loss: 0.2565 (0.2612)  classification_loss: 1.3005 (1.2760)  loss_mask: 0.0006 (0.0186)  time: 0.1980  data: 0.0002  max mem: 5511
[09:04:22.701587] Epoch: [62]  [400/781]  eta: 0:01:16  lr: 0.000085  training_loss: 1.5390 (1.5550)  mae_loss: 0.2529 (0.2610)  classification_loss: 1.2760 (1.2763)  loss_mask: 0.0004 (0.0177)  time: 0.1969  data: 0.0003  max mem: 5511
[09:04:26.664287] Epoch: [62]  [420/781]  eta: 0:01:12  lr: 0.000085  training_loss: 1.5018 (1.5530)  mae_loss: 0.2576 (0.2614)  classification_loss: 1.2568 (1.2747)  loss_mask: 0.0002 (0.0168)  time: 0.1980  data: 0.0004  max mem: 5511
[09:04:30.618547] Epoch: [62]  [440/781]  eta: 0:01:08  lr: 0.000085  training_loss: 1.4968 (1.5508)  mae_loss: 0.2476 (0.2610)  classification_loss: 1.2193 (1.2738)  loss_mask: 0.0003 (0.0161)  time: 0.1976  data: 0.0002  max mem: 5511
[09:04:34.572573] Epoch: [62]  [460/781]  eta: 0:01:04  lr: 0.000085  training_loss: 1.4651 (1.5471)  mae_loss: 0.2468 (0.2609)  classification_loss: 1.2260 (1.2708)  loss_mask: 0.0003 (0.0154)  time: 0.1976  data: 0.0003  max mem: 5511
[09:04:38.563711] Epoch: [62]  [480/781]  eta: 0:01:00  lr: 0.000085  training_loss: 1.5191 (1.5468)  mae_loss: 0.2585 (0.2608)  classification_loss: 1.2710 (1.2711)  loss_mask: 0.0005 (0.0148)  time: 0.1994  data: 0.0002  max mem: 5511
[09:04:42.533656] Epoch: [62]  [500/781]  eta: 0:00:56  lr: 0.000085  training_loss: 1.4724 (1.5452)  mae_loss: 0.2570 (0.2606)  classification_loss: 1.2555 (1.2703)  loss_mask: 0.0003 (0.0143)  time: 0.1984  data: 0.0002  max mem: 5511
[09:04:46.479183] Epoch: [62]  [520/781]  eta: 0:00:52  lr: 0.000084  training_loss: 1.4829 (1.5435)  mae_loss: 0.2530 (0.2605)  classification_loss: 1.2313 (1.2693)  loss_mask: 0.0002 (0.0137)  time: 0.1972  data: 0.0002  max mem: 5511
[09:04:50.480218] Epoch: [62]  [540/781]  eta: 0:00:48  lr: 0.000084  training_loss: 1.5257 (1.5430)  mae_loss: 0.2453 (0.2604)  classification_loss: 1.2816 (1.2694)  loss_mask: 0.0002 (0.0132)  time: 0.2000  data: 0.0002  max mem: 5511
[09:04:54.451767] Epoch: [62]  [560/781]  eta: 0:00:44  lr: 0.000084  training_loss: 1.5041 (1.5427)  mae_loss: 0.2565 (0.2603)  classification_loss: 1.2579 (1.2696)  loss_mask: 0.0002 (0.0128)  time: 0.1985  data: 0.0002  max mem: 5511
[09:04:58.429887] Epoch: [62]  [580/781]  eta: 0:00:40  lr: 0.000084  training_loss: 1.5067 (1.5425)  mae_loss: 0.2485 (0.2601)  classification_loss: 1.2571 (1.2701)  loss_mask: 0.0002 (0.0123)  time: 0.1987  data: 0.0002  max mem: 5511
[09:05:02.383854] Epoch: [62]  [600/781]  eta: 0:00:36  lr: 0.000084  training_loss: 1.5047 (1.5409)  mae_loss: 0.2532 (0.2599)  classification_loss: 1.2273 (1.2690)  loss_mask: 0.0002 (0.0119)  time: 0.1976  data: 0.0002  max mem: 5511
[09:05:06.339506] Epoch: [62]  [620/781]  eta: 0:00:32  lr: 0.000084  training_loss: 1.5431 (1.5407)  mae_loss: 0.2596 (0.2603)  classification_loss: 1.2601 (1.2688)  loss_mask: 0.0002 (0.0116)  time: 0.1977  data: 0.0003  max mem: 5511
[09:05:10.275292] Epoch: [62]  [640/781]  eta: 0:00:28  lr: 0.000084  training_loss: 1.5322 (1.5406)  mae_loss: 0.2598 (0.2603)  classification_loss: 1.2563 (1.2691)  loss_mask: 0.0002 (0.0112)  time: 0.1967  data: 0.0002  max mem: 5511
[09:05:14.214689] Epoch: [62]  [660/781]  eta: 0:00:24  lr: 0.000084  training_loss: 1.5326 (1.5408)  mae_loss: 0.2614 (0.2608)  classification_loss: 1.2733 (1.2692)  loss_mask: 0.0002 (0.0109)  time: 0.1969  data: 0.0002  max mem: 5511
[09:05:18.193687] Epoch: [62]  [680/781]  eta: 0:00:20  lr: 0.000084  training_loss: 1.4905 (1.5396)  mae_loss: 0.2528 (0.2604)  classification_loss: 1.2376 (1.2686)  loss_mask: 0.0001 (0.0106)  time: 0.1989  data: 0.0002  max mem: 5511
[09:05:22.205226] Epoch: [62]  [700/781]  eta: 0:00:16  lr: 0.000084  training_loss: 1.5392 (1.5399)  mae_loss: 0.2647 (0.2606)  classification_loss: 1.2811 (1.2690)  loss_mask: 0.0001 (0.0103)  time: 0.2005  data: 0.0003  max mem: 5511
[09:05:26.160549] Epoch: [62]  [720/781]  eta: 0:00:12  lr: 0.000083  training_loss: 1.5503 (1.5397)  mae_loss: 0.2515 (0.2604)  classification_loss: 1.2830 (1.2694)  loss_mask: 0.0002 (0.0100)  time: 0.1977  data: 0.0002  max mem: 5511
[09:05:30.097343] Epoch: [62]  [740/781]  eta: 0:00:08  lr: 0.000083  training_loss: 1.5148 (1.5395)  mae_loss: 0.2527 (0.2605)  classification_loss: 1.2371 (1.2693)  loss_mask: 0.0002 (0.0097)  time: 0.1967  data: 0.0003  max mem: 5511
[09:05:34.078057] Epoch: [62]  [760/781]  eta: 0:00:04  lr: 0.000083  training_loss: 1.5554 (1.5393)  mae_loss: 0.2474 (0.2603)  classification_loss: 1.2799 (1.2696)  loss_mask: 0.0001 (0.0095)  time: 0.1989  data: 0.0003  max mem: 5511
[09:05:38.121839] Epoch: [62]  [780/781]  eta: 0:00:00  lr: 0.000083  training_loss: 1.5433 (1.5394)  mae_loss: 0.2727 (0.2607)  classification_loss: 1.2667 (1.2695)  loss_mask: 0.0001 (0.0092)  time: 0.2021  data: 0.0002  max mem: 5511
[09:05:38.289032] Epoch: [62] Total time: 0:02:35 (0.1995 s / it)
[09:05:38.289487] Averaged stats: lr: 0.000083  training_loss: 1.5433 (1.5394)  mae_loss: 0.2727 (0.2607)  classification_loss: 1.2667 (1.2695)  loss_mask: 0.0001 (0.0092)
[09:05:38.906461] Test:  [  0/157]  eta: 0:01:36  testing_loss: 0.4823 (0.4823)  acc1: 90.6250 (90.6250)  acc5: 98.4375 (98.4375)  time: 0.6126  data: 0.5787  max mem: 5511
[09:05:39.190929] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.5556 (0.5567)  acc1: 84.3750 (83.0966)  acc5: 100.0000 (99.5739)  time: 0.0814  data: 0.0527  max mem: 5511
[09:05:39.474765] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.5312 (0.5358)  acc1: 82.8125 (83.4077)  acc5: 100.0000 (99.6280)  time: 0.0282  data: 0.0002  max mem: 5511
[09:05:39.758118] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.5312 (0.5473)  acc1: 82.8125 (82.8125)  acc5: 100.0000 (99.3952)  time: 0.0282  data: 0.0002  max mem: 5511
[09:05:40.046407] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.5456 (0.5532)  acc1: 81.2500 (82.6982)  acc5: 100.0000 (99.3902)  time: 0.0285  data: 0.0002  max mem: 5511
[09:05:40.337499] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5225 (0.5440)  acc1: 85.9375 (83.4559)  acc5: 100.0000 (99.3873)  time: 0.0288  data: 0.0002  max mem: 5511
[09:05:40.625613] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4926 (0.5387)  acc1: 85.9375 (83.6322)  acc5: 100.0000 (99.3596)  time: 0.0288  data: 0.0002  max mem: 5511
[09:05:40.911890] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5112 (0.5376)  acc1: 84.3750 (83.7368)  acc5: 98.4375 (99.2958)  time: 0.0285  data: 0.0002  max mem: 5511
[09:05:41.197309] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5557 (0.5478)  acc1: 82.8125 (83.4684)  acc5: 98.4375 (99.2284)  time: 0.0284  data: 0.0002  max mem: 5511
[09:05:41.484353] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5540 (0.5462)  acc1: 82.8125 (83.6538)  acc5: 98.4375 (99.1930)  time: 0.0285  data: 0.0002  max mem: 5511
[09:05:41.771793] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5376 (0.5484)  acc1: 84.3750 (83.6788)  acc5: 98.4375 (99.1491)  time: 0.0286  data: 0.0002  max mem: 5511
[09:05:42.065854] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5804 (0.5503)  acc1: 82.8125 (83.4319)  acc5: 100.0000 (99.1836)  time: 0.0289  data: 0.0002  max mem: 5511
[09:05:42.359625] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5778 (0.5485)  acc1: 81.2500 (83.3807)  acc5: 100.0000 (99.1865)  time: 0.0292  data: 0.0002  max mem: 5511
[09:05:42.651293] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5401 (0.5488)  acc1: 81.2500 (83.2896)  acc5: 100.0000 (99.2247)  time: 0.0291  data: 0.0002  max mem: 5511
[09:05:42.943993] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5495 (0.5479)  acc1: 82.8125 (83.3666)  acc5: 100.0000 (99.2465)  time: 0.0291  data: 0.0002  max mem: 5511
[09:05:43.225996] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5307 (0.5459)  acc1: 84.3750 (83.4541)  acc5: 100.0000 (99.2343)  time: 0.0286  data: 0.0001  max mem: 5511
[09:05:43.376128] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5221 (0.5463)  acc1: 84.3750 (83.4700)  acc5: 100.0000 (99.2500)  time: 0.0271  data: 0.0001  max mem: 5511
[09:05:43.542329] Test: Total time: 0:00:05 (0.0334 s / it)
[09:05:43.542845] * Acc@1 83.470 Acc@5 99.250 loss 0.546
[09:05:43.543312] Accuracy of the network on the 10000 test images: 83.5%
[09:05:43.543549] Max accuracy: 83.47%
[09:05:43.773309] log_dir: ./output_dir
[09:05:44.740241] Epoch: [63]  [  0/781]  eta: 0:12:33  lr: 0.000083  training_loss: 1.3758 (1.3758)  mae_loss: 0.2206 (0.2206)  classification_loss: 1.1551 (1.1551)  loss_mask: 0.0001 (0.0001)  time: 0.9652  data: 0.7322  max mem: 5511
[09:05:48.699886] Epoch: [63]  [ 20/781]  eta: 0:02:58  lr: 0.000083  training_loss: 1.4431 (1.4609)  mae_loss: 0.2463 (0.2449)  classification_loss: 1.2119 (1.2158)  loss_mask: 0.0002 (0.0002)  time: 0.1979  data: 0.0002  max mem: 5511
[09:05:52.637485] Epoch: [63]  [ 40/781]  eta: 0:02:40  lr: 0.000083  training_loss: 1.5429 (1.4936)  mae_loss: 0.2524 (0.2488)  classification_loss: 1.2868 (1.2445)  loss_mask: 0.0001 (0.0002)  time: 0.1968  data: 0.0002  max mem: 5511
[09:05:56.601294] Epoch: [63]  [ 60/781]  eta: 0:02:31  lr: 0.000083  training_loss: 1.5297 (1.5062)  mae_loss: 0.2595 (0.2527)  classification_loss: 1.2560 (1.2533)  loss_mask: 0.0002 (0.0002)  time: 0.1981  data: 0.0002  max mem: 5511
[09:06:00.533971] Epoch: [63]  [ 80/781]  eta: 0:02:24  lr: 0.000083  training_loss: 1.4946 (1.5116)  mae_loss: 0.2590 (0.2541)  classification_loss: 1.2628 (1.2572)  loss_mask: 0.0002 (0.0002)  time: 0.1965  data: 0.0003  max mem: 5511
[09:06:04.467387] Epoch: [63]  [100/781]  eta: 0:02:19  lr: 0.000083  training_loss: 1.5813 (1.5226)  mae_loss: 0.2477 (0.2542)  classification_loss: 1.2864 (1.2675)  loss_mask: 0.0005 (0.0009)  time: 0.1966  data: 0.0003  max mem: 5511
[09:06:08.399487] Epoch: [63]  [120/781]  eta: 0:02:14  lr: 0.000083  training_loss: 1.5802 (1.5297)  mae_loss: 0.2532 (0.2542)  classification_loss: 1.2771 (1.2646)  loss_mask: 0.0217 (0.0109)  time: 0.1965  data: 0.0002  max mem: 5511
[09:06:12.363306] Epoch: [63]  [140/781]  eta: 0:02:09  lr: 0.000082  training_loss: 1.6101 (1.5428)  mae_loss: 0.2554 (0.2547)  classification_loss: 1.2160 (1.2621)  loss_mask: 0.0940 (0.0259)  time: 0.1981  data: 0.0002  max mem: 5511
[09:06:16.304492] Epoch: [63]  [160/781]  eta: 0:02:05  lr: 0.000082  training_loss: 1.4800 (1.5401)  mae_loss: 0.2458 (0.2548)  classification_loss: 1.2024 (1.2586)  loss_mask: 0.0279 (0.0268)  time: 0.1970  data: 0.0002  max mem: 5511
[09:06:20.227593] Epoch: [63]  [180/781]  eta: 0:02:00  lr: 0.000082  training_loss: 1.5185 (1.5396)  mae_loss: 0.2372 (0.2540)  classification_loss: 1.2547 (1.2605)  loss_mask: 0.0110 (0.0251)  time: 0.1961  data: 0.0003  max mem: 5511
[09:06:24.153027] Epoch: [63]  [200/781]  eta: 0:01:56  lr: 0.000082  training_loss: 1.5199 (1.5366)  mae_loss: 0.2442 (0.2536)  classification_loss: 1.2579 (1.2597)  loss_mask: 0.0026 (0.0232)  time: 0.1961  data: 0.0003  max mem: 5511
[09:06:28.130217] Epoch: [63]  [220/781]  eta: 0:01:52  lr: 0.000082  training_loss: 1.5140 (1.5339)  mae_loss: 0.2406 (0.2531)  classification_loss: 1.2732 (1.2594)  loss_mask: 0.0019 (0.0214)  time: 0.1988  data: 0.0004  max mem: 5511
[09:06:32.083205] Epoch: [63]  [240/781]  eta: 0:01:48  lr: 0.000082  training_loss: 1.4888 (1.5337)  mae_loss: 0.2513 (0.2543)  classification_loss: 1.2484 (1.2597)  loss_mask: 0.0012 (0.0198)  time: 0.1976  data: 0.0003  max mem: 5511
[09:06:36.038471] Epoch: [63]  [260/781]  eta: 0:01:44  lr: 0.000082  training_loss: 1.5397 (1.5327)  mae_loss: 0.2514 (0.2545)  classification_loss: 1.2720 (1.2599)  loss_mask: 0.0006 (0.0183)  time: 0.1977  data: 0.0003  max mem: 5511
[09:06:39.978461] Epoch: [63]  [280/781]  eta: 0:01:40  lr: 0.000082  training_loss: 1.5373 (1.5340)  mae_loss: 0.2509 (0.2547)  classification_loss: 1.2928 (1.2622)  loss_mask: 0.0006 (0.0170)  time: 0.1969  data: 0.0003  max mem: 5511
[09:06:43.931059] Epoch: [63]  [300/781]  eta: 0:01:36  lr: 0.000082  training_loss: 1.5300 (1.5340)  mae_loss: 0.2624 (0.2549)  classification_loss: 1.2507 (1.2631)  loss_mask: 0.0005 (0.0160)  time: 0.1975  data: 0.0002  max mem: 5511
[09:06:47.890202] Epoch: [63]  [320/781]  eta: 0:01:32  lr: 0.000082  training_loss: 1.5004 (1.5312)  mae_loss: 0.2427 (0.2546)  classification_loss: 1.2456 (1.2614)  loss_mask: 0.0009 (0.0151)  time: 0.1979  data: 0.0002  max mem: 5511
[09:06:51.830283] Epoch: [63]  [340/781]  eta: 0:01:27  lr: 0.000081  training_loss: 1.5191 (1.5295)  mae_loss: 0.2553 (0.2550)  classification_loss: 1.2356 (1.2597)  loss_mask: 0.0024 (0.0148)  time: 0.1969  data: 0.0002  max mem: 5511
[09:06:55.790428] Epoch: [63]  [360/781]  eta: 0:01:23  lr: 0.000081  training_loss: 1.5693 (1.5292)  mae_loss: 0.2575 (0.2552)  classification_loss: 1.2536 (1.2589)  loss_mask: 0.0048 (0.0151)  time: 0.1979  data: 0.0002  max mem: 5511
[09:06:59.721902] Epoch: [63]  [380/781]  eta: 0:01:19  lr: 0.000081  training_loss: 1.4891 (1.5295)  mae_loss: 0.2700 (0.2559)  classification_loss: 1.2550 (1.2584)  loss_mask: 0.0074 (0.0153)  time: 0.1965  data: 0.0002  max mem: 5511
[09:07:03.677569] Epoch: [63]  [400/781]  eta: 0:01:15  lr: 0.000081  training_loss: 1.5069 (1.5294)  mae_loss: 0.2400 (0.2557)  classification_loss: 1.2401 (1.2586)  loss_mask: 0.0059 (0.0151)  time: 0.1977  data: 0.0002  max mem: 5511
[09:07:07.635730] Epoch: [63]  [420/781]  eta: 0:01:11  lr: 0.000081  training_loss: 1.5112 (1.5289)  mae_loss: 0.2518 (0.2558)  classification_loss: 1.2624 (1.2585)  loss_mask: 0.0024 (0.0146)  time: 0.1978  data: 0.0002  max mem: 5511
[09:07:11.614298] Epoch: [63]  [440/781]  eta: 0:01:07  lr: 0.000081  training_loss: 1.4981 (1.5289)  mae_loss: 0.2536 (0.2558)  classification_loss: 1.2645 (1.2592)  loss_mask: 0.0005 (0.0140)  time: 0.1988  data: 0.0002  max mem: 5511
[09:07:15.646744] Epoch: [63]  [460/781]  eta: 0:01:03  lr: 0.000081  training_loss: 1.5235 (1.5291)  mae_loss: 0.2455 (0.2557)  classification_loss: 1.2777 (1.2600)  loss_mask: 0.0002 (0.0134)  time: 0.2015  data: 0.0002  max mem: 5511
[09:07:19.657962] Epoch: [63]  [480/781]  eta: 0:00:59  lr: 0.000081  training_loss: 1.5493 (1.5298)  mae_loss: 0.2424 (0.2556)  classification_loss: 1.2991 (1.2613)  loss_mask: 0.0002 (0.0129)  time: 0.2005  data: 0.0002  max mem: 5511
[09:07:23.606243] Epoch: [63]  [500/781]  eta: 0:00:55  lr: 0.000081  training_loss: 1.4859 (1.5298)  mae_loss: 0.2428 (0.2554)  classification_loss: 1.2380 (1.2620)  loss_mask: 0.0002 (0.0124)  time: 0.1973  data: 0.0003  max mem: 5511
[09:07:27.528712] Epoch: [63]  [520/781]  eta: 0:00:51  lr: 0.000081  training_loss: 1.4950 (1.5290)  mae_loss: 0.2491 (0.2553)  classification_loss: 1.2455 (1.2618)  loss_mask: 0.0002 (0.0119)  time: 0.1960  data: 0.0003  max mem: 5511
[09:07:31.492837] Epoch: [63]  [540/781]  eta: 0:00:47  lr: 0.000080  training_loss: 1.4996 (1.5304)  mae_loss: 0.2646 (0.2558)  classification_loss: 1.2425 (1.2632)  loss_mask: 0.0002 (0.0115)  time: 0.1981  data: 0.0003  max mem: 5511
[09:07:35.401787] Epoch: [63]  [560/781]  eta: 0:00:43  lr: 0.000080  training_loss: 1.5339 (1.5307)  mae_loss: 0.2514 (0.2557)  classification_loss: 1.2584 (1.2638)  loss_mask: 0.0002 (0.0112)  time: 0.1954  data: 0.0002  max mem: 5511
[09:07:39.325878] Epoch: [63]  [580/781]  eta: 0:00:39  lr: 0.000080  training_loss: 1.5505 (1.5321)  mae_loss: 0.2706 (0.2565)  classification_loss: 1.2603 (1.2646)  loss_mask: 0.0004 (0.0109)  time: 0.1961  data: 0.0002  max mem: 5511
[09:07:43.266104] Epoch: [63]  [600/781]  eta: 0:00:35  lr: 0.000080  training_loss: 1.5061 (1.5318)  mae_loss: 0.2641 (0.2569)  classification_loss: 1.2545 (1.2643)  loss_mask: 0.0008 (0.0106)  time: 0.1969  data: 0.0002  max mem: 5511
[09:07:47.207238] Epoch: [63]  [620/781]  eta: 0:00:31  lr: 0.000080  training_loss: 1.5162 (1.5313)  mae_loss: 0.2528 (0.2569)  classification_loss: 1.2423 (1.2641)  loss_mask: 0.0002 (0.0103)  time: 0.1970  data: 0.0002  max mem: 5511
[09:07:51.145507] Epoch: [63]  [640/781]  eta: 0:00:28  lr: 0.000080  training_loss: 1.5205 (1.5311)  mae_loss: 0.2558 (0.2571)  classification_loss: 1.2603 (1.2641)  loss_mask: 0.0002 (0.0099)  time: 0.1968  data: 0.0002  max mem: 5511
[09:07:55.078626] Epoch: [63]  [660/781]  eta: 0:00:24  lr: 0.000080  training_loss: 1.4841 (1.5301)  mae_loss: 0.2591 (0.2572)  classification_loss: 1.2301 (1.2633)  loss_mask: 0.0002 (0.0097)  time: 0.1966  data: 0.0002  max mem: 5511
[09:07:59.016386] Epoch: [63]  [680/781]  eta: 0:00:20  lr: 0.000080  training_loss: 1.4818 (1.5300)  mae_loss: 0.2512 (0.2569)  classification_loss: 1.2334 (1.2638)  loss_mask: 0.0001 (0.0094)  time: 0.1968  data: 0.0002  max mem: 5511
[09:08:02.967715] Epoch: [63]  [700/781]  eta: 0:00:16  lr: 0.000080  training_loss: 1.5052 (1.5300)  mae_loss: 0.2532 (0.2570)  classification_loss: 1.2566 (1.2639)  loss_mask: 0.0001 (0.0091)  time: 0.1974  data: 0.0002  max mem: 5511
[09:08:06.942881] Epoch: [63]  [720/781]  eta: 0:00:12  lr: 0.000080  training_loss: 1.5649 (1.5305)  mae_loss: 0.2625 (0.2571)  classification_loss: 1.2960 (1.2646)  loss_mask: 0.0001 (0.0089)  time: 0.1987  data: 0.0003  max mem: 5511
[09:08:10.891766] Epoch: [63]  [740/781]  eta: 0:00:08  lr: 0.000079  training_loss: 1.4519 (1.5292)  mae_loss: 0.2586 (0.2572)  classification_loss: 1.2114 (1.2633)  loss_mask: 0.0001 (0.0086)  time: 0.1974  data: 0.0002  max mem: 5511
[09:08:14.842960] Epoch: [63]  [760/781]  eta: 0:00:04  lr: 0.000079  training_loss: 1.5129 (1.5292)  mae_loss: 0.2455 (0.2572)  classification_loss: 1.2593 (1.2635)  loss_mask: 0.0002 (0.0085)  time: 0.1975  data: 0.0002  max mem: 5511
[09:08:18.772445] Epoch: [63]  [780/781]  eta: 0:00:00  lr: 0.000079  training_loss: 1.4683 (1.5277)  mae_loss: 0.2515 (0.2571)  classification_loss: 1.2069 (1.2622)  loss_mask: 0.0003 (0.0083)  time: 0.1964  data: 0.0002  max mem: 5511
[09:08:18.936448] Epoch: [63] Total time: 0:02:35 (0.1987 s / it)
[09:08:18.937690] Averaged stats: lr: 0.000079  training_loss: 1.4683 (1.5277)  mae_loss: 0.2515 (0.2571)  classification_loss: 1.2069 (1.2622)  loss_mask: 0.0003 (0.0083)
[09:08:19.527381] Test:  [  0/157]  eta: 0:01:31  testing_loss: 0.4814 (0.4814)  acc1: 89.0625 (89.0625)  acc5: 98.4375 (98.4375)  time: 0.5850  data: 0.5539  max mem: 5511
[09:08:19.811882] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.5683 (0.5781)  acc1: 81.2500 (79.6875)  acc5: 100.0000 (99.5739)  time: 0.0789  data: 0.0505  max mem: 5511
[09:08:20.095777] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.5293 (0.5456)  acc1: 82.8125 (81.7708)  acc5: 100.0000 (99.4792)  time: 0.0283  data: 0.0002  max mem: 5511
[09:08:20.380657] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.5230 (0.5584)  acc1: 82.8125 (81.3004)  acc5: 100.0000 (99.2440)  time: 0.0283  data: 0.0002  max mem: 5511
[09:08:20.665678] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.5448 (0.5607)  acc1: 82.8125 (81.7835)  acc5: 98.4375 (99.1616)  time: 0.0284  data: 0.0002  max mem: 5511
[09:08:20.948631] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5276 (0.5499)  acc1: 84.3750 (82.3836)  acc5: 100.0000 (99.2034)  time: 0.0283  data: 0.0002  max mem: 5511
[09:08:21.232709] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5011 (0.5438)  acc1: 84.3750 (82.7100)  acc5: 100.0000 (99.2059)  time: 0.0282  data: 0.0003  max mem: 5511
[09:08:21.516238] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5336 (0.5447)  acc1: 84.3750 (82.8125)  acc5: 100.0000 (99.1637)  time: 0.0283  data: 0.0002  max mem: 5511
[09:08:21.802458] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5791 (0.5545)  acc1: 82.8125 (82.5231)  acc5: 98.4375 (99.1127)  time: 0.0284  data: 0.0002  max mem: 5511
[09:08:22.088734] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5829 (0.5526)  acc1: 81.2500 (82.6408)  acc5: 98.4375 (99.0900)  time: 0.0285  data: 0.0002  max mem: 5511
[09:08:22.373811] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5648 (0.5575)  acc1: 84.3750 (82.6423)  acc5: 100.0000 (99.1027)  time: 0.0284  data: 0.0002  max mem: 5511
[09:08:22.658419] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5850 (0.5595)  acc1: 81.2500 (82.5310)  acc5: 100.0000 (99.0850)  time: 0.0283  data: 0.0002  max mem: 5511
[09:08:22.943605] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5747 (0.5575)  acc1: 81.2500 (82.5542)  acc5: 100.0000 (99.0961)  time: 0.0284  data: 0.0002  max mem: 5511
[09:08:23.227001] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5548 (0.5580)  acc1: 81.2500 (82.5501)  acc5: 100.0000 (99.1412)  time: 0.0283  data: 0.0002  max mem: 5511
[09:08:23.509313] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5494 (0.5563)  acc1: 84.3750 (82.6352)  acc5: 100.0000 (99.1800)  time: 0.0282  data: 0.0002  max mem: 5511
[09:08:23.790299] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4998 (0.5535)  acc1: 84.3750 (82.7090)  acc5: 100.0000 (99.1515)  time: 0.0280  data: 0.0001  max mem: 5511
[09:08:23.943169] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4929 (0.5530)  acc1: 84.3750 (82.7400)  acc5: 100.0000 (99.1700)  time: 0.0272  data: 0.0001  max mem: 5511
[09:08:24.116475] Test: Total time: 0:00:05 (0.0330 s / it)
[09:08:24.117104] * Acc@1 82.740 Acc@5 99.170 loss 0.553
[09:08:24.117514] Accuracy of the network on the 10000 test images: 82.7%
[09:08:24.117684] Max accuracy: 83.47%
[09:08:24.436637] log_dir: ./output_dir
[09:08:25.271635] Epoch: [64]  [  0/781]  eta: 0:10:50  lr: 0.000079  training_loss: 1.2993 (1.2993)  mae_loss: 0.2547 (0.2547)  classification_loss: 1.0438 (1.0438)  loss_mask: 0.0009 (0.0009)  time: 0.8331  data: 0.5842  max mem: 5511
[09:08:29.214884] Epoch: [64]  [ 20/781]  eta: 0:02:53  lr: 0.000079  training_loss: 1.4974 (1.4739)  mae_loss: 0.2548 (0.2644)  classification_loss: 1.2282 (1.2091)  loss_mask: 0.0002 (0.0004)  time: 0.1971  data: 0.0002  max mem: 5511
[09:08:33.153263] Epoch: [64]  [ 40/781]  eta: 0:02:37  lr: 0.000079  training_loss: 1.4503 (1.4870)  mae_loss: 0.2483 (0.2607)  classification_loss: 1.2079 (1.2259)  loss_mask: 0.0002 (0.0003)  time: 0.1968  data: 0.0002  max mem: 5511
[09:08:37.126861] Epoch: [64]  [ 60/781]  eta: 0:02:29  lr: 0.000079  training_loss: 1.5404 (1.5045)  mae_loss: 0.2520 (0.2639)  classification_loss: 1.2566 (1.2404)  loss_mask: 0.0001 (0.0003)  time: 0.1986  data: 0.0002  max mem: 5511
[09:08:41.078357] Epoch: [64]  [ 80/781]  eta: 0:02:23  lr: 0.000079  training_loss: 1.4732 (1.4935)  mae_loss: 0.2407 (0.2582)  classification_loss: 1.2485 (1.2351)  loss_mask: 0.0001 (0.0002)  time: 0.1975  data: 0.0002  max mem: 5511
[09:08:45.036128] Epoch: [64]  [100/781]  eta: 0:02:18  lr: 0.000079  training_loss: 1.4912 (1.4939)  mae_loss: 0.2444 (0.2549)  classification_loss: 1.2345 (1.2388)  loss_mask: 0.0002 (0.0002)  time: 0.1978  data: 0.0002  max mem: 5511
[09:08:48.975034] Epoch: [64]  [120/781]  eta: 0:02:13  lr: 0.000079  training_loss: 1.4838 (1.4893)  mae_loss: 0.2600 (0.2566)  classification_loss: 1.2258 (1.2325)  loss_mask: 0.0001 (0.0002)  time: 0.1969  data: 0.0003  max mem: 5511
[09:08:52.925557] Epoch: [64]  [140/781]  eta: 0:02:09  lr: 0.000079  training_loss: 1.5440 (1.4932)  mae_loss: 0.2368 (0.2554)  classification_loss: 1.2837 (1.2375)  loss_mask: 0.0002 (0.0002)  time: 0.1974  data: 0.0002  max mem: 5511
[09:08:56.856080] Epoch: [64]  [160/781]  eta: 0:02:04  lr: 0.000079  training_loss: 1.5169 (1.4964)  mae_loss: 0.2492 (0.2550)  classification_loss: 1.2659 (1.2412)  loss_mask: 0.0001 (0.0002)  time: 0.1964  data: 0.0002  max mem: 5511
[09:09:00.831595] Epoch: [64]  [180/781]  eta: 0:02:00  lr: 0.000078  training_loss: 1.4702 (1.4954)  mae_loss: 0.2504 (0.2546)  classification_loss: 1.2147 (1.2405)  loss_mask: 0.0001 (0.0002)  time: 0.1987  data: 0.0002  max mem: 5511
[09:09:04.766032] Epoch: [64]  [200/781]  eta: 0:01:56  lr: 0.000078  training_loss: 1.4991 (1.4957)  mae_loss: 0.2606 (0.2552)  classification_loss: 1.2326 (1.2397)  loss_mask: 0.0002 (0.0008)  time: 0.1966  data: 0.0002  max mem: 5511
[09:09:08.713620] Epoch: [64]  [220/781]  eta: 0:01:52  lr: 0.000078  training_loss: 1.4853 (1.4961)  mae_loss: 0.2549 (0.2558)  classification_loss: 1.2518 (1.2395)  loss_mask: 0.0003 (0.0008)  time: 0.1972  data: 0.0002  max mem: 5511
[09:09:12.676983] Epoch: [64]  [240/781]  eta: 0:01:48  lr: 0.000078  training_loss: 1.4595 (1.4975)  mae_loss: 0.2424 (0.2555)  classification_loss: 1.2220 (1.2412)  loss_mask: 0.0002 (0.0008)  time: 0.1981  data: 0.0002  max mem: 5511
[09:09:16.639782] Epoch: [64]  [260/781]  eta: 0:01:44  lr: 0.000078  training_loss: 1.4932 (1.4980)  mae_loss: 0.2426 (0.2549)  classification_loss: 1.2492 (1.2423)  loss_mask: 0.0002 (0.0007)  time: 0.1981  data: 0.0003  max mem: 5511
[09:09:20.627437] Epoch: [64]  [280/781]  eta: 0:01:40  lr: 0.000078  training_loss: 1.4835 (1.4963)  mae_loss: 0.2600 (0.2557)  classification_loss: 1.2201 (1.2400)  loss_mask: 0.0001 (0.0007)  time: 0.1993  data: 0.0003  max mem: 5511
[09:09:24.580670] Epoch: [64]  [300/781]  eta: 0:01:36  lr: 0.000078  training_loss: 1.5034 (1.4984)  mae_loss: 0.2532 (0.2562)  classification_loss: 1.2449 (1.2415)  loss_mask: 0.0001 (0.0007)  time: 0.1976  data: 0.0002  max mem: 5511
[09:09:28.517286] Epoch: [64]  [320/781]  eta: 0:01:31  lr: 0.000078  training_loss: 1.4931 (1.4981)  mae_loss: 0.2479 (0.2556)  classification_loss: 1.2427 (1.2419)  loss_mask: 0.0001 (0.0006)  time: 0.1967  data: 0.0002  max mem: 5511
[09:09:32.457415] Epoch: [64]  [340/781]  eta: 0:01:27  lr: 0.000078  training_loss: 1.4616 (1.4976)  mae_loss: 0.2665 (0.2559)  classification_loss: 1.1991 (1.2411)  loss_mask: 0.0001 (0.0006)  time: 0.1969  data: 0.0002  max mem: 5511
[09:09:36.415365] Epoch: [64]  [360/781]  eta: 0:01:23  lr: 0.000078  training_loss: 1.5042 (1.4980)  mae_loss: 0.2469 (0.2557)  classification_loss: 1.2529 (1.2417)  loss_mask: 0.0001 (0.0006)  time: 0.1978  data: 0.0002  max mem: 5511
[09:09:40.406230] Epoch: [64]  [380/781]  eta: 0:01:19  lr: 0.000077  training_loss: 1.5759 (1.5027)  mae_loss: 0.2757 (0.2567)  classification_loss: 1.3049 (1.2454)  loss_mask: 0.0001 (0.0006)  time: 0.1995  data: 0.0003  max mem: 5511
[09:09:44.336239] Epoch: [64]  [400/781]  eta: 0:01:15  lr: 0.000077  training_loss: 1.4823 (1.5029)  mae_loss: 0.2541 (0.2568)  classification_loss: 1.2312 (1.2455)  loss_mask: 0.0001 (0.0005)  time: 0.1964  data: 0.0003  max mem: 5511
[09:09:48.282349] Epoch: [64]  [420/781]  eta: 0:01:11  lr: 0.000077  training_loss: 1.5195 (1.5042)  mae_loss: 0.2532 (0.2567)  classification_loss: 1.2648 (1.2470)  loss_mask: 0.0001 (0.0005)  time: 0.1972  data: 0.0002  max mem: 5511
[09:09:52.219022] Epoch: [64]  [440/781]  eta: 0:01:07  lr: 0.000077  training_loss: 1.4946 (1.5039)  mae_loss: 0.2479 (0.2564)  classification_loss: 1.2355 (1.2470)  loss_mask: 0.0001 (0.0005)  time: 0.1968  data: 0.0002  max mem: 5511
[09:09:56.165604] Epoch: [64]  [460/781]  eta: 0:01:03  lr: 0.000077  training_loss: 1.4735 (1.5020)  mae_loss: 0.2420 (0.2561)  classification_loss: 1.2101 (1.2454)  loss_mask: 0.0001 (0.0005)  time: 0.1973  data: 0.0002  max mem: 5511
[09:10:00.132047] Epoch: [64]  [480/781]  eta: 0:00:59  lr: 0.000077  training_loss: 1.4806 (1.5007)  mae_loss: 0.2607 (0.2564)  classification_loss: 1.2240 (1.2438)  loss_mask: 0.0001 (0.0005)  time: 0.1982  data: 0.0002  max mem: 5511
[09:10:04.101453] Epoch: [64]  [500/781]  eta: 0:00:55  lr: 0.000077  training_loss: 1.5318 (1.5018)  mae_loss: 0.2650 (0.2566)  classification_loss: 1.2667 (1.2447)  loss_mask: 0.0001 (0.0005)  time: 0.1984  data: 0.0002  max mem: 5511
[09:10:08.037709] Epoch: [64]  [520/781]  eta: 0:00:51  lr: 0.000077  training_loss: 1.4757 (1.5009)  mae_loss: 0.2578 (0.2566)  classification_loss: 1.2223 (1.2438)  loss_mask: 0.0001 (0.0005)  time: 0.1967  data: 0.0003  max mem: 5511
[09:10:11.996173] Epoch: [64]  [540/781]  eta: 0:00:47  lr: 0.000077  training_loss: 1.5036 (1.5026)  mae_loss: 0.2591 (0.2569)  classification_loss: 1.2647 (1.2453)  loss_mask: 0.0001 (0.0004)  time: 0.1978  data: 0.0002  max mem: 5511
[09:10:15.960270] Epoch: [64]  [560/781]  eta: 0:00:43  lr: 0.000077  training_loss: 1.4692 (1.5016)  mae_loss: 0.2492 (0.2568)  classification_loss: 1.2141 (1.2444)  loss_mask: 0.0001 (0.0004)  time: 0.1981  data: 0.0002  max mem: 5511
[09:10:19.923451] Epoch: [64]  [580/781]  eta: 0:00:39  lr: 0.000076  training_loss: 1.5067 (1.5021)  mae_loss: 0.2447 (0.2565)  classification_loss: 1.2592 (1.2451)  loss_mask: 0.0001 (0.0004)  time: 0.1980  data: 0.0002  max mem: 5511
[09:10:23.853507] Epoch: [64]  [600/781]  eta: 0:00:35  lr: 0.000076  training_loss: 1.5702 (1.5032)  mae_loss: 0.2565 (0.2567)  classification_loss: 1.2818 (1.2462)  loss_mask: 0.0001 (0.0004)  time: 0.1964  data: 0.0003  max mem: 5511
[09:10:27.849733] Epoch: [64]  [620/781]  eta: 0:00:31  lr: 0.000076  training_loss: 1.5303 (1.5038)  mae_loss: 0.2616 (0.2570)  classification_loss: 1.2247 (1.2464)  loss_mask: 0.0001 (0.0004)  time: 0.1997  data: 0.0003  max mem: 5511
[09:10:31.792061] Epoch: [64]  [640/781]  eta: 0:00:28  lr: 0.000076  training_loss: 1.5171 (1.5040)  mae_loss: 0.2381 (0.2568)  classification_loss: 1.2336 (1.2468)  loss_mask: 0.0001 (0.0004)  time: 0.1970  data: 0.0003  max mem: 5511
[09:10:35.758578] Epoch: [64]  [660/781]  eta: 0:00:24  lr: 0.000076  training_loss: 1.5342 (1.5051)  mae_loss: 0.2656 (0.2570)  classification_loss: 1.2672 (1.2476)  loss_mask: 0.0001 (0.0004)  time: 0.1982  data: 0.0003  max mem: 5511
[09:10:39.690535] Epoch: [64]  [680/781]  eta: 0:00:20  lr: 0.000076  training_loss: 1.4886 (1.5057)  mae_loss: 0.2545 (0.2572)  classification_loss: 1.2266 (1.2481)  loss_mask: 0.0001 (0.0004)  time: 0.1965  data: 0.0003  max mem: 5511
[09:10:43.672303] Epoch: [64]  [700/781]  eta: 0:00:16  lr: 0.000076  training_loss: 1.5609 (1.5073)  mae_loss: 0.2607 (0.2574)  classification_loss: 1.3102 (1.2495)  loss_mask: 0.0001 (0.0004)  time: 0.1990  data: 0.0002  max mem: 5511
[09:10:47.656392] Epoch: [64]  [720/781]  eta: 0:00:12  lr: 0.000076  training_loss: 1.5256 (1.5087)  mae_loss: 0.2426 (0.2573)  classification_loss: 1.2965 (1.2510)  loss_mask: 0.0001 (0.0004)  time: 0.1991  data: 0.0002  max mem: 5511
[09:10:51.587149] Epoch: [64]  [740/781]  eta: 0:00:08  lr: 0.000076  training_loss: 1.4256 (1.5076)  mae_loss: 0.2427 (0.2570)  classification_loss: 1.2016 (1.2502)  loss_mask: 0.0001 (0.0004)  time: 0.1965  data: 0.0002  max mem: 5511
[09:10:55.543199] Epoch: [64]  [760/781]  eta: 0:00:04  lr: 0.000076  training_loss: 1.5440 (1.5078)  mae_loss: 0.2522 (0.2570)  classification_loss: 1.2549 (1.2504)  loss_mask: 0.0001 (0.0003)  time: 0.1977  data: 0.0003  max mem: 5511
[09:10:59.468431] Epoch: [64]  [780/781]  eta: 0:00:00  lr: 0.000075  training_loss: 1.4729 (1.5072)  mae_loss: 0.2555 (0.2568)  classification_loss: 1.2526 (1.2501)  loss_mask: 0.0001 (0.0003)  time: 0.1962  data: 0.0002  max mem: 5511
[09:10:59.646985] Epoch: [64] Total time: 0:02:35 (0.1987 s / it)
[09:10:59.647443] Averaged stats: lr: 0.000075  training_loss: 1.4729 (1.5072)  mae_loss: 0.2555 (0.2568)  classification_loss: 1.2526 (1.2501)  loss_mask: 0.0001 (0.0003)
[09:11:00.392905] Test:  [  0/157]  eta: 0:01:56  testing_loss: 0.4326 (0.4326)  acc1: 92.1875 (92.1875)  acc5: 100.0000 (100.0000)  time: 0.7402  data: 0.7103  max mem: 5511
[09:11:00.677197] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.5630 (0.5738)  acc1: 81.2500 (81.2500)  acc5: 100.0000 (99.4318)  time: 0.0929  data: 0.0647  max mem: 5511
[09:11:00.960364] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5367 (0.5501)  acc1: 82.8125 (82.5149)  acc5: 100.0000 (99.5536)  time: 0.0282  data: 0.0002  max mem: 5511
[09:11:01.242617] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5191 (0.5674)  acc1: 82.8125 (81.6532)  acc5: 100.0000 (99.3448)  time: 0.0281  data: 0.0002  max mem: 5511
[09:11:01.525889] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5506 (0.5690)  acc1: 81.2500 (81.8216)  acc5: 100.0000 (99.2759)  time: 0.0282  data: 0.0002  max mem: 5511
[09:11:01.808085] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5380 (0.5616)  acc1: 82.8125 (82.1998)  acc5: 100.0000 (99.2953)  time: 0.0282  data: 0.0002  max mem: 5511
[09:11:02.091173] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5238 (0.5538)  acc1: 82.8125 (82.4795)  acc5: 100.0000 (99.3084)  time: 0.0281  data: 0.0002  max mem: 5511
[09:11:02.374056] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5316 (0.5532)  acc1: 81.2500 (82.5704)  acc5: 100.0000 (99.3178)  time: 0.0282  data: 0.0002  max mem: 5511
[09:11:02.658196] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5726 (0.5600)  acc1: 81.2500 (82.4074)  acc5: 100.0000 (99.3056)  time: 0.0282  data: 0.0002  max mem: 5511
[09:11:02.941003] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5816 (0.5565)  acc1: 82.8125 (82.6751)  acc5: 98.4375 (99.2788)  time: 0.0282  data: 0.0002  max mem: 5511
[09:11:03.224402] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.5737 (0.5614)  acc1: 81.2500 (82.5186)  acc5: 98.4375 (99.2265)  time: 0.0282  data: 0.0002  max mem: 5511
[09:11:03.509848] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6135 (0.5618)  acc1: 79.6875 (82.4747)  acc5: 98.4375 (99.2117)  time: 0.0283  data: 0.0002  max mem: 5511
[09:11:03.793521] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5351 (0.5580)  acc1: 81.2500 (82.5026)  acc5: 100.0000 (99.1865)  time: 0.0283  data: 0.0002  max mem: 5511
[09:11:04.076356] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5351 (0.5581)  acc1: 81.2500 (82.4547)  acc5: 100.0000 (99.2128)  time: 0.0282  data: 0.0002  max mem: 5511
[09:11:04.359523] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5253 (0.5561)  acc1: 82.8125 (82.5687)  acc5: 100.0000 (99.2575)  time: 0.0282  data: 0.0002  max mem: 5511
[09:11:04.642180] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5227 (0.5531)  acc1: 82.8125 (82.6676)  acc5: 100.0000 (99.2653)  time: 0.0282  data: 0.0001  max mem: 5511
[09:11:04.793511] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5038 (0.5533)  acc1: 82.8125 (82.6300)  acc5: 100.0000 (99.2600)  time: 0.0271  data: 0.0001  max mem: 5511
[09:11:04.971202] Test: Total time: 0:00:05 (0.0339 s / it)
[09:11:04.971932] * Acc@1 82.630 Acc@5 99.260 loss 0.553
[09:11:04.972237] Accuracy of the network on the 10000 test images: 82.6%
[09:11:04.972415] Max accuracy: 83.47%
[09:11:05.199204] log_dir: ./output_dir
[09:11:06.041015] Epoch: [65]  [  0/781]  eta: 0:10:56  lr: 0.000075  training_loss: 1.3629 (1.3629)  mae_loss: 0.2679 (0.2679)  classification_loss: 1.0949 (1.0949)  loss_mask: 0.0001 (0.0001)  time: 0.8402  data: 0.6159  max mem: 5511
[09:11:09.988042] Epoch: [65]  [ 20/781]  eta: 0:02:53  lr: 0.000075  training_loss: 1.4829 (1.4863)  mae_loss: 0.2697 (0.2700)  classification_loss: 1.1926 (1.2162)  loss_mask: 0.0001 (0.0001)  time: 0.1972  data: 0.0002  max mem: 5511
[09:11:13.964310] Epoch: [65]  [ 40/781]  eta: 0:02:38  lr: 0.000075  training_loss: 1.5107 (1.5077)  mae_loss: 0.2621 (0.2664)  classification_loss: 1.2706 (1.2412)  loss_mask: 0.0001 (0.0001)  time: 0.1987  data: 0.0002  max mem: 5511
[09:11:17.889474] Epoch: [65]  [ 60/781]  eta: 0:02:29  lr: 0.000075  training_loss: 1.5143 (1.5132)  mae_loss: 0.2438 (0.2605)  classification_loss: 1.2462 (1.2526)  loss_mask: 0.0001 (0.0001)  time: 0.1962  data: 0.0004  max mem: 5511
[09:11:21.864451] Epoch: [65]  [ 80/781]  eta: 0:02:24  lr: 0.000075  training_loss: 1.5245 (1.5219)  mae_loss: 0.2485 (0.2576)  classification_loss: 1.2753 (1.2643)  loss_mask: 0.0001 (0.0001)  time: 0.1987  data: 0.0002  max mem: 5511
[09:11:25.802847] Epoch: [65]  [100/781]  eta: 0:02:18  lr: 0.000075  training_loss: 1.5229 (1.5280)  mae_loss: 0.2676 (0.2592)  classification_loss: 1.2648 (1.2687)  loss_mask: 0.0001 (0.0001)  time: 0.1968  data: 0.0002  max mem: 5511
[09:11:29.768344] Epoch: [65]  [120/781]  eta: 0:02:14  lr: 0.000075  training_loss: 1.4806 (1.5192)  mae_loss: 0.2297 (0.2557)  classification_loss: 1.2234 (1.2634)  loss_mask: 0.0001 (0.0001)  time: 0.1981  data: 0.0002  max mem: 5511
[09:11:33.719204] Epoch: [65]  [140/781]  eta: 0:02:09  lr: 0.000075  training_loss: 1.4809 (1.5094)  mae_loss: 0.2511 (0.2552)  classification_loss: 1.2326 (1.2541)  loss_mask: 0.0001 (0.0001)  time: 0.1975  data: 0.0003  max mem: 5511
[09:11:37.650327] Epoch: [65]  [160/781]  eta: 0:02:05  lr: 0.000075  training_loss: 1.5305 (1.5099)  mae_loss: 0.2578 (0.2560)  classification_loss: 1.2537 (1.2538)  loss_mask: 0.0001 (0.0001)  time: 0.1965  data: 0.0003  max mem: 5511
[09:11:41.631219] Epoch: [65]  [180/781]  eta: 0:02:00  lr: 0.000075  training_loss: 1.4679 (1.5062)  mae_loss: 0.2542 (0.2565)  classification_loss: 1.1998 (1.2496)  loss_mask: 0.0002 (0.0002)  time: 0.1990  data: 0.0003  max mem: 5511
[09:11:45.573952] Epoch: [65]  [200/781]  eta: 0:01:56  lr: 0.000075  training_loss: 1.4899 (1.5038)  mae_loss: 0.2501 (0.2566)  classification_loss: 1.1849 (1.2469)  loss_mask: 0.0005 (0.0003)  time: 0.1971  data: 0.0002  max mem: 5511
[09:11:49.495104] Epoch: [65]  [220/781]  eta: 0:01:52  lr: 0.000074  training_loss: 1.4934 (1.5050)  mae_loss: 0.2495 (0.2567)  classification_loss: 1.2587 (1.2480)  loss_mask: 0.0001 (0.0003)  time: 0.1960  data: 0.0002  max mem: 5511
[09:11:53.451097] Epoch: [65]  [240/781]  eta: 0:01:48  lr: 0.000074  training_loss: 1.5146 (1.5032)  mae_loss: 0.2591 (0.2570)  classification_loss: 1.2542 (1.2460)  loss_mask: 0.0001 (0.0003)  time: 0.1977  data: 0.0002  max mem: 5511
[09:11:57.410967] Epoch: [65]  [260/781]  eta: 0:01:44  lr: 0.000074  training_loss: 1.5087 (1.5041)  mae_loss: 0.2452 (0.2560)  classification_loss: 1.2552 (1.2478)  loss_mask: 0.0001 (0.0003)  time: 0.1979  data: 0.0002  max mem: 5511
[09:12:01.350489] Epoch: [65]  [280/781]  eta: 0:01:40  lr: 0.000074  training_loss: 1.4543 (1.5022)  mae_loss: 0.2314 (0.2556)  classification_loss: 1.2192 (1.2463)  loss_mask: 0.0001 (0.0003)  time: 0.1969  data: 0.0002  max mem: 5511
[09:12:05.289497] Epoch: [65]  [300/781]  eta: 0:01:35  lr: 0.000074  training_loss: 1.5087 (1.5047)  mae_loss: 0.2541 (0.2559)  classification_loss: 1.2644 (1.2485)  loss_mask: 0.0001 (0.0003)  time: 0.1969  data: 0.0002  max mem: 5511
[09:12:09.224465] Epoch: [65]  [320/781]  eta: 0:01:31  lr: 0.000074  training_loss: 1.5359 (1.5050)  mae_loss: 0.2452 (0.2559)  classification_loss: 1.2571 (1.2487)  loss_mask: 0.0004 (0.0004)  time: 0.1967  data: 0.0002  max mem: 5511
[09:12:13.197133] Epoch: [65]  [340/781]  eta: 0:01:27  lr: 0.000074  training_loss: 1.4840 (1.5040)  mae_loss: 0.2378 (0.2553)  classification_loss: 1.2333 (1.2482)  loss_mask: 0.0002 (0.0004)  time: 0.1985  data: 0.0002  max mem: 5511
[09:12:17.172296] Epoch: [65]  [360/781]  eta: 0:01:23  lr: 0.000074  training_loss: 1.5349 (1.5054)  mae_loss: 0.2385 (0.2549)  classification_loss: 1.2974 (1.2501)  loss_mask: 0.0001 (0.0005)  time: 0.1987  data: 0.0002  max mem: 5511
[09:12:21.107669] Epoch: [65]  [380/781]  eta: 0:01:19  lr: 0.000074  training_loss: 1.5417 (1.5129)  mae_loss: 0.2677 (0.2557)  classification_loss: 1.2226 (1.2491)  loss_mask: 0.0046 (0.0081)  time: 0.1967  data: 0.0003  max mem: 5511
[09:12:25.057214] Epoch: [65]  [400/781]  eta: 0:01:15  lr: 0.000074  training_loss: 1.5871 (1.5175)  mae_loss: 0.2635 (0.2564)  classification_loss: 1.2019 (1.2477)  loss_mask: 0.0620 (0.0134)  time: 0.1974  data: 0.0002  max mem: 5511
[09:12:28.997345] Epoch: [65]  [420/781]  eta: 0:01:11  lr: 0.000073  training_loss: 1.5327 (1.5193)  mae_loss: 0.2646 (0.2569)  classification_loss: 1.2374 (1.2482)  loss_mask: 0.0294 (0.0142)  time: 0.1969  data: 0.0003  max mem: 5511
[09:12:32.939582] Epoch: [65]  [440/781]  eta: 0:01:07  lr: 0.000073  training_loss: 1.5375 (1.5203)  mae_loss: 0.2478 (0.2565)  classification_loss: 1.2853 (1.2492)  loss_mask: 0.0134 (0.0145)  time: 0.1970  data: 0.0002  max mem: 5511
[09:12:36.869296] Epoch: [65]  [460/781]  eta: 0:01:03  lr: 0.000073  training_loss: 1.4451 (1.5166)  mae_loss: 0.2590 (0.2564)  classification_loss: 1.2067 (1.2460)  loss_mask: 0.0063 (0.0142)  time: 0.1964  data: 0.0002  max mem: 5511
[09:12:40.807366] Epoch: [65]  [480/781]  eta: 0:00:59  lr: 0.000073  training_loss: 1.4595 (1.5156)  mae_loss: 0.2415 (0.2561)  classification_loss: 1.2229 (1.2457)  loss_mask: 0.0026 (0.0137)  time: 0.1968  data: 0.0002  max mem: 5511
[09:12:44.768043] Epoch: [65]  [500/781]  eta: 0:00:55  lr: 0.000073  training_loss: 1.4644 (1.5140)  mae_loss: 0.2425 (0.2558)  classification_loss: 1.2279 (1.2449)  loss_mask: 0.0019 (0.0133)  time: 0.1979  data: 0.0002  max mem: 5511
[09:12:48.709758] Epoch: [65]  [520/781]  eta: 0:00:51  lr: 0.000073  training_loss: 1.4508 (1.5136)  mae_loss: 0.2500 (0.2555)  classification_loss: 1.2302 (1.2452)  loss_mask: 0.0015 (0.0129)  time: 0.1970  data: 0.0002  max mem: 5511
[09:12:52.644966] Epoch: [65]  [540/781]  eta: 0:00:47  lr: 0.000073  training_loss: 1.5293 (1.5133)  mae_loss: 0.2433 (0.2552)  classification_loss: 1.2567 (1.2455)  loss_mask: 0.0012 (0.0125)  time: 0.1967  data: 0.0003  max mem: 5511
[09:12:56.609543] Epoch: [65]  [560/781]  eta: 0:00:43  lr: 0.000073  training_loss: 1.5007 (1.5126)  mae_loss: 0.2561 (0.2552)  classification_loss: 1.2335 (1.2454)  loss_mask: 0.0009 (0.0121)  time: 0.1981  data: 0.0002  max mem: 5511
[09:13:00.590059] Epoch: [65]  [580/781]  eta: 0:00:39  lr: 0.000073  training_loss: 1.5339 (1.5128)  mae_loss: 0.2607 (0.2552)  classification_loss: 1.2801 (1.2459)  loss_mask: 0.0008 (0.0117)  time: 0.1989  data: 0.0003  max mem: 5511
[09:13:04.545037] Epoch: [65]  [600/781]  eta: 0:00:35  lr: 0.000073  training_loss: 1.4937 (1.5126)  mae_loss: 0.2445 (0.2555)  classification_loss: 1.2087 (1.2458)  loss_mask: 0.0005 (0.0114)  time: 0.1977  data: 0.0003  max mem: 5511
[09:13:08.502031] Epoch: [65]  [620/781]  eta: 0:00:31  lr: 0.000073  training_loss: 1.5375 (1.5135)  mae_loss: 0.2780 (0.2559)  classification_loss: 1.2687 (1.2465)  loss_mask: 0.0005 (0.0110)  time: 0.1978  data: 0.0002  max mem: 5511
[09:13:12.439250] Epoch: [65]  [640/781]  eta: 0:00:27  lr: 0.000072  training_loss: 1.5013 (1.5135)  mae_loss: 0.2723 (0.2564)  classification_loss: 1.2425 (1.2465)  loss_mask: 0.0003 (0.0107)  time: 0.1968  data: 0.0003  max mem: 5511

[09:13:16.377752] Epoch: [65]  [660/781]  eta: 0:00:24  lr: 0.000072  training_loss: 1.5155 (1.5143)  mae_loss: 0.2612 (0.2563)  classification_loss: 1.2669 (1.2476)  loss_mask: 0.0004 (0.0104)  time: 0.1968  data: 0.0002  max mem: 5511
[09:13:20.355459] Epoch: [65]  [680/781]  eta: 0:00:20  lr: 0.000072  training_loss: 1.5219 (1.5147)  mae_loss: 0.2492 (0.2562)  classification_loss: 1.2687 (1.2484)  loss_mask: 0.0003 (0.0101)  time: 0.1988  data: 0.0004  max mem: 5511
[09:13:24.311482] Epoch: [65]  [700/781]  eta: 0:00:16  lr: 0.000072  training_loss: 1.4915 (1.5153)  mae_loss: 0.2572 (0.2564)  classification_loss: 1.2371 (1.2492)  loss_mask: 0.0003 (0.0098)  time: 0.1977  data: 0.0002  max mem: 5511
[09:13:28.251596] Epoch: [65]  [720/781]  eta: 0:00:12  lr: 0.000072  training_loss: 1.5242 (1.5162)  mae_loss: 0.2573 (0.2563)  classification_loss: 1.2782 (1.2503)  loss_mask: 0.0004 (0.0095)  time: 0.1969  data: 0.0002  max mem: 5511
[09:13:32.188491] Epoch: [65]  [740/781]  eta: 0:00:08  lr: 0.000072  training_loss: 1.4545 (1.5154)  mae_loss: 0.2380 (0.2562)  classification_loss: 1.2189 (1.2499)  loss_mask: 0.0002 (0.0093)  time: 0.1967  data: 0.0003  max mem: 5511
[09:13:36.125988] Epoch: [65]  [760/781]  eta: 0:00:04  lr: 0.000072  training_loss: 1.4929 (1.5156)  mae_loss: 0.2560 (0.2564)  classification_loss: 1.2439 (1.2502)  loss_mask: 0.0003 (0.0090)  time: 0.1967  data: 0.0004  max mem: 5511
[09:13:40.079337] Epoch: [65]  [780/781]  eta: 0:00:00  lr: 0.000072  training_loss: 1.4754 (1.5145)  mae_loss: 0.2426 (0.2561)  classification_loss: 1.2207 (1.2496)  loss_mask: 0.0002 (0.0088)  time: 0.1976  data: 0.0002  max mem: 5511
[09:13:40.267747] Epoch: [65] Total time: 0:02:35 (0.1986 s / it)
[09:13:40.269096] Averaged stats: lr: 0.000072  training_loss: 1.4754 (1.5145)  mae_loss: 0.2426 (0.2561)  classification_loss: 1.2207 (1.2496)  loss_mask: 0.0002 (0.0088)
[09:13:40.845876] Test:  [  0/157]  eta: 0:01:29  testing_loss: 0.4177 (0.4177)  acc1: 89.0625 (89.0625)  acc5: 100.0000 (100.0000)  time: 0.5720  data: 0.5425  max mem: 5511
[09:13:41.143985] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.5697 (0.5501)  acc1: 81.2500 (82.5284)  acc5: 100.0000 (99.5739)  time: 0.0790  data: 0.0505  max mem: 5511
[09:13:41.434385] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4950 (0.5213)  acc1: 82.8125 (84.1518)  acc5: 100.0000 (99.6280)  time: 0.0293  data: 0.0007  max mem: 5511
[09:13:41.726343] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.4876 (0.5311)  acc1: 84.3750 (82.9637)  acc5: 100.0000 (99.3952)  time: 0.0290  data: 0.0002  max mem: 5511
[09:13:42.019050] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.5179 (0.5377)  acc1: 82.8125 (82.6220)  acc5: 98.4375 (99.2378)  time: 0.0291  data: 0.0002  max mem: 5511
[09:13:42.333414] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5175 (0.5295)  acc1: 82.8125 (83.2108)  acc5: 100.0000 (99.2341)  time: 0.0302  data: 0.0002  max mem: 5511
[09:13:42.622530] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4743 (0.5223)  acc1: 85.9375 (83.5041)  acc5: 100.0000 (99.2572)  time: 0.0300  data: 0.0002  max mem: 5511
[09:13:42.909882] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4943 (0.5216)  acc1: 84.3750 (83.5167)  acc5: 98.4375 (99.1637)  time: 0.0287  data: 0.0002  max mem: 5511
[09:13:43.198163] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5280 (0.5323)  acc1: 82.8125 (83.2755)  acc5: 98.4375 (99.1705)  time: 0.0287  data: 0.0002  max mem: 5511
[09:13:43.490256] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5349 (0.5295)  acc1: 82.8125 (83.4993)  acc5: 100.0000 (99.1415)  time: 0.0289  data: 0.0003  max mem: 5511
[09:13:43.783070] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5201 (0.5324)  acc1: 82.8125 (83.3694)  acc5: 98.4375 (99.1337)  time: 0.0291  data: 0.0004  max mem: 5511
[09:13:44.084557] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5840 (0.5343)  acc1: 81.2500 (83.2489)  acc5: 100.0000 (99.1413)  time: 0.0295  data: 0.0003  max mem: 5511
[09:13:44.375950] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5840 (0.5339)  acc1: 82.8125 (83.2515)  acc5: 100.0000 (99.1348)  time: 0.0295  data: 0.0003  max mem: 5511
[09:13:44.676683] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5420 (0.5346)  acc1: 81.2500 (83.2538)  acc5: 98.4375 (99.1412)  time: 0.0295  data: 0.0003  max mem: 5511
[09:13:44.962875] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5237 (0.5333)  acc1: 81.2500 (83.3223)  acc5: 100.0000 (99.1467)  time: 0.0292  data: 0.0002  max mem: 5511
[09:13:45.245036] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4972 (0.5303)  acc1: 84.3750 (83.4023)  acc5: 100.0000 (99.1618)  time: 0.0283  data: 0.0001  max mem: 5511
[09:13:45.396773] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4853 (0.5311)  acc1: 82.8125 (83.3800)  acc5: 100.0000 (99.1700)  time: 0.0272  data: 0.0001  max mem: 5511
[09:13:45.567824] Test: Total time: 0:00:05 (0.0337 s / it)
[09:13:45.568636] * Acc@1 83.380 Acc@5 99.170 loss 0.531
[09:13:45.568971] Accuracy of the network on the 10000 test images: 83.4%
[09:13:45.569183] Max accuracy: 83.47%
[09:13:45.771537] log_dir: ./output_dir
[09:13:46.729062] Epoch: [66]  [  0/781]  eta: 0:12:26  lr: 0.000072  training_loss: 1.4181 (1.4181)  mae_loss: 0.2850 (0.2850)  classification_loss: 1.1330 (1.1330)  loss_mask: 0.0001 (0.0001)  time: 0.9556  data: 0.7441  max mem: 5511
[09:13:50.720753] Epoch: [66]  [ 20/781]  eta: 0:02:59  lr: 0.000072  training_loss: 1.4321 (1.4643)  mae_loss: 0.2601 (0.2598)  classification_loss: 1.1827 (1.2043)  loss_mask: 0.0003 (0.0003)  time: 0.1995  data: 0.0003  max mem: 5511
[09:13:54.677527] Epoch: [66]  [ 40/781]  eta: 0:02:40  lr: 0.000072  training_loss: 1.4550 (1.4730)  mae_loss: 0.2335 (0.2514)  classification_loss: 1.2313 (1.2214)  loss_mask: 0.0002 (0.0003)  time: 0.1978  data: 0.0002  max mem: 5511
[09:13:58.613664] Epoch: [66]  [ 60/781]  eta: 0:02:31  lr: 0.000071  training_loss: 1.5206 (1.4846)  mae_loss: 0.2576 (0.2543)  classification_loss: 1.2493 (1.2300)  loss_mask: 0.0002 (0.0002)  time: 0.1967  data: 0.0002  max mem: 5511
[09:14:02.577892] Epoch: [66]  [ 80/781]  eta: 0:02:25  lr: 0.000071  training_loss: 1.4644 (1.4852)  mae_loss: 0.2411 (0.2524)  classification_loss: 1.2173 (1.2326)  loss_mask: 0.0002 (0.0003)  time: 0.1981  data: 0.0003  max mem: 5511
[09:14:06.564375] Epoch: [66]  [100/781]  eta: 0:02:20  lr: 0.000071  training_loss: 1.4549 (1.4781)  mae_loss: 0.2484 (0.2512)  classification_loss: 1.2010 (1.2266)  loss_mask: 0.0002 (0.0003)  time: 0.1992  data: 0.0002  max mem: 5511
[09:14:10.512559] Epoch: [66]  [120/781]  eta: 0:02:15  lr: 0.000071  training_loss: 1.4756 (1.4780)  mae_loss: 0.2598 (0.2519)  classification_loss: 1.2182 (1.2258)  loss_mask: 0.0002 (0.0002)  time: 0.1973  data: 0.0002  max mem: 5511
[09:14:14.510168] Epoch: [66]  [140/781]  eta: 0:02:10  lr: 0.000071  training_loss: 1.4603 (1.4776)  mae_loss: 0.2319 (0.2506)  classification_loss: 1.2042 (1.2267)  loss_mask: 0.0002 (0.0003)  time: 0.1998  data: 0.0002  max mem: 5511
[09:14:18.459804] Epoch: [66]  [160/781]  eta: 0:02:06  lr: 0.000071  training_loss: 1.5021 (1.4820)  mae_loss: 0.2635 (0.2518)  classification_loss: 1.2463 (1.2299)  loss_mask: 0.0002 (0.0003)  time: 0.1974  data: 0.0002  max mem: 5511
[09:14:22.412911] Epoch: [66]  [180/781]  eta: 0:02:01  lr: 0.000071  training_loss: 1.4651 (1.4848)  mae_loss: 0.2624 (0.2532)  classification_loss: 1.1960 (1.2313)  loss_mask: 0.0001 (0.0002)  time: 0.1976  data: 0.0002  max mem: 5511
[09:14:26.364063] Epoch: [66]  [200/781]  eta: 0:01:57  lr: 0.000071  training_loss: 1.5197 (1.4862)  mae_loss: 0.2494 (0.2535)  classification_loss: 1.2529 (1.2325)  loss_mask: 0.0002 (0.0002)  time: 0.1975  data: 0.0002  max mem: 5511
[09:14:30.339273] Epoch: [66]  [220/781]  eta: 0:01:53  lr: 0.000071  training_loss: 1.5084 (1.4861)  mae_loss: 0.2529 (0.2533)  classification_loss: 1.2614 (1.2326)  loss_mask: 0.0002 (0.0002)  time: 0.1987  data: 0.0004  max mem: 5511
[09:14:34.269508] Epoch: [66]  [240/781]  eta: 0:01:48  lr: 0.000071  training_loss: 1.4480 (1.4855)  mae_loss: 0.2463 (0.2525)  classification_loss: 1.1922 (1.2327)  loss_mask: 0.0002 (0.0002)  time: 0.1964  data: 0.0002  max mem: 5511
[09:14:38.277474] Epoch: [66]  [260/781]  eta: 0:01:44  lr: 0.000071  training_loss: 1.4896 (1.4858)  mae_loss: 0.2432 (0.2525)  classification_loss: 1.2301 (1.2331)  loss_mask: 0.0002 (0.0002)  time: 0.2003  data: 0.0002  max mem: 5511
[09:14:42.213788] Epoch: [66]  [280/781]  eta: 0:01:40  lr: 0.000070  training_loss: 1.4922 (1.4873)  mae_loss: 0.2665 (0.2530)  classification_loss: 1.2370 (1.2341)  loss_mask: 0.0002 (0.0002)  time: 0.1967  data: 0.0004  max mem: 5511
[09:14:46.197482] Epoch: [66]  [300/781]  eta: 0:01:36  lr: 0.000070  training_loss: 1.5464 (1.4910)  mae_loss: 0.2469 (0.2527)  classification_loss: 1.2909 (1.2380)  loss_mask: 0.0001 (0.0002)  time: 0.1991  data: 0.0005  max mem: 5511
[09:14:50.160343] Epoch: [66]  [320/781]  eta: 0:01:32  lr: 0.000070  training_loss: 1.4848 (1.4910)  mae_loss: 0.2579 (0.2531)  classification_loss: 1.2261 (1.2377)  loss_mask: 0.0002 (0.0002)  time: 0.1980  data: 0.0003  max mem: 5511
[09:14:54.109754] Epoch: [66]  [340/781]  eta: 0:01:28  lr: 0.000070  training_loss: 1.4633 (1.4903)  mae_loss: 0.2450 (0.2529)  classification_loss: 1.1965 (1.2371)  loss_mask: 0.0002 (0.0002)  time: 0.1974  data: 0.0002  max mem: 5511
[09:14:58.067044] Epoch: [66]  [360/781]  eta: 0:01:24  lr: 0.000070  training_loss: 1.4829 (1.4899)  mae_loss: 0.2590 (0.2535)  classification_loss: 1.2275 (1.2362)  loss_mask: 0.0001 (0.0002)  time: 0.1977  data: 0.0003  max mem: 5511
[09:15:01.996796] Epoch: [66]  [380/781]  eta: 0:01:20  lr: 0.000070  training_loss: 1.4119 (1.4874)  mae_loss: 0.2565 (0.2537)  classification_loss: 1.1717 (1.2335)  loss_mask: 0.0001 (0.0002)  time: 0.1964  data: 0.0003  max mem: 5511
[09:15:05.929901] Epoch: [66]  [400/781]  eta: 0:01:16  lr: 0.000070  training_loss: 1.4934 (1.4885)  mae_loss: 0.2428 (0.2533)  classification_loss: 1.2360 (1.2349)  loss_mask: 0.0001 (0.0002)  time: 0.1966  data: 0.0002  max mem: 5511
[09:15:09.864873] Epoch: [66]  [420/781]  eta: 0:01:12  lr: 0.000070  training_loss: 1.4995 (1.4888)  mae_loss: 0.2664 (0.2540)  classification_loss: 1.2231 (1.2345)  loss_mask: 0.0001 (0.0002)  time: 0.1967  data: 0.0002  max mem: 5511
[09:15:13.838969] Epoch: [66]  [440/781]  eta: 0:01:08  lr: 0.000070  training_loss: 1.4523 (1.4886)  mae_loss: 0.2523 (0.2539)  classification_loss: 1.2312 (1.2345)  loss_mask: 0.0002 (0.0002)  time: 0.1986  data: 0.0003  max mem: 5511
[09:15:17.796466] Epoch: [66]  [460/781]  eta: 0:01:04  lr: 0.000070  training_loss: 1.4651 (1.4890)  mae_loss: 0.2471 (0.2539)  classification_loss: 1.2192 (1.2349)  loss_mask: 0.0001 (0.0002)  time: 0.1978  data: 0.0002  max mem: 5511
[09:15:21.746770] Epoch: [66]  [480/781]  eta: 0:01:00  lr: 0.000069  training_loss: 1.4809 (1.4894)  mae_loss: 0.2470 (0.2538)  classification_loss: 1.2369 (1.2354)  loss_mask: 0.0001 (0.0002)  time: 0.1974  data: 0.0002  max mem: 5511
[09:15:25.689435] Epoch: [66]  [500/781]  eta: 0:00:56  lr: 0.000069  training_loss: 1.5252 (1.4906)  mae_loss: 0.2377 (0.2535)  classification_loss: 1.2643 (1.2369)  loss_mask: 0.0001 (0.0002)  time: 0.1971  data: 0.0002  max mem: 5511
[09:15:29.635718] Epoch: [66]  [520/781]  eta: 0:00:52  lr: 0.000069  training_loss: 1.4935 (1.4918)  mae_loss: 0.2530 (0.2536)  classification_loss: 1.2724 (1.2380)  loss_mask: 0.0002 (0.0002)  time: 0.1972  data: 0.0002  max mem: 5511
[09:15:33.568344] Epoch: [66]  [540/781]  eta: 0:00:47  lr: 0.000069  training_loss: 1.4908 (1.4924)  mae_loss: 0.2526 (0.2536)  classification_loss: 1.2232 (1.2385)  loss_mask: 0.0001 (0.0002)  time: 0.1965  data: 0.0002  max mem: 5511
[09:15:37.492000] Epoch: [66]  [560/781]  eta: 0:00:43  lr: 0.000069  training_loss: 1.4758 (1.4922)  mae_loss: 0.2538 (0.2539)  classification_loss: 1.2392 (1.2381)  loss_mask: 0.0001 (0.0002)  time: 0.1961  data: 0.0002  max mem: 5511
[09:15:41.430729] Epoch: [66]  [580/781]  eta: 0:00:39  lr: 0.000069  training_loss: 1.4610 (1.4916)  mae_loss: 0.2641 (0.2542)  classification_loss: 1.2004 (1.2372)  loss_mask: 0.0001 (0.0002)  time: 0.1968  data: 0.0003  max mem: 5511
[09:15:45.388946] Epoch: [66]  [600/781]  eta: 0:00:36  lr: 0.000069  training_loss: 1.5161 (1.4927)  mae_loss: 0.2601 (0.2544)  classification_loss: 1.2422 (1.2382)  loss_mask: 0.0001 (0.0002)  time: 0.1978  data: 0.0002  max mem: 5511
[09:15:49.367549] Epoch: [66]  [620/781]  eta: 0:00:32  lr: 0.000069  training_loss: 1.4987 (1.4928)  mae_loss: 0.2495 (0.2546)  classification_loss: 1.2459 (1.2381)  loss_mask: 0.0001 (0.0002)  time: 0.1988  data: 0.0002  max mem: 5511
[09:15:53.309236] Epoch: [66]  [640/781]  eta: 0:00:28  lr: 0.000069  training_loss: 1.4406 (1.4925)  mae_loss: 0.2532 (0.2544)  classification_loss: 1.1902 (1.2378)  loss_mask: 0.0001 (0.0002)  time: 0.1970  data: 0.0002  max mem: 5511
[09:15:57.261908] Epoch: [66]  [660/781]  eta: 0:00:24  lr: 0.000069  training_loss: 1.5267 (1.4934)  mae_loss: 0.2453 (0.2544)  classification_loss: 1.2808 (1.2387)  loss_mask: 0.0006 (0.0002)  time: 0.1975  data: 0.0002  max mem: 5511
[09:16:01.207293] Epoch: [66]  [680/781]  eta: 0:00:20  lr: 0.000069  training_loss: 1.4595 (1.4933)  mae_loss: 0.2648 (0.2547)  classification_loss: 1.2120 (1.2384)  loss_mask: 0.0004 (0.0003)  time: 0.1972  data: 0.0005  max mem: 5511
[09:16:05.156033] Epoch: [66]  [700/781]  eta: 0:00:16  lr: 0.000068  training_loss: 1.5461 (1.4950)  mae_loss: 0.2557 (0.2548)  classification_loss: 1.2831 (1.2397)  loss_mask: 0.0025 (0.0004)  time: 0.1973  data: 0.0002  max mem: 5511
[09:16:09.094998] Epoch: [66]  [720/781]  eta: 0:00:12  lr: 0.000068  training_loss: 1.5133 (1.4955)  mae_loss: 0.2553 (0.2549)  classification_loss: 1.2441 (1.2402)  loss_mask: 0.0002 (0.0004)  time: 0.1969  data: 0.0002  max mem: 5511
[09:16:13.019867] Epoch: [66]  [740/781]  eta: 0:00:08  lr: 0.000068  training_loss: 1.4303 (1.4947)  mae_loss: 0.2369 (0.2547)  classification_loss: 1.1933 (1.2396)  loss_mask: 0.0001 (0.0004)  time: 0.1962  data: 0.0002  max mem: 5511
[09:16:16.994570] Epoch: [66]  [760/781]  eta: 0:00:04  lr: 0.000068  training_loss: 1.5326 (1.4953)  mae_loss: 0.2448 (0.2544)  classification_loss: 1.2767 (1.2405)  loss_mask: 0.0001 (0.0004)  time: 0.1987  data: 0.0002  max mem: 5511
[09:16:20.934311] Epoch: [66]  [780/781]  eta: 0:00:00  lr: 0.000068  training_loss: 1.5064 (1.4955)  mae_loss: 0.2581 (0.2545)  classification_loss: 1.2486 (1.2406)  loss_mask: 0.0001 (0.0004)  time: 0.1969  data: 0.0002  max mem: 5511
[09:16:21.096518] Epoch: [66] Total time: 0:02:35 (0.1989 s / it)
[09:16:21.097034] Averaged stats: lr: 0.000068  training_loss: 1.5064 (1.4955)  mae_loss: 0.2581 (0.2545)  classification_loss: 1.2486 (1.2406)  loss_mask: 0.0001 (0.0004)
[09:16:21.822933] Test:  [  0/157]  eta: 0:01:53  testing_loss: 0.4591 (0.4591)  acc1: 85.9375 (85.9375)  acc5: 100.0000 (100.0000)  time: 0.7210  data: 0.6913  max mem: 5511
[09:16:22.107939] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.5247 (0.5437)  acc1: 82.8125 (82.1023)  acc5: 100.0000 (99.2898)  time: 0.0913  data: 0.0630  max mem: 5511
[09:16:22.399772] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5050 (0.5222)  acc1: 84.3750 (83.5565)  acc5: 100.0000 (99.4048)  time: 0.0287  data: 0.0002  max mem: 5511
[09:16:22.683484] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5050 (0.5373)  acc1: 84.3750 (82.8629)  acc5: 100.0000 (99.2944)  time: 0.0287  data: 0.0002  max mem: 5511
[09:16:22.968683] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5254 (0.5438)  acc1: 81.2500 (82.5457)  acc5: 98.4375 (99.1997)  time: 0.0283  data: 0.0002  max mem: 5511
[09:16:23.253757] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5160 (0.5351)  acc1: 82.8125 (83.1495)  acc5: 100.0000 (99.2341)  time: 0.0284  data: 0.0002  max mem: 5511
[09:16:23.536624] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4778 (0.5300)  acc1: 85.9375 (83.2992)  acc5: 100.0000 (99.2572)  time: 0.0283  data: 0.0002  max mem: 5511
[09:16:23.819811] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5117 (0.5298)  acc1: 84.3750 (83.4067)  acc5: 100.0000 (99.2518)  time: 0.0282  data: 0.0002  max mem: 5511
[09:16:24.104468] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5407 (0.5401)  acc1: 82.8125 (82.9668)  acc5: 98.4375 (99.1898)  time: 0.0283  data: 0.0002  max mem: 5511
[09:16:24.386928] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5215 (0.5362)  acc1: 82.8125 (83.2074)  acc5: 100.0000 (99.2273)  time: 0.0282  data: 0.0002  max mem: 5511
[09:16:24.669584] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.5215 (0.5390)  acc1: 84.3750 (83.1838)  acc5: 100.0000 (99.2110)  time: 0.0281  data: 0.0002  max mem: 5511
[09:16:24.955536] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5699 (0.5400)  acc1: 82.8125 (83.1222)  acc5: 100.0000 (99.2258)  time: 0.0283  data: 0.0002  max mem: 5511
[09:16:25.242432] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5076 (0.5355)  acc1: 84.3750 (83.2774)  acc5: 100.0000 (99.2252)  time: 0.0285  data: 0.0002  max mem: 5511
[09:16:25.527790] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5076 (0.5346)  acc1: 81.2500 (83.3135)  acc5: 100.0000 (99.2366)  time: 0.0285  data: 0.0002  max mem: 5511
[09:16:25.810068] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5152 (0.5327)  acc1: 82.8125 (83.4441)  acc5: 100.0000 (99.2354)  time: 0.0283  data: 0.0002  max mem: 5511
[09:16:26.092075] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5080 (0.5299)  acc1: 84.3750 (83.6093)  acc5: 98.4375 (99.2343)  time: 0.0281  data: 0.0001  max mem: 5511
[09:16:26.242046] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5023 (0.5304)  acc1: 84.3750 (83.6000)  acc5: 100.0000 (99.2300)  time: 0.0271  data: 0.0001  max mem: 5511
[09:16:26.416348] Test: Total time: 0:00:05 (0.0339 s / it)
[09:16:26.417440] * Acc@1 83.600 Acc@5 99.230 loss 0.530
[09:16:26.417740] Accuracy of the network on the 10000 test images: 83.6%
[09:16:26.417927] Max accuracy: 83.60%
[09:16:26.669676] log_dir: ./output_dir
[09:16:27.521061] Epoch: [67]  [  0/781]  eta: 0:11:03  lr: 0.000068  training_loss: 1.4293 (1.4293)  mae_loss: 0.2458 (0.2458)  classification_loss: 1.1834 (1.1834)  loss_mask: 0.0001 (0.0001)  time: 0.8495  data: 0.6320  max mem: 5511
[09:16:31.491451] Epoch: [67]  [ 20/781]  eta: 0:02:54  lr: 0.000068  training_loss: 1.4775 (1.4920)  mae_loss: 0.2593 (0.2596)  classification_loss: 1.1853 (1.2323)  loss_mask: 0.0001 (0.0001)  time: 0.1984  data: 0.0002  max mem: 5511
[09:16:35.434478] Epoch: [67]  [ 40/781]  eta: 0:02:38  lr: 0.000068  training_loss: 1.4796 (1.4928)  mae_loss: 0.2501 (0.2581)  classification_loss: 1.2398 (1.2346)  loss_mask: 0.0002 (0.0001)  time: 0.1971  data: 0.0002  max mem: 5511
[09:16:39.377477] Epoch: [67]  [ 60/781]  eta: 0:02:30  lr: 0.000068  training_loss: 1.4796 (1.4917)  mae_loss: 0.2339 (0.2531)  classification_loss: 1.2286 (1.2385)  loss_mask: 0.0001 (0.0001)  time: 0.1971  data: 0.0002  max mem: 5511
[09:16:43.319537] Epoch: [67]  [ 80/781]  eta: 0:02:24  lr: 0.000068  training_loss: 1.4510 (1.4852)  mae_loss: 0.2420 (0.2516)  classification_loss: 1.2019 (1.2334)  loss_mask: 0.0001 (0.0001)  time: 0.1970  data: 0.0005  max mem: 5511
[09:16:47.264950] Epoch: [67]  [100/781]  eta: 0:02:18  lr: 0.000068  training_loss: 1.4683 (1.4857)  mae_loss: 0.2478 (0.2510)  classification_loss: 1.2229 (1.2346)  loss_mask: 0.0001 (0.0001)  time: 0.1972  data: 0.0002  max mem: 5511
[09:16:51.202837] Epoch: [67]  [120/781]  eta: 0:02:13  lr: 0.000068  training_loss: 1.4603 (1.4805)  mae_loss: 0.2279 (0.2488)  classification_loss: 1.2314 (1.2316)  loss_mask: 0.0001 (0.0001)  time: 0.1968  data: 0.0002  max mem: 5511
[09:16:55.128676] Epoch: [67]  [140/781]  eta: 0:02:09  lr: 0.000067  training_loss: 1.4535 (1.4806)  mae_loss: 0.2494 (0.2495)  classification_loss: 1.2291 (1.2309)  loss_mask: 0.0001 (0.0001)  time: 0.1962  data: 0.0003  max mem: 5511
[09:16:59.107815] Epoch: [67]  [160/781]  eta: 0:02:05  lr: 0.000067  training_loss: 1.4441 (1.4783)  mae_loss: 0.2407 (0.2487)  classification_loss: 1.2034 (1.2295)  loss_mask: 0.0001 (0.0001)  time: 0.1989  data: 0.0003  max mem: 5511
[09:17:03.053709] Epoch: [67]  [180/781]  eta: 0:02:00  lr: 0.000067  training_loss: 1.5182 (1.4805)  mae_loss: 0.2392 (0.2487)  classification_loss: 1.2398 (1.2316)  loss_mask: 0.0002 (0.0002)  time: 0.1972  data: 0.0002  max mem: 5511
[09:17:07.039016] Epoch: [67]  [200/781]  eta: 0:01:56  lr: 0.000067  training_loss: 1.4817 (1.4792)  mae_loss: 0.2579 (0.2501)  classification_loss: 1.2133 (1.2290)  loss_mask: 0.0001 (0.0002)  time: 0.1992  data: 0.0006  max mem: 5511
[09:17:10.992519] Epoch: [67]  [220/781]  eta: 0:01:52  lr: 0.000067  training_loss: 1.5163 (1.4830)  mae_loss: 0.2536 (0.2506)  classification_loss: 1.2569 (1.2322)  loss_mask: 0.0001 (0.0002)  time: 0.1976  data: 0.0002  max mem: 5511
[09:17:14.947178] Epoch: [67]  [240/781]  eta: 0:01:48  lr: 0.000067  training_loss: 1.4646 (1.4837)  mae_loss: 0.2529 (0.2511)  classification_loss: 1.2100 (1.2324)  loss_mask: 0.0001 (0.0001)  time: 0.1976  data: 0.0002  max mem: 5511
[09:17:18.905079] Epoch: [67]  [260/781]  eta: 0:01:44  lr: 0.000067  training_loss: 1.4635 (1.4822)  mae_loss: 0.2463 (0.2506)  classification_loss: 1.2013 (1.2315)  loss_mask: 0.0001 (0.0001)  time: 0.1978  data: 0.0002  max mem: 5511
[09:17:22.857661] Epoch: [67]  [280/781]  eta: 0:01:40  lr: 0.000067  training_loss: 1.5039 (1.4839)  mae_loss: 0.2592 (0.2513)  classification_loss: 1.2418 (1.2324)  loss_mask: 0.0001 (0.0001)  time: 0.1975  data: 0.0002  max mem: 5511
[09:17:26.801837] Epoch: [67]  [300/781]  eta: 0:01:36  lr: 0.000067  training_loss: 1.4787 (1.4850)  mae_loss: 0.2382 (0.2515)  classification_loss: 1.2411 (1.2334)  loss_mask: 0.0001 (0.0001)  time: 0.1971  data: 0.0003  max mem: 5511
[09:17:30.729325] Epoch: [67]  [320/781]  eta: 0:01:31  lr: 0.000067  training_loss: 1.4911 (1.4855)  mae_loss: 0.2560 (0.2517)  classification_loss: 1.2155 (1.2336)  loss_mask: 0.0001 (0.0001)  time: 0.1963  data: 0.0002  max mem: 5511
[09:17:34.677345] Epoch: [67]  [340/781]  eta: 0:01:27  lr: 0.000066  training_loss: 1.5191 (1.4874)  mae_loss: 0.2608 (0.2525)  classification_loss: 1.2657 (1.2348)  loss_mask: 0.0001 (0.0001)  time: 0.1973  data: 0.0003  max mem: 5511
[09:17:38.608369] Epoch: [67]  [360/781]  eta: 0:01:23  lr: 0.000066  training_loss: 1.4494 (1.4845)  mae_loss: 0.2423 (0.2523)  classification_loss: 1.1866 (1.2320)  loss_mask: 0.0001 (0.0001)  time: 0.1965  data: 0.0002  max mem: 5511
[09:17:42.538049] Epoch: [67]  [380/781]  eta: 0:01:19  lr: 0.000066  training_loss: 1.4627 (1.4845)  mae_loss: 0.2511 (0.2526)  classification_loss: 1.2038 (1.2318)  loss_mask: 0.0001 (0.0001)  time: 0.1964  data: 0.0002  max mem: 5511
[09:17:46.486278] Epoch: [67]  [400/781]  eta: 0:01:15  lr: 0.000066  training_loss: 1.5193 (1.4858)  mae_loss: 0.2489 (0.2527)  classification_loss: 1.2439 (1.2329)  loss_mask: 0.0001 (0.0001)  time: 0.1973  data: 0.0002  max mem: 5511
[09:17:50.460698] Epoch: [67]  [420/781]  eta: 0:01:11  lr: 0.000066  training_loss: 1.4399 (1.4846)  mae_loss: 0.2519 (0.2530)  classification_loss: 1.2016 (1.2315)  loss_mask: 0.0001 (0.0001)  time: 0.1986  data: 0.0002  max mem: 5511
[09:17:54.399880] Epoch: [67]  [440/781]  eta: 0:01:07  lr: 0.000066  training_loss: 1.4843 (1.4842)  mae_loss: 0.2523 (0.2532)  classification_loss: 1.2342 (1.2309)  loss_mask: 0.0001 (0.0001)  time: 0.1968  data: 0.0002  max mem: 5511
[09:17:58.349647] Epoch: [67]  [460/781]  eta: 0:01:03  lr: 0.000066  training_loss: 1.4702 (1.4836)  mae_loss: 0.2327 (0.2523)  classification_loss: 1.2082 (1.2311)  loss_mask: 0.0001 (0.0001)  time: 0.1974  data: 0.0002  max mem: 5511
[09:18:02.312246] Epoch: [67]  [480/781]  eta: 0:00:59  lr: 0.000066  training_loss: 1.4908 (1.4845)  mae_loss: 0.2473 (0.2523)  classification_loss: 1.2345 (1.2321)  loss_mask: 0.0001 (0.0001)  time: 0.1981  data: 0.0002  max mem: 5511
[09:18:06.241054] Epoch: [67]  [500/781]  eta: 0:00:55  lr: 0.000066  training_loss: 1.4763 (1.4849)  mae_loss: 0.2553 (0.2525)  classification_loss: 1.2309 (1.2323)  loss_mask: 0.0001 (0.0001)  time: 0.1963  data: 0.0002  max mem: 5511
[09:18:10.173115] Epoch: [67]  [520/781]  eta: 0:00:51  lr: 0.000066  training_loss: 1.4662 (1.4848)  mae_loss: 0.2578 (0.2527)  classification_loss: 1.2171 (1.2320)  loss_mask: 0.0001 (0.0001)  time: 0.1965  data: 0.0002  max mem: 5511
[09:18:14.122569] Epoch: [67]  [540/781]  eta: 0:00:47  lr: 0.000066  training_loss: 1.4947 (1.4856)  mae_loss: 0.2382 (0.2524)  classification_loss: 1.2591 (1.2331)  loss_mask: 0.0001 (0.0001)  time: 0.1974  data: 0.0002  max mem: 5511
[09:18:18.051944] Epoch: [67]  [560/781]  eta: 0:00:43  lr: 0.000065  training_loss: 1.4776 (1.4853)  mae_loss: 0.2515 (0.2526)  classification_loss: 1.2229 (1.2326)  loss_mask: 0.0001 (0.0001)  time: 0.1964  data: 0.0002  max mem: 5511
[09:18:22.052544] Epoch: [67]  [580/781]  eta: 0:00:39  lr: 0.000065  training_loss: 1.4643 (1.4847)  mae_loss: 0.2650 (0.2530)  classification_loss: 1.1869 (1.2315)  loss_mask: 0.0001 (0.0001)  time: 0.1999  data: 0.0002  max mem: 5511
[09:18:26.021193] Epoch: [67]  [600/781]  eta: 0:00:35  lr: 0.000065  training_loss: 1.4409 (1.4841)  mae_loss: 0.2484 (0.2532)  classification_loss: 1.1811 (1.2309)  loss_mask: 0.0001 (0.0001)  time: 0.1983  data: 0.0002  max mem: 5511
[09:18:29.979046] Epoch: [67]  [620/781]  eta: 0:00:31  lr: 0.000065  training_loss: 1.4706 (1.4840)  mae_loss: 0.2592 (0.2533)  classification_loss: 1.2103 (1.2306)  loss_mask: 0.0001 (0.0001)  time: 0.1978  data: 0.0003  max mem: 5511
[09:18:33.947536] Epoch: [67]  [640/781]  eta: 0:00:27  lr: 0.000065  training_loss: 1.4741 (1.4846)  mae_loss: 0.2626 (0.2536)  classification_loss: 1.2063 (1.2309)  loss_mask: 0.0001 (0.0001)  time: 0.1983  data: 0.0002  max mem: 5511
[09:18:37.877342] Epoch: [67]  [660/781]  eta: 0:00:24  lr: 0.000065  training_loss: 1.5105 (1.4850)  mae_loss: 0.2504 (0.2539)  classification_loss: 1.2506 (1.2311)  loss_mask: 0.0001 (0.0001)  time: 0.1964  data: 0.0003  max mem: 5511
[09:18:41.822849] Epoch: [67]  [680/781]  eta: 0:00:20  lr: 0.000065  training_loss: 1.4802 (1.4851)  mae_loss: 0.2388 (0.2536)  classification_loss: 1.2460 (1.2313)  loss_mask: 0.0001 (0.0001)  time: 0.1972  data: 0.0003  max mem: 5511
[09:18:45.787033] Epoch: [67]  [700/781]  eta: 0:00:16  lr: 0.000065  training_loss: 1.5270 (1.4866)  mae_loss: 0.2609 (0.2538)  classification_loss: 1.2772 (1.2326)  loss_mask: 0.0004 (0.0002)  time: 0.1981  data: 0.0002  max mem: 5511
[09:18:49.735920] Epoch: [67]  [720/781]  eta: 0:00:12  lr: 0.000065  training_loss: 1.4821 (1.4866)  mae_loss: 0.2539 (0.2539)  classification_loss: 1.2239 (1.2326)  loss_mask: 0.0003 (0.0002)  time: 0.1973  data: 0.0003  max mem: 5511
[09:18:53.665056] Epoch: [67]  [740/781]  eta: 0:00:08  lr: 0.000065  training_loss: 1.4390 (1.4861)  mae_loss: 0.2241 (0.2535)  classification_loss: 1.2071 (1.2325)  loss_mask: 0.0001 (0.0002)  time: 0.1964  data: 0.0002  max mem: 5511
[09:18:57.606404] Epoch: [67]  [760/781]  eta: 0:00:04  lr: 0.000065  training_loss: 1.5067 (1.4868)  mae_loss: 0.2441 (0.2532)  classification_loss: 1.2667 (1.2334)  loss_mask: 0.0001 (0.0002)  time: 0.1970  data: 0.0003  max mem: 5511
[09:19:01.554511] Epoch: [67]  [780/781]  eta: 0:00:00  lr: 0.000064  training_loss: 1.5112 (1.4874)  mae_loss: 0.2585 (0.2537)  classification_loss: 1.2589 (1.2336)  loss_mask: 0.0001 (0.0002)  time: 0.1973  data: 0.0002  max mem: 5511
[09:19:01.734136] Epoch: [67] Total time: 0:02:35 (0.1985 s / it)
[09:19:01.734605] Averaged stats: lr: 0.000064  training_loss: 1.5112 (1.4874)  mae_loss: 0.2585 (0.2537)  classification_loss: 1.2589 (1.2336)  loss_mask: 0.0001 (0.0002)
[09:19:02.434608] Test:  [  0/157]  eta: 0:01:49  testing_loss: 0.4206 (0.4206)  acc1: 92.1875 (92.1875)  acc5: 98.4375 (98.4375)  time: 0.6954  data: 0.6661  max mem: 5511
[09:19:02.723095] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.5156 (0.5178)  acc1: 82.8125 (84.2330)  acc5: 100.0000 (99.4318)  time: 0.0891  data: 0.0608  max mem: 5511
[09:19:03.009345] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4979 (0.4937)  acc1: 84.3750 (85.2679)  acc5: 100.0000 (99.5536)  time: 0.0284  data: 0.0002  max mem: 5511
[09:19:03.298354] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4945 (0.5129)  acc1: 84.3750 (84.4254)  acc5: 100.0000 (99.4456)  time: 0.0285  data: 0.0002  max mem: 5511
[09:19:03.587291] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5135 (0.5214)  acc1: 84.3750 (84.0320)  acc5: 100.0000 (99.3902)  time: 0.0286  data: 0.0002  max mem: 5511
[09:19:03.878070] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5135 (0.5167)  acc1: 84.3750 (84.3750)  acc5: 100.0000 (99.3873)  time: 0.0288  data: 0.0002  max mem: 5511
[09:19:04.163184] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4909 (0.5134)  acc1: 84.3750 (84.5031)  acc5: 100.0000 (99.3852)  time: 0.0286  data: 0.0002  max mem: 5511
[09:19:04.450959] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5004 (0.5130)  acc1: 84.3750 (84.5070)  acc5: 100.0000 (99.3178)  time: 0.0285  data: 0.0002  max mem: 5511
[09:19:04.741796] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5204 (0.5218)  acc1: 84.3750 (84.2207)  acc5: 98.4375 (99.2863)  time: 0.0288  data: 0.0002  max mem: 5511
[09:19:05.028458] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5080 (0.5171)  acc1: 84.3750 (84.4093)  acc5: 100.0000 (99.2788)  time: 0.0287  data: 0.0002  max mem: 5511
[09:19:05.328506] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.5073 (0.5186)  acc1: 84.3750 (84.4524)  acc5: 100.0000 (99.2729)  time: 0.0292  data: 0.0002  max mem: 5511
[09:19:05.615081] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5270 (0.5217)  acc1: 82.8125 (84.2624)  acc5: 100.0000 (99.2680)  time: 0.0292  data: 0.0002  max mem: 5511
[09:19:05.904596] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5265 (0.5186)  acc1: 82.8125 (84.3233)  acc5: 100.0000 (99.3027)  time: 0.0287  data: 0.0002  max mem: 5511
[09:19:06.200181] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5133 (0.5193)  acc1: 84.3750 (84.2677)  acc5: 100.0000 (99.3201)  time: 0.0291  data: 0.0002  max mem: 5511
[09:19:06.489917] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5025 (0.5166)  acc1: 85.9375 (84.4969)  acc5: 100.0000 (99.3240)  time: 0.0291  data: 0.0002  max mem: 5511
[09:19:06.777335] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5023 (0.5150)  acc1: 85.9375 (84.6026)  acc5: 100.0000 (99.3171)  time: 0.0286  data: 0.0002  max mem: 5511
[09:19:06.932715] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4711 (0.5147)  acc1: 85.9375 (84.5900)  acc5: 100.0000 (99.3200)  time: 0.0276  data: 0.0002  max mem: 5511
[09:19:07.113697] Test: Total time: 0:00:05 (0.0342 s / it)
[09:19:07.114349] * Acc@1 84.590 Acc@5 99.320 loss 0.515
[09:19:07.114658] Accuracy of the network on the 10000 test images: 84.6%
[09:19:07.114933] Max accuracy: 84.59%
[09:19:07.498386] log_dir: ./output_dir
[09:19:08.367570] Epoch: [68]  [  0/781]  eta: 0:11:17  lr: 0.000064  training_loss: 1.3605 (1.3605)  mae_loss: 0.2399 (0.2399)  classification_loss: 1.1204 (1.1204)  loss_mask: 0.0001 (0.0001)  time: 0.8672  data: 0.6510  max mem: 5511
[09:19:12.316683] Epoch: [68]  [ 20/781]  eta: 0:02:54  lr: 0.000064  training_loss: 1.4733 (1.4825)  mae_loss: 0.2560 (0.2518)  classification_loss: 1.2087 (1.2250)  loss_mask: 0.0001 (0.0056)  time: 0.1974  data: 0.0001  max mem: 5511
[09:19:16.271028] Epoch: [68]  [ 40/781]  eta: 0:02:38  lr: 0.000064  training_loss: 1.4833 (1.4889)  mae_loss: 0.2620 (0.2539)  classification_loss: 1.2158 (1.2252)  loss_mask: 0.0007 (0.0098)  time: 0.1976  data: 0.0002  max mem: 5511
[09:19:20.223048] Epoch: [68]  [ 60/781]  eta: 0:02:30  lr: 0.000064  training_loss: 1.7085 (1.5614)  mae_loss: 0.2527 (0.2566)  classification_loss: 1.3067 (1.2496)  loss_mask: 0.0865 (0.0552)  time: 0.1975  data: 0.0002  max mem: 5511
[09:19:24.149946] Epoch: [68]  [ 80/781]  eta: 0:02:24  lr: 0.000064  training_loss: 1.5380 (1.5552)  mae_loss: 0.2717 (0.2591)  classification_loss: 1.2413 (1.2456)  loss_mask: 0.0262 (0.0504)  time: 0.1963  data: 0.0003  max mem: 5511
[09:19:28.088422] Epoch: [68]  [100/781]  eta: 0:02:18  lr: 0.000064  training_loss: 1.4794 (1.5444)  mae_loss: 0.2584 (0.2597)  classification_loss: 1.2425 (1.2413)  loss_mask: 0.0144 (0.0434)  time: 0.1968  data: 0.0003  max mem: 5511
[09:19:32.041982] Epoch: [68]  [120/781]  eta: 0:02:14  lr: 0.000064  training_loss: 1.5284 (1.5398)  mae_loss: 0.2447 (0.2578)  classification_loss: 1.2436 (1.2444)  loss_mask: 0.0075 (0.0377)  time: 0.1976  data: 0.0002  max mem: 5511
[09:19:36.030754] Epoch: [68]  [140/781]  eta: 0:02:09  lr: 0.000064  training_loss: 1.5240 (1.5386)  mae_loss: 0.2624 (0.2581)  classification_loss: 1.2547 (1.2474)  loss_mask: 0.0039 (0.0330)  time: 0.1994  data: 0.0003  max mem: 5511
[09:19:40.000265] Epoch: [68]  [160/781]  eta: 0:02:05  lr: 0.000064  training_loss: 1.4918 (1.5335)  mae_loss: 0.2462 (0.2572)  classification_loss: 1.2354 (1.2471)  loss_mask: 0.0017 (0.0292)  time: 0.1984  data: 0.0002  max mem: 5511
[09:19:43.929961] Epoch: [68]  [180/781]  eta: 0:02:00  lr: 0.000064  training_loss: 1.5092 (1.5304)  mae_loss: 0.2568 (0.2583)  classification_loss: 1.2188 (1.2453)  loss_mask: 0.0018 (0.0267)  time: 0.1964  data: 0.0002  max mem: 5511
[09:19:47.912474] Epoch: [68]  [200/781]  eta: 0:01:56  lr: 0.000064  training_loss: 1.4693 (1.5261)  mae_loss: 0.2390 (0.2568)  classification_loss: 1.2131 (1.2447)  loss_mask: 0.0021 (0.0245)  time: 0.1990  data: 0.0003  max mem: 5511
[09:19:51.851221] Epoch: [68]  [220/781]  eta: 0:01:52  lr: 0.000063  training_loss: 1.4687 (1.5202)  mae_loss: 0.2371 (0.2559)  classification_loss: 1.2357 (1.2419)  loss_mask: 0.0008 (0.0224)  time: 0.1968  data: 0.0003  max mem: 5511
[09:19:55.782727] Epoch: [68]  [240/781]  eta: 0:01:48  lr: 0.000063  training_loss: 1.4585 (1.5169)  mae_loss: 0.2579 (0.2559)  classification_loss: 1.2058 (1.2403)  loss_mask: 0.0005 (0.0207)  time: 0.1965  data: 0.0002  max mem: 5511
[09:19:59.747183] Epoch: [68]  [260/781]  eta: 0:01:44  lr: 0.000063  training_loss: 1.4873 (1.5150)  mae_loss: 0.2529 (0.2559)  classification_loss: 1.2105 (1.2394)  loss_mask: 0.0009 (0.0197)  time: 0.1981  data: 0.0003  max mem: 5511
[09:20:03.752286] Epoch: [68]  [280/781]  eta: 0:01:40  lr: 0.000063  training_loss: 1.5360 (1.5162)  mae_loss: 0.2574 (0.2560)  classification_loss: 1.2817 (1.2407)  loss_mask: 0.0032 (0.0195)  time: 0.2002  data: 0.0002  max mem: 5511
[09:20:07.690039] Epoch: [68]  [300/781]  eta: 0:01:36  lr: 0.000063  training_loss: 1.4880 (1.5161)  mae_loss: 0.2674 (0.2568)  classification_loss: 1.1828 (1.2391)  loss_mask: 0.0058 (0.0201)  time: 0.1968  data: 0.0003  max mem: 5511
[09:20:11.617169] Epoch: [68]  [320/781]  eta: 0:01:32  lr: 0.000063  training_loss: 1.4579 (1.5139)  mae_loss: 0.2490 (0.2563)  classification_loss: 1.1958 (1.2373)  loss_mask: 0.0153 (0.0203)  time: 0.1963  data: 0.0004  max mem: 5511
[09:20:15.544302] Epoch: [68]  [340/781]  eta: 0:01:27  lr: 0.000063  training_loss: 1.4501 (1.5102)  mae_loss: 0.2497 (0.2560)  classification_loss: 1.2065 (1.2346)  loss_mask: 0.0033 (0.0196)  time: 0.1963  data: 0.0003  max mem: 5511
[09:20:19.527556] Epoch: [68]  [360/781]  eta: 0:01:23  lr: 0.000063  training_loss: 1.4957 (1.5107)  mae_loss: 0.2445 (0.2555)  classification_loss: 1.2499 (1.2363)  loss_mask: 0.0014 (0.0189)  time: 0.1991  data: 0.0004  max mem: 5511
[09:20:23.455558] Epoch: [68]  [380/781]  eta: 0:01:19  lr: 0.000063  training_loss: 1.5045 (1.5103)  mae_loss: 0.2629 (0.2561)  classification_loss: 1.2257 (1.2362)  loss_mask: 0.0009 (0.0180)  time: 0.1963  data: 0.0002  max mem: 5511
[09:20:27.405377] Epoch: [68]  [400/781]  eta: 0:01:15  lr: 0.000063  training_loss: 1.5016 (1.5102)  mae_loss: 0.2615 (0.2565)  classification_loss: 1.2308 (1.2366)  loss_mask: 0.0012 (0.0172)  time: 0.1974  data: 0.0002  max mem: 5511
[09:20:31.337171] Epoch: [68]  [420/781]  eta: 0:01:11  lr: 0.000063  training_loss: 1.5400 (1.5115)  mae_loss: 0.2585 (0.2568)  classification_loss: 1.2757 (1.2384)  loss_mask: 0.0005 (0.0164)  time: 0.1965  data: 0.0002  max mem: 5511
[09:20:35.316942] Epoch: [68]  [440/781]  eta: 0:01:07  lr: 0.000062  training_loss: 1.5015 (1.5103)  mae_loss: 0.2543 (0.2566)  classification_loss: 1.2205 (1.2381)  loss_mask: 0.0003 (0.0157)  time: 0.1989  data: 0.0002  max mem: 5511
[09:20:39.273602] Epoch: [68]  [460/781]  eta: 0:01:03  lr: 0.000062  training_loss: 1.4238 (1.5080)  mae_loss: 0.2503 (0.2562)  classification_loss: 1.1901 (1.2368)  loss_mask: 0.0002 (0.0150)  time: 0.1977  data: 0.0003  max mem: 5511
[09:20:43.231969] Epoch: [68]  [480/781]  eta: 0:00:59  lr: 0.000062  training_loss: 1.4361 (1.5055)  mae_loss: 0.2350 (0.2557)  classification_loss: 1.2156 (1.2355)  loss_mask: 0.0003 (0.0144)  time: 0.1978  data: 0.0003  max mem: 5511
[09:20:47.210454] Epoch: [68]  [500/781]  eta: 0:00:55  lr: 0.000062  training_loss: 1.4419 (1.5039)  mae_loss: 0.2576 (0.2558)  classification_loss: 1.2034 (1.2343)  loss_mask: 0.0002 (0.0138)  time: 0.1988  data: 0.0002  max mem: 5511
[09:20:51.182048] Epoch: [68]  [520/781]  eta: 0:00:51  lr: 0.000062  training_loss: 1.4528 (1.5027)  mae_loss: 0.2578 (0.2560)  classification_loss: 1.1947 (1.2334)  loss_mask: 0.0002 (0.0133)  time: 0.1985  data: 0.0002  max mem: 5511
[09:20:55.124766] Epoch: [68]  [540/781]  eta: 0:00:47  lr: 0.000062  training_loss: 1.4869 (1.5030)  mae_loss: 0.2509 (0.2561)  classification_loss: 1.2407 (1.2341)  loss_mask: 0.0002 (0.0128)  time: 0.1971  data: 0.0002  max mem: 5511
[09:20:59.087917] Epoch: [68]  [560/781]  eta: 0:00:43  lr: 0.000062  training_loss: 1.5143 (1.5035)  mae_loss: 0.2597 (0.2562)  classification_loss: 1.2457 (1.2350)  loss_mask: 0.0003 (0.0124)  time: 0.1981  data: 0.0002  max mem: 5511
[09:21:03.030193] Epoch: [68]  [580/781]  eta: 0:00:39  lr: 0.000062  training_loss: 1.4664 (1.5025)  mae_loss: 0.2655 (0.2562)  classification_loss: 1.2186 (1.2343)  loss_mask: 0.0002 (0.0120)  time: 0.1970  data: 0.0002  max mem: 5511
[09:21:06.983013] Epoch: [68]  [600/781]  eta: 0:00:35  lr: 0.000062  training_loss: 1.4927 (1.5025)  mae_loss: 0.2504 (0.2564)  classification_loss: 1.2141 (1.2345)  loss_mask: 0.0002 (0.0116)  time: 0.1975  data: 0.0002  max mem: 5511
[09:21:10.937506] Epoch: [68]  [620/781]  eta: 0:00:31  lr: 0.000062  training_loss: 1.4209 (1.5011)  mae_loss: 0.2545 (0.2564)  classification_loss: 1.1602 (1.2335)  loss_mask: 0.0002 (0.0112)  time: 0.1976  data: 0.0002  max mem: 5511
[09:21:14.913556] Epoch: [68]  [640/781]  eta: 0:00:28  lr: 0.000062  training_loss: 1.4323 (1.4994)  mae_loss: 0.2408 (0.2560)  classification_loss: 1.1866 (1.2325)  loss_mask: 0.0002 (0.0109)  time: 0.1987  data: 0.0002  max mem: 5511
[09:21:18.893522] Epoch: [68]  [660/781]  eta: 0:00:24  lr: 0.000061  training_loss: 1.4870 (1.4993)  mae_loss: 0.2577 (0.2562)  classification_loss: 1.2301 (1.2325)  loss_mask: 0.0001 (0.0105)  time: 0.1989  data: 0.0002  max mem: 5511
[09:21:22.830332] Epoch: [68]  [680/781]  eta: 0:00:20  lr: 0.000061  training_loss: 1.4548 (1.4983)  mae_loss: 0.2523 (0.2562)  classification_loss: 1.1880 (1.2318)  loss_mask: 0.0001 (0.0102)  time: 0.1968  data: 0.0002  max mem: 5511
[09:21:26.786157] Epoch: [68]  [700/781]  eta: 0:00:16  lr: 0.000061  training_loss: 1.4673 (1.4981)  mae_loss: 0.2590 (0.2564)  classification_loss: 1.2088 (1.2317)  loss_mask: 0.0001 (0.0100)  time: 0.1977  data: 0.0002  max mem: 5511
[09:21:30.728698] Epoch: [68]  [720/781]  eta: 0:00:12  lr: 0.000061  training_loss: 1.4913 (1.4984)  mae_loss: 0.2550 (0.2564)  classification_loss: 1.2451 (1.2323)  loss_mask: 0.0001 (0.0097)  time: 0.1971  data: 0.0002  max mem: 5511
[09:21:34.678467] Epoch: [68]  [740/781]  eta: 0:00:08  lr: 0.000061  training_loss: 1.4851 (1.4976)  mae_loss: 0.2475 (0.2562)  classification_loss: 1.2411 (1.2320)  loss_mask: 0.0001 (0.0094)  time: 0.1974  data: 0.0003  max mem: 5511
[09:21:38.635512] Epoch: [68]  [760/781]  eta: 0:00:04  lr: 0.000061  training_loss: 1.4978 (1.4983)  mae_loss: 0.2526 (0.2562)  classification_loss: 1.2452 (1.2329)  loss_mask: 0.0001 (0.0092)  time: 0.1977  data: 0.0002  max mem: 5511
[09:21:42.564583] Epoch: [68]  [780/781]  eta: 0:00:00  lr: 0.000061  training_loss: 1.4454 (1.4973)  mae_loss: 0.2431 (0.2562)  classification_loss: 1.1893 (1.2321)  loss_mask: 0.0001 (0.0090)  time: 0.1963  data: 0.0002  max mem: 5511
[09:21:42.744302] Epoch: [68] Total time: 0:02:35 (0.1988 s / it)
[09:21:42.744773] Averaged stats: lr: 0.000061  training_loss: 1.4454 (1.4973)  mae_loss: 0.2431 (0.2562)  classification_loss: 1.1893 (1.2321)  loss_mask: 0.0001 (0.0090)
[09:21:43.385327] Test:  [  0/157]  eta: 0:01:39  testing_loss: 0.4622 (0.4622)  acc1: 92.1875 (92.1875)  acc5: 98.4375 (98.4375)  time: 0.6348  data: 0.5998  max mem: 5511
[09:21:43.672971] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5395 (0.5450)  acc1: 81.2500 (83.0966)  acc5: 100.0000 (99.2898)  time: 0.0836  data: 0.0547  max mem: 5511
[09:21:43.957804] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.5047 (0.5197)  acc1: 82.8125 (84.2262)  acc5: 100.0000 (99.4048)  time: 0.0284  data: 0.0002  max mem: 5511
[09:21:44.243594] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4886 (0.5337)  acc1: 84.3750 (83.7198)  acc5: 100.0000 (99.2944)  time: 0.0284  data: 0.0002  max mem: 5511
[09:21:44.530408] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5176 (0.5436)  acc1: 82.8125 (83.4223)  acc5: 100.0000 (99.1997)  time: 0.0285  data: 0.0002  max mem: 5511
[09:21:44.816531] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5176 (0.5391)  acc1: 82.8125 (83.4559)  acc5: 100.0000 (99.2034)  time: 0.0285  data: 0.0002  max mem: 5511
[09:21:45.100287] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5124 (0.5358)  acc1: 82.8125 (83.2223)  acc5: 100.0000 (99.2059)  time: 0.0284  data: 0.0002  max mem: 5511
[09:21:45.384771] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5146 (0.5334)  acc1: 82.8125 (83.3627)  acc5: 100.0000 (99.1857)  time: 0.0283  data: 0.0002  max mem: 5511
[09:21:45.670711] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5410 (0.5391)  acc1: 82.8125 (83.1404)  acc5: 98.4375 (99.1512)  time: 0.0284  data: 0.0002  max mem: 5511
[09:21:45.954757] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5371 (0.5350)  acc1: 84.3750 (83.3791)  acc5: 100.0000 (99.1587)  time: 0.0284  data: 0.0002  max mem: 5511
[09:21:46.237730] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5200 (0.5372)  acc1: 84.3750 (83.3385)  acc5: 100.0000 (99.1646)  time: 0.0282  data: 0.0002  max mem: 5511
[09:21:46.519829] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5566 (0.5395)  acc1: 84.3750 (83.2911)  acc5: 100.0000 (99.1554)  time: 0.0281  data: 0.0002  max mem: 5511
[09:21:46.804702] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5329 (0.5381)  acc1: 84.3750 (83.2645)  acc5: 100.0000 (99.1477)  time: 0.0282  data: 0.0002  max mem: 5511
[09:21:47.094108] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5306 (0.5390)  acc1: 81.2500 (83.2538)  acc5: 100.0000 (99.1651)  time: 0.0286  data: 0.0002  max mem: 5511
[09:21:47.375692] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5016 (0.5366)  acc1: 84.3750 (83.3887)  acc5: 100.0000 (99.2021)  time: 0.0284  data: 0.0002  max mem: 5511
[09:21:47.657783] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5127 (0.5350)  acc1: 84.3750 (83.4644)  acc5: 100.0000 (99.2032)  time: 0.0281  data: 0.0001  max mem: 5511
[09:21:47.809680] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5012 (0.5348)  acc1: 84.3750 (83.4300)  acc5: 100.0000 (99.2100)  time: 0.0272  data: 0.0001  max mem: 5511
[09:21:47.968495] Test: Total time: 0:00:05 (0.0332 s / it)
[09:21:47.969548] * Acc@1 83.430 Acc@5 99.210 loss 0.535
[09:21:47.970006] Accuracy of the network on the 10000 test images: 83.4%
[09:21:47.970199] Max accuracy: 84.59%
[09:21:48.122362] log_dir: ./output_dir
[09:21:48.940166] Epoch: [69]  [  0/781]  eta: 0:10:37  lr: 0.000061  training_loss: 1.4026 (1.4026)  mae_loss: 0.2338 (0.2338)  classification_loss: 1.1685 (1.1685)  loss_mask: 0.0002 (0.0002)  time: 0.8160  data: 0.6037  max mem: 5511
[09:21:52.872547] Epoch: [69]  [ 20/781]  eta: 0:02:51  lr: 0.000061  training_loss: 1.4668 (1.4741)  mae_loss: 0.2586 (0.2546)  classification_loss: 1.2215 (1.2192)  loss_mask: 0.0002 (0.0003)  time: 0.1965  data: 0.0002  max mem: 5511
[09:21:56.803399] Epoch: [69]  [ 40/781]  eta: 0:02:36  lr: 0.000061  training_loss: 1.4988 (1.4809)  mae_loss: 0.2370 (0.2505)  classification_loss: 1.2164 (1.2302)  loss_mask: 0.0001 (0.0002)  time: 0.1965  data: 0.0002  max mem: 5511
[09:22:00.744226] Epoch: [69]  [ 60/781]  eta: 0:02:29  lr: 0.000061  training_loss: 1.4905 (1.4948)  mae_loss: 0.2532 (0.2522)  classification_loss: 1.2324 (1.2425)  loss_mask: 0.0001 (0.0002)  time: 0.1970  data: 0.0003  max mem: 5511
[09:22:04.712792] Epoch: [69]  [ 80/781]  eta: 0:02:23  lr: 0.000061  training_loss: 1.4443 (1.4904)  mae_loss: 0.2584 (0.2544)  classification_loss: 1.1792 (1.2358)  loss_mask: 0.0001 (0.0002)  time: 0.1983  data: 0.0002  max mem: 5511
[09:22:08.655420] Epoch: [69]  [100/781]  eta: 0:02:18  lr: 0.000060  training_loss: 1.4644 (1.4896)  mae_loss: 0.2497 (0.2546)  classification_loss: 1.2222 (1.2348)  loss_mask: 0.0001 (0.0002)  time: 0.1970  data: 0.0003  max mem: 5511
[09:22:12.601915] Epoch: [69]  [120/781]  eta: 0:02:13  lr: 0.000060  training_loss: 1.4876 (1.4901)  mae_loss: 0.2481 (0.2544)  classification_loss: 1.2400 (1.2346)  loss_mask: 0.0002 (0.0012)  time: 0.1972  data: 0.0002  max mem: 5511
[09:22:16.547841] Epoch: [69]  [140/781]  eta: 0:02:09  lr: 0.000060  training_loss: 1.4393 (1.4840)  mae_loss: 0.2475 (0.2533)  classification_loss: 1.1987 (1.2297)  loss_mask: 0.0002 (0.0011)  time: 0.1972  data: 0.0002  max mem: 5511
[09:22:20.491503] Epoch: [69]  [160/781]  eta: 0:02:04  lr: 0.000060  training_loss: 1.4629 (1.4802)  mae_loss: 0.2409 (0.2525)  classification_loss: 1.2091 (1.2268)  loss_mask: 0.0002 (0.0010)  time: 0.1971  data: 0.0002  max mem: 5511
[09:22:24.474566] Epoch: [69]  [180/781]  eta: 0:02:00  lr: 0.000060  training_loss: 1.4796 (1.4805)  mae_loss: 0.2394 (0.2526)  classification_loss: 1.2376 (1.2270)  loss_mask: 0.0001 (0.0009)  time: 0.1990  data: 0.0003  max mem: 5511
[09:22:28.413041] Epoch: [69]  [200/781]  eta: 0:01:56  lr: 0.000060  training_loss: 1.4414 (1.4792)  mae_loss: 0.2446 (0.2516)  classification_loss: 1.2113 (1.2267)  loss_mask: 0.0001 (0.0008)  time: 0.1968  data: 0.0002  max mem: 5511
[09:22:32.377980] Epoch: [69]  [220/781]  eta: 0:01:52  lr: 0.000060  training_loss: 1.4679 (1.4776)  mae_loss: 0.2423 (0.2509)  classification_loss: 1.2364 (1.2260)  loss_mask: 0.0001 (0.0008)  time: 0.1982  data: 0.0002  max mem: 5511
[09:22:36.307513] Epoch: [69]  [240/781]  eta: 0:01:48  lr: 0.000060  training_loss: 1.4785 (1.4781)  mae_loss: 0.2605 (0.2515)  classification_loss: 1.2324 (1.2258)  loss_mask: 0.0001 (0.0008)  time: 0.1964  data: 0.0003  max mem: 5511
[09:22:40.296629] Epoch: [69]  [260/781]  eta: 0:01:44  lr: 0.000060  training_loss: 1.4601 (1.4791)  mae_loss: 0.2436 (0.2516)  classification_loss: 1.1925 (1.2261)  loss_mask: 0.0001 (0.0014)  time: 0.1993  data: 0.0003  max mem: 5511
[09:22:44.233877] Epoch: [69]  [280/781]  eta: 0:01:39  lr: 0.000060  training_loss: 1.4866 (1.4802)  mae_loss: 0.2582 (0.2522)  classification_loss: 1.2383 (1.2265)  loss_mask: 0.0004 (0.0014)  time: 0.1967  data: 0.0002  max mem: 5511
[09:22:48.182549] Epoch: [69]  [300/781]  eta: 0:01:35  lr: 0.000060  training_loss: 1.4557 (1.4789)  mae_loss: 0.2463 (0.2524)  classification_loss: 1.1913 (1.2248)  loss_mask: 0.0003 (0.0017)  time: 0.1974  data: 0.0004  max mem: 5511
[09:22:52.127978] Epoch: [69]  [320/781]  eta: 0:01:31  lr: 0.000059  training_loss: 1.4553 (1.4780)  mae_loss: 0.2491 (0.2524)  classification_loss: 1.2065 (1.2240)  loss_mask: 0.0002 (0.0017)  time: 0.1972  data: 0.0002  max mem: 5511
[09:22:56.089960] Epoch: [69]  [340/781]  eta: 0:01:27  lr: 0.000059  training_loss: 1.4310 (1.4760)  mae_loss: 0.2484 (0.2523)  classification_loss: 1.1569 (1.2220)  loss_mask: 0.0004 (0.0017)  time: 0.1980  data: 0.0002  max mem: 5511
[09:23:00.054288] Epoch: [69]  [360/781]  eta: 0:01:23  lr: 0.000059  training_loss: 1.4648 (1.4778)  mae_loss: 0.2483 (0.2518)  classification_loss: 1.2470 (1.2244)  loss_mask: 0.0003 (0.0017)  time: 0.1981  data: 0.0004  max mem: 5511
[09:23:04.011463] Epoch: [69]  [380/781]  eta: 0:01:19  lr: 0.000059  training_loss: 1.4846 (1.4811)  mae_loss: 0.2526 (0.2522)  classification_loss: 1.2320 (1.2273)  loss_mask: 0.0001 (0.0016)  time: 0.1978  data: 0.0002  max mem: 5511
[09:23:07.936262] Epoch: [69]  [400/781]  eta: 0:01:15  lr: 0.000059  training_loss: 1.5007 (1.4824)  mae_loss: 0.2520 (0.2525)  classification_loss: 1.2293 (1.2277)  loss_mask: 0.0002 (0.0022)  time: 0.1961  data: 0.0002  max mem: 5511
[09:23:11.868981] Epoch: [69]  [420/781]  eta: 0:01:11  lr: 0.000059  training_loss: 1.4829 (1.4833)  mae_loss: 0.2448 (0.2525)  classification_loss: 1.1801 (1.2259)  loss_mask: 0.0301 (0.0050)  time: 0.1966  data: 0.0002  max mem: 5511
[09:23:15.835624] Epoch: [69]  [440/781]  eta: 0:01:07  lr: 0.000059  training_loss: 1.5264 (1.4851)  mae_loss: 0.2389 (0.2526)  classification_loss: 1.1849 (1.2251)  loss_mask: 0.0240 (0.0074)  time: 0.1983  data: 0.0004  max mem: 5511
[09:23:19.811314] Epoch: [69]  [460/781]  eta: 0:01:03  lr: 0.000059  training_loss: 1.4554 (1.4843)  mae_loss: 0.2560 (0.2528)  classification_loss: 1.1972 (1.2242)  loss_mask: 0.0038 (0.0073)  time: 0.1987  data: 0.0002  max mem: 5511
[09:23:23.769355] Epoch: [69]  [480/781]  eta: 0:00:59  lr: 0.000059  training_loss: 1.4393 (1.4852)  mae_loss: 0.2461 (0.2527)  classification_loss: 1.1828 (1.2254)  loss_mask: 0.0015 (0.0071)  time: 0.1978  data: 0.0002  max mem: 5511
[09:23:27.701443] Epoch: [69]  [500/781]  eta: 0:00:55  lr: 0.000059  training_loss: 1.4860 (1.4859)  mae_loss: 0.2548 (0.2527)  classification_loss: 1.2303 (1.2264)  loss_mask: 0.0006 (0.0068)  time: 0.1965  data: 0.0002  max mem: 5511
[09:23:31.675714] Epoch: [69]  [520/781]  eta: 0:00:51  lr: 0.000059  training_loss: 1.4500 (1.4850)  mae_loss: 0.2394 (0.2524)  classification_loss: 1.2013 (1.2259)  loss_mask: 0.0006 (0.0066)  time: 0.1986  data: 0.0003  max mem: 5511
[09:23:35.626135] Epoch: [69]  [540/781]  eta: 0:00:47  lr: 0.000058  training_loss: 1.4773 (1.4849)  mae_loss: 0.2479 (0.2525)  classification_loss: 1.2250 (1.2258)  loss_mask: 0.0007 (0.0065)  time: 0.1974  data: 0.0002  max mem: 5511
[09:23:39.560100] Epoch: [69]  [560/781]  eta: 0:00:43  lr: 0.000058  training_loss: 1.4422 (1.4833)  mae_loss: 0.2447 (0.2523)  classification_loss: 1.2100 (1.2246)  loss_mask: 0.0012 (0.0064)  time: 0.1966  data: 0.0002  max mem: 5511
[09:23:43.500032] Epoch: [69]  [580/781]  eta: 0:00:39  lr: 0.000058  training_loss: 1.4670 (1.4837)  mae_loss: 0.2501 (0.2524)  classification_loss: 1.2115 (1.2251)  loss_mask: 0.0006 (0.0062)  time: 0.1969  data: 0.0002  max mem: 5511
[09:23:47.431410] Epoch: [69]  [600/781]  eta: 0:00:35  lr: 0.000058  training_loss: 1.4851 (1.4844)  mae_loss: 0.2573 (0.2526)  classification_loss: 1.2354 (1.2257)  loss_mask: 0.0006 (0.0061)  time: 0.1965  data: 0.0002  max mem: 5511
[09:23:51.386834] Epoch: [69]  [620/781]  eta: 0:00:31  lr: 0.000058  training_loss: 1.4803 (1.4848)  mae_loss: 0.2524 (0.2529)  classification_loss: 1.2281 (1.2260)  loss_mask: 0.0004 (0.0059)  time: 0.1977  data: 0.0002  max mem: 5511
[09:23:55.337164] Epoch: [69]  [640/781]  eta: 0:00:27  lr: 0.000058  training_loss: 1.4645 (1.4845)  mae_loss: 0.2550 (0.2529)  classification_loss: 1.2297 (1.2259)  loss_mask: 0.0003 (0.0057)  time: 0.1974  data: 0.0003  max mem: 5511
[09:23:59.298357] Epoch: [69]  [660/781]  eta: 0:00:24  lr: 0.000058  training_loss: 1.4886 (1.4853)  mae_loss: 0.2426 (0.2528)  classification_loss: 1.2453 (1.2270)  loss_mask: 0.0003 (0.0056)  time: 0.1980  data: 0.0003  max mem: 5511
[09:24:03.233385] Epoch: [69]  [680/781]  eta: 0:00:20  lr: 0.000058  training_loss: 1.4477 (1.4848)  mae_loss: 0.2440 (0.2528)  classification_loss: 1.2114 (1.2266)  loss_mask: 0.0002 (0.0054)  time: 0.1967  data: 0.0003  max mem: 5511
[09:24:07.196194] Epoch: [69]  [700/781]  eta: 0:00:16  lr: 0.000058  training_loss: 1.5356 (1.4856)  mae_loss: 0.2559 (0.2529)  classification_loss: 1.2486 (1.2274)  loss_mask: 0.0002 (0.0053)  time: 0.1981  data: 0.0003  max mem: 5511
[09:24:11.153479] Epoch: [69]  [720/781]  eta: 0:00:12  lr: 0.000058  training_loss: 1.5195 (1.4862)  mae_loss: 0.2612 (0.2530)  classification_loss: 1.2557 (1.2281)  loss_mask: 0.0002 (0.0051)  time: 0.1978  data: 0.0002  max mem: 5511
[09:24:15.105572] Epoch: [69]  [740/781]  eta: 0:00:08  lr: 0.000058  training_loss: 1.4467 (1.4856)  mae_loss: 0.2575 (0.2530)  classification_loss: 1.1890 (1.2276)  loss_mask: 0.0002 (0.0050)  time: 0.1975  data: 0.0002  max mem: 5511
[09:24:19.083055] Epoch: [69]  [760/781]  eta: 0:00:04  lr: 0.000057  training_loss: 1.5112 (1.4861)  mae_loss: 0.2507 (0.2530)  classification_loss: 1.2603 (1.2282)  loss_mask: 0.0002 (0.0049)  time: 0.1988  data: 0.0003  max mem: 5511
[09:24:23.076000] Epoch: [69]  [780/781]  eta: 0:00:00  lr: 0.000057  training_loss: 1.4882 (1.4870)  mae_loss: 0.2566 (0.2531)  classification_loss: 1.2523 (1.2292)  loss_mask: 0.0002 (0.0048)  time: 0.1996  data: 0.0002  max mem: 5511
[09:24:23.257814] Epoch: [69] Total time: 0:02:35 (0.1986 s / it)
[09:24:23.258400] Averaged stats: lr: 0.000057  training_loss: 1.4882 (1.4870)  mae_loss: 0.2566 (0.2531)  classification_loss: 1.2523 (1.2292)  loss_mask: 0.0002 (0.0048)
[09:24:23.896963] Test:  [  0/157]  eta: 0:01:39  testing_loss: 0.4450 (0.4450)  acc1: 89.0625 (89.0625)  acc5: 100.0000 (100.0000)  time: 0.6344  data: 0.6049  max mem: 5511
[09:24:24.195243] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5357 (0.5356)  acc1: 84.3750 (83.6648)  acc5: 100.0000 (99.7159)  time: 0.0846  data: 0.0563  max mem: 5511
[09:24:24.493202] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5182 (0.5181)  acc1: 84.3750 (84.1518)  acc5: 100.0000 (99.5536)  time: 0.0297  data: 0.0009  max mem: 5511
[09:24:24.791824] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5044 (0.5360)  acc1: 82.8125 (82.5605)  acc5: 100.0000 (99.3448)  time: 0.0297  data: 0.0006  max mem: 5511
[09:24:25.078271] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5115 (0.5395)  acc1: 81.2500 (82.5076)  acc5: 98.4375 (99.2378)  time: 0.0291  data: 0.0004  max mem: 5511
[09:24:25.362224] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5115 (0.5327)  acc1: 84.3750 (82.8431)  acc5: 100.0000 (99.1728)  time: 0.0283  data: 0.0002  max mem: 5511
[09:24:25.649477] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4858 (0.5246)  acc1: 84.3750 (83.1967)  acc5: 100.0000 (99.2059)  time: 0.0284  data: 0.0002  max mem: 5511
[09:24:25.934045] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5235 (0.5263)  acc1: 84.3750 (83.3407)  acc5: 100.0000 (99.2298)  time: 0.0285  data: 0.0002  max mem: 5511
[09:24:26.222148] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5378 (0.5331)  acc1: 84.3750 (83.2369)  acc5: 100.0000 (99.2284)  time: 0.0285  data: 0.0002  max mem: 5511
[09:24:26.507835] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5105 (0.5256)  acc1: 85.9375 (83.6538)  acc5: 100.0000 (99.2102)  time: 0.0285  data: 0.0002  max mem: 5511
[09:24:26.795769] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4941 (0.5264)  acc1: 85.9375 (83.6479)  acc5: 100.0000 (99.2110)  time: 0.0286  data: 0.0002  max mem: 5511
[09:24:27.082949] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5624 (0.5277)  acc1: 84.3750 (83.7134)  acc5: 100.0000 (99.1695)  time: 0.0286  data: 0.0002  max mem: 5511
[09:24:27.369824] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5624 (0.5265)  acc1: 82.8125 (83.6519)  acc5: 100.0000 (99.1736)  time: 0.0286  data: 0.0002  max mem: 5511
[09:24:27.655036] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5148 (0.5271)  acc1: 82.8125 (83.6236)  acc5: 100.0000 (99.2009)  time: 0.0285  data: 0.0002  max mem: 5511
[09:24:27.940391] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5148 (0.5258)  acc1: 82.8125 (83.6879)  acc5: 100.0000 (99.2354)  time: 0.0284  data: 0.0002  max mem: 5511
[09:24:28.223708] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5189 (0.5251)  acc1: 82.8125 (83.7334)  acc5: 100.0000 (99.2343)  time: 0.0283  data: 0.0001  max mem: 5511
[09:24:28.375782] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5189 (0.5252)  acc1: 82.8125 (83.7600)  acc5: 100.0000 (99.2300)  time: 0.0272  data: 0.0001  max mem: 5511
[09:24:28.557653] Test: Total time: 0:00:05 (0.0337 s / it)
[09:24:28.558168] * Acc@1 83.760 Acc@5 99.230 loss 0.525
[09:24:28.558504] Accuracy of the network on the 10000 test images: 83.8%
[09:24:28.558695] Max accuracy: 84.59%
[09:24:28.737792] log_dir: ./output_dir
[09:24:29.696239] Epoch: [70]  [  0/781]  eta: 0:12:27  lr: 0.000057  training_loss: 1.2581 (1.2581)  mae_loss: 0.2092 (0.2092)  classification_loss: 1.0488 (1.0488)  loss_mask: 0.0001 (0.0001)  time: 0.9566  data: 0.7174  max mem: 5511
[09:24:33.706580] Epoch: [70]  [ 20/781]  eta: 0:02:59  lr: 0.000057  training_loss: 1.4109 (1.4385)  mae_loss: 0.2411 (0.2453)  classification_loss: 1.1374 (1.1872)  loss_mask: 0.0004 (0.0060)  time: 0.2004  data: 0.0002  max mem: 5511
[09:24:37.692276] Epoch: [70]  [ 40/781]  eta: 0:02:41  lr: 0.000057  training_loss: 1.4576 (1.4400)  mae_loss: 0.2468 (0.2473)  classification_loss: 1.1932 (1.1867)  loss_mask: 0.0005 (0.0060)  time: 0.1992  data: 0.0002  max mem: 5511
[09:24:41.673879] Epoch: [70]  [ 60/781]  eta: 0:02:32  lr: 0.000057  training_loss: 1.4794 (1.4540)  mae_loss: 0.2502 (0.2497)  classification_loss: 1.1896 (1.1982)  loss_mask: 0.0009 (0.0061)  time: 0.1990  data: 0.0002  max mem: 5511
[09:24:45.633483] Epoch: [70]  [ 80/781]  eta: 0:02:26  lr: 0.000057  training_loss: 1.4451 (1.4539)  mae_loss: 0.2326 (0.2492)  classification_loss: 1.2028 (1.1981)  loss_mask: 0.0024 (0.0066)  time: 0.1979  data: 0.0002  max mem: 5511
[09:24:49.597482] Epoch: [70]  [100/781]  eta: 0:02:20  lr: 0.000057  training_loss: 1.4498 (1.4540)  mae_loss: 0.2329 (0.2467)  classification_loss: 1.2117 (1.2015)  loss_mask: 0.0015 (0.0058)  time: 0.1981  data: 0.0003  max mem: 5511
[09:24:53.568987] Epoch: [70]  [120/781]  eta: 0:02:15  lr: 0.000057  training_loss: 1.4579 (1.4548)  mae_loss: 0.2460 (0.2471)  classification_loss: 1.1985 (1.2023)  loss_mask: 0.0009 (0.0054)  time: 0.1985  data: 0.0002  max mem: 5511
[09:24:57.507609] Epoch: [70]  [140/781]  eta: 0:02:10  lr: 0.000057  training_loss: 1.4178 (1.4509)  mae_loss: 0.2404 (0.2474)  classification_loss: 1.1575 (1.1988)  loss_mask: 0.0003 (0.0047)  time: 0.1968  data: 0.0002  max mem: 5511
[09:25:01.487933] Epoch: [70]  [160/781]  eta: 0:02:06  lr: 0.000057  training_loss: 1.4734 (1.4559)  mae_loss: 0.2486 (0.2488)  classification_loss: 1.2077 (1.2028)  loss_mask: 0.0002 (0.0042)  time: 0.1989  data: 0.0002  max mem: 5511
[09:25:05.431511] Epoch: [70]  [180/781]  eta: 0:02:01  lr: 0.000057  training_loss: 1.4498 (1.4577)  mae_loss: 0.2509 (0.2499)  classification_loss: 1.1769 (1.2040)  loss_mask: 0.0002 (0.0037)  time: 0.1971  data: 0.0002  max mem: 5511

[09:25:09.436123] Epoch: [70]  [200/781]  eta: 0:01:57  lr: 0.000057  training_loss: 1.4215 (1.4560)  mae_loss: 0.2585 (0.2508)  classification_loss: 1.1840 (1.2018)  loss_mask: 0.0001 (0.0034)  time: 0.2001  data: 0.0002  max mem: 5511
[09:25:13.398370] Epoch: [70]  [220/781]  eta: 0:01:53  lr: 0.000056  training_loss: 1.4750 (1.4573)  mae_loss: 0.2377 (0.2504)  classification_loss: 1.2290 (1.2039)  loss_mask: 0.0001 (0.0031)  time: 0.1980  data: 0.0002  max mem: 5511
[09:25:17.368356] Epoch: [70]  [240/781]  eta: 0:01:49  lr: 0.000056  training_loss: 1.4580 (1.4578)  mae_loss: 0.2502 (0.2510)  classification_loss: 1.1865 (1.2040)  loss_mask: 0.0001 (0.0028)  time: 0.1984  data: 0.0002  max mem: 5511
[09:25:21.366017] Epoch: [70]  [260/781]  eta: 0:01:45  lr: 0.000056  training_loss: 1.4290 (1.4592)  mae_loss: 0.2569 (0.2516)  classification_loss: 1.1893 (1.2050)  loss_mask: 0.0001 (0.0026)  time: 0.1998  data: 0.0002  max mem: 5511
[09:25:25.323483] Epoch: [70]  [280/781]  eta: 0:01:40  lr: 0.000056  training_loss: 1.4476 (1.4575)  mae_loss: 0.2484 (0.2514)  classification_loss: 1.1927 (1.2037)  loss_mask: 0.0001 (0.0025)  time: 0.1978  data: 0.0002  max mem: 5511
[09:25:29.285810] Epoch: [70]  [300/781]  eta: 0:01:36  lr: 0.000056  training_loss: 1.4491 (1.4579)  mae_loss: 0.2515 (0.2515)  classification_loss: 1.2202 (1.2041)  loss_mask: 0.0001 (0.0023)  time: 0.1980  data: 0.0003  max mem: 5511
[09:25:33.250072] Epoch: [70]  [320/781]  eta: 0:01:32  lr: 0.000056  training_loss: 1.4966 (1.4591)  mae_loss: 0.2544 (0.2515)  classification_loss: 1.2421 (1.2055)  loss_mask: 0.0001 (0.0022)  time: 0.1981  data: 0.0002  max mem: 5511
[09:25:37.203833] Epoch: [70]  [340/781]  eta: 0:01:28  lr: 0.000056  training_loss: 1.4515 (1.4582)  mae_loss: 0.2513 (0.2514)  classification_loss: 1.2106 (1.2045)  loss_mask: 0.0001 (0.0022)  time: 0.1976  data: 0.0002  max mem: 5511
[09:25:41.151098] Epoch: [70]  [360/781]  eta: 0:01:24  lr: 0.000056  training_loss: 1.4625 (1.4590)  mae_loss: 0.2431 (0.2509)  classification_loss: 1.2037 (1.2060)  loss_mask: 0.0004 (0.0021)  time: 0.1973  data: 0.0002  max mem: 5511
[09:25:45.090050] Epoch: [70]  [380/781]  eta: 0:01:20  lr: 0.000056  training_loss: 1.5299 (1.4622)  mae_loss: 0.2502 (0.2514)  classification_loss: 1.2728 (1.2088)  loss_mask: 0.0002 (0.0020)  time: 0.1969  data: 0.0002  max mem: 5511
[09:25:49.032778] Epoch: [70]  [400/781]  eta: 0:01:16  lr: 0.000056  training_loss: 1.4078 (1.4609)  mae_loss: 0.2485 (0.2516)  classification_loss: 1.1418 (1.2073)  loss_mask: 0.0001 (0.0019)  time: 0.1971  data: 0.0003  max mem: 5511
[09:25:52.979126] Epoch: [70]  [420/781]  eta: 0:01:12  lr: 0.000056  training_loss: 1.4544 (1.4606)  mae_loss: 0.2642 (0.2524)  classification_loss: 1.1760 (1.2063)  loss_mask: 0.0002 (0.0019)  time: 0.1972  data: 0.0004  max mem: 5511
[09:25:56.950552] Epoch: [70]  [440/781]  eta: 0:01:08  lr: 0.000055  training_loss: 1.4243 (1.4609)  mae_loss: 0.2550 (0.2525)  classification_loss: 1.1751 (1.2066)  loss_mask: 0.0001 (0.0018)  time: 0.1985  data: 0.0003  max mem: 5511
[09:26:00.897019] Epoch: [70]  [460/781]  eta: 0:01:04  lr: 0.000055  training_loss: 1.4293 (1.4599)  mae_loss: 0.2412 (0.2523)  classification_loss: 1.1806 (1.2059)  loss_mask: 0.0001 (0.0017)  time: 0.1972  data: 0.0002  max mem: 5511
[09:26:04.876596] Epoch: [70]  [480/781]  eta: 0:01:00  lr: 0.000055  training_loss: 1.5227 (1.4612)  mae_loss: 0.2517 (0.2524)  classification_loss: 1.2585 (1.2072)  loss_mask: 0.0001 (0.0016)  time: 0.1989  data: 0.0002  max mem: 5511
[09:26:08.830093] Epoch: [70]  [500/781]  eta: 0:00:56  lr: 0.000055  training_loss: 1.4942 (1.4625)  mae_loss: 0.2461 (0.2526)  classification_loss: 1.2016 (1.2083)  loss_mask: 0.0001 (0.0016)  time: 0.1976  data: 0.0002  max mem: 5511
[09:26:12.814304] Epoch: [70]  [520/781]  eta: 0:00:52  lr: 0.000055  training_loss: 1.4382 (1.4622)  mae_loss: 0.2496 (0.2525)  classification_loss: 1.1877 (1.2082)  loss_mask: 0.0001 (0.0015)  time: 0.1991  data: 0.0002  max mem: 5511
[09:26:16.763515] Epoch: [70]  [540/781]  eta: 0:00:48  lr: 0.000055  training_loss: 1.4616 (1.4629)  mae_loss: 0.2406 (0.2523)  classification_loss: 1.2052 (1.2091)  loss_mask: 0.0001 (0.0015)  time: 0.1974  data: 0.0004  max mem: 5511
[09:26:20.713596] Epoch: [70]  [560/781]  eta: 0:00:44  lr: 0.000055  training_loss: 1.4369 (1.4627)  mae_loss: 0.2451 (0.2525)  classification_loss: 1.1971 (1.2088)  loss_mask: 0.0001 (0.0014)  time: 0.1974  data: 0.0007  max mem: 5511
[09:26:24.656659] Epoch: [70]  [580/781]  eta: 0:00:40  lr: 0.000055  training_loss: 1.4098 (1.4617)  mae_loss: 0.2544 (0.2528)  classification_loss: 1.1621 (1.2075)  loss_mask: 0.0001 (0.0014)  time: 0.1971  data: 0.0002  max mem: 5511
[09:26:28.650803] Epoch: [70]  [600/781]  eta: 0:00:36  lr: 0.000055  training_loss: 1.4384 (1.4624)  mae_loss: 0.2792 (0.2534)  classification_loss: 1.2086 (1.2077)  loss_mask: 0.0001 (0.0013)  time: 0.1996  data: 0.0002  max mem: 5511
[09:26:32.595864] Epoch: [70]  [620/781]  eta: 0:00:32  lr: 0.000055  training_loss: 1.4862 (1.4628)  mae_loss: 0.2618 (0.2536)  classification_loss: 1.2006 (1.2080)  loss_mask: 0.0001 (0.0013)  time: 0.1972  data: 0.0002  max mem: 5511
[09:26:36.527349] Epoch: [70]  [640/781]  eta: 0:00:28  lr: 0.000055  training_loss: 1.4237 (1.4623)  mae_loss: 0.2429 (0.2532)  classification_loss: 1.1932 (1.2078)  loss_mask: 0.0001 (0.0013)  time: 0.1965  data: 0.0003  max mem: 5511
[09:26:40.506995] Epoch: [70]  [660/781]  eta: 0:00:24  lr: 0.000055  training_loss: 1.4368 (1.4618)  mae_loss: 0.2393 (0.2530)  classification_loss: 1.1972 (1.2076)  loss_mask: 0.0001 (0.0012)  time: 0.1989  data: 0.0002  max mem: 5511
[09:26:44.465027] Epoch: [70]  [680/781]  eta: 0:00:20  lr: 0.000054  training_loss: 1.4732 (1.4620)  mae_loss: 0.2632 (0.2533)  classification_loss: 1.2232 (1.2075)  loss_mask: 0.0001 (0.0012)  time: 0.1978  data: 0.0002  max mem: 5511
[09:26:48.457508] Epoch: [70]  [700/781]  eta: 0:00:16  lr: 0.000054  training_loss: 1.4761 (1.4633)  mae_loss: 0.2448 (0.2532)  classification_loss: 1.2313 (1.2090)  loss_mask: 0.0001 (0.0012)  time: 0.1995  data: 0.0002  max mem: 5511
[09:26:52.431198] Epoch: [70]  [720/781]  eta: 0:00:12  lr: 0.000054  training_loss: 1.5313 (1.4647)  mae_loss: 0.2538 (0.2533)  classification_loss: 1.2612 (1.2102)  loss_mask: 0.0001 (0.0011)  time: 0.1986  data: 0.0002  max mem: 5511
[09:26:56.424596] Epoch: [70]  [740/781]  eta: 0:00:08  lr: 0.000054  training_loss: 1.4874 (1.4649)  mae_loss: 0.2628 (0.2536)  classification_loss: 1.2183 (1.2102)  loss_mask: 0.0001 (0.0011)  time: 0.1996  data: 0.0002  max mem: 5511
[09:27:00.389159] Epoch: [70]  [760/781]  eta: 0:00:04  lr: 0.000054  training_loss: 1.5045 (1.4654)  mae_loss: 0.2629 (0.2539)  classification_loss: 1.2232 (1.2104)  loss_mask: 0.0001 (0.0011)  time: 0.1981  data: 0.0003  max mem: 5511
[09:27:04.338612] Epoch: [70]  [780/781]  eta: 0:00:00  lr: 0.000054  training_loss: 1.4571 (1.4651)  mae_loss: 0.2473 (0.2539)  classification_loss: 1.1744 (1.2102)  loss_mask: 0.0001 (0.0011)  time: 0.1974  data: 0.0005  max mem: 5511
[09:27:04.510940] Epoch: [70] Total time: 0:02:35 (0.1995 s / it)
[09:27:04.511401] Averaged stats: lr: 0.000054  training_loss: 1.4571 (1.4651)  mae_loss: 0.2473 (0.2539)  classification_loss: 1.1744 (1.2102)  loss_mask: 0.0001 (0.0011)
[09:27:05.509448] Test:  [  0/157]  eta: 0:01:28  testing_loss: 0.4762 (0.4762)  acc1: 89.0625 (89.0625)  acc5: 98.4375 (98.4375)  time: 0.5637  data: 0.5342  max mem: 5511
[09:27:05.797511] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.5177 (0.5358)  acc1: 82.8125 (83.6648)  acc5: 98.4375 (99.1477)  time: 0.0773  data: 0.0488  max mem: 5511
[09:27:06.080186] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.5035 (0.5098)  acc1: 84.3750 (84.4494)  acc5: 100.0000 (99.4048)  time: 0.0284  data: 0.0002  max mem: 5511
[09:27:06.363940] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.4930 (0.5212)  acc1: 84.3750 (83.6694)  acc5: 100.0000 (99.2944)  time: 0.0282  data: 0.0002  max mem: 5511
[09:27:06.650724] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.5103 (0.5278)  acc1: 82.8125 (83.6509)  acc5: 100.0000 (99.1997)  time: 0.0284  data: 0.0002  max mem: 5511
[09:27:06.936916] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5092 (0.5226)  acc1: 84.3750 (83.9461)  acc5: 100.0000 (99.1422)  time: 0.0285  data: 0.0002  max mem: 5511
[09:27:07.220927] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4823 (0.5147)  acc1: 85.9375 (84.1701)  acc5: 100.0000 (99.1547)  time: 0.0284  data: 0.0002  max mem: 5511
[09:27:07.504077] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4804 (0.5140)  acc1: 84.3750 (84.1769)  acc5: 100.0000 (99.1637)  time: 0.0282  data: 0.0002  max mem: 5511
[09:27:07.790960] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5164 (0.5217)  acc1: 82.8125 (83.9120)  acc5: 98.4375 (99.0548)  time: 0.0284  data: 0.0002  max mem: 5511
[09:27:08.074736] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4886 (0.5147)  acc1: 84.3750 (84.2205)  acc5: 98.4375 (99.0556)  time: 0.0284  data: 0.0002  max mem: 5511
[09:27:08.358577] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4695 (0.5142)  acc1: 87.5000 (84.3750)  acc5: 98.4375 (99.0718)  time: 0.0282  data: 0.0002  max mem: 5511
[09:27:08.643303] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5285 (0.5170)  acc1: 85.9375 (84.2483)  acc5: 100.0000 (99.0569)  time: 0.0283  data: 0.0002  max mem: 5511
[09:27:08.929481] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5285 (0.5147)  acc1: 84.3750 (84.3233)  acc5: 100.0000 (99.0702)  time: 0.0284  data: 0.0002  max mem: 5511
[09:27:09.214777] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4843 (0.5156)  acc1: 84.3750 (84.2438)  acc5: 100.0000 (99.1293)  time: 0.0284  data: 0.0002  max mem: 5511
[09:27:09.498220] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4843 (0.5130)  acc1: 84.3750 (84.4415)  acc5: 100.0000 (99.1689)  time: 0.0283  data: 0.0002  max mem: 5511
[09:27:09.779121] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4729 (0.5113)  acc1: 84.3750 (84.4992)  acc5: 100.0000 (99.1618)  time: 0.0281  data: 0.0001  max mem: 5511
[09:27:09.930738] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4649 (0.5109)  acc1: 85.9375 (84.4900)  acc5: 100.0000 (99.1600)  time: 0.0271  data: 0.0001  max mem: 5511
[09:27:10.085280] Test: Total time: 0:00:05 (0.0327 s / it)
[09:27:10.086355] * Acc@1 84.490 Acc@5 99.160 loss 0.511
[09:27:10.086670] Accuracy of the network on the 10000 test images: 84.5%
[09:27:10.086846] Max accuracy: 84.59%
[09:27:10.259888] log_dir: ./output_dir
[09:27:11.073931] Epoch: [71]  [  0/781]  eta: 0:10:34  lr: 0.000054  training_loss: 1.4039 (1.4039)  mae_loss: 0.2753 (0.2753)  classification_loss: 1.1285 (1.1285)  loss_mask: 0.0001 (0.0001)  time: 0.8121  data: 0.5812  max mem: 5511
[09:27:15.025273] Epoch: [71]  [ 20/781]  eta: 0:02:52  lr: 0.000054  training_loss: 1.4407 (1.4403)  mae_loss: 0.2497 (0.2568)  classification_loss: 1.1922 (1.1835)  loss_mask: 0.0001 (0.0001)  time: 0.1975  data: 0.0002  max mem: 5511
[09:27:18.974965] Epoch: [71]  [ 40/781]  eta: 0:02:37  lr: 0.000054  training_loss: 1.5064 (1.4794)  mae_loss: 0.2512 (0.2567)  classification_loss: 1.2461 (1.2226)  loss_mask: 0.0001 (0.0001)  time: 0.1974  data: 0.0002  max mem: 5511
[09:27:22.926520] Epoch: [71]  [ 60/781]  eta: 0:02:29  lr: 0.000054  training_loss: 1.4576 (1.4760)  mae_loss: 0.2637 (0.2579)  classification_loss: 1.1896 (1.2179)  loss_mask: 0.0001 (0.0001)  time: 0.1975  data: 0.0003  max mem: 5511
[09:27:26.907808] Epoch: [71]  [ 80/781]  eta: 0:02:24  lr: 0.000054  training_loss: 1.4352 (1.4678)  mae_loss: 0.2464 (0.2564)  classification_loss: 1.1856 (1.2113)  loss_mask: 0.0001 (0.0001)  time: 0.1990  data: 0.0003  max mem: 5511
[09:27:30.903372] Epoch: [71]  [100/781]  eta: 0:02:19  lr: 0.000054  training_loss: 1.4038 (1.4597)  mae_loss: 0.2476 (0.2558)  classification_loss: 1.1520 (1.2038)  loss_mask: 0.0001 (0.0001)  time: 0.1997  data: 0.0002  max mem: 5511
[09:27:34.847080] Epoch: [71]  [120/781]  eta: 0:02:14  lr: 0.000053  training_loss: 1.5012 (1.4637)  mae_loss: 0.2388 (0.2537)  classification_loss: 1.2500 (1.2099)  loss_mask: 0.0001 (0.0001)  time: 0.1971  data: 0.0002  max mem: 5511
[09:27:38.825464] Epoch: [71]  [140/781]  eta: 0:02:09  lr: 0.000053  training_loss: 1.4655 (1.4618)  mae_loss: 0.2602 (0.2541)  classification_loss: 1.1970 (1.2076)  loss_mask: 0.0001 (0.0001)  time: 0.1988  data: 0.0002  max mem: 5511
[09:27:42.761742] Epoch: [71]  [160/781]  eta: 0:02:05  lr: 0.000053  training_loss: 1.5036 (1.4633)  mae_loss: 0.2449 (0.2533)  classification_loss: 1.2223 (1.2090)  loss_mask: 0.0002 (0.0010)  time: 0.1967  data: 0.0002  max mem: 5511
[09:27:46.715598] Epoch: [71]  [180/781]  eta: 0:02:00  lr: 0.000053  training_loss: 1.4310 (1.4607)  mae_loss: 0.2316 (0.2519)  classification_loss: 1.1664 (1.2078)  loss_mask: 0.0002 (0.0009)  time: 0.1976  data: 0.0003  max mem: 5511
[09:27:50.662042] Epoch: [71]  [200/781]  eta: 0:01:56  lr: 0.000053  training_loss: 1.4320 (1.4581)  mae_loss: 0.2474 (0.2518)  classification_loss: 1.1895 (1.2054)  loss_mask: 0.0001 (0.0009)  time: 0.1972  data: 0.0002  max mem: 5511
[09:27:54.612693] Epoch: [71]  [220/781]  eta: 0:01:52  lr: 0.000053  training_loss: 1.4630 (1.4589)  mae_loss: 0.2549 (0.2528)  classification_loss: 1.2054 (1.2053)  loss_mask: 0.0001 (0.0008)  time: 0.1974  data: 0.0002  max mem: 5511
[09:27:58.589141] Epoch: [71]  [240/781]  eta: 0:01:48  lr: 0.000053  training_loss: 1.4920 (1.4616)  mae_loss: 0.2619 (0.2539)  classification_loss: 1.1908 (1.2069)  loss_mask: 0.0001 (0.0007)  time: 0.1987  data: 0.0002  max mem: 5511
[09:28:02.543162] Epoch: [71]  [260/781]  eta: 0:01:44  lr: 0.000053  training_loss: 1.4301 (1.4607)  mae_loss: 0.2385 (0.2532)  classification_loss: 1.1852 (1.2068)  loss_mask: 0.0001 (0.0007)  time: 0.1976  data: 0.0002  max mem: 5511
[09:28:06.509889] Epoch: [71]  [280/781]  eta: 0:01:40  lr: 0.000053  training_loss: 1.4840 (1.4620)  mae_loss: 0.2560 (0.2533)  classification_loss: 1.2297 (1.2081)  loss_mask: 0.0001 (0.0007)  time: 0.1982  data: 0.0002  max mem: 5511
[09:28:10.453599] Epoch: [71]  [300/781]  eta: 0:01:36  lr: 0.000053  training_loss: 1.4531 (1.4627)  mae_loss: 0.2571 (0.2536)  classification_loss: 1.1892 (1.2085)  loss_mask: 0.0001 (0.0006)  time: 0.1971  data: 0.0003  max mem: 5511
[09:28:14.390774] Epoch: [71]  [320/781]  eta: 0:01:32  lr: 0.000053  training_loss: 1.3985 (1.4600)  mae_loss: 0.2382 (0.2528)  classification_loss: 1.1651 (1.2066)  loss_mask: 0.0001 (0.0006)  time: 0.1968  data: 0.0002  max mem: 5511
[09:28:18.349397] Epoch: [71]  [340/781]  eta: 0:01:28  lr: 0.000053  training_loss: 1.4302 (1.4588)  mae_loss: 0.2452 (0.2525)  classification_loss: 1.1862 (1.2058)  loss_mask: 0.0001 (0.0006)  time: 0.1978  data: 0.0004  max mem: 5511
[09:28:22.306674] Epoch: [71]  [360/781]  eta: 0:01:23  lr: 0.000052  training_loss: 1.4719 (1.4597)  mae_loss: 0.2510 (0.2522)  classification_loss: 1.2407 (1.2070)  loss_mask: 0.0001 (0.0005)  time: 0.1978  data: 0.0004  max mem: 5511
[09:28:26.289393] Epoch: [71]  [380/781]  eta: 0:01:19  lr: 0.000052  training_loss: 1.4392 (1.4592)  mae_loss: 0.2518 (0.2520)  classification_loss: 1.1719 (1.2067)  loss_mask: 0.0001 (0.0005)  time: 0.1991  data: 0.0003  max mem: 5511
[09:28:30.246332] Epoch: [71]  [400/781]  eta: 0:01:15  lr: 0.000052  training_loss: 1.4703 (1.4605)  mae_loss: 0.2411 (0.2520)  classification_loss: 1.2159 (1.2080)  loss_mask: 0.0001 (0.0005)  time: 0.1978  data: 0.0002  max mem: 5511
[09:28:34.216390] Epoch: [71]  [420/781]  eta: 0:01:11  lr: 0.000052  training_loss: 1.4992 (1.4614)  mae_loss: 0.2655 (0.2523)  classification_loss: 1.2202 (1.2079)  loss_mask: 0.0004 (0.0012)  time: 0.1984  data: 0.0002  max mem: 5511
[09:28:38.193717] Epoch: [71]  [440/781]  eta: 0:01:07  lr: 0.000052  training_loss: 1.4917 (1.4628)  mae_loss: 0.2570 (0.2528)  classification_loss: 1.2359 (1.2086)  loss_mask: 0.0014 (0.0015)  time: 0.1987  data: 0.0002  max mem: 5511
[09:28:42.161434] Epoch: [71]  [460/781]  eta: 0:01:03  lr: 0.000052  training_loss: 1.4645 (1.4633)  mae_loss: 0.2575 (0.2529)  classification_loss: 1.1472 (1.2064)  loss_mask: 0.0317 (0.0040)  time: 0.1983  data: 0.0002  max mem: 5511
[09:28:46.124803] Epoch: [71]  [480/781]  eta: 0:00:59  lr: 0.000052  training_loss: 1.4579 (1.4645)  mae_loss: 0.2528 (0.2529)  classification_loss: 1.1885 (1.2067)  loss_mask: 0.0203 (0.0050)  time: 0.1981  data: 0.0002  max mem: 5511
[09:28:50.090870] Epoch: [71]  [500/781]  eta: 0:00:55  lr: 0.000052  training_loss: 1.4557 (1.4652)  mae_loss: 0.2506 (0.2529)  classification_loss: 1.1941 (1.2071)  loss_mask: 0.0041 (0.0051)  time: 0.1982  data: 0.0003  max mem: 5511
[09:28:54.051577] Epoch: [71]  [520/781]  eta: 0:00:51  lr: 0.000052  training_loss: 1.5096 (1.4662)  mae_loss: 0.2551 (0.2534)  classification_loss: 1.2153 (1.2078)  loss_mask: 0.0018 (0.0050)  time: 0.1980  data: 0.0005  max mem: 5511
[09:28:58.000455] Epoch: [71]  [540/781]  eta: 0:00:47  lr: 0.000052  training_loss: 1.4544 (1.4660)  mae_loss: 0.2539 (0.2536)  classification_loss: 1.1734 (1.2076)  loss_mask: 0.0009 (0.0049)  time: 0.1974  data: 0.0002  max mem: 5511
[09:29:01.959951] Epoch: [71]  [560/781]  eta: 0:00:43  lr: 0.000052  training_loss: 1.4618 (1.4659)  mae_loss: 0.2542 (0.2538)  classification_loss: 1.1771 (1.2074)  loss_mask: 0.0004 (0.0047)  time: 0.1979  data: 0.0003  max mem: 5511
[09:29:05.914855] Epoch: [71]  [580/781]  eta: 0:00:39  lr: 0.000052  training_loss: 1.4716 (1.4661)  mae_loss: 0.2649 (0.2541)  classification_loss: 1.2097 (1.2075)  loss_mask: 0.0004 (0.0046)  time: 0.1977  data: 0.0002  max mem: 5511
[09:29:09.888337] Epoch: [71]  [600/781]  eta: 0:00:36  lr: 0.000051  training_loss: 1.4412 (1.4665)  mae_loss: 0.2457 (0.2539)  classification_loss: 1.1809 (1.2082)  loss_mask: 0.0003 (0.0044)  time: 0.1986  data: 0.0002  max mem: 5511
[09:29:13.853079] Epoch: [71]  [620/781]  eta: 0:00:32  lr: 0.000051  training_loss: 1.4295 (1.4657)  mae_loss: 0.2529 (0.2539)  classification_loss: 1.1909 (1.2075)  loss_mask: 0.0002 (0.0043)  time: 0.1982  data: 0.0002  max mem: 5511
[09:29:17.809301] Epoch: [71]  [640/781]  eta: 0:00:28  lr: 0.000051  training_loss: 1.4567 (1.4662)  mae_loss: 0.2480 (0.2538)  classification_loss: 1.2155 (1.2083)  loss_mask: 0.0003 (0.0042)  time: 0.1977  data: 0.0002  max mem: 5511
[09:29:21.776468] Epoch: [71]  [660/781]  eta: 0:00:24  lr: 0.000051  training_loss: 1.4441 (1.4656)  mae_loss: 0.2434 (0.2536)  classification_loss: 1.2004 (1.2080)  loss_mask: 0.0003 (0.0040)  time: 0.1982  data: 0.0002  max mem: 5511
[09:29:25.732995] Epoch: [71]  [680/781]  eta: 0:00:20  lr: 0.000051  training_loss: 1.4298 (1.4658)  mae_loss: 0.2505 (0.2535)  classification_loss: 1.1845 (1.2083)  loss_mask: 0.0002 (0.0039)  time: 0.1977  data: 0.0003  max mem: 5511
[09:29:29.678216] Epoch: [71]  [700/781]  eta: 0:00:16  lr: 0.000051  training_loss: 1.4744 (1.4658)  mae_loss: 0.2446 (0.2535)  classification_loss: 1.1996 (1.2085)  loss_mask: 0.0002 (0.0038)  time: 0.1972  data: 0.0002  max mem: 5511
[09:29:33.633543] Epoch: [71]  [720/781]  eta: 0:00:12  lr: 0.000051  training_loss: 1.4865 (1.4670)  mae_loss: 0.2462 (0.2534)  classification_loss: 1.2517 (1.2099)  loss_mask: 0.0001 (0.0037)  time: 0.1977  data: 0.0003  max mem: 5511
[09:29:37.586347] Epoch: [71]  [740/781]  eta: 0:00:08  lr: 0.000051  training_loss: 1.4077 (1.4657)  mae_loss: 0.2378 (0.2532)  classification_loss: 1.1543 (1.2088)  loss_mask: 0.0002 (0.0036)  time: 0.1975  data: 0.0004  max mem: 5511
[09:29:41.516545] Epoch: [71]  [760/781]  eta: 0:00:04  lr: 0.000051  training_loss: 1.4944 (1.4667)  mae_loss: 0.2558 (0.2533)  classification_loss: 1.2342 (1.2098)  loss_mask: 0.0002 (0.0035)  time: 0.1964  data: 0.0002  max mem: 5511
[09:29:45.465298] Epoch: [71]  [780/781]  eta: 0:00:00  lr: 0.000051  training_loss: 1.4706 (1.4666)  mae_loss: 0.2463 (0.2531)  classification_loss: 1.2173 (1.2100)  loss_mask: 0.0002 (0.0035)  time: 0.1973  data: 0.0002  max mem: 5511
[09:29:45.628005] Epoch: [71] Total time: 0:02:35 (0.1989 s / it)
[09:29:45.628657] Averaged stats: lr: 0.000051  training_loss: 1.4706 (1.4666)  mae_loss: 0.2463 (0.2531)  classification_loss: 1.2173 (1.2100)  loss_mask: 0.0002 (0.0035)
[09:29:46.407202] Test:  [  0/157]  eta: 0:02:01  testing_loss: 0.5021 (0.5021)  acc1: 85.9375 (85.9375)  acc5: 100.0000 (100.0000)  time: 0.7713  data: 0.7274  max mem: 5511
[09:29:46.698761] Test:  [ 10/157]  eta: 0:00:14  testing_loss: 0.5315 (0.5362)  acc1: 84.3750 (83.2386)  acc5: 100.0000 (99.5739)  time: 0.0964  data: 0.0663  max mem: 5511
[09:29:46.994208] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4789 (0.5041)  acc1: 85.9375 (84.6726)  acc5: 100.0000 (99.4792)  time: 0.0291  data: 0.0003  max mem: 5511
[09:29:47.290513] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4755 (0.5195)  acc1: 85.9375 (83.6694)  acc5: 100.0000 (99.3448)  time: 0.0294  data: 0.0003  max mem: 5511
[09:29:47.577937] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5082 (0.5234)  acc1: 82.8125 (83.6509)  acc5: 98.4375 (99.2378)  time: 0.0290  data: 0.0002  max mem: 5511
[09:29:47.867758] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4962 (0.5129)  acc1: 84.3750 (84.1912)  acc5: 98.4375 (99.2647)  time: 0.0287  data: 0.0002  max mem: 5511
[09:29:48.158142] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4464 (0.5051)  acc1: 85.9375 (84.4006)  acc5: 100.0000 (99.2572)  time: 0.0289  data: 0.0003  max mem: 5511
[09:29:48.447717] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4717 (0.5030)  acc1: 84.3750 (84.5511)  acc5: 100.0000 (99.2298)  time: 0.0288  data: 0.0002  max mem: 5511
[09:29:48.736156] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5085 (0.5125)  acc1: 84.3750 (84.2593)  acc5: 98.4375 (99.1319)  time: 0.0287  data: 0.0002  max mem: 5511
[09:29:49.023124] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5289 (0.5079)  acc1: 85.9375 (84.5639)  acc5: 98.4375 (99.1071)  time: 0.0286  data: 0.0002  max mem: 5511
[09:29:49.318095] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.4977 (0.5102)  acc1: 85.9375 (84.5142)  acc5: 100.0000 (99.1337)  time: 0.0290  data: 0.0002  max mem: 5511
[09:29:49.603383] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5270 (0.5121)  acc1: 84.3750 (84.4313)  acc5: 100.0000 (99.0991)  time: 0.0289  data: 0.0002  max mem: 5511
[09:29:49.890743] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5036 (0.5100)  acc1: 84.3750 (84.5300)  acc5: 100.0000 (99.1219)  time: 0.0285  data: 0.0002  max mem: 5511
[09:29:50.184466] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4722 (0.5099)  acc1: 82.8125 (84.4585)  acc5: 100.0000 (99.1531)  time: 0.0288  data: 0.0002  max mem: 5511
[09:29:50.474401] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4804 (0.5070)  acc1: 84.3750 (84.5745)  acc5: 100.0000 (99.1910)  time: 0.0290  data: 0.0002  max mem: 5511
[09:29:50.760430] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4804 (0.5059)  acc1: 85.9375 (84.5820)  acc5: 100.0000 (99.2032)  time: 0.0286  data: 0.0002  max mem: 5511
[09:29:50.915672] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4743 (0.5057)  acc1: 85.9375 (84.5900)  acc5: 100.0000 (99.2100)  time: 0.0276  data: 0.0002  max mem: 5511
[09:29:51.093780] Test: Total time: 0:00:05 (0.0348 s / it)
[09:29:51.094270] * Acc@1 84.590 Acc@5 99.210 loss 0.506
[09:29:51.094615] Accuracy of the network on the 10000 test images: 84.6%
[09:29:51.094872] Max accuracy: 84.59%
[09:29:51.332001] log_dir: ./output_dir
[09:29:52.162706] Epoch: [72]  [  0/781]  eta: 0:10:47  lr: 0.000051  training_loss: 1.3332 (1.3332)  mae_loss: 0.3026 (0.3026)  classification_loss: 1.0305 (1.0305)  loss_mask: 0.0001 (0.0001)  time: 0.8289  data: 0.6220  max mem: 5511
[09:29:56.119061] Epoch: [72]  [ 20/781]  eta: 0:02:53  lr: 0.000051  training_loss: 1.3706 (1.4020)  mae_loss: 0.2693 (0.2667)  classification_loss: 1.1288 (1.1348)  loss_mask: 0.0002 (0.0005)  time: 0.1977  data: 0.0002  max mem: 5511
[09:30:00.056154] Epoch: [72]  [ 40/781]  eta: 0:02:37  lr: 0.000050  training_loss: 1.4502 (1.4295)  mae_loss: 0.2575 (0.2588)  classification_loss: 1.1925 (1.1703)  loss_mask: 0.0002 (0.0005)  time: 0.1968  data: 0.0002  max mem: 5511
[09:30:04.059157] Epoch: [72]  [ 60/781]  eta: 0:02:30  lr: 0.000050  training_loss: 1.4702 (1.4442)  mae_loss: 0.2433 (0.2542)  classification_loss: 1.2273 (1.1896)  loss_mask: 0.0001 (0.0004)  time: 0.2001  data: 0.0002  max mem: 5511
[09:30:08.014012] Epoch: [72]  [ 80/781]  eta: 0:02:24  lr: 0.000050  training_loss: 1.4275 (1.4525)  mae_loss: 0.2407 (0.2541)  classification_loss: 1.1828 (1.1982)  loss_mask: 0.0001 (0.0003)  time: 0.1977  data: 0.0002  max mem: 5511
[09:30:11.957892] Epoch: [72]  [100/781]  eta: 0:02:19  lr: 0.000050  training_loss: 1.4420 (1.4559)  mae_loss: 0.2527 (0.2531)  classification_loss: 1.1891 (1.2025)  loss_mask: 0.0001 (0.0003)  time: 0.1971  data: 0.0002  max mem: 5511
[09:30:15.956218] Epoch: [72]  [120/781]  eta: 0:02:14  lr: 0.000050  training_loss: 1.4079 (1.4499)  mae_loss: 0.2322 (0.2505)  classification_loss: 1.1781 (1.1991)  loss_mask: 0.0001 (0.0002)  time: 0.1998  data: 0.0002  max mem: 5511
[09:30:19.901582] Epoch: [72]  [140/781]  eta: 0:02:09  lr: 0.000050  training_loss: 1.4639 (1.4538)  mae_loss: 0.2545 (0.2509)  classification_loss: 1.2215 (1.2027)  loss_mask: 0.0001 (0.0002)  time: 0.1972  data: 0.0002  max mem: 5511
[09:30:23.829547] Epoch: [72]  [160/781]  eta: 0:02:05  lr: 0.000050  training_loss: 1.4454 (1.4516)  mae_loss: 0.2393 (0.2498)  classification_loss: 1.2006 (1.2015)  loss_mask: 0.0001 (0.0002)  time: 0.1963  data: 0.0003  max mem: 5511
[09:30:27.784982] Epoch: [72]  [180/781]  eta: 0:02:00  lr: 0.000050  training_loss: 1.4130 (1.4485)  mae_loss: 0.2382 (0.2491)  classification_loss: 1.1592 (1.1991)  loss_mask: 0.0001 (0.0002)  time: 0.1977  data: 0.0002  max mem: 5511
[09:30:31.732819] Epoch: [72]  [200/781]  eta: 0:01:56  lr: 0.000050  training_loss: 1.4963 (1.4522)  mae_loss: 0.2493 (0.2503)  classification_loss: 1.2179 (1.2018)  loss_mask: 0.0001 (0.0002)  time: 0.1973  data: 0.0002  max mem: 5511
[09:30:35.659096] Epoch: [72]  [220/781]  eta: 0:01:52  lr: 0.000050  training_loss: 1.4667 (1.4550)  mae_loss: 0.2479 (0.2499)  classification_loss: 1.2168 (1.2049)  loss_mask: 0.0001 (0.0002)  time: 0.1962  data: 0.0003  max mem: 5511
[09:30:39.613020] Epoch: [72]  [240/781]  eta: 0:01:48  lr: 0.000050  training_loss: 1.4644 (1.4553)  mae_loss: 0.2490 (0.2504)  classification_loss: 1.2108 (1.2047)  loss_mask: 0.0001 (0.0002)  time: 0.1976  data: 0.0002  max mem: 5511
[09:30:43.560266] Epoch: [72]  [260/781]  eta: 0:01:44  lr: 0.000050  training_loss: 1.4618 (1.4561)  mae_loss: 0.2517 (0.2507)  classification_loss: 1.1981 (1.2052)  loss_mask: 0.0001 (0.0002)  time: 0.1973  data: 0.0004  max mem: 5511
[09:30:47.534493] Epoch: [72]  [280/781]  eta: 0:01:40  lr: 0.000049  training_loss: 1.4508 (1.4563)  mae_loss: 0.2538 (0.2510)  classification_loss: 1.2080 (1.2051)  loss_mask: 0.0001 (0.0002)  time: 0.1986  data: 0.0003  max mem: 5511

[09:30:51.476014] Epoch: [72]  [300/781]  eta: 0:01:36  lr: 0.000049  training_loss: 1.4685 (1.4573)  mae_loss: 0.2460 (0.2513)  classification_loss: 1.1979 (1.2059)  loss_mask: 0.0001 (0.0002)  time: 0.1970  data: 0.0003  max mem: 5511
[09:30:55.418835] Epoch: [72]  [320/781]  eta: 0:01:31  lr: 0.000049  training_loss: 1.4485 (1.4566)  mae_loss: 0.2388 (0.2510)  classification_loss: 1.2019 (1.2054)  loss_mask: 0.0001 (0.0002)  time: 0.1970  data: 0.0002  max mem: 5511
[09:30:59.360184] Epoch: [72]  [340/781]  eta: 0:01:27  lr: 0.000049  training_loss: 1.3959 (1.4552)  mae_loss: 0.2534 (0.2511)  classification_loss: 1.1516 (1.2040)  loss_mask: 0.0001 (0.0002)  time: 0.1970  data: 0.0002  max mem: 5511
[09:31:03.328441] Epoch: [72]  [360/781]  eta: 0:01:23  lr: 0.000049  training_loss: 1.4948 (1.4559)  mae_loss: 0.2362 (0.2503)  classification_loss: 1.2437 (1.2054)  loss_mask: 0.0001 (0.0002)  time: 0.1983  data: 0.0003  max mem: 5511
[09:31:07.319424] Epoch: [72]  [380/781]  eta: 0:01:19  lr: 0.000049  training_loss: 1.4737 (1.4570)  mae_loss: 0.2532 (0.2504)  classification_loss: 1.2000 (1.2064)  loss_mask: 0.0001 (0.0002)  time: 0.1994  data: 0.0004  max mem: 5511
[09:31:11.298004] Epoch: [72]  [400/781]  eta: 0:01:15  lr: 0.000049  training_loss: 1.4364 (1.4558)  mae_loss: 0.2429 (0.2503)  classification_loss: 1.1438 (1.2053)  loss_mask: 0.0001 (0.0002)  time: 0.1989  data: 0.0004  max mem: 5511
[09:31:15.231010] Epoch: [72]  [420/781]  eta: 0:01:11  lr: 0.000049  training_loss: 1.4687 (1.4566)  mae_loss: 0.2618 (0.2508)  classification_loss: 1.2124 (1.2056)  loss_mask: 0.0001 (0.0002)  time: 0.1966  data: 0.0002  max mem: 5511
[09:31:19.195782] Epoch: [72]  [440/781]  eta: 0:01:07  lr: 0.000049  training_loss: 1.4023 (1.4546)  mae_loss: 0.2432 (0.2506)  classification_loss: 1.1482 (1.2039)  loss_mask: 0.0001 (0.0002)  time: 0.1982  data: 0.0003  max mem: 5511
[09:31:23.143534] Epoch: [72]  [460/781]  eta: 0:01:03  lr: 0.000049  training_loss: 1.4008 (1.4540)  mae_loss: 0.2531 (0.2509)  classification_loss: 1.1549 (1.2030)  loss_mask: 0.0001 (0.0002)  time: 0.1973  data: 0.0002  max mem: 5511
[09:31:27.107057] Epoch: [72]  [480/781]  eta: 0:00:59  lr: 0.000049  training_loss: 1.4505 (1.4547)  mae_loss: 0.2579 (0.2510)  classification_loss: 1.2072 (1.2036)  loss_mask: 0.0001 (0.0002)  time: 0.1981  data: 0.0002  max mem: 5511
[09:31:31.025093] Epoch: [72]  [500/781]  eta: 0:00:55  lr: 0.000049  training_loss: 1.4239 (1.4540)  mae_loss: 0.2431 (0.2508)  classification_loss: 1.1857 (1.2030)  loss_mask: 0.0001 (0.0001)  time: 0.1958  data: 0.0002  max mem: 5511
[09:31:34.986472] Epoch: [72]  [520/781]  eta: 0:00:51  lr: 0.000048  training_loss: 1.4247 (1.4538)  mae_loss: 0.2576 (0.2511)  classification_loss: 1.1550 (1.2025)  loss_mask: 0.0001 (0.0001)  time: 0.1979  data: 0.0002  max mem: 5511
[09:31:38.937502] Epoch: [72]  [540/781]  eta: 0:00:47  lr: 0.000048  training_loss: 1.4557 (1.4530)  mae_loss: 0.2497 (0.2512)  classification_loss: 1.1890 (1.2016)  loss_mask: 0.0001 (0.0001)  time: 0.1975  data: 0.0002  max mem: 5511
[09:31:42.870315] Epoch: [72]  [560/781]  eta: 0:00:43  lr: 0.000048  training_loss: 1.4364 (1.4537)  mae_loss: 0.2471 (0.2513)  classification_loss: 1.1704 (1.2018)  loss_mask: 0.0019 (0.0006)  time: 0.1966  data: 0.0002  max mem: 5511
[09:31:46.816444] Epoch: [72]  [580/781]  eta: 0:00:39  lr: 0.000048  training_loss: 1.5251 (1.4570)  mae_loss: 0.2424 (0.2511)  classification_loss: 1.1974 (1.2017)  loss_mask: 0.0706 (0.0042)  time: 0.1972  data: 0.0003  max mem: 5511
[09:31:50.780431] Epoch: [72]  [600/781]  eta: 0:00:35  lr: 0.000048  training_loss: 1.5544 (1.4599)  mae_loss: 0.2470 (0.2510)  classification_loss: 1.2472 (1.2028)  loss_mask: 0.0512 (0.0061)  time: 0.1981  data: 0.0002  max mem: 5511
[09:31:54.740365] Epoch: [72]  [620/781]  eta: 0:00:31  lr: 0.000048  training_loss: 1.4103 (1.4598)  mae_loss: 0.2499 (0.2508)  classification_loss: 1.1428 (1.2021)  loss_mask: 0.0250 (0.0070)  time: 0.1979  data: 0.0003  max mem: 5511
[09:31:58.692056] Epoch: [72]  [640/781]  eta: 0:00:28  lr: 0.000048  training_loss: 1.4458 (1.4590)  mae_loss: 0.2479 (0.2509)  classification_loss: 1.1976 (1.2011)  loss_mask: 0.0075 (0.0070)  time: 0.1975  data: 0.0003  max mem: 5511
[09:32:02.625223] Epoch: [72]  [660/781]  eta: 0:00:24  lr: 0.000048  training_loss: 1.4596 (1.4595)  mae_loss: 0.2323 (0.2505)  classification_loss: 1.2515 (1.2022)  loss_mask: 0.0031 (0.0069)  time: 0.1966  data: 0.0002  max mem: 5511
[09:32:06.557690] Epoch: [72]  [680/781]  eta: 0:00:20  lr: 0.000048  training_loss: 1.4548 (1.4594)  mae_loss: 0.2348 (0.2501)  classification_loss: 1.2227 (1.2025)  loss_mask: 0.0017 (0.0067)  time: 0.1965  data: 0.0003  max mem: 5511
[09:32:10.525268] Epoch: [72]  [700/781]  eta: 0:00:16  lr: 0.000048  training_loss: 1.4592 (1.4591)  mae_loss: 0.2476 (0.2500)  classification_loss: 1.1910 (1.2025)  loss_mask: 0.0006 (0.0066)  time: 0.1983  data: 0.0002  max mem: 5511
[09:32:14.457886] Epoch: [72]  [720/781]  eta: 0:00:12  lr: 0.000048  training_loss: 1.4718 (1.4591)  mae_loss: 0.2573 (0.2502)  classification_loss: 1.1945 (1.2025)  loss_mask: 0.0007 (0.0064)  time: 0.1965  data: 0.0002  max mem: 5511
[09:32:18.409325] Epoch: [72]  [740/781]  eta: 0:00:08  lr: 0.000048  training_loss: 1.4039 (1.4577)  mae_loss: 0.2471 (0.2502)  classification_loss: 1.1507 (1.2012)  loss_mask: 0.0006 (0.0063)  time: 0.1975  data: 0.0003  max mem: 5511
[09:32:22.336647] Epoch: [72]  [760/781]  eta: 0:00:04  lr: 0.000048  training_loss: 1.4342 (1.4579)  mae_loss: 0.2423 (0.2501)  classification_loss: 1.2164 (1.2017)  loss_mask: 0.0005 (0.0061)  time: 0.1963  data: 0.0003  max mem: 5511
[09:32:26.274265] Epoch: [72]  [780/781]  eta: 0:00:00  lr: 0.000047  training_loss: 1.4546 (1.4579)  mae_loss: 0.2524 (0.2505)  classification_loss: 1.1935 (1.2014)  loss_mask: 0.0005 (0.0060)  time: 0.1968  data: 0.0003  max mem: 5511
[09:32:26.459560] Epoch: [72] Total time: 0:02:35 (0.1986 s / it)
[09:32:26.460033] Averaged stats: lr: 0.000047  training_loss: 1.4546 (1.4579)  mae_loss: 0.2524 (0.2505)  classification_loss: 1.1935 (1.2014)  loss_mask: 0.0005 (0.0060)
[09:32:27.160169] Test:  [  0/157]  eta: 0:01:49  testing_loss: 0.4450 (0.4450)  acc1: 89.0625 (89.0625)  acc5: 100.0000 (100.0000)  time: 0.6961  data: 0.6667  max mem: 5511
[09:32:27.449942] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.5098 (0.5096)  acc1: 84.3750 (83.8068)  acc5: 100.0000 (99.5739)  time: 0.0895  data: 0.0607  max mem: 5511
[09:32:27.734376] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4748 (0.4883)  acc1: 84.3750 (84.6726)  acc5: 100.0000 (99.5536)  time: 0.0286  data: 0.0002  max mem: 5511
[09:32:28.022527] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4839 (0.5042)  acc1: 84.3750 (84.1230)  acc5: 100.0000 (99.4456)  time: 0.0285  data: 0.0002  max mem: 5511
[09:32:28.308626] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4839 (0.5045)  acc1: 84.3750 (83.9939)  acc5: 98.4375 (99.3140)  time: 0.0286  data: 0.0002  max mem: 5511
[09:32:28.595487] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4548 (0.4944)  acc1: 85.9375 (84.7120)  acc5: 98.4375 (99.3260)  time: 0.0285  data: 0.0002  max mem: 5511
[09:32:28.879768] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4268 (0.4862)  acc1: 87.5000 (85.0666)  acc5: 100.0000 (99.3340)  time: 0.0284  data: 0.0002  max mem: 5511
[09:32:29.164514] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4493 (0.4872)  acc1: 85.9375 (85.0132)  acc5: 100.0000 (99.3398)  time: 0.0283  data: 0.0004  max mem: 5511
[09:32:29.457349] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5100 (0.4966)  acc1: 82.8125 (84.7415)  acc5: 100.0000 (99.2284)  time: 0.0287  data: 0.0003  max mem: 5511
[09:32:29.742274] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5088 (0.4924)  acc1: 84.3750 (84.9245)  acc5: 98.4375 (99.2102)  time: 0.0287  data: 0.0002  max mem: 5511
[09:32:30.026756] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.4675 (0.4938)  acc1: 87.5000 (85.0093)  acc5: 100.0000 (99.2265)  time: 0.0282  data: 0.0002  max mem: 5511
[09:32:30.312948] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5157 (0.4960)  acc1: 87.5000 (85.0084)  acc5: 100.0000 (99.1695)  time: 0.0283  data: 0.0002  max mem: 5511
[09:32:30.599811] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5158 (0.4948)  acc1: 82.8125 (84.9561)  acc5: 100.0000 (99.2252)  time: 0.0285  data: 0.0002  max mem: 5511
[09:32:30.889499] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4824 (0.4954)  acc1: 84.3750 (84.9117)  acc5: 100.0000 (99.2605)  time: 0.0287  data: 0.0002  max mem: 5511
[09:32:31.174050] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4748 (0.4933)  acc1: 85.9375 (85.0510)  acc5: 100.0000 (99.3129)  time: 0.0285  data: 0.0002  max mem: 5511
[09:32:31.455991] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4748 (0.4910)  acc1: 87.5000 (85.1200)  acc5: 100.0000 (99.3067)  time: 0.0282  data: 0.0001  max mem: 5511
[09:32:31.608056] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4609 (0.4915)  acc1: 85.9375 (85.1000)  acc5: 100.0000 (99.3200)  time: 0.0272  data: 0.0001  max mem: 5511
[09:32:31.783638] Test: Total time: 0:00:05 (0.0339 s / it)
[09:32:31.784996] * Acc@1 85.100 Acc@5 99.320 loss 0.491
[09:32:31.785495] Accuracy of the network on the 10000 test images: 85.1%
[09:32:31.785925] Max accuracy: 85.10%
[09:32:31.942033] log_dir: ./output_dir
[09:32:32.917909] Epoch: [73]  [  0/781]  eta: 0:12:40  lr: 0.000047  training_loss: 1.3945 (1.3945)  mae_loss: 0.2959 (0.2959)  classification_loss: 1.0981 (1.0981)  loss_mask: 0.0005 (0.0005)  time: 0.9740  data: 0.7214  max mem: 5511
[09:32:36.854156] Epoch: [73]  [ 20/781]  eta: 0:02:57  lr: 0.000047  training_loss: 1.4280 (1.4214)  mae_loss: 0.2543 (0.2614)  classification_loss: 1.1743 (1.1596)  loss_mask: 0.0004 (0.0004)  time: 0.1967  data: 0.0002  max mem: 5511
[09:32:40.787930] Epoch: [73]  [ 40/781]  eta: 0:02:39  lr: 0.000047  training_loss: 1.4409 (1.4367)  mae_loss: 0.2488 (0.2589)  classification_loss: 1.1685 (1.1774)  loss_mask: 0.0003 (0.0004)  time: 0.1966  data: 0.0006  max mem: 5511
[09:32:44.715724] Epoch: [73]  [ 60/781]  eta: 0:02:30  lr: 0.000047  training_loss: 1.4882 (1.4425)  mae_loss: 0.2486 (0.2567)  classification_loss: 1.2419 (1.1854)  loss_mask: 0.0003 (0.0004)  time: 0.1963  data: 0.0003  max mem: 5511
[09:32:48.663632] Epoch: [73]  [ 80/781]  eta: 0:02:24  lr: 0.000047  training_loss: 1.3832 (1.4299)  mae_loss: 0.2368 (0.2516)  classification_loss: 1.1359 (1.1780)  loss_mask: 0.0003 (0.0004)  time: 0.1973  data: 0.0003  max mem: 5511
[09:32:52.594307] Epoch: [73]  [100/781]  eta: 0:02:19  lr: 0.000047  training_loss: 1.3907 (1.4262)  mae_loss: 0.2426 (0.2492)  classification_loss: 1.1617 (1.1767)  loss_mask: 0.0003 (0.0003)  time: 0.1964  data: 0.0002  max mem: 5511
[09:32:56.524691] Epoch: [73]  [120/781]  eta: 0:02:14  lr: 0.000047  training_loss: 1.3916 (1.4202)  mae_loss: 0.2401 (0.2483)  classification_loss: 1.1512 (1.1717)  loss_mask: 0.0002 (0.0003)  time: 0.1964  data: 0.0002  max mem: 5511
[09:33:00.456307] Epoch: [73]  [140/781]  eta: 0:02:09  lr: 0.000047  training_loss: 1.4477 (1.4250)  mae_loss: 0.2380 (0.2476)  classification_loss: 1.2227 (1.1770)  loss_mask: 0.0002 (0.0003)  time: 0.1965  data: 0.0002  max mem: 5511
[09:33:04.382336] Epoch: [73]  [160/781]  eta: 0:02:05  lr: 0.000047  training_loss: 1.4356 (1.4282)  mae_loss: 0.2526 (0.2485)  classification_loss: 1.1789 (1.1794)  loss_mask: 0.0002 (0.0003)  time: 0.1962  data: 0.0002  max mem: 5511
[09:33:08.309592] Epoch: [73]  [180/781]  eta: 0:02:00  lr: 0.000047  training_loss: 1.4796 (1.4352)  mae_loss: 0.2397 (0.2490)  classification_loss: 1.2210 (1.1859)  loss_mask: 0.0002 (0.0003)  time: 0.1963  data: 0.0002  max mem: 5511
[09:33:12.268533] Epoch: [73]  [200/781]  eta: 0:01:56  lr: 0.000047  training_loss: 1.4655 (1.4363)  mae_loss: 0.2502 (0.2499)  classification_loss: 1.2170 (1.1861)  loss_mask: 0.0002 (0.0003)  time: 0.1979  data: 0.0002  max mem: 5511
[09:33:16.230824] Epoch: [73]  [220/781]  eta: 0:01:52  lr: 0.000047  training_loss: 1.5116 (1.4420)  mae_loss: 0.2472 (0.2501)  classification_loss: 1.2736 (1.1916)  loss_mask: 0.0002 (0.0003)  time: 0.1980  data: 0.0003  max mem: 5511
[09:33:20.153040] Epoch: [73]  [240/781]  eta: 0:01:48  lr: 0.000046  training_loss: 1.4344 (1.4443)  mae_loss: 0.2512 (0.2506)  classification_loss: 1.1889 (1.1935)  loss_mask: 0.0002 (0.0003)  time: 0.1960  data: 0.0002  max mem: 5511
[09:33:24.066290] Epoch: [73]  [260/781]  eta: 0:01:44  lr: 0.000046  training_loss: 1.3878 (1.4418)  mae_loss: 0.2331 (0.2496)  classification_loss: 1.1623 (1.1919)  loss_mask: 0.0002 (0.0003)  time: 0.1956  data: 0.0001  max mem: 5511
[09:33:28.015251] Epoch: [73]  [280/781]  eta: 0:01:39  lr: 0.000046  training_loss: 1.3727 (1.4398)  mae_loss: 0.2453 (0.2496)  classification_loss: 1.1236 (1.1899)  loss_mask: 0.0002 (0.0003)  time: 0.1974  data: 0.0002  max mem: 5511
[09:33:31.948475] Epoch: [73]  [300/781]  eta: 0:01:35  lr: 0.000046  training_loss: 1.4621 (1.4422)  mae_loss: 0.2615 (0.2506)  classification_loss: 1.2089 (1.1914)  loss_mask: 0.0001 (0.0003)  time: 0.1966  data: 0.0002  max mem: 5511
[09:33:35.898036] Epoch: [73]  [320/781]  eta: 0:01:31  lr: 0.000046  training_loss: 1.4446 (1.4427)  mae_loss: 0.2439 (0.2506)  classification_loss: 1.1951 (1.1919)  loss_mask: 0.0002 (0.0003)  time: 0.1974  data: 0.0002  max mem: 5511
[09:33:39.825704] Epoch: [73]  [340/781]  eta: 0:01:27  lr: 0.000046  training_loss: 1.4184 (1.4415)  mae_loss: 0.2474 (0.2507)  classification_loss: 1.1274 (1.1900)  loss_mask: 0.0013 (0.0008)  time: 0.1963  data: 0.0002  max mem: 5511
[09:33:43.767637] Epoch: [73]  [360/781]  eta: 0:01:23  lr: 0.000046  training_loss: 1.4640 (1.4431)  mae_loss: 0.2600 (0.2512)  classification_loss: 1.1873 (1.1910)  loss_mask: 0.0004 (0.0009)  time: 0.1970  data: 0.0002  max mem: 5511
[09:33:47.720881] Epoch: [73]  [380/781]  eta: 0:01:19  lr: 0.000046  training_loss: 1.4758 (1.4446)  mae_loss: 0.2505 (0.2515)  classification_loss: 1.2250 (1.1922)  loss_mask: 0.0004 (0.0009)  time: 0.1976  data: 0.0002  max mem: 5511
[09:33:51.650119] Epoch: [73]  [400/781]  eta: 0:01:15  lr: 0.000046  training_loss: 1.4964 (1.4471)  mae_loss: 0.2660 (0.2520)  classification_loss: 1.2319 (1.1939)  loss_mask: 0.0003 (0.0012)  time: 0.1964  data: 0.0002  max mem: 5511
[09:33:55.609144] Epoch: [73]  [420/781]  eta: 0:01:11  lr: 0.000046  training_loss: 1.4299 (1.4469)  mae_loss: 0.2581 (0.2522)  classification_loss: 1.1838 (1.1936)  loss_mask: 0.0002 (0.0012)  time: 0.1979  data: 0.0002  max mem: 5511
[09:33:59.533558] Epoch: [73]  [440/781]  eta: 0:01:07  lr: 0.000046  training_loss: 1.4735 (1.4481)  mae_loss: 0.2444 (0.2520)  classification_loss: 1.1951 (1.1935)  loss_mask: 0.0023 (0.0026)  time: 0.1961  data: 0.0002  max mem: 5511
[09:34:03.482997] Epoch: [73]  [460/781]  eta: 0:01:03  lr: 0.000046  training_loss: 1.4470 (1.4482)  mae_loss: 0.2358 (0.2516)  classification_loss: 1.1719 (1.1925)  loss_mask: 0.0179 (0.0041)  time: 0.1974  data: 0.0003  max mem: 5511
[09:34:07.446352] Epoch: [73]  [480/781]  eta: 0:00:59  lr: 0.000045  training_loss: 1.4177 (1.4477)  mae_loss: 0.2553 (0.2517)  classification_loss: 1.1764 (1.1918)  loss_mask: 0.0031 (0.0041)  time: 0.1981  data: 0.0002  max mem: 5511
[09:34:11.390500] Epoch: [73]  [500/781]  eta: 0:00:55  lr: 0.000045  training_loss: 1.5037 (1.4489)  mae_loss: 0.2625 (0.2521)  classification_loss: 1.2213 (1.1928)  loss_mask: 0.0011 (0.0040)  time: 0.1971  data: 0.0003  max mem: 5511
[09:34:15.345153] Epoch: [73]  [520/781]  eta: 0:00:51  lr: 0.000045  training_loss: 1.4652 (1.4493)  mae_loss: 0.2566 (0.2522)  classification_loss: 1.2075 (1.1932)  loss_mask: 0.0005 (0.0039)  time: 0.1976  data: 0.0003  max mem: 5511
[09:34:19.306448] Epoch: [73]  [540/781]  eta: 0:00:47  lr: 0.000045  training_loss: 1.3891 (1.4486)  mae_loss: 0.2450 (0.2521)  classification_loss: 1.1411 (1.1927)  loss_mask: 0.0005 (0.0038)  time: 0.1980  data: 0.0002  max mem: 5511
[09:34:23.237277] Epoch: [73]  [560/781]  eta: 0:00:43  lr: 0.000045  training_loss: 1.4158 (1.4470)  mae_loss: 0.2524 (0.2519)  classification_loss: 1.1559 (1.1915)  loss_mask: 0.0004 (0.0037)  time: 0.1965  data: 0.0002  max mem: 5511
[09:34:27.224506] Epoch: [73]  [580/781]  eta: 0:00:39  lr: 0.000045  training_loss: 1.4867 (1.4472)  mae_loss: 0.2463 (0.2521)  classification_loss: 1.1771 (1.1915)  loss_mask: 0.0003 (0.0035)  time: 0.1993  data: 0.0004  max mem: 5511
[09:34:31.167334] Epoch: [73]  [600/781]  eta: 0:00:35  lr: 0.000045  training_loss: 1.4357 (1.4468)  mae_loss: 0.2433 (0.2519)  classification_loss: 1.1802 (1.1915)  loss_mask: 0.0003 (0.0034)  time: 0.1971  data: 0.0002  max mem: 5511
[09:34:35.110243] Epoch: [73]  [620/781]  eta: 0:00:31  lr: 0.000045  training_loss: 1.4149 (1.4467)  mae_loss: 0.2569 (0.2520)  classification_loss: 1.1541 (1.1913)  loss_mask: 0.0008 (0.0034)  time: 0.1971  data: 0.0002  max mem: 5511
[09:34:39.026048] Epoch: [73]  [640/781]  eta: 0:00:27  lr: 0.000045  training_loss: 1.4556 (1.4470)  mae_loss: 0.2545 (0.2524)  classification_loss: 1.1762 (1.1912)  loss_mask: 0.0004 (0.0033)  time: 0.1957  data: 0.0002  max mem: 5511
[09:34:42.975890] Epoch: [73]  [660/781]  eta: 0:00:23  lr: 0.000045  training_loss: 1.4238 (1.4469)  mae_loss: 0.2509 (0.2524)  classification_loss: 1.1903 (1.1913)  loss_mask: 0.0002 (0.0032)  time: 0.1974  data: 0.0002  max mem: 5511
[09:34:46.925130] Epoch: [73]  [680/781]  eta: 0:00:20  lr: 0.000045  training_loss: 1.4330 (1.4466)  mae_loss: 0.2415 (0.2521)  classification_loss: 1.1857 (1.1913)  loss_mask: 0.0002 (0.0031)  time: 0.1974  data: 0.0002  max mem: 5511
[09:34:50.868152] Epoch: [73]  [700/781]  eta: 0:00:16  lr: 0.000045  training_loss: 1.4567 (1.4474)  mae_loss: 0.2504 (0.2523)  classification_loss: 1.1961 (1.1920)  loss_mask: 0.0001 (0.0031)  time: 0.1971  data: 0.0002  max mem: 5511
[09:34:54.854280] Epoch: [73]  [720/781]  eta: 0:00:12  lr: 0.000044  training_loss: 1.4690 (1.4485)  mae_loss: 0.2525 (0.2524)  classification_loss: 1.2172 (1.1932)  loss_mask: 0.0001 (0.0030)  time: 0.1992  data: 0.0002  max mem: 5511
[09:34:58.789535] Epoch: [73]  [740/781]  eta: 0:00:08  lr: 0.000044  training_loss: 1.3995 (1.4484)  mae_loss: 0.2567 (0.2526)  classification_loss: 1.1664 (1.1929)  loss_mask: 0.0002 (0.0029)  time: 0.1967  data: 0.0002  max mem: 5511
[09:35:02.722857] Epoch: [73]  [760/781]  eta: 0:00:04  lr: 0.000044  training_loss: 1.4825 (1.4491)  mae_loss: 0.2629 (0.2529)  classification_loss: 1.2234 (1.1934)  loss_mask: 0.0002 (0.0028)  time: 0.1966  data: 0.0002  max mem: 5511
[09:35:06.690576] Epoch: [73]  [780/781]  eta: 0:00:00  lr: 0.000044  training_loss: 1.4126 (1.4486)  mae_loss: 0.2607 (0.2530)  classification_loss: 1.1872 (1.1928)  loss_mask: 0.0002 (0.0028)  time: 0.1983  data: 0.0002  max mem: 5511
[09:35:06.891099] Epoch: [73] Total time: 0:02:34 (0.1984 s / it)
[09:35:06.891810] Averaged stats: lr: 0.000044  training_loss: 1.4126 (1.4486)  mae_loss: 0.2607 (0.2530)  classification_loss: 1.1872 (1.1928)  loss_mask: 0.0002 (0.0028)
[09:35:07.483269] Test:  [  0/157]  eta: 0:01:32  testing_loss: 0.4146 (0.4146)  acc1: 93.7500 (93.7500)  acc5: 100.0000 (100.0000)  time: 0.5876  data: 0.5580  max mem: 5511
[09:35:07.767377] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.4950 (0.4998)  acc1: 84.3750 (84.6591)  acc5: 100.0000 (99.8580)  time: 0.0791  data: 0.0509  max mem: 5511
[09:35:08.051060] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4706 (0.4825)  acc1: 84.3750 (85.0446)  acc5: 100.0000 (99.8512)  time: 0.0283  data: 0.0002  max mem: 5511
[09:35:08.333362] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.4649 (0.4991)  acc1: 84.3750 (84.1230)  acc5: 100.0000 (99.6976)  time: 0.0282  data: 0.0002  max mem: 5511
[09:35:08.621311] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.4766 (0.5002)  acc1: 84.3750 (84.2607)  acc5: 100.0000 (99.5808)  time: 0.0284  data: 0.0002  max mem: 5511
[09:35:08.905382] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4766 (0.4944)  acc1: 84.3750 (84.5588)  acc5: 100.0000 (99.5404)  time: 0.0285  data: 0.0002  max mem: 5511
[09:35:09.191140] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4555 (0.4875)  acc1: 84.3750 (84.8105)  acc5: 100.0000 (99.4877)  time: 0.0283  data: 0.0002  max mem: 5511
[09:35:09.478106] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4678 (0.4871)  acc1: 85.9375 (84.9472)  acc5: 100.0000 (99.4718)  time: 0.0285  data: 0.0002  max mem: 5511
[09:35:09.762303] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5005 (0.4939)  acc1: 84.3750 (84.7415)  acc5: 100.0000 (99.4599)  time: 0.0284  data: 0.0002  max mem: 5511
[09:35:10.046463] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4834 (0.4896)  acc1: 84.3750 (84.9073)  acc5: 100.0000 (99.4505)  time: 0.0283  data: 0.0002  max mem: 5511
[09:35:10.331285] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4680 (0.4908)  acc1: 84.3750 (84.9629)  acc5: 100.0000 (99.4431)  time: 0.0283  data: 0.0002  max mem: 5511
[09:35:10.614592] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5122 (0.4931)  acc1: 84.3750 (84.9662)  acc5: 100.0000 (99.4229)  time: 0.0283  data: 0.0002  max mem: 5511
[09:35:10.897516] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5002 (0.4927)  acc1: 85.9375 (85.0207)  acc5: 100.0000 (99.4318)  time: 0.0282  data: 0.0002  max mem: 5511
[09:35:11.180644] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4806 (0.4928)  acc1: 84.3750 (84.9594)  acc5: 100.0000 (99.4394)  time: 0.0282  data: 0.0002  max mem: 5511
[09:35:11.463583] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4721 (0.4894)  acc1: 82.8125 (85.0510)  acc5: 100.0000 (99.4681)  time: 0.0282  data: 0.0002  max mem: 5511
[09:35:11.743505] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4721 (0.4880)  acc1: 84.3750 (85.0683)  acc5: 100.0000 (99.4516)  time: 0.0280  data: 0.0001  max mem: 5511
[09:35:11.895691] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4376 (0.4873)  acc1: 85.9375 (85.0600)  acc5: 100.0000 (99.4500)  time: 0.0271  data: 0.0001  max mem: 5511
[09:35:12.047453] Test: Total time: 0:00:05 (0.0328 s / it)
[09:35:12.048015] * Acc@1 85.060 Acc@5 99.450 loss 0.487
[09:35:12.048363] Accuracy of the network on the 10000 test images: 85.1%
[09:35:12.048541] Max accuracy: 85.10%
[09:35:12.314383] log_dir: ./output_dir
[09:35:13.325552] Epoch: [74]  [  0/781]  eta: 0:13:08  lr: 0.000044  training_loss: 1.3737 (1.3737)  mae_loss: 0.2639 (0.2639)  classification_loss: 1.1097 (1.1097)  loss_mask: 0.0001 (0.0001)  time: 1.0095  data: 0.7884  max mem: 5511
[09:35:17.271741] Epoch: [74]  [ 20/781]  eta: 0:02:59  lr: 0.000044  training_loss: 1.4668 (1.4431)  mae_loss: 0.2522 (0.2552)  classification_loss: 1.1747 (1.1878)  loss_mask: 0.0001 (0.0002)  time: 0.1972  data: 0.0002  max mem: 5511
[09:35:21.202006] Epoch: [74]  [ 40/781]  eta: 0:02:40  lr: 0.000044  training_loss: 1.4733 (1.4515)  mae_loss: 0.2511 (0.2548)  classification_loss: 1.2125 (1.1966)  loss_mask: 0.0001 (0.0001)  time: 0.1964  data: 0.0003  max mem: 5511
[09:35:25.125358] Epoch: [74]  [ 60/781]  eta: 0:02:31  lr: 0.000044  training_loss: 1.4306 (1.4548)  mae_loss: 0.2415 (0.2534)  classification_loss: 1.2257 (1.2012)  loss_mask: 0.0002 (0.0002)  time: 0.1961  data: 0.0002  max mem: 5511
[09:35:29.105738] Epoch: [74]  [ 80/781]  eta: 0:02:25  lr: 0.000044  training_loss: 1.4013 (1.4497)  mae_loss: 0.2474 (0.2553)  classification_loss: 1.1501 (1.1942)  loss_mask: 0.0001 (0.0002)  time: 0.1989  data: 0.0002  max mem: 5511
[09:35:33.069134] Epoch: [74]  [100/781]  eta: 0:02:19  lr: 0.000044  training_loss: 1.4416 (1.4523)  mae_loss: 0.2578 (0.2547)  classification_loss: 1.2213 (1.1975)  loss_mask: 0.0001 (0.0002)  time: 0.1981  data: 0.0002  max mem: 5511
[09:35:37.002133] Epoch: [74]  [120/781]  eta: 0:02:14  lr: 0.000044  training_loss: 1.3830 (1.4453)  mae_loss: 0.2467 (0.2536)  classification_loss: 1.1359 (1.1915)  loss_mask: 0.0002 (0.0002)  time: 0.1966  data: 0.0002  max mem: 5511
[09:35:40.948831] Epoch: [74]  [140/781]  eta: 0:02:10  lr: 0.000044  training_loss: 1.4110 (1.4429)  mae_loss: 0.2497 (0.2549)  classification_loss: 1.1517 (1.1878)  loss_mask: 0.0001 (0.0002)  time: 0.1972  data: 0.0004  max mem: 5511
[09:35:44.893425] Epoch: [74]  [160/781]  eta: 0:02:05  lr: 0.000044  training_loss: 1.3858 (1.4402)  mae_loss: 0.2467 (0.2535)  classification_loss: 1.1278 (1.1866)  loss_mask: 0.0001 (0.0002)  time: 0.1971  data: 0.0002  max mem: 5511
[09:35:48.845030] Epoch: [74]  [180/781]  eta: 0:02:01  lr: 0.000044  training_loss: 1.4871 (1.4439)  mae_loss: 0.2487 (0.2542)  classification_loss: 1.2241 (1.1895)  loss_mask: 0.0001 (0.0002)  time: 0.1975  data: 0.0002  max mem: 5511
[09:35:52.861263] Epoch: [74]  [200/781]  eta: 0:01:57  lr: 0.000043  training_loss: 1.4524 (1.4440)  mae_loss: 0.2457 (0.2535)  classification_loss: 1.2032 (1.1904)  loss_mask: 0.0001 (0.0002)  time: 0.2007  data: 0.0002  max mem: 5511
[09:35:56.806671] Epoch: [74]  [220/781]  eta: 0:01:52  lr: 0.000043  training_loss: 1.4560 (1.4452)  mae_loss: 0.2493 (0.2530)  classification_loss: 1.1945 (1.1920)  loss_mask: 0.0001 (0.0002)  time: 0.1972  data: 0.0002  max mem: 5511
[09:36:00.774536] Epoch: [74]  [240/781]  eta: 0:01:48  lr: 0.000043  training_loss: 1.4494 (1.4466)  mae_loss: 0.2693 (0.2541)  classification_loss: 1.1721 (1.1923)  loss_mask: 0.0001 (0.0002)  time: 0.1983  data: 0.0002  max mem: 5511
[09:36:04.734404] Epoch: [74]  [260/781]  eta: 0:01:44  lr: 0.000043  training_loss: 1.4532 (1.4463)  mae_loss: 0.2273 (0.2530)  classification_loss: 1.2139 (1.1931)  loss_mask: 0.0001 (0.0002)  time: 0.1979  data: 0.0004  max mem: 5511
[09:36:08.760424] Epoch: [74]  [280/781]  eta: 0:01:40  lr: 0.000043  training_loss: 1.4599 (1.4461)  mae_loss: 0.2634 (0.2534)  classification_loss: 1.1836 (1.1926)  loss_mask: 0.0001 (0.0002)  time: 0.2012  data: 0.0002  max mem: 5511
[09:36:12.718614] Epoch: [74]  [300/781]  eta: 0:01:36  lr: 0.000043  training_loss: 1.4552 (1.4468)  mae_loss: 0.2400 (0.2528)  classification_loss: 1.2157 (1.1938)  loss_mask: 0.0001 (0.0002)  time: 0.1978  data: 0.0003  max mem: 5511
[09:36:16.658113] Epoch: [74]  [320/781]  eta: 0:01:32  lr: 0.000043  training_loss: 1.4371 (1.4483)  mae_loss: 0.2533 (0.2526)  classification_loss: 1.1832 (1.1955)  loss_mask: 0.0001 (0.0002)  time: 0.1969  data: 0.0002  max mem: 5511
[09:36:20.604507] Epoch: [74]  [340/781]  eta: 0:01:28  lr: 0.000043  training_loss: 1.4301 (1.4467)  mae_loss: 0.2447 (0.2526)  classification_loss: 1.1737 (1.1940)  loss_mask: 0.0001 (0.0001)  time: 0.1972  data: 0.0002  max mem: 5511
[09:36:24.544334] Epoch: [74]  [360/781]  eta: 0:01:24  lr: 0.000043  training_loss: 1.4480 (1.4466)  mae_loss: 0.2404 (0.2523)  classification_loss: 1.1874 (1.1942)  loss_mask: 0.0001 (0.0001)  time: 0.1969  data: 0.0002  max mem: 5511
[09:36:28.502073] Epoch: [74]  [380/781]  eta: 0:01:20  lr: 0.000043  training_loss: 1.4311 (1.4476)  mae_loss: 0.2533 (0.2524)  classification_loss: 1.2025 (1.1951)  loss_mask: 0.0001 (0.0001)  time: 0.1978  data: 0.0003  max mem: 5511
[09:36:32.444453] Epoch: [74]  [400/781]  eta: 0:01:16  lr: 0.000043  training_loss: 1.4501 (1.4468)  mae_loss: 0.2542 (0.2527)  classification_loss: 1.1550 (1.1937)  loss_mask: 0.0001 (0.0004)  time: 0.1970  data: 0.0003  max mem: 5511
[09:36:36.414236] Epoch: [74]  [420/781]  eta: 0:01:12  lr: 0.000043  training_loss: 1.4074 (1.4464)  mae_loss: 0.2458 (0.2528)  classification_loss: 1.1683 (1.1933)  loss_mask: 0.0004 (0.0004)  time: 0.1984  data: 0.0002  max mem: 5511
[09:36:40.362104] Epoch: [74]  [440/781]  eta: 0:01:08  lr: 0.000043  training_loss: 1.4329 (1.4459)  mae_loss: 0.2507 (0.2526)  classification_loss: 1.1986 (1.1929)  loss_mask: 0.0001 (0.0004)  time: 0.1973  data: 0.0002  max mem: 5511
[09:36:44.297904] Epoch: [74]  [460/781]  eta: 0:01:04  lr: 0.000042  training_loss: 1.3675 (1.4439)  mae_loss: 0.2367 (0.2521)  classification_loss: 1.1313 (1.1915)  loss_mask: 0.0002 (0.0004)  time: 0.1967  data: 0.0003  max mem: 5511
[09:36:48.279397] Epoch: [74]  [480/781]  eta: 0:01:00  lr: 0.000042  training_loss: 1.4310 (1.4432)  mae_loss: 0.2328 (0.2513)  classification_loss: 1.2059 (1.1915)  loss_mask: 0.0001 (0.0004)  time: 0.1990  data: 0.0003  max mem: 5511
[09:36:52.236947] Epoch: [74]  [500/781]  eta: 0:00:56  lr: 0.000042  training_loss: 1.4241 (1.4422)  mae_loss: 0.2512 (0.2512)  classification_loss: 1.1699 (1.1905)  loss_mask: 0.0001 (0.0004)  time: 0.1978  data: 0.0002  max mem: 5511
[09:36:56.193762] Epoch: [74]  [520/781]  eta: 0:00:52  lr: 0.000042  training_loss: 1.4501 (1.4419)  mae_loss: 0.2408 (0.2509)  classification_loss: 1.1915 (1.1906)  loss_mask: 0.0001 (0.0004)  time: 0.1978  data: 0.0003  max mem: 5511
[09:37:00.140669] Epoch: [74]  [540/781]  eta: 0:00:48  lr: 0.000042  training_loss: 1.4835 (1.4434)  mae_loss: 0.2652 (0.2512)  classification_loss: 1.2142 (1.1919)  loss_mask: 0.0001 (0.0004)  time: 0.1973  data: 0.0002  max mem: 5511
[09:37:04.114582] Epoch: [74]  [560/781]  eta: 0:00:44  lr: 0.000042  training_loss: 1.4163 (1.4427)  mae_loss: 0.2520 (0.2512)  classification_loss: 1.1968 (1.1911)  loss_mask: 0.0001 (0.0004)  time: 0.1986  data: 0.0002  max mem: 5511

[09:37:08.059761] Epoch: [74]  [580/781]  eta: 0:00:40  lr: 0.000042  training_loss: 1.4430 (1.4429)  mae_loss: 0.2583 (0.2516)  classification_loss: 1.1994 (1.1910)  loss_mask: 0.0001 (0.0004)  time: 0.1971  data: 0.0002  max mem: 5511
[09:37:12.008190] Epoch: [74]  [600/781]  eta: 0:00:36  lr: 0.000042  training_loss: 1.4161 (1.4427)  mae_loss: 0.2352 (0.2515)  classification_loss: 1.1809 (1.1909)  loss_mask: 0.0001 (0.0004)  time: 0.1973  data: 0.0002  max mem: 5511
[09:37:15.955517] Epoch: [74]  [620/781]  eta: 0:00:32  lr: 0.000042  training_loss: 1.4344 (1.4434)  mae_loss: 0.2561 (0.2516)  classification_loss: 1.2023 (1.1915)  loss_mask: 0.0001 (0.0003)  time: 0.1973  data: 0.0003  max mem: 5511
[09:37:19.905732] Epoch: [74]  [640/781]  eta: 0:00:28  lr: 0.000042  training_loss: 1.4646 (1.4436)  mae_loss: 0.2509 (0.2516)  classification_loss: 1.2076 (1.1917)  loss_mask: 0.0001 (0.0003)  time: 0.1974  data: 0.0003  max mem: 5511
[09:37:23.845049] Epoch: [74]  [660/781]  eta: 0:00:24  lr: 0.000042  training_loss: 1.4676 (1.4444)  mae_loss: 0.2557 (0.2518)  classification_loss: 1.1929 (1.1922)  loss_mask: 0.0003 (0.0005)  time: 0.1969  data: 0.0002  max mem: 5511
[09:37:27.819931] Epoch: [74]  [680/781]  eta: 0:00:20  lr: 0.000042  training_loss: 1.4531 (1.4444)  mae_loss: 0.2489 (0.2517)  classification_loss: 1.2181 (1.1922)  loss_mask: 0.0003 (0.0005)  time: 0.1987  data: 0.0003  max mem: 5511
[09:37:31.778646] Epoch: [74]  [700/781]  eta: 0:00:16  lr: 0.000041  training_loss: 1.4514 (1.4448)  mae_loss: 0.2451 (0.2518)  classification_loss: 1.1962 (1.1925)  loss_mask: 0.0001 (0.0005)  time: 0.1978  data: 0.0002  max mem: 5511
[09:37:35.709723] Epoch: [74]  [720/781]  eta: 0:00:12  lr: 0.000041  training_loss: 1.3792 (1.4440)  mae_loss: 0.2317 (0.2514)  classification_loss: 1.1517 (1.1922)  loss_mask: 0.0001 (0.0005)  time: 0.1965  data: 0.0002  max mem: 5511
[09:37:39.677370] Epoch: [74]  [740/781]  eta: 0:00:08  lr: 0.000041  training_loss: 1.4204 (1.4433)  mae_loss: 0.2536 (0.2515)  classification_loss: 1.1396 (1.1913)  loss_mask: 0.0001 (0.0004)  time: 0.1983  data: 0.0002  max mem: 5511
[09:37:43.637655] Epoch: [74]  [760/781]  eta: 0:00:04  lr: 0.000041  training_loss: 1.4208 (1.4430)  mae_loss: 0.2520 (0.2515)  classification_loss: 1.1738 (1.1911)  loss_mask: 0.0001 (0.0004)  time: 0.1979  data: 0.0002  max mem: 5511
[09:37:47.573612] Epoch: [74]  [780/781]  eta: 0:00:00  lr: 0.000041  training_loss: 1.4811 (1.4438)  mae_loss: 0.2535 (0.2515)  classification_loss: 1.2030 (1.1919)  loss_mask: 0.0001 (0.0004)  time: 0.1967  data: 0.0002  max mem: 5511
[09:37:47.759611] Epoch: [74] Total time: 0:02:35 (0.1990 s / it)
[09:37:47.761457] Averaged stats: lr: 0.000041  training_loss: 1.4811 (1.4438)  mae_loss: 0.2535 (0.2515)  classification_loss: 1.2030 (1.1919)  loss_mask: 0.0001 (0.0004)
[09:37:48.403575] Test:  [  0/157]  eta: 0:01:40  testing_loss: 0.4515 (0.4515)  acc1: 87.5000 (87.5000)  acc5: 96.8750 (96.8750)  time: 0.6370  data: 0.5862  max mem: 5511
[09:37:48.701262] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5136 (0.5172)  acc1: 85.9375 (85.0852)  acc5: 100.0000 (99.1477)  time: 0.0848  data: 0.0539  max mem: 5511
[09:37:48.993813] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4701 (0.4864)  acc1: 85.9375 (85.8631)  acc5: 100.0000 (99.4048)  time: 0.0293  data: 0.0004  max mem: 5511
[09:37:49.280589] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4692 (0.4981)  acc1: 85.9375 (85.1815)  acc5: 100.0000 (99.1935)  time: 0.0288  data: 0.0002  max mem: 5511
[09:37:49.572976] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4998 (0.5053)  acc1: 84.3750 (85.0229)  acc5: 98.4375 (99.1616)  time: 0.0288  data: 0.0002  max mem: 5511
[09:37:49.861117] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4704 (0.4954)  acc1: 84.3750 (85.5086)  acc5: 100.0000 (99.2647)  time: 0.0288  data: 0.0002  max mem: 5511
[09:37:50.157752] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4539 (0.4920)  acc1: 85.9375 (85.5533)  acc5: 100.0000 (99.3084)  time: 0.0291  data: 0.0002  max mem: 5511
[09:37:50.453746] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4654 (0.4930)  acc1: 85.9375 (85.5194)  acc5: 100.0000 (99.3178)  time: 0.0295  data: 0.0002  max mem: 5511
[09:37:50.744282] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5188 (0.5002)  acc1: 84.3750 (85.3202)  acc5: 98.4375 (99.2477)  time: 0.0292  data: 0.0002  max mem: 5511
[09:37:51.031480] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4688 (0.4945)  acc1: 87.5000 (85.4911)  acc5: 98.4375 (99.2445)  time: 0.0287  data: 0.0002  max mem: 5511
[09:37:51.320406] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4688 (0.4958)  acc1: 87.5000 (85.5043)  acc5: 98.4375 (99.2110)  time: 0.0287  data: 0.0002  max mem: 5511
[09:37:51.608570] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5089 (0.4981)  acc1: 85.9375 (85.5011)  acc5: 98.4375 (99.1976)  time: 0.0287  data: 0.0002  max mem: 5511
[09:37:51.897896] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5089 (0.4963)  acc1: 85.9375 (85.5243)  acc5: 100.0000 (99.1994)  time: 0.0287  data: 0.0002  max mem: 5511
[09:37:52.187098] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4934 (0.4971)  acc1: 84.3750 (85.4127)  acc5: 100.0000 (99.2128)  time: 0.0288  data: 0.0002  max mem: 5511
[09:37:52.472652] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4934 (0.4952)  acc1: 85.9375 (85.5386)  acc5: 100.0000 (99.2465)  time: 0.0286  data: 0.0002  max mem: 5511
[09:37:52.756914] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4749 (0.4929)  acc1: 85.9375 (85.5132)  acc5: 100.0000 (99.2550)  time: 0.0283  data: 0.0002  max mem: 5511
[09:37:52.911073] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4670 (0.4933)  acc1: 85.9375 (85.4900)  acc5: 100.0000 (99.2700)  time: 0.0274  data: 0.0002  max mem: 5511
[09:37:53.068676] Test: Total time: 0:00:05 (0.0338 s / it)
[09:37:53.069160] * Acc@1 85.490 Acc@5 99.270 loss 0.493
[09:37:53.069452] Accuracy of the network on the 10000 test images: 85.5%
[09:37:53.069668] Max accuracy: 85.49%
[09:37:53.264642] log_dir: ./output_dir
[09:37:54.111440] Epoch: [75]  [  0/781]  eta: 0:10:59  lr: 0.000041  training_loss: 1.4519 (1.4519)  mae_loss: 0.2615 (0.2615)  classification_loss: 1.1903 (1.1903)  loss_mask: 0.0000 (0.0000)  time: 0.8442  data: 0.6294  max mem: 5511
[09:37:58.100329] Epoch: [75]  [ 20/781]  eta: 0:02:55  lr: 0.000041  training_loss: 1.4113 (1.4166)  mae_loss: 0.2486 (0.2535)  classification_loss: 1.1630 (1.1630)  loss_mask: 0.0001 (0.0001)  time: 0.1993  data: 0.0002  max mem: 5511
[09:38:02.074240] Epoch: [75]  [ 40/781]  eta: 0:02:39  lr: 0.000041  training_loss: 1.4316 (1.4305)  mae_loss: 0.2382 (0.2487)  classification_loss: 1.1981 (1.1817)  loss_mask: 0.0001 (0.0001)  time: 0.1986  data: 0.0003  max mem: 5511
[09:38:06.010981] Epoch: [75]  [ 60/781]  eta: 0:02:30  lr: 0.000041  training_loss: 1.4082 (1.4294)  mae_loss: 0.2401 (0.2462)  classification_loss: 1.1729 (1.1831)  loss_mask: 0.0001 (0.0001)  time: 0.1968  data: 0.0002  max mem: 5511
[09:38:09.936300] Epoch: [75]  [ 80/781]  eta: 0:02:24  lr: 0.000041  training_loss: 1.4670 (1.4336)  mae_loss: 0.2441 (0.2463)  classification_loss: 1.2193 (1.1872)  loss_mask: 0.0001 (0.0001)  time: 0.1962  data: 0.0002  max mem: 5511
[09:38:13.880529] Epoch: [75]  [100/781]  eta: 0:02:18  lr: 0.000041  training_loss: 1.4395 (1.4373)  mae_loss: 0.2574 (0.2489)  classification_loss: 1.1330 (1.1883)  loss_mask: 0.0001 (0.0001)  time: 0.1971  data: 0.0003  max mem: 5511
[09:38:17.848658] Epoch: [75]  [120/781]  eta: 0:02:14  lr: 0.000041  training_loss: 1.4444 (1.4375)  mae_loss: 0.2535 (0.2500)  classification_loss: 1.1682 (1.1874)  loss_mask: 0.0001 (0.0001)  time: 0.1983  data: 0.0002  max mem: 5511
[09:38:21.808536] Epoch: [75]  [140/781]  eta: 0:02:09  lr: 0.000041  training_loss: 1.4175 (1.4323)  mae_loss: 0.2465 (0.2509)  classification_loss: 1.1655 (1.1813)  loss_mask: 0.0001 (0.0001)  time: 0.1979  data: 0.0002  max mem: 5511
[09:38:25.773861] Epoch: [75]  [160/781]  eta: 0:02:05  lr: 0.000041  training_loss: 1.4301 (1.4328)  mae_loss: 0.2574 (0.2505)  classification_loss: 1.1611 (1.1822)  loss_mask: 0.0001 (0.0001)  time: 0.1981  data: 0.0003  max mem: 5511
[09:38:29.715319] Epoch: [75]  [180/781]  eta: 0:02:00  lr: 0.000040  training_loss: 1.4046 (1.4326)  mae_loss: 0.2461 (0.2509)  classification_loss: 1.1451 (1.1815)  loss_mask: 0.0001 (0.0001)  time: 0.1970  data: 0.0004  max mem: 5511
[09:38:33.670215] Epoch: [75]  [200/781]  eta: 0:01:56  lr: 0.000040  training_loss: 1.4522 (1.4338)  mae_loss: 0.2463 (0.2511)  classification_loss: 1.2073 (1.1826)  loss_mask: 0.0001 (0.0001)  time: 0.1977  data: 0.0005  max mem: 5511
[09:38:37.629906] Epoch: [75]  [220/781]  eta: 0:01:52  lr: 0.000040  training_loss: 1.4593 (1.4370)  mae_loss: 0.2458 (0.2509)  classification_loss: 1.2176 (1.1860)  loss_mask: 0.0001 (0.0001)  time: 0.1979  data: 0.0003  max mem: 5511
[09:38:41.573157] Epoch: [75]  [240/781]  eta: 0:01:48  lr: 0.000040  training_loss: 1.4644 (1.4385)  mae_loss: 0.2500 (0.2511)  classification_loss: 1.2289 (1.1872)  loss_mask: 0.0001 (0.0001)  time: 0.1971  data: 0.0003  max mem: 5511
[09:38:45.553812] Epoch: [75]  [260/781]  eta: 0:01:44  lr: 0.000040  training_loss: 1.4595 (1.4404)  mae_loss: 0.2470 (0.2513)  classification_loss: 1.1917 (1.1889)  loss_mask: 0.0001 (0.0001)  time: 0.1989  data: 0.0002  max mem: 5511
[09:38:49.514199] Epoch: [75]  [280/781]  eta: 0:01:40  lr: 0.000040  training_loss: 1.4578 (1.4405)  mae_loss: 0.2506 (0.2514)  classification_loss: 1.1977 (1.1890)  loss_mask: 0.0001 (0.0001)  time: 0.1979  data: 0.0002  max mem: 5511
[09:38:53.482864] Epoch: [75]  [300/781]  eta: 0:01:36  lr: 0.000040  training_loss: 1.4237 (1.4420)  mae_loss: 0.2595 (0.2516)  classification_loss: 1.1698 (1.1903)  loss_mask: 0.0001 (0.0001)  time: 0.1984  data: 0.0002  max mem: 5511
[09:38:57.428938] Epoch: [75]  [320/781]  eta: 0:01:32  lr: 0.000040  training_loss: 1.4194 (1.4426)  mae_loss: 0.2349 (0.2511)  classification_loss: 1.1937 (1.1915)  loss_mask: 0.0001 (0.0001)  time: 0.1972  data: 0.0003  max mem: 5511
[09:39:01.388195] Epoch: [75]  [340/781]  eta: 0:01:28  lr: 0.000040  training_loss: 1.3829 (1.4407)  mae_loss: 0.2496 (0.2511)  classification_loss: 1.1269 (1.1895)  loss_mask: 0.0001 (0.0001)  time: 0.1979  data: 0.0003  max mem: 5511
[09:39:05.318804] Epoch: [75]  [360/781]  eta: 0:01:23  lr: 0.000040  training_loss: 1.4459 (1.4409)  mae_loss: 0.2577 (0.2514)  classification_loss: 1.1958 (1.1894)  loss_mask: 0.0001 (0.0001)  time: 0.1964  data: 0.0002  max mem: 5511
[09:39:09.263682] Epoch: [75]  [380/781]  eta: 0:01:19  lr: 0.000040  training_loss: 1.4493 (1.4419)  mae_loss: 0.2480 (0.2515)  classification_loss: 1.2047 (1.1903)  loss_mask: 0.0001 (0.0001)  time: 0.1971  data: 0.0002  max mem: 5511
[09:39:13.238725] Epoch: [75]  [400/781]  eta: 0:01:15  lr: 0.000040  training_loss: 1.4730 (1.4427)  mae_loss: 0.2537 (0.2519)  classification_loss: 1.1904 (1.1906)  loss_mask: 0.0001 (0.0001)  time: 0.1987  data: 0.0003  max mem: 5511
[09:39:17.175632] Epoch: [75]  [420/781]  eta: 0:01:11  lr: 0.000040  training_loss: 1.4603 (1.4440)  mae_loss: 0.2473 (0.2522)  classification_loss: 1.2141 (1.1916)  loss_mask: 0.0001 (0.0001)  time: 0.1968  data: 0.0002  max mem: 5511
[09:39:21.134735] Epoch: [75]  [440/781]  eta: 0:01:07  lr: 0.000039  training_loss: 1.4304 (1.4418)  mae_loss: 0.2455 (0.2521)  classification_loss: 1.1538 (1.1897)  loss_mask: 0.0001 (0.0001)  time: 0.1979  data: 0.0003  max mem: 5511
[09:39:25.072107] Epoch: [75]  [460/781]  eta: 0:01:03  lr: 0.000039  training_loss: 1.3269 (1.4384)  mae_loss: 0.2387 (0.2515)  classification_loss: 1.0997 (1.1868)  loss_mask: 0.0001 (0.0001)  time: 0.1968  data: 0.0003  max mem: 5511
[09:39:29.031483] Epoch: [75]  [480/781]  eta: 0:00:59  lr: 0.000039  training_loss: 1.4206 (1.4380)  mae_loss: 0.2330 (0.2511)  classification_loss: 1.1788 (1.1868)  loss_mask: 0.0001 (0.0001)  time: 0.1979  data: 0.0002  max mem: 5511
[09:39:32.967070] Epoch: [75]  [500/781]  eta: 0:00:55  lr: 0.000039  training_loss: 1.4519 (1.4384)  mae_loss: 0.2464 (0.2514)  classification_loss: 1.2123 (1.1869)  loss_mask: 0.0001 (0.0001)  time: 0.1967  data: 0.0002  max mem: 5511
[09:39:36.904067] Epoch: [75]  [520/781]  eta: 0:00:51  lr: 0.000039  training_loss: 1.3796 (1.4374)  mae_loss: 0.2452 (0.2513)  classification_loss: 1.1338 (1.1860)  loss_mask: 0.0000 (0.0001)  time: 0.1968  data: 0.0002  max mem: 5511
[09:39:40.823853] Epoch: [75]  [540/781]  eta: 0:00:47  lr: 0.000039  training_loss: 1.4490 (1.4381)  mae_loss: 0.2508 (0.2515)  classification_loss: 1.2103 (1.1865)  loss_mask: 0.0001 (0.0001)  time: 0.1959  data: 0.0002  max mem: 5511
[09:39:44.759143] Epoch: [75]  [560/781]  eta: 0:00:43  lr: 0.000039  training_loss: 1.4715 (1.4386)  mae_loss: 0.2446 (0.2518)  classification_loss: 1.2197 (1.1868)  loss_mask: 0.0001 (0.0001)  time: 0.1967  data: 0.0002  max mem: 5511
[09:39:48.719697] Epoch: [75]  [580/781]  eta: 0:00:39  lr: 0.000039  training_loss: 1.4032 (1.4371)  mae_loss: 0.2370 (0.2515)  classification_loss: 1.1347 (1.1855)  loss_mask: 0.0001 (0.0001)  time: 0.1979  data: 0.0002  max mem: 5511
[09:39:52.688742] Epoch: [75]  [600/781]  eta: 0:00:35  lr: 0.000039  training_loss: 1.4086 (1.4368)  mae_loss: 0.2387 (0.2512)  classification_loss: 1.1796 (1.1855)  loss_mask: 0.0001 (0.0001)  time: 0.1984  data: 0.0002  max mem: 5511
[09:39:56.629072] Epoch: [75]  [620/781]  eta: 0:00:31  lr: 0.000039  training_loss: 1.4469 (1.4371)  mae_loss: 0.2525 (0.2514)  classification_loss: 1.1611 (1.1857)  loss_mask: 0.0001 (0.0001)  time: 0.1969  data: 0.0002  max mem: 5511
[09:40:00.611786] Epoch: [75]  [640/781]  eta: 0:00:27  lr: 0.000039  training_loss: 1.4030 (1.4365)  mae_loss: 0.2637 (0.2515)  classification_loss: 1.1526 (1.1849)  loss_mask: 0.0001 (0.0001)  time: 0.1991  data: 0.0002  max mem: 5511
[09:40:04.552919] Epoch: [75]  [660/781]  eta: 0:00:24  lr: 0.000039  training_loss: 1.4296 (1.4363)  mae_loss: 0.2460 (0.2515)  classification_loss: 1.1776 (1.1847)  loss_mask: 0.0001 (0.0001)  time: 0.1970  data: 0.0002  max mem: 5511
[09:40:08.482834] Epoch: [75]  [680/781]  eta: 0:00:20  lr: 0.000039  training_loss: 1.4748 (1.4374)  mae_loss: 0.2625 (0.2520)  classification_loss: 1.2197 (1.1854)  loss_mask: 0.0001 (0.0001)  time: 0.1964  data: 0.0002  max mem: 5511
[09:40:12.474158] Epoch: [75]  [700/781]  eta: 0:00:16  lr: 0.000039  training_loss: 1.4351 (1.4379)  mae_loss: 0.2538 (0.2523)  classification_loss: 1.1694 (1.1855)  loss_mask: 0.0001 (0.0001)  time: 0.1995  data: 0.0002  max mem: 5511
[09:40:16.482860] Epoch: [75]  [720/781]  eta: 0:00:12  lr: 0.000038  training_loss: 1.4965 (1.4394)  mae_loss: 0.2675 (0.2527)  classification_loss: 1.2289 (1.1866)  loss_mask: 0.0001 (0.0001)  time: 0.2003  data: 0.0002  max mem: 5511
[09:40:20.428416] Epoch: [75]  [740/781]  eta: 0:00:08  lr: 0.000038  training_loss: 1.3921 (1.4385)  mae_loss: 0.2514 (0.2527)  classification_loss: 1.1351 (1.1857)  loss_mask: 0.0001 (0.0001)  time: 0.1972  data: 0.0002  max mem: 5511
[09:40:24.392783] Epoch: [75]  [760/781]  eta: 0:00:04  lr: 0.000038  training_loss: 1.4527 (1.4387)  mae_loss: 0.2409 (0.2524)  classification_loss: 1.2117 (1.1862)  loss_mask: 0.0001 (0.0001)  time: 0.1981  data: 0.0002  max mem: 5511
[09:40:28.335912] Epoch: [75]  [780/781]  eta: 0:00:00  lr: 0.000038  training_loss: 1.4477 (1.4396)  mae_loss: 0.2593 (0.2526)  classification_loss: 1.2068 (1.1869)  loss_mask: 0.0001 (0.0001)  time: 0.1971  data: 0.0002  max mem: 5511
[09:40:28.511679] Epoch: [75] Total time: 0:02:35 (0.1988 s / it)
[09:40:28.512171] Averaged stats: lr: 0.000038  training_loss: 1.4477 (1.4396)  mae_loss: 0.2593 (0.2526)  classification_loss: 1.2068 (1.1869)  loss_mask: 0.0001 (0.0001)
[09:40:29.179192] Test:  [  0/157]  eta: 0:01:44  testing_loss: 0.4390 (0.4390)  acc1: 92.1875 (92.1875)  acc5: 98.4375 (98.4375)  time: 0.6625  data: 0.6304  max mem: 5511
[09:40:29.480648] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4987 (0.5034)  acc1: 84.3750 (83.9489)  acc5: 100.0000 (99.5739)  time: 0.0875  data: 0.0576  max mem: 5511
[09:40:29.763418] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4743 (0.4775)  acc1: 85.9375 (84.7470)  acc5: 100.0000 (99.7768)  time: 0.0291  data: 0.0003  max mem: 5511
[09:40:30.047275] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4459 (0.4935)  acc1: 85.9375 (84.3246)  acc5: 100.0000 (99.4456)  time: 0.0282  data: 0.0002  max mem: 5511
[09:40:30.337854] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4704 (0.4952)  acc1: 84.3750 (84.4512)  acc5: 98.4375 (99.3140)  time: 0.0286  data: 0.0002  max mem: 5511
[09:40:30.624910] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4685 (0.4879)  acc1: 85.9375 (84.9265)  acc5: 98.4375 (99.2953)  time: 0.0288  data: 0.0002  max mem: 5511
[09:40:30.908956] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4416 (0.4809)  acc1: 87.5000 (85.3740)  acc5: 100.0000 (99.2572)  time: 0.0284  data: 0.0002  max mem: 5511
[09:40:31.192680] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4774 (0.4812)  acc1: 85.9375 (85.3653)  acc5: 100.0000 (99.2738)  time: 0.0283  data: 0.0002  max mem: 5511
[09:40:31.476532] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5038 (0.4890)  acc1: 85.9375 (85.2431)  acc5: 98.4375 (99.2284)  time: 0.0283  data: 0.0002  max mem: 5511
[09:40:31.759562] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4649 (0.4825)  acc1: 85.9375 (85.5254)  acc5: 98.4375 (99.1758)  time: 0.0282  data: 0.0002  max mem: 5511
[09:40:32.045806] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4615 (0.4843)  acc1: 85.9375 (85.5198)  acc5: 98.4375 (99.1491)  time: 0.0283  data: 0.0002  max mem: 5511
[09:40:32.343349] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4995 (0.4848)  acc1: 85.9375 (85.5011)  acc5: 98.4375 (99.1695)  time: 0.0290  data: 0.0002  max mem: 5511
[09:40:32.633061] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4765 (0.4834)  acc1: 85.9375 (85.6147)  acc5: 100.0000 (99.1736)  time: 0.0292  data: 0.0002  max mem: 5511
[09:40:32.921581] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4686 (0.4841)  acc1: 84.3750 (85.5916)  acc5: 100.0000 (99.2247)  time: 0.0288  data: 0.0003  max mem: 5511
[09:40:33.205017] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4518 (0.4818)  acc1: 85.9375 (85.6937)  acc5: 100.0000 (99.2465)  time: 0.0285  data: 0.0003  max mem: 5511
[09:40:33.486753] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4540 (0.4807)  acc1: 85.9375 (85.7099)  acc5: 100.0000 (99.2550)  time: 0.0281  data: 0.0001  max mem: 5511
[09:40:33.638710] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4701 (0.4814)  acc1: 85.9375 (85.7000)  acc5: 100.0000 (99.2600)  time: 0.0271  data: 0.0001  max mem: 5511
[09:40:33.782997] Test: Total time: 0:00:05 (0.0336 s / it)
[09:40:33.783827] * Acc@1 85.700 Acc@5 99.260 loss 0.481
[09:40:33.784316] Accuracy of the network on the 10000 test images: 85.7%
[09:40:33.784595] Max accuracy: 85.70%
[09:40:34.106987] log_dir: ./output_dir
[09:40:35.104912] Epoch: [76]  [  0/781]  eta: 0:12:57  lr: 0.000038  training_loss: 1.4774 (1.4774)  mae_loss: 0.2574 (0.2574)  classification_loss: 1.2200 (1.2200)  loss_mask: 0.0001 (0.0001)  time: 0.9960  data: 0.7476  max mem: 5511
[09:40:39.047184] Epoch: [76]  [ 20/781]  eta: 0:02:58  lr: 0.000038  training_loss: 1.4107 (1.4267)  mae_loss: 0.2552 (0.2581)  classification_loss: 1.1274 (1.1686)  loss_mask: 0.0001 (0.0001)  time: 0.1970  data: 0.0004  max mem: 5511
[09:40:43.025401] Epoch: [76]  [ 40/781]  eta: 0:02:41  lr: 0.000038  training_loss: 1.4531 (1.4320)  mae_loss: 0.2364 (0.2492)  classification_loss: 1.2037 (1.1827)  loss_mask: 0.0001 (0.0001)  time: 0.1988  data: 0.0002  max mem: 5511
[09:40:47.006254] Epoch: [76]  [ 60/781]  eta: 0:02:32  lr: 0.000038  training_loss: 1.4486 (1.4412)  mae_loss: 0.2497 (0.2511)  classification_loss: 1.1943 (1.1900)  loss_mask: 0.0001 (0.0001)  time: 0.1989  data: 0.0002  max mem: 5511
[09:40:50.947109] Epoch: [76]  [ 80/781]  eta: 0:02:25  lr: 0.000038  training_loss: 1.4188 (1.4341)  mae_loss: 0.2498 (0.2513)  classification_loss: 1.1512 (1.1827)  loss_mask: 0.0001 (0.0001)  time: 0.1970  data: 0.0002  max mem: 5511
[09:40:54.905366] Epoch: [76]  [100/781]  eta: 0:02:20  lr: 0.000038  training_loss: 1.4336 (1.4391)  mae_loss: 0.2562 (0.2529)  classification_loss: 1.1861 (1.1861)  loss_mask: 0.0001 (0.0001)  time: 0.1978  data: 0.0002  max mem: 5511
[09:40:58.847985] Epoch: [76]  [120/781]  eta: 0:02:15  lr: 0.000038  training_loss: 1.4199 (1.4346)  mae_loss: 0.2250 (0.2501)  classification_loss: 1.1644 (1.1844)  loss_mask: 0.0001 (0.0001)  time: 0.1970  data: 0.0002  max mem: 5511
[09:41:02.820518] Epoch: [76]  [140/781]  eta: 0:02:10  lr: 0.000038  training_loss: 1.4651 (1.4399)  mae_loss: 0.2547 (0.2511)  classification_loss: 1.2315 (1.1887)  loss_mask: 0.0001 (0.0001)  time: 0.1985  data: 0.0003  max mem: 5511
[09:41:06.762206] Epoch: [76]  [160/781]  eta: 0:02:05  lr: 0.000038  training_loss: 1.4150 (1.4353)  mae_loss: 0.2469 (0.2506)  classification_loss: 1.1663 (1.1846)  loss_mask: 0.0001 (0.0001)  time: 0.1970  data: 0.0002  max mem: 5511
[09:41:10.735643] Epoch: [76]  [180/781]  eta: 0:02:01  lr: 0.000038  training_loss: 1.4380 (1.4365)  mae_loss: 0.2420 (0.2505)  classification_loss: 1.1850 (1.1860)  loss_mask: 0.0001 (0.0001)  time: 0.1986  data: 0.0002  max mem: 5511
[09:41:14.663252] Epoch: [76]  [200/781]  eta: 0:01:57  lr: 0.000037  training_loss: 1.4672 (1.4379)  mae_loss: 0.2549 (0.2512)  classification_loss: 1.1863 (1.1866)  loss_mask: 0.0001 (0.0001)  time: 0.1963  data: 0.0002  max mem: 5511
[09:41:18.602250] Epoch: [76]  [220/781]  eta: 0:01:52  lr: 0.000037  training_loss: 1.3954 (1.4363)  mae_loss: 0.2441 (0.2513)  classification_loss: 1.1301 (1.1850)  loss_mask: 0.0001 (0.0001)  time: 0.1969  data: 0.0002  max mem: 5511
[09:41:22.533210] Epoch: [76]  [240/781]  eta: 0:01:48  lr: 0.000037  training_loss: 1.4061 (1.4350)  mae_loss: 0.2496 (0.2518)  classification_loss: 1.1456 (1.1831)  loss_mask: 0.0001 (0.0001)  time: 0.1965  data: 0.0003  max mem: 5511
[09:41:26.478042] Epoch: [76]  [260/781]  eta: 0:01:44  lr: 0.000037  training_loss: 1.4557 (1.4341)  mae_loss: 0.2477 (0.2516)  classification_loss: 1.1823 (1.1824)  loss_mask: 0.0001 (0.0001)  time: 0.1972  data: 0.0002  max mem: 5511

[09:41:30.436705] Epoch: [76]  [280/781]  eta: 0:01:40  lr: 0.000037  training_loss: 1.4406 (1.4342)  mae_loss: 0.2488 (0.2519)  classification_loss: 1.1985 (1.1822)  loss_mask: 0.0001 (0.0001)  time: 0.1979  data: 0.0004  max mem: 5511
[09:41:34.370992] Epoch: [76]  [300/781]  eta: 0:01:36  lr: 0.000037  training_loss: 1.4477 (1.4341)  mae_loss: 0.2574 (0.2522)  classification_loss: 1.1805 (1.1819)  loss_mask: 0.0001 (0.0001)  time: 0.1966  data: 0.0002  max mem: 5511
[09:41:38.309008] Epoch: [76]  [320/781]  eta: 0:01:32  lr: 0.000037  training_loss: 1.4364 (1.4329)  mae_loss: 0.2427 (0.2518)  classification_loss: 1.1937 (1.1810)  loss_mask: 0.0001 (0.0001)  time: 0.1968  data: 0.0002  max mem: 5511
[09:41:42.306683] Epoch: [76]  [340/781]  eta: 0:01:28  lr: 0.000037  training_loss: 1.3950 (1.4308)  mae_loss: 0.2429 (0.2510)  classification_loss: 1.1628 (1.1793)  loss_mask: 0.0002 (0.0004)  time: 0.1998  data: 0.0002  max mem: 5511
[09:41:46.234741] Epoch: [76]  [360/781]  eta: 0:01:24  lr: 0.000037  training_loss: 1.4143 (1.4312)  mae_loss: 0.2445 (0.2506)  classification_loss: 1.1710 (1.1802)  loss_mask: 0.0001 (0.0004)  time: 0.1963  data: 0.0002  max mem: 5511
[09:41:50.200147] Epoch: [76]  [380/781]  eta: 0:01:20  lr: 0.000037  training_loss: 1.4358 (1.4314)  mae_loss: 0.2443 (0.2509)  classification_loss: 1.1545 (1.1802)  loss_mask: 0.0001 (0.0004)  time: 0.1982  data: 0.0002  max mem: 5511
[09:41:54.141774] Epoch: [76]  [400/781]  eta: 0:01:16  lr: 0.000037  training_loss: 1.4473 (1.4316)  mae_loss: 0.2538 (0.2510)  classification_loss: 1.1893 (1.1802)  loss_mask: 0.0001 (0.0004)  time: 0.1970  data: 0.0003  max mem: 5511
[09:41:58.104208] Epoch: [76]  [420/781]  eta: 0:01:11  lr: 0.000037  training_loss: 1.4222 (1.4313)  mae_loss: 0.2449 (0.2509)  classification_loss: 1.1628 (1.1800)  loss_mask: 0.0001 (0.0004)  time: 0.1980  data: 0.0003  max mem: 5511
[09:42:02.045064] Epoch: [76]  [440/781]  eta: 0:01:07  lr: 0.000037  training_loss: 1.4218 (1.4308)  mae_loss: 0.2571 (0.2513)  classification_loss: 1.1673 (1.1792)  loss_mask: 0.0001 (0.0004)  time: 0.1969  data: 0.0002  max mem: 5511
[09:42:06.013690] Epoch: [76]  [460/781]  eta: 0:01:03  lr: 0.000036  training_loss: 1.3839 (1.4291)  mae_loss: 0.2384 (0.2509)  classification_loss: 1.1533 (1.1778)  loss_mask: 0.0001 (0.0003)  time: 0.1983  data: 0.0002  max mem: 5511
[09:42:09.976305] Epoch: [76]  [480/781]  eta: 0:00:59  lr: 0.000036  training_loss: 1.4760 (1.4312)  mae_loss: 0.2499 (0.2509)  classification_loss: 1.2249 (1.1800)  loss_mask: 0.0001 (0.0003)  time: 0.1980  data: 0.0003  max mem: 5511
[09:42:13.941388] Epoch: [76]  [500/781]  eta: 0:00:55  lr: 0.000036  training_loss: 1.4215 (1.4308)  mae_loss: 0.2424 (0.2506)  classification_loss: 1.1889 (1.1800)  loss_mask: 0.0001 (0.0003)  time: 0.1981  data: 0.0003  max mem: 5511
[09:42:17.883344] Epoch: [76]  [520/781]  eta: 0:00:51  lr: 0.000036  training_loss: 1.4325 (1.4320)  mae_loss: 0.2489 (0.2506)  classification_loss: 1.1759 (1.1810)  loss_mask: 0.0001 (0.0003)  time: 0.1970  data: 0.0002  max mem: 5511
[09:42:21.847131] Epoch: [76]  [540/781]  eta: 0:00:47  lr: 0.000036  training_loss: 1.4449 (1.4325)  mae_loss: 0.2505 (0.2508)  classification_loss: 1.1849 (1.1814)  loss_mask: 0.0001 (0.0003)  time: 0.1981  data: 0.0002  max mem: 5511
[09:42:25.794621] Epoch: [76]  [560/781]  eta: 0:00:43  lr: 0.000036  training_loss: 1.4337 (1.4325)  mae_loss: 0.2431 (0.2506)  classification_loss: 1.1778 (1.1815)  loss_mask: 0.0001 (0.0004)  time: 0.1973  data: 0.0003  max mem: 5511
[09:42:29.741755] Epoch: [76]  [580/781]  eta: 0:00:39  lr: 0.000036  training_loss: 1.4346 (1.4328)  mae_loss: 0.2646 (0.2509)  classification_loss: 1.1814 (1.1814)  loss_mask: 0.0004 (0.0005)  time: 0.1973  data: 0.0002  max mem: 5511
[09:42:33.686176] Epoch: [76]  [600/781]  eta: 0:00:35  lr: 0.000036  training_loss: 1.3930 (1.4323)  mae_loss: 0.2583 (0.2513)  classification_loss: 1.1206 (1.1805)  loss_mask: 0.0002 (0.0005)  time: 0.1971  data: 0.0002  max mem: 5511
[09:42:37.667455] Epoch: [76]  [620/781]  eta: 0:00:32  lr: 0.000036  training_loss: 1.4238 (1.4322)  mae_loss: 0.2520 (0.2515)  classification_loss: 1.1835 (1.1802)  loss_mask: 0.0003 (0.0005)  time: 0.1990  data: 0.0002  max mem: 5511
[09:42:41.613960] Epoch: [76]  [640/781]  eta: 0:00:28  lr: 0.000036  training_loss: 1.3803 (1.4317)  mae_loss: 0.2457 (0.2514)  classification_loss: 1.1274 (1.1799)  loss_mask: 0.0001 (0.0005)  time: 0.1972  data: 0.0003  max mem: 5511
[09:42:45.583635] Epoch: [76]  [660/781]  eta: 0:00:24  lr: 0.000036  training_loss: 1.3933 (1.4310)  mae_loss: 0.2524 (0.2513)  classification_loss: 1.1324 (1.1793)  loss_mask: 0.0001 (0.0005)  time: 0.1984  data: 0.0002  max mem: 5511
[09:42:49.581861] Epoch: [76]  [680/781]  eta: 0:00:20  lr: 0.000036  training_loss: 1.4464 (1.4314)  mae_loss: 0.2286 (0.2511)  classification_loss: 1.2059 (1.1799)  loss_mask: 0.0001 (0.0005)  time: 0.1998  data: 0.0002  max mem: 5511
[09:42:53.569982] Epoch: [76]  [700/781]  eta: 0:00:16  lr: 0.000036  training_loss: 1.4659 (1.4320)  mae_loss: 0.2528 (0.2512)  classification_loss: 1.2126 (1.1803)  loss_mask: 0.0001 (0.0005)  time: 0.1993  data: 0.0003  max mem: 5511
[09:42:57.541175] Epoch: [76]  [720/781]  eta: 0:00:12  lr: 0.000036  training_loss: 1.4641 (1.4331)  mae_loss: 0.2561 (0.2511)  classification_loss: 1.2419 (1.1816)  loss_mask: 0.0001 (0.0004)  time: 0.1985  data: 0.0003  max mem: 5511
[09:43:01.505321] Epoch: [76]  [740/781]  eta: 0:00:08  lr: 0.000035  training_loss: 1.3939 (1.4325)  mae_loss: 0.2361 (0.2508)  classification_loss: 1.1665 (1.1812)  loss_mask: 0.0001 (0.0005)  time: 0.1981  data: 0.0002  max mem: 5511
[09:43:05.509815] Epoch: [76]  [760/781]  eta: 0:00:04  lr: 0.000035  training_loss: 1.4305 (1.4320)  mae_loss: 0.2387 (0.2506)  classification_loss: 1.1590 (1.1809)  loss_mask: 0.0001 (0.0005)  time: 0.2001  data: 0.0002  max mem: 5511
[09:43:09.417235] Epoch: [76]  [780/781]  eta: 0:00:00  lr: 0.000035  training_loss: 1.4030 (1.4313)  mae_loss: 0.2425 (0.2507)  classification_loss: 1.1572 (1.1802)  loss_mask: 0.0001 (0.0004)  time: 0.1953  data: 0.0002  max mem: 5511
[09:43:09.591414] Epoch: [76] Total time: 0:02:35 (0.1991 s / it)
[09:43:09.592303] Averaged stats: lr: 0.000035  training_loss: 1.4030 (1.4313)  mae_loss: 0.2425 (0.2507)  classification_loss: 1.1572 (1.1802)  loss_mask: 0.0001 (0.0004)
[09:43:10.222979] Test:  [  0/157]  eta: 0:01:38  testing_loss: 0.4118 (0.4118)  acc1: 85.9375 (85.9375)  acc5: 100.0000 (100.0000)  time: 0.6256  data: 0.5899  max mem: 5511
[09:43:10.513442] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5058 (0.4975)  acc1: 84.3750 (84.3750)  acc5: 100.0000 (99.1477)  time: 0.0831  data: 0.0538  max mem: 5511
[09:43:10.801192] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4767 (0.4740)  acc1: 84.3750 (84.8214)  acc5: 100.0000 (99.3304)  time: 0.0288  data: 0.0002  max mem: 5511
[09:43:11.095288] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4588 (0.4850)  acc1: 84.3750 (84.5262)  acc5: 100.0000 (99.2440)  time: 0.0290  data: 0.0002  max mem: 5511
[09:43:11.381042] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4692 (0.4936)  acc1: 84.3750 (84.4512)  acc5: 98.4375 (99.1235)  time: 0.0288  data: 0.0002  max mem: 5511
[09:43:11.665108] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4716 (0.4856)  acc1: 85.9375 (85.0490)  acc5: 98.4375 (99.1422)  time: 0.0283  data: 0.0002  max mem: 5511
[09:43:11.947581] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4469 (0.4789)  acc1: 85.9375 (85.2203)  acc5: 100.0000 (99.1803)  time: 0.0282  data: 0.0002  max mem: 5511
[09:43:12.231594] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4546 (0.4791)  acc1: 85.9375 (85.3213)  acc5: 100.0000 (99.1857)  time: 0.0282  data: 0.0002  max mem: 5511
[09:43:12.516016] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4823 (0.4856)  acc1: 85.9375 (85.1852)  acc5: 98.4375 (99.1127)  time: 0.0283  data: 0.0002  max mem: 5511
[09:43:12.812127] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4932 (0.4813)  acc1: 85.9375 (85.4396)  acc5: 98.4375 (99.1243)  time: 0.0289  data: 0.0004  max mem: 5511
[09:43:13.101037] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4562 (0.4838)  acc1: 85.9375 (85.3187)  acc5: 100.0000 (99.1027)  time: 0.0291  data: 0.0005  max mem: 5511
[09:43:13.385913] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5023 (0.4869)  acc1: 85.9375 (85.2618)  acc5: 98.4375 (99.0850)  time: 0.0285  data: 0.0003  max mem: 5511
[09:43:13.670405] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5023 (0.4856)  acc1: 84.3750 (85.2273)  acc5: 100.0000 (99.0961)  time: 0.0283  data: 0.0002  max mem: 5511
[09:43:13.954790] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4848 (0.4854)  acc1: 84.3750 (85.2457)  acc5: 100.0000 (99.1174)  time: 0.0283  data: 0.0002  max mem: 5511
[09:43:14.238380] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4590 (0.4835)  acc1: 85.9375 (85.3613)  acc5: 100.0000 (99.1578)  time: 0.0283  data: 0.0002  max mem: 5511
[09:43:14.518252] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4747 (0.4821)  acc1: 85.9375 (85.4512)  acc5: 100.0000 (99.1618)  time: 0.0280  data: 0.0001  max mem: 5511
[09:43:14.670403] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4523 (0.4815)  acc1: 87.5000 (85.4800)  acc5: 100.0000 (99.1700)  time: 0.0271  data: 0.0001  max mem: 5511
[09:43:14.862232] Test: Total time: 0:00:05 (0.0335 s / it)
[09:43:14.862698] * Acc@1 85.480 Acc@5 99.170 loss 0.481
[09:43:14.863009] Accuracy of the network on the 10000 test images: 85.5%
[09:43:14.863225] Max accuracy: 85.70%
[09:43:15.055897] log_dir: ./output_dir
[09:43:15.868003] Epoch: [77]  [  0/781]  eta: 0:10:32  lr: 0.000035  training_loss: 1.4212 (1.4212)  mae_loss: 0.2397 (0.2397)  classification_loss: 1.1815 (1.1815)  loss_mask: 0.0001 (0.0001)  time: 0.8103  data: 0.5955  max mem: 5511
[09:43:19.831255] Epoch: [77]  [ 20/781]  eta: 0:02:52  lr: 0.000035  training_loss: 1.3980 (1.3968)  mae_loss: 0.2458 (0.2473)  classification_loss: 1.1392 (1.1494)  loss_mask: 0.0001 (0.0001)  time: 0.1981  data: 0.0002  max mem: 5511
[09:43:23.761701] Epoch: [77]  [ 40/781]  eta: 0:02:37  lr: 0.000035  training_loss: 1.4703 (1.4146)  mae_loss: 0.2551 (0.2503)  classification_loss: 1.1884 (1.1642)  loss_mask: 0.0001 (0.0001)  time: 0.1964  data: 0.0002  max mem: 5511
[09:43:27.710396] Epoch: [77]  [ 60/781]  eta: 0:02:29  lr: 0.000035  training_loss: 1.4460 (1.4317)  mae_loss: 0.2559 (0.2526)  classification_loss: 1.1998 (1.1790)  loss_mask: 0.0000 (0.0001)  time: 0.1973  data: 0.0002  max mem: 5511
[09:43:31.743985] Epoch: [77]  [ 80/781]  eta: 0:02:24  lr: 0.000035  training_loss: 1.4541 (1.4308)  mae_loss: 0.2667 (0.2533)  classification_loss: 1.2070 (1.1774)  loss_mask: 0.0001 (0.0001)  time: 0.2016  data: 0.0003  max mem: 5511
[09:43:35.691403] Epoch: [77]  [100/781]  eta: 0:02:19  lr: 0.000035  training_loss: 1.4463 (1.4324)  mae_loss: 0.2358 (0.2516)  classification_loss: 1.1851 (1.1807)  loss_mask: 0.0000 (0.0001)  time: 0.1973  data: 0.0002  max mem: 5511
[09:43:39.636024] Epoch: [77]  [120/781]  eta: 0:02:14  lr: 0.000035  training_loss: 1.4341 (1.4367)  mae_loss: 0.2355 (0.2511)  classification_loss: 1.1721 (1.1856)  loss_mask: 0.0000 (0.0001)  time: 0.1972  data: 0.0002  max mem: 5511
[09:43:43.575653] Epoch: [77]  [140/781]  eta: 0:02:09  lr: 0.000035  training_loss: 1.4255 (1.4348)  mae_loss: 0.2466 (0.2505)  classification_loss: 1.1680 (1.1842)  loss_mask: 0.0001 (0.0001)  time: 0.1969  data: 0.0002  max mem: 5511
[09:43:47.535701] Epoch: [77]  [160/781]  eta: 0:02:05  lr: 0.000035  training_loss: 1.4256 (1.4343)  mae_loss: 0.2419 (0.2495)  classification_loss: 1.1872 (1.1847)  loss_mask: 0.0000 (0.0001)  time: 0.1979  data: 0.0002  max mem: 5511
[09:43:51.467917] Epoch: [77]  [180/781]  eta: 0:02:00  lr: 0.000035  training_loss: 1.4084 (1.4347)  mae_loss: 0.2446 (0.2496)  classification_loss: 1.2005 (1.1850)  loss_mask: 0.0001 (0.0001)  time: 0.1965  data: 0.0002  max mem: 5511
[09:43:55.414775] Epoch: [77]  [200/781]  eta: 0:01:56  lr: 0.000035  training_loss: 1.4200 (1.4297)  mae_loss: 0.2589 (0.2502)  classification_loss: 1.1371 (1.1795)  loss_mask: 0.0001 (0.0001)  time: 0.1972  data: 0.0003  max mem: 5511
[09:43:59.369623] Epoch: [77]  [220/781]  eta: 0:01:52  lr: 0.000035  training_loss: 1.4151 (1.4278)  mae_loss: 0.2365 (0.2496)  classification_loss: 1.1678 (1.1782)  loss_mask: 0.0001 (0.0001)  time: 0.1976  data: 0.0002  max mem: 5511
[09:44:03.320538] Epoch: [77]  [240/781]  eta: 0:01:48  lr: 0.000034  training_loss: 1.3654 (1.4261)  mae_loss: 0.2371 (0.2494)  classification_loss: 1.1194 (1.1766)  loss_mask: 0.0001 (0.0001)  time: 0.1975  data: 0.0002  max mem: 5511
[09:44:07.284159] Epoch: [77]  [260/781]  eta: 0:01:44  lr: 0.000034  training_loss: 1.4577 (1.4293)  mae_loss: 0.2587 (0.2499)  classification_loss: 1.2062 (1.1793)  loss_mask: 0.0001 (0.0001)  time: 0.1981  data: 0.0002  max mem: 5511
[09:44:11.286261] Epoch: [77]  [280/781]  eta: 0:01:40  lr: 0.000034  training_loss: 1.3986 (1.4284)  mae_loss: 0.2520 (0.2505)  classification_loss: 1.1520 (1.1778)  loss_mask: 0.0000 (0.0001)  time: 0.2000  data: 0.0002  max mem: 5511
[09:44:15.237185] Epoch: [77]  [300/781]  eta: 0:01:36  lr: 0.000034  training_loss: 1.4582 (1.4302)  mae_loss: 0.2477 (0.2510)  classification_loss: 1.1939 (1.1791)  loss_mask: 0.0001 (0.0001)  time: 0.1975  data: 0.0002  max mem: 5511
[09:44:19.176165] Epoch: [77]  [320/781]  eta: 0:01:32  lr: 0.000034  training_loss: 1.4000 (1.4285)  mae_loss: 0.2446 (0.2506)  classification_loss: 1.1671 (1.1779)  loss_mask: 0.0000 (0.0001)  time: 0.1968  data: 0.0002  max mem: 5511
[09:44:23.122730] Epoch: [77]  [340/781]  eta: 0:01:27  lr: 0.000034  training_loss: 1.4042 (1.4287)  mae_loss: 0.2572 (0.2515)  classification_loss: 1.1332 (1.1771)  loss_mask: 0.0001 (0.0001)  time: 0.1972  data: 0.0003  max mem: 5511
[09:44:27.062482] Epoch: [77]  [360/781]  eta: 0:01:23  lr: 0.000034  training_loss: 1.4356 (1.4296)  mae_loss: 0.2498 (0.2513)  classification_loss: 1.1435 (1.1769)  loss_mask: 0.0001 (0.0014)  time: 0.1969  data: 0.0002  max mem: 5511
[09:44:31.009000] Epoch: [77]  [380/781]  eta: 0:01:19  lr: 0.000034  training_loss: 1.4581 (1.4317)  mae_loss: 0.2447 (0.2514)  classification_loss: 1.1948 (1.1786)  loss_mask: 0.0015 (0.0018)  time: 0.1972  data: 0.0003  max mem: 5511
[09:44:34.936946] Epoch: [77]  [400/781]  eta: 0:01:15  lr: 0.000034  training_loss: 1.4436 (1.4317)  mae_loss: 0.2412 (0.2513)  classification_loss: 1.2027 (1.1786)  loss_mask: 0.0009 (0.0018)  time: 0.1963  data: 0.0002  max mem: 5511
[09:44:38.873835] Epoch: [77]  [420/781]  eta: 0:01:11  lr: 0.000034  training_loss: 1.4117 (1.4310)  mae_loss: 0.2377 (0.2510)  classification_loss: 1.1594 (1.1783)  loss_mask: 0.0002 (0.0017)  time: 0.1967  data: 0.0002  max mem: 5511
[09:44:42.806689] Epoch: [77]  [440/781]  eta: 0:01:07  lr: 0.000034  training_loss: 1.3919 (1.4302)  mae_loss: 0.2580 (0.2515)  classification_loss: 1.1166 (1.1771)  loss_mask: 0.0002 (0.0017)  time: 0.1966  data: 0.0002  max mem: 5511
[09:44:46.743902] Epoch: [77]  [460/781]  eta: 0:01:03  lr: 0.000034  training_loss: 1.3896 (1.4293)  mae_loss: 0.2489 (0.2514)  classification_loss: 1.1540 (1.1764)  loss_mask: 0.0002 (0.0016)  time: 0.1968  data: 0.0002  max mem: 5511
[09:44:50.671400] Epoch: [77]  [480/781]  eta: 0:00:59  lr: 0.000034  training_loss: 1.4174 (1.4290)  mae_loss: 0.2534 (0.2515)  classification_loss: 1.1524 (1.1759)  loss_mask: 0.0001 (0.0015)  time: 0.1963  data: 0.0002  max mem: 5511
[09:44:54.606313] Epoch: [77]  [500/781]  eta: 0:00:55  lr: 0.000034  training_loss: 1.4290 (1.4304)  mae_loss: 0.2573 (0.2519)  classification_loss: 1.1635 (1.1770)  loss_mask: 0.0001 (0.0015)  time: 0.1966  data: 0.0002  max mem: 5511
[09:44:58.559534] Epoch: [77]  [520/781]  eta: 0:00:51  lr: 0.000033  training_loss: 1.4230 (1.4303)  mae_loss: 0.2528 (0.2520)  classification_loss: 1.1586 (1.1769)  loss_mask: 0.0001 (0.0014)  time: 0.1976  data: 0.0002  max mem: 5511
[09:45:02.488915] Epoch: [77]  [540/781]  eta: 0:00:47  lr: 0.000033  training_loss: 1.4062 (1.4313)  mae_loss: 0.2500 (0.2519)  classification_loss: 1.1643 (1.1780)  loss_mask: 0.0001 (0.0014)  time: 0.1964  data: 0.0002  max mem: 5511
[09:45:06.446145] Epoch: [77]  [560/781]  eta: 0:00:43  lr: 0.000033  training_loss: 1.4060 (1.4302)  mae_loss: 0.2473 (0.2517)  classification_loss: 1.1559 (1.1771)  loss_mask: 0.0001 (0.0013)  time: 0.1978  data: 0.0003  max mem: 5511
[09:45:10.392350] Epoch: [77]  [580/781]  eta: 0:00:39  lr: 0.000033  training_loss: 1.4035 (1.4304)  mae_loss: 0.2492 (0.2518)  classification_loss: 1.1734 (1.1772)  loss_mask: 0.0001 (0.0013)  time: 0.1972  data: 0.0002  max mem: 5511
[09:45:14.328169] Epoch: [77]  [600/781]  eta: 0:00:35  lr: 0.000033  training_loss: 1.3853 (1.4296)  mae_loss: 0.2446 (0.2516)  classification_loss: 1.1461 (1.1767)  loss_mask: 0.0001 (0.0013)  time: 0.1967  data: 0.0003  max mem: 5511
[09:45:18.285311] Epoch: [77]  [620/781]  eta: 0:00:31  lr: 0.000033  training_loss: 1.4235 (1.4300)  mae_loss: 0.2507 (0.2518)  classification_loss: 1.1360 (1.1768)  loss_mask: 0.0001 (0.0014)  time: 0.1978  data: 0.0003  max mem: 5511
[09:45:22.231408] Epoch: [77]  [640/781]  eta: 0:00:27  lr: 0.000033  training_loss: 1.4141 (1.4294)  mae_loss: 0.2462 (0.2518)  classification_loss: 1.1560 (1.1761)  loss_mask: 0.0002 (0.0014)  time: 0.1972  data: 0.0002  max mem: 5511
[09:45:26.180765] Epoch: [77]  [660/781]  eta: 0:00:23  lr: 0.000033  training_loss: 1.4239 (1.4292)  mae_loss: 0.2276 (0.2514)  classification_loss: 1.1661 (1.1764)  loss_mask: 0.0002 (0.0014)  time: 0.1974  data: 0.0002  max mem: 5511
[09:45:30.122393] Epoch: [77]  [680/781]  eta: 0:00:20  lr: 0.000033  training_loss: 1.3939 (1.4289)  mae_loss: 0.2407 (0.2512)  classification_loss: 1.1921 (1.1763)  loss_mask: 0.0001 (0.0014)  time: 0.1970  data: 0.0002  max mem: 5511
[09:45:34.094098] Epoch: [77]  [700/781]  eta: 0:00:16  lr: 0.000033  training_loss: 1.4123 (1.4293)  mae_loss: 0.2434 (0.2514)  classification_loss: 1.1647 (1.1766)  loss_mask: 0.0001 (0.0013)  time: 0.1985  data: 0.0002  max mem: 5511
[09:45:38.045699] Epoch: [77]  [720/781]  eta: 0:00:12  lr: 0.000033  training_loss: 1.4666 (1.4301)  mae_loss: 0.2302 (0.2512)  classification_loss: 1.2191 (1.1776)  loss_mask: 0.0001 (0.0013)  time: 0.1975  data: 0.0004  max mem: 5511
[09:45:42.003891] Epoch: [77]  [740/781]  eta: 0:00:08  lr: 0.000033  training_loss: 1.4236 (1.4302)  mae_loss: 0.2662 (0.2517)  classification_loss: 1.1749 (1.1773)  loss_mask: 0.0001 (0.0013)  time: 0.1978  data: 0.0002  max mem: 5511
[09:45:45.943972] Epoch: [77]  [760/781]  eta: 0:00:04  lr: 0.000033  training_loss: 1.4368 (1.4304)  mae_loss: 0.2503 (0.2519)  classification_loss: 1.1724 (1.1773)  loss_mask: 0.0001 (0.0012)  time: 0.1969  data: 0.0003  max mem: 5511
[09:45:49.913918] Epoch: [77]  [780/781]  eta: 0:00:00  lr: 0.000033  training_loss: 1.3913 (1.4297)  mae_loss: 0.2408 (0.2517)  classification_loss: 1.1551 (1.1768)  loss_mask: 0.0001 (0.0012)  time: 0.1984  data: 0.0003  max mem: 5511
[09:45:50.080611] Epoch: [77] Total time: 0:02:35 (0.1985 s / it)
[09:45:50.081121] Averaged stats: lr: 0.000033  training_loss: 1.3913 (1.4297)  mae_loss: 0.2408 (0.2517)  classification_loss: 1.1551 (1.1768)  loss_mask: 0.0001 (0.0012)
[09:45:50.770984] Test:  [  0/157]  eta: 0:01:47  testing_loss: 0.4325 (0.4325)  acc1: 90.6250 (90.6250)  acc5: 100.0000 (100.0000)  time: 0.6851  data: 0.6260  max mem: 5511
[09:45:51.065955] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.5001 (0.4868)  acc1: 85.9375 (84.9432)  acc5: 100.0000 (99.7159)  time: 0.0889  data: 0.0571  max mem: 5511
[09:45:51.353215] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4470 (0.4656)  acc1: 87.5000 (86.3839)  acc5: 100.0000 (99.5536)  time: 0.0289  data: 0.0003  max mem: 5511
[09:45:51.641316] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4516 (0.4841)  acc1: 87.5000 (85.2319)  acc5: 100.0000 (99.4456)  time: 0.0286  data: 0.0003  max mem: 5511
[09:45:51.927598] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4753 (0.4878)  acc1: 84.3750 (85.4040)  acc5: 100.0000 (99.3902)  time: 0.0286  data: 0.0002  max mem: 5511
[09:45:52.213182] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4615 (0.4797)  acc1: 87.5000 (85.7230)  acc5: 100.0000 (99.3873)  time: 0.0285  data: 0.0002  max mem: 5511
[09:45:52.503182] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4426 (0.4726)  acc1: 87.5000 (85.9375)  acc5: 100.0000 (99.4109)  time: 0.0286  data: 0.0002  max mem: 5511
[09:45:52.800001] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4488 (0.4719)  acc1: 85.9375 (85.9155)  acc5: 100.0000 (99.4278)  time: 0.0291  data: 0.0002  max mem: 5511
[09:45:53.094391] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4758 (0.4792)  acc1: 84.3750 (85.7253)  acc5: 100.0000 (99.3827)  time: 0.0294  data: 0.0003  max mem: 5511
[09:45:53.379565] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4647 (0.4738)  acc1: 87.5000 (86.0749)  acc5: 98.4375 (99.2960)  time: 0.0288  data: 0.0003  max mem: 5511
[09:45:53.663302] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.4367 (0.4753)  acc1: 87.5000 (86.0613)  acc5: 98.4375 (99.2729)  time: 0.0283  data: 0.0002  max mem: 5511
[09:45:53.946160] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4974 (0.4776)  acc1: 85.9375 (85.9657)  acc5: 98.4375 (99.2821)  time: 0.0282  data: 0.0002  max mem: 5511
[09:45:54.230023] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4974 (0.4759)  acc1: 84.3750 (85.9892)  acc5: 100.0000 (99.2769)  time: 0.0282  data: 0.0002  max mem: 5511
[09:45:54.511791] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4411 (0.4760)  acc1: 84.3750 (85.9614)  acc5: 100.0000 (99.2605)  time: 0.0282  data: 0.0001  max mem: 5511
[09:45:54.797129] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4397 (0.4735)  acc1: 85.9375 (85.9929)  acc5: 100.0000 (99.3129)  time: 0.0282  data: 0.0002  max mem: 5511
[09:45:55.078332] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4397 (0.4717)  acc1: 85.9375 (86.0617)  acc5: 100.0000 (99.2964)  time: 0.0282  data: 0.0001  max mem: 5511
[09:45:55.230517] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4541 (0.4726)  acc1: 85.9375 (86.0200)  acc5: 100.0000 (99.3100)  time: 0.0272  data: 0.0001  max mem: 5511
[09:45:55.405816] Test: Total time: 0:00:05 (0.0339 s / it)
[09:45:55.406493] * Acc@1 86.020 Acc@5 99.310 loss 0.473
[09:45:55.406850] Accuracy of the network on the 10000 test images: 86.0%
[09:45:55.407042] Max accuracy: 86.02%
[09:45:55.784171] log_dir: ./output_dir
[09:45:56.675992] Epoch: [78]  [  0/781]  eta: 0:11:35  lr: 0.000033  training_loss: 1.3559 (1.3559)  mae_loss: 0.2508 (0.2508)  classification_loss: 1.1049 (1.1049)  loss_mask: 0.0002 (0.0002)  time: 0.8900  data: 0.6867  max mem: 5511
[09:46:00.649636] Epoch: [78]  [ 20/781]  eta: 0:02:56  lr: 0.000032  training_loss: 1.3925 (1.3997)  mae_loss: 0.2550 (0.2542)  classification_loss: 1.1362 (1.1454)  loss_mask: 0.0001 (0.0001)  time: 0.1986  data: 0.0002  max mem: 5511
[09:46:04.572794] Epoch: [78]  [ 40/781]  eta: 0:02:38  lr: 0.000032  training_loss: 1.4533 (1.4129)  mae_loss: 0.2550 (0.2550)  classification_loss: 1.2105 (1.1578)  loss_mask: 0.0001 (0.0001)  time: 0.1961  data: 0.0002  max mem: 5511
[09:46:08.524171] Epoch: [78]  [ 60/781]  eta: 0:02:30  lr: 0.000032  training_loss: 1.4527 (1.4284)  mae_loss: 0.2647 (0.2584)  classification_loss: 1.1874 (1.1699)  loss_mask: 0.0001 (0.0001)  time: 0.1975  data: 0.0002  max mem: 5511
[09:46:12.496331] Epoch: [78]  [ 80/781]  eta: 0:02:24  lr: 0.000032  training_loss: 1.4346 (1.4326)  mae_loss: 0.2425 (0.2559)  classification_loss: 1.1796 (1.1766)  loss_mask: 0.0001 (0.0001)  time: 0.1985  data: 0.0002  max mem: 5511
[09:46:16.441274] Epoch: [78]  [100/781]  eta: 0:02:19  lr: 0.000032  training_loss: 1.4423 (1.4321)  mae_loss: 0.2396 (0.2535)  classification_loss: 1.1877 (1.1785)  loss_mask: 0.0001 (0.0001)  time: 0.1972  data: 0.0002  max mem: 5511
[09:46:20.455467] Epoch: [78]  [120/781]  eta: 0:02:14  lr: 0.000032  training_loss: 1.3850 (1.4267)  mae_loss: 0.2415 (0.2523)  classification_loss: 1.1291 (1.1744)  loss_mask: 0.0001 (0.0001)  time: 0.2006  data: 0.0003  max mem: 5511
[09:46:24.403614] Epoch: [78]  [140/781]  eta: 0:02:10  lr: 0.000032  training_loss: 1.3853 (1.4201)  mae_loss: 0.2549 (0.2534)  classification_loss: 1.0993 (1.1666)  loss_mask: 0.0001 (0.0001)  time: 0.1973  data: 0.0002  max mem: 5511
[09:46:28.350765] Epoch: [78]  [160/781]  eta: 0:02:05  lr: 0.000032  training_loss: 1.3740 (1.4174)  mae_loss: 0.2544 (0.2534)  classification_loss: 1.1072 (1.1640)  loss_mask: 0.0001 (0.0001)  time: 0.1973  data: 0.0002  max mem: 5511
[09:46:32.300443] Epoch: [78]  [180/781]  eta: 0:02:01  lr: 0.000032  training_loss: 1.4183 (1.4192)  mae_loss: 0.2417 (0.2528)  classification_loss: 1.1797 (1.1663)  loss_mask: 0.0001 (0.0001)  time: 0.1974  data: 0.0002  max mem: 5511
[09:46:36.246733] Epoch: [78]  [200/781]  eta: 0:01:56  lr: 0.000032  training_loss: 1.4203 (1.4188)  mae_loss: 0.2423 (0.2523)  classification_loss: 1.1696 (1.1665)  loss_mask: 0.0001 (0.0001)  time: 0.1972  data: 0.0002  max mem: 5511
[09:46:40.177127] Epoch: [78]  [220/781]  eta: 0:01:52  lr: 0.000032  training_loss: 1.3801 (1.4165)  mae_loss: 0.2403 (0.2513)  classification_loss: 1.1251 (1.1651)  loss_mask: 0.0001 (0.0001)  time: 0.1964  data: 0.0002  max mem: 5511
[09:46:44.147892] Epoch: [78]  [240/781]  eta: 0:01:48  lr: 0.000032  training_loss: 1.3903 (1.4175)  mae_loss: 0.2659 (0.2519)  classification_loss: 1.1565 (1.1656)  loss_mask: 0.0001 (0.0001)  time: 0.1984  data: 0.0002  max mem: 5511
[09:46:48.106158] Epoch: [78]  [260/781]  eta: 0:01:44  lr: 0.000032  training_loss: 1.4139 (1.4194)  mae_loss: 0.2605 (0.2518)  classification_loss: 1.1791 (1.1675)  loss_mask: 0.0001 (0.0001)  time: 0.1978  data: 0.0002  max mem: 5511
[09:46:52.061514] Epoch: [78]  [280/781]  eta: 0:01:40  lr: 0.000032  training_loss: 1.4113 (1.4206)  mae_loss: 0.2636 (0.2527)  classification_loss: 1.1748 (1.1678)  loss_mask: 0.0001 (0.0001)  time: 0.1977  data: 0.0002  max mem: 5511
[09:46:56.014041] Epoch: [78]  [300/781]  eta: 0:01:36  lr: 0.000031  training_loss: 1.4078 (1.4205)  mae_loss: 0.2477 (0.2525)  classification_loss: 1.1576 (1.1679)  loss_mask: 0.0001 (0.0001)  time: 0.1975  data: 0.0003  max mem: 5511
[09:46:59.957756] Epoch: [78]  [320/781]  eta: 0:01:32  lr: 0.000031  training_loss: 1.3717 (1.4195)  mae_loss: 0.2483 (0.2526)  classification_loss: 1.1468 (1.1668)  loss_mask: 0.0001 (0.0001)  time: 0.1971  data: 0.0002  max mem: 5511
[09:47:03.934145] Epoch: [78]  [340/781]  eta: 0:01:28  lr: 0.000031  training_loss: 1.3555 (1.4175)  mae_loss: 0.2402 (0.2523)  classification_loss: 1.1131 (1.1652)  loss_mask: 0.0001 (0.0001)  time: 0.1987  data: 0.0002  max mem: 5511
[09:47:07.925933] Epoch: [78]  [360/781]  eta: 0:01:24  lr: 0.000031  training_loss: 1.4510 (1.4184)  mae_loss: 0.2479 (0.2521)  classification_loss: 1.1957 (1.1662)  loss_mask: 0.0001 (0.0001)  time: 0.1995  data: 0.0003  max mem: 5511
[09:47:11.856988] Epoch: [78]  [380/781]  eta: 0:01:20  lr: 0.000031  training_loss: 1.4260 (1.4196)  mae_loss: 0.2516 (0.2525)  classification_loss: 1.1739 (1.1671)  loss_mask: 0.0001 (0.0001)  time: 0.1965  data: 0.0003  max mem: 5511
[09:47:15.819558] Epoch: [78]  [400/781]  eta: 0:01:16  lr: 0.000031  training_loss: 1.3715 (1.4190)  mae_loss: 0.2561 (0.2526)  classification_loss: 1.1074 (1.1664)  loss_mask: 0.0001 (0.0001)  time: 0.1980  data: 0.0002  max mem: 5511
[09:47:19.754600] Epoch: [78]  [420/781]  eta: 0:01:11  lr: 0.000031  training_loss: 1.3986 (1.4187)  mae_loss: 0.2532 (0.2526)  classification_loss: 1.1425 (1.1660)  loss_mask: 0.0001 (0.0001)  time: 0.1967  data: 0.0002  max mem: 5511
[09:47:23.709277] Epoch: [78]  [440/781]  eta: 0:01:07  lr: 0.000031  training_loss: 1.4388 (1.4200)  mae_loss: 0.2435 (0.2525)  classification_loss: 1.1799 (1.1674)  loss_mask: 0.0001 (0.0001)  time: 0.1977  data: 0.0003  max mem: 5511
[09:47:27.655862] Epoch: [78]  [460/781]  eta: 0:01:03  lr: 0.000031  training_loss: 1.3882 (1.4187)  mae_loss: 0.2382 (0.2519)  classification_loss: 1.1499 (1.1667)  loss_mask: 0.0001 (0.0001)  time: 0.1972  data: 0.0002  max mem: 5511
[09:47:31.583285] Epoch: [78]  [480/781]  eta: 0:00:59  lr: 0.000031  training_loss: 1.3985 (1.4187)  mae_loss: 0.2401 (0.2518)  classification_loss: 1.1468 (1.1668)  loss_mask: 0.0001 (0.0001)  time: 0.1963  data: 0.0002  max mem: 5511
[09:47:35.555506] Epoch: [78]  [500/781]  eta: 0:00:55  lr: 0.000031  training_loss: 1.4144 (1.4191)  mae_loss: 0.2495 (0.2518)  classification_loss: 1.1497 (1.1672)  loss_mask: 0.0001 (0.0001)  time: 0.1985  data: 0.0003  max mem: 5511
[09:47:39.516365] Epoch: [78]  [520/781]  eta: 0:00:51  lr: 0.000031  training_loss: 1.4247 (1.4197)  mae_loss: 0.2629 (0.2521)  classification_loss: 1.1591 (1.1674)  loss_mask: 0.0001 (0.0001)  time: 0.1979  data: 0.0003  max mem: 5511
[09:47:43.494324] Epoch: [78]  [540/781]  eta: 0:00:47  lr: 0.000031  training_loss: 1.4189 (1.4195)  mae_loss: 0.2348 (0.2518)  classification_loss: 1.1567 (1.1676)  loss_mask: 0.0001 (0.0001)  time: 0.1987  data: 0.0003  max mem: 5511
[09:47:47.443476] Epoch: [78]  [560/781]  eta: 0:00:43  lr: 0.000031  training_loss: 1.4314 (1.4194)  mae_loss: 0.2369 (0.2515)  classification_loss: 1.2040 (1.1678)  loss_mask: 0.0001 (0.0001)  time: 0.1973  data: 0.0003  max mem: 5511
[09:47:51.446670] Epoch: [78]  [580/781]  eta: 0:00:39  lr: 0.000031  training_loss: 1.4229 (1.4203)  mae_loss: 0.2454 (0.2516)  classification_loss: 1.1846 (1.1686)  loss_mask: 0.0001 (0.0001)  time: 0.2001  data: 0.0002  max mem: 5511
[09:47:55.393186] Epoch: [78]  [600/781]  eta: 0:00:36  lr: 0.000030  training_loss: 1.4274 (1.4196)  mae_loss: 0.2496 (0.2515)  classification_loss: 1.1625 (1.1680)  loss_mask: 0.0001 (0.0001)  time: 0.1972  data: 0.0002  max mem: 5511
[09:47:59.351163] Epoch: [78]  [620/781]  eta: 0:00:32  lr: 0.000030  training_loss: 1.3832 (1.4187)  mae_loss: 0.2534 (0.2519)  classification_loss: 1.1369 (1.1668)  loss_mask: 0.0001 (0.0001)  time: 0.1978  data: 0.0002  max mem: 5511
[09:48:03.314121] Epoch: [78]  [640/781]  eta: 0:00:28  lr: 0.000030  training_loss: 1.3949 (1.4184)  mae_loss: 0.2408 (0.2518)  classification_loss: 1.1380 (1.1665)  loss_mask: 0.0001 (0.0001)  time: 0.1981  data: 0.0002  max mem: 5511
[09:48:07.268056] Epoch: [78]  [660/781]  eta: 0:00:24  lr: 0.000030  training_loss: 1.4095 (1.4186)  mae_loss: 0.2435 (0.2517)  classification_loss: 1.1618 (1.1668)  loss_mask: 0.0001 (0.0001)  time: 0.1976  data: 0.0002  max mem: 5511
[09:48:11.230348] Epoch: [78]  [680/781]  eta: 0:00:20  lr: 0.000030  training_loss: 1.4587 (1.4201)  mae_loss: 0.2487 (0.2517)  classification_loss: 1.1988 (1.1683)  loss_mask: 0.0001 (0.0001)  time: 0.1980  data: 0.0004  max mem: 5511
[09:48:15.225184] Epoch: [78]  [700/781]  eta: 0:00:16  lr: 0.000030  training_loss: 1.4390 (1.4205)  mae_loss: 0.2539 (0.2519)  classification_loss: 1.2135 (1.1685)  loss_mask: 0.0001 (0.0001)  time: 0.1996  data: 0.0003  max mem: 5511
[09:48:19.161071] Epoch: [78]  [720/781]  eta: 0:00:12  lr: 0.000030  training_loss: 1.4093 (1.4204)  mae_loss: 0.2436 (0.2515)  classification_loss: 1.1824 (1.1689)  loss_mask: 0.0001 (0.0001)  time: 0.1967  data: 0.0002  max mem: 5511
[09:48:23.100317] Epoch: [78]  [740/781]  eta: 0:00:08  lr: 0.000030  training_loss: 1.3165 (1.4185)  mae_loss: 0.2468 (0.2515)  classification_loss: 1.0850 (1.1669)  loss_mask: 0.0001 (0.0001)  time: 0.1969  data: 0.0003  max mem: 5511
[09:48:27.042274] Epoch: [78]  [760/781]  eta: 0:00:04  lr: 0.000030  training_loss: 1.4722 (1.4197)  mae_loss: 0.2378 (0.2513)  classification_loss: 1.2463 (1.1683)  loss_mask: 0.0001 (0.0001)  time: 0.1970  data: 0.0002  max mem: 5511
[09:48:30.989503] Epoch: [78]  [780/781]  eta: 0:00:00  lr: 0.000030  training_loss: 1.4083 (1.4192)  mae_loss: 0.2397 (0.2511)  classification_loss: 1.1580 (1.1681)  loss_mask: 0.0001 (0.0001)  time: 0.1973  data: 0.0003  max mem: 5511
[09:48:31.167500] Epoch: [78] Total time: 0:02:35 (0.1990 s / it)
[09:48:31.167990] Averaged stats: lr: 0.000030  training_loss: 1.4083 (1.4192)  mae_loss: 0.2397 (0.2511)  classification_loss: 1.1580 (1.1681)  loss_mask: 0.0001 (0.0001)
[09:48:31.845097] Test:  [  0/157]  eta: 0:01:45  testing_loss: 0.4237 (0.4237)  acc1: 89.0625 (89.0625)  acc5: 100.0000 (100.0000)  time: 0.6720  data: 0.6423  max mem: 5511
[09:48:32.134667] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4836 (0.4990)  acc1: 84.3750 (84.3750)  acc5: 100.0000 (99.5739)  time: 0.0871  data: 0.0586  max mem: 5511
[09:48:32.432824] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4514 (0.4705)  acc1: 84.3750 (85.6399)  acc5: 100.0000 (99.6280)  time: 0.0292  data: 0.0005  max mem: 5511
[09:48:32.726253] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4514 (0.4871)  acc1: 85.9375 (84.7278)  acc5: 100.0000 (99.4456)  time: 0.0295  data: 0.0006  max mem: 5511
[09:48:33.017904] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4696 (0.4913)  acc1: 85.9375 (84.9466)  acc5: 98.4375 (99.3140)  time: 0.0291  data: 0.0003  max mem: 5511
[09:48:33.305746] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4696 (0.4843)  acc1: 85.9375 (85.2941)  acc5: 100.0000 (99.3260)  time: 0.0288  data: 0.0002  max mem: 5511
[09:48:33.594302] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4542 (0.4742)  acc1: 87.5000 (85.8350)  acc5: 100.0000 (99.3340)  time: 0.0287  data: 0.0002  max mem: 5511
[09:48:33.882094] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4252 (0.4732)  acc1: 87.5000 (86.0035)  acc5: 100.0000 (99.3618)  time: 0.0287  data: 0.0002  max mem: 5511
[09:48:34.169372] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4713 (0.4809)  acc1: 84.3750 (85.7832)  acc5: 100.0000 (99.3441)  time: 0.0286  data: 0.0002  max mem: 5511
[09:48:34.453322] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4393 (0.4753)  acc1: 85.9375 (86.0749)  acc5: 100.0000 (99.3304)  time: 0.0284  data: 0.0002  max mem: 5511
[09:48:34.737148] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.4393 (0.4769)  acc1: 85.9375 (85.9994)  acc5: 100.0000 (99.3348)  time: 0.0283  data: 0.0002  max mem: 5511
[09:48:35.022178] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5108 (0.4803)  acc1: 84.3750 (85.9375)  acc5: 100.0000 (99.3102)  time: 0.0283  data: 0.0002  max mem: 5511
[09:48:35.307396] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5108 (0.4801)  acc1: 84.3750 (85.9892)  acc5: 100.0000 (99.2898)  time: 0.0284  data: 0.0002  max mem: 5511
[09:48:35.595070] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4569 (0.4802)  acc1: 84.3750 (85.8659)  acc5: 100.0000 (99.2963)  time: 0.0285  data: 0.0002  max mem: 5511
[09:48:35.881172] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4522 (0.4783)  acc1: 85.9375 (85.9818)  acc5: 100.0000 (99.3240)  time: 0.0285  data: 0.0002  max mem: 5511
[09:48:36.163784] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4522 (0.4768)  acc1: 87.5000 (85.9996)  acc5: 100.0000 (99.3171)  time: 0.0283  data: 0.0001  max mem: 5511
[09:48:36.316436] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4415 (0.4767)  acc1: 85.9375 (85.9300)  acc5: 100.0000 (99.3200)  time: 0.0272  data: 0.0001  max mem: 5511
[09:48:36.484824] Test: Total time: 0:00:05 (0.0338 s / it)
[09:48:36.485359] * Acc@1 85.930 Acc@5 99.320 loss 0.477
[09:48:36.485772] Accuracy of the network on the 10000 test images: 85.9%
[09:48:36.486087] Max accuracy: 86.02%
[09:48:36.873353] log_dir: ./output_dir
[09:48:37.897170] Epoch: [79]  [  0/781]  eta: 0:13:18  lr: 0.000030  training_loss: 1.2802 (1.2802)  mae_loss: 0.2481 (0.2481)  classification_loss: 1.0321 (1.0321)  loss_mask: 0.0000 (0.0000)  time: 1.0220  data: 0.7655  max mem: 5511
[09:48:41.872252] Epoch: [79]  [ 20/781]  eta: 0:03:01  lr: 0.000030  training_loss: 1.3714 (1.3870)  mae_loss: 0.2481 (0.2531)  classification_loss: 1.1027 (1.1338)  loss_mask: 0.0001 (0.0001)  time: 0.1987  data: 0.0002  max mem: 5511
[09:48:45.833239] Epoch: [79]  [ 40/781]  eta: 0:02:41  lr: 0.000030  training_loss: 1.4134 (1.4002)  mae_loss: 0.2569 (0.2546)  classification_loss: 1.1720 (1.1455)  loss_mask: 0.0001 (0.0001)  time: 0.1980  data: 0.0003  max mem: 5511
[09:48:49.769301] Epoch: [79]  [ 60/781]  eta: 0:02:32  lr: 0.000030  training_loss: 1.4059 (1.4046)  mae_loss: 0.2401 (0.2517)  classification_loss: 1.1439 (1.1528)  loss_mask: 0.0000 (0.0001)  time: 0.1967  data: 0.0003  max mem: 5511
[09:48:53.734551] Epoch: [79]  [ 80/781]  eta: 0:02:25  lr: 0.000030  training_loss: 1.3847 (1.4004)  mae_loss: 0.2619 (0.2530)  classification_loss: 1.1153 (1.1473)  loss_mask: 0.0001 (0.0001)  time: 0.1982  data: 0.0003  max mem: 5511
[09:48:57.690879] Epoch: [79]  [100/781]  eta: 0:02:20  lr: 0.000029  training_loss: 1.4290 (1.4113)  mae_loss: 0.2466 (0.2525)  classification_loss: 1.2035 (1.1587)  loss_mask: 0.0001 (0.0001)  time: 0.1977  data: 0.0002  max mem: 5511
[09:49:01.651792] Epoch: [79]  [120/781]  eta: 0:02:15  lr: 0.000029  training_loss: 1.3763 (1.4093)  mae_loss: 0.2458 (0.2503)  classification_loss: 1.1317 (1.1589)  loss_mask: 0.0001 (0.0001)  time: 0.1980  data: 0.0003  max mem: 5511
[09:49:05.587418] Epoch: [79]  [140/781]  eta: 0:02:10  lr: 0.000029  training_loss: 1.3579 (1.4042)  mae_loss: 0.2596 (0.2510)  classification_loss: 1.0863 (1.1532)  loss_mask: 0.0001 (0.0001)  time: 0.1967  data: 0.0002  max mem: 5511
[09:49:09.527246] Epoch: [79]  [160/781]  eta: 0:02:05  lr: 0.000029  training_loss: 1.3774 (1.4037)  mae_loss: 0.2464 (0.2497)  classification_loss: 1.1545 (1.1539)  loss_mask: 0.0000 (0.0001)  time: 0.1969  data: 0.0003  max mem: 5511
[09:49:13.500372] Epoch: [79]  [180/781]  eta: 0:02:01  lr: 0.000029  training_loss: 1.4082 (1.4049)  mae_loss: 0.2563 (0.2502)  classification_loss: 1.1318 (1.1546)  loss_mask: 0.0001 (0.0001)  time: 0.1986  data: 0.0003  max mem: 5511
[09:49:17.425708] Epoch: [79]  [200/781]  eta: 0:01:57  lr: 0.000029  training_loss: 1.4172 (1.4048)  mae_loss: 0.2473 (0.2500)  classification_loss: 1.1940 (1.1548)  loss_mask: 0.0000 (0.0001)  time: 0.1962  data: 0.0002  max mem: 5511
[09:49:21.389659] Epoch: [79]  [220/781]  eta: 0:01:52  lr: 0.000029  training_loss: 1.4214 (1.4086)  mae_loss: 0.2376 (0.2496)  classification_loss: 1.1941 (1.1590)  loss_mask: 0.0000 (0.0001)  time: 0.1981  data: 0.0003  max mem: 5511
[09:49:25.347795] Epoch: [79]  [240/781]  eta: 0:01:48  lr: 0.000029  training_loss: 1.4286 (1.4098)  mae_loss: 0.2496 (0.2498)  classification_loss: 1.1709 (1.1600)  loss_mask: 0.0000 (0.0001)  time: 0.1978  data: 0.0002  max mem: 5511
[09:49:29.292704] Epoch: [79]  [260/781]  eta: 0:01:44  lr: 0.000029  training_loss: 1.4095 (1.4119)  mae_loss: 0.2342 (0.2497)  classification_loss: 1.1604 (1.1622)  loss_mask: 0.0001 (0.0001)  time: 0.1972  data: 0.0002  max mem: 5511
[09:49:33.217608] Epoch: [79]  [280/781]  eta: 0:01:40  lr: 0.000029  training_loss: 1.3635 (1.4118)  mae_loss: 0.2515 (0.2495)  classification_loss: 1.1310 (1.1622)  loss_mask: 0.0001 (0.0001)  time: 0.1962  data: 0.0002  max mem: 5511
[09:49:37.158740] Epoch: [79]  [300/781]  eta: 0:01:36  lr: 0.000029  training_loss: 1.4194 (1.4111)  mae_loss: 0.2324 (0.2487)  classification_loss: 1.1717 (1.1624)  loss_mask: 0.0000 (0.0001)  time: 0.1970  data: 0.0002  max mem: 5511
[09:49:41.117073] Epoch: [79]  [320/781]  eta: 0:01:32  lr: 0.000029  training_loss: 1.4326 (1.4138)  mae_loss: 0.2626 (0.2496)  classification_loss: 1.1784 (1.1641)  loss_mask: 0.0001 (0.0001)  time: 0.1978  data: 0.0002  max mem: 5511
[09:49:45.108065] Epoch: [79]  [340/781]  eta: 0:01:28  lr: 0.000029  training_loss: 1.4012 (1.4142)  mae_loss: 0.2460 (0.2494)  classification_loss: 1.1694 (1.1647)  loss_mask: 0.0000 (0.0001)  time: 0.1995  data: 0.0002  max mem: 5511
[09:49:49.056305] Epoch: [79]  [360/781]  eta: 0:01:24  lr: 0.000029  training_loss: 1.4220 (1.4152)  mae_loss: 0.2519 (0.2496)  classification_loss: 1.1501 (1.1655)  loss_mask: 0.0001 (0.0001)  time: 0.1973  data: 0.0004  max mem: 5511
[09:49:53.001800] Epoch: [79]  [380/781]  eta: 0:01:20  lr: 0.000029  training_loss: 1.4080 (1.4173)  mae_loss: 0.2422 (0.2494)  classification_loss: 1.1707 (1.1678)  loss_mask: 0.0001 (0.0001)  time: 0.1972  data: 0.0002  max mem: 5511
[09:49:56.969000] Epoch: [79]  [400/781]  eta: 0:01:16  lr: 0.000028  training_loss: 1.4575 (1.4180)  mae_loss: 0.2460 (0.2498)  classification_loss: 1.1771 (1.1681)  loss_mask: 0.0001 (0.0001)  time: 0.1983  data: 0.0002  max mem: 5511
[09:50:00.915354] Epoch: [79]  [420/781]  eta: 0:01:12  lr: 0.000028  training_loss: 1.4630 (1.4196)  mae_loss: 0.2560 (0.2499)  classification_loss: 1.1983 (1.1696)  loss_mask: 0.0001 (0.0001)  time: 0.1972  data: 0.0002  max mem: 5511
[09:50:04.857805] Epoch: [79]  [440/781]  eta: 0:01:08  lr: 0.000028  training_loss: 1.4759 (1.4215)  mae_loss: 0.2571 (0.2504)  classification_loss: 1.2061 (1.1710)  loss_mask: 0.0001 (0.0001)  time: 0.1970  data: 0.0002  max mem: 5511
[09:50:08.797648] Epoch: [79]  [460/781]  eta: 0:01:03  lr: 0.000028  training_loss: 1.3615 (1.4191)  mae_loss: 0.2395 (0.2500)  classification_loss: 1.1227 (1.1690)  loss_mask: 0.0001 (0.0001)  time: 0.1969  data: 0.0002  max mem: 5511
[09:50:12.740192] Epoch: [79]  [480/781]  eta: 0:00:59  lr: 0.000028  training_loss: 1.4446 (1.4204)  mae_loss: 0.2442 (0.2501)  classification_loss: 1.2014 (1.1703)  loss_mask: 0.0001 (0.0001)  time: 0.1970  data: 0.0003  max mem: 5511
[09:50:16.710182] Epoch: [79]  [500/781]  eta: 0:00:55  lr: 0.000028  training_loss: 1.4611 (1.4215)  mae_loss: 0.2499 (0.2502)  classification_loss: 1.1805 (1.1711)  loss_mask: 0.0001 (0.0002)  time: 0.1984  data: 0.0003  max mem: 5511
[09:50:20.647651] Epoch: [79]  [520/781]  eta: 0:00:51  lr: 0.000028  training_loss: 1.3705 (1.4204)  mae_loss: 0.2321 (0.2499)  classification_loss: 1.1289 (1.1702)  loss_mask: 0.0002 (0.0002)  time: 0.1968  data: 0.0002  max mem: 5511
[09:50:24.576523] Epoch: [79]  [540/781]  eta: 0:00:47  lr: 0.000028  training_loss: 1.4106 (1.4201)  mae_loss: 0.2414 (0.2500)  classification_loss: 1.1548 (1.1699)  loss_mask: 0.0001 (0.0002)  time: 0.1963  data: 0.0002  max mem: 5511
[09:50:28.524085] Epoch: [79]  [560/781]  eta: 0:00:43  lr: 0.000028  training_loss: 1.4601 (1.4203)  mae_loss: 0.2566 (0.2502)  classification_loss: 1.1934 (1.1698)  loss_mask: 0.0001 (0.0002)  time: 0.1973  data: 0.0003  max mem: 5511
[09:50:32.467523] Epoch: [79]  [580/781]  eta: 0:00:39  lr: 0.000028  training_loss: 1.4285 (1.4208)  mae_loss: 0.2386 (0.2502)  classification_loss: 1.1629 (1.1704)  loss_mask: 0.0001 (0.0002)  time: 0.1971  data: 0.0002  max mem: 5511
[09:50:36.463083] Epoch: [79]  [600/781]  eta: 0:00:35  lr: 0.000028  training_loss: 1.4029 (1.4200)  mae_loss: 0.2458 (0.2500)  classification_loss: 1.1650 (1.1697)  loss_mask: 0.0001 (0.0002)  time: 0.1997  data: 0.0002  max mem: 5511
[09:50:40.397197] Epoch: [79]  [620/781]  eta: 0:00:32  lr: 0.000028  training_loss: 1.3885 (1.4189)  mae_loss: 0.2429 (0.2501)  classification_loss: 1.1195 (1.1685)  loss_mask: 0.0001 (0.0002)  time: 0.1966  data: 0.0003  max mem: 5511
[09:50:44.353749] Epoch: [79]  [640/781]  eta: 0:00:28  lr: 0.000028  training_loss: 1.4276 (1.4187)  mae_loss: 0.2506 (0.2501)  classification_loss: 1.1682 (1.1683)  loss_mask: 0.0001 (0.0002)  time: 0.1977  data: 0.0003  max mem: 5511
[09:50:48.308731] Epoch: [79]  [660/781]  eta: 0:00:24  lr: 0.000028  training_loss: 1.3673 (1.4178)  mae_loss: 0.2456 (0.2501)  classification_loss: 1.1425 (1.1675)  loss_mask: 0.0001 (0.0002)  time: 0.1977  data: 0.0002  max mem: 5511
[09:50:52.251700] Epoch: [79]  [680/781]  eta: 0:00:20  lr: 0.000028  training_loss: 1.4218 (1.4186)  mae_loss: 0.2440 (0.2502)  classification_loss: 1.2017 (1.1682)  loss_mask: 0.0001 (0.0002)  time: 0.1970  data: 0.0002  max mem: 5511
[09:50:56.197900] Epoch: [79]  [700/781]  eta: 0:00:16  lr: 0.000028  training_loss: 1.4033 (1.4184)  mae_loss: 0.2420 (0.2499)  classification_loss: 1.1354 (1.1682)  loss_mask: 0.0001 (0.0002)  time: 0.1972  data: 0.0003  max mem: 5511
[09:51:00.143149] Epoch: [79]  [720/781]  eta: 0:00:12  lr: 0.000027  training_loss: 1.4191 (1.4184)  mae_loss: 0.2387 (0.2498)  classification_loss: 1.1827 (1.1684)  loss_mask: 0.0001 (0.0002)  time: 0.1972  data: 0.0002  max mem: 5511
[09:51:04.103708] Epoch: [79]  [740/781]  eta: 0:00:08  lr: 0.000027  training_loss: 1.4446 (1.4189)  mae_loss: 0.2532 (0.2499)  classification_loss: 1.1966 (1.1688)  loss_mask: 0.0001 (0.0002)  time: 0.1980  data: 0.0002  max mem: 5511
[09:51:08.051623] Epoch: [79]  [760/781]  eta: 0:00:04  lr: 0.000027  training_loss: 1.3962 (1.4190)  mae_loss: 0.2393 (0.2496)  classification_loss: 1.1752 (1.1692)  loss_mask: 0.0001 (0.0002)  time: 0.1973  data: 0.0002  max mem: 5511
[09:51:12.022165] Epoch: [79]  [780/781]  eta: 0:00:00  lr: 0.000027  training_loss: 1.4246 (1.4194)  mae_loss: 0.2498 (0.2497)  classification_loss: 1.1584 (1.1695)  loss_mask: 0.0001 (0.0002)  time: 0.1984  data: 0.0002  max mem: 5511
[09:51:12.206813] Epoch: [79] Total time: 0:02:35 (0.1989 s / it)
[09:51:12.207408] Averaged stats: lr: 0.000027  training_loss: 1.4246 (1.4194)  mae_loss: 0.2498 (0.2497)  classification_loss: 1.1584 (1.1695)  loss_mask: 0.0001 (0.0002)
[09:51:12.903092] Test:  [  0/157]  eta: 0:01:48  testing_loss: 0.4341 (0.4341)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.6882  data: 0.6549  max mem: 5511
[09:51:13.198187] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.4695 (0.4791)  acc1: 84.3750 (84.9432)  acc5: 100.0000 (99.4318)  time: 0.0892  data: 0.0597  max mem: 5511
[09:51:13.482885] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4514 (0.4571)  acc1: 85.9375 (86.0119)  acc5: 100.0000 (99.4792)  time: 0.0288  data: 0.0002  max mem: 5511
[09:51:13.771353] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4314 (0.4721)  acc1: 85.9375 (85.3327)  acc5: 100.0000 (99.4456)  time: 0.0285  data: 0.0002  max mem: 5511
[09:51:14.057792] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4652 (0.4807)  acc1: 84.3750 (85.1753)  acc5: 100.0000 (99.3521)  time: 0.0285  data: 0.0002  max mem: 5511
[09:51:14.351530] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4500 (0.4730)  acc1: 85.9375 (85.5392)  acc5: 100.0000 (99.3260)  time: 0.0289  data: 0.0002  max mem: 5511
[09:51:14.636125] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4380 (0.4657)  acc1: 87.5000 (85.9887)  acc5: 100.0000 (99.3340)  time: 0.0288  data: 0.0002  max mem: 5511
[09:51:14.922337] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4280 (0.4634)  acc1: 87.5000 (86.0475)  acc5: 100.0000 (99.3178)  time: 0.0284  data: 0.0002  max mem: 5511
[09:51:15.206336] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4636 (0.4715)  acc1: 85.9375 (85.8989)  acc5: 100.0000 (99.2284)  time: 0.0284  data: 0.0002  max mem: 5511
[09:51:15.490472] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4505 (0.4655)  acc1: 85.9375 (86.1951)  acc5: 100.0000 (99.2273)  time: 0.0283  data: 0.0002  max mem: 5511
[09:51:15.774908] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.4314 (0.4668)  acc1: 87.5000 (86.1696)  acc5: 100.0000 (99.2265)  time: 0.0283  data: 0.0002  max mem: 5511
[09:51:16.059172] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4719 (0.4692)  acc1: 85.9375 (86.1346)  acc5: 100.0000 (99.2258)  time: 0.0283  data: 0.0002  max mem: 5511
[09:51:16.342254] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4719 (0.4682)  acc1: 85.9375 (86.1958)  acc5: 100.0000 (99.2252)  time: 0.0282  data: 0.0002  max mem: 5511
[09:51:16.626147] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4895 (0.4682)  acc1: 84.3750 (86.1164)  acc5: 100.0000 (99.2486)  time: 0.0282  data: 0.0002  max mem: 5511
[09:51:16.908983] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4333 (0.4656)  acc1: 87.5000 (86.2035)  acc5: 100.0000 (99.2686)  time: 0.0282  data: 0.0002  max mem: 5511
[09:51:17.191368] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4584 (0.4648)  acc1: 85.9375 (86.2272)  acc5: 100.0000 (99.2757)  time: 0.0281  data: 0.0001  max mem: 5511
[09:51:17.342498] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4584 (0.4647)  acc1: 85.9375 (86.2100)  acc5: 100.0000 (99.2800)  time: 0.0272  data: 0.0001  max mem: 5511
[09:51:17.498495] Test: Total time: 0:00:05 (0.0337 s / it)
[09:51:17.499292] * Acc@1 86.210 Acc@5 99.280 loss 0.465
[09:51:17.499646] Accuracy of the network on the 10000 test images: 86.2%
[09:51:17.499883] Max accuracy: 86.21%
[09:51:17.764601] log_dir: ./output_dir
[09:51:18.626719] Epoch: [80]  [  0/781]  eta: 0:11:11  lr: 0.000027  training_loss: 1.3162 (1.3162)  mae_loss: 0.2482 (0.2482)  classification_loss: 1.0679 (1.0679)  loss_mask: 0.0001 (0.0001)  time: 0.8602  data: 0.6372  max mem: 5511
[09:51:22.567614] Epoch: [80]  [ 20/781]  eta: 0:02:53  lr: 0.000027  training_loss: 1.3871 (1.4026)  mae_loss: 0.2401 (0.2476)  classification_loss: 1.1579 (1.1549)  loss_mask: 0.0001 (0.0001)  time: 0.1970  data: 0.0003  max mem: 5511
[09:51:26.540638] Epoch: [80]  [ 40/781]  eta: 0:02:38  lr: 0.000027  training_loss: 1.4251 (1.4087)  mae_loss: 0.2480 (0.2446)  classification_loss: 1.1918 (1.1641)  loss_mask: 0.0001 (0.0001)  time: 0.1986  data: 0.0002  max mem: 5511
[09:51:30.507219] Epoch: [80]  [ 60/781]  eta: 0:02:30  lr: 0.000027  training_loss: 1.4303 (1.4271)  mae_loss: 0.2499 (0.2480)  classification_loss: 1.1822 (1.1790)  loss_mask: 0.0001 (0.0001)  time: 0.1982  data: 0.0003  max mem: 5511
[09:51:34.468033] Epoch: [80]  [ 80/781]  eta: 0:02:24  lr: 0.000027  training_loss: 1.4049 (1.4197)  mae_loss: 0.2484 (0.2476)  classification_loss: 1.1426 (1.1721)  loss_mask: 0.0001 (0.0001)  time: 0.1979  data: 0.0002  max mem: 5511
[09:51:38.407260] Epoch: [80]  [100/781]  eta: 0:02:19  lr: 0.000027  training_loss: 1.3759 (1.4129)  mae_loss: 0.2486 (0.2474)  classification_loss: 1.1146 (1.1654)  loss_mask: 0.0001 (0.0001)  time: 0.1969  data: 0.0002  max mem: 5511
[09:51:42.379560] Epoch: [80]  [120/781]  eta: 0:02:14  lr: 0.000027  training_loss: 1.3839 (1.4125)  mae_loss: 0.2464 (0.2479)  classification_loss: 1.1231 (1.1645)  loss_mask: 0.0001 (0.0001)  time: 0.1985  data: 0.0002  max mem: 5511
[09:51:46.364459] Epoch: [80]  [140/781]  eta: 0:02:09  lr: 0.000027  training_loss: 1.3413 (1.4091)  mae_loss: 0.2449 (0.2483)  classification_loss: 1.0912 (1.1607)  loss_mask: 0.0001 (0.0001)  time: 0.1991  data: 0.0002  max mem: 5511
[09:51:50.315865] Epoch: [80]  [160/781]  eta: 0:02:05  lr: 0.000027  training_loss: 1.3699 (1.4076)  mae_loss: 0.2484 (0.2489)  classification_loss: 1.1188 (1.1587)  loss_mask: 0.0001 (0.0001)  time: 0.1975  data: 0.0002  max mem: 5511
[09:51:54.270805] Epoch: [80]  [180/781]  eta: 0:02:01  lr: 0.000027  training_loss: 1.4263 (1.4099)  mae_loss: 0.2421 (0.2477)  classification_loss: 1.1842 (1.1621)  loss_mask: 0.0000 (0.0001)  time: 0.1977  data: 0.0002  max mem: 5511
[09:51:58.221854] Epoch: [80]  [200/781]  eta: 0:01:56  lr: 0.000027  training_loss: 1.3403 (1.4052)  mae_loss: 0.2428 (0.2473)  classification_loss: 1.0928 (1.1578)  loss_mask: 0.0000 (0.0001)  time: 0.1975  data: 0.0002  max mem: 5511
[09:52:02.153080] Epoch: [80]  [220/781]  eta: 0:01:52  lr: 0.000027  training_loss: 1.3782 (1.4041)  mae_loss: 0.2371 (0.2467)  classification_loss: 1.1410 (1.1574)  loss_mask: 0.0001 (0.0001)  time: 0.1965  data: 0.0002  max mem: 5511
[09:52:06.085126] Epoch: [80]  [240/781]  eta: 0:01:48  lr: 0.000026  training_loss: 1.4037 (1.4059)  mae_loss: 0.2536 (0.2473)  classification_loss: 1.1466 (1.1586)  loss_mask: 0.0000 (0.0001)  time: 0.1965  data: 0.0002  max mem: 5511
[09:52:10.048535] Epoch: [80]  [260/781]  eta: 0:01:44  lr: 0.000026  training_loss: 1.3971 (1.4075)  mae_loss: 0.2364 (0.2470)  classification_loss: 1.1738 (1.1604)  loss_mask: 0.0001 (0.0001)  time: 0.1981  data: 0.0002  max mem: 5511
[09:52:14.008150] Epoch: [80]  [280/781]  eta: 0:01:40  lr: 0.000026  training_loss: 1.4154 (1.4073)  mae_loss: 0.2415 (0.2472)  classification_loss: 1.1485 (1.1600)  loss_mask: 0.0000 (0.0001)  time: 0.1979  data: 0.0003  max mem: 5511
[09:52:17.957330] Epoch: [80]  [300/781]  eta: 0:01:36  lr: 0.000026  training_loss: 1.4239 (1.4076)  mae_loss: 0.2365 (0.2470)  classification_loss: 1.1825 (1.1606)  loss_mask: 0.0000 (0.0001)  time: 0.1974  data: 0.0002  max mem: 5511
[09:52:21.878538] Epoch: [80]  [320/781]  eta: 0:01:32  lr: 0.000026  training_loss: 1.4075 (1.4088)  mae_loss: 0.2519 (0.2471)  classification_loss: 1.1747 (1.1616)  loss_mask: 0.0000 (0.0001)  time: 0.1960  data: 0.0002  max mem: 5511
[09:52:25.823034] Epoch: [80]  [340/781]  eta: 0:01:27  lr: 0.000026  training_loss: 1.3787 (1.4080)  mae_loss: 0.2427 (0.2476)  classification_loss: 1.0982 (1.1603)  loss_mask: 0.0001 (0.0001)  time: 0.1971  data: 0.0002  max mem: 5511
[09:52:29.751521] Epoch: [80]  [360/781]  eta: 0:01:23  lr: 0.000026  training_loss: 1.3836 (1.4066)  mae_loss: 0.2445 (0.2476)  classification_loss: 1.1313 (1.1590)  loss_mask: 0.0001 (0.0001)  time: 0.1964  data: 0.0003  max mem: 5511
[09:52:33.697598] Epoch: [80]  [380/781]  eta: 0:01:19  lr: 0.000026  training_loss: 1.3758 (1.4056)  mae_loss: 0.2363 (0.2474)  classification_loss: 1.1195 (1.1582)  loss_mask: 0.0000 (0.0001)  time: 0.1972  data: 0.0002  max mem: 5511
[09:52:37.639395] Epoch: [80]  [400/781]  eta: 0:01:15  lr: 0.000026  training_loss: 1.3451 (1.4046)  mae_loss: 0.2533 (0.2476)  classification_loss: 1.0918 (1.1569)  loss_mask: 0.0000 (0.0001)  time: 0.1970  data: 0.0002  max mem: 5511
[09:52:41.613503] Epoch: [80]  [420/781]  eta: 0:01:11  lr: 0.000026  training_loss: 1.3998 (1.4053)  mae_loss: 0.2616 (0.2482)  classification_loss: 1.1334 (1.1571)  loss_mask: 0.0000 (0.0001)  time: 0.1986  data: 0.0002  max mem: 5511
[09:52:45.561960] Epoch: [80]  [440/781]  eta: 0:01:07  lr: 0.000026  training_loss: 1.4071 (1.4065)  mae_loss: 0.2498 (0.2484)  classification_loss: 1.1472 (1.1580)  loss_mask: 0.0001 (0.0001)  time: 0.1973  data: 0.0002  max mem: 5511
[09:52:49.524002] Epoch: [80]  [460/781]  eta: 0:01:03  lr: 0.000026  training_loss: 1.4045 (1.4064)  mae_loss: 0.2480 (0.2485)  classification_loss: 1.1649 (1.1579)  loss_mask: 0.0001 (0.0001)  time: 0.1980  data: 0.0002  max mem: 5511
[09:52:53.484025] Epoch: [80]  [480/781]  eta: 0:00:59  lr: 0.000026  training_loss: 1.3877 (1.4059)  mae_loss: 0.2532 (0.2486)  classification_loss: 1.1420 (1.1573)  loss_mask: 0.0001 (0.0001)  time: 0.1979  data: 0.0002  max mem: 5511
[09:52:57.440504] Epoch: [80]  [500/781]  eta: 0:00:55  lr: 0.000026  training_loss: 1.4220 (1.4060)  mae_loss: 0.2372 (0.2486)  classification_loss: 1.1566 (1.1574)  loss_mask: 0.0001 (0.0001)  time: 0.1977  data: 0.0002  max mem: 5511
[09:53:01.382576] Epoch: [80]  [520/781]  eta: 0:00:51  lr: 0.000026  training_loss: 1.4165 (1.4062)  mae_loss: 0.2532 (0.2490)  classification_loss: 1.1906 (1.1572)  loss_mask: 0.0001 (0.0001)  time: 0.1970  data: 0.0002  max mem: 5511
[09:53:05.330423] Epoch: [80]  [540/781]  eta: 0:00:47  lr: 0.000026  training_loss: 1.4150 (1.4068)  mae_loss: 0.2494 (0.2490)  classification_loss: 1.1550 (1.1577)  loss_mask: 0.0001 (0.0001)  time: 0.1973  data: 0.0003  max mem: 5511
[09:53:09.303950] Epoch: [80]  [560/781]  eta: 0:00:43  lr: 0.000025  training_loss: 1.4139 (1.4072)  mae_loss: 0.2543 (0.2495)  classification_loss: 1.1469 (1.1576)  loss_mask: 0.0001 (0.0001)  time: 0.1986  data: 0.0006  max mem: 5511
[09:53:13.243482] Epoch: [80]  [580/781]  eta: 0:00:39  lr: 0.000025  training_loss: 1.4197 (1.4080)  mae_loss: 0.2448 (0.2494)  classification_loss: 1.1561 (1.1586)  loss_mask: 0.0000 (0.0001)  time: 0.1969  data: 0.0002  max mem: 5511
[09:53:17.195191] Epoch: [80]  [600/781]  eta: 0:00:35  lr: 0.000025  training_loss: 1.3781 (1.4069)  mae_loss: 0.2295 (0.2489)  classification_loss: 1.1175 (1.1579)  loss_mask: 0.0000 (0.0001)  time: 0.1975  data: 0.0002  max mem: 5511
[09:53:21.134004] Epoch: [80]  [620/781]  eta: 0:00:31  lr: 0.000025  training_loss: 1.4289 (1.4073)  mae_loss: 0.2446 (0.2491)  classification_loss: 1.1644 (1.1582)  loss_mask: 0.0000 (0.0001)  time: 0.1968  data: 0.0002  max mem: 5511
[09:53:25.074488] Epoch: [80]  [640/781]  eta: 0:00:27  lr: 0.000025  training_loss: 1.4129 (1.4071)  mae_loss: 0.2348 (0.2487)  classification_loss: 1.1348 (1.1583)  loss_mask: 0.0001 (0.0001)  time: 0.1970  data: 0.0002  max mem: 5511
[09:53:28.999329] Epoch: [80]  [660/781]  eta: 0:00:24  lr: 0.000025  training_loss: 1.4022 (1.4071)  mae_loss: 0.2557 (0.2490)  classification_loss: 1.1427 (1.1581)  loss_mask: 0.0000 (0.0001)  time: 0.1962  data: 0.0002  max mem: 5511
[09:53:32.958766] Epoch: [80]  [680/781]  eta: 0:00:20  lr: 0.000025  training_loss: 1.4013 (1.4077)  mae_loss: 0.2431 (0.2491)  classification_loss: 1.1554 (1.1586)  loss_mask: 0.0000 (0.0001)  time: 0.1979  data: 0.0002  max mem: 5511
[09:53:36.897992] Epoch: [80]  [700/781]  eta: 0:00:16  lr: 0.000025  training_loss: 1.4348 (1.4095)  mae_loss: 0.2478 (0.2491)  classification_loss: 1.2008 (1.1603)  loss_mask: 0.0000 (0.0001)  time: 0.1968  data: 0.0002  max mem: 5511
[09:53:40.841574] Epoch: [80]  [720/781]  eta: 0:00:12  lr: 0.000025  training_loss: 1.4128 (1.4099)  mae_loss: 0.2410 (0.2491)  classification_loss: 1.1610 (1.1608)  loss_mask: 0.0000 (0.0001)  time: 0.1971  data: 0.0002  max mem: 5511
[09:53:44.780419] Epoch: [80]  [740/781]  eta: 0:00:08  lr: 0.000025  training_loss: 1.3966 (1.4094)  mae_loss: 0.2639 (0.2493)  classification_loss: 1.1149 (1.1600)  loss_mask: 0.0000 (0.0001)  time: 0.1969  data: 0.0002  max mem: 5511
[09:53:48.707661] Epoch: [80]  [760/781]  eta: 0:00:04  lr: 0.000025  training_loss: 1.4281 (1.4091)  mae_loss: 0.2463 (0.2495)  classification_loss: 1.1438 (1.1595)  loss_mask: 0.0000 (0.0001)  time: 0.1963  data: 0.0002  max mem: 5511
[09:53:52.645576] Epoch: [80]  [780/781]  eta: 0:00:00  lr: 0.000025  training_loss: 1.3649 (1.4085)  mae_loss: 0.2500 (0.2496)  classification_loss: 1.1138 (1.1588)  loss_mask: 0.0000 (0.0001)  time: 0.1968  data: 0.0002  max mem: 5511
[09:53:52.823750] Epoch: [80] Total time: 0:02:35 (0.1985 s / it)
[09:53:52.824976] Averaged stats: lr: 0.000025  training_loss: 1.3649 (1.4085)  mae_loss: 0.2500 (0.2496)  classification_loss: 1.1138 (1.1588)  loss_mask: 0.0000 (0.0001)
[09:53:54.192673] Test:  [  0/157]  eta: 0:01:38  testing_loss: 0.3884 (0.3884)  acc1: 89.0625 (89.0625)  acc5: 100.0000 (100.0000)  time: 0.6249  data: 0.5943  max mem: 5511
[09:53:54.482180] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5033 (0.4984)  acc1: 85.9375 (84.8011)  acc5: 100.0000 (99.4318)  time: 0.0829  data: 0.0542  max mem: 5511
[09:53:54.772973] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4655 (0.4765)  acc1: 85.9375 (85.7143)  acc5: 100.0000 (99.4792)  time: 0.0288  data: 0.0002  max mem: 5511
[09:53:55.057593] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4583 (0.4908)  acc1: 84.3750 (85.0302)  acc5: 100.0000 (99.2944)  time: 0.0286  data: 0.0002  max mem: 5511
[09:53:55.345784] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4667 (0.4958)  acc1: 82.8125 (84.9466)  acc5: 100.0000 (99.2378)  time: 0.0285  data: 0.0002  max mem: 5511
[09:53:55.631683] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4587 (0.4888)  acc1: 85.9375 (85.4473)  acc5: 100.0000 (99.2341)  time: 0.0285  data: 0.0002  max mem: 5511
[09:53:55.918285] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4516 (0.4825)  acc1: 87.5000 (85.6301)  acc5: 100.0000 (99.2572)  time: 0.0285  data: 0.0002  max mem: 5511
[09:53:56.214080] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4549 (0.4813)  acc1: 87.5000 (85.6954)  acc5: 100.0000 (99.2298)  time: 0.0289  data: 0.0003  max mem: 5511
[09:53:56.509677] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4864 (0.4869)  acc1: 85.9375 (85.5517)  acc5: 98.4375 (99.1898)  time: 0.0294  data: 0.0003  max mem: 5511
[09:53:56.799176] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4679 (0.4811)  acc1: 85.9375 (85.7658)  acc5: 98.4375 (99.1587)  time: 0.0291  data: 0.0002  max mem: 5511
[09:53:57.093015] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4448 (0.4827)  acc1: 87.5000 (85.8292)  acc5: 98.4375 (99.1646)  time: 0.0290  data: 0.0003  max mem: 5511
[09:53:57.380836] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4851 (0.4850)  acc1: 85.9375 (85.7404)  acc5: 100.0000 (99.1695)  time: 0.0289  data: 0.0003  max mem: 5511
[09:53:57.677000] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4808 (0.4833)  acc1: 85.9375 (85.8471)  acc5: 98.4375 (99.1477)  time: 0.0290  data: 0.0002  max mem: 5511
[09:53:57.963650] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4381 (0.4829)  acc1: 85.9375 (85.8182)  acc5: 100.0000 (99.1651)  time: 0.0290  data: 0.0003  max mem: 5511
[09:53:58.249871] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4394 (0.4802)  acc1: 85.9375 (85.9929)  acc5: 100.0000 (99.1800)  time: 0.0285  data: 0.0002  max mem: 5511
[09:53:58.535484] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4678 (0.4791)  acc1: 85.9375 (85.9892)  acc5: 100.0000 (99.2032)  time: 0.0284  data: 0.0002  max mem: 5511
[09:53:58.691031] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4678 (0.4790)  acc1: 85.9375 (86.0000)  acc5: 100.0000 (99.2100)  time: 0.0276  data: 0.0002  max mem: 5511
[09:53:58.885544] Test: Total time: 0:00:05 (0.0339 s / it)
[09:53:58.886292] * Acc@1 86.000 Acc@5 99.210 loss 0.479
[09:53:58.886858] Accuracy of the network on the 10000 test images: 86.0%
[09:53:58.887265] Max accuracy: 86.21%
[09:53:59.061051] log_dir: ./output_dir
[09:53:59.900312] Epoch: [81]  [  0/781]  eta: 0:10:54  lr: 0.000025  training_loss: 1.2562 (1.2562)  mae_loss: 0.2493 (0.2493)  classification_loss: 1.0068 (1.0068)  loss_mask: 0.0001 (0.0001)  time: 0.8375  data: 0.6278  max mem: 5511
[09:54:03.854120] Epoch: [81]  [ 20/781]  eta: 0:02:53  lr: 0.000025  training_loss: 1.3894 (1.3919)  mae_loss: 0.2445 (0.2539)  classification_loss: 1.1016 (1.1379)  loss_mask: 0.0000 (0.0001)  time: 0.1976  data: 0.0002  max mem: 5511
[09:54:07.802926] Epoch: [81]  [ 40/781]  eta: 0:02:37  lr: 0.000025  training_loss: 1.3607 (1.3818)  mae_loss: 0.2377 (0.2483)  classification_loss: 1.1365 (1.1335)  loss_mask: 0.0000 (0.0001)  time: 0.1973  data: 0.0002  max mem: 5511
[09:54:11.763622] Epoch: [81]  [ 60/781]  eta: 0:02:30  lr: 0.000025  training_loss: 1.3785 (1.3847)  mae_loss: 0.2286 (0.2449)  classification_loss: 1.1671 (1.1397)  loss_mask: 0.0000 (0.0000)  time: 0.1979  data: 0.0003  max mem: 5511
[09:54:15.716867] Epoch: [81]  [ 80/781]  eta: 0:02:24  lr: 0.000025  training_loss: 1.3549 (1.3799)  mae_loss: 0.2411 (0.2445)  classification_loss: 1.1130 (1.1354)  loss_mask: 0.0000 (0.0000)  time: 0.1976  data: 0.0003  max mem: 5511
[09:54:19.695112] Epoch: [81]  [100/781]  eta: 0:02:19  lr: 0.000024  training_loss: 1.4114 (1.3888)  mae_loss: 0.2560 (0.2480)  classification_loss: 1.1490 (1.1407)  loss_mask: 0.0000 (0.0000)  time: 0.1988  data: 0.0003  max mem: 5511
[09:54:23.618250] Epoch: [81]  [120/781]  eta: 0:02:14  lr: 0.000024  training_loss: 1.3209 (1.3820)  mae_loss: 0.2473 (0.2471)  classification_loss: 1.0894 (1.1349)  loss_mask: 0.0000 (0.0000)  time: 0.1961  data: 0.0003  max mem: 5511
[09:54:27.536250] Epoch: [81]  [140/781]  eta: 0:02:09  lr: 0.000024  training_loss: 1.4105 (1.3846)  mae_loss: 0.2523 (0.2478)  classification_loss: 1.1559 (1.1368)  loss_mask: 0.0000 (0.0000)  time: 0.1958  data: 0.0004  max mem: 5511
[09:54:31.483333] Epoch: [81]  [160/781]  eta: 0:02:04  lr: 0.000024  training_loss: 1.4028 (1.3874)  mae_loss: 0.2474 (0.2478)  classification_loss: 1.1617 (1.1396)  loss_mask: 0.0000 (0.0000)  time: 0.1973  data: 0.0003  max mem: 5511
[09:54:35.426836] Epoch: [81]  [180/781]  eta: 0:02:00  lr: 0.000024  training_loss: 1.3978 (1.3917)  mae_loss: 0.2575 (0.2491)  classification_loss: 1.1508 (1.1426)  loss_mask: 0.0001 (0.0001)  time: 0.1971  data: 0.0003  max mem: 5511
[09:54:39.358187] Epoch: [81]  [200/781]  eta: 0:01:56  lr: 0.000024  training_loss: 1.3807 (1.3907)  mae_loss: 0.2432 (0.2489)  classification_loss: 1.1218 (1.1417)  loss_mask: 0.0000 (0.0001)  time: 0.1965  data: 0.0003  max mem: 5511
[09:54:43.300287] Epoch: [81]  [220/781]  eta: 0:01:52  lr: 0.000024  training_loss: 1.3876 (1.3929)  mae_loss: 0.2585 (0.2494)  classification_loss: 1.1517 (1.1435)  loss_mask: 0.0000 (0.0001)  time: 0.1970  data: 0.0002  max mem: 5511
[09:54:47.256410] Epoch: [81]  [240/781]  eta: 0:01:48  lr: 0.000024  training_loss: 1.3967 (1.3931)  mae_loss: 0.2509 (0.2497)  classification_loss: 1.1348 (1.1434)  loss_mask: 0.0000 (0.0001)  time: 0.1977  data: 0.0002  max mem: 5511
[09:54:51.206951] Epoch: [81]  [260/781]  eta: 0:01:44  lr: 0.000024  training_loss: 1.3402 (1.3906)  mae_loss: 0.2362 (0.2491)  classification_loss: 1.1157 (1.1415)  loss_mask: 0.0000 (0.0001)  time: 0.1974  data: 0.0002  max mem: 5511
[09:54:55.176153] Epoch: [81]  [280/781]  eta: 0:01:40  lr: 0.000024  training_loss: 1.4244 (1.3920)  mae_loss: 0.2579 (0.2496)  classification_loss: 1.1669 (1.1423)  loss_mask: 0.0000 (0.0001)  time: 0.1984  data: 0.0002  max mem: 5511
[09:54:59.133924] Epoch: [81]  [300/781]  eta: 0:01:35  lr: 0.000024  training_loss: 1.4870 (1.3986)  mae_loss: 0.2477 (0.2498)  classification_loss: 1.2397 (1.1487)  loss_mask: 0.0000 (0.0001)  time: 0.1978  data: 0.0002  max mem: 5511
[09:55:03.087477] Epoch: [81]  [320/781]  eta: 0:01:31  lr: 0.000024  training_loss: 1.4128 (1.3990)  mae_loss: 0.2512 (0.2493)  classification_loss: 1.1705 (1.1497)  loss_mask: 0.0000 (0.0001)  time: 0.1975  data: 0.0003  max mem: 5511
[09:55:07.071315] Epoch: [81]  [340/781]  eta: 0:01:27  lr: 0.000024  training_loss: 1.4259 (1.4015)  mae_loss: 0.2407 (0.2489)  classification_loss: 1.1616 (1.1525)  loss_mask: 0.0000 (0.0001)  time: 0.1990  data: 0.0002  max mem: 5511
[09:55:11.004039] Epoch: [81]  [360/781]  eta: 0:01:23  lr: 0.000024  training_loss: 1.4051 (1.4024)  mae_loss: 0.2331 (0.2486)  classification_loss: 1.1777 (1.1538)  loss_mask: 0.0000 (0.0001)  time: 0.1966  data: 0.0002  max mem: 5511
[09:55:14.940924] Epoch: [81]  [380/781]  eta: 0:01:19  lr: 0.000024  training_loss: 1.4286 (1.4055)  mae_loss: 0.2449 (0.2490)  classification_loss: 1.1749 (1.1564)  loss_mask: 0.0000 (0.0001)  time: 0.1968  data: 0.0002  max mem: 5511
[09:55:18.869492] Epoch: [81]  [400/781]  eta: 0:01:15  lr: 0.000024  training_loss: 1.3602 (1.4045)  mae_loss: 0.2413 (0.2488)  classification_loss: 1.1198 (1.1555)  loss_mask: 0.0003 (0.0001)  time: 0.1964  data: 0.0003  max mem: 5511
[09:55:22.828065] Epoch: [81]  [420/781]  eta: 0:01:11  lr: 0.000023  training_loss: 1.3796 (1.4037)  mae_loss: 0.2379 (0.2484)  classification_loss: 1.1449 (1.1551)  loss_mask: 0.0000 (0.0001)  time: 0.1978  data: 0.0002  max mem: 5511
[09:55:26.780688] Epoch: [81]  [440/781]  eta: 0:01:07  lr: 0.000023  training_loss: 1.3305 (1.4012)  mae_loss: 0.2418 (0.2481)  classification_loss: 1.0853 (1.1530)  loss_mask: 0.0000 (0.0001)  time: 0.1975  data: 0.0002  max mem: 5511
[09:55:30.729851] Epoch: [81]  [460/781]  eta: 0:01:03  lr: 0.000023  training_loss: 1.3538 (1.3991)  mae_loss: 0.2457 (0.2481)  classification_loss: 1.1214 (1.1510)  loss_mask: 0.0000 (0.0001)  time: 0.1974  data: 0.0002  max mem: 5511
[09:55:34.685918] Epoch: [81]  [480/781]  eta: 0:00:59  lr: 0.000023  training_loss: 1.4126 (1.4002)  mae_loss: 0.2417 (0.2479)  classification_loss: 1.1442 (1.1523)  loss_mask: 0.0000 (0.0001)  time: 0.1977  data: 0.0003  max mem: 5511
[09:55:38.621010] Epoch: [81]  [500/781]  eta: 0:00:55  lr: 0.000023  training_loss: 1.4378 (1.4026)  mae_loss: 0.2409 (0.2482)  classification_loss: 1.1935 (1.1543)  loss_mask: 0.0000 (0.0001)  time: 0.1967  data: 0.0002  max mem: 5511
[09:55:42.558368] Epoch: [81]  [520/781]  eta: 0:00:51  lr: 0.000023  training_loss: 1.4096 (1.4024)  mae_loss: 0.2460 (0.2483)  classification_loss: 1.1155 (1.1541)  loss_mask: 0.0000 (0.0001)  time: 0.1968  data: 0.0003  max mem: 5511
[09:55:46.490399] Epoch: [81]  [540/781]  eta: 0:00:47  lr: 0.000023  training_loss: 1.4466 (1.4040)  mae_loss: 0.2497 (0.2488)  classification_loss: 1.1752 (1.1551)  loss_mask: 0.0000 (0.0001)  time: 0.1965  data: 0.0003  max mem: 5511
[09:55:50.425963] Epoch: [81]  [560/781]  eta: 0:00:43  lr: 0.000023  training_loss: 1.4011 (1.4047)  mae_loss: 0.2576 (0.2491)  classification_loss: 1.1608 (1.1556)  loss_mask: 0.0000 (0.0001)  time: 0.1967  data: 0.0002  max mem: 5511
[09:55:54.376569] Epoch: [81]  [580/781]  eta: 0:00:39  lr: 0.000023  training_loss: 1.3928 (1.4041)  mae_loss: 0.2607 (0.2493)  classification_loss: 1.1335 (1.1548)  loss_mask: 0.0000 (0.0001)  time: 0.1975  data: 0.0003  max mem: 5511
[09:55:58.330727] Epoch: [81]  [600/781]  eta: 0:00:35  lr: 0.000023  training_loss: 1.3401 (1.4035)  mae_loss: 0.2357 (0.2491)  classification_loss: 1.1174 (1.1543)  loss_mask: 0.0000 (0.0001)  time: 0.1976  data: 0.0005  max mem: 5511
[09:56:02.291122] Epoch: [81]  [620/781]  eta: 0:00:31  lr: 0.000023  training_loss: 1.3831 (1.4030)  mae_loss: 0.2442 (0.2489)  classification_loss: 1.1438 (1.1541)  loss_mask: 0.0000 (0.0001)  time: 0.1979  data: 0.0002  max mem: 5511
[09:56:06.242449] Epoch: [81]  [640/781]  eta: 0:00:27  lr: 0.000023  training_loss: 1.4106 (1.4034)  mae_loss: 0.2459 (0.2488)  classification_loss: 1.1790 (1.1545)  loss_mask: 0.0000 (0.0001)  time: 0.1975  data: 0.0003  max mem: 5511
[09:56:10.195480] Epoch: [81]  [660/781]  eta: 0:00:23  lr: 0.000023  training_loss: 1.3592 (1.4025)  mae_loss: 0.2361 (0.2484)  classification_loss: 1.1011 (1.1540)  loss_mask: 0.0000 (0.0001)  time: 0.1976  data: 0.0002  max mem: 5511
[09:56:14.162064] Epoch: [81]  [680/781]  eta: 0:00:20  lr: 0.000023  training_loss: 1.4195 (1.4028)  mae_loss: 0.2612 (0.2487)  classification_loss: 1.1489 (1.1540)  loss_mask: 0.0000 (0.0001)  time: 0.1983  data: 0.0002  max mem: 5511
[09:56:18.146816] Epoch: [81]  [700/781]  eta: 0:00:16  lr: 0.000023  training_loss: 1.3897 (1.4026)  mae_loss: 0.2369 (0.2487)  classification_loss: 1.1652 (1.1539)  loss_mask: 0.0000 (0.0001)  time: 0.1991  data: 0.0002  max mem: 5511
[09:56:22.095427] Epoch: [81]  [720/781]  eta: 0:00:12  lr: 0.000023  training_loss: 1.3670 (1.4022)  mae_loss: 0.2438 (0.2486)  classification_loss: 1.1305 (1.1535)  loss_mask: 0.0000 (0.0001)  time: 0.1973  data: 0.0002  max mem: 5511
[09:56:26.084555] Epoch: [81]  [740/781]  eta: 0:00:08  lr: 0.000023  training_loss: 1.4053 (1.4013)  mae_loss: 0.2404 (0.2485)  classification_loss: 1.1531 (1.1527)  loss_mask: 0.0000 (0.0001)  time: 0.1994  data: 0.0003  max mem: 5511
[09:56:30.016444] Epoch: [81]  [760/781]  eta: 0:00:04  lr: 0.000022  training_loss: 1.3493 (1.4010)  mae_loss: 0.2470 (0.2486)  classification_loss: 1.1092 (1.1524)  loss_mask: 0.0000 (0.0001)  time: 0.1965  data: 0.0002  max mem: 5511
[09:56:33.941341] Epoch: [81]  [780/781]  eta: 0:00:00  lr: 0.000022  training_loss: 1.4198 (1.4013)  mae_loss: 0.2463 (0.2486)  classification_loss: 1.1571 (1.1526)  loss_mask: 0.0001 (0.0001)  time: 0.1962  data: 0.0002  max mem: 5511
[09:56:34.132418] Epoch: [81] Total time: 0:02:35 (0.1986 s / it)
[09:56:34.132928] Averaged stats: lr: 0.000022  training_loss: 1.4198 (1.4013)  mae_loss: 0.2463 (0.2486)  classification_loss: 1.1571 (1.1526)  loss_mask: 0.0001 (0.0001)
[09:56:34.732720] Test:  [  0/157]  eta: 0:01:32  testing_loss: 0.4296 (0.4296)  acc1: 89.0625 (89.0625)  acc5: 100.0000 (100.0000)  time: 0.5872  data: 0.5577  max mem: 5511
[09:56:35.020581] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.4704 (0.4787)  acc1: 85.9375 (85.9375)  acc5: 100.0000 (99.7159)  time: 0.0793  data: 0.0509  max mem: 5511
[09:56:35.304505] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4646 (0.4582)  acc1: 85.9375 (86.3839)  acc5: 100.0000 (99.4792)  time: 0.0284  data: 0.0002  max mem: 5511
[09:56:35.586411] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.4411 (0.4733)  acc1: 85.9375 (85.4335)  acc5: 100.0000 (99.3448)  time: 0.0282  data: 0.0002  max mem: 5511
[09:56:35.868758] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.4526 (0.4772)  acc1: 84.3750 (85.4802)  acc5: 98.4375 (99.2378)  time: 0.0281  data: 0.0002  max mem: 5511
[09:56:36.150993] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4550 (0.4692)  acc1: 85.9375 (86.0294)  acc5: 100.0000 (99.2341)  time: 0.0281  data: 0.0002  max mem: 5511
[09:56:36.438104] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4277 (0.4623)  acc1: 85.9375 (86.2193)  acc5: 100.0000 (99.2316)  time: 0.0283  data: 0.0002  max mem: 5511
[09:56:36.724829] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4390 (0.4613)  acc1: 87.5000 (86.4657)  acc5: 98.4375 (99.1857)  time: 0.0286  data: 0.0002  max mem: 5511
[09:56:37.011885] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4617 (0.4681)  acc1: 85.9375 (86.2654)  acc5: 98.4375 (99.1512)  time: 0.0285  data: 0.0002  max mem: 5511
[09:56:37.299756] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4546 (0.4621)  acc1: 85.9375 (86.5041)  acc5: 98.4375 (99.1587)  time: 0.0285  data: 0.0002  max mem: 5511
[09:56:37.586122] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4370 (0.4629)  acc1: 89.0625 (86.6027)  acc5: 100.0000 (99.1955)  time: 0.0285  data: 0.0002  max mem: 5511
[09:56:37.874195] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4651 (0.4646)  acc1: 89.0625 (86.6132)  acc5: 100.0000 (99.1836)  time: 0.0286  data: 0.0002  max mem: 5511
[09:56:38.158166] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4770 (0.4629)  acc1: 85.9375 (86.6219)  acc5: 100.0000 (99.1865)  time: 0.0284  data: 0.0003  max mem: 5511
[09:56:38.441047] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4698 (0.4624)  acc1: 84.3750 (86.5577)  acc5: 100.0000 (99.2009)  time: 0.0282  data: 0.0002  max mem: 5511
[09:56:38.724762] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4191 (0.4603)  acc1: 87.5000 (86.6578)  acc5: 100.0000 (99.2354)  time: 0.0282  data: 0.0002  max mem: 5511
[09:56:39.005037] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4447 (0.4590)  acc1: 87.5000 (86.6618)  acc5: 100.0000 (99.2446)  time: 0.0281  data: 0.0001  max mem: 5511
[09:56:39.156240] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4447 (0.4588)  acc1: 85.9375 (86.6100)  acc5: 100.0000 (99.2600)  time: 0.0270  data: 0.0001  max mem: 5511
[09:56:39.324313] Test: Total time: 0:00:05 (0.0330 s / it)
[09:56:39.324783] * Acc@1 86.610 Acc@5 99.260 loss 0.459
[09:56:39.325120] Accuracy of the network on the 10000 test images: 86.6%
[09:56:39.325318] Max accuracy: 86.61%
[09:56:39.684002] log_dir: ./output_dir
[09:56:40.496990] Epoch: [82]  [  0/781]  eta: 0:10:33  lr: 0.000022  training_loss: 1.3235 (1.3235)  mae_loss: 0.2421 (0.2421)  classification_loss: 1.0811 (1.0811)  loss_mask: 0.0002 (0.0002)  time: 0.8108  data: 0.5754  max mem: 5511
[09:56:44.436416] Epoch: [82]  [ 20/781]  eta: 0:02:52  lr: 0.000022  training_loss: 1.3948 (1.3938)  mae_loss: 0.2523 (0.2501)  classification_loss: 1.1404 (1.1436)  loss_mask: 0.0001 (0.0001)  time: 0.1969  data: 0.0002  max mem: 5511
[09:56:48.399848] Epoch: [82]  [ 40/781]  eta: 0:02:37  lr: 0.000022  training_loss: 1.4393 (1.4128)  mae_loss: 0.2405 (0.2496)  classification_loss: 1.2018 (1.1631)  loss_mask: 0.0001 (0.0001)  time: 0.1981  data: 0.0003  max mem: 5511
[09:56:52.354679] Epoch: [82]  [ 60/781]  eta: 0:02:29  lr: 0.000022  training_loss: 1.3861 (1.4059)  mae_loss: 0.2477 (0.2478)  classification_loss: 1.1277 (1.1580)  loss_mask: 0.0000 (0.0001)  time: 0.1976  data: 0.0002  max mem: 5511
[09:56:56.284473] Epoch: [82]  [ 80/781]  eta: 0:02:23  lr: 0.000022  training_loss: 1.4097 (1.4099)  mae_loss: 0.2439 (0.2495)  classification_loss: 1.1756 (1.1602)  loss_mask: 0.0000 (0.0001)  time: 0.1964  data: 0.0004  max mem: 5511
[09:57:00.234177] Epoch: [82]  [100/781]  eta: 0:02:18  lr: 0.000022  training_loss: 1.3881 (1.4107)  mae_loss: 0.2461 (0.2489)  classification_loss: 1.1786 (1.1617)  loss_mask: 0.0000 (0.0001)  time: 0.1974  data: 0.0002  max mem: 5511
[09:57:04.194847] Epoch: [82]  [120/781]  eta: 0:02:13  lr: 0.000022  training_loss: 1.4320 (1.4118)  mae_loss: 0.2511 (0.2503)  classification_loss: 1.1522 (1.1614)  loss_mask: 0.0000 (0.0001)  time: 0.1980  data: 0.0002  max mem: 5511
[09:57:08.146416] Epoch: [82]  [140/781]  eta: 0:02:09  lr: 0.000022  training_loss: 1.3742 (1.4101)  mae_loss: 0.2602 (0.2513)  classification_loss: 1.1316 (1.1587)  loss_mask: 0.0000 (0.0001)  time: 0.1975  data: 0.0002  max mem: 5511
[09:57:12.088540] Epoch: [82]  [160/781]  eta: 0:02:04  lr: 0.000022  training_loss: 1.4110 (1.4086)  mae_loss: 0.2411 (0.2504)  classification_loss: 1.1751 (1.1581)  loss_mask: 0.0000 (0.0001)  time: 0.1970  data: 0.0002  max mem: 5511
[09:57:16.028190] Epoch: [82]  [180/781]  eta: 0:02:00  lr: 0.000022  training_loss: 1.4235 (1.4110)  mae_loss: 0.2504 (0.2509)  classification_loss: 1.1731 (1.1600)  loss_mask: 0.0000 (0.0001)  time: 0.1969  data: 0.0002  max mem: 5511
[09:57:19.962347] Epoch: [82]  [200/781]  eta: 0:01:56  lr: 0.000022  training_loss: 1.3934 (1.4087)  mae_loss: 0.2330 (0.2496)  classification_loss: 1.1515 (1.1590)  loss_mask: 0.0000 (0.0001)  time: 0.1966  data: 0.0002  max mem: 5511
[09:57:23.923611] Epoch: [82]  [220/781]  eta: 0:01:52  lr: 0.000022  training_loss: 1.4277 (1.4082)  mae_loss: 0.2442 (0.2494)  classification_loss: 1.1896 (1.1588)  loss_mask: 0.0000 (0.0001)  time: 0.1980  data: 0.0002  max mem: 5511
[09:57:27.885616] Epoch: [82]  [240/781]  eta: 0:01:48  lr: 0.000022  training_loss: 1.3948 (1.4083)  mae_loss: 0.2393 (0.2490)  classification_loss: 1.1470 (1.1593)  loss_mask: 0.0000 (0.0001)  time: 0.1980  data: 0.0003  max mem: 5511
[09:57:31.836420] Epoch: [82]  [260/781]  eta: 0:01:44  lr: 0.000022  training_loss: 1.3919 (1.4058)  mae_loss: 0.2404 (0.2488)  classification_loss: 1.1294 (1.1570)  loss_mask: 0.0000 (0.0001)  time: 0.1975  data: 0.0003  max mem: 5511
[09:57:35.770109] Epoch: [82]  [280/781]  eta: 0:01:39  lr: 0.000022  training_loss: 1.3520 (1.4030)  mae_loss: 0.2393 (0.2486)  classification_loss: 1.1144 (1.1543)  loss_mask: 0.0000 (0.0001)  time: 0.1966  data: 0.0003  max mem: 5511
[09:57:39.711764] Epoch: [82]  [300/781]  eta: 0:01:35  lr: 0.000022  training_loss: 1.3926 (1.4025)  mae_loss: 0.2315 (0.2481)  classification_loss: 1.1512 (1.1544)  loss_mask: 0.0000 (0.0001)  time: 0.1970  data: 0.0002  max mem: 5511
[09:57:43.643405] Epoch: [82]  [320/781]  eta: 0:01:31  lr: 0.000021  training_loss: 1.3995 (1.4025)  mae_loss: 0.2439 (0.2482)  classification_loss: 1.1671 (1.1542)  loss_mask: 0.0000 (0.0001)  time: 0.1965  data: 0.0002  max mem: 5511
[09:57:47.594011] Epoch: [82]  [340/781]  eta: 0:01:27  lr: 0.000021  training_loss: 1.3825 (1.4031)  mae_loss: 0.2550 (0.2485)  classification_loss: 1.1195 (1.1545)  loss_mask: 0.0000 (0.0001)  time: 0.1974  data: 0.0002  max mem: 5511
[09:57:51.552334] Epoch: [82]  [360/781]  eta: 0:01:23  lr: 0.000021  training_loss: 1.4331 (1.4049)  mae_loss: 0.2456 (0.2487)  classification_loss: 1.1736 (1.1562)  loss_mask: 0.0000 (0.0001)  time: 0.1978  data: 0.0002  max mem: 5511
[09:57:55.520285] Epoch: [82]  [380/781]  eta: 0:01:19  lr: 0.000021  training_loss: 1.3943 (1.4065)  mae_loss: 0.2344 (0.2483)  classification_loss: 1.1620 (1.1574)  loss_mask: 0.0001 (0.0008)  time: 0.1983  data: 0.0003  max mem: 5511
[09:57:59.456229] Epoch: [82]  [400/781]  eta: 0:01:15  lr: 0.000021  training_loss: 1.4197 (1.4076)  mae_loss: 0.2563 (0.2490)  classification_loss: 1.1580 (1.1579)  loss_mask: 0.0001 (0.0007)  time: 0.1967  data: 0.0002  max mem: 5511
[09:58:03.413370] Epoch: [82]  [420/781]  eta: 0:01:11  lr: 0.000021  training_loss: 1.4105 (1.4083)  mae_loss: 0.2441 (0.2497)  classification_loss: 1.1457 (1.1579)  loss_mask: 0.0000 (0.0007)  time: 0.1978  data: 0.0002  max mem: 5511
[09:58:07.368075] Epoch: [82]  [440/781]  eta: 0:01:07  lr: 0.000021  training_loss: 1.4040 (1.4074)  mae_loss: 0.2441 (0.2496)  classification_loss: 1.1529 (1.1571)  loss_mask: 0.0000 (0.0007)  time: 0.1976  data: 0.0002  max mem: 5511
[09:58:11.307966] Epoch: [82]  [460/781]  eta: 0:01:03  lr: 0.000021  training_loss: 1.3682 (1.4057)  mae_loss: 0.2231 (0.2488)  classification_loss: 1.1053 (1.1562)  loss_mask: 0.0000 (0.0007)  time: 0.1969  data: 0.0002  max mem: 5511
[09:58:15.271353] Epoch: [82]  [480/781]  eta: 0:00:59  lr: 0.000021  training_loss: 1.4074 (1.4061)  mae_loss: 0.2453 (0.2490)  classification_loss: 1.1577 (1.1565)  loss_mask: 0.0000 (0.0006)  time: 0.1981  data: 0.0002  max mem: 5511
[09:58:19.236304] Epoch: [82]  [500/781]  eta: 0:00:55  lr: 0.000021  training_loss: 1.3542 (1.4050)  mae_loss: 0.2397 (0.2489)  classification_loss: 1.1225 (1.1555)  loss_mask: 0.0000 (0.0006)  time: 0.1982  data: 0.0002  max mem: 5511
[09:58:23.182215] Epoch: [82]  [520/781]  eta: 0:00:51  lr: 0.000021  training_loss: 1.4106 (1.4056)  mae_loss: 0.2601 (0.2493)  classification_loss: 1.1523 (1.1557)  loss_mask: 0.0000 (0.0006)  time: 0.1972  data: 0.0002  max mem: 5511
[09:58:27.125749] Epoch: [82]  [540/781]  eta: 0:00:47  lr: 0.000021  training_loss: 1.3639 (1.4050)  mae_loss: 0.2513 (0.2497)  classification_loss: 1.1060 (1.1547)  loss_mask: 0.0000 (0.0006)  time: 0.1970  data: 0.0002  max mem: 5511
[09:58:31.113593] Epoch: [82]  [560/781]  eta: 0:00:43  lr: 0.000021  training_loss: 1.3857 (1.4043)  mae_loss: 0.2414 (0.2496)  classification_loss: 1.1353 (1.1541)  loss_mask: 0.0001 (0.0006)  time: 0.1993  data: 0.0002  max mem: 5511
[09:58:35.075712] Epoch: [82]  [580/781]  eta: 0:00:39  lr: 0.000021  training_loss: 1.3664 (1.4041)  mae_loss: 0.2512 (0.2497)  classification_loss: 1.1138 (1.1539)  loss_mask: 0.0001 (0.0005)  time: 0.1980  data: 0.0002  max mem: 5511
[09:58:39.020560] Epoch: [82]  [600/781]  eta: 0:00:35  lr: 0.000021  training_loss: 1.3763 (1.4035)  mae_loss: 0.2464 (0.2494)  classification_loss: 1.1511 (1.1535)  loss_mask: 0.0001 (0.0005)  time: 0.1972  data: 0.0003  max mem: 5511
[09:58:42.986202] Epoch: [82]  [620/781]  eta: 0:00:31  lr: 0.000021  training_loss: 1.3502 (1.4027)  mae_loss: 0.2465 (0.2495)  classification_loss: 1.1132 (1.1527)  loss_mask: 0.0000 (0.0005)  time: 0.1982  data: 0.0002  max mem: 5511
[09:58:46.958775] Epoch: [82]  [640/781]  eta: 0:00:27  lr: 0.000021  training_loss: 1.3478 (1.4017)  mae_loss: 0.2445 (0.2494)  classification_loss: 1.1056 (1.1517)  loss_mask: 0.0000 (0.0005)  time: 0.1985  data: 0.0002  max mem: 5511
[09:58:50.917897] Epoch: [82]  [660/781]  eta: 0:00:24  lr: 0.000021  training_loss: 1.3958 (1.4019)  mae_loss: 0.2404 (0.2493)  classification_loss: 1.1701 (1.1521)  loss_mask: 0.0000 (0.0005)  time: 0.1979  data: 0.0002  max mem: 5511
[09:58:54.856250] Epoch: [82]  [680/781]  eta: 0:00:20  lr: 0.000020  training_loss: 1.3886 (1.4021)  mae_loss: 0.2473 (0.2494)  classification_loss: 1.1661 (1.1523)  loss_mask: 0.0000 (0.0005)  time: 0.1968  data: 0.0002  max mem: 5511
[09:58:58.839228] Epoch: [82]  [700/781]  eta: 0:00:16  lr: 0.000020  training_loss: 1.4298 (1.4024)  mae_loss: 0.2377 (0.2490)  classification_loss: 1.1890 (1.1530)  loss_mask: 0.0000 (0.0005)  time: 0.1991  data: 0.0002  max mem: 5511
[09:59:02.800438] Epoch: [82]  [720/781]  eta: 0:00:12  lr: 0.000020  training_loss: 1.4037 (1.4026)  mae_loss: 0.2313 (0.2488)  classification_loss: 1.1503 (1.1533)  loss_mask: 0.0000 (0.0004)  time: 0.1980  data: 0.0003  max mem: 5511
[09:59:06.760989] Epoch: [82]  [740/781]  eta: 0:00:08  lr: 0.000020  training_loss: 1.3913 (1.4024)  mae_loss: 0.2390 (0.2488)  classification_loss: 1.1409 (1.1532)  loss_mask: 0.0000 (0.0004)  time: 0.1979  data: 0.0003  max mem: 5511
[09:59:10.723985] Epoch: [82]  [760/781]  eta: 0:00:04  lr: 0.000020  training_loss: 1.4625 (1.4035)  mae_loss: 0.2426 (0.2488)  classification_loss: 1.2189 (1.1543)  loss_mask: 0.0000 (0.0004)  time: 0.1980  data: 0.0003  max mem: 5511
[09:59:14.653140] Epoch: [82]  [780/781]  eta: 0:00:00  lr: 0.000020  training_loss: 1.4068 (1.4040)  mae_loss: 0.2392 (0.2486)  classification_loss: 1.1848 (1.1549)  loss_mask: 0.0000 (0.0004)  time: 0.1963  data: 0.0002  max mem: 5511
[09:59:14.816475] Epoch: [82] Total time: 0:02:35 (0.1986 s / it)
[09:59:14.817755] Averaged stats: lr: 0.000020  training_loss: 1.4068 (1.4040)  mae_loss: 0.2392 (0.2486)  classification_loss: 1.1848 (1.1549)  loss_mask: 0.0000 (0.0004)
[09:59:15.410002] Test:  [  0/157]  eta: 0:01:32  testing_loss: 0.4351 (0.4351)  acc1: 90.6250 (90.6250)  acc5: 100.0000 (100.0000)  time: 0.5881  data: 0.5482  max mem: 5511
[09:59:15.697743] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.4662 (0.4782)  acc1: 85.9375 (85.6534)  acc5: 100.0000 (99.8580)  time: 0.0794  data: 0.0501  max mem: 5511
[09:59:15.980811] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4467 (0.4573)  acc1: 85.9375 (86.1607)  acc5: 100.0000 (99.5536)  time: 0.0284  data: 0.0002  max mem: 5511
[09:59:16.264261] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.4231 (0.4709)  acc1: 85.9375 (85.7359)  acc5: 100.0000 (99.4456)  time: 0.0282  data: 0.0002  max mem: 5511
[09:59:16.549685] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.4469 (0.4752)  acc1: 85.9375 (85.7851)  acc5: 98.4375 (99.2759)  time: 0.0283  data: 0.0002  max mem: 5511
[09:59:16.833962] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4448 (0.4663)  acc1: 85.9375 (86.1213)  acc5: 98.4375 (99.2647)  time: 0.0283  data: 0.0002  max mem: 5511
[09:59:17.117131] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4348 (0.4612)  acc1: 85.9375 (86.2961)  acc5: 100.0000 (99.2828)  time: 0.0282  data: 0.0002  max mem: 5511
[09:59:17.405609] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4454 (0.4609)  acc1: 85.9375 (86.2676)  acc5: 100.0000 (99.3178)  time: 0.0285  data: 0.0002  max mem: 5511
[09:59:17.689617] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4765 (0.4684)  acc1: 84.3750 (86.0532)  acc5: 100.0000 (99.2863)  time: 0.0285  data: 0.0002  max mem: 5511
[09:59:17.974222] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4565 (0.4631)  acc1: 85.9375 (86.1951)  acc5: 100.0000 (99.2617)  time: 0.0283  data: 0.0002  max mem: 5511
[09:59:18.257924] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4329 (0.4643)  acc1: 85.9375 (86.2005)  acc5: 100.0000 (99.2884)  time: 0.0282  data: 0.0002  max mem: 5511
[09:59:18.541327] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4675 (0.4667)  acc1: 85.9375 (86.1627)  acc5: 100.0000 (99.2962)  time: 0.0282  data: 0.0002  max mem: 5511
[09:59:18.824987] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4630 (0.4649)  acc1: 85.9375 (86.2216)  acc5: 100.0000 (99.2769)  time: 0.0282  data: 0.0002  max mem: 5511
[09:59:19.108064] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4230 (0.4649)  acc1: 84.3750 (86.1522)  acc5: 100.0000 (99.2963)  time: 0.0282  data: 0.0002  max mem: 5511
[09:59:19.391905] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4230 (0.4624)  acc1: 85.9375 (86.2810)  acc5: 100.0000 (99.3240)  time: 0.0282  data: 0.0002  max mem: 5511
[09:59:19.671922] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4340 (0.4604)  acc1: 87.5000 (86.3928)  acc5: 100.0000 (99.3377)  time: 0.0280  data: 0.0001  max mem: 5511
[09:59:19.824188] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4340 (0.4601)  acc1: 87.5000 (86.4000)  acc5: 100.0000 (99.3400)  time: 0.0271  data: 0.0001  max mem: 5511
[09:59:19.972017] Test: Total time: 0:00:05 (0.0328 s / it)
[09:59:19.972602] * Acc@1 86.400 Acc@5 99.340 loss 0.460
[09:59:19.972926] Accuracy of the network on the 10000 test images: 86.4%
[09:59:19.973120] Max accuracy: 86.61%
[09:59:20.228406] log_dir: ./output_dir
[09:59:21.083464] Epoch: [83]  [  0/781]  eta: 0:11:06  lr: 0.000020  training_loss: 1.0858 (1.0858)  mae_loss: 0.2027 (0.2027)  classification_loss: 0.8832 (0.8832)  loss_mask: 0.0000 (0.0000)  time: 0.8530  data: 0.6342  max mem: 5511
[09:59:25.017172] Epoch: [83]  [ 20/781]  eta: 0:02:53  lr: 0.000020  training_loss: 1.4018 (1.3900)  mae_loss: 0.2498 (0.2554)  classification_loss: 1.1163 (1.1345)  loss_mask: 0.0000 (0.0000)  time: 0.1966  data: 0.0002  max mem: 5511
[09:59:28.948032] Epoch: [83]  [ 40/781]  eta: 0:02:37  lr: 0.000020  training_loss: 1.4038 (1.3971)  mae_loss: 0.2436 (0.2486)  classification_loss: 1.1774 (1.1485)  loss_mask: 0.0000 (0.0000)  time: 0.1965  data: 0.0002  max mem: 5511
[09:59:32.906309] Epoch: [83]  [ 60/781]  eta: 0:02:29  lr: 0.000020  training_loss: 1.4059 (1.4025)  mae_loss: 0.2551 (0.2533)  classification_loss: 1.1284 (1.1492)  loss_mask: 0.0000 (0.0000)  time: 0.1978  data: 0.0003  max mem: 5511
[09:59:36.881275] Epoch: [83]  [ 80/781]  eta: 0:02:24  lr: 0.000020  training_loss: 1.3804 (1.3990)  mae_loss: 0.2367 (0.2512)  classification_loss: 1.1405 (1.1478)  loss_mask: 0.0000 (0.0000)  time: 0.1986  data: 0.0004  max mem: 5511
[09:59:40.831637] Epoch: [83]  [100/781]  eta: 0:02:18  lr: 0.000020  training_loss: 1.4010 (1.4036)  mae_loss: 0.2536 (0.2501)  classification_loss: 1.1639 (1.1534)  loss_mask: 0.0000 (0.0000)  time: 0.1974  data: 0.0002  max mem: 5511
[09:59:44.814791] Epoch: [83]  [120/781]  eta: 0:02:14  lr: 0.000020  training_loss: 1.3440 (1.4004)  mae_loss: 0.2372 (0.2484)  classification_loss: 1.1113 (1.1519)  loss_mask: 0.0000 (0.0000)  time: 0.1991  data: 0.0002  max mem: 5511
[09:59:48.751350] Epoch: [83]  [140/781]  eta: 0:02:09  lr: 0.000020  training_loss: 1.3605 (1.3952)  mae_loss: 0.2532 (0.2488)  classification_loss: 1.1248 (1.1464)  loss_mask: 0.0000 (0.0000)  time: 0.1967  data: 0.0002  max mem: 5511
[09:59:52.698686] Epoch: [83]  [160/781]  eta: 0:02:05  lr: 0.000020  training_loss: 1.3876 (1.3925)  mae_loss: 0.2462 (0.2487)  classification_loss: 1.1457 (1.1437)  loss_mask: 0.0000 (0.0000)  time: 0.1973  data: 0.0003  max mem: 5511
[09:59:56.681257] Epoch: [83]  [180/781]  eta: 0:02:00  lr: 0.000020  training_loss: 1.3795 (1.3912)  mae_loss: 0.2489 (0.2487)  classification_loss: 1.1200 (1.1425)  loss_mask: 0.0000 (0.0000)  time: 0.1990  data: 0.0002  max mem: 5511
[10:00:00.636220] Epoch: [83]  [200/781]  eta: 0:01:56  lr: 0.000020  training_loss: 1.3915 (1.3940)  mae_loss: 0.2455 (0.2490)  classification_loss: 1.1375 (1.1449)  loss_mask: 0.0000 (0.0000)  time: 0.1976  data: 0.0003  max mem: 5511
[10:00:04.583464] Epoch: [83]  [220/781]  eta: 0:01:52  lr: 0.000020  training_loss: 1.3909 (1.3946)  mae_loss: 0.2409 (0.2483)  classification_loss: 1.1480 (1.1462)  loss_mask: 0.0000 (0.0000)  time: 0.1973  data: 0.0002  max mem: 5511
[10:00:08.543130] Epoch: [83]  [240/781]  eta: 0:01:48  lr: 0.000019  training_loss: 1.4253 (1.3958)  mae_loss: 0.2508 (0.2489)  classification_loss: 1.1673 (1.1468)  loss_mask: 0.0000 (0.0000)  time: 0.1979  data: 0.0003  max mem: 5511
[10:00:12.509841] Epoch: [83]  [260/781]  eta: 0:01:44  lr: 0.000019  training_loss: 1.3927 (1.3955)  mae_loss: 0.2322 (0.2485)  classification_loss: 1.1482 (1.1470)  loss_mask: 0.0000 (0.0000)  time: 0.1983  data: 0.0002  max mem: 5511
[10:00:16.465235] Epoch: [83]  [280/781]  eta: 0:01:40  lr: 0.000019  training_loss: 1.3959 (1.3942)  mae_loss: 0.2422 (0.2487)  classification_loss: 1.1523 (1.1455)  loss_mask: 0.0000 (0.0000)  time: 0.1977  data: 0.0002  max mem: 5511
[10:00:20.398242] Epoch: [83]  [300/781]  eta: 0:01:36  lr: 0.000019  training_loss: 1.4124 (1.3956)  mae_loss: 0.2329 (0.2487)  classification_loss: 1.1852 (1.1468)  loss_mask: 0.0000 (0.0000)  time: 0.1966  data: 0.0003  max mem: 5511
[10:00:24.340133] Epoch: [83]  [320/781]  eta: 0:01:32  lr: 0.000019  training_loss: 1.3657 (1.3944)  mae_loss: 0.2369 (0.2490)  classification_loss: 1.1043 (1.1453)  loss_mask: 0.0000 (0.0000)  time: 0.1970  data: 0.0002  max mem: 5511
[10:00:28.290798] Epoch: [83]  [340/781]  eta: 0:01:27  lr: 0.000019  training_loss: 1.3782 (1.3942)  mae_loss: 0.2592 (0.2496)  classification_loss: 1.1087 (1.1446)  loss_mask: 0.0001 (0.0001)  time: 0.1974  data: 0.0002  max mem: 5511
[10:00:32.220364] Epoch: [83]  [360/781]  eta: 0:01:23  lr: 0.000019  training_loss: 1.3970 (1.3947)  mae_loss: 0.2580 (0.2497)  classification_loss: 1.1520 (1.1446)  loss_mask: 0.0001 (0.0004)  time: 0.1964  data: 0.0003  max mem: 5511
[10:00:36.157044] Epoch: [83]  [380/781]  eta: 0:01:19  lr: 0.000019  training_loss: 1.3772 (1.3944)  mae_loss: 0.2457 (0.2497)  classification_loss: 1.1397 (1.1443)  loss_mask: 0.0001 (0.0004)  time: 0.1967  data: 0.0002  max mem: 5511
[10:00:40.132021] Epoch: [83]  [400/781]  eta: 0:01:15  lr: 0.000019  training_loss: 1.3751 (1.3939)  mae_loss: 0.2463 (0.2497)  classification_loss: 1.1208 (1.1438)  loss_mask: 0.0001 (0.0004)  time: 0.1987  data: 0.0002  max mem: 5511
[10:00:44.104405] Epoch: [83]  [420/781]  eta: 0:01:11  lr: 0.000019  training_loss: 1.3907 (1.3947)  mae_loss: 0.2414 (0.2493)  classification_loss: 1.1645 (1.1450)  loss_mask: 0.0000 (0.0004)  time: 0.1986  data: 0.0002  max mem: 5511
[10:00:48.051590] Epoch: [83]  [440/781]  eta: 0:01:07  lr: 0.000019  training_loss: 1.3763 (1.3937)  mae_loss: 0.2443 (0.2492)  classification_loss: 1.1411 (1.1442)  loss_mask: 0.0000 (0.0003)  time: 0.1973  data: 0.0002  max mem: 5511
[10:00:51.996616] Epoch: [83]  [460/781]  eta: 0:01:03  lr: 0.000019  training_loss: 1.3890 (1.3936)  mae_loss: 0.2403 (0.2488)  classification_loss: 1.1486 (1.1445)  loss_mask: 0.0000 (0.0003)  time: 0.1972  data: 0.0002  max mem: 5511
[10:00:55.936978] Epoch: [83]  [480/781]  eta: 0:00:59  lr: 0.000019  training_loss: 1.3718 (1.3940)  mae_loss: 0.2406 (0.2487)  classification_loss: 1.1343 (1.1450)  loss_mask: 0.0000 (0.0003)  time: 0.1969  data: 0.0002  max mem: 5511
[10:00:59.893147] Epoch: [83]  [500/781]  eta: 0:00:55  lr: 0.000019  training_loss: 1.3930 (1.3947)  mae_loss: 0.2383 (0.2484)  classification_loss: 1.1334 (1.1460)  loss_mask: 0.0000 (0.0003)  time: 0.1977  data: 0.0002  max mem: 5511
[10:01:03.811572] Epoch: [83]  [520/781]  eta: 0:00:51  lr: 0.000019  training_loss: 1.4027 (1.3950)  mae_loss: 0.2463 (0.2486)  classification_loss: 1.1592 (1.1461)  loss_mask: 0.0000 (0.0003)  time: 0.1958  data: 0.0002  max mem: 5511
[10:01:07.757287] Epoch: [83]  [540/781]  eta: 0:00:47  lr: 0.000019  training_loss: 1.4157 (1.3959)  mae_loss: 0.2420 (0.2486)  classification_loss: 1.1624 (1.1470)  loss_mask: 0.0000 (0.0003)  time: 0.1972  data: 0.0002  max mem: 5511
[10:01:11.691920] Epoch: [83]  [560/781]  eta: 0:00:43  lr: 0.000019  training_loss: 1.3599 (1.3952)  mae_loss: 0.2439 (0.2485)  classification_loss: 1.0970 (1.1464)  loss_mask: 0.0000 (0.0003)  time: 0.1967  data: 0.0002  max mem: 5511
[10:01:15.663757] Epoch: [83]  [580/781]  eta: 0:00:39  lr: 0.000019  training_loss: 1.3801 (1.3950)  mae_loss: 0.2482 (0.2485)  classification_loss: 1.1298 (1.1462)  loss_mask: 0.0000 (0.0003)  time: 0.1985  data: 0.0002  max mem: 5511
[10:01:19.647726] Epoch: [83]  [600/781]  eta: 0:00:35  lr: 0.000019  training_loss: 1.3928 (1.3949)  mae_loss: 0.2418 (0.2486)  classification_loss: 1.1485 (1.1460)  loss_mask: 0.0000 (0.0003)  time: 0.1991  data: 0.0003  max mem: 5511
[10:01:23.592029] Epoch: [83]  [620/781]  eta: 0:00:31  lr: 0.000018  training_loss: 1.4155 (1.3956)  mae_loss: 0.2622 (0.2490)  classification_loss: 1.1563 (1.1463)  loss_mask: 0.0000 (0.0003)  time: 0.1971  data: 0.0002  max mem: 5511
[10:01:27.537625] Epoch: [83]  [640/781]  eta: 0:00:27  lr: 0.000018  training_loss: 1.4037 (1.3953)  mae_loss: 0.2463 (0.2491)  classification_loss: 1.1564 (1.1460)  loss_mask: 0.0001 (0.0003)  time: 0.1972  data: 0.0002  max mem: 5511
[10:01:31.526388] Epoch: [83]  [660/781]  eta: 0:00:24  lr: 0.000018  training_loss: 1.3672 (1.3945)  mae_loss: 0.2299 (0.2488)  classification_loss: 1.1231 (1.1455)  loss_mask: 0.0000 (0.0003)  time: 0.1993  data: 0.0002  max mem: 5511
[10:01:35.496335] Epoch: [83]  [680/781]  eta: 0:00:20  lr: 0.000018  training_loss: 1.3918 (1.3948)  mae_loss: 0.2444 (0.2488)  classification_loss: 1.1650 (1.1457)  loss_mask: 0.0000 (0.0003)  time: 0.1984  data: 0.0002  max mem: 5511
[10:01:39.433014] Epoch: [83]  [700/781]  eta: 0:00:16  lr: 0.000018  training_loss: 1.4543 (1.3962)  mae_loss: 0.2517 (0.2490)  classification_loss: 1.1977 (1.1469)  loss_mask: 0.0000 (0.0002)  time: 0.1967  data: 0.0002  max mem: 5511
[10:01:43.360304] Epoch: [83]  [720/781]  eta: 0:00:12  lr: 0.000018  training_loss: 1.4428 (1.3977)  mae_loss: 0.2454 (0.2490)  classification_loss: 1.1835 (1.1485)  loss_mask: 0.0000 (0.0002)  time: 0.1963  data: 0.0002  max mem: 5511
[10:01:47.315558] Epoch: [83]  [740/781]  eta: 0:00:08  lr: 0.000018  training_loss: 1.3538 (1.3972)  mae_loss: 0.2399 (0.2490)  classification_loss: 1.1055 (1.1480)  loss_mask: 0.0000 (0.0002)  time: 0.1977  data: 0.0002  max mem: 5511
[10:01:51.250572] Epoch: [83]  [760/781]  eta: 0:00:04  lr: 0.000018  training_loss: 1.4122 (1.3978)  mae_loss: 0.2556 (0.2491)  classification_loss: 1.1541 (1.1484)  loss_mask: 0.0000 (0.0002)  time: 0.1967  data: 0.0002  max mem: 5511
[10:01:55.204157] Epoch: [83]  [780/781]  eta: 0:00:00  lr: 0.000018  training_loss: 1.4357 (1.3986)  mae_loss: 0.2481 (0.2490)  classification_loss: 1.1898 (1.1493)  loss_mask: 0.0000 (0.0002)  time: 0.1976  data: 0.0002  max mem: 5511
[10:01:55.375364] Epoch: [83] Total time: 0:02:35 (0.1987 s / it)
[10:01:55.375890] Averaged stats: lr: 0.000018  training_loss: 1.4357 (1.3986)  mae_loss: 0.2481 (0.2490)  classification_loss: 1.1898 (1.1493)  loss_mask: 0.0000 (0.0002)
[10:01:55.950639] Test:  [  0/157]  eta: 0:01:29  testing_loss: 0.4429 (0.4429)  acc1: 90.6250 (90.6250)  acc5: 100.0000 (100.0000)  time: 0.5705  data: 0.5412  max mem: 5511
[10:01:56.271971] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.4725 (0.4749)  acc1: 85.9375 (85.7955)  acc5: 100.0000 (99.7159)  time: 0.0807  data: 0.0508  max mem: 5511
[10:01:56.559090] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4551 (0.4546)  acc1: 85.9375 (86.4583)  acc5: 100.0000 (99.5536)  time: 0.0302  data: 0.0009  max mem: 5511
[10:01:56.843745] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.4259 (0.4684)  acc1: 87.5000 (85.8367)  acc5: 100.0000 (99.4456)  time: 0.0284  data: 0.0002  max mem: 5511
[10:01:57.144585] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4473 (0.4731)  acc1: 85.9375 (85.8994)  acc5: 100.0000 (99.3902)  time: 0.0291  data: 0.0002  max mem: 5511
[10:01:57.430420] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4314 (0.4648)  acc1: 87.5000 (86.2439)  acc5: 100.0000 (99.3260)  time: 0.0292  data: 0.0002  max mem: 5511
[10:01:57.717493] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4281 (0.4613)  acc1: 87.5000 (86.3986)  acc5: 100.0000 (99.3340)  time: 0.0285  data: 0.0002  max mem: 5511
[10:01:58.004388] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4415 (0.4603)  acc1: 87.5000 (86.4217)  acc5: 100.0000 (99.3618)  time: 0.0286  data: 0.0002  max mem: 5511
[10:01:58.298208] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4553 (0.4676)  acc1: 85.9375 (86.1690)  acc5: 100.0000 (99.3634)  time: 0.0289  data: 0.0002  max mem: 5511
[10:01:58.593555] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4333 (0.4615)  acc1: 85.9375 (86.3839)  acc5: 100.0000 (99.3304)  time: 0.0293  data: 0.0002  max mem: 5511
[10:01:58.881692] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4228 (0.4635)  acc1: 85.9375 (86.3088)  acc5: 100.0000 (99.3657)  time: 0.0291  data: 0.0002  max mem: 5511
[10:01:59.168461] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4812 (0.4662)  acc1: 85.9375 (86.3739)  acc5: 100.0000 (99.2821)  time: 0.0286  data: 0.0002  max mem: 5511
[10:01:59.453643] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4605 (0.4650)  acc1: 85.9375 (86.4024)  acc5: 98.4375 (99.2769)  time: 0.0284  data: 0.0002  max mem: 5511
[10:01:59.741464] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4247 (0.4655)  acc1: 84.3750 (86.2953)  acc5: 100.0000 (99.2844)  time: 0.0285  data: 0.0002  max mem: 5511
[10:02:00.034032] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4156 (0.4625)  acc1: 85.9375 (86.4362)  acc5: 100.0000 (99.3019)  time: 0.0288  data: 0.0002  max mem: 5511
[10:02:00.315033] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4364 (0.4608)  acc1: 85.9375 (86.4238)  acc5: 100.0000 (99.3067)  time: 0.0285  data: 0.0002  max mem: 5511
[10:02:00.465587] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4383 (0.4614)  acc1: 85.9375 (86.3800)  acc5: 100.0000 (99.3100)  time: 0.0271  data: 0.0001  max mem: 5511
[10:02:00.632483] Test: Total time: 0:00:05 (0.0335 s / it)
[10:02:00.633312] * Acc@1 86.380 Acc@5 99.310 loss 0.461
[10:02:00.633718] Accuracy of the network on the 10000 test images: 86.4%
[10:02:00.634008] Max accuracy: 86.61%
[10:02:01.011851] log_dir: ./output_dir
[10:02:01.889184] Epoch: [84]  [  0/781]  eta: 0:11:23  lr: 0.000018  training_loss: 1.2825 (1.2825)  mae_loss: 0.2707 (0.2707)  classification_loss: 1.0118 (1.0118)  loss_mask: 0.0000 (0.0000)  time: 0.8755  data: 0.6466  max mem: 5511
[10:02:05.825302] Epoch: [84]  [ 20/781]  eta: 0:02:54  lr: 0.000018  training_loss: 1.3370 (1.3461)  mae_loss: 0.2494 (0.2512)  classification_loss: 1.1160 (1.0949)  loss_mask: 0.0000 (0.0000)  time: 0.1967  data: 0.0002  max mem: 5511
[10:02:09.800362] Epoch: [84]  [ 40/781]  eta: 0:02:38  lr: 0.000018  training_loss: 1.4068 (1.3673)  mae_loss: 0.2340 (0.2468)  classification_loss: 1.1411 (1.1205)  loss_mask: 0.0000 (0.0000)  time: 0.1986  data: 0.0002  max mem: 5511
[10:02:13.751357] Epoch: [84]  [ 60/781]  eta: 0:02:30  lr: 0.000018  training_loss: 1.4080 (1.3743)  mae_loss: 0.2386 (0.2451)  classification_loss: 1.1540 (1.1291)  loss_mask: 0.0000 (0.0000)  time: 0.1974  data: 0.0002  max mem: 5511
[10:02:17.743639] Epoch: [84]  [ 80/781]  eta: 0:02:24  lr: 0.000018  training_loss: 1.3859 (1.3843)  mae_loss: 0.2535 (0.2468)  classification_loss: 1.1398 (1.1375)  loss_mask: 0.0000 (0.0000)  time: 0.1995  data: 0.0003  max mem: 5511
[10:02:21.691796] Epoch: [84]  [100/781]  eta: 0:02:19  lr: 0.000018  training_loss: 1.4357 (1.3909)  mae_loss: 0.2485 (0.2469)  classification_loss: 1.1815 (1.1440)  loss_mask: 0.0000 (0.0000)  time: 0.1973  data: 0.0002  max mem: 5511
[10:02:25.683315] Epoch: [84]  [120/781]  eta: 0:02:14  lr: 0.000018  training_loss: 1.4185 (1.3934)  mae_loss: 0.2316 (0.2456)  classification_loss: 1.1465 (1.1477)  loss_mask: 0.0000 (0.0001)  time: 0.1995  data: 0.0002  max mem: 5511
[10:02:29.645946] Epoch: [84]  [140/781]  eta: 0:02:10  lr: 0.000018  training_loss: 1.3794 (1.3904)  mae_loss: 0.2406 (0.2462)  classification_loss: 1.1325 (1.1442)  loss_mask: 0.0000 (0.0001)  time: 0.1980  data: 0.0002  max mem: 5511
[10:02:33.582430] Epoch: [84]  [160/781]  eta: 0:02:05  lr: 0.000018  training_loss: 1.3468 (1.3909)  mae_loss: 0.2522 (0.2469)  classification_loss: 1.0965 (1.1440)  loss_mask: 0.0000 (0.0001)  time: 0.1967  data: 0.0002  max mem: 5511
[10:02:37.557749] Epoch: [84]  [180/781]  eta: 0:02:01  lr: 0.000018  training_loss: 1.4111 (1.3946)  mae_loss: 0.2476 (0.2477)  classification_loss: 1.1633 (1.1469)  loss_mask: 0.0000 (0.0001)  time: 0.1987  data: 0.0002  max mem: 5511
[10:02:41.510071] Epoch: [84]  [200/781]  eta: 0:01:56  lr: 0.000017  training_loss: 1.3845 (1.3941)  mae_loss: 0.2347 (0.2473)  classification_loss: 1.1370 (1.1468)  loss_mask: 0.0000 (0.0001)  time: 0.1975  data: 0.0002  max mem: 5511
[10:02:45.457622] Epoch: [84]  [220/781]  eta: 0:01:52  lr: 0.000017  training_loss: 1.3613 (1.3933)  mae_loss: 0.2478 (0.2471)  classification_loss: 1.1180 (1.1461)  loss_mask: 0.0001 (0.0001)  time: 0.1973  data: 0.0002  max mem: 5511
[10:02:49.390507] Epoch: [84]  [240/781]  eta: 0:01:48  lr: 0.000017  training_loss: 1.3690 (1.3921)  mae_loss: 0.2399 (0.2469)  classification_loss: 1.1484 (1.1452)  loss_mask: 0.0000 (0.0001)  time: 0.1966  data: 0.0003  max mem: 5511
[10:02:53.324632] Epoch: [84]  [260/781]  eta: 0:01:44  lr: 0.000017  training_loss: 1.3706 (1.3907)  mae_loss: 0.2345 (0.2465)  classification_loss: 1.1377 (1.1441)  loss_mask: 0.0000 (0.0001)  time: 0.1966  data: 0.0004  max mem: 5511
[10:02:57.252356] Epoch: [84]  [280/781]  eta: 0:01:40  lr: 0.000017  training_loss: 1.3398 (1.3892)  mae_loss: 0.2259 (0.2455)  classification_loss: 1.1069 (1.1437)  loss_mask: 0.0000 (0.0001)  time: 0.1963  data: 0.0003  max mem: 5511
[10:03:01.252938] Epoch: [84]  [300/781]  eta: 0:01:36  lr: 0.000017  training_loss: 1.3597 (1.3899)  mae_loss: 0.2488 (0.2463)  classification_loss: 1.1022 (1.1435)  loss_mask: 0.0000 (0.0001)  time: 0.2000  data: 0.0002  max mem: 5511
[10:03:05.225308] Epoch: [84]  [320/781]  eta: 0:01:32  lr: 0.000017  training_loss: 1.3662 (1.3905)  mae_loss: 0.2624 (0.2474)  classification_loss: 1.1198 (1.1431)  loss_mask: 0.0000 (0.0001)  time: 0.1985  data: 0.0003  max mem: 5511
[10:03:09.203690] Epoch: [84]  [340/781]  eta: 0:01:28  lr: 0.000017  training_loss: 1.3735 (1.3895)  mae_loss: 0.2350 (0.2472)  classification_loss: 1.1275 (1.1422)  loss_mask: 0.0000 (0.0001)  time: 0.1988  data: 0.0002  max mem: 5511
[10:03:13.169965] Epoch: [84]  [360/781]  eta: 0:01:24  lr: 0.000017  training_loss: 1.3832 (1.3909)  mae_loss: 0.2377 (0.2466)  classification_loss: 1.1643 (1.1442)  loss_mask: 0.0000 (0.0001)  time: 0.1982  data: 0.0003  max mem: 5511
[10:03:17.110958] Epoch: [84]  [380/781]  eta: 0:01:20  lr: 0.000017  training_loss: 1.4146 (1.3921)  mae_loss: 0.2535 (0.2473)  classification_loss: 1.1552 (1.1447)  loss_mask: 0.0000 (0.0001)  time: 0.1970  data: 0.0003  max mem: 5511
[10:03:21.037270] Epoch: [84]  [400/781]  eta: 0:01:15  lr: 0.000017  training_loss: 1.3450 (1.3908)  mae_loss: 0.2419 (0.2474)  classification_loss: 1.1105 (1.1434)  loss_mask: 0.0000 (0.0001)  time: 0.1962  data: 0.0003  max mem: 5511
[10:03:24.999225] Epoch: [84]  [420/781]  eta: 0:01:11  lr: 0.000017  training_loss: 1.3593 (1.3902)  mae_loss: 0.2517 (0.2476)  classification_loss: 1.0833 (1.1425)  loss_mask: 0.0000 (0.0001)  time: 0.1980  data: 0.0003  max mem: 5511
[10:03:28.926154] Epoch: [84]  [440/781]  eta: 0:01:07  lr: 0.000017  training_loss: 1.3118 (1.3879)  mae_loss: 0.2412 (0.2472)  classification_loss: 1.0850 (1.1406)  loss_mask: 0.0000 (0.0001)  time: 0.1963  data: 0.0002  max mem: 5511
[10:03:32.877028] Epoch: [84]  [460/781]  eta: 0:01:03  lr: 0.000017  training_loss: 1.3716 (1.3880)  mae_loss: 0.2395 (0.2475)  classification_loss: 1.0861 (1.1405)  loss_mask: 0.0000 (0.0001)  time: 0.1974  data: 0.0002  max mem: 5511
[10:03:36.863059] Epoch: [84]  [480/781]  eta: 0:00:59  lr: 0.000017  training_loss: 1.4040 (1.3883)  mae_loss: 0.2320 (0.2470)  classification_loss: 1.1652 (1.1412)  loss_mask: 0.0000 (0.0001)  time: 0.1992  data: 0.0002  max mem: 5511
[10:03:40.794039] Epoch: [84]  [500/781]  eta: 0:00:55  lr: 0.000017  training_loss: 1.3543 (1.3890)  mae_loss: 0.2463 (0.2473)  classification_loss: 1.0976 (1.1417)  loss_mask: 0.0000 (0.0001)  time: 0.1964  data: 0.0002  max mem: 5511
[10:03:44.752579] Epoch: [84]  [520/781]  eta: 0:00:51  lr: 0.000017  training_loss: 1.3967 (1.3888)  mae_loss: 0.2418 (0.2471)  classification_loss: 1.1382 (1.1417)  loss_mask: 0.0000 (0.0001)  time: 0.1978  data: 0.0002  max mem: 5511
[10:03:48.692318] Epoch: [84]  [540/781]  eta: 0:00:47  lr: 0.000017  training_loss: 1.3684 (1.3886)  mae_loss: 0.2425 (0.2469)  classification_loss: 1.1403 (1.1416)  loss_mask: 0.0000 (0.0001)  time: 0.1969  data: 0.0002  max mem: 5511
[10:03:52.655748] Epoch: [84]  [560/781]  eta: 0:00:43  lr: 0.000017  training_loss: 1.3759 (1.3877)  mae_loss: 0.2428 (0.2469)  classification_loss: 1.1314 (1.1408)  loss_mask: 0.0000 (0.0001)  time: 0.1981  data: 0.0002  max mem: 5511
[10:03:56.668700] Epoch: [84]  [580/781]  eta: 0:00:39  lr: 0.000017  training_loss: 1.3474 (1.3868)  mae_loss: 0.2290 (0.2465)  classification_loss: 1.1118 (1.1402)  loss_mask: 0.0000 (0.0001)  time: 0.2006  data: 0.0002  max mem: 5511
[10:04:00.623410] Epoch: [84]  [600/781]  eta: 0:00:36  lr: 0.000016  training_loss: 1.2926 (1.3846)  mae_loss: 0.2466 (0.2466)  classification_loss: 1.0481 (1.1379)  loss_mask: 0.0000 (0.0001)  time: 0.1976  data: 0.0002  max mem: 5511
[10:04:04.541630] Epoch: [84]  [620/781]  eta: 0:00:32  lr: 0.000016  training_loss: 1.3635 (1.3836)  mae_loss: 0.2532 (0.2468)  classification_loss: 1.1049 (1.1368)  loss_mask: 0.0000 (0.0001)  time: 0.1958  data: 0.0002  max mem: 5511
[10:04:08.511726] Epoch: [84]  [640/781]  eta: 0:00:28  lr: 0.000016  training_loss: 1.3951 (1.3846)  mae_loss: 0.2379 (0.2469)  classification_loss: 1.1368 (1.1377)  loss_mask: 0.0000 (0.0001)  time: 0.1984  data: 0.0003  max mem: 5511
[10:04:12.488031] Epoch: [84]  [660/781]  eta: 0:00:24  lr: 0.000016  training_loss: 1.3840 (1.3851)  mae_loss: 0.2331 (0.2468)  classification_loss: 1.1560 (1.1383)  loss_mask: 0.0000 (0.0001)  time: 0.1987  data: 0.0002  max mem: 5511
[10:04:16.445551] Epoch: [84]  [680/781]  eta: 0:00:20  lr: 0.000016  training_loss: 1.3701 (1.3855)  mae_loss: 0.2407 (0.2466)  classification_loss: 1.1087 (1.1388)  loss_mask: 0.0000 (0.0001)  time: 0.1978  data: 0.0003  max mem: 5511
[10:04:20.389561] Epoch: [84]  [700/781]  eta: 0:00:16  lr: 0.000016  training_loss: 1.3614 (1.3851)  mae_loss: 0.2474 (0.2467)  classification_loss: 1.1350 (1.1383)  loss_mask: 0.0000 (0.0001)  time: 0.1971  data: 0.0003  max mem: 5511
[10:04:24.329197] Epoch: [84]  [720/781]  eta: 0:00:12  lr: 0.000016  training_loss: 1.4160 (1.3866)  mae_loss: 0.2459 (0.2467)  classification_loss: 1.1893 (1.1398)  loss_mask: 0.0000 (0.0001)  time: 0.1969  data: 0.0003  max mem: 5511
[10:04:28.315445] Epoch: [84]  [740/781]  eta: 0:00:08  lr: 0.000016  training_loss: 1.3665 (1.3859)  mae_loss: 0.2434 (0.2466)  classification_loss: 1.1347 (1.1393)  loss_mask: 0.0000 (0.0001)  time: 0.1992  data: 0.0003  max mem: 5511
[10:04:32.286872] Epoch: [84]  [760/781]  eta: 0:00:04  lr: 0.000016  training_loss: 1.3933 (1.3861)  mae_loss: 0.2411 (0.2468)  classification_loss: 1.1258 (1.1392)  loss_mask: 0.0000 (0.0001)  time: 0.1985  data: 0.0003  max mem: 5511
[10:04:36.243337] Epoch: [84]  [780/781]  eta: 0:00:00  lr: 0.000016  training_loss: 1.4193 (1.3868)  mae_loss: 0.2450 (0.2468)  classification_loss: 1.1597 (1.1400)  loss_mask: 0.0000 (0.0000)  time: 0.1977  data: 0.0002  max mem: 5511
[10:04:36.407746] Epoch: [84] Total time: 0:02:35 (0.1990 s / it)
[10:04:36.408463] Averaged stats: lr: 0.000016  training_loss: 1.4193 (1.3868)  mae_loss: 0.2450 (0.2468)  classification_loss: 1.1597 (1.1400)  loss_mask: 0.0000 (0.0000)
[10:04:37.142354] Test:  [  0/157]  eta: 0:01:54  testing_loss: 0.4077 (0.4077)  acc1: 89.0625 (89.0625)  acc5: 100.0000 (100.0000)  time: 0.7295  data: 0.6824  max mem: 5511
[10:04:37.429994] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.4623 (0.4669)  acc1: 87.5000 (86.0795)  acc5: 100.0000 (99.7159)  time: 0.0923  data: 0.0622  max mem: 5511
[10:04:37.718812] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4536 (0.4444)  acc1: 87.5000 (86.3839)  acc5: 100.0000 (99.6280)  time: 0.0287  data: 0.0002  max mem: 5511
[10:04:38.005213] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4148 (0.4594)  acc1: 87.5000 (86.0887)  acc5: 100.0000 (99.5464)  time: 0.0286  data: 0.0002  max mem: 5511
[10:04:38.298330] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4300 (0.4671)  acc1: 85.9375 (86.0137)  acc5: 100.0000 (99.3902)  time: 0.0288  data: 0.0002  max mem: 5511
[10:04:38.587908] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4522 (0.4611)  acc1: 85.9375 (86.2745)  acc5: 98.4375 (99.3260)  time: 0.0289  data: 0.0002  max mem: 5511
[10:04:38.887666] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4367 (0.4573)  acc1: 87.5000 (86.5266)  acc5: 100.0000 (99.3340)  time: 0.0293  data: 0.0002  max mem: 5511
[10:04:39.176110] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4376 (0.4557)  acc1: 87.5000 (86.6637)  acc5: 100.0000 (99.3838)  time: 0.0293  data: 0.0002  max mem: 5511
[10:04:39.464119] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4671 (0.4635)  acc1: 85.9375 (86.5162)  acc5: 100.0000 (99.3248)  time: 0.0287  data: 0.0003  max mem: 5511
[10:04:39.750315] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4469 (0.4575)  acc1: 85.9375 (86.6930)  acc5: 98.4375 (99.2960)  time: 0.0286  data: 0.0004  max mem: 5511
[10:04:40.038583] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.4175 (0.4579)  acc1: 87.5000 (86.6646)  acc5: 100.0000 (99.3348)  time: 0.0286  data: 0.0003  max mem: 5511
[10:04:40.326901] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4816 (0.4612)  acc1: 85.9375 (86.5428)  acc5: 100.0000 (99.3243)  time: 0.0287  data: 0.0003  max mem: 5511
[10:04:40.613376] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4862 (0.4602)  acc1: 85.9375 (86.6736)  acc5: 100.0000 (99.3414)  time: 0.0286  data: 0.0002  max mem: 5511
[10:04:40.909967] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4358 (0.4600)  acc1: 85.9375 (86.5935)  acc5: 100.0000 (99.3440)  time: 0.0290  data: 0.0002  max mem: 5511
[10:04:41.197007] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4358 (0.4576)  acc1: 87.5000 (86.7243)  acc5: 100.0000 (99.3684)  time: 0.0290  data: 0.0002  max mem: 5511
[10:04:41.482481] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4336 (0.4561)  acc1: 87.5000 (86.7239)  acc5: 100.0000 (99.3688)  time: 0.0285  data: 0.0002  max mem: 5511
[10:04:41.638238] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4296 (0.4563)  acc1: 87.5000 (86.7300)  acc5: 100.0000 (99.3700)  time: 0.0276  data: 0.0002  max mem: 5511
[10:04:41.802834] Test: Total time: 0:00:05 (0.0343 s / it)
[10:04:41.803298] * Acc@1 86.730 Acc@5 99.370 loss 0.456
[10:04:41.803615] Accuracy of the network on the 10000 test images: 86.7%
[10:04:41.803837] Max accuracy: 86.73%
[10:04:42.214332] log_dir: ./output_dir
[10:04:43.184779] Epoch: [85]  [  0/781]  eta: 0:12:36  lr: 0.000016  training_loss: 1.2617 (1.2617)  mae_loss: 0.2307 (0.2307)  classification_loss: 1.0310 (1.0310)  loss_mask: 0.0000 (0.0000)  time: 0.9685  data: 0.7092  max mem: 5511
[10:04:47.159331] Epoch: [85]  [ 20/781]  eta: 0:02:59  lr: 0.000016  training_loss: 1.3788 (1.3914)  mae_loss: 0.2481 (0.2491)  classification_loss: 1.1486 (1.1422)  loss_mask: 0.0000 (0.0000)  time: 0.1986  data: 0.0002  max mem: 5511
[10:04:51.086012] Epoch: [85]  [ 40/781]  eta: 0:02:40  lr: 0.000016  training_loss: 1.4201 (1.4062)  mae_loss: 0.2556 (0.2510)  classification_loss: 1.1617 (1.1552)  loss_mask: 0.0000 (0.0000)  time: 0.1963  data: 0.0002  max mem: 5511
[10:04:55.069583] Epoch: [85]  [ 60/781]  eta: 0:02:31  lr: 0.000016  training_loss: 1.3666 (1.4032)  mae_loss: 0.2477 (0.2533)  classification_loss: 1.1417 (1.1499)  loss_mask: 0.0000 (0.0000)  time: 0.1991  data: 0.0002  max mem: 5511
[10:04:59.021402] Epoch: [85]  [ 80/781]  eta: 0:02:25  lr: 0.000016  training_loss: 1.3283 (1.3919)  mae_loss: 0.2417 (0.2510)  classification_loss: 1.0873 (1.1408)  loss_mask: 0.0000 (0.0001)  time: 0.1975  data: 0.0002  max mem: 5511
[10:05:02.976256] Epoch: [85]  [100/781]  eta: 0:02:19  lr: 0.000016  training_loss: 1.4005 (1.3962)  mae_loss: 0.2547 (0.2502)  classification_loss: 1.1642 (1.1459)  loss_mask: 0.0000 (0.0001)  time: 0.1976  data: 0.0002  max mem: 5511
[10:05:06.938940] Epoch: [85]  [120/781]  eta: 0:02:14  lr: 0.000016  training_loss: 1.3384 (1.3908)  mae_loss: 0.2285 (0.2476)  classification_loss: 1.1161 (1.1432)  loss_mask: 0.0000 (0.0001)  time: 0.1981  data: 0.0002  max mem: 5511
[10:05:10.900113] Epoch: [85]  [140/781]  eta: 0:02:10  lr: 0.000016  training_loss: 1.3313 (1.3833)  mae_loss: 0.2372 (0.2459)  classification_loss: 1.0823 (1.1373)  loss_mask: 0.0000 (0.0000)  time: 0.1980  data: 0.0002  max mem: 5511
[10:05:14.858188] Epoch: [85]  [160/781]  eta: 0:02:05  lr: 0.000016  training_loss: 1.3477 (1.3807)  mae_loss: 0.2506 (0.2464)  classification_loss: 1.0954 (1.1342)  loss_mask: 0.0000 (0.0000)  time: 0.1978  data: 0.0002  max mem: 5511
[10:05:18.855796] Epoch: [85]  [180/781]  eta: 0:02:01  lr: 0.000016  training_loss: 1.3985 (1.3822)  mae_loss: 0.2594 (0.2480)  classification_loss: 1.1462 (1.1342)  loss_mask: 0.0000 (0.0000)  time: 0.1998  data: 0.0002  max mem: 5511
[10:05:22.785255] Epoch: [85]  [200/781]  eta: 0:01:57  lr: 0.000016  training_loss: 1.3397 (1.3810)  mae_loss: 0.2403 (0.2477)  classification_loss: 1.0963 (1.1332)  loss_mask: 0.0000 (0.0000)  time: 0.1964  data: 0.0002  max mem: 5511
[10:05:26.720965] Epoch: [85]  [220/781]  eta: 0:01:52  lr: 0.000015  training_loss: 1.3657 (1.3792)  mae_loss: 0.2401 (0.2473)  classification_loss: 1.1088 (1.1318)  loss_mask: 0.0000 (0.0000)  time: 0.1967  data: 0.0004  max mem: 5511
[10:05:30.697735] Epoch: [85]  [240/781]  eta: 0:01:48  lr: 0.000015  training_loss: 1.4300 (1.3827)  mae_loss: 0.2415 (0.2471)  classification_loss: 1.1694 (1.1356)  loss_mask: 0.0000 (0.0000)  time: 0.1987  data: 0.0002  max mem: 5511
[10:05:34.662824] Epoch: [85]  [260/781]  eta: 0:01:44  lr: 0.000015  training_loss: 1.4069 (1.3856)  mae_loss: 0.2478 (0.2469)  classification_loss: 1.1835 (1.1386)  loss_mask: 0.0000 (0.0000)  time: 0.1982  data: 0.0002  max mem: 5511
[10:05:38.633949] Epoch: [85]  [280/781]  eta: 0:01:40  lr: 0.000015  training_loss: 1.4491 (1.3877)  mae_loss: 0.2512 (0.2473)  classification_loss: 1.1770 (1.1403)  loss_mask: 0.0001 (0.0001)  time: 0.1985  data: 0.0003  max mem: 5511
[10:05:42.602208] Epoch: [85]  [300/781]  eta: 0:01:36  lr: 0.000015  training_loss: 1.3568 (1.3867)  mae_loss: 0.2399 (0.2473)  classification_loss: 1.1246 (1.1393)  loss_mask: 0.0000 (0.0000)  time: 0.1983  data: 0.0003  max mem: 5511
[10:05:46.544957] Epoch: [85]  [320/781]  eta: 0:01:32  lr: 0.000015  training_loss: 1.3491 (1.3843)  mae_loss: 0.2474 (0.2476)  classification_loss: 1.0713 (1.1366)  loss_mask: 0.0000 (0.0000)  time: 0.1970  data: 0.0002  max mem: 5511
[10:05:50.525948] Epoch: [85]  [340/781]  eta: 0:01:28  lr: 0.000015  training_loss: 1.3554 (1.3846)  mae_loss: 0.2572 (0.2485)  classification_loss: 1.1023 (1.1360)  loss_mask: 0.0000 (0.0000)  time: 0.1989  data: 0.0002  max mem: 5511
[10:05:54.494071] Epoch: [85]  [360/781]  eta: 0:01:24  lr: 0.000015  training_loss: 1.4169 (1.3858)  mae_loss: 0.2555 (0.2492)  classification_loss: 1.1494 (1.1366)  loss_mask: 0.0000 (0.0000)  time: 0.1983  data: 0.0002  max mem: 5511
[10:05:58.448995] Epoch: [85]  [380/781]  eta: 0:01:20  lr: 0.000015  training_loss: 1.3593 (1.3864)  mae_loss: 0.2593 (0.2499)  classification_loss: 1.0984 (1.1364)  loss_mask: 0.0000 (0.0000)  time: 0.1977  data: 0.0002  max mem: 5511
[10:06:02.397117] Epoch: [85]  [400/781]  eta: 0:01:16  lr: 0.000015  training_loss: 1.4139 (1.3887)  mae_loss: 0.2599 (0.2502)  classification_loss: 1.1804 (1.1384)  loss_mask: 0.0000 (0.0000)  time: 0.1973  data: 0.0003  max mem: 5511
[10:06:06.348082] Epoch: [85]  [420/781]  eta: 0:01:12  lr: 0.000015  training_loss: 1.3978 (1.3883)  mae_loss: 0.2480 (0.2504)  classification_loss: 1.1291 (1.1379)  loss_mask: 0.0000 (0.0000)  time: 0.1975  data: 0.0002  max mem: 5511
[10:06:10.289930] Epoch: [85]  [440/781]  eta: 0:01:08  lr: 0.000015  training_loss: 1.3825 (1.3882)  mae_loss: 0.2297 (0.2497)  classification_loss: 1.1346 (1.1384)  loss_mask: 0.0000 (0.0000)  time: 0.1970  data: 0.0003  max mem: 5511
[10:06:14.262633] Epoch: [85]  [460/781]  eta: 0:01:04  lr: 0.000015  training_loss: 1.3582 (1.3879)  mae_loss: 0.2422 (0.2496)  classification_loss: 1.1313 (1.1382)  loss_mask: 0.0000 (0.0000)  time: 0.1985  data: 0.0002  max mem: 5511
[10:06:18.241730] Epoch: [85]  [480/781]  eta: 0:01:00  lr: 0.000015  training_loss: 1.3948 (1.3887)  mae_loss: 0.2535 (0.2498)  classification_loss: 1.1325 (1.1388)  loss_mask: 0.0000 (0.0000)  time: 0.1988  data: 0.0002  max mem: 5511
[10:06:22.201064] Epoch: [85]  [500/781]  eta: 0:00:56  lr: 0.000015  training_loss: 1.4314 (1.3900)  mae_loss: 0.2519 (0.2498)  classification_loss: 1.1741 (1.1401)  loss_mask: 0.0000 (0.0000)  time: 0.1979  data: 0.0002  max mem: 5511
[10:06:26.155170] Epoch: [85]  [520/781]  eta: 0:00:52  lr: 0.000015  training_loss: 1.3781 (1.3896)  mae_loss: 0.2547 (0.2502)  classification_loss: 1.1182 (1.1393)  loss_mask: 0.0000 (0.0000)  time: 0.1976  data: 0.0002  max mem: 5511
[10:06:30.099703] Epoch: [85]  [540/781]  eta: 0:00:48  lr: 0.000015  training_loss: 1.4485 (1.3912)  mae_loss: 0.2519 (0.2505)  classification_loss: 1.1843 (1.1406)  loss_mask: 0.0000 (0.0000)  time: 0.1971  data: 0.0002  max mem: 5511
[10:06:34.050750] Epoch: [85]  [560/781]  eta: 0:00:44  lr: 0.000015  training_loss: 1.3626 (1.3903)  mae_loss: 0.2441 (0.2504)  classification_loss: 1.1431 (1.1398)  loss_mask: 0.0000 (0.0000)  time: 0.1975  data: 0.0002  max mem: 5511
[10:06:37.995887] Epoch: [85]  [580/781]  eta: 0:00:40  lr: 0.000015  training_loss: 1.3878 (1.3906)  mae_loss: 0.2372 (0.2503)  classification_loss: 1.1380 (1.1403)  loss_mask: 0.0000 (0.0000)  time: 0.1972  data: 0.0002  max mem: 5511
[10:06:41.931450] Epoch: [85]  [600/781]  eta: 0:00:36  lr: 0.000015  training_loss: 1.3948 (1.3913)  mae_loss: 0.2494 (0.2504)  classification_loss: 1.1468 (1.1409)  loss_mask: 0.0000 (0.0000)  time: 0.1967  data: 0.0002  max mem: 5511
[10:06:45.874724] Epoch: [85]  [620/781]  eta: 0:00:32  lr: 0.000014  training_loss: 1.3442 (1.3913)  mae_loss: 0.2521 (0.2504)  classification_loss: 1.1112 (1.1408)  loss_mask: 0.0000 (0.0000)  time: 0.1971  data: 0.0001  max mem: 5511
[10:06:49.819222] Epoch: [85]  [640/781]  eta: 0:00:28  lr: 0.000014  training_loss: 1.3538 (1.3909)  mae_loss: 0.2419 (0.2502)  classification_loss: 1.1043 (1.1406)  loss_mask: 0.0000 (0.0000)  time: 0.1972  data: 0.0002  max mem: 5511
[10:06:53.796897] Epoch: [85]  [660/781]  eta: 0:00:24  lr: 0.000014  training_loss: 1.4024 (1.3913)  mae_loss: 0.2408 (0.2501)  classification_loss: 1.1615 (1.1412)  loss_mask: 0.0000 (0.0000)  time: 0.1988  data: 0.0002  max mem: 5511
[10:06:57.759465] Epoch: [85]  [680/781]  eta: 0:00:20  lr: 0.000014  training_loss: 1.3808 (1.3914)  mae_loss: 0.2539 (0.2502)  classification_loss: 1.1349 (1.1412)  loss_mask: 0.0000 (0.0000)  time: 0.1980  data: 0.0002  max mem: 5511
[10:07:01.731727] Epoch: [85]  [700/781]  eta: 0:00:16  lr: 0.000014  training_loss: 1.4077 (1.3916)  mae_loss: 0.2414 (0.2503)  classification_loss: 1.1345 (1.1413)  loss_mask: 0.0000 (0.0000)  time: 0.1985  data: 0.0003  max mem: 5511
[10:07:05.685201] Epoch: [85]  [720/781]  eta: 0:00:12  lr: 0.000014  training_loss: 1.3960 (1.3917)  mae_loss: 0.2469 (0.2503)  classification_loss: 1.1195 (1.1413)  loss_mask: 0.0000 (0.0000)  time: 0.1976  data: 0.0002  max mem: 5511
[10:07:09.620664] Epoch: [85]  [740/781]  eta: 0:00:08  lr: 0.000014  training_loss: 1.3484 (1.3910)  mae_loss: 0.2501 (0.2503)  classification_loss: 1.0996 (1.1406)  loss_mask: 0.0000 (0.0000)  time: 0.1967  data: 0.0003  max mem: 5511
[10:07:13.573211] Epoch: [85]  [760/781]  eta: 0:00:04  lr: 0.000014  training_loss: 1.4063 (1.3915)  mae_loss: 0.2492 (0.2504)  classification_loss: 1.1122 (1.1410)  loss_mask: 0.0000 (0.0000)  time: 0.1975  data: 0.0002  max mem: 5511
[10:07:17.523246] Epoch: [85]  [780/781]  eta: 0:00:00  lr: 0.000014  training_loss: 1.4111 (1.3914)  mae_loss: 0.2457 (0.2503)  classification_loss: 1.1493 (1.1411)  loss_mask: 0.0000 (0.0000)  time: 0.1974  data: 0.0002  max mem: 5511
[10:07:17.668636] Epoch: [85] Total time: 0:02:35 (0.1990 s / it)
[10:07:17.669126] Averaged stats: lr: 0.000014  training_loss: 1.4111 (1.3914)  mae_loss: 0.2457 (0.2503)  classification_loss: 1.1493 (1.1411)  loss_mask: 0.0000 (0.0000)
[10:07:18.256644] Test:  [  0/157]  eta: 0:01:31  testing_loss: 0.4397 (0.4397)  acc1: 89.0625 (89.0625)  acc5: 100.0000 (100.0000)  time: 0.5827  data: 0.5516  max mem: 5511
[10:07:18.549832] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.4499 (0.4664)  acc1: 85.9375 (85.6534)  acc5: 100.0000 (99.7159)  time: 0.0795  data: 0.0504  max mem: 5511
[10:07:18.841190] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4431 (0.4482)  acc1: 85.9375 (86.2351)  acc5: 100.0000 (99.5536)  time: 0.0291  data: 0.0002  max mem: 5511
[10:07:19.130871] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.4374 (0.4644)  acc1: 87.5000 (85.6855)  acc5: 100.0000 (99.4456)  time: 0.0289  data: 0.0002  max mem: 5511
[10:07:19.421883] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.4524 (0.4707)  acc1: 87.5000 (85.8994)  acc5: 98.4375 (99.2378)  time: 0.0289  data: 0.0003  max mem: 5511
[10:07:19.712087] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4524 (0.4636)  acc1: 87.5000 (86.3051)  acc5: 98.4375 (99.2647)  time: 0.0289  data: 0.0003  max mem: 5511
[10:07:19.997637] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4308 (0.4581)  acc1: 87.5000 (86.5779)  acc5: 100.0000 (99.2572)  time: 0.0286  data: 0.0003  max mem: 5511
[10:07:20.282451] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4342 (0.4574)  acc1: 85.9375 (86.6417)  acc5: 100.0000 (99.2738)  time: 0.0284  data: 0.0003  max mem: 5511
[10:07:20.566719] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4519 (0.4649)  acc1: 85.9375 (86.4583)  acc5: 98.4375 (99.2284)  time: 0.0283  data: 0.0002  max mem: 5511
[10:07:20.850681] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4430 (0.4577)  acc1: 85.9375 (86.7273)  acc5: 98.4375 (99.2102)  time: 0.0283  data: 0.0002  max mem: 5511
[10:07:21.135559] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4037 (0.4585)  acc1: 87.5000 (86.7265)  acc5: 100.0000 (99.2265)  time: 0.0283  data: 0.0002  max mem: 5511
[10:07:21.420232] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4698 (0.4610)  acc1: 85.9375 (86.6695)  acc5: 100.0000 (99.2258)  time: 0.0283  data: 0.0002  max mem: 5511
[10:07:21.704726] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4576 (0.4599)  acc1: 87.5000 (86.6736)  acc5: 100.0000 (99.1994)  time: 0.0283  data: 0.0002  max mem: 5511
[10:07:21.990285] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4235 (0.4598)  acc1: 87.5000 (86.5935)  acc5: 100.0000 (99.2128)  time: 0.0283  data: 0.0002  max mem: 5511
[10:07:22.273158] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4230 (0.4568)  acc1: 87.5000 (86.6689)  acc5: 100.0000 (99.2465)  time: 0.0283  data: 0.0002  max mem: 5511
[10:07:22.554291] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4242 (0.4555)  acc1: 87.5000 (86.7032)  acc5: 100.0000 (99.2550)  time: 0.0281  data: 0.0001  max mem: 5511
[10:07:22.706058] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4367 (0.4557)  acc1: 87.5000 (86.7100)  acc5: 100.0000 (99.2500)  time: 0.0271  data: 0.0001  max mem: 5511
[10:07:22.869419] Test: Total time: 0:00:05 (0.0331 s / it)
[10:07:22.870227] * Acc@1 86.710 Acc@5 99.250 loss 0.456
[10:07:22.870773] Accuracy of the network on the 10000 test images: 86.7%
[10:07:22.871566] Max accuracy: 86.73%
[10:07:23.190593] log_dir: ./output_dir
[10:07:23.972928] Epoch: [86]  [  0/781]  eta: 0:10:09  lr: 0.000014  training_loss: 1.3806 (1.3806)  mae_loss: 0.2747 (0.2747)  classification_loss: 1.1059 (1.1059)  loss_mask: 0.0000 (0.0000)  time: 0.7804  data: 0.5551  max mem: 5511
[10:07:27.920462] Epoch: [86]  [ 20/781]  eta: 0:02:51  lr: 0.000014  training_loss: 1.3440 (1.3745)  mae_loss: 0.2472 (0.2477)  classification_loss: 1.1337 (1.1268)  loss_mask: 0.0000 (0.0000)  time: 0.1973  data: 0.0002  max mem: 5511
[10:07:31.931621] Epoch: [86]  [ 40/781]  eta: 0:02:37  lr: 0.000014  training_loss: 1.3828 (1.3815)  mae_loss: 0.2462 (0.2464)  classification_loss: 1.1216 (1.1350)  loss_mask: 0.0000 (0.0000)  time: 0.2005  data: 0.0003  max mem: 5511
[10:07:35.878144] Epoch: [86]  [ 60/781]  eta: 0:02:29  lr: 0.000014  training_loss: 1.3994 (1.3928)  mae_loss: 0.2450 (0.2498)  classification_loss: 1.1408 (1.1430)  loss_mask: 0.0000 (0.0000)  time: 0.1972  data: 0.0002  max mem: 5511
[10:07:39.827581] Epoch: [86]  [ 80/781]  eta: 0:02:23  lr: 0.000014  training_loss: 1.3712 (1.3904)  mae_loss: 0.2511 (0.2491)  classification_loss: 1.1324 (1.1412)  loss_mask: 0.0000 (0.0000)  time: 0.1974  data: 0.0002  max mem: 5511
[10:07:43.747056] Epoch: [86]  [100/781]  eta: 0:02:18  lr: 0.000014  training_loss: 1.4272 (1.3962)  mae_loss: 0.2367 (0.2481)  classification_loss: 1.2024 (1.1481)  loss_mask: 0.0000 (0.0000)  time: 0.1959  data: 0.0003  max mem: 5511
[10:07:47.687106] Epoch: [86]  [120/781]  eta: 0:02:13  lr: 0.000014  training_loss: 1.3883 (1.3937)  mae_loss: 0.2273 (0.2469)  classification_loss: 1.1528 (1.1468)  loss_mask: 0.0000 (0.0000)  time: 0.1969  data: 0.0002  max mem: 5511
[10:07:51.683966] Epoch: [86]  [140/781]  eta: 0:02:09  lr: 0.000014  training_loss: 1.3372 (1.3895)  mae_loss: 0.2437 (0.2471)  classification_loss: 1.1084 (1.1423)  loss_mask: 0.0000 (0.0000)  time: 0.1997  data: 0.0002  max mem: 5511
[10:07:55.623947] Epoch: [86]  [160/781]  eta: 0:02:05  lr: 0.000014  training_loss: 1.3416 (1.3822)  mae_loss: 0.2474 (0.2476)  classification_loss: 1.0848 (1.1346)  loss_mask: 0.0000 (0.0000)  time: 0.1969  data: 0.0002  max mem: 5511
[10:07:59.607382] Epoch: [86]  [180/781]  eta: 0:02:00  lr: 0.000014  training_loss: 1.3836 (1.3807)  mae_loss: 0.2423 (0.2472)  classification_loss: 1.1386 (1.1335)  loss_mask: 0.0000 (0.0000)  time: 0.1990  data: 0.0002  max mem: 5511
[10:08:03.552741] Epoch: [86]  [200/781]  eta: 0:01:56  lr: 0.000014  training_loss: 1.3715 (1.3818)  mae_loss: 0.2428 (0.2472)  classification_loss: 1.1381 (1.1346)  loss_mask: 0.0000 (0.0000)  time: 0.1972  data: 0.0002  max mem: 5511
[10:08:07.552405] Epoch: [86]  [220/781]  eta: 0:01:52  lr: 0.000014  training_loss: 1.3710 (1.3807)  mae_loss: 0.2316 (0.2462)  classification_loss: 1.1387 (1.1345)  loss_mask: 0.0000 (0.0000)  time: 0.1999  data: 0.0003  max mem: 5511
[10:08:11.541091] Epoch: [86]  [240/781]  eta: 0:01:48  lr: 0.000014  training_loss: 1.4359 (1.3847)  mae_loss: 0.2449 (0.2465)  classification_loss: 1.1767 (1.1381)  loss_mask: 0.0000 (0.0000)  time: 0.1994  data: 0.0004  max mem: 5511
[10:08:15.485561] Epoch: [86]  [260/781]  eta: 0:01:44  lr: 0.000014  training_loss: 1.4016 (1.3861)  mae_loss: 0.2485 (0.2470)  classification_loss: 1.1434 (1.1391)  loss_mask: 0.0000 (0.0000)  time: 0.1971  data: 0.0002  max mem: 5511
[10:08:19.439102] Epoch: [86]  [280/781]  eta: 0:01:40  lr: 0.000013  training_loss: 1.3439 (1.3839)  mae_loss: 0.2345 (0.2465)  classification_loss: 1.1065 (1.1373)  loss_mask: 0.0000 (0.0000)  time: 0.1976  data: 0.0002  max mem: 5511
[10:08:23.406072] Epoch: [86]  [300/781]  eta: 0:01:36  lr: 0.000013  training_loss: 1.3517 (1.3820)  mae_loss: 0.2418 (0.2467)  classification_loss: 1.1140 (1.1353)  loss_mask: 0.0000 (0.0000)  time: 0.1982  data: 0.0002  max mem: 5511
[10:08:27.340324] Epoch: [86]  [320/781]  eta: 0:01:32  lr: 0.000013  training_loss: 1.3466 (1.3811)  mae_loss: 0.2527 (0.2470)  classification_loss: 1.0966 (1.1341)  loss_mask: 0.0000 (0.0000)  time: 0.1966  data: 0.0002  max mem: 5511
[10:08:31.285159] Epoch: [86]  [340/781]  eta: 0:01:28  lr: 0.000013  training_loss: 1.3519 (1.3812)  mae_loss: 0.2528 (0.2472)  classification_loss: 1.1033 (1.1340)  loss_mask: 0.0000 (0.0000)  time: 0.1972  data: 0.0002  max mem: 5511
[10:08:35.280059] Epoch: [86]  [360/781]  eta: 0:01:24  lr: 0.000013  training_loss: 1.3364 (1.3801)  mae_loss: 0.2397 (0.2474)  classification_loss: 1.0955 (1.1326)  loss_mask: 0.0000 (0.0000)  time: 0.1997  data: 0.0002  max mem: 5511
[10:08:39.221719] Epoch: [86]  [380/781]  eta: 0:01:19  lr: 0.000013  training_loss: 1.3567 (1.3805)  mae_loss: 0.2424 (0.2475)  classification_loss: 1.1158 (1.1330)  loss_mask: 0.0000 (0.0000)  time: 0.1970  data: 0.0002  max mem: 5511
[10:08:43.168744] Epoch: [86]  [400/781]  eta: 0:01:15  lr: 0.000013  training_loss: 1.4305 (1.3826)  mae_loss: 0.2479 (0.2476)  classification_loss: 1.1890 (1.1350)  loss_mask: 0.0000 (0.0000)  time: 0.1973  data: 0.0003  max mem: 5511
[10:08:47.150345] Epoch: [86]  [420/781]  eta: 0:01:11  lr: 0.000013  training_loss: 1.3871 (1.3832)  mae_loss: 0.2552 (0.2479)  classification_loss: 1.1204 (1.1353)  loss_mask: 0.0000 (0.0000)  time: 0.1990  data: 0.0003  max mem: 5511
[10:08:51.105609] Epoch: [86]  [440/781]  eta: 0:01:07  lr: 0.000013  training_loss: 1.3783 (1.3836)  mae_loss: 0.2564 (0.2482)  classification_loss: 1.1120 (1.1354)  loss_mask: 0.0000 (0.0000)  time: 0.1977  data: 0.0002  max mem: 5511
[10:08:55.063458] Epoch: [86]  [460/781]  eta: 0:01:03  lr: 0.000013  training_loss: 1.3439 (1.3828)  mae_loss: 0.2547 (0.2483)  classification_loss: 1.0952 (1.1345)  loss_mask: 0.0000 (0.0000)  time: 0.1978  data: 0.0002  max mem: 5511
[10:08:59.026533] Epoch: [86]  [480/781]  eta: 0:00:59  lr: 0.000013  training_loss: 1.3620 (1.3824)  mae_loss: 0.2487 (0.2483)  classification_loss: 1.1068 (1.1341)  loss_mask: 0.0000 (0.0000)  time: 0.1981  data: 0.0002  max mem: 5511
[10:09:03.003664] Epoch: [86]  [500/781]  eta: 0:00:55  lr: 0.000013  training_loss: 1.3269 (1.3820)  mae_loss: 0.2339 (0.2481)  classification_loss: 1.1077 (1.1338)  loss_mask: 0.0000 (0.0000)  time: 0.1988  data: 0.0002  max mem: 5511
[10:09:06.957525] Epoch: [86]  [520/781]  eta: 0:00:51  lr: 0.000013  training_loss: 1.3941 (1.3826)  mae_loss: 0.2475 (0.2484)  classification_loss: 1.1465 (1.1342)  loss_mask: 0.0000 (0.0000)  time: 0.1976  data: 0.0002  max mem: 5511
[10:09:10.941076] Epoch: [86]  [540/781]  eta: 0:00:47  lr: 0.000013  training_loss: 1.3763 (1.3828)  mae_loss: 0.2490 (0.2486)  classification_loss: 1.1105 (1.1342)  loss_mask: 0.0000 (0.0000)  time: 0.1991  data: 0.0003  max mem: 5511
[10:09:14.887065] Epoch: [86]  [560/781]  eta: 0:00:43  lr: 0.000013  training_loss: 1.3825 (1.3820)  mae_loss: 0.2502 (0.2487)  classification_loss: 1.1112 (1.1333)  loss_mask: 0.0000 (0.0000)  time: 0.1972  data: 0.0003  max mem: 5511
[10:09:18.879876] Epoch: [86]  [580/781]  eta: 0:00:40  lr: 0.000013  training_loss: 1.3742 (1.3813)  mae_loss: 0.2401 (0.2485)  classification_loss: 1.1227 (1.1328)  loss_mask: 0.0000 (0.0000)  time: 0.1995  data: 0.0003  max mem: 5511
[10:09:22.836033] Epoch: [86]  [600/781]  eta: 0:00:36  lr: 0.000013  training_loss: 1.3826 (1.3819)  mae_loss: 0.2506 (0.2484)  classification_loss: 1.1543 (1.1335)  loss_mask: 0.0000 (0.0000)  time: 0.1976  data: 0.0002  max mem: 5511
[10:09:26.783770] Epoch: [86]  [620/781]  eta: 0:00:32  lr: 0.000013  training_loss: 1.3686 (1.3818)  mae_loss: 0.2574 (0.2486)  classification_loss: 1.1096 (1.1331)  loss_mask: 0.0000 (0.0000)  time: 0.1973  data: 0.0002  max mem: 5511
[10:09:30.749467] Epoch: [86]  [640/781]  eta: 0:00:28  lr: 0.000013  training_loss: 1.3729 (1.3813)  mae_loss: 0.2396 (0.2485)  classification_loss: 1.1309 (1.1327)  loss_mask: 0.0000 (0.0000)  time: 0.1982  data: 0.0004  max mem: 5511
[10:09:34.730765] Epoch: [86]  [660/781]  eta: 0:00:24  lr: 0.000013  training_loss: 1.3264 (1.3811)  mae_loss: 0.2236 (0.2484)  classification_loss: 1.0961 (1.1327)  loss_mask: 0.0000 (0.0000)  time: 0.1990  data: 0.0002  max mem: 5511
[10:09:38.717049] Epoch: [86]  [680/781]  eta: 0:00:20  lr: 0.000013  training_loss: 1.3801 (1.3811)  mae_loss: 0.2449 (0.2482)  classification_loss: 1.1214 (1.1329)  loss_mask: 0.0000 (0.0000)  time: 0.1992  data: 0.0003  max mem: 5511
[10:09:42.696169] Epoch: [86]  [700/781]  eta: 0:00:16  lr: 0.000013  training_loss: 1.4696 (1.3829)  mae_loss: 0.2601 (0.2485)  classification_loss: 1.1967 (1.1344)  loss_mask: 0.0000 (0.0000)  time: 0.1988  data: 0.0002  max mem: 5511
[10:09:46.717295] Epoch: [86]  [720/781]  eta: 0:00:12  lr: 0.000012  training_loss: 1.3851 (1.3839)  mae_loss: 0.2369 (0.2484)  classification_loss: 1.1531 (1.1355)  loss_mask: 0.0000 (0.0000)  time: 0.2010  data: 0.0002  max mem: 5511
[10:09:50.670000] Epoch: [86]  [740/781]  eta: 0:00:08  lr: 0.000012  training_loss: 1.3699 (1.3839)  mae_loss: 0.2514 (0.2484)  classification_loss: 1.1196 (1.1355)  loss_mask: 0.0000 (0.0000)  time: 0.1976  data: 0.0002  max mem: 5511
[10:09:54.657194] Epoch: [86]  [760/781]  eta: 0:00:04  lr: 0.000012  training_loss: 1.4198 (1.3839)  mae_loss: 0.2446 (0.2482)  classification_loss: 1.1737 (1.1356)  loss_mask: 0.0000 (0.0000)  time: 0.1993  data: 0.0002  max mem: 5511
[10:09:58.605124] Epoch: [86]  [780/781]  eta: 0:00:00  lr: 0.000012  training_loss: 1.3106 (1.3831)  mae_loss: 0.2414 (0.2481)  classification_loss: 1.0426 (1.1349)  loss_mask: 0.0000 (0.0000)  time: 0.1973  data: 0.0002  max mem: 5511
[10:09:58.778209] Epoch: [86] Total time: 0:02:35 (0.1992 s / it)
[10:09:58.778700] Averaged stats: lr: 0.000012  training_loss: 1.3106 (1.3831)  mae_loss: 0.2414 (0.2481)  classification_loss: 1.0426 (1.1349)  loss_mask: 0.0000 (0.0000)
[10:09:59.505689] Test:  [  0/157]  eta: 0:01:53  testing_loss: 0.4050 (0.4050)  acc1: 89.0625 (89.0625)  acc5: 100.0000 (100.0000)  time: 0.7224  data: 0.6897  max mem: 5511
[10:09:59.803715] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.4626 (0.4608)  acc1: 87.5000 (86.3636)  acc5: 100.0000 (99.7159)  time: 0.0926  data: 0.0629  max mem: 5511
[10:10:00.089542] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4365 (0.4409)  acc1: 87.5000 (87.1280)  acc5: 100.0000 (99.5536)  time: 0.0290  data: 0.0002  max mem: 5511
[10:10:00.374070] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4206 (0.4587)  acc1: 87.5000 (86.4415)  acc5: 100.0000 (99.4456)  time: 0.0283  data: 0.0002  max mem: 5511
[10:10:00.658563] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4431 (0.4648)  acc1: 85.9375 (86.2424)  acc5: 100.0000 (99.3521)  time: 0.0283  data: 0.0002  max mem: 5511
[10:10:00.949649] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4292 (0.4556)  acc1: 85.9375 (86.4583)  acc5: 100.0000 (99.2953)  time: 0.0286  data: 0.0002  max mem: 5511
[10:10:01.246125] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4218 (0.4512)  acc1: 87.5000 (86.6803)  acc5: 100.0000 (99.2828)  time: 0.0292  data: 0.0004  max mem: 5511
[10:10:01.536280] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4284 (0.4508)  acc1: 87.5000 (86.7077)  acc5: 100.0000 (99.3178)  time: 0.0292  data: 0.0003  max mem: 5511
[10:10:01.833406] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4519 (0.4590)  acc1: 85.9375 (86.5162)  acc5: 100.0000 (99.2670)  time: 0.0292  data: 0.0003  max mem: 5511
[10:10:02.120243] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4356 (0.4527)  acc1: 85.9375 (86.7960)  acc5: 100.0000 (99.2445)  time: 0.0291  data: 0.0003  max mem: 5511
[10:10:02.405158] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.4201 (0.4539)  acc1: 87.5000 (86.8502)  acc5: 100.0000 (99.2884)  time: 0.0284  data: 0.0003  max mem: 5511
[10:10:02.690795] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4610 (0.4564)  acc1: 87.5000 (86.8102)  acc5: 100.0000 (99.2117)  time: 0.0283  data: 0.0003  max mem: 5511
[10:10:02.978059] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4610 (0.4560)  acc1: 85.9375 (86.8156)  acc5: 98.4375 (99.1994)  time: 0.0284  data: 0.0002  max mem: 5511
[10:10:03.262803] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4293 (0.4562)  acc1: 85.9375 (86.6770)  acc5: 100.0000 (99.2128)  time: 0.0284  data: 0.0002  max mem: 5511
[10:10:03.548159] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4245 (0.4537)  acc1: 87.5000 (86.7686)  acc5: 100.0000 (99.2354)  time: 0.0283  data: 0.0002  max mem: 5511
[10:10:03.831358] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4245 (0.4519)  acc1: 89.0625 (86.7964)  acc5: 100.0000 (99.2343)  time: 0.0282  data: 0.0001  max mem: 5511
[10:10:03.984442] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4347 (0.4525)  acc1: 87.5000 (86.7600)  acc5: 100.0000 (99.2300)  time: 0.0272  data: 0.0001  max mem: 5511
[10:10:04.181936] Test: Total time: 0:00:05 (0.0344 s / it)
[10:10:04.182650] * Acc@1 86.760 Acc@5 99.230 loss 0.453
[10:10:04.182952] Accuracy of the network on the 10000 test images: 86.8%
[10:10:04.183130] Max accuracy: 86.76%
[10:10:04.635724] log_dir: ./output_dir
[10:10:05.520651] Epoch: [87]  [  0/781]  eta: 0:11:29  lr: 0.000012  training_loss: 1.3046 (1.3046)  mae_loss: 0.2368 (0.2368)  classification_loss: 1.0678 (1.0678)  loss_mask: 0.0000 (0.0000)  time: 0.8831  data: 0.6586  max mem: 5511
[10:10:09.495015] Epoch: [87]  [ 20/781]  eta: 0:02:55  lr: 0.000012  training_loss: 1.3611 (1.3546)  mae_loss: 0.2398 (0.2521)  classification_loss: 1.1122 (1.1025)  loss_mask: 0.0000 (0.0000)  time: 0.1986  data: 0.0002  max mem: 5511
[10:10:13.455749] Epoch: [87]  [ 40/781]  eta: 0:02:39  lr: 0.000012  training_loss: 1.3535 (1.3685)  mae_loss: 0.2478 (0.2490)  classification_loss: 1.1115 (1.1195)  loss_mask: 0.0000 (0.0000)  time: 0.1980  data: 0.0002  max mem: 5511
[10:10:17.396024] Epoch: [87]  [ 60/781]  eta: 0:02:30  lr: 0.000012  training_loss: 1.4009 (1.3724)  mae_loss: 0.2511 (0.2498)  classification_loss: 1.1231 (1.1226)  loss_mask: 0.0000 (0.0000)  time: 0.1969  data: 0.0002  max mem: 5511
[10:10:21.345382] Epoch: [87]  [ 80/781]  eta: 0:02:24  lr: 0.000012  training_loss: 1.3251 (1.3708)  mae_loss: 0.2527 (0.2503)  classification_loss: 1.0744 (1.1205)  loss_mask: 0.0000 (0.0000)  time: 0.1974  data: 0.0002  max mem: 5511
[10:10:25.295983] Epoch: [87]  [100/781]  eta: 0:02:19  lr: 0.000012  training_loss: 1.3854 (1.3780)  mae_loss: 0.2399 (0.2480)  classification_loss: 1.1495 (1.1300)  loss_mask: 0.0000 (0.0000)  time: 0.1975  data: 0.0002  max mem: 5511
[10:10:29.256572] Epoch: [87]  [120/781]  eta: 0:02:14  lr: 0.000012  training_loss: 1.3497 (1.3780)  mae_loss: 0.2471 (0.2490)  classification_loss: 1.1091 (1.1290)  loss_mask: 0.0000 (0.0000)  time: 0.1979  data: 0.0003  max mem: 5511
[10:10:33.207468] Epoch: [87]  [140/781]  eta: 0:02:09  lr: 0.000012  training_loss: 1.3598 (1.3774)  mae_loss: 0.2430 (0.2487)  classification_loss: 1.1131 (1.1287)  loss_mask: 0.0000 (0.0000)  time: 0.1974  data: 0.0003  max mem: 5511
[10:10:37.182250] Epoch: [87]  [160/781]  eta: 0:02:05  lr: 0.000012  training_loss: 1.2948 (1.3716)  mae_loss: 0.2539 (0.2492)  classification_loss: 1.0522 (1.1223)  loss_mask: 0.0000 (0.0000)  time: 0.1986  data: 0.0002  max mem: 5511
[10:10:41.145974] Epoch: [87]  [180/781]  eta: 0:02:01  lr: 0.000012  training_loss: 1.3742 (1.3733)  mae_loss: 0.2495 (0.2491)  classification_loss: 1.1275 (1.1242)  loss_mask: 0.0000 (0.0000)  time: 0.1981  data: 0.0002  max mem: 5511
[10:10:45.117876] Epoch: [87]  [200/781]  eta: 0:01:56  lr: 0.000012  training_loss: 1.3926 (1.3726)  mae_loss: 0.2316 (0.2480)  classification_loss: 1.1467 (1.1246)  loss_mask: 0.0000 (0.0000)  time: 0.1985  data: 0.0002  max mem: 5511
[10:10:49.086156] Epoch: [87]  [220/781]  eta: 0:01:52  lr: 0.000012  training_loss: 1.4056 (1.3751)  mae_loss: 0.2395 (0.2474)  classification_loss: 1.1745 (1.1277)  loss_mask: 0.0000 (0.0000)  time: 0.1983  data: 0.0002  max mem: 5511
[10:10:53.032239] Epoch: [87]  [240/781]  eta: 0:01:48  lr: 0.000012  training_loss: 1.4082 (1.3783)  mae_loss: 0.2508 (0.2480)  classification_loss: 1.1435 (1.1302)  loss_mask: 0.0000 (0.0000)  time: 0.1972  data: 0.0002  max mem: 5511
[10:10:57.006480] Epoch: [87]  [260/781]  eta: 0:01:44  lr: 0.000012  training_loss: 1.3595 (1.3791)  mae_loss: 0.2631 (0.2485)  classification_loss: 1.1307 (1.1305)  loss_mask: 0.0000 (0.0000)  time: 0.1986  data: 0.0002  max mem: 5511
[10:11:00.943623] Epoch: [87]  [280/781]  eta: 0:01:40  lr: 0.000012  training_loss: 1.3666 (1.3794)  mae_loss: 0.2376 (0.2481)  classification_loss: 1.1428 (1.1313)  loss_mask: 0.0000 (0.0000)  time: 0.1968  data: 0.0002  max mem: 5511
[10:11:04.952680] Epoch: [87]  [300/781]  eta: 0:01:36  lr: 0.000012  training_loss: 1.4129 (1.3795)  mae_loss: 0.2458 (0.2483)  classification_loss: 1.1441 (1.1312)  loss_mask: 0.0000 (0.0000)  time: 0.2004  data: 0.0002  max mem: 5511
[10:11:08.916242] Epoch: [87]  [320/781]  eta: 0:01:32  lr: 0.000012  training_loss: 1.3947 (1.3807)  mae_loss: 0.2480 (0.2482)  classification_loss: 1.1636 (1.1324)  loss_mask: 0.0000 (0.0000)  time: 0.1981  data: 0.0002  max mem: 5511
[10:11:12.877962] Epoch: [87]  [340/781]  eta: 0:01:28  lr: 0.000012  training_loss: 1.3553 (1.3810)  mae_loss: 0.2515 (0.2483)  classification_loss: 1.1272 (1.1327)  loss_mask: 0.0000 (0.0000)  time: 0.1980  data: 0.0002  max mem: 5511
[10:11:16.810955] Epoch: [87]  [360/781]  eta: 0:01:24  lr: 0.000012  training_loss: 1.3546 (1.3810)  mae_loss: 0.2397 (0.2484)  classification_loss: 1.1047 (1.1326)  loss_mask: 0.0000 (0.0000)  time: 0.1966  data: 0.0003  max mem: 5511
[10:11:20.769601] Epoch: [87]  [380/781]  eta: 0:01:20  lr: 0.000012  training_loss: 1.3568 (1.3820)  mae_loss: 0.2427 (0.2484)  classification_loss: 1.1172 (1.1336)  loss_mask: 0.0000 (0.0000)  time: 0.1978  data: 0.0002  max mem: 5511
[10:11:24.743983] Epoch: [87]  [400/781]  eta: 0:01:16  lr: 0.000011  training_loss: 1.3738 (1.3815)  mae_loss: 0.2366 (0.2483)  classification_loss: 1.1384 (1.1332)  loss_mask: 0.0000 (0.0000)  time: 0.1986  data: 0.0002  max mem: 5511
[10:11:28.688483] Epoch: [87]  [420/781]  eta: 0:01:12  lr: 0.000011  training_loss: 1.3673 (1.3818)  mae_loss: 0.2438 (0.2485)  classification_loss: 1.1235 (1.1332)  loss_mask: 0.0000 (0.0000)  time: 0.1971  data: 0.0002  max mem: 5511
[10:11:32.631094] Epoch: [87]  [440/781]  eta: 0:01:08  lr: 0.000011  training_loss: 1.3731 (1.3814)  mae_loss: 0.2346 (0.2482)  classification_loss: 1.1297 (1.1331)  loss_mask: 0.0000 (0.0000)  time: 0.1970  data: 0.0002  max mem: 5511
[10:11:36.582172] Epoch: [87]  [460/781]  eta: 0:01:03  lr: 0.000011  training_loss: 1.3581 (1.3810)  mae_loss: 0.2356 (0.2485)  classification_loss: 1.1148 (1.1325)  loss_mask: 0.0000 (0.0000)  time: 0.1975  data: 0.0002  max mem: 5511
[10:11:40.528514] Epoch: [87]  [480/781]  eta: 0:00:59  lr: 0.000011  training_loss: 1.3901 (1.3810)  mae_loss: 0.2430 (0.2484)  classification_loss: 1.1389 (1.1325)  loss_mask: 0.0000 (0.0000)  time: 0.1972  data: 0.0003  max mem: 5511
[10:11:44.506177] Epoch: [87]  [500/781]  eta: 0:00:55  lr: 0.000011  training_loss: 1.4088 (1.3824)  mae_loss: 0.2592 (0.2488)  classification_loss: 1.1808 (1.1336)  loss_mask: 0.0000 (0.0000)  time: 0.1988  data: 0.0002  max mem: 5511
[10:11:48.462404] Epoch: [87]  [520/781]  eta: 0:00:51  lr: 0.000011  training_loss: 1.3455 (1.3817)  mae_loss: 0.2477 (0.2488)  classification_loss: 1.1061 (1.1329)  loss_mask: 0.0000 (0.0000)  time: 0.1977  data: 0.0004  max mem: 5511
[10:11:52.461494] Epoch: [87]  [540/781]  eta: 0:00:48  lr: 0.000011  training_loss: 1.3933 (1.3827)  mae_loss: 0.2555 (0.2490)  classification_loss: 1.1318 (1.1336)  loss_mask: 0.0000 (0.0000)  time: 0.1999  data: 0.0003  max mem: 5511
[10:11:56.419829] Epoch: [87]  [560/781]  eta: 0:00:44  lr: 0.000011  training_loss: 1.3423 (1.3822)  mae_loss: 0.2443 (0.2489)  classification_loss: 1.1029 (1.1333)  loss_mask: 0.0000 (0.0000)  time: 0.1978  data: 0.0002  max mem: 5511
[10:12:00.365088] Epoch: [87]  [580/781]  eta: 0:00:40  lr: 0.000011  training_loss: 1.3942 (1.3824)  mae_loss: 0.2405 (0.2488)  classification_loss: 1.1488 (1.1336)  loss_mask: 0.0000 (0.0000)  time: 0.1972  data: 0.0002  max mem: 5511
[10:12:04.368374] Epoch: [87]  [600/781]  eta: 0:00:36  lr: 0.000011  training_loss: 1.3491 (1.3826)  mae_loss: 0.2426 (0.2487)  classification_loss: 1.1086 (1.1339)  loss_mask: 0.0000 (0.0000)  time: 0.2000  data: 0.0002  max mem: 5511
[10:12:08.341228] Epoch: [87]  [620/781]  eta: 0:00:32  lr: 0.000011  training_loss: 1.3907 (1.3830)  mae_loss: 0.2528 (0.2488)  classification_loss: 1.1379 (1.1342)  loss_mask: 0.0000 (0.0000)  time: 0.1985  data: 0.0002  max mem: 5511
[10:12:12.296908] Epoch: [87]  [640/781]  eta: 0:00:28  lr: 0.000011  training_loss: 1.3554 (1.3826)  mae_loss: 0.2327 (0.2485)  classification_loss: 1.0933 (1.1341)  loss_mask: 0.0000 (0.0000)  time: 0.1977  data: 0.0004  max mem: 5511
[10:12:16.298116] Epoch: [87]  [660/781]  eta: 0:00:24  lr: 0.000011  training_loss: 1.3668 (1.3817)  mae_loss: 0.2443 (0.2486)  classification_loss: 1.0905 (1.1331)  loss_mask: 0.0000 (0.0000)  time: 0.2000  data: 0.0003  max mem: 5511
[10:12:20.254626] Epoch: [87]  [680/781]  eta: 0:00:20  lr: 0.000011  training_loss: 1.3704 (1.3817)  mae_loss: 0.2344 (0.2485)  classification_loss: 1.1286 (1.1331)  loss_mask: 0.0000 (0.0000)  time: 0.1977  data: 0.0002  max mem: 5511
[10:12:24.188940] Epoch: [87]  [700/781]  eta: 0:00:16  lr: 0.000011  training_loss: 1.3860 (1.3817)  mae_loss: 0.2296 (0.2479)  classification_loss: 1.1568 (1.1338)  loss_mask: 0.0000 (0.0000)  time: 0.1966  data: 0.0002  max mem: 5511
[10:12:28.125615] Epoch: [87]  [720/781]  eta: 0:00:12  lr: 0.000011  training_loss: 1.4037 (1.3825)  mae_loss: 0.2385 (0.2479)  classification_loss: 1.1528 (1.1346)  loss_mask: 0.0000 (0.0000)  time: 0.1967  data: 0.0003  max mem: 5511
[10:12:32.077366] Epoch: [87]  [740/781]  eta: 0:00:08  lr: 0.000011  training_loss: 1.3881 (1.3830)  mae_loss: 0.2522 (0.2480)  classification_loss: 1.1320 (1.1350)  loss_mask: 0.0000 (0.0000)  time: 0.1975  data: 0.0002  max mem: 5511
[10:12:36.032973] Epoch: [87]  [760/781]  eta: 0:00:04  lr: 0.000011  training_loss: 1.3914 (1.3833)  mae_loss: 0.2261 (0.2476)  classification_loss: 1.1633 (1.1356)  loss_mask: 0.0000 (0.0000)  time: 0.1977  data: 0.0002  max mem: 5511
[10:12:39.959733] Epoch: [87]  [780/781]  eta: 0:00:00  lr: 0.000011  training_loss: 1.3720 (1.3835)  mae_loss: 0.2347 (0.2474)  classification_loss: 1.1348 (1.1361)  loss_mask: 0.0000 (0.0000)  time: 0.1962  data: 0.0002  max mem: 5511
[10:12:40.134755] Epoch: [87] Total time: 0:02:35 (0.1991 s / it)
[10:12:40.135225] Averaged stats: lr: 0.000011  training_loss: 1.3720 (1.3835)  mae_loss: 0.2347 (0.2474)  classification_loss: 1.1348 (1.1361)  loss_mask: 0.0000 (0.0000)
[10:12:40.760555] Test:  [  0/157]  eta: 0:01:37  testing_loss: 0.4250 (0.4250)  acc1: 89.0625 (89.0625)  acc5: 100.0000 (100.0000)  time: 0.6208  data: 0.5894  max mem: 5511
[10:12:41.049491] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4470 (0.4540)  acc1: 87.5000 (86.6477)  acc5: 100.0000 (99.7159)  time: 0.0825  data: 0.0539  max mem: 5511
[10:12:41.335192] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4326 (0.4387)  acc1: 87.5000 (86.6071)  acc5: 100.0000 (99.6280)  time: 0.0286  data: 0.0002  max mem: 5511
[10:12:41.631158] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4200 (0.4544)  acc1: 87.5000 (85.9879)  acc5: 100.0000 (99.4456)  time: 0.0289  data: 0.0002  max mem: 5511
[10:12:41.920207] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4273 (0.4572)  acc1: 85.9375 (86.1280)  acc5: 100.0000 (99.3902)  time: 0.0291  data: 0.0002  max mem: 5511
[10:12:42.208482] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4113 (0.4481)  acc1: 85.9375 (86.4890)  acc5: 100.0000 (99.4179)  time: 0.0287  data: 0.0002  max mem: 5511
[10:12:42.492518] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4230 (0.4454)  acc1: 85.9375 (86.6547)  acc5: 100.0000 (99.4109)  time: 0.0285  data: 0.0002  max mem: 5511
[10:12:42.777411] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4380 (0.4451)  acc1: 87.5000 (86.7518)  acc5: 100.0000 (99.4058)  time: 0.0283  data: 0.0002  max mem: 5511
[10:12:43.061674] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4602 (0.4530)  acc1: 85.9375 (86.5934)  acc5: 100.0000 (99.3634)  time: 0.0283  data: 0.0002  max mem: 5511
[10:12:43.344771] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4342 (0.4469)  acc1: 87.5000 (86.9162)  acc5: 100.0000 (99.3304)  time: 0.0283  data: 0.0002  max mem: 5511
[10:12:43.638754] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4172 (0.4481)  acc1: 89.0625 (86.9585)  acc5: 100.0000 (99.3657)  time: 0.0287  data: 0.0002  max mem: 5511
[10:12:43.924247] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4459 (0.4499)  acc1: 87.5000 (86.9369)  acc5: 100.0000 (99.3384)  time: 0.0289  data: 0.0002  max mem: 5511
[10:12:44.207465] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4331 (0.4490)  acc1: 85.9375 (86.9706)  acc5: 100.0000 (99.3285)  time: 0.0283  data: 0.0002  max mem: 5511
[10:12:44.493971] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4301 (0.4495)  acc1: 85.9375 (86.8321)  acc5: 100.0000 (99.3440)  time: 0.0284  data: 0.0002  max mem: 5511
[10:12:44.779304] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4145 (0.4468)  acc1: 85.9375 (86.9348)  acc5: 100.0000 (99.3794)  time: 0.0285  data: 0.0002  max mem: 5511
[10:12:45.065188] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4182 (0.4451)  acc1: 87.5000 (86.9412)  acc5: 100.0000 (99.3895)  time: 0.0284  data: 0.0001  max mem: 5511
[10:12:45.219556] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4192 (0.4452)  acc1: 89.0625 (86.9600)  acc5: 100.0000 (99.3900)  time: 0.0275  data: 0.0002  max mem: 5511
[10:12:45.456188] Test: Total time: 0:00:05 (0.0339 s / it)
[10:12:45.456762] * Acc@1 86.960 Acc@5 99.390 loss 0.445
[10:12:45.457572] Accuracy of the network on the 10000 test images: 87.0%
[10:12:45.458006] Max accuracy: 86.96%
[10:12:45.649671] log_dir: ./output_dir
[10:12:46.466173] Epoch: [88]  [  0/781]  eta: 0:10:36  lr: 0.000011  training_loss: 1.2635 (1.2635)  mae_loss: 0.2346 (0.2346)  classification_loss: 1.0289 (1.0289)  loss_mask: 0.0000 (0.0000)  time: 0.8147  data: 0.6000  max mem: 5511
[10:12:50.434217] Epoch: [88]  [ 20/781]  eta: 0:02:53  lr: 0.000011  training_loss: 1.3742 (1.3504)  mae_loss: 0.2507 (0.2531)  classification_loss: 1.1036 (1.0974)  loss_mask: 0.0000 (0.0000)  time: 0.1983  data: 0.0003  max mem: 5511
[10:12:54.391847] Epoch: [88]  [ 40/781]  eta: 0:02:37  lr: 0.000011  training_loss: 1.3928 (1.3661)  mae_loss: 0.2472 (0.2531)  classification_loss: 1.1099 (1.1130)  loss_mask: 0.0000 (0.0000)  time: 0.1978  data: 0.0002  max mem: 5511
[10:12:58.384966] Epoch: [88]  [ 60/781]  eta: 0:02:30  lr: 0.000011  training_loss: 1.3468 (1.3697)  mae_loss: 0.2503 (0.2522)  classification_loss: 1.1009 (1.1174)  loss_mask: 0.0000 (0.0000)  time: 0.1996  data: 0.0002  max mem: 5511
[10:13:02.368328] Epoch: [88]  [ 80/781]  eta: 0:02:24  lr: 0.000011  training_loss: 1.3927 (1.3713)  mae_loss: 0.2412 (0.2497)  classification_loss: 1.1405 (1.1215)  loss_mask: 0.0000 (0.0000)  time: 0.1991  data: 0.0004  max mem: 5511
[10:13:06.330528] Epoch: [88]  [100/781]  eta: 0:02:19  lr: 0.000010  training_loss: 1.4304 (1.3808)  mae_loss: 0.2442 (0.2511)  classification_loss: 1.1688 (1.1296)  loss_mask: 0.0000 (0.0000)  time: 0.1980  data: 0.0003  max mem: 5511
[10:13:10.322682] Epoch: [88]  [120/781]  eta: 0:02:14  lr: 0.000010  training_loss: 1.3726 (1.3760)  mae_loss: 0.2455 (0.2513)  classification_loss: 1.1021 (1.1248)  loss_mask: 0.0000 (0.0000)  time: 0.1995  data: 0.0003  max mem: 5511
[10:13:14.317135] Epoch: [88]  [140/781]  eta: 0:02:10  lr: 0.000010  training_loss: 1.3489 (1.3731)  mae_loss: 0.2417 (0.2521)  classification_loss: 1.0727 (1.1209)  loss_mask: 0.0000 (0.0000)  time: 0.1996  data: 0.0002  max mem: 5511
[10:13:18.259961] Epoch: [88]  [160/781]  eta: 0:02:05  lr: 0.000010  training_loss: 1.3366 (1.3699)  mae_loss: 0.2400 (0.2512)  classification_loss: 1.0894 (1.1187)  loss_mask: 0.0000 (0.0000)  time: 0.1970  data: 0.0002  max mem: 5511
[10:13:22.226556] Epoch: [88]  [180/781]  eta: 0:02:01  lr: 0.000010  training_loss: 1.3562 (1.3704)  mae_loss: 0.2394 (0.2512)  classification_loss: 1.1027 (1.1192)  loss_mask: 0.0000 (0.0000)  time: 0.1982  data: 0.0002  max mem: 5511
[10:13:26.180998] Epoch: [88]  [200/781]  eta: 0:01:57  lr: 0.000010  training_loss: 1.3829 (1.3691)  mae_loss: 0.2407 (0.2510)  classification_loss: 1.0952 (1.1181)  loss_mask: 0.0000 (0.0000)  time: 0.1976  data: 0.0002  max mem: 5511
[10:13:30.156087] Epoch: [88]  [220/781]  eta: 0:01:52  lr: 0.000010  training_loss: 1.3713 (1.3690)  mae_loss: 0.2416 (0.2512)  classification_loss: 1.0995 (1.1178)  loss_mask: 0.0000 (0.0000)  time: 0.1987  data: 0.0002  max mem: 5511
[10:13:34.127589] Epoch: [88]  [240/781]  eta: 0:01:48  lr: 0.000010  training_loss: 1.3481 (1.3687)  mae_loss: 0.2451 (0.2505)  classification_loss: 1.0997 (1.1182)  loss_mask: 0.0000 (0.0000)  time: 0.1985  data: 0.0002  max mem: 5511
[10:13:38.092316] Epoch: [88]  [260/781]  eta: 0:01:44  lr: 0.000010  training_loss: 1.4094 (1.3719)  mae_loss: 0.2435 (0.2499)  classification_loss: 1.1497 (1.1220)  loss_mask: 0.0000 (0.0000)  time: 0.1982  data: 0.0002  max mem: 5511
[10:13:42.055282] Epoch: [88]  [280/781]  eta: 0:01:40  lr: 0.000010  training_loss: 1.3955 (1.3725)  mae_loss: 0.2531 (0.2504)  classification_loss: 1.1151 (1.1220)  loss_mask: 0.0000 (0.0000)  time: 0.1981  data: 0.0002  max mem: 5511
[10:13:46.013085] Epoch: [88]  [300/781]  eta: 0:01:36  lr: 0.000010  training_loss: 1.3786 (1.3734)  mae_loss: 0.2428 (0.2508)  classification_loss: 1.1337 (1.1225)  loss_mask: 0.0000 (0.0000)  time: 0.1978  data: 0.0003  max mem: 5511
[10:13:49.991041] Epoch: [88]  [320/781]  eta: 0:01:32  lr: 0.000010  training_loss: 1.3229 (1.3717)  mae_loss: 0.2435 (0.2502)  classification_loss: 1.0880 (1.1215)  loss_mask: 0.0000 (0.0000)  time: 0.1988  data: 0.0002  max mem: 5511
[10:13:53.965018] Epoch: [88]  [340/781]  eta: 0:01:28  lr: 0.000010  training_loss: 1.3172 (1.3705)  mae_loss: 0.2333 (0.2494)  classification_loss: 1.0842 (1.1211)  loss_mask: 0.0000 (0.0000)  time: 0.1986  data: 0.0002  max mem: 5511
[10:13:57.955948] Epoch: [88]  [360/781]  eta: 0:01:24  lr: 0.000010  training_loss: 1.4196 (1.3719)  mae_loss: 0.2467 (0.2494)  classification_loss: 1.1189 (1.1225)  loss_mask: 0.0000 (0.0000)  time: 0.1994  data: 0.0002  max mem: 5511
[10:14:01.920479] Epoch: [88]  [380/781]  eta: 0:01:20  lr: 0.000010  training_loss: 1.3999 (1.3744)  mae_loss: 0.2373 (0.2492)  classification_loss: 1.1793 (1.1252)  loss_mask: 0.0000 (0.0000)  time: 0.1981  data: 0.0002  max mem: 5511
[10:14:05.853184] Epoch: [88]  [400/781]  eta: 0:01:16  lr: 0.000010  training_loss: 1.3461 (1.3742)  mae_loss: 0.2324 (0.2491)  classification_loss: 1.0858 (1.1250)  loss_mask: 0.0000 (0.0000)  time: 0.1965  data: 0.0002  max mem: 5511
[10:14:09.783205] Epoch: [88]  [420/781]  eta: 0:01:12  lr: 0.000010  training_loss: 1.3915 (1.3745)  mae_loss: 0.2463 (0.2491)  classification_loss: 1.1483 (1.1254)  loss_mask: 0.0000 (0.0000)  time: 0.1964  data: 0.0002  max mem: 5511
[10:14:13.750118] Epoch: [88]  [440/781]  eta: 0:01:08  lr: 0.000010  training_loss: 1.3428 (1.3739)  mae_loss: 0.2416 (0.2488)  classification_loss: 1.0923 (1.1252)  loss_mask: 0.0000 (0.0000)  time: 0.1983  data: 0.0002  max mem: 5511
[10:14:17.701435] Epoch: [88]  [460/781]  eta: 0:01:04  lr: 0.000010  training_loss: 1.3616 (1.3756)  mae_loss: 0.2552 (0.2492)  classification_loss: 1.1138 (1.1264)  loss_mask: 0.0000 (0.0000)  time: 0.1975  data: 0.0003  max mem: 5511
[10:14:21.651907] Epoch: [88]  [480/781]  eta: 0:01:00  lr: 0.000010  training_loss: 1.3969 (1.3765)  mae_loss: 0.2546 (0.2494)  classification_loss: 1.1620 (1.1270)  loss_mask: 0.0000 (0.0000)  time: 0.1974  data: 0.0002  max mem: 5511
[10:14:25.596993] Epoch: [88]  [500/781]  eta: 0:00:56  lr: 0.000010  training_loss: 1.3866 (1.3774)  mae_loss: 0.2401 (0.2491)  classification_loss: 1.1523 (1.1283)  loss_mask: 0.0000 (0.0000)  time: 0.1972  data: 0.0002  max mem: 5511
[10:14:29.525938] Epoch: [88]  [520/781]  eta: 0:00:52  lr: 0.000010  training_loss: 1.3399 (1.3765)  mae_loss: 0.2424 (0.2487)  classification_loss: 1.1116 (1.1278)  loss_mask: 0.0000 (0.0000)  time: 0.1964  data: 0.0002  max mem: 5511
[10:14:33.510752] Epoch: [88]  [540/781]  eta: 0:00:48  lr: 0.000010  training_loss: 1.3247 (1.3759)  mae_loss: 0.2353 (0.2486)  classification_loss: 1.0751 (1.1273)  loss_mask: 0.0000 (0.0000)  time: 0.1992  data: 0.0003  max mem: 5511
[10:14:37.444181] Epoch: [88]  [560/781]  eta: 0:00:44  lr: 0.000010  training_loss: 1.3324 (1.3748)  mae_loss: 0.2415 (0.2483)  classification_loss: 1.1012 (1.1264)  loss_mask: 0.0000 (0.0000)  time: 0.1966  data: 0.0003  max mem: 5511
[10:14:41.426511] Epoch: [88]  [580/781]  eta: 0:00:40  lr: 0.000010  training_loss: 1.3176 (1.3742)  mae_loss: 0.2531 (0.2483)  classification_loss: 1.0886 (1.1258)  loss_mask: 0.0000 (0.0000)  time: 0.1990  data: 0.0003  max mem: 5511
[10:14:45.396899] Epoch: [88]  [600/781]  eta: 0:00:36  lr: 0.000009  training_loss: 1.3465 (1.3740)  mae_loss: 0.2379 (0.2483)  classification_loss: 1.1279 (1.1257)  loss_mask: 0.0000 (0.0000)  time: 0.1984  data: 0.0002  max mem: 5511
[10:14:49.392663] Epoch: [88]  [620/781]  eta: 0:00:32  lr: 0.000009  training_loss: 1.3796 (1.3741)  mae_loss: 0.2612 (0.2485)  classification_loss: 1.1196 (1.1256)  loss_mask: 0.0000 (0.0000)  time: 0.1997  data: 0.0002  max mem: 5511
[10:14:53.353842] Epoch: [88]  [640/781]  eta: 0:00:28  lr: 0.000009  training_loss: 1.3410 (1.3741)  mae_loss: 0.2375 (0.2482)  classification_loss: 1.1256 (1.1258)  loss_mask: 0.0000 (0.0000)  time: 0.1980  data: 0.0003  max mem: 5511
[10:14:57.281731] Epoch: [88]  [660/781]  eta: 0:00:24  lr: 0.000009  training_loss: 1.4079 (1.3746)  mae_loss: 0.2405 (0.2481)  classification_loss: 1.1673 (1.1264)  loss_mask: 0.0002 (0.0001)  time: 0.1963  data: 0.0002  max mem: 5511
[10:15:01.229886] Epoch: [88]  [680/781]  eta: 0:00:20  lr: 0.000009  training_loss: 1.3857 (1.3751)  mae_loss: 0.2487 (0.2482)  classification_loss: 1.1473 (1.1268)  loss_mask: 0.0002 (0.0001)  time: 0.1973  data: 0.0004  max mem: 5511
[10:15:05.150239] Epoch: [88]  [700/781]  eta: 0:00:16  lr: 0.000009  training_loss: 1.3884 (1.3764)  mae_loss: 0.2372 (0.2482)  classification_loss: 1.1707 (1.1281)  loss_mask: 0.0000 (0.0001)  time: 0.1959  data: 0.0002  max mem: 5511
[10:15:09.065370] Epoch: [88]  [720/781]  eta: 0:00:12  lr: 0.000009  training_loss: 1.3871 (1.3768)  mae_loss: 0.2432 (0.2482)  classification_loss: 1.1274 (1.1284)  loss_mask: 0.0001 (0.0002)  time: 0.1957  data: 0.0002  max mem: 5511
[10:15:13.017490] Epoch: [88]  [740/781]  eta: 0:00:08  lr: 0.000009  training_loss: 1.3893 (1.3765)  mae_loss: 0.2416 (0.2482)  classification_loss: 1.1252 (1.1282)  loss_mask: 0.0000 (0.0002)  time: 0.1975  data: 0.0002  max mem: 5511
[10:15:16.971450] Epoch: [88]  [760/781]  eta: 0:00:04  lr: 0.000009  training_loss: 1.4139 (1.3775)  mae_loss: 0.2518 (0.2484)  classification_loss: 1.1644 (1.1289)  loss_mask: 0.0000 (0.0002)  time: 0.1976  data: 0.0002  max mem: 5511
[10:15:20.900600] Epoch: [88]  [780/781]  eta: 0:00:00  lr: 0.000009  training_loss: 1.3768 (1.3780)  mae_loss: 0.2405 (0.2483)  classification_loss: 1.1350 (1.1295)  loss_mask: 0.0000 (0.0002)  time: 0.1964  data: 0.0002  max mem: 5511
[10:15:21.048524] Epoch: [88] Total time: 0:02:35 (0.1990 s / it)
[10:15:21.049061] Averaged stats: lr: 0.000009  training_loss: 1.3768 (1.3780)  mae_loss: 0.2405 (0.2483)  classification_loss: 1.1350 (1.1295)  loss_mask: 0.0000 (0.0002)
[10:15:21.806808] Test:  [  0/157]  eta: 0:01:57  testing_loss: 0.4035 (0.4035)  acc1: 90.6250 (90.6250)  acc5: 100.0000 (100.0000)  time: 0.7509  data: 0.7129  max mem: 5511
[10:15:22.091517] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.4534 (0.4608)  acc1: 87.5000 (86.9318)  acc5: 100.0000 (99.5739)  time: 0.0940  data: 0.0649  max mem: 5511
[10:15:22.378251] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4434 (0.4446)  acc1: 87.5000 (86.9048)  acc5: 100.0000 (99.4792)  time: 0.0284  data: 0.0001  max mem: 5511
[10:15:22.662743] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4264 (0.4613)  acc1: 85.9375 (85.9375)  acc5: 100.0000 (99.3952)  time: 0.0284  data: 0.0002  max mem: 5511
[10:15:22.947994] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4439 (0.4665)  acc1: 84.3750 (85.9756)  acc5: 98.4375 (99.2759)  time: 0.0284  data: 0.0002  max mem: 5511
[10:15:23.231076] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4212 (0.4564)  acc1: 87.5000 (86.3358)  acc5: 100.0000 (99.2647)  time: 0.0283  data: 0.0002  max mem: 5511
[10:15:23.518945] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4124 (0.4508)  acc1: 87.5000 (86.6291)  acc5: 100.0000 (99.2828)  time: 0.0284  data: 0.0003  max mem: 5511
[10:15:23.806149] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4317 (0.4498)  acc1: 87.5000 (86.7077)  acc5: 100.0000 (99.2958)  time: 0.0286  data: 0.0003  max mem: 5511
[10:15:24.093303] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4566 (0.4578)  acc1: 85.9375 (86.5355)  acc5: 100.0000 (99.2670)  time: 0.0286  data: 0.0002  max mem: 5511
[10:15:24.381770] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4502 (0.4514)  acc1: 85.9375 (86.7445)  acc5: 100.0000 (99.2445)  time: 0.0286  data: 0.0003  max mem: 5511
[10:15:24.666948] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.4115 (0.4529)  acc1: 87.5000 (86.7574)  acc5: 100.0000 (99.2729)  time: 0.0286  data: 0.0003  max mem: 5511
[10:15:24.950530] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4639 (0.4549)  acc1: 85.9375 (86.7962)  acc5: 100.0000 (99.2821)  time: 0.0283  data: 0.0002  max mem: 5511
[10:15:25.233133] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4439 (0.4543)  acc1: 85.9375 (86.8285)  acc5: 100.0000 (99.2639)  time: 0.0282  data: 0.0002  max mem: 5511
[10:15:25.518344] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4336 (0.4540)  acc1: 85.9375 (86.7963)  acc5: 100.0000 (99.2724)  time: 0.0283  data: 0.0002  max mem: 5511
[10:15:25.802073] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4286 (0.4513)  acc1: 87.5000 (86.8684)  acc5: 100.0000 (99.3129)  time: 0.0283  data: 0.0002  max mem: 5511
[10:15:26.083919] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4286 (0.4491)  acc1: 87.5000 (86.8688)  acc5: 100.0000 (99.3377)  time: 0.0281  data: 0.0001  max mem: 5511
[10:15:26.236205] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4322 (0.4489)  acc1: 87.5000 (86.8500)  acc5: 100.0000 (99.3400)  time: 0.0272  data: 0.0001  max mem: 5511
[10:15:26.407097] Test: Total time: 0:00:05 (0.0341 s / it)
[10:15:26.407555] * Acc@1 86.850 Acc@5 99.340 loss 0.449
[10:15:26.407847] Accuracy of the network on the 10000 test images: 86.8%
[10:15:26.408029] Max accuracy: 86.96%
[10:15:26.614655] log_dir: ./output_dir
[10:15:27.441688] Epoch: [89]  [  0/781]  eta: 0:10:44  lr: 0.000009  training_loss: 1.1671 (1.1671)  mae_loss: 0.2261 (0.2261)  classification_loss: 0.9410 (0.9410)  loss_mask: 0.0000 (0.0000)  time: 0.8252  data: 0.5969  max mem: 5511
[10:15:31.400183] Epoch: [89]  [ 20/781]  eta: 0:02:53  lr: 0.000009  training_loss: 1.3383 (1.3547)  mae_loss: 0.2515 (0.2525)  classification_loss: 1.1102 (1.1022)  loss_mask: 0.0000 (0.0000)  time: 0.1978  data: 0.0002  max mem: 5511
[10:15:35.331679] Epoch: [89]  [ 40/781]  eta: 0:02:37  lr: 0.000009  training_loss: 1.3549 (1.3725)  mae_loss: 0.2323 (0.2455)  classification_loss: 1.1370 (1.1270)  loss_mask: 0.0000 (0.0000)  time: 0.1965  data: 0.0002  max mem: 5511
[10:15:39.298578] Epoch: [89]  [ 60/781]  eta: 0:02:29  lr: 0.000009  training_loss: 1.3957 (1.3881)  mae_loss: 0.2441 (0.2459)  classification_loss: 1.1595 (1.1422)  loss_mask: 0.0000 (0.0000)  time: 0.1983  data: 0.0002  max mem: 5511
[10:15:43.248411] Epoch: [89]  [ 80/781]  eta: 0:02:23  lr: 0.000009  training_loss: 1.3205 (1.3818)  mae_loss: 0.2402 (0.2465)  classification_loss: 1.0813 (1.1353)  loss_mask: 0.0000 (0.0000)  time: 0.1974  data: 0.0002  max mem: 5511
[10:15:47.179832] Epoch: [89]  [100/781]  eta: 0:02:18  lr: 0.000009  training_loss: 1.3827 (1.3811)  mae_loss: 0.2472 (0.2474)  classification_loss: 1.1373 (1.1337)  loss_mask: 0.0000 (0.0000)  time: 0.1965  data: 0.0002  max mem: 5511
[10:15:51.122455] Epoch: [89]  [120/781]  eta: 0:02:13  lr: 0.000009  training_loss: 1.3536 (1.3743)  mae_loss: 0.2402 (0.2464)  classification_loss: 1.1045 (1.1279)  loss_mask: 0.0000 (0.0000)  time: 0.1971  data: 0.0002  max mem: 5511
[10:15:55.062903] Epoch: [89]  [140/781]  eta: 0:02:09  lr: 0.000009  training_loss: 1.3711 (1.3756)  mae_loss: 0.2499 (0.2476)  classification_loss: 1.1042 (1.1280)  loss_mask: 0.0000 (0.0000)  time: 0.1970  data: 0.0002  max mem: 5511
[10:15:59.020234] Epoch: [89]  [160/781]  eta: 0:02:04  lr: 0.000009  training_loss: 1.3537 (1.3742)  mae_loss: 0.2440 (0.2474)  classification_loss: 1.1047 (1.1267)  loss_mask: 0.0000 (0.0000)  time: 0.1978  data: 0.0003  max mem: 5511
[10:16:02.983514] Epoch: [89]  [180/781]  eta: 0:02:00  lr: 0.000009  training_loss: 1.3870 (1.3774)  mae_loss: 0.2429 (0.2476)  classification_loss: 1.1456 (1.1298)  loss_mask: 0.0000 (0.0000)  time: 0.1981  data: 0.0003  max mem: 5511
[10:16:06.934272] Epoch: [89]  [200/781]  eta: 0:01:56  lr: 0.000009  training_loss: 1.4260 (1.3804)  mae_loss: 0.2480 (0.2482)  classification_loss: 1.1765 (1.1321)  loss_mask: 0.0000 (0.0000)  time: 0.1975  data: 0.0002  max mem: 5511
[10:16:10.892482] Epoch: [89]  [220/781]  eta: 0:01:52  lr: 0.000009  training_loss: 1.3854 (1.3799)  mae_loss: 0.2311 (0.2471)  classification_loss: 1.1509 (1.1328)  loss_mask: 0.0000 (0.0000)  time: 0.1978  data: 0.0002  max mem: 5511
[10:16:14.847993] Epoch: [89]  [240/781]  eta: 0:01:48  lr: 0.000009  training_loss: 1.3900 (1.3828)  mae_loss: 0.2543 (0.2482)  classification_loss: 1.1470 (1.1346)  loss_mask: 0.0000 (0.0000)  time: 0.1977  data: 0.0002  max mem: 5511
[10:16:18.781048] Epoch: [89]  [260/781]  eta: 0:01:44  lr: 0.000009  training_loss: 1.3338 (1.3813)  mae_loss: 0.2436 (0.2478)  classification_loss: 1.0893 (1.1334)  loss_mask: 0.0000 (0.0001)  time: 0.1966  data: 0.0002  max mem: 5511
[10:16:22.724215] Epoch: [89]  [280/781]  eta: 0:01:39  lr: 0.000009  training_loss: 1.3775 (1.3824)  mae_loss: 0.2411 (0.2479)  classification_loss: 1.1200 (1.1345)  loss_mask: 0.0000 (0.0001)  time: 0.1971  data: 0.0002  max mem: 5511
[10:16:26.720248] Epoch: [89]  [300/781]  eta: 0:01:36  lr: 0.000009  training_loss: 1.3874 (1.3834)  mae_loss: 0.2463 (0.2478)  classification_loss: 1.1304 (1.1356)  loss_mask: 0.0000 (0.0001)  time: 0.1997  data: 0.0002  max mem: 5511
[10:16:30.665111] Epoch: [89]  [320/781]  eta: 0:01:31  lr: 0.000009  training_loss: 1.3649 (1.3840)  mae_loss: 0.2434 (0.2478)  classification_loss: 1.1336 (1.1361)  loss_mask: 0.0000 (0.0001)  time: 0.1972  data: 0.0002  max mem: 5511
[10:16:34.606372] Epoch: [89]  [340/781]  eta: 0:01:27  lr: 0.000009  training_loss: 1.3912 (1.3832)  mae_loss: 0.2613 (0.2489)  classification_loss: 1.0921 (1.1342)  loss_mask: 0.0000 (0.0001)  time: 0.1970  data: 0.0002  max mem: 5511
[10:16:38.543423] Epoch: [89]  [360/781]  eta: 0:01:23  lr: 0.000008  training_loss: 1.3613 (1.3836)  mae_loss: 0.2310 (0.2483)  classification_loss: 1.1244 (1.1352)  loss_mask: 0.0000 (0.0001)  time: 0.1968  data: 0.0003  max mem: 5511
[10:16:42.485463] Epoch: [89]  [380/781]  eta: 0:01:19  lr: 0.000008  training_loss: 1.3627 (1.3830)  mae_loss: 0.2586 (0.2490)  classification_loss: 1.1194 (1.1339)  loss_mask: 0.0000 (0.0001)  time: 0.1970  data: 0.0003  max mem: 5511
[10:16:46.476956] Epoch: [89]  [400/781]  eta: 0:01:15  lr: 0.000008  training_loss: 1.3622 (1.3814)  mae_loss: 0.2387 (0.2489)  classification_loss: 1.0931 (1.1324)  loss_mask: 0.0000 (0.0001)  time: 0.1995  data: 0.0003  max mem: 5511
[10:16:50.408050] Epoch: [89]  [420/781]  eta: 0:01:11  lr: 0.000008  training_loss: 1.3723 (1.3805)  mae_loss: 0.2446 (0.2490)  classification_loss: 1.0973 (1.1314)  loss_mask: 0.0000 (0.0001)  time: 0.1964  data: 0.0002  max mem: 5511
[10:16:54.345212] Epoch: [89]  [440/781]  eta: 0:01:07  lr: 0.000008  training_loss: 1.3703 (1.3799)  mae_loss: 0.2400 (0.2489)  classification_loss: 1.1321 (1.1310)  loss_mask: 0.0000 (0.0001)  time: 0.1968  data: 0.0002  max mem: 5511
[10:16:58.284822] Epoch: [89]  [460/781]  eta: 0:01:03  lr: 0.000008  training_loss: 1.3551 (1.3786)  mae_loss: 0.2467 (0.2488)  classification_loss: 1.0994 (1.1298)  loss_mask: 0.0000 (0.0001)  time: 0.1969  data: 0.0002  max mem: 5511
[10:17:02.220537] Epoch: [89]  [480/781]  eta: 0:00:59  lr: 0.000008  training_loss: 1.3855 (1.3792)  mae_loss: 0.2349 (0.2483)  classification_loss: 1.1547 (1.1308)  loss_mask: 0.0001 (0.0001)  time: 0.1967  data: 0.0003  max mem: 5511
[10:17:06.187542] Epoch: [89]  [500/781]  eta: 0:00:55  lr: 0.000008  training_loss: 1.3514 (1.3781)  mae_loss: 0.2447 (0.2481)  classification_loss: 1.1262 (1.1299)  loss_mask: 0.0000 (0.0001)  time: 0.1983  data: 0.0002  max mem: 5511
[10:17:10.154690] Epoch: [89]  [520/781]  eta: 0:00:51  lr: 0.000008  training_loss: 1.3893 (1.3783)  mae_loss: 0.2413 (0.2479)  classification_loss: 1.1565 (1.1303)  loss_mask: 0.0000 (0.0001)  time: 0.1983  data: 0.0002  max mem: 5511
[10:17:14.103729] Epoch: [89]  [540/781]  eta: 0:00:47  lr: 0.000008  training_loss: 1.3747 (1.3782)  mae_loss: 0.2461 (0.2483)  classification_loss: 1.1280 (1.1298)  loss_mask: 0.0000 (0.0001)  time: 0.1974  data: 0.0002  max mem: 5511
[10:17:18.086701] Epoch: [89]  [560/781]  eta: 0:00:43  lr: 0.000008  training_loss: 1.3139 (1.3773)  mae_loss: 0.2502 (0.2485)  classification_loss: 1.0790 (1.1287)  loss_mask: 0.0000 (0.0001)  time: 0.1991  data: 0.0006  max mem: 5511
[10:17:22.044762] Epoch: [89]  [580/781]  eta: 0:00:39  lr: 0.000008  training_loss: 1.3613 (1.3775)  mae_loss: 0.2483 (0.2488)  classification_loss: 1.1089 (1.1286)  loss_mask: 0.0000 (0.0001)  time: 0.1978  data: 0.0003  max mem: 5511
[10:17:25.997267] Epoch: [89]  [600/781]  eta: 0:00:35  lr: 0.000008  training_loss: 1.3500 (1.3765)  mae_loss: 0.2393 (0.2486)  classification_loss: 1.0977 (1.1278)  loss_mask: 0.0000 (0.0001)  time: 0.1975  data: 0.0002  max mem: 5511
[10:17:29.948477] Epoch: [89]  [620/781]  eta: 0:00:31  lr: 0.000008  training_loss: 1.3207 (1.3759)  mae_loss: 0.2366 (0.2484)  classification_loss: 1.0808 (1.1274)  loss_mask: 0.0000 (0.0001)  time: 0.1975  data: 0.0002  max mem: 5511
[10:17:33.970450] Epoch: [89]  [640/781]  eta: 0:00:28  lr: 0.000008  training_loss: 1.3907 (1.3762)  mae_loss: 0.2329 (0.2482)  classification_loss: 1.1221 (1.1279)  loss_mask: 0.0000 (0.0001)  time: 0.2010  data: 0.0003  max mem: 5511
[10:17:37.943356] Epoch: [89]  [660/781]  eta: 0:00:24  lr: 0.000008  training_loss: 1.4128 (1.3773)  mae_loss: 0.2393 (0.2483)  classification_loss: 1.1685 (1.1289)  loss_mask: 0.0000 (0.0001)  time: 0.1986  data: 0.0002  max mem: 5511
[10:17:41.939538] Epoch: [89]  [680/781]  eta: 0:00:20  lr: 0.000008  training_loss: 1.3622 (1.3775)  mae_loss: 0.2376 (0.2482)  classification_loss: 1.1157 (1.1292)  loss_mask: 0.0000 (0.0001)  time: 0.1997  data: 0.0003  max mem: 5511
[10:17:45.913938] Epoch: [89]  [700/781]  eta: 0:00:16  lr: 0.000008  training_loss: 1.4188 (1.3783)  mae_loss: 0.2486 (0.2484)  classification_loss: 1.1580 (1.1298)  loss_mask: 0.0000 (0.0001)  time: 0.1986  data: 0.0002  max mem: 5511
[10:17:49.874555] Epoch: [89]  [720/781]  eta: 0:00:12  lr: 0.000008  training_loss: 1.4075 (1.3791)  mae_loss: 0.2458 (0.2485)  classification_loss: 1.1800 (1.1306)  loss_mask: 0.0000 (0.0001)  time: 0.1979  data: 0.0002  max mem: 5511
[10:17:53.832311] Epoch: [89]  [740/781]  eta: 0:00:08  lr: 0.000008  training_loss: 1.3799 (1.3789)  mae_loss: 0.2425 (0.2483)  classification_loss: 1.1085 (1.1305)  loss_mask: 0.0000 (0.0001)  time: 0.1978  data: 0.0003  max mem: 5511

[10:17:57.782244] Epoch: [89]  [760/781]  eta: 0:00:04  lr: 0.000008  training_loss: 1.3697 (1.3791)  mae_loss: 0.2478 (0.2486)  classification_loss: 1.1351 (1.1304)  loss_mask: 0.0000 (0.0001)  time: 0.1974  data: 0.0002  max mem: 5511
[10:18:01.715453] Epoch: [89]  [780/781]  eta: 0:00:00  lr: 0.000008  training_loss: 1.3995 (1.3805)  mae_loss: 0.2516 (0.2490)  classification_loss: 1.1709 (1.1314)  loss_mask: 0.0000 (0.0001)  time: 0.1966  data: 0.0002  max mem: 5511
[10:18:01.875915] Epoch: [89] Total time: 0:02:35 (0.1988 s / it)
[10:18:01.876659] Averaged stats: lr: 0.000008  training_loss: 1.3995 (1.3805)  mae_loss: 0.2516 (0.2490)  classification_loss: 1.1709 (1.1314)  loss_mask: 0.0000 (0.0001)
[10:18:02.547921] Test:  [  0/157]  eta: 0:01:44  testing_loss: 0.4230 (0.4230)  acc1: 90.6250 (90.6250)  acc5: 100.0000 (100.0000)  time: 0.6666  data: 0.6323  max mem: 5511
[10:18:02.841362] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4590 (0.4574)  acc1: 85.9375 (86.3636)  acc5: 100.0000 (99.7159)  time: 0.0870  data: 0.0577  max mem: 5511
[10:18:03.135149] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4185 (0.4370)  acc1: 87.5000 (86.9048)  acc5: 100.0000 (99.5536)  time: 0.0292  data: 0.0002  max mem: 5511
[10:18:03.421563] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4185 (0.4533)  acc1: 87.5000 (86.3911)  acc5: 100.0000 (99.4456)  time: 0.0289  data: 0.0002  max mem: 5511
[10:18:03.705804] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4292 (0.4562)  acc1: 85.9375 (86.5473)  acc5: 100.0000 (99.3902)  time: 0.0284  data: 0.0002  max mem: 5511
[10:18:03.990751] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4145 (0.4471)  acc1: 87.5000 (86.8260)  acc5: 100.0000 (99.3873)  time: 0.0283  data: 0.0002  max mem: 5511
[10:18:04.281369] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4126 (0.4429)  acc1: 87.5000 (86.9365)  acc5: 100.0000 (99.3852)  time: 0.0286  data: 0.0004  max mem: 5511
[10:18:04.566198] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4238 (0.4423)  acc1: 85.9375 (86.9058)  acc5: 100.0000 (99.4278)  time: 0.0286  data: 0.0004  max mem: 5511
[10:18:04.854564] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4529 (0.4498)  acc1: 85.9375 (86.6319)  acc5: 100.0000 (99.3634)  time: 0.0285  data: 0.0002  max mem: 5511
[10:18:05.138360] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4264 (0.4437)  acc1: 85.9375 (86.8647)  acc5: 100.0000 (99.3304)  time: 0.0285  data: 0.0002  max mem: 5511
[10:18:05.422173] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4053 (0.4450)  acc1: 87.5000 (87.0050)  acc5: 100.0000 (99.3657)  time: 0.0283  data: 0.0002  max mem: 5511
[10:18:05.706197] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4474 (0.4468)  acc1: 87.5000 (86.9510)  acc5: 100.0000 (99.3384)  time: 0.0283  data: 0.0002  max mem: 5511
[10:18:05.989504] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4415 (0.4460)  acc1: 85.9375 (87.0093)  acc5: 100.0000 (99.3285)  time: 0.0282  data: 0.0002  max mem: 5511
[10:18:06.273524] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4288 (0.4459)  acc1: 85.9375 (86.9871)  acc5: 100.0000 (99.3321)  time: 0.0282  data: 0.0002  max mem: 5511
[10:18:06.566389] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4288 (0.4432)  acc1: 87.5000 (87.1121)  acc5: 100.0000 (99.3684)  time: 0.0287  data: 0.0002  max mem: 5511
[10:18:06.852785] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4178 (0.4412)  acc1: 89.0625 (87.1482)  acc5: 100.0000 (99.3688)  time: 0.0288  data: 0.0002  max mem: 5511
[10:18:07.007831] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4104 (0.4410)  acc1: 87.5000 (87.1300)  acc5: 100.0000 (99.3700)  time: 0.0276  data: 0.0002  max mem: 5511
[10:18:07.167767] Test: Total time: 0:00:05 (0.0337 s / it)
[10:18:07.168284] * Acc@1 87.130 Acc@5 99.370 loss 0.441
[10:18:07.168708] Accuracy of the network on the 10000 test images: 87.1%
[10:18:07.168945] Max accuracy: 87.13%
[10:18:07.561348] log_dir: ./output_dir
[10:18:08.538318] Epoch: [90]  [  0/781]  eta: 0:12:41  lr: 0.000008  training_loss: 1.3165 (1.3165)  mae_loss: 0.2335 (0.2335)  classification_loss: 1.0830 (1.0830)  loss_mask: 0.0000 (0.0000)  time: 0.9751  data: 0.7389  max mem: 5511
[10:18:12.514920] Epoch: [90]  [ 20/781]  eta: 0:02:59  lr: 0.000008  training_loss: 1.3426 (1.3561)  mae_loss: 0.2424 (0.2468)  classification_loss: 1.0972 (1.1093)  loss_mask: 0.0000 (0.0000)  time: 0.1987  data: 0.0002  max mem: 5511
[10:18:16.473868] Epoch: [90]  [ 40/781]  eta: 0:02:40  lr: 0.000008  training_loss: 1.3606 (1.3589)  mae_loss: 0.2456 (0.2464)  classification_loss: 1.1150 (1.1125)  loss_mask: 0.0000 (0.0000)  time: 0.1979  data: 0.0002  max mem: 5511
[10:18:20.414152] Epoch: [90]  [ 60/781]  eta: 0:02:31  lr: 0.000008  training_loss: 1.3856 (1.3711)  mae_loss: 0.2480 (0.2468)  classification_loss: 1.1425 (1.1243)  loss_mask: 0.0000 (0.0000)  time: 0.1969  data: 0.0002  max mem: 5511
[10:18:24.345386] Epoch: [90]  [ 80/781]  eta: 0:02:25  lr: 0.000008  training_loss: 1.3059 (1.3647)  mae_loss: 0.2438 (0.2450)  classification_loss: 1.0722 (1.1197)  loss_mask: 0.0000 (0.0000)  time: 0.1965  data: 0.0003  max mem: 5511
[10:18:28.290365] Epoch: [90]  [100/781]  eta: 0:02:19  lr: 0.000008  training_loss: 1.3929 (1.3743)  mae_loss: 0.2447 (0.2455)  classification_loss: 1.1623 (1.1288)  loss_mask: 0.0000 (0.0000)  time: 0.1972  data: 0.0002  max mem: 5511
[10:18:32.235814] Epoch: [90]  [120/781]  eta: 0:02:14  lr: 0.000008  training_loss: 1.3657 (1.3762)  mae_loss: 0.2499 (0.2456)  classification_loss: 1.1102 (1.1306)  loss_mask: 0.0000 (0.0000)  time: 0.1972  data: 0.0002  max mem: 5511
[10:18:36.179145] Epoch: [90]  [140/781]  eta: 0:02:10  lr: 0.000008  training_loss: 1.3966 (1.3766)  mae_loss: 0.2315 (0.2449)  classification_loss: 1.1439 (1.1316)  loss_mask: 0.0000 (0.0000)  time: 0.1971  data: 0.0002  max mem: 5511
[10:18:40.123869] Epoch: [90]  [160/781]  eta: 0:02:05  lr: 0.000007  training_loss: 1.3875 (1.3794)  mae_loss: 0.2517 (0.2469)  classification_loss: 1.1484 (1.1324)  loss_mask: 0.0000 (0.0000)  time: 0.1971  data: 0.0002  max mem: 5511
[10:18:44.054738] Epoch: [90]  [180/781]  eta: 0:02:01  lr: 0.000007  training_loss: 1.3790 (1.3769)  mae_loss: 0.2438 (0.2462)  classification_loss: 1.1214 (1.1306)  loss_mask: 0.0000 (0.0000)  time: 0.1965  data: 0.0002  max mem: 5511
[10:18:47.997556] Epoch: [90]  [200/781]  eta: 0:01:56  lr: 0.000007  training_loss: 1.3104 (1.3730)  mae_loss: 0.2477 (0.2463)  classification_loss: 1.0740 (1.1267)  loss_mask: 0.0000 (0.0000)  time: 0.1971  data: 0.0002  max mem: 5511
[10:18:51.946523] Epoch: [90]  [220/781]  eta: 0:01:52  lr: 0.000007  training_loss: 1.4164 (1.3734)  mae_loss: 0.2385 (0.2456)  classification_loss: 1.1407 (1.1278)  loss_mask: 0.0000 (0.0000)  time: 0.1974  data: 0.0002  max mem: 5511
[10:18:55.893469] Epoch: [90]  [240/781]  eta: 0:01:48  lr: 0.000007  training_loss: 1.3638 (1.3734)  mae_loss: 0.2510 (0.2467)  classification_loss: 1.1219 (1.1267)  loss_mask: 0.0000 (0.0000)  time: 0.1973  data: 0.0005  max mem: 5511
[10:18:59.846234] Epoch: [90]  [260/781]  eta: 0:01:44  lr: 0.000007  training_loss: 1.3627 (1.3741)  mae_loss: 0.2475 (0.2466)  classification_loss: 1.1197 (1.1275)  loss_mask: 0.0000 (0.0000)  time: 0.1976  data: 0.0002  max mem: 5511
[10:19:03.772567] Epoch: [90]  [280/781]  eta: 0:01:40  lr: 0.000007  training_loss: 1.3674 (1.3730)  mae_loss: 0.2414 (0.2468)  classification_loss: 1.0909 (1.1262)  loss_mask: 0.0000 (0.0000)  time: 0.1962  data: 0.0003  max mem: 5511
[10:19:07.709311] Epoch: [90]  [300/781]  eta: 0:01:36  lr: 0.000007  training_loss: 1.3941 (1.3756)  mae_loss: 0.2323 (0.2464)  classification_loss: 1.1639 (1.1291)  loss_mask: 0.0000 (0.0000)  time: 0.1968  data: 0.0004  max mem: 5511
[10:19:11.652820] Epoch: [90]  [320/781]  eta: 0:01:32  lr: 0.000007  training_loss: 1.3780 (1.3751)  mae_loss: 0.2340 (0.2462)  classification_loss: 1.1140 (1.1289)  loss_mask: 0.0000 (0.0000)  time: 0.1971  data: 0.0002  max mem: 5511
[10:19:15.596472] Epoch: [90]  [340/781]  eta: 0:01:27  lr: 0.000007  training_loss: 1.3803 (1.3751)  mae_loss: 0.2582 (0.2471)  classification_loss: 1.1205 (1.1279)  loss_mask: 0.0000 (0.0000)  time: 0.1971  data: 0.0003  max mem: 5511
[10:19:19.560880] Epoch: [90]  [360/781]  eta: 0:01:23  lr: 0.000007  training_loss: 1.3277 (1.3726)  mae_loss: 0.2319 (0.2462)  classification_loss: 1.1041 (1.1264)  loss_mask: 0.0000 (0.0000)  time: 0.1981  data: 0.0002  max mem: 5511
[10:19:23.516328] Epoch: [90]  [380/781]  eta: 0:01:19  lr: 0.000007  training_loss: 1.3580 (1.3729)  mae_loss: 0.2521 (0.2464)  classification_loss: 1.1167 (1.1265)  loss_mask: 0.0000 (0.0000)  time: 0.1977  data: 0.0002  max mem: 5511
[10:19:27.452331] Epoch: [90]  [400/781]  eta: 0:01:15  lr: 0.000007  training_loss: 1.2974 (1.3697)  mae_loss: 0.2384 (0.2457)  classification_loss: 1.0748 (1.1239)  loss_mask: 0.0000 (0.0000)  time: 0.1967  data: 0.0002  max mem: 5511
[10:19:31.404249] Epoch: [90]  [420/781]  eta: 0:01:11  lr: 0.000007  training_loss: 1.3271 (1.3693)  mae_loss: 0.2530 (0.2462)  classification_loss: 1.0855 (1.1231)  loss_mask: 0.0000 (0.0000)  time: 0.1975  data: 0.0002  max mem: 5511
[10:19:35.342927] Epoch: [90]  [440/781]  eta: 0:01:07  lr: 0.000007  training_loss: 1.4549 (1.3709)  mae_loss: 0.2531 (0.2464)  classification_loss: 1.1770 (1.1245)  loss_mask: 0.0000 (0.0000)  time: 0.1969  data: 0.0002  max mem: 5511
[10:19:39.300272] Epoch: [90]  [460/781]  eta: 0:01:03  lr: 0.000007  training_loss: 1.3416 (1.3699)  mae_loss: 0.2378 (0.2461)  classification_loss: 1.0897 (1.1239)  loss_mask: 0.0000 (0.0000)  time: 0.1978  data: 0.0002  max mem: 5511
[10:19:43.244234] Epoch: [90]  [480/781]  eta: 0:00:59  lr: 0.000007  training_loss: 1.3364 (1.3690)  mae_loss: 0.2377 (0.2459)  classification_loss: 1.1013 (1.1230)  loss_mask: 0.0000 (0.0000)  time: 0.1971  data: 0.0002  max mem: 5511
[10:19:47.179269] Epoch: [90]  [500/781]  eta: 0:00:55  lr: 0.000007  training_loss: 1.3492 (1.3693)  mae_loss: 0.2443 (0.2463)  classification_loss: 1.1246 (1.1229)  loss_mask: 0.0000 (0.0000)  time: 0.1967  data: 0.0002  max mem: 5511
[10:19:51.119611] Epoch: [90]  [520/781]  eta: 0:00:51  lr: 0.000007  training_loss: 1.3708 (1.3692)  mae_loss: 0.2486 (0.2462)  classification_loss: 1.1050 (1.1230)  loss_mask: 0.0000 (0.0000)  time: 0.1969  data: 0.0002  max mem: 5511
[10:19:55.049082] Epoch: [90]  [540/781]  eta: 0:00:47  lr: 0.000007  training_loss: 1.3573 (1.3700)  mae_loss: 0.2427 (0.2465)  classification_loss: 1.1205 (1.1234)  loss_mask: 0.0000 (0.0000)  time: 0.1964  data: 0.0002  max mem: 5511
[10:19:58.994375] Epoch: [90]  [560/781]  eta: 0:00:43  lr: 0.000007  training_loss: 1.3626 (1.3699)  mae_loss: 0.2330 (0.2464)  classification_loss: 1.1171 (1.1234)  loss_mask: 0.0000 (0.0000)  time: 0.1972  data: 0.0002  max mem: 5511
[10:20:02.960355] Epoch: [90]  [580/781]  eta: 0:00:39  lr: 0.000007  training_loss: 1.4012 (1.3711)  mae_loss: 0.2436 (0.2464)  classification_loss: 1.1599 (1.1247)  loss_mask: 0.0000 (0.0000)  time: 0.1982  data: 0.0004  max mem: 5511
[10:20:06.913047] Epoch: [90]  [600/781]  eta: 0:00:35  lr: 0.000007  training_loss: 1.3459 (1.3709)  mae_loss: 0.2433 (0.2464)  classification_loss: 1.1012 (1.1244)  loss_mask: 0.0000 (0.0000)  time: 0.1975  data: 0.0002  max mem: 5511
[10:20:10.864252] Epoch: [90]  [620/781]  eta: 0:00:31  lr: 0.000007  training_loss: 1.3649 (1.3705)  mae_loss: 0.2508 (0.2466)  classification_loss: 1.1112 (1.1239)  loss_mask: 0.0000 (0.0000)  time: 0.1975  data: 0.0002  max mem: 5511
[10:20:14.857027] Epoch: [90]  [640/781]  eta: 0:00:27  lr: 0.000007  training_loss: 1.3827 (1.3712)  mae_loss: 0.2321 (0.2464)  classification_loss: 1.1467 (1.1248)  loss_mask: 0.0000 (0.0000)  time: 0.1995  data: 0.0002  max mem: 5511
[10:20:18.813665] Epoch: [90]  [660/781]  eta: 0:00:24  lr: 0.000007  training_loss: 1.3702 (1.3714)  mae_loss: 0.2542 (0.2467)  classification_loss: 1.1345 (1.1246)  loss_mask: 0.0000 (0.0000)  time: 0.1977  data: 0.0002  max mem: 5511
[10:20:22.778492] Epoch: [90]  [680/781]  eta: 0:00:20  lr: 0.000007  training_loss: 1.3782 (1.3718)  mae_loss: 0.2514 (0.2470)  classification_loss: 1.1104 (1.1248)  loss_mask: 0.0000 (0.0000)  time: 0.1982  data: 0.0004  max mem: 5511
[10:20:26.725962] Epoch: [90]  [700/781]  eta: 0:00:16  lr: 0.000007  training_loss: 1.3493 (1.3723)  mae_loss: 0.2333 (0.2468)  classification_loss: 1.1199 (1.1255)  loss_mask: 0.0000 (0.0000)  time: 0.1973  data: 0.0002  max mem: 5511
[10:20:30.701652] Epoch: [90]  [720/781]  eta: 0:00:12  lr: 0.000007  training_loss: 1.3472 (1.3715)  mae_loss: 0.2412 (0.2469)  classification_loss: 1.1148 (1.1246)  loss_mask: 0.0000 (0.0000)  time: 0.1987  data: 0.0002  max mem: 5511
[10:20:34.722407] Epoch: [90]  [740/781]  eta: 0:00:08  lr: 0.000007  training_loss: 1.3182 (1.3714)  mae_loss: 0.2436 (0.2466)  classification_loss: 1.1116 (1.1246)  loss_mask: 0.0001 (0.0001)  time: 0.2009  data: 0.0003  max mem: 5511
[10:20:38.670584] Epoch: [90]  [760/781]  eta: 0:00:04  lr: 0.000007  training_loss: 1.4168 (1.3723)  mae_loss: 0.2403 (0.2465)  classification_loss: 1.1805 (1.1257)  loss_mask: 0.0000 (0.0001)  time: 0.1973  data: 0.0002  max mem: 5511
[10:20:42.606004] Epoch: [90]  [780/781]  eta: 0:00:00  lr: 0.000006  training_loss: 1.3836 (1.3735)  mae_loss: 0.2336 (0.2463)  classification_loss: 1.1588 (1.1271)  loss_mask: 0.0001 (0.0001)  time: 0.1967  data: 0.0002  max mem: 5511
[10:20:42.783948] Epoch: [90] Total time: 0:02:35 (0.1987 s / it)
[10:20:42.784681] Averaged stats: lr: 0.000006  training_loss: 1.3836 (1.3735)  mae_loss: 0.2336 (0.2463)  classification_loss: 1.1588 (1.1271)  loss_mask: 0.0001 (0.0001)
[10:20:44.203942] Test:  [  0/157]  eta: 0:02:01  testing_loss: 0.4241 (0.4241)  acc1: 89.0625 (89.0625)  acc5: 100.0000 (100.0000)  time: 0.7744  data: 0.7435  max mem: 5511
[10:20:44.494623] Test:  [ 10/157]  eta: 0:00:14  testing_loss: 0.4530 (0.4589)  acc1: 87.5000 (86.5057)  acc5: 100.0000 (99.7159)  time: 0.0967  data: 0.0678  max mem: 5511
[10:20:44.779352] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4188 (0.4369)  acc1: 87.5000 (86.9792)  acc5: 100.0000 (99.6280)  time: 0.0286  data: 0.0002  max mem: 5511
[10:20:45.066990] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4092 (0.4528)  acc1: 85.9375 (86.3407)  acc5: 100.0000 (99.4456)  time: 0.0285  data: 0.0002  max mem: 5511
[10:20:45.354371] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4235 (0.4551)  acc1: 85.9375 (86.4710)  acc5: 100.0000 (99.3521)  time: 0.0286  data: 0.0002  max mem: 5511
[10:20:45.646550] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4159 (0.4462)  acc1: 87.5000 (86.8566)  acc5: 100.0000 (99.3260)  time: 0.0288  data: 0.0002  max mem: 5511
[10:20:45.951631] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4074 (0.4419)  acc1: 87.5000 (87.0902)  acc5: 100.0000 (99.3340)  time: 0.0296  data: 0.0003  max mem: 5511
[10:20:46.254070] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4334 (0.4409)  acc1: 85.9375 (87.0379)  acc5: 100.0000 (99.3618)  time: 0.0302  data: 0.0003  max mem: 5511
[10:20:46.541731] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4468 (0.4493)  acc1: 84.3750 (86.8056)  acc5: 100.0000 (99.3248)  time: 0.0294  data: 0.0002  max mem: 5511
[10:20:46.838324] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4384 (0.4428)  acc1: 87.5000 (87.0536)  acc5: 100.0000 (99.2960)  time: 0.0291  data: 0.0002  max mem: 5511
[10:20:47.128838] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.4062 (0.4433)  acc1: 89.0625 (87.1132)  acc5: 100.0000 (99.3348)  time: 0.0292  data: 0.0002  max mem: 5511
[10:20:47.420384] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4402 (0.4457)  acc1: 85.9375 (87.0918)  acc5: 100.0000 (99.3102)  time: 0.0290  data: 0.0002  max mem: 5511
[10:20:47.710186] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4402 (0.4452)  acc1: 85.9375 (87.1513)  acc5: 100.0000 (99.3156)  time: 0.0289  data: 0.0002  max mem: 5511
[10:20:47.993420] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4250 (0.4451)  acc1: 85.9375 (87.0825)  acc5: 100.0000 (99.3201)  time: 0.0285  data: 0.0002  max mem: 5511
[10:20:48.278359] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4088 (0.4425)  acc1: 87.5000 (87.1676)  acc5: 100.0000 (99.3573)  time: 0.0283  data: 0.0002  max mem: 5511
[10:20:48.559972] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4088 (0.4409)  acc1: 87.5000 (87.1689)  acc5: 100.0000 (99.3688)  time: 0.0282  data: 0.0001  max mem: 5511
[10:20:48.715704] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4116 (0.4412)  acc1: 87.5000 (87.1600)  acc5: 100.0000 (99.3700)  time: 0.0273  data: 0.0001  max mem: 5511
[10:20:48.891530] Test: Total time: 0:00:05 (0.0348 s / it)
[10:20:48.892097] * Acc@1 87.160 Acc@5 99.370 loss 0.441
[10:20:48.892417] Accuracy of the network on the 10000 test images: 87.2%
[10:20:48.892619] Max accuracy: 87.16%
[10:20:49.261292] log_dir: ./output_dir
[10:20:50.239309] Epoch: [91]  [  0/781]  eta: 0:12:42  lr: 0.000006  training_loss: 1.2398 (1.2398)  mae_loss: 0.2369 (0.2369)  classification_loss: 1.0029 (1.0029)  loss_mask: 0.0000 (0.0000)  time: 0.9760  data: 0.7415  max mem: 5511
[10:20:54.169221] Epoch: [91]  [ 20/781]  eta: 0:02:57  lr: 0.000006  training_loss: 1.3389 (1.3579)  mae_loss: 0.2423 (0.2438)  classification_loss: 1.0915 (1.1141)  loss_mask: 0.0001 (0.0001)  time: 0.1964  data: 0.0002  max mem: 5511
[10:20:58.122033] Epoch: [91]  [ 40/781]  eta: 0:02:40  lr: 0.000006  training_loss: 1.3841 (1.3805)  mae_loss: 0.2641 (0.2516)  classification_loss: 1.1388 (1.1289)  loss_mask: 0.0000 (0.0000)  time: 0.1976  data: 0.0004  max mem: 5511
[10:21:02.114370] Epoch: [91]  [ 60/781]  eta: 0:02:31  lr: 0.000006  training_loss: 1.3503 (1.3804)  mae_loss: 0.2370 (0.2487)  classification_loss: 1.1303 (1.1317)  loss_mask: 0.0000 (0.0000)  time: 0.1995  data: 0.0004  max mem: 5511
[10:21:06.062502] Epoch: [91]  [ 80/781]  eta: 0:02:25  lr: 0.000006  training_loss: 1.3756 (1.3784)  mae_loss: 0.2398 (0.2490)  classification_loss: 1.1323 (1.1294)  loss_mask: 0.0000 (0.0000)  time: 0.1973  data: 0.0002  max mem: 5511
[10:21:10.023913] Epoch: [91]  [100/781]  eta: 0:02:19  lr: 0.000006  training_loss: 1.3682 (1.3789)  mae_loss: 0.2356 (0.2475)  classification_loss: 1.1440 (1.1314)  loss_mask: 0.0000 (0.0000)  time: 0.1980  data: 0.0002  max mem: 5511
[10:21:13.956873] Epoch: [91]  [120/781]  eta: 0:02:14  lr: 0.000006  training_loss: 1.3748 (1.3789)  mae_loss: 0.2389 (0.2472)  classification_loss: 1.1071 (1.1316)  loss_mask: 0.0000 (0.0000)  time: 0.1965  data: 0.0002  max mem: 5511
[10:21:17.918505] Epoch: [91]  [140/781]  eta: 0:02:10  lr: 0.000006  training_loss: 1.3900 (1.3773)  mae_loss: 0.2600 (0.2493)  classification_loss: 1.1272 (1.1280)  loss_mask: 0.0000 (0.0000)  time: 0.1980  data: 0.0002  max mem: 5511
[10:21:21.865076] Epoch: [91]  [160/781]  eta: 0:02:05  lr: 0.000006  training_loss: 1.3285 (1.3725)  mae_loss: 0.2437 (0.2488)  classification_loss: 1.0835 (1.1237)  loss_mask: 0.0000 (0.0000)  time: 0.1973  data: 0.0002  max mem: 5511
[10:21:25.817622] Epoch: [91]  [180/781]  eta: 0:02:01  lr: 0.000006  training_loss: 1.3780 (1.3749)  mae_loss: 0.2410 (0.2483)  classification_loss: 1.1525 (1.1265)  loss_mask: 0.0000 (0.0000)  time: 0.1975  data: 0.0002  max mem: 5511
[10:21:29.801089] Epoch: [91]  [200/781]  eta: 0:01:57  lr: 0.000006  training_loss: 1.3470 (1.3737)  mae_loss: 0.2346 (0.2464)  classification_loss: 1.1197 (1.1272)  loss_mask: 0.0000 (0.0000)  time: 0.1991  data: 0.0002  max mem: 5511
[10:21:33.786707] Epoch: [91]  [220/781]  eta: 0:01:52  lr: 0.000006  training_loss: 1.3959 (1.3765)  mae_loss: 0.2544 (0.2475)  classification_loss: 1.1348 (1.1290)  loss_mask: 0.0000 (0.0000)  time: 0.1992  data: 0.0002  max mem: 5511
[10:21:37.732735] Epoch: [91]  [240/781]  eta: 0:01:48  lr: 0.000006  training_loss: 1.3522 (1.3747)  mae_loss: 0.2459 (0.2474)  classification_loss: 1.0853 (1.1273)  loss_mask: 0.0000 (0.0000)  time: 0.1972  data: 0.0002  max mem: 5511
[10:21:41.676052] Epoch: [91]  [260/781]  eta: 0:01:44  lr: 0.000006  training_loss: 1.3315 (1.3718)  mae_loss: 0.2335 (0.2467)  classification_loss: 1.0945 (1.1251)  loss_mask: 0.0000 (0.0000)  time: 0.1971  data: 0.0003  max mem: 5511
[10:21:45.628296] Epoch: [91]  [280/781]  eta: 0:01:40  lr: 0.000006  training_loss: 1.4115 (1.3743)  mae_loss: 0.2527 (0.2475)  classification_loss: 1.1529 (1.1267)  loss_mask: 0.0000 (0.0000)  time: 0.1975  data: 0.0004  max mem: 5511
[10:21:49.582436] Epoch: [91]  [300/781]  eta: 0:01:36  lr: 0.000006  training_loss: 1.3398 (1.3748)  mae_loss: 0.2459 (0.2477)  classification_loss: 1.0939 (1.1271)  loss_mask: 0.0000 (0.0000)  time: 0.1976  data: 0.0002  max mem: 5511
[10:21:53.522331] Epoch: [91]  [320/781]  eta: 0:01:32  lr: 0.000006  training_loss: 1.3550 (1.3749)  mae_loss: 0.2553 (0.2485)  classification_loss: 1.0870 (1.1263)  loss_mask: 0.0000 (0.0001)  time: 0.1969  data: 0.0002  max mem: 5511
[10:21:57.477091] Epoch: [91]  [340/781]  eta: 0:01:28  lr: 0.000006  training_loss: 1.3675 (1.3758)  mae_loss: 0.2551 (0.2489)  classification_loss: 1.1438 (1.1268)  loss_mask: 0.0001 (0.0001)  time: 0.1977  data: 0.0002  max mem: 5511
[10:22:01.416188] Epoch: [91]  [360/781]  eta: 0:01:24  lr: 0.000006  training_loss: 1.3736 (1.3754)  mae_loss: 0.2513 (0.2492)  classification_loss: 1.1316 (1.1261)  loss_mask: 0.0000 (0.0001)  time: 0.1969  data: 0.0003  max mem: 5511
[10:22:05.351929] Epoch: [91]  [380/781]  eta: 0:01:20  lr: 0.000006  training_loss: 1.4193 (1.3768)  mae_loss: 0.2523 (0.2496)  classification_loss: 1.1581 (1.1271)  loss_mask: 0.0000 (0.0001)  time: 0.1967  data: 0.0002  max mem: 5511
[10:22:09.282811] Epoch: [91]  [400/781]  eta: 0:01:15  lr: 0.000006  training_loss: 1.3965 (1.3786)  mae_loss: 0.2586 (0.2498)  classification_loss: 1.1206 (1.1288)  loss_mask: 0.0000 (0.0001)  time: 0.1964  data: 0.0002  max mem: 5511
[10:22:13.220981] Epoch: [91]  [420/781]  eta: 0:01:11  lr: 0.000006  training_loss: 1.4024 (1.3788)  mae_loss: 0.2490 (0.2496)  classification_loss: 1.1664 (1.1290)  loss_mask: 0.0001 (0.0002)  time: 0.1968  data: 0.0002  max mem: 5511
[10:22:17.191458] Epoch: [91]  [440/781]  eta: 0:01:07  lr: 0.000006  training_loss: 1.3720 (1.3781)  mae_loss: 0.2417 (0.2494)  classification_loss: 1.1040 (1.1284)  loss_mask: 0.0000 (0.0002)  time: 0.1984  data: 0.0002  max mem: 5511
[10:22:21.131342] Epoch: [91]  [460/781]  eta: 0:01:03  lr: 0.000006  training_loss: 1.3540 (1.3768)  mae_loss: 0.2294 (0.2490)  classification_loss: 1.0880 (1.1276)  loss_mask: 0.0000 (0.0002)  time: 0.1969  data: 0.0002  max mem: 5511
[10:22:25.069099] Epoch: [91]  [480/781]  eta: 0:00:59  lr: 0.000006  training_loss: 1.3555 (1.3770)  mae_loss: 0.2449 (0.2490)  classification_loss: 1.1206 (1.1277)  loss_mask: 0.0000 (0.0002)  time: 0.1968  data: 0.0002  max mem: 5511
[10:22:29.031945] Epoch: [91]  [500/781]  eta: 0:00:55  lr: 0.000006  training_loss: 1.3636 (1.3771)  mae_loss: 0.2427 (0.2487)  classification_loss: 1.1287 (1.1282)  loss_mask: 0.0000 (0.0002)  time: 0.1981  data: 0.0002  max mem: 5511
[10:22:33.005333] Epoch: [91]  [520/781]  eta: 0:00:51  lr: 0.000006  training_loss: 1.3431 (1.3761)  mae_loss: 0.2392 (0.2485)  classification_loss: 1.0905 (1.1274)  loss_mask: 0.0000 (0.0002)  time: 0.1986  data: 0.0002  max mem: 5511
[10:22:36.967305] Epoch: [91]  [540/781]  eta: 0:00:47  lr: 0.000006  training_loss: 1.3583 (1.3755)  mae_loss: 0.2268 (0.2483)  classification_loss: 1.1089 (1.1270)  loss_mask: 0.0000 (0.0002)  time: 0.1980  data: 0.0002  max mem: 5511
[10:22:40.904900] Epoch: [91]  [560/781]  eta: 0:00:43  lr: 0.000006  training_loss: 1.3805 (1.3751)  mae_loss: 0.2487 (0.2488)  classification_loss: 1.1025 (1.1262)  loss_mask: 0.0000 (0.0002)  time: 0.1968  data: 0.0002  max mem: 5511
[10:22:44.870739] Epoch: [91]  [580/781]  eta: 0:00:39  lr: 0.000006  training_loss: 1.3227 (1.3749)  mae_loss: 0.2556 (0.2489)  classification_loss: 1.0959 (1.1258)  loss_mask: 0.0000 (0.0002)  time: 0.1982  data: 0.0002  max mem: 5511
[10:22:48.843814] Epoch: [91]  [600/781]  eta: 0:00:35  lr: 0.000006  training_loss: 1.3389 (1.3735)  mae_loss: 0.2459 (0.2489)  classification_loss: 1.0692 (1.1244)  loss_mask: 0.0000 (0.0002)  time: 0.1986  data: 0.0002  max mem: 5511
[10:22:52.784114] Epoch: [91]  [620/781]  eta: 0:00:32  lr: 0.000006  training_loss: 1.3745 (1.3729)  mae_loss: 0.2421 (0.2489)  classification_loss: 1.1199 (1.1238)  loss_mask: 0.0000 (0.0002)  time: 0.1969  data: 0.0002  max mem: 5511
[10:22:56.757517] Epoch: [91]  [640/781]  eta: 0:00:28  lr: 0.000006  training_loss: 1.3925 (1.3734)  mae_loss: 0.2610 (0.2492)  classification_loss: 1.1464 (1.1238)  loss_mask: 0.0000 (0.0003)  time: 0.1986  data: 0.0002  max mem: 5511
[10:23:00.761141] Epoch: [91]  [660/781]  eta: 0:00:24  lr: 0.000005  training_loss: 1.3675 (1.3736)  mae_loss: 0.2453 (0.2491)  classification_loss: 1.1134 (1.1242)  loss_mask: 0.0001 (0.0003)  time: 0.2001  data: 0.0002  max mem: 5511
[10:23:04.722010] Epoch: [91]  [680/781]  eta: 0:00:20  lr: 0.000005  training_loss: 1.3564 (1.3732)  mae_loss: 0.2478 (0.2491)  classification_loss: 1.1103 (1.1238)  loss_mask: 0.0001 (0.0003)  time: 0.1979  data: 0.0002  max mem: 5511
[10:23:08.723147] Epoch: [91]  [700/781]  eta: 0:00:16  lr: 0.000005  training_loss: 1.3257 (1.3722)  mae_loss: 0.2503 (0.2491)  classification_loss: 1.0889 (1.1227)  loss_mask: 0.0000 (0.0003)  time: 0.2000  data: 0.0003  max mem: 5511
[10:23:12.712768] Epoch: [91]  [720/781]  eta: 0:00:12  lr: 0.000005  training_loss: 1.3421 (1.3719)  mae_loss: 0.2459 (0.2491)  classification_loss: 1.1068 (1.1225)  loss_mask: 0.0000 (0.0003)  time: 0.1994  data: 0.0003  max mem: 5511
[10:23:16.660340] Epoch: [91]  [740/781]  eta: 0:00:08  lr: 0.000005  training_loss: 1.3718 (1.3724)  mae_loss: 0.2493 (0.2493)  classification_loss: 1.1053 (1.1228)  loss_mask: 0.0000 (0.0003)  time: 0.1973  data: 0.0002  max mem: 5511
[10:23:20.632364] Epoch: [91]  [760/781]  eta: 0:00:04  lr: 0.000005  training_loss: 1.4135 (1.3740)  mae_loss: 0.2515 (0.2495)  classification_loss: 1.1631 (1.1242)  loss_mask: 0.0000 (0.0003)  time: 0.1985  data: 0.0003  max mem: 5511
[10:23:24.566687] Epoch: [91]  [780/781]  eta: 0:00:00  lr: 0.000005  training_loss: 1.3865 (1.3738)  mae_loss: 0.2499 (0.2496)  classification_loss: 1.1240 (1.1239)  loss_mask: 0.0000 (0.0003)  time: 0.1966  data: 0.0002  max mem: 5511
[10:23:24.755377] Epoch: [91] Total time: 0:02:35 (0.1991 s / it)
[10:23:24.756073] Averaged stats: lr: 0.000005  training_loss: 1.3865 (1.3738)  mae_loss: 0.2499 (0.2496)  classification_loss: 1.1240 (1.1239)  loss_mask: 0.0000 (0.0003)
[10:23:25.370062] Test:  [  0/157]  eta: 0:01:35  testing_loss: 0.4285 (0.4285)  acc1: 89.0625 (89.0625)  acc5: 98.4375 (98.4375)  time: 0.6098  data: 0.5750  max mem: 5511
[10:23:25.668752] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4517 (0.4554)  acc1: 89.0625 (87.3580)  acc5: 100.0000 (99.4318)  time: 0.0824  data: 0.0526  max mem: 5511
[10:23:25.955426] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4238 (0.4386)  acc1: 89.0625 (87.5744)  acc5: 100.0000 (99.4792)  time: 0.0291  data: 0.0003  max mem: 5511
[10:23:26.249954] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4140 (0.4532)  acc1: 87.5000 (86.7944)  acc5: 100.0000 (99.3952)  time: 0.0289  data: 0.0003  max mem: 5511
[10:23:26.542970] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4289 (0.4582)  acc1: 87.5000 (86.8902)  acc5: 100.0000 (99.3521)  time: 0.0292  data: 0.0003  max mem: 5511
[10:23:26.829757] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4074 (0.4481)  acc1: 87.5000 (87.2549)  acc5: 100.0000 (99.3260)  time: 0.0288  data: 0.0002  max mem: 5511
[10:23:27.118005] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4074 (0.4432)  acc1: 87.5000 (87.4232)  acc5: 100.0000 (99.3340)  time: 0.0286  data: 0.0002  max mem: 5511
[10:23:27.404482] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4318 (0.4418)  acc1: 85.9375 (87.3900)  acc5: 100.0000 (99.2738)  time: 0.0286  data: 0.0002  max mem: 5511
[10:23:27.694015] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4451 (0.4498)  acc1: 85.9375 (87.1142)  acc5: 98.4375 (99.2284)  time: 0.0287  data: 0.0003  max mem: 5511
[10:23:27.984673] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4317 (0.4435)  acc1: 85.9375 (87.4141)  acc5: 98.4375 (99.2102)  time: 0.0288  data: 0.0004  max mem: 5511
[10:23:28.277588] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.3940 (0.4448)  acc1: 89.0625 (87.4226)  acc5: 100.0000 (99.2574)  time: 0.0290  data: 0.0003  max mem: 5511
[10:23:28.562534] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4472 (0.4471)  acc1: 85.9375 (87.3029)  acc5: 100.0000 (99.2399)  time: 0.0288  data: 0.0002  max mem: 5511
[10:23:28.846776] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4465 (0.4458)  acc1: 85.9375 (87.3192)  acc5: 100.0000 (99.2381)  time: 0.0283  data: 0.0002  max mem: 5511
[10:23:29.131720] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4172 (0.4458)  acc1: 85.9375 (87.2853)  acc5: 100.0000 (99.2605)  time: 0.0283  data: 0.0002  max mem: 5511
[10:23:29.414683] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4172 (0.4432)  acc1: 87.5000 (87.3559)  acc5: 100.0000 (99.3019)  time: 0.0283  data: 0.0002  max mem: 5511
[10:23:29.695815] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4157 (0.4413)  acc1: 87.5000 (87.3655)  acc5: 100.0000 (99.3171)  time: 0.0281  data: 0.0001  max mem: 5511
[10:23:29.847593] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4157 (0.4410)  acc1: 87.5000 (87.3600)  acc5: 100.0000 (99.3100)  time: 0.0272  data: 0.0001  max mem: 5511
[10:23:30.010310] Test: Total time: 0:00:05 (0.0334 s / it)
[10:23:30.010749] * Acc@1 87.360 Acc@5 99.310 loss 0.441
[10:23:30.011038] Accuracy of the network on the 10000 test images: 87.4%
[10:23:30.011242] Max accuracy: 87.36%
[10:23:30.344914] log_dir: ./output_dir
[10:23:31.334594] Epoch: [92]  [  0/781]  eta: 0:12:51  lr: 0.000005  training_loss: 1.1739 (1.1739)  mae_loss: 0.2593 (0.2593)  classification_loss: 0.9146 (0.9146)  loss_mask: 0.0000 (0.0000)  time: 0.9879  data: 0.7809  max mem: 5511
[10:23:35.301787] Epoch: [92]  [ 20/781]  eta: 0:02:59  lr: 0.000005  training_loss: 1.3546 (1.3428)  mae_loss: 0.2447 (0.2519)  classification_loss: 1.0878 (1.0909)  loss_mask: 0.0000 (0.0000)  time: 0.1983  data: 0.0003  max mem: 5511
[10:23:39.269598] Epoch: [92]  [ 40/781]  eta: 0:02:41  lr: 0.000005  training_loss: 1.3739 (1.3658)  mae_loss: 0.2401 (0.2491)  classification_loss: 1.1527 (1.1166)  loss_mask: 0.0000 (0.0000)  time: 0.1982  data: 0.0002  max mem: 5511
[10:23:43.227823] Epoch: [92]  [ 60/781]  eta: 0:02:32  lr: 0.000005  training_loss: 1.3782 (1.3657)  mae_loss: 0.2441 (0.2485)  classification_loss: 1.1035 (1.1172)  loss_mask: 0.0000 (0.0000)  time: 0.1978  data: 0.0005  max mem: 5511
[10:23:47.182103] Epoch: [92]  [ 80/781]  eta: 0:02:25  lr: 0.000005  training_loss: 1.3800 (1.3728)  mae_loss: 0.2464 (0.2471)  classification_loss: 1.1474 (1.1250)  loss_mask: 0.0000 (0.0007)  time: 0.1976  data: 0.0003  max mem: 5511
[10:23:51.169736] Epoch: [92]  [100/781]  eta: 0:02:20  lr: 0.000005  training_loss: 1.3562 (1.3798)  mae_loss: 0.2462 (0.2480)  classification_loss: 1.1256 (1.1313)  loss_mask: 0.0000 (0.0006)  time: 0.1993  data: 0.0002  max mem: 5511
[10:23:55.115829] Epoch: [92]  [120/781]  eta: 0:02:15  lr: 0.000005  training_loss: 1.3968 (1.3846)  mae_loss: 0.2497 (0.2489)  classification_loss: 1.1450 (1.1346)  loss_mask: 0.0000 (0.0011)  time: 0.1972  data: 0.0002  max mem: 5511
[10:23:59.061003] Epoch: [92]  [140/781]  eta: 0:02:10  lr: 0.000005  training_loss: 1.3267 (1.3793)  mae_loss: 0.2380 (0.2481)  classification_loss: 1.1121 (1.1302)  loss_mask: 0.0000 (0.0009)  time: 0.1972  data: 0.0002  max mem: 5511
[10:24:03.009652] Epoch: [92]  [160/781]  eta: 0:02:05  lr: 0.000005  training_loss: 1.3692 (1.3799)  mae_loss: 0.2628 (0.2497)  classification_loss: 1.1071 (1.1294)  loss_mask: 0.0000 (0.0008)  time: 0.1973  data: 0.0002  max mem: 5511
[10:24:06.933784] Epoch: [92]  [180/781]  eta: 0:02:01  lr: 0.000005  training_loss: 1.3322 (1.3806)  mae_loss: 0.2511 (0.2497)  classification_loss: 1.1108 (1.1301)  loss_mask: 0.0000 (0.0007)  time: 0.1961  data: 0.0002  max mem: 5511
[10:24:10.861835] Epoch: [92]  [200/781]  eta: 0:01:57  lr: 0.000005  training_loss: 1.3584 (1.3782)  mae_loss: 0.2474 (0.2495)  classification_loss: 1.1095 (1.1281)  loss_mask: 0.0000 (0.0007)  time: 0.1963  data: 0.0002  max mem: 5511
[10:24:14.807927] Epoch: [92]  [220/781]  eta: 0:01:52  lr: 0.000005  training_loss: 1.3337 (1.3774)  mae_loss: 0.2513 (0.2495)  classification_loss: 1.0970 (1.1273)  loss_mask: 0.0000 (0.0006)  time: 0.1972  data: 0.0002  max mem: 5511
[10:24:18.753517] Epoch: [92]  [240/781]  eta: 0:01:48  lr: 0.000005  training_loss: 1.3455 (1.3773)  mae_loss: 0.2443 (0.2495)  classification_loss: 1.0736 (1.1272)  loss_mask: 0.0000 (0.0006)  time: 0.1972  data: 0.0002  max mem: 5511
[10:24:22.712303] Epoch: [92]  [260/781]  eta: 0:01:44  lr: 0.000005  training_loss: 1.3694 (1.3764)  mae_loss: 0.2398 (0.2491)  classification_loss: 1.1259 (1.1268)  loss_mask: 0.0000 (0.0005)  time: 0.1978  data: 0.0002  max mem: 5511
[10:24:26.636394] Epoch: [92]  [280/781]  eta: 0:01:40  lr: 0.000005  training_loss: 1.3544 (1.3754)  mae_loss: 0.2461 (0.2490)  classification_loss: 1.1252 (1.1259)  loss_mask: 0.0000 (0.0005)  time: 0.1961  data: 0.0003  max mem: 5511
[10:24:30.593319] Epoch: [92]  [300/781]  eta: 0:01:36  lr: 0.000005  training_loss: 1.3959 (1.3768)  mae_loss: 0.2466 (0.2490)  classification_loss: 1.1493 (1.1273)  loss_mask: 0.0000 (0.0005)  time: 0.1978  data: 0.0002  max mem: 5511
[10:24:34.529946] Epoch: [92]  [320/781]  eta: 0:01:32  lr: 0.000005  training_loss: 1.3552 (1.3764)  mae_loss: 0.2450 (0.2489)  classification_loss: 1.1241 (1.1270)  loss_mask: 0.0000 (0.0004)  time: 0.1967  data: 0.0002  max mem: 5511
[10:24:38.475336] Epoch: [92]  [340/781]  eta: 0:01:28  lr: 0.000005  training_loss: 1.3917 (1.3765)  mae_loss: 0.2501 (0.2491)  classification_loss: 1.1302 (1.1270)  loss_mask: 0.0000 (0.0004)  time: 0.1972  data: 0.0002  max mem: 5511
[10:24:42.411695] Epoch: [92]  [360/781]  eta: 0:01:24  lr: 0.000005  training_loss: 1.3646 (1.3757)  mae_loss: 0.2406 (0.2491)  classification_loss: 1.1008 (1.1262)  loss_mask: 0.0000 (0.0004)  time: 0.1967  data: 0.0002  max mem: 5511
[10:24:46.339880] Epoch: [92]  [380/781]  eta: 0:01:19  lr: 0.000005  training_loss: 1.3955 (1.3773)  mae_loss: 0.2434 (0.2492)  classification_loss: 1.1532 (1.1277)  loss_mask: 0.0000 (0.0004)  time: 0.1963  data: 0.0002  max mem: 5511
[10:24:50.289166] Epoch: [92]  [400/781]  eta: 0:01:15  lr: 0.000005  training_loss: 1.3850 (1.3772)  mae_loss: 0.2411 (0.2490)  classification_loss: 1.1439 (1.1279)  loss_mask: 0.0000 (0.0003)  time: 0.1974  data: 0.0002  max mem: 5511
[10:24:54.240060] Epoch: [92]  [420/781]  eta: 0:01:11  lr: 0.000005  training_loss: 1.3911 (1.3774)  mae_loss: 0.2577 (0.2498)  classification_loss: 1.1279 (1.1273)  loss_mask: 0.0000 (0.0003)  time: 0.1975  data: 0.0002  max mem: 5511
[10:24:58.183062] Epoch: [92]  [440/781]  eta: 0:01:07  lr: 0.000005  training_loss: 1.3233 (1.3755)  mae_loss: 0.2435 (0.2498)  classification_loss: 1.0743 (1.1254)  loss_mask: 0.0000 (0.0003)  time: 0.1971  data: 0.0002  max mem: 5511
[10:25:02.123941] Epoch: [92]  [460/781]  eta: 0:01:03  lr: 0.000005  training_loss: 1.2997 (1.3744)  mae_loss: 0.2314 (0.2490)  classification_loss: 1.0827 (1.1251)  loss_mask: 0.0000 (0.0003)  time: 0.1970  data: 0.0002  max mem: 5511
[10:25:06.096764] Epoch: [92]  [480/781]  eta: 0:00:59  lr: 0.000005  training_loss: 1.4411 (1.3763)  mae_loss: 0.2338 (0.2486)  classification_loss: 1.2005 (1.1274)  loss_mask: 0.0000 (0.0003)  time: 0.1986  data: 0.0002  max mem: 5511
[10:25:10.065586] Epoch: [92]  [500/781]  eta: 0:00:55  lr: 0.000005  training_loss: 1.3900 (1.3770)  mae_loss: 0.2460 (0.2491)  classification_loss: 1.1281 (1.1276)  loss_mask: 0.0000 (0.0003)  time: 0.1983  data: 0.0002  max mem: 5511
[10:25:14.042647] Epoch: [92]  [520/781]  eta: 0:00:51  lr: 0.000005  training_loss: 1.3497 (1.3766)  mae_loss: 0.2507 (0.2494)  classification_loss: 1.0921 (1.1269)  loss_mask: 0.0000 (0.0003)  time: 0.1988  data: 0.0003  max mem: 5511
[10:25:18.018328] Epoch: [92]  [540/781]  eta: 0:00:47  lr: 0.000005  training_loss: 1.4029 (1.3785)  mae_loss: 0.2438 (0.2496)  classification_loss: 1.1631 (1.1286)  loss_mask: 0.0000 (0.0003)  time: 0.1987  data: 0.0002  max mem: 5511
[10:25:22.002844] Epoch: [92]  [560/781]  eta: 0:00:43  lr: 0.000005  training_loss: 1.3754 (1.3778)  mae_loss: 0.2473 (0.2496)  classification_loss: 1.1256 (1.1279)  loss_mask: 0.0000 (0.0003)  time: 0.1991  data: 0.0003  max mem: 5511
[10:25:25.968898] Epoch: [92]  [580/781]  eta: 0:00:39  lr: 0.000005  training_loss: 1.3627 (1.3776)  mae_loss: 0.2428 (0.2496)  classification_loss: 1.1269 (1.1277)  loss_mask: 0.0000 (0.0002)  time: 0.1982  data: 0.0003  max mem: 5511
[10:25:29.942617] Epoch: [92]  [600/781]  eta: 0:00:36  lr: 0.000005  training_loss: 1.3395 (1.3775)  mae_loss: 0.2346 (0.2492)  classification_loss: 1.0946 (1.1280)  loss_mask: 0.0000 (0.0002)  time: 0.1986  data: 0.0006  max mem: 5511
[10:25:33.917352] Epoch: [92]  [620/781]  eta: 0:00:32  lr: 0.000005  training_loss: 1.3721 (1.3770)  mae_loss: 0.2554 (0.2494)  classification_loss: 1.1141 (1.1274)  loss_mask: 0.0000 (0.0002)  time: 0.1987  data: 0.0002  max mem: 5511
[10:25:37.910351] Epoch: [92]  [640/781]  eta: 0:00:28  lr: 0.000004  training_loss: 1.3870 (1.3774)  mae_loss: 0.2559 (0.2497)  classification_loss: 1.1231 (1.1275)  loss_mask: 0.0000 (0.0002)  time: 0.1996  data: 0.0002  max mem: 5511
[10:25:41.872128] Epoch: [92]  [660/781]  eta: 0:00:24  lr: 0.000004  training_loss: 1.3905 (1.3780)  mae_loss: 0.2370 (0.2495)  classification_loss: 1.1591 (1.1283)  loss_mask: 0.0000 (0.0002)  time: 0.1980  data: 0.0002  max mem: 5511
[10:25:45.799533] Epoch: [92]  [680/781]  eta: 0:00:20  lr: 0.000004  training_loss: 1.3848 (1.3785)  mae_loss: 0.2452 (0.2493)  classification_loss: 1.1614 (1.1290)  loss_mask: 0.0000 (0.0002)  time: 0.1963  data: 0.0002  max mem: 5511
[10:25:49.757992] Epoch: [92]  [700/781]  eta: 0:00:16  lr: 0.000004  training_loss: 1.3846 (1.3786)  mae_loss: 0.2537 (0.2496)  classification_loss: 1.1491 (1.1287)  loss_mask: 0.0000 (0.0002)  time: 0.1978  data: 0.0002  max mem: 5511
[10:25:53.724994] Epoch: [92]  [720/781]  eta: 0:00:12  lr: 0.000004  training_loss: 1.3676 (1.3784)  mae_loss: 0.2455 (0.2496)  classification_loss: 1.1261 (1.1286)  loss_mask: 0.0000 (0.0002)  time: 0.1983  data: 0.0004  max mem: 5511
[10:25:57.698528] Epoch: [92]  [740/781]  eta: 0:00:08  lr: 0.000004  training_loss: 1.3239 (1.3775)  mae_loss: 0.2430 (0.2495)  classification_loss: 1.0999 (1.1278)  loss_mask: 0.0000 (0.0002)  time: 0.1986  data: 0.0002  max mem: 5511
[10:26:01.699991] Epoch: [92]  [760/781]  eta: 0:00:04  lr: 0.000004  training_loss: 1.4075 (1.3786)  mae_loss: 0.2515 (0.2495)  classification_loss: 1.1519 (1.1289)  loss_mask: 0.0000 (0.0002)  time: 0.2000  data: 0.0005  max mem: 5511
[10:26:05.641746] Epoch: [92]  [780/781]  eta: 0:00:00  lr: 0.000004  training_loss: 1.3990 (1.3787)  mae_loss: 0.2440 (0.2494)  classification_loss: 1.1559 (1.1292)  loss_mask: 0.0000 (0.0002)  time: 0.1970  data: 0.0003  max mem: 5511
[10:26:05.821575] Epoch: [92] Total time: 0:02:35 (0.1991 s / it)
[10:26:05.822145] Averaged stats: lr: 0.000004  training_loss: 1.3990 (1.3787)  mae_loss: 0.2440 (0.2494)  classification_loss: 1.1559 (1.1292)  loss_mask: 0.0000 (0.0002)
[10:26:06.541283] Test:  [  0/157]  eta: 0:01:52  testing_loss: 0.4238 (0.4238)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (100.0000)  time: 0.7141  data: 0.6849  max mem: 5511
[10:26:06.841651] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.4448 (0.4539)  acc1: 87.5000 (86.9318)  acc5: 100.0000 (99.7159)  time: 0.0920  data: 0.0624  max mem: 5511
[10:26:07.126543] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4222 (0.4381)  acc1: 87.5000 (87.2024)  acc5: 100.0000 (99.6280)  time: 0.0291  data: 0.0002  max mem: 5511
[10:26:07.419661] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4109 (0.4536)  acc1: 87.5000 (86.6935)  acc5: 100.0000 (99.5464)  time: 0.0287  data: 0.0002  max mem: 5511
[10:26:07.709517] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4251 (0.4571)  acc1: 87.5000 (86.6235)  acc5: 100.0000 (99.4284)  time: 0.0289  data: 0.0002  max mem: 5511
[10:26:07.998489] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4111 (0.4470)  acc1: 87.5000 (87.1017)  acc5: 100.0000 (99.3873)  time: 0.0288  data: 0.0003  max mem: 5511
[10:26:08.286293] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4111 (0.4423)  acc1: 87.5000 (87.1670)  acc5: 100.0000 (99.3852)  time: 0.0287  data: 0.0002  max mem: 5511
[10:26:08.577235] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4306 (0.4396)  acc1: 85.9375 (87.1699)  acc5: 100.0000 (99.3838)  time: 0.0288  data: 0.0002  max mem: 5511
[10:26:08.864879] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4344 (0.4477)  acc1: 85.9375 (86.9406)  acc5: 98.4375 (99.3056)  time: 0.0288  data: 0.0002  max mem: 5511
[10:26:09.150478] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4319 (0.4415)  acc1: 89.0625 (87.2768)  acc5: 98.4375 (99.2788)  time: 0.0285  data: 0.0002  max mem: 5511
[10:26:09.435898] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.3958 (0.4425)  acc1: 89.0625 (87.3762)  acc5: 100.0000 (99.3038)  time: 0.0284  data: 0.0002  max mem: 5511
[10:26:09.719655] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4445 (0.4446)  acc1: 87.5000 (87.3874)  acc5: 100.0000 (99.2821)  time: 0.0283  data: 0.0002  max mem: 5511
[10:26:10.005668] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4445 (0.4433)  acc1: 87.5000 (87.4096)  acc5: 100.0000 (99.2898)  time: 0.0284  data: 0.0002  max mem: 5511
[10:26:10.289670] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4228 (0.4430)  acc1: 85.9375 (87.3807)  acc5: 100.0000 (99.2963)  time: 0.0284  data: 0.0002  max mem: 5511
[10:26:10.573504] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4181 (0.4399)  acc1: 85.9375 (87.4446)  acc5: 100.0000 (99.3351)  time: 0.0283  data: 0.0002  max mem: 5511
[10:26:10.855576] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4134 (0.4380)  acc1: 89.0625 (87.4690)  acc5: 100.0000 (99.3481)  time: 0.0282  data: 0.0002  max mem: 5511
[10:26:11.011346] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4134 (0.4378)  acc1: 89.0625 (87.4400)  acc5: 100.0000 (99.3300)  time: 0.0274  data: 0.0001  max mem: 5511
[10:26:11.169600] Test: Total time: 0:00:05 (0.0340 s / it)
[10:26:11.170347] * Acc@1 87.440 Acc@5 99.330 loss 0.438
[10:26:11.170786] Accuracy of the network on the 10000 test images: 87.4%
[10:26:11.171095] Max accuracy: 87.44%
[10:26:11.596321] log_dir: ./output_dir
[10:26:12.527661] Epoch: [93]  [  0/781]  eta: 0:12:06  lr: 0.000004  training_loss: 1.1557 (1.1557)  mae_loss: 0.2066 (0.2066)  classification_loss: 0.9491 (0.9491)  loss_mask: 0.0000 (0.0000)  time: 0.9296  data: 0.6960  max mem: 5511
[10:26:16.464138] Epoch: [93]  [ 20/781]  eta: 0:02:56  lr: 0.000004  training_loss: 1.3202 (1.3279)  mae_loss: 0.2569 (0.2516)  classification_loss: 1.0738 (1.0763)  loss_mask: 0.0000 (0.0000)  time: 0.1967  data: 0.0002  max mem: 5511
[10:26:20.399674] Epoch: [93]  [ 40/781]  eta: 0:02:39  lr: 0.000004  training_loss: 1.3330 (1.3548)  mae_loss: 0.2242 (0.2430)  classification_loss: 1.1130 (1.1118)  loss_mask: 0.0000 (0.0000)  time: 0.1967  data: 0.0002  max mem: 5511
[10:26:24.360834] Epoch: [93]  [ 60/781]  eta: 0:02:30  lr: 0.000004  training_loss: 1.3856 (1.3610)  mae_loss: 0.2603 (0.2487)  classification_loss: 1.1253 (1.1123)  loss_mask: 0.0000 (0.0000)  time: 0.1980  data: 0.0002  max mem: 5511
[10:26:28.300785] Epoch: [93]  [ 80/781]  eta: 0:02:24  lr: 0.000004  training_loss: 1.3354 (1.3566)  mae_loss: 0.2392 (0.2497)  classification_loss: 1.0826 (1.1069)  loss_mask: 0.0000 (0.0000)  time: 0.1969  data: 0.0002  max mem: 5511
[10:26:32.245623] Epoch: [93]  [100/781]  eta: 0:02:19  lr: 0.000004  training_loss: 1.3871 (1.3605)  mae_loss: 0.2311 (0.2486)  classification_loss: 1.1289 (1.1119)  loss_mask: 0.0000 (0.0000)  time: 0.1972  data: 0.0002  max mem: 5511
[10:26:36.199055] Epoch: [93]  [120/781]  eta: 0:02:14  lr: 0.000004  training_loss: 1.3815 (1.3606)  mae_loss: 0.2468 (0.2484)  classification_loss: 1.1330 (1.1122)  loss_mask: 0.0000 (0.0000)  time: 0.1976  data: 0.0002  max mem: 5511
[10:26:40.168990] Epoch: [93]  [140/781]  eta: 0:02:09  lr: 0.000004  training_loss: 1.3523 (1.3597)  mae_loss: 0.2358 (0.2474)  classification_loss: 1.1215 (1.1123)  loss_mask: 0.0000 (0.0000)  time: 0.1984  data: 0.0002  max mem: 5511
[10:26:44.108993] Epoch: [93]  [160/781]  eta: 0:02:05  lr: 0.000004  training_loss: 1.3029 (1.3564)  mae_loss: 0.2427 (0.2474)  classification_loss: 1.1027 (1.1090)  loss_mask: 0.0000 (0.0000)  time: 0.1969  data: 0.0002  max mem: 5511
[10:26:48.077012] Epoch: [93]  [180/781]  eta: 0:02:01  lr: 0.000004  training_loss: 1.3319 (1.3551)  mae_loss: 0.2484 (0.2475)  classification_loss: 1.1165 (1.1076)  loss_mask: 0.0000 (0.0000)  time: 0.1983  data: 0.0003  max mem: 5511
[10:26:52.036665] Epoch: [93]  [200/781]  eta: 0:01:56  lr: 0.000004  training_loss: 1.3220 (1.3544)  mae_loss: 0.2411 (0.2470)  classification_loss: 1.0781 (1.1074)  loss_mask: 0.0000 (0.0000)  time: 0.1979  data: 0.0002  max mem: 5511
[10:26:56.022287] Epoch: [93]  [220/781]  eta: 0:01:52  lr: 0.000004  training_loss: 1.3532 (1.3528)  mae_loss: 0.2267 (0.2459)  classification_loss: 1.1005 (1.1069)  loss_mask: 0.0000 (0.0000)  time: 0.1992  data: 0.0002  max mem: 5511
[10:27:00.006101] Epoch: [93]  [240/781]  eta: 0:01:48  lr: 0.000004  training_loss: 1.3759 (1.3537)  mae_loss: 0.2318 (0.2459)  classification_loss: 1.1226 (1.1077)  loss_mask: 0.0000 (0.0000)  time: 0.1991  data: 0.0002  max mem: 5511
[10:27:03.956140] Epoch: [93]  [260/781]  eta: 0:01:44  lr: 0.000004  training_loss: 1.3612 (1.3553)  mae_loss: 0.2457 (0.2465)  classification_loss: 1.1210 (1.1088)  loss_mask: 0.0000 (0.0000)  time: 0.1974  data: 0.0002  max mem: 5511
[10:27:07.899330] Epoch: [93]  [280/781]  eta: 0:01:40  lr: 0.000004  training_loss: 1.3704 (1.3566)  mae_loss: 0.2439 (0.2465)  classification_loss: 1.0881 (1.1100)  loss_mask: 0.0000 (0.0000)  time: 0.1971  data: 0.0002  max mem: 5511
[10:27:11.878450] Epoch: [93]  [300/781]  eta: 0:01:36  lr: 0.000004  training_loss: 1.3621 (1.3570)  mae_loss: 0.2507 (0.2468)  classification_loss: 1.1149 (1.1102)  loss_mask: 0.0000 (0.0000)  time: 0.1988  data: 0.0002  max mem: 5511
[10:27:15.865043] Epoch: [93]  [320/781]  eta: 0:01:32  lr: 0.000004  training_loss: 1.3401 (1.3584)  mae_loss: 0.2368 (0.2463)  classification_loss: 1.0853 (1.1121)  loss_mask: 0.0000 (0.0000)  time: 0.1992  data: 0.0002  max mem: 5511
[10:27:19.844051] Epoch: [93]  [340/781]  eta: 0:01:28  lr: 0.000004  training_loss: 1.3167 (1.3580)  mae_loss: 0.2478 (0.2468)  classification_loss: 1.0628 (1.1111)  loss_mask: 0.0000 (0.0000)  time: 0.1988  data: 0.0002  max mem: 5511
[10:27:23.801722] Epoch: [93]  [360/781]  eta: 0:01:24  lr: 0.000004  training_loss: 1.4017 (1.3602)  mae_loss: 0.2522 (0.2474)  classification_loss: 1.1225 (1.1128)  loss_mask: 0.0000 (0.0000)  time: 0.1978  data: 0.0002  max mem: 5511
[10:27:27.766585] Epoch: [93]  [380/781]  eta: 0:01:20  lr: 0.000004  training_loss: 1.3106 (1.3590)  mae_loss: 0.2412 (0.2472)  classification_loss: 1.0894 (1.1118)  loss_mask: 0.0000 (0.0000)  time: 0.1981  data: 0.0002  max mem: 5511
[10:27:31.766812] Epoch: [93]  [400/781]  eta: 0:01:16  lr: 0.000004  training_loss: 1.4106 (1.3602)  mae_loss: 0.2506 (0.2478)  classification_loss: 1.1403 (1.1123)  loss_mask: 0.0000 (0.0000)  time: 0.1999  data: 0.0002  max mem: 5511
[10:27:35.727477] Epoch: [93]  [420/781]  eta: 0:01:12  lr: 0.000004  training_loss: 1.3198 (1.3592)  mae_loss: 0.2286 (0.2474)  classification_loss: 1.0915 (1.1117)  loss_mask: 0.0000 (0.0000)  time: 0.1980  data: 0.0002  max mem: 5511
[10:27:39.692559] Epoch: [93]  [440/781]  eta: 0:01:08  lr: 0.000004  training_loss: 1.3870 (1.3608)  mae_loss: 0.2415 (0.2473)  classification_loss: 1.1347 (1.1135)  loss_mask: 0.0000 (0.0000)  time: 0.1982  data: 0.0002  max mem: 5511
[10:27:43.671261] Epoch: [93]  [460/781]  eta: 0:01:04  lr: 0.000004  training_loss: 1.3253 (1.3589)  mae_loss: 0.2399 (0.2470)  classification_loss: 1.0762 (1.1119)  loss_mask: 0.0000 (0.0000)  time: 0.1988  data: 0.0002  max mem: 5511
[10:27:47.638920] Epoch: [93]  [480/781]  eta: 0:01:00  lr: 0.000004  training_loss: 1.4126 (1.3607)  mae_loss: 0.2471 (0.2471)  classification_loss: 1.1439 (1.1136)  loss_mask: 0.0000 (0.0000)  time: 0.1983  data: 0.0003  max mem: 5511
[10:27:51.619230] Epoch: [93]  [500/781]  eta: 0:00:56  lr: 0.000004  training_loss: 1.3854 (1.3613)  mae_loss: 0.2301 (0.2467)  classification_loss: 1.1221 (1.1146)  loss_mask: 0.0000 (0.0000)  time: 0.1989  data: 0.0002  max mem: 5511
[10:27:55.562035] Epoch: [93]  [520/781]  eta: 0:00:52  lr: 0.000004  training_loss: 1.3242 (1.3618)  mae_loss: 0.2449 (0.2469)  classification_loss: 1.0890 (1.1149)  loss_mask: 0.0000 (0.0000)  time: 0.1971  data: 0.0002  max mem: 5511
[10:27:59.481299] Epoch: [93]  [540/781]  eta: 0:00:48  lr: 0.000004  training_loss: 1.3466 (1.3620)  mae_loss: 0.2317 (0.2468)  classification_loss: 1.1111 (1.1152)  loss_mask: 0.0000 (0.0000)  time: 0.1959  data: 0.0002  max mem: 5511
[10:28:03.474912] Epoch: [93]  [560/781]  eta: 0:00:44  lr: 0.000004  training_loss: 1.3975 (1.3627)  mae_loss: 0.2407 (0.2467)  classification_loss: 1.1625 (1.1159)  loss_mask: 0.0000 (0.0000)  time: 0.1996  data: 0.0002  max mem: 5511
[10:28:07.467798] Epoch: [93]  [580/781]  eta: 0:00:40  lr: 0.000004  training_loss: 1.3057 (1.3611)  mae_loss: 0.2434 (0.2466)  classification_loss: 1.0663 (1.1145)  loss_mask: 0.0000 (0.0000)  time: 0.1995  data: 0.0002  max mem: 5511
[10:28:11.436445] Epoch: [93]  [600/781]  eta: 0:00:36  lr: 0.000004  training_loss: 1.3357 (1.3611)  mae_loss: 0.2554 (0.2469)  classification_loss: 1.0794 (1.1142)  loss_mask: 0.0000 (0.0000)  time: 0.1983  data: 0.0002  max mem: 5511
[10:28:15.391319] Epoch: [93]  [620/781]  eta: 0:00:32  lr: 0.000004  training_loss: 1.3905 (1.3621)  mae_loss: 0.2628 (0.2473)  classification_loss: 1.1009 (1.1148)  loss_mask: 0.0000 (0.0000)  time: 0.1977  data: 0.0002  max mem: 5511
[10:28:19.361500] Epoch: [93]  [640/781]  eta: 0:00:28  lr: 0.000004  training_loss: 1.3832 (1.3634)  mae_loss: 0.2607 (0.2479)  classification_loss: 1.1013 (1.1155)  loss_mask: 0.0000 (0.0000)  time: 0.1984  data: 0.0005  max mem: 5511
[10:28:23.353683] Epoch: [93]  [660/781]  eta: 0:00:24  lr: 0.000004  training_loss: 1.3506 (1.3635)  mae_loss: 0.2335 (0.2478)  classification_loss: 1.0905 (1.1156)  loss_mask: 0.0000 (0.0000)  time: 0.1995  data: 0.0002  max mem: 5511
[10:28:27.300118] Epoch: [93]  [680/781]  eta: 0:00:20  lr: 0.000004  training_loss: 1.4074 (1.3644)  mae_loss: 0.2559 (0.2480)  classification_loss: 1.1508 (1.1164)  loss_mask: 0.0000 (0.0000)  time: 0.1972  data: 0.0002  max mem: 5511
[10:28:31.250354] Epoch: [93]  [700/781]  eta: 0:00:16  lr: 0.000004  training_loss: 1.3376 (1.3642)  mae_loss: 0.2523 (0.2481)  classification_loss: 1.0416 (1.1161)  loss_mask: 0.0000 (0.0000)  time: 0.1974  data: 0.0002  max mem: 5511
[10:28:35.206585] Epoch: [93]  [720/781]  eta: 0:00:12  lr: 0.000004  training_loss: 1.3945 (1.3653)  mae_loss: 0.2328 (0.2479)  classification_loss: 1.1641 (1.1174)  loss_mask: 0.0000 (0.0000)  time: 0.1977  data: 0.0002  max mem: 5511
[10:28:39.170911] Epoch: [93]  [740/781]  eta: 0:00:08  lr: 0.000003  training_loss: 1.3390 (1.3644)  mae_loss: 0.2260 (0.2476)  classification_loss: 1.0942 (1.1167)  loss_mask: 0.0000 (0.0000)  time: 0.1981  data: 0.0002  max mem: 5511
[10:28:43.161766] Epoch: [93]  [760/781]  eta: 0:00:04  lr: 0.000003  training_loss: 1.3831 (1.3650)  mae_loss: 0.2464 (0.2478)  classification_loss: 1.1333 (1.1172)  loss_mask: 0.0000 (0.0000)  time: 0.1995  data: 0.0003  max mem: 5511
[10:28:47.100812] Epoch: [93]  [780/781]  eta: 0:00:00  lr: 0.000003  training_loss: 1.3306 (1.3642)  mae_loss: 0.2327 (0.2477)  classification_loss: 1.0798 (1.1165)  loss_mask: 0.0000 (0.0000)  time: 0.1968  data: 0.0002  max mem: 5511
[10:28:47.278324] Epoch: [93] Total time: 0:02:35 (0.1993 s / it)
[10:28:47.279208] Averaged stats: lr: 0.000003  training_loss: 1.3306 (1.3642)  mae_loss: 0.2327 (0.2477)  classification_loss: 1.0798 (1.1165)  loss_mask: 0.0000 (0.0000)
[10:28:48.129496] Test:  [  0/157]  eta: 0:02:12  testing_loss: 0.4224 (0.4224)  acc1: 89.0625 (89.0625)  acc5: 100.0000 (100.0000)  time: 0.8462  data: 0.8114  max mem: 5511
[10:28:48.420658] Test:  [ 10/157]  eta: 0:00:15  testing_loss: 0.4472 (0.4525)  acc1: 89.0625 (87.5000)  acc5: 100.0000 (99.4318)  time: 0.1031  data: 0.0740  max mem: 5511
[10:28:48.715052] Test:  [ 20/157]  eta: 0:00:09  testing_loss: 0.4302 (0.4351)  acc1: 87.5000 (87.6488)  acc5: 100.0000 (99.4048)  time: 0.0290  data: 0.0002  max mem: 5511
[10:28:49.007461] Test:  [ 30/157]  eta: 0:00:07  testing_loss: 0.4163 (0.4521)  acc1: 87.5000 (86.8448)  acc5: 100.0000 (99.3448)  time: 0.0292  data: 0.0003  max mem: 5511
[10:28:49.302513] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4312 (0.4556)  acc1: 85.9375 (86.7378)  acc5: 100.0000 (99.2378)  time: 0.0292  data: 0.0003  max mem: 5511
[10:28:49.590315] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4147 (0.4453)  acc1: 87.5000 (87.1324)  acc5: 100.0000 (99.2341)  time: 0.0290  data: 0.0002  max mem: 5511
[10:28:49.881994] Test:  [ 60/157]  eta: 0:00:04  testing_loss: 0.4046 (0.4405)  acc1: 87.5000 (87.1926)  acc5: 100.0000 (99.2572)  time: 0.0288  data: 0.0002  max mem: 5511
[10:28:50.170639] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4286 (0.4391)  acc1: 85.9375 (87.2139)  acc5: 100.0000 (99.2518)  time: 0.0289  data: 0.0002  max mem: 5511
[10:28:50.462498] Test:  [ 80/157]  eta: 0:00:03  testing_loss: 0.4411 (0.4476)  acc1: 85.9375 (86.9406)  acc5: 98.4375 (99.1898)  time: 0.0289  data: 0.0002  max mem: 5511
[10:28:50.756669] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4209 (0.4412)  acc1: 87.5000 (87.1909)  acc5: 98.4375 (99.1930)  time: 0.0291  data: 0.0003  max mem: 5511
[10:28:51.050472] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.4009 (0.4421)  acc1: 89.0625 (87.2679)  acc5: 100.0000 (99.2265)  time: 0.0293  data: 0.0003  max mem: 5511
[10:28:51.347980] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4588 (0.4443)  acc1: 89.0625 (87.2044)  acc5: 100.0000 (99.2258)  time: 0.0294  data: 0.0002  max mem: 5511
[10:28:51.638096] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4588 (0.4437)  acc1: 85.9375 (87.2159)  acc5: 100.0000 (99.2381)  time: 0.0292  data: 0.0002  max mem: 5511
[10:28:51.929849] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4233 (0.4432)  acc1: 85.9375 (87.2137)  acc5: 100.0000 (99.2605)  time: 0.0290  data: 0.0002  max mem: 5511
[10:28:52.213960] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4062 (0.4401)  acc1: 89.0625 (87.3227)  acc5: 100.0000 (99.3019)  time: 0.0287  data: 0.0002  max mem: 5511
[10:28:52.495452] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4062 (0.4383)  acc1: 89.0625 (87.3655)  acc5: 100.0000 (99.3171)  time: 0.0282  data: 0.0001  max mem: 5511
[10:28:52.648692] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4096 (0.4382)  acc1: 87.5000 (87.3300)  acc5: 100.0000 (99.3200)  time: 0.0272  data: 0.0001  max mem: 5511
[10:28:52.820778] Test: Total time: 0:00:05 (0.0353 s / it)
[10:28:52.821371] * Acc@1 87.330 Acc@5 99.320 loss 0.438
[10:28:52.821879] Accuracy of the network on the 10000 test images: 87.3%
[10:28:52.822172] Max accuracy: 87.44%
[10:28:53.012082] log_dir: ./output_dir
[10:28:53.911989] Epoch: [94]  [  0/781]  eta: 0:11:41  lr: 0.000003  training_loss: 1.2846 (1.2846)  mae_loss: 0.2680 (0.2680)  classification_loss: 1.0149 (1.0149)  loss_mask: 0.0017 (0.0017)  time: 0.8978  data: 0.6668  max mem: 5511
[10:28:57.867928] Epoch: [94]  [ 20/781]  eta: 0:02:55  lr: 0.000003  training_loss: 1.3066 (1.3387)  mae_loss: 0.2558 (0.2580)  classification_loss: 1.0422 (1.0789)  loss_mask: 0.0001 (0.0017)  time: 0.1977  data: 0.0003  max mem: 5511
[10:29:01.795793] Epoch: [94]  [ 40/781]  eta: 0:02:38  lr: 0.000003  training_loss: 1.3859 (1.3602)  mae_loss: 0.2407 (0.2505)  classification_loss: 1.1237 (1.1088)  loss_mask: 0.0001 (0.0009)  time: 0.1963  data: 0.0002  max mem: 5511
[10:29:05.772288] Epoch: [94]  [ 60/781]  eta: 0:02:30  lr: 0.000003  training_loss: 1.3476 (1.3649)  mae_loss: 0.2501 (0.2484)  classification_loss: 1.1359 (1.1158)  loss_mask: 0.0000 (0.0007)  time: 0.1987  data: 0.0002  max mem: 5511
[10:29:09.746630] Epoch: [94]  [ 80/781]  eta: 0:02:24  lr: 0.000003  training_loss: 1.3368 (1.3651)  mae_loss: 0.2568 (0.2512)  classification_loss: 1.1028 (1.1134)  loss_mask: 0.0000 (0.0005)  time: 0.1986  data: 0.0003  max mem: 5511
[10:29:13.709516] Epoch: [94]  [100/781]  eta: 0:02:19  lr: 0.000003  training_loss: 1.3839 (1.3690)  mae_loss: 0.2353 (0.2488)  classification_loss: 1.1459 (1.1197)  loss_mask: 0.0000 (0.0004)  time: 0.1980  data: 0.0003  max mem: 5511
[10:29:17.639444] Epoch: [94]  [120/781]  eta: 0:02:14  lr: 0.000003  training_loss: 1.3831 (1.3699)  mae_loss: 0.2329 (0.2481)  classification_loss: 1.1409 (1.1215)  loss_mask: 0.0000 (0.0004)  time: 0.1964  data: 0.0002  max mem: 5511
[10:29:21.596020] Epoch: [94]  [140/781]  eta: 0:02:09  lr: 0.000003  training_loss: 1.3555 (1.3695)  mae_loss: 0.2500 (0.2481)  classification_loss: 1.1007 (1.1211)  loss_mask: 0.0000 (0.0003)  time: 0.1977  data: 0.0002  max mem: 5511
[10:29:25.533088] Epoch: [94]  [160/781]  eta: 0:02:05  lr: 0.000003  training_loss: 1.3168 (1.3626)  mae_loss: 0.2349 (0.2468)  classification_loss: 1.0690 (1.1155)  loss_mask: 0.0000 (0.0003)  time: 0.1968  data: 0.0002  max mem: 5511
[10:29:29.497221] Epoch: [94]  [180/781]  eta: 0:02:01  lr: 0.000003  training_loss: 1.3831 (1.3652)  mae_loss: 0.2359 (0.2463)  classification_loss: 1.1362 (1.1187)  loss_mask: 0.0000 (0.0002)  time: 0.1981  data: 0.0003  max mem: 5511
[10:29:33.434772] Epoch: [94]  [200/781]  eta: 0:01:56  lr: 0.000003  training_loss: 1.3346 (1.3625)  mae_loss: 0.2370 (0.2461)  classification_loss: 1.0761 (1.1162)  loss_mask: 0.0000 (0.0002)  time: 0.1968  data: 0.0003  max mem: 5511
[10:29:37.380475] Epoch: [94]  [220/781]  eta: 0:01:52  lr: 0.000003  training_loss: 1.3763 (1.3674)  mae_loss: 0.2460 (0.2474)  classification_loss: 1.1213 (1.1198)  loss_mask: 0.0000 (0.0002)  time: 0.1972  data: 0.0002  max mem: 5511
[10:29:41.341356] Epoch: [94]  [240/781]  eta: 0:01:48  lr: 0.000003  training_loss: 1.3530 (1.3682)  mae_loss: 0.2440 (0.2472)  classification_loss: 1.1126 (1.1209)  loss_mask: 0.0000 (0.0002)  time: 0.1980  data: 0.0002  max mem: 5511
[10:29:45.302884] Epoch: [94]  [260/781]  eta: 0:01:44  lr: 0.000003  training_loss: 1.3895 (1.3703)  mae_loss: 0.2379 (0.2474)  classification_loss: 1.1310 (1.1228)  loss_mask: 0.0000 (0.0002)  time: 0.1980  data: 0.0003  max mem: 5511
[10:29:49.252990] Epoch: [94]  [280/781]  eta: 0:01:40  lr: 0.000003  training_loss: 1.3725 (1.3697)  mae_loss: 0.2565 (0.2482)  classification_loss: 1.1018 (1.1213)  loss_mask: 0.0000 (0.0002)  time: 0.1974  data: 0.0003  max mem: 5511
[10:29:53.271947] Epoch: [94]  [300/781]  eta: 0:01:36  lr: 0.000003  training_loss: 1.3448 (1.3687)  mae_loss: 0.2422 (0.2478)  classification_loss: 1.1205 (1.1208)  loss_mask: 0.0000 (0.0002)  time: 0.2009  data: 0.0002  max mem: 5511
[10:29:57.214205] Epoch: [94]  [320/781]  eta: 0:01:32  lr: 0.000003  training_loss: 1.3995 (1.3704)  mae_loss: 0.2502 (0.2482)  classification_loss: 1.1402 (1.1220)  loss_mask: 0.0000 (0.0002)  time: 0.1970  data: 0.0002  max mem: 5511
[10:30:01.192867] Epoch: [94]  [340/781]  eta: 0:01:28  lr: 0.000003  training_loss: 1.2852 (1.3663)  mae_loss: 0.2534 (0.2483)  classification_loss: 1.0236 (1.1178)  loss_mask: 0.0000 (0.0001)  time: 0.1988  data: 0.0002  max mem: 5511
[10:30:05.137185] Epoch: [94]  [360/781]  eta: 0:01:24  lr: 0.000003  training_loss: 1.3653 (1.3662)  mae_loss: 0.2416 (0.2483)  classification_loss: 1.1253 (1.1178)  loss_mask: 0.0000 (0.0001)  time: 0.1971  data: 0.0002  max mem: 5511
[10:30:09.106601] Epoch: [94]  [380/781]  eta: 0:01:20  lr: 0.000003  training_loss: 1.3654 (1.3668)  mae_loss: 0.2363 (0.2478)  classification_loss: 1.1207 (1.1189)  loss_mask: 0.0000 (0.0001)  time: 0.1984  data: 0.0003  max mem: 5511
[10:30:13.068818] Epoch: [94]  [400/781]  eta: 0:01:16  lr: 0.000003  training_loss: 1.3544 (1.3666)  mae_loss: 0.2510 (0.2481)  classification_loss: 1.0804 (1.1184)  loss_mask: 0.0000 (0.0001)  time: 0.1980  data: 0.0003  max mem: 5511
[10:30:17.018889] Epoch: [94]  [420/781]  eta: 0:01:12  lr: 0.000003  training_loss: 1.3366 (1.3655)  mae_loss: 0.2394 (0.2481)  classification_loss: 1.0984 (1.1173)  loss_mask: 0.0000 (0.0001)  time: 0.1974  data: 0.0002  max mem: 5511
[10:30:20.958177] Epoch: [94]  [440/781]  eta: 0:01:07  lr: 0.000003  training_loss: 1.3339 (1.3659)  mae_loss: 0.2391 (0.2479)  classification_loss: 1.1050 (1.1179)  loss_mask: 0.0000 (0.0001)  time: 0.1969  data: 0.0002  max mem: 5511
[10:30:24.917026] Epoch: [94]  [460/781]  eta: 0:01:03  lr: 0.000003  training_loss: 1.3099 (1.3647)  mae_loss: 0.2326 (0.2477)  classification_loss: 1.0502 (1.1168)  loss_mask: 0.0000 (0.0001)  time: 0.1978  data: 0.0002  max mem: 5511
[10:30:28.871112] Epoch: [94]  [480/781]  eta: 0:00:59  lr: 0.000003  training_loss: 1.3316 (1.3645)  mae_loss: 0.2432 (0.2476)  classification_loss: 1.0807 (1.1168)  loss_mask: 0.0000 (0.0001)  time: 0.1976  data: 0.0004  max mem: 5511
[10:30:32.873980] Epoch: [94]  [500/781]  eta: 0:00:55  lr: 0.000003  training_loss: 1.3558 (1.3643)  mae_loss: 0.2407 (0.2477)  classification_loss: 1.1040 (1.1166)  loss_mask: 0.0000 (0.0001)  time: 0.2001  data: 0.0002  max mem: 5511
[10:30:36.831166] Epoch: [94]  [520/781]  eta: 0:00:51  lr: 0.000003  training_loss: 1.3969 (1.3654)  mae_loss: 0.2556 (0.2479)  classification_loss: 1.1282 (1.1175)  loss_mask: 0.0000 (0.0001)  time: 0.1978  data: 0.0002  max mem: 5511
[10:30:40.798281] Epoch: [94]  [540/781]  eta: 0:00:47  lr: 0.000003  training_loss: 1.3681 (1.3652)  mae_loss: 0.2397 (0.2480)  classification_loss: 1.0949 (1.1171)  loss_mask: 0.0000 (0.0001)  time: 0.1982  data: 0.0002  max mem: 5511
[10:30:44.785273] Epoch: [94]  [560/781]  eta: 0:00:44  lr: 0.000003  training_loss: 1.3830 (1.3654)  mae_loss: 0.2342 (0.2478)  classification_loss: 1.1299 (1.1174)  loss_mask: 0.0000 (0.0001)  time: 0.1992  data: 0.0002  max mem: 5511
[10:30:48.741320] Epoch: [94]  [580/781]  eta: 0:00:40  lr: 0.000003  training_loss: 1.3587 (1.3659)  mae_loss: 0.2359 (0.2475)  classification_loss: 1.1110 (1.1183)  loss_mask: 0.0000 (0.0001)  time: 0.1977  data: 0.0003  max mem: 5511
[10:30:52.683464] Epoch: [94]  [600/781]  eta: 0:00:36  lr: 0.000003  training_loss: 1.3137 (1.3652)  mae_loss: 0.2427 (0.2477)  classification_loss: 1.0620 (1.1174)  loss_mask: 0.0000 (0.0001)  time: 0.1970  data: 0.0002  max mem: 5511
[10:30:56.648783] Epoch: [94]  [620/781]  eta: 0:00:32  lr: 0.000003  training_loss: 1.3734 (1.3652)  mae_loss: 0.2404 (0.2476)  classification_loss: 1.1193 (1.1175)  loss_mask: 0.0000 (0.0001)  time: 0.1982  data: 0.0002  max mem: 5511
[10:31:00.587114] Epoch: [94]  [640/781]  eta: 0:00:28  lr: 0.000003  training_loss: 1.3806 (1.3659)  mae_loss: 0.2512 (0.2477)  classification_loss: 1.1500 (1.1181)  loss_mask: 0.0000 (0.0001)  time: 0.1968  data: 0.0002  max mem: 5511
[10:31:04.547954] Epoch: [94]  [660/781]  eta: 0:00:24  lr: 0.000003  training_loss: 1.4161 (1.3669)  mae_loss: 0.2469 (0.2476)  classification_loss: 1.1647 (1.1193)  loss_mask: 0.0000 (0.0001)  time: 0.1980  data: 0.0004  max mem: 5511
[10:31:08.518358] Epoch: [94]  [680/781]  eta: 0:00:20  lr: 0.000003  training_loss: 1.3358 (1.3670)  mae_loss: 0.2526 (0.2477)  classification_loss: 1.1021 (1.1192)  loss_mask: 0.0000 (0.0001)  time: 0.1984  data: 0.0002  max mem: 5511
[10:31:12.485016] Epoch: [94]  [700/781]  eta: 0:00:16  lr: 0.000003  training_loss: 1.3589 (1.3676)  mae_loss: 0.2453 (0.2476)  classification_loss: 1.1304 (1.1199)  loss_mask: 0.0000 (0.0001)  time: 0.1982  data: 0.0003  max mem: 5511
[10:31:16.442739] Epoch: [94]  [720/781]  eta: 0:00:12  lr: 0.000003  training_loss: 1.3302 (1.3671)  mae_loss: 0.2401 (0.2474)  classification_loss: 1.0703 (1.1197)  loss_mask: 0.0000 (0.0001)  time: 0.1978  data: 0.0003  max mem: 5511
[10:31:20.409750] Epoch: [94]  [740/781]  eta: 0:00:08  lr: 0.000003  training_loss: 1.3069 (1.3659)  mae_loss: 0.2397 (0.2474)  classification_loss: 1.0745 (1.1185)  loss_mask: 0.0000 (0.0001)  time: 0.1982  data: 0.0002  max mem: 5511
[10:31:24.350777] Epoch: [94]  [760/781]  eta: 0:00:04  lr: 0.000003  training_loss: 1.4171 (1.3670)  mae_loss: 0.2659 (0.2477)  classification_loss: 1.1499 (1.1192)  loss_mask: 0.0000 (0.0001)  time: 0.1969  data: 0.0002  max mem: 5511
[10:31:28.308414] Epoch: [94]  [780/781]  eta: 0:00:00  lr: 0.000003  training_loss: 1.3793 (1.3672)  mae_loss: 0.2511 (0.2478)  classification_loss: 1.1195 (1.1193)  loss_mask: 0.0000 (0.0001)  time: 0.1978  data: 0.0002  max mem: 5511
[10:31:28.497075] Epoch: [94] Total time: 0:02:35 (0.1991 s / it)
[10:31:28.497862] Averaged stats: lr: 0.000003  training_loss: 1.3793 (1.3672)  mae_loss: 0.2511 (0.2478)  classification_loss: 1.1195 (1.1193)  loss_mask: 0.0000 (0.0001)
[10:31:29.230943] Test:  [  0/157]  eta: 0:01:54  testing_loss: 0.4325 (0.4325)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (100.0000)  time: 0.7274  data: 0.6976  max mem: 5511
[10:31:29.525296] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.4463 (0.4498)  acc1: 87.5000 (87.0739)  acc5: 100.0000 (99.7159)  time: 0.0927  data: 0.0639  max mem: 5511
[10:31:29.825019] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4211 (0.4320)  acc1: 87.5000 (87.4256)  acc5: 100.0000 (99.5536)  time: 0.0295  data: 0.0004  max mem: 5511
[10:31:30.110005] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4174 (0.4492)  acc1: 87.5000 (86.9456)  acc5: 100.0000 (99.4456)  time: 0.0291  data: 0.0002  max mem: 5511
[10:31:30.398971] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4345 (0.4524)  acc1: 85.9375 (86.8521)  acc5: 100.0000 (99.3521)  time: 0.0286  data: 0.0002  max mem: 5511
[10:31:30.682459] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4103 (0.4428)  acc1: 87.5000 (87.1630)  acc5: 100.0000 (99.3566)  time: 0.0285  data: 0.0002  max mem: 5511
[10:31:30.979869] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4058 (0.4377)  acc1: 87.5000 (87.3207)  acc5: 100.0000 (99.3596)  time: 0.0289  data: 0.0004  max mem: 5511
[10:31:31.268684] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4320 (0.4368)  acc1: 85.9375 (87.3019)  acc5: 100.0000 (99.3618)  time: 0.0292  data: 0.0004  max mem: 5511
[10:31:31.558022] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4456 (0.4456)  acc1: 85.9375 (87.0177)  acc5: 100.0000 (99.2863)  time: 0.0288  data: 0.0002  max mem: 5511
[10:31:31.846491] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4222 (0.4392)  acc1: 87.5000 (87.2940)  acc5: 98.4375 (99.2617)  time: 0.0287  data: 0.0002  max mem: 5511
[10:31:32.139225] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.3898 (0.4401)  acc1: 89.0625 (87.4072)  acc5: 100.0000 (99.3038)  time: 0.0289  data: 0.0003  max mem: 5511
[10:31:32.427304] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4366 (0.4419)  acc1: 87.5000 (87.3170)  acc5: 100.0000 (99.2821)  time: 0.0289  data: 0.0002  max mem: 5511
[10:31:32.712556] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4366 (0.4410)  acc1: 84.3750 (87.3580)  acc5: 100.0000 (99.2769)  time: 0.0285  data: 0.0002  max mem: 5511
[10:31:33.000657] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4179 (0.4409)  acc1: 84.3750 (87.3688)  acc5: 100.0000 (99.2963)  time: 0.0285  data: 0.0003  max mem: 5511
[10:31:33.286302] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4179 (0.4381)  acc1: 87.5000 (87.4446)  acc5: 100.0000 (99.3351)  time: 0.0285  data: 0.0003  max mem: 5511
[10:31:33.570245] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4119 (0.4365)  acc1: 87.5000 (87.4483)  acc5: 100.0000 (99.3481)  time: 0.0283  data: 0.0002  max mem: 5511
[10:31:33.724027] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4119 (0.4364)  acc1: 87.5000 (87.4100)  acc5: 100.0000 (99.3500)  time: 0.0274  data: 0.0002  max mem: 5511
[10:31:33.917150] Test: Total time: 0:00:05 (0.0345 s / it)
[10:31:33.917631] * Acc@1 87.410 Acc@5 99.350 loss 0.436
[10:31:33.917927] Accuracy of the network on the 10000 test images: 87.4%
[10:31:33.918128] Max accuracy: 87.44%
[10:31:34.245033] log_dir: ./output_dir
[10:31:35.155824] Epoch: [95]  [  0/781]  eta: 0:11:49  lr: 0.000003  training_loss: 1.2377 (1.2377)  mae_loss: 0.2149 (0.2149)  classification_loss: 1.0228 (1.0228)  loss_mask: 0.0000 (0.0000)  time: 0.9089  data: 0.6596  max mem: 5511
[10:31:39.134977] Epoch: [95]  [ 20/781]  eta: 0:02:57  lr: 0.000003  training_loss: 1.3323 (1.3341)  mae_loss: 0.2324 (0.2381)  classification_loss: 1.1057 (1.0960)  loss_mask: 0.0000 (0.0000)  time: 0.1989  data: 0.0002  max mem: 5511
[10:31:43.120144] Epoch: [95]  [ 40/781]  eta: 0:02:40  lr: 0.000003  training_loss: 1.3636 (1.3605)  mae_loss: 0.2391 (0.2404)  classification_loss: 1.1135 (1.1201)  loss_mask: 0.0000 (0.0000)  time: 0.1992  data: 0.0002  max mem: 5511
[10:31:47.069743] Epoch: [95]  [ 60/781]  eta: 0:02:31  lr: 0.000003  training_loss: 1.3844 (1.3647)  mae_loss: 0.2565 (0.2451)  classification_loss: 1.1166 (1.1196)  loss_mask: 0.0000 (0.0000)  time: 0.1974  data: 0.0002  max mem: 5511
[10:31:51.016660] Epoch: [95]  [ 80/781]  eta: 0:02:25  lr: 0.000003  training_loss: 1.3871 (1.3642)  mae_loss: 0.2357 (0.2441)  classification_loss: 1.1557 (1.1200)  loss_mask: 0.0000 (0.0000)  time: 0.1972  data: 0.0002  max mem: 5511
[10:31:54.980921] Epoch: [95]  [100/781]  eta: 0:02:19  lr: 0.000003  training_loss: 1.3586 (1.3635)  mae_loss: 0.2567 (0.2486)  classification_loss: 1.1019 (1.1149)  loss_mask: 0.0000 (0.0000)  time: 0.1981  data: 0.0002  max mem: 5511
[10:31:58.924617] Epoch: [95]  [120/781]  eta: 0:02:14  lr: 0.000003  training_loss: 1.3492 (1.3627)  mae_loss: 0.2485 (0.2483)  classification_loss: 1.0875 (1.1144)  loss_mask: 0.0000 (0.0000)  time: 0.1971  data: 0.0003  max mem: 5511
[10:32:02.917777] Epoch: [95]  [140/781]  eta: 0:02:10  lr: 0.000003  training_loss: 1.3917 (1.3656)  mae_loss: 0.2449 (0.2484)  classification_loss: 1.1087 (1.1172)  loss_mask: 0.0000 (0.0000)  time: 0.1996  data: 0.0003  max mem: 5511
[10:32:06.907563] Epoch: [95]  [160/781]  eta: 0:02:05  lr: 0.000003  training_loss: 1.3274 (1.3634)  mae_loss: 0.2439 (0.2485)  classification_loss: 1.0828 (1.1148)  loss_mask: 0.0000 (0.0000)  time: 0.1994  data: 0.0002  max mem: 5511
[10:32:10.864944] Epoch: [95]  [180/781]  eta: 0:02:01  lr: 0.000003  training_loss: 1.3827 (1.3627)  mae_loss: 0.2548 (0.2489)  classification_loss: 1.1111 (1.1138)  loss_mask: 0.0000 (0.0000)  time: 0.1978  data: 0.0002  max mem: 5511
[10:32:14.822794] Epoch: [95]  [200/781]  eta: 0:01:57  lr: 0.000003  training_loss: 1.3328 (1.3615)  mae_loss: 0.2486 (0.2489)  classification_loss: 1.1004 (1.1126)  loss_mask: 0.0000 (0.0000)  time: 0.1978  data: 0.0002  max mem: 5511
[10:32:18.785981] Epoch: [95]  [220/781]  eta: 0:01:53  lr: 0.000003  training_loss: 1.3362 (1.3603)  mae_loss: 0.2370 (0.2475)  classification_loss: 1.1149 (1.1128)  loss_mask: 0.0000 (0.0000)  time: 0.1981  data: 0.0004  max mem: 5511
[10:32:22.749380] Epoch: [95]  [240/781]  eta: 0:01:48  lr: 0.000002  training_loss: 1.3795 (1.3640)  mae_loss: 0.2329 (0.2473)  classification_loss: 1.1274 (1.1166)  loss_mask: 0.0000 (0.0000)  time: 0.1981  data: 0.0002  max mem: 5511
[10:32:26.703457] Epoch: [95]  [260/781]  eta: 0:01:44  lr: 0.000002  training_loss: 1.3674 (1.3631)  mae_loss: 0.2353 (0.2471)  classification_loss: 1.1348 (1.1160)  loss_mask: 0.0000 (0.0000)  time: 0.1976  data: 0.0002  max mem: 5511
[10:32:30.646206] Epoch: [95]  [280/781]  eta: 0:01:40  lr: 0.000002  training_loss: 1.3637 (1.3645)  mae_loss: 0.2318 (0.2473)  classification_loss: 1.1317 (1.1169)  loss_mask: 0.0000 (0.0003)  time: 0.1971  data: 0.0003  max mem: 5511
[10:32:34.602428] Epoch: [95]  [300/781]  eta: 0:01:36  lr: 0.000002  training_loss: 1.3383 (1.3650)  mae_loss: 0.2460 (0.2473)  classification_loss: 1.1095 (1.1174)  loss_mask: 0.0001 (0.0003)  time: 0.1977  data: 0.0003  max mem: 5511
[10:32:38.562689] Epoch: [95]  [320/781]  eta: 0:01:32  lr: 0.000002  training_loss: 1.3740 (1.3663)  mae_loss: 0.2407 (0.2473)  classification_loss: 1.1300 (1.1188)  loss_mask: 0.0001 (0.0003)  time: 0.1979  data: 0.0002  max mem: 5511
[10:32:42.529231] Epoch: [95]  [340/781]  eta: 0:01:28  lr: 0.000002  training_loss: 1.3566 (1.3645)  mae_loss: 0.2554 (0.2473)  classification_loss: 1.0897 (1.1169)  loss_mask: 0.0000 (0.0003)  time: 0.1982  data: 0.0003  max mem: 5511
[10:32:46.524466] Epoch: [95]  [360/781]  eta: 0:01:24  lr: 0.000002  training_loss: 1.3442 (1.3660)  mae_loss: 0.2497 (0.2475)  classification_loss: 1.1102 (1.1181)  loss_mask: 0.0000 (0.0003)  time: 0.1997  data: 0.0002  max mem: 5511
[10:32:50.468433] Epoch: [95]  [380/781]  eta: 0:01:20  lr: 0.000002  training_loss: 1.3148 (1.3650)  mae_loss: 0.2367 (0.2472)  classification_loss: 1.1070 (1.1176)  loss_mask: 0.0000 (0.0003)  time: 0.1971  data: 0.0002  max mem: 5511
[10:32:54.411000] Epoch: [95]  [400/781]  eta: 0:01:16  lr: 0.000002  training_loss: 1.3386 (1.3643)  mae_loss: 0.2438 (0.2474)  classification_loss: 1.0901 (1.1167)  loss_mask: 0.0000 (0.0003)  time: 0.1970  data: 0.0003  max mem: 5511
[10:32:58.393480] Epoch: [95]  [420/781]  eta: 0:01:12  lr: 0.000002  training_loss: 1.3586 (1.3637)  mae_loss: 0.2612 (0.2481)  classification_loss: 1.0800 (1.1154)  loss_mask: 0.0000 (0.0002)  time: 0.1991  data: 0.0003  max mem: 5511
[10:33:02.360878] Epoch: [95]  [440/781]  eta: 0:01:08  lr: 0.000002  training_loss: 1.3068 (1.3620)  mae_loss: 0.2384 (0.2478)  classification_loss: 1.0731 (1.1140)  loss_mask: 0.0000 (0.0002)  time: 0.1983  data: 0.0002  max mem: 5511
[10:33:06.323077] Epoch: [95]  [460/781]  eta: 0:01:04  lr: 0.000002  training_loss: 1.3822 (1.3618)  mae_loss: 0.2303 (0.2476)  classification_loss: 1.1285 (1.1140)  loss_mask: 0.0000 (0.0002)  time: 0.1980  data: 0.0002  max mem: 5511
[10:33:10.266067] Epoch: [95]  [480/781]  eta: 0:01:00  lr: 0.000002  training_loss: 1.3692 (1.3633)  mae_loss: 0.2646 (0.2481)  classification_loss: 1.1190 (1.1150)  loss_mask: 0.0000 (0.0002)  time: 0.1971  data: 0.0003  max mem: 5511
[10:33:14.206511] Epoch: [95]  [500/781]  eta: 0:00:56  lr: 0.000002  training_loss: 1.3255 (1.3631)  mae_loss: 0.2482 (0.2483)  classification_loss: 1.0825 (1.1145)  loss_mask: 0.0000 (0.0002)  time: 0.1969  data: 0.0002  max mem: 5511
[10:33:18.170564] Epoch: [95]  [520/781]  eta: 0:00:52  lr: 0.000002  training_loss: 1.3768 (1.3641)  mae_loss: 0.2464 (0.2480)  classification_loss: 1.1246 (1.1159)  loss_mask: 0.0000 (0.0002)  time: 0.1981  data: 0.0002  max mem: 5511
[10:33:22.105287] Epoch: [95]  [540/781]  eta: 0:00:48  lr: 0.000002  training_loss: 1.3958 (1.3652)  mae_loss: 0.2478 (0.2483)  classification_loss: 1.1455 (1.1168)  loss_mask: 0.0000 (0.0002)  time: 0.1966  data: 0.0003  max mem: 5511
[10:33:26.043671] Epoch: [95]  [560/781]  eta: 0:00:44  lr: 0.000002  training_loss: 1.3507 (1.3649)  mae_loss: 0.2429 (0.2482)  classification_loss: 1.0699 (1.1165)  loss_mask: 0.0000 (0.0002)  time: 0.1968  data: 0.0003  max mem: 5511
[10:33:29.996290] Epoch: [95]  [580/781]  eta: 0:00:40  lr: 0.000002  training_loss: 1.3410 (1.3646)  mae_loss: 0.2474 (0.2482)  classification_loss: 1.1077 (1.1163)  loss_mask: 0.0000 (0.0002)  time: 0.1975  data: 0.0002  max mem: 5511
[10:33:33.979649] Epoch: [95]  [600/781]  eta: 0:00:36  lr: 0.000002  training_loss: 1.3503 (1.3646)  mae_loss: 0.2489 (0.2482)  classification_loss: 1.0997 (1.1163)  loss_mask: 0.0000 (0.0002)  time: 0.1991  data: 0.0004  max mem: 5511
[10:33:37.938867] Epoch: [95]  [620/781]  eta: 0:00:32  lr: 0.000002  training_loss: 1.3379 (1.3640)  mae_loss: 0.2401 (0.2483)  classification_loss: 1.0789 (1.1156)  loss_mask: 0.0000 (0.0002)  time: 0.1979  data: 0.0002  max mem: 5511
[10:33:41.885629] Epoch: [95]  [640/781]  eta: 0:00:28  lr: 0.000002  training_loss: 1.3538 (1.3642)  mae_loss: 0.2493 (0.2483)  classification_loss: 1.1014 (1.1157)  loss_mask: 0.0000 (0.0002)  time: 0.1973  data: 0.0002  max mem: 5511
[10:33:45.844513] Epoch: [95]  [660/781]  eta: 0:00:24  lr: 0.000002  training_loss: 1.3482 (1.3644)  mae_loss: 0.2443 (0.2485)  classification_loss: 1.0959 (1.1158)  loss_mask: 0.0000 (0.0002)  time: 0.1979  data: 0.0002  max mem: 5511
[10:33:49.810761] Epoch: [95]  [680/781]  eta: 0:00:20  lr: 0.000002  training_loss: 1.4058 (1.3647)  mae_loss: 0.2462 (0.2486)  classification_loss: 1.1513 (1.1159)  loss_mask: 0.0000 (0.0002)  time: 0.1982  data: 0.0002  max mem: 5511
[10:33:53.774815] Epoch: [95]  [700/781]  eta: 0:00:16  lr: 0.000002  training_loss: 1.3593 (1.3647)  mae_loss: 0.2447 (0.2486)  classification_loss: 1.1022 (1.1159)  loss_mask: 0.0000 (0.0002)  time: 0.1981  data: 0.0002  max mem: 5511
[10:33:57.728083] Epoch: [95]  [720/781]  eta: 0:00:12  lr: 0.000002  training_loss: 1.3305 (1.3638)  mae_loss: 0.2397 (0.2485)  classification_loss: 1.0867 (1.1152)  loss_mask: 0.0000 (0.0002)  time: 0.1976  data: 0.0002  max mem: 5511
[10:34:01.686683] Epoch: [95]  [740/781]  eta: 0:00:08  lr: 0.000002  training_loss: 1.3312 (1.3629)  mae_loss: 0.2401 (0.2483)  classification_loss: 1.0826 (1.1144)  loss_mask: 0.0000 (0.0002)  time: 0.1978  data: 0.0002  max mem: 5511
[10:34:05.707389] Epoch: [95]  [760/781]  eta: 0:00:04  lr: 0.000002  training_loss: 1.3624 (1.3636)  mae_loss: 0.2387 (0.2481)  classification_loss: 1.1277 (1.1154)  loss_mask: 0.0000 (0.0001)  time: 0.2009  data: 0.0002  max mem: 5511
[10:34:09.643244] Epoch: [95]  [780/781]  eta: 0:00:00  lr: 0.000002  training_loss: 1.3608 (1.3640)  mae_loss: 0.2451 (0.2481)  classification_loss: 1.1177 (1.1158)  loss_mask: 0.0000 (0.0001)  time: 0.1967  data: 0.0002  max mem: 5511
[10:34:09.821260] Epoch: [95] Total time: 0:02:35 (0.1992 s / it)
[10:34:09.821922] Averaged stats: lr: 0.000002  training_loss: 1.3608 (1.3640)  mae_loss: 0.2451 (0.2481)  classification_loss: 1.1177 (1.1158)  loss_mask: 0.0000 (0.0001)
[10:34:10.558744] Test:  [  0/157]  eta: 0:01:54  testing_loss: 0.4332 (0.4332)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (100.0000)  time: 0.7308  data: 0.6699  max mem: 5511
[10:34:10.847654] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.4437 (0.4501)  acc1: 89.0625 (87.6420)  acc5: 100.0000 (99.7159)  time: 0.0924  data: 0.0611  max mem: 5511
[10:34:11.134656] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4239 (0.4314)  acc1: 89.0625 (87.7976)  acc5: 100.0000 (99.5536)  time: 0.0286  data: 0.0002  max mem: 5511
[10:34:11.425969] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4116 (0.4484)  acc1: 87.5000 (87.2984)  acc5: 100.0000 (99.4456)  time: 0.0288  data: 0.0003  max mem: 5511
[10:34:11.713933] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4231 (0.4518)  acc1: 87.5000 (87.1951)  acc5: 100.0000 (99.3140)  time: 0.0288  data: 0.0003  max mem: 5511
[10:34:11.997927] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4117 (0.4416)  acc1: 87.5000 (87.5919)  acc5: 100.0000 (99.2953)  time: 0.0285  data: 0.0002  max mem: 5511
[10:34:12.282710] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4016 (0.4361)  acc1: 89.0625 (87.6537)  acc5: 100.0000 (99.3084)  time: 0.0283  data: 0.0002  max mem: 5511
[10:34:12.573043] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4258 (0.4351)  acc1: 87.5000 (87.6540)  acc5: 100.0000 (99.3178)  time: 0.0286  data: 0.0002  max mem: 5511
[10:34:12.859990] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4422 (0.4437)  acc1: 85.9375 (87.3843)  acc5: 100.0000 (99.2670)  time: 0.0287  data: 0.0002  max mem: 5511
[10:34:13.145131] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4269 (0.4370)  acc1: 87.5000 (87.6374)  acc5: 100.0000 (99.2445)  time: 0.0285  data: 0.0002  max mem: 5511
[10:34:13.430249] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.3910 (0.4381)  acc1: 89.0625 (87.7166)  acc5: 100.0000 (99.2884)  time: 0.0284  data: 0.0002  max mem: 5511
[10:34:13.715326] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4376 (0.4403)  acc1: 87.5000 (87.6267)  acc5: 100.0000 (99.2821)  time: 0.0284  data: 0.0002  max mem: 5511
[10:34:13.999671] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4376 (0.4394)  acc1: 85.9375 (87.6420)  acc5: 100.0000 (99.2769)  time: 0.0283  data: 0.0002  max mem: 5511
[10:34:14.285440] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4181 (0.4393)  acc1: 85.9375 (87.5716)  acc5: 100.0000 (99.2963)  time: 0.0284  data: 0.0002  max mem: 5511
[10:34:14.568651] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4181 (0.4367)  acc1: 87.5000 (87.6551)  acc5: 100.0000 (99.3351)  time: 0.0283  data: 0.0002  max mem: 5511
[10:34:14.851199] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4175 (0.4349)  acc1: 87.5000 (87.6449)  acc5: 100.0000 (99.3481)  time: 0.0282  data: 0.0001  max mem: 5511
[10:34:15.001751] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4156 (0.4347)  acc1: 87.5000 (87.6200)  acc5: 100.0000 (99.3500)  time: 0.0272  data: 0.0001  max mem: 5511
[10:34:15.159032] Test: Total time: 0:00:05 (0.0340 s / it)
[10:34:15.160305] * Acc@1 87.620 Acc@5 99.350 loss 0.435
[10:34:15.161347] Accuracy of the network on the 10000 test images: 87.6%
[10:34:15.161798] Max accuracy: 87.62%
[10:34:15.551032] log_dir: ./output_dir
[10:34:16.357203] Epoch: [96]  [  0/781]  eta: 0:10:27  lr: 0.000002  training_loss: 1.1536 (1.1536)  mae_loss: 0.2601 (0.2601)  classification_loss: 0.8935 (0.8935)  loss_mask: 0.0000 (0.0000)  time: 0.8036  data: 0.5987  max mem: 5511
[10:34:20.317150] Epoch: [96]  [ 20/781]  eta: 0:02:52  lr: 0.000002  training_loss: 1.3469 (1.3338)  mae_loss: 0.2343 (0.2444)  classification_loss: 1.0994 (1.0894)  loss_mask: 0.0000 (0.0000)  time: 0.1979  data: 0.0003  max mem: 5511
[10:34:24.265670] Epoch: [96]  [ 40/781]  eta: 0:02:37  lr: 0.000002  training_loss: 1.3773 (1.3523)  mae_loss: 0.2467 (0.2481)  classification_loss: 1.1002 (1.1042)  loss_mask: 0.0000 (0.0000)  time: 0.1973  data: 0.0003  max mem: 5511
[10:34:28.205470] Epoch: [96]  [ 60/781]  eta: 0:02:29  lr: 0.000002  training_loss: 1.3930 (1.3660)  mae_loss: 0.2360 (0.2470)  classification_loss: 1.1130 (1.1190)  loss_mask: 0.0000 (0.0000)  time: 0.1969  data: 0.0003  max mem: 5511
[10:34:32.150034] Epoch: [96]  [ 80/781]  eta: 0:02:23  lr: 0.000002  training_loss: 1.3618 (1.3688)  mae_loss: 0.2478 (0.2474)  classification_loss: 1.1145 (1.1214)  loss_mask: 0.0000 (0.0000)  time: 0.1971  data: 0.0003  max mem: 5511
[10:34:36.106980] Epoch: [96]  [100/781]  eta: 0:02:18  lr: 0.000002  training_loss: 1.3605 (1.3655)  mae_loss: 0.2549 (0.2481)  classification_loss: 1.1015 (1.1173)  loss_mask: 0.0000 (0.0000)  time: 0.1978  data: 0.0002  max mem: 5511
[10:34:40.031194] Epoch: [96]  [120/781]  eta: 0:02:13  lr: 0.000002  training_loss: 1.3597 (1.3630)  mae_loss: 0.2521 (0.2469)  classification_loss: 1.1198 (1.1150)  loss_mask: 0.0000 (0.0011)  time: 0.1961  data: 0.0003  max mem: 5511
[10:34:43.966437] Epoch: [96]  [140/781]  eta: 0:02:09  lr: 0.000002  training_loss: 1.2826 (1.3565)  mae_loss: 0.2361 (0.2468)  classification_loss: 1.0579 (1.1088)  loss_mask: 0.0000 (0.0009)  time: 0.1967  data: 0.0002  max mem: 5511
[10:34:47.925979] Epoch: [96]  [160/781]  eta: 0:02:04  lr: 0.000002  training_loss: 1.3428 (1.3562)  mae_loss: 0.2432 (0.2474)  classification_loss: 1.0667 (1.1080)  loss_mask: 0.0000 (0.0008)  time: 0.1979  data: 0.0002  max mem: 5511
[10:34:51.874900] Epoch: [96]  [180/781]  eta: 0:02:00  lr: 0.000002  training_loss: 1.3357 (1.3573)  mae_loss: 0.2409 (0.2473)  classification_loss: 1.0867 (1.1092)  loss_mask: 0.0000 (0.0007)  time: 0.1974  data: 0.0002  max mem: 5511
[10:34:55.837301] Epoch: [96]  [200/781]  eta: 0:01:56  lr: 0.000002  training_loss: 1.3535 (1.3614)  mae_loss: 0.2494 (0.2483)  classification_loss: 1.1025 (1.1124)  loss_mask: 0.0000 (0.0007)  time: 0.1980  data: 0.0002  max mem: 5511
[10:34:59.763397] Epoch: [96]  [220/781]  eta: 0:01:52  lr: 0.000002  training_loss: 1.3386 (1.3623)  mae_loss: 0.2385 (0.2479)  classification_loss: 1.0822 (1.1138)  loss_mask: 0.0000 (0.0006)  time: 0.1962  data: 0.0002  max mem: 5511
[10:35:03.706085] Epoch: [96]  [240/781]  eta: 0:01:48  lr: 0.000002  training_loss: 1.4058 (1.3653)  mae_loss: 0.2492 (0.2482)  classification_loss: 1.1444 (1.1165)  loss_mask: 0.0000 (0.0005)  time: 0.1970  data: 0.0002  max mem: 5511
[10:35:07.659355] Epoch: [96]  [260/781]  eta: 0:01:43  lr: 0.000002  training_loss: 1.3643 (1.3660)  mae_loss: 0.2418 (0.2485)  classification_loss: 1.1058 (1.1170)  loss_mask: 0.0000 (0.0005)  time: 0.1976  data: 0.0003  max mem: 5511
[10:35:11.643495] Epoch: [96]  [280/781]  eta: 0:01:39  lr: 0.000002  training_loss: 1.3933 (1.3671)  mae_loss: 0.2423 (0.2486)  classification_loss: 1.1561 (1.1181)  loss_mask: 0.0000 (0.0005)  time: 0.1991  data: 0.0002  max mem: 5511
[10:35:15.621850] Epoch: [96]  [300/781]  eta: 0:01:35  lr: 0.000002  training_loss: 1.3579 (1.3676)  mae_loss: 0.2376 (0.2479)  classification_loss: 1.1266 (1.1193)  loss_mask: 0.0000 (0.0004)  time: 0.1988  data: 0.0003  max mem: 5511
[10:35:19.566605] Epoch: [96]  [320/781]  eta: 0:01:31  lr: 0.000002  training_loss: 1.4130 (1.3697)  mae_loss: 0.2453 (0.2482)  classification_loss: 1.1389 (1.1212)  loss_mask: 0.0000 (0.0004)  time: 0.1972  data: 0.0002  max mem: 5511
[10:35:23.540886] Epoch: [96]  [340/781]  eta: 0:01:27  lr: 0.000002  training_loss: 1.3166 (1.3683)  mae_loss: 0.2330 (0.2483)  classification_loss: 1.0879 (1.1196)  loss_mask: 0.0000 (0.0004)  time: 0.1986  data: 0.0005  max mem: 5511
[10:35:27.488520] Epoch: [96]  [360/781]  eta: 0:01:23  lr: 0.000002  training_loss: 1.3398 (1.3678)  mae_loss: 0.2537 (0.2487)  classification_loss: 1.1103 (1.1188)  loss_mask: 0.0000 (0.0004)  time: 0.1973  data: 0.0002  max mem: 5511
[10:35:31.458870] Epoch: [96]  [380/781]  eta: 0:01:19  lr: 0.000002  training_loss: 1.3726 (1.3676)  mae_loss: 0.2563 (0.2491)  classification_loss: 1.1073 (1.1182)  loss_mask: 0.0000 (0.0004)  time: 0.1984  data: 0.0002  max mem: 5511
[10:35:35.403891] Epoch: [96]  [400/781]  eta: 0:01:15  lr: 0.000002  training_loss: 1.3426 (1.3681)  mae_loss: 0.2546 (0.2495)  classification_loss: 1.1075 (1.1183)  loss_mask: 0.0000 (0.0003)  time: 0.1972  data: 0.0002  max mem: 5511
[10:35:39.354301] Epoch: [96]  [420/781]  eta: 0:01:11  lr: 0.000002  training_loss: 1.3396 (1.3677)  mae_loss: 0.2372 (0.2492)  classification_loss: 1.1124 (1.1182)  loss_mask: 0.0000 (0.0003)  time: 0.1974  data: 0.0002  max mem: 5511
[10:35:43.309047] Epoch: [96]  [440/781]  eta: 0:01:07  lr: 0.000002  training_loss: 1.3097 (1.3667)  mae_loss: 0.2350 (0.2490)  classification_loss: 1.0729 (1.1174)  loss_mask: 0.0000 (0.0003)  time: 0.1976  data: 0.0002  max mem: 5511
[10:35:47.273396] Epoch: [96]  [460/781]  eta: 0:01:03  lr: 0.000002  training_loss: 1.3918 (1.3663)  mae_loss: 0.2435 (0.2489)  classification_loss: 1.1214 (1.1171)  loss_mask: 0.0000 (0.0003)  time: 0.1981  data: 0.0003  max mem: 5511
[10:35:51.228021] Epoch: [96]  [480/781]  eta: 0:00:59  lr: 0.000002  training_loss: 1.3349 (1.3654)  mae_loss: 0.2480 (0.2488)  classification_loss: 1.1052 (1.1163)  loss_mask: 0.0000 (0.0003)  time: 0.1976  data: 0.0002  max mem: 5511
[10:35:55.179693] Epoch: [96]  [500/781]  eta: 0:00:55  lr: 0.000002  training_loss: 1.3698 (1.3655)  mae_loss: 0.2435 (0.2489)  classification_loss: 1.1354 (1.1163)  loss_mask: 0.0000 (0.0003)  time: 0.1975  data: 0.0003  max mem: 5511
[10:35:59.185026] Epoch: [96]  [520/781]  eta: 0:00:51  lr: 0.000002  training_loss: 1.3768 (1.3666)  mae_loss: 0.2457 (0.2489)  classification_loss: 1.1291 (1.1175)  loss_mask: 0.0000 (0.0003)  time: 0.2002  data: 0.0002  max mem: 5511
[10:36:03.162309] Epoch: [96]  [540/781]  eta: 0:00:47  lr: 0.000002  training_loss: 1.3586 (1.3665)  mae_loss: 0.2414 (0.2487)  classification_loss: 1.0903 (1.1176)  loss_mask: 0.0000 (0.0003)  time: 0.1988  data: 0.0002  max mem: 5511
[10:36:07.152096] Epoch: [96]  [560/781]  eta: 0:00:43  lr: 0.000002  training_loss: 1.3226 (1.3657)  mae_loss: 0.2405 (0.2484)  classification_loss: 1.0870 (1.1170)  loss_mask: 0.0000 (0.0003)  time: 0.1994  data: 0.0003  max mem: 5511
[10:36:11.104669] Epoch: [96]  [580/781]  eta: 0:00:39  lr: 0.000002  training_loss: 1.3454 (1.3650)  mae_loss: 0.2287 (0.2482)  classification_loss: 1.0830 (1.1166)  loss_mask: 0.0000 (0.0002)  time: 0.1976  data: 0.0003  max mem: 5511
[10:36:15.053249] Epoch: [96]  [600/781]  eta: 0:00:35  lr: 0.000002  training_loss: 1.3454 (1.3639)  mae_loss: 0.2421 (0.2483)  classification_loss: 1.0817 (1.1154)  loss_mask: 0.0000 (0.0002)  time: 0.1973  data: 0.0002  max mem: 5511
[10:36:19.024898] Epoch: [96]  [620/781]  eta: 0:00:31  lr: 0.000002  training_loss: 1.3563 (1.3640)  mae_loss: 0.2447 (0.2483)  classification_loss: 1.0972 (1.1155)  loss_mask: 0.0000 (0.0002)  time: 0.1985  data: 0.0002  max mem: 5511
[10:36:22.997200] Epoch: [96]  [640/781]  eta: 0:00:28  lr: 0.000002  training_loss: 1.3510 (1.3647)  mae_loss: 0.2394 (0.2481)  classification_loss: 1.1245 (1.1164)  loss_mask: 0.0000 (0.0002)  time: 0.1985  data: 0.0003  max mem: 5511
[10:36:26.958982] Epoch: [96]  [660/781]  eta: 0:00:24  lr: 0.000002  training_loss: 1.3496 (1.3654)  mae_loss: 0.2376 (0.2481)  classification_loss: 1.1205 (1.1171)  loss_mask: 0.0000 (0.0002)  time: 0.1980  data: 0.0002  max mem: 5511
[10:36:30.904800] Epoch: [96]  [680/781]  eta: 0:00:20  lr: 0.000002  training_loss: 1.3833 (1.3663)  mae_loss: 0.2387 (0.2480)  classification_loss: 1.1488 (1.1182)  loss_mask: 0.0000 (0.0002)  time: 0.1972  data: 0.0002  max mem: 5511
[10:36:34.843367] Epoch: [96]  [700/781]  eta: 0:00:16  lr: 0.000002  training_loss: 1.3308 (1.3663)  mae_loss: 0.2424 (0.2479)  classification_loss: 1.0761 (1.1182)  loss_mask: 0.0000 (0.0002)  time: 0.1968  data: 0.0002  max mem: 5511
[10:36:38.798149] Epoch: [96]  [720/781]  eta: 0:00:12  lr: 0.000002  training_loss: 1.3832 (1.3669)  mae_loss: 0.2480 (0.2479)  classification_loss: 1.1608 (1.1188)  loss_mask: 0.0000 (0.0002)  time: 0.1977  data: 0.0002  max mem: 5511
[10:36:42.734512] Epoch: [96]  [740/781]  eta: 0:00:08  lr: 0.000002  training_loss: 1.3406 (1.3662)  mae_loss: 0.2440 (0.2478)  classification_loss: 1.1037 (1.1182)  loss_mask: 0.0000 (0.0002)  time: 0.1967  data: 0.0003  max mem: 5511
[10:36:46.699940] Epoch: [96]  [760/781]  eta: 0:00:04  lr: 0.000002  training_loss: 1.3951 (1.3670)  mae_loss: 0.2541 (0.2478)  classification_loss: 1.1549 (1.1190)  loss_mask: 0.0000 (0.0002)  time: 0.1982  data: 0.0002  max mem: 5511
[10:36:50.722419] Epoch: [96]  [780/781]  eta: 0:00:00  lr: 0.000002  training_loss: 1.3721 (1.3672)  mae_loss: 0.2428 (0.2477)  classification_loss: 1.1266 (1.1193)  loss_mask: 0.0000 (0.0002)  time: 0.2010  data: 0.0002  max mem: 5511
[10:36:50.885150] Epoch: [96] Total time: 0:02:35 (0.1989 s / it)
[10:36:50.885622] Averaged stats: lr: 0.000002  training_loss: 1.3721 (1.3672)  mae_loss: 0.2428 (0.2477)  classification_loss: 1.1266 (1.1193)  loss_mask: 0.0000 (0.0002)
[10:36:51.490591] Test:  [  0/157]  eta: 0:01:34  testing_loss: 0.4348 (0.4348)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.5995  data: 0.5701  max mem: 5511
[10:36:51.781381] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.4404 (0.4525)  acc1: 87.5000 (87.0739)  acc5: 100.0000 (99.5739)  time: 0.0808  data: 0.0525  max mem: 5511
[10:36:52.069017] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4208 (0.4327)  acc1: 89.0625 (87.6488)  acc5: 100.0000 (99.4792)  time: 0.0288  data: 0.0005  max mem: 5511
[10:36:52.368998] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4118 (0.4488)  acc1: 87.5000 (86.9456)  acc5: 100.0000 (99.4456)  time: 0.0292  data: 0.0003  max mem: 5511
[10:36:52.660328] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4266 (0.4524)  acc1: 85.9375 (86.8521)  acc5: 100.0000 (99.3140)  time: 0.0294  data: 0.0003  max mem: 5511
[10:36:52.951730] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4071 (0.4426)  acc1: 87.5000 (87.2243)  acc5: 100.0000 (99.2953)  time: 0.0290  data: 0.0002  max mem: 5511
[10:36:53.243582] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.3994 (0.4373)  acc1: 87.5000 (87.2951)  acc5: 100.0000 (99.3084)  time: 0.0290  data: 0.0002  max mem: 5511
[10:36:53.530837] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4286 (0.4366)  acc1: 87.5000 (87.3239)  acc5: 100.0000 (99.2738)  time: 0.0288  data: 0.0003  max mem: 5511
[10:36:53.819840] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4420 (0.4450)  acc1: 85.9375 (87.0370)  acc5: 100.0000 (99.2284)  time: 0.0287  data: 0.0003  max mem: 5511
[10:36:54.107615] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4323 (0.4385)  acc1: 89.0625 (87.4141)  acc5: 100.0000 (99.2102)  time: 0.0287  data: 0.0002  max mem: 5511
[10:36:54.394429] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.3941 (0.4398)  acc1: 89.0625 (87.5000)  acc5: 100.0000 (99.2574)  time: 0.0286  data: 0.0002  max mem: 5511
[10:36:54.679321] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4475 (0.4423)  acc1: 87.5000 (87.4437)  acc5: 100.0000 (99.2399)  time: 0.0285  data: 0.0002  max mem: 5511
[10:36:54.964068] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4475 (0.4415)  acc1: 85.9375 (87.4613)  acc5: 100.0000 (99.2510)  time: 0.0284  data: 0.0002  max mem: 5511
[10:36:55.250379] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4144 (0.4414)  acc1: 85.9375 (87.3807)  acc5: 100.0000 (99.2724)  time: 0.0284  data: 0.0002  max mem: 5511
[10:36:55.535802] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4144 (0.4389)  acc1: 87.5000 (87.4557)  acc5: 100.0000 (99.3129)  time: 0.0284  data: 0.0002  max mem: 5511
[10:36:55.819188] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4214 (0.4373)  acc1: 87.5000 (87.4379)  acc5: 100.0000 (99.3274)  time: 0.0283  data: 0.0001  max mem: 5511
[10:36:55.971759] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4151 (0.4372)  acc1: 87.5000 (87.3900)  acc5: 100.0000 (99.3300)  time: 0.0273  data: 0.0001  max mem: 5511
[10:36:56.145870] Test: Total time: 0:00:05 (0.0335 s / it)
[10:36:56.146484] * Acc@1 87.390 Acc@5 99.330 loss 0.437
[10:36:56.146873] Accuracy of the network on the 10000 test images: 87.4%
[10:36:56.147130] Max accuracy: 87.62%
[10:36:56.493841] log_dir: ./output_dir
[10:36:57.447743] Epoch: [97]  [  0/781]  eta: 0:12:23  lr: 0.000002  training_loss: 1.1594 (1.1594)  mae_loss: 0.2230 (0.2230)  classification_loss: 0.9363 (0.9363)  loss_mask: 0.0000 (0.0000)  time: 0.9519  data: 0.7207  max mem: 5511
[10:37:01.393085] Epoch: [97]  [ 20/781]  eta: 0:02:57  lr: 0.000002  training_loss: 1.3137 (1.3258)  mae_loss: 0.2477 (0.2467)  classification_loss: 1.0658 (1.0791)  loss_mask: 0.0000 (0.0000)  time: 0.1972  data: 0.0002  max mem: 5511
[10:37:05.342552] Epoch: [97]  [ 40/781]  eta: 0:02:39  lr: 0.000002  training_loss: 1.3822 (1.3553)  mae_loss: 0.2328 (0.2429)  classification_loss: 1.1467 (1.1124)  loss_mask: 0.0000 (0.0000)  time: 0.1974  data: 0.0002  max mem: 5511
[10:37:09.362962] Epoch: [97]  [ 60/781]  eta: 0:02:32  lr: 0.000002  training_loss: 1.3898 (1.3639)  mae_loss: 0.2616 (0.2484)  classification_loss: 1.1142 (1.1154)  loss_mask: 0.0000 (0.0000)  time: 0.2009  data: 0.0003  max mem: 5511
[10:37:13.335282] Epoch: [97]  [ 80/781]  eta: 0:02:25  lr: 0.000002  training_loss: 1.3418 (1.3645)  mae_loss: 0.2417 (0.2471)  classification_loss: 1.1071 (1.1174)  loss_mask: 0.0000 (0.0000)  time: 0.1985  data: 0.0005  max mem: 5511
[10:37:17.298034] Epoch: [97]  [100/781]  eta: 0:02:20  lr: 0.000002  training_loss: 1.3285 (1.3604)  mae_loss: 0.2363 (0.2463)  classification_loss: 1.0902 (1.1140)  loss_mask: 0.0000 (0.0000)  time: 0.1980  data: 0.0003  max mem: 5511
[10:37:21.281950] Epoch: [97]  [120/781]  eta: 0:02:15  lr: 0.000002  training_loss: 1.3119 (1.3555)  mae_loss: 0.2362 (0.2468)  classification_loss: 1.0580 (1.1086)  loss_mask: 0.0000 (0.0000)  time: 0.1991  data: 0.0003  max mem: 5511
[10:37:25.223452] Epoch: [97]  [140/781]  eta: 0:02:10  lr: 0.000002  training_loss: 1.3245 (1.3527)  mae_loss: 0.2333 (0.2454)  classification_loss: 1.0892 (1.1073)  loss_mask: 0.0000 (0.0000)  time: 0.1970  data: 0.0002  max mem: 5511
[10:37:29.195937] Epoch: [97]  [160/781]  eta: 0:02:06  lr: 0.000002  training_loss: 1.3733 (1.3533)  mae_loss: 0.2533 (0.2463)  classification_loss: 1.1127 (1.1070)  loss_mask: 0.0000 (0.0000)  time: 0.1985  data: 0.0002  max mem: 5511
[10:37:33.143942] Epoch: [97]  [180/781]  eta: 0:02:01  lr: 0.000002  training_loss: 1.3528 (1.3551)  mae_loss: 0.2496 (0.2474)  classification_loss: 1.0885 (1.1077)  loss_mask: 0.0000 (0.0000)  time: 0.1973  data: 0.0003  max mem: 5511
[10:37:37.084315] Epoch: [97]  [200/781]  eta: 0:01:57  lr: 0.000002  training_loss: 1.3022 (1.3518)  mae_loss: 0.2364 (0.2462)  classification_loss: 1.0846 (1.1056)  loss_mask: 0.0000 (0.0000)  time: 0.1969  data: 0.0002  max mem: 5511
[10:37:41.038543] Epoch: [97]  [220/781]  eta: 0:01:53  lr: 0.000002  training_loss: 1.3954 (1.3549)  mae_loss: 0.2541 (0.2471)  classification_loss: 1.1312 (1.1078)  loss_mask: 0.0000 (0.0000)  time: 0.1976  data: 0.0003  max mem: 5511
[10:37:45.013917] Epoch: [97]  [240/781]  eta: 0:01:48  lr: 0.000001  training_loss: 1.3350 (1.3562)  mae_loss: 0.2445 (0.2473)  classification_loss: 1.0923 (1.1089)  loss_mask: 0.0000 (0.0000)  time: 0.1987  data: 0.0002  max mem: 5511
[10:37:48.968115] Epoch: [97]  [260/781]  eta: 0:01:44  lr: 0.000001  training_loss: 1.3460 (1.3577)  mae_loss: 0.2453 (0.2470)  classification_loss: 1.1210 (1.1107)  loss_mask: 0.0000 (0.0000)  time: 0.1976  data: 0.0003  max mem: 5511
[10:37:52.911053] Epoch: [97]  [280/781]  eta: 0:01:40  lr: 0.000001  training_loss: 1.3673 (1.3581)  mae_loss: 0.2460 (0.2470)  classification_loss: 1.1261 (1.1106)  loss_mask: 0.0000 (0.0005)  time: 0.1971  data: 0.0002  max mem: 5511
[10:37:56.871220] Epoch: [97]  [300/781]  eta: 0:01:36  lr: 0.000001  training_loss: 1.3296 (1.3571)  mae_loss: 0.2430 (0.2470)  classification_loss: 1.0921 (1.1096)  loss_mask: 0.0000 (0.0005)  time: 0.1979  data: 0.0002  max mem: 5511
[10:38:00.823436] Epoch: [97]  [320/781]  eta: 0:01:32  lr: 0.000001  training_loss: 1.3894 (1.3588)  mae_loss: 0.2609 (0.2477)  classification_loss: 1.1323 (1.1106)  loss_mask: 0.0000 (0.0004)  time: 0.1975  data: 0.0002  max mem: 5511
[10:38:04.759439] Epoch: [97]  [340/781]  eta: 0:01:28  lr: 0.000001  training_loss: 1.2773 (1.3552)  mae_loss: 0.2458 (0.2479)  classification_loss: 1.0249 (1.1069)  loss_mask: 0.0000 (0.0004)  time: 0.1967  data: 0.0002  max mem: 5511
[10:38:08.701364] Epoch: [97]  [360/781]  eta: 0:01:24  lr: 0.000001  training_loss: 1.3384 (1.3542)  mae_loss: 0.2328 (0.2475)  classification_loss: 1.0948 (1.1064)  loss_mask: 0.0000 (0.0004)  time: 0.1970  data: 0.0002  max mem: 5511
[10:38:12.655137] Epoch: [97]  [380/781]  eta: 0:01:20  lr: 0.000001  training_loss: 1.3242 (1.3535)  mae_loss: 0.2210 (0.2470)  classification_loss: 1.1004 (1.1061)  loss_mask: 0.0000 (0.0004)  time: 0.1976  data: 0.0002  max mem: 5511
[10:38:16.634653] Epoch: [97]  [400/781]  eta: 0:01:16  lr: 0.000001  training_loss: 1.3453 (1.3523)  mae_loss: 0.2368 (0.2467)  classification_loss: 1.0761 (1.1052)  loss_mask: 0.0000 (0.0004)  time: 0.1989  data: 0.0002  max mem: 5511
[10:38:20.618426] Epoch: [97]  [420/781]  eta: 0:01:12  lr: 0.000001  training_loss: 1.3815 (1.3536)  mae_loss: 0.2385 (0.2466)  classification_loss: 1.1381 (1.1066)  loss_mask: 0.0000 (0.0004)  time: 0.1991  data: 0.0002  max mem: 5511
[10:38:24.574216] Epoch: [97]  [440/781]  eta: 0:01:08  lr: 0.000001  training_loss: 1.3802 (1.3539)  mae_loss: 0.2478 (0.2472)  classification_loss: 1.1251 (1.1064)  loss_mask: 0.0000 (0.0003)  time: 0.1977  data: 0.0002  max mem: 5511
[10:38:28.553000] Epoch: [97]  [460/781]  eta: 0:01:04  lr: 0.000001  training_loss: 1.3089 (1.3524)  mae_loss: 0.2326 (0.2469)  classification_loss: 1.0878 (1.1052)  loss_mask: 0.0000 (0.0003)  time: 0.1989  data: 0.0002  max mem: 5511
[10:38:32.536658] Epoch: [97]  [480/781]  eta: 0:01:00  lr: 0.000001  training_loss: 1.3361 (1.3521)  mae_loss: 0.2378 (0.2467)  classification_loss: 1.0802 (1.1051)  loss_mask: 0.0000 (0.0003)  time: 0.1991  data: 0.0003  max mem: 5511
[10:38:36.483321] Epoch: [97]  [500/781]  eta: 0:00:56  lr: 0.000001  training_loss: 1.3312 (1.3524)  mae_loss: 0.2405 (0.2464)  classification_loss: 1.1069 (1.1056)  loss_mask: 0.0000 (0.0003)  time: 0.1973  data: 0.0002  max mem: 5511
[10:38:40.446898] Epoch: [97]  [520/781]  eta: 0:00:52  lr: 0.000001  training_loss: 1.3272 (1.3522)  mae_loss: 0.2591 (0.2467)  classification_loss: 1.0665 (1.1051)  loss_mask: 0.0000 (0.0003)  time: 0.1981  data: 0.0002  max mem: 5511
[10:38:44.401657] Epoch: [97]  [540/781]  eta: 0:00:48  lr: 0.000001  training_loss: 1.3579 (1.3528)  mae_loss: 0.2433 (0.2467)  classification_loss: 1.1183 (1.1058)  loss_mask: 0.0000 (0.0003)  time: 0.1976  data: 0.0002  max mem: 5511
[10:38:48.369111] Epoch: [97]  [560/781]  eta: 0:00:44  lr: 0.000001  training_loss: 1.3136 (1.3528)  mae_loss: 0.2368 (0.2466)  classification_loss: 1.0915 (1.1059)  loss_mask: 0.0000 (0.0003)  time: 0.1983  data: 0.0002  max mem: 5511
[10:38:52.337592] Epoch: [97]  [580/781]  eta: 0:00:40  lr: 0.000001  training_loss: 1.3439 (1.3532)  mae_loss: 0.2387 (0.2465)  classification_loss: 1.1019 (1.1065)  loss_mask: 0.0000 (0.0003)  time: 0.1983  data: 0.0002  max mem: 5511
[10:38:56.346221] Epoch: [97]  [600/781]  eta: 0:00:36  lr: 0.000001  training_loss: 1.3905 (1.3542)  mae_loss: 0.2518 (0.2465)  classification_loss: 1.1409 (1.1075)  loss_mask: 0.0000 (0.0003)  time: 0.2003  data: 0.0002  max mem: 5511
[10:39:00.297748] Epoch: [97]  [620/781]  eta: 0:00:32  lr: 0.000001  training_loss: 1.3338 (1.3540)  mae_loss: 0.2479 (0.2467)  classification_loss: 1.1034 (1.1070)  loss_mask: 0.0000 (0.0002)  time: 0.1975  data: 0.0002  max mem: 5511
[10:39:04.246068] Epoch: [97]  [640/781]  eta: 0:00:28  lr: 0.000001  training_loss: 1.3272 (1.3545)  mae_loss: 0.2479 (0.2468)  classification_loss: 1.1004 (1.1074)  loss_mask: 0.0000 (0.0002)  time: 0.1973  data: 0.0003  max mem: 5511
[10:39:08.195323] Epoch: [97]  [660/781]  eta: 0:00:24  lr: 0.000001  training_loss: 1.3615 (1.3549)  mae_loss: 0.2438 (0.2468)  classification_loss: 1.1246 (1.1078)  loss_mask: 0.0000 (0.0002)  time: 0.1974  data: 0.0003  max mem: 5511
[10:39:12.146530] Epoch: [97]  [680/781]  eta: 0:00:20  lr: 0.000001  training_loss: 1.4341 (1.3563)  mae_loss: 0.2549 (0.2470)  classification_loss: 1.1616 (1.1090)  loss_mask: 0.0000 (0.0002)  time: 0.1975  data: 0.0003  max mem: 5511
[10:39:16.084027] Epoch: [97]  [700/781]  eta: 0:00:16  lr: 0.000001  training_loss: 1.3303 (1.3558)  mae_loss: 0.2354 (0.2468)  classification_loss: 1.1242 (1.1088)  loss_mask: 0.0000 (0.0002)  time: 0.1968  data: 0.0002  max mem: 5511
[10:39:20.159689] Epoch: [97]  [720/781]  eta: 0:00:12  lr: 0.000001  training_loss: 1.3701 (1.3572)  mae_loss: 0.2384 (0.2469)  classification_loss: 1.1377 (1.1101)  loss_mask: 0.0000 (0.0002)  time: 0.2037  data: 0.0002  max mem: 5511
[10:39:24.102651] Epoch: [97]  [740/781]  eta: 0:00:08  lr: 0.000001  training_loss: 1.3186 (1.3572)  mae_loss: 0.2480 (0.2471)  classification_loss: 1.0619 (1.1099)  loss_mask: 0.0000 (0.0002)  time: 0.1971  data: 0.0002  max mem: 5511
[10:39:28.043196] Epoch: [97]  [760/781]  eta: 0:00:04  lr: 0.000001  training_loss: 1.3749 (1.3582)  mae_loss: 0.2527 (0.2475)  classification_loss: 1.1085 (1.1105)  loss_mask: 0.0000 (0.0002)  time: 0.1969  data: 0.0003  max mem: 5511
[10:39:32.001451] Epoch: [97]  [780/781]  eta: 0:00:00  lr: 0.000001  training_loss: 1.3801 (1.3590)  mae_loss: 0.2483 (0.2475)  classification_loss: 1.1343 (1.1113)  loss_mask: 0.0000 (0.0002)  time: 0.1978  data: 0.0002  max mem: 5511
[10:39:32.172079] Epoch: [97] Total time: 0:02:35 (0.1993 s / it)
[10:39:32.172561] Averaged stats: lr: 0.000001  training_loss: 1.3801 (1.3590)  mae_loss: 0.2483 (0.2475)  classification_loss: 1.1343 (1.1113)  loss_mask: 0.0000 (0.0002)
[10:39:32.954114] Test:  [  0/157]  eta: 0:02:01  testing_loss: 0.4481 (0.4481)  acc1: 85.9375 (85.9375)  acc5: 100.0000 (100.0000)  time: 0.7759  data: 0.7434  max mem: 5511
[10:39:33.240697] Test:  [ 10/157]  eta: 0:00:14  testing_loss: 0.4481 (0.4519)  acc1: 87.5000 (87.0739)  acc5: 100.0000 (99.5739)  time: 0.0963  data: 0.0678  max mem: 5511
[10:39:33.536091] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4165 (0.4316)  acc1: 89.0625 (87.5744)  acc5: 100.0000 (99.4792)  time: 0.0289  data: 0.0004  max mem: 5511
[10:39:33.820631] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4111 (0.4482)  acc1: 87.5000 (86.9960)  acc5: 100.0000 (99.3952)  time: 0.0288  data: 0.0004  max mem: 5511
[10:39:34.137869] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4234 (0.4511)  acc1: 87.5000 (86.9665)  acc5: 100.0000 (99.3521)  time: 0.0299  data: 0.0003  max mem: 5511
[10:39:34.425594] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4073 (0.4416)  acc1: 87.5000 (87.2855)  acc5: 100.0000 (99.3260)  time: 0.0300  data: 0.0003  max mem: 5511
[10:39:34.719319] Test:  [ 60/157]  eta: 0:00:04  testing_loss: 0.4022 (0.4367)  acc1: 89.0625 (87.3975)  acc5: 100.0000 (99.3340)  time: 0.0289  data: 0.0002  max mem: 5511
[10:39:35.005432] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4266 (0.4358)  acc1: 87.5000 (87.3680)  acc5: 100.0000 (99.3398)  time: 0.0288  data: 0.0002  max mem: 5511
[10:39:35.290811] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4360 (0.4440)  acc1: 85.9375 (87.1142)  acc5: 100.0000 (99.3248)  time: 0.0284  data: 0.0002  max mem: 5511
[10:39:35.577743] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4268 (0.4376)  acc1: 87.5000 (87.3970)  acc5: 100.0000 (99.2960)  time: 0.0285  data: 0.0002  max mem: 5511
[10:39:35.864031] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.3978 (0.4386)  acc1: 89.0625 (87.5000)  acc5: 100.0000 (99.3348)  time: 0.0285  data: 0.0002  max mem: 5511
[10:39:36.151767] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4471 (0.4410)  acc1: 87.5000 (87.4155)  acc5: 100.0000 (99.3102)  time: 0.0285  data: 0.0002  max mem: 5511
[10:39:36.438080] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4471 (0.4403)  acc1: 85.9375 (87.4613)  acc5: 100.0000 (99.3156)  time: 0.0285  data: 0.0002  max mem: 5511
[10:39:36.724991] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4096 (0.4403)  acc1: 85.9375 (87.4284)  acc5: 100.0000 (99.3321)  time: 0.0285  data: 0.0002  max mem: 5511
[10:39:37.012446] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4096 (0.4376)  acc1: 87.5000 (87.4889)  acc5: 100.0000 (99.3573)  time: 0.0286  data: 0.0002  max mem: 5511
[10:39:37.297580] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4208 (0.4361)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (99.3791)  time: 0.0285  data: 0.0002  max mem: 5511
[10:39:37.449023] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4132 (0.4360)  acc1: 87.5000 (87.4700)  acc5: 100.0000 (99.3700)  time: 0.0274  data: 0.0001  max mem: 5511
[10:39:37.629023] Test: Total time: 0:00:05 (0.0347 s / it)
[10:39:37.629627] * Acc@1 87.470 Acc@5 99.370 loss 0.436
[10:39:37.629948] Accuracy of the network on the 10000 test images: 87.5%
[10:39:37.630170] Max accuracy: 87.62%
[10:39:37.896242] log_dir: ./output_dir
[10:39:38.896681] Epoch: [98]  [  0/781]  eta: 0:12:59  lr: 0.000001  training_loss: 1.3361 (1.3361)  mae_loss: 0.2115 (0.2115)  classification_loss: 1.1245 (1.1245)  loss_mask: 0.0000 (0.0000)  time: 0.9980  data: 0.7660  max mem: 5511
[10:39:42.864480] Epoch: [98]  [ 20/781]  eta: 0:02:59  lr: 0.000001  training_loss: 1.3187 (1.3451)  mae_loss: 0.2400 (0.2482)  classification_loss: 1.0940 (1.0968)  loss_mask: 0.0000 (0.0000)  time: 0.1983  data: 0.0002  max mem: 5511
[10:39:46.804462] Epoch: [98]  [ 40/781]  eta: 0:02:40  lr: 0.000001  training_loss: 1.3157 (1.3422)  mae_loss: 0.2452 (0.2464)  classification_loss: 1.0834 (1.0958)  loss_mask: 0.0000 (0.0000)  time: 0.1969  data: 0.0003  max mem: 5511
[10:39:50.786622] Epoch: [98]  [ 60/781]  eta: 0:02:32  lr: 0.000001  training_loss: 1.3354 (1.3524)  mae_loss: 0.2585 (0.2515)  classification_loss: 1.0657 (1.1009)  loss_mask: 0.0000 (0.0000)  time: 0.1990  data: 0.0002  max mem: 5511
[10:39:54.765849] Epoch: [98]  [ 80/781]  eta: 0:02:25  lr: 0.000001  training_loss: 1.3568 (1.3498)  mae_loss: 0.2347 (0.2475)  classification_loss: 1.1161 (1.1022)  loss_mask: 0.0000 (0.0000)  time: 0.1989  data: 0.0002  max mem: 5511
[10:39:58.750920] Epoch: [98]  [100/781]  eta: 0:02:20  lr: 0.000001  training_loss: 1.3880 (1.3553)  mae_loss: 0.2413 (0.2471)  classification_loss: 1.1274 (1.1081)  loss_mask: 0.0000 (0.0000)  time: 0.1992  data: 0.0004  max mem: 5511
[10:40:02.720367] Epoch: [98]  [120/781]  eta: 0:02:15  lr: 0.000001  training_loss: 1.3334 (1.3515)  mae_loss: 0.2382 (0.2457)  classification_loss: 1.0850 (1.1058)  loss_mask: 0.0000 (0.0000)  time: 0.1984  data: 0.0003  max mem: 5511
[10:40:06.676264] Epoch: [98]  [140/781]  eta: 0:02:10  lr: 0.000001  training_loss: 1.3189 (1.3520)  mae_loss: 0.2395 (0.2454)  classification_loss: 1.0828 (1.1066)  loss_mask: 0.0000 (0.0000)  time: 0.1977  data: 0.0003  max mem: 5511
[10:40:10.640492] Epoch: [98]  [160/781]  eta: 0:02:06  lr: 0.000001  training_loss: 1.3185 (1.3504)  mae_loss: 0.2441 (0.2450)  classification_loss: 1.0708 (1.1053)  loss_mask: 0.0000 (0.0000)  time: 0.1981  data: 0.0002  max mem: 5511
[10:40:14.625503] Epoch: [98]  [180/781]  eta: 0:02:01  lr: 0.000001  training_loss: 1.3561 (1.3539)  mae_loss: 0.2419 (0.2461)  classification_loss: 1.1416 (1.1077)  loss_mask: 0.0000 (0.0000)  time: 0.1991  data: 0.0002  max mem: 5511
[10:40:18.613126] Epoch: [98]  [200/781]  eta: 0:01:57  lr: 0.000001  training_loss: 1.3278 (1.3559)  mae_loss: 0.2371 (0.2459)  classification_loss: 1.0921 (1.1100)  loss_mask: 0.0000 (0.0000)  time: 0.1993  data: 0.0003  max mem: 5511
[10:40:22.578591] Epoch: [98]  [220/781]  eta: 0:01:53  lr: 0.000001  training_loss: 1.3697 (1.3561)  mae_loss: 0.2215 (0.2449)  classification_loss: 1.1438 (1.1111)  loss_mask: 0.0000 (0.0000)  time: 0.1982  data: 0.0002  max mem: 5511
[10:40:26.559751] Epoch: [98]  [240/781]  eta: 0:01:49  lr: 0.000001  training_loss: 1.3722 (1.3577)  mae_loss: 0.2383 (0.2448)  classification_loss: 1.1341 (1.1128)  loss_mask: 0.0000 (0.0000)  time: 0.1990  data: 0.0002  max mem: 5511
[10:40:30.532753] Epoch: [98]  [260/781]  eta: 0:01:45  lr: 0.000001  training_loss: 1.3874 (1.3598)  mae_loss: 0.2368 (0.2448)  classification_loss: 1.1274 (1.1149)  loss_mask: 0.0000 (0.0000)  time: 0.1986  data: 0.0003  max mem: 5511
[10:40:34.488376] Epoch: [98]  [280/781]  eta: 0:01:40  lr: 0.000001  training_loss: 1.3567 (1.3596)  mae_loss: 0.2389 (0.2449)  classification_loss: 1.0847 (1.1147)  loss_mask: 0.0000 (0.0000)  time: 0.1977  data: 0.0003  max mem: 5511
[10:40:38.461474] Epoch: [98]  [300/781]  eta: 0:01:36  lr: 0.000001  training_loss: 1.3363 (1.3590)  mae_loss: 0.2385 (0.2448)  classification_loss: 1.0685 (1.1142)  loss_mask: 0.0000 (0.0000)  time: 0.1986  data: 0.0003  max mem: 5511
[10:40:42.445162] Epoch: [98]  [320/781]  eta: 0:01:32  lr: 0.000001  training_loss: 1.3586 (1.3597)  mae_loss: 0.2438 (0.2450)  classification_loss: 1.1042 (1.1147)  loss_mask: 0.0000 (0.0000)  time: 0.1991  data: 0.0002  max mem: 5511
[10:40:46.437243] Epoch: [98]  [340/781]  eta: 0:01:28  lr: 0.000001  training_loss: 1.3571 (1.3594)  mae_loss: 0.2573 (0.2457)  classification_loss: 1.0775 (1.1135)  loss_mask: 0.0000 (0.0002)  time: 0.1995  data: 0.0002  max mem: 5511
[10:40:50.406953] Epoch: [98]  [360/781]  eta: 0:01:24  lr: 0.000001  training_loss: 1.3848 (1.3604)  mae_loss: 0.2362 (0.2457)  classification_loss: 1.1227 (1.1145)  loss_mask: 0.0000 (0.0002)  time: 0.1984  data: 0.0003  max mem: 5511
[10:40:54.360341] Epoch: [98]  [380/781]  eta: 0:01:20  lr: 0.000001  training_loss: 1.3587 (1.3609)  mae_loss: 0.2405 (0.2461)  classification_loss: 1.1214 (1.1146)  loss_mask: 0.0001 (0.0002)  time: 0.1976  data: 0.0002  max mem: 5511
[10:40:58.318472] Epoch: [98]  [400/781]  eta: 0:01:16  lr: 0.000001  training_loss: 1.3729 (1.3622)  mae_loss: 0.2418 (0.2458)  classification_loss: 1.1478 (1.1162)  loss_mask: 0.0000 (0.0002)  time: 0.1978  data: 0.0002  max mem: 5511
[10:41:02.279886] Epoch: [98]  [420/781]  eta: 0:01:12  lr: 0.000001  training_loss: 1.3502 (1.3627)  mae_loss: 0.2584 (0.2465)  classification_loss: 1.0898 (1.1160)  loss_mask: 0.0000 (0.0002)  time: 0.1980  data: 0.0002  max mem: 5511
[10:41:06.237215] Epoch: [98]  [440/781]  eta: 0:01:08  lr: 0.000001  training_loss: 1.3485 (1.3630)  mae_loss: 0.2602 (0.2470)  classification_loss: 1.0780 (1.1157)  loss_mask: 0.0000 (0.0002)  time: 0.1978  data: 0.0003  max mem: 5511
[10:41:10.188169] Epoch: [98]  [460/781]  eta: 0:01:04  lr: 0.000001  training_loss: 1.3415 (1.3622)  mae_loss: 0.2516 (0.2470)  classification_loss: 1.0990 (1.1150)  loss_mask: 0.0000 (0.0002)  time: 0.1975  data: 0.0002  max mem: 5511
[10:41:14.170829] Epoch: [98]  [480/781]  eta: 0:01:00  lr: 0.000001  training_loss: 1.3307 (1.3626)  mae_loss: 0.2395 (0.2469)  classification_loss: 1.0909 (1.1155)  loss_mask: 0.0000 (0.0002)  time: 0.1990  data: 0.0002  max mem: 5511
[10:41:18.134146] Epoch: [98]  [500/781]  eta: 0:00:56  lr: 0.000001  training_loss: 1.3297 (1.3621)  mae_loss: 0.2377 (0.2466)  classification_loss: 1.1064 (1.1153)  loss_mask: 0.0000 (0.0002)  time: 0.1981  data: 0.0003  max mem: 5511
[10:41:22.107636] Epoch: [98]  [520/781]  eta: 0:00:52  lr: 0.000001  training_loss: 1.3970 (1.3623)  mae_loss: 0.2378 (0.2464)  classification_loss: 1.1413 (1.1157)  loss_mask: 0.0000 (0.0002)  time: 0.1986  data: 0.0003  max mem: 5511
[10:41:26.048086] Epoch: [98]  [540/781]  eta: 0:00:48  lr: 0.000001  training_loss: 1.3857 (1.3635)  mae_loss: 0.2317 (0.2459)  classification_loss: 1.1421 (1.1174)  loss_mask: 0.0000 (0.0002)  time: 0.1969  data: 0.0002  max mem: 5511
[10:41:29.996389] Epoch: [98]  [560/781]  eta: 0:00:44  lr: 0.000001  training_loss: 1.3463 (1.3634)  mae_loss: 0.2509 (0.2460)  classification_loss: 1.1027 (1.1172)  loss_mask: 0.0000 (0.0002)  time: 0.1973  data: 0.0002  max mem: 5511
[10:41:33.966364] Epoch: [98]  [580/781]  eta: 0:00:40  lr: 0.000001  training_loss: 1.3848 (1.3640)  mae_loss: 0.2438 (0.2463)  classification_loss: 1.1263 (1.1176)  loss_mask: 0.0000 (0.0002)  time: 0.1984  data: 0.0002  max mem: 5511
[10:41:37.913781] Epoch: [98]  [600/781]  eta: 0:00:36  lr: 0.000001  training_loss: 1.3231 (1.3637)  mae_loss: 0.2496 (0.2463)  classification_loss: 1.0991 (1.1173)  loss_mask: 0.0000 (0.0002)  time: 0.1973  data: 0.0002  max mem: 5511
[10:41:41.856591] Epoch: [98]  [620/781]  eta: 0:00:32  lr: 0.000001  training_loss: 1.3538 (1.3636)  mae_loss: 0.2347 (0.2462)  classification_loss: 1.1065 (1.1172)  loss_mask: 0.0000 (0.0002)  time: 0.1971  data: 0.0002  max mem: 5511
[10:41:45.796237] Epoch: [98]  [640/781]  eta: 0:00:28  lr: 0.000001  training_loss: 1.3606 (1.3631)  mae_loss: 0.2365 (0.2460)  classification_loss: 1.0766 (1.1170)  loss_mask: 0.0000 (0.0002)  time: 0.1969  data: 0.0002  max mem: 5511
[10:41:49.763827] Epoch: [98]  [660/781]  eta: 0:00:24  lr: 0.000001  training_loss: 1.3725 (1.3635)  mae_loss: 0.2567 (0.2461)  classification_loss: 1.1439 (1.1173)  loss_mask: 0.0000 (0.0001)  time: 0.1983  data: 0.0002  max mem: 5511
[10:41:53.715361] Epoch: [98]  [680/781]  eta: 0:00:20  lr: 0.000001  training_loss: 1.3835 (1.3642)  mae_loss: 0.2665 (0.2466)  classification_loss: 1.1321 (1.1175)  loss_mask: 0.0000 (0.0001)  time: 0.1975  data: 0.0003  max mem: 5511
[10:41:57.655169] Epoch: [98]  [700/781]  eta: 0:00:16  lr: 0.000001  training_loss: 1.3311 (1.3633)  mae_loss: 0.2380 (0.2466)  classification_loss: 1.0859 (1.1166)  loss_mask: 0.0000 (0.0001)  time: 0.1969  data: 0.0002  max mem: 5511
[10:42:01.611112] Epoch: [98]  [720/781]  eta: 0:00:12  lr: 0.000001  training_loss: 1.3969 (1.3639)  mae_loss: 0.2358 (0.2466)  classification_loss: 1.1333 (1.1172)  loss_mask: 0.0000 (0.0001)  time: 0.1977  data: 0.0002  max mem: 5511
[10:42:05.542635] Epoch: [98]  [740/781]  eta: 0:00:08  lr: 0.000001  training_loss: 1.3014 (1.3630)  mae_loss: 0.2633 (0.2469)  classification_loss: 1.0396 (1.1159)  loss_mask: 0.0000 (0.0001)  time: 0.1965  data: 0.0002  max mem: 5511
[10:42:09.525434] Epoch: [98]  [760/781]  eta: 0:00:04  lr: 0.000001  training_loss: 1.3532 (1.3634)  mae_loss: 0.2432 (0.2468)  classification_loss: 1.1043 (1.1165)  loss_mask: 0.0000 (0.0001)  time: 0.1990  data: 0.0002  max mem: 5511
[10:42:13.478334] Epoch: [98]  [780/781]  eta: 0:00:00  lr: 0.000001  training_loss: 1.3682 (1.3631)  mae_loss: 0.2615 (0.2470)  classification_loss: 1.1156 (1.1159)  loss_mask: 0.0000 (0.0001)  time: 0.1975  data: 0.0002  max mem: 5511
[10:42:13.655373] Epoch: [98] Total time: 0:02:35 (0.1994 s / it)
[10:42:13.655930] Averaged stats: lr: 0.000001  training_loss: 1.3682 (1.3631)  mae_loss: 0.2615 (0.2470)  classification_loss: 1.1156 (1.1159)  loss_mask: 0.0000 (0.0001)
[10:42:14.274311] Test:  [  0/157]  eta: 0:01:36  testing_loss: 0.4413 (0.4413)  acc1: 85.9375 (85.9375)  acc5: 100.0000 (100.0000)  time: 0.6142  data: 0.5837  max mem: 5511
[10:42:14.562823] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4435 (0.4515)  acc1: 87.5000 (86.7898)  acc5: 100.0000 (99.5739)  time: 0.0819  data: 0.0533  max mem: 5511
[10:42:14.847633] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4214 (0.4314)  acc1: 87.5000 (87.2024)  acc5: 100.0000 (99.4792)  time: 0.0285  data: 0.0002  max mem: 5511
[10:42:15.131682] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4117 (0.4475)  acc1: 89.0625 (86.8952)  acc5: 100.0000 (99.3952)  time: 0.0283  data: 0.0001  max mem: 5511
[10:42:15.417893] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4221 (0.4505)  acc1: 87.5000 (86.8902)  acc5: 100.0000 (99.3140)  time: 0.0284  data: 0.0002  max mem: 5511
[10:42:15.702501] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4134 (0.4411)  acc1: 87.5000 (87.2855)  acc5: 100.0000 (99.2953)  time: 0.0284  data: 0.0002  max mem: 5511
[10:42:15.990216] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4048 (0.4363)  acc1: 87.5000 (87.3719)  acc5: 100.0000 (99.3084)  time: 0.0285  data: 0.0002  max mem: 5511
[10:42:16.281489] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4268 (0.4355)  acc1: 87.5000 (87.4120)  acc5: 100.0000 (99.2958)  time: 0.0288  data: 0.0002  max mem: 5511
[10:42:16.569748] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4366 (0.4439)  acc1: 87.5000 (87.1528)  acc5: 100.0000 (99.2670)  time: 0.0289  data: 0.0002  max mem: 5511
[10:42:16.859607] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4270 (0.4376)  acc1: 87.5000 (87.4485)  acc5: 100.0000 (99.2445)  time: 0.0288  data: 0.0002  max mem: 5511
[10:42:17.147201] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.3919 (0.4388)  acc1: 89.0625 (87.5309)  acc5: 100.0000 (99.2884)  time: 0.0287  data: 0.0002  max mem: 5511
[10:42:17.431837] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4418 (0.4411)  acc1: 87.5000 (87.4155)  acc5: 100.0000 (99.2821)  time: 0.0285  data: 0.0002  max mem: 5511
[10:42:17.716737] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4418 (0.4403)  acc1: 85.9375 (87.4225)  acc5: 100.0000 (99.2898)  time: 0.0283  data: 0.0002  max mem: 5511
[10:42:18.009817] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4142 (0.4402)  acc1: 85.9375 (87.3449)  acc5: 100.0000 (99.3082)  time: 0.0287  data: 0.0002  max mem: 5511
[10:42:18.296497] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4142 (0.4377)  acc1: 87.5000 (87.4003)  acc5: 100.0000 (99.3462)  time: 0.0289  data: 0.0002  max mem: 5511
[10:42:18.580569] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4209 (0.4361)  acc1: 87.5000 (87.3965)  acc5: 100.0000 (99.3688)  time: 0.0284  data: 0.0001  max mem: 5511
[10:42:18.733238] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4095 (0.4358)  acc1: 87.5000 (87.3700)  acc5: 100.0000 (99.3700)  time: 0.0273  data: 0.0001  max mem: 5511
[10:42:18.895688] Test: Total time: 0:00:05 (0.0334 s / it)
[10:42:18.896169] * Acc@1 87.370 Acc@5 99.370 loss 0.436
[10:42:18.896498] Accuracy of the network on the 10000 test images: 87.4%
[10:42:18.896683] Max accuracy: 87.62%
[10:42:19.512028] log_dir: ./output_dir
[10:42:20.457628] Epoch: [99]  [  0/781]  eta: 0:12:16  lr: 0.000001  training_loss: 1.0658 (1.0658)  mae_loss: 0.2065 (0.2065)  classification_loss: 0.8593 (0.8593)  loss_mask: 0.0000 (0.0000)  time: 0.9435  data: 0.7410  max mem: 5511
[10:42:24.401304] Epoch: [99]  [ 20/781]  eta: 0:02:57  lr: 0.000001  training_loss: 1.3440 (1.3456)  mae_loss: 0.2423 (0.2507)  classification_loss: 1.1023 (1.0949)  loss_mask: 0.0000 (0.0000)  time: 0.1971  data: 0.0002  max mem: 5511
[10:42:28.329631] Epoch: [99]  [ 40/781]  eta: 0:02:39  lr: 0.000001  training_loss: 1.3285 (1.3405)  mae_loss: 0.2453 (0.2462)  classification_loss: 1.0589 (1.0943)  loss_mask: 0.0000 (0.0000)  time: 0.1963  data: 0.0002  max mem: 5511
[10:42:32.281430] Epoch: [99]  [ 60/781]  eta: 0:02:30  lr: 0.000001  training_loss: 1.3637 (1.3470)  mae_loss: 0.2349 (0.2439)  classification_loss: 1.1170 (1.1032)  loss_mask: 0.0000 (0.0000)  time: 0.1975  data: 0.0003  max mem: 5511
[10:42:36.223108] Epoch: [99]  [ 80/781]  eta: 0:02:24  lr: 0.000001  training_loss: 1.3331 (1.3433)  mae_loss: 0.2307 (0.2438)  classification_loss: 1.0831 (1.0995)  loss_mask: 0.0000 (0.0000)  time: 0.1970  data: 0.0002  max mem: 5511
[10:42:40.165179] Epoch: [99]  [100/781]  eta: 0:02:19  lr: 0.000001  training_loss: 1.3392 (1.3481)  mae_loss: 0.2493 (0.2445)  classification_loss: 1.0782 (1.1036)  loss_mask: 0.0000 (0.0000)  time: 0.1970  data: 0.0002  max mem: 5511
[10:42:44.129841] Epoch: [99]  [120/781]  eta: 0:02:14  lr: 0.000001  training_loss: 1.3718 (1.3561)  mae_loss: 0.2440 (0.2447)  classification_loss: 1.1443 (1.1114)  loss_mask: 0.0000 (0.0000)  time: 0.1981  data: 0.0003  max mem: 5511
[10:42:48.075062] Epoch: [99]  [140/781]  eta: 0:02:09  lr: 0.000001  training_loss: 1.2991 (1.3539)  mae_loss: 0.2382 (0.2438)  classification_loss: 1.0589 (1.1102)  loss_mask: 0.0000 (0.0000)  time: 0.1972  data: 0.0003  max mem: 5511
[10:42:52.011001] Epoch: [99]  [160/781]  eta: 0:02:05  lr: 0.000001  training_loss: 1.3179 (1.3494)  mae_loss: 0.2314 (0.2430)  classification_loss: 1.0648 (1.1064)  loss_mask: 0.0000 (0.0000)  time: 0.1967  data: 0.0003  max mem: 5511
[10:42:55.955820] Epoch: [99]  [180/781]  eta: 0:02:00  lr: 0.000001  training_loss: 1.3712 (1.3535)  mae_loss: 0.2607 (0.2448)  classification_loss: 1.1029 (1.1086)  loss_mask: 0.0000 (0.0000)  time: 0.1972  data: 0.0002  max mem: 5511
[10:42:59.920084] Epoch: [99]  [200/781]  eta: 0:01:56  lr: 0.000001  training_loss: 1.3462 (1.3543)  mae_loss: 0.2445 (0.2457)  classification_loss: 1.0962 (1.1086)  loss_mask: 0.0000 (0.0000)  time: 0.1981  data: 0.0002  max mem: 5511
[10:43:03.904229] Epoch: [99]  [220/781]  eta: 0:01:52  lr: 0.000001  training_loss: 1.3641 (1.3573)  mae_loss: 0.2613 (0.2472)  classification_loss: 1.0978 (1.1101)  loss_mask: 0.0000 (0.0000)  time: 0.1991  data: 0.0002  max mem: 5511
[10:43:07.875178] Epoch: [99]  [240/781]  eta: 0:01:48  lr: 0.000001  training_loss: 1.3845 (1.3604)  mae_loss: 0.2360 (0.2466)  classification_loss: 1.1474 (1.1137)  loss_mask: 0.0000 (0.0000)  time: 0.1985  data: 0.0002  max mem: 5511
[10:43:11.854842] Epoch: [99]  [260/781]  eta: 0:01:44  lr: 0.000001  training_loss: 1.3841 (1.3611)  mae_loss: 0.2498 (0.2469)  classification_loss: 1.1220 (1.1141)  loss_mask: 0.0000 (0.0000)  time: 0.1989  data: 0.0002  max mem: 5511
[10:43:15.843016] Epoch: [99]  [280/781]  eta: 0:01:40  lr: 0.000001  training_loss: 1.3623 (1.3592)  mae_loss: 0.2460 (0.2470)  classification_loss: 1.1175 (1.1122)  loss_mask: 0.0000 (0.0000)  time: 0.1993  data: 0.0002  max mem: 5511
[10:43:19.821951] Epoch: [99]  [300/781]  eta: 0:01:36  lr: 0.000001  training_loss: 1.3256 (1.3561)  mae_loss: 0.2344 (0.2464)  classification_loss: 1.0994 (1.1097)  loss_mask: 0.0000 (0.0000)  time: 0.1989  data: 0.0004  max mem: 5511
[10:43:23.772692] Epoch: [99]  [320/781]  eta: 0:01:32  lr: 0.000001  training_loss: 1.3975 (1.3593)  mae_loss: 0.2494 (0.2465)  classification_loss: 1.1606 (1.1128)  loss_mask: 0.0000 (0.0000)  time: 0.1974  data: 0.0002  max mem: 5511
[10:43:27.721852] Epoch: [99]  [340/781]  eta: 0:01:28  lr: 0.000001  training_loss: 1.3206 (1.3574)  mae_loss: 0.2472 (0.2469)  classification_loss: 1.0455 (1.1105)  loss_mask: 0.0000 (0.0000)  time: 0.1974  data: 0.0002  max mem: 5511
[10:43:31.664475] Epoch: [99]  [360/781]  eta: 0:01:24  lr: 0.000001  training_loss: 1.4154 (1.3602)  mae_loss: 0.2499 (0.2470)  classification_loss: 1.1617 (1.1131)  loss_mask: 0.0000 (0.0000)  time: 0.1970  data: 0.0002  max mem: 5511
[10:43:35.617892] Epoch: [99]  [380/781]  eta: 0:01:20  lr: 0.000001  training_loss: 1.3671 (1.3613)  mae_loss: 0.2495 (0.2477)  classification_loss: 1.1128 (1.1136)  loss_mask: 0.0000 (0.0000)  time: 0.1976  data: 0.0002  max mem: 5511
[10:43:39.599502] Epoch: [99]  [400/781]  eta: 0:01:16  lr: 0.000001  training_loss: 1.3274 (1.3615)  mae_loss: 0.2481 (0.2480)  classification_loss: 1.1062 (1.1135)  loss_mask: 0.0000 (0.0000)  time: 0.1990  data: 0.0003  max mem: 5511
[10:43:43.559080] Epoch: [99]  [420/781]  eta: 0:01:12  lr: 0.000001  training_loss: 1.3454 (1.3620)  mae_loss: 0.2482 (0.2482)  classification_loss: 1.0914 (1.1138)  loss_mask: 0.0000 (0.0000)  time: 0.1979  data: 0.0002  max mem: 5511
[10:43:47.515419] Epoch: [99]  [440/781]  eta: 0:01:08  lr: 0.000001  training_loss: 1.3509 (1.3619)  mae_loss: 0.2515 (0.2485)  classification_loss: 1.0771 (1.1134)  loss_mask: 0.0000 (0.0000)  time: 0.1977  data: 0.0002  max mem: 5511
[10:43:51.454158] Epoch: [99]  [460/781]  eta: 0:01:03  lr: 0.000001  training_loss: 1.3551 (1.3612)  mae_loss: 0.2392 (0.2484)  classification_loss: 1.0856 (1.1127)  loss_mask: 0.0000 (0.0000)  time: 0.1969  data: 0.0002  max mem: 5511
[10:43:55.445247] Epoch: [99]  [480/781]  eta: 0:01:00  lr: 0.000001  training_loss: 1.3598 (1.3615)  mae_loss: 0.2588 (0.2485)  classification_loss: 1.0975 (1.1130)  loss_mask: 0.0000 (0.0000)  time: 0.1995  data: 0.0002  max mem: 5511
[10:43:59.389544] Epoch: [99]  [500/781]  eta: 0:00:55  lr: 0.000001  training_loss: 1.3581 (1.3613)  mae_loss: 0.2429 (0.2486)  classification_loss: 1.1091 (1.1127)  loss_mask: 0.0000 (0.0000)  time: 0.1971  data: 0.0002  max mem: 5511
[10:44:03.332969] Epoch: [99]  [520/781]  eta: 0:00:51  lr: 0.000001  training_loss: 1.3764 (1.3620)  mae_loss: 0.2416 (0.2488)  classification_loss: 1.1120 (1.1131)  loss_mask: 0.0000 (0.0000)  time: 0.1971  data: 0.0002  max mem: 5511
[10:44:07.306632] Epoch: [99]  [540/781]  eta: 0:00:47  lr: 0.000001  training_loss: 1.3231 (1.3608)  mae_loss: 0.2447 (0.2489)  classification_loss: 1.0720 (1.1119)  loss_mask: 0.0000 (0.0000)  time: 0.1986  data: 0.0002  max mem: 5511
[10:44:11.284148] Epoch: [99]  [560/781]  eta: 0:00:44  lr: 0.000001  training_loss: 1.3358 (1.3605)  mae_loss: 0.2416 (0.2486)  classification_loss: 1.0941 (1.1119)  loss_mask: 0.0000 (0.0000)  time: 0.1988  data: 0.0002  max mem: 5511
[10:44:15.203737] Epoch: [99]  [580/781]  eta: 0:00:40  lr: 0.000001  training_loss: 1.3330 (1.3600)  mae_loss: 0.2438 (0.2486)  classification_loss: 1.0704 (1.1114)  loss_mask: 0.0000 (0.0000)  time: 0.1959  data: 0.0003  max mem: 5511
[10:44:19.161948] Epoch: [99]  [600/781]  eta: 0:00:36  lr: 0.000001  training_loss: 1.3445 (1.3598)  mae_loss: 0.2472 (0.2486)  classification_loss: 1.1030 (1.1112)  loss_mask: 0.0000 (0.0000)  time: 0.1978  data: 0.0003  max mem: 5511
[10:44:23.126648] Epoch: [99]  [620/781]  eta: 0:00:32  lr: 0.000001  training_loss: 1.3704 (1.3606)  mae_loss: 0.2552 (0.2488)  classification_loss: 1.1125 (1.1118)  loss_mask: 0.0000 (0.0000)  time: 0.1981  data: 0.0003  max mem: 5511
[10:44:27.091704] Epoch: [99]  [640/781]  eta: 0:00:28  lr: 0.000001  training_loss: 1.3853 (1.3617)  mae_loss: 0.2459 (0.2489)  classification_loss: 1.1413 (1.1127)  loss_mask: 0.0000 (0.0000)  time: 0.1982  data: 0.0003  max mem: 5511
[10:44:31.040686] Epoch: [99]  [660/781]  eta: 0:00:24  lr: 0.000001  training_loss: 1.3354 (1.3613)  mae_loss: 0.2389 (0.2489)  classification_loss: 1.1025 (1.1123)  loss_mask: 0.0000 (0.0000)  time: 0.1974  data: 0.0003  max mem: 5511
[10:44:34.996151] Epoch: [99]  [680/781]  eta: 0:00:20  lr: 0.000001  training_loss: 1.3603 (1.3619)  mae_loss: 0.2461 (0.2490)  classification_loss: 1.1573 (1.1129)  loss_mask: 0.0000 (0.0000)  time: 0.1977  data: 0.0002  max mem: 5511
[10:44:38.963508] Epoch: [99]  [700/781]  eta: 0:00:16  lr: 0.000001  training_loss: 1.3523 (1.3627)  mae_loss: 0.2440 (0.2490)  classification_loss: 1.1291 (1.1137)  loss_mask: 0.0000 (0.0000)  time: 0.1983  data: 0.0002  max mem: 5511
[10:44:42.926386] Epoch: [99]  [720/781]  eta: 0:00:12  lr: 0.000001  training_loss: 1.4521 (1.3645)  mae_loss: 0.2402 (0.2488)  classification_loss: 1.2012 (1.1157)  loss_mask: 0.0000 (0.0000)  time: 0.1981  data: 0.0002  max mem: 5511
[10:44:46.897321] Epoch: [99]  [740/781]  eta: 0:00:08  lr: 0.000001  training_loss: 1.3455 (1.3648)  mae_loss: 0.2306 (0.2486)  classification_loss: 1.1010 (1.1161)  loss_mask: 0.0000 (0.0000)  time: 0.1985  data: 0.0004  max mem: 5511
[10:44:50.897293] Epoch: [99]  [760/781]  eta: 0:00:04  lr: 0.000001  training_loss: 1.3801 (1.3653)  mae_loss: 0.2461 (0.2486)  classification_loss: 1.1270 (1.1166)  loss_mask: 0.0000 (0.0000)  time: 0.1999  data: 0.0002  max mem: 5511
[10:44:54.823292] Epoch: [99]  [780/781]  eta: 0:00:00  lr: 0.000001  training_loss: 1.3820 (1.3657)  mae_loss: 0.2478 (0.2486)  classification_loss: 1.1332 (1.1170)  loss_mask: 0.0000 (0.0000)  time: 0.1962  data: 0.0002  max mem: 5511
[10:44:54.998897] Epoch: [99] Total time: 0:02:35 (0.1991 s / it)
[10:44:54.999406] Averaged stats: lr: 0.000001  training_loss: 1.3820 (1.3657)  mae_loss: 0.2478 (0.2486)  classification_loss: 1.1332 (1.1170)  loss_mask: 0.0000 (0.0000)
[10:44:55.727629] Test:  [  0/157]  eta: 0:01:53  testing_loss: 0.4313 (0.4313)  acc1: 85.9375 (85.9375)  acc5: 100.0000 (100.0000)  time: 0.7234  data: 0.6915  max mem: 5511
[10:44:56.011876] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.4399 (0.4496)  acc1: 87.5000 (87.0739)  acc5: 100.0000 (99.5739)  time: 0.0915  data: 0.0630  max mem: 5511
[10:44:56.293536] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4209 (0.4307)  acc1: 89.0625 (87.4256)  acc5: 100.0000 (99.4792)  time: 0.0282  data: 0.0002  max mem: 5511
[10:44:56.576708] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4176 (0.4471)  acc1: 89.0625 (87.0464)  acc5: 100.0000 (99.3952)  time: 0.0281  data: 0.0002  max mem: 5511
[10:44:56.864478] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4245 (0.4498)  acc1: 87.5000 (87.0046)  acc5: 100.0000 (99.3140)  time: 0.0284  data: 0.0002  max mem: 5511
[10:44:57.151791] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4007 (0.4404)  acc1: 87.5000 (87.3775)  acc5: 100.0000 (99.2953)  time: 0.0286  data: 0.0002  max mem: 5511
[10:44:57.440838] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4028 (0.4354)  acc1: 89.0625 (87.4744)  acc5: 100.0000 (99.3084)  time: 0.0287  data: 0.0002  max mem: 5511
[10:44:57.735586] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4222 (0.4344)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (99.2958)  time: 0.0290  data: 0.0002  max mem: 5511
[10:44:58.026193] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4351 (0.4430)  acc1: 85.9375 (87.1335)  acc5: 100.0000 (99.2863)  time: 0.0291  data: 0.0003  max mem: 5511
[10:44:58.321981] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4260 (0.4367)  acc1: 87.5000 (87.4828)  acc5: 100.0000 (99.2617)  time: 0.0291  data: 0.0003  max mem: 5511
[10:44:58.613083] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.3914 (0.4381)  acc1: 89.0625 (87.5619)  acc5: 100.0000 (99.3038)  time: 0.0292  data: 0.0002  max mem: 5511
[10:44:58.914903] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4367 (0.4403)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (99.2962)  time: 0.0295  data: 0.0003  max mem: 5511
[10:44:59.198260] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4367 (0.4396)  acc1: 85.9375 (87.5129)  acc5: 100.0000 (99.3156)  time: 0.0291  data: 0.0003  max mem: 5511
[10:44:59.482765] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4118 (0.4397)  acc1: 85.9375 (87.4761)  acc5: 100.0000 (99.3321)  time: 0.0283  data: 0.0002  max mem: 5511
[10:44:59.765418] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4118 (0.4370)  acc1: 87.5000 (87.5554)  acc5: 100.0000 (99.3684)  time: 0.0282  data: 0.0002  max mem: 5511
[10:45:00.050718] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4224 (0.4355)  acc1: 87.5000 (87.5517)  acc5: 100.0000 (99.3895)  time: 0.0283  data: 0.0002  max mem: 5511
[10:45:00.204272] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4091 (0.4353)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (99.3900)  time: 0.0274  data: 0.0001  max mem: 5511
[10:45:00.370326] Test: Total time: 0:00:05 (0.0342 s / it)
[10:45:00.370824] * Acc@1 87.500 Acc@5 99.390 loss 0.435
[10:45:00.371136] Accuracy of the network on the 10000 test images: 87.5%
[10:45:00.371372] Max accuracy: 87.62%
[10:45:00.804704] Training time 4:28:09