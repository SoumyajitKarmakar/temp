Not using distributed mode
[20:53:07.232551] job dir: /notebooks/CVPR2023
[20:53:07.232968] Namespace(batch_size=64,
epochs=100,
accum_iter=1,
model='mae_vit_tiny',
norm_pix_loss=False,
dataset='c10',
input_size=32,
patch_size=2,
mask_ratio=0.75,
lambda_weight=0.1,
drop_path=0.1,
clip_grad=None,
weight_decay=0.05,
lr=None,
blr=0.001,
layer_decay=0.75,
min_lr=1e-06,
warmup_epochs=5,
color_jitter=None,
aa='rand-m9-mstd0.5-inc1',
smoothing=0.1,
reprob=0.25,
remode='pixel',
recount=1,
resplit=False,
mixup=0,
cutmix=0,
cutmix_minmax=None,
mixup_prob=1.0,
mixup_switch_prob=0.5,
mixup_mode='batch',
finetune='',
global_pool=True,
data_path='/datasets01/imagenet_full_size/061417/',
nb_classes=10,
output_dir='./output_dir',
log_dir='./output_dir',
device='cuda',
seed=0,
resume='',
start_epoch=0,
eval=False,
dist_eval=False,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[20:53:08.329290] Files already downloaded and verified
/usr/local/lib/python3.9/dist-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
[20:53:09.093056] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCropAndInterpolation(size=(32, 32), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic)
               RandomHorizontalFlip(p=0.5)
               RandAugment(n=2, ops=
           	AugmentOp(name=AutoContrast, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Equalize, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Invert, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Rotate, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=PosterizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeAdd, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ColorIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ContrastIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=BrightnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SharpnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearX, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearY, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateXRel, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateYRel, p=0.5, m=9, mstd=0.5))
               ToTensor()
               Normalize(mean=tensor([0.4914, 0.4822, 0.4465]), std=tensor([0.2023, 0.1994, 0.2010]))
               RandomErasing(p=0.25, mode=pixel, count=(1, 1))
           )
[20:53:09.516427] Files already downloaded and verified
[20:53:09.937531] Dataset CIFAR10
    Number of datapoints: 10000
    Root location: ./data
    Split: Test
    StandardTransform
Transform: Compose(
               Resize(size=36, interpolation=bicubic, max_size=None, antialias=None)
               CenterCrop(size=(32, 32))
               ToTensor()
               Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.201))
           )
[20:53:09.938148] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f7ea3a37eb0>
/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[20:53:21.520919] Model = MaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(2, 2), stride=(2, 2))
    (norm): Identity()
  )
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (7): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (8): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (9): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (10): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (11): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
  (decoder_embed): Linear(in_features=192, out_features=128, bias=True)
  (decoder_blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=128, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=128, out_features=128, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=512, out_features=128, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=128, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=128, out_features=128, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=512, out_features=128, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (decoder_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
  (decoder_pred): Linear(in_features=128, out_features=12, bias=True)
  (head): Linear(in_features=192, out_features=10, bias=True)
  (classifier_mask): Sequential(
    (0): Linear(in_features=192, out_features=5, bias=True)
    (1): LogSoftmax(dim=1)
  )
)
[20:53:21.521756] number of params (M): 5.77
[20:53:21.521965] base lr: 1.00e-03
[20:53:21.522135] actual lr: 2.50e-04
[20:53:21.522280] accumulate grad iterations: 1
[20:53:21.522420] effective batch size: 64
[20:53:21.523657] criterion = LabelSmoothingCrossEntropy()
[20:53:21.523930] Start training for 100 epochs
[20:53:21.525083] log_dir: ./output_dir
[20:53:27.535766] Epoch: [0]  [  0/781]  eta: 1:18:11  lr: 0.000000  training_loss: 3.2333 (3.2333)  mae_loss: 0.3814 (0.3814)  classification_loss: 2.6555 (2.6555)  loss_mask: 0.1965 (0.1965)  time: 6.0076  data: 0.4938  max mem: 5446
[20:53:31.439124] Epoch: [0]  [ 20/781]  eta: 0:05:59  lr: 0.000001  training_loss: 3.1618 (3.1707)  mae_loss: 0.3951 (0.3935)  classification_loss: 2.5632 (2.5809)  loss_mask: 0.1971 (0.1962)  time: 0.1951  data: 0.0002  max mem: 5508
[20:53:35.339745] Epoch: [0]  [ 40/781]  eta: 0:04:09  lr: 0.000003  training_loss: 2.9730 (3.0775)  mae_loss: 0.3584 (0.3768)  classification_loss: 2.4220 (2.5047)  loss_mask: 0.1949 (0.1961)  time: 0.1950  data: 0.0002  max mem: 5508
[20:53:39.234764] Epoch: [0]  [ 60/781]  eta: 0:03:29  lr: 0.000004  training_loss: 2.8291 (2.9946)  mae_loss: 0.3044 (0.3527)  classification_loss: 2.3347 (2.4486)  loss_mask: 0.1897 (0.1933)  time: 0.1947  data: 0.0002  max mem: 5508
[20:53:43.143587] Epoch: [0]  [ 80/781]  eta: 0:03:07  lr: 0.000005  training_loss: 2.7456 (2.9310)  mae_loss: 0.2516 (0.3267)  classification_loss: 2.3015 (2.4135)  loss_mask: 0.1834 (0.1907)  time: 0.1954  data: 0.0002  max mem: 5508
[20:53:47.056770] Epoch: [0]  [100/781]  eta: 0:02:52  lr: 0.000006  training_loss: 2.6693 (2.8798)  mae_loss: 0.2040 (0.3032)  classification_loss: 2.2865 (2.3886)  loss_mask: 0.1772 (0.1880)  time: 0.1956  data: 0.0002  max mem: 5508
[20:53:50.968558] Epoch: [0]  [120/781]  eta: 0:02:40  lr: 0.000008  training_loss: 2.6431 (2.8404)  mae_loss: 0.1906 (0.2848)  classification_loss: 2.2768 (2.3702)  loss_mask: 0.1713 (0.1854)  time: 0.1955  data: 0.0003  max mem: 5508
[20:53:54.892284] Epoch: [0]  [140/781]  eta: 0:02:31  lr: 0.000009  training_loss: 2.5896 (2.8057)  mae_loss: 0.1724 (0.2691)  classification_loss: 2.2488 (2.3533)  loss_mask: 0.1699 (0.1832)  time: 0.1961  data: 0.0002  max mem: 5508
[20:53:58.832347] Epoch: [0]  [160/781]  eta: 0:02:23  lr: 0.000010  training_loss: 2.5666 (2.7772)  mae_loss: 0.1601 (0.2562)  classification_loss: 2.2422 (2.3398)  loss_mask: 0.1662 (0.1812)  time: 0.1969  data: 0.0002  max mem: 5508
[20:54:02.746688] Epoch: [0]  [180/781]  eta: 0:02:16  lr: 0.000012  training_loss: 2.5991 (2.7570)  mae_loss: 0.1615 (0.2458)  classification_loss: 2.2774 (2.3314)  loss_mask: 0.1663 (0.1798)  time: 0.1956  data: 0.0002  max mem: 5508
[20:54:06.664568] Epoch: [0]  [200/781]  eta: 0:02:10  lr: 0.000013  training_loss: 2.5590 (2.7374)  mae_loss: 0.1529 (0.2368)  classification_loss: 2.2331 (2.3222)  loss_mask: 0.1660 (0.1784)  time: 0.1958  data: 0.0002  max mem: 5508
[20:54:10.574033] Epoch: [0]  [220/781]  eta: 0:02:04  lr: 0.000014  training_loss: 2.5459 (2.7213)  mae_loss: 0.1521 (0.2291)  classification_loss: 2.2346 (2.3151)  loss_mask: 0.1621 (0.1771)  time: 0.1954  data: 0.0002  max mem: 5508
[20:54:14.476590] Epoch: [0]  [240/781]  eta: 0:01:58  lr: 0.000015  training_loss: 2.5770 (2.7098)  mae_loss: 0.1509 (0.2228)  classification_loss: 2.2618 (2.3110)  loss_mask: 0.1633 (0.1760)  time: 0.1951  data: 0.0004  max mem: 5508
[20:54:18.382069] Epoch: [0]  [260/781]  eta: 0:01:53  lr: 0.000017  training_loss: 2.5572 (2.6983)  mae_loss: 0.1474 (0.2170)  classification_loss: 2.2580 (2.3064)  loss_mask: 0.1611 (0.1749)  time: 0.1952  data: 0.0002  max mem: 5508
[20:54:22.306258] Epoch: [0]  [280/781]  eta: 0:01:48  lr: 0.000018  training_loss: 2.5572 (2.6883)  mae_loss: 0.1502 (0.2121)  classification_loss: 2.2546 (2.3023)  loss_mask: 0.1622 (0.1739)  time: 0.1961  data: 0.0002  max mem: 5508
[20:54:26.223496] Epoch: [0]  [300/781]  eta: 0:01:43  lr: 0.000019  training_loss: 2.5734 (2.6805)  mae_loss: 0.1414 (0.2076)  classification_loss: 2.2569 (2.2997)  loss_mask: 0.1617 (0.1732)  time: 0.1958  data: 0.0002  max mem: 5508
[20:54:30.131199] Epoch: [0]  [320/781]  eta: 0:01:38  lr: 0.000020  training_loss: 2.5580 (2.6729)  mae_loss: 0.1405 (0.2036)  classification_loss: 2.2644 (2.2969)  loss_mask: 0.1609 (0.1724)  time: 0.1953  data: 0.0002  max mem: 5508
[20:54:34.073205] Epoch: [0]  [340/781]  eta: 0:01:33  lr: 0.000022  training_loss: 2.5382 (2.6654)  mae_loss: 0.1395 (0.2000)  classification_loss: 2.2438 (2.2938)  loss_mask: 0.1620 (0.1717)  time: 0.1970  data: 0.0002  max mem: 5508
[20:54:38.051952] Epoch: [0]  [360/781]  eta: 0:01:29  lr: 0.000023  training_loss: 2.5416 (2.6581)  mae_loss: 0.1337 (0.1965)  classification_loss: 2.2447 (2.2906)  loss_mask: 0.1595 (0.1710)  time: 0.1989  data: 0.0002  max mem: 5508
[20:54:41.994468] Epoch: [0]  [380/781]  eta: 0:01:24  lr: 0.000024  training_loss: 2.5422 (2.6524)  mae_loss: 0.1348 (0.1934)  classification_loss: 2.2573 (2.2885)  loss_mask: 0.1600 (0.1705)  time: 0.1970  data: 0.0002  max mem: 5508
[20:54:45.918818] Epoch: [0]  [400/781]  eta: 0:01:20  lr: 0.000026  training_loss: 2.5095 (2.6460)  mae_loss: 0.1397 (0.1908)  classification_loss: 2.2202 (2.2853)  loss_mask: 0.1569 (0.1698)  time: 0.1961  data: 0.0002  max mem: 5508
[20:54:49.838545] Epoch: [0]  [420/781]  eta: 0:01:15  lr: 0.000027  training_loss: 2.5511 (2.6415)  mae_loss: 0.1344 (0.1882)  classification_loss: 2.2527 (2.2839)  loss_mask: 0.1608 (0.1694)  time: 0.1959  data: 0.0002  max mem: 5508
[20:54:53.774044] Epoch: [0]  [440/781]  eta: 0:01:11  lr: 0.000028  training_loss: 2.5409 (2.6370)  mae_loss: 0.1369 (0.1861)  classification_loss: 2.2314 (2.2819)  loss_mask: 0.1605 (0.1690)  time: 0.1967  data: 0.0002  max mem: 5508
[20:54:57.691835] Epoch: [0]  [460/781]  eta: 0:01:06  lr: 0.000029  training_loss: 2.5039 (2.6311)  mae_loss: 0.1390 (0.1840)  classification_loss: 2.1954 (2.2786)  loss_mask: 0.1596 (0.1685)  time: 0.1958  data: 0.0002  max mem: 5508
[20:55:01.618834] Epoch: [0]  [480/781]  eta: 0:01:02  lr: 0.000031  training_loss: 2.5234 (2.6276)  mae_loss: 0.1364 (0.1820)  classification_loss: 2.2330 (2.2774)  loss_mask: 0.1584 (0.1682)  time: 0.1963  data: 0.0003  max mem: 5508
[20:55:05.540101] Epoch: [0]  [500/781]  eta: 0:00:58  lr: 0.000032  training_loss: 2.5114 (2.6233)  mae_loss: 0.1295 (0.1799)  classification_loss: 2.2291 (2.2757)  loss_mask: 0.1563 (0.1677)  time: 0.1960  data: 0.0003  max mem: 5508
[20:55:09.479589] Epoch: [0]  [520/781]  eta: 0:00:54  lr: 0.000033  training_loss: 2.5266 (2.6196)  mae_loss: 0.1379 (0.1784)  classification_loss: 2.2233 (2.2739)  loss_mask: 0.1567 (0.1673)  time: 0.1969  data: 0.0004  max mem: 5508
[20:55:13.393296] Epoch: [0]  [540/781]  eta: 0:00:49  lr: 0.000035  training_loss: 2.5204 (2.6165)  mae_loss: 0.1291 (0.1766)  classification_loss: 2.2302 (2.2729)  loss_mask: 0.1590 (0.1670)  time: 0.1955  data: 0.0002  max mem: 5508
[20:55:17.320821] Epoch: [0]  [560/781]  eta: 0:00:45  lr: 0.000036  training_loss: 2.5104 (2.6133)  mae_loss: 0.1284 (0.1751)  classification_loss: 2.2228 (2.2715)  loss_mask: 0.1576 (0.1667)  time: 0.1963  data: 0.0003  max mem: 5508
[20:55:21.227407] Epoch: [0]  [580/781]  eta: 0:00:41  lr: 0.000037  training_loss: 2.4963 (2.6096)  mae_loss: 0.1299 (0.1735)  classification_loss: 2.2162 (2.2698)  loss_mask: 0.1575 (0.1664)  time: 0.1952  data: 0.0002  max mem: 5508
[20:55:25.195465] Epoch: [0]  [600/781]  eta: 0:00:37  lr: 0.000038  training_loss: 2.4960 (2.6060)  mae_loss: 0.1237 (0.1720)  classification_loss: 2.2133 (2.2680)  loss_mask: 0.1580 (0.1661)  time: 0.1983  data: 0.0002  max mem: 5508
[20:55:29.117347] Epoch: [0]  [620/781]  eta: 0:00:33  lr: 0.000040  training_loss: 2.5152 (2.6033)  mae_loss: 0.1286 (0.1706)  classification_loss: 2.2295 (2.2669)  loss_mask: 0.1582 (0.1658)  time: 0.1960  data: 0.0002  max mem: 5508
[20:55:33.048253] Epoch: [0]  [640/781]  eta: 0:00:28  lr: 0.000041  training_loss: 2.5067 (2.6007)  mae_loss: 0.1226 (0.1691)  classification_loss: 2.2205 (2.2660)  loss_mask: 0.1579 (0.1656)  time: 0.1965  data: 0.0003  max mem: 5508
[20:55:36.959951] Epoch: [0]  [660/781]  eta: 0:00:24  lr: 0.000042  training_loss: 2.5061 (2.5980)  mae_loss: 0.1281 (0.1679)  classification_loss: 2.2146 (2.2648)  loss_mask: 0.1566 (0.1653)  time: 0.1955  data: 0.0003  max mem: 5508
[20:55:40.941514] Epoch: [0]  [680/781]  eta: 0:00:20  lr: 0.000044  training_loss: 2.4901 (2.5949)  mae_loss: 0.1263 (0.1667)  classification_loss: 2.2077 (2.2631)  loss_mask: 0.1573 (0.1651)  time: 0.1990  data: 0.0003  max mem: 5508
[20:55:44.856325] Epoch: [0]  [700/781]  eta: 0:00:16  lr: 0.000045  training_loss: 2.4801 (2.5918)  mae_loss: 0.1159 (0.1653)  classification_loss: 2.2149 (2.2616)  loss_mask: 0.1565 (0.1649)  time: 0.1957  data: 0.0002  max mem: 5508
[20:55:48.847403] Epoch: [0]  [720/781]  eta: 0:00:12  lr: 0.000046  training_loss: 2.5272 (2.5898)  mae_loss: 0.1235 (0.1641)  classification_loss: 2.2473 (2.2611)  loss_mask: 0.1554 (0.1646)  time: 0.1995  data: 0.0002  max mem: 5508
[20:55:52.769168] Epoch: [0]  [740/781]  eta: 0:00:08  lr: 0.000047  training_loss: 2.5143 (2.5875)  mae_loss: 0.1159 (0.1629)  classification_loss: 2.2329 (2.2603)  loss_mask: 0.1549 (0.1644)  time: 0.1960  data: 0.0002  max mem: 5508
[20:55:56.693526] Epoch: [0]  [760/781]  eta: 0:00:04  lr: 0.000049  training_loss: 2.4921 (2.5851)  mae_loss: 0.1168 (0.1616)  classification_loss: 2.2119 (2.2592)  loss_mask: 0.1570 (0.1642)  time: 0.1961  data: 0.0002  max mem: 5508
[20:56:00.652381] Epoch: [0]  [780/781]  eta: 0:00:00  lr: 0.000050  training_loss: 2.4811 (2.5827)  mae_loss: 0.1202 (0.1606)  classification_loss: 2.2062 (2.2581)  loss_mask: 0.1563 (0.1640)  time: 0.1979  data: 0.0002  max mem: 5508
[20:56:00.783073] Epoch: [0] Total time: 0:02:39 (0.2039 s / it)
[20:56:00.783792] Averaged stats: lr: 0.000050  training_loss: 2.4811 (2.5827)  mae_loss: 0.1202 (0.1606)  classification_loss: 2.2062 (2.2581)  loss_mask: 0.1563 (0.1640)
[20:56:02.826783] Test:  [  0/157]  eta: 0:01:40  testing_loss: 1.8719 (1.8719)  acc1: 40.6250 (40.6250)  acc5: 90.6250 (90.6250)  time: 0.6382  data: 0.5960  max mem: 5508
[20:56:03.125214] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.9954 (1.9821)  acc1: 34.3750 (32.8125)  acc5: 84.3750 (83.5227)  time: 0.0849  data: 0.0544  max mem: 5508
[20:56:03.410059] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.9928 (1.9878)  acc1: 29.6875 (30.8780)  acc5: 81.2500 (82.6637)  time: 0.0290  data: 0.0002  max mem: 5508
[20:56:03.697471] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.9849 (1.9804)  acc1: 29.6875 (31.5524)  acc5: 82.8125 (82.9637)  time: 0.0285  data: 0.0002  max mem: 5508
[20:56:03.981405] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.9757 (1.9810)  acc1: 31.2500 (31.5549)  acc5: 82.8125 (82.7744)  time: 0.0284  data: 0.0002  max mem: 5508
[20:56:04.264193] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.9862 (1.9818)  acc1: 29.6875 (30.9743)  acc5: 84.3750 (83.1189)  time: 0.0282  data: 0.0002  max mem: 5508
[20:56:04.547881] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.9864 (1.9814)  acc1: 31.2500 (31.1475)  acc5: 85.9375 (83.4016)  time: 0.0282  data: 0.0002  max mem: 5508
[20:56:04.831215] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.9779 (1.9826)  acc1: 31.2500 (31.0079)  acc5: 84.3750 (83.4507)  time: 0.0282  data: 0.0002  max mem: 5508
[20:56:05.115976] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.9679 (1.9807)  acc1: 29.6875 (31.1535)  acc5: 84.3750 (83.7384)  time: 0.0282  data: 0.0002  max mem: 5508
[20:56:05.397655] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.9906 (1.9825)  acc1: 29.6875 (30.7692)  acc5: 85.9375 (83.9114)  time: 0.0281  data: 0.0002  max mem: 5508
[20:56:05.682325] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.9968 (1.9837)  acc1: 29.6875 (30.9561)  acc5: 82.8125 (83.6170)  time: 0.0281  data: 0.0002  max mem: 5508
[20:56:05.965264] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.9904 (1.9844)  acc1: 32.8125 (31.1655)  acc5: 82.8125 (83.6430)  time: 0.0282  data: 0.0002  max mem: 5508
[20:56:06.261871] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.9556 (1.9815)  acc1: 31.2500 (31.1854)  acc5: 84.3750 (83.8843)  time: 0.0288  data: 0.0002  max mem: 5508
[20:56:06.548184] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.9753 (1.9830)  acc1: 31.2500 (31.3096)  acc5: 84.3750 (83.7428)  time: 0.0290  data: 0.0002  max mem: 5508
[20:56:06.832303] Test:  [140/157]  eta: 0:00:00  testing_loss: 2.0015 (1.9827)  acc1: 31.2500 (31.3276)  acc5: 82.8125 (83.7434)  time: 0.0284  data: 0.0002  max mem: 5508
[20:56:07.113696] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.9779 (1.9812)  acc1: 31.2500 (31.4880)  acc5: 84.3750 (83.8783)  time: 0.0281  data: 0.0001  max mem: 5508
[20:56:07.404700] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.9648 (1.9801)  acc1: 32.8125 (31.3900)  acc5: 84.3750 (83.9200)  time: 0.0341  data: 0.0001  max mem: 5508
[20:56:07.558557] Test: Total time: 0:00:05 (0.0342 s / it)
[20:56:07.559326] * Acc@1 31.390 Acc@5 83.920 loss 1.980
[20:56:07.559682] Accuracy of the network on the 10000 test images: 31.4%
[20:56:07.560143] Max accuracy: 31.39%
[20:56:08.059445] log_dir: ./output_dir
[20:56:08.958950] Epoch: [1]  [  0/781]  eta: 0:11:41  lr: 0.000050  training_loss: 2.4665 (2.4665)  mae_loss: 0.1180 (0.1180)  classification_loss: 2.1933 (2.1933)  loss_mask: 0.1552 (0.1552)  time: 0.8977  data: 0.6362  max mem: 5511
[20:56:12.876029] Epoch: [1]  [ 20/781]  eta: 0:02:54  lr: 0.000051  training_loss: 2.4751 (2.4859)  mae_loss: 0.1201 (0.1213)  classification_loss: 2.1936 (2.2102)  loss_mask: 0.1546 (0.1544)  time: 0.1958  data: 0.0002  max mem: 5511
[20:56:16.837151] Epoch: [1]  [ 40/781]  eta: 0:02:38  lr: 0.000053  training_loss: 2.4794 (2.4932)  mae_loss: 0.1127 (0.1192)  classification_loss: 2.2125 (2.2193)  loss_mask: 0.1541 (0.1547)  time: 0.1980  data: 0.0003  max mem: 5511
[20:56:20.794383] Epoch: [1]  [ 60/781]  eta: 0:02:30  lr: 0.000054  training_loss: 2.4937 (2.4910)  mae_loss: 0.1175 (0.1182)  classification_loss: 2.2163 (2.2177)  loss_mask: 0.1552 (0.1551)  time: 0.1978  data: 0.0003  max mem: 5511
[20:56:24.721756] Epoch: [1]  [ 80/781]  eta: 0:02:24  lr: 0.000055  training_loss: 2.4753 (2.4925)  mae_loss: 0.1164 (0.1181)  classification_loss: 2.2153 (2.2192)  loss_mask: 0.1533 (0.1553)  time: 0.1963  data: 0.0003  max mem: 5511
[20:56:28.651550] Epoch: [1]  [100/781]  eta: 0:02:18  lr: 0.000056  training_loss: 2.4526 (2.4902)  mae_loss: 0.1162 (0.1177)  classification_loss: 2.1926 (2.2172)  loss_mask: 0.1546 (0.1553)  time: 0.1964  data: 0.0002  max mem: 5511
[20:56:32.570155] Epoch: [1]  [120/781]  eta: 0:02:13  lr: 0.000058  training_loss: 2.4748 (2.4867)  mae_loss: 0.1119 (0.1167)  classification_loss: 2.2083 (2.2147)  loss_mask: 0.1556 (0.1553)  time: 0.1958  data: 0.0002  max mem: 5511
[20:56:36.496270] Epoch: [1]  [140/781]  eta: 0:02:09  lr: 0.000059  training_loss: 2.4246 (2.4801)  mae_loss: 0.1085 (0.1160)  classification_loss: 2.1688 (2.2087)  loss_mask: 0.1564 (0.1555)  time: 0.1962  data: 0.0003  max mem: 5511
[20:56:40.413677] Epoch: [1]  [160/781]  eta: 0:02:04  lr: 0.000060  training_loss: 2.4712 (2.4782)  mae_loss: 0.1168 (0.1158)  classification_loss: 2.1964 (2.2069)  loss_mask: 0.1546 (0.1554)  time: 0.1958  data: 0.0002  max mem: 5511
[20:56:44.344926] Epoch: [1]  [180/781]  eta: 0:02:00  lr: 0.000062  training_loss: 2.4669 (2.4776)  mae_loss: 0.1125 (0.1157)  classification_loss: 2.2022 (2.2065)  loss_mask: 0.1543 (0.1554)  time: 0.1965  data: 0.0003  max mem: 5511
[20:56:48.278053] Epoch: [1]  [200/781]  eta: 0:01:56  lr: 0.000063  training_loss: 2.4564 (2.4757)  mae_loss: 0.1147 (0.1154)  classification_loss: 2.1841 (2.2050)  loss_mask: 0.1539 (0.1553)  time: 0.1966  data: 0.0002  max mem: 5511
[20:56:52.210310] Epoch: [1]  [220/781]  eta: 0:01:52  lr: 0.000064  training_loss: 2.4650 (2.4734)  mae_loss: 0.1145 (0.1153)  classification_loss: 2.1894 (2.2027)  loss_mask: 0.1543 (0.1553)  time: 0.1965  data: 0.0002  max mem: 5511
[20:56:56.174953] Epoch: [1]  [240/781]  eta: 0:01:47  lr: 0.000065  training_loss: 2.4417 (2.4712)  mae_loss: 0.1153 (0.1153)  classification_loss: 2.1778 (2.2006)  loss_mask: 0.1552 (0.1553)  time: 0.1981  data: 0.0002  max mem: 5511
[20:57:00.153265] Epoch: [1]  [260/781]  eta: 0:01:43  lr: 0.000067  training_loss: 2.4318 (2.4689)  mae_loss: 0.1146 (0.1152)  classification_loss: 2.1498 (2.1984)  loss_mask: 0.1557 (0.1553)  time: 0.1988  data: 0.0003  max mem: 5511
[20:57:04.111802] Epoch: [1]  [280/781]  eta: 0:01:39  lr: 0.000068  training_loss: 2.4685 (2.4684)  mae_loss: 0.1143 (0.1152)  classification_loss: 2.2005 (2.1981)  loss_mask: 0.1530 (0.1552)  time: 0.1978  data: 0.0003  max mem: 5511
[20:57:08.045126] Epoch: [1]  [300/781]  eta: 0:01:35  lr: 0.000069  training_loss: 2.4382 (2.4672)  mae_loss: 0.1148 (0.1151)  classification_loss: 2.1658 (2.1970)  loss_mask: 0.1535 (0.1551)  time: 0.1966  data: 0.0002  max mem: 5511
[20:57:11.963097] Epoch: [1]  [320/781]  eta: 0:01:31  lr: 0.000070  training_loss: 2.4137 (2.4649)  mae_loss: 0.1083 (0.1147)  classification_loss: 2.1448 (2.1951)  loss_mask: 0.1544 (0.1550)  time: 0.1958  data: 0.0002  max mem: 5511
[20:57:15.934566] Epoch: [1]  [340/781]  eta: 0:01:27  lr: 0.000072  training_loss: 2.4529 (2.4639)  mae_loss: 0.1145 (0.1147)  classification_loss: 2.1797 (2.1942)  loss_mask: 0.1540 (0.1550)  time: 0.1985  data: 0.0003  max mem: 5511
[20:57:19.894291] Epoch: [1]  [360/781]  eta: 0:01:23  lr: 0.000073  training_loss: 2.4709 (2.4636)  mae_loss: 0.1152 (0.1147)  classification_loss: 2.1933 (2.1941)  loss_mask: 0.1527 (0.1548)  time: 0.1979  data: 0.0003  max mem: 5511
[20:57:23.805476] Epoch: [1]  [380/781]  eta: 0:01:19  lr: 0.000074  training_loss: 2.4390 (2.4631)  mae_loss: 0.1110 (0.1146)  classification_loss: 2.1688 (2.1939)  loss_mask: 0.1522 (0.1547)  time: 0.1955  data: 0.0002  max mem: 5511
[20:57:27.750755] Epoch: [1]  [400/781]  eta: 0:01:15  lr: 0.000076  training_loss: 2.4266 (2.4616)  mae_loss: 0.1190 (0.1148)  classification_loss: 2.1564 (2.1923)  loss_mask: 0.1504 (0.1545)  time: 0.1972  data: 0.0002  max mem: 5511
[20:57:31.689961] Epoch: [1]  [420/781]  eta: 0:01:11  lr: 0.000077  training_loss: 2.4169 (2.4594)  mae_loss: 0.1111 (0.1147)  classification_loss: 2.1660 (2.1904)  loss_mask: 0.1496 (0.1543)  time: 0.1969  data: 0.0002  max mem: 5511
[20:57:35.629609] Epoch: [1]  [440/781]  eta: 0:01:07  lr: 0.000078  training_loss: 2.4099 (2.4578)  mae_loss: 0.1134 (0.1147)  classification_loss: 2.1314 (2.1889)  loss_mask: 0.1522 (0.1542)  time: 0.1969  data: 0.0002  max mem: 5511
[20:57:39.588331] Epoch: [1]  [460/781]  eta: 0:01:03  lr: 0.000079  training_loss: 2.4109 (2.4554)  mae_loss: 0.1128 (0.1146)  classification_loss: 2.1466 (2.1867)  loss_mask: 0.1508 (0.1540)  time: 0.1978  data: 0.0003  max mem: 5511
[20:57:43.579430] Epoch: [1]  [480/781]  eta: 0:00:59  lr: 0.000081  training_loss: 2.4332 (2.4544)  mae_loss: 0.1098 (0.1144)  classification_loss: 2.1816 (2.1862)  loss_mask: 0.1477 (0.1539)  time: 0.1994  data: 0.0002  max mem: 5511
[20:57:47.532319] Epoch: [1]  [500/781]  eta: 0:00:55  lr: 0.000082  training_loss: 2.4163 (2.4529)  mae_loss: 0.1122 (0.1144)  classification_loss: 2.1619 (2.1849)  loss_mask: 0.1485 (0.1537)  time: 0.1976  data: 0.0004  max mem: 5511
[20:57:51.456475] Epoch: [1]  [520/781]  eta: 0:00:51  lr: 0.000083  training_loss: 2.3654 (2.4505)  mae_loss: 0.1127 (0.1143)  classification_loss: 2.1176 (2.1827)  loss_mask: 0.1479 (0.1535)  time: 0.1961  data: 0.0002  max mem: 5511
[20:57:55.372598] Epoch: [1]  [540/781]  eta: 0:00:47  lr: 0.000085  training_loss: 2.4201 (2.4495)  mae_loss: 0.1100 (0.1142)  classification_loss: 2.1556 (2.1820)  loss_mask: 0.1475 (0.1533)  time: 0.1957  data: 0.0002  max mem: 5511
[20:57:59.304707] Epoch: [1]  [560/781]  eta: 0:00:43  lr: 0.000086  training_loss: 2.4137 (2.4482)  mae_loss: 0.1122 (0.1142)  classification_loss: 2.1594 (2.1809)  loss_mask: 0.1464 (0.1531)  time: 0.1965  data: 0.0002  max mem: 5511
[20:58:03.206158] Epoch: [1]  [580/781]  eta: 0:00:39  lr: 0.000087  training_loss: 2.3936 (2.4469)  mae_loss: 0.1118 (0.1142)  classification_loss: 2.1385 (2.1798)  loss_mask: 0.1466 (0.1528)  time: 0.1950  data: 0.0003  max mem: 5511
[20:58:07.136048] Epoch: [1]  [600/781]  eta: 0:00:35  lr: 0.000088  training_loss: 2.3850 (2.4452)  mae_loss: 0.1113 (0.1141)  classification_loss: 2.1234 (2.1784)  loss_mask: 0.1487 (0.1527)  time: 0.1964  data: 0.0002  max mem: 5511
[20:58:11.069182] Epoch: [1]  [620/781]  eta: 0:00:31  lr: 0.000090  training_loss: 2.3795 (2.4430)  mae_loss: 0.1113 (0.1140)  classification_loss: 2.1271 (2.1765)  loss_mask: 0.1454 (0.1524)  time: 0.1966  data: 0.0002  max mem: 5511
[20:58:15.038031] Epoch: [1]  [640/781]  eta: 0:00:27  lr: 0.000091  training_loss: 2.3741 (2.4413)  mae_loss: 0.1124 (0.1141)  classification_loss: 2.1148 (2.1750)  loss_mask: 0.1423 (0.1521)  time: 0.1983  data: 0.0002  max mem: 5511
[20:58:18.968338] Epoch: [1]  [660/781]  eta: 0:00:23  lr: 0.000092  training_loss: 2.3711 (2.4393)  mae_loss: 0.1110 (0.1140)  classification_loss: 2.1320 (2.1733)  loss_mask: 0.1428 (0.1519)  time: 0.1964  data: 0.0002  max mem: 5511
[20:58:22.896913] Epoch: [1]  [680/781]  eta: 0:00:19  lr: 0.000094  training_loss: 2.4053 (2.4380)  mae_loss: 0.1102 (0.1140)  classification_loss: 2.1373 (2.1724)  loss_mask: 0.1424 (0.1516)  time: 0.1963  data: 0.0003  max mem: 5511
[20:58:26.861130] Epoch: [1]  [700/781]  eta: 0:00:16  lr: 0.000095  training_loss: 2.3922 (2.4365)  mae_loss: 0.1119 (0.1139)  classification_loss: 2.1336 (2.1712)  loss_mask: 0.1433 (0.1514)  time: 0.1981  data: 0.0003  max mem: 5511
[20:58:30.801062] Epoch: [1]  [720/781]  eta: 0:00:12  lr: 0.000096  training_loss: 2.3807 (2.4351)  mae_loss: 0.1049 (0.1137)  classification_loss: 2.1252 (2.1702)  loss_mask: 0.1453 (0.1513)  time: 0.1969  data: 0.0003  max mem: 5511
[20:58:34.725933] Epoch: [1]  [740/781]  eta: 0:00:08  lr: 0.000097  training_loss: 2.3834 (2.4340)  mae_loss: 0.1051 (0.1136)  classification_loss: 2.1299 (2.1693)  loss_mask: 0.1422 (0.1511)  time: 0.1962  data: 0.0002  max mem: 5511
[20:58:38.649042] Epoch: [1]  [760/781]  eta: 0:00:04  lr: 0.000099  training_loss: 2.3555 (2.4322)  mae_loss: 0.1069 (0.1135)  classification_loss: 2.1033 (2.1679)  loss_mask: 0.1390 (0.1508)  time: 0.1961  data: 0.0002  max mem: 5511
[20:58:42.568592] Epoch: [1]  [780/781]  eta: 0:00:00  lr: 0.000100  training_loss: 2.4058 (2.4314)  mae_loss: 0.1110 (0.1134)  classification_loss: 2.1580 (2.1675)  loss_mask: 0.1407 (0.1505)  time: 0.1959  data: 0.0002  max mem: 5511
[20:58:42.694842] Epoch: [1] Total time: 0:02:34 (0.1980 s / it)
[20:58:42.695334] Averaged stats: lr: 0.000100  training_loss: 2.4058 (2.4314)  mae_loss: 0.1110 (0.1134)  classification_loss: 2.1580 (2.1675)  loss_mask: 0.1407 (0.1505)
[20:58:43.334196] Test:  [  0/157]  eta: 0:01:39  testing_loss: 1.7136 (1.7136)  acc1: 40.6250 (40.6250)  acc5: 84.3750 (84.3750)  time: 0.6338  data: 0.5944  max mem: 5511
[20:58:43.629723] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.8273 (1.8068)  acc1: 39.0625 (38.4943)  acc5: 87.5000 (87.9261)  time: 0.0842  data: 0.0545  max mem: 5511
[20:58:43.920085] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.8028 (1.7951)  acc1: 40.6250 (40.9226)  acc5: 87.5000 (87.0536)  time: 0.0291  data: 0.0003  max mem: 5511
[20:58:44.203931] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.7771 (1.7883)  acc1: 40.6250 (40.6754)  acc5: 87.5000 (87.7016)  time: 0.0286  data: 0.0002  max mem: 5511
[20:58:44.494567] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.7906 (1.7963)  acc1: 35.9375 (39.8247)  acc5: 87.5000 (87.1189)  time: 0.0286  data: 0.0002  max mem: 5511
[20:58:44.778581] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.8148 (1.7976)  acc1: 35.9375 (39.2770)  acc5: 87.5000 (87.1936)  time: 0.0286  data: 0.0002  max mem: 5511
[20:58:45.062341] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.7888 (1.7935)  acc1: 39.0625 (39.7285)  acc5: 87.5000 (87.3719)  time: 0.0283  data: 0.0002  max mem: 5511
[20:58:45.346073] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.7790 (1.7954)  acc1: 39.0625 (39.6567)  acc5: 89.0625 (87.3239)  time: 0.0282  data: 0.0002  max mem: 5511
[20:58:45.630083] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.7790 (1.7936)  acc1: 40.6250 (39.8148)  acc5: 89.0625 (87.6157)  time: 0.0283  data: 0.0002  max mem: 5511
[20:58:45.916501] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.7871 (1.7966)  acc1: 39.0625 (39.5948)  acc5: 89.0625 (87.5515)  time: 0.0284  data: 0.0002  max mem: 5511
[20:58:46.199711] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.8396 (1.8011)  acc1: 35.9375 (39.1708)  acc5: 85.9375 (87.3144)  time: 0.0283  data: 0.0002  max mem: 5511
[20:58:46.484408] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.8362 (1.8026)  acc1: 35.9375 (39.2314)  acc5: 85.9375 (87.2607)  time: 0.0283  data: 0.0002  max mem: 5511
[20:58:46.767828] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.8089 (1.8001)  acc1: 39.0625 (39.2433)  acc5: 89.0625 (87.4096)  time: 0.0283  data: 0.0002  max mem: 5511
[20:58:47.053878] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.7882 (1.8021)  acc1: 37.5000 (39.1460)  acc5: 89.0625 (87.3688)  time: 0.0284  data: 0.0002  max mem: 5511
[20:58:47.335071] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.8094 (1.8011)  acc1: 37.5000 (39.2176)  acc5: 87.5000 (87.3670)  time: 0.0282  data: 0.0002  max mem: 5511
[20:58:47.618529] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.7976 (1.7995)  acc1: 39.0625 (39.3419)  acc5: 85.9375 (87.3241)  time: 0.0281  data: 0.0001  max mem: 5511
[20:58:47.770062] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.7654 (1.7999)  acc1: 39.0625 (39.1900)  acc5: 85.9375 (87.3500)  time: 0.0272  data: 0.0001  max mem: 5511
[20:58:47.912068] Test: Total time: 0:00:05 (0.0332 s / it)
[20:58:47.912930] * Acc@1 39.190 Acc@5 87.350 loss 1.800
[20:58:47.913242] Accuracy of the network on the 10000 test images: 39.2%
[20:58:47.913512] Max accuracy: 39.19%
[20:58:48.153197] log_dir: ./output_dir
[20:58:49.059705] Epoch: [2]  [  0/781]  eta: 0:11:46  lr: 0.000100  training_loss: 2.4226 (2.4226)  mae_loss: 0.1137 (0.1137)  classification_loss: 2.1629 (2.1629)  loss_mask: 0.1460 (0.1460)  time: 0.9045  data: 0.6518  max mem: 5511
[20:58:53.030119] Epoch: [2]  [ 20/781]  eta: 0:02:56  lr: 0.000101  training_loss: 2.3420 (2.3573)  mae_loss: 0.1043 (0.1061)  classification_loss: 2.1075 (2.1118)  loss_mask: 0.1375 (0.1393)  time: 0.1984  data: 0.0002  max mem: 5511
[20:58:56.944842] Epoch: [2]  [ 40/781]  eta: 0:02:38  lr: 0.000103  training_loss: 2.3601 (2.3616)  mae_loss: 0.1107 (0.1071)  classification_loss: 2.1128 (2.1153)  loss_mask: 0.1387 (0.1392)  time: 0.1956  data: 0.0002  max mem: 5511
[20:59:00.848382] Epoch: [2]  [ 60/781]  eta: 0:02:29  lr: 0.000104  training_loss: 2.3493 (2.3606)  mae_loss: 0.1077 (0.1075)  classification_loss: 2.1013 (2.1143)  loss_mask: 0.1384 (0.1387)  time: 0.1951  data: 0.0002  max mem: 5511
[20:59:04.764877] Epoch: [2]  [ 80/781]  eta: 0:02:23  lr: 0.000105  training_loss: 2.3515 (2.3624)  mae_loss: 0.1050 (0.1071)  classification_loss: 2.1191 (2.1177)  loss_mask: 0.1347 (0.1376)  time: 0.1957  data: 0.0002  max mem: 5511
[20:59:08.703729] Epoch: [2]  [100/781]  eta: 0:02:18  lr: 0.000106  training_loss: 2.3693 (2.3618)  mae_loss: 0.0992 (0.1065)  classification_loss: 2.1159 (2.1176)  loss_mask: 0.1351 (0.1377)  time: 0.1967  data: 0.0002  max mem: 5511
[20:59:12.669546] Epoch: [2]  [120/781]  eta: 0:02:13  lr: 0.000108  training_loss: 2.3782 (2.3630)  mae_loss: 0.1051 (0.1061)  classification_loss: 2.1249 (2.1189)  loss_mask: 0.1382 (0.1379)  time: 0.1982  data: 0.0002  max mem: 5511
[20:59:16.596232] Epoch: [2]  [140/781]  eta: 0:02:09  lr: 0.000109  training_loss: 2.3696 (2.3616)  mae_loss: 0.1040 (0.1058)  classification_loss: 2.1295 (2.1186)  loss_mask: 0.1330 (0.1371)  time: 0.1963  data: 0.0002  max mem: 5511
[20:59:20.550994] Epoch: [2]  [160/781]  eta: 0:02:04  lr: 0.000110  training_loss: 2.2990 (2.3573)  mae_loss: 0.1037 (0.1057)  classification_loss: 2.0759 (2.1150)  loss_mask: 0.1315 (0.1366)  time: 0.1976  data: 0.0002  max mem: 5511
[20:59:24.515467] Epoch: [2]  [180/781]  eta: 0:02:00  lr: 0.000112  training_loss: 2.3349 (2.3555)  mae_loss: 0.1068 (0.1059)  classification_loss: 2.1014 (2.1137)  loss_mask: 0.1314 (0.1359)  time: 0.1981  data: 0.0002  max mem: 5511
[20:59:28.435043] Epoch: [2]  [200/781]  eta: 0:01:56  lr: 0.000113  training_loss: 2.3122 (2.3533)  mae_loss: 0.1072 (0.1060)  classification_loss: 2.0774 (2.1117)  loss_mask: 0.1323 (0.1356)  time: 0.1959  data: 0.0002  max mem: 5511
[20:59:32.365353] Epoch: [2]  [220/781]  eta: 0:01:52  lr: 0.000114  training_loss: 2.2962 (2.3489)  mae_loss: 0.1000 (0.1055)  classification_loss: 2.0615 (2.1083)  loss_mask: 0.1325 (0.1352)  time: 0.1964  data: 0.0002  max mem: 5511
[20:59:36.318455] Epoch: [2]  [240/781]  eta: 0:01:48  lr: 0.000115  training_loss: 2.3564 (2.3479)  mae_loss: 0.1031 (0.1054)  classification_loss: 2.1095 (2.1079)  loss_mask: 0.1293 (0.1346)  time: 0.1976  data: 0.0002  max mem: 5511
[20:59:40.244494] Epoch: [2]  [260/781]  eta: 0:01:43  lr: 0.000117  training_loss: 2.3122 (2.3453)  mae_loss: 0.1007 (0.1051)  classification_loss: 2.0854 (2.1063)  loss_mask: 0.1268 (0.1339)  time: 0.1962  data: 0.0003  max mem: 5511
[20:59:44.196888] Epoch: [2]  [280/781]  eta: 0:01:39  lr: 0.000118  training_loss: 2.3175 (2.3444)  mae_loss: 0.1060 (0.1051)  classification_loss: 2.0905 (2.1060)  loss_mask: 0.1263 (0.1333)  time: 0.1975  data: 0.0002  max mem: 5511
[20:59:48.119778] Epoch: [2]  [300/781]  eta: 0:01:35  lr: 0.000119  training_loss: 2.3317 (2.3433)  mae_loss: 0.1002 (0.1049)  classification_loss: 2.1079 (2.1059)  loss_mask: 0.1206 (0.1325)  time: 0.1961  data: 0.0002  max mem: 5511
[20:59:52.043645] Epoch: [2]  [320/781]  eta: 0:01:31  lr: 0.000120  training_loss: 2.2976 (2.3410)  mae_loss: 0.1001 (0.1047)  classification_loss: 2.0707 (2.1045)  loss_mask: 0.1181 (0.1318)  time: 0.1961  data: 0.0001  max mem: 5511
[20:59:55.999985] Epoch: [2]  [340/781]  eta: 0:01:27  lr: 0.000122  training_loss: 2.3289 (2.3401)  mae_loss: 0.0996 (0.1044)  classification_loss: 2.1102 (2.1045)  loss_mask: 0.1207 (0.1311)  time: 0.1977  data: 0.0003  max mem: 5511
[20:59:59.986156] Epoch: [2]  [360/781]  eta: 0:01:23  lr: 0.000123  training_loss: 2.2739 (2.3376)  mae_loss: 0.1017 (0.1043)  classification_loss: 2.0575 (2.1026)  loss_mask: 0.1217 (0.1306)  time: 0.1992  data: 0.0002  max mem: 5511
[21:00:03.916776] Epoch: [2]  [380/781]  eta: 0:01:19  lr: 0.000124  training_loss: 2.3004 (2.3366)  mae_loss: 0.1009 (0.1041)  classification_loss: 2.0871 (2.1023)  loss_mask: 0.1218 (0.1301)  time: 0.1964  data: 0.0002  max mem: 5511
[21:00:07.847760] Epoch: [2]  [400/781]  eta: 0:01:15  lr: 0.000126  training_loss: 2.2721 (2.3340)  mae_loss: 0.0980 (0.1039)  classification_loss: 2.0490 (2.1006)  loss_mask: 0.1203 (0.1295)  time: 0.1965  data: 0.0002  max mem: 5511
[21:00:11.811658] Epoch: [2]  [420/781]  eta: 0:01:11  lr: 0.000127  training_loss: 2.2870 (2.3315)  mae_loss: 0.1032 (0.1039)  classification_loss: 2.0694 (2.0989)  loss_mask: 0.1118 (0.1288)  time: 0.1981  data: 0.0003  max mem: 5511
[21:00:15.734079] Epoch: [2]  [440/781]  eta: 0:01:07  lr: 0.000128  training_loss: 2.3062 (2.3304)  mae_loss: 0.1044 (0.1040)  classification_loss: 2.0505 (2.0981)  loss_mask: 0.1173 (0.1283)  time: 0.1960  data: 0.0002  max mem: 5511
[21:00:19.667892] Epoch: [2]  [460/781]  eta: 0:01:03  lr: 0.000129  training_loss: 2.2701 (2.3281)  mae_loss: 0.0982 (0.1037)  classification_loss: 2.0457 (2.0966)  loss_mask: 0.1148 (0.1277)  time: 0.1966  data: 0.0002  max mem: 5511
[21:00:23.634609] Epoch: [2]  [480/781]  eta: 0:00:59  lr: 0.000131  training_loss: 2.2743 (2.3268)  mae_loss: 0.1019 (0.1037)  classification_loss: 2.0576 (2.0960)  loss_mask: 0.1109 (0.1270)  time: 0.1983  data: 0.0002  max mem: 5511
[21:00:27.559481] Epoch: [2]  [500/781]  eta: 0:00:55  lr: 0.000132  training_loss: 2.2784 (2.3251)  mae_loss: 0.1009 (0.1036)  classification_loss: 2.0746 (2.0949)  loss_mask: 0.1101 (0.1266)  time: 0.1961  data: 0.0002  max mem: 5511
[21:00:31.488312] Epoch: [2]  [520/781]  eta: 0:00:51  lr: 0.000133  training_loss: 2.2591 (2.3234)  mae_loss: 0.1006 (0.1035)  classification_loss: 2.0574 (2.0940)  loss_mask: 0.1049 (0.1258)  time: 0.1963  data: 0.0002  max mem: 5511
[21:00:35.410545] Epoch: [2]  [540/781]  eta: 0:00:47  lr: 0.000135  training_loss: 2.2619 (2.3227)  mae_loss: 0.0987 (0.1034)  classification_loss: 2.0595 (2.0939)  loss_mask: 0.1127 (0.1253)  time: 0.1960  data: 0.0002  max mem: 5511
[21:00:39.346810] Epoch: [2]  [560/781]  eta: 0:00:43  lr: 0.000136  training_loss: 2.2600 (2.3206)  mae_loss: 0.0985 (0.1033)  classification_loss: 2.0377 (2.0925)  loss_mask: 0.1109 (0.1248)  time: 0.1967  data: 0.0002  max mem: 5511
[21:00:43.303528] Epoch: [2]  [580/781]  eta: 0:00:39  lr: 0.000137  training_loss: 2.2552 (2.3190)  mae_loss: 0.0942 (0.1031)  classification_loss: 2.0585 (2.0916)  loss_mask: 0.1073 (0.1243)  time: 0.1978  data: 0.0002  max mem: 5511
[21:00:47.239351] Epoch: [2]  [600/781]  eta: 0:00:35  lr: 0.000138  training_loss: 2.2585 (2.3176)  mae_loss: 0.0996 (0.1030)  classification_loss: 2.0611 (2.0908)  loss_mask: 0.1095 (0.1237)  time: 0.1967  data: 0.0003  max mem: 5511
[21:00:51.167079] Epoch: [2]  [620/781]  eta: 0:00:31  lr: 0.000140  training_loss: 2.2496 (2.3155)  mae_loss: 0.0972 (0.1028)  classification_loss: 2.0499 (2.0895)  loss_mask: 0.1052 (0.1232)  time: 0.1963  data: 0.0002  max mem: 5511
[21:00:55.126550] Epoch: [2]  [640/781]  eta: 0:00:27  lr: 0.000141  training_loss: 2.2466 (2.3135)  mae_loss: 0.0998 (0.1027)  classification_loss: 2.0427 (2.0881)  loss_mask: 0.1044 (0.1227)  time: 0.1979  data: 0.0002  max mem: 5511
[21:00:59.069812] Epoch: [2]  [660/781]  eta: 0:00:23  lr: 0.000142  training_loss: 2.2632 (2.3115)  mae_loss: 0.0942 (0.1025)  classification_loss: 2.0591 (2.0868)  loss_mask: 0.1022 (0.1222)  time: 0.1971  data: 0.0002  max mem: 5511
[21:01:03.022252] Epoch: [2]  [680/781]  eta: 0:00:19  lr: 0.000144  training_loss: 2.2523 (2.3098)  mae_loss: 0.0973 (0.1024)  classification_loss: 2.0404 (2.0856)  loss_mask: 0.1059 (0.1217)  time: 0.1975  data: 0.0002  max mem: 5511
[21:01:06.947500] Epoch: [2]  [700/781]  eta: 0:00:16  lr: 0.000145  training_loss: 2.2431 (2.3084)  mae_loss: 0.0966 (0.1023)  classification_loss: 2.0309 (2.0846)  loss_mask: 0.1089 (0.1215)  time: 0.1962  data: 0.0002  max mem: 5511
[21:01:10.885067] Epoch: [2]  [720/781]  eta: 0:00:12  lr: 0.000146  training_loss: 2.2454 (2.3072)  mae_loss: 0.0952 (0.1022)  classification_loss: 2.0453 (2.0840)  loss_mask: 0.1070 (0.1210)  time: 0.1968  data: 0.0002  max mem: 5511
[21:01:14.817960] Epoch: [2]  [740/781]  eta: 0:00:08  lr: 0.000147  training_loss: 2.2677 (2.3060)  mae_loss: 0.0934 (0.1020)  classification_loss: 2.0494 (2.0834)  loss_mask: 0.1034 (0.1206)  time: 0.1965  data: 0.0002  max mem: 5511
[21:01:18.757290] Epoch: [2]  [760/781]  eta: 0:00:04  lr: 0.000149  training_loss: 2.2463 (2.3049)  mae_loss: 0.0942 (0.1018)  classification_loss: 2.0605 (2.0829)  loss_mask: 0.1045 (0.1202)  time: 0.1969  data: 0.0002  max mem: 5511
[21:01:22.654119] Epoch: [2]  [780/781]  eta: 0:00:00  lr: 0.000150  training_loss: 2.2378 (2.3034)  mae_loss: 0.1015 (0.1018)  classification_loss: 2.0191 (2.0817)  loss_mask: 0.1043 (0.1199)  time: 0.1948  data: 0.0001  max mem: 5511
[21:01:22.814652] Epoch: [2] Total time: 0:02:34 (0.1980 s / it)
[21:01:22.815107] Averaged stats: lr: 0.000150  training_loss: 2.2378 (2.3034)  mae_loss: 0.1015 (0.1018)  classification_loss: 2.0191 (2.0817)  loss_mask: 0.1043 (0.1199)
[21:01:23.434106] Test:  [  0/157]  eta: 0:01:36  testing_loss: 1.5480 (1.5480)  acc1: 45.3125 (45.3125)  acc5: 90.6250 (90.6250)  time: 0.6139  data: 0.5843  max mem: 5511
[21:01:23.720218] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.5758 (1.6035)  acc1: 43.7500 (43.8920)  acc5: 92.1875 (92.6136)  time: 0.0817  data: 0.0533  max mem: 5511
[21:01:24.005756] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.5733 (1.5845)  acc1: 46.8750 (47.6935)  acc5: 92.1875 (91.4435)  time: 0.0284  data: 0.0002  max mem: 5511
[21:01:24.291507] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.5699 (1.5810)  acc1: 50.0000 (47.8327)  acc5: 90.6250 (91.3810)  time: 0.0284  data: 0.0002  max mem: 5511
[21:01:24.583886] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.5852 (1.5876)  acc1: 46.8750 (47.4848)  acc5: 90.6250 (91.2348)  time: 0.0287  data: 0.0003  max mem: 5511
[21:01:24.873497] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.5915 (1.5867)  acc1: 46.8750 (47.5184)  acc5: 90.6250 (91.2377)  time: 0.0289  data: 0.0003  max mem: 5511
[21:01:25.160476] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.5633 (1.5794)  acc1: 48.4375 (47.9508)  acc5: 92.1875 (91.5215)  time: 0.0287  data: 0.0002  max mem: 5511
[21:01:25.446258] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.5446 (1.5782)  acc1: 48.4375 (48.1514)  acc5: 92.1875 (91.6153)  time: 0.0285  data: 0.0002  max mem: 5511
[21:01:25.733416] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.5628 (1.5787)  acc1: 46.8750 (48.1481)  acc5: 92.1875 (91.5702)  time: 0.0285  data: 0.0002  max mem: 5511
[21:01:26.017710] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.5921 (1.5819)  acc1: 46.8750 (48.2143)  acc5: 90.6250 (91.5522)  time: 0.0284  data: 0.0002  max mem: 5511
[21:01:26.301298] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.6114 (1.5858)  acc1: 46.8750 (47.8342)  acc5: 92.1875 (91.6460)  time: 0.0283  data: 0.0002  max mem: 5511
[21:01:26.590885] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.6048 (1.5874)  acc1: 45.3125 (47.8604)  acc5: 92.1875 (91.6385)  time: 0.0285  data: 0.0002  max mem: 5511
[21:01:26.873770] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.5585 (1.5848)  acc1: 51.5625 (48.1405)  acc5: 90.6250 (91.6451)  time: 0.0285  data: 0.0002  max mem: 5511
[21:01:27.158330] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.5649 (1.5875)  acc1: 48.4375 (47.9604)  acc5: 90.6250 (91.5434)  time: 0.0282  data: 0.0002  max mem: 5511
[21:01:27.441202] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.5695 (1.5876)  acc1: 48.4375 (48.0940)  acc5: 90.6250 (91.5226)  time: 0.0283  data: 0.0002  max mem: 5511
[21:01:27.721622] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.5695 (1.5861)  acc1: 50.0000 (48.1892)  acc5: 90.6250 (91.4425)  time: 0.0280  data: 0.0001  max mem: 5511
[21:01:27.871323] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.5606 (1.5867)  acc1: 46.8750 (48.0800)  acc5: 90.6250 (91.4500)  time: 0.0270  data: 0.0001  max mem: 5511
[21:01:28.036067] Test: Total time: 0:00:05 (0.0332 s / it)
[21:01:28.036519] * Acc@1 48.080 Acc@5 91.450 loss 1.587
[21:01:28.036868] Accuracy of the network on the 10000 test images: 48.1%
[21:01:28.037944] Max accuracy: 48.08%
[21:01:28.242914] log_dir: ./output_dir
[21:01:29.090126] Epoch: [3]  [  0/781]  eta: 0:11:00  lr: 0.000150  training_loss: 2.1486 (2.1486)  mae_loss: 0.0897 (0.0897)  classification_loss: 1.9456 (1.9456)  loss_mask: 0.1133 (0.1133)  time: 0.8456  data: 0.6212  max mem: 5511
[21:01:33.018930] Epoch: [3]  [ 20/781]  eta: 0:02:52  lr: 0.000151  training_loss: 2.2251 (2.2231)  mae_loss: 0.0936 (0.0965)  classification_loss: 2.0070 (2.0204)  loss_mask: 0.1038 (0.1062)  time: 0.1963  data: 0.0003  max mem: 5511
[21:01:36.943535] Epoch: [3]  [ 40/781]  eta: 0:02:37  lr: 0.000153  training_loss: 2.2900 (2.2577)  mae_loss: 0.0997 (0.0981)  classification_loss: 2.0844 (2.0524)  loss_mask: 0.1043 (0.1072)  time: 0.1962  data: 0.0003  max mem: 5511
[21:01:40.878138] Epoch: [3]  [ 60/781]  eta: 0:02:29  lr: 0.000154  training_loss: 2.2441 (2.2538)  mae_loss: 0.0949 (0.0974)  classification_loss: 2.0433 (2.0522)  loss_mask: 0.0983 (0.1043)  time: 0.1966  data: 0.0002  max mem: 5511
[21:01:44.835206] Epoch: [3]  [ 80/781]  eta: 0:02:23  lr: 0.000155  training_loss: 2.2249 (2.2527)  mae_loss: 0.0950 (0.0971)  classification_loss: 2.0241 (2.0517)  loss_mask: 0.1042 (0.1039)  time: 0.1977  data: 0.0002  max mem: 5511
[21:01:48.774947] Epoch: [3]  [100/781]  eta: 0:02:18  lr: 0.000156  training_loss: 2.2700 (2.2572)  mae_loss: 0.0966 (0.0965)  classification_loss: 2.0775 (2.0575)  loss_mask: 0.0985 (0.1032)  time: 0.1969  data: 0.0002  max mem: 5511
[21:01:52.733208] Epoch: [3]  [120/781]  eta: 0:02:13  lr: 0.000158  training_loss: 2.2047 (2.2521)  mae_loss: 0.0928 (0.0960)  classification_loss: 2.0075 (2.0524)  loss_mask: 0.1068 (0.1036)  time: 0.1978  data: 0.0004  max mem: 5511
[21:01:56.685510] Epoch: [3]  [140/781]  eta: 0:02:09  lr: 0.000159  training_loss: 2.2070 (2.2496)  mae_loss: 0.0920 (0.0957)  classification_loss: 2.0031 (2.0507)  loss_mask: 0.0999 (0.1033)  time: 0.1975  data: 0.0002  max mem: 5511
[21:02:00.628593] Epoch: [3]  [160/781]  eta: 0:02:04  lr: 0.000160  training_loss: 2.2595 (2.2497)  mae_loss: 0.0923 (0.0954)  classification_loss: 2.0779 (2.0515)  loss_mask: 0.0985 (0.1028)  time: 0.1971  data: 0.0001  max mem: 5511
[21:02:04.577765] Epoch: [3]  [180/781]  eta: 0:02:00  lr: 0.000162  training_loss: 2.2473 (2.2477)  mae_loss: 0.0936 (0.0954)  classification_loss: 2.0504 (2.0497)  loss_mask: 0.0972 (0.1026)  time: 0.1973  data: 0.0002  max mem: 5511
[21:02:08.536164] Epoch: [3]  [200/781]  eta: 0:01:56  lr: 0.000163  training_loss: 2.2479 (2.2463)  mae_loss: 0.0962 (0.0955)  classification_loss: 2.0444 (2.0485)  loss_mask: 0.0977 (0.1023)  time: 0.1978  data: 0.0002  max mem: 5511
[21:02:12.489451] Epoch: [3]  [220/781]  eta: 0:01:52  lr: 0.000164  training_loss: 2.2344 (2.2450)  mae_loss: 0.0962 (0.0957)  classification_loss: 2.0408 (2.0472)  loss_mask: 0.1006 (0.1021)  time: 0.1976  data: 0.0002  max mem: 5511
[21:02:16.437965] Epoch: [3]  [240/781]  eta: 0:01:48  lr: 0.000165  training_loss: 2.2345 (2.2447)  mae_loss: 0.0934 (0.0957)  classification_loss: 2.0331 (2.0471)  loss_mask: 0.0965 (0.1019)  time: 0.1974  data: 0.0002  max mem: 5511
[21:02:20.372979] Epoch: [3]  [260/781]  eta: 0:01:44  lr: 0.000167  training_loss: 2.1641 (2.2402)  mae_loss: 0.0946 (0.0957)  classification_loss: 1.9620 (2.0429)  loss_mask: 0.0985 (0.1016)  time: 0.1967  data: 0.0002  max mem: 5511
[21:02:24.347092] Epoch: [3]  [280/781]  eta: 0:01:39  lr: 0.000168  training_loss: 2.2271 (2.2408)  mae_loss: 0.0945 (0.0956)  classification_loss: 2.0371 (2.0436)  loss_mask: 0.0972 (0.1017)  time: 0.1986  data: 0.0003  max mem: 5511
[21:02:28.278107] Epoch: [3]  [300/781]  eta: 0:01:35  lr: 0.000169  training_loss: 2.2165 (2.2403)  mae_loss: 0.0944 (0.0955)  classification_loss: 2.0186 (2.0431)  loss_mask: 0.1028 (0.1017)  time: 0.1965  data: 0.0002  max mem: 5511
[21:02:32.223236] Epoch: [3]  [320/781]  eta: 0:01:31  lr: 0.000170  training_loss: 2.1971 (2.2376)  mae_loss: 0.0911 (0.0953)  classification_loss: 1.9811 (2.0403)  loss_mask: 0.1043 (0.1021)  time: 0.1972  data: 0.0002  max mem: 5511
[21:02:36.181016] Epoch: [3]  [340/781]  eta: 0:01:27  lr: 0.000172  training_loss: 2.2011 (2.2358)  mae_loss: 0.0922 (0.0952)  classification_loss: 2.0105 (2.0384)  loss_mask: 0.1022 (0.1022)  time: 0.1978  data: 0.0002  max mem: 5511
[21:02:40.128959] Epoch: [3]  [360/781]  eta: 0:01:23  lr: 0.000173  training_loss: 2.1780 (2.2335)  mae_loss: 0.0882 (0.0949)  classification_loss: 1.9975 (2.0370)  loss_mask: 0.0910 (0.1016)  time: 0.1973  data: 0.0002  max mem: 5511
[21:02:44.075624] Epoch: [3]  [380/781]  eta: 0:01:19  lr: 0.000174  training_loss: 2.2095 (2.2318)  mae_loss: 0.0932 (0.0948)  classification_loss: 2.0150 (2.0357)  loss_mask: 0.0939 (0.1013)  time: 0.1973  data: 0.0002  max mem: 5511
[21:02:48.032075] Epoch: [3]  [400/781]  eta: 0:01:15  lr: 0.000176  training_loss: 2.2076 (2.2303)  mae_loss: 0.0916 (0.0946)  classification_loss: 2.0121 (2.0345)  loss_mask: 0.0959 (0.1012)  time: 0.1977  data: 0.0002  max mem: 5511
[21:02:51.983870] Epoch: [3]  [420/781]  eta: 0:01:11  lr: 0.000177  training_loss: 2.1507 (2.2277)  mae_loss: 0.0890 (0.0944)  classification_loss: 1.9838 (2.0325)  loss_mask: 0.0925 (0.1008)  time: 0.1975  data: 0.0002  max mem: 5511
[21:02:55.908227] Epoch: [3]  [440/781]  eta: 0:01:07  lr: 0.000178  training_loss: 2.1834 (2.2245)  mae_loss: 0.0923 (0.0942)  classification_loss: 1.9871 (2.0298)  loss_mask: 0.0928 (0.1005)  time: 0.1961  data: 0.0002  max mem: 5511
[21:02:59.841094] Epoch: [3]  [460/781]  eta: 0:01:03  lr: 0.000179  training_loss: 2.1980 (2.2235)  mae_loss: 0.0860 (0.0938)  classification_loss: 2.0207 (2.0295)  loss_mask: 0.0908 (0.1001)  time: 0.1966  data: 0.0002  max mem: 5511
[21:03:03.758133] Epoch: [3]  [480/781]  eta: 0:00:59  lr: 0.000181  training_loss: 2.1730 (2.2214)  mae_loss: 0.0894 (0.0936)  classification_loss: 1.9977 (2.0280)  loss_mask: 0.0899 (0.0998)  time: 0.1957  data: 0.0002  max mem: 5511
[21:03:07.682017] Epoch: [3]  [500/781]  eta: 0:00:55  lr: 0.000182  training_loss: 2.1670 (2.2193)  mae_loss: 0.0869 (0.0934)  classification_loss: 2.0004 (2.0266)  loss_mask: 0.0884 (0.0994)  time: 0.1961  data: 0.0002  max mem: 5511
[21:03:11.626626] Epoch: [3]  [520/781]  eta: 0:00:51  lr: 0.000183  training_loss: 2.1724 (2.2180)  mae_loss: 0.0811 (0.0930)  classification_loss: 1.9954 (2.0260)  loss_mask: 0.0870 (0.0989)  time: 0.1972  data: 0.0002  max mem: 5511
[21:03:15.545456] Epoch: [3]  [540/781]  eta: 0:00:47  lr: 0.000185  training_loss: 2.2255 (2.2175)  mae_loss: 0.0868 (0.0928)  classification_loss: 2.0370 (2.0259)  loss_mask: 0.0908 (0.0987)  time: 0.1959  data: 0.0002  max mem: 5511
[21:03:19.471253] Epoch: [3]  [560/781]  eta: 0:00:43  lr: 0.000186  training_loss: 2.1619 (2.2158)  mae_loss: 0.0839 (0.0926)  classification_loss: 1.9989 (2.0248)  loss_mask: 0.0855 (0.0983)  time: 0.1962  data: 0.0002  max mem: 5511
[21:03:23.391840] Epoch: [3]  [580/781]  eta: 0:00:39  lr: 0.000187  training_loss: 2.1904 (2.2148)  mae_loss: 0.0814 (0.0922)  classification_loss: 2.0063 (2.0244)  loss_mask: 0.0918 (0.0981)  time: 0.1960  data: 0.0002  max mem: 5511
[21:03:27.313924] Epoch: [3]  [600/781]  eta: 0:00:35  lr: 0.000188  training_loss: 2.1912 (2.2135)  mae_loss: 0.0844 (0.0920)  classification_loss: 1.9968 (2.0234)  loss_mask: 0.0928 (0.0981)  time: 0.1960  data: 0.0003  max mem: 5511
[21:03:31.261226] Epoch: [3]  [620/781]  eta: 0:00:31  lr: 0.000190  training_loss: 2.1504 (2.2114)  mae_loss: 0.0795 (0.0916)  classification_loss: 1.9717 (2.0218)  loss_mask: 0.0914 (0.0980)  time: 0.1973  data: 0.0002  max mem: 5511
[21:03:35.176308] Epoch: [3]  [640/781]  eta: 0:00:27  lr: 0.000191  training_loss: 2.1771 (2.2103)  mae_loss: 0.0782 (0.0912)  classification_loss: 2.0059 (2.0212)  loss_mask: 0.0927 (0.0979)  time: 0.1957  data: 0.0002  max mem: 5511
[21:03:39.143901] Epoch: [3]  [660/781]  eta: 0:00:23  lr: 0.000192  training_loss: 2.1688 (2.2087)  mae_loss: 0.0764 (0.0908)  classification_loss: 1.9982 (2.0202)  loss_mask: 0.0920 (0.0977)  time: 0.1983  data: 0.0002  max mem: 5511
[21:03:43.090465] Epoch: [3]  [680/781]  eta: 0:00:19  lr: 0.000194  training_loss: 2.1384 (2.2069)  mae_loss: 0.0769 (0.0904)  classification_loss: 1.9786 (2.0191)  loss_mask: 0.0890 (0.0974)  time: 0.1972  data: 0.0002  max mem: 5511
[21:03:47.040704] Epoch: [3]  [700/781]  eta: 0:00:16  lr: 0.000195  training_loss: 2.1727 (2.2050)  mae_loss: 0.0754 (0.0899)  classification_loss: 1.9954 (2.0178)  loss_mask: 0.0929 (0.0973)  time: 0.1974  data: 0.0003  max mem: 5511
[21:03:50.993501] Epoch: [3]  [720/781]  eta: 0:00:12  lr: 0.000196  training_loss: 2.1536 (2.2036)  mae_loss: 0.0678 (0.0894)  classification_loss: 1.9684 (2.0170)  loss_mask: 0.0944 (0.0972)  time: 0.1976  data: 0.0002  max mem: 5511
[21:03:54.922828] Epoch: [3]  [740/781]  eta: 0:00:08  lr: 0.000197  training_loss: 2.1356 (2.2015)  mae_loss: 0.0691 (0.0888)  classification_loss: 1.9714 (2.0156)  loss_mask: 0.0894 (0.0971)  time: 0.1964  data: 0.0002  max mem: 5511
[21:03:58.873510] Epoch: [3]  [760/781]  eta: 0:00:04  lr: 0.000199  training_loss: 2.1736 (2.2009)  mae_loss: 0.0676 (0.0883)  classification_loss: 2.0187 (2.0157)  loss_mask: 0.0907 (0.0969)  time: 0.1975  data: 0.0002  max mem: 5511
[21:04:02.799165] Epoch: [3]  [780/781]  eta: 0:00:00  lr: 0.000200  training_loss: 2.1284 (2.1993)  mae_loss: 0.0688 (0.0878)  classification_loss: 1.9805 (2.0149)  loss_mask: 0.0814 (0.0966)  time: 0.1962  data: 0.0002  max mem: 5511
[21:04:02.943022] Epoch: [3] Total time: 0:02:34 (0.1981 s / it)
[21:04:02.943941] Averaged stats: lr: 0.000200  training_loss: 2.1284 (2.1993)  mae_loss: 0.0688 (0.0878)  classification_loss: 1.9805 (2.0149)  loss_mask: 0.0814 (0.0966)
[21:04:03.563860] Test:  [  0/157]  eta: 0:01:36  testing_loss: 1.3838 (1.3838)  acc1: 50.0000 (50.0000)  acc5: 92.1875 (92.1875)  time: 0.6157  data: 0.5839  max mem: 5511
[21:04:03.850128] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.5215 (1.5268)  acc1: 45.3125 (45.3125)  acc5: 92.1875 (91.9034)  time: 0.0818  data: 0.0533  max mem: 5511
[21:04:04.138462] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.5145 (1.5084)  acc1: 46.8750 (47.0982)  acc5: 92.1875 (91.9643)  time: 0.0286  data: 0.0002  max mem: 5511
[21:04:04.429327] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.5162 (1.5099)  acc1: 48.4375 (47.1774)  acc5: 92.1875 (91.9355)  time: 0.0288  data: 0.0002  max mem: 5511
[21:04:04.716458] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.5194 (1.5158)  acc1: 46.8750 (47.1037)  acc5: 92.1875 (91.8445)  time: 0.0288  data: 0.0002  max mem: 5511
[21:04:05.003568] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.5086 (1.5138)  acc1: 46.8750 (47.4571)  acc5: 92.1875 (92.1569)  time: 0.0286  data: 0.0002  max mem: 5511
[21:04:05.293897] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.4768 (1.5060)  acc1: 48.4375 (47.8484)  acc5: 93.7500 (92.4180)  time: 0.0287  data: 0.0004  max mem: 5511
[21:04:05.579438] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.4756 (1.5029)  acc1: 48.4375 (47.8213)  acc5: 93.7500 (92.5396)  time: 0.0287  data: 0.0004  max mem: 5511
[21:04:05.864104] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.4776 (1.5034)  acc1: 48.4375 (47.8588)  acc5: 92.1875 (92.5926)  time: 0.0284  data: 0.0002  max mem: 5511
[21:04:06.146937] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.5366 (1.5057)  acc1: 48.4375 (48.0082)  acc5: 92.1875 (92.4794)  time: 0.0283  data: 0.0002  max mem: 5511
[21:04:06.431662] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.5441 (1.5102)  acc1: 45.3125 (47.6021)  acc5: 92.1875 (92.4350)  time: 0.0283  data: 0.0002  max mem: 5511
[21:04:06.715952] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.5291 (1.5115)  acc1: 46.8750 (47.7055)  acc5: 92.1875 (92.4690)  time: 0.0283  data: 0.0002  max mem: 5511
[21:04:07.002016] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.4980 (1.5090)  acc1: 48.4375 (47.9081)  acc5: 93.7500 (92.6007)  time: 0.0284  data: 0.0002  max mem: 5511
[21:04:07.287162] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.4980 (1.5102)  acc1: 48.4375 (48.0081)  acc5: 93.7500 (92.5692)  time: 0.0284  data: 0.0002  max mem: 5511
[21:04:07.569641] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.5066 (1.5097)  acc1: 50.0000 (48.2270)  acc5: 92.1875 (92.5532)  time: 0.0282  data: 0.0002  max mem: 5511
[21:04:07.851633] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.4790 (1.5085)  acc1: 51.5625 (48.4892)  acc5: 90.6250 (92.4358)  time: 0.0281  data: 0.0001  max mem: 5511
[21:04:08.003248] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.4692 (1.5069)  acc1: 51.5625 (48.4100)  acc5: 93.7500 (92.4900)  time: 0.0272  data: 0.0001  max mem: 5511
[21:04:08.150204] Test: Total time: 0:00:05 (0.0331 s / it)
[21:04:08.151107] * Acc@1 48.410 Acc@5 92.490 loss 1.507
[21:04:08.151488] Accuracy of the network on the 10000 test images: 48.4%
[21:04:08.151747] Max accuracy: 48.41%
[21:04:08.407234] log_dir: ./output_dir
[21:04:09.311861] Epoch: [4]  [  0/781]  eta: 0:11:44  lr: 0.000200  training_loss: 2.0140 (2.0140)  mae_loss: 0.0726 (0.0726)  classification_loss: 1.8543 (1.8543)  loss_mask: 0.0871 (0.0871)  time: 0.9026  data: 0.6913  max mem: 5511
[21:04:13.230489] Epoch: [4]  [ 20/781]  eta: 0:02:54  lr: 0.000201  training_loss: 2.1136 (2.1110)  mae_loss: 0.0637 (0.0640)  classification_loss: 1.9392 (1.9606)  loss_mask: 0.0831 (0.0864)  time: 0.1958  data: 0.0002  max mem: 5511
[21:04:17.166319] Epoch: [4]  [ 40/781]  eta: 0:02:38  lr: 0.000203  training_loss: 2.1417 (2.1271)  mae_loss: 0.0632 (0.0639)  classification_loss: 1.9885 (1.9745)  loss_mask: 0.0930 (0.0887)  time: 0.1967  data: 0.0002  max mem: 5511
[21:04:21.124718] Epoch: [4]  [ 60/781]  eta: 0:02:30  lr: 0.000204  training_loss: 2.1037 (2.1302)  mae_loss: 0.0596 (0.0628)  classification_loss: 1.9713 (1.9790)  loss_mask: 0.0887 (0.0885)  time: 0.1978  data: 0.0002  max mem: 5511
[21:04:25.043232] Epoch: [4]  [ 80/781]  eta: 0:02:23  lr: 0.000205  training_loss: 2.1554 (2.1367)  mae_loss: 0.0611 (0.0625)  classification_loss: 2.0097 (1.9864)  loss_mask: 0.0834 (0.0878)  time: 0.1958  data: 0.0002  max mem: 5511
[21:04:28.952927] Epoch: [4]  [100/781]  eta: 0:02:18  lr: 0.000206  training_loss: 2.1608 (2.1392)  mae_loss: 0.0568 (0.0615)  classification_loss: 2.0222 (1.9899)  loss_mask: 0.0884 (0.0878)  time: 0.1954  data: 0.0002  max mem: 5511
[21:04:32.860769] Epoch: [4]  [120/781]  eta: 0:02:13  lr: 0.000208  training_loss: 2.0991 (2.1347)  mae_loss: 0.0573 (0.0609)  classification_loss: 1.9533 (1.9866)  loss_mask: 0.0841 (0.0872)  time: 0.1953  data: 0.0002  max mem: 5511
[21:04:36.821085] Epoch: [4]  [140/781]  eta: 0:02:09  lr: 0.000209  training_loss: 2.0774 (2.1264)  mae_loss: 0.0550 (0.0602)  classification_loss: 1.9318 (1.9792)  loss_mask: 0.0876 (0.0871)  time: 0.1979  data: 0.0002  max mem: 5511
[21:04:40.775524] Epoch: [4]  [160/781]  eta: 0:02:04  lr: 0.000210  training_loss: 2.0881 (2.1193)  mae_loss: 0.0559 (0.0597)  classification_loss: 1.9215 (1.9716)  loss_mask: 0.0941 (0.0880)  time: 0.1976  data: 0.0003  max mem: 5511
[21:04:44.711550] Epoch: [4]  [180/781]  eta: 0:02:00  lr: 0.000212  training_loss: 2.0678 (2.1180)  mae_loss: 0.0533 (0.0593)  classification_loss: 1.9240 (1.9709)  loss_mask: 0.0861 (0.0878)  time: 0.1967  data: 0.0002  max mem: 5511
[21:04:48.627438] Epoch: [4]  [200/781]  eta: 0:01:56  lr: 0.000213  training_loss: 2.0866 (2.1184)  mae_loss: 0.0530 (0.0588)  classification_loss: 1.9450 (1.9719)  loss_mask: 0.0855 (0.0877)  time: 0.1957  data: 0.0002  max mem: 5511
[21:04:52.575036] Epoch: [4]  [220/781]  eta: 0:01:52  lr: 0.000214  training_loss: 2.1042 (2.1175)  mae_loss: 0.0551 (0.0585)  classification_loss: 1.9664 (1.9720)  loss_mask: 0.0788 (0.0870)  time: 0.1973  data: 0.0002  max mem: 5511
[21:04:56.493304] Epoch: [4]  [240/781]  eta: 0:01:47  lr: 0.000215  training_loss: 2.1110 (2.1186)  mae_loss: 0.0556 (0.0582)  classification_loss: 1.9806 (1.9737)  loss_mask: 0.0797 (0.0866)  time: 0.1958  data: 0.0002  max mem: 5511
[21:05:00.432675] Epoch: [4]  [260/781]  eta: 0:01:43  lr: 0.000217  training_loss: 2.1012 (2.1170)  mae_loss: 0.0543 (0.0579)  classification_loss: 1.9571 (1.9725)  loss_mask: 0.0851 (0.0866)  time: 0.1969  data: 0.0002  max mem: 5511
[21:05:04.400190] Epoch: [4]  [280/781]  eta: 0:01:39  lr: 0.000218  training_loss: 2.0854 (2.1169)  mae_loss: 0.0529 (0.0577)  classification_loss: 1.9573 (1.9727)  loss_mask: 0.0844 (0.0865)  time: 0.1983  data: 0.0002  max mem: 5511
[21:05:08.333419] Epoch: [4]  [300/781]  eta: 0:01:35  lr: 0.000219  training_loss: 2.1317 (2.1169)  mae_loss: 0.0519 (0.0574)  classification_loss: 1.9921 (1.9733)  loss_mask: 0.0816 (0.0862)  time: 0.1966  data: 0.0002  max mem: 5511
[21:05:12.271018] Epoch: [4]  [320/781]  eta: 0:01:31  lr: 0.000220  training_loss: 2.0777 (2.1148)  mae_loss: 0.0511 (0.0571)  classification_loss: 1.9410 (1.9715)  loss_mask: 0.0834 (0.0862)  time: 0.1968  data: 0.0002  max mem: 5511
[21:05:16.247226] Epoch: [4]  [340/781]  eta: 0:01:27  lr: 0.000222  training_loss: 2.0861 (2.1133)  mae_loss: 0.0535 (0.0569)  classification_loss: 1.9483 (1.9703)  loss_mask: 0.0839 (0.0861)  time: 0.1987  data: 0.0002  max mem: 5511
[21:05:20.220493] Epoch: [4]  [360/781]  eta: 0:01:23  lr: 0.000223  training_loss: 2.1043 (2.1137)  mae_loss: 0.0534 (0.0567)  classification_loss: 1.9616 (1.9707)  loss_mask: 0.0884 (0.0863)  time: 0.1986  data: 0.0002  max mem: 5511
[21:05:24.167101] Epoch: [4]  [380/781]  eta: 0:01:19  lr: 0.000224  training_loss: 2.1190 (2.1132)  mae_loss: 0.0515 (0.0565)  classification_loss: 1.9739 (1.9703)  loss_mask: 0.0845 (0.0864)  time: 0.1972  data: 0.0002  max mem: 5511
[21:05:28.095418] Epoch: [4]  [400/781]  eta: 0:01:15  lr: 0.000226  training_loss: 2.0745 (2.1117)  mae_loss: 0.0512 (0.0562)  classification_loss: 1.9426 (1.9694)  loss_mask: 0.0790 (0.0861)  time: 0.1963  data: 0.0002  max mem: 5511
[21:05:32.018951] Epoch: [4]  [420/781]  eta: 0:01:11  lr: 0.000227  training_loss: 2.0693 (2.1100)  mae_loss: 0.0490 (0.0559)  classification_loss: 1.9526 (1.9680)  loss_mask: 0.0819 (0.0861)  time: 0.1960  data: 0.0002  max mem: 5511
[21:05:35.978850] Epoch: [4]  [440/781]  eta: 0:01:07  lr: 0.000228  training_loss: 2.0837 (2.1089)  mae_loss: 0.0503 (0.0557)  classification_loss: 1.9425 (1.9672)  loss_mask: 0.0858 (0.0861)  time: 0.1979  data: 0.0002  max mem: 5511
[21:05:39.912333] Epoch: [4]  [460/781]  eta: 0:01:03  lr: 0.000229  training_loss: 2.0813 (2.1080)  mae_loss: 0.0479 (0.0554)  classification_loss: 1.9364 (1.9666)  loss_mask: 0.0821 (0.0860)  time: 0.1966  data: 0.0002  max mem: 5511
[21:05:43.849677] Epoch: [4]  [480/781]  eta: 0:00:59  lr: 0.000231  training_loss: 2.1210 (2.1085)  mae_loss: 0.0470 (0.0551)  classification_loss: 1.9927 (1.9675)  loss_mask: 0.0797 (0.0858)  time: 0.1968  data: 0.0002  max mem: 5511
[21:05:47.833027] Epoch: [4]  [500/781]  eta: 0:00:55  lr: 0.000232  training_loss: 2.0460 (2.1066)  mae_loss: 0.0487 (0.0549)  classification_loss: 1.9043 (1.9658)  loss_mask: 0.0851 (0.0859)  time: 0.1991  data: 0.0002  max mem: 5511
[21:05:51.760881] Epoch: [4]  [520/781]  eta: 0:00:51  lr: 0.000233  training_loss: 2.0339 (2.1046)  mae_loss: 0.0496 (0.0548)  classification_loss: 1.9041 (1.9640)  loss_mask: 0.0857 (0.0859)  time: 0.1963  data: 0.0002  max mem: 5511
[21:05:55.729968] Epoch: [4]  [540/781]  eta: 0:00:47  lr: 0.000235  training_loss: 2.0789 (2.1048)  mae_loss: 0.0505 (0.0547)  classification_loss: 1.9435 (1.9642)  loss_mask: 0.0824 (0.0859)  time: 0.1984  data: 0.0002  max mem: 5511
[21:05:59.670902] Epoch: [4]  [560/781]  eta: 0:00:43  lr: 0.000236  training_loss: 2.0503 (2.1034)  mae_loss: 0.0488 (0.0545)  classification_loss: 1.9199 (1.9632)  loss_mask: 0.0782 (0.0857)  time: 0.1970  data: 0.0002  max mem: 5511
[21:06:03.623105] Epoch: [4]  [580/781]  eta: 0:00:39  lr: 0.000237  training_loss: 2.1051 (2.1028)  mae_loss: 0.0504 (0.0543)  classification_loss: 1.9720 (1.9630)  loss_mask: 0.0793 (0.0855)  time: 0.1975  data: 0.0002  max mem: 5511
[21:06:07.536875] Epoch: [4]  [600/781]  eta: 0:00:35  lr: 0.000238  training_loss: 2.0679 (2.1017)  mae_loss: 0.0497 (0.0542)  classification_loss: 1.9240 (1.9620)  loss_mask: 0.0842 (0.0856)  time: 0.1956  data: 0.0002  max mem: 5511
[21:06:11.482435] Epoch: [4]  [620/781]  eta: 0:00:31  lr: 0.000240  training_loss: 2.0279 (2.0995)  mae_loss: 0.0495 (0.0540)  classification_loss: 1.9052 (1.9601)  loss_mask: 0.0793 (0.0855)  time: 0.1972  data: 0.0002  max mem: 5511
[21:06:15.451648] Epoch: [4]  [640/781]  eta: 0:00:27  lr: 0.000241  training_loss: 2.0567 (2.0985)  mae_loss: 0.0480 (0.0538)  classification_loss: 1.9258 (1.9594)  loss_mask: 0.0798 (0.0853)  time: 0.1984  data: 0.0002  max mem: 5511
[21:06:19.423057] Epoch: [4]  [660/781]  eta: 0:00:23  lr: 0.000242  training_loss: 2.0401 (2.0972)  mae_loss: 0.0506 (0.0537)  classification_loss: 1.9209 (1.9586)  loss_mask: 0.0711 (0.0849)  time: 0.1985  data: 0.0002  max mem: 5511
[21:06:23.384788] Epoch: [4]  [680/781]  eta: 0:00:20  lr: 0.000244  training_loss: 2.0692 (2.0970)  mae_loss: 0.0473 (0.0535)  classification_loss: 1.9303 (1.9586)  loss_mask: 0.0758 (0.0848)  time: 0.1980  data: 0.0002  max mem: 5511
[21:06:27.343864] Epoch: [4]  [700/781]  eta: 0:00:16  lr: 0.000245  training_loss: 2.0991 (2.0978)  mae_loss: 0.0485 (0.0534)  classification_loss: 1.9555 (1.9587)  loss_mask: 0.1036 (0.0856)  time: 0.1978  data: 0.0002  max mem: 5511
[21:06:31.276455] Epoch: [4]  [720/781]  eta: 0:00:12  lr: 0.000246  training_loss: 2.0950 (2.0979)  mae_loss: 0.0465 (0.0533)  classification_loss: 1.9494 (1.9588)  loss_mask: 0.0953 (0.0859)  time: 0.1965  data: 0.0002  max mem: 5511
[21:06:35.204725] Epoch: [4]  [740/781]  eta: 0:00:08  lr: 0.000247  training_loss: 2.0598 (2.0966)  mae_loss: 0.0488 (0.0531)  classification_loss: 1.9314 (1.9577)  loss_mask: 0.0802 (0.0857)  time: 0.1963  data: 0.0002  max mem: 5511
[21:06:39.138852] Epoch: [4]  [760/781]  eta: 0:00:04  lr: 0.000249  training_loss: 2.0575 (2.0956)  mae_loss: 0.0488 (0.0530)  classification_loss: 1.9241 (1.9571)  loss_mask: 0.0770 (0.0855)  time: 0.1966  data: 0.0003  max mem: 5511
[21:06:43.051507] Epoch: [4]  [780/781]  eta: 0:00:00  lr: 0.000250  training_loss: 2.0159 (2.0938)  mae_loss: 0.0474 (0.0529)  classification_loss: 1.8929 (1.9556)  loss_mask: 0.0744 (0.0853)  time: 0.1955  data: 0.0002  max mem: 5511
[21:06:43.204251] Epoch: [4] Total time: 0:02:34 (0.1982 s / it)
[21:06:43.205008] Averaged stats: lr: 0.000250  training_loss: 2.0159 (2.0938)  mae_loss: 0.0474 (0.0529)  classification_loss: 1.8929 (1.9556)  loss_mask: 0.0744 (0.0853)
[21:06:43.851748] Test:  [  0/157]  eta: 0:01:40  testing_loss: 1.3510 (1.3510)  acc1: 53.1250 (53.1250)  acc5: 92.1875 (92.1875)  time: 0.6427  data: 0.6063  max mem: 5511
[21:06:44.152068] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.3745 (1.4088)  acc1: 53.1250 (51.4205)  acc5: 93.7500 (94.0341)  time: 0.0855  data: 0.0564  max mem: 5511
[21:06:44.435874] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.3566 (1.3829)  acc1: 53.1250 (52.1577)  acc5: 93.7500 (93.6756)  time: 0.0290  data: 0.0008  max mem: 5511
[21:06:44.721648] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.3957 (1.3914)  acc1: 54.6875 (52.2681)  acc5: 93.7500 (93.3468)  time: 0.0283  data: 0.0002  max mem: 5511
[21:06:45.008041] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.4052 (1.4020)  acc1: 51.5625 (51.9817)  acc5: 92.1875 (92.9878)  time: 0.0285  data: 0.0002  max mem: 5511
[21:06:45.293780] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.3745 (1.3974)  acc1: 50.0000 (52.2365)  acc5: 92.1875 (93.1373)  time: 0.0285  data: 0.0002  max mem: 5511
[21:06:45.587799] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.3686 (1.3911)  acc1: 51.5625 (52.2541)  acc5: 93.7500 (93.2121)  time: 0.0288  data: 0.0002  max mem: 5511
[21:06:45.875075] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.3588 (1.3866)  acc1: 53.1250 (52.6188)  acc5: 93.7500 (93.3319)  time: 0.0289  data: 0.0003  max mem: 5511
[21:06:46.159236] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.3588 (1.3896)  acc1: 53.1250 (52.5463)  acc5: 92.1875 (93.1134)  time: 0.0284  data: 0.0003  max mem: 5511
[21:06:46.447050] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.4215 (1.3912)  acc1: 51.5625 (52.6614)  acc5: 92.1875 (93.0632)  time: 0.0284  data: 0.0002  max mem: 5511
[21:06:46.733877] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.4572 (1.3969)  acc1: 51.5625 (52.3515)  acc5: 92.1875 (92.9765)  time: 0.0286  data: 0.0002  max mem: 5511
[21:06:47.024617] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.4566 (1.3971)  acc1: 50.0000 (52.3508)  acc5: 92.1875 (93.0180)  time: 0.0287  data: 0.0002  max mem: 5511
[21:06:47.311625] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.3881 (1.3948)  acc1: 53.1250 (52.4535)  acc5: 93.7500 (93.0914)  time: 0.0287  data: 0.0002  max mem: 5511
[21:06:47.596439] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.3814 (1.3959)  acc1: 53.1250 (52.3974)  acc5: 93.7500 (93.0701)  time: 0.0284  data: 0.0003  max mem: 5511
[21:06:47.879984] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.3932 (1.3954)  acc1: 54.6875 (52.6928)  acc5: 92.1875 (92.9965)  time: 0.0283  data: 0.0002  max mem: 5511
[21:06:48.162765] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.3808 (1.3925)  acc1: 56.2500 (52.8353)  acc5: 92.1875 (92.9429)  time: 0.0282  data: 0.0002  max mem: 5511
[21:06:48.315828] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.3625 (1.3931)  acc1: 56.2500 (52.6500)  acc5: 92.1875 (92.9500)  time: 0.0273  data: 0.0001  max mem: 5511
[21:06:48.463972] Test: Total time: 0:00:05 (0.0335 s / it)
[21:06:48.464910] * Acc@1 52.650 Acc@5 92.950 loss 1.393
[21:06:48.465388] Accuracy of the network on the 10000 test images: 52.6%
[21:06:48.465579] Max accuracy: 52.65%
[21:06:48.710494] log_dir: ./output_dir
[21:06:49.577648] Epoch: [5]  [  0/781]  eta: 0:11:15  lr: 0.000250  training_loss: 1.8875 (1.8875)  mae_loss: 0.0497 (0.0497)  classification_loss: 1.7703 (1.7703)  loss_mask: 0.0675 (0.0675)  time: 0.8653  data: 0.6264  max mem: 5511
[21:06:53.510375] Epoch: [5]  [ 20/781]  eta: 0:02:53  lr: 0.000250  training_loss: 2.0450 (2.0336)  mae_loss: 0.0471 (0.0485)  classification_loss: 1.9270 (1.9069)  loss_mask: 0.0759 (0.0781)  time: 0.1965  data: 0.0003  max mem: 5511
[21:06:57.443662] Epoch: [5]  [ 40/781]  eta: 0:02:37  lr: 0.000250  training_loss: 2.1036 (2.0623)  mae_loss: 0.0499 (0.0493)  classification_loss: 1.9579 (1.9343)  loss_mask: 0.0778 (0.0787)  time: 0.1966  data: 0.0002  max mem: 5511
[21:07:01.371672] Epoch: [5]  [ 60/781]  eta: 0:02:29  lr: 0.000250  training_loss: 2.0505 (2.0527)  mae_loss: 0.0484 (0.0492)  classification_loss: 1.9209 (1.9258)  loss_mask: 0.0749 (0.0778)  time: 0.1963  data: 0.0002  max mem: 5511
[21:07:05.296035] Epoch: [5]  [ 80/781]  eta: 0:02:23  lr: 0.000250  training_loss: 2.0767 (2.0567)  mae_loss: 0.0476 (0.0490)  classification_loss: 1.9291 (1.9291)  loss_mask: 0.0823 (0.0787)  time: 0.1961  data: 0.0002  max mem: 5511
[21:07:09.211014] Epoch: [5]  [100/781]  eta: 0:02:18  lr: 0.000250  training_loss: 2.0548 (2.0554)  mae_loss: 0.0477 (0.0488)  classification_loss: 1.9247 (1.9277)  loss_mask: 0.0789 (0.0789)  time: 0.1957  data: 0.0002  max mem: 5511
[21:07:13.124568] Epoch: [5]  [120/781]  eta: 0:02:13  lr: 0.000250  training_loss: 2.0322 (2.0523)  mae_loss: 0.0454 (0.0483)  classification_loss: 1.9023 (1.9246)  loss_mask: 0.0817 (0.0794)  time: 0.1956  data: 0.0002  max mem: 5511
[21:07:17.035885] Epoch: [5]  [140/781]  eta: 0:02:08  lr: 0.000250  training_loss: 1.9586 (2.0453)  mae_loss: 0.0472 (0.0482)  classification_loss: 1.8355 (1.9178)  loss_mask: 0.0740 (0.0794)  time: 0.1955  data: 0.0002  max mem: 5511
[21:07:20.949971] Epoch: [5]  [160/781]  eta: 0:02:04  lr: 0.000250  training_loss: 2.0203 (2.0430)  mae_loss: 0.0468 (0.0480)  classification_loss: 1.9100 (1.9158)  loss_mask: 0.0781 (0.0792)  time: 0.1956  data: 0.0002  max mem: 5511
[21:07:24.878763] Epoch: [5]  [180/781]  eta: 0:02:00  lr: 0.000250  training_loss: 2.0236 (2.0434)  mae_loss: 0.0475 (0.0479)  classification_loss: 1.8961 (1.9163)  loss_mask: 0.0780 (0.0792)  time: 0.1963  data: 0.0002  max mem: 5511
[21:07:28.796742] Epoch: [5]  [200/781]  eta: 0:01:55  lr: 0.000250  training_loss: 2.0429 (2.0443)  mae_loss: 0.0488 (0.0480)  classification_loss: 1.9017 (1.9169)  loss_mask: 0.0801 (0.0794)  time: 0.1958  data: 0.0002  max mem: 5511
[21:07:32.706490] Epoch: [5]  [220/781]  eta: 0:01:51  lr: 0.000250  training_loss: 2.0271 (2.0442)  mae_loss: 0.0480 (0.0481)  classification_loss: 1.9154 (1.9174)  loss_mask: 0.0717 (0.0788)  time: 0.1954  data: 0.0002  max mem: 5511
[21:07:36.654324] Epoch: [5]  [240/781]  eta: 0:01:47  lr: 0.000250  training_loss: 2.0532 (2.0460)  mae_loss: 0.0492 (0.0481)  classification_loss: 1.9139 (1.9192)  loss_mask: 0.0757 (0.0787)  time: 0.1973  data: 0.0003  max mem: 5511
[21:07:40.576573] Epoch: [5]  [260/781]  eta: 0:01:43  lr: 0.000250  training_loss: 2.0223 (2.0450)  mae_loss: 0.0485 (0.0481)  classification_loss: 1.8946 (1.9181)  loss_mask: 0.0769 (0.0787)  time: 0.1960  data: 0.0003  max mem: 5511
[21:07:44.486259] Epoch: [5]  [280/781]  eta: 0:01:39  lr: 0.000250  training_loss: 2.0530 (2.0461)  mae_loss: 0.0497 (0.0482)  classification_loss: 1.9107 (1.9184)  loss_mask: 0.0822 (0.0795)  time: 0.1954  data: 0.0002  max mem: 5511
[21:07:48.412569] Epoch: [5]  [300/781]  eta: 0:01:35  lr: 0.000250  training_loss: 2.0644 (2.0473)  mae_loss: 0.0457 (0.0481)  classification_loss: 1.9466 (1.9191)  loss_mask: 0.0903 (0.0801)  time: 0.1962  data: 0.0002  max mem: 5511
[21:07:52.342300] Epoch: [5]  [320/781]  eta: 0:01:31  lr: 0.000250  training_loss: 2.0678 (2.0481)  mae_loss: 0.0496 (0.0481)  classification_loss: 1.9294 (1.9197)  loss_mask: 0.0813 (0.0802)  time: 0.1963  data: 0.0003  max mem: 5511
[21:07:56.275202] Epoch: [5]  [340/781]  eta: 0:01:27  lr: 0.000250  training_loss: 2.0014 (2.0456)  mae_loss: 0.0462 (0.0480)  classification_loss: 1.8940 (1.9178)  loss_mask: 0.0687 (0.0798)  time: 0.1965  data: 0.0007  max mem: 5511
[21:08:00.202903] Epoch: [5]  [360/781]  eta: 0:01:23  lr: 0.000250  training_loss: 2.0195 (2.0446)  mae_loss: 0.0483 (0.0480)  classification_loss: 1.9057 (1.9171)  loss_mask: 0.0747 (0.0795)  time: 0.1963  data: 0.0002  max mem: 5511
[21:08:04.145451] Epoch: [5]  [380/781]  eta: 0:01:19  lr: 0.000250  training_loss: 2.0234 (2.0434)  mae_loss: 0.0465 (0.0480)  classification_loss: 1.9062 (1.9162)  loss_mask: 0.0738 (0.0792)  time: 0.1970  data: 0.0002  max mem: 5511
[21:08:08.051692] Epoch: [5]  [400/781]  eta: 0:01:15  lr: 0.000250  training_loss: 2.0174 (2.0430)  mae_loss: 0.0466 (0.0479)  classification_loss: 1.9026 (1.9162)  loss_mask: 0.0728 (0.0789)  time: 0.1952  data: 0.0002  max mem: 5511
[21:08:11.977842] Epoch: [5]  [420/781]  eta: 0:01:11  lr: 0.000250  training_loss: 2.0443 (2.0437)  mae_loss: 0.0481 (0.0479)  classification_loss: 1.9181 (1.9165)  loss_mask: 0.0872 (0.0793)  time: 0.1962  data: 0.0002  max mem: 5511
[21:08:15.896913] Epoch: [5]  [440/781]  eta: 0:01:07  lr: 0.000250  training_loss: 2.0377 (2.0426)  mae_loss: 0.0470 (0.0479)  classification_loss: 1.9136 (1.9157)  loss_mask: 0.0727 (0.0790)  time: 0.1958  data: 0.0002  max mem: 5511
[21:08:19.811325] Epoch: [5]  [460/781]  eta: 0:01:03  lr: 0.000250  training_loss: 1.9890 (2.0403)  mae_loss: 0.0457 (0.0478)  classification_loss: 1.8824 (1.9137)  loss_mask: 0.0715 (0.0788)  time: 0.1956  data: 0.0002  max mem: 5511
[21:08:23.746201] Epoch: [5]  [480/781]  eta: 0:00:59  lr: 0.000250  training_loss: 2.0305 (2.0403)  mae_loss: 0.0442 (0.0477)  classification_loss: 1.9258 (1.9141)  loss_mask: 0.0671 (0.0785)  time: 0.1967  data: 0.0002  max mem: 5511
[21:08:27.716713] Epoch: [5]  [500/781]  eta: 0:00:55  lr: 0.000250  training_loss: 2.0097 (2.0395)  mae_loss: 0.0492 (0.0477)  classification_loss: 1.8928 (1.9135)  loss_mask: 0.0728 (0.0782)  time: 0.1984  data: 0.0004  max mem: 5511
[21:08:31.619540] Epoch: [5]  [520/781]  eta: 0:00:51  lr: 0.000250  training_loss: 2.0098 (2.0385)  mae_loss: 0.0457 (0.0477)  classification_loss: 1.8977 (1.9126)  loss_mask: 0.0736 (0.0783)  time: 0.1951  data: 0.0002  max mem: 5511
[21:08:35.514593] Epoch: [5]  [540/781]  eta: 0:00:47  lr: 0.000250  training_loss: 2.0024 (2.0385)  mae_loss: 0.0456 (0.0476)  classification_loss: 1.8955 (1.9129)  loss_mask: 0.0701 (0.0780)  time: 0.1947  data: 0.0003  max mem: 5511
[21:08:39.461482] Epoch: [5]  [560/781]  eta: 0:00:43  lr: 0.000250  training_loss: 1.9960 (2.0374)  mae_loss: 0.0481 (0.0476)  classification_loss: 1.8709 (1.9121)  loss_mask: 0.0699 (0.0777)  time: 0.1973  data: 0.0003  max mem: 5511
[21:08:43.410451] Epoch: [5]  [580/781]  eta: 0:00:39  lr: 0.000250  training_loss: 2.0159 (2.0363)  mae_loss: 0.0458 (0.0475)  classification_loss: 1.9018 (1.9113)  loss_mask: 0.0721 (0.0775)  time: 0.1974  data: 0.0002  max mem: 5511
[21:08:47.322218] Epoch: [5]  [600/781]  eta: 0:00:35  lr: 0.000250  training_loss: 2.0537 (2.0364)  mae_loss: 0.0474 (0.0475)  classification_loss: 1.9298 (1.9113)  loss_mask: 0.0779 (0.0776)  time: 0.1955  data: 0.0002  max mem: 5511
[21:08:51.313745] Epoch: [5]  [620/781]  eta: 0:00:31  lr: 0.000250  training_loss: 2.0029 (2.0356)  mae_loss: 0.0498 (0.0476)  classification_loss: 1.8814 (1.9106)  loss_mask: 0.0715 (0.0774)  time: 0.1995  data: 0.0001  max mem: 5511
[21:08:55.235270] Epoch: [5]  [640/781]  eta: 0:00:27  lr: 0.000250  training_loss: 2.0133 (2.0357)  mae_loss: 0.0480 (0.0476)  classification_loss: 1.9057 (1.9108)  loss_mask: 0.0721 (0.0773)  time: 0.1960  data: 0.0002  max mem: 5511
[21:08:59.163347] Epoch: [5]  [660/781]  eta: 0:00:23  lr: 0.000250  training_loss: 1.9688 (2.0349)  mae_loss: 0.0465 (0.0476)  classification_loss: 1.8576 (1.9101)  loss_mask: 0.0739 (0.0772)  time: 0.1963  data: 0.0002  max mem: 5511
[21:09:03.102341] Epoch: [5]  [680/781]  eta: 0:00:19  lr: 0.000250  training_loss: 1.9997 (2.0339)  mae_loss: 0.0469 (0.0476)  classification_loss: 1.8780 (1.9093)  loss_mask: 0.0692 (0.0770)  time: 0.1969  data: 0.0003  max mem: 5511
[21:09:07.052921] Epoch: [5]  [700/781]  eta: 0:00:15  lr: 0.000250  training_loss: 2.0106 (2.0329)  mae_loss: 0.0477 (0.0475)  classification_loss: 1.8899 (1.9084)  loss_mask: 0.0705 (0.0769)  time: 0.1974  data: 0.0002  max mem: 5511
[21:09:10.989986] Epoch: [5]  [720/781]  eta: 0:00:12  lr: 0.000250  training_loss: 2.0291 (2.0326)  mae_loss: 0.0439 (0.0475)  classification_loss: 1.9069 (1.9082)  loss_mask: 0.0766 (0.0770)  time: 0.1968  data: 0.0002  max mem: 5511
[21:09:14.960870] Epoch: [5]  [740/781]  eta: 0:00:08  lr: 0.000250  training_loss: 2.0132 (2.0320)  mae_loss: 0.0477 (0.0474)  classification_loss: 1.8992 (1.9076)  loss_mask: 0.0733 (0.0769)  time: 0.1985  data: 0.0002  max mem: 5511
[21:09:18.896091] Epoch: [5]  [760/781]  eta: 0:00:04  lr: 0.000250  training_loss: 1.9899 (2.0309)  mae_loss: 0.0465 (0.0474)  classification_loss: 1.8708 (1.9067)  loss_mask: 0.0719 (0.0768)  time: 0.1966  data: 0.0002  max mem: 5511
[21:09:22.803014] Epoch: [5]  [780/781]  eta: 0:00:00  lr: 0.000250  training_loss: 1.9967 (2.0306)  mae_loss: 0.0444 (0.0474)  classification_loss: 1.8822 (1.9065)  loss_mask: 0.0701 (0.0767)  time: 0.1953  data: 0.0002  max mem: 5511
[21:09:22.967368] Epoch: [5] Total time: 0:02:34 (0.1975 s / it)
[21:09:22.967837] Averaged stats: lr: 0.000250  training_loss: 1.9967 (2.0306)  mae_loss: 0.0444 (0.0474)  classification_loss: 1.8822 (1.9065)  loss_mask: 0.0701 (0.0767)
[21:09:23.602684] Test:  [  0/157]  eta: 0:01:38  testing_loss: 1.2132 (1.2132)  acc1: 65.6250 (65.6250)  acc5: 96.8750 (96.8750)  time: 0.6300  data: 0.5992  max mem: 5511
[21:09:23.897907] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.3272 (1.3391)  acc1: 53.1250 (54.6875)  acc5: 95.3125 (95.1705)  time: 0.0839  data: 0.0546  max mem: 5511
[21:09:24.181840] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.3058 (1.3179)  acc1: 53.1250 (55.0595)  acc5: 95.3125 (94.7173)  time: 0.0288  data: 0.0002  max mem: 5511
[21:09:24.467671] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.3138 (1.3255)  acc1: 53.1250 (54.9395)  acc5: 93.7500 (94.4556)  time: 0.0283  data: 0.0002  max mem: 5511
[21:09:24.753364] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.3421 (1.3321)  acc1: 54.6875 (55.1067)  acc5: 93.7500 (94.1692)  time: 0.0285  data: 0.0002  max mem: 5511
[21:09:25.043590] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.3027 (1.3241)  acc1: 57.8125 (55.5760)  acc5: 95.3125 (94.4547)  time: 0.0287  data: 0.0002  max mem: 5511
[21:09:25.330672] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.2848 (1.3159)  acc1: 59.3750 (56.2756)  acc5: 95.3125 (94.4416)  time: 0.0287  data: 0.0003  max mem: 5511
[21:09:25.620239] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.2745 (1.3065)  acc1: 60.9375 (56.9542)  acc5: 93.7500 (94.4982)  time: 0.0287  data: 0.0002  max mem: 5511
[21:09:25.907081] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.2745 (1.3071)  acc1: 60.9375 (56.8866)  acc5: 95.3125 (94.5216)  time: 0.0287  data: 0.0002  max mem: 5511
[21:09:26.191624] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.3179 (1.3074)  acc1: 56.2500 (56.8681)  acc5: 93.7500 (94.5227)  time: 0.0284  data: 0.0002  max mem: 5511
[21:09:26.475687] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.3333 (1.3141)  acc1: 56.2500 (56.3892)  acc5: 95.3125 (94.5854)  time: 0.0283  data: 0.0002  max mem: 5511
[21:09:26.761475] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.3539 (1.3143)  acc1: 51.5625 (56.2218)  acc5: 95.3125 (94.6087)  time: 0.0284  data: 0.0002  max mem: 5511
[21:09:27.055740] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.3063 (1.3113)  acc1: 56.2500 (56.3275)  acc5: 95.3125 (94.6410)  time: 0.0288  data: 0.0002  max mem: 5511
[21:09:27.351138] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.3063 (1.3131)  acc1: 57.8125 (56.2977)  acc5: 93.7500 (94.5730)  time: 0.0293  data: 0.0003  max mem: 5511
[21:09:27.636477] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.3174 (1.3136)  acc1: 56.2500 (56.4051)  acc5: 93.7500 (94.5479)  time: 0.0288  data: 0.0003  max mem: 5511
[21:09:27.919583] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.2901 (1.3112)  acc1: 57.8125 (56.4880)  acc5: 95.3125 (94.5675)  time: 0.0283  data: 0.0002  max mem: 5511
[21:09:28.071928] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.2891 (1.3113)  acc1: 54.6875 (56.3300)  acc5: 95.3125 (94.6500)  time: 0.0273  data: 0.0001  max mem: 5511
[21:09:28.225715] Test: Total time: 0:00:05 (0.0335 s / it)
[21:09:28.226206] * Acc@1 56.330 Acc@5 94.650 loss 1.311
[21:09:28.226499] Accuracy of the network on the 10000 test images: 56.3%
[21:09:28.226684] Max accuracy: 56.33%
[21:09:28.604133] log_dir: ./output_dir
[21:09:29.429078] Epoch: [6]  [  0/781]  eta: 0:10:42  lr: 0.000250  training_loss: 1.9588 (1.9588)  mae_loss: 0.0508 (0.0508)  classification_loss: 1.8353 (1.8353)  loss_mask: 0.0727 (0.0727)  time: 0.8232  data: 0.6043  max mem: 5511
[21:09:33.381381] Epoch: [6]  [ 20/781]  eta: 0:02:52  lr: 0.000250  training_loss: 1.9691 (1.9712)  mae_loss: 0.0469 (0.0466)  classification_loss: 1.8521 (1.8548)  loss_mask: 0.0705 (0.0698)  time: 0.1975  data: 0.0002  max mem: 5511
[21:09:37.326504] Epoch: [6]  [ 40/781]  eta: 0:02:37  lr: 0.000250  training_loss: 1.9878 (1.9701)  mae_loss: 0.0482 (0.0470)  classification_loss: 1.8684 (1.8532)  loss_mask: 0.0702 (0.0699)  time: 0.1972  data: 0.0002  max mem: 5511
[21:09:41.259134] Epoch: [6]  [ 60/781]  eta: 0:02:29  lr: 0.000250  training_loss: 2.0015 (1.9777)  mae_loss: 0.0465 (0.0471)  classification_loss: 1.8875 (1.8601)  loss_mask: 0.0700 (0.0705)  time: 0.1966  data: 0.0002  max mem: 5511
[21:09:45.185028] Epoch: [6]  [ 80/781]  eta: 0:02:23  lr: 0.000250  training_loss: 2.0051 (1.9869)  mae_loss: 0.0459 (0.0469)  classification_loss: 1.8779 (1.8691)  loss_mask: 0.0735 (0.0709)  time: 0.1962  data: 0.0002  max mem: 5511
[21:09:49.223350] Epoch: [6]  [100/781]  eta: 0:02:18  lr: 0.000250  training_loss: 1.9792 (1.9921)  mae_loss: 0.0451 (0.0469)  classification_loss: 1.8704 (1.8751)  loss_mask: 0.0662 (0.0701)  time: 0.2018  data: 0.0002  max mem: 5511
[21:09:53.187569] Epoch: [6]  [120/781]  eta: 0:02:14  lr: 0.000250  training_loss: 1.9704 (1.9877)  mae_loss: 0.0476 (0.0468)  classification_loss: 1.8658 (1.8707)  loss_mask: 0.0703 (0.0702)  time: 0.1981  data: 0.0002  max mem: 5511
[21:09:57.133675] Epoch: [6]  [140/781]  eta: 0:02:09  lr: 0.000250  training_loss: 1.9526 (1.9839)  mae_loss: 0.0451 (0.0466)  classification_loss: 1.8415 (1.8674)  loss_mask: 0.0689 (0.0699)  time: 0.1972  data: 0.0002  max mem: 5511
[21:10:01.081356] Epoch: [6]  [160/781]  eta: 0:02:05  lr: 0.000250  training_loss: 1.9716 (1.9846)  mae_loss: 0.0442 (0.0463)  classification_loss: 1.8594 (1.8678)  loss_mask: 0.0718 (0.0704)  time: 0.1973  data: 0.0002  max mem: 5511
[21:10:05.021473] Epoch: [6]  [180/781]  eta: 0:02:00  lr: 0.000250  training_loss: 1.9507 (1.9797)  mae_loss: 0.0435 (0.0462)  classification_loss: 1.8293 (1.8634)  loss_mask: 0.0640 (0.0701)  time: 0.1969  data: 0.0002  max mem: 5511
[21:10:08.961239] Epoch: [6]  [200/781]  eta: 0:01:56  lr: 0.000250  training_loss: 1.9731 (1.9812)  mae_loss: 0.0458 (0.0462)  classification_loss: 1.8522 (1.8649)  loss_mask: 0.0670 (0.0701)  time: 0.1969  data: 0.0002  max mem: 5511
[21:10:12.882497] Epoch: [6]  [220/781]  eta: 0:01:52  lr: 0.000250  training_loss: 1.9785 (1.9820)  mae_loss: 0.0443 (0.0462)  classification_loss: 1.8526 (1.8649)  loss_mask: 0.0774 (0.0709)  time: 0.1960  data: 0.0002  max mem: 5511
[21:10:16.824376] Epoch: [6]  [240/781]  eta: 0:01:48  lr: 0.000250  training_loss: 1.9970 (1.9852)  mae_loss: 0.0442 (0.0461)  classification_loss: 1.8615 (1.8679)  loss_mask: 0.0747 (0.0712)  time: 0.1970  data: 0.0002  max mem: 5511
[21:10:20.752432] Epoch: [6]  [260/781]  eta: 0:01:44  lr: 0.000250  training_loss: 1.9651 (1.9844)  mae_loss: 0.0449 (0.0461)  classification_loss: 1.8526 (1.8673)  loss_mask: 0.0684 (0.0710)  time: 0.1963  data: 0.0002  max mem: 5511
[21:10:24.668339] Epoch: [6]  [280/781]  eta: 0:01:39  lr: 0.000250  training_loss: 1.9617 (1.9832)  mae_loss: 0.0453 (0.0462)  classification_loss: 1.8444 (1.8659)  loss_mask: 0.0691 (0.0710)  time: 0.1957  data: 0.0002  max mem: 5511
[21:10:28.607256] Epoch: [6]  [300/781]  eta: 0:01:35  lr: 0.000250  training_loss: 2.0318 (1.9855)  mae_loss: 0.0462 (0.0462)  classification_loss: 1.9059 (1.8679)  loss_mask: 0.0753 (0.0714)  time: 0.1969  data: 0.0002  max mem: 5511
[21:10:32.559768] Epoch: [6]  [320/781]  eta: 0:01:31  lr: 0.000250  training_loss: 1.9689 (1.9852)  mae_loss: 0.0445 (0.0462)  classification_loss: 1.8508 (1.8677)  loss_mask: 0.0687 (0.0713)  time: 0.1975  data: 0.0002  max mem: 5511
[21:10:36.529636] Epoch: [6]  [340/781]  eta: 0:01:27  lr: 0.000250  training_loss: 1.9159 (1.9818)  mae_loss: 0.0472 (0.0462)  classification_loss: 1.7974 (1.8644)  loss_mask: 0.0670 (0.0712)  time: 0.1984  data: 0.0002  max mem: 5511
[21:10:40.525936] Epoch: [6]  [360/781]  eta: 0:01:23  lr: 0.000250  training_loss: 2.0046 (1.9830)  mae_loss: 0.0455 (0.0462)  classification_loss: 1.8706 (1.8651)  loss_mask: 0.0800 (0.0717)  time: 0.1997  data: 0.0003  max mem: 5511
[21:10:44.454286] Epoch: [6]  [380/781]  eta: 0:01:19  lr: 0.000250  training_loss: 1.9649 (1.9832)  mae_loss: 0.0472 (0.0463)  classification_loss: 1.8513 (1.8653)  loss_mask: 0.0701 (0.0716)  time: 0.1963  data: 0.0002  max mem: 5511
[21:10:48.386698] Epoch: [6]  [400/781]  eta: 0:01:15  lr: 0.000250  training_loss: 1.9999 (1.9837)  mae_loss: 0.0447 (0.0462)  classification_loss: 1.8902 (1.8658)  loss_mask: 0.0680 (0.0716)  time: 0.1965  data: 0.0002  max mem: 5511
[21:10:52.315494] Epoch: [6]  [420/781]  eta: 0:01:11  lr: 0.000250  training_loss: 1.9682 (1.9835)  mae_loss: 0.0459 (0.0462)  classification_loss: 1.8506 (1.8659)  loss_mask: 0.0667 (0.0714)  time: 0.1964  data: 0.0002  max mem: 5511
[21:10:56.252576] Epoch: [6]  [440/781]  eta: 0:01:07  lr: 0.000250  training_loss: 1.9339 (1.9829)  mae_loss: 0.0457 (0.0462)  classification_loss: 1.8166 (1.8654)  loss_mask: 0.0682 (0.0713)  time: 0.1968  data: 0.0002  max mem: 5511
[21:11:00.184996] Epoch: [6]  [460/781]  eta: 0:01:03  lr: 0.000250  training_loss: 1.9369 (1.9813)  mae_loss: 0.0440 (0.0461)  classification_loss: 1.8217 (1.8640)  loss_mask: 0.0688 (0.0712)  time: 0.1965  data: 0.0002  max mem: 5511
[21:11:04.114431] Epoch: [6]  [480/781]  eta: 0:00:59  lr: 0.000250  training_loss: 1.9836 (1.9810)  mae_loss: 0.0459 (0.0461)  classification_loss: 1.8706 (1.8638)  loss_mask: 0.0652 (0.0711)  time: 0.1964  data: 0.0002  max mem: 5511
[21:11:08.056671] Epoch: [6]  [500/781]  eta: 0:00:55  lr: 0.000250  training_loss: 1.9665 (1.9811)  mae_loss: 0.0453 (0.0461)  classification_loss: 1.8583 (1.8639)  loss_mask: 0.0699 (0.0711)  time: 0.1970  data: 0.0002  max mem: 5511
[21:11:11.987086] Epoch: [6]  [520/781]  eta: 0:00:51  lr: 0.000250  training_loss: 1.9702 (1.9810)  mae_loss: 0.0453 (0.0461)  classification_loss: 1.8754 (1.8639)  loss_mask: 0.0691 (0.0710)  time: 0.1964  data: 0.0002  max mem: 5511
[21:11:15.913848] Epoch: [6]  [540/781]  eta: 0:00:47  lr: 0.000250  training_loss: 1.9948 (1.9814)  mae_loss: 0.0461 (0.0461)  classification_loss: 1.8879 (1.8646)  loss_mask: 0.0634 (0.0707)  time: 0.1962  data: 0.0002  max mem: 5511
[21:11:19.843706] Epoch: [6]  [560/781]  eta: 0:00:43  lr: 0.000250  training_loss: 1.9701 (1.9801)  mae_loss: 0.0472 (0.0461)  classification_loss: 1.8609 (1.8634)  loss_mask: 0.0657 (0.0705)  time: 0.1964  data: 0.0002  max mem: 5511
[21:11:23.768359] Epoch: [6]  [580/781]  eta: 0:00:39  lr: 0.000250  training_loss: 1.9667 (1.9798)  mae_loss: 0.0486 (0.0462)  classification_loss: 1.8496 (1.8630)  loss_mask: 0.0725 (0.0706)  time: 0.1962  data: 0.0002  max mem: 5511
[21:11:27.722786] Epoch: [6]  [600/781]  eta: 0:00:35  lr: 0.000250  training_loss: 1.9526 (1.9789)  mae_loss: 0.0474 (0.0462)  classification_loss: 1.8434 (1.8622)  loss_mask: 0.0640 (0.0705)  time: 0.1976  data: 0.0002  max mem: 5511
[21:11:31.656938] Epoch: [6]  [620/781]  eta: 0:00:31  lr: 0.000250  training_loss: 1.9311 (1.9775)  mae_loss: 0.0467 (0.0462)  classification_loss: 1.8149 (1.8609)  loss_mask: 0.0627 (0.0703)  time: 0.1966  data: 0.0002  max mem: 5511
[21:11:35.629737] Epoch: [6]  [640/781]  eta: 0:00:27  lr: 0.000250  training_loss: 1.9868 (1.9783)  mae_loss: 0.0458 (0.0463)  classification_loss: 1.8824 (1.8618)  loss_mask: 0.0690 (0.0703)  time: 0.1986  data: 0.0002  max mem: 5511
[21:11:39.545009] Epoch: [6]  [660/781]  eta: 0:00:23  lr: 0.000250  training_loss: 1.9741 (1.9786)  mae_loss: 0.0448 (0.0462)  classification_loss: 1.8600 (1.8622)  loss_mask: 0.0669 (0.0702)  time: 0.1957  data: 0.0002  max mem: 5511
[21:11:43.519622] Epoch: [6]  [680/781]  eta: 0:00:19  lr: 0.000250  training_loss: 1.9388 (1.9779)  mae_loss: 0.0441 (0.0462)  classification_loss: 1.8176 (1.8614)  loss_mask: 0.0691 (0.0704)  time: 0.1986  data: 0.0004  max mem: 5511
[21:11:47.459413] Epoch: [6]  [700/781]  eta: 0:00:16  lr: 0.000250  training_loss: 1.9507 (1.9778)  mae_loss: 0.0452 (0.0462)  classification_loss: 1.8454 (1.8613)  loss_mask: 0.0677 (0.0703)  time: 0.1969  data: 0.0002  max mem: 5511
[21:11:51.411403] Epoch: [6]  [720/781]  eta: 0:00:12  lr: 0.000250  training_loss: 1.9691 (1.9778)  mae_loss: 0.0446 (0.0461)  classification_loss: 1.8422 (1.8613)  loss_mask: 0.0712 (0.0704)  time: 0.1975  data: 0.0002  max mem: 5511
[21:11:55.411569] Epoch: [6]  [740/781]  eta: 0:00:08  lr: 0.000250  training_loss: 1.9428 (1.9769)  mae_loss: 0.0451 (0.0461)  classification_loss: 1.8275 (1.8604)  loss_mask: 0.0694 (0.0704)  time: 0.1999  data: 0.0004  max mem: 5511
[21:11:59.368877] Epoch: [6]  [760/781]  eta: 0:00:04  lr: 0.000250  training_loss: 1.9222 (1.9766)  mae_loss: 0.0439 (0.0460)  classification_loss: 1.8120 (1.8603)  loss_mask: 0.0667 (0.0703)  time: 0.1978  data: 0.0003  max mem: 5511
[21:12:03.293907] Epoch: [6]  [780/781]  eta: 0:00:00  lr: 0.000250  training_loss: 1.9760 (1.9764)  mae_loss: 0.0449 (0.0460)  classification_loss: 1.8713 (1.8602)  loss_mask: 0.0643 (0.0702)  time: 0.1962  data: 0.0002  max mem: 5511
[21:12:03.445721] Epoch: [6] Total time: 0:02:34 (0.1983 s / it)
[21:12:03.446163] Averaged stats: lr: 0.000250  training_loss: 1.9760 (1.9764)  mae_loss: 0.0449 (0.0460)  classification_loss: 1.8713 (1.8602)  loss_mask: 0.0643 (0.0702)
[21:12:04.107722] Test:  [  0/157]  eta: 0:01:42  testing_loss: 1.1820 (1.1820)  acc1: 62.5000 (62.5000)  acc5: 95.3125 (95.3125)  time: 0.6550  data: 0.6132  max mem: 5511
[21:12:04.397238] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.3079 (1.3413)  acc1: 54.6875 (53.1250)  acc5: 95.3125 (94.0341)  time: 0.0857  data: 0.0559  max mem: 5511
[21:12:04.682483] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.3047 (1.3078)  acc1: 54.6875 (54.7619)  acc5: 93.7500 (94.2708)  time: 0.0285  data: 0.0002  max mem: 5511
[21:12:04.966594] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.2720 (1.3069)  acc1: 56.2500 (55.3931)  acc5: 93.7500 (94.3044)  time: 0.0283  data: 0.0002  max mem: 5511
[21:12:05.249682] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.2973 (1.3106)  acc1: 54.6875 (55.0686)  acc5: 93.7500 (94.0549)  time: 0.0282  data: 0.0002  max mem: 5511
[21:12:05.536969] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.2973 (1.3067)  acc1: 54.6875 (55.4841)  acc5: 93.7500 (94.1483)  time: 0.0284  data: 0.0002  max mem: 5511
[21:12:05.821832] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.2818 (1.3014)  acc1: 56.2500 (55.5328)  acc5: 93.7500 (94.1598)  time: 0.0284  data: 0.0002  max mem: 5511
[21:12:06.104922] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.2758 (1.2969)  acc1: 59.3750 (56.3160)  acc5: 93.7500 (94.2342)  time: 0.0283  data: 0.0002  max mem: 5511
[21:12:06.388432] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.2757 (1.2975)  acc1: 57.8125 (56.2307)  acc5: 95.3125 (94.2515)  time: 0.0282  data: 0.0002  max mem: 5511
[21:12:06.671829] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.3183 (1.2993)  acc1: 54.6875 (56.1470)  acc5: 95.3125 (94.2995)  time: 0.0282  data: 0.0002  max mem: 5511
[21:12:06.954470] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.3468 (1.3047)  acc1: 53.1250 (55.7550)  acc5: 95.3125 (94.3224)  time: 0.0282  data: 0.0002  max mem: 5511
[21:12:07.237926] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.3646 (1.3058)  acc1: 53.1250 (55.7432)  acc5: 93.7500 (94.3694)  time: 0.0282  data: 0.0002  max mem: 5511
[21:12:07.520163] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.3040 (1.3027)  acc1: 59.3750 (55.9272)  acc5: 95.3125 (94.4086)  time: 0.0282  data: 0.0002  max mem: 5511
[21:12:07.803550] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.3092 (1.3065)  acc1: 54.6875 (55.8325)  acc5: 93.7500 (94.2867)  time: 0.0282  data: 0.0002  max mem: 5511
[21:12:08.085818] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.2996 (1.3060)  acc1: 54.6875 (55.9065)  acc5: 92.1875 (94.2154)  time: 0.0282  data: 0.0002  max mem: 5511
[21:12:08.367584] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.2890 (1.3033)  acc1: 54.6875 (56.0017)  acc5: 93.7500 (94.0915)  time: 0.0281  data: 0.0001  max mem: 5511
[21:12:08.517943] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.2904 (1.3052)  acc1: 53.1250 (55.9400)  acc5: 93.7500 (94.1800)  time: 0.0271  data: 0.0001  max mem: 5511
[21:12:08.676602] Test: Total time: 0:00:05 (0.0333 s / it)
[21:12:08.677158] * Acc@1 55.940 Acc@5 94.180 loss 1.305
[21:12:08.677622] Accuracy of the network on the 10000 test images: 55.9%
[21:12:08.677948] Max accuracy: 56.33%
[21:12:08.863680] log_dir: ./output_dir
[21:12:09.731740] Epoch: [7]  [  0/781]  eta: 0:11:16  lr: 0.000250  training_loss: 1.8777 (1.8777)  mae_loss: 0.0465 (0.0465)  classification_loss: 1.7726 (1.7726)  loss_mask: 0.0587 (0.0587)  time: 0.8660  data: 0.6334  max mem: 5511
[21:12:13.719640] Epoch: [7]  [ 20/781]  eta: 0:02:55  lr: 0.000250  training_loss: 1.9301 (1.9174)  mae_loss: 0.0451 (0.0452)  classification_loss: 1.8160 (1.8098)  loss_mask: 0.0608 (0.0624)  time: 0.1993  data: 0.0002  max mem: 5511
[21:12:17.678399] Epoch: [7]  [ 40/781]  eta: 0:02:39  lr: 0.000250  training_loss: 1.9918 (1.9425)  mae_loss: 0.0468 (0.0459)  classification_loss: 1.8636 (1.8311)  loss_mask: 0.0692 (0.0655)  time: 0.1978  data: 0.0003  max mem: 5511
[21:12:21.638374] Epoch: [7]  [ 60/781]  eta: 0:02:30  lr: 0.000250  training_loss: 1.9778 (1.9569)  mae_loss: 0.0455 (0.0459)  classification_loss: 1.8693 (1.8445)  loss_mask: 0.0665 (0.0666)  time: 0.1979  data: 0.0003  max mem: 5511
[21:12:25.566585] Epoch: [7]  [ 80/781]  eta: 0:02:24  lr: 0.000250  training_loss: 1.9752 (1.9578)  mae_loss: 0.0444 (0.0457)  classification_loss: 1.8577 (1.8452)  loss_mask: 0.0666 (0.0669)  time: 0.1963  data: 0.0002  max mem: 5511
[21:12:29.497936] Epoch: [7]  [100/781]  eta: 0:02:19  lr: 0.000250  training_loss: 1.9165 (1.9519)  mae_loss: 0.0425 (0.0453)  classification_loss: 1.8176 (1.8408)  loss_mask: 0.0593 (0.0658)  time: 0.1965  data: 0.0002  max mem: 5511
[21:12:33.407787] Epoch: [7]  [120/781]  eta: 0:02:14  lr: 0.000250  training_loss: 1.9196 (1.9472)  mae_loss: 0.0444 (0.0452)  classification_loss: 1.8080 (1.8363)  loss_mask: 0.0648 (0.0657)  time: 0.1954  data: 0.0002  max mem: 5511
[21:12:37.421717] Epoch: [7]  [140/781]  eta: 0:02:09  lr: 0.000250  training_loss: 1.9070 (1.9409)  mae_loss: 0.0440 (0.0450)  classification_loss: 1.8080 (1.8307)  loss_mask: 0.0583 (0.0652)  time: 0.2006  data: 0.0002  max mem: 5511
[21:12:41.372661] Epoch: [7]  [160/781]  eta: 0:02:05  lr: 0.000250  training_loss: 1.9312 (1.9427)  mae_loss: 0.0464 (0.0451)  classification_loss: 1.8216 (1.8325)  loss_mask: 0.0645 (0.0651)  time: 0.1975  data: 0.0002  max mem: 5511
[21:12:45.300959] Epoch: [7]  [180/781]  eta: 0:02:00  lr: 0.000250  training_loss: 1.9481 (1.9414)  mae_loss: 0.0454 (0.0452)  classification_loss: 1.8351 (1.8312)  loss_mask: 0.0662 (0.0651)  time: 0.1963  data: 0.0002  max mem: 5511
[21:12:49.236522] Epoch: [7]  [200/781]  eta: 0:01:56  lr: 0.000250  training_loss: 1.9475 (1.9433)  mae_loss: 0.0453 (0.0451)  classification_loss: 1.8448 (1.8333)  loss_mask: 0.0623 (0.0649)  time: 0.1967  data: 0.0002  max mem: 5511
[21:12:53.188615] Epoch: [7]  [220/781]  eta: 0:01:52  lr: 0.000250  training_loss: 1.9133 (1.9429)  mae_loss: 0.0444 (0.0451)  classification_loss: 1.8015 (1.8329)  loss_mask: 0.0621 (0.0649)  time: 0.1975  data: 0.0004  max mem: 5511
[21:12:57.151171] Epoch: [7]  [240/781]  eta: 0:01:48  lr: 0.000250  training_loss: 1.9525 (1.9451)  mae_loss: 0.0461 (0.0452)  classification_loss: 1.8401 (1.8351)  loss_mask: 0.0627 (0.0647)  time: 0.1980  data: 0.0002  max mem: 5511
[21:13:01.133721] Epoch: [7]  [260/781]  eta: 0:01:44  lr: 0.000250  training_loss: 1.9452 (1.9456)  mae_loss: 0.0452 (0.0452)  classification_loss: 1.8414 (1.8357)  loss_mask: 0.0637 (0.0647)  time: 0.1990  data: 0.0002  max mem: 5511
[21:13:05.064726] Epoch: [7]  [280/781]  eta: 0:01:40  lr: 0.000250  training_loss: 1.9559 (1.9467)  mae_loss: 0.0445 (0.0453)  classification_loss: 1.8324 (1.8367)  loss_mask: 0.0615 (0.0647)  time: 0.1965  data: 0.0002  max mem: 5511
[21:13:08.997287] Epoch: [7]  [300/781]  eta: 0:01:36  lr: 0.000250  training_loss: 1.9743 (1.9490)  mae_loss: 0.0444 (0.0452)  classification_loss: 1.8666 (1.8391)  loss_mask: 0.0652 (0.0646)  time: 0.1965  data: 0.0003  max mem: 5511
[21:13:12.923985] Epoch: [7]  [320/781]  eta: 0:01:31  lr: 0.000250  training_loss: 1.9162 (1.9475)  mae_loss: 0.0449 (0.0452)  classification_loss: 1.8157 (1.8379)  loss_mask: 0.0560 (0.0643)  time: 0.1963  data: 0.0002  max mem: 5511
[21:13:16.883018] Epoch: [7]  [340/781]  eta: 0:01:27  lr: 0.000250  training_loss: 1.9203 (1.9470)  mae_loss: 0.0452 (0.0452)  classification_loss: 1.8054 (1.8375)  loss_mask: 0.0624 (0.0643)  time: 0.1979  data: 0.0002  max mem: 5511
[21:13:20.806400] Epoch: [7]  [360/781]  eta: 0:01:23  lr: 0.000250  training_loss: 1.9098 (1.9453)  mae_loss: 0.0456 (0.0453)  classification_loss: 1.8057 (1.8355)  loss_mask: 0.0639 (0.0644)  time: 0.1961  data: 0.0001  max mem: 5511
[21:13:24.739764] Epoch: [7]  [380/781]  eta: 0:01:19  lr: 0.000250  training_loss: 1.9510 (1.9460)  mae_loss: 0.0462 (0.0454)  classification_loss: 1.8381 (1.8362)  loss_mask: 0.0609 (0.0644)  time: 0.1966  data: 0.0002  max mem: 5511
[21:13:28.688206] Epoch: [7]  [400/781]  eta: 0:01:15  lr: 0.000250  training_loss: 1.9282 (1.9450)  mae_loss: 0.0433 (0.0453)  classification_loss: 1.8048 (1.8350)  loss_mask: 0.0653 (0.0646)  time: 0.1973  data: 0.0002  max mem: 5511
[21:13:32.617909] Epoch: [7]  [420/781]  eta: 0:01:11  lr: 0.000250  training_loss: 1.9545 (1.9444)  mae_loss: 0.0461 (0.0454)  classification_loss: 1.8480 (1.8346)  loss_mask: 0.0597 (0.0645)  time: 0.1964  data: 0.0002  max mem: 5511
[21:13:36.579203] Epoch: [7]  [440/781]  eta: 0:01:07  lr: 0.000250  training_loss: 1.9371 (1.9437)  mae_loss: 0.0439 (0.0453)  classification_loss: 1.8197 (1.8338)  loss_mask: 0.0625 (0.0645)  time: 0.1980  data: 0.0002  max mem: 5511
[21:13:40.537847] Epoch: [7]  [460/781]  eta: 0:01:03  lr: 0.000250  training_loss: 1.8795 (1.9422)  mae_loss: 0.0457 (0.0453)  classification_loss: 1.7559 (1.8324)  loss_mask: 0.0614 (0.0645)  time: 0.1979  data: 0.0002  max mem: 5511
[21:13:44.472206] Epoch: [7]  [480/781]  eta: 0:00:59  lr: 0.000250  training_loss: 1.9617 (1.9426)  mae_loss: 0.0434 (0.0453)  classification_loss: 1.8534 (1.8329)  loss_mask: 0.0623 (0.0644)  time: 0.1966  data: 0.0002  max mem: 5511
[21:13:48.416330] Epoch: [7]  [500/781]  eta: 0:00:55  lr: 0.000250  training_loss: 1.9141 (1.9416)  mae_loss: 0.0433 (0.0452)  classification_loss: 1.8165 (1.8322)  loss_mask: 0.0583 (0.0643)  time: 0.1971  data: 0.0005  max mem: 5511
[21:13:52.399825] Epoch: [7]  [520/781]  eta: 0:00:51  lr: 0.000250  training_loss: 1.9405 (1.9418)  mae_loss: 0.0444 (0.0451)  classification_loss: 1.8306 (1.8325)  loss_mask: 0.0591 (0.0641)  time: 0.1991  data: 0.0002  max mem: 5511
[21:13:56.342035] Epoch: [7]  [540/781]  eta: 0:00:47  lr: 0.000250  training_loss: 1.9496 (1.9417)  mae_loss: 0.0429 (0.0451)  classification_loss: 1.8322 (1.8326)  loss_mask: 0.0614 (0.0640)  time: 0.1970  data: 0.0004  max mem: 5511
[21:14:00.276935] Epoch: [7]  [560/781]  eta: 0:00:43  lr: 0.000249  training_loss: 1.9158 (1.9415)  mae_loss: 0.0440 (0.0451)  classification_loss: 1.8143 (1.8327)  loss_mask: 0.0582 (0.0637)  time: 0.1966  data: 0.0002  max mem: 5511
[21:14:04.202665] Epoch: [7]  [580/781]  eta: 0:00:39  lr: 0.000249  training_loss: 1.8871 (1.9405)  mae_loss: 0.0422 (0.0450)  classification_loss: 1.7910 (1.8319)  loss_mask: 0.0575 (0.0635)  time: 0.1962  data: 0.0002  max mem: 5511
[21:14:08.134956] Epoch: [7]  [600/781]  eta: 0:00:35  lr: 0.000249  training_loss: 1.9171 (1.9399)  mae_loss: 0.0428 (0.0450)  classification_loss: 1.7959 (1.8314)  loss_mask: 0.0558 (0.0634)  time: 0.1965  data: 0.0002  max mem: 5511
[21:14:12.089789] Epoch: [7]  [620/781]  eta: 0:00:31  lr: 0.000249  training_loss: 1.8906 (1.9389)  mae_loss: 0.0443 (0.0450)  classification_loss: 1.7826 (1.8306)  loss_mask: 0.0582 (0.0633)  time: 0.1977  data: 0.0004  max mem: 5511
[21:14:16.074747] Epoch: [7]  [640/781]  eta: 0:00:27  lr: 0.000249  training_loss: 1.8986 (1.9382)  mae_loss: 0.0466 (0.0450)  classification_loss: 1.7816 (1.8298)  loss_mask: 0.0610 (0.0633)  time: 0.1991  data: 0.0002  max mem: 5511
[21:14:20.047445] Epoch: [7]  [660/781]  eta: 0:00:24  lr: 0.000249  training_loss: 1.9535 (1.9388)  mae_loss: 0.0432 (0.0450)  classification_loss: 1.8358 (1.8305)  loss_mask: 0.0594 (0.0632)  time: 0.1985  data: 0.0002  max mem: 5511
[21:14:24.006783] Epoch: [7]  [680/781]  eta: 0:00:20  lr: 0.000249  training_loss: 1.9865 (1.9393)  mae_loss: 0.0446 (0.0450)  classification_loss: 1.8680 (1.8310)  loss_mask: 0.0614 (0.0633)  time: 0.1979  data: 0.0002  max mem: 5511
[21:14:28.003052] Epoch: [7]  [700/781]  eta: 0:00:16  lr: 0.000249  training_loss: 1.9281 (1.9391)  mae_loss: 0.0447 (0.0450)  classification_loss: 1.8208 (1.8310)  loss_mask: 0.0563 (0.0631)  time: 0.1997  data: 0.0002  max mem: 5511
[21:14:31.927052] Epoch: [7]  [720/781]  eta: 0:00:12  lr: 0.000249  training_loss: 1.9437 (1.9393)  mae_loss: 0.0421 (0.0449)  classification_loss: 1.8507 (1.8313)  loss_mask: 0.0569 (0.0631)  time: 0.1961  data: 0.0002  max mem: 5511
[21:14:35.882585] Epoch: [7]  [740/781]  eta: 0:00:08  lr: 0.000249  training_loss: 1.9152 (1.9386)  mae_loss: 0.0430 (0.0449)  classification_loss: 1.8182 (1.8307)  loss_mask: 0.0587 (0.0630)  time: 0.1977  data: 0.0002  max mem: 5511
[21:14:39.830264] Epoch: [7]  [760/781]  eta: 0:00:04  lr: 0.000249  training_loss: 1.9721 (1.9388)  mae_loss: 0.0427 (0.0449)  classification_loss: 1.8714 (1.8310)  loss_mask: 0.0596 (0.0629)  time: 0.1973  data: 0.0002  max mem: 5511
[21:14:43.758455] Epoch: [7]  [780/781]  eta: 0:00:00  lr: 0.000249  training_loss: 1.8980 (1.9385)  mae_loss: 0.0434 (0.0448)  classification_loss: 1.7883 (1.8305)  loss_mask: 0.0634 (0.0631)  time: 0.1963  data: 0.0002  max mem: 5511
[21:14:43.920908] Epoch: [7] Total time: 0:02:35 (0.1985 s / it)
[21:14:43.921405] Averaged stats: lr: 0.000249  training_loss: 1.8980 (1.9385)  mae_loss: 0.0434 (0.0448)  classification_loss: 1.7883 (1.8305)  loss_mask: 0.0634 (0.0631)
[21:14:44.565304] Test:  [  0/157]  eta: 0:01:40  testing_loss: 1.1554 (1.1554)  acc1: 59.3750 (59.3750)  acc5: 90.6250 (90.6250)  time: 0.6375  data: 0.6080  max mem: 5511
[21:14:44.853315] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.2590 (1.2635)  acc1: 56.2500 (55.5398)  acc5: 95.3125 (94.0341)  time: 0.0838  data: 0.0555  max mem: 5511
[21:14:45.144919] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.2359 (1.2284)  acc1: 56.2500 (56.5476)  acc5: 95.3125 (94.9405)  time: 0.0288  data: 0.0002  max mem: 5511
[21:14:45.429259] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.2191 (1.2332)  acc1: 56.2500 (56.7540)  acc5: 95.3125 (95.0605)  time: 0.0287  data: 0.0003  max mem: 5511
[21:14:45.714554] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.1950 (1.2290)  acc1: 56.2500 (56.8598)  acc5: 93.7500 (94.7790)  time: 0.0284  data: 0.0003  max mem: 5511
[21:14:45.999022] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.1735 (1.2230)  acc1: 57.8125 (57.4142)  acc5: 95.3125 (95.0061)  time: 0.0283  data: 0.0002  max mem: 5511
[21:14:46.282098] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.1735 (1.2158)  acc1: 59.3750 (57.5564)  acc5: 95.3125 (95.0307)  time: 0.0282  data: 0.0002  max mem: 5511
[21:14:46.565715] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.1562 (1.2055)  acc1: 60.9375 (58.3187)  acc5: 95.3125 (95.1364)  time: 0.0282  data: 0.0002  max mem: 5511
[21:14:46.849186] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.1798 (1.2064)  acc1: 59.3750 (58.1790)  acc5: 95.3125 (95.0424)  time: 0.0282  data: 0.0002  max mem: 5511
[21:14:47.131590] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.2355 (1.2106)  acc1: 56.2500 (57.8984)  acc5: 95.3125 (95.0893)  time: 0.0281  data: 0.0002  max mem: 5511
[21:14:47.413297] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.2823 (1.2174)  acc1: 54.6875 (57.8125)  acc5: 95.3125 (95.0340)  time: 0.0280  data: 0.0001  max mem: 5511
[21:14:47.697220] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.2592 (1.2181)  acc1: 54.6875 (57.7562)  acc5: 95.3125 (95.1577)  time: 0.0281  data: 0.0001  max mem: 5511
[21:14:47.981600] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.1954 (1.2153)  acc1: 56.2500 (57.7608)  acc5: 96.8750 (95.2350)  time: 0.0283  data: 0.0002  max mem: 5511
[21:14:48.264846] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.1764 (1.2174)  acc1: 56.2500 (57.5978)  acc5: 95.3125 (95.1932)  time: 0.0282  data: 0.0002  max mem: 5511
[21:14:48.547209] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.2500 (1.2208)  acc1: 56.2500 (57.5133)  acc5: 95.3125 (95.1463)  time: 0.0281  data: 0.0001  max mem: 5511
[21:14:48.827135] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.2324 (1.2175)  acc1: 57.8125 (57.7194)  acc5: 95.3125 (95.2194)  time: 0.0280  data: 0.0001  max mem: 5511
[21:14:48.978226] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.2083 (1.2180)  acc1: 57.8125 (57.5800)  acc5: 95.3125 (95.2500)  time: 0.0270  data: 0.0001  max mem: 5511
[21:14:49.133608] Test: Total time: 0:00:05 (0.0332 s / it)
[21:14:49.134123] * Acc@1 57.580 Acc@5 95.250 loss 1.218
[21:14:49.134412] Accuracy of the network on the 10000 test images: 57.6%
[21:14:49.134587] Max accuracy: 57.58%
[21:14:49.335551] log_dir: ./output_dir
[21:14:50.221077] Epoch: [8]  [  0/781]  eta: 0:11:30  lr: 0.000249  training_loss: 1.7493 (1.7493)  mae_loss: 0.0459 (0.0459)  classification_loss: 1.6343 (1.6343)  loss_mask: 0.0691 (0.0691)  time: 0.8837  data: 0.6814  max mem: 5511
[21:14:54.146904] Epoch: [8]  [ 20/781]  eta: 0:02:54  lr: 0.000249  training_loss: 1.9147 (1.9252)  mae_loss: 0.0429 (0.0434)  classification_loss: 1.7943 (1.8162)  loss_mask: 0.0629 (0.0656)  time: 0.1962  data: 0.0002  max mem: 5511
[21:14:58.089157] Epoch: [8]  [ 40/781]  eta: 0:02:38  lr: 0.000249  training_loss: 1.9246 (1.9205)  mae_loss: 0.0435 (0.0435)  classification_loss: 1.8276 (1.8122)  loss_mask: 0.0641 (0.0648)  time: 0.1970  data: 0.0005  max mem: 5511
[21:15:02.038038] Epoch: [8]  [ 60/781]  eta: 0:02:30  lr: 0.000249  training_loss: 1.8912 (1.9125)  mae_loss: 0.0428 (0.0435)  classification_loss: 1.7780 (1.8050)  loss_mask: 0.0624 (0.0640)  time: 0.1973  data: 0.0006  max mem: 5511
[21:15:05.993567] Epoch: [8]  [ 80/781]  eta: 0:02:24  lr: 0.000249  training_loss: 1.9622 (1.9278)  mae_loss: 0.0435 (0.0435)  classification_loss: 1.8508 (1.8201)  loss_mask: 0.0648 (0.0643)  time: 0.1977  data: 0.0002  max mem: 5511
[21:15:09.932671] Epoch: [8]  [100/781]  eta: 0:02:18  lr: 0.000249  training_loss: 1.9215 (1.9304)  mae_loss: 0.0420 (0.0431)  classification_loss: 1.8318 (1.8243)  loss_mask: 0.0594 (0.0630)  time: 0.1969  data: 0.0002  max mem: 5511
[21:15:13.868228] Epoch: [8]  [120/781]  eta: 0:02:13  lr: 0.000249  training_loss: 1.9223 (1.9250)  mae_loss: 0.0421 (0.0429)  classification_loss: 1.8007 (1.8202)  loss_mask: 0.0564 (0.0619)  time: 0.1967  data: 0.0002  max mem: 5511
[21:15:17.844919] Epoch: [8]  [140/781]  eta: 0:02:09  lr: 0.000249  training_loss: 1.8749 (1.9178)  mae_loss: 0.0418 (0.0428)  classification_loss: 1.7733 (1.8139)  loss_mask: 0.0538 (0.0611)  time: 0.1988  data: 0.0002  max mem: 5511
[21:15:21.784621] Epoch: [8]  [160/781]  eta: 0:02:05  lr: 0.000249  training_loss: 1.9255 (1.9187)  mae_loss: 0.0437 (0.0429)  classification_loss: 1.8171 (1.8151)  loss_mask: 0.0579 (0.0607)  time: 0.1969  data: 0.0002  max mem: 5511
[21:15:25.707670] Epoch: [8]  [180/781]  eta: 0:02:00  lr: 0.000249  training_loss: 1.9108 (1.9165)  mae_loss: 0.0437 (0.0430)  classification_loss: 1.8029 (1.8129)  loss_mask: 0.0606 (0.0606)  time: 0.1961  data: 0.0002  max mem: 5511
[21:15:29.641851] Epoch: [8]  [200/781]  eta: 0:01:56  lr: 0.000249  training_loss: 1.9011 (1.9159)  mae_loss: 0.0425 (0.0429)  classification_loss: 1.7969 (1.8120)  loss_mask: 0.0648 (0.0610)  time: 0.1966  data: 0.0002  max mem: 5511
[21:15:33.611442] Epoch: [8]  [220/781]  eta: 0:01:52  lr: 0.000249  training_loss: 1.9281 (1.9162)  mae_loss: 0.0416 (0.0430)  classification_loss: 1.8240 (1.8113)  loss_mask: 0.0683 (0.0619)  time: 0.1984  data: 0.0002  max mem: 5511
[21:15:37.538252] Epoch: [8]  [240/781]  eta: 0:01:48  lr: 0.000249  training_loss: 1.9248 (1.9169)  mae_loss: 0.0431 (0.0429)  classification_loss: 1.8006 (1.8121)  loss_mask: 0.0562 (0.0619)  time: 0.1963  data: 0.0003  max mem: 5511
[21:15:41.446154] Epoch: [8]  [260/781]  eta: 0:01:43  lr: 0.000249  training_loss: 1.8964 (1.9169)  mae_loss: 0.0423 (0.0428)  classification_loss: 1.7915 (1.8124)  loss_mask: 0.0579 (0.0616)  time: 0.1953  data: 0.0003  max mem: 5511
[21:15:45.386729] Epoch: [8]  [280/781]  eta: 0:01:39  lr: 0.000249  training_loss: 1.9136 (1.9184)  mae_loss: 0.0459 (0.0430)  classification_loss: 1.8098 (1.8136)  loss_mask: 0.0599 (0.0617)  time: 0.1969  data: 0.0002  max mem: 5511
[21:15:49.335863] Epoch: [8]  [300/781]  eta: 0:01:35  lr: 0.000249  training_loss: 1.9594 (1.9199)  mae_loss: 0.0406 (0.0430)  classification_loss: 1.8288 (1.8150)  loss_mask: 0.0610 (0.0620)  time: 0.1973  data: 0.0002  max mem: 5511
[21:15:53.259737] Epoch: [8]  [320/781]  eta: 0:01:31  lr: 0.000249  training_loss: 1.8962 (1.9187)  mae_loss: 0.0431 (0.0430)  classification_loss: 1.7938 (1.8140)  loss_mask: 0.0559 (0.0617)  time: 0.1961  data: 0.0002  max mem: 5511
[21:15:57.203818] Epoch: [8]  [340/781]  eta: 0:01:27  lr: 0.000249  training_loss: 1.9308 (1.9187)  mae_loss: 0.0433 (0.0431)  classification_loss: 1.8280 (1.8144)  loss_mask: 0.0508 (0.0612)  time: 0.1971  data: 0.0002  max mem: 5511
[21:16:01.219152] Epoch: [8]  [360/781]  eta: 0:01:23  lr: 0.000249  training_loss: 1.9339 (1.9189)  mae_loss: 0.0443 (0.0432)  classification_loss: 1.8350 (1.8150)  loss_mask: 0.0526 (0.0607)  time: 0.2007  data: 0.0002  max mem: 5511
[21:16:05.142716] Epoch: [8]  [380/781]  eta: 0:01:19  lr: 0.000249  training_loss: 1.9174 (1.9177)  mae_loss: 0.0424 (0.0431)  classification_loss: 1.8206 (1.8142)  loss_mask: 0.0500 (0.0604)  time: 0.1961  data: 0.0002  max mem: 5511
[21:16:09.062232] Epoch: [8]  [400/781]  eta: 0:01:15  lr: 0.000249  training_loss: 1.9375 (1.9181)  mae_loss: 0.0407 (0.0431)  classification_loss: 1.8379 (1.8148)  loss_mask: 0.0585 (0.0603)  time: 0.1959  data: 0.0002  max mem: 5511
[21:16:12.966629] Epoch: [8]  [420/781]  eta: 0:01:11  lr: 0.000249  training_loss: 1.8851 (1.9172)  mae_loss: 0.0452 (0.0432)  classification_loss: 1.7677 (1.8135)  loss_mask: 0.0617 (0.0605)  time: 0.1951  data: 0.0002  max mem: 5511
[21:16:16.932004] Epoch: [8]  [440/781]  eta: 0:01:07  lr: 0.000249  training_loss: 1.8427 (1.9149)  mae_loss: 0.0421 (0.0432)  classification_loss: 1.7398 (1.8113)  loss_mask: 0.0570 (0.0603)  time: 0.1982  data: 0.0002  max mem: 5511
[21:16:20.858401] Epoch: [8]  [460/781]  eta: 0:01:03  lr: 0.000249  training_loss: 1.8791 (1.9147)  mae_loss: 0.0417 (0.0432)  classification_loss: 1.7828 (1.8111)  loss_mask: 0.0587 (0.0603)  time: 0.1962  data: 0.0002  max mem: 5511
[21:16:24.777816] Epoch: [8]  [480/781]  eta: 0:00:59  lr: 0.000249  training_loss: 1.8813 (1.9138)  mae_loss: 0.0416 (0.0432)  classification_loss: 1.7974 (1.8105)  loss_mask: 0.0542 (0.0601)  time: 0.1959  data: 0.0003  max mem: 5511
[21:16:28.687466] Epoch: [8]  [500/781]  eta: 0:00:55  lr: 0.000249  training_loss: 1.9071 (1.9141)  mae_loss: 0.0416 (0.0431)  classification_loss: 1.8025 (1.8108)  loss_mask: 0.0565 (0.0602)  time: 0.1954  data: 0.0002  max mem: 5511
[21:16:32.606849] Epoch: [8]  [520/781]  eta: 0:00:51  lr: 0.000249  training_loss: 1.8662 (1.9133)  mae_loss: 0.0427 (0.0432)  classification_loss: 1.7715 (1.8102)  loss_mask: 0.0555 (0.0600)  time: 0.1959  data: 0.0002  max mem: 5511
[21:16:36.539044] Epoch: [8]  [540/781]  eta: 0:00:47  lr: 0.000249  training_loss: 1.8877 (1.9127)  mae_loss: 0.0428 (0.0431)  classification_loss: 1.7880 (1.8097)  loss_mask: 0.0528 (0.0598)  time: 0.1965  data: 0.0002  max mem: 5511
[21:16:40.489719] Epoch: [8]  [560/781]  eta: 0:00:43  lr: 0.000249  training_loss: 1.8751 (1.9117)  mae_loss: 0.0406 (0.0431)  classification_loss: 1.7787 (1.8090)  loss_mask: 0.0543 (0.0596)  time: 0.1974  data: 0.0002  max mem: 5511
[21:16:44.410078] Epoch: [8]  [580/781]  eta: 0:00:39  lr: 0.000249  training_loss: 1.9365 (1.9121)  mae_loss: 0.0432 (0.0431)  classification_loss: 1.8488 (1.8096)  loss_mask: 0.0516 (0.0593)  time: 0.1959  data: 0.0002  max mem: 5511
[21:16:48.367859] Epoch: [8]  [600/781]  eta: 0:00:35  lr: 0.000249  training_loss: 1.8587 (1.9116)  mae_loss: 0.0422 (0.0431)  classification_loss: 1.7546 (1.8091)  loss_mask: 0.0598 (0.0595)  time: 0.1978  data: 0.0007  max mem: 5511
[21:16:52.339448] Epoch: [8]  [620/781]  eta: 0:00:31  lr: 0.000249  training_loss: 1.8629 (1.9109)  mae_loss: 0.0409 (0.0430)  classification_loss: 1.7452 (1.8081)  loss_mask: 0.0665 (0.0598)  time: 0.1985  data: 0.0004  max mem: 5511
[21:16:56.329747] Epoch: [8]  [640/781]  eta: 0:00:27  lr: 0.000249  training_loss: 1.8673 (1.9096)  mae_loss: 0.0421 (0.0430)  classification_loss: 1.7698 (1.8070)  loss_mask: 0.0526 (0.0596)  time: 0.1994  data: 0.0002  max mem: 5511
[21:17:00.257415] Epoch: [8]  [660/781]  eta: 0:00:23  lr: 0.000249  training_loss: 1.8951 (1.9093)  mae_loss: 0.0426 (0.0430)  classification_loss: 1.7981 (1.8070)  loss_mask: 0.0498 (0.0593)  time: 0.1963  data: 0.0002  max mem: 5511
[21:17:04.203136] Epoch: [8]  [680/781]  eta: 0:00:19  lr: 0.000249  training_loss: 1.8466 (1.9078)  mae_loss: 0.0408 (0.0429)  classification_loss: 1.7477 (1.8058)  loss_mask: 0.0492 (0.0591)  time: 0.1971  data: 0.0002  max mem: 5511
[21:17:08.150003] Epoch: [8]  [700/781]  eta: 0:00:16  lr: 0.000249  training_loss: 1.8957 (1.9073)  mae_loss: 0.0391 (0.0429)  classification_loss: 1.8043 (1.8055)  loss_mask: 0.0492 (0.0589)  time: 0.1973  data: 0.0002  max mem: 5511
[21:17:12.163966] Epoch: [8]  [720/781]  eta: 0:00:12  lr: 0.000249  training_loss: 1.8880 (1.9068)  mae_loss: 0.0396 (0.0428)  classification_loss: 1.7992 (1.8053)  loss_mask: 0.0527 (0.0587)  time: 0.2006  data: 0.0002  max mem: 5511
[21:17:16.086999] Epoch: [8]  [740/781]  eta: 0:00:08  lr: 0.000249  training_loss: 1.8693 (1.9057)  mae_loss: 0.0388 (0.0427)  classification_loss: 1.7803 (1.8043)  loss_mask: 0.0542 (0.0587)  time: 0.1961  data: 0.0003  max mem: 5511
[21:17:19.992211] Epoch: [8]  [760/781]  eta: 0:00:04  lr: 0.000249  training_loss: 1.8897 (1.9058)  mae_loss: 0.0433 (0.0428)  classification_loss: 1.7894 (1.8045)  loss_mask: 0.0511 (0.0585)  time: 0.1952  data: 0.0003  max mem: 5511
[21:17:23.895395] Epoch: [8]  [780/781]  eta: 0:00:00  lr: 0.000249  training_loss: 1.9210 (1.9055)  mae_loss: 0.0411 (0.0427)  classification_loss: 1.8239 (1.8044)  loss_mask: 0.0554 (0.0584)  time: 0.1951  data: 0.0002  max mem: 5511
[21:17:24.048897] Epoch: [8] Total time: 0:02:34 (0.1981 s / it)
[21:17:24.049376] Averaged stats: lr: 0.000249  training_loss: 1.9210 (1.9055)  mae_loss: 0.0411 (0.0427)  classification_loss: 1.8239 (1.8044)  loss_mask: 0.0554 (0.0584)
[21:17:24.673211] Test:  [  0/157]  eta: 0:01:37  testing_loss: 1.2028 (1.2028)  acc1: 54.6875 (54.6875)  acc5: 96.8750 (96.8750)  time: 0.6192  data: 0.5735  max mem: 5511
[21:17:24.958226] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.2355 (1.2683)  acc1: 56.2500 (54.9716)  acc5: 95.3125 (96.0227)  time: 0.0820  data: 0.0523  max mem: 5511
[21:17:25.242805] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.2275 (1.2358)  acc1: 56.2500 (57.3661)  acc5: 95.3125 (95.9821)  time: 0.0283  data: 0.0002  max mem: 5511
[21:17:25.527393] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.2516 (1.2381)  acc1: 59.3750 (57.8125)  acc5: 95.3125 (95.7661)  time: 0.0283  data: 0.0002  max mem: 5511
[21:17:25.810648] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 1.2306 (1.2292)  acc1: 57.8125 (58.0793)  acc5: 95.3125 (95.6555)  time: 0.0283  data: 0.0002  max mem: 5511
[21:17:26.093686] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.2059 (1.2252)  acc1: 60.9375 (58.6397)  acc5: 95.3125 (95.6189)  time: 0.0282  data: 0.0002  max mem: 5511
[21:17:26.376531] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.2059 (1.2198)  acc1: 60.9375 (58.8371)  acc5: 95.3125 (95.5430)  time: 0.0282  data: 0.0002  max mem: 5511
[21:17:26.659086] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.1501 (1.2096)  acc1: 60.9375 (59.2430)  acc5: 95.3125 (95.5766)  time: 0.0282  data: 0.0002  max mem: 5511
[21:17:26.941807] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.1632 (1.2106)  acc1: 60.9375 (59.3750)  acc5: 95.3125 (95.5826)  time: 0.0281  data: 0.0002  max mem: 5511
[21:17:27.224798] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.2260 (1.2129)  acc1: 59.3750 (59.1861)  acc5: 95.3125 (95.5701)  time: 0.0282  data: 0.0002  max mem: 5511
[21:17:27.507755] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.2285 (1.2204)  acc1: 57.8125 (58.6943)  acc5: 96.8750 (95.6683)  time: 0.0282  data: 0.0002  max mem: 5511
[21:17:27.791983] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.2698 (1.2213)  acc1: 54.6875 (58.6149)  acc5: 96.8750 (95.6222)  time: 0.0282  data: 0.0002  max mem: 5511
[21:17:28.078555] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.1807 (1.2172)  acc1: 60.9375 (58.7810)  acc5: 95.3125 (95.6999)  time: 0.0284  data: 0.0003  max mem: 5511
[21:17:28.368232] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.2186 (1.2197)  acc1: 57.8125 (58.5520)  acc5: 95.3125 (95.6345)  time: 0.0287  data: 0.0003  max mem: 5511
[21:17:28.651538] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.2382 (1.2201)  acc1: 57.8125 (58.5550)  acc5: 93.7500 (95.5009)  time: 0.0285  data: 0.0002  max mem: 5511
[21:17:28.934527] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.1909 (1.2161)  acc1: 59.3750 (58.8266)  acc5: 93.7500 (95.4574)  time: 0.0281  data: 0.0002  max mem: 5511
[21:17:29.088916] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.1999 (1.2179)  acc1: 59.3750 (58.7200)  acc5: 93.7500 (95.4500)  time: 0.0272  data: 0.0002  max mem: 5511
[21:17:29.239441] Test: Total time: 0:00:05 (0.0330 s / it)
[21:17:29.239921] * Acc@1 58.720 Acc@5 95.450 loss 1.218
[21:17:29.240241] Accuracy of the network on the 10000 test images: 58.7%
[21:17:29.240431] Max accuracy: 58.72%
[21:17:29.486718] log_dir: ./output_dir
[21:17:30.407442] Epoch: [9]  [  0/781]  eta: 0:11:57  lr: 0.000249  training_loss: 1.7802 (1.7802)  mae_loss: 0.0393 (0.0393)  classification_loss: 1.6949 (1.6949)  loss_mask: 0.0459 (0.0459)  time: 0.9189  data: 0.7111  max mem: 5511
[21:17:34.348082] Epoch: [9]  [ 20/781]  eta: 0:02:56  lr: 0.000249  training_loss: 1.8848 (1.8994)  mae_loss: 0.0412 (0.0416)  classification_loss: 1.7874 (1.8093)  loss_mask: 0.0489 (0.0485)  time: 0.1969  data: 0.0002  max mem: 5511
[21:17:38.298479] Epoch: [9]  [ 40/781]  eta: 0:02:39  lr: 0.000249  training_loss: 1.8636 (1.8856)  mae_loss: 0.0417 (0.0418)  classification_loss: 1.7660 (1.7942)  loss_mask: 0.0522 (0.0496)  time: 0.1974  data: 0.0002  max mem: 5511
[21:17:42.231344] Epoch: [9]  [ 60/781]  eta: 0:02:30  lr: 0.000249  training_loss: 1.9109 (1.9010)  mae_loss: 0.0413 (0.0420)  classification_loss: 1.8111 (1.8070)  loss_mask: 0.0561 (0.0520)  time: 0.1965  data: 0.0002  max mem: 5511
[21:17:46.166029] Epoch: [9]  [ 80/781]  eta: 0:02:24  lr: 0.000249  training_loss: 1.8605 (1.8975)  mae_loss: 0.0408 (0.0420)  classification_loss: 1.7747 (1.8034)  loss_mask: 0.0500 (0.0521)  time: 0.1966  data: 0.0002  max mem: 5511
[21:17:50.113340] Epoch: [9]  [100/781]  eta: 0:02:18  lr: 0.000249  training_loss: 1.8581 (1.8933)  mae_loss: 0.0397 (0.0418)  classification_loss: 1.7607 (1.7987)  loss_mask: 0.0575 (0.0528)  time: 0.1973  data: 0.0002  max mem: 5511
[21:17:54.072327] Epoch: [9]  [120/781]  eta: 0:02:14  lr: 0.000249  training_loss: 1.8426 (1.8861)  mae_loss: 0.0402 (0.0416)  classification_loss: 1.7488 (1.7917)  loss_mask: 0.0491 (0.0527)  time: 0.1979  data: 0.0002  max mem: 5511
[21:17:58.002383] Epoch: [9]  [140/781]  eta: 0:02:09  lr: 0.000249  training_loss: 1.8285 (1.8815)  mae_loss: 0.0398 (0.0414)  classification_loss: 1.7394 (1.7878)  loss_mask: 0.0505 (0.0523)  time: 0.1964  data: 0.0002  max mem: 5511
[21:18:01.945380] Epoch: [9]  [160/781]  eta: 0:02:05  lr: 0.000249  training_loss: 1.8075 (1.8757)  mae_loss: 0.0410 (0.0414)  classification_loss: 1.7112 (1.7819)  loss_mask: 0.0519 (0.0524)  time: 0.1971  data: 0.0002  max mem: 5511
[21:18:05.895158] Epoch: [9]  [180/781]  eta: 0:02:00  lr: 0.000249  training_loss: 1.8356 (1.8729)  mae_loss: 0.0400 (0.0413)  classification_loss: 1.7489 (1.7786)  loss_mask: 0.0565 (0.0530)  time: 0.1974  data: 0.0002  max mem: 5511
[21:18:09.844962] Epoch: [9]  [200/781]  eta: 0:01:56  lr: 0.000249  training_loss: 1.8624 (1.8736)  mae_loss: 0.0409 (0.0413)  classification_loss: 1.7422 (1.7779)  loss_mask: 0.0624 (0.0544)  time: 0.1974  data: 0.0003  max mem: 5511
[21:18:13.800930] Epoch: [9]  [220/781]  eta: 0:01:52  lr: 0.000249  training_loss: 1.8903 (1.8744)  mae_loss: 0.0394 (0.0411)  classification_loss: 1.8052 (1.7789)  loss_mask: 0.0514 (0.0543)  time: 0.1977  data: 0.0002  max mem: 5511
[21:18:17.777819] Epoch: [9]  [240/781]  eta: 0:01:48  lr: 0.000249  training_loss: 1.8229 (1.8740)  mae_loss: 0.0406 (0.0411)  classification_loss: 1.7350 (1.7788)  loss_mask: 0.0524 (0.0541)  time: 0.1988  data: 0.0002  max mem: 5511
[21:18:21.803271] Epoch: [9]  [260/781]  eta: 0:01:44  lr: 0.000249  training_loss: 1.8764 (1.8733)  mae_loss: 0.0422 (0.0412)  classification_loss: 1.7735 (1.7778)  loss_mask: 0.0524 (0.0542)  time: 0.2012  data: 0.0002  max mem: 5511
[21:18:25.729993] Epoch: [9]  [280/781]  eta: 0:01:40  lr: 0.000249  training_loss: 1.8806 (1.8745)  mae_loss: 0.0410 (0.0412)  classification_loss: 1.7920 (1.7791)  loss_mask: 0.0483 (0.0541)  time: 0.1962  data: 0.0002  max mem: 5511
[21:18:29.665298] Epoch: [9]  [300/781]  eta: 0:01:36  lr: 0.000249  training_loss: 1.8575 (1.8754)  mae_loss: 0.0410 (0.0412)  classification_loss: 1.7765 (1.7805)  loss_mask: 0.0493 (0.0538)  time: 0.1967  data: 0.0002  max mem: 5511
[21:18:33.599226] Epoch: [9]  [320/781]  eta: 0:01:32  lr: 0.000249  training_loss: 1.8393 (1.8739)  mae_loss: 0.0384 (0.0411)  classification_loss: 1.7550 (1.7794)  loss_mask: 0.0482 (0.0534)  time: 0.1966  data: 0.0001  max mem: 5511
[21:18:37.545678] Epoch: [9]  [340/781]  eta: 0:01:27  lr: 0.000249  training_loss: 1.8583 (1.8726)  mae_loss: 0.0411 (0.0411)  classification_loss: 1.7644 (1.7781)  loss_mask: 0.0519 (0.0534)  time: 0.1972  data: 0.0002  max mem: 5511
[21:18:41.493328] Epoch: [9]  [360/781]  eta: 0:01:23  lr: 0.000249  training_loss: 1.8800 (1.8738)  mae_loss: 0.0397 (0.0410)  classification_loss: 1.8016 (1.7796)  loss_mask: 0.0482 (0.0532)  time: 0.1973  data: 0.0003  max mem: 5511
[21:18:45.466398] Epoch: [9]  [380/781]  eta: 0:01:19  lr: 0.000249  training_loss: 1.8666 (1.8739)  mae_loss: 0.0431 (0.0411)  classification_loss: 1.7668 (1.7798)  loss_mask: 0.0526 (0.0531)  time: 0.1986  data: 0.0003  max mem: 5511
[21:18:49.419116] Epoch: [9]  [400/781]  eta: 0:01:15  lr: 0.000249  training_loss: 1.8789 (1.8735)  mae_loss: 0.0389 (0.0410)  classification_loss: 1.7899 (1.7796)  loss_mask: 0.0483 (0.0530)  time: 0.1976  data: 0.0002  max mem: 5511
[21:18:53.360039] Epoch: [9]  [420/781]  eta: 0:01:11  lr: 0.000249  training_loss: 1.8298 (1.8708)  mae_loss: 0.0405 (0.0409)  classification_loss: 1.7368 (1.7770)  loss_mask: 0.0500 (0.0529)  time: 0.1970  data: 0.0002  max mem: 5511
[21:18:57.297889] Epoch: [9]  [440/781]  eta: 0:01:07  lr: 0.000249  training_loss: 1.8371 (1.8704)  mae_loss: 0.0408 (0.0409)  classification_loss: 1.7481 (1.7767)  loss_mask: 0.0503 (0.0528)  time: 0.1968  data: 0.0003  max mem: 5511
[21:19:01.255563] Epoch: [9]  [460/781]  eta: 0:01:03  lr: 0.000249  training_loss: 1.8251 (1.8688)  mae_loss: 0.0389 (0.0409)  classification_loss: 1.7338 (1.7753)  loss_mask: 0.0462 (0.0526)  time: 0.1978  data: 0.0003  max mem: 5511
[21:19:05.201321] Epoch: [9]  [480/781]  eta: 0:00:59  lr: 0.000249  training_loss: 1.8686 (1.8698)  mae_loss: 0.0410 (0.0409)  classification_loss: 1.7941 (1.7767)  loss_mask: 0.0422 (0.0522)  time: 0.1972  data: 0.0002  max mem: 5511
[21:19:09.146095] Epoch: [9]  [500/781]  eta: 0:00:55  lr: 0.000249  training_loss: 1.8500 (1.8693)  mae_loss: 0.0397 (0.0408)  classification_loss: 1.7641 (1.7764)  loss_mask: 0.0461 (0.0521)  time: 0.1972  data: 0.0002  max mem: 5511
[21:19:13.111623] Epoch: [9]  [520/781]  eta: 0:00:51  lr: 0.000249  training_loss: 1.9191 (1.8702)  mae_loss: 0.0431 (0.0409)  classification_loss: 1.8278 (1.7774)  loss_mask: 0.0485 (0.0519)  time: 0.1982  data: 0.0002  max mem: 5511
[21:19:17.064974] Epoch: [9]  [540/781]  eta: 0:00:47  lr: 0.000249  training_loss: 1.8657 (1.8710)  mae_loss: 0.0405 (0.0409)  classification_loss: 1.7853 (1.7783)  loss_mask: 0.0500 (0.0518)  time: 0.1976  data: 0.0003  max mem: 5511
[21:19:20.997986] Epoch: [9]  [560/781]  eta: 0:00:43  lr: 0.000248  training_loss: 1.8366 (1.8703)  mae_loss: 0.0393 (0.0408)  classification_loss: 1.7638 (1.7780)  loss_mask: 0.0443 (0.0515)  time: 0.1965  data: 0.0002  max mem: 5511
[21:19:24.926415] Epoch: [9]  [580/781]  eta: 0:00:39  lr: 0.000248  training_loss: 1.8473 (1.8695)  mae_loss: 0.0389 (0.0408)  classification_loss: 1.7599 (1.7774)  loss_mask: 0.0426 (0.0513)  time: 0.1963  data: 0.0002  max mem: 5511
[21:19:28.896592] Epoch: [9]  [600/781]  eta: 0:00:35  lr: 0.000248  training_loss: 1.8299 (1.8683)  mae_loss: 0.0395 (0.0408)  classification_loss: 1.7485 (1.7765)  loss_mask: 0.0388 (0.0510)  time: 0.1984  data: 0.0002  max mem: 5511
[21:19:32.806859] Epoch: [9]  [620/781]  eta: 0:00:31  lr: 0.000248  training_loss: 1.8625 (1.8678)  mae_loss: 0.0411 (0.0408)  classification_loss: 1.7759 (1.7762)  loss_mask: 0.0426 (0.0507)  time: 0.1954  data: 0.0002  max mem: 5511
[21:19:36.731398] Epoch: [9]  [640/781]  eta: 0:00:27  lr: 0.000248  training_loss: 1.8145 (1.8665)  mae_loss: 0.0403 (0.0408)  classification_loss: 1.7340 (1.7751)  loss_mask: 0.0447 (0.0506)  time: 0.1961  data: 0.0003  max mem: 5511
[21:19:40.670873] Epoch: [9]  [660/781]  eta: 0:00:24  lr: 0.000248  training_loss: 1.8426 (1.8663)  mae_loss: 0.0405 (0.0408)  classification_loss: 1.7771 (1.7750)  loss_mask: 0.0457 (0.0504)  time: 0.1969  data: 0.0003  max mem: 5511
[21:19:44.599714] Epoch: [9]  [680/781]  eta: 0:00:20  lr: 0.000248  training_loss: 1.8329 (1.8658)  mae_loss: 0.0399 (0.0408)  classification_loss: 1.7390 (1.7745)  loss_mask: 0.0495 (0.0504)  time: 0.1964  data: 0.0002  max mem: 5511
[21:19:48.531280] Epoch: [9]  [700/781]  eta: 0:00:16  lr: 0.000248  training_loss: 1.8218 (1.8650)  mae_loss: 0.0400 (0.0408)  classification_loss: 1.7195 (1.7735)  loss_mask: 0.0595 (0.0507)  time: 0.1965  data: 0.0003  max mem: 5511
[21:19:52.442688] Epoch: [9]  [720/781]  eta: 0:00:12  lr: 0.000248  training_loss: 1.8162 (1.8639)  mae_loss: 0.0379 (0.0407)  classification_loss: 1.7332 (1.7726)  loss_mask: 0.0463 (0.0506)  time: 0.1955  data: 0.0002  max mem: 5511
[21:19:56.355890] Epoch: [9]  [740/781]  eta: 0:00:08  lr: 0.000248  training_loss: 1.8521 (1.8633)  mae_loss: 0.0419 (0.0407)  classification_loss: 1.7660 (1.7722)  loss_mask: 0.0402 (0.0504)  time: 0.1955  data: 0.0002  max mem: 5511
[21:20:00.300695] Epoch: [9]  [760/781]  eta: 0:00:04  lr: 0.000248  training_loss: 1.9032 (1.8638)  mae_loss: 0.0396 (0.0407)  classification_loss: 1.8208 (1.7728)  loss_mask: 0.0446 (0.0503)  time: 0.1971  data: 0.0002  max mem: 5511
[21:20:04.257412] Epoch: [9]  [780/781]  eta: 0:00:00  lr: 0.000248  training_loss: 1.8708 (1.8637)  mae_loss: 0.0377 (0.0407)  classification_loss: 1.7900 (1.7729)  loss_mask: 0.0469 (0.0501)  time: 0.1977  data: 0.0002  max mem: 5511
[21:20:04.421731] Epoch: [9] Total time: 0:02:34 (0.1984 s / it)
[21:20:04.422227] Averaged stats: lr: 0.000248  training_loss: 1.8708 (1.8637)  mae_loss: 0.0377 (0.0407)  classification_loss: 1.7900 (1.7729)  loss_mask: 0.0469 (0.0501)
[21:20:05.054000] Test:  [  0/157]  eta: 0:01:38  testing_loss: 1.1362 (1.1362)  acc1: 65.6250 (65.6250)  acc5: 92.1875 (92.1875)  time: 0.6277  data: 0.5970  max mem: 5511
[21:20:05.342048] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.1882 (1.2021)  acc1: 59.3750 (59.3750)  acc5: 96.8750 (96.4489)  time: 0.0831  data: 0.0545  max mem: 5511
[21:20:05.627252] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.1471 (1.1688)  acc1: 60.9375 (60.5655)  acc5: 96.8750 (96.4286)  time: 0.0285  data: 0.0002  max mem: 5511
[21:20:05.910228] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.1527 (1.1784)  acc1: 62.5000 (60.6351)  acc5: 96.8750 (96.0181)  time: 0.0283  data: 0.0002  max mem: 5511
[21:20:06.195234] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.1654 (1.1787)  acc1: 60.9375 (60.4040)  acc5: 95.3125 (95.8460)  time: 0.0283  data: 0.0002  max mem: 5511
[21:20:06.478234] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.1554 (1.1737)  acc1: 60.9375 (60.9375)  acc5: 95.3125 (95.7414)  time: 0.0283  data: 0.0002  max mem: 5511
[21:20:06.761711] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.1330 (1.1676)  acc1: 60.9375 (60.9119)  acc5: 96.8750 (95.7736)  time: 0.0282  data: 0.0002  max mem: 5511
[21:20:07.046186] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.1082 (1.1589)  acc1: 62.5000 (61.5757)  acc5: 96.8750 (95.8627)  time: 0.0283  data: 0.0002  max mem: 5511
[21:20:07.328198] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.1223 (1.1576)  acc1: 65.6250 (61.6319)  acc5: 96.8750 (95.9491)  time: 0.0282  data: 0.0002  max mem: 5511
[21:20:07.612607] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.1468 (1.1579)  acc1: 60.9375 (61.5385)  acc5: 96.8750 (96.0165)  time: 0.0282  data: 0.0002  max mem: 5511
[21:20:07.896369] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.1917 (1.1667)  acc1: 57.8125 (61.1231)  acc5: 96.8750 (96.0241)  time: 0.0283  data: 0.0002  max mem: 5511
[21:20:08.180609] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.2302 (1.1676)  acc1: 57.8125 (61.1205)  acc5: 96.8750 (96.0867)  time: 0.0283  data: 0.0002  max mem: 5511
[21:20:08.463930] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.1607 (1.1632)  acc1: 60.9375 (61.2474)  acc5: 96.8750 (96.1002)  time: 0.0283  data: 0.0002  max mem: 5511
[21:20:08.747002] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.1607 (1.1671)  acc1: 57.8125 (60.6990)  acc5: 95.3125 (96.0878)  time: 0.0282  data: 0.0002  max mem: 5511
[21:20:09.029386] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.1747 (1.1677)  acc1: 56.2500 (60.6826)  acc5: 95.3125 (96.0217)  time: 0.0281  data: 0.0002  max mem: 5511
[21:20:09.310412] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.1482 (1.1658)  acc1: 64.0625 (60.8651)  acc5: 95.3125 (95.9851)  time: 0.0280  data: 0.0001  max mem: 5511
[21:20:09.461885] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.1396 (1.1670)  acc1: 60.9375 (60.7200)  acc5: 95.3125 (96.0500)  time: 0.0271  data: 0.0001  max mem: 5511
[21:20:09.617684] Test: Total time: 0:00:05 (0.0331 s / it)
[21:20:09.620388] * Acc@1 60.720 Acc@5 96.050 loss 1.167
[21:20:09.621469] Accuracy of the network on the 10000 test images: 60.7%
[21:20:09.621701] Max accuracy: 60.72%
[21:20:09.869218] log_dir: ./output_dir
[21:20:10.720220] Epoch: [10]  [  0/781]  eta: 0:11:03  lr: 0.000248  training_loss: 1.7503 (1.7503)  mae_loss: 0.0355 (0.0355)  classification_loss: 1.6689 (1.6689)  loss_mask: 0.0459 (0.0459)  time: 0.8491  data: 0.6158  max mem: 5511
[21:20:14.705120] Epoch: [10]  [ 20/781]  eta: 0:02:55  lr: 0.000248  training_loss: 1.8494 (1.8469)  mae_loss: 0.0390 (0.0393)  classification_loss: 1.7517 (1.7568)  loss_mask: 0.0502 (0.0508)  time: 0.1990  data: 0.0002  max mem: 5511
[21:20:18.701652] Epoch: [10]  [ 40/781]  eta: 0:02:39  lr: 0.000248  training_loss: 1.8367 (1.8446)  mae_loss: 0.0404 (0.0399)  classification_loss: 1.7486 (1.7543)  loss_mask: 0.0460 (0.0504)  time: 0.1998  data: 0.0002  max mem: 5511
[21:20:22.663593] Epoch: [10]  [ 60/781]  eta: 0:02:31  lr: 0.000248  training_loss: 1.8878 (1.8611)  mae_loss: 0.0390 (0.0400)  classification_loss: 1.8088 (1.7725)  loss_mask: 0.0416 (0.0486)  time: 0.1980  data: 0.0002  max mem: 5511
[21:20:26.618855] Epoch: [10]  [ 80/781]  eta: 0:02:24  lr: 0.000248  training_loss: 1.8502 (1.8612)  mae_loss: 0.0399 (0.0400)  classification_loss: 1.7557 (1.7737)  loss_mask: 0.0412 (0.0474)  time: 0.1977  data: 0.0002  max mem: 5511
[21:20:30.635560] Epoch: [10]  [100/781]  eta: 0:02:19  lr: 0.000248  training_loss: 1.8128 (1.8543)  mae_loss: 0.0400 (0.0398)  classification_loss: 1.7280 (1.7688)  loss_mask: 0.0392 (0.0457)  time: 0.2007  data: 0.0002  max mem: 5511
[21:20:34.637286] Epoch: [10]  [120/781]  eta: 0:02:15  lr: 0.000248  training_loss: 1.8467 (1.8531)  mae_loss: 0.0389 (0.0398)  classification_loss: 1.7573 (1.7673)  loss_mask: 0.0455 (0.0461)  time: 0.2000  data: 0.0002  max mem: 5511
[21:20:38.558420] Epoch: [10]  [140/781]  eta: 0:02:10  lr: 0.000248  training_loss: 1.7828 (1.8424)  mae_loss: 0.0399 (0.0397)  classification_loss: 1.7010 (1.7568)  loss_mask: 0.0445 (0.0458)  time: 0.1960  data: 0.0002  max mem: 5511
[21:20:42.487970] Epoch: [10]  [160/781]  eta: 0:02:05  lr: 0.000248  training_loss: 1.8239 (1.8384)  mae_loss: 0.0398 (0.0399)  classification_loss: 1.7460 (1.7529)  loss_mask: 0.0449 (0.0457)  time: 0.1964  data: 0.0002  max mem: 5511
[21:20:46.399557] Epoch: [10]  [180/781]  eta: 0:02:01  lr: 0.000248  training_loss: 1.8674 (1.8417)  mae_loss: 0.0414 (0.0400)  classification_loss: 1.7861 (1.7563)  loss_mask: 0.0422 (0.0455)  time: 0.1955  data: 0.0002  max mem: 5511
[21:20:50.315812] Epoch: [10]  [200/781]  eta: 0:01:56  lr: 0.000248  training_loss: 1.8647 (1.8432)  mae_loss: 0.0405 (0.0399)  classification_loss: 1.7664 (1.7574)  loss_mask: 0.0463 (0.0459)  time: 0.1957  data: 0.0003  max mem: 5511
[21:20:54.225915] Epoch: [10]  [220/781]  eta: 0:01:52  lr: 0.000248  training_loss: 1.8587 (1.8452)  mae_loss: 0.0373 (0.0399)  classification_loss: 1.7759 (1.7592)  loss_mask: 0.0460 (0.0462)  time: 0.1954  data: 0.0003  max mem: 5511
[21:20:58.156862] Epoch: [10]  [240/781]  eta: 0:01:48  lr: 0.000248  training_loss: 1.8251 (1.8444)  mae_loss: 0.0398 (0.0400)  classification_loss: 1.7364 (1.7582)  loss_mask: 0.0435 (0.0462)  time: 0.1965  data: 0.0003  max mem: 5511
[21:21:02.121074] Epoch: [10]  [260/781]  eta: 0:01:44  lr: 0.000248  training_loss: 1.8101 (1.8417)  mae_loss: 0.0392 (0.0399)  classification_loss: 1.7168 (1.7553)  loss_mask: 0.0458 (0.0464)  time: 0.1981  data: 0.0002  max mem: 5511
[21:21:06.076750] Epoch: [10]  [280/781]  eta: 0:01:40  lr: 0.000248  training_loss: 1.8252 (1.8415)  mae_loss: 0.0401 (0.0399)  classification_loss: 1.7332 (1.7554)  loss_mask: 0.0408 (0.0463)  time: 0.1977  data: 0.0002  max mem: 5511
[21:21:10.021314] Epoch: [10]  [300/781]  eta: 0:01:36  lr: 0.000248  training_loss: 1.8777 (1.8430)  mae_loss: 0.0412 (0.0400)  classification_loss: 1.7870 (1.7570)  loss_mask: 0.0416 (0.0460)  time: 0.1971  data: 0.0002  max mem: 5511
[21:21:13.949017] Epoch: [10]  [320/781]  eta: 0:01:31  lr: 0.000248  training_loss: 1.8057 (1.8415)  mae_loss: 0.0386 (0.0400)  classification_loss: 1.7322 (1.7560)  loss_mask: 0.0360 (0.0455)  time: 0.1963  data: 0.0002  max mem: 5511
[21:21:17.884439] Epoch: [10]  [340/781]  eta: 0:01:27  lr: 0.000248  training_loss: 1.8418 (1.8418)  mae_loss: 0.0396 (0.0400)  classification_loss: 1.7577 (1.7568)  loss_mask: 0.0397 (0.0451)  time: 0.1967  data: 0.0002  max mem: 5511
[21:21:21.838489] Epoch: [10]  [360/781]  eta: 0:01:23  lr: 0.000248  training_loss: 1.8453 (1.8427)  mae_loss: 0.0390 (0.0399)  classification_loss: 1.7602 (1.7577)  loss_mask: 0.0401 (0.0450)  time: 0.1976  data: 0.0002  max mem: 5511
[21:21:25.746863] Epoch: [10]  [380/781]  eta: 0:01:19  lr: 0.000248  training_loss: 1.8260 (1.8420)  mae_loss: 0.0404 (0.0400)  classification_loss: 1.7482 (1.7569)  loss_mask: 0.0446 (0.0451)  time: 0.1953  data: 0.0002  max mem: 5511
[21:21:29.678672] Epoch: [10]  [400/781]  eta: 0:01:15  lr: 0.000248  training_loss: 1.8597 (1.8423)  mae_loss: 0.0380 (0.0399)  classification_loss: 1.7832 (1.7574)  loss_mask: 0.0419 (0.0450)  time: 0.1965  data: 0.0003  max mem: 5511
[21:21:33.629694] Epoch: [10]  [420/781]  eta: 0:01:11  lr: 0.000248  training_loss: 1.7919 (1.8401)  mae_loss: 0.0382 (0.0399)  classification_loss: 1.7213 (1.7555)  loss_mask: 0.0389 (0.0448)  time: 0.1975  data: 0.0002  max mem: 5511
[21:21:37.600082] Epoch: [10]  [440/781]  eta: 0:01:07  lr: 0.000248  training_loss: 1.8353 (1.8404)  mae_loss: 0.0403 (0.0399)  classification_loss: 1.7561 (1.7560)  loss_mask: 0.0401 (0.0445)  time: 0.1984  data: 0.0002  max mem: 5511
[21:21:41.511016] Epoch: [10]  [460/781]  eta: 0:01:03  lr: 0.000248  training_loss: 1.8248 (1.8391)  mae_loss: 0.0367 (0.0398)  classification_loss: 1.7434 (1.7548)  loss_mask: 0.0419 (0.0445)  time: 0.1955  data: 0.0002  max mem: 5511
[21:21:45.443438] Epoch: [10]  [480/781]  eta: 0:00:59  lr: 0.000248  training_loss: 1.8486 (1.8397)  mae_loss: 0.0401 (0.0399)  classification_loss: 1.7434 (1.7552)  loss_mask: 0.0502 (0.0447)  time: 0.1965  data: 0.0001  max mem: 5511
[21:21:49.368164] Epoch: [10]  [500/781]  eta: 0:00:55  lr: 0.000248  training_loss: 1.8313 (1.8401)  mae_loss: 0.0398 (0.0399)  classification_loss: 1.7549 (1.7556)  loss_mask: 0.0426 (0.0446)  time: 0.1961  data: 0.0002  max mem: 5511
[21:21:53.301340] Epoch: [10]  [520/781]  eta: 0:00:51  lr: 0.000248  training_loss: 1.7988 (1.8391)  mae_loss: 0.0415 (0.0399)  classification_loss: 1.7252 (1.7545)  loss_mask: 0.0393 (0.0446)  time: 0.1965  data: 0.0002  max mem: 5511
[21:21:57.238070] Epoch: [10]  [540/781]  eta: 0:00:47  lr: 0.000248  training_loss: 1.8262 (1.8400)  mae_loss: 0.0393 (0.0399)  classification_loss: 1.7479 (1.7556)  loss_mask: 0.0387 (0.0445)  time: 0.1968  data: 0.0002  max mem: 5511
[21:22:01.172258] Epoch: [10]  [560/781]  eta: 0:00:43  lr: 0.000248  training_loss: 1.8261 (1.8399)  mae_loss: 0.0393 (0.0399)  classification_loss: 1.7482 (1.7557)  loss_mask: 0.0351 (0.0442)  time: 0.1966  data: 0.0002  max mem: 5511
[21:22:05.148311] Epoch: [10]  [580/781]  eta: 0:00:39  lr: 0.000248  training_loss: 1.8208 (1.8394)  mae_loss: 0.0389 (0.0399)  classification_loss: 1.7438 (1.7554)  loss_mask: 0.0365 (0.0441)  time: 0.1987  data: 0.0002  max mem: 5511
[21:22:09.092126] Epoch: [10]  [600/781]  eta: 0:00:35  lr: 0.000248  training_loss: 1.8063 (1.8387)  mae_loss: 0.0387 (0.0399)  classification_loss: 1.7265 (1.7548)  loss_mask: 0.0387 (0.0439)  time: 0.1971  data: 0.0002  max mem: 5511
[21:22:13.029692] Epoch: [10]  [620/781]  eta: 0:00:31  lr: 0.000248  training_loss: 1.7828 (1.8375)  mae_loss: 0.0395 (0.0400)  classification_loss: 1.7014 (1.7537)  loss_mask: 0.0368 (0.0437)  time: 0.1968  data: 0.0002  max mem: 5511
[21:22:17.015729] Epoch: [10]  [640/781]  eta: 0:00:27  lr: 0.000248  training_loss: 1.7896 (1.8366)  mae_loss: 0.0386 (0.0400)  classification_loss: 1.7175 (1.7531)  loss_mask: 0.0341 (0.0435)  time: 0.1992  data: 0.0002  max mem: 5511
[21:22:20.975633] Epoch: [10]  [660/781]  eta: 0:00:23  lr: 0.000248  training_loss: 1.8148 (1.8365)  mae_loss: 0.0384 (0.0399)  classification_loss: 1.7395 (1.7533)  loss_mask: 0.0364 (0.0433)  time: 0.1979  data: 0.0005  max mem: 5511
[21:22:24.901145] Epoch: [10]  [680/781]  eta: 0:00:20  lr: 0.000248  training_loss: 1.8064 (1.8359)  mae_loss: 0.0381 (0.0399)  classification_loss: 1.7307 (1.7529)  loss_mask: 0.0372 (0.0432)  time: 0.1962  data: 0.0002  max mem: 5511
[21:22:28.816693] Epoch: [10]  [700/781]  eta: 0:00:16  lr: 0.000248  training_loss: 1.8013 (1.8351)  mae_loss: 0.0398 (0.0399)  classification_loss: 1.7114 (1.7518)  loss_mask: 0.0493 (0.0434)  time: 0.1957  data: 0.0002  max mem: 5511
[21:22:32.739837] Epoch: [10]  [720/781]  eta: 0:00:12  lr: 0.000248  training_loss: 1.7674 (1.8343)  mae_loss: 0.0379 (0.0398)  classification_loss: 1.6710 (1.7508)  loss_mask: 0.0436 (0.0436)  time: 0.1961  data: 0.0002  max mem: 5511
[21:22:36.641179] Epoch: [10]  [740/781]  eta: 0:00:08  lr: 0.000248  training_loss: 1.7839 (1.8330)  mae_loss: 0.0381 (0.0398)  classification_loss: 1.6967 (1.7496)  loss_mask: 0.0425 (0.0436)  time: 0.1949  data: 0.0002  max mem: 5511
[21:22:40.547209] Epoch: [10]  [760/781]  eta: 0:00:04  lr: 0.000248  training_loss: 1.8336 (1.8331)  mae_loss: 0.0359 (0.0397)  classification_loss: 1.7567 (1.7498)  loss_mask: 0.0406 (0.0435)  time: 0.1952  data: 0.0002  max mem: 5511
[21:22:44.454890] Epoch: [10]  [780/781]  eta: 0:00:00  lr: 0.000248  training_loss: 1.8240 (1.8328)  mae_loss: 0.0373 (0.0397)  classification_loss: 1.7281 (1.7496)  loss_mask: 0.0409 (0.0435)  time: 0.1953  data: 0.0002  max mem: 5511
[21:22:44.609126] Epoch: [10] Total time: 0:02:34 (0.1981 s / it)
[21:22:44.609592] Averaged stats: lr: 0.000248  training_loss: 1.8240 (1.8328)  mae_loss: 0.0373 (0.0397)  classification_loss: 1.7281 (1.7496)  loss_mask: 0.0409 (0.0435)
[21:22:46.285501] Test:  [  0/157]  eta: 0:01:40  testing_loss: 1.1480 (1.1480)  acc1: 59.3750 (59.3750)  acc5: 92.1875 (92.1875)  time: 0.6405  data: 0.6068  max mem: 5511
[21:22:46.578221] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.1595 (1.1736)  acc1: 59.3750 (59.0909)  acc5: 95.3125 (95.5966)  time: 0.0847  data: 0.0553  max mem: 5511
[21:22:46.862221] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.1061 (1.1397)  acc1: 62.5000 (60.9375)  acc5: 96.8750 (96.1310)  time: 0.0287  data: 0.0002  max mem: 5511
[21:22:47.149499] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.1375 (1.1436)  acc1: 62.5000 (61.1895)  acc5: 96.8750 (95.9173)  time: 0.0284  data: 0.0002  max mem: 5511
[21:22:47.432667] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.1375 (1.1405)  acc1: 62.5000 (61.2805)  acc5: 96.8750 (96.0366)  time: 0.0283  data: 0.0002  max mem: 5511
[21:22:47.719161] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.1063 (1.1343)  acc1: 64.0625 (61.9792)  acc5: 96.8750 (96.1703)  time: 0.0284  data: 0.0002  max mem: 5511
[21:22:48.007259] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.0870 (1.1292)  acc1: 64.0625 (62.2439)  acc5: 96.8750 (96.1066)  time: 0.0286  data: 0.0002  max mem: 5511
[21:22:48.290972] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.0698 (1.1198)  acc1: 64.0625 (62.7421)  acc5: 96.8750 (96.1488)  time: 0.0285  data: 0.0002  max mem: 5511
[21:22:48.572756] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.1167 (1.1226)  acc1: 64.0625 (62.5772)  acc5: 96.8750 (96.1806)  time: 0.0281  data: 0.0002  max mem: 5511
[21:22:48.854768] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.1564 (1.1277)  acc1: 59.3750 (62.2081)  acc5: 96.8750 (96.1882)  time: 0.0281  data: 0.0001  max mem: 5511
[21:22:49.138633] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.1936 (1.1347)  acc1: 57.8125 (61.7574)  acc5: 95.3125 (96.1788)  time: 0.0282  data: 0.0002  max mem: 5511
[21:22:49.421026] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.2024 (1.1353)  acc1: 59.3750 (62.0495)  acc5: 95.3125 (96.2134)  time: 0.0282  data: 0.0002  max mem: 5511
[21:22:49.702378] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.1000 (1.1301)  acc1: 64.0625 (62.0480)  acc5: 96.8750 (96.2552)  time: 0.0281  data: 0.0001  max mem: 5511
[21:22:49.985231] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.1215 (1.1343)  acc1: 62.5000 (61.8321)  acc5: 96.8750 (96.1474)  time: 0.0281  data: 0.0001  max mem: 5511
[21:22:50.265793] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.1733 (1.1352)  acc1: 60.9375 (61.6135)  acc5: 95.3125 (96.1436)  time: 0.0281  data: 0.0001  max mem: 5511
[21:22:50.546244] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.1153 (1.1315)  acc1: 62.5000 (61.7550)  acc5: 95.3125 (96.1300)  time: 0.0279  data: 0.0001  max mem: 5511
[21:22:50.698230] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.0982 (1.1335)  acc1: 62.5000 (61.7300)  acc5: 96.8750 (96.1700)  time: 0.0271  data: 0.0001  max mem: 5511
[21:22:50.849014] Test: Total time: 0:00:05 (0.0332 s / it)
[21:22:50.849504] * Acc@1 61.730 Acc@5 96.170 loss 1.134
[21:22:50.849792] Accuracy of the network on the 10000 test images: 61.7%
[21:22:50.850022] Max accuracy: 61.73%
[21:22:51.041332] log_dir: ./output_dir
[21:22:51.916249] Epoch: [11]  [  0/781]  eta: 0:11:22  lr: 0.000248  training_loss: 1.6574 (1.6574)  mae_loss: 0.0369 (0.0369)  classification_loss: 1.5889 (1.5889)  loss_mask: 0.0316 (0.0316)  time: 0.8733  data: 0.6696  max mem: 5511
[21:22:55.852927] Epoch: [11]  [ 20/781]  eta: 0:02:54  lr: 0.000248  training_loss: 1.7829 (1.7930)  mae_loss: 0.0383 (0.0388)  classification_loss: 1.7102 (1.7166)  loss_mask: 0.0365 (0.0376)  time: 0.1967  data: 0.0002  max mem: 5511
[21:22:59.773574] Epoch: [11]  [ 40/781]  eta: 0:02:37  lr: 0.000248  training_loss: 1.8324 (1.8062)  mae_loss: 0.0387 (0.0389)  classification_loss: 1.7631 (1.7307)  loss_mask: 0.0355 (0.0367)  time: 0.1960  data: 0.0003  max mem: 5511
[21:23:03.716841] Epoch: [11]  [ 60/781]  eta: 0:02:29  lr: 0.000247  training_loss: 1.8132 (1.8063)  mae_loss: 0.0380 (0.0388)  classification_loss: 1.7337 (1.7309)  loss_mask: 0.0364 (0.0366)  time: 0.1970  data: 0.0002  max mem: 5511
[21:23:07.710058] Epoch: [11]  [ 80/781]  eta: 0:02:24  lr: 0.000247  training_loss: 1.7913 (1.8081)  mae_loss: 0.0383 (0.0390)  classification_loss: 1.7147 (1.7322)  loss_mask: 0.0388 (0.0370)  time: 0.1995  data: 0.0002  max mem: 5511
[21:23:11.642548] Epoch: [11]  [100/781]  eta: 0:02:18  lr: 0.000247  training_loss: 1.8401 (1.8096)  mae_loss: 0.0380 (0.0388)  classification_loss: 1.7728 (1.7347)  loss_mask: 0.0325 (0.0361)  time: 0.1966  data: 0.0002  max mem: 5511
[21:23:15.596466] Epoch: [11]  [120/781]  eta: 0:02:14  lr: 0.000247  training_loss: 1.7821 (1.8077)  mae_loss: 0.0363 (0.0385)  classification_loss: 1.7104 (1.7337)  loss_mask: 0.0324 (0.0355)  time: 0.1976  data: 0.0002  max mem: 5511
[21:23:19.530585] Epoch: [11]  [140/781]  eta: 0:02:09  lr: 0.000247  training_loss: 1.7623 (1.8028)  mae_loss: 0.0368 (0.0384)  classification_loss: 1.6991 (1.7292)  loss_mask: 0.0339 (0.0352)  time: 0.1966  data: 0.0002  max mem: 5511
[21:23:23.502402] Epoch: [11]  [160/781]  eta: 0:02:05  lr: 0.000247  training_loss: 1.7992 (1.8036)  mae_loss: 0.0369 (0.0384)  classification_loss: 1.7217 (1.7299)  loss_mask: 0.0338 (0.0353)  time: 0.1985  data: 0.0002  max mem: 5511

[21:23:27.509225] Epoch: [11]  [180/781]  eta: 0:02:01  lr: 0.000247  training_loss: 1.7962 (1.8038)  mae_loss: 0.0383 (0.0384)  classification_loss: 1.7284 (1.7296)  loss_mask: 0.0380 (0.0358)  time: 0.2003  data: 0.0002  max mem: 5511
[21:23:31.446169] Epoch: [11]  [200/781]  eta: 0:01:56  lr: 0.000247  training_loss: 1.7807 (1.8009)  mae_loss: 0.0383 (0.0384)  classification_loss: 1.7057 (1.7267)  loss_mask: 0.0350 (0.0358)  time: 0.1968  data: 0.0002  max mem: 5511
[21:23:35.375128] Epoch: [11]  [220/781]  eta: 0:01:52  lr: 0.000247  training_loss: 1.8114 (1.8005)  mae_loss: 0.0367 (0.0383)  classification_loss: 1.7348 (1.7264)  loss_mask: 0.0342 (0.0358)  time: 0.1963  data: 0.0002  max mem: 5511
[21:23:39.303082] Epoch: [11]  [240/781]  eta: 0:01:48  lr: 0.000247  training_loss: 1.7987 (1.8022)  mae_loss: 0.0376 (0.0383)  classification_loss: 1.7213 (1.7281)  loss_mask: 0.0357 (0.0359)  time: 0.1963  data: 0.0002  max mem: 5511
[21:23:43.222241] Epoch: [11]  [260/781]  eta: 0:01:44  lr: 0.000247  training_loss: 1.7019 (1.7967)  mae_loss: 0.0373 (0.0382)  classification_loss: 1.6369 (1.7228)  loss_mask: 0.0340 (0.0357)  time: 0.1958  data: 0.0003  max mem: 5511
[21:23:47.156897] Epoch: [11]  [280/781]  eta: 0:01:39  lr: 0.000247  training_loss: 1.7825 (1.7961)  mae_loss: 0.0376 (0.0381)  classification_loss: 1.7086 (1.7225)  loss_mask: 0.0325 (0.0355)  time: 0.1967  data: 0.0004  max mem: 5511
[21:23:51.112483] Epoch: [11]  [300/781]  eta: 0:01:35  lr: 0.000247  training_loss: 1.7747 (1.7965)  mae_loss: 0.0372 (0.0382)  classification_loss: 1.7018 (1.7230)  loss_mask: 0.0318 (0.0353)  time: 0.1977  data: 0.0004  max mem: 5511
[21:23:55.040846] Epoch: [11]  [320/781]  eta: 0:01:31  lr: 0.000247  training_loss: 1.7970 (1.7963)  mae_loss: 0.0360 (0.0381)  classification_loss: 1.7237 (1.7226)  loss_mask: 0.0360 (0.0355)  time: 0.1963  data: 0.0002  max mem: 5511
[21:23:58.980061] Epoch: [11]  [340/781]  eta: 0:01:27  lr: 0.000247  training_loss: 1.7852 (1.7957)  mae_loss: 0.0367 (0.0381)  classification_loss: 1.6944 (1.7218)  loss_mask: 0.0347 (0.0358)  time: 0.1969  data: 0.0003  max mem: 5511
[21:24:02.907157] Epoch: [11]  [360/781]  eta: 0:01:23  lr: 0.000247  training_loss: 1.8196 (1.7971)  mae_loss: 0.0362 (0.0380)  classification_loss: 1.7526 (1.7235)  loss_mask: 0.0325 (0.0356)  time: 0.1963  data: 0.0002  max mem: 5511
[21:24:06.848093] Epoch: [11]  [380/781]  eta: 0:01:19  lr: 0.000247  training_loss: 1.8160 (1.7978)  mae_loss: 0.0396 (0.0380)  classification_loss: 1.7349 (1.7244)  loss_mask: 0.0284 (0.0354)  time: 0.1970  data: 0.0002  max mem: 5511
[21:24:10.826620] Epoch: [11]  [400/781]  eta: 0:01:15  lr: 0.000247  training_loss: 1.7748 (1.7971)  mae_loss: 0.0367 (0.0380)  classification_loss: 1.7067 (1.7240)  loss_mask: 0.0316 (0.0352)  time: 0.1988  data: 0.0002  max mem: 5511
[21:24:14.767019] Epoch: [11]  [420/781]  eta: 0:01:11  lr: 0.000247  training_loss: 1.7894 (1.7964)  mae_loss: 0.0382 (0.0380)  classification_loss: 1.7242 (1.7235)  loss_mask: 0.0267 (0.0348)  time: 0.1969  data: 0.0002  max mem: 5511
[21:24:18.715243] Epoch: [11]  [440/781]  eta: 0:01:07  lr: 0.000247  training_loss: 1.7983 (1.7966)  mae_loss: 0.0372 (0.0380)  classification_loss: 1.7380 (1.7241)  loss_mask: 0.0273 (0.0345)  time: 0.1973  data: 0.0002  max mem: 5511
[21:24:22.716395] Epoch: [11]  [460/781]  eta: 0:01:03  lr: 0.000247  training_loss: 1.7488 (1.7954)  mae_loss: 0.0366 (0.0380)  classification_loss: 1.6764 (1.7229)  loss_mask: 0.0346 (0.0345)  time: 0.2000  data: 0.0002  max mem: 5511
[21:24:26.661020] Epoch: [11]  [480/781]  eta: 0:00:59  lr: 0.000247  training_loss: 1.7975 (1.7973)  mae_loss: 0.0364 (0.0379)  classification_loss: 1.7197 (1.7246)  loss_mask: 0.0349 (0.0348)  time: 0.1971  data: 0.0002  max mem: 5511
[21:24:30.583827] Epoch: [11]  [500/781]  eta: 0:00:55  lr: 0.000247  training_loss: 1.8043 (1.7983)  mae_loss: 0.0375 (0.0379)  classification_loss: 1.7278 (1.7254)  loss_mask: 0.0369 (0.0349)  time: 0.1960  data: 0.0003  max mem: 5511
[21:24:34.522190] Epoch: [11]  [520/781]  eta: 0:00:51  lr: 0.000247  training_loss: 1.7922 (1.7987)  mae_loss: 0.0381 (0.0379)  classification_loss: 1.7372 (1.7259)  loss_mask: 0.0326 (0.0348)  time: 0.1968  data: 0.0002  max mem: 5511
[21:24:38.470755] Epoch: [11]  [540/781]  eta: 0:00:47  lr: 0.000247  training_loss: 1.7653 (1.7983)  mae_loss: 0.0380 (0.0379)  classification_loss: 1.7091 (1.7259)  loss_mask: 0.0273 (0.0345)  time: 0.1973  data: 0.0003  max mem: 5511
[21:24:42.426557] Epoch: [11]  [560/781]  eta: 0:00:43  lr: 0.000247  training_loss: 1.7544 (1.7978)  mae_loss: 0.0366 (0.0379)  classification_loss: 1.6879 (1.7255)  loss_mask: 0.0289 (0.0343)  time: 0.1977  data: 0.0003  max mem: 5511
[21:24:46.373181] Epoch: [11]  [580/781]  eta: 0:00:39  lr: 0.000247  training_loss: 1.7478 (1.7960)  mae_loss: 0.0368 (0.0379)  classification_loss: 1.6889 (1.7239)  loss_mask: 0.0285 (0.0342)  time: 0.1972  data: 0.0002  max mem: 5511
[21:24:50.318338] Epoch: [11]  [600/781]  eta: 0:00:35  lr: 0.000247  training_loss: 1.7722 (1.7953)  mae_loss: 0.0369 (0.0379)  classification_loss: 1.7108 (1.7234)  loss_mask: 0.0292 (0.0341)  time: 0.1972  data: 0.0002  max mem: 5511
[21:24:54.274582] Epoch: [11]  [620/781]  eta: 0:00:31  lr: 0.000247  training_loss: 1.7907 (1.7947)  mae_loss: 0.0369 (0.0379)  classification_loss: 1.7229 (1.7228)  loss_mask: 0.0318 (0.0340)  time: 0.1977  data: 0.0003  max mem: 5511
[21:24:58.203950] Epoch: [11]  [640/781]  eta: 0:00:27  lr: 0.000247  training_loss: 1.7312 (1.7938)  mae_loss: 0.0353 (0.0378)  classification_loss: 1.6733 (1.7223)  loss_mask: 0.0239 (0.0337)  time: 0.1964  data: 0.0002  max mem: 5511
[21:25:02.120472] Epoch: [11]  [660/781]  eta: 0:00:23  lr: 0.000247  training_loss: 1.8049 (1.7943)  mae_loss: 0.0370 (0.0378)  classification_loss: 1.7391 (1.7229)  loss_mask: 0.0281 (0.0336)  time: 0.1958  data: 0.0002  max mem: 5511
[21:25:06.115435] Epoch: [11]  [680/781]  eta: 0:00:20  lr: 0.000247  training_loss: 1.7631 (1.7939)  mae_loss: 0.0379 (0.0378)  classification_loss: 1.7033 (1.7226)  loss_mask: 0.0282 (0.0335)  time: 0.1997  data: 0.0002  max mem: 5511
[21:25:10.072388] Epoch: [11]  [700/781]  eta: 0:00:16  lr: 0.000247  training_loss: 1.8001 (1.7941)  mae_loss: 0.0358 (0.0378)  classification_loss: 1.7239 (1.7229)  loss_mask: 0.0298 (0.0335)  time: 0.1978  data: 0.0002  max mem: 5511
[21:25:14.012837] Epoch: [11]  [720/781]  eta: 0:00:12  lr: 0.000247  training_loss: 1.7813 (1.7940)  mae_loss: 0.0365 (0.0377)  classification_loss: 1.7137 (1.7230)  loss_mask: 0.0262 (0.0333)  time: 0.1969  data: 0.0002  max mem: 5511
[21:25:17.974657] Epoch: [11]  [740/781]  eta: 0:00:08  lr: 0.000247  training_loss: 1.7942 (1.7933)  mae_loss: 0.0367 (0.0377)  classification_loss: 1.7243 (1.7225)  loss_mask: 0.0257 (0.0331)  time: 0.1980  data: 0.0002  max mem: 5511
[21:25:21.911594] Epoch: [11]  [760/781]  eta: 0:00:04  lr: 0.000247  training_loss: 1.8087 (1.7934)  mae_loss: 0.0388 (0.0378)  classification_loss: 1.7376 (1.7226)  loss_mask: 0.0283 (0.0330)  time: 0.1968  data: 0.0003  max mem: 5511
[21:25:25.821896] Epoch: [11]  [780/781]  eta: 0:00:00  lr: 0.000247  training_loss: 1.8317 (1.7935)  mae_loss: 0.0351 (0.0377)  classification_loss: 1.7591 (1.7229)  loss_mask: 0.0282 (0.0329)  time: 0.1954  data: 0.0002  max mem: 5511
[21:25:25.974925] Epoch: [11] Total time: 0:02:34 (0.1984 s / it)
[21:25:25.975465] Averaged stats: lr: 0.000247  training_loss: 1.8317 (1.7935)  mae_loss: 0.0351 (0.0377)  classification_loss: 1.7591 (1.7229)  loss_mask: 0.0282 (0.0329)
[21:25:26.602325] Test:  [  0/157]  eta: 0:01:37  testing_loss: 1.1006 (1.1006)  acc1: 68.7500 (68.7500)  acc5: 92.1875 (92.1875)  time: 0.6214  data: 0.5831  max mem: 5511
[21:25:26.891363] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.1142 (1.1382)  acc1: 60.9375 (62.0739)  acc5: 95.3125 (96.0227)  time: 0.0826  data: 0.0533  max mem: 5511
[21:25:27.180317] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.0929 (1.1015)  acc1: 60.9375 (62.7976)  acc5: 98.4375 (97.0238)  time: 0.0287  data: 0.0003  max mem: 5511
[21:25:27.465010] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.0992 (1.0971)  acc1: 64.0625 (63.8105)  acc5: 96.8750 (96.6230)  time: 0.0285  data: 0.0003  max mem: 5511
[21:25:27.755439] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.0992 (1.0972)  acc1: 65.6250 (63.9482)  acc5: 96.8750 (96.6463)  time: 0.0286  data: 0.0002  max mem: 5511
[21:25:28.041765] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.0852 (1.0931)  acc1: 65.6250 (64.1850)  acc5: 96.8750 (96.6912)  time: 0.0287  data: 0.0002  max mem: 5511
[21:25:28.330602] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.0857 (1.0910)  acc1: 62.5000 (63.9600)  acc5: 96.8750 (96.6445)  time: 0.0286  data: 0.0002  max mem: 5511
[21:25:28.617315] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.0504 (1.0818)  acc1: 64.0625 (64.5026)  acc5: 96.8750 (96.7430)  time: 0.0286  data: 0.0002  max mem: 5511
[21:25:28.906917] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.0698 (1.0872)  acc1: 67.1875 (64.6026)  acc5: 96.8750 (96.6628)  time: 0.0287  data: 0.0002  max mem: 5511
[21:25:29.192038] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.1267 (1.0928)  acc1: 62.5000 (64.1484)  acc5: 96.8750 (96.6690)  time: 0.0286  data: 0.0002  max mem: 5511
[21:25:29.475329] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.1509 (1.0976)  acc1: 57.8125 (63.8459)  acc5: 96.8750 (96.6120)  time: 0.0283  data: 0.0002  max mem: 5511
[21:25:29.758689] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.1387 (1.0986)  acc1: 62.5000 (63.8373)  acc5: 98.4375 (96.7202)  time: 0.0282  data: 0.0002  max mem: 5511
[21:25:30.042972] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.0699 (1.0951)  acc1: 64.0625 (63.9979)  acc5: 96.8750 (96.7200)  time: 0.0283  data: 0.0002  max mem: 5511
[21:25:30.327535] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.0710 (1.0989)  acc1: 64.0625 (63.7166)  acc5: 96.8750 (96.6603)  time: 0.0283  data: 0.0002  max mem: 5511
[21:25:30.609394] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.1060 (1.0995)  acc1: 62.5000 (63.7522)  acc5: 95.3125 (96.5980)  time: 0.0282  data: 0.0001  max mem: 5511
[21:25:30.891354] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.0801 (1.0959)  acc1: 65.6250 (63.9487)  acc5: 95.3125 (96.5749)  time: 0.0281  data: 0.0001  max mem: 5511
[21:25:31.045307] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.0768 (1.0980)  acc1: 62.5000 (63.7500)  acc5: 96.8750 (96.5700)  time: 0.0272  data: 0.0001  max mem: 5511
[21:25:31.199119] Test: Total time: 0:00:05 (0.0332 s / it)
[21:25:31.199822] * Acc@1 63.750 Acc@5 96.570 loss 1.098
[21:25:31.200110] Accuracy of the network on the 10000 test images: 63.8%
[21:25:31.200293] Max accuracy: 63.75%
[21:25:31.415171] log_dir: ./output_dir
[21:25:32.282206] Epoch: [12]  [  0/781]  eta: 0:11:15  lr: 0.000247  training_loss: 1.6531 (1.6531)  mae_loss: 0.0392 (0.0392)  classification_loss: 1.6008 (1.6008)  loss_mask: 0.0131 (0.0131)  time: 0.8652  data: 0.6420  max mem: 5511
[21:25:36.210265] Epoch: [12]  [ 20/781]  eta: 0:02:53  lr: 0.000247  training_loss: 1.7666 (1.7607)  mae_loss: 0.0360 (0.0364)  classification_loss: 1.7039 (1.6997)  loss_mask: 0.0245 (0.0245)  time: 0.1963  data: 0.0002  max mem: 5511
[21:25:40.149692] Epoch: [12]  [ 40/781]  eta: 0:02:37  lr: 0.000247  training_loss: 1.7744 (1.7762)  mae_loss: 0.0358 (0.0362)  classification_loss: 1.7126 (1.7147)  loss_mask: 0.0252 (0.0253)  time: 0.1969  data: 0.0002  max mem: 5511
[21:25:44.105690] Epoch: [12]  [ 60/781]  eta: 0:02:29  lr: 0.000247  training_loss: 1.7763 (1.7857)  mae_loss: 0.0375 (0.0367)  classification_loss: 1.7188 (1.7237)  loss_mask: 0.0258 (0.0253)  time: 0.1977  data: 0.0002  max mem: 5511
[21:25:48.053189] Epoch: [12]  [ 80/781]  eta: 0:02:23  lr: 0.000247  training_loss: 1.7544 (1.7826)  mae_loss: 0.0365 (0.0366)  classification_loss: 1.7055 (1.7208)  loss_mask: 0.0251 (0.0253)  time: 0.1973  data: 0.0002  max mem: 5511
[21:25:52.040626] Epoch: [12]  [100/781]  eta: 0:02:18  lr: 0.000247  training_loss: 1.7758 (1.7814)  mae_loss: 0.0346 (0.0364)  classification_loss: 1.7185 (1.7201)  loss_mask: 0.0209 (0.0250)  time: 0.1993  data: 0.0002  max mem: 5511
[21:25:55.970925] Epoch: [12]  [120/781]  eta: 0:02:14  lr: 0.000247  training_loss: 1.7142 (1.7722)  mae_loss: 0.0347 (0.0362)  classification_loss: 1.6519 (1.7111)  loss_mask: 0.0246 (0.0250)  time: 0.1964  data: 0.0002  max mem: 5511
[21:25:59.939379] Epoch: [12]  [140/781]  eta: 0:02:09  lr: 0.000247  training_loss: 1.7651 (1.7731)  mae_loss: 0.0353 (0.0362)  classification_loss: 1.6918 (1.7112)  loss_mask: 0.0283 (0.0257)  time: 0.1983  data: 0.0003  max mem: 5511
[21:26:03.881008] Epoch: [12]  [160/781]  eta: 0:02:05  lr: 0.000246  training_loss: 1.7699 (1.7720)  mae_loss: 0.0358 (0.0362)  classification_loss: 1.6934 (1.7096)  loss_mask: 0.0287 (0.0261)  time: 0.1970  data: 0.0002  max mem: 5511
[21:26:07.851661] Epoch: [12]  [180/781]  eta: 0:02:00  lr: 0.000246  training_loss: 1.7681 (1.7711)  mae_loss: 0.0383 (0.0365)  classification_loss: 1.6966 (1.7081)  loss_mask: 0.0299 (0.0265)  time: 0.1984  data: 0.0002  max mem: 5511
[21:26:11.774705] Epoch: [12]  [200/781]  eta: 0:01:56  lr: 0.000246  training_loss: 1.7920 (1.7733)  mae_loss: 0.0356 (0.0364)  classification_loss: 1.7244 (1.7103)  loss_mask: 0.0281 (0.0266)  time: 0.1961  data: 0.0002  max mem: 5511
[21:26:15.709726] Epoch: [12]  [220/781]  eta: 0:01:52  lr: 0.000246  training_loss: 1.7539 (1.7719)  mae_loss: 0.0350 (0.0364)  classification_loss: 1.6946 (1.7087)  loss_mask: 0.0268 (0.0269)  time: 0.1967  data: 0.0003  max mem: 5511
[21:26:19.649421] Epoch: [12]  [240/781]  eta: 0:01:48  lr: 0.000246  training_loss: 1.7448 (1.7702)  mae_loss: 0.0358 (0.0363)  classification_loss: 1.6897 (1.7072)  loss_mask: 0.0234 (0.0267)  time: 0.1969  data: 0.0002  max mem: 5511
[21:26:23.570034] Epoch: [12]  [260/781]  eta: 0:01:44  lr: 0.000246  training_loss: 1.7775 (1.7698)  mae_loss: 0.0345 (0.0362)  classification_loss: 1.7004 (1.7063)  loss_mask: 0.0355 (0.0273)  time: 0.1959  data: 0.0002  max mem: 5511
[21:26:27.525030] Epoch: [12]  [280/781]  eta: 0:01:39  lr: 0.000246  training_loss: 1.7635 (1.7688)  mae_loss: 0.0369 (0.0364)  classification_loss: 1.6899 (1.7047)  loss_mask: 0.0315 (0.0277)  time: 0.1977  data: 0.0002  max mem: 5511
[21:26:31.459323] Epoch: [12]  [300/781]  eta: 0:01:35  lr: 0.000246  training_loss: 1.8056 (1.7708)  mae_loss: 0.0347 (0.0364)  classification_loss: 1.7384 (1.7069)  loss_mask: 0.0227 (0.0274)  time: 0.1966  data: 0.0002  max mem: 5511
[21:26:35.460400] Epoch: [12]  [320/781]  eta: 0:01:31  lr: 0.000246  training_loss: 1.7561 (1.7697)  mae_loss: 0.0339 (0.0364)  classification_loss: 1.6924 (1.7062)  loss_mask: 0.0210 (0.0271)  time: 0.2000  data: 0.0003  max mem: 5511
[21:26:39.392744] Epoch: [12]  [340/781]  eta: 0:01:27  lr: 0.000246  training_loss: 1.7196 (1.7679)  mae_loss: 0.0352 (0.0364)  classification_loss: 1.6685 (1.7043)  loss_mask: 0.0221 (0.0272)  time: 0.1965  data: 0.0002  max mem: 5511
[21:26:43.324547] Epoch: [12]  [360/781]  eta: 0:01:23  lr: 0.000246  training_loss: 1.7631 (1.7681)  mae_loss: 0.0358 (0.0364)  classification_loss: 1.6912 (1.7046)  loss_mask: 0.0250 (0.0271)  time: 0.1965  data: 0.0002  max mem: 5511
[21:26:47.256264] Epoch: [12]  [380/781]  eta: 0:01:19  lr: 0.000246  training_loss: 1.8092 (1.7706)  mae_loss: 0.0381 (0.0365)  classification_loss: 1.7318 (1.7068)  loss_mask: 0.0309 (0.0274)  time: 0.1965  data: 0.0002  max mem: 5511
[21:26:51.194153] Epoch: [12]  [400/781]  eta: 0:01:15  lr: 0.000246  training_loss: 1.7322 (1.7693)  mae_loss: 0.0374 (0.0366)  classification_loss: 1.6661 (1.7055)  loss_mask: 0.0254 (0.0273)  time: 0.1968  data: 0.0002  max mem: 5511
[21:26:55.127907] Epoch: [12]  [420/781]  eta: 0:01:11  lr: 0.000246  training_loss: 1.7593 (1.7693)  mae_loss: 0.0360 (0.0366)  classification_loss: 1.7003 (1.7055)  loss_mask: 0.0229 (0.0272)  time: 0.1966  data: 0.0002  max mem: 5511
[21:26:59.099234] Epoch: [12]  [440/781]  eta: 0:01:07  lr: 0.000246  training_loss: 1.7739 (1.7696)  mae_loss: 0.0354 (0.0366)  classification_loss: 1.7068 (1.7060)  loss_mask: 0.0233 (0.0271)  time: 0.1985  data: 0.0002  max mem: 5511
[21:27:03.027343] Epoch: [12]  [460/781]  eta: 0:01:03  lr: 0.000246  training_loss: 1.7379 (1.7674)  mae_loss: 0.0345 (0.0365)  classification_loss: 1.6838 (1.7039)  loss_mask: 0.0224 (0.0270)  time: 0.1963  data: 0.0002  max mem: 5511
[21:27:06.945235] Epoch: [12]  [480/781]  eta: 0:00:59  lr: 0.000246  training_loss: 1.7218 (1.7669)  mae_loss: 0.0352 (0.0364)  classification_loss: 1.6470 (1.7037)  loss_mask: 0.0197 (0.0268)  time: 0.1958  data: 0.0002  max mem: 5511
[21:27:10.925551] Epoch: [12]  [500/781]  eta: 0:00:55  lr: 0.000246  training_loss: 1.7188 (1.7650)  mae_loss: 0.0347 (0.0364)  classification_loss: 1.6622 (1.7019)  loss_mask: 0.0219 (0.0266)  time: 0.1989  data: 0.0002  max mem: 5511
[21:27:14.859326] Epoch: [12]  [520/781]  eta: 0:00:51  lr: 0.000246  training_loss: 1.7279 (1.7641)  mae_loss: 0.0349 (0.0363)  classification_loss: 1.6629 (1.7013)  loss_mask: 0.0205 (0.0264)  time: 0.1966  data: 0.0002  max mem: 5511
[21:27:18.812728] Epoch: [12]  [540/781]  eta: 0:00:47  lr: 0.000246  training_loss: 1.7118 (1.7633)  mae_loss: 0.0347 (0.0364)  classification_loss: 1.6438 (1.7004)  loss_mask: 0.0261 (0.0265)  time: 0.1976  data: 0.0002  max mem: 5511
[21:27:22.749001] Epoch: [12]  [560/781]  eta: 0:00:43  lr: 0.000246  training_loss: 1.7527 (1.7636)  mae_loss: 0.0374 (0.0364)  classification_loss: 1.6946 (1.7006)  loss_mask: 0.0251 (0.0266)  time: 0.1967  data: 0.0002  max mem: 5511
[21:27:26.676061] Epoch: [12]  [580/781]  eta: 0:00:39  lr: 0.000246  training_loss: 1.7311 (1.7627)  mae_loss: 0.0359 (0.0364)  classification_loss: 1.6630 (1.6999)  loss_mask: 0.0206 (0.0265)  time: 0.1963  data: 0.0003  max mem: 5511
[21:27:30.593973] Epoch: [12]  [600/781]  eta: 0:00:35  lr: 0.000246  training_loss: 1.7050 (1.7626)  mae_loss: 0.0358 (0.0363)  classification_loss: 1.6465 (1.6998)  loss_mask: 0.0243 (0.0264)  time: 0.1958  data: 0.0002  max mem: 5511
[21:27:34.534081] Epoch: [12]  [620/781]  eta: 0:00:31  lr: 0.000246  training_loss: 1.7154 (1.7607)  mae_loss: 0.0355 (0.0364)  classification_loss: 1.6610 (1.6981)  loss_mask: 0.0203 (0.0262)  time: 0.1969  data: 0.0002  max mem: 5511
[21:27:38.484105] Epoch: [12]  [640/781]  eta: 0:00:27  lr: 0.000246  training_loss: 1.7462 (1.7602)  mae_loss: 0.0344 (0.0363)  classification_loss: 1.6898 (1.6975)  loss_mask: 0.0259 (0.0263)  time: 0.1974  data: 0.0004  max mem: 5511
[21:27:42.417386] Epoch: [12]  [660/781]  eta: 0:00:23  lr: 0.000246  training_loss: 1.7478 (1.7595)  mae_loss: 0.0350 (0.0363)  classification_loss: 1.6867 (1.6971)  loss_mask: 0.0208 (0.0261)  time: 0.1966  data: 0.0002  max mem: 5511
[21:27:46.340493] Epoch: [12]  [680/781]  eta: 0:00:20  lr: 0.000246  training_loss: 1.7567 (1.7600)  mae_loss: 0.0353 (0.0363)  classification_loss: 1.7000 (1.6976)  loss_mask: 0.0224 (0.0261)  time: 0.1960  data: 0.0002  max mem: 5511
[21:27:50.327890] Epoch: [12]  [700/781]  eta: 0:00:16  lr: 0.000246  training_loss: 1.7667 (1.7598)  mae_loss: 0.0351 (0.0363)  classification_loss: 1.6953 (1.6974)  loss_mask: 0.0211 (0.0261)  time: 0.1993  data: 0.0002  max mem: 5511
[21:27:54.270792] Epoch: [12]  [720/781]  eta: 0:00:12  lr: 0.000246  training_loss: 1.7629 (1.7607)  mae_loss: 0.0365 (0.0363)  classification_loss: 1.7099 (1.6985)  loss_mask: 0.0168 (0.0259)  time: 0.1971  data: 0.0002  max mem: 5511
[21:27:58.215031] Epoch: [12]  [740/781]  eta: 0:00:08  lr: 0.000246  training_loss: 1.6812 (1.7592)  mae_loss: 0.0352 (0.0363)  classification_loss: 1.6224 (1.6972)  loss_mask: 0.0206 (0.0258)  time: 0.1971  data: 0.0003  max mem: 5511
[21:28:02.130745] Epoch: [12]  [760/781]  eta: 0:00:04  lr: 0.000246  training_loss: 1.7678 (1.7593)  mae_loss: 0.0359 (0.0363)  classification_loss: 1.7015 (1.6973)  loss_mask: 0.0222 (0.0257)  time: 0.1957  data: 0.0002  max mem: 5511
[21:28:06.133690] Epoch: [12]  [780/781]  eta: 0:00:00  lr: 0.000246  training_loss: 1.7651 (1.7594)  mae_loss: 0.0348 (0.0362)  classification_loss: 1.7154 (1.6974)  loss_mask: 0.0227 (0.0257)  time: 0.2000  data: 0.0002  max mem: 5511
[21:28:06.289567] Epoch: [12] Total time: 0:02:34 (0.1983 s / it)
[21:28:06.290071] Averaged stats: lr: 0.000246  training_loss: 1.7651 (1.7594)  mae_loss: 0.0348 (0.0362)  classification_loss: 1.7154 (1.6974)  loss_mask: 0.0227 (0.0257)
[21:28:06.932400] Test:  [  0/157]  eta: 0:01:40  testing_loss: 1.0527 (1.0527)  acc1: 64.0625 (64.0625)  acc5: 90.6250 (90.6250)  time: 0.6377  data: 0.6083  max mem: 5511
[21:28:07.217370] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.1026 (1.1335)  acc1: 62.5000 (61.7898)  acc5: 95.3125 (95.4545)  time: 0.0837  data: 0.0555  max mem: 5511
[21:28:07.507829] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.0698 (1.0896)  acc1: 62.5000 (63.6905)  acc5: 96.8750 (96.8006)  time: 0.0286  data: 0.0002  max mem: 5511
[21:28:07.794289] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.0828 (1.1018)  acc1: 62.5000 (63.6593)  acc5: 98.4375 (96.3710)  time: 0.0287  data: 0.0002  max mem: 5511
[21:28:08.083600] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.0801 (1.0989)  acc1: 64.0625 (63.5290)  acc5: 95.3125 (96.3415)  time: 0.0286  data: 0.0002  max mem: 5511
[21:28:08.379149] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.0797 (1.0960)  acc1: 64.0625 (63.9706)  acc5: 96.8750 (96.4154)  time: 0.0291  data: 0.0002  max mem: 5511
[21:28:08.673387] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.0854 (1.0932)  acc1: 62.5000 (63.8064)  acc5: 96.8750 (96.4395)  time: 0.0294  data: 0.0004  max mem: 5511
[21:28:08.958885] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.0180 (1.0832)  acc1: 65.6250 (64.1505)  acc5: 96.8750 (96.5229)  time: 0.0288  data: 0.0004  max mem: 5511
[21:28:09.245627] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.0249 (1.0824)  acc1: 65.6250 (64.2554)  acc5: 96.8750 (96.6049)  time: 0.0284  data: 0.0002  max mem: 5511
[21:28:09.534014] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.1111 (1.0883)  acc1: 64.0625 (64.0797)  acc5: 96.8750 (96.6518)  time: 0.0286  data: 0.0002  max mem: 5511
[21:28:09.819812] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.1561 (1.0950)  acc1: 60.9375 (63.9078)  acc5: 96.8750 (96.6275)  time: 0.0285  data: 0.0002  max mem: 5511
[21:28:10.106029] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.1451 (1.0961)  acc1: 60.9375 (63.7528)  acc5: 96.8750 (96.6216)  time: 0.0284  data: 0.0002  max mem: 5511
[21:28:10.393957] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.0744 (1.0898)  acc1: 65.6250 (64.0625)  acc5: 96.8750 (96.6813)  time: 0.0285  data: 0.0002  max mem: 5511
[21:28:10.678791] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.0733 (1.0930)  acc1: 62.5000 (63.6689)  acc5: 96.8750 (96.6484)  time: 0.0285  data: 0.0002  max mem: 5511
[21:28:10.966025] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.1028 (1.0931)  acc1: 59.3750 (63.6414)  acc5: 95.3125 (96.6090)  time: 0.0284  data: 0.0002  max mem: 5511
[21:28:11.252102] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.0828 (1.0900)  acc1: 64.0625 (63.7210)  acc5: 96.8750 (96.6370)  time: 0.0285  data: 0.0002  max mem: 5511
[21:28:11.403693] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.0643 (1.0920)  acc1: 62.5000 (63.5600)  acc5: 96.8750 (96.6600)  time: 0.0273  data: 0.0002  max mem: 5511
[21:28:11.572680] Test: Total time: 0:00:05 (0.0336 s / it)
[21:28:11.573170] * Acc@1 63.560 Acc@5 96.660 loss 1.092
[21:28:11.573478] Accuracy of the network on the 10000 test images: 63.6%
[21:28:11.573690] Max accuracy: 63.75%
[21:28:11.917673] log_dir: ./output_dir
[21:28:12.845029] Epoch: [13]  [  0/781]  eta: 0:12:02  lr: 0.000246  training_loss: 1.6743 (1.6743)  mae_loss: 0.0401 (0.0401)  classification_loss: 1.6158 (1.6158)  loss_mask: 0.0185 (0.0185)  time: 0.9255  data: 0.6859  max mem: 5511
[21:28:16.828416] Epoch: [13]  [ 20/781]  eta: 0:02:57  lr: 0.000246  training_loss: 1.7324 (1.7141)  mae_loss: 0.0343 (0.0353)  classification_loss: 1.6614 (1.6517)  loss_mask: 0.0289 (0.0271)  time: 0.1990  data: 0.0002  max mem: 5511
[21:28:20.750267] Epoch: [13]  [ 40/781]  eta: 0:02:39  lr: 0.000246  training_loss: 1.7682 (1.7348)  mae_loss: 0.0366 (0.0361)  classification_loss: 1.7030 (1.6745)  loss_mask: 0.0193 (0.0242)  time: 0.1960  data: 0.0003  max mem: 5511
[21:28:24.691308] Epoch: [13]  [ 60/781]  eta: 0:02:30  lr: 0.000246  training_loss: 1.8021 (1.7531)  mae_loss: 0.0356 (0.0364)  classification_loss: 1.7372 (1.6932)  loss_mask: 0.0208 (0.0235)  time: 0.1969  data: 0.0003  max mem: 5511
[21:28:28.635698] Epoch: [13]  [ 80/781]  eta: 0:02:24  lr: 0.000246  training_loss: 1.7868 (1.7568)  mae_loss: 0.0349 (0.0363)  classification_loss: 1.7287 (1.6979)  loss_mask: 0.0199 (0.0227)  time: 0.1971  data: 0.0002  max mem: 5511
[21:28:32.598326] Epoch: [13]  [100/781]  eta: 0:02:19  lr: 0.000246  training_loss: 1.7415 (1.7571)  mae_loss: 0.0352 (0.0361)  classification_loss: 1.6940 (1.6984)  loss_mask: 0.0212 (0.0226)  time: 0.1980  data: 0.0003  max mem: 5511
[21:28:36.534337] Epoch: [13]  [120/781]  eta: 0:02:14  lr: 0.000246  training_loss: 1.7204 (1.7544)  mae_loss: 0.0357 (0.0360)  classification_loss: 1.6675 (1.6961)  loss_mask: 0.0183 (0.0223)  time: 0.1967  data: 0.0003  max mem: 5511
[21:28:40.462456] Epoch: [13]  [140/781]  eta: 0:02:09  lr: 0.000245  training_loss: 1.7091 (1.7506)  mae_loss: 0.0361 (0.0359)  classification_loss: 1.6465 (1.6921)  loss_mask: 0.0229 (0.0226)  time: 0.1963  data: 0.0002  max mem: 5511
[21:28:44.426318] Epoch: [13]  [160/781]  eta: 0:02:05  lr: 0.000245  training_loss: 1.7306 (1.7494)  mae_loss: 0.0322 (0.0356)  classification_loss: 1.6701 (1.6915)  loss_mask: 0.0182 (0.0223)  time: 0.1981  data: 0.0002  max mem: 5511
[21:28:48.359311] Epoch: [13]  [180/781]  eta: 0:02:00  lr: 0.000245  training_loss: 1.6720 (1.7447)  mae_loss: 0.0359 (0.0357)  classification_loss: 1.6164 (1.6863)  loss_mask: 0.0243 (0.0227)  time: 0.1965  data: 0.0002  max mem: 5511
[21:28:52.304359] Epoch: [13]  [200/781]  eta: 0:01:56  lr: 0.000245  training_loss: 1.7372 (1.7445)  mae_loss: 0.0363 (0.0357)  classification_loss: 1.6778 (1.6863)  loss_mask: 0.0191 (0.0225)  time: 0.1971  data: 0.0002  max mem: 5511
[21:28:56.207407] Epoch: [13]  [220/781]  eta: 0:01:52  lr: 0.000245  training_loss: 1.7386 (1.7451)  mae_loss: 0.0338 (0.0356)  classification_loss: 1.6881 (1.6870)  loss_mask: 0.0212 (0.0225)  time: 0.1951  data: 0.0002  max mem: 5511
[21:29:00.164147] Epoch: [13]  [240/781]  eta: 0:01:48  lr: 0.000245  training_loss: 1.7757 (1.7484)  mae_loss: 0.0349 (0.0356)  classification_loss: 1.7136 (1.6902)  loss_mask: 0.0230 (0.0226)  time: 0.1977  data: 0.0003  max mem: 5511
[21:29:04.106580] Epoch: [13]  [260/781]  eta: 0:01:44  lr: 0.000245  training_loss: 1.7101 (1.7473)  mae_loss: 0.0336 (0.0355)  classification_loss: 1.6440 (1.6892)  loss_mask: 0.0208 (0.0226)  time: 0.1970  data: 0.0003  max mem: 5511
[21:29:08.032269] Epoch: [13]  [280/781]  eta: 0:01:39  lr: 0.000245  training_loss: 1.7369 (1.7452)  mae_loss: 0.0359 (0.0356)  classification_loss: 1.6645 (1.6871)  loss_mask: 0.0195 (0.0225)  time: 0.1962  data: 0.0002  max mem: 5511

[21:29:11.966712] Epoch: [13]  [300/781]  eta: 0:01:35  lr: 0.000245  training_loss: 1.7066 (1.7429)  mae_loss: 0.0358 (0.0356)  classification_loss: 1.6538 (1.6851)  loss_mask: 0.0161 (0.0222)  time: 0.1966  data: 0.0002  max mem: 5511
[21:29:15.915063] Epoch: [13]  [320/781]  eta: 0:01:31  lr: 0.000245  training_loss: 1.7789 (1.7446)  mae_loss: 0.0344 (0.0356)  classification_loss: 1.7199 (1.6865)  loss_mask: 0.0234 (0.0225)  time: 0.1973  data: 0.0002  max mem: 5511
[21:29:19.840216] Epoch: [13]  [340/781]  eta: 0:01:27  lr: 0.000245  training_loss: 1.7131 (1.7441)  mae_loss: 0.0333 (0.0355)  classification_loss: 1.6613 (1.6861)  loss_mask: 0.0205 (0.0225)  time: 0.1962  data: 0.0002  max mem: 5511
[21:29:23.779772] Epoch: [13]  [360/781]  eta: 0:01:23  lr: 0.000245  training_loss: 1.7237 (1.7433)  mae_loss: 0.0346 (0.0355)  classification_loss: 1.6685 (1.6853)  loss_mask: 0.0198 (0.0225)  time: 0.1969  data: 0.0003  max mem: 5511
[21:29:27.715049] Epoch: [13]  [380/781]  eta: 0:01:19  lr: 0.000245  training_loss: 1.7411 (1.7432)  mae_loss: 0.0349 (0.0355)  classification_loss: 1.6879 (1.6852)  loss_mask: 0.0212 (0.0225)  time: 0.1967  data: 0.0003  max mem: 5511
[21:29:31.695695] Epoch: [13]  [400/781]  eta: 0:01:15  lr: 0.000245  training_loss: 1.7665 (1.7438)  mae_loss: 0.0344 (0.0354)  classification_loss: 1.7146 (1.6858)  loss_mask: 0.0253 (0.0226)  time: 0.1989  data: 0.0003  max mem: 5511
[21:29:35.624186] Epoch: [13]  [420/781]  eta: 0:01:11  lr: 0.000245  training_loss: 1.7578 (1.7442)  mae_loss: 0.0350 (0.0354)  classification_loss: 1.7015 (1.6861)  loss_mask: 0.0216 (0.0226)  time: 0.1963  data: 0.0002  max mem: 5511
[21:29:39.569655] Epoch: [13]  [440/781]  eta: 0:01:07  lr: 0.000245  training_loss: 1.7111 (1.7428)  mae_loss: 0.0345 (0.0354)  classification_loss: 1.6481 (1.6849)  loss_mask: 0.0181 (0.0225)  time: 0.1972  data: 0.0003  max mem: 5511
[21:29:43.489203] Epoch: [13]  [460/781]  eta: 0:01:03  lr: 0.000245  training_loss: 1.7303 (1.7418)  mae_loss: 0.0321 (0.0354)  classification_loss: 1.6746 (1.6840)  loss_mask: 0.0208 (0.0225)  time: 0.1959  data: 0.0003  max mem: 5511
[21:29:47.458925] Epoch: [13]  [480/781]  eta: 0:00:59  lr: 0.000245  training_loss: 1.7709 (1.7430)  mae_loss: 0.0345 (0.0353)  classification_loss: 1.7091 (1.6852)  loss_mask: 0.0211 (0.0224)  time: 0.1984  data: 0.0002  max mem: 5511
[21:29:51.385240] Epoch: [13]  [500/781]  eta: 0:00:55  lr: 0.000245  training_loss: 1.7297 (1.7435)  mae_loss: 0.0331 (0.0353)  classification_loss: 1.6687 (1.6857)  loss_mask: 0.0248 (0.0225)  time: 0.1962  data: 0.0002  max mem: 5511
[21:29:55.331998] Epoch: [13]  [520/781]  eta: 0:00:51  lr: 0.000245  training_loss: 1.7078 (1.7426)  mae_loss: 0.0338 (0.0352)  classification_loss: 1.6493 (1.6850)  loss_mask: 0.0192 (0.0224)  time: 0.1972  data: 0.0002  max mem: 5511
[21:29:59.262101] Epoch: [13]  [540/781]  eta: 0:00:47  lr: 0.000245  training_loss: 1.7416 (1.7433)  mae_loss: 0.0371 (0.0353)  classification_loss: 1.6827 (1.6857)  loss_mask: 0.0193 (0.0223)  time: 0.1964  data: 0.0002  max mem: 5511
[21:30:03.194242] Epoch: [13]  [560/781]  eta: 0:00:43  lr: 0.000245  training_loss: 1.6684 (1.7423)  mae_loss: 0.0348 (0.0353)  classification_loss: 1.6090 (1.6849)  loss_mask: 0.0173 (0.0221)  time: 0.1965  data: 0.0002  max mem: 5511
[21:30:07.133261] Epoch: [13]  [580/781]  eta: 0:00:39  lr: 0.000245  training_loss: 1.7314 (1.7419)  mae_loss: 0.0338 (0.0353)  classification_loss: 1.6825 (1.6846)  loss_mask: 0.0176 (0.0221)  time: 0.1968  data: 0.0004  max mem: 5511
[21:30:11.078933] Epoch: [13]  [600/781]  eta: 0:00:35  lr: 0.000245  training_loss: 1.6629 (1.7403)  mae_loss: 0.0346 (0.0353)  classification_loss: 1.6051 (1.6831)  loss_mask: 0.0193 (0.0220)  time: 0.1972  data: 0.0002  max mem: 5511
[21:30:15.035650] Epoch: [13]  [620/781]  eta: 0:00:31  lr: 0.000245  training_loss: 1.6785 (1.7392)  mae_loss: 0.0337 (0.0352)  classification_loss: 1.6240 (1.6819)  loss_mask: 0.0196 (0.0221)  time: 0.1978  data: 0.0002  max mem: 5511
[21:30:18.964920] Epoch: [13]  [640/781]  eta: 0:00:27  lr: 0.000245  training_loss: 1.7128 (1.7379)  mae_loss: 0.0348 (0.0352)  classification_loss: 1.6498 (1.6806)  loss_mask: 0.0215 (0.0221)  time: 0.1963  data: 0.0002  max mem: 5511
[21:30:22.880413] Epoch: [13]  [660/781]  eta: 0:00:23  lr: 0.000245  training_loss: 1.6939 (1.7377)  mae_loss: 0.0331 (0.0352)  classification_loss: 1.6420 (1.6805)  loss_mask: 0.0213 (0.0220)  time: 0.1957  data: 0.0002  max mem: 5511
[21:30:26.799282] Epoch: [13]  [680/781]  eta: 0:00:19  lr: 0.000245  training_loss: 1.7290 (1.7380)  mae_loss: 0.0345 (0.0352)  classification_loss: 1.6874 (1.6810)  loss_mask: 0.0151 (0.0218)  time: 0.1959  data: 0.0002  max mem: 5511
[21:30:30.734276] Epoch: [13]  [700/781]  eta: 0:00:16  lr: 0.000245  training_loss: 1.7392 (1.7383)  mae_loss: 0.0362 (0.0352)  classification_loss: 1.6792 (1.6815)  loss_mask: 0.0151 (0.0216)  time: 0.1967  data: 0.0002  max mem: 5511
[21:30:34.709118] Epoch: [13]  [720/781]  eta: 0:00:12  lr: 0.000245  training_loss: 1.6782 (1.7371)  mae_loss: 0.0336 (0.0351)  classification_loss: 1.6268 (1.6805)  loss_mask: 0.0137 (0.0214)  time: 0.1987  data: 0.0003  max mem: 5511
[21:30:38.631095] Epoch: [13]  [740/781]  eta: 0:00:08  lr: 0.000245  training_loss: 1.6961 (1.7357)  mae_loss: 0.0336 (0.0351)  classification_loss: 1.6514 (1.6794)  loss_mask: 0.0158 (0.0213)  time: 0.1960  data: 0.0002  max mem: 5511
[21:30:42.561501] Epoch: [13]  [760/781]  eta: 0:00:04  lr: 0.000245  training_loss: 1.7101 (1.7361)  mae_loss: 0.0345 (0.0351)  classification_loss: 1.6687 (1.6797)  loss_mask: 0.0159 (0.0213)  time: 0.1964  data: 0.0002  max mem: 5511
[21:30:46.477482] Epoch: [13]  [780/781]  eta: 0:00:00  lr: 0.000245  training_loss: 1.7437 (1.7362)  mae_loss: 0.0338 (0.0351)  classification_loss: 1.6885 (1.6800)  loss_mask: 0.0164 (0.0212)  time: 0.1957  data: 0.0002  max mem: 5511
[21:30:46.628451] Epoch: [13] Total time: 0:02:34 (0.1981 s / it)
[21:30:46.628947] Averaged stats: lr: 0.000245  training_loss: 1.7437 (1.7362)  mae_loss: 0.0338 (0.0351)  classification_loss: 1.6885 (1.6800)  loss_mask: 0.0164 (0.0212)
[21:30:47.262968] Test:  [  0/157]  eta: 0:01:38  testing_loss: 1.0130 (1.0130)  acc1: 70.3125 (70.3125)  acc5: 95.3125 (95.3125)  time: 0.6294  data: 0.6000  max mem: 5511
[21:30:47.549379] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.0671 (1.0890)  acc1: 65.6250 (62.2159)  acc5: 96.8750 (96.5909)  time: 0.0831  data: 0.0547  max mem: 5511
[21:30:47.832360] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.0311 (1.0462)  acc1: 65.6250 (63.9137)  acc5: 96.8750 (97.4702)  time: 0.0283  data: 0.0002  max mem: 5511
[21:30:48.117080] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.0311 (1.0514)  acc1: 65.6250 (64.3145)  acc5: 96.8750 (96.9254)  time: 0.0282  data: 0.0002  max mem: 5511
[21:30:48.402500] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.0469 (1.0485)  acc1: 65.6250 (64.4817)  acc5: 95.3125 (96.6082)  time: 0.0284  data: 0.0002  max mem: 5511
[21:30:48.687106] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.0228 (1.0433)  acc1: 65.6250 (64.8284)  acc5: 96.8750 (96.7525)  time: 0.0284  data: 0.0002  max mem: 5511
[21:30:48.968839] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.0218 (1.0398)  acc1: 67.1875 (65.0359)  acc5: 96.8750 (96.6701)  time: 0.0282  data: 0.0002  max mem: 5511
[21:30:49.253600] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.9707 (1.0284)  acc1: 67.1875 (65.5810)  acc5: 96.8750 (96.6549)  time: 0.0282  data: 0.0002  max mem: 5511
[21:30:49.542238] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.9654 (1.0313)  acc1: 67.1875 (65.4900)  acc5: 95.3125 (96.4892)  time: 0.0285  data: 0.0002  max mem: 5511
[21:30:49.825244] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.0709 (1.0359)  acc1: 64.0625 (65.1271)  acc5: 96.8750 (96.5144)  time: 0.0284  data: 0.0002  max mem: 5511
[21:30:50.107884] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.1000 (1.0413)  acc1: 60.9375 (64.9907)  acc5: 96.8750 (96.5347)  time: 0.0282  data: 0.0001  max mem: 5511
[21:30:50.392289] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.0630 (1.0416)  acc1: 64.0625 (65.0056)  acc5: 95.3125 (96.5231)  time: 0.0282  data: 0.0002  max mem: 5511
[21:30:50.676585] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.0001 (1.0358)  acc1: 64.0625 (65.1860)  acc5: 96.8750 (96.6038)  time: 0.0283  data: 0.0003  max mem: 5511
[21:30:50.959065] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.0111 (1.0413)  acc1: 60.9375 (64.6827)  acc5: 96.8750 (96.6245)  time: 0.0282  data: 0.0002  max mem: 5511
[21:30:51.242359] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.0808 (1.0405)  acc1: 60.9375 (64.9269)  acc5: 96.8750 (96.5869)  time: 0.0282  data: 0.0002  max mem: 5511
[21:30:51.522011] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.0099 (1.0368)  acc1: 67.1875 (65.0662)  acc5: 96.8750 (96.5853)  time: 0.0280  data: 0.0001  max mem: 5511
[21:30:51.675504] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.0062 (1.0394)  acc1: 67.1875 (64.9500)  acc5: 96.8750 (96.5900)  time: 0.0271  data: 0.0001  max mem: 5511
[21:30:51.819206] Test: Total time: 0:00:05 (0.0330 s / it)
[21:30:51.819655] * Acc@1 64.950 Acc@5 96.590 loss 1.039
[21:30:51.819948] Accuracy of the network on the 10000 test images: 65.0%
[21:30:51.820195] Max accuracy: 64.95%
[21:30:51.939799] log_dir: ./output_dir
[21:30:52.838678] Epoch: [14]  [  0/781]  eta: 0:11:40  lr: 0.000245  training_loss: 1.6407 (1.6407)  mae_loss: 0.0312 (0.0312)  classification_loss: 1.5966 (1.5966)  loss_mask: 0.0129 (0.0129)  time: 0.8970  data: 0.6617  max mem: 5511
[21:30:56.809422] Epoch: [14]  [ 20/781]  eta: 0:02:56  lr: 0.000244  training_loss: 1.7028 (1.6912)  mae_loss: 0.0331 (0.0330)  classification_loss: 1.6463 (1.6394)  loss_mask: 0.0186 (0.0188)  time: 0.1984  data: 0.0002  max mem: 5511
[21:31:00.739219] Epoch: [14]  [ 40/781]  eta: 0:02:38  lr: 0.000244  training_loss: 1.7708 (1.7080)  mae_loss: 0.0334 (0.0335)  classification_loss: 1.7135 (1.6562)  loss_mask: 0.0172 (0.0183)  time: 0.1964  data: 0.0002  max mem: 5511
[21:31:04.680371] Epoch: [14]  [ 60/781]  eta: 0:02:30  lr: 0.000244  training_loss: 1.7579 (1.7243)  mae_loss: 0.0352 (0.0341)  classification_loss: 1.7091 (1.6717)  loss_mask: 0.0187 (0.0185)  time: 0.1970  data: 0.0002  max mem: 5511
[21:31:08.577931] Epoch: [14]  [ 80/781]  eta: 0:02:23  lr: 0.000244  training_loss: 1.7539 (1.7304)  mae_loss: 0.0330 (0.0341)  classification_loss: 1.7018 (1.6774)  loss_mask: 0.0203 (0.0189)  time: 0.1948  data: 0.0004  max mem: 5511
[21:31:12.491472] Epoch: [14]  [100/781]  eta: 0:02:18  lr: 0.000244  training_loss: 1.7685 (1.7405)  mae_loss: 0.0335 (0.0340)  classification_loss: 1.7134 (1.6864)  loss_mask: 0.0214 (0.0201)  time: 0.1956  data: 0.0002  max mem: 5511
[21:31:16.443116] Epoch: [14]  [120/781]  eta: 0:02:13  lr: 0.000244  training_loss: 1.7201 (1.7375)  mae_loss: 0.0335 (0.0340)  classification_loss: 1.6667 (1.6830)  loss_mask: 0.0166 (0.0205)  time: 0.1975  data: 0.0002  max mem: 5511
[21:31:20.374832] Epoch: [14]  [140/781]  eta: 0:02:09  lr: 0.000244  training_loss: 1.6617 (1.7257)  mae_loss: 0.0351 (0.0340)  classification_loss: 1.6118 (1.6710)  loss_mask: 0.0188 (0.0207)  time: 0.1965  data: 0.0004  max mem: 5511
[21:31:24.310705] Epoch: [14]  [160/781]  eta: 0:02:04  lr: 0.000244  training_loss: 1.6461 (1.7193)  mae_loss: 0.0324 (0.0338)  classification_loss: 1.5966 (1.6652)  loss_mask: 0.0151 (0.0203)  time: 0.1967  data: 0.0002  max mem: 5511
[21:31:28.226758] Epoch: [14]  [180/781]  eta: 0:02:00  lr: 0.000244  training_loss: 1.6936 (1.7180)  mae_loss: 0.0352 (0.0340)  classification_loss: 1.6441 (1.6640)  loss_mask: 0.0160 (0.0200)  time: 0.1957  data: 0.0002  max mem: 5511
[21:31:32.170043] Epoch: [14]  [200/781]  eta: 0:01:56  lr: 0.000244  training_loss: 1.7200 (1.7171)  mae_loss: 0.0324 (0.0340)  classification_loss: 1.6736 (1.6636)  loss_mask: 0.0151 (0.0196)  time: 0.1971  data: 0.0002  max mem: 5511
[21:31:36.099731] Epoch: [14]  [220/781]  eta: 0:01:52  lr: 0.000244  training_loss: 1.6977 (1.7161)  mae_loss: 0.0335 (0.0339)  classification_loss: 1.6545 (1.6630)  loss_mask: 0.0135 (0.0191)  time: 0.1964  data: 0.0002  max mem: 5511
[21:31:40.044232] Epoch: [14]  [240/781]  eta: 0:01:47  lr: 0.000244  training_loss: 1.7262 (1.7156)  mae_loss: 0.0335 (0.0339)  classification_loss: 1.6712 (1.6627)  loss_mask: 0.0160 (0.0189)  time: 0.1972  data: 0.0002  max mem: 5511
[21:31:43.972965] Epoch: [14]  [260/781]  eta: 0:01:43  lr: 0.000244  training_loss: 1.7089 (1.7153)  mae_loss: 0.0319 (0.0339)  classification_loss: 1.6327 (1.6623)  loss_mask: 0.0183 (0.0192)  time: 0.1963  data: 0.0002  max mem: 5511
[21:31:47.891993] Epoch: [14]  [280/781]  eta: 0:01:39  lr: 0.000244  training_loss: 1.7309 (1.7160)  mae_loss: 0.0349 (0.0339)  classification_loss: 1.6807 (1.6630)  loss_mask: 0.0167 (0.0190)  time: 0.1959  data: 0.0002  max mem: 5511
[21:31:51.851355] Epoch: [14]  [300/781]  eta: 0:01:35  lr: 0.000244  training_loss: 1.7411 (1.7172)  mae_loss: 0.0347 (0.0340)  classification_loss: 1.6851 (1.6641)  loss_mask: 0.0186 (0.0191)  time: 0.1979  data: 0.0003  max mem: 5511
[21:31:55.778446] Epoch: [14]  [320/781]  eta: 0:01:31  lr: 0.000244  training_loss: 1.6748 (1.7168)  mae_loss: 0.0333 (0.0339)  classification_loss: 1.6273 (1.6636)  loss_mask: 0.0194 (0.0192)  time: 0.1963  data: 0.0002  max mem: 5511
[21:31:59.718695] Epoch: [14]  [340/781]  eta: 0:01:27  lr: 0.000244  training_loss: 1.7062 (1.7173)  mae_loss: 0.0345 (0.0340)  classification_loss: 1.6537 (1.6643)  loss_mask: 0.0156 (0.0191)  time: 0.1969  data: 0.0002  max mem: 5511
[21:32:03.670785] Epoch: [14]  [360/781]  eta: 0:01:23  lr: 0.000244  training_loss: 1.7060 (1.7177)  mae_loss: 0.0332 (0.0340)  classification_loss: 1.6532 (1.6647)  loss_mask: 0.0162 (0.0190)  time: 0.1975  data: 0.0002  max mem: 5511
[21:32:07.687836] Epoch: [14]  [380/781]  eta: 0:01:19  lr: 0.000244  training_loss: 1.7203 (1.7182)  mae_loss: 0.0329 (0.0340)  classification_loss: 1.6510 (1.6650)  loss_mask: 0.0189 (0.0191)  time: 0.2008  data: 0.0003  max mem: 5511
[21:32:11.614627] Epoch: [14]  [400/781]  eta: 0:01:15  lr: 0.000244  training_loss: 1.6879 (1.7178)  mae_loss: 0.0336 (0.0340)  classification_loss: 1.6431 (1.6649)  loss_mask: 0.0132 (0.0189)  time: 0.1963  data: 0.0003  max mem: 5511
[21:32:15.545625] Epoch: [14]  [420/781]  eta: 0:01:11  lr: 0.000244  training_loss: 1.6937 (1.7173)  mae_loss: 0.0347 (0.0341)  classification_loss: 1.6516 (1.6644)  loss_mask: 0.0134 (0.0188)  time: 0.1965  data: 0.0003  max mem: 5511
[21:32:19.471229] Epoch: [14]  [440/781]  eta: 0:01:07  lr: 0.000244  training_loss: 1.6825 (1.7165)  mae_loss: 0.0332 (0.0341)  classification_loss: 1.6438 (1.6639)  loss_mask: 0.0129 (0.0185)  time: 0.1962  data: 0.0003  max mem: 5511
[21:32:23.402920] Epoch: [14]  [460/781]  eta: 0:01:03  lr: 0.000244  training_loss: 1.7210 (1.7156)  mae_loss: 0.0337 (0.0341)  classification_loss: 1.6724 (1.6631)  loss_mask: 0.0136 (0.0184)  time: 0.1965  data: 0.0003  max mem: 5511
[21:32:27.339022] Epoch: [14]  [480/781]  eta: 0:00:59  lr: 0.000244  training_loss: 1.7286 (1.7164)  mae_loss: 0.0330 (0.0341)  classification_loss: 1.6743 (1.6639)  loss_mask: 0.0162 (0.0184)  time: 0.1967  data: 0.0002  max mem: 5511
[21:32:31.250260] Epoch: [14]  [500/781]  eta: 0:00:55  lr: 0.000244  training_loss: 1.6900 (1.7161)  mae_loss: 0.0335 (0.0340)  classification_loss: 1.6325 (1.6636)  loss_mask: 0.0194 (0.0185)  time: 0.1955  data: 0.0003  max mem: 5511
[21:32:35.174305] Epoch: [14]  [520/781]  eta: 0:00:51  lr: 0.000244  training_loss: 1.6837 (1.7159)  mae_loss: 0.0350 (0.0341)  classification_loss: 1.6290 (1.6632)  loss_mask: 0.0210 (0.0186)  time: 0.1961  data: 0.0002  max mem: 5511
[21:32:39.083254] Epoch: [14]  [540/781]  eta: 0:00:47  lr: 0.000244  training_loss: 1.7677 (1.7174)  mae_loss: 0.0336 (0.0341)  classification_loss: 1.7127 (1.6644)  loss_mask: 0.0239 (0.0188)  time: 0.1954  data: 0.0002  max mem: 5511
[21:32:43.035064] Epoch: [14]  [560/781]  eta: 0:00:43  lr: 0.000244  training_loss: 1.6690 (1.7156)  mae_loss: 0.0316 (0.0341)  classification_loss: 1.6287 (1.6628)  loss_mask: 0.0164 (0.0188)  time: 0.1975  data: 0.0002  max mem: 5511
[21:32:47.027167] Epoch: [14]  [580/781]  eta: 0:00:39  lr: 0.000244  training_loss: 1.7127 (1.7163)  mae_loss: 0.0338 (0.0341)  classification_loss: 1.6538 (1.6634)  loss_mask: 0.0210 (0.0189)  time: 0.1995  data: 0.0002  max mem: 5511
[21:32:50.953172] Epoch: [14]  [600/781]  eta: 0:00:35  lr: 0.000244  training_loss: 1.6589 (1.7149)  mae_loss: 0.0359 (0.0341)  classification_loss: 1.6084 (1.6619)  loss_mask: 0.0163 (0.0188)  time: 0.1962  data: 0.0002  max mem: 5511
[21:32:54.885833] Epoch: [14]  [620/781]  eta: 0:00:31  lr: 0.000244  training_loss: 1.7163 (1.7145)  mae_loss: 0.0343 (0.0342)  classification_loss: 1.6660 (1.6615)  loss_mask: 0.0221 (0.0189)  time: 0.1966  data: 0.0003  max mem: 5511
[21:32:58.804933] Epoch: [14]  [640/781]  eta: 0:00:27  lr: 0.000243  training_loss: 1.6723 (1.7133)  mae_loss: 0.0320 (0.0341)  classification_loss: 1.6269 (1.6604)  loss_mask: 0.0163 (0.0188)  time: 0.1959  data: 0.0003  max mem: 5511
[21:33:02.722678] Epoch: [14]  [660/781]  eta: 0:00:23  lr: 0.000243  training_loss: 1.7453 (1.7139)  mae_loss: 0.0327 (0.0341)  classification_loss: 1.6955 (1.6610)  loss_mask: 0.0146 (0.0188)  time: 0.1958  data: 0.0003  max mem: 5511
[21:33:06.666370] Epoch: [14]  [680/781]  eta: 0:00:19  lr: 0.000243  training_loss: 1.7214 (1.7137)  mae_loss: 0.0333 (0.0341)  classification_loss: 1.6773 (1.6610)  loss_mask: 0.0141 (0.0186)  time: 0.1971  data: 0.0002  max mem: 5511
[21:33:10.634093] Epoch: [14]  [700/781]  eta: 0:00:16  lr: 0.000243  training_loss: 1.6572 (1.7127)  mae_loss: 0.0329 (0.0340)  classification_loss: 1.6092 (1.6601)  loss_mask: 0.0143 (0.0185)  time: 0.1983  data: 0.0003  max mem: 5511
[21:33:14.557654] Epoch: [14]  [720/781]  eta: 0:00:12  lr: 0.000243  training_loss: 1.7035 (1.7131)  mae_loss: 0.0319 (0.0340)  classification_loss: 1.6645 (1.6607)  loss_mask: 0.0153 (0.0185)  time: 0.1961  data: 0.0003  max mem: 5511
[21:33:18.468194] Epoch: [14]  [740/781]  eta: 0:00:08  lr: 0.000243  training_loss: 1.7101 (1.7127)  mae_loss: 0.0300 (0.0340)  classification_loss: 1.6584 (1.6604)  loss_mask: 0.0151 (0.0184)  time: 0.1954  data: 0.0003  max mem: 5511
[21:33:22.373747] Epoch: [14]  [760/781]  eta: 0:00:04  lr: 0.000243  training_loss: 1.7579 (1.7136)  mae_loss: 0.0328 (0.0340)  classification_loss: 1.7046 (1.6613)  loss_mask: 0.0142 (0.0183)  time: 0.1952  data: 0.0002  max mem: 5511
[21:33:26.301591] Epoch: [14]  [780/781]  eta: 0:00:00  lr: 0.000243  training_loss: 1.6693 (1.7128)  mae_loss: 0.0327 (0.0340)  classification_loss: 1.6173 (1.6606)  loss_mask: 0.0138 (0.0183)  time: 0.1963  data: 0.0003  max mem: 5511
[21:33:26.448300] Epoch: [14] Total time: 0:02:34 (0.1978 s / it)
[21:33:26.449177] Averaged stats: lr: 0.000243  training_loss: 1.6693 (1.7128)  mae_loss: 0.0327 (0.0340)  classification_loss: 1.6173 (1.6606)  loss_mask: 0.0138 (0.0183)
[21:33:27.092470] Test:  [  0/157]  eta: 0:01:40  testing_loss: 0.9796 (0.9796)  acc1: 60.9375 (60.9375)  acc5: 90.6250 (90.6250)  time: 0.6387  data: 0.6067  max mem: 5511
[21:33:27.377602] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.0427 (1.0381)  acc1: 64.0625 (64.0625)  acc5: 98.4375 (97.0170)  time: 0.0838  data: 0.0554  max mem: 5511
[21:33:27.661338] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.0170 (0.9959)  acc1: 64.0625 (66.0714)  acc5: 98.4375 (97.4702)  time: 0.0283  data: 0.0002  max mem: 5511
[21:33:27.946068] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.0170 (1.0149)  acc1: 67.1875 (65.7258)  acc5: 96.8750 (96.9758)  time: 0.0282  data: 0.0002  max mem: 5511
[21:33:28.228722] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.0305 (1.0153)  acc1: 65.6250 (65.8537)  acc5: 96.8750 (96.9131)  time: 0.0282  data: 0.0002  max mem: 5511
[21:33:28.526254] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.9732 (1.0067)  acc1: 68.7500 (66.7279)  acc5: 96.8750 (96.8750)  time: 0.0289  data: 0.0007  max mem: 5511
[21:33:28.813247] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.9760 (1.0011)  acc1: 68.7500 (66.8289)  acc5: 96.8750 (96.7982)  time: 0.0291  data: 0.0007  max mem: 5511
[21:33:29.099821] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.9403 (0.9880)  acc1: 65.6250 (67.3636)  acc5: 96.8750 (96.8970)  time: 0.0285  data: 0.0002  max mem: 5511
[21:33:29.384915] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.9345 (0.9904)  acc1: 65.6250 (67.2647)  acc5: 96.8750 (96.8750)  time: 0.0285  data: 0.0002  max mem: 5511
[21:33:29.668733] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.0440 (0.9965)  acc1: 64.0625 (66.7582)  acc5: 96.8750 (96.8750)  time: 0.0283  data: 0.0002  max mem: 5511
[21:33:29.959467] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.0565 (0.9997)  acc1: 64.0625 (66.5842)  acc5: 96.8750 (96.9214)  time: 0.0286  data: 0.0002  max mem: 5511
[21:33:30.247755] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.0307 (0.9996)  acc1: 64.0625 (66.6385)  acc5: 96.8750 (96.9172)  time: 0.0288  data: 0.0002  max mem: 5511
[21:33:30.537130] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.9896 (0.9939)  acc1: 68.7500 (66.9034)  acc5: 96.8750 (96.9783)  time: 0.0288  data: 0.0002  max mem: 5511
[21:33:30.826479] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.9904 (0.9983)  acc1: 65.6250 (66.5434)  acc5: 96.8750 (96.9943)  time: 0.0288  data: 0.0002  max mem: 5511
[21:33:31.112589] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.0327 (0.9984)  acc1: 65.6250 (66.6334)  acc5: 96.8750 (97.0191)  time: 0.0286  data: 0.0003  max mem: 5511
[21:33:31.394614] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.9532 (0.9947)  acc1: 68.7500 (66.8978)  acc5: 96.8750 (97.0406)  time: 0.0282  data: 0.0002  max mem: 5511
[21:33:31.547412] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.9532 (0.9981)  acc1: 68.7500 (66.7300)  acc5: 98.4375 (97.0600)  time: 0.0272  data: 0.0001  max mem: 5511
[21:33:31.701962] Test: Total time: 0:00:05 (0.0334 s / it)
[21:33:31.702415] * Acc@1 66.730 Acc@5 97.060 loss 0.998
[21:33:31.702698] Accuracy of the network on the 10000 test images: 66.7%
[21:33:31.702875] Max accuracy: 66.73%
[21:33:31.915149] log_dir: ./output_dir
[21:33:32.812351] Epoch: [15]  [  0/781]  eta: 0:11:39  lr: 0.000243  training_loss: 1.7922 (1.7922)  mae_loss: 0.0290 (0.0290)  classification_loss: 1.7523 (1.7523)  loss_mask: 0.0109 (0.0109)  time: 0.8952  data: 0.6744  max mem: 5511
[21:33:36.752986] Epoch: [15]  [ 20/781]  eta: 0:02:55  lr: 0.000243  training_loss: 1.6756 (1.6679)  mae_loss: 0.0351 (0.0347)  classification_loss: 1.6136 (1.6153)  loss_mask: 0.0135 (0.0180)  time: 0.1969  data: 0.0002  max mem: 5511
[21:33:40.678997] Epoch: [15]  [ 40/781]  eta: 0:02:38  lr: 0.000243  training_loss: 1.6950 (1.6845)  mae_loss: 0.0341 (0.0347)  classification_loss: 1.6372 (1.6323)  loss_mask: 0.0155 (0.0175)  time: 0.1962  data: 0.0002  max mem: 5511
[21:33:44.610537] Epoch: [15]  [ 60/781]  eta: 0:02:29  lr: 0.000243  training_loss: 1.7505 (1.7039)  mae_loss: 0.0326 (0.0345)  classification_loss: 1.7006 (1.6521)  loss_mask: 0.0146 (0.0173)  time: 0.1965  data: 0.0002  max mem: 5511
[21:33:48.559634] Epoch: [15]  [ 80/781]  eta: 0:02:23  lr: 0.000243  training_loss: 1.7149 (1.7073)  mae_loss: 0.0330 (0.0343)  classification_loss: 1.6685 (1.6557)  loss_mask: 0.0167 (0.0173)  time: 0.1974  data: 0.0004  max mem: 5511
[21:33:52.501778] Epoch: [15]  [100/781]  eta: 0:02:18  lr: 0.000243  training_loss: 1.7196 (1.7116)  mae_loss: 0.0330 (0.0341)  classification_loss: 1.6707 (1.6601)  loss_mask: 0.0124 (0.0174)  time: 0.1970  data: 0.0002  max mem: 5511
[21:33:56.466271] Epoch: [15]  [120/781]  eta: 0:02:14  lr: 0.000243  training_loss: 1.7400 (1.7101)  mae_loss: 0.0308 (0.0338)  classification_loss: 1.6701 (1.6579)  loss_mask: 0.0203 (0.0184)  time: 0.1981  data: 0.0002  max mem: 5511
[21:34:00.395169] Epoch: [15]  [140/781]  eta: 0:02:09  lr: 0.000243  training_loss: 1.6701 (1.7054)  mae_loss: 0.0320 (0.0337)  classification_loss: 1.6154 (1.6533)  loss_mask: 0.0164 (0.0185)  time: 0.1963  data: 0.0004  max mem: 5511
[21:34:04.332452] Epoch: [15]  [160/781]  eta: 0:02:04  lr: 0.000243  training_loss: 1.6934 (1.7060)  mae_loss: 0.0340 (0.0338)  classification_loss: 1.6333 (1.6533)  loss_mask: 0.0205 (0.0189)  time: 0.1968  data: 0.0002  max mem: 5511
[21:34:08.313265] Epoch: [15]  [180/781]  eta: 0:02:00  lr: 0.000243  training_loss: 1.6373 (1.7020)  mae_loss: 0.0324 (0.0337)  classification_loss: 1.5864 (1.6494)  loss_mask: 0.0166 (0.0188)  time: 0.1988  data: 0.0002  max mem: 5511
[21:34:12.262160] Epoch: [15]  [200/781]  eta: 0:01:56  lr: 0.000243  training_loss: 1.6682 (1.6995)  mae_loss: 0.0335 (0.0337)  classification_loss: 1.6223 (1.6475)  loss_mask: 0.0132 (0.0183)  time: 0.1972  data: 0.0002  max mem: 5511
[21:34:16.226198] Epoch: [15]  [220/781]  eta: 0:01:52  lr: 0.000243  training_loss: 1.6825 (1.6996)  mae_loss: 0.0328 (0.0336)  classification_loss: 1.6315 (1.6480)  loss_mask: 0.0133 (0.0179)  time: 0.1981  data: 0.0003  max mem: 5511
[21:34:20.184844] Epoch: [15]  [240/781]  eta: 0:01:48  lr: 0.000243  training_loss: 1.6999 (1.7002)  mae_loss: 0.0330 (0.0336)  classification_loss: 1.6463 (1.6488)  loss_mask: 0.0147 (0.0178)  time: 0.1978  data: 0.0002  max mem: 5511
[21:34:24.109276] Epoch: [15]  [260/781]  eta: 0:01:44  lr: 0.000243  training_loss: 1.6145 (1.6967)  mae_loss: 0.0349 (0.0338)  classification_loss: 1.5516 (1.6453)  loss_mask: 0.0146 (0.0176)  time: 0.1961  data: 0.0003  max mem: 5511
[21:34:28.080408] Epoch: [15]  [280/781]  eta: 0:01:40  lr: 0.000243  training_loss: 1.7209 (1.6980)  mae_loss: 0.0339 (0.0338)  classification_loss: 1.6713 (1.6468)  loss_mask: 0.0149 (0.0174)  time: 0.1985  data: 0.0002  max mem: 5511
[21:34:32.020359] Epoch: [15]  [300/781]  eta: 0:01:35  lr: 0.000243  training_loss: 1.6651 (1.6970)  mae_loss: 0.0334 (0.0338)  classification_loss: 1.6070 (1.6457)  loss_mask: 0.0172 (0.0175)  time: 0.1969  data: 0.0002  max mem: 5511
[21:34:35.971140] Epoch: [15]  [320/781]  eta: 0:01:31  lr: 0.000243  training_loss: 1.7214 (1.6988)  mae_loss: 0.0333 (0.0337)  classification_loss: 1.6676 (1.6476)  loss_mask: 0.0147 (0.0174)  time: 0.1975  data: 0.0002  max mem: 5511
[21:34:39.892307] Epoch: [15]  [340/781]  eta: 0:01:27  lr: 0.000243  training_loss: 1.6555 (1.6986)  mae_loss: 0.0330 (0.0337)  classification_loss: 1.6085 (1.6477)  loss_mask: 0.0136 (0.0172)  time: 0.1960  data: 0.0003  max mem: 5511
[21:34:43.900160] Epoch: [15]  [360/781]  eta: 0:01:23  lr: 0.000243  training_loss: 1.7032 (1.6987)  mae_loss: 0.0337 (0.0337)  classification_loss: 1.6546 (1.6481)  loss_mask: 0.0113 (0.0169)  time: 0.2002  data: 0.0002  max mem: 5511
[21:34:47.834119] Epoch: [15]  [380/781]  eta: 0:01:19  lr: 0.000243  training_loss: 1.6738 (1.6983)  mae_loss: 0.0341 (0.0337)  classification_loss: 1.6327 (1.6479)  loss_mask: 0.0117 (0.0167)  time: 0.1966  data: 0.0002  max mem: 5511
[21:34:51.764984] Epoch: [15]  [400/781]  eta: 0:01:15  lr: 0.000243  training_loss: 1.6936 (1.6992)  mae_loss: 0.0324 (0.0337)  classification_loss: 1.6517 (1.6491)  loss_mask: 0.0112 (0.0165)  time: 0.1965  data: 0.0003  max mem: 5511
[21:34:55.683774] Epoch: [15]  [420/781]  eta: 0:01:11  lr: 0.000243  training_loss: 1.6516 (1.6976)  mae_loss: 0.0344 (0.0337)  classification_loss: 1.5993 (1.6476)  loss_mask: 0.0110 (0.0162)  time: 0.1959  data: 0.0002  max mem: 5511
[21:34:59.624026] Epoch: [15]  [440/781]  eta: 0:01:07  lr: 0.000242  training_loss: 1.6842 (1.6986)  mae_loss: 0.0335 (0.0337)  classification_loss: 1.6420 (1.6487)  loss_mask: 0.0136 (0.0162)  time: 0.1969  data: 0.0003  max mem: 5511
[21:35:03.562440] Epoch: [15]  [460/781]  eta: 0:01:03  lr: 0.000242  training_loss: 1.5903 (1.6964)  mae_loss: 0.0337 (0.0337)  classification_loss: 1.5525 (1.6467)  loss_mask: 0.0107 (0.0160)  time: 0.1968  data: 0.0002  max mem: 5511
[21:35:07.490409] Epoch: [15]  [480/781]  eta: 0:00:59  lr: 0.000242  training_loss: 1.6950 (1.6962)  mae_loss: 0.0337 (0.0337)  classification_loss: 1.6439 (1.6466)  loss_mask: 0.0128 (0.0159)  time: 0.1963  data: 0.0003  max mem: 5511
[21:35:11.423529] Epoch: [15]  [500/781]  eta: 0:00:55  lr: 0.000242  training_loss: 1.7070 (1.6961)  mae_loss: 0.0322 (0.0336)  classification_loss: 1.6525 (1.6464)  loss_mask: 0.0185 (0.0160)  time: 0.1965  data: 0.0002  max mem: 5511
[21:35:15.393973] Epoch: [15]  [520/781]  eta: 0:00:51  lr: 0.000242  training_loss: 1.6968 (1.6951)  mae_loss: 0.0326 (0.0336)  classification_loss: 1.6526 (1.6455)  loss_mask: 0.0145 (0.0160)  time: 0.1984  data: 0.0002  max mem: 5511
[21:35:19.327688] Epoch: [15]  [540/781]  eta: 0:00:47  lr: 0.000242  training_loss: 1.7376 (1.6963)  mae_loss: 0.0326 (0.0336)  classification_loss: 1.6853 (1.6468)  loss_mask: 0.0145 (0.0160)  time: 0.1966  data: 0.0003  max mem: 5511
[21:35:23.275194] Epoch: [15]  [560/781]  eta: 0:00:43  lr: 0.000242  training_loss: 1.6872 (1.6960)  mae_loss: 0.0343 (0.0336)  classification_loss: 1.6402 (1.6465)  loss_mask: 0.0119 (0.0159)  time: 0.1973  data: 0.0004  max mem: 5511
[21:35:27.193363] Epoch: [15]  [580/781]  eta: 0:00:39  lr: 0.000242  training_loss: 1.6974 (1.6959)  mae_loss: 0.0343 (0.0336)  classification_loss: 1.6255 (1.6461)  loss_mask: 0.0200 (0.0162)  time: 0.1958  data: 0.0004  max mem: 5511
[21:35:31.146016] Epoch: [15]  [600/781]  eta: 0:00:35  lr: 0.000242  training_loss: 1.6802 (1.6952)  mae_loss: 0.0311 (0.0336)  classification_loss: 1.6262 (1.6455)  loss_mask: 0.0167 (0.0162)  time: 0.1976  data: 0.0005  max mem: 5511
[21:35:35.093676] Epoch: [15]  [620/781]  eta: 0:00:31  lr: 0.000242  training_loss: 1.6766 (1.6949)  mae_loss: 0.0331 (0.0336)  classification_loss: 1.6299 (1.6452)  loss_mask: 0.0132 (0.0161)  time: 0.1973  data: 0.0002  max mem: 5511
[21:35:39.023935] Epoch: [15]  [640/781]  eta: 0:00:27  lr: 0.000242  training_loss: 1.6901 (1.6943)  mae_loss: 0.0346 (0.0336)  classification_loss: 1.6400 (1.6446)  loss_mask: 0.0138 (0.0161)  time: 0.1964  data: 0.0002  max mem: 5511
[21:35:42.940814] Epoch: [15]  [660/781]  eta: 0:00:23  lr: 0.000242  training_loss: 1.6993 (1.6942)  mae_loss: 0.0322 (0.0336)  classification_loss: 1.6582 (1.6446)  loss_mask: 0.0122 (0.0160)  time: 0.1958  data: 0.0002  max mem: 5511
[21:35:46.890301] Epoch: [15]  [680/781]  eta: 0:00:20  lr: 0.000242  training_loss: 1.6963 (1.6944)  mae_loss: 0.0320 (0.0336)  classification_loss: 1.6430 (1.6449)  loss_mask: 0.0107 (0.0159)  time: 0.1974  data: 0.0002  max mem: 5511
[21:35:50.808764] Epoch: [15]  [700/781]  eta: 0:00:16  lr: 0.000242  training_loss: 1.6837 (1.6944)  mae_loss: 0.0325 (0.0335)  classification_loss: 1.6238 (1.6448)  loss_mask: 0.0222 (0.0161)  time: 0.1958  data: 0.0002  max mem: 5511
[21:35:54.740537] Epoch: [15]  [720/781]  eta: 0:00:12  lr: 0.000242  training_loss: 1.7089 (1.6949)  mae_loss: 0.0338 (0.0335)  classification_loss: 1.6433 (1.6451)  loss_mask: 0.0179 (0.0162)  time: 0.1965  data: 0.0002  max mem: 5511
[21:35:58.667953] Epoch: [15]  [740/781]  eta: 0:00:08  lr: 0.000242  training_loss: 1.6962 (1.6948)  mae_loss: 0.0329 (0.0335)  classification_loss: 1.6515 (1.6451)  loss_mask: 0.0137 (0.0162)  time: 0.1963  data: 0.0003  max mem: 5511
[21:36:02.598688] Epoch: [15]  [760/781]  eta: 0:00:04  lr: 0.000242  training_loss: 1.7224 (1.6955)  mae_loss: 0.0315 (0.0335)  classification_loss: 1.6831 (1.6459)  loss_mask: 0.0121 (0.0161)  time: 0.1965  data: 0.0003  max mem: 5511
[21:36:06.515860] Epoch: [15]  [780/781]  eta: 0:00:00  lr: 0.000242  training_loss: 1.6743 (1.6953)  mae_loss: 0.0306 (0.0335)  classification_loss: 1.6269 (1.6459)  loss_mask: 0.0113 (0.0160)  time: 0.1958  data: 0.0002  max mem: 5511
[21:36:06.675560] Epoch: [15] Total time: 0:02:34 (0.1982 s / it)
[21:36:06.676202] Averaged stats: lr: 0.000242  training_loss: 1.6743 (1.6953)  mae_loss: 0.0306 (0.0335)  classification_loss: 1.6269 (1.6459)  loss_mask: 0.0113 (0.0160)
[21:36:07.324814] Test:  [  0/157]  eta: 0:01:41  testing_loss: 0.9253 (0.9253)  acc1: 70.3125 (70.3125)  acc5: 93.7500 (93.7500)  time: 0.6443  data: 0.6124  max mem: 5511
[21:36:07.612696] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.9800 (1.0382)  acc1: 67.1875 (64.6307)  acc5: 96.8750 (96.4489)  time: 0.0845  data: 0.0560  max mem: 5511
[21:36:07.899412] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.9685 (0.9895)  acc1: 67.1875 (66.4435)  acc5: 96.8750 (97.1726)  time: 0.0286  data: 0.0003  max mem: 5511
[21:36:08.191158] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.9580 (0.9912)  acc1: 68.7500 (66.3810)  acc5: 96.8750 (96.9758)  time: 0.0288  data: 0.0002  max mem: 5511
[21:36:08.489858] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.9698 (0.9889)  acc1: 67.1875 (66.6159)  acc5: 96.8750 (96.9512)  time: 0.0294  data: 0.0002  max mem: 5511
[21:36:08.778028] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.9647 (0.9847)  acc1: 67.1875 (67.0650)  acc5: 96.8750 (97.0282)  time: 0.0292  data: 0.0002  max mem: 5511
[21:36:09.080451] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.9570 (0.9830)  acc1: 65.6250 (66.9570)  acc5: 96.8750 (96.8750)  time: 0.0294  data: 0.0002  max mem: 5511
[21:36:09.366906] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.9400 (0.9726)  acc1: 67.1875 (67.2535)  acc5: 96.8750 (96.9630)  time: 0.0293  data: 0.0002  max mem: 5511
[21:36:09.651523] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.9400 (0.9748)  acc1: 67.1875 (67.3032)  acc5: 96.8750 (96.8943)  time: 0.0284  data: 0.0002  max mem: 5511
[21:36:09.942035] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.9835 (0.9757)  acc1: 67.1875 (67.2390)  acc5: 96.8750 (96.9437)  time: 0.0286  data: 0.0002  max mem: 5511
[21:36:10.229754] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.9900 (0.9793)  acc1: 67.1875 (67.1566)  acc5: 96.8750 (96.9833)  time: 0.0287  data: 0.0002  max mem: 5511
[21:36:10.514802] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.9972 (0.9805)  acc1: 67.1875 (67.1734)  acc5: 96.8750 (96.9735)  time: 0.0285  data: 0.0002  max mem: 5511
[21:36:10.801404] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.9533 (0.9748)  acc1: 70.3125 (67.4587)  acc5: 96.8750 (97.0041)  time: 0.0285  data: 0.0002  max mem: 5511
[21:36:11.092160] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.9628 (0.9791)  acc1: 65.6250 (67.0682)  acc5: 96.8750 (97.0301)  time: 0.0287  data: 0.0002  max mem: 5511
[21:36:11.375844] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.0051 (0.9791)  acc1: 65.6250 (67.2762)  acc5: 96.8750 (96.9858)  time: 0.0286  data: 0.0002  max mem: 5511
[21:36:11.657194] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.9471 (0.9774)  acc1: 67.1875 (67.3427)  acc5: 96.8750 (96.9578)  time: 0.0281  data: 0.0002  max mem: 5511
[21:36:11.808296] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.9368 (0.9808)  acc1: 67.1875 (67.1900)  acc5: 96.8750 (96.9400)  time: 0.0271  data: 0.0001  max mem: 5511
[21:36:11.962732] Test: Total time: 0:00:05 (0.0337 s / it)
[21:36:11.963184] * Acc@1 67.190 Acc@5 96.940 loss 0.981
[21:36:11.963470] Accuracy of the network on the 10000 test images: 67.2%
[21:36:11.963690] Max accuracy: 67.19%
[21:36:12.223480] log_dir: ./output_dir
[21:36:13.092779] Epoch: [16]  [  0/781]  eta: 0:11:17  lr: 0.000242  training_loss: 1.7022 (1.7022)  mae_loss: 0.0344 (0.0344)  classification_loss: 1.6544 (1.6544)  loss_mask: 0.0134 (0.0134)  time: 0.8678  data: 0.6603  max mem: 5511
[21:36:17.039302] Epoch: [16]  [ 20/781]  eta: 0:02:54  lr: 0.000242  training_loss: 1.6346 (1.6777)  mae_loss: 0.0330 (0.0330)  classification_loss: 1.5886 (1.6324)  loss_mask: 0.0109 (0.0122)  time: 0.1972  data: 0.0002  max mem: 5511
[21:36:20.982146] Epoch: [16]  [ 40/781]  eta: 0:02:38  lr: 0.000242  training_loss: 1.6941 (1.6857)  mae_loss: 0.0336 (0.0330)  classification_loss: 1.6403 (1.6367)  loss_mask: 0.0169 (0.0160)  time: 0.1971  data: 0.0006  max mem: 5511
[21:36:24.954526] Epoch: [16]  [ 60/781]  eta: 0:02:30  lr: 0.000242  training_loss: 1.6931 (1.6927)  mae_loss: 0.0323 (0.0331)  classification_loss: 1.6391 (1.6411)  loss_mask: 0.0209 (0.0186)  time: 0.1985  data: 0.0002  max mem: 5511
[21:36:28.917557] Epoch: [16]  [ 80/781]  eta: 0:02:24  lr: 0.000242  training_loss: 1.6953 (1.6886)  mae_loss: 0.0340 (0.0331)  classification_loss: 1.6528 (1.6366)  loss_mask: 0.0155 (0.0189)  time: 0.1981  data: 0.0002  max mem: 5511
[21:36:32.856529] Epoch: [16]  [100/781]  eta: 0:02:19  lr: 0.000242  training_loss: 1.6330 (1.6793)  mae_loss: 0.0309 (0.0328)  classification_loss: 1.5944 (1.6282)  loss_mask: 0.0142 (0.0182)  time: 0.1969  data: 0.0002  max mem: 5511
[21:36:36.784545] Epoch: [16]  [120/781]  eta: 0:02:14  lr: 0.000242  training_loss: 1.6982 (1.6804)  mae_loss: 0.0329 (0.0329)  classification_loss: 1.6462 (1.6298)  loss_mask: 0.0142 (0.0178)  time: 0.1963  data: 0.0002  max mem: 5511

[21:36:40.705125] Epoch: [16]  [140/781]  eta: 0:02:09  lr: 0.000242  training_loss: 1.6782 (1.6790)  mae_loss: 0.0358 (0.0332)  classification_loss: 1.6268 (1.6286)  loss_mask: 0.0137 (0.0172)  time: 0.1959  data: 0.0003  max mem: 5511
[21:36:44.626996] Epoch: [16]  [160/781]  eta: 0:02:04  lr: 0.000242  training_loss: 1.6721 (1.6818)  mae_loss: 0.0311 (0.0331)  classification_loss: 1.6255 (1.6318)  loss_mask: 0.0138 (0.0169)  time: 0.1960  data: 0.0002  max mem: 5511
[21:36:48.545016] Epoch: [16]  [180/781]  eta: 0:02:00  lr: 0.000242  training_loss: 1.6983 (1.6809)  mae_loss: 0.0337 (0.0331)  classification_loss: 1.6553 (1.6312)  loss_mask: 0.0139 (0.0167)  time: 0.1958  data: 0.0002  max mem: 5511
[21:36:52.473160] Epoch: [16]  [200/781]  eta: 0:01:56  lr: 0.000241  training_loss: 1.6823 (1.6836)  mae_loss: 0.0330 (0.0332)  classification_loss: 1.6323 (1.6343)  loss_mask: 0.0110 (0.0161)  time: 0.1963  data: 0.0002  max mem: 5511
[21:36:56.410191] Epoch: [16]  [220/781]  eta: 0:01:52  lr: 0.000241  training_loss: 1.6763 (1.6834)  mae_loss: 0.0334 (0.0332)  classification_loss: 1.6373 (1.6345)  loss_mask: 0.0111 (0.0157)  time: 0.1967  data: 0.0003  max mem: 5511
[21:37:00.351864] Epoch: [16]  [240/781]  eta: 0:01:47  lr: 0.000241  training_loss: 1.6565 (1.6818)  mae_loss: 0.0319 (0.0331)  classification_loss: 1.6095 (1.6331)  loss_mask: 0.0146 (0.0156)  time: 0.1970  data: 0.0002  max mem: 5511
[21:37:04.266024] Epoch: [16]  [260/781]  eta: 0:01:43  lr: 0.000241  training_loss: 1.6961 (1.6831)  mae_loss: 0.0315 (0.0330)  classification_loss: 1.6543 (1.6339)  loss_mask: 0.0182 (0.0161)  time: 0.1956  data: 0.0003  max mem: 5511
[21:37:08.252829] Epoch: [16]  [280/781]  eta: 0:01:39  lr: 0.000241  training_loss: 1.6649 (1.6833)  mae_loss: 0.0326 (0.0331)  classification_loss: 1.6098 (1.6341)  loss_mask: 0.0143 (0.0161)  time: 0.1993  data: 0.0002  max mem: 5511
[21:37:12.187851] Epoch: [16]  [300/781]  eta: 0:01:35  lr: 0.000241  training_loss: 1.6790 (1.6850)  mae_loss: 0.0323 (0.0330)  classification_loss: 1.6301 (1.6359)  loss_mask: 0.0135 (0.0160)  time: 0.1967  data: 0.0002  max mem: 5511
[21:37:16.117714] Epoch: [16]  [320/781]  eta: 0:01:31  lr: 0.000241  training_loss: 1.6942 (1.6854)  mae_loss: 0.0329 (0.0330)  classification_loss: 1.6387 (1.6364)  loss_mask: 0.0126 (0.0161)  time: 0.1964  data: 0.0002  max mem: 5511
[21:37:20.047515] Epoch: [16]  [340/781]  eta: 0:01:27  lr: 0.000241  training_loss: 1.6228 (1.6822)  mae_loss: 0.0342 (0.0330)  classification_loss: 1.5838 (1.6334)  loss_mask: 0.0108 (0.0158)  time: 0.1964  data: 0.0002  max mem: 5511
[21:37:23.991594] Epoch: [16]  [360/781]  eta: 0:01:23  lr: 0.000241  training_loss: 1.6874 (1.6829)  mae_loss: 0.0322 (0.0330)  classification_loss: 1.6445 (1.6343)  loss_mask: 0.0133 (0.0157)  time: 0.1971  data: 0.0004  max mem: 5511
[21:37:27.933733] Epoch: [16]  [380/781]  eta: 0:01:19  lr: 0.000241  training_loss: 1.6775 (1.6830)  mae_loss: 0.0351 (0.0331)  classification_loss: 1.6325 (1.6345)  loss_mask: 0.0108 (0.0154)  time: 0.1970  data: 0.0002  max mem: 5511
[21:37:31.884675] Epoch: [16]  [400/781]  eta: 0:01:15  lr: 0.000241  training_loss: 1.6428 (1.6816)  mae_loss: 0.0329 (0.0331)  classification_loss: 1.6041 (1.6332)  loss_mask: 0.0106 (0.0152)  time: 0.1975  data: 0.0002  max mem: 5511
[21:37:35.909997] Epoch: [16]  [420/781]  eta: 0:01:11  lr: 0.000241  training_loss: 1.6800 (1.6821)  mae_loss: 0.0321 (0.0331)  classification_loss: 1.6379 (1.6340)  loss_mask: 0.0104 (0.0150)  time: 0.2012  data: 0.0002  max mem: 5511
[21:37:39.869885] Epoch: [16]  [440/781]  eta: 0:01:07  lr: 0.000241  training_loss: 1.6454 (1.6807)  mae_loss: 0.0324 (0.0331)  classification_loss: 1.6068 (1.6328)  loss_mask: 0.0100 (0.0148)  time: 0.1979  data: 0.0003  max mem: 5511
[21:37:43.813975] Epoch: [16]  [460/781]  eta: 0:01:03  lr: 0.000241  training_loss: 1.6172 (1.6785)  mae_loss: 0.0320 (0.0330)  classification_loss: 1.5628 (1.6304)  loss_mask: 0.0177 (0.0151)  time: 0.1971  data: 0.0002  max mem: 5511
[21:37:47.739376] Epoch: [16]  [480/781]  eta: 0:00:59  lr: 0.000241  training_loss: 1.6946 (1.6791)  mae_loss: 0.0320 (0.0330)  classification_loss: 1.6483 (1.6310)  loss_mask: 0.0132 (0.0150)  time: 0.1962  data: 0.0003  max mem: 5511
[21:37:51.694478] Epoch: [16]  [500/781]  eta: 0:00:55  lr: 0.000241  training_loss: 1.6144 (1.6777)  mae_loss: 0.0327 (0.0331)  classification_loss: 1.5732 (1.6296)  loss_mask: 0.0122 (0.0149)  time: 0.1977  data: 0.0002  max mem: 5511
[21:37:55.608412] Epoch: [16]  [520/781]  eta: 0:00:51  lr: 0.000241  training_loss: 1.6853 (1.6782)  mae_loss: 0.0335 (0.0331)  classification_loss: 1.6433 (1.6302)  loss_mask: 0.0111 (0.0148)  time: 0.1956  data: 0.0002  max mem: 5511
[21:37:59.556679] Epoch: [16]  [540/781]  eta: 0:00:47  lr: 0.000241  training_loss: 1.6514 (1.6773)  mae_loss: 0.0321 (0.0331)  classification_loss: 1.6030 (1.6295)  loss_mask: 0.0122 (0.0147)  time: 0.1973  data: 0.0002  max mem: 5511
[21:38:03.520839] Epoch: [16]  [560/781]  eta: 0:00:43  lr: 0.000241  training_loss: 1.6662 (1.6780)  mae_loss: 0.0316 (0.0331)  classification_loss: 1.6215 (1.6302)  loss_mask: 0.0127 (0.0147)  time: 0.1981  data: 0.0002  max mem: 5511
[21:38:07.474886] Epoch: [16]  [580/781]  eta: 0:00:39  lr: 0.000241  training_loss: 1.7161 (1.6790)  mae_loss: 0.0308 (0.0330)  classification_loss: 1.6715 (1.6313)  loss_mask: 0.0120 (0.0146)  time: 0.1976  data: 0.0002  max mem: 5511
[21:38:11.426128] Epoch: [16]  [600/781]  eta: 0:00:35  lr: 0.000241  training_loss: 1.6661 (1.6792)  mae_loss: 0.0341 (0.0331)  classification_loss: 1.6155 (1.6316)  loss_mask: 0.0096 (0.0146)  time: 0.1975  data: 0.0002  max mem: 5511
[21:38:15.389271] Epoch: [16]  [620/781]  eta: 0:00:31  lr: 0.000241  training_loss: 1.6365 (1.6789)  mae_loss: 0.0345 (0.0331)  classification_loss: 1.5860 (1.6313)  loss_mask: 0.0112 (0.0145)  time: 0.1981  data: 0.0002  max mem: 5511
[21:38:19.316421] Epoch: [16]  [640/781]  eta: 0:00:27  lr: 0.000241  training_loss: 1.6541 (1.6788)  mae_loss: 0.0325 (0.0331)  classification_loss: 1.6160 (1.6313)  loss_mask: 0.0107 (0.0144)  time: 0.1963  data: 0.0002  max mem: 5511
[21:38:23.252042] Epoch: [16]  [660/781]  eta: 0:00:23  lr: 0.000241  training_loss: 1.6927 (1.6789)  mae_loss: 0.0334 (0.0331)  classification_loss: 1.6459 (1.6315)  loss_mask: 0.0128 (0.0144)  time: 0.1967  data: 0.0003  max mem: 5511
[21:38:27.192429] Epoch: [16]  [680/781]  eta: 0:00:20  lr: 0.000241  training_loss: 1.6529 (1.6784)  mae_loss: 0.0339 (0.0331)  classification_loss: 1.6061 (1.6311)  loss_mask: 0.0081 (0.0142)  time: 0.1969  data: 0.0002  max mem: 5511
[21:38:31.142593] Epoch: [16]  [700/781]  eta: 0:00:16  lr: 0.000240  training_loss: 1.6288 (1.6778)  mae_loss: 0.0331 (0.0331)  classification_loss: 1.5828 (1.6305)  loss_mask: 0.0115 (0.0141)  time: 0.1974  data: 0.0002  max mem: 5511
[21:38:35.110873] Epoch: [16]  [720/781]  eta: 0:00:12  lr: 0.000240  training_loss: 1.6399 (1.6774)  mae_loss: 0.0334 (0.0331)  classification_loss: 1.5932 (1.6302)  loss_mask: 0.0131 (0.0141)  time: 0.1983  data: 0.0002  max mem: 5511
[21:38:39.053169] Epoch: [16]  [740/781]  eta: 0:00:08  lr: 0.000240  training_loss: 1.6501 (1.6770)  mae_loss: 0.0309 (0.0331)  classification_loss: 1.6062 (1.6297)  loss_mask: 0.0117 (0.0142)  time: 0.1970  data: 0.0002  max mem: 5511
[21:38:43.040937] Epoch: [16]  [760/781]  eta: 0:00:04  lr: 0.000240  training_loss: 1.6790 (1.6776)  mae_loss: 0.0327 (0.0331)  classification_loss: 1.6398 (1.6303)  loss_mask: 0.0107 (0.0142)  time: 0.1993  data: 0.0002  max mem: 5511
[21:38:46.988238] Epoch: [16]  [780/781]  eta: 0:00:00  lr: 0.000240  training_loss: 1.6716 (1.6775)  mae_loss: 0.0319 (0.0331)  classification_loss: 1.6202 (1.6301)  loss_mask: 0.0167 (0.0143)  time: 0.1973  data: 0.0002  max mem: 5511
[21:38:47.141945] Epoch: [16] Total time: 0:02:34 (0.1984 s / it)
[21:38:47.142422] Averaged stats: lr: 0.000240  training_loss: 1.6716 (1.6775)  mae_loss: 0.0319 (0.0331)  classification_loss: 1.6202 (1.6301)  loss_mask: 0.0167 (0.0143)
[21:38:47.776495] Test:  [  0/157]  eta: 0:01:38  testing_loss: 0.9201 (0.9201)  acc1: 75.0000 (75.0000)  acc5: 95.3125 (95.3125)  time: 0.6284  data: 0.5983  max mem: 5511
[21:38:48.067785] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.9932 (1.0093)  acc1: 62.5000 (63.7784)  acc5: 98.4375 (98.1534)  time: 0.0833  data: 0.0546  max mem: 5511
[21:38:48.350608] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.9360 (0.9582)  acc1: 67.1875 (66.6667)  acc5: 98.4375 (98.1399)  time: 0.0285  data: 0.0002  max mem: 5511
[21:38:48.635546] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.9442 (0.9715)  acc1: 68.7500 (66.6331)  acc5: 96.8750 (97.5302)  time: 0.0283  data: 0.0002  max mem: 5511
[21:38:48.917754] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.9576 (0.9752)  acc1: 65.6250 (65.9299)  acc5: 96.8750 (97.4466)  time: 0.0282  data: 0.0002  max mem: 5511
[21:38:49.200783] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.9543 (0.9692)  acc1: 65.6250 (66.1765)  acc5: 96.8750 (97.3652)  time: 0.0281  data: 0.0002  max mem: 5511
[21:38:49.485632] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.9337 (0.9671)  acc1: 65.6250 (66.2398)  acc5: 96.8750 (97.4641)  time: 0.0283  data: 0.0002  max mem: 5511
[21:38:49.773079] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.9009 (0.9544)  acc1: 67.1875 (66.8574)  acc5: 98.4375 (97.5352)  time: 0.0285  data: 0.0002  max mem: 5511
[21:38:50.065389] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.8957 (0.9598)  acc1: 68.7500 (66.7631)  acc5: 96.8750 (97.3765)  time: 0.0288  data: 0.0002  max mem: 5511
[21:38:50.356459] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.9535 (0.9627)  acc1: 67.1875 (66.7067)  acc5: 98.4375 (97.4931)  time: 0.0289  data: 0.0002  max mem: 5511
[21:38:50.649025] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.9950 (0.9666)  acc1: 67.1875 (66.6925)  acc5: 98.4375 (97.5402)  time: 0.0290  data: 0.0002  max mem: 5511
[21:38:50.934659] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.0159 (0.9655)  acc1: 67.1875 (66.7934)  acc5: 98.4375 (97.5366)  time: 0.0287  data: 0.0002  max mem: 5511
[21:38:51.219082] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.9384 (0.9606)  acc1: 67.1875 (67.0067)  acc5: 98.4375 (97.5852)  time: 0.0283  data: 0.0002  max mem: 5511
[21:38:51.505292] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.9733 (0.9656)  acc1: 62.5000 (66.6388)  acc5: 98.4375 (97.5787)  time: 0.0284  data: 0.0002  max mem: 5511
[21:38:51.792034] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.9909 (0.9644)  acc1: 65.6250 (66.8218)  acc5: 96.8750 (97.5177)  time: 0.0285  data: 0.0002  max mem: 5511
[21:38:52.073687] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.9557 (0.9618)  acc1: 67.1875 (66.9081)  acc5: 95.3125 (97.4131)  time: 0.0283  data: 0.0001  max mem: 5511
[21:38:52.225244] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.9285 (0.9649)  acc1: 67.1875 (66.7900)  acc5: 95.3125 (97.3900)  time: 0.0271  data: 0.0001  max mem: 5511
[21:38:52.408448] Test: Total time: 0:00:05 (0.0335 s / it)
[21:38:52.409559] * Acc@1 66.790 Acc@5 97.390 loss 0.965
[21:38:52.409858] Accuracy of the network on the 10000 test images: 66.8%
[21:38:52.410061] Max accuracy: 67.19%
[21:38:52.592363] log_dir: ./output_dir
[21:38:53.476260] Epoch: [17]  [  0/781]  eta: 0:11:29  lr: 0.000240  training_loss: 1.4823 (1.4823)  mae_loss: 0.0270 (0.0270)  classification_loss: 1.4422 (1.4422)  loss_mask: 0.0131 (0.0131)  time: 0.8822  data: 0.6756  max mem: 5511
[21:38:57.421493] Epoch: [17]  [ 20/781]  eta: 0:02:54  lr: 0.000240  training_loss: 1.6388 (1.6589)  mae_loss: 0.0327 (0.0321)  classification_loss: 1.5898 (1.6071)  loss_mask: 0.0199 (0.0198)  time: 0.1972  data: 0.0001  max mem: 5511
[21:39:01.346867] Epoch: [17]  [ 40/781]  eta: 0:02:38  lr: 0.000240  training_loss: 1.7004 (1.6809)  mae_loss: 0.0339 (0.0330)  classification_loss: 1.6489 (1.6281)  loss_mask: 0.0180 (0.0198)  time: 0.1962  data: 0.0004  max mem: 5511
[21:39:05.273371] Epoch: [17]  [ 60/781]  eta: 0:02:29  lr: 0.000240  training_loss: 1.6622 (1.6835)  mae_loss: 0.0312 (0.0329)  classification_loss: 1.6227 (1.6313)  loss_mask: 0.0155 (0.0193)  time: 0.1962  data: 0.0002  max mem: 5511
[21:39:09.224509] Epoch: [17]  [ 80/781]  eta: 0:02:23  lr: 0.000240  training_loss: 1.6715 (1.6820)  mae_loss: 0.0307 (0.0327)  classification_loss: 1.6197 (1.6319)  loss_mask: 0.0105 (0.0174)  time: 0.1975  data: 0.0002  max mem: 5511
[21:39:13.177990] Epoch: [17]  [100/781]  eta: 0:02:18  lr: 0.000240  training_loss: 1.6472 (1.6771)  mae_loss: 0.0314 (0.0324)  classification_loss: 1.6076 (1.6286)  loss_mask: 0.0098 (0.0161)  time: 0.1976  data: 0.0002  max mem: 5511
[21:39:17.095994] Epoch: [17]  [120/781]  eta: 0:02:13  lr: 0.000240  training_loss: 1.6061 (1.6711)  mae_loss: 0.0311 (0.0324)  classification_loss: 1.5682 (1.6235)  loss_mask: 0.0110 (0.0152)  time: 0.1958  data: 0.0003  max mem: 5511
[21:39:21.024684] Epoch: [17]  [140/781]  eta: 0:02:09  lr: 0.000240  training_loss: 1.6265 (1.6641)  mae_loss: 0.0318 (0.0322)  classification_loss: 1.5887 (1.6169)  loss_mask: 0.0131 (0.0149)  time: 0.1964  data: 0.0002  max mem: 5511
[21:39:24.964365] Epoch: [17]  [160/781]  eta: 0:02:04  lr: 0.000240  training_loss: 1.6665 (1.6659)  mae_loss: 0.0312 (0.0322)  classification_loss: 1.6242 (1.6190)  loss_mask: 0.0117 (0.0148)  time: 0.1969  data: 0.0002  max mem: 5511
[21:39:28.923947] Epoch: [17]  [180/781]  eta: 0:02:00  lr: 0.000240  training_loss: 1.6348 (1.6631)  mae_loss: 0.0325 (0.0322)  classification_loss: 1.5898 (1.6162)  loss_mask: 0.0125 (0.0146)  time: 0.1979  data: 0.0002  max mem: 5511
[21:39:32.879206] Epoch: [17]  [200/781]  eta: 0:01:56  lr: 0.000240  training_loss: 1.6355 (1.6633)  mae_loss: 0.0334 (0.0323)  classification_loss: 1.5799 (1.6161)  loss_mask: 0.0141 (0.0148)  time: 0.1977  data: 0.0002  max mem: 5511
[21:39:36.866551] Epoch: [17]  [220/781]  eta: 0:01:52  lr: 0.000240  training_loss: 1.6153 (1.6640)  mae_loss: 0.0329 (0.0323)  classification_loss: 1.5657 (1.6163)  loss_mask: 0.0200 (0.0153)  time: 0.1993  data: 0.0002  max mem: 5511
[21:39:40.818137] Epoch: [17]  [240/781]  eta: 0:01:48  lr: 0.000240  training_loss: 1.6693 (1.6670)  mae_loss: 0.0346 (0.0326)  classification_loss: 1.6253 (1.6193)  loss_mask: 0.0126 (0.0152)  time: 0.1975  data: 0.0002  max mem: 5511
[21:39:44.758745] Epoch: [17]  [260/781]  eta: 0:01:44  lr: 0.000240  training_loss: 1.6680 (1.6648)  mae_loss: 0.0327 (0.0326)  classification_loss: 1.6110 (1.6172)  loss_mask: 0.0129 (0.0151)  time: 0.1970  data: 0.0002  max mem: 5511
[21:39:48.690132] Epoch: [17]  [280/781]  eta: 0:01:39  lr: 0.000240  training_loss: 1.6612 (1.6669)  mae_loss: 0.0328 (0.0326)  classification_loss: 1.5950 (1.6189)  loss_mask: 0.0151 (0.0153)  time: 0.1965  data: 0.0002  max mem: 5511
[21:39:52.593255] Epoch: [17]  [300/781]  eta: 0:01:35  lr: 0.000240  training_loss: 1.6817 (1.6669)  mae_loss: 0.0300 (0.0325)  classification_loss: 1.6421 (1.6190)  loss_mask: 0.0142 (0.0154)  time: 0.1951  data: 0.0002  max mem: 5511
[21:39:56.538756] Epoch: [17]  [320/781]  eta: 0:01:31  lr: 0.000240  training_loss: 1.6270 (1.6665)  mae_loss: 0.0316 (0.0324)  classification_loss: 1.5825 (1.6185)  loss_mask: 0.0176 (0.0156)  time: 0.1972  data: 0.0005  max mem: 5511
[21:40:00.486419] Epoch: [17]  [340/781]  eta: 0:01:27  lr: 0.000240  training_loss: 1.6725 (1.6669)  mae_loss: 0.0319 (0.0325)  classification_loss: 1.6128 (1.6190)  loss_mask: 0.0108 (0.0154)  time: 0.1973  data: 0.0003  max mem: 5511
[21:40:04.426734] Epoch: [17]  [360/781]  eta: 0:01:23  lr: 0.000240  training_loss: 1.6664 (1.6685)  mae_loss: 0.0308 (0.0324)  classification_loss: 1.6205 (1.6208)  loss_mask: 0.0136 (0.0153)  time: 0.1969  data: 0.0002  max mem: 5511
[21:40:08.347237] Epoch: [17]  [380/781]  eta: 0:01:19  lr: 0.000240  training_loss: 1.6454 (1.6687)  mae_loss: 0.0327 (0.0324)  classification_loss: 1.6052 (1.6211)  loss_mask: 0.0109 (0.0152)  time: 0.1959  data: 0.0002  max mem: 5511
[21:40:12.291172] Epoch: [17]  [400/781]  eta: 0:01:15  lr: 0.000239  training_loss: 1.6422 (1.6678)  mae_loss: 0.0318 (0.0324)  classification_loss: 1.5894 (1.6203)  loss_mask: 0.0133 (0.0151)  time: 0.1971  data: 0.0002  max mem: 5511
[21:40:16.215069] Epoch: [17]  [420/781]  eta: 0:01:11  lr: 0.000239  training_loss: 1.6834 (1.6687)  mae_loss: 0.0331 (0.0324)  classification_loss: 1.6322 (1.6212)  loss_mask: 0.0126 (0.0151)  time: 0.1961  data: 0.0002  max mem: 5511
[21:40:20.138773] Epoch: [17]  [440/781]  eta: 0:01:07  lr: 0.000239  training_loss: 1.7087 (1.6681)  mae_loss: 0.0303 (0.0324)  classification_loss: 1.6629 (1.6207)  loss_mask: 0.0138 (0.0150)  time: 0.1961  data: 0.0002  max mem: 5511
[21:40:24.121722] Epoch: [17]  [460/781]  eta: 0:01:03  lr: 0.000239  training_loss: 1.6349 (1.6668)  mae_loss: 0.0307 (0.0324)  classification_loss: 1.5851 (1.6194)  loss_mask: 0.0121 (0.0150)  time: 0.1991  data: 0.0003  max mem: 5511
[21:40:28.082793] Epoch: [17]  [480/781]  eta: 0:00:59  lr: 0.000239  training_loss: 1.6190 (1.6654)  mae_loss: 0.0300 (0.0323)  classification_loss: 1.5704 (1.6180)  loss_mask: 0.0173 (0.0151)  time: 0.1980  data: 0.0002  max mem: 5511
[21:40:32.025266] Epoch: [17]  [500/781]  eta: 0:00:55  lr: 0.000239  training_loss: 1.6465 (1.6653)  mae_loss: 0.0319 (0.0323)  classification_loss: 1.5870 (1.6179)  loss_mask: 0.0140 (0.0151)  time: 0.1970  data: 0.0003  max mem: 5511
[21:40:35.977602] Epoch: [17]  [520/781]  eta: 0:00:51  lr: 0.000239  training_loss: 1.6168 (1.6646)  mae_loss: 0.0316 (0.0323)  classification_loss: 1.5746 (1.6173)  loss_mask: 0.0114 (0.0150)  time: 0.1975  data: 0.0002  max mem: 5511
[21:40:39.945361] Epoch: [17]  [540/781]  eta: 0:00:47  lr: 0.000239  training_loss: 1.6328 (1.6641)  mae_loss: 0.0326 (0.0323)  classification_loss: 1.5850 (1.6169)  loss_mask: 0.0101 (0.0149)  time: 0.1983  data: 0.0002  max mem: 5511
[21:40:43.893046] Epoch: [17]  [560/781]  eta: 0:00:43  lr: 0.000239  training_loss: 1.6755 (1.6647)  mae_loss: 0.0314 (0.0323)  classification_loss: 1.6253 (1.6174)  loss_mask: 0.0163 (0.0150)  time: 0.1973  data: 0.0003  max mem: 5511
[21:40:47.849072] Epoch: [17]  [580/781]  eta: 0:00:39  lr: 0.000239  training_loss: 1.6603 (1.6648)  mae_loss: 0.0301 (0.0322)  classification_loss: 1.6161 (1.6176)  loss_mask: 0.0113 (0.0150)  time: 0.1977  data: 0.0002  max mem: 5511
[21:40:51.766137] Epoch: [17]  [600/781]  eta: 0:00:35  lr: 0.000239  training_loss: 1.6206 (1.6628)  mae_loss: 0.0318 (0.0322)  classification_loss: 1.5804 (1.6157)  loss_mask: 0.0085 (0.0149)  time: 0.1958  data: 0.0003  max mem: 5511
[21:40:55.684310] Epoch: [17]  [620/781]  eta: 0:00:31  lr: 0.000239  training_loss: 1.6201 (1.6613)  mae_loss: 0.0318 (0.0322)  classification_loss: 1.5724 (1.6143)  loss_mask: 0.0097 (0.0148)  time: 0.1958  data: 0.0002  max mem: 5511
[21:40:59.627518] Epoch: [17]  [640/781]  eta: 0:00:27  lr: 0.000239  training_loss: 1.6282 (1.6611)  mae_loss: 0.0321 (0.0322)  classification_loss: 1.5845 (1.6142)  loss_mask: 0.0095 (0.0147)  time: 0.1970  data: 0.0002  max mem: 5511
[21:41:03.580252] Epoch: [17]  [660/781]  eta: 0:00:23  lr: 0.000239  training_loss: 1.6696 (1.6621)  mae_loss: 0.0333 (0.0323)  classification_loss: 1.6189 (1.6152)  loss_mask: 0.0111 (0.0146)  time: 0.1975  data: 0.0002  max mem: 5511
[21:41:07.495942] Epoch: [17]  [680/781]  eta: 0:00:19  lr: 0.000239  training_loss: 1.6709 (1.6624)  mae_loss: 0.0328 (0.0323)  classification_loss: 1.6254 (1.6157)  loss_mask: 0.0107 (0.0145)  time: 0.1957  data: 0.0002  max mem: 5511
[21:41:11.417106] Epoch: [17]  [700/781]  eta: 0:00:16  lr: 0.000239  training_loss: 1.6758 (1.6624)  mae_loss: 0.0312 (0.0323)  classification_loss: 1.6350 (1.6158)  loss_mask: 0.0095 (0.0144)  time: 0.1960  data: 0.0002  max mem: 5511
[21:41:15.346812] Epoch: [17]  [720/781]  eta: 0:00:12  lr: 0.000239  training_loss: 1.6178 (1.6619)  mae_loss: 0.0321 (0.0322)  classification_loss: 1.5755 (1.6153)  loss_mask: 0.0102 (0.0143)  time: 0.1964  data: 0.0002  max mem: 5511
[21:41:19.265237] Epoch: [17]  [740/781]  eta: 0:00:08  lr: 0.000239  training_loss: 1.6051 (1.6605)  mae_loss: 0.0314 (0.0322)  classification_loss: 1.5693 (1.6141)  loss_mask: 0.0092 (0.0142)  time: 0.1958  data: 0.0002  max mem: 5511
[21:41:23.185911] Epoch: [17]  [760/781]  eta: 0:00:04  lr: 0.000239  training_loss: 1.6260 (1.6603)  mae_loss: 0.0317 (0.0322)  classification_loss: 1.5800 (1.6139)  loss_mask: 0.0133 (0.0142)  time: 0.1960  data: 0.0002  max mem: 5511
[21:41:27.080494] Epoch: [17]  [780/781]  eta: 0:00:00  lr: 0.000239  training_loss: 1.6732 (1.6607)  mae_loss: 0.0316 (0.0322)  classification_loss: 1.6249 (1.6143)  loss_mask: 0.0109 (0.0142)  time: 0.1946  data: 0.0001  max mem: 5511
[21:41:27.234435] Epoch: [17] Total time: 0:02:34 (0.1980 s / it)
[21:41:27.234910] Averaged stats: lr: 0.000239  training_loss: 1.6732 (1.6607)  mae_loss: 0.0316 (0.0322)  classification_loss: 1.6249 (1.6143)  loss_mask: 0.0109 (0.0142)
[21:41:27.913445] Test:  [  0/157]  eta: 0:01:45  testing_loss: 0.9529 (0.9529)  acc1: 67.1875 (67.1875)  acc5: 95.3125 (95.3125)  time: 0.6728  data: 0.6405  max mem: 5511
[21:41:28.205556] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.9860 (1.0019)  acc1: 65.6250 (66.4773)  acc5: 98.4375 (97.3011)  time: 0.0875  data: 0.0587  max mem: 5511
[21:41:28.501163] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.9487 (0.9526)  acc1: 67.1875 (68.3036)  acc5: 98.4375 (97.9167)  time: 0.0292  data: 0.0005  max mem: 5511
[21:41:28.787979] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.9702 (0.9621)  acc1: 68.7500 (68.6996)  acc5: 98.4375 (97.4798)  time: 0.0289  data: 0.0004  max mem: 5511
[21:41:29.074298] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.9723 (0.9580)  acc1: 68.7500 (68.6738)  acc5: 96.8750 (97.6372)  time: 0.0285  data: 0.0004  max mem: 5511
[21:41:29.358506] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.9260 (0.9531)  acc1: 68.7500 (68.7500)  acc5: 96.8750 (97.5184)  time: 0.0284  data: 0.0003  max mem: 5511
[21:41:29.641491] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.9452 (0.9548)  acc1: 67.1875 (68.4170)  acc5: 96.8750 (97.2848)  time: 0.0282  data: 0.0002  max mem: 5511
[21:41:29.926794] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.9008 (0.9419)  acc1: 68.7500 (68.9701)  acc5: 96.8750 (97.4692)  time: 0.0283  data: 0.0002  max mem: 5511
[21:41:30.210903] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.9035 (0.9468)  acc1: 68.7500 (68.8079)  acc5: 98.4375 (97.3765)  time: 0.0283  data: 0.0002  max mem: 5511
[21:41:30.497229] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.9400 (0.9491)  acc1: 68.7500 (68.6641)  acc5: 98.4375 (97.4416)  time: 0.0284  data: 0.0002  max mem: 5511
[21:41:30.785561] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.9516 (0.9537)  acc1: 67.1875 (68.4251)  acc5: 98.4375 (97.4319)  time: 0.0286  data: 0.0002  max mem: 5511
[21:41:31.077184] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.9750 (0.9548)  acc1: 67.1875 (68.4403)  acc5: 98.4375 (97.4521)  time: 0.0289  data: 0.0004  max mem: 5511
[21:41:31.372165] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.9334 (0.9502)  acc1: 68.7500 (68.5305)  acc5: 98.4375 (97.5077)  time: 0.0292  data: 0.0004  max mem: 5511
[21:41:31.657537] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.9304 (0.9533)  acc1: 67.1875 (68.3087)  acc5: 98.4375 (97.4833)  time: 0.0289  data: 0.0002  max mem: 5511
[21:41:31.940967] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.9402 (0.9521)  acc1: 68.7500 (68.5062)  acc5: 96.8750 (97.4623)  time: 0.0283  data: 0.0002  max mem: 5511
[21:41:32.221375] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.9322 (0.9488)  acc1: 70.3125 (68.7293)  acc5: 98.4375 (97.5062)  time: 0.0281  data: 0.0001  max mem: 5511
[21:41:32.373856] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.9289 (0.9511)  acc1: 70.3125 (68.5900)  acc5: 98.4375 (97.5300)  time: 0.0272  data: 0.0001  max mem: 5511
[21:41:32.524616] Test: Total time: 0:00:05 (0.0337 s / it)
[21:41:32.525214] * Acc@1 68.590 Acc@5 97.530 loss 0.951
[21:41:32.525553] Accuracy of the network on the 10000 test images: 68.6%
[21:41:32.525787] Max accuracy: 68.59%
[21:41:32.781122] log_dir: ./output_dir
[21:41:33.646778] Epoch: [18]  [  0/781]  eta: 0:11:14  lr: 0.000239  training_loss: 1.4838 (1.4838)  mae_loss: 0.0259 (0.0259)  classification_loss: 1.4477 (1.4477)  loss_mask: 0.0103 (0.0103)  time: 0.8636  data: 0.6289  max mem: 5511
[21:41:37.576833] Epoch: [18]  [ 20/781]  eta: 0:02:53  lr: 0.000239  training_loss: 1.5759 (1.5929)  mae_loss: 0.0316 (0.0315)  classification_loss: 1.5319 (1.5496)  loss_mask: 0.0117 (0.0117)  time: 0.1964  data: 0.0001  max mem: 5511
[21:41:41.526093] Epoch: [18]  [ 40/781]  eta: 0:02:37  lr: 0.000239  training_loss: 1.6524 (1.6269)  mae_loss: 0.0331 (0.0324)  classification_loss: 1.6088 (1.5814)  loss_mask: 0.0124 (0.0132)  time: 0.1974  data: 0.0002  max mem: 5511
[21:41:45.464486] Epoch: [18]  [ 60/781]  eta: 0:02:29  lr: 0.000239  training_loss: 1.6471 (1.6397)  mae_loss: 0.0326 (0.0326)  classification_loss: 1.6042 (1.5935)  loss_mask: 0.0127 (0.0136)  time: 0.1968  data: 0.0002  max mem: 5511
[21:41:49.373119] Epoch: [18]  [ 80/781]  eta: 0:02:23  lr: 0.000238  training_loss: 1.6733 (1.6410)  mae_loss: 0.0328 (0.0327)  classification_loss: 1.6205 (1.5941)  loss_mask: 0.0149 (0.0142)  time: 0.1953  data: 0.0003  max mem: 5511
[21:41:53.322239] Epoch: [18]  [100/781]  eta: 0:02:18  lr: 0.000238  training_loss: 1.6372 (1.6405)  mae_loss: 0.0328 (0.0329)  classification_loss: 1.5863 (1.5933)  loss_mask: 0.0130 (0.0142)  time: 0.1974  data: 0.0002  max mem: 5511
[21:41:57.270422] Epoch: [18]  [120/781]  eta: 0:02:13  lr: 0.000238  training_loss: 1.5798 (1.6385)  mae_loss: 0.0321 (0.0327)  classification_loss: 1.5378 (1.5918)  loss_mask: 0.0121 (0.0139)  time: 0.1973  data: 0.0002  max mem: 5511
[21:42:01.241496] Epoch: [18]  [140/781]  eta: 0:02:09  lr: 0.000238  training_loss: 1.6200 (1.6390)  mae_loss: 0.0318 (0.0327)  classification_loss: 1.5719 (1.5926)  loss_mask: 0.0116 (0.0137)  time: 0.1984  data: 0.0002  max mem: 5511
[21:42:05.167696] Epoch: [18]  [160/781]  eta: 0:02:04  lr: 0.000238  training_loss: 1.7021 (1.6439)  mae_loss: 0.0310 (0.0326)  classification_loss: 1.6499 (1.5981)  loss_mask: 0.0084 (0.0132)  time: 0.1962  data: 0.0002  max mem: 5511
[21:42:09.113045] Epoch: [18]  [180/781]  eta: 0:02:00  lr: 0.000238  training_loss: 1.6185 (1.6423)  mae_loss: 0.0324 (0.0326)  classification_loss: 1.5677 (1.5966)  loss_mask: 0.0119 (0.0131)  time: 0.1972  data: 0.0003  max mem: 5511
[21:42:13.050065] Epoch: [18]  [200/781]  eta: 0:01:56  lr: 0.000238  training_loss: 1.6599 (1.6454)  mae_loss: 0.0336 (0.0326)  classification_loss: 1.6080 (1.5996)  loss_mask: 0.0130 (0.0132)  time: 0.1968  data: 0.0002  max mem: 5511
[21:42:16.998783] Epoch: [18]  [220/781]  eta: 0:01:52  lr: 0.000238  training_loss: 1.6630 (1.6497)  mae_loss: 0.0308 (0.0325)  classification_loss: 1.6098 (1.6038)  loss_mask: 0.0125 (0.0134)  time: 0.1973  data: 0.0002  max mem: 5511
[21:42:20.991220] Epoch: [18]  [240/781]  eta: 0:01:48  lr: 0.000238  training_loss: 1.6464 (1.6492)  mae_loss: 0.0314 (0.0325)  classification_loss: 1.6059 (1.6034)  loss_mask: 0.0109 (0.0133)  time: 0.1995  data: 0.0002  max mem: 5511
[21:42:24.894115] Epoch: [18]  [260/781]  eta: 0:01:43  lr: 0.000238  training_loss: 1.6234 (1.6473)  mae_loss: 0.0308 (0.0324)  classification_loss: 1.5889 (1.6018)  loss_mask: 0.0100 (0.0131)  time: 0.1951  data: 0.0002  max mem: 5511
[21:42:28.854538] Epoch: [18]  [280/781]  eta: 0:01:39  lr: 0.000238  training_loss: 1.6245 (1.6476)  mae_loss: 0.0335 (0.0324)  classification_loss: 1.5860 (1.6023)  loss_mask: 0.0086 (0.0129)  time: 0.1979  data: 0.0002  max mem: 5511
[21:42:32.845763] Epoch: [18]  [300/781]  eta: 0:01:35  lr: 0.000238  training_loss: 1.6515 (1.6481)  mae_loss: 0.0325 (0.0324)  classification_loss: 1.6145 (1.6030)  loss_mask: 0.0102 (0.0127)  time: 0.1995  data: 0.0002  max mem: 5511
[21:42:36.774764] Epoch: [18]  [320/781]  eta: 0:01:31  lr: 0.000238  training_loss: 1.6380 (1.6475)  mae_loss: 0.0321 (0.0325)  classification_loss: 1.5935 (1.6023)  loss_mask: 0.0124 (0.0127)  time: 0.1964  data: 0.0002  max mem: 5511
[21:42:40.729968] Epoch: [18]  [340/781]  eta: 0:01:27  lr: 0.000238  training_loss: 1.6092 (1.6459)  mae_loss: 0.0340 (0.0325)  classification_loss: 1.5561 (1.6009)  loss_mask: 0.0099 (0.0125)  time: 0.1977  data: 0.0002  max mem: 5511
[21:42:44.678983] Epoch: [18]  [360/781]  eta: 0:01:23  lr: 0.000238  training_loss: 1.6821 (1.6470)  mae_loss: 0.0320 (0.0325)  classification_loss: 1.6385 (1.6019)  loss_mask: 0.0121 (0.0126)  time: 0.1973  data: 0.0002  max mem: 5511
[21:42:48.626222] Epoch: [18]  [380/781]  eta: 0:01:19  lr: 0.000238  training_loss: 1.6376 (1.6474)  mae_loss: 0.0321 (0.0326)  classification_loss: 1.5937 (1.6021)  loss_mask: 0.0126 (0.0127)  time: 0.1973  data: 0.0002  max mem: 5511
[21:42:52.547041] Epoch: [18]  [400/781]  eta: 0:01:15  lr: 0.000238  training_loss: 1.6554 (1.6479)  mae_loss: 0.0337 (0.0326)  classification_loss: 1.6143 (1.6026)  loss_mask: 0.0118 (0.0126)  time: 0.1959  data: 0.0002  max mem: 5511
[21:42:56.467464] Epoch: [18]  [420/781]  eta: 0:01:11  lr: 0.000238  training_loss: 1.6043 (1.6464)  mae_loss: 0.0317 (0.0326)  classification_loss: 1.5626 (1.6012)  loss_mask: 0.0117 (0.0126)  time: 0.1959  data: 0.0002  max mem: 5511
[21:43:00.426710] Epoch: [18]  [440/781]  eta: 0:01:07  lr: 0.000238  training_loss: 1.6135 (1.6460)  mae_loss: 0.0326 (0.0326)  classification_loss: 1.5718 (1.6009)  loss_mask: 0.0098 (0.0125)  time: 0.1979  data: 0.0002  max mem: 5511
[21:43:04.362385] Epoch: [18]  [460/781]  eta: 0:01:03  lr: 0.000238  training_loss: 1.5714 (1.6445)  mae_loss: 0.0296 (0.0325)  classification_loss: 1.5289 (1.5996)  loss_mask: 0.0089 (0.0124)  time: 0.1966  data: 0.0002  max mem: 5511
[21:43:08.288285] Epoch: [18]  [480/781]  eta: 0:00:59  lr: 0.000238  training_loss: 1.6312 (1.6443)  mae_loss: 0.0320 (0.0325)  classification_loss: 1.5875 (1.5995)  loss_mask: 0.0092 (0.0123)  time: 0.1962  data: 0.0002  max mem: 5511
[21:43:12.246361] Epoch: [18]  [500/781]  eta: 0:00:55  lr: 0.000238  training_loss: 1.6710 (1.6453)  mae_loss: 0.0315 (0.0325)  classification_loss: 1.6214 (1.6005)  loss_mask: 0.0095 (0.0123)  time: 0.1978  data: 0.0004  max mem: 5511
[21:43:16.173284] Epoch: [18]  [520/781]  eta: 0:00:51  lr: 0.000238  training_loss: 1.6121 (1.6442)  mae_loss: 0.0318 (0.0325)  classification_loss: 1.5719 (1.5995)  loss_mask: 0.0101 (0.0123)  time: 0.1963  data: 0.0003  max mem: 5511
[21:43:20.089786] Epoch: [18]  [540/781]  eta: 0:00:47  lr: 0.000237  training_loss: 1.6255 (1.6441)  mae_loss: 0.0309 (0.0324)  classification_loss: 1.5847 (1.5994)  loss_mask: 0.0097 (0.0123)  time: 0.1957  data: 0.0002  max mem: 5511
[21:43:23.995070] Epoch: [18]  [560/781]  eta: 0:00:43  lr: 0.000237  training_loss: 1.6647 (1.6447)  mae_loss: 0.0327 (0.0325)  classification_loss: 1.6223 (1.6000)  loss_mask: 0.0097 (0.0122)  time: 0.1952  data: 0.0003  max mem: 5511
[21:43:27.916048] Epoch: [18]  [580/781]  eta: 0:00:39  lr: 0.000237  training_loss: 1.5881 (1.6442)  mae_loss: 0.0312 (0.0325)  classification_loss: 1.5482 (1.5996)  loss_mask: 0.0093 (0.0121)  time: 0.1960  data: 0.0004  max mem: 5511
[21:43:31.837570] Epoch: [18]  [600/781]  eta: 0:00:35  lr: 0.000237  training_loss: 1.6270 (1.6438)  mae_loss: 0.0315 (0.0324)  classification_loss: 1.5891 (1.5994)  loss_mask: 0.0080 (0.0120)  time: 0.1960  data: 0.0002  max mem: 5511
[21:43:35.807046] Epoch: [18]  [620/781]  eta: 0:00:31  lr: 0.000237  training_loss: 1.6770 (1.6442)  mae_loss: 0.0315 (0.0324)  classification_loss: 1.6216 (1.5997)  loss_mask: 0.0157 (0.0121)  time: 0.1984  data: 0.0002  max mem: 5511
[21:43:39.749892] Epoch: [18]  [640/781]  eta: 0:00:27  lr: 0.000237  training_loss: 1.6519 (1.6439)  mae_loss: 0.0317 (0.0324)  classification_loss: 1.6099 (1.5993)  loss_mask: 0.0146 (0.0122)  time: 0.1971  data: 0.0002  max mem: 5511
[21:43:43.679627] Epoch: [18]  [660/781]  eta: 0:00:23  lr: 0.000237  training_loss: 1.6333 (1.6437)  mae_loss: 0.0320 (0.0324)  classification_loss: 1.5865 (1.5990)  loss_mask: 0.0155 (0.0123)  time: 0.1964  data: 0.0002  max mem: 5511
[21:43:47.611252] Epoch: [18]  [680/781]  eta: 0:00:19  lr: 0.000237  training_loss: 1.6327 (1.6443)  mae_loss: 0.0320 (0.0324)  classification_loss: 1.5857 (1.5996)  loss_mask: 0.0128 (0.0124)  time: 0.1965  data: 0.0002  max mem: 5511
[21:43:51.569515] Epoch: [18]  [700/781]  eta: 0:00:16  lr: 0.000237  training_loss: 1.5879 (1.6436)  mae_loss: 0.0329 (0.0324)  classification_loss: 1.5407 (1.5987)  loss_mask: 0.0141 (0.0125)  time: 0.1978  data: 0.0002  max mem: 5511
[21:43:55.520791] Epoch: [18]  [720/781]  eta: 0:00:12  lr: 0.000237  training_loss: 1.6220 (1.6434)  mae_loss: 0.0315 (0.0324)  classification_loss: 1.5844 (1.5986)  loss_mask: 0.0106 (0.0125)  time: 0.1975  data: 0.0002  max mem: 5511
[21:43:59.458216] Epoch: [18]  [740/781]  eta: 0:00:08  lr: 0.000237  training_loss: 1.6354 (1.6426)  mae_loss: 0.0309 (0.0323)  classification_loss: 1.5962 (1.5979)  loss_mask: 0.0091 (0.0124)  time: 0.1967  data: 0.0002  max mem: 5511
[21:44:03.430960] Epoch: [18]  [760/781]  eta: 0:00:04  lr: 0.000237  training_loss: 1.6399 (1.6425)  mae_loss: 0.0316 (0.0323)  classification_loss: 1.5980 (1.5978)  loss_mask: 0.0114 (0.0124)  time: 0.1986  data: 0.0002  max mem: 5511
[21:44:07.353405] Epoch: [18]  [780/781]  eta: 0:00:00  lr: 0.000237  training_loss: 1.5847 (1.6419)  mae_loss: 0.0305 (0.0323)  classification_loss: 1.5304 (1.5973)  loss_mask: 0.0084 (0.0124)  time: 0.1960  data: 0.0002  max mem: 5511
[21:44:07.505573] Epoch: [18] Total time: 0:02:34 (0.1981 s / it)
[21:44:07.507037] Averaged stats: lr: 0.000237  training_loss: 1.5847 (1.6419)  mae_loss: 0.0305 (0.0323)  classification_loss: 1.5304 (1.5973)  loss_mask: 0.0084 (0.0124)
[21:44:08.138348] Test:  [  0/157]  eta: 0:01:38  testing_loss: 0.9345 (0.9345)  acc1: 70.3125 (70.3125)  acc5: 95.3125 (95.3125)  time: 0.6267  data: 0.5962  max mem: 5511
[21:44:08.429129] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.9773 (0.9947)  acc1: 68.7500 (67.0455)  acc5: 98.4375 (98.0114)  time: 0.0832  data: 0.0545  max mem: 5511
[21:44:08.714060] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.9259 (0.9498)  acc1: 68.7500 (67.9315)  acc5: 98.4375 (97.6935)  time: 0.0286  data: 0.0002  max mem: 5511
[21:44:09.002810] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.9259 (0.9530)  acc1: 68.7500 (68.2964)  acc5: 96.8750 (97.3286)  time: 0.0285  data: 0.0002  max mem: 5511
[21:44:09.289502] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.9241 (0.9526)  acc1: 68.7500 (68.0640)  acc5: 96.8750 (97.1418)  time: 0.0285  data: 0.0002  max mem: 5511
[21:44:09.572954] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.9570 (0.9497)  acc1: 68.7500 (68.1373)  acc5: 96.8750 (97.0895)  time: 0.0283  data: 0.0002  max mem: 5511
[21:44:09.856298] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.9520 (0.9478)  acc1: 67.1875 (68.1096)  acc5: 96.8750 (97.0287)  time: 0.0282  data: 0.0002  max mem: 5511
[21:44:10.140222] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.8926 (0.9396)  acc1: 68.7500 (68.5079)  acc5: 96.8750 (97.0731)  time: 0.0282  data: 0.0002  max mem: 5511
[21:44:10.424030] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.8797 (0.9459)  acc1: 67.1875 (68.2099)  acc5: 96.8750 (97.0293)  time: 0.0282  data: 0.0002  max mem: 5511
[21:44:10.714453] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.9674 (0.9474)  acc1: 67.1875 (68.1662)  acc5: 98.4375 (97.2012)  time: 0.0285  data: 0.0002  max mem: 5511
[21:44:11.005108] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.9674 (0.9510)  acc1: 67.1875 (68.0693)  acc5: 98.4375 (97.2153)  time: 0.0289  data: 0.0005  max mem: 5511
[21:44:11.290211] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.9769 (0.9526)  acc1: 67.1875 (68.0602)  acc5: 96.8750 (97.1988)  time: 0.0287  data: 0.0005  max mem: 5511
[21:44:11.573773] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.9461 (0.9466)  acc1: 70.3125 (68.4143)  acc5: 96.8750 (97.1978)  time: 0.0283  data: 0.0002  max mem: 5511
[21:44:11.856110] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.9313 (0.9509)  acc1: 67.1875 (68.1656)  acc5: 96.8750 (97.2090)  time: 0.0282  data: 0.0002  max mem: 5511
[21:44:12.139879] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.0323 (0.9525)  acc1: 65.6250 (68.2292)  acc5: 96.8750 (97.0966)  time: 0.0282  data: 0.0002  max mem: 5511
[21:44:12.424846] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.9205 (0.9490)  acc1: 70.3125 (68.4189)  acc5: 96.8750 (97.1026)  time: 0.0283  data: 0.0001  max mem: 5511
[21:44:12.576687] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.8959 (0.9514)  acc1: 70.3125 (68.3400)  acc5: 96.8750 (97.1400)  time: 0.0273  data: 0.0001  max mem: 5511
[21:44:12.721598] Test: Total time: 0:00:05 (0.0332 s / it)
[21:44:12.722052] * Acc@1 68.340 Acc@5 97.140 loss 0.951
[21:44:12.722359] Accuracy of the network on the 10000 test images: 68.3%
[21:44:12.722556] Max accuracy: 68.59%
[21:44:12.883538] log_dir: ./output_dir
[21:44:13.781786] Epoch: [19]  [  0/781]  eta: 0:11:40  lr: 0.000237  training_loss: 1.6682 (1.6682)  mae_loss: 0.0326 (0.0326)  classification_loss: 1.6283 (1.6283)  loss_mask: 0.0073 (0.0073)  time: 0.8963  data: 0.6871  max mem: 5511
[21:44:17.705896] Epoch: [19]  [ 20/781]  eta: 0:02:54  lr: 0.000237  training_loss: 1.6165 (1.6217)  mae_loss: 0.0319 (0.0314)  classification_loss: 1.5729 (1.5807)  loss_mask: 0.0086 (0.0096)  time: 0.1961  data: 0.0002  max mem: 5511
[21:44:21.635369] Epoch: [19]  [ 40/781]  eta: 0:02:38  lr: 0.000237  training_loss: 1.6184 (1.6191)  mae_loss: 0.0311 (0.0314)  classification_loss: 1.5813 (1.5775)  loss_mask: 0.0106 (0.0103)  time: 0.1964  data: 0.0002  max mem: 5511
[21:44:25.577581] Epoch: [19]  [ 60/781]  eta: 0:02:29  lr: 0.000237  training_loss: 1.6818 (1.6373)  mae_loss: 0.0290 (0.0311)  classification_loss: 1.6369 (1.5963)  loss_mask: 0.0086 (0.0099)  time: 0.1970  data: 0.0002  max mem: 5511
[21:44:29.493882] Epoch: [19]  [ 80/781]  eta: 0:02:23  lr: 0.000237  training_loss: 1.6622 (1.6346)  mae_loss: 0.0302 (0.0309)  classification_loss: 1.6190 (1.5941)  loss_mask: 0.0078 (0.0096)  time: 0.1957  data: 0.0002  max mem: 5511
[21:44:33.437376] Epoch: [19]  [100/781]  eta: 0:02:18  lr: 0.000237  training_loss: 1.6298 (1.6366)  mae_loss: 0.0306 (0.0311)  classification_loss: 1.5923 (1.5960)  loss_mask: 0.0088 (0.0095)  time: 0.1971  data: 0.0004  max mem: 5511
[21:44:37.390610] Epoch: [19]  [120/781]  eta: 0:02:13  lr: 0.000237  training_loss: 1.6179 (1.6349)  mae_loss: 0.0293 (0.0309)  classification_loss: 1.5793 (1.5944)  loss_mask: 0.0105 (0.0096)  time: 0.1976  data: 0.0003  max mem: 5511
[21:44:41.309664] Epoch: [19]  [140/781]  eta: 0:02:09  lr: 0.000237  training_loss: 1.5823 (1.6288)  mae_loss: 0.0328 (0.0311)  classification_loss: 1.5402 (1.5877)  loss_mask: 0.0116 (0.0100)  time: 0.1959  data: 0.0002  max mem: 5511
[21:44:45.266607] Epoch: [19]  [160/781]  eta: 0:02:04  lr: 0.000237  training_loss: 1.5807 (1.6272)  mae_loss: 0.0314 (0.0311)  classification_loss: 1.5247 (1.5853)  loss_mask: 0.0146 (0.0108)  time: 0.1978  data: 0.0002  max mem: 5511
[21:44:49.213189] Epoch: [19]  [180/781]  eta: 0:02:00  lr: 0.000236  training_loss: 1.6261 (1.6274)  mae_loss: 0.0322 (0.0313)  classification_loss: 1.5836 (1.5848)  loss_mask: 0.0127 (0.0113)  time: 0.1972  data: 0.0002  max mem: 5511
[21:44:53.156429] Epoch: [19]  [200/781]  eta: 0:01:56  lr: 0.000236  training_loss: 1.6601 (1.6300)  mae_loss: 0.0327 (0.0314)  classification_loss: 1.6200 (1.5874)  loss_mask: 0.0085 (0.0112)  time: 0.1971  data: 0.0002  max mem: 5511
[21:44:57.097383] Epoch: [19]  [220/781]  eta: 0:01:52  lr: 0.000236  training_loss: 1.6303 (1.6308)  mae_loss: 0.0313 (0.0316)  classification_loss: 1.5913 (1.5882)  loss_mask: 0.0088 (0.0111)  time: 0.1970  data: 0.0002  max mem: 5511
[21:45:01.045138] Epoch: [19]  [240/781]  eta: 0:01:48  lr: 0.000236  training_loss: 1.6545 (1.6315)  mae_loss: 0.0313 (0.0316)  classification_loss: 1.6107 (1.5889)  loss_mask: 0.0092 (0.0110)  time: 0.1973  data: 0.0002  max mem: 5511
[21:45:05.013752] Epoch: [19]  [260/781]  eta: 0:01:44  lr: 0.000236  training_loss: 1.6496 (1.6306)  mae_loss: 0.0312 (0.0315)  classification_loss: 1.6054 (1.5882)  loss_mask: 0.0093 (0.0109)  time: 0.1984  data: 0.0002  max mem: 5511
[21:45:08.996058] Epoch: [19]  [280/781]  eta: 0:01:39  lr: 0.000236  training_loss: 1.6206 (1.6323)  mae_loss: 0.0329 (0.0316)  classification_loss: 1.5736 (1.5896)  loss_mask: 0.0135 (0.0111)  time: 0.1990  data: 0.0002  max mem: 5511
[21:45:12.938445] Epoch: [19]  [300/781]  eta: 0:01:35  lr: 0.000236  training_loss: 1.6470 (1.6327)  mae_loss: 0.0313 (0.0317)  classification_loss: 1.5941 (1.5900)  loss_mask: 0.0087 (0.0110)  time: 0.1970  data: 0.0002  max mem: 5511
[21:45:16.893071] Epoch: [19]  [320/781]  eta: 0:01:31  lr: 0.000236  training_loss: 1.6095 (1.6306)  mae_loss: 0.0335 (0.0318)  classification_loss: 1.5638 (1.5878)  loss_mask: 0.0082 (0.0110)  time: 0.1976  data: 0.0002  max mem: 5511
[21:45:20.818784] Epoch: [19]  [340/781]  eta: 0:01:27  lr: 0.000236  training_loss: 1.5878 (1.6295)  mae_loss: 0.0310 (0.0318)  classification_loss: 1.5522 (1.5868)  loss_mask: 0.0103 (0.0110)  time: 0.1962  data: 0.0002  max mem: 5511
[21:45:24.761608] Epoch: [19]  [360/781]  eta: 0:01:23  lr: 0.000236  training_loss: 1.6280 (1.6299)  mae_loss: 0.0330 (0.0319)  classification_loss: 1.5828 (1.5870)  loss_mask: 0.0101 (0.0110)  time: 0.1970  data: 0.0002  max mem: 5511
[21:45:28.694165] Epoch: [19]  [380/781]  eta: 0:01:19  lr: 0.000236  training_loss: 1.6144 (1.6297)  mae_loss: 0.0308 (0.0318)  classification_loss: 1.5808 (1.5869)  loss_mask: 0.0080 (0.0110)  time: 0.1965  data: 0.0002  max mem: 5511
[21:45:32.612707] Epoch: [19]  [400/781]  eta: 0:01:15  lr: 0.000236  training_loss: 1.5701 (1.6275)  mae_loss: 0.0323 (0.0319)  classification_loss: 1.5230 (1.5845)  loss_mask: 0.0098 (0.0110)  time: 0.1958  data: 0.0002  max mem: 5511
[21:45:36.530529] Epoch: [19]  [420/781]  eta: 0:01:11  lr: 0.000236  training_loss: 1.6522 (1.6279)  mae_loss: 0.0321 (0.0319)  classification_loss: 1.6171 (1.5850)  loss_mask: 0.0084 (0.0110)  time: 0.1958  data: 0.0002  max mem: 5511
[21:45:40.499780] Epoch: [19]  [440/781]  eta: 0:01:07  lr: 0.000236  training_loss: 1.6074 (1.6279)  mae_loss: 0.0333 (0.0320)  classification_loss: 1.5656 (1.5850)  loss_mask: 0.0088 (0.0110)  time: 0.1984  data: 0.0002  max mem: 5511
[21:45:44.423185] Epoch: [19]  [460/781]  eta: 0:01:03  lr: 0.000236  training_loss: 1.6299 (1.6288)  mae_loss: 0.0307 (0.0320)  classification_loss: 1.5865 (1.5857)  loss_mask: 0.0130 (0.0111)  time: 0.1961  data: 0.0002  max mem: 5511
[21:45:48.345649] Epoch: [19]  [480/781]  eta: 0:00:59  lr: 0.000236  training_loss: 1.6323 (1.6292)  mae_loss: 0.0300 (0.0319)  classification_loss: 1.5891 (1.5861)  loss_mask: 0.0129 (0.0112)  time: 0.1960  data: 0.0002  max mem: 5511
[21:45:52.342457] Epoch: [19]  [500/781]  eta: 0:00:55  lr: 0.000236  training_loss: 1.6481 (1.6305)  mae_loss: 0.0303 (0.0319)  classification_loss: 1.5981 (1.5870)  loss_mask: 0.0162 (0.0116)  time: 0.1997  data: 0.0003  max mem: 5511
[21:45:56.273621] Epoch: [19]  [520/781]  eta: 0:00:51  lr: 0.000236  training_loss: 1.6322 (1.6306)  mae_loss: 0.0322 (0.0319)  classification_loss: 1.5917 (1.5869)  loss_mask: 0.0135 (0.0117)  time: 0.1964  data: 0.0002  max mem: 5511
[21:46:00.205164] Epoch: [19]  [540/781]  eta: 0:00:47  lr: 0.000236  training_loss: 1.6014 (1.6297)  mae_loss: 0.0321 (0.0320)  classification_loss: 1.5499 (1.5861)  loss_mask: 0.0080 (0.0116)  time: 0.1965  data: 0.0002  max mem: 5511
[21:46:04.138517] Epoch: [19]  [560/781]  eta: 0:00:43  lr: 0.000236  training_loss: 1.6173 (1.6290)  mae_loss: 0.0316 (0.0320)  classification_loss: 1.5791 (1.5854)  loss_mask: 0.0121 (0.0116)  time: 0.1966  data: 0.0002  max mem: 5511
[21:46:08.101611] Epoch: [19]  [580/781]  eta: 0:00:39  lr: 0.000235  training_loss: 1.6239 (1.6293)  mae_loss: 0.0305 (0.0319)  classification_loss: 1.5843 (1.5856)  loss_mask: 0.0157 (0.0118)  time: 0.1981  data: 0.0002  max mem: 5511
[21:46:12.022304] Epoch: [19]  [600/781]  eta: 0:00:35  lr: 0.000235  training_loss: 1.5473 (1.6276)  mae_loss: 0.0306 (0.0319)  classification_loss: 1.5085 (1.5840)  loss_mask: 0.0083 (0.0117)  time: 0.1960  data: 0.0002  max mem: 5511
[21:46:15.942139] Epoch: [19]  [620/781]  eta: 0:00:31  lr: 0.000235  training_loss: 1.5866 (1.6268)  mae_loss: 0.0326 (0.0319)  classification_loss: 1.5434 (1.5831)  loss_mask: 0.0122 (0.0118)  time: 0.1959  data: 0.0003  max mem: 5511
[21:46:19.873199] Epoch: [19]  [640/781]  eta: 0:00:27  lr: 0.000235  training_loss: 1.6145 (1.6274)  mae_loss: 0.0316 (0.0319)  classification_loss: 1.5735 (1.5838)  loss_mask: 0.0086 (0.0118)  time: 0.1964  data: 0.0002  max mem: 5511
[21:46:23.799300] Epoch: [19]  [660/781]  eta: 0:00:23  lr: 0.000235  training_loss: 1.6547 (1.6279)  mae_loss: 0.0289 (0.0318)  classification_loss: 1.6136 (1.5844)  loss_mask: 0.0084 (0.0117)  time: 0.1962  data: 0.0002  max mem: 5511
[21:46:27.723265] Epoch: [19]  [680/781]  eta: 0:00:19  lr: 0.000235  training_loss: 1.6486 (1.6284)  mae_loss: 0.0316 (0.0318)  classification_loss: 1.6057 (1.5849)  loss_mask: 0.0116 (0.0117)  time: 0.1961  data: 0.0002  max mem: 5511
[21:46:31.678421] Epoch: [19]  [700/781]  eta: 0:00:16  lr: 0.000235  training_loss: 1.6179 (1.6279)  mae_loss: 0.0319 (0.0318)  classification_loss: 1.5622 (1.5843)  loss_mask: 0.0112 (0.0118)  time: 0.1977  data: 0.0002  max mem: 5511
[21:46:35.645010] Epoch: [19]  [720/781]  eta: 0:00:12  lr: 0.000235  training_loss: 1.6201 (1.6275)  mae_loss: 0.0316 (0.0318)  classification_loss: 1.5777 (1.5839)  loss_mask: 0.0106 (0.0117)  time: 0.1982  data: 0.0002  max mem: 5511
[21:46:39.600952] Epoch: [19]  [740/781]  eta: 0:00:08  lr: 0.000235  training_loss: 1.5505 (1.6260)  mae_loss: 0.0304 (0.0318)  classification_loss: 1.5069 (1.5826)  loss_mask: 0.0086 (0.0117)  time: 0.1977  data: 0.0002  max mem: 5511
[21:46:43.562072] Epoch: [19]  [760/781]  eta: 0:00:04  lr: 0.000235  training_loss: 1.6641 (1.6273)  mae_loss: 0.0310 (0.0318)  classification_loss: 1.6234 (1.5839)  loss_mask: 0.0082 (0.0116)  time: 0.1980  data: 0.0002  max mem: 5511
[21:46:47.522476] Epoch: [19]  [780/781]  eta: 0:00:00  lr: 0.000235  training_loss: 1.6272 (1.6273)  mae_loss: 0.0305 (0.0317)  classification_loss: 1.5803 (1.5839)  loss_mask: 0.0097 (0.0116)  time: 0.1979  data: 0.0002  max mem: 5511
[21:46:47.682072] Epoch: [19] Total time: 0:02:34 (0.1982 s / it)
[21:46:47.682536] Averaged stats: lr: 0.000235  training_loss: 1.6272 (1.6273)  mae_loss: 0.0305 (0.0317)  classification_loss: 1.5803 (1.5839)  loss_mask: 0.0097 (0.0116)
[21:46:48.327244] Test:  [  0/157]  eta: 0:01:40  testing_loss: 0.8846 (0.8846)  acc1: 73.4375 (73.4375)  acc5: 98.4375 (98.4375)  time: 0.6406  data: 0.6101  max mem: 5511
[21:46:48.612180] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.9340 (0.9394)  acc1: 70.3125 (69.3182)  acc5: 98.4375 (97.8693)  time: 0.0840  data: 0.0557  max mem: 5511
[21:46:48.898603] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.8998 (0.9028)  acc1: 70.3125 (70.3869)  acc5: 98.4375 (98.2143)  time: 0.0284  data: 0.0002  max mem: 5511
[21:46:49.183793] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.9190 (0.9171)  acc1: 68.7500 (69.9093)  acc5: 98.4375 (97.8831)  time: 0.0284  data: 0.0002  max mem: 5511
[21:46:49.467418] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.9197 (0.9177)  acc1: 70.3125 (69.4360)  acc5: 98.4375 (97.7896)  time: 0.0283  data: 0.0002  max mem: 5511
[21:46:49.752359] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.9013 (0.9122)  acc1: 70.3125 (69.8223)  acc5: 96.8750 (97.6716)  time: 0.0283  data: 0.0002  max mem: 5511
[21:46:50.038236] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.8799 (0.9115)  acc1: 71.8750 (70.1076)  acc5: 96.8750 (97.5922)  time: 0.0284  data: 0.0002  max mem: 5511
[21:46:50.325458] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.8488 (0.9000)  acc1: 71.8750 (70.3345)  acc5: 98.4375 (97.5792)  time: 0.0285  data: 0.0002  max mem: 5511
[21:46:50.611592] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.8461 (0.9050)  acc1: 68.7500 (70.2160)  acc5: 98.4375 (97.5887)  time: 0.0285  data: 0.0002  max mem: 5511
[21:46:50.897094] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.9186 (0.9069)  acc1: 68.7500 (70.0549)  acc5: 98.4375 (97.5962)  time: 0.0284  data: 0.0002  max mem: 5511
[21:46:51.182324] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.9186 (0.9105)  acc1: 67.1875 (69.9412)  acc5: 98.4375 (97.5866)  time: 0.0284  data: 0.0002  max mem: 5511
[21:46:51.470255] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.9113 (0.9091)  acc1: 70.3125 (70.2280)  acc5: 98.4375 (97.6633)  time: 0.0285  data: 0.0002  max mem: 5511
[21:46:51.755534] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.8526 (0.9024)  acc1: 73.4375 (70.5062)  acc5: 98.4375 (97.7273)  time: 0.0285  data: 0.0002  max mem: 5511
[21:46:52.040390] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.8601 (0.9055)  acc1: 68.7500 (70.2290)  acc5: 98.4375 (97.7457)  time: 0.0284  data: 0.0002  max mem: 5511
[21:46:52.328717] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.9495 (0.9067)  acc1: 67.1875 (70.1684)  acc5: 96.8750 (97.6950)  time: 0.0285  data: 0.0002  max mem: 5511
[21:46:52.612050] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.9489 (0.9059)  acc1: 67.1875 (70.1055)  acc5: 96.8750 (97.6407)  time: 0.0284  data: 0.0001  max mem: 5511
[21:46:52.764671] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.9043 (0.9077)  acc1: 67.1875 (70.0500)  acc5: 96.8750 (97.6500)  time: 0.0272  data: 0.0001  max mem: 5511
[21:46:52.903629] Test: Total time: 0:00:05 (0.0332 s / it)
[21:46:52.904256] * Acc@1 70.050 Acc@5 97.650 loss 0.908
[21:46:52.904630] Accuracy of the network on the 10000 test images: 70.0%
[21:46:52.904863] Max accuracy: 70.05%
[21:46:53.120688] log_dir: ./output_dir
[21:46:54.010057] Epoch: [20]  [  0/781]  eta: 0:11:33  lr: 0.000235  training_loss: 1.6518 (1.6518)  mae_loss: 0.0325 (0.0325)  classification_loss: 1.5991 (1.5991)  loss_mask: 0.0202 (0.0202)  time: 0.8877  data: 0.6644  max mem: 5511
[21:46:57.940852] Epoch: [20]  [ 20/781]  eta: 0:02:54  lr: 0.000235  training_loss: 1.5834 (1.6122)  mae_loss: 0.0308 (0.0314)  classification_loss: 1.5406 (1.5697)  loss_mask: 0.0095 (0.0111)  time: 0.1964  data: 0.0002  max mem: 5511
[21:47:01.870636] Epoch: [20]  [ 40/781]  eta: 0:02:38  lr: 0.000235  training_loss: 1.6222 (1.6186)  mae_loss: 0.0318 (0.0312)  classification_loss: 1.5883 (1.5778)  loss_mask: 0.0074 (0.0096)  time: 0.1964  data: 0.0002  max mem: 5511
[21:47:05.789309] Epoch: [20]  [ 60/781]  eta: 0:02:29  lr: 0.000235  training_loss: 1.6135 (1.6208)  mae_loss: 0.0313 (0.0313)  classification_loss: 1.5745 (1.5799)  loss_mask: 0.0075 (0.0095)  time: 0.1958  data: 0.0003  max mem: 5511
[21:47:09.711649] Epoch: [20]  [ 80/781]  eta: 0:02:23  lr: 0.000235  training_loss: 1.6152 (1.6205)  mae_loss: 0.0301 (0.0313)  classification_loss: 1.5689 (1.5783)  loss_mask: 0.0109 (0.0110)  time: 0.1960  data: 0.0002  max mem: 5511
[21:47:13.661174] Epoch: [20]  [100/781]  eta: 0:02:18  lr: 0.000235  training_loss: 1.6245 (1.6212)  mae_loss: 0.0301 (0.0312)  classification_loss: 1.5753 (1.5779)  loss_mask: 0.0156 (0.0121)  time: 0.1974  data: 0.0002  max mem: 5511
[21:47:17.586741] Epoch: [20]  [120/781]  eta: 0:02:13  lr: 0.000235  training_loss: 1.5607 (1.6128)  mae_loss: 0.0314 (0.0312)  classification_loss: 1.5214 (1.5702)  loss_mask: 0.0075 (0.0114)  time: 0.1962  data: 0.0002  max mem: 5511
[21:47:21.500850] Epoch: [20]  [140/781]  eta: 0:02:08  lr: 0.000235  training_loss: 1.6025 (1.6112)  mae_loss: 0.0304 (0.0312)  classification_loss: 1.5615 (1.5689)  loss_mask: 0.0079 (0.0110)  time: 0.1956  data: 0.0002  max mem: 5511
[21:47:25.411787] Epoch: [20]  [160/781]  eta: 0:02:04  lr: 0.000235  training_loss: 1.6567 (1.6158)  mae_loss: 0.0313 (0.0312)  classification_loss: 1.6085 (1.5736)  loss_mask: 0.0070 (0.0110)  time: 0.1954  data: 0.0002  max mem: 5511
[21:47:29.339217] Epoch: [20]  [180/781]  eta: 0:02:00  lr: 0.000235  training_loss: 1.5718 (1.6123)  mae_loss: 0.0304 (0.0312)  classification_loss: 1.5360 (1.5704)  loss_mask: 0.0092 (0.0108)  time: 0.1963  data: 0.0002  max mem: 5511
[21:47:33.259642] Epoch: [20]  [200/781]  eta: 0:01:55  lr: 0.000234  training_loss: 1.6310 (1.6148)  mae_loss: 0.0301 (0.0311)  classification_loss: 1.5932 (1.5732)  loss_mask: 0.0089 (0.0106)  time: 0.1959  data: 0.0002  max mem: 5511
[21:47:37.198211] Epoch: [20]  [220/781]  eta: 0:01:51  lr: 0.000234  training_loss: 1.6501 (1.6168)  mae_loss: 0.0308 (0.0311)  classification_loss: 1.6138 (1.5753)  loss_mask: 0.0078 (0.0104)  time: 0.1968  data: 0.0002  max mem: 5511
[21:47:41.143643] Epoch: [20]  [240/781]  eta: 0:01:47  lr: 0.000234  training_loss: 1.5822 (1.6170)  mae_loss: 0.0301 (0.0310)  classification_loss: 1.5389 (1.5755)  loss_mask: 0.0089 (0.0105)  time: 0.1972  data: 0.0003  max mem: 5511
[21:47:45.070967] Epoch: [20]  [260/781]  eta: 0:01:43  lr: 0.000234  training_loss: 1.5667 (1.6147)  mae_loss: 0.0311 (0.0311)  classification_loss: 1.5230 (1.5731)  loss_mask: 0.0084 (0.0104)  time: 0.1963  data: 0.0002  max mem: 5511
[21:47:49.015256] Epoch: [20]  [280/781]  eta: 0:01:39  lr: 0.000234  training_loss: 1.6397 (1.6170)  mae_loss: 0.0305 (0.0311)  classification_loss: 1.6004 (1.5754)  loss_mask: 0.0080 (0.0105)  time: 0.1971  data: 0.0002  max mem: 5511
[21:47:52.933466] Epoch: [20]  [300/781]  eta: 0:01:35  lr: 0.000234  training_loss: 1.6536 (1.6192)  mae_loss: 0.0302 (0.0311)  classification_loss: 1.6124 (1.5777)  loss_mask: 0.0079 (0.0104)  time: 0.1958  data: 0.0001  max mem: 5511
[21:47:56.865814] Epoch: [20]  [320/781]  eta: 0:01:31  lr: 0.000234  training_loss: 1.6303 (1.6191)  mae_loss: 0.0306 (0.0311)  classification_loss: 1.5905 (1.5776)  loss_mask: 0.0115 (0.0105)  time: 0.1965  data: 0.0001  max mem: 5511
[21:48:00.812027] Epoch: [20]  [340/781]  eta: 0:01:27  lr: 0.000234  training_loss: 1.6019 (1.6179)  mae_loss: 0.0309 (0.0311)  classification_loss: 1.5625 (1.5766)  loss_mask: 0.0063 (0.0103)  time: 0.1972  data: 0.0003  max mem: 5511
[21:48:04.789222] Epoch: [20]  [360/781]  eta: 0:01:23  lr: 0.000234  training_loss: 1.6447 (1.6199)  mae_loss: 0.0313 (0.0311)  classification_loss: 1.6053 (1.5785)  loss_mask: 0.0080 (0.0103)  time: 0.1988  data: 0.0002  max mem: 5511
[21:48:08.710266] Epoch: [20]  [380/781]  eta: 0:01:19  lr: 0.000234  training_loss: 1.5933 (1.6202)  mae_loss: 0.0309 (0.0312)  classification_loss: 1.5492 (1.5786)  loss_mask: 0.0120 (0.0105)  time: 0.1960  data: 0.0002  max mem: 5511
[21:48:12.671791] Epoch: [20]  [400/781]  eta: 0:01:15  lr: 0.000234  training_loss: 1.6373 (1.6209)  mae_loss: 0.0317 (0.0312)  classification_loss: 1.6022 (1.5793)  loss_mask: 0.0086 (0.0104)  time: 0.1980  data: 0.0002  max mem: 5511
[21:48:16.609712] Epoch: [20]  [420/781]  eta: 0:01:11  lr: 0.000234  training_loss: 1.6039 (1.6210)  mae_loss: 0.0313 (0.0312)  classification_loss: 1.5621 (1.5794)  loss_mask: 0.0087 (0.0104)  time: 0.1968  data: 0.0002  max mem: 5511
[21:48:20.598542] Epoch: [20]  [440/781]  eta: 0:01:07  lr: 0.000234  training_loss: 1.5879 (1.6204)  mae_loss: 0.0308 (0.0312)  classification_loss: 1.5495 (1.5786)  loss_mask: 0.0108 (0.0106)  time: 0.1994  data: 0.0002  max mem: 5511
[21:48:24.518601] Epoch: [20]  [460/781]  eta: 0:01:03  lr: 0.000234  training_loss: 1.5824 (1.6184)  mae_loss: 0.0306 (0.0311)  classification_loss: 1.5429 (1.5767)  loss_mask: 0.0096 (0.0105)  time: 0.1959  data: 0.0002  max mem: 5511
[21:48:28.468890] Epoch: [20]  [480/781]  eta: 0:00:59  lr: 0.000234  training_loss: 1.6025 (1.6171)  mae_loss: 0.0294 (0.0311)  classification_loss: 1.5678 (1.5755)  loss_mask: 0.0075 (0.0105)  time: 0.1974  data: 0.0004  max mem: 5511
[21:48:32.385899] Epoch: [20]  [500/781]  eta: 0:00:55  lr: 0.000234  training_loss: 1.5858 (1.6168)  mae_loss: 0.0313 (0.0311)  classification_loss: 1.5484 (1.5753)  loss_mask: 0.0090 (0.0105)  time: 0.1958  data: 0.0002  max mem: 5511
[21:48:36.401790] Epoch: [20]  [520/781]  eta: 0:00:51  lr: 0.000234  training_loss: 1.6160 (1.6166)  mae_loss: 0.0302 (0.0311)  classification_loss: 1.5721 (1.5750)  loss_mask: 0.0108 (0.0105)  time: 0.2007  data: 0.0004  max mem: 5511
[21:48:40.324508] Epoch: [20]  [540/781]  eta: 0:00:47  lr: 0.000234  training_loss: 1.5497 (1.6145)  mae_loss: 0.0315 (0.0311)  classification_loss: 1.5136 (1.5730)  loss_mask: 0.0075 (0.0104)  time: 0.1960  data: 0.0003  max mem: 5511
[21:48:44.244118] Epoch: [20]  [560/781]  eta: 0:00:43  lr: 0.000234  training_loss: 1.5641 (1.6140)  mae_loss: 0.0324 (0.0311)  classification_loss: 1.5113 (1.5721)  loss_mask: 0.0098 (0.0108)  time: 0.1959  data: 0.0002  max mem: 5511
[21:48:48.181602] Epoch: [20]  [580/781]  eta: 0:00:39  lr: 0.000234  training_loss: 1.6193 (1.6143)  mae_loss: 0.0305 (0.0311)  classification_loss: 1.5692 (1.5724)  loss_mask: 0.0112 (0.0108)  time: 0.1967  data: 0.0002  max mem: 5511
[21:48:52.128053] Epoch: [20]  [600/781]  eta: 0:00:35  lr: 0.000233  training_loss: 1.5867 (1.6136)  mae_loss: 0.0315 (0.0312)  classification_loss: 1.5338 (1.5716)  loss_mask: 0.0094 (0.0108)  time: 0.1972  data: 0.0001  max mem: 5511
[21:48:56.057435] Epoch: [20]  [620/781]  eta: 0:00:31  lr: 0.000233  training_loss: 1.5513 (1.6129)  mae_loss: 0.0322 (0.0312)  classification_loss: 1.5116 (1.5709)  loss_mask: 0.0085 (0.0107)  time: 0.1964  data: 0.0002  max mem: 5511
[21:49:00.025376] Epoch: [20]  [640/781]  eta: 0:00:27  lr: 0.000233  training_loss: 1.6244 (1.6139)  mae_loss: 0.0311 (0.0312)  classification_loss: 1.5865 (1.5720)  loss_mask: 0.0083 (0.0107)  time: 0.1983  data: 0.0002  max mem: 5511
[21:49:03.959089] Epoch: [20]  [660/781]  eta: 0:00:23  lr: 0.000233  training_loss: 1.5633 (1.6132)  mae_loss: 0.0310 (0.0312)  classification_loss: 1.5252 (1.5713)  loss_mask: 0.0102 (0.0107)  time: 0.1966  data: 0.0002  max mem: 5511
[21:49:07.900458] Epoch: [20]  [680/781]  eta: 0:00:19  lr: 0.000233  training_loss: 1.6072 (1.6132)  mae_loss: 0.0305 (0.0312)  classification_loss: 1.5625 (1.5712)  loss_mask: 0.0097 (0.0107)  time: 0.1970  data: 0.0002  max mem: 5511
[21:49:11.828261] Epoch: [20]  [700/781]  eta: 0:00:16  lr: 0.000233  training_loss: 1.6456 (1.6137)  mae_loss: 0.0289 (0.0312)  classification_loss: 1.6056 (1.5719)  loss_mask: 0.0088 (0.0107)  time: 0.1963  data: 0.0002  max mem: 5511
[21:49:15.759214] Epoch: [20]  [720/781]  eta: 0:00:12  lr: 0.000233  training_loss: 1.6156 (1.6138)  mae_loss: 0.0292 (0.0312)  classification_loss: 1.5801 (1.5720)  loss_mask: 0.0087 (0.0106)  time: 0.1965  data: 0.0002  max mem: 5511
[21:49:19.693876] Epoch: [20]  [740/781]  eta: 0:00:08  lr: 0.000233  training_loss: 1.5945 (1.6135)  mae_loss: 0.0312 (0.0312)  classification_loss: 1.5386 (1.5715)  loss_mask: 0.0114 (0.0108)  time: 0.1966  data: 0.0002  max mem: 5511
[21:49:23.621115] Epoch: [20]  [760/781]  eta: 0:00:04  lr: 0.000233  training_loss: 1.5819 (1.6134)  mae_loss: 0.0308 (0.0312)  classification_loss: 1.5398 (1.5712)  loss_mask: 0.0130 (0.0110)  time: 0.1963  data: 0.0002  max mem: 5511
[21:49:27.527330] Epoch: [20]  [780/781]  eta: 0:00:00  lr: 0.000233  training_loss: 1.6291 (1.6138)  mae_loss: 0.0314 (0.0312)  classification_loss: 1.5919 (1.5717)  loss_mask: 0.0115 (0.0110)  time: 0.1952  data: 0.0001  max mem: 5511
[21:49:27.695404] Epoch: [20] Total time: 0:02:34 (0.1979 s / it)
[21:49:27.695877] Averaged stats: lr: 0.000233  training_loss: 1.6291 (1.6138)  mae_loss: 0.0314 (0.0312)  classification_loss: 1.5919 (1.5717)  loss_mask: 0.0115 (0.0110)
[21:49:29.307301] Test:  [  0/157]  eta: 0:01:38  testing_loss: 0.8949 (0.8949)  acc1: 68.7500 (68.7500)  acc5: 95.3125 (95.3125)  time: 0.6252  data: 0.5939  max mem: 5511
[21:49:29.597374] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.9151 (0.9535)  acc1: 68.7500 (68.3239)  acc5: 98.4375 (98.2955)  time: 0.0830  data: 0.0542  max mem: 5511
[21:49:29.886049] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.9045 (0.9075)  acc1: 68.7500 (70.9077)  acc5: 98.4375 (98.2887)  time: 0.0288  data: 0.0002  max mem: 5511
[21:49:30.169322] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.9156 (0.9146)  acc1: 68.7500 (70.3125)  acc5: 98.4375 (97.6815)  time: 0.0285  data: 0.0002  max mem: 5511
[21:49:30.457252] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.9354 (0.9216)  acc1: 67.1875 (69.6265)  acc5: 96.8750 (97.6753)  time: 0.0284  data: 0.0002  max mem: 5511
[21:49:30.741871] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.9086 (0.9159)  acc1: 68.7500 (69.9142)  acc5: 98.4375 (97.7328)  time: 0.0285  data: 0.0002  max mem: 5511
[21:49:31.028743] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.8869 (0.9102)  acc1: 70.3125 (70.1332)  acc5: 98.4375 (97.6947)  time: 0.0285  data: 0.0002  max mem: 5511
[21:49:31.313280] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.8474 (0.9015)  acc1: 71.8750 (70.5106)  acc5: 98.4375 (97.7773)  time: 0.0284  data: 0.0002  max mem: 5511
[21:49:31.605825] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.8706 (0.9066)  acc1: 70.3125 (70.3125)  acc5: 98.4375 (97.6466)  time: 0.0287  data: 0.0002  max mem: 5511
[21:49:31.895185] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.9185 (0.9072)  acc1: 70.3125 (70.4842)  acc5: 96.8750 (97.6648)  time: 0.0290  data: 0.0002  max mem: 5511
[21:49:32.178745] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.9385 (0.9117)  acc1: 68.7500 (70.1887)  acc5: 96.8750 (97.5866)  time: 0.0285  data: 0.0002  max mem: 5511
[21:49:32.463954] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.9658 (0.9118)  acc1: 67.1875 (70.1858)  acc5: 96.8750 (97.5929)  time: 0.0283  data: 0.0002  max mem: 5511
[21:49:32.759282] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.8709 (0.9061)  acc1: 70.3125 (70.3254)  acc5: 96.8750 (97.5981)  time: 0.0289  data: 0.0002  max mem: 5511
[21:49:33.047539] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.8753 (0.9102)  acc1: 68.7500 (70.1336)  acc5: 98.4375 (97.5906)  time: 0.0290  data: 0.0002  max mem: 5511
[21:49:33.331802] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.9391 (0.9102)  acc1: 68.7500 (70.0909)  acc5: 96.8750 (97.5399)  time: 0.0285  data: 0.0002  max mem: 5511
[21:49:33.613000] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.8805 (0.9077)  acc1: 70.3125 (70.1780)  acc5: 96.8750 (97.5683)  time: 0.0281  data: 0.0002  max mem: 5511
[21:49:33.765266] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.8649 (0.9108)  acc1: 70.3125 (70.0700)  acc5: 96.8750 (97.5700)  time: 0.0272  data: 0.0001  max mem: 5511
[21:49:33.922746] Test: Total time: 0:00:05 (0.0334 s / it)
[21:49:33.923319] * Acc@1 70.070 Acc@5 97.570 loss 0.911
[21:49:33.923624] Accuracy of the network on the 10000 test images: 70.1%
[21:49:33.923952] Max accuracy: 70.07%
[21:49:34.239912] log_dir: ./output_dir
[21:49:35.102206] Epoch: [21]  [  0/781]  eta: 0:11:12  lr: 0.000233  training_loss: 1.4882 (1.4882)  mae_loss: 0.0336 (0.0336)  classification_loss: 1.4366 (1.4366)  loss_mask: 0.0181 (0.0181)  time: 0.8606  data: 0.6462  max mem: 5511
[21:49:39.041451] Epoch: [21]  [ 20/781]  eta: 0:02:53  lr: 0.000233  training_loss: 1.5741 (1.5773)  mae_loss: 0.0310 (0.0314)  classification_loss: 1.5327 (1.5318)  loss_mask: 0.0098 (0.0141)  time: 0.1968  data: 0.0002  max mem: 5511
[21:49:43.006710] Epoch: [21]  [ 40/781]  eta: 0:02:38  lr: 0.000233  training_loss: 1.5885 (1.5800)  mae_loss: 0.0295 (0.0311)  classification_loss: 1.5535 (1.5375)  loss_mask: 0.0083 (0.0115)  time: 0.1982  data: 0.0002  max mem: 5511
[21:49:46.988476] Epoch: [21]  [ 60/781]  eta: 0:02:30  lr: 0.000233  training_loss: 1.5924 (1.5952)  mae_loss: 0.0309 (0.0313)  classification_loss: 1.5521 (1.5536)  loss_mask: 0.0074 (0.0103)  time: 0.1990  data: 0.0002  max mem: 5511
[21:49:50.963001] Epoch: [21]  [ 80/781]  eta: 0:02:24  lr: 0.000233  training_loss: 1.5832 (1.5920)  mae_loss: 0.0318 (0.0314)  classification_loss: 1.5392 (1.5502)  loss_mask: 0.0075 (0.0104)  time: 0.1986  data: 0.0002  max mem: 5511
[21:49:54.891107] Epoch: [21]  [100/781]  eta: 0:02:19  lr: 0.000233  training_loss: 1.6073 (1.5996)  mae_loss: 0.0310 (0.0313)  classification_loss: 1.5735 (1.5584)  loss_mask: 0.0064 (0.0099)  time: 0.1963  data: 0.0002  max mem: 5511
[21:49:58.819044] Epoch: [21]  [120/781]  eta: 0:02:14  lr: 0.000233  training_loss: 1.5512 (1.5973)  mae_loss: 0.0298 (0.0312)  classification_loss: 1.5129 (1.5564)  loss_mask: 0.0080 (0.0097)  time: 0.1963  data: 0.0002  max mem: 5511
[21:50:02.742961] Epoch: [21]  [140/781]  eta: 0:02:09  lr: 0.000233  training_loss: 1.5830 (1.5968)  mae_loss: 0.0292 (0.0312)  classification_loss: 1.5430 (1.5559)  loss_mask: 0.0088 (0.0097)  time: 0.1961  data: 0.0002  max mem: 5511
[21:50:06.671642] Epoch: [21]  [160/781]  eta: 0:02:05  lr: 0.000233  training_loss: 1.5801 (1.5981)  mae_loss: 0.0294 (0.0310)  classification_loss: 1.5379 (1.5574)  loss_mask: 0.0092 (0.0096)  time: 0.1964  data: 0.0002  max mem: 5511
[21:50:10.576602] Epoch: [21]  [180/781]  eta: 0:02:00  lr: 0.000232  training_loss: 1.5459 (1.5985)  mae_loss: 0.0305 (0.0309)  classification_loss: 1.4998 (1.5574)  loss_mask: 0.0111 (0.0102)  time: 0.1952  data: 0.0002  max mem: 5511
[21:50:14.507730] Epoch: [21]  [200/781]  eta: 0:01:56  lr: 0.000232  training_loss: 1.5949 (1.5984)  mae_loss: 0.0303 (0.0309)  classification_loss: 1.5511 (1.5570)  loss_mask: 0.0109 (0.0106)  time: 0.1965  data: 0.0002  max mem: 5511
[21:50:18.433389] Epoch: [21]  [220/781]  eta: 0:01:52  lr: 0.000232  training_loss: 1.5929 (1.5982)  mae_loss: 0.0311 (0.0308)  classification_loss: 1.5557 (1.5567)  loss_mask: 0.0114 (0.0107)  time: 0.1962  data: 0.0002  max mem: 5511
[21:50:22.348384] Epoch: [21]  [240/781]  eta: 0:01:47  lr: 0.000232  training_loss: 1.5401 (1.5960)  mae_loss: 0.0292 (0.0308)  classification_loss: 1.5026 (1.5546)  loss_mask: 0.0075 (0.0106)  time: 0.1957  data: 0.0003  max mem: 5511
[21:50:26.284414] Epoch: [21]  [260/781]  eta: 0:01:43  lr: 0.000232  training_loss: 1.5595 (1.5954)  mae_loss: 0.0317 (0.0309)  classification_loss: 1.5151 (1.5541)  loss_mask: 0.0076 (0.0105)  time: 0.1967  data: 0.0002  max mem: 5511
[21:50:30.190358] Epoch: [21]  [280/781]  eta: 0:01:39  lr: 0.000232  training_loss: 1.6118 (1.5980)  mae_loss: 0.0319 (0.0310)  classification_loss: 1.5657 (1.5566)  loss_mask: 0.0089 (0.0104)  time: 0.1952  data: 0.0001  max mem: 5511
[21:50:34.148527] Epoch: [21]  [300/781]  eta: 0:01:35  lr: 0.000232  training_loss: 1.6186 (1.6006)  mae_loss: 0.0314 (0.0310)  classification_loss: 1.5802 (1.5592)  loss_mask: 0.0084 (0.0104)  time: 0.1978  data: 0.0002  max mem: 5511
[21:50:38.074329] Epoch: [21]  [320/781]  eta: 0:01:31  lr: 0.000232  training_loss: 1.6146 (1.6016)  mae_loss: 0.0309 (0.0310)  classification_loss: 1.5761 (1.5604)  loss_mask: 0.0063 (0.0103)  time: 0.1962  data: 0.0002  max mem: 5511
[21:50:42.012547] Epoch: [21]  [340/781]  eta: 0:01:27  lr: 0.000232  training_loss: 1.5482 (1.6005)  mae_loss: 0.0326 (0.0311)  classification_loss: 1.5075 (1.5589)  loss_mask: 0.0092 (0.0104)  time: 0.1968  data: 0.0002  max mem: 5511

[21:50:45.945950] Epoch: [21]  [360/781]  eta: 0:01:23  lr: 0.000232  training_loss: 1.5973 (1.6024)  mae_loss: 0.0311 (0.0312)  classification_loss: 1.5510 (1.5605)  loss_mask: 0.0129 (0.0108)  time: 0.1966  data: 0.0003  max mem: 5511
[21:50:49.875787] Epoch: [21]  [380/781]  eta: 0:01:19  lr: 0.000232  training_loss: 1.6297 (1.6040)  mae_loss: 0.0307 (0.0311)  classification_loss: 1.5907 (1.5620)  loss_mask: 0.0120 (0.0109)  time: 0.1964  data: 0.0002  max mem: 5511
[21:50:53.825555] Epoch: [21]  [400/781]  eta: 0:01:15  lr: 0.000232  training_loss: 1.5814 (1.6031)  mae_loss: 0.0322 (0.0312)  classification_loss: 1.5388 (1.5611)  loss_mask: 0.0075 (0.0108)  time: 0.1974  data: 0.0002  max mem: 5511
[21:50:57.743182] Epoch: [21]  [420/781]  eta: 0:01:11  lr: 0.000232  training_loss: 1.6068 (1.6029)  mae_loss: 0.0312 (0.0312)  classification_loss: 1.5622 (1.5610)  loss_mask: 0.0076 (0.0107)  time: 0.1958  data: 0.0002  max mem: 5511
[21:51:01.663123] Epoch: [21]  [440/781]  eta: 0:01:07  lr: 0.000232  training_loss: 1.5604 (1.6011)  mae_loss: 0.0297 (0.0312)  classification_loss: 1.5233 (1.5592)  loss_mask: 0.0104 (0.0107)  time: 0.1959  data: 0.0002  max mem: 5511
[21:51:05.620440] Epoch: [21]  [460/781]  eta: 0:01:03  lr: 0.000232  training_loss: 1.5689 (1.6006)  mae_loss: 0.0303 (0.0311)  classification_loss: 1.5075 (1.5585)  loss_mask: 0.0141 (0.0110)  time: 0.1978  data: 0.0002  max mem: 5511
[21:51:09.568796] Epoch: [21]  [480/781]  eta: 0:00:59  lr: 0.000232  training_loss: 1.5991 (1.6000)  mae_loss: 0.0303 (0.0311)  classification_loss: 1.5534 (1.5580)  loss_mask: 0.0091 (0.0109)  time: 0.1973  data: 0.0001  max mem: 5511
[21:51:13.502475] Epoch: [21]  [500/781]  eta: 0:00:55  lr: 0.000232  training_loss: 1.6125 (1.6000)  mae_loss: 0.0309 (0.0310)  classification_loss: 1.5708 (1.5581)  loss_mask: 0.0074 (0.0108)  time: 0.1966  data: 0.0002  max mem: 5511
[21:51:17.422819] Epoch: [21]  [520/781]  eta: 0:00:51  lr: 0.000232  training_loss: 1.5733 (1.5999)  mae_loss: 0.0330 (0.0311)  classification_loss: 1.5317 (1.5580)  loss_mask: 0.0081 (0.0107)  time: 0.1959  data: 0.0002  max mem: 5511
[21:51:21.342200] Epoch: [21]  [540/781]  eta: 0:00:47  lr: 0.000232  training_loss: 1.6178 (1.6006)  mae_loss: 0.0315 (0.0311)  classification_loss: 1.5809 (1.5589)  loss_mask: 0.0068 (0.0106)  time: 0.1959  data: 0.0002  max mem: 5511
[21:51:25.281047] Epoch: [21]  [560/781]  eta: 0:00:43  lr: 0.000231  training_loss: 1.5955 (1.5998)  mae_loss: 0.0314 (0.0312)  classification_loss: 1.5583 (1.5581)  loss_mask: 0.0088 (0.0106)  time: 0.1969  data: 0.0002  max mem: 5511
[21:51:29.245104] Epoch: [21]  [580/781]  eta: 0:00:39  lr: 0.000231  training_loss: 1.5890 (1.6006)  mae_loss: 0.0311 (0.0312)  classification_loss: 1.5507 (1.5588)  loss_mask: 0.0085 (0.0106)  time: 0.1981  data: 0.0003  max mem: 5511
[21:51:33.187763] Epoch: [21]  [600/781]  eta: 0:00:35  lr: 0.000231  training_loss: 1.5374 (1.6000)  mae_loss: 0.0307 (0.0311)  classification_loss: 1.4949 (1.5584)  loss_mask: 0.0067 (0.0105)  time: 0.1970  data: 0.0002  max mem: 5511
[21:51:37.137142] Epoch: [21]  [620/781]  eta: 0:00:31  lr: 0.000231  training_loss: 1.5760 (1.5991)  mae_loss: 0.0310 (0.0311)  classification_loss: 1.5333 (1.5576)  loss_mask: 0.0071 (0.0104)  time: 0.1974  data: 0.0003  max mem: 5511
[21:51:41.102908] Epoch: [21]  [640/781]  eta: 0:00:27  lr: 0.000231  training_loss: 1.5514 (1.5985)  mae_loss: 0.0299 (0.0311)  classification_loss: 1.5116 (1.5571)  loss_mask: 0.0053 (0.0103)  time: 0.1982  data: 0.0003  max mem: 5511
[21:51:45.068085] Epoch: [21]  [660/781]  eta: 0:00:23  lr: 0.000231  training_loss: 1.6007 (1.5988)  mae_loss: 0.0313 (0.0311)  classification_loss: 1.5630 (1.5574)  loss_mask: 0.0064 (0.0102)  time: 0.1982  data: 0.0003  max mem: 5511
[21:51:49.004540] Epoch: [21]  [680/781]  eta: 0:00:19  lr: 0.000231  training_loss: 1.6068 (1.5988)  mae_loss: 0.0314 (0.0312)  classification_loss: 1.5682 (1.5576)  loss_mask: 0.0069 (0.0101)  time: 0.1967  data: 0.0002  max mem: 5511
[21:51:52.942750] Epoch: [21]  [700/781]  eta: 0:00:16  lr: 0.000231  training_loss: 1.5660 (1.5987)  mae_loss: 0.0315 (0.0312)  classification_loss: 1.5195 (1.5575)  loss_mask: 0.0075 (0.0101)  time: 0.1968  data: 0.0002  max mem: 5511
[21:51:56.883518] Epoch: [21]  [720/781]  eta: 0:00:12  lr: 0.000231  training_loss: 1.6068 (1.5990)  mae_loss: 0.0317 (0.0312)  classification_loss: 1.5610 (1.5577)  loss_mask: 0.0104 (0.0101)  time: 0.1970  data: 0.0001  max mem: 5511
[21:52:00.792871] Epoch: [21]  [740/781]  eta: 0:00:08  lr: 0.000231  training_loss: 1.5881 (1.5984)  mae_loss: 0.0311 (0.0312)  classification_loss: 1.5510 (1.5571)  loss_mask: 0.0088 (0.0101)  time: 0.1954  data: 0.0002  max mem: 5511
[21:52:04.721334] Epoch: [21]  [760/781]  eta: 0:00:04  lr: 0.000231  training_loss: 1.5726 (1.5982)  mae_loss: 0.0300 (0.0312)  classification_loss: 1.5330 (1.5570)  loss_mask: 0.0079 (0.0101)  time: 0.1963  data: 0.0002  max mem: 5511
[21:52:08.632860] Epoch: [21]  [780/781]  eta: 0:00:00  lr: 0.000231  training_loss: 1.6177 (1.5988)  mae_loss: 0.0303 (0.0312)  classification_loss: 1.5667 (1.5575)  loss_mask: 0.0091 (0.0101)  time: 0.1955  data: 0.0001  max mem: 5511
[21:52:08.796983] Epoch: [21] Total time: 0:02:34 (0.1979 s / it)
[21:52:08.797901] Averaged stats: lr: 0.000231  training_loss: 1.6177 (1.5988)  mae_loss: 0.0303 (0.0312)  classification_loss: 1.5667 (1.5575)  loss_mask: 0.0091 (0.0101)
[21:52:09.445763] Test:  [  0/157]  eta: 0:01:41  testing_loss: 0.9442 (0.9442)  acc1: 68.7500 (68.7500)  acc5: 92.1875 (92.1875)  time: 0.6435  data: 0.6138  max mem: 5511
[21:52:09.739086] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.9442 (0.9400)  acc1: 68.7500 (69.3182)  acc5: 98.4375 (98.0114)  time: 0.0849  data: 0.0560  max mem: 5511
[21:52:10.022947] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.9291 (0.9156)  acc1: 68.7500 (70.0893)  acc5: 98.4375 (97.8423)  time: 0.0286  data: 0.0002  max mem: 5511
[21:52:10.313978] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.9300 (0.9248)  acc1: 70.3125 (69.9093)  acc5: 96.8750 (97.4798)  time: 0.0286  data: 0.0002  max mem: 5511
[21:52:10.607012] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.9164 (0.9252)  acc1: 70.3125 (69.8933)  acc5: 98.4375 (97.4466)  time: 0.0290  data: 0.0002  max mem: 5511
[21:52:10.894661] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.8945 (0.9192)  acc1: 70.3125 (70.3431)  acc5: 96.8750 (97.4571)  time: 0.0288  data: 0.0002  max mem: 5511
[21:52:11.189117] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.8791 (0.9167)  acc1: 70.3125 (70.0820)  acc5: 96.8750 (97.4641)  time: 0.0289  data: 0.0002  max mem: 5511
[21:52:11.476475] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.8662 (0.9063)  acc1: 70.3125 (70.3785)  acc5: 98.4375 (97.5572)  time: 0.0289  data: 0.0002  max mem: 5511
[21:52:11.773530] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.8662 (0.9121)  acc1: 68.7500 (70.0231)  acc5: 98.4375 (97.5309)  time: 0.0290  data: 0.0002  max mem: 5511
[21:52:12.064503] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.9217 (0.9138)  acc1: 68.7500 (69.7630)  acc5: 96.8750 (97.5618)  time: 0.0292  data: 0.0002  max mem: 5511
[21:52:12.353703] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.9290 (0.9157)  acc1: 68.7500 (69.7865)  acc5: 98.4375 (97.6021)  time: 0.0289  data: 0.0003  max mem: 5511
[21:52:12.642949] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.9591 (0.9185)  acc1: 70.3125 (69.8339)  acc5: 98.4375 (97.6211)  time: 0.0288  data: 0.0003  max mem: 5511
[21:52:12.933506] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.9063 (0.9136)  acc1: 71.8750 (70.0671)  acc5: 98.4375 (97.6627)  time: 0.0288  data: 0.0002  max mem: 5511
[21:52:13.219253] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.9041 (0.9167)  acc1: 71.8750 (70.0024)  acc5: 98.4375 (97.6741)  time: 0.0287  data: 0.0002  max mem: 5511
[21:52:13.505130] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.9419 (0.9164)  acc1: 70.3125 (70.1241)  acc5: 96.8750 (97.6396)  time: 0.0284  data: 0.0002  max mem: 5511
[21:52:13.787058] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.9310 (0.9155)  acc1: 70.3125 (70.1883)  acc5: 96.8750 (97.6200)  time: 0.0282  data: 0.0002  max mem: 5511
[21:52:13.940772] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.9310 (0.9183)  acc1: 68.7500 (70.0400)  acc5: 96.8750 (97.5900)  time: 0.0273  data: 0.0001  max mem: 5511
[21:52:14.110631] Test: Total time: 0:00:05 (0.0338 s / it)
[21:52:14.111108] * Acc@1 70.040 Acc@5 97.590 loss 0.918
[21:52:14.111389] Accuracy of the network on the 10000 test images: 70.0%
[21:52:14.111602] Max accuracy: 70.07%
[21:52:14.339504] log_dir: ./output_dir
[21:52:15.208360] Epoch: [22]  [  0/781]  eta: 0:11:17  lr: 0.000231  training_loss: 1.5217 (1.5217)  mae_loss: 0.0327 (0.0327)  classification_loss: 1.4743 (1.4743)  loss_mask: 0.0147 (0.0147)  time: 0.8670  data: 0.6472  max mem: 5511
[21:52:19.147538] Epoch: [22]  [ 20/781]  eta: 0:02:54  lr: 0.000231  training_loss: 1.5986 (1.5920)  mae_loss: 0.0316 (0.0318)  classification_loss: 1.5583 (1.5447)  loss_mask: 0.0129 (0.0156)  time: 0.1969  data: 0.0002  max mem: 5511
[21:52:23.096119] Epoch: [22]  [ 40/781]  eta: 0:02:38  lr: 0.000231  training_loss: 1.6044 (1.5898)  mae_loss: 0.0308 (0.0316)  classification_loss: 1.5560 (1.5448)  loss_mask: 0.0088 (0.0134)  time: 0.1973  data: 0.0002  max mem: 5511
[21:52:27.027231] Epoch: [22]  [ 60/781]  eta: 0:02:29  lr: 0.000231  training_loss: 1.5828 (1.5919)  mae_loss: 0.0317 (0.0315)  classification_loss: 1.5449 (1.5478)  loss_mask: 0.0081 (0.0126)  time: 0.1964  data: 0.0002  max mem: 5511
[21:52:30.935695] Epoch: [22]  [ 80/781]  eta: 0:02:23  lr: 0.000231  training_loss: 1.5992 (1.5930)  mae_loss: 0.0318 (0.0317)  classification_loss: 1.5513 (1.5472)  loss_mask: 0.0186 (0.0141)  time: 0.1953  data: 0.0002  max mem: 5511
[21:52:34.862031] Epoch: [22]  [100/781]  eta: 0:02:18  lr: 0.000231  training_loss: 1.5804 (1.5943)  mae_loss: 0.0304 (0.0316)  classification_loss: 1.5249 (1.5494)  loss_mask: 0.0081 (0.0133)  time: 0.1962  data: 0.0002  max mem: 5511
[21:52:38.786260] Epoch: [22]  [120/781]  eta: 0:02:13  lr: 0.000231  training_loss: 1.5639 (1.5907)  mae_loss: 0.0296 (0.0314)  classification_loss: 1.5224 (1.5463)  loss_mask: 0.0091 (0.0131)  time: 0.1961  data: 0.0003  max mem: 5511
[21:52:42.707646] Epoch: [22]  [140/781]  eta: 0:02:08  lr: 0.000230  training_loss: 1.5884 (1.5871)  mae_loss: 0.0309 (0.0314)  classification_loss: 1.5281 (1.5415)  loss_mask: 0.0208 (0.0143)  time: 0.1960  data: 0.0002  max mem: 5511
[21:52:46.716539] Epoch: [22]  [160/781]  eta: 0:02:04  lr: 0.000230  training_loss: 1.5801 (1.5866)  mae_loss: 0.0320 (0.0314)  classification_loss: 1.5284 (1.5406)  loss_mask: 0.0122 (0.0147)  time: 0.2004  data: 0.0003  max mem: 5511
[21:52:50.653919] Epoch: [22]  [180/781]  eta: 0:02:00  lr: 0.000230  training_loss: 1.5776 (1.5863)  mae_loss: 0.0307 (0.0314)  classification_loss: 1.5310 (1.5407)  loss_mask: 0.0119 (0.0142)  time: 0.1968  data: 0.0002  max mem: 5511
[21:52:54.590331] Epoch: [22]  [200/781]  eta: 0:01:56  lr: 0.000230  training_loss: 1.5497 (1.5836)  mae_loss: 0.0289 (0.0312)  classification_loss: 1.5119 (1.5381)  loss_mask: 0.0140 (0.0143)  time: 0.1967  data: 0.0002  max mem: 5511
[21:52:58.540809] Epoch: [22]  [220/781]  eta: 0:01:52  lr: 0.000230  training_loss: 1.6168 (1.5879)  mae_loss: 0.0293 (0.0311)  classification_loss: 1.5716 (1.5427)  loss_mask: 0.0096 (0.0140)  time: 0.1974  data: 0.0002  max mem: 5511
[21:53:02.479470] Epoch: [22]  [240/781]  eta: 0:01:48  lr: 0.000230  training_loss: 1.5585 (1.5861)  mae_loss: 0.0306 (0.0311)  classification_loss: 1.5259 (1.5414)  loss_mask: 0.0077 (0.0136)  time: 0.1969  data: 0.0002  max mem: 5511
[21:53:06.486945] Epoch: [22]  [260/781]  eta: 0:01:44  lr: 0.000230  training_loss: 1.5596 (1.5856)  mae_loss: 0.0308 (0.0311)  classification_loss: 1.5228 (1.5414)  loss_mask: 0.0073 (0.0131)  time: 0.2003  data: 0.0002  max mem: 5511
[21:53:10.409131] Epoch: [22]  [280/781]  eta: 0:01:39  lr: 0.000230  training_loss: 1.5553 (1.5854)  mae_loss: 0.0314 (0.0311)  classification_loss: 1.5159 (1.5416)  loss_mask: 0.0066 (0.0127)  time: 0.1960  data: 0.0003  max mem: 5511
[21:53:14.386351] Epoch: [22]  [300/781]  eta: 0:01:35  lr: 0.000230  training_loss: 1.5751 (1.5866)  mae_loss: 0.0299 (0.0311)  classification_loss: 1.5391 (1.5431)  loss_mask: 0.0073 (0.0124)  time: 0.1988  data: 0.0002  max mem: 5511
[21:53:18.303073] Epoch: [22]  [320/781]  eta: 0:01:31  lr: 0.000230  training_loss: 1.5937 (1.5861)  mae_loss: 0.0294 (0.0310)  classification_loss: 1.5557 (1.5430)  loss_mask: 0.0079 (0.0121)  time: 0.1958  data: 0.0002  max mem: 5511
[21:53:22.284616] Epoch: [22]  [340/781]  eta: 0:01:27  lr: 0.000230  training_loss: 1.5736 (1.5865)  mae_loss: 0.0308 (0.0310)  classification_loss: 1.5322 (1.5437)  loss_mask: 0.0058 (0.0118)  time: 0.1990  data: 0.0002  max mem: 5511
[21:53:26.217883] Epoch: [22]  [360/781]  eta: 0:01:23  lr: 0.000230  training_loss: 1.6052 (1.5872)  mae_loss: 0.0287 (0.0309)  classification_loss: 1.5647 (1.5446)  loss_mask: 0.0104 (0.0117)  time: 0.1966  data: 0.0002  max mem: 5511
[21:53:30.166890] Epoch: [22]  [380/781]  eta: 0:01:19  lr: 0.000230  training_loss: 1.5667 (1.5859)  mae_loss: 0.0294 (0.0309)  classification_loss: 1.5318 (1.5434)  loss_mask: 0.0070 (0.0115)  time: 0.1974  data: 0.0002  max mem: 5511
[21:53:34.143503] Epoch: [22]  [400/781]  eta: 0:01:15  lr: 0.000230  training_loss: 1.5316 (1.5844)  mae_loss: 0.0313 (0.0310)  classification_loss: 1.4924 (1.5420)  loss_mask: 0.0076 (0.0114)  time: 0.1987  data: 0.0002  max mem: 5511
[21:53:38.080503] Epoch: [22]  [420/781]  eta: 0:01:11  lr: 0.000230  training_loss: 1.5821 (1.5854)  mae_loss: 0.0318 (0.0310)  classification_loss: 1.5353 (1.5430)  loss_mask: 0.0084 (0.0113)  time: 0.1968  data: 0.0002  max mem: 5511
[21:53:42.015563] Epoch: [22]  [440/781]  eta: 0:01:07  lr: 0.000230  training_loss: 1.5443 (1.5839)  mae_loss: 0.0310 (0.0311)  classification_loss: 1.5050 (1.5416)  loss_mask: 0.0083 (0.0112)  time: 0.1967  data: 0.0002  max mem: 5511
[21:53:45.969295] Epoch: [22]  [460/781]  eta: 0:01:03  lr: 0.000230  training_loss: 1.5610 (1.5835)  mae_loss: 0.0316 (0.0311)  classification_loss: 1.5222 (1.5414)  loss_mask: 0.0054 (0.0110)  time: 0.1976  data: 0.0002  max mem: 5511
[21:53:49.917841] Epoch: [22]  [480/781]  eta: 0:00:59  lr: 0.000229  training_loss: 1.5672 (1.5830)  mae_loss: 0.0300 (0.0310)  classification_loss: 1.5267 (1.5411)  loss_mask: 0.0075 (0.0109)  time: 0.1973  data: 0.0002  max mem: 5511
[21:53:53.838888] Epoch: [22]  [500/781]  eta: 0:00:55  lr: 0.000229  training_loss: 1.5844 (1.5838)  mae_loss: 0.0295 (0.0310)  classification_loss: 1.5496 (1.5420)  loss_mask: 0.0081 (0.0109)  time: 0.1960  data: 0.0002  max mem: 5511
[21:53:57.761735] Epoch: [22]  [520/781]  eta: 0:00:51  lr: 0.000229  training_loss: 1.5542 (1.5824)  mae_loss: 0.0324 (0.0310)  classification_loss: 1.5146 (1.5405)  loss_mask: 0.0102 (0.0109)  time: 0.1961  data: 0.0002  max mem: 5511
[21:54:01.813111] Epoch: [22]  [540/781]  eta: 0:00:47  lr: 0.000229  training_loss: 1.6272 (1.5836)  mae_loss: 0.0303 (0.0310)  classification_loss: 1.5681 (1.5417)  loss_mask: 0.0105 (0.0109)  time: 0.2025  data: 0.0004  max mem: 5511
[21:54:05.746279] Epoch: [22]  [560/781]  eta: 0:00:43  lr: 0.000229  training_loss: 1.6018 (1.5835)  mae_loss: 0.0321 (0.0310)  classification_loss: 1.5482 (1.5415)  loss_mask: 0.0087 (0.0110)  time: 0.1966  data: 0.0002  max mem: 5511
[21:54:09.740047] Epoch: [22]  [580/781]  eta: 0:00:39  lr: 0.000229  training_loss: 1.6148 (1.5841)  mae_loss: 0.0283 (0.0310)  classification_loss: 1.5712 (1.5422)  loss_mask: 0.0087 (0.0109)  time: 0.1996  data: 0.0004  max mem: 5511
[21:54:13.678619] Epoch: [22]  [600/781]  eta: 0:00:35  lr: 0.000229  training_loss: 1.6312 (1.5847)  mae_loss: 0.0319 (0.0310)  classification_loss: 1.5828 (1.5426)  loss_mask: 0.0141 (0.0111)  time: 0.1968  data: 0.0002  max mem: 5511
[21:54:17.618940] Epoch: [22]  [620/781]  eta: 0:00:31  lr: 0.000229  training_loss: 1.5529 (1.5841)  mae_loss: 0.0302 (0.0310)  classification_loss: 1.5156 (1.5422)  loss_mask: 0.0057 (0.0110)  time: 0.1969  data: 0.0002  max mem: 5511
[21:54:21.558296] Epoch: [22]  [640/781]  eta: 0:00:27  lr: 0.000229  training_loss: 1.5510 (1.5834)  mae_loss: 0.0306 (0.0310)  classification_loss: 1.5044 (1.5415)  loss_mask: 0.0079 (0.0109)  time: 0.1969  data: 0.0003  max mem: 5511
[21:54:25.513294] Epoch: [22]  [660/781]  eta: 0:00:24  lr: 0.000229  training_loss: 1.5799 (1.5834)  mae_loss: 0.0308 (0.0310)  classification_loss: 1.5425 (1.5415)  loss_mask: 0.0077 (0.0109)  time: 0.1977  data: 0.0002  max mem: 5511
[21:54:29.442046] Epoch: [22]  [680/781]  eta: 0:00:20  lr: 0.000229  training_loss: 1.5806 (1.5828)  mae_loss: 0.0304 (0.0309)  classification_loss: 1.5408 (1.5411)  loss_mask: 0.0072 (0.0108)  time: 0.1963  data: 0.0002  max mem: 5511
[21:54:33.402620] Epoch: [22]  [700/781]  eta: 0:00:16  lr: 0.000229  training_loss: 1.5655 (1.5826)  mae_loss: 0.0313 (0.0309)  classification_loss: 1.5293 (1.5409)  loss_mask: 0.0083 (0.0107)  time: 0.1979  data: 0.0002  max mem: 5511
[21:54:37.373780] Epoch: [22]  [720/781]  eta: 0:00:12  lr: 0.000229  training_loss: 1.5722 (1.5822)  mae_loss: 0.0306 (0.0310)  classification_loss: 1.5342 (1.5406)  loss_mask: 0.0075 (0.0107)  time: 0.1985  data: 0.0003  max mem: 5511
[21:54:41.309658] Epoch: [22]  [740/781]  eta: 0:00:08  lr: 0.000229  training_loss: 1.5486 (1.5820)  mae_loss: 0.0284 (0.0309)  classification_loss: 1.5169 (1.5405)  loss_mask: 0.0070 (0.0106)  time: 0.1967  data: 0.0002  max mem: 5511
[21:54:45.227523] Epoch: [22]  [760/781]  eta: 0:00:04  lr: 0.000229  training_loss: 1.5686 (1.5819)  mae_loss: 0.0309 (0.0309)  classification_loss: 1.5309 (1.5404)  loss_mask: 0.0073 (0.0105)  time: 0.1958  data: 0.0002  max mem: 5511
[21:54:49.150998] Epoch: [22]  [780/781]  eta: 0:00:00  lr: 0.000229  training_loss: 1.5923 (1.5827)  mae_loss: 0.0298 (0.0309)  classification_loss: 1.5533 (1.5413)  loss_mask: 0.0079 (0.0105)  time: 0.1961  data: 0.0002  max mem: 5511
[21:54:49.311152] Epoch: [22] Total time: 0:02:34 (0.1984 s / it)
[21:54:49.311653] Averaged stats: lr: 0.000229  training_loss: 1.5923 (1.5827)  mae_loss: 0.0298 (0.0309)  classification_loss: 1.5533 (1.5413)  loss_mask: 0.0079 (0.0105)
[21:54:49.941454] Test:  [  0/157]  eta: 0:01:38  testing_loss: 0.8732 (0.8732)  acc1: 73.4375 (73.4375)  acc5: 96.8750 (96.8750)  time: 0.6252  data: 0.5920  max mem: 5511
[21:54:50.233419] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.8732 (0.8898)  acc1: 73.4375 (72.3011)  acc5: 98.4375 (98.1534)  time: 0.0832  data: 0.0540  max mem: 5511
[21:54:50.523237] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.8548 (0.8422)  acc1: 73.4375 (73.7351)  acc5: 98.4375 (98.3631)  time: 0.0289  data: 0.0002  max mem: 5511
[21:54:50.810632] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.8436 (0.8530)  acc1: 75.0000 (73.5383)  acc5: 98.4375 (97.9839)  time: 0.0287  data: 0.0002  max mem: 5511
[21:54:51.098195] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.8436 (0.8522)  acc1: 73.4375 (73.5518)  acc5: 98.4375 (97.9802)  time: 0.0286  data: 0.0002  max mem: 5511
[21:54:51.385178] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.8302 (0.8465)  acc1: 73.4375 (73.4988)  acc5: 98.4375 (97.9473)  time: 0.0285  data: 0.0002  max mem: 5511
[21:54:51.671896] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.8389 (0.8477)  acc1: 71.8750 (73.1045)  acc5: 98.4375 (97.9252)  time: 0.0284  data: 0.0002  max mem: 5511
[21:54:51.956836] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.8030 (0.8355)  acc1: 73.4375 (73.4595)  acc5: 98.4375 (97.9974)  time: 0.0284  data: 0.0002  max mem: 5511
[21:54:52.244091] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7845 (0.8427)  acc1: 73.4375 (73.1481)  acc5: 98.4375 (97.9167)  time: 0.0285  data: 0.0002  max mem: 5511
[21:54:52.530459] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.8303 (0.8456)  acc1: 71.8750 (73.0254)  acc5: 98.4375 (97.9052)  time: 0.0285  data: 0.0002  max mem: 5511
[21:54:52.819473] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.8383 (0.8480)  acc1: 71.8750 (73.0043)  acc5: 98.4375 (97.9425)  time: 0.0286  data: 0.0002  max mem: 5511
[21:54:53.102821] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.8666 (0.8492)  acc1: 71.8750 (72.8604)  acc5: 98.4375 (98.0152)  time: 0.0285  data: 0.0002  max mem: 5511
[21:54:53.389526] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.8468 (0.8456)  acc1: 73.4375 (73.0243)  acc5: 98.4375 (98.0114)  time: 0.0284  data: 0.0001  max mem: 5511
[21:54:53.681186] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.8335 (0.8480)  acc1: 70.3125 (72.7934)  acc5: 98.4375 (98.0439)  time: 0.0287  data: 0.0002  max mem: 5511
[21:54:53.963846] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.8513 (0.8469)  acc1: 70.3125 (72.7172)  acc5: 98.4375 (98.0386)  time: 0.0285  data: 0.0002  max mem: 5511
[21:54:54.243750] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.8513 (0.8470)  acc1: 70.3125 (72.6718)  acc5: 98.4375 (97.9512)  time: 0.0280  data: 0.0001  max mem: 5511
[21:54:54.395815] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.8719 (0.8492)  acc1: 70.3125 (72.5500)  acc5: 96.8750 (97.9100)  time: 0.0271  data: 0.0001  max mem: 5511
[21:54:54.534186] Test: Total time: 0:00:05 (0.0332 s / it)
[21:54:54.534689] * Acc@1 72.550 Acc@5 97.910 loss 0.849
[21:54:54.534982] Accuracy of the network on the 10000 test images: 72.5%
[21:54:54.535166] Max accuracy: 72.55%
[21:54:54.622132] log_dir: ./output_dir
[21:54:55.504980] Epoch: [23]  [  0/781]  eta: 0:11:28  lr: 0.000229  training_loss: 1.4731 (1.4731)  mae_loss: 0.0332 (0.0332)  classification_loss: 1.4239 (1.4239)  loss_mask: 0.0161 (0.0161)  time: 0.8812  data: 0.6747  max mem: 5511
[21:54:59.433434] Epoch: [23]  [ 20/781]  eta: 0:02:54  lr: 0.000229  training_loss: 1.5418 (1.5492)  mae_loss: 0.0326 (0.0319)  classification_loss: 1.5025 (1.5083)  loss_mask: 0.0071 (0.0090)  time: 0.1963  data: 0.0004  max mem: 5511
[21:55:03.365072] Epoch: [23]  [ 40/781]  eta: 0:02:37  lr: 0.000228  training_loss: 1.5271 (1.5542)  mae_loss: 0.0314 (0.0318)  classification_loss: 1.4915 (1.5143)  loss_mask: 0.0062 (0.0081)  time: 0.1965  data: 0.0002  max mem: 5511
[21:55:07.317980] Epoch: [23]  [ 60/781]  eta: 0:02:29  lr: 0.000228  training_loss: 1.5608 (1.5624)  mae_loss: 0.0313 (0.0318)  classification_loss: 1.5211 (1.5221)  loss_mask: 0.0088 (0.0085)  time: 0.1976  data: 0.0002  max mem: 5511
[21:55:11.247128] Epoch: [23]  [ 80/781]  eta: 0:02:23  lr: 0.000228  training_loss: 1.5224 (1.5618)  mae_loss: 0.0314 (0.0318)  classification_loss: 1.4744 (1.5215)  loss_mask: 0.0085 (0.0085)  time: 0.1964  data: 0.0003  max mem: 5511
[21:55:15.146753] Epoch: [23]  [100/781]  eta: 0:02:18  lr: 0.000228  training_loss: 1.5572 (1.5660)  mae_loss: 0.0298 (0.0316)  classification_loss: 1.5169 (1.5260)  loss_mask: 0.0071 (0.0085)  time: 0.1949  data: 0.0002  max mem: 5511
[21:55:19.135529] Epoch: [23]  [120/781]  eta: 0:02:13  lr: 0.000228  training_loss: 1.5340 (1.5637)  mae_loss: 0.0278 (0.0311)  classification_loss: 1.4984 (1.5243)  loss_mask: 0.0058 (0.0082)  time: 0.1993  data: 0.0002  max mem: 5511
[21:55:23.075719] Epoch: [23]  [140/781]  eta: 0:02:09  lr: 0.000228  training_loss: 1.5098 (1.5610)  mae_loss: 0.0305 (0.0310)  classification_loss: 1.4752 (1.5218)  loss_mask: 0.0079 (0.0083)  time: 0.1968  data: 0.0003  max mem: 5511
[21:55:27.006581] Epoch: [23]  [160/781]  eta: 0:02:04  lr: 0.000228  training_loss: 1.5787 (1.5652)  mae_loss: 0.0299 (0.0309)  classification_loss: 1.5364 (1.5258)  loss_mask: 0.0067 (0.0085)  time: 0.1965  data: 0.0002  max mem: 5511
[21:55:30.937533] Epoch: [23]  [180/781]  eta: 0:02:00  lr: 0.000228  training_loss: 1.5815 (1.5663)  mae_loss: 0.0313 (0.0310)  classification_loss: 1.5446 (1.5267)  loss_mask: 0.0067 (0.0086)  time: 0.1964  data: 0.0002  max mem: 5511
[21:55:34.911417] Epoch: [23]  [200/781]  eta: 0:01:56  lr: 0.000228  training_loss: 1.5612 (1.5652)  mae_loss: 0.0290 (0.0308)  classification_loss: 1.5174 (1.5255)  loss_mask: 0.0087 (0.0088)  time: 0.1986  data: 0.0005  max mem: 5511
[21:55:38.880354] Epoch: [23]  [220/781]  eta: 0:01:52  lr: 0.000228  training_loss: 1.5855 (1.5669)  mae_loss: 0.0298 (0.0307)  classification_loss: 1.5502 (1.5274)  loss_mask: 0.0057 (0.0087)  time: 0.1983  data: 0.0002  max mem: 5511
[21:55:42.844079] Epoch: [23]  [240/781]  eta: 0:01:48  lr: 0.000228  training_loss: 1.5583 (1.5689)  mae_loss: 0.0309 (0.0307)  classification_loss: 1.5223 (1.5295)  loss_mask: 0.0071 (0.0087)  time: 0.1981  data: 0.0003  max mem: 5511
[21:55:46.813479] Epoch: [23]  [260/781]  eta: 0:01:44  lr: 0.000228  training_loss: 1.5564 (1.5684)  mae_loss: 0.0295 (0.0307)  classification_loss: 1.5250 (1.5292)  loss_mask: 0.0071 (0.0085)  time: 0.1984  data: 0.0002  max mem: 5511
[21:55:50.735311] Epoch: [23]  [280/781]  eta: 0:01:39  lr: 0.000228  training_loss: 1.5880 (1.5701)  mae_loss: 0.0312 (0.0307)  classification_loss: 1.5544 (1.5310)  loss_mask: 0.0055 (0.0084)  time: 0.1960  data: 0.0002  max mem: 5511
[21:55:54.702307] Epoch: [23]  [300/781]  eta: 0:01:35  lr: 0.000228  training_loss: 1.6128 (1.5724)  mae_loss: 0.0298 (0.0307)  classification_loss: 1.5787 (1.5332)  loss_mask: 0.0085 (0.0085)  time: 0.1982  data: 0.0002  max mem: 5511
[21:55:58.639258] Epoch: [23]  [320/781]  eta: 0:01:31  lr: 0.000228  training_loss: 1.5580 (1.5733)  mae_loss: 0.0311 (0.0308)  classification_loss: 1.5147 (1.5341)  loss_mask: 0.0075 (0.0084)  time: 0.1968  data: 0.0002  max mem: 5511
[21:56:02.570036] Epoch: [23]  [340/781]  eta: 0:01:27  lr: 0.000228  training_loss: 1.5301 (1.5705)  mae_loss: 0.0295 (0.0307)  classification_loss: 1.4796 (1.5314)  loss_mask: 0.0072 (0.0084)  time: 0.1965  data: 0.0002  max mem: 5511
[21:56:06.502727] Epoch: [23]  [360/781]  eta: 0:01:23  lr: 0.000228  training_loss: 1.5870 (1.5722)  mae_loss: 0.0321 (0.0308)  classification_loss: 1.5449 (1.5330)  loss_mask: 0.0074 (0.0084)  time: 0.1965  data: 0.0001  max mem: 5511
[21:56:10.450343] Epoch: [23]  [380/781]  eta: 0:01:19  lr: 0.000227  training_loss: 1.5572 (1.5709)  mae_loss: 0.0325 (0.0309)  classification_loss: 1.5212 (1.5316)  loss_mask: 0.0072 (0.0084)  time: 0.1973  data: 0.0003  max mem: 5511
[21:56:14.412405] Epoch: [23]  [400/781]  eta: 0:01:15  lr: 0.000227  training_loss: 1.5354 (1.5700)  mae_loss: 0.0309 (0.0309)  classification_loss: 1.4959 (1.5306)  loss_mask: 0.0096 (0.0086)  time: 0.1980  data: 0.0002  max mem: 5511
[21:56:18.347506] Epoch: [23]  [420/781]  eta: 0:01:11  lr: 0.000227  training_loss: 1.6230 (1.5718)  mae_loss: 0.0293 (0.0308)  classification_loss: 1.5887 (1.5323)  loss_mask: 0.0065 (0.0086)  time: 0.1967  data: 0.0002  max mem: 5511
[21:56:22.310965] Epoch: [23]  [440/781]  eta: 0:01:07  lr: 0.000227  training_loss: 1.5803 (1.5731)  mae_loss: 0.0298 (0.0308)  classification_loss: 1.5369 (1.5337)  loss_mask: 0.0065 (0.0087)  time: 0.1981  data: 0.0002  max mem: 5511
[21:56:26.293201] Epoch: [23]  [460/781]  eta: 0:01:03  lr: 0.000227  training_loss: 1.6038 (1.5739)  mae_loss: 0.0301 (0.0307)  classification_loss: 1.5744 (1.5344)  loss_mask: 0.0081 (0.0087)  time: 0.1990  data: 0.0002  max mem: 5511
[21:56:30.209072] Epoch: [23]  [480/781]  eta: 0:00:59  lr: 0.000227  training_loss: 1.5473 (1.5737)  mae_loss: 0.0305 (0.0307)  classification_loss: 1.5119 (1.5343)  loss_mask: 0.0058 (0.0087)  time: 0.1957  data: 0.0003  max mem: 5511
[21:56:34.120603] Epoch: [23]  [500/781]  eta: 0:00:55  lr: 0.000227  training_loss: 1.5643 (1.5736)  mae_loss: 0.0291 (0.0307)  classification_loss: 1.5233 (1.5342)  loss_mask: 0.0095 (0.0088)  time: 0.1955  data: 0.0002  max mem: 5511
[21:56:38.070863] Epoch: [23]  [520/781]  eta: 0:00:51  lr: 0.000227  training_loss: 1.5213 (1.5733)  mae_loss: 0.0300 (0.0306)  classification_loss: 1.4809 (1.5339)  loss_mask: 0.0070 (0.0088)  time: 0.1974  data: 0.0002  max mem: 5511
[21:56:41.997502] Epoch: [23]  [540/781]  eta: 0:00:47  lr: 0.000227  training_loss: 1.5522 (1.5733)  mae_loss: 0.0305 (0.0307)  classification_loss: 1.5112 (1.5339)  loss_mask: 0.0058 (0.0087)  time: 0.1962  data: 0.0002  max mem: 5511
[21:56:45.934714] Epoch: [23]  [560/781]  eta: 0:00:43  lr: 0.000227  training_loss: 1.5659 (1.5726)  mae_loss: 0.0293 (0.0307)  classification_loss: 1.5163 (1.5333)  loss_mask: 0.0069 (0.0086)  time: 0.1968  data: 0.0002  max mem: 5511
[21:56:49.867838] Epoch: [23]  [580/781]  eta: 0:00:39  lr: 0.000227  training_loss: 1.5255 (1.5715)  mae_loss: 0.0302 (0.0307)  classification_loss: 1.4885 (1.5322)  loss_mask: 0.0071 (0.0086)  time: 0.1966  data: 0.0002  max mem: 5511
[21:56:53.795910] Epoch: [23]  [600/781]  eta: 0:00:35  lr: 0.000227  training_loss: 1.4975 (1.5707)  mae_loss: 0.0298 (0.0306)  classification_loss: 1.4637 (1.5315)  loss_mask: 0.0078 (0.0086)  time: 0.1963  data: 0.0002  max mem: 5511
[21:56:57.723689] Epoch: [23]  [620/781]  eta: 0:00:31  lr: 0.000227  training_loss: 1.5877 (1.5716)  mae_loss: 0.0321 (0.0307)  classification_loss: 1.5442 (1.5324)  loss_mask: 0.0079 (0.0086)  time: 0.1963  data: 0.0002  max mem: 5511
[21:57:01.664843] Epoch: [23]  [640/781]  eta: 0:00:27  lr: 0.000227  training_loss: 1.5507 (1.5716)  mae_loss: 0.0310 (0.0306)  classification_loss: 1.5038 (1.5323)  loss_mask: 0.0110 (0.0087)  time: 0.1970  data: 0.0002  max mem: 5511
[21:57:05.582360] Epoch: [23]  [660/781]  eta: 0:00:23  lr: 0.000227  training_loss: 1.5526 (1.5715)  mae_loss: 0.0302 (0.0306)  classification_loss: 1.5077 (1.5321)  loss_mask: 0.0104 (0.0088)  time: 0.1958  data: 0.0002  max mem: 5511
[21:57:09.524181] Epoch: [23]  [680/781]  eta: 0:00:19  lr: 0.000227  training_loss: 1.6088 (1.5717)  mae_loss: 0.0306 (0.0306)  classification_loss: 1.5622 (1.5324)  loss_mask: 0.0070 (0.0087)  time: 0.1970  data: 0.0002  max mem: 5511
[21:57:13.454093] Epoch: [23]  [700/781]  eta: 0:00:16  lr: 0.000226  training_loss: 1.5422 (1.5712)  mae_loss: 0.0308 (0.0306)  classification_loss: 1.5073 (1.5319)  loss_mask: 0.0061 (0.0087)  time: 0.1964  data: 0.0002  max mem: 5511
[21:57:17.432534] Epoch: [23]  [720/781]  eta: 0:00:12  lr: 0.000226  training_loss: 1.5540 (1.5713)  mae_loss: 0.0302 (0.0306)  classification_loss: 1.5134 (1.5319)  loss_mask: 0.0095 (0.0088)  time: 0.1988  data: 0.0002  max mem: 5511
[21:57:21.352558] Epoch: [23]  [740/781]  eta: 0:00:08  lr: 0.000226  training_loss: 1.5236 (1.5705)  mae_loss: 0.0300 (0.0306)  classification_loss: 1.4846 (1.5310)  loss_mask: 0.0084 (0.0089)  time: 0.1959  data: 0.0003  max mem: 5511
[21:57:25.307519] Epoch: [23]  [760/781]  eta: 0:00:04  lr: 0.000226  training_loss: 1.5549 (1.5707)  mae_loss: 0.0308 (0.0306)  classification_loss: 1.5100 (1.5312)  loss_mask: 0.0091 (0.0090)  time: 0.1977  data: 0.0002  max mem: 5511
[21:57:29.230033] Epoch: [23]  [780/781]  eta: 0:00:00  lr: 0.000226  training_loss: 1.5763 (1.5708)  mae_loss: 0.0323 (0.0306)  classification_loss: 1.5362 (1.5312)  loss_mask: 0.0075 (0.0090)  time: 0.1960  data: 0.0002  max mem: 5511
[21:57:29.386374] Epoch: [23] Total time: 0:02:34 (0.1982 s / it)
[21:57:29.386871] Averaged stats: lr: 0.000226  training_loss: 1.5763 (1.5708)  mae_loss: 0.0323 (0.0306)  classification_loss: 1.5362 (1.5312)  loss_mask: 0.0075 (0.0090)
[21:57:30.077310] Test:  [  0/157]  eta: 0:01:47  testing_loss: 0.9198 (0.9198)  acc1: 73.4375 (73.4375)  acc5: 92.1875 (92.1875)  time: 0.6848  data: 0.6511  max mem: 5511
[21:57:30.365882] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.9040 (0.9163)  acc1: 70.3125 (69.6023)  acc5: 98.4375 (97.4432)  time: 0.0883  data: 0.0595  max mem: 5511
[21:57:30.649395] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.8819 (0.8787)  acc1: 71.8750 (71.4286)  acc5: 98.4375 (97.7679)  time: 0.0285  data: 0.0003  max mem: 5511
[21:57:30.933976] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.8675 (0.8913)  acc1: 71.8750 (71.1694)  acc5: 98.4375 (97.5806)  time: 0.0283  data: 0.0002  max mem: 5511
[21:57:31.222787] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.8938 (0.8964)  acc1: 70.3125 (70.8841)  acc5: 98.4375 (97.5991)  time: 0.0285  data: 0.0002  max mem: 5511
[21:57:31.512055] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.8938 (0.8896)  acc1: 70.3125 (71.3235)  acc5: 98.4375 (97.5490)  time: 0.0287  data: 0.0002  max mem: 5511
[21:57:31.805340] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.8624 (0.8852)  acc1: 71.8750 (71.2346)  acc5: 96.8750 (97.5410)  time: 0.0289  data: 0.0003  max mem: 5511
[21:57:32.093713] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.8409 (0.8777)  acc1: 71.8750 (71.4789)  acc5: 98.4375 (97.6452)  time: 0.0289  data: 0.0005  max mem: 5511
[21:57:32.382310] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.8524 (0.8837)  acc1: 71.8750 (71.2770)  acc5: 98.4375 (97.6659)  time: 0.0287  data: 0.0004  max mem: 5511
[21:57:32.671803] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.9004 (0.8832)  acc1: 71.8750 (71.1710)  acc5: 96.8750 (97.6648)  time: 0.0288  data: 0.0002  max mem: 5511
[21:57:32.958032] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.9069 (0.8844)  acc1: 71.8750 (71.1324)  acc5: 98.4375 (97.7413)  time: 0.0287  data: 0.0002  max mem: 5511
[21:57:33.250193] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.9084 (0.8837)  acc1: 68.7500 (71.0304)  acc5: 98.4375 (97.8322)  time: 0.0288  data: 0.0003  max mem: 5511
[21:57:33.542049] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.8540 (0.8786)  acc1: 71.8750 (71.2552)  acc5: 98.4375 (97.7918)  time: 0.0290  data: 0.0003  max mem: 5511
[21:57:33.829933] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.8358 (0.8804)  acc1: 70.3125 (71.1116)  acc5: 96.8750 (97.7815)  time: 0.0288  data: 0.0002  max mem: 5511
[21:57:34.114705] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.8996 (0.8810)  acc1: 70.3125 (71.2101)  acc5: 98.4375 (97.7726)  time: 0.0285  data: 0.0002  max mem: 5511
[21:57:34.397620] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.8533 (0.8780)  acc1: 73.4375 (71.3162)  acc5: 98.4375 (97.7752)  time: 0.0283  data: 0.0001  max mem: 5511
[21:57:34.551110] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.8533 (0.8788)  acc1: 73.4375 (71.3000)  acc5: 98.4375 (97.8000)  time: 0.0273  data: 0.0001  max mem: 5511
[21:57:34.717259] Test: Total time: 0:00:05 (0.0339 s / it)
[21:57:34.718933] * Acc@1 71.300 Acc@5 97.800 loss 0.879
[21:57:34.719676] Accuracy of the network on the 10000 test images: 71.3%
[21:57:34.720209] Max accuracy: 72.55%
[21:57:34.873878] log_dir: ./output_dir
[21:57:35.754796] Epoch: [24]  [  0/781]  eta: 0:11:25  lr: 0.000226  training_loss: 1.4942 (1.4942)  mae_loss: 0.0331 (0.0331)  classification_loss: 1.4577 (1.4577)  loss_mask: 0.0035 (0.0035)  time: 0.8783  data: 0.6424  max mem: 5511
[21:57:39.682203] Epoch: [24]  [ 20/781]  eta: 0:02:54  lr: 0.000226  training_loss: 1.4991 (1.5496)  mae_loss: 0.0294 (0.0298)  classification_loss: 1.4649 (1.5115)  loss_mask: 0.0081 (0.0083)  time: 0.1963  data: 0.0002  max mem: 5511
[21:57:43.654750] Epoch: [24]  [ 40/781]  eta: 0:02:38  lr: 0.000226  training_loss: 1.5557 (1.5666)  mae_loss: 0.0299 (0.0301)  classification_loss: 1.5123 (1.5254)  loss_mask: 0.0105 (0.0111)  time: 0.1985  data: 0.0002  max mem: 5511
[21:57:47.601178] Epoch: [24]  [ 60/781]  eta: 0:02:30  lr: 0.000226  training_loss: 1.5449 (1.5736)  mae_loss: 0.0291 (0.0299)  classification_loss: 1.5094 (1.5340)  loss_mask: 0.0058 (0.0097)  time: 0.1972  data: 0.0002  max mem: 5511
[21:57:51.535808] Epoch: [24]  [ 80/781]  eta: 0:02:24  lr: 0.000226  training_loss: 1.5371 (1.5635)  mae_loss: 0.0308 (0.0303)  classification_loss: 1.4976 (1.5245)  loss_mask: 0.0052 (0.0087)  time: 0.1966  data: 0.0002  max mem: 5511
[21:57:55.478268] Epoch: [24]  [100/781]  eta: 0:02:18  lr: 0.000226  training_loss: 1.5641 (1.5663)  mae_loss: 0.0288 (0.0302)  classification_loss: 1.5215 (1.5279)  loss_mask: 0.0054 (0.0082)  time: 0.1970  data: 0.0002  max mem: 5511
[21:57:59.395640] Epoch: [24]  [120/781]  eta: 0:02:13  lr: 0.000226  training_loss: 1.4895 (1.5595)  mae_loss: 0.0291 (0.0301)  classification_loss: 1.4525 (1.5215)  loss_mask: 0.0053 (0.0079)  time: 0.1958  data: 0.0002  max mem: 5511
[21:58:03.408610] Epoch: [24]  [140/781]  eta: 0:02:09  lr: 0.000226  training_loss: 1.4924 (1.5561)  mae_loss: 0.0309 (0.0302)  classification_loss: 1.4510 (1.5181)  loss_mask: 0.0049 (0.0078)  time: 0.2006  data: 0.0002  max mem: 5511
[21:58:07.359667] Epoch: [24]  [160/781]  eta: 0:02:05  lr: 0.000226  training_loss: 1.5817 (1.5556)  mae_loss: 0.0294 (0.0302)  classification_loss: 1.5361 (1.5171)  loss_mask: 0.0104 (0.0083)  time: 0.1974  data: 0.0004  max mem: 5511
[21:58:11.324787] Epoch: [24]  [180/781]  eta: 0:02:00  lr: 0.000226  training_loss: 1.5309 (1.5550)  mae_loss: 0.0308 (0.0304)  classification_loss: 1.4920 (1.5158)  loss_mask: 0.0116 (0.0088)  time: 0.1982  data: 0.0003  max mem: 5511
[21:58:15.246561] Epoch: [24]  [200/781]  eta: 0:01:56  lr: 0.000226  training_loss: 1.5255 (1.5542)  mae_loss: 0.0290 (0.0302)  classification_loss: 1.4801 (1.5146)  loss_mask: 0.0114 (0.0094)  time: 0.1960  data: 0.0002  max mem: 5511
[21:58:19.197166] Epoch: [24]  [220/781]  eta: 0:01:52  lr: 0.000226  training_loss: 1.6165 (1.5594)  mae_loss: 0.0310 (0.0303)  classification_loss: 1.5781 (1.5196)  loss_mask: 0.0087 (0.0095)  time: 0.1974  data: 0.0002  max mem: 5511
[21:58:23.173834] Epoch: [24]  [240/781]  eta: 0:01:48  lr: 0.000225  training_loss: 1.5869 (1.5616)  mae_loss: 0.0295 (0.0303)  classification_loss: 1.5498 (1.5218)  loss_mask: 0.0082 (0.0095)  time: 0.1988  data: 0.0002  max mem: 5511
[21:58:27.110464] Epoch: [24]  [260/781]  eta: 0:01:44  lr: 0.000225  training_loss: 1.5475 (1.5621)  mae_loss: 0.0293 (0.0302)  classification_loss: 1.5120 (1.5224)  loss_mask: 0.0067 (0.0095)  time: 0.1967  data: 0.0002  max mem: 5511
[21:58:31.048287] Epoch: [24]  [280/781]  eta: 0:01:40  lr: 0.000225  training_loss: 1.5331 (1.5594)  mae_loss: 0.0302 (0.0302)  classification_loss: 1.4840 (1.5195)  loss_mask: 0.0095 (0.0097)  time: 0.1968  data: 0.0002  max mem: 5511
[21:58:34.969677] Epoch: [24]  [300/781]  eta: 0:01:35  lr: 0.000225  training_loss: 1.5849 (1.5598)  mae_loss: 0.0307 (0.0302)  classification_loss: 1.5487 (1.5201)  loss_mask: 0.0074 (0.0095)  time: 0.1960  data: 0.0002  max mem: 5511
[21:58:38.936772] Epoch: [24]  [320/781]  eta: 0:01:31  lr: 0.000225  training_loss: 1.5892 (1.5618)  mae_loss: 0.0293 (0.0302)  classification_loss: 1.5538 (1.5222)  loss_mask: 0.0061 (0.0094)  time: 0.1983  data: 0.0002  max mem: 5511
[21:58:42.912110] Epoch: [24]  [340/781]  eta: 0:01:27  lr: 0.000225  training_loss: 1.5774 (1.5634)  mae_loss: 0.0319 (0.0303)  classification_loss: 1.5337 (1.5237)  loss_mask: 0.0087 (0.0094)  time: 0.1987  data: 0.0001  max mem: 5511
[21:58:46.846886] Epoch: [24]  [360/781]  eta: 0:01:23  lr: 0.000225  training_loss: 1.5785 (1.5639)  mae_loss: 0.0303 (0.0303)  classification_loss: 1.5253 (1.5240)  loss_mask: 0.0068 (0.0096)  time: 0.1966  data: 0.0003  max mem: 5511
[21:58:50.764051] Epoch: [24]  [380/781]  eta: 0:01:19  lr: 0.000225  training_loss: 1.5430 (1.5631)  mae_loss: 0.0315 (0.0303)  classification_loss: 1.4949 (1.5231)  loss_mask: 0.0092 (0.0097)  time: 0.1958  data: 0.0002  max mem: 5511
[21:58:54.714736] Epoch: [24]  [400/781]  eta: 0:01:15  lr: 0.000225  training_loss: 1.5403 (1.5620)  mae_loss: 0.0304 (0.0303)  classification_loss: 1.5052 (1.5222)  loss_mask: 0.0062 (0.0096)  time: 0.1974  data: 0.0004  max mem: 5511
[21:58:58.670805] Epoch: [24]  [420/781]  eta: 0:01:11  lr: 0.000225  training_loss: 1.5375 (1.5609)  mae_loss: 0.0305 (0.0303)  classification_loss: 1.4992 (1.5211)  loss_mask: 0.0064 (0.0095)  time: 0.1977  data: 0.0002  max mem: 5511
[21:59:02.597888] Epoch: [24]  [440/781]  eta: 0:01:07  lr: 0.000225  training_loss: 1.5371 (1.5596)  mae_loss: 0.0285 (0.0303)  classification_loss: 1.4993 (1.5198)  loss_mask: 0.0096 (0.0095)  time: 0.1963  data: 0.0002  max mem: 5511
[21:59:06.553302] Epoch: [24]  [460/781]  eta: 0:01:03  lr: 0.000225  training_loss: 1.5659 (1.5597)  mae_loss: 0.0291 (0.0303)  classification_loss: 1.5254 (1.5200)  loss_mask: 0.0071 (0.0094)  time: 0.1977  data: 0.0002  max mem: 5511
[21:59:10.478370] Epoch: [24]  [480/781]  eta: 0:00:59  lr: 0.000225  training_loss: 1.5857 (1.5604)  mae_loss: 0.0292 (0.0302)  classification_loss: 1.5561 (1.5209)  loss_mask: 0.0048 (0.0093)  time: 0.1962  data: 0.0002  max mem: 5511
[21:59:14.443043] Epoch: [24]  [500/781]  eta: 0:00:55  lr: 0.000225  training_loss: 1.5350 (1.5596)  mae_loss: 0.0294 (0.0302)  classification_loss: 1.5010 (1.5203)  loss_mask: 0.0047 (0.0091)  time: 0.1982  data: 0.0002  max mem: 5511
[21:59:18.372142] Epoch: [24]  [520/781]  eta: 0:00:51  lr: 0.000225  training_loss: 1.5385 (1.5597)  mae_loss: 0.0299 (0.0302)  classification_loss: 1.4983 (1.5205)  loss_mask: 0.0064 (0.0090)  time: 0.1964  data: 0.0002  max mem: 5511
[21:59:22.292867] Epoch: [24]  [540/781]  eta: 0:00:47  lr: 0.000225  training_loss: 1.5813 (1.5598)  mae_loss: 0.0292 (0.0302)  classification_loss: 1.5389 (1.5206)  loss_mask: 0.0078 (0.0091)  time: 0.1960  data: 0.0002  max mem: 5511
[21:59:26.210409] Epoch: [24]  [560/781]  eta: 0:00:43  lr: 0.000224  training_loss: 1.5226 (1.5582)  mae_loss: 0.0297 (0.0301)  classification_loss: 1.4921 (1.5191)  loss_mask: 0.0052 (0.0090)  time: 0.1958  data: 0.0002  max mem: 5511
[21:59:30.115312] Epoch: [24]  [580/781]  eta: 0:00:39  lr: 0.000224  training_loss: 1.5547 (1.5578)  mae_loss: 0.0301 (0.0302)  classification_loss: 1.5122 (1.5187)  loss_mask: 0.0056 (0.0089)  time: 0.1951  data: 0.0002  max mem: 5511
[21:59:34.069188] Epoch: [24]  [600/781]  eta: 0:00:35  lr: 0.000224  training_loss: 1.5063 (1.5570)  mae_loss: 0.0295 (0.0302)  classification_loss: 1.4717 (1.5179)  loss_mask: 0.0094 (0.0089)  time: 0.1976  data: 0.0002  max mem: 5511
[21:59:38.012518] Epoch: [24]  [620/781]  eta: 0:00:31  lr: 0.000224  training_loss: 1.5914 (1.5580)  mae_loss: 0.0310 (0.0302)  classification_loss: 1.5467 (1.5190)  loss_mask: 0.0069 (0.0089)  time: 0.1971  data: 0.0002  max mem: 5511
[21:59:41.919162] Epoch: [24]  [640/781]  eta: 0:00:27  lr: 0.000224  training_loss: 1.5649 (1.5572)  mae_loss: 0.0291 (0.0302)  classification_loss: 1.5206 (1.5181)  loss_mask: 0.0092 (0.0089)  time: 0.1952  data: 0.0002  max mem: 5511
[21:59:45.864044] Epoch: [24]  [660/781]  eta: 0:00:23  lr: 0.000224  training_loss: 1.5411 (1.5576)  mae_loss: 0.0293 (0.0301)  classification_loss: 1.5082 (1.5186)  loss_mask: 0.0077 (0.0089)  time: 0.1972  data: 0.0002  max mem: 5511
[21:59:49.794562] Epoch: [24]  [680/781]  eta: 0:00:20  lr: 0.000224  training_loss: 1.5367 (1.5575)  mae_loss: 0.0304 (0.0301)  classification_loss: 1.4949 (1.5185)  loss_mask: 0.0104 (0.0089)  time: 0.1964  data: 0.0002  max mem: 5511
[21:59:53.711685] Epoch: [24]  [700/781]  eta: 0:00:16  lr: 0.000224  training_loss: 1.5379 (1.5571)  mae_loss: 0.0294 (0.0301)  classification_loss: 1.5012 (1.5181)  loss_mask: 0.0069 (0.0089)  time: 0.1958  data: 0.0003  max mem: 5511
[21:59:57.663050] Epoch: [24]  [720/781]  eta: 0:00:12  lr: 0.000224  training_loss: 1.5560 (1.5570)  mae_loss: 0.0297 (0.0301)  classification_loss: 1.5203 (1.5180)  loss_mask: 0.0076 (0.0089)  time: 0.1975  data: 0.0002  max mem: 5511
[22:00:01.608188] Epoch: [24]  [740/781]  eta: 0:00:08  lr: 0.000224  training_loss: 1.5161 (1.5563)  mae_loss: 0.0290 (0.0301)  classification_loss: 1.4709 (1.5171)  loss_mask: 0.0086 (0.0090)  time: 0.1972  data: 0.0004  max mem: 5511
[22:00:05.541565] Epoch: [24]  [760/781]  eta: 0:00:04  lr: 0.000224  training_loss: 1.5722 (1.5564)  mae_loss: 0.0314 (0.0302)  classification_loss: 1.5207 (1.5171)  loss_mask: 0.0130 (0.0092)  time: 0.1966  data: 0.0002  max mem: 5511
[22:00:09.479204] Epoch: [24]  [780/781]  eta: 0:00:00  lr: 0.000224  training_loss: 1.5454 (1.5562)  mae_loss: 0.0313 (0.0302)  classification_loss: 1.5033 (1.5169)  loss_mask: 0.0080 (0.0092)  time: 0.1968  data: 0.0002  max mem: 5511
[22:00:09.621629] Epoch: [24] Total time: 0:02:34 (0.1981 s / it)
[22:00:09.622334] Averaged stats: lr: 0.000224  training_loss: 1.5454 (1.5562)  mae_loss: 0.0313 (0.0302)  classification_loss: 1.5033 (1.5169)  loss_mask: 0.0080 (0.0092)
[22:00:10.224793] Test:  [  0/157]  eta: 0:01:33  testing_loss: 0.8547 (0.8547)  acc1: 75.0000 (75.0000)  acc5: 96.8750 (96.8750)  time: 0.5978  data: 0.5676  max mem: 5511
[22:00:10.515008] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.8753 (0.8818)  acc1: 71.8750 (70.8807)  acc5: 98.4375 (98.4375)  time: 0.0806  data: 0.0519  max mem: 5511
[22:00:10.804880] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.8439 (0.8386)  acc1: 71.8750 (72.3214)  acc5: 98.4375 (98.4375)  time: 0.0289  data: 0.0003  max mem: 5511
[22:00:11.089346] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.8439 (0.8468)  acc1: 71.8750 (71.9758)  acc5: 98.4375 (98.2359)  time: 0.0286  data: 0.0002  max mem: 5511
[22:00:11.375249] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.8288 (0.8493)  acc1: 71.8750 (71.8369)  acc5: 98.4375 (98.0183)  time: 0.0284  data: 0.0002  max mem: 5511
[22:00:11.665360] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.8222 (0.8407)  acc1: 73.4375 (72.3346)  acc5: 98.4375 (98.0392)  time: 0.0287  data: 0.0002  max mem: 5511
[22:00:11.955964] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.8276 (0.8401)  acc1: 71.8750 (72.0031)  acc5: 98.4375 (97.9764)  time: 0.0289  data: 0.0002  max mem: 5511
[22:00:12.246009] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.8101 (0.8324)  acc1: 71.8750 (72.3812)  acc5: 98.4375 (98.0854)  time: 0.0289  data: 0.0002  max mem: 5511
[22:00:12.532081] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.8101 (0.8411)  acc1: 73.4375 (72.2029)  acc5: 98.4375 (98.0131)  time: 0.0287  data: 0.0002  max mem: 5511
[22:00:12.814907] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.8259 (0.8404)  acc1: 71.8750 (72.2184)  acc5: 98.4375 (98.0426)  time: 0.0283  data: 0.0002  max mem: 5511
[22:00:13.100393] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.8259 (0.8420)  acc1: 71.8750 (72.1380)  acc5: 98.4375 (98.0817)  time: 0.0283  data: 0.0002  max mem: 5511
[22:00:13.389894] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.8532 (0.8441)  acc1: 68.7500 (71.9595)  acc5: 98.4375 (98.0856)  time: 0.0286  data: 0.0002  max mem: 5511
[22:00:13.678910] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.8303 (0.8391)  acc1: 71.8750 (72.2107)  acc5: 98.4375 (98.0630)  time: 0.0288  data: 0.0002  max mem: 5511
[22:00:13.967423] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.8303 (0.8425)  acc1: 71.8750 (71.9704)  acc5: 98.4375 (98.0558)  time: 0.0287  data: 0.0002  max mem: 5511
[22:00:14.251636] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.8521 (0.8419)  acc1: 70.3125 (72.0412)  acc5: 96.8750 (98.0053)  time: 0.0285  data: 0.0002  max mem: 5511
[22:00:14.534988] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.8457 (0.8392)  acc1: 71.8750 (72.1026)  acc5: 96.8750 (97.9925)  time: 0.0282  data: 0.0001  max mem: 5511
[22:00:14.688021] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.8436 (0.8417)  acc1: 70.3125 (71.9800)  acc5: 98.4375 (97.9600)  time: 0.0273  data: 0.0001  max mem: 5511
[22:00:14.855592] Test: Total time: 0:00:05 (0.0333 s / it)
[22:00:14.856172] * Acc@1 71.980 Acc@5 97.960 loss 0.842
[22:00:14.856494] Accuracy of the network on the 10000 test images: 72.0%
[22:00:14.856791] Max accuracy: 72.55%
[22:00:15.088421] log_dir: ./output_dir
[22:00:15.951127] Epoch: [25]  [  0/781]  eta: 0:11:12  lr: 0.000224  training_loss: 1.4785 (1.4785)  mae_loss: 0.0327 (0.0327)  classification_loss: 1.4406 (1.4406)  loss_mask: 0.0053 (0.0053)  time: 0.8609  data: 0.6380  max mem: 5511
[22:00:19.925383] Epoch: [25]  [ 20/781]  eta: 0:02:55  lr: 0.000224  training_loss: 1.5326 (1.5463)  mae_loss: 0.0320 (0.0318)  classification_loss: 1.4954 (1.5046)  loss_mask: 0.0085 (0.0100)  time: 0.1986  data: 0.0002  max mem: 5511
[22:00:23.891605] Epoch: [25]  [ 40/781]  eta: 0:02:39  lr: 0.000224  training_loss: 1.5515 (1.5631)  mae_loss: 0.0284 (0.0309)  classification_loss: 1.5182 (1.5224)  loss_mask: 0.0088 (0.0098)  time: 0.1982  data: 0.0002  max mem: 5511
[22:00:27.832147] Epoch: [25]  [ 60/781]  eta: 0:02:30  lr: 0.000224  training_loss: 1.5752 (1.5718)  mae_loss: 0.0301 (0.0310)  classification_loss: 1.5360 (1.5309)  loss_mask: 0.0086 (0.0099)  time: 0.1969  data: 0.0002  max mem: 5511
[22:00:31.776256] Epoch: [25]  [ 80/781]  eta: 0:02:24  lr: 0.000223  training_loss: 1.5516 (1.5730)  mae_loss: 0.0295 (0.0307)  classification_loss: 1.5159 (1.5323)  loss_mask: 0.0105 (0.0100)  time: 0.1971  data: 0.0002  max mem: 5511
[22:00:35.714457] Epoch: [25]  [100/781]  eta: 0:02:19  lr: 0.000223  training_loss: 1.5755 (1.5735)  mae_loss: 0.0290 (0.0305)  classification_loss: 1.5423 (1.5331)  loss_mask: 0.0074 (0.0099)  time: 0.1968  data: 0.0002  max mem: 5511
[22:00:39.660042] Epoch: [25]  [120/781]  eta: 0:02:14  lr: 0.000223  training_loss: 1.5636 (1.5672)  mae_loss: 0.0284 (0.0302)  classification_loss: 1.5111 (1.5274)  loss_mask: 0.0079 (0.0096)  time: 0.1972  data: 0.0002  max mem: 5511
[22:00:43.597695] Epoch: [25]  [140/781]  eta: 0:02:09  lr: 0.000223  training_loss: 1.5540 (1.5619)  mae_loss: 0.0303 (0.0303)  classification_loss: 1.5119 (1.5225)  loss_mask: 0.0047 (0.0092)  time: 0.1968  data: 0.0003  max mem: 5511
[22:00:47.535426] Epoch: [25]  [160/781]  eta: 0:02:05  lr: 0.000223  training_loss: 1.5570 (1.5602)  mae_loss: 0.0289 (0.0301)  classification_loss: 1.5189 (1.5211)  loss_mask: 0.0063 (0.0089)  time: 0.1967  data: 0.0002  max mem: 5511
[22:00:51.496143] Epoch: [25]  [180/781]  eta: 0:02:00  lr: 0.000223  training_loss: 1.5451 (1.5587)  mae_loss: 0.0326 (0.0303)  classification_loss: 1.4969 (1.5194)  loss_mask: 0.0068 (0.0090)  time: 0.1979  data: 0.0002  max mem: 5511
[22:00:55.419780] Epoch: [25]  [200/781]  eta: 0:01:56  lr: 0.000223  training_loss: 1.4934 (1.5538)  mae_loss: 0.0305 (0.0303)  classification_loss: 1.4516 (1.5144)  loss_mask: 0.0080 (0.0090)  time: 0.1961  data: 0.0002  max mem: 5511
[22:00:59.337745] Epoch: [25]  [220/781]  eta: 0:01:52  lr: 0.000223  training_loss: 1.5625 (1.5549)  mae_loss: 0.0299 (0.0303)  classification_loss: 1.5113 (1.5153)  loss_mask: 0.0125 (0.0093)  time: 0.1958  data: 0.0002  max mem: 5511
[22:01:03.275818] Epoch: [25]  [240/781]  eta: 0:01:48  lr: 0.000223  training_loss: 1.5577 (1.5527)  mae_loss: 0.0305 (0.0303)  classification_loss: 1.5220 (1.5129)  loss_mask: 0.0095 (0.0095)  time: 0.1968  data: 0.0002  max mem: 5511
[22:01:07.205342] Epoch: [25]  [260/781]  eta: 0:01:43  lr: 0.000223  training_loss: 1.5798 (1.5532)  mae_loss: 0.0288 (0.0303)  classification_loss: 1.5331 (1.5134)  loss_mask: 0.0074 (0.0096)  time: 0.1964  data: 0.0002  max mem: 5511
[22:01:11.125528] Epoch: [25]  [280/781]  eta: 0:01:39  lr: 0.000223  training_loss: 1.5638 (1.5535)  mae_loss: 0.0325 (0.0304)  classification_loss: 1.5219 (1.5134)  loss_mask: 0.0096 (0.0097)  time: 0.1959  data: 0.0002  max mem: 5511
[22:01:15.073722] Epoch: [25]  [300/781]  eta: 0:01:35  lr: 0.000223  training_loss: 1.5544 (1.5546)  mae_loss: 0.0296 (0.0303)  classification_loss: 1.5263 (1.5146)  loss_mask: 0.0072 (0.0097)  time: 0.1973  data: 0.0002  max mem: 5511
[22:01:18.989185] Epoch: [25]  [320/781]  eta: 0:01:31  lr: 0.000223  training_loss: 1.5619 (1.5551)  mae_loss: 0.0311 (0.0303)  classification_loss: 1.5229 (1.5151)  loss_mask: 0.0085 (0.0097)  time: 0.1957  data: 0.0002  max mem: 5511

[22:01:22.916513] Epoch: [25]  [340/781]  eta: 0:01:27  lr: 0.000223  training_loss: 1.5290 (1.5527)  mae_loss: 0.0294 (0.0303)  classification_loss: 1.4912 (1.5129)  loss_mask: 0.0069 (0.0095)  time: 0.1963  data: 0.0005  max mem: 5511
[22:01:26.837253] Epoch: [25]  [360/781]  eta: 0:01:23  lr: 0.000223  training_loss: 1.5312 (1.5522)  mae_loss: 0.0292 (0.0303)  classification_loss: 1.4963 (1.5125)  loss_mask: 0.0061 (0.0094)  time: 0.1959  data: 0.0002  max mem: 5511
[22:01:30.788659] Epoch: [25]  [380/781]  eta: 0:01:19  lr: 0.000223  training_loss: 1.5423 (1.5546)  mae_loss: 0.0308 (0.0303)  classification_loss: 1.5148 (1.5149)  loss_mask: 0.0067 (0.0094)  time: 0.1975  data: 0.0002  max mem: 5511
[22:01:34.723197] Epoch: [25]  [400/781]  eta: 0:01:15  lr: 0.000222  training_loss: 1.5449 (1.5537)  mae_loss: 0.0310 (0.0303)  classification_loss: 1.5079 (1.5141)  loss_mask: 0.0058 (0.0092)  time: 0.1966  data: 0.0003  max mem: 5511
[22:01:38.669623] Epoch: [25]  [420/781]  eta: 0:01:11  lr: 0.000222  training_loss: 1.5169 (1.5520)  mae_loss: 0.0305 (0.0303)  classification_loss: 1.4761 (1.5125)  loss_mask: 0.0057 (0.0091)  time: 0.1972  data: 0.0005  max mem: 5511
[22:01:42.615287] Epoch: [25]  [440/781]  eta: 0:01:07  lr: 0.000222  training_loss: 1.5470 (1.5513)  mae_loss: 0.0287 (0.0303)  classification_loss: 1.5120 (1.5119)  loss_mask: 0.0081 (0.0091)  time: 0.1972  data: 0.0002  max mem: 5511
[22:01:46.532288] Epoch: [25]  [460/781]  eta: 0:01:03  lr: 0.000222  training_loss: 1.5066 (1.5506)  mae_loss: 0.0303 (0.0303)  classification_loss: 1.4638 (1.5113)  loss_mask: 0.0077 (0.0091)  time: 0.1958  data: 0.0002  max mem: 5511
[22:01:50.463866] Epoch: [25]  [480/781]  eta: 0:00:59  lr: 0.000222  training_loss: 1.5743 (1.5501)  mae_loss: 0.0298 (0.0303)  classification_loss: 1.5403 (1.5108)  loss_mask: 0.0070 (0.0090)  time: 0.1965  data: 0.0002  max mem: 5511
[22:01:54.422501] Epoch: [25]  [500/781]  eta: 0:00:55  lr: 0.000222  training_loss: 1.5260 (1.5499)  mae_loss: 0.0291 (0.0303)  classification_loss: 1.4931 (1.5104)  loss_mask: 0.0135 (0.0092)  time: 0.1978  data: 0.0002  max mem: 5511
[22:01:58.381185] Epoch: [25]  [520/781]  eta: 0:00:51  lr: 0.000222  training_loss: 1.5303 (1.5495)  mae_loss: 0.0300 (0.0303)  classification_loss: 1.4872 (1.5100)  loss_mask: 0.0082 (0.0092)  time: 0.1978  data: 0.0002  max mem: 5511
[22:02:02.357619] Epoch: [25]  [540/781]  eta: 0:00:47  lr: 0.000222  training_loss: 1.5358 (1.5494)  mae_loss: 0.0295 (0.0303)  classification_loss: 1.4952 (1.5099)  loss_mask: 0.0073 (0.0092)  time: 0.1987  data: 0.0003  max mem: 5511
[22:02:06.303325] Epoch: [25]  [560/781]  eta: 0:00:43  lr: 0.000222  training_loss: 1.5109 (1.5490)  mae_loss: 0.0299 (0.0303)  classification_loss: 1.4775 (1.5096)  loss_mask: 0.0058 (0.0091)  time: 0.1972  data: 0.0004  max mem: 5511
[22:02:10.238385] Epoch: [25]  [580/781]  eta: 0:00:39  lr: 0.000222  training_loss: 1.5015 (1.5482)  mae_loss: 0.0300 (0.0303)  classification_loss: 1.4626 (1.5089)  loss_mask: 0.0061 (0.0090)  time: 0.1967  data: 0.0002  max mem: 5511
[22:02:14.185662] Epoch: [25]  [600/781]  eta: 0:00:35  lr: 0.000222  training_loss: 1.5479 (1.5481)  mae_loss: 0.0289 (0.0303)  classification_loss: 1.5053 (1.5087)  loss_mask: 0.0092 (0.0090)  time: 0.1973  data: 0.0002  max mem: 5511
[22:02:18.148134] Epoch: [25]  [620/781]  eta: 0:00:31  lr: 0.000222  training_loss: 1.5671 (1.5488)  mae_loss: 0.0314 (0.0304)  classification_loss: 1.5230 (1.5094)  loss_mask: 0.0067 (0.0090)  time: 0.1981  data: 0.0002  max mem: 5511
[22:02:22.086209] Epoch: [25]  [640/781]  eta: 0:00:27  lr: 0.000222  training_loss: 1.5170 (1.5481)  mae_loss: 0.0286 (0.0304)  classification_loss: 1.4767 (1.5087)  loss_mask: 0.0069 (0.0090)  time: 0.1968  data: 0.0002  max mem: 5511
[22:02:26.027106] Epoch: [25]  [660/781]  eta: 0:00:23  lr: 0.000222  training_loss: 1.5550 (1.5485)  mae_loss: 0.0296 (0.0304)  classification_loss: 1.5211 (1.5093)  loss_mask: 0.0059 (0.0089)  time: 0.1970  data: 0.0002  max mem: 5511
[22:02:29.986027] Epoch: [25]  [680/781]  eta: 0:00:19  lr: 0.000222  training_loss: 1.5725 (1.5496)  mae_loss: 0.0297 (0.0303)  classification_loss: 1.5360 (1.5105)  loss_mask: 0.0056 (0.0088)  time: 0.1979  data: 0.0002  max mem: 5511
[22:02:33.967245] Epoch: [25]  [700/781]  eta: 0:00:16  lr: 0.000221  training_loss: 1.5439 (1.5500)  mae_loss: 0.0289 (0.0303)  classification_loss: 1.5056 (1.5109)  loss_mask: 0.0072 (0.0088)  time: 0.1990  data: 0.0003  max mem: 5511
[22:02:37.959094] Epoch: [25]  [720/781]  eta: 0:00:12  lr: 0.000221  training_loss: 1.6110 (1.5511)  mae_loss: 0.0282 (0.0303)  classification_loss: 1.5731 (1.5121)  loss_mask: 0.0060 (0.0088)  time: 0.1995  data: 0.0002  max mem: 5511
[22:02:41.887745] Epoch: [25]  [740/781]  eta: 0:00:08  lr: 0.000221  training_loss: 1.4992 (1.5501)  mae_loss: 0.0307 (0.0303)  classification_loss: 1.4669 (1.5111)  loss_mask: 0.0051 (0.0087)  time: 0.1963  data: 0.0002  max mem: 5511
[22:02:45.834024] Epoch: [25]  [760/781]  eta: 0:00:04  lr: 0.000221  training_loss: 1.5206 (1.5500)  mae_loss: 0.0300 (0.0302)  classification_loss: 1.4785 (1.5109)  loss_mask: 0.0103 (0.0089)  time: 0.1972  data: 0.0002  max mem: 5511
[22:02:49.748124] Epoch: [25]  [780/781]  eta: 0:00:00  lr: 0.000221  training_loss: 1.5048 (1.5495)  mae_loss: 0.0292 (0.0302)  classification_loss: 1.4693 (1.5104)  loss_mask: 0.0087 (0.0089)  time: 0.1956  data: 0.0002  max mem: 5511
[22:02:49.893420] Epoch: [25] Total time: 0:02:34 (0.1982 s / it)
[22:02:49.894945] Averaged stats: lr: 0.000221  training_loss: 1.5048 (1.5495)  mae_loss: 0.0292 (0.0302)  classification_loss: 1.4693 (1.5104)  loss_mask: 0.0087 (0.0089)
[22:02:50.528228] Test:  [  0/157]  eta: 0:01:38  testing_loss: 0.8202 (0.8202)  acc1: 71.8750 (71.8750)  acc5: 96.8750 (96.8750)  time: 0.6283  data: 0.5847  max mem: 5511
[22:02:50.817385] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.8202 (0.8316)  acc1: 75.0000 (72.7273)  acc5: 98.4375 (98.4375)  time: 0.0831  data: 0.0534  max mem: 5511
[22:02:51.105213] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.7893 (0.7984)  acc1: 75.0000 (74.1815)  acc5: 98.4375 (98.5863)  time: 0.0286  data: 0.0002  max mem: 5511
[22:02:51.392328] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.7893 (0.8031)  acc1: 73.4375 (74.1935)  acc5: 98.4375 (98.1351)  time: 0.0285  data: 0.0002  max mem: 5511
[22:02:51.676373] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.7776 (0.8073)  acc1: 75.0000 (73.7805)  acc5: 96.8750 (98.0564)  time: 0.0284  data: 0.0002  max mem: 5511
[22:02:51.960561] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.7776 (0.8027)  acc1: 75.0000 (73.9890)  acc5: 98.4375 (97.9779)  time: 0.0283  data: 0.0002  max mem: 5511
[22:02:52.244513] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.7774 (0.8014)  acc1: 73.4375 (73.7193)  acc5: 98.4375 (98.0020)  time: 0.0283  data: 0.0001  max mem: 5511
[22:02:52.530733] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7381 (0.7927)  acc1: 73.4375 (74.0757)  acc5: 98.4375 (98.0854)  time: 0.0284  data: 0.0002  max mem: 5511
[22:02:52.813139] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7518 (0.7978)  acc1: 73.4375 (73.8426)  acc5: 98.4375 (98.0131)  time: 0.0283  data: 0.0001  max mem: 5511
[22:02:53.097237] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.8095 (0.7971)  acc1: 73.4375 (73.8839)  acc5: 98.4375 (98.0426)  time: 0.0282  data: 0.0001  max mem: 5511
[22:02:53.385600] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.8207 (0.8009)  acc1: 73.4375 (73.8088)  acc5: 98.4375 (98.0198)  time: 0.0285  data: 0.0002  max mem: 5511
[22:02:53.681699] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.8353 (0.8010)  acc1: 71.8750 (73.7613)  acc5: 98.4375 (98.0293)  time: 0.0290  data: 0.0003  max mem: 5511
[22:02:53.977649] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.7886 (0.7992)  acc1: 73.4375 (73.8120)  acc5: 98.4375 (98.0630)  time: 0.0294  data: 0.0003  max mem: 5511
[22:02:54.260546] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.7398 (0.7995)  acc1: 75.0000 (73.7715)  acc5: 98.4375 (98.0916)  time: 0.0288  data: 0.0002  max mem: 5511
[22:02:54.542846] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7418 (0.7987)  acc1: 75.0000 (73.8586)  acc5: 98.4375 (98.0940)  time: 0.0281  data: 0.0001  max mem: 5511
[22:02:54.826512] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7928 (0.7992)  acc1: 73.4375 (73.8928)  acc5: 98.4375 (98.0443)  time: 0.0282  data: 0.0001  max mem: 5511
[22:02:54.982388] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7605 (0.8003)  acc1: 73.4375 (73.8500)  acc5: 98.4375 (98.0600)  time: 0.0274  data: 0.0001  max mem: 5511
[22:02:55.151500] Test: Total time: 0:00:05 (0.0335 s / it)
[22:02:55.151952] * Acc@1 73.850 Acc@5 98.060 loss 0.800
[22:02:55.152232] Accuracy of the network on the 10000 test images: 73.8%
[22:02:55.152405] Max accuracy: 73.85%
[22:02:55.443110] log_dir: ./output_dir
[22:02:56.300966] Epoch: [26]  [  0/781]  eta: 0:11:08  lr: 0.000221  training_loss: 1.4213 (1.4213)  mae_loss: 0.0315 (0.0315)  classification_loss: 1.3841 (1.3841)  loss_mask: 0.0057 (0.0057)  time: 0.8560  data: 0.6390  max mem: 5511
[22:03:00.234324] Epoch: [26]  [ 20/781]  eta: 0:02:53  lr: 0.000221  training_loss: 1.5045 (1.5213)  mae_loss: 0.0319 (0.0318)  classification_loss: 1.4666 (1.4818)  loss_mask: 0.0067 (0.0077)  time: 0.1966  data: 0.0002  max mem: 5511
[22:03:04.176957] Epoch: [26]  [ 40/781]  eta: 0:02:37  lr: 0.000221  training_loss: 1.5326 (1.5330)  mae_loss: 0.0293 (0.0309)  classification_loss: 1.4941 (1.4920)  loss_mask: 0.0101 (0.0101)  time: 0.1971  data: 0.0002  max mem: 5511
[22:03:08.134660] Epoch: [26]  [ 60/781]  eta: 0:02:29  lr: 0.000221  training_loss: 1.5856 (1.5523)  mae_loss: 0.0311 (0.0311)  classification_loss: 1.5395 (1.5100)  loss_mask: 0.0105 (0.0112)  time: 0.1978  data: 0.0002  max mem: 5511
[22:03:12.061860] Epoch: [26]  [ 80/781]  eta: 0:02:23  lr: 0.000221  training_loss: 1.5147 (1.5499)  mae_loss: 0.0299 (0.0308)  classification_loss: 1.4745 (1.5079)  loss_mask: 0.0083 (0.0111)  time: 0.1963  data: 0.0002  max mem: 5511
[22:03:15.990991] Epoch: [26]  [100/781]  eta: 0:02:18  lr: 0.000221  training_loss: 1.5552 (1.5499)  mae_loss: 0.0282 (0.0305)  classification_loss: 1.5166 (1.5093)  loss_mask: 0.0053 (0.0101)  time: 0.1964  data: 0.0002  max mem: 5511
[22:03:19.927768] Epoch: [26]  [120/781]  eta: 0:02:13  lr: 0.000221  training_loss: 1.5615 (1.5493)  mae_loss: 0.0292 (0.0303)  classification_loss: 1.5212 (1.5093)  loss_mask: 0.0068 (0.0098)  time: 0.1968  data: 0.0002  max mem: 5511
[22:03:23.854595] Epoch: [26]  [140/781]  eta: 0:02:09  lr: 0.000221  training_loss: 1.4511 (1.5391)  mae_loss: 0.0294 (0.0302)  classification_loss: 1.4162 (1.4989)  loss_mask: 0.0090 (0.0099)  time: 0.1962  data: 0.0002  max mem: 5511
[22:03:27.835881] Epoch: [26]  [160/781]  eta: 0:02:04  lr: 0.000221  training_loss: 1.5226 (1.5415)  mae_loss: 0.0297 (0.0302)  classification_loss: 1.4893 (1.5017)  loss_mask: 0.0065 (0.0096)  time: 0.1990  data: 0.0002  max mem: 5511
[22:03:31.838951] Epoch: [26]  [180/781]  eta: 0:02:00  lr: 0.000221  training_loss: 1.5318 (1.5429)  mae_loss: 0.0298 (0.0302)  classification_loss: 1.4974 (1.5034)  loss_mask: 0.0051 (0.0093)  time: 0.2000  data: 0.0002  max mem: 5511
[22:03:35.769490] Epoch: [26]  [200/781]  eta: 0:01:56  lr: 0.000220  training_loss: 1.5515 (1.5433)  mae_loss: 0.0298 (0.0302)  classification_loss: 1.5175 (1.5038)  loss_mask: 0.0073 (0.0093)  time: 0.1964  data: 0.0002  max mem: 5511
[22:03:39.724051] Epoch: [26]  [220/781]  eta: 0:01:52  lr: 0.000220  training_loss: 1.5161 (1.5443)  mae_loss: 0.0275 (0.0300)  classification_loss: 1.4775 (1.5044)  loss_mask: 0.0138 (0.0099)  time: 0.1977  data: 0.0002  max mem: 5511
[22:03:43.682076] Epoch: [26]  [240/781]  eta: 0:01:48  lr: 0.000220  training_loss: 1.4940 (1.5427)  mae_loss: 0.0291 (0.0300)  classification_loss: 1.4566 (1.5027)  loss_mask: 0.0082 (0.0099)  time: 0.1978  data: 0.0002  max mem: 5511
[22:03:47.643252] Epoch: [26]  [260/781]  eta: 0:01:44  lr: 0.000220  training_loss: 1.5506 (1.5423)  mae_loss: 0.0302 (0.0301)  classification_loss: 1.5072 (1.5022)  loss_mask: 0.0096 (0.0101)  time: 0.1980  data: 0.0002  max mem: 5511
[22:03:51.613087] Epoch: [26]  [280/781]  eta: 0:01:40  lr: 0.000220  training_loss: 1.5311 (1.5426)  mae_loss: 0.0282 (0.0300)  classification_loss: 1.4921 (1.5027)  loss_mask: 0.0074 (0.0100)  time: 0.1984  data: 0.0002  max mem: 5511
[22:03:55.542813] Epoch: [26]  [300/781]  eta: 0:01:35  lr: 0.000220  training_loss: 1.5260 (1.5424)  mae_loss: 0.0321 (0.0301)  classification_loss: 1.4855 (1.5026)  loss_mask: 0.0056 (0.0097)  time: 0.1964  data: 0.0002  max mem: 5511
[22:03:59.472145] Epoch: [26]  [320/781]  eta: 0:01:31  lr: 0.000220  training_loss: 1.5226 (1.5425)  mae_loss: 0.0289 (0.0301)  classification_loss: 1.4863 (1.5028)  loss_mask: 0.0071 (0.0096)  time: 0.1964  data: 0.0002  max mem: 5511
[22:04:03.395094] Epoch: [26]  [340/781]  eta: 0:01:27  lr: 0.000220  training_loss: 1.4856 (1.5412)  mae_loss: 0.0293 (0.0301)  classification_loss: 1.4440 (1.5016)  loss_mask: 0.0085 (0.0096)  time: 0.1961  data: 0.0003  max mem: 5511
[22:04:07.325413] Epoch: [26]  [360/781]  eta: 0:01:23  lr: 0.000220  training_loss: 1.5303 (1.5421)  mae_loss: 0.0297 (0.0301)  classification_loss: 1.4990 (1.5027)  loss_mask: 0.0050 (0.0093)  time: 0.1964  data: 0.0002  max mem: 5511
[22:04:11.281339] Epoch: [26]  [380/781]  eta: 0:01:19  lr: 0.000220  training_loss: 1.5655 (1.5430)  mae_loss: 0.0299 (0.0301)  classification_loss: 1.5258 (1.5036)  loss_mask: 0.0056 (0.0092)  time: 0.1977  data: 0.0002  max mem: 5511
[22:04:15.212526] Epoch: [26]  [400/781]  eta: 0:01:15  lr: 0.000220  training_loss: 1.5169 (1.5421)  mae_loss: 0.0290 (0.0301)  classification_loss: 1.4665 (1.5024)  loss_mask: 0.0120 (0.0095)  time: 0.1965  data: 0.0002  max mem: 5511
[22:04:19.143924] Epoch: [26]  [420/781]  eta: 0:01:11  lr: 0.000220  training_loss: 1.5246 (1.5414)  mae_loss: 0.0283 (0.0301)  classification_loss: 1.4935 (1.5018)  loss_mask: 0.0075 (0.0095)  time: 0.1965  data: 0.0002  max mem: 5511
[22:04:23.118828] Epoch: [26]  [440/781]  eta: 0:01:07  lr: 0.000220  training_loss: 1.5457 (1.5412)  mae_loss: 0.0292 (0.0301)  classification_loss: 1.5101 (1.5018)  loss_mask: 0.0064 (0.0094)  time: 0.1987  data: 0.0003  max mem: 5511
[22:04:27.040346] Epoch: [26]  [460/781]  eta: 0:01:03  lr: 0.000220  training_loss: 1.4957 (1.5389)  mae_loss: 0.0291 (0.0300)  classification_loss: 1.4529 (1.4996)  loss_mask: 0.0064 (0.0093)  time: 0.1960  data: 0.0002  max mem: 5511
[22:04:30.945443] Epoch: [26]  [480/781]  eta: 0:00:59  lr: 0.000220  training_loss: 1.5200 (1.5384)  mae_loss: 0.0270 (0.0299)  classification_loss: 1.4934 (1.4992)  loss_mask: 0.0058 (0.0092)  time: 0.1952  data: 0.0003  max mem: 5511
[22:04:34.844357] Epoch: [26]  [500/781]  eta: 0:00:55  lr: 0.000219  training_loss: 1.5097 (1.5380)  mae_loss: 0.0290 (0.0299)  classification_loss: 1.4775 (1.4990)  loss_mask: 0.0058 (0.0091)  time: 0.1949  data: 0.0003  max mem: 5511
[22:04:38.757355] Epoch: [26]  [520/781]  eta: 0:00:51  lr: 0.000219  training_loss: 1.5117 (1.5366)  mae_loss: 0.0284 (0.0299)  classification_loss: 1.4709 (1.4976)  loss_mask: 0.0095 (0.0091)  time: 0.1956  data: 0.0002  max mem: 5511
[22:04:42.683270] Epoch: [26]  [540/781]  eta: 0:00:47  lr: 0.000219  training_loss: 1.5735 (1.5379)  mae_loss: 0.0287 (0.0299)  classification_loss: 1.5174 (1.4985)  loss_mask: 0.0160 (0.0095)  time: 0.1962  data: 0.0002  max mem: 5511
[22:04:46.603477] Epoch: [26]  [560/781]  eta: 0:00:43  lr: 0.000219  training_loss: 1.5205 (1.5370)  mae_loss: 0.0292 (0.0299)  classification_loss: 1.4846 (1.4976)  loss_mask: 0.0071 (0.0095)  time: 0.1959  data: 0.0002  max mem: 5511
[22:04:50.519702] Epoch: [26]  [580/781]  eta: 0:00:39  lr: 0.000219  training_loss: 1.5504 (1.5373)  mae_loss: 0.0301 (0.0299)  classification_loss: 1.5109 (1.4980)  loss_mask: 0.0068 (0.0094)  time: 0.1957  data: 0.0002  max mem: 5511
[22:04:54.449626] Epoch: [26]  [600/781]  eta: 0:00:35  lr: 0.000219  training_loss: 1.4853 (1.5373)  mae_loss: 0.0295 (0.0299)  classification_loss: 1.4489 (1.4981)  loss_mask: 0.0048 (0.0093)  time: 0.1964  data: 0.0002  max mem: 5511
[22:04:58.387660] Epoch: [26]  [620/781]  eta: 0:00:31  lr: 0.000219  training_loss: 1.5268 (1.5374)  mae_loss: 0.0289 (0.0299)  classification_loss: 1.4889 (1.4982)  loss_mask: 0.0071 (0.0093)  time: 0.1968  data: 0.0003  max mem: 5511
[22:05:02.319313] Epoch: [26]  [640/781]  eta: 0:00:27  lr: 0.000219  training_loss: 1.5217 (1.5374)  mae_loss: 0.0295 (0.0299)  classification_loss: 1.4796 (1.4983)  loss_mask: 0.0086 (0.0093)  time: 0.1965  data: 0.0003  max mem: 5511
[22:05:06.252483] Epoch: [26]  [660/781]  eta: 0:00:23  lr: 0.000219  training_loss: 1.5197 (1.5364)  mae_loss: 0.0306 (0.0299)  classification_loss: 1.4849 (1.4973)  loss_mask: 0.0057 (0.0092)  time: 0.1966  data: 0.0002  max mem: 5511
[22:05:10.204291] Epoch: [26]  [680/781]  eta: 0:00:19  lr: 0.000219  training_loss: 1.5544 (1.5372)  mae_loss: 0.0305 (0.0299)  classification_loss: 1.5202 (1.4982)  loss_mask: 0.0060 (0.0091)  time: 0.1975  data: 0.0003  max mem: 5511
[22:05:14.150624] Epoch: [26]  [700/781]  eta: 0:00:16  lr: 0.000219  training_loss: 1.5525 (1.5376)  mae_loss: 0.0291 (0.0299)  classification_loss: 1.5130 (1.4985)  loss_mask: 0.0091 (0.0091)  time: 0.1972  data: 0.0003  max mem: 5511
[22:05:18.090654] Epoch: [26]  [720/781]  eta: 0:00:12  lr: 0.000219  training_loss: 1.4980 (1.5370)  mae_loss: 0.0301 (0.0299)  classification_loss: 1.4579 (1.4979)  loss_mask: 0.0096 (0.0091)  time: 0.1969  data: 0.0003  max mem: 5511
[22:05:22.015496] Epoch: [26]  [740/781]  eta: 0:00:08  lr: 0.000219  training_loss: 1.5095 (1.5361)  mae_loss: 0.0287 (0.0299)  classification_loss: 1.4728 (1.4972)  loss_mask: 0.0061 (0.0091)  time: 0.1962  data: 0.0003  max mem: 5511
[22:05:25.925241] Epoch: [26]  [760/781]  eta: 0:00:04  lr: 0.000219  training_loss: 1.5421 (1.5362)  mae_loss: 0.0301 (0.0299)  classification_loss: 1.5073 (1.4973)  loss_mask: 0.0052 (0.0090)  time: 0.1954  data: 0.0002  max mem: 5511
[22:05:29.950168] Epoch: [26]  [780/781]  eta: 0:00:00  lr: 0.000218  training_loss: 1.5099 (1.5358)  mae_loss: 0.0302 (0.0299)  classification_loss: 1.4721 (1.4969)  loss_mask: 0.0053 (0.0090)  time: 0.2012  data: 0.0002  max mem: 5511
[22:05:30.106661] Epoch: [26] Total time: 0:02:34 (0.1980 s / it)
[22:05:30.107702] Averaged stats: lr: 0.000218  training_loss: 1.5099 (1.5358)  mae_loss: 0.0302 (0.0299)  classification_loss: 1.4721 (1.4969)  loss_mask: 0.0053 (0.0090)
[22:05:30.730880] Test:  [  0/157]  eta: 0:01:37  testing_loss: 0.7993 (0.7993)  acc1: 76.5625 (76.5625)  acc5: 98.4375 (98.4375)  time: 0.6182  data: 0.5869  max mem: 5511
[22:05:31.020075] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.8136 (0.8254)  acc1: 71.8750 (72.7273)  acc5: 98.4375 (99.0057)  time: 0.0823  data: 0.0535  max mem: 5511
[22:05:31.304811] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.7925 (0.7913)  acc1: 75.0000 (74.1071)  acc5: 98.4375 (98.6607)  time: 0.0285  data: 0.0002  max mem: 5511
[22:05:31.589363] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.7925 (0.8028)  acc1: 75.0000 (74.3448)  acc5: 98.4375 (98.4375)  time: 0.0283  data: 0.0002  max mem: 5511
[22:05:31.873451] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.7851 (0.8017)  acc1: 76.5625 (74.3140)  acc5: 98.4375 (98.3232)  time: 0.0283  data: 0.0002  max mem: 5511
[22:05:32.157359] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.7529 (0.7966)  acc1: 73.4375 (74.2034)  acc5: 96.8750 (98.2537)  time: 0.0283  data: 0.0002  max mem: 5511
[22:05:32.441871] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.7984 (0.7959)  acc1: 71.8750 (73.6680)  acc5: 98.4375 (98.1557)  time: 0.0283  data: 0.0002  max mem: 5511
[22:05:32.726071] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7368 (0.7846)  acc1: 73.4375 (74.0537)  acc5: 98.4375 (98.2394)  time: 0.0283  data: 0.0002  max mem: 5511
[22:05:33.011918] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7506 (0.7875)  acc1: 75.0000 (73.9005)  acc5: 98.4375 (98.2832)  time: 0.0284  data: 0.0002  max mem: 5511
[22:05:33.302756] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7669 (0.7874)  acc1: 73.4375 (73.8496)  acc5: 98.4375 (98.2315)  time: 0.0287  data: 0.0002  max mem: 5511
[22:05:33.593914] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.7669 (0.7906)  acc1: 73.4375 (73.7160)  acc5: 98.4375 (98.2364)  time: 0.0289  data: 0.0002  max mem: 5511
[22:05:33.879238] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.8192 (0.7901)  acc1: 73.4375 (73.8880)  acc5: 98.4375 (98.2264)  time: 0.0286  data: 0.0002  max mem: 5511
[22:05:34.162001] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.7709 (0.7868)  acc1: 75.0000 (73.9411)  acc5: 98.4375 (98.2309)  time: 0.0283  data: 0.0002  max mem: 5511
[22:05:34.447795] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.7243 (0.7865)  acc1: 75.0000 (73.8550)  acc5: 98.4375 (98.2586)  time: 0.0282  data: 0.0002  max mem: 5511
[22:05:34.730958] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7945 (0.7850)  acc1: 73.4375 (73.9583)  acc5: 98.4375 (98.2934)  time: 0.0282  data: 0.0002  max mem: 5511
[22:05:35.012790] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7900 (0.7833)  acc1: 75.0000 (74.0170)  acc5: 98.4375 (98.2409)  time: 0.0281  data: 0.0001  max mem: 5511
[22:05:35.165557] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7900 (0.7845)  acc1: 75.0000 (73.9800)  acc5: 98.4375 (98.2700)  time: 0.0271  data: 0.0001  max mem: 5511
[22:05:35.319303] Test: Total time: 0:00:05 (0.0332 s / it)
[22:05:35.320034] * Acc@1 73.980 Acc@5 98.270 loss 0.785
[22:05:35.320379] Accuracy of the network on the 10000 test images: 74.0%
[22:05:35.320852] Max accuracy: 73.98%
[22:05:35.701518] log_dir: ./output_dir
[22:05:36.594368] Epoch: [27]  [  0/781]  eta: 0:11:35  lr: 0.000218  training_loss: 1.6398 (1.6398)  mae_loss: 0.0346 (0.0346)  classification_loss: 1.5981 (1.5981)  loss_mask: 0.0071 (0.0071)  time: 0.8907  data: 0.6872  max mem: 5511
[22:05:40.542671] Epoch: [27]  [ 20/781]  eta: 0:02:55  lr: 0.000218  training_loss: 1.4968 (1.5201)  mae_loss: 0.0294 (0.0301)  classification_loss: 1.4548 (1.4835)  loss_mask: 0.0060 (0.0065)  time: 0.1973  data: 0.0002  max mem: 5511
[22:05:44.481361] Epoch: [27]  [ 40/781]  eta: 0:02:38  lr: 0.000218  training_loss: 1.5300 (1.5331)  mae_loss: 0.0292 (0.0300)  classification_loss: 1.4923 (1.4962)  loss_mask: 0.0063 (0.0069)  time: 0.1968  data: 0.0002  max mem: 5511
[22:05:48.396313] Epoch: [27]  [ 60/781]  eta: 0:02:29  lr: 0.000218  training_loss: 1.5178 (1.5274)  mae_loss: 0.0303 (0.0301)  classification_loss: 1.4782 (1.4882)  loss_mask: 0.0091 (0.0091)  time: 0.1957  data: 0.0002  max mem: 5511
[22:05:52.304469] Epoch: [27]  [ 80/781]  eta: 0:02:23  lr: 0.000218  training_loss: 1.5393 (1.5380)  mae_loss: 0.0303 (0.0301)  classification_loss: 1.4987 (1.4986)  loss_mask: 0.0079 (0.0093)  time: 0.1953  data: 0.0003  max mem: 5511
[22:05:56.211910] Epoch: [27]  [100/781]  eta: 0:02:18  lr: 0.000218  training_loss: 1.5342 (1.5327)  mae_loss: 0.0290 (0.0298)  classification_loss: 1.4888 (1.4938)  loss_mask: 0.0072 (0.0090)  time: 0.1953  data: 0.0003  max mem: 5511
[22:06:00.132393] Epoch: [27]  [120/781]  eta: 0:02:13  lr: 0.000218  training_loss: 1.4967 (1.5303)  mae_loss: 0.0296 (0.0298)  classification_loss: 1.4572 (1.4917)  loss_mask: 0.0069 (0.0087)  time: 0.1959  data: 0.0003  max mem: 5511
[22:06:04.050461] Epoch: [27]  [140/781]  eta: 0:02:08  lr: 0.000218  training_loss: 1.5117 (1.5309)  mae_loss: 0.0300 (0.0299)  classification_loss: 1.4772 (1.4922)  loss_mask: 0.0072 (0.0088)  time: 0.1958  data: 0.0003  max mem: 5511
[22:06:08.015902] Epoch: [27]  [160/781]  eta: 0:02:04  lr: 0.000218  training_loss: 1.4278 (1.5259)  mae_loss: 0.0311 (0.0300)  classification_loss: 1.3901 (1.4870)  loss_mask: 0.0081 (0.0088)  time: 0.1982  data: 0.0002  max mem: 5511
[22:06:11.975085] Epoch: [27]  [180/781]  eta: 0:02:00  lr: 0.000218  training_loss: 1.5336 (1.5248)  mae_loss: 0.0294 (0.0299)  classification_loss: 1.4888 (1.4864)  loss_mask: 0.0052 (0.0085)  time: 0.1979  data: 0.0003  max mem: 5511
[22:06:15.922492] Epoch: [27]  [200/781]  eta: 0:01:56  lr: 0.000218  training_loss: 1.5104 (1.5262)  mae_loss: 0.0290 (0.0298)  classification_loss: 1.4778 (1.4880)  loss_mask: 0.0065 (0.0084)  time: 0.1973  data: 0.0003  max mem: 5511
[22:06:19.877773] Epoch: [27]  [220/781]  eta: 0:01:52  lr: 0.000218  training_loss: 1.5802 (1.5279)  mae_loss: 0.0297 (0.0299)  classification_loss: 1.5389 (1.4894)  loss_mask: 0.0081 (0.0087)  time: 0.1977  data: 0.0003  max mem: 5511
[22:06:23.819690] Epoch: [27]  [240/781]  eta: 0:01:47  lr: 0.000218  training_loss: 1.5157 (1.5288)  mae_loss: 0.0300 (0.0299)  classification_loss: 1.4699 (1.4902)  loss_mask: 0.0083 (0.0088)  time: 0.1970  data: 0.0002  max mem: 5511
[22:06:27.751579] Epoch: [27]  [260/781]  eta: 0:01:43  lr: 0.000218  training_loss: 1.4771 (1.5258)  mae_loss: 0.0279 (0.0298)  classification_loss: 1.4410 (1.4873)  loss_mask: 0.0062 (0.0087)  time: 0.1965  data: 0.0002  max mem: 5511
[22:06:31.669454] Epoch: [27]  [280/781]  eta: 0:01:39  lr: 0.000217  training_loss: 1.5283 (1.5269)  mae_loss: 0.0298 (0.0298)  classification_loss: 1.4874 (1.4886)  loss_mask: 0.0064 (0.0086)  time: 0.1958  data: 0.0002  max mem: 5511
[22:06:35.620367] Epoch: [27]  [300/781]  eta: 0:01:35  lr: 0.000217  training_loss: 1.5733 (1.5296)  mae_loss: 0.0290 (0.0298)  classification_loss: 1.5390 (1.4913)  loss_mask: 0.0056 (0.0084)  time: 0.1974  data: 0.0003  max mem: 5511
[22:06:39.558887] Epoch: [27]  [320/781]  eta: 0:01:31  lr: 0.000217  training_loss: 1.5006 (1.5301)  mae_loss: 0.0295 (0.0298)  classification_loss: 1.4671 (1.4919)  loss_mask: 0.0069 (0.0084)  time: 0.1968  data: 0.0002  max mem: 5511
[22:06:43.496848] Epoch: [27]  [340/781]  eta: 0:01:27  lr: 0.000217  training_loss: 1.4660 (1.5286)  mae_loss: 0.0280 (0.0297)  classification_loss: 1.4240 (1.4899)  loss_mask: 0.0143 (0.0090)  time: 0.1968  data: 0.0002  max mem: 5511
[22:06:47.415953] Epoch: [27]  [360/781]  eta: 0:01:23  lr: 0.000217  training_loss: 1.5786 (1.5303)  mae_loss: 0.0291 (0.0297)  classification_loss: 1.4919 (1.4909)  loss_mask: 0.0187 (0.0097)  time: 0.1958  data: 0.0003  max mem: 5511
[22:06:51.323847] Epoch: [27]  [380/781]  eta: 0:01:19  lr: 0.000217  training_loss: 1.5527 (1.5319)  mae_loss: 0.0298 (0.0297)  classification_loss: 1.5119 (1.4925)  loss_mask: 0.0088 (0.0096)  time: 0.1953  data: 0.0002  max mem: 5511
[22:06:55.234392] Epoch: [27]  [400/781]  eta: 0:01:15  lr: 0.000217  training_loss: 1.4622 (1.5289)  mae_loss: 0.0307 (0.0298)  classification_loss: 1.4227 (1.4897)  loss_mask: 0.0062 (0.0094)  time: 0.1954  data: 0.0002  max mem: 5511
[22:06:59.285074] Epoch: [27]  [420/781]  eta: 0:01:11  lr: 0.000217  training_loss: 1.4868 (1.5290)  mae_loss: 0.0307 (0.0298)  classification_loss: 1.4489 (1.4899)  loss_mask: 0.0070 (0.0093)  time: 0.2024  data: 0.0003  max mem: 5511
[22:07:03.211129] Epoch: [27]  [440/781]  eta: 0:01:07  lr: 0.000217  training_loss: 1.4940 (1.5277)  mae_loss: 0.0286 (0.0298)  classification_loss: 1.4614 (1.4886)  loss_mask: 0.0047 (0.0092)  time: 0.1962  data: 0.0002  max mem: 5511
[22:07:07.187029] Epoch: [27]  [460/781]  eta: 0:01:03  lr: 0.000217  training_loss: 1.5413 (1.5276)  mae_loss: 0.0299 (0.0298)  classification_loss: 1.5043 (1.4886)  loss_mask: 0.0065 (0.0091)  time: 0.1987  data: 0.0002  max mem: 5511
[22:07:11.127359] Epoch: [27]  [480/781]  eta: 0:00:59  lr: 0.000217  training_loss: 1.5492 (1.5283)  mae_loss: 0.0295 (0.0298)  classification_loss: 1.5091 (1.4894)  loss_mask: 0.0056 (0.0090)  time: 0.1969  data: 0.0002  max mem: 5511
[22:07:15.078717] Epoch: [27]  [500/781]  eta: 0:00:55  lr: 0.000217  training_loss: 1.5164 (1.5277)  mae_loss: 0.0306 (0.0298)  classification_loss: 1.4742 (1.4888)  loss_mask: 0.0086 (0.0091)  time: 0.1975  data: 0.0002  max mem: 5511
[22:07:19.072055] Epoch: [27]  [520/781]  eta: 0:00:51  lr: 0.000217  training_loss: 1.5458 (1.5282)  mae_loss: 0.0313 (0.0299)  classification_loss: 1.5109 (1.4892)  loss_mask: 0.0082 (0.0091)  time: 0.1996  data: 0.0002  max mem: 5511
[22:07:22.985953] Epoch: [27]  [540/781]  eta: 0:00:47  lr: 0.000217  training_loss: 1.5476 (1.5286)  mae_loss: 0.0282 (0.0299)  classification_loss: 1.5140 (1.4896)  loss_mask: 0.0082 (0.0091)  time: 0.1956  data: 0.0002  max mem: 5511
[22:07:26.890581] Epoch: [27]  [560/781]  eta: 0:00:43  lr: 0.000216  training_loss: 1.4665 (1.5271)  mae_loss: 0.0291 (0.0298)  classification_loss: 1.4294 (1.4880)  loss_mask: 0.0100 (0.0093)  time: 0.1951  data: 0.0002  max mem: 5511
[22:07:30.805786] Epoch: [27]  [580/781]  eta: 0:00:39  lr: 0.000216  training_loss: 1.5303 (1.5277)  mae_loss: 0.0291 (0.0298)  classification_loss: 1.4969 (1.4885)  loss_mask: 0.0087 (0.0094)  time: 0.1957  data: 0.0002  max mem: 5511
[22:07:34.749019] Epoch: [27]  [600/781]  eta: 0:00:35  lr: 0.000216  training_loss: 1.4202 (1.5255)  mae_loss: 0.0280 (0.0298)  classification_loss: 1.3877 (1.4864)  loss_mask: 0.0072 (0.0093)  time: 0.1971  data: 0.0002  max mem: 5511
[22:07:38.701345] Epoch: [27]  [620/781]  eta: 0:00:31  lr: 0.000216  training_loss: 1.4842 (1.5247)  mae_loss: 0.0297 (0.0298)  classification_loss: 1.4437 (1.4857)  loss_mask: 0.0049 (0.0093)  time: 0.1975  data: 0.0002  max mem: 5511
[22:07:42.655399] Epoch: [27]  [640/781]  eta: 0:00:27  lr: 0.000216  training_loss: 1.4938 (1.5236)  mae_loss: 0.0294 (0.0298)  classification_loss: 1.4605 (1.4847)  loss_mask: 0.0054 (0.0092)  time: 0.1976  data: 0.0002  max mem: 5511
[22:07:46.614012] Epoch: [27]  [660/781]  eta: 0:00:23  lr: 0.000216  training_loss: 1.4972 (1.5230)  mae_loss: 0.0307 (0.0298)  classification_loss: 1.4568 (1.4839)  loss_mask: 0.0103 (0.0092)  time: 0.1978  data: 0.0002  max mem: 5511
[22:07:50.550562] Epoch: [27]  [680/781]  eta: 0:00:19  lr: 0.000216  training_loss: 1.5692 (1.5235)  mae_loss: 0.0298 (0.0298)  classification_loss: 1.5243 (1.4845)  loss_mask: 0.0067 (0.0092)  time: 0.1967  data: 0.0002  max mem: 5511
[22:07:54.508733] Epoch: [27]  [700/781]  eta: 0:00:16  lr: 0.000216  training_loss: 1.5238 (1.5245)  mae_loss: 0.0294 (0.0298)  classification_loss: 1.4914 (1.4855)  loss_mask: 0.0052 (0.0091)  time: 0.1978  data: 0.0002  max mem: 5511
[22:07:58.467357] Epoch: [27]  [720/781]  eta: 0:00:12  lr: 0.000216  training_loss: 1.4916 (1.5244)  mae_loss: 0.0284 (0.0298)  classification_loss: 1.4574 (1.4855)  loss_mask: 0.0064 (0.0091)  time: 0.1978  data: 0.0002  max mem: 5511
[22:08:02.387144] Epoch: [27]  [740/781]  eta: 0:00:08  lr: 0.000216  training_loss: 1.4227 (1.5225)  mae_loss: 0.0298 (0.0298)  classification_loss: 1.3806 (1.4837)  loss_mask: 0.0056 (0.0090)  time: 0.1959  data: 0.0002  max mem: 5511
[22:08:06.324631] Epoch: [27]  [760/781]  eta: 0:00:04  lr: 0.000216  training_loss: 1.5247 (1.5227)  mae_loss: 0.0281 (0.0298)  classification_loss: 1.4841 (1.4840)  loss_mask: 0.0048 (0.0089)  time: 0.1968  data: 0.0002  max mem: 5511
[22:08:10.272503] Epoch: [27]  [780/781]  eta: 0:00:00  lr: 0.000216  training_loss: 1.5608 (1.5233)  mae_loss: 0.0281 (0.0298)  classification_loss: 1.5246 (1.4846)  loss_mask: 0.0096 (0.0090)  time: 0.1973  data: 0.0001  max mem: 5511
[22:08:10.421752] Epoch: [27] Total time: 0:02:34 (0.1981 s / it)
[22:08:10.422193] Averaged stats: lr: 0.000216  training_loss: 1.5608 (1.5233)  mae_loss: 0.0281 (0.0298)  classification_loss: 1.5246 (1.4846)  loss_mask: 0.0096 (0.0090)
[22:08:11.017695] Test:  [  0/157]  eta: 0:01:32  testing_loss: 0.8328 (0.8328)  acc1: 76.5625 (76.5625)  acc5: 98.4375 (98.4375)  time: 0.5915  data: 0.5616  max mem: 5511
[22:08:11.316431] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.8328 (0.8462)  acc1: 75.0000 (74.2898)  acc5: 98.4375 (98.2955)  time: 0.0808  data: 0.0521  max mem: 5511
[22:08:11.600034] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.7574 (0.8073)  acc1: 75.0000 (75.6696)  acc5: 98.4375 (98.5119)  time: 0.0290  data: 0.0007  max mem: 5511
[22:08:11.885814] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.7677 (0.8118)  acc1: 75.0000 (75.1008)  acc5: 98.4375 (98.0343)  time: 0.0283  data: 0.0002  max mem: 5511
[22:08:12.171807] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.7962 (0.8142)  acc1: 73.4375 (74.4665)  acc5: 96.8750 (97.9802)  time: 0.0285  data: 0.0002  max mem: 5511
[22:08:12.464195] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.7949 (0.8079)  acc1: 75.0000 (74.5098)  acc5: 98.4375 (97.9779)  time: 0.0287  data: 0.0002  max mem: 5511
[22:08:12.762145] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.8053 (0.8061)  acc1: 73.4375 (74.2316)  acc5: 98.4375 (98.0277)  time: 0.0293  data: 0.0005  max mem: 5511
[22:08:13.050773] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7405 (0.7951)  acc1: 75.0000 (74.6919)  acc5: 98.4375 (98.0634)  time: 0.0291  data: 0.0006  max mem: 5511
[22:08:13.336627] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7584 (0.8060)  acc1: 75.0000 (74.0548)  acc5: 98.4375 (98.0517)  time: 0.0285  data: 0.0002  max mem: 5511
[22:08:13.639599] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.8087 (0.8047)  acc1: 71.8750 (74.0556)  acc5: 98.4375 (98.1113)  time: 0.0293  data: 0.0003  max mem: 5511
[22:08:13.926550] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.7998 (0.8092)  acc1: 73.4375 (73.8552)  acc5: 98.4375 (98.1436)  time: 0.0293  data: 0.0003  max mem: 5511
[22:08:14.211232] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.8257 (0.8070)  acc1: 71.8750 (73.9583)  acc5: 98.4375 (98.1700)  time: 0.0285  data: 0.0002  max mem: 5511
[22:08:14.499339] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.7712 (0.8031)  acc1: 73.4375 (74.0444)  acc5: 98.4375 (98.1792)  time: 0.0285  data: 0.0002  max mem: 5511
[22:08:14.783511] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.7712 (0.8045)  acc1: 73.4375 (73.9742)  acc5: 98.4375 (98.2109)  time: 0.0285  data: 0.0002  max mem: 5511
[22:08:15.067121] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.8053 (0.8051)  acc1: 71.8750 (74.0137)  acc5: 98.4375 (98.1937)  time: 0.0283  data: 0.0002  max mem: 5511
[22:08:15.350072] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.8053 (0.8024)  acc1: 73.4375 (74.0170)  acc5: 98.4375 (98.1788)  time: 0.0282  data: 0.0002  max mem: 5511
[22:08:15.501263] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7947 (0.8036)  acc1: 75.0000 (73.9900)  acc5: 98.4375 (98.1700)  time: 0.0272  data: 0.0001  max mem: 5511
[22:08:15.657128] Test: Total time: 0:00:05 (0.0333 s / it)
[22:08:15.657602] * Acc@1 73.990 Acc@5 98.170 loss 0.804
[22:08:15.657911] Accuracy of the network on the 10000 test images: 74.0%
[22:08:15.658106] Max accuracy: 73.99%
[22:08:16.012895] log_dir: ./output_dir
[22:08:16.863218] Epoch: [28]  [  0/781]  eta: 0:11:02  lr: 0.000216  training_loss: 1.6844 (1.6844)  mae_loss: 0.0317 (0.0317)  classification_loss: 1.6380 (1.6380)  loss_mask: 0.0147 (0.0147)  time: 0.8485  data: 0.6210  max mem: 5511
[22:08:20.791917] Epoch: [28]  [ 20/781]  eta: 0:02:53  lr: 0.000216  training_loss: 1.5290 (1.5514)  mae_loss: 0.0313 (0.0308)  classification_loss: 1.4859 (1.5063)  loss_mask: 0.0074 (0.0143)  time: 0.1963  data: 0.0002  max mem: 5511
[22:08:24.710120] Epoch: [28]  [ 40/781]  eta: 0:02:37  lr: 0.000216  training_loss: 1.5111 (1.5476)  mae_loss: 0.0290 (0.0304)  classification_loss: 1.4656 (1.5045)  loss_mask: 0.0078 (0.0127)  time: 0.1958  data: 0.0002  max mem: 5511
[22:08:28.635415] Epoch: [28]  [ 60/781]  eta: 0:02:29  lr: 0.000215  training_loss: 1.4630 (1.5371)  mae_loss: 0.0286 (0.0300)  classification_loss: 1.4284 (1.4954)  loss_mask: 0.0070 (0.0117)  time: 0.1962  data: 0.0002  max mem: 5511
[22:08:32.564945] Epoch: [28]  [ 80/781]  eta: 0:02:23  lr: 0.000215  training_loss: 1.5010 (1.5289)  mae_loss: 0.0289 (0.0298)  classification_loss: 1.4660 (1.4885)  loss_mask: 0.0059 (0.0106)  time: 0.1964  data: 0.0002  max mem: 5511
[22:08:36.498984] Epoch: [28]  [100/781]  eta: 0:02:18  lr: 0.000215  training_loss: 1.5159 (1.5262)  mae_loss: 0.0296 (0.0298)  classification_loss: 1.4861 (1.4867)  loss_mask: 0.0057 (0.0097)  time: 0.1966  data: 0.0002  max mem: 5511
[22:08:40.424247] Epoch: [28]  [120/781]  eta: 0:02:13  lr: 0.000215  training_loss: 1.5019 (1.5215)  mae_loss: 0.0287 (0.0296)  classification_loss: 1.4667 (1.4830)  loss_mask: 0.0050 (0.0089)  time: 0.1962  data: 0.0002  max mem: 5511
[22:08:44.404514] Epoch: [28]  [140/781]  eta: 0:02:09  lr: 0.000215  training_loss: 1.5207 (1.5214)  mae_loss: 0.0295 (0.0296)  classification_loss: 1.4884 (1.4833)  loss_mask: 0.0060 (0.0085)  time: 0.1989  data: 0.0002  max mem: 5511
[22:08:48.312237] Epoch: [28]  [160/781]  eta: 0:02:04  lr: 0.000215  training_loss: 1.5218 (1.5233)  mae_loss: 0.0282 (0.0296)  classification_loss: 1.4946 (1.4857)  loss_mask: 0.0048 (0.0081)  time: 0.1953  data: 0.0003  max mem: 5511
[22:08:52.225964] Epoch: [28]  [180/781]  eta: 0:02:00  lr: 0.000215  training_loss: 1.5543 (1.5240)  mae_loss: 0.0298 (0.0297)  classification_loss: 1.5163 (1.4864)  loss_mask: 0.0036 (0.0078)  time: 0.1956  data: 0.0002  max mem: 5511
[22:08:56.141311] Epoch: [28]  [200/781]  eta: 0:01:55  lr: 0.000215  training_loss: 1.5265 (1.5225)  mae_loss: 0.0291 (0.0297)  classification_loss: 1.4889 (1.4852)  loss_mask: 0.0059 (0.0077)  time: 0.1957  data: 0.0002  max mem: 5511
[22:09:00.075669] Epoch: [28]  [220/781]  eta: 0:01:51  lr: 0.000215  training_loss: 1.4942 (1.5212)  mae_loss: 0.0296 (0.0296)  classification_loss: 1.4580 (1.4839)  loss_mask: 0.0054 (0.0076)  time: 0.1966  data: 0.0002  max mem: 5511
[22:09:03.983959] Epoch: [28]  [240/781]  eta: 0:01:47  lr: 0.000215  training_loss: 1.5644 (1.5250)  mae_loss: 0.0296 (0.0297)  classification_loss: 1.5205 (1.4877)  loss_mask: 0.0057 (0.0076)  time: 0.1953  data: 0.0003  max mem: 5511
[22:09:07.929400] Epoch: [28]  [260/781]  eta: 0:01:43  lr: 0.000215  training_loss: 1.5140 (1.5242)  mae_loss: 0.0294 (0.0297)  classification_loss: 1.4718 (1.4865)  loss_mask: 0.0103 (0.0080)  time: 0.1972  data: 0.0003  max mem: 5511
[22:09:11.881788] Epoch: [28]  [280/781]  eta: 0:01:39  lr: 0.000215  training_loss: 1.5353 (1.5243)  mae_loss: 0.0308 (0.0297)  classification_loss: 1.4891 (1.4864)  loss_mask: 0.0078 (0.0082)  time: 0.1975  data: 0.0002  max mem: 5511
[22:09:15.855960] Epoch: [28]  [300/781]  eta: 0:01:35  lr: 0.000215  training_loss: 1.5163 (1.5243)  mae_loss: 0.0295 (0.0297)  classification_loss: 1.4831 (1.4861)  loss_mask: 0.0129 (0.0086)  time: 0.1986  data: 0.0002  max mem: 5511
[22:09:19.782884] Epoch: [28]  [320/781]  eta: 0:01:31  lr: 0.000215  training_loss: 1.5221 (1.5229)  mae_loss: 0.0277 (0.0297)  classification_loss: 1.4855 (1.4847)  loss_mask: 0.0080 (0.0086)  time: 0.1963  data: 0.0002  max mem: 5511
[22:09:23.713930] Epoch: [28]  [340/781]  eta: 0:01:27  lr: 0.000214  training_loss: 1.4918 (1.5222)  mae_loss: 0.0308 (0.0298)  classification_loss: 1.4530 (1.4840)  loss_mask: 0.0055 (0.0085)  time: 0.1965  data: 0.0002  max mem: 5511
[22:09:27.683450] Epoch: [28]  [360/781]  eta: 0:01:23  lr: 0.000214  training_loss: 1.4776 (1.5219)  mae_loss: 0.0291 (0.0297)  classification_loss: 1.4443 (1.4839)  loss_mask: 0.0054 (0.0083)  time: 0.1984  data: 0.0002  max mem: 5511
[22:09:31.625547] Epoch: [28]  [380/781]  eta: 0:01:19  lr: 0.000214  training_loss: 1.5257 (1.5221)  mae_loss: 0.0298 (0.0297)  classification_loss: 1.4888 (1.4842)  loss_mask: 0.0051 (0.0082)  time: 0.1970  data: 0.0002  max mem: 5511
[22:09:35.551362] Epoch: [28]  [400/781]  eta: 0:01:15  lr: 0.000214  training_loss: 1.4852 (1.5214)  mae_loss: 0.0314 (0.0298)  classification_loss: 1.4328 (1.4834)  loss_mask: 0.0063 (0.0081)  time: 0.1962  data: 0.0003  max mem: 5511
[22:09:39.460100] Epoch: [28]  [420/781]  eta: 0:01:11  lr: 0.000214  training_loss: 1.4769 (1.5211)  mae_loss: 0.0287 (0.0298)  classification_loss: 1.4384 (1.4830)  loss_mask: 0.0112 (0.0083)  time: 0.1954  data: 0.0002  max mem: 5511
[22:09:43.386729] Epoch: [28]  [440/781]  eta: 0:01:07  lr: 0.000214  training_loss: 1.5666 (1.5231)  mae_loss: 0.0290 (0.0298)  classification_loss: 1.5337 (1.4850)  loss_mask: 0.0072 (0.0083)  time: 0.1963  data: 0.0002  max mem: 5511
[22:09:47.300939] Epoch: [28]  [460/781]  eta: 0:01:03  lr: 0.000214  training_loss: 1.4738 (1.5221)  mae_loss: 0.0309 (0.0298)  classification_loss: 1.4407 (1.4837)  loss_mask: 0.0104 (0.0086)  time: 0.1956  data: 0.0002  max mem: 5511
[22:09:51.229330] Epoch: [28]  [480/781]  eta: 0:00:59  lr: 0.000214  training_loss: 1.5416 (1.5219)  mae_loss: 0.0292 (0.0298)  classification_loss: 1.4909 (1.4834)  loss_mask: 0.0104 (0.0087)  time: 0.1963  data: 0.0002  max mem: 5511
[22:09:55.181682] Epoch: [28]  [500/781]  eta: 0:00:55  lr: 0.000214  training_loss: 1.5454 (1.5228)  mae_loss: 0.0289 (0.0297)  classification_loss: 1.5067 (1.4843)  loss_mask: 0.0084 (0.0088)  time: 0.1975  data: 0.0003  max mem: 5511
[22:09:59.095270] Epoch: [28]  [520/781]  eta: 0:00:51  lr: 0.000214  training_loss: 1.5153 (1.5230)  mae_loss: 0.0293 (0.0298)  classification_loss: 1.4778 (1.4845)  loss_mask: 0.0067 (0.0087)  time: 0.1956  data: 0.0002  max mem: 5511
[22:10:03.032566] Epoch: [28]  [540/781]  eta: 0:00:47  lr: 0.000214  training_loss: 1.5244 (1.5237)  mae_loss: 0.0285 (0.0297)  classification_loss: 1.4836 (1.4852)  loss_mask: 0.0101 (0.0088)  time: 0.1968  data: 0.0002  max mem: 5511
[22:10:06.980675] Epoch: [28]  [560/781]  eta: 0:00:43  lr: 0.000214  training_loss: 1.4960 (1.5227)  mae_loss: 0.0288 (0.0297)  classification_loss: 1.4593 (1.4842)  loss_mask: 0.0089 (0.0088)  time: 0.1973  data: 0.0002  max mem: 5511
[22:10:10.897682] Epoch: [28]  [580/781]  eta: 0:00:39  lr: 0.000214  training_loss: 1.5446 (1.5231)  mae_loss: 0.0284 (0.0297)  classification_loss: 1.5073 (1.4846)  loss_mask: 0.0067 (0.0088)  time: 0.1958  data: 0.0002  max mem: 5511
[22:10:14.794341] Epoch: [28]  [600/781]  eta: 0:00:35  lr: 0.000213  training_loss: 1.5331 (1.5239)  mae_loss: 0.0278 (0.0297)  classification_loss: 1.4959 (1.4854)  loss_mask: 0.0073 (0.0088)  time: 0.1947  data: 0.0003  max mem: 5511
[22:10:18.716268] Epoch: [28]  [620/781]  eta: 0:00:31  lr: 0.000213  training_loss: 1.4763 (1.5226)  mae_loss: 0.0299 (0.0297)  classification_loss: 1.4427 (1.4841)  loss_mask: 0.0082 (0.0088)  time: 0.1960  data: 0.0002  max mem: 5511
[22:10:22.639755] Epoch: [28]  [640/781]  eta: 0:00:27  lr: 0.000213  training_loss: 1.5074 (1.5222)  mae_loss: 0.0296 (0.0297)  classification_loss: 1.4678 (1.4838)  loss_mask: 0.0055 (0.0088)  time: 0.1961  data: 0.0002  max mem: 5511
[22:10:26.598196] Epoch: [28]  [660/781]  eta: 0:00:23  lr: 0.000213  training_loss: 1.5310 (1.5229)  mae_loss: 0.0289 (0.0297)  classification_loss: 1.4995 (1.4845)  loss_mask: 0.0058 (0.0087)  time: 0.1978  data: 0.0002  max mem: 5511
[22:10:30.513040] Epoch: [28]  [680/781]  eta: 0:00:19  lr: 0.000213  training_loss: 1.5188 (1.5238)  mae_loss: 0.0305 (0.0297)  classification_loss: 1.4828 (1.4855)  loss_mask: 0.0043 (0.0086)  time: 0.1957  data: 0.0002  max mem: 5511
[22:10:34.448107] Epoch: [28]  [700/781]  eta: 0:00:15  lr: 0.000213  training_loss: 1.5260 (1.5249)  mae_loss: 0.0299 (0.0297)  classification_loss: 1.4880 (1.4866)  loss_mask: 0.0059 (0.0086)  time: 0.1967  data: 0.0003  max mem: 5511
[22:10:38.377803] Epoch: [28]  [720/781]  eta: 0:00:12  lr: 0.000213  training_loss: 1.5172 (1.5254)  mae_loss: 0.0289 (0.0297)  classification_loss: 1.4790 (1.4872)  loss_mask: 0.0064 (0.0085)  time: 0.1964  data: 0.0002  max mem: 5511
[22:10:42.305737] Epoch: [28]  [740/781]  eta: 0:00:08  lr: 0.000213  training_loss: 1.5302 (1.5254)  mae_loss: 0.0285 (0.0297)  classification_loss: 1.4984 (1.4872)  loss_mask: 0.0066 (0.0085)  time: 0.1963  data: 0.0003  max mem: 5511
[22:10:46.211653] Epoch: [28]  [760/781]  eta: 0:00:04  lr: 0.000213  training_loss: 1.5383 (1.5260)  mae_loss: 0.0301 (0.0297)  classification_loss: 1.4975 (1.4877)  loss_mask: 0.0067 (0.0085)  time: 0.1952  data: 0.0002  max mem: 5511
[22:10:50.167108] Epoch: [28]  [780/781]  eta: 0:00:00  lr: 0.000213  training_loss: 1.4767 (1.5255)  mae_loss: 0.0283 (0.0297)  classification_loss: 1.4493 (1.4873)  loss_mask: 0.0058 (0.0085)  time: 0.1977  data: 0.0002  max mem: 5511
[22:10:50.320406] Epoch: [28] Total time: 0:02:34 (0.1976 s / it)
[22:10:50.321124] Averaged stats: lr: 0.000213  training_loss: 1.4767 (1.5255)  mae_loss: 0.0283 (0.0297)  classification_loss: 1.4493 (1.4873)  loss_mask: 0.0058 (0.0085)
[22:10:50.949472] Test:  [  0/157]  eta: 0:01:37  testing_loss: 0.6847 (0.6847)  acc1: 78.1250 (78.1250)  acc5: 98.4375 (98.4375)  time: 0.6238  data: 0.5930  max mem: 5511
[22:10:51.240995] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.8169 (0.8177)  acc1: 71.8750 (72.8693)  acc5: 98.4375 (98.7216)  time: 0.0830  data: 0.0545  max mem: 5511
[22:10:51.527098] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.7784 (0.7866)  acc1: 73.4375 (75.0000)  acc5: 98.4375 (98.5863)  time: 0.0287  data: 0.0004  max mem: 5511
[22:10:51.817531] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.7799 (0.7965)  acc1: 75.0000 (74.4960)  acc5: 98.4375 (98.3367)  time: 0.0287  data: 0.0002  max mem: 5511
[22:10:52.102801] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.7799 (0.7951)  acc1: 75.0000 (74.8095)  acc5: 98.4375 (98.3232)  time: 0.0286  data: 0.0002  max mem: 5511
[22:10:52.388920] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.7558 (0.7836)  acc1: 75.0000 (75.1225)  acc5: 98.4375 (98.2230)  time: 0.0284  data: 0.0003  max mem: 5511
[22:10:52.675334] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.7593 (0.7835)  acc1: 73.4375 (74.6158)  acc5: 96.8750 (98.1814)  time: 0.0285  data: 0.0003  max mem: 5511
[22:10:52.959993] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7292 (0.7702)  acc1: 75.0000 (75.0000)  acc5: 98.4375 (98.2394)  time: 0.0284  data: 0.0002  max mem: 5511
[22:10:53.246707] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7292 (0.7784)  acc1: 75.0000 (74.5563)  acc5: 98.4375 (98.2253)  time: 0.0284  data: 0.0002  max mem: 5511
[22:10:53.546735] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.8027 (0.7797)  acc1: 71.8750 (74.3990)  acc5: 98.4375 (98.2315)  time: 0.0292  data: 0.0002  max mem: 5511
[22:10:53.840168] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.8022 (0.7839)  acc1: 71.8750 (74.2110)  acc5: 98.4375 (98.1900)  time: 0.0295  data: 0.0002  max mem: 5511
[22:10:54.127530] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.8025 (0.7854)  acc1: 73.4375 (74.1413)  acc5: 98.4375 (98.1982)  time: 0.0289  data: 0.0003  max mem: 5511
[22:10:54.417501] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.7670 (0.7802)  acc1: 73.4375 (74.2769)  acc5: 98.4375 (98.1921)  time: 0.0287  data: 0.0002  max mem: 5511
[22:10:54.704117] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.7469 (0.7808)  acc1: 73.4375 (74.0816)  acc5: 98.4375 (98.1990)  time: 0.0287  data: 0.0002  max mem: 5511
[22:10:54.988485] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7469 (0.7791)  acc1: 71.8750 (74.1467)  acc5: 98.4375 (98.2048)  time: 0.0284  data: 0.0002  max mem: 5511
[22:10:55.271257] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7422 (0.7777)  acc1: 75.0000 (74.1825)  acc5: 98.4375 (98.2099)  time: 0.0282  data: 0.0002  max mem: 5511
[22:10:55.424379] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7422 (0.7788)  acc1: 75.0000 (74.1900)  acc5: 98.4375 (98.2200)  time: 0.0273  data: 0.0001  max mem: 5511
[22:10:55.585013] Test: Total time: 0:00:05 (0.0335 s / it)
[22:10:55.586239] * Acc@1 74.190 Acc@5 98.220 loss 0.779
[22:10:55.586529] Accuracy of the network on the 10000 test images: 74.2%
[22:10:55.586704] Max accuracy: 74.19%
[22:10:55.965421] log_dir: ./output_dir
[22:10:56.793778] Epoch: [29]  [  0/781]  eta: 0:10:45  lr: 0.000213  training_loss: 1.3733 (1.3733)  mae_loss: 0.0309 (0.0309)  classification_loss: 1.3365 (1.3365)  loss_mask: 0.0058 (0.0058)  time: 0.8266  data: 0.6083  max mem: 5511
[22:11:00.802134] Epoch: [29]  [ 20/781]  eta: 0:02:55  lr: 0.000213  training_loss: 1.4476 (1.4804)  mae_loss: 0.0301 (0.0306)  classification_loss: 1.4061 (1.4371)  loss_mask: 0.0078 (0.0127)  time: 0.2003  data: 0.0002  max mem: 5511
[22:11:04.790636] Epoch: [29]  [ 40/781]  eta: 0:02:39  lr: 0.000213  training_loss: 1.5166 (1.5097)  mae_loss: 0.0302 (0.0304)  classification_loss: 1.4773 (1.4694)  loss_mask: 0.0066 (0.0099)  time: 0.1993  data: 0.0002  max mem: 5511
[22:11:08.747549] Epoch: [29]  [ 60/781]  eta: 0:02:30  lr: 0.000213  training_loss: 1.4825 (1.5055)  mae_loss: 0.0294 (0.0304)  classification_loss: 1.4451 (1.4666)  loss_mask: 0.0055 (0.0085)  time: 0.1978  data: 0.0002  max mem: 5511
[22:11:12.686007] Epoch: [29]  [ 80/781]  eta: 0:02:24  lr: 0.000213  training_loss: 1.5151 (1.5038)  mae_loss: 0.0296 (0.0304)  classification_loss: 1.4633 (1.4653)  loss_mask: 0.0058 (0.0081)  time: 0.1968  data: 0.0003  max mem: 5511
[22:11:16.624930] Epoch: [29]  [100/781]  eta: 0:02:19  lr: 0.000212  training_loss: 1.4897 (1.5033)  mae_loss: 0.0285 (0.0302)  classification_loss: 1.4593 (1.4654)  loss_mask: 0.0056 (0.0078)  time: 0.1968  data: 0.0003  max mem: 5511
[22:11:20.579520] Epoch: [29]  [120/781]  eta: 0:02:14  lr: 0.000212  training_loss: 1.5277 (1.5112)  mae_loss: 0.0293 (0.0299)  classification_loss: 1.4931 (1.4735)  loss_mask: 0.0065 (0.0078)  time: 0.1976  data: 0.0002  max mem: 5511
[22:11:24.529190] Epoch: [29]  [140/781]  eta: 0:02:09  lr: 0.000212  training_loss: 1.4452 (1.5029)  mae_loss: 0.0289 (0.0298)  classification_loss: 1.4090 (1.4651)  loss_mask: 0.0079 (0.0079)  time: 0.1974  data: 0.0003  max mem: 5511
[22:11:28.448498] Epoch: [29]  [160/781]  eta: 0:02:05  lr: 0.000212  training_loss: 1.4591 (1.4997)  mae_loss: 0.0279 (0.0296)  classification_loss: 1.4247 (1.4617)  loss_mask: 0.0079 (0.0084)  time: 0.1959  data: 0.0002  max mem: 5511
[22:11:32.372076] Epoch: [29]  [180/781]  eta: 0:02:00  lr: 0.000212  training_loss: 1.5005 (1.5005)  mae_loss: 0.0298 (0.0297)  classification_loss: 1.4510 (1.4622)  loss_mask: 0.0076 (0.0086)  time: 0.1961  data: 0.0002  max mem: 5511
[22:11:36.317424] Epoch: [29]  [200/781]  eta: 0:01:56  lr: 0.000212  training_loss: 1.5183 (1.5006)  mae_loss: 0.0309 (0.0298)  classification_loss: 1.4818 (1.4625)  loss_mask: 0.0038 (0.0083)  time: 0.1970  data: 0.0002  max mem: 5511
[22:11:40.252863] Epoch: [29]  [220/781]  eta: 0:01:52  lr: 0.000212  training_loss: 1.4375 (1.4939)  mae_loss: 0.0265 (0.0296)  classification_loss: 1.4015 (1.4561)  loss_mask: 0.0055 (0.0081)  time: 0.1967  data: 0.0002  max mem: 5511
[22:11:44.203923] Epoch: [29]  [240/781]  eta: 0:01:48  lr: 0.000212  training_loss: 1.4668 (1.4926)  mae_loss: 0.0306 (0.0297)  classification_loss: 1.4285 (1.4544)  loss_mask: 0.0109 (0.0085)  time: 0.1975  data: 0.0005  max mem: 5511
[22:11:48.115317] Epoch: [29]  [260/781]  eta: 0:01:44  lr: 0.000212  training_loss: 1.4609 (1.4918)  mae_loss: 0.0292 (0.0297)  classification_loss: 1.4130 (1.4536)  loss_mask: 0.0061 (0.0085)  time: 0.1955  data: 0.0002  max mem: 5511
[22:11:52.054839] Epoch: [29]  [280/781]  eta: 0:01:39  lr: 0.000212  training_loss: 1.5181 (1.4946)  mae_loss: 0.0297 (0.0297)  classification_loss: 1.4771 (1.4565)  loss_mask: 0.0061 (0.0084)  time: 0.1969  data: 0.0002  max mem: 5511
[22:11:55.985013] Epoch: [29]  [300/781]  eta: 0:01:35  lr: 0.000212  training_loss: 1.5215 (1.4966)  mae_loss: 0.0296 (0.0298)  classification_loss: 1.4794 (1.4585)  loss_mask: 0.0074 (0.0084)  time: 0.1964  data: 0.0002  max mem: 5511
[22:11:59.925282] Epoch: [29]  [320/781]  eta: 0:01:31  lr: 0.000212  training_loss: 1.4606 (1.4960)  mae_loss: 0.0283 (0.0297)  classification_loss: 1.4246 (1.4579)  loss_mask: 0.0083 (0.0084)  time: 0.1969  data: 0.0002  max mem: 5511
[22:12:03.909824] Epoch: [29]  [340/781]  eta: 0:01:27  lr: 0.000212  training_loss: 1.4390 (1.4949)  mae_loss: 0.0305 (0.0298)  classification_loss: 1.4002 (1.4569)  loss_mask: 0.0053 (0.0083)  time: 0.1991  data: 0.0002  max mem: 5511
[22:12:07.849039] Epoch: [29]  [360/781]  eta: 0:01:23  lr: 0.000211  training_loss: 1.5308 (1.4969)  mae_loss: 0.0301 (0.0298)  classification_loss: 1.4931 (1.4589)  loss_mask: 0.0067 (0.0083)  time: 0.1969  data: 0.0002  max mem: 5511
[22:12:11.761215] Epoch: [29]  [380/781]  eta: 0:01:19  lr: 0.000211  training_loss: 1.5397 (1.4990)  mae_loss: 0.0293 (0.0298)  classification_loss: 1.5073 (1.4610)  loss_mask: 0.0055 (0.0082)  time: 0.1955  data: 0.0002  max mem: 5511
[22:12:15.676148] Epoch: [29]  [400/781]  eta: 0:01:15  lr: 0.000211  training_loss: 1.4968 (1.4993)  mae_loss: 0.0276 (0.0297)  classification_loss: 1.4636 (1.4615)  loss_mask: 0.0056 (0.0081)  time: 0.1957  data: 0.0003  max mem: 5511
[22:12:19.581746] Epoch: [29]  [420/781]  eta: 0:01:11  lr: 0.000211  training_loss: 1.4857 (1.4988)  mae_loss: 0.0290 (0.0297)  classification_loss: 1.4512 (1.4611)  loss_mask: 0.0044 (0.0080)  time: 0.1952  data: 0.0003  max mem: 5511
[22:12:23.521795] Epoch: [29]  [440/781]  eta: 0:01:07  lr: 0.000211  training_loss: 1.4634 (1.4986)  mae_loss: 0.0298 (0.0297)  classification_loss: 1.4223 (1.4610)  loss_mask: 0.0068 (0.0079)  time: 0.1969  data: 0.0003  max mem: 5511
[22:12:27.445933] Epoch: [29]  [460/781]  eta: 0:01:03  lr: 0.000211  training_loss: 1.4649 (1.4965)  mae_loss: 0.0276 (0.0297)  classification_loss: 1.4279 (1.4589)  loss_mask: 0.0048 (0.0079)  time: 0.1961  data: 0.0002  max mem: 5511
[22:12:31.406702] Epoch: [29]  [480/781]  eta: 0:00:59  lr: 0.000211  training_loss: 1.5013 (1.4976)  mae_loss: 0.0278 (0.0296)  classification_loss: 1.4585 (1.4601)  loss_mask: 0.0051 (0.0078)  time: 0.1979  data: 0.0002  max mem: 5511
[22:12:35.331916] Epoch: [29]  [500/781]  eta: 0:00:55  lr: 0.000211  training_loss: 1.4470 (1.4974)  mae_loss: 0.0290 (0.0296)  classification_loss: 1.4114 (1.4600)  loss_mask: 0.0057 (0.0077)  time: 0.1962  data: 0.0003  max mem: 5511
[22:12:39.288080] Epoch: [29]  [520/781]  eta: 0:00:51  lr: 0.000211  training_loss: 1.4906 (1.4970)  mae_loss: 0.0287 (0.0296)  classification_loss: 1.4542 (1.4595)  loss_mask: 0.0085 (0.0078)  time: 0.1977  data: 0.0003  max mem: 5511
[22:12:43.239893] Epoch: [29]  [540/781]  eta: 0:00:47  lr: 0.000211  training_loss: 1.4521 (1.4969)  mae_loss: 0.0296 (0.0296)  classification_loss: 1.4155 (1.4594)  loss_mask: 0.0081 (0.0079)  time: 0.1975  data: 0.0002  max mem: 5511
[22:12:47.158962] Epoch: [29]  [560/781]  eta: 0:00:43  lr: 0.000211  training_loss: 1.4638 (1.4958)  mae_loss: 0.0291 (0.0296)  classification_loss: 1.4318 (1.4584)  loss_mask: 0.0049 (0.0078)  time: 0.1959  data: 0.0003  max mem: 5511
[22:12:51.076828] Epoch: [29]  [580/781]  eta: 0:00:39  lr: 0.000211  training_loss: 1.4969 (1.4956)  mae_loss: 0.0298 (0.0296)  classification_loss: 1.4529 (1.4583)  loss_mask: 0.0043 (0.0077)  time: 0.1958  data: 0.0002  max mem: 5511
[22:12:54.989942] Epoch: [29]  [600/781]  eta: 0:00:35  lr: 0.000211  training_loss: 1.4799 (1.4957)  mae_loss: 0.0285 (0.0296)  classification_loss: 1.4251 (1.4583)  loss_mask: 0.0083 (0.0077)  time: 0.1956  data: 0.0002  max mem: 5511
[22:12:58.935857] Epoch: [29]  [620/781]  eta: 0:00:31  lr: 0.000210  training_loss: 1.4913 (1.4958)  mae_loss: 0.0293 (0.0296)  classification_loss: 1.4579 (1.4584)  loss_mask: 0.0082 (0.0078)  time: 0.1972  data: 0.0002  max mem: 5511
[22:13:02.838218] Epoch: [29]  [640/781]  eta: 0:00:27  lr: 0.000210  training_loss: 1.4665 (1.4963)  mae_loss: 0.0307 (0.0297)  classification_loss: 1.4326 (1.4589)  loss_mask: 0.0054 (0.0077)  time: 0.1950  data: 0.0002  max mem: 5511
[22:13:06.748291] Epoch: [29]  [660/781]  eta: 0:00:23  lr: 0.000210  training_loss: 1.5175 (1.4965)  mae_loss: 0.0292 (0.0296)  classification_loss: 1.4820 (1.4591)  loss_mask: 0.0071 (0.0078)  time: 0.1954  data: 0.0002  max mem: 5511
[22:13:10.726786] Epoch: [29]  [680/781]  eta: 0:00:19  lr: 0.000210  training_loss: 1.5131 (1.4976)  mae_loss: 0.0279 (0.0296)  classification_loss: 1.4825 (1.4601)  loss_mask: 0.0051 (0.0078)  time: 0.1988  data: 0.0003  max mem: 5511
[22:13:14.654620] Epoch: [29]  [700/781]  eta: 0:00:16  lr: 0.000210  training_loss: 1.4791 (1.4978)  mae_loss: 0.0297 (0.0296)  classification_loss: 1.4462 (1.4604)  loss_mask: 0.0042 (0.0077)  time: 0.1963  data: 0.0002  max mem: 5511
[22:13:18.610114] Epoch: [29]  [720/781]  eta: 0:00:12  lr: 0.000210  training_loss: 1.4792 (1.4975)  mae_loss: 0.0286 (0.0296)  classification_loss: 1.4467 (1.4602)  loss_mask: 0.0049 (0.0077)  time: 0.1977  data: 0.0002  max mem: 5511
[22:13:22.544513] Epoch: [29]  [740/781]  eta: 0:00:08  lr: 0.000210  training_loss: 1.4970 (1.4976)  mae_loss: 0.0297 (0.0296)  classification_loss: 1.4493 (1.4603)  loss_mask: 0.0078 (0.0077)  time: 0.1966  data: 0.0002  max mem: 5511
[22:13:26.509452] Epoch: [29]  [760/781]  eta: 0:00:04  lr: 0.000210  training_loss: 1.5062 (1.4986)  mae_loss: 0.0301 (0.0296)  classification_loss: 1.4679 (1.4613)  loss_mask: 0.0053 (0.0077)  time: 0.1982  data: 0.0003  max mem: 5511
[22:13:30.422165] Epoch: [29]  [780/781]  eta: 0:00:00  lr: 0.000210  training_loss: 1.5162 (1.4988)  mae_loss: 0.0284 (0.0296)  classification_loss: 1.4864 (1.4615)  loss_mask: 0.0072 (0.0077)  time: 0.1956  data: 0.0002  max mem: 5511
[22:13:30.572870] Epoch: [29] Total time: 0:02:34 (0.1980 s / it)
[22:13:30.573588] Averaged stats: lr: 0.000210  training_loss: 1.5162 (1.4988)  mae_loss: 0.0284 (0.0296)  classification_loss: 1.4864 (1.4615)  loss_mask: 0.0072 (0.0077)
[22:13:31.201897] Test:  [  0/157]  eta: 0:01:37  testing_loss: 0.7392 (0.7392)  acc1: 78.1250 (78.1250)  acc5: 100.0000 (100.0000)  time: 0.6212  data: 0.5839  max mem: 5511
[22:13:31.491425] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.7915 (0.8067)  acc1: 71.8750 (73.8636)  acc5: 98.4375 (98.5795)  time: 0.0825  data: 0.0533  max mem: 5511
[22:13:31.781916] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.7876 (0.7810)  acc1: 71.8750 (74.5536)  acc5: 98.4375 (98.5863)  time: 0.0288  data: 0.0002  max mem: 5511
[22:13:32.071121] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.7870 (0.7862)  acc1: 76.5625 (75.0000)  acc5: 98.4375 (98.3367)  time: 0.0289  data: 0.0002  max mem: 5511
[22:13:32.356793] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.7918 (0.7918)  acc1: 75.0000 (74.5046)  acc5: 98.4375 (98.2088)  time: 0.0286  data: 0.0002  max mem: 5511
[22:13:32.643475] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.7854 (0.7857)  acc1: 73.4375 (74.7243)  acc5: 98.4375 (98.2230)  time: 0.0285  data: 0.0002  max mem: 5511
[22:13:32.933761] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.7794 (0.7843)  acc1: 73.4375 (74.7439)  acc5: 98.4375 (98.2838)  time: 0.0287  data: 0.0002  max mem: 5511
[22:13:33.222274] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7362 (0.7734)  acc1: 76.5625 (75.2201)  acc5: 98.4375 (98.3495)  time: 0.0288  data: 0.0002  max mem: 5511
[22:13:33.509469] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7362 (0.7809)  acc1: 76.5625 (74.7299)  acc5: 98.4375 (98.3410)  time: 0.0286  data: 0.0002  max mem: 5511
[22:13:33.796425] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7633 (0.7798)  acc1: 73.4375 (74.9657)  acc5: 98.4375 (98.3345)  time: 0.0286  data: 0.0002  max mem: 5511
[22:13:34.085373] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.7917 (0.7837)  acc1: 75.0000 (74.9226)  acc5: 98.4375 (98.3601)  time: 0.0286  data: 0.0002  max mem: 5511
[22:13:34.376809] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.8024 (0.7855)  acc1: 73.4375 (74.8452)  acc5: 98.4375 (98.3530)  time: 0.0289  data: 0.0002  max mem: 5511
[22:13:34.668548] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.7329 (0.7797)  acc1: 75.0000 (75.1550)  acc5: 98.4375 (98.3600)  time: 0.0290  data: 0.0002  max mem: 5511
[22:13:34.954007] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.7412 (0.7814)  acc1: 76.5625 (75.0358)  acc5: 98.4375 (98.3659)  time: 0.0287  data: 0.0002  max mem: 5511
[22:13:35.239708] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7434 (0.7798)  acc1: 75.0000 (75.0887)  acc5: 98.4375 (98.3267)  time: 0.0284  data: 0.0002  max mem: 5511
[22:13:35.520201] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7376 (0.7772)  acc1: 75.0000 (75.2483)  acc5: 98.4375 (98.2926)  time: 0.0281  data: 0.0001  max mem: 5511
[22:13:35.672419] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7376 (0.7787)  acc1: 75.0000 (75.2400)  acc5: 98.4375 (98.3000)  time: 0.0271  data: 0.0001  max mem: 5511
[22:13:35.819389] Test: Total time: 0:00:05 (0.0334 s / it)
[22:13:35.819840] * Acc@1 75.240 Acc@5 98.300 loss 0.779
[22:13:35.820117] Accuracy of the network on the 10000 test images: 75.2%
[22:13:35.820302] Max accuracy: 75.24%
[22:13:36.269467] log_dir: ./output_dir
[22:13:37.140212] Epoch: [30]  [  0/781]  eta: 0:11:18  lr: 0.000210  training_loss: 1.3820 (1.3820)  mae_loss: 0.0270 (0.0270)  classification_loss: 1.3468 (1.3468)  loss_mask: 0.0082 (0.0082)  time: 0.8691  data: 0.6370  max mem: 5511
[22:13:41.061606] Epoch: [30]  [ 20/781]  eta: 0:02:53  lr: 0.000210  training_loss: 1.4759 (1.4851)  mae_loss: 0.0290 (0.0288)  classification_loss: 1.4423 (1.4488)  loss_mask: 0.0063 (0.0074)  time: 0.1960  data: 0.0002  max mem: 5511
[22:13:45.002025] Epoch: [30]  [ 40/781]  eta: 0:02:37  lr: 0.000210  training_loss: 1.4997 (1.4936)  mae_loss: 0.0298 (0.0294)  classification_loss: 1.4616 (1.4576)  loss_mask: 0.0058 (0.0067)  time: 0.1969  data: 0.0002  max mem: 5511
[22:13:48.904253] Epoch: [30]  [ 60/781]  eta: 0:02:29  lr: 0.000210  training_loss: 1.4815 (1.4987)  mae_loss: 0.0283 (0.0293)  classification_loss: 1.4501 (1.4632)  loss_mask: 0.0049 (0.0061)  time: 0.1950  data: 0.0002  max mem: 5511
[22:13:52.813958] Epoch: [30]  [ 80/781]  eta: 0:02:23  lr: 0.000210  training_loss: 1.4197 (1.4898)  mae_loss: 0.0288 (0.0294)  classification_loss: 1.3854 (1.4548)  loss_mask: 0.0043 (0.0057)  time: 0.1954  data: 0.0002  max mem: 5511
[22:13:56.747381] Epoch: [30]  [100/781]  eta: 0:02:18  lr: 0.000209  training_loss: 1.4675 (1.4894)  mae_loss: 0.0288 (0.0294)  classification_loss: 1.4326 (1.4546)  loss_mask: 0.0041 (0.0054)  time: 0.1966  data: 0.0002  max mem: 5511
[22:14:00.721120] Epoch: [30]  [120/781]  eta: 0:02:13  lr: 0.000209  training_loss: 1.4504 (1.4860)  mae_loss: 0.0301 (0.0296)  classification_loss: 1.4106 (1.4502)  loss_mask: 0.0074 (0.0062)  time: 0.1986  data: 0.0002  max mem: 5511
[22:14:04.650946] Epoch: [30]  [140/781]  eta: 0:02:08  lr: 0.000209  training_loss: 1.4605 (1.4828)  mae_loss: 0.0290 (0.0296)  classification_loss: 1.4248 (1.4467)  loss_mask: 0.0071 (0.0064)  time: 0.1964  data: 0.0002  max mem: 5511
[22:14:08.561504] Epoch: [30]  [160/781]  eta: 0:02:04  lr: 0.000209  training_loss: 1.5103 (1.4862)  mae_loss: 0.0289 (0.0297)  classification_loss: 1.4765 (1.4502)  loss_mask: 0.0052 (0.0063)  time: 0.1954  data: 0.0002  max mem: 5511
[22:14:12.527279] Epoch: [30]  [180/781]  eta: 0:02:00  lr: 0.000209  training_loss: 1.4886 (1.4872)  mae_loss: 0.0288 (0.0296)  classification_loss: 1.4568 (1.4511)  loss_mask: 0.0069 (0.0065)  time: 0.1982  data: 0.0002  max mem: 5511
[22:14:16.465479] Epoch: [30]  [200/781]  eta: 0:01:56  lr: 0.000209  training_loss: 1.4791 (1.4858)  mae_loss: 0.0287 (0.0295)  classification_loss: 1.4477 (1.4499)  loss_mask: 0.0047 (0.0065)  time: 0.1968  data: 0.0002  max mem: 5511
[22:14:20.439135] Epoch: [30]  [220/781]  eta: 0:01:52  lr: 0.000209  training_loss: 1.5153 (1.4882)  mae_loss: 0.0306 (0.0295)  classification_loss: 1.4756 (1.4523)  loss_mask: 0.0044 (0.0064)  time: 0.1986  data: 0.0003  max mem: 5511
[22:14:24.400530] Epoch: [30]  [240/781]  eta: 0:01:47  lr: 0.000209  training_loss: 1.4871 (1.4877)  mae_loss: 0.0301 (0.0296)  classification_loss: 1.4526 (1.4518)  loss_mask: 0.0048 (0.0063)  time: 0.1979  data: 0.0002  max mem: 5511
[22:14:28.359383] Epoch: [30]  [260/781]  eta: 0:01:43  lr: 0.000209  training_loss: 1.5328 (1.4908)  mae_loss: 0.0284 (0.0295)  classification_loss: 1.4788 (1.4548)  loss_mask: 0.0070 (0.0065)  time: 0.1979  data: 0.0002  max mem: 5511
[22:14:32.332127] Epoch: [30]  [280/781]  eta: 0:01:39  lr: 0.000209  training_loss: 1.5126 (1.4913)  mae_loss: 0.0288 (0.0295)  classification_loss: 1.4537 (1.4551)  loss_mask: 0.0072 (0.0067)  time: 0.1985  data: 0.0002  max mem: 5511
[22:14:36.291418] Epoch: [30]  [300/781]  eta: 0:01:35  lr: 0.000209  training_loss: 1.4780 (1.4935)  mae_loss: 0.0305 (0.0295)  classification_loss: 1.4329 (1.4570)  loss_mask: 0.0066 (0.0069)  time: 0.1979  data: 0.0002  max mem: 5511
[22:14:40.233694] Epoch: [30]  [320/781]  eta: 0:01:31  lr: 0.000209  training_loss: 1.4870 (1.4947)  mae_loss: 0.0287 (0.0295)  classification_loss: 1.4530 (1.4582)  loss_mask: 0.0063 (0.0070)  time: 0.1970  data: 0.0002  max mem: 5511
[22:14:44.221490] Epoch: [30]  [340/781]  eta: 0:01:27  lr: 0.000208  training_loss: 1.4707 (1.4944)  mae_loss: 0.0310 (0.0295)  classification_loss: 1.4298 (1.4579)  loss_mask: 0.0054 (0.0070)  time: 0.1993  data: 0.0002  max mem: 5511
[22:14:48.169277] Epoch: [30]  [360/781]  eta: 0:01:23  lr: 0.000208  training_loss: 1.4579 (1.4941)  mae_loss: 0.0303 (0.0295)  classification_loss: 1.4173 (1.4575)  loss_mask: 0.0070 (0.0071)  time: 0.1973  data: 0.0002  max mem: 5511
[22:14:52.097134] Epoch: [30]  [380/781]  eta: 0:01:19  lr: 0.000208  training_loss: 1.5077 (1.4950)  mae_loss: 0.0282 (0.0295)  classification_loss: 1.4748 (1.4585)  loss_mask: 0.0050 (0.0070)  time: 0.1963  data: 0.0002  max mem: 5511
[22:14:56.054544] Epoch: [30]  [400/781]  eta: 0:01:15  lr: 0.000208  training_loss: 1.4824 (1.4938)  mae_loss: 0.0293 (0.0295)  classification_loss: 1.4264 (1.4569)  loss_mask: 0.0094 (0.0074)  time: 0.1978  data: 0.0003  max mem: 5511
[22:15:00.020269] Epoch: [30]  [420/781]  eta: 0:01:11  lr: 0.000208  training_loss: 1.5054 (1.4942)  mae_loss: 0.0289 (0.0294)  classification_loss: 1.4748 (1.4572)  loss_mask: 0.0074 (0.0075)  time: 0.1982  data: 0.0002  max mem: 5511
[22:15:03.970274] Epoch: [30]  [440/781]  eta: 0:01:07  lr: 0.000208  training_loss: 1.4789 (1.4945)  mae_loss: 0.0287 (0.0294)  classification_loss: 1.4420 (1.4576)  loss_mask: 0.0057 (0.0075)  time: 0.1974  data: 0.0002  max mem: 5511
[22:15:07.921651] Epoch: [30]  [460/781]  eta: 0:01:03  lr: 0.000208  training_loss: 1.4281 (1.4922)  mae_loss: 0.0292 (0.0294)  classification_loss: 1.3890 (1.4552)  loss_mask: 0.0075 (0.0075)  time: 0.1975  data: 0.0002  max mem: 5511
[22:15:11.884070] Epoch: [30]  [480/781]  eta: 0:00:59  lr: 0.000208  training_loss: 1.5577 (1.4940)  mae_loss: 0.0293 (0.0294)  classification_loss: 1.5217 (1.4567)  loss_mask: 0.0095 (0.0078)  time: 0.1980  data: 0.0002  max mem: 5511
[22:15:15.862494] Epoch: [30]  [500/781]  eta: 0:00:55  lr: 0.000208  training_loss: 1.5484 (1.4959)  mae_loss: 0.0286 (0.0294)  classification_loss: 1.5012 (1.4586)  loss_mask: 0.0089 (0.0079)  time: 0.1988  data: 0.0002  max mem: 5511
[22:15:19.794036] Epoch: [30]  [520/781]  eta: 0:00:51  lr: 0.000208  training_loss: 1.5040 (1.4952)  mae_loss: 0.0289 (0.0294)  classification_loss: 1.4614 (1.4580)  loss_mask: 0.0050 (0.0078)  time: 0.1965  data: 0.0002  max mem: 5511
[22:15:23.712290] Epoch: [30]  [540/781]  eta: 0:00:47  lr: 0.000208  training_loss: 1.5010 (1.4958)  mae_loss: 0.0291 (0.0294)  classification_loss: 1.4700 (1.4585)  loss_mask: 0.0062 (0.0079)  time: 0.1958  data: 0.0002  max mem: 5511
[22:15:27.637766] Epoch: [30]  [560/781]  eta: 0:00:43  lr: 0.000208  training_loss: 1.4733 (1.4956)  mae_loss: 0.0314 (0.0294)  classification_loss: 1.4406 (1.4584)  loss_mask: 0.0045 (0.0078)  time: 0.1962  data: 0.0006  max mem: 5511
[22:15:31.552130] Epoch: [30]  [580/781]  eta: 0:00:39  lr: 0.000208  training_loss: 1.4503 (1.4951)  mae_loss: 0.0278 (0.0294)  classification_loss: 1.4092 (1.4579)  loss_mask: 0.0062 (0.0077)  time: 0.1956  data: 0.0002  max mem: 5511
[22:15:35.476107] Epoch: [30]  [600/781]  eta: 0:00:35  lr: 0.000207  training_loss: 1.5009 (1.4951)  mae_loss: 0.0288 (0.0294)  classification_loss: 1.4695 (1.4580)  loss_mask: 0.0054 (0.0077)  time: 0.1961  data: 0.0002  max mem: 5511
[22:15:39.425363] Epoch: [30]  [620/781]  eta: 0:00:31  lr: 0.000207  training_loss: 1.4777 (1.4944)  mae_loss: 0.0282 (0.0294)  classification_loss: 1.4375 (1.4574)  loss_mask: 0.0055 (0.0076)  time: 0.1974  data: 0.0002  max mem: 5511
[22:15:43.361143] Epoch: [30]  [640/781]  eta: 0:00:27  lr: 0.000207  training_loss: 1.4623 (1.4939)  mae_loss: 0.0285 (0.0294)  classification_loss: 1.4268 (1.4570)  loss_mask: 0.0036 (0.0075)  time: 0.1967  data: 0.0002  max mem: 5511
[22:15:47.302777] Epoch: [30]  [660/781]  eta: 0:00:23  lr: 0.000207  training_loss: 1.5134 (1.4941)  mae_loss: 0.0289 (0.0294)  classification_loss: 1.4716 (1.4573)  loss_mask: 0.0040 (0.0074)  time: 0.1970  data: 0.0003  max mem: 5511
[22:15:51.283107] Epoch: [30]  [680/781]  eta: 0:00:20  lr: 0.000207  training_loss: 1.4578 (1.4932)  mae_loss: 0.0303 (0.0294)  classification_loss: 1.4213 (1.4562)  loss_mask: 0.0057 (0.0075)  time: 0.1989  data: 0.0002  max mem: 5511
[22:15:55.237852] Epoch: [30]  [700/781]  eta: 0:00:16  lr: 0.000207  training_loss: 1.5084 (1.4937)  mae_loss: 0.0310 (0.0295)  classification_loss: 1.4639 (1.4566)  loss_mask: 0.0069 (0.0076)  time: 0.1977  data: 0.0002  max mem: 5511
[22:15:59.191969] Epoch: [30]  [720/781]  eta: 0:00:12  lr: 0.000207  training_loss: 1.4884 (1.4941)  mae_loss: 0.0292 (0.0295)  classification_loss: 1.4489 (1.4571)  loss_mask: 0.0052 (0.0076)  time: 0.1976  data: 0.0003  max mem: 5511
[22:16:03.139629] Epoch: [30]  [740/781]  eta: 0:00:08  lr: 0.000207  training_loss: 1.4566 (1.4937)  mae_loss: 0.0285 (0.0294)  classification_loss: 1.4186 (1.4568)  loss_mask: 0.0046 (0.0075)  time: 0.1973  data: 0.0002  max mem: 5511
[22:16:07.114718] Epoch: [30]  [760/781]  eta: 0:00:04  lr: 0.000207  training_loss: 1.5358 (1.4951)  mae_loss: 0.0298 (0.0294)  classification_loss: 1.4993 (1.4581)  loss_mask: 0.0057 (0.0075)  time: 0.1987  data: 0.0002  max mem: 5511
[22:16:11.081770] Epoch: [30]  [780/781]  eta: 0:00:00  lr: 0.000207  training_loss: 1.4712 (1.4947)  mae_loss: 0.0309 (0.0295)  classification_loss: 1.4311 (1.4578)  loss_mask: 0.0051 (0.0075)  time: 0.1983  data: 0.0002  max mem: 5511
[22:16:11.239896] Epoch: [30] Total time: 0:02:34 (0.1984 s / it)
[22:16:11.240379] Averaged stats: lr: 0.000207  training_loss: 1.4712 (1.4947)  mae_loss: 0.0309 (0.0295)  classification_loss: 1.4311 (1.4578)  loss_mask: 0.0051 (0.0075)
[22:16:12.663320] Test:  [  0/157]  eta: 0:01:42  testing_loss: 0.7068 (0.7068)  acc1: 79.6875 (79.6875)  acc5: 100.0000 (100.0000)  time: 0.6544  data: 0.6247  max mem: 5511
[22:16:12.951012] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.7681 (0.8025)  acc1: 73.4375 (73.8636)  acc5: 100.0000 (99.0057)  time: 0.0854  data: 0.0570  max mem: 5511
[22:16:13.237157] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.7629 (0.7607)  acc1: 75.0000 (75.2976)  acc5: 98.4375 (98.7351)  time: 0.0285  data: 0.0002  max mem: 5511
[22:16:13.524128] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.7725 (0.7718)  acc1: 75.0000 (74.7480)  acc5: 98.4375 (98.4375)  time: 0.0285  data: 0.0002  max mem: 5511
[22:16:13.812051] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.7853 (0.7762)  acc1: 73.4375 (74.5046)  acc5: 96.8750 (98.1326)  time: 0.0286  data: 0.0002  max mem: 5511
[22:16:14.097580] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.7496 (0.7662)  acc1: 75.0000 (75.0000)  acc5: 96.8750 (98.1005)  time: 0.0285  data: 0.0002  max mem: 5511
[22:16:14.382429] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.7525 (0.7664)  acc1: 73.4375 (74.7695)  acc5: 98.4375 (98.1814)  time: 0.0284  data: 0.0002  max mem: 5511
[22:16:14.667565] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7308 (0.7569)  acc1: 73.4375 (75.0660)  acc5: 98.4375 (98.2835)  time: 0.0283  data: 0.0002  max mem: 5511
[22:16:14.952441] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7377 (0.7638)  acc1: 75.0000 (74.7492)  acc5: 98.4375 (98.2060)  time: 0.0283  data: 0.0002  max mem: 5511
[22:16:15.237573] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7630 (0.7648)  acc1: 73.4375 (74.8455)  acc5: 98.4375 (98.1799)  time: 0.0283  data: 0.0002  max mem: 5511
[22:16:15.521606] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.7591 (0.7668)  acc1: 73.4375 (74.7679)  acc5: 98.4375 (98.2364)  time: 0.0283  data: 0.0002  max mem: 5511
[22:16:15.808254] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.7788 (0.7677)  acc1: 71.8750 (74.7325)  acc5: 98.4375 (98.2545)  time: 0.0284  data: 0.0002  max mem: 5511
[22:16:16.098294] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.7566 (0.7630)  acc1: 76.5625 (74.9871)  acc5: 98.4375 (98.2696)  time: 0.0287  data: 0.0002  max mem: 5511
[22:16:16.386507] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.7398 (0.7653)  acc1: 76.5625 (74.9046)  acc5: 98.4375 (98.2824)  time: 0.0288  data: 0.0002  max mem: 5511
[22:16:16.671736] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7478 (0.7650)  acc1: 75.0000 (75.0332)  acc5: 98.4375 (98.2270)  time: 0.0285  data: 0.0002  max mem: 5511
[22:16:16.956389] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7700 (0.7643)  acc1: 75.0000 (75.0621)  acc5: 98.4375 (98.1995)  time: 0.0283  data: 0.0001  max mem: 5511
[22:16:17.110431] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7665 (0.7654)  acc1: 76.5625 (75.0900)  acc5: 98.4375 (98.2200)  time: 0.0274  data: 0.0001  max mem: 5511
[22:16:17.272622] Test: Total time: 0:00:05 (0.0335 s / it)
[22:16:17.273130] * Acc@1 75.090 Acc@5 98.220 loss 0.765
[22:16:17.273438] Accuracy of the network on the 10000 test images: 75.1%
[22:16:17.273637] Max accuracy: 75.24%
[22:16:17.636843] log_dir: ./output_dir
[22:16:18.475436] Epoch: [31]  [  0/781]  eta: 0:10:53  lr: 0.000207  training_loss: 1.4228 (1.4228)  mae_loss: 0.0326 (0.0326)  classification_loss: 1.3791 (1.3791)  loss_mask: 0.0111 (0.0111)  time: 0.8368  data: 0.6204  max mem: 5511
[22:16:22.407040] Epoch: [31]  [ 20/781]  eta: 0:02:52  lr: 0.000207  training_loss: 1.4863 (1.4799)  mae_loss: 0.0294 (0.0298)  classification_loss: 1.4511 (1.4437)  loss_mask: 0.0056 (0.0063)  time: 0.1964  data: 0.0002  max mem: 5511
[22:16:26.337006] Epoch: [31]  [ 40/781]  eta: 0:02:37  lr: 0.000207  training_loss: 1.4960 (1.4825)  mae_loss: 0.0302 (0.0293)  classification_loss: 1.4648 (1.4464)  loss_mask: 0.0064 (0.0068)  time: 0.1964  data: 0.0003  max mem: 5511
[22:16:30.337410] Epoch: [31]  [ 60/781]  eta: 0:02:30  lr: 0.000207  training_loss: 1.4872 (1.4968)  mae_loss: 0.0299 (0.0298)  classification_loss: 1.4561 (1.4603)  loss_mask: 0.0051 (0.0068)  time: 0.1999  data: 0.0002  max mem: 5511
[22:16:34.317607] Epoch: [31]  [ 80/781]  eta: 0:02:24  lr: 0.000206  training_loss: 1.4939 (1.4954)  mae_loss: 0.0285 (0.0296)  classification_loss: 1.4618 (1.4588)  loss_mask: 0.0065 (0.0069)  time: 0.1989  data: 0.0002  max mem: 5511
[22:16:38.252231] Epoch: [31]  [100/781]  eta: 0:02:18  lr: 0.000206  training_loss: 1.5134 (1.5019)  mae_loss: 0.0286 (0.0296)  classification_loss: 1.4858 (1.4647)  loss_mask: 0.0076 (0.0077)  time: 0.1966  data: 0.0002  max mem: 5511
[22:16:42.172865] Epoch: [31]  [120/781]  eta: 0:02:13  lr: 0.000206  training_loss: 1.4529 (1.4975)  mae_loss: 0.0276 (0.0295)  classification_loss: 1.4242 (1.4589)  loss_mask: 0.0086 (0.0090)  time: 0.1960  data: 0.0003  max mem: 5511
[22:16:46.113322] Epoch: [31]  [140/781]  eta: 0:02:09  lr: 0.000206  training_loss: 1.4621 (1.4968)  mae_loss: 0.0286 (0.0295)  classification_loss: 1.4244 (1.4581)  loss_mask: 0.0086 (0.0092)  time: 0.1969  data: 0.0003  max mem: 5511
[22:16:50.058263] Epoch: [31]  [160/781]  eta: 0:02:04  lr: 0.000206  training_loss: 1.5116 (1.4972)  mae_loss: 0.0276 (0.0294)  classification_loss: 1.4806 (1.4591)  loss_mask: 0.0038 (0.0087)  time: 0.1972  data: 0.0002  max mem: 5511
[22:16:53.993697] Epoch: [31]  [180/781]  eta: 0:02:00  lr: 0.000206  training_loss: 1.4792 (1.4956)  mae_loss: 0.0284 (0.0294)  classification_loss: 1.4480 (1.4578)  loss_mask: 0.0050 (0.0084)  time: 0.1967  data: 0.0002  max mem: 5511
[22:16:57.971471] Epoch: [31]  [200/781]  eta: 0:01:56  lr: 0.000206  training_loss: 1.5060 (1.4962)  mae_loss: 0.0287 (0.0293)  classification_loss: 1.4759 (1.4589)  loss_mask: 0.0046 (0.0081)  time: 0.1988  data: 0.0003  max mem: 5511
[22:17:01.913591] Epoch: [31]  [220/781]  eta: 0:01:52  lr: 0.000206  training_loss: 1.5067 (1.4959)  mae_loss: 0.0304 (0.0293)  classification_loss: 1.4691 (1.4587)  loss_mask: 0.0051 (0.0079)  time: 0.1970  data: 0.0002  max mem: 5511
[22:17:05.931994] Epoch: [31]  [240/781]  eta: 0:01:48  lr: 0.000206  training_loss: 1.4853 (1.4950)  mae_loss: 0.0315 (0.0295)  classification_loss: 1.4379 (1.4577)  loss_mask: 0.0050 (0.0078)  time: 0.2008  data: 0.0002  max mem: 5511
[22:17:09.849990] Epoch: [31]  [260/781]  eta: 0:01:44  lr: 0.000206  training_loss: 1.4720 (1.4940)  mae_loss: 0.0288 (0.0295)  classification_loss: 1.4408 (1.4570)  loss_mask: 0.0050 (0.0076)  time: 0.1958  data: 0.0002  max mem: 5511
[22:17:13.778679] Epoch: [31]  [280/781]  eta: 0:01:40  lr: 0.000206  training_loss: 1.4386 (1.4913)  mae_loss: 0.0292 (0.0295)  classification_loss: 1.4073 (1.4544)  loss_mask: 0.0049 (0.0074)  time: 0.1964  data: 0.0002  max mem: 5511
[22:17:17.740248] Epoch: [31]  [300/781]  eta: 0:01:35  lr: 0.000206  training_loss: 1.4977 (1.4932)  mae_loss: 0.0284 (0.0295)  classification_loss: 1.4574 (1.4563)  loss_mask: 0.0056 (0.0074)  time: 0.1980  data: 0.0002  max mem: 5511
[22:17:21.675937] Epoch: [31]  [320/781]  eta: 0:01:31  lr: 0.000205  training_loss: 1.4464 (1.4916)  mae_loss: 0.0286 (0.0295)  classification_loss: 1.4141 (1.4546)  loss_mask: 0.0060 (0.0075)  time: 0.1967  data: 0.0002  max mem: 5511
[22:17:25.602735] Epoch: [31]  [340/781]  eta: 0:01:27  lr: 0.000205  training_loss: 1.4711 (1.4909)  mae_loss: 0.0278 (0.0295)  classification_loss: 1.4332 (1.4540)  loss_mask: 0.0047 (0.0074)  time: 0.1963  data: 0.0003  max mem: 5511
[22:17:29.509355] Epoch: [31]  [360/781]  eta: 0:01:23  lr: 0.000205  training_loss: 1.4634 (1.4897)  mae_loss: 0.0293 (0.0295)  classification_loss: 1.4255 (1.4530)  loss_mask: 0.0040 (0.0072)  time: 0.1953  data: 0.0003  max mem: 5511
[22:17:33.453454] Epoch: [31]  [380/781]  eta: 0:01:19  lr: 0.000205  training_loss: 1.4898 (1.4902)  mae_loss: 0.0304 (0.0295)  classification_loss: 1.4537 (1.4536)  loss_mask: 0.0060 (0.0071)  time: 0.1971  data: 0.0002  max mem: 5511
[22:17:37.369818] Epoch: [31]  [400/781]  eta: 0:01:15  lr: 0.000205  training_loss: 1.4684 (1.4889)  mae_loss: 0.0285 (0.0295)  classification_loss: 1.4315 (1.4522)  loss_mask: 0.0076 (0.0072)  time: 0.1957  data: 0.0002  max mem: 5511
[22:17:41.383060] Epoch: [31]  [420/781]  eta: 0:01:11  lr: 0.000205  training_loss: 1.5121 (1.4915)  mae_loss: 0.0273 (0.0294)  classification_loss: 1.4865 (1.4550)  loss_mask: 0.0055 (0.0071)  time: 0.2006  data: 0.0002  max mem: 5511
[22:17:45.345950] Epoch: [31]  [440/781]  eta: 0:01:07  lr: 0.000205  training_loss: 1.3953 (1.4878)  mae_loss: 0.0296 (0.0295)  classification_loss: 1.3527 (1.4513)  loss_mask: 0.0046 (0.0070)  time: 0.1981  data: 0.0003  max mem: 5511
[22:17:49.302582] Epoch: [31]  [460/781]  eta: 0:01:03  lr: 0.000205  training_loss: 1.4517 (1.4861)  mae_loss: 0.0297 (0.0295)  classification_loss: 1.4005 (1.4495)  loss_mask: 0.0079 (0.0071)  time: 0.1977  data: 0.0002  max mem: 5511
[22:17:53.225283] Epoch: [31]  [480/781]  eta: 0:00:59  lr: 0.000205  training_loss: 1.5325 (1.4874)  mae_loss: 0.0297 (0.0295)  classification_loss: 1.4804 (1.4505)  loss_mask: 0.0126 (0.0074)  time: 0.1960  data: 0.0002  max mem: 5511
[22:17:57.152263] Epoch: [31]  [500/781]  eta: 0:00:55  lr: 0.000205  training_loss: 1.5005 (1.4877)  mae_loss: 0.0289 (0.0295)  classification_loss: 1.4507 (1.4507)  loss_mask: 0.0072 (0.0074)  time: 0.1963  data: 0.0003  max mem: 5511
[22:18:01.107753] Epoch: [31]  [520/781]  eta: 0:00:51  lr: 0.000205  training_loss: 1.4951 (1.4887)  mae_loss: 0.0295 (0.0295)  classification_loss: 1.4649 (1.4517)  loss_mask: 0.0062 (0.0074)  time: 0.1977  data: 0.0002  max mem: 5511
[22:18:05.017357] Epoch: [31]  [540/781]  eta: 0:00:47  lr: 0.000205  training_loss: 1.4869 (1.4890)  mae_loss: 0.0298 (0.0295)  classification_loss: 1.4513 (1.4521)  loss_mask: 0.0050 (0.0074)  time: 0.1954  data: 0.0002  max mem: 5511
[22:18:08.951634] Epoch: [31]  [560/781]  eta: 0:00:43  lr: 0.000204  training_loss: 1.4582 (1.4882)  mae_loss: 0.0295 (0.0295)  classification_loss: 1.4176 (1.4512)  loss_mask: 0.0057 (0.0074)  time: 0.1966  data: 0.0002  max mem: 5511
[22:18:12.916472] Epoch: [31]  [580/781]  eta: 0:00:39  lr: 0.000204  training_loss: 1.4447 (1.4873)  mae_loss: 0.0296 (0.0295)  classification_loss: 1.4064 (1.4504)  loss_mask: 0.0064 (0.0074)  time: 0.1982  data: 0.0003  max mem: 5511
[22:18:16.859082] Epoch: [31]  [600/781]  eta: 0:00:35  lr: 0.000204  training_loss: 1.4635 (1.4874)  mae_loss: 0.0283 (0.0295)  classification_loss: 1.4229 (1.4503)  loss_mask: 0.0097 (0.0075)  time: 0.1970  data: 0.0002  max mem: 5511
[22:18:20.794364] Epoch: [31]  [620/781]  eta: 0:00:31  lr: 0.000204  training_loss: 1.4529 (1.4863)  mae_loss: 0.0302 (0.0295)  classification_loss: 1.4163 (1.4493)  loss_mask: 0.0073 (0.0076)  time: 0.1967  data: 0.0003  max mem: 5511
[22:18:24.725024] Epoch: [31]  [640/781]  eta: 0:00:27  lr: 0.000204  training_loss: 1.4610 (1.4855)  mae_loss: 0.0285 (0.0295)  classification_loss: 1.4253 (1.4485)  loss_mask: 0.0059 (0.0075)  time: 0.1964  data: 0.0002  max mem: 5511
[22:18:28.656735] Epoch: [31]  [660/781]  eta: 0:00:23  lr: 0.000204  training_loss: 1.5273 (1.4869)  mae_loss: 0.0304 (0.0295)  classification_loss: 1.4842 (1.4498)  loss_mask: 0.0074 (0.0075)  time: 0.1965  data: 0.0002  max mem: 5511
[22:18:32.611310] Epoch: [31]  [680/781]  eta: 0:00:20  lr: 0.000204  training_loss: 1.4833 (1.4872)  mae_loss: 0.0292 (0.0295)  classification_loss: 1.4460 (1.4501)  loss_mask: 0.0075 (0.0075)  time: 0.1976  data: 0.0002  max mem: 5511
[22:18:36.544006] Epoch: [31]  [700/781]  eta: 0:00:16  lr: 0.000204  training_loss: 1.4808 (1.4871)  mae_loss: 0.0282 (0.0295)  classification_loss: 1.4410 (1.4501)  loss_mask: 0.0051 (0.0075)  time: 0.1965  data: 0.0002  max mem: 5511
[22:18:40.478511] Epoch: [31]  [720/781]  eta: 0:00:12  lr: 0.000204  training_loss: 1.5066 (1.4874)  mae_loss: 0.0291 (0.0295)  classification_loss: 1.4694 (1.4504)  loss_mask: 0.0040 (0.0074)  time: 0.1966  data: 0.0002  max mem: 5511
[22:18:44.429981] Epoch: [31]  [740/781]  eta: 0:00:08  lr: 0.000204  training_loss: 1.4652 (1.4861)  mae_loss: 0.0287 (0.0295)  classification_loss: 1.4313 (1.4492)  loss_mask: 0.0041 (0.0073)  time: 0.1975  data: 0.0002  max mem: 5511
[22:18:48.402890] Epoch: [31]  [760/781]  eta: 0:00:04  lr: 0.000204  training_loss: 1.5280 (1.4869)  mae_loss: 0.0290 (0.0295)  classification_loss: 1.4896 (1.4501)  loss_mask: 0.0046 (0.0073)  time: 0.1985  data: 0.0002  max mem: 5511
[22:18:52.332006] Epoch: [31]  [780/781]  eta: 0:00:00  lr: 0.000204  training_loss: 1.4858 (1.4873)  mae_loss: 0.0281 (0.0295)  classification_loss: 1.4519 (1.4506)  loss_mask: 0.0066 (0.0073)  time: 0.1964  data: 0.0002  max mem: 5511
[22:18:52.511623] Epoch: [31] Total time: 0:02:34 (0.1983 s / it)
[22:18:52.512170] Averaged stats: lr: 0.000204  training_loss: 1.4858 (1.4873)  mae_loss: 0.0281 (0.0295)  classification_loss: 1.4519 (1.4506)  loss_mask: 0.0066 (0.0073)
[22:18:53.164382] Test:  [  0/157]  eta: 0:01:41  testing_loss: 0.6532 (0.6532)  acc1: 78.1250 (78.1250)  acc5: 98.4375 (98.4375)  time: 0.6476  data: 0.6152  max mem: 5511
[22:18:53.449528] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.7567 (0.7938)  acc1: 76.5625 (75.4261)  acc5: 98.4375 (98.0114)  time: 0.0846  data: 0.0561  max mem: 5511
[22:18:53.737481] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.7565 (0.7586)  acc1: 75.0000 (75.9673)  acc5: 98.4375 (98.1399)  time: 0.0285  data: 0.0002  max mem: 5511
[22:18:54.029623] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.7637 (0.7673)  acc1: 76.5625 (75.8065)  acc5: 98.4375 (98.0343)  time: 0.0288  data: 0.0002  max mem: 5511
[22:18:54.321698] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.7637 (0.7663)  acc1: 76.5625 (75.7241)  acc5: 98.4375 (98.0564)  time: 0.0290  data: 0.0002  max mem: 5511
[22:18:54.611491] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.7375 (0.7622)  acc1: 75.0000 (75.7047)  acc5: 98.4375 (98.1924)  time: 0.0289  data: 0.0002  max mem: 5511
[22:18:54.906182] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.7625 (0.7626)  acc1: 75.0000 (75.5123)  acc5: 98.4375 (98.2582)  time: 0.0291  data: 0.0002  max mem: 5511
[22:18:55.199916] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6984 (0.7522)  acc1: 76.5625 (75.9903)  acc5: 98.4375 (98.3935)  time: 0.0293  data: 0.0002  max mem: 5511
[22:18:55.484985] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7165 (0.7577)  acc1: 75.0000 (75.5401)  acc5: 98.4375 (98.3218)  time: 0.0288  data: 0.0002  max mem: 5511
[22:18:55.773318] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7304 (0.7551)  acc1: 76.5625 (75.8413)  acc5: 98.4375 (98.3001)  time: 0.0285  data: 0.0002  max mem: 5511
[22:18:56.062529] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.7709 (0.7595)  acc1: 75.0000 (75.6033)  acc5: 98.4375 (98.2519)  time: 0.0287  data: 0.0002  max mem: 5511
[22:18:56.346726] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.8064 (0.7613)  acc1: 73.4375 (75.6194)  acc5: 98.4375 (98.2686)  time: 0.0285  data: 0.0002  max mem: 5511
[22:18:56.632397] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.7371 (0.7571)  acc1: 75.0000 (75.7619)  acc5: 98.4375 (98.2696)  time: 0.0284  data: 0.0002  max mem: 5511
[22:18:56.917709] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.7661 (0.7597)  acc1: 76.5625 (75.7514)  acc5: 98.4375 (98.2944)  time: 0.0284  data: 0.0002  max mem: 5511
[22:18:57.203219] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7549 (0.7593)  acc1: 76.5625 (75.7646)  acc5: 98.4375 (98.2380)  time: 0.0283  data: 0.0002  max mem: 5511
[22:18:57.486856] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7549 (0.7590)  acc1: 76.5625 (75.7968)  acc5: 98.4375 (98.2409)  time: 0.0282  data: 0.0002  max mem: 5511
[22:18:57.639441] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7538 (0.7612)  acc1: 76.5625 (75.7100)  acc5: 98.4375 (98.2500)  time: 0.0272  data: 0.0001  max mem: 5511
[22:18:57.794297] Test: Total time: 0:00:05 (0.0336 s / it)
[22:18:57.794875] * Acc@1 75.710 Acc@5 98.250 loss 0.761
[22:18:57.795179] Accuracy of the network on the 10000 test images: 75.7%
[22:18:57.795352] Max accuracy: 75.71%
[22:18:58.131553] log_dir: ./output_dir
[22:18:59.003410] Epoch: [32]  [  0/781]  eta: 0:11:19  lr: 0.000204  training_loss: 1.3114 (1.3114)  mae_loss: 0.0300 (0.0300)  classification_loss: 1.2740 (1.2740)  loss_mask: 0.0074 (0.0074)  time: 0.8697  data: 0.6630  max mem: 5511
[22:19:02.918817] Epoch: [32]  [ 20/781]  eta: 0:02:53  lr: 0.000204  training_loss: 1.4730 (1.4666)  mae_loss: 0.0282 (0.0296)  classification_loss: 1.4356 (1.4322)  loss_mask: 0.0036 (0.0048)  time: 0.1957  data: 0.0002  max mem: 5511
[22:19:06.848175] Epoch: [32]  [ 40/781]  eta: 0:02:37  lr: 0.000203  training_loss: 1.4568 (1.4619)  mae_loss: 0.0283 (0.0287)  classification_loss: 1.4306 (1.4285)  loss_mask: 0.0039 (0.0047)  time: 0.1964  data: 0.0002  max mem: 5511
[22:19:10.816752] Epoch: [32]  [ 60/781]  eta: 0:02:29  lr: 0.000203  training_loss: 1.5121 (1.4733)  mae_loss: 0.0292 (0.0292)  classification_loss: 1.4723 (1.4389)  loss_mask: 0.0053 (0.0052)  time: 0.1983  data: 0.0002  max mem: 5511
[22:19:14.792756] Epoch: [32]  [ 80/781]  eta: 0:02:24  lr: 0.000203  training_loss: 1.4851 (1.4797)  mae_loss: 0.0289 (0.0293)  classification_loss: 1.4483 (1.4450)  loss_mask: 0.0053 (0.0054)  time: 0.1987  data: 0.0002  max mem: 5511
[22:19:18.737934] Epoch: [32]  [100/781]  eta: 0:02:18  lr: 0.000203  training_loss: 1.5154 (1.4836)  mae_loss: 0.0276 (0.0291)  classification_loss: 1.4799 (1.4488)  loss_mask: 0.0049 (0.0057)  time: 0.1972  data: 0.0002  max mem: 5511
[22:19:22.696727] Epoch: [32]  [120/781]  eta: 0:02:14  lr: 0.000203  training_loss: 1.4674 (1.4817)  mae_loss: 0.0282 (0.0291)  classification_loss: 1.4225 (1.4462)  loss_mask: 0.0095 (0.0065)  time: 0.1978  data: 0.0002  max mem: 5511
[22:19:26.653756] Epoch: [32]  [140/781]  eta: 0:02:09  lr: 0.000203  training_loss: 1.4664 (1.4828)  mae_loss: 0.0281 (0.0290)  classification_loss: 1.4276 (1.4465)  loss_mask: 0.0098 (0.0074)  time: 0.1978  data: 0.0002  max mem: 5511
[22:19:30.573936] Epoch: [32]  [160/781]  eta: 0:02:05  lr: 0.000203  training_loss: 1.4315 (1.4778)  mae_loss: 0.0284 (0.0289)  classification_loss: 1.3993 (1.4415)  loss_mask: 0.0069 (0.0074)  time: 0.1959  data: 0.0002  max mem: 5511
[22:19:34.566945] Epoch: [32]  [180/781]  eta: 0:02:00  lr: 0.000203  training_loss: 1.4642 (1.4768)  mae_loss: 0.0281 (0.0289)  classification_loss: 1.4350 (1.4407)  loss_mask: 0.0047 (0.0072)  time: 0.1996  data: 0.0003  max mem: 5511
[22:19:38.485404] Epoch: [32]  [200/781]  eta: 0:01:56  lr: 0.000203  training_loss: 1.4295 (1.4740)  mae_loss: 0.0292 (0.0290)  classification_loss: 1.3949 (1.4378)  loss_mask: 0.0067 (0.0072)  time: 0.1958  data: 0.0002  max mem: 5511
[22:19:42.407612] Epoch: [32]  [220/781]  eta: 0:01:52  lr: 0.000203  training_loss: 1.4753 (1.4746)  mae_loss: 0.0279 (0.0289)  classification_loss: 1.4386 (1.4387)  loss_mask: 0.0047 (0.0070)  time: 0.1960  data: 0.0003  max mem: 5511
[22:19:46.368614] Epoch: [32]  [240/781]  eta: 0:01:48  lr: 0.000203  training_loss: 1.5031 (1.4762)  mae_loss: 0.0280 (0.0289)  classification_loss: 1.4672 (1.4405)  loss_mask: 0.0046 (0.0069)  time: 0.1980  data: 0.0003  max mem: 5511
[22:19:50.335135] Epoch: [32]  [260/781]  eta: 0:01:44  lr: 0.000203  training_loss: 1.4534 (1.4757)  mae_loss: 0.0289 (0.0289)  classification_loss: 1.4201 (1.4399)  loss_mask: 0.0053 (0.0069)  time: 0.1982  data: 0.0002  max mem: 5511
[22:19:54.281974] Epoch: [32]  [280/781]  eta: 0:01:40  lr: 0.000202  training_loss: 1.5124 (1.4775)  mae_loss: 0.0294 (0.0289)  classification_loss: 1.4693 (1.4416)  loss_mask: 0.0050 (0.0069)  time: 0.1972  data: 0.0002  max mem: 5511
[22:19:58.217516] Epoch: [32]  [300/781]  eta: 0:01:35  lr: 0.000202  training_loss: 1.4521 (1.4778)  mae_loss: 0.0299 (0.0290)  classification_loss: 1.4129 (1.4418)  loss_mask: 0.0065 (0.0070)  time: 0.1967  data: 0.0003  max mem: 5511
[22:20:02.149883] Epoch: [32]  [320/781]  eta: 0:01:31  lr: 0.000202  training_loss: 1.4336 (1.4748)  mae_loss: 0.0282 (0.0290)  classification_loss: 1.3876 (1.4388)  loss_mask: 0.0057 (0.0070)  time: 0.1965  data: 0.0002  max mem: 5511
[22:20:06.101042] Epoch: [32]  [340/781]  eta: 0:01:27  lr: 0.000202  training_loss: 1.4230 (1.4744)  mae_loss: 0.0286 (0.0290)  classification_loss: 1.3893 (1.4384)  loss_mask: 0.0046 (0.0069)  time: 0.1975  data: 0.0002  max mem: 5511
[22:20:10.045497] Epoch: [32]  [360/781]  eta: 0:01:23  lr: 0.000202  training_loss: 1.4438 (1.4731)  mae_loss: 0.0279 (0.0290)  classification_loss: 1.4027 (1.4372)  loss_mask: 0.0059 (0.0069)  time: 0.1971  data: 0.0002  max mem: 5511
[22:20:13.976528] Epoch: [32]  [380/781]  eta: 0:01:19  lr: 0.000202  training_loss: 1.4818 (1.4736)  mae_loss: 0.0280 (0.0290)  classification_loss: 1.4405 (1.4376)  loss_mask: 0.0062 (0.0070)  time: 0.1965  data: 0.0002  max mem: 5511
[22:20:17.895473] Epoch: [32]  [400/781]  eta: 0:01:15  lr: 0.000202  training_loss: 1.4792 (1.4743)  mae_loss: 0.0268 (0.0289)  classification_loss: 1.4508 (1.4383)  loss_mask: 0.0057 (0.0070)  time: 0.1959  data: 0.0002  max mem: 5511
[22:20:21.829658] Epoch: [32]  [420/781]  eta: 0:01:11  lr: 0.000202  training_loss: 1.5035 (1.4760)  mae_loss: 0.0301 (0.0290)  classification_loss: 1.4666 (1.4400)  loss_mask: 0.0045 (0.0070)  time: 0.1966  data: 0.0003  max mem: 5511
[22:20:25.790145] Epoch: [32]  [440/781]  eta: 0:01:07  lr: 0.000202  training_loss: 1.4646 (1.4762)  mae_loss: 0.0305 (0.0290)  classification_loss: 1.4248 (1.4404)  loss_mask: 0.0048 (0.0069)  time: 0.1979  data: 0.0002  max mem: 5511
[22:20:29.758870] Epoch: [32]  [460/781]  eta: 0:01:03  lr: 0.000202  training_loss: 1.4366 (1.4738)  mae_loss: 0.0280 (0.0290)  classification_loss: 1.3998 (1.4380)  loss_mask: 0.0049 (0.0068)  time: 0.1984  data: 0.0004  max mem: 5511
[22:20:33.688948] Epoch: [32]  [480/781]  eta: 0:00:59  lr: 0.000202  training_loss: 1.5077 (1.4743)  mae_loss: 0.0285 (0.0289)  classification_loss: 1.4756 (1.4386)  loss_mask: 0.0051 (0.0067)  time: 0.1964  data: 0.0002  max mem: 5511
[22:20:37.611623] Epoch: [32]  [500/781]  eta: 0:00:55  lr: 0.000202  training_loss: 1.5058 (1.4748)  mae_loss: 0.0279 (0.0289)  classification_loss: 1.4768 (1.4392)  loss_mask: 0.0046 (0.0067)  time: 0.1960  data: 0.0002  max mem: 5511
[22:20:41.540578] Epoch: [32]  [520/781]  eta: 0:00:51  lr: 0.000201  training_loss: 1.4939 (1.4757)  mae_loss: 0.0295 (0.0289)  classification_loss: 1.4600 (1.4401)  loss_mask: 0.0066 (0.0067)  time: 0.1963  data: 0.0003  max mem: 5511
[22:20:45.478043] Epoch: [32]  [540/781]  eta: 0:00:47  lr: 0.000201  training_loss: 1.4972 (1.4758)  mae_loss: 0.0295 (0.0290)  classification_loss: 1.4593 (1.4402)  loss_mask: 0.0052 (0.0067)  time: 0.1967  data: 0.0002  max mem: 5511
[22:20:49.412803] Epoch: [32]  [560/781]  eta: 0:00:43  lr: 0.000201  training_loss: 1.4371 (1.4757)  mae_loss: 0.0283 (0.0289)  classification_loss: 1.3946 (1.4399)  loss_mask: 0.0084 (0.0068)  time: 0.1967  data: 0.0003  max mem: 5511
[22:20:53.347832] Epoch: [32]  [580/781]  eta: 0:00:39  lr: 0.000201  training_loss: 1.4334 (1.4752)  mae_loss: 0.0288 (0.0289)  classification_loss: 1.3967 (1.4394)  loss_mask: 0.0078 (0.0069)  time: 0.1967  data: 0.0003  max mem: 5511
[22:20:57.321613] Epoch: [32]  [600/781]  eta: 0:00:35  lr: 0.000201  training_loss: 1.4452 (1.4748)  mae_loss: 0.0290 (0.0289)  classification_loss: 1.4079 (1.4389)  loss_mask: 0.0053 (0.0069)  time: 0.1986  data: 0.0003  max mem: 5511
[22:21:01.250745] Epoch: [32]  [620/781]  eta: 0:00:31  lr: 0.000201  training_loss: 1.4675 (1.4743)  mae_loss: 0.0302 (0.0290)  classification_loss: 1.4207 (1.4383)  loss_mask: 0.0079 (0.0070)  time: 0.1964  data: 0.0003  max mem: 5511
[22:21:05.176328] Epoch: [32]  [640/781]  eta: 0:00:27  lr: 0.000201  training_loss: 1.4494 (1.4732)  mae_loss: 0.0279 (0.0290)  classification_loss: 1.4149 (1.4373)  loss_mask: 0.0051 (0.0069)  time: 0.1962  data: 0.0003  max mem: 5511
[22:21:09.078247] Epoch: [32]  [660/781]  eta: 0:00:23  lr: 0.000201  training_loss: 1.4654 (1.4742)  mae_loss: 0.0293 (0.0290)  classification_loss: 1.4292 (1.4383)  loss_mask: 0.0039 (0.0069)  time: 0.1950  data: 0.0002  max mem: 5511
[22:21:13.005313] Epoch: [32]  [680/781]  eta: 0:00:19  lr: 0.000201  training_loss: 1.4374 (1.4734)  mae_loss: 0.0295 (0.0290)  classification_loss: 1.4066 (1.4376)  loss_mask: 0.0043 (0.0068)  time: 0.1962  data: 0.0002  max mem: 5511
[22:21:16.934093] Epoch: [32]  [700/781]  eta: 0:00:16  lr: 0.000201  training_loss: 1.4665 (1.4737)  mae_loss: 0.0290 (0.0290)  classification_loss: 1.4263 (1.4379)  loss_mask: 0.0056 (0.0068)  time: 0.1963  data: 0.0002  max mem: 5511
[22:21:20.882857] Epoch: [32]  [720/781]  eta: 0:00:12  lr: 0.000201  training_loss: 1.4927 (1.4740)  mae_loss: 0.0284 (0.0290)  classification_loss: 1.4540 (1.4382)  loss_mask: 0.0061 (0.0068)  time: 0.1973  data: 0.0003  max mem: 5511
[22:21:24.825976] Epoch: [32]  [740/781]  eta: 0:00:08  lr: 0.000201  training_loss: 1.3811 (1.4721)  mae_loss: 0.0283 (0.0290)  classification_loss: 1.3460 (1.4363)  loss_mask: 0.0057 (0.0068)  time: 0.1971  data: 0.0002  max mem: 5511

[22:21:28.738333] Epoch: [32]  [760/781]  eta: 0:00:04  lr: 0.000200  training_loss: 1.5056 (1.4734)  mae_loss: 0.0275 (0.0290)  classification_loss: 1.4722 (1.4376)  loss_mask: 0.0052 (0.0068)  time: 0.1955  data: 0.0002  max mem: 5511
[22:21:32.648031] Epoch: [32]  [780/781]  eta: 0:00:00  lr: 0.000200  training_loss: 1.4812 (1.4736)  mae_loss: 0.0274 (0.0290)  classification_loss: 1.4487 (1.4378)  loss_mask: 0.0046 (0.0068)  time: 0.1954  data: 0.0002  max mem: 5511
[22:21:32.800728] Epoch: [32] Total time: 0:02:34 (0.1980 s / it)
[22:21:32.801586] Averaged stats: lr: 0.000200  training_loss: 1.4812 (1.4736)  mae_loss: 0.0274 (0.0290)  classification_loss: 1.4487 (1.4378)  loss_mask: 0.0046 (0.0068)
[22:21:33.429752] Test:  [  0/157]  eta: 0:01:37  testing_loss: 0.6563 (0.6563)  acc1: 82.8125 (82.8125)  acc5: 98.4375 (98.4375)  time: 0.6196  data: 0.5901  max mem: 5511
[22:21:33.730477] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.7368 (0.7720)  acc1: 76.5625 (75.8523)  acc5: 98.4375 (98.2955)  time: 0.0835  data: 0.0539  max mem: 5511
[22:21:34.014642] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.7499 (0.7307)  acc1: 76.5625 (76.8601)  acc5: 98.4375 (98.5863)  time: 0.0291  data: 0.0002  max mem: 5511
[22:21:34.305560] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.7572 (0.7406)  acc1: 75.0000 (76.2601)  acc5: 98.4375 (98.4375)  time: 0.0286  data: 0.0004  max mem: 5511
[22:21:34.589026] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.7406 (0.7405)  acc1: 75.0000 (76.5244)  acc5: 98.4375 (98.3613)  time: 0.0286  data: 0.0004  max mem: 5511
[22:21:34.872479] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.7045 (0.7314)  acc1: 76.5625 (77.0833)  acc5: 98.4375 (98.4375)  time: 0.0282  data: 0.0002  max mem: 5511
[22:21:35.155803] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6878 (0.7296)  acc1: 76.5625 (76.7162)  acc5: 98.4375 (98.4631)  time: 0.0282  data: 0.0002  max mem: 5511
[22:21:35.438371] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6840 (0.7200)  acc1: 76.5625 (77.0026)  acc5: 98.4375 (98.5695)  time: 0.0282  data: 0.0002  max mem: 5511
[22:21:35.722742] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7274 (0.7291)  acc1: 76.5625 (76.5239)  acc5: 98.4375 (98.4954)  time: 0.0282  data: 0.0002  max mem: 5511
[22:21:36.007196] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7432 (0.7289)  acc1: 75.0000 (76.6140)  acc5: 98.4375 (98.4375)  time: 0.0283  data: 0.0002  max mem: 5511
[22:21:36.294568] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.7245 (0.7303)  acc1: 76.5625 (76.5625)  acc5: 98.4375 (98.3911)  time: 0.0285  data: 0.0002  max mem: 5511
[22:21:36.585545] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.7290 (0.7293)  acc1: 75.0000 (76.5203)  acc5: 98.4375 (98.3530)  time: 0.0288  data: 0.0005  max mem: 5511
[22:21:36.870198] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.7194 (0.7260)  acc1: 76.5625 (76.5367)  acc5: 98.4375 (98.3084)  time: 0.0286  data: 0.0005  max mem: 5511
[22:21:37.156219] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.7194 (0.7279)  acc1: 78.1250 (76.4790)  acc5: 98.4375 (98.3063)  time: 0.0284  data: 0.0002  max mem: 5511
[22:21:37.438130] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7397 (0.7280)  acc1: 76.5625 (76.4517)  acc5: 98.4375 (98.3156)  time: 0.0283  data: 0.0002  max mem: 5511
[22:21:37.721628] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7397 (0.7277)  acc1: 75.0000 (76.4590)  acc5: 98.4375 (98.2512)  time: 0.0281  data: 0.0001  max mem: 5511
[22:21:37.879262] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7233 (0.7287)  acc1: 75.0000 (76.4400)  acc5: 98.4375 (98.2600)  time: 0.0275  data: 0.0001  max mem: 5511
[22:21:38.044212] Test: Total time: 0:00:05 (0.0334 s / it)
[22:21:38.045586] * Acc@1 76.440 Acc@5 98.260 loss 0.729
[22:21:38.046392] Accuracy of the network on the 10000 test images: 76.4%
[22:21:38.047154] Max accuracy: 76.44%
[22:21:38.464755] log_dir: ./output_dir
[22:21:39.328816] Epoch: [33]  [  0/781]  eta: 0:11:12  lr: 0.000200  training_loss: 1.3787 (1.3787)  mae_loss: 0.0278 (0.0278)  classification_loss: 1.3496 (1.3496)  loss_mask: 0.0013 (0.0013)  time: 0.8610  data: 0.6393  max mem: 5511
[22:21:43.262406] Epoch: [33]  [ 20/781]  eta: 0:02:53  lr: 0.000200  training_loss: 1.4355 (1.4519)  mae_loss: 0.0274 (0.0278)  classification_loss: 1.4046 (1.4186)  loss_mask: 0.0052 (0.0055)  time: 0.1966  data: 0.0002  max mem: 5511
[22:21:47.193479] Epoch: [33]  [ 40/781]  eta: 0:02:37  lr: 0.000200  training_loss: 1.4438 (1.4661)  mae_loss: 0.0289 (0.0284)  classification_loss: 1.4053 (1.4301)  loss_mask: 0.0091 (0.0076)  time: 0.1965  data: 0.0002  max mem: 5511
[22:21:51.127900] Epoch: [33]  [ 60/781]  eta: 0:02:29  lr: 0.000200  training_loss: 1.4610 (1.4695)  mae_loss: 0.0290 (0.0284)  classification_loss: 1.4242 (1.4331)  loss_mask: 0.0080 (0.0080)  time: 0.1966  data: 0.0003  max mem: 5511
[22:21:55.066243] Epoch: [33]  [ 80/781]  eta: 0:02:23  lr: 0.000200  training_loss: 1.4949 (1.4742)  mae_loss: 0.0269 (0.0282)  classification_loss: 1.4542 (1.4380)  loss_mask: 0.0063 (0.0080)  time: 0.1968  data: 0.0002  max mem: 5511
[22:21:59.016892] Epoch: [33]  [100/781]  eta: 0:02:18  lr: 0.000200  training_loss: 1.4521 (1.4697)  mae_loss: 0.0262 (0.0280)  classification_loss: 1.4213 (1.4340)  loss_mask: 0.0056 (0.0078)  time: 0.1975  data: 0.0003  max mem: 5511
[22:22:02.953673] Epoch: [33]  [120/781]  eta: 0:02:13  lr: 0.000200  training_loss: 1.4114 (1.4665)  mae_loss: 0.0269 (0.0279)  classification_loss: 1.3772 (1.4305)  loss_mask: 0.0075 (0.0081)  time: 0.1967  data: 0.0002  max mem: 5511
[22:22:06.916003] Epoch: [33]  [140/781]  eta: 0:02:09  lr: 0.000200  training_loss: 1.5175 (1.4696)  mae_loss: 0.0290 (0.0281)  classification_loss: 1.4778 (1.4329)  loss_mask: 0.0115 (0.0086)  time: 0.1980  data: 0.0002  max mem: 5511
[22:22:10.842782] Epoch: [33]  [160/781]  eta: 0:02:04  lr: 0.000200  training_loss: 1.4515 (1.4670)  mae_loss: 0.0271 (0.0280)  classification_loss: 1.4204 (1.4306)  loss_mask: 0.0062 (0.0084)  time: 0.1963  data: 0.0002  max mem: 5511

[22:22:14.784624] Epoch: [33]  [180/781]  eta: 0:02:00  lr: 0.000200  training_loss: 1.4586 (1.4642)  mae_loss: 0.0290 (0.0281)  classification_loss: 1.4221 (1.4282)  loss_mask: 0.0035 (0.0078)  time: 0.1970  data: 0.0003  max mem: 5511
[22:22:18.714390] Epoch: [33]  [200/781]  eta: 0:01:56  lr: 0.000199  training_loss: 1.4592 (1.4630)  mae_loss: 0.0262 (0.0282)  classification_loss: 1.4195 (1.4272)  loss_mask: 0.0035 (0.0075)  time: 0.1964  data: 0.0003  max mem: 5511
[22:22:22.614377] Epoch: [33]  [220/781]  eta: 0:01:52  lr: 0.000199  training_loss: 1.4199 (1.4608)  mae_loss: 0.0266 (0.0281)  classification_loss: 1.3897 (1.4252)  loss_mask: 0.0046 (0.0075)  time: 0.1949  data: 0.0003  max mem: 5511
[22:22:26.592616] Epoch: [33]  [240/781]  eta: 0:01:47  lr: 0.000199  training_loss: 1.4466 (1.4597)  mae_loss: 0.0286 (0.0282)  classification_loss: 1.4013 (1.4238)  loss_mask: 0.0109 (0.0077)  time: 0.1988  data: 0.0003  max mem: 5511
[22:22:30.513020] Epoch: [33]  [260/781]  eta: 0:01:43  lr: 0.000199  training_loss: 1.4434 (1.4599)  mae_loss: 0.0296 (0.0284)  classification_loss: 1.4079 (1.4240)  loss_mask: 0.0046 (0.0075)  time: 0.1959  data: 0.0002  max mem: 5511
[22:22:34.460629] Epoch: [33]  [280/781]  eta: 0:01:39  lr: 0.000199  training_loss: 1.4658 (1.4605)  mae_loss: 0.0290 (0.0285)  classification_loss: 1.4324 (1.4247)  loss_mask: 0.0051 (0.0074)  time: 0.1973  data: 0.0002  max mem: 5511
[22:22:38.382862] Epoch: [33]  [300/781]  eta: 0:01:35  lr: 0.000199  training_loss: 1.4428 (1.4608)  mae_loss: 0.0287 (0.0285)  classification_loss: 1.4134 (1.4252)  loss_mask: 0.0035 (0.0071)  time: 0.1960  data: 0.0003  max mem: 5511
[22:22:42.328303] Epoch: [33]  [320/781]  eta: 0:01:31  lr: 0.000199  training_loss: 1.5017 (1.4618)  mae_loss: 0.0282 (0.0285)  classification_loss: 1.4510 (1.4262)  loss_mask: 0.0046 (0.0071)  time: 0.1972  data: 0.0002  max mem: 5511
[22:22:46.307765] Epoch: [33]  [340/781]  eta: 0:01:27  lr: 0.000199  training_loss: 1.4311 (1.4614)  mae_loss: 0.0280 (0.0285)  classification_loss: 1.3947 (1.4258)  loss_mask: 0.0059 (0.0071)  time: 0.1989  data: 0.0002  max mem: 5511
[22:22:50.246160] Epoch: [33]  [360/781]  eta: 0:01:23  lr: 0.000199  training_loss: 1.5204 (1.4639)  mae_loss: 0.0285 (0.0285)  classification_loss: 1.4796 (1.4277)  loss_mask: 0.0124 (0.0077)  time: 0.1968  data: 0.0002  max mem: 5511
[22:22:54.177065] Epoch: [33]  [380/781]  eta: 0:01:19  lr: 0.000199  training_loss: 1.4333 (1.4633)  mae_loss: 0.0271 (0.0284)  classification_loss: 1.3908 (1.4272)  loss_mask: 0.0060 (0.0076)  time: 0.1965  data: 0.0002  max mem: 5511
[22:22:58.110505] Epoch: [33]  [400/781]  eta: 0:01:15  lr: 0.000199  training_loss: 1.4579 (1.4618)  mae_loss: 0.0299 (0.0285)  classification_loss: 1.4220 (1.4258)  loss_mask: 0.0042 (0.0075)  time: 0.1966  data: 0.0003  max mem: 5511
[22:23:02.034477] Epoch: [33]  [420/781]  eta: 0:01:11  lr: 0.000199  training_loss: 1.4715 (1.4623)  mae_loss: 0.0302 (0.0286)  classification_loss: 1.4324 (1.4264)  loss_mask: 0.0036 (0.0073)  time: 0.1961  data: 0.0002  max mem: 5511
[22:23:05.994215] Epoch: [33]  [440/781]  eta: 0:01:07  lr: 0.000198  training_loss: 1.4949 (1.4629)  mae_loss: 0.0285 (0.0286)  classification_loss: 1.4632 (1.4271)  loss_mask: 0.0040 (0.0073)  time: 0.1979  data: 0.0002  max mem: 5511
[22:23:09.951536] Epoch: [33]  [460/781]  eta: 0:01:03  lr: 0.000198  training_loss: 1.4362 (1.4624)  mae_loss: 0.0279 (0.0285)  classification_loss: 1.4079 (1.4268)  loss_mask: 0.0034 (0.0071)  time: 0.1977  data: 0.0002  max mem: 5511
[22:23:13.906323] Epoch: [33]  [480/781]  eta: 0:00:59  lr: 0.000198  training_loss: 1.4833 (1.4639)  mae_loss: 0.0271 (0.0285)  classification_loss: 1.4477 (1.4283)  loss_mask: 0.0060 (0.0070)  time: 0.1977  data: 0.0006  max mem: 5511
[22:23:17.833123] Epoch: [33]  [500/781]  eta: 0:00:55  lr: 0.000198  training_loss: 1.4689 (1.4637)  mae_loss: 0.0290 (0.0285)  classification_loss: 1.4311 (1.4283)  loss_mask: 0.0035 (0.0069)  time: 0.1963  data: 0.0002  max mem: 5511
[22:23:21.823014] Epoch: [33]  [520/781]  eta: 0:00:51  lr: 0.000198  training_loss: 1.4710 (1.4640)  mae_loss: 0.0294 (0.0286)  classification_loss: 1.4309 (1.4286)  loss_mask: 0.0041 (0.0068)  time: 0.1994  data: 0.0005  max mem: 5511
[22:23:25.782960] Epoch: [33]  [540/781]  eta: 0:00:47  lr: 0.000198  training_loss: 1.4898 (1.4656)  mae_loss: 0.0297 (0.0286)  classification_loss: 1.4565 (1.4302)  loss_mask: 0.0040 (0.0068)  time: 0.1979  data: 0.0002  max mem: 5511
[22:23:29.737690] Epoch: [33]  [560/781]  eta: 0:00:43  lr: 0.000198  training_loss: 1.4524 (1.4659)  mae_loss: 0.0276 (0.0286)  classification_loss: 1.4214 (1.4305)  loss_mask: 0.0046 (0.0067)  time: 0.1977  data: 0.0002  max mem: 5511
[22:23:33.664780] Epoch: [33]  [580/781]  eta: 0:00:39  lr: 0.000198  training_loss: 1.4777 (1.4660)  mae_loss: 0.0287 (0.0286)  classification_loss: 1.4418 (1.4307)  loss_mask: 0.0051 (0.0067)  time: 0.1963  data: 0.0002  max mem: 5511
[22:23:37.609709] Epoch: [33]  [600/781]  eta: 0:00:35  lr: 0.000198  training_loss: 1.4016 (1.4664)  mae_loss: 0.0288 (0.0286)  classification_loss: 1.3645 (1.4312)  loss_mask: 0.0038 (0.0066)  time: 0.1972  data: 0.0002  max mem: 5511
[22:23:41.547626] Epoch: [33]  [620/781]  eta: 0:00:31  lr: 0.000198  training_loss: 1.4398 (1.4658)  mae_loss: 0.0297 (0.0287)  classification_loss: 1.4022 (1.4305)  loss_mask: 0.0044 (0.0066)  time: 0.1968  data: 0.0002  max mem: 5511
[22:23:45.488008] Epoch: [33]  [640/781]  eta: 0:00:27  lr: 0.000198  training_loss: 1.4024 (1.4638)  mae_loss: 0.0272 (0.0286)  classification_loss: 1.3715 (1.4286)  loss_mask: 0.0051 (0.0066)  time: 0.1969  data: 0.0002  max mem: 5511
[22:23:49.441953] Epoch: [33]  [660/781]  eta: 0:00:23  lr: 0.000198  training_loss: 1.4339 (1.4638)  mae_loss: 0.0293 (0.0287)  classification_loss: 1.3989 (1.4286)  loss_mask: 0.0035 (0.0066)  time: 0.1976  data: 0.0002  max mem: 5511
[22:23:53.375872] Epoch: [33]  [680/781]  eta: 0:00:19  lr: 0.000197  training_loss: 1.4374 (1.4639)  mae_loss: 0.0289 (0.0287)  classification_loss: 1.4003 (1.4287)  loss_mask: 0.0055 (0.0066)  time: 0.1966  data: 0.0003  max mem: 5511
[22:23:57.317789] Epoch: [33]  [700/781]  eta: 0:00:16  lr: 0.000197  training_loss: 1.4689 (1.4643)  mae_loss: 0.0284 (0.0287)  classification_loss: 1.4387 (1.4292)  loss_mask: 0.0031 (0.0065)  time: 0.1968  data: 0.0002  max mem: 5511
[22:24:01.261411] Epoch: [33]  [720/781]  eta: 0:00:12  lr: 0.000197  training_loss: 1.4783 (1.4655)  mae_loss: 0.0293 (0.0287)  classification_loss: 1.4465 (1.4303)  loss_mask: 0.0040 (0.0064)  time: 0.1971  data: 0.0004  max mem: 5511
[22:24:05.181669] Epoch: [33]  [740/781]  eta: 0:00:08  lr: 0.000197  training_loss: 1.3876 (1.4636)  mae_loss: 0.0283 (0.0287)  classification_loss: 1.3541 (1.4285)  loss_mask: 0.0057 (0.0065)  time: 0.1959  data: 0.0002  max mem: 5511
[22:24:09.137579] Epoch: [33]  [760/781]  eta: 0:00:04  lr: 0.000197  training_loss: 1.4648 (1.4642)  mae_loss: 0.0299 (0.0287)  classification_loss: 1.4283 (1.4289)  loss_mask: 0.0071 (0.0066)  time: 0.1977  data: 0.0002  max mem: 5511
[22:24:13.049597] Epoch: [33]  [780/781]  eta: 0:00:00  lr: 0.000197  training_loss: 1.4858 (1.4647)  mae_loss: 0.0291 (0.0287)  classification_loss: 1.4518 (1.4294)  loss_mask: 0.0039 (0.0065)  time: 0.1955  data: 0.0002  max mem: 5511
[22:24:13.196514] Epoch: [33] Total time: 0:02:34 (0.1981 s / it)
[22:24:13.197025] Averaged stats: lr: 0.000197  training_loss: 1.4858 (1.4647)  mae_loss: 0.0291 (0.0287)  classification_loss: 1.4518 (1.4294)  loss_mask: 0.0039 (0.0065)
[22:24:13.835522] Test:  [  0/157]  eta: 0:01:39  testing_loss: 0.6576 (0.6576)  acc1: 79.6875 (79.6875)  acc5: 96.8750 (96.8750)  time: 0.6338  data: 0.6004  max mem: 5511
[22:24:14.126853] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.7620 (0.7867)  acc1: 76.5625 (74.0057)  acc5: 98.4375 (98.7216)  time: 0.0839  data: 0.0549  max mem: 5511
[22:24:14.413099] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.7508 (0.7451)  acc1: 76.5625 (75.8185)  acc5: 98.4375 (98.6607)  time: 0.0287  data: 0.0002  max mem: 5511
[22:24:14.700421] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.7275 (0.7514)  acc1: 76.5625 (75.5040)  acc5: 98.4375 (98.5887)  time: 0.0285  data: 0.0002  max mem: 5511
[22:24:14.985209] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.7215 (0.7466)  acc1: 75.0000 (75.7622)  acc5: 98.4375 (98.4375)  time: 0.0285  data: 0.0003  max mem: 5511
[22:24:15.271733] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6970 (0.7364)  acc1: 78.1250 (76.4400)  acc5: 98.4375 (98.3456)  time: 0.0284  data: 0.0002  max mem: 5511
[22:24:15.558406] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.7097 (0.7354)  acc1: 78.1250 (76.4088)  acc5: 98.4375 (98.3350)  time: 0.0285  data: 0.0002  max mem: 5511
[22:24:15.845480] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7001 (0.7252)  acc1: 78.1250 (76.8046)  acc5: 98.4375 (98.3935)  time: 0.0285  data: 0.0002  max mem: 5511
[22:24:16.135161] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7000 (0.7335)  acc1: 78.1250 (76.5625)  acc5: 96.8750 (98.1867)  time: 0.0287  data: 0.0002  max mem: 5511
[22:24:16.423344] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7314 (0.7309)  acc1: 75.0000 (76.6999)  acc5: 98.4375 (98.2315)  time: 0.0287  data: 0.0002  max mem: 5511
[22:24:16.708033] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.7305 (0.7337)  acc1: 76.5625 (76.8719)  acc5: 98.4375 (98.1900)  time: 0.0285  data: 0.0002  max mem: 5511
[22:24:16.996496] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.7341 (0.7331)  acc1: 76.5625 (76.8440)  acc5: 98.4375 (98.2404)  time: 0.0285  data: 0.0002  max mem: 5511
[22:24:17.284504] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6734 (0.7277)  acc1: 78.1250 (77.0403)  acc5: 98.4375 (98.2825)  time: 0.0287  data: 0.0002  max mem: 5511
[22:24:17.568700] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6707 (0.7282)  acc1: 78.1250 (76.9323)  acc5: 98.4375 (98.2705)  time: 0.0285  data: 0.0002  max mem: 5511
[22:24:17.851991] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7189 (0.7273)  acc1: 76.5625 (76.8839)  acc5: 98.4375 (98.2824)  time: 0.0282  data: 0.0002  max mem: 5511
[22:24:18.133703] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7173 (0.7262)  acc1: 75.0000 (76.8626)  acc5: 98.4375 (98.2926)  time: 0.0281  data: 0.0001  max mem: 5511
[22:24:18.292045] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7189 (0.7273)  acc1: 76.5625 (76.8500)  acc5: 98.4375 (98.2900)  time: 0.0275  data: 0.0001  max mem: 5511
[22:24:18.467550] Test: Total time: 0:00:05 (0.0335 s / it)
[22:24:18.468006] * Acc@1 76.850 Acc@5 98.290 loss 0.727
[22:24:18.468285] Accuracy of the network on the 10000 test images: 76.8%
[22:24:18.468533] Max accuracy: 76.85%
[22:24:18.650469] log_dir: ./output_dir
[22:24:19.605596] Epoch: [34]  [  0/781]  eta: 0:12:24  lr: 0.000197  training_loss: 1.4009 (1.4009)  mae_loss: 0.0260 (0.0260)  classification_loss: 1.3708 (1.3708)  loss_mask: 0.0040 (0.0040)  time: 0.9533  data: 0.7459  max mem: 5511
[22:24:23.538504] Epoch: [34]  [ 20/781]  eta: 0:02:56  lr: 0.000197  training_loss: 1.4246 (1.4204)  mae_loss: 0.0283 (0.0284)  classification_loss: 1.3931 (1.3870)  loss_mask: 0.0049 (0.0050)  time: 0.1966  data: 0.0002  max mem: 5511
[22:24:27.472174] Epoch: [34]  [ 40/781]  eta: 0:02:39  lr: 0.000197  training_loss: 1.4552 (1.4338)  mae_loss: 0.0299 (0.0292)  classification_loss: 1.4215 (1.3983)  loss_mask: 0.0066 (0.0062)  time: 0.1966  data: 0.0002  max mem: 5511
[22:24:31.382534] Epoch: [34]  [ 60/781]  eta: 0:02:30  lr: 0.000197  training_loss: 1.4457 (1.4465)  mae_loss: 0.0292 (0.0293)  classification_loss: 1.4087 (1.4106)  loss_mask: 0.0060 (0.0066)  time: 0.1954  data: 0.0002  max mem: 5511
[22:24:35.383538] Epoch: [34]  [ 80/781]  eta: 0:02:24  lr: 0.000197  training_loss: 1.4330 (1.4499)  mae_loss: 0.0282 (0.0291)  classification_loss: 1.3962 (1.4141)  loss_mask: 0.0051 (0.0067)  time: 0.2000  data: 0.0002  max mem: 5511
[22:24:39.291516] Epoch: [34]  [100/781]  eta: 0:02:19  lr: 0.000197  training_loss: 1.5060 (1.4581)  mae_loss: 0.0280 (0.0291)  classification_loss: 1.4673 (1.4202)  loss_mask: 0.0123 (0.0088)  time: 0.1953  data: 0.0002  max mem: 5511
[22:24:43.236471] Epoch: [34]  [120/781]  eta: 0:02:14  lr: 0.000196  training_loss: 1.5009 (1.4638)  mae_loss: 0.0280 (0.0290)  classification_loss: 1.4529 (1.4250)  loss_mask: 0.0113 (0.0098)  time: 0.1972  data: 0.0002  max mem: 5511
[22:24:47.167668] Epoch: [34]  [140/781]  eta: 0:02:09  lr: 0.000196  training_loss: 1.4375 (1.4588)  mae_loss: 0.0294 (0.0290)  classification_loss: 1.4039 (1.4203)  loss_mask: 0.0049 (0.0095)  time: 0.1965  data: 0.0002  max mem: 5511
[22:24:51.108849] Epoch: [34]  [160/781]  eta: 0:02:05  lr: 0.000196  training_loss: 1.4634 (1.4580)  mae_loss: 0.0286 (0.0290)  classification_loss: 1.4288 (1.4201)  loss_mask: 0.0037 (0.0089)  time: 0.1970  data: 0.0002  max mem: 5511
[22:24:55.043183] Epoch: [34]  [180/781]  eta: 0:02:00  lr: 0.000196  training_loss: 1.4549 (1.4566)  mae_loss: 0.0287 (0.0291)  classification_loss: 1.4259 (1.4190)  loss_mask: 0.0041 (0.0084)  time: 0.1966  data: 0.0002  max mem: 5511
[22:24:58.953587] Epoch: [34]  [200/781]  eta: 0:01:56  lr: 0.000196  training_loss: 1.4844 (1.4594)  mae_loss: 0.0280 (0.0290)  classification_loss: 1.4470 (1.4220)  loss_mask: 0.0054 (0.0083)  time: 0.1954  data: 0.0002  max mem: 5511
[22:25:02.897876] Epoch: [34]  [220/781]  eta: 0:01:52  lr: 0.000196  training_loss: 1.4622 (1.4608)  mae_loss: 0.0289 (0.0289)  classification_loss: 1.4286 (1.4239)  loss_mask: 0.0037 (0.0080)  time: 0.1971  data: 0.0002  max mem: 5511
[22:25:06.822702] Epoch: [34]  [240/781]  eta: 0:01:48  lr: 0.000196  training_loss: 1.4391 (1.4602)  mae_loss: 0.0287 (0.0289)  classification_loss: 1.4090 (1.4236)  loss_mask: 0.0044 (0.0077)  time: 0.1961  data: 0.0002  max mem: 5511
[22:25:10.764760] Epoch: [34]  [260/781]  eta: 0:01:43  lr: 0.000196  training_loss: 1.4308 (1.4576)  mae_loss: 0.0287 (0.0289)  classification_loss: 1.3999 (1.4213)  loss_mask: 0.0033 (0.0074)  time: 0.1970  data: 0.0002  max mem: 5511
[22:25:14.718660] Epoch: [34]  [280/781]  eta: 0:01:39  lr: 0.000196  training_loss: 1.4622 (1.4569)  mae_loss: 0.0299 (0.0290)  classification_loss: 1.4252 (1.4207)  loss_mask: 0.0036 (0.0072)  time: 0.1976  data: 0.0002  max mem: 5511
[22:25:18.660057] Epoch: [34]  [300/781]  eta: 0:01:35  lr: 0.000196  training_loss: 1.4502 (1.4563)  mae_loss: 0.0289 (0.0289)  classification_loss: 1.4168 (1.4202)  loss_mask: 0.0057 (0.0072)  time: 0.1970  data: 0.0005  max mem: 5511
[22:25:22.592796] Epoch: [34]  [320/781]  eta: 0:01:31  lr: 0.000196  training_loss: 1.4506 (1.4562)  mae_loss: 0.0276 (0.0289)  classification_loss: 1.4190 (1.4204)  loss_mask: 0.0032 (0.0069)  time: 0.1966  data: 0.0002  max mem: 5511
[22:25:26.565487] Epoch: [34]  [340/781]  eta: 0:01:27  lr: 0.000196  training_loss: 1.3834 (1.4533)  mae_loss: 0.0277 (0.0289)  classification_loss: 1.3453 (1.4176)  loss_mask: 0.0051 (0.0069)  time: 0.1986  data: 0.0002  max mem: 5511
[22:25:30.504927] Epoch: [34]  [360/781]  eta: 0:01:23  lr: 0.000195  training_loss: 1.4454 (1.4537)  mae_loss: 0.0281 (0.0289)  classification_loss: 1.4142 (1.4179)  loss_mask: 0.0052 (0.0070)  time: 0.1969  data: 0.0002  max mem: 5511
[22:25:34.459911] Epoch: [34]  [380/781]  eta: 0:01:19  lr: 0.000195  training_loss: 1.4496 (1.4541)  mae_loss: 0.0280 (0.0289)  classification_loss: 1.4195 (1.4184)  loss_mask: 0.0050 (0.0069)  time: 0.1977  data: 0.0002  max mem: 5511
[22:25:38.397279] Epoch: [34]  [400/781]  eta: 0:01:15  lr: 0.000195  training_loss: 1.4300 (1.4531)  mae_loss: 0.0289 (0.0289)  classification_loss: 1.3996 (1.4174)  loss_mask: 0.0066 (0.0069)  time: 0.1968  data: 0.0002  max mem: 5511
[22:25:42.359019] Epoch: [34]  [420/781]  eta: 0:01:11  lr: 0.000195  training_loss: 1.4628 (1.4533)  mae_loss: 0.0276 (0.0288)  classification_loss: 1.4329 (1.4176)  loss_mask: 0.0048 (0.0069)  time: 0.1980  data: 0.0003  max mem: 5511
[22:25:46.322922] Epoch: [34]  [440/781]  eta: 0:01:07  lr: 0.000195  training_loss: 1.4285 (1.4522)  mae_loss: 0.0287 (0.0288)  classification_loss: 1.3920 (1.4166)  loss_mask: 0.0038 (0.0068)  time: 0.1981  data: 0.0004  max mem: 5511
[22:25:50.279812] Epoch: [34]  [460/781]  eta: 0:01:03  lr: 0.000195  training_loss: 1.4334 (1.4509)  mae_loss: 0.0279 (0.0288)  classification_loss: 1.4010 (1.4155)  loss_mask: 0.0031 (0.0067)  time: 0.1977  data: 0.0002  max mem: 5511
[22:25:54.213250] Epoch: [34]  [480/781]  eta: 0:00:59  lr: 0.000195  training_loss: 1.4528 (1.4510)  mae_loss: 0.0292 (0.0288)  classification_loss: 1.4210 (1.4157)  loss_mask: 0.0029 (0.0065)  time: 0.1966  data: 0.0002  max mem: 5511
[22:25:58.162671] Epoch: [34]  [500/781]  eta: 0:00:55  lr: 0.000195  training_loss: 1.4546 (1.4511)  mae_loss: 0.0289 (0.0288)  classification_loss: 1.4250 (1.4159)  loss_mask: 0.0025 (0.0064)  time: 0.1974  data: 0.0002  max mem: 5511
[22:26:02.105127] Epoch: [34]  [520/781]  eta: 0:00:51  lr: 0.000195  training_loss: 1.4660 (1.4521)  mae_loss: 0.0280 (0.0288)  classification_loss: 1.4385 (1.4170)  loss_mask: 0.0043 (0.0063)  time: 0.1970  data: 0.0004  max mem: 5511
[22:26:06.048214] Epoch: [34]  [540/781]  eta: 0:00:47  lr: 0.000195  training_loss: 1.5034 (1.4543)  mae_loss: 0.0287 (0.0288)  classification_loss: 1.4598 (1.4188)  loss_mask: 0.0077 (0.0067)  time: 0.1971  data: 0.0003  max mem: 5511
[22:26:10.007215] Epoch: [34]  [560/781]  eta: 0:00:43  lr: 0.000195  training_loss: 1.4358 (1.4538)  mae_loss: 0.0294 (0.0288)  classification_loss: 1.4036 (1.4180)  loss_mask: 0.0080 (0.0070)  time: 0.1979  data: 0.0002  max mem: 5511
[22:26:13.936217] Epoch: [34]  [580/781]  eta: 0:00:39  lr: 0.000194  training_loss: 1.3793 (1.4528)  mae_loss: 0.0280 (0.0288)  classification_loss: 1.3515 (1.4170)  loss_mask: 0.0050 (0.0070)  time: 0.1964  data: 0.0002  max mem: 5511
[22:26:17.903265] Epoch: [34]  [600/781]  eta: 0:00:35  lr: 0.000194  training_loss: 1.4377 (1.4527)  mae_loss: 0.0294 (0.0288)  classification_loss: 1.3967 (1.4170)  loss_mask: 0.0056 (0.0070)  time: 0.1982  data: 0.0002  max mem: 5511
[22:26:21.843065] Epoch: [34]  [620/781]  eta: 0:00:31  lr: 0.000194  training_loss: 1.4541 (1.4528)  mae_loss: 0.0285 (0.0288)  classification_loss: 1.4166 (1.4170)  loss_mask: 0.0063 (0.0070)  time: 0.1969  data: 0.0002  max mem: 5511
[22:26:25.814749] Epoch: [34]  [640/781]  eta: 0:00:27  lr: 0.000194  training_loss: 1.5036 (1.4545)  mae_loss: 0.0299 (0.0288)  classification_loss: 1.4673 (1.4186)  loss_mask: 0.0074 (0.0071)  time: 0.1985  data: 0.0003  max mem: 5511
[22:26:29.761421] Epoch: [34]  [660/781]  eta: 0:00:23  lr: 0.000194  training_loss: 1.4774 (1.4550)  mae_loss: 0.0285 (0.0288)  classification_loss: 1.4390 (1.4192)  loss_mask: 0.0041 (0.0070)  time: 0.1972  data: 0.0002  max mem: 5511
[22:26:33.738317] Epoch: [34]  [680/781]  eta: 0:00:20  lr: 0.000194  training_loss: 1.4237 (1.4551)  mae_loss: 0.0285 (0.0288)  classification_loss: 1.3913 (1.4194)  loss_mask: 0.0048 (0.0070)  time: 0.1987  data: 0.0002  max mem: 5511
[22:26:37.672087] Epoch: [34]  [700/781]  eta: 0:00:16  lr: 0.000194  training_loss: 1.4842 (1.4558)  mae_loss: 0.0288 (0.0288)  classification_loss: 1.4453 (1.4201)  loss_mask: 0.0055 (0.0070)  time: 0.1966  data: 0.0002  max mem: 5511
[22:26:41.627648] Epoch: [34]  [720/781]  eta: 0:00:12  lr: 0.000194  training_loss: 1.4467 (1.4558)  mae_loss: 0.0279 (0.0288)  classification_loss: 1.4035 (1.4200)  loss_mask: 0.0078 (0.0070)  time: 0.1977  data: 0.0006  max mem: 5511
[22:26:45.577733] Epoch: [34]  [740/781]  eta: 0:00:08  lr: 0.000194  training_loss: 1.4139 (1.4543)  mae_loss: 0.0285 (0.0288)  classification_loss: 1.3605 (1.4184)  loss_mask: 0.0106 (0.0071)  time: 0.1974  data: 0.0002  max mem: 5511
[22:26:49.510580] Epoch: [34]  [760/781]  eta: 0:00:04  lr: 0.000194  training_loss: 1.4723 (1.4551)  mae_loss: 0.0292 (0.0288)  classification_loss: 1.4406 (1.4191)  loss_mask: 0.0066 (0.0072)  time: 0.1966  data: 0.0002  max mem: 5511
[22:26:53.419637] Epoch: [34]  [780/781]  eta: 0:00:00  lr: 0.000194  training_loss: 1.4416 (1.4549)  mae_loss: 0.0285 (0.0288)  classification_loss: 1.4132 (1.4189)  loss_mask: 0.0061 (0.0072)  time: 0.1954  data: 0.0002  max mem: 5511
[22:26:53.570362] Epoch: [34] Total time: 0:02:34 (0.1984 s / it)
[22:26:53.570818] Averaged stats: lr: 0.000194  training_loss: 1.4416 (1.4549)  mae_loss: 0.0285 (0.0288)  classification_loss: 1.4132 (1.4189)  loss_mask: 0.0061 (0.0072)
[22:26:54.207577] Test:  [  0/157]  eta: 0:01:39  testing_loss: 0.6675 (0.6675)  acc1: 76.5625 (76.5625)  acc5: 98.4375 (98.4375)  time: 0.6323  data: 0.6024  max mem: 5511
[22:26:54.493452] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.6995 (0.7313)  acc1: 76.5625 (77.2727)  acc5: 98.4375 (98.8636)  time: 0.0833  data: 0.0549  max mem: 5511
[22:26:54.777546] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.6888 (0.6983)  acc1: 76.5625 (77.9018)  acc5: 98.4375 (98.8095)  time: 0.0283  data: 0.0002  max mem: 5511
[22:26:55.065285] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6909 (0.7085)  acc1: 76.5625 (77.0161)  acc5: 98.4375 (98.4879)  time: 0.0285  data: 0.0002  max mem: 5511
[22:26:55.351803] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.7106 (0.7092)  acc1: 75.0000 (77.0960)  acc5: 98.4375 (98.4375)  time: 0.0286  data: 0.0002  max mem: 5511
[22:26:55.645997] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6900 (0.7011)  acc1: 78.1250 (77.7574)  acc5: 98.4375 (98.3456)  time: 0.0289  data: 0.0002  max mem: 5511
[22:26:55.932941] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6900 (0.7018)  acc1: 76.5625 (77.4590)  acc5: 98.4375 (98.4887)  time: 0.0289  data: 0.0002  max mem: 5511
[22:26:56.220260] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6688 (0.6939)  acc1: 78.1250 (77.9489)  acc5: 100.0000 (98.6136)  time: 0.0286  data: 0.0002  max mem: 5511
[22:26:56.519413] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6864 (0.7024)  acc1: 78.1250 (77.6813)  acc5: 98.4375 (98.5725)  time: 0.0292  data: 0.0002  max mem: 5511
[22:26:56.805013] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7009 (0.6992)  acc1: 76.5625 (77.7988)  acc5: 98.4375 (98.5920)  time: 0.0291  data: 0.0002  max mem: 5511
[22:26:57.089583] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.7085 (0.7028)  acc1: 76.5625 (77.6609)  acc5: 98.4375 (98.5767)  time: 0.0284  data: 0.0002  max mem: 5511
[22:26:57.374448] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.7301 (0.7029)  acc1: 76.5625 (77.6042)  acc5: 98.4375 (98.5783)  time: 0.0283  data: 0.0002  max mem: 5511
[22:26:57.660486] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.7017 (0.6996)  acc1: 76.5625 (77.6989)  acc5: 98.4375 (98.6054)  time: 0.0284  data: 0.0002  max mem: 5511
[22:26:57.947213] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.7017 (0.6992)  acc1: 76.5625 (77.6360)  acc5: 100.0000 (98.6403)  time: 0.0285  data: 0.0002  max mem: 5511
[22:26:58.229189] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6658 (0.6967)  acc1: 79.6875 (77.7926)  acc5: 98.4375 (98.6591)  time: 0.0283  data: 0.0002  max mem: 5511
[22:26:58.508836] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6669 (0.6951)  acc1: 79.6875 (77.8974)  acc5: 98.4375 (98.6651)  time: 0.0280  data: 0.0001  max mem: 5511
[22:26:58.659997] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6751 (0.6966)  acc1: 79.6875 (77.8400)  acc5: 98.4375 (98.6800)  time: 0.0270  data: 0.0001  max mem: 5511
[22:26:58.814103] Test: Total time: 0:00:05 (0.0334 s / it)
[22:26:58.814715] * Acc@1 77.840 Acc@5 98.680 loss 0.697
[22:26:58.815005] Accuracy of the network on the 10000 test images: 77.8%
[22:26:58.815190] Max accuracy: 77.84%
[22:26:59.266443] log_dir: ./output_dir
[22:27:00.106116] Epoch: [35]  [  0/781]  eta: 0:10:54  lr: 0.000194  training_loss: 1.3534 (1.3534)  mae_loss: 0.0309 (0.0309)  classification_loss: 1.3098 (1.3098)  loss_mask: 0.0126 (0.0126)  time: 0.8379  data: 0.6233  max mem: 5511
[22:27:04.053996] Epoch: [35]  [ 20/781]  eta: 0:02:53  lr: 0.000194  training_loss: 1.3772 (1.4049)  mae_loss: 0.0275 (0.0283)  classification_loss: 1.3463 (1.3701)  loss_mask: 0.0054 (0.0065)  time: 0.1973  data: 0.0002  max mem: 5511
[22:27:08.016936] Epoch: [35]  [ 40/781]  eta: 0:02:38  lr: 0.000193  training_loss: 1.4269 (1.4141)  mae_loss: 0.0272 (0.0277)  classification_loss: 1.3975 (1.3798)  loss_mask: 0.0055 (0.0066)  time: 0.1981  data: 0.0002  max mem: 5511
[22:27:11.953647] Epoch: [35]  [ 60/781]  eta: 0:02:29  lr: 0.000193  training_loss: 1.4551 (1.4301)  mae_loss: 0.0280 (0.0278)  classification_loss: 1.4179 (1.3955)  loss_mask: 0.0063 (0.0068)  time: 0.1967  data: 0.0003  max mem: 5511
[22:27:15.897749] Epoch: [35]  [ 80/781]  eta: 0:02:23  lr: 0.000193  training_loss: 1.4108 (1.4335)  mae_loss: 0.0287 (0.0282)  classification_loss: 1.3779 (1.3986)  loss_mask: 0.0052 (0.0066)  time: 0.1971  data: 0.0002  max mem: 5511
[22:27:19.815877] Epoch: [35]  [100/781]  eta: 0:02:18  lr: 0.000193  training_loss: 1.4564 (1.4387)  mae_loss: 0.0277 (0.0282)  classification_loss: 1.4288 (1.4042)  loss_mask: 0.0050 (0.0063)  time: 0.1958  data: 0.0002  max mem: 5511
[22:27:23.764597] Epoch: [35]  [120/781]  eta: 0:02:13  lr: 0.000193  training_loss: 1.4327 (1.4374)  mae_loss: 0.0267 (0.0282)  classification_loss: 1.3972 (1.4031)  loss_mask: 0.0052 (0.0061)  time: 0.1973  data: 0.0002  max mem: 5511
[22:27:27.683167] Epoch: [35]  [140/781]  eta: 0:02:09  lr: 0.000193  training_loss: 1.3901 (1.4339)  mae_loss: 0.0287 (0.0282)  classification_loss: 1.3640 (1.3995)  loss_mask: 0.0059 (0.0062)  time: 0.1958  data: 0.0002  max mem: 5511
[22:27:31.614012] Epoch: [35]  [160/781]  eta: 0:02:04  lr: 0.000193  training_loss: 1.4227 (1.4332)  mae_loss: 0.0266 (0.0280)  classification_loss: 1.3891 (1.3987)  loss_mask: 0.0084 (0.0065)  time: 0.1965  data: 0.0002  max mem: 5511
[22:27:35.536695] Epoch: [35]  [180/781]  eta: 0:02:00  lr: 0.000193  training_loss: 1.4174 (1.4316)  mae_loss: 0.0275 (0.0280)  classification_loss: 1.3820 (1.3971)  loss_mask: 0.0053 (0.0065)  time: 0.1961  data: 0.0002  max mem: 5511
[22:27:39.472278] Epoch: [35]  [200/781]  eta: 0:01:56  lr: 0.000193  training_loss: 1.4697 (1.4352)  mae_loss: 0.0287 (0.0281)  classification_loss: 1.4345 (1.4007)  loss_mask: 0.0046 (0.0065)  time: 0.1967  data: 0.0002  max mem: 5511
[22:27:43.425058] Epoch: [35]  [220/781]  eta: 0:01:52  lr: 0.000193  training_loss: 1.4311 (1.4345)  mae_loss: 0.0276 (0.0280)  classification_loss: 1.3970 (1.4001)  loss_mask: 0.0053 (0.0064)  time: 0.1975  data: 0.0002  max mem: 5511
[22:27:47.345264] Epoch: [35]  [240/781]  eta: 0:01:47  lr: 0.000193  training_loss: 1.4088 (1.4349)  mae_loss: 0.0264 (0.0280)  classification_loss: 1.3840 (1.4007)  loss_mask: 0.0030 (0.0062)  time: 0.1959  data: 0.0002  max mem: 5511
[22:27:51.270401] Epoch: [35]  [260/781]  eta: 0:01:43  lr: 0.000192  training_loss: 1.4330 (1.4343)  mae_loss: 0.0284 (0.0280)  classification_loss: 1.4019 (1.4004)  loss_mask: 0.0027 (0.0060)  time: 0.1962  data: 0.0002  max mem: 5511
[22:27:55.214627] Epoch: [35]  [280/781]  eta: 0:01:39  lr: 0.000192  training_loss: 1.4543 (1.4339)  mae_loss: 0.0303 (0.0282)  classification_loss: 1.4170 (1.3997)  loss_mask: 0.0060 (0.0060)  time: 0.1971  data: 0.0002  max mem: 5511
[22:27:59.131284] Epoch: [35]  [300/781]  eta: 0:01:35  lr: 0.000192  training_loss: 1.4515 (1.4345)  mae_loss: 0.0266 (0.0282)  classification_loss: 1.4227 (1.4004)  loss_mask: 0.0043 (0.0060)  time: 0.1957  data: 0.0002  max mem: 5511
[22:28:03.052733] Epoch: [35]  [320/781]  eta: 0:01:31  lr: 0.000192  training_loss: 1.4258 (1.4345)  mae_loss: 0.0274 (0.0282)  classification_loss: 1.3937 (1.4005)  loss_mask: 0.0037 (0.0058)  time: 0.1960  data: 0.0002  max mem: 5511
[22:28:06.982590] Epoch: [35]  [340/781]  eta: 0:01:27  lr: 0.000192  training_loss: 1.4352 (1.4333)  mae_loss: 0.0291 (0.0282)  classification_loss: 1.4042 (1.3994)  loss_mask: 0.0033 (0.0057)  time: 0.1964  data: 0.0002  max mem: 5511
[22:28:10.930442] Epoch: [35]  [360/781]  eta: 0:01:23  lr: 0.000192  training_loss: 1.4484 (1.4353)  mae_loss: 0.0297 (0.0283)  classification_loss: 1.4106 (1.4014)  loss_mask: 0.0036 (0.0057)  time: 0.1973  data: 0.0004  max mem: 5511
[22:28:14.872200] Epoch: [35]  [380/781]  eta: 0:01:19  lr: 0.000192  training_loss: 1.4443 (1.4358)  mae_loss: 0.0285 (0.0283)  classification_loss: 1.4115 (1.4019)  loss_mask: 0.0028 (0.0057)  time: 0.1970  data: 0.0002  max mem: 5511
[22:28:18.823492] Epoch: [35]  [400/781]  eta: 0:01:15  lr: 0.000192  training_loss: 1.4003 (1.4352)  mae_loss: 0.0271 (0.0283)  classification_loss: 1.3621 (1.4012)  loss_mask: 0.0050 (0.0057)  time: 0.1975  data: 0.0003  max mem: 5511
[22:28:22.760626] Epoch: [35]  [420/781]  eta: 0:01:11  lr: 0.000192  training_loss: 1.4531 (1.4365)  mae_loss: 0.0273 (0.0283)  classification_loss: 1.4191 (1.4025)  loss_mask: 0.0034 (0.0057)  time: 0.1968  data: 0.0003  max mem: 5511
[22:28:26.721514] Epoch: [35]  [440/781]  eta: 0:01:07  lr: 0.000192  training_loss: 1.3780 (1.4351)  mae_loss: 0.0280 (0.0283)  classification_loss: 1.3489 (1.4013)  loss_mask: 0.0032 (0.0056)  time: 0.1980  data: 0.0003  max mem: 5511
[22:28:30.660704] Epoch: [35]  [460/781]  eta: 0:01:03  lr: 0.000192  training_loss: 1.3985 (1.4347)  mae_loss: 0.0256 (0.0282)  classification_loss: 1.3701 (1.4009)  loss_mask: 0.0030 (0.0055)  time: 0.1969  data: 0.0002  max mem: 5511
[22:28:34.619784] Epoch: [35]  [480/781]  eta: 0:00:59  lr: 0.000191  training_loss: 1.4116 (1.4336)  mae_loss: 0.0283 (0.0282)  classification_loss: 1.3784 (1.3999)  loss_mask: 0.0052 (0.0055)  time: 0.1979  data: 0.0002  max mem: 5511
[22:28:38.601194] Epoch: [35]  [500/781]  eta: 0:00:55  lr: 0.000191  training_loss: 1.4921 (1.4354)  mae_loss: 0.0281 (0.0283)  classification_loss: 1.4670 (1.4017)  loss_mask: 0.0032 (0.0054)  time: 0.1990  data: 0.0002  max mem: 5511
[22:28:42.544711] Epoch: [35]  [520/781]  eta: 0:00:51  lr: 0.000191  training_loss: 1.4502 (1.4356)  mae_loss: 0.0299 (0.0283)  classification_loss: 1.4158 (1.4019)  loss_mask: 0.0028 (0.0053)  time: 0.1971  data: 0.0002  max mem: 5511
[22:28:46.516759] Epoch: [35]  [540/781]  eta: 0:00:47  lr: 0.000191  training_loss: 1.4595 (1.4370)  mae_loss: 0.0288 (0.0284)  classification_loss: 1.4157 (1.4033)  loss_mask: 0.0034 (0.0053)  time: 0.1985  data: 0.0002  max mem: 5511
[22:28:50.457614] Epoch: [35]  [560/781]  eta: 0:00:43  lr: 0.000191  training_loss: 1.4317 (1.4371)  mae_loss: 0.0277 (0.0284)  classification_loss: 1.4039 (1.4034)  loss_mask: 0.0038 (0.0052)  time: 0.1970  data: 0.0002  max mem: 5511
[22:28:54.382683] Epoch: [35]  [580/781]  eta: 0:00:39  lr: 0.000191  training_loss: 1.4330 (1.4376)  mae_loss: 0.0287 (0.0284)  classification_loss: 1.3871 (1.4039)  loss_mask: 0.0052 (0.0053)  time: 0.1962  data: 0.0002  max mem: 5511
[22:28:58.309302] Epoch: [35]  [600/781]  eta: 0:00:35  lr: 0.000191  training_loss: 1.4530 (1.4381)  mae_loss: 0.0290 (0.0285)  classification_loss: 1.4137 (1.4042)  loss_mask: 0.0086 (0.0054)  time: 0.1962  data: 0.0002  max mem: 5511
[22:29:02.235308] Epoch: [35]  [620/781]  eta: 0:00:31  lr: 0.000191  training_loss: 1.3864 (1.4369)  mae_loss: 0.0284 (0.0285)  classification_loss: 1.3537 (1.4030)  loss_mask: 0.0065 (0.0054)  time: 0.1962  data: 0.0002  max mem: 5511
[22:29:06.157346] Epoch: [35]  [640/781]  eta: 0:00:27  lr: 0.000191  training_loss: 1.4444 (1.4369)  mae_loss: 0.0281 (0.0285)  classification_loss: 1.4082 (1.4030)  loss_mask: 0.0036 (0.0054)  time: 0.1960  data: 0.0003  max mem: 5511
[22:29:10.081852] Epoch: [35]  [660/781]  eta: 0:00:23  lr: 0.000191  training_loss: 1.4574 (1.4371)  mae_loss: 0.0283 (0.0285)  classification_loss: 1.4256 (1.4032)  loss_mask: 0.0033 (0.0054)  time: 0.1961  data: 0.0002  max mem: 5511
[22:29:14.021713] Epoch: [35]  [680/781]  eta: 0:00:19  lr: 0.000191  training_loss: 1.4442 (1.4385)  mae_loss: 0.0292 (0.0285)  classification_loss: 1.4110 (1.4045)  loss_mask: 0.0071 (0.0055)  time: 0.1969  data: 0.0002  max mem: 5511
[22:29:17.978594] Epoch: [35]  [700/781]  eta: 0:00:16  lr: 0.000190  training_loss: 1.4311 (1.4392)  mae_loss: 0.0281 (0.0285)  classification_loss: 1.3919 (1.4051)  loss_mask: 0.0065 (0.0056)  time: 0.1978  data: 0.0002  max mem: 5511
[22:29:21.904798] Epoch: [35]  [720/781]  eta: 0:00:12  lr: 0.000190  training_loss: 1.4155 (1.4393)  mae_loss: 0.0287 (0.0285)  classification_loss: 1.3789 (1.4053)  loss_mask: 0.0040 (0.0055)  time: 0.1962  data: 0.0002  max mem: 5511
[22:29:25.822544] Epoch: [35]  [740/781]  eta: 0:00:08  lr: 0.000190  training_loss: 1.3882 (1.4383)  mae_loss: 0.0281 (0.0285)  classification_loss: 1.3487 (1.4043)  loss_mask: 0.0049 (0.0055)  time: 0.1958  data: 0.0003  max mem: 5511
[22:29:29.752770] Epoch: [35]  [760/781]  eta: 0:00:04  lr: 0.000190  training_loss: 1.4903 (1.4389)  mae_loss: 0.0274 (0.0285)  classification_loss: 1.4455 (1.4049)  loss_mask: 0.0045 (0.0055)  time: 0.1964  data: 0.0002  max mem: 5511
[22:29:33.698181] Epoch: [35]  [780/781]  eta: 0:00:00  lr: 0.000190  training_loss: 1.4700 (1.4392)  mae_loss: 0.0282 (0.0285)  classification_loss: 1.4392 (1.4052)  loss_mask: 0.0038 (0.0055)  time: 0.1972  data: 0.0002  max mem: 5511
[22:29:33.852476] Epoch: [35] Total time: 0:02:34 (0.1979 s / it)
[22:29:33.853121] Averaged stats: lr: 0.000190  training_loss: 1.4700 (1.4392)  mae_loss: 0.0282 (0.0285)  classification_loss: 1.4392 (1.4052)  loss_mask: 0.0038 (0.0055)
[22:29:34.473339] Test:  [  0/157]  eta: 0:01:36  testing_loss: 0.6371 (0.6371)  acc1: 85.9375 (85.9375)  acc5: 100.0000 (100.0000)  time: 0.6156  data: 0.5832  max mem: 5511
[22:29:34.762191] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.7373 (0.7233)  acc1: 78.1250 (76.7045)  acc5: 98.4375 (98.7216)  time: 0.0821  data: 0.0534  max mem: 5511
[22:29:35.052432] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.7040 (0.6992)  acc1: 78.1250 (78.1994)  acc5: 98.4375 (98.4375)  time: 0.0288  data: 0.0003  max mem: 5511
[22:29:35.350203] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.7056 (0.7112)  acc1: 76.5625 (77.6714)  acc5: 98.4375 (98.3367)  time: 0.0292  data: 0.0002  max mem: 5511
[22:29:35.637446] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.7056 (0.7079)  acc1: 78.1250 (78.0488)  acc5: 98.4375 (98.3613)  time: 0.0291  data: 0.0002  max mem: 5511
[22:29:35.930052] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6795 (0.6993)  acc1: 79.6875 (78.4926)  acc5: 98.4375 (98.4069)  time: 0.0289  data: 0.0002  max mem: 5511
[22:29:36.214915] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6669 (0.7020)  acc1: 75.0000 (77.9457)  acc5: 100.0000 (98.5143)  time: 0.0287  data: 0.0002  max mem: 5511
[22:29:36.504328] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6616 (0.6947)  acc1: 78.1250 (78.1470)  acc5: 100.0000 (98.6796)  time: 0.0286  data: 0.0002  max mem: 5511
[22:29:36.797400] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6731 (0.7023)  acc1: 78.1250 (77.7585)  acc5: 98.4375 (98.4761)  time: 0.0290  data: 0.0002  max mem: 5511
[22:29:37.086215] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6731 (0.6983)  acc1: 78.1250 (78.0391)  acc5: 98.4375 (98.4890)  time: 0.0290  data: 0.0002  max mem: 5511
[22:29:37.382801] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.6710 (0.7003)  acc1: 78.1250 (77.8620)  acc5: 98.4375 (98.5458)  time: 0.0291  data: 0.0003  max mem: 5511
[22:29:37.671368] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.7166 (0.7009)  acc1: 76.5625 (77.8716)  acc5: 98.4375 (98.5642)  time: 0.0291  data: 0.0003  max mem: 5511
[22:29:37.963057] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6751 (0.6980)  acc1: 76.5625 (77.9442)  acc5: 98.4375 (98.5537)  time: 0.0289  data: 0.0003  max mem: 5511
[22:29:38.252386] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6506 (0.6993)  acc1: 75.0000 (77.7552)  acc5: 98.4375 (98.5806)  time: 0.0289  data: 0.0003  max mem: 5511
[22:29:38.541615] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6984 (0.6982)  acc1: 78.1250 (77.8590)  acc5: 98.4375 (98.5483)  time: 0.0288  data: 0.0002  max mem: 5511
[22:29:38.823062] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6858 (0.6961)  acc1: 78.1250 (77.9594)  acc5: 98.4375 (98.5513)  time: 0.0284  data: 0.0001  max mem: 5511
[22:29:38.975401] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6858 (0.6978)  acc1: 78.1250 (77.9400)  acc5: 98.4375 (98.5800)  time: 0.0272  data: 0.0001  max mem: 5511
[22:29:39.146633] Test: Total time: 0:00:05 (0.0337 s / it)
[22:29:39.147112] * Acc@1 77.940 Acc@5 98.580 loss 0.698
[22:29:39.147578] Accuracy of the network on the 10000 test images: 77.9%
[22:29:39.147765] Max accuracy: 77.94%
[22:29:39.472669] log_dir: ./output_dir
[22:29:40.369825] Epoch: [36]  [  0/781]  eta: 0:11:38  lr: 0.000190  training_loss: 1.4318 (1.4318)  mae_loss: 0.0255 (0.0255)  classification_loss: 1.4021 (1.4021)  loss_mask: 0.0042 (0.0042)  time: 0.8949  data: 0.6924  max mem: 5511
[22:29:44.299601] Epoch: [36]  [ 20/781]  eta: 0:02:54  lr: 0.000190  training_loss: 1.4500 (1.4391)  mae_loss: 0.0284 (0.0288)  classification_loss: 1.4095 (1.4034)  loss_mask: 0.0056 (0.0069)  time: 0.1963  data: 0.0002  max mem: 5511
[22:29:48.289470] Epoch: [36]  [ 40/781]  eta: 0:02:39  lr: 0.000190  training_loss: 1.4667 (1.4482)  mae_loss: 0.0290 (0.0289)  classification_loss: 1.4296 (1.4134)  loss_mask: 0.0046 (0.0059)  time: 0.1994  data: 0.0002  max mem: 5511
[22:29:52.224563] Epoch: [36]  [ 60/781]  eta: 0:02:30  lr: 0.000190  training_loss: 1.4696 (1.4542)  mae_loss: 0.0296 (0.0292)  classification_loss: 1.4171 (1.4186)  loss_mask: 0.0049 (0.0064)  time: 0.1966  data: 0.0002  max mem: 5511
[22:29:56.170935] Epoch: [36]  [ 80/781]  eta: 0:02:24  lr: 0.000190  training_loss: 1.4759 (1.4557)  mae_loss: 0.0294 (0.0291)  classification_loss: 1.4322 (1.4200)  loss_mask: 0.0059 (0.0066)  time: 0.1972  data: 0.0002  max mem: 5511
[22:30:00.145213] Epoch: [36]  [100/781]  eta: 0:02:19  lr: 0.000190  training_loss: 1.4526 (1.4510)  mae_loss: 0.0295 (0.0291)  classification_loss: 1.4144 (1.4154)  loss_mask: 0.0036 (0.0065)  time: 0.1986  data: 0.0003  max mem: 5511
[22:30:04.101894] Epoch: [36]  [120/781]  eta: 0:02:14  lr: 0.000190  training_loss: 1.4296 (1.4494)  mae_loss: 0.0278 (0.0290)  classification_loss: 1.3932 (1.4137)  loss_mask: 0.0079 (0.0067)  time: 0.1977  data: 0.0002  max mem: 5511
[22:30:08.044138] Epoch: [36]  [140/781]  eta: 0:02:09  lr: 0.000189  training_loss: 1.4187 (1.4454)  mae_loss: 0.0288 (0.0289)  classification_loss: 1.3837 (1.4099)  loss_mask: 0.0060 (0.0067)  time: 0.1970  data: 0.0002  max mem: 5511
[22:30:11.969631] Epoch: [36]  [160/781]  eta: 0:02:05  lr: 0.000189  training_loss: 1.4145 (1.4419)  mae_loss: 0.0278 (0.0288)  classification_loss: 1.3811 (1.4063)  loss_mask: 0.0065 (0.0068)  time: 0.1962  data: 0.0002  max mem: 5511
[22:30:15.912451] Epoch: [36]  [180/781]  eta: 0:02:00  lr: 0.000189  training_loss: 1.4102 (1.4395)  mae_loss: 0.0284 (0.0288)  classification_loss: 1.3744 (1.4036)  loss_mask: 0.0057 (0.0071)  time: 0.1971  data: 0.0003  max mem: 5511
[22:30:19.829718] Epoch: [36]  [200/781]  eta: 0:01:56  lr: 0.000189  training_loss: 1.4406 (1.4395)  mae_loss: 0.0285 (0.0287)  classification_loss: 1.4094 (1.4036)  loss_mask: 0.0060 (0.0071)  time: 0.1958  data: 0.0003  max mem: 5511
[22:30:23.731852] Epoch: [36]  [220/781]  eta: 0:01:52  lr: 0.000189  training_loss: 1.4651 (1.4419)  mae_loss: 0.0292 (0.0288)  classification_loss: 1.4291 (1.4059)  loss_mask: 0.0057 (0.0072)  time: 0.1950  data: 0.0002  max mem: 5511
[22:30:27.656859] Epoch: [36]  [240/781]  eta: 0:01:48  lr: 0.000189  training_loss: 1.3617 (1.4400)  mae_loss: 0.0284 (0.0288)  classification_loss: 1.3301 (1.4041)  loss_mask: 0.0049 (0.0071)  time: 0.1962  data: 0.0002  max mem: 5511
[22:30:31.607049] Epoch: [36]  [260/781]  eta: 0:01:44  lr: 0.000189  training_loss: 1.4724 (1.4393)  mae_loss: 0.0276 (0.0287)  classification_loss: 1.4349 (1.4036)  loss_mask: 0.0041 (0.0069)  time: 0.1974  data: 0.0002  max mem: 5511
[22:30:35.534997] Epoch: [36]  [280/781]  eta: 0:01:39  lr: 0.000189  training_loss: 1.4039 (1.4377)  mae_loss: 0.0279 (0.0286)  classification_loss: 1.3714 (1.4024)  loss_mask: 0.0039 (0.0067)  time: 0.1963  data: 0.0002  max mem: 5511
[22:30:39.449429] Epoch: [36]  [300/781]  eta: 0:01:35  lr: 0.000189  training_loss: 1.4512 (1.4393)  mae_loss: 0.0283 (0.0286)  classification_loss: 1.4180 (1.4042)  loss_mask: 0.0038 (0.0066)  time: 0.1956  data: 0.0003  max mem: 5511
[22:30:43.410032] Epoch: [36]  [320/781]  eta: 0:01:31  lr: 0.000189  training_loss: 1.3915 (1.4379)  mae_loss: 0.0270 (0.0286)  classification_loss: 1.3613 (1.4030)  loss_mask: 0.0034 (0.0064)  time: 0.1980  data: 0.0002  max mem: 5511
[22:30:47.328564] Epoch: [36]  [340/781]  eta: 0:01:27  lr: 0.000189  training_loss: 1.4498 (1.4381)  mae_loss: 0.0296 (0.0286)  classification_loss: 1.4137 (1.4031)  loss_mask: 0.0052 (0.0064)  time: 0.1958  data: 0.0003  max mem: 5511
[22:30:51.293583] Epoch: [36]  [360/781]  eta: 0:01:23  lr: 0.000188  training_loss: 1.4386 (1.4386)  mae_loss: 0.0276 (0.0285)  classification_loss: 1.4032 (1.4035)  loss_mask: 0.0056 (0.0065)  time: 0.1982  data: 0.0003  max mem: 5511
[22:30:55.253080] Epoch: [36]  [380/781]  eta: 0:01:19  lr: 0.000188  training_loss: 1.4526 (1.4400)  mae_loss: 0.0289 (0.0286)  classification_loss: 1.4208 (1.4046)  loss_mask: 0.0067 (0.0068)  time: 0.1979  data: 0.0002  max mem: 5511
[22:30:59.178494] Epoch: [36]  [400/781]  eta: 0:01:15  lr: 0.000188  training_loss: 1.4023 (1.4383)  mae_loss: 0.0284 (0.0286)  classification_loss: 1.3670 (1.4029)  loss_mask: 0.0066 (0.0068)  time: 0.1962  data: 0.0004  max mem: 5511
[22:31:03.126725] Epoch: [36]  [420/781]  eta: 0:01:11  lr: 0.000188  training_loss: 1.4297 (1.4385)  mae_loss: 0.0290 (0.0287)  classification_loss: 1.3682 (1.4028)  loss_mask: 0.0103 (0.0070)  time: 0.1973  data: 0.0002  max mem: 5511
[22:31:07.055976] Epoch: [36]  [440/781]  eta: 0:01:07  lr: 0.000188  training_loss: 1.4470 (1.4395)  mae_loss: 0.0281 (0.0286)  classification_loss: 1.4130 (1.4036)  loss_mask: 0.0084 (0.0072)  time: 0.1964  data: 0.0002  max mem: 5511
[22:31:10.988761] Epoch: [36]  [460/781]  eta: 0:01:03  lr: 0.000188  training_loss: 1.3680 (1.4373)  mae_loss: 0.0260 (0.0286)  classification_loss: 1.3364 (1.4015)  loss_mask: 0.0054 (0.0072)  time: 0.1966  data: 0.0002  max mem: 5511
[22:31:14.918385] Epoch: [36]  [480/781]  eta: 0:00:59  lr: 0.000188  training_loss: 1.4670 (1.4380)  mae_loss: 0.0281 (0.0286)  classification_loss: 1.4292 (1.4023)  loss_mask: 0.0039 (0.0071)  time: 0.1964  data: 0.0002  max mem: 5511
[22:31:18.828391] Epoch: [36]  [500/781]  eta: 0:00:55  lr: 0.000188  training_loss: 1.4854 (1.4394)  mae_loss: 0.0268 (0.0286)  classification_loss: 1.4540 (1.4039)  loss_mask: 0.0037 (0.0070)  time: 0.1954  data: 0.0002  max mem: 5511
[22:31:22.765451] Epoch: [36]  [520/781]  eta: 0:00:51  lr: 0.000188  training_loss: 1.4411 (1.4390)  mae_loss: 0.0290 (0.0286)  classification_loss: 1.4127 (1.4036)  loss_mask: 0.0038 (0.0069)  time: 0.1968  data: 0.0002  max mem: 5511
[22:31:26.690252] Epoch: [36]  [540/781]  eta: 0:00:47  lr: 0.000188  training_loss: 1.4538 (1.4397)  mae_loss: 0.0278 (0.0285)  classification_loss: 1.4229 (1.4044)  loss_mask: 0.0027 (0.0067)  time: 0.1962  data: 0.0002  max mem: 5511
[22:31:30.619525] Epoch: [36]  [560/781]  eta: 0:00:43  lr: 0.000188  training_loss: 1.3884 (1.4390)  mae_loss: 0.0286 (0.0286)  classification_loss: 1.3612 (1.4037)  loss_mask: 0.0037 (0.0067)  time: 0.1964  data: 0.0002  max mem: 5511
[22:31:34.521767] Epoch: [36]  [580/781]  eta: 0:00:39  lr: 0.000187  training_loss: 1.4528 (1.4393)  mae_loss: 0.0292 (0.0286)  classification_loss: 1.4225 (1.4041)  loss_mask: 0.0032 (0.0065)  time: 0.1950  data: 0.0002  max mem: 5511
[22:31:38.436138] Epoch: [36]  [600/781]  eta: 0:00:35  lr: 0.000187  training_loss: 1.3887 (1.4383)  mae_loss: 0.0287 (0.0286)  classification_loss: 1.3564 (1.4032)  loss_mask: 0.0036 (0.0064)  time: 0.1956  data: 0.0002  max mem: 5511
[22:31:42.394595] Epoch: [36]  [620/781]  eta: 0:00:31  lr: 0.000187  training_loss: 1.4697 (1.4385)  mae_loss: 0.0278 (0.0286)  classification_loss: 1.4354 (1.4035)  loss_mask: 0.0032 (0.0064)  time: 0.1978  data: 0.0003  max mem: 5511
[22:31:46.335350] Epoch: [36]  [640/781]  eta: 0:00:27  lr: 0.000187  training_loss: 1.4488 (1.4382)  mae_loss: 0.0277 (0.0285)  classification_loss: 1.4016 (1.4032)  loss_mask: 0.0055 (0.0064)  time: 0.1969  data: 0.0002  max mem: 5511
[22:31:50.282776] Epoch: [36]  [660/781]  eta: 0:00:23  lr: 0.000187  training_loss: 1.3786 (1.4376)  mae_loss: 0.0299 (0.0286)  classification_loss: 1.3437 (1.4026)  loss_mask: 0.0057 (0.0064)  time: 0.1973  data: 0.0002  max mem: 5511
[22:31:54.261534] Epoch: [36]  [680/781]  eta: 0:00:19  lr: 0.000187  training_loss: 1.4385 (1.4376)  mae_loss: 0.0296 (0.0286)  classification_loss: 1.4047 (1.4026)  loss_mask: 0.0048 (0.0064)  time: 0.1989  data: 0.0002  max mem: 5511
[22:31:58.184033] Epoch: [36]  [700/781]  eta: 0:00:16  lr: 0.000187  training_loss: 1.3977 (1.4378)  mae_loss: 0.0274 (0.0286)  classification_loss: 1.3642 (1.4029)  loss_mask: 0.0035 (0.0064)  time: 0.1960  data: 0.0002  max mem: 5511
[22:32:02.104337] Epoch: [36]  [720/781]  eta: 0:00:12  lr: 0.000187  training_loss: 1.4259 (1.4379)  mae_loss: 0.0281 (0.0286)  classification_loss: 1.3917 (1.4028)  loss_mask: 0.0059 (0.0064)  time: 0.1959  data: 0.0002  max mem: 5511
[22:32:06.039534] Epoch: [36]  [740/781]  eta: 0:00:08  lr: 0.000187  training_loss: 1.4153 (1.4372)  mae_loss: 0.0286 (0.0286)  classification_loss: 1.3798 (1.4022)  loss_mask: 0.0036 (0.0064)  time: 0.1966  data: 0.0002  max mem: 5511
[22:32:09.984379] Epoch: [36]  [760/781]  eta: 0:00:04  lr: 0.000187  training_loss: 1.4836 (1.4383)  mae_loss: 0.0278 (0.0286)  classification_loss: 1.4486 (1.4034)  loss_mask: 0.0040 (0.0063)  time: 0.1972  data: 0.0002  max mem: 5511
[22:32:13.906364] Epoch: [36]  [780/781]  eta: 0:00:00  lr: 0.000187  training_loss: 1.4206 (1.4384)  mae_loss: 0.0279 (0.0286)  classification_loss: 1.3890 (1.4035)  loss_mask: 0.0029 (0.0063)  time: 0.1960  data: 0.0002  max mem: 5511
[22:32:14.064364] Epoch: [36] Total time: 0:02:34 (0.1979 s / it)
[22:32:14.065042] Averaged stats: lr: 0.000187  training_loss: 1.4206 (1.4384)  mae_loss: 0.0279 (0.0286)  classification_loss: 1.3890 (1.4035)  loss_mask: 0.0029 (0.0063)
[22:32:14.721549] Test:  [  0/157]  eta: 0:01:42  testing_loss: 0.6479 (0.6479)  acc1: 78.1250 (78.1250)  acc5: 96.8750 (96.8750)  time: 0.6515  data: 0.6209  max mem: 5511
[22:32:15.013592] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.7599 (0.7381)  acc1: 76.5625 (75.2841)  acc5: 98.4375 (98.7216)  time: 0.0855  data: 0.0566  max mem: 5511
[22:32:15.299918] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.7174 (0.7036)  acc1: 75.0000 (76.8601)  acc5: 98.4375 (98.7351)  time: 0.0287  data: 0.0002  max mem: 5511
[22:32:15.589633] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6934 (0.7018)  acc1: 78.1250 (77.4194)  acc5: 98.4375 (98.5887)  time: 0.0286  data: 0.0002  max mem: 5511
[22:32:15.879559] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6974 (0.7047)  acc1: 78.1250 (77.2104)  acc5: 98.4375 (98.4756)  time: 0.0289  data: 0.0002  max mem: 5511
[22:32:16.168342] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6814 (0.6953)  acc1: 76.5625 (77.5429)  acc5: 98.4375 (98.4988)  time: 0.0288  data: 0.0002  max mem: 5511
[22:32:16.457453] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6814 (0.6974)  acc1: 76.5625 (77.2797)  acc5: 100.0000 (98.6168)  time: 0.0287  data: 0.0002  max mem: 5511
[22:32:16.741450] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6501 (0.6856)  acc1: 78.1250 (77.8389)  acc5: 100.0000 (98.7456)  time: 0.0285  data: 0.0002  max mem: 5511
[22:32:17.026754] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6557 (0.6958)  acc1: 79.6875 (77.5270)  acc5: 100.0000 (98.7076)  time: 0.0283  data: 0.0002  max mem: 5511
[22:32:17.310457] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6849 (0.6924)  acc1: 76.5625 (77.7473)  acc5: 98.4375 (98.7466)  time: 0.0283  data: 0.0002  max mem: 5511
[22:32:17.593372] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.6891 (0.6956)  acc1: 78.1250 (77.6300)  acc5: 98.4375 (98.7160)  time: 0.0282  data: 0.0002  max mem: 5511
[22:32:17.877559] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.7253 (0.6942)  acc1: 78.1250 (77.7449)  acc5: 98.4375 (98.7190)  time: 0.0282  data: 0.0002  max mem: 5511
[22:32:18.160950] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6589 (0.6911)  acc1: 78.1250 (77.7763)  acc5: 98.4375 (98.6829)  time: 0.0282  data: 0.0002  max mem: 5511
[22:32:18.445747] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6875 (0.6910)  acc1: 75.0000 (77.6837)  acc5: 98.4375 (98.6880)  time: 0.0283  data: 0.0002  max mem: 5511
[22:32:18.727462] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7163 (0.6919)  acc1: 75.0000 (77.5598)  acc5: 98.4375 (98.6813)  time: 0.0282  data: 0.0001  max mem: 5511
[22:32:19.010584] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6824 (0.6914)  acc1: 78.1250 (77.5041)  acc5: 98.4375 (98.6548)  time: 0.0281  data: 0.0001  max mem: 5511
[22:32:19.162068] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6548 (0.6919)  acc1: 78.1250 (77.4600)  acc5: 98.4375 (98.6600)  time: 0.0271  data: 0.0001  max mem: 5511
[22:32:19.310101] Test: Total time: 0:00:05 (0.0334 s / it)
[22:32:19.310525] * Acc@1 77.460 Acc@5 98.660 loss 0.692
[22:32:19.310813] Accuracy of the network on the 10000 test images: 77.5%
[22:32:19.310983] Max accuracy: 77.94%
[22:32:19.516307] log_dir: ./output_dir
[22:32:20.360869] Epoch: [37]  [  0/781]  eta: 0:10:58  lr: 0.000187  training_loss: 1.4139 (1.4139)  mae_loss: 0.0295 (0.0295)  classification_loss: 1.3803 (1.3803)  loss_mask: 0.0041 (0.0041)  time: 0.8427  data: 0.6196  max mem: 5511
[22:32:24.317734] Epoch: [37]  [ 20/781]  eta: 0:02:53  lr: 0.000186  training_loss: 1.3605 (1.3851)  mae_loss: 0.0286 (0.0276)  classification_loss: 1.3313 (1.3544)  loss_mask: 0.0026 (0.0031)  time: 0.1977  data: 0.0001  max mem: 5511
[22:32:28.307455] Epoch: [37]  [ 40/781]  eta: 0:02:38  lr: 0.000186  training_loss: 1.4267 (1.4131)  mae_loss: 0.0289 (0.0285)  classification_loss: 1.3946 (1.3810)  loss_mask: 0.0027 (0.0037)  time: 0.1994  data: 0.0002  max mem: 5511
[22:32:32.256846] Epoch: [37]  [ 60/781]  eta: 0:02:30  lr: 0.000186  training_loss: 1.4721 (1.4296)  mae_loss: 0.0275 (0.0285)  classification_loss: 1.4349 (1.3966)  loss_mask: 0.0046 (0.0045)  time: 0.1974  data: 0.0002  max mem: 5511
[22:32:36.230543] Epoch: [37]  [ 80/781]  eta: 0:02:24  lr: 0.000186  training_loss: 1.4079 (1.4290)  mae_loss: 0.0284 (0.0285)  classification_loss: 1.3713 (1.3954)  loss_mask: 0.0056 (0.0051)  time: 0.1986  data: 0.0003  max mem: 5511
[22:32:40.178481] Epoch: [37]  [100/781]  eta: 0:02:19  lr: 0.000186  training_loss: 1.4255 (1.4300)  mae_loss: 0.0284 (0.0286)  classification_loss: 1.3929 (1.3966)  loss_mask: 0.0040 (0.0048)  time: 0.1973  data: 0.0003  max mem: 5511
[22:32:44.120404] Epoch: [37]  [120/781]  eta: 0:02:14  lr: 0.000186  training_loss: 1.3791 (1.4224)  mae_loss: 0.0268 (0.0284)  classification_loss: 1.3448 (1.3893)  loss_mask: 0.0032 (0.0047)  time: 0.1970  data: 0.0002  max mem: 5511
[22:32:48.078825] Epoch: [37]  [140/781]  eta: 0:02:09  lr: 0.000186  training_loss: 1.4203 (1.4256)  mae_loss: 0.0282 (0.0285)  classification_loss: 1.3883 (1.3926)  loss_mask: 0.0037 (0.0046)  time: 0.1978  data: 0.0002  max mem: 5511
[22:32:52.053142] Epoch: [37]  [160/781]  eta: 0:02:05  lr: 0.000186  training_loss: 1.4220 (1.4251)  mae_loss: 0.0290 (0.0285)  classification_loss: 1.3923 (1.3916)  loss_mask: 0.0073 (0.0049)  time: 0.1986  data: 0.0002  max mem: 5511
[22:32:56.005979] Epoch: [37]  [180/781]  eta: 0:02:01  lr: 0.000186  training_loss: 1.4177 (1.4259)  mae_loss: 0.0288 (0.0285)  classification_loss: 1.3734 (1.3921)  loss_mask: 0.0043 (0.0053)  time: 0.1976  data: 0.0003  max mem: 5511
[22:32:59.922735] Epoch: [37]  [200/781]  eta: 0:01:56  lr: 0.000186  training_loss: 1.4520 (1.4316)  mae_loss: 0.0270 (0.0285)  classification_loss: 1.4106 (1.3970)  loss_mask: 0.0110 (0.0061)  time: 0.1957  data: 0.0002  max mem: 5511
[22:33:03.856476] Epoch: [37]  [220/781]  eta: 0:01:52  lr: 0.000186  training_loss: 1.4481 (1.4319)  mae_loss: 0.0292 (0.0286)  classification_loss: 1.3993 (1.3962)  loss_mask: 0.0108 (0.0070)  time: 0.1966  data: 0.0002  max mem: 5511
[22:33:07.805499] Epoch: [37]  [240/781]  eta: 0:01:48  lr: 0.000185  training_loss: 1.3933 (1.4313)  mae_loss: 0.0287 (0.0287)  classification_loss: 1.3579 (1.3952)  loss_mask: 0.0079 (0.0075)  time: 0.1974  data: 0.0002  max mem: 5511
[22:33:11.721417] Epoch: [37]  [260/781]  eta: 0:01:44  lr: 0.000185  training_loss: 1.4036 (1.4303)  mae_loss: 0.0264 (0.0285)  classification_loss: 1.3793 (1.3945)  loss_mask: 0.0036 (0.0073)  time: 0.1957  data: 0.0002  max mem: 5511
[22:33:15.632601] Epoch: [37]  [280/781]  eta: 0:01:40  lr: 0.000185  training_loss: 1.4296 (1.4294)  mae_loss: 0.0275 (0.0285)  classification_loss: 1.3996 (1.3939)  loss_mask: 0.0038 (0.0070)  time: 0.1955  data: 0.0002  max mem: 5511
[22:33:19.577883] Epoch: [37]  [300/781]  eta: 0:01:35  lr: 0.000185  training_loss: 1.4576 (1.4302)  mae_loss: 0.0284 (0.0285)  classification_loss: 1.4319 (1.3948)  loss_mask: 0.0041 (0.0069)  time: 0.1972  data: 0.0002  max mem: 5511
[22:33:23.534037] Epoch: [37]  [320/781]  eta: 0:01:31  lr: 0.000185  training_loss: 1.4147 (1.4308)  mae_loss: 0.0288 (0.0285)  classification_loss: 1.3711 (1.3952)  loss_mask: 0.0071 (0.0071)  time: 0.1977  data: 0.0002  max mem: 5511
[22:33:27.477304] Epoch: [37]  [340/781]  eta: 0:01:27  lr: 0.000185  training_loss: 1.3970 (1.4306)  mae_loss: 0.0285 (0.0285)  classification_loss: 1.3678 (1.3951)  loss_mask: 0.0046 (0.0070)  time: 0.1971  data: 0.0002  max mem: 5511
[22:33:31.442877] Epoch: [37]  [360/781]  eta: 0:01:23  lr: 0.000185  training_loss: 1.4805 (1.4318)  mae_loss: 0.0296 (0.0286)  classification_loss: 1.4409 (1.3962)  loss_mask: 0.0074 (0.0071)  time: 0.1981  data: 0.0002  max mem: 5511
[22:33:35.372529] Epoch: [37]  [380/781]  eta: 0:01:19  lr: 0.000185  training_loss: 1.4290 (1.4316)  mae_loss: 0.0304 (0.0286)  classification_loss: 1.3955 (1.3959)  loss_mask: 0.0046 (0.0070)  time: 0.1964  data: 0.0002  max mem: 5511
[22:33:39.322805] Epoch: [37]  [400/781]  eta: 0:01:15  lr: 0.000185  training_loss: 1.4136 (1.4319)  mae_loss: 0.0282 (0.0287)  classification_loss: 1.3804 (1.3962)  loss_mask: 0.0049 (0.0070)  time: 0.1974  data: 0.0002  max mem: 5511
[22:33:43.276289] Epoch: [37]  [420/781]  eta: 0:01:11  lr: 0.000185  training_loss: 1.4317 (1.4321)  mae_loss: 0.0280 (0.0287)  classification_loss: 1.3967 (1.3964)  loss_mask: 0.0056 (0.0071)  time: 0.1975  data: 0.0004  max mem: 5511
[22:33:47.238273] Epoch: [37]  [440/781]  eta: 0:01:07  lr: 0.000185  training_loss: 1.3979 (1.4317)  mae_loss: 0.0275 (0.0286)  classification_loss: 1.3681 (1.3961)  loss_mask: 0.0045 (0.0070)  time: 0.1980  data: 0.0002  max mem: 5511
[22:33:51.173820] Epoch: [37]  [460/781]  eta: 0:01:03  lr: 0.000184  training_loss: 1.3970 (1.4307)  mae_loss: 0.0269 (0.0286)  classification_loss: 1.3698 (1.3952)  loss_mask: 0.0047 (0.0069)  time: 0.1967  data: 0.0002  max mem: 5511
[22:33:55.094620] Epoch: [37]  [480/781]  eta: 0:00:59  lr: 0.000184  training_loss: 1.4397 (1.4305)  mae_loss: 0.0284 (0.0286)  classification_loss: 1.3985 (1.3951)  loss_mask: 0.0038 (0.0069)  time: 0.1960  data: 0.0003  max mem: 5511
[22:33:59.013184] Epoch: [37]  [500/781]  eta: 0:00:55  lr: 0.000184  training_loss: 1.4204 (1.4304)  mae_loss: 0.0269 (0.0285)  classification_loss: 1.3888 (1.3952)  loss_mask: 0.0035 (0.0067)  time: 0.1958  data: 0.0002  max mem: 5511
[22:34:02.929929] Epoch: [37]  [520/781]  eta: 0:00:51  lr: 0.000184  training_loss: 1.4061 (1.4303)  mae_loss: 0.0297 (0.0286)  classification_loss: 1.3751 (1.3951)  loss_mask: 0.0024 (0.0066)  time: 0.1958  data: 0.0002  max mem: 5511
[22:34:06.881792] Epoch: [37]  [540/781]  eta: 0:00:47  lr: 0.000184  training_loss: 1.4407 (1.4313)  mae_loss: 0.0273 (0.0286)  classification_loss: 1.4116 (1.3962)  loss_mask: 0.0032 (0.0065)  time: 0.1975  data: 0.0002  max mem: 5511
[22:34:10.825565] Epoch: [37]  [560/781]  eta: 0:00:43  lr: 0.000184  training_loss: 1.3490 (1.4303)  mae_loss: 0.0288 (0.0286)  classification_loss: 1.3211 (1.3953)  loss_mask: 0.0035 (0.0064)  time: 0.1971  data: 0.0003  max mem: 5511
[22:34:14.811625] Epoch: [37]  [580/781]  eta: 0:00:39  lr: 0.000184  training_loss: 1.3965 (1.4303)  mae_loss: 0.0276 (0.0285)  classification_loss: 1.3662 (1.3954)  loss_mask: 0.0044 (0.0063)  time: 0.1992  data: 0.0002  max mem: 5511
[22:34:18.735420] Epoch: [37]  [600/781]  eta: 0:00:35  lr: 0.000184  training_loss: 1.3833 (1.4298)  mae_loss: 0.0277 (0.0285)  classification_loss: 1.3540 (1.3950)  loss_mask: 0.0034 (0.0062)  time: 0.1961  data: 0.0002  max mem: 5511
[22:34:22.736812] Epoch: [37]  [620/781]  eta: 0:00:31  lr: 0.000184  training_loss: 1.3873 (1.4291)  mae_loss: 0.0276 (0.0285)  classification_loss: 1.3562 (1.3943)  loss_mask: 0.0047 (0.0062)  time: 0.2000  data: 0.0003  max mem: 5511
[22:34:26.664628] Epoch: [37]  [640/781]  eta: 0:00:27  lr: 0.000184  training_loss: 1.4641 (1.4301)  mae_loss: 0.0295 (0.0286)  classification_loss: 1.4257 (1.3954)  loss_mask: 0.0039 (0.0062)  time: 0.1963  data: 0.0002  max mem: 5511
[22:34:30.610222] Epoch: [37]  [660/781]  eta: 0:00:23  lr: 0.000184  training_loss: 1.4417 (1.4304)  mae_loss: 0.0293 (0.0286)  classification_loss: 1.4082 (1.3957)  loss_mask: 0.0028 (0.0061)  time: 0.1972  data: 0.0004  max mem: 5511
[22:34:34.565195] Epoch: [37]  [680/781]  eta: 0:00:20  lr: 0.000183  training_loss: 1.4484 (1.4311)  mae_loss: 0.0268 (0.0286)  classification_loss: 1.4198 (1.3965)  loss_mask: 0.0026 (0.0060)  time: 0.1977  data: 0.0002  max mem: 5511
[22:34:38.491370] Epoch: [37]  [700/781]  eta: 0:00:16  lr: 0.000183  training_loss: 1.4546 (1.4325)  mae_loss: 0.0283 (0.0286)  classification_loss: 1.4233 (1.3979)  loss_mask: 0.0031 (0.0060)  time: 0.1962  data: 0.0002  max mem: 5511
[22:34:42.439447] Epoch: [37]  [720/781]  eta: 0:00:12  lr: 0.000183  training_loss: 1.4176 (1.4327)  mae_loss: 0.0279 (0.0286)  classification_loss: 1.3851 (1.3982)  loss_mask: 0.0033 (0.0059)  time: 0.1973  data: 0.0002  max mem: 5511
[22:34:46.372936] Epoch: [37]  [740/781]  eta: 0:00:08  lr: 0.000183  training_loss: 1.3934 (1.4321)  mae_loss: 0.0267 (0.0285)  classification_loss: 1.3678 (1.3976)  loss_mask: 0.0043 (0.0060)  time: 0.1966  data: 0.0003  max mem: 5511
[22:34:50.280505] Epoch: [37]  [760/781]  eta: 0:00:04  lr: 0.000183  training_loss: 1.4233 (1.4327)  mae_loss: 0.0283 (0.0285)  classification_loss: 1.3969 (1.3982)  loss_mask: 0.0059 (0.0060)  time: 0.1953  data: 0.0002  max mem: 5511
[22:34:54.229270] Epoch: [37]  [780/781]  eta: 0:00:00  lr: 0.000183  training_loss: 1.3877 (1.4317)  mae_loss: 0.0269 (0.0285)  classification_loss: 1.3553 (1.3971)  loss_mask: 0.0089 (0.0061)  time: 0.1973  data: 0.0002  max mem: 5511
[22:34:54.393759] Epoch: [37] Total time: 0:02:34 (0.1983 s / it)
[22:34:54.394520] Averaged stats: lr: 0.000183  training_loss: 1.3877 (1.4317)  mae_loss: 0.0269 (0.0285)  classification_loss: 1.3553 (1.3971)  loss_mask: 0.0089 (0.0061)
[22:34:55.038608] Test:  [  0/157]  eta: 0:01:38  testing_loss: 0.6647 (0.6647)  acc1: 78.1250 (78.1250)  acc5: 98.4375 (98.4375)  time: 0.6245  data: 0.5949  max mem: 5511
[22:34:55.338246] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.6831 (0.6836)  acc1: 78.1250 (77.6989)  acc5: 100.0000 (99.1477)  time: 0.0839  data: 0.0546  max mem: 5511
[22:34:55.625457] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.6723 (0.6655)  acc1: 78.1250 (78.3482)  acc5: 100.0000 (99.1071)  time: 0.0292  data: 0.0004  max mem: 5511
[22:34:55.910916] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6723 (0.6807)  acc1: 78.1250 (77.9738)  acc5: 98.4375 (98.9415)  time: 0.0285  data: 0.0002  max mem: 5511
[22:34:56.202458] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6988 (0.6849)  acc1: 78.1250 (77.7439)  acc5: 98.4375 (98.8567)  time: 0.0287  data: 0.0002  max mem: 5511
[22:34:56.488754] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6666 (0.6769)  acc1: 79.6875 (78.1863)  acc5: 98.4375 (98.7439)  time: 0.0287  data: 0.0002  max mem: 5511
[22:34:56.776989] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6484 (0.6754)  acc1: 79.6875 (78.0994)  acc5: 98.4375 (98.7193)  time: 0.0286  data: 0.0002  max mem: 5511
[22:34:57.062819] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6484 (0.6693)  acc1: 79.6875 (78.1690)  acc5: 98.4375 (98.7236)  time: 0.0286  data: 0.0002  max mem: 5511
[22:34:57.346685] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6682 (0.6795)  acc1: 78.1250 (77.8742)  acc5: 98.4375 (98.6883)  time: 0.0284  data: 0.0002  max mem: 5511
[22:34:57.629815] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6712 (0.6752)  acc1: 78.1250 (78.0391)  acc5: 98.4375 (98.6607)  time: 0.0282  data: 0.0002  max mem: 5511
[22:34:57.914980] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.6587 (0.6770)  acc1: 76.5625 (77.8929)  acc5: 98.4375 (98.6850)  time: 0.0283  data: 0.0002  max mem: 5511
[22:34:58.205909] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6770 (0.6761)  acc1: 76.5625 (77.9702)  acc5: 98.4375 (98.6205)  time: 0.0286  data: 0.0002  max mem: 5511
[22:34:58.493202] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6384 (0.6712)  acc1: 78.1250 (78.1637)  acc5: 98.4375 (98.6441)  time: 0.0288  data: 0.0002  max mem: 5511
[22:34:58.778558] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6384 (0.6713)  acc1: 79.6875 (78.2801)  acc5: 98.4375 (98.6403)  time: 0.0285  data: 0.0002  max mem: 5511
[22:34:59.063054] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6782 (0.6713)  acc1: 79.6875 (78.4131)  acc5: 98.4375 (98.6148)  time: 0.0283  data: 0.0002  max mem: 5511
[22:34:59.343788] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6507 (0.6688)  acc1: 79.6875 (78.4665)  acc5: 98.4375 (98.6548)  time: 0.0281  data: 0.0001  max mem: 5511
[22:34:59.495012] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6552 (0.6705)  acc1: 79.6875 (78.4300)  acc5: 98.4375 (98.6500)  time: 0.0271  data: 0.0001  max mem: 5511
[22:34:59.646928] Test: Total time: 0:00:05 (0.0334 s / it)
[22:34:59.647387] * Acc@1 78.430 Acc@5 98.650 loss 0.670
[22:34:59.647670] Accuracy of the network on the 10000 test images: 78.4%
[22:34:59.647871] Max accuracy: 78.43%
[22:34:59.906783] log_dir: ./output_dir
[22:35:00.796144] Epoch: [38]  [  0/781]  eta: 0:11:32  lr: 0.000183  training_loss: 1.0498 (1.0498)  mae_loss: 0.0241 (0.0241)  classification_loss: 1.0148 (1.0148)  loss_mask: 0.0109 (0.0109)  time: 0.8873  data: 0.6612  max mem: 5511
[22:35:04.751735] Epoch: [38]  [ 20/781]  eta: 0:02:55  lr: 0.000183  training_loss: 1.4427 (1.4111)  mae_loss: 0.0292 (0.0295)  classification_loss: 1.4071 (1.3724)  loss_mask: 0.0074 (0.0091)  time: 0.1977  data: 0.0003  max mem: 5511
[22:35:08.675312] Epoch: [38]  [ 40/781]  eta: 0:02:38  lr: 0.000183  training_loss: 1.4286 (1.4177)  mae_loss: 0.0275 (0.0285)  classification_loss: 1.3846 (1.3821)  loss_mask: 0.0043 (0.0071)  time: 0.1961  data: 0.0003  max mem: 5511
[22:35:12.692535] Epoch: [38]  [ 60/781]  eta: 0:02:31  lr: 0.000183  training_loss: 1.4007 (1.4223)  mae_loss: 0.0291 (0.0287)  classification_loss: 1.3752 (1.3872)  loss_mask: 0.0036 (0.0063)  time: 0.2008  data: 0.0005  max mem: 5511
[22:35:16.645132] Epoch: [38]  [ 80/781]  eta: 0:02:24  lr: 0.000183  training_loss: 1.3855 (1.4243)  mae_loss: 0.0270 (0.0285)  classification_loss: 1.3500 (1.3899)  loss_mask: 0.0042 (0.0059)  time: 0.1975  data: 0.0002  max mem: 5511
[22:35:20.577659] Epoch: [38]  [100/781]  eta: 0:02:19  lr: 0.000182  training_loss: 1.4315 (1.4253)  mae_loss: 0.0272 (0.0284)  classification_loss: 1.3988 (1.3912)  loss_mask: 0.0042 (0.0057)  time: 0.1966  data: 0.0002  max mem: 5511
[22:35:24.511600] Epoch: [38]  [120/781]  eta: 0:02:14  lr: 0.000182  training_loss: 1.3786 (1.4174)  mae_loss: 0.0270 (0.0282)  classification_loss: 1.3448 (1.3838)  loss_mask: 0.0032 (0.0054)  time: 0.1966  data: 0.0002  max mem: 5511
[22:35:28.447783] Epoch: [38]  [140/781]  eta: 0:02:09  lr: 0.000182  training_loss: 1.3850 (1.4128)  mae_loss: 0.0271 (0.0282)  classification_loss: 1.3450 (1.3793)  loss_mask: 0.0029 (0.0053)  time: 0.1967  data: 0.0003  max mem: 5511
[22:35:32.368793] Epoch: [38]  [160/781]  eta: 0:02:05  lr: 0.000182  training_loss: 1.4229 (1.4142)  mae_loss: 0.0282 (0.0282)  classification_loss: 1.3735 (1.3805)  loss_mask: 0.0047 (0.0055)  time: 0.1960  data: 0.0002  max mem: 5511
[22:35:36.356842] Epoch: [38]  [180/781]  eta: 0:02:00  lr: 0.000182  training_loss: 1.4402 (1.4145)  mae_loss: 0.0277 (0.0282)  classification_loss: 1.4052 (1.3808)  loss_mask: 0.0040 (0.0054)  time: 0.1993  data: 0.0003  max mem: 5511
[22:35:40.315103] Epoch: [38]  [200/781]  eta: 0:01:56  lr: 0.000182  training_loss: 1.4225 (1.4180)  mae_loss: 0.0280 (0.0282)  classification_loss: 1.3815 (1.3841)  loss_mask: 0.0058 (0.0058)  time: 0.1978  data: 0.0002  max mem: 5511
[22:35:44.244327] Epoch: [38]  [220/781]  eta: 0:01:52  lr: 0.000182  training_loss: 1.3869 (1.4168)  mae_loss: 0.0271 (0.0281)  classification_loss: 1.3528 (1.3828)  loss_mask: 0.0077 (0.0059)  time: 0.1963  data: 0.0003  max mem: 5511
[22:35:48.203877] Epoch: [38]  [240/781]  eta: 0:01:48  lr: 0.000182  training_loss: 1.4339 (1.4184)  mae_loss: 0.0270 (0.0281)  classification_loss: 1.4010 (1.3846)  loss_mask: 0.0032 (0.0057)  time: 0.1979  data: 0.0002  max mem: 5511
[22:35:52.126727] Epoch: [38]  [260/781]  eta: 0:01:44  lr: 0.000182  training_loss: 1.4061 (1.4162)  mae_loss: 0.0280 (0.0281)  classification_loss: 1.3759 (1.3825)  loss_mask: 0.0032 (0.0056)  time: 0.1961  data: 0.0002  max mem: 5511
[22:35:56.055089] Epoch: [38]  [280/781]  eta: 0:01:40  lr: 0.000182  training_loss: 1.4410 (1.4175)  mae_loss: 0.0269 (0.0280)  classification_loss: 1.4111 (1.3838)  loss_mask: 0.0050 (0.0056)  time: 0.1963  data: 0.0002  max mem: 5511
[22:35:59.968784] Epoch: [38]  [300/781]  eta: 0:01:35  lr: 0.000182  training_loss: 1.4690 (1.4192)  mae_loss: 0.0272 (0.0280)  classification_loss: 1.4402 (1.3857)  loss_mask: 0.0040 (0.0055)  time: 0.1956  data: 0.0002  max mem: 5511
[22:36:03.907929] Epoch: [38]  [320/781]  eta: 0:01:31  lr: 0.000181  training_loss: 1.4170 (1.4194)  mae_loss: 0.0276 (0.0281)  classification_loss: 1.3852 (1.3857)  loss_mask: 0.0059 (0.0056)  time: 0.1969  data: 0.0004  max mem: 5511
[22:36:07.868826] Epoch: [38]  [340/781]  eta: 0:01:27  lr: 0.000181  training_loss: 1.4121 (1.4198)  mae_loss: 0.0294 (0.0281)  classification_loss: 1.3754 (1.3861)  loss_mask: 0.0043 (0.0056)  time: 0.1980  data: 0.0003  max mem: 5511
[22:36:11.790539] Epoch: [38]  [360/781]  eta: 0:01:23  lr: 0.000181  training_loss: 1.4524 (1.4213)  mae_loss: 0.0271 (0.0281)  classification_loss: 1.4245 (1.3876)  loss_mask: 0.0045 (0.0056)  time: 0.1960  data: 0.0002  max mem: 5511
[22:36:15.707161] Epoch: [38]  [380/781]  eta: 0:01:19  lr: 0.000181  training_loss: 1.3717 (1.4186)  mae_loss: 0.0276 (0.0281)  classification_loss: 1.3371 (1.3850)  loss_mask: 0.0033 (0.0055)  time: 0.1957  data: 0.0002  max mem: 5511
[22:36:19.618952] Epoch: [38]  [400/781]  eta: 0:01:15  lr: 0.000181  training_loss: 1.3629 (1.4164)  mae_loss: 0.0282 (0.0282)  classification_loss: 1.3238 (1.3828)  loss_mask: 0.0036 (0.0054)  time: 0.1955  data: 0.0004  max mem: 5511
[22:36:23.545681] Epoch: [38]  [420/781]  eta: 0:01:11  lr: 0.000181  training_loss: 1.3740 (1.4165)  mae_loss: 0.0283 (0.0282)  classification_loss: 1.3409 (1.3829)  loss_mask: 0.0034 (0.0053)  time: 0.1963  data: 0.0002  max mem: 5511
[22:36:27.482883] Epoch: [38]  [440/781]  eta: 0:01:07  lr: 0.000181  training_loss: 1.4058 (1.4169)  mae_loss: 0.0279 (0.0282)  classification_loss: 1.3744 (1.3834)  loss_mask: 0.0034 (0.0053)  time: 0.1968  data: 0.0002  max mem: 5511
[22:36:31.410525] Epoch: [38]  [460/781]  eta: 0:01:03  lr: 0.000181  training_loss: 1.4248 (1.4170)  mae_loss: 0.0267 (0.0281)  classification_loss: 1.3936 (1.3836)  loss_mask: 0.0042 (0.0053)  time: 0.1963  data: 0.0002  max mem: 5511
[22:36:35.369079] Epoch: [38]  [480/781]  eta: 0:00:59  lr: 0.000181  training_loss: 1.4575 (1.4184)  mae_loss: 0.0275 (0.0282)  classification_loss: 1.4120 (1.3847)  loss_mask: 0.0088 (0.0055)  time: 0.1978  data: 0.0002  max mem: 5511
[22:36:39.336872] Epoch: [38]  [500/781]  eta: 0:00:55  lr: 0.000181  training_loss: 1.3791 (1.4182)  mae_loss: 0.0277 (0.0282)  classification_loss: 1.3408 (1.3845)  loss_mask: 0.0038 (0.0056)  time: 0.1983  data: 0.0002  max mem: 5511
[22:36:43.338529] Epoch: [38]  [520/781]  eta: 0:00:51  lr: 0.000180  training_loss: 1.3563 (1.4170)  mae_loss: 0.0275 (0.0282)  classification_loss: 1.3149 (1.3832)  loss_mask: 0.0040 (0.0056)  time: 0.2000  data: 0.0004  max mem: 5511
[22:36:47.316738] Epoch: [38]  [540/781]  eta: 0:00:47  lr: 0.000180  training_loss: 1.4376 (1.4176)  mae_loss: 0.0271 (0.0282)  classification_loss: 1.4017 (1.3839)  loss_mask: 0.0037 (0.0055)  time: 0.1988  data: 0.0002  max mem: 5511
[22:36:51.260534] Epoch: [38]  [560/781]  eta: 0:00:43  lr: 0.000180  training_loss: 1.3752 (1.4170)  mae_loss: 0.0275 (0.0282)  classification_loss: 1.3375 (1.3832)  loss_mask: 0.0061 (0.0056)  time: 0.1971  data: 0.0002  max mem: 5511
[22:36:55.198413] Epoch: [38]  [580/781]  eta: 0:00:39  lr: 0.000180  training_loss: 1.4330 (1.4174)  mae_loss: 0.0289 (0.0283)  classification_loss: 1.3760 (1.3833)  loss_mask: 0.0081 (0.0059)  time: 0.1968  data: 0.0002  max mem: 5511
[22:36:59.152126] Epoch: [38]  [600/781]  eta: 0:00:35  lr: 0.000180  training_loss: 1.4136 (1.4177)  mae_loss: 0.0266 (0.0282)  classification_loss: 1.3763 (1.3835)  loss_mask: 0.0087 (0.0060)  time: 0.1976  data: 0.0002  max mem: 5511
[22:37:03.103210] Epoch: [38]  [620/781]  eta: 0:00:31  lr: 0.000180  training_loss: 1.3562 (1.4169)  mae_loss: 0.0291 (0.0283)  classification_loss: 1.3242 (1.3827)  loss_mask: 0.0052 (0.0060)  time: 0.1975  data: 0.0002  max mem: 5511
[22:37:07.035633] Epoch: [38]  [640/781]  eta: 0:00:27  lr: 0.000180  training_loss: 1.3955 (1.4164)  mae_loss: 0.0261 (0.0282)  classification_loss: 1.3663 (1.3822)  loss_mask: 0.0035 (0.0059)  time: 0.1965  data: 0.0002  max mem: 5511
[22:37:10.965230] Epoch: [38]  [660/781]  eta: 0:00:23  lr: 0.000180  training_loss: 1.4320 (1.4173)  mae_loss: 0.0280 (0.0282)  classification_loss: 1.4010 (1.3833)  loss_mask: 0.0034 (0.0058)  time: 0.1964  data: 0.0002  max mem: 5511
[22:37:14.962524] Epoch: [38]  [680/781]  eta: 0:00:20  lr: 0.000180  training_loss: 1.4183 (1.4178)  mae_loss: 0.0293 (0.0283)  classification_loss: 1.3869 (1.3837)  loss_mask: 0.0039 (0.0058)  time: 0.1998  data: 0.0003  max mem: 5511
[22:37:18.890680] Epoch: [38]  [700/781]  eta: 0:00:16  lr: 0.000180  training_loss: 1.4442 (1.4176)  mae_loss: 0.0290 (0.0283)  classification_loss: 1.4107 (1.3835)  loss_mask: 0.0023 (0.0057)  time: 0.1963  data: 0.0002  max mem: 5511
[22:37:22.824480] Epoch: [38]  [720/781]  eta: 0:00:12  lr: 0.000180  training_loss: 1.4531 (1.4189)  mae_loss: 0.0270 (0.0283)  classification_loss: 1.4268 (1.3849)  loss_mask: 0.0036 (0.0057)  time: 0.1966  data: 0.0002  max mem: 5511
[22:37:26.752470] Epoch: [38]  [740/781]  eta: 0:00:08  lr: 0.000179  training_loss: 1.3750 (1.4179)  mae_loss: 0.0269 (0.0283)  classification_loss: 1.3463 (1.3840)  loss_mask: 0.0023 (0.0056)  time: 0.1963  data: 0.0002  max mem: 5511
[22:37:30.682903] Epoch: [38]  [760/781]  eta: 0:00:04  lr: 0.000179  training_loss: 1.4667 (1.4192)  mae_loss: 0.0282 (0.0283)  classification_loss: 1.4350 (1.3853)  loss_mask: 0.0035 (0.0056)  time: 0.1964  data: 0.0002  max mem: 5511
[22:37:34.614463] Epoch: [38]  [780/781]  eta: 0:00:00  lr: 0.000179  training_loss: 1.4129 (1.4187)  mae_loss: 0.0288 (0.0283)  classification_loss: 1.3778 (1.3848)  loss_mask: 0.0033 (0.0055)  time: 0.1965  data: 0.0002  max mem: 5511
[22:37:34.767666] Epoch: [38] Total time: 0:02:34 (0.1983 s / it)
[22:37:34.768462] Averaged stats: lr: 0.000179  training_loss: 1.4129 (1.4187)  mae_loss: 0.0288 (0.0283)  classification_loss: 1.3778 (1.3848)  loss_mask: 0.0033 (0.0055)
[22:37:35.428019] Test:  [  0/157]  eta: 0:01:42  testing_loss: 0.6248 (0.6248)  acc1: 82.8125 (82.8125)  acc5: 98.4375 (98.4375)  time: 0.6549  data: 0.6234  max mem: 5511
[22:37:35.721806] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.6742 (0.6899)  acc1: 78.1250 (77.9830)  acc5: 98.4375 (99.0057)  time: 0.0860  data: 0.0568  max mem: 5511
[22:37:36.009119] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.6742 (0.6704)  acc1: 78.1250 (78.7946)  acc5: 98.4375 (99.0327)  time: 0.0289  data: 0.0002  max mem: 5511
[22:37:36.298336] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.7019 (0.6918)  acc1: 76.5625 (77.8730)  acc5: 98.4375 (98.6895)  time: 0.0285  data: 0.0002  max mem: 5511
[22:37:36.585213] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.7104 (0.6976)  acc1: 76.5625 (77.7439)  acc5: 98.4375 (98.6280)  time: 0.0285  data: 0.0002  max mem: 5511
[22:37:36.868821] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6787 (0.6874)  acc1: 78.1250 (78.3088)  acc5: 98.4375 (98.5907)  time: 0.0284  data: 0.0002  max mem: 5511
[22:37:37.156516] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6557 (0.6860)  acc1: 79.6875 (78.2275)  acc5: 98.4375 (98.6168)  time: 0.0284  data: 0.0002  max mem: 5511
[22:37:37.440240] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6661 (0.6788)  acc1: 79.6875 (78.4331)  acc5: 98.4375 (98.6796)  time: 0.0284  data: 0.0002  max mem: 5511
[22:37:37.723888] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6661 (0.6849)  acc1: 78.1250 (78.1250)  acc5: 98.4375 (98.7076)  time: 0.0282  data: 0.0002  max mem: 5511
[22:37:38.007463] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6970 (0.6827)  acc1: 78.1250 (78.3482)  acc5: 98.4375 (98.6264)  time: 0.0282  data: 0.0002  max mem: 5511
[22:37:38.290836] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.7037 (0.6897)  acc1: 78.1250 (78.0012)  acc5: 98.4375 (98.5767)  time: 0.0282  data: 0.0002  max mem: 5511
[22:37:38.576534] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.7068 (0.6871)  acc1: 76.5625 (78.0968)  acc5: 98.4375 (98.5923)  time: 0.0283  data: 0.0002  max mem: 5511
[22:37:38.859763] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6915 (0.6857)  acc1: 78.1250 (78.0863)  acc5: 98.4375 (98.6054)  time: 0.0283  data: 0.0002  max mem: 5511
[22:37:39.143903] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6939 (0.6859)  acc1: 78.1250 (78.1131)  acc5: 98.4375 (98.6403)  time: 0.0282  data: 0.0002  max mem: 5511
[22:37:39.427755] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7022 (0.6880)  acc1: 76.5625 (78.0363)  acc5: 98.4375 (98.6259)  time: 0.0282  data: 0.0002  max mem: 5511
[22:37:39.709504] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7028 (0.6865)  acc1: 75.0000 (78.0319)  acc5: 98.4375 (98.6134)  time: 0.0281  data: 0.0001  max mem: 5511
[22:37:39.861500] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6830 (0.6883)  acc1: 78.1250 (77.9500)  acc5: 98.4375 (98.6200)  time: 0.0271  data: 0.0001  max mem: 5511
[22:37:40.006921] Test: Total time: 0:00:05 (0.0333 s / it)
[22:37:40.007392] * Acc@1 77.950 Acc@5 98.620 loss 0.688
[22:37:40.007686] Accuracy of the network on the 10000 test images: 78.0%
[22:37:40.007872] Max accuracy: 78.43%
[22:37:40.211904] log_dir: ./output_dir
[22:37:41.119005] Epoch: [39]  [  0/781]  eta: 0:11:47  lr: 0.000179  training_loss: 1.1626 (1.1626)  mae_loss: 0.0253 (0.0253)  classification_loss: 1.1347 (1.1347)  loss_mask: 0.0025 (0.0025)  time: 0.9053  data: 0.6944  max mem: 5511
[22:37:45.034393] Epoch: [39]  [ 20/781]  eta: 0:02:54  lr: 0.000179  training_loss: 1.3969 (1.3953)  mae_loss: 0.0280 (0.0287)  classification_loss: 1.3622 (1.3629)  loss_mask: 0.0035 (0.0038)  time: 0.1957  data: 0.0002  max mem: 5511
[22:37:48.957516] Epoch: [39]  [ 40/781]  eta: 0:02:37  lr: 0.000179  training_loss: 1.4591 (1.4197)  mae_loss: 0.0279 (0.0282)  classification_loss: 1.4228 (1.3830)  loss_mask: 0.0057 (0.0085)  time: 0.1960  data: 0.0002  max mem: 5511
[22:37:52.878573] Epoch: [39]  [ 60/781]  eta: 0:02:29  lr: 0.000179  training_loss: 1.3554 (1.4228)  mae_loss: 0.0273 (0.0281)  classification_loss: 1.3175 (1.3838)  loss_mask: 0.0115 (0.0109)  time: 0.1960  data: 0.0002  max mem: 5511
[22:37:56.833943] Epoch: [39]  [ 80/781]  eta: 0:02:23  lr: 0.000179  training_loss: 1.3772 (1.4208)  mae_loss: 0.0268 (0.0279)  classification_loss: 1.3491 (1.3828)  loss_mask: 0.0059 (0.0101)  time: 0.1977  data: 0.0002  max mem: 5511
[22:38:00.749648] Epoch: [39]  [100/781]  eta: 0:02:18  lr: 0.000179  training_loss: 1.3883 (1.4152)  mae_loss: 0.0285 (0.0279)  classification_loss: 1.3467 (1.3780)  loss_mask: 0.0050 (0.0093)  time: 0.1957  data: 0.0002  max mem: 5511
[22:38:04.685250] Epoch: [39]  [120/781]  eta: 0:02:13  lr: 0.000179  training_loss: 1.3826 (1.4089)  mae_loss: 0.0265 (0.0279)  classification_loss: 1.3483 (1.3727)  loss_mask: 0.0032 (0.0084)  time: 0.1967  data: 0.0002  max mem: 5511
[22:38:08.648858] Epoch: [39]  [140/781]  eta: 0:02:09  lr: 0.000179  training_loss: 1.3737 (1.4064)  mae_loss: 0.0274 (0.0278)  classification_loss: 1.3413 (1.3710)  loss_mask: 0.0027 (0.0076)  time: 0.1981  data: 0.0004  max mem: 5511
[22:38:12.580675] Epoch: [39]  [160/781]  eta: 0:02:04  lr: 0.000178  training_loss: 1.3440 (1.3994)  mae_loss: 0.0266 (0.0276)  classification_loss: 1.3153 (1.3647)  loss_mask: 0.0039 (0.0072)  time: 0.1965  data: 0.0002  max mem: 5511
[22:38:16.556793] Epoch: [39]  [180/781]  eta: 0:02:00  lr: 0.000178  training_loss: 1.3883 (1.3981)  mae_loss: 0.0284 (0.0277)  classification_loss: 1.3558 (1.3637)  loss_mask: 0.0025 (0.0067)  time: 0.1987  data: 0.0003  max mem: 5511
[22:38:20.522564] Epoch: [39]  [200/781]  eta: 0:01:56  lr: 0.000178  training_loss: 1.4233 (1.3998)  mae_loss: 0.0258 (0.0276)  classification_loss: 1.3831 (1.3658)  loss_mask: 0.0026 (0.0064)  time: 0.1982  data: 0.0003  max mem: 5511
[22:38:24.443265] Epoch: [39]  [220/781]  eta: 0:01:52  lr: 0.000178  training_loss: 1.4141 (1.4024)  mae_loss: 0.0277 (0.0277)  classification_loss: 1.3819 (1.3686)  loss_mask: 0.0036 (0.0061)  time: 0.1960  data: 0.0002  max mem: 5511
[22:38:28.426506] Epoch: [39]  [240/781]  eta: 0:01:48  lr: 0.000178  training_loss: 1.4569 (1.4073)  mae_loss: 0.0281 (0.0277)  classification_loss: 1.4282 (1.3735)  loss_mask: 0.0047 (0.0061)  time: 0.1991  data: 0.0003  max mem: 5511
[22:38:32.406019] Epoch: [39]  [260/781]  eta: 0:01:44  lr: 0.000178  training_loss: 1.4150 (1.4097)  mae_loss: 0.0270 (0.0277)  classification_loss: 1.3863 (1.3759)  loss_mask: 0.0040 (0.0060)  time: 0.1989  data: 0.0003  max mem: 5511
[22:38:36.321758] Epoch: [39]  [280/781]  eta: 0:01:39  lr: 0.000178  training_loss: 1.4151 (1.4116)  mae_loss: 0.0277 (0.0278)  classification_loss: 1.3868 (1.3779)  loss_mask: 0.0032 (0.0060)  time: 0.1957  data: 0.0002  max mem: 5511
[22:38:40.286003] Epoch: [39]  [300/781]  eta: 0:01:35  lr: 0.000178  training_loss: 1.3730 (1.4086)  mae_loss: 0.0274 (0.0277)  classification_loss: 1.3378 (1.3747)  loss_mask: 0.0108 (0.0062)  time: 0.1981  data: 0.0002  max mem: 5511
[22:38:44.235082] Epoch: [39]  [320/781]  eta: 0:01:31  lr: 0.000178  training_loss: 1.3869 (1.4089)  mae_loss: 0.0273 (0.0277)  classification_loss: 1.3494 (1.3748)  loss_mask: 0.0074 (0.0064)  time: 0.1973  data: 0.0002  max mem: 5511
[22:38:48.174992] Epoch: [39]  [340/781]  eta: 0:01:27  lr: 0.000178  training_loss: 1.4137 (1.4097)  mae_loss: 0.0281 (0.0278)  classification_loss: 1.3757 (1.3755)  loss_mask: 0.0056 (0.0065)  time: 0.1969  data: 0.0002  max mem: 5511
[22:38:52.123697] Epoch: [39]  [360/781]  eta: 0:01:23  lr: 0.000178  training_loss: 1.3986 (1.4099)  mae_loss: 0.0259 (0.0277)  classification_loss: 1.3593 (1.3755)  loss_mask: 0.0091 (0.0067)  time: 0.1973  data: 0.0002  max mem: 5511
[22:38:56.063097] Epoch: [39]  [380/781]  eta: 0:01:19  lr: 0.000177  training_loss: 1.4400 (1.4110)  mae_loss: 0.0281 (0.0277)  classification_loss: 1.4056 (1.3767)  loss_mask: 0.0042 (0.0066)  time: 0.1969  data: 0.0002  max mem: 5511
[22:39:00.002620] Epoch: [39]  [400/781]  eta: 0:01:15  lr: 0.000177  training_loss: 1.4089 (1.4118)  mae_loss: 0.0290 (0.0278)  classification_loss: 1.3756 (1.3775)  loss_mask: 0.0047 (0.0065)  time: 0.1969  data: 0.0003  max mem: 5511
[22:39:03.954442] Epoch: [39]  [420/781]  eta: 0:01:11  lr: 0.000177  training_loss: 1.3801 (1.4110)  mae_loss: 0.0269 (0.0278)  classification_loss: 1.3480 (1.3768)  loss_mask: 0.0038 (0.0064)  time: 0.1974  data: 0.0002  max mem: 5511
[22:39:07.874656] Epoch: [39]  [440/781]  eta: 0:01:07  lr: 0.000177  training_loss: 1.3818 (1.4105)  mae_loss: 0.0272 (0.0278)  classification_loss: 1.3540 (1.3764)  loss_mask: 0.0035 (0.0063)  time: 0.1959  data: 0.0002  max mem: 5511
[22:39:11.798493] Epoch: [39]  [460/781]  eta: 0:01:03  lr: 0.000177  training_loss: 1.4034 (1.4102)  mae_loss: 0.0272 (0.0278)  classification_loss: 1.3630 (1.3762)  loss_mask: 0.0025 (0.0062)  time: 0.1961  data: 0.0002  max mem: 5511
[22:39:15.812096] Epoch: [39]  [480/781]  eta: 0:00:59  lr: 0.000177  training_loss: 1.4181 (1.4111)  mae_loss: 0.0275 (0.0278)  classification_loss: 1.3855 (1.3771)  loss_mask: 0.0044 (0.0062)  time: 0.2006  data: 0.0002  max mem: 5511
[22:39:19.738097] Epoch: [39]  [500/781]  eta: 0:00:55  lr: 0.000177  training_loss: 1.4141 (1.4112)  mae_loss: 0.0285 (0.0278)  classification_loss: 1.3812 (1.3773)  loss_mask: 0.0044 (0.0061)  time: 0.1962  data: 0.0002  max mem: 5511
[22:39:23.664186] Epoch: [39]  [520/781]  eta: 0:00:51  lr: 0.000177  training_loss: 1.4024 (1.4109)  mae_loss: 0.0285 (0.0279)  classification_loss: 1.3687 (1.3769)  loss_mask: 0.0047 (0.0061)  time: 0.1962  data: 0.0002  max mem: 5511
[22:39:27.592866] Epoch: [39]  [540/781]  eta: 0:00:47  lr: 0.000177  training_loss: 1.4332 (1.4116)  mae_loss: 0.0269 (0.0279)  classification_loss: 1.3981 (1.3777)  loss_mask: 0.0038 (0.0060)  time: 0.1963  data: 0.0002  max mem: 5511
[22:39:31.511621] Epoch: [39]  [560/781]  eta: 0:00:43  lr: 0.000177  training_loss: 1.4445 (1.4124)  mae_loss: 0.0281 (0.0279)  classification_loss: 1.4055 (1.3786)  loss_mask: 0.0045 (0.0060)  time: 0.1959  data: 0.0002  max mem: 5511
[22:39:35.425912] Epoch: [39]  [580/781]  eta: 0:00:39  lr: 0.000176  training_loss: 1.3905 (1.4128)  mae_loss: 0.0270 (0.0279)  classification_loss: 1.3613 (1.3790)  loss_mask: 0.0029 (0.0059)  time: 0.1956  data: 0.0002  max mem: 5511
[22:39:39.348792] Epoch: [39]  [600/781]  eta: 0:00:35  lr: 0.000176  training_loss: 1.3523 (1.4111)  mae_loss: 0.0283 (0.0279)  classification_loss: 1.3204 (1.3774)  loss_mask: 0.0019 (0.0058)  time: 0.1961  data: 0.0002  max mem: 5511
[22:39:43.333626] Epoch: [39]  [620/781]  eta: 0:00:31  lr: 0.000176  training_loss: 1.4027 (1.4111)  mae_loss: 0.0290 (0.0280)  classification_loss: 1.3632 (1.3774)  loss_mask: 0.0023 (0.0057)  time: 0.1992  data: 0.0002  max mem: 5511
[22:39:47.263243] Epoch: [39]  [640/781]  eta: 0:00:27  lr: 0.000176  training_loss: 1.3911 (1.4107)  mae_loss: 0.0278 (0.0280)  classification_loss: 1.3623 (1.3771)  loss_mask: 0.0019 (0.0056)  time: 0.1964  data: 0.0002  max mem: 5511
[22:39:51.190857] Epoch: [39]  [660/781]  eta: 0:00:23  lr: 0.000176  training_loss: 1.3950 (1.4107)  mae_loss: 0.0290 (0.0280)  classification_loss: 1.3569 (1.3772)  loss_mask: 0.0031 (0.0055)  time: 0.1963  data: 0.0002  max mem: 5511
[22:39:55.124137] Epoch: [39]  [680/781]  eta: 0:00:19  lr: 0.000176  training_loss: 1.4588 (1.4119)  mae_loss: 0.0283 (0.0280)  classification_loss: 1.4283 (1.3785)  loss_mask: 0.0025 (0.0054)  time: 0.1966  data: 0.0002  max mem: 5511
[22:39:59.057796] Epoch: [39]  [700/781]  eta: 0:00:16  lr: 0.000176  training_loss: 1.4216 (1.4124)  mae_loss: 0.0282 (0.0281)  classification_loss: 1.3913 (1.3789)  loss_mask: 0.0044 (0.0054)  time: 0.1966  data: 0.0002  max mem: 5511
[22:40:02.978791] Epoch: [39]  [720/781]  eta: 0:00:12  lr: 0.000176  training_loss: 1.4029 (1.4126)  mae_loss: 0.0280 (0.0281)  classification_loss: 1.3593 (1.3788)  loss_mask: 0.0092 (0.0057)  time: 0.1959  data: 0.0003  max mem: 5511
[22:40:06.918120] Epoch: [39]  [740/781]  eta: 0:00:08  lr: 0.000176  training_loss: 1.3620 (1.4117)  mae_loss: 0.0267 (0.0280)  classification_loss: 1.3302 (1.3777)  loss_mask: 0.0114 (0.0059)  time: 0.1969  data: 0.0002  max mem: 5511
[22:40:10.866613] Epoch: [39]  [760/781]  eta: 0:00:04  lr: 0.000176  training_loss: 1.4256 (1.4125)  mae_loss: 0.0285 (0.0281)  classification_loss: 1.3909 (1.3785)  loss_mask: 0.0041 (0.0059)  time: 0.1973  data: 0.0002  max mem: 5511
[22:40:14.814416] Epoch: [39]  [780/781]  eta: 0:00:00  lr: 0.000176  training_loss: 1.4332 (1.4132)  mae_loss: 0.0279 (0.0281)  classification_loss: 1.3961 (1.3793)  loss_mask: 0.0044 (0.0059)  time: 0.1973  data: 0.0002  max mem: 5511
[22:40:14.963424] Epoch: [39] Total time: 0:02:34 (0.1981 s / it)
[22:40:14.963932] Averaged stats: lr: 0.000176  training_loss: 1.4332 (1.4132)  mae_loss: 0.0279 (0.0281)  classification_loss: 1.3961 (1.3793)  loss_mask: 0.0044 (0.0059)
[22:40:15.598672] Test:  [  0/157]  eta: 0:01:38  testing_loss: 0.6345 (0.6345)  acc1: 79.6875 (79.6875)  acc5: 96.8750 (96.8750)  time: 0.6301  data: 0.5997  max mem: 5511
[22:40:15.884723] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.6950 (0.6900)  acc1: 79.6875 (78.4091)  acc5: 98.4375 (98.7216)  time: 0.0831  data: 0.0547  max mem: 5511
[22:40:16.172910] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.6916 (0.6671)  acc1: 76.5625 (78.8690)  acc5: 98.4375 (98.9583)  time: 0.0285  data: 0.0002  max mem: 5511
[22:40:16.457969] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6653 (0.6807)  acc1: 76.5625 (78.5282)  acc5: 98.4375 (98.5887)  time: 0.0285  data: 0.0002  max mem: 5511
[22:40:16.742790] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6653 (0.6782)  acc1: 78.1250 (78.6585)  acc5: 98.4375 (98.4756)  time: 0.0284  data: 0.0002  max mem: 5511
[22:40:17.028924] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6487 (0.6705)  acc1: 81.2500 (79.0441)  acc5: 98.4375 (98.5294)  time: 0.0284  data: 0.0002  max mem: 5511
[22:40:17.311722] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6349 (0.6654)  acc1: 81.2500 (79.0471)  acc5: 98.4375 (98.5656)  time: 0.0283  data: 0.0002  max mem: 5511
[22:40:17.595670] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6206 (0.6574)  acc1: 79.6875 (79.3574)  acc5: 98.4375 (98.6136)  time: 0.0282  data: 0.0002  max mem: 5511
[22:40:17.880102] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6274 (0.6660)  acc1: 79.6875 (78.9545)  acc5: 98.4375 (98.6497)  time: 0.0283  data: 0.0002  max mem: 5511
[22:40:18.163997] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6844 (0.6637)  acc1: 76.5625 (79.1209)  acc5: 98.4375 (98.6779)  time: 0.0283  data: 0.0002  max mem: 5511
[22:40:18.447333] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.6844 (0.6674)  acc1: 78.1250 (78.8366)  acc5: 98.4375 (98.7160)  time: 0.0282  data: 0.0002  max mem: 5511
[22:40:18.730828] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6597 (0.6656)  acc1: 78.1250 (78.8570)  acc5: 98.4375 (98.7190)  time: 0.0282  data: 0.0002  max mem: 5511
[22:40:19.015127] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6211 (0.6636)  acc1: 78.1250 (78.7707)  acc5: 98.4375 (98.7216)  time: 0.0283  data: 0.0002  max mem: 5511
[22:40:19.305172] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6211 (0.6627)  acc1: 76.5625 (78.7452)  acc5: 98.4375 (98.7595)  time: 0.0286  data: 0.0002  max mem: 5511
[22:40:19.586199] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6576 (0.6619)  acc1: 78.1250 (78.8453)  acc5: 98.4375 (98.7589)  time: 0.0284  data: 0.0002  max mem: 5511
[22:40:19.866372] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6603 (0.6609)  acc1: 78.1250 (78.8907)  acc5: 98.4375 (98.7479)  time: 0.0279  data: 0.0001  max mem: 5511
[22:40:20.017745] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6733 (0.6618)  acc1: 78.1250 (78.7900)  acc5: 98.4375 (98.7600)  time: 0.0270  data: 0.0001  max mem: 5511
[22:40:20.179586] Test: Total time: 0:00:05 (0.0332 s / it)
[22:40:20.180093] * Acc@1 78.790 Acc@5 98.760 loss 0.662
[22:40:20.180389] Accuracy of the network on the 10000 test images: 78.8%
[22:40:20.180569] Max accuracy: 78.79%
[22:40:20.503920] log_dir: ./output_dir
[22:40:21.418977] Epoch: [40]  [  0/781]  eta: 0:11:53  lr: 0.000176  training_loss: 1.2954 (1.2954)  mae_loss: 0.0229 (0.0229)  classification_loss: 1.2704 (1.2704)  loss_mask: 0.0021 (0.0021)  time: 0.9131  data: 0.7045  max mem: 5511
[22:40:25.402439] Epoch: [40]  [ 20/781]  eta: 0:02:57  lr: 0.000175  training_loss: 1.3473 (1.3390)  mae_loss: 0.0307 (0.0296)  classification_loss: 1.3142 (1.3057)  loss_mask: 0.0030 (0.0037)  time: 0.1991  data: 0.0002  max mem: 5511
[22:40:29.326127] Epoch: [40]  [ 40/781]  eta: 0:02:39  lr: 0.000175  training_loss: 1.4380 (1.3829)  mae_loss: 0.0273 (0.0290)  classification_loss: 1.3950 (1.3501)  loss_mask: 0.0029 (0.0039)  time: 0.1961  data: 0.0002  max mem: 5511
[22:40:33.264880] Epoch: [40]  [ 60/781]  eta: 0:02:30  lr: 0.000175  training_loss: 1.3745 (1.3918)  mae_loss: 0.0275 (0.0285)  classification_loss: 1.3404 (1.3595)  loss_mask: 0.0029 (0.0038)  time: 0.1968  data: 0.0002  max mem: 5511
[22:40:37.185838] Epoch: [40]  [ 80/781]  eta: 0:02:24  lr: 0.000175  training_loss: 1.3629 (1.3847)  mae_loss: 0.0255 (0.0282)  classification_loss: 1.3343 (1.3526)  loss_mask: 0.0034 (0.0040)  time: 0.1960  data: 0.0002  max mem: 5511
[22:40:41.114793] Epoch: [40]  [100/781]  eta: 0:02:18  lr: 0.000175  training_loss: 1.4499 (1.3962)  mae_loss: 0.0279 (0.0282)  classification_loss: 1.4193 (1.3641)  loss_mask: 0.0026 (0.0038)  time: 0.1963  data: 0.0002  max mem: 5511
[22:40:45.069901] Epoch: [40]  [120/781]  eta: 0:02:14  lr: 0.000175  training_loss: 1.3385 (1.3931)  mae_loss: 0.0275 (0.0281)  classification_loss: 1.3129 (1.3612)  loss_mask: 0.0028 (0.0038)  time: 0.1977  data: 0.0002  max mem: 5511
[22:40:48.995394] Epoch: [40]  [140/781]  eta: 0:02:09  lr: 0.000175  training_loss: 1.3764 (1.3913)  mae_loss: 0.0276 (0.0282)  classification_loss: 1.3416 (1.3593)  loss_mask: 0.0029 (0.0037)  time: 0.1962  data: 0.0002  max mem: 5511
[22:40:52.937963] Epoch: [40]  [160/781]  eta: 0:02:05  lr: 0.000175  training_loss: 1.3717 (1.3919)  mae_loss: 0.0277 (0.0282)  classification_loss: 1.3416 (1.3600)  loss_mask: 0.0028 (0.0037)  time: 0.1970  data: 0.0002  max mem: 5511
[22:40:56.861993] Epoch: [40]  [180/781]  eta: 0:02:00  lr: 0.000175  training_loss: 1.3921 (1.3918)  mae_loss: 0.0269 (0.0282)  classification_loss: 1.3626 (1.3600)  loss_mask: 0.0026 (0.0036)  time: 0.1961  data: 0.0002  max mem: 5511
[22:41:00.794185] Epoch: [40]  [200/781]  eta: 0:01:56  lr: 0.000175  training_loss: 1.3715 (1.3913)  mae_loss: 0.0275 (0.0281)  classification_loss: 1.3383 (1.3597)  loss_mask: 0.0024 (0.0035)  time: 0.1965  data: 0.0002  max mem: 5511
[22:41:04.727492] Epoch: [40]  [220/781]  eta: 0:01:52  lr: 0.000174  training_loss: 1.3878 (1.3901)  mae_loss: 0.0295 (0.0282)  classification_loss: 1.3538 (1.3584)  loss_mask: 0.0026 (0.0034)  time: 0.1966  data: 0.0002  max mem: 5511
[22:41:08.663287] Epoch: [40]  [240/781]  eta: 0:01:48  lr: 0.000174  training_loss: 1.3850 (1.3916)  mae_loss: 0.0270 (0.0281)  classification_loss: 1.3555 (1.3601)  loss_mask: 0.0025 (0.0034)  time: 0.1967  data: 0.0002  max mem: 5511
[22:41:12.595032] Epoch: [40]  [260/781]  eta: 0:01:43  lr: 0.000174  training_loss: 1.3862 (1.3949)  mae_loss: 0.0287 (0.0282)  classification_loss: 1.3595 (1.3634)  loss_mask: 0.0025 (0.0034)  time: 0.1965  data: 0.0002  max mem: 5511
[22:41:16.561245] Epoch: [40]  [280/781]  eta: 0:01:39  lr: 0.000174  training_loss: 1.3669 (1.3949)  mae_loss: 0.0290 (0.0283)  classification_loss: 1.3326 (1.3631)  loss_mask: 0.0024 (0.0034)  time: 0.1982  data: 0.0002  max mem: 5511
[22:41:20.468469] Epoch: [40]  [300/781]  eta: 0:01:35  lr: 0.000174  training_loss: 1.4196 (1.3985)  mae_loss: 0.0268 (0.0283)  classification_loss: 1.3821 (1.3667)  loss_mask: 0.0039 (0.0035)  time: 0.1953  data: 0.0002  max mem: 5511
[22:41:24.390644] Epoch: [40]  [320/781]  eta: 0:01:31  lr: 0.000174  training_loss: 1.3921 (1.3988)  mae_loss: 0.0301 (0.0284)  classification_loss: 1.3571 (1.3667)  loss_mask: 0.0075 (0.0037)  time: 0.1960  data: 0.0002  max mem: 5511
[22:41:28.322112] Epoch: [40]  [340/781]  eta: 0:01:27  lr: 0.000174  training_loss: 1.3776 (1.3976)  mae_loss: 0.0295 (0.0284)  classification_loss: 1.3522 (1.3653)  loss_mask: 0.0039 (0.0039)  time: 0.1965  data: 0.0002  max mem: 5511
[22:41:32.266913] Epoch: [40]  [360/781]  eta: 0:01:23  lr: 0.000174  training_loss: 1.4018 (1.3974)  mae_loss: 0.0278 (0.0284)  classification_loss: 1.3704 (1.3650)  loss_mask: 0.0031 (0.0039)  time: 0.1971  data: 0.0002  max mem: 5511
[22:41:36.212012] Epoch: [40]  [380/781]  eta: 0:01:19  lr: 0.000174  training_loss: 1.3652 (1.3985)  mae_loss: 0.0277 (0.0284)  classification_loss: 1.3354 (1.3661)  loss_mask: 0.0032 (0.0040)  time: 0.1971  data: 0.0002  max mem: 5511
[22:41:40.152240] Epoch: [40]  [400/781]  eta: 0:01:15  lr: 0.000174  training_loss: 1.4133 (1.3993)  mae_loss: 0.0263 (0.0283)  classification_loss: 1.3843 (1.3669)  loss_mask: 0.0047 (0.0040)  time: 0.1969  data: 0.0004  max mem: 5511
[22:41:44.142019] Epoch: [40]  [420/781]  eta: 0:01:11  lr: 0.000173  training_loss: 1.3658 (1.3985)  mae_loss: 0.0274 (0.0283)  classification_loss: 1.3328 (1.3661)  loss_mask: 0.0033 (0.0040)  time: 0.1994  data: 0.0003  max mem: 5511
[22:41:48.092477] Epoch: [40]  [440/781]  eta: 0:01:07  lr: 0.000173  training_loss: 1.3957 (1.3987)  mae_loss: 0.0273 (0.0283)  classification_loss: 1.3639 (1.3664)  loss_mask: 0.0024 (0.0040)  time: 0.1974  data: 0.0003  max mem: 5511
[22:41:52.015617] Epoch: [40]  [460/781]  eta: 0:01:03  lr: 0.000173  training_loss: 1.3595 (1.3974)  mae_loss: 0.0274 (0.0283)  classification_loss: 1.3301 (1.3651)  loss_mask: 0.0026 (0.0040)  time: 0.1960  data: 0.0001  max mem: 5511
[22:41:55.939568] Epoch: [40]  [480/781]  eta: 0:00:59  lr: 0.000173  training_loss: 1.3942 (1.3971)  mae_loss: 0.0265 (0.0283)  classification_loss: 1.3628 (1.3647)  loss_mask: 0.0055 (0.0041)  time: 0.1961  data: 0.0003  max mem: 5511
[22:41:59.863261] Epoch: [40]  [500/781]  eta: 0:00:55  lr: 0.000173  training_loss: 1.3604 (1.3971)  mae_loss: 0.0287 (0.0283)  classification_loss: 1.3336 (1.3647)  loss_mask: 0.0024 (0.0041)  time: 0.1961  data: 0.0002  max mem: 5511
[22:42:03.783603] Epoch: [40]  [520/781]  eta: 0:00:51  lr: 0.000173  training_loss: 1.4109 (1.3976)  mae_loss: 0.0292 (0.0283)  classification_loss: 1.3801 (1.3651)  loss_mask: 0.0038 (0.0041)  time: 0.1959  data: 0.0002  max mem: 5511
[22:42:07.703427] Epoch: [40]  [540/781]  eta: 0:00:47  lr: 0.000173  training_loss: 1.4169 (1.3975)  mae_loss: 0.0278 (0.0283)  classification_loss: 1.3821 (1.3651)  loss_mask: 0.0039 (0.0041)  time: 0.1959  data: 0.0003  max mem: 5511
[22:42:11.638375] Epoch: [40]  [560/781]  eta: 0:00:43  lr: 0.000173  training_loss: 1.3577 (1.3966)  mae_loss: 0.0286 (0.0283)  classification_loss: 1.3257 (1.3641)  loss_mask: 0.0051 (0.0042)  time: 0.1967  data: 0.0002  max mem: 5511
[22:42:15.579187] Epoch: [40]  [580/781]  eta: 0:00:39  lr: 0.000173  training_loss: 1.3659 (1.3958)  mae_loss: 0.0270 (0.0283)  classification_loss: 1.3360 (1.3632)  loss_mask: 0.0049 (0.0043)  time: 0.1970  data: 0.0002  max mem: 5511
[22:42:19.529594] Epoch: [40]  [600/781]  eta: 0:00:35  lr: 0.000173  training_loss: 1.3624 (1.3951)  mae_loss: 0.0272 (0.0283)  classification_loss: 1.3347 (1.3626)  loss_mask: 0.0028 (0.0043)  time: 0.1974  data: 0.0002  max mem: 5511
[22:42:23.470280] Epoch: [40]  [620/781]  eta: 0:00:31  lr: 0.000173  training_loss: 1.4161 (1.3954)  mae_loss: 0.0282 (0.0283)  classification_loss: 1.3845 (1.3630)  loss_mask: 0.0021 (0.0042)  time: 0.1970  data: 0.0002  max mem: 5511
[22:42:27.383372] Epoch: [40]  [640/781]  eta: 0:00:27  lr: 0.000172  training_loss: 1.3802 (1.3954)  mae_loss: 0.0277 (0.0283)  classification_loss: 1.3505 (1.3630)  loss_mask: 0.0025 (0.0042)  time: 0.1956  data: 0.0002  max mem: 5511
[22:42:31.302162] Epoch: [40]  [660/781]  eta: 0:00:23  lr: 0.000172  training_loss: 1.4005 (1.3956)  mae_loss: 0.0277 (0.0282)  classification_loss: 1.3683 (1.3632)  loss_mask: 0.0029 (0.0041)  time: 0.1959  data: 0.0002  max mem: 5511
[22:42:35.248402] Epoch: [40]  [680/781]  eta: 0:00:19  lr: 0.000172  training_loss: 1.4506 (1.3968)  mae_loss: 0.0268 (0.0282)  classification_loss: 1.4223 (1.3644)  loss_mask: 0.0037 (0.0042)  time: 0.1972  data: 0.0002  max mem: 5511
[22:42:39.195770] Epoch: [40]  [700/781]  eta: 0:00:16  lr: 0.000172  training_loss: 1.4315 (1.3980)  mae_loss: 0.0270 (0.0282)  classification_loss: 1.3871 (1.3656)  loss_mask: 0.0067 (0.0043)  time: 0.1973  data: 0.0002  max mem: 5511
[22:42:43.126156] Epoch: [40]  [720/781]  eta: 0:00:12  lr: 0.000172  training_loss: 1.3341 (1.3974)  mae_loss: 0.0279 (0.0282)  classification_loss: 1.2975 (1.3647)  loss_mask: 0.0121 (0.0045)  time: 0.1964  data: 0.0003  max mem: 5511
[22:42:47.065903] Epoch: [40]  [740/781]  eta: 0:00:08  lr: 0.000172  training_loss: 1.3438 (1.3968)  mae_loss: 0.0260 (0.0282)  classification_loss: 1.3121 (1.3640)  loss_mask: 0.0044 (0.0046)  time: 0.1969  data: 0.0002  max mem: 5511
[22:42:51.005785] Epoch: [40]  [760/781]  eta: 0:00:04  lr: 0.000172  training_loss: 1.4226 (1.3976)  mae_loss: 0.0276 (0.0282)  classification_loss: 1.3831 (1.3648)  loss_mask: 0.0045 (0.0046)  time: 0.1969  data: 0.0003  max mem: 5511
[22:42:54.981504] Epoch: [40]  [780/781]  eta: 0:00:00  lr: 0.000172  training_loss: 1.4180 (1.3982)  mae_loss: 0.0279 (0.0281)  classification_loss: 1.3855 (1.3655)  loss_mask: 0.0027 (0.0046)  time: 0.1987  data: 0.0002  max mem: 5511
[22:42:55.141967] Epoch: [40] Total time: 0:02:34 (0.1980 s / it)
[22:42:55.143526] Averaged stats: lr: 0.000172  training_loss: 1.4180 (1.3982)  mae_loss: 0.0279 (0.0281)  classification_loss: 1.3855 (1.3655)  loss_mask: 0.0027 (0.0046)
[22:42:56.432687] Test:  [  0/157]  eta: 0:01:34  testing_loss: 0.6463 (0.6463)  acc1: 75.0000 (75.0000)  acc5: 98.4375 (98.4375)  time: 0.6044  data: 0.5735  max mem: 5511
[22:42:56.732019] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.7262 (0.7242)  acc1: 76.5625 (76.4205)  acc5: 98.4375 (98.7216)  time: 0.0819  data: 0.0524  max mem: 5511
[22:42:57.018263] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.6847 (0.6830)  acc1: 78.1250 (78.4226)  acc5: 98.4375 (98.8095)  time: 0.0291  data: 0.0002  max mem: 5511
[22:42:57.308586] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6847 (0.6973)  acc1: 79.6875 (78.2762)  acc5: 98.4375 (98.5383)  time: 0.0287  data: 0.0002  max mem: 5511
[22:42:57.598738] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6953 (0.6891)  acc1: 79.6875 (78.4299)  acc5: 98.4375 (98.4756)  time: 0.0288  data: 0.0002  max mem: 5511
[22:42:57.887404] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6585 (0.6832)  acc1: 78.1250 (78.7377)  acc5: 98.4375 (98.4988)  time: 0.0287  data: 0.0002  max mem: 5511
[22:42:58.179465] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6804 (0.6806)  acc1: 78.1250 (78.3811)  acc5: 100.0000 (98.6424)  time: 0.0289  data: 0.0002  max mem: 5511
[22:42:58.468778] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6318 (0.6715)  acc1: 78.1250 (78.6532)  acc5: 100.0000 (98.7456)  time: 0.0289  data: 0.0004  max mem: 5511
[22:42:58.753056] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6437 (0.6749)  acc1: 78.1250 (78.4722)  acc5: 98.4375 (98.7461)  time: 0.0285  data: 0.0004  max mem: 5511
[22:42:59.039550] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6745 (0.6749)  acc1: 78.1250 (78.5199)  acc5: 98.4375 (98.6779)  time: 0.0284  data: 0.0002  max mem: 5511
[22:42:59.328440] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.7080 (0.6804)  acc1: 76.5625 (78.0476)  acc5: 98.4375 (98.6850)  time: 0.0286  data: 0.0002  max mem: 5511
[22:42:59.618876] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.7177 (0.6799)  acc1: 75.0000 (78.0124)  acc5: 98.4375 (98.7050)  time: 0.0288  data: 0.0002  max mem: 5511
[22:42:59.905371] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6515 (0.6764)  acc1: 78.1250 (78.1508)  acc5: 98.4375 (98.7087)  time: 0.0287  data: 0.0002  max mem: 5511
[22:43:00.203849] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6572 (0.6756)  acc1: 79.6875 (78.2801)  acc5: 98.4375 (98.7118)  time: 0.0291  data: 0.0002  max mem: 5511
[22:43:00.486919] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6723 (0.6741)  acc1: 79.6875 (78.3577)  acc5: 98.4375 (98.7256)  time: 0.0289  data: 0.0002  max mem: 5511
[22:43:00.770728] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6800 (0.6730)  acc1: 78.1250 (78.4044)  acc5: 100.0000 (98.7376)  time: 0.0282  data: 0.0002  max mem: 5511
[22:43:00.922248] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6834 (0.6736)  acc1: 78.1250 (78.3700)  acc5: 100.0000 (98.7400)  time: 0.0273  data: 0.0001  max mem: 5511
[22:43:01.091018] Test: Total time: 0:00:05 (0.0335 s / it)
[22:43:01.091752] * Acc@1 78.370 Acc@5 98.740 loss 0.674
[22:43:01.092053] Accuracy of the network on the 10000 test images: 78.4%
[22:43:01.092274] Max accuracy: 78.79%
[22:43:01.406967] log_dir: ./output_dir
[22:43:02.281776] Epoch: [41]  [  0/781]  eta: 0:11:21  lr: 0.000172  training_loss: 1.3049 (1.3049)  mae_loss: 0.0239 (0.0239)  classification_loss: 1.2762 (1.2762)  loss_mask: 0.0049 (0.0049)  time: 0.8729  data: 0.6651  max mem: 5511
[22:43:06.208362] Epoch: [41]  [ 20/781]  eta: 0:02:53  lr: 0.000172  training_loss: 1.4037 (1.3798)  mae_loss: 0.0280 (0.0277)  classification_loss: 1.3732 (1.3469)  loss_mask: 0.0039 (0.0052)  time: 0.1962  data: 0.0002  max mem: 5511
[22:43:10.139605] Epoch: [41]  [ 40/781]  eta: 0:02:37  lr: 0.000172  training_loss: 1.3699 (1.3882)  mae_loss: 0.0274 (0.0277)  classification_loss: 1.3429 (1.3559)  loss_mask: 0.0032 (0.0046)  time: 0.1965  data: 0.0003  max mem: 5511
[22:43:14.107859] Epoch: [41]  [ 60/781]  eta: 0:02:30  lr: 0.000171  training_loss: 1.4222 (1.4037)  mae_loss: 0.0268 (0.0275)  classification_loss: 1.3893 (1.3715)  loss_mask: 0.0048 (0.0047)  time: 0.1983  data: 0.0003  max mem: 5511
[22:43:18.030432] Epoch: [41]  [ 80/781]  eta: 0:02:23  lr: 0.000171  training_loss: 1.3874 (1.4022)  mae_loss: 0.0279 (0.0279)  classification_loss: 1.3589 (1.3697)  loss_mask: 0.0029 (0.0046)  time: 0.1960  data: 0.0002  max mem: 5511
[22:43:21.958491] Epoch: [41]  [100/781]  eta: 0:02:18  lr: 0.000171  training_loss: 1.4164 (1.4059)  mae_loss: 0.0277 (0.0281)  classification_loss: 1.3864 (1.3733)  loss_mask: 0.0034 (0.0045)  time: 0.1963  data: 0.0003  max mem: 5511
[22:43:25.873200] Epoch: [41]  [120/781]  eta: 0:02:13  lr: 0.000171  training_loss: 1.3418 (1.4025)  mae_loss: 0.0272 (0.0281)  classification_loss: 1.3118 (1.3701)  loss_mask: 0.0031 (0.0043)  time: 0.1956  data: 0.0003  max mem: 5511
[22:43:29.791940] Epoch: [41]  [140/781]  eta: 0:02:08  lr: 0.000171  training_loss: 1.3314 (1.3984)  mae_loss: 0.0290 (0.0282)  classification_loss: 1.2952 (1.3648)  loss_mask: 0.0075 (0.0053)  time: 0.1959  data: 0.0003  max mem: 5511
[22:43:33.730191] Epoch: [41]  [160/781]  eta: 0:02:04  lr: 0.000171  training_loss: 1.4210 (1.3996)  mae_loss: 0.0273 (0.0281)  classification_loss: 1.3436 (1.3647)  loss_mask: 0.0095 (0.0069)  time: 0.1968  data: 0.0002  max mem: 5511
[22:43:37.657629] Epoch: [41]  [180/781]  eta: 0:02:00  lr: 0.000171  training_loss: 1.3616 (1.3944)  mae_loss: 0.0268 (0.0280)  classification_loss: 1.3255 (1.3589)  loss_mask: 0.0094 (0.0076)  time: 0.1963  data: 0.0003  max mem: 5511
[22:43:41.627094] Epoch: [41]  [200/781]  eta: 0:01:56  lr: 0.000171  training_loss: 1.3869 (1.3948)  mae_loss: 0.0275 (0.0280)  classification_loss: 1.3513 (1.3592)  loss_mask: 0.0070 (0.0076)  time: 0.1984  data: 0.0002  max mem: 5511
[22:43:45.575286] Epoch: [41]  [220/781]  eta: 0:01:52  lr: 0.000171  training_loss: 1.4254 (1.3952)  mae_loss: 0.0284 (0.0280)  classification_loss: 1.3941 (1.3599)  loss_mask: 0.0034 (0.0073)  time: 0.1973  data: 0.0002  max mem: 5511
[22:43:49.510045] Epoch: [41]  [240/781]  eta: 0:01:47  lr: 0.000171  training_loss: 1.3703 (1.3946)  mae_loss: 0.0260 (0.0279)  classification_loss: 1.3422 (1.3597)  loss_mask: 0.0029 (0.0069)  time: 0.1966  data: 0.0002  max mem: 5511
[22:43:53.436075] Epoch: [41]  [260/781]  eta: 0:01:43  lr: 0.000170  training_loss: 1.3895 (1.3943)  mae_loss: 0.0277 (0.0279)  classification_loss: 1.3616 (1.3597)  loss_mask: 0.0030 (0.0067)  time: 0.1962  data: 0.0003  max mem: 5511
[22:43:57.363388] Epoch: [41]  [280/781]  eta: 0:01:39  lr: 0.000170  training_loss: 1.3692 (1.3927)  mae_loss: 0.0277 (0.0279)  classification_loss: 1.3351 (1.3584)  loss_mask: 0.0020 (0.0064)  time: 0.1963  data: 0.0003  max mem: 5511
[22:44:01.288207] Epoch: [41]  [300/781]  eta: 0:01:35  lr: 0.000170  training_loss: 1.3985 (1.3934)  mae_loss: 0.0260 (0.0278)  classification_loss: 1.3509 (1.3592)  loss_mask: 0.0040 (0.0064)  time: 0.1962  data: 0.0002  max mem: 5511
[22:44:05.238691] Epoch: [41]  [320/781]  eta: 0:01:31  lr: 0.000170  training_loss: 1.3741 (1.3941)  mae_loss: 0.0266 (0.0278)  classification_loss: 1.3421 (1.3600)  loss_mask: 0.0039 (0.0062)  time: 0.1974  data: 0.0004  max mem: 5511
[22:44:09.214514] Epoch: [41]  [340/781]  eta: 0:01:27  lr: 0.000170  training_loss: 1.4247 (1.3953)  mae_loss: 0.0276 (0.0278)  classification_loss: 1.3898 (1.3611)  loss_mask: 0.0072 (0.0064)  time: 0.1987  data: 0.0003  max mem: 5511
[22:44:13.147780] Epoch: [41]  [360/781]  eta: 0:01:23  lr: 0.000170  training_loss: 1.3993 (1.3966)  mae_loss: 0.0274 (0.0278)  classification_loss: 1.3720 (1.3626)  loss_mask: 0.0035 (0.0063)  time: 0.1966  data: 0.0002  max mem: 5511
[22:44:17.098454] Epoch: [41]  [380/781]  eta: 0:01:19  lr: 0.000170  training_loss: 1.4331 (1.3983)  mae_loss: 0.0274 (0.0278)  classification_loss: 1.4024 (1.3643)  loss_mask: 0.0028 (0.0061)  time: 0.1974  data: 0.0002  max mem: 5511
[22:44:21.048143] Epoch: [41]  [400/781]  eta: 0:01:15  lr: 0.000170  training_loss: 1.3566 (1.3982)  mae_loss: 0.0263 (0.0278)  classification_loss: 1.3292 (1.3645)  loss_mask: 0.0020 (0.0059)  time: 0.1974  data: 0.0003  max mem: 5511
[22:44:24.965535] Epoch: [41]  [420/781]  eta: 0:01:11  lr: 0.000170  training_loss: 1.3547 (1.3970)  mae_loss: 0.0278 (0.0278)  classification_loss: 1.3241 (1.3634)  loss_mask: 0.0020 (0.0058)  time: 0.1958  data: 0.0002  max mem: 5511
[22:44:28.876243] Epoch: [41]  [440/781]  eta: 0:01:07  lr: 0.000170  training_loss: 1.3713 (1.3960)  mae_loss: 0.0274 (0.0278)  classification_loss: 1.3405 (1.3626)  loss_mask: 0.0021 (0.0056)  time: 0.1955  data: 0.0002  max mem: 5511
[22:44:32.796772] Epoch: [41]  [460/781]  eta: 0:01:03  lr: 0.000169  training_loss: 1.3646 (1.3942)  mae_loss: 0.0279 (0.0278)  classification_loss: 1.3316 (1.3609)  loss_mask: 0.0017 (0.0054)  time: 0.1959  data: 0.0002  max mem: 5511
[22:44:36.715556] Epoch: [41]  [480/781]  eta: 0:00:59  lr: 0.000169  training_loss: 1.3859 (1.3940)  mae_loss: 0.0254 (0.0278)  classification_loss: 1.3593 (1.3608)  loss_mask: 0.0033 (0.0054)  time: 0.1959  data: 0.0001  max mem: 5511
[22:44:40.645012] Epoch: [41]  [500/781]  eta: 0:00:55  lr: 0.000169  training_loss: 1.3534 (1.3940)  mae_loss: 0.0280 (0.0278)  classification_loss: 1.3199 (1.3609)  loss_mask: 0.0031 (0.0053)  time: 0.1964  data: 0.0003  max mem: 5511
[22:44:44.589984] Epoch: [41]  [520/781]  eta: 0:00:51  lr: 0.000169  training_loss: 1.3932 (1.3938)  mae_loss: 0.0287 (0.0278)  classification_loss: 1.3544 (1.3606)  loss_mask: 0.0071 (0.0054)  time: 0.1971  data: 0.0002  max mem: 5511
[22:44:48.527805] Epoch: [41]  [540/781]  eta: 0:00:47  lr: 0.000169  training_loss: 1.4102 (1.3944)  mae_loss: 0.0279 (0.0278)  classification_loss: 1.3611 (1.3612)  loss_mask: 0.0043 (0.0054)  time: 0.1968  data: 0.0002  max mem: 5511
[22:44:52.455926] Epoch: [41]  [560/781]  eta: 0:00:43  lr: 0.000169  training_loss: 1.4403 (1.3955)  mae_loss: 0.0294 (0.0279)  classification_loss: 1.4068 (1.3623)  loss_mask: 0.0027 (0.0053)  time: 0.1963  data: 0.0002  max mem: 5511
[22:44:56.396402] Epoch: [41]  [580/781]  eta: 0:00:39  lr: 0.000169  training_loss: 1.3893 (1.3954)  mae_loss: 0.0283 (0.0279)  classification_loss: 1.3563 (1.3622)  loss_mask: 0.0021 (0.0052)  time: 0.1969  data: 0.0002  max mem: 5511
[22:45:00.325672] Epoch: [41]  [600/781]  eta: 0:00:35  lr: 0.000169  training_loss: 1.3876 (1.3952)  mae_loss: 0.0272 (0.0279)  classification_loss: 1.3573 (1.3620)  loss_mask: 0.0023 (0.0052)  time: 0.1964  data: 0.0002  max mem: 5511
[22:45:04.276869] Epoch: [41]  [620/781]  eta: 0:00:31  lr: 0.000169  training_loss: 1.3482 (1.3949)  mae_loss: 0.0284 (0.0279)  classification_loss: 1.3196 (1.3619)  loss_mask: 0.0026 (0.0051)  time: 0.1975  data: 0.0002  max mem: 5511
[22:45:08.207665] Epoch: [41]  [640/781]  eta: 0:00:27  lr: 0.000169  training_loss: 1.3787 (1.3943)  mae_loss: 0.0263 (0.0279)  classification_loss: 1.3464 (1.3613)  loss_mask: 0.0034 (0.0051)  time: 0.1965  data: 0.0002  max mem: 5511
[22:45:12.148671] Epoch: [41]  [660/781]  eta: 0:00:23  lr: 0.000168  training_loss: 1.4176 (1.3948)  mae_loss: 0.0274 (0.0279)  classification_loss: 1.3821 (1.3619)  loss_mask: 0.0020 (0.0050)  time: 0.1970  data: 0.0005  max mem: 5511
[22:45:16.073177] Epoch: [41]  [680/781]  eta: 0:00:19  lr: 0.000168  training_loss: 1.4087 (1.3956)  mae_loss: 0.0270 (0.0279)  classification_loss: 1.3834 (1.3628)  loss_mask: 0.0022 (0.0049)  time: 0.1961  data: 0.0006  max mem: 5511
[22:45:20.014734] Epoch: [41]  [700/781]  eta: 0:00:16  lr: 0.000168  training_loss: 1.3863 (1.3955)  mae_loss: 0.0278 (0.0279)  classification_loss: 1.3527 (1.3627)  loss_mask: 0.0025 (0.0049)  time: 0.1970  data: 0.0002  max mem: 5511
[22:45:23.936950] Epoch: [41]  [720/781]  eta: 0:00:12  lr: 0.000168  training_loss: 1.3958 (1.3955)  mae_loss: 0.0291 (0.0280)  classification_loss: 1.3666 (1.3628)  loss_mask: 0.0032 (0.0048)  time: 0.1960  data: 0.0002  max mem: 5511
[22:45:27.855069] Epoch: [41]  [740/781]  eta: 0:00:08  lr: 0.000168  training_loss: 1.3643 (1.3955)  mae_loss: 0.0289 (0.0280)  classification_loss: 1.3332 (1.3627)  loss_mask: 0.0038 (0.0048)  time: 0.1958  data: 0.0002  max mem: 5511
[22:45:31.758083] Epoch: [41]  [760/781]  eta: 0:00:04  lr: 0.000168  training_loss: 1.4040 (1.3958)  mae_loss: 0.0268 (0.0279)  classification_loss: 1.3492 (1.3629)  loss_mask: 0.0060 (0.0049)  time: 0.1951  data: 0.0002  max mem: 5511
[22:45:35.656915] Epoch: [41]  [780/781]  eta: 0:00:00  lr: 0.000168  training_loss: 1.4279 (1.3968)  mae_loss: 0.0288 (0.0280)  classification_loss: 1.3660 (1.3632)  loss_mask: 0.0161 (0.0056)  time: 0.1949  data: 0.0002  max mem: 5511
[22:45:35.813761] Epoch: [41] Total time: 0:02:34 (0.1977 s / it)
[22:45:35.814303] Averaged stats: lr: 0.000168  training_loss: 1.4279 (1.3968)  mae_loss: 0.0288 (0.0280)  classification_loss: 1.3660 (1.3632)  loss_mask: 0.0161 (0.0056)
[22:45:36.447260] Test:  [  0/157]  eta: 0:01:38  testing_loss: 0.6385 (0.6385)  acc1: 78.1250 (78.1250)  acc5: 100.0000 (100.0000)  time: 0.6290  data: 0.5978  max mem: 5511
[22:45:36.740456] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.7299 (0.7274)  acc1: 75.0000 (75.2841)  acc5: 98.4375 (98.7216)  time: 0.0832  data: 0.0545  max mem: 5511
[22:45:37.024002] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.6627 (0.6765)  acc1: 78.1250 (77.6042)  acc5: 98.4375 (98.7351)  time: 0.0284  data: 0.0002  max mem: 5511
[22:45:37.307890] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6810 (0.6944)  acc1: 78.1250 (77.3690)  acc5: 98.4375 (98.3367)  time: 0.0282  data: 0.0002  max mem: 5511
[22:45:37.594533] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6827 (0.6892)  acc1: 78.1250 (77.7058)  acc5: 98.4375 (98.3994)  time: 0.0284  data: 0.0002  max mem: 5511
[22:45:37.878093] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6425 (0.6820)  acc1: 79.6875 (78.0944)  acc5: 98.4375 (98.4988)  time: 0.0284  data: 0.0002  max mem: 5511
[22:45:38.160424] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6386 (0.6760)  acc1: 79.6875 (78.3043)  acc5: 100.0000 (98.6680)  time: 0.0282  data: 0.0002  max mem: 5511
[22:45:38.447023] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6296 (0.6658)  acc1: 81.2500 (78.7632)  acc5: 100.0000 (98.7456)  time: 0.0283  data: 0.0002  max mem: 5511
[22:45:38.745452] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6431 (0.6731)  acc1: 81.2500 (78.4529)  acc5: 98.4375 (98.7076)  time: 0.0291  data: 0.0002  max mem: 5511
[22:45:39.031156] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6999 (0.6702)  acc1: 76.5625 (78.5886)  acc5: 98.4375 (98.6779)  time: 0.0290  data: 0.0002  max mem: 5511
[22:45:39.314691] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.6944 (0.6727)  acc1: 78.1250 (78.4499)  acc5: 98.4375 (98.7005)  time: 0.0283  data: 0.0002  max mem: 5511
[22:45:39.598437] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.7013 (0.6732)  acc1: 76.5625 (78.4910)  acc5: 98.4375 (98.6205)  time: 0.0282  data: 0.0002  max mem: 5511
[22:45:39.882723] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.7084 (0.6721)  acc1: 78.1250 (78.4478)  acc5: 98.4375 (98.6570)  time: 0.0283  data: 0.0002  max mem: 5511
[22:45:40.165807] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6892 (0.6725)  acc1: 76.5625 (78.2920)  acc5: 98.4375 (98.6522)  time: 0.0282  data: 0.0002  max mem: 5511
[22:45:40.448458] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6892 (0.6733)  acc1: 79.6875 (78.3245)  acc5: 98.4375 (98.6370)  time: 0.0282  data: 0.0002  max mem: 5511
[22:45:40.730942] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6772 (0.6729)  acc1: 76.5625 (78.3113)  acc5: 98.4375 (98.6238)  time: 0.0281  data: 0.0001  max mem: 5511
[22:45:40.881328] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6598 (0.6731)  acc1: 76.5625 (78.2500)  acc5: 98.4375 (98.6200)  time: 0.0271  data: 0.0001  max mem: 5511
[22:45:41.026320] Test: Total time: 0:00:05 (0.0332 s / it)
[22:45:41.027083] * Acc@1 78.250 Acc@5 98.620 loss 0.673
[22:45:41.027387] Accuracy of the network on the 10000 test images: 78.2%
[22:45:41.027592] Max accuracy: 78.79%
[22:45:41.245198] log_dir: ./output_dir
[22:45:42.100552] Epoch: [42]  [  0/781]  eta: 0:11:06  lr: 0.000168  training_loss: 1.3503 (1.3503)  mae_loss: 0.0272 (0.0272)  classification_loss: 1.3148 (1.3148)  loss_mask: 0.0082 (0.0082)  time: 0.8537  data: 0.6164  max mem: 5511
[22:45:46.026268] Epoch: [42]  [ 20/781]  eta: 0:02:53  lr: 0.000168  training_loss: 1.3390 (1.3862)  mae_loss: 0.0285 (0.0287)  classification_loss: 1.2936 (1.3491)  loss_mask: 0.0062 (0.0085)  time: 0.1962  data: 0.0002  max mem: 5511
[22:45:49.949944] Epoch: [42]  [ 40/781]  eta: 0:02:37  lr: 0.000168  training_loss: 1.4190 (1.3950)  mae_loss: 0.0279 (0.0283)  classification_loss: 1.3906 (1.3601)  loss_mask: 0.0046 (0.0067)  time: 0.1961  data: 0.0002  max mem: 5511
[22:45:53.896538] Epoch: [42]  [ 60/781]  eta: 0:02:29  lr: 0.000168  training_loss: 1.4081 (1.4075)  mae_loss: 0.0280 (0.0283)  classification_loss: 1.3807 (1.3738)  loss_mask: 0.0029 (0.0055)  time: 0.1972  data: 0.0002  max mem: 5511
[22:45:57.819664] Epoch: [42]  [ 80/781]  eta: 0:02:23  lr: 0.000167  training_loss: 1.3553 (1.3950)  mae_loss: 0.0286 (0.0284)  classification_loss: 1.3113 (1.3616)  loss_mask: 0.0029 (0.0051)  time: 0.1960  data: 0.0002  max mem: 5511
[22:46:01.775549] Epoch: [42]  [100/781]  eta: 0:02:18  lr: 0.000167  training_loss: 1.4840 (1.4127)  mae_loss: 0.0288 (0.0285)  classification_loss: 1.4511 (1.3797)  loss_mask: 0.0014 (0.0046)  time: 0.1977  data: 0.0002  max mem: 5511
[22:46:05.698930] Epoch: [42]  [120/781]  eta: 0:02:13  lr: 0.000167  training_loss: 1.3323 (1.4046)  mae_loss: 0.0290 (0.0284)  classification_loss: 1.2993 (1.3719)  loss_mask: 0.0023 (0.0043)  time: 0.1960  data: 0.0002  max mem: 5511
[22:46:09.646850] Epoch: [42]  [140/781]  eta: 0:02:09  lr: 0.000167  training_loss: 1.3845 (1.4007)  mae_loss: 0.0270 (0.0283)  classification_loss: 1.3471 (1.3678)  loss_mask: 0.0044 (0.0046)  time: 0.1973  data: 0.0002  max mem: 5511
[22:46:13.580617] Epoch: [42]  [160/781]  eta: 0:02:04  lr: 0.000167  training_loss: 1.3623 (1.3963)  mae_loss: 0.0264 (0.0281)  classification_loss: 1.3281 (1.3637)  loss_mask: 0.0025 (0.0044)  time: 0.1966  data: 0.0002  max mem: 5511
[22:46:17.508576] Epoch: [42]  [180/781]  eta: 0:02:00  lr: 0.000167  training_loss: 1.3651 (1.3918)  mae_loss: 0.0270 (0.0282)  classification_loss: 1.3358 (1.3595)  loss_mask: 0.0023 (0.0042)  time: 0.1963  data: 0.0002  max mem: 5511
[22:46:21.434296] Epoch: [42]  [200/781]  eta: 0:01:56  lr: 0.000167  training_loss: 1.3636 (1.3887)  mae_loss: 0.0288 (0.0282)  classification_loss: 1.3312 (1.3565)  loss_mask: 0.0021 (0.0040)  time: 0.1962  data: 0.0002  max mem: 5511
[22:46:25.373552] Epoch: [42]  [220/781]  eta: 0:01:51  lr: 0.000167  training_loss: 1.3534 (1.3857)  mae_loss: 0.0264 (0.0281)  classification_loss: 1.3277 (1.3536)  loss_mask: 0.0024 (0.0040)  time: 0.1969  data: 0.0003  max mem: 5511
[22:46:29.302582] Epoch: [42]  [240/781]  eta: 0:01:47  lr: 0.000167  training_loss: 1.3647 (1.3861)  mae_loss: 0.0276 (0.0281)  classification_loss: 1.3256 (1.3538)  loss_mask: 0.0057 (0.0042)  time: 0.1964  data: 0.0002  max mem: 5511
[22:46:33.206412] Epoch: [42]  [260/781]  eta: 0:01:43  lr: 0.000167  training_loss: 1.3759 (1.3868)  mae_loss: 0.0257 (0.0280)  classification_loss: 1.3493 (1.3546)  loss_mask: 0.0032 (0.0042)  time: 0.1951  data: 0.0002  max mem: 5511
[22:46:37.132711] Epoch: [42]  [280/781]  eta: 0:01:39  lr: 0.000166  training_loss: 1.3412 (1.3844)  mae_loss: 0.0287 (0.0280)  classification_loss: 1.3118 (1.3522)  loss_mask: 0.0027 (0.0041)  time: 0.1962  data: 0.0002  max mem: 5511
[22:46:41.115596] Epoch: [42]  [300/781]  eta: 0:01:35  lr: 0.000166  training_loss: 1.3735 (1.3850)  mae_loss: 0.0267 (0.0280)  classification_loss: 1.3468 (1.3529)  loss_mask: 0.0028 (0.0041)  time: 0.1991  data: 0.0002  max mem: 5511
[22:46:45.067419] Epoch: [42]  [320/781]  eta: 0:01:31  lr: 0.000166  training_loss: 1.3982 (1.3862)  mae_loss: 0.0259 (0.0279)  classification_loss: 1.3691 (1.3542)  loss_mask: 0.0028 (0.0041)  time: 0.1975  data: 0.0002  max mem: 5511
[22:46:49.009992] Epoch: [42]  [340/781]  eta: 0:01:27  lr: 0.000166  training_loss: 1.3822 (1.3864)  mae_loss: 0.0279 (0.0279)  classification_loss: 1.3551 (1.3545)  loss_mask: 0.0034 (0.0041)  time: 0.1970  data: 0.0002  max mem: 5511
[22:46:52.936998] Epoch: [42]  [360/781]  eta: 0:01:23  lr: 0.000166  training_loss: 1.3872 (1.3862)  mae_loss: 0.0274 (0.0279)  classification_loss: 1.3517 (1.3542)  loss_mask: 0.0035 (0.0041)  time: 0.1963  data: 0.0002  max mem: 5511
[22:46:56.945976] Epoch: [42]  [380/781]  eta: 0:01:19  lr: 0.000166  training_loss: 1.3955 (1.3871)  mae_loss: 0.0275 (0.0279)  classification_loss: 1.3648 (1.3551)  loss_mask: 0.0032 (0.0041)  time: 0.2004  data: 0.0003  max mem: 5511
[22:47:00.887163] Epoch: [42]  [400/781]  eta: 0:01:15  lr: 0.000166  training_loss: 1.3895 (1.3867)  mae_loss: 0.0268 (0.0279)  classification_loss: 1.3589 (1.3547)  loss_mask: 0.0023 (0.0041)  time: 0.1970  data: 0.0003  max mem: 5511
[22:47:04.818488] Epoch: [42]  [420/781]  eta: 0:01:11  lr: 0.000166  training_loss: 1.3767 (1.3868)  mae_loss: 0.0287 (0.0280)  classification_loss: 1.3423 (1.3549)  loss_mask: 0.0018 (0.0040)  time: 0.1965  data: 0.0003  max mem: 5511
[22:47:08.728197] Epoch: [42]  [440/781]  eta: 0:01:07  lr: 0.000166  training_loss: 1.4368 (1.3893)  mae_loss: 0.0283 (0.0280)  classification_loss: 1.4057 (1.3573)  loss_mask: 0.0024 (0.0040)  time: 0.1954  data: 0.0002  max mem: 5511
[22:47:12.636379] Epoch: [42]  [460/781]  eta: 0:01:03  lr: 0.000166  training_loss: 1.3183 (1.3877)  mae_loss: 0.0266 (0.0280)  classification_loss: 1.2957 (1.3558)  loss_mask: 0.0018 (0.0039)  time: 0.1953  data: 0.0002  max mem: 5511
[22:47:16.579088] Epoch: [42]  [480/781]  eta: 0:00:59  lr: 0.000165  training_loss: 1.3976 (1.3884)  mae_loss: 0.0273 (0.0280)  classification_loss: 1.3667 (1.3566)  loss_mask: 0.0021 (0.0039)  time: 0.1971  data: 0.0002  max mem: 5511
[22:47:20.498354] Epoch: [42]  [500/781]  eta: 0:00:55  lr: 0.000165  training_loss: 1.4090 (1.3892)  mae_loss: 0.0288 (0.0280)  classification_loss: 1.3667 (1.3572)  loss_mask: 0.0069 (0.0040)  time: 0.1959  data: 0.0002  max mem: 5511
[22:47:24.439464] Epoch: [42]  [520/781]  eta: 0:00:51  lr: 0.000165  training_loss: 1.4031 (1.3904)  mae_loss: 0.0287 (0.0280)  classification_loss: 1.3709 (1.3583)  loss_mask: 0.0040 (0.0041)  time: 0.1970  data: 0.0003  max mem: 5511
[22:47:28.354503] Epoch: [42]  [540/781]  eta: 0:00:47  lr: 0.000165  training_loss: 1.3786 (1.3907)  mae_loss: 0.0289 (0.0280)  classification_loss: 1.3521 (1.3585)  loss_mask: 0.0038 (0.0041)  time: 0.1957  data: 0.0002  max mem: 5511
[22:47:32.304784] Epoch: [42]  [560/781]  eta: 0:00:43  lr: 0.000165  training_loss: 1.3960 (1.3906)  mae_loss: 0.0283 (0.0280)  classification_loss: 1.3666 (1.3584)  loss_mask: 0.0037 (0.0041)  time: 0.1974  data: 0.0002  max mem: 5511
[22:47:36.231275] Epoch: [42]  [580/781]  eta: 0:00:39  lr: 0.000165  training_loss: 1.3628 (1.3904)  mae_loss: 0.0272 (0.0280)  classification_loss: 1.3305 (1.3582)  loss_mask: 0.0042 (0.0042)  time: 0.1962  data: 0.0002  max mem: 5511
[22:47:40.153210] Epoch: [42]  [600/781]  eta: 0:00:35  lr: 0.000165  training_loss: 1.3342 (1.3898)  mae_loss: 0.0283 (0.0281)  classification_loss: 1.2952 (1.3574)  loss_mask: 0.0061 (0.0043)  time: 0.1960  data: 0.0004  max mem: 5511
[22:47:44.106693] Epoch: [42]  [620/781]  eta: 0:00:31  lr: 0.000165  training_loss: 1.4016 (1.3900)  mae_loss: 0.0297 (0.0281)  classification_loss: 1.3698 (1.3576)  loss_mask: 0.0029 (0.0043)  time: 0.1976  data: 0.0002  max mem: 5511
[22:47:48.002374] Epoch: [42]  [640/781]  eta: 0:00:27  lr: 0.000165  training_loss: 1.3699 (1.3900)  mae_loss: 0.0278 (0.0281)  classification_loss: 1.3396 (1.3577)  loss_mask: 0.0023 (0.0042)  time: 0.1947  data: 0.0003  max mem: 5511
[22:47:51.923164] Epoch: [42]  [660/781]  eta: 0:00:23  lr: 0.000165  training_loss: 1.3540 (1.3896)  mae_loss: 0.0271 (0.0281)  classification_loss: 1.3262 (1.3573)  loss_mask: 0.0023 (0.0042)  time: 0.1960  data: 0.0002  max mem: 5511
[22:47:55.895985] Epoch: [42]  [680/781]  eta: 0:00:19  lr: 0.000164  training_loss: 1.3770 (1.3893)  mae_loss: 0.0279 (0.0281)  classification_loss: 1.3478 (1.3571)  loss_mask: 0.0017 (0.0041)  time: 0.1985  data: 0.0003  max mem: 5511
[22:47:59.837827] Epoch: [42]  [700/781]  eta: 0:00:16  lr: 0.000164  training_loss: 1.4297 (1.3901)  mae_loss: 0.0290 (0.0281)  classification_loss: 1.3943 (1.3579)  loss_mask: 0.0019 (0.0041)  time: 0.1970  data: 0.0002  max mem: 5511
[22:48:03.774448] Epoch: [42]  [720/781]  eta: 0:00:12  lr: 0.000164  training_loss: 1.3793 (1.3899)  mae_loss: 0.0264 (0.0281)  classification_loss: 1.3469 (1.3578)  loss_mask: 0.0016 (0.0040)  time: 0.1967  data: 0.0002  max mem: 5511
[22:48:07.763865] Epoch: [42]  [740/781]  eta: 0:00:08  lr: 0.000164  training_loss: 1.3403 (1.3891)  mae_loss: 0.0269 (0.0280)  classification_loss: 1.3146 (1.3571)  loss_mask: 0.0020 (0.0040)  time: 0.1994  data: 0.0002  max mem: 5511
[22:48:11.705546] Epoch: [42]  [760/781]  eta: 0:00:04  lr: 0.000164  training_loss: 1.3979 (1.3898)  mae_loss: 0.0286 (0.0280)  classification_loss: 1.3608 (1.3578)  loss_mask: 0.0020 (0.0039)  time: 0.1970  data: 0.0005  max mem: 5511
[22:48:15.635032] Epoch: [42]  [780/781]  eta: 0:00:00  lr: 0.000164  training_loss: 1.3433 (1.3891)  mae_loss: 0.0279 (0.0280)  classification_loss: 1.3159 (1.3572)  loss_mask: 0.0015 (0.0039)  time: 0.1964  data: 0.0002  max mem: 5511
[22:48:15.793157] Epoch: [42] Total time: 0:02:34 (0.1979 s / it)
[22:48:15.793607] Averaged stats: lr: 0.000164  training_loss: 1.3433 (1.3891)  mae_loss: 0.0279 (0.0280)  classification_loss: 1.3159 (1.3572)  loss_mask: 0.0015 (0.0039)
[22:48:16.448239] Test:  [  0/157]  eta: 0:01:42  testing_loss: 0.5882 (0.5882)  acc1: 84.3750 (84.3750)  acc5: 98.4375 (98.4375)  time: 0.6508  data: 0.6212  max mem: 5511
[22:48:16.732816] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.6688 (0.6656)  acc1: 79.6875 (79.4034)  acc5: 100.0000 (99.2898)  time: 0.0849  data: 0.0567  max mem: 5511
[22:48:17.017466] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.6482 (0.6351)  acc1: 79.6875 (80.0595)  acc5: 100.0000 (99.3304)  time: 0.0283  data: 0.0002  max mem: 5511
[22:48:17.302515] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6642 (0.6545)  acc1: 79.6875 (79.4355)  acc5: 100.0000 (99.0927)  time: 0.0284  data: 0.0002  max mem: 5511
[22:48:17.590189] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6834 (0.6566)  acc1: 78.1250 (78.8872)  acc5: 98.4375 (98.9710)  time: 0.0285  data: 0.0004  max mem: 5511
[22:48:17.880992] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6122 (0.6469)  acc1: 79.6875 (79.5343)  acc5: 98.4375 (98.8664)  time: 0.0288  data: 0.0007  max mem: 5511
[22:48:18.168088] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6048 (0.6413)  acc1: 81.2500 (79.5338)  acc5: 100.0000 (98.9754)  time: 0.0288  data: 0.0005  max mem: 5511
[22:48:18.454443] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5734 (0.6288)  acc1: 82.8125 (80.1937)  acc5: 100.0000 (98.9877)  time: 0.0286  data: 0.0002  max mem: 5511
[22:48:18.745393] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5917 (0.6397)  acc1: 79.6875 (79.5525)  acc5: 100.0000 (99.0355)  time: 0.0287  data: 0.0002  max mem: 5511
[22:48:19.034030] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6279 (0.6381)  acc1: 78.1250 (79.5673)  acc5: 98.4375 (99.0213)  time: 0.0288  data: 0.0004  max mem: 5511
[22:48:19.317937] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.6369 (0.6413)  acc1: 79.6875 (79.5328)  acc5: 98.4375 (98.9325)  time: 0.0285  data: 0.0004  max mem: 5511
[22:48:19.602000] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6681 (0.6404)  acc1: 79.6875 (79.6030)  acc5: 98.4375 (98.9302)  time: 0.0282  data: 0.0002  max mem: 5511
[22:48:19.887926] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6171 (0.6377)  acc1: 79.6875 (79.6358)  acc5: 98.4375 (98.9024)  time: 0.0284  data: 0.0002  max mem: 5511
[22:48:20.171943] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6163 (0.6378)  acc1: 79.6875 (79.6159)  acc5: 98.4375 (98.9027)  time: 0.0284  data: 0.0002  max mem: 5511
[22:48:20.454834] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6534 (0.6379)  acc1: 79.6875 (79.7318)  acc5: 98.4375 (98.8697)  time: 0.0282  data: 0.0002  max mem: 5511
[22:48:20.736711] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6425 (0.6361)  acc1: 81.2500 (79.8738)  acc5: 98.4375 (98.8721)  time: 0.0281  data: 0.0001  max mem: 5511
[22:48:20.887508] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6138 (0.6357)  acc1: 81.2500 (79.8200)  acc5: 98.4375 (98.8900)  time: 0.0271  data: 0.0001  max mem: 5511
[22:48:21.027297] Test: Total time: 0:00:05 (0.0333 s / it)
[22:48:21.027752] * Acc@1 79.820 Acc@5 98.890 loss 0.636
[22:48:21.028080] Accuracy of the network on the 10000 test images: 79.8%
[22:48:21.028276] Max accuracy: 79.82%
[22:48:21.422858] log_dir: ./output_dir
[22:48:22.278488] Epoch: [43]  [  0/781]  eta: 0:11:06  lr: 0.000164  training_loss: 1.2998 (1.2998)  mae_loss: 0.0295 (0.0295)  classification_loss: 1.2692 (1.2692)  loss_mask: 0.0010 (0.0010)  time: 0.8536  data: 0.6390  max mem: 5511
[22:48:26.256728] Epoch: [43]  [ 20/781]  eta: 0:02:55  lr: 0.000164  training_loss: 1.3300 (1.3493)  mae_loss: 0.0281 (0.0281)  classification_loss: 1.2998 (1.3191)  loss_mask: 0.0017 (0.0021)  time: 0.1988  data: 0.0003  max mem: 5511
[22:48:30.185612] Epoch: [43]  [ 40/781]  eta: 0:02:38  lr: 0.000164  training_loss: 1.3640 (1.3596)  mae_loss: 0.0295 (0.0283)  classification_loss: 1.3313 (1.3288)  loss_mask: 0.0024 (0.0025)  time: 0.1963  data: 0.0003  max mem: 5511
[22:48:34.107943] Epoch: [43]  [ 60/781]  eta: 0:02:29  lr: 0.000164  training_loss: 1.4206 (1.3878)  mae_loss: 0.0293 (0.0285)  classification_loss: 1.3922 (1.3563)  loss_mask: 0.0024 (0.0030)  time: 0.1960  data: 0.0002  max mem: 5511
[22:48:38.032911] Epoch: [43]  [ 80/781]  eta: 0:02:23  lr: 0.000164  training_loss: 1.3566 (1.3802)  mae_loss: 0.0278 (0.0284)  classification_loss: 1.3277 (1.3488)  loss_mask: 0.0017 (0.0030)  time: 0.1962  data: 0.0002  max mem: 5511
[22:48:42.030028] Epoch: [43]  [100/781]  eta: 0:02:18  lr: 0.000163  training_loss: 1.4064 (1.3873)  mae_loss: 0.0256 (0.0281)  classification_loss: 1.3760 (1.3563)  loss_mask: 0.0019 (0.0030)  time: 0.1998  data: 0.0002  max mem: 5511
[22:48:45.951392] Epoch: [43]  [120/781]  eta: 0:02:13  lr: 0.000163  training_loss: 1.3689 (1.3857)  mae_loss: 0.0277 (0.0280)  classification_loss: 1.3408 (1.3547)  loss_mask: 0.0023 (0.0030)  time: 0.1960  data: 0.0002  max mem: 5511
[22:48:49.899092] Epoch: [43]  [140/781]  eta: 0:02:09  lr: 0.000163  training_loss: 1.3624 (1.3845)  mae_loss: 0.0272 (0.0280)  classification_loss: 1.3331 (1.3535)  loss_mask: 0.0027 (0.0030)  time: 0.1973  data: 0.0002  max mem: 5511
[22:48:53.816303] Epoch: [43]  [160/781]  eta: 0:02:04  lr: 0.000163  training_loss: 1.3011 (1.3775)  mae_loss: 0.0283 (0.0281)  classification_loss: 1.2755 (1.3465)  loss_mask: 0.0017 (0.0029)  time: 0.1958  data: 0.0002  max mem: 5511
[22:48:57.743785] Epoch: [43]  [180/781]  eta: 0:02:00  lr: 0.000163  training_loss: 1.3586 (1.3775)  mae_loss: 0.0275 (0.0281)  classification_loss: 1.3263 (1.3465)  loss_mask: 0.0024 (0.0029)  time: 0.1963  data: 0.0002  max mem: 5511
[22:49:01.724081] Epoch: [43]  [200/781]  eta: 0:01:56  lr: 0.000163  training_loss: 1.4146 (1.3766)  mae_loss: 0.0268 (0.0280)  classification_loss: 1.3872 (1.3456)  loss_mask: 0.0024 (0.0031)  time: 0.1989  data: 0.0003  max mem: 5511
[22:49:05.679349] Epoch: [43]  [220/781]  eta: 0:01:52  lr: 0.000163  training_loss: 1.3653 (1.3768)  mae_loss: 0.0261 (0.0280)  classification_loss: 1.3386 (1.3458)  loss_mask: 0.0016 (0.0030)  time: 0.1977  data: 0.0002  max mem: 5511
[22:49:09.614351] Epoch: [43]  [240/781]  eta: 0:01:48  lr: 0.000163  training_loss: 1.4145 (1.3789)  mae_loss: 0.0290 (0.0281)  classification_loss: 1.3896 (1.3480)  loss_mask: 0.0016 (0.0029)  time: 0.1967  data: 0.0002  max mem: 5511
[22:49:13.552791] Epoch: [43]  [260/781]  eta: 0:01:44  lr: 0.000163  training_loss: 1.3847 (1.3799)  mae_loss: 0.0278 (0.0281)  classification_loss: 1.3550 (1.3491)  loss_mask: 0.0017 (0.0028)  time: 0.1968  data: 0.0002  max mem: 5511
[22:49:17.489601] Epoch: [43]  [280/781]  eta: 0:01:39  lr: 0.000163  training_loss: 1.3707 (1.3806)  mae_loss: 0.0261 (0.0280)  classification_loss: 1.3429 (1.3497)  loss_mask: 0.0029 (0.0029)  time: 0.1968  data: 0.0002  max mem: 5511
[22:49:21.477485] Epoch: [43]  [300/781]  eta: 0:01:35  lr: 0.000162  training_loss: 1.3560 (1.3817)  mae_loss: 0.0264 (0.0279)  classification_loss: 1.3279 (1.3509)  loss_mask: 0.0024 (0.0029)  time: 0.1993  data: 0.0002  max mem: 5511
[22:49:25.395929] Epoch: [43]  [320/781]  eta: 0:01:31  lr: 0.000162  training_loss: 1.3419 (1.3810)  mae_loss: 0.0263 (0.0279)  classification_loss: 1.3176 (1.3503)  loss_mask: 0.0016 (0.0029)  time: 0.1958  data: 0.0002  max mem: 5511
[22:49:29.365248] Epoch: [43]  [340/781]  eta: 0:01:27  lr: 0.000162  training_loss: 1.3369 (1.3800)  mae_loss: 0.0276 (0.0279)  classification_loss: 1.3079 (1.3492)  loss_mask: 0.0017 (0.0028)  time: 0.1984  data: 0.0003  max mem: 5511
[22:49:33.315324] Epoch: [43]  [360/781]  eta: 0:01:23  lr: 0.000162  training_loss: 1.3564 (1.3805)  mae_loss: 0.0270 (0.0279)  classification_loss: 1.3321 (1.3498)  loss_mask: 0.0015 (0.0028)  time: 0.1974  data: 0.0003  max mem: 5511
[22:49:37.259743] Epoch: [43]  [380/781]  eta: 0:01:19  lr: 0.000162  training_loss: 1.3557 (1.3796)  mae_loss: 0.0271 (0.0279)  classification_loss: 1.3238 (1.3488)  loss_mask: 0.0024 (0.0029)  time: 0.1971  data: 0.0002  max mem: 5511
[22:49:41.193334] Epoch: [43]  [400/781]  eta: 0:01:15  lr: 0.000162  training_loss: 1.3823 (1.3795)  mae_loss: 0.0288 (0.0279)  classification_loss: 1.3470 (1.3487)  loss_mask: 0.0022 (0.0028)  time: 0.1966  data: 0.0002  max mem: 5511
[22:49:45.121302] Epoch: [43]  [420/781]  eta: 0:01:11  lr: 0.000162  training_loss: 1.3876 (1.3793)  mae_loss: 0.0283 (0.0279)  classification_loss: 1.3553 (1.3485)  loss_mask: 0.0017 (0.0028)  time: 0.1963  data: 0.0002  max mem: 5511
[22:49:49.047783] Epoch: [43]  [440/781]  eta: 0:01:07  lr: 0.000162  training_loss: 1.3478 (1.3781)  mae_loss: 0.0287 (0.0280)  classification_loss: 1.3173 (1.3473)  loss_mask: 0.0020 (0.0028)  time: 0.1962  data: 0.0003  max mem: 5511
[22:49:53.007407] Epoch: [43]  [460/781]  eta: 0:01:03  lr: 0.000162  training_loss: 1.3064 (1.3759)  mae_loss: 0.0257 (0.0279)  classification_loss: 1.2800 (1.3453)  loss_mask: 0.0016 (0.0028)  time: 0.1979  data: 0.0002  max mem: 5511
[22:49:56.927152] Epoch: [43]  [480/781]  eta: 0:00:59  lr: 0.000162  training_loss: 1.4481 (1.3788)  mae_loss: 0.0285 (0.0279)  classification_loss: 1.4153 (1.3481)  loss_mask: 0.0023 (0.0028)  time: 0.1959  data: 0.0002  max mem: 5511
[22:50:00.853522] Epoch: [43]  [500/781]  eta: 0:00:55  lr: 0.000161  training_loss: 1.3752 (1.3789)  mae_loss: 0.0267 (0.0279)  classification_loss: 1.3412 (1.3480)  loss_mask: 0.0063 (0.0030)  time: 0.1962  data: 0.0002  max mem: 5511
[22:50:04.829060] Epoch: [43]  [520/781]  eta: 0:00:51  lr: 0.000161  training_loss: 1.4105 (1.3798)  mae_loss: 0.0264 (0.0279)  classification_loss: 1.3746 (1.3488)  loss_mask: 0.0041 (0.0031)  time: 0.1987  data: 0.0002  max mem: 5511
[22:50:08.774511] Epoch: [43]  [540/781]  eta: 0:00:47  lr: 0.000161  training_loss: 1.3710 (1.3789)  mae_loss: 0.0265 (0.0278)  classification_loss: 1.3278 (1.3478)  loss_mask: 0.0055 (0.0033)  time: 0.1971  data: 0.0002  max mem: 5511
[22:50:12.694769] Epoch: [43]  [560/781]  eta: 0:00:43  lr: 0.000161  training_loss: 1.3836 (1.3790)  mae_loss: 0.0274 (0.0278)  classification_loss: 1.3522 (1.3477)  loss_mask: 0.0052 (0.0035)  time: 0.1959  data: 0.0003  max mem: 5511
[22:50:16.651562] Epoch: [43]  [580/781]  eta: 0:00:39  lr: 0.000161  training_loss: 1.3918 (1.3796)  mae_loss: 0.0276 (0.0278)  classification_loss: 1.3549 (1.3484)  loss_mask: 0.0036 (0.0035)  time: 0.1978  data: 0.0002  max mem: 5511
[22:50:20.563256] Epoch: [43]  [600/781]  eta: 0:00:35  lr: 0.000161  training_loss: 1.3886 (1.3798)  mae_loss: 0.0277 (0.0278)  classification_loss: 1.3603 (1.3486)  loss_mask: 0.0021 (0.0035)  time: 0.1955  data: 0.0002  max mem: 5511
[22:50:24.495026] Epoch: [43]  [620/781]  eta: 0:00:31  lr: 0.000161  training_loss: 1.3713 (1.3797)  mae_loss: 0.0280 (0.0278)  classification_loss: 1.3430 (1.3485)  loss_mask: 0.0024 (0.0034)  time: 0.1965  data: 0.0002  max mem: 5511
[22:50:28.419981] Epoch: [43]  [640/781]  eta: 0:00:27  lr: 0.000161  training_loss: 1.3797 (1.3792)  mae_loss: 0.0259 (0.0277)  classification_loss: 1.3568 (1.3480)  loss_mask: 0.0021 (0.0034)  time: 0.1962  data: 0.0002  max mem: 5511
[22:50:32.349585] Epoch: [43]  [660/781]  eta: 0:00:23  lr: 0.000161  training_loss: 1.4095 (1.3798)  mae_loss: 0.0277 (0.0277)  classification_loss: 1.3741 (1.3487)  loss_mask: 0.0021 (0.0034)  time: 0.1964  data: 0.0002  max mem: 5511
[22:50:36.283670] Epoch: [43]  [680/781]  eta: 0:00:19  lr: 0.000161  training_loss: 1.3534 (1.3791)  mae_loss: 0.0267 (0.0277)  classification_loss: 1.3226 (1.3480)  loss_mask: 0.0016 (0.0033)  time: 0.1966  data: 0.0003  max mem: 5511
[22:50:40.227280] Epoch: [43]  [700/781]  eta: 0:00:16  lr: 0.000160  training_loss: 1.3570 (1.3785)  mae_loss: 0.0274 (0.0277)  classification_loss: 1.3191 (1.3474)  loss_mask: 0.0034 (0.0034)  time: 0.1971  data: 0.0002  max mem: 5511
[22:50:44.154763] Epoch: [43]  [720/781]  eta: 0:00:12  lr: 0.000160  training_loss: 1.3336 (1.3781)  mae_loss: 0.0260 (0.0277)  classification_loss: 1.3074 (1.3469)  loss_mask: 0.0030 (0.0034)  time: 0.1963  data: 0.0002  max mem: 5511
[22:50:48.158587] Epoch: [43]  [740/781]  eta: 0:00:08  lr: 0.000160  training_loss: 1.3723 (1.3772)  mae_loss: 0.0271 (0.0277)  classification_loss: 1.3296 (1.3460)  loss_mask: 0.0051 (0.0035)  time: 0.2001  data: 0.0003  max mem: 5511
[22:50:52.092445] Epoch: [43]  [760/781]  eta: 0:00:04  lr: 0.000160  training_loss: 1.3775 (1.3785)  mae_loss: 0.0265 (0.0277)  classification_loss: 1.3457 (1.3473)  loss_mask: 0.0015 (0.0035)  time: 0.1966  data: 0.0003  max mem: 5511
[22:50:56.011146] Epoch: [43]  [780/781]  eta: 0:00:00  lr: 0.000160  training_loss: 1.3581 (1.3782)  mae_loss: 0.0262 (0.0277)  classification_loss: 1.3278 (1.3470)  loss_mask: 0.0018 (0.0035)  time: 0.1959  data: 0.0002  max mem: 5511
[22:50:56.169889] Epoch: [43] Total time: 0:02:34 (0.1981 s / it)
[22:50:56.170410] Averaged stats: lr: 0.000160  training_loss: 1.3581 (1.3782)  mae_loss: 0.0262 (0.0277)  classification_loss: 1.3278 (1.3470)  loss_mask: 0.0018 (0.0035)
[22:50:56.791420] Test:  [  0/157]  eta: 0:01:36  testing_loss: 0.6033 (0.6033)  acc1: 82.8125 (82.8125)  acc5: 98.4375 (98.4375)  time: 0.6165  data: 0.5869  max mem: 5511
[22:50:57.085442] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.6871 (0.6921)  acc1: 79.6875 (77.5568)  acc5: 98.4375 (98.4375)  time: 0.0826  data: 0.0539  max mem: 5511
[22:50:57.382546] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.6559 (0.6606)  acc1: 79.6875 (78.7946)  acc5: 98.4375 (98.6607)  time: 0.0294  data: 0.0005  max mem: 5511
[22:50:57.673302] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6248 (0.6731)  acc1: 78.1250 (78.4778)  acc5: 98.4375 (98.6391)  time: 0.0292  data: 0.0004  max mem: 5511
[22:50:57.961605] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6818 (0.6697)  acc1: 78.1250 (78.8491)  acc5: 98.4375 (98.6280)  time: 0.0288  data: 0.0003  max mem: 5511
[22:50:58.249702] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6397 (0.6619)  acc1: 79.6875 (79.5650)  acc5: 98.4375 (98.5294)  time: 0.0287  data: 0.0002  max mem: 5511
[22:50:58.537142] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6496 (0.6604)  acc1: 78.1250 (79.3545)  acc5: 98.4375 (98.5912)  time: 0.0286  data: 0.0002  max mem: 5511
[22:50:58.821223] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6352 (0.6539)  acc1: 78.1250 (79.5114)  acc5: 100.0000 (98.7236)  time: 0.0284  data: 0.0002  max mem: 5511
[22:50:59.110049] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6484 (0.6632)  acc1: 79.6875 (79.1474)  acc5: 100.0000 (98.7654)  time: 0.0285  data: 0.0002  max mem: 5511
[22:50:59.401300] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6484 (0.6616)  acc1: 79.6875 (79.3441)  acc5: 98.4375 (98.6951)  time: 0.0289  data: 0.0002  max mem: 5511
[22:50:59.689532] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.6617 (0.6654)  acc1: 79.6875 (79.1615)  acc5: 98.4375 (98.6850)  time: 0.0289  data: 0.0002  max mem: 5511
[22:50:59.977640] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6647 (0.6651)  acc1: 78.1250 (79.1807)  acc5: 100.0000 (98.7331)  time: 0.0287  data: 0.0002  max mem: 5511
[22:51:00.269584] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6635 (0.6626)  acc1: 79.6875 (79.2743)  acc5: 98.4375 (98.7345)  time: 0.0288  data: 0.0003  max mem: 5511
[22:51:00.560494] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6727 (0.6630)  acc1: 79.6875 (79.3177)  acc5: 98.4375 (98.6999)  time: 0.0289  data: 0.0004  max mem: 5511
[22:51:00.850135] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6435 (0.6626)  acc1: 79.6875 (79.2553)  acc5: 98.4375 (98.6813)  time: 0.0289  data: 0.0004  max mem: 5511
[22:51:01.131702] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6373 (0.6608)  acc1: 78.1250 (79.2425)  acc5: 98.4375 (98.7272)  time: 0.0284  data: 0.0003  max mem: 5511
[22:51:01.285046] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6318 (0.6608)  acc1: 78.1250 (79.2000)  acc5: 98.4375 (98.7400)  time: 0.0272  data: 0.0002  max mem: 5511
[22:51:01.435746] Test: Total time: 0:00:05 (0.0335 s / it)
[22:51:01.436205] * Acc@1 79.200 Acc@5 98.740 loss 0.661
[22:51:01.436690] Accuracy of the network on the 10000 test images: 79.2%
[22:51:01.436912] Max accuracy: 79.82%
[22:51:01.606454] log_dir: ./output_dir
[22:51:02.456452] Epoch: [44]  [  0/781]  eta: 0:11:02  lr: 0.000160  training_loss: 1.1911 (1.1911)  mae_loss: 0.0305 (0.0305)  classification_loss: 1.1575 (1.1575)  loss_mask: 0.0031 (0.0031)  time: 0.8481  data: 0.6092  max mem: 5511
[22:51:06.467518] Epoch: [44]  [ 20/781]  eta: 0:02:55  lr: 0.000160  training_loss: 1.3309 (1.3232)  mae_loss: 0.0271 (0.0280)  classification_loss: 1.2980 (1.2933)  loss_mask: 0.0014 (0.0019)  time: 0.2004  data: 0.0002  max mem: 5511
[22:51:10.411859] Epoch: [44]  [ 40/781]  eta: 0:02:39  lr: 0.000160  training_loss: 1.3475 (1.3403)  mae_loss: 0.0268 (0.0275)  classification_loss: 1.3201 (1.3111)  loss_mask: 0.0013 (0.0017)  time: 0.1971  data: 0.0002  max mem: 5511
[22:51:14.349853] Epoch: [44]  [ 60/781]  eta: 0:02:30  lr: 0.000160  training_loss: 1.4014 (1.3579)  mae_loss: 0.0274 (0.0276)  classification_loss: 1.3687 (1.3286)  loss_mask: 0.0013 (0.0017)  time: 0.1968  data: 0.0002  max mem: 5511
[22:51:18.283293] Epoch: [44]  [ 80/781]  eta: 0:02:24  lr: 0.000160  training_loss: 1.3953 (1.3665)  mae_loss: 0.0276 (0.0279)  classification_loss: 1.3577 (1.3360)  loss_mask: 0.0033 (0.0026)  time: 0.1966  data: 0.0002  max mem: 5511
[22:51:22.221870] Epoch: [44]  [100/781]  eta: 0:02:18  lr: 0.000160  training_loss: 1.3777 (1.3649)  mae_loss: 0.0277 (0.0279)  classification_loss: 1.3352 (1.3336)  loss_mask: 0.0053 (0.0034)  time: 0.1968  data: 0.0002  max mem: 5511
[22:51:26.152197] Epoch: [44]  [120/781]  eta: 0:02:14  lr: 0.000159  training_loss: 1.3496 (1.3650)  mae_loss: 0.0267 (0.0278)  classification_loss: 1.3146 (1.3335)  loss_mask: 0.0038 (0.0038)  time: 0.1964  data: 0.0002  max mem: 5511
[22:51:30.094125] Epoch: [44]  [140/781]  eta: 0:02:09  lr: 0.000159  training_loss: 1.3471 (1.3651)  mae_loss: 0.0277 (0.0279)  classification_loss: 1.3161 (1.3335)  loss_mask: 0.0023 (0.0038)  time: 0.1970  data: 0.0002  max mem: 5511
[22:51:34.021081] Epoch: [44]  [160/781]  eta: 0:02:04  lr: 0.000159  training_loss: 1.4022 (1.3677)  mae_loss: 0.0259 (0.0277)  classification_loss: 1.3601 (1.3342)  loss_mask: 0.0156 (0.0058)  time: 0.1963  data: 0.0002  max mem: 5511
[22:51:37.966130] Epoch: [44]  [180/781]  eta: 0:02:00  lr: 0.000159  training_loss: 1.3351 (1.3673)  mae_loss: 0.0267 (0.0277)  classification_loss: 1.2894 (1.3336)  loss_mask: 0.0065 (0.0061)  time: 0.1972  data: 0.0002  max mem: 5511
[22:51:41.896435] Epoch: [44]  [200/781]  eta: 0:01:56  lr: 0.000159  training_loss: 1.3449 (1.3672)  mae_loss: 0.0267 (0.0277)  classification_loss: 1.3086 (1.3335)  loss_mask: 0.0049 (0.0060)  time: 0.1964  data: 0.0002  max mem: 5511
[22:51:45.827640] Epoch: [44]  [220/781]  eta: 0:01:52  lr: 0.000159  training_loss: 1.3827 (1.3666)  mae_loss: 0.0266 (0.0276)  classification_loss: 1.3520 (1.3333)  loss_mask: 0.0030 (0.0058)  time: 0.1965  data: 0.0002  max mem: 5511
[22:51:49.737958] Epoch: [44]  [240/781]  eta: 0:01:47  lr: 0.000159  training_loss: 1.3688 (1.3674)  mae_loss: 0.0270 (0.0276)  classification_loss: 1.3355 (1.3341)  loss_mask: 0.0049 (0.0057)  time: 0.1954  data: 0.0002  max mem: 5511
[22:51:53.680262] Epoch: [44]  [260/781]  eta: 0:01:43  lr: 0.000159  training_loss: 1.4002 (1.3684)  mae_loss: 0.0268 (0.0275)  classification_loss: 1.3665 (1.3351)  loss_mask: 0.0062 (0.0059)  time: 0.1970  data: 0.0002  max mem: 5511
[22:51:57.632845] Epoch: [44]  [280/781]  eta: 0:01:39  lr: 0.000159  training_loss: 1.3400 (1.3674)  mae_loss: 0.0263 (0.0275)  classification_loss: 1.3150 (1.3343)  loss_mask: 0.0022 (0.0056)  time: 0.1975  data: 0.0002  max mem: 5511
[22:52:01.568510] Epoch: [44]  [300/781]  eta: 0:01:35  lr: 0.000159  training_loss: 1.3872 (1.3689)  mae_loss: 0.0251 (0.0274)  classification_loss: 1.3572 (1.3360)  loss_mask: 0.0024 (0.0054)  time: 0.1967  data: 0.0002  max mem: 5511
[22:52:05.529701] Epoch: [44]  [320/781]  eta: 0:01:31  lr: 0.000158  training_loss: 1.3891 (1.3711)  mae_loss: 0.0281 (0.0275)  classification_loss: 1.3654 (1.3384)  loss_mask: 0.0017 (0.0052)  time: 0.1979  data: 0.0002  max mem: 5511
[22:52:09.486477] Epoch: [44]  [340/781]  eta: 0:01:27  lr: 0.000158  training_loss: 1.3425 (1.3687)  mae_loss: 0.0275 (0.0275)  classification_loss: 1.3108 (1.3361)  loss_mask: 0.0017 (0.0050)  time: 0.1978  data: 0.0002  max mem: 5511
[22:52:13.400762] Epoch: [44]  [360/781]  eta: 0:01:23  lr: 0.000158  training_loss: 1.4040 (1.3714)  mae_loss: 0.0274 (0.0275)  classification_loss: 1.3716 (1.3389)  loss_mask: 0.0020 (0.0050)  time: 0.1956  data: 0.0003  max mem: 5511
[22:52:17.333250] Epoch: [44]  [380/781]  eta: 0:01:19  lr: 0.000158  training_loss: 1.3578 (1.3710)  mae_loss: 0.0279 (0.0276)  classification_loss: 1.3247 (1.3386)  loss_mask: 0.0014 (0.0048)  time: 0.1965  data: 0.0003  max mem: 5511
[22:52:21.277273] Epoch: [44]  [400/781]  eta: 0:01:15  lr: 0.000158  training_loss: 1.3973 (1.3723)  mae_loss: 0.0270 (0.0276)  classification_loss: 1.3645 (1.3400)  loss_mask: 0.0019 (0.0047)  time: 0.1971  data: 0.0002  max mem: 5511
[22:52:25.251142] Epoch: [44]  [420/781]  eta: 0:01:11  lr: 0.000158  training_loss: 1.3487 (1.3714)  mae_loss: 0.0275 (0.0276)  classification_loss: 1.3239 (1.3392)  loss_mask: 0.0024 (0.0047)  time: 0.1986  data: 0.0002  max mem: 5511
[22:52:29.173412] Epoch: [44]  [440/781]  eta: 0:01:07  lr: 0.000158  training_loss: 1.3530 (1.3703)  mae_loss: 0.0280 (0.0276)  classification_loss: 1.3171 (1.3376)  loss_mask: 0.0073 (0.0051)  time: 0.1960  data: 0.0002  max mem: 5511
[22:52:33.130458] Epoch: [44]  [460/781]  eta: 0:01:03  lr: 0.000158  training_loss: 1.3016 (1.3684)  mae_loss: 0.0274 (0.0275)  classification_loss: 1.2628 (1.3356)  loss_mask: 0.0056 (0.0052)  time: 0.1978  data: 0.0002  max mem: 5511
[22:52:37.050595] Epoch: [44]  [480/781]  eta: 0:00:59  lr: 0.000158  training_loss: 1.3634 (1.3696)  mae_loss: 0.0275 (0.0275)  classification_loss: 1.3395 (1.3367)  loss_mask: 0.0040 (0.0053)  time: 0.1959  data: 0.0002  max mem: 5511
[22:52:41.037366] Epoch: [44]  [500/781]  eta: 0:00:55  lr: 0.000157  training_loss: 1.3889 (1.3706)  mae_loss: 0.0265 (0.0275)  classification_loss: 1.3612 (1.3378)  loss_mask: 0.0024 (0.0052)  time: 0.1992  data: 0.0003  max mem: 5511
[22:52:44.954938] Epoch: [44]  [520/781]  eta: 0:00:51  lr: 0.000157  training_loss: 1.3351 (1.3710)  mae_loss: 0.0286 (0.0275)  classification_loss: 1.2997 (1.3381)  loss_mask: 0.0049 (0.0053)  time: 0.1958  data: 0.0002  max mem: 5511
[22:52:48.884108] Epoch: [44]  [540/781]  eta: 0:00:47  lr: 0.000157  training_loss: 1.3593 (1.3713)  mae_loss: 0.0271 (0.0275)  classification_loss: 1.3251 (1.3383)  loss_mask: 0.0044 (0.0054)  time: 0.1964  data: 0.0002  max mem: 5511
[22:52:52.830305] Epoch: [44]  [560/781]  eta: 0:00:43  lr: 0.000157  training_loss: 1.3553 (1.3720)  mae_loss: 0.0267 (0.0275)  classification_loss: 1.3200 (1.3390)  loss_mask: 0.0056 (0.0054)  time: 0.1972  data: 0.0002  max mem: 5511
[22:52:56.742146] Epoch: [44]  [580/781]  eta: 0:00:39  lr: 0.000157  training_loss: 1.3800 (1.3723)  mae_loss: 0.0280 (0.0275)  classification_loss: 1.3544 (1.3394)  loss_mask: 0.0040 (0.0054)  time: 0.1955  data: 0.0002  max mem: 5511
[22:53:00.659275] Epoch: [44]  [600/781]  eta: 0:00:35  lr: 0.000157  training_loss: 1.3434 (1.3708)  mae_loss: 0.0283 (0.0276)  classification_loss: 1.3105 (1.3379)  loss_mask: 0.0020 (0.0053)  time: 0.1958  data: 0.0002  max mem: 5511
[22:53:04.577629] Epoch: [44]  [620/781]  eta: 0:00:31  lr: 0.000157  training_loss: 1.3355 (1.3701)  mae_loss: 0.0288 (0.0276)  classification_loss: 1.3047 (1.3373)  loss_mask: 0.0019 (0.0052)  time: 0.1958  data: 0.0002  max mem: 5511
[22:53:08.492412] Epoch: [44]  [640/781]  eta: 0:00:27  lr: 0.000157  training_loss: 1.4160 (1.3707)  mae_loss: 0.0264 (0.0276)  classification_loss: 1.3914 (1.3380)  loss_mask: 0.0023 (0.0051)  time: 0.1956  data: 0.0002  max mem: 5511
[22:53:12.406101] Epoch: [44]  [660/781]  eta: 0:00:23  lr: 0.000157  training_loss: 1.3527 (1.3702)  mae_loss: 0.0269 (0.0276)  classification_loss: 1.3200 (1.3376)  loss_mask: 0.0016 (0.0050)  time: 0.1956  data: 0.0002  max mem: 5511
[22:53:16.335576] Epoch: [44]  [680/781]  eta: 0:00:19  lr: 0.000157  training_loss: 1.3793 (1.3713)  mae_loss: 0.0284 (0.0276)  classification_loss: 1.3521 (1.3387)  loss_mask: 0.0011 (0.0049)  time: 0.1964  data: 0.0002  max mem: 5511
[22:53:20.251579] Epoch: [44]  [700/781]  eta: 0:00:16  lr: 0.000156  training_loss: 1.3765 (1.3714)  mae_loss: 0.0270 (0.0276)  classification_loss: 1.3476 (1.3389)  loss_mask: 0.0036 (0.0049)  time: 0.1957  data: 0.0002  max mem: 5511
[22:53:24.171324] Epoch: [44]  [720/781]  eta: 0:00:12  lr: 0.000156  training_loss: 1.3977 (1.3714)  mae_loss: 0.0265 (0.0276)  classification_loss: 1.3688 (1.3389)  loss_mask: 0.0022 (0.0049)  time: 0.1959  data: 0.0002  max mem: 5511
[22:53:28.093505] Epoch: [44]  [740/781]  eta: 0:00:08  lr: 0.000156  training_loss: 1.3156 (1.3703)  mae_loss: 0.0281 (0.0276)  classification_loss: 1.2909 (1.3379)  loss_mask: 0.0022 (0.0048)  time: 0.1960  data: 0.0002  max mem: 5511
[22:53:32.015843] Epoch: [44]  [760/781]  eta: 0:00:04  lr: 0.000156  training_loss: 1.3850 (1.3706)  mae_loss: 0.0265 (0.0276)  classification_loss: 1.3486 (1.3382)  loss_mask: 0.0014 (0.0047)  time: 0.1960  data: 0.0002  max mem: 5511
[22:53:35.934123] Epoch: [44]  [780/781]  eta: 0:00:00  lr: 0.000156  training_loss: 1.3826 (1.3706)  mae_loss: 0.0280 (0.0276)  classification_loss: 1.3538 (1.3383)  loss_mask: 0.0021 (0.0047)  time: 0.1958  data: 0.0002  max mem: 5511
[22:53:36.095795] Epoch: [44] Total time: 0:02:34 (0.1978 s / it)
[22:53:36.096313] Averaged stats: lr: 0.000156  training_loss: 1.3826 (1.3706)  mae_loss: 0.0280 (0.0276)  classification_loss: 1.3538 (1.3383)  loss_mask: 0.0021 (0.0047)
[22:53:36.727284] Test:  [  0/157]  eta: 0:01:38  testing_loss: 0.6160 (0.6160)  acc1: 79.6875 (79.6875)  acc5: 100.0000 (100.0000)  time: 0.6267  data: 0.5972  max mem: 5511
[22:53:37.012114] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.6761 (0.6897)  acc1: 79.6875 (78.2670)  acc5: 98.4375 (98.7216)  time: 0.0827  data: 0.0545  max mem: 5511
[22:53:37.298019] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.6414 (0.6515)  acc1: 81.2500 (79.6131)  acc5: 98.4375 (98.8839)  time: 0.0284  data: 0.0002  max mem: 5511
[22:53:37.583207] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6414 (0.6618)  acc1: 81.2500 (79.1835)  acc5: 98.4375 (98.7399)  time: 0.0283  data: 0.0002  max mem: 5511
[22:53:37.877563] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6676 (0.6640)  acc1: 79.6875 (79.2302)  acc5: 98.4375 (98.5518)  time: 0.0288  data: 0.0002  max mem: 5511
[22:53:38.163712] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6359 (0.6506)  acc1: 79.6875 (80.1777)  acc5: 98.4375 (98.4988)  time: 0.0289  data: 0.0002  max mem: 5511
[22:53:38.451940] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6226 (0.6490)  acc1: 81.2500 (80.1998)  acc5: 98.4375 (98.5656)  time: 0.0286  data: 0.0004  max mem: 5511
[22:53:38.739169] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6048 (0.6407)  acc1: 81.2500 (80.3697)  acc5: 98.4375 (98.5915)  time: 0.0286  data: 0.0004  max mem: 5511
[22:53:39.024941] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6262 (0.6466)  acc1: 79.6875 (80.0926)  acc5: 98.4375 (98.6304)  time: 0.0285  data: 0.0002  max mem: 5511
[22:53:39.309116] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6321 (0.6430)  acc1: 79.6875 (80.2885)  acc5: 98.4375 (98.6264)  time: 0.0284  data: 0.0002  max mem: 5511
[22:53:39.591647] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.6457 (0.6480)  acc1: 79.6875 (80.0433)  acc5: 98.4375 (98.6386)  time: 0.0282  data: 0.0002  max mem: 5511
[22:53:39.875299] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6733 (0.6470)  acc1: 78.1250 (80.0816)  acc5: 100.0000 (98.7050)  time: 0.0282  data: 0.0001  max mem: 5511
[22:53:40.158082] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6503 (0.6450)  acc1: 79.6875 (80.0749)  acc5: 98.4375 (98.6829)  time: 0.0282  data: 0.0002  max mem: 5511
[22:53:40.444612] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6122 (0.6446)  acc1: 79.6875 (80.1527)  acc5: 98.4375 (98.6999)  time: 0.0283  data: 0.0002  max mem: 5511
[22:53:40.731619] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6277 (0.6458)  acc1: 79.6875 (80.0643)  acc5: 98.4375 (98.7035)  time: 0.0285  data: 0.0003  max mem: 5511
[22:53:41.013972] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6609 (0.6439)  acc1: 79.6875 (80.1325)  acc5: 98.4375 (98.7169)  time: 0.0283  data: 0.0002  max mem: 5511
[22:53:41.167920] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6277 (0.6437)  acc1: 79.6875 (80.1000)  acc5: 98.4375 (98.7100)  time: 0.0273  data: 0.0002  max mem: 5511
[22:53:41.321043] Test: Total time: 0:00:05 (0.0333 s / it)
[22:53:41.321781] * Acc@1 80.100 Acc@5 98.710 loss 0.644
[22:53:41.322076] Accuracy of the network on the 10000 test images: 80.1%
[22:53:41.322298] Max accuracy: 80.10%
[22:53:41.615424] log_dir: ./output_dir
[22:53:42.477712] Epoch: [45]  [  0/781]  eta: 0:11:11  lr: 0.000156  training_loss: 1.3748 (1.3748)  mae_loss: 0.0223 (0.0223)  classification_loss: 1.3478 (1.3478)  loss_mask: 0.0047 (0.0047)  time: 0.8603  data: 0.6387  max mem: 5511
[22:53:46.398814] Epoch: [45]  [ 20/781]  eta: 0:02:53  lr: 0.000156  training_loss: 1.3733 (1.3771)  mae_loss: 0.0280 (0.0282)  classification_loss: 1.3418 (1.3462)  loss_mask: 0.0020 (0.0026)  time: 0.1960  data: 0.0003  max mem: 5511
[22:53:50.356075] Epoch: [45]  [ 40/781]  eta: 0:02:37  lr: 0.000156  training_loss: 1.3500 (1.3627)  mae_loss: 0.0272 (0.0282)  classification_loss: 1.3204 (1.3309)  loss_mask: 0.0028 (0.0035)  time: 0.1978  data: 0.0003  max mem: 5511
[22:53:54.289810] Epoch: [45]  [ 60/781]  eta: 0:02:29  lr: 0.000156  training_loss: 1.3584 (1.3645)  mae_loss: 0.0265 (0.0281)  classification_loss: 1.3293 (1.3327)  loss_mask: 0.0027 (0.0037)  time: 0.1966  data: 0.0002  max mem: 5511
[22:53:58.209193] Epoch: [45]  [ 80/781]  eta: 0:02:23  lr: 0.000156  training_loss: 1.3553 (1.3624)  mae_loss: 0.0285 (0.0282)  classification_loss: 1.3192 (1.3310)  loss_mask: 0.0017 (0.0033)  time: 0.1959  data: 0.0003  max mem: 5511
[22:54:02.137469] Epoch: [45]  [100/781]  eta: 0:02:18  lr: 0.000156  training_loss: 1.3834 (1.3656)  mae_loss: 0.0267 (0.0280)  classification_loss: 1.3505 (1.3346)  loss_mask: 0.0018 (0.0030)  time: 0.1963  data: 0.0002  max mem: 5511
[22:54:06.077262] Epoch: [45]  [120/781]  eta: 0:02:13  lr: 0.000155  training_loss: 1.2958 (1.3564)  mae_loss: 0.0271 (0.0279)  classification_loss: 1.2677 (1.3257)  loss_mask: 0.0017 (0.0029)  time: 0.1969  data: 0.0002  max mem: 5511
[22:54:10.005842] Epoch: [45]  [140/781]  eta: 0:02:09  lr: 0.000155  training_loss: 1.3450 (1.3554)  mae_loss: 0.0271 (0.0279)  classification_loss: 1.3193 (1.3249)  loss_mask: 0.0011 (0.0027)  time: 0.1963  data: 0.0002  max mem: 5511
[22:54:13.935472] Epoch: [45]  [160/781]  eta: 0:02:04  lr: 0.000155  training_loss: 1.3801 (1.3595)  mae_loss: 0.0276 (0.0278)  classification_loss: 1.3494 (1.3291)  loss_mask: 0.0014 (0.0026)  time: 0.1964  data: 0.0002  max mem: 5511
[22:54:17.865381] Epoch: [45]  [180/781]  eta: 0:02:00  lr: 0.000155  training_loss: 1.3087 (1.3565)  mae_loss: 0.0283 (0.0279)  classification_loss: 1.2753 (1.3261)  loss_mask: 0.0020 (0.0025)  time: 0.1964  data: 0.0002  max mem: 5511
[22:54:21.793421] Epoch: [45]  [200/781]  eta: 0:01:56  lr: 0.000155  training_loss: 1.3014 (1.3540)  mae_loss: 0.0280 (0.0279)  classification_loss: 1.2700 (1.3236)  loss_mask: 0.0018 (0.0025)  time: 0.1963  data: 0.0002  max mem: 5511
[22:54:25.719514] Epoch: [45]  [220/781]  eta: 0:01:51  lr: 0.000155  training_loss: 1.3370 (1.3538)  mae_loss: 0.0264 (0.0278)  classification_loss: 1.3107 (1.3234)  loss_mask: 0.0017 (0.0025)  time: 0.1962  data: 0.0002  max mem: 5511
[22:54:29.643945] Epoch: [45]  [240/781]  eta: 0:01:47  lr: 0.000155  training_loss: 1.3469 (1.3560)  mae_loss: 0.0274 (0.0278)  classification_loss: 1.3175 (1.3255)  loss_mask: 0.0032 (0.0027)  time: 0.1961  data: 0.0002  max mem: 5511
[22:54:33.575962] Epoch: [45]  [260/781]  eta: 0:01:43  lr: 0.000155  training_loss: 1.3608 (1.3557)  mae_loss: 0.0272 (0.0278)  classification_loss: 1.3284 (1.3252)  loss_mask: 0.0026 (0.0028)  time: 0.1965  data: 0.0002  max mem: 5511
[22:54:37.508273] Epoch: [45]  [280/781]  eta: 0:01:39  lr: 0.000155  training_loss: 1.3009 (1.3533)  mae_loss: 0.0279 (0.0278)  classification_loss: 1.2734 (1.3226)  loss_mask: 0.0027 (0.0029)  time: 0.1965  data: 0.0002  max mem: 5511
[22:54:41.400792] Epoch: [45]  [300/781]  eta: 0:01:35  lr: 0.000155  training_loss: 1.3780 (1.3558)  mae_loss: 0.0279 (0.0278)  classification_loss: 1.3420 (1.3251)  loss_mask: 0.0018 (0.0029)  time: 0.1945  data: 0.0002  max mem: 5511
[22:54:45.305326] Epoch: [45]  [320/781]  eta: 0:01:31  lr: 0.000154  training_loss: 1.3697 (1.3574)  mae_loss: 0.0289 (0.0278)  classification_loss: 1.3404 (1.3267)  loss_mask: 0.0024 (0.0029)  time: 0.1952  data: 0.0002  max mem: 5511
[22:54:49.218487] Epoch: [45]  [340/781]  eta: 0:01:27  lr: 0.000154  training_loss: 1.2951 (1.3559)  mae_loss: 0.0277 (0.0279)  classification_loss: 1.2670 (1.3252)  loss_mask: 0.0012 (0.0028)  time: 0.1955  data: 0.0002  max mem: 5511
[22:54:53.145407] Epoch: [45]  [360/781]  eta: 0:01:23  lr: 0.000154  training_loss: 1.3354 (1.3561)  mae_loss: 0.0285 (0.0279)  classification_loss: 1.3095 (1.3253)  loss_mask: 0.0026 (0.0028)  time: 0.1963  data: 0.0002  max mem: 5511
[22:54:57.093063] Epoch: [45]  [380/781]  eta: 0:01:19  lr: 0.000154  training_loss: 1.3438 (1.3561)  mae_loss: 0.0277 (0.0279)  classification_loss: 1.3102 (1.3253)  loss_mask: 0.0030 (0.0029)  time: 0.1973  data: 0.0002  max mem: 5511
[22:55:01.037126] Epoch: [45]  [400/781]  eta: 0:01:15  lr: 0.000154  training_loss: 1.3492 (1.3559)  mae_loss: 0.0281 (0.0280)  classification_loss: 1.3167 (1.3249)  loss_mask: 0.0054 (0.0030)  time: 0.1971  data: 0.0002  max mem: 5511
[22:55:04.978558] Epoch: [45]  [420/781]  eta: 0:01:11  lr: 0.000154  training_loss: 1.3418 (1.3554)  mae_loss: 0.0288 (0.0280)  classification_loss: 1.3112 (1.3240)  loss_mask: 0.0076 (0.0034)  time: 0.1970  data: 0.0003  max mem: 5511
[22:55:08.900365] Epoch: [45]  [440/781]  eta: 0:01:07  lr: 0.000154  training_loss: 1.3795 (1.3560)  mae_loss: 0.0261 (0.0280)  classification_loss: 1.3467 (1.3246)  loss_mask: 0.0051 (0.0035)  time: 0.1960  data: 0.0003  max mem: 5511
[22:55:12.823011] Epoch: [45]  [460/781]  eta: 0:01:03  lr: 0.000154  training_loss: 1.3157 (1.3554)  mae_loss: 0.0263 (0.0279)  classification_loss: 1.2831 (1.3240)  loss_mask: 0.0021 (0.0035)  time: 0.1960  data: 0.0003  max mem: 5511
[22:55:16.777026] Epoch: [45]  [480/781]  eta: 0:00:59  lr: 0.000154  training_loss: 1.2983 (1.3549)  mae_loss: 0.0271 (0.0279)  classification_loss: 1.2695 (1.3235)  loss_mask: 0.0025 (0.0035)  time: 0.1976  data: 0.0005  max mem: 5511
[22:55:20.717099] Epoch: [45]  [500/781]  eta: 0:00:55  lr: 0.000154  training_loss: 1.3794 (1.3561)  mae_loss: 0.0278 (0.0279)  classification_loss: 1.3508 (1.3246)  loss_mask: 0.0051 (0.0037)  time: 0.1969  data: 0.0004  max mem: 5511
[22:55:24.672678] Epoch: [45]  [520/781]  eta: 0:00:51  lr: 0.000153  training_loss: 1.3044 (1.3555)  mae_loss: 0.0290 (0.0279)  classification_loss: 1.2724 (1.3239)  loss_mask: 0.0018 (0.0036)  time: 0.1976  data: 0.0002  max mem: 5511
[22:55:28.624748] Epoch: [45]  [540/781]  eta: 0:00:47  lr: 0.000153  training_loss: 1.3833 (1.3562)  mae_loss: 0.0273 (0.0279)  classification_loss: 1.3523 (1.3247)  loss_mask: 0.0018 (0.0036)  time: 0.1975  data: 0.0003  max mem: 5511
[22:55:32.632720] Epoch: [45]  [560/781]  eta: 0:00:43  lr: 0.000153  training_loss: 1.3194 (1.3555)  mae_loss: 0.0267 (0.0279)  classification_loss: 1.2967 (1.3241)  loss_mask: 0.0009 (0.0035)  time: 0.2002  data: 0.0003  max mem: 5511
[22:55:36.586177] Epoch: [45]  [580/781]  eta: 0:00:39  lr: 0.000153  training_loss: 1.3201 (1.3552)  mae_loss: 0.0273 (0.0279)  classification_loss: 1.2975 (1.3239)  loss_mask: 0.0016 (0.0035)  time: 0.1976  data: 0.0002  max mem: 5511
[22:55:40.521655] Epoch: [45]  [600/781]  eta: 0:00:35  lr: 0.000153  training_loss: 1.3156 (1.3538)  mae_loss: 0.0275 (0.0278)  classification_loss: 1.2847 (1.3225)  loss_mask: 0.0012 (0.0034)  time: 0.1967  data: 0.0002  max mem: 5511
[22:55:44.540396] Epoch: [45]  [620/781]  eta: 0:00:31  lr: 0.000153  training_loss: 1.3784 (1.3548)  mae_loss: 0.0297 (0.0279)  classification_loss: 1.3446 (1.3236)  loss_mask: 0.0015 (0.0034)  time: 0.2008  data: 0.0002  max mem: 5511
[22:55:48.510300] Epoch: [45]  [640/781]  eta: 0:00:27  lr: 0.000153  training_loss: 1.3534 (1.3544)  mae_loss: 0.0269 (0.0279)  classification_loss: 1.3206 (1.3232)  loss_mask: 0.0014 (0.0033)  time: 0.1984  data: 0.0002  max mem: 5511
[22:55:52.464264] Epoch: [45]  [660/781]  eta: 0:00:23  lr: 0.000153  training_loss: 1.3447 (1.3547)  mae_loss: 0.0275 (0.0279)  classification_loss: 1.3163 (1.3235)  loss_mask: 0.0018 (0.0033)  time: 0.1976  data: 0.0003  max mem: 5511
[22:55:56.401768] Epoch: [45]  [680/781]  eta: 0:00:19  lr: 0.000153  training_loss: 1.4063 (1.3558)  mae_loss: 0.0276 (0.0279)  classification_loss: 1.3759 (1.3246)  loss_mask: 0.0044 (0.0033)  time: 0.1968  data: 0.0002  max mem: 5511
[22:56:00.324740] Epoch: [45]  [700/781]  eta: 0:00:16  lr: 0.000152  training_loss: 1.3812 (1.3565)  mae_loss: 0.0267 (0.0279)  classification_loss: 1.3488 (1.3252)  loss_mask: 0.0047 (0.0034)  time: 0.1961  data: 0.0002  max mem: 5511
[22:56:04.281711] Epoch: [45]  [720/781]  eta: 0:00:12  lr: 0.000152  training_loss: 1.3560 (1.3567)  mae_loss: 0.0280 (0.0279)  classification_loss: 1.3263 (1.3253)  loss_mask: 0.0024 (0.0035)  time: 0.1978  data: 0.0002  max mem: 5511
[22:56:08.206275] Epoch: [45]  [740/781]  eta: 0:00:08  lr: 0.000152  training_loss: 1.3177 (1.3560)  mae_loss: 0.0257 (0.0279)  classification_loss: 1.2839 (1.3246)  loss_mask: 0.0034 (0.0036)  time: 0.1961  data: 0.0002  max mem: 5511
[22:56:12.144563] Epoch: [45]  [760/781]  eta: 0:00:04  lr: 0.000152  training_loss: 1.3653 (1.3566)  mae_loss: 0.0265 (0.0278)  classification_loss: 1.3372 (1.3252)  loss_mask: 0.0019 (0.0035)  time: 0.1968  data: 0.0004  max mem: 5511
[22:56:16.056162] Epoch: [45]  [780/781]  eta: 0:00:00  lr: 0.000152  training_loss: 1.3686 (1.3572)  mae_loss: 0.0255 (0.0278)  classification_loss: 1.3381 (1.3259)  loss_mask: 0.0021 (0.0035)  time: 0.1955  data: 0.0002  max mem: 5511
[22:56:16.214039] Epoch: [45] Total time: 0:02:34 (0.1979 s / it)
[22:56:16.214569] Averaged stats: lr: 0.000152  training_loss: 1.3686 (1.3572)  mae_loss: 0.0255 (0.0278)  classification_loss: 1.3381 (1.3259)  loss_mask: 0.0021 (0.0035)
[22:56:16.847785] Test:  [  0/157]  eta: 0:01:38  testing_loss: 0.5895 (0.5895)  acc1: 78.1250 (78.1250)  acc5: 98.4375 (98.4375)  time: 0.6268  data: 0.5972  max mem: 5511
[22:56:17.136796] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.6382 (0.6529)  acc1: 78.1250 (79.4034)  acc5: 100.0000 (99.2898)  time: 0.0828  data: 0.0545  max mem: 5511
[22:56:17.425537] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.6284 (0.6231)  acc1: 79.6875 (80.2827)  acc5: 100.0000 (99.1815)  time: 0.0286  data: 0.0003  max mem: 5511
[22:56:17.713899] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6249 (0.6326)  acc1: 79.6875 (80.1411)  acc5: 98.4375 (98.9415)  time: 0.0287  data: 0.0003  max mem: 5511
[22:56:17.998573] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6346 (0.6339)  acc1: 79.6875 (79.9924)  acc5: 98.4375 (98.9329)  time: 0.0285  data: 0.0002  max mem: 5511
[22:56:18.283618] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5988 (0.6251)  acc1: 79.6875 (80.4841)  acc5: 98.4375 (98.8971)  time: 0.0283  data: 0.0002  max mem: 5511
[22:56:18.567972] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5657 (0.6217)  acc1: 81.2500 (80.6352)  acc5: 100.0000 (98.8986)  time: 0.0283  data: 0.0002  max mem: 5511
[22:56:18.853301] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5543 (0.6113)  acc1: 81.2500 (80.8979)  acc5: 100.0000 (98.9877)  time: 0.0283  data: 0.0002  max mem: 5511
[22:56:19.141689] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5856 (0.6188)  acc1: 81.2500 (80.5556)  acc5: 100.0000 (98.9776)  time: 0.0285  data: 0.0002  max mem: 5511
[22:56:19.426104] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6167 (0.6176)  acc1: 79.6875 (80.7692)  acc5: 98.4375 (98.9526)  time: 0.0284  data: 0.0002  max mem: 5511
[22:56:19.717546] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.6129 (0.6218)  acc1: 79.6875 (80.5538)  acc5: 98.4375 (98.9325)  time: 0.0286  data: 0.0002  max mem: 5511
[22:56:20.008704] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6451 (0.6242)  acc1: 79.6875 (80.6306)  acc5: 98.4375 (98.9161)  time: 0.0290  data: 0.0003  max mem: 5511
[22:56:20.302802] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6217 (0.6230)  acc1: 78.1250 (80.4623)  acc5: 100.0000 (98.9669)  time: 0.0291  data: 0.0005  max mem: 5511
[22:56:20.593050] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6104 (0.6236)  acc1: 79.6875 (80.4986)  acc5: 100.0000 (98.9504)  time: 0.0291  data: 0.0004  max mem: 5511
[22:56:20.877451] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6264 (0.6237)  acc1: 79.6875 (80.4743)  acc5: 98.4375 (98.9251)  time: 0.0286  data: 0.0002  max mem: 5511
[22:56:21.157924] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6264 (0.6238)  acc1: 79.6875 (80.4325)  acc5: 98.4375 (98.9031)  time: 0.0281  data: 0.0001  max mem: 5511
[22:56:21.309491] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6083 (0.6244)  acc1: 78.1250 (80.3900)  acc5: 98.4375 (98.9100)  time: 0.0270  data: 0.0001  max mem: 5511
[22:56:21.479791] Test: Total time: 0:00:05 (0.0335 s / it)
[22:56:21.480949] * Acc@1 80.390 Acc@5 98.910 loss 0.624
[22:56:21.481337] Accuracy of the network on the 10000 test images: 80.4%
[22:56:21.481562] Max accuracy: 80.39%
[22:56:21.698920] log_dir: ./output_dir
[22:56:22.632319] Epoch: [46]  [  0/781]  eta: 0:12:07  lr: 0.000152  training_loss: 1.1729 (1.1729)  mae_loss: 0.0234 (0.0234)  classification_loss: 1.1479 (1.1479)  loss_mask: 0.0016 (0.0016)  time: 0.9316  data: 0.7201  max mem: 5511
[22:56:26.599757] Epoch: [46]  [ 20/781]  eta: 0:02:57  lr: 0.000152  training_loss: 1.3177 (1.3139)  mae_loss: 0.0277 (0.0275)  classification_loss: 1.2903 (1.2833)  loss_mask: 0.0017 (0.0031)  time: 0.1983  data: 0.0002  max mem: 5511
[22:56:30.555394] Epoch: [46]  [ 40/781]  eta: 0:02:39  lr: 0.000152  training_loss: 1.3528 (1.3371)  mae_loss: 0.0284 (0.0278)  classification_loss: 1.3202 (1.3050)  loss_mask: 0.0047 (0.0043)  time: 0.1977  data: 0.0002  max mem: 5511
[22:56:34.559597] Epoch: [46]  [ 60/781]  eta: 0:02:31  lr: 0.000152  training_loss: 1.4034 (1.3507)  mae_loss: 0.0268 (0.0275)  classification_loss: 1.3686 (1.3186)  loss_mask: 0.0030 (0.0045)  time: 0.2001  data: 0.0002  max mem: 5511
[22:56:38.512441] Epoch: [46]  [ 80/781]  eta: 0:02:25  lr: 0.000152  training_loss: 1.3334 (1.3476)  mae_loss: 0.0273 (0.0274)  classification_loss: 1.3011 (1.3157)  loss_mask: 0.0037 (0.0045)  time: 0.1976  data: 0.0002  max mem: 5511
[22:56:42.490695] Epoch: [46]  [100/781]  eta: 0:02:20  lr: 0.000152  training_loss: 1.3114 (1.3470)  mae_loss: 0.0276 (0.0273)  classification_loss: 1.2863 (1.3157)  loss_mask: 0.0018 (0.0040)  time: 0.1988  data: 0.0002  max mem: 5511
[22:56:46.422324] Epoch: [46]  [120/781]  eta: 0:02:14  lr: 0.000151  training_loss: 1.3677 (1.3478)  mae_loss: 0.0275 (0.0274)  classification_loss: 1.3391 (1.3168)  loss_mask: 0.0011 (0.0035)  time: 0.1965  data: 0.0002  max mem: 5511
[22:56:50.398439] Epoch: [46]  [140/781]  eta: 0:02:10  lr: 0.000151  training_loss: 1.3475 (1.3471)  mae_loss: 0.0278 (0.0275)  classification_loss: 1.3211 (1.3163)  loss_mask: 0.0014 (0.0033)  time: 0.1987  data: 0.0003  max mem: 5511
[22:56:54.336793] Epoch: [46]  [160/781]  eta: 0:02:05  lr: 0.000151  training_loss: 1.3473 (1.3468)  mae_loss: 0.0266 (0.0274)  classification_loss: 1.3170 (1.3162)  loss_mask: 0.0019 (0.0032)  time: 0.1968  data: 0.0002  max mem: 5511
[22:56:58.257865] Epoch: [46]  [180/781]  eta: 0:02:01  lr: 0.000151  training_loss: 1.3421 (1.3462)  mae_loss: 0.0281 (0.0275)  classification_loss: 1.3034 (1.3156)  loss_mask: 0.0020 (0.0031)  time: 0.1959  data: 0.0002  max mem: 5511
[22:57:02.188811] Epoch: [46]  [200/781]  eta: 0:01:56  lr: 0.000151  training_loss: 1.3589 (1.3476)  mae_loss: 0.0267 (0.0274)  classification_loss: 1.3274 (1.3170)  loss_mask: 0.0027 (0.0031)  time: 0.1965  data: 0.0002  max mem: 5511
[22:57:06.164198] Epoch: [46]  [220/781]  eta: 0:01:52  lr: 0.000151  training_loss: 1.3510 (1.3462)  mae_loss: 0.0268 (0.0274)  classification_loss: 1.3159 (1.3155)  loss_mask: 0.0054 (0.0033)  time: 0.1987  data: 0.0004  max mem: 5511
[22:57:10.169364] Epoch: [46]  [240/781]  eta: 0:01:48  lr: 0.000151  training_loss: 1.3700 (1.3488)  mae_loss: 0.0277 (0.0274)  classification_loss: 1.3416 (1.3181)  loss_mask: 0.0024 (0.0033)  time: 0.2002  data: 0.0002  max mem: 5511
[22:57:14.136148] Epoch: [46]  [260/781]  eta: 0:01:44  lr: 0.000151  training_loss: 1.3745 (1.3488)  mae_loss: 0.0275 (0.0274)  classification_loss: 1.3483 (1.3182)  loss_mask: 0.0013 (0.0031)  time: 0.1983  data: 0.0002  max mem: 5511
[22:57:18.086580] Epoch: [46]  [280/781]  eta: 0:01:40  lr: 0.000151  training_loss: 1.3574 (1.3483)  mae_loss: 0.0271 (0.0275)  classification_loss: 1.3287 (1.3178)  loss_mask: 0.0012 (0.0030)  time: 0.1974  data: 0.0002  max mem: 5511
[22:57:22.016195] Epoch: [46]  [300/781]  eta: 0:01:36  lr: 0.000151  training_loss: 1.3182 (1.3460)  mae_loss: 0.0280 (0.0275)  classification_loss: 1.2848 (1.3155)  loss_mask: 0.0011 (0.0029)  time: 0.1964  data: 0.0002  max mem: 5511
[22:57:25.926984] Epoch: [46]  [320/781]  eta: 0:01:32  lr: 0.000150  training_loss: 1.3346 (1.3459)  mae_loss: 0.0284 (0.0275)  classification_loss: 1.3035 (1.3155)  loss_mask: 0.0010 (0.0029)  time: 0.1955  data: 0.0003  max mem: 5511
[22:57:29.887722] Epoch: [46]  [340/781]  eta: 0:01:28  lr: 0.000150  training_loss: 1.3522 (1.3455)  mae_loss: 0.0283 (0.0276)  classification_loss: 1.3168 (1.3149)  loss_mask: 0.0029 (0.0030)  time: 0.1979  data: 0.0002  max mem: 5511
[22:57:33.815870] Epoch: [46]  [360/781]  eta: 0:01:24  lr: 0.000150  training_loss: 1.3916 (1.3485)  mae_loss: 0.0274 (0.0276)  classification_loss: 1.3527 (1.3172)  loss_mask: 0.0080 (0.0036)  time: 0.1963  data: 0.0002  max mem: 5511
[22:57:37.734143] Epoch: [46]  [380/781]  eta: 0:01:19  lr: 0.000150  training_loss: 1.3521 (1.3496)  mae_loss: 0.0272 (0.0276)  classification_loss: 1.3135 (1.3178)  loss_mask: 0.0104 (0.0041)  time: 0.1958  data: 0.0002  max mem: 5511
[22:57:41.681883] Epoch: [46]  [400/781]  eta: 0:01:15  lr: 0.000150  training_loss: 1.3438 (1.3506)  mae_loss: 0.0280 (0.0277)  classification_loss: 1.3059 (1.3185)  loss_mask: 0.0107 (0.0045)  time: 0.1973  data: 0.0003  max mem: 5511
[22:57:45.610619] Epoch: [46]  [420/781]  eta: 0:01:11  lr: 0.000150  training_loss: 1.3160 (1.3489)  mae_loss: 0.0279 (0.0277)  classification_loss: 1.2756 (1.3166)  loss_mask: 0.0069 (0.0047)  time: 0.1964  data: 0.0002  max mem: 5511
[22:57:49.540212] Epoch: [46]  [440/781]  eta: 0:01:07  lr: 0.000150  training_loss: 1.3663 (1.3492)  mae_loss: 0.0276 (0.0277)  classification_loss: 1.3231 (1.3164)  loss_mask: 0.0100 (0.0051)  time: 0.1964  data: 0.0002  max mem: 5511

[22:57:53.482721] Epoch: [46]  [460/781]  eta: 0:01:03  lr: 0.000150  training_loss: 1.3284 (1.3488)  mae_loss: 0.0267 (0.0277)  classification_loss: 1.2799 (1.3158)  loss_mask: 0.0083 (0.0052)  time: 0.1970  data: 0.0003  max mem: 5511
[22:57:57.425752] Epoch: [46]  [480/781]  eta: 0:00:59  lr: 0.000150  training_loss: 1.4122 (1.3513)  mae_loss: 0.0271 (0.0277)  classification_loss: 1.3776 (1.3184)  loss_mask: 0.0065 (0.0053)  time: 0.1971  data: 0.0002  max mem: 5511
[22:58:01.375170] Epoch: [46]  [500/781]  eta: 0:00:55  lr: 0.000149  training_loss: 1.3736 (1.3523)  mae_loss: 0.0272 (0.0276)  classification_loss: 1.3403 (1.3194)  loss_mask: 0.0043 (0.0053)  time: 0.1974  data: 0.0002  max mem: 5511
[22:58:05.410134] Epoch: [46]  [520/781]  eta: 0:00:51  lr: 0.000149  training_loss: 1.3299 (1.3526)  mae_loss: 0.0286 (0.0276)  classification_loss: 1.2994 (1.3198)  loss_mask: 0.0024 (0.0052)  time: 0.2016  data: 0.0003  max mem: 5511
[22:58:09.394874] Epoch: [46]  [540/781]  eta: 0:00:47  lr: 0.000149  training_loss: 1.3693 (1.3524)  mae_loss: 0.0274 (0.0277)  classification_loss: 1.3381 (1.3197)  loss_mask: 0.0015 (0.0050)  time: 0.1991  data: 0.0002  max mem: 5511
[22:58:13.339320] Epoch: [46]  [560/781]  eta: 0:00:43  lr: 0.000149  training_loss: 1.3142 (1.3512)  mae_loss: 0.0262 (0.0276)  classification_loss: 1.2867 (1.3185)  loss_mask: 0.0025 (0.0050)  time: 0.1971  data: 0.0002  max mem: 5511
[22:58:17.270890] Epoch: [46]  [580/781]  eta: 0:00:39  lr: 0.000149  training_loss: 1.3999 (1.3526)  mae_loss: 0.0277 (0.0276)  classification_loss: 1.3691 (1.3201)  loss_mask: 0.0022 (0.0049)  time: 0.1965  data: 0.0002  max mem: 5511
[22:58:21.198305] Epoch: [46]  [600/781]  eta: 0:00:35  lr: 0.000149  training_loss: 1.3434 (1.3521)  mae_loss: 0.0267 (0.0276)  classification_loss: 1.3159 (1.3196)  loss_mask: 0.0034 (0.0049)  time: 0.1963  data: 0.0002  max mem: 5511
[22:58:25.139725] Epoch: [46]  [620/781]  eta: 0:00:31  lr: 0.000149  training_loss: 1.3975 (1.3525)  mae_loss: 0.0261 (0.0276)  classification_loss: 1.3631 (1.3200)  loss_mask: 0.0059 (0.0049)  time: 0.1970  data: 0.0002  max mem: 5511
[22:58:29.084537] Epoch: [46]  [640/781]  eta: 0:00:28  lr: 0.000149  training_loss: 1.3740 (1.3531)  mae_loss: 0.0264 (0.0276)  classification_loss: 1.3438 (1.3205)  loss_mask: 0.0046 (0.0049)  time: 0.1972  data: 0.0003  max mem: 5511
[22:58:33.030951] Epoch: [46]  [660/781]  eta: 0:00:24  lr: 0.000149  training_loss: 1.3469 (1.3530)  mae_loss: 0.0270 (0.0276)  classification_loss: 1.3149 (1.3205)  loss_mask: 0.0024 (0.0049)  time: 0.1972  data: 0.0002  max mem: 5511
[22:58:37.003632] Epoch: [46]  [680/781]  eta: 0:00:20  lr: 0.000149  training_loss: 1.3544 (1.3534)  mae_loss: 0.0266 (0.0276)  classification_loss: 1.3277 (1.3210)  loss_mask: 0.0023 (0.0048)  time: 0.1985  data: 0.0002  max mem: 5511
[22:58:40.958343] Epoch: [46]  [700/781]  eta: 0:00:16  lr: 0.000148  training_loss: 1.3219 (1.3537)  mae_loss: 0.0272 (0.0276)  classification_loss: 1.2914 (1.3214)  loss_mask: 0.0014 (0.0047)  time: 0.1976  data: 0.0002  max mem: 5511
[22:58:44.903762] Epoch: [46]  [720/781]  eta: 0:00:12  lr: 0.000148  training_loss: 1.3746 (1.3547)  mae_loss: 0.0278 (0.0276)  classification_loss: 1.3482 (1.3225)  loss_mask: 0.0012 (0.0046)  time: 0.1972  data: 0.0005  max mem: 5511
[22:58:48.842067] Epoch: [46]  [740/781]  eta: 0:00:08  lr: 0.000148  training_loss: 1.3328 (1.3546)  mae_loss: 0.0270 (0.0276)  classification_loss: 1.3045 (1.3224)  loss_mask: 0.0015 (0.0046)  time: 0.1968  data: 0.0002  max mem: 5511
[22:58:52.783496] Epoch: [46]  [760/781]  eta: 0:00:04  lr: 0.000148  training_loss: 1.3655 (1.3554)  mae_loss: 0.0262 (0.0276)  classification_loss: 1.3350 (1.3233)  loss_mask: 0.0018 (0.0045)  time: 0.1970  data: 0.0002  max mem: 5511
[22:58:56.711057] Epoch: [46]  [780/781]  eta: 0:00:00  lr: 0.000148  training_loss: 1.3754 (1.3565)  mae_loss: 0.0274 (0.0276)  classification_loss: 1.3475 (1.3245)  loss_mask: 0.0020 (0.0045)  time: 0.1963  data: 0.0002  max mem: 5511
[22:58:56.862114] Epoch: [46] Total time: 0:02:35 (0.1987 s / it)
[22:58:56.862627] Averaged stats: lr: 0.000148  training_loss: 1.3754 (1.3565)  mae_loss: 0.0274 (0.0276)  classification_loss: 1.3475 (1.3245)  loss_mask: 0.0020 (0.0045)
[22:58:57.525166] Test:  [  0/157]  eta: 0:01:43  testing_loss: 0.6155 (0.6155)  acc1: 82.8125 (82.8125)  acc5: 98.4375 (98.4375)  time: 0.6577  data: 0.6179  max mem: 5511
[22:58:57.814556] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.6264 (0.6508)  acc1: 79.6875 (78.6932)  acc5: 98.4375 (99.0057)  time: 0.0857  data: 0.0564  max mem: 5511
[22:58:58.103955] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.6219 (0.6162)  acc1: 79.6875 (80.6548)  acc5: 98.4375 (98.8095)  time: 0.0287  data: 0.0002  max mem: 5511
[22:58:58.392927] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5982 (0.6283)  acc1: 81.2500 (80.4940)  acc5: 98.4375 (98.7399)  time: 0.0288  data: 0.0002  max mem: 5511
[22:58:58.678092] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6135 (0.6293)  acc1: 79.6875 (80.7546)  acc5: 98.4375 (98.7424)  time: 0.0286  data: 0.0002  max mem: 5511
[22:58:58.964938] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5911 (0.6205)  acc1: 82.8125 (81.1581)  acc5: 98.4375 (98.6826)  time: 0.0284  data: 0.0002  max mem: 5511
[22:58:59.249773] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5993 (0.6197)  acc1: 81.2500 (81.1475)  acc5: 98.4375 (98.7449)  time: 0.0284  data: 0.0002  max mem: 5511
[22:58:59.535874] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5697 (0.6096)  acc1: 82.8125 (81.5361)  acc5: 100.0000 (98.7896)  time: 0.0284  data: 0.0002  max mem: 5511
[22:58:59.821367] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5871 (0.6203)  acc1: 82.8125 (81.0571)  acc5: 98.4375 (98.7269)  time: 0.0284  data: 0.0002  max mem: 5511
[22:59:00.108569] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6553 (0.6195)  acc1: 78.1250 (81.2157)  acc5: 98.4375 (98.7294)  time: 0.0285  data: 0.0002  max mem: 5511
[22:59:00.398731] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.6182 (0.6217)  acc1: 79.6875 (81.0025)  acc5: 98.4375 (98.7005)  time: 0.0287  data: 0.0002  max mem: 5511
[22:59:00.686640] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6409 (0.6233)  acc1: 79.6875 (80.8559)  acc5: 98.4375 (98.6909)  time: 0.0287  data: 0.0002  max mem: 5511
[22:59:00.977636] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5955 (0.6202)  acc1: 79.6875 (80.8755)  acc5: 98.4375 (98.7345)  time: 0.0287  data: 0.0002  max mem: 5511
[22:59:01.264113] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6312 (0.6222)  acc1: 81.2500 (80.8087)  acc5: 98.4375 (98.7238)  time: 0.0287  data: 0.0002  max mem: 5511
[22:59:01.546109] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6174 (0.6197)  acc1: 81.2500 (80.9176)  acc5: 98.4375 (98.7699)  time: 0.0282  data: 0.0002  max mem: 5511
[22:59:01.827541] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6017 (0.6179)  acc1: 81.2500 (80.9706)  acc5: 98.4375 (98.7790)  time: 0.0280  data: 0.0001  max mem: 5511
[22:59:01.980798] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5598 (0.6167)  acc1: 81.2500 (80.9900)  acc5: 98.4375 (98.7800)  time: 0.0271  data: 0.0001  max mem: 5511
[22:59:02.126101] Test: Total time: 0:00:05 (0.0335 s / it)
[22:59:02.126594] * Acc@1 80.990 Acc@5 98.780 loss 0.617
[22:59:02.126885] Accuracy of the network on the 10000 test images: 81.0%
[22:59:02.127063] Max accuracy: 80.99%
[22:59:02.337977] log_dir: ./output_dir
[22:59:03.208746] Epoch: [47]  [  0/781]  eta: 0:11:18  lr: 0.000148  training_loss: 1.1999 (1.1999)  mae_loss: 0.0277 (0.0277)  classification_loss: 1.1708 (1.1708)  loss_mask: 0.0014 (0.0014)  time: 0.8689  data: 0.6432  max mem: 5511
[22:59:07.159760] Epoch: [47]  [ 20/781]  eta: 0:02:54  lr: 0.000148  training_loss: 1.3228 (1.3566)  mae_loss: 0.0265 (0.0270)  classification_loss: 1.2877 (1.3206)  loss_mask: 0.0079 (0.0090)  time: 0.1975  data: 0.0003  max mem: 5511
[22:59:11.083943] Epoch: [47]  [ 40/781]  eta: 0:02:37  lr: 0.000148  training_loss: 1.3725 (1.3715)  mae_loss: 0.0271 (0.0271)  classification_loss: 1.3340 (1.3357)  loss_mask: 0.0061 (0.0086)  time: 0.1961  data: 0.0002  max mem: 5511
[22:59:15.035690] Epoch: [47]  [ 60/781]  eta: 0:02:29  lr: 0.000148  training_loss: 1.3502 (1.3713)  mae_loss: 0.0281 (0.0275)  classification_loss: 1.3218 (1.3370)  loss_mask: 0.0022 (0.0067)  time: 0.1975  data: 0.0002  max mem: 5511
[22:59:18.982592] Epoch: [47]  [ 80/781]  eta: 0:02:23  lr: 0.000148  training_loss: 1.3284 (1.3619)  mae_loss: 0.0267 (0.0274)  classification_loss: 1.2984 (1.3289)  loss_mask: 0.0019 (0.0056)  time: 0.1973  data: 0.0002  max mem: 5511
[22:59:22.931112] Epoch: [47]  [100/781]  eta: 0:02:18  lr: 0.000148  training_loss: 1.3834 (1.3645)  mae_loss: 0.0259 (0.0274)  classification_loss: 1.3470 (1.3314)  loss_mask: 0.0032 (0.0057)  time: 0.1973  data: 0.0006  max mem: 5511
[22:59:26.866412] Epoch: [47]  [120/781]  eta: 0:02:13  lr: 0.000147  training_loss: 1.3277 (1.3582)  mae_loss: 0.0265 (0.0274)  classification_loss: 1.2993 (1.3253)  loss_mask: 0.0028 (0.0055)  time: 0.1967  data: 0.0002  max mem: 5511
[22:59:30.800876] Epoch: [47]  [140/781]  eta: 0:02:09  lr: 0.000147  training_loss: 1.3476 (1.3519)  mae_loss: 0.0275 (0.0274)  classification_loss: 1.3145 (1.3187)  loss_mask: 0.0077 (0.0058)  time: 0.1966  data: 0.0002  max mem: 5511
[22:59:34.726082] Epoch: [47]  [160/781]  eta: 0:02:04  lr: 0.000147  training_loss: 1.3573 (1.3520)  mae_loss: 0.0265 (0.0273)  classification_loss: 1.3220 (1.3191)  loss_mask: 0.0023 (0.0056)  time: 0.1962  data: 0.0002  max mem: 5511
[22:59:38.657337] Epoch: [47]  [180/781]  eta: 0:02:00  lr: 0.000147  training_loss: 1.3132 (1.3519)  mae_loss: 0.0273 (0.0273)  classification_loss: 1.2825 (1.3194)  loss_mask: 0.0020 (0.0052)  time: 0.1965  data: 0.0003  max mem: 5511
[22:59:42.579809] Epoch: [47]  [200/781]  eta: 0:01:56  lr: 0.000147  training_loss: 1.3153 (1.3497)  mae_loss: 0.0267 (0.0272)  classification_loss: 1.2893 (1.3176)  loss_mask: 0.0012 (0.0048)  time: 0.1960  data: 0.0002  max mem: 5511
[22:59:46.518870] Epoch: [47]  [220/781]  eta: 0:01:52  lr: 0.000147  training_loss: 1.3839 (1.3526)  mae_loss: 0.0264 (0.0272)  classification_loss: 1.3562 (1.3208)  loss_mask: 0.0013 (0.0045)  time: 0.1969  data: 0.0002  max mem: 5511
[22:59:50.509564] Epoch: [47]  [240/781]  eta: 0:01:48  lr: 0.000147  training_loss: 1.3548 (1.3541)  mae_loss: 0.0276 (0.0272)  classification_loss: 1.3241 (1.3225)  loss_mask: 0.0016 (0.0043)  time: 0.1995  data: 0.0003  max mem: 5511
[22:59:54.444488] Epoch: [47]  [260/781]  eta: 0:01:43  lr: 0.000147  training_loss: 1.3416 (1.3543)  mae_loss: 0.0280 (0.0273)  classification_loss: 1.3121 (1.3229)  loss_mask: 0.0015 (0.0041)  time: 0.1967  data: 0.0002  max mem: 5511
[22:59:58.408070] Epoch: [47]  [280/781]  eta: 0:01:39  lr: 0.000147  training_loss: 1.3176 (1.3517)  mae_loss: 0.0284 (0.0274)  classification_loss: 1.2894 (1.3204)  loss_mask: 0.0013 (0.0039)  time: 0.1981  data: 0.0002  max mem: 5511
[23:00:02.356974] Epoch: [47]  [300/781]  eta: 0:01:35  lr: 0.000146  training_loss: 1.3765 (1.3547)  mae_loss: 0.0287 (0.0274)  classification_loss: 1.3437 (1.3235)  loss_mask: 0.0011 (0.0037)  time: 0.1974  data: 0.0002  max mem: 5511
[23:00:06.274618] Epoch: [47]  [320/781]  eta: 0:01:31  lr: 0.000146  training_loss: 1.2687 (1.3515)  mae_loss: 0.0278 (0.0275)  classification_loss: 1.2434 (1.3204)  loss_mask: 0.0011 (0.0036)  time: 0.1958  data: 0.0002  max mem: 5511
[23:00:10.208830] Epoch: [47]  [340/781]  eta: 0:01:27  lr: 0.000146  training_loss: 1.3773 (1.3513)  mae_loss: 0.0270 (0.0275)  classification_loss: 1.3540 (1.3204)  loss_mask: 0.0014 (0.0035)  time: 0.1966  data: 0.0003  max mem: 5511
[23:00:14.149818] Epoch: [47]  [360/781]  eta: 0:01:23  lr: 0.000146  training_loss: 1.3769 (1.3528)  mae_loss: 0.0274 (0.0274)  classification_loss: 1.3525 (1.3220)  loss_mask: 0.0013 (0.0034)  time: 0.1970  data: 0.0002  max mem: 5511
[23:00:18.097817] Epoch: [47]  [380/781]  eta: 0:01:19  lr: 0.000146  training_loss: 1.3535 (1.3520)  mae_loss: 0.0281 (0.0275)  classification_loss: 1.3258 (1.3212)  loss_mask: 0.0010 (0.0033)  time: 0.1973  data: 0.0002  max mem: 5511
[23:00:22.037533] Epoch: [47]  [400/781]  eta: 0:01:15  lr: 0.000146  training_loss: 1.2775 (1.3504)  mae_loss: 0.0273 (0.0275)  classification_loss: 1.2488 (1.3197)  loss_mask: 0.0009 (0.0032)  time: 0.1969  data: 0.0002  max mem: 5511
[23:00:25.994530] Epoch: [47]  [420/781]  eta: 0:01:11  lr: 0.000146  training_loss: 1.3633 (1.3509)  mae_loss: 0.0272 (0.0275)  classification_loss: 1.3399 (1.3202)  loss_mask: 0.0010 (0.0031)  time: 0.1977  data: 0.0003  max mem: 5511
[23:00:29.979293] Epoch: [47]  [440/781]  eta: 0:01:07  lr: 0.000146  training_loss: 1.3243 (1.3486)  mae_loss: 0.0268 (0.0275)  classification_loss: 1.2992 (1.3181)  loss_mask: 0.0009 (0.0030)  time: 0.1991  data: 0.0003  max mem: 5511
[23:00:33.913264] Epoch: [47]  [460/781]  eta: 0:01:03  lr: 0.000146  training_loss: 1.3331 (1.3468)  mae_loss: 0.0260 (0.0275)  classification_loss: 1.3072 (1.3163)  loss_mask: 0.0011 (0.0030)  time: 0.1966  data: 0.0002  max mem: 5511
[23:00:37.839572] Epoch: [47]  [480/781]  eta: 0:00:59  lr: 0.000146  training_loss: 1.3685 (1.3478)  mae_loss: 0.0267 (0.0275)  classification_loss: 1.3422 (1.3174)  loss_mask: 0.0009 (0.0029)  time: 0.1962  data: 0.0002  max mem: 5511
[23:00:41.794419] Epoch: [47]  [500/781]  eta: 0:00:55  lr: 0.000145  training_loss: 1.3620 (1.3482)  mae_loss: 0.0268 (0.0275)  classification_loss: 1.3307 (1.3178)  loss_mask: 0.0015 (0.0029)  time: 0.1977  data: 0.0002  max mem: 5511
[23:00:45.714328] Epoch: [47]  [520/781]  eta: 0:00:51  lr: 0.000145  training_loss: 1.3408 (1.3484)  mae_loss: 0.0264 (0.0275)  classification_loss: 1.3113 (1.3181)  loss_mask: 0.0015 (0.0028)  time: 0.1959  data: 0.0003  max mem: 5511
[23:00:49.617459] Epoch: [47]  [540/781]  eta: 0:00:47  lr: 0.000145  training_loss: 1.3391 (1.3489)  mae_loss: 0.0284 (0.0275)  classification_loss: 1.3085 (1.3186)  loss_mask: 0.0011 (0.0028)  time: 0.1951  data: 0.0002  max mem: 5511
[23:00:53.549752] Epoch: [47]  [560/781]  eta: 0:00:43  lr: 0.000145  training_loss: 1.3770 (1.3500)  mae_loss: 0.0258 (0.0275)  classification_loss: 1.3506 (1.3198)  loss_mask: 0.0008 (0.0027)  time: 0.1965  data: 0.0002  max mem: 5511
[23:00:57.451847] Epoch: [47]  [580/781]  eta: 0:00:39  lr: 0.000145  training_loss: 1.3711 (1.3503)  mae_loss: 0.0273 (0.0275)  classification_loss: 1.3366 (1.3202)  loss_mask: 0.0011 (0.0027)  time: 0.1950  data: 0.0002  max mem: 5511
[23:01:01.367163] Epoch: [47]  [600/781]  eta: 0:00:35  lr: 0.000145  training_loss: 1.3130 (1.3498)  mae_loss: 0.0267 (0.0275)  classification_loss: 1.2829 (1.3197)  loss_mask: 0.0009 (0.0026)  time: 0.1957  data: 0.0001  max mem: 5511
[23:01:05.325631] Epoch: [47]  [620/781]  eta: 0:00:31  lr: 0.000145  training_loss: 1.3362 (1.3490)  mae_loss: 0.0290 (0.0275)  classification_loss: 1.3033 (1.3189)  loss_mask: 0.0016 (0.0026)  time: 0.1978  data: 0.0002  max mem: 5511
[23:01:09.286300] Epoch: [47]  [640/781]  eta: 0:00:27  lr: 0.000145  training_loss: 1.3545 (1.3492)  mae_loss: 0.0260 (0.0275)  classification_loss: 1.3183 (1.3191)  loss_mask: 0.0024 (0.0027)  time: 0.1979  data: 0.0003  max mem: 5511
[23:01:13.210892] Epoch: [47]  [660/781]  eta: 0:00:23  lr: 0.000145  training_loss: 1.2748 (1.3486)  mae_loss: 0.0267 (0.0275)  classification_loss: 1.2372 (1.3183)  loss_mask: 0.0064 (0.0029)  time: 0.1961  data: 0.0002  max mem: 5511
[23:01:17.202951] Epoch: [47]  [680/781]  eta: 0:00:19  lr: 0.000144  training_loss: 1.3571 (1.3494)  mae_loss: 0.0270 (0.0275)  classification_loss: 1.3129 (1.3184)  loss_mask: 0.0113 (0.0035)  time: 0.1995  data: 0.0002  max mem: 5511
[23:01:21.130091] Epoch: [47]  [700/781]  eta: 0:00:16  lr: 0.000144  training_loss: 1.3455 (1.3507)  mae_loss: 0.0283 (0.0275)  classification_loss: 1.3036 (1.3196)  loss_mask: 0.0058 (0.0036)  time: 0.1963  data: 0.0002  max mem: 5511
[23:01:25.062949] Epoch: [47]  [720/781]  eta: 0:00:12  lr: 0.000144  training_loss: 1.3689 (1.3511)  mae_loss: 0.0290 (0.0276)  classification_loss: 1.3308 (1.3198)  loss_mask: 0.0054 (0.0037)  time: 0.1966  data: 0.0002  max mem: 5511
[23:01:29.000575] Epoch: [47]  [740/781]  eta: 0:00:08  lr: 0.000144  training_loss: 1.3256 (1.3512)  mae_loss: 0.0272 (0.0276)  classification_loss: 1.2946 (1.3198)  loss_mask: 0.0049 (0.0038)  time: 0.1967  data: 0.0002  max mem: 5511
[23:01:32.906739] Epoch: [47]  [760/781]  eta: 0:00:04  lr: 0.000144  training_loss: 1.3526 (1.3510)  mae_loss: 0.0265 (0.0275)  classification_loss: 1.3214 (1.3197)  loss_mask: 0.0030 (0.0037)  time: 0.1952  data: 0.0002  max mem: 5511
[23:01:36.815140] Epoch: [47]  [780/781]  eta: 0:00:00  lr: 0.000144  training_loss: 1.3262 (1.3507)  mae_loss: 0.0266 (0.0275)  classification_loss: 1.2969 (1.3195)  loss_mask: 0.0015 (0.0037)  time: 0.1953  data: 0.0002  max mem: 5511
[23:01:36.977875] Epoch: [47] Total time: 0:02:34 (0.1980 s / it)
[23:01:36.978474] Averaged stats: lr: 0.000144  training_loss: 1.3262 (1.3507)  mae_loss: 0.0266 (0.0275)  classification_loss: 1.2969 (1.3195)  loss_mask: 0.0015 (0.0037)
[23:01:37.636376] Test:  [  0/157]  eta: 0:01:42  testing_loss: 0.5789 (0.5789)  acc1: 82.8125 (82.8125)  acc5: 100.0000 (100.0000)  time: 0.6532  data: 0.6230  max mem: 5511
[23:01:37.922065] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.6944 (0.6663)  acc1: 78.1250 (78.8352)  acc5: 98.4375 (98.4375)  time: 0.0852  data: 0.0568  max mem: 5511
[23:01:38.211223] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.6729 (0.6471)  acc1: 79.6875 (79.3899)  acc5: 98.4375 (98.3631)  time: 0.0286  data: 0.0002  max mem: 5511
[23:01:38.505437] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6383 (0.6533)  acc1: 79.6875 (79.2339)  acc5: 98.4375 (98.2863)  time: 0.0290  data: 0.0002  max mem: 5511
[23:01:38.790528] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6349 (0.6508)  acc1: 81.2500 (79.7256)  acc5: 98.4375 (98.3994)  time: 0.0288  data: 0.0002  max mem: 5511
[23:01:39.077186] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5853 (0.6339)  acc1: 81.2500 (80.2696)  acc5: 98.4375 (98.4681)  time: 0.0285  data: 0.0002  max mem: 5511
[23:01:39.361437] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5940 (0.6347)  acc1: 79.6875 (79.9949)  acc5: 98.4375 (98.4631)  time: 0.0284  data: 0.0002  max mem: 5511
[23:01:39.643913] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5710 (0.6229)  acc1: 81.2500 (80.3697)  acc5: 100.0000 (98.6136)  time: 0.0282  data: 0.0002  max mem: 5511
[23:01:39.929805] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5783 (0.6319)  acc1: 81.2500 (80.0540)  acc5: 100.0000 (98.5532)  time: 0.0283  data: 0.0002  max mem: 5511
[23:01:40.212709] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6006 (0.6264)  acc1: 79.6875 (80.3915)  acc5: 98.4375 (98.6264)  time: 0.0283  data: 0.0002  max mem: 5511
[23:01:40.498171] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.6217 (0.6300)  acc1: 79.6875 (80.2754)  acc5: 100.0000 (98.6386)  time: 0.0283  data: 0.0002  max mem: 5511
[23:01:40.797989] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6493 (0.6300)  acc1: 78.1250 (80.2224)  acc5: 100.0000 (98.6346)  time: 0.0291  data: 0.0002  max mem: 5511
[23:01:41.083060] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6049 (0.6275)  acc1: 78.1250 (80.2557)  acc5: 98.4375 (98.6570)  time: 0.0291  data: 0.0002  max mem: 5511
[23:01:41.365993] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5914 (0.6261)  acc1: 78.1250 (80.2362)  acc5: 98.4375 (98.6641)  time: 0.0283  data: 0.0002  max mem: 5511
[23:01:41.649484] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5935 (0.6269)  acc1: 79.6875 (80.1640)  acc5: 98.4375 (98.6480)  time: 0.0281  data: 0.0002  max mem: 5511
[23:01:41.932411] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5978 (0.6257)  acc1: 81.2500 (80.2152)  acc5: 98.4375 (98.6548)  time: 0.0281  data: 0.0001  max mem: 5511
[23:01:42.085128] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5881 (0.6242)  acc1: 81.2500 (80.2600)  acc5: 98.4375 (98.6700)  time: 0.0272  data: 0.0001  max mem: 5511
[23:01:42.234086] Test: Total time: 0:00:05 (0.0335 s / it)
[23:01:42.234550] * Acc@1 80.260 Acc@5 98.670 loss 0.624
[23:01:42.234854] Accuracy of the network on the 10000 test images: 80.3%
[23:01:42.235028] Max accuracy: 80.99%
[23:01:42.459268] log_dir: ./output_dir
[23:01:43.344335] Epoch: [48]  [  0/781]  eta: 0:11:29  lr: 0.000144  training_loss: 1.3450 (1.3450)  mae_loss: 0.0264 (0.0264)  classification_loss: 1.3161 (1.3161)  loss_mask: 0.0025 (0.0025)  time: 0.8830  data: 0.6679  max mem: 5511
[23:01:47.289540] Epoch: [48]  [ 20/781]  eta: 0:02:54  lr: 0.000144  training_loss: 1.3440 (1.3415)  mae_loss: 0.0288 (0.0287)  classification_loss: 1.3105 (1.3099)  loss_mask: 0.0021 (0.0029)  time: 0.1972  data: 0.0002  max mem: 5511
[23:01:51.245714] Epoch: [48]  [ 40/781]  eta: 0:02:38  lr: 0.000144  training_loss: 1.4086 (1.3547)  mae_loss: 0.0264 (0.0277)  classification_loss: 1.3833 (1.3235)  loss_mask: 0.0031 (0.0035)  time: 0.1977  data: 0.0002  max mem: 5511
[23:01:55.174132] Epoch: [48]  [ 60/781]  eta: 0:02:30  lr: 0.000144  training_loss: 1.3757 (1.3602)  mae_loss: 0.0269 (0.0276)  classification_loss: 1.3354 (1.3271)  loss_mask: 0.0057 (0.0055)  time: 0.1963  data: 0.0002  max mem: 5511
[23:01:59.134249] Epoch: [48]  [ 80/781]  eta: 0:02:24  lr: 0.000144  training_loss: 1.3355 (1.3572)  mae_loss: 0.0263 (0.0273)  classification_loss: 1.2852 (1.3224)  loss_mask: 0.0079 (0.0074)  time: 0.1979  data: 0.0002  max mem: 5511
[23:02:03.088561] Epoch: [48]  [100/781]  eta: 0:02:19  lr: 0.000143  training_loss: 1.3426 (1.3551)  mae_loss: 0.0277 (0.0274)  classification_loss: 1.3127 (1.3202)  loss_mask: 0.0076 (0.0075)  time: 0.1976  data: 0.0003  max mem: 5511
[23:02:07.020681] Epoch: [48]  [120/781]  eta: 0:02:14  lr: 0.000143  training_loss: 1.3229 (1.3486)  mae_loss: 0.0277 (0.0275)  classification_loss: 1.2873 (1.3139)  loss_mask: 0.0050 (0.0072)  time: 0.1965  data: 0.0002  max mem: 5511
[23:02:10.957324] Epoch: [48]  [140/781]  eta: 0:02:09  lr: 0.000143  training_loss: 1.2644 (1.3410)  mae_loss: 0.0268 (0.0275)  classification_loss: 1.2348 (1.3069)  loss_mask: 0.0024 (0.0066)  time: 0.1967  data: 0.0002  max mem: 5511
[23:02:14.911478] Epoch: [48]  [160/781]  eta: 0:02:05  lr: 0.000143  training_loss: 1.3127 (1.3373)  mae_loss: 0.0263 (0.0274)  classification_loss: 1.2812 (1.3038)  loss_mask: 0.0018 (0.0061)  time: 0.1976  data: 0.0002  max mem: 5511
[23:02:18.867451] Epoch: [48]  [180/781]  eta: 0:02:00  lr: 0.000143  training_loss: 1.3152 (1.3394)  mae_loss: 0.0268 (0.0273)  classification_loss: 1.2875 (1.3065)  loss_mask: 0.0018 (0.0056)  time: 0.1977  data: 0.0003  max mem: 5511
[23:02:22.812013] Epoch: [48]  [200/781]  eta: 0:01:56  lr: 0.000143  training_loss: 1.3321 (1.3398)  mae_loss: 0.0262 (0.0274)  classification_loss: 1.2995 (1.3072)  loss_mask: 0.0014 (0.0052)  time: 0.1971  data: 0.0003  max mem: 5511
[23:02:26.752132] Epoch: [48]  [220/781]  eta: 0:01:52  lr: 0.000143  training_loss: 1.3172 (1.3397)  mae_loss: 0.0251 (0.0272)  classification_loss: 1.2890 (1.3075)  loss_mask: 0.0011 (0.0049)  time: 0.1969  data: 0.0002  max mem: 5511
[23:02:30.678832] Epoch: [48]  [240/781]  eta: 0:01:48  lr: 0.000143  training_loss: 1.2961 (1.3381)  mae_loss: 0.0267 (0.0272)  classification_loss: 1.2682 (1.3062)  loss_mask: 0.0020 (0.0047)  time: 0.1963  data: 0.0003  max mem: 5511
[23:02:34.612565] Epoch: [48]  [260/781]  eta: 0:01:44  lr: 0.000143  training_loss: 1.3574 (1.3386)  mae_loss: 0.0260 (0.0272)  classification_loss: 1.3281 (1.3070)  loss_mask: 0.0017 (0.0045)  time: 0.1966  data: 0.0002  max mem: 5511
[23:02:38.572681] Epoch: [48]  [280/781]  eta: 0:01:39  lr: 0.000142  training_loss: 1.3138 (1.3383)  mae_loss: 0.0293 (0.0273)  classification_loss: 1.2816 (1.3067)  loss_mask: 0.0019 (0.0043)  time: 0.1979  data: 0.0002  max mem: 5511
[23:02:42.497994] Epoch: [48]  [300/781]  eta: 0:01:35  lr: 0.000142  training_loss: 1.3360 (1.3387)  mae_loss: 0.0269 (0.0273)  classification_loss: 1.3085 (1.3073)  loss_mask: 0.0013 (0.0041)  time: 0.1962  data: 0.0002  max mem: 5511
[23:02:46.419553] Epoch: [48]  [320/781]  eta: 0:01:31  lr: 0.000142  training_loss: 1.3111 (1.3379)  mae_loss: 0.0264 (0.0272)  classification_loss: 1.2871 (1.3067)  loss_mask: 0.0010 (0.0040)  time: 0.1960  data: 0.0002  max mem: 5511
[23:02:50.371815] Epoch: [48]  [340/781]  eta: 0:01:27  lr: 0.000142  training_loss: 1.3677 (1.3397)  mae_loss: 0.0275 (0.0273)  classification_loss: 1.3350 (1.3086)  loss_mask: 0.0013 (0.0038)  time: 0.1975  data: 0.0002  max mem: 5511
[23:02:54.324372] Epoch: [48]  [360/781]  eta: 0:01:23  lr: 0.000142  training_loss: 1.3393 (1.3402)  mae_loss: 0.0267 (0.0273)  classification_loss: 1.3108 (1.3092)  loss_mask: 0.0012 (0.0037)  time: 0.1975  data: 0.0002  max mem: 5511
[23:02:58.282107] Epoch: [48]  [380/781]  eta: 0:01:19  lr: 0.000142  training_loss: 1.3385 (1.3396)  mae_loss: 0.0272 (0.0273)  classification_loss: 1.3140 (1.3088)  loss_mask: 0.0009 (0.0036)  time: 0.1978  data: 0.0002  max mem: 5511
[23:03:02.226079] Epoch: [48]  [400/781]  eta: 0:01:15  lr: 0.000142  training_loss: 1.3332 (1.3388)  mae_loss: 0.0276 (0.0273)  classification_loss: 1.3052 (1.3081)  loss_mask: 0.0008 (0.0034)  time: 0.1971  data: 0.0002  max mem: 5511
[23:03:06.176875] Epoch: [48]  [420/781]  eta: 0:01:11  lr: 0.000142  training_loss: 1.3643 (1.3409)  mae_loss: 0.0270 (0.0273)  classification_loss: 1.3345 (1.3103)  loss_mask: 0.0012 (0.0033)  time: 0.1974  data: 0.0002  max mem: 5511
[23:03:10.135301] Epoch: [48]  [440/781]  eta: 0:01:07  lr: 0.000142  training_loss: 1.3092 (1.3404)  mae_loss: 0.0276 (0.0273)  classification_loss: 1.2824 (1.3098)  loss_mask: 0.0018 (0.0033)  time: 0.1979  data: 0.0003  max mem: 5511
[23:03:14.053041] Epoch: [48]  [460/781]  eta: 0:01:03  lr: 0.000142  training_loss: 1.2636 (1.3382)  mae_loss: 0.0278 (0.0273)  classification_loss: 1.2349 (1.3076)  loss_mask: 0.0012 (0.0032)  time: 0.1958  data: 0.0002  max mem: 5511
[23:03:17.998697] Epoch: [48]  [480/781]  eta: 0:00:59  lr: 0.000141  training_loss: 1.3559 (1.3394)  mae_loss: 0.0259 (0.0273)  classification_loss: 1.3318 (1.3089)  loss_mask: 0.0016 (0.0032)  time: 0.1972  data: 0.0004  max mem: 5511
[23:03:21.913335] Epoch: [48]  [500/781]  eta: 0:00:55  lr: 0.000141  training_loss: 1.3278 (1.3400)  mae_loss: 0.0275 (0.0273)  classification_loss: 1.2928 (1.3093)  loss_mask: 0.0053 (0.0034)  time: 0.1956  data: 0.0002  max mem: 5511
[23:03:25.900093] Epoch: [48]  [520/781]  eta: 0:00:51  lr: 0.000141  training_loss: 1.3614 (1.3410)  mae_loss: 0.0283 (0.0273)  classification_loss: 1.3225 (1.3102)  loss_mask: 0.0035 (0.0034)  time: 0.1992  data: 0.0002  max mem: 5511
[23:03:29.850024] Epoch: [48]  [540/781]  eta: 0:00:47  lr: 0.000141  training_loss: 1.3449 (1.3414)  mae_loss: 0.0288 (0.0274)  classification_loss: 1.3028 (1.3106)  loss_mask: 0.0014 (0.0034)  time: 0.1974  data: 0.0002  max mem: 5511
[23:03:33.780121] Epoch: [48]  [560/781]  eta: 0:00:43  lr: 0.000141  training_loss: 1.3339 (1.3409)  mae_loss: 0.0270 (0.0274)  classification_loss: 1.3051 (1.3101)  loss_mask: 0.0012 (0.0033)  time: 0.1964  data: 0.0002  max mem: 5511
[23:03:37.707011] Epoch: [48]  [580/781]  eta: 0:00:39  lr: 0.000141  training_loss: 1.3399 (1.3411)  mae_loss: 0.0276 (0.0275)  classification_loss: 1.3084 (1.3104)  loss_mask: 0.0012 (0.0033)  time: 0.1963  data: 0.0002  max mem: 5511
[23:03:41.652582] Epoch: [48]  [600/781]  eta: 0:00:35  lr: 0.000141  training_loss: 1.3002 (1.3405)  mae_loss: 0.0246 (0.0274)  classification_loss: 1.2722 (1.3098)  loss_mask: 0.0025 (0.0033)  time: 0.1971  data: 0.0002  max mem: 5511
[23:03:45.577503] Epoch: [48]  [620/781]  eta: 0:00:31  lr: 0.000141  training_loss: 1.3412 (1.3413)  mae_loss: 0.0272 (0.0274)  classification_loss: 1.3051 (1.3105)  loss_mask: 0.0055 (0.0034)  time: 0.1962  data: 0.0002  max mem: 5511
[23:03:49.525656] Epoch: [48]  [640/781]  eta: 0:00:27  lr: 0.000141  training_loss: 1.3035 (1.3403)  mae_loss: 0.0265 (0.0274)  classification_loss: 1.2604 (1.3093)  loss_mask: 0.0058 (0.0036)  time: 0.1973  data: 0.0002  max mem: 5511
[23:03:53.472848] Epoch: [48]  [660/781]  eta: 0:00:23  lr: 0.000141  training_loss: 1.3246 (1.3403)  mae_loss: 0.0262 (0.0273)  classification_loss: 1.2891 (1.3092)  loss_mask: 0.0111 (0.0038)  time: 0.1973  data: 0.0002  max mem: 5511
[23:03:57.430308] Epoch: [48]  [680/781]  eta: 0:00:20  lr: 0.000140  training_loss: 1.3267 (1.3405)  mae_loss: 0.0272 (0.0273)  classification_loss: 1.2946 (1.3093)  loss_mask: 0.0028 (0.0038)  time: 0.1978  data: 0.0003  max mem: 5511
[23:04:01.379420] Epoch: [48]  [700/781]  eta: 0:00:16  lr: 0.000140  training_loss: 1.3049 (1.3402)  mae_loss: 0.0268 (0.0273)  classification_loss: 1.2759 (1.3092)  loss_mask: 0.0017 (0.0038)  time: 0.1974  data: 0.0003  max mem: 5511
[23:04:05.336919] Epoch: [48]  [720/781]  eta: 0:00:12  lr: 0.000140  training_loss: 1.3694 (1.3414)  mae_loss: 0.0271 (0.0273)  classification_loss: 1.3405 (1.3103)  loss_mask: 0.0015 (0.0037)  time: 0.1978  data: 0.0002  max mem: 5511
[23:04:09.265842] Epoch: [48]  [740/781]  eta: 0:00:08  lr: 0.000140  training_loss: 1.3274 (1.3410)  mae_loss: 0.0266 (0.0273)  classification_loss: 1.2965 (1.3100)  loss_mask: 0.0017 (0.0037)  time: 0.1963  data: 0.0002  max mem: 5511
[23:04:13.196368] Epoch: [48]  [760/781]  eta: 0:00:04  lr: 0.000140  training_loss: 1.3742 (1.3413)  mae_loss: 0.0257 (0.0273)  classification_loss: 1.3486 (1.3103)  loss_mask: 0.0014 (0.0036)  time: 0.1964  data: 0.0002  max mem: 5511
[23:04:17.118611] Epoch: [48]  [780/781]  eta: 0:00:00  lr: 0.000140  training_loss: 1.3464 (1.3418)  mae_loss: 0.0267 (0.0273)  classification_loss: 1.3156 (1.3109)  loss_mask: 0.0017 (0.0036)  time: 0.1960  data: 0.0004  max mem: 5511
[23:04:17.287986] Epoch: [48] Total time: 0:02:34 (0.1982 s / it)
[23:04:17.288761] Averaged stats: lr: 0.000140  training_loss: 1.3464 (1.3418)  mae_loss: 0.0267 (0.0273)  classification_loss: 1.3156 (1.3109)  loss_mask: 0.0017 (0.0036)
[23:04:17.954656] Test:  [  0/157]  eta: 0:01:43  testing_loss: 0.5644 (0.5644)  acc1: 79.6875 (79.6875)  acc5: 98.4375 (98.4375)  time: 0.6617  data: 0.6304  max mem: 5511
[23:04:18.249162] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.6456 (0.6624)  acc1: 79.6875 (78.4091)  acc5: 98.4375 (99.0057)  time: 0.0867  data: 0.0579  max mem: 5511
[23:04:18.538916] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.6325 (0.6065)  acc1: 81.2500 (81.1012)  acc5: 100.0000 (99.3304)  time: 0.0290  data: 0.0004  max mem: 5511
[23:04:18.827414] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5787 (0.6213)  acc1: 81.2500 (80.5444)  acc5: 100.0000 (99.0927)  time: 0.0288  data: 0.0002  max mem: 5511
[23:04:19.112630] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6388 (0.6237)  acc1: 79.6875 (80.6402)  acc5: 98.4375 (98.9329)  time: 0.0285  data: 0.0002  max mem: 5511
[23:04:19.396096] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5945 (0.6119)  acc1: 81.2500 (81.0968)  acc5: 98.4375 (98.9277)  time: 0.0283  data: 0.0002  max mem: 5511
[23:04:19.679649] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5823 (0.6096)  acc1: 81.2500 (80.9682)  acc5: 100.0000 (98.9498)  time: 0.0282  data: 0.0002  max mem: 5511
[23:04:19.967132] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5909 (0.6034)  acc1: 79.6875 (80.9859)  acc5: 100.0000 (99.0097)  time: 0.0284  data: 0.0002  max mem: 5511
[23:04:20.257442] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5938 (0.6085)  acc1: 78.1250 (80.6713)  acc5: 98.4375 (98.9390)  time: 0.0287  data: 0.0002  max mem: 5511
[23:04:20.545589] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5952 (0.6052)  acc1: 79.6875 (80.8723)  acc5: 98.4375 (98.9183)  time: 0.0287  data: 0.0002  max mem: 5511
[23:04:20.829476] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5952 (0.6057)  acc1: 81.2500 (80.8168)  acc5: 98.4375 (98.9325)  time: 0.0285  data: 0.0002  max mem: 5511
[23:04:21.118319] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6337 (0.6081)  acc1: 79.6875 (80.7714)  acc5: 100.0000 (98.9724)  time: 0.0285  data: 0.0002  max mem: 5511
[23:04:21.404416] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5758 (0.6043)  acc1: 79.6875 (80.8755)  acc5: 100.0000 (98.9928)  time: 0.0286  data: 0.0002  max mem: 5511
[23:04:21.702979] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5741 (0.6046)  acc1: 81.2500 (80.9399)  acc5: 98.4375 (98.9742)  time: 0.0290  data: 0.0002  max mem: 5511
[23:04:21.987830] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5953 (0.6036)  acc1: 81.2500 (80.9619)  acc5: 98.4375 (98.9694)  time: 0.0289  data: 0.0002  max mem: 5511
[23:04:22.269089] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5953 (0.6027)  acc1: 81.2500 (80.9913)  acc5: 98.4375 (98.9963)  time: 0.0281  data: 0.0001  max mem: 5511
[23:04:22.423357] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5797 (0.6014)  acc1: 82.8125 (81.0600)  acc5: 100.0000 (99.0100)  time: 0.0273  data: 0.0001  max mem: 5511
[23:04:22.580348] Test: Total time: 0:00:05 (0.0337 s / it)
[23:04:22.581094] * Acc@1 81.060 Acc@5 99.010 loss 0.601
[23:04:22.581391] Accuracy of the network on the 10000 test images: 81.1%
[23:04:22.581567] Max accuracy: 81.06%
[23:04:22.752007] log_dir: ./output_dir
[23:04:23.615425] Epoch: [49]  [  0/781]  eta: 0:11:13  lr: 0.000140  training_loss: 1.3157 (1.3157)  mae_loss: 0.0266 (0.0266)  classification_loss: 1.2841 (1.2841)  loss_mask: 0.0050 (0.0050)  time: 0.8617  data: 0.6348  max mem: 5511
[23:04:27.560136] Epoch: [49]  [ 20/781]  eta: 0:02:54  lr: 0.000140  training_loss: 1.3791 (1.3482)  mae_loss: 0.0253 (0.0272)  classification_loss: 1.3510 (1.3179)  loss_mask: 0.0025 (0.0031)  time: 0.1971  data: 0.0002  max mem: 5511
[23:04:31.492358] Epoch: [49]  [ 40/781]  eta: 0:02:37  lr: 0.000140  training_loss: 1.3360 (1.3504)  mae_loss: 0.0270 (0.0276)  classification_loss: 1.3056 (1.3199)  loss_mask: 0.0018 (0.0029)  time: 0.1965  data: 0.0002  max mem: 5511
[23:04:35.421513] Epoch: [49]  [ 60/781]  eta: 0:02:29  lr: 0.000140  training_loss: 1.3556 (1.3519)  mae_loss: 0.0287 (0.0281)  classification_loss: 1.3232 (1.3212)  loss_mask: 0.0018 (0.0026)  time: 0.1964  data: 0.0002  max mem: 5511
[23:04:39.353485] Epoch: [49]  [ 80/781]  eta: 0:02:23  lr: 0.000139  training_loss: 1.3269 (1.3509)  mae_loss: 0.0288 (0.0284)  classification_loss: 1.3010 (1.3199)  loss_mask: 0.0018 (0.0025)  time: 0.1965  data: 0.0002  max mem: 5511
[23:04:43.317177] Epoch: [49]  [100/781]  eta: 0:02:18  lr: 0.000139  training_loss: 1.2987 (1.3442)  mae_loss: 0.0271 (0.0282)  classification_loss: 1.2641 (1.3137)  loss_mask: 0.0013 (0.0023)  time: 0.1981  data: 0.0002  max mem: 5511
[23:04:47.264196] Epoch: [49]  [120/781]  eta: 0:02:13  lr: 0.000139  training_loss: 1.2926 (1.3402)  mae_loss: 0.0269 (0.0281)  classification_loss: 1.2639 (1.3099)  loss_mask: 0.0013 (0.0022)  time: 0.1973  data: 0.0002  max mem: 5511
[23:04:51.193514] Epoch: [49]  [140/781]  eta: 0:02:09  lr: 0.000139  training_loss: 1.3734 (1.3404)  mae_loss: 0.0271 (0.0280)  classification_loss: 1.3421 (1.3105)  loss_mask: 0.0008 (0.0020)  time: 0.1964  data: 0.0003  max mem: 5511
[23:04:55.118373] Epoch: [49]  [160/781]  eta: 0:02:04  lr: 0.000139  training_loss: 1.3209 (1.3375)  mae_loss: 0.0268 (0.0278)  classification_loss: 1.2893 (1.3078)  loss_mask: 0.0008 (0.0019)  time: 0.1961  data: 0.0002  max mem: 5511
[23:04:59.054618] Epoch: [49]  [180/781]  eta: 0:02:00  lr: 0.000139  training_loss: 1.3523 (1.3368)  mae_loss: 0.0270 (0.0278)  classification_loss: 1.3128 (1.3069)  loss_mask: 0.0029 (0.0021)  time: 0.1967  data: 0.0003  max mem: 5511
[23:05:03.003841] Epoch: [49]  [200/781]  eta: 0:01:56  lr: 0.000139  training_loss: 1.3202 (1.3360)  mae_loss: 0.0263 (0.0276)  classification_loss: 1.2884 (1.3062)  loss_mask: 0.0017 (0.0022)  time: 0.1974  data: 0.0002  max mem: 5511
[23:05:06.931977] Epoch: [49]  [220/781]  eta: 0:01:52  lr: 0.000139  training_loss: 1.3184 (1.3376)  mae_loss: 0.0263 (0.0276)  classification_loss: 1.2938 (1.3079)  loss_mask: 0.0012 (0.0021)  time: 0.1963  data: 0.0002  max mem: 5511
[23:05:10.858589] Epoch: [49]  [240/781]  eta: 0:01:47  lr: 0.000139  training_loss: 1.3676 (1.3395)  mae_loss: 0.0279 (0.0276)  classification_loss: 1.3366 (1.3099)  loss_mask: 0.0012 (0.0021)  time: 0.1962  data: 0.0003  max mem: 5511
[23:05:14.793066] Epoch: [49]  [260/781]  eta: 0:01:43  lr: 0.000139  training_loss: 1.3100 (1.3392)  mae_loss: 0.0266 (0.0275)  classification_loss: 1.2842 (1.3096)  loss_mask: 0.0014 (0.0021)  time: 0.1966  data: 0.0002  max mem: 5511
[23:05:18.739424] Epoch: [49]  [280/781]  eta: 0:01:39  lr: 0.000138  training_loss: 1.3504 (1.3410)  mae_loss: 0.0269 (0.0275)  classification_loss: 1.3212 (1.3114)  loss_mask: 0.0021 (0.0022)  time: 0.1972  data: 0.0002  max mem: 5511
[23:05:22.679520] Epoch: [49]  [300/781]  eta: 0:01:35  lr: 0.000138  training_loss: 1.3687 (1.3429)  mae_loss: 0.0286 (0.0276)  classification_loss: 1.3344 (1.3129)  loss_mask: 0.0032 (0.0024)  time: 0.1969  data: 0.0002  max mem: 5511
[23:05:26.602382] Epoch: [49]  [320/781]  eta: 0:01:31  lr: 0.000138  training_loss: 1.3265 (1.3413)  mae_loss: 0.0265 (0.0275)  classification_loss: 1.2967 (1.3112)  loss_mask: 0.0021 (0.0025)  time: 0.1961  data: 0.0002  max mem: 5511
[23:05:30.549339] Epoch: [49]  [340/781]  eta: 0:01:27  lr: 0.000138  training_loss: 1.3023 (1.3388)  mae_loss: 0.0258 (0.0275)  classification_loss: 1.2749 (1.3088)  loss_mask: 0.0014 (0.0025)  time: 0.1973  data: 0.0002  max mem: 5511
[23:05:34.497728] Epoch: [49]  [360/781]  eta: 0:01:23  lr: 0.000138  training_loss: 1.3129 (1.3390)  mae_loss: 0.0263 (0.0274)  classification_loss: 1.2809 (1.3090)  loss_mask: 0.0026 (0.0026)  time: 0.1973  data: 0.0002  max mem: 5511
[23:05:38.419356] Epoch: [49]  [380/781]  eta: 0:01:19  lr: 0.000138  training_loss: 1.3348 (1.3390)  mae_loss: 0.0268 (0.0275)  classification_loss: 1.3015 (1.3090)  loss_mask: 0.0010 (0.0025)  time: 0.1960  data: 0.0002  max mem: 5511
[23:05:42.354093] Epoch: [49]  [400/781]  eta: 0:01:15  lr: 0.000138  training_loss: 1.3402 (1.3399)  mae_loss: 0.0278 (0.0275)  classification_loss: 1.3111 (1.3100)  loss_mask: 0.0009 (0.0024)  time: 0.1966  data: 0.0002  max mem: 5511
[23:05:46.310089] Epoch: [49]  [420/781]  eta: 0:01:11  lr: 0.000138  training_loss: 1.3457 (1.3405)  mae_loss: 0.0280 (0.0275)  classification_loss: 1.3134 (1.3107)  loss_mask: 0.0010 (0.0023)  time: 0.1977  data: 0.0002  max mem: 5511
[23:05:50.291990] Epoch: [49]  [440/781]  eta: 0:01:07  lr: 0.000138  training_loss: 1.2982 (1.3402)  mae_loss: 0.0267 (0.0274)  classification_loss: 1.2709 (1.3105)  loss_mask: 0.0009 (0.0023)  time: 0.1990  data: 0.0002  max mem: 5511
[23:05:54.229913] Epoch: [49]  [460/781]  eta: 0:01:03  lr: 0.000137  training_loss: 1.2873 (1.3389)  mae_loss: 0.0271 (0.0274)  classification_loss: 1.2631 (1.3092)  loss_mask: 0.0009 (0.0022)  time: 0.1968  data: 0.0002  max mem: 5511
[23:05:58.161806] Epoch: [49]  [480/781]  eta: 0:00:59  lr: 0.000137  training_loss: 1.3151 (1.3392)  mae_loss: 0.0263 (0.0274)  classification_loss: 1.2880 (1.3096)  loss_mask: 0.0007 (0.0022)  time: 0.1965  data: 0.0002  max mem: 5511
[23:06:02.079221] Epoch: [49]  [500/781]  eta: 0:00:55  lr: 0.000137  training_loss: 1.3499 (1.3393)  mae_loss: 0.0271 (0.0274)  classification_loss: 1.3234 (1.3098)  loss_mask: 0.0007 (0.0021)  time: 0.1958  data: 0.0002  max mem: 5511
[23:06:05.993936] Epoch: [49]  [520/781]  eta: 0:00:51  lr: 0.000137  training_loss: 1.3327 (1.3397)  mae_loss: 0.0243 (0.0273)  classification_loss: 1.3071 (1.3103)  loss_mask: 0.0010 (0.0021)  time: 0.1956  data: 0.0002  max mem: 5511
[23:06:09.918671] Epoch: [49]  [540/781]  eta: 0:00:47  lr: 0.000137  training_loss: 1.3478 (1.3403)  mae_loss: 0.0257 (0.0273)  classification_loss: 1.3212 (1.3110)  loss_mask: 0.0008 (0.0021)  time: 0.1961  data: 0.0002  max mem: 5511
[23:06:13.842589] Epoch: [49]  [560/781]  eta: 0:00:43  lr: 0.000137  training_loss: 1.3322 (1.3409)  mae_loss: 0.0280 (0.0273)  classification_loss: 1.3049 (1.3115)  loss_mask: 0.0010 (0.0020)  time: 0.1961  data: 0.0002  max mem: 5511
[23:06:17.799775] Epoch: [49]  [580/781]  eta: 0:00:39  lr: 0.000137  training_loss: 1.3213 (1.3406)  mae_loss: 0.0269 (0.0273)  classification_loss: 1.2950 (1.3113)  loss_mask: 0.0011 (0.0020)  time: 0.1977  data: 0.0003  max mem: 5511
[23:06:21.739720] Epoch: [49]  [600/781]  eta: 0:00:35  lr: 0.000137  training_loss: 1.3162 (1.3397)  mae_loss: 0.0269 (0.0273)  classification_loss: 1.2877 (1.3104)  loss_mask: 0.0008 (0.0020)  time: 0.1969  data: 0.0002  max mem: 5511
[23:06:25.700181] Epoch: [49]  [620/781]  eta: 0:00:31  lr: 0.000137  training_loss: 1.2801 (1.3385)  mae_loss: 0.0281 (0.0274)  classification_loss: 1.2525 (1.3092)  loss_mask: 0.0010 (0.0019)  time: 0.1979  data: 0.0002  max mem: 5511
[23:06:29.654395] Epoch: [49]  [640/781]  eta: 0:00:27  lr: 0.000137  training_loss: 1.3142 (1.3382)  mae_loss: 0.0272 (0.0274)  classification_loss: 1.2876 (1.3088)  loss_mask: 0.0016 (0.0019)  time: 0.1976  data: 0.0002  max mem: 5511
[23:06:33.572821] Epoch: [49]  [660/781]  eta: 0:00:23  lr: 0.000136  training_loss: 1.3685 (1.3391)  mae_loss: 0.0262 (0.0274)  classification_loss: 1.3429 (1.3098)  loss_mask: 0.0008 (0.0019)  time: 0.1958  data: 0.0002  max mem: 5511
[23:06:37.496265] Epoch: [49]  [680/781]  eta: 0:00:19  lr: 0.000136  training_loss: 1.3229 (1.3394)  mae_loss: 0.0272 (0.0274)  classification_loss: 1.2944 (1.3101)  loss_mask: 0.0016 (0.0019)  time: 0.1961  data: 0.0003  max mem: 5511
[23:06:41.426895] Epoch: [49]  [700/781]  eta: 0:00:16  lr: 0.000136  training_loss: 1.3663 (1.3399)  mae_loss: 0.0264 (0.0274)  classification_loss: 1.3355 (1.3105)  loss_mask: 0.0030 (0.0020)  time: 0.1965  data: 0.0002  max mem: 5511
[23:06:45.333056] Epoch: [49]  [720/781]  eta: 0:00:12  lr: 0.000136  training_loss: 1.3186 (1.3398)  mae_loss: 0.0265 (0.0274)  classification_loss: 1.2878 (1.3104)  loss_mask: 0.0021 (0.0021)  time: 0.1952  data: 0.0002  max mem: 5511
[23:06:49.239271] Epoch: [49]  [740/781]  eta: 0:00:08  lr: 0.000136  training_loss: 1.3219 (1.3396)  mae_loss: 0.0257 (0.0273)  classification_loss: 1.2680 (1.3097)  loss_mask: 0.0170 (0.0026)  time: 0.1952  data: 0.0002  max mem: 5511
[23:06:53.145198] Epoch: [49]  [760/781]  eta: 0:00:04  lr: 0.000136  training_loss: 1.3375 (1.3404)  mae_loss: 0.0284 (0.0274)  classification_loss: 1.2825 (1.3099)  loss_mask: 0.0189 (0.0032)  time: 0.1952  data: 0.0002  max mem: 5511
[23:06:57.056228] Epoch: [49]  [780/781]  eta: 0:00:00  lr: 0.000136  training_loss: 1.3842 (1.3413)  mae_loss: 0.0265 (0.0274)  classification_loss: 1.3408 (1.3106)  loss_mask: 0.0094 (0.0034)  time: 0.1955  data: 0.0002  max mem: 5511
[23:06:57.209818] Epoch: [49] Total time: 0:02:34 (0.1978 s / it)
[23:06:57.210261] Averaged stats: lr: 0.000136  training_loss: 1.3842 (1.3413)  mae_loss: 0.0265 (0.0274)  classification_loss: 1.3408 (1.3106)  loss_mask: 0.0094 (0.0034)
[23:06:57.825699] Test:  [  0/157]  eta: 0:01:35  testing_loss: 0.6038 (0.6038)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.6095  data: 0.5795  max mem: 5511
[23:06:58.133669] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.6334 (0.6193)  acc1: 81.2500 (79.6875)  acc5: 100.0000 (99.4318)  time: 0.0831  data: 0.0528  max mem: 5511
[23:06:58.425648] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.5926 (0.5881)  acc1: 82.8125 (81.6964)  acc5: 100.0000 (99.1815)  time: 0.0297  data: 0.0002  max mem: 5511
[23:06:58.710657] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5958 (0.6101)  acc1: 82.8125 (81.4516)  acc5: 98.4375 (99.0927)  time: 0.0287  data: 0.0002  max mem: 5511
[23:06:59.002192] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6156 (0.6110)  acc1: 81.2500 (81.6692)  acc5: 98.4375 (99.0473)  time: 0.0287  data: 0.0002  max mem: 5511
[23:06:59.290316] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6017 (0.5984)  acc1: 82.8125 (82.0466)  acc5: 100.0000 (99.0809)  time: 0.0288  data: 0.0002  max mem: 5511
[23:06:59.573932] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5428 (0.5909)  acc1: 84.3750 (82.1465)  acc5: 100.0000 (99.1291)  time: 0.0284  data: 0.0002  max mem: 5511
[23:06:59.859979] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5419 (0.5833)  acc1: 84.3750 (82.3283)  acc5: 100.0000 (99.2077)  time: 0.0283  data: 0.0002  max mem: 5511
[23:07:00.143923] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5747 (0.5935)  acc1: 81.2500 (81.9059)  acc5: 100.0000 (99.0934)  time: 0.0284  data: 0.0002  max mem: 5511
[23:07:00.433891] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6130 (0.5913)  acc1: 79.6875 (81.7651)  acc5: 98.4375 (99.1071)  time: 0.0285  data: 0.0002  max mem: 5511
[23:07:00.719999] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.6006 (0.5947)  acc1: 81.2500 (81.6832)  acc5: 98.4375 (99.0873)  time: 0.0286  data: 0.0002  max mem: 5511
[23:07:01.003517] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6220 (0.5958)  acc1: 81.2500 (81.5878)  acc5: 98.4375 (99.0287)  time: 0.0283  data: 0.0002  max mem: 5511
[23:07:01.291319] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5952 (0.5923)  acc1: 81.2500 (81.6632)  acc5: 98.4375 (99.0573)  time: 0.0284  data: 0.0002  max mem: 5511
[23:07:01.580319] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5596 (0.5925)  acc1: 81.2500 (81.6675)  acc5: 100.0000 (99.0577)  time: 0.0287  data: 0.0002  max mem: 5511
[23:07:01.863212] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6124 (0.5925)  acc1: 82.8125 (81.6489)  acc5: 100.0000 (99.0691)  time: 0.0285  data: 0.0002  max mem: 5511
[23:07:02.143493] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5905 (0.5909)  acc1: 82.8125 (81.7674)  acc5: 98.4375 (99.0377)  time: 0.0280  data: 0.0001  max mem: 5511
[23:07:02.292967] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5586 (0.5893)  acc1: 82.8125 (81.7600)  acc5: 98.4375 (99.0500)  time: 0.0270  data: 0.0001  max mem: 5511
[23:07:02.451434] Test: Total time: 0:00:05 (0.0334 s / it)
[23:07:02.451951] * Acc@1 81.760 Acc@5 99.050 loss 0.589
[23:07:02.452249] Accuracy of the network on the 10000 test images: 81.8%
[23:07:02.452466] Max accuracy: 81.76%
[23:07:02.750499] log_dir: ./output_dir
[23:07:03.561889] Epoch: [50]  [  0/781]  eta: 0:10:32  lr: 0.000136  training_loss: 1.1168 (1.1168)  mae_loss: 0.0305 (0.0305)  classification_loss: 1.0804 (1.0804)  loss_mask: 0.0059 (0.0059)  time: 0.8096  data: 0.6088  max mem: 5511
[23:07:07.479049] Epoch: [50]  [ 20/781]  eta: 0:02:51  lr: 0.000136  training_loss: 1.2684 (1.2756)  mae_loss: 0.0263 (0.0277)  classification_loss: 1.2358 (1.2411)  loss_mask: 0.0076 (0.0068)  time: 0.1958  data: 0.0002  max mem: 5511
[23:07:11.391052] Epoch: [50]  [ 40/781]  eta: 0:02:36  lr: 0.000136  training_loss: 1.3105 (1.2986)  mae_loss: 0.0269 (0.0272)  classification_loss: 1.2796 (1.2662)  loss_mask: 0.0032 (0.0052)  time: 0.1955  data: 0.0003  max mem: 5511
[23:07:15.327119] Epoch: [50]  [ 60/781]  eta: 0:02:28  lr: 0.000135  training_loss: 1.3288 (1.3085)  mae_loss: 0.0270 (0.0270)  classification_loss: 1.2968 (1.2773)  loss_mask: 0.0020 (0.0042)  time: 0.1967  data: 0.0002  max mem: 5511
[23:07:19.241809] Epoch: [50]  [ 80/781]  eta: 0:02:22  lr: 0.000135  training_loss: 1.3293 (1.3132)  mae_loss: 0.0280 (0.0274)  classification_loss: 1.2949 (1.2820)  loss_mask: 0.0019 (0.0038)  time: 0.1957  data: 0.0002  max mem: 5511
[23:07:23.155451] Epoch: [50]  [100/781]  eta: 0:02:17  lr: 0.000135  training_loss: 1.3503 (1.3227)  mae_loss: 0.0260 (0.0273)  classification_loss: 1.3234 (1.2919)  loss_mask: 0.0018 (0.0035)  time: 0.1956  data: 0.0002  max mem: 5511
[23:07:27.081433] Epoch: [50]  [120/781]  eta: 0:02:12  lr: 0.000135  training_loss: 1.3312 (1.3240)  mae_loss: 0.0266 (0.0273)  classification_loss: 1.2981 (1.2935)  loss_mask: 0.0015 (0.0032)  time: 0.1962  data: 0.0002  max mem: 5511
[23:07:30.990577] Epoch: [50]  [140/781]  eta: 0:02:08  lr: 0.000135  training_loss: 1.3174 (1.3237)  mae_loss: 0.0252 (0.0271)  classification_loss: 1.2904 (1.2936)  loss_mask: 0.0010 (0.0030)  time: 0.1953  data: 0.0002  max mem: 5511
[23:07:34.892325] Epoch: [50]  [160/781]  eta: 0:02:03  lr: 0.000135  training_loss: 1.3284 (1.3267)  mae_loss: 0.0272 (0.0272)  classification_loss: 1.2997 (1.2968)  loss_mask: 0.0008 (0.0027)  time: 0.1950  data: 0.0002  max mem: 5511
[23:07:38.802660] Epoch: [50]  [180/781]  eta: 0:01:59  lr: 0.000135  training_loss: 1.2979 (1.3251)  mae_loss: 0.0283 (0.0273)  classification_loss: 1.2682 (1.2952)  loss_mask: 0.0009 (0.0025)  time: 0.1954  data: 0.0002  max mem: 5511
[23:07:42.749104] Epoch: [50]  [200/781]  eta: 0:01:55  lr: 0.000135  training_loss: 1.3385 (1.3264)  mae_loss: 0.0258 (0.0272)  classification_loss: 1.3137 (1.2968)  loss_mask: 0.0009 (0.0024)  time: 0.1972  data: 0.0002  max mem: 5511
[23:07:46.694019] Epoch: [50]  [220/781]  eta: 0:01:51  lr: 0.000135  training_loss: 1.2940 (1.3229)  mae_loss: 0.0264 (0.0272)  classification_loss: 1.2613 (1.2934)  loss_mask: 0.0010 (0.0023)  time: 0.1971  data: 0.0002  max mem: 5511
[23:07:50.658411] Epoch: [50]  [240/781]  eta: 0:01:47  lr: 0.000135  training_loss: 1.3410 (1.3236)  mae_loss: 0.0255 (0.0272)  classification_loss: 1.3132 (1.2943)  loss_mask: 0.0011 (0.0022)  time: 0.1981  data: 0.0003  max mem: 5511
[23:07:54.620924] Epoch: [50]  [260/781]  eta: 0:01:43  lr: 0.000134  training_loss: 1.3199 (1.3252)  mae_loss: 0.0236 (0.0270)  classification_loss: 1.2960 (1.2959)  loss_mask: 0.0010 (0.0022)  time: 0.1980  data: 0.0002  max mem: 5511
[23:07:58.575385] Epoch: [50]  [280/781]  eta: 0:01:39  lr: 0.000134  training_loss: 1.3140 (1.3257)  mae_loss: 0.0268 (0.0271)  classification_loss: 1.2843 (1.2964)  loss_mask: 0.0014 (0.0022)  time: 0.1976  data: 0.0002  max mem: 5511
[23:08:02.494308] Epoch: [50]  [300/781]  eta: 0:01:35  lr: 0.000134  training_loss: 1.3448 (1.3281)  mae_loss: 0.0260 (0.0270)  classification_loss: 1.3111 (1.2989)  loss_mask: 0.0016 (0.0022)  time: 0.1959  data: 0.0002  max mem: 5511
[23:08:06.436521] Epoch: [50]  [320/781]  eta: 0:01:31  lr: 0.000134  training_loss: 1.3202 (1.3276)  mae_loss: 0.0270 (0.0271)  classification_loss: 1.2920 (1.2984)  loss_mask: 0.0013 (0.0021)  time: 0.1970  data: 0.0002  max mem: 5511
[23:08:10.366361] Epoch: [50]  [340/781]  eta: 0:01:27  lr: 0.000134  training_loss: 1.2640 (1.3248)  mae_loss: 0.0279 (0.0271)  classification_loss: 1.2340 (1.2957)  loss_mask: 0.0007 (0.0021)  time: 0.1964  data: 0.0002  max mem: 5511
[23:08:14.319992] Epoch: [50]  [360/781]  eta: 0:01:23  lr: 0.000134  training_loss: 1.2902 (1.3236)  mae_loss: 0.0272 (0.0271)  classification_loss: 1.2575 (1.2943)  loss_mask: 0.0016 (0.0021)  time: 0.1976  data: 0.0002  max mem: 5511
[23:08:18.264860] Epoch: [50]  [380/781]  eta: 0:01:19  lr: 0.000134  training_loss: 1.2894 (1.3236)  mae_loss: 0.0278 (0.0272)  classification_loss: 1.2571 (1.2942)  loss_mask: 0.0024 (0.0022)  time: 0.1971  data: 0.0002  max mem: 5511
[23:08:22.195290] Epoch: [50]  [400/781]  eta: 0:01:15  lr: 0.000134  training_loss: 1.3417 (1.3242)  mae_loss: 0.0271 (0.0272)  classification_loss: 1.3115 (1.2948)  loss_mask: 0.0022 (0.0022)  time: 0.1964  data: 0.0002  max mem: 5511
[23:08:26.111893] Epoch: [50]  [420/781]  eta: 0:01:11  lr: 0.000134  training_loss: 1.2896 (1.3229)  mae_loss: 0.0269 (0.0272)  classification_loss: 1.2605 (1.2935)  loss_mask: 0.0014 (0.0022)  time: 0.1957  data: 0.0002  max mem: 5511
[23:08:30.037748] Epoch: [50]  [440/781]  eta: 0:01:07  lr: 0.000133  training_loss: 1.2752 (1.3218)  mae_loss: 0.0265 (0.0272)  classification_loss: 1.2478 (1.2924)  loss_mask: 0.0010 (0.0021)  time: 0.1962  data: 0.0002  max mem: 5511
[23:08:33.981842] Epoch: [50]  [460/781]  eta: 0:01:03  lr: 0.000133  training_loss: 1.3161 (1.3216)  mae_loss: 0.0267 (0.0272)  classification_loss: 1.2899 (1.2923)  loss_mask: 0.0007 (0.0021)  time: 0.1971  data: 0.0002  max mem: 5511
[23:08:37.913203] Epoch: [50]  [480/781]  eta: 0:00:59  lr: 0.000133  training_loss: 1.3408 (1.3227)  mae_loss: 0.0271 (0.0272)  classification_loss: 1.3109 (1.2933)  loss_mask: 0.0019 (0.0022)  time: 0.1965  data: 0.0002  max mem: 5511
[23:08:41.906444] Epoch: [50]  [500/781]  eta: 0:00:55  lr: 0.000133  training_loss: 1.3577 (1.3237)  mae_loss: 0.0282 (0.0272)  classification_loss: 1.3045 (1.2939)  loss_mask: 0.0084 (0.0026)  time: 0.1995  data: 0.0002  max mem: 5511
[23:08:45.837981] Epoch: [50]  [520/781]  eta: 0:00:51  lr: 0.000133  training_loss: 1.3160 (1.3227)  mae_loss: 0.0267 (0.0272)  classification_loss: 1.2813 (1.2928)  loss_mask: 0.0053 (0.0027)  time: 0.1965  data: 0.0003  max mem: 5511
[23:08:49.783652] Epoch: [50]  [540/781]  eta: 0:00:47  lr: 0.000133  training_loss: 1.3359 (1.3244)  mae_loss: 0.0276 (0.0272)  classification_loss: 1.3010 (1.2943)  loss_mask: 0.0071 (0.0029)  time: 0.1972  data: 0.0002  max mem: 5511
[23:08:53.709084] Epoch: [50]  [560/781]  eta: 0:00:43  lr: 0.000133  training_loss: 1.3621 (1.3259)  mae_loss: 0.0259 (0.0272)  classification_loss: 1.3347 (1.2957)  loss_mask: 0.0022 (0.0029)  time: 0.1962  data: 0.0002  max mem: 5511
[23:08:57.672214] Epoch: [50]  [580/781]  eta: 0:00:39  lr: 0.000133  training_loss: 1.3003 (1.3258)  mae_loss: 0.0270 (0.0272)  classification_loss: 1.2709 (1.2957)  loss_mask: 0.0019 (0.0029)  time: 0.1981  data: 0.0002  max mem: 5511
[23:09:01.593329] Epoch: [50]  [600/781]  eta: 0:00:35  lr: 0.000133  training_loss: 1.2460 (1.3235)  mae_loss: 0.0272 (0.0272)  classification_loss: 1.2212 (1.2935)  loss_mask: 0.0013 (0.0029)  time: 0.1960  data: 0.0002  max mem: 5511
[23:09:05.526754] Epoch: [50]  [620/781]  eta: 0:00:31  lr: 0.000133  training_loss: 1.2782 (1.3223)  mae_loss: 0.0295 (0.0273)  classification_loss: 1.2498 (1.2922)  loss_mask: 0.0014 (0.0028)  time: 0.1966  data: 0.0002  max mem: 5511
[23:09:09.465955] Epoch: [50]  [640/781]  eta: 0:00:27  lr: 0.000132  training_loss: 1.2555 (1.3215)  mae_loss: 0.0256 (0.0272)  classification_loss: 1.2279 (1.2915)  loss_mask: 0.0011 (0.0028)  time: 0.1969  data: 0.0002  max mem: 5511
[23:09:13.394028] Epoch: [50]  [660/781]  eta: 0:00:23  lr: 0.000132  training_loss: 1.3477 (1.3215)  mae_loss: 0.0277 (0.0273)  classification_loss: 1.3164 (1.2915)  loss_mask: 0.0015 (0.0028)  time: 0.1963  data: 0.0002  max mem: 5511
[23:09:17.355353] Epoch: [50]  [680/781]  eta: 0:00:19  lr: 0.000132  training_loss: 1.2954 (1.3220)  mae_loss: 0.0267 (0.0272)  classification_loss: 1.2662 (1.2920)  loss_mask: 0.0014 (0.0028)  time: 0.1980  data: 0.0002  max mem: 5511
[23:09:21.276458] Epoch: [50]  [700/781]  eta: 0:00:15  lr: 0.000132  training_loss: 1.3422 (1.3227)  mae_loss: 0.0273 (0.0273)  classification_loss: 1.3122 (1.2927)  loss_mask: 0.0011 (0.0027)  time: 0.1960  data: 0.0002  max mem: 5511
[23:09:25.189987] Epoch: [50]  [720/781]  eta: 0:00:12  lr: 0.000132  training_loss: 1.3580 (1.3237)  mae_loss: 0.0281 (0.0273)  classification_loss: 1.3263 (1.2937)  loss_mask: 0.0009 (0.0027)  time: 0.1956  data: 0.0002  max mem: 5511
[23:09:29.118209] Epoch: [50]  [740/781]  eta: 0:00:08  lr: 0.000132  training_loss: 1.2682 (1.3232)  mae_loss: 0.0280 (0.0273)  classification_loss: 1.2447 (1.2932)  loss_mask: 0.0007 (0.0027)  time: 0.1963  data: 0.0002  max mem: 5511
[23:09:33.063193] Epoch: [50]  [760/781]  eta: 0:00:04  lr: 0.000132  training_loss: 1.3434 (1.3238)  mae_loss: 0.0273 (0.0273)  classification_loss: 1.3184 (1.2939)  loss_mask: 0.0008 (0.0026)  time: 0.1972  data: 0.0002  max mem: 5511
[23:09:36.981103] Epoch: [50]  [780/781]  eta: 0:00:00  lr: 0.000132  training_loss: 1.3412 (1.3250)  mae_loss: 0.0278 (0.0274)  classification_loss: 1.3144 (1.2950)  loss_mask: 0.0011 (0.0026)  time: 0.1958  data: 0.0002  max mem: 5511
[23:09:37.129132] Epoch: [50] Total time: 0:02:34 (0.1977 s / it)
[23:09:37.129714] Averaged stats: lr: 0.000132  training_loss: 1.3412 (1.3250)  mae_loss: 0.0278 (0.0274)  classification_loss: 1.3144 (1.2950)  loss_mask: 0.0011 (0.0026)
[23:09:38.401757] Test:  [  0/157]  eta: 0:01:39  testing_loss: 0.6282 (0.6282)  acc1: 79.6875 (79.6875)  acc5: 98.4375 (98.4375)  time: 0.6353  data: 0.6004  max mem: 5511
[23:09:38.689849] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.6785 (0.6667)  acc1: 81.2500 (79.1193)  acc5: 98.4375 (98.7216)  time: 0.0838  data: 0.0548  max mem: 5511
[23:09:38.981403] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.6519 (0.6301)  acc1: 81.2500 (79.6875)  acc5: 98.4375 (98.8095)  time: 0.0288  data: 0.0003  max mem: 5511
[23:09:39.268597] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6398 (0.6513)  acc1: 78.1250 (79.0827)  acc5: 98.4375 (98.4879)  time: 0.0288  data: 0.0003  max mem: 5511
[23:09:39.556099] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6414 (0.6501)  acc1: 78.1250 (79.0396)  acc5: 98.4375 (98.4756)  time: 0.0286  data: 0.0002  max mem: 5511
[23:09:39.845008] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6072 (0.6361)  acc1: 81.2500 (79.6875)  acc5: 98.4375 (98.5294)  time: 0.0287  data: 0.0003  max mem: 5511
[23:09:40.135969] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5952 (0.6290)  acc1: 81.2500 (79.7643)  acc5: 98.4375 (98.6424)  time: 0.0288  data: 0.0004  max mem: 5511
[23:09:40.426928] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5967 (0.6229)  acc1: 81.2500 (80.1056)  acc5: 100.0000 (98.7236)  time: 0.0289  data: 0.0005  max mem: 5511
[23:09:40.715168] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5967 (0.6289)  acc1: 81.2500 (79.9383)  acc5: 100.0000 (98.7076)  time: 0.0288  data: 0.0005  max mem: 5511
[23:09:40.998283] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6306 (0.6270)  acc1: 81.2500 (80.0309)  acc5: 98.4375 (98.7637)  time: 0.0284  data: 0.0003  max mem: 5511
[23:09:41.280515] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.6190 (0.6275)  acc1: 79.6875 (79.9660)  acc5: 98.4375 (98.7624)  time: 0.0281  data: 0.0002  max mem: 5511
[23:09:41.563849] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6190 (0.6273)  acc1: 79.6875 (80.0394)  acc5: 98.4375 (98.7753)  time: 0.0282  data: 0.0002  max mem: 5511
[23:09:41.852783] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6050 (0.6261)  acc1: 79.6875 (80.0362)  acc5: 100.0000 (98.7991)  time: 0.0285  data: 0.0002  max mem: 5511
[23:09:42.141237] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6038 (0.6276)  acc1: 79.6875 (79.9857)  acc5: 98.4375 (98.8192)  time: 0.0287  data: 0.0003  max mem: 5511
[23:09:42.430689] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6286 (0.6284)  acc1: 79.6875 (80.0532)  acc5: 98.4375 (98.8032)  time: 0.0288  data: 0.0003  max mem: 5511
[23:09:42.711784] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6286 (0.6291)  acc1: 79.6875 (80.0083)  acc5: 98.4375 (98.8204)  time: 0.0284  data: 0.0001  max mem: 5511
[23:09:42.862734] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6050 (0.6284)  acc1: 79.6875 (79.9800)  acc5: 98.4375 (98.8300)  time: 0.0271  data: 0.0001  max mem: 5511
[23:09:43.024837] Test: Total time: 0:00:05 (0.0335 s / it)
[23:09:43.025390] * Acc@1 79.980 Acc@5 98.830 loss 0.628
[23:09:43.025738] Accuracy of the network on the 10000 test images: 80.0%
[23:09:43.026001] Max accuracy: 81.76%
[23:09:43.225221] log_dir: ./output_dir
[23:09:44.066292] Epoch: [51]  [  0/781]  eta: 0:10:55  lr: 0.000132  training_loss: 1.1911 (1.1911)  mae_loss: 0.0267 (0.0267)  classification_loss: 1.1628 (1.1628)  loss_mask: 0.0015 (0.0015)  time: 0.8392  data: 0.6298  max mem: 5511
[23:09:47.974407] Epoch: [51]  [ 20/781]  eta: 0:02:51  lr: 0.000132  training_loss: 1.2650 (1.2730)  mae_loss: 0.0276 (0.0281)  classification_loss: 1.2379 (1.2432)  loss_mask: 0.0016 (0.0017)  time: 0.1953  data: 0.0002  max mem: 5511
[23:09:51.899599] Epoch: [51]  [ 40/781]  eta: 0:02:36  lr: 0.000131  training_loss: 1.2939 (1.3134)  mae_loss: 0.0289 (0.0282)  classification_loss: 1.2650 (1.2837)  loss_mask: 0.0010 (0.0016)  time: 0.1962  data: 0.0002  max mem: 5511
[23:09:55.834013] Epoch: [51]  [ 60/781]  eta: 0:02:28  lr: 0.000131  training_loss: 1.2984 (1.3061)  mae_loss: 0.0269 (0.0277)  classification_loss: 1.2766 (1.2768)  loss_mask: 0.0014 (0.0016)  time: 0.1966  data: 0.0002  max mem: 5511
[23:09:59.784807] Epoch: [51]  [ 80/781]  eta: 0:02:23  lr: 0.000131  training_loss: 1.2689 (1.3067)  mae_loss: 0.0253 (0.0274)  classification_loss: 1.2407 (1.2779)  loss_mask: 0.0008 (0.0014)  time: 0.1975  data: 0.0003  max mem: 5511
[23:10:03.704734] Epoch: [51]  [100/781]  eta: 0:02:18  lr: 0.000131  training_loss: 1.2980 (1.3110)  mae_loss: 0.0270 (0.0274)  classification_loss: 1.2644 (1.2822)  loss_mask: 0.0008 (0.0013)  time: 0.1959  data: 0.0002  max mem: 5511
[23:10:07.619851] Epoch: [51]  [120/781]  eta: 0:02:13  lr: 0.000131  training_loss: 1.3076 (1.3139)  mae_loss: 0.0276 (0.0275)  classification_loss: 1.2811 (1.2851)  loss_mask: 0.0009 (0.0013)  time: 0.1957  data: 0.0002  max mem: 5511
[23:10:11.533280] Epoch: [51]  [140/781]  eta: 0:02:08  lr: 0.000131  training_loss: 1.2988 (1.3147)  mae_loss: 0.0263 (0.0274)  classification_loss: 1.2721 (1.2859)  loss_mask: 0.0013 (0.0014)  time: 0.1956  data: 0.0002  max mem: 5511
[23:10:15.465102] Epoch: [51]  [160/781]  eta: 0:02:04  lr: 0.000131  training_loss: 1.2807 (1.3127)  mae_loss: 0.0250 (0.0272)  classification_loss: 1.2549 (1.2842)  loss_mask: 0.0007 (0.0013)  time: 0.1965  data: 0.0002  max mem: 5511
[23:10:19.420621] Epoch: [51]  [180/781]  eta: 0:02:00  lr: 0.000131  training_loss: 1.3244 (1.3167)  mae_loss: 0.0264 (0.0272)  classification_loss: 1.2865 (1.2882)  loss_mask: 0.0007 (0.0013)  time: 0.1977  data: 0.0002  max mem: 5511
[23:10:23.335191] Epoch: [51]  [200/781]  eta: 0:01:55  lr: 0.000131  training_loss: 1.3063 (1.3167)  mae_loss: 0.0286 (0.0273)  classification_loss: 1.2764 (1.2881)  loss_mask: 0.0005 (0.0013)  time: 0.1956  data: 0.0002  max mem: 5511
[23:10:27.270591] Epoch: [51]  [220/781]  eta: 0:01:51  lr: 0.000131  training_loss: 1.2877 (1.3152)  mae_loss: 0.0260 (0.0272)  classification_loss: 1.2563 (1.2867)  loss_mask: 0.0008 (0.0013)  time: 0.1967  data: 0.0002  max mem: 5511
[23:10:31.210164] Epoch: [51]  [240/781]  eta: 0:01:47  lr: 0.000130  training_loss: 1.3397 (1.3163)  mae_loss: 0.0272 (0.0273)  classification_loss: 1.3106 (1.2879)  loss_mask: 0.0006 (0.0012)  time: 0.1969  data: 0.0002  max mem: 5511
[23:10:35.130273] Epoch: [51]  [260/781]  eta: 0:01:43  lr: 0.000130  training_loss: 1.3089 (1.3165)  mae_loss: 0.0271 (0.0273)  classification_loss: 1.2808 (1.2881)  loss_mask: 0.0006 (0.0012)  time: 0.1959  data: 0.0002  max mem: 5511
[23:10:39.058975] Epoch: [51]  [280/781]  eta: 0:01:39  lr: 0.000130  training_loss: 1.2984 (1.3150)  mae_loss: 0.0257 (0.0272)  classification_loss: 1.2718 (1.2864)  loss_mask: 0.0023 (0.0013)  time: 0.1963  data: 0.0002  max mem: 5511
[23:10:42.986175] Epoch: [51]  [300/781]  eta: 0:01:35  lr: 0.000130  training_loss: 1.3127 (1.3153)  mae_loss: 0.0266 (0.0272)  classification_loss: 1.2843 (1.2859)  loss_mask: 0.0087 (0.0022)  time: 0.1963  data: 0.0002  max mem: 5511
[23:10:46.914719] Epoch: [51]  [320/781]  eta: 0:01:31  lr: 0.000130  training_loss: 1.2766 (1.3153)  mae_loss: 0.0260 (0.0271)  classification_loss: 1.2427 (1.2852)  loss_mask: 0.0082 (0.0029)  time: 0.1963  data: 0.0002  max mem: 5511
[23:10:50.853250] Epoch: [51]  [340/781]  eta: 0:01:27  lr: 0.000130  training_loss: 1.2836 (1.3132)  mae_loss: 0.0276 (0.0272)  classification_loss: 1.2317 (1.2827)  loss_mask: 0.0064 (0.0034)  time: 0.1968  data: 0.0002  max mem: 5511
[23:10:54.774885] Epoch: [51]  [360/781]  eta: 0:01:23  lr: 0.000130  training_loss: 1.3330 (1.3147)  mae_loss: 0.0273 (0.0272)  classification_loss: 1.3030 (1.2840)  loss_mask: 0.0040 (0.0035)  time: 0.1960  data: 0.0002  max mem: 5511

[23:10:58.684810] Epoch: [51]  [380/781]  eta: 0:01:19  lr: 0.000130  training_loss: 1.3345 (1.3159)  mae_loss: 0.0283 (0.0272)  classification_loss: 1.3016 (1.2851)  loss_mask: 0.0033 (0.0035)  time: 0.1954  data: 0.0002  max mem: 5511
[23:11:02.616899] Epoch: [51]  [400/781]  eta: 0:01:15  lr: 0.000130  training_loss: 1.3310 (1.3162)  mae_loss: 0.0274 (0.0273)  classification_loss: 1.3032 (1.2855)  loss_mask: 0.0015 (0.0034)  time: 0.1965  data: 0.0002  max mem: 5511
[23:11:06.565864] Epoch: [51]  [420/781]  eta: 0:01:11  lr: 0.000129  training_loss: 1.2557 (1.3155)  mae_loss: 0.0268 (0.0272)  classification_loss: 1.2304 (1.2849)  loss_mask: 0.0009 (0.0033)  time: 0.1974  data: 0.0002  max mem: 5511
[23:11:10.478323] Epoch: [51]  [440/781]  eta: 0:01:07  lr: 0.000129  training_loss: 1.2775 (1.3142)  mae_loss: 0.0263 (0.0272)  classification_loss: 1.2484 (1.2838)  loss_mask: 0.0010 (0.0032)  time: 0.1955  data: 0.0002  max mem: 5511
[23:11:14.401024] Epoch: [51]  [460/781]  eta: 0:01:03  lr: 0.000129  training_loss: 1.2861 (1.3132)  mae_loss: 0.0273 (0.0272)  classification_loss: 1.2572 (1.2829)  loss_mask: 0.0011 (0.0031)  time: 0.1961  data: 0.0002  max mem: 5511
[23:11:18.349684] Epoch: [51]  [480/781]  eta: 0:00:59  lr: 0.000129  training_loss: 1.3498 (1.3138)  mae_loss: 0.0268 (0.0272)  classification_loss: 1.3161 (1.2836)  loss_mask: 0.0010 (0.0031)  time: 0.1974  data: 0.0002  max mem: 5511
[23:11:22.294249] Epoch: [51]  [500/781]  eta: 0:00:55  lr: 0.000129  training_loss: 1.3564 (1.3140)  mae_loss: 0.0265 (0.0272)  classification_loss: 1.3283 (1.2839)  loss_mask: 0.0007 (0.0030)  time: 0.1972  data: 0.0002  max mem: 5511
[23:11:26.225658] Epoch: [51]  [520/781]  eta: 0:00:51  lr: 0.000129  training_loss: 1.3748 (1.3160)  mae_loss: 0.0267 (0.0272)  classification_loss: 1.3474 (1.2859)  loss_mask: 0.0007 (0.0029)  time: 0.1965  data: 0.0002  max mem: 5511
[23:11:30.188068] Epoch: [51]  [540/781]  eta: 0:00:47  lr: 0.000129  training_loss: 1.3014 (1.3157)  mae_loss: 0.0275 (0.0271)  classification_loss: 1.2688 (1.2857)  loss_mask: 0.0009 (0.0028)  time: 0.1980  data: 0.0002  max mem: 5511
[23:11:34.111221] Epoch: [51]  [560/781]  eta: 0:00:43  lr: 0.000129  training_loss: 1.2529 (1.3141)  mae_loss: 0.0301 (0.0272)  classification_loss: 1.2235 (1.2841)  loss_mask: 0.0008 (0.0028)  time: 0.1961  data: 0.0002  max mem: 5511
[23:11:38.026230] Epoch: [51]  [580/781]  eta: 0:00:39  lr: 0.000129  training_loss: 1.2972 (1.3142)  mae_loss: 0.0272 (0.0272)  classification_loss: 1.2695 (1.2843)  loss_mask: 0.0007 (0.0027)  time: 0.1957  data: 0.0002  max mem: 5511
[23:11:41.972136] Epoch: [51]  [600/781]  eta: 0:00:35  lr: 0.000129  training_loss: 1.3229 (1.3140)  mae_loss: 0.0276 (0.0272)  classification_loss: 1.2935 (1.2841)  loss_mask: 0.0008 (0.0027)  time: 0.1972  data: 0.0002  max mem: 5511
[23:11:45.926322] Epoch: [51]  [620/781]  eta: 0:00:31  lr: 0.000128  training_loss: 1.2997 (1.3145)  mae_loss: 0.0265 (0.0272)  classification_loss: 1.2755 (1.2847)  loss_mask: 0.0006 (0.0026)  time: 0.1976  data: 0.0002  max mem: 5511
[23:11:49.857441] Epoch: [51]  [640/781]  eta: 0:00:27  lr: 0.000128  training_loss: 1.3271 (1.3150)  mae_loss: 0.0264 (0.0272)  classification_loss: 1.3004 (1.2852)  loss_mask: 0.0008 (0.0026)  time: 0.1965  data: 0.0002  max mem: 5511
[23:11:53.789872] Epoch: [51]  [660/781]  eta: 0:00:23  lr: 0.000128  training_loss: 1.2880 (1.3146)  mae_loss: 0.0266 (0.0272)  classification_loss: 1.2585 (1.2849)  loss_mask: 0.0007 (0.0025)  time: 0.1965  data: 0.0002  max mem: 5511
[23:11:57.732312] Epoch: [51]  [680/781]  eta: 0:00:19  lr: 0.000128  training_loss: 1.2780 (1.3148)  mae_loss: 0.0271 (0.0272)  classification_loss: 1.2513 (1.2851)  loss_mask: 0.0005 (0.0025)  time: 0.1970  data: 0.0005  max mem: 5511
[23:12:01.663183] Epoch: [51]  [700/781]  eta: 0:00:15  lr: 0.000128  training_loss: 1.3646 (1.3154)  mae_loss: 0.0270 (0.0272)  classification_loss: 1.3364 (1.2857)  loss_mask: 0.0008 (0.0024)  time: 0.1964  data: 0.0002  max mem: 5511
[23:12:05.595675] Epoch: [51]  [720/781]  eta: 0:00:12  lr: 0.000128  training_loss: 1.3253 (1.3158)  mae_loss: 0.0256 (0.0272)  classification_loss: 1.2967 (1.2861)  loss_mask: 0.0008 (0.0024)  time: 0.1965  data: 0.0003  max mem: 5511
[23:12:09.557862] Epoch: [51]  [740/781]  eta: 0:00:08  lr: 0.000128  training_loss: 1.2581 (1.3146)  mae_loss: 0.0281 (0.0273)  classification_loss: 1.2255 (1.2849)  loss_mask: 0.0006 (0.0024)  time: 0.1980  data: 0.0003  max mem: 5511
[23:12:13.505533] Epoch: [51]  [760/781]  eta: 0:00:04  lr: 0.000128  training_loss: 1.3407 (1.3151)  mae_loss: 0.0275 (0.0272)  classification_loss: 1.3091 (1.2856)  loss_mask: 0.0006 (0.0023)  time: 0.1973  data: 0.0002  max mem: 5511
[23:12:17.415338] Epoch: [51]  [780/781]  eta: 0:00:00  lr: 0.000128  training_loss: 1.3201 (1.3160)  mae_loss: 0.0273 (0.0273)  classification_loss: 1.2924 (1.2864)  loss_mask: 0.0006 (0.0023)  time: 0.1954  data: 0.0002  max mem: 5511
[23:12:17.563540] Epoch: [51] Total time: 0:02:34 (0.1976 s / it)
[23:12:17.564160] Averaged stats: lr: 0.000128  training_loss: 1.3201 (1.3160)  mae_loss: 0.0273 (0.0273)  classification_loss: 1.2924 (1.2864)  loss_mask: 0.0006 (0.0023)
[23:12:18.189253] Test:  [  0/157]  eta: 0:01:37  testing_loss: 0.5535 (0.5535)  acc1: 79.6875 (79.6875)  acc5: 98.4375 (98.4375)  time: 0.6204  data: 0.5909  max mem: 5511
[23:12:18.485314] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.6115 (0.6104)  acc1: 81.2500 (81.2500)  acc5: 98.4375 (99.1477)  time: 0.0831  data: 0.0539  max mem: 5511
[23:12:18.767415] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.6046 (0.5797)  acc1: 82.8125 (82.2173)  acc5: 100.0000 (99.2560)  time: 0.0287  data: 0.0002  max mem: 5511
[23:12:19.050583] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5837 (0.5906)  acc1: 81.2500 (81.4516)  acc5: 98.4375 (99.1431)  time: 0.0281  data: 0.0002  max mem: 5511
[23:12:19.333577] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5837 (0.5881)  acc1: 81.2500 (81.8598)  acc5: 98.4375 (99.0091)  time: 0.0282  data: 0.0002  max mem: 5511
[23:12:19.619949] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5632 (0.5803)  acc1: 82.8125 (82.1078)  acc5: 100.0000 (99.0809)  time: 0.0283  data: 0.0002  max mem: 5511
[23:12:19.905246] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5306 (0.5759)  acc1: 82.8125 (82.1977)  acc5: 100.0000 (99.1547)  time: 0.0284  data: 0.0002  max mem: 5511
[23:12:20.192920] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5314 (0.5695)  acc1: 82.8125 (82.2843)  acc5: 100.0000 (99.1857)  time: 0.0284  data: 0.0002  max mem: 5511
[23:12:20.476770] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5419 (0.5772)  acc1: 79.6875 (81.8480)  acc5: 98.4375 (99.0741)  time: 0.0284  data: 0.0002  max mem: 5511
[23:12:20.761619] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5922 (0.5750)  acc1: 81.2500 (82.1085)  acc5: 98.4375 (99.0385)  time: 0.0283  data: 0.0002  max mem: 5511
[23:12:21.049497] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5827 (0.5774)  acc1: 81.2500 (82.0235)  acc5: 98.4375 (98.9790)  time: 0.0284  data: 0.0002  max mem: 5511
[23:12:21.340290] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5827 (0.5796)  acc1: 81.2500 (81.9398)  acc5: 98.4375 (98.9443)  time: 0.0287  data: 0.0002  max mem: 5511
[23:12:21.627286] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5740 (0.5776)  acc1: 81.2500 (81.9861)  acc5: 100.0000 (98.9928)  time: 0.0287  data: 0.0002  max mem: 5511
[23:12:21.912613] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5543 (0.5766)  acc1: 84.3750 (81.9895)  acc5: 100.0000 (98.9862)  time: 0.0285  data: 0.0002  max mem: 5511
[23:12:22.195480] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5260 (0.5752)  acc1: 84.3750 (82.1698)  acc5: 98.4375 (98.9805)  time: 0.0282  data: 0.0002  max mem: 5511
[23:12:22.479875] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5759 (0.5748)  acc1: 81.2500 (82.2123)  acc5: 98.4375 (98.9859)  time: 0.0282  data: 0.0002  max mem: 5511
[23:12:22.634016] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5759 (0.5749)  acc1: 82.8125 (82.2000)  acc5: 98.4375 (99.0000)  time: 0.0274  data: 0.0001  max mem: 5511
[23:12:22.799148] Test: Total time: 0:00:05 (0.0333 s / it)
[23:12:22.799594] * Acc@1 82.200 Acc@5 99.000 loss 0.575
[23:12:22.799890] Accuracy of the network on the 10000 test images: 82.2%
[23:12:22.800062] Max accuracy: 82.20%
[23:12:23.327529] log_dir: ./output_dir
[23:12:24.150181] Epoch: [52]  [  0/781]  eta: 0:10:41  lr: 0.000128  training_loss: 1.1576 (1.1576)  mae_loss: 0.0254 (0.0254)  classification_loss: 1.1316 (1.1316)  loss_mask: 0.0006 (0.0006)  time: 0.8209  data: 0.6149  max mem: 5511
[23:12:28.090122] Epoch: [52]  [ 20/781]  eta: 0:02:52  lr: 0.000127  training_loss: 1.2740 (1.2822)  mae_loss: 0.0273 (0.0272)  classification_loss: 1.2479 (1.2542)  loss_mask: 0.0006 (0.0008)  time: 0.1969  data: 0.0002  max mem: 5511
[23:12:32.008716] Epoch: [52]  [ 40/781]  eta: 0:02:36  lr: 0.000127  training_loss: 1.3084 (1.3095)  mae_loss: 0.0259 (0.0270)  classification_loss: 1.2820 (1.2813)  loss_mask: 0.0007 (0.0012)  time: 0.1958  data: 0.0002  max mem: 5511
[23:12:35.951615] Epoch: [52]  [ 60/781]  eta: 0:02:29  lr: 0.000127  training_loss: 1.2924 (1.3058)  mae_loss: 0.0274 (0.0270)  classification_loss: 1.2616 (1.2773)  loss_mask: 0.0016 (0.0015)  time: 0.1971  data: 0.0003  max mem: 5511
[23:12:39.904069] Epoch: [52]  [ 80/781]  eta: 0:02:23  lr: 0.000127  training_loss: 1.2483 (1.2982)  mae_loss: 0.0270 (0.0270)  classification_loss: 1.2210 (1.2696)  loss_mask: 0.0008 (0.0016)  time: 0.1975  data: 0.0002  max mem: 5511
[23:12:43.846894] Epoch: [52]  [100/781]  eta: 0:02:18  lr: 0.000127  training_loss: 1.3051 (1.3003)  mae_loss: 0.0274 (0.0270)  classification_loss: 1.2752 (1.2717)  loss_mask: 0.0010 (0.0016)  time: 0.1971  data: 0.0002  max mem: 5511
[23:12:47.807148] Epoch: [52]  [120/781]  eta: 0:02:13  lr: 0.000127  training_loss: 1.3080 (1.3018)  mae_loss: 0.0251 (0.0268)  classification_loss: 1.2827 (1.2735)  loss_mask: 0.0007 (0.0015)  time: 0.1979  data: 0.0002  max mem: 5511
[23:12:51.739717] Epoch: [52]  [140/781]  eta: 0:02:09  lr: 0.000127  training_loss: 1.2918 (1.3018)  mae_loss: 0.0260 (0.0268)  classification_loss: 1.2653 (1.2736)  loss_mask: 0.0009 (0.0014)  time: 0.1966  data: 0.0002  max mem: 5511
[23:12:55.711126] Epoch: [52]  [160/781]  eta: 0:02:04  lr: 0.000127  training_loss: 1.2952 (1.3029)  mae_loss: 0.0268 (0.0268)  classification_loss: 1.2691 (1.2747)  loss_mask: 0.0008 (0.0015)  time: 0.1985  data: 0.0002  max mem: 5511
[23:12:59.627036] Epoch: [52]  [180/781]  eta: 0:02:00  lr: 0.000127  training_loss: 1.3186 (1.3045)  mae_loss: 0.0260 (0.0267)  classification_loss: 1.2920 (1.2763)  loss_mask: 0.0010 (0.0015)  time: 0.1957  data: 0.0002  max mem: 5511
[23:13:03.561136] Epoch: [52]  [200/781]  eta: 0:01:56  lr: 0.000127  training_loss: 1.2970 (1.3049)  mae_loss: 0.0259 (0.0267)  classification_loss: 1.2724 (1.2768)  loss_mask: 0.0007 (0.0015)  time: 0.1966  data: 0.0002  max mem: 5511
[23:13:07.507894] Epoch: [52]  [220/781]  eta: 0:01:52  lr: 0.000126  training_loss: 1.3224 (1.3067)  mae_loss: 0.0272 (0.0267)  classification_loss: 1.2921 (1.2783)  loss_mask: 0.0012 (0.0017)  time: 0.1973  data: 0.0002  max mem: 5511
[23:13:11.449637] Epoch: [52]  [240/781]  eta: 0:01:47  lr: 0.000126  training_loss: 1.3095 (1.3062)  mae_loss: 0.0256 (0.0267)  classification_loss: 1.2816 (1.2774)  loss_mask: 0.0027 (0.0021)  time: 0.1970  data: 0.0003  max mem: 5511
[23:13:15.412332] Epoch: [52]  [260/781]  eta: 0:01:43  lr: 0.000126  training_loss: 1.3629 (1.3089)  mae_loss: 0.0253 (0.0267)  classification_loss: 1.2840 (1.2778)  loss_mask: 0.0279 (0.0044)  time: 0.1980  data: 0.0002  max mem: 5511
[23:13:19.357721] Epoch: [52]  [280/781]  eta: 0:01:39  lr: 0.000126  training_loss: 1.3504 (1.3110)  mae_loss: 0.0282 (0.0268)  classification_loss: 1.2880 (1.2789)  loss_mask: 0.0125 (0.0054)  time: 0.1972  data: 0.0003  max mem: 5511
[23:13:23.325359] Epoch: [52]  [300/781]  eta: 0:01:35  lr: 0.000126  training_loss: 1.3097 (1.3113)  mae_loss: 0.0272 (0.0268)  classification_loss: 1.2754 (1.2790)  loss_mask: 0.0056 (0.0054)  time: 0.1983  data: 0.0002  max mem: 5511
[23:13:27.253724] Epoch: [52]  [320/781]  eta: 0:01:31  lr: 0.000126  training_loss: 1.2813 (1.3092)  mae_loss: 0.0271 (0.0268)  classification_loss: 1.2456 (1.2770)  loss_mask: 0.0032 (0.0053)  time: 0.1963  data: 0.0002  max mem: 5511
[23:13:31.174609] Epoch: [52]  [340/781]  eta: 0:01:27  lr: 0.000126  training_loss: 1.3170 (1.3102)  mae_loss: 0.0273 (0.0269)  classification_loss: 1.2883 (1.2782)  loss_mask: 0.0020 (0.0051)  time: 0.1960  data: 0.0002  max mem: 5511
[23:13:35.109736] Epoch: [52]  [360/781]  eta: 0:01:23  lr: 0.000126  training_loss: 1.3393 (1.3119)  mae_loss: 0.0257 (0.0269)  classification_loss: 1.3080 (1.2801)  loss_mask: 0.0015 (0.0049)  time: 0.1967  data: 0.0002  max mem: 5511
[23:13:39.047472] Epoch: [52]  [380/781]  eta: 0:01:19  lr: 0.000126  training_loss: 1.2926 (1.3103)  mae_loss: 0.0272 (0.0269)  classification_loss: 1.2585 (1.2786)  loss_mask: 0.0011 (0.0047)  time: 0.1968  data: 0.0002  max mem: 5511
[23:13:43.006562] Epoch: [52]  [400/781]  eta: 0:01:15  lr: 0.000125  training_loss: 1.3464 (1.3121)  mae_loss: 0.0260 (0.0269)  classification_loss: 1.3222 (1.2805)  loss_mask: 0.0014 (0.0046)  time: 0.1979  data: 0.0002  max mem: 5511
[23:13:46.940065] Epoch: [52]  [420/781]  eta: 0:01:11  lr: 0.000125  training_loss: 1.3046 (1.3110)  mae_loss: 0.0254 (0.0269)  classification_loss: 1.2809 (1.2796)  loss_mask: 0.0010 (0.0045)  time: 0.1966  data: 0.0003  max mem: 5511
[23:13:50.855061] Epoch: [52]  [440/781]  eta: 0:01:07  lr: 0.000125  training_loss: 1.2740 (1.3095)  mae_loss: 0.0256 (0.0268)  classification_loss: 1.2436 (1.2780)  loss_mask: 0.0056 (0.0047)  time: 0.1957  data: 0.0003  max mem: 5511
[23:13:54.834947] Epoch: [52]  [460/781]  eta: 0:01:03  lr: 0.000125  training_loss: 1.3220 (1.3107)  mae_loss: 0.0261 (0.0268)  classification_loss: 1.2923 (1.2791)  loss_mask: 0.0035 (0.0047)  time: 0.1989  data: 0.0002  max mem: 5511
[23:13:58.769981] Epoch: [52]  [480/781]  eta: 0:00:59  lr: 0.000125  training_loss: 1.3061 (1.3105)  mae_loss: 0.0242 (0.0267)  classification_loss: 1.2737 (1.2790)  loss_mask: 0.0029 (0.0047)  time: 0.1967  data: 0.0002  max mem: 5511
[23:14:02.734459] Epoch: [52]  [500/781]  eta: 0:00:55  lr: 0.000125  training_loss: 1.2639 (1.3092)  mae_loss: 0.0269 (0.0268)  classification_loss: 1.2278 (1.2776)  loss_mask: 0.0051 (0.0048)  time: 0.1981  data: 0.0002  max mem: 5511
[23:14:06.665943] Epoch: [52]  [520/781]  eta: 0:00:51  lr: 0.000125  training_loss: 1.2810 (1.3083)  mae_loss: 0.0259 (0.0267)  classification_loss: 1.2504 (1.2768)  loss_mask: 0.0022 (0.0047)  time: 0.1965  data: 0.0003  max mem: 5511
[23:14:10.647635] Epoch: [52]  [540/781]  eta: 0:00:47  lr: 0.000125  training_loss: 1.3299 (1.3089)  mae_loss: 0.0274 (0.0268)  classification_loss: 1.2949 (1.2775)  loss_mask: 0.0016 (0.0046)  time: 0.1990  data: 0.0002  max mem: 5511
[23:14:14.568435] Epoch: [52]  [560/781]  eta: 0:00:43  lr: 0.000125  training_loss: 1.2922 (1.3087)  mae_loss: 0.0274 (0.0268)  classification_loss: 1.2644 (1.2774)  loss_mask: 0.0010 (0.0045)  time: 0.1960  data: 0.0002  max mem: 5511
[23:14:18.506175] Epoch: [52]  [580/781]  eta: 0:00:39  lr: 0.000125  training_loss: 1.3087 (1.3086)  mae_loss: 0.0273 (0.0269)  classification_loss: 1.2679 (1.2774)  loss_mask: 0.0010 (0.0044)  time: 0.1968  data: 0.0002  max mem: 5511
[23:14:22.433120] Epoch: [52]  [600/781]  eta: 0:00:35  lr: 0.000124  training_loss: 1.2954 (1.3090)  mae_loss: 0.0264 (0.0269)  classification_loss: 1.2688 (1.2778)  loss_mask: 0.0012 (0.0043)  time: 0.1963  data: 0.0002  max mem: 5511
[23:14:26.380932] Epoch: [52]  [620/781]  eta: 0:00:31  lr: 0.000124  training_loss: 1.3375 (1.3093)  mae_loss: 0.0270 (0.0269)  classification_loss: 1.3082 (1.2782)  loss_mask: 0.0007 (0.0042)  time: 0.1973  data: 0.0002  max mem: 5511
[23:14:30.306418] Epoch: [52]  [640/781]  eta: 0:00:27  lr: 0.000124  training_loss: 1.3136 (1.3090)  mae_loss: 0.0266 (0.0269)  classification_loss: 1.2883 (1.2780)  loss_mask: 0.0008 (0.0041)  time: 0.1962  data: 0.0002  max mem: 5511
[23:14:34.238801] Epoch: [52]  [660/781]  eta: 0:00:23  lr: 0.000124  training_loss: 1.3024 (1.3091)  mae_loss: 0.0272 (0.0269)  classification_loss: 1.2691 (1.2782)  loss_mask: 0.0011 (0.0040)  time: 0.1965  data: 0.0002  max mem: 5511
[23:14:38.239203] Epoch: [52]  [680/781]  eta: 0:00:19  lr: 0.000124  training_loss: 1.3132 (1.3083)  mae_loss: 0.0254 (0.0269)  classification_loss: 1.2826 (1.2775)  loss_mask: 0.0007 (0.0039)  time: 0.1999  data: 0.0003  max mem: 5511
[23:14:42.204601] Epoch: [52]  [700/781]  eta: 0:00:16  lr: 0.000124  training_loss: 1.3213 (1.3088)  mae_loss: 0.0260 (0.0269)  classification_loss: 1.2899 (1.2781)  loss_mask: 0.0008 (0.0038)  time: 0.1981  data: 0.0002  max mem: 5511
[23:14:46.133740] Epoch: [52]  [720/781]  eta: 0:00:12  lr: 0.000124  training_loss: 1.3436 (1.3096)  mae_loss: 0.0277 (0.0269)  classification_loss: 1.3164 (1.2789)  loss_mask: 0.0005 (0.0038)  time: 0.1964  data: 0.0002  max mem: 5511
[23:14:50.084346] Epoch: [52]  [740/781]  eta: 0:00:08  lr: 0.000124  training_loss: 1.2858 (1.3089)  mae_loss: 0.0259 (0.0269)  classification_loss: 1.2626 (1.2784)  loss_mask: 0.0006 (0.0037)  time: 0.1975  data: 0.0002  max mem: 5511
[23:14:54.019487] Epoch: [52]  [760/781]  eta: 0:00:04  lr: 0.000124  training_loss: 1.2956 (1.3090)  mae_loss: 0.0269 (0.0269)  classification_loss: 1.2705 (1.2786)  loss_mask: 0.0006 (0.0036)  time: 0.1967  data: 0.0002  max mem: 5511
[23:14:57.934938] Epoch: [52]  [780/781]  eta: 0:00:00  lr: 0.000123  training_loss: 1.2764 (1.3093)  mae_loss: 0.0258 (0.0269)  classification_loss: 1.2528 (1.2789)  loss_mask: 0.0007 (0.0035)  time: 0.1957  data: 0.0002  max mem: 5511
[23:14:58.092360] Epoch: [52] Total time: 0:02:34 (0.1982 s / it)
[23:14:58.093316] Averaged stats: lr: 0.000123  training_loss: 1.2764 (1.3093)  mae_loss: 0.0258 (0.0269)  classification_loss: 1.2528 (1.2789)  loss_mask: 0.0007 (0.0035)
[23:14:58.763530] Test:  [  0/157]  eta: 0:01:44  testing_loss: 0.6059 (0.6059)  acc1: 79.6875 (79.6875)  acc5: 98.4375 (98.4375)  time: 0.6656  data: 0.6351  max mem: 5511
[23:14:59.052156] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.6359 (0.6162)  acc1: 79.6875 (79.6875)  acc5: 98.4375 (99.1477)  time: 0.0866  data: 0.0580  max mem: 5511
[23:14:59.336432] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.6030 (0.5792)  acc1: 81.2500 (81.5476)  acc5: 98.4375 (99.1071)  time: 0.0285  data: 0.0002  max mem: 5511
[23:14:59.619228] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6023 (0.6029)  acc1: 79.6875 (80.6956)  acc5: 98.4375 (99.0423)  time: 0.0282  data: 0.0001  max mem: 5511
[23:14:59.907958] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6038 (0.6031)  acc1: 79.6875 (80.8308)  acc5: 98.4375 (98.9710)  time: 0.0284  data: 0.0002  max mem: 5511
[23:15:00.193389] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5805 (0.5901)  acc1: 82.8125 (81.4951)  acc5: 98.4375 (99.0196)  time: 0.0285  data: 0.0002  max mem: 5511
[23:15:00.476898] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5176 (0.5857)  acc1: 82.8125 (81.6342)  acc5: 100.0000 (99.0779)  time: 0.0283  data: 0.0002  max mem: 5511
[23:15:00.764288] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5176 (0.5785)  acc1: 82.8125 (81.8882)  acc5: 100.0000 (99.1637)  time: 0.0284  data: 0.0002  max mem: 5511
[23:15:01.049127] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5384 (0.5840)  acc1: 82.8125 (81.8673)  acc5: 100.0000 (99.1127)  time: 0.0284  data: 0.0002  max mem: 5511
[23:15:01.338250] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5634 (0.5805)  acc1: 81.2500 (82.0055)  acc5: 98.4375 (99.1071)  time: 0.0285  data: 0.0002  max mem: 5511
[23:15:01.625144] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5537 (0.5821)  acc1: 81.2500 (81.9152)  acc5: 100.0000 (99.1491)  time: 0.0287  data: 0.0002  max mem: 5511
[23:15:01.910234] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5705 (0.5812)  acc1: 82.8125 (81.9538)  acc5: 100.0000 (99.1413)  time: 0.0285  data: 0.0002  max mem: 5511
[23:15:02.193786] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5666 (0.5803)  acc1: 81.2500 (81.9086)  acc5: 100.0000 (99.1736)  time: 0.0283  data: 0.0002  max mem: 5511
[23:15:02.479390] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5666 (0.5815)  acc1: 79.6875 (81.8106)  acc5: 100.0000 (99.1770)  time: 0.0283  data: 0.0002  max mem: 5511
[23:15:02.768919] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6049 (0.5815)  acc1: 81.2500 (81.8816)  acc5: 98.4375 (99.1467)  time: 0.0286  data: 0.0002  max mem: 5511
[23:15:03.053803] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6040 (0.5813)  acc1: 82.8125 (81.9226)  acc5: 98.4375 (99.1308)  time: 0.0285  data: 0.0002  max mem: 5511
[23:15:03.206987] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5692 (0.5801)  acc1: 81.2500 (81.9200)  acc5: 98.4375 (99.1400)  time: 0.0274  data: 0.0002  max mem: 5511
[23:15:03.366418] Test: Total time: 0:00:05 (0.0336 s / it)
[23:15:03.366927] * Acc@1 81.920 Acc@5 99.140 loss 0.580
[23:15:03.367294] Accuracy of the network on the 10000 test images: 81.9%
[23:15:03.367559] Max accuracy: 82.20%
[23:15:03.612511] log_dir: ./output_dir
[23:15:04.523819] Epoch: [53]  [  0/781]  eta: 0:11:50  lr: 0.000123  training_loss: 1.2319 (1.2319)  mae_loss: 0.0280 (0.0280)  classification_loss: 1.2037 (1.2037)  loss_mask: 0.0002 (0.0002)  time: 0.9095  data: 0.6832  max mem: 5511
[23:15:08.455062] Epoch: [53]  [ 20/781]  eta: 0:02:55  lr: 0.000123  training_loss: 1.2813 (1.2756)  mae_loss: 0.0259 (0.0271)  classification_loss: 1.2539 (1.2476)  loss_mask: 0.0007 (0.0008)  time: 0.1965  data: 0.0002  max mem: 5511
[23:15:12.447474] Epoch: [53]  [ 40/781]  eta: 0:02:39  lr: 0.000123  training_loss: 1.2919 (1.2920)  mae_loss: 0.0256 (0.0267)  classification_loss: 1.2655 (1.2646)  loss_mask: 0.0006 (0.0007)  time: 0.1995  data: 0.0002  max mem: 5511
[23:15:16.430101] Epoch: [53]  [ 60/781]  eta: 0:02:31  lr: 0.000123  training_loss: 1.2697 (1.2990)  mae_loss: 0.0267 (0.0270)  classification_loss: 1.2449 (1.2712)  loss_mask: 0.0006 (0.0009)  time: 0.1990  data: 0.0002  max mem: 5511
[23:15:20.353493] Epoch: [53]  [ 80/781]  eta: 0:02:24  lr: 0.000123  training_loss: 1.2844 (1.3001)  mae_loss: 0.0269 (0.0269)  classification_loss: 1.2578 (1.2723)  loss_mask: 0.0007 (0.0009)  time: 0.1961  data: 0.0002  max mem: 5511
[23:15:24.254020] Epoch: [53]  [100/781]  eta: 0:02:19  lr: 0.000123  training_loss: 1.3053 (1.3036)  mae_loss: 0.0265 (0.0270)  classification_loss: 1.2792 (1.2757)  loss_mask: 0.0005 (0.0009)  time: 0.1949  data: 0.0002  max mem: 5511
[23:15:28.227694] Epoch: [53]  [120/781]  eta: 0:02:14  lr: 0.000123  training_loss: 1.2850 (1.3016)  mae_loss: 0.0254 (0.0267)  classification_loss: 1.2590 (1.2737)  loss_mask: 0.0018 (0.0011)  time: 0.1986  data: 0.0003  max mem: 5511
[23:15:32.174395] Epoch: [53]  [140/781]  eta: 0:02:09  lr: 0.000123  training_loss: 1.2421 (1.2953)  mae_loss: 0.0277 (0.0268)  classification_loss: 1.2140 (1.2674)  loss_mask: 0.0010 (0.0012)  time: 0.1972  data: 0.0002  max mem: 5511
[23:15:36.109521] Epoch: [53]  [160/781]  eta: 0:02:05  lr: 0.000123  training_loss: 1.2971 (1.2978)  mae_loss: 0.0259 (0.0268)  classification_loss: 1.2662 (1.2698)  loss_mask: 0.0016 (0.0013)  time: 0.1967  data: 0.0003  max mem: 5511
[23:15:40.043321] Epoch: [53]  [180/781]  eta: 0:02:00  lr: 0.000122  training_loss: 1.3265 (1.3005)  mae_loss: 0.0269 (0.0268)  classification_loss: 1.2943 (1.2724)  loss_mask: 0.0009 (0.0013)  time: 0.1966  data: 0.0002  max mem: 5511
[23:15:44.014756] Epoch: [53]  [200/781]  eta: 0:01:56  lr: 0.000122  training_loss: 1.3260 (1.3000)  mae_loss: 0.0267 (0.0268)  classification_loss: 1.3021 (1.2718)  loss_mask: 0.0005 (0.0013)  time: 0.1985  data: 0.0003  max mem: 5511
[23:15:47.952870] Epoch: [53]  [220/781]  eta: 0:01:52  lr: 0.000122  training_loss: 1.2744 (1.2978)  mae_loss: 0.0256 (0.0268)  classification_loss: 1.2489 (1.2697)  loss_mask: 0.0005 (0.0013)  time: 0.1968  data: 0.0002  max mem: 5511
[23:15:51.869714] Epoch: [53]  [240/781]  eta: 0:01:48  lr: 0.000122  training_loss: 1.2651 (1.2974)  mae_loss: 0.0263 (0.0268)  classification_loss: 1.2406 (1.2693)  loss_mask: 0.0007 (0.0013)  time: 0.1958  data: 0.0002  max mem: 5511
[23:15:55.791745] Epoch: [53]  [260/781]  eta: 0:01:44  lr: 0.000122  training_loss: 1.2669 (1.2969)  mae_loss: 0.0269 (0.0268)  classification_loss: 1.2431 (1.2688)  loss_mask: 0.0006 (0.0012)  time: 0.1960  data: 0.0003  max mem: 5511
[23:15:59.717312] Epoch: [53]  [280/781]  eta: 0:01:39  lr: 0.000122  training_loss: 1.2727 (1.2953)  mae_loss: 0.0276 (0.0269)  classification_loss: 1.2480 (1.2671)  loss_mask: 0.0013 (0.0013)  time: 0.1961  data: 0.0002  max mem: 5511
[23:16:03.621316] Epoch: [53]  [300/781]  eta: 0:01:35  lr: 0.000122  training_loss: 1.2870 (1.2950)  mae_loss: 0.0277 (0.0269)  classification_loss: 1.2610 (1.2666)  loss_mask: 0.0023 (0.0015)  time: 0.1951  data: 0.0002  max mem: 5511
[23:16:07.544057] Epoch: [53]  [320/781]  eta: 0:01:31  lr: 0.000122  training_loss: 1.2786 (1.2947)  mae_loss: 0.0255 (0.0269)  classification_loss: 1.2496 (1.2663)  loss_mask: 0.0010 (0.0015)  time: 0.1960  data: 0.0002  max mem: 5511
[23:16:11.508208] Epoch: [53]  [340/781]  eta: 0:01:27  lr: 0.000122  training_loss: 1.2498 (1.2938)  mae_loss: 0.0271 (0.0269)  classification_loss: 1.2234 (1.2654)  loss_mask: 0.0010 (0.0016)  time: 0.1981  data: 0.0002  max mem: 5511
[23:16:15.440518] Epoch: [53]  [360/781]  eta: 0:01:23  lr: 0.000122  training_loss: 1.3243 (1.2962)  mae_loss: 0.0260 (0.0269)  classification_loss: 1.2984 (1.2678)  loss_mask: 0.0012 (0.0016)  time: 0.1965  data: 0.0002  max mem: 5511
[23:16:19.366572] Epoch: [53]  [380/781]  eta: 0:01:19  lr: 0.000121  training_loss: 1.2517 (1.2957)  mae_loss: 0.0262 (0.0268)  classification_loss: 1.2249 (1.2673)  loss_mask: 0.0006 (0.0016)  time: 0.1962  data: 0.0002  max mem: 5511
[23:16:23.312981] Epoch: [53]  [400/781]  eta: 0:01:15  lr: 0.000121  training_loss: 1.2833 (1.2954)  mae_loss: 0.0262 (0.0268)  classification_loss: 1.2583 (1.2671)  loss_mask: 0.0007 (0.0015)  time: 0.1972  data: 0.0002  max mem: 5511
[23:16:27.256807] Epoch: [53]  [420/781]  eta: 0:01:11  lr: 0.000121  training_loss: 1.2749 (1.2946)  mae_loss: 0.0268 (0.0268)  classification_loss: 1.2462 (1.2664)  loss_mask: 0.0005 (0.0015)  time: 0.1971  data: 0.0002  max mem: 5511

[23:16:31.186213] Epoch: [53]  [440/781]  eta: 0:01:07  lr: 0.000121  training_loss: 1.2934 (1.2948)  mae_loss: 0.0266 (0.0268)  classification_loss: 1.2656 (1.2666)  loss_mask: 0.0004 (0.0014)  time: 0.1964  data: 0.0002  max mem: 5511
[23:16:35.139125] Epoch: [53]  [460/781]  eta: 0:01:03  lr: 0.000121  training_loss: 1.2621 (1.2947)  mae_loss: 0.0264 (0.0268)  classification_loss: 1.2338 (1.2665)  loss_mask: 0.0003 (0.0014)  time: 0.1976  data: 0.0003  max mem: 5511
[23:16:39.073269] Epoch: [53]  [480/781]  eta: 0:00:59  lr: 0.000121  training_loss: 1.2861 (1.2958)  mae_loss: 0.0269 (0.0268)  classification_loss: 1.2590 (1.2676)  loss_mask: 0.0004 (0.0013)  time: 0.1966  data: 0.0002  max mem: 5511
[23:16:43.019629] Epoch: [53]  [500/781]  eta: 0:00:55  lr: 0.000121  training_loss: 1.3223 (1.2977)  mae_loss: 0.0266 (0.0268)  classification_loss: 1.2962 (1.2696)  loss_mask: 0.0003 (0.0013)  time: 0.1972  data: 0.0002  max mem: 5511
[23:16:46.947350] Epoch: [53]  [520/781]  eta: 0:00:51  lr: 0.000121  training_loss: 1.3058 (1.2980)  mae_loss: 0.0276 (0.0268)  classification_loss: 1.2704 (1.2698)  loss_mask: 0.0006 (0.0013)  time: 0.1963  data: 0.0002  max mem: 5511
[23:16:50.874684] Epoch: [53]  [540/781]  eta: 0:00:47  lr: 0.000121  training_loss: 1.3049 (1.2983)  mae_loss: 0.0262 (0.0268)  classification_loss: 1.2739 (1.2699)  loss_mask: 0.0041 (0.0015)  time: 0.1963  data: 0.0002  max mem: 5511
[23:16:54.810264] Epoch: [53]  [560/781]  eta: 0:00:43  lr: 0.000120  training_loss: 1.3147 (1.2988)  mae_loss: 0.0265 (0.0268)  classification_loss: 1.2780 (1.2701)  loss_mask: 0.0073 (0.0019)  time: 0.1967  data: 0.0002  max mem: 5511
[23:16:58.729602] Epoch: [53]  [580/781]  eta: 0:00:39  lr: 0.000120  training_loss: 1.2723 (1.2982)  mae_loss: 0.0287 (0.0269)  classification_loss: 1.2390 (1.2692)  loss_mask: 0.0061 (0.0020)  time: 0.1959  data: 0.0003  max mem: 5511
[23:17:02.683283] Epoch: [53]  [600/781]  eta: 0:00:35  lr: 0.000120  training_loss: 1.2880 (1.2978)  mae_loss: 0.0269 (0.0269)  classification_loss: 1.2576 (1.2688)  loss_mask: 0.0018 (0.0020)  time: 0.1976  data: 0.0002  max mem: 5511
[23:17:06.613768] Epoch: [53]  [620/781]  eta: 0:00:31  lr: 0.000120  training_loss: 1.2489 (1.2973)  mae_loss: 0.0268 (0.0269)  classification_loss: 1.2239 (1.2683)  loss_mask: 0.0022 (0.0021)  time: 0.1964  data: 0.0002  max mem: 5511
[23:17:10.566143] Epoch: [53]  [640/781]  eta: 0:00:27  lr: 0.000120  training_loss: 1.2181 (1.2957)  mae_loss: 0.0268 (0.0269)  classification_loss: 1.1945 (1.2667)  loss_mask: 0.0012 (0.0021)  time: 0.1975  data: 0.0002  max mem: 5511
[23:17:14.550700] Epoch: [53]  [660/781]  eta: 0:00:23  lr: 0.000120  training_loss: 1.2725 (1.2956)  mae_loss: 0.0257 (0.0269)  classification_loss: 1.2509 (1.2665)  loss_mask: 0.0020 (0.0022)  time: 0.1991  data: 0.0002  max mem: 5511
[23:17:18.498911] Epoch: [53]  [680/781]  eta: 0:00:19  lr: 0.000120  training_loss: 1.3260 (1.2962)  mae_loss: 0.0268 (0.0269)  classification_loss: 1.2954 (1.2672)  loss_mask: 0.0010 (0.0022)  time: 0.1973  data: 0.0006  max mem: 5511
[23:17:22.419448] Epoch: [53]  [700/781]  eta: 0:00:16  lr: 0.000120  training_loss: 1.3235 (1.2979)  mae_loss: 0.0269 (0.0269)  classification_loss: 1.2904 (1.2688)  loss_mask: 0.0007 (0.0021)  time: 0.1959  data: 0.0002  max mem: 5511
[23:17:26.353741] Epoch: [53]  [720/781]  eta: 0:00:12  lr: 0.000120  training_loss: 1.3468 (1.2987)  mae_loss: 0.0269 (0.0269)  classification_loss: 1.3126 (1.2697)  loss_mask: 0.0007 (0.0021)  time: 0.1966  data: 0.0002  max mem: 5511
[23:17:30.309849] Epoch: [53]  [740/781]  eta: 0:00:08  lr: 0.000120  training_loss: 1.3034 (1.2985)  mae_loss: 0.0261 (0.0269)  classification_loss: 1.2754 (1.2695)  loss_mask: 0.0014 (0.0021)  time: 0.1977  data: 0.0002  max mem: 5511
[23:17:34.310678] Epoch: [53]  [760/781]  eta: 0:00:04  lr: 0.000119  training_loss: 1.3384 (1.2998)  mae_loss: 0.0254 (0.0269)  classification_loss: 1.3038 (1.2705)  loss_mask: 0.0091 (0.0024)  time: 0.1999  data: 0.0002  max mem: 5511
[23:17:38.233751] Epoch: [53]  [780/781]  eta: 0:00:00  lr: 0.000119  training_loss: 1.3436 (1.3003)  mae_loss: 0.0265 (0.0269)  classification_loss: 1.3060 (1.2709)  loss_mask: 0.0061 (0.0025)  time: 0.1961  data: 0.0002  max mem: 5511
[23:17:38.390176] Epoch: [53] Total time: 0:02:34 (0.1982 s / it)
[23:17:38.390830] Averaged stats: lr: 0.000119  training_loss: 1.3436 (1.3003)  mae_loss: 0.0265 (0.0269)  classification_loss: 1.3060 (1.2709)  loss_mask: 0.0061 (0.0025)
[23:17:39.011653] Test:  [  0/157]  eta: 0:01:36  testing_loss: 0.4793 (0.4793)  acc1: 81.2500 (81.2500)  acc5: 100.0000 (100.0000)  time: 0.6166  data: 0.5872  max mem: 5511
[23:17:39.309560] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5912 (0.5882)  acc1: 81.2500 (81.3920)  acc5: 100.0000 (99.1477)  time: 0.0830  data: 0.0537  max mem: 5511
[23:17:39.595297] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.5618 (0.5651)  acc1: 82.8125 (82.1429)  acc5: 98.4375 (99.1815)  time: 0.0290  data: 0.0003  max mem: 5511
[23:17:39.881354] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5571 (0.5892)  acc1: 81.2500 (81.1996)  acc5: 98.4375 (98.9919)  time: 0.0284  data: 0.0002  max mem: 5511
[23:17:40.166949] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5938 (0.5875)  acc1: 79.6875 (81.3262)  acc5: 98.4375 (99.0091)  time: 0.0284  data: 0.0002  max mem: 5511
[23:17:40.450335] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5677 (0.5786)  acc1: 82.8125 (81.6176)  acc5: 100.0000 (99.0502)  time: 0.0283  data: 0.0002  max mem: 5511
[23:17:40.735223] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5358 (0.5729)  acc1: 82.8125 (81.7623)  acc5: 100.0000 (99.0266)  time: 0.0283  data: 0.0002  max mem: 5511
[23:17:41.022515] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5230 (0.5686)  acc1: 82.8125 (82.1963)  acc5: 100.0000 (99.0757)  time: 0.0284  data: 0.0002  max mem: 5511
[23:17:41.306287] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5520 (0.5769)  acc1: 82.8125 (81.9252)  acc5: 100.0000 (99.0741)  time: 0.0284  data: 0.0002  max mem: 5511
[23:17:41.589897] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5520 (0.5739)  acc1: 82.8125 (82.0913)  acc5: 98.4375 (99.0556)  time: 0.0282  data: 0.0002  max mem: 5511
[23:17:41.873508] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5453 (0.5750)  acc1: 82.8125 (82.0390)  acc5: 98.4375 (98.9944)  time: 0.0282  data: 0.0002  max mem: 5511
[23:17:42.157181] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5830 (0.5772)  acc1: 81.2500 (82.0101)  acc5: 98.4375 (98.9583)  time: 0.0282  data: 0.0002  max mem: 5511
[23:17:42.441529] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6115 (0.5780)  acc1: 81.2500 (82.0506)  acc5: 98.4375 (98.9928)  time: 0.0283  data: 0.0002  max mem: 5511
[23:17:42.733176] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6027 (0.5788)  acc1: 81.2500 (81.9895)  acc5: 100.0000 (99.0219)  time: 0.0287  data: 0.0002  max mem: 5511
[23:17:43.017601] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5772 (0.5780)  acc1: 81.2500 (82.0257)  acc5: 100.0000 (99.0470)  time: 0.0287  data: 0.0002  max mem: 5511
[23:17:43.298494] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5836 (0.5778)  acc1: 82.8125 (82.0882)  acc5: 100.0000 (99.0377)  time: 0.0281  data: 0.0001  max mem: 5511
[23:17:43.449434] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5565 (0.5776)  acc1: 84.3750 (82.1200)  acc5: 98.4375 (99.0400)  time: 0.0271  data: 0.0001  max mem: 5511
[23:17:43.610163] Test: Total time: 0:00:05 (0.0332 s / it)
[23:17:43.611063] * Acc@1 82.120 Acc@5 99.040 loss 0.578
[23:17:43.611371] Accuracy of the network on the 10000 test images: 82.1%
[23:17:43.611573] Max accuracy: 82.20%
[23:17:43.949006] log_dir: ./output_dir
[23:17:44.812082] Epoch: [54]  [  0/781]  eta: 0:11:12  lr: 0.000119  training_loss: 1.2196 (1.2196)  mae_loss: 0.0295 (0.0295)  classification_loss: 1.1851 (1.1851)  loss_mask: 0.0050 (0.0050)  time: 0.8613  data: 0.6424  max mem: 5511
[23:17:48.774023] Epoch: [54]  [ 20/781]  eta: 0:02:54  lr: 0.000119  training_loss: 1.2716 (1.2781)  mae_loss: 0.0253 (0.0256)  classification_loss: 1.2370 (1.2476)  loss_mask: 0.0029 (0.0049)  time: 0.1980  data: 0.0002  max mem: 5511
[23:17:52.728283] Epoch: [54]  [ 40/781]  eta: 0:02:38  lr: 0.000119  training_loss: 1.2949 (1.2960)  mae_loss: 0.0274 (0.0267)  classification_loss: 1.2582 (1.2647)  loss_mask: 0.0031 (0.0046)  time: 0.1976  data: 0.0002  max mem: 5511
[23:17:56.664298] Epoch: [54]  [ 60/781]  eta: 0:02:30  lr: 0.000119  training_loss: 1.3307 (1.3116)  mae_loss: 0.0269 (0.0271)  classification_loss: 1.2988 (1.2796)  loss_mask: 0.0058 (0.0049)  time: 0.1967  data: 0.0002  max mem: 5511
[23:18:00.608817] Epoch: [54]  [ 80/781]  eta: 0:02:24  lr: 0.000119  training_loss: 1.2777 (1.3058)  mae_loss: 0.0267 (0.0271)  classification_loss: 1.2454 (1.2737)  loss_mask: 0.0047 (0.0050)  time: 0.1971  data: 0.0002  max mem: 5511
[23:18:04.532127] Epoch: [54]  [100/781]  eta: 0:02:18  lr: 0.000119  training_loss: 1.3263 (1.3086)  mae_loss: 0.0259 (0.0272)  classification_loss: 1.3014 (1.2771)  loss_mask: 0.0014 (0.0044)  time: 0.1961  data: 0.0003  max mem: 5511
[23:18:08.450518] Epoch: [54]  [120/781]  eta: 0:02:13  lr: 0.000119  training_loss: 1.2611 (1.3045)  mae_loss: 0.0247 (0.0270)  classification_loss: 1.2344 (1.2737)  loss_mask: 0.0012 (0.0039)  time: 0.1958  data: 0.0001  max mem: 5511
[23:18:12.402261] Epoch: [54]  [140/781]  eta: 0:02:09  lr: 0.000119  training_loss: 1.2764 (1.3019)  mae_loss: 0.0266 (0.0269)  classification_loss: 1.2456 (1.2714)  loss_mask: 0.0012 (0.0037)  time: 0.1975  data: 0.0002  max mem: 5511
[23:18:16.327657] Epoch: [54]  [160/781]  eta: 0:02:04  lr: 0.000118  training_loss: 1.3123 (1.3019)  mae_loss: 0.0275 (0.0270)  classification_loss: 1.2797 (1.2715)  loss_mask: 0.0013 (0.0035)  time: 0.1961  data: 0.0003  max mem: 5511
[23:18:20.264942] Epoch: [54]  [180/781]  eta: 0:02:00  lr: 0.000118  training_loss: 1.2686 (1.3006)  mae_loss: 0.0274 (0.0271)  classification_loss: 1.2407 (1.2701)  loss_mask: 0.0016 (0.0034)  time: 0.1968  data: 0.0004  max mem: 5511
[23:18:24.204479] Epoch: [54]  [200/781]  eta: 0:01:56  lr: 0.000118  training_loss: 1.3089 (1.3019)  mae_loss: 0.0245 (0.0269)  classification_loss: 1.2851 (1.2717)  loss_mask: 0.0017 (0.0032)  time: 0.1969  data: 0.0003  max mem: 5511
[23:18:28.156907] Epoch: [54]  [220/781]  eta: 0:01:52  lr: 0.000118  training_loss: 1.3277 (1.3060)  mae_loss: 0.0262 (0.0270)  classification_loss: 1.2977 (1.2760)  loss_mask: 0.0010 (0.0031)  time: 0.1975  data: 0.0002  max mem: 5511
[23:18:32.104120] Epoch: [54]  [240/781]  eta: 0:01:48  lr: 0.000118  training_loss: 1.3177 (1.3062)  mae_loss: 0.0258 (0.0269)  classification_loss: 1.2817 (1.2762)  loss_mask: 0.0019 (0.0031)  time: 0.1972  data: 0.0002  max mem: 5511
[23:18:36.033466] Epoch: [54]  [260/781]  eta: 0:01:43  lr: 0.000118  training_loss: 1.3375 (1.3068)  mae_loss: 0.0269 (0.0270)  classification_loss: 1.3000 (1.2766)  loss_mask: 0.0026 (0.0032)  time: 0.1964  data: 0.0004  max mem: 5511
[23:18:39.998430] Epoch: [54]  [280/781]  eta: 0:01:39  lr: 0.000118  training_loss: 1.3136 (1.3060)  mae_loss: 0.0268 (0.0270)  classification_loss: 1.2775 (1.2756)  loss_mask: 0.0029 (0.0034)  time: 0.1982  data: 0.0002  max mem: 5511
[23:18:43.931985] Epoch: [54]  [300/781]  eta: 0:01:35  lr: 0.000118  training_loss: 1.3212 (1.3063)  mae_loss: 0.0277 (0.0271)  classification_loss: 1.2876 (1.2757)  loss_mask: 0.0034 (0.0035)  time: 0.1966  data: 0.0002  max mem: 5511
[23:18:47.873120] Epoch: [54]  [320/781]  eta: 0:01:31  lr: 0.000118  training_loss: 1.3258 (1.3061)  mae_loss: 0.0263 (0.0271)  classification_loss: 1.2864 (1.2754)  loss_mask: 0.0026 (0.0036)  time: 0.1970  data: 0.0002  max mem: 5511
[23:18:51.807055] Epoch: [54]  [340/781]  eta: 0:01:27  lr: 0.000118  training_loss: 1.2766 (1.3046)  mae_loss: 0.0293 (0.0272)  classification_loss: 1.2426 (1.2739)  loss_mask: 0.0018 (0.0035)  time: 0.1966  data: 0.0002  max mem: 5511
[23:18:55.712710] Epoch: [54]  [360/781]  eta: 0:01:23  lr: 0.000117  training_loss: 1.2984 (1.3043)  mae_loss: 0.0277 (0.0272)  classification_loss: 1.2725 (1.2737)  loss_mask: 0.0014 (0.0034)  time: 0.1952  data: 0.0002  max mem: 5511
[23:18:59.637441] Epoch: [54]  [380/781]  eta: 0:01:19  lr: 0.000117  training_loss: 1.2925 (1.3035)  mae_loss: 0.0269 (0.0272)  classification_loss: 1.2680 (1.2730)  loss_mask: 0.0012 (0.0033)  time: 0.1961  data: 0.0002  max mem: 5511
[23:19:03.550465] Epoch: [54]  [400/781]  eta: 0:01:15  lr: 0.000117  training_loss: 1.2684 (1.3029)  mae_loss: 0.0251 (0.0271)  classification_loss: 1.2404 (1.2725)  loss_mask: 0.0023 (0.0033)  time: 0.1956  data: 0.0002  max mem: 5511
[23:19:07.497211] Epoch: [54]  [420/781]  eta: 0:01:11  lr: 0.000117  training_loss: 1.2867 (1.3029)  mae_loss: 0.0264 (0.0271)  classification_loss: 1.2570 (1.2725)  loss_mask: 0.0018 (0.0033)  time: 0.1973  data: 0.0003  max mem: 5511
[23:19:11.440193] Epoch: [54]  [440/781]  eta: 0:01:07  lr: 0.000117  training_loss: 1.2688 (1.3012)  mae_loss: 0.0292 (0.0271)  classification_loss: 1.2362 (1.2709)  loss_mask: 0.0010 (0.0032)  time: 0.1971  data: 0.0002  max mem: 5511
[23:19:15.436427] Epoch: [54]  [460/781]  eta: 0:01:03  lr: 0.000117  training_loss: 1.2390 (1.2979)  mae_loss: 0.0262 (0.0271)  classification_loss: 1.2150 (1.2677)  loss_mask: 0.0006 (0.0031)  time: 0.1997  data: 0.0002  max mem: 5511
[23:19:19.367637] Epoch: [54]  [480/781]  eta: 0:00:59  lr: 0.000117  training_loss: 1.2946 (1.2989)  mae_loss: 0.0254 (0.0270)  classification_loss: 1.2669 (1.2689)  loss_mask: 0.0007 (0.0030)  time: 0.1965  data: 0.0002  max mem: 5511
[23:19:23.306365] Epoch: [54]  [500/781]  eta: 0:00:55  lr: 0.000117  training_loss: 1.2636 (1.2983)  mae_loss: 0.0273 (0.0271)  classification_loss: 1.2345 (1.2683)  loss_mask: 0.0006 (0.0029)  time: 0.1968  data: 0.0002  max mem: 5511
[23:19:27.246222] Epoch: [54]  [520/781]  eta: 0:00:51  lr: 0.000117  training_loss: 1.2500 (1.2973)  mae_loss: 0.0272 (0.0271)  classification_loss: 1.2250 (1.2674)  loss_mask: 0.0005 (0.0028)  time: 0.1969  data: 0.0002  max mem: 5511
[23:19:31.177778] Epoch: [54]  [540/781]  eta: 0:00:47  lr: 0.000116  training_loss: 1.2797 (1.2979)  mae_loss: 0.0283 (0.0271)  classification_loss: 1.2513 (1.2681)  loss_mask: 0.0006 (0.0027)  time: 0.1965  data: 0.0002  max mem: 5511
[23:19:35.101987] Epoch: [54]  [560/781]  eta: 0:00:43  lr: 0.000116  training_loss: 1.2804 (1.2967)  mae_loss: 0.0259 (0.0271)  classification_loss: 1.2578 (1.2670)  loss_mask: 0.0005 (0.0027)  time: 0.1961  data: 0.0002  max mem: 5511
[23:19:39.051127] Epoch: [54]  [580/781]  eta: 0:00:39  lr: 0.000116  training_loss: 1.2988 (1.2961)  mae_loss: 0.0267 (0.0271)  classification_loss: 1.2693 (1.2664)  loss_mask: 0.0009 (0.0026)  time: 0.1974  data: 0.0002  max mem: 5511
[23:19:43.016015] Epoch: [54]  [600/781]  eta: 0:00:35  lr: 0.000116  training_loss: 1.2554 (1.2954)  mae_loss: 0.0272 (0.0271)  classification_loss: 1.2223 (1.2656)  loss_mask: 0.0025 (0.0027)  time: 0.1981  data: 0.0002  max mem: 5511
[23:19:46.963689] Epoch: [54]  [620/781]  eta: 0:00:31  lr: 0.000116  training_loss: 1.2630 (1.2950)  mae_loss: 0.0264 (0.0270)  classification_loss: 1.2313 (1.2652)  loss_mask: 0.0016 (0.0027)  time: 0.1973  data: 0.0002  max mem: 5511
[23:19:50.883229] Epoch: [54]  [640/781]  eta: 0:00:27  lr: 0.000116  training_loss: 1.2750 (1.2948)  mae_loss: 0.0265 (0.0271)  classification_loss: 1.2470 (1.2651)  loss_mask: 0.0008 (0.0027)  time: 0.1959  data: 0.0002  max mem: 5511
[23:19:54.815389] Epoch: [54]  [660/781]  eta: 0:00:23  lr: 0.000116  training_loss: 1.3122 (1.2958)  mae_loss: 0.0272 (0.0271)  classification_loss: 1.2801 (1.2661)  loss_mask: 0.0007 (0.0026)  time: 0.1965  data: 0.0002  max mem: 5511
[23:19:58.777104] Epoch: [54]  [680/781]  eta: 0:00:19  lr: 0.000116  training_loss: 1.3586 (1.2970)  mae_loss: 0.0258 (0.0271)  classification_loss: 1.3252 (1.2673)  loss_mask: 0.0011 (0.0026)  time: 0.1980  data: 0.0002  max mem: 5511
[23:20:02.752392] Epoch: [54]  [700/781]  eta: 0:00:16  lr: 0.000116  training_loss: 1.2751 (1.2966)  mae_loss: 0.0272 (0.0271)  classification_loss: 1.2501 (1.2670)  loss_mask: 0.0008 (0.0025)  time: 0.1987  data: 0.0002  max mem: 5511
[23:20:06.684837] Epoch: [54]  [720/781]  eta: 0:00:12  lr: 0.000116  training_loss: 1.3381 (1.2978)  mae_loss: 0.0268 (0.0271)  classification_loss: 1.3082 (1.2683)  loss_mask: 0.0005 (0.0025)  time: 0.1965  data: 0.0002  max mem: 5511
[23:20:10.598045] Epoch: [54]  [740/781]  eta: 0:00:08  lr: 0.000115  training_loss: 1.2778 (1.2979)  mae_loss: 0.0273 (0.0271)  classification_loss: 1.2569 (1.2684)  loss_mask: 0.0007 (0.0024)  time: 0.1956  data: 0.0002  max mem: 5511
[23:20:14.521038] Epoch: [54]  [760/781]  eta: 0:00:04  lr: 0.000115  training_loss: 1.3143 (1.2993)  mae_loss: 0.0253 (0.0271)  classification_loss: 1.2857 (1.2698)  loss_mask: 0.0006 (0.0024)  time: 0.1961  data: 0.0002  max mem: 5511
[23:20:18.429031] Epoch: [54]  [780/781]  eta: 0:00:00  lr: 0.000115  training_loss: 1.2998 (1.2996)  mae_loss: 0.0270 (0.0271)  classification_loss: 1.2723 (1.2701)  loss_mask: 0.0009 (0.0024)  time: 0.1953  data: 0.0002  max mem: 5511
[23:20:18.596433] Epoch: [54] Total time: 0:02:34 (0.1980 s / it)
[23:20:18.597069] Averaged stats: lr: 0.000115  training_loss: 1.2998 (1.2996)  mae_loss: 0.0270 (0.0271)  classification_loss: 1.2723 (1.2701)  loss_mask: 0.0009 (0.0024)
[23:20:19.242691] Test:  [  0/157]  eta: 0:01:40  testing_loss: 0.4781 (0.4781)  acc1: 89.0625 (89.0625)  acc5: 98.4375 (98.4375)  time: 0.6411  data: 0.6098  max mem: 5511
[23:20:19.525696] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5634 (0.5728)  acc1: 82.8125 (82.6705)  acc5: 100.0000 (99.2898)  time: 0.0838  data: 0.0556  max mem: 5511
[23:20:19.809171] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.5309 (0.5383)  acc1: 85.9375 (83.7054)  acc5: 100.0000 (99.3304)  time: 0.0282  data: 0.0002  max mem: 5511
[23:20:20.093342] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5446 (0.5565)  acc1: 82.8125 (82.9133)  acc5: 100.0000 (99.0423)  time: 0.0283  data: 0.0002  max mem: 5511
[23:20:20.377992] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5765 (0.5589)  acc1: 81.2500 (82.7363)  acc5: 98.4375 (99.0854)  time: 0.0283  data: 0.0002  max mem: 5511
[23:20:20.662119] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5513 (0.5537)  acc1: 82.8125 (83.0270)  acc5: 100.0000 (99.1728)  time: 0.0283  data: 0.0002  max mem: 5511
[23:20:20.947068] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5110 (0.5487)  acc1: 84.3750 (83.2223)  acc5: 100.0000 (99.2059)  time: 0.0283  data: 0.0002  max mem: 5511
[23:20:21.237178] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5094 (0.5409)  acc1: 85.9375 (83.7148)  acc5: 100.0000 (99.2077)  time: 0.0286  data: 0.0004  max mem: 5511
[23:20:21.525833] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5142 (0.5486)  acc1: 85.9375 (83.3526)  acc5: 100.0000 (99.1898)  time: 0.0288  data: 0.0004  max mem: 5511
[23:20:21.814616] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5391 (0.5460)  acc1: 82.8125 (83.4650)  acc5: 98.4375 (99.1243)  time: 0.0287  data: 0.0002  max mem: 5511
[23:20:22.100292] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5380 (0.5497)  acc1: 79.6875 (83.1683)  acc5: 100.0000 (99.1182)  time: 0.0286  data: 0.0002  max mem: 5511
[23:20:22.390244] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5882 (0.5496)  acc1: 79.6875 (83.1363)  acc5: 100.0000 (99.1554)  time: 0.0286  data: 0.0002  max mem: 5511
[23:20:22.681744] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5466 (0.5476)  acc1: 81.2500 (83.1224)  acc5: 100.0000 (99.1736)  time: 0.0289  data: 0.0002  max mem: 5511
[23:20:22.969642] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5462 (0.5469)  acc1: 82.8125 (83.1465)  acc5: 100.0000 (99.2009)  time: 0.0288  data: 0.0003  max mem: 5511
[23:20:23.257404] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5408 (0.5465)  acc1: 82.8125 (83.1449)  acc5: 100.0000 (99.2132)  time: 0.0286  data: 0.0003  max mem: 5511
[23:20:23.540005] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5581 (0.5464)  acc1: 81.2500 (83.1333)  acc5: 100.0000 (99.2343)  time: 0.0284  data: 0.0001  max mem: 5511
[23:20:23.692479] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5195 (0.5454)  acc1: 84.3750 (83.1300)  acc5: 100.0000 (99.2400)  time: 0.0272  data: 0.0001  max mem: 5511
[23:20:23.841467] Test: Total time: 0:00:05 (0.0334 s / it)
[23:20:23.842395] * Acc@1 83.130 Acc@5 99.240 loss 0.545
[23:20:23.842741] Accuracy of the network on the 10000 test images: 83.1%
[23:20:23.842916] Max accuracy: 83.13%
[23:20:24.031363] log_dir: ./output_dir
[23:20:24.920118] Epoch: [55]  [  0/781]  eta: 0:11:32  lr: 0.000115  training_loss: 1.2888 (1.2888)  mae_loss: 0.0247 (0.0247)  classification_loss: 1.2630 (1.2630)  loss_mask: 0.0011 (0.0011)  time: 0.8868  data: 0.6757  max mem: 5511
[23:20:28.892704] Epoch: [55]  [ 20/781]  eta: 0:02:56  lr: 0.000115  training_loss: 1.2349 (1.2793)  mae_loss: 0.0284 (0.0282)  classification_loss: 1.2077 (1.2499)  loss_mask: 0.0010 (0.0012)  time: 0.1985  data: 0.0002  max mem: 5511
[23:20:32.835462] Epoch: [55]  [ 40/781]  eta: 0:02:39  lr: 0.000115  training_loss: 1.3361 (1.2903)  mae_loss: 0.0266 (0.0277)  classification_loss: 1.3045 (1.2614)  loss_mask: 0.0007 (0.0011)  time: 0.1970  data: 0.0002  max mem: 5511
[23:20:36.774265] Epoch: [55]  [ 60/781]  eta: 0:02:30  lr: 0.000115  training_loss: 1.3321 (1.2935)  mae_loss: 0.0268 (0.0275)  classification_loss: 1.3039 (1.2645)  loss_mask: 0.0017 (0.0015)  time: 0.1969  data: 0.0002  max mem: 5511
[23:20:40.692876] Epoch: [55]  [ 80/781]  eta: 0:02:24  lr: 0.000115  training_loss: 1.2411 (1.2891)  mae_loss: 0.0259 (0.0274)  classification_loss: 1.2125 (1.2603)  loss_mask: 0.0006 (0.0014)  time: 0.1958  data: 0.0002  max mem: 5511
[23:20:44.609533] Epoch: [55]  [100/781]  eta: 0:02:18  lr: 0.000115  training_loss: 1.2802 (1.2883)  mae_loss: 0.0268 (0.0273)  classification_loss: 1.2478 (1.2597)  loss_mask: 0.0007 (0.0013)  time: 0.1958  data: 0.0002  max mem: 5511
[23:20:48.526348] Epoch: [55]  [120/781]  eta: 0:02:13  lr: 0.000115  training_loss: 1.2693 (1.2918)  mae_loss: 0.0261 (0.0272)  classification_loss: 1.2439 (1.2634)  loss_mask: 0.0008 (0.0013)  time: 0.1958  data: 0.0002  max mem: 5511
[23:20:52.450421] Epoch: [55]  [140/781]  eta: 0:02:09  lr: 0.000114  training_loss: 1.2553 (1.2876)  mae_loss: 0.0262 (0.0271)  classification_loss: 1.2302 (1.2592)  loss_mask: 0.0010 (0.0013)  time: 0.1961  data: 0.0002  max mem: 5511
[23:20:56.422396] Epoch: [55]  [160/781]  eta: 0:02:04  lr: 0.000114  training_loss: 1.2359 (1.2843)  mae_loss: 0.0264 (0.0270)  classification_loss: 1.2071 (1.2561)  loss_mask: 0.0008 (0.0013)  time: 0.1985  data: 0.0002  max mem: 5511
[23:21:00.342407] Epoch: [55]  [180/781]  eta: 0:02:00  lr: 0.000114  training_loss: 1.2516 (1.2824)  mae_loss: 0.0277 (0.0271)  classification_loss: 1.2205 (1.2541)  loss_mask: 0.0009 (0.0012)  time: 0.1959  data: 0.0002  max mem: 5511
[23:21:04.298853] Epoch: [55]  [200/781]  eta: 0:01:56  lr: 0.000114  training_loss: 1.2918 (1.2829)  mae_loss: 0.0265 (0.0270)  classification_loss: 1.2654 (1.2544)  loss_mask: 0.0014 (0.0015)  time: 0.1977  data: 0.0002  max mem: 5511
[23:21:08.270014] Epoch: [55]  [220/781]  eta: 0:01:52  lr: 0.000114  training_loss: 1.2712 (1.2855)  mae_loss: 0.0265 (0.0270)  classification_loss: 1.2388 (1.2570)  loss_mask: 0.0009 (0.0015)  time: 0.1984  data: 0.0002  max mem: 5511
[23:21:12.197148] Epoch: [55]  [240/781]  eta: 0:01:48  lr: 0.000114  training_loss: 1.2788 (1.2849)  mae_loss: 0.0276 (0.0270)  classification_loss: 1.2469 (1.2564)  loss_mask: 0.0009 (0.0015)  time: 0.1963  data: 0.0002  max mem: 5511
[23:21:16.129925] Epoch: [55]  [260/781]  eta: 0:01:43  lr: 0.000114  training_loss: 1.3109 (1.2865)  mae_loss: 0.0270 (0.0270)  classification_loss: 1.2888 (1.2581)  loss_mask: 0.0005 (0.0014)  time: 0.1966  data: 0.0002  max mem: 5511
[23:21:20.061427] Epoch: [55]  [280/781]  eta: 0:01:39  lr: 0.000114  training_loss: 1.2446 (1.2852)  mae_loss: 0.0271 (0.0271)  classification_loss: 1.2154 (1.2568)  loss_mask: 0.0005 (0.0014)  time: 0.1965  data: 0.0002  max mem: 5511
[23:21:23.981451] Epoch: [55]  [300/781]  eta: 0:01:35  lr: 0.000114  training_loss: 1.2982 (1.2862)  mae_loss: 0.0260 (0.0270)  classification_loss: 1.2724 (1.2578)  loss_mask: 0.0005 (0.0013)  time: 0.1959  data: 0.0002  max mem: 5511
[23:21:27.922985] Epoch: [55]  [320/781]  eta: 0:01:31  lr: 0.000114  training_loss: 1.2878 (1.2873)  mae_loss: 0.0278 (0.0271)  classification_loss: 1.2558 (1.2589)  loss_mask: 0.0007 (0.0013)  time: 0.1970  data: 0.0002  max mem: 5511
[23:21:31.822742] Epoch: [55]  [340/781]  eta: 0:01:27  lr: 0.000113  training_loss: 1.2563 (1.2861)  mae_loss: 0.0267 (0.0271)  classification_loss: 1.2255 (1.2577)  loss_mask: 0.0007 (0.0013)  time: 0.1949  data: 0.0002  max mem: 5511
[23:21:35.749778] Epoch: [55]  [360/781]  eta: 0:01:23  lr: 0.000113  training_loss: 1.2955 (1.2868)  mae_loss: 0.0264 (0.0271)  classification_loss: 1.2689 (1.2584)  loss_mask: 0.0003 (0.0012)  time: 0.1963  data: 0.0003  max mem: 5511
[23:21:39.689428] Epoch: [55]  [380/781]  eta: 0:01:19  lr: 0.000113  training_loss: 1.2372 (1.2863)  mae_loss: 0.0267 (0.0271)  classification_loss: 1.2147 (1.2580)  loss_mask: 0.0003 (0.0012)  time: 0.1969  data: 0.0002  max mem: 5511
[23:21:43.636302] Epoch: [55]  [400/781]  eta: 0:01:15  lr: 0.000113  training_loss: 1.2596 (1.2857)  mae_loss: 0.0268 (0.0271)  classification_loss: 1.2322 (1.2575)  loss_mask: 0.0003 (0.0012)  time: 0.1973  data: 0.0003  max mem: 5511
[23:21:47.605034] Epoch: [55]  [420/781]  eta: 0:01:11  lr: 0.000113  training_loss: 1.2447 (1.2852)  mae_loss: 0.0262 (0.0271)  classification_loss: 1.2198 (1.2569)  loss_mask: 0.0004 (0.0011)  time: 0.1983  data: 0.0003  max mem: 5511
[23:21:51.570072] Epoch: [55]  [440/781]  eta: 0:01:07  lr: 0.000113  training_loss: 1.2410 (1.2842)  mae_loss: 0.0260 (0.0271)  classification_loss: 1.2132 (1.2559)  loss_mask: 0.0011 (0.0012)  time: 0.1982  data: 0.0002  max mem: 5511
[23:21:55.502126] Epoch: [55]  [460/781]  eta: 0:01:03  lr: 0.000113  training_loss: 1.2417 (1.2824)  mae_loss: 0.0256 (0.0271)  classification_loss: 1.2157 (1.2542)  loss_mask: 0.0007 (0.0011)  time: 0.1965  data: 0.0002  max mem: 5511
[23:21:59.404943] Epoch: [55]  [480/781]  eta: 0:00:59  lr: 0.000113  training_loss: 1.3437 (1.2844)  mae_loss: 0.0260 (0.0271)  classification_loss: 1.3194 (1.2563)  loss_mask: 0.0004 (0.0011)  time: 0.1951  data: 0.0002  max mem: 5511
[23:22:03.347277] Epoch: [55]  [500/781]  eta: 0:00:55  lr: 0.000113  training_loss: 1.2768 (1.2851)  mae_loss: 0.0270 (0.0271)  classification_loss: 1.2474 (1.2569)  loss_mask: 0.0005 (0.0011)  time: 0.1970  data: 0.0002  max mem: 5511
[23:22:07.257285] Epoch: [55]  [520/781]  eta: 0:00:51  lr: 0.000112  training_loss: 1.2641 (1.2843)  mae_loss: 0.0263 (0.0271)  classification_loss: 1.2402 (1.2562)  loss_mask: 0.0004 (0.0011)  time: 0.1954  data: 0.0002  max mem: 5511
[23:22:11.173745] Epoch: [55]  [540/781]  eta: 0:00:47  lr: 0.000112  training_loss: 1.3408 (1.2859)  mae_loss: 0.0270 (0.0271)  classification_loss: 1.3136 (1.2577)  loss_mask: 0.0004 (0.0011)  time: 0.1957  data: 0.0002  max mem: 5511
[23:22:15.125562] Epoch: [55]  [560/781]  eta: 0:00:43  lr: 0.000112  training_loss: 1.2667 (1.2857)  mae_loss: 0.0271 (0.0271)  classification_loss: 1.2351 (1.2575)  loss_mask: 0.0006 (0.0011)  time: 0.1975  data: 0.0004  max mem: 5511
[23:22:19.056115] Epoch: [55]  [580/781]  eta: 0:00:39  lr: 0.000112  training_loss: 1.2930 (1.2866)  mae_loss: 0.0267 (0.0271)  classification_loss: 1.2601 (1.2579)  loss_mask: 0.0051 (0.0015)  time: 0.1964  data: 0.0002  max mem: 5511
[23:22:22.990558] Epoch: [55]  [600/781]  eta: 0:00:35  lr: 0.000112  training_loss: 1.2778 (1.2863)  mae_loss: 0.0272 (0.0271)  classification_loss: 1.2201 (1.2569)  loss_mask: 0.0171 (0.0023)  time: 0.1966  data: 0.0003  max mem: 5511
[23:22:26.940688] Epoch: [55]  [620/781]  eta: 0:00:31  lr: 0.000112  training_loss: 1.2871 (1.2870)  mae_loss: 0.0256 (0.0271)  classification_loss: 1.2591 (1.2574)  loss_mask: 0.0054 (0.0024)  time: 0.1974  data: 0.0002  max mem: 5511
[23:22:30.906129] Epoch: [55]  [640/781]  eta: 0:00:27  lr: 0.000112  training_loss: 1.3007 (1.2871)  mae_loss: 0.0265 (0.0271)  classification_loss: 1.2681 (1.2575)  loss_mask: 0.0033 (0.0025)  time: 0.1982  data: 0.0002  max mem: 5511
[23:22:34.839520] Epoch: [55]  [660/781]  eta: 0:00:23  lr: 0.000112  training_loss: 1.2985 (1.2873)  mae_loss: 0.0259 (0.0270)  classification_loss: 1.2715 (1.2578)  loss_mask: 0.0014 (0.0025)  time: 0.1966  data: 0.0002  max mem: 5511
[23:22:38.799914] Epoch: [55]  [680/781]  eta: 0:00:19  lr: 0.000112  training_loss: 1.3179 (1.2878)  mae_loss: 0.0292 (0.0271)  classification_loss: 1.2887 (1.2583)  loss_mask: 0.0010 (0.0024)  time: 0.1979  data: 0.0002  max mem: 5511
[23:22:42.766307] Epoch: [55]  [700/781]  eta: 0:00:16  lr: 0.000112  training_loss: 1.2953 (1.2877)  mae_loss: 0.0263 (0.0271)  classification_loss: 1.2704 (1.2582)  loss_mask: 0.0008 (0.0024)  time: 0.1982  data: 0.0002  max mem: 5511
[23:22:46.715660] Epoch: [55]  [720/781]  eta: 0:00:12  lr: 0.000111  training_loss: 1.3122 (1.2891)  mae_loss: 0.0274 (0.0271)  classification_loss: 1.2851 (1.2597)  loss_mask: 0.0010 (0.0023)  time: 0.1974  data: 0.0002  max mem: 5511
[23:22:50.639391] Epoch: [55]  [740/781]  eta: 0:00:08  lr: 0.000111  training_loss: 1.2423 (1.2881)  mae_loss: 0.0251 (0.0271)  classification_loss: 1.2105 (1.2587)  loss_mask: 0.0012 (0.0023)  time: 0.1960  data: 0.0002  max mem: 5511
[23:22:54.585453] Epoch: [55]  [760/781]  eta: 0:00:04  lr: 0.000111  training_loss: 1.2858 (1.2879)  mae_loss: 0.0255 (0.0271)  classification_loss: 1.2593 (1.2585)  loss_mask: 0.0009 (0.0023)  time: 0.1972  data: 0.0002  max mem: 5511
[23:22:58.509536] Epoch: [55]  [780/781]  eta: 0:00:00  lr: 0.000111  training_loss: 1.2535 (1.2874)  mae_loss: 0.0265 (0.0270)  classification_loss: 1.2266 (1.2581)  loss_mask: 0.0007 (0.0023)  time: 0.1961  data: 0.0002  max mem: 5511
[23:22:58.670737] Epoch: [55] Total time: 0:02:34 (0.1980 s / it)
[23:22:58.671198] Averaged stats: lr: 0.000111  training_loss: 1.2535 (1.2874)  mae_loss: 0.0265 (0.0270)  classification_loss: 1.2266 (1.2581)  loss_mask: 0.0007 (0.0023)
[23:22:59.326005] Test:  [  0/157]  eta: 0:01:42  testing_loss: 0.5999 (0.5999)  acc1: 81.2500 (81.2500)  acc5: 100.0000 (100.0000)  time: 0.6510  data: 0.6197  max mem: 5511
[23:22:59.613335] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5801 (0.6086)  acc1: 81.2500 (81.2500)  acc5: 100.0000 (99.0057)  time: 0.0851  data: 0.0565  max mem: 5511
[23:22:59.902084] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.5774 (0.5549)  acc1: 84.3750 (83.2589)  acc5: 100.0000 (99.3304)  time: 0.0286  data: 0.0003  max mem: 5511
[23:23:00.193421] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5508 (0.5811)  acc1: 79.6875 (81.6028)  acc5: 100.0000 (99.1935)  time: 0.0289  data: 0.0003  max mem: 5511
[23:23:00.490747] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6147 (0.5809)  acc1: 79.6875 (82.0884)  acc5: 98.4375 (98.9710)  time: 0.0293  data: 0.0002  max mem: 5511
[23:23:00.777834] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5675 (0.5770)  acc1: 84.3750 (82.2610)  acc5: 98.4375 (98.9583)  time: 0.0291  data: 0.0002  max mem: 5511
[23:23:01.066187] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5578 (0.5766)  acc1: 82.8125 (82.0953)  acc5: 100.0000 (99.0266)  time: 0.0286  data: 0.0002  max mem: 5511
[23:23:01.356520] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5466 (0.5687)  acc1: 82.8125 (82.3283)  acc5: 100.0000 (99.0317)  time: 0.0288  data: 0.0002  max mem: 5511
[23:23:01.646967] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5466 (0.5742)  acc1: 82.8125 (82.2531)  acc5: 98.4375 (98.9390)  time: 0.0289  data: 0.0002  max mem: 5511
[23:23:01.936404] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5570 (0.5713)  acc1: 82.8125 (82.4519)  acc5: 98.4375 (98.9011)  time: 0.0289  data: 0.0002  max mem: 5511
[23:23:02.229080] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5880 (0.5770)  acc1: 81.2500 (82.0854)  acc5: 98.4375 (98.9016)  time: 0.0289  data: 0.0002  max mem: 5511
[23:23:02.522156] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6090 (0.5791)  acc1: 79.6875 (81.9679)  acc5: 98.4375 (98.8598)  time: 0.0291  data: 0.0002  max mem: 5511
[23:23:02.810198] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5967 (0.5782)  acc1: 79.6875 (81.9344)  acc5: 98.4375 (98.8507)  time: 0.0289  data: 0.0002  max mem: 5511
[23:23:03.094711] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5658 (0.5773)  acc1: 81.2500 (81.9776)  acc5: 100.0000 (98.8550)  time: 0.0285  data: 0.0002  max mem: 5511
[23:23:03.380388] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5658 (0.5758)  acc1: 82.8125 (82.0146)  acc5: 100.0000 (98.8808)  time: 0.0284  data: 0.0002  max mem: 5511
[23:23:03.661177] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5846 (0.5753)  acc1: 81.2500 (82.0261)  acc5: 100.0000 (98.9031)  time: 0.0282  data: 0.0001  max mem: 5511
[23:23:03.812972] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5607 (0.5754)  acc1: 81.2500 (81.9900)  acc5: 98.4375 (98.8800)  time: 0.0271  data: 0.0001  max mem: 5511
[23:23:03.986797] Test: Total time: 0:00:05 (0.0338 s / it)
[23:23:03.987604] * Acc@1 81.990 Acc@5 98.880 loss 0.575
[23:23:03.987978] Accuracy of the network on the 10000 test images: 82.0%
[23:23:03.988180] Max accuracy: 83.13%
[23:23:04.355079] log_dir: ./output_dir
[23:23:05.236624] Epoch: [56]  [  0/781]  eta: 0:11:27  lr: 0.000111  training_loss: 1.1398 (1.1398)  mae_loss: 0.0319 (0.0319)  classification_loss: 1.1075 (1.1075)  loss_mask: 0.0005 (0.0005)  time: 0.8799  data: 0.6432  max mem: 5511
[23:23:09.184834] Epoch: [56]  [ 20/781]  eta: 0:02:54  lr: 0.000111  training_loss: 1.2271 (1.2524)  mae_loss: 0.0266 (0.0271)  classification_loss: 1.2020 (1.2245)  loss_mask: 0.0007 (0.0008)  time: 0.1973  data: 0.0002  max mem: 5511
[23:23:13.140396] Epoch: [56]  [ 40/781]  eta: 0:02:38  lr: 0.000111  training_loss: 1.2889 (1.2690)  mae_loss: 0.0244 (0.0264)  classification_loss: 1.2635 (1.2416)  loss_mask: 0.0006 (0.0010)  time: 0.1976  data: 0.0003  max mem: 5511
[23:23:17.088110] Epoch: [56]  [ 60/781]  eta: 0:02:30  lr: 0.000111  training_loss: 1.2286 (1.2729)  mae_loss: 0.0260 (0.0262)  classification_loss: 1.2050 (1.2458)  loss_mask: 0.0004 (0.0009)  time: 0.1973  data: 0.0004  max mem: 5511
[23:23:21.020958] Epoch: [56]  [ 80/781]  eta: 0:02:24  lr: 0.000111  training_loss: 1.2811 (1.2686)  mae_loss: 0.0268 (0.0264)  classification_loss: 1.2496 (1.2414)  loss_mask: 0.0003 (0.0007)  time: 0.1965  data: 0.0002  max mem: 5511
[23:23:24.959484] Epoch: [56]  [100/781]  eta: 0:02:18  lr: 0.000111  training_loss: 1.3076 (1.2740)  mae_loss: 0.0280 (0.0266)  classification_loss: 1.2833 (1.2466)  loss_mask: 0.0006 (0.0008)  time: 0.1968  data: 0.0002  max mem: 5511
[23:23:28.902468] Epoch: [56]  [120/781]  eta: 0:02:14  lr: 0.000110  training_loss: 1.2575 (1.2733)  mae_loss: 0.0250 (0.0264)  classification_loss: 1.2246 (1.2460)  loss_mask: 0.0005 (0.0008)  time: 0.1971  data: 0.0003  max mem: 5511
[23:23:32.841320] Epoch: [56]  [140/781]  eta: 0:02:09  lr: 0.000110  training_loss: 1.2539 (1.2711)  mae_loss: 0.0272 (0.0265)  classification_loss: 1.2265 (1.2439)  loss_mask: 0.0004 (0.0008)  time: 0.1968  data: 0.0002  max mem: 5511
[23:23:36.746422] Epoch: [56]  [160/781]  eta: 0:02:04  lr: 0.000110  training_loss: 1.2373 (1.2683)  mae_loss: 0.0270 (0.0265)  classification_loss: 1.2068 (1.2411)  loss_mask: 0.0004 (0.0007)  time: 0.1952  data: 0.0002  max mem: 5511
[23:23:40.677417] Epoch: [56]  [180/781]  eta: 0:02:00  lr: 0.000110  training_loss: 1.2461 (1.2671)  mae_loss: 0.0263 (0.0264)  classification_loss: 1.2227 (1.2400)  loss_mask: 0.0004 (0.0007)  time: 0.1965  data: 0.0002  max mem: 5511
[23:23:44.596549] Epoch: [56]  [200/781]  eta: 0:01:56  lr: 0.000110  training_loss: 1.2548 (1.2681)  mae_loss: 0.0256 (0.0264)  classification_loss: 1.2267 (1.2410)  loss_mask: 0.0004 (0.0007)  time: 0.1959  data: 0.0002  max mem: 5511
[23:23:48.509853] Epoch: [56]  [220/781]  eta: 0:01:52  lr: 0.000110  training_loss: 1.2404 (1.2683)  mae_loss: 0.0255 (0.0264)  classification_loss: 1.2138 (1.2412)  loss_mask: 0.0005 (0.0007)  time: 0.1956  data: 0.0002  max mem: 5511
[23:23:52.438637] Epoch: [56]  [240/781]  eta: 0:01:47  lr: 0.000110  training_loss: 1.2549 (1.2672)  mae_loss: 0.0252 (0.0264)  classification_loss: 1.2305 (1.2401)  loss_mask: 0.0006 (0.0008)  time: 0.1963  data: 0.0002  max mem: 5511
[23:23:56.353753] Epoch: [56]  [260/781]  eta: 0:01:43  lr: 0.000110  training_loss: 1.2816 (1.2676)  mae_loss: 0.0267 (0.0264)  classification_loss: 1.2508 (1.2403)  loss_mask: 0.0019 (0.0009)  time: 0.1957  data: 0.0002  max mem: 5511
[23:24:00.295500] Epoch: [56]  [280/781]  eta: 0:01:39  lr: 0.000110  training_loss: 1.3060 (1.2709)  mae_loss: 0.0275 (0.0265)  classification_loss: 1.2759 (1.2433)  loss_mask: 0.0016 (0.0010)  time: 0.1970  data: 0.0002  max mem: 5511
[23:24:04.227771] Epoch: [56]  [300/781]  eta: 0:01:35  lr: 0.000110  training_loss: 1.2314 (1.2686)  mae_loss: 0.0261 (0.0265)  classification_loss: 1.2068 (1.2411)  loss_mask: 0.0007 (0.0010)  time: 0.1965  data: 0.0002  max mem: 5511
[23:24:08.167683] Epoch: [56]  [320/781]  eta: 0:01:31  lr: 0.000109  training_loss: 1.2555 (1.2676)  mae_loss: 0.0263 (0.0265)  classification_loss: 1.2261 (1.2400)  loss_mask: 0.0007 (0.0010)  time: 0.1969  data: 0.0002  max mem: 5511
[23:24:12.162142] Epoch: [56]  [340/781]  eta: 0:01:27  lr: 0.000109  training_loss: 1.2493 (1.2668)  mae_loss: 0.0274 (0.0266)  classification_loss: 1.2178 (1.2392)  loss_mask: 0.0007 (0.0010)  time: 0.1996  data: 0.0002  max mem: 5511
[23:24:16.131408] Epoch: [56]  [360/781]  eta: 0:01:23  lr: 0.000109  training_loss: 1.3162 (1.2691)  mae_loss: 0.0264 (0.0266)  classification_loss: 1.2874 (1.2413)  loss_mask: 0.0013 (0.0012)  time: 0.1984  data: 0.0004  max mem: 5511
[23:24:20.041842] Epoch: [56]  [380/781]  eta: 0:01:19  lr: 0.000109  training_loss: 1.2339 (1.2692)  mae_loss: 0.0272 (0.0267)  classification_loss: 1.2070 (1.2413)  loss_mask: 0.0017 (0.0013)  time: 0.1954  data: 0.0002  max mem: 5511
[23:24:23.963938] Epoch: [56]  [400/781]  eta: 0:01:15  lr: 0.000109  training_loss: 1.2670 (1.2697)  mae_loss: 0.0264 (0.0266)  classification_loss: 1.2412 (1.2417)  loss_mask: 0.0018 (0.0014)  time: 0.1960  data: 0.0002  max mem: 5511

[23:24:27.908085] Epoch: [56]  [420/781]  eta: 0:01:11  lr: 0.000109  training_loss: 1.2565 (1.2698)  mae_loss: 0.0250 (0.0266)  classification_loss: 1.2319 (1.2417)  loss_mask: 0.0033 (0.0015)  time: 0.1971  data: 0.0002  max mem: 5511
[23:24:31.842151] Epoch: [56]  [440/781]  eta: 0:01:07  lr: 0.000109  training_loss: 1.2906 (1.2709)  mae_loss: 0.0267 (0.0266)  classification_loss: 1.2632 (1.2427)  loss_mask: 0.0027 (0.0016)  time: 0.1966  data: 0.0003  max mem: 5511
[23:24:35.815429] Epoch: [56]  [460/781]  eta: 0:01:03  lr: 0.000109  training_loss: 1.2426 (1.2700)  mae_loss: 0.0265 (0.0266)  classification_loss: 1.2198 (1.2418)  loss_mask: 0.0010 (0.0016)  time: 0.1986  data: 0.0002  max mem: 5511
[23:24:39.760178] Epoch: [56]  [480/781]  eta: 0:00:59  lr: 0.000109  training_loss: 1.2968 (1.2718)  mae_loss: 0.0266 (0.0266)  classification_loss: 1.2603 (1.2434)  loss_mask: 0.0031 (0.0017)  time: 0.1971  data: 0.0002  max mem: 5511
[23:24:43.708525] Epoch: [56]  [500/781]  eta: 0:00:55  lr: 0.000109  training_loss: 1.3611 (1.2749)  mae_loss: 0.0270 (0.0266)  classification_loss: 1.3250 (1.2463)  loss_mask: 0.0058 (0.0020)  time: 0.1973  data: 0.0002  max mem: 5511
[23:24:47.625717] Epoch: [56]  [520/781]  eta: 0:00:51  lr: 0.000108  training_loss: 1.2932 (1.2761)  mae_loss: 0.0268 (0.0266)  classification_loss: 1.2577 (1.2473)  loss_mask: 0.0026 (0.0021)  time: 0.1958  data: 0.0002  max mem: 5511
[23:24:51.538621] Epoch: [56]  [540/781]  eta: 0:00:47  lr: 0.000108  training_loss: 1.2861 (1.2769)  mae_loss: 0.0272 (0.0267)  classification_loss: 1.2536 (1.2480)  loss_mask: 0.0024 (0.0022)  time: 0.1956  data: 0.0002  max mem: 5511
[23:24:55.545166] Epoch: [56]  [560/781]  eta: 0:00:43  lr: 0.000108  training_loss: 1.2366 (1.2753)  mae_loss: 0.0265 (0.0267)  classification_loss: 1.2093 (1.2464)  loss_mask: 0.0013 (0.0022)  time: 0.2002  data: 0.0002  max mem: 5511
[23:24:59.501133] Epoch: [56]  [580/781]  eta: 0:00:39  lr: 0.000108  training_loss: 1.2513 (1.2748)  mae_loss: 0.0267 (0.0267)  classification_loss: 1.2229 (1.2459)  loss_mask: 0.0016 (0.0022)  time: 0.1977  data: 0.0002  max mem: 5511
[23:25:03.424221] Epoch: [56]  [600/781]  eta: 0:00:35  lr: 0.000108  training_loss: 1.2815 (1.2760)  mae_loss: 0.0271 (0.0267)  classification_loss: 1.2489 (1.2470)  loss_mask: 0.0016 (0.0022)  time: 0.1961  data: 0.0002  max mem: 5511
[23:25:07.349190] Epoch: [56]  [620/781]  eta: 0:00:31  lr: 0.000108  training_loss: 1.2989 (1.2761)  mae_loss: 0.0271 (0.0267)  classification_loss: 1.2652 (1.2472)  loss_mask: 0.0012 (0.0022)  time: 0.1961  data: 0.0002  max mem: 5511
[23:25:11.281817] Epoch: [56]  [640/781]  eta: 0:00:27  lr: 0.000108  training_loss: 1.3048 (1.2763)  mae_loss: 0.0253 (0.0267)  classification_loss: 1.2716 (1.2473)  loss_mask: 0.0012 (0.0023)  time: 0.1965  data: 0.0002  max mem: 5511
[23:25:15.204483] Epoch: [56]  [660/781]  eta: 0:00:23  lr: 0.000108  training_loss: 1.2764 (1.2766)  mae_loss: 0.0272 (0.0267)  classification_loss: 1.2515 (1.2477)  loss_mask: 0.0005 (0.0022)  time: 0.1960  data: 0.0002  max mem: 5511
[23:25:19.163888] Epoch: [56]  [680/781]  eta: 0:00:19  lr: 0.000108  training_loss: 1.2932 (1.2770)  mae_loss: 0.0259 (0.0267)  classification_loss: 1.2678 (1.2481)  loss_mask: 0.0005 (0.0022)  time: 0.1979  data: 0.0002  max mem: 5511
[23:25:23.087316] Epoch: [56]  [700/781]  eta: 0:00:16  lr: 0.000107  training_loss: 1.2546 (1.2768)  mae_loss: 0.0267 (0.0267)  classification_loss: 1.2260 (1.2480)  loss_mask: 0.0008 (0.0021)  time: 0.1961  data: 0.0002  max mem: 5511
[23:25:27.011605] Epoch: [56]  [720/781]  eta: 0:00:12  lr: 0.000107  training_loss: 1.3406 (1.2788)  mae_loss: 0.0262 (0.0267)  classification_loss: 1.3163 (1.2500)  loss_mask: 0.0005 (0.0021)  time: 0.1961  data: 0.0002  max mem: 5511
[23:25:30.917369] Epoch: [56]  [740/781]  eta: 0:00:08  lr: 0.000107  training_loss: 1.2472 (1.2784)  mae_loss: 0.0262 (0.0267)  classification_loss: 1.2164 (1.2496)  loss_mask: 0.0003 (0.0021)  time: 0.1952  data: 0.0003  max mem: 5511
[23:25:34.845459] Epoch: [56]  [760/781]  eta: 0:00:04  lr: 0.000107  training_loss: 1.3088 (1.2791)  mae_loss: 0.0266 (0.0267)  classification_loss: 1.2828 (1.2503)  loss_mask: 0.0004 (0.0020)  time: 0.1963  data: 0.0002  max mem: 5511
[23:25:38.754832] Epoch: [56]  [780/781]  eta: 0:00:00  lr: 0.000107  training_loss: 1.2776 (1.2797)  mae_loss: 0.0265 (0.0267)  classification_loss: 1.2424 (1.2510)  loss_mask: 0.0008 (0.0020)  time: 0.1954  data: 0.0002  max mem: 5511
[23:25:38.908872] Epoch: [56] Total time: 0:02:34 (0.1979 s / it)
[23:25:38.909350] Averaged stats: lr: 0.000107  training_loss: 1.2776 (1.2797)  mae_loss: 0.0265 (0.0267)  classification_loss: 1.2424 (1.2510)  loss_mask: 0.0008 (0.0020)
[23:25:39.584170] Test:  [  0/157]  eta: 0:01:45  testing_loss: 0.5595 (0.5595)  acc1: 84.3750 (84.3750)  acc5: 98.4375 (98.4375)  time: 0.6697  data: 0.6378  max mem: 5511
[23:25:39.880612] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5484 (0.5598)  acc1: 84.3750 (82.8125)  acc5: 100.0000 (99.1477)  time: 0.0877  data: 0.0581  max mem: 5511
[23:25:40.164208] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5321 (0.5253)  acc1: 82.8125 (84.1518)  acc5: 100.0000 (99.2560)  time: 0.0289  data: 0.0002  max mem: 5511
[23:25:40.455725] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5595 (0.5491)  acc1: 82.8125 (83.0645)  acc5: 98.4375 (98.9415)  time: 0.0286  data: 0.0002  max mem: 5511
[23:25:40.742578] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5595 (0.5521)  acc1: 82.8125 (83.1936)  acc5: 98.4375 (98.9329)  time: 0.0288  data: 0.0002  max mem: 5511
[23:25:41.033483] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5129 (0.5420)  acc1: 84.3750 (83.7316)  acc5: 100.0000 (98.9890)  time: 0.0288  data: 0.0002  max mem: 5511
[23:25:41.322285] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4933 (0.5393)  acc1: 84.3750 (83.7859)  acc5: 100.0000 (99.0266)  time: 0.0288  data: 0.0002  max mem: 5511
[23:25:41.625736] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4827 (0.5320)  acc1: 85.9375 (84.0009)  acc5: 100.0000 (99.0977)  time: 0.0294  data: 0.0002  max mem: 5511
[23:25:41.913507] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5238 (0.5380)  acc1: 84.3750 (83.8349)  acc5: 98.4375 (99.0162)  time: 0.0294  data: 0.0002  max mem: 5511
[23:25:42.198317] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5238 (0.5336)  acc1: 84.3750 (84.1003)  acc5: 98.4375 (99.0041)  time: 0.0285  data: 0.0002  max mem: 5511
[23:25:42.491513] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.5799 (0.5389)  acc1: 82.8125 (83.8954)  acc5: 98.4375 (99.0099)  time: 0.0287  data: 0.0002  max mem: 5511
[23:25:42.777581] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5860 (0.5397)  acc1: 81.2500 (83.8542)  acc5: 100.0000 (99.0287)  time: 0.0288  data: 0.0002  max mem: 5511
[23:25:43.063468] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5699 (0.5396)  acc1: 82.8125 (83.8585)  acc5: 100.0000 (99.0702)  time: 0.0284  data: 0.0002  max mem: 5511
[23:25:43.346772] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5207 (0.5393)  acc1: 82.8125 (83.8621)  acc5: 100.0000 (99.0339)  time: 0.0283  data: 0.0002  max mem: 5511
[23:25:43.632149] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5207 (0.5376)  acc1: 82.8125 (83.8763)  acc5: 100.0000 (99.0691)  time: 0.0283  data: 0.0002  max mem: 5511
[23:25:43.915815] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5625 (0.5386)  acc1: 82.8125 (83.8576)  acc5: 100.0000 (99.0894)  time: 0.0283  data: 0.0002  max mem: 5511
[23:25:44.070424] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5463 (0.5388)  acc1: 82.8125 (83.9000)  acc5: 100.0000 (99.0800)  time: 0.0274  data: 0.0002  max mem: 5511
[23:25:44.211268] Test: Total time: 0:00:05 (0.0337 s / it)
[23:25:44.211725] * Acc@1 83.900 Acc@5 99.080 loss 0.539
[23:25:44.212044] Accuracy of the network on the 10000 test images: 83.9%
[23:25:44.212243] Max accuracy: 83.90%
[23:25:44.457471] log_dir: ./output_dir
[23:25:45.305192] Epoch: [57]  [  0/781]  eta: 0:11:00  lr: 0.000107  training_loss: 1.4310 (1.4310)  mae_loss: 0.0251 (0.0251)  classification_loss: 1.3984 (1.3984)  loss_mask: 0.0075 (0.0075)  time: 0.8458  data: 0.6222  max mem: 5511
[23:25:49.238832] Epoch: [57]  [ 20/781]  eta: 0:02:53  lr: 0.000107  training_loss: 1.2297 (1.2632)  mae_loss: 0.0265 (0.0266)  classification_loss: 1.2019 (1.2330)  loss_mask: 0.0011 (0.0036)  time: 0.1966  data: 0.0002  max mem: 5511
[23:25:53.210762] Epoch: [57]  [ 40/781]  eta: 0:02:38  lr: 0.000107  training_loss: 1.2862 (1.2781)  mae_loss: 0.0261 (0.0264)  classification_loss: 1.2633 (1.2493)  loss_mask: 0.0006 (0.0024)  time: 0.1985  data: 0.0003  max mem: 5511
[23:25:57.157947] Epoch: [57]  [ 60/781]  eta: 0:02:30  lr: 0.000107  training_loss: 1.3172 (1.2914)  mae_loss: 0.0258 (0.0264)  classification_loss: 1.2873 (1.2631)  loss_mask: 0.0006 (0.0019)  time: 0.1973  data: 0.0002  max mem: 5511
[23:26:01.095731] Epoch: [57]  [ 80/781]  eta: 0:02:23  lr: 0.000107  training_loss: 1.2036 (1.2791)  mae_loss: 0.0253 (0.0264)  classification_loss: 1.1789 (1.2510)  loss_mask: 0.0005 (0.0016)  time: 0.1968  data: 0.0002  max mem: 5511
[23:26:05.043517] Epoch: [57]  [100/781]  eta: 0:02:18  lr: 0.000107  training_loss: 1.2763 (1.2804)  mae_loss: 0.0277 (0.0266)  classification_loss: 1.2497 (1.2523)  loss_mask: 0.0005 (0.0014)  time: 0.1973  data: 0.0002  max mem: 5511
[23:26:08.984002] Epoch: [57]  [120/781]  eta: 0:02:13  lr: 0.000106  training_loss: 1.2652 (1.2797)  mae_loss: 0.0248 (0.0265)  classification_loss: 1.2347 (1.2519)  loss_mask: 0.0006 (0.0013)  time: 0.1969  data: 0.0002  max mem: 5511
[23:26:12.916827] Epoch: [57]  [140/781]  eta: 0:02:09  lr: 0.000106  training_loss: 1.2343 (1.2744)  mae_loss: 0.0260 (0.0265)  classification_loss: 1.2081 (1.2466)  loss_mask: 0.0005 (0.0012)  time: 0.1966  data: 0.0002  max mem: 5511
[23:26:16.882997] Epoch: [57]  [160/781]  eta: 0:02:05  lr: 0.000106  training_loss: 1.2970 (1.2771)  mae_loss: 0.0253 (0.0265)  classification_loss: 1.2652 (1.2495)  loss_mask: 0.0004 (0.0011)  time: 0.1982  data: 0.0002  max mem: 5511
[23:26:20.830814] Epoch: [57]  [180/781]  eta: 0:02:00  lr: 0.000106  training_loss: 1.2794 (1.2780)  mae_loss: 0.0274 (0.0266)  classification_loss: 1.2557 (1.2502)  loss_mask: 0.0007 (0.0012)  time: 0.1973  data: 0.0002  max mem: 5511
[23:26:24.749126] Epoch: [57]  [200/781]  eta: 0:01:56  lr: 0.000106  training_loss: 1.2764 (1.2788)  mae_loss: 0.0260 (0.0266)  classification_loss: 1.2529 (1.2510)  loss_mask: 0.0004 (0.0012)  time: 0.1958  data: 0.0002  max mem: 5511
[23:26:28.703450] Epoch: [57]  [220/781]  eta: 0:01:52  lr: 0.000106  training_loss: 1.2615 (1.2749)  mae_loss: 0.0262 (0.0266)  classification_loss: 1.2185 (1.2464)  loss_mask: 0.0048 (0.0019)  time: 0.1976  data: 0.0005  max mem: 5511
[23:26:32.650899] Epoch: [57]  [240/781]  eta: 0:01:48  lr: 0.000106  training_loss: 1.2695 (1.2748)  mae_loss: 0.0270 (0.0267)  classification_loss: 1.2381 (1.2460)  loss_mask: 0.0022 (0.0021)  time: 0.1973  data: 0.0004  max mem: 5511
[23:26:36.579597] Epoch: [57]  [260/781]  eta: 0:01:43  lr: 0.000106  training_loss: 1.2465 (1.2732)  mae_loss: 0.0276 (0.0267)  classification_loss: 1.2177 (1.2444)  loss_mask: 0.0016 (0.0021)  time: 0.1964  data: 0.0002  max mem: 5511
[23:26:40.514196] Epoch: [57]  [280/781]  eta: 0:01:39  lr: 0.000106  training_loss: 1.2570 (1.2734)  mae_loss: 0.0260 (0.0266)  classification_loss: 1.2305 (1.2447)  loss_mask: 0.0007 (0.0020)  time: 0.1966  data: 0.0002  max mem: 5511
[23:26:44.448136] Epoch: [57]  [300/781]  eta: 0:01:35  lr: 0.000105  training_loss: 1.2776 (1.2749)  mae_loss: 0.0273 (0.0267)  classification_loss: 1.2522 (1.2463)  loss_mask: 0.0005 (0.0019)  time: 0.1966  data: 0.0002  max mem: 5511
[23:26:48.385258] Epoch: [57]  [320/781]  eta: 0:01:31  lr: 0.000105  training_loss: 1.2883 (1.2749)  mae_loss: 0.0258 (0.0266)  classification_loss: 1.2590 (1.2464)  loss_mask: 0.0005 (0.0018)  time: 0.1968  data: 0.0002  max mem: 5511
[23:26:52.326434] Epoch: [57]  [340/781]  eta: 0:01:27  lr: 0.000105  training_loss: 1.2836 (1.2762)  mae_loss: 0.0262 (0.0266)  classification_loss: 1.2574 (1.2478)  loss_mask: 0.0005 (0.0018)  time: 0.1970  data: 0.0002  max mem: 5511
[23:26:56.246451] Epoch: [57]  [360/781]  eta: 0:01:23  lr: 0.000105  training_loss: 1.2849 (1.2776)  mae_loss: 0.0260 (0.0266)  classification_loss: 1.2553 (1.2493)  loss_mask: 0.0003 (0.0017)  time: 0.1959  data: 0.0002  max mem: 5511
[23:27:00.187329] Epoch: [57]  [380/781]  eta: 0:01:19  lr: 0.000105  training_loss: 1.2865 (1.2783)  mae_loss: 0.0280 (0.0267)  classification_loss: 1.2596 (1.2500)  loss_mask: 0.0002 (0.0016)  time: 0.1970  data: 0.0002  max mem: 5511

[23:27:04.122400] Epoch: [57]  [400/781]  eta: 0:01:15  lr: 0.000105  training_loss: 1.2685 (1.2781)  mae_loss: 0.0260 (0.0267)  classification_loss: 1.2425 (1.2498)  loss_mask: 0.0004 (0.0016)  time: 0.1967  data: 0.0002  max mem: 5511
[23:27:08.106048] Epoch: [57]  [420/781]  eta: 0:01:11  lr: 0.000105  training_loss: 1.2801 (1.2779)  mae_loss: 0.0257 (0.0267)  classification_loss: 1.2533 (1.2496)  loss_mask: 0.0005 (0.0016)  time: 0.1991  data: 0.0004  max mem: 5511
[23:27:12.040363] Epoch: [57]  [440/781]  eta: 0:01:07  lr: 0.000105  training_loss: 1.2442 (1.2776)  mae_loss: 0.0264 (0.0267)  classification_loss: 1.2170 (1.2494)  loss_mask: 0.0009 (0.0015)  time: 0.1966  data: 0.0003  max mem: 5511
[23:27:15.958735] Epoch: [57]  [460/781]  eta: 0:01:03  lr: 0.000105  training_loss: 1.2340 (1.2762)  mae_loss: 0.0266 (0.0267)  classification_loss: 1.2116 (1.2480)  loss_mask: 0.0003 (0.0015)  time: 0.1958  data: 0.0002  max mem: 5511
[23:27:19.926979] Epoch: [57]  [480/781]  eta: 0:00:59  lr: 0.000105  training_loss: 1.3053 (1.2769)  mae_loss: 0.0259 (0.0267)  classification_loss: 1.2720 (1.2487)  loss_mask: 0.0003 (0.0014)  time: 0.1983  data: 0.0002  max mem: 5511
[23:27:23.882617] Epoch: [57]  [500/781]  eta: 0:00:55  lr: 0.000104  training_loss: 1.2710 (1.2774)  mae_loss: 0.0267 (0.0267)  classification_loss: 1.2486 (1.2492)  loss_mask: 0.0004 (0.0014)  time: 0.1977  data: 0.0003  max mem: 5511
[23:27:27.878103] Epoch: [57]  [520/781]  eta: 0:00:51  lr: 0.000104  training_loss: 1.3042 (1.2780)  mae_loss: 0.0275 (0.0268)  classification_loss: 1.2789 (1.2498)  loss_mask: 0.0004 (0.0014)  time: 0.1997  data: 0.0002  max mem: 5511
[23:27:31.815486] Epoch: [57]  [540/781]  eta: 0:00:47  lr: 0.000104  training_loss: 1.3011 (1.2789)  mae_loss: 0.0256 (0.0267)  classification_loss: 1.2740 (1.2507)  loss_mask: 0.0004 (0.0014)  time: 0.1968  data: 0.0002  max mem: 5511
[23:27:35.759104] Epoch: [57]  [560/781]  eta: 0:00:43  lr: 0.000104  training_loss: 1.2516 (1.2784)  mae_loss: 0.0261 (0.0267)  classification_loss: 1.2249 (1.2503)  loss_mask: 0.0005 (0.0014)  time: 0.1971  data: 0.0002  max mem: 5511
[23:27:39.674908] Epoch: [57]  [580/781]  eta: 0:00:39  lr: 0.000104  training_loss: 1.2343 (1.2770)  mae_loss: 0.0245 (0.0267)  classification_loss: 1.2086 (1.2489)  loss_mask: 0.0012 (0.0014)  time: 0.1957  data: 0.0002  max mem: 5511
[23:27:43.604072] Epoch: [57]  [600/781]  eta: 0:00:35  lr: 0.000104  training_loss: 1.1977 (1.2755)  mae_loss: 0.0264 (0.0267)  classification_loss: 1.1721 (1.2475)  loss_mask: 0.0005 (0.0014)  time: 0.1964  data: 0.0002  max mem: 5511
[23:27:47.561162] Epoch: [57]  [620/781]  eta: 0:00:31  lr: 0.000104  training_loss: 1.2925 (1.2755)  mae_loss: 0.0277 (0.0267)  classification_loss: 1.2644 (1.2475)  loss_mask: 0.0003 (0.0013)  time: 0.1978  data: 0.0002  max mem: 5511
[23:27:51.499942] Epoch: [57]  [640/781]  eta: 0:00:27  lr: 0.000104  training_loss: 1.2786 (1.2757)  mae_loss: 0.0272 (0.0267)  classification_loss: 1.2553 (1.2476)  loss_mask: 0.0003 (0.0013)  time: 0.1968  data: 0.0002  max mem: 5511
[23:27:55.479147] Epoch: [57]  [660/781]  eta: 0:00:23  lr: 0.000104  training_loss: 1.3207 (1.2763)  mae_loss: 0.0250 (0.0267)  classification_loss: 1.2924 (1.2483)  loss_mask: 0.0002 (0.0013)  time: 0.1989  data: 0.0002  max mem: 5511
[23:27:59.404953] Epoch: [57]  [680/781]  eta: 0:00:20  lr: 0.000104  training_loss: 1.3087 (1.2768)  mae_loss: 0.0262 (0.0267)  classification_loss: 1.2822 (1.2489)  loss_mask: 0.0003 (0.0012)  time: 0.1962  data: 0.0002  max mem: 5511
[23:28:03.367531] Epoch: [57]  [700/781]  eta: 0:00:16  lr: 0.000103  training_loss: 1.2425 (1.2757)  mae_loss: 0.0262 (0.0267)  classification_loss: 1.2161 (1.2478)  loss_mask: 0.0002 (0.0012)  time: 0.1980  data: 0.0002  max mem: 5511
[23:28:07.302661] Epoch: [57]  [720/781]  eta: 0:00:12  lr: 0.000103  training_loss: 1.2737 (1.2760)  mae_loss: 0.0275 (0.0267)  classification_loss: 1.2441 (1.2481)  loss_mask: 0.0003 (0.0012)  time: 0.1967  data: 0.0002  max mem: 5511
[23:28:11.234701] Epoch: [57]  [740/781]  eta: 0:00:08  lr: 0.000103  training_loss: 1.1936 (1.2745)  mae_loss: 0.0266 (0.0267)  classification_loss: 1.1673 (1.2466)  loss_mask: 0.0005 (0.0012)  time: 0.1965  data: 0.0002  max mem: 5511
[23:28:15.172276] Epoch: [57]  [760/781]  eta: 0:00:04  lr: 0.000103  training_loss: 1.3211 (1.2752)  mae_loss: 0.0252 (0.0267)  classification_loss: 1.2955 (1.2473)  loss_mask: 0.0005 (0.0012)  time: 0.1968  data: 0.0002  max mem: 5511
[23:28:19.083435] Epoch: [57]  [780/781]  eta: 0:00:00  lr: 0.000103  training_loss: 1.2415 (1.2753)  mae_loss: 0.0270 (0.0268)  classification_loss: 1.2165 (1.2474)  loss_mask: 0.0004 (0.0012)  time: 0.1955  data: 0.0002  max mem: 5511
[23:28:19.233198] Epoch: [57] Total time: 0:02:34 (0.1982 s / it)
[23:28:19.233679] Averaged stats: lr: 0.000103  training_loss: 1.2415 (1.2753)  mae_loss: 0.0270 (0.0268)  classification_loss: 1.2165 (1.2474)  loss_mask: 0.0004 (0.0012)
[23:28:19.897441] Test:  [  0/157]  eta: 0:01:43  testing_loss: 0.5559 (0.5559)  acc1: 82.8125 (82.8125)  acc5: 98.4375 (98.4375)  time: 0.6593  data: 0.6282  max mem: 5511
[23:28:20.185405] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5559 (0.5718)  acc1: 82.8125 (82.6705)  acc5: 98.4375 (99.0057)  time: 0.0860  data: 0.0573  max mem: 5511
[23:28:20.472484] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5410 (0.5402)  acc1: 82.8125 (84.0774)  acc5: 98.4375 (99.1815)  time: 0.0286  data: 0.0002  max mem: 5511
[23:28:20.760625] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5533 (0.5579)  acc1: 81.2500 (83.1149)  acc5: 100.0000 (98.9415)  time: 0.0286  data: 0.0002  max mem: 5511
[23:28:21.048175] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5546 (0.5538)  acc1: 82.8125 (83.3841)  acc5: 98.4375 (98.9710)  time: 0.0286  data: 0.0002  max mem: 5511
[23:28:21.336080] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5383 (0.5500)  acc1: 82.8125 (83.5172)  acc5: 100.0000 (98.9583)  time: 0.0286  data: 0.0002  max mem: 5511
[23:28:21.620976] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5079 (0.5442)  acc1: 84.3750 (83.7090)  acc5: 100.0000 (99.0266)  time: 0.0285  data: 0.0002  max mem: 5511
[23:28:21.909321] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5012 (0.5374)  acc1: 84.3750 (83.8688)  acc5: 100.0000 (99.0537)  time: 0.0285  data: 0.0002  max mem: 5511
[23:28:22.194696] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5153 (0.5443)  acc1: 82.8125 (83.5841)  acc5: 98.4375 (98.9969)  time: 0.0286  data: 0.0002  max mem: 5511
[23:28:22.488334] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5465 (0.5428)  acc1: 82.8125 (83.6882)  acc5: 98.4375 (98.9698)  time: 0.0288  data: 0.0003  max mem: 5511
[23:28:22.773465] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5807 (0.5492)  acc1: 82.8125 (83.4313)  acc5: 98.4375 (98.9171)  time: 0.0288  data: 0.0003  max mem: 5511
[23:28:23.059051] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6106 (0.5489)  acc1: 82.8125 (83.4882)  acc5: 98.4375 (98.9302)  time: 0.0284  data: 0.0002  max mem: 5511
[23:28:23.345407] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5514 (0.5476)  acc1: 82.8125 (83.3678)  acc5: 100.0000 (98.9540)  time: 0.0285  data: 0.0002  max mem: 5511
[23:28:23.630521] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5272 (0.5466)  acc1: 81.2500 (83.4089)  acc5: 100.0000 (98.9742)  time: 0.0285  data: 0.0002  max mem: 5511
[23:28:23.912728] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5288 (0.5445)  acc1: 84.3750 (83.4552)  acc5: 100.0000 (99.0027)  time: 0.0282  data: 0.0002  max mem: 5511
[23:28:24.191810] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5562 (0.5466)  acc1: 81.2500 (83.2678)  acc5: 100.0000 (99.0273)  time: 0.0280  data: 0.0001  max mem: 5511
[23:28:24.342478] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5607 (0.5463)  acc1: 81.2500 (83.3200)  acc5: 100.0000 (98.9900)  time: 0.0270  data: 0.0001  max mem: 5511
[23:28:24.503642] Test: Total time: 0:00:05 (0.0335 s / it)
[23:28:24.504174] * Acc@1 83.320 Acc@5 98.990 loss 0.546
[23:28:24.504489] Accuracy of the network on the 10000 test images: 83.3%
[23:28:24.504718] Max accuracy: 83.90%
[23:28:24.853498] log_dir: ./output_dir
[23:28:25.697997] Epoch: [58]  [  0/781]  eta: 0:10:58  lr: 0.000103  training_loss: 1.2354 (1.2354)  mae_loss: 0.0288 (0.0288)  classification_loss: 1.2065 (1.2065)  loss_mask: 0.0002 (0.0002)  time: 0.8427  data: 0.6155  max mem: 5511
[23:28:29.614080] Epoch: [58]  [ 20/781]  eta: 0:02:52  lr: 0.000103  training_loss: 1.2176 (1.2410)  mae_loss: 0.0252 (0.0260)  classification_loss: 1.1887 (1.2141)  loss_mask: 0.0005 (0.0008)  time: 0.1957  data: 0.0001  max mem: 5511
[23:28:33.581038] Epoch: [58]  [ 40/781]  eta: 0:02:37  lr: 0.000103  training_loss: 1.3004 (1.2721)  mae_loss: 0.0243 (0.0257)  classification_loss: 1.2763 (1.2458)  loss_mask: 0.0003 (0.0006)  time: 0.1983  data: 0.0002  max mem: 5511
[23:28:37.495480] Epoch: [58]  [ 60/781]  eta: 0:02:29  lr: 0.000103  training_loss: 1.3020 (1.2864)  mae_loss: 0.0273 (0.0264)  classification_loss: 1.2730 (1.2593)  loss_mask: 0.0003 (0.0007)  time: 0.1956  data: 0.0002  max mem: 5511
[23:28:41.444111] Epoch: [58]  [ 80/781]  eta: 0:02:23  lr: 0.000103  training_loss: 1.2828 (1.2808)  mae_loss: 0.0258 (0.0264)  classification_loss: 1.2573 (1.2537)  loss_mask: 0.0004 (0.0007)  time: 0.1973  data: 0.0002  max mem: 5511
[23:28:45.346156] Epoch: [58]  [100/781]  eta: 0:02:18  lr: 0.000102  training_loss: 1.2625 (1.2814)  mae_loss: 0.0246 (0.0263)  classification_loss: 1.2370 (1.2544)  loss_mask: 0.0005 (0.0007)  time: 0.1950  data: 0.0002  max mem: 5511
[23:28:49.308320] Epoch: [58]  [120/781]  eta: 0:02:13  lr: 0.000102  training_loss: 1.2239 (1.2781)  mae_loss: 0.0253 (0.0263)  classification_loss: 1.1969 (1.2511)  loss_mask: 0.0003 (0.0007)  time: 0.1980  data: 0.0002  max mem: 5511
[23:28:53.239465] Epoch: [58]  [140/781]  eta: 0:02:08  lr: 0.000102  training_loss: 1.2370 (1.2738)  mae_loss: 0.0260 (0.0263)  classification_loss: 1.2094 (1.2468)  loss_mask: 0.0002 (0.0007)  time: 0.1965  data: 0.0002  max mem: 5511
[23:28:57.171135] Epoch: [58]  [160/781]  eta: 0:02:04  lr: 0.000102  training_loss: 1.2217 (1.2674)  mae_loss: 0.0253 (0.0263)  classification_loss: 1.1918 (1.2404)  loss_mask: 0.0004 (0.0007)  time: 0.1965  data: 0.0002  max mem: 5511
[23:29:01.101607] Epoch: [58]  [180/781]  eta: 0:02:00  lr: 0.000102  training_loss: 1.2718 (1.2678)  mae_loss: 0.0266 (0.0264)  classification_loss: 1.2431 (1.2407)  loss_mask: 0.0003 (0.0007)  time: 0.1965  data: 0.0004  max mem: 5511
[23:29:05.015045] Epoch: [58]  [200/781]  eta: 0:01:56  lr: 0.000102  training_loss: 1.2708 (1.2710)  mae_loss: 0.0262 (0.0264)  classification_loss: 1.2445 (1.2439)  loss_mask: 0.0003 (0.0006)  time: 0.1956  data: 0.0003  max mem: 5511
[23:29:08.925085] Epoch: [58]  [220/781]  eta: 0:01:51  lr: 0.000102  training_loss: 1.2903 (1.2717)  mae_loss: 0.0252 (0.0264)  classification_loss: 1.2639 (1.2446)  loss_mask: 0.0003 (0.0006)  time: 0.1954  data: 0.0003  max mem: 5511
[23:29:12.875078] Epoch: [58]  [240/781]  eta: 0:01:47  lr: 0.000102  training_loss: 1.2408 (1.2715)  mae_loss: 0.0267 (0.0264)  classification_loss: 1.2120 (1.2444)  loss_mask: 0.0005 (0.0006)  time: 0.1974  data: 0.0002  max mem: 5511
[23:29:16.791800] Epoch: [58]  [260/781]  eta: 0:01:43  lr: 0.000102  training_loss: 1.2907 (1.2731)  mae_loss: 0.0279 (0.0265)  classification_loss: 1.2599 (1.2458)  loss_mask: 0.0010 (0.0007)  time: 0.1958  data: 0.0002  max mem: 5511
[23:29:20.715348] Epoch: [58]  [280/781]  eta: 0:01:39  lr: 0.000102  training_loss: 1.2797 (1.2719)  mae_loss: 0.0265 (0.0265)  classification_loss: 1.2531 (1.2446)  loss_mask: 0.0005 (0.0007)  time: 0.1961  data: 0.0002  max mem: 5511
[23:29:24.643581] Epoch: [58]  [300/781]  eta: 0:01:35  lr: 0.000101  training_loss: 1.2497 (1.2706)  mae_loss: 0.0257 (0.0265)  classification_loss: 1.2220 (1.2433)  loss_mask: 0.0005 (0.0007)  time: 0.1963  data: 0.0002  max mem: 5511
[23:29:28.602118] Epoch: [58]  [320/781]  eta: 0:01:31  lr: 0.000101  training_loss: 1.2728 (1.2709)  mae_loss: 0.0255 (0.0265)  classification_loss: 1.2424 (1.2437)  loss_mask: 0.0003 (0.0007)  time: 0.1978  data: 0.0002  max mem: 5511
[23:29:32.540718] Epoch: [58]  [340/781]  eta: 0:01:27  lr: 0.000101  training_loss: 1.2463 (1.2692)  mae_loss: 0.0264 (0.0266)  classification_loss: 1.2108 (1.2418)  loss_mask: 0.0005 (0.0007)  time: 0.1968  data: 0.0002  max mem: 5511
[23:29:36.444723] Epoch: [58]  [360/781]  eta: 0:01:23  lr: 0.000101  training_loss: 1.2462 (1.2692)  mae_loss: 0.0264 (0.0266)  classification_loss: 1.2126 (1.2418)  loss_mask: 0.0005 (0.0007)  time: 0.1951  data: 0.0003  max mem: 5511
[23:29:40.355899] Epoch: [58]  [380/781]  eta: 0:01:19  lr: 0.000101  training_loss: 1.2384 (1.2691)  mae_loss: 0.0262 (0.0266)  classification_loss: 1.2152 (1.2418)  loss_mask: 0.0004 (0.0007)  time: 0.1955  data: 0.0003  max mem: 5511
[23:29:44.310817] Epoch: [58]  [400/781]  eta: 0:01:15  lr: 0.000101  training_loss: 1.2860 (1.2692)  mae_loss: 0.0259 (0.0266)  classification_loss: 1.2576 (1.2419)  loss_mask: 0.0006 (0.0007)  time: 0.1976  data: 0.0002  max mem: 5511
[23:29:48.237617] Epoch: [58]  [420/781]  eta: 0:01:11  lr: 0.000101  training_loss: 1.2227 (1.2676)  mae_loss: 0.0259 (0.0266)  classification_loss: 1.1976 (1.2403)  loss_mask: 0.0003 (0.0007)  time: 0.1963  data: 0.0002  max mem: 5511
[23:29:52.163444] Epoch: [58]  [440/781]  eta: 0:01:07  lr: 0.000101  training_loss: 1.2614 (1.2668)  mae_loss: 0.0261 (0.0265)  classification_loss: 1.2361 (1.2395)  loss_mask: 0.0004 (0.0008)  time: 0.1962  data: 0.0005  max mem: 5511
[23:29:56.102074] Epoch: [58]  [460/781]  eta: 0:01:03  lr: 0.000101  training_loss: 1.2816 (1.2681)  mae_loss: 0.0264 (0.0266)  classification_loss: 1.2547 (1.2406)  loss_mask: 0.0030 (0.0010)  time: 0.1968  data: 0.0002  max mem: 5511
[23:30:00.037878] Epoch: [58]  [480/781]  eta: 0:00:59  lr: 0.000100  training_loss: 1.2675 (1.2680)  mae_loss: 0.0279 (0.0266)  classification_loss: 1.2278 (1.2402)  loss_mask: 0.0028 (0.0012)  time: 0.1967  data: 0.0002  max mem: 5511
[23:30:03.952458] Epoch: [58]  [500/781]  eta: 0:00:55  lr: 0.000100  training_loss: 1.2736 (1.2683)  mae_loss: 0.0271 (0.0266)  classification_loss: 1.2433 (1.2405)  loss_mask: 0.0009 (0.0012)  time: 0.1957  data: 0.0002  max mem: 5511
[23:30:07.873005] Epoch: [58]  [520/781]  eta: 0:00:51  lr: 0.000100  training_loss: 1.2803 (1.2691)  mae_loss: 0.0248 (0.0266)  classification_loss: 1.2493 (1.2412)  loss_mask: 0.0012 (0.0012)  time: 0.1959  data: 0.0002  max mem: 5511
[23:30:11.791551] Epoch: [58]  [540/781]  eta: 0:00:47  lr: 0.000100  training_loss: 1.2939 (1.2692)  mae_loss: 0.0252 (0.0266)  classification_loss: 1.2659 (1.2412)  loss_mask: 0.0024 (0.0013)  time: 0.1958  data: 0.0002  max mem: 5511
[23:30:15.702429] Epoch: [58]  [560/781]  eta: 0:00:43  lr: 0.000100  training_loss: 1.2419 (1.2683)  mae_loss: 0.0261 (0.0266)  classification_loss: 1.2054 (1.2402)  loss_mask: 0.0056 (0.0015)  time: 0.1955  data: 0.0003  max mem: 5511
[23:30:19.621325] Epoch: [58]  [580/781]  eta: 0:00:39  lr: 0.000100  training_loss: 1.2472 (1.2680)  mae_loss: 0.0268 (0.0266)  classification_loss: 1.2190 (1.2397)  loss_mask: 0.0033 (0.0016)  time: 0.1959  data: 0.0003  max mem: 5511
[23:30:23.561009] Epoch: [58]  [600/781]  eta: 0:00:35  lr: 0.000100  training_loss: 1.1808 (1.2667)  mae_loss: 0.0259 (0.0266)  classification_loss: 1.1527 (1.2385)  loss_mask: 0.0017 (0.0016)  time: 0.1969  data: 0.0002  max mem: 5511
[23:30:27.510295] Epoch: [58]  [620/781]  eta: 0:00:31  lr: 0.000100  training_loss: 1.2400 (1.2663)  mae_loss: 0.0247 (0.0266)  classification_loss: 1.2106 (1.2380)  loss_mask: 0.0022 (0.0017)  time: 0.1974  data: 0.0002  max mem: 5511
[23:30:31.454379] Epoch: [58]  [640/781]  eta: 0:00:27  lr: 0.000100  training_loss: 1.2162 (1.2656)  mae_loss: 0.0270 (0.0266)  classification_loss: 1.1895 (1.2373)  loss_mask: 0.0009 (0.0017)  time: 0.1971  data: 0.0002  max mem: 5511
[23:30:35.380884] Epoch: [58]  [660/781]  eta: 0:00:23  lr: 0.000100  training_loss: 1.2918 (1.2663)  mae_loss: 0.0246 (0.0266)  classification_loss: 1.2656 (1.2381)  loss_mask: 0.0007 (0.0017)  time: 0.1962  data: 0.0002  max mem: 5511
[23:30:39.314302] Epoch: [58]  [680/781]  eta: 0:00:19  lr: 0.000099  training_loss: 1.2791 (1.2664)  mae_loss: 0.0278 (0.0266)  classification_loss: 1.2511 (1.2382)  loss_mask: 0.0003 (0.0016)  time: 0.1965  data: 0.0002  max mem: 5511
[23:30:43.265109] Epoch: [58]  [700/781]  eta: 0:00:15  lr: 0.000099  training_loss: 1.2858 (1.2673)  mae_loss: 0.0269 (0.0266)  classification_loss: 1.2597 (1.2390)  loss_mask: 0.0003 (0.0016)  time: 0.1974  data: 0.0003  max mem: 5511
[23:30:47.214254] Epoch: [58]  [720/781]  eta: 0:00:12  lr: 0.000099  training_loss: 1.2991 (1.2685)  mae_loss: 0.0258 (0.0266)  classification_loss: 1.2753 (1.2403)  loss_mask: 0.0005 (0.0016)  time: 0.1974  data: 0.0003  max mem: 5511
[23:30:51.160622] Epoch: [58]  [740/781]  eta: 0:00:08  lr: 0.000099  training_loss: 1.2030 (1.2674)  mae_loss: 0.0255 (0.0266)  classification_loss: 1.1765 (1.2391)  loss_mask: 0.0042 (0.0017)  time: 0.1972  data: 0.0003  max mem: 5511
[23:30:55.131075] Epoch: [58]  [760/781]  eta: 0:00:04  lr: 0.000099  training_loss: 1.2737 (1.2682)  mae_loss: 0.0258 (0.0266)  classification_loss: 1.2368 (1.2398)  loss_mask: 0.0057 (0.0018)  time: 0.1984  data: 0.0002  max mem: 5511
[23:30:59.052864] Epoch: [58]  [780/781]  eta: 0:00:00  lr: 0.000099  training_loss: 1.2739 (1.2689)  mae_loss: 0.0265 (0.0266)  classification_loss: 1.2442 (1.2405)  loss_mask: 0.0014 (0.0019)  time: 0.1960  data: 0.0002  max mem: 5511
[23:30:59.228185] Epoch: [58] Total time: 0:02:34 (0.1977 s / it)
[23:30:59.228627] Averaged stats: lr: 0.000099  training_loss: 1.2739 (1.2689)  mae_loss: 0.0265 (0.0266)  classification_loss: 1.2442 (1.2405)  loss_mask: 0.0014 (0.0019)
[23:30:59.897180] Test:  [  0/157]  eta: 0:01:44  testing_loss: 0.5334 (0.5334)  acc1: 84.3750 (84.3750)  acc5: 98.4375 (98.4375)  time: 0.6640  data: 0.6299  max mem: 5511
[23:31:00.186141] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5636 (0.5789)  acc1: 81.2500 (81.3920)  acc5: 100.0000 (99.5739)  time: 0.0864  data: 0.0574  max mem: 5511
[23:31:00.477759] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5482 (0.5453)  acc1: 82.8125 (83.2589)  acc5: 100.0000 (99.7024)  time: 0.0288  data: 0.0002  max mem: 5511
[23:31:00.765264] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5401 (0.5574)  acc1: 84.3750 (82.8629)  acc5: 100.0000 (99.4456)  time: 0.0288  data: 0.0002  max mem: 5511
[23:31:01.057694] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5307 (0.5536)  acc1: 84.3750 (83.3079)  acc5: 100.0000 (99.3521)  time: 0.0288  data: 0.0002  max mem: 5511
[23:31:01.345568] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5155 (0.5515)  acc1: 84.3750 (83.3333)  acc5: 100.0000 (99.3260)  time: 0.0288  data: 0.0002  max mem: 5511
[23:31:01.636833] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5303 (0.5459)  acc1: 84.3750 (83.5553)  acc5: 100.0000 (99.3340)  time: 0.0288  data: 0.0002  max mem: 5511
[23:31:01.925951] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5051 (0.5373)  acc1: 84.3750 (83.8468)  acc5: 100.0000 (99.3618)  time: 0.0289  data: 0.0003  max mem: 5511
[23:31:02.216699] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5417 (0.5449)  acc1: 81.2500 (83.6998)  acc5: 100.0000 (99.3056)  time: 0.0287  data: 0.0002  max mem: 5511
[23:31:02.509473] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5606 (0.5445)  acc1: 81.2500 (83.7054)  acc5: 98.4375 (99.2273)  time: 0.0289  data: 0.0003  max mem: 5511
[23:31:02.797284] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5931 (0.5504)  acc1: 81.2500 (83.5087)  acc5: 98.4375 (99.1801)  time: 0.0288  data: 0.0003  max mem: 5511
[23:31:03.084408] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5971 (0.5515)  acc1: 81.2500 (83.5163)  acc5: 98.4375 (99.2117)  time: 0.0286  data: 0.0002  max mem: 5511
[23:31:03.373433] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5838 (0.5510)  acc1: 81.2500 (83.4065)  acc5: 100.0000 (99.2123)  time: 0.0287  data: 0.0002  max mem: 5511
[23:31:03.657079] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5185 (0.5501)  acc1: 82.8125 (83.3731)  acc5: 100.0000 (99.2366)  time: 0.0285  data: 0.0002  max mem: 5511
[23:31:03.942554] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5185 (0.5485)  acc1: 82.8125 (83.3444)  acc5: 100.0000 (99.2686)  time: 0.0283  data: 0.0001  max mem: 5511
[23:31:04.223222] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5500 (0.5480)  acc1: 82.8125 (83.2885)  acc5: 100.0000 (99.2653)  time: 0.0281  data: 0.0001  max mem: 5511
[23:31:04.376611] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5442 (0.5478)  acc1: 82.8125 (83.3200)  acc5: 100.0000 (99.2600)  time: 0.0271  data: 0.0001  max mem: 5511
[23:31:04.525224] Test: Total time: 0:00:05 (0.0337 s / it)
[23:31:04.526005] * Acc@1 83.320 Acc@5 99.260 loss 0.548
[23:31:04.526320] Accuracy of the network on the 10000 test images: 83.3%
[23:31:04.526545] Max accuracy: 83.90%
[23:31:05.005415] log_dir: ./output_dir
[23:31:05.875920] Epoch: [59]  [  0/781]  eta: 0:11:18  lr: 0.000099  training_loss: 0.9981 (0.9981)  mae_loss: 0.0236 (0.0236)  classification_loss: 0.9734 (0.9734)  loss_mask: 0.0011 (0.0011)  time: 0.8687  data: 0.6316  max mem: 5511
[23:31:09.836933] Epoch: [59]  [ 20/781]  eta: 0:02:54  lr: 0.000099  training_loss: 1.1940 (1.2088)  mae_loss: 0.0280 (0.0277)  classification_loss: 1.1647 (1.1802)  loss_mask: 0.0005 (0.0010)  time: 0.1980  data: 0.0002  max mem: 5511
[23:31:13.768465] Epoch: [59]  [ 40/781]  eta: 0:02:38  lr: 0.000099  training_loss: 1.2457 (1.2335)  mae_loss: 0.0266 (0.0273)  classification_loss: 1.2201 (1.2051)  loss_mask: 0.0009 (0.0012)  time: 0.1965  data: 0.0003  max mem: 5511
[23:31:17.710126] Epoch: [59]  [ 60/781]  eta: 0:02:30  lr: 0.000099  training_loss: 1.2614 (1.2443)  mae_loss: 0.0271 (0.0272)  classification_loss: 1.2311 (1.2160)  loss_mask: 0.0007 (0.0011)  time: 0.1970  data: 0.0002  max mem: 5511
[23:31:21.638908] Epoch: [59]  [ 80/781]  eta: 0:02:23  lr: 0.000099  training_loss: 1.2257 (1.2399)  mae_loss: 0.0264 (0.0273)  classification_loss: 1.2005 (1.2117)  loss_mask: 0.0004 (0.0010)  time: 0.1963  data: 0.0002  max mem: 5511
[23:31:25.588890] Epoch: [59]  [100/781]  eta: 0:02:18  lr: 0.000098  training_loss: 1.2747 (1.2477)  mae_loss: 0.0268 (0.0272)  classification_loss: 1.2463 (1.2194)  loss_mask: 0.0006 (0.0011)  time: 0.1974  data: 0.0002  max mem: 5511
[23:31:29.537317] Epoch: [59]  [120/781]  eta: 0:02:13  lr: 0.000098  training_loss: 1.2293 (1.2469)  mae_loss: 0.0256 (0.0270)  classification_loss: 1.2008 (1.2186)  loss_mask: 0.0017 (0.0014)  time: 0.1973  data: 0.0002  max mem: 5511
[23:31:33.446897] Epoch: [59]  [140/781]  eta: 0:02:09  lr: 0.000098  training_loss: 1.2129 (1.2462)  mae_loss: 0.0271 (0.0269)  classification_loss: 1.1843 (1.2179)  loss_mask: 0.0009 (0.0014)  time: 0.1954  data: 0.0003  max mem: 5511
[23:31:37.396685] Epoch: [59]  [160/781]  eta: 0:02:04  lr: 0.000098  training_loss: 1.2388 (1.2460)  mae_loss: 0.0252 (0.0268)  classification_loss: 1.2158 (1.2177)  loss_mask: 0.0014 (0.0016)  time: 0.1974  data: 0.0003  max mem: 5511
[23:31:41.344663] Epoch: [59]  [180/781]  eta: 0:02:00  lr: 0.000098  training_loss: 1.2459 (1.2495)  mae_loss: 0.0265 (0.0268)  classification_loss: 1.2168 (1.2212)  loss_mask: 0.0012 (0.0015)  time: 0.1973  data: 0.0002  max mem: 5511
[23:31:45.324672] Epoch: [59]  [200/781]  eta: 0:01:56  lr: 0.000098  training_loss: 1.1947 (1.2486)  mae_loss: 0.0261 (0.0268)  classification_loss: 1.1653 (1.2204)  loss_mask: 0.0003 (0.0014)  time: 0.1989  data: 0.0002  max mem: 5511
[23:31:49.275171] Epoch: [59]  [220/781]  eta: 0:01:52  lr: 0.000098  training_loss: 1.2769 (1.2536)  mae_loss: 0.0270 (0.0268)  classification_loss: 1.2517 (1.2254)  loss_mask: 0.0005 (0.0014)  time: 0.1974  data: 0.0002  max mem: 5511
[23:31:53.217280] Epoch: [59]  [240/781]  eta: 0:01:48  lr: 0.000098  training_loss: 1.2620 (1.2544)  mae_loss: 0.0244 (0.0267)  classification_loss: 1.2377 (1.2265)  loss_mask: 0.0004 (0.0013)  time: 0.1969  data: 0.0002  max mem: 5511
[23:31:57.160304] Epoch: [59]  [260/781]  eta: 0:01:44  lr: 0.000098  training_loss: 1.2584 (1.2537)  mae_loss: 0.0271 (0.0267)  classification_loss: 1.2307 (1.2258)  loss_mask: 0.0005 (0.0012)  time: 0.1971  data: 0.0002  max mem: 5511
[23:32:01.094496] Epoch: [59]  [280/781]  eta: 0:01:39  lr: 0.000098  training_loss: 1.2566 (1.2544)  mae_loss: 0.0266 (0.0267)  classification_loss: 1.2286 (1.2266)  loss_mask: 0.0004 (0.0012)  time: 0.1966  data: 0.0002  max mem: 5511
[23:32:05.048466] Epoch: [59]  [300/781]  eta: 0:01:35  lr: 0.000097  training_loss: 1.2735 (1.2562)  mae_loss: 0.0260 (0.0266)  classification_loss: 1.2444 (1.2284)  loss_mask: 0.0003 (0.0011)  time: 0.1976  data: 0.0002  max mem: 5511
[23:32:08.994589] Epoch: [59]  [320/781]  eta: 0:01:31  lr: 0.000097  training_loss: 1.2540 (1.2556)  mae_loss: 0.0249 (0.0266)  classification_loss: 1.2260 (1.2279)  loss_mask: 0.0003 (0.0011)  time: 0.1972  data: 0.0002  max mem: 5511
[23:32:12.936997] Epoch: [59]  [340/781]  eta: 0:01:27  lr: 0.000097  training_loss: 1.2238 (1.2537)  mae_loss: 0.0274 (0.0266)  classification_loss: 1.1951 (1.2261)  loss_mask: 0.0003 (0.0010)  time: 0.1970  data: 0.0002  max mem: 5511
[23:32:16.855564] Epoch: [59]  [360/781]  eta: 0:01:23  lr: 0.000097  training_loss: 1.2539 (1.2546)  mae_loss: 0.0263 (0.0266)  classification_loss: 1.2268 (1.2269)  loss_mask: 0.0009 (0.0011)  time: 0.1958  data: 0.0002  max mem: 5511
[23:32:20.803462] Epoch: [59]  [380/781]  eta: 0:01:19  lr: 0.000097  training_loss: 1.2641 (1.2549)  mae_loss: 0.0267 (0.0266)  classification_loss: 1.2366 (1.2269)  loss_mask: 0.0037 (0.0013)  time: 0.1973  data: 0.0002  max mem: 5511
[23:32:24.732410] Epoch: [59]  [400/781]  eta: 0:01:15  lr: 0.000097  training_loss: 1.2475 (1.2561)  mae_loss: 0.0255 (0.0266)  classification_loss: 1.2157 (1.2280)  loss_mask: 0.0018 (0.0015)  time: 0.1964  data: 0.0003  max mem: 5511
[23:32:28.661979] Epoch: [59]  [420/781]  eta: 0:01:11  lr: 0.000097  training_loss: 1.2764 (1.2559)  mae_loss: 0.0251 (0.0265)  classification_loss: 1.2488 (1.2279)  loss_mask: 0.0012 (0.0015)  time: 0.1964  data: 0.0002  max mem: 5511
[23:32:32.591549] Epoch: [59]  [440/781]  eta: 0:01:07  lr: 0.000097  training_loss: 1.2221 (1.2555)  mae_loss: 0.0264 (0.0265)  classification_loss: 1.1971 (1.2273)  loss_mask: 0.0010 (0.0016)  time: 0.1964  data: 0.0002  max mem: 5511
[23:32:36.506359] Epoch: [59]  [460/781]  eta: 0:01:03  lr: 0.000097  training_loss: 1.2788 (1.2547)  mae_loss: 0.0262 (0.0265)  classification_loss: 1.2493 (1.2266)  loss_mask: 0.0008 (0.0016)  time: 0.1957  data: 0.0002  max mem: 5511
[23:32:40.438918] Epoch: [59]  [480/781]  eta: 0:00:59  lr: 0.000096  training_loss: 1.2692 (1.2542)  mae_loss: 0.0259 (0.0265)  classification_loss: 1.2435 (1.2261)  loss_mask: 0.0011 (0.0016)  time: 0.1966  data: 0.0002  max mem: 5511
[23:32:44.367020] Epoch: [59]  [500/781]  eta: 0:00:55  lr: 0.000096  training_loss: 1.2972 (1.2565)  mae_loss: 0.0257 (0.0265)  classification_loss: 1.2636 (1.2284)  loss_mask: 0.0007 (0.0016)  time: 0.1963  data: 0.0002  max mem: 5511
[23:32:48.285937] Epoch: [59]  [520/781]  eta: 0:00:51  lr: 0.000096  training_loss: 1.2499 (1.2565)  mae_loss: 0.0267 (0.0265)  classification_loss: 1.2192 (1.2284)  loss_mask: 0.0004 (0.0016)  time: 0.1959  data: 0.0002  max mem: 5511
[23:32:52.196418] Epoch: [59]  [540/781]  eta: 0:00:47  lr: 0.000096  training_loss: 1.2709 (1.2571)  mae_loss: 0.0273 (0.0265)  classification_loss: 1.2422 (1.2290)  loss_mask: 0.0004 (0.0015)  time: 0.1954  data: 0.0002  max mem: 5511
[23:32:56.125296] Epoch: [59]  [560/781]  eta: 0:00:43  lr: 0.000096  training_loss: 1.2501 (1.2568)  mae_loss: 0.0264 (0.0265)  classification_loss: 1.2228 (1.2287)  loss_mask: 0.0005 (0.0015)  time: 0.1964  data: 0.0002  max mem: 5511
[23:33:00.043217] Epoch: [59]  [580/781]  eta: 0:00:39  lr: 0.000096  training_loss: 1.2668 (1.2567)  mae_loss: 0.0255 (0.0266)  classification_loss: 1.2408 (1.2287)  loss_mask: 0.0003 (0.0015)  time: 0.1958  data: 0.0002  max mem: 5511
[23:33:04.069580] Epoch: [59]  [600/781]  eta: 0:00:35  lr: 0.000096  training_loss: 1.2517 (1.2563)  mae_loss: 0.0259 (0.0266)  classification_loss: 1.2274 (1.2283)  loss_mask: 0.0003 (0.0014)  time: 0.2012  data: 0.0002  max mem: 5511
[23:33:08.016364] Epoch: [59]  [620/781]  eta: 0:00:31  lr: 0.000096  training_loss: 1.2356 (1.2558)  mae_loss: 0.0276 (0.0266)  classification_loss: 1.2048 (1.2278)  loss_mask: 0.0002 (0.0014)  time: 0.1972  data: 0.0002  max mem: 5511
[23:33:11.977125] Epoch: [59]  [640/781]  eta: 0:00:27  lr: 0.000096  training_loss: 1.2582 (1.2562)  mae_loss: 0.0255 (0.0266)  classification_loss: 1.2348 (1.2282)  loss_mask: 0.0004 (0.0014)  time: 0.1980  data: 0.0002  max mem: 5511
[23:33:15.892342] Epoch: [59]  [660/781]  eta: 0:00:23  lr: 0.000096  training_loss: 1.2863 (1.2561)  mae_loss: 0.0252 (0.0265)  classification_loss: 1.2602 (1.2281)  loss_mask: 0.0012 (0.0014)  time: 0.1957  data: 0.0002  max mem: 5511
[23:33:19.807851] Epoch: [59]  [680/781]  eta: 0:00:19  lr: 0.000095  training_loss: 1.2238 (1.2556)  mae_loss: 0.0275 (0.0266)  classification_loss: 1.1959 (1.2275)  loss_mask: 0.0013 (0.0014)  time: 0.1957  data: 0.0003  max mem: 5511
[23:33:23.731171] Epoch: [59]  [700/781]  eta: 0:00:16  lr: 0.000095  training_loss: 1.2519 (1.2555)  mae_loss: 0.0272 (0.0266)  classification_loss: 1.2043 (1.2274)  loss_mask: 0.0013 (0.0015)  time: 0.1961  data: 0.0002  max mem: 5511
[23:33:27.660920] Epoch: [59]  [720/781]  eta: 0:00:12  lr: 0.000095  training_loss: 1.2819 (1.2563)  mae_loss: 0.0246 (0.0266)  classification_loss: 1.2485 (1.2280)  loss_mask: 0.0063 (0.0017)  time: 0.1964  data: 0.0002  max mem: 5511
[23:33:31.597961] Epoch: [59]  [740/781]  eta: 0:00:08  lr: 0.000095  training_loss: 1.2225 (1.2560)  mae_loss: 0.0259 (0.0266)  classification_loss: 1.1907 (1.2273)  loss_mask: 0.0128 (0.0021)  time: 0.1968  data: 0.0003  max mem: 5511
[23:33:35.528472] Epoch: [59]  [760/781]  eta: 0:00:04  lr: 0.000095  training_loss: 1.2631 (1.2564)  mae_loss: 0.0279 (0.0266)  classification_loss: 1.2279 (1.2276)  loss_mask: 0.0035 (0.0022)  time: 0.1964  data: 0.0002  max mem: 5511
[23:33:39.433173] Epoch: [59]  [780/781]  eta: 0:00:00  lr: 0.000095  training_loss: 1.2151 (1.2555)  mae_loss: 0.0269 (0.0266)  classification_loss: 1.1842 (1.2267)  loss_mask: 0.0035 (0.0022)  time: 0.1952  data: 0.0002  max mem: 5511
[23:33:39.612782] Epoch: [59] Total time: 0:02:34 (0.1980 s / it)
[23:33:39.613275] Averaged stats: lr: 0.000095  training_loss: 1.2151 (1.2555)  mae_loss: 0.0269 (0.0266)  classification_loss: 1.1842 (1.2267)  loss_mask: 0.0035 (0.0022)
[23:33:40.224129] Test:  [  0/157]  eta: 0:01:35  testing_loss: 0.5392 (0.5392)  acc1: 84.3750 (84.3750)  acc5: 100.0000 (100.0000)  time: 0.6065  data: 0.5767  max mem: 5511
[23:33:40.517388] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.5388 (0.5509)  acc1: 84.3750 (83.2386)  acc5: 100.0000 (99.5739)  time: 0.0816  data: 0.0527  max mem: 5511
[23:33:40.803231] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.5337 (0.5201)  acc1: 84.3750 (84.4494)  acc5: 100.0000 (99.6280)  time: 0.0288  data: 0.0002  max mem: 5511
[23:33:41.088459] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5337 (0.5441)  acc1: 82.8125 (82.9133)  acc5: 100.0000 (99.2944)  time: 0.0284  data: 0.0002  max mem: 5511
[23:33:41.370916] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.5207 (0.5428)  acc1: 82.8125 (83.1174)  acc5: 98.4375 (99.1616)  time: 0.0283  data: 0.0002  max mem: 5511
[23:33:41.654314] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5142 (0.5352)  acc1: 84.3750 (83.5478)  acc5: 100.0000 (99.2647)  time: 0.0282  data: 0.0002  max mem: 5511
[23:33:41.937636] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5039 (0.5293)  acc1: 84.3750 (83.8883)  acc5: 100.0000 (99.3340)  time: 0.0282  data: 0.0002  max mem: 5511
[23:33:42.225546] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4998 (0.5238)  acc1: 82.8125 (84.0009)  acc5: 100.0000 (99.3618)  time: 0.0284  data: 0.0004  max mem: 5511
[23:33:42.511577] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5145 (0.5287)  acc1: 82.8125 (83.7963)  acc5: 98.4375 (99.3056)  time: 0.0286  data: 0.0005  max mem: 5511
[23:33:42.799628] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5403 (0.5271)  acc1: 82.8125 (83.9801)  acc5: 98.4375 (99.2788)  time: 0.0285  data: 0.0002  max mem: 5511
[23:33:43.086820] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5368 (0.5282)  acc1: 82.8125 (83.9418)  acc5: 98.4375 (99.2884)  time: 0.0286  data: 0.0002  max mem: 5511
[23:33:43.372498] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5516 (0.5280)  acc1: 82.8125 (84.0090)  acc5: 100.0000 (99.3102)  time: 0.0285  data: 0.0002  max mem: 5511
[23:33:43.657138] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5532 (0.5288)  acc1: 82.8125 (83.9101)  acc5: 100.0000 (99.3027)  time: 0.0284  data: 0.0002  max mem: 5511
[23:33:43.943734] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5111 (0.5273)  acc1: 82.8125 (84.0291)  acc5: 100.0000 (99.2963)  time: 0.0284  data: 0.0002  max mem: 5511
[23:33:44.228889] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5326 (0.5264)  acc1: 84.3750 (84.0647)  acc5: 100.0000 (99.3129)  time: 0.0284  data: 0.0002  max mem: 5511
[23:33:44.509774] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5409 (0.5270)  acc1: 84.3750 (84.0335)  acc5: 100.0000 (99.3067)  time: 0.0282  data: 0.0002  max mem: 5511
[23:33:44.660930] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5054 (0.5262)  acc1: 84.3750 (84.0200)  acc5: 100.0000 (99.2900)  time: 0.0270  data: 0.0001  max mem: 5511
[23:33:44.813834] Test: Total time: 0:00:05 (0.0331 s / it)
[23:33:44.814335] * Acc@1 84.020 Acc@5 99.290 loss 0.526
[23:33:44.814638] Accuracy of the network on the 10000 test images: 84.0%
[23:33:44.814836] Max accuracy: 84.02%
[23:33:45.009348] log_dir: ./output_dir
[23:33:45.916013] Epoch: [60]  [  0/781]  eta: 0:11:46  lr: 0.000095  training_loss: 1.1100 (1.1100)  mae_loss: 0.0223 (0.0223)  classification_loss: 1.0871 (1.0871)  loss_mask: 0.0005 (0.0005)  time: 0.9043  data: 0.6914  max mem: 5511
[23:33:49.867915] Epoch: [60]  [ 20/781]  eta: 0:02:55  lr: 0.000095  training_loss: 1.2017 (1.2272)  mae_loss: 0.0250 (0.0257)  classification_loss: 1.1677 (1.1994)  loss_mask: 0.0022 (0.0021)  time: 0.1974  data: 0.0002  max mem: 5511
[23:33:53.805271] Epoch: [60]  [ 40/781]  eta: 0:02:38  lr: 0.000095  training_loss: 1.2303 (1.2355)  mae_loss: 0.0262 (0.0260)  classification_loss: 1.1992 (1.2079)  loss_mask: 0.0008 (0.0016)  time: 0.1968  data: 0.0002  max mem: 5511
[23:33:57.744603] Epoch: [60]  [ 60/781]  eta: 0:02:30  lr: 0.000095  training_loss: 1.2493 (1.2489)  mae_loss: 0.0260 (0.0262)  classification_loss: 1.2207 (1.2214)  loss_mask: 0.0006 (0.0013)  time: 0.1969  data: 0.0002  max mem: 5511
[23:34:01.718537] Epoch: [60]  [ 80/781]  eta: 0:02:24  lr: 0.000095  training_loss: 1.2308 (1.2458)  mae_loss: 0.0264 (0.0263)  classification_loss: 1.2039 (1.2183)  loss_mask: 0.0005 (0.0012)  time: 0.1986  data: 0.0002  max mem: 5511
[23:34:05.738042] Epoch: [60]  [100/781]  eta: 0:02:19  lr: 0.000094  training_loss: 1.2436 (1.2475)  mae_loss: 0.0280 (0.0265)  classification_loss: 1.2152 (1.2199)  loss_mask: 0.0004 (0.0011)  time: 0.2009  data: 0.0002  max mem: 5511
[23:34:09.692694] Epoch: [60]  [120/781]  eta: 0:02:14  lr: 0.000094  training_loss: 1.2613 (1.2472)  mae_loss: 0.0255 (0.0265)  classification_loss: 1.2350 (1.2197)  loss_mask: 0.0007 (0.0011)  time: 0.1976  data: 0.0003  max mem: 5511
[23:34:13.608968] Epoch: [60]  [140/781]  eta: 0:02:09  lr: 0.000094  training_loss: 1.2430 (1.2468)  mae_loss: 0.0263 (0.0264)  classification_loss: 1.2153 (1.2193)  loss_mask: 0.0004 (0.0011)  time: 0.1957  data: 0.0003  max mem: 5511
[23:34:17.536212] Epoch: [60]  [160/781]  eta: 0:02:05  lr: 0.000094  training_loss: 1.2243 (1.2456)  mae_loss: 0.0267 (0.0265)  classification_loss: 1.2007 (1.2180)  loss_mask: 0.0011 (0.0012)  time: 0.1963  data: 0.0002  max mem: 5511
[23:34:21.502870] Epoch: [60]  [180/781]  eta: 0:02:01  lr: 0.000094  training_loss: 1.2105 (1.2462)  mae_loss: 0.0258 (0.0264)  classification_loss: 1.1810 (1.2184)  loss_mask: 0.0020 (0.0014)  time: 0.1983  data: 0.0002  max mem: 5511
[23:34:25.501232] Epoch: [60]  [200/781]  eta: 0:01:56  lr: 0.000094  training_loss: 1.2611 (1.2488)  mae_loss: 0.0254 (0.0263)  classification_loss: 1.2351 (1.2211)  loss_mask: 0.0012 (0.0014)  time: 0.1998  data: 0.0002  max mem: 5511
[23:34:29.457920] Epoch: [60]  [220/781]  eta: 0:01:52  lr: 0.000094  training_loss: 1.2534 (1.2500)  mae_loss: 0.0255 (0.0263)  classification_loss: 1.2275 (1.2224)  loss_mask: 0.0004 (0.0013)  time: 0.1977  data: 0.0002  max mem: 5511
[23:34:33.472992] Epoch: [60]  [240/781]  eta: 0:01:48  lr: 0.000094  training_loss: 1.2677 (1.2518)  mae_loss: 0.0274 (0.0264)  classification_loss: 1.2389 (1.2241)  loss_mask: 0.0003 (0.0013)  time: 0.2007  data: 0.0002  max mem: 5511
[23:34:37.400066] Epoch: [60]  [260/781]  eta: 0:01:44  lr: 0.000094  training_loss: 1.2850 (1.2537)  mae_loss: 0.0257 (0.0264)  classification_loss: 1.2581 (1.2260)  loss_mask: 0.0009 (0.0013)  time: 0.1963  data: 0.0003  max mem: 5511
[23:34:41.327387] Epoch: [60]  [280/781]  eta: 0:01:40  lr: 0.000094  training_loss: 1.2515 (1.2542)  mae_loss: 0.0274 (0.0265)  classification_loss: 1.2205 (1.2264)  loss_mask: 0.0005 (0.0012)  time: 0.1963  data: 0.0003  max mem: 5511
[23:34:45.259035] Epoch: [60]  [300/781]  eta: 0:01:36  lr: 0.000093  training_loss: 1.2718 (1.2567)  mae_loss: 0.0256 (0.0264)  classification_loss: 1.2456 (1.2290)  loss_mask: 0.0009 (0.0013)  time: 0.1965  data: 0.0003  max mem: 5511
[23:34:49.204021] Epoch: [60]  [320/781]  eta: 0:01:32  lr: 0.000093  training_loss: 1.2202 (1.2555)  mae_loss: 0.0270 (0.0265)  classification_loss: 1.1910 (1.2278)  loss_mask: 0.0006 (0.0013)  time: 0.1972  data: 0.0002  max mem: 5511
[23:34:53.128353] Epoch: [60]  [340/781]  eta: 0:01:28  lr: 0.000093  training_loss: 1.2382 (1.2543)  mae_loss: 0.0271 (0.0265)  classification_loss: 1.2150 (1.2266)  loss_mask: 0.0005 (0.0012)  time: 0.1961  data: 0.0002  max mem: 5511
[23:34:57.060739] Epoch: [60]  [360/781]  eta: 0:01:23  lr: 0.000093  training_loss: 1.2466 (1.2544)  mae_loss: 0.0264 (0.0265)  classification_loss: 1.2198 (1.2268)  loss_mask: 0.0003 (0.0012)  time: 0.1965  data: 0.0002  max mem: 5511
[23:35:00.994426] Epoch: [60]  [380/781]  eta: 0:01:19  lr: 0.000093  training_loss: 1.2573 (1.2549)  mae_loss: 0.0267 (0.0265)  classification_loss: 1.2303 (1.2272)  loss_mask: 0.0003 (0.0011)  time: 0.1966  data: 0.0002  max mem: 5511
[23:35:04.958152] Epoch: [60]  [400/781]  eta: 0:01:15  lr: 0.000093  training_loss: 1.1974 (1.2534)  mae_loss: 0.0267 (0.0266)  classification_loss: 1.1668 (1.2258)  loss_mask: 0.0003 (0.0011)  time: 0.1981  data: 0.0002  max mem: 5511
[23:35:08.896573] Epoch: [60]  [420/781]  eta: 0:01:11  lr: 0.000093  training_loss: 1.2095 (1.2530)  mae_loss: 0.0266 (0.0266)  classification_loss: 1.1797 (1.2254)  loss_mask: 0.0003 (0.0011)  time: 0.1968  data: 0.0003  max mem: 5511
[23:35:12.875773] Epoch: [60]  [440/781]  eta: 0:01:07  lr: 0.000093  training_loss: 1.2494 (1.2540)  mae_loss: 0.0265 (0.0266)  classification_loss: 1.2266 (1.2263)  loss_mask: 0.0003 (0.0010)  time: 0.1989  data: 0.0005  max mem: 5511
[23:35:16.841078] Epoch: [60]  [460/781]  eta: 0:01:03  lr: 0.000093  training_loss: 1.2294 (1.2539)  mae_loss: 0.0273 (0.0266)  classification_loss: 1.1996 (1.2263)  loss_mask: 0.0002 (0.0010)  time: 0.1982  data: 0.0002  max mem: 5511
[23:35:20.812508] Epoch: [60]  [480/781]  eta: 0:00:59  lr: 0.000092  training_loss: 1.2463 (1.2539)  mae_loss: 0.0237 (0.0266)  classification_loss: 1.2235 (1.2264)  loss_mask: 0.0002 (0.0010)  time: 0.1985  data: 0.0002  max mem: 5511
[23:35:24.742701] Epoch: [60]  [500/781]  eta: 0:00:55  lr: 0.000092  training_loss: 1.2349 (1.2527)  mae_loss: 0.0265 (0.0266)  classification_loss: 1.2092 (1.2252)  loss_mask: 0.0002 (0.0009)  time: 0.1964  data: 0.0002  max mem: 5511
[23:35:28.692389] Epoch: [60]  [520/781]  eta: 0:00:51  lr: 0.000092  training_loss: 1.2316 (1.2517)  mae_loss: 0.0258 (0.0266)  classification_loss: 1.2088 (1.2242)  loss_mask: 0.0003 (0.0009)  time: 0.1974  data: 0.0002  max mem: 5511
[23:35:32.638957] Epoch: [60]  [540/781]  eta: 0:00:47  lr: 0.000092  training_loss: 1.2449 (1.2517)  mae_loss: 0.0256 (0.0266)  classification_loss: 1.2172 (1.2243)  loss_mask: 0.0003 (0.0009)  time: 0.1972  data: 0.0002  max mem: 5511
[23:35:36.563894] Epoch: [60]  [560/781]  eta: 0:00:43  lr: 0.000092  training_loss: 1.2441 (1.2519)  mae_loss: 0.0271 (0.0266)  classification_loss: 1.2185 (1.2244)  loss_mask: 0.0004 (0.0009)  time: 0.1962  data: 0.0003  max mem: 5511

[23:35:40.492923] Epoch: [60]  [580/781]  eta: 0:00:39  lr: 0.000092  training_loss: 1.2822 (1.2532)  mae_loss: 0.0267 (0.0266)  classification_loss: 1.2525 (1.2257)  loss_mask: 0.0005 (0.0009)  time: 0.1964  data: 0.0002  max mem: 5511
[23:35:44.422391] Epoch: [60]  [600/781]  eta: 0:00:35  lr: 0.000092  training_loss: 1.2608 (1.2532)  mae_loss: 0.0268 (0.0266)  classification_loss: 1.2316 (1.2257)  loss_mask: 0.0004 (0.0009)  time: 0.1964  data: 0.0002  max mem: 5511
[23:35:48.352434] Epoch: [60]  [620/781]  eta: 0:00:31  lr: 0.000092  training_loss: 1.2431 (1.2536)  mae_loss: 0.0259 (0.0266)  classification_loss: 1.2199 (1.2262)  loss_mask: 0.0002 (0.0009)  time: 0.1964  data: 0.0003  max mem: 5511
[23:35:52.365562] Epoch: [60]  [640/781]  eta: 0:00:28  lr: 0.000092  training_loss: 1.1970 (1.2534)  mae_loss: 0.0267 (0.0266)  classification_loss: 1.1698 (1.2259)  loss_mask: 0.0002 (0.0008)  time: 0.2006  data: 0.0002  max mem: 5511
[23:35:56.328914] Epoch: [60]  [660/781]  eta: 0:00:24  lr: 0.000092  training_loss: 1.1991 (1.2523)  mae_loss: 0.0271 (0.0266)  classification_loss: 1.1655 (1.2248)  loss_mask: 0.0003 (0.0008)  time: 0.1980  data: 0.0002  max mem: 5511
[23:36:00.255972] Epoch: [60]  [680/781]  eta: 0:00:20  lr: 0.000091  training_loss: 1.2404 (1.2520)  mae_loss: 0.0259 (0.0266)  classification_loss: 1.2130 (1.2245)  loss_mask: 0.0004 (0.0008)  time: 0.1963  data: 0.0003  max mem: 5511
[23:36:04.218384] Epoch: [60]  [700/781]  eta: 0:00:16  lr: 0.000091  training_loss: 1.3060 (1.2524)  mae_loss: 0.0264 (0.0266)  classification_loss: 1.2805 (1.2249)  loss_mask: 0.0003 (0.0008)  time: 0.1980  data: 0.0003  max mem: 5511
[23:36:08.183928] Epoch: [60]  [720/781]  eta: 0:00:12  lr: 0.000091  training_loss: 1.2820 (1.2530)  mae_loss: 0.0249 (0.0266)  classification_loss: 1.2586 (1.2256)  loss_mask: 0.0004 (0.0008)  time: 0.1982  data: 0.0002  max mem: 5511
[23:36:12.114707] Epoch: [60]  [740/781]  eta: 0:00:08  lr: 0.000091  training_loss: 1.2671 (1.2530)  mae_loss: 0.0257 (0.0266)  classification_loss: 1.2378 (1.2256)  loss_mask: 0.0002 (0.0008)  time: 0.1965  data: 0.0002  max mem: 5511
[23:36:16.039095] Epoch: [60]  [760/781]  eta: 0:00:04  lr: 0.000091  training_loss: 1.2942 (1.2540)  mae_loss: 0.0257 (0.0266)  classification_loss: 1.2664 (1.2266)  loss_mask: 0.0002 (0.0008)  time: 0.1961  data: 0.0002  max mem: 5511
[23:36:19.964508] Epoch: [60]  [780/781]  eta: 0:00:00  lr: 0.000091  training_loss: 1.2358 (1.2537)  mae_loss: 0.0263 (0.0266)  classification_loss: 1.2107 (1.2263)  loss_mask: 0.0004 (0.0008)  time: 0.1962  data: 0.0002  max mem: 5511
[23:36:20.133413] Epoch: [60] Total time: 0:02:35 (0.1986 s / it)
[23:36:20.133873] Averaged stats: lr: 0.000091  training_loss: 1.2358 (1.2537)  mae_loss: 0.0263 (0.0266)  classification_loss: 1.2107 (1.2263)  loss_mask: 0.0004 (0.0008)
[23:36:21.334487] Test:  [  0/157]  eta: 0:01:39  testing_loss: 0.5440 (0.5440)  acc1: 79.6875 (79.6875)  acc5: 100.0000 (100.0000)  time: 0.6368  data: 0.6031  max mem: 5511
[23:36:21.620523] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5165 (0.5414)  acc1: 82.8125 (84.0909)  acc5: 100.0000 (99.4318)  time: 0.0837  data: 0.0551  max mem: 5511
[23:36:21.914063] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.5127 (0.5212)  acc1: 84.3750 (84.5238)  acc5: 98.4375 (99.2560)  time: 0.0288  data: 0.0003  max mem: 5511
[23:36:22.207368] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5511 (0.5386)  acc1: 82.8125 (83.7198)  acc5: 98.4375 (99.1431)  time: 0.0292  data: 0.0002  max mem: 5511
[23:36:22.502459] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5331 (0.5364)  acc1: 84.3750 (84.0701)  acc5: 98.4375 (99.1235)  time: 0.0292  data: 0.0002  max mem: 5511
[23:36:22.795803] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5176 (0.5299)  acc1: 85.9375 (84.4669)  acc5: 100.0000 (99.1728)  time: 0.0292  data: 0.0002  max mem: 5511
[23:36:23.089528] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5034 (0.5265)  acc1: 85.9375 (84.5287)  acc5: 100.0000 (99.1547)  time: 0.0291  data: 0.0002  max mem: 5511
[23:36:23.381726] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4903 (0.5175)  acc1: 85.9375 (84.7711)  acc5: 100.0000 (99.2077)  time: 0.0291  data: 0.0002  max mem: 5511
[23:36:23.667692] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5092 (0.5256)  acc1: 84.3750 (84.3750)  acc5: 100.0000 (99.1512)  time: 0.0287  data: 0.0002  max mem: 5511
[23:36:23.956947] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5032 (0.5199)  acc1: 84.3750 (84.5467)  acc5: 98.4375 (99.1415)  time: 0.0286  data: 0.0002  max mem: 5511
[23:36:24.250436] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5123 (0.5229)  acc1: 82.8125 (84.3750)  acc5: 100.0000 (99.1337)  time: 0.0290  data: 0.0002  max mem: 5511
[23:36:24.537310] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5304 (0.5238)  acc1: 82.8125 (84.2765)  acc5: 100.0000 (99.1554)  time: 0.0289  data: 0.0002  max mem: 5511
[23:36:24.824323] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5304 (0.5241)  acc1: 81.2500 (84.0522)  acc5: 100.0000 (99.1606)  time: 0.0285  data: 0.0002  max mem: 5511
[23:36:25.119407] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5526 (0.5243)  acc1: 81.2500 (83.8979)  acc5: 100.0000 (99.2009)  time: 0.0290  data: 0.0002  max mem: 5511
[23:36:25.405644] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5027 (0.5235)  acc1: 84.3750 (83.9761)  acc5: 100.0000 (99.2354)  time: 0.0289  data: 0.0002  max mem: 5511
[23:36:25.689074] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5147 (0.5233)  acc1: 82.8125 (83.8990)  acc5: 100.0000 (99.2550)  time: 0.0283  data: 0.0002  max mem: 5511
[23:36:25.845693] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4530 (0.5217)  acc1: 84.3750 (83.9900)  acc5: 100.0000 (99.2400)  time: 0.0275  data: 0.0002  max mem: 5511
[23:36:26.006616] Test: Total time: 0:00:05 (0.0338 s / it)
[23:36:26.007104] * Acc@1 83.990 Acc@5 99.240 loss 0.522
[23:36:26.007435] Accuracy of the network on the 10000 test images: 84.0%
[23:36:26.007701] Max accuracy: 84.02%
[23:36:26.188801] log_dir: ./output_dir
[23:36:27.060280] Epoch: [61]  [  0/781]  eta: 0:11:18  lr: 0.000091  training_loss: 1.0125 (1.0125)  mae_loss: 0.0270 (0.0270)  classification_loss: 0.9852 (0.9852)  loss_mask: 0.0003 (0.0003)  time: 0.8694  data: 0.6514  max mem: 5511
[23:36:30.991786] Epoch: [61]  [ 20/781]  eta: 0:02:53  lr: 0.000091  training_loss: 1.2208 (1.2366)  mae_loss: 0.0280 (0.0281)  classification_loss: 1.1933 (1.2081)  loss_mask: 0.0003 (0.0004)  time: 0.1965  data: 0.0001  max mem: 5511
[23:36:34.912192] Epoch: [61]  [ 40/781]  eta: 0:02:37  lr: 0.000091  training_loss: 1.2718 (1.2511)  mae_loss: 0.0275 (0.0276)  classification_loss: 1.2439 (1.2230)  loss_mask: 0.0003 (0.0004)  time: 0.1959  data: 0.0002  max mem: 5511
[23:36:38.876109] Epoch: [61]  [ 60/781]  eta: 0:02:29  lr: 0.000091  training_loss: 1.2920 (1.2679)  mae_loss: 0.0273 (0.0276)  classification_loss: 1.2686 (1.2399)  loss_mask: 0.0002 (0.0004)  time: 0.1981  data: 0.0002  max mem: 5511
[23:36:42.828889] Epoch: [61]  [ 80/781]  eta: 0:02:23  lr: 0.000091  training_loss: 1.2311 (1.2634)  mae_loss: 0.0269 (0.0273)  classification_loss: 1.2026 (1.2357)  loss_mask: 0.0002 (0.0004)  time: 0.1976  data: 0.0002  max mem: 5511
[23:36:46.766933] Epoch: [61]  [100/781]  eta: 0:02:18  lr: 0.000090  training_loss: 1.2179 (1.2622)  mae_loss: 0.0262 (0.0272)  classification_loss: 1.1929 (1.2346)  loss_mask: 0.0002 (0.0003)  time: 0.1968  data: 0.0002  max mem: 5511
[23:36:50.753070] Epoch: [61]  [120/781]  eta: 0:02:14  lr: 0.000090  training_loss: 1.2211 (1.2567)  mae_loss: 0.0262 (0.0271)  classification_loss: 1.1954 (1.2293)  loss_mask: 0.0002 (0.0003)  time: 0.1992  data: 0.0003  max mem: 5511
[23:36:54.717607] Epoch: [61]  [140/781]  eta: 0:02:09  lr: 0.000090  training_loss: 1.2363 (1.2557)  mae_loss: 0.0264 (0.0271)  classification_loss: 1.2086 (1.2283)  loss_mask: 0.0002 (0.0004)  time: 0.1981  data: 0.0002  max mem: 5511
[23:36:58.657878] Epoch: [61]  [160/781]  eta: 0:02:05  lr: 0.000090  training_loss: 1.1904 (1.2522)  mae_loss: 0.0255 (0.0269)  classification_loss: 1.1645 (1.2249)  loss_mask: 0.0009 (0.0004)  time: 0.1969  data: 0.0002  max mem: 5511
[23:37:02.585749] Epoch: [61]  [180/781]  eta: 0:02:00  lr: 0.000090  training_loss: 1.2399 (1.2519)  mae_loss: 0.0263 (0.0269)  classification_loss: 1.2151 (1.2245)  loss_mask: 0.0005 (0.0005)  time: 0.1963  data: 0.0002  max mem: 5511
[23:37:06.527536] Epoch: [61]  [200/781]  eta: 0:01:56  lr: 0.000090  training_loss: 1.2161 (1.2517)  mae_loss: 0.0253 (0.0268)  classification_loss: 1.1884 (1.2244)  loss_mask: 0.0003 (0.0005)  time: 0.1970  data: 0.0002  max mem: 5511
[23:37:10.468369] Epoch: [61]  [220/781]  eta: 0:01:52  lr: 0.000090  training_loss: 1.2122 (1.2499)  mae_loss: 0.0252 (0.0267)  classification_loss: 1.1844 (1.2226)  loss_mask: 0.0005 (0.0006)  time: 0.1970  data: 0.0002  max mem: 5511
[23:37:14.416578] Epoch: [61]  [240/781]  eta: 0:01:48  lr: 0.000090  training_loss: 1.2176 (1.2477)  mae_loss: 0.0260 (0.0267)  classification_loss: 1.1907 (1.2205)  loss_mask: 0.0003 (0.0006)  time: 0.1973  data: 0.0002  max mem: 5511
[23:37:18.340571] Epoch: [61]  [260/781]  eta: 0:01:44  lr: 0.000090  training_loss: 1.2245 (1.2476)  mae_loss: 0.0264 (0.0267)  classification_loss: 1.1966 (1.2204)  loss_mask: 0.0003 (0.0006)  time: 0.1961  data: 0.0002  max mem: 5511
[23:37:22.317105] Epoch: [61]  [280/781]  eta: 0:01:40  lr: 0.000090  training_loss: 1.2142 (1.2463)  mae_loss: 0.0263 (0.0267)  classification_loss: 1.1830 (1.2190)  loss_mask: 0.0007 (0.0006)  time: 0.1987  data: 0.0002  max mem: 5511
[23:37:26.290648] Epoch: [61]  [300/781]  eta: 0:01:35  lr: 0.000089  training_loss: 1.2757 (1.2472)  mae_loss: 0.0266 (0.0267)  classification_loss: 1.2474 (1.2199)  loss_mask: 0.0006 (0.0007)  time: 0.1985  data: 0.0003  max mem: 5511
[23:37:30.258172] Epoch: [61]  [320/781]  eta: 0:01:31  lr: 0.000089  training_loss: 1.2271 (1.2472)  mae_loss: 0.0256 (0.0267)  classification_loss: 1.2012 (1.2198)  loss_mask: 0.0003 (0.0007)  time: 0.1983  data: 0.0002  max mem: 5511
[23:37:34.218665] Epoch: [61]  [340/781]  eta: 0:01:27  lr: 0.000089  training_loss: 1.2776 (1.2483)  mae_loss: 0.0282 (0.0268)  classification_loss: 1.2437 (1.2209)  loss_mask: 0.0001 (0.0007)  time: 0.1979  data: 0.0002  max mem: 5511
[23:37:38.133441] Epoch: [61]  [360/781]  eta: 0:01:23  lr: 0.000089  training_loss: 1.2189 (1.2464)  mae_loss: 0.0252 (0.0267)  classification_loss: 1.1907 (1.2191)  loss_mask: 0.0003 (0.0007)  time: 0.1956  data: 0.0003  max mem: 5511
[23:37:42.056293] Epoch: [61]  [380/781]  eta: 0:01:19  lr: 0.000089  training_loss: 1.2349 (1.2467)  mae_loss: 0.0246 (0.0266)  classification_loss: 1.2085 (1.2194)  loss_mask: 0.0003 (0.0007)  time: 0.1960  data: 0.0002  max mem: 5511
[23:37:46.040822] Epoch: [61]  [400/781]  eta: 0:01:15  lr: 0.000089  training_loss: 1.2598 (1.2475)  mae_loss: 0.0269 (0.0267)  classification_loss: 1.2327 (1.2202)  loss_mask: 0.0003 (0.0006)  time: 0.1991  data: 0.0002  max mem: 5511
[23:37:49.989766] Epoch: [61]  [420/781]  eta: 0:01:11  lr: 0.000089  training_loss: 1.2499 (1.2477)  mae_loss: 0.0257 (0.0266)  classification_loss: 1.2243 (1.2205)  loss_mask: 0.0002 (0.0006)  time: 0.1974  data: 0.0003  max mem: 5511
[23:37:53.921144] Epoch: [61]  [440/781]  eta: 0:01:07  lr: 0.000089  training_loss: 1.2647 (1.2476)  mae_loss: 0.0273 (0.0267)  classification_loss: 1.2356 (1.2203)  loss_mask: 0.0002 (0.0006)  time: 0.1965  data: 0.0002  max mem: 5511
[23:37:57.842748] Epoch: [61]  [460/781]  eta: 0:01:03  lr: 0.000089  training_loss: 1.1832 (1.2473)  mae_loss: 0.0265 (0.0267)  classification_loss: 1.1545 (1.2200)  loss_mask: 0.0002 (0.0006)  time: 0.1960  data: 0.0002  max mem: 5511
[23:38:01.762634] Epoch: [61]  [480/781]  eta: 0:00:59  lr: 0.000089  training_loss: 1.2488 (1.2473)  mae_loss: 0.0266 (0.0267)  classification_loss: 1.2200 (1.2201)  loss_mask: 0.0002 (0.0006)  time: 0.1959  data: 0.0002  max mem: 5511
[23:38:05.698551] Epoch: [61]  [500/781]  eta: 0:00:55  lr: 0.000088  training_loss: 1.2148 (1.2461)  mae_loss: 0.0247 (0.0266)  classification_loss: 1.1907 (1.2188)  loss_mask: 0.0002 (0.0006)  time: 0.1967  data: 0.0002  max mem: 5511
[23:38:09.627891] Epoch: [61]  [520/781]  eta: 0:00:51  lr: 0.000088  training_loss: 1.1988 (1.2449)  mae_loss: 0.0265 (0.0266)  classification_loss: 1.1724 (1.2176)  loss_mask: 0.0002 (0.0006)  time: 0.1964  data: 0.0002  max mem: 5511
[23:38:13.568250] Epoch: [61]  [540/781]  eta: 0:00:47  lr: 0.000088  training_loss: 1.2149 (1.2441)  mae_loss: 0.0262 (0.0266)  classification_loss: 1.1882 (1.2169)  loss_mask: 0.0007 (0.0006)  time: 0.1969  data: 0.0002  max mem: 5511
[23:38:17.524841] Epoch: [61]  [560/781]  eta: 0:00:43  lr: 0.000088  training_loss: 1.2392 (1.2439)  mae_loss: 0.0246 (0.0266)  classification_loss: 1.2096 (1.2165)  loss_mask: 0.0022 (0.0008)  time: 0.1978  data: 0.0002  max mem: 5511
[23:38:21.461252] Epoch: [61]  [580/781]  eta: 0:00:39  lr: 0.000088  training_loss: 1.1905 (1.2421)  mae_loss: 0.0270 (0.0266)  classification_loss: 1.1560 (1.2145)  loss_mask: 0.0050 (0.0010)  time: 0.1967  data: 0.0002  max mem: 5511
[23:38:25.432459] Epoch: [61]  [600/781]  eta: 0:00:35  lr: 0.000088  training_loss: 1.2017 (1.2409)  mae_loss: 0.0262 (0.0266)  classification_loss: 1.1659 (1.2130)  loss_mask: 0.0053 (0.0012)  time: 0.1985  data: 0.0002  max mem: 5511
[23:38:29.362704] Epoch: [61]  [620/781]  eta: 0:00:31  lr: 0.000088  training_loss: 1.2675 (1.2419)  mae_loss: 0.0246 (0.0265)  classification_loss: 1.2353 (1.2141)  loss_mask: 0.0014 (0.0013)  time: 0.1964  data: 0.0002  max mem: 5511
[23:38:33.310756] Epoch: [61]  [640/781]  eta: 0:00:27  lr: 0.000088  training_loss: 1.2629 (1.2424)  mae_loss: 0.0286 (0.0266)  classification_loss: 1.2330 (1.2146)  loss_mask: 0.0006 (0.0013)  time: 0.1973  data: 0.0002  max mem: 5511
[23:38:37.227050] Epoch: [61]  [660/781]  eta: 0:00:23  lr: 0.000088  training_loss: 1.2388 (1.2424)  mae_loss: 0.0260 (0.0266)  classification_loss: 1.2131 (1.2145)  loss_mask: 0.0005 (0.0012)  time: 0.1957  data: 0.0002  max mem: 5511
[23:38:41.147978] Epoch: [61]  [680/781]  eta: 0:00:20  lr: 0.000088  training_loss: 1.1943 (1.2419)  mae_loss: 0.0251 (0.0265)  classification_loss: 1.1678 (1.2141)  loss_mask: 0.0005 (0.0012)  time: 0.1960  data: 0.0002  max mem: 5511
[23:38:45.071492] Epoch: [61]  [700/781]  eta: 0:00:16  lr: 0.000087  training_loss: 1.2246 (1.2417)  mae_loss: 0.0240 (0.0265)  classification_loss: 1.1996 (1.2140)  loss_mask: 0.0007 (0.0012)  time: 0.1961  data: 0.0002  max mem: 5511
[23:38:48.991547] Epoch: [61]  [720/781]  eta: 0:00:12  lr: 0.000087  training_loss: 1.3154 (1.2436)  mae_loss: 0.0276 (0.0265)  classification_loss: 1.2876 (1.2159)  loss_mask: 0.0003 (0.0012)  time: 0.1959  data: 0.0003  max mem: 5511
[23:38:52.938984] Epoch: [61]  [740/781]  eta: 0:00:08  lr: 0.000087  training_loss: 1.2118 (1.2432)  mae_loss: 0.0258 (0.0265)  classification_loss: 1.1839 (1.2155)  loss_mask: 0.0003 (0.0012)  time: 0.1973  data: 0.0003  max mem: 5511
[23:38:56.908810] Epoch: [61]  [760/781]  eta: 0:00:04  lr: 0.000087  training_loss: 1.2631 (1.2435)  mae_loss: 0.0259 (0.0265)  classification_loss: 1.2344 (1.2159)  loss_mask: 0.0002 (0.0012)  time: 0.1984  data: 0.0002  max mem: 5511
[23:39:00.821379] Epoch: [61]  [780/781]  eta: 0:00:00  lr: 0.000087  training_loss: 1.2143 (1.2434)  mae_loss: 0.0257 (0.0265)  classification_loss: 1.1890 (1.2158)  loss_mask: 0.0003 (0.0011)  time: 0.1955  data: 0.0002  max mem: 5511
[23:39:00.981198] Epoch: [61] Total time: 0:02:34 (0.1982 s / it)
[23:39:00.981787] Averaged stats: lr: 0.000087  training_loss: 1.2143 (1.2434)  mae_loss: 0.0257 (0.0265)  classification_loss: 1.1890 (1.2158)  loss_mask: 0.0003 (0.0011)
[23:39:01.599900] Test:  [  0/157]  eta: 0:01:36  testing_loss: 0.5768 (0.5768)  acc1: 79.6875 (79.6875)  acc5: 100.0000 (100.0000)  time: 0.6130  data: 0.5836  max mem: 5511
[23:39:01.886819] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.5696 (0.5524)  acc1: 81.2500 (82.5284)  acc5: 100.0000 (99.5739)  time: 0.0816  data: 0.0532  max mem: 5511
[23:39:02.170087] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4924 (0.5239)  acc1: 84.3750 (83.7798)  acc5: 100.0000 (99.5536)  time: 0.0283  data: 0.0002  max mem: 5511
[23:39:02.454533] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.5396 (0.5483)  acc1: 82.8125 (83.2661)  acc5: 100.0000 (99.2944)  time: 0.0282  data: 0.0002  max mem: 5511
[23:39:02.738145] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.5261 (0.5429)  acc1: 84.3750 (83.6890)  acc5: 98.4375 (99.1235)  time: 0.0282  data: 0.0002  max mem: 5511
[23:39:03.024261] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5012 (0.5379)  acc1: 85.9375 (83.8848)  acc5: 100.0000 (99.1115)  time: 0.0283  data: 0.0002  max mem: 5511
[23:39:03.307728] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4644 (0.5297)  acc1: 84.3750 (84.1189)  acc5: 100.0000 (99.1547)  time: 0.0283  data: 0.0002  max mem: 5511
[23:39:03.594160] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4644 (0.5239)  acc1: 85.9375 (84.2210)  acc5: 100.0000 (99.2077)  time: 0.0283  data: 0.0002  max mem: 5511
[23:39:03.881549] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5234 (0.5318)  acc1: 82.8125 (83.7191)  acc5: 100.0000 (99.1705)  time: 0.0285  data: 0.0002  max mem: 5511
[23:39:04.177881] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5237 (0.5285)  acc1: 81.2500 (83.8599)  acc5: 98.4375 (99.1587)  time: 0.0291  data: 0.0002  max mem: 5511
[23:39:04.462571] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5485 (0.5338)  acc1: 82.8125 (83.6479)  acc5: 98.4375 (99.1801)  time: 0.0289  data: 0.0002  max mem: 5511
[23:39:04.749874] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5700 (0.5341)  acc1: 82.8125 (83.6289)  acc5: 100.0000 (99.1976)  time: 0.0285  data: 0.0002  max mem: 5511
[23:39:05.037051] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5137 (0.5318)  acc1: 84.3750 (83.7552)  acc5: 100.0000 (99.2252)  time: 0.0286  data: 0.0002  max mem: 5511
[23:39:05.323008] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5113 (0.5312)  acc1: 82.8125 (83.7071)  acc5: 100.0000 (99.2247)  time: 0.0285  data: 0.0002  max mem: 5511
[23:39:05.607882] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5226 (0.5302)  acc1: 82.8125 (83.6990)  acc5: 100.0000 (99.2575)  time: 0.0284  data: 0.0002  max mem: 5511
[23:39:05.889956] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5326 (0.5310)  acc1: 82.8125 (83.6817)  acc5: 100.0000 (99.2550)  time: 0.0281  data: 0.0002  max mem: 5511
[23:39:06.042113] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5326 (0.5314)  acc1: 82.8125 (83.6800)  acc5: 100.0000 (99.2600)  time: 0.0272  data: 0.0001  max mem: 5511
[23:39:06.222940] Test: Total time: 0:00:05 (0.0334 s / it)
[23:39:06.223851] * Acc@1 83.680 Acc@5 99.260 loss 0.531
[23:39:06.224191] Accuracy of the network on the 10000 test images: 83.7%
[23:39:06.224440] Max accuracy: 84.02%
[23:39:06.481637] log_dir: ./output_dir
[23:39:07.303250] Epoch: [62]  [  0/781]  eta: 0:10:40  lr: 0.000087  training_loss: 1.1587 (1.1587)  mae_loss: 0.0229 (0.0229)  classification_loss: 1.1356 (1.1356)  loss_mask: 0.0002 (0.0002)  time: 0.8198  data: 0.6076  max mem: 5511
[23:39:11.216106] Epoch: [62]  [ 20/781]  eta: 0:02:51  lr: 0.000087  training_loss: 1.1634 (1.2069)  mae_loss: 0.0260 (0.0264)  classification_loss: 1.1340 (1.1800)  loss_mask: 0.0005 (0.0006)  time: 0.1955  data: 0.0002  max mem: 5511
[23:39:15.158177] Epoch: [62]  [ 40/781]  eta: 0:02:36  lr: 0.000087  training_loss: 1.2359 (1.2244)  mae_loss: 0.0271 (0.0266)  classification_loss: 1.2136 (1.1974)  loss_mask: 0.0002 (0.0004)  time: 0.1970  data: 0.0003  max mem: 5511
[23:39:19.132940] Epoch: [62]  [ 60/781]  eta: 0:02:29  lr: 0.000087  training_loss: 1.2347 (1.2314)  mae_loss: 0.0266 (0.0267)  classification_loss: 1.2066 (1.2043)  loss_mask: 0.0002 (0.0005)  time: 0.1986  data: 0.0003  max mem: 5511
[23:39:23.082936] Epoch: [62]  [ 80/781]  eta: 0:02:23  lr: 0.000087  training_loss: 1.2565 (1.2340)  mae_loss: 0.0273 (0.0268)  classification_loss: 1.2248 (1.2068)  loss_mask: 0.0002 (0.0004)  time: 0.1974  data: 0.0002  max mem: 5511
[23:39:27.007212] Epoch: [62]  [100/781]  eta: 0:02:18  lr: 0.000087  training_loss: 1.2638 (1.2409)  mae_loss: 0.0275 (0.0270)  classification_loss: 1.2371 (1.2135)  loss_mask: 0.0002 (0.0004)  time: 0.1961  data: 0.0002  max mem: 5511
[23:39:30.984964] Epoch: [62]  [120/781]  eta: 0:02:13  lr: 0.000086  training_loss: 1.2323 (1.2443)  mae_loss: 0.0251 (0.0268)  classification_loss: 1.2060 (1.2171)  loss_mask: 0.0002 (0.0004)  time: 0.1988  data: 0.0003  max mem: 5511
[23:39:34.912764] Epoch: [62]  [140/781]  eta: 0:02:09  lr: 0.000086  training_loss: 1.2169 (1.2424)  mae_loss: 0.0281 (0.0271)  classification_loss: 1.1886 (1.2150)  loss_mask: 0.0002 (0.0003)  time: 0.1963  data: 0.0002  max mem: 5511
[23:39:38.908687] Epoch: [62]  [160/781]  eta: 0:02:05  lr: 0.000086  training_loss: 1.2452 (1.2424)  mae_loss: 0.0259 (0.0269)  classification_loss: 1.2176 (1.2150)  loss_mask: 0.0004 (0.0005)  time: 0.1997  data: 0.0002  max mem: 5511
[23:39:42.867113] Epoch: [62]  [180/781]  eta: 0:02:00  lr: 0.000086  training_loss: 1.2138 (1.2425)  mae_loss: 0.0266 (0.0270)  classification_loss: 1.1803 (1.2149)  loss_mask: 0.0009 (0.0007)  time: 0.1978  data: 0.0003  max mem: 5511
[23:39:46.798663] Epoch: [62]  [200/781]  eta: 0:01:56  lr: 0.000086  training_loss: 1.2763 (1.2467)  mae_loss: 0.0280 (0.0271)  classification_loss: 1.2445 (1.2189)  loss_mask: 0.0005 (0.0007)  time: 0.1965  data: 0.0002  max mem: 5511
[23:39:50.715626] Epoch: [62]  [220/781]  eta: 0:01:52  lr: 0.000086  training_loss: 1.2307 (1.2476)  mae_loss: 0.0266 (0.0270)  classification_loss: 1.2024 (1.2200)  loss_mask: 0.0008 (0.0007)  time: 0.1958  data: 0.0002  max mem: 5511
[23:39:54.633503] Epoch: [62]  [240/781]  eta: 0:01:48  lr: 0.000086  training_loss: 1.2413 (1.2489)  mae_loss: 0.0263 (0.0270)  classification_loss: 1.2157 (1.2212)  loss_mask: 0.0004 (0.0007)  time: 0.1958  data: 0.0003  max mem: 5511
[23:39:58.612108] Epoch: [62]  [260/781]  eta: 0:01:44  lr: 0.000086  training_loss: 1.2277 (1.2491)  mae_loss: 0.0264 (0.0270)  classification_loss: 1.1984 (1.2214)  loss_mask: 0.0003 (0.0007)  time: 0.1988  data: 0.0002  max mem: 5511
[23:40:02.531837] Epoch: [62]  [280/781]  eta: 0:01:39  lr: 0.000086  training_loss: 1.2704 (1.2481)  mae_loss: 0.0276 (0.0271)  classification_loss: 1.2451 (1.2204)  loss_mask: 0.0002 (0.0007)  time: 0.1959  data: 0.0003  max mem: 5511
[23:40:06.458724] Epoch: [62]  [300/781]  eta: 0:01:35  lr: 0.000086  training_loss: 1.2568 (1.2501)  mae_loss: 0.0253 (0.0269)  classification_loss: 1.2269 (1.2225)  loss_mask: 0.0003 (0.0007)  time: 0.1963  data: 0.0003  max mem: 5511
[23:40:10.424528] Epoch: [62]  [320/781]  eta: 0:01:31  lr: 0.000085  training_loss: 1.2144 (1.2486)  mae_loss: 0.0251 (0.0269)  classification_loss: 1.1876 (1.2211)  loss_mask: 0.0002 (0.0006)  time: 0.1982  data: 0.0002  max mem: 5511
[23:40:14.373010] Epoch: [62]  [340/781]  eta: 0:01:27  lr: 0.000085  training_loss: 1.2360 (1.2470)  mae_loss: 0.0267 (0.0269)  classification_loss: 1.2089 (1.2195)  loss_mask: 0.0001 (0.0006)  time: 0.1973  data: 0.0002  max mem: 5511
[23:40:18.352136] Epoch: [62]  [360/781]  eta: 0:01:23  lr: 0.000085  training_loss: 1.1910 (1.2453)  mae_loss: 0.0268 (0.0269)  classification_loss: 1.1653 (1.2179)  loss_mask: 0.0002 (0.0006)  time: 0.1989  data: 0.0002  max mem: 5511
[23:40:22.302463] Epoch: [62]  [380/781]  eta: 0:01:19  lr: 0.000085  training_loss: 1.2654 (1.2468)  mae_loss: 0.0262 (0.0268)  classification_loss: 1.2327 (1.2194)  loss_mask: 0.0004 (0.0006)  time: 0.1974  data: 0.0002  max mem: 5511
[23:40:26.230201] Epoch: [62]  [400/781]  eta: 0:01:15  lr: 0.000085  training_loss: 1.2379 (1.2476)  mae_loss: 0.0257 (0.0268)  classification_loss: 1.2080 (1.2202)  loss_mask: 0.0003 (0.0006)  time: 0.1963  data: 0.0002  max mem: 5511
[23:40:30.145541] Epoch: [62]  [420/781]  eta: 0:01:11  lr: 0.000085  training_loss: 1.1769 (1.2450)  mae_loss: 0.0266 (0.0268)  classification_loss: 1.1498 (1.2175)  loss_mask: 0.0002 (0.0006)  time: 0.1957  data: 0.0002  max mem: 5511
[23:40:34.108586] Epoch: [62]  [440/781]  eta: 0:01:07  lr: 0.000085  training_loss: 1.1963 (1.2441)  mae_loss: 0.0254 (0.0268)  classification_loss: 1.1719 (1.2167)  loss_mask: 0.0002 (0.0006)  time: 0.1981  data: 0.0003  max mem: 5511
[23:40:38.053695] Epoch: [62]  [460/781]  eta: 0:01:03  lr: 0.000085  training_loss: 1.1458 (1.2411)  mae_loss: 0.0255 (0.0268)  classification_loss: 1.1222 (1.2137)  loss_mask: 0.0003 (0.0006)  time: 0.1972  data: 0.0002  max mem: 5511
[23:40:41.986049] Epoch: [62]  [480/781]  eta: 0:00:59  lr: 0.000085  training_loss: 1.2525 (1.2418)  mae_loss: 0.0267 (0.0268)  classification_loss: 1.2268 (1.2144)  loss_mask: 0.0002 (0.0006)  time: 0.1965  data: 0.0002  max mem: 5511
[23:40:45.938989] Epoch: [62]  [500/781]  eta: 0:00:55  lr: 0.000085  training_loss: 1.2207 (1.2417)  mae_loss: 0.0262 (0.0268)  classification_loss: 1.1957 (1.2144)  loss_mask: 0.0002 (0.0005)  time: 0.1976  data: 0.0002  max mem: 5511
[23:40:49.869363] Epoch: [62]  [520/781]  eta: 0:00:51  lr: 0.000084  training_loss: 1.2065 (1.2400)  mae_loss: 0.0263 (0.0268)  classification_loss: 1.1813 (1.2127)  loss_mask: 0.0002 (0.0005)  time: 0.1964  data: 0.0002  max mem: 5511
[23:40:53.790710] Epoch: [62]  [540/781]  eta: 0:00:47  lr: 0.000084  training_loss: 1.1694 (1.2386)  mae_loss: 0.0251 (0.0267)  classification_loss: 1.1431 (1.2114)  loss_mask: 0.0001 (0.0005)  time: 0.1960  data: 0.0002  max mem: 5511
[23:40:57.725383] Epoch: [62]  [560/781]  eta: 0:00:43  lr: 0.000084  training_loss: 1.2526 (1.2392)  mae_loss: 0.0267 (0.0267)  classification_loss: 1.2243 (1.2119)  loss_mask: 0.0001 (0.0005)  time: 0.1966  data: 0.0002  max mem: 5511
[23:41:01.663448] Epoch: [62]  [580/781]  eta: 0:00:39  lr: 0.000084  training_loss: 1.2413 (1.2398)  mae_loss: 0.0255 (0.0267)  classification_loss: 1.2158 (1.2126)  loss_mask: 0.0002 (0.0005)  time: 0.1968  data: 0.0003  max mem: 5511
[23:41:05.629266] Epoch: [62]  [600/781]  eta: 0:00:35  lr: 0.000084  training_loss: 1.2012 (1.2386)  mae_loss: 0.0258 (0.0267)  classification_loss: 1.1754 (1.2114)  loss_mask: 0.0002 (0.0005)  time: 0.1982  data: 0.0002  max mem: 5511
[23:41:09.563737] Epoch: [62]  [620/781]  eta: 0:00:31  lr: 0.000084  training_loss: 1.2377 (1.2391)  mae_loss: 0.0265 (0.0267)  classification_loss: 1.2110 (1.2119)  loss_mask: 0.0001 (0.0005)  time: 0.1966  data: 0.0002  max mem: 5511
[23:41:13.529259] Epoch: [62]  [640/781]  eta: 0:00:27  lr: 0.000084  training_loss: 1.2191 (1.2393)  mae_loss: 0.0269 (0.0267)  classification_loss: 1.1859 (1.2120)  loss_mask: 0.0004 (0.0005)  time: 0.1982  data: 0.0003  max mem: 5511
[23:41:17.479970] Epoch: [62]  [660/781]  eta: 0:00:23  lr: 0.000084  training_loss: 1.2105 (1.2394)  mae_loss: 0.0270 (0.0268)  classification_loss: 1.1816 (1.2121)  loss_mask: 0.0002 (0.0005)  time: 0.1975  data: 0.0003  max mem: 5511
[23:41:21.481980] Epoch: [62]  [680/781]  eta: 0:00:20  lr: 0.000084  training_loss: 1.2183 (1.2395)  mae_loss: 0.0260 (0.0267)  classification_loss: 1.1953 (1.2122)  loss_mask: 0.0002 (0.0005)  time: 0.2000  data: 0.0002  max mem: 5511
[23:41:25.428074] Epoch: [62]  [700/781]  eta: 0:00:16  lr: 0.000084  training_loss: 1.2798 (1.2399)  mae_loss: 0.0274 (0.0268)  classification_loss: 1.2541 (1.2126)  loss_mask: 0.0001 (0.0005)  time: 0.1972  data: 0.0002  max mem: 5511
[23:41:29.395909] Epoch: [62]  [720/781]  eta: 0:00:12  lr: 0.000083  training_loss: 1.2633 (1.2400)  mae_loss: 0.0260 (0.0267)  classification_loss: 1.2375 (1.2127)  loss_mask: 0.0001 (0.0005)  time: 0.1983  data: 0.0002  max mem: 5511
[23:41:33.312325] Epoch: [62]  [740/781]  eta: 0:00:08  lr: 0.000083  training_loss: 1.2353 (1.2402)  mae_loss: 0.0260 (0.0268)  classification_loss: 1.2111 (1.2130)  loss_mask: 0.0002 (0.0005)  time: 0.1957  data: 0.0002  max mem: 5511
[23:41:37.280559] Epoch: [62]  [760/781]  eta: 0:00:04  lr: 0.000083  training_loss: 1.2409 (1.2401)  mae_loss: 0.0254 (0.0267)  classification_loss: 1.2132 (1.2128)  loss_mask: 0.0002 (0.0005)  time: 0.1983  data: 0.0002  max mem: 5511
[23:41:41.238856] Epoch: [62]  [780/781]  eta: 0:00:00  lr: 0.000083  training_loss: 1.2187 (1.2398)  mae_loss: 0.0281 (0.0268)  classification_loss: 1.1881 (1.2125)  loss_mask: 0.0001 (0.0005)  time: 0.1978  data: 0.0002  max mem: 5511
[23:41:41.398578] Epoch: [62] Total time: 0:02:34 (0.1984 s / it)
[23:41:41.399059] Averaged stats: lr: 0.000083  training_loss: 1.2187 (1.2398)  mae_loss: 0.0281 (0.0268)  classification_loss: 1.1881 (1.2125)  loss_mask: 0.0001 (0.0005)
[23:41:42.038516] Test:  [  0/157]  eta: 0:01:39  testing_loss: 0.5619 (0.5619)  acc1: 82.8125 (82.8125)  acc5: 98.4375 (98.4375)  time: 0.6351  data: 0.6007  max mem: 5511
[23:41:42.330462] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5499 (0.5579)  acc1: 82.8125 (82.3864)  acc5: 98.4375 (99.0057)  time: 0.0841  data: 0.0549  max mem: 5511
[23:41:42.618942] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.5205 (0.5132)  acc1: 82.8125 (84.3750)  acc5: 100.0000 (99.3304)  time: 0.0289  data: 0.0003  max mem: 5511
[23:41:42.908096] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5359 (0.5299)  acc1: 84.3750 (84.0222)  acc5: 100.0000 (99.1935)  time: 0.0287  data: 0.0002  max mem: 5511
[23:41:43.197588] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5234 (0.5283)  acc1: 84.3750 (84.2988)  acc5: 98.4375 (99.0854)  time: 0.0288  data: 0.0002  max mem: 5511
[23:41:43.485336] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4881 (0.5211)  acc1: 84.3750 (84.3750)  acc5: 98.4375 (99.0502)  time: 0.0287  data: 0.0002  max mem: 5511
[23:41:43.769273] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4844 (0.5148)  acc1: 84.3750 (84.6055)  acc5: 100.0000 (99.1547)  time: 0.0284  data: 0.0002  max mem: 5511
[23:41:44.057476] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4669 (0.5057)  acc1: 84.3750 (84.8151)  acc5: 100.0000 (99.1857)  time: 0.0285  data: 0.0002  max mem: 5511
[23:41:44.343814] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4705 (0.5146)  acc1: 84.3750 (84.4907)  acc5: 100.0000 (99.2091)  time: 0.0286  data: 0.0002  max mem: 5511
[23:41:44.637192] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5010 (0.5117)  acc1: 84.3750 (84.5467)  acc5: 98.4375 (99.1930)  time: 0.0288  data: 0.0002  max mem: 5511
[23:41:44.921600] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5209 (0.5146)  acc1: 84.3750 (84.3905)  acc5: 98.4375 (99.1646)  time: 0.0287  data: 0.0002  max mem: 5511
[23:41:45.206631] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5372 (0.5139)  acc1: 82.8125 (84.4172)  acc5: 100.0000 (99.1976)  time: 0.0283  data: 0.0002  max mem: 5511
[23:41:45.492075] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5156 (0.5128)  acc1: 82.8125 (84.4396)  acc5: 100.0000 (99.2123)  time: 0.0284  data: 0.0002  max mem: 5511
[23:41:45.783418] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4860 (0.5128)  acc1: 84.3750 (84.4227)  acc5: 100.0000 (99.2247)  time: 0.0287  data: 0.0002  max mem: 5511
[23:41:46.066656] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4930 (0.5131)  acc1: 84.3750 (84.4415)  acc5: 100.0000 (99.2465)  time: 0.0286  data: 0.0002  max mem: 5511
[23:41:46.349764] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5102 (0.5125)  acc1: 84.3750 (84.4371)  acc5: 100.0000 (99.2446)  time: 0.0282  data: 0.0001  max mem: 5511
[23:41:46.505410] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5102 (0.5128)  acc1: 84.3750 (84.4300)  acc5: 100.0000 (99.2400)  time: 0.0274  data: 0.0001  max mem: 5511
[23:41:46.667624] Test: Total time: 0:00:05 (0.0335 s / it)
[23:41:46.668190] * Acc@1 84.430 Acc@5 99.240 loss 0.513
[23:41:46.668485] Accuracy of the network on the 10000 test images: 84.4%
[23:41:46.668728] Max accuracy: 84.43%
[23:41:47.020764] log_dir: ./output_dir
[23:41:47.905647] Epoch: [63]  [  0/781]  eta: 0:11:29  lr: 0.000083  training_loss: 1.1605 (1.1605)  mae_loss: 0.0224 (0.0224)  classification_loss: 1.1378 (1.1378)  loss_mask: 0.0004 (0.0004)  time: 0.8829  data: 0.6682  max mem: 5511
[23:41:51.851594] Epoch: [63]  [ 20/781]  eta: 0:02:54  lr: 0.000083  training_loss: 1.1502 (1.1731)  mae_loss: 0.0252 (0.0251)  classification_loss: 1.1295 (1.1476)  loss_mask: 0.0002 (0.0003)  time: 0.1972  data: 0.0002  max mem: 5511
[23:41:55.806791] Epoch: [63]  [ 40/781]  eta: 0:02:38  lr: 0.000083  training_loss: 1.2734 (1.2107)  mae_loss: 0.0261 (0.0256)  classification_loss: 1.2465 (1.1848)  loss_mask: 0.0002 (0.0003)  time: 0.1976  data: 0.0002  max mem: 5511
[23:41:59.759805] Epoch: [63]  [ 60/781]  eta: 0:02:30  lr: 0.000083  training_loss: 1.2306 (1.2260)  mae_loss: 0.0268 (0.0260)  classification_loss: 1.2037 (1.1997)  loss_mask: 0.0002 (0.0003)  time: 0.1976  data: 0.0002  max mem: 5511
[23:42:03.726753] Epoch: [63]  [ 80/781]  eta: 0:02:24  lr: 0.000083  training_loss: 1.2075 (1.2283)  mae_loss: 0.0267 (0.0261)  classification_loss: 1.1757 (1.2017)  loss_mask: 0.0002 (0.0004)  time: 0.1983  data: 0.0002  max mem: 5511
[23:42:07.653575] Epoch: [63]  [100/781]  eta: 0:02:19  lr: 0.000083  training_loss: 1.2246 (1.2313)  mae_loss: 0.0255 (0.0262)  classification_loss: 1.1966 (1.2046)  loss_mask: 0.0002 (0.0005)  time: 0.1962  data: 0.0002  max mem: 5511
[23:42:11.599080] Epoch: [63]  [120/781]  eta: 0:02:14  lr: 0.000083  training_loss: 1.2192 (1.2284)  mae_loss: 0.0264 (0.0262)  classification_loss: 1.1930 (1.2018)  loss_mask: 0.0001 (0.0004)  time: 0.1972  data: 0.0002  max mem: 5511
[23:42:15.545001] Epoch: [63]  [140/781]  eta: 0:02:09  lr: 0.000082  training_loss: 1.2234 (1.2296)  mae_loss: 0.0261 (0.0262)  classification_loss: 1.1944 (1.2030)  loss_mask: 0.0002 (0.0004)  time: 0.1972  data: 0.0002  max mem: 5511
[23:42:19.490916] Epoch: [63]  [160/781]  eta: 0:02:05  lr: 0.000082  training_loss: 1.2085 (1.2273)  mae_loss: 0.0255 (0.0262)  classification_loss: 1.1841 (1.2008)  loss_mask: 0.0002 (0.0004)  time: 0.1972  data: 0.0002  max mem: 5511
[23:42:23.406182] Epoch: [63]  [180/781]  eta: 0:02:00  lr: 0.000082  training_loss: 1.1994 (1.2293)  mae_loss: 0.0245 (0.0261)  classification_loss: 1.1754 (1.2028)  loss_mask: 0.0001 (0.0004)  time: 0.1957  data: 0.0002  max mem: 5511
[23:42:27.342580] Epoch: [63]  [200/781]  eta: 0:01:56  lr: 0.000082  training_loss: 1.2642 (1.2295)  mae_loss: 0.0254 (0.0261)  classification_loss: 1.2381 (1.2031)  loss_mask: 0.0001 (0.0004)  time: 0.1967  data: 0.0002  max mem: 5511
[23:42:31.272771] Epoch: [63]  [220/781]  eta: 0:01:52  lr: 0.000082  training_loss: 1.2039 (1.2284)  mae_loss: 0.0245 (0.0260)  classification_loss: 1.1795 (1.2020)  loss_mask: 0.0004 (0.0004)  time: 0.1964  data: 0.0003  max mem: 5511
[23:42:35.215900] Epoch: [63]  [240/781]  eta: 0:01:48  lr: 0.000082  training_loss: 1.2338 (1.2298)  mae_loss: 0.0257 (0.0261)  classification_loss: 1.2088 (1.2032)  loss_mask: 0.0005 (0.0005)  time: 0.1971  data: 0.0002  max mem: 5511
[23:42:39.132940] Epoch: [63]  [260/781]  eta: 0:01:43  lr: 0.000082  training_loss: 1.2461 (1.2296)  mae_loss: 0.0259 (0.0261)  classification_loss: 1.1920 (1.2024)  loss_mask: 0.0020 (0.0010)  time: 0.1958  data: 0.0002  max mem: 5511
[23:42:43.065977] Epoch: [63]  [280/781]  eta: 0:01:39  lr: 0.000082  training_loss: 1.2609 (1.2317)  mae_loss: 0.0257 (0.0262)  classification_loss: 1.2268 (1.2044)  loss_mask: 0.0010 (0.0011)  time: 0.1966  data: 0.0002  max mem: 5511
[23:42:47.004285] Epoch: [63]  [300/781]  eta: 0:01:35  lr: 0.000082  training_loss: 1.2256 (1.2330)  mae_loss: 0.0271 (0.0262)  classification_loss: 1.1968 (1.2057)  loss_mask: 0.0005 (0.0011)  time: 0.1968  data: 0.0002  max mem: 5511
[23:42:50.957137] Epoch: [63]  [320/781]  eta: 0:01:31  lr: 0.000082  training_loss: 1.2174 (1.2325)  mae_loss: 0.0248 (0.0261)  classification_loss: 1.1927 (1.2053)  loss_mask: 0.0003 (0.0011)  time: 0.1975  data: 0.0002  max mem: 5511
[23:42:54.903480] Epoch: [63]  [340/781]  eta: 0:01:27  lr: 0.000081  training_loss: 1.2087 (1.2314)  mae_loss: 0.0263 (0.0262)  classification_loss: 1.1823 (1.2042)  loss_mask: 0.0002 (0.0010)  time: 0.1972  data: 0.0002  max mem: 5511
[23:42:58.842221] Epoch: [63]  [360/781]  eta: 0:01:23  lr: 0.000081  training_loss: 1.2168 (1.2312)  mae_loss: 0.0263 (0.0262)  classification_loss: 1.1929 (1.2040)  loss_mask: 0.0002 (0.0010)  time: 0.1968  data: 0.0002  max mem: 5511
[23:43:02.778526] Epoch: [63]  [380/781]  eta: 0:01:19  lr: 0.000081  training_loss: 1.2014 (1.2320)  mae_loss: 0.0276 (0.0263)  classification_loss: 1.1738 (1.2048)  loss_mask: 0.0001 (0.0009)  time: 0.1967  data: 0.0003  max mem: 5511
[23:43:06.704110] Epoch: [63]  [400/781]  eta: 0:01:15  lr: 0.000081  training_loss: 1.2509 (1.2331)  mae_loss: 0.0252 (0.0263)  classification_loss: 1.2211 (1.2058)  loss_mask: 0.0005 (0.0009)  time: 0.1962  data: 0.0002  max mem: 5511
[23:43:10.627927] Epoch: [63]  [420/781]  eta: 0:01:11  lr: 0.000081  training_loss: 1.1827 (1.2313)  mae_loss: 0.0261 (0.0263)  classification_loss: 1.1519 (1.2041)  loss_mask: 0.0012 (0.0010)  time: 0.1961  data: 0.0002  max mem: 5511
[23:43:14.558364] Epoch: [63]  [440/781]  eta: 0:01:07  lr: 0.000081  training_loss: 1.2214 (1.2320)  mae_loss: 0.0261 (0.0263)  classification_loss: 1.1956 (1.2047)  loss_mask: 0.0003 (0.0010)  time: 0.1964  data: 0.0002  max mem: 5511
[23:43:18.523643] Epoch: [63]  [460/781]  eta: 0:01:03  lr: 0.000081  training_loss: 1.2324 (1.2323)  mae_loss: 0.0254 (0.0263)  classification_loss: 1.2064 (1.2050)  loss_mask: 0.0004 (0.0011)  time: 0.1982  data: 0.0002  max mem: 5511
[23:43:22.468229] Epoch: [63]  [480/781]  eta: 0:00:59  lr: 0.000081  training_loss: 1.2403 (1.2335)  mae_loss: 0.0254 (0.0263)  classification_loss: 1.2094 (1.2060)  loss_mask: 0.0031 (0.0013)  time: 0.1971  data: 0.0003  max mem: 5511
[23:43:26.387916] Epoch: [63]  [500/781]  eta: 0:00:55  lr: 0.000081  training_loss: 1.2285 (1.2337)  mae_loss: 0.0249 (0.0262)  classification_loss: 1.2000 (1.2061)  loss_mask: 0.0017 (0.0014)  time: 0.1959  data: 0.0002  max mem: 5511
[23:43:30.308955] Epoch: [63]  [520/781]  eta: 0:00:51  lr: 0.000081  training_loss: 1.2342 (1.2335)  mae_loss: 0.0257 (0.0262)  classification_loss: 1.2094 (1.2056)  loss_mask: 0.0074 (0.0017)  time: 0.1959  data: 0.0002  max mem: 5511
[23:43:34.254804] Epoch: [63]  [540/781]  eta: 0:00:47  lr: 0.000080  training_loss: 1.2734 (1.2346)  mae_loss: 0.0272 (0.0263)  classification_loss: 1.2399 (1.2064)  loss_mask: 0.0034 (0.0019)  time: 0.1972  data: 0.0003  max mem: 5511
[23:43:38.182735] Epoch: [63]  [560/781]  eta: 0:00:43  lr: 0.000080  training_loss: 1.2758 (1.2353)  mae_loss: 0.0259 (0.0263)  classification_loss: 1.2469 (1.2071)  loss_mask: 0.0019 (0.0020)  time: 0.1963  data: 0.0002  max mem: 5511
[23:43:42.111390] Epoch: [63]  [580/781]  eta: 0:00:39  lr: 0.000080  training_loss: 1.2448 (1.2367)  mae_loss: 0.0280 (0.0264)  classification_loss: 1.2138 (1.2084)  loss_mask: 0.0012 (0.0020)  time: 0.1963  data: 0.0002  max mem: 5511
[23:43:46.029085] Epoch: [63]  [600/781]  eta: 0:00:35  lr: 0.000080  training_loss: 1.2378 (1.2372)  mae_loss: 0.0270 (0.0264)  classification_loss: 1.2070 (1.2088)  loss_mask: 0.0010 (0.0019)  time: 0.1958  data: 0.0002  max mem: 5511
[23:43:49.962149] Epoch: [63]  [620/781]  eta: 0:00:31  lr: 0.000080  training_loss: 1.2111 (1.2362)  mae_loss: 0.0263 (0.0264)  classification_loss: 1.1857 (1.2079)  loss_mask: 0.0006 (0.0019)  time: 0.1966  data: 0.0002  max mem: 5511
[23:43:53.898232] Epoch: [63]  [640/781]  eta: 0:00:27  lr: 0.000080  training_loss: 1.2300 (1.2362)  mae_loss: 0.0262 (0.0264)  classification_loss: 1.2030 (1.2079)  loss_mask: 0.0005 (0.0019)  time: 0.1967  data: 0.0002  max mem: 5511
[23:43:57.857278] Epoch: [63]  [660/781]  eta: 0:00:23  lr: 0.000080  training_loss: 1.2280 (1.2357)  mae_loss: 0.0266 (0.0264)  classification_loss: 1.2002 (1.2074)  loss_mask: 0.0004 (0.0018)  time: 0.1979  data: 0.0002  max mem: 5511
[23:44:01.801851] Epoch: [63]  [680/781]  eta: 0:00:19  lr: 0.000080  training_loss: 1.2102 (1.2356)  mae_loss: 0.0256 (0.0264)  classification_loss: 1.1872 (1.2074)  loss_mask: 0.0002 (0.0018)  time: 0.1971  data: 0.0002  max mem: 5511
[23:44:05.723343] Epoch: [63]  [700/781]  eta: 0:00:16  lr: 0.000080  training_loss: 1.2150 (1.2354)  mae_loss: 0.0261 (0.0264)  classification_loss: 1.1846 (1.2072)  loss_mask: 0.0002 (0.0017)  time: 0.1960  data: 0.0002  max mem: 5511
[23:44:09.652367] Epoch: [63]  [720/781]  eta: 0:00:12  lr: 0.000080  training_loss: 1.2410 (1.2357)  mae_loss: 0.0269 (0.0264)  classification_loss: 1.2098 (1.2076)  loss_mask: 0.0003 (0.0017)  time: 0.1964  data: 0.0002  max mem: 5511
[23:44:13.590937] Epoch: [63]  [740/781]  eta: 0:00:08  lr: 0.000079  training_loss: 1.1976 (1.2353)  mae_loss: 0.0263 (0.0264)  classification_loss: 1.1743 (1.2071)  loss_mask: 0.0002 (0.0017)  time: 0.1968  data: 0.0003  max mem: 5511
[23:44:17.541641] Epoch: [63]  [760/781]  eta: 0:00:04  lr: 0.000079  training_loss: 1.2202 (1.2358)  mae_loss: 0.0250 (0.0264)  classification_loss: 1.1892 (1.2077)  loss_mask: 0.0007 (0.0017)  time: 0.1974  data: 0.0002  max mem: 5511
[23:44:21.469829] Epoch: [63]  [780/781]  eta: 0:00:00  lr: 0.000079  training_loss: 1.2049 (1.2353)  mae_loss: 0.0257 (0.0264)  classification_loss: 1.1782 (1.2073)  loss_mask: 0.0003 (0.0017)  time: 0.1963  data: 0.0002  max mem: 5511
[23:44:21.632958] Epoch: [63] Total time: 0:02:34 (0.1980 s / it)
[23:44:21.633451] Averaged stats: lr: 0.000079  training_loss: 1.2049 (1.2353)  mae_loss: 0.0257 (0.0264)  classification_loss: 1.1782 (1.2073)  loss_mask: 0.0003 (0.0017)
[23:44:22.279032] Test:  [  0/157]  eta: 0:01:40  testing_loss: 0.5278 (0.5278)  acc1: 84.3750 (84.3750)  acc5: 96.8750 (96.8750)  time: 0.6411  data: 0.6115  max mem: 5511
[23:44:22.579913] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5278 (0.5400)  acc1: 84.3750 (83.3807)  acc5: 100.0000 (99.1477)  time: 0.0855  data: 0.0558  max mem: 5511
[23:44:22.871681] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5041 (0.4959)  acc1: 84.3750 (85.1190)  acc5: 100.0000 (99.2560)  time: 0.0294  data: 0.0003  max mem: 5511
[23:44:23.161166] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5146 (0.5237)  acc1: 84.3750 (83.9718)  acc5: 100.0000 (99.0927)  time: 0.0288  data: 0.0003  max mem: 5511
[23:44:23.447100] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5259 (0.5249)  acc1: 84.3750 (84.2226)  acc5: 98.4375 (98.9710)  time: 0.0286  data: 0.0002  max mem: 5511
[23:44:23.733947] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5039 (0.5211)  acc1: 84.3750 (84.2525)  acc5: 100.0000 (99.0196)  time: 0.0285  data: 0.0002  max mem: 5511
[23:44:24.018024] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5006 (0.5173)  acc1: 84.3750 (84.3494)  acc5: 100.0000 (99.0779)  time: 0.0284  data: 0.0002  max mem: 5511
[23:44:24.307337] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4938 (0.5114)  acc1: 84.3750 (84.4410)  acc5: 100.0000 (99.1197)  time: 0.0285  data: 0.0002  max mem: 5511
[23:44:24.592408] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4938 (0.5177)  acc1: 84.3750 (84.3943)  acc5: 100.0000 (99.1319)  time: 0.0286  data: 0.0002  max mem: 5511
[23:44:24.875891] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5219 (0.5137)  acc1: 84.3750 (84.5810)  acc5: 98.4375 (99.0900)  time: 0.0283  data: 0.0002  max mem: 5511
[23:44:25.159190] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5378 (0.5184)  acc1: 84.3750 (84.3595)  acc5: 98.4375 (99.0408)  time: 0.0282  data: 0.0002  max mem: 5511
[23:44:25.445996] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5401 (0.5189)  acc1: 82.8125 (84.3328)  acc5: 98.4375 (99.0709)  time: 0.0284  data: 0.0002  max mem: 5511
[23:44:25.730603] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5394 (0.5188)  acc1: 82.8125 (84.2846)  acc5: 100.0000 (99.0702)  time: 0.0284  data: 0.0002  max mem: 5511
[23:44:26.013380] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4779 (0.5176)  acc1: 85.9375 (84.3989)  acc5: 100.0000 (99.0577)  time: 0.0282  data: 0.0002  max mem: 5511
[23:44:26.297319] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5055 (0.5158)  acc1: 85.9375 (84.3972)  acc5: 100.0000 (99.0913)  time: 0.0282  data: 0.0002  max mem: 5511
[23:44:26.591215] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5153 (0.5151)  acc1: 84.3750 (84.4267)  acc5: 100.0000 (99.0791)  time: 0.0287  data: 0.0002  max mem: 5511
[23:44:26.746409] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4951 (0.5147)  acc1: 84.3750 (84.4200)  acc5: 98.4375 (99.0700)  time: 0.0279  data: 0.0001  max mem: 5511
[23:44:26.911267] Test: Total time: 0:00:05 (0.0336 s / it)
[23:44:26.912033] * Acc@1 84.420 Acc@5 99.070 loss 0.515
[23:44:26.912551] Accuracy of the network on the 10000 test images: 84.4%
[23:44:26.913386] Max accuracy: 84.43%
[23:44:27.136554] log_dir: ./output_dir
[23:44:28.044913] Epoch: [64]  [  0/781]  eta: 0:11:47  lr: 0.000079  training_loss: 1.0686 (1.0686)  mae_loss: 0.0259 (0.0259)  classification_loss: 1.0420 (1.0420)  loss_mask: 0.0007 (0.0007)  time: 0.9061  data: 0.6733  max mem: 5511
[23:44:31.988814] Epoch: [64]  [ 20/781]  eta: 0:02:55  lr: 0.000079  training_loss: 1.1949 (1.1876)  mae_loss: 0.0260 (0.0271)  classification_loss: 1.1686 (1.1603)  loss_mask: 0.0002 (0.0003)  time: 0.1971  data: 0.0002  max mem: 5511
[23:44:35.933429] Epoch: [64]  [ 40/781]  eta: 0:02:38  lr: 0.000079  training_loss: 1.2048 (1.2013)  mae_loss: 0.0254 (0.0267)  classification_loss: 1.1779 (1.1742)  loss_mask: 0.0003 (0.0004)  time: 0.1971  data: 0.0002  max mem: 5511
[23:44:39.847917] Epoch: [64]  [ 60/781]  eta: 0:02:30  lr: 0.000079  training_loss: 1.2343 (1.2160)  mae_loss: 0.0259 (0.0271)  classification_loss: 1.1999 (1.1886)  loss_mask: 0.0002 (0.0004)  time: 0.1956  data: 0.0003  max mem: 5511
[23:44:43.774439] Epoch: [64]  [ 80/781]  eta: 0:02:23  lr: 0.000079  training_loss: 1.1928 (1.2087)  mae_loss: 0.0242 (0.0265)  classification_loss: 1.1669 (1.1818)  loss_mask: 0.0003 (0.0004)  time: 0.1962  data: 0.0002  max mem: 5511
[23:44:47.698634] Epoch: [64]  [100/781]  eta: 0:02:18  lr: 0.000079  training_loss: 1.2257 (1.2128)  mae_loss: 0.0248 (0.0262)  classification_loss: 1.2000 (1.1861)  loss_mask: 0.0003 (0.0005)  time: 0.1961  data: 0.0002  max mem: 5511
[23:44:51.630210] Epoch: [64]  [120/781]  eta: 0:02:13  lr: 0.000079  training_loss: 1.1783 (1.2104)  mae_loss: 0.0268 (0.0264)  classification_loss: 1.1568 (1.1835)  loss_mask: 0.0004 (0.0005)  time: 0.1965  data: 0.0002  max mem: 5511
[23:44:55.581754] Epoch: [64]  [140/781]  eta: 0:02:09  lr: 0.000079  training_loss: 1.2345 (1.2121)  mae_loss: 0.0247 (0.0263)  classification_loss: 1.2110 (1.1853)  loss_mask: 0.0002 (0.0005)  time: 0.1974  data: 0.0002  max mem: 5511
[23:44:59.524542] Epoch: [64]  [160/781]  eta: 0:02:04  lr: 0.000079  training_loss: 1.1881 (1.2120)  mae_loss: 0.0257 (0.0262)  classification_loss: 1.1591 (1.1852)  loss_mask: 0.0003 (0.0006)  time: 0.1971  data: 0.0002  max mem: 5511
[23:45:03.543927] Epoch: [64]  [180/781]  eta: 0:02:00  lr: 0.000078  training_loss: 1.2085 (1.2122)  mae_loss: 0.0256 (0.0262)  classification_loss: 1.1804 (1.1855)  loss_mask: 0.0003 (0.0006)  time: 0.2009  data: 0.0002  max mem: 5511
[23:45:07.472402] Epoch: [64]  [200/781]  eta: 0:01:56  lr: 0.000078  training_loss: 1.2260 (1.2130)  mae_loss: 0.0267 (0.0262)  classification_loss: 1.2025 (1.1863)  loss_mask: 0.0003 (0.0005)  time: 0.1963  data: 0.0002  max mem: 5511
[23:45:11.400158] Epoch: [64]  [220/781]  eta: 0:01:52  lr: 0.000078  training_loss: 1.2165 (1.2131)  mae_loss: 0.0263 (0.0263)  classification_loss: 1.1910 (1.1863)  loss_mask: 0.0002 (0.0005)  time: 0.1963  data: 0.0002  max mem: 5511
[23:45:15.321793] Epoch: [64]  [240/781]  eta: 0:01:48  lr: 0.000078  training_loss: 1.1976 (1.2145)  mae_loss: 0.0250 (0.0263)  classification_loss: 1.1741 (1.1878)  loss_mask: 0.0002 (0.0005)  time: 0.1959  data: 0.0003  max mem: 5511
[23:45:19.252475] Epoch: [64]  [260/781]  eta: 0:01:43  lr: 0.000078  training_loss: 1.1833 (1.2147)  mae_loss: 0.0249 (0.0262)  classification_loss: 1.1596 (1.1880)  loss_mask: 0.0003 (0.0005)  time: 0.1964  data: 0.0004  max mem: 5511
[23:45:23.205976] Epoch: [64]  [280/781]  eta: 0:01:39  lr: 0.000078  training_loss: 1.1777 (1.2121)  mae_loss: 0.0268 (0.0263)  classification_loss: 1.1525 (1.1853)  loss_mask: 0.0004 (0.0005)  time: 0.1976  data: 0.0002  max mem: 5511
[23:45:27.134339] Epoch: [64]  [300/781]  eta: 0:01:35  lr: 0.000078  training_loss: 1.2536 (1.2144)  mae_loss: 0.0260 (0.0263)  classification_loss: 1.2201 (1.1876)  loss_mask: 0.0005 (0.0005)  time: 0.1963  data: 0.0002  max mem: 5511
[23:45:31.049880] Epoch: [64]  [320/781]  eta: 0:01:31  lr: 0.000078  training_loss: 1.2309 (1.2164)  mae_loss: 0.0255 (0.0263)  classification_loss: 1.2094 (1.1896)  loss_mask: 0.0003 (0.0005)  time: 0.1957  data: 0.0002  max mem: 5511
[23:45:34.996698] Epoch: [64]  [340/781]  eta: 0:01:27  lr: 0.000078  training_loss: 1.1545 (1.2148)  mae_loss: 0.0271 (0.0263)  classification_loss: 1.1260 (1.1880)  loss_mask: 0.0002 (0.0005)  time: 0.1973  data: 0.0002  max mem: 5511
[23:45:38.916670] Epoch: [64]  [360/781]  eta: 0:01:23  lr: 0.000078  training_loss: 1.1906 (1.2152)  mae_loss: 0.0252 (0.0263)  classification_loss: 1.1639 (1.1884)  loss_mask: 0.0003 (0.0005)  time: 0.1959  data: 0.0002  max mem: 5511
[23:45:42.838040] Epoch: [64]  [380/781]  eta: 0:01:19  lr: 0.000077  training_loss: 1.2865 (1.2190)  mae_loss: 0.0288 (0.0264)  classification_loss: 1.2589 (1.1922)  loss_mask: 0.0002 (0.0005)  time: 0.1960  data: 0.0002  max mem: 5511
[23:45:46.765305] Epoch: [64]  [400/781]  eta: 0:01:15  lr: 0.000077  training_loss: 1.2406 (1.2202)  mae_loss: 0.0265 (0.0264)  classification_loss: 1.2142 (1.1933)  loss_mask: 0.0002 (0.0005)  time: 0.1963  data: 0.0002  max mem: 5511
[23:45:50.682961] Epoch: [64]  [420/781]  eta: 0:01:11  lr: 0.000077  training_loss: 1.2390 (1.2215)  mae_loss: 0.0261 (0.0264)  classification_loss: 1.2107 (1.1946)  loss_mask: 0.0002 (0.0005)  time: 0.1958  data: 0.0002  max mem: 5511
[23:45:54.612460] Epoch: [64]  [440/781]  eta: 0:01:07  lr: 0.000077  training_loss: 1.2251 (1.2219)  mae_loss: 0.0258 (0.0264)  classification_loss: 1.1998 (1.1950)  loss_mask: 0.0003 (0.0005)  time: 0.1964  data: 0.0002  max mem: 5511
[23:45:58.525323] Epoch: [64]  [460/781]  eta: 0:01:03  lr: 0.000077  training_loss: 1.2046 (1.2213)  mae_loss: 0.0251 (0.0263)  classification_loss: 1.1792 (1.1944)  loss_mask: 0.0002 (0.0005)  time: 0.1956  data: 0.0002  max mem: 5511
[23:46:02.476700] Epoch: [64]  [480/781]  eta: 0:00:59  lr: 0.000077  training_loss: 1.1859 (1.2198)  mae_loss: 0.0270 (0.0264)  classification_loss: 1.1614 (1.1930)  loss_mask: 0.0002 (0.0005)  time: 0.1975  data: 0.0002  max mem: 5511
[23:46:06.401575] Epoch: [64]  [500/781]  eta: 0:00:55  lr: 0.000077  training_loss: 1.2309 (1.2202)  mae_loss: 0.0272 (0.0264)  classification_loss: 1.2087 (1.1933)  loss_mask: 0.0001 (0.0005)  time: 0.1962  data: 0.0003  max mem: 5511
[23:46:10.328810] Epoch: [64]  [520/781]  eta: 0:00:51  lr: 0.000077  training_loss: 1.1832 (1.2188)  mae_loss: 0.0263 (0.0264)  classification_loss: 1.1582 (1.1919)  loss_mask: 0.0003 (0.0005)  time: 0.1963  data: 0.0002  max mem: 5511
[23:46:14.275609] Epoch: [64]  [540/781]  eta: 0:00:47  lr: 0.000077  training_loss: 1.2197 (1.2194)  mae_loss: 0.0270 (0.0264)  classification_loss: 1.1912 (1.1925)  loss_mask: 0.0002 (0.0005)  time: 0.1973  data: 0.0002  max mem: 5511
[23:46:18.226740] Epoch: [64]  [560/781]  eta: 0:00:43  lr: 0.000077  training_loss: 1.2016 (1.2191)  mae_loss: 0.0254 (0.0264)  classification_loss: 1.1777 (1.1922)  loss_mask: 0.0002 (0.0005)  time: 0.1975  data: 0.0002  max mem: 5511
[23:46:22.151725] Epoch: [64]  [580/781]  eta: 0:00:39  lr: 0.000076  training_loss: 1.1857 (1.2191)  mae_loss: 0.0250 (0.0264)  classification_loss: 1.1586 (1.1923)  loss_mask: 0.0002 (0.0005)  time: 0.1962  data: 0.0002  max mem: 5511
[23:46:26.071820] Epoch: [64]  [600/781]  eta: 0:00:35  lr: 0.000076  training_loss: 1.2298 (1.2197)  mae_loss: 0.0262 (0.0264)  classification_loss: 1.2037 (1.1928)  loss_mask: 0.0003 (0.0005)  time: 0.1959  data: 0.0002  max mem: 5511
[23:46:30.021915] Epoch: [64]  [620/781]  eta: 0:00:31  lr: 0.000076  training_loss: 1.2140 (1.2199)  mae_loss: 0.0273 (0.0264)  classification_loss: 1.1839 (1.1930)  loss_mask: 0.0002 (0.0005)  time: 0.1974  data: 0.0002  max mem: 5511
[23:46:33.990646] Epoch: [64]  [640/781]  eta: 0:00:27  lr: 0.000076  training_loss: 1.2030 (1.2200)  mae_loss: 0.0245 (0.0264)  classification_loss: 1.1716 (1.1931)  loss_mask: 0.0003 (0.0005)  time: 0.1984  data: 0.0002  max mem: 5511
[23:46:37.900628] Epoch: [64]  [660/781]  eta: 0:00:23  lr: 0.000076  training_loss: 1.2404 (1.2211)  mae_loss: 0.0273 (0.0264)  classification_loss: 1.2151 (1.1942)  loss_mask: 0.0001 (0.0005)  time: 0.1954  data: 0.0001  max mem: 5511
[23:46:41.820518] Epoch: [64]  [680/781]  eta: 0:00:19  lr: 0.000076  training_loss: 1.2314 (1.2218)  mae_loss: 0.0262 (0.0264)  classification_loss: 1.2064 (1.1948)  loss_mask: 0.0001 (0.0005)  time: 0.1959  data: 0.0002  max mem: 5511
[23:46:45.747690] Epoch: [64]  [700/781]  eta: 0:00:16  lr: 0.000076  training_loss: 1.2692 (1.2230)  mae_loss: 0.0267 (0.0265)  classification_loss: 1.2402 (1.1961)  loss_mask: 0.0002 (0.0005)  time: 0.1963  data: 0.0002  max mem: 5511
[23:46:49.685739] Epoch: [64]  [720/781]  eta: 0:00:12  lr: 0.000076  training_loss: 1.2442 (1.2240)  mae_loss: 0.0254 (0.0264)  classification_loss: 1.2175 (1.1971)  loss_mask: 0.0003 (0.0005)  time: 0.1968  data: 0.0002  max mem: 5511
[23:46:53.632839] Epoch: [64]  [740/781]  eta: 0:00:08  lr: 0.000076  training_loss: 1.1545 (1.2229)  mae_loss: 0.0249 (0.0264)  classification_loss: 1.1336 (1.1960)  loss_mask: 0.0002 (0.0005)  time: 0.1973  data: 0.0002  max mem: 5511
[23:46:57.603031] Epoch: [64]  [760/781]  eta: 0:00:04  lr: 0.000076  training_loss: 1.2080 (1.2228)  mae_loss: 0.0260 (0.0264)  classification_loss: 1.1782 (1.1959)  loss_mask: 0.0002 (0.0005)  time: 0.1984  data: 0.0002  max mem: 5511
[23:47:01.533884] Epoch: [64]  [780/781]  eta: 0:00:00  lr: 0.000075  training_loss: 1.2064 (1.2227)  mae_loss: 0.0259 (0.0264)  classification_loss: 1.1814 (1.1958)  loss_mask: 0.0001 (0.0005)  time: 0.1964  data: 0.0002  max mem: 5511
[23:47:01.685980] Epoch: [64] Total time: 0:02:34 (0.1979 s / it)
[23:47:01.686465] Averaged stats: lr: 0.000075  training_loss: 1.2064 (1.2227)  mae_loss: 0.0259 (0.0264)  classification_loss: 1.1814 (1.1958)  loss_mask: 0.0001 (0.0005)
[23:47:02.364033] Test:  [  0/157]  eta: 0:01:45  testing_loss: 0.5052 (0.5052)  acc1: 84.3750 (84.3750)  acc5: 98.4375 (98.4375)  time: 0.6734  data: 0.6150  max mem: 5511
[23:47:02.650616] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5867 (0.5678)  acc1: 82.8125 (82.1023)  acc5: 98.4375 (99.0057)  time: 0.0871  data: 0.0561  max mem: 5511
[23:47:02.936990] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5367 (0.5172)  acc1: 82.8125 (83.7798)  acc5: 100.0000 (99.2560)  time: 0.0285  data: 0.0002  max mem: 5511
[23:47:03.223851] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5208 (0.5405)  acc1: 82.8125 (82.8125)  acc5: 100.0000 (99.1431)  time: 0.0285  data: 0.0002  max mem: 5511
[23:47:03.518159] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5425 (0.5354)  acc1: 84.3750 (83.4223)  acc5: 100.0000 (99.0854)  time: 0.0289  data: 0.0002  max mem: 5511
[23:47:03.802986] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4985 (0.5260)  acc1: 85.9375 (83.7316)  acc5: 100.0000 (99.1115)  time: 0.0288  data: 0.0002  max mem: 5511
[23:47:04.088159] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4657 (0.5203)  acc1: 85.9375 (83.9395)  acc5: 100.0000 (99.1035)  time: 0.0283  data: 0.0002  max mem: 5511
[23:47:04.371390] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4657 (0.5151)  acc1: 84.3750 (84.0449)  acc5: 100.0000 (99.1197)  time: 0.0283  data: 0.0002  max mem: 5511
[23:47:04.661166] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5120 (0.5218)  acc1: 84.3750 (84.0085)  acc5: 100.0000 (99.1127)  time: 0.0285  data: 0.0002  max mem: 5511
[23:47:04.955474] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5287 (0.5192)  acc1: 84.3750 (84.3235)  acc5: 98.4375 (99.0728)  time: 0.0290  data: 0.0002  max mem: 5511
[23:47:05.249282] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5465 (0.5246)  acc1: 82.8125 (84.0192)  acc5: 98.4375 (99.0254)  time: 0.0292  data: 0.0002  max mem: 5511
[23:47:05.537615] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5709 (0.5261)  acc1: 81.2500 (83.9668)  acc5: 98.4375 (99.0287)  time: 0.0289  data: 0.0002  max mem: 5511
[23:47:05.829104] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5493 (0.5259)  acc1: 82.8125 (83.9747)  acc5: 98.4375 (99.0315)  time: 0.0288  data: 0.0002  max mem: 5511
[23:47:06.117641] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4935 (0.5259)  acc1: 84.3750 (84.0052)  acc5: 98.4375 (99.0339)  time: 0.0289  data: 0.0002  max mem: 5511
[23:47:06.404262] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4857 (0.5229)  acc1: 85.9375 (84.1423)  acc5: 100.0000 (99.0691)  time: 0.0286  data: 0.0002  max mem: 5511
[23:47:06.689574] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5108 (0.5228)  acc1: 84.3750 (84.1163)  acc5: 100.0000 (99.0791)  time: 0.0284  data: 0.0002  max mem: 5511
[23:47:06.844290] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5108 (0.5224)  acc1: 84.3750 (84.1000)  acc5: 100.0000 (99.0800)  time: 0.0275  data: 0.0002  max mem: 5511
[23:47:07.009888] Test: Total time: 0:00:05 (0.0339 s / it)
[23:47:07.010574] * Acc@1 84.100 Acc@5 99.080 loss 0.522
[23:47:07.010854] Accuracy of the network on the 10000 test images: 84.1%
[23:47:07.011035] Max accuracy: 84.43%
[23:47:07.242301] log_dir: ./output_dir
[23:47:08.140664] Epoch: [65]  [  0/781]  eta: 0:11:40  lr: 0.000075  training_loss: 1.1806 (1.1806)  mae_loss: 0.0278 (0.0278)  classification_loss: 1.1526 (1.1526)  loss_mask: 0.0002 (0.0002)  time: 0.8964  data: 0.6780  max mem: 5511
[23:47:12.088688] Epoch: [65]  [ 20/781]  eta: 0:02:55  lr: 0.000075  training_loss: 1.1867 (1.1883)  mae_loss: 0.0278 (0.0278)  classification_loss: 1.1577 (1.1603)  loss_mask: 0.0001 (0.0002)  time: 0.1973  data: 0.0002  max mem: 5511
[23:47:16.040751] Epoch: [65]  [ 40/781]  eta: 0:02:38  lr: 0.000075  training_loss: 1.2183 (1.2080)  mae_loss: 0.0270 (0.0274)  classification_loss: 1.1936 (1.1803)  loss_mask: 0.0001 (0.0003)  time: 0.1975  data: 0.0002  max mem: 5511
[23:47:19.982475] Epoch: [65]  [ 60/781]  eta: 0:02:30  lr: 0.000075  training_loss: 1.1959 (1.2152)  mae_loss: 0.0249 (0.0268)  classification_loss: 1.1716 (1.1881)  loss_mask: 0.0001 (0.0004)  time: 0.1970  data: 0.0002  max mem: 5511
[23:47:23.956590] Epoch: [65]  [ 80/781]  eta: 0:02:24  lr: 0.000075  training_loss: 1.2469 (1.2261)  mae_loss: 0.0255 (0.0265)  classification_loss: 1.2184 (1.1990)  loss_mask: 0.0006 (0.0006)  time: 0.1986  data: 0.0002  max mem: 5511
[23:47:27.885568] Epoch: [65]  [100/781]  eta: 0:02:19  lr: 0.000075  training_loss: 1.2100 (1.2271)  mae_loss: 0.0271 (0.0267)  classification_loss: 1.1837 (1.1999)  loss_mask: 0.0002 (0.0006)  time: 0.1964  data: 0.0003  max mem: 5511
[23:47:31.807806] Epoch: [65]  [120/781]  eta: 0:02:14  lr: 0.000075  training_loss: 1.2102 (1.2244)  mae_loss: 0.0236 (0.0263)  classification_loss: 1.1820 (1.1972)  loss_mask: 0.0003 (0.0009)  time: 0.1960  data: 0.0003  max mem: 5511
[23:47:35.787492] Epoch: [65]  [140/781]  eta: 0:02:09  lr: 0.000075  training_loss: 1.1493 (1.2158)  mae_loss: 0.0260 (0.0263)  classification_loss: 1.1224 (1.1883)  loss_mask: 0.0017 (0.0012)  time: 0.1989  data: 0.0002  max mem: 5511
[23:47:39.736244] Epoch: [65]  [160/781]  eta: 0:02:05  lr: 0.000075  training_loss: 1.1973 (1.2166)  mae_loss: 0.0264 (0.0263)  classification_loss: 1.1766 (1.1891)  loss_mask: 0.0006 (0.0012)  time: 0.1973  data: 0.0002  max mem: 5511
[23:47:43.660155] Epoch: [65]  [180/781]  eta: 0:02:00  lr: 0.000075  training_loss: 1.1940 (1.2157)  mae_loss: 0.0261 (0.0264)  classification_loss: 1.1688 (1.1883)  loss_mask: 0.0003 (0.0011)  time: 0.1961  data: 0.0002  max mem: 5511
[23:47:47.582939] Epoch: [65]  [200/781]  eta: 0:01:56  lr: 0.000075  training_loss: 1.1851 (1.2128)  mae_loss: 0.0256 (0.0264)  classification_loss: 1.1597 (1.1853)  loss_mask: 0.0002 (0.0010)  time: 0.1961  data: 0.0002  max mem: 5511
[23:47:51.552961] Epoch: [65]  [220/781]  eta: 0:01:52  lr: 0.000074  training_loss: 1.2441 (1.2157)  mae_loss: 0.0256 (0.0264)  classification_loss: 1.2172 (1.1884)  loss_mask: 0.0002 (0.0009)  time: 0.1984  data: 0.0002  max mem: 5511
[23:47:55.475403] Epoch: [65]  [240/781]  eta: 0:01:48  lr: 0.000074  training_loss: 1.1957 (1.2149)  mae_loss: 0.0268 (0.0264)  classification_loss: 1.1715 (1.1876)  loss_mask: 0.0001 (0.0009)  time: 0.1960  data: 0.0002  max mem: 5511
[23:47:59.405104] Epoch: [65]  [260/781]  eta: 0:01:44  lr: 0.000074  training_loss: 1.2724 (1.2184)  mae_loss: 0.0246 (0.0264)  classification_loss: 1.2429 (1.1913)  loss_mask: 0.0002 (0.0008)  time: 0.1964  data: 0.0002  max mem: 5511
[23:48:03.332195] Epoch: [65]  [280/781]  eta: 0:01:39  lr: 0.000074  training_loss: 1.2256 (1.2191)  mae_loss: 0.0240 (0.0263)  classification_loss: 1.1968 (1.1920)  loss_mask: 0.0001 (0.0008)  time: 0.1962  data: 0.0002  max mem: 5511
[23:48:07.297380] Epoch: [65]  [300/781]  eta: 0:01:35  lr: 0.000074  training_loss: 1.2193 (1.2203)  mae_loss: 0.0262 (0.0263)  classification_loss: 1.1938 (1.1932)  loss_mask: 0.0001 (0.0007)  time: 0.1981  data: 0.0002  max mem: 5511
[23:48:11.246482] Epoch: [65]  [320/781]  eta: 0:01:31  lr: 0.000074  training_loss: 1.2029 (1.2203)  mae_loss: 0.0252 (0.0263)  classification_loss: 1.1765 (1.1932)  loss_mask: 0.0002 (0.0007)  time: 0.1974  data: 0.0004  max mem: 5511
[23:48:15.175007] Epoch: [65]  [340/781]  eta: 0:01:27  lr: 0.000074  training_loss: 1.1969 (1.2198)  mae_loss: 0.0243 (0.0263)  classification_loss: 1.1755 (1.1927)  loss_mask: 0.0005 (0.0008)  time: 0.1963  data: 0.0002  max mem: 5511
[23:48:19.097106] Epoch: [65]  [360/781]  eta: 0:01:23  lr: 0.000074  training_loss: 1.2386 (1.2206)  mae_loss: 0.0246 (0.0262)  classification_loss: 1.2120 (1.1934)  loss_mask: 0.0024 (0.0010)  time: 0.1960  data: 0.0002  max mem: 5511
[23:48:23.038533] Epoch: [65]  [380/781]  eta: 0:01:19  lr: 0.000074  training_loss: 1.2213 (1.2205)  mae_loss: 0.0274 (0.0263)  classification_loss: 1.1877 (1.1931)  loss_mask: 0.0009 (0.0010)  time: 0.1970  data: 0.0002  max mem: 5511
[23:48:27.014457] Epoch: [65]  [400/781]  eta: 0:01:15  lr: 0.000074  training_loss: 1.1863 (1.2193)  mae_loss: 0.0271 (0.0264)  classification_loss: 1.1570 (1.1919)  loss_mask: 0.0005 (0.0010)  time: 0.1987  data: 0.0002  max mem: 5511
[23:48:30.922223] Epoch: [65]  [420/781]  eta: 0:01:11  lr: 0.000073  training_loss: 1.2127 (1.2191)  mae_loss: 0.0269 (0.0264)  classification_loss: 1.1869 (1.1917)  loss_mask: 0.0005 (0.0010)  time: 0.1953  data: 0.0002  max mem: 5511
[23:48:34.859931] Epoch: [65]  [440/781]  eta: 0:01:07  lr: 0.000073  training_loss: 1.2362 (1.2199)  mae_loss: 0.0258 (0.0264)  classification_loss: 1.2085 (1.1925)  loss_mask: 0.0011 (0.0010)  time: 0.1968  data: 0.0002  max mem: 5511
[23:48:38.775863] Epoch: [65]  [460/781]  eta: 0:01:03  lr: 0.000073  training_loss: 1.1851 (1.2178)  mae_loss: 0.0266 (0.0264)  classification_loss: 1.1607 (1.1905)  loss_mask: 0.0002 (0.0010)  time: 0.1957  data: 0.0002  max mem: 5511
[23:48:42.749250] Epoch: [65]  [480/781]  eta: 0:00:59  lr: 0.000073  training_loss: 1.1708 (1.2177)  mae_loss: 0.0248 (0.0263)  classification_loss: 1.1444 (1.1904)  loss_mask: 0.0002 (0.0010)  time: 0.1986  data: 0.0002  max mem: 5511
[23:48:46.684507] Epoch: [65]  [500/781]  eta: 0:00:55  lr: 0.000073  training_loss: 1.1921 (1.2171)  mae_loss: 0.0248 (0.0263)  classification_loss: 1.1640 (1.1898)  loss_mask: 0.0001 (0.0010)  time: 0.1967  data: 0.0003  max mem: 5511
[23:48:50.617460] Epoch: [65]  [520/781]  eta: 0:00:51  lr: 0.000073  training_loss: 1.2169 (1.2178)  mae_loss: 0.0254 (0.0263)  classification_loss: 1.1938 (1.1906)  loss_mask: 0.0002 (0.0009)  time: 0.1965  data: 0.0002  max mem: 5511
[23:48:54.545839] Epoch: [65]  [540/781]  eta: 0:00:47  lr: 0.000073  training_loss: 1.2357 (1.2183)  mae_loss: 0.0249 (0.0263)  classification_loss: 1.2028 (1.1911)  loss_mask: 0.0001 (0.0009)  time: 0.1963  data: 0.0002  max mem: 5511
[23:48:58.474463] Epoch: [65]  [560/781]  eta: 0:00:43  lr: 0.000073  training_loss: 1.1962 (1.2178)  mae_loss: 0.0259 (0.0263)  classification_loss: 1.1730 (1.1906)  loss_mask: 0.0001 (0.0009)  time: 0.1964  data: 0.0003  max mem: 5511
[23:49:02.398857] Epoch: [65]  [580/781]  eta: 0:00:39  lr: 0.000073  training_loss: 1.2097 (1.2181)  mae_loss: 0.0266 (0.0263)  classification_loss: 1.1891 (1.1910)  loss_mask: 0.0002 (0.0009)  time: 0.1961  data: 0.0002  max mem: 5511
[23:49:06.352789] Epoch: [65]  [600/781]  eta: 0:00:35  lr: 0.000073  training_loss: 1.2194 (1.2180)  mae_loss: 0.0252 (0.0263)  classification_loss: 1.1839 (1.1909)  loss_mask: 0.0001 (0.0009)  time: 0.1976  data: 0.0002  max mem: 5511
[23:49:10.297974] Epoch: [65]  [620/781]  eta: 0:00:31  lr: 0.000073  training_loss: 1.2100 (1.2186)  mae_loss: 0.0287 (0.0263)  classification_loss: 1.1847 (1.1915)  loss_mask: 0.0002 (0.0008)  time: 0.1972  data: 0.0003  max mem: 5511
[23:49:14.239837] Epoch: [65]  [640/781]  eta: 0:00:27  lr: 0.000072  training_loss: 1.2117 (1.2188)  mae_loss: 0.0281 (0.0264)  classification_loss: 1.1909 (1.1916)  loss_mask: 0.0002 (0.0008)  time: 0.1970  data: 0.0002  max mem: 5511
[23:49:18.176574] Epoch: [65]  [660/781]  eta: 0:00:23  lr: 0.000072  training_loss: 1.2273 (1.2196)  mae_loss: 0.0271 (0.0264)  classification_loss: 1.2000 (1.1924)  loss_mask: 0.0001 (0.0008)  time: 0.1968  data: 0.0002  max mem: 5511
[23:49:22.103539] Epoch: [65]  [680/781]  eta: 0:00:19  lr: 0.000072  training_loss: 1.2361 (1.2200)  mae_loss: 0.0257 (0.0264)  classification_loss: 1.2079 (1.1928)  loss_mask: 0.0001 (0.0008)  time: 0.1963  data: 0.0002  max mem: 5511
[23:49:26.050361] Epoch: [65]  [700/781]  eta: 0:00:16  lr: 0.000072  training_loss: 1.2202 (1.2210)  mae_loss: 0.0268 (0.0264)  classification_loss: 1.1959 (1.1938)  loss_mask: 0.0001 (0.0008)  time: 0.1973  data: 0.0002  max mem: 5511
[23:49:29.985991] Epoch: [65]  [720/781]  eta: 0:00:12  lr: 0.000072  training_loss: 1.2382 (1.2214)  mae_loss: 0.0262 (0.0264)  classification_loss: 1.2116 (1.1943)  loss_mask: 0.0002 (0.0008)  time: 0.1967  data: 0.0002  max mem: 5511
[23:49:33.938257] Epoch: [65]  [740/781]  eta: 0:00:08  lr: 0.000072  training_loss: 1.2248 (1.2214)  mae_loss: 0.0246 (0.0264)  classification_loss: 1.1990 (1.1943)  loss_mask: 0.0001 (0.0008)  time: 0.1975  data: 0.0002  max mem: 5511
[23:49:37.864036] Epoch: [65]  [760/781]  eta: 0:00:04  lr: 0.000072  training_loss: 1.2171 (1.2218)  mae_loss: 0.0262 (0.0264)  classification_loss: 1.1908 (1.1947)  loss_mask: 0.0002 (0.0008)  time: 0.1962  data: 0.0002  max mem: 5511
[23:49:41.780876] Epoch: [65]  [780/781]  eta: 0:00:00  lr: 0.000072  training_loss: 1.2060 (1.2213)  mae_loss: 0.0250 (0.0264)  classification_loss: 1.1810 (1.1942)  loss_mask: 0.0003 (0.0007)  time: 0.1958  data: 0.0002  max mem: 5511
[23:49:41.942868] Epoch: [65] Total time: 0:02:34 (0.1981 s / it)
[23:49:41.943360] Averaged stats: lr: 0.000072  training_loss: 1.2060 (1.2213)  mae_loss: 0.0250 (0.0264)  classification_loss: 1.1810 (1.1942)  loss_mask: 0.0003 (0.0007)
[23:49:42.587021] Test:  [  0/157]  eta: 0:01:40  testing_loss: 0.5023 (0.5023)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.6396  data: 0.6094  max mem: 5511
[23:49:42.876755] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5050 (0.5458)  acc1: 85.9375 (82.9545)  acc5: 100.0000 (99.5739)  time: 0.0842  data: 0.0556  max mem: 5511
[23:49:43.163675] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4885 (0.5067)  acc1: 85.9375 (84.5982)  acc5: 100.0000 (99.5536)  time: 0.0286  data: 0.0002  max mem: 5511
[23:49:43.454362] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4981 (0.5250)  acc1: 82.8125 (83.5181)  acc5: 100.0000 (99.3952)  time: 0.0287  data: 0.0002  max mem: 5511
[23:49:43.745349] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5214 (0.5249)  acc1: 84.3750 (83.9558)  acc5: 100.0000 (99.1997)  time: 0.0289  data: 0.0004  max mem: 5511
[23:49:44.029342] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4920 (0.5192)  acc1: 85.9375 (84.2218)  acc5: 100.0000 (99.2341)  time: 0.0286  data: 0.0004  max mem: 5511
[23:49:44.317476] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4751 (0.5127)  acc1: 85.9375 (84.4006)  acc5: 100.0000 (99.2572)  time: 0.0285  data: 0.0002  max mem: 5511
[23:49:44.610514] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4715 (0.5067)  acc1: 85.9375 (84.5951)  acc5: 100.0000 (99.2518)  time: 0.0289  data: 0.0002  max mem: 5511
[23:49:44.895232] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5006 (0.5141)  acc1: 84.3750 (84.4136)  acc5: 98.4375 (99.2091)  time: 0.0288  data: 0.0002  max mem: 5511
[23:49:45.182816] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4888 (0.5090)  acc1: 85.9375 (84.7012)  acc5: 98.4375 (99.1758)  time: 0.0285  data: 0.0002  max mem: 5511
[23:49:45.469985] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4888 (0.5123)  acc1: 85.9375 (84.6535)  acc5: 98.4375 (99.1801)  time: 0.0286  data: 0.0002  max mem: 5511
[23:49:45.757607] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5370 (0.5121)  acc1: 84.3750 (84.6565)  acc5: 100.0000 (99.1976)  time: 0.0286  data: 0.0002  max mem: 5511
[23:49:46.045053] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5370 (0.5140)  acc1: 84.3750 (84.6074)  acc5: 100.0000 (99.1865)  time: 0.0286  data: 0.0002  max mem: 5511
[23:49:46.334209] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5072 (0.5137)  acc1: 84.3750 (84.6374)  acc5: 98.4375 (99.1770)  time: 0.0286  data: 0.0002  max mem: 5511
[23:49:46.616605] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5072 (0.5145)  acc1: 85.9375 (84.5412)  acc5: 100.0000 (99.2132)  time: 0.0284  data: 0.0002  max mem: 5511
[23:49:46.898794] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5480 (0.5139)  acc1: 82.8125 (84.5095)  acc5: 100.0000 (99.2239)  time: 0.0281  data: 0.0001  max mem: 5511
[23:49:47.050641] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4858 (0.5141)  acc1: 84.3750 (84.5000)  acc5: 100.0000 (99.2100)  time: 0.0272  data: 0.0001  max mem: 5511
[23:49:47.202758] Test: Total time: 0:00:05 (0.0335 s / it)
[23:49:47.203438] * Acc@1 84.500 Acc@5 99.210 loss 0.514
[23:49:47.203945] Accuracy of the network on the 10000 test images: 84.5%
[23:49:47.204228] Max accuracy: 84.50%
[23:49:47.525174] log_dir: ./output_dir
[23:49:48.381028] Epoch: [66]  [  0/781]  eta: 0:11:07  lr: 0.000072  training_loss: 1.0696 (1.0696)  mae_loss: 0.0291 (0.0291)  classification_loss: 1.0403 (1.0403)  loss_mask: 0.0003 (0.0003)  time: 0.8541  data: 0.6355  max mem: 5511
[23:49:52.329765] Epoch: [66]  [ 20/781]  eta: 0:02:53  lr: 0.000072  training_loss: 1.1701 (1.1603)  mae_loss: 0.0266 (0.0268)  classification_loss: 1.1421 (1.1333)  loss_mask: 0.0002 (0.0002)  time: 0.1973  data: 0.0002  max mem: 5511
[23:49:56.260703] Epoch: [66]  [ 40/781]  eta: 0:02:37  lr: 0.000072  training_loss: 1.2241 (1.1925)  mae_loss: 0.0241 (0.0259)  classification_loss: 1.1951 (1.1663)  loss_mask: 0.0001 (0.0003)  time: 0.1965  data: 0.0004  max mem: 5511
[23:50:00.193858] Epoch: [66]  [ 60/781]  eta: 0:02:29  lr: 0.000071  training_loss: 1.2453 (1.2092)  mae_loss: 0.0268 (0.0262)  classification_loss: 1.2128 (1.1827)  loss_mask: 0.0002 (0.0003)  time: 0.1966  data: 0.0002  max mem: 5511
[23:50:04.231888] Epoch: [66]  [ 80/781]  eta: 0:02:24  lr: 0.000071  training_loss: 1.2255 (1.2149)  mae_loss: 0.0248 (0.0260)  classification_loss: 1.2021 (1.1884)  loss_mask: 0.0005 (0.0004)  time: 0.2018  data: 0.0002  max mem: 5511
[23:50:08.178050] Epoch: [66]  [100/781]  eta: 0:02:19  lr: 0.000071  training_loss: 1.1795 (1.2068)  mae_loss: 0.0255 (0.0259)  classification_loss: 1.1554 (1.1803)  loss_mask: 0.0004 (0.0005)  time: 0.1972  data: 0.0002  max mem: 5511
[23:50:12.191730] Epoch: [66]  [120/781]  eta: 0:02:14  lr: 0.000071  training_loss: 1.1772 (1.2042)  mae_loss: 0.0270 (0.0260)  classification_loss: 1.1459 (1.1777)  loss_mask: 0.0007 (0.0006)  time: 0.2006  data: 0.0002  max mem: 5511
[23:50:16.122941] Epoch: [66]  [140/781]  eta: 0:02:09  lr: 0.000071  training_loss: 1.1710 (1.2050)  mae_loss: 0.0241 (0.0258)  classification_loss: 1.1433 (1.1787)  loss_mask: 0.0002 (0.0005)  time: 0.1965  data: 0.0002  max mem: 5511
[23:50:20.139123] Epoch: [66]  [160/781]  eta: 0:02:05  lr: 0.000071  training_loss: 1.2137 (1.2067)  mae_loss: 0.0274 (0.0260)  classification_loss: 1.1933 (1.1802)  loss_mask: 0.0001 (0.0005)  time: 0.2007  data: 0.0004  max mem: 5511
[23:50:24.059405] Epoch: [66]  [180/781]  eta: 0:02:01  lr: 0.000071  training_loss: 1.1896 (1.2070)  mae_loss: 0.0268 (0.0261)  classification_loss: 1.1574 (1.1804)  loss_mask: 0.0003 (0.0005)  time: 0.1959  data: 0.0003  max mem: 5511
[23:50:27.984717] Epoch: [66]  [200/781]  eta: 0:01:56  lr: 0.000071  training_loss: 1.2077 (1.2062)  mae_loss: 0.0256 (0.0261)  classification_loss: 1.1806 (1.1796)  loss_mask: 0.0001 (0.0005)  time: 0.1962  data: 0.0001  max mem: 5511
[23:50:31.908026] Epoch: [66]  [220/781]  eta: 0:01:52  lr: 0.000071  training_loss: 1.1857 (1.2058)  mae_loss: 0.0261 (0.0261)  classification_loss: 1.1603 (1.1792)  loss_mask: 0.0002 (0.0005)  time: 0.1961  data: 0.0002  max mem: 5511
[23:50:35.874600] Epoch: [66]  [240/781]  eta: 0:01:48  lr: 0.000071  training_loss: 1.1832 (1.2066)  mae_loss: 0.0255 (0.0260)  classification_loss: 1.1623 (1.1801)  loss_mask: 0.0004 (0.0005)  time: 0.1982  data: 0.0002  max mem: 5511

[23:50:39.810334] Epoch: [66]  [260/781]  eta: 0:01:44  lr: 0.000071  training_loss: 1.1973 (1.2072)  mae_loss: 0.0250 (0.0260)  classification_loss: 1.1739 (1.1807)  loss_mask: 0.0002 (0.0005)  time: 0.1967  data: 0.0002  max mem: 5511
[23:50:43.741872] Epoch: [66]  [280/781]  eta: 0:01:40  lr: 0.000070  training_loss: 1.2057 (1.2086)  mae_loss: 0.0277 (0.0261)  classification_loss: 1.1822 (1.1820)  loss_mask: 0.0003 (0.0005)  time: 0.1965  data: 0.0002  max mem: 5511
[23:50:47.701413] Epoch: [66]  [300/781]  eta: 0:01:36  lr: 0.000070  training_loss: 1.2612 (1.2125)  mae_loss: 0.0257 (0.0261)  classification_loss: 1.2310 (1.1860)  loss_mask: 0.0002 (0.0005)  time: 0.1979  data: 0.0002  max mem: 5511
[23:50:51.651585] Epoch: [66]  [320/781]  eta: 0:01:32  lr: 0.000070  training_loss: 1.1875 (1.2114)  mae_loss: 0.0263 (0.0261)  classification_loss: 1.1602 (1.1848)  loss_mask: 0.0006 (0.0005)  time: 0.1974  data: 0.0002  max mem: 5511
[23:50:55.612663] Epoch: [66]  [340/781]  eta: 0:01:28  lr: 0.000070  training_loss: 1.1957 (1.2126)  mae_loss: 0.0252 (0.0261)  classification_loss: 1.1699 (1.1859)  loss_mask: 0.0008 (0.0006)  time: 0.1979  data: 0.0002  max mem: 5511
[23:50:59.538405] Epoch: [66]  [360/781]  eta: 0:01:23  lr: 0.000070  training_loss: 1.2239 (1.2131)  mae_loss: 0.0268 (0.0261)  classification_loss: 1.1897 (1.1864)  loss_mask: 0.0004 (0.0006)  time: 0.1962  data: 0.0002  max mem: 5511
[23:51:03.457971] Epoch: [66]  [380/781]  eta: 0:01:19  lr: 0.000070  training_loss: 1.2042 (1.2119)  mae_loss: 0.0262 (0.0261)  classification_loss: 1.1766 (1.1852)  loss_mask: 0.0001 (0.0006)  time: 0.1959  data: 0.0002  max mem: 5511
[23:51:07.373114] Epoch: [66]  [400/781]  eta: 0:01:15  lr: 0.000070  training_loss: 1.2161 (1.2125)  mae_loss: 0.0253 (0.0261)  classification_loss: 1.1926 (1.1858)  loss_mask: 0.0001 (0.0005)  time: 0.1957  data: 0.0002  max mem: 5511
[23:51:11.300076] Epoch: [66]  [420/781]  eta: 0:01:11  lr: 0.000070  training_loss: 1.1952 (1.2116)  mae_loss: 0.0276 (0.0262)  classification_loss: 1.1686 (1.1849)  loss_mask: 0.0001 (0.0005)  time: 0.1963  data: 0.0002  max mem: 5511
[23:51:15.221902] Epoch: [66]  [440/781]  eta: 0:01:07  lr: 0.000070  training_loss: 1.1871 (1.2110)  mae_loss: 0.0261 (0.0262)  classification_loss: 1.1626 (1.1843)  loss_mask: 0.0001 (0.0005)  time: 0.1960  data: 0.0003  max mem: 5511
[23:51:19.186553] Epoch: [66]  [460/781]  eta: 0:01:03  lr: 0.000070  training_loss: 1.1949 (1.2114)  mae_loss: 0.0254 (0.0262)  classification_loss: 1.1631 (1.1847)  loss_mask: 0.0001 (0.0005)  time: 0.1982  data: 0.0002  max mem: 5511
[23:51:23.164753] Epoch: [66]  [480/781]  eta: 0:00:59  lr: 0.000069  training_loss: 1.1729 (1.2114)  mae_loss: 0.0252 (0.0262)  classification_loss: 1.1518 (1.1848)  loss_mask: 0.0001 (0.0005)  time: 0.1988  data: 0.0002  max mem: 5511
[23:51:27.097586] Epoch: [66]  [500/781]  eta: 0:00:55  lr: 0.000069  training_loss: 1.2327 (1.2125)  mae_loss: 0.0245 (0.0261)  classification_loss: 1.2035 (1.1859)  loss_mask: 0.0001 (0.0005)  time: 0.1966  data: 0.0002  max mem: 5511
[23:51:31.021258] Epoch: [66]  [520/781]  eta: 0:00:51  lr: 0.000069  training_loss: 1.2145 (1.2135)  mae_loss: 0.0263 (0.0261)  classification_loss: 1.1917 (1.1869)  loss_mask: 0.0001 (0.0005)  time: 0.1961  data: 0.0001  max mem: 5511
[23:51:34.944018] Epoch: [66]  [540/781]  eta: 0:00:47  lr: 0.000069  training_loss: 1.1986 (1.2139)  mae_loss: 0.0263 (0.0261)  classification_loss: 1.1716 (1.1873)  loss_mask: 0.0001 (0.0004)  time: 0.1960  data: 0.0002  max mem: 5511
[23:51:38.888155] Epoch: [66]  [560/781]  eta: 0:00:43  lr: 0.000069  training_loss: 1.2212 (1.2141)  mae_loss: 0.0264 (0.0262)  classification_loss: 1.1992 (1.1875)  loss_mask: 0.0001 (0.0004)  time: 0.1971  data: 0.0002  max mem: 5511
[23:51:42.808446] Epoch: [66]  [580/781]  eta: 0:00:39  lr: 0.000069  training_loss: 1.2111 (1.2137)  mae_loss: 0.0271 (0.0262)  classification_loss: 1.1827 (1.1870)  loss_mask: 0.0001 (0.0004)  time: 0.1959  data: 0.0002  max mem: 5511
[23:51:46.802433] Epoch: [66]  [600/781]  eta: 0:00:35  lr: 0.000069  training_loss: 1.2316 (1.2147)  mae_loss: 0.0268 (0.0262)  classification_loss: 1.2052 (1.1881)  loss_mask: 0.0001 (0.0004)  time: 0.1996  data: 0.0003  max mem: 5511
[23:51:50.754142] Epoch: [66]  [620/781]  eta: 0:00:31  lr: 0.000069  training_loss: 1.1774 (1.2145)  mae_loss: 0.0263 (0.0262)  classification_loss: 1.1500 (1.1879)  loss_mask: 0.0002 (0.0004)  time: 0.1975  data: 0.0002  max mem: 5511
[23:51:54.678467] Epoch: [66]  [640/781]  eta: 0:00:27  lr: 0.000069  training_loss: 1.1712 (1.2139)  mae_loss: 0.0260 (0.0262)  classification_loss: 1.1524 (1.1872)  loss_mask: 0.0002 (0.0004)  time: 0.1961  data: 0.0002  max mem: 5511
[23:51:58.584255] Epoch: [66]  [660/781]  eta: 0:00:23  lr: 0.000069  training_loss: 1.2379 (1.2149)  mae_loss: 0.0251 (0.0262)  classification_loss: 1.2150 (1.1883)  loss_mask: 0.0001 (0.0004)  time: 0.1952  data: 0.0002  max mem: 5511
[23:52:02.510339] Epoch: [66]  [680/781]  eta: 0:00:20  lr: 0.000069  training_loss: 1.1886 (1.2143)  mae_loss: 0.0271 (0.0262)  classification_loss: 1.1614 (1.1876)  loss_mask: 0.0001 (0.0004)  time: 0.1962  data: 0.0002  max mem: 5511
[23:52:06.429902] Epoch: [66]  [700/781]  eta: 0:00:16  lr: 0.000068  training_loss: 1.2417 (1.2144)  mae_loss: 0.0268 (0.0262)  classification_loss: 1.2114 (1.1878)  loss_mask: 0.0002 (0.0004)  time: 0.1959  data: 0.0002  max mem: 5511
[23:52:10.401062] Epoch: [66]  [720/781]  eta: 0:00:12  lr: 0.000068  training_loss: 1.2439 (1.2148)  mae_loss: 0.0265 (0.0263)  classification_loss: 1.2188 (1.1881)  loss_mask: 0.0002 (0.0004)  time: 0.1985  data: 0.0002  max mem: 5511
[23:52:14.331425] Epoch: [66]  [740/781]  eta: 0:00:08  lr: 0.000068  training_loss: 1.1675 (1.2142)  mae_loss: 0.0242 (0.0262)  classification_loss: 1.1444 (1.1876)  loss_mask: 0.0003 (0.0004)  time: 0.1964  data: 0.0002  max mem: 5511
[23:52:18.262916] Epoch: [66]  [760/781]  eta: 0:00:04  lr: 0.000068  training_loss: 1.2461 (1.2157)  mae_loss: 0.0249 (0.0262)  classification_loss: 1.2183 (1.1890)  loss_mask: 0.0002 (0.0004)  time: 0.1965  data: 0.0004  max mem: 5511
[23:52:22.194264] Epoch: [66]  [780/781]  eta: 0:00:00  lr: 0.000068  training_loss: 1.2467 (1.2167)  mae_loss: 0.0267 (0.0262)  classification_loss: 1.2197 (1.1900)  loss_mask: 0.0002 (0.0004)  time: 0.1965  data: 0.0002  max mem: 5511
[23:52:22.365179] Epoch: [66] Total time: 0:02:34 (0.1983 s / it)
[23:52:22.365973] Averaged stats: lr: 0.000068  training_loss: 1.2467 (1.2167)  mae_loss: 0.0267 (0.0262)  classification_loss: 1.2197 (1.1900)  loss_mask: 0.0002 (0.0004)
[23:52:22.998069] Test:  [  0/157]  eta: 0:01:38  testing_loss: 0.5463 (0.5463)  acc1: 84.3750 (84.3750)  acc5: 98.4375 (98.4375)  time: 0.6281  data: 0.5979  max mem: 5511
[23:52:23.291072] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5229 (0.5348)  acc1: 84.3750 (84.2330)  acc5: 100.0000 (99.4318)  time: 0.0836  data: 0.0546  max mem: 5511
[23:52:23.575394] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4938 (0.4901)  acc1: 85.9375 (85.4911)  acc5: 100.0000 (99.5536)  time: 0.0287  data: 0.0002  max mem: 5511
[23:52:23.859575] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5126 (0.5135)  acc1: 84.3750 (84.5262)  acc5: 100.0000 (99.4960)  time: 0.0282  data: 0.0002  max mem: 5511
[23:52:24.151265] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5180 (0.5106)  acc1: 84.3750 (84.6418)  acc5: 98.4375 (99.2378)  time: 0.0286  data: 0.0002  max mem: 5511
[23:52:24.437327] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4984 (0.5099)  acc1: 84.3750 (84.6201)  acc5: 98.4375 (99.2341)  time: 0.0288  data: 0.0002  max mem: 5511
[23:52:24.724706] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4917 (0.5016)  acc1: 85.9375 (84.8617)  acc5: 100.0000 (99.2828)  time: 0.0285  data: 0.0002  max mem: 5511
[23:52:25.009586] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4517 (0.4930)  acc1: 85.9375 (85.0792)  acc5: 100.0000 (99.2738)  time: 0.0285  data: 0.0002  max mem: 5511
[23:52:25.302898] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4983 (0.5015)  acc1: 84.3750 (84.8187)  acc5: 100.0000 (99.2477)  time: 0.0288  data: 0.0003  max mem: 5511
[23:52:25.594217] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5198 (0.4992)  acc1: 84.3750 (84.8901)  acc5: 98.4375 (99.1587)  time: 0.0291  data: 0.0003  max mem: 5511
[23:52:25.880331] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5198 (0.5049)  acc1: 84.3750 (84.6225)  acc5: 98.4375 (99.1182)  time: 0.0287  data: 0.0002  max mem: 5511
[23:52:26.165695] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5262 (0.5040)  acc1: 81.2500 (84.5158)  acc5: 98.4375 (99.1413)  time: 0.0284  data: 0.0002  max mem: 5511
[23:52:26.449962] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4898 (0.5039)  acc1: 84.3750 (84.4783)  acc5: 100.0000 (99.1477)  time: 0.0284  data: 0.0002  max mem: 5511
[23:52:26.735193] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5009 (0.5038)  acc1: 82.8125 (84.3750)  acc5: 100.0000 (99.1531)  time: 0.0283  data: 0.0002  max mem: 5511
[23:52:27.019665] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5009 (0.5030)  acc1: 82.8125 (84.3750)  acc5: 100.0000 (99.1800)  time: 0.0283  data: 0.0002  max mem: 5511
[23:52:27.302612] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4974 (0.5027)  acc1: 82.8125 (84.3233)  acc5: 100.0000 (99.1929)  time: 0.0282  data: 0.0002  max mem: 5511
[23:52:27.454655] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4618 (0.5014)  acc1: 85.9375 (84.3800)  acc5: 100.0000 (99.1700)  time: 0.0273  data: 0.0001  max mem: 5511
[23:52:27.608209] Test: Total time: 0:00:05 (0.0334 s / it)
[23:52:27.608757] * Acc@1 84.380 Acc@5 99.170 loss 0.501
[23:52:27.609046] Accuracy of the network on the 10000 test images: 84.4%
[23:52:27.609226] Max accuracy: 84.50%
[23:52:27.807406] log_dir: ./output_dir
[23:52:28.691699] Epoch: [67]  [  0/781]  eta: 0:11:29  lr: 0.000068  training_loss: 1.0899 (1.0899)  mae_loss: 0.0252 (0.0252)  classification_loss: 1.0646 (1.0646)  loss_mask: 0.0001 (0.0001)  time: 0.8825  data: 0.6687  max mem: 5511
[23:52:32.693851] Epoch: [67]  [ 20/781]  eta: 0:02:56  lr: 0.000068  training_loss: 1.1736 (1.1994)  mae_loss: 0.0266 (0.0267)  classification_loss: 1.1452 (1.1720)  loss_mask: 0.0002 (0.0007)  time: 0.2000  data: 0.0003  max mem: 5511
[23:52:36.650595] Epoch: [67]  [ 40/781]  eta: 0:02:39  lr: 0.000068  training_loss: 1.2032 (1.2067)  mae_loss: 0.0258 (0.0266)  classification_loss: 1.1726 (1.1792)  loss_mask: 0.0002 (0.0008)  time: 0.1978  data: 0.0002  max mem: 5511
[23:52:40.578659] Epoch: [67]  [ 60/781]  eta: 0:02:30  lr: 0.000068  training_loss: 1.2555 (1.2194)  mae_loss: 0.0243 (0.0262)  classification_loss: 1.2316 (1.1926)  loss_mask: 0.0002 (0.0007)  time: 0.1963  data: 0.0002  max mem: 5511
[23:52:44.512337] Epoch: [67]  [ 80/781]  eta: 0:02:24  lr: 0.000068  training_loss: 1.1513 (1.2105)  mae_loss: 0.0251 (0.0260)  classification_loss: 1.1242 (1.1840)  loss_mask: 0.0001 (0.0005)  time: 0.1966  data: 0.0002  max mem: 5511
[23:52:48.425817] Epoch: [67]  [100/781]  eta: 0:02:18  lr: 0.000068  training_loss: 1.1673 (1.2061)  mae_loss: 0.0256 (0.0259)  classification_loss: 1.1430 (1.1797)  loss_mask: 0.0001 (0.0005)  time: 0.1956  data: 0.0003  max mem: 5511
[23:52:52.361448] Epoch: [67]  [120/781]  eta: 0:02:14  lr: 0.000068  training_loss: 1.1753 (1.2022)  mae_loss: 0.0235 (0.0257)  classification_loss: 1.1476 (1.1760)  loss_mask: 0.0001 (0.0004)  time: 0.1967  data: 0.0002  max mem: 5511
[23:52:56.297637] Epoch: [67]  [140/781]  eta: 0:02:09  lr: 0.000067  training_loss: 1.2207 (1.2052)  mae_loss: 0.0259 (0.0258)  classification_loss: 1.1821 (1.1781)  loss_mask: 0.0016 (0.0014)  time: 0.1967  data: 0.0002  max mem: 5511
[23:53:00.215351] Epoch: [67]  [160/781]  eta: 0:02:04  lr: 0.000067  training_loss: 1.1888 (1.2043)  mae_loss: 0.0248 (0.0257)  classification_loss: 1.1529 (1.1759)  loss_mask: 0.0108 (0.0027)  time: 0.1958  data: 0.0001  max mem: 5511
[23:53:04.169580] Epoch: [67]  [180/781]  eta: 0:02:00  lr: 0.000067  training_loss: 1.2188 (1.2065)  mae_loss: 0.0243 (0.0257)  classification_loss: 1.1745 (1.1773)  loss_mask: 0.0035 (0.0035)  time: 0.1976  data: 0.0002  max mem: 5511
[23:53:08.106506] Epoch: [67]  [200/781]  eta: 0:01:56  lr: 0.000067  training_loss: 1.2057 (1.2059)  mae_loss: 0.0266 (0.0258)  classification_loss: 1.1745 (1.1765)  loss_mask: 0.0040 (0.0036)  time: 0.1968  data: 0.0002  max mem: 5511
[23:53:12.063349] Epoch: [67]  [220/781]  eta: 0:01:52  lr: 0.000067  training_loss: 1.2114 (1.2079)  mae_loss: 0.0259 (0.0258)  classification_loss: 1.1853 (1.1786)  loss_mask: 0.0015 (0.0035)  time: 0.1977  data: 0.0002  max mem: 5511
[23:53:15.997037] Epoch: [67]  [240/781]  eta: 0:01:48  lr: 0.000067  training_loss: 1.1961 (1.2097)  mae_loss: 0.0260 (0.0259)  classification_loss: 1.1668 (1.1806)  loss_mask: 0.0005 (0.0033)  time: 0.1966  data: 0.0002  max mem: 5511
[23:53:19.941775] Epoch: [67]  [260/781]  eta: 0:01:44  lr: 0.000067  training_loss: 1.1389 (1.2094)  mae_loss: 0.0253 (0.0258)  classification_loss: 1.1126 (1.1806)  loss_mask: 0.0004 (0.0030)  time: 0.1971  data: 0.0002  max mem: 5511
[23:53:23.954672] Epoch: [67]  [280/781]  eta: 0:01:40  lr: 0.000067  training_loss: 1.1998 (1.2104)  mae_loss: 0.0269 (0.0259)  classification_loss: 1.1713 (1.1816)  loss_mask: 0.0003 (0.0028)  time: 0.2005  data: 0.0003  max mem: 5511
[23:53:27.894263] Epoch: [67]  [300/781]  eta: 0:01:35  lr: 0.000067  training_loss: 1.2167 (1.2114)  mae_loss: 0.0250 (0.0259)  classification_loss: 1.1861 (1.1828)  loss_mask: 0.0003 (0.0027)  time: 0.1969  data: 0.0002  max mem: 5511
[23:53:31.804961] Epoch: [67]  [320/781]  eta: 0:01:31  lr: 0.000067  training_loss: 1.1968 (1.2114)  mae_loss: 0.0263 (0.0260)  classification_loss: 1.1722 (1.1828)  loss_mask: 0.0002 (0.0026)  time: 0.1954  data: 0.0002  max mem: 5511
[23:53:35.741350] Epoch: [67]  [340/781]  eta: 0:01:27  lr: 0.000066  training_loss: 1.1948 (1.2118)  mae_loss: 0.0267 (0.0260)  classification_loss: 1.1697 (1.1833)  loss_mask: 0.0004 (0.0025)  time: 0.1967  data: 0.0002  max mem: 5511
[23:53:39.670346] Epoch: [67]  [360/781]  eta: 0:01:23  lr: 0.000066  training_loss: 1.1563 (1.2092)  mae_loss: 0.0250 (0.0260)  classification_loss: 1.1282 (1.1808)  loss_mask: 0.0007 (0.0024)  time: 0.1964  data: 0.0002  max mem: 5511
[23:53:43.613070] Epoch: [67]  [380/781]  eta: 0:01:19  lr: 0.000066  training_loss: 1.1447 (1.2081)  mae_loss: 0.0258 (0.0260)  classification_loss: 1.1176 (1.1797)  loss_mask: 0.0002 (0.0023)  time: 0.1970  data: 0.0002  max mem: 5511
[23:53:47.547493] Epoch: [67]  [400/781]  eta: 0:01:15  lr: 0.000066  training_loss: 1.2105 (1.2085)  mae_loss: 0.0257 (0.0260)  classification_loss: 1.1765 (1.1803)  loss_mask: 0.0002 (0.0022)  time: 0.1966  data: 0.0002  max mem: 5511
[23:53:51.470850] Epoch: [67]  [420/781]  eta: 0:01:11  lr: 0.000066  training_loss: 1.1623 (1.2079)  mae_loss: 0.0260 (0.0261)  classification_loss: 1.1361 (1.1797)  loss_mask: 0.0002 (0.0021)  time: 0.1961  data: 0.0003  max mem: 5511
[23:53:55.412772] Epoch: [67]  [440/781]  eta: 0:01:07  lr: 0.000066  training_loss: 1.1926 (1.2076)  mae_loss: 0.0259 (0.0261)  classification_loss: 1.1637 (1.1795)  loss_mask: 0.0002 (0.0020)  time: 0.1970  data: 0.0002  max mem: 5511
[23:53:59.332486] Epoch: [67]  [460/781]  eta: 0:01:03  lr: 0.000066  training_loss: 1.1918 (1.2069)  mae_loss: 0.0237 (0.0260)  classification_loss: 1.1671 (1.1789)  loss_mask: 0.0002 (0.0020)  time: 0.1959  data: 0.0003  max mem: 5511
[23:54:03.266455] Epoch: [67]  [480/781]  eta: 0:00:59  lr: 0.000066  training_loss: 1.1872 (1.2061)  mae_loss: 0.0255 (0.0260)  classification_loss: 1.1655 (1.1782)  loss_mask: 0.0001 (0.0019)  time: 0.1966  data: 0.0002  max mem: 5511
[23:54:07.185501] Epoch: [67]  [500/781]  eta: 0:00:55  lr: 0.000066  training_loss: 1.1816 (1.2062)  mae_loss: 0.0261 (0.0260)  classification_loss: 1.1542 (1.1784)  loss_mask: 0.0002 (0.0018)  time: 0.1959  data: 0.0002  max mem: 5511
[23:54:11.114441] Epoch: [67]  [520/781]  eta: 0:00:51  lr: 0.000066  training_loss: 1.1455 (1.2054)  mae_loss: 0.0266 (0.0260)  classification_loss: 1.1240 (1.1776)  loss_mask: 0.0001 (0.0017)  time: 0.1964  data: 0.0003  max mem: 5511
[23:54:15.055560] Epoch: [67]  [540/781]  eta: 0:00:47  lr: 0.000066  training_loss: 1.1971 (1.2059)  mae_loss: 0.0245 (0.0260)  classification_loss: 1.1765 (1.1782)  loss_mask: 0.0001 (0.0017)  time: 0.1970  data: 0.0003  max mem: 5511
[23:54:18.986811] Epoch: [67]  [560/781]  eta: 0:00:43  lr: 0.000065  training_loss: 1.1919 (1.2063)  mae_loss: 0.0259 (0.0260)  classification_loss: 1.1619 (1.1787)  loss_mask: 0.0001 (0.0016)  time: 0.1965  data: 0.0002  max mem: 5511
[23:54:22.901898] Epoch: [67]  [580/781]  eta: 0:00:39  lr: 0.000065  training_loss: 1.1666 (1.2052)  mae_loss: 0.0273 (0.0261)  classification_loss: 1.1408 (1.1776)  loss_mask: 0.0001 (0.0016)  time: 0.1957  data: 0.0002  max mem: 5511
[23:54:26.824065] Epoch: [67]  [600/781]  eta: 0:00:35  lr: 0.000065  training_loss: 1.1367 (1.2043)  mae_loss: 0.0256 (0.0261)  classification_loss: 1.1109 (1.1767)  loss_mask: 0.0002 (0.0016)  time: 0.1960  data: 0.0002  max mem: 5511
[23:54:30.759314] Epoch: [67]  [620/781]  eta: 0:00:31  lr: 0.000065  training_loss: 1.1953 (1.2046)  mae_loss: 0.0268 (0.0261)  classification_loss: 1.1702 (1.1770)  loss_mask: 0.0001 (0.0015)  time: 0.1967  data: 0.0002  max mem: 5511
[23:54:34.708327] Epoch: [67]  [640/781]  eta: 0:00:27  lr: 0.000065  training_loss: 1.1906 (1.2043)  mae_loss: 0.0269 (0.0261)  classification_loss: 1.1596 (1.1767)  loss_mask: 0.0002 (0.0015)  time: 0.1974  data: 0.0002  max mem: 5511
[23:54:38.653484] Epoch: [67]  [660/781]  eta: 0:00:23  lr: 0.000065  training_loss: 1.2181 (1.2044)  mae_loss: 0.0261 (0.0261)  classification_loss: 1.1863 (1.1768)  loss_mask: 0.0002 (0.0015)  time: 0.1971  data: 0.0004  max mem: 5511
[23:54:42.629320] Epoch: [67]  [680/781]  eta: 0:00:19  lr: 0.000065  training_loss: 1.2276 (1.2046)  mae_loss: 0.0242 (0.0261)  classification_loss: 1.1979 (1.1770)  loss_mask: 0.0002 (0.0014)  time: 0.1987  data: 0.0002  max mem: 5511
[23:54:46.612976] Epoch: [67]  [700/781]  eta: 0:00:16  lr: 0.000065  training_loss: 1.2470 (1.2055)  mae_loss: 0.0270 (0.0261)  classification_loss: 1.2185 (1.1779)  loss_mask: 0.0001 (0.0014)  time: 0.1991  data: 0.0002  max mem: 5511
[23:54:50.615623] Epoch: [67]  [720/781]  eta: 0:00:12  lr: 0.000065  training_loss: 1.2243 (1.2055)  mae_loss: 0.0258 (0.0261)  classification_loss: 1.1983 (1.1780)  loss_mask: 0.0002 (0.0014)  time: 0.2000  data: 0.0002  max mem: 5511
[23:54:54.554303] Epoch: [67]  [740/781]  eta: 0:00:08  lr: 0.000065  training_loss: 1.1483 (1.2049)  mae_loss: 0.0232 (0.0261)  classification_loss: 1.1250 (1.1774)  loss_mask: 0.0001 (0.0013)  time: 0.1968  data: 0.0002  max mem: 5511
[23:54:58.521015] Epoch: [67]  [760/781]  eta: 0:00:04  lr: 0.000065  training_loss: 1.2660 (1.2063)  mae_loss: 0.0249 (0.0261)  classification_loss: 1.2423 (1.1789)  loss_mask: 0.0001 (0.0013)  time: 0.1983  data: 0.0002  max mem: 5511
[23:55:02.553869] Epoch: [67]  [780/781]  eta: 0:00:00  lr: 0.000064  training_loss: 1.2299 (1.2070)  mae_loss: 0.0261 (0.0261)  classification_loss: 1.2049 (1.1796)  loss_mask: 0.0001 (0.0013)  time: 0.2015  data: 0.0002  max mem: 5511
[23:55:02.709608] Epoch: [67] Total time: 0:02:34 (0.1983 s / it)
[23:55:02.710067] Averaged stats: lr: 0.000064  training_loss: 1.2299 (1.2070)  mae_loss: 0.0261 (0.0261)  classification_loss: 1.2049 (1.1796)  loss_mask: 0.0001 (0.0013)
[23:55:03.381403] Test:  [  0/157]  eta: 0:01:44  testing_loss: 0.4943 (0.4943)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.6668  data: 0.6361  max mem: 5511
[23:55:03.673451] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4943 (0.5196)  acc1: 84.3750 (83.9489)  acc5: 100.0000 (99.2898)  time: 0.0869  data: 0.0580  max mem: 5511
[23:55:03.961519] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4685 (0.4735)  acc1: 85.9375 (85.5655)  acc5: 100.0000 (99.3304)  time: 0.0288  data: 0.0001  max mem: 5511
[23:55:04.249439] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4942 (0.5027)  acc1: 84.3750 (84.4758)  acc5: 98.4375 (99.1431)  time: 0.0287  data: 0.0002  max mem: 5511
[23:55:04.538095] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4882 (0.5007)  acc1: 84.3750 (84.6418)  acc5: 98.4375 (99.0091)  time: 0.0287  data: 0.0002  max mem: 5511
[23:55:04.825993] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4790 (0.4964)  acc1: 85.9375 (85.1103)  acc5: 100.0000 (99.0502)  time: 0.0287  data: 0.0002  max mem: 5511
[23:55:05.111333] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4631 (0.4914)  acc1: 87.5000 (85.4508)  acc5: 100.0000 (99.1035)  time: 0.0285  data: 0.0002  max mem: 5511
[23:55:05.394399] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4572 (0.4859)  acc1: 87.5000 (85.6954)  acc5: 100.0000 (99.2077)  time: 0.0283  data: 0.0002  max mem: 5511
[23:55:05.677679] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4735 (0.4935)  acc1: 84.3750 (85.1852)  acc5: 100.0000 (99.1705)  time: 0.0282  data: 0.0002  max mem: 5511
[23:55:05.961321] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5025 (0.4902)  acc1: 84.3750 (85.4224)  acc5: 98.4375 (99.1243)  time: 0.0282  data: 0.0002  max mem: 5511
[23:55:06.246598] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5030 (0.4935)  acc1: 85.9375 (85.3496)  acc5: 98.4375 (99.1491)  time: 0.0283  data: 0.0002  max mem: 5511
[23:55:06.534155] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5363 (0.4947)  acc1: 85.9375 (85.4026)  acc5: 100.0000 (99.1976)  time: 0.0285  data: 0.0004  max mem: 5511
[23:55:06.822123] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5066 (0.4943)  acc1: 84.3750 (85.3822)  acc5: 100.0000 (99.1865)  time: 0.0286  data: 0.0003  max mem: 5511
[23:55:07.111104] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4797 (0.4936)  acc1: 84.3750 (85.3769)  acc5: 100.0000 (99.2009)  time: 0.0287  data: 0.0002  max mem: 5511
[23:55:07.396419] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4481 (0.4928)  acc1: 84.3750 (85.3280)  acc5: 100.0000 (99.2243)  time: 0.0286  data: 0.0002  max mem: 5511
[23:55:07.679041] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4915 (0.4934)  acc1: 82.8125 (85.2339)  acc5: 100.0000 (99.2343)  time: 0.0282  data: 0.0001  max mem: 5511
[23:55:07.831077] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4614 (0.4924)  acc1: 85.9375 (85.3000)  acc5: 100.0000 (99.2300)  time: 0.0271  data: 0.0001  max mem: 5511
[23:55:07.981716] Test: Total time: 0:00:05 (0.0336 s / it)
[23:55:07.982415] * Acc@1 85.300 Acc@5 99.230 loss 0.492
[23:55:07.982706] Accuracy of the network on the 10000 test images: 85.3%
[23:55:07.982881] Max accuracy: 85.30%
[23:55:08.191415] log_dir: ./output_dir
[23:55:09.029412] Epoch: [68]  [  0/781]  eta: 0:10:53  lr: 0.000064  training_loss: 1.0923 (1.0923)  mae_loss: 0.0246 (0.0246)  classification_loss: 1.0676 (1.0676)  loss_mask: 0.0001 (0.0001)  time: 0.8363  data: 0.6130  max mem: 5511
[23:55:13.004072] Epoch: [68]  [ 20/781]  eta: 0:02:54  lr: 0.000064  training_loss: 1.2140 (1.2060)  mae_loss: 0.0262 (0.0258)  classification_loss: 1.1849 (1.1801)  loss_mask: 0.0001 (0.0001)  time: 0.1986  data: 0.0003  max mem: 5511
[23:55:16.973972] Epoch: [68]  [ 40/781]  eta: 0:02:38  lr: 0.000064  training_loss: 1.1790 (1.2025)  mae_loss: 0.0272 (0.0260)  classification_loss: 1.1543 (1.1763)  loss_mask: 0.0002 (0.0001)  time: 0.1984  data: 0.0002  max mem: 5511
[23:55:20.920035] Epoch: [68]  [ 60/781]  eta: 0:02:30  lr: 0.000064  training_loss: 1.2471 (1.2128)  mae_loss: 0.0254 (0.0261)  classification_loss: 1.2182 (1.1866)  loss_mask: 0.0001 (0.0001)  time: 0.1972  data: 0.0002  max mem: 5511
[23:55:24.846235] Epoch: [68]  [ 80/781]  eta: 0:02:24  lr: 0.000064  training_loss: 1.1718 (1.2019)  mae_loss: 0.0276 (0.0263)  classification_loss: 1.1476 (1.1754)  loss_mask: 0.0001 (0.0001)  time: 0.1962  data: 0.0002  max mem: 5511
[23:55:28.776503] Epoch: [68]  [100/781]  eta: 0:02:18  lr: 0.000064  training_loss: 1.2023 (1.2041)  mae_loss: 0.0262 (0.0264)  classification_loss: 1.1784 (1.1776)  loss_mask: 0.0001 (0.0001)  time: 0.1964  data: 0.0002  max mem: 5511
[23:55:32.721596] Epoch: [68]  [120/781]  eta: 0:02:13  lr: 0.000064  training_loss: 1.2181 (1.2054)  mae_loss: 0.0249 (0.0262)  classification_loss: 1.1864 (1.1790)  loss_mask: 0.0001 (0.0002)  time: 0.1972  data: 0.0002  max mem: 5511
[23:55:36.675257] Epoch: [68]  [140/781]  eta: 0:02:09  lr: 0.000064  training_loss: 1.2280 (1.2098)  mae_loss: 0.0268 (0.0263)  classification_loss: 1.2044 (1.1833)  loss_mask: 0.0001 (0.0002)  time: 0.1976  data: 0.0003  max mem: 5511
[23:55:40.620714] Epoch: [68]  [160/781]  eta: 0:02:05  lr: 0.000064  training_loss: 1.2428 (1.2116)  mae_loss: 0.0254 (0.0262)  classification_loss: 1.2170 (1.1852)  loss_mask: 0.0001 (0.0002)  time: 0.1972  data: 0.0003  max mem: 5511
[23:55:44.562219] Epoch: [68]  [180/781]  eta: 0:02:00  lr: 0.000064  training_loss: 1.2565 (1.2133)  mae_loss: 0.0262 (0.0264)  classification_loss: 1.2282 (1.1868)  loss_mask: 0.0001 (0.0001)  time: 0.1970  data: 0.0002  max mem: 5511
[23:55:48.512077] Epoch: [68]  [200/781]  eta: 0:01:56  lr: 0.000064  training_loss: 1.2088 (1.2105)  mae_loss: 0.0244 (0.0262)  classification_loss: 1.1838 (1.1842)  loss_mask: 0.0001 (0.0001)  time: 0.1974  data: 0.0002  max mem: 5511
[23:55:52.473809] Epoch: [68]  [220/781]  eta: 0:01:52  lr: 0.000063  training_loss: 1.1864 (1.2079)  mae_loss: 0.0244 (0.0261)  classification_loss: 1.1571 (1.1817)  loss_mask: 0.0001 (0.0001)  time: 0.1980  data: 0.0002  max mem: 5511
[23:55:56.430392] Epoch: [68]  [240/781]  eta: 0:01:48  lr: 0.000063  training_loss: 1.1800 (1.2073)  mae_loss: 0.0269 (0.0261)  classification_loss: 1.1518 (1.1810)  loss_mask: 0.0001 (0.0001)  time: 0.1978  data: 0.0002  max mem: 5511
[23:56:00.377519] Epoch: [68]  [260/781]  eta: 0:01:44  lr: 0.000063  training_loss: 1.1662 (1.2048)  mae_loss: 0.0257 (0.0262)  classification_loss: 1.1384 (1.1785)  loss_mask: 0.0001 (0.0001)  time: 0.1973  data: 0.0002  max mem: 5511
[23:56:04.299699] Epoch: [68]  [280/781]  eta: 0:01:39  lr: 0.000063  training_loss: 1.2394 (1.2063)  mae_loss: 0.0264 (0.0262)  classification_loss: 1.2096 (1.1799)  loss_mask: 0.0001 (0.0001)  time: 0.1960  data: 0.0002  max mem: 5511
[23:56:08.212250] Epoch: [68]  [300/781]  eta: 0:01:35  lr: 0.000063  training_loss: 1.1584 (1.2043)  mae_loss: 0.0269 (0.0263)  classification_loss: 1.1315 (1.1779)  loss_mask: 0.0001 (0.0001)  time: 0.1956  data: 0.0002  max mem: 5511
[23:56:12.141276] Epoch: [68]  [320/781]  eta: 0:01:31  lr: 0.000063  training_loss: 1.1240 (1.2023)  mae_loss: 0.0256 (0.0262)  classification_loss: 1.0986 (1.1760)  loss_mask: 0.0001 (0.0001)  time: 0.1964  data: 0.0002  max mem: 5511
[23:56:16.106093] Epoch: [68]  [340/781]  eta: 0:01:27  lr: 0.000063  training_loss: 1.2065 (1.2017)  mae_loss: 0.0258 (0.0262)  classification_loss: 1.1804 (1.1754)  loss_mask: 0.0001 (0.0001)  time: 0.1981  data: 0.0002  max mem: 5511
[23:56:20.021456] Epoch: [68]  [360/781]  eta: 0:01:23  lr: 0.000063  training_loss: 1.2366 (1.2039)  mae_loss: 0.0253 (0.0261)  classification_loss: 1.2123 (1.1776)  loss_mask: 0.0001 (0.0001)  time: 0.1957  data: 0.0002  max mem: 5511
[23:56:23.932911] Epoch: [68]  [380/781]  eta: 0:01:19  lr: 0.000063  training_loss: 1.2086 (1.2047)  mae_loss: 0.0271 (0.0262)  classification_loss: 1.1804 (1.1783)  loss_mask: 0.0001 (0.0001)  time: 0.1955  data: 0.0003  max mem: 5511
[23:56:27.871848] Epoch: [68]  [400/781]  eta: 0:01:15  lr: 0.000063  training_loss: 1.2291 (1.2062)  mae_loss: 0.0269 (0.0262)  classification_loss: 1.2040 (1.1798)  loss_mask: 0.0001 (0.0001)  time: 0.1969  data: 0.0003  max mem: 5511
[23:56:31.849411] Epoch: [68]  [420/781]  eta: 0:01:11  lr: 0.000063  training_loss: 1.2396 (1.2077)  mae_loss: 0.0268 (0.0263)  classification_loss: 1.2159 (1.1813)  loss_mask: 0.0001 (0.0001)  time: 0.1988  data: 0.0002  max mem: 5511
[23:56:35.787027] Epoch: [68]  [440/781]  eta: 0:01:07  lr: 0.000062  training_loss: 1.1960 (1.2077)  mae_loss: 0.0258 (0.0263)  classification_loss: 1.1688 (1.1813)  loss_mask: 0.0001 (0.0001)  time: 0.1968  data: 0.0002  max mem: 5511
[23:56:39.707720] Epoch: [68]  [460/781]  eta: 0:01:03  lr: 0.000062  training_loss: 1.1708 (1.2072)  mae_loss: 0.0260 (0.0262)  classification_loss: 1.1505 (1.1808)  loss_mask: 0.0001 (0.0001)  time: 0.1960  data: 0.0002  max mem: 5511
[23:56:43.680486] Epoch: [68]  [480/781]  eta: 0:00:59  lr: 0.000062  training_loss: 1.2131 (1.2067)  mae_loss: 0.0244 (0.0262)  classification_loss: 1.1912 (1.1804)  loss_mask: 0.0001 (0.0001)  time: 0.1986  data: 0.0002  max mem: 5511
[23:56:47.606998] Epoch: [68]  [500/781]  eta: 0:00:55  lr: 0.000062  training_loss: 1.1679 (1.2064)  mae_loss: 0.0262 (0.0262)  classification_loss: 1.1446 (1.1801)  loss_mask: 0.0001 (0.0001)  time: 0.1962  data: 0.0002  max mem: 5511
[23:56:51.545402] Epoch: [68]  [520/781]  eta: 0:00:51  lr: 0.000062  training_loss: 1.1490 (1.2046)  mae_loss: 0.0267 (0.0262)  classification_loss: 1.1180 (1.1783)  loss_mask: 0.0001 (0.0001)  time: 0.1968  data: 0.0004  max mem: 5511
[23:56:55.498868] Epoch: [68]  [540/781]  eta: 0:00:47  lr: 0.000062  training_loss: 1.2002 (1.2053)  mae_loss: 0.0258 (0.0262)  classification_loss: 1.1734 (1.1789)  loss_mask: 0.0001 (0.0001)  time: 0.1976  data: 0.0002  max mem: 5511
[23:56:59.433013] Epoch: [68]  [560/781]  eta: 0:00:43  lr: 0.000062  training_loss: 1.2250 (1.2062)  mae_loss: 0.0268 (0.0262)  classification_loss: 1.1981 (1.1798)  loss_mask: 0.0001 (0.0001)  time: 0.1966  data: 0.0002  max mem: 5511
[23:57:03.397010] Epoch: [68]  [580/781]  eta: 0:00:39  lr: 0.000062  training_loss: 1.1856 (1.2057)  mae_loss: 0.0269 (0.0262)  classification_loss: 1.1558 (1.1793)  loss_mask: 0.0001 (0.0001)  time: 0.1981  data: 0.0002  max mem: 5511
[23:57:07.337930] Epoch: [68]  [600/781]  eta: 0:00:35  lr: 0.000062  training_loss: 1.1906 (1.2060)  mae_loss: 0.0260 (0.0263)  classification_loss: 1.1595 (1.1796)  loss_mask: 0.0001 (0.0001)  time: 0.1969  data: 0.0003  max mem: 5511
[23:57:11.292173] Epoch: [68]  [620/781]  eta: 0:00:31  lr: 0.000062  training_loss: 1.1702 (1.2061)  mae_loss: 0.0264 (0.0263)  classification_loss: 1.1433 (1.1797)  loss_mask: 0.0001 (0.0001)  time: 0.1976  data: 0.0001  max mem: 5511
[23:57:15.232839] Epoch: [68]  [640/781]  eta: 0:00:27  lr: 0.000062  training_loss: 1.1580 (1.2053)  mae_loss: 0.0249 (0.0262)  classification_loss: 1.1298 (1.1789)  loss_mask: 0.0001 (0.0001)  time: 0.1969  data: 0.0002  max mem: 5511
[23:57:19.208456] Epoch: [68]  [660/781]  eta: 0:00:23  lr: 0.000061  training_loss: 1.2062 (1.2053)  mae_loss: 0.0264 (0.0263)  classification_loss: 1.1823 (1.1789)  loss_mask: 0.0001 (0.0001)  time: 0.1987  data: 0.0003  max mem: 5511
[23:57:23.189872] Epoch: [68]  [680/781]  eta: 0:00:20  lr: 0.000061  training_loss: 1.1911 (1.2044)  mae_loss: 0.0260 (0.0263)  classification_loss: 1.1669 (1.1780)  loss_mask: 0.0001 (0.0001)  time: 0.1990  data: 0.0002  max mem: 5511
[23:57:27.118986] Epoch: [68]  [700/781]  eta: 0:00:16  lr: 0.000061  training_loss: 1.2058 (1.2047)  mae_loss: 0.0265 (0.0263)  classification_loss: 1.1747 (1.1783)  loss_mask: 0.0001 (0.0001)  time: 0.1964  data: 0.0002  max mem: 5511
[23:57:31.080982] Epoch: [68]  [720/781]  eta: 0:00:12  lr: 0.000061  training_loss: 1.2400 (1.2056)  mae_loss: 0.0265 (0.0263)  classification_loss: 1.2130 (1.1792)  loss_mask: 0.0001 (0.0001)  time: 0.1980  data: 0.0002  max mem: 5511
[23:57:35.021058] Epoch: [68]  [740/781]  eta: 0:00:08  lr: 0.000061  training_loss: 1.1663 (1.2048)  mae_loss: 0.0255 (0.0263)  classification_loss: 1.1405 (1.1784)  loss_mask: 0.0001 (0.0001)  time: 0.1969  data: 0.0002  max mem: 5511
[23:57:38.999882] Epoch: [68]  [760/781]  eta: 0:00:04  lr: 0.000061  training_loss: 1.1981 (1.2055)  mae_loss: 0.0261 (0.0263)  classification_loss: 1.1719 (1.1791)  loss_mask: 0.0001 (0.0001)  time: 0.1988  data: 0.0002  max mem: 5511
[23:57:42.965808] Epoch: [68]  [780/781]  eta: 0:00:00  lr: 0.000061  training_loss: 1.1664 (1.2051)  mae_loss: 0.0246 (0.0263)  classification_loss: 1.1367 (1.1787)  loss_mask: 0.0001 (0.0001)  time: 0.1982  data: 0.0002  max mem: 5511
[23:57:43.114849] Epoch: [68] Total time: 0:02:34 (0.1984 s / it)
[23:57:43.115335] Averaged stats: lr: 0.000061  training_loss: 1.1664 (1.2051)  mae_loss: 0.0246 (0.0263)  classification_loss: 1.1367 (1.1787)  loss_mask: 0.0001 (0.0001)
[23:57:43.753023] Test:  [  0/157]  eta: 0:01:39  testing_loss: 0.6002 (0.6002)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.6331  data: 0.5997  max mem: 5511
[23:57:44.040320] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5133 (0.5313)  acc1: 82.8125 (84.3750)  acc5: 98.4375 (99.1477)  time: 0.0835  data: 0.0548  max mem: 5511
[23:57:44.330236] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4736 (0.4769)  acc1: 84.3750 (85.6399)  acc5: 100.0000 (99.3304)  time: 0.0287  data: 0.0004  max mem: 5511
[23:57:44.613505] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4573 (0.4963)  acc1: 84.3750 (84.6270)  acc5: 100.0000 (99.1935)  time: 0.0285  data: 0.0003  max mem: 5511
[23:57:44.901376] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5105 (0.4983)  acc1: 84.3750 (84.7942)  acc5: 100.0000 (99.1616)  time: 0.0284  data: 0.0002  max mem: 5511
[23:57:45.189411] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4957 (0.4947)  acc1: 85.9375 (85.0797)  acc5: 100.0000 (99.1728)  time: 0.0287  data: 0.0002  max mem: 5511
[23:57:45.479572] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4707 (0.4908)  acc1: 84.3750 (85.1178)  acc5: 100.0000 (99.2059)  time: 0.0287  data: 0.0002  max mem: 5511
[23:57:45.766003] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4484 (0.4839)  acc1: 84.3750 (85.4533)  acc5: 100.0000 (99.2298)  time: 0.0287  data: 0.0002  max mem: 5511
[23:57:46.053417] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4881 (0.4915)  acc1: 84.3750 (85.2238)  acc5: 100.0000 (99.2284)  time: 0.0285  data: 0.0002  max mem: 5511
[23:57:46.334877] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5100 (0.4882)  acc1: 84.3750 (85.3537)  acc5: 98.4375 (99.1758)  time: 0.0283  data: 0.0002  max mem: 5511
[23:57:46.620088] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4867 (0.4914)  acc1: 84.3750 (85.1949)  acc5: 98.4375 (99.1646)  time: 0.0282  data: 0.0002  max mem: 5511
[23:57:46.903799] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4984 (0.4917)  acc1: 84.3750 (85.2477)  acc5: 98.4375 (99.1554)  time: 0.0283  data: 0.0002  max mem: 5511
[23:57:47.193792] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4984 (0.4926)  acc1: 85.9375 (85.2144)  acc5: 98.4375 (99.1477)  time: 0.0286  data: 0.0004  max mem: 5511
[23:57:47.479338] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5054 (0.4936)  acc1: 84.3750 (85.1264)  acc5: 98.4375 (99.1412)  time: 0.0286  data: 0.0004  max mem: 5511
[23:57:47.765525] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4709 (0.4907)  acc1: 85.9375 (85.2061)  acc5: 100.0000 (99.1689)  time: 0.0285  data: 0.0002  max mem: 5511
[23:57:48.046672] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4709 (0.4904)  acc1: 85.9375 (85.2339)  acc5: 100.0000 (99.1618)  time: 0.0282  data: 0.0001  max mem: 5511
[23:57:48.196871] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4614 (0.4901)  acc1: 84.3750 (85.2100)  acc5: 100.0000 (99.1600)  time: 0.0271  data: 0.0001  max mem: 5511
[23:57:48.341012] Test: Total time: 0:00:05 (0.0333 s / it)
[23:57:48.341472] * Acc@1 85.210 Acc@5 99.160 loss 0.490
[23:57:48.341789] Accuracy of the network on the 10000 test images: 85.2%
[23:57:48.342028] Max accuracy: 85.30%
[23:57:48.578714] log_dir: ./output_dir
[23:57:49.467944] Epoch: [69]  [  0/781]  eta: 0:11:33  lr: 0.000061  training_loss: 1.0831 (1.0831)  mae_loss: 0.0238 (0.0238)  classification_loss: 1.0592 (1.0592)  loss_mask: 0.0001 (0.0001)  time: 0.8875  data: 0.6426  max mem: 5511
[23:57:53.419326] Epoch: [69]  [ 20/781]  eta: 0:02:55  lr: 0.000061  training_loss: 1.2187 (1.1998)  mae_loss: 0.0265 (0.0262)  classification_loss: 1.1882 (1.1735)  loss_mask: 0.0001 (0.0001)  time: 0.1975  data: 0.0002  max mem: 5511
[23:57:57.407819] Epoch: [69]  [ 40/781]  eta: 0:02:39  lr: 0.000061  training_loss: 1.2264 (1.2116)  mae_loss: 0.0246 (0.0258)  classification_loss: 1.2007 (1.1856)  loss_mask: 0.0001 (0.0003)  time: 0.1993  data: 0.0002  max mem: 5511
[23:58:01.325202] Epoch: [69]  [ 60/781]  eta: 0:02:30  lr: 0.000061  training_loss: 1.2019 (1.2152)  mae_loss: 0.0258 (0.0259)  classification_loss: 1.1767 (1.1890)  loss_mask: 0.0001 (0.0002)  time: 0.1958  data: 0.0002  max mem: 5511
[23:58:05.238254] Epoch: [69]  [ 80/781]  eta: 0:02:24  lr: 0.000061  training_loss: 1.1753 (1.2105)  mae_loss: 0.0261 (0.0261)  classification_loss: 1.1474 (1.1842)  loss_mask: 0.0001 (0.0002)  time: 0.1956  data: 0.0002  max mem: 5511
[23:58:09.157779] Epoch: [69]  [100/781]  eta: 0:02:18  lr: 0.000060  training_loss: 1.1791 (1.2081)  mae_loss: 0.0257 (0.0262)  classification_loss: 1.1534 (1.1817)  loss_mask: 0.0001 (0.0002)  time: 0.1959  data: 0.0003  max mem: 5511
[23:58:13.052457] Epoch: [69]  [120/781]  eta: 0:02:13  lr: 0.000060  training_loss: 1.2090 (1.2063)  mae_loss: 0.0251 (0.0261)  classification_loss: 1.1818 (1.1800)  loss_mask: 0.0001 (0.0002)  time: 0.1947  data: 0.0002  max mem: 5511
[23:58:16.985177] Epoch: [69]  [140/781]  eta: 0:02:09  lr: 0.000060  training_loss: 1.1663 (1.2061)  mae_loss: 0.0254 (0.0260)  classification_loss: 1.1413 (1.1799)  loss_mask: 0.0001 (0.0002)  time: 0.1966  data: 0.0002  max mem: 5511
[23:58:20.905665] Epoch: [69]  [160/781]  eta: 0:02:04  lr: 0.000060  training_loss: 1.1825 (1.2026)  mae_loss: 0.0245 (0.0259)  classification_loss: 1.1556 (1.1766)  loss_mask: 0.0001 (0.0002)  time: 0.1959  data: 0.0005  max mem: 5511
[23:58:24.831693] Epoch: [69]  [180/781]  eta: 0:02:00  lr: 0.000060  training_loss: 1.1950 (1.2023)  mae_loss: 0.0247 (0.0259)  classification_loss: 1.1704 (1.1762)  loss_mask: 0.0001 (0.0001)  time: 0.1962  data: 0.0003  max mem: 5511
[23:58:28.745100] Epoch: [69]  [200/781]  eta: 0:01:56  lr: 0.000060  training_loss: 1.2267 (1.2039)  mae_loss: 0.0250 (0.0259)  classification_loss: 1.2037 (1.1769)  loss_mask: 0.0013 (0.0011)  time: 0.1956  data: 0.0002  max mem: 5511
[23:58:32.652105] Epoch: [69]  [220/781]  eta: 0:01:51  lr: 0.000060  training_loss: 1.2098 (1.2048)  mae_loss: 0.0247 (0.0258)  classification_loss: 1.1717 (1.1750)  loss_mask: 0.0231 (0.0040)  time: 0.1953  data: 0.0002  max mem: 5511
[23:58:36.580416] Epoch: [69]  [240/781]  eta: 0:01:47  lr: 0.000060  training_loss: 1.2153 (1.2051)  mae_loss: 0.0268 (0.0259)  classification_loss: 1.1857 (1.1752)  loss_mask: 0.0032 (0.0040)  time: 0.1963  data: 0.0002  max mem: 5511
[23:58:40.523130] Epoch: [69]  [260/781]  eta: 0:01:43  lr: 0.000060  training_loss: 1.1605 (1.2041)  mae_loss: 0.0252 (0.0259)  classification_loss: 1.1301 (1.1743)  loss_mask: 0.0015 (0.0038)  time: 0.1971  data: 0.0002  max mem: 5511

[23:58:44.475284] Epoch: [69]  [280/781]  eta: 0:01:39  lr: 0.000060  training_loss: 1.1916 (1.2035)  mae_loss: 0.0269 (0.0259)  classification_loss: 1.1628 (1.1739)  loss_mask: 0.0009 (0.0036)  time: 0.1975  data: 0.0004  max mem: 5511
[23:58:48.378530] Epoch: [69]  [300/781]  eta: 0:01:35  lr: 0.000060  training_loss: 1.2141 (1.2036)  mae_loss: 0.0252 (0.0260)  classification_loss: 1.1826 (1.1741)  loss_mask: 0.0006 (0.0035)  time: 0.1951  data: 0.0003  max mem: 5511
[23:58:52.308191] Epoch: [69]  [320/781]  eta: 0:01:31  lr: 0.000059  training_loss: 1.2401 (1.2037)  mae_loss: 0.0254 (0.0260)  classification_loss: 1.2122 (1.1745)  loss_mask: 0.0005 (0.0033)  time: 0.1964  data: 0.0002  max mem: 5511
[23:58:56.204603] Epoch: [69]  [340/781]  eta: 0:01:27  lr: 0.000059  training_loss: 1.1749 (1.2026)  mae_loss: 0.0255 (0.0259)  classification_loss: 1.1491 (1.1735)  loss_mask: 0.0004 (0.0031)  time: 0.1947  data: 0.0002  max mem: 5511
[23:59:00.105385] Epoch: [69]  [360/781]  eta: 0:01:23  lr: 0.000059  training_loss: 1.2473 (1.2043)  mae_loss: 0.0252 (0.0259)  classification_loss: 1.2215 (1.1754)  loss_mask: 0.0005 (0.0030)  time: 0.1950  data: 0.0002  max mem: 5511
[23:59:04.006393] Epoch: [69]  [380/781]  eta: 0:01:19  lr: 0.000059  training_loss: 1.2408 (1.2067)  mae_loss: 0.0260 (0.0259)  classification_loss: 1.2150 (1.1779)  loss_mask: 0.0003 (0.0028)  time: 0.1950  data: 0.0002  max mem: 5511
[23:59:07.895208] Epoch: [69]  [400/781]  eta: 0:01:15  lr: 0.000059  training_loss: 1.2064 (1.2075)  mae_loss: 0.0259 (0.0260)  classification_loss: 1.1717 (1.1789)  loss_mask: 0.0002 (0.0027)  time: 0.1944  data: 0.0002  max mem: 5511
[23:59:11.814996] Epoch: [69]  [420/781]  eta: 0:01:11  lr: 0.000059  training_loss: 1.1741 (1.2054)  mae_loss: 0.0251 (0.0260)  classification_loss: 1.1516 (1.1769)  loss_mask: 0.0003 (0.0026)  time: 0.1959  data: 0.0002  max mem: 5511
[23:59:15.717258] Epoch: [69]  [440/781]  eta: 0:01:07  lr: 0.000059  training_loss: 1.1916 (1.2045)  mae_loss: 0.0247 (0.0260)  classification_loss: 1.1635 (1.1761)  loss_mask: 0.0003 (0.0025)  time: 0.1950  data: 0.0002  max mem: 5511
[23:59:19.639369] Epoch: [69]  [460/781]  eta: 0:01:03  lr: 0.000059  training_loss: 1.1830 (1.2041)  mae_loss: 0.0262 (0.0260)  classification_loss: 1.1566 (1.1757)  loss_mask: 0.0001 (0.0024)  time: 0.1960  data: 0.0002  max mem: 5511
[23:59:23.544124] Epoch: [69]  [480/781]  eta: 0:00:59  lr: 0.000059  training_loss: 1.1893 (1.2042)  mae_loss: 0.0256 (0.0260)  classification_loss: 1.1646 (1.1759)  loss_mask: 0.0002 (0.0023)  time: 0.1951  data: 0.0002  max mem: 5511
[23:59:27.454446] Epoch: [69]  [500/781]  eta: 0:00:55  lr: 0.000059  training_loss: 1.1860 (1.2046)  mae_loss: 0.0261 (0.0260)  classification_loss: 1.1639 (1.1764)  loss_mask: 0.0002 (0.0022)  time: 0.1954  data: 0.0002  max mem: 5511
[23:59:31.371797] Epoch: [69]  [520/781]  eta: 0:00:51  lr: 0.000059  training_loss: 1.1654 (1.2036)  mae_loss: 0.0250 (0.0259)  classification_loss: 1.1382 (1.1755)  loss_mask: 0.0003 (0.0022)  time: 0.1957  data: 0.0003  max mem: 5511
[23:59:35.316677] Epoch: [69]  [540/781]  eta: 0:00:47  lr: 0.000058  training_loss: 1.2105 (1.2032)  mae_loss: 0.0252 (0.0259)  classification_loss: 1.1882 (1.1752)  loss_mask: 0.0002 (0.0021)  time: 0.1971  data: 0.0003  max mem: 5511
[23:59:39.255395] Epoch: [69]  [560/781]  eta: 0:00:43  lr: 0.000058  training_loss: 1.1633 (1.2023)  mae_loss: 0.0255 (0.0259)  classification_loss: 1.1365 (1.1743)  loss_mask: 0.0002 (0.0020)  time: 0.1968  data: 0.0002  max mem: 5511
[23:59:43.166047] Epoch: [69]  [580/781]  eta: 0:00:39  lr: 0.000058  training_loss: 1.1743 (1.2021)  mae_loss: 0.0261 (0.0259)  classification_loss: 1.1464 (1.1742)  loss_mask: 0.0002 (0.0020)  time: 0.1955  data: 0.0003  max mem: 5511
[23:59:47.103686] Epoch: [69]  [600/781]  eta: 0:00:35  lr: 0.000058  training_loss: 1.2167 (1.2025)  mae_loss: 0.0266 (0.0260)  classification_loss: 1.1897 (1.1746)  loss_mask: 0.0002 (0.0019)  time: 0.1968  data: 0.0002  max mem: 5511
[23:59:51.027851] Epoch: [69]  [620/781]  eta: 0:00:31  lr: 0.000058  training_loss: 1.2058 (1.2033)  mae_loss: 0.0262 (0.0260)  classification_loss: 1.1782 (1.1755)  loss_mask: 0.0002 (0.0019)  time: 0.1961  data: 0.0002  max mem: 5511
[23:59:54.944232] Epoch: [69]  [640/781]  eta: 0:00:27  lr: 0.000058  training_loss: 1.1566 (1.2025)  mae_loss: 0.0258 (0.0260)  classification_loss: 1.1357 (1.1747)  loss_mask: 0.0002 (0.0018)  time: 0.1957  data: 0.0002  max mem: 5511
[23:59:58.845497] Epoch: [69]  [660/781]  eta: 0:00:23  lr: 0.000058  training_loss: 1.2099 (1.2031)  mae_loss: 0.0253 (0.0260)  classification_loss: 1.1815 (1.1754)  loss_mask: 0.0001 (0.0018)  time: 0.1950  data: 0.0002  max mem: 5511
[00:00:02.787274] Epoch: [69]  [680/781]  eta: 0:00:19  lr: 0.000058  training_loss: 1.1869 (1.2025)  mae_loss: 0.0250 (0.0260)  classification_loss: 1.1627 (1.1748)  loss_mask: 0.0001 (0.0017)  time: 0.1970  data: 0.0003  max mem: 5511
[00:00:06.708832] Epoch: [69]  [700/781]  eta: 0:00:15  lr: 0.000058  training_loss: 1.2007 (1.2031)  mae_loss: 0.0263 (0.0260)  classification_loss: 1.1772 (1.1754)  loss_mask: 0.0001 (0.0017)  time: 0.1960  data: 0.0002  max mem: 5511
[00:00:10.617586] Epoch: [69]  [720/781]  eta: 0:00:12  lr: 0.000058  training_loss: 1.2096 (1.2035)  mae_loss: 0.0269 (0.0260)  classification_loss: 1.1873 (1.1758)  loss_mask: 0.0002 (0.0017)  time: 0.1953  data: 0.0003  max mem: 5511
[00:00:14.531186] Epoch: [69]  [740/781]  eta: 0:00:08  lr: 0.000058  training_loss: 1.1835 (1.2028)  mae_loss: 0.0264 (0.0260)  classification_loss: 1.1526 (1.1752)  loss_mask: 0.0001 (0.0016)  time: 0.1956  data: 0.0002  max mem: 5511
[00:00:18.463901] Epoch: [69]  [760/781]  eta: 0:00:04  lr: 0.000057  training_loss: 1.2241 (1.2035)  mae_loss: 0.0258 (0.0260)  classification_loss: 1.1983 (1.1760)  loss_mask: 0.0001 (0.0016)  time: 0.1966  data: 0.0002  max mem: 5511
[00:00:22.417348] Epoch: [69]  [780/781]  eta: 0:00:00  lr: 0.000057  training_loss: 1.2669 (1.2050)  mae_loss: 0.0265 (0.0260)  classification_loss: 1.2379 (1.1774)  loss_mask: 0.0001 (0.0016)  time: 0.1976  data: 0.0002  max mem: 5511
[00:00:22.578479] Epoch: [69] Total time: 0:02:33 (0.1972 s / it)
[00:00:22.578953] Averaged stats: lr: 0.000057  training_loss: 1.2669 (1.2050)  mae_loss: 0.0265 (0.0260)  classification_loss: 1.2379 (1.1774)  loss_mask: 0.0001 (0.0016)
[00:00:23.219722] Test:  [  0/157]  eta: 0:01:39  testing_loss: 0.5215 (0.5215)  acc1: 84.3750 (84.3750)  acc5: 100.0000 (100.0000)  time: 0.6366  data: 0.6056  max mem: 5511
[00:00:23.502685] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5215 (0.5265)  acc1: 84.3750 (83.6648)  acc5: 100.0000 (99.7159)  time: 0.0834  data: 0.0552  max mem: 5511
[00:00:23.783326] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4699 (0.4744)  acc1: 85.9375 (86.3095)  acc5: 100.0000 (99.7768)  time: 0.0280  data: 0.0002  max mem: 5511
[00:00:24.069294] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4699 (0.4926)  acc1: 84.3750 (85.1815)  acc5: 100.0000 (99.5464)  time: 0.0282  data: 0.0002  max mem: 5511
[00:00:24.360683] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5037 (0.4963)  acc1: 84.3750 (85.2896)  acc5: 100.0000 (99.4284)  time: 0.0287  data: 0.0002  max mem: 5511
[00:00:24.645773] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4917 (0.4916)  acc1: 85.9375 (85.3248)  acc5: 100.0000 (99.3873)  time: 0.0287  data: 0.0003  max mem: 5511
[00:00:24.928814] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4948 (0.4876)  acc1: 85.9375 (85.5277)  acc5: 100.0000 (99.3852)  time: 0.0283  data: 0.0003  max mem: 5511
[00:00:25.211858] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4762 (0.4829)  acc1: 85.9375 (85.8715)  acc5: 100.0000 (99.3838)  time: 0.0282  data: 0.0002  max mem: 5511
[00:00:25.496998] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4979 (0.4899)  acc1: 85.9375 (85.6674)  acc5: 100.0000 (99.3056)  time: 0.0282  data: 0.0002  max mem: 5511
[00:00:25.783582] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4874 (0.4852)  acc1: 85.9375 (85.8860)  acc5: 98.4375 (99.2445)  time: 0.0284  data: 0.0002  max mem: 5511
[00:00:26.065996] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4794 (0.4877)  acc1: 87.5000 (85.7519)  acc5: 98.4375 (99.2265)  time: 0.0283  data: 0.0002  max mem: 5511
[00:00:26.353592] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4794 (0.4866)  acc1: 85.9375 (85.7827)  acc5: 100.0000 (99.2258)  time: 0.0284  data: 0.0004  max mem: 5511
[00:00:26.635566] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4735 (0.4873)  acc1: 85.9375 (85.7825)  acc5: 100.0000 (99.2252)  time: 0.0284  data: 0.0004  max mem: 5511
[00:00:26.918095] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4793 (0.4880)  acc1: 84.3750 (85.7228)  acc5: 100.0000 (99.2486)  time: 0.0281  data: 0.0002  max mem: 5511
[00:00:27.199670] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4927 (0.4872)  acc1: 84.3750 (85.7159)  acc5: 100.0000 (99.2797)  time: 0.0281  data: 0.0002  max mem: 5511
[00:00:27.478152] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4971 (0.4871)  acc1: 84.3750 (85.6581)  acc5: 100.0000 (99.2757)  time: 0.0279  data: 0.0001  max mem: 5511
[00:00:27.631078] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4927 (0.4872)  acc1: 82.8125 (85.5700)  acc5: 100.0000 (99.2800)  time: 0.0270  data: 0.0001  max mem: 5511
[00:00:27.800560] Test: Total time: 0:00:05 (0.0332 s / it)
[00:00:27.802125] * Acc@1 85.570 Acc@5 99.280 loss 0.487
[00:00:27.802523] Accuracy of the network on the 10000 test images: 85.6%
[00:00:27.802755] Max accuracy: 85.57%
[00:00:28.086389] log_dir: ./output_dir
[00:00:28.950107] Epoch: [70]  [  0/781]  eta: 0:11:13  lr: 0.000057  training_loss: 0.9473 (0.9473)  mae_loss: 0.0217 (0.0217)  classification_loss: 0.9256 (0.9256)  loss_mask: 0.0001 (0.0001)  time: 0.8619  data: 0.6439  max mem: 5511
[00:00:32.870871] Epoch: [70]  [ 20/781]  eta: 0:02:53  lr: 0.000057  training_loss: 1.1184 (1.1570)  mae_loss: 0.0249 (0.0253)  classification_loss: 1.1007 (1.1316)  loss_mask: 0.0001 (0.0001)  time: 0.1959  data: 0.0005  max mem: 5511
[00:00:36.859402] Epoch: [70]  [ 40/781]  eta: 0:02:38  lr: 0.000057  training_loss: 1.1877 (1.1661)  mae_loss: 0.0255 (0.0255)  classification_loss: 1.1644 (1.1403)  loss_mask: 0.0001 (0.0003)  time: 0.1993  data: 0.0003  max mem: 5511
[00:00:40.787195] Epoch: [70]  [ 60/781]  eta: 0:02:30  lr: 0.000057  training_loss: 1.2290 (1.1807)  mae_loss: 0.0259 (0.0257)  classification_loss: 1.2059 (1.1547)  loss_mask: 0.0003 (0.0003)  time: 0.1963  data: 0.0003  max mem: 5511
[00:00:44.781057] Epoch: [70]  [ 80/781]  eta: 0:02:24  lr: 0.000057  training_loss: 1.1424 (1.1753)  mae_loss: 0.0242 (0.0256)  classification_loss: 1.1216 (1.1494)  loss_mask: 0.0002 (0.0003)  time: 0.1996  data: 0.0002  max mem: 5511
[00:00:48.724088] Epoch: [70]  [100/781]  eta: 0:02:19  lr: 0.000057  training_loss: 1.1768 (1.1760)  mae_loss: 0.0243 (0.0253)  classification_loss: 1.1539 (1.1504)  loss_mask: 0.0001 (0.0003)  time: 0.1971  data: 0.0002  max mem: 5511
[00:00:52.648439] Epoch: [70]  [120/781]  eta: 0:02:14  lr: 0.000057  training_loss: 1.1979 (1.1778)  mae_loss: 0.0256 (0.0254)  classification_loss: 1.1721 (1.1520)  loss_mask: 0.0001 (0.0004)  time: 0.1961  data: 0.0002  max mem: 5511
[00:00:56.567034] Epoch: [70]  [140/781]  eta: 0:02:09  lr: 0.000057  training_loss: 1.1522 (1.1738)  mae_loss: 0.0247 (0.0254)  classification_loss: 1.1213 (1.1480)  loss_mask: 0.0002 (0.0003)  time: 0.1959  data: 0.0002  max mem: 5511
[00:01:00.538910] Epoch: [70]  [160/781]  eta: 0:02:05  lr: 0.000057  training_loss: 1.2002 (1.1782)  mae_loss: 0.0256 (0.0256)  classification_loss: 1.1745 (1.1522)  loss_mask: 0.0001 (0.0003)  time: 0.1985  data: 0.0002  max mem: 5511
[00:01:04.498058] Epoch: [70]  [180/781]  eta: 0:02:00  lr: 0.000057  training_loss: 1.1508 (1.1790)  mae_loss: 0.0261 (0.0257)  classification_loss: 1.1241 (1.1530)  loss_mask: 0.0001 (0.0003)  time: 0.1979  data: 0.0003  max mem: 5511
[00:01:08.415725] Epoch: [70]  [200/781]  eta: 0:01:56  lr: 0.000057  training_loss: 1.1757 (1.1794)  mae_loss: 0.0269 (0.0258)  classification_loss: 1.1444 (1.1533)  loss_mask: 0.0001 (0.0003)  time: 0.1958  data: 0.0003  max mem: 5511
[00:01:12.357587] Epoch: [70]  [220/781]  eta: 0:01:52  lr: 0.000056  training_loss: 1.2282 (1.1809)  mae_loss: 0.0249 (0.0258)  classification_loss: 1.2052 (1.1548)  loss_mask: 0.0001 (0.0003)  time: 0.1970  data: 0.0002  max mem: 5511
[00:01:16.328305] Epoch: [70]  [240/781]  eta: 0:01:48  lr: 0.000056  training_loss: 1.1294 (1.1801)  mae_loss: 0.0262 (0.0259)  classification_loss: 1.1062 (1.1539)  loss_mask: 0.0001 (0.0003)  time: 0.1985  data: 0.0002  max mem: 5511
[00:01:20.236199] Epoch: [70]  [260/781]  eta: 0:01:44  lr: 0.000056  training_loss: 1.1637 (1.1791)  mae_loss: 0.0265 (0.0259)  classification_loss: 1.1346 (1.1529)  loss_mask: 0.0001 (0.0003)  time: 0.1953  data: 0.0002  max mem: 5511
[00:01:24.198659] Epoch: [70]  [280/781]  eta: 0:01:39  lr: 0.000056  training_loss: 1.1384 (1.1778)  mae_loss: 0.0256 (0.0259)  classification_loss: 1.1114 (1.1516)  loss_mask: 0.0001 (0.0003)  time: 0.1980  data: 0.0002  max mem: 5511
[00:01:28.129646] Epoch: [70]  [300/781]  eta: 0:01:35  lr: 0.000056  training_loss: 1.2003 (1.1792)  mae_loss: 0.0259 (0.0259)  classification_loss: 1.1737 (1.1531)  loss_mask: 0.0001 (0.0003)  time: 0.1965  data: 0.0002  max mem: 5511
[00:01:32.104716] Epoch: [70]  [320/781]  eta: 0:01:31  lr: 0.000056  training_loss: 1.2370 (1.1818)  mae_loss: 0.0260 (0.0259)  classification_loss: 1.2085 (1.1556)  loss_mask: 0.0001 (0.0003)  time: 0.1987  data: 0.0002  max mem: 5511
[00:01:36.073407] Epoch: [70]  [340/781]  eta: 0:01:27  lr: 0.000056  training_loss: 1.1194 (1.1819)  mae_loss: 0.0256 (0.0259)  classification_loss: 1.0900 (1.1556)  loss_mask: 0.0002 (0.0003)  time: 0.1983  data: 0.0002  max mem: 5511
[00:01:40.009043] Epoch: [70]  [360/781]  eta: 0:01:23  lr: 0.000056  training_loss: 1.2222 (1.1847)  mae_loss: 0.0248 (0.0258)  classification_loss: 1.1973 (1.1585)  loss_mask: 0.0001 (0.0003)  time: 0.1967  data: 0.0002  max mem: 5511
[00:01:43.924131] Epoch: [70]  [380/781]  eta: 0:01:19  lr: 0.000056  training_loss: 1.2425 (1.1881)  mae_loss: 0.0259 (0.0259)  classification_loss: 1.2119 (1.1619)  loss_mask: 0.0001 (0.0003)  time: 0.1957  data: 0.0002  max mem: 5511
[00:01:47.825168] Epoch: [70]  [400/781]  eta: 0:01:15  lr: 0.000056  training_loss: 1.1502 (1.1865)  mae_loss: 0.0258 (0.0259)  classification_loss: 1.1206 (1.1602)  loss_mask: 0.0002 (0.0003)  time: 0.1950  data: 0.0002  max mem: 5511
[00:01:51.735644] Epoch: [70]  [420/781]  eta: 0:01:11  lr: 0.000056  training_loss: 1.1550 (1.1861)  mae_loss: 0.0272 (0.0260)  classification_loss: 1.1284 (1.1598)  loss_mask: 0.0001 (0.0003)  time: 0.1955  data: 0.0003  max mem: 5511
[00:01:55.643083] Epoch: [70]  [440/781]  eta: 0:01:07  lr: 0.000055  training_loss: 1.1783 (1.1868)  mae_loss: 0.0258 (0.0260)  classification_loss: 1.1529 (1.1605)  loss_mask: 0.0001 (0.0003)  time: 0.1953  data: 0.0002  max mem: 5511
[00:01:59.568303] Epoch: [70]  [460/781]  eta: 0:01:03  lr: 0.000055  training_loss: 1.1781 (1.1860)  mae_loss: 0.0248 (0.0260)  classification_loss: 1.1532 (1.1597)  loss_mask: 0.0001 (0.0003)  time: 0.1962  data: 0.0002  max mem: 5511
[00:02:03.520614] Epoch: [70]  [480/781]  eta: 0:00:59  lr: 0.000055  training_loss: 1.2271 (1.1869)  mae_loss: 0.0257 (0.0260)  classification_loss: 1.1966 (1.1606)  loss_mask: 0.0001 (0.0003)  time: 0.1975  data: 0.0002  max mem: 5511
[00:02:07.458715] Epoch: [70]  [500/781]  eta: 0:00:55  lr: 0.000055  training_loss: 1.1861 (1.1878)  mae_loss: 0.0253 (0.0260)  classification_loss: 1.1615 (1.1616)  loss_mask: 0.0001 (0.0003)  time: 0.1968  data: 0.0002  max mem: 5511
[00:02:11.362177] Epoch: [70]  [520/781]  eta: 0:00:51  lr: 0.000055  training_loss: 1.1651 (1.1873)  mae_loss: 0.0256 (0.0260)  classification_loss: 1.1401 (1.1610)  loss_mask: 0.0001 (0.0003)  time: 0.1951  data: 0.0002  max mem: 5511
[00:02:15.260410] Epoch: [70]  [540/781]  eta: 0:00:47  lr: 0.000055  training_loss: 1.1966 (1.1887)  mae_loss: 0.0248 (0.0260)  classification_loss: 1.1708 (1.1624)  loss_mask: 0.0002 (0.0003)  time: 0.1948  data: 0.0002  max mem: 5511
[00:02:19.174955] Epoch: [70]  [560/781]  eta: 0:00:43  lr: 0.000055  training_loss: 1.1992 (1.1894)  mae_loss: 0.0254 (0.0260)  classification_loss: 1.1677 (1.1631)  loss_mask: 0.0002 (0.0003)  time: 0.1956  data: 0.0002  max mem: 5511
[00:02:23.087989] Epoch: [70]  [580/781]  eta: 0:00:39  lr: 0.000055  training_loss: 1.1270 (1.1877)  mae_loss: 0.0268 (0.0260)  classification_loss: 1.0986 (1.1614)  loss_mask: 0.0002 (0.0003)  time: 0.1955  data: 0.0003  max mem: 5511
[00:02:26.993969] Epoch: [70]  [600/781]  eta: 0:00:35  lr: 0.000055  training_loss: 1.1746 (1.1876)  mae_loss: 0.0288 (0.0261)  classification_loss: 1.1538 (1.1612)  loss_mask: 0.0001 (0.0003)  time: 0.1952  data: 0.0002  max mem: 5511
[00:02:30.928706] Epoch: [70]  [620/781]  eta: 0:00:31  lr: 0.000055  training_loss: 1.1711 (1.1873)  mae_loss: 0.0271 (0.0261)  classification_loss: 1.1426 (1.1609)  loss_mask: 0.0001 (0.0003)  time: 0.1966  data: 0.0002  max mem: 5511
[00:02:34.922426] Epoch: [70]  [640/781]  eta: 0:00:27  lr: 0.000055  training_loss: 1.1482 (1.1867)  mae_loss: 0.0248 (0.0261)  classification_loss: 1.1257 (1.1603)  loss_mask: 0.0001 (0.0003)  time: 0.1996  data: 0.0002  max mem: 5511
[00:02:38.901403] Epoch: [70]  [660/781]  eta: 0:00:23  lr: 0.000055  training_loss: 1.1712 (1.1865)  mae_loss: 0.0247 (0.0260)  classification_loss: 1.1481 (1.1602)  loss_mask: 0.0001 (0.0003)  time: 0.1989  data: 0.0003  max mem: 5511
[00:02:42.896675] Epoch: [70]  [680/781]  eta: 0:00:19  lr: 0.000054  training_loss: 1.1701 (1.1861)  mae_loss: 0.0272 (0.0261)  classification_loss: 1.1405 (1.1597)  loss_mask: 0.0001 (0.0003)  time: 0.1997  data: 0.0002  max mem: 5511
[00:02:46.839849] Epoch: [70]  [700/781]  eta: 0:00:16  lr: 0.000054  training_loss: 1.1794 (1.1868)  mae_loss: 0.0250 (0.0261)  classification_loss: 1.1504 (1.1605)  loss_mask: 0.0001 (0.0003)  time: 0.1970  data: 0.0003  max mem: 5511
[00:02:50.779544] Epoch: [70]  [720/781]  eta: 0:00:12  lr: 0.000054  training_loss: 1.2319 (1.1878)  mae_loss: 0.0259 (0.0261)  classification_loss: 1.2060 (1.1615)  loss_mask: 0.0001 (0.0003)  time: 0.1969  data: 0.0003  max mem: 5511
[00:02:54.709535] Epoch: [70]  [740/781]  eta: 0:00:08  lr: 0.000054  training_loss: 1.2022 (1.1879)  mae_loss: 0.0267 (0.0261)  classification_loss: 1.1768 (1.1615)  loss_mask: 0.0001 (0.0003)  time: 0.1964  data: 0.0002  max mem: 5511
[00:02:58.633507] Epoch: [70]  [760/781]  eta: 0:00:04  lr: 0.000054  training_loss: 1.2066 (1.1882)  mae_loss: 0.0270 (0.0261)  classification_loss: 1.1752 (1.1618)  loss_mask: 0.0001 (0.0003)  time: 0.1961  data: 0.0002  max mem: 5511
[00:03:02.515883] Epoch: [70]  [780/781]  eta: 0:00:00  lr: 0.000054  training_loss: 1.1671 (1.1877)  mae_loss: 0.0254 (0.0261)  classification_loss: 1.1412 (1.1613)  loss_mask: 0.0001 (0.0003)  time: 0.1940  data: 0.0002  max mem: 5511
[00:03:02.669680] Epoch: [70] Total time: 0:02:34 (0.1979 s / it)
[00:03:02.670247] Averaged stats: lr: 0.000054  training_loss: 1.1671 (1.1877)  mae_loss: 0.0254 (0.0261)  classification_loss: 1.1412 (1.1613)  loss_mask: 0.0001 (0.0003)
[00:03:04.025446] Test:  [  0/157]  eta: 0:01:39  testing_loss: 0.5926 (0.5926)  acc1: 82.8125 (82.8125)  acc5: 98.4375 (98.4375)  time: 0.6321  data: 0.6015  max mem: 5511
[00:03:04.308378] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5092 (0.5228)  acc1: 82.8125 (83.9489)  acc5: 98.4375 (99.1477)  time: 0.0830  data: 0.0548  max mem: 5511
[00:03:04.592979] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4652 (0.4805)  acc1: 84.3750 (85.4911)  acc5: 100.0000 (99.4048)  time: 0.0281  data: 0.0002  max mem: 5511
[00:03:04.877551] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4821 (0.4974)  acc1: 84.3750 (84.6270)  acc5: 100.0000 (99.3448)  time: 0.0283  data: 0.0002  max mem: 5511
[00:03:05.164143] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5020 (0.4993)  acc1: 84.3750 (85.0610)  acc5: 100.0000 (99.2378)  time: 0.0284  data: 0.0002  max mem: 5511
[00:03:05.445729] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4880 (0.4946)  acc1: 87.5000 (85.3554)  acc5: 100.0000 (99.2034)  time: 0.0283  data: 0.0002  max mem: 5511
[00:03:05.729196] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4660 (0.4898)  acc1: 87.5000 (85.6045)  acc5: 100.0000 (99.1803)  time: 0.0281  data: 0.0002  max mem: 5511
[00:03:06.012085] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4626 (0.4839)  acc1: 87.5000 (85.8715)  acc5: 100.0000 (99.1637)  time: 0.0282  data: 0.0002  max mem: 5511
[00:03:06.297338] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4936 (0.4914)  acc1: 85.9375 (85.7060)  acc5: 100.0000 (99.1705)  time: 0.0283  data: 0.0002  max mem: 5511
[00:03:06.579608] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4696 (0.4867)  acc1: 85.9375 (85.9032)  acc5: 100.0000 (99.1243)  time: 0.0282  data: 0.0002  max mem: 5511
[00:03:06.863139] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5043 (0.4907)  acc1: 85.9375 (85.7209)  acc5: 98.4375 (99.0873)  time: 0.0281  data: 0.0002  max mem: 5511
[00:03:07.146011] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5159 (0.4903)  acc1: 85.9375 (85.7404)  acc5: 98.4375 (99.0850)  time: 0.0281  data: 0.0002  max mem: 5511
[00:03:07.427487] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5085 (0.4898)  acc1: 85.9375 (85.6921)  acc5: 98.4375 (99.0961)  time: 0.0281  data: 0.0002  max mem: 5511
[00:03:07.709729] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4615 (0.4884)  acc1: 84.3750 (85.6990)  acc5: 100.0000 (99.1412)  time: 0.0281  data: 0.0002  max mem: 5511
[00:03:07.990563] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4511 (0.4860)  acc1: 85.9375 (85.7713)  acc5: 100.0000 (99.1800)  time: 0.0280  data: 0.0002  max mem: 5511
[00:03:08.269016] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4862 (0.4873)  acc1: 84.3750 (85.7099)  acc5: 100.0000 (99.1722)  time: 0.0278  data: 0.0001  max mem: 5511
[00:03:08.420462] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4569 (0.4850)  acc1: 84.3750 (85.6900)  acc5: 100.0000 (99.1700)  time: 0.0270  data: 0.0001  max mem: 5511
[00:03:08.575138] Test: Total time: 0:00:05 (0.0330 s / it)
[00:03:08.575615] * Acc@1 85.690 Acc@5 99.170 loss 0.485
[00:03:08.575914] Accuracy of the network on the 10000 test images: 85.7%
[00:03:08.576089] Max accuracy: 85.69%
[00:03:08.766258] log_dir: ./output_dir
[00:03:09.628050] Epoch: [71]  [  0/781]  eta: 0:11:11  lr: 0.000054  training_loss: 1.1259 (1.1259)  mae_loss: 0.0282 (0.0282)  classification_loss: 1.0976 (1.0976)  loss_mask: 0.0001 (0.0001)  time: 0.8599  data: 0.6357  max mem: 5511
[00:03:13.593942] Epoch: [71]  [ 20/781]  eta: 0:02:54  lr: 0.000054  training_loss: 1.1857 (1.1539)  mae_loss: 0.0255 (0.0263)  classification_loss: 1.1622 (1.1275)  loss_mask: 0.0001 (0.0001)  time: 0.1982  data: 0.0002  max mem: 5511
[00:03:17.502777] Epoch: [71]  [ 40/781]  eta: 0:02:37  lr: 0.000054  training_loss: 1.2301 (1.2033)  mae_loss: 0.0262 (0.0263)  classification_loss: 1.2034 (1.1769)  loss_mask: 0.0001 (0.0001)  time: 0.1954  data: 0.0002  max mem: 5511
[00:03:21.416229] Epoch: [71]  [ 60/781]  eta: 0:02:29  lr: 0.000054  training_loss: 1.2144 (1.2030)  mae_loss: 0.0270 (0.0265)  classification_loss: 1.1818 (1.1764)  loss_mask: 0.0001 (0.0001)  time: 0.1956  data: 0.0002  max mem: 5511
[00:03:25.314735] Epoch: [71]  [ 80/781]  eta: 0:02:23  lr: 0.000054  training_loss: 1.1642 (1.1948)  mae_loss: 0.0254 (0.0263)  classification_loss: 1.1406 (1.1684)  loss_mask: 0.0001 (0.0001)  time: 0.1948  data: 0.0002  max mem: 5511
[00:03:29.233056] Epoch: [71]  [100/781]  eta: 0:02:17  lr: 0.000054  training_loss: 1.1793 (1.1882)  mae_loss: 0.0254 (0.0262)  classification_loss: 1.1539 (1.1619)  loss_mask: 0.0001 (0.0001)  time: 0.1958  data: 0.0002  max mem: 5511
[00:03:33.191995] Epoch: [71]  [120/781]  eta: 0:02:13  lr: 0.000053  training_loss: 1.1933 (1.1939)  mae_loss: 0.0245 (0.0260)  classification_loss: 1.1672 (1.1677)  loss_mask: 0.0002 (0.0001)  time: 0.1979  data: 0.0002  max mem: 5511
[00:03:37.128326] Epoch: [71]  [140/781]  eta: 0:02:08  lr: 0.000053  training_loss: 1.2024 (1.1904)  mae_loss: 0.0268 (0.0261)  classification_loss: 1.1742 (1.1642)  loss_mask: 0.0001 (0.0001)  time: 0.1967  data: 0.0002  max mem: 5511
[00:03:41.055919] Epoch: [71]  [160/781]  eta: 0:02:04  lr: 0.000053  training_loss: 1.2120 (1.1903)  mae_loss: 0.0253 (0.0260)  classification_loss: 1.1894 (1.1641)  loss_mask: 0.0001 (0.0001)  time: 0.1963  data: 0.0002  max mem: 5511
[00:03:44.979065] Epoch: [71]  [180/781]  eta: 0:02:00  lr: 0.000053  training_loss: 1.1889 (1.1895)  mae_loss: 0.0239 (0.0259)  classification_loss: 1.1650 (1.1635)  loss_mask: 0.0001 (0.0001)  time: 0.1961  data: 0.0002  max mem: 5511
[00:03:48.901962] Epoch: [71]  [200/781]  eta: 0:01:55  lr: 0.000053  training_loss: 1.1194 (1.1851)  mae_loss: 0.0253 (0.0258)  classification_loss: 1.0971 (1.1591)  loss_mask: 0.0001 (0.0001)  time: 0.1960  data: 0.0003  max mem: 5511
[00:03:52.800150] Epoch: [71]  [220/781]  eta: 0:01:51  lr: 0.000053  training_loss: 1.1857 (1.1850)  mae_loss: 0.0264 (0.0259)  classification_loss: 1.1634 (1.1589)  loss_mask: 0.0001 (0.0001)  time: 0.1948  data: 0.0003  max mem: 5511
[00:03:56.715088] Epoch: [71]  [240/781]  eta: 0:01:47  lr: 0.000053  training_loss: 1.1919 (1.1867)  mae_loss: 0.0267 (0.0261)  classification_loss: 1.1630 (1.1605)  loss_mask: 0.0001 (0.0001)  time: 0.1957  data: 0.0002  max mem: 5511
[00:04:00.663318] Epoch: [71]  [260/781]  eta: 0:01:43  lr: 0.000053  training_loss: 1.1838 (1.1856)  mae_loss: 0.0246 (0.0260)  classification_loss: 1.1612 (1.1595)  loss_mask: 0.0001 (0.0001)  time: 0.1973  data: 0.0002  max mem: 5511
[00:04:04.571670] Epoch: [71]  [280/781]  eta: 0:01:39  lr: 0.000053  training_loss: 1.1923 (1.1876)  mae_loss: 0.0263 (0.0260)  classification_loss: 1.1686 (1.1614)  loss_mask: 0.0001 (0.0001)  time: 0.1953  data: 0.0002  max mem: 5511
[00:04:08.491817] Epoch: [71]  [300/781]  eta: 0:01:35  lr: 0.000053  training_loss: 1.1484 (1.1879)  mae_loss: 0.0266 (0.0260)  classification_loss: 1.1201 (1.1617)  loss_mask: 0.0001 (0.0001)  time: 0.1959  data: 0.0002  max mem: 5511
[00:04:12.422524] Epoch: [71]  [320/781]  eta: 0:01:31  lr: 0.000053  training_loss: 1.1543 (1.1863)  mae_loss: 0.0246 (0.0260)  classification_loss: 1.1297 (1.1602)  loss_mask: 0.0001 (0.0001)  time: 0.1964  data: 0.0003  max mem: 5511
[00:04:16.342477] Epoch: [71]  [340/781]  eta: 0:01:27  lr: 0.000053  training_loss: 1.1642 (1.1845)  mae_loss: 0.0251 (0.0259)  classification_loss: 1.1346 (1.1585)  loss_mask: 0.0001 (0.0001)  time: 0.1959  data: 0.0002  max mem: 5511
[00:04:20.251146] Epoch: [71]  [360/781]  eta: 0:01:23  lr: 0.000052  training_loss: 1.1917 (1.1852)  mae_loss: 0.0255 (0.0259)  classification_loss: 1.1715 (1.1591)  loss_mask: 0.0001 (0.0001)  time: 0.1954  data: 0.0001  max mem: 5511
[00:04:24.157140] Epoch: [71]  [380/781]  eta: 0:01:19  lr: 0.000052  training_loss: 1.1873 (1.1853)  mae_loss: 0.0261 (0.0259)  classification_loss: 1.1603 (1.1593)  loss_mask: 0.0001 (0.0001)  time: 0.1952  data: 0.0002  max mem: 5511
[00:04:28.089330] Epoch: [71]  [400/781]  eta: 0:01:15  lr: 0.000052  training_loss: 1.2090 (1.1867)  mae_loss: 0.0248 (0.0259)  classification_loss: 1.1846 (1.1607)  loss_mask: 0.0001 (0.0001)  time: 0.1965  data: 0.0002  max mem: 5511
[00:04:32.040199] Epoch: [71]  [420/781]  eta: 0:01:11  lr: 0.000052  training_loss: 1.1560 (1.1853)  mae_loss: 0.0269 (0.0259)  classification_loss: 1.1322 (1.1593)  loss_mask: 0.0001 (0.0002)  time: 0.1975  data: 0.0002  max mem: 5511
[00:04:35.976243] Epoch: [71]  [440/781]  eta: 0:01:07  lr: 0.000052  training_loss: 1.2294 (1.1870)  mae_loss: 0.0266 (0.0260)  classification_loss: 1.2049 (1.1608)  loss_mask: 0.0002 (0.0002)  time: 0.1967  data: 0.0002  max mem: 5511
[00:04:39.925823] Epoch: [71]  [460/781]  eta: 0:01:03  lr: 0.000052  training_loss: 1.1362 (1.1854)  mae_loss: 0.0267 (0.0260)  classification_loss: 1.1084 (1.1592)  loss_mask: 0.0001 (0.0002)  time: 0.1974  data: 0.0002  max mem: 5511
[00:04:43.884963] Epoch: [71]  [480/781]  eta: 0:00:59  lr: 0.000052  training_loss: 1.1925 (1.1856)  mae_loss: 0.0259 (0.0260)  classification_loss: 1.1645 (1.1594)  loss_mask: 0.0001 (0.0002)  time: 0.1979  data: 0.0002  max mem: 5511
[00:04:47.789966] Epoch: [71]  [500/781]  eta: 0:00:55  lr: 0.000052  training_loss: 1.2086 (1.1865)  mae_loss: 0.0259 (0.0260)  classification_loss: 1.1849 (1.1604)  loss_mask: 0.0001 (0.0002)  time: 0.1952  data: 0.0002  max mem: 5511
[00:04:51.692706] Epoch: [71]  [520/781]  eta: 0:00:51  lr: 0.000052  training_loss: 1.2058 (1.1872)  mae_loss: 0.0258 (0.0260)  classification_loss: 1.1808 (1.1610)  loss_mask: 0.0001 (0.0002)  time: 0.1950  data: 0.0002  max mem: 5511
[00:04:55.627679] Epoch: [71]  [540/781]  eta: 0:00:47  lr: 0.000052  training_loss: 1.1911 (1.1872)  mae_loss: 0.0260 (0.0260)  classification_loss: 1.1654 (1.1610)  loss_mask: 0.0001 (0.0002)  time: 0.1967  data: 0.0002  max mem: 5511
[00:04:59.598886] Epoch: [71]  [560/781]  eta: 0:00:43  lr: 0.000052  training_loss: 1.1483 (1.1866)  mae_loss: 0.0262 (0.0261)  classification_loss: 1.1219 (1.1603)  loss_mask: 0.0001 (0.0002)  time: 0.1985  data: 0.0002  max mem: 5511
[00:05:03.509896] Epoch: [71]  [580/781]  eta: 0:00:39  lr: 0.000052  training_loss: 1.1648 (1.1868)  mae_loss: 0.0268 (0.0261)  classification_loss: 1.1308 (1.1605)  loss_mask: 0.0001 (0.0002)  time: 0.1955  data: 0.0002  max mem: 5511
[00:05:07.481998] Epoch: [71]  [600/781]  eta: 0:00:35  lr: 0.000051  training_loss: 1.2148 (1.1874)  mae_loss: 0.0252 (0.0261)  classification_loss: 1.1899 (1.1612)  loss_mask: 0.0001 (0.0002)  time: 0.1985  data: 0.0002  max mem: 5511
[00:05:11.392858] Epoch: [71]  [620/781]  eta: 0:00:31  lr: 0.000051  training_loss: 1.1854 (1.1870)  mae_loss: 0.0257 (0.0261)  classification_loss: 1.1565 (1.1608)  loss_mask: 0.0001 (0.0002)  time: 0.1955  data: 0.0002  max mem: 5511
[00:05:15.314013] Epoch: [71]  [640/781]  eta: 0:00:27  lr: 0.000051  training_loss: 1.1919 (1.1877)  mae_loss: 0.0253 (0.0261)  classification_loss: 1.1699 (1.1614)  loss_mask: 0.0001 (0.0002)  time: 0.1960  data: 0.0002  max mem: 5511
[00:05:19.237222] Epoch: [71]  [660/781]  eta: 0:00:23  lr: 0.000051  training_loss: 1.1583 (1.1869)  mae_loss: 0.0250 (0.0260)  classification_loss: 1.1315 (1.1607)  loss_mask: 0.0002 (0.0002)  time: 0.1961  data: 0.0002  max mem: 5511
[00:05:23.133419] Epoch: [71]  [680/781]  eta: 0:00:19  lr: 0.000051  training_loss: 1.1777 (1.1866)  mae_loss: 0.0259 (0.0260)  classification_loss: 1.1445 (1.1603)  loss_mask: 0.0001 (0.0002)  time: 0.1947  data: 0.0002  max mem: 5511
[00:05:27.074785] Epoch: [71]  [700/781]  eta: 0:00:15  lr: 0.000051  training_loss: 1.1597 (1.1866)  mae_loss: 0.0254 (0.0260)  classification_loss: 1.1330 (1.1603)  loss_mask: 0.0001 (0.0002)  time: 0.1970  data: 0.0002  max mem: 5511
[00:05:30.988300] Epoch: [71]  [720/781]  eta: 0:00:12  lr: 0.000051  training_loss: 1.2273 (1.1875)  mae_loss: 0.0254 (0.0260)  classification_loss: 1.2003 (1.1612)  loss_mask: 0.0001 (0.0002)  time: 0.1956  data: 0.0002  max mem: 5511
[00:05:34.891311] Epoch: [71]  [740/781]  eta: 0:00:08  lr: 0.000051  training_loss: 1.1253 (1.1867)  mae_loss: 0.0245 (0.0260)  classification_loss: 1.1022 (1.1605)  loss_mask: 0.0001 (0.0002)  time: 0.1951  data: 0.0002  max mem: 5511
[00:05:38.820883] Epoch: [71]  [760/781]  eta: 0:00:04  lr: 0.000051  training_loss: 1.2269 (1.1877)  mae_loss: 0.0265 (0.0260)  classification_loss: 1.2017 (1.1615)  loss_mask: 0.0001 (0.0002)  time: 0.1964  data: 0.0002  max mem: 5511
[00:05:42.720176] Epoch: [71]  [780/781]  eta: 0:00:00  lr: 0.000051  training_loss: 1.2142 (1.1879)  mae_loss: 0.0255 (0.0260)  classification_loss: 1.1868 (1.1617)  loss_mask: 0.0001 (0.0002)  time: 0.1949  data: 0.0002  max mem: 5511
[00:05:42.875682] Epoch: [71] Total time: 0:02:34 (0.1973 s / it)
[00:05:42.876159] Averaged stats: lr: 0.000051  training_loss: 1.2142 (1.1879)  mae_loss: 0.0255 (0.0260)  classification_loss: 1.1868 (1.1617)  loss_mask: 0.0001 (0.0002)
[00:05:43.495193] Test:  [  0/157]  eta: 0:01:36  testing_loss: 0.5569 (0.5569)  acc1: 84.3750 (84.3750)  acc5: 96.8750 (96.8750)  time: 0.6151  data: 0.5859  max mem: 5511
[00:05:43.798919] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5333 (0.5221)  acc1: 82.8125 (83.2386)  acc5: 100.0000 (99.4318)  time: 0.0833  data: 0.0545  max mem: 5511
[00:05:44.087450] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4494 (0.4708)  acc1: 85.9375 (85.2679)  acc5: 100.0000 (99.5536)  time: 0.0294  data: 0.0009  max mem: 5511
[00:05:44.376367] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4453 (0.4892)  acc1: 85.9375 (84.5262)  acc5: 100.0000 (99.3952)  time: 0.0287  data: 0.0003  max mem: 5511
[00:05:44.661309] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5025 (0.4934)  acc1: 85.9375 (84.9848)  acc5: 100.0000 (99.2759)  time: 0.0285  data: 0.0002  max mem: 5511
[00:05:44.950205] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4813 (0.4867)  acc1: 85.9375 (85.4167)  acc5: 100.0000 (99.2647)  time: 0.0285  data: 0.0002  max mem: 5511
[00:05:45.238108] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4362 (0.4816)  acc1: 87.5000 (85.7070)  acc5: 100.0000 (99.2828)  time: 0.0287  data: 0.0002  max mem: 5511
[00:05:45.529995] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4362 (0.4729)  acc1: 87.5000 (86.0255)  acc5: 100.0000 (99.3398)  time: 0.0288  data: 0.0002  max mem: 5511
[00:05:45.829509] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4750 (0.4802)  acc1: 84.3750 (85.7639)  acc5: 100.0000 (99.3827)  time: 0.0294  data: 0.0003  max mem: 5511
[00:05:46.124292] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4819 (0.4749)  acc1: 82.8125 (85.9890)  acc5: 100.0000 (99.3475)  time: 0.0295  data: 0.0003  max mem: 5511
[00:05:46.412979] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4852 (0.4784)  acc1: 84.3750 (85.8911)  acc5: 98.4375 (99.3193)  time: 0.0290  data: 0.0003  max mem: 5511
[00:05:46.701380] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4985 (0.4788)  acc1: 84.3750 (85.9657)  acc5: 100.0000 (99.3243)  time: 0.0287  data: 0.0002  max mem: 5511
[00:05:46.985222] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4825 (0.4792)  acc1: 84.3750 (85.8213)  acc5: 100.0000 (99.3414)  time: 0.0284  data: 0.0002  max mem: 5511
[00:05:47.272557] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4510 (0.4796)  acc1: 82.8125 (85.7467)  acc5: 100.0000 (99.3559)  time: 0.0284  data: 0.0002  max mem: 5511
[00:05:47.562781] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4364 (0.4784)  acc1: 85.9375 (85.7713)  acc5: 100.0000 (99.3684)  time: 0.0287  data: 0.0003  max mem: 5511
[00:05:47.845703] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5030 (0.4788)  acc1: 85.9375 (85.7512)  acc5: 98.4375 (99.3171)  time: 0.0285  data: 0.0002  max mem: 5511
[00:05:47.999034] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4787 (0.4797)  acc1: 85.9375 (85.6600)  acc5: 98.4375 (99.3000)  time: 0.0273  data: 0.0002  max mem: 5511
[00:05:48.149184] Test: Total time: 0:00:05 (0.0336 s / it)
[00:05:48.149728] * Acc@1 85.660 Acc@5 99.300 loss 0.480
[00:05:48.150056] Accuracy of the network on the 10000 test images: 85.7%
[00:05:48.150278] Max accuracy: 85.69%
[00:05:48.518642] log_dir: ./output_dir
[00:05:49.375959] Epoch: [72]  [  0/781]  eta: 0:11:08  lr: 0.000051  training_loss: 0.9732 (0.9732)  mae_loss: 0.0310 (0.0310)  classification_loss: 0.9422 (0.9422)  loss_mask: 0.0001 (0.0001)  time: 0.8558  data: 0.6559  max mem: 5511
[00:05:53.308052] Epoch: [72]  [ 20/781]  eta: 0:02:53  lr: 0.000051  training_loss: 1.1292 (1.1333)  mae_loss: 0.0280 (0.0274)  classification_loss: 1.1049 (1.1057)  loss_mask: 0.0001 (0.0001)  time: 0.1965  data: 0.0004  max mem: 5511
[00:05:57.230631] Epoch: [72]  [ 40/781]  eta: 0:02:37  lr: 0.000050  training_loss: 1.1993 (1.1602)  mae_loss: 0.0263 (0.0266)  classification_loss: 1.1727 (1.1336)  loss_mask: 0.0001 (0.0001)  time: 0.1960  data: 0.0002  max mem: 5511
[00:06:01.201121] Epoch: [72]  [ 60/781]  eta: 0:02:29  lr: 0.000050  training_loss: 1.1827 (1.1739)  mae_loss: 0.0250 (0.0261)  classification_loss: 1.1527 (1.1477)  loss_mask: 0.0001 (0.0001)  time: 0.1984  data: 0.0002  max mem: 5511
[00:06:05.127003] Epoch: [72]  [ 80/781]  eta: 0:02:23  lr: 0.000050  training_loss: 1.1720 (1.1837)  mae_loss: 0.0247 (0.0261)  classification_loss: 1.1419 (1.1573)  loss_mask: 0.0001 (0.0002)  time: 0.1962  data: 0.0002  max mem: 5511
[00:06:09.036382] Epoch: [72]  [100/781]  eta: 0:02:18  lr: 0.000050  training_loss: 1.2006 (1.1910)  mae_loss: 0.0259 (0.0260)  classification_loss: 1.1730 (1.1647)  loss_mask: 0.0001 (0.0002)  time: 0.1954  data: 0.0002  max mem: 5511
[00:06:12.926731] Epoch: [72]  [120/781]  eta: 0:02:13  lr: 0.000050  training_loss: 1.1933 (1.1896)  mae_loss: 0.0239 (0.0257)  classification_loss: 1.1700 (1.1636)  loss_mask: 0.0001 (0.0003)  time: 0.1944  data: 0.0002  max mem: 5511
[00:06:16.853958] Epoch: [72]  [140/781]  eta: 0:02:08  lr: 0.000050  training_loss: 1.2158 (1.1911)  mae_loss: 0.0257 (0.0258)  classification_loss: 1.1867 (1.1651)  loss_mask: 0.0001 (0.0003)  time: 0.1963  data: 0.0002  max mem: 5511
[00:06:20.795374] Epoch: [72]  [160/781]  eta: 0:02:04  lr: 0.000050  training_loss: 1.1533 (1.1882)  mae_loss: 0.0248 (0.0257)  classification_loss: 1.1267 (1.1623)  loss_mask: 0.0001 (0.0003)  time: 0.1970  data: 0.0005  max mem: 5511
[00:06:24.727459] Epoch: [72]  [180/781]  eta: 0:02:00  lr: 0.000050  training_loss: 1.1752 (1.1851)  mae_loss: 0.0245 (0.0256)  classification_loss: 1.1508 (1.1593)  loss_mask: 0.0001 (0.0002)  time: 0.1965  data: 0.0002  max mem: 5511
[00:06:28.652021] Epoch: [72]  [200/781]  eta: 0:01:55  lr: 0.000050  training_loss: 1.2580 (1.1890)  mae_loss: 0.0257 (0.0257)  classification_loss: 1.2304 (1.1631)  loss_mask: 0.0001 (0.0002)  time: 0.1961  data: 0.0002  max mem: 5511
[00:06:32.589022] Epoch: [72]  [220/781]  eta: 0:01:51  lr: 0.000050  training_loss: 1.1603 (1.1897)  mae_loss: 0.0253 (0.0257)  classification_loss: 1.1315 (1.1638)  loss_mask: 0.0001 (0.0002)  time: 0.1968  data: 0.0002  max mem: 5511
[00:06:36.511242] Epoch: [72]  [240/781]  eta: 0:01:47  lr: 0.000050  training_loss: 1.1375 (1.1875)  mae_loss: 0.0253 (0.0257)  classification_loss: 1.1139 (1.1615)  loss_mask: 0.0001 (0.0002)  time: 0.1960  data: 0.0002  max mem: 5511
[00:06:40.421485] Epoch: [72]  [260/781]  eta: 0:01:43  lr: 0.000050  training_loss: 1.2005 (1.1871)  mae_loss: 0.0257 (0.0257)  classification_loss: 1.1711 (1.1611)  loss_mask: 0.0001 (0.0002)  time: 0.1954  data: 0.0002  max mem: 5511
[00:06:44.346240] Epoch: [72]  [280/781]  eta: 0:01:39  lr: 0.000049  training_loss: 1.1398 (1.1857)  mae_loss: 0.0262 (0.0258)  classification_loss: 1.1156 (1.1596)  loss_mask: 0.0001 (0.0002)  time: 0.1962  data: 0.0002  max mem: 5511
[00:06:48.263609] Epoch: [72]  [300/781]  eta: 0:01:35  lr: 0.000049  training_loss: 1.1918 (1.1867)  mae_loss: 0.0254 (0.0258)  classification_loss: 1.1666 (1.1607)  loss_mask: 0.0001 (0.0002)  time: 0.1958  data: 0.0002  max mem: 5511
[00:06:52.186052] Epoch: [72]  [320/781]  eta: 0:01:31  lr: 0.000049  training_loss: 1.1849 (1.1865)  mae_loss: 0.0246 (0.0258)  classification_loss: 1.1576 (1.1605)  loss_mask: 0.0001 (0.0002)  time: 0.1960  data: 0.0002  max mem: 5511
[00:06:56.126236] Epoch: [72]  [340/781]  eta: 0:01:27  lr: 0.000049  training_loss: 1.1606 (1.1851)  mae_loss: 0.0262 (0.0258)  classification_loss: 1.1342 (1.1591)  loss_mask: 0.0001 (0.0002)  time: 0.1969  data: 0.0002  max mem: 5511
[00:07:00.052494] Epoch: [72]  [360/781]  eta: 0:01:23  lr: 0.000049  training_loss: 1.2281 (1.1869)  mae_loss: 0.0245 (0.0257)  classification_loss: 1.2026 (1.1610)  loss_mask: 0.0001 (0.0002)  time: 0.1962  data: 0.0002  max mem: 5511
[00:07:03.956872] Epoch: [72]  [380/781]  eta: 0:01:19  lr: 0.000049  training_loss: 1.1417 (1.1870)  mae_loss: 0.0261 (0.0257)  classification_loss: 1.1152 (1.1611)  loss_mask: 0.0001 (0.0002)  time: 0.1951  data: 0.0002  max mem: 5511
[00:07:07.884782] Epoch: [72]  [400/781]  eta: 0:01:15  lr: 0.000049  training_loss: 1.1284 (1.1857)  mae_loss: 0.0251 (0.0257)  classification_loss: 1.0980 (1.1598)  loss_mask: 0.0001 (0.0002)  time: 0.1963  data: 0.0002  max mem: 5511
[00:07:11.809594] Epoch: [72]  [420/781]  eta: 0:01:11  lr: 0.000049  training_loss: 1.1802 (1.1861)  mae_loss: 0.0270 (0.0258)  classification_loss: 1.1509 (1.1601)  loss_mask: 0.0002 (0.0002)  time: 0.1961  data: 0.0002  max mem: 5511
[00:07:15.759342] Epoch: [72]  [440/781]  eta: 0:01:07  lr: 0.000049  training_loss: 1.1735 (1.1854)  mae_loss: 0.0247 (0.0257)  classification_loss: 1.1485 (1.1594)  loss_mask: 0.0002 (0.0002)  time: 0.1974  data: 0.0002  max mem: 5511
[00:07:19.776245] Epoch: [72]  [460/781]  eta: 0:01:03  lr: 0.000049  training_loss: 1.1246 (1.1844)  mae_loss: 0.0257 (0.0258)  classification_loss: 1.0985 (1.1584)  loss_mask: 0.0001 (0.0002)  time: 0.2008  data: 0.0002  max mem: 5511
[00:07:23.691078] Epoch: [72]  [480/781]  eta: 0:00:59  lr: 0.000049  training_loss: 1.2049 (1.1846)  mae_loss: 0.0265 (0.0258)  classification_loss: 1.1755 (1.1586)  loss_mask: 0.0001 (0.0002)  time: 0.1957  data: 0.0002  max mem: 5511
[00:07:27.614679] Epoch: [72]  [500/781]  eta: 0:00:55  lr: 0.000049  training_loss: 1.1737 (1.1843)  mae_loss: 0.0249 (0.0258)  classification_loss: 1.1471 (1.1583)  loss_mask: 0.0001 (0.0002)  time: 0.1961  data: 0.0002  max mem: 5511
[00:07:31.544255] Epoch: [72]  [520/781]  eta: 0:00:51  lr: 0.000048  training_loss: 1.1411 (1.1833)  mae_loss: 0.0266 (0.0258)  classification_loss: 1.1136 (1.1573)  loss_mask: 0.0001 (0.0002)  time: 0.1964  data: 0.0002  max mem: 5511
[00:07:35.463155] Epoch: [72]  [540/781]  eta: 0:00:47  lr: 0.000048  training_loss: 1.1420 (1.1823)  mae_loss: 0.0259 (0.0258)  classification_loss: 1.1138 (1.1563)  loss_mask: 0.0001 (0.0002)  time: 0.1959  data: 0.0003  max mem: 5511
[00:07:39.392900] Epoch: [72]  [560/781]  eta: 0:00:43  lr: 0.000048  training_loss: 1.1669 (1.1826)  mae_loss: 0.0258 (0.0258)  classification_loss: 1.1409 (1.1565)  loss_mask: 0.0001 (0.0002)  time: 0.1964  data: 0.0003  max mem: 5511
[00:07:43.321263] Epoch: [72]  [580/781]  eta: 0:00:39  lr: 0.000048  training_loss: 1.1617 (1.1823)  mae_loss: 0.0248 (0.0258)  classification_loss: 1.1367 (1.1563)  loss_mask: 0.0001 (0.0002)  time: 0.1963  data: 0.0002  max mem: 5511

[00:07:47.250157] Epoch: [72]  [600/781]  eta: 0:00:35  lr: 0.000048  training_loss: 1.1949 (1.1827)  mae_loss: 0.0256 (0.0258)  classification_loss: 1.1660 (1.1567)  loss_mask: 0.0001 (0.0002)  time: 0.1964  data: 0.0003  max mem: 5511
[00:07:51.179915] Epoch: [72]  [620/781]  eta: 0:00:31  lr: 0.000048  training_loss: 1.1729 (1.1823)  mae_loss: 0.0258 (0.0258)  classification_loss: 1.1464 (1.1563)  loss_mask: 0.0001 (0.0002)  time: 0.1964  data: 0.0003  max mem: 5511
[00:07:55.109460] Epoch: [72]  [640/781]  eta: 0:00:27  lr: 0.000048  training_loss: 1.1770 (1.1817)  mae_loss: 0.0255 (0.0258)  classification_loss: 1.1460 (1.1557)  loss_mask: 0.0001 (0.0002)  time: 0.1964  data: 0.0002  max mem: 5511
[00:07:59.068096] Epoch: [72]  [660/781]  eta: 0:00:23  lr: 0.000048  training_loss: 1.1984 (1.1825)  mae_loss: 0.0239 (0.0257)  classification_loss: 1.1743 (1.1566)  loss_mask: 0.0001 (0.0002)  time: 0.1979  data: 0.0002  max mem: 5511
[00:08:02.980184] Epoch: [72]  [680/781]  eta: 0:00:19  lr: 0.000048  training_loss: 1.1645 (1.1825)  mae_loss: 0.0243 (0.0257)  classification_loss: 1.1445 (1.1566)  loss_mask: 0.0001 (0.0002)  time: 0.1955  data: 0.0002  max mem: 5511
[00:08:06.877072] Epoch: [72]  [700/781]  eta: 0:00:15  lr: 0.000048  training_loss: 1.1607 (1.1819)  mae_loss: 0.0255 (0.0257)  classification_loss: 1.1370 (1.1560)  loss_mask: 0.0001 (0.0002)  time: 0.1948  data: 0.0003  max mem: 5511
[00:08:10.800136] Epoch: [72]  [720/781]  eta: 0:00:12  lr: 0.000048  training_loss: 1.1924 (1.1823)  mae_loss: 0.0262 (0.0257)  classification_loss: 1.1709 (1.1564)  loss_mask: 0.0001 (0.0002)  time: 0.1961  data: 0.0002  max mem: 5511
[00:08:14.715012] Epoch: [72]  [740/781]  eta: 0:00:08  lr: 0.000048  training_loss: 1.1489 (1.1815)  mae_loss: 0.0254 (0.0257)  classification_loss: 1.1241 (1.1556)  loss_mask: 0.0001 (0.0002)  time: 0.1956  data: 0.0002  max mem: 5511
[00:08:18.645642] Epoch: [72]  [760/781]  eta: 0:00:04  lr: 0.000048  training_loss: 1.1721 (1.1819)  mae_loss: 0.0249 (0.0257)  classification_loss: 1.1458 (1.1560)  loss_mask: 0.0001 (0.0002)  time: 0.1965  data: 0.0002  max mem: 5511
[00:08:22.580548] Epoch: [72]  [780/781]  eta: 0:00:00  lr: 0.000047  training_loss: 1.1756 (1.1823)  mae_loss: 0.0263 (0.0257)  classification_loss: 1.1507 (1.1564)  loss_mask: 0.0001 (0.0002)  time: 0.1967  data: 0.0002  max mem: 5511
[00:08:22.731002] Epoch: [72] Total time: 0:02:34 (0.1975 s / it)
[00:08:22.731777] Averaged stats: lr: 0.000047  training_loss: 1.1756 (1.1823)  mae_loss: 0.0263 (0.0257)  classification_loss: 1.1507 (1.1564)  loss_mask: 0.0001 (0.0002)
[00:08:23.348295] Test:  [  0/157]  eta: 0:01:36  testing_loss: 0.5248 (0.5248)  acc1: 82.8125 (82.8125)  acc5: 100.0000 (100.0000)  time: 0.6116  data: 0.5671  max mem: 5511
[00:08:23.640225] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4653 (0.5071)  acc1: 84.3750 (84.2330)  acc5: 100.0000 (99.5739)  time: 0.0820  data: 0.0517  max mem: 5511
[00:08:23.925293] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4579 (0.4652)  acc1: 85.9375 (85.8631)  acc5: 100.0000 (99.7024)  time: 0.0287  data: 0.0002  max mem: 5511
[00:08:24.212315] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4719 (0.4859)  acc1: 85.9375 (85.0302)  acc5: 100.0000 (99.3952)  time: 0.0285  data: 0.0002  max mem: 5511
[00:08:24.501817] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5061 (0.4869)  acc1: 84.3750 (85.1372)  acc5: 98.4375 (99.2759)  time: 0.0287  data: 0.0002  max mem: 5511
[00:08:24.786597] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4708 (0.4795)  acc1: 85.9375 (85.4473)  acc5: 100.0000 (99.3260)  time: 0.0286  data: 0.0002  max mem: 5511
[00:08:25.068878] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4426 (0.4742)  acc1: 87.5000 (85.7070)  acc5: 100.0000 (99.3340)  time: 0.0282  data: 0.0002  max mem: 5511
[00:08:25.351150] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4365 (0.4691)  acc1: 87.5000 (86.0255)  acc5: 100.0000 (99.3398)  time: 0.0281  data: 0.0002  max mem: 5511
[00:08:25.641024] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4564 (0.4756)  acc1: 85.9375 (85.7832)  acc5: 100.0000 (99.3827)  time: 0.0285  data: 0.0004  max mem: 5511
[00:08:25.926809] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5061 (0.4740)  acc1: 84.3750 (85.9203)  acc5: 100.0000 (99.3475)  time: 0.0286  data: 0.0004  max mem: 5511
[00:08:26.217962] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5025 (0.4777)  acc1: 84.3750 (85.7519)  acc5: 98.4375 (99.3193)  time: 0.0287  data: 0.0002  max mem: 5511
[00:08:26.499177] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5049 (0.4775)  acc1: 84.3750 (85.7404)  acc5: 98.4375 (99.3102)  time: 0.0285  data: 0.0002  max mem: 5511
[00:08:26.781232] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4690 (0.4778)  acc1: 84.3750 (85.7051)  acc5: 100.0000 (99.3543)  time: 0.0280  data: 0.0002  max mem: 5511
[00:08:27.063423] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4583 (0.4782)  acc1: 85.9375 (85.6990)  acc5: 100.0000 (99.3678)  time: 0.0281  data: 0.0002  max mem: 5511
[00:08:27.345444] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4583 (0.4764)  acc1: 87.5000 (85.7824)  acc5: 100.0000 (99.3794)  time: 0.0281  data: 0.0001  max mem: 5511
[00:08:27.625150] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4664 (0.4753)  acc1: 87.5000 (85.8030)  acc5: 100.0000 (99.3584)  time: 0.0280  data: 0.0001  max mem: 5511
[00:08:27.776624] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4664 (0.4765)  acc1: 85.9375 (85.7400)  acc5: 100.0000 (99.3600)  time: 0.0270  data: 0.0001  max mem: 5511
[00:08:27.921277] Test: Total time: 0:00:05 (0.0330 s / it)
[00:08:27.921788] * Acc@1 85.740 Acc@5 99.360 loss 0.477
[00:08:27.922205] Accuracy of the network on the 10000 test images: 85.7%
[00:08:27.922783] Max accuracy: 85.74%
[00:08:28.160336] log_dir: ./output_dir
[00:08:29.047671] Epoch: [73]  [  0/781]  eta: 0:11:30  lr: 0.000047  training_loss: 1.0817 (1.0817)  mae_loss: 0.0298 (0.0298)  classification_loss: 1.0519 (1.0519)  loss_mask: 0.0001 (0.0001)  time: 0.8845  data: 0.6662  max mem: 5511
[00:08:32.965689] Epoch: [73]  [ 20/781]  eta: 0:02:53  lr: 0.000047  training_loss: 1.1139 (1.1365)  mae_loss: 0.0263 (0.0268)  classification_loss: 1.0853 (1.1096)  loss_mask: 0.0001 (0.0001)  time: 0.1958  data: 0.0002  max mem: 5511
[00:08:36.887484] Epoch: [73]  [ 40/781]  eta: 0:02:37  lr: 0.000047  training_loss: 1.1932 (1.1582)  mae_loss: 0.0254 (0.0266)  classification_loss: 1.1581 (1.1315)  loss_mask: 0.0001 (0.0001)  time: 0.1960  data: 0.0002  max mem: 5511
[00:08:40.796128] Epoch: [73]  [ 60/781]  eta: 0:02:29  lr: 0.000047  training_loss: 1.1960 (1.1717)  mae_loss: 0.0257 (0.0264)  classification_loss: 1.1676 (1.1452)  loss_mask: 0.0001 (0.0001)  time: 0.1953  data: 0.0002  max mem: 5511
[00:08:44.731715] Epoch: [73]  [ 80/781]  eta: 0:02:23  lr: 0.000047  training_loss: 1.1339 (1.1666)  mae_loss: 0.0244 (0.0259)  classification_loss: 1.1122 (1.1407)  loss_mask: 0.0001 (0.0001)  time: 0.1967  data: 0.0002  max mem: 5511
[00:08:48.653225] Epoch: [73]  [100/781]  eta: 0:02:18  lr: 0.000047  training_loss: 1.1419 (1.1664)  mae_loss: 0.0248 (0.0256)  classification_loss: 1.1156 (1.1407)  loss_mask: 0.0001 (0.0001)  time: 0.1959  data: 0.0002  max mem: 5511
[00:08:52.574004] Epoch: [73]  [120/781]  eta: 0:02:13  lr: 0.000047  training_loss: 1.1137 (1.1616)  mae_loss: 0.0247 (0.0255)  classification_loss: 1.0890 (1.1360)  loss_mask: 0.0001 (0.0001)  time: 0.1960  data: 0.0002  max mem: 5511
[00:08:56.484864] Epoch: [73]  [140/781]  eta: 0:02:08  lr: 0.000047  training_loss: 1.1239 (1.1598)  mae_loss: 0.0247 (0.0255)  classification_loss: 1.1012 (1.1341)  loss_mask: 0.0001 (0.0002)  time: 0.1955  data: 0.0002  max mem: 5511
[00:09:00.415173] Epoch: [73]  [160/781]  eta: 0:02:04  lr: 0.000047  training_loss: 1.1679 (1.1617)  mae_loss: 0.0259 (0.0256)  classification_loss: 1.1378 (1.1358)  loss_mask: 0.0002 (0.0003)  time: 0.1964  data: 0.0002  max mem: 5511
[00:09:04.340021] Epoch: [73]  [180/781]  eta: 0:02:00  lr: 0.000047  training_loss: 1.1719 (1.1671)  mae_loss: 0.0243 (0.0256)  classification_loss: 1.1490 (1.1412)  loss_mask: 0.0002 (0.0003)  time: 0.1962  data: 0.0004  max mem: 5511
[00:09:08.249850] Epoch: [73]  [200/781]  eta: 0:01:55  lr: 0.000047  training_loss: 1.1591 (1.1674)  mae_loss: 0.0258 (0.0257)  classification_loss: 1.1335 (1.1413)  loss_mask: 0.0002 (0.0004)  time: 0.1954  data: 0.0003  max mem: 5511
[00:09:12.169351] Epoch: [73]  [220/781]  eta: 0:01:51  lr: 0.000047  training_loss: 1.2365 (1.1730)  mae_loss: 0.0251 (0.0257)  classification_loss: 1.1800 (1.1456)  loss_mask: 0.0068 (0.0017)  time: 0.1959  data: 0.0002  max mem: 5511
[00:09:16.095897] Epoch: [73]  [240/781]  eta: 0:01:47  lr: 0.000046  training_loss: 1.1665 (1.1748)  mae_loss: 0.0257 (0.0258)  classification_loss: 1.1388 (1.1472)  loss_mask: 0.0020 (0.0019)  time: 0.1962  data: 0.0004  max mem: 5511
[00:09:20.015542] Epoch: [73]  [260/781]  eta: 0:01:43  lr: 0.000046  training_loss: 1.1239 (1.1727)  mae_loss: 0.0243 (0.0257)  classification_loss: 1.0982 (1.1452)  loss_mask: 0.0005 (0.0018)  time: 0.1959  data: 0.0003  max mem: 5511
[00:09:23.958468] Epoch: [73]  [280/781]  eta: 0:01:39  lr: 0.000046  training_loss: 1.1225 (1.1703)  mae_loss: 0.0254 (0.0257)  classification_loss: 1.0961 (1.1429)  loss_mask: 0.0002 (0.0017)  time: 0.1971  data: 0.0002  max mem: 5511
[00:09:27.877916] Epoch: [73]  [300/781]  eta: 0:01:35  lr: 0.000046  training_loss: 1.1912 (1.1733)  mae_loss: 0.0269 (0.0258)  classification_loss: 1.1686 (1.1459)  loss_mask: 0.0001 (0.0016)  time: 0.1959  data: 0.0002  max mem: 5511
[00:09:31.781667] Epoch: [73]  [320/781]  eta: 0:01:31  lr: 0.000046  training_loss: 1.1523 (1.1727)  mae_loss: 0.0252 (0.0258)  classification_loss: 1.1292 (1.1454)  loss_mask: 0.0002 (0.0015)  time: 0.1951  data: 0.0002  max mem: 5511
[00:09:35.748352] Epoch: [73]  [340/781]  eta: 0:01:27  lr: 0.000046  training_loss: 1.1193 (1.1709)  mae_loss: 0.0251 (0.0258)  classification_loss: 1.0939 (1.1437)  loss_mask: 0.0001 (0.0014)  time: 0.1983  data: 0.0003  max mem: 5511
[00:09:39.658821] Epoch: [73]  [360/781]  eta: 0:01:23  lr: 0.000046  training_loss: 1.1772 (1.1720)  mae_loss: 0.0268 (0.0258)  classification_loss: 1.1465 (1.1448)  loss_mask: 0.0001 (0.0013)  time: 0.1954  data: 0.0002  max mem: 5511
[00:09:43.603540] Epoch: [73]  [380/781]  eta: 0:01:19  lr: 0.000046  training_loss: 1.1967 (1.1731)  mae_loss: 0.0260 (0.0259)  classification_loss: 1.1737 (1.1460)  loss_mask: 0.0001 (0.0013)  time: 0.1971  data: 0.0002  max mem: 5511
[00:09:47.531098] Epoch: [73]  [400/781]  eta: 0:01:15  lr: 0.000046  training_loss: 1.1900 (1.1745)  mae_loss: 0.0275 (0.0259)  classification_loss: 1.1618 (1.1474)  loss_mask: 0.0001 (0.0012)  time: 0.1963  data: 0.0002  max mem: 5511
[00:09:51.446478] Epoch: [73]  [420/781]  eta: 0:01:11  lr: 0.000046  training_loss: 1.1519 (1.1739)  mae_loss: 0.0262 (0.0259)  classification_loss: 1.1202 (1.1468)  loss_mask: 0.0001 (0.0012)  time: 0.1957  data: 0.0004  max mem: 5511
[00:09:55.375448] Epoch: [73]  [440/781]  eta: 0:01:07  lr: 0.000046  training_loss: 1.1571 (1.1736)  mae_loss: 0.0253 (0.0259)  classification_loss: 1.1330 (1.1465)  loss_mask: 0.0001 (0.0011)  time: 0.1964  data: 0.0003  max mem: 5511
[00:09:59.282404] Epoch: [73]  [460/781]  eta: 0:01:03  lr: 0.000046  training_loss: 1.1348 (1.1729)  mae_loss: 0.0241 (0.0259)  classification_loss: 1.1080 (1.1459)  loss_mask: 0.0001 (0.0011)  time: 0.1953  data: 0.0002  max mem: 5511
[00:10:03.204377] Epoch: [73]  [480/781]  eta: 0:00:59  lr: 0.000045  training_loss: 1.1618 (1.1724)  mae_loss: 0.0262 (0.0259)  classification_loss: 1.1350 (1.1455)  loss_mask: 0.0001 (0.0011)  time: 0.1960  data: 0.0002  max mem: 5511
[00:10:07.108931] Epoch: [73]  [500/781]  eta: 0:00:55  lr: 0.000045  training_loss: 1.1536 (1.1726)  mae_loss: 0.0268 (0.0259)  classification_loss: 1.1284 (1.1456)  loss_mask: 0.0001 (0.0010)  time: 0.1951  data: 0.0002  max mem: 5511
[00:10:11.046353] Epoch: [73]  [520/781]  eta: 0:00:51  lr: 0.000045  training_loss: 1.1468 (1.1728)  mae_loss: 0.0263 (0.0259)  classification_loss: 1.1160 (1.1459)  loss_mask: 0.0001 (0.0010)  time: 0.1968  data: 0.0002  max mem: 5511
[00:10:14.954299] Epoch: [73]  [540/781]  eta: 0:00:47  lr: 0.000045  training_loss: 1.1593 (1.1724)  mae_loss: 0.0252 (0.0259)  classification_loss: 1.1392 (1.1455)  loss_mask: 0.0001 (0.0010)  time: 0.1953  data: 0.0002  max mem: 5511
[00:10:18.902346] Epoch: [73]  [560/781]  eta: 0:00:43  lr: 0.000045  training_loss: 1.1423 (1.1717)  mae_loss: 0.0260 (0.0259)  classification_loss: 1.1148 (1.1449)  loss_mask: 0.0001 (0.0009)  time: 0.1973  data: 0.0002  max mem: 5511
[00:10:22.838297] Epoch: [73]  [580/781]  eta: 0:00:39  lr: 0.000045  training_loss: 1.1680 (1.1720)  mae_loss: 0.0256 (0.0259)  classification_loss: 1.1425 (1.1451)  loss_mask: 0.0001 (0.0009)  time: 0.1967  data: 0.0003  max mem: 5511
[00:10:26.768409] Epoch: [73]  [600/781]  eta: 0:00:35  lr: 0.000045  training_loss: 1.1534 (1.1715)  mae_loss: 0.0252 (0.0259)  classification_loss: 1.1258 (1.1447)  loss_mask: 0.0001 (0.0009)  time: 0.1964  data: 0.0002  max mem: 5511
[00:10:30.690210] Epoch: [73]  [620/781]  eta: 0:00:31  lr: 0.000045  training_loss: 1.1134 (1.1710)  mae_loss: 0.0265 (0.0259)  classification_loss: 1.0862 (1.1442)  loss_mask: 0.0001 (0.0008)  time: 0.1960  data: 0.0002  max mem: 5511
[00:10:34.597971] Epoch: [73]  [640/781]  eta: 0:00:27  lr: 0.000045  training_loss: 1.1135 (1.1710)  mae_loss: 0.0261 (0.0259)  classification_loss: 1.0880 (1.1442)  loss_mask: 0.0001 (0.0008)  time: 0.1953  data: 0.0003  max mem: 5511
[00:10:38.527640] Epoch: [73]  [660/781]  eta: 0:00:23  lr: 0.000045  training_loss: 1.1760 (1.1712)  mae_loss: 0.0256 (0.0259)  classification_loss: 1.1515 (1.1444)  loss_mask: 0.0001 (0.0008)  time: 0.1964  data: 0.0002  max mem: 5511
[00:10:42.466209] Epoch: [73]  [680/781]  eta: 0:00:19  lr: 0.000045  training_loss: 1.1773 (1.1719)  mae_loss: 0.0248 (0.0259)  classification_loss: 1.1536 (1.1452)  loss_mask: 0.0001 (0.0008)  time: 0.1968  data: 0.0002  max mem: 5511
[00:10:46.390300] Epoch: [73]  [700/781]  eta: 0:00:15  lr: 0.000045  training_loss: 1.1976 (1.1729)  mae_loss: 0.0258 (0.0259)  classification_loss: 1.1701 (1.1461)  loss_mask: 0.0001 (0.0008)  time: 0.1961  data: 0.0002  max mem: 5511
[00:10:50.344786] Epoch: [73]  [720/781]  eta: 0:00:12  lr: 0.000044  training_loss: 1.1527 (1.1734)  mae_loss: 0.0260 (0.0259)  classification_loss: 1.1211 (1.1467)  loss_mask: 0.0001 (0.0008)  time: 0.1976  data: 0.0002  max mem: 5511
[00:10:54.259220] Epoch: [73]  [740/781]  eta: 0:00:08  lr: 0.000044  training_loss: 1.1547 (1.1729)  mae_loss: 0.0264 (0.0260)  classification_loss: 1.1218 (1.1462)  loss_mask: 0.0001 (0.0007)  time: 0.1956  data: 0.0002  max mem: 5511
[00:10:58.184769] Epoch: [73]  [760/781]  eta: 0:00:04  lr: 0.000044  training_loss: 1.1901 (1.1737)  mae_loss: 0.0269 (0.0260)  classification_loss: 1.1618 (1.1470)  loss_mask: 0.0001 (0.0007)  time: 0.1962  data: 0.0002  max mem: 5511
[00:11:02.076233] Epoch: [73]  [780/781]  eta: 0:00:00  lr: 0.000044  training_loss: 1.1458 (1.1736)  mae_loss: 0.0269 (0.0260)  classification_loss: 1.1183 (1.1469)  loss_mask: 0.0001 (0.0007)  time: 0.1945  data: 0.0002  max mem: 5511
[00:11:02.226010] Epoch: [73] Total time: 0:02:34 (0.1973 s / it)
[00:11:02.226542] Averaged stats: lr: 0.000044  training_loss: 1.1458 (1.1736)  mae_loss: 0.0269 (0.0260)  classification_loss: 1.1183 (1.1469)  loss_mask: 0.0001 (0.0007)
[00:11:02.868830] Test:  [  0/157]  eta: 0:01:39  testing_loss: 0.5469 (0.5469)  acc1: 81.2500 (81.2500)  acc5: 96.8750 (96.8750)  time: 0.6363  data: 0.6026  max mem: 5511
[00:11:03.181462] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5357 (0.5147)  acc1: 84.3750 (84.2330)  acc5: 100.0000 (99.0057)  time: 0.0857  data: 0.0550  max mem: 5511
[00:11:03.465683] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4541 (0.4663)  acc1: 84.3750 (86.4583)  acc5: 100.0000 (99.3304)  time: 0.0294  data: 0.0002  max mem: 5511
[00:11:03.749169] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4536 (0.4848)  acc1: 85.9375 (85.3831)  acc5: 100.0000 (99.1935)  time: 0.0282  data: 0.0002  max mem: 5511
[00:11:04.036482] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4912 (0.4883)  acc1: 85.9375 (85.2896)  acc5: 98.4375 (99.0854)  time: 0.0284  data: 0.0002  max mem: 5511
[00:11:04.330102] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4736 (0.4804)  acc1: 85.9375 (85.4779)  acc5: 100.0000 (99.1728)  time: 0.0289  data: 0.0002  max mem: 5511
[00:11:04.616970] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4494 (0.4743)  acc1: 85.9375 (85.7326)  acc5: 100.0000 (99.2059)  time: 0.0289  data: 0.0002  max mem: 5511
[00:11:04.902195] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4346 (0.4691)  acc1: 87.5000 (85.9815)  acc5: 100.0000 (99.2077)  time: 0.0285  data: 0.0002  max mem: 5511
[00:11:05.190820] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4528 (0.4763)  acc1: 85.9375 (85.7446)  acc5: 100.0000 (99.2477)  time: 0.0286  data: 0.0002  max mem: 5511
[00:11:05.475774] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4631 (0.4712)  acc1: 84.3750 (85.8516)  acc5: 100.0000 (99.2273)  time: 0.0286  data: 0.0002  max mem: 5511
[00:11:05.760156] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4652 (0.4724)  acc1: 85.9375 (85.9220)  acc5: 98.4375 (99.2110)  time: 0.0283  data: 0.0002  max mem: 5511
[00:11:06.045774] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4864 (0.4716)  acc1: 85.9375 (86.0360)  acc5: 100.0000 (99.2399)  time: 0.0284  data: 0.0002  max mem: 5511
[00:11:06.329684] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4941 (0.4728)  acc1: 85.9375 (86.0021)  acc5: 100.0000 (99.2510)  time: 0.0283  data: 0.0002  max mem: 5511
[00:11:06.622564] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4614 (0.4741)  acc1: 85.9375 (85.8898)  acc5: 100.0000 (99.2724)  time: 0.0287  data: 0.0002  max mem: 5511
[00:11:06.905117] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4477 (0.4728)  acc1: 85.9375 (85.9264)  acc5: 100.0000 (99.3019)  time: 0.0286  data: 0.0002  max mem: 5511
[00:11:07.184658] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4746 (0.4724)  acc1: 85.9375 (85.8961)  acc5: 100.0000 (99.3067)  time: 0.0280  data: 0.0001  max mem: 5511
[00:11:07.334506] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4266 (0.4722)  acc1: 87.5000 (85.8900)  acc5: 100.0000 (99.2900)  time: 0.0269  data: 0.0001  max mem: 5511
[00:11:07.472924] Test: Total time: 0:00:05 (0.0334 s / it)
[00:11:07.473398] * Acc@1 85.890 Acc@5 99.290 loss 0.472
[00:11:07.473680] Accuracy of the network on the 10000 test images: 85.9%
[00:11:07.473867] Max accuracy: 85.89%
[00:11:07.764584] log_dir: ./output_dir
[00:11:08.670560] Epoch: [74]  [  0/781]  eta: 0:11:46  lr: 0.000044  training_loss: 1.0872 (1.0872)  mae_loss: 0.0270 (0.0270)  classification_loss: 1.0601 (1.0601)  loss_mask: 0.0001 (0.0001)  time: 0.9044  data: 0.6914  max mem: 5511
[00:11:12.634988] Epoch: [74]  [ 20/781]  eta: 0:02:56  lr: 0.000044  training_loss: 1.1616 (1.1512)  mae_loss: 0.0259 (0.0263)  classification_loss: 1.1322 (1.1248)  loss_mask: 0.0001 (0.0001)  time: 0.1981  data: 0.0002  max mem: 5511
[00:11:16.582529] Epoch: [74]  [ 40/781]  eta: 0:02:39  lr: 0.000044  training_loss: 1.2194 (1.1831)  mae_loss: 0.0258 (0.0262)  classification_loss: 1.1921 (1.1568)  loss_mask: 0.0001 (0.0001)  time: 0.1973  data: 0.0002  max mem: 5511
[00:11:20.514351] Epoch: [74]  [ 60/781]  eta: 0:02:30  lr: 0.000044  training_loss: 1.2125 (1.1862)  mae_loss: 0.0250 (0.0261)  classification_loss: 1.1827 (1.1601)  loss_mask: 0.0001 (0.0001)  time: 0.1965  data: 0.0002  max mem: 5511
[00:11:24.419524] Epoch: [74]  [ 80/781]  eta: 0:02:24  lr: 0.000044  training_loss: 1.1324 (1.1792)  mae_loss: 0.0257 (0.0263)  classification_loss: 1.1098 (1.1529)  loss_mask: 0.0001 (0.0001)  time: 0.1952  data: 0.0002  max mem: 5511
[00:11:28.344212] Epoch: [74]  [100/781]  eta: 0:02:18  lr: 0.000044  training_loss: 1.1442 (1.1788)  mae_loss: 0.0268 (0.0262)  classification_loss: 1.1165 (1.1525)  loss_mask: 0.0001 (0.0001)  time: 0.1962  data: 0.0002  max mem: 5511
[00:11:32.272850] Epoch: [74]  [120/781]  eta: 0:02:13  lr: 0.000044  training_loss: 1.1458 (1.1723)  mae_loss: 0.0254 (0.0261)  classification_loss: 1.1173 (1.1460)  loss_mask: 0.0001 (0.0001)  time: 0.1963  data: 0.0002  max mem: 5511
[00:11:36.242575] Epoch: [74]  [140/781]  eta: 0:02:09  lr: 0.000044  training_loss: 1.1024 (1.1668)  mae_loss: 0.0259 (0.0262)  classification_loss: 1.0773 (1.1405)  loss_mask: 0.0001 (0.0001)  time: 0.1984  data: 0.0003  max mem: 5511
[00:11:40.154968] Epoch: [74]  [160/781]  eta: 0:02:04  lr: 0.000044  training_loss: 1.1669 (1.1650)  mae_loss: 0.0253 (0.0261)  classification_loss: 1.1404 (1.1388)  loss_mask: 0.0001 (0.0001)  time: 0.1955  data: 0.0002  max mem: 5511
[00:11:44.081033] Epoch: [74]  [180/781]  eta: 0:02:00  lr: 0.000044  training_loss: 1.1843 (1.1686)  mae_loss: 0.0258 (0.0262)  classification_loss: 1.1585 (1.1424)  loss_mask: 0.0001 (0.0001)  time: 0.1962  data: 0.0003  max mem: 5511
[00:11:48.089842] Epoch: [74]  [200/781]  eta: 0:01:56  lr: 0.000043  training_loss: 1.1840 (1.1703)  mae_loss: 0.0251 (0.0261)  classification_loss: 1.1575 (1.1441)  loss_mask: 0.0001 (0.0001)  time: 0.2003  data: 0.0002  max mem: 5511
[00:11:52.035351] Epoch: [74]  [220/781]  eta: 0:01:52  lr: 0.000043  training_loss: 1.1702 (1.1711)  mae_loss: 0.0257 (0.0260)  classification_loss: 1.1462 (1.1450)  loss_mask: 0.0001 (0.0001)  time: 0.1972  data: 0.0002  max mem: 5511
[00:11:55.951780] Epoch: [74]  [240/781]  eta: 0:01:48  lr: 0.000043  training_loss: 1.1756 (1.1717)  mae_loss: 0.0279 (0.0261)  classification_loss: 1.1518 (1.1455)  loss_mask: 0.0001 (0.0001)  time: 0.1957  data: 0.0002  max mem: 5511
[00:11:59.908937] Epoch: [74]  [260/781]  eta: 0:01:44  lr: 0.000043  training_loss: 1.1373 (1.1716)  mae_loss: 0.0240 (0.0260)  classification_loss: 1.1141 (1.1455)  loss_mask: 0.0001 (0.0001)  time: 0.1977  data: 0.0002  max mem: 5511
[00:12:03.822360] Epoch: [74]  [280/781]  eta: 0:01:39  lr: 0.000043  training_loss: 1.1961 (1.1705)  mae_loss: 0.0268 (0.0261)  classification_loss: 1.1677 (1.1444)  loss_mask: 0.0001 (0.0001)  time: 0.1956  data: 0.0002  max mem: 5511
[00:12:07.775257] Epoch: [74]  [300/781]  eta: 0:01:35  lr: 0.000043  training_loss: 1.2016 (1.1728)  mae_loss: 0.0248 (0.0260)  classification_loss: 1.1755 (1.1467)  loss_mask: 0.0001 (0.0001)  time: 0.1976  data: 0.0003  max mem: 5511
[00:12:11.726895] Epoch: [74]  [320/781]  eta: 0:01:31  lr: 0.000043  training_loss: 1.1801 (1.1744)  mae_loss: 0.0263 (0.0260)  classification_loss: 1.1548 (1.1483)  loss_mask: 0.0001 (0.0001)  time: 0.1975  data: 0.0002  max mem: 5511
[00:12:15.659826] Epoch: [74]  [340/781]  eta: 0:01:27  lr: 0.000043  training_loss: 1.1730 (1.1734)  mae_loss: 0.0253 (0.0260)  classification_loss: 1.1450 (1.1473)  loss_mask: 0.0001 (0.0001)  time: 0.1966  data: 0.0003  max mem: 5511
[00:12:19.587135] Epoch: [74]  [360/781]  eta: 0:01:23  lr: 0.000043  training_loss: 1.1832 (1.1741)  mae_loss: 0.0245 (0.0260)  classification_loss: 1.1540 (1.1480)  loss_mask: 0.0001 (0.0001)  time: 0.1963  data: 0.0002  max mem: 5511
[00:12:23.564205] Epoch: [74]  [380/781]  eta: 0:01:19  lr: 0.000043  training_loss: 1.1765 (1.1753)  mae_loss: 0.0260 (0.0260)  classification_loss: 1.1488 (1.1492)  loss_mask: 0.0001 (0.0001)  time: 0.1988  data: 0.0002  max mem: 5511
[00:12:27.488070] Epoch: [74]  [400/781]  eta: 0:01:15  lr: 0.000043  training_loss: 1.1393 (1.1730)  mae_loss: 0.0262 (0.0260)  classification_loss: 1.1131 (1.1469)  loss_mask: 0.0001 (0.0001)  time: 0.1961  data: 0.0002  max mem: 5511
[00:12:31.384422] Epoch: [74]  [420/781]  eta: 0:01:11  lr: 0.000043  training_loss: 1.1822 (1.1728)  mae_loss: 0.0255 (0.0260)  classification_loss: 1.1578 (1.1467)  loss_mask: 0.0001 (0.0001)  time: 0.1947  data: 0.0001  max mem: 5511
[00:12:35.344019] Epoch: [74]  [440/781]  eta: 0:01:07  lr: 0.000043  training_loss: 1.1669 (1.1726)  mae_loss: 0.0257 (0.0260)  classification_loss: 1.1394 (1.1465)  loss_mask: 0.0001 (0.0001)  time: 0.1979  data: 0.0004  max mem: 5511
[00:12:39.278682] Epoch: [74]  [460/781]  eta: 0:01:03  lr: 0.000042  training_loss: 1.1180 (1.1704)  mae_loss: 0.0242 (0.0259)  classification_loss: 1.0937 (1.1444)  loss_mask: 0.0001 (0.0001)  time: 0.1966  data: 0.0002  max mem: 5511
[00:12:43.215579] Epoch: [74]  [480/781]  eta: 0:00:59  lr: 0.000042  training_loss: 1.2009 (1.1710)  mae_loss: 0.0239 (0.0258)  classification_loss: 1.1716 (1.1451)  loss_mask: 0.0001 (0.0001)  time: 0.1968  data: 0.0002  max mem: 5511
[00:12:47.143589] Epoch: [74]  [500/781]  eta: 0:00:55  lr: 0.000042  training_loss: 1.1164 (1.1695)  mae_loss: 0.0258 (0.0258)  classification_loss: 1.0911 (1.1436)  loss_mask: 0.0001 (0.0001)  time: 0.1963  data: 0.0003  max mem: 5511
[00:12:51.076326] Epoch: [74]  [520/781]  eta: 0:00:51  lr: 0.000042  training_loss: 1.1351 (1.1690)  mae_loss: 0.0246 (0.0258)  classification_loss: 1.1117 (1.1431)  loss_mask: 0.0001 (0.0001)  time: 0.1966  data: 0.0003  max mem: 5511
[00:12:54.991290] Epoch: [74]  [540/781]  eta: 0:00:47  lr: 0.000042  training_loss: 1.1493 (1.1693)  mae_loss: 0.0272 (0.0258)  classification_loss: 1.1282 (1.1434)  loss_mask: 0.0001 (0.0001)  time: 0.1957  data: 0.0002  max mem: 5511
[00:12:58.929238] Epoch: [74]  [560/781]  eta: 0:00:43  lr: 0.000042  training_loss: 1.1522 (1.1694)  mae_loss: 0.0258 (0.0258)  classification_loss: 1.1279 (1.1435)  loss_mask: 0.0001 (0.0001)  time: 0.1968  data: 0.0002  max mem: 5511
[00:13:02.844663] Epoch: [74]  [580/781]  eta: 0:00:39  lr: 0.000042  training_loss: 1.1778 (1.1697)  mae_loss: 0.0264 (0.0259)  classification_loss: 1.1526 (1.1437)  loss_mask: 0.0001 (0.0001)  time: 0.1957  data: 0.0002  max mem: 5511
[00:13:06.761993] Epoch: [74]  [600/781]  eta: 0:00:35  lr: 0.000042  training_loss: 1.1795 (1.1696)  mae_loss: 0.0244 (0.0259)  classification_loss: 1.1514 (1.1437)  loss_mask: 0.0001 (0.0001)  time: 0.1958  data: 0.0003  max mem: 5511
[00:13:10.718415] Epoch: [74]  [620/781]  eta: 0:00:31  lr: 0.000042  training_loss: 1.1666 (1.1697)  mae_loss: 0.0262 (0.0259)  classification_loss: 1.1386 (1.1437)  loss_mask: 0.0001 (0.0001)  time: 0.1977  data: 0.0002  max mem: 5511
[00:13:14.646829] Epoch: [74]  [640/781]  eta: 0:00:27  lr: 0.000042  training_loss: 1.1356 (1.1699)  mae_loss: 0.0258 (0.0259)  classification_loss: 1.1137 (1.1439)  loss_mask: 0.0001 (0.0001)  time: 0.1963  data: 0.0003  max mem: 5511
[00:13:18.563404] Epoch: [74]  [660/781]  eta: 0:00:23  lr: 0.000042  training_loss: 1.1580 (1.1700)  mae_loss: 0.0260 (0.0259)  classification_loss: 1.1341 (1.1440)  loss_mask: 0.0001 (0.0001)  time: 0.1957  data: 0.0002  max mem: 5511
[00:13:22.481833] Epoch: [74]  [680/781]  eta: 0:00:19  lr: 0.000042  training_loss: 1.1536 (1.1701)  mae_loss: 0.0254 (0.0259)  classification_loss: 1.1270 (1.1441)  loss_mask: 0.0001 (0.0001)  time: 0.1958  data: 0.0003  max mem: 5511
[00:13:26.397303] Epoch: [74]  [700/781]  eta: 0:00:16  lr: 0.000041  training_loss: 1.1429 (1.1696)  mae_loss: 0.0253 (0.0259)  classification_loss: 1.1149 (1.1436)  loss_mask: 0.0001 (0.0001)  time: 0.1957  data: 0.0002  max mem: 5511
[00:13:30.314350] Epoch: [74]  [720/781]  eta: 0:00:12  lr: 0.000041  training_loss: 1.1539 (1.1691)  mae_loss: 0.0238 (0.0259)  classification_loss: 1.1298 (1.1431)  loss_mask: 0.0001 (0.0001)  time: 0.1957  data: 0.0002  max mem: 5511
[00:13:34.227838] Epoch: [74]  [740/781]  eta: 0:00:08  lr: 0.000041  training_loss: 1.1273 (1.1687)  mae_loss: 0.0259 (0.0259)  classification_loss: 1.0972 (1.1427)  loss_mask: 0.0001 (0.0001)  time: 0.1956  data: 0.0002  max mem: 5511
[00:13:38.179662] Epoch: [74]  [760/781]  eta: 0:00:04  lr: 0.000041  training_loss: 1.1682 (1.1691)  mae_loss: 0.0259 (0.0259)  classification_loss: 1.1453 (1.1431)  loss_mask: 0.0001 (0.0001)  time: 0.1975  data: 0.0003  max mem: 5511
[00:13:42.125559] Epoch: [74]  [780/781]  eta: 0:00:00  lr: 0.000041  training_loss: 1.1825 (1.1696)  mae_loss: 0.0260 (0.0259)  classification_loss: 1.1565 (1.1436)  loss_mask: 0.0001 (0.0001)  time: 0.1972  data: 0.0002  max mem: 5511
[00:13:42.278111] Epoch: [74] Total time: 0:02:34 (0.1978 s / it)
[00:13:42.278586] Averaged stats: lr: 0.000041  training_loss: 1.1825 (1.1696)  mae_loss: 0.0260 (0.0259)  classification_loss: 1.1565 (1.1436)  loss_mask: 0.0001 (0.0001)
[00:13:42.917538] Test:  [  0/157]  eta: 0:01:38  testing_loss: 0.5687 (0.5687)  acc1: 82.8125 (82.8125)  acc5: 100.0000 (100.0000)  time: 0.6258  data: 0.5963  max mem: 5511
[00:13:43.210058] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4982 (0.5177)  acc1: 84.3750 (84.2330)  acc5: 100.0000 (99.5739)  time: 0.0833  data: 0.0548  max mem: 5511
[00:13:43.497080] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4601 (0.4670)  acc1: 87.5000 (86.2351)  acc5: 100.0000 (99.4792)  time: 0.0288  data: 0.0005  max mem: 5511
[00:13:43.783096] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4601 (0.4840)  acc1: 85.9375 (85.1815)  acc5: 100.0000 (99.2440)  time: 0.0285  data: 0.0003  max mem: 5511
[00:13:44.066425] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4912 (0.4861)  acc1: 85.9375 (85.5183)  acc5: 98.4375 (99.0854)  time: 0.0283  data: 0.0003  max mem: 5511
[00:13:44.352018] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4598 (0.4786)  acc1: 87.5000 (86.0294)  acc5: 100.0000 (99.1422)  time: 0.0283  data: 0.0002  max mem: 5511
[00:13:44.638756] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4518 (0.4753)  acc1: 87.5000 (86.1168)  acc5: 100.0000 (99.1547)  time: 0.0284  data: 0.0002  max mem: 5511
[00:13:44.925267] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4695 (0.4743)  acc1: 85.9375 (86.1356)  acc5: 100.0000 (99.1417)  time: 0.0284  data: 0.0003  max mem: 5511
[00:13:45.217113] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5106 (0.4802)  acc1: 85.9375 (85.9375)  acc5: 100.0000 (99.2284)  time: 0.0287  data: 0.0002  max mem: 5511
[00:13:45.503378] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4729 (0.4762)  acc1: 85.9375 (86.1092)  acc5: 100.0000 (99.1930)  time: 0.0288  data: 0.0002  max mem: 5511
[00:13:45.795435] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4702 (0.4778)  acc1: 85.9375 (86.0149)  acc5: 98.4375 (99.2110)  time: 0.0287  data: 0.0003  max mem: 5511
[00:13:46.081550] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4872 (0.4783)  acc1: 85.9375 (85.9797)  acc5: 100.0000 (99.2117)  time: 0.0287  data: 0.0003  max mem: 5511
[00:13:46.373460] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4872 (0.4788)  acc1: 84.3750 (85.9504)  acc5: 98.4375 (99.1994)  time: 0.0288  data: 0.0002  max mem: 5511
[00:13:46.661873] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4754 (0.4807)  acc1: 84.3750 (85.7824)  acc5: 98.4375 (99.1531)  time: 0.0289  data: 0.0002  max mem: 5511
[00:13:46.946118] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4658 (0.4783)  acc1: 84.3750 (85.8932)  acc5: 100.0000 (99.1910)  time: 0.0285  data: 0.0002  max mem: 5511
[00:13:47.227014] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4702 (0.4789)  acc1: 84.3750 (85.8651)  acc5: 100.0000 (99.1722)  time: 0.0281  data: 0.0001  max mem: 5511
[00:13:47.379715] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4431 (0.4784)  acc1: 85.9375 (85.8300)  acc5: 100.0000 (99.1800)  time: 0.0271  data: 0.0001  max mem: 5511
[00:13:47.528744] Test: Total time: 0:00:05 (0.0334 s / it)
[00:13:47.529599] * Acc@1 85.830 Acc@5 99.180 loss 0.478
[00:13:47.529901] Accuracy of the network on the 10000 test images: 85.8%
[00:13:47.530094] Max accuracy: 85.89%
[00:13:48.063597] log_dir: ./output_dir
[00:13:48.954680] Epoch: [75]  [  0/781]  eta: 0:11:34  lr: 0.000041  training_loss: 1.1392 (1.1392)  mae_loss: 0.0273 (0.0273)  classification_loss: 1.1119 (1.1119)  loss_mask: 0.0001 (0.0001)  time: 0.8887  data: 0.6542  max mem: 5511
[00:13:52.909223] Epoch: [75]  [ 20/781]  eta: 0:02:55  lr: 0.000041  training_loss: 1.1356 (1.1375)  mae_loss: 0.0255 (0.0260)  classification_loss: 1.1095 (1.1113)  loss_mask: 0.0001 (0.0002)  time: 0.1976  data: 0.0002  max mem: 5511
[00:13:56.864973] Epoch: [75]  [ 40/781]  eta: 0:02:38  lr: 0.000041  training_loss: 1.1505 (1.1585)  mae_loss: 0.0247 (0.0255)  classification_loss: 1.1239 (1.1328)  loss_mask: 0.0001 (0.0001)  time: 0.1977  data: 0.0002  max mem: 5511
[00:14:00.774009] Epoch: [75]  [ 60/781]  eta: 0:02:30  lr: 0.000041  training_loss: 1.1695 (1.1584)  mae_loss: 0.0247 (0.0253)  classification_loss: 1.1488 (1.1330)  loss_mask: 0.0001 (0.0001)  time: 0.1954  data: 0.0002  max mem: 5511
[00:14:04.684851] Epoch: [75]  [ 80/781]  eta: 0:02:23  lr: 0.000041  training_loss: 1.2089 (1.1666)  mae_loss: 0.0250 (0.0253)  classification_loss: 1.1839 (1.1411)  loss_mask: 0.0001 (0.0001)  time: 0.1954  data: 0.0002  max mem: 5511
[00:14:08.603909] Epoch: [75]  [100/781]  eta: 0:02:18  lr: 0.000041  training_loss: 1.1519 (1.1674)  mae_loss: 0.0267 (0.0256)  classification_loss: 1.1207 (1.1417)  loss_mask: 0.0001 (0.0001)  time: 0.1959  data: 0.0002  max mem: 5511
[00:14:12.522412] Epoch: [75]  [120/781]  eta: 0:02:13  lr: 0.000041  training_loss: 1.1446 (1.1684)  mae_loss: 0.0260 (0.0257)  classification_loss: 1.1185 (1.1426)  loss_mask: 0.0001 (0.0001)  time: 0.1958  data: 0.0002  max mem: 5511
[00:14:16.444593] Epoch: [75]  [140/781]  eta: 0:02:08  lr: 0.000041  training_loss: 1.1428 (1.1637)  mae_loss: 0.0255 (0.0258)  classification_loss: 1.1100 (1.1378)  loss_mask: 0.0001 (0.0001)  time: 0.1960  data: 0.0002  max mem: 5511
[00:14:20.357690] Epoch: [75]  [160/781]  eta: 0:02:04  lr: 0.000041  training_loss: 1.1453 (1.1641)  mae_loss: 0.0264 (0.0258)  classification_loss: 1.1158 (1.1383)  loss_mask: 0.0000 (0.0001)  time: 0.1956  data: 0.0002  max mem: 5511
[00:14:24.295971] Epoch: [75]  [180/781]  eta: 0:02:00  lr: 0.000040  training_loss: 1.1468 (1.1630)  mae_loss: 0.0251 (0.0258)  classification_loss: 1.1193 (1.1372)  loss_mask: 0.0000 (0.0001)  time: 0.1968  data: 0.0002  max mem: 5511
[00:14:28.204821] Epoch: [75]  [200/781]  eta: 0:01:55  lr: 0.000040  training_loss: 1.1328 (1.1625)  mae_loss: 0.0251 (0.0258)  classification_loss: 1.1105 (1.1366)  loss_mask: 0.0001 (0.0001)  time: 0.1954  data: 0.0002  max mem: 5511
[00:14:32.106944] Epoch: [75]  [220/781]  eta: 0:01:51  lr: 0.000040  training_loss: 1.1828 (1.1657)  mae_loss: 0.0254 (0.0258)  classification_loss: 1.1581 (1.1398)  loss_mask: 0.0001 (0.0001)  time: 0.1950  data: 0.0002  max mem: 5511
[00:14:36.043740] Epoch: [75]  [240/781]  eta: 0:01:47  lr: 0.000040  training_loss: 1.1601 (1.1677)  mae_loss: 0.0257 (0.0258)  classification_loss: 1.1342 (1.1417)  loss_mask: 0.0001 (0.0001)  time: 0.1968  data: 0.0002  max mem: 5511
[00:14:39.954651] Epoch: [75]  [260/781]  eta: 0:01:43  lr: 0.000040  training_loss: 1.1544 (1.1679)  mae_loss: 0.0253 (0.0258)  classification_loss: 1.1291 (1.1420)  loss_mask: 0.0001 (0.0001)  time: 0.1955  data: 0.0002  max mem: 5511
[00:14:43.870191] Epoch: [75]  [280/781]  eta: 0:01:39  lr: 0.000040  training_loss: 1.1579 (1.1671)  mae_loss: 0.0257 (0.0259)  classification_loss: 1.1280 (1.1412)  loss_mask: 0.0000 (0.0001)  time: 0.1957  data: 0.0002  max mem: 5511
[00:14:47.808825] Epoch: [75]  [300/781]  eta: 0:01:35  lr: 0.000040  training_loss: 1.1664 (1.1694)  mae_loss: 0.0267 (0.0259)  classification_loss: 1.1468 (1.1434)  loss_mask: 0.0001 (0.0001)  time: 0.1968  data: 0.0003  max mem: 5511
[00:14:51.762380] Epoch: [75]  [320/781]  eta: 0:01:31  lr: 0.000040  training_loss: 1.1383 (1.1713)  mae_loss: 0.0239 (0.0258)  classification_loss: 1.1156 (1.1454)  loss_mask: 0.0001 (0.0001)  time: 0.1976  data: 0.0003  max mem: 5511
[00:14:55.665059] Epoch: [75]  [340/781]  eta: 0:01:27  lr: 0.000040  training_loss: 1.1522 (1.1692)  mae_loss: 0.0257 (0.0258)  classification_loss: 1.1237 (1.1433)  loss_mask: 0.0001 (0.0001)  time: 0.1951  data: 0.0002  max mem: 5511
[00:14:59.602220] Epoch: [75]  [360/781]  eta: 0:01:23  lr: 0.000040  training_loss: 1.1750 (1.1690)  mae_loss: 0.0267 (0.0259)  classification_loss: 1.1498 (1.1430)  loss_mask: 0.0001 (0.0001)  time: 0.1968  data: 0.0003  max mem: 5511
[00:15:03.584192] Epoch: [75]  [380/781]  eta: 0:01:19  lr: 0.000040  training_loss: 1.1527 (1.1702)  mae_loss: 0.0256 (0.0259)  classification_loss: 1.1308 (1.1442)  loss_mask: 0.0000 (0.0001)  time: 0.1990  data: 0.0002  max mem: 5511
[00:15:07.536135] Epoch: [75]  [400/781]  eta: 0:01:15  lr: 0.000040  training_loss: 1.1822 (1.1709)  mae_loss: 0.0254 (0.0259)  classification_loss: 1.1555 (1.1449)  loss_mask: 0.0001 (0.0001)  time: 0.1975  data: 0.0002  max mem: 5511
[00:15:11.454522] Epoch: [75]  [420/781]  eta: 0:01:11  lr: 0.000040  training_loss: 1.1796 (1.1715)  mae_loss: 0.0254 (0.0259)  classification_loss: 1.1496 (1.1455)  loss_mask: 0.0001 (0.0001)  time: 0.1958  data: 0.0002  max mem: 5511
[00:15:15.398670] Epoch: [75]  [440/781]  eta: 0:01:07  lr: 0.000039  training_loss: 1.1345 (1.1701)  mae_loss: 0.0253 (0.0259)  classification_loss: 1.1109 (1.1441)  loss_mask: 0.0001 (0.0001)  time: 0.1971  data: 0.0002  max mem: 5511
[00:15:19.384571] Epoch: [75]  [460/781]  eta: 0:01:03  lr: 0.000039  training_loss: 1.0707 (1.1675)  mae_loss: 0.0244 (0.0259)  classification_loss: 1.0444 (1.1416)  loss_mask: 0.0000 (0.0001)  time: 0.1992  data: 0.0003  max mem: 5511
[00:15:23.305723] Epoch: [75]  [480/781]  eta: 0:00:59  lr: 0.000039  training_loss: 1.1321 (1.1668)  mae_loss: 0.0241 (0.0258)  classification_loss: 1.1081 (1.1409)  loss_mask: 0.0001 (0.0001)  time: 0.1960  data: 0.0005  max mem: 5511
[00:15:27.220952] Epoch: [75]  [500/781]  eta: 0:00:55  lr: 0.000039  training_loss: 1.1733 (1.1667)  mae_loss: 0.0257 (0.0258)  classification_loss: 1.1366 (1.1408)  loss_mask: 0.0000 (0.0001)  time: 0.1956  data: 0.0002  max mem: 5511
[00:15:31.158570] Epoch: [75]  [520/781]  eta: 0:00:51  lr: 0.000039  training_loss: 1.1527 (1.1657)  mae_loss: 0.0251 (0.0258)  classification_loss: 1.1235 (1.1398)  loss_mask: 0.0001 (0.0001)  time: 0.1968  data: 0.0002  max mem: 5511
[00:15:35.129286] Epoch: [75]  [540/781]  eta: 0:00:47  lr: 0.000039  training_loss: 1.1403 (1.1653)  mae_loss: 0.0258 (0.0259)  classification_loss: 1.1186 (1.1393)  loss_mask: 0.0001 (0.0001)  time: 0.1984  data: 0.0002  max mem: 5511
[00:15:39.074330] Epoch: [75]  [560/781]  eta: 0:00:43  lr: 0.000039  training_loss: 1.1519 (1.1654)  mae_loss: 0.0252 (0.0259)  classification_loss: 1.1319 (1.1394)  loss_mask: 0.0000 (0.0001)  time: 0.1972  data: 0.0003  max mem: 5511
[00:15:42.992194] Epoch: [75]  [580/781]  eta: 0:00:39  lr: 0.000039  training_loss: 1.1263 (1.1648)  mae_loss: 0.0246 (0.0259)  classification_loss: 1.1040 (1.1389)  loss_mask: 0.0001 (0.0001)  time: 0.1958  data: 0.0002  max mem: 5511
[00:15:46.909794] Epoch: [75]  [600/781]  eta: 0:00:35  lr: 0.000039  training_loss: 1.1447 (1.1648)  mae_loss: 0.0247 (0.0258)  classification_loss: 1.1199 (1.1389)  loss_mask: 0.0000 (0.0001)  time: 0.1958  data: 0.0002  max mem: 5511
[00:15:50.840083] Epoch: [75]  [620/781]  eta: 0:00:31  lr: 0.000039  training_loss: 1.1621 (1.1647)  mae_loss: 0.0261 (0.0258)  classification_loss: 1.1345 (1.1388)  loss_mask: 0.0001 (0.0001)  time: 0.1964  data: 0.0002  max mem: 5511
[00:15:54.770898] Epoch: [75]  [640/781]  eta: 0:00:27  lr: 0.000039  training_loss: 1.1235 (1.1645)  mae_loss: 0.0268 (0.0259)  classification_loss: 1.0939 (1.1385)  loss_mask: 0.0001 (0.0001)  time: 0.1965  data: 0.0002  max mem: 5511
[00:15:58.687032] Epoch: [75]  [660/781]  eta: 0:00:23  lr: 0.000039  training_loss: 1.1167 (1.1635)  mae_loss: 0.0255 (0.0259)  classification_loss: 1.0890 (1.1376)  loss_mask: 0.0001 (0.0001)  time: 0.1957  data: 0.0002  max mem: 5511
[00:16:02.581829] Epoch: [75]  [680/781]  eta: 0:00:19  lr: 0.000039  training_loss: 1.2019 (1.1648)  mae_loss: 0.0271 (0.0259)  classification_loss: 1.1757 (1.1388)  loss_mask: 0.0001 (0.0001)  time: 0.1946  data: 0.0003  max mem: 5511
[00:16:06.526833] Epoch: [75]  [700/781]  eta: 0:00:15  lr: 0.000039  training_loss: 1.1647 (1.1653)  mae_loss: 0.0262 (0.0259)  classification_loss: 1.1367 (1.1393)  loss_mask: 0.0001 (0.0001)  time: 0.1972  data: 0.0002  max mem: 5511
[00:16:10.446559] Epoch: [75]  [720/781]  eta: 0:00:12  lr: 0.000038  training_loss: 1.1830 (1.1660)  mae_loss: 0.0275 (0.0260)  classification_loss: 1.1564 (1.1399)  loss_mask: 0.0001 (0.0001)  time: 0.1959  data: 0.0003  max mem: 5511
[00:16:14.354607] Epoch: [75]  [740/781]  eta: 0:00:08  lr: 0.000038  training_loss: 1.1417 (1.1653)  mae_loss: 0.0261 (0.0260)  classification_loss: 1.1171 (1.1392)  loss_mask: 0.0000 (0.0001)  time: 0.1953  data: 0.0002  max mem: 5511
[00:16:18.288470] Epoch: [75]  [760/781]  eta: 0:00:04  lr: 0.000038  training_loss: 1.1721 (1.1654)  mae_loss: 0.0250 (0.0259)  classification_loss: 1.1461 (1.1394)  loss_mask: 0.0000 (0.0001)  time: 0.1966  data: 0.0003  max mem: 5511
[00:16:22.170558] Epoch: [75]  [780/781]  eta: 0:00:00  lr: 0.000038  training_loss: 1.1653 (1.1664)  mae_loss: 0.0270 (0.0260)  classification_loss: 1.1379 (1.1404)  loss_mask: 0.0000 (0.0001)  time: 0.1940  data: 0.0002  max mem: 5511
[00:16:22.346856] Epoch: [75] Total time: 0:02:34 (0.1975 s / it)
[00:16:22.347380] Averaged stats: lr: 0.000038  training_loss: 1.1653 (1.1664)  mae_loss: 0.0270 (0.0260)  classification_loss: 1.1379 (1.1404)  loss_mask: 0.0000 (0.0001)
[00:16:22.969575] Test:  [  0/157]  eta: 0:01:36  testing_loss: 0.4934 (0.4934)  acc1: 89.0625 (89.0625)  acc5: 96.8750 (96.8750)  time: 0.6175  data: 0.5869  max mem: 5511
[00:16:23.254040] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4623 (0.4887)  acc1: 84.3750 (85.7955)  acc5: 100.0000 (99.0057)  time: 0.0817  data: 0.0535  max mem: 5511
[00:16:23.537388] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4349 (0.4396)  acc1: 87.5000 (87.4256)  acc5: 100.0000 (99.4048)  time: 0.0282  data: 0.0002  max mem: 5511
[00:16:23.829979] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4349 (0.4576)  acc1: 87.5000 (86.3407)  acc5: 100.0000 (99.3952)  time: 0.0287  data: 0.0003  max mem: 5511
[00:16:24.115843] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4758 (0.4646)  acc1: 85.9375 (86.5091)  acc5: 98.4375 (99.1616)  time: 0.0288  data: 0.0004  max mem: 5511
[00:16:24.407696] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4633 (0.4599)  acc1: 87.5000 (86.6728)  acc5: 98.4375 (99.1728)  time: 0.0287  data: 0.0002  max mem: 5511
[00:16:24.695854] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4221 (0.4571)  acc1: 85.9375 (86.6035)  acc5: 100.0000 (99.2059)  time: 0.0289  data: 0.0002  max mem: 5511
[00:16:24.978854] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4346 (0.4528)  acc1: 87.5000 (86.8398)  acc5: 100.0000 (99.2298)  time: 0.0284  data: 0.0002  max mem: 5511
[00:16:25.264043] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4549 (0.4605)  acc1: 87.5000 (86.5741)  acc5: 100.0000 (99.2670)  time: 0.0283  data: 0.0002  max mem: 5511
[00:16:25.547218] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4703 (0.4564)  acc1: 87.5000 (86.8304)  acc5: 100.0000 (99.2788)  time: 0.0282  data: 0.0002  max mem: 5511
[00:16:25.831492] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4708 (0.4605)  acc1: 87.5000 (86.7110)  acc5: 100.0000 (99.2574)  time: 0.0282  data: 0.0002  max mem: 5511
[00:16:26.114972] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4792 (0.4597)  acc1: 87.5000 (86.7680)  acc5: 100.0000 (99.2821)  time: 0.0282  data: 0.0002  max mem: 5511
[00:16:26.397671] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4504 (0.4601)  acc1: 85.9375 (86.6606)  acc5: 100.0000 (99.2769)  time: 0.0281  data: 0.0002  max mem: 5511
[00:16:26.680904] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4504 (0.4615)  acc1: 84.3750 (86.5577)  acc5: 100.0000 (99.2844)  time: 0.0282  data: 0.0002  max mem: 5511
[00:16:26.963329] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4243 (0.4607)  acc1: 85.9375 (86.5470)  acc5: 100.0000 (99.3129)  time: 0.0282  data: 0.0002  max mem: 5511
[00:16:27.244014] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4695 (0.4614)  acc1: 84.3750 (86.4652)  acc5: 100.0000 (99.2964)  time: 0.0280  data: 0.0001  max mem: 5511
[00:16:27.394960] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4431 (0.4615)  acc1: 84.3750 (86.4300)  acc5: 100.0000 (99.2800)  time: 0.0270  data: 0.0001  max mem: 5511
[00:16:27.558696] Test: Total time: 0:00:05 (0.0332 s / it)
[00:16:27.559407] * Acc@1 86.430 Acc@5 99.280 loss 0.462
[00:16:27.559706] Accuracy of the network on the 10000 test images: 86.4%
[00:16:27.559879] Max accuracy: 86.43%
[00:16:27.956836] log_dir: ./output_dir
[00:16:28.829691] Epoch: [76]  [  0/781]  eta: 0:11:19  lr: 0.000038  training_loss: 1.0166 (1.0166)  mae_loss: 0.0261 (0.0261)  classification_loss: 0.9905 (0.9905)  loss_mask: 0.0001 (0.0001)  time: 0.8703  data: 0.6315  max mem: 5511
[00:16:32.762926] Epoch: [76]  [ 20/781]  eta: 0:02:54  lr: 0.000038  training_loss: 1.1137 (1.1267)  mae_loss: 0.0261 (0.0264)  classification_loss: 1.0875 (1.0995)  loss_mask: 0.0002 (0.0008)  time: 0.1966  data: 0.0002  max mem: 5511
[00:16:36.708781] Epoch: [76]  [ 40/781]  eta: 0:02:38  lr: 0.000038  training_loss: 1.1461 (1.1438)  mae_loss: 0.0244 (0.0256)  classification_loss: 1.1213 (1.1177)  loss_mask: 0.0001 (0.0005)  time: 0.1972  data: 0.0002  max mem: 5511
[00:16:40.661768] Epoch: [76]  [ 60/781]  eta: 0:02:30  lr: 0.000038  training_loss: 1.1396 (1.1468)  mae_loss: 0.0260 (0.0258)  classification_loss: 1.1132 (1.1206)  loss_mask: 0.0001 (0.0004)  time: 0.1976  data: 0.0002  max mem: 5511
[00:16:44.606294] Epoch: [76]  [ 80/781]  eta: 0:02:23  lr: 0.000038  training_loss: 1.1289 (1.1423)  mae_loss: 0.0259 (0.0258)  classification_loss: 1.0995 (1.1162)  loss_mask: 0.0000 (0.0003)  time: 0.1971  data: 0.0002  max mem: 5511
[00:16:48.554835] Epoch: [76]  [100/781]  eta: 0:02:18  lr: 0.000038  training_loss: 1.1583 (1.1471)  mae_loss: 0.0261 (0.0260)  classification_loss: 1.1291 (1.1209)  loss_mask: 0.0001 (0.0003)  time: 0.1973  data: 0.0004  max mem: 5511
[00:16:52.506941] Epoch: [76]  [120/781]  eta: 0:02:14  lr: 0.000038  training_loss: 1.1636 (1.1529)  mae_loss: 0.0233 (0.0257)  classification_loss: 1.1386 (1.1270)  loss_mask: 0.0000 (0.0002)  time: 0.1975  data: 0.0003  max mem: 5511
[00:16:56.435053] Epoch: [76]  [140/781]  eta: 0:02:09  lr: 0.000038  training_loss: 1.1999 (1.1585)  mae_loss: 0.0261 (0.0258)  classification_loss: 1.1709 (1.1325)  loss_mask: 0.0001 (0.0002)  time: 0.1963  data: 0.0002  max mem: 5511
[00:17:00.381384] Epoch: [76]  [160/781]  eta: 0:02:04  lr: 0.000038  training_loss: 1.1335 (1.1577)  mae_loss: 0.0254 (0.0258)  classification_loss: 1.1081 (1.1317)  loss_mask: 0.0001 (0.0002)  time: 0.1972  data: 0.0002  max mem: 5511
[00:17:04.296458] Epoch: [76]  [180/781]  eta: 0:02:00  lr: 0.000038  training_loss: 1.1655 (1.1586)  mae_loss: 0.0254 (0.0257)  classification_loss: 1.1408 (1.1326)  loss_mask: 0.0001 (0.0002)  time: 0.1957  data: 0.0003  max mem: 5511
[00:17:08.244451] Epoch: [76]  [200/781]  eta: 0:01:56  lr: 0.000037  training_loss: 1.1831 (1.1599)  mae_loss: 0.0264 (0.0258)  classification_loss: 1.1545 (1.1339)  loss_mask: 0.0000 (0.0002)  time: 0.1973  data: 0.0002  max mem: 5511
[00:17:12.174497] Epoch: [76]  [220/781]  eta: 0:01:52  lr: 0.000037  training_loss: 1.1587 (1.1603)  mae_loss: 0.0251 (0.0258)  classification_loss: 1.1359 (1.1343)  loss_mask: 0.0001 (0.0002)  time: 0.1964  data: 0.0002  max mem: 5511
[00:17:16.102646] Epoch: [76]  [240/781]  eta: 0:01:48  lr: 0.000037  training_loss: 1.1621 (1.1631)  mae_loss: 0.0257 (0.0259)  classification_loss: 1.1308 (1.1371)  loss_mask: 0.0000 (0.0001)  time: 0.1963  data: 0.0002  max mem: 5511
[00:17:20.059221] Epoch: [76]  [260/781]  eta: 0:01:43  lr: 0.000037  training_loss: 1.1849 (1.1640)  mae_loss: 0.0257 (0.0259)  classification_loss: 1.1576 (1.1380)  loss_mask: 0.0000 (0.0001)  time: 0.1978  data: 0.0003  max mem: 5511
[00:17:23.967778] Epoch: [76]  [280/781]  eta: 0:01:39  lr: 0.000037  training_loss: 1.1399 (1.1627)  mae_loss: 0.0258 (0.0259)  classification_loss: 1.1119 (1.1366)  loss_mask: 0.0001 (0.0001)  time: 0.1953  data: 0.0003  max mem: 5511
[00:17:27.874121] Epoch: [76]  [300/781]  eta: 0:01:35  lr: 0.000037  training_loss: 1.1601 (1.1622)  mae_loss: 0.0265 (0.0259)  classification_loss: 1.1312 (1.1361)  loss_mask: 0.0001 (0.0001)  time: 0.1952  data: 0.0002  max mem: 5511
[00:17:31.773819] Epoch: [76]  [320/781]  eta: 0:01:31  lr: 0.000037  training_loss: 1.1300 (1.1617)  mae_loss: 0.0250 (0.0259)  classification_loss: 1.1047 (1.1357)  loss_mask: 0.0001 (0.0001)  time: 0.1949  data: 0.0002  max mem: 5511
[00:17:35.711317] Epoch: [76]  [340/781]  eta: 0:01:27  lr: 0.000037  training_loss: 1.1578 (1.1613)  mae_loss: 0.0249 (0.0258)  classification_loss: 1.1312 (1.1354)  loss_mask: 0.0001 (0.0001)  time: 0.1968  data: 0.0004  max mem: 5511
[00:17:39.648610] Epoch: [76]  [360/781]  eta: 0:01:23  lr: 0.000037  training_loss: 1.1626 (1.1630)  mae_loss: 0.0254 (0.0258)  classification_loss: 1.1360 (1.1370)  loss_mask: 0.0002 (0.0002)  time: 0.1968  data: 0.0003  max mem: 5511
[00:17:43.624845] Epoch: [76]  [380/781]  eta: 0:01:19  lr: 0.000037  training_loss: 1.1627 (1.1636)  mae_loss: 0.0258 (0.0258)  classification_loss: 1.1380 (1.1376)  loss_mask: 0.0001 (0.0002)  time: 0.1987  data: 0.0004  max mem: 5511
[00:17:47.532427] Epoch: [76]  [400/781]  eta: 0:01:15  lr: 0.000037  training_loss: 1.1537 (1.1639)  mae_loss: 0.0260 (0.0258)  classification_loss: 1.1254 (1.1378)  loss_mask: 0.0001 (0.0002)  time: 0.1953  data: 0.0003  max mem: 5511
[00:17:51.451925] Epoch: [76]  [420/781]  eta: 0:01:11  lr: 0.000037  training_loss: 1.1795 (1.1641)  mae_loss: 0.0253 (0.0258)  classification_loss: 1.1572 (1.1381)  loss_mask: 0.0001 (0.0002)  time: 0.1959  data: 0.0002  max mem: 5511
[00:17:55.382432] Epoch: [76]  [440/781]  eta: 0:01:07  lr: 0.000037  training_loss: 1.1209 (1.1627)  mae_loss: 0.0265 (0.0258)  classification_loss: 1.0926 (1.1366)  loss_mask: 0.0001 (0.0002)  time: 0.1964  data: 0.0003  max mem: 5511
[00:17:59.299906] Epoch: [76]  [460/781]  eta: 0:01:03  lr: 0.000036  training_loss: 1.0901 (1.1605)  mae_loss: 0.0244 (0.0258)  classification_loss: 1.0652 (1.1345)  loss_mask: 0.0001 (0.0002)  time: 0.1957  data: 0.0002  max mem: 5511
[00:18:03.222169] Epoch: [76]  [480/781]  eta: 0:00:59  lr: 0.000036  training_loss: 1.1716 (1.1622)  mae_loss: 0.0254 (0.0258)  classification_loss: 1.1506 (1.1362)  loss_mask: 0.0001 (0.0002)  time: 0.1960  data: 0.0003  max mem: 5511
[00:18:07.138973] Epoch: [76]  [500/781]  eta: 0:00:55  lr: 0.000036  training_loss: 1.1673 (1.1615)  mae_loss: 0.0250 (0.0258)  classification_loss: 1.1415 (1.1355)  loss_mask: 0.0001 (0.0002)  time: 0.1958  data: 0.0002  max mem: 5511
[00:18:11.066172] Epoch: [76]  [520/781]  eta: 0:00:51  lr: 0.000036  training_loss: 1.1513 (1.1619)  mae_loss: 0.0256 (0.0258)  classification_loss: 1.1257 (1.1360)  loss_mask: 0.0000 (0.0002)  time: 0.1963  data: 0.0003  max mem: 5511
[00:18:14.986322] Epoch: [76]  [540/781]  eta: 0:00:47  lr: 0.000036  training_loss: 1.1336 (1.1623)  mae_loss: 0.0262 (0.0258)  classification_loss: 1.1083 (1.1364)  loss_mask: 0.0000 (0.0002)  time: 0.1959  data: 0.0002  max mem: 5511
[00:18:18.906173] Epoch: [76]  [560/781]  eta: 0:00:43  lr: 0.000036  training_loss: 1.1624 (1.1623)  mae_loss: 0.0250 (0.0258)  classification_loss: 1.1400 (1.1364)  loss_mask: 0.0000 (0.0002)  time: 0.1959  data: 0.0002  max mem: 5511
[00:18:22.826179] Epoch: [76]  [580/781]  eta: 0:00:39  lr: 0.000036  training_loss: 1.1392 (1.1620)  mae_loss: 0.0273 (0.0258)  classification_loss: 1.1118 (1.1361)  loss_mask: 0.0000 (0.0002)  time: 0.1959  data: 0.0002  max mem: 5511
[00:18:26.747071] Epoch: [76]  [600/781]  eta: 0:00:35  lr: 0.000036  training_loss: 1.1179 (1.1603)  mae_loss: 0.0269 (0.0258)  classification_loss: 1.0898 (1.1343)  loss_mask: 0.0000 (0.0002)  time: 0.1960  data: 0.0005  max mem: 5511
[00:18:30.679157] Epoch: [76]  [620/781]  eta: 0:00:31  lr: 0.000036  training_loss: 1.1857 (1.1602)  mae_loss: 0.0259 (0.0259)  classification_loss: 1.1594 (1.1342)  loss_mask: 0.0001 (0.0002)  time: 0.1965  data: 0.0002  max mem: 5511
[00:18:34.597392] Epoch: [76]  [640/781]  eta: 0:00:27  lr: 0.000036  training_loss: 1.1200 (1.1593)  mae_loss: 0.0258 (0.0258)  classification_loss: 1.0901 (1.1333)  loss_mask: 0.0000 (0.0002)  time: 0.1958  data: 0.0002  max mem: 5511
[00:18:38.525801] Epoch: [76]  [660/781]  eta: 0:00:23  lr: 0.000036  training_loss: 1.1623 (1.1601)  mae_loss: 0.0261 (0.0258)  classification_loss: 1.1377 (1.1341)  loss_mask: 0.0000 (0.0002)  time: 0.1963  data: 0.0002  max mem: 5511
[00:18:42.455806] Epoch: [76]  [680/781]  eta: 0:00:19  lr: 0.000036  training_loss: 1.1663 (1.1607)  mae_loss: 0.0235 (0.0258)  classification_loss: 1.1350 (1.1347)  loss_mask: 0.0000 (0.0001)  time: 0.1964  data: 0.0003  max mem: 5511
[00:18:46.363850] Epoch: [76]  [700/781]  eta: 0:00:15  lr: 0.000036  training_loss: 1.1372 (1.1603)  mae_loss: 0.0259 (0.0258)  classification_loss: 1.1211 (1.1343)  loss_mask: 0.0001 (0.0001)  time: 0.1953  data: 0.0002  max mem: 5511
[00:18:50.291259] Epoch: [76]  [720/781]  eta: 0:00:12  lr: 0.000036  training_loss: 1.1846 (1.1608)  mae_loss: 0.0262 (0.0258)  classification_loss: 1.1618 (1.1349)  loss_mask: 0.0001 (0.0001)  time: 0.1963  data: 0.0002  max mem: 5511
[00:18:54.260865] Epoch: [76]  [740/781]  eta: 0:00:08  lr: 0.000035  training_loss: 1.1421 (1.1601)  mae_loss: 0.0242 (0.0258)  classification_loss: 1.1200 (1.1342)  loss_mask: 0.0000 (0.0001)  time: 0.1984  data: 0.0003  max mem: 5511
[00:18:58.196217] Epoch: [76]  [760/781]  eta: 0:00:04  lr: 0.000035  training_loss: 1.1611 (1.1599)  mae_loss: 0.0247 (0.0258)  classification_loss: 1.1377 (1.1340)  loss_mask: 0.0000 (0.0001)  time: 0.1967  data: 0.0002  max mem: 5511
[00:19:02.109386] Epoch: [76]  [780/781]  eta: 0:00:00  lr: 0.000035  training_loss: 1.1600 (1.1594)  mae_loss: 0.0248 (0.0258)  classification_loss: 1.1352 (1.1335)  loss_mask: 0.0001 (0.0002)  time: 0.1956  data: 0.0002  max mem: 5511
[00:19:02.265421] Epoch: [76] Total time: 0:02:34 (0.1976 s / it)
[00:19:02.265878] Averaged stats: lr: 0.000035  training_loss: 1.1600 (1.1594)  mae_loss: 0.0248 (0.0258)  classification_loss: 1.1352 (1.1335)  loss_mask: 0.0001 (0.0002)
[00:19:02.918048] Test:  [  0/157]  eta: 0:01:41  testing_loss: 0.5033 (0.5033)  acc1: 82.8125 (82.8125)  acc5: 100.0000 (100.0000)  time: 0.6465  data: 0.6076  max mem: 5511
[00:19:03.210029] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4654 (0.5004)  acc1: 85.9375 (85.3693)  acc5: 100.0000 (99.4318)  time: 0.0851  data: 0.0557  max mem: 5511
[00:19:03.495285] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4448 (0.4544)  acc1: 85.9375 (86.5327)  acc5: 100.0000 (99.4048)  time: 0.0286  data: 0.0003  max mem: 5511
[00:19:03.778486] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4693 (0.4693)  acc1: 85.9375 (85.6855)  acc5: 100.0000 (99.2440)  time: 0.0283  data: 0.0002  max mem: 5511
[00:19:04.063148] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4904 (0.4772)  acc1: 85.9375 (85.8232)  acc5: 98.4375 (99.0854)  time: 0.0283  data: 0.0002  max mem: 5511
[00:19:04.344878] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4716 (0.4695)  acc1: 87.5000 (86.1520)  acc5: 100.0000 (99.1115)  time: 0.0282  data: 0.0002  max mem: 5511
[00:19:04.628899] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4246 (0.4660)  acc1: 85.9375 (86.2449)  acc5: 100.0000 (99.1803)  time: 0.0282  data: 0.0002  max mem: 5511
[00:19:04.911086] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4300 (0.4608)  acc1: 87.5000 (86.5317)  acc5: 100.0000 (99.1857)  time: 0.0282  data: 0.0002  max mem: 5511
[00:19:05.194023] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4535 (0.4673)  acc1: 87.5000 (86.2269)  acc5: 100.0000 (99.2477)  time: 0.0281  data: 0.0002  max mem: 5511
[00:19:05.474795] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4731 (0.4627)  acc1: 85.9375 (86.4183)  acc5: 100.0000 (99.2102)  time: 0.0281  data: 0.0001  max mem: 5511
[00:19:05.758135] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4710 (0.4679)  acc1: 85.9375 (86.2624)  acc5: 98.4375 (99.1801)  time: 0.0281  data: 0.0002  max mem: 5511
[00:19:06.039679] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5094 (0.4687)  acc1: 85.9375 (86.2050)  acc5: 100.0000 (99.2258)  time: 0.0281  data: 0.0002  max mem: 5511
[00:19:06.321226] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4646 (0.4689)  acc1: 85.9375 (86.2345)  acc5: 100.0000 (99.2123)  time: 0.0280  data: 0.0002  max mem: 5511
[00:19:06.604111] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4447 (0.4694)  acc1: 85.9375 (86.1880)  acc5: 98.4375 (99.2009)  time: 0.0281  data: 0.0002  max mem: 5511
[00:19:06.886735] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4447 (0.4675)  acc1: 85.9375 (86.2699)  acc5: 100.0000 (99.2243)  time: 0.0281  data: 0.0002  max mem: 5511
[00:19:07.166784] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4551 (0.4683)  acc1: 85.9375 (86.2376)  acc5: 100.0000 (99.2239)  time: 0.0280  data: 0.0001  max mem: 5511
[00:19:07.317280] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4393 (0.4678)  acc1: 85.9375 (86.2000)  acc5: 100.0000 (99.2300)  time: 0.0270  data: 0.0001  max mem: 5511
[00:19:07.475451] Test: Total time: 0:00:05 (0.0332 s / it)
[00:19:07.475912] * Acc@1 86.200 Acc@5 99.230 loss 0.468
[00:19:07.476384] Accuracy of the network on the 10000 test images: 86.2%
[00:19:07.476572] Max accuracy: 86.43%
[00:19:07.886561] log_dir: ./output_dir
[00:19:08.788960] Epoch: [77]  [  0/781]  eta: 0:11:43  lr: 0.000035  training_loss: 1.1559 (1.1559)  mae_loss: 0.0246 (0.0246)  classification_loss: 1.1303 (1.1303)  loss_mask: 0.0010 (0.0010)  time: 0.9004  data: 0.6813  max mem: 5511
[00:19:12.724403] Epoch: [77]  [ 20/781]  eta: 0:02:55  lr: 0.000035  training_loss: 1.1553 (1.1334)  mae_loss: 0.0251 (0.0254)  classification_loss: 1.1310 (1.1077)  loss_mask: 0.0001 (0.0002)  time: 0.1967  data: 0.0002  max mem: 5511
[00:19:16.632196] Epoch: [77]  [ 40/781]  eta: 0:02:37  lr: 0.000035  training_loss: 1.1760 (1.1488)  mae_loss: 0.0263 (0.0258)  classification_loss: 1.1491 (1.1229)  loss_mask: 0.0001 (0.0002)  time: 0.1953  data: 0.0002  max mem: 5511
[00:19:20.559883] Epoch: [77]  [ 60/781]  eta: 0:02:29  lr: 0.000035  training_loss: 1.1667 (1.1523)  mae_loss: 0.0263 (0.0260)  classification_loss: 1.1443 (1.1262)  loss_mask: 0.0001 (0.0001)  time: 0.1963  data: 0.0002  max mem: 5511
[00:19:24.478192] Epoch: [77]  [ 80/781]  eta: 0:02:23  lr: 0.000035  training_loss: 1.1588 (1.1534)  mae_loss: 0.0271 (0.0261)  classification_loss: 1.1327 (1.1272)  loss_mask: 0.0001 (0.0002)  time: 0.1958  data: 0.0002  max mem: 5511
[00:19:28.434470] Epoch: [77]  [100/781]  eta: 0:02:18  lr: 0.000035  training_loss: 1.1553 (1.1576)  mae_loss: 0.0243 (0.0259)  classification_loss: 1.1288 (1.1315)  loss_mask: 0.0001 (0.0001)  time: 0.1977  data: 0.0002  max mem: 5511

[00:19:32.337080] Epoch: [77]  [120/781]  eta: 0:02:13  lr: 0.000035  training_loss: 1.1850 (1.1620)  mae_loss: 0.0247 (0.0258)  classification_loss: 1.1544 (1.1361)  loss_mask: 0.0001 (0.0001)  time: 0.1950  data: 0.0001  max mem: 5511
[00:19:36.264902] Epoch: [77]  [140/781]  eta: 0:02:08  lr: 0.000035  training_loss: 1.1237 (1.1595)  mae_loss: 0.0253 (0.0258)  classification_loss: 1.0990 (1.1336)  loss_mask: 0.0001 (0.0001)  time: 0.1963  data: 0.0002  max mem: 5511
[00:19:40.195339] Epoch: [77]  [160/781]  eta: 0:02:04  lr: 0.000035  training_loss: 1.1735 (1.1601)  mae_loss: 0.0250 (0.0257)  classification_loss: 1.1478 (1.1342)  loss_mask: 0.0001 (0.0001)  time: 0.1964  data: 0.0002  max mem: 5511
[00:19:44.106867] Epoch: [77]  [180/781]  eta: 0:02:00  lr: 0.000035  training_loss: 1.1642 (1.1612)  mae_loss: 0.0255 (0.0257)  classification_loss: 1.1393 (1.1354)  loss_mask: 0.0001 (0.0001)  time: 0.1954  data: 0.0003  max mem: 5511
[00:19:48.077212] Epoch: [77]  [200/781]  eta: 0:01:56  lr: 0.000035  training_loss: 1.1241 (1.1578)  mae_loss: 0.0265 (0.0258)  classification_loss: 1.1033 (1.1318)  loss_mask: 0.0001 (0.0002)  time: 0.1984  data: 0.0002  max mem: 5511
[00:19:52.025990] Epoch: [77]  [220/781]  eta: 0:01:51  lr: 0.000035  training_loss: 1.1183 (1.1548)  mae_loss: 0.0245 (0.0257)  classification_loss: 1.0935 (1.1289)  loss_mask: 0.0001 (0.0002)  time: 0.1973  data: 0.0003  max mem: 5511
[00:19:55.974395] Epoch: [77]  [240/781]  eta: 0:01:47  lr: 0.000034  training_loss: 1.1485 (1.1555)  mae_loss: 0.0245 (0.0257)  classification_loss: 1.1238 (1.1296)  loss_mask: 0.0001 (0.0002)  time: 0.1973  data: 0.0002  max mem: 5511
[00:19:59.908610] Epoch: [77]  [260/781]  eta: 0:01:43  lr: 0.000034  training_loss: 1.1523 (1.1565)  mae_loss: 0.0263 (0.0257)  classification_loss: 1.1229 (1.1307)  loss_mask: 0.0001 (0.0001)  time: 0.1966  data: 0.0002  max mem: 5511
[00:20:03.867943] Epoch: [77]  [280/781]  eta: 0:01:39  lr: 0.000034  training_loss: 1.1371 (1.1551)  mae_loss: 0.0255 (0.0258)  classification_loss: 1.1076 (1.1292)  loss_mask: 0.0000 (0.0001)  time: 0.1979  data: 0.0002  max mem: 5511
[00:20:07.841826] Epoch: [77]  [300/781]  eta: 0:01:35  lr: 0.000034  training_loss: 1.1666 (1.1573)  mae_loss: 0.0256 (0.0258)  classification_loss: 1.1425 (1.1314)  loss_mask: 0.0000 (0.0001)  time: 0.1986  data: 0.0002  max mem: 5511
[00:20:11.782887] Epoch: [77]  [320/781]  eta: 0:01:31  lr: 0.000034  training_loss: 1.1011 (1.1563)  mae_loss: 0.0251 (0.0258)  classification_loss: 1.0719 (1.1304)  loss_mask: 0.0000 (0.0001)  time: 0.1970  data: 0.0002  max mem: 5511
[00:20:15.694575] Epoch: [77]  [340/781]  eta: 0:01:27  lr: 0.000034  training_loss: 1.1325 (1.1554)  mae_loss: 0.0262 (0.0259)  classification_loss: 1.0978 (1.1294)  loss_mask: 0.0001 (0.0001)  time: 0.1955  data: 0.0002  max mem: 5511
[00:20:19.601601] Epoch: [77]  [360/781]  eta: 0:01:23  lr: 0.000034  training_loss: 1.1548 (1.1560)  mae_loss: 0.0256 (0.0258)  classification_loss: 1.1321 (1.1301)  loss_mask: 0.0000 (0.0001)  time: 0.1953  data: 0.0002  max mem: 5511
[00:20:23.524503] Epoch: [77]  [380/781]  eta: 0:01:19  lr: 0.000034  training_loss: 1.1776 (1.1576)  mae_loss: 0.0252 (0.0258)  classification_loss: 1.1523 (1.1316)  loss_mask: 0.0001 (0.0001)  time: 0.1960  data: 0.0002  max mem: 5511
[00:20:27.440412] Epoch: [77]  [400/781]  eta: 0:01:15  lr: 0.000034  training_loss: 1.1655 (1.1574)  mae_loss: 0.0249 (0.0258)  classification_loss: 1.1391 (1.1314)  loss_mask: 0.0001 (0.0001)  time: 0.1957  data: 0.0002  max mem: 5511
[00:20:31.346774] Epoch: [77]  [420/781]  eta: 0:01:11  lr: 0.000034  training_loss: 1.1542 (1.1577)  mae_loss: 0.0244 (0.0258)  classification_loss: 1.1255 (1.1318)  loss_mask: 0.0000 (0.0001)  time: 0.1952  data: 0.0002  max mem: 5511
[00:20:35.255790] Epoch: [77]  [440/781]  eta: 0:01:07  lr: 0.000034  training_loss: 1.1257 (1.1563)  mae_loss: 0.0266 (0.0258)  classification_loss: 1.0990 (1.1303)  loss_mask: 0.0000 (0.0001)  time: 0.1954  data: 0.0007  max mem: 5511
[00:20:39.145377] Epoch: [77]  [460/781]  eta: 0:01:03  lr: 0.000034  training_loss: 1.1595 (1.1562)  mae_loss: 0.0255 (0.0258)  classification_loss: 1.1329 (1.1303)  loss_mask: 0.0000 (0.0001)  time: 0.1944  data: 0.0002  max mem: 5511
[00:20:43.045674] Epoch: [77]  [480/781]  eta: 0:00:59  lr: 0.000034  training_loss: 1.1501 (1.1555)  mae_loss: 0.0263 (0.0259)  classification_loss: 1.1261 (1.1295)  loss_mask: 0.0000 (0.0001)  time: 0.1949  data: 0.0002  max mem: 5511
[00:20:46.999643] Epoch: [77]  [500/781]  eta: 0:00:55  lr: 0.000034  training_loss: 1.2076 (1.1573)  mae_loss: 0.0266 (0.0259)  classification_loss: 1.1828 (1.1313)  loss_mask: 0.0001 (0.0001)  time: 0.1976  data: 0.0003  max mem: 5511
[00:20:50.951219] Epoch: [77]  [520/781]  eta: 0:00:51  lr: 0.000033  training_loss: 1.1352 (1.1571)  mae_loss: 0.0261 (0.0259)  classification_loss: 1.1130 (1.1310)  loss_mask: 0.0001 (0.0001)  time: 0.1975  data: 0.0002  max mem: 5511
[00:20:54.890105] Epoch: [77]  [540/781]  eta: 0:00:47  lr: 0.000033  training_loss: 1.1830 (1.1585)  mae_loss: 0.0254 (0.0259)  classification_loss: 1.1546 (1.1325)  loss_mask: 0.0001 (0.0001)  time: 0.1969  data: 0.0002  max mem: 5511
[00:20:58.807265] Epoch: [77]  [560/781]  eta: 0:00:43  lr: 0.000033  training_loss: 1.1344 (1.1579)  mae_loss: 0.0255 (0.0259)  classification_loss: 1.1083 (1.1319)  loss_mask: 0.0000 (0.0001)  time: 0.1958  data: 0.0002  max mem: 5511
[00:21:02.730405] Epoch: [77]  [580/781]  eta: 0:00:39  lr: 0.000033  training_loss: 1.1363 (1.1575)  mae_loss: 0.0254 (0.0259)  classification_loss: 1.1134 (1.1315)  loss_mask: 0.0001 (0.0001)  time: 0.1961  data: 0.0002  max mem: 5511
[00:21:06.640331] Epoch: [77]  [600/781]  eta: 0:00:35  lr: 0.000033  training_loss: 1.1067 (1.1565)  mae_loss: 0.0251 (0.0259)  classification_loss: 1.0816 (1.1306)  loss_mask: 0.0000 (0.0001)  time: 0.1954  data: 0.0002  max mem: 5511
[00:21:10.564757] Epoch: [77]  [620/781]  eta: 0:00:31  lr: 0.000033  training_loss: 1.1290 (1.1566)  mae_loss: 0.0264 (0.0259)  classification_loss: 1.1025 (1.1306)  loss_mask: 0.0000 (0.0001)  time: 0.1961  data: 0.0003  max mem: 5511
[00:21:14.489072] Epoch: [77]  [640/781]  eta: 0:00:27  lr: 0.000033  training_loss: 1.1049 (1.1557)  mae_loss: 0.0257 (0.0259)  classification_loss: 1.0746 (1.1297)  loss_mask: 0.0001 (0.0001)  time: 0.1961  data: 0.0002  max mem: 5511
[00:21:18.415220] Epoch: [77]  [660/781]  eta: 0:00:23  lr: 0.000033  training_loss: 1.1591 (1.1564)  mae_loss: 0.0235 (0.0258)  classification_loss: 1.1311 (1.1304)  loss_mask: 0.0001 (0.0001)  time: 0.1962  data: 0.0002  max mem: 5511
[00:21:22.339399] Epoch: [77]  [680/781]  eta: 0:00:19  lr: 0.000033  training_loss: 1.1408 (1.1560)  mae_loss: 0.0250 (0.0258)  classification_loss: 1.1161 (1.1301)  loss_mask: 0.0001 (0.0001)  time: 0.1960  data: 0.0002  max mem: 5511
[00:21:26.263569] Epoch: [77]  [700/781]  eta: 0:00:15  lr: 0.000033  training_loss: 1.1089 (1.1562)  mae_loss: 0.0250 (0.0258)  classification_loss: 1.0839 (1.1302)  loss_mask: 0.0001 (0.0001)  time: 0.1961  data: 0.0003  max mem: 5511
[00:21:30.214804] Epoch: [77]  [720/781]  eta: 0:00:12  lr: 0.000033  training_loss: 1.1846 (1.1568)  mae_loss: 0.0240 (0.0258)  classification_loss: 1.1586 (1.1308)  loss_mask: 0.0000 (0.0001)  time: 0.1975  data: 0.0002  max mem: 5511
[00:21:34.138464] Epoch: [77]  [740/781]  eta: 0:00:08  lr: 0.000033  training_loss: 1.1177 (1.1565)  mae_loss: 0.0272 (0.0259)  classification_loss: 1.0866 (1.1305)  loss_mask: 0.0000 (0.0001)  time: 0.1961  data: 0.0002  max mem: 5511
[00:21:38.056781] Epoch: [77]  [760/781]  eta: 0:00:04  lr: 0.000033  training_loss: 1.1534 (1.1568)  mae_loss: 0.0260 (0.0259)  classification_loss: 1.1306 (1.1307)  loss_mask: 0.0001 (0.0001)  time: 0.1958  data: 0.0002  max mem: 5511
[00:21:41.966067] Epoch: [77]  [780/781]  eta: 0:00:00  lr: 0.000033  training_loss: 1.1622 (1.1567)  mae_loss: 0.0248 (0.0259)  classification_loss: 1.1348 (1.1307)  loss_mask: 0.0000 (0.0001)  time: 0.1954  data: 0.0002  max mem: 5511
[00:21:42.128045] Epoch: [77] Total time: 0:02:34 (0.1975 s / it)
[00:21:42.128760] Averaged stats: lr: 0.000033  training_loss: 1.1622 (1.1567)  mae_loss: 0.0248 (0.0259)  classification_loss: 1.1348 (1.1307)  loss_mask: 0.0000 (0.0001)
[00:21:42.766445] Test:  [  0/157]  eta: 0:01:38  testing_loss: 0.5790 (0.5790)  acc1: 82.8125 (82.8125)  acc5: 98.4375 (98.4375)  time: 0.6292  data: 0.5967  max mem: 5511
[00:21:43.052121] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4875 (0.5004)  acc1: 84.3750 (84.8011)  acc5: 100.0000 (99.2898)  time: 0.0828  data: 0.0544  max mem: 5511
[00:21:43.337908] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4377 (0.4552)  acc1: 85.9375 (86.7560)  acc5: 100.0000 (99.4792)  time: 0.0282  data: 0.0002  max mem: 5511
[00:21:43.623561] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4291 (0.4634)  acc1: 85.9375 (86.0887)  acc5: 100.0000 (99.3448)  time: 0.0283  data: 0.0002  max mem: 5511
[00:21:43.906053] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4770 (0.4667)  acc1: 85.9375 (86.3948)  acc5: 100.0000 (99.2378)  time: 0.0283  data: 0.0002  max mem: 5511
[00:21:44.190177] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4667 (0.4603)  acc1: 87.5000 (86.6422)  acc5: 100.0000 (99.1728)  time: 0.0282  data: 0.0002  max mem: 5511
[00:21:44.481518] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4276 (0.4564)  acc1: 87.5000 (86.7316)  acc5: 100.0000 (99.2059)  time: 0.0286  data: 0.0002  max mem: 5511
[00:21:44.770138] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4006 (0.4505)  acc1: 87.5000 (86.9938)  acc5: 100.0000 (99.2518)  time: 0.0289  data: 0.0002  max mem: 5511
[00:21:45.052836] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4650 (0.4575)  acc1: 87.5000 (86.6319)  acc5: 100.0000 (99.3056)  time: 0.0284  data: 0.0002  max mem: 5511
[00:21:45.336731] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4693 (0.4536)  acc1: 84.3750 (86.7445)  acc5: 100.0000 (99.2617)  time: 0.0282  data: 0.0002  max mem: 5511
[00:21:45.621572] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4695 (0.4575)  acc1: 85.9375 (86.6337)  acc5: 98.4375 (99.2265)  time: 0.0282  data: 0.0002  max mem: 5511
[00:21:45.904227] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4886 (0.4575)  acc1: 85.9375 (86.6413)  acc5: 100.0000 (99.2680)  time: 0.0282  data: 0.0002  max mem: 5511
[00:21:46.189645] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4834 (0.4572)  acc1: 87.5000 (86.6736)  acc5: 100.0000 (99.3027)  time: 0.0282  data: 0.0002  max mem: 5511
[00:21:46.474247] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4766 (0.4582)  acc1: 85.9375 (86.5697)  acc5: 100.0000 (99.3082)  time: 0.0283  data: 0.0002  max mem: 5511
[00:21:46.758330] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4459 (0.4569)  acc1: 85.9375 (86.6578)  acc5: 100.0000 (99.3351)  time: 0.0283  data: 0.0002  max mem: 5511
[00:21:47.039072] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4572 (0.4577)  acc1: 85.9375 (86.6722)  acc5: 100.0000 (99.3377)  time: 0.0281  data: 0.0001  max mem: 5511
[00:21:47.195177] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4433 (0.4576)  acc1: 87.5000 (86.6400)  acc5: 100.0000 (99.3300)  time: 0.0273  data: 0.0001  max mem: 5511
[00:21:47.343540] Test: Total time: 0:00:05 (0.0332 s / it)
[00:21:47.344005] * Acc@1 86.640 Acc@5 99.330 loss 0.458
[00:21:47.344298] Accuracy of the network on the 10000 test images: 86.6%
[00:21:47.344473] Max accuracy: 86.64%
[00:21:47.582261] log_dir: ./output_dir
[00:21:48.433719] Epoch: [78]  [  0/781]  eta: 0:11:03  lr: 0.000033  training_loss: 1.0602 (1.0602)  mae_loss: 0.0257 (0.0257)  classification_loss: 1.0344 (1.0344)  loss_mask: 0.0001 (0.0001)  time: 0.8498  data: 0.6266  max mem: 5511
[00:21:52.398704] Epoch: [78]  [ 20/781]  eta: 0:02:54  lr: 0.000032  training_loss: 1.1485 (1.1315)  mae_loss: 0.0265 (0.0262)  classification_loss: 1.1177 (1.1048)  loss_mask: 0.0000 (0.0005)  time: 0.1982  data: 0.0002  max mem: 5511
[00:21:56.312285] Epoch: [78]  [ 40/781]  eta: 0:02:37  lr: 0.000032  training_loss: 1.1432 (1.1450)  mae_loss: 0.0263 (0.0263)  classification_loss: 1.1157 (1.1180)  loss_mask: 0.0003 (0.0007)  time: 0.1956  data: 0.0002  max mem: 5511
[00:22:00.247592] Epoch: [78]  [ 60/781]  eta: 0:02:29  lr: 0.000032  training_loss: 1.1360 (1.1547)  mae_loss: 0.0272 (0.0266)  classification_loss: 1.1123 (1.1276)  loss_mask: 0.0001 (0.0005)  time: 0.1966  data: 0.0002  max mem: 5511
[00:22:04.167807] Epoch: [78]  [ 80/781]  eta: 0:02:23  lr: 0.000032  training_loss: 1.1860 (1.1620)  mae_loss: 0.0248 (0.0263)  classification_loss: 1.1589 (1.1353)  loss_mask: 0.0001 (0.0004)  time: 0.1959  data: 0.0002  max mem: 5511
[00:22:08.100407] Epoch: [78]  [100/781]  eta: 0:02:18  lr: 0.000032  training_loss: 1.1821 (1.1650)  mae_loss: 0.0245 (0.0261)  classification_loss: 1.1580 (1.1385)  loss_mask: 0.0001 (0.0004)  time: 0.1966  data: 0.0002  max mem: 5511
[00:22:12.107477] Epoch: [78]  [120/781]  eta: 0:02:13  lr: 0.000032  training_loss: 1.1075 (1.1597)  mae_loss: 0.0246 (0.0260)  classification_loss: 1.0799 (1.1334)  loss_mask: 0.0001 (0.0003)  time: 0.2003  data: 0.0002  max mem: 5511
[00:22:16.037600] Epoch: [78]  [140/781]  eta: 0:02:09  lr: 0.000032  training_loss: 1.1106 (1.1542)  mae_loss: 0.0260 (0.0261)  classification_loss: 1.0776 (1.1279)  loss_mask: 0.0000 (0.0003)  time: 0.1964  data: 0.0002  max mem: 5511
[00:22:19.959042] Epoch: [78]  [160/781]  eta: 0:02:04  lr: 0.000032  training_loss: 1.1403 (1.1540)  mae_loss: 0.0264 (0.0261)  classification_loss: 1.1143 (1.1277)  loss_mask: 0.0000 (0.0003)  time: 0.1960  data: 0.0002  max mem: 5511
[00:22:23.885761] Epoch: [78]  [180/781]  eta: 0:02:00  lr: 0.000032  training_loss: 1.1717 (1.1557)  mae_loss: 0.0250 (0.0260)  classification_loss: 1.1458 (1.1295)  loss_mask: 0.0000 (0.0002)  time: 0.1963  data: 0.0002  max mem: 5511
[00:22:27.793908] Epoch: [78]  [200/781]  eta: 0:01:56  lr: 0.000032  training_loss: 1.1993 (1.1567)  mae_loss: 0.0252 (0.0260)  classification_loss: 1.1713 (1.1305)  loss_mask: 0.0000 (0.0002)  time: 0.1953  data: 0.0002  max mem: 5511
[00:22:31.723530] Epoch: [78]  [220/781]  eta: 0:01:51  lr: 0.000032  training_loss: 1.1289 (1.1566)  mae_loss: 0.0247 (0.0259)  classification_loss: 1.1054 (1.1306)  loss_mask: 0.0000 (0.0002)  time: 0.1964  data: 0.0002  max mem: 5511
[00:22:35.645940] Epoch: [78]  [240/781]  eta: 0:01:47  lr: 0.000032  training_loss: 1.1641 (1.1585)  mae_loss: 0.0276 (0.0259)  classification_loss: 1.1402 (1.1324)  loss_mask: 0.0000 (0.0002)  time: 0.1960  data: 0.0002  max mem: 5511
[00:22:39.568725] Epoch: [78]  [260/781]  eta: 0:01:43  lr: 0.000032  training_loss: 1.1367 (1.1583)  mae_loss: 0.0268 (0.0259)  classification_loss: 1.1127 (1.1322)  loss_mask: 0.0000 (0.0002)  time: 0.1961  data: 0.0002  max mem: 5511
[00:22:43.503798] Epoch: [78]  [280/781]  eta: 0:01:39  lr: 0.000032  training_loss: 1.1422 (1.1592)  mae_loss: 0.0268 (0.0260)  classification_loss: 1.1176 (1.1330)  loss_mask: 0.0001 (0.0002)  time: 0.1967  data: 0.0002  max mem: 5511
[00:22:47.436791] Epoch: [78]  [300/781]  eta: 0:01:35  lr: 0.000031  training_loss: 1.1534 (1.1594)  mae_loss: 0.0256 (0.0260)  classification_loss: 1.1259 (1.1332)  loss_mask: 0.0001 (0.0002)  time: 0.1966  data: 0.0002  max mem: 5511
[00:22:51.395327] Epoch: [78]  [320/781]  eta: 0:01:31  lr: 0.000031  training_loss: 1.1154 (1.1586)  mae_loss: 0.0255 (0.0260)  classification_loss: 1.0947 (1.1324)  loss_mask: 0.0001 (0.0002)  time: 0.1978  data: 0.0002  max mem: 5511
[00:22:55.322625] Epoch: [78]  [340/781]  eta: 0:01:27  lr: 0.000031  training_loss: 1.0999 (1.1558)  mae_loss: 0.0246 (0.0260)  classification_loss: 1.0689 (1.1297)  loss_mask: 0.0000 (0.0002)  time: 0.1963  data: 0.0002  max mem: 5511
[00:22:59.277041] Epoch: [78]  [360/781]  eta: 0:01:23  lr: 0.000031  training_loss: 1.1996 (1.1564)  mae_loss: 0.0255 (0.0259)  classification_loss: 1.1746 (1.1303)  loss_mask: 0.0001 (0.0002)  time: 0.1976  data: 0.0002  max mem: 5511
[00:23:03.203886] Epoch: [78]  [380/781]  eta: 0:01:19  lr: 0.000031  training_loss: 1.1473 (1.1568)  mae_loss: 0.0257 (0.0260)  classification_loss: 1.1250 (1.1307)  loss_mask: 0.0000 (0.0002)  time: 0.1963  data: 0.0002  max mem: 5511
[00:23:07.134095] Epoch: [78]  [400/781]  eta: 0:01:15  lr: 0.000031  training_loss: 1.1204 (1.1567)  mae_loss: 0.0265 (0.0260)  classification_loss: 1.1020 (1.1306)  loss_mask: 0.0001 (0.0002)  time: 0.1964  data: 0.0002  max mem: 5511
[00:23:11.057404] Epoch: [78]  [420/781]  eta: 0:01:11  lr: 0.000031  training_loss: 1.1381 (1.1561)  mae_loss: 0.0261 (0.0260)  classification_loss: 1.1104 (1.1300)  loss_mask: 0.0001 (0.0002)  time: 0.1961  data: 0.0003  max mem: 5511
[00:23:14.981880] Epoch: [78]  [440/781]  eta: 0:01:07  lr: 0.000031  training_loss: 1.1426 (1.1576)  mae_loss: 0.0252 (0.0260)  classification_loss: 1.1124 (1.1314)  loss_mask: 0.0000 (0.0001)  time: 0.1961  data: 0.0002  max mem: 5511
[00:23:18.920473] Epoch: [78]  [460/781]  eta: 0:01:03  lr: 0.000031  training_loss: 1.1049 (1.1560)  mae_loss: 0.0245 (0.0259)  classification_loss: 1.0791 (1.1300)  loss_mask: 0.0000 (0.0001)  time: 0.1968  data: 0.0002  max mem: 5511
[00:23:22.839463] Epoch: [78]  [480/781]  eta: 0:00:59  lr: 0.000031  training_loss: 1.1041 (1.1553)  mae_loss: 0.0247 (0.0259)  classification_loss: 1.0763 (1.1293)  loss_mask: 0.0000 (0.0001)  time: 0.1959  data: 0.0002  max mem: 5511
[00:23:26.740693] Epoch: [78]  [500/781]  eta: 0:00:55  lr: 0.000031  training_loss: 1.1366 (1.1548)  mae_loss: 0.0258 (0.0259)  classification_loss: 1.1106 (1.1288)  loss_mask: 0.0000 (0.0001)  time: 0.1950  data: 0.0002  max mem: 5511
[00:23:30.638451] Epoch: [78]  [520/781]  eta: 0:00:51  lr: 0.000031  training_loss: 1.1332 (1.1544)  mae_loss: 0.0271 (0.0259)  classification_loss: 1.1133 (1.1283)  loss_mask: 0.0000 (0.0001)  time: 0.1948  data: 0.0002  max mem: 5511
[00:23:34.531314] Epoch: [78]  [540/781]  eta: 0:00:47  lr: 0.000031  training_loss: 1.1547 (1.1545)  mae_loss: 0.0242 (0.0259)  classification_loss: 1.1293 (1.1285)  loss_mask: 0.0001 (0.0001)  time: 0.1946  data: 0.0002  max mem: 5511
[00:23:38.482936] Epoch: [78]  [560/781]  eta: 0:00:43  lr: 0.000031  training_loss: 1.1833 (1.1545)  mae_loss: 0.0246 (0.0259)  classification_loss: 1.1609 (1.1285)  loss_mask: 0.0000 (0.0001)  time: 0.1975  data: 0.0003  max mem: 5511
[00:23:42.409263] Epoch: [78]  [580/781]  eta: 0:00:39  lr: 0.000031  training_loss: 1.1946 (1.1552)  mae_loss: 0.0253 (0.0259)  classification_loss: 1.1711 (1.1292)  loss_mask: 0.0000 (0.0001)  time: 0.1962  data: 0.0002  max mem: 5511
[00:23:46.369474] Epoch: [78]  [600/781]  eta: 0:00:35  lr: 0.000030  training_loss: 1.1338 (1.1546)  mae_loss: 0.0257 (0.0259)  classification_loss: 1.1050 (1.1287)  loss_mask: 0.0000 (0.0001)  time: 0.1979  data: 0.0002  max mem: 5511
[00:23:50.302593] Epoch: [78]  [620/781]  eta: 0:00:31  lr: 0.000030  training_loss: 1.1140 (1.1531)  mae_loss: 0.0262 (0.0259)  classification_loss: 1.0894 (1.1270)  loss_mask: 0.0001 (0.0001)  time: 0.1966  data: 0.0002  max mem: 5511
[00:23:54.207693] Epoch: [78]  [640/781]  eta: 0:00:27  lr: 0.000030  training_loss: 1.1408 (1.1527)  mae_loss: 0.0252 (0.0259)  classification_loss: 1.1161 (1.1267)  loss_mask: 0.0001 (0.0001)  time: 0.1952  data: 0.0003  max mem: 5511
[00:23:58.192084] Epoch: [78]  [660/781]  eta: 0:00:23  lr: 0.000030  training_loss: 1.1905 (1.1535)  mae_loss: 0.0251 (0.0259)  classification_loss: 1.1650 (1.1275)  loss_mask: 0.0000 (0.0001)  time: 0.1991  data: 0.0003  max mem: 5511
[00:24:02.111658] Epoch: [78]  [680/781]  eta: 0:00:19  lr: 0.000030  training_loss: 1.2138 (1.1554)  mae_loss: 0.0256 (0.0259)  classification_loss: 1.1884 (1.1294)  loss_mask: 0.0000 (0.0001)  time: 0.1959  data: 0.0002  max mem: 5511
[00:24:06.011910] Epoch: [78]  [700/781]  eta: 0:00:15  lr: 0.000030  training_loss: 1.1686 (1.1550)  mae_loss: 0.0261 (0.0259)  classification_loss: 1.1405 (1.1289)  loss_mask: 0.0001 (0.0001)  time: 0.1949  data: 0.0002  max mem: 5511
[00:24:09.927158] Epoch: [78]  [720/781]  eta: 0:00:12  lr: 0.000030  training_loss: 1.1404 (1.1547)  mae_loss: 0.0250 (0.0259)  classification_loss: 1.1141 (1.1287)  loss_mask: 0.0003 (0.0002)  time: 0.1957  data: 0.0002  max mem: 5511
[00:24:13.841233] Epoch: [78]  [740/781]  eta: 0:00:08  lr: 0.000030  training_loss: 1.0906 (1.1530)  mae_loss: 0.0254 (0.0259)  classification_loss: 1.0604 (1.1269)  loss_mask: 0.0001 (0.0002)  time: 0.1956  data: 0.0002  max mem: 5511
[00:24:17.765228] Epoch: [78]  [760/781]  eta: 0:00:04  lr: 0.000030  training_loss: 1.1903 (1.1541)  mae_loss: 0.0246 (0.0258)  classification_loss: 1.1656 (1.1281)  loss_mask: 0.0001 (0.0002)  time: 0.1961  data: 0.0002  max mem: 5511
[00:24:21.695104] Epoch: [78]  [780/781]  eta: 0:00:00  lr: 0.000030  training_loss: 1.1536 (1.1542)  mae_loss: 0.0247 (0.0258)  classification_loss: 1.1250 (1.1282)  loss_mask: 0.0000 (0.0002)  time: 0.1964  data: 0.0002  max mem: 5511
[00:24:21.844498] Epoch: [78] Total time: 0:02:34 (0.1975 s / it)
[00:24:21.845102] Averaged stats: lr: 0.000030  training_loss: 1.1536 (1.1542)  mae_loss: 0.0247 (0.0258)  classification_loss: 1.1250 (1.1282)  loss_mask: 0.0000 (0.0002)
[00:24:22.522040] Test:  [  0/157]  eta: 0:01:45  testing_loss: 0.5256 (0.5256)  acc1: 85.9375 (85.9375)  acc5: 100.0000 (100.0000)  time: 0.6722  data: 0.6418  max mem: 5511
[00:24:22.809537] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4531 (0.4853)  acc1: 85.9375 (85.5114)  acc5: 100.0000 (99.4318)  time: 0.0869  data: 0.0586  max mem: 5511
[00:24:23.094202] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4214 (0.4384)  acc1: 87.5000 (87.8720)  acc5: 100.0000 (99.4792)  time: 0.0284  data: 0.0002  max mem: 5511
[00:24:23.378604] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4403 (0.4554)  acc1: 87.5000 (86.9960)  acc5: 100.0000 (99.3952)  time: 0.0283  data: 0.0002  max mem: 5511
[00:24:23.661573] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4616 (0.4621)  acc1: 87.5000 (86.8902)  acc5: 100.0000 (99.2378)  time: 0.0282  data: 0.0002  max mem: 5511
[00:24:23.944520] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4529 (0.4578)  acc1: 87.5000 (87.0711)  acc5: 100.0000 (99.3260)  time: 0.0282  data: 0.0002  max mem: 5511
[00:24:24.225192] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4188 (0.4556)  acc1: 87.5000 (87.1414)  acc5: 100.0000 (99.3596)  time: 0.0281  data: 0.0002  max mem: 5511
[00:24:24.507719] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4310 (0.4507)  acc1: 87.5000 (87.3019)  acc5: 100.0000 (99.3838)  time: 0.0280  data: 0.0002  max mem: 5511
[00:24:24.789412] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4573 (0.4573)  acc1: 85.9375 (86.9792)  acc5: 100.0000 (99.3827)  time: 0.0281  data: 0.0002  max mem: 5511
[00:24:25.071950] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4571 (0.4538)  acc1: 85.9375 (87.1394)  acc5: 100.0000 (99.3304)  time: 0.0281  data: 0.0002  max mem: 5511
[00:24:25.354068] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4571 (0.4572)  acc1: 85.9375 (87.0668)  acc5: 98.4375 (99.2884)  time: 0.0281  data: 0.0002  max mem: 5511
[00:24:25.638038] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4968 (0.4573)  acc1: 85.9375 (87.0495)  acc5: 98.4375 (99.2821)  time: 0.0282  data: 0.0002  max mem: 5511
[00:24:25.925246] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4529 (0.4569)  acc1: 85.9375 (86.9835)  acc5: 100.0000 (99.2898)  time: 0.0284  data: 0.0002  max mem: 5511
[00:24:26.209689] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4410 (0.4575)  acc1: 85.9375 (86.9633)  acc5: 100.0000 (99.2844)  time: 0.0284  data: 0.0002  max mem: 5511
[00:24:26.495976] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4497 (0.4570)  acc1: 85.9375 (86.9681)  acc5: 100.0000 (99.3129)  time: 0.0284  data: 0.0002  max mem: 5511
[00:24:26.775777] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4620 (0.4583)  acc1: 85.9375 (86.9205)  acc5: 100.0000 (99.2860)  time: 0.0282  data: 0.0001  max mem: 5511
[00:24:26.927080] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4273 (0.4573)  acc1: 85.9375 (86.8700)  acc5: 100.0000 (99.2900)  time: 0.0271  data: 0.0001  max mem: 5511
[00:24:27.088177] Test: Total time: 0:00:05 (0.0334 s / it)
[00:24:27.089038] * Acc@1 86.870 Acc@5 99.290 loss 0.457
[00:24:27.089347] Accuracy of the network on the 10000 test images: 86.9%
[00:24:27.089568] Max accuracy: 86.87%
[00:24:27.247369] log_dir: ./output_dir
[00:24:28.070376] Epoch: [79]  [  0/781]  eta: 0:10:41  lr: 0.000030  training_loss: 0.9669 (0.9669)  mae_loss: 0.0255 (0.0255)  classification_loss: 0.9414 (0.9414)  loss_mask: 0.0000 (0.0000)  time: 0.8214  data: 0.6112  max mem: 5511
[00:24:32.015689] Epoch: [79]  [ 20/781]  eta: 0:02:52  lr: 0.000030  training_loss: 1.0784 (1.0965)  mae_loss: 0.0252 (0.0259)  classification_loss: 1.0589 (1.0706)  loss_mask: 0.0001 (0.0001)  time: 0.1972  data: 0.0002  max mem: 5511
[00:24:35.933425] Epoch: [79]  [ 40/781]  eta: 0:02:36  lr: 0.000030  training_loss: 1.1100 (1.1152)  mae_loss: 0.0265 (0.0261)  classification_loss: 1.0829 (1.0889)  loss_mask: 0.0000 (0.0003)  time: 0.1958  data: 0.0002  max mem: 5511
[00:24:39.854621] Epoch: [79]  [ 60/781]  eta: 0:02:28  lr: 0.000030  training_loss: 1.1272 (1.1190)  mae_loss: 0.0248 (0.0259)  classification_loss: 1.1004 (1.0929)  loss_mask: 0.0002 (0.0002)  time: 0.1960  data: 0.0002  max mem: 5511
[00:24:43.791903] Epoch: [79]  [ 80/781]  eta: 0:02:23  lr: 0.000030  training_loss: 1.1172 (1.1154)  mae_loss: 0.0265 (0.0260)  classification_loss: 1.0922 (1.0892)  loss_mask: 0.0001 (0.0002)  time: 0.1968  data: 0.0003  max mem: 5511
[00:24:47.712676] Epoch: [79]  [100/781]  eta: 0:02:17  lr: 0.000029  training_loss: 1.1466 (1.1249)  mae_loss: 0.0254 (0.0259)  classification_loss: 1.1210 (1.0988)  loss_mask: 0.0001 (0.0002)  time: 0.1959  data: 0.0002  max mem: 5511
[00:24:51.642050] Epoch: [79]  [120/781]  eta: 0:02:13  lr: 0.000029  training_loss: 1.1375 (1.1287)  mae_loss: 0.0251 (0.0257)  classification_loss: 1.1071 (1.1028)  loss_mask: 0.0001 (0.0002)  time: 0.1964  data: 0.0002  max mem: 5511
[00:24:55.567446] Epoch: [79]  [140/781]  eta: 0:02:08  lr: 0.000029  training_loss: 1.1376 (1.1263)  mae_loss: 0.0263 (0.0258)  classification_loss: 1.1104 (1.1004)  loss_mask: 0.0000 (0.0002)  time: 0.1962  data: 0.0002  max mem: 5511
[00:24:59.478845] Epoch: [79]  [160/781]  eta: 0:02:04  lr: 0.000029  training_loss: 1.1519 (1.1279)  mae_loss: 0.0253 (0.0257)  classification_loss: 1.1243 (1.1021)  loss_mask: 0.0001 (0.0002)  time: 0.1955  data: 0.0002  max mem: 5511
[00:25:03.408201] Epoch: [79]  [180/781]  eta: 0:02:00  lr: 0.000029  training_loss: 1.1490 (1.1311)  mae_loss: 0.0264 (0.0257)  classification_loss: 1.1223 (1.1052)  loss_mask: 0.0001 (0.0001)  time: 0.1964  data: 0.0002  max mem: 5511
[00:25:07.345078] Epoch: [79]  [200/781]  eta: 0:01:55  lr: 0.000029  training_loss: 1.1447 (1.1335)  mae_loss: 0.0253 (0.0257)  classification_loss: 1.1194 (1.1076)  loss_mask: 0.0000 (0.0001)  time: 0.1967  data: 0.0002  max mem: 5511
[00:25:11.256200] Epoch: [79]  [220/781]  eta: 0:01:51  lr: 0.000029  training_loss: 1.1547 (1.1365)  mae_loss: 0.0245 (0.0257)  classification_loss: 1.1240 (1.1107)  loss_mask: 0.0001 (0.0002)  time: 0.1955  data: 0.0002  max mem: 5511
[00:25:15.179702] Epoch: [79]  [240/781]  eta: 0:01:47  lr: 0.000029  training_loss: 1.1309 (1.1367)  mae_loss: 0.0256 (0.0257)  classification_loss: 1.1079 (1.1109)  loss_mask: 0.0001 (0.0002)  time: 0.1960  data: 0.0002  max mem: 5511
[00:25:19.083615] Epoch: [79]  [260/781]  eta: 0:01:43  lr: 0.000029  training_loss: 1.1064 (1.1374)  mae_loss: 0.0241 (0.0257)  classification_loss: 1.0825 (1.1116)  loss_mask: 0.0000 (0.0001)  time: 0.1951  data: 0.0002  max mem: 5511
[00:25:22.998490] Epoch: [79]  [280/781]  eta: 0:01:39  lr: 0.000029  training_loss: 1.1292 (1.1384)  mae_loss: 0.0262 (0.0257)  classification_loss: 1.1084 (1.1126)  loss_mask: 0.0000 (0.0001)  time: 0.1957  data: 0.0002  max mem: 5511
[00:25:26.917069] Epoch: [79]  [300/781]  eta: 0:01:35  lr: 0.000029  training_loss: 1.1450 (1.1395)  mae_loss: 0.0238 (0.0256)  classification_loss: 1.1185 (1.1138)  loss_mask: 0.0000 (0.0001)  time: 0.1958  data: 0.0001  max mem: 5511
[00:25:30.829014] Epoch: [79]  [320/781]  eta: 0:01:31  lr: 0.000029  training_loss: 1.1830 (1.1421)  mae_loss: 0.0273 (0.0257)  classification_loss: 1.1496 (1.1163)  loss_mask: 0.0001 (0.0001)  time: 0.1955  data: 0.0002  max mem: 5511
[00:25:34.776367] Epoch: [79]  [340/781]  eta: 0:01:27  lr: 0.000029  training_loss: 1.1600 (1.1435)  mae_loss: 0.0252 (0.0257)  classification_loss: 1.1333 (1.1177)  loss_mask: 0.0000 (0.0001)  time: 0.1973  data: 0.0003  max mem: 5511
[00:25:38.679981] Epoch: [79]  [360/781]  eta: 0:01:23  lr: 0.000029  training_loss: 1.1292 (1.1443)  mae_loss: 0.0259 (0.0257)  classification_loss: 1.1032 (1.1185)  loss_mask: 0.0000 (0.0001)  time: 0.1951  data: 0.0003  max mem: 5511
[00:25:42.632097] Epoch: [79]  [380/781]  eta: 0:01:19  lr: 0.000029  training_loss: 1.1299 (1.1460)  mae_loss: 0.0248 (0.0256)  classification_loss: 1.1117 (1.1202)  loss_mask: 0.0000 (0.0001)  time: 0.1975  data: 0.0002  max mem: 5511
[00:25:46.555233] Epoch: [79]  [400/781]  eta: 0:01:15  lr: 0.000028  training_loss: 1.1574 (1.1469)  mae_loss: 0.0253 (0.0257)  classification_loss: 1.1280 (1.1210)  loss_mask: 0.0001 (0.0001)  time: 0.1961  data: 0.0008  max mem: 5511
[00:25:50.475843] Epoch: [79]  [420/781]  eta: 0:01:11  lr: 0.000028  training_loss: 1.1618 (1.1473)  mae_loss: 0.0263 (0.0257)  classification_loss: 1.1316 (1.1215)  loss_mask: 0.0000 (0.0001)  time: 0.1959  data: 0.0002  max mem: 5511
[00:25:54.423408] Epoch: [79]  [440/781]  eta: 0:01:07  lr: 0.000028  training_loss: 1.1715 (1.1488)  mae_loss: 0.0266 (0.0257)  classification_loss: 1.1451 (1.1229)  loss_mask: 0.0000 (0.0001)  time: 0.1973  data: 0.0002  max mem: 5511
[00:25:58.347640] Epoch: [79]  [460/781]  eta: 0:01:03  lr: 0.000028  training_loss: 1.0956 (1.1473)  mae_loss: 0.0249 (0.0257)  classification_loss: 1.0734 (1.1215)  loss_mask: 0.0000 (0.0001)  time: 0.1961  data: 0.0003  max mem: 5511
[00:26:02.274380] Epoch: [79]  [480/781]  eta: 0:00:59  lr: 0.000028  training_loss: 1.1469 (1.1480)  mae_loss: 0.0248 (0.0257)  classification_loss: 1.1235 (1.1221)  loss_mask: 0.0000 (0.0001)  time: 0.1962  data: 0.0002  max mem: 5511
[00:26:06.215637] Epoch: [79]  [500/781]  eta: 0:00:55  lr: 0.000028  training_loss: 1.1702 (1.1495)  mae_loss: 0.0258 (0.0257)  classification_loss: 1.1309 (1.1236)  loss_mask: 0.0000 (0.0001)  time: 0.1970  data: 0.0002  max mem: 5511
[00:26:10.130460] Epoch: [79]  [520/781]  eta: 0:00:51  lr: 0.000028  training_loss: 1.1346 (1.1493)  mae_loss: 0.0238 (0.0257)  classification_loss: 1.1056 (1.1234)  loss_mask: 0.0000 (0.0001)  time: 0.1957  data: 0.0002  max mem: 5511
[00:26:14.093111] Epoch: [79]  [540/781]  eta: 0:00:47  lr: 0.000028  training_loss: 1.1048 (1.1483)  mae_loss: 0.0250 (0.0257)  classification_loss: 1.0771 (1.1224)  loss_mask: 0.0001 (0.0001)  time: 0.1980  data: 0.0002  max mem: 5511
[00:26:18.007252] Epoch: [79]  [560/781]  eta: 0:00:43  lr: 0.000028  training_loss: 1.1435 (1.1486)  mae_loss: 0.0264 (0.0257)  classification_loss: 1.1224 (1.1227)  loss_mask: 0.0001 (0.0001)  time: 0.1956  data: 0.0002  max mem: 5511
[00:26:21.987580] Epoch: [79]  [580/781]  eta: 0:00:39  lr: 0.000028  training_loss: 1.1572 (1.1495)  mae_loss: 0.0246 (0.0257)  classification_loss: 1.1295 (1.1237)  loss_mask: 0.0001 (0.0001)  time: 0.1989  data: 0.0002  max mem: 5511
[00:26:25.907554] Epoch: [79]  [600/781]  eta: 0:00:35  lr: 0.000028  training_loss: 1.1592 (1.1494)  mae_loss: 0.0249 (0.0257)  classification_loss: 1.1309 (1.1235)  loss_mask: 0.0000 (0.0001)  time: 0.1959  data: 0.0002  max mem: 5511
[00:26:29.828694] Epoch: [79]  [620/781]  eta: 0:00:31  lr: 0.000028  training_loss: 1.1395 (1.1487)  mae_loss: 0.0247 (0.0257)  classification_loss: 1.1115 (1.1229)  loss_mask: 0.0000 (0.0001)  time: 0.1960  data: 0.0002  max mem: 5511
[00:26:33.717916] Epoch: [79]  [640/781]  eta: 0:00:27  lr: 0.000028  training_loss: 1.1244 (1.1484)  mae_loss: 0.0255 (0.0257)  classification_loss: 1.0995 (1.1226)  loss_mask: 0.0000 (0.0001)  time: 0.1944  data: 0.0002  max mem: 5511
[00:26:37.692497] Epoch: [79]  [660/781]  eta: 0:00:23  lr: 0.000028  training_loss: 1.1013 (1.1475)  mae_loss: 0.0253 (0.0257)  classification_loss: 1.0757 (1.1216)  loss_mask: 0.0000 (0.0001)  time: 0.1987  data: 0.0002  max mem: 5511
[00:26:41.598005] Epoch: [79]  [680/781]  eta: 0:00:19  lr: 0.000028  training_loss: 1.1656 (1.1484)  mae_loss: 0.0250 (0.0257)  classification_loss: 1.1406 (1.1225)  loss_mask: 0.0001 (0.0001)  time: 0.1952  data: 0.0002  max mem: 5511
[00:26:45.499248] Epoch: [79]  [700/781]  eta: 0:00:15  lr: 0.000028  training_loss: 1.1373 (1.1484)  mae_loss: 0.0250 (0.0257)  classification_loss: 1.1142 (1.1225)  loss_mask: 0.0001 (0.0001)  time: 0.1950  data: 0.0002  max mem: 5511
[00:26:49.408376] Epoch: [79]  [720/781]  eta: 0:00:12  lr: 0.000027  training_loss: 1.1294 (1.1477)  mae_loss: 0.0249 (0.0257)  classification_loss: 1.1044 (1.1218)  loss_mask: 0.0001 (0.0001)  time: 0.1954  data: 0.0002  max mem: 5511
[00:26:53.344777] Epoch: [79]  [740/781]  eta: 0:00:08  lr: 0.000027  training_loss: 1.1280 (1.1483)  mae_loss: 0.0259 (0.0257)  classification_loss: 1.1044 (1.1224)  loss_mask: 0.0001 (0.0002)  time: 0.1967  data: 0.0003  max mem: 5511
[00:26:57.269475] Epoch: [79]  [760/781]  eta: 0:00:04  lr: 0.000027  training_loss: 1.1431 (1.1489)  mae_loss: 0.0246 (0.0257)  classification_loss: 1.1161 (1.1230)  loss_mask: 0.0000 (0.0002)  time: 0.1962  data: 0.0002  max mem: 5511
[00:27:01.184279] Epoch: [79]  [780/781]  eta: 0:00:00  lr: 0.000027  training_loss: 1.1409 (1.1489)  mae_loss: 0.0255 (0.0257)  classification_loss: 1.1108 (1.1231)  loss_mask: 0.0001 (0.0002)  time: 0.1957  data: 0.0002  max mem: 5511
[00:27:01.325547] Epoch: [79] Total time: 0:02:34 (0.1973 s / it)
[00:27:01.326003] Averaged stats: lr: 0.000027  training_loss: 1.1409 (1.1489)  mae_loss: 0.0255 (0.0257)  classification_loss: 1.1108 (1.1231)  loss_mask: 0.0001 (0.0002)
[00:27:01.965162] Test:  [  0/157]  eta: 0:01:39  testing_loss: 0.5318 (0.5318)  acc1: 82.8125 (82.8125)  acc5: 100.0000 (100.0000)  time: 0.6328  data: 0.6033  max mem: 5511
[00:27:02.257608] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4957 (0.4939)  acc1: 84.3750 (85.2273)  acc5: 100.0000 (99.2898)  time: 0.0839  data: 0.0550  max mem: 5511
[00:27:02.544606] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4364 (0.4477)  acc1: 87.5000 (87.3512)  acc5: 100.0000 (99.4048)  time: 0.0288  data: 0.0002  max mem: 5511
[00:27:02.830660] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4291 (0.4577)  acc1: 87.5000 (86.2399)  acc5: 100.0000 (99.3448)  time: 0.0285  data: 0.0002  max mem: 5511
[00:27:03.114363] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4303 (0.4653)  acc1: 85.9375 (86.2805)  acc5: 100.0000 (99.3140)  time: 0.0283  data: 0.0002  max mem: 5511
[00:27:03.397490] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4293 (0.4599)  acc1: 87.5000 (86.6115)  acc5: 100.0000 (99.3566)  time: 0.0282  data: 0.0002  max mem: 5511
[00:27:03.678509] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4048 (0.4576)  acc1: 87.5000 (86.6291)  acc5: 100.0000 (99.3340)  time: 0.0281  data: 0.0002  max mem: 5511
[00:27:03.960857] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4163 (0.4507)  acc1: 89.0625 (86.8838)  acc5: 100.0000 (99.3618)  time: 0.0280  data: 0.0002  max mem: 5511
[00:27:04.243529] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4492 (0.4554)  acc1: 85.9375 (86.6512)  acc5: 100.0000 (99.3827)  time: 0.0281  data: 0.0002  max mem: 5511
[00:27:04.527438] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4711 (0.4504)  acc1: 85.9375 (86.8647)  acc5: 100.0000 (99.3132)  time: 0.0282  data: 0.0002  max mem: 5511
[00:27:04.807831] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4704 (0.4545)  acc1: 85.9375 (86.7729)  acc5: 98.4375 (99.2420)  time: 0.0281  data: 0.0002  max mem: 5511
[00:27:05.091416] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4769 (0.4547)  acc1: 85.9375 (86.7399)  acc5: 98.4375 (99.2539)  time: 0.0281  data: 0.0002  max mem: 5511
[00:27:05.378380] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4769 (0.4550)  acc1: 85.9375 (86.6994)  acc5: 100.0000 (99.2769)  time: 0.0284  data: 0.0002  max mem: 5511
[00:27:05.669387] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4754 (0.4554)  acc1: 84.3750 (86.6531)  acc5: 100.0000 (99.2963)  time: 0.0287  data: 0.0002  max mem: 5511
[00:27:05.950615] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4488 (0.4558)  acc1: 85.9375 (86.6356)  acc5: 100.0000 (99.3129)  time: 0.0285  data: 0.0002  max mem: 5511
[00:27:06.229901] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4549 (0.4561)  acc1: 85.9375 (86.6411)  acc5: 100.0000 (99.3067)  time: 0.0279  data: 0.0001  max mem: 5511
[00:27:06.380986] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4438 (0.4546)  acc1: 85.9375 (86.6600)  acc5: 100.0000 (99.3100)  time: 0.0270  data: 0.0001  max mem: 5511
[00:27:06.521044] Test: Total time: 0:00:05 (0.0331 s / it)
[00:27:06.521603] * Acc@1 86.660 Acc@5 99.310 loss 0.455
[00:27:06.521908] Accuracy of the network on the 10000 test images: 86.7%
[00:27:06.522089] Max accuracy: 86.87%
[00:27:06.741842] log_dir: ./output_dir
[00:27:07.548687] Epoch: [80]  [  0/781]  eta: 0:10:28  lr: 0.000027  training_loss: 1.0682 (1.0682)  mae_loss: 0.0261 (0.0261)  classification_loss: 1.0421 (1.0421)  loss_mask: 0.0000 (0.0000)  time: 0.8051  data: 0.6023  max mem: 5511
[00:27:11.483065] Epoch: [80]  [ 20/781]  eta: 0:02:51  lr: 0.000027  training_loss: 1.1386 (1.1434)  mae_loss: 0.0249 (0.0254)  classification_loss: 1.1137 (1.1180)  loss_mask: 0.0000 (0.0001)  time: 0.1966  data: 0.0002  max mem: 5511
[00:27:15.453133] Epoch: [80]  [ 40/781]  eta: 0:02:37  lr: 0.000027  training_loss: 1.1468 (1.1484)  mae_loss: 0.0255 (0.0251)  classification_loss: 1.1188 (1.1232)  loss_mask: 0.0000 (0.0000)  time: 0.1984  data: 0.0002  max mem: 5511
[00:27:19.372682] Epoch: [80]  [ 60/781]  eta: 0:02:29  lr: 0.000027  training_loss: 1.1158 (1.1551)  mae_loss: 0.0256 (0.0255)  classification_loss: 1.0843 (1.1296)  loss_mask: 0.0000 (0.0000)  time: 0.1959  data: 0.0002  max mem: 5511
[00:27:23.352873] Epoch: [80]  [ 80/781]  eta: 0:02:23  lr: 0.000027  training_loss: 1.1037 (1.1472)  mae_loss: 0.0255 (0.0254)  classification_loss: 1.0764 (1.1217)  loss_mask: 0.0000 (0.0000)  time: 0.1989  data: 0.0002  max mem: 5511
[00:27:27.287653] Epoch: [80]  [100/781]  eta: 0:02:18  lr: 0.000027  training_loss: 1.1117 (1.1423)  mae_loss: 0.0255 (0.0254)  classification_loss: 1.0890 (1.1169)  loss_mask: 0.0000 (0.0001)  time: 0.1967  data: 0.0002  max mem: 5511
[00:27:31.205616] Epoch: [80]  [120/781]  eta: 0:02:13  lr: 0.000027  training_loss: 1.1371 (1.1439)  mae_loss: 0.0254 (0.0255)  classification_loss: 1.1148 (1.1184)  loss_mask: 0.0000 (0.0000)  time: 0.1958  data: 0.0003  max mem: 5511
[00:27:35.118248] Epoch: [80]  [140/781]  eta: 0:02:08  lr: 0.000027  training_loss: 1.1008 (1.1456)  mae_loss: 0.0250 (0.0255)  classification_loss: 1.0786 (1.1201)  loss_mask: 0.0000 (0.0000)  time: 0.1956  data: 0.0002  max mem: 5511

[00:27:39.031359] Epoch: [80]  [160/781]  eta: 0:02:04  lr: 0.000027  training_loss: 1.1359 (1.1435)  mae_loss: 0.0255 (0.0256)  classification_loss: 1.1108 (1.1179)  loss_mask: 0.0001 (0.0001)  time: 0.1956  data: 0.0003  max mem: 5511
[00:27:42.955538] Epoch: [80]  [180/781]  eta: 0:02:00  lr: 0.000027  training_loss: 1.1279 (1.1441)  mae_loss: 0.0247 (0.0254)  classification_loss: 1.1036 (1.1186)  loss_mask: 0.0000 (0.0001)  time: 0.1961  data: 0.0002  max mem: 5511
[00:27:46.877397] Epoch: [80]  [200/781]  eta: 0:01:55  lr: 0.000027  training_loss: 1.0693 (1.1394)  mae_loss: 0.0250 (0.0254)  classification_loss: 1.0460 (1.1140)  loss_mask: 0.0000 (0.0000)  time: 0.1960  data: 0.0003  max mem: 5511
[00:27:50.816777] Epoch: [80]  [220/781]  eta: 0:01:51  lr: 0.000027  training_loss: 1.1068 (1.1376)  mae_loss: 0.0245 (0.0253)  classification_loss: 1.0768 (1.1122)  loss_mask: 0.0000 (0.0000)  time: 0.1969  data: 0.0002  max mem: 5511
[00:27:54.730107] Epoch: [80]  [240/781]  eta: 0:01:47  lr: 0.000026  training_loss: 1.1049 (1.1373)  mae_loss: 0.0264 (0.0254)  classification_loss: 1.0825 (1.1119)  loss_mask: 0.0000 (0.0000)  time: 0.1956  data: 0.0002  max mem: 5511
[00:27:58.650245] Epoch: [80]  [260/781]  eta: 0:01:43  lr: 0.000026  training_loss: 1.1350 (1.1400)  mae_loss: 0.0242 (0.0254)  classification_loss: 1.1044 (1.1146)  loss_mask: 0.0000 (0.0000)  time: 0.1959  data: 0.0003  max mem: 5511
[00:28:02.568983] Epoch: [80]  [280/781]  eta: 0:01:39  lr: 0.000026  training_loss: 1.1176 (1.1395)  mae_loss: 0.0248 (0.0254)  classification_loss: 1.0937 (1.1141)  loss_mask: 0.0000 (0.0000)  time: 0.1959  data: 0.0002  max mem: 5511
[00:28:06.542862] Epoch: [80]  [300/781]  eta: 0:01:35  lr: 0.000026  training_loss: 1.1310 (1.1402)  mae_loss: 0.0245 (0.0254)  classification_loss: 1.1006 (1.1147)  loss_mask: 0.0000 (0.0000)  time: 0.1986  data: 0.0004  max mem: 5511
[00:28:10.465927] Epoch: [80]  [320/781]  eta: 0:01:31  lr: 0.000026  training_loss: 1.2052 (1.1416)  mae_loss: 0.0261 (0.0254)  classification_loss: 1.1738 (1.1162)  loss_mask: 0.0000 (0.0000)  time: 0.1961  data: 0.0002  max mem: 5511
[00:28:14.449296] Epoch: [80]  [340/781]  eta: 0:01:27  lr: 0.000026  training_loss: 1.1142 (1.1407)  mae_loss: 0.0248 (0.0254)  classification_loss: 1.0917 (1.1152)  loss_mask: 0.0000 (0.0000)  time: 0.1991  data: 0.0002  max mem: 5511
[00:28:18.376277] Epoch: [80]  [360/781]  eta: 0:01:23  lr: 0.000026  training_loss: 1.0955 (1.1393)  mae_loss: 0.0250 (0.0254)  classification_loss: 1.0681 (1.1138)  loss_mask: 0.0000 (0.0001)  time: 0.1962  data: 0.0003  max mem: 5511
[00:28:22.285220] Epoch: [80]  [380/781]  eta: 0:01:19  lr: 0.000026  training_loss: 1.0858 (1.1381)  mae_loss: 0.0242 (0.0254)  classification_loss: 1.0629 (1.1126)  loss_mask: 0.0000 (0.0001)  time: 0.1953  data: 0.0002  max mem: 5511
[00:28:26.203319] Epoch: [80]  [400/781]  eta: 0:01:15  lr: 0.000026  training_loss: 1.1366 (1.1376)  mae_loss: 0.0261 (0.0254)  classification_loss: 1.1101 (1.1121)  loss_mask: 0.0001 (0.0001)  time: 0.1958  data: 0.0002  max mem: 5511
[00:28:30.117847] Epoch: [80]  [420/781]  eta: 0:01:11  lr: 0.000026  training_loss: 1.1387 (1.1381)  mae_loss: 0.0271 (0.0255)  classification_loss: 1.1159 (1.1125)  loss_mask: 0.0000 (0.0001)  time: 0.1956  data: 0.0002  max mem: 5511
[00:28:34.050828] Epoch: [80]  [440/781]  eta: 0:01:07  lr: 0.000026  training_loss: 1.1414 (1.1386)  mae_loss: 0.0257 (0.0255)  classification_loss: 1.1148 (1.1130)  loss_mask: 0.0000 (0.0001)  time: 0.1966  data: 0.0002  max mem: 5511
[00:28:37.980450] Epoch: [80]  [460/781]  eta: 0:01:03  lr: 0.000026  training_loss: 1.1288 (1.1392)  mae_loss: 0.0256 (0.0255)  classification_loss: 1.1033 (1.1136)  loss_mask: 0.0000 (0.0001)  time: 0.1964  data: 0.0002  max mem: 5511
[00:28:41.884335] Epoch: [80]  [480/781]  eta: 0:00:59  lr: 0.000026  training_loss: 1.1113 (1.1381)  mae_loss: 0.0258 (0.0256)  classification_loss: 1.0846 (1.1125)  loss_mask: 0.0000 (0.0001)  time: 0.1951  data: 0.0002  max mem: 5511
[00:28:45.794489] Epoch: [80]  [500/781]  eta: 0:00:55  lr: 0.000026  training_loss: 1.1020 (1.1372)  mae_loss: 0.0244 (0.0256)  classification_loss: 1.0767 (1.1116)  loss_mask: 0.0000 (0.0001)  time: 0.1954  data: 0.0002  max mem: 5511
[00:28:49.763034] Epoch: [80]  [520/781]  eta: 0:00:51  lr: 0.000026  training_loss: 1.1064 (1.1364)  mae_loss: 0.0261 (0.0256)  classification_loss: 1.0847 (1.1108)  loss_mask: 0.0000 (0.0001)  time: 0.1983  data: 0.0002  max mem: 5511
[00:28:53.660227] Epoch: [80]  [540/781]  eta: 0:00:47  lr: 0.000026  training_loss: 1.1222 (1.1371)  mae_loss: 0.0258 (0.0256)  classification_loss: 1.0957 (1.1114)  loss_mask: 0.0000 (0.0001)  time: 0.1948  data: 0.0001  max mem: 5511
[00:28:57.630130] Epoch: [80]  [560/781]  eta: 0:00:43  lr: 0.000025  training_loss: 1.1443 (1.1367)  mae_loss: 0.0262 (0.0256)  classification_loss: 1.1185 (1.1110)  loss_mask: 0.0001 (0.0001)  time: 0.1984  data: 0.0002  max mem: 5511
[00:29:01.604762] Epoch: [80]  [580/781]  eta: 0:00:39  lr: 0.000025  training_loss: 1.1644 (1.1377)  mae_loss: 0.0252 (0.0256)  classification_loss: 1.1400 (1.1120)  loss_mask: 0.0001 (0.0001)  time: 0.1986  data: 0.0002  max mem: 5511
[00:29:05.538044] Epoch: [80]  [600/781]  eta: 0:00:35  lr: 0.000025  training_loss: 1.1238 (1.1370)  mae_loss: 0.0237 (0.0256)  classification_loss: 1.0962 (1.1113)  loss_mask: 0.0001 (0.0001)  time: 0.1966  data: 0.0002  max mem: 5511
[00:29:09.462804] Epoch: [80]  [620/781]  eta: 0:00:31  lr: 0.000025  training_loss: 1.1392 (1.1373)  mae_loss: 0.0253 (0.0256)  classification_loss: 1.1111 (1.1117)  loss_mask: 0.0001 (0.0001)  time: 0.1962  data: 0.0002  max mem: 5511
[00:29:13.390933] Epoch: [80]  [640/781]  eta: 0:00:27  lr: 0.000025  training_loss: 1.1087 (1.1370)  mae_loss: 0.0242 (0.0256)  classification_loss: 1.0897 (1.1114)  loss_mask: 0.0000 (0.0001)  time: 0.1963  data: 0.0002  max mem: 5511
[00:29:17.308467] Epoch: [80]  [660/781]  eta: 0:00:23  lr: 0.000025  training_loss: 1.1302 (1.1370)  mae_loss: 0.0263 (0.0256)  classification_loss: 1.1084 (1.1113)  loss_mask: 0.0000 (0.0001)  time: 0.1958  data: 0.0002  max mem: 5511
[00:29:21.235496] Epoch: [80]  [680/781]  eta: 0:00:19  lr: 0.000025  training_loss: 1.1776 (1.1377)  mae_loss: 0.0252 (0.0256)  classification_loss: 1.1527 (1.1120)  loss_mask: 0.0001 (0.0001)  time: 0.1963  data: 0.0002  max mem: 5511
[00:29:25.137262] Epoch: [80]  [700/781]  eta: 0:00:15  lr: 0.000025  training_loss: 1.1724 (1.1389)  mae_loss: 0.0254 (0.0256)  classification_loss: 1.1465 (1.1132)  loss_mask: 0.0000 (0.0001)  time: 0.1950  data: 0.0002  max mem: 5511
[00:29:29.059880] Epoch: [80]  [720/781]  eta: 0:00:12  lr: 0.000025  training_loss: 1.1365 (1.1394)  mae_loss: 0.0248 (0.0256)  classification_loss: 1.1112 (1.1137)  loss_mask: 0.0000 (0.0001)  time: 0.1961  data: 0.0002  max mem: 5511
[00:29:32.989404] Epoch: [80]  [740/781]  eta: 0:00:08  lr: 0.000025  training_loss: 1.0976 (1.1383)  mae_loss: 0.0270 (0.0256)  classification_loss: 1.0683 (1.1126)  loss_mask: 0.0000 (0.0001)  time: 0.1964  data: 0.0002  max mem: 5511
[00:29:36.884781] Epoch: [80]  [760/781]  eta: 0:00:04  lr: 0.000025  training_loss: 1.1639 (1.1385)  mae_loss: 0.0253 (0.0257)  classification_loss: 1.1387 (1.1127)  loss_mask: 0.0001 (0.0001)  time: 0.1947  data: 0.0002  max mem: 5511
[00:29:40.778689] Epoch: [80]  [780/781]  eta: 0:00:00  lr: 0.000025  training_loss: 1.1341 (1.1386)  mae_loss: 0.0260 (0.0257)  classification_loss: 1.1081 (1.1128)  loss_mask: 0.0001 (0.0001)  time: 0.1946  data: 0.0002  max mem: 5511
[00:29:40.929422] Epoch: [80] Total time: 0:02:34 (0.1974 s / it)
[00:29:40.929901] Averaged stats: lr: 0.000025  training_loss: 1.1341 (1.1386)  mae_loss: 0.0260 (0.0257)  classification_loss: 1.1081 (1.1128)  loss_mask: 0.0001 (0.0001)
[00:29:42.263430] Test:  [  0/157]  eta: 0:01:42  testing_loss: 0.5261 (0.5261)  acc1: 79.6875 (79.6875)  acc5: 100.0000 (100.0000)  time: 0.6552  data: 0.6188  max mem: 5511
[00:29:42.553497] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4527 (0.4962)  acc1: 85.9375 (85.6534)  acc5: 100.0000 (99.7159)  time: 0.0857  data: 0.0565  max mem: 5511
[00:29:42.841955] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4434 (0.4495)  acc1: 87.5000 (86.8304)  acc5: 100.0000 (99.4792)  time: 0.0287  data: 0.0002  max mem: 5511
[00:29:43.133871] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4492 (0.4656)  acc1: 87.5000 (86.0887)  acc5: 100.0000 (99.3952)  time: 0.0288  data: 0.0002  max mem: 5511
[00:29:43.421465] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4764 (0.4700)  acc1: 87.5000 (86.2043)  acc5: 98.4375 (99.1997)  time: 0.0288  data: 0.0002  max mem: 5511
[00:29:43.705892] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4431 (0.4634)  acc1: 87.5000 (86.6115)  acc5: 100.0000 (99.2953)  time: 0.0284  data: 0.0002  max mem: 5511
[00:29:43.996324] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4063 (0.4610)  acc1: 89.0625 (86.8596)  acc5: 100.0000 (99.3084)  time: 0.0286  data: 0.0002  max mem: 5511
[00:29:44.280226] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4040 (0.4545)  acc1: 89.0625 (87.1259)  acc5: 100.0000 (99.3618)  time: 0.0286  data: 0.0002  max mem: 5511
[00:29:44.564107] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4527 (0.4614)  acc1: 85.9375 (86.9020)  acc5: 100.0000 (99.3634)  time: 0.0283  data: 0.0002  max mem: 5511
[00:29:44.846914] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4751 (0.4583)  acc1: 85.9375 (87.0364)  acc5: 98.4375 (99.3304)  time: 0.0282  data: 0.0002  max mem: 5511
[00:29:45.129545] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4554 (0.4603)  acc1: 87.5000 (86.9585)  acc5: 98.4375 (99.3193)  time: 0.0281  data: 0.0002  max mem: 5511
[00:29:45.411220] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4744 (0.4592)  acc1: 85.9375 (87.0495)  acc5: 100.0000 (99.3102)  time: 0.0281  data: 0.0002  max mem: 5511
[00:29:45.694076] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4862 (0.4596)  acc1: 85.9375 (87.0222)  acc5: 100.0000 (99.3156)  time: 0.0281  data: 0.0002  max mem: 5511
[00:29:45.975912] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4621 (0.4598)  acc1: 85.9375 (86.9752)  acc5: 100.0000 (99.3201)  time: 0.0281  data: 0.0002  max mem: 5511
[00:29:46.256942] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4412 (0.4587)  acc1: 87.5000 (86.9792)  acc5: 100.0000 (99.3351)  time: 0.0280  data: 0.0001  max mem: 5511
[00:29:46.535522] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4592 (0.4588)  acc1: 85.9375 (86.9516)  acc5: 100.0000 (99.3171)  time: 0.0279  data: 0.0001  max mem: 5511
[00:29:46.685266] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4464 (0.4576)  acc1: 85.9375 (86.9500)  acc5: 100.0000 (99.3200)  time: 0.0269  data: 0.0001  max mem: 5511
[00:29:46.836344] Test: Total time: 0:00:05 (0.0333 s / it)
[00:29:46.836877] * Acc@1 86.950 Acc@5 99.320 loss 0.458
[00:29:46.837204] Accuracy of the network on the 10000 test images: 87.0%
[00:29:46.837424] Max accuracy: 86.95%
[00:29:47.381679] log_dir: ./output_dir
[00:29:48.255154] Epoch: [81]  [  0/781]  eta: 0:11:20  lr: 0.000025  training_loss: 0.9428 (0.9428)  mae_loss: 0.0254 (0.0254)  classification_loss: 0.9173 (0.9173)  loss_mask: 0.0000 (0.0000)  time: 0.8717  data: 0.6466  max mem: 5511
[00:29:52.198126] Epoch: [81]  [ 20/781]  eta: 0:02:54  lr: 0.000025  training_loss: 1.0956 (1.1014)  mae_loss: 0.0254 (0.0261)  classification_loss: 1.0657 (1.0753)  loss_mask: 0.0000 (0.0001)  time: 0.1970  data: 0.0003  max mem: 5511
[00:29:56.159896] Epoch: [81]  [ 40/781]  eta: 0:02:38  lr: 0.000025  training_loss: 1.1054 (1.1056)  mae_loss: 0.0249 (0.0255)  classification_loss: 1.0805 (1.0800)  loss_mask: 0.0000 (0.0001)  time: 0.1980  data: 0.0002  max mem: 5511
[00:30:00.097884] Epoch: [81]  [ 60/781]  eta: 0:02:30  lr: 0.000025  training_loss: 1.1346 (1.1177)  mae_loss: 0.0236 (0.0252)  classification_loss: 1.1102 (1.0924)  loss_mask: 0.0000 (0.0001)  time: 0.1968  data: 0.0002  max mem: 5511
[00:30:04.004519] Epoch: [81]  [ 80/781]  eta: 0:02:23  lr: 0.000025  training_loss: 1.0853 (1.1138)  mae_loss: 0.0247 (0.0251)  classification_loss: 1.0592 (1.0886)  loss_mask: 0.0000 (0.0001)  time: 0.1953  data: 0.0002  max mem: 5511
[00:30:07.911357] Epoch: [81]  [100/781]  eta: 0:02:18  lr: 0.000024  training_loss: 1.1217 (1.1185)  mae_loss: 0.0261 (0.0255)  classification_loss: 1.0963 (1.0929)  loss_mask: 0.0000 (0.0001)  time: 0.1953  data: 0.0002  max mem: 5511
[00:30:11.865315] Epoch: [81]  [120/781]  eta: 0:02:13  lr: 0.000024  training_loss: 1.0922 (1.1168)  mae_loss: 0.0252 (0.0254)  classification_loss: 1.0708 (1.0912)  loss_mask: 0.0001 (0.0002)  time: 0.1976  data: 0.0002  max mem: 5511
[00:30:15.788610] Epoch: [81]  [140/781]  eta: 0:02:09  lr: 0.000024  training_loss: 1.1155 (1.1197)  mae_loss: 0.0260 (0.0255)  classification_loss: 1.0890 (1.0941)  loss_mask: 0.0001 (0.0002)  time: 0.1961  data: 0.0002  max mem: 5511
[00:30:19.706396] Epoch: [81]  [160/781]  eta: 0:02:04  lr: 0.000024  training_loss: 1.1383 (1.1222)  mae_loss: 0.0251 (0.0255)  classification_loss: 1.1050 (1.0965)  loss_mask: 0.0000 (0.0001)  time: 0.1958  data: 0.0001  max mem: 5511
[00:30:23.627927] Epoch: [81]  [180/781]  eta: 0:02:00  lr: 0.000024  training_loss: 1.1193 (1.1240)  mae_loss: 0.0264 (0.0256)  classification_loss: 1.0928 (1.0982)  loss_mask: 0.0001 (0.0002)  time: 0.1960  data: 0.0002  max mem: 5511
[00:30:27.542279] Epoch: [81]  [200/781]  eta: 0:01:56  lr: 0.000024  training_loss: 1.0853 (1.1234)  mae_loss: 0.0248 (0.0256)  classification_loss: 1.0569 (1.0976)  loss_mask: 0.0001 (0.0002)  time: 0.1956  data: 0.0002  max mem: 5511
[00:30:31.464907] Epoch: [81]  [220/781]  eta: 0:01:51  lr: 0.000024  training_loss: 1.1624 (1.1241)  mae_loss: 0.0265 (0.0256)  classification_loss: 1.1331 (1.0983)  loss_mask: 0.0001 (0.0002)  time: 0.1960  data: 0.0002  max mem: 5511
[00:30:35.418821] Epoch: [81]  [240/781]  eta: 0:01:47  lr: 0.000024  training_loss: 1.1480 (1.1254)  mae_loss: 0.0256 (0.0257)  classification_loss: 1.1207 (1.0996)  loss_mask: 0.0000 (0.0002)  time: 0.1976  data: 0.0002  max mem: 5511
[00:30:39.387000] Epoch: [81]  [260/781]  eta: 0:01:43  lr: 0.000024  training_loss: 1.1102 (1.1253)  mae_loss: 0.0242 (0.0256)  classification_loss: 1.0891 (1.0995)  loss_mask: 0.0000 (0.0002)  time: 0.1983  data: 0.0002  max mem: 5511
[00:30:43.361002] Epoch: [81]  [280/781]  eta: 0:01:39  lr: 0.000024  training_loss: 1.1057 (1.1267)  mae_loss: 0.0266 (0.0257)  classification_loss: 1.0848 (1.1009)  loss_mask: 0.0000 (0.0001)  time: 0.1986  data: 0.0002  max mem: 5511
[00:30:47.282771] Epoch: [81]  [300/781]  eta: 0:01:35  lr: 0.000024  training_loss: 1.1795 (1.1318)  mae_loss: 0.0251 (0.0257)  classification_loss: 1.1577 (1.1059)  loss_mask: 0.0000 (0.0001)  time: 0.1960  data: 0.0003  max mem: 5511
[00:30:51.220726] Epoch: [81]  [320/781]  eta: 0:01:31  lr: 0.000024  training_loss: 1.1485 (1.1325)  mae_loss: 0.0258 (0.0256)  classification_loss: 1.1227 (1.1067)  loss_mask: 0.0000 (0.0001)  time: 0.1968  data: 0.0003  max mem: 5511
[00:30:55.140381] Epoch: [81]  [340/781]  eta: 0:01:27  lr: 0.000024  training_loss: 1.1328 (1.1339)  mae_loss: 0.0245 (0.0256)  classification_loss: 1.1051 (1.1082)  loss_mask: 0.0000 (0.0001)  time: 0.1959  data: 0.0004  max mem: 5511
[00:30:59.110679] Epoch: [81]  [360/781]  eta: 0:01:23  lr: 0.000024  training_loss: 1.1388 (1.1348)  mae_loss: 0.0242 (0.0255)  classification_loss: 1.1114 (1.1091)  loss_mask: 0.0000 (0.0001)  time: 0.1984  data: 0.0002  max mem: 5511
[00:31:03.022116] Epoch: [81]  [380/781]  eta: 0:01:19  lr: 0.000024  training_loss: 1.1364 (1.1372)  mae_loss: 0.0253 (0.0256)  classification_loss: 1.1121 (1.1115)  loss_mask: 0.0000 (0.0001)  time: 0.1955  data: 0.0002  max mem: 5511
[00:31:06.933152] Epoch: [81]  [400/781]  eta: 0:01:15  lr: 0.000024  training_loss: 1.1306 (1.1374)  mae_loss: 0.0246 (0.0256)  classification_loss: 1.1014 (1.1117)  loss_mask: 0.0000 (0.0001)  time: 0.1955  data: 0.0002  max mem: 5511
[00:31:10.870221] Epoch: [81]  [420/781]  eta: 0:01:11  lr: 0.000023  training_loss: 1.1100 (1.1364)  mae_loss: 0.0244 (0.0255)  classification_loss: 1.0855 (1.1108)  loss_mask: 0.0000 (0.0001)  time: 0.1968  data: 0.0002  max mem: 5511
[00:31:14.777413] Epoch: [81]  [440/781]  eta: 0:01:07  lr: 0.000023  training_loss: 1.1012 (1.1342)  mae_loss: 0.0251 (0.0255)  classification_loss: 1.0775 (1.1086)  loss_mask: 0.0000 (0.0001)  time: 0.1953  data: 0.0002  max mem: 5511
[00:31:18.720281] Epoch: [81]  [460/781]  eta: 0:01:03  lr: 0.000023  training_loss: 1.1121 (1.1330)  mae_loss: 0.0253 (0.0255)  classification_loss: 1.0925 (1.1074)  loss_mask: 0.0000 (0.0001)  time: 0.1971  data: 0.0002  max mem: 5511
[00:31:22.651181] Epoch: [81]  [480/781]  eta: 0:00:59  lr: 0.000023  training_loss: 1.1452 (1.1339)  mae_loss: 0.0248 (0.0255)  classification_loss: 1.1151 (1.1084)  loss_mask: 0.0000 (0.0001)  time: 0.1964  data: 0.0002  max mem: 5511
[00:31:26.564426] Epoch: [81]  [500/781]  eta: 0:00:55  lr: 0.000023  training_loss: 1.1386 (1.1360)  mae_loss: 0.0249 (0.0255)  classification_loss: 1.1081 (1.1104)  loss_mask: 0.0000 (0.0001)  time: 0.1956  data: 0.0002  max mem: 5511
[00:31:30.491845] Epoch: [81]  [520/781]  eta: 0:00:51  lr: 0.000023  training_loss: 1.1250 (1.1357)  mae_loss: 0.0251 (0.0255)  classification_loss: 1.1034 (1.1101)  loss_mask: 0.0000 (0.0001)  time: 0.1963  data: 0.0002  max mem: 5511
[00:31:34.462541] Epoch: [81]  [540/781]  eta: 0:00:47  lr: 0.000023  training_loss: 1.1496 (1.1359)  mae_loss: 0.0257 (0.0256)  classification_loss: 1.1232 (1.1103)  loss_mask: 0.0000 (0.0001)  time: 0.1985  data: 0.0003  max mem: 5511
[00:31:38.397948] Epoch: [81]  [560/781]  eta: 0:00:43  lr: 0.000023  training_loss: 1.1376 (1.1364)  mae_loss: 0.0264 (0.0256)  classification_loss: 1.1146 (1.1107)  loss_mask: 0.0000 (0.0001)  time: 0.1967  data: 0.0002  max mem: 5511
[00:31:42.336149] Epoch: [81]  [580/781]  eta: 0:00:39  lr: 0.000023  training_loss: 1.1079 (1.1362)  mae_loss: 0.0270 (0.0256)  classification_loss: 1.0769 (1.1105)  loss_mask: 0.0000 (0.0001)  time: 0.1968  data: 0.0002  max mem: 5511
[00:31:46.252152] Epoch: [81]  [600/781]  eta: 0:00:35  lr: 0.000023  training_loss: 1.1395 (1.1358)  mae_loss: 0.0242 (0.0256)  classification_loss: 1.1168 (1.1101)  loss_mask: 0.0000 (0.0001)  time: 0.1957  data: 0.0002  max mem: 5511
[00:31:50.169359] Epoch: [81]  [620/781]  eta: 0:00:31  lr: 0.000023  training_loss: 1.1227 (1.1361)  mae_loss: 0.0252 (0.0256)  classification_loss: 1.1004 (1.1104)  loss_mask: 0.0000 (0.0001)  time: 0.1957  data: 0.0002  max mem: 5511
[00:31:54.074440] Epoch: [81]  [640/781]  eta: 0:00:27  lr: 0.000023  training_loss: 1.1334 (1.1365)  mae_loss: 0.0256 (0.0256)  classification_loss: 1.1085 (1.1109)  loss_mask: 0.0000 (0.0001)  time: 0.1952  data: 0.0003  max mem: 5511
[00:31:58.042919] Epoch: [81]  [660/781]  eta: 0:00:23  lr: 0.000023  training_loss: 1.0933 (1.1359)  mae_loss: 0.0242 (0.0255)  classification_loss: 1.0676 (1.1102)  loss_mask: 0.0000 (0.0001)  time: 0.1983  data: 0.0003  max mem: 5511
[00:32:02.008742] Epoch: [81]  [680/781]  eta: 0:00:19  lr: 0.000023  training_loss: 1.1383 (1.1362)  mae_loss: 0.0265 (0.0256)  classification_loss: 1.1145 (1.1106)  loss_mask: 0.0000 (0.0001)  time: 0.1982  data: 0.0003  max mem: 5511
[00:32:05.952255] Epoch: [81]  [700/781]  eta: 0:00:16  lr: 0.000023  training_loss: 1.1359 (1.1363)  mae_loss: 0.0245 (0.0256)  classification_loss: 1.1121 (1.1106)  loss_mask: 0.0000 (0.0001)  time: 0.1971  data: 0.0002  max mem: 5511
[00:32:09.878334] Epoch: [81]  [720/781]  eta: 0:00:12  lr: 0.000023  training_loss: 1.0678 (1.1354)  mae_loss: 0.0252 (0.0256)  classification_loss: 1.0450 (1.1098)  loss_mask: 0.0000 (0.0001)  time: 0.1962  data: 0.0002  max mem: 5511
[00:32:13.813042] Epoch: [81]  [740/781]  eta: 0:00:08  lr: 0.000023  training_loss: 1.0992 (1.1350)  mae_loss: 0.0249 (0.0255)  classification_loss: 1.0751 (1.1094)  loss_mask: 0.0000 (0.0001)  time: 0.1966  data: 0.0002  max mem: 5511
[00:32:17.795354] Epoch: [81]  [760/781]  eta: 0:00:04  lr: 0.000022  training_loss: 1.1146 (1.1348)  mae_loss: 0.0256 (0.0256)  classification_loss: 1.0870 (1.1091)  loss_mask: 0.0000 (0.0001)  time: 0.1990  data: 0.0002  max mem: 5511
[00:32:21.685609] Epoch: [81]  [780/781]  eta: 0:00:00  lr: 0.000022  training_loss: 1.1345 (1.1350)  mae_loss: 0.0254 (0.0256)  classification_loss: 1.1093 (1.1094)  loss_mask: 0.0000 (0.0001)  time: 0.1944  data: 0.0002  max mem: 5511
[00:32:21.831511] Epoch: [81] Total time: 0:02:34 (0.1978 s / it)
[00:32:21.832587] Averaged stats: lr: 0.000022  training_loss: 1.1345 (1.1350)  mae_loss: 0.0254 (0.0256)  classification_loss: 1.1093 (1.1094)  loss_mask: 0.0000 (0.0001)
[00:32:22.487223] Test:  [  0/157]  eta: 0:01:42  testing_loss: 0.5119 (0.5119)  acc1: 84.3750 (84.3750)  acc5: 98.4375 (98.4375)  time: 0.6499  data: 0.6087  max mem: 5511
[00:32:22.780446] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4873 (0.4914)  acc1: 85.9375 (85.3693)  acc5: 100.0000 (99.2898)  time: 0.0855  data: 0.0559  max mem: 5511
[00:32:23.061938] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4378 (0.4485)  acc1: 87.5000 (86.4583)  acc5: 100.0000 (99.4792)  time: 0.0285  data: 0.0004  max mem: 5511
[00:32:23.353930] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4127 (0.4573)  acc1: 87.5000 (85.9879)  acc5: 100.0000 (99.1935)  time: 0.0286  data: 0.0003  max mem: 5511
[00:32:23.635379] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4571 (0.4629)  acc1: 87.5000 (86.1662)  acc5: 98.4375 (99.1235)  time: 0.0286  data: 0.0003  max mem: 5511
[00:32:23.919650] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4463 (0.4560)  acc1: 87.5000 (86.6728)  acc5: 100.0000 (99.2647)  time: 0.0281  data: 0.0002  max mem: 5511
[00:32:24.202330] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.3984 (0.4529)  acc1: 87.5000 (86.9365)  acc5: 100.0000 (99.3084)  time: 0.0282  data: 0.0002  max mem: 5511
[00:32:24.498834] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4171 (0.4493)  acc1: 87.5000 (87.1479)  acc5: 100.0000 (99.3398)  time: 0.0288  data: 0.0003  max mem: 5511
[00:32:24.785326] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4666 (0.4572)  acc1: 85.9375 (86.7670)  acc5: 100.0000 (99.3441)  time: 0.0290  data: 0.0003  max mem: 5511
[00:32:25.070028] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4832 (0.4537)  acc1: 85.9375 (86.9505)  acc5: 100.0000 (99.2960)  time: 0.0284  data: 0.0002  max mem: 5511
[00:32:25.353318] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4527 (0.4558)  acc1: 89.0625 (86.9121)  acc5: 98.4375 (99.2729)  time: 0.0283  data: 0.0002  max mem: 5511
[00:32:25.645408] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4724 (0.4551)  acc1: 85.9375 (86.9369)  acc5: 100.0000 (99.3102)  time: 0.0286  data: 0.0002  max mem: 5511
[00:32:25.933335] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4931 (0.4560)  acc1: 85.9375 (86.9060)  acc5: 100.0000 (99.3156)  time: 0.0289  data: 0.0002  max mem: 5511
[00:32:26.217065] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4349 (0.4566)  acc1: 85.9375 (86.8321)  acc5: 100.0000 (99.3440)  time: 0.0285  data: 0.0002  max mem: 5511
[00:32:26.498985] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4381 (0.4557)  acc1: 85.9375 (86.8129)  acc5: 100.0000 (99.3573)  time: 0.0282  data: 0.0002  max mem: 5511
[00:32:26.779255] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4464 (0.4562)  acc1: 85.9375 (86.7239)  acc5: 100.0000 (99.3584)  time: 0.0280  data: 0.0001  max mem: 5511
[00:32:26.929611] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4426 (0.4555)  acc1: 85.9375 (86.7300)  acc5: 100.0000 (99.3700)  time: 0.0270  data: 0.0001  max mem: 5511
[00:32:27.085415] Test: Total time: 0:00:05 (0.0334 s / it)
[00:32:27.085877] * Acc@1 86.730 Acc@5 99.370 loss 0.456
[00:32:27.086157] Accuracy of the network on the 10000 test images: 86.7%
[00:32:27.086326] Max accuracy: 86.95%
[00:32:27.437686] log_dir: ./output_dir
[00:32:28.288119] Epoch: [82]  [  0/781]  eta: 0:11:02  lr: 0.000022  training_loss: 1.1220 (1.1220)  mae_loss: 0.0250 (0.0250)  classification_loss: 1.0969 (1.0969)  loss_mask: 0.0000 (0.0000)  time: 0.8486  data: 0.6189  max mem: 5511
[00:32:32.226339] Epoch: [82]  [ 20/781]  eta: 0:02:53  lr: 0.000022  training_loss: 1.1442 (1.1418)  mae_loss: 0.0258 (0.0257)  classification_loss: 1.1145 (1.1161)  loss_mask: 0.0000 (0.0000)  time: 0.1968  data: 0.0002  max mem: 5511
[00:32:36.127466] Epoch: [82]  [ 40/781]  eta: 0:02:36  lr: 0.000022  training_loss: 1.1539 (1.1521)  mae_loss: 0.0245 (0.0256)  classification_loss: 1.1255 (1.1264)  loss_mask: 0.0000 (0.0001)  time: 0.1950  data: 0.0002  max mem: 5511
[00:32:40.048622] Epoch: [82]  [ 60/781]  eta: 0:02:28  lr: 0.000022  training_loss: 1.1236 (1.1478)  mae_loss: 0.0254 (0.0255)  classification_loss: 1.0912 (1.1222)  loss_mask: 0.0001 (0.0001)  time: 0.1960  data: 0.0002  max mem: 5511
[00:32:43.967698] Epoch: [82]  [ 80/781]  eta: 0:02:22  lr: 0.000022  training_loss: 1.1188 (1.1443)  mae_loss: 0.0252 (0.0256)  classification_loss: 1.0897 (1.1186)  loss_mask: 0.0000 (0.0001)  time: 0.1959  data: 0.0002  max mem: 5511
[00:32:47.873619] Epoch: [82]  [100/781]  eta: 0:02:17  lr: 0.000022  training_loss: 1.1387 (1.1473)  mae_loss: 0.0254 (0.0255)  classification_loss: 1.1122 (1.1216)  loss_mask: 0.0000 (0.0001)  time: 0.1952  data: 0.0002  max mem: 5511
[00:32:51.777483] Epoch: [82]  [120/781]  eta: 0:02:12  lr: 0.000022  training_loss: 1.1349 (1.1449)  mae_loss: 0.0259 (0.0257)  classification_loss: 1.1037 (1.1191)  loss_mask: 0.0000 (0.0001)  time: 0.1951  data: 0.0001  max mem: 5511
[00:32:55.695780] Epoch: [82]  [140/781]  eta: 0:02:08  lr: 0.000022  training_loss: 1.0989 (1.1400)  mae_loss: 0.0265 (0.0258)  classification_loss: 1.0705 (1.1141)  loss_mask: 0.0000 (0.0001)  time: 0.1958  data: 0.0002  max mem: 5511
[00:32:59.612428] Epoch: [82]  [160/781]  eta: 0:02:04  lr: 0.000022  training_loss: 1.1399 (1.1413)  mae_loss: 0.0248 (0.0257)  classification_loss: 1.1152 (1.1155)  loss_mask: 0.0000 (0.0001)  time: 0.1957  data: 0.0002  max mem: 5511
[00:33:03.584582] Epoch: [82]  [180/781]  eta: 0:01:59  lr: 0.000022  training_loss: 1.1166 (1.1416)  mae_loss: 0.0255 (0.0258)  classification_loss: 1.0911 (1.1158)  loss_mask: 0.0000 (0.0001)  time: 0.1985  data: 0.0004  max mem: 5511
[00:33:07.525914] Epoch: [82]  [200/781]  eta: 0:01:55  lr: 0.000022  training_loss: 1.1336 (1.1417)  mae_loss: 0.0236 (0.0256)  classification_loss: 1.1020 (1.1160)  loss_mask: 0.0000 (0.0001)  time: 0.1970  data: 0.0002  max mem: 5511
[00:33:11.458566] Epoch: [82]  [220/781]  eta: 0:01:51  lr: 0.000022  training_loss: 1.1261 (1.1411)  mae_loss: 0.0253 (0.0256)  classification_loss: 1.0990 (1.1154)  loss_mask: 0.0000 (0.0001)  time: 0.1966  data: 0.0002  max mem: 5511
[00:33:15.391317] Epoch: [82]  [240/781]  eta: 0:01:47  lr: 0.000022  training_loss: 1.1498 (1.1405)  mae_loss: 0.0246 (0.0256)  classification_loss: 1.1287 (1.1148)  loss_mask: 0.0000 (0.0001)  time: 0.1966  data: 0.0002  max mem: 5511
[00:33:19.299090] Epoch: [82]  [260/781]  eta: 0:01:43  lr: 0.000022  training_loss: 1.1121 (1.1387)  mae_loss: 0.0245 (0.0255)  classification_loss: 1.0837 (1.1131)  loss_mask: 0.0000 (0.0001)  time: 0.1953  data: 0.0002  max mem: 5511
[00:33:23.203624] Epoch: [82]  [280/781]  eta: 0:01:39  lr: 0.000022  training_loss: 1.1064 (1.1379)  mae_loss: 0.0245 (0.0255)  classification_loss: 1.0808 (1.1123)  loss_mask: 0.0000 (0.0001)  time: 0.1951  data: 0.0002  max mem: 5511
[00:33:27.125420] Epoch: [82]  [300/781]  eta: 0:01:35  lr: 0.000022  training_loss: 1.1361 (1.1376)  mae_loss: 0.0240 (0.0255)  classification_loss: 1.1084 (1.1120)  loss_mask: 0.0000 (0.0001)  time: 0.1960  data: 0.0002  max mem: 5511
[00:33:31.066786] Epoch: [82]  [320/781]  eta: 0:01:31  lr: 0.000021  training_loss: 1.1171 (1.1368)  mae_loss: 0.0251 (0.0255)  classification_loss: 1.0920 (1.1112)  loss_mask: 0.0000 (0.0001)  time: 0.1970  data: 0.0002  max mem: 5511
[00:33:34.983907] Epoch: [82]  [340/781]  eta: 0:01:27  lr: 0.000021  training_loss: 1.1313 (1.1372)  mae_loss: 0.0266 (0.0255)  classification_loss: 1.1054 (1.1116)  loss_mask: 0.0000 (0.0001)  time: 0.1958  data: 0.0002  max mem: 5511
[00:33:38.911706] Epoch: [82]  [360/781]  eta: 0:01:23  lr: 0.000021  training_loss: 1.1791 (1.1389)  mae_loss: 0.0255 (0.0255)  classification_loss: 1.1577 (1.1133)  loss_mask: 0.0000 (0.0001)  time: 0.1963  data: 0.0002  max mem: 5511
[00:33:42.853257] Epoch: [82]  [380/781]  eta: 0:01:19  lr: 0.000021  training_loss: 1.1385 (1.1390)  mae_loss: 0.0245 (0.0255)  classification_loss: 1.1129 (1.1134)  loss_mask: 0.0001 (0.0001)  time: 0.1970  data: 0.0004  max mem: 5511
[00:33:46.801917] Epoch: [82]  [400/781]  eta: 0:01:15  lr: 0.000021  training_loss: 1.1291 (1.1389)  mae_loss: 0.0266 (0.0256)  classification_loss: 1.1044 (1.1132)  loss_mask: 0.0001 (0.0001)  time: 0.1973  data: 0.0002  max mem: 5511
[00:33:50.734087] Epoch: [82]  [420/781]  eta: 0:01:11  lr: 0.000021  training_loss: 1.1385 (1.1393)  mae_loss: 0.0249 (0.0257)  classification_loss: 1.1128 (1.1135)  loss_mask: 0.0000 (0.0001)  time: 0.1965  data: 0.0002  max mem: 5511
[00:33:54.639765] Epoch: [82]  [440/781]  eta: 0:01:07  lr: 0.000021  training_loss: 1.1603 (1.1391)  mae_loss: 0.0252 (0.0257)  classification_loss: 1.1345 (1.1134)  loss_mask: 0.0000 (0.0001)  time: 0.1952  data: 0.0002  max mem: 5511
[00:33:58.562834] Epoch: [82]  [460/781]  eta: 0:01:03  lr: 0.000021  training_loss: 1.1224 (1.1383)  mae_loss: 0.0230 (0.0256)  classification_loss: 1.0997 (1.1126)  loss_mask: 0.0000 (0.0001)  time: 0.1961  data: 0.0002  max mem: 5511
[00:34:02.569178] Epoch: [82]  [480/781]  eta: 0:00:59  lr: 0.000021  training_loss: 1.1081 (1.1384)  mae_loss: 0.0251 (0.0256)  classification_loss: 1.0829 (1.1127)  loss_mask: 0.0000 (0.0001)  time: 0.2002  data: 0.0002  max mem: 5511
[00:34:06.523998] Epoch: [82]  [500/781]  eta: 0:00:55  lr: 0.000021  training_loss: 1.1011 (1.1376)  mae_loss: 0.0246 (0.0256)  classification_loss: 1.0710 (1.1119)  loss_mask: 0.0001 (0.0001)  time: 0.1976  data: 0.0002  max mem: 5511
[00:34:10.453392] Epoch: [82]  [520/781]  eta: 0:00:51  lr: 0.000021  training_loss: 1.1258 (1.1385)  mae_loss: 0.0270 (0.0256)  classification_loss: 1.1013 (1.1127)  loss_mask: 0.0000 (0.0001)  time: 0.1964  data: 0.0003  max mem: 5511
[00:34:14.365825] Epoch: [82]  [540/781]  eta: 0:00:47  lr: 0.000021  training_loss: 1.1064 (1.1378)  mae_loss: 0.0259 (0.0257)  classification_loss: 1.0779 (1.1120)  loss_mask: 0.0000 (0.0001)  time: 0.1955  data: 0.0002  max mem: 5511
[00:34:18.282143] Epoch: [82]  [560/781]  eta: 0:00:43  lr: 0.000021  training_loss: 1.1052 (1.1372)  mae_loss: 0.0247 (0.0257)  classification_loss: 1.0758 (1.1114)  loss_mask: 0.0001 (0.0001)  time: 0.1957  data: 0.0003  max mem: 5511
[00:34:22.201435] Epoch: [82]  [580/781]  eta: 0:00:39  lr: 0.000021  training_loss: 1.0937 (1.1370)  mae_loss: 0.0259 (0.0257)  classification_loss: 1.0699 (1.1112)  loss_mask: 0.0000 (0.0001)  time: 0.1958  data: 0.0002  max mem: 5511
[00:34:26.119633] Epoch: [82]  [600/781]  eta: 0:00:35  lr: 0.000021  training_loss: 1.1347 (1.1367)  mae_loss: 0.0253 (0.0256)  classification_loss: 1.1120 (1.1109)  loss_mask: 0.0000 (0.0001)  time: 0.1958  data: 0.0002  max mem: 5511
[00:34:30.049378] Epoch: [82]  [620/781]  eta: 0:00:31  lr: 0.000021  training_loss: 1.1048 (1.1365)  mae_loss: 0.0253 (0.0256)  classification_loss: 1.0794 (1.1107)  loss_mask: 0.0000 (0.0001)  time: 0.1964  data: 0.0002  max mem: 5511
[00:34:33.960247] Epoch: [82]  [640/781]  eta: 0:00:27  lr: 0.000021  training_loss: 1.0823 (1.1353)  mae_loss: 0.0251 (0.0256)  classification_loss: 1.0574 (1.1095)  loss_mask: 0.0000 (0.0001)  time: 0.1954  data: 0.0002  max mem: 5511
[00:34:37.868792] Epoch: [82]  [660/781]  eta: 0:00:23  lr: 0.000021  training_loss: 1.0938 (1.1354)  mae_loss: 0.0246 (0.0256)  classification_loss: 1.0661 (1.1096)  loss_mask: 0.0000 (0.0001)  time: 0.1953  data: 0.0002  max mem: 5511
[00:34:41.772884] Epoch: [82]  [680/781]  eta: 0:00:19  lr: 0.000020  training_loss: 1.1334 (1.1356)  mae_loss: 0.0257 (0.0256)  classification_loss: 1.1073 (1.1099)  loss_mask: 0.0000 (0.0001)  time: 0.1951  data: 0.0002  max mem: 5511
[00:34:45.693228] Epoch: [82]  [700/781]  eta: 0:00:15  lr: 0.000020  training_loss: 1.1525 (1.1362)  mae_loss: 0.0246 (0.0256)  classification_loss: 1.1239 (1.1105)  loss_mask: 0.0000 (0.0001)  time: 0.1959  data: 0.0003  max mem: 5511
[00:34:49.585899] Epoch: [82]  [720/781]  eta: 0:00:12  lr: 0.000020  training_loss: 1.1564 (1.1366)  mae_loss: 0.0241 (0.0256)  classification_loss: 1.1323 (1.1109)  loss_mask: 0.0000 (0.0001)  time: 0.1945  data: 0.0002  max mem: 5511
[00:34:53.484511] Epoch: [82]  [740/781]  eta: 0:00:08  lr: 0.000020  training_loss: 1.1347 (1.1367)  mae_loss: 0.0243 (0.0256)  classification_loss: 1.1106 (1.1110)  loss_mask: 0.0001 (0.0001)  time: 0.1948  data: 0.0002  max mem: 5511
[00:34:57.411038] Epoch: [82]  [760/781]  eta: 0:00:04  lr: 0.000020  training_loss: 1.1686 (1.1380)  mae_loss: 0.0250 (0.0256)  classification_loss: 1.1432 (1.1123)  loss_mask: 0.0000 (0.0001)  time: 0.1963  data: 0.0002  max mem: 5511
[00:35:01.384038] Epoch: [82]  [780/781]  eta: 0:00:00  lr: 0.000020  training_loss: 1.1581 (1.1390)  mae_loss: 0.0248 (0.0256)  classification_loss: 1.1281 (1.1134)  loss_mask: 0.0000 (0.0001)  time: 0.1986  data: 0.0002  max mem: 5511
[00:35:01.532718] Epoch: [82] Total time: 0:02:34 (0.1973 s / it)
[00:35:01.533295] Averaged stats: lr: 0.000020  training_loss: 1.1581 (1.1390)  mae_loss: 0.0248 (0.0256)  classification_loss: 1.1281 (1.1134)  loss_mask: 0.0000 (0.0001)
[00:35:02.161057] Test:  [  0/157]  eta: 0:01:37  testing_loss: 0.4954 (0.4954)  acc1: 84.3750 (84.3750)  acc5: 100.0000 (100.0000)  time: 0.6235  data: 0.5928  max mem: 5511
[00:35:02.449091] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4515 (0.4727)  acc1: 85.9375 (86.0795)  acc5: 100.0000 (99.7159)  time: 0.0826  data: 0.0541  max mem: 5511
[00:35:02.738015] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4234 (0.4321)  acc1: 87.5000 (86.9792)  acc5: 100.0000 (99.6280)  time: 0.0287  data: 0.0002  max mem: 5511
[00:35:03.025825] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4491 (0.4491)  acc1: 87.5000 (85.9879)  acc5: 100.0000 (99.3952)  time: 0.0287  data: 0.0002  max mem: 5511
[00:35:03.309843] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4609 (0.4547)  acc1: 87.5000 (86.0899)  acc5: 98.4375 (99.3140)  time: 0.0285  data: 0.0002  max mem: 5511
[00:35:03.598051] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4446 (0.4491)  acc1: 87.5000 (86.4583)  acc5: 100.0000 (99.3260)  time: 0.0284  data: 0.0002  max mem: 5511
[00:35:03.885399] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.3879 (0.4472)  acc1: 87.5000 (86.6803)  acc5: 100.0000 (99.3084)  time: 0.0286  data: 0.0002  max mem: 5511
[00:35:04.169547] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4145 (0.4418)  acc1: 89.0625 (86.9278)  acc5: 100.0000 (99.2958)  time: 0.0284  data: 0.0002  max mem: 5511
[00:35:04.456591] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4328 (0.4485)  acc1: 87.5000 (86.8441)  acc5: 100.0000 (99.3441)  time: 0.0284  data: 0.0003  max mem: 5511
[00:35:04.740900] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4414 (0.4452)  acc1: 89.0625 (87.1223)  acc5: 100.0000 (99.3132)  time: 0.0284  data: 0.0003  max mem: 5511
[00:35:05.026537] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4505 (0.4472)  acc1: 89.0625 (87.1287)  acc5: 98.4375 (99.3193)  time: 0.0283  data: 0.0002  max mem: 5511
[00:35:05.317328] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4562 (0.4464)  acc1: 87.5000 (87.1340)  acc5: 100.0000 (99.3384)  time: 0.0287  data: 0.0002  max mem: 5511
[00:35:05.600422] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4453 (0.4469)  acc1: 87.5000 (87.1126)  acc5: 100.0000 (99.3673)  time: 0.0285  data: 0.0002  max mem: 5511
[00:35:05.881778] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4273 (0.4476)  acc1: 85.9375 (87.0587)  acc5: 100.0000 (99.3798)  time: 0.0281  data: 0.0002  max mem: 5511
[00:35:06.161850] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4237 (0.4460)  acc1: 87.5000 (87.1343)  acc5: 100.0000 (99.4016)  time: 0.0279  data: 0.0001  max mem: 5511
[00:35:06.443078] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4364 (0.4467)  acc1: 85.9375 (87.1378)  acc5: 100.0000 (99.4102)  time: 0.0279  data: 0.0001  max mem: 5511
[00:35:06.594929] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4302 (0.4462)  acc1: 87.5000 (87.1100)  acc5: 100.0000 (99.4000)  time: 0.0271  data: 0.0001  max mem: 5511
[00:35:06.737600] Test: Total time: 0:00:05 (0.0331 s / it)
[00:35:06.738183] * Acc@1 87.110 Acc@5 99.400 loss 0.446
[00:35:06.738489] Accuracy of the network on the 10000 test images: 87.1%
[00:35:06.738671] Max accuracy: 87.11%
[00:35:07.083102] log_dir: ./output_dir
[00:35:07.956537] Epoch: [83]  [  0/781]  eta: 0:11:20  lr: 0.000020  training_loss: 0.9134 (0.9134)  mae_loss: 0.0212 (0.0212)  classification_loss: 0.8922 (0.8922)  loss_mask: 0.0000 (0.0000)  time: 0.8717  data: 0.6438  max mem: 5511
[00:35:11.889952] Epoch: [83]  [ 20/781]  eta: 0:02:54  lr: 0.000020  training_loss: 1.0964 (1.1129)  mae_loss: 0.0255 (0.0262)  classification_loss: 1.0656 (1.0866)  loss_mask: 0.0000 (0.0000)  time: 0.1966  data: 0.0002  max mem: 5511
[00:35:15.823313] Epoch: [83]  [ 40/781]  eta: 0:02:37  lr: 0.000020  training_loss: 1.1232 (1.1279)  mae_loss: 0.0249 (0.0255)  classification_loss: 1.0966 (1.1023)  loss_mask: 0.0000 (0.0000)  time: 0.1966  data: 0.0002  max mem: 5511
[00:35:19.745574] Epoch: [83]  [ 60/781]  eta: 0:02:29  lr: 0.000020  training_loss: 1.1242 (1.1306)  mae_loss: 0.0265 (0.0260)  classification_loss: 1.0977 (1.1046)  loss_mask: 0.0000 (0.0000)  time: 0.1960  data: 0.0002  max mem: 5511
[00:35:23.721786] Epoch: [83]  [ 80/781]  eta: 0:02:23  lr: 0.000020  training_loss: 1.1234 (1.1306)  mae_loss: 0.0242 (0.0258)  classification_loss: 1.0964 (1.1048)  loss_mask: 0.0000 (0.0000)  time: 0.1987  data: 0.0002  max mem: 5511
[00:35:27.637807] Epoch: [83]  [100/781]  eta: 0:02:18  lr: 0.000020  training_loss: 1.1603 (1.1362)  mae_loss: 0.0260 (0.0257)  classification_loss: 1.1331 (1.1104)  loss_mask: 0.0000 (0.0000)  time: 0.1957  data: 0.0002  max mem: 5511
[00:35:31.562704] Epoch: [83]  [120/781]  eta: 0:02:13  lr: 0.000020  training_loss: 1.1264 (1.1345)  mae_loss: 0.0244 (0.0255)  classification_loss: 1.1019 (1.1089)  loss_mask: 0.0000 (0.0000)  time: 0.1962  data: 0.0003  max mem: 5511
[00:35:35.463200] Epoch: [83]  [140/781]  eta: 0:02:08  lr: 0.000020  training_loss: 1.1190 (1.1303)  mae_loss: 0.0260 (0.0256)  classification_loss: 1.0945 (1.1047)  loss_mask: 0.0000 (0.0000)  time: 0.1950  data: 0.0002  max mem: 5511
[00:35:39.408339] Epoch: [83]  [160/781]  eta: 0:02:04  lr: 0.000020  training_loss: 1.1114 (1.1293)  mae_loss: 0.0254 (0.0256)  classification_loss: 1.0859 (1.1037)  loss_mask: 0.0000 (0.0001)  time: 0.1972  data: 0.0002  max mem: 5511
[00:35:43.314644] Epoch: [83]  [180/781]  eta: 0:02:00  lr: 0.000020  training_loss: 1.1125 (1.1285)  mae_loss: 0.0256 (0.0256)  classification_loss: 1.0853 (1.1029)  loss_mask: 0.0000 (0.0001)  time: 0.1952  data: 0.0002  max mem: 5511
[00:35:47.255084] Epoch: [83]  [200/781]  eta: 0:01:56  lr: 0.000020  training_loss: 1.1388 (1.1319)  mae_loss: 0.0251 (0.0256)  classification_loss: 1.1160 (1.1062)  loss_mask: 0.0000 (0.0001)  time: 0.1969  data: 0.0005  max mem: 5511
[00:35:51.167728] Epoch: [83]  [220/781]  eta: 0:01:51  lr: 0.000020  training_loss: 1.1404 (1.1324)  mae_loss: 0.0246 (0.0255)  classification_loss: 1.1121 (1.1068)  loss_mask: 0.0000 (0.0001)  time: 0.1955  data: 0.0002  max mem: 5511
[00:35:55.097279] Epoch: [83]  [240/781]  eta: 0:01:47  lr: 0.000019  training_loss: 1.1428 (1.1337)  mae_loss: 0.0255 (0.0256)  classification_loss: 1.1175 (1.1080)  loss_mask: 0.0000 (0.0001)  time: 0.1964  data: 0.0002  max mem: 5511
[00:35:59.005176] Epoch: [83]  [260/781]  eta: 0:01:43  lr: 0.000019  training_loss: 1.1391 (1.1342)  mae_loss: 0.0240 (0.0255)  classification_loss: 1.1167 (1.1086)  loss_mask: 0.0000 (0.0001)  time: 0.1953  data: 0.0002  max mem: 5511
[00:36:02.930874] Epoch: [83]  [280/781]  eta: 0:01:39  lr: 0.000019  training_loss: 1.1294 (1.1332)  mae_loss: 0.0253 (0.0256)  classification_loss: 1.1069 (1.1076)  loss_mask: 0.0000 (0.0001)  time: 0.1962  data: 0.0003  max mem: 5511
[00:36:06.862717] Epoch: [83]  [300/781]  eta: 0:01:35  lr: 0.000019  training_loss: 1.1407 (1.1342)  mae_loss: 0.0239 (0.0256)  classification_loss: 1.1190 (1.1085)  loss_mask: 0.0000 (0.0001)  time: 0.1965  data: 0.0002  max mem: 5511
[00:36:10.775385] Epoch: [83]  [320/781]  eta: 0:01:31  lr: 0.000019  training_loss: 1.0790 (1.1326)  mae_loss: 0.0244 (0.0256)  classification_loss: 1.0504 (1.1070)  loss_mask: 0.0000 (0.0001)  time: 0.1956  data: 0.0002  max mem: 5511
[00:36:14.686817] Epoch: [83]  [340/781]  eta: 0:01:27  lr: 0.000019  training_loss: 1.1250 (1.1327)  mae_loss: 0.0268 (0.0256)  classification_loss: 1.1007 (1.1070)  loss_mask: 0.0000 (0.0001)  time: 0.1955  data: 0.0002  max mem: 5511
[00:36:18.588035] Epoch: [83]  [360/781]  eta: 0:01:23  lr: 0.000019  training_loss: 1.1005 (1.1319)  mae_loss: 0.0263 (0.0257)  classification_loss: 1.0788 (1.1062)  loss_mask: 0.0000 (0.0001)  time: 0.1950  data: 0.0002  max mem: 5511
[00:36:22.514481] Epoch: [83]  [380/781]  eta: 0:01:19  lr: 0.000019  training_loss: 1.1109 (1.1316)  mae_loss: 0.0252 (0.0257)  classification_loss: 1.0835 (1.1059)  loss_mask: 0.0000 (0.0001)  time: 0.1962  data: 0.0002  max mem: 5511
[00:36:26.451170] Epoch: [83]  [400/781]  eta: 0:01:15  lr: 0.000019  training_loss: 1.1083 (1.1319)  mae_loss: 0.0255 (0.0257)  classification_loss: 1.0828 (1.1062)  loss_mask: 0.0000 (0.0001)  time: 0.1967  data: 0.0002  max mem: 5511
[00:36:30.389930] Epoch: [83]  [420/781]  eta: 0:01:11  lr: 0.000019  training_loss: 1.1695 (1.1325)  mae_loss: 0.0247 (0.0256)  classification_loss: 1.1457 (1.1068)  loss_mask: 0.0000 (0.0001)  time: 0.1969  data: 0.0003  max mem: 5511
[00:36:34.319421] Epoch: [83]  [440/781]  eta: 0:01:07  lr: 0.000019  training_loss: 1.0982 (1.1321)  mae_loss: 0.0249 (0.0256)  classification_loss: 1.0762 (1.1065)  loss_mask: 0.0000 (0.0001)  time: 0.1964  data: 0.0002  max mem: 5511
[00:36:38.217572] Epoch: [83]  [460/781]  eta: 0:01:03  lr: 0.000019  training_loss: 1.1085 (1.1318)  mae_loss: 0.0248 (0.0256)  classification_loss: 1.0829 (1.1062)  loss_mask: 0.0001 (0.0001)  time: 0.1948  data: 0.0002  max mem: 5511
[00:36:42.130709] Epoch: [83]  [480/781]  eta: 0:00:59  lr: 0.000019  training_loss: 1.1378 (1.1319)  mae_loss: 0.0247 (0.0256)  classification_loss: 1.1111 (1.1063)  loss_mask: 0.0000 (0.0001)  time: 0.1956  data: 0.0002  max mem: 5511
[00:36:46.066212] Epoch: [83]  [500/781]  eta: 0:00:55  lr: 0.000019  training_loss: 1.1367 (1.1328)  mae_loss: 0.0247 (0.0255)  classification_loss: 1.1116 (1.1072)  loss_mask: 0.0000 (0.0001)  time: 0.1967  data: 0.0002  max mem: 5511
[00:36:50.016546] Epoch: [83]  [520/781]  eta: 0:00:51  lr: 0.000019  training_loss: 1.0675 (1.1319)  mae_loss: 0.0257 (0.0255)  classification_loss: 1.0412 (1.1063)  loss_mask: 0.0000 (0.0001)  time: 0.1974  data: 0.0002  max mem: 5511
[00:36:53.938412] Epoch: [83]  [540/781]  eta: 0:00:47  lr: 0.000019  training_loss: 1.1557 (1.1329)  mae_loss: 0.0248 (0.0256)  classification_loss: 1.1285 (1.1072)  loss_mask: 0.0000 (0.0001)  time: 0.1960  data: 0.0003  max mem: 5511
[00:36:57.851304] Epoch: [83]  [560/781]  eta: 0:00:43  lr: 0.000019  training_loss: 1.0651 (1.1321)  mae_loss: 0.0250 (0.0255)  classification_loss: 1.0384 (1.1065)  loss_mask: 0.0000 (0.0001)  time: 0.1956  data: 0.0002  max mem: 5511
[00:37:01.768061] Epoch: [83]  [580/781]  eta: 0:00:39  lr: 0.000019  training_loss: 1.1172 (1.1317)  mae_loss: 0.0253 (0.0255)  classification_loss: 1.0900 (1.1061)  loss_mask: 0.0000 (0.0001)  time: 0.1957  data: 0.0003  max mem: 5511
[00:37:05.705209] Epoch: [83]  [600/781]  eta: 0:00:35  lr: 0.000019  training_loss: 1.1339 (1.1319)  mae_loss: 0.0250 (0.0255)  classification_loss: 1.1102 (1.1063)  loss_mask: 0.0001 (0.0001)  time: 0.1968  data: 0.0002  max mem: 5511
[00:37:09.613136] Epoch: [83]  [620/781]  eta: 0:00:31  lr: 0.000018  training_loss: 1.1453 (1.1320)  mae_loss: 0.0273 (0.0256)  classification_loss: 1.1162 (1.1064)  loss_mask: 0.0001 (0.0001)  time: 0.1953  data: 0.0002  max mem: 5511
[00:37:13.544001] Epoch: [83]  [640/781]  eta: 0:00:27  lr: 0.000018  training_loss: 1.1328 (1.1316)  mae_loss: 0.0253 (0.0256)  classification_loss: 1.0977 (1.1060)  loss_mask: 0.0000 (0.0001)  time: 0.1965  data: 0.0003  max mem: 5511
[00:37:17.443840] Epoch: [83]  [660/781]  eta: 0:00:23  lr: 0.000018  training_loss: 1.0929 (1.1312)  mae_loss: 0.0239 (0.0256)  classification_loss: 1.0672 (1.1055)  loss_mask: 0.0000 (0.0001)  time: 0.1949  data: 0.0002  max mem: 5511
[00:37:21.355840] Epoch: [83]  [680/781]  eta: 0:00:19  lr: 0.000018  training_loss: 1.1242 (1.1311)  mae_loss: 0.0253 (0.0256)  classification_loss: 1.0987 (1.1055)  loss_mask: 0.0000 (0.0001)  time: 0.1955  data: 0.0002  max mem: 5511
[00:37:25.310283] Epoch: [83]  [700/781]  eta: 0:00:15  lr: 0.000018  training_loss: 1.1757 (1.1317)  mae_loss: 0.0259 (0.0256)  classification_loss: 1.1486 (1.1061)  loss_mask: 0.0001 (0.0001)  time: 0.1975  data: 0.0002  max mem: 5511
[00:37:29.222124] Epoch: [83]  [720/781]  eta: 0:00:12  lr: 0.000018  training_loss: 1.1591 (1.1328)  mae_loss: 0.0255 (0.0256)  classification_loss: 1.1337 (1.1071)  loss_mask: 0.0000 (0.0001)  time: 0.1955  data: 0.0002  max mem: 5511
[00:37:33.133407] Epoch: [83]  [740/781]  eta: 0:00:08  lr: 0.000018  training_loss: 1.1128 (1.1327)  mae_loss: 0.0245 (0.0256)  classification_loss: 1.0861 (1.1070)  loss_mask: 0.0000 (0.0001)  time: 0.1955  data: 0.0003  max mem: 5511
[00:37:37.090107] Epoch: [83]  [760/781]  eta: 0:00:04  lr: 0.000018  training_loss: 1.1370 (1.1330)  mae_loss: 0.0264 (0.0256)  classification_loss: 1.1109 (1.1073)  loss_mask: 0.0000 (0.0001)  time: 0.1977  data: 0.0003  max mem: 5511
[00:37:41.001161] Epoch: [83]  [780/781]  eta: 0:00:00  lr: 0.000018  training_loss: 1.1791 (1.1338)  mae_loss: 0.0253 (0.0256)  classification_loss: 1.1512 (1.1081)  loss_mask: 0.0000 (0.0001)  time: 0.1955  data: 0.0002  max mem: 5511
[00:37:41.154386] Epoch: [83] Total time: 0:02:34 (0.1973 s / it)
[00:37:41.154884] Averaged stats: lr: 0.000018  training_loss: 1.1791 (1.1338)  mae_loss: 0.0253 (0.0256)  classification_loss: 1.1512 (1.1081)  loss_mask: 0.0000 (0.0001)
[00:37:41.805707] Test:  [  0/157]  eta: 0:01:41  testing_loss: 0.5708 (0.5708)  acc1: 84.3750 (84.3750)  acc5: 100.0000 (100.0000)  time: 0.6461  data: 0.6147  max mem: 5511
[00:37:42.094179] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4484 (0.4818)  acc1: 85.9375 (85.5114)  acc5: 100.0000 (99.7159)  time: 0.0848  data: 0.0561  max mem: 5511
[00:37:42.379688] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4294 (0.4359)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (99.5536)  time: 0.0285  data: 0.0002  max mem: 5511
[00:37:42.666116] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4224 (0.4508)  acc1: 87.5000 (86.6431)  acc5: 100.0000 (99.2440)  time: 0.0285  data: 0.0002  max mem: 5511
[00:37:42.953479] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4304 (0.4546)  acc1: 87.5000 (86.6235)  acc5: 98.4375 (99.1616)  time: 0.0286  data: 0.0002  max mem: 5511
[00:37:43.238499] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4304 (0.4482)  acc1: 87.5000 (86.8873)  acc5: 100.0000 (99.2953)  time: 0.0285  data: 0.0002  max mem: 5511
[00:37:43.525291] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.3961 (0.4457)  acc1: 85.9375 (87.0133)  acc5: 100.0000 (99.3084)  time: 0.0285  data: 0.0002  max mem: 5511
[00:37:43.809268] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4189 (0.4395)  acc1: 87.5000 (87.3019)  acc5: 100.0000 (99.3398)  time: 0.0284  data: 0.0002  max mem: 5511
[00:37:44.101536] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4290 (0.4462)  acc1: 87.5000 (86.9985)  acc5: 100.0000 (99.4020)  time: 0.0287  data: 0.0005  max mem: 5511
[00:37:44.389879] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4339 (0.4427)  acc1: 87.5000 (87.2768)  acc5: 100.0000 (99.3475)  time: 0.0289  data: 0.0004  max mem: 5511
[00:37:44.680387] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4429 (0.4454)  acc1: 87.5000 (87.1906)  acc5: 98.4375 (99.3348)  time: 0.0288  data: 0.0003  max mem: 5511
[00:37:44.969068] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4581 (0.4457)  acc1: 85.9375 (87.2325)  acc5: 100.0000 (99.3243)  time: 0.0288  data: 0.0004  max mem: 5511
[00:37:45.260409] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4643 (0.4460)  acc1: 85.9375 (87.2417)  acc5: 100.0000 (99.3543)  time: 0.0289  data: 0.0003  max mem: 5511
[00:37:45.547446] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4342 (0.4470)  acc1: 85.9375 (87.1780)  acc5: 100.0000 (99.3678)  time: 0.0288  data: 0.0002  max mem: 5511
[00:37:45.830100] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4342 (0.4462)  acc1: 87.5000 (87.2008)  acc5: 100.0000 (99.3905)  time: 0.0284  data: 0.0002  max mem: 5511
[00:37:46.110681] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4610 (0.4466)  acc1: 87.5000 (87.1585)  acc5: 100.0000 (99.3895)  time: 0.0280  data: 0.0001  max mem: 5511
[00:37:46.261595] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4282 (0.4464)  acc1: 85.9375 (87.0800)  acc5: 100.0000 (99.3900)  time: 0.0270  data: 0.0001  max mem: 5511
[00:37:46.427000] Test: Total time: 0:00:05 (0.0336 s / it)
[00:37:46.427419] * Acc@1 87.080 Acc@5 99.390 loss 0.446
[00:37:46.427704] Accuracy of the network on the 10000 test images: 87.1%
[00:37:46.427914] Max accuracy: 87.11%
[00:37:46.732571] log_dir: ./output_dir
[00:37:47.615529] Epoch: [84]  [  0/781]  eta: 0:11:28  lr: 0.000018  training_loss: 0.9897 (0.9897)  mae_loss: 0.0279 (0.0279)  classification_loss: 0.9619 (0.9619)  loss_mask: 0.0000 (0.0000)  time: 0.8811  data: 0.6596  max mem: 5511
[00:37:51.517875] Epoch: [84]  [ 20/781]  eta: 0:02:53  lr: 0.000018  training_loss: 1.0446 (1.0495)  mae_loss: 0.0255 (0.0258)  classification_loss: 1.0223 (1.0237)  loss_mask: 0.0000 (0.0000)  time: 0.1950  data: 0.0002  max mem: 5511
[00:37:55.446170] Epoch: [84]  [ 40/781]  eta: 0:02:37  lr: 0.000018  training_loss: 1.0865 (1.0760)  mae_loss: 0.0243 (0.0254)  classification_loss: 1.0596 (1.0506)  loss_mask: 0.0000 (0.0000)  time: 0.1963  data: 0.0002  max mem: 5511
[00:37:59.333797] Epoch: [84]  [ 60/781]  eta: 0:02:28  lr: 0.000018  training_loss: 1.1696 (1.1004)  mae_loss: 0.0249 (0.0252)  classification_loss: 1.1484 (1.0752)  loss_mask: 0.0000 (0.0000)  time: 0.1943  data: 0.0002  max mem: 5511
[00:38:03.222561] Epoch: [84]  [ 80/781]  eta: 0:02:22  lr: 0.000018  training_loss: 1.1435 (1.1127)  mae_loss: 0.0259 (0.0254)  classification_loss: 1.1160 (1.0872)  loss_mask: 0.0000 (0.0000)  time: 0.1944  data: 0.0002  max mem: 5511
[00:38:07.133948] Epoch: [84]  [100/781]  eta: 0:02:17  lr: 0.000018  training_loss: 1.1359 (1.1186)  mae_loss: 0.0255 (0.0254)  classification_loss: 1.1121 (1.0931)  loss_mask: 0.0000 (0.0000)  time: 0.1955  data: 0.0002  max mem: 5511
[00:38:11.101249] Epoch: [84]  [120/781]  eta: 0:02:13  lr: 0.000018  training_loss: 1.1421 (1.1254)  mae_loss: 0.0242 (0.0253)  classification_loss: 1.1140 (1.1001)  loss_mask: 0.0000 (0.0000)  time: 0.1983  data: 0.0002  max mem: 5511
[00:38:15.010557] Epoch: [84]  [140/781]  eta: 0:02:08  lr: 0.000018  training_loss: 1.1423 (1.1254)  mae_loss: 0.0246 (0.0253)  classification_loss: 1.1201 (1.1001)  loss_mask: 0.0000 (0.0000)  time: 0.1954  data: 0.0002  max mem: 5511
[00:38:18.943962] Epoch: [84]  [160/781]  eta: 0:02:04  lr: 0.000018  training_loss: 1.0935 (1.1257)  mae_loss: 0.0260 (0.0254)  classification_loss: 1.0693 (1.1003)  loss_mask: 0.0000 (0.0000)  time: 0.1966  data: 0.0002  max mem: 5511
[00:38:22.909156] Epoch: [84]  [180/781]  eta: 0:02:00  lr: 0.000018  training_loss: 1.1677 (1.1299)  mae_loss: 0.0253 (0.0255)  classification_loss: 1.1382 (1.1044)  loss_mask: 0.0000 (0.0000)  time: 0.1982  data: 0.0004  max mem: 5511
[00:38:26.853971] Epoch: [84]  [200/781]  eta: 0:01:55  lr: 0.000017  training_loss: 1.1133 (1.1299)  mae_loss: 0.0242 (0.0254)  classification_loss: 1.0891 (1.1045)  loss_mask: 0.0000 (0.0000)  time: 0.1972  data: 0.0003  max mem: 5511
[00:38:30.782806] Epoch: [84]  [220/781]  eta: 0:01:51  lr: 0.000017  training_loss: 1.1191 (1.1302)  mae_loss: 0.0253 (0.0254)  classification_loss: 1.0938 (1.1048)  loss_mask: 0.0000 (0.0000)  time: 0.1964  data: 0.0002  max mem: 5511
[00:38:34.752560] Epoch: [84]  [240/781]  eta: 0:01:47  lr: 0.000017  training_loss: 1.1193 (1.1304)  mae_loss: 0.0247 (0.0254)  classification_loss: 1.0981 (1.1050)  loss_mask: 0.0000 (0.0000)  time: 0.1984  data: 0.0002  max mem: 5511
[00:38:38.704269] Epoch: [84]  [260/781]  eta: 0:01:43  lr: 0.000017  training_loss: 1.1210 (1.1312)  mae_loss: 0.0241 (0.0253)  classification_loss: 1.0967 (1.1058)  loss_mask: 0.0000 (0.0000)  time: 0.1975  data: 0.0002  max mem: 5511
[00:38:42.594011] Epoch: [84]  [280/781]  eta: 0:01:39  lr: 0.000017  training_loss: 1.0877 (1.1307)  mae_loss: 0.0231 (0.0252)  classification_loss: 1.0623 (1.1054)  loss_mask: 0.0000 (0.0000)  time: 0.1944  data: 0.0002  max mem: 5511
[00:38:46.494416] Epoch: [84]  [300/781]  eta: 0:01:35  lr: 0.000017  training_loss: 1.1030 (1.1297)  mae_loss: 0.0258 (0.0253)  classification_loss: 1.0772 (1.1043)  loss_mask: 0.0000 (0.0001)  time: 0.1949  data: 0.0002  max mem: 5511
[00:38:50.390069] Epoch: [84]  [320/781]  eta: 0:01:31  lr: 0.000017  training_loss: 1.1021 (1.1292)  mae_loss: 0.0270 (0.0254)  classification_loss: 1.0795 (1.1037)  loss_mask: 0.0001 (0.0001)  time: 0.1947  data: 0.0002  max mem: 5511
[00:38:54.345436] Epoch: [84]  [340/781]  eta: 0:01:27  lr: 0.000017  training_loss: 1.1129 (1.1283)  mae_loss: 0.0242 (0.0254)  classification_loss: 1.0868 (1.1029)  loss_mask: 0.0000 (0.0001)  time: 0.1977  data: 0.0002  max mem: 5511
[00:38:58.281214] Epoch: [84]  [360/781]  eta: 0:01:23  lr: 0.000017  training_loss: 1.1208 (1.1298)  mae_loss: 0.0246 (0.0254)  classification_loss: 1.0971 (1.1044)  loss_mask: 0.0000 (0.0001)  time: 0.1967  data: 0.0002  max mem: 5511
[00:39:02.211635] Epoch: [84]  [380/781]  eta: 0:01:19  lr: 0.000017  training_loss: 1.1180 (1.1310)  mae_loss: 0.0260 (0.0254)  classification_loss: 1.0895 (1.1055)  loss_mask: 0.0000 (0.0001)  time: 0.1964  data: 0.0003  max mem: 5511
[00:39:06.130556] Epoch: [84]  [400/781]  eta: 0:01:15  lr: 0.000017  training_loss: 1.0911 (1.1302)  mae_loss: 0.0249 (0.0254)  classification_loss: 1.0632 (1.1047)  loss_mask: 0.0000 (0.0001)  time: 0.1959  data: 0.0002  max mem: 5511
[00:39:10.065325] Epoch: [84]  [420/781]  eta: 0:01:11  lr: 0.000017  training_loss: 1.0700 (1.1293)  mae_loss: 0.0258 (0.0255)  classification_loss: 1.0410 (1.1038)  loss_mask: 0.0000 (0.0001)  time: 0.1967  data: 0.0003  max mem: 5511
[00:39:14.009967] Epoch: [84]  [440/781]  eta: 0:01:07  lr: 0.000017  training_loss: 1.0836 (1.1281)  mae_loss: 0.0248 (0.0254)  classification_loss: 1.0576 (1.1027)  loss_mask: 0.0000 (0.0001)  time: 0.1972  data: 0.0002  max mem: 5511
[00:39:17.947511] Epoch: [84]  [460/781]  eta: 0:01:03  lr: 0.000017  training_loss: 1.1265 (1.1287)  mae_loss: 0.0247 (0.0254)  classification_loss: 1.1034 (1.1032)  loss_mask: 0.0000 (0.0001)  time: 0.1968  data: 0.0002  max mem: 5511
[00:39:21.875219] Epoch: [84]  [480/781]  eta: 0:00:59  lr: 0.000017  training_loss: 1.1279 (1.1285)  mae_loss: 0.0240 (0.0254)  classification_loss: 1.1030 (1.1030)  loss_mask: 0.0000 (0.0001)  time: 0.1963  data: 0.0002  max mem: 5511
[00:39:25.773536] Epoch: [84]  [500/781]  eta: 0:00:55  lr: 0.000017  training_loss: 1.1015 (1.1289)  mae_loss: 0.0251 (0.0254)  classification_loss: 1.0730 (1.1035)  loss_mask: 0.0000 (0.0001)  time: 0.1948  data: 0.0002  max mem: 5511
[00:39:29.691140] Epoch: [84]  [520/781]  eta: 0:00:51  lr: 0.000017  training_loss: 1.1120 (1.1285)  mae_loss: 0.0252 (0.0254)  classification_loss: 1.0854 (1.1031)  loss_mask: 0.0000 (0.0001)  time: 0.1958  data: 0.0002  max mem: 5511
[00:39:33.609886] Epoch: [84]  [540/781]  eta: 0:00:47  lr: 0.000017  training_loss: 1.1047 (1.1276)  mae_loss: 0.0253 (0.0254)  classification_loss: 1.0811 (1.1022)  loss_mask: 0.0000 (0.0001)  time: 0.1959  data: 0.0002  max mem: 5511
[00:39:37.557166] Epoch: [84]  [560/781]  eta: 0:00:43  lr: 0.000017  training_loss: 1.0879 (1.1270)  mae_loss: 0.0247 (0.0254)  classification_loss: 1.0667 (1.1016)  loss_mask: 0.0000 (0.0001)  time: 0.1973  data: 0.0002  max mem: 5511
[00:39:41.480214] Epoch: [84]  [580/781]  eta: 0:00:39  lr: 0.000017  training_loss: 1.1195 (1.1266)  mae_loss: 0.0236 (0.0253)  classification_loss: 1.0962 (1.1012)  loss_mask: 0.0000 (0.0001)  time: 0.1961  data: 0.0002  max mem: 5511
[00:39:45.403614] Epoch: [84]  [600/781]  eta: 0:00:35  lr: 0.000016  training_loss: 1.0294 (1.1239)  mae_loss: 0.0252 (0.0254)  classification_loss: 1.0010 (1.0985)  loss_mask: 0.0000 (0.0001)  time: 0.1961  data: 0.0002  max mem: 5511
[00:39:49.325112] Epoch: [84]  [620/781]  eta: 0:00:31  lr: 0.000016  training_loss: 1.0740 (1.1226)  mae_loss: 0.0262 (0.0254)  classification_loss: 1.0472 (1.0972)  loss_mask: 0.0000 (0.0001)  time: 0.1960  data: 0.0002  max mem: 5511
[00:39:53.223300] Epoch: [84]  [640/781]  eta: 0:00:27  lr: 0.000016  training_loss: 1.1169 (1.1231)  mae_loss: 0.0247 (0.0254)  classification_loss: 1.0874 (1.0977)  loss_mask: 0.0000 (0.0001)  time: 0.1948  data: 0.0002  max mem: 5511
[00:39:57.205963] Epoch: [84]  [660/781]  eta: 0:00:23  lr: 0.000016  training_loss: 1.1102 (1.1235)  mae_loss: 0.0239 (0.0254)  classification_loss: 1.0843 (1.0981)  loss_mask: 0.0000 (0.0001)  time: 0.1991  data: 0.0003  max mem: 5511
[00:40:01.125876] Epoch: [84]  [680/781]  eta: 0:00:19  lr: 0.000016  training_loss: 1.1150 (1.1242)  mae_loss: 0.0250 (0.0254)  classification_loss: 1.0934 (1.0987)  loss_mask: 0.0000 (0.0001)  time: 0.1959  data: 0.0003  max mem: 5511
[00:40:05.095418] Epoch: [84]  [700/781]  eta: 0:00:15  lr: 0.000016  training_loss: 1.1095 (1.1235)  mae_loss: 0.0255 (0.0254)  classification_loss: 1.0822 (1.0981)  loss_mask: 0.0001 (0.0001)  time: 0.1984  data: 0.0003  max mem: 5511
[00:40:09.001406] Epoch: [84]  [720/781]  eta: 0:00:12  lr: 0.000016  training_loss: 1.1719 (1.1249)  mae_loss: 0.0251 (0.0254)  classification_loss: 1.1447 (1.0994)  loss_mask: 0.0001 (0.0001)  time: 0.1952  data: 0.0002  max mem: 5511
[00:40:12.901483] Epoch: [84]  [740/781]  eta: 0:00:08  lr: 0.000016  training_loss: 1.1056 (1.1246)  mae_loss: 0.0251 (0.0254)  classification_loss: 1.0805 (1.0992)  loss_mask: 0.0000 (0.0001)  time: 0.1949  data: 0.0002  max mem: 5511
[00:40:16.815718] Epoch: [84]  [760/781]  eta: 0:00:04  lr: 0.000016  training_loss: 1.1295 (1.1247)  mae_loss: 0.0249 (0.0254)  classification_loss: 1.1001 (1.0992)  loss_mask: 0.0000 (0.0001)  time: 0.1956  data: 0.0002  max mem: 5511
[00:40:20.760482] Epoch: [84]  [780/781]  eta: 0:00:00  lr: 0.000016  training_loss: 1.1431 (1.1254)  mae_loss: 0.0252 (0.0254)  classification_loss: 1.1191 (1.1000)  loss_mask: 0.0000 (0.0001)  time: 0.1971  data: 0.0002  max mem: 5511
[00:40:20.913723] Epoch: [84] Total time: 0:02:34 (0.1974 s / it)
[00:40:20.914225] Averaged stats: lr: 0.000016  training_loss: 1.1431 (1.1254)  mae_loss: 0.0252 (0.0254)  classification_loss: 1.1191 (1.1000)  loss_mask: 0.0000 (0.0001)
[00:40:21.544580] Test:  [  0/157]  eta: 0:01:36  testing_loss: 0.5713 (0.5713)  acc1: 85.9375 (85.9375)  acc5: 100.0000 (100.0000)  time: 0.6172  data: 0.5861  max mem: 5511
[00:40:21.845103] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4512 (0.4748)  acc1: 85.9375 (86.2216)  acc5: 100.0000 (99.4318)  time: 0.0831  data: 0.0538  max mem: 5511
[00:40:22.134158] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4202 (0.4315)  acc1: 87.5000 (87.8720)  acc5: 100.0000 (99.4048)  time: 0.0292  data: 0.0004  max mem: 5511
[00:40:22.420539] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4219 (0.4459)  acc1: 87.5000 (87.0464)  acc5: 100.0000 (99.1935)  time: 0.0286  data: 0.0002  max mem: 5511
[00:40:22.709270] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4641 (0.4522)  acc1: 87.5000 (86.9665)  acc5: 98.4375 (99.1616)  time: 0.0286  data: 0.0002  max mem: 5511
[00:40:22.996417] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4484 (0.4475)  acc1: 87.5000 (87.2855)  acc5: 100.0000 (99.2953)  time: 0.0286  data: 0.0002  max mem: 5511
[00:40:23.293395] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.3984 (0.4452)  acc1: 89.0625 (87.5768)  acc5: 100.0000 (99.3084)  time: 0.0291  data: 0.0002  max mem: 5511
[00:40:23.580088] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4156 (0.4403)  acc1: 89.0625 (87.6981)  acc5: 100.0000 (99.3618)  time: 0.0291  data: 0.0003  max mem: 5511
[00:40:23.866782] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4316 (0.4478)  acc1: 87.5000 (87.3071)  acc5: 100.0000 (99.3441)  time: 0.0285  data: 0.0003  max mem: 5511
[00:40:24.151396] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4467 (0.4435)  acc1: 87.5000 (87.5859)  acc5: 100.0000 (99.3304)  time: 0.0284  data: 0.0002  max mem: 5511
[00:40:24.437889] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4456 (0.4459)  acc1: 87.5000 (87.4536)  acc5: 98.4375 (99.3038)  time: 0.0284  data: 0.0002  max mem: 5511
[00:40:24.728423] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4459 (0.4466)  acc1: 87.5000 (87.4437)  acc5: 100.0000 (99.3384)  time: 0.0287  data: 0.0002  max mem: 5511
[00:40:25.013303] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4774 (0.4478)  acc1: 85.9375 (87.3838)  acc5: 100.0000 (99.3285)  time: 0.0286  data: 0.0002  max mem: 5511
[00:40:25.310906] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4295 (0.4493)  acc1: 87.5000 (87.3092)  acc5: 100.0000 (99.3321)  time: 0.0290  data: 0.0002  max mem: 5511
[00:40:25.596777] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4231 (0.4483)  acc1: 87.5000 (87.3227)  acc5: 100.0000 (99.3573)  time: 0.0290  data: 0.0002  max mem: 5511
[00:40:25.877192] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4419 (0.4490)  acc1: 85.9375 (87.2827)  acc5: 100.0000 (99.3688)  time: 0.0282  data: 0.0002  max mem: 5511
[00:40:26.028786] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4122 (0.4482)  acc1: 87.5000 (87.2800)  acc5: 100.0000 (99.3600)  time: 0.0271  data: 0.0001  max mem: 5511
[00:40:26.192669] Test: Total time: 0:00:05 (0.0336 s / it)
[00:40:26.193142] * Acc@1 87.280 Acc@5 99.360 loss 0.448
[00:40:26.193450] Accuracy of the network on the 10000 test images: 87.3%
[00:40:26.193648] Max accuracy: 87.28%
[00:40:26.502930] log_dir: ./output_dir
[00:40:27.393515] Epoch: [85]  [  0/781]  eta: 0:11:34  lr: 0.000016  training_loss: 0.9739 (0.9739)  mae_loss: 0.0239 (0.0239)  classification_loss: 0.9500 (0.9500)  loss_mask: 0.0000 (0.0000)  time: 0.8886  data: 0.6752  max mem: 5511
[00:40:31.336629] Epoch: [85]  [ 20/781]  eta: 0:02:55  lr: 0.000016  training_loss: 1.0995 (1.1207)  mae_loss: 0.0256 (0.0257)  classification_loss: 1.0708 (1.0949)  loss_mask: 0.0000 (0.0001)  time: 0.1971  data: 0.0002  max mem: 5511
[00:40:35.276310] Epoch: [85]  [ 40/781]  eta: 0:02:38  lr: 0.000016  training_loss: 1.1422 (1.1317)  mae_loss: 0.0263 (0.0259)  classification_loss: 1.1159 (1.1058)  loss_mask: 0.0000 (0.0000)  time: 0.1969  data: 0.0002  max mem: 5511
[00:40:39.197580] Epoch: [85]  [ 60/781]  eta: 0:02:29  lr: 0.000016  training_loss: 1.0856 (1.1344)  mae_loss: 0.0254 (0.0260)  classification_loss: 1.0606 (1.1083)  loss_mask: 0.0000 (0.0000)  time: 0.1960  data: 0.0002  max mem: 5511
[00:40:43.152234] Epoch: [85]  [ 80/781]  eta: 0:02:24  lr: 0.000016  training_loss: 1.0647 (1.1275)  mae_loss: 0.0249 (0.0258)  classification_loss: 1.0415 (1.1016)  loss_mask: 0.0000 (0.0000)  time: 0.1977  data: 0.0003  max mem: 5511
[00:40:47.073653] Epoch: [85]  [100/781]  eta: 0:02:18  lr: 0.000016  training_loss: 1.1140 (1.1290)  mae_loss: 0.0261 (0.0258)  classification_loss: 1.0865 (1.1032)  loss_mask: 0.0000 (0.0000)  time: 0.1960  data: 0.0002  max mem: 5511
[00:40:50.981958] Epoch: [85]  [120/781]  eta: 0:02:13  lr: 0.000016  training_loss: 1.1104 (1.1241)  mae_loss: 0.0238 (0.0255)  classification_loss: 1.0809 (1.0986)  loss_mask: 0.0000 (0.0000)  time: 0.1953  data: 0.0002  max mem: 5511
[00:40:54.896164] Epoch: [85]  [140/781]  eta: 0:02:09  lr: 0.000016  training_loss: 1.0424 (1.1178)  mae_loss: 0.0241 (0.0253)  classification_loss: 1.0196 (1.0925)  loss_mask: 0.0000 (0.0000)  time: 0.1956  data: 0.0003  max mem: 5511
[00:40:58.829705] Epoch: [85]  [160/781]  eta: 0:02:04  lr: 0.000016  training_loss: 1.1042 (1.1169)  mae_loss: 0.0257 (0.0253)  classification_loss: 1.0773 (1.0915)  loss_mask: 0.0000 (0.0000)  time: 0.1966  data: 0.0002  max mem: 5511
[00:41:02.743241] Epoch: [85]  [180/781]  eta: 0:02:00  lr: 0.000016  training_loss: 1.1258 (1.1168)  mae_loss: 0.0268 (0.0255)  classification_loss: 1.0982 (1.0913)  loss_mask: 0.0000 (0.0000)  time: 0.1956  data: 0.0003  max mem: 5511
[00:41:06.650360] Epoch: [85]  [200/781]  eta: 0:01:55  lr: 0.000016  training_loss: 1.1100 (1.1180)  mae_loss: 0.0248 (0.0255)  classification_loss: 1.0856 (1.0925)  loss_mask: 0.0001 (0.0000)  time: 0.1953  data: 0.0002  max mem: 5511
[00:41:10.561508] Epoch: [85]  [220/781]  eta: 0:01:51  lr: 0.000015  training_loss: 1.1062 (1.1161)  mae_loss: 0.0248 (0.0254)  classification_loss: 1.0793 (1.0906)  loss_mask: 0.0000 (0.0000)  time: 0.1955  data: 0.0002  max mem: 5511
[00:41:14.502458] Epoch: [85]  [240/781]  eta: 0:01:47  lr: 0.000015  training_loss: 1.1768 (1.1197)  mae_loss: 0.0254 (0.0254)  classification_loss: 1.1474 (1.0942)  loss_mask: 0.0000 (0.0000)  time: 0.1970  data: 0.0002  max mem: 5511
[00:41:18.417617] Epoch: [85]  [260/781]  eta: 0:01:43  lr: 0.000015  training_loss: 1.1239 (1.1209)  mae_loss: 0.0255 (0.0254)  classification_loss: 1.1014 (1.0954)  loss_mask: 0.0000 (0.0000)  time: 0.1957  data: 0.0002  max mem: 5511
[00:41:22.355532] Epoch: [85]  [280/781]  eta: 0:01:39  lr: 0.000015  training_loss: 1.1643 (1.1234)  mae_loss: 0.0258 (0.0254)  classification_loss: 1.1362 (1.0979)  loss_mask: 0.0000 (0.0000)  time: 0.1968  data: 0.0002  max mem: 5511
[00:41:26.275190] Epoch: [85]  [300/781]  eta: 0:01:35  lr: 0.000015  training_loss: 1.1063 (1.1231)  mae_loss: 0.0243 (0.0254)  classification_loss: 1.0851 (1.0976)  loss_mask: 0.0000 (0.0000)  time: 0.1959  data: 0.0006  max mem: 5511
[00:41:30.198650] Epoch: [85]  [320/781]  eta: 0:01:31  lr: 0.000015  training_loss: 1.0742 (1.1211)  mae_loss: 0.0257 (0.0255)  classification_loss: 1.0449 (1.0956)  loss_mask: 0.0000 (0.0000)  time: 0.1961  data: 0.0002  max mem: 5511
[00:41:34.122233] Epoch: [85]  [340/781]  eta: 0:01:27  lr: 0.000015  training_loss: 1.0619 (1.1199)  mae_loss: 0.0261 (0.0256)  classification_loss: 1.0381 (1.0943)  loss_mask: 0.0000 (0.0000)  time: 0.1961  data: 0.0002  max mem: 5511
[00:41:38.028614] Epoch: [85]  [360/781]  eta: 0:01:23  lr: 0.000015  training_loss: 1.1520 (1.1196)  mae_loss: 0.0261 (0.0256)  classification_loss: 1.1245 (1.0939)  loss_mask: 0.0000 (0.0000)  time: 0.1952  data: 0.0002  max mem: 5511
[00:41:41.933649] Epoch: [85]  [380/781]  eta: 0:01:19  lr: 0.000015  training_loss: 1.1237 (1.1200)  mae_loss: 0.0272 (0.0257)  classification_loss: 1.0897 (1.0943)  loss_mask: 0.0000 (0.0000)  time: 0.1952  data: 0.0002  max mem: 5511
[00:41:45.844375] Epoch: [85]  [400/781]  eta: 0:01:15  lr: 0.000015  training_loss: 1.1531 (1.1221)  mae_loss: 0.0267 (0.0257)  classification_loss: 1.1263 (1.0963)  loss_mask: 0.0000 (0.0000)  time: 0.1955  data: 0.0002  max mem: 5511
[00:41:49.735527] Epoch: [85]  [420/781]  eta: 0:01:11  lr: 0.000015  training_loss: 1.1285 (1.1220)  mae_loss: 0.0257 (0.0257)  classification_loss: 1.1032 (1.0962)  loss_mask: 0.0000 (0.0000)  time: 0.1945  data: 0.0002  max mem: 5511
[00:41:53.639937] Epoch: [85]  [440/781]  eta: 0:01:07  lr: 0.000015  training_loss: 1.1114 (1.1225)  mae_loss: 0.0236 (0.0257)  classification_loss: 1.0896 (1.0968)  loss_mask: 0.0000 (0.0001)  time: 0.1951  data: 0.0002  max mem: 5511
[00:41:57.544284] Epoch: [85]  [460/781]  eta: 0:01:03  lr: 0.000015  training_loss: 1.1204 (1.1235)  mae_loss: 0.0248 (0.0257)  classification_loss: 1.0879 (1.0977)  loss_mask: 0.0000 (0.0001)  time: 0.1951  data: 0.0002  max mem: 5511
[00:42:01.469832] Epoch: [85]  [480/781]  eta: 0:00:59  lr: 0.000015  training_loss: 1.1329 (1.1241)  mae_loss: 0.0259 (0.0257)  classification_loss: 1.1044 (1.0983)  loss_mask: 0.0000 (0.0001)  time: 0.1962  data: 0.0003  max mem: 5511
[00:42:05.380377] Epoch: [85]  [500/781]  eta: 0:00:55  lr: 0.000015  training_loss: 1.1383 (1.1249)  mae_loss: 0.0257 (0.0257)  classification_loss: 1.1161 (1.0991)  loss_mask: 0.0000 (0.0001)  time: 0.1954  data: 0.0002  max mem: 5511
[00:42:09.301631] Epoch: [85]  [520/781]  eta: 0:00:51  lr: 0.000015  training_loss: 1.1198 (1.1241)  mae_loss: 0.0261 (0.0257)  classification_loss: 1.0921 (1.0983)  loss_mask: 0.0000 (0.0001)  time: 0.1960  data: 0.0002  max mem: 5511
[00:42:13.207945] Epoch: [85]  [540/781]  eta: 0:00:47  lr: 0.000015  training_loss: 1.1339 (1.1247)  mae_loss: 0.0259 (0.0258)  classification_loss: 1.1113 (1.0988)  loss_mask: 0.0000 (0.0001)  time: 0.1951  data: 0.0002  max mem: 5511
[00:42:17.191819] Epoch: [85]  [560/781]  eta: 0:00:43  lr: 0.000015  training_loss: 1.0881 (1.1236)  mae_loss: 0.0251 (0.0258)  classification_loss: 1.0660 (1.0978)  loss_mask: 0.0000 (0.0001)  time: 0.1991  data: 0.0003  max mem: 5511
[00:42:21.114333] Epoch: [85]  [580/781]  eta: 0:00:39  lr: 0.000015  training_loss: 1.1006 (1.1240)  mae_loss: 0.0246 (0.0257)  classification_loss: 1.0763 (1.0982)  loss_mask: 0.0000 (0.0001)  time: 0.1960  data: 0.0002  max mem: 5511
[00:42:25.036466] Epoch: [85]  [600/781]  eta: 0:00:35  lr: 0.000015  training_loss: 1.0978 (1.1241)  mae_loss: 0.0257 (0.0257)  classification_loss: 1.0720 (1.0983)  loss_mask: 0.0000 (0.0001)  time: 0.1960  data: 0.0002  max mem: 5511
[00:42:28.940495] Epoch: [85]  [620/781]  eta: 0:00:31  lr: 0.000014  training_loss: 1.1141 (1.1240)  mae_loss: 0.0256 (0.0258)  classification_loss: 1.0879 (1.0982)  loss_mask: 0.0000 (0.0001)  time: 0.1951  data: 0.0002  max mem: 5511
[00:42:32.906333] Epoch: [85]  [640/781]  eta: 0:00:27  lr: 0.000014  training_loss: 1.0906 (1.1239)  mae_loss: 0.0248 (0.0257)  classification_loss: 1.0662 (1.0981)  loss_mask: 0.0000 (0.0001)  time: 0.1982  data: 0.0002  max mem: 5511
[00:42:36.835538] Epoch: [85]  [660/781]  eta: 0:00:23  lr: 0.000014  training_loss: 1.1421 (1.1244)  mae_loss: 0.0246 (0.0257)  classification_loss: 1.1165 (1.0986)  loss_mask: 0.0000 (0.0001)  time: 0.1964  data: 0.0002  max mem: 5511
[00:42:40.745002] Epoch: [85]  [680/781]  eta: 0:00:19  lr: 0.000014  training_loss: 1.1206 (1.1243)  mae_loss: 0.0262 (0.0257)  classification_loss: 1.0921 (1.0985)  loss_mask: 0.0000 (0.0001)  time: 0.1954  data: 0.0003  max mem: 5511
[00:42:44.672151] Epoch: [85]  [700/781]  eta: 0:00:15  lr: 0.000014  training_loss: 1.1244 (1.1243)  mae_loss: 0.0252 (0.0257)  classification_loss: 1.1007 (1.0985)  loss_mask: 0.0000 (0.0001)  time: 0.1963  data: 0.0002  max mem: 5511
[00:42:48.600718] Epoch: [85]  [720/781]  eta: 0:00:12  lr: 0.000014  training_loss: 1.1280 (1.1244)  mae_loss: 0.0255 (0.0257)  classification_loss: 1.1025 (1.0986)  loss_mask: 0.0000 (0.0001)  time: 0.1963  data: 0.0002  max mem: 5511
[00:42:52.527622] Epoch: [85]  [740/781]  eta: 0:00:08  lr: 0.000014  training_loss: 1.1076 (1.1236)  mae_loss: 0.0256 (0.0257)  classification_loss: 1.0797 (1.0978)  loss_mask: 0.0000 (0.0001)  time: 0.1963  data: 0.0002  max mem: 5511
[00:42:56.445915] Epoch: [85]  [760/781]  eta: 0:00:04  lr: 0.000014  training_loss: 1.0937 (1.1241)  mae_loss: 0.0257 (0.0258)  classification_loss: 1.0625 (1.0982)  loss_mask: 0.0000 (0.0001)  time: 0.1958  data: 0.0002  max mem: 5511
[00:43:00.365839] Epoch: [85]  [780/781]  eta: 0:00:00  lr: 0.000014  training_loss: 1.1774 (1.1250)  mae_loss: 0.0253 (0.0257)  classification_loss: 1.1485 (1.0992)  loss_mask: 0.0000 (0.0001)  time: 0.1959  data: 0.0002  max mem: 5511
[00:43:00.520308] Epoch: [85] Total time: 0:02:34 (0.1972 s / it)
[00:43:00.520827] Averaged stats: lr: 0.000014  training_loss: 1.1774 (1.1250)  mae_loss: 0.0253 (0.0257)  classification_loss: 1.1485 (1.0992)  loss_mask: 0.0000 (0.0001)
[00:43:01.185602] Test:  [  0/157]  eta: 0:01:43  testing_loss: 0.5507 (0.5507)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.6610  data: 0.6310  max mem: 5511
[00:43:01.473032] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4455 (0.4816)  acc1: 85.9375 (85.6534)  acc5: 100.0000 (99.4318)  time: 0.0861  data: 0.0575  max mem: 5511
[00:43:01.761355] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4370 (0.4362)  acc1: 87.5000 (86.9792)  acc5: 100.0000 (99.4048)  time: 0.0286  data: 0.0002  max mem: 5511
[00:43:02.045659] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4283 (0.4484)  acc1: 87.5000 (86.4919)  acc5: 100.0000 (99.1935)  time: 0.0285  data: 0.0002  max mem: 5511
[00:43:02.327488] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4535 (0.4546)  acc1: 87.5000 (86.5854)  acc5: 98.4375 (99.0854)  time: 0.0282  data: 0.0002  max mem: 5511
[00:43:02.618855] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4535 (0.4485)  acc1: 87.5000 (86.9485)  acc5: 98.4375 (99.1115)  time: 0.0285  data: 0.0002  max mem: 5511
[00:43:02.905351] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4070 (0.4465)  acc1: 87.5000 (87.1414)  acc5: 100.0000 (99.1547)  time: 0.0287  data: 0.0002  max mem: 5511
[00:43:03.192468] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4349 (0.4421)  acc1: 87.5000 (87.3460)  acc5: 100.0000 (99.2077)  time: 0.0285  data: 0.0002  max mem: 5511
[00:43:03.475498] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4473 (0.4480)  acc1: 85.9375 (86.9985)  acc5: 100.0000 (99.2477)  time: 0.0284  data: 0.0002  max mem: 5511
[00:43:03.761619] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4473 (0.4436)  acc1: 85.9375 (87.2596)  acc5: 100.0000 (99.2102)  time: 0.0283  data: 0.0002  max mem: 5511
[00:43:04.049111] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4422 (0.4460)  acc1: 85.9375 (87.0823)  acc5: 98.4375 (99.1955)  time: 0.0285  data: 0.0002  max mem: 5511
[00:43:04.337039] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4563 (0.4459)  acc1: 85.9375 (87.1340)  acc5: 100.0000 (99.2258)  time: 0.0286  data: 0.0002  max mem: 5511
[00:43:04.622067] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4606 (0.4461)  acc1: 87.5000 (87.2159)  acc5: 100.0000 (99.1994)  time: 0.0285  data: 0.0002  max mem: 5511
[00:43:04.906431] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4287 (0.4469)  acc1: 87.5000 (87.1541)  acc5: 98.4375 (99.2009)  time: 0.0283  data: 0.0002  max mem: 5511
[00:43:05.189609] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4242 (0.4457)  acc1: 87.5000 (87.2340)  acc5: 100.0000 (99.2354)  time: 0.0283  data: 0.0002  max mem: 5511
[00:43:05.470520] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4462 (0.4464)  acc1: 87.5000 (87.1896)  acc5: 100.0000 (99.2446)  time: 0.0281  data: 0.0002  max mem: 5511
[00:43:05.622706] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4222 (0.4455)  acc1: 87.5000 (87.2100)  acc5: 100.0000 (99.2400)  time: 0.0271  data: 0.0001  max mem: 5511
[00:43:05.790952] Test: Total time: 0:00:05 (0.0335 s / it)
[00:43:05.791411] * Acc@1 87.210 Acc@5 99.240 loss 0.446
[00:43:05.791818] Accuracy of the network on the 10000 test images: 87.2%
[00:43:05.792051] Max accuracy: 87.28%
[00:43:06.126496] log_dir: ./output_dir
[00:43:06.965164] Epoch: [86]  [  0/781]  eta: 0:10:53  lr: 0.000014  training_loss: 1.0669 (1.0669)  mae_loss: 0.0280 (0.0280)  classification_loss: 1.0389 (1.0389)  loss_mask: 0.0000 (0.0000)  time: 0.8368  data: 0.6303  max mem: 5511
[00:43:10.881438] Epoch: [86]  [ 20/781]  eta: 0:02:52  lr: 0.000014  training_loss: 1.0714 (1.1083)  mae_loss: 0.0256 (0.0254)  classification_loss: 1.0485 (1.0828)  loss_mask: 0.0000 (0.0000)  time: 0.1957  data: 0.0002  max mem: 5511
[00:43:14.803169] Epoch: [86]  [ 40/781]  eta: 0:02:36  lr: 0.000014  training_loss: 1.1045 (1.1092)  mae_loss: 0.0252 (0.0253)  classification_loss: 1.0783 (1.0838)  loss_mask: 0.0000 (0.0000)  time: 0.1960  data: 0.0002  max mem: 5511
[00:43:18.713349] Epoch: [86]  [ 60/781]  eta: 0:02:28  lr: 0.000014  training_loss: 1.1487 (1.1184)  mae_loss: 0.0252 (0.0257)  classification_loss: 1.1189 (1.0927)  loss_mask: 0.0000 (0.0000)  time: 0.1954  data: 0.0002  max mem: 5511
[00:43:22.624038] Epoch: [86]  [ 80/781]  eta: 0:02:22  lr: 0.000014  training_loss: 1.1064 (1.1249)  mae_loss: 0.0257 (0.0256)  classification_loss: 1.0836 (1.0992)  loss_mask: 0.0000 (0.0000)  time: 0.1954  data: 0.0001  max mem: 5511
[00:43:26.524486] Epoch: [86]  [100/781]  eta: 0:02:17  lr: 0.000014  training_loss: 1.1768 (1.1328)  mae_loss: 0.0244 (0.0255)  classification_loss: 1.1461 (1.1072)  loss_mask: 0.0000 (0.0001)  time: 0.1949  data: 0.0003  max mem: 5511
[00:43:30.435604] Epoch: [86]  [120/781]  eta: 0:02:12  lr: 0.000014  training_loss: 1.1047 (1.1314)  mae_loss: 0.0233 (0.0254)  classification_loss: 1.0824 (1.1059)  loss_mask: 0.0001 (0.0001)  time: 0.1955  data: 0.0002  max mem: 5511
[00:43:34.367842] Epoch: [86]  [140/781]  eta: 0:02:08  lr: 0.000014  training_loss: 1.1171 (1.1276)  mae_loss: 0.0254 (0.0254)  classification_loss: 1.0872 (1.1022)  loss_mask: 0.0000 (0.0001)  time: 0.1965  data: 0.0002  max mem: 5511
[00:43:38.279560] Epoch: [86]  [160/781]  eta: 0:02:03  lr: 0.000014  training_loss: 1.0817 (1.1212)  mae_loss: 0.0256 (0.0255)  classification_loss: 1.0554 (1.0957)  loss_mask: 0.0000 (0.0001)  time: 0.1955  data: 0.0002  max mem: 5511
[00:43:42.214108] Epoch: [86]  [180/781]  eta: 0:01:59  lr: 0.000014  training_loss: 1.1158 (1.1191)  mae_loss: 0.0248 (0.0254)  classification_loss: 1.0888 (1.0936)  loss_mask: 0.0001 (0.0001)  time: 0.1966  data: 0.0003  max mem: 5511
[00:43:46.232349] Epoch: [86]  [200/781]  eta: 0:01:55  lr: 0.000014  training_loss: 1.1118 (1.1191)  mae_loss: 0.0252 (0.0254)  classification_loss: 1.0793 (1.0936)  loss_mask: 0.0001 (0.0001)  time: 0.2008  data: 0.0003  max mem: 5511
[00:43:50.119629] Epoch: [86]  [220/781]  eta: 0:01:51  lr: 0.000014  training_loss: 1.1097 (1.1189)  mae_loss: 0.0240 (0.0253)  classification_loss: 1.0885 (1.0935)  loss_mask: 0.0000 (0.0001)  time: 0.1943  data: 0.0002  max mem: 5511
[00:43:54.071599] Epoch: [86]  [240/781]  eta: 0:01:47  lr: 0.000014  training_loss: 1.1671 (1.1222)  mae_loss: 0.0252 (0.0254)  classification_loss: 1.1382 (1.0968)  loss_mask: 0.0000 (0.0001)  time: 0.1975  data: 0.0002  max mem: 5511
[00:43:57.989116] Epoch: [86]  [260/781]  eta: 0:01:43  lr: 0.000014  training_loss: 1.1549 (1.1235)  mae_loss: 0.0260 (0.0254)  classification_loss: 1.1234 (1.0980)  loss_mask: 0.0000 (0.0001)  time: 0.1957  data: 0.0002  max mem: 5511
[00:44:01.910136] Epoch: [86]  [280/781]  eta: 0:01:39  lr: 0.000013  training_loss: 1.1088 (1.1225)  mae_loss: 0.0242 (0.0254)  classification_loss: 1.0825 (1.0971)  loss_mask: 0.0000 (0.0001)  time: 0.1960  data: 0.0003  max mem: 5511
[00:44:05.860169] Epoch: [86]  [300/781]  eta: 0:01:35  lr: 0.000013  training_loss: 1.0802 (1.1206)  mae_loss: 0.0248 (0.0254)  classification_loss: 1.0540 (1.0952)  loss_mask: 0.0000 (0.0001)  time: 0.1974  data: 0.0002  max mem: 5511
[00:44:09.770620] Epoch: [86]  [320/781]  eta: 0:01:31  lr: 0.000013  training_loss: 1.1077 (1.1206)  mae_loss: 0.0260 (0.0254)  classification_loss: 1.0812 (1.0951)  loss_mask: 0.0000 (0.0001)  time: 0.1954  data: 0.0002  max mem: 5511
[00:44:13.678691] Epoch: [86]  [340/781]  eta: 0:01:27  lr: 0.000013  training_loss: 1.1191 (1.1202)  mae_loss: 0.0261 (0.0254)  classification_loss: 1.0947 (1.0947)  loss_mask: 0.0000 (0.0001)  time: 0.1953  data: 0.0002  max mem: 5511
[00:44:17.621984] Epoch: [86]  [360/781]  eta: 0:01:23  lr: 0.000013  training_loss: 1.1048 (1.1199)  mae_loss: 0.0246 (0.0254)  classification_loss: 1.0818 (1.0944)  loss_mask: 0.0000 (0.0001)  time: 0.1971  data: 0.0002  max mem: 5511
[00:44:21.552008] Epoch: [86]  [380/781]  eta: 0:01:19  lr: 0.000013  training_loss: 1.1252 (1.1205)  mae_loss: 0.0250 (0.0255)  classification_loss: 1.0955 (1.0949)  loss_mask: 0.0000 (0.0001)  time: 0.1964  data: 0.0002  max mem: 5511
[00:44:25.486316] Epoch: [86]  [400/781]  eta: 0:01:15  lr: 0.000013  training_loss: 1.1480 (1.1222)  mae_loss: 0.0257 (0.0255)  classification_loss: 1.1234 (1.0967)  loss_mask: 0.0000 (0.0001)  time: 0.1966  data: 0.0005  max mem: 5511
[00:44:29.420364] Epoch: [86]  [420/781]  eta: 0:01:11  lr: 0.000013  training_loss: 1.1270 (1.1223)  mae_loss: 0.0261 (0.0255)  classification_loss: 1.0974 (1.0967)  loss_mask: 0.0000 (0.0001)  time: 0.1966  data: 0.0002  max mem: 5511
[00:44:33.356163] Epoch: [86]  [440/781]  eta: 0:01:07  lr: 0.000013  training_loss: 1.1064 (1.1232)  mae_loss: 0.0260 (0.0255)  classification_loss: 1.0756 (1.0976)  loss_mask: 0.0000 (0.0001)  time: 0.1967  data: 0.0003  max mem: 5511
[00:44:37.269445] Epoch: [86]  [460/781]  eta: 0:01:03  lr: 0.000013  training_loss: 1.1080 (1.1223)  mae_loss: 0.0264 (0.0255)  classification_loss: 1.0841 (1.0967)  loss_mask: 0.0000 (0.0001)  time: 0.1956  data: 0.0002  max mem: 5511
[00:44:41.167804] Epoch: [86]  [480/781]  eta: 0:00:59  lr: 0.000013  training_loss: 1.0953 (1.1220)  mae_loss: 0.0255 (0.0255)  classification_loss: 1.0698 (1.0964)  loss_mask: 0.0000 (0.0001)  time: 0.1948  data: 0.0002  max mem: 5511
[00:44:45.077373] Epoch: [86]  [500/781]  eta: 0:00:55  lr: 0.000013  training_loss: 1.1131 (1.1220)  mae_loss: 0.0242 (0.0255)  classification_loss: 1.0906 (1.0964)  loss_mask: 0.0000 (0.0001)  time: 0.1953  data: 0.0002  max mem: 5511
[00:44:48.986728] Epoch: [86]  [520/781]  eta: 0:00:51  lr: 0.000013  training_loss: 1.1065 (1.1216)  mae_loss: 0.0255 (0.0255)  classification_loss: 1.0783 (1.0960)  loss_mask: 0.0000 (0.0001)  time: 0.1954  data: 0.0002  max mem: 5511
[00:44:52.909485] Epoch: [86]  [540/781]  eta: 0:00:47  lr: 0.000013  training_loss: 1.1214 (1.1216)  mae_loss: 0.0258 (0.0256)  classification_loss: 1.0962 (1.0960)  loss_mask: 0.0000 (0.0001)  time: 0.1961  data: 0.0002  max mem: 5511
[00:44:56.834952] Epoch: [86]  [560/781]  eta: 0:00:43  lr: 0.000013  training_loss: 1.0862 (1.1215)  mae_loss: 0.0258 (0.0256)  classification_loss: 1.0635 (1.0959)  loss_mask: 0.0000 (0.0001)  time: 0.1962  data: 0.0002  max mem: 5511
[00:45:00.790396] Epoch: [86]  [580/781]  eta: 0:00:39  lr: 0.000013  training_loss: 1.0791 (1.1213)  mae_loss: 0.0248 (0.0256)  classification_loss: 1.0506 (1.0957)  loss_mask: 0.0000 (0.0001)  time: 0.1977  data: 0.0003  max mem: 5511
[00:45:04.726319] Epoch: [86]  [600/781]  eta: 0:00:35  lr: 0.000013  training_loss: 1.1069 (1.1216)  mae_loss: 0.0255 (0.0255)  classification_loss: 1.0852 (1.0960)  loss_mask: 0.0000 (0.0001)  time: 0.1967  data: 0.0002  max mem: 5511
[00:45:08.659583] Epoch: [86]  [620/781]  eta: 0:00:31  lr: 0.000013  training_loss: 1.1392 (1.1216)  mae_loss: 0.0265 (0.0256)  classification_loss: 1.1164 (1.0960)  loss_mask: 0.0000 (0.0001)  time: 0.1966  data: 0.0002  max mem: 5511
[00:45:12.582266] Epoch: [86]  [640/781]  eta: 0:00:27  lr: 0.000013  training_loss: 1.1088 (1.1208)  mae_loss: 0.0245 (0.0256)  classification_loss: 1.0797 (1.0952)  loss_mask: 0.0000 (0.0001)  time: 0.1960  data: 0.0002  max mem: 5511
[00:45:16.579115] Epoch: [86]  [660/781]  eta: 0:00:23  lr: 0.000013  training_loss: 1.0830 (1.1206)  mae_loss: 0.0230 (0.0255)  classification_loss: 1.0608 (1.0950)  loss_mask: 0.0000 (0.0001)  time: 0.1998  data: 0.0002  max mem: 5511
[00:45:20.523323] Epoch: [86]  [680/781]  eta: 0:00:19  lr: 0.000013  training_loss: 1.1173 (1.1201)  mae_loss: 0.0251 (0.0255)  classification_loss: 1.0936 (1.0945)  loss_mask: 0.0000 (0.0001)  time: 0.1971  data: 0.0002  max mem: 5511
[00:45:24.433466] Epoch: [86]  [700/781]  eta: 0:00:15  lr: 0.000013  training_loss: 1.1666 (1.1214)  mae_loss: 0.0270 (0.0256)  classification_loss: 1.1386 (1.0957)  loss_mask: 0.0000 (0.0001)  time: 0.1954  data: 0.0002  max mem: 5511
[00:45:28.398955] Epoch: [86]  [720/781]  eta: 0:00:12  lr: 0.000012  training_loss: 1.0977 (1.1216)  mae_loss: 0.0239 (0.0255)  classification_loss: 1.0692 (1.0960)  loss_mask: 0.0000 (0.0001)  time: 0.1982  data: 0.0002  max mem: 5511
[00:45:32.322396] Epoch: [86]  [740/781]  eta: 0:00:08  lr: 0.000012  training_loss: 1.1322 (1.1217)  mae_loss: 0.0257 (0.0255)  classification_loss: 1.1083 (1.0961)  loss_mask: 0.0000 (0.0001)  time: 0.1961  data: 0.0002  max mem: 5511
[00:45:36.248944] Epoch: [86]  [760/781]  eta: 0:00:04  lr: 0.000012  training_loss: 1.1107 (1.1218)  mae_loss: 0.0253 (0.0255)  classification_loss: 1.0851 (1.0962)  loss_mask: 0.0000 (0.0001)  time: 0.1962  data: 0.0002  max mem: 5511
[00:45:40.183746] Epoch: [86]  [780/781]  eta: 0:00:00  lr: 0.000012  training_loss: 1.0859 (1.1218)  mae_loss: 0.0247 (0.0255)  classification_loss: 1.0590 (1.0962)  loss_mask: 0.0000 (0.0001)  time: 0.1967  data: 0.0002  max mem: 5511
[00:45:40.330082] Epoch: [86] Total time: 0:02:34 (0.1974 s / it)
[00:45:40.330636] Averaged stats: lr: 0.000012  training_loss: 1.0859 (1.1218)  mae_loss: 0.0247 (0.0255)  classification_loss: 1.0590 (1.0962)  loss_mask: 0.0000 (0.0001)
[00:45:40.970197] Test:  [  0/157]  eta: 0:01:39  testing_loss: 0.5014 (0.5014)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.6350  data: 0.6054  max mem: 5511
[00:45:41.255649] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4222 (0.4602)  acc1: 87.5000 (86.3636)  acc5: 100.0000 (99.5739)  time: 0.0835  data: 0.0552  max mem: 5511
[00:45:41.545243] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4177 (0.4206)  acc1: 89.0625 (87.6488)  acc5: 100.0000 (99.5536)  time: 0.0286  data: 0.0002  max mem: 5511
[00:45:41.833596] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4170 (0.4368)  acc1: 89.0625 (87.1976)  acc5: 100.0000 (99.3952)  time: 0.0287  data: 0.0003  max mem: 5511
[00:45:42.117216] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4339 (0.4437)  acc1: 85.9375 (87.0808)  acc5: 98.4375 (99.1616)  time: 0.0284  data: 0.0003  max mem: 5511
[00:45:42.403815] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4339 (0.4378)  acc1: 87.5000 (87.2549)  acc5: 100.0000 (99.2647)  time: 0.0284  data: 0.0003  max mem: 5511
[00:45:42.690950] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4117 (0.4370)  acc1: 89.0625 (87.2951)  acc5: 100.0000 (99.2572)  time: 0.0285  data: 0.0003  max mem: 5511
[00:45:42.977872] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4151 (0.4326)  acc1: 87.5000 (87.4560)  acc5: 100.0000 (99.3398)  time: 0.0285  data: 0.0003  max mem: 5511
[00:45:43.260367] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4317 (0.4407)  acc1: 85.9375 (87.0949)  acc5: 100.0000 (99.3441)  time: 0.0283  data: 0.0003  max mem: 5511
[00:45:43.547164] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4300 (0.4356)  acc1: 87.5000 (87.4828)  acc5: 100.0000 (99.3132)  time: 0.0283  data: 0.0002  max mem: 5511
[00:45:43.834043] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4316 (0.4380)  acc1: 89.0625 (87.3608)  acc5: 98.4375 (99.2884)  time: 0.0286  data: 0.0002  max mem: 5511
[00:45:44.118381] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4496 (0.4376)  acc1: 85.9375 (87.4015)  acc5: 98.4375 (99.2680)  time: 0.0284  data: 0.0002  max mem: 5511
[00:45:44.408345] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4688 (0.4386)  acc1: 85.9375 (87.4096)  acc5: 98.4375 (99.2510)  time: 0.0286  data: 0.0002  max mem: 5511
[00:45:44.690642] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4404 (0.4404)  acc1: 85.9375 (87.3330)  acc5: 100.0000 (99.2724)  time: 0.0285  data: 0.0002  max mem: 5511
[00:45:44.979637] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4218 (0.4395)  acc1: 85.9375 (87.3338)  acc5: 100.0000 (99.3019)  time: 0.0284  data: 0.0002  max mem: 5511
[00:45:45.259820] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4548 (0.4403)  acc1: 85.9375 (87.2827)  acc5: 100.0000 (99.2964)  time: 0.0283  data: 0.0002  max mem: 5511
[00:45:45.410998] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4153 (0.4396)  acc1: 85.9375 (87.2600)  acc5: 100.0000 (99.3000)  time: 0.0271  data: 0.0001  max mem: 5511
[00:45:45.573762] Test: Total time: 0:00:05 (0.0334 s / it)
[00:45:45.574185] * Acc@1 87.260 Acc@5 99.300 loss 0.440
[00:45:45.574455] Accuracy of the network on the 10000 test images: 87.3%
[00:45:45.574618] Max accuracy: 87.28%
[00:45:45.913833] log_dir: ./output_dir
[00:45:46.762858] Epoch: [87]  [  0/781]  eta: 0:11:01  lr: 0.000012  training_loss: 1.0242 (1.0242)  mae_loss: 0.0245 (0.0245)  classification_loss: 0.9996 (0.9996)  loss_mask: 0.0001 (0.0001)  time: 0.8471  data: 0.6023  max mem: 5511
[00:45:50.693553] Epoch: [87]  [ 20/781]  eta: 0:02:53  lr: 0.000012  training_loss: 1.0635 (1.0740)  mae_loss: 0.0246 (0.0259)  classification_loss: 1.0391 (1.0480)  loss_mask: 0.0000 (0.0000)  time: 0.1964  data: 0.0004  max mem: 5511
[00:45:54.625175] Epoch: [87]  [ 40/781]  eta: 0:02:37  lr: 0.000012  training_loss: 1.1111 (1.0997)  mae_loss: 0.0259 (0.0256)  classification_loss: 1.0846 (1.0740)  loss_mask: 0.0000 (0.0000)  time: 0.1965  data: 0.0003  max mem: 5511
[00:45:58.537830] Epoch: [87]  [ 60/781]  eta: 0:02:29  lr: 0.000012  training_loss: 1.1154 (1.1043)  mae_loss: 0.0259 (0.0257)  classification_loss: 1.0906 (1.0785)  loss_mask: 0.0000 (0.0000)  time: 0.1956  data: 0.0002  max mem: 5511
[00:46:02.466162] Epoch: [87]  [ 80/781]  eta: 0:02:23  lr: 0.000012  training_loss: 1.0687 (1.1017)  mae_loss: 0.0261 (0.0257)  classification_loss: 1.0438 (1.0759)  loss_mask: 0.0000 (0.0000)  time: 0.1963  data: 0.0002  max mem: 5511
[00:46:06.402778] Epoch: [87]  [100/781]  eta: 0:02:18  lr: 0.000012  training_loss: 1.1570 (1.1135)  mae_loss: 0.0245 (0.0255)  classification_loss: 1.1320 (1.0880)  loss_mask: 0.0000 (0.0000)  time: 0.1967  data: 0.0002  max mem: 5511
[00:46:10.321025] Epoch: [87]  [120/781]  eta: 0:02:13  lr: 0.000012  training_loss: 1.1102 (1.1122)  mae_loss: 0.0254 (0.0256)  classification_loss: 1.0831 (1.0865)  loss_mask: 0.0000 (0.0001)  time: 0.1958  data: 0.0002  max mem: 5511
[00:46:14.259079] Epoch: [87]  [140/781]  eta: 0:02:08  lr: 0.000012  training_loss: 1.1188 (1.1142)  mae_loss: 0.0250 (0.0256)  classification_loss: 1.0923 (1.0886)  loss_mask: 0.0001 (0.0001)  time: 0.1968  data: 0.0003  max mem: 5511
[00:46:18.166498] Epoch: [87]  [160/781]  eta: 0:02:04  lr: 0.000012  training_loss: 1.0384 (1.1075)  mae_loss: 0.0257 (0.0256)  classification_loss: 1.0106 (1.0818)  loss_mask: 0.0000 (0.0001)  time: 0.1953  data: 0.0002  max mem: 5511
[00:46:22.097178] Epoch: [87]  [180/781]  eta: 0:02:00  lr: 0.000012  training_loss: 1.1296 (1.1104)  mae_loss: 0.0255 (0.0256)  classification_loss: 1.1023 (1.0847)  loss_mask: 0.0000 (0.0001)  time: 0.1964  data: 0.0002  max mem: 5511
[00:46:26.024014] Epoch: [87]  [200/781]  eta: 0:01:55  lr: 0.000012  training_loss: 1.1018 (1.1124)  mae_loss: 0.0238 (0.0255)  classification_loss: 1.0803 (1.0868)  loss_mask: 0.0000 (0.0001)  time: 0.1963  data: 0.0002  max mem: 5511
[00:46:30.002608] Epoch: [87]  [220/781]  eta: 0:01:51  lr: 0.000012  training_loss: 1.1554 (1.1153)  mae_loss: 0.0245 (0.0254)  classification_loss: 1.1291 (1.0898)  loss_mask: 0.0001 (0.0001)  time: 0.1988  data: 0.0002  max mem: 5511
[00:46:33.983414] Epoch: [87]  [240/781]  eta: 0:01:47  lr: 0.000012  training_loss: 1.1628 (1.1178)  mae_loss: 0.0261 (0.0255)  classification_loss: 1.1329 (1.0922)  loss_mask: 0.0000 (0.0001)  time: 0.1987  data: 0.0002  max mem: 5511
[00:46:37.913106] Epoch: [87]  [260/781]  eta: 0:01:43  lr: 0.000012  training_loss: 1.1084 (1.1179)  mae_loss: 0.0266 (0.0256)  classification_loss: 1.0868 (1.0923)  loss_mask: 0.0000 (0.0001)  time: 0.1964  data: 0.0003  max mem: 5511
[00:46:41.844877] Epoch: [87]  [280/781]  eta: 0:01:39  lr: 0.000012  training_loss: 1.1125 (1.1190)  mae_loss: 0.0250 (0.0255)  classification_loss: 1.0863 (1.0934)  loss_mask: 0.0000 (0.0001)  time: 0.1965  data: 0.0002  max mem: 5511
[00:46:45.821699] Epoch: [87]  [300/781]  eta: 0:01:35  lr: 0.000012  training_loss: 1.1094 (1.1185)  mae_loss: 0.0252 (0.0255)  classification_loss: 1.0823 (1.0929)  loss_mask: 0.0000 (0.0001)  time: 0.1987  data: 0.0002  max mem: 5511
[00:46:49.719538] Epoch: [87]  [320/781]  eta: 0:01:31  lr: 0.000012  training_loss: 1.1194 (1.1190)  mae_loss: 0.0255 (0.0255)  classification_loss: 1.0939 (1.0933)  loss_mask: 0.0000 (0.0001)  time: 0.1948  data: 0.0003  max mem: 5511
[00:46:53.641326] Epoch: [87]  [340/781]  eta: 0:01:27  lr: 0.000012  training_loss: 1.1068 (1.1201)  mae_loss: 0.0260 (0.0255)  classification_loss: 1.0815 (1.0944)  loss_mask: 0.0001 (0.0001)  time: 0.1960  data: 0.0003  max mem: 5511
[00:46:57.563081] Epoch: [87]  [360/781]  eta: 0:01:23  lr: 0.000012  training_loss: 1.1177 (1.1208)  mae_loss: 0.0247 (0.0255)  classification_loss: 1.0913 (1.0951)  loss_mask: 0.0001 (0.0001)  time: 0.1960  data: 0.0002  max mem: 5511
[00:47:01.488648] Epoch: [87]  [380/781]  eta: 0:01:19  lr: 0.000012  training_loss: 1.0942 (1.1202)  mae_loss: 0.0252 (0.0255)  classification_loss: 1.0696 (1.0946)  loss_mask: 0.0001 (0.0001)  time: 0.1961  data: 0.0003  max mem: 5511
[00:47:05.399378] Epoch: [87]  [400/781]  eta: 0:01:15  lr: 0.000011  training_loss: 1.1020 (1.1202)  mae_loss: 0.0246 (0.0255)  classification_loss: 1.0784 (1.0945)  loss_mask: 0.0000 (0.0001)  time: 0.1955  data: 0.0002  max mem: 5511
[00:47:09.306643] Epoch: [87]  [420/781]  eta: 0:01:11  lr: 0.000011  training_loss: 1.1113 (1.1200)  mae_loss: 0.0252 (0.0255)  classification_loss: 1.0816 (1.0943)  loss_mask: 0.0000 (0.0001)  time: 0.1953  data: 0.0002  max mem: 5511
[00:47:13.250479] Epoch: [87]  [440/781]  eta: 0:01:07  lr: 0.000011  training_loss: 1.1080 (1.1200)  mae_loss: 0.0244 (0.0255)  classification_loss: 1.0850 (1.0944)  loss_mask: 0.0000 (0.0001)  time: 0.1971  data: 0.0003  max mem: 5511
[00:47:17.174343] Epoch: [87]  [460/781]  eta: 0:01:03  lr: 0.000011  training_loss: 1.0706 (1.1195)  mae_loss: 0.0238 (0.0255)  classification_loss: 1.0418 (1.0938)  loss_mask: 0.0000 (0.0001)  time: 0.1961  data: 0.0001  max mem: 5511
[00:47:21.116680] Epoch: [87]  [480/781]  eta: 0:00:59  lr: 0.000011  training_loss: 1.1318 (1.1196)  mae_loss: 0.0249 (0.0255)  classification_loss: 1.1106 (1.0940)  loss_mask: 0.0000 (0.0001)  time: 0.1970  data: 0.0003  max mem: 5511
[00:47:25.047656] Epoch: [87]  [500/781]  eta: 0:00:55  lr: 0.000011  training_loss: 1.1238 (1.1201)  mae_loss: 0.0265 (0.0256)  classification_loss: 1.0968 (1.0944)  loss_mask: 0.0000 (0.0001)  time: 0.1965  data: 0.0002  max mem: 5511
[00:47:28.993560] Epoch: [87]  [520/781]  eta: 0:00:51  lr: 0.000011  training_loss: 1.0833 (1.1201)  mae_loss: 0.0255 (0.0256)  classification_loss: 1.0586 (1.0945)  loss_mask: 0.0000 (0.0001)  time: 0.1971  data: 0.0002  max mem: 5511
[00:47:32.948069] Epoch: [87]  [540/781]  eta: 0:00:47  lr: 0.000011  training_loss: 1.1391 (1.1209)  mae_loss: 0.0264 (0.0256)  classification_loss: 1.1127 (1.0952)  loss_mask: 0.0000 (0.0001)  time: 0.1976  data: 0.0005  max mem: 5511
[00:47:36.862073] Epoch: [87]  [560/781]  eta: 0:00:43  lr: 0.000011  training_loss: 1.0913 (1.1214)  mae_loss: 0.0251 (0.0256)  classification_loss: 1.0691 (1.0957)  loss_mask: 0.0000 (0.0001)  time: 0.1956  data: 0.0002  max mem: 5511
[00:47:40.780240] Epoch: [87]  [580/781]  eta: 0:00:39  lr: 0.000011  training_loss: 1.1331 (1.1226)  mae_loss: 0.0246 (0.0256)  classification_loss: 1.1056 (1.0969)  loss_mask: 0.0000 (0.0001)  time: 0.1958  data: 0.0003  max mem: 5511
[00:47:44.701012] Epoch: [87]  [600/781]  eta: 0:00:35  lr: 0.000011  training_loss: 1.0656 (1.1220)  mae_loss: 0.0251 (0.0256)  classification_loss: 1.0440 (1.0963)  loss_mask: 0.0000 (0.0001)  time: 0.1960  data: 0.0003  max mem: 5511
[00:47:48.607642] Epoch: [87]  [620/781]  eta: 0:00:31  lr: 0.000011  training_loss: 1.0829 (1.1218)  mae_loss: 0.0260 (0.0256)  classification_loss: 1.0559 (1.0961)  loss_mask: 0.0000 (0.0001)  time: 0.1953  data: 0.0002  max mem: 5511
[00:47:52.521705] Epoch: [87]  [640/781]  eta: 0:00:27  lr: 0.000011  training_loss: 1.1103 (1.1215)  mae_loss: 0.0240 (0.0255)  classification_loss: 1.0863 (1.0959)  loss_mask: 0.0000 (0.0001)  time: 0.1956  data: 0.0003  max mem: 5511
[00:47:56.420562] Epoch: [87]  [660/781]  eta: 0:00:23  lr: 0.000011  training_loss: 1.1139 (1.1207)  mae_loss: 0.0252 (0.0256)  classification_loss: 1.0859 (1.0951)  loss_mask: 0.0000 (0.0001)  time: 0.1949  data: 0.0002  max mem: 5511
[00:48:00.331868] Epoch: [87]  [680/781]  eta: 0:00:19  lr: 0.000011  training_loss: 1.1515 (1.1213)  mae_loss: 0.0242 (0.0255)  classification_loss: 1.1300 (1.0957)  loss_mask: 0.0000 (0.0001)  time: 0.1955  data: 0.0002  max mem: 5511
[00:48:04.231625] Epoch: [87]  [700/781]  eta: 0:00:15  lr: 0.000011  training_loss: 1.1243 (1.1216)  mae_loss: 0.0233 (0.0255)  classification_loss: 1.1038 (1.0961)  loss_mask: 0.0000 (0.0001)  time: 0.1949  data: 0.0002  max mem: 5511
[00:48:08.155242] Epoch: [87]  [720/781]  eta: 0:00:12  lr: 0.000011  training_loss: 1.1191 (1.1225)  mae_loss: 0.0246 (0.0255)  classification_loss: 1.0945 (1.0969)  loss_mask: 0.0000 (0.0001)  time: 0.1961  data: 0.0002  max mem: 5511
[00:48:12.107506] Epoch: [87]  [740/781]  eta: 0:00:08  lr: 0.000011  training_loss: 1.1186 (1.1230)  mae_loss: 0.0260 (0.0255)  classification_loss: 1.0951 (1.0974)  loss_mask: 0.0000 (0.0001)  time: 0.1975  data: 0.0002  max mem: 5511
[00:48:16.038269] Epoch: [87]  [760/781]  eta: 0:00:04  lr: 0.000011  training_loss: 1.1419 (1.1237)  mae_loss: 0.0234 (0.0255)  classification_loss: 1.1171 (1.0981)  loss_mask: 0.0000 (0.0001)  time: 0.1965  data: 0.0003  max mem: 5511
[00:48:19.949028] Epoch: [87]  [780/781]  eta: 0:00:00  lr: 0.000011  training_loss: 1.1306 (1.1240)  mae_loss: 0.0242 (0.0254)  classification_loss: 1.1061 (1.0985)  loss_mask: 0.0000 (0.0001)  time: 0.1954  data: 0.0002  max mem: 5511
[00:48:20.106552] Epoch: [87] Total time: 0:02:34 (0.1974 s / it)
[00:48:20.107035] Averaged stats: lr: 0.000011  training_loss: 1.1306 (1.1240)  mae_loss: 0.0242 (0.0254)  classification_loss: 1.1061 (1.0985)  loss_mask: 0.0000 (0.0001)
[00:48:20.743620] Test:  [  0/157]  eta: 0:01:39  testing_loss: 0.5206 (0.5206)  acc1: 85.9375 (85.9375)  acc5: 96.8750 (96.8750)  time: 0.6326  data: 0.5950  max mem: 5511
[00:48:21.044666] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4335 (0.4674)  acc1: 85.9375 (85.6534)  acc5: 100.0000 (99.2898)  time: 0.0846  data: 0.0544  max mem: 5511
[00:48:21.329921] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4048 (0.4259)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (99.4048)  time: 0.0291  data: 0.0003  max mem: 5511
[00:48:21.617173] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4269 (0.4420)  acc1: 87.5000 (86.7944)  acc5: 100.0000 (99.3448)  time: 0.0284  data: 0.0002  max mem: 5511
[00:48:21.907218] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4399 (0.4463)  acc1: 85.9375 (86.6616)  acc5: 98.4375 (99.2378)  time: 0.0286  data: 0.0003  max mem: 5511
[00:48:22.189711] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4319 (0.4396)  acc1: 87.5000 (87.0711)  acc5: 100.0000 (99.3566)  time: 0.0285  data: 0.0002  max mem: 5511
[00:48:22.474870] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.3931 (0.4375)  acc1: 87.5000 (87.2695)  acc5: 100.0000 (99.3852)  time: 0.0283  data: 0.0002  max mem: 5511
[00:48:22.757214] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4052 (0.4324)  acc1: 87.5000 (87.5440)  acc5: 100.0000 (99.4058)  time: 0.0283  data: 0.0002  max mem: 5511
[00:48:23.041099] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4161 (0.4401)  acc1: 87.5000 (87.1528)  acc5: 100.0000 (99.4406)  time: 0.0282  data: 0.0002  max mem: 5511
[00:48:23.324483] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4285 (0.4354)  acc1: 87.5000 (87.5172)  acc5: 100.0000 (99.3990)  time: 0.0282  data: 0.0002  max mem: 5511
[00:48:23.609381] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4285 (0.4374)  acc1: 87.5000 (87.3917)  acc5: 98.4375 (99.3657)  time: 0.0283  data: 0.0002  max mem: 5511
[00:48:23.895253] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4397 (0.4369)  acc1: 87.5000 (87.4296)  acc5: 98.4375 (99.3525)  time: 0.0284  data: 0.0002  max mem: 5511
[00:48:24.179373] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4731 (0.4377)  acc1: 87.5000 (87.4096)  acc5: 100.0000 (99.3802)  time: 0.0283  data: 0.0002  max mem: 5511
[00:48:24.465640] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4296 (0.4394)  acc1: 85.9375 (87.2972)  acc5: 100.0000 (99.3678)  time: 0.0284  data: 0.0002  max mem: 5511
[00:48:24.749236] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4263 (0.4383)  acc1: 87.5000 (87.3116)  acc5: 100.0000 (99.3905)  time: 0.0283  data: 0.0002  max mem: 5511
[00:48:25.030137] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4318 (0.4389)  acc1: 87.5000 (87.3551)  acc5: 100.0000 (99.3895)  time: 0.0281  data: 0.0002  max mem: 5511
[00:48:25.180469] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4181 (0.4380)  acc1: 87.5000 (87.4000)  acc5: 100.0000 (99.4000)  time: 0.0270  data: 0.0001  max mem: 5511
[00:48:25.331267] Test: Total time: 0:00:05 (0.0333 s / it)
[00:48:25.331805] * Acc@1 87.400 Acc@5 99.400 loss 0.438
[00:48:25.332137] Accuracy of the network on the 10000 test images: 87.4%
[00:48:25.332350] Max accuracy: 87.40%
[00:48:25.637369] log_dir: ./output_dir
[00:48:26.522832] Epoch: [88]  [  0/781]  eta: 0:11:29  lr: 0.000011  training_loss: 0.9378 (0.9378)  mae_loss: 0.0242 (0.0242)  classification_loss: 0.9136 (0.9136)  loss_mask: 0.0000 (0.0000)  time: 0.8832  data: 0.6475  max mem: 5511
[00:48:30.438906] Epoch: [88]  [ 20/781]  eta: 0:02:53  lr: 0.000011  training_loss: 1.0623 (1.0789)  mae_loss: 0.0259 (0.0260)  classification_loss: 1.0428 (1.0529)  loss_mask: 0.0000 (0.0000)  time: 0.1957  data: 0.0002  max mem: 5511
[00:48:34.367103] Epoch: [88]  [ 40/781]  eta: 0:02:37  lr: 0.000011  training_loss: 1.0992 (1.0939)  mae_loss: 0.0255 (0.0260)  classification_loss: 1.0738 (1.0678)  loss_mask: 0.0000 (0.0000)  time: 0.1963  data: 0.0002  max mem: 5511
[00:48:38.279880] Epoch: [88]  [ 60/781]  eta: 0:02:29  lr: 0.000011  training_loss: 1.1051 (1.0999)  mae_loss: 0.0263 (0.0259)  classification_loss: 1.0761 (1.0740)  loss_mask: 0.0000 (0.0000)  time: 0.1956  data: 0.0002  max mem: 5511
[00:48:42.178318] Epoch: [88]  [ 80/781]  eta: 0:02:23  lr: 0.000011  training_loss: 1.0853 (1.1003)  mae_loss: 0.0248 (0.0257)  classification_loss: 1.0605 (1.0746)  loss_mask: 0.0000 (0.0000)  time: 0.1948  data: 0.0002  max mem: 5511
[00:48:46.087077] Epoch: [88]  [100/781]  eta: 0:02:17  lr: 0.000010  training_loss: 1.1514 (1.1074)  mae_loss: 0.0253 (0.0258)  classification_loss: 1.1268 (1.0815)  loss_mask: 0.0000 (0.0000)  time: 0.1954  data: 0.0002  max mem: 5511
[00:48:50.096381] Epoch: [88]  [120/781]  eta: 0:02:13  lr: 0.000010  training_loss: 1.0986 (1.1048)  mae_loss: 0.0257 (0.0258)  classification_loss: 1.0738 (1.0789)  loss_mask: 0.0000 (0.0000)  time: 0.2004  data: 0.0002  max mem: 5511
[00:48:54.013988] Epoch: [88]  [140/781]  eta: 0:02:08  lr: 0.000010  training_loss: 1.1154 (1.1048)  mae_loss: 0.0251 (0.0259)  classification_loss: 1.0896 (1.0788)  loss_mask: 0.0000 (0.0000)  time: 0.1958  data: 0.0002  max mem: 5511
[00:48:57.963752] Epoch: [88]  [160/781]  eta: 0:02:04  lr: 0.000010  training_loss: 1.0876 (1.1043)  mae_loss: 0.0249 (0.0258)  classification_loss: 1.0588 (1.0785)  loss_mask: 0.0000 (0.0000)  time: 0.1974  data: 0.0003  max mem: 5511
[00:49:01.913197] Epoch: [88]  [180/781]  eta: 0:02:00  lr: 0.000010  training_loss: 1.0880 (1.1059)  mae_loss: 0.0248 (0.0258)  classification_loss: 1.0622 (1.0800)  loss_mask: 0.0000 (0.0000)  time: 0.1974  data: 0.0002  max mem: 5511
[00:49:05.883002] Epoch: [88]  [200/781]  eta: 0:01:56  lr: 0.000010  training_loss: 1.1050 (1.1065)  mae_loss: 0.0250 (0.0258)  classification_loss: 1.0748 (1.0806)  loss_mask: 0.0001 (0.0001)  time: 0.1984  data: 0.0002  max mem: 5511
[00:49:09.811466] Epoch: [88]  [220/781]  eta: 0:01:52  lr: 0.000010  training_loss: 1.0938 (1.1060)  mae_loss: 0.0249 (0.0258)  classification_loss: 1.0693 (1.0801)  loss_mask: 0.0001 (0.0001)  time: 0.1963  data: 0.0002  max mem: 5511
[00:49:13.720291] Epoch: [88]  [240/781]  eta: 0:01:47  lr: 0.000010  training_loss: 1.1059 (1.1055)  mae_loss: 0.0252 (0.0257)  classification_loss: 1.0849 (1.0796)  loss_mask: 0.0001 (0.0001)  time: 0.1954  data: 0.0002  max mem: 5511
[00:49:17.659661] Epoch: [88]  [260/781]  eta: 0:01:43  lr: 0.000010  training_loss: 1.1270 (1.1079)  mae_loss: 0.0251 (0.0257)  classification_loss: 1.1022 (1.0821)  loss_mask: 0.0000 (0.0001)  time: 0.1969  data: 0.0002  max mem: 5511
[00:49:21.619528] Epoch: [88]  [280/781]  eta: 0:01:39  lr: 0.000010  training_loss: 1.1161 (1.1078)  mae_loss: 0.0259 (0.0257)  classification_loss: 1.0883 (1.0819)  loss_mask: 0.0000 (0.0001)  time: 0.1979  data: 0.0002  max mem: 5511
[00:49:25.518740] Epoch: [88]  [300/781]  eta: 0:01:35  lr: 0.000010  training_loss: 1.0900 (1.1088)  mae_loss: 0.0251 (0.0258)  classification_loss: 1.0666 (1.0829)  loss_mask: 0.0000 (0.0001)  time: 0.1949  data: 0.0002  max mem: 5511
[00:49:29.459813] Epoch: [88]  [320/781]  eta: 0:01:31  lr: 0.000010  training_loss: 1.0580 (1.1062)  mae_loss: 0.0247 (0.0257)  classification_loss: 1.0332 (1.0803)  loss_mask: 0.0000 (0.0001)  time: 0.1970  data: 0.0002  max mem: 5511
[00:49:33.424024] Epoch: [88]  [340/781]  eta: 0:01:27  lr: 0.000010  training_loss: 1.0999 (1.1060)  mae_loss: 0.0238 (0.0256)  classification_loss: 1.0726 (1.0802)  loss_mask: 0.0000 (0.0001)  time: 0.1981  data: 0.0002  max mem: 5511
[00:49:37.335199] Epoch: [88]  [360/781]  eta: 0:01:23  lr: 0.000010  training_loss: 1.0885 (1.1065)  mae_loss: 0.0253 (0.0256)  classification_loss: 1.0608 (1.0807)  loss_mask: 0.0000 (0.0001)  time: 0.1955  data: 0.0002  max mem: 5511
[00:49:41.268539] Epoch: [88]  [380/781]  eta: 0:01:19  lr: 0.000010  training_loss: 1.1628 (1.1098)  mae_loss: 0.0244 (0.0256)  classification_loss: 1.1430 (1.0840)  loss_mask: 0.0000 (0.0001)  time: 0.1966  data: 0.0003  max mem: 5511
[00:49:45.193778] Epoch: [88]  [400/781]  eta: 0:01:15  lr: 0.000010  training_loss: 1.0585 (1.1096)  mae_loss: 0.0242 (0.0256)  classification_loss: 1.0277 (1.0839)  loss_mask: 0.0000 (0.0001)  time: 0.1962  data: 0.0002  max mem: 5511
[00:49:49.113006] Epoch: [88]  [420/781]  eta: 0:01:11  lr: 0.000010  training_loss: 1.1298 (1.1103)  mae_loss: 0.0256 (0.0256)  classification_loss: 1.1048 (1.0846)  loss_mask: 0.0000 (0.0001)  time: 0.1959  data: 0.0003  max mem: 5511
[00:49:53.044876] Epoch: [88]  [440/781]  eta: 0:01:07  lr: 0.000010  training_loss: 1.0859 (1.1100)  mae_loss: 0.0247 (0.0256)  classification_loss: 1.0601 (1.0844)  loss_mask: 0.0000 (0.0001)  time: 0.1965  data: 0.0002  max mem: 5511
[00:49:56.978889] Epoch: [88]  [460/781]  eta: 0:01:03  lr: 0.000010  training_loss: 1.1263 (1.1100)  mae_loss: 0.0263 (0.0256)  classification_loss: 1.0975 (1.0843)  loss_mask: 0.0000 (0.0001)  time: 0.1966  data: 0.0002  max mem: 5511
[00:50:00.907366] Epoch: [88]  [480/781]  eta: 0:00:59  lr: 0.000010  training_loss: 1.1139 (1.1105)  mae_loss: 0.0261 (0.0256)  classification_loss: 1.0891 (1.0848)  loss_mask: 0.0000 (0.0001)  time: 0.1964  data: 0.0004  max mem: 5511
[00:50:04.945550] Epoch: [88]  [500/781]  eta: 0:00:55  lr: 0.000010  training_loss: 1.1392 (1.1115)  mae_loss: 0.0246 (0.0256)  classification_loss: 1.1128 (1.0858)  loss_mask: 0.0000 (0.0001)  time: 0.2018  data: 0.0004  max mem: 5511
[00:50:08.870708] Epoch: [88]  [520/781]  eta: 0:00:51  lr: 0.000010  training_loss: 1.1326 (1.1123)  mae_loss: 0.0248 (0.0256)  classification_loss: 1.1037 (1.0866)  loss_mask: 0.0000 (0.0001)  time: 0.1962  data: 0.0003  max mem: 5511
[00:50:12.802786] Epoch: [88]  [540/781]  eta: 0:00:47  lr: 0.000010  training_loss: 1.0985 (1.1120)  mae_loss: 0.0245 (0.0256)  classification_loss: 1.0766 (1.0864)  loss_mask: 0.0000 (0.0001)  time: 0.1965  data: 0.0002  max mem: 5511
[00:50:16.762573] Epoch: [88]  [560/781]  eta: 0:00:43  lr: 0.000010  training_loss: 1.1252 (1.1121)  mae_loss: 0.0249 (0.0255)  classification_loss: 1.1017 (1.0865)  loss_mask: 0.0001 (0.0001)  time: 0.1979  data: 0.0002  max mem: 5511
[00:50:20.743530] Epoch: [88]  [580/781]  eta: 0:00:39  lr: 0.000010  training_loss: 1.0950 (1.1124)  mae_loss: 0.0260 (0.0255)  classification_loss: 1.0655 (1.0868)  loss_mask: 0.0000 (0.0001)  time: 0.1990  data: 0.0002  max mem: 5511
[00:50:24.681074] Epoch: [88]  [600/781]  eta: 0:00:35  lr: 0.000009  training_loss: 1.0585 (1.1117)  mae_loss: 0.0243 (0.0255)  classification_loss: 1.0357 (1.0860)  loss_mask: 0.0000 (0.0001)  time: 0.1968  data: 0.0002  max mem: 5511
[00:50:28.623434] Epoch: [88]  [620/781]  eta: 0:00:31  lr: 0.000009  training_loss: 1.1264 (1.1123)  mae_loss: 0.0267 (0.0256)  classification_loss: 1.0996 (1.0866)  loss_mask: 0.0000 (0.0001)  time: 0.1970  data: 0.0002  max mem: 5511
[00:50:32.540380] Epoch: [88]  [640/781]  eta: 0:00:27  lr: 0.000009  training_loss: 1.0754 (1.1119)  mae_loss: 0.0241 (0.0255)  classification_loss: 1.0502 (1.0862)  loss_mask: 0.0000 (0.0001)  time: 0.1958  data: 0.0002  max mem: 5511
[00:50:36.574146] Epoch: [88]  [660/781]  eta: 0:00:23  lr: 0.000009  training_loss: 1.1286 (1.1121)  mae_loss: 0.0249 (0.0255)  classification_loss: 1.1046 (1.0865)  loss_mask: 0.0000 (0.0001)  time: 0.2016  data: 0.0003  max mem: 5511
[00:50:40.486681] Epoch: [88]  [680/781]  eta: 0:00:19  lr: 0.000009  training_loss: 1.1396 (1.1132)  mae_loss: 0.0260 (0.0255)  classification_loss: 1.1108 (1.0875)  loss_mask: 0.0000 (0.0001)  time: 0.1956  data: 0.0003  max mem: 5511
[00:50:44.486342] Epoch: [88]  [700/781]  eta: 0:00:16  lr: 0.000009  training_loss: 1.1363 (1.1141)  mae_loss: 0.0241 (0.0255)  classification_loss: 1.1144 (1.0885)  loss_mask: 0.0000 (0.0001)  time: 0.1999  data: 0.0002  max mem: 5511
[00:50:48.403251] Epoch: [88]  [720/781]  eta: 0:00:12  lr: 0.000009  training_loss: 1.1078 (1.1141)  mae_loss: 0.0249 (0.0255)  classification_loss: 1.0829 (1.0885)  loss_mask: 0.0000 (0.0001)  time: 0.1958  data: 0.0002  max mem: 5511
[00:50:52.303021] Epoch: [88]  [740/781]  eta: 0:00:08  lr: 0.000009  training_loss: 1.0622 (1.1132)  mae_loss: 0.0252 (0.0255)  classification_loss: 1.0349 (1.0876)  loss_mask: 0.0000 (0.0001)  time: 0.1949  data: 0.0002  max mem: 5511
[00:50:56.239317] Epoch: [88]  [760/781]  eta: 0:00:04  lr: 0.000009  training_loss: 1.1780 (1.1143)  mae_loss: 0.0258 (0.0256)  classification_loss: 1.1482 (1.0886)  loss_mask: 0.0000 (0.0001)  time: 0.1967  data: 0.0002  max mem: 5511
[00:51:00.137512] Epoch: [88]  [780/781]  eta: 0:00:00  lr: 0.000009  training_loss: 1.1351 (1.1150)  mae_loss: 0.0247 (0.0255)  classification_loss: 1.1082 (1.0894)  loss_mask: 0.0000 (0.0001)  time: 0.1948  data: 0.0002  max mem: 5511
[00:51:00.285790] Epoch: [88] Total time: 0:02:34 (0.1980 s / it)
[00:51:00.286252] Averaged stats: lr: 0.000009  training_loss: 1.1351 (1.1150)  mae_loss: 0.0247 (0.0255)  classification_loss: 1.1082 (1.0894)  loss_mask: 0.0000 (0.0001)
[00:51:00.906060] Test:  [  0/157]  eta: 0:01:36  testing_loss: 0.5402 (0.5402)  acc1: 84.3750 (84.3750)  acc5: 100.0000 (100.0000)  time: 0.6155  data: 0.5758  max mem: 5511
[00:51:01.191374] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.4341 (0.4719)  acc1: 84.3750 (85.0852)  acc5: 100.0000 (99.5739)  time: 0.0816  data: 0.0525  max mem: 5511
[00:51:01.474319] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4123 (0.4311)  acc1: 87.5000 (86.3839)  acc5: 100.0000 (99.4792)  time: 0.0282  data: 0.0002  max mem: 5511
[00:51:01.756003] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.4354 (0.4481)  acc1: 85.9375 (85.9879)  acc5: 100.0000 (99.2944)  time: 0.0280  data: 0.0002  max mem: 5511
[00:51:02.039259] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.4436 (0.4517)  acc1: 87.5000 (86.2424)  acc5: 98.4375 (99.1997)  time: 0.0281  data: 0.0002  max mem: 5511
[00:51:02.320620] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4364 (0.4451)  acc1: 87.5000 (86.6115)  acc5: 100.0000 (99.3260)  time: 0.0281  data: 0.0002  max mem: 5511
[00:51:02.602947] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4080 (0.4437)  acc1: 89.0625 (86.7316)  acc5: 100.0000 (99.3340)  time: 0.0281  data: 0.0002  max mem: 5511
[00:51:02.890978] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4074 (0.4376)  acc1: 87.5000 (86.9938)  acc5: 100.0000 (99.3618)  time: 0.0284  data: 0.0002  max mem: 5511
[00:51:03.174540] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4403 (0.4449)  acc1: 85.9375 (86.6898)  acc5: 100.0000 (99.4020)  time: 0.0284  data: 0.0002  max mem: 5511
[00:51:03.455998] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4403 (0.4400)  acc1: 87.5000 (87.0536)  acc5: 100.0000 (99.3647)  time: 0.0281  data: 0.0002  max mem: 5511
[00:51:03.737645] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4290 (0.4412)  acc1: 89.0625 (87.0359)  acc5: 98.4375 (99.3502)  time: 0.0280  data: 0.0002  max mem: 5511
[00:51:04.021286] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4292 (0.4408)  acc1: 85.9375 (87.0214)  acc5: 98.4375 (99.3243)  time: 0.0281  data: 0.0002  max mem: 5511
[00:51:04.304970] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4560 (0.4406)  acc1: 85.9375 (87.0480)  acc5: 98.4375 (99.3156)  time: 0.0282  data: 0.0002  max mem: 5511
[00:51:04.587763] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4324 (0.4421)  acc1: 85.9375 (87.0348)  acc5: 100.0000 (99.3201)  time: 0.0282  data: 0.0002  max mem: 5511
[00:51:04.869585] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4324 (0.4408)  acc1: 85.9375 (87.1011)  acc5: 100.0000 (99.3462)  time: 0.0281  data: 0.0002  max mem: 5511
[00:51:05.150461] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4479 (0.4407)  acc1: 85.9375 (87.1689)  acc5: 100.0000 (99.3584)  time: 0.0280  data: 0.0002  max mem: 5511
[00:51:05.300212] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.3953 (0.4397)  acc1: 89.0625 (87.1900)  acc5: 100.0000 (99.3600)  time: 0.0270  data: 0.0001  max mem: 5511
[00:51:05.461806] Test: Total time: 0:00:05 (0.0329 s / it)
[00:51:05.462303] * Acc@1 87.190 Acc@5 99.360 loss 0.440
[00:51:05.462624] Accuracy of the network on the 10000 test images: 87.2%
[00:51:05.462832] Max accuracy: 87.40%
[00:51:05.644063] log_dir: ./output_dir
[00:51:06.489042] Epoch: [89]  [  0/781]  eta: 0:10:58  lr: 0.000009  training_loss: 1.0394 (1.0394)  mae_loss: 0.0233 (0.0233)  classification_loss: 1.0160 (1.0160)  loss_mask: 0.0000 (0.0000)  time: 0.8432  data: 0.6398  max mem: 5511
[00:51:10.472100] Epoch: [89]  [ 20/781]  eta: 0:02:54  lr: 0.000009  training_loss: 1.0910 (1.0899)  mae_loss: 0.0260 (0.0259)  classification_loss: 1.0641 (1.0640)  loss_mask: 0.0000 (0.0000)  time: 0.1990  data: 0.0003  max mem: 5511
[00:51:14.398846] Epoch: [89]  [ 40/781]  eta: 0:02:38  lr: 0.000009  training_loss: 1.1408 (1.1145)  mae_loss: 0.0238 (0.0253)  classification_loss: 1.1169 (1.0893)  loss_mask: 0.0000 (0.0000)  time: 0.1963  data: 0.0002  max mem: 5511
[00:51:18.301647] Epoch: [89]  [ 60/781]  eta: 0:02:29  lr: 0.000009  training_loss: 1.1433 (1.1375)  mae_loss: 0.0252 (0.0253)  classification_loss: 1.1176 (1.1122)  loss_mask: 0.0000 (0.0000)  time: 0.1950  data: 0.0002  max mem: 5511
[00:51:22.211033] Epoch: [89]  [ 80/781]  eta: 0:02:23  lr: 0.000009  training_loss: 1.0901 (1.1271)  mae_loss: 0.0247 (0.0253)  classification_loss: 1.0666 (1.1018)  loss_mask: 0.0000 (0.0000)  time: 0.1954  data: 0.0002  max mem: 5511
[00:51:26.125376] Epoch: [89]  [100/781]  eta: 0:02:18  lr: 0.000009  training_loss: 1.1151 (1.1238)  mae_loss: 0.0255 (0.0254)  classification_loss: 1.0920 (1.0984)  loss_mask: 0.0000 (0.0000)  time: 0.1956  data: 0.0002  max mem: 5511
[00:51:30.084468] Epoch: [89]  [120/781]  eta: 0:02:13  lr: 0.000009  training_loss: 1.0909 (1.1212)  mae_loss: 0.0249 (0.0253)  classification_loss: 1.0619 (1.0958)  loss_mask: 0.0000 (0.0000)  time: 0.1979  data: 0.0002  max mem: 5511
[00:51:34.006815] Epoch: [89]  [140/781]  eta: 0:02:08  lr: 0.000009  training_loss: 1.1036 (1.1209)  mae_loss: 0.0257 (0.0255)  classification_loss: 1.0765 (1.0954)  loss_mask: 0.0000 (0.0000)  time: 0.1960  data: 0.0002  max mem: 5511
[00:51:37.922270] Epoch: [89]  [160/781]  eta: 0:02:04  lr: 0.000009  training_loss: 1.0835 (1.1172)  mae_loss: 0.0251 (0.0255)  classification_loss: 1.0584 (1.0917)  loss_mask: 0.0000 (0.0000)  time: 0.1957  data: 0.0002  max mem: 5511
[00:51:41.877753] Epoch: [89]  [180/781]  eta: 0:02:00  lr: 0.000009  training_loss: 1.1282 (1.1195)  mae_loss: 0.0252 (0.0255)  classification_loss: 1.1056 (1.0940)  loss_mask: 0.0000 (0.0000)  time: 0.1977  data: 0.0002  max mem: 5511
[00:51:45.810868] Epoch: [89]  [200/781]  eta: 0:01:56  lr: 0.000009  training_loss: 1.1285 (1.1225)  mae_loss: 0.0254 (0.0255)  classification_loss: 1.0967 (1.0969)  loss_mask: 0.0000 (0.0000)  time: 0.1966  data: 0.0002  max mem: 5511
[00:51:49.746791] Epoch: [89]  [220/781]  eta: 0:01:51  lr: 0.000009  training_loss: 1.1076 (1.1211)  mae_loss: 0.0237 (0.0254)  classification_loss: 1.0802 (1.0956)  loss_mask: 0.0000 (0.0000)  time: 0.1967  data: 0.0002  max mem: 5511
[00:51:53.659987] Epoch: [89]  [240/781]  eta: 0:01:47  lr: 0.000009  training_loss: 1.1242 (1.1228)  mae_loss: 0.0265 (0.0255)  classification_loss: 1.0976 (1.0973)  loss_mask: 0.0000 (0.0000)  time: 0.1956  data: 0.0002  max mem: 5511
[00:51:57.584597] Epoch: [89]  [260/781]  eta: 0:01:43  lr: 0.000009  training_loss: 1.1218 (1.1232)  mae_loss: 0.0254 (0.0255)  classification_loss: 1.0976 (1.0976)  loss_mask: 0.0001 (0.0001)  time: 0.1962  data: 0.0003  max mem: 5511
[00:52:01.521968] Epoch: [89]  [280/781]  eta: 0:01:39  lr: 0.000009  training_loss: 1.1258 (1.1234)  mae_loss: 0.0250 (0.0255)  classification_loss: 1.1004 (1.0978)  loss_mask: 0.0000 (0.0001)  time: 0.1968  data: 0.0002  max mem: 5511
[00:52:05.453729] Epoch: [89]  [300/781]  eta: 0:01:35  lr: 0.000009  training_loss: 1.0993 (1.1226)  mae_loss: 0.0253 (0.0255)  classification_loss: 1.0697 (1.0970)  loss_mask: 0.0000 (0.0001)  time: 0.1965  data: 0.0002  max mem: 5511
[00:52:09.364361] Epoch: [89]  [320/781]  eta: 0:01:31  lr: 0.000009  training_loss: 1.1014 (1.1223)  mae_loss: 0.0251 (0.0255)  classification_loss: 1.0788 (1.0967)  loss_mask: 0.0000 (0.0001)  time: 0.1954  data: 0.0002  max mem: 5511
[00:52:13.267357] Epoch: [89]  [340/781]  eta: 0:01:27  lr: 0.000009  training_loss: 1.1065 (1.1211)  mae_loss: 0.0269 (0.0256)  classification_loss: 1.0777 (1.0954)  loss_mask: 0.0000 (0.0001)  time: 0.1950  data: 0.0002  max mem: 5511
[00:52:17.170285] Epoch: [89]  [360/781]  eta: 0:01:23  lr: 0.000008  training_loss: 1.1333 (1.1219)  mae_loss: 0.0238 (0.0255)  classification_loss: 1.1095 (1.0962)  loss_mask: 0.0000 (0.0001)  time: 0.1951  data: 0.0003  max mem: 5511
[00:52:21.116330] Epoch: [89]  [380/781]  eta: 0:01:19  lr: 0.000008  training_loss: 1.1132 (1.1205)  mae_loss: 0.0267 (0.0256)  classification_loss: 1.0875 (1.0948)  loss_mask: 0.0000 (0.0001)  time: 0.1972  data: 0.0002  max mem: 5511
[00:52:25.032974] Epoch: [89]  [400/781]  eta: 0:01:15  lr: 0.000008  training_loss: 1.0928 (1.1195)  mae_loss: 0.0245 (0.0256)  classification_loss: 1.0681 (1.0938)  loss_mask: 0.0000 (0.0001)  time: 0.1957  data: 0.0002  max mem: 5511
[00:52:28.940197] Epoch: [89]  [420/781]  eta: 0:01:11  lr: 0.000008  training_loss: 1.0801 (1.1185)  mae_loss: 0.0251 (0.0256)  classification_loss: 1.0553 (1.0928)  loss_mask: 0.0000 (0.0001)  time: 0.1953  data: 0.0002  max mem: 5511
[00:52:32.847280] Epoch: [89]  [440/781]  eta: 0:01:07  lr: 0.000008  training_loss: 1.1195 (1.1183)  mae_loss: 0.0245 (0.0256)  classification_loss: 1.0908 (1.0926)  loss_mask: 0.0000 (0.0001)  time: 0.1953  data: 0.0002  max mem: 5511
[00:52:36.733410] Epoch: [89]  [460/781]  eta: 0:01:03  lr: 0.000008  training_loss: 1.0782 (1.1178)  mae_loss: 0.0254 (0.0256)  classification_loss: 1.0558 (1.0921)  loss_mask: 0.0000 (0.0001)  time: 0.1942  data: 0.0002  max mem: 5511
[00:52:40.674103] Epoch: [89]  [480/781]  eta: 0:00:59  lr: 0.000008  training_loss: 1.1355 (1.1185)  mae_loss: 0.0242 (0.0255)  classification_loss: 1.1121 (1.0929)  loss_mask: 0.0000 (0.0001)  time: 0.1970  data: 0.0002  max mem: 5511
[00:52:44.576514] Epoch: [89]  [500/781]  eta: 0:00:55  lr: 0.000008  training_loss: 1.0800 (1.1175)  mae_loss: 0.0251 (0.0255)  classification_loss: 1.0493 (1.0919)  loss_mask: 0.0000 (0.0001)  time: 0.1950  data: 0.0002  max mem: 5511
[00:52:48.499861] Epoch: [89]  [520/781]  eta: 0:00:51  lr: 0.000008  training_loss: 1.1335 (1.1177)  mae_loss: 0.0246 (0.0255)  classification_loss: 1.1096 (1.0921)  loss_mask: 0.0000 (0.0001)  time: 0.1961  data: 0.0002  max mem: 5511
[00:52:52.420440] Epoch: [89]  [540/781]  eta: 0:00:47  lr: 0.000008  training_loss: 1.0936 (1.1173)  mae_loss: 0.0254 (0.0255)  classification_loss: 1.0666 (1.0917)  loss_mask: 0.0000 (0.0001)  time: 0.1959  data: 0.0002  max mem: 5511
[00:52:56.340313] Epoch: [89]  [560/781]  eta: 0:00:43  lr: 0.000008  training_loss: 1.0823 (1.1166)  mae_loss: 0.0257 (0.0256)  classification_loss: 1.0564 (1.0910)  loss_mask: 0.0000 (0.0001)  time: 0.1959  data: 0.0002  max mem: 5511
[00:53:00.260403] Epoch: [89]  [580/781]  eta: 0:00:39  lr: 0.000008  training_loss: 1.0765 (1.1159)  mae_loss: 0.0256 (0.0256)  classification_loss: 1.0497 (1.0902)  loss_mask: 0.0000 (0.0001)  time: 0.1959  data: 0.0002  max mem: 5511
[00:53:04.214125] Epoch: [89]  [600/781]  eta: 0:00:35  lr: 0.000008  training_loss: 1.0977 (1.1151)  mae_loss: 0.0245 (0.0256)  classification_loss: 1.0759 (1.0894)  loss_mask: 0.0000 (0.0001)  time: 0.1976  data: 0.0002  max mem: 5511
[00:53:08.129467] Epoch: [89]  [620/781]  eta: 0:00:31  lr: 0.000008  training_loss: 1.0822 (1.1148)  mae_loss: 0.0243 (0.0255)  classification_loss: 1.0583 (1.0892)  loss_mask: 0.0000 (0.0001)  time: 0.1957  data: 0.0003  max mem: 5511
[00:53:12.081132] Epoch: [89]  [640/781]  eta: 0:00:27  lr: 0.000008  training_loss: 1.1302 (1.1151)  mae_loss: 0.0241 (0.0255)  classification_loss: 1.1095 (1.0895)  loss_mask: 0.0000 (0.0001)  time: 0.1975  data: 0.0002  max mem: 5511
[00:53:15.998845] Epoch: [89]  [660/781]  eta: 0:00:23  lr: 0.000008  training_loss: 1.1272 (1.1160)  mae_loss: 0.0245 (0.0255)  classification_loss: 1.1027 (1.0904)  loss_mask: 0.0000 (0.0001)  time: 0.1958  data: 0.0002  max mem: 5511
[00:53:19.908722] Epoch: [89]  [680/781]  eta: 0:00:19  lr: 0.000008  training_loss: 1.1020 (1.1160)  mae_loss: 0.0242 (0.0255)  classification_loss: 1.0769 (1.0904)  loss_mask: 0.0000 (0.0001)  time: 0.1954  data: 0.0002  max mem: 5511
[00:53:23.857088] Epoch: [89]  [700/781]  eta: 0:00:15  lr: 0.000008  training_loss: 1.1257 (1.1169)  mae_loss: 0.0261 (0.0255)  classification_loss: 1.0997 (1.0913)  loss_mask: 0.0000 (0.0001)  time: 0.1973  data: 0.0002  max mem: 5511
[00:53:27.775549] Epoch: [89]  [720/781]  eta: 0:00:12  lr: 0.000008  training_loss: 1.1267 (1.1170)  mae_loss: 0.0254 (0.0255)  classification_loss: 1.1023 (1.0914)  loss_mask: 0.0000 (0.0001)  time: 0.1958  data: 0.0002  max mem: 5511
[00:53:31.706255] Epoch: [89]  [740/781]  eta: 0:00:08  lr: 0.000008  training_loss: 1.0848 (1.1170)  mae_loss: 0.0250 (0.0255)  classification_loss: 1.0606 (1.0914)  loss_mask: 0.0000 (0.0001)  time: 0.1965  data: 0.0003  max mem: 5511
[00:53:35.638097] Epoch: [89]  [760/781]  eta: 0:00:04  lr: 0.000008  training_loss: 1.0949 (1.1168)  mae_loss: 0.0255 (0.0256)  classification_loss: 1.0714 (1.0912)  loss_mask: 0.0000 (0.0001)  time: 0.1965  data: 0.0002  max mem: 5511
[00:53:39.604094] Epoch: [89]  [780/781]  eta: 0:00:00  lr: 0.000008  training_loss: 1.0832 (1.1175)  mae_loss: 0.0257 (0.0256)  classification_loss: 1.0597 (1.0918)  loss_mask: 0.0000 (0.0001)  time: 0.1982  data: 0.0002  max mem: 5511
[00:53:39.745386] Epoch: [89] Total time: 0:02:34 (0.1973 s / it)
[00:53:39.745881] Averaged stats: lr: 0.000008  training_loss: 1.0832 (1.1175)  mae_loss: 0.0257 (0.0256)  classification_loss: 1.0597 (1.0918)  loss_mask: 0.0000 (0.0001)
[00:53:40.381526] Test:  [  0/157]  eta: 0:01:38  testing_loss: 0.5259 (0.5259)  acc1: 84.3750 (84.3750)  acc5: 100.0000 (100.0000)  time: 0.6291  data: 0.5866  max mem: 5511
[00:53:40.666944] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4419 (0.4698)  acc1: 85.9375 (85.3693)  acc5: 100.0000 (99.4318)  time: 0.0830  data: 0.0536  max mem: 5511
[00:53:40.950386] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4243 (0.4238)  acc1: 87.5000 (87.1280)  acc5: 100.0000 (99.4048)  time: 0.0283  data: 0.0002  max mem: 5511
[00:53:41.240207] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4243 (0.4426)  acc1: 87.5000 (86.6431)  acc5: 100.0000 (99.3448)  time: 0.0285  data: 0.0002  max mem: 5511
[00:53:41.525945] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4356 (0.4459)  acc1: 87.5000 (86.8140)  acc5: 98.4375 (99.1997)  time: 0.0286  data: 0.0002  max mem: 5511
[00:53:41.810329] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4162 (0.4393)  acc1: 87.5000 (87.0404)  acc5: 100.0000 (99.3260)  time: 0.0283  data: 0.0002  max mem: 5511
[00:53:42.094127] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4055 (0.4379)  acc1: 87.5000 (87.2695)  acc5: 100.0000 (99.3340)  time: 0.0283  data: 0.0002  max mem: 5511
[00:53:42.378686] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4103 (0.4326)  acc1: 89.0625 (87.4780)  acc5: 100.0000 (99.3618)  time: 0.0283  data: 0.0002  max mem: 5511
[00:53:42.664504] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4277 (0.4393)  acc1: 87.5000 (87.1142)  acc5: 100.0000 (99.4020)  time: 0.0284  data: 0.0002  max mem: 5511
[00:53:42.948466] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4346 (0.4347)  acc1: 87.5000 (87.4313)  acc5: 100.0000 (99.3475)  time: 0.0283  data: 0.0002  max mem: 5511
[00:53:43.235689] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4200 (0.4368)  acc1: 89.0625 (87.4226)  acc5: 98.4375 (99.3193)  time: 0.0284  data: 0.0002  max mem: 5511
[00:53:43.518579] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4511 (0.4368)  acc1: 87.5000 (87.4296)  acc5: 98.4375 (99.3102)  time: 0.0284  data: 0.0002  max mem: 5511
[00:53:43.803879] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4487 (0.4368)  acc1: 87.5000 (87.4483)  acc5: 100.0000 (99.3156)  time: 0.0283  data: 0.0002  max mem: 5511
[00:53:44.088238] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4250 (0.4384)  acc1: 87.5000 (87.3927)  acc5: 100.0000 (99.3082)  time: 0.0284  data: 0.0002  max mem: 5511
[00:53:44.372481] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4164 (0.4371)  acc1: 87.5000 (87.4335)  acc5: 100.0000 (99.3351)  time: 0.0283  data: 0.0002  max mem: 5511
[00:53:44.654139] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4468 (0.4376)  acc1: 87.5000 (87.4483)  acc5: 100.0000 (99.3481)  time: 0.0282  data: 0.0002  max mem: 5511
[00:53:44.806816] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4121 (0.4368)  acc1: 87.5000 (87.4700)  acc5: 100.0000 (99.3600)  time: 0.0272  data: 0.0001  max mem: 5511
[00:53:44.959214] Test: Total time: 0:00:05 (0.0332 s / it)
[00:53:44.959698] * Acc@1 87.470 Acc@5 99.360 loss 0.437
[00:53:44.959980] Accuracy of the network on the 10000 test images: 87.5%
[00:53:44.960153] Max accuracy: 87.47%
[00:53:45.659000] log_dir: ./output_dir
[00:53:46.550092] Epoch: [90]  [  0/781]  eta: 0:11:34  lr: 0.000008  training_loss: 1.0353 (1.0353)  mae_loss: 0.0239 (0.0239)  classification_loss: 1.0114 (1.0114)  loss_mask: 0.0001 (0.0001)  time: 0.8894  data: 0.6258  max mem: 5511
[00:53:50.476848] Epoch: [90]  [ 20/781]  eta: 0:02:54  lr: 0.000008  training_loss: 1.0974 (1.1028)  mae_loss: 0.0249 (0.0254)  classification_loss: 1.0703 (1.0773)  loss_mask: 0.0000 (0.0000)  time: 0.1962  data: 0.0005  max mem: 5511
[00:53:54.377229] Epoch: [90]  [ 40/781]  eta: 0:02:37  lr: 0.000008  training_loss: 1.1404 (1.1179)  mae_loss: 0.0252 (0.0254)  classification_loss: 1.1154 (1.0925)  loss_mask: 0.0000 (0.0000)  time: 0.1949  data: 0.0002  max mem: 5511
[00:53:58.296629] Epoch: [90]  [ 60/781]  eta: 0:02:29  lr: 0.000008  training_loss: 1.0969 (1.1200)  mae_loss: 0.0256 (0.0254)  classification_loss: 1.0710 (1.0946)  loss_mask: 0.0000 (0.0000)  time: 0.1959  data: 0.0002  max mem: 5511
[00:54:02.229263] Epoch: [90]  [ 80/781]  eta: 0:02:23  lr: 0.000008  training_loss: 1.0751 (1.1084)  mae_loss: 0.0250 (0.0252)  classification_loss: 1.0486 (1.0832)  loss_mask: 0.0000 (0.0000)  time: 0.1965  data: 0.0002  max mem: 5511
[00:54:06.148331] Epoch: [90]  [100/781]  eta: 0:02:18  lr: 0.000008  training_loss: 1.1504 (1.1168)  mae_loss: 0.0253 (0.0252)  classification_loss: 1.1268 (1.0916)  loss_mask: 0.0000 (0.0000)  time: 0.1958  data: 0.0003  max mem: 5511
[00:54:10.072264] Epoch: [90]  [120/781]  eta: 0:02:13  lr: 0.000008  training_loss: 1.1178 (1.1175)  mae_loss: 0.0257 (0.0252)  classification_loss: 1.0910 (1.0923)  loss_mask: 0.0000 (0.0000)  time: 0.1961  data: 0.0002  max mem: 5511
[00:54:14.028876] Epoch: [90]  [140/781]  eta: 0:02:08  lr: 0.000008  training_loss: 1.1417 (1.1210)  mae_loss: 0.0240 (0.0252)  classification_loss: 1.1177 (1.0958)  loss_mask: 0.0000 (0.0000)  time: 0.1977  data: 0.0003  max mem: 5511
[00:54:17.968012] Epoch: [90]  [160/781]  eta: 0:02:04  lr: 0.000007  training_loss: 1.0893 (1.1193)  mae_loss: 0.0261 (0.0254)  classification_loss: 1.0645 (1.0939)  loss_mask: 0.0000 (0.0000)  time: 0.1968  data: 0.0002  max mem: 5511
[00:54:21.898848] Epoch: [90]  [180/781]  eta: 0:02:00  lr: 0.000007  training_loss: 1.1220 (1.1203)  mae_loss: 0.0253 (0.0253)  classification_loss: 1.0957 (1.0950)  loss_mask: 0.0000 (0.0000)  time: 0.1965  data: 0.0002  max mem: 5511
[00:54:25.877421] Epoch: [90]  [200/781]  eta: 0:01:56  lr: 0.000007  training_loss: 1.0671 (1.1168)  mae_loss: 0.0252 (0.0253)  classification_loss: 1.0432 (1.0915)  loss_mask: 0.0000 (0.0001)  time: 0.1988  data: 0.0004  max mem: 5511
[00:54:29.814925] Epoch: [90]  [220/781]  eta: 0:01:52  lr: 0.000007  training_loss: 1.1113 (1.1165)  mae_loss: 0.0243 (0.0252)  classification_loss: 1.0846 (1.0912)  loss_mask: 0.0000 (0.0001)  time: 0.1968  data: 0.0002  max mem: 5511
[00:54:33.763742] Epoch: [90]  [240/781]  eta: 0:01:47  lr: 0.000007  training_loss: 1.0859 (1.1167)  mae_loss: 0.0262 (0.0254)  classification_loss: 1.0561 (1.0913)  loss_mask: 0.0000 (0.0001)  time: 0.1973  data: 0.0002  max mem: 5511
[00:54:37.701906] Epoch: [90]  [260/781]  eta: 0:01:43  lr: 0.000007  training_loss: 1.1109 (1.1172)  mae_loss: 0.0253 (0.0253)  classification_loss: 1.0843 (1.0918)  loss_mask: 0.0000 (0.0001)  time: 0.1968  data: 0.0002  max mem: 5511
[00:54:41.684462] Epoch: [90]  [280/781]  eta: 0:01:39  lr: 0.000007  training_loss: 1.0815 (1.1155)  mae_loss: 0.0249 (0.0253)  classification_loss: 1.0585 (1.0901)  loss_mask: 0.0000 (0.0001)  time: 0.1990  data: 0.0002  max mem: 5511
[00:54:45.626041] Epoch: [90]  [300/781]  eta: 0:01:35  lr: 0.000007  training_loss: 1.1127 (1.1157)  mae_loss: 0.0239 (0.0253)  classification_loss: 1.0820 (1.0903)  loss_mask: 0.0000 (0.0001)  time: 0.1970  data: 0.0002  max mem: 5511
[00:54:49.557391] Epoch: [90]  [320/781]  eta: 0:01:31  lr: 0.000007  training_loss: 1.1463 (1.1162)  mae_loss: 0.0241 (0.0253)  classification_loss: 1.1211 (1.0909)  loss_mask: 0.0000 (0.0001)  time: 0.1965  data: 0.0002  max mem: 5511
[00:54:53.479212] Epoch: [90]  [340/781]  eta: 0:01:27  lr: 0.000007  training_loss: 1.1150 (1.1151)  mae_loss: 0.0266 (0.0254)  classification_loss: 1.0874 (1.0897)  loss_mask: 0.0000 (0.0001)  time: 0.1960  data: 0.0002  max mem: 5511
[00:54:57.397783] Epoch: [90]  [360/781]  eta: 0:01:23  lr: 0.000007  training_loss: 1.0794 (1.1137)  mae_loss: 0.0238 (0.0253)  classification_loss: 1.0531 (1.0884)  loss_mask: 0.0000 (0.0001)  time: 0.1958  data: 0.0002  max mem: 5511
[00:55:01.314438] Epoch: [90]  [380/781]  eta: 0:01:19  lr: 0.000007  training_loss: 1.1283 (1.1153)  mae_loss: 0.0259 (0.0253)  classification_loss: 1.1020 (1.0899)  loss_mask: 0.0000 (0.0001)  time: 0.1958  data: 0.0002  max mem: 5511
[00:55:05.245423] Epoch: [90]  [400/781]  eta: 0:01:15  lr: 0.000007  training_loss: 1.0499 (1.1138)  mae_loss: 0.0247 (0.0253)  classification_loss: 1.0245 (1.0885)  loss_mask: 0.0000 (0.0001)  time: 0.1965  data: 0.0002  max mem: 5511
[00:55:09.185318] Epoch: [90]  [420/781]  eta: 0:01:11  lr: 0.000007  training_loss: 1.0777 (1.1126)  mae_loss: 0.0258 (0.0253)  classification_loss: 1.0561 (1.0873)  loss_mask: 0.0000 (0.0001)  time: 0.1969  data: 0.0002  max mem: 5511
[00:55:13.142164] Epoch: [90]  [440/781]  eta: 0:01:07  lr: 0.000007  training_loss: 1.1436 (1.1129)  mae_loss: 0.0259 (0.0253)  classification_loss: 1.1151 (1.0875)  loss_mask: 0.0000 (0.0001)  time: 0.1978  data: 0.0003  max mem: 5511
[00:55:17.074139] Epoch: [90]  [460/781]  eta: 0:01:03  lr: 0.000007  training_loss: 1.0562 (1.1122)  mae_loss: 0.0244 (0.0253)  classification_loss: 1.0295 (1.0868)  loss_mask: 0.0000 (0.0001)  time: 0.1965  data: 0.0002  max mem: 5511
[00:55:20.998891] Epoch: [90]  [480/781]  eta: 0:00:59  lr: 0.000007  training_loss: 1.1017 (1.1119)  mae_loss: 0.0245 (0.0253)  classification_loss: 1.0743 (1.0866)  loss_mask: 0.0000 (0.0001)  time: 0.1961  data: 0.0002  max mem: 5511
[00:55:24.927510] Epoch: [90]  [500/781]  eta: 0:00:55  lr: 0.000007  training_loss: 1.0750 (1.1110)  mae_loss: 0.0255 (0.0253)  classification_loss: 1.0519 (1.0856)  loss_mask: 0.0000 (0.0001)  time: 0.1963  data: 0.0002  max mem: 5511
[00:55:28.831043] Epoch: [90]  [520/781]  eta: 0:00:51  lr: 0.000007  training_loss: 1.1310 (1.1109)  mae_loss: 0.0256 (0.0253)  classification_loss: 1.1059 (1.0855)  loss_mask: 0.0000 (0.0001)  time: 0.1951  data: 0.0002  max mem: 5511
[00:55:32.752111] Epoch: [90]  [540/781]  eta: 0:00:47  lr: 0.000007  training_loss: 1.0814 (1.1103)  mae_loss: 0.0252 (0.0253)  classification_loss: 1.0537 (1.0849)  loss_mask: 0.0000 (0.0001)  time: 0.1959  data: 0.0002  max mem: 5511
[00:55:36.671024] Epoch: [90]  [560/781]  eta: 0:00:43  lr: 0.000007  training_loss: 1.1320 (1.1111)  mae_loss: 0.0241 (0.0253)  classification_loss: 1.1118 (1.0858)  loss_mask: 0.0000 (0.0001)  time: 0.1959  data: 0.0002  max mem: 5511
[00:55:40.594748] Epoch: [90]  [580/781]  eta: 0:00:39  lr: 0.000007  training_loss: 1.1233 (1.1120)  mae_loss: 0.0251 (0.0253)  classification_loss: 1.1000 (1.0867)  loss_mask: 0.0000 (0.0001)  time: 0.1961  data: 0.0002  max mem: 5511
[00:55:44.514149] Epoch: [90]  [600/781]  eta: 0:00:35  lr: 0.000007  training_loss: 1.0853 (1.1117)  mae_loss: 0.0249 (0.0253)  classification_loss: 1.0597 (1.0863)  loss_mask: 0.0000 (0.0001)  time: 0.1959  data: 0.0002  max mem: 5511
[00:55:48.470252] Epoch: [90]  [620/781]  eta: 0:00:31  lr: 0.000007  training_loss: 1.1179 (1.1116)  mae_loss: 0.0255 (0.0253)  classification_loss: 1.0935 (1.0862)  loss_mask: 0.0000 (0.0001)  time: 0.1977  data: 0.0002  max mem: 5511
[00:55:52.424806] Epoch: [90]  [640/781]  eta: 0:00:27  lr: 0.000007  training_loss: 1.1351 (1.1120)  mae_loss: 0.0240 (0.0253)  classification_loss: 1.1077 (1.0866)  loss_mask: 0.0000 (0.0000)  time: 0.1976  data: 0.0002  max mem: 5511
[00:55:56.356272] Epoch: [90]  [660/781]  eta: 0:00:23  lr: 0.000007  training_loss: 1.1106 (1.1122)  mae_loss: 0.0264 (0.0254)  classification_loss: 1.0863 (1.0868)  loss_mask: 0.0000 (0.0000)  time: 0.1965  data: 0.0002  max mem: 5511
[00:56:00.289237] Epoch: [90]  [680/781]  eta: 0:00:19  lr: 0.000007  training_loss: 1.0960 (1.1120)  mae_loss: 0.0257 (0.0254)  classification_loss: 1.0689 (1.0865)  loss_mask: 0.0000 (0.0000)  time: 0.1966  data: 0.0002  max mem: 5511
[00:56:04.212738] Epoch: [90]  [700/781]  eta: 0:00:16  lr: 0.000007  training_loss: 1.1025 (1.1125)  mae_loss: 0.0242 (0.0254)  classification_loss: 1.0753 (1.0871)  loss_mask: 0.0000 (0.0000)  time: 0.1961  data: 0.0002  max mem: 5511
[00:56:08.119393] Epoch: [90]  [720/781]  eta: 0:00:12  lr: 0.000007  training_loss: 1.0788 (1.1117)  mae_loss: 0.0247 (0.0254)  classification_loss: 1.0512 (1.0863)  loss_mask: 0.0000 (0.0000)  time: 0.1953  data: 0.0003  max mem: 5511
[00:56:12.028398] Epoch: [90]  [740/781]  eta: 0:00:08  lr: 0.000007  training_loss: 1.0848 (1.1111)  mae_loss: 0.0250 (0.0253)  classification_loss: 1.0600 (1.0857)  loss_mask: 0.0000 (0.0000)  time: 0.1954  data: 0.0002  max mem: 5511
[00:56:16.005172] Epoch: [90]  [760/781]  eta: 0:00:04  lr: 0.000007  training_loss: 1.1771 (1.1123)  mae_loss: 0.0250 (0.0253)  classification_loss: 1.1523 (1.0869)  loss_mask: 0.0000 (0.0000)  time: 0.1987  data: 0.0002  max mem: 5511
[00:56:19.902163] Epoch: [90]  [780/781]  eta: 0:00:00  lr: 0.000006  training_loss: 1.1612 (1.1139)  mae_loss: 0.0240 (0.0253)  classification_loss: 1.1346 (1.0885)  loss_mask: 0.0000 (0.0000)  time: 0.1948  data: 0.0002  max mem: 5511
[00:56:20.054503] Epoch: [90] Total time: 0:02:34 (0.1977 s / it)
[00:56:20.055035] Averaged stats: lr: 0.000006  training_loss: 1.1612 (1.1139)  mae_loss: 0.0240 (0.0253)  classification_loss: 1.1346 (1.0885)  loss_mask: 0.0000 (0.0000)
[00:56:21.343102] Test:  [  0/157]  eta: 0:01:34  testing_loss: 0.5284 (0.5284)  acc1: 84.3750 (84.3750)  acc5: 98.4375 (98.4375)  time: 0.6034  data: 0.5735  max mem: 5511
[00:56:21.633908] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.4578 (0.4628)  acc1: 85.9375 (85.9375)  acc5: 100.0000 (99.5739)  time: 0.0811  data: 0.0526  max mem: 5511
[00:56:21.920753] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4068 (0.4204)  acc1: 85.9375 (87.4256)  acc5: 100.0000 (99.5536)  time: 0.0287  data: 0.0004  max mem: 5511
[00:56:22.205446] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.4182 (0.4367)  acc1: 87.5000 (86.9960)  acc5: 100.0000 (99.2440)  time: 0.0284  data: 0.0002  max mem: 5511
[00:56:22.495446] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.4254 (0.4411)  acc1: 87.5000 (87.0427)  acc5: 98.4375 (99.2378)  time: 0.0286  data: 0.0002  max mem: 5511
[00:56:22.786700] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4254 (0.4354)  acc1: 87.5000 (87.3162)  acc5: 100.0000 (99.3566)  time: 0.0289  data: 0.0002  max mem: 5511
[00:56:23.076830] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4124 (0.4342)  acc1: 89.0625 (87.4744)  acc5: 100.0000 (99.3852)  time: 0.0289  data: 0.0002  max mem: 5511
[00:56:23.359435] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4124 (0.4291)  acc1: 89.0625 (87.6981)  acc5: 100.0000 (99.3618)  time: 0.0285  data: 0.0002  max mem: 5511
[00:56:23.642941] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4309 (0.4366)  acc1: 87.5000 (87.3650)  acc5: 100.0000 (99.3827)  time: 0.0282  data: 0.0002  max mem: 5511
[00:56:23.928800] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4189 (0.4316)  acc1: 87.5000 (87.7232)  acc5: 100.0000 (99.3475)  time: 0.0283  data: 0.0002  max mem: 5511
[00:56:24.217321] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4189 (0.4337)  acc1: 90.6250 (87.6856)  acc5: 98.4375 (99.3502)  time: 0.0286  data: 0.0002  max mem: 5511
[00:56:24.499540] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4495 (0.4339)  acc1: 85.9375 (87.6408)  acc5: 100.0000 (99.3384)  time: 0.0284  data: 0.0002  max mem: 5511
[00:56:24.789763] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4491 (0.4342)  acc1: 85.9375 (87.6550)  acc5: 100.0000 (99.3673)  time: 0.0285  data: 0.0002  max mem: 5511
[00:56:25.073985] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4152 (0.4362)  acc1: 85.9375 (87.5835)  acc5: 100.0000 (99.3321)  time: 0.0286  data: 0.0002  max mem: 5511
[00:56:25.355620] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4192 (0.4354)  acc1: 85.9375 (87.5332)  acc5: 100.0000 (99.3573)  time: 0.0282  data: 0.0002  max mem: 5511
[00:56:25.636092] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4423 (0.4355)  acc1: 87.5000 (87.5621)  acc5: 100.0000 (99.3584)  time: 0.0280  data: 0.0002  max mem: 5511
[00:56:25.786212] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4196 (0.4346)  acc1: 89.0625 (87.5900)  acc5: 100.0000 (99.3600)  time: 0.0270  data: 0.0001  max mem: 5511
[00:56:25.930257] Test: Total time: 0:00:05 (0.0331 s / it)
[00:56:25.930732] * Acc@1 87.590 Acc@5 99.360 loss 0.435
[00:56:25.931019] Accuracy of the network on the 10000 test images: 87.6%
[00:56:25.931192] Max accuracy: 87.59%
[00:56:26.217524] log_dir: ./output_dir
[00:56:27.056172] Epoch: [91]  [  0/781]  eta: 0:10:53  lr: 0.000006  training_loss: 0.9529 (0.9529)  mae_loss: 0.0244 (0.0244)  classification_loss: 0.9285 (0.9285)  loss_mask: 0.0000 (0.0000)  time: 0.8369  data: 0.6176  max mem: 5511
[00:56:30.977902] Epoch: [91]  [ 20/781]  eta: 0:02:52  lr: 0.000006  training_loss: 1.0806 (1.0913)  mae_loss: 0.0254 (0.0251)  classification_loss: 1.0560 (1.0661)  loss_mask: 0.0000 (0.0000)  time: 0.1960  data: 0.0002  max mem: 5511
[00:56:34.921257] Epoch: [91]  [ 40/781]  eta: 0:02:37  lr: 0.000006  training_loss: 1.1412 (1.1107)  mae_loss: 0.0272 (0.0259)  classification_loss: 1.1125 (1.0848)  loss_mask: 0.0000 (0.0000)  time: 0.1971  data: 0.0002  max mem: 5511
[00:56:38.853204] Epoch: [91]  [ 60/781]  eta: 0:02:29  lr: 0.000006  training_loss: 1.1364 (1.1174)  mae_loss: 0.0243 (0.0256)  classification_loss: 1.1147 (1.0918)  loss_mask: 0.0000 (0.0000)  time: 0.1965  data: 0.0002  max mem: 5511
[00:56:42.775064] Epoch: [91]  [ 80/781]  eta: 0:02:23  lr: 0.000006  training_loss: 1.0901 (1.1173)  mae_loss: 0.0245 (0.0256)  classification_loss: 1.0614 (1.0917)  loss_mask: 0.0000 (0.0000)  time: 0.1960  data: 0.0002  max mem: 5511
[00:56:46.688092] Epoch: [91]  [100/781]  eta: 0:02:17  lr: 0.000006  training_loss: 1.0873 (1.1159)  mae_loss: 0.0245 (0.0255)  classification_loss: 1.0651 (1.0904)  loss_mask: 0.0000 (0.0000)  time: 0.1956  data: 0.0002  max mem: 5511
[00:56:50.596441] Epoch: [91]  [120/781]  eta: 0:02:13  lr: 0.000006  training_loss: 1.0907 (1.1123)  mae_loss: 0.0246 (0.0254)  classification_loss: 1.0652 (1.0868)  loss_mask: 0.0000 (0.0000)  time: 0.1953  data: 0.0003  max mem: 5511
[00:56:54.494055] Epoch: [91]  [140/781]  eta: 0:02:08  lr: 0.000006  training_loss: 1.0691 (1.1129)  mae_loss: 0.0267 (0.0256)  classification_loss: 1.0476 (1.0873)  loss_mask: 0.0000 (0.0000)  time: 0.1948  data: 0.0003  max mem: 5511
[00:56:58.406395] Epoch: [91]  [160/781]  eta: 0:02:04  lr: 0.000006  training_loss: 1.0595 (1.1101)  mae_loss: 0.0250 (0.0256)  classification_loss: 1.0320 (1.0845)  loss_mask: 0.0000 (0.0000)  time: 0.1955  data: 0.0002  max mem: 5511
[00:57:02.379028] Epoch: [91]  [180/781]  eta: 0:02:00  lr: 0.000006  training_loss: 1.1296 (1.1125)  mae_loss: 0.0247 (0.0255)  classification_loss: 1.1056 (1.0869)  loss_mask: 0.0000 (0.0000)  time: 0.1985  data: 0.0003  max mem: 5511
[00:57:06.292872] Epoch: [91]  [200/781]  eta: 0:01:55  lr: 0.000006  training_loss: 1.1041 (1.1138)  mae_loss: 0.0242 (0.0254)  classification_loss: 1.0863 (1.0884)  loss_mask: 0.0000 (0.0000)  time: 0.1956  data: 0.0002  max mem: 5511
[00:57:10.203886] Epoch: [91]  [220/781]  eta: 0:01:51  lr: 0.000006  training_loss: 1.1749 (1.1170)  mae_loss: 0.0261 (0.0255)  classification_loss: 1.1468 (1.0915)  loss_mask: 0.0000 (0.0000)  time: 0.1955  data: 0.0003  max mem: 5511
[00:57:14.206526] Epoch: [91]  [240/781]  eta: 0:01:47  lr: 0.000006  training_loss: 1.0922 (1.1167)  mae_loss: 0.0253 (0.0255)  classification_loss: 1.0642 (1.0912)  loss_mask: 0.0000 (0.0000)  time: 0.2000  data: 0.0002  max mem: 5511
[00:57:18.123017] Epoch: [91]  [260/781]  eta: 0:01:43  lr: 0.000006  training_loss: 1.0494 (1.1145)  mae_loss: 0.0240 (0.0254)  classification_loss: 1.0207 (1.0891)  loss_mask: 0.0000 (0.0000)  time: 0.1957  data: 0.0002  max mem: 5511
[00:57:22.033419] Epoch: [91]  [280/781]  eta: 0:01:39  lr: 0.000006  training_loss: 1.1206 (1.1151)  mae_loss: 0.0262 (0.0255)  classification_loss: 1.0942 (1.0896)  loss_mask: 0.0000 (0.0000)  time: 0.1954  data: 0.0003  max mem: 5511
[00:57:25.946937] Epoch: [91]  [300/781]  eta: 0:01:35  lr: 0.000006  training_loss: 1.1012 (1.1163)  mae_loss: 0.0253 (0.0255)  classification_loss: 1.0780 (1.0908)  loss_mask: 0.0000 (0.0000)  time: 0.1956  data: 0.0002  max mem: 5511
[00:57:29.908597] Epoch: [91]  [320/781]  eta: 0:01:31  lr: 0.000006  training_loss: 1.0924 (1.1165)  mae_loss: 0.0263 (0.0256)  classification_loss: 1.0625 (1.0909)  loss_mask: 0.0000 (0.0000)  time: 0.1980  data: 0.0002  max mem: 5511
[00:57:33.883265] Epoch: [91]  [340/781]  eta: 0:01:27  lr: 0.000006  training_loss: 1.0991 (1.1170)  mae_loss: 0.0265 (0.0256)  classification_loss: 1.0750 (1.0913)  loss_mask: 0.0000 (0.0000)  time: 0.1986  data: 0.0002  max mem: 5511
[00:57:37.837625] Epoch: [91]  [360/781]  eta: 0:01:23  lr: 0.000006  training_loss: 1.0830 (1.1164)  mae_loss: 0.0257 (0.0256)  classification_loss: 1.0548 (1.0907)  loss_mask: 0.0000 (0.0000)  time: 0.1976  data: 0.0002  max mem: 5511
[00:57:41.751495] Epoch: [91]  [380/781]  eta: 0:01:19  lr: 0.000006  training_loss: 1.1833 (1.1199)  mae_loss: 0.0260 (0.0257)  classification_loss: 1.1538 (1.0942)  loss_mask: 0.0000 (0.0000)  time: 0.1956  data: 0.0002  max mem: 5511
[00:57:45.665970] Epoch: [91]  [400/781]  eta: 0:01:15  lr: 0.000006  training_loss: 1.1219 (1.1203)  mae_loss: 0.0266 (0.0257)  classification_loss: 1.0950 (1.0945)  loss_mask: 0.0000 (0.0000)  time: 0.1956  data: 0.0004  max mem: 5511
[00:57:49.588220] Epoch: [91]  [420/781]  eta: 0:01:11  lr: 0.000006  training_loss: 1.1184 (1.1198)  mae_loss: 0.0255 (0.0257)  classification_loss: 1.0933 (1.0940)  loss_mask: 0.0000 (0.0001)  time: 0.1960  data: 0.0002  max mem: 5511
[00:57:53.502594] Epoch: [91]  [440/781]  eta: 0:01:07  lr: 0.000006  training_loss: 1.0892 (1.1190)  mae_loss: 0.0251 (0.0257)  classification_loss: 1.0655 (1.0932)  loss_mask: 0.0000 (0.0001)  time: 0.1956  data: 0.0003  max mem: 5511
[00:57:57.450709] Epoch: [91]  [460/781]  eta: 0:01:03  lr: 0.000006  training_loss: 1.0981 (1.1175)  mae_loss: 0.0236 (0.0256)  classification_loss: 1.0747 (1.0918)  loss_mask: 0.0000 (0.0001)  time: 0.1973  data: 0.0002  max mem: 5511
[00:58:01.387439] Epoch: [91]  [480/781]  eta: 0:00:59  lr: 0.000006  training_loss: 1.0973 (1.1176)  mae_loss: 0.0257 (0.0256)  classification_loss: 1.0698 (1.0919)  loss_mask: 0.0000 (0.0001)  time: 0.1968  data: 0.0002  max mem: 5511
[00:58:05.329916] Epoch: [91]  [500/781]  eta: 0:00:55  lr: 0.000006  training_loss: 1.0985 (1.1173)  mae_loss: 0.0250 (0.0256)  classification_loss: 1.0730 (1.0917)  loss_mask: 0.0000 (0.0001)  time: 0.1970  data: 0.0002  max mem: 5511
[00:58:09.239965] Epoch: [91]  [520/781]  eta: 0:00:51  lr: 0.000006  training_loss: 1.0840 (1.1170)  mae_loss: 0.0243 (0.0256)  classification_loss: 1.0583 (1.0914)  loss_mask: 0.0000 (0.0001)  time: 0.1954  data: 0.0003  max mem: 5511
[00:58:13.158037] Epoch: [91]  [540/781]  eta: 0:00:47  lr: 0.000006  training_loss: 1.0757 (1.1162)  mae_loss: 0.0239 (0.0255)  classification_loss: 1.0529 (1.0906)  loss_mask: 0.0000 (0.0001)  time: 0.1958  data: 0.0002  max mem: 5511
[00:58:17.095572] Epoch: [91]  [560/781]  eta: 0:00:43  lr: 0.000006  training_loss: 1.1027 (1.1157)  mae_loss: 0.0257 (0.0256)  classification_loss: 1.0740 (1.0900)  loss_mask: 0.0000 (0.0001)  time: 0.1968  data: 0.0002  max mem: 5511
[00:58:21.021155] Epoch: [91]  [580/781]  eta: 0:00:39  lr: 0.000006  training_loss: 1.0886 (1.1144)  mae_loss: 0.0262 (0.0256)  classification_loss: 1.0619 (1.0888)  loss_mask: 0.0000 (0.0001)  time: 0.1962  data: 0.0002  max mem: 5511
[00:58:24.950361] Epoch: [91]  [600/781]  eta: 0:00:35  lr: 0.000006  training_loss: 1.0721 (1.1136)  mae_loss: 0.0250 (0.0256)  classification_loss: 1.0456 (1.0880)  loss_mask: 0.0000 (0.0001)  time: 0.1964  data: 0.0003  max mem: 5511
[00:58:28.906234] Epoch: [91]  [620/781]  eta: 0:00:31  lr: 0.000006  training_loss: 1.1054 (1.1133)  mae_loss: 0.0251 (0.0256)  classification_loss: 1.0799 (1.0876)  loss_mask: 0.0000 (0.0001)  time: 0.1977  data: 0.0002  max mem: 5511
[00:58:32.889504] Epoch: [91]  [640/781]  eta: 0:00:27  lr: 0.000006  training_loss: 1.0772 (1.1127)  mae_loss: 0.0268 (0.0256)  classification_loss: 1.0559 (1.0870)  loss_mask: 0.0000 (0.0001)  time: 0.1991  data: 0.0003  max mem: 5511
[00:58:36.828531] Epoch: [91]  [660/781]  eta: 0:00:23  lr: 0.000005  training_loss: 1.0934 (1.1129)  mae_loss: 0.0252 (0.0256)  classification_loss: 1.0664 (1.0872)  loss_mask: 0.0000 (0.0001)  time: 0.1968  data: 0.0002  max mem: 5511
[00:58:40.771962] Epoch: [91]  [680/781]  eta: 0:00:19  lr: 0.000005  training_loss: 1.0787 (1.1128)  mae_loss: 0.0257 (0.0256)  classification_loss: 1.0530 (1.0871)  loss_mask: 0.0000 (0.0001)  time: 0.1971  data: 0.0002  max mem: 5511
[00:58:44.711206] Epoch: [91]  [700/781]  eta: 0:00:15  lr: 0.000005  training_loss: 1.1169 (1.1122)  mae_loss: 0.0257 (0.0256)  classification_loss: 1.0887 (1.0865)  loss_mask: 0.0000 (0.0001)  time: 0.1969  data: 0.0005  max mem: 5511
[00:58:48.617190] Epoch: [91]  [720/781]  eta: 0:00:12  lr: 0.000005  training_loss: 1.1170 (1.1123)  mae_loss: 0.0253 (0.0256)  classification_loss: 1.0843 (1.0866)  loss_mask: 0.0000 (0.0001)  time: 0.1952  data: 0.0002  max mem: 5511
[00:58:52.537822] Epoch: [91]  [740/781]  eta: 0:00:08  lr: 0.000005  training_loss: 1.0822 (1.1121)  mae_loss: 0.0257 (0.0256)  classification_loss: 1.0560 (1.0864)  loss_mask: 0.0000 (0.0001)  time: 0.1960  data: 0.0002  max mem: 5511
[00:58:56.487973] Epoch: [91]  [760/781]  eta: 0:00:04  lr: 0.000005  training_loss: 1.1515 (1.1134)  mae_loss: 0.0260 (0.0257)  classification_loss: 1.1288 (1.0877)  loss_mask: 0.0000 (0.0001)  time: 0.1974  data: 0.0002  max mem: 5511
[00:59:00.384978] Epoch: [91]  [780/781]  eta: 0:00:00  lr: 0.000005  training_loss: 1.1394 (1.1138)  mae_loss: 0.0256 (0.0257)  classification_loss: 1.1150 (1.0881)  loss_mask: 0.0000 (0.0001)  time: 0.1948  data: 0.0001  max mem: 5511
[00:59:00.539865] Epoch: [91] Total time: 0:02:34 (0.1976 s / it)
[00:59:00.540766] Averaged stats: lr: 0.000005  training_loss: 1.1394 (1.1138)  mae_loss: 0.0256 (0.0257)  classification_loss: 1.1150 (1.0881)  loss_mask: 0.0000 (0.0001)
[00:59:01.183585] Test:  [  0/157]  eta: 0:01:40  testing_loss: 0.5387 (0.5387)  acc1: 84.3750 (84.3750)  acc5: 100.0000 (100.0000)  time: 0.6387  data: 0.6091  max mem: 5511
[00:59:01.468105] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4548 (0.4678)  acc1: 85.9375 (85.7955)  acc5: 100.0000 (99.5739)  time: 0.0838  data: 0.0556  max mem: 5511
[00:59:01.752068] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4097 (0.4232)  acc1: 87.5000 (87.2768)  acc5: 100.0000 (99.4792)  time: 0.0283  data: 0.0002  max mem: 5511
[00:59:02.039362] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4170 (0.4411)  acc1: 89.0625 (86.7944)  acc5: 100.0000 (99.2944)  time: 0.0284  data: 0.0002  max mem: 5511
[00:59:02.322933] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4485 (0.4465)  acc1: 87.5000 (86.8140)  acc5: 98.4375 (99.1616)  time: 0.0284  data: 0.0002  max mem: 5511
[00:59:02.609760] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4323 (0.4404)  acc1: 87.5000 (87.0711)  acc5: 100.0000 (99.2953)  time: 0.0284  data: 0.0002  max mem: 5511
[00:59:02.892436] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4167 (0.4385)  acc1: 85.9375 (87.1926)  acc5: 100.0000 (99.3340)  time: 0.0284  data: 0.0002  max mem: 5511
[00:59:03.176496] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4108 (0.4326)  acc1: 89.0625 (87.4340)  acc5: 100.0000 (99.3398)  time: 0.0282  data: 0.0002  max mem: 5511
[00:59:03.460204] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4187 (0.4400)  acc1: 85.9375 (87.1914)  acc5: 100.0000 (99.3827)  time: 0.0283  data: 0.0002  max mem: 5511
[00:59:03.743477] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4282 (0.4350)  acc1: 89.0625 (87.5172)  acc5: 100.0000 (99.3304)  time: 0.0282  data: 0.0002  max mem: 5511
[00:59:04.025784] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4340 (0.4377)  acc1: 87.5000 (87.4845)  acc5: 98.4375 (99.3193)  time: 0.0281  data: 0.0002  max mem: 5511
[00:59:04.310367] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4652 (0.4376)  acc1: 87.5000 (87.4718)  acc5: 100.0000 (99.3102)  time: 0.0282  data: 0.0002  max mem: 5511
[00:59:04.601251] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4699 (0.4382)  acc1: 87.5000 (87.4225)  acc5: 100.0000 (99.3285)  time: 0.0286  data: 0.0002  max mem: 5511
[00:59:04.884169] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4196 (0.4396)  acc1: 87.5000 (87.4165)  acc5: 100.0000 (99.3440)  time: 0.0285  data: 0.0002  max mem: 5511
[00:59:05.166231] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4217 (0.4390)  acc1: 85.9375 (87.3892)  acc5: 100.0000 (99.3684)  time: 0.0281  data: 0.0002  max mem: 5511
[00:59:05.445989] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4481 (0.4393)  acc1: 87.5000 (87.4586)  acc5: 100.0000 (99.3791)  time: 0.0280  data: 0.0001  max mem: 5511
[00:59:05.596185] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4217 (0.4381)  acc1: 87.5000 (87.4700)  acc5: 100.0000 (99.3800)  time: 0.0270  data: 0.0001  max mem: 5511
[00:59:05.743113] Test: Total time: 0:00:05 (0.0331 s / it)
[00:59:05.743587] * Acc@1 87.470 Acc@5 99.380 loss 0.438
[00:59:05.743889] Accuracy of the network on the 10000 test images: 87.5%
[00:59:05.744067] Max accuracy: 87.59%
[00:59:06.032305] log_dir: ./output_dir
[00:59:06.887360] Epoch: [92]  [  0/781]  eta: 0:11:06  lr: 0.000005  training_loss: 0.8637 (0.8637)  mae_loss: 0.0262 (0.0262)  classification_loss: 0.8375 (0.8375)  loss_mask: 0.0000 (0.0000)  time: 0.8532  data: 0.6467  max mem: 5511
[00:59:10.808866] Epoch: [92]  [ 20/781]  eta: 0:02:52  lr: 0.000005  training_loss: 1.1073 (1.0776)  mae_loss: 0.0254 (0.0258)  classification_loss: 1.0795 (1.0518)  loss_mask: 0.0000 (0.0000)  time: 0.1960  data: 0.0003  max mem: 5511
[00:59:14.754910] Epoch: [92]  [ 40/781]  eta: 0:02:37  lr: 0.000005  training_loss: 1.1128 (1.1033)  mae_loss: 0.0247 (0.0256)  classification_loss: 1.0875 (1.0777)  loss_mask: 0.0000 (0.0000)  time: 0.1972  data: 0.0002  max mem: 5511
[00:59:18.672528] Epoch: [92]  [ 60/781]  eta: 0:02:29  lr: 0.000005  training_loss: 1.1233 (1.1099)  mae_loss: 0.0249 (0.0255)  classification_loss: 1.1023 (1.0842)  loss_mask: 0.0000 (0.0001)  time: 0.1958  data: 0.0002  max mem: 5511
[00:59:22.591664] Epoch: [92]  [ 80/781]  eta: 0:02:23  lr: 0.000005  training_loss: 1.0965 (1.1108)  mae_loss: 0.0256 (0.0254)  classification_loss: 1.0775 (1.0853)  loss_mask: 0.0000 (0.0001)  time: 0.1959  data: 0.0002  max mem: 5511
[00:59:26.502501] Epoch: [92]  [100/781]  eta: 0:02:17  lr: 0.000005  training_loss: 1.1052 (1.1132)  mae_loss: 0.0251 (0.0255)  classification_loss: 1.0783 (1.0876)  loss_mask: 0.0000 (0.0001)  time: 0.1955  data: 0.0002  max mem: 5511
[00:59:30.443565] Epoch: [92]  [120/781]  eta: 0:02:13  lr: 0.000005  training_loss: 1.1623 (1.1200)  mae_loss: 0.0258 (0.0256)  classification_loss: 1.1358 (1.0944)  loss_mask: 0.0000 (0.0001)  time: 0.1970  data: 0.0002  max mem: 5511
[00:59:34.382744] Epoch: [92]  [140/781]  eta: 0:02:08  lr: 0.000005  training_loss: 1.1125 (1.1176)  mae_loss: 0.0246 (0.0255)  classification_loss: 1.0883 (1.0920)  loss_mask: 0.0000 (0.0001)  time: 0.1969  data: 0.0002  max mem: 5511
[00:59:38.304270] Epoch: [92]  [160/781]  eta: 0:02:04  lr: 0.000005  training_loss: 1.0976 (1.1191)  mae_loss: 0.0271 (0.0256)  classification_loss: 1.0705 (1.0934)  loss_mask: 0.0000 (0.0001)  time: 0.1960  data: 0.0002  max mem: 5511
[00:59:42.259302] Epoch: [92]  [180/781]  eta: 0:02:00  lr: 0.000005  training_loss: 1.1109 (1.1198)  mae_loss: 0.0259 (0.0256)  classification_loss: 1.0815 (1.0941)  loss_mask: 0.0000 (0.0001)  time: 0.1977  data: 0.0002  max mem: 5511
[00:59:46.199215] Epoch: [92]  [200/781]  eta: 0:01:56  lr: 0.000005  training_loss: 1.1158 (1.1179)  mae_loss: 0.0252 (0.0256)  classification_loss: 1.0844 (1.0922)  loss_mask: 0.0000 (0.0001)  time: 0.1969  data: 0.0004  max mem: 5511
[00:59:50.128254] Epoch: [92]  [220/781]  eta: 0:01:51  lr: 0.000005  training_loss: 1.1129 (1.1173)  mae_loss: 0.0261 (0.0256)  classification_loss: 1.0862 (1.0916)  loss_mask: 0.0000 (0.0001)  time: 0.1964  data: 0.0002  max mem: 5511
[00:59:54.076437] Epoch: [92]  [240/781]  eta: 0:01:47  lr: 0.000005  training_loss: 1.0769 (1.1179)  mae_loss: 0.0252 (0.0256)  classification_loss: 1.0523 (1.0922)  loss_mask: 0.0000 (0.0001)  time: 0.1973  data: 0.0002  max mem: 5511
[00:59:57.983910] Epoch: [92]  [260/781]  eta: 0:01:43  lr: 0.000005  training_loss: 1.1266 (1.1180)  mae_loss: 0.0248 (0.0256)  classification_loss: 1.1007 (1.0924)  loss_mask: 0.0000 (0.0001)  time: 0.1953  data: 0.0002  max mem: 5511
[01:00:01.926781] Epoch: [92]  [280/781]  eta: 0:01:39  lr: 0.000005  training_loss: 1.0831 (1.1164)  mae_loss: 0.0251 (0.0256)  classification_loss: 1.0535 (1.0907)  loss_mask: 0.0000 (0.0001)  time: 0.1971  data: 0.0002  max mem: 5511
[01:00:05.851361] Epoch: [92]  [300/781]  eta: 0:01:35  lr: 0.000005  training_loss: 1.0678 (1.1166)  mae_loss: 0.0255 (0.0256)  classification_loss: 1.0383 (1.0908)  loss_mask: 0.0001 (0.0002)  time: 0.1962  data: 0.0002  max mem: 5511
[01:00:09.764204] Epoch: [92]  [320/781]  eta: 0:01:31  lr: 0.000005  training_loss: 1.1149 (1.1167)  mae_loss: 0.0251 (0.0256)  classification_loss: 1.0888 (1.0910)  loss_mask: 0.0000 (0.0002)  time: 0.1955  data: 0.0003  max mem: 5511
[01:00:13.704177] Epoch: [92]  [340/781]  eta: 0:01:27  lr: 0.000005  training_loss: 1.1513 (1.1172)  mae_loss: 0.0258 (0.0256)  classification_loss: 1.1278 (1.0914)  loss_mask: 0.0000 (0.0001)  time: 0.1969  data: 0.0003  max mem: 5511
[01:00:17.607114] Epoch: [92]  [360/781]  eta: 0:01:23  lr: 0.000005  training_loss: 1.0749 (1.1156)  mae_loss: 0.0248 (0.0256)  classification_loss: 1.0497 (1.0899)  loss_mask: 0.0000 (0.0001)  time: 0.1951  data: 0.0002  max mem: 5511
[01:00:21.540060] Epoch: [92]  [380/781]  eta: 0:01:19  lr: 0.000005  training_loss: 1.1604 (1.1173)  mae_loss: 0.0251 (0.0256)  classification_loss: 1.1383 (1.0916)  loss_mask: 0.0000 (0.0001)  time: 0.1966  data: 0.0003  max mem: 5511
[01:00:25.466204] Epoch: [92]  [400/781]  eta: 0:01:15  lr: 0.000005  training_loss: 1.0780 (1.1160)  mae_loss: 0.0247 (0.0256)  classification_loss: 1.0527 (1.0903)  loss_mask: 0.0000 (0.0001)  time: 0.1962  data: 0.0002  max mem: 5511
[01:00:29.401162] Epoch: [92]  [420/781]  eta: 0:01:11  lr: 0.000005  training_loss: 1.1336 (1.1156)  mae_loss: 0.0265 (0.0257)  classification_loss: 1.1080 (1.0898)  loss_mask: 0.0000 (0.0001)  time: 0.1966  data: 0.0002  max mem: 5511
[01:00:33.328852] Epoch: [92]  [440/781]  eta: 0:01:07  lr: 0.000005  training_loss: 1.0561 (1.1135)  mae_loss: 0.0252 (0.0257)  classification_loss: 1.0333 (1.0877)  loss_mask: 0.0000 (0.0001)  time: 0.1963  data: 0.0002  max mem: 5511
[01:00:37.262639] Epoch: [92]  [460/781]  eta: 0:01:03  lr: 0.000005  training_loss: 1.0729 (1.1120)  mae_loss: 0.0239 (0.0256)  classification_loss: 1.0528 (1.0863)  loss_mask: 0.0000 (0.0001)  time: 0.1966  data: 0.0002  max mem: 5511
[01:00:41.206968] Epoch: [92]  [480/781]  eta: 0:00:59  lr: 0.000005  training_loss: 1.1517 (1.1137)  mae_loss: 0.0239 (0.0256)  classification_loss: 1.1301 (1.0880)  loss_mask: 0.0000 (0.0001)  time: 0.1971  data: 0.0002  max mem: 5511
[01:00:45.104148] Epoch: [92]  [500/781]  eta: 0:00:55  lr: 0.000005  training_loss: 1.1279 (1.1142)  mae_loss: 0.0254 (0.0256)  classification_loss: 1.0979 (1.0885)  loss_mask: 0.0000 (0.0001)  time: 0.1948  data: 0.0002  max mem: 5511
[01:00:49.018896] Epoch: [92]  [520/781]  eta: 0:00:51  lr: 0.000005  training_loss: 1.0967 (1.1143)  mae_loss: 0.0259 (0.0256)  classification_loss: 1.0707 (1.0885)  loss_mask: 0.0000 (0.0001)  time: 0.1956  data: 0.0002  max mem: 5511
[01:00:52.924574] Epoch: [92]  [540/781]  eta: 0:00:47  lr: 0.000005  training_loss: 1.1570 (1.1157)  mae_loss: 0.0251 (0.0257)  classification_loss: 1.1312 (1.0899)  loss_mask: 0.0000 (0.0001)  time: 0.1952  data: 0.0002  max mem: 5511
[01:00:56.884455] Epoch: [92]  [560/781]  eta: 0:00:43  lr: 0.000005  training_loss: 1.1359 (1.1159)  mae_loss: 0.0255 (0.0256)  classification_loss: 1.1110 (1.0901)  loss_mask: 0.0000 (0.0001)  time: 0.1979  data: 0.0002  max mem: 5511
[01:01:00.817608] Epoch: [92]  [580/781]  eta: 0:00:39  lr: 0.000005  training_loss: 1.1134 (1.1156)  mae_loss: 0.0249 (0.0256)  classification_loss: 1.0893 (1.0898)  loss_mask: 0.0000 (0.0001)  time: 0.1966  data: 0.0002  max mem: 5511
[01:01:04.745942] Epoch: [92]  [600/781]  eta: 0:00:35  lr: 0.000005  training_loss: 1.0942 (1.1158)  mae_loss: 0.0248 (0.0256)  classification_loss: 1.0692 (1.0901)  loss_mask: 0.0000 (0.0001)  time: 0.1963  data: 0.0002  max mem: 5511
[01:01:08.683307] Epoch: [92]  [620/781]  eta: 0:00:31  lr: 0.000005  training_loss: 1.0922 (1.1156)  mae_loss: 0.0261 (0.0256)  classification_loss: 1.0658 (1.0899)  loss_mask: 0.0000 (0.0001)  time: 0.1968  data: 0.0002  max mem: 5511
[01:01:12.619313] Epoch: [92]  [640/781]  eta: 0:00:27  lr: 0.000004  training_loss: 1.1195 (1.1162)  mae_loss: 0.0263 (0.0257)  classification_loss: 1.0982 (1.0904)  loss_mask: 0.0000 (0.0001)  time: 0.1967  data: 0.0002  max mem: 5511
[01:01:16.571211] Epoch: [92]  [660/781]  eta: 0:00:23  lr: 0.000004  training_loss: 1.1355 (1.1168)  mae_loss: 0.0246 (0.0256)  classification_loss: 1.1117 (1.0911)  loss_mask: 0.0000 (0.0001)  time: 0.1975  data: 0.0002  max mem: 5511
[01:01:20.489197] Epoch: [92]  [680/781]  eta: 0:00:19  lr: 0.000004  training_loss: 1.1066 (1.1174)  mae_loss: 0.0253 (0.0256)  classification_loss: 1.0829 (1.0916)  loss_mask: 0.0000 (0.0001)  time: 0.1958  data: 0.0003  max mem: 5511
[01:01:24.383168] Epoch: [92]  [700/781]  eta: 0:00:15  lr: 0.000004  training_loss: 1.1490 (1.1173)  mae_loss: 0.0259 (0.0257)  classification_loss: 1.1201 (1.0916)  loss_mask: 0.0000 (0.0001)  time: 0.1946  data: 0.0003  max mem: 5511
[01:01:28.299842] Epoch: [92]  [720/781]  eta: 0:00:12  lr: 0.000004  training_loss: 1.0938 (1.1166)  mae_loss: 0.0251 (0.0257)  classification_loss: 1.0719 (1.0908)  loss_mask: 0.0001 (0.0001)  time: 0.1958  data: 0.0002  max mem: 5511
[01:01:32.197878] Epoch: [92]  [740/781]  eta: 0:00:08  lr: 0.000004  training_loss: 1.0908 (1.1161)  mae_loss: 0.0250 (0.0256)  classification_loss: 1.0677 (1.0903)  loss_mask: 0.0000 (0.0001)  time: 0.1948  data: 0.0002  max mem: 5511
[01:01:36.129695] Epoch: [92]  [760/781]  eta: 0:00:04  lr: 0.000004  training_loss: 1.1507 (1.1168)  mae_loss: 0.0259 (0.0256)  classification_loss: 1.1266 (1.0910)  loss_mask: 0.0000 (0.0001)  time: 0.1965  data: 0.0004  max mem: 5511
[01:01:40.036207] Epoch: [92]  [780/781]  eta: 0:00:00  lr: 0.000004  training_loss: 1.1446 (1.1170)  mae_loss: 0.0253 (0.0256)  classification_loss: 1.1176 (1.0913)  loss_mask: 0.0000 (0.0001)  time: 0.1953  data: 0.0002  max mem: 5511
[01:01:40.191931] Epoch: [92] Total time: 0:02:34 (0.1974 s / it)
[01:01:40.192424] Averaged stats: lr: 0.000004  training_loss: 1.1446 (1.1170)  mae_loss: 0.0253 (0.0256)  classification_loss: 1.1176 (1.0913)  loss_mask: 0.0000 (0.0001)
[01:01:40.814489] Test:  [  0/157]  eta: 0:01:36  testing_loss: 0.5316 (0.5316)  acc1: 84.3750 (84.3750)  acc5: 100.0000 (100.0000)  time: 0.6117  data: 0.5820  max mem: 5511
[01:01:41.108333] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4523 (0.4626)  acc1: 85.9375 (86.0795)  acc5: 100.0000 (99.5739)  time: 0.0821  data: 0.0533  max mem: 5511
[01:01:41.394684] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4153 (0.4195)  acc1: 87.5000 (87.7976)  acc5: 100.0000 (99.4792)  time: 0.0288  data: 0.0003  max mem: 5511
[01:01:41.691049] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4153 (0.4378)  acc1: 87.5000 (87.1472)  acc5: 100.0000 (99.1935)  time: 0.0290  data: 0.0003  max mem: 5511
[01:01:41.978602] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4461 (0.4435)  acc1: 87.5000 (87.0046)  acc5: 98.4375 (99.0473)  time: 0.0290  data: 0.0003  max mem: 5511
[01:01:42.275679] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4365 (0.4368)  acc1: 87.5000 (87.4081)  acc5: 100.0000 (99.2034)  time: 0.0290  data: 0.0002  max mem: 5511
[01:01:42.567119] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4041 (0.4348)  acc1: 89.0625 (87.4744)  acc5: 100.0000 (99.2316)  time: 0.0292  data: 0.0002  max mem: 5511
[01:01:42.869090] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4087 (0.4298)  acc1: 89.0625 (87.6981)  acc5: 100.0000 (99.2298)  time: 0.0295  data: 0.0002  max mem: 5511
[01:01:43.153339] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4232 (0.4369)  acc1: 87.5000 (87.4421)  acc5: 100.0000 (99.2670)  time: 0.0291  data: 0.0002  max mem: 5511
[01:01:43.442090] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4232 (0.4319)  acc1: 89.0625 (87.7232)  acc5: 100.0000 (99.2445)  time: 0.0285  data: 0.0002  max mem: 5511
[01:01:43.726334] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4234 (0.4341)  acc1: 89.0625 (87.6702)  acc5: 98.4375 (99.2420)  time: 0.0285  data: 0.0002  max mem: 5511
[01:01:44.011243] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4527 (0.4340)  acc1: 87.5000 (87.6830)  acc5: 100.0000 (99.2539)  time: 0.0283  data: 0.0002  max mem: 5511
[01:01:44.299754] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4789 (0.4348)  acc1: 87.5000 (87.6291)  acc5: 100.0000 (99.2898)  time: 0.0285  data: 0.0002  max mem: 5511
[01:01:44.594276] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4157 (0.4361)  acc1: 87.5000 (87.5596)  acc5: 100.0000 (99.2844)  time: 0.0290  data: 0.0002  max mem: 5511
[01:01:44.884002] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4126 (0.4348)  acc1: 87.5000 (87.5665)  acc5: 100.0000 (99.3129)  time: 0.0290  data: 0.0002  max mem: 5511
[01:01:45.166686] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4310 (0.4348)  acc1: 87.5000 (87.6035)  acc5: 100.0000 (99.3274)  time: 0.0285  data: 0.0002  max mem: 5511
[01:01:45.316836] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4126 (0.4338)  acc1: 87.5000 (87.6100)  acc5: 100.0000 (99.3300)  time: 0.0272  data: 0.0001  max mem: 5511
[01:01:45.465726] Test: Total time: 0:00:05 (0.0336 s / it)
[01:01:45.466438] * Acc@1 87.610 Acc@5 99.330 loss 0.434
[01:01:45.466735] Accuracy of the network on the 10000 test images: 87.6%
[01:01:45.466934] Max accuracy: 87.61%
[01:01:45.766113] log_dir: ./output_dir
[01:01:46.601507] Epoch: [93]  [  0/781]  eta: 0:10:51  lr: 0.000004  training_loss: 1.0137 (1.0137)  mae_loss: 0.0212 (0.0212)  classification_loss: 0.9925 (0.9925)  loss_mask: 0.0000 (0.0000)  time: 0.8337  data: 0.6187  max mem: 5511
[01:01:50.533873] Epoch: [93]  [ 20/781]  eta: 0:02:52  lr: 0.000004  training_loss: 1.0564 (1.0707)  mae_loss: 0.0263 (0.0258)  classification_loss: 1.0274 (1.0448)  loss_mask: 0.0000 (0.0000)  time: 0.1965  data: 0.0002  max mem: 5511
[01:01:54.463755] Epoch: [93]  [ 40/781]  eta: 0:02:37  lr: 0.000004  training_loss: 1.0874 (1.0860)  mae_loss: 0.0230 (0.0250)  classification_loss: 1.0646 (1.0610)  loss_mask: 0.0000 (0.0000)  time: 0.1964  data: 0.0003  max mem: 5511
[01:01:58.387225] Epoch: [93]  [ 60/781]  eta: 0:02:29  lr: 0.000004  training_loss: 1.0897 (1.0919)  mae_loss: 0.0267 (0.0255)  classification_loss: 1.0611 (1.0663)  loss_mask: 0.0000 (0.0000)  time: 0.1961  data: 0.0002  max mem: 5511
[01:02:02.317270] Epoch: [93]  [ 80/781]  eta: 0:02:23  lr: 0.000004  training_loss: 1.1003 (1.0904)  mae_loss: 0.0245 (0.0256)  classification_loss: 1.0653 (1.0647)  loss_mask: 0.0000 (0.0000)  time: 0.1964  data: 0.0003  max mem: 5511
[01:02:06.269422] Epoch: [93]  [100/781]  eta: 0:02:18  lr: 0.000004  training_loss: 1.1277 (1.0959)  mae_loss: 0.0238 (0.0255)  classification_loss: 1.1032 (1.0703)  loss_mask: 0.0000 (0.0000)  time: 0.1975  data: 0.0003  max mem: 5511
[01:02:10.180230] Epoch: [93]  [120/781]  eta: 0:02:13  lr: 0.000004  training_loss: 1.0885 (1.0959)  mae_loss: 0.0253 (0.0255)  classification_loss: 1.0649 (1.0704)  loss_mask: 0.0000 (0.0000)  time: 0.1955  data: 0.0002  max mem: 5511
[01:02:14.113722] Epoch: [93]  [140/781]  eta: 0:02:08  lr: 0.000004  training_loss: 1.0819 (1.0943)  mae_loss: 0.0241 (0.0254)  classification_loss: 1.0586 (1.0689)  loss_mask: 0.0000 (0.0000)  time: 0.1966  data: 0.0002  max mem: 5511
[01:02:18.029918] Epoch: [93]  [160/781]  eta: 0:02:04  lr: 0.000004  training_loss: 1.0960 (1.0937)  mae_loss: 0.0246 (0.0254)  classification_loss: 1.0725 (1.0682)  loss_mask: 0.0000 (0.0000)  time: 0.1957  data: 0.0002  max mem: 5511
[01:02:21.951669] Epoch: [93]  [180/781]  eta: 0:02:00  lr: 0.000004  training_loss: 1.0856 (1.0929)  mae_loss: 0.0256 (0.0254)  classification_loss: 1.0573 (1.0674)  loss_mask: 0.0000 (0.0001)  time: 0.1960  data: 0.0002  max mem: 5511
[01:02:25.898345] Epoch: [93]  [200/781]  eta: 0:01:55  lr: 0.000004  training_loss: 1.0929 (1.0930)  mae_loss: 0.0249 (0.0254)  classification_loss: 1.0653 (1.0675)  loss_mask: 0.0001 (0.0001)  time: 0.1973  data: 0.0002  max mem: 5511
[01:02:29.807391] Epoch: [93]  [220/781]  eta: 0:01:51  lr: 0.000004  training_loss: 1.0912 (1.0931)  mae_loss: 0.0234 (0.0253)  classification_loss: 1.0667 (1.0677)  loss_mask: 0.0000 (0.0001)  time: 0.1953  data: 0.0002  max mem: 5511
[01:02:33.719445] Epoch: [93]  [240/781]  eta: 0:01:47  lr: 0.000004  training_loss: 1.1204 (1.0936)  mae_loss: 0.0240 (0.0253)  classification_loss: 1.0941 (1.0683)  loss_mask: 0.0000 (0.0001)  time: 0.1955  data: 0.0002  max mem: 5511
[01:02:37.654926] Epoch: [93]  [260/781]  eta: 0:01:43  lr: 0.000004  training_loss: 1.0931 (1.0935)  mae_loss: 0.0255 (0.0253)  classification_loss: 1.0631 (1.0681)  loss_mask: 0.0000 (0.0001)  time: 0.1967  data: 0.0002  max mem: 5511
[01:02:41.566583] Epoch: [93]  [280/781]  eta: 0:01:39  lr: 0.000004  training_loss: 1.0877 (1.0942)  mae_loss: 0.0251 (0.0253)  classification_loss: 1.0591 (1.0688)  loss_mask: 0.0000 (0.0001)  time: 0.1955  data: 0.0002  max mem: 5511
[01:02:45.490493] Epoch: [93]  [300/781]  eta: 0:01:35  lr: 0.000004  training_loss: 1.1071 (1.0952)  mae_loss: 0.0257 (0.0254)  classification_loss: 1.0809 (1.0698)  loss_mask: 0.0000 (0.0001)  time: 0.1961  data: 0.0002  max mem: 5511
[01:02:49.446055] Epoch: [93]  [320/781]  eta: 0:01:31  lr: 0.000004  training_loss: 1.0934 (1.0964)  mae_loss: 0.0246 (0.0253)  classification_loss: 1.0692 (1.0710)  loss_mask: 0.0000 (0.0001)  time: 0.1977  data: 0.0002  max mem: 5511
[01:02:53.378007] Epoch: [93]  [340/781]  eta: 0:01:27  lr: 0.000004  training_loss: 1.0688 (1.0952)  mae_loss: 0.0256 (0.0254)  classification_loss: 1.0436 (1.0697)  loss_mask: 0.0000 (0.0001)  time: 0.1965  data: 0.0002  max mem: 5511
[01:02:57.287496] Epoch: [93]  [360/781]  eta: 0:01:23  lr: 0.000004  training_loss: 1.1147 (1.0969)  mae_loss: 0.0258 (0.0254)  classification_loss: 1.0856 (1.0714)  loss_mask: 0.0000 (0.0001)  time: 0.1954  data: 0.0002  max mem: 5511
[01:03:01.172618] Epoch: [93]  [380/781]  eta: 0:01:19  lr: 0.000004  training_loss: 1.0525 (1.0966)  mae_loss: 0.0248 (0.0254)  classification_loss: 1.0298 (1.0711)  loss_mask: 0.0000 (0.0001)  time: 0.1942  data: 0.0002  max mem: 5511
[01:03:05.059074] Epoch: [93]  [400/781]  eta: 0:01:15  lr: 0.000004  training_loss: 1.1275 (1.0971)  mae_loss: 0.0260 (0.0255)  classification_loss: 1.1027 (1.0715)  loss_mask: 0.0000 (0.0001)  time: 0.1942  data: 0.0002  max mem: 5511
[01:03:08.958002] Epoch: [93]  [420/781]  eta: 0:01:11  lr: 0.000004  training_loss: 1.0693 (1.0969)  mae_loss: 0.0237 (0.0254)  classification_loss: 1.0456 (1.0714)  loss_mask: 0.0000 (0.0001)  time: 0.1949  data: 0.0002  max mem: 5511
[01:03:12.898627] Epoch: [93]  [440/781]  eta: 0:01:07  lr: 0.000004  training_loss: 1.1623 (1.0994)  mae_loss: 0.0248 (0.0254)  classification_loss: 1.1375 (1.0739)  loss_mask: 0.0000 (0.0001)  time: 0.1969  data: 0.0005  max mem: 5511
[01:03:16.820053] Epoch: [93]  [460/781]  eta: 0:01:03  lr: 0.000004  training_loss: 1.0382 (1.0978)  mae_loss: 0.0253 (0.0254)  classification_loss: 1.0156 (1.0723)  loss_mask: 0.0000 (0.0001)  time: 0.1960  data: 0.0002  max mem: 5511
[01:03:20.713704] Epoch: [93]  [480/781]  eta: 0:00:59  lr: 0.000004  training_loss: 1.1333 (1.0999)  mae_loss: 0.0254 (0.0254)  classification_loss: 1.1083 (1.0744)  loss_mask: 0.0000 (0.0001)  time: 0.1946  data: 0.0002  max mem: 5511
[01:03:24.597561] Epoch: [93]  [500/781]  eta: 0:00:55  lr: 0.000004  training_loss: 1.1086 (1.1006)  mae_loss: 0.0238 (0.0254)  classification_loss: 1.0813 (1.0752)  loss_mask: 0.0000 (0.0001)  time: 0.1941  data: 0.0002  max mem: 5511
[01:03:28.487471] Epoch: [93]  [520/781]  eta: 0:00:51  lr: 0.000004  training_loss: 1.0812 (1.1004)  mae_loss: 0.0252 (0.0254)  classification_loss: 1.0573 (1.0750)  loss_mask: 0.0000 (0.0001)  time: 0.1944  data: 0.0002  max mem: 5511
[01:03:32.396795] Epoch: [93]  [540/781]  eta: 0:00:47  lr: 0.000004  training_loss: 1.0875 (1.1007)  mae_loss: 0.0237 (0.0254)  classification_loss: 1.0651 (1.0753)  loss_mask: 0.0000 (0.0001)  time: 0.1954  data: 0.0002  max mem: 5511
[01:03:36.322469] Epoch: [93]  [560/781]  eta: 0:00:43  lr: 0.000004  training_loss: 1.1217 (1.1018)  mae_loss: 0.0245 (0.0254)  classification_loss: 1.0979 (1.0764)  loss_mask: 0.0000 (0.0001)  time: 0.1962  data: 0.0002  max mem: 5511
[01:03:40.267076] Epoch: [93]  [580/781]  eta: 0:00:39  lr: 0.000004  training_loss: 1.0714 (1.1014)  mae_loss: 0.0249 (0.0253)  classification_loss: 1.0463 (1.0760)  loss_mask: 0.0000 (0.0001)  time: 0.1972  data: 0.0002  max mem: 5511
[01:03:44.195302] Epoch: [93]  [600/781]  eta: 0:00:35  lr: 0.000004  training_loss: 1.0840 (1.1010)  mae_loss: 0.0261 (0.0254)  classification_loss: 1.0580 (1.0756)  loss_mask: 0.0000 (0.0001)  time: 0.1963  data: 0.0002  max mem: 5511
[01:03:48.105912] Epoch: [93]  [620/781]  eta: 0:00:31  lr: 0.000004  training_loss: 1.1273 (1.1016)  mae_loss: 0.0269 (0.0254)  classification_loss: 1.1013 (1.0762)  loss_mask: 0.0000 (0.0001)  time: 0.1954  data: 0.0002  max mem: 5511
[01:03:52.054293] Epoch: [93]  [640/781]  eta: 0:00:27  lr: 0.000004  training_loss: 1.0890 (1.1021)  mae_loss: 0.0268 (0.0255)  classification_loss: 1.0600 (1.0766)  loss_mask: 0.0000 (0.0001)  time: 0.1973  data: 0.0003  max mem: 5511
[01:03:55.970372] Epoch: [93]  [660/781]  eta: 0:00:23  lr: 0.000004  training_loss: 1.1078 (1.1024)  mae_loss: 0.0244 (0.0255)  classification_loss: 1.0842 (1.0768)  loss_mask: 0.0000 (0.0000)  time: 0.1957  data: 0.0002  max mem: 5511
[01:03:59.882043] Epoch: [93]  [680/781]  eta: 0:00:19  lr: 0.000004  training_loss: 1.1099 (1.1027)  mae_loss: 0.0265 (0.0255)  classification_loss: 1.0909 (1.0772)  loss_mask: 0.0000 (0.0000)  time: 0.1955  data: 0.0002  max mem: 5511
[01:04:03.845673] Epoch: [93]  [700/781]  eta: 0:00:15  lr: 0.000004  training_loss: 1.0907 (1.1028)  mae_loss: 0.0259 (0.0255)  classification_loss: 1.0610 (1.0773)  loss_mask: 0.0000 (0.0000)  time: 0.1981  data: 0.0002  max mem: 5511
[01:04:07.752273] Epoch: [93]  [720/781]  eta: 0:00:12  lr: 0.000004  training_loss: 1.1403 (1.1041)  mae_loss: 0.0235 (0.0255)  classification_loss: 1.1152 (1.0785)  loss_mask: 0.0000 (0.0000)  time: 0.1952  data: 0.0002  max mem: 5511
[01:04:11.677029] Epoch: [93]  [740/781]  eta: 0:00:08  lr: 0.000003  training_loss: 1.0837 (1.1032)  mae_loss: 0.0233 (0.0254)  classification_loss: 1.0625 (1.0777)  loss_mask: 0.0000 (0.0000)  time: 0.1962  data: 0.0002  max mem: 5511
[01:04:15.576917] Epoch: [93]  [760/781]  eta: 0:00:04  lr: 0.000003  training_loss: 1.1070 (1.1038)  mae_loss: 0.0253 (0.0255)  classification_loss: 1.0785 (1.0783)  loss_mask: 0.0000 (0.0000)  time: 0.1949  data: 0.0002  max mem: 5511
[01:04:19.486707] Epoch: [93]  [780/781]  eta: 0:00:00  lr: 0.000003  training_loss: 1.0756 (1.1031)  mae_loss: 0.0241 (0.0255)  classification_loss: 1.0492 (1.0776)  loss_mask: 0.0000 (0.0000)  time: 0.1954  data: 0.0002  max mem: 5511
[01:04:19.632787] Epoch: [93] Total time: 0:02:33 (0.1970 s / it)
[01:04:19.633259] Averaged stats: lr: 0.000003  training_loss: 1.0756 (1.1031)  mae_loss: 0.0241 (0.0255)  classification_loss: 1.0492 (1.0776)  loss_mask: 0.0000 (0.0000)
[01:04:20.286096] Test:  [  0/157]  eta: 0:01:41  testing_loss: 0.5446 (0.5446)  acc1: 82.8125 (82.8125)  acc5: 98.4375 (98.4375)  time: 0.6485  data: 0.6192  max mem: 5511
[01:04:20.573126] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4410 (0.4646)  acc1: 85.9375 (85.3693)  acc5: 100.0000 (99.4318)  time: 0.0849  data: 0.0564  max mem: 5511
[01:04:20.858376] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4128 (0.4205)  acc1: 87.5000 (87.3512)  acc5: 100.0000 (99.4792)  time: 0.0285  data: 0.0002  max mem: 5511
[01:04:21.143904] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4215 (0.4392)  acc1: 87.5000 (86.8448)  acc5: 100.0000 (99.2944)  time: 0.0284  data: 0.0002  max mem: 5511
[01:04:21.433502] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4418 (0.4450)  acc1: 87.5000 (86.8521)  acc5: 100.0000 (99.1616)  time: 0.0286  data: 0.0005  max mem: 5511
[01:04:21.718471] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4409 (0.4386)  acc1: 87.5000 (87.1324)  acc5: 100.0000 (99.2953)  time: 0.0286  data: 0.0005  max mem: 5511
[01:04:22.009377] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4037 (0.4365)  acc1: 87.5000 (87.3207)  acc5: 100.0000 (99.3084)  time: 0.0287  data: 0.0002  max mem: 5511
[01:04:22.294271] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4131 (0.4318)  acc1: 87.5000 (87.5660)  acc5: 100.0000 (99.3178)  time: 0.0286  data: 0.0002  max mem: 5511
[01:04:22.579723] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4343 (0.4385)  acc1: 87.5000 (87.3264)  acc5: 100.0000 (99.3248)  time: 0.0283  data: 0.0002  max mem: 5511
[01:04:22.861855] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4278 (0.4331)  acc1: 89.0625 (87.7060)  acc5: 100.0000 (99.2788)  time: 0.0282  data: 0.0002  max mem: 5511
[01:04:23.145030] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4219 (0.4348)  acc1: 89.0625 (87.6856)  acc5: 98.4375 (99.2884)  time: 0.0281  data: 0.0002  max mem: 5511
[01:04:23.428662] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4365 (0.4350)  acc1: 87.5000 (87.6971)  acc5: 100.0000 (99.3102)  time: 0.0282  data: 0.0002  max mem: 5511
[01:04:23.712389] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4710 (0.4359)  acc1: 87.5000 (87.6679)  acc5: 100.0000 (99.3156)  time: 0.0282  data: 0.0002  max mem: 5511
[01:04:23.994400] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4224 (0.4373)  acc1: 87.5000 (87.5716)  acc5: 100.0000 (99.3321)  time: 0.0282  data: 0.0002  max mem: 5511
[01:04:24.276155] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4133 (0.4363)  acc1: 85.9375 (87.5332)  acc5: 100.0000 (99.3573)  time: 0.0281  data: 0.0002  max mem: 5511
[01:04:24.557030] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4462 (0.4363)  acc1: 87.5000 (87.5828)  acc5: 100.0000 (99.3688)  time: 0.0280  data: 0.0001  max mem: 5511
[01:04:24.708751] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4133 (0.4353)  acc1: 87.5000 (87.5900)  acc5: 100.0000 (99.3700)  time: 0.0271  data: 0.0001  max mem: 5511
[01:04:24.849858] Test: Total time: 0:00:05 (0.0332 s / it)
[01:04:24.850527] * Acc@1 87.590 Acc@5 99.370 loss 0.435
[01:04:24.850968] Accuracy of the network on the 10000 test images: 87.6%
[01:04:24.851264] Max accuracy: 87.61%
[01:04:25.185072] log_dir: ./output_dir
[01:04:26.036585] Epoch: [94]  [  0/781]  eta: 0:11:03  lr: 0.000003  training_loss: 1.1222 (1.1222)  mae_loss: 0.0275 (0.0275)  classification_loss: 1.0947 (1.0947)  loss_mask: 0.0000 (0.0000)  time: 0.8496  data: 0.6247  max mem: 5511
[01:04:29.995420] Epoch: [94]  [ 20/781]  eta: 0:02:54  lr: 0.000003  training_loss: 1.0731 (1.0925)  mae_loss: 0.0263 (0.0265)  classification_loss: 1.0430 (1.0660)  loss_mask: 0.0000 (0.0000)  time: 0.1978  data: 0.0002  max mem: 5511

[01:04:33.902788] Epoch: [94]  [ 40/781]  eta: 0:02:37  lr: 0.000003  training_loss: 1.0897 (1.0976)  mae_loss: 0.0247 (0.0258)  classification_loss: 1.0612 (1.0716)  loss_mask: 0.0000 (0.0002)  time: 0.1953  data: 0.0002  max mem: 5511
[01:04:37.802288] Epoch: [94]  [ 60/781]  eta: 0:02:29  lr: 0.000003  training_loss: 1.0812 (1.1023)  mae_loss: 0.0259 (0.0256)  classification_loss: 1.0594 (1.0766)  loss_mask: 0.0000 (0.0002)  time: 0.1949  data: 0.0002  max mem: 5511
[01:04:41.719935] Epoch: [94]  [ 80/781]  eta: 0:02:23  lr: 0.000003  training_loss: 1.0667 (1.0982)  mae_loss: 0.0263 (0.0259)  classification_loss: 1.0431 (1.0722)  loss_mask: 0.0000 (0.0002)  time: 0.1958  data: 0.0002  max mem: 5511
[01:04:45.651867] Epoch: [94]  [100/781]  eta: 0:02:17  lr: 0.000003  training_loss: 1.1057 (1.1032)  mae_loss: 0.0244 (0.0256)  classification_loss: 1.0853 (1.0774)  loss_mask: 0.0000 (0.0001)  time: 0.1965  data: 0.0002  max mem: 5511
[01:04:49.621319] Epoch: [94]  [120/781]  eta: 0:02:13  lr: 0.000003  training_loss: 1.0983 (1.1040)  mae_loss: 0.0241 (0.0255)  classification_loss: 1.0711 (1.0784)  loss_mask: 0.0000 (0.0001)  time: 0.1984  data: 0.0002  max mem: 5511
[01:04:53.573070] Epoch: [94]  [140/781]  eta: 0:02:08  lr: 0.000003  training_loss: 1.1102 (1.1055)  mae_loss: 0.0258 (0.0255)  classification_loss: 1.0862 (1.0799)  loss_mask: 0.0000 (0.0001)  time: 0.1975  data: 0.0003  max mem: 5511
[01:04:57.490425] Epoch: [94]  [160/781]  eta: 0:02:04  lr: 0.000003  training_loss: 1.0851 (1.0998)  mae_loss: 0.0242 (0.0254)  classification_loss: 1.0605 (1.0742)  loss_mask: 0.0000 (0.0001)  time: 0.1958  data: 0.0002  max mem: 5511
[01:05:01.427007] Epoch: [94]  [180/781]  eta: 0:02:00  lr: 0.000003  training_loss: 1.1341 (1.1031)  mae_loss: 0.0240 (0.0254)  classification_loss: 1.1103 (1.0777)  loss_mask: 0.0000 (0.0001)  time: 0.1966  data: 0.0002  max mem: 5511
[01:05:05.370911] Epoch: [94]  [200/781]  eta: 0:01:56  lr: 0.000003  training_loss: 1.0585 (1.1013)  mae_loss: 0.0242 (0.0253)  classification_loss: 1.0317 (1.0759)  loss_mask: 0.0000 (0.0001)  time: 0.1971  data: 0.0003  max mem: 5511
[01:05:09.333503] Epoch: [94]  [220/781]  eta: 0:01:52  lr: 0.000003  training_loss: 1.1207 (1.1043)  mae_loss: 0.0253 (0.0255)  classification_loss: 1.0954 (1.0788)  loss_mask: 0.0000 (0.0001)  time: 0.1980  data: 0.0002  max mem: 5511
[01:05:13.267284] Epoch: [94]  [240/781]  eta: 0:01:47  lr: 0.000003  training_loss: 1.0782 (1.1061)  mae_loss: 0.0250 (0.0254)  classification_loss: 1.0572 (1.0806)  loss_mask: 0.0000 (0.0001)  time: 0.1966  data: 0.0002  max mem: 5511
[01:05:17.189689] Epoch: [94]  [260/781]  eta: 0:01:43  lr: 0.000003  training_loss: 1.1067 (1.1066)  mae_loss: 0.0243 (0.0255)  classification_loss: 1.0791 (1.0811)  loss_mask: 0.0000 (0.0001)  time: 0.1960  data: 0.0002  max mem: 5511
[01:05:21.109840] Epoch: [94]  [280/781]  eta: 0:01:39  lr: 0.000003  training_loss: 1.1027 (1.1061)  mae_loss: 0.0264 (0.0255)  classification_loss: 1.0748 (1.0805)  loss_mask: 0.0000 (0.0001)  time: 0.1959  data: 0.0003  max mem: 5511
[01:05:25.055375] Epoch: [94]  [300/781]  eta: 0:01:35  lr: 0.000003  training_loss: 1.0618 (1.1053)  mae_loss: 0.0249 (0.0255)  classification_loss: 1.0358 (1.0798)  loss_mask: 0.0000 (0.0001)  time: 0.1972  data: 0.0002  max mem: 5511
[01:05:28.961558] Epoch: [94]  [320/781]  eta: 0:01:31  lr: 0.000003  training_loss: 1.0754 (1.1060)  mae_loss: 0.0255 (0.0255)  classification_loss: 1.0445 (1.0804)  loss_mask: 0.0000 (0.0001)  time: 0.1952  data: 0.0002  max mem: 5511
[01:05:32.868911] Epoch: [94]  [340/781]  eta: 0:01:27  lr: 0.000003  training_loss: 1.0252 (1.1029)  mae_loss: 0.0262 (0.0255)  classification_loss: 1.0038 (1.0773)  loss_mask: 0.0000 (0.0001)  time: 0.1953  data: 0.0002  max mem: 5511
[01:05:36.803647] Epoch: [94]  [360/781]  eta: 0:01:23  lr: 0.000003  training_loss: 1.0747 (1.1016)  mae_loss: 0.0249 (0.0255)  classification_loss: 1.0496 (1.0760)  loss_mask: 0.0000 (0.0001)  time: 0.1967  data: 0.0002  max mem: 5511
[01:05:40.728249] Epoch: [94]  [380/781]  eta: 0:01:19  lr: 0.000003  training_loss: 1.0983 (1.1022)  mae_loss: 0.0246 (0.0255)  classification_loss: 1.0734 (1.0766)  loss_mask: 0.0000 (0.0001)  time: 0.1962  data: 0.0002  max mem: 5511
[01:05:44.666728] Epoch: [94]  [400/781]  eta: 0:01:15  lr: 0.000003  training_loss: 1.0952 (1.1016)  mae_loss: 0.0259 (0.0255)  classification_loss: 1.0709 (1.0760)  loss_mask: 0.0000 (0.0001)  time: 0.1968  data: 0.0002  max mem: 5511
[01:05:48.594143] Epoch: [94]  [420/781]  eta: 0:01:11  lr: 0.000003  training_loss: 1.0788 (1.1007)  mae_loss: 0.0247 (0.0255)  classification_loss: 1.0540 (1.0751)  loss_mask: 0.0000 (0.0001)  time: 0.1963  data: 0.0002  max mem: 5511
[01:05:52.595201] Epoch: [94]  [440/781]  eta: 0:01:07  lr: 0.000003  training_loss: 1.0785 (1.1000)  mae_loss: 0.0245 (0.0255)  classification_loss: 1.0519 (1.0745)  loss_mask: 0.0000 (0.0001)  time: 0.2000  data: 0.0002  max mem: 5511
[01:05:56.506997] Epoch: [94]  [460/781]  eta: 0:01:03  lr: 0.000003  training_loss: 1.0706 (1.0996)  mae_loss: 0.0239 (0.0255)  classification_loss: 1.0465 (1.0741)  loss_mask: 0.0000 (0.0001)  time: 0.1955  data: 0.0003  max mem: 5511
[01:06:00.396270] Epoch: [94]  [480/781]  eta: 0:00:59  lr: 0.000003  training_loss: 1.0857 (1.0996)  mae_loss: 0.0249 (0.0255)  classification_loss: 1.0621 (1.0741)  loss_mask: 0.0000 (0.0001)  time: 0.1944  data: 0.0003  max mem: 5511
[01:06:04.282751] Epoch: [94]  [500/781]  eta: 0:00:55  lr: 0.000003  training_loss: 1.1030 (1.0999)  mae_loss: 0.0250 (0.0255)  classification_loss: 1.0828 (1.0743)  loss_mask: 0.0000 (0.0001)  time: 0.1943  data: 0.0003  max mem: 5511
[01:06:08.174854] Epoch: [94]  [520/781]  eta: 0:00:51  lr: 0.000003  training_loss: 1.1061 (1.1007)  mae_loss: 0.0262 (0.0255)  classification_loss: 1.0802 (1.0752)  loss_mask: 0.0000 (0.0001)  time: 0.1945  data: 0.0003  max mem: 5511
[01:06:12.087709] Epoch: [94]  [540/781]  eta: 0:00:47  lr: 0.000003  training_loss: 1.1088 (1.1007)  mae_loss: 0.0248 (0.0255)  classification_loss: 1.0848 (1.0752)  loss_mask: 0.0000 (0.0001)  time: 0.1956  data: 0.0002  max mem: 5511
[01:06:16.002025] Epoch: [94]  [560/781]  eta: 0:00:43  lr: 0.000003  training_loss: 1.1222 (1.1015)  mae_loss: 0.0242 (0.0255)  classification_loss: 1.0942 (1.0760)  loss_mask: 0.0000 (0.0001)  time: 0.1956  data: 0.0003  max mem: 5511
[01:06:19.921328] Epoch: [94]  [580/781]  eta: 0:00:39  lr: 0.000003  training_loss: 1.0976 (1.1024)  mae_loss: 0.0241 (0.0255)  classification_loss: 1.0738 (1.0769)  loss_mask: 0.0000 (0.0001)  time: 0.1959  data: 0.0003  max mem: 5511
[01:06:23.867023] Epoch: [94]  [600/781]  eta: 0:00:35  lr: 0.000003  training_loss: 1.0649 (1.1017)  mae_loss: 0.0245 (0.0255)  classification_loss: 1.0382 (1.0761)  loss_mask: 0.0000 (0.0001)  time: 0.1972  data: 0.0002  max mem: 5511
[01:06:27.780758] Epoch: [94]  [620/781]  eta: 0:00:31  lr: 0.000003  training_loss: 1.0916 (1.1016)  mae_loss: 0.0246 (0.0255)  classification_loss: 1.0636 (1.0761)  loss_mask: 0.0000 (0.0001)  time: 0.1956  data: 0.0002  max mem: 5511
[01:06:31.698805] Epoch: [94]  [640/781]  eta: 0:00:27  lr: 0.000003  training_loss: 1.1456 (1.1023)  mae_loss: 0.0256 (0.0255)  classification_loss: 1.1172 (1.0768)  loss_mask: 0.0000 (0.0001)  time: 0.1958  data: 0.0001  max mem: 5511
[01:06:35.613524] Epoch: [94]  [660/781]  eta: 0:00:23  lr: 0.000003  training_loss: 1.1177 (1.1031)  mae_loss: 0.0253 (0.0255)  classification_loss: 1.0913 (1.0775)  loss_mask: 0.0000 (0.0001)  time: 0.1956  data: 0.0002  max mem: 5511
[01:06:39.539003] Epoch: [94]  [680/781]  eta: 0:00:19  lr: 0.000003  training_loss: 1.0650 (1.1028)  mae_loss: 0.0258 (0.0255)  classification_loss: 1.0346 (1.0773)  loss_mask: 0.0000 (0.0001)  time: 0.1962  data: 0.0002  max mem: 5511
[01:06:43.499787] Epoch: [94]  [700/781]  eta: 0:00:15  lr: 0.000003  training_loss: 1.1448 (1.1037)  mae_loss: 0.0252 (0.0255)  classification_loss: 1.1159 (1.0782)  loss_mask: 0.0000 (0.0001)  time: 0.1980  data: 0.0002  max mem: 5511
[01:06:47.442801] Epoch: [94]  [720/781]  eta: 0:00:12  lr: 0.000003  training_loss: 1.0623 (1.1035)  mae_loss: 0.0246 (0.0254)  classification_loss: 1.0354 (1.0780)  loss_mask: 0.0000 (0.0001)  time: 0.1971  data: 0.0002  max mem: 5511
[01:06:51.364170] Epoch: [94]  [740/781]  eta: 0:00:08  lr: 0.000003  training_loss: 1.0479 (1.1025)  mae_loss: 0.0243 (0.0254)  classification_loss: 1.0235 (1.0770)  loss_mask: 0.0000 (0.0001)  time: 0.1960  data: 0.0002  max mem: 5511
[01:06:55.328167] Epoch: [94]  [760/781]  eta: 0:00:04  lr: 0.000003  training_loss: 1.1245 (1.1029)  mae_loss: 0.0270 (0.0255)  classification_loss: 1.0923 (1.0774)  loss_mask: 0.0000 (0.0001)  time: 0.1981  data: 0.0003  max mem: 5511
[01:06:59.228397] Epoch: [94]  [780/781]  eta: 0:00:00  lr: 0.000003  training_loss: 1.1202 (1.1034)  mae_loss: 0.0259 (0.0255)  classification_loss: 1.0989 (1.0778)  loss_mask: 0.0000 (0.0001)  time: 0.1949  data: 0.0002  max mem: 5511
[01:06:59.383371] Epoch: [94] Total time: 0:02:34 (0.1974 s / it)
[01:06:59.383803] Averaged stats: lr: 0.000003  training_loss: 1.1202 (1.1034)  mae_loss: 0.0259 (0.0255)  classification_loss: 1.0989 (1.0778)  loss_mask: 0.0000 (0.0001)
[01:07:00.009524] Test:  [  0/157]  eta: 0:01:37  testing_loss: 0.5482 (0.5482)  acc1: 82.8125 (82.8125)  acc5: 98.4375 (98.4375)  time: 0.6217  data: 0.5786  max mem: 5511
[01:07:00.293110] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4450 (0.4623)  acc1: 85.9375 (85.5114)  acc5: 100.0000 (99.4318)  time: 0.0821  data: 0.0528  max mem: 5511
[01:07:00.577988] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4242 (0.4186)  acc1: 87.5000 (87.7232)  acc5: 100.0000 (99.4792)  time: 0.0283  data: 0.0002  max mem: 5511
[01:07:00.866027] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4264 (0.4372)  acc1: 87.5000 (87.1976)  acc5: 100.0000 (99.3448)  time: 0.0285  data: 0.0002  max mem: 5511
[01:07:01.148149] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4432 (0.4426)  acc1: 87.5000 (87.1189)  acc5: 100.0000 (99.1997)  time: 0.0284  data: 0.0002  max mem: 5511
[01:07:01.431973] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4358 (0.4362)  acc1: 87.5000 (87.3775)  acc5: 100.0000 (99.2953)  time: 0.0282  data: 0.0002  max mem: 5511
[01:07:01.714831] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.3997 (0.4340)  acc1: 87.5000 (87.6281)  acc5: 100.0000 (99.3084)  time: 0.0282  data: 0.0002  max mem: 5511
[01:07:01.998095] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.3997 (0.4288)  acc1: 87.5000 (87.8961)  acc5: 100.0000 (99.2958)  time: 0.0282  data: 0.0002  max mem: 5511
[01:07:02.281389] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4204 (0.4354)  acc1: 87.5000 (87.5965)  acc5: 100.0000 (99.3056)  time: 0.0282  data: 0.0002  max mem: 5511
[01:07:02.570486] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4206 (0.4304)  acc1: 87.5000 (87.9293)  acc5: 100.0000 (99.2617)  time: 0.0285  data: 0.0002  max mem: 5511
[01:07:02.854567] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4232 (0.4323)  acc1: 89.0625 (87.9022)  acc5: 98.4375 (99.2729)  time: 0.0285  data: 0.0002  max mem: 5511
[01:07:03.145301] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4381 (0.4322)  acc1: 87.5000 (87.8660)  acc5: 100.0000 (99.2962)  time: 0.0286  data: 0.0002  max mem: 5511
[01:07:03.437746] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4733 (0.4327)  acc1: 85.9375 (87.7970)  acc5: 100.0000 (99.3156)  time: 0.0290  data: 0.0002  max mem: 5511
[01:07:03.725198] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4170 (0.4340)  acc1: 85.9375 (87.7028)  acc5: 100.0000 (99.3201)  time: 0.0289  data: 0.0003  max mem: 5511
[01:07:04.008391] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4103 (0.4331)  acc1: 85.9375 (87.6330)  acc5: 100.0000 (99.3462)  time: 0.0284  data: 0.0003  max mem: 5511
[01:07:04.287707] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4410 (0.4334)  acc1: 85.9375 (87.6552)  acc5: 100.0000 (99.3584)  time: 0.0280  data: 0.0002  max mem: 5511
[01:07:04.438981] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4103 (0.4322)  acc1: 87.5000 (87.6600)  acc5: 100.0000 (99.3600)  time: 0.0270  data: 0.0001  max mem: 5511
[01:07:04.608779] Test: Total time: 0:00:05 (0.0333 s / it)
[01:07:04.609246] * Acc@1 87.660 Acc@5 99.360 loss 0.432
[01:07:04.609548] Accuracy of the network on the 10000 test images: 87.7%
[01:07:04.609716] Max accuracy: 87.66%
[01:07:04.876111] log_dir: ./output_dir
[01:07:05.761454] Epoch: [95]  [  0/781]  eta: 0:11:30  lr: 0.000003  training_loss: 1.1101 (1.1101)  mae_loss: 0.0218 (0.0218)  classification_loss: 1.0883 (1.0883)  loss_mask: 0.0001 (0.0001)  time: 0.8836  data: 0.6590  max mem: 5511
[01:07:09.670278] Epoch: [95]  [ 20/781]  eta: 0:02:53  lr: 0.000003  training_loss: 1.0971 (1.0970)  mae_loss: 0.0241 (0.0245)  classification_loss: 1.0753 (1.0715)  loss_mask: 0.0000 (0.0010)  time: 0.1953  data: 0.0004  max mem: 5511
[01:07:13.605725] Epoch: [95]  [ 40/781]  eta: 0:02:37  lr: 0.000003  training_loss: 1.1370 (1.1169)  mae_loss: 0.0247 (0.0247)  classification_loss: 1.1174 (1.0916)  loss_mask: 0.0000 (0.0005)  time: 0.1967  data: 0.0002  max mem: 5511
[01:07:17.583638] Epoch: [95]  [ 60/781]  eta: 0:02:30  lr: 0.000003  training_loss: 1.1082 (1.1142)  mae_loss: 0.0261 (0.0252)  classification_loss: 1.0790 (1.0887)  loss_mask: 0.0000 (0.0004)  time: 0.1988  data: 0.0002  max mem: 5511
[01:07:21.482118] Epoch: [95]  [ 80/781]  eta: 0:02:23  lr: 0.000003  training_loss: 1.1009 (1.1114)  mae_loss: 0.0244 (0.0251)  classification_loss: 1.0732 (1.0860)  loss_mask: 0.0000 (0.0003)  time: 0.1948  data: 0.0002  max mem: 5511
[01:07:25.436496] Epoch: [95]  [100/781]  eta: 0:02:18  lr: 0.000003  training_loss: 1.1092 (1.1108)  mae_loss: 0.0268 (0.0256)  classification_loss: 1.0851 (1.0850)  loss_mask: 0.0000 (0.0002)  time: 0.1976  data: 0.0002  max mem: 5511
[01:07:29.372308] Epoch: [95]  [120/781]  eta: 0:02:13  lr: 0.000003  training_loss: 1.1064 (1.1105)  mae_loss: 0.0253 (0.0255)  classification_loss: 1.0740 (1.0847)  loss_mask: 0.0000 (0.0002)  time: 0.1967  data: 0.0002  max mem: 5511
[01:07:33.324435] Epoch: [95]  [140/781]  eta: 0:02:09  lr: 0.000003  training_loss: 1.1427 (1.1123)  mae_loss: 0.0251 (0.0256)  classification_loss: 1.1162 (1.0866)  loss_mask: 0.0000 (0.0002)  time: 0.1975  data: 0.0002  max mem: 5511
[01:07:37.318765] Epoch: [95]  [160/781]  eta: 0:02:05  lr: 0.000003  training_loss: 1.0832 (1.1124)  mae_loss: 0.0253 (0.0256)  classification_loss: 1.0578 (1.0866)  loss_mask: 0.0000 (0.0002)  time: 0.1996  data: 0.0003  max mem: 5511
[01:07:41.228080] Epoch: [95]  [180/781]  eta: 0:02:00  lr: 0.000003  training_loss: 1.1138 (1.1117)  mae_loss: 0.0260 (0.0256)  classification_loss: 1.0895 (1.0859)  loss_mask: 0.0000 (0.0002)  time: 0.1954  data: 0.0002  max mem: 5511
[01:07:45.239847] Epoch: [95]  [200/781]  eta: 0:01:56  lr: 0.000003  training_loss: 1.1210 (1.1128)  mae_loss: 0.0254 (0.0256)  classification_loss: 1.1005 (1.0870)  loss_mask: 0.0000 (0.0002)  time: 0.2005  data: 0.0003  max mem: 5511
[01:07:49.151275] Epoch: [95]  [220/781]  eta: 0:01:52  lr: 0.000003  training_loss: 1.0718 (1.1115)  mae_loss: 0.0242 (0.0254)  classification_loss: 1.0498 (1.0859)  loss_mask: 0.0000 (0.0002)  time: 0.1955  data: 0.0002  max mem: 5511
[01:07:53.106354] Epoch: [95]  [240/781]  eta: 0:01:48  lr: 0.000002  training_loss: 1.1061 (1.1132)  mae_loss: 0.0243 (0.0254)  classification_loss: 1.0885 (1.0876)  loss_mask: 0.0000 (0.0001)  time: 0.1977  data: 0.0002  max mem: 5511
[01:07:57.022297] Epoch: [95]  [260/781]  eta: 0:01:44  lr: 0.000002  training_loss: 1.0911 (1.1115)  mae_loss: 0.0243 (0.0254)  classification_loss: 1.0702 (1.0860)  loss_mask: 0.0000 (0.0001)  time: 0.1957  data: 0.0002  max mem: 5511
[01:08:00.987892] Epoch: [95]  [280/781]  eta: 0:01:39  lr: 0.000002  training_loss: 1.1237 (1.1120)  mae_loss: 0.0241 (0.0254)  classification_loss: 1.1016 (1.0864)  loss_mask: 0.0000 (0.0001)  time: 0.1982  data: 0.0002  max mem: 5511
[01:08:04.900374] Epoch: [95]  [300/781]  eta: 0:01:35  lr: 0.000002  training_loss: 1.1314 (1.1126)  mae_loss: 0.0253 (0.0254)  classification_loss: 1.1057 (1.0871)  loss_mask: 0.0000 (0.0001)  time: 0.1955  data: 0.0003  max mem: 5511
[01:08:08.826533] Epoch: [95]  [320/781]  eta: 0:01:31  lr: 0.000002  training_loss: 1.1249 (1.1142)  mae_loss: 0.0245 (0.0254)  classification_loss: 1.1027 (1.0887)  loss_mask: 0.0000 (0.0001)  time: 0.1962  data: 0.0002  max mem: 5511
[01:08:12.762894] Epoch: [95]  [340/781]  eta: 0:01:27  lr: 0.000002  training_loss: 1.0785 (1.1131)  mae_loss: 0.0260 (0.0254)  classification_loss: 1.0490 (1.0876)  loss_mask: 0.0000 (0.0001)  time: 0.1967  data: 0.0002  max mem: 5511
[01:08:16.718447] Epoch: [95]  [360/781]  eta: 0:01:23  lr: 0.000002  training_loss: 1.1414 (1.1148)  mae_loss: 0.0257 (0.0255)  classification_loss: 1.1159 (1.0892)  loss_mask: 0.0000 (0.0001)  time: 0.1977  data: 0.0002  max mem: 5511
[01:08:20.617875] Epoch: [95]  [380/781]  eta: 0:01:19  lr: 0.000002  training_loss: 1.1060 (1.1153)  mae_loss: 0.0244 (0.0254)  classification_loss: 1.0816 (1.0898)  loss_mask: 0.0000 (0.0001)  time: 0.1949  data: 0.0002  max mem: 5511
[01:08:24.557997] Epoch: [95]  [400/781]  eta: 0:01:15  lr: 0.000002  training_loss: 1.0958 (1.1138)  mae_loss: 0.0254 (0.0254)  classification_loss: 1.0662 (1.0883)  loss_mask: 0.0000 (0.0001)  time: 0.1969  data: 0.0003  max mem: 5511
[01:08:28.531752] Epoch: [95]  [420/781]  eta: 0:01:11  lr: 0.000002  training_loss: 1.0627 (1.1124)  mae_loss: 0.0270 (0.0255)  classification_loss: 1.0331 (1.0868)  loss_mask: 0.0000 (0.0001)  time: 0.1986  data: 0.0002  max mem: 5511
[01:08:32.452552] Epoch: [95]  [440/781]  eta: 0:01:07  lr: 0.000002  training_loss: 1.0814 (1.1109)  mae_loss: 0.0246 (0.0255)  classification_loss: 1.0516 (1.0853)  loss_mask: 0.0000 (0.0001)  time: 0.1960  data: 0.0002  max mem: 5511
[01:08:36.377775] Epoch: [95]  [460/781]  eta: 0:01:03  lr: 0.000002  training_loss: 1.1218 (1.1108)  mae_loss: 0.0238 (0.0255)  classification_loss: 1.0959 (1.0852)  loss_mask: 0.0000 (0.0001)  time: 0.1962  data: 0.0002  max mem: 5511
[01:08:40.301139] Epoch: [95]  [480/781]  eta: 0:00:59  lr: 0.000002  training_loss: 1.0990 (1.1102)  mae_loss: 0.0268 (0.0255)  classification_loss: 1.0713 (1.0846)  loss_mask: 0.0000 (0.0001)  time: 0.1961  data: 0.0003  max mem: 5511
[01:08:44.220578] Epoch: [95]  [500/781]  eta: 0:00:55  lr: 0.000002  training_loss: 1.0551 (1.1094)  mae_loss: 0.0254 (0.0255)  classification_loss: 1.0263 (1.0837)  loss_mask: 0.0000 (0.0001)  time: 0.1959  data: 0.0002  max mem: 5511
[01:08:48.152950] Epoch: [95]  [520/781]  eta: 0:00:51  lr: 0.000002  training_loss: 1.1309 (1.1106)  mae_loss: 0.0253 (0.0255)  classification_loss: 1.1051 (1.0850)  loss_mask: 0.0000 (0.0001)  time: 0.1965  data: 0.0002  max mem: 5511
[01:08:52.099502] Epoch: [95]  [540/781]  eta: 0:00:47  lr: 0.000002  training_loss: 1.1511 (1.1118)  mae_loss: 0.0255 (0.0255)  classification_loss: 1.1222 (1.0862)  loss_mask: 0.0000 (0.0001)  time: 0.1972  data: 0.0002  max mem: 5511
[01:08:56.016183] Epoch: [95]  [560/781]  eta: 0:00:43  lr: 0.000002  training_loss: 1.1243 (1.1127)  mae_loss: 0.0251 (0.0255)  classification_loss: 1.1008 (1.0871)  loss_mask: 0.0000 (0.0001)  time: 0.1957  data: 0.0002  max mem: 5511
[01:08:59.951429] Epoch: [95]  [580/781]  eta: 0:00:39  lr: 0.000002  training_loss: 1.0864 (1.1130)  mae_loss: 0.0251 (0.0255)  classification_loss: 1.0656 (1.0874)  loss_mask: 0.0000 (0.0001)  time: 0.1967  data: 0.0002  max mem: 5511
[01:09:03.871986] Epoch: [95]  [600/781]  eta: 0:00:35  lr: 0.000002  training_loss: 1.0726 (1.1121)  mae_loss: 0.0256 (0.0255)  classification_loss: 1.0516 (1.0865)  loss_mask: 0.0000 (0.0001)  time: 0.1959  data: 0.0004  max mem: 5511
[01:09:07.804212] Epoch: [95]  [620/781]  eta: 0:00:31  lr: 0.000002  training_loss: 1.0944 (1.1115)  mae_loss: 0.0246 (0.0255)  classification_loss: 1.0739 (1.0859)  loss_mask: 0.0000 (0.0001)  time: 0.1965  data: 0.0005  max mem: 5511
[01:09:11.753843] Epoch: [95]  [640/781]  eta: 0:00:27  lr: 0.000002  training_loss: 1.0960 (1.1114)  mae_loss: 0.0258 (0.0255)  classification_loss: 1.0722 (1.0858)  loss_mask: 0.0000 (0.0001)  time: 0.1974  data: 0.0003  max mem: 5511
[01:09:15.653085] Epoch: [95]  [660/781]  eta: 0:00:23  lr: 0.000002  training_loss: 1.1078 (1.1115)  mae_loss: 0.0248 (0.0256)  classification_loss: 1.0777 (1.0859)  loss_mask: 0.0000 (0.0001)  time: 0.1949  data: 0.0002  max mem: 5511
[01:09:19.565808] Epoch: [95]  [680/781]  eta: 0:00:19  lr: 0.000002  training_loss: 1.1266 (1.1120)  mae_loss: 0.0255 (0.0256)  classification_loss: 1.1001 (1.0864)  loss_mask: 0.0000 (0.0001)  time: 0.1955  data: 0.0002  max mem: 5511
[01:09:23.466252] Epoch: [95]  [700/781]  eta: 0:00:16  lr: 0.000002  training_loss: 1.1082 (1.1116)  mae_loss: 0.0252 (0.0256)  classification_loss: 1.0834 (1.0860)  loss_mask: 0.0000 (0.0001)  time: 0.1949  data: 0.0002  max mem: 5511
[01:09:27.391493] Epoch: [95]  [720/781]  eta: 0:00:12  lr: 0.000002  training_loss: 1.0733 (1.1108)  mae_loss: 0.0249 (0.0256)  classification_loss: 1.0468 (1.0852)  loss_mask: 0.0000 (0.0001)  time: 0.1962  data: 0.0002  max mem: 5511
[01:09:31.308358] Epoch: [95]  [740/781]  eta: 0:00:08  lr: 0.000002  training_loss: 1.0552 (1.1100)  mae_loss: 0.0247 (0.0255)  classification_loss: 1.0291 (1.0844)  loss_mask: 0.0000 (0.0001)  time: 0.1958  data: 0.0002  max mem: 5511
[01:09:35.220490] Epoch: [95]  [760/781]  eta: 0:00:04  lr: 0.000002  training_loss: 1.0635 (1.1100)  mae_loss: 0.0245 (0.0255)  classification_loss: 1.0405 (1.0844)  loss_mask: 0.0000 (0.0001)  time: 0.1955  data: 0.0002  max mem: 5511
[01:09:39.138508] Epoch: [95]  [780/781]  eta: 0:00:00  lr: 0.000002  training_loss: 1.1231 (1.1105)  mae_loss: 0.0255 (0.0255)  classification_loss: 1.0969 (1.0849)  loss_mask: 0.0000 (0.0001)  time: 0.1958  data: 0.0001  max mem: 5511
[01:09:39.299313] Epoch: [95] Total time: 0:02:34 (0.1977 s / it)
[01:09:39.300284] Averaged stats: lr: 0.000002  training_loss: 1.1231 (1.1105)  mae_loss: 0.0255 (0.0255)  classification_loss: 1.0969 (1.0849)  loss_mask: 0.0000 (0.0001)
[01:09:39.987679] Test:  [  0/157]  eta: 0:01:47  testing_loss: 0.5417 (0.5417)  acc1: 84.3750 (84.3750)  acc5: 100.0000 (100.0000)  time: 0.6826  data: 0.6496  max mem: 5511
[01:09:40.274750] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4342 (0.4612)  acc1: 85.9375 (85.9375)  acc5: 100.0000 (99.5739)  time: 0.0880  data: 0.0592  max mem: 5511
[01:09:40.565178] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4104 (0.4168)  acc1: 87.5000 (87.5744)  acc5: 100.0000 (99.6280)  time: 0.0287  data: 0.0002  max mem: 5511
[01:09:40.852665] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4172 (0.4345)  acc1: 89.0625 (87.1472)  acc5: 100.0000 (99.3952)  time: 0.0287  data: 0.0002  max mem: 5511
[01:09:41.136868] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4371 (0.4404)  acc1: 87.5000 (87.0808)  acc5: 100.0000 (99.2759)  time: 0.0284  data: 0.0002  max mem: 5511
[01:09:41.421018] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4335 (0.4338)  acc1: 87.5000 (87.3775)  acc5: 100.0000 (99.3873)  time: 0.0283  data: 0.0002  max mem: 5511
[01:09:41.706859] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4105 (0.4317)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (99.3852)  time: 0.0284  data: 0.0002  max mem: 5511
[01:09:41.994079] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4105 (0.4268)  acc1: 87.5000 (87.7861)  acc5: 100.0000 (99.3838)  time: 0.0285  data: 0.0002  max mem: 5511
[01:09:42.281218] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4227 (0.4336)  acc1: 87.5000 (87.4807)  acc5: 100.0000 (99.4020)  time: 0.0286  data: 0.0002  max mem: 5511
[01:09:42.570051] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4197 (0.4288)  acc1: 89.0625 (87.7919)  acc5: 100.0000 (99.3475)  time: 0.0287  data: 0.0002  max mem: 5511
[01:09:42.851762] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4194 (0.4307)  acc1: 89.0625 (87.7785)  acc5: 98.4375 (99.3348)  time: 0.0284  data: 0.0002  max mem: 5511
[01:09:43.142189] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4370 (0.4307)  acc1: 87.5000 (87.7815)  acc5: 100.0000 (99.3525)  time: 0.0285  data: 0.0002  max mem: 5511
[01:09:43.429184] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4711 (0.4312)  acc1: 85.9375 (87.7583)  acc5: 100.0000 (99.3673)  time: 0.0287  data: 0.0002  max mem: 5511
[01:09:43.712946] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4151 (0.4325)  acc1: 87.5000 (87.6908)  acc5: 100.0000 (99.3678)  time: 0.0284  data: 0.0002  max mem: 5511
[01:09:43.997017] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4146 (0.4316)  acc1: 87.5000 (87.6551)  acc5: 100.0000 (99.3905)  time: 0.0283  data: 0.0001  max mem: 5511
[01:09:44.278082] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4411 (0.4319)  acc1: 87.5000 (87.6966)  acc5: 100.0000 (99.3998)  time: 0.0281  data: 0.0001  max mem: 5511
[01:09:44.429323] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4146 (0.4307)  acc1: 89.0625 (87.7400)  acc5: 100.0000 (99.4100)  time: 0.0271  data: 0.0001  max mem: 5511
[01:09:44.607967] Test: Total time: 0:00:05 (0.0338 s / it)
[01:09:44.608791] * Acc@1 87.740 Acc@5 99.410 loss 0.431
[01:09:44.609107] Accuracy of the network on the 10000 test images: 87.7%
[01:09:44.609286] Max accuracy: 87.74%
[01:09:44.787177] log_dir: ./output_dir
[01:09:45.680810] Epoch: [96]  [  0/781]  eta: 0:11:36  lr: 0.000002  training_loss: 0.9518 (0.9518)  mae_loss: 0.0271 (0.0271)  classification_loss: 0.9247 (0.9247)  loss_mask: 0.0000 (0.0000)  time: 0.8917  data: 0.6741  max mem: 5511
[01:09:49.610328] Epoch: [96]  [ 20/781]  eta: 0:02:54  lr: 0.000002  training_loss: 1.0749 (1.0637)  mae_loss: 0.0245 (0.0252)  classification_loss: 1.0512 (1.0385)  loss_mask: 0.0000 (0.0001)  time: 0.1964  data: 0.0003  max mem: 5511
[01:09:53.536781] Epoch: [96]  [ 40/781]  eta: 0:02:38  lr: 0.000002  training_loss: 1.1165 (1.0889)  mae_loss: 0.0254 (0.0256)  classification_loss: 1.0911 (1.0633)  loss_mask: 0.0000 (0.0001)  time: 0.1962  data: 0.0002  max mem: 5511
[01:09:57.470122] Epoch: [96]  [ 60/781]  eta: 0:02:29  lr: 0.000002  training_loss: 1.1269 (1.1064)  mae_loss: 0.0242 (0.0254)  classification_loss: 1.0997 (1.0810)  loss_mask: 0.0000 (0.0000)  time: 0.1966  data: 0.0002  max mem: 5511
[01:10:01.418748] Epoch: [96]  [ 80/781]  eta: 0:02:23  lr: 0.000002  training_loss: 1.1007 (1.1065)  mae_loss: 0.0255 (0.0254)  classification_loss: 1.0706 (1.0810)  loss_mask: 0.0000 (0.0000)  time: 0.1973  data: 0.0002  max mem: 5511
[01:10:05.339041] Epoch: [96]  [100/781]  eta: 0:02:18  lr: 0.000002  training_loss: 1.0604 (1.0999)  mae_loss: 0.0262 (0.0255)  classification_loss: 1.0335 (1.0743)  loss_mask: 0.0000 (0.0000)  time: 0.1959  data: 0.0002  max mem: 5511
[01:10:09.247792] Epoch: [96]  [120/781]  eta: 0:02:13  lr: 0.000002  training_loss: 1.0985 (1.0971)  mae_loss: 0.0257 (0.0254)  classification_loss: 1.0711 (1.0717)  loss_mask: 0.0000 (0.0000)  time: 0.1954  data: 0.0002  max mem: 5511
[01:10:13.181449] Epoch: [96]  [140/781]  eta: 0:02:09  lr: 0.000002  training_loss: 1.0778 (1.0944)  mae_loss: 0.0241 (0.0254)  classification_loss: 1.0507 (1.0689)  loss_mask: 0.0000 (0.0001)  time: 0.1966  data: 0.0002  max mem: 5511
[01:10:17.105874] Epoch: [96]  [160/781]  eta: 0:02:04  lr: 0.000002  training_loss: 1.0982 (1.0975)  mae_loss: 0.0250 (0.0254)  classification_loss: 1.0732 (1.0720)  loss_mask: 0.0000 (0.0001)  time: 0.1961  data: 0.0003  max mem: 5511
[01:10:21.060066] Epoch: [96]  [180/781]  eta: 0:02:00  lr: 0.000002  training_loss: 1.0949 (1.0972)  mae_loss: 0.0247 (0.0254)  classification_loss: 1.0678 (1.0717)  loss_mask: 0.0000 (0.0001)  time: 0.1976  data: 0.0002  max mem: 5511
[01:10:24.984607] Epoch: [96]  [200/781]  eta: 0:01:56  lr: 0.000002  training_loss: 1.1064 (1.1014)  mae_loss: 0.0256 (0.0255)  classification_loss: 1.0814 (1.0758)  loss_mask: 0.0000 (0.0001)  time: 0.1961  data: 0.0002  max mem: 5511
[01:10:28.884690] Epoch: [96]  [220/781]  eta: 0:01:51  lr: 0.000002  training_loss: 1.1111 (1.1028)  mae_loss: 0.0244 (0.0255)  classification_loss: 1.0875 (1.0772)  loss_mask: 0.0000 (0.0001)  time: 0.1949  data: 0.0002  max mem: 5511
[01:10:32.799002] Epoch: [96]  [240/781]  eta: 0:01:47  lr: 0.000002  training_loss: 1.1477 (1.1057)  mae_loss: 0.0254 (0.0255)  classification_loss: 1.1173 (1.0801)  loss_mask: 0.0000 (0.0001)  time: 0.1956  data: 0.0002  max mem: 5511
[01:10:36.724565] Epoch: [96]  [260/781]  eta: 0:01:43  lr: 0.000002  training_loss: 1.0793 (1.1058)  mae_loss: 0.0249 (0.0256)  classification_loss: 1.0552 (1.0802)  loss_mask: 0.0000 (0.0000)  time: 0.1962  data: 0.0002  max mem: 5511
[01:10:40.673830] Epoch: [96]  [280/781]  eta: 0:01:39  lr: 0.000002  training_loss: 1.1331 (1.1069)  mae_loss: 0.0252 (0.0256)  classification_loss: 1.1042 (1.0812)  loss_mask: 0.0000 (0.0000)  time: 0.1974  data: 0.0002  max mem: 5511
[01:10:44.595686] Epoch: [96]  [300/781]  eta: 0:01:35  lr: 0.000002  training_loss: 1.1074 (1.1081)  mae_loss: 0.0242 (0.0255)  classification_loss: 1.0829 (1.0825)  loss_mask: 0.0000 (0.0000)  time: 0.1960  data: 0.0002  max mem: 5511
[01:10:48.528971] Epoch: [96]  [320/781]  eta: 0:01:31  lr: 0.000002  training_loss: 1.1015 (1.1092)  mae_loss: 0.0252 (0.0255)  classification_loss: 1.0735 (1.0836)  loss_mask: 0.0000 (0.0000)  time: 0.1966  data: 0.0002  max mem: 5511
[01:10:52.430402] Epoch: [96]  [340/781]  eta: 0:01:27  lr: 0.000002  training_loss: 1.0725 (1.1099)  mae_loss: 0.0238 (0.0255)  classification_loss: 1.0481 (1.0843)  loss_mask: 0.0000 (0.0000)  time: 0.1950  data: 0.0003  max mem: 5511
[01:10:56.367990] Epoch: [96]  [360/781]  eta: 0:01:23  lr: 0.000002  training_loss: 1.0956 (1.1091)  mae_loss: 0.0262 (0.0256)  classification_loss: 1.0724 (1.0835)  loss_mask: 0.0000 (0.0000)  time: 0.1968  data: 0.0002  max mem: 5511
[01:11:00.348286] Epoch: [96]  [380/781]  eta: 0:01:19  lr: 0.000002  training_loss: 1.0920 (1.1091)  mae_loss: 0.0264 (0.0256)  classification_loss: 1.0665 (1.0834)  loss_mask: 0.0000 (0.0000)  time: 0.1989  data: 0.0003  max mem: 5511
[01:11:04.292856] Epoch: [96]  [400/781]  eta: 0:01:15  lr: 0.000002  training_loss: 1.0871 (1.1092)  mae_loss: 0.0261 (0.0256)  classification_loss: 1.0609 (1.0835)  loss_mask: 0.0000 (0.0001)  time: 0.1972  data: 0.0002  max mem: 5511
[01:11:08.270751] Epoch: [96]  [420/781]  eta: 0:01:11  lr: 0.000002  training_loss: 1.0822 (1.1091)  mae_loss: 0.0240 (0.0256)  classification_loss: 1.0608 (1.0834)  loss_mask: 0.0000 (0.0001)  time: 0.1988  data: 0.0002  max mem: 5511
[01:11:12.195670] Epoch: [96]  [440/781]  eta: 0:01:07  lr: 0.000002  training_loss: 1.0835 (1.1085)  mae_loss: 0.0245 (0.0256)  classification_loss: 1.0590 (1.0828)  loss_mask: 0.0000 (0.0001)  time: 0.1962  data: 0.0002  max mem: 5511
[01:11:16.119697] Epoch: [96]  [460/781]  eta: 0:01:03  lr: 0.000002  training_loss: 1.0762 (1.1076)  mae_loss: 0.0249 (0.0256)  classification_loss: 1.0512 (1.0819)  loss_mask: 0.0000 (0.0001)  time: 0.1961  data: 0.0003  max mem: 5511
[01:11:20.032848] Epoch: [96]  [480/781]  eta: 0:00:59  lr: 0.000002  training_loss: 1.0901 (1.1073)  mae_loss: 0.0254 (0.0256)  classification_loss: 1.0653 (1.0817)  loss_mask: 0.0000 (0.0001)  time: 0.1956  data: 0.0002  max mem: 5511
[01:11:23.957137] Epoch: [96]  [500/781]  eta: 0:00:55  lr: 0.000002  training_loss: 1.0834 (1.1072)  mae_loss: 0.0253 (0.0256)  classification_loss: 1.0487 (1.0815)  loss_mask: 0.0000 (0.0001)  time: 0.1961  data: 0.0002  max mem: 5511
[01:11:27.910223] Epoch: [96]  [520/781]  eta: 0:00:51  lr: 0.000002  training_loss: 1.1435 (1.1080)  mae_loss: 0.0251 (0.0256)  classification_loss: 1.1181 (1.0824)  loss_mask: 0.0000 (0.0001)  time: 0.1975  data: 0.0002  max mem: 5511
[01:11:31.817120] Epoch: [96]  [540/781]  eta: 0:00:47  lr: 0.000002  training_loss: 1.1121 (1.1084)  mae_loss: 0.0248 (0.0256)  classification_loss: 1.0825 (1.0827)  loss_mask: 0.0000 (0.0001)  time: 0.1953  data: 0.0002  max mem: 5511
[01:11:35.731035] Epoch: [96]  [560/781]  eta: 0:00:43  lr: 0.000002  training_loss: 1.0641 (1.1074)  mae_loss: 0.0248 (0.0255)  classification_loss: 1.0381 (1.0818)  loss_mask: 0.0000 (0.0001)  time: 0.1956  data: 0.0005  max mem: 5511
[01:11:39.633602] Epoch: [96]  [580/781]  eta: 0:00:39  lr: 0.000002  training_loss: 1.0792 (1.1074)  mae_loss: 0.0235 (0.0255)  classification_loss: 1.0518 (1.0818)  loss_mask: 0.0000 (0.0001)  time: 0.1950  data: 0.0002  max mem: 5511
[01:11:43.566760] Epoch: [96]  [600/781]  eta: 0:00:35  lr: 0.000002  training_loss: 1.0658 (1.1067)  mae_loss: 0.0247 (0.0255)  classification_loss: 1.0397 (1.0811)  loss_mask: 0.0000 (0.0001)  time: 0.1966  data: 0.0002  max mem: 5511
[01:11:47.486037] Epoch: [96]  [620/781]  eta: 0:00:31  lr: 0.000002  training_loss: 1.0685 (1.1063)  mae_loss: 0.0253 (0.0255)  classification_loss: 1.0342 (1.0807)  loss_mask: 0.0000 (0.0001)  time: 0.1959  data: 0.0003  max mem: 5511
[01:11:51.397458] Epoch: [96]  [640/781]  eta: 0:00:27  lr: 0.000002  training_loss: 1.1234 (1.1072)  mae_loss: 0.0247 (0.0255)  classification_loss: 1.0985 (1.0816)  loss_mask: 0.0000 (0.0001)  time: 0.1955  data: 0.0002  max mem: 5511
[01:11:55.303078] Epoch: [96]  [660/781]  eta: 0:00:23  lr: 0.000002  training_loss: 1.1137 (1.1074)  mae_loss: 0.0243 (0.0255)  classification_loss: 1.0876 (1.0818)  loss_mask: 0.0000 (0.0001)  time: 0.1952  data: 0.0002  max mem: 5511
[01:11:59.216998] Epoch: [96]  [680/781]  eta: 0:00:19  lr: 0.000002  training_loss: 1.1390 (1.1086)  mae_loss: 0.0243 (0.0255)  classification_loss: 1.1176 (1.0830)  loss_mask: 0.0000 (0.0001)  time: 0.1956  data: 0.0002  max mem: 5511
[01:12:03.153393] Epoch: [96]  [700/781]  eta: 0:00:15  lr: 0.000002  training_loss: 1.0650 (1.1080)  mae_loss: 0.0248 (0.0255)  classification_loss: 1.0382 (1.0825)  loss_mask: 0.0000 (0.0001)  time: 0.1967  data: 0.0002  max mem: 5511
[01:12:07.079209] Epoch: [96]  [720/781]  eta: 0:00:12  lr: 0.000002  training_loss: 1.1334 (1.1089)  mae_loss: 0.0255 (0.0255)  classification_loss: 1.1091 (1.0834)  loss_mask: 0.0000 (0.0001)  time: 0.1962  data: 0.0002  max mem: 5511
[01:12:10.994002] Epoch: [96]  [740/781]  eta: 0:00:08  lr: 0.000002  training_loss: 1.0716 (1.1081)  mae_loss: 0.0252 (0.0255)  classification_loss: 1.0457 (1.0825)  loss_mask: 0.0000 (0.0001)  time: 0.1957  data: 0.0002  max mem: 5511
[01:12:14.917017] Epoch: [96]  [760/781]  eta: 0:00:04  lr: 0.000002  training_loss: 1.1468 (1.1091)  mae_loss: 0.0262 (0.0255)  classification_loss: 1.1205 (1.0835)  loss_mask: 0.0000 (0.0001)  time: 0.1960  data: 0.0002  max mem: 5511
[01:12:18.885964] Epoch: [96]  [780/781]  eta: 0:00:00  lr: 0.000002  training_loss: 1.1316 (1.1097)  mae_loss: 0.0256 (0.0255)  classification_loss: 1.1047 (1.0842)  loss_mask: 0.0000 (0.0001)  time: 0.1984  data: 0.0002  max mem: 5511
[01:12:19.030810] Epoch: [96] Total time: 0:02:34 (0.1975 s / it)
[01:12:19.031302] Averaged stats: lr: 0.000002  training_loss: 1.1316 (1.1097)  mae_loss: 0.0256 (0.0255)  classification_loss: 1.1047 (1.0842)  loss_mask: 0.0000 (0.0001)
[01:12:19.686270] Test:  [  0/157]  eta: 0:01:42  testing_loss: 0.5441 (0.5441)  acc1: 84.3750 (84.3750)  acc5: 100.0000 (100.0000)  time: 0.6509  data: 0.6205  max mem: 5511
[01:12:19.974407] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4342 (0.4628)  acc1: 85.9375 (86.2216)  acc5: 100.0000 (99.5739)  time: 0.0852  data: 0.0565  max mem: 5511
[01:12:20.266227] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4097 (0.4170)  acc1: 87.5000 (87.7976)  acc5: 100.0000 (99.4792)  time: 0.0288  data: 0.0002  max mem: 5511
[01:12:20.553357] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4203 (0.4354)  acc1: 89.0625 (87.3488)  acc5: 100.0000 (99.2944)  time: 0.0288  data: 0.0002  max mem: 5511
[01:12:20.836316] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4382 (0.4411)  acc1: 87.5000 (87.2332)  acc5: 98.4375 (99.1235)  time: 0.0284  data: 0.0002  max mem: 5511
[01:12:21.129889] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4337 (0.4343)  acc1: 87.5000 (87.5306)  acc5: 100.0000 (99.2647)  time: 0.0287  data: 0.0002  max mem: 5511
[01:12:21.420799] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4087 (0.4326)  acc1: 87.5000 (87.6025)  acc5: 100.0000 (99.2828)  time: 0.0291  data: 0.0002  max mem: 5511
[01:12:21.703858] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4087 (0.4279)  acc1: 87.5000 (87.8961)  acc5: 100.0000 (99.3178)  time: 0.0286  data: 0.0002  max mem: 5511
[01:12:21.987347] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4282 (0.4345)  acc1: 87.5000 (87.6543)  acc5: 100.0000 (99.3441)  time: 0.0282  data: 0.0002  max mem: 5511
[01:12:22.274429] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4228 (0.4298)  acc1: 87.5000 (87.9121)  acc5: 100.0000 (99.2960)  time: 0.0284  data: 0.0002  max mem: 5511
[01:12:22.561275] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4206 (0.4316)  acc1: 89.0625 (87.8868)  acc5: 98.4375 (99.2884)  time: 0.0286  data: 0.0002  max mem: 5511
[01:12:22.845380] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4351 (0.4319)  acc1: 89.0625 (87.8801)  acc5: 100.0000 (99.2962)  time: 0.0284  data: 0.0002  max mem: 5511
[01:12:23.129256] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4713 (0.4326)  acc1: 87.5000 (87.8487)  acc5: 100.0000 (99.3156)  time: 0.0283  data: 0.0002  max mem: 5511
[01:12:23.410452] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4320 (0.4337)  acc1: 87.5000 (87.7863)  acc5: 100.0000 (99.3082)  time: 0.0281  data: 0.0002  max mem: 5511
[01:12:23.691806] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4138 (0.4328)  acc1: 87.5000 (87.7438)  acc5: 100.0000 (99.3351)  time: 0.0280  data: 0.0001  max mem: 5511
[01:12:23.970418] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4406 (0.4332)  acc1: 87.5000 (87.7897)  acc5: 100.0000 (99.3377)  time: 0.0279  data: 0.0001  max mem: 5511
[01:12:24.120101] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4138 (0.4321)  acc1: 89.0625 (87.8200)  acc5: 100.0000 (99.3400)  time: 0.0269  data: 0.0001  max mem: 5511
[01:12:24.273540] Test: Total time: 0:00:05 (0.0334 s / it)
[01:12:24.274107] * Acc@1 87.820 Acc@5 99.340 loss 0.432
[01:12:24.274424] Accuracy of the network on the 10000 test images: 87.8%
[01:12:24.274601] Max accuracy: 87.82%
[01:12:24.435321] log_dir: ./output_dir
[01:12:25.311031] Epoch: [97]  [  0/781]  eta: 0:11:22  lr: 0.000002  training_loss: 0.8823 (0.8823)  mae_loss: 0.0228 (0.0228)  classification_loss: 0.8595 (0.8595)  loss_mask: 0.0000 (0.0000)  time: 0.8739  data: 0.6645  max mem: 5511
[01:12:29.257491] Epoch: [97]  [ 20/781]  eta: 0:02:54  lr: 0.000002  training_loss: 1.0642 (1.0696)  mae_loss: 0.0254 (0.0253)  classification_loss: 1.0380 (1.0434)  loss_mask: 0.0000 (0.0008)  time: 0.1972  data: 0.0004  max mem: 5511
[01:12:33.183540] Epoch: [97]  [ 40/781]  eta: 0:02:38  lr: 0.000002  training_loss: 1.1189 (1.0992)  mae_loss: 0.0242 (0.0250)  classification_loss: 1.0940 (1.0738)  loss_mask: 0.0000 (0.0004)  time: 0.1962  data: 0.0002  max mem: 5511
[01:12:37.090321] Epoch: [97]  [ 60/781]  eta: 0:02:29  lr: 0.000002  training_loss: 1.1198 (1.1049)  mae_loss: 0.0266 (0.0255)  classification_loss: 1.0932 (1.0790)  loss_mask: 0.0000 (0.0003)  time: 0.1953  data: 0.0002  max mem: 5511
[01:12:41.019417] Epoch: [97]  [ 80/781]  eta: 0:02:23  lr: 0.000002  training_loss: 1.0985 (1.1078)  mae_loss: 0.0248 (0.0254)  classification_loss: 1.0738 (1.0822)  loss_mask: 0.0000 (0.0002)  time: 0.1964  data: 0.0002  max mem: 5511
[01:12:44.946275] Epoch: [97]  [100/781]  eta: 0:02:18  lr: 0.000002  training_loss: 1.0714 (1.1019)  mae_loss: 0.0243 (0.0253)  classification_loss: 1.0475 (1.0764)  loss_mask: 0.0000 (0.0002)  time: 0.1963  data: 0.0002  max mem: 5511
[01:12:48.866309] Epoch: [97]  [120/781]  eta: 0:02:13  lr: 0.000002  training_loss: 1.0721 (1.0997)  mae_loss: 0.0243 (0.0253)  classification_loss: 1.0489 (1.0741)  loss_mask: 0.0000 (0.0002)  time: 0.1959  data: 0.0002  max mem: 5511
[01:12:52.784366] Epoch: [97]  [140/781]  eta: 0:02:08  lr: 0.000002  training_loss: 1.0653 (1.0963)  mae_loss: 0.0239 (0.0252)  classification_loss: 1.0430 (1.0710)  loss_mask: 0.0000 (0.0001)  time: 0.1958  data: 0.0003  max mem: 5511
[01:12:56.704058] Epoch: [97]  [160/781]  eta: 0:02:04  lr: 0.000002  training_loss: 1.0845 (1.0953)  mae_loss: 0.0260 (0.0253)  classification_loss: 1.0611 (1.0698)  loss_mask: 0.0000 (0.0001)  time: 0.1959  data: 0.0002  max mem: 5511
[01:13:00.617021] Epoch: [97]  [180/781]  eta: 0:02:00  lr: 0.000002  training_loss: 1.0949 (1.0961)  mae_loss: 0.0259 (0.0254)  classification_loss: 1.0711 (1.0706)  loss_mask: 0.0000 (0.0001)  time: 0.1956  data: 0.0002  max mem: 5511
[01:13:04.556170] Epoch: [97]  [200/781]  eta: 0:01:55  lr: 0.000002  training_loss: 1.0867 (1.0959)  mae_loss: 0.0243 (0.0253)  classification_loss: 1.0617 (1.0705)  loss_mask: 0.0000 (0.0001)  time: 0.1969  data: 0.0002  max mem: 5511
[01:13:08.478764] Epoch: [97]  [220/781]  eta: 0:01:51  lr: 0.000002  training_loss: 1.1297 (1.0980)  mae_loss: 0.0261 (0.0254)  classification_loss: 1.1016 (1.0725)  loss_mask: 0.0000 (0.0001)  time: 0.1961  data: 0.0002  max mem: 5511
[01:13:12.374177] Epoch: [97]  [240/781]  eta: 0:01:47  lr: 0.000001  training_loss: 1.0883 (1.1007)  mae_loss: 0.0254 (0.0254)  classification_loss: 1.0661 (1.0752)  loss_mask: 0.0000 (0.0001)  time: 0.1947  data: 0.0002  max mem: 5511
[01:13:16.307389] Epoch: [97]  [260/781]  eta: 0:01:43  lr: 0.000001  training_loss: 1.0759 (1.1006)  mae_loss: 0.0248 (0.0254)  classification_loss: 1.0530 (1.0751)  loss_mask: 0.0000 (0.0001)  time: 0.1966  data: 0.0002  max mem: 5511
[01:13:20.246149] Epoch: [97]  [280/781]  eta: 0:01:39  lr: 0.000001  training_loss: 1.0933 (1.1004)  mae_loss: 0.0251 (0.0254)  classification_loss: 1.0699 (1.0749)  loss_mask: 0.0000 (0.0001)  time: 0.1968  data: 0.0002  max mem: 5511
[01:13:24.170487] Epoch: [97]  [300/781]  eta: 0:01:35  lr: 0.000001  training_loss: 1.0759 (1.0991)  mae_loss: 0.0246 (0.0254)  classification_loss: 1.0503 (1.0736)  loss_mask: 0.0000 (0.0001)  time: 0.1961  data: 0.0002  max mem: 5511
[01:13:28.088293] Epoch: [97]  [320/781]  eta: 0:01:31  lr: 0.000001  training_loss: 1.1172 (1.1006)  mae_loss: 0.0270 (0.0254)  classification_loss: 1.0927 (1.0751)  loss_mask: 0.0000 (0.0001)  time: 0.1958  data: 0.0002  max mem: 5511
[01:13:32.020397] Epoch: [97]  [340/781]  eta: 0:01:27  lr: 0.000001  training_loss: 1.0580 (1.0984)  mae_loss: 0.0250 (0.0255)  classification_loss: 1.0343 (1.0729)  loss_mask: 0.0000 (0.0001)  time: 0.1965  data: 0.0002  max mem: 5511
[01:13:35.963851] Epoch: [97]  [360/781]  eta: 0:01:23  lr: 0.000001  training_loss: 1.0651 (1.0978)  mae_loss: 0.0238 (0.0254)  classification_loss: 1.0450 (1.0723)  loss_mask: 0.0000 (0.0001)  time: 0.1971  data: 0.0003  max mem: 5511
[01:13:39.884889] Epoch: [97]  [380/781]  eta: 0:01:19  lr: 0.000001  training_loss: 1.0726 (1.0979)  mae_loss: 0.0225 (0.0254)  classification_loss: 1.0450 (1.0724)  loss_mask: 0.0000 (0.0001)  time: 0.1960  data: 0.0002  max mem: 5511
[01:13:43.799346] Epoch: [97]  [400/781]  eta: 0:01:15  lr: 0.000001  training_loss: 1.0702 (1.0977)  mae_loss: 0.0243 (0.0253)  classification_loss: 1.0408 (1.0723)  loss_mask: 0.0000 (0.0001)  time: 0.1956  data: 0.0002  max mem: 5511
[01:13:47.701766] Epoch: [97]  [420/781]  eta: 0:01:11  lr: 0.000001  training_loss: 1.1081 (1.0994)  mae_loss: 0.0245 (0.0253)  classification_loss: 1.0826 (1.0740)  loss_mask: 0.0000 (0.0001)  time: 0.1950  data: 0.0002  max mem: 5511
[01:13:51.627428] Epoch: [97]  [440/781]  eta: 0:01:07  lr: 0.000001  training_loss: 1.0886 (1.0989)  mae_loss: 0.0253 (0.0254)  classification_loss: 1.0632 (1.0735)  loss_mask: 0.0000 (0.0001)  time: 0.1962  data: 0.0002  max mem: 5511
[01:13:55.537558] Epoch: [97]  [460/781]  eta: 0:01:03  lr: 0.000001  training_loss: 1.0724 (1.0983)  mae_loss: 0.0239 (0.0254)  classification_loss: 1.0486 (1.0728)  loss_mask: 0.0000 (0.0001)  time: 0.1954  data: 0.0002  max mem: 5511
[01:13:59.454027] Epoch: [97]  [480/781]  eta: 0:00:59  lr: 0.000001  training_loss: 1.0792 (1.0989)  mae_loss: 0.0248 (0.0254)  classification_loss: 1.0543 (1.0735)  loss_mask: 0.0000 (0.0001)  time: 0.1957  data: 0.0003  max mem: 5511
[01:14:03.370433] Epoch: [97]  [500/781]  eta: 0:00:55  lr: 0.000001  training_loss: 1.0801 (1.0992)  mae_loss: 0.0244 (0.0253)  classification_loss: 1.0578 (1.0738)  loss_mask: 0.0000 (0.0001)  time: 0.1957  data: 0.0002  max mem: 5511
[01:14:07.294543] Epoch: [97]  [520/781]  eta: 0:00:51  lr: 0.000001  training_loss: 1.0843 (1.0986)  mae_loss: 0.0265 (0.0254)  classification_loss: 1.0562 (1.0732)  loss_mask: 0.0000 (0.0001)  time: 0.1961  data: 0.0002  max mem: 5511
[01:14:11.219894] Epoch: [97]  [540/781]  eta: 0:00:47  lr: 0.000001  training_loss: 1.0831 (1.0989)  mae_loss: 0.0248 (0.0253)  classification_loss: 1.0579 (1.0735)  loss_mask: 0.0000 (0.0001)  time: 0.1962  data: 0.0002  max mem: 5511
[01:14:15.165358] Epoch: [97]  [560/781]  eta: 0:00:43  lr: 0.000001  training_loss: 1.1001 (1.0992)  mae_loss: 0.0247 (0.0253)  classification_loss: 1.0751 (1.0738)  loss_mask: 0.0000 (0.0001)  time: 0.1972  data: 0.0003  max mem: 5511
[01:14:19.087659] Epoch: [97]  [580/781]  eta: 0:00:39  lr: 0.000001  training_loss: 1.1061 (1.1003)  mae_loss: 0.0246 (0.0253)  classification_loss: 1.0779 (1.0749)  loss_mask: 0.0000 (0.0001)  time: 0.1960  data: 0.0002  max mem: 5511
[01:14:23.029338] Epoch: [97]  [600/781]  eta: 0:00:35  lr: 0.000001  training_loss: 1.0768 (1.1002)  mae_loss: 0.0262 (0.0253)  classification_loss: 1.0486 (1.0748)  loss_mask: 0.0000 (0.0001)  time: 0.1970  data: 0.0002  max mem: 5511
[01:14:26.961986] Epoch: [97]  [620/781]  eta: 0:00:31  lr: 0.000001  training_loss: 1.0863 (1.0997)  mae_loss: 0.0253 (0.0254)  classification_loss: 1.0651 (1.0743)  loss_mask: 0.0000 (0.0001)  time: 0.1965  data: 0.0003  max mem: 5511
[01:14:30.874410] Epoch: [97]  [640/781]  eta: 0:00:27  lr: 0.000001  training_loss: 1.0917 (1.0996)  mae_loss: 0.0258 (0.0254)  classification_loss: 1.0641 (1.0742)  loss_mask: 0.0000 (0.0001)  time: 0.1955  data: 0.0002  max mem: 5511

[01:14:34.781356] Epoch: [97]  [660/781]  eta: 0:00:23  lr: 0.000001  training_loss: 1.1017 (1.1001)  mae_loss: 0.0251 (0.0254)  classification_loss: 1.0782 (1.0747)  loss_mask: 0.0000 (0.0001)  time: 0.1953  data: 0.0002  max mem: 5511
[01:14:38.688341] Epoch: [97]  [680/781]  eta: 0:00:19  lr: 0.000001  training_loss: 1.0962 (1.1003)  mae_loss: 0.0260 (0.0254)  classification_loss: 1.0672 (1.0748)  loss_mask: 0.0000 (0.0001)  time: 0.1953  data: 0.0002  max mem: 5511
[01:14:42.589585] Epoch: [97]  [700/781]  eta: 0:00:15  lr: 0.000001  training_loss: 1.0878 (1.1000)  mae_loss: 0.0241 (0.0254)  classification_loss: 1.0641 (1.0745)  loss_mask: 0.0000 (0.0001)  time: 0.1950  data: 0.0002  max mem: 5511
[01:14:46.523033] Epoch: [97]  [720/781]  eta: 0:00:12  lr: 0.000001  training_loss: 1.1004 (1.1011)  mae_loss: 0.0245 (0.0254)  classification_loss: 1.0706 (1.0757)  loss_mask: 0.0000 (0.0001)  time: 0.1966  data: 0.0003  max mem: 5511
[01:14:50.478256] Epoch: [97]  [740/781]  eta: 0:00:08  lr: 0.000001  training_loss: 1.0926 (1.1021)  mae_loss: 0.0258 (0.0254)  classification_loss: 1.0694 (1.0766)  loss_mask: 0.0000 (0.0001)  time: 0.1977  data: 0.0003  max mem: 5511
[01:14:54.422556] Epoch: [97]  [760/781]  eta: 0:00:04  lr: 0.000001  training_loss: 1.0816 (1.1023)  mae_loss: 0.0260 (0.0254)  classification_loss: 1.0543 (1.0768)  loss_mask: 0.0000 (0.0001)  time: 0.1971  data: 0.0002  max mem: 5511
[01:14:58.327420] Epoch: [97]  [780/781]  eta: 0:00:00  lr: 0.000001  training_loss: 1.1435 (1.1033)  mae_loss: 0.0252 (0.0254)  classification_loss: 1.1130 (1.0778)  loss_mask: 0.0000 (0.0001)  time: 0.1951  data: 0.0002  max mem: 5511
[01:14:58.490734] Epoch: [97] Total time: 0:02:34 (0.1973 s / it)
[01:14:58.491220] Averaged stats: lr: 0.000001  training_loss: 1.1435 (1.1033)  mae_loss: 0.0252 (0.0254)  classification_loss: 1.1130 (1.0778)  loss_mask: 0.0000 (0.0001)
[01:14:59.120193] Test:  [  0/157]  eta: 0:01:38  testing_loss: 0.5526 (0.5526)  acc1: 84.3750 (84.3750)  acc5: 98.4375 (98.4375)  time: 0.6248  data: 0.5951  max mem: 5511
[01:14:59.420397] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4325 (0.4603)  acc1: 85.9375 (86.5057)  acc5: 100.0000 (99.4318)  time: 0.0839  data: 0.0543  max mem: 5511
[01:14:59.711046] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4011 (0.4157)  acc1: 87.5000 (88.2440)  acc5: 100.0000 (99.4792)  time: 0.0294  data: 0.0002  max mem: 5511
[01:14:59.997692] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4211 (0.4342)  acc1: 89.0625 (87.6008)  acc5: 100.0000 (99.3448)  time: 0.0287  data: 0.0002  max mem: 5511
[01:15:00.281930] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4408 (0.4408)  acc1: 87.5000 (87.4238)  acc5: 98.4375 (99.1616)  time: 0.0284  data: 0.0002  max mem: 5511
[01:15:00.570897] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4371 (0.4342)  acc1: 87.5000 (87.6532)  acc5: 100.0000 (99.2647)  time: 0.0285  data: 0.0002  max mem: 5511
[01:15:00.862681] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4093 (0.4321)  acc1: 87.5000 (87.6793)  acc5: 100.0000 (99.2828)  time: 0.0289  data: 0.0002  max mem: 5511
[01:15:01.147107] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4148 (0.4280)  acc1: 87.5000 (87.9401)  acc5: 100.0000 (99.2958)  time: 0.0287  data: 0.0002  max mem: 5511
[01:15:01.441732] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4271 (0.4345)  acc1: 87.5000 (87.6736)  acc5: 100.0000 (99.3248)  time: 0.0288  data: 0.0004  max mem: 5511
[01:15:01.723308] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4258 (0.4299)  acc1: 87.5000 (87.9464)  acc5: 100.0000 (99.2960)  time: 0.0287  data: 0.0004  max mem: 5511
[01:15:02.009600] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4258 (0.4318)  acc1: 89.0625 (87.9177)  acc5: 98.4375 (99.2884)  time: 0.0283  data: 0.0002  max mem: 5511
[01:15:02.292499] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4341 (0.4317)  acc1: 87.5000 (87.8801)  acc5: 100.0000 (99.3102)  time: 0.0283  data: 0.0002  max mem: 5511
[01:15:02.576291] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4751 (0.4325)  acc1: 87.5000 (87.8099)  acc5: 100.0000 (99.3285)  time: 0.0282  data: 0.0002  max mem: 5511
[01:15:02.858370] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4328 (0.4337)  acc1: 87.5000 (87.7385)  acc5: 100.0000 (99.3201)  time: 0.0282  data: 0.0002  max mem: 5511
[01:15:03.142204] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4160 (0.4328)  acc1: 87.5000 (87.6884)  acc5: 100.0000 (99.3462)  time: 0.0282  data: 0.0002  max mem: 5511
[01:15:03.423879] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4420 (0.4333)  acc1: 87.5000 (87.7380)  acc5: 100.0000 (99.3481)  time: 0.0281  data: 0.0002  max mem: 5511
[01:15:03.575565] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4242 (0.4322)  acc1: 89.0625 (87.7900)  acc5: 100.0000 (99.3500)  time: 0.0272  data: 0.0001  max mem: 5511
[01:15:03.711321] Test: Total time: 0:00:05 (0.0332 s / it)
[01:15:03.711837] * Acc@1 87.790 Acc@5 99.350 loss 0.432
[01:15:03.712119] Accuracy of the network on the 10000 test images: 87.8%
[01:15:03.712293] Max accuracy: 87.82%
[01:15:04.066149] log_dir: ./output_dir
[01:15:04.946357] Epoch: [98]  [  0/781]  eta: 0:11:26  lr: 0.000001  training_loss: 1.0996 (1.0996)  mae_loss: 0.0217 (0.0217)  classification_loss: 1.0779 (1.0779)  loss_mask: 0.0000 (0.0000)  time: 0.8785  data: 0.6583  max mem: 5511
[01:15:08.885613] Epoch: [98]  [ 20/781]  eta: 0:02:54  lr: 0.000001  training_loss: 1.0657 (1.0660)  mae_loss: 0.0252 (0.0256)  classification_loss: 1.0440 (1.0403)  loss_mask: 0.0000 (0.0000)  time: 0.1969  data: 0.0002  max mem: 5511
[01:15:12.804016] Epoch: [98]  [ 40/781]  eta: 0:02:37  lr: 0.000001  training_loss: 1.1024 (1.0713)  mae_loss: 0.0255 (0.0254)  classification_loss: 1.0795 (1.0459)  loss_mask: 0.0000 (0.0000)  time: 0.1958  data: 0.0002  max mem: 5511
[01:15:16.788501] Epoch: [98]  [ 60/781]  eta: 0:02:30  lr: 0.000001  training_loss: 1.0338 (1.0771)  mae_loss: 0.0272 (0.0259)  classification_loss: 1.0048 (1.0512)  loss_mask: 0.0000 (0.0000)  time: 0.1991  data: 0.0004  max mem: 5511
[01:15:20.714676] Epoch: [98]  [ 80/781]  eta: 0:02:24  lr: 0.000001  training_loss: 1.1071 (1.0828)  mae_loss: 0.0244 (0.0255)  classification_loss: 1.0765 (1.0573)  loss_mask: 0.0000 (0.0000)  time: 0.1962  data: 0.0003  max mem: 5511
[01:15:24.625913] Epoch: [98]  [100/781]  eta: 0:02:18  lr: 0.000001  training_loss: 1.1236 (1.0936)  mae_loss: 0.0246 (0.0254)  classification_loss: 1.0980 (1.0681)  loss_mask: 0.0000 (0.0000)  time: 0.1955  data: 0.0002  max mem: 5511
[01:15:28.536006] Epoch: [98]  [120/781]  eta: 0:02:13  lr: 0.000001  training_loss: 1.0757 (1.0941)  mae_loss: 0.0244 (0.0253)  classification_loss: 1.0492 (1.0688)  loss_mask: 0.0000 (0.0000)  time: 0.1954  data: 0.0002  max mem: 5511
[01:15:32.470127] Epoch: [98]  [140/781]  eta: 0:02:09  lr: 0.000001  training_loss: 1.0994 (1.0979)  mae_loss: 0.0246 (0.0252)  classification_loss: 1.0782 (1.0726)  loss_mask: 0.0000 (0.0000)  time: 0.1966  data: 0.0002  max mem: 5511
[01:15:36.372582] Epoch: [98]  [160/781]  eta: 0:02:04  lr: 0.000001  training_loss: 1.0925 (1.0990)  mae_loss: 0.0251 (0.0252)  classification_loss: 1.0614 (1.0737)  loss_mask: 0.0000 (0.0000)  time: 0.1950  data: 0.0002  max mem: 5511
[01:15:40.317443] Epoch: [98]  [180/781]  eta: 0:02:00  lr: 0.000001  training_loss: 1.1105 (1.0993)  mae_loss: 0.0250 (0.0253)  classification_loss: 1.0841 (1.0739)  loss_mask: 0.0000 (0.0000)  time: 0.1972  data: 0.0002  max mem: 5511
[01:15:44.229356] Epoch: [98]  [200/781]  eta: 0:01:56  lr: 0.000001  training_loss: 1.0538 (1.0983)  mae_loss: 0.0243 (0.0253)  classification_loss: 1.0302 (1.0730)  loss_mask: 0.0000 (0.0000)  time: 0.1955  data: 0.0002  max mem: 5511
[01:15:48.116861] Epoch: [98]  [220/781]  eta: 0:01:51  lr: 0.000001  training_loss: 1.0944 (1.0991)  mae_loss: 0.0230 (0.0252)  classification_loss: 1.0697 (1.0739)  loss_mask: 0.0000 (0.0000)  time: 0.1943  data: 0.0002  max mem: 5511
[01:15:52.002666] Epoch: [98]  [240/781]  eta: 0:01:47  lr: 0.000001  training_loss: 1.1506 (1.1020)  mae_loss: 0.0246 (0.0252)  classification_loss: 1.1255 (1.0768)  loss_mask: 0.0000 (0.0000)  time: 0.1942  data: 0.0002  max mem: 5511
[01:15:55.962766] Epoch: [98]  [260/781]  eta: 0:01:43  lr: 0.000001  training_loss: 1.1276 (1.1026)  mae_loss: 0.0244 (0.0252)  classification_loss: 1.0992 (1.0774)  loss_mask: 0.0000 (0.0000)  time: 0.1979  data: 0.0003  max mem: 5511
[01:15:59.893633] Epoch: [98]  [280/781]  eta: 0:01:39  lr: 0.000001  training_loss: 1.0820 (1.1021)  mae_loss: 0.0245 (0.0252)  classification_loss: 1.0567 (1.0768)  loss_mask: 0.0000 (0.0000)  time: 0.1965  data: 0.0002  max mem: 5511
[01:16:03.837662] Epoch: [98]  [300/781]  eta: 0:01:35  lr: 0.000001  training_loss: 1.0548 (1.1017)  mae_loss: 0.0242 (0.0252)  classification_loss: 1.0269 (1.0765)  loss_mask: 0.0000 (0.0000)  time: 0.1971  data: 0.0002  max mem: 5511
[01:16:07.767826] Epoch: [98]  [320/781]  eta: 0:01:31  lr: 0.000001  training_loss: 1.1054 (1.1016)  mae_loss: 0.0254 (0.0252)  classification_loss: 1.0791 (1.0764)  loss_mask: 0.0000 (0.0000)  time: 0.1964  data: 0.0002  max mem: 5511
[01:16:11.693637] Epoch: [98]  [340/781]  eta: 0:01:27  lr: 0.000001  training_loss: 1.1138 (1.1019)  mae_loss: 0.0261 (0.0253)  classification_loss: 1.0876 (1.0766)  loss_mask: 0.0000 (0.0000)  time: 0.1962  data: 0.0002  max mem: 5511
[01:16:15.616461] Epoch: [98]  [360/781]  eta: 0:01:23  lr: 0.000001  training_loss: 1.1037 (1.1023)  mae_loss: 0.0246 (0.0253)  classification_loss: 1.0746 (1.0770)  loss_mask: 0.0000 (0.0000)  time: 0.1961  data: 0.0002  max mem: 5511
[01:16:19.551910] Epoch: [98]  [380/781]  eta: 0:01:19  lr: 0.000001  training_loss: 1.0868 (1.1028)  mae_loss: 0.0249 (0.0253)  classification_loss: 1.0600 (1.0774)  loss_mask: 0.0000 (0.0000)  time: 0.1967  data: 0.0002  max mem: 5511
[01:16:23.485498] Epoch: [98]  [400/781]  eta: 0:01:15  lr: 0.000001  training_loss: 1.1434 (1.1059)  mae_loss: 0.0249 (0.0253)  classification_loss: 1.1171 (1.0806)  loss_mask: 0.0000 (0.0000)  time: 0.1966  data: 0.0002  max mem: 5511
[01:16:27.403351] Epoch: [98]  [420/781]  eta: 0:01:11  lr: 0.000001  training_loss: 1.0786 (1.1053)  mae_loss: 0.0265 (0.0253)  classification_loss: 1.0517 (1.0799)  loss_mask: 0.0000 (0.0000)  time: 0.1958  data: 0.0002  max mem: 5511
[01:16:31.313456] Epoch: [98]  [440/781]  eta: 0:01:07  lr: 0.000001  training_loss: 1.1017 (1.1052)  mae_loss: 0.0268 (0.0254)  classification_loss: 1.0696 (1.0797)  loss_mask: 0.0000 (0.0000)  time: 0.1954  data: 0.0002  max mem: 5511
[01:16:35.232855] Epoch: [98]  [460/781]  eta: 0:01:03  lr: 0.000001  training_loss: 1.1002 (1.1047)  mae_loss: 0.0256 (0.0254)  classification_loss: 1.0742 (1.0792)  loss_mask: 0.0000 (0.0001)  time: 0.1959  data: 0.0002  max mem: 5511
[01:16:39.176274] Epoch: [98]  [480/781]  eta: 0:00:59  lr: 0.000001  training_loss: 1.0763 (1.1049)  mae_loss: 0.0247 (0.0254)  classification_loss: 1.0514 (1.0794)  loss_mask: 0.0000 (0.0001)  time: 0.1971  data: 0.0003  max mem: 5511
[01:16:43.115765] Epoch: [98]  [500/781]  eta: 0:00:55  lr: 0.000001  training_loss: 1.0528 (1.1036)  mae_loss: 0.0246 (0.0254)  classification_loss: 1.0209 (1.0782)  loss_mask: 0.0000 (0.0001)  time: 0.1969  data: 0.0002  max mem: 5511
[01:16:47.071960] Epoch: [98]  [520/781]  eta: 0:00:51  lr: 0.000001  training_loss: 1.0272 (1.1024)  mae_loss: 0.0248 (0.0253)  classification_loss: 1.0026 (1.0769)  loss_mask: 0.0000 (0.0001)  time: 0.1977  data: 0.0002  max mem: 5511
[01:16:51.009722] Epoch: [98]  [540/781]  eta: 0:00:47  lr: 0.000001  training_loss: 1.1352 (1.1036)  mae_loss: 0.0236 (0.0253)  classification_loss: 1.1088 (1.0783)  loss_mask: 0.0000 (0.0001)  time: 0.1968  data: 0.0002  max mem: 5511
[01:16:54.932068] Epoch: [98]  [560/781]  eta: 0:00:43  lr: 0.000001  training_loss: 1.1138 (1.1038)  mae_loss: 0.0256 (0.0253)  classification_loss: 1.0866 (1.0784)  loss_mask: 0.0000 (0.0001)  time: 0.1960  data: 0.0002  max mem: 5511
[01:16:58.848175] Epoch: [98]  [580/781]  eta: 0:00:39  lr: 0.000001  training_loss: 1.1149 (1.1045)  mae_loss: 0.0253 (0.0253)  classification_loss: 1.0893 (1.0791)  loss_mask: 0.0000 (0.0001)  time: 0.1957  data: 0.0002  max mem: 5511
[01:17:02.822511] Epoch: [98]  [600/781]  eta: 0:00:35  lr: 0.000001  training_loss: 1.0781 (1.1045)  mae_loss: 0.0255 (0.0253)  classification_loss: 1.0500 (1.0791)  loss_mask: 0.0000 (0.0001)  time: 0.1986  data: 0.0002  max mem: 5511
[01:17:06.731166] Epoch: [98]  [620/781]  eta: 0:00:31  lr: 0.000001  training_loss: 1.0982 (1.1047)  mae_loss: 0.0241 (0.0253)  classification_loss: 1.0733 (1.0793)  loss_mask: 0.0000 (0.0001)  time: 0.1954  data: 0.0002  max mem: 5511
[01:17:10.653811] Epoch: [98]  [640/781]  eta: 0:00:27  lr: 0.000001  training_loss: 1.1000 (1.1054)  mae_loss: 0.0242 (0.0253)  classification_loss: 1.0716 (1.0800)  loss_mask: 0.0000 (0.0001)  time: 0.1960  data: 0.0002  max mem: 5511
[01:17:14.598319] Epoch: [98]  [660/781]  eta: 0:00:23  lr: 0.000001  training_loss: 1.1126 (1.1055)  mae_loss: 0.0263 (0.0253)  classification_loss: 1.0852 (1.0801)  loss_mask: 0.0000 (0.0001)  time: 0.1971  data: 0.0002  max mem: 5511
[01:17:18.546644] Epoch: [98]  [680/781]  eta: 0:00:19  lr: 0.000001  training_loss: 1.0803 (1.1059)  mae_loss: 0.0274 (0.0254)  classification_loss: 1.0506 (1.0804)  loss_mask: 0.0000 (0.0001)  time: 0.1973  data: 0.0002  max mem: 5511
[01:17:22.464955] Epoch: [98]  [700/781]  eta: 0:00:15  lr: 0.000001  training_loss: 1.0909 (1.1056)  mae_loss: 0.0243 (0.0254)  classification_loss: 1.0641 (1.0801)  loss_mask: 0.0000 (0.0001)  time: 0.1958  data: 0.0002  max mem: 5511
[01:17:26.375283] Epoch: [98]  [720/781]  eta: 0:00:12  lr: 0.000001  training_loss: 1.1138 (1.1060)  mae_loss: 0.0248 (0.0254)  classification_loss: 1.0872 (1.0806)  loss_mask: 0.0000 (0.0001)  time: 0.1954  data: 0.0002  max mem: 5511
[01:17:30.310249] Epoch: [98]  [740/781]  eta: 0:00:08  lr: 0.000001  training_loss: 1.0199 (1.1047)  mae_loss: 0.0271 (0.0254)  classification_loss: 0.9912 (1.0792)  loss_mask: 0.0000 (0.0001)  time: 0.1967  data: 0.0002  max mem: 5511
[01:17:34.236000] Epoch: [98]  [760/781]  eta: 0:00:04  lr: 0.000001  training_loss: 1.0835 (1.1058)  mae_loss: 0.0251 (0.0254)  classification_loss: 1.0595 (1.0804)  loss_mask: 0.0000 (0.0001)  time: 0.1962  data: 0.0003  max mem: 5511
[01:17:38.214505] Epoch: [98]  [780/781]  eta: 0:00:00  lr: 0.000001  training_loss: 1.1084 (1.1060)  mae_loss: 0.0266 (0.0254)  classification_loss: 1.0850 (1.0805)  loss_mask: 0.0000 (0.0001)  time: 0.1989  data: 0.0002  max mem: 5511
[01:17:38.359261] Epoch: [98] Total time: 0:02:34 (0.1976 s / it)
[01:17:38.359714] Averaged stats: lr: 0.000001  training_loss: 1.1084 (1.1060)  mae_loss: 0.0266 (0.0254)  classification_loss: 1.0850 (1.0805)  loss_mask: 0.0000 (0.0001)
[01:17:38.965847] Test:  [  0/157]  eta: 0:01:34  testing_loss: 0.5519 (0.5519)  acc1: 84.3750 (84.3750)  acc5: 98.4375 (98.4375)  time: 0.6022  data: 0.5728  max mem: 5511
[01:17:39.277232] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4300 (0.4601)  acc1: 84.3750 (85.7955)  acc5: 100.0000 (99.4318)  time: 0.0829  data: 0.0544  max mem: 5511
[01:17:39.561429] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4067 (0.4152)  acc1: 87.5000 (87.9464)  acc5: 100.0000 (99.5536)  time: 0.0296  data: 0.0014  max mem: 5511
[01:17:39.849291] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4234 (0.4340)  acc1: 89.0625 (87.2984)  acc5: 100.0000 (99.4456)  time: 0.0285  data: 0.0002  max mem: 5511
[01:17:40.140768] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4435 (0.4403)  acc1: 87.5000 (87.1951)  acc5: 98.4375 (99.2759)  time: 0.0288  data: 0.0002  max mem: 5511
[01:17:40.427248] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4389 (0.4340)  acc1: 87.5000 (87.4694)  acc5: 100.0000 (99.3566)  time: 0.0288  data: 0.0002  max mem: 5511
[01:17:40.709637] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4096 (0.4319)  acc1: 87.5000 (87.6025)  acc5: 100.0000 (99.3596)  time: 0.0283  data: 0.0002  max mem: 5511
[01:17:40.993949] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4150 (0.4279)  acc1: 89.0625 (87.8521)  acc5: 100.0000 (99.3398)  time: 0.0282  data: 0.0002  max mem: 5511
[01:17:41.275194] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4233 (0.4345)  acc1: 87.5000 (87.5579)  acc5: 100.0000 (99.3634)  time: 0.0281  data: 0.0002  max mem: 5511
[01:17:41.565130] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4267 (0.4300)  acc1: 89.0625 (87.8606)  acc5: 100.0000 (99.3132)  time: 0.0284  data: 0.0002  max mem: 5511
[01:17:41.849233] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4267 (0.4320)  acc1: 89.0625 (87.8403)  acc5: 98.4375 (99.3038)  time: 0.0286  data: 0.0002  max mem: 5511
[01:17:42.133348] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4361 (0.4319)  acc1: 87.5000 (87.8097)  acc5: 100.0000 (99.3243)  time: 0.0283  data: 0.0002  max mem: 5511
[01:17:42.428547] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4709 (0.4325)  acc1: 87.5000 (87.7454)  acc5: 100.0000 (99.3414)  time: 0.0288  data: 0.0002  max mem: 5511
[01:17:42.711109] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4294 (0.4337)  acc1: 87.5000 (87.6431)  acc5: 100.0000 (99.3321)  time: 0.0288  data: 0.0002  max mem: 5511
[01:17:42.993262] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4131 (0.4327)  acc1: 87.5000 (87.6108)  acc5: 100.0000 (99.3573)  time: 0.0281  data: 0.0002  max mem: 5511
[01:17:43.273086] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4451 (0.4332)  acc1: 87.5000 (87.6552)  acc5: 100.0000 (99.3584)  time: 0.0280  data: 0.0001  max mem: 5511
[01:17:43.423598] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4220 (0.4319)  acc1: 89.0625 (87.7000)  acc5: 100.0000 (99.3600)  time: 0.0270  data: 0.0001  max mem: 5511
[01:17:43.586951] Test: Total time: 0:00:05 (0.0333 s / it)
[01:17:43.587401] * Acc@1 87.700 Acc@5 99.360 loss 0.432
[01:17:43.587687] Accuracy of the network on the 10000 test images: 87.7%
[01:17:43.587866] Max accuracy: 87.82%
[01:17:43.902945] log_dir: ./output_dir
[01:17:44.780776] Epoch: [99]  [  0/781]  eta: 0:11:23  lr: 0.000001  training_loss: 0.8863 (0.8863)  mae_loss: 0.0208 (0.0208)  classification_loss: 0.8654 (0.8654)  loss_mask: 0.0000 (0.0000)  time: 0.8756  data: 0.6540  max mem: 5511
[01:17:48.682707] Epoch: [99]  [ 20/781]  eta: 0:02:53  lr: 0.000001  training_loss: 1.1140 (1.0941)  mae_loss: 0.0252 (0.0257)  classification_loss: 1.0847 (1.0684)  loss_mask: 0.0000 (0.0000)  time: 0.1950  data: 0.0002  max mem: 5511
[01:17:52.599798] Epoch: [99]  [ 40/781]  eta: 0:02:37  lr: 0.000001  training_loss: 1.0858 (1.0940)  mae_loss: 0.0254 (0.0253)  classification_loss: 1.0619 (1.0687)  loss_mask: 0.0000 (0.0000)  time: 0.1958  data: 0.0002  max mem: 5511
[01:17:56.550589] Epoch: [99]  [ 60/781]  eta: 0:02:29  lr: 0.000001  training_loss: 1.0923 (1.1032)  mae_loss: 0.0240 (0.0250)  classification_loss: 1.0658 (1.0782)  loss_mask: 0.0000 (0.0000)  time: 0.1975  data: 0.0003  max mem: 5511
[01:18:00.494731] Epoch: [99]  [ 80/781]  eta: 0:02:23  lr: 0.000001  training_loss: 1.0497 (1.0980)  mae_loss: 0.0239 (0.0250)  classification_loss: 1.0269 (1.0730)  loss_mask: 0.0000 (0.0000)  time: 0.1971  data: 0.0002  max mem: 5511
[01:18:04.444945] Epoch: [99]  [100/781]  eta: 0:02:18  lr: 0.000001  training_loss: 1.1067 (1.1024)  mae_loss: 0.0259 (0.0251)  classification_loss: 1.0807 (1.0773)  loss_mask: 0.0000 (0.0000)  time: 0.1974  data: 0.0002  max mem: 5511
[01:18:08.401927] Epoch: [99]  [120/781]  eta: 0:02:13  lr: 0.000001  training_loss: 1.1401 (1.1085)  mae_loss: 0.0251 (0.0251)  classification_loss: 1.1134 (1.0833)  loss_mask: 0.0000 (0.0000)  time: 0.1978  data: 0.0002  max mem: 5511
[01:18:12.343391] Epoch: [99]  [140/781]  eta: 0:02:09  lr: 0.000001  training_loss: 1.0769 (1.1065)  mae_loss: 0.0245 (0.0250)  classification_loss: 1.0513 (1.0815)  loss_mask: 0.0000 (0.0000)  time: 0.1969  data: 0.0002  max mem: 5511
[01:18:16.282226] Epoch: [99]  [160/781]  eta: 0:02:04  lr: 0.000001  training_loss: 1.0541 (1.1030)  mae_loss: 0.0236 (0.0250)  classification_loss: 1.0322 (1.0780)  loss_mask: 0.0000 (0.0000)  time: 0.1969  data: 0.0002  max mem: 5511
[01:18:20.206701] Epoch: [99]  [180/781]  eta: 0:02:00  lr: 0.000001  training_loss: 1.0843 (1.1022)  mae_loss: 0.0269 (0.0251)  classification_loss: 1.0648 (1.0770)  loss_mask: 0.0000 (0.0000)  time: 0.1962  data: 0.0002  max mem: 5511
[01:18:24.105612] Epoch: [99]  [200/781]  eta: 0:01:56  lr: 0.000001  training_loss: 1.1371 (1.1032)  mae_loss: 0.0256 (0.0252)  classification_loss: 1.1059 (1.0779)  loss_mask: 0.0000 (0.0001)  time: 0.1949  data: 0.0002  max mem: 5511
[01:18:28.032465] Epoch: [99]  [220/781]  eta: 0:01:51  lr: 0.000001  training_loss: 1.0903 (1.1036)  mae_loss: 0.0266 (0.0254)  classification_loss: 1.0625 (1.0782)  loss_mask: 0.0000 (0.0001)  time: 0.1962  data: 0.0002  max mem: 5511
[01:18:31.943759] Epoch: [99]  [240/781]  eta: 0:01:47  lr: 0.000001  training_loss: 1.0864 (1.1071)  mae_loss: 0.0246 (0.0253)  classification_loss: 1.0618 (1.0816)  loss_mask: 0.0000 (0.0001)  time: 0.1955  data: 0.0002  max mem: 5511
[01:18:35.850581] Epoch: [99]  [260/781]  eta: 0:01:43  lr: 0.000001  training_loss: 1.1415 (1.1088)  mae_loss: 0.0254 (0.0254)  classification_loss: 1.1137 (1.0833)  loss_mask: 0.0000 (0.0001)  time: 0.1953  data: 0.0002  max mem: 5511
[01:18:39.791385] Epoch: [99]  [280/781]  eta: 0:01:39  lr: 0.000001  training_loss: 1.0843 (1.1067)  mae_loss: 0.0252 (0.0254)  classification_loss: 1.0556 (1.0812)  loss_mask: 0.0000 (0.0001)  time: 0.1970  data: 0.0003  max mem: 5511
[01:18:43.733924] Epoch: [99]  [300/781]  eta: 0:01:35  lr: 0.000001  training_loss: 1.0801 (1.1039)  mae_loss: 0.0242 (0.0253)  classification_loss: 1.0577 (1.0785)  loss_mask: 0.0000 (0.0001)  time: 0.1970  data: 0.0002  max mem: 5511
[01:18:47.682917] Epoch: [99]  [320/781]  eta: 0:01:31  lr: 0.000001  training_loss: 1.1137 (1.1054)  mae_loss: 0.0254 (0.0253)  classification_loss: 1.0862 (1.0800)  loss_mask: 0.0000 (0.0001)  time: 0.1974  data: 0.0002  max mem: 5511
[01:18:51.588375] Epoch: [99]  [340/781]  eta: 0:01:27  lr: 0.000001  training_loss: 1.0718 (1.1041)  mae_loss: 0.0252 (0.0254)  classification_loss: 1.0478 (1.0786)  loss_mask: 0.0000 (0.0001)  time: 0.1952  data: 0.0002  max mem: 5511
[01:18:55.561622] Epoch: [99]  [360/781]  eta: 0:01:23  lr: 0.000001  training_loss: 1.1254 (1.1061)  mae_loss: 0.0256 (0.0254)  classification_loss: 1.0959 (1.0807)  loss_mask: 0.0000 (0.0001)  time: 0.1986  data: 0.0003  max mem: 5511
[01:18:59.479369] Epoch: [99]  [380/781]  eta: 0:01:19  lr: 0.000001  training_loss: 1.1136 (1.1069)  mae_loss: 0.0257 (0.0254)  classification_loss: 1.0889 (1.0814)  loss_mask: 0.0000 (0.0001)  time: 0.1958  data: 0.0002  max mem: 5511
[01:19:03.434046] Epoch: [99]  [400/781]  eta: 0:01:15  lr: 0.000001  training_loss: 1.1197 (1.1064)  mae_loss: 0.0258 (0.0255)  classification_loss: 1.0919 (1.0809)  loss_mask: 0.0000 (0.0001)  time: 0.1976  data: 0.0002  max mem: 5511
[01:19:07.369766] Epoch: [99]  [420/781]  eta: 0:01:11  lr: 0.000001  training_loss: 1.0819 (1.1049)  mae_loss: 0.0259 (0.0255)  classification_loss: 1.0574 (1.0794)  loss_mask: 0.0000 (0.0001)  time: 0.1967  data: 0.0002  max mem: 5511
[01:19:11.286798] Epoch: [99]  [440/781]  eta: 0:01:07  lr: 0.000001  training_loss: 1.1145 (1.1051)  mae_loss: 0.0258 (0.0255)  classification_loss: 1.0899 (1.0796)  loss_mask: 0.0000 (0.0001)  time: 0.1957  data: 0.0002  max mem: 5511
[01:19:15.224339] Epoch: [99]  [460/781]  eta: 0:01:03  lr: 0.000001  training_loss: 1.0617 (1.1046)  mae_loss: 0.0249 (0.0255)  classification_loss: 1.0334 (1.0790)  loss_mask: 0.0000 (0.0001)  time: 0.1968  data: 0.0002  max mem: 5511
[01:19:19.144090] Epoch: [99]  [480/781]  eta: 0:00:59  lr: 0.000001  training_loss: 1.0961 (1.1044)  mae_loss: 0.0264 (0.0255)  classification_loss: 1.0724 (1.0788)  loss_mask: 0.0000 (0.0001)  time: 0.1959  data: 0.0002  max mem: 5511
[01:19:23.081519] Epoch: [99]  [500/781]  eta: 0:00:55  lr: 0.000001  training_loss: 1.0974 (1.1044)  mae_loss: 0.0247 (0.0255)  classification_loss: 1.0715 (1.0788)  loss_mask: 0.0000 (0.0001)  time: 0.1968  data: 0.0002  max mem: 5511
[01:19:27.018224] Epoch: [99]  [520/781]  eta: 0:00:51  lr: 0.000001  training_loss: 1.1206 (1.1047)  mae_loss: 0.0249 (0.0256)  classification_loss: 1.0906 (1.0790)  loss_mask: 0.0000 (0.0001)  time: 0.1968  data: 0.0001  max mem: 5511
[01:19:30.965870] Epoch: [99]  [540/781]  eta: 0:00:47  lr: 0.000001  training_loss: 1.0450 (1.1031)  mae_loss: 0.0252 (0.0256)  classification_loss: 1.0198 (1.0775)  loss_mask: 0.0000 (0.0001)  time: 0.1973  data: 0.0002  max mem: 5511
[01:19:34.883691] Epoch: [99]  [560/781]  eta: 0:00:43  lr: 0.000001  training_loss: 1.1228 (1.1033)  mae_loss: 0.0246 (0.0255)  classification_loss: 1.0995 (1.0777)  loss_mask: 0.0000 (0.0001)  time: 0.1958  data: 0.0003  max mem: 5511
[01:19:38.793123] Epoch: [99]  [580/781]  eta: 0:00:39  lr: 0.000001  training_loss: 1.0791 (1.1028)  mae_loss: 0.0253 (0.0255)  classification_loss: 1.0577 (1.0772)  loss_mask: 0.0000 (0.0001)  time: 0.1954  data: 0.0002  max mem: 5511
[01:19:42.712495] Epoch: [99]  [600/781]  eta: 0:00:35  lr: 0.000001  training_loss: 1.0891 (1.1019)  mae_loss: 0.0252 (0.0255)  classification_loss: 1.0611 (1.0763)  loss_mask: 0.0000 (0.0001)  time: 0.1959  data: 0.0002  max mem: 5511
[01:19:46.636767] Epoch: [99]  [620/781]  eta: 0:00:31  lr: 0.000001  training_loss: 1.0938 (1.1019)  mae_loss: 0.0260 (0.0256)  classification_loss: 1.0621 (1.0763)  loss_mask: 0.0000 (0.0001)  time: 0.1961  data: 0.0002  max mem: 5511
[01:19:50.557059] Epoch: [99]  [640/781]  eta: 0:00:27  lr: 0.000001  training_loss: 1.1311 (1.1027)  mae_loss: 0.0250 (0.0256)  classification_loss: 1.1085 (1.0771)  loss_mask: 0.0000 (0.0001)  time: 0.1959  data: 0.0002  max mem: 5511
[01:19:54.476535] Epoch: [99]  [660/781]  eta: 0:00:23  lr: 0.000001  training_loss: 1.1328 (1.1024)  mae_loss: 0.0248 (0.0256)  classification_loss: 1.1026 (1.0767)  loss_mask: 0.0000 (0.0001)  time: 0.1959  data: 0.0002  max mem: 5511
[01:19:58.383364] Epoch: [99]  [680/781]  eta: 0:00:19  lr: 0.000001  training_loss: 1.1435 (1.1033)  mae_loss: 0.0254 (0.0256)  classification_loss: 1.1195 (1.0777)  loss_mask: 0.0000 (0.0001)  time: 0.1952  data: 0.0002  max mem: 5511
[01:20:02.282936] Epoch: [99]  [700/781]  eta: 0:00:15  lr: 0.000001  training_loss: 1.1303 (1.1044)  mae_loss: 0.0250 (0.0256)  classification_loss: 1.1072 (1.0788)  loss_mask: 0.0000 (0.0000)  time: 0.1949  data: 0.0002  max mem: 5511
[01:20:06.192280] Epoch: [99]  [720/781]  eta: 0:00:12  lr: 0.000001  training_loss: 1.1366 (1.1055)  mae_loss: 0.0245 (0.0256)  classification_loss: 1.1138 (1.0799)  loss_mask: 0.0000 (0.0000)  time: 0.1954  data: 0.0002  max mem: 5511
[01:20:10.142796] Epoch: [99]  [740/781]  eta: 0:00:08  lr: 0.000001  training_loss: 1.1155 (1.1060)  mae_loss: 0.0235 (0.0256)  classification_loss: 1.0873 (1.0804)  loss_mask: 0.0000 (0.0000)  time: 0.1974  data: 0.0002  max mem: 5511
[01:20:14.068209] Epoch: [99]  [760/781]  eta: 0:00:04  lr: 0.000001  training_loss: 1.1404 (1.1063)  mae_loss: 0.0251 (0.0256)  classification_loss: 1.1137 (1.0807)  loss_mask: 0.0000 (0.0000)  time: 0.1962  data: 0.0002  max mem: 5511
[01:20:17.972494] Epoch: [99]  [780/781]  eta: 0:00:00  lr: 0.000001  training_loss: 1.1049 (1.1061)  mae_loss: 0.0253 (0.0256)  classification_loss: 1.0832 (1.0805)  loss_mask: 0.0000 (0.0000)  time: 0.1951  data: 0.0002  max mem: 5511
[01:20:18.114102] Epoch: [99] Total time: 0:02:34 (0.1975 s / it)
[01:20:18.114588] Averaged stats: lr: 0.000001  training_loss: 1.1049 (1.1061)  mae_loss: 0.0253 (0.0256)  classification_loss: 1.0832 (1.0805)  loss_mask: 0.0000 (0.0000)
[01:20:18.752150] Test:  [  0/157]  eta: 0:01:39  testing_loss: 0.5403 (0.5403)  acc1: 84.3750 (84.3750)  acc5: 100.0000 (100.0000)  time: 0.6332  data: 0.5969  max mem: 5511
[01:20:19.045153] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4341 (0.4620)  acc1: 85.9375 (85.9375)  acc5: 100.0000 (99.5739)  time: 0.0839  data: 0.0544  max mem: 5511
[01:20:19.327624] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4083 (0.4163)  acc1: 87.5000 (87.9464)  acc5: 100.0000 (99.5536)  time: 0.0286  data: 0.0002  max mem: 5511
[01:20:19.610415] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4210 (0.4343)  acc1: 89.0625 (87.3992)  acc5: 100.0000 (99.3952)  time: 0.0281  data: 0.0002  max mem: 5511
[01:20:19.894987] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4424 (0.4406)  acc1: 87.5000 (87.2713)  acc5: 98.4375 (99.2378)  time: 0.0282  data: 0.0002  max mem: 5511
[01:20:20.188923] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4392 (0.4343)  acc1: 87.5000 (87.5613)  acc5: 100.0000 (99.3260)  time: 0.0288  data: 0.0002  max mem: 5511
[01:20:20.474513] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4054 (0.4319)  acc1: 87.5000 (87.6537)  acc5: 100.0000 (99.3340)  time: 0.0288  data: 0.0002  max mem: 5511
[01:20:20.757758] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4154 (0.4276)  acc1: 87.5000 (87.8961)  acc5: 100.0000 (99.3178)  time: 0.0283  data: 0.0002  max mem: 5511
[01:20:21.041821] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4243 (0.4343)  acc1: 87.5000 (87.5772)  acc5: 100.0000 (99.3441)  time: 0.0282  data: 0.0002  max mem: 5511
[01:20:21.325808] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4257 (0.4301)  acc1: 87.5000 (87.8091)  acc5: 100.0000 (99.3132)  time: 0.0282  data: 0.0002  max mem: 5511
[01:20:21.608036] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4257 (0.4321)  acc1: 89.0625 (87.7939)  acc5: 98.4375 (99.3038)  time: 0.0281  data: 0.0002  max mem: 5511
[01:20:21.891370] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4382 (0.4320)  acc1: 85.9375 (87.7956)  acc5: 100.0000 (99.3243)  time: 0.0281  data: 0.0002  max mem: 5511
[01:20:22.174516] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4724 (0.4327)  acc1: 87.5000 (87.7712)  acc5: 100.0000 (99.3414)  time: 0.0281  data: 0.0002  max mem: 5511
[01:20:22.466700] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4295 (0.4339)  acc1: 87.5000 (87.6670)  acc5: 100.0000 (99.3321)  time: 0.0286  data: 0.0002  max mem: 5511
[01:20:22.749119] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4164 (0.4330)  acc1: 85.9375 (87.6219)  acc5: 100.0000 (99.3573)  time: 0.0286  data: 0.0002  max mem: 5511
[01:20:23.030285] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4393 (0.4334)  acc1: 85.9375 (87.6656)  acc5: 100.0000 (99.3584)  time: 0.0281  data: 0.0001  max mem: 5511
[01:20:23.181289] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4188 (0.4321)  acc1: 89.0625 (87.7100)  acc5: 100.0000 (99.3600)  time: 0.0271  data: 0.0001  max mem: 5511
[01:20:23.324715] Test: Total time: 0:00:05 (0.0332 s / it)
[01:20:23.325206] * Acc@1 87.710 Acc@5 99.360 loss 0.432
[01:20:23.325559] Accuracy of the network on the 10000 test images: 87.7%
[01:20:23.325937] Max accuracy: 87.82%
[01:20:23.520097] Training time 4:27:01