Not using distributed mode
[05:49:01.601513] job dir: /notebooks/CVPR2023
[05:49:01.602102] Namespace(batch_size=64,
epochs=100,
accum_iter=1,
model='mae_vit_tiny',
norm_pix_loss=False,
dataset='c10',
input_size=32,
patch_size=2,
mask_ratio=0.75,
lambda_weight=0.1,
drop_path=0.1,
clip_grad=None,
weight_decay=0.05,
lr=None,
blr=0.001,
layer_decay=0.75,
min_lr=1e-06,
warmup_epochs=5,
color_jitter=None,
aa='rand-m9-mstd0.5-inc1',
smoothing=0.1,
reprob=0.25,
remode='pixel',
recount=1,
resplit=False,
mixup=0,
cutmix=0,
cutmix_minmax=None,
mixup_prob=1.0,
mixup_switch_prob=0.5,
mixup_mode='batch',
finetune='',
global_pool=True,
data_path='/datasets01/imagenet_full_size/061417/',
nb_classes=10,
output_dir='./output_dir',
log_dir='./output_dir',
device='cuda',
seed=0,
resume='',
start_epoch=0,
eval=False,
dist_eval=False,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[05:49:02.371198] Files already downloaded and verified
/usr/local/lib/python3.9/dist-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
[05:49:03.201620] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCropAndInterpolation(size=(32, 32), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic)
               RandomHorizontalFlip(p=0.5)
               RandAugment(n=2, ops=
           	AugmentOp(name=AutoContrast, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Equalize, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Invert, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Rotate, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=PosterizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeAdd, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ColorIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ContrastIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=BrightnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SharpnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearX, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearY, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateXRel, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateYRel, p=0.5, m=9, mstd=0.5))
               ToTensor()
               Normalize(mean=tensor([0.4914, 0.4822, 0.4465]), std=tensor([0.2023, 0.1994, 0.2010]))
               RandomErasing(p=0.25, mode=pixel, count=(1, 1))
           )
[05:49:03.576182] Files already downloaded and verified
[05:49:04.009225] Dataset CIFAR10
    Number of datapoints: 10000
    Root location: ./data
    Split: Test
    StandardTransform
Transform: Compose(
               Resize(size=36, interpolation=bicubic, max_size=None, antialias=None)
               CenterCrop(size=(32, 32))
               ToTensor()
               Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.201))
           )
[05:49:04.009915] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7fcaaf50efa0>
/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[05:49:09.228523] Model = MaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(2, 2), stride=(2, 2))
    (norm): Identity()
  )
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (7): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (8): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (9): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (10): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (11): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
  (decoder_embed): Linear(in_features=192, out_features=128, bias=True)
  (decoder_blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=128, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=128, out_features=128, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=512, out_features=128, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=128, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=128, out_features=128, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=512, out_features=128, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (decoder_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
  (decoder_pred): Linear(in_features=128, out_features=12, bias=True)
  (head): Linear(in_features=192, out_features=10, bias=True)
  (classifier_mask): Sequential(
    (0): Linear(in_features=192, out_features=5, bias=True)
    (1): LogSoftmax(dim=1)
  )
)
[05:49:09.229341] number of params (M): 5.77
[05:49:09.229548] base lr: 1.00e-03
[05:49:09.229715] actual lr: 2.50e-04
[05:49:09.229861] accumulate grad iterations: 1
[05:49:09.230002] effective batch size: 64
[05:49:09.231142] criterion = LabelSmoothingCrossEntropy()
[05:49:09.231394] Start training for 100 epochs
[05:49:09.232458] log_dir: ./output_dir
[05:49:11.929330] Epoch: [0]  [  0/781]  eta: 0:35:04  lr: 0.000000  training_loss: 6.5847 (6.5847)  mae_loss: 3.8136 (3.8136)  classification_loss: 2.6555 (2.6555)  loss_mask: 1.9646 (1.9646)  time: 2.6942  data: 0.4246  max mem: 5042
[05:49:15.354458] Epoch: [0]  [ 20/781]  eta: 0:03:41  lr: 0.000001  training_loss: 6.4051 (6.4713)  mae_loss: 3.9853 (3.9929)  classification_loss: 2.5956 (2.5991)  loss_mask: 1.9218 (1.9361)  time: 0.1711  data: 0.0002  max mem: 6050
[05:49:18.765017] Epoch: [0]  [ 40/781]  eta: 0:02:52  lr: 0.000003  training_loss: 6.0693 (6.2953)  mae_loss: 3.9925 (4.0072)  classification_loss: 2.4896 (2.5479)  loss_mask: 1.7899 (1.8737)  time: 0.1705  data: 0.0003  max mem: 6050
[05:49:22.202673] Epoch: [0]  [ 60/781]  eta: 0:02:33  lr: 0.000004  training_loss: 5.7474 (6.1298)  mae_loss: 4.0440 (4.0329)  classification_loss: 2.4083 (2.4996)  loss_mask: 1.6945 (1.8151)  time: 0.1718  data: 0.0002  max mem: 6050
[05:49:25.617618] Epoch: [0]  [ 80/781]  eta: 0:02:21  lr: 0.000005  training_loss: 5.6693 (6.0236)  mae_loss: 4.0309 (4.0431)  classification_loss: 2.3433 (2.4607)  loss_mask: 1.6772 (1.7814)  time: 0.1707  data: 0.0002  max mem: 6050
[05:49:29.043828] Epoch: [0]  [100/781]  eta: 0:02:13  lr: 0.000006  training_loss: 5.6274 (5.9426)  mae_loss: 4.0383 (4.0474)  classification_loss: 2.3091 (2.4310)  loss_mask: 1.6553 (1.7558)  time: 0.1712  data: 0.0002  max mem: 6050
[05:49:32.461954] Epoch: [0]  [120/781]  eta: 0:02:06  lr: 0.000008  training_loss: 5.5235 (5.8759)  mae_loss: 4.1583 (4.0667)  classification_loss: 2.3008 (2.4099)  loss_mask: 1.6061 (1.7330)  time: 0.1708  data: 0.0004  max mem: 6050
[05:49:35.945533] Epoch: [0]  [140/781]  eta: 0:02:01  lr: 0.000009  training_loss: 5.5173 (5.8282)  mae_loss: 4.1354 (4.0788)  classification_loss: 2.2872 (2.3923)  loss_mask: 1.6207 (1.7179)  time: 0.1741  data: 0.0002  max mem: 6050
[05:49:39.386790] Epoch: [0]  [160/781]  eta: 0:01:56  lr: 0.000010  training_loss: 5.5026 (5.7857)  mae_loss: 4.1242 (4.0852)  classification_loss: 2.2704 (2.3781)  loss_mask: 1.6077 (1.7038)  time: 0.1720  data: 0.0002  max mem: 6050
[05:49:42.876143] Epoch: [0]  [180/781]  eta: 0:01:51  lr: 0.000012  training_loss: 5.4888 (5.7528)  mae_loss: 4.1377 (4.0946)  classification_loss: 2.2828 (2.3672)  loss_mask: 1.6008 (1.6928)  time: 0.1744  data: 0.0004  max mem: 6050
[05:49:46.306927] Epoch: [0]  [200/781]  eta: 0:01:47  lr: 0.000013  training_loss: 5.4666 (5.7227)  mae_loss: 4.1692 (4.1024)  classification_loss: 2.2525 (2.3564)  loss_mask: 1.5931 (1.6831)  time: 0.1715  data: 0.0003  max mem: 6050
[05:49:49.754661] Epoch: [0]  [220/781]  eta: 0:01:42  lr: 0.000014  training_loss: 5.3989 (5.6963)  mae_loss: 4.1697 (4.1088)  classification_loss: 2.2532 (2.3480)  loss_mask: 1.5797 (1.6742)  time: 0.1723  data: 0.0003  max mem: 6050
[05:49:53.208270] Epoch: [0]  [240/781]  eta: 0:01:38  lr: 0.000015  training_loss: 5.4335 (5.6743)  mae_loss: 4.2061 (4.1197)  classification_loss: 2.2654 (2.3419)  loss_mask: 1.5773 (1.6662)  time: 0.1726  data: 0.0002  max mem: 6050
[05:49:56.690335] Epoch: [0]  [260/781]  eta: 0:01:34  lr: 0.000017  training_loss: 5.4110 (5.6529)  mae_loss: 4.1592 (4.1277)  classification_loss: 2.2442 (2.3353)  loss_mask: 1.5733 (1.6588)  time: 0.1740  data: 0.0002  max mem: 6050
[05:50:00.143283] Epoch: [0]  [280/781]  eta: 0:01:30  lr: 0.000018  training_loss: 5.3848 (5.6326)  mae_loss: 4.2998 (4.1385)  classification_loss: 2.2504 (2.3299)  loss_mask: 1.5535 (1.6514)  time: 0.1726  data: 0.0002  max mem: 6050
[05:50:03.601006] Epoch: [0]  [300/781]  eta: 0:01:26  lr: 0.000019  training_loss: 5.3358 (5.6154)  mae_loss: 4.1891 (4.1416)  classification_loss: 2.2631 (2.3254)  loss_mask: 1.5545 (1.6450)  time: 0.1728  data: 0.0002  max mem: 6050
[05:50:07.068656] Epoch: [0]  [320/781]  eta: 0:01:23  lr: 0.000020  training_loss: 5.3585 (5.5977)  mae_loss: 4.2117 (4.1474)  classification_loss: 2.2416 (2.3208)  loss_mask: 1.5366 (1.6384)  time: 0.1733  data: 0.0002  max mem: 6050
[05:50:10.583586] Epoch: [0]  [340/781]  eta: 0:01:19  lr: 0.000022  training_loss: 5.3306 (5.5829)  mae_loss: 4.1589 (4.1496)  classification_loss: 2.2573 (2.3166)  loss_mask: 1.5414 (1.6332)  time: 0.1757  data: 0.0003  max mem: 6050
[05:50:14.090209] Epoch: [0]  [360/781]  eta: 0:01:15  lr: 0.000023  training_loss: 5.2646 (5.5663)  mae_loss: 4.1759 (4.1512)  classification_loss: 2.2639 (2.3127)  loss_mask: 1.5114 (1.6268)  time: 0.1753  data: 0.0003  max mem: 6050
[05:50:17.592565] Epoch: [0]  [380/781]  eta: 0:01:11  lr: 0.000024  training_loss: 5.3036 (5.5525)  mae_loss: 4.1839 (4.1524)  classification_loss: 2.2483 (2.3094)  loss_mask: 1.5323 (1.6215)  time: 0.1750  data: 0.0003  max mem: 6050
[05:50:21.038436] Epoch: [0]  [400/781]  eta: 0:01:08  lr: 0.000026  training_loss: 5.2317 (5.5370)  mae_loss: 4.1829 (4.1547)  classification_loss: 2.2284 (2.3057)  loss_mask: 1.5017 (1.6157)  time: 0.1722  data: 0.0003  max mem: 6050
[05:50:24.465097] Epoch: [0]  [420/781]  eta: 0:01:04  lr: 0.000027  training_loss: 5.2024 (5.5212)  mae_loss: 4.0676 (4.1513)  classification_loss: 2.2505 (2.3037)  loss_mask: 1.4680 (1.6087)  time: 0.1713  data: 0.0002  max mem: 6050
[05:50:27.903524] Epoch: [0]  [440/781]  eta: 0:01:00  lr: 0.000028  training_loss: 5.1106 (5.5036)  mae_loss: 4.1094 (4.1514)  classification_loss: 2.2474 (2.3007)  loss_mask: 1.4404 (1.6014)  time: 0.1718  data: 0.0002  max mem: 6050
[05:50:31.391820] Epoch: [0]  [460/781]  eta: 0:00:57  lr: 0.000029  training_loss: 5.0430 (5.4846)  mae_loss: 4.0645 (4.1469)  classification_loss: 2.2018 (2.2969)  loss_mask: 1.3960 (1.5939)  time: 0.1743  data: 0.0004  max mem: 6050
[05:50:34.846045] Epoch: [0]  [480/781]  eta: 0:00:53  lr: 0.000031  training_loss: 4.9831 (5.4656)  mae_loss: 4.0165 (4.1411)  classification_loss: 2.2336 (2.2947)  loss_mask: 1.3705 (1.5854)  time: 0.1726  data: 0.0005  max mem: 6050
[05:50:38.283360] Epoch: [0]  [500/781]  eta: 0:00:49  lr: 0.000032  training_loss: 4.9588 (5.4456)  mae_loss: 3.9174 (4.1334)  classification_loss: 2.2312 (2.2923)  loss_mask: 1.3639 (1.5766)  time: 0.1717  data: 0.0002  max mem: 6050
[05:50:41.731201] Epoch: [0]  [520/781]  eta: 0:00:46  lr: 0.000033  training_loss: 4.7883 (5.4204)  mae_loss: 3.9968 (4.1275)  classification_loss: 2.2501 (2.2904)  loss_mask: 1.2671 (1.5650)  time: 0.1723  data: 0.0002  max mem: 6050
[05:50:45.188284] Epoch: [0]  [540/781]  eta: 0:00:42  lr: 0.000035  training_loss: 4.6666 (5.3939)  mae_loss: 3.8920 (4.1203)  classification_loss: 2.2369 (2.2890)  loss_mask: 1.1921 (1.5525)  time: 0.1727  data: 0.0002  max mem: 6050
[05:50:48.665964] Epoch: [0]  [560/781]  eta: 0:00:39  lr: 0.000036  training_loss: 4.6053 (5.3648)  mae_loss: 3.8983 (4.1134)  classification_loss: 2.2332 (2.2873)  loss_mask: 1.1761 (1.5388)  time: 0.1738  data: 0.0003  max mem: 6050
[05:50:52.143936] Epoch: [0]  [580/781]  eta: 0:00:35  lr: 0.000037  training_loss: 4.5077 (5.3341)  mae_loss: 3.8192 (4.1046)  classification_loss: 2.2161 (2.2851)  loss_mask: 1.1251 (1.5245)  time: 0.1738  data: 0.0002  max mem: 6050
[05:50:55.576754] Epoch: [0]  [600/781]  eta: 0:00:32  lr: 0.000038  training_loss: 4.3500 (5.3018)  mae_loss: 3.9469 (4.0995)  classification_loss: 2.2182 (2.2830)  loss_mask: 1.0627 (1.5094)  time: 0.1716  data: 0.0002  max mem: 6050
[05:50:59.026179] Epoch: [0]  [620/781]  eta: 0:00:28  lr: 0.000040  training_loss: 4.4967 (5.2765)  mae_loss: 3.9537 (4.0953)  classification_loss: 2.2254 (2.2813)  loss_mask: 1.1391 (1.4976)  time: 0.1724  data: 0.0002  max mem: 6050
[05:51:02.493176] Epoch: [0]  [640/781]  eta: 0:00:24  lr: 0.000041  training_loss: 4.3829 (5.2497)  mae_loss: 3.9088 (4.0896)  classification_loss: 2.2265 (2.2797)  loss_mask: 1.0786 (1.4850)  time: 0.1733  data: 0.0003  max mem: 6050
[05:51:05.979271] Epoch: [0]  [660/781]  eta: 0:00:21  lr: 0.000042  training_loss: 4.2807 (5.2216)  mae_loss: 3.9365 (4.0861)  classification_loss: 2.2381 (2.2784)  loss_mask: 1.0204 (1.4716)  time: 0.1742  data: 0.0002  max mem: 6050
[05:51:09.471987] Epoch: [0]  [680/781]  eta: 0:00:17  lr: 0.000044  training_loss: 4.2630 (5.1943)  mae_loss: 3.8436 (4.0802)  classification_loss: 2.2174 (2.2769)  loss_mask: 1.0135 (1.4587)  time: 0.1745  data: 0.0002  max mem: 6050
[05:51:12.930457] Epoch: [0]  [700/781]  eta: 0:00:14  lr: 0.000045  training_loss: 4.2519 (5.1676)  mae_loss: 3.8121 (4.0728)  classification_loss: 2.2077 (2.2752)  loss_mask: 1.0003 (1.4462)  time: 0.1728  data: 0.0003  max mem: 6050
[05:51:16.380618] Epoch: [0]  [720/781]  eta: 0:00:10  lr: 0.000046  training_loss: 4.1679 (5.1415)  mae_loss: 4.0176 (4.0711)  classification_loss: 2.2407 (2.2742)  loss_mask: 0.9700 (1.4336)  time: 0.1724  data: 0.0002  max mem: 6050
[05:51:19.830483] Epoch: [0]  [740/781]  eta: 0:00:07  lr: 0.000047  training_loss: 4.1104 (5.1148)  mae_loss: 3.9275 (4.0654)  classification_loss: 2.2369 (2.2731)  loss_mask: 0.9389 (1.4208)  time: 0.1724  data: 0.0002  max mem: 6050
[05:51:23.294594] Epoch: [0]  [760/781]  eta: 0:00:03  lr: 0.000049  training_loss: 4.0471 (5.0868)  mae_loss: 3.8325 (4.0599)  classification_loss: 2.2148 (2.2721)  loss_mask: 0.8913 (1.4074)  time: 0.1731  data: 0.0003  max mem: 6050
[05:51:26.736715] Epoch: [0]  [780/781]  eta: 0:00:00  lr: 0.000050  training_loss: 3.9939 (5.0583)  mae_loss: 3.8584 (4.0545)  classification_loss: 2.2156 (2.2710)  loss_mask: 0.8676 (1.3937)  time: 0.1720  data: 0.0002  max mem: 6050
[05:51:26.848692] Epoch: [0] Total time: 0:02:17 (0.1762 s / it)
[05:51:26.849196] Averaged stats: lr: 0.000050  training_loss: 3.9939 (5.0583)  mae_loss: 3.8584 (4.0545)  classification_loss: 2.2156 (2.2710)  loss_mask: 0.8676 (1.3937)
[05:51:28.501980] Test:  [  0/157]  eta: 0:01:59  testing_loss: 2.0153 (2.0153)  acc1: 34.3750 (34.3750)  acc5: 89.0625 (89.0625)  time: 0.7604  data: 0.7214  max mem: 6050
[05:51:28.813388] Test:  [ 10/157]  eta: 0:00:14  testing_loss: 2.0893 (2.0743)  acc1: 25.0000 (26.9886)  acc5: 76.5625 (77.5568)  time: 0.0973  data: 0.0659  max mem: 6050
[05:51:29.113845] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 2.0816 (2.0697)  acc1: 26.5625 (27.4554)  acc5: 76.5625 (78.3482)  time: 0.0304  data: 0.0003  max mem: 6050
[05:51:29.407529] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 2.0407 (2.0621)  acc1: 29.6875 (28.6794)  acc5: 78.1250 (78.3266)  time: 0.0296  data: 0.0002  max mem: 6050
[05:51:29.696688] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 2.0570 (2.0624)  acc1: 29.6875 (28.6585)  acc5: 78.1250 (78.1631)  time: 0.0289  data: 0.0002  max mem: 6050
[05:51:29.979718] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 2.0630 (2.0631)  acc1: 28.1250 (28.2169)  acc5: 79.6875 (78.3395)  time: 0.0284  data: 0.0002  max mem: 6050
[05:51:30.261773] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 2.0535 (2.0626)  acc1: 28.1250 (28.3811)  acc5: 81.2500 (78.5605)  time: 0.0281  data: 0.0002  max mem: 6050
[05:51:30.544422] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 2.0503 (2.0606)  acc1: 26.5625 (28.1250)  acc5: 81.2500 (78.8512)  time: 0.0281  data: 0.0002  max mem: 6050
[05:51:30.829483] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 2.0457 (2.0608)  acc1: 25.0000 (27.8549)  acc5: 81.2500 (79.2245)  time: 0.0283  data: 0.0002  max mem: 6050
[05:51:31.113061] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 2.0800 (2.0642)  acc1: 25.0000 (27.5755)  acc5: 79.6875 (79.2067)  time: 0.0283  data: 0.0002  max mem: 6050
[05:51:31.396710] Test:  [100/157]  eta: 0:00:02  testing_loss: 2.0800 (2.0641)  acc1: 26.5625 (27.5835)  acc5: 78.1250 (79.3472)  time: 0.0282  data: 0.0002  max mem: 6050
[05:51:31.679619] Test:  [110/157]  eta: 0:00:01  testing_loss: 2.0645 (2.0647)  acc1: 25.0000 (27.3367)  acc5: 76.5625 (79.3074)  time: 0.0282  data: 0.0002  max mem: 6050
[05:51:31.963629] Test:  [120/157]  eta: 0:00:01  testing_loss: 2.0491 (2.0626)  acc1: 25.0000 (27.3244)  acc5: 79.6875 (79.3905)  time: 0.0282  data: 0.0002  max mem: 6050
[05:51:32.247716] Test:  [130/157]  eta: 0:00:00  testing_loss: 2.0501 (2.0637)  acc1: 26.5625 (27.5525)  acc5: 78.1250 (79.2343)  time: 0.0283  data: 0.0002  max mem: 6050
[05:51:32.531590] Test:  [140/157]  eta: 0:00:00  testing_loss: 2.0610 (2.0623)  acc1: 29.6875 (27.6485)  acc5: 79.6875 (79.3551)  time: 0.0283  data: 0.0002  max mem: 6050
[05:51:32.814399] Test:  [150/157]  eta: 0:00:00  testing_loss: 2.0505 (2.0619)  acc1: 29.6875 (27.6490)  acc5: 81.2500 (79.5012)  time: 0.0282  data: 0.0001  max mem: 6050
[05:51:33.110453] Test:  [156/157]  eta: 0:00:00  testing_loss: 2.0390 (2.0613)  acc1: 28.1250 (27.6600)  acc5: 81.2500 (79.5500)  time: 0.0344  data: 0.0001  max mem: 6050
[05:51:33.274183] Test: Total time: 0:00:05 (0.0353 s / it)
[05:51:33.274755] * Acc@1 27.660 Acc@5 79.550 loss 2.061
[05:51:33.275082] Accuracy of the network on the 10000 test images: 27.7%
[05:51:33.275234] Max accuracy: 27.66%
[05:51:33.479755] log_dir: ./output_dir
[05:51:34.285455] Epoch: [1]  [  0/781]  eta: 0:10:27  lr: 0.000050  training_loss: 4.1229 (4.1229)  mae_loss: 3.8749 (3.8749)  classification_loss: 2.2386 (2.2386)  loss_mask: 0.9421 (0.9421)  time: 0.8036  data: 0.5741  max mem: 6050
[05:51:37.745194] Epoch: [1]  [ 20/781]  eta: 0:02:34  lr: 0.000051  training_loss: 4.0518 (4.0920)  mae_loss: 3.7881 (3.8117)  classification_loss: 2.2175 (2.2288)  loss_mask: 0.9054 (0.9316)  time: 0.1729  data: 0.0001  max mem: 6052
[05:51:41.236573] Epoch: [1]  [ 40/781]  eta: 0:02:20  lr: 0.000053  training_loss: 3.9324 (4.0442)  mae_loss: 3.8094 (3.8133)  classification_loss: 2.2204 (2.2291)  loss_mask: 0.8642 (0.9076)  time: 0.1745  data: 0.0002  max mem: 6052
[05:51:44.697567] Epoch: [1]  [ 60/781]  eta: 0:02:12  lr: 0.000054  training_loss: 3.9116 (4.0262)  mae_loss: 3.7616 (3.8050)  classification_loss: 2.2378 (2.2315)  loss_mask: 0.8461 (0.8974)  time: 0.1730  data: 0.0002  max mem: 6052
[05:51:48.170134] Epoch: [1]  [ 80/781]  eta: 0:02:07  lr: 0.000055  training_loss: 3.9354 (4.0332)  mae_loss: 3.8261 (3.8097)  classification_loss: 2.2246 (2.2313)  loss_mask: 0.8498 (0.9010)  time: 0.1735  data: 0.0002  max mem: 6052
[05:51:51.638311] Epoch: [1]  [100/781]  eta: 0:02:02  lr: 0.000056  training_loss: 4.0075 (4.0401)  mae_loss: 3.8378 (3.8250)  classification_loss: 2.2182 (2.2287)  loss_mask: 0.8989 (0.9057)  time: 0.1733  data: 0.0002  max mem: 6052
[05:51:55.106787] Epoch: [1]  [120/781]  eta: 0:01:58  lr: 0.000058  training_loss: 3.8912 (4.0292)  mae_loss: 3.8534 (3.8275)  classification_loss: 2.2295 (2.2282)  loss_mask: 0.8200 (0.9005)  time: 0.1733  data: 0.0006  max mem: 6052
[05:51:58.585384] Epoch: [1]  [140/781]  eta: 0:01:54  lr: 0.000059  training_loss: 3.8378 (3.9991)  mae_loss: 3.7773 (3.8227)  classification_loss: 2.1963 (2.2249)  loss_mask: 0.8019 (0.8871)  time: 0.1738  data: 0.0002  max mem: 6052
[05:52:02.033300] Epoch: [1]  [160/781]  eta: 0:01:50  lr: 0.000060  training_loss: 4.1228 (4.0123)  mae_loss: 3.7987 (3.8183)  classification_loss: 2.2209 (2.2241)  loss_mask: 0.9511 (0.8941)  time: 0.1723  data: 0.0002  max mem: 6052
[05:52:05.496344] Epoch: [1]  [180/781]  eta: 0:01:46  lr: 0.000062  training_loss: 3.9702 (4.0079)  mae_loss: 3.8527 (3.8222)  classification_loss: 2.2042 (2.2239)  loss_mask: 0.8519 (0.8920)  time: 0.1731  data: 0.0002  max mem: 6052
[05:52:08.965863] Epoch: [1]  [200/781]  eta: 0:01:42  lr: 0.000063  training_loss: 3.8298 (3.9903)  mae_loss: 3.8023 (3.8171)  classification_loss: 2.2125 (2.2228)  loss_mask: 0.8157 (0.8838)  time: 0.1734  data: 0.0002  max mem: 6052
[05:52:12.435803] Epoch: [1]  [220/781]  eta: 0:01:38  lr: 0.000064  training_loss: 3.7782 (3.9761)  mae_loss: 3.8030 (3.8182)  classification_loss: 2.2097 (2.2225)  loss_mask: 0.7808 (0.8768)  time: 0.1734  data: 0.0003  max mem: 6052
[05:52:15.896533] Epoch: [1]  [240/781]  eta: 0:01:35  lr: 0.000065  training_loss: 3.8943 (3.9744)  mae_loss: 3.8524 (3.8176)  classification_loss: 2.2159 (2.2222)  loss_mask: 0.8320 (0.8761)  time: 0.1730  data: 0.0004  max mem: 6052
[05:52:19.341527] Epoch: [1]  [260/781]  eta: 0:01:31  lr: 0.000067  training_loss: 3.8258 (3.9670)  mae_loss: 3.8008 (3.8177)  classification_loss: 2.2247 (2.2217)  loss_mask: 0.7959 (0.8726)  time: 0.1722  data: 0.0002  max mem: 6052
[05:52:22.802616] Epoch: [1]  [280/781]  eta: 0:01:27  lr: 0.000068  training_loss: 3.7986 (3.9601)  mae_loss: 3.8573 (3.8216)  classification_loss: 2.2422 (2.2230)  loss_mask: 0.7688 (0.8685)  time: 0.1730  data: 0.0002  max mem: 6052
[05:52:26.295192] Epoch: [1]  [300/781]  eta: 0:01:24  lr: 0.000069  training_loss: 3.6476 (3.9412)  mae_loss: 3.6982 (3.8171)  classification_loss: 2.2109 (2.2225)  loss_mask: 0.7060 (0.8593)  time: 0.1745  data: 0.0002  max mem: 6052
[05:52:29.776753] Epoch: [1]  [320/781]  eta: 0:01:20  lr: 0.000070  training_loss: 3.7089 (3.9279)  mae_loss: 3.5884 (3.8048)  classification_loss: 2.1965 (2.2215)  loss_mask: 0.7448 (0.8532)  time: 0.1740  data: 0.0002  max mem: 6052
[05:52:33.236751] Epoch: [1]  [340/781]  eta: 0:01:17  lr: 0.000072  training_loss: 3.7699 (3.9180)  mae_loss: 3.6600 (3.7959)  classification_loss: 2.2426 (2.2218)  loss_mask: 0.7556 (0.8481)  time: 0.1729  data: 0.0002  max mem: 6052
[05:52:36.693201] Epoch: [1]  [360/781]  eta: 0:01:13  lr: 0.000073  training_loss: 3.8217 (3.9135)  mae_loss: 3.7414 (3.7957)  classification_loss: 2.2399 (2.2220)  loss_mask: 0.7874 (0.8458)  time: 0.1727  data: 0.0002  max mem: 6052
[05:52:40.166558] Epoch: [1]  [380/781]  eta: 0:01:10  lr: 0.000074  training_loss: 3.8570 (3.9130)  mae_loss: 3.8469 (3.7984)  classification_loss: 2.2361 (2.2228)  loss_mask: 0.8103 (0.8451)  time: 0.1735  data: 0.0002  max mem: 6052

[05:52:43.619073] Epoch: [1]  [400/781]  eta: 0:01:06  lr: 0.000076  training_loss: 3.6990 (3.9032)  mae_loss: 3.8845 (3.8039)  classification_loss: 2.2253 (2.2226)  loss_mask: 0.7221 (0.8403)  time: 0.1725  data: 0.0002  max mem: 6052
[05:52:47.081550] Epoch: [1]  [420/781]  eta: 0:01:03  lr: 0.000077  training_loss: 3.7259 (3.8951)  mae_loss: 3.8282 (3.8054)  classification_loss: 2.2103 (2.2219)  loss_mask: 0.7392 (0.8366)  time: 0.1730  data: 0.0002  max mem: 6052
[05:52:50.526745] Epoch: [1]  [440/781]  eta: 0:00:59  lr: 0.000078  training_loss: 3.6076 (3.8843)  mae_loss: 3.8248 (3.8065)  classification_loss: 2.2087 (2.2214)  loss_mask: 0.7159 (0.8315)  time: 0.1722  data: 0.0002  max mem: 6052
[05:52:53.988513] Epoch: [1]  [460/781]  eta: 0:00:56  lr: 0.000079  training_loss: 3.6368 (3.8734)  mae_loss: 3.7226 (3.8045)  classification_loss: 2.1894 (2.2209)  loss_mask: 0.7001 (0.8263)  time: 0.1730  data: 0.0002  max mem: 6052
[05:52:57.450840] Epoch: [1]  [480/781]  eta: 0:00:52  lr: 0.000081  training_loss: 3.8598 (3.8766)  mae_loss: 3.7459 (3.8018)  classification_loss: 2.2166 (2.2216)  loss_mask: 0.8161 (0.8275)  time: 0.1730  data: 0.0002  max mem: 6052
[05:53:00.923490] Epoch: [1]  [500/781]  eta: 0:00:49  lr: 0.000082  training_loss: 3.9809 (3.8809)  mae_loss: 3.8601 (3.8068)  classification_loss: 2.2077 (2.2209)  loss_mask: 0.8988 (0.8300)  time: 0.1735  data: 0.0002  max mem: 6052
[05:53:04.389445] Epoch: [1]  [520/781]  eta: 0:00:45  lr: 0.000083  training_loss: 3.6173 (3.8723)  mae_loss: 3.8415 (3.8073)  classification_loss: 2.2210 (2.2208)  loss_mask: 0.7171 (0.8258)  time: 0.1732  data: 0.0004  max mem: 6052
[05:53:07.893565] Epoch: [1]  [540/781]  eta: 0:00:42  lr: 0.000085  training_loss: 3.6130 (3.8646)  mae_loss: 3.7415 (3.8051)  classification_loss: 2.2103 (2.2209)  loss_mask: 0.6951 (0.8218)  time: 0.1751  data: 0.0005  max mem: 6052
[05:53:11.362493] Epoch: [1]  [560/781]  eta: 0:00:38  lr: 0.000086  training_loss: 3.6128 (3.8576)  mae_loss: 3.7992 (3.8053)  classification_loss: 2.2143 (2.2209)  loss_mask: 0.7161 (0.8184)  time: 0.1733  data: 0.0002  max mem: 6052
[05:53:14.868888] Epoch: [1]  [580/781]  eta: 0:00:35  lr: 0.000087  training_loss: 3.8118 (3.8576)  mae_loss: 3.8283 (3.8067)  classification_loss: 2.2153 (2.2208)  loss_mask: 0.8162 (0.8184)  time: 0.1752  data: 0.0002  max mem: 6052
[05:53:18.335074] Epoch: [1]  [600/781]  eta: 0:00:31  lr: 0.000088  training_loss: 3.9123 (3.8596)  mae_loss: 3.8780 (3.8105)  classification_loss: 2.2087 (2.2206)  loss_mask: 0.8358 (0.8195)  time: 0.1732  data: 0.0002  max mem: 6052
[05:53:21.820143] Epoch: [1]  [620/781]  eta: 0:00:28  lr: 0.000090  training_loss: 3.7449 (3.8569)  mae_loss: 3.8770 (3.8132)  classification_loss: 2.2097 (2.2202)  loss_mask: 0.7605 (0.8183)  time: 0.1742  data: 0.0002  max mem: 6052
[05:53:25.281758] Epoch: [1]  [640/781]  eta: 0:00:24  lr: 0.000091  training_loss: 3.6493 (3.8506)  mae_loss: 3.8540 (3.8159)  classification_loss: 2.1908 (2.2197)  loss_mask: 0.7411 (0.8154)  time: 0.1730  data: 0.0003  max mem: 6052
[05:53:28.744404] Epoch: [1]  [660/781]  eta: 0:00:21  lr: 0.000092  training_loss: 3.7585 (3.8490)  mae_loss: 3.6353 (3.8103)  classification_loss: 2.2025 (2.2196)  loss_mask: 0.8077 (0.8147)  time: 0.1730  data: 0.0003  max mem: 6052
[05:53:32.200377] Epoch: [1]  [680/781]  eta: 0:00:17  lr: 0.000094  training_loss: 3.6662 (3.8444)  mae_loss: 3.7648 (3.8095)  classification_loss: 2.2285 (2.2200)  loss_mask: 0.7213 (0.8122)  time: 0.1727  data: 0.0002  max mem: 6052
[05:53:35.711263] Epoch: [1]  [700/781]  eta: 0:00:14  lr: 0.000095  training_loss: 3.7002 (3.8408)  mae_loss: 3.6881 (3.8061)  classification_loss: 2.1994 (2.2197)  loss_mask: 0.7537 (0.8105)  time: 0.1755  data: 0.0002  max mem: 6052
[05:53:39.181611] Epoch: [1]  [720/781]  eta: 0:00:10  lr: 0.000096  training_loss: 3.6339 (3.8361)  mae_loss: 3.6958 (3.8047)  classification_loss: 2.2121 (2.2198)  loss_mask: 0.7139 (0.8082)  time: 0.1734  data: 0.0003  max mem: 6052
[05:53:42.624767] Epoch: [1]  [740/781]  eta: 0:00:07  lr: 0.000097  training_loss: 3.7605 (3.8354)  mae_loss: 3.6689 (3.8010)  classification_loss: 2.2329 (2.2201)  loss_mask: 0.7703 (0.8077)  time: 0.1721  data: 0.0002  max mem: 6052
[05:53:46.084145] Epoch: [1]  [760/781]  eta: 0:00:03  lr: 0.000099  training_loss: 3.6620 (3.8321)  mae_loss: 3.7913 (3.8003)  classification_loss: 2.2045 (2.2201)  loss_mask: 0.7362 (0.8060)  time: 0.1729  data: 0.0003  max mem: 6052
[05:53:49.546026] Epoch: [1]  [780/781]  eta: 0:00:00  lr: 0.000100  training_loss: 3.6063 (3.8271)  mae_loss: 3.8411 (3.8003)  classification_loss: 2.2201 (2.2205)  loss_mask: 0.6813 (0.8033)  time: 0.1730  data: 0.0002  max mem: 6052
[05:53:49.707192] Epoch: [1] Total time: 0:02:16 (0.1744 s / it)
[05:53:49.707724] Averaged stats: lr: 0.000100  training_loss: 3.6063 (3.8271)  mae_loss: 3.8411 (3.8003)  classification_loss: 2.2201 (2.2205)  loss_mask: 0.6813 (0.8033)
[05:53:50.277184] Test:  [  0/157]  eta: 0:01:28  testing_loss: 1.9547 (1.9547)  acc1: 37.5000 (37.5000)  acc5: 85.9375 (85.9375)  time: 0.5652  data: 0.5317  max mem: 6052
[05:53:50.575656] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 2.0701 (2.0335)  acc1: 26.5625 (28.1250)  acc5: 81.2500 (81.2500)  time: 0.0783  data: 0.0485  max mem: 6052
[05:53:50.862030] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 2.0476 (2.0355)  acc1: 26.5625 (27.9018)  acc5: 81.2500 (80.6548)  time: 0.0291  data: 0.0002  max mem: 6052
[05:53:51.149923] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 2.0234 (2.0290)  acc1: 29.6875 (28.7298)  acc5: 81.2500 (81.0484)  time: 0.0286  data: 0.0002  max mem: 6052
[05:53:51.437700] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 2.0201 (2.0285)  acc1: 26.5625 (28.5061)  acc5: 81.2500 (80.9070)  time: 0.0286  data: 0.0003  max mem: 6052
[05:53:51.725878] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 2.0283 (2.0290)  acc1: 26.5625 (28.5233)  acc5: 79.6875 (81.0662)  time: 0.0286  data: 0.0003  max mem: 6052
[05:53:52.016950] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 2.0255 (2.0284)  acc1: 28.1250 (28.3555)  acc5: 82.8125 (81.3268)  time: 0.0288  data: 0.0002  max mem: 6052
[05:53:52.304030] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 2.0144 (2.0277)  acc1: 26.5625 (28.0590)  acc5: 84.3750 (81.6681)  time: 0.0287  data: 0.0002  max mem: 6052
[05:53:52.595309] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 2.0068 (2.0267)  acc1: 26.5625 (27.9514)  acc5: 84.3750 (82.1759)  time: 0.0288  data: 0.0002  max mem: 6052
[05:53:52.884077] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 2.0469 (2.0302)  acc1: 25.0000 (27.5927)  acc5: 81.2500 (81.9712)  time: 0.0289  data: 0.0002  max mem: 6052
[05:53:53.178162] Test:  [100/157]  eta: 0:00:01  testing_loss: 2.0552 (2.0308)  acc1: 25.0000 (27.5990)  acc5: 81.2500 (82.0235)  time: 0.0290  data: 0.0002  max mem: 6052
[05:53:53.469791] Test:  [110/157]  eta: 0:00:01  testing_loss: 2.0494 (2.0315)  acc1: 26.5625 (27.6042)  acc5: 81.2500 (82.1087)  time: 0.0291  data: 0.0002  max mem: 6052
[05:53:53.757523] Test:  [120/157]  eta: 0:00:01  testing_loss: 2.0098 (2.0284)  acc1: 25.0000 (27.5439)  acc5: 82.8125 (82.3089)  time: 0.0288  data: 0.0002  max mem: 6052
[05:53:54.049448] Test:  [130/157]  eta: 0:00:00  testing_loss: 2.0138 (2.0289)  acc1: 28.1250 (27.7791)  acc5: 81.2500 (82.0849)  time: 0.0288  data: 0.0002  max mem: 6052
[05:53:54.338901] Test:  [140/157]  eta: 0:00:00  testing_loss: 2.0283 (2.0278)  acc1: 29.6875 (27.8258)  acc5: 81.2500 (82.0700)  time: 0.0289  data: 0.0002  max mem: 6052
[05:53:54.620726] Test:  [150/157]  eta: 0:00:00  testing_loss: 2.0183 (2.0270)  acc1: 28.1250 (27.7835)  acc5: 82.8125 (82.2123)  time: 0.0284  data: 0.0001  max mem: 6052
[05:53:54.773255] Test:  [156/157]  eta: 0:00:00  testing_loss: 2.0060 (2.0260)  acc1: 26.5625 (27.6500)  acc5: 82.8125 (82.2200)  time: 0.0272  data: 0.0001  max mem: 6052
[05:53:54.934806] Test: Total time: 0:00:05 (0.0333 s / it)
[05:53:54.935290] * Acc@1 27.650 Acc@5 82.220 loss 2.026
[05:53:54.935582] Accuracy of the network on the 10000 test images: 27.6%
[05:53:54.935770] Max accuracy: 27.66%
[05:53:55.039579] log_dir: ./output_dir
[05:53:55.841561] Epoch: [2]  [  0/781]  eta: 0:10:24  lr: 0.000100  training_loss: 3.4250 (3.4250)  mae_loss: 3.8241 (3.8241)  classification_loss: 2.2275 (2.2275)  loss_mask: 0.5988 (0.5988)  time: 0.8001  data: 0.6045  max mem: 6052
[05:53:59.300368] Epoch: [2]  [ 20/781]  eta: 0:02:34  lr: 0.000101  training_loss: 3.5503 (3.5624)  mae_loss: 3.6533 (3.6751)  classification_loss: 2.2178 (2.2061)  loss_mask: 0.6616 (0.6782)  time: 0.1728  data: 0.0002  max mem: 6052
[05:54:02.802667] Epoch: [2]  [ 40/781]  eta: 0:02:20  lr: 0.000103  training_loss: 3.5117 (3.5505)  mae_loss: 3.5636 (3.6579)  classification_loss: 2.2124 (2.2148)  loss_mask: 0.6371 (0.6679)  time: 0.1750  data: 0.0003  max mem: 6052
[05:54:06.254344] Epoch: [2]  [ 60/781]  eta: 0:02:12  lr: 0.000104  training_loss: 3.5758 (3.5815)  mae_loss: 3.6369 (3.6533)  classification_loss: 2.2205 (2.2199)  loss_mask: 0.7074 (0.6808)  time: 0.1725  data: 0.0002  max mem: 6052
[05:54:09.749436] Epoch: [2]  [ 80/781]  eta: 0:02:07  lr: 0.000105  training_loss: 3.6864 (3.6072)  mae_loss: 3.7267 (3.6820)  classification_loss: 2.2143 (2.2234)  loss_mask: 0.7208 (0.6919)  time: 0.1747  data: 0.0002  max mem: 6052
[05:54:13.214655] Epoch: [2]  [100/781]  eta: 0:02:02  lr: 0.000106  training_loss: 3.5170 (3.6028)  mae_loss: 3.6564 (3.6848)  classification_loss: 2.2156 (2.2209)  loss_mask: 0.6358 (0.6909)  time: 0.1732  data: 0.0002  max mem: 6052
[05:54:16.672592] Epoch: [2]  [120/781]  eta: 0:01:58  lr: 0.000108  training_loss: 3.6007 (3.6213)  mae_loss: 3.5758 (3.6745)  classification_loss: 2.2223 (2.2234)  loss_mask: 0.6936 (0.6989)  time: 0.1728  data: 0.0002  max mem: 6052
[05:54:20.147417] Epoch: [2]  [140/781]  eta: 0:01:54  lr: 0.000109  training_loss: 3.5598 (3.6216)  mae_loss: 3.7427 (3.6882)  classification_loss: 2.2099 (2.2214)  loss_mask: 0.6839 (0.7001)  time: 0.1737  data: 0.0002  max mem: 6052
[05:54:23.607108] Epoch: [2]  [160/781]  eta: 0:01:50  lr: 0.000110  training_loss: 3.7290 (3.6315)  mae_loss: 3.6422 (3.6853)  classification_loss: 2.2025 (2.2198)  loss_mask: 0.7604 (0.7058)  time: 0.1729  data: 0.0002  max mem: 6052
[05:54:27.062119] Epoch: [2]  [180/781]  eta: 0:01:46  lr: 0.000112  training_loss: 3.5969 (3.6344)  mae_loss: 3.7768 (3.6971)  classification_loss: 2.2083 (2.2198)  loss_mask: 0.7045 (0.7073)  time: 0.1727  data: 0.0002  max mem: 6052
[05:54:30.527281] Epoch: [2]  [200/781]  eta: 0:01:42  lr: 0.000113  training_loss: 3.4990 (3.6220)  mae_loss: 3.7265 (3.7036)  classification_loss: 2.2121 (2.2179)  loss_mask: 0.6548 (0.7020)  time: 0.1732  data: 0.0002  max mem: 6052
[05:54:33.988883] Epoch: [2]  [220/781]  eta: 0:01:38  lr: 0.000114  training_loss: 3.4006 (3.6070)  mae_loss: 3.5558 (3.6961)  classification_loss: 2.1943 (2.2163)  loss_mask: 0.6040 (0.6954)  time: 0.1730  data: 0.0002  max mem: 6052
[05:54:37.441811] Epoch: [2]  [240/781]  eta: 0:01:35  lr: 0.000115  training_loss: 3.7122 (3.6111)  mae_loss: 3.5993 (3.6887)  classification_loss: 2.2167 (2.2162)  loss_mask: 0.7429 (0.6975)  time: 0.1726  data: 0.0004  max mem: 6052
[05:54:40.895336] Epoch: [2]  [260/781]  eta: 0:01:31  lr: 0.000117  training_loss: 3.6287 (3.6141)  mae_loss: 3.7719 (3.6940)  classification_loss: 2.1885 (2.2156)  loss_mask: 0.7206 (0.6992)  time: 0.1726  data: 0.0002  max mem: 6052
[05:54:44.375223] Epoch: [2]  [280/781]  eta: 0:01:27  lr: 0.000118  training_loss: 3.6010 (3.6136)  mae_loss: 3.8038 (3.7024)  classification_loss: 2.2167 (2.2160)  loss_mask: 0.6925 (0.6988)  time: 0.1739  data: 0.0002  max mem: 6052
[05:54:47.843960] Epoch: [2]  [300/781]  eta: 0:01:24  lr: 0.000119  training_loss: 3.6626 (3.6198)  mae_loss: 3.7458 (3.7063)  classification_loss: 2.2133 (2.2159)  loss_mask: 0.7111 (0.7020)  time: 0.1733  data: 0.0002  max mem: 6052
[05:54:51.300229] Epoch: [2]  [320/781]  eta: 0:01:20  lr: 0.000120  training_loss: 3.6069 (3.6192)  mae_loss: 3.8398 (3.7137)  classification_loss: 2.2080 (2.2158)  loss_mask: 0.6670 (0.7017)  time: 0.1727  data: 0.0002  max mem: 6052
[05:54:54.768366] Epoch: [2]  [340/781]  eta: 0:01:17  lr: 0.000122  training_loss: 3.5479 (3.6149)  mae_loss: 3.7091 (3.7145)  classification_loss: 2.2150 (2.2160)  loss_mask: 0.6741 (0.6995)  time: 0.1733  data: 0.0003  max mem: 6052
[05:54:58.219684] Epoch: [2]  [360/781]  eta: 0:01:13  lr: 0.000123  training_loss: 3.4698 (3.6079)  mae_loss: 3.6554 (3.7135)  classification_loss: 2.2156 (2.2157)  loss_mask: 0.6152 (0.6961)  time: 0.1725  data: 0.0003  max mem: 6052
[05:55:01.679501] Epoch: [2]  [380/781]  eta: 0:01:10  lr: 0.000124  training_loss: 3.6941 (3.6132)  mae_loss: 3.8102 (3.7185)  classification_loss: 2.2152 (2.2162)  loss_mask: 0.7339 (0.6985)  time: 0.1729  data: 0.0002  max mem: 6052
[05:55:05.117669] Epoch: [2]  [400/781]  eta: 0:01:06  lr: 0.000126  training_loss: 3.6313 (3.6127)  mae_loss: 3.8558 (3.7271)  classification_loss: 2.1765 (2.2156)  loss_mask: 0.7005 (0.6986)  time: 0.1718  data: 0.0002  max mem: 6052
[05:55:08.568295] Epoch: [2]  [420/781]  eta: 0:01:03  lr: 0.000127  training_loss: 3.4552 (3.6051)  mae_loss: 3.8529 (3.7324)  classification_loss: 2.2070 (2.2155)  loss_mask: 0.6036 (0.6948)  time: 0.1724  data: 0.0002  max mem: 6052
[05:55:12.017246] Epoch: [2]  [440/781]  eta: 0:00:59  lr: 0.000128  training_loss: 3.5873 (3.6084)  mae_loss: 3.7500 (3.7343)  classification_loss: 2.1990 (2.2153)  loss_mask: 0.6937 (0.6966)  time: 0.1723  data: 0.0002  max mem: 6052
[05:55:15.465663] Epoch: [2]  [460/781]  eta: 0:00:55  lr: 0.000129  training_loss: 3.4473 (3.6019)  mae_loss: 3.7469 (3.7343)  classification_loss: 2.1937 (2.2140)  loss_mask: 0.6259 (0.6940)  time: 0.1723  data: 0.0002  max mem: 6052
[05:55:18.916727] Epoch: [2]  [480/781]  eta: 0:00:52  lr: 0.000131  training_loss: 3.5543 (3.6007)  mae_loss: 3.6891 (3.7330)  classification_loss: 2.2355 (2.2149)  loss_mask: 0.6440 (0.6929)  time: 0.1724  data: 0.0002  max mem: 6052
[05:55:22.371592] Epoch: [2]  [500/781]  eta: 0:00:48  lr: 0.000132  training_loss: 3.6088 (3.6019)  mae_loss: 3.6887 (3.7308)  classification_loss: 2.1960 (2.2142)  loss_mask: 0.7072 (0.6938)  time: 0.1727  data: 0.0002  max mem: 6052
[05:55:25.878038] Epoch: [2]  [520/781]  eta: 0:00:45  lr: 0.000133  training_loss: 3.6370 (3.6052)  mae_loss: 3.7966 (3.7346)  classification_loss: 2.2058 (2.2141)  loss_mask: 0.7207 (0.6955)  time: 0.1752  data: 0.0002  max mem: 6052
[05:55:29.346028] Epoch: [2]  [540/781]  eta: 0:00:41  lr: 0.000135  training_loss: 3.5596 (3.6034)  mae_loss: 3.7622 (3.7372)  classification_loss: 2.2385 (2.2143)  loss_mask: 0.6763 (0.6946)  time: 0.1733  data: 0.0002  max mem: 6052
[05:55:32.793740] Epoch: [2]  [560/781]  eta: 0:00:38  lr: 0.000136  training_loss: 3.4536 (3.5997)  mae_loss: 3.7492 (3.7381)  classification_loss: 2.2040 (2.2140)  loss_mask: 0.6161 (0.6928)  time: 0.1723  data: 0.0002  max mem: 6052
[05:55:36.270024] Epoch: [2]  [580/781]  eta: 0:00:35  lr: 0.000137  training_loss: 3.5072 (3.5958)  mae_loss: 3.7286 (3.7377)  classification_loss: 2.1941 (2.2134)  loss_mask: 0.6305 (0.6912)  time: 0.1737  data: 0.0002  max mem: 6052
[05:55:39.745554] Epoch: [2]  [600/781]  eta: 0:00:31  lr: 0.000138  training_loss: 3.4739 (3.5927)  mae_loss: 3.6952 (3.7376)  classification_loss: 2.2069 (2.2134)  loss_mask: 0.6393 (0.6896)  time: 0.1737  data: 0.0002  max mem: 6052
[05:55:43.277497] Epoch: [2]  [620/781]  eta: 0:00:28  lr: 0.000140  training_loss: 3.3925 (3.5871)  mae_loss: 3.7938 (3.7392)  classification_loss: 2.2084 (2.2134)  loss_mask: 0.5797 (0.6868)  time: 0.1765  data: 0.0003  max mem: 6052
[05:55:46.734186] Epoch: [2]  [640/781]  eta: 0:00:24  lr: 0.000141  training_loss: 3.4845 (3.5858)  mae_loss: 3.8186 (3.7412)  classification_loss: 2.1936 (2.2132)  loss_mask: 0.6633 (0.6863)  time: 0.1727  data: 0.0002  max mem: 6052
[05:55:50.206334] Epoch: [2]  [660/781]  eta: 0:00:21  lr: 0.000142  training_loss: 3.4289 (3.5828)  mae_loss: 3.8832 (3.7452)  classification_loss: 2.1909 (2.2127)  loss_mask: 0.6329 (0.6850)  time: 0.1735  data: 0.0002  max mem: 6052
[05:55:53.659378] Epoch: [2]  [680/781]  eta: 0:00:17  lr: 0.000144  training_loss: 3.4764 (3.5812)  mae_loss: 3.7599 (3.7456)  classification_loss: 2.1986 (2.2126)  loss_mask: 0.6419 (0.6843)  time: 0.1725  data: 0.0002  max mem: 6052
[05:55:57.149344] Epoch: [2]  [700/781]  eta: 0:00:14  lr: 0.000145  training_loss: 3.6759 (3.5841)  mae_loss: 3.8186 (3.7475)  classification_loss: 2.1864 (2.2121)  loss_mask: 0.7210 (0.6860)  time: 0.1743  data: 0.0002  max mem: 6052
[05:56:00.598531] Epoch: [2]  [720/781]  eta: 0:00:10  lr: 0.000146  training_loss: 3.6678 (3.5873)  mae_loss: 3.9309 (3.7526)  classification_loss: 2.2078 (2.2122)  loss_mask: 0.7277 (0.6876)  time: 0.1724  data: 0.0002  max mem: 6052
[05:56:04.038860] Epoch: [2]  [740/781]  eta: 0:00:07  lr: 0.000147  training_loss: 3.4258 (3.5847)  mae_loss: 3.9564 (3.7572)  classification_loss: 2.1985 (2.2121)  loss_mask: 0.6240 (0.6863)  time: 0.1719  data: 0.0002  max mem: 6052
[05:56:07.517655] Epoch: [2]  [760/781]  eta: 0:00:03  lr: 0.000149  training_loss: 3.5952 (3.5843)  mae_loss: 3.6915 (3.7561)  classification_loss: 2.1944 (2.2121)  loss_mask: 0.6959 (0.6861)  time: 0.1738  data: 0.0002  max mem: 6052
[05:56:10.944994] Epoch: [2]  [780/781]  eta: 0:00:00  lr: 0.000150  training_loss: 3.6025 (3.5861)  mae_loss: 3.7752 (3.7569)  classification_loss: 2.1910 (2.2118)  loss_mask: 0.7113 (0.6871)  time: 0.1713  data: 0.0002  max mem: 6052
[05:56:11.115409] Epoch: [2] Total time: 0:02:16 (0.1742 s / it)
[05:56:11.116046] Averaged stats: lr: 0.000150  training_loss: 3.6025 (3.5861)  mae_loss: 3.7752 (3.7569)  classification_loss: 2.1910 (2.2118)  loss_mask: 0.7113 (0.6871)
[05:56:11.853465] Test:  [  0/157]  eta: 0:01:55  testing_loss: 1.9056 (1.9056)  acc1: 45.3125 (45.3125)  acc5: 85.9375 (85.9375)  time: 0.7331  data: 0.7010  max mem: 6052
[05:56:12.137153] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 2.0120 (1.9946)  acc1: 28.1250 (31.9602)  acc5: 81.2500 (82.2443)  time: 0.0922  data: 0.0639  max mem: 6052
[05:56:12.419674] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.9910 (1.9980)  acc1: 28.1250 (31.3244)  acc5: 81.2500 (81.9196)  time: 0.0281  data: 0.0002  max mem: 6052
[05:56:12.702950] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.9659 (1.9857)  acc1: 31.2500 (32.1573)  acc5: 82.8125 (82.6109)  time: 0.0282  data: 0.0002  max mem: 6052
[05:56:12.987007] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.9729 (1.9854)  acc1: 32.8125 (32.3171)  acc5: 82.8125 (82.2409)  time: 0.0282  data: 0.0002  max mem: 6052
[05:56:13.269080] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.9850 (1.9859)  acc1: 29.6875 (32.0159)  acc5: 81.2500 (82.4755)  time: 0.0282  data: 0.0003  max mem: 6052
[05:56:13.551376] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.9773 (1.9832)  acc1: 29.6875 (32.1209)  acc5: 84.3750 (82.8893)  time: 0.0281  data: 0.0004  max mem: 6052
[05:56:13.834047] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.9646 (1.9832)  acc1: 29.6875 (31.6461)  acc5: 84.3750 (83.1646)  time: 0.0281  data: 0.0003  max mem: 6052
[05:56:14.117897] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.9670 (1.9808)  acc1: 29.6875 (31.6937)  acc5: 84.3750 (83.5455)  time: 0.0282  data: 0.0002  max mem: 6052
[05:56:14.401948] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.9982 (1.9835)  acc1: 31.2500 (31.4389)  acc5: 82.8125 (83.4478)  time: 0.0282  data: 0.0002  max mem: 6052
[05:56:14.686017] Test:  [100/157]  eta: 0:00:02  testing_loss: 2.0139 (1.9840)  acc1: 28.1250 (31.4202)  acc5: 82.8125 (83.5396)  time: 0.0283  data: 0.0002  max mem: 6052
[05:56:14.969651] Test:  [110/157]  eta: 0:00:01  testing_loss: 2.0012 (1.9851)  acc1: 31.2500 (31.5597)  acc5: 82.8125 (83.4600)  time: 0.0283  data: 0.0002  max mem: 6052
[05:56:15.254493] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.9556 (1.9809)  acc1: 31.2500 (31.6116)  acc5: 82.8125 (83.7552)  time: 0.0283  data: 0.0002  max mem: 6052
[05:56:15.539740] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.9688 (1.9818)  acc1: 34.3750 (31.7629)  acc5: 84.3750 (83.6594)  time: 0.0284  data: 0.0002  max mem: 6052
[05:56:15.824979] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.9930 (1.9814)  acc1: 34.3750 (31.8152)  acc5: 82.8125 (83.6215)  time: 0.0284  data: 0.0003  max mem: 6052
[05:56:16.107338] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.9913 (1.9802)  acc1: 31.2500 (31.8398)  acc5: 82.8125 (83.6817)  time: 0.0282  data: 0.0002  max mem: 6052
[05:56:16.260189] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.9515 (1.9794)  acc1: 31.2500 (31.7000)  acc5: 84.3750 (83.7400)  time: 0.0273  data: 0.0001  max mem: 6052
[05:56:16.435544] Test: Total time: 0:00:05 (0.0339 s / it)
[05:56:16.436389] * Acc@1 31.700 Acc@5 83.740 loss 1.979
[05:56:16.436762] Accuracy of the network on the 10000 test images: 31.7%
[05:56:16.437003] Max accuracy: 31.70%
[05:56:16.607398] log_dir: ./output_dir
[05:56:17.431098] Epoch: [3]  [  0/781]  eta: 0:10:41  lr: 0.000150  training_loss: 3.5074 (3.5074)  mae_loss: 3.7694 (3.7694)  classification_loss: 2.1095 (2.1095)  loss_mask: 0.6990 (0.6990)  time: 0.8219  data: 0.6163  max mem: 6052
[05:56:20.903046] Epoch: [3]  [ 20/781]  eta: 0:02:35  lr: 0.000151  training_loss: 3.5468 (3.5563)  mae_loss: 3.8248 (3.8392)  classification_loss: 2.1807 (2.1832)  loss_mask: 0.6884 (0.6865)  time: 0.1735  data: 0.0002  max mem: 6052
[05:56:24.355665] Epoch: [3]  [ 40/781]  eta: 0:02:19  lr: 0.000153  training_loss: 3.6175 (3.5849)  mae_loss: 3.8206 (3.8488)  classification_loss: 2.2379 (2.2049)  loss_mask: 0.6873 (0.6900)  time: 0.1726  data: 0.0002  max mem: 6052
[05:56:27.823288] Epoch: [3]  [ 60/781]  eta: 0:02:12  lr: 0.000154  training_loss: 3.4107 (3.5416)  mae_loss: 3.8913 (3.8652)  classification_loss: 2.2023 (2.2070)  loss_mask: 0.5959 (0.6673)  time: 0.1733  data: 0.0002  max mem: 6052
[05:56:31.280532] Epoch: [3]  [ 80/781]  eta: 0:02:06  lr: 0.000155  training_loss: 3.5303 (3.5558)  mae_loss: 3.7396 (3.8406)  classification_loss: 2.2352 (2.2150)  loss_mask: 0.6546 (0.6704)  time: 0.1728  data: 0.0003  max mem: 6052
[05:56:34.731181] Epoch: [3]  [100/781]  eta: 0:02:02  lr: 0.000156  training_loss: 3.6456 (3.5615)  mae_loss: 3.7259 (3.8179)  classification_loss: 2.2423 (2.2172)  loss_mask: 0.6997 (0.6721)  time: 0.1724  data: 0.0002  max mem: 6052
[05:56:38.173563] Epoch: [3]  [120/781]  eta: 0:01:57  lr: 0.000158  training_loss: 3.5517 (3.5705)  mae_loss: 3.7587 (3.8095)  classification_loss: 2.2054 (2.2160)  loss_mask: 0.6813 (0.6772)  time: 0.1720  data: 0.0003  max mem: 6052
[05:56:41.621994] Epoch: [3]  [140/781]  eta: 0:01:53  lr: 0.000159  training_loss: 3.3869 (3.5479)  mae_loss: 3.9900 (3.8258)  classification_loss: 2.2130 (2.2159)  loss_mask: 0.5765 (0.6660)  time: 0.1723  data: 0.0002  max mem: 6052
[05:56:45.071214] Epoch: [3]  [160/781]  eta: 0:01:49  lr: 0.000160  training_loss: 3.3402 (3.5231)  mae_loss: 3.6504 (3.8087)  classification_loss: 2.1958 (2.2149)  loss_mask: 0.5657 (0.6541)  time: 0.1724  data: 0.0002  max mem: 6052
[05:56:48.501313] Epoch: [3]  [180/781]  eta: 0:01:45  lr: 0.000162  training_loss: 3.4151 (3.5158)  mae_loss: 3.7550 (3.8055)  classification_loss: 2.2112 (2.2143)  loss_mask: 0.6131 (0.6508)  time: 0.1714  data: 0.0002  max mem: 6052
[05:56:51.940246] Epoch: [3]  [200/781]  eta: 0:01:42  lr: 0.000163  training_loss: 3.4384 (3.5141)  mae_loss: 3.8321 (3.8086)  classification_loss: 2.2023 (2.2126)  loss_mask: 0.6249 (0.6508)  time: 0.1719  data: 0.0002  max mem: 6052
[05:56:55.433343] Epoch: [3]  [220/781]  eta: 0:01:38  lr: 0.000164  training_loss: 3.4976 (3.5156)  mae_loss: 3.8151 (3.8120)  classification_loss: 2.2103 (2.2113)  loss_mask: 0.6440 (0.6522)  time: 0.1746  data: 0.0003  max mem: 6052
[05:56:58.877632] Epoch: [3]  [240/781]  eta: 0:01:34  lr: 0.000165  training_loss: 3.4327 (3.5104)  mae_loss: 3.7244 (3.8104)  classification_loss: 2.2159 (2.2116)  loss_mask: 0.6085 (0.6494)  time: 0.1721  data: 0.0002  max mem: 6052
[05:57:02.336605] Epoch: [3]  [260/781]  eta: 0:01:31  lr: 0.000167  training_loss: 3.3918 (3.5031)  mae_loss: 3.8448 (3.8122)  classification_loss: 2.1997 (2.2105)  loss_mask: 0.6188 (0.6463)  time: 0.1729  data: 0.0003  max mem: 6052
[05:57:05.820882] Epoch: [3]  [280/781]  eta: 0:01:27  lr: 0.000168  training_loss: 3.5955 (3.5167)  mae_loss: 3.8247 (3.8127)  classification_loss: 2.2419 (2.2125)  loss_mask: 0.6914 (0.6521)  time: 0.1741  data: 0.0002  max mem: 6052
[05:57:09.304132] Epoch: [3]  [300/781]  eta: 0:01:24  lr: 0.000169  training_loss: 3.5046 (3.5185)  mae_loss: 3.8657 (3.8136)  classification_loss: 2.2130 (2.2128)  loss_mask: 0.6522 (0.6528)  time: 0.1740  data: 0.0002  max mem: 6052
[05:57:12.764302] Epoch: [3]  [320/781]  eta: 0:01:20  lr: 0.000170  training_loss: 3.4619 (3.5155)  mae_loss: 3.7736 (3.8099)  classification_loss: 2.1974 (2.2117)  loss_mask: 0.6229 (0.6519)  time: 0.1729  data: 0.0003  max mem: 6052
[05:57:16.204508] Epoch: [3]  [340/781]  eta: 0:01:17  lr: 0.000172  training_loss: 3.4404 (3.5156)  mae_loss: 3.9183 (3.8172)  classification_loss: 2.2172 (2.2114)  loss_mask: 0.6074 (0.6521)  time: 0.1719  data: 0.0002  max mem: 6052
[05:57:19.670711] Epoch: [3]  [360/781]  eta: 0:01:13  lr: 0.000173  training_loss: 3.3876 (3.5119)  mae_loss: 3.7993 (3.8198)  classification_loss: 2.2057 (2.2112)  loss_mask: 0.5935 (0.6503)  time: 0.1732  data: 0.0002  max mem: 6052
[05:57:23.126203] Epoch: [3]  [380/781]  eta: 0:01:09  lr: 0.000174  training_loss: 3.4988 (3.5152)  mae_loss: 3.8220 (3.8225)  classification_loss: 2.2137 (2.2109)  loss_mask: 0.6611 (0.6521)  time: 0.1727  data: 0.0002  max mem: 6052
[05:57:26.573348] Epoch: [3]  [400/781]  eta: 0:01:06  lr: 0.000176  training_loss: 3.4637 (3.5131)  mae_loss: 3.8309 (3.8233)  classification_loss: 2.2267 (2.2121)  loss_mask: 0.5796 (0.6505)  time: 0.1723  data: 0.0002  max mem: 6052

[05:57:30.050203] Epoch: [3]  [420/781]  eta: 0:01:02  lr: 0.000177  training_loss: 3.5042 (3.5119)  mae_loss: 3.8685 (3.8243)  classification_loss: 2.1959 (2.2118)  loss_mask: 0.6463 (0.6501)  time: 0.1738  data: 0.0002  max mem: 6052
[05:57:33.545740] Epoch: [3]  [440/781]  eta: 0:00:59  lr: 0.000178  training_loss: 3.5114 (3.5122)  mae_loss: 4.0219 (3.8326)  classification_loss: 2.2037 (2.2111)  loss_mask: 0.6476 (0.6506)  time: 0.1747  data: 0.0002  max mem: 6052
[05:57:37.006825] Epoch: [3]  [460/781]  eta: 0:00:55  lr: 0.000179  training_loss: 3.3586 (3.5065)  mae_loss: 3.8140 (3.8324)  classification_loss: 2.2032 (2.2107)  loss_mask: 0.5700 (0.6479)  time: 0.1729  data: 0.0002  max mem: 6052
[05:57:40.460768] Epoch: [3]  [480/781]  eta: 0:00:52  lr: 0.000181  training_loss: 3.4430 (3.5040)  mae_loss: 3.8044 (3.8310)  classification_loss: 2.2138 (2.2105)  loss_mask: 0.6037 (0.6467)  time: 0.1726  data: 0.0002  max mem: 6052
[05:57:43.918232] Epoch: [3]  [500/781]  eta: 0:00:48  lr: 0.000182  training_loss: 3.3866 (3.4994)  mae_loss: 3.8546 (3.8327)  classification_loss: 2.1931 (2.2098)  loss_mask: 0.5805 (0.6448)  time: 0.1728  data: 0.0003  max mem: 6052
[05:57:47.367977] Epoch: [3]  [520/781]  eta: 0:00:45  lr: 0.000183  training_loss: 3.3949 (3.4955)  mae_loss: 3.9336 (3.8356)  classification_loss: 2.1950 (2.2096)  loss_mask: 0.5939 (0.6430)  time: 0.1724  data: 0.0002  max mem: 6052
[05:57:50.835739] Epoch: [3]  [540/781]  eta: 0:00:41  lr: 0.000185  training_loss: 3.3731 (3.4928)  mae_loss: 3.7922 (3.8352)  classification_loss: 2.2145 (2.2103)  loss_mask: 0.5834 (0.6413)  time: 0.1733  data: 0.0002  max mem: 6052
[05:57:54.301972] Epoch: [3]  [560/781]  eta: 0:00:38  lr: 0.000186  training_loss: 3.3145 (3.4870)  mae_loss: 3.7259 (3.8315)  classification_loss: 2.1851 (2.2098)  loss_mask: 0.5423 (0.6386)  time: 0.1732  data: 0.0002  max mem: 6052
[05:57:57.760406] Epoch: [3]  [580/781]  eta: 0:00:34  lr: 0.000187  training_loss: 3.3560 (3.4838)  mae_loss: 3.6841 (3.8285)  classification_loss: 2.2132 (2.2098)  loss_mask: 0.5622 (0.6370)  time: 0.1728  data: 0.0003  max mem: 6052
[05:58:01.203266] Epoch: [3]  [600/781]  eta: 0:00:31  lr: 0.000188  training_loss: 3.3925 (3.4827)  mae_loss: 3.8058 (3.8268)  classification_loss: 2.1920 (2.2095)  loss_mask: 0.5956 (0.6366)  time: 0.1721  data: 0.0002  max mem: 6052
[05:58:04.648910] Epoch: [3]  [620/781]  eta: 0:00:27  lr: 0.000190  training_loss: 3.2773 (3.4777)  mae_loss: 3.7958 (3.8259)  classification_loss: 2.1932 (2.2094)  loss_mask: 0.5432 (0.6341)  time: 0.1722  data: 0.0004  max mem: 6052
[05:58:08.121733] Epoch: [3]  [640/781]  eta: 0:00:24  lr: 0.000191  training_loss: 3.3913 (3.4753)  mae_loss: 3.7950 (3.8258)  classification_loss: 2.2003 (2.2094)  loss_mask: 0.5901 (0.6330)  time: 0.1731  data: 0.0002  max mem: 6052
[05:58:11.570628] Epoch: [3]  [660/781]  eta: 0:00:21  lr: 0.000192  training_loss: 3.2779 (3.4701)  mae_loss: 3.7498 (3.8230)  classification_loss: 2.1886 (2.2090)  loss_mask: 0.5644 (0.6305)  time: 0.1723  data: 0.0002  max mem: 6052
[05:58:15.033962] Epoch: [3]  [680/781]  eta: 0:00:17  lr: 0.000194  training_loss: 3.4049 (3.4693)  mae_loss: 3.6974 (3.8204)  classification_loss: 2.1763 (2.2083)  loss_mask: 0.6151 (0.6305)  time: 0.1731  data: 0.0003  max mem: 6052
[05:58:18.506684] Epoch: [3]  [700/781]  eta: 0:00:14  lr: 0.000195  training_loss: 3.5370 (3.4722)  mae_loss: 3.8722 (3.8217)  classification_loss: 2.1827 (2.2077)  loss_mask: 0.6804 (0.6322)  time: 0.1735  data: 0.0002  max mem: 6052
[05:58:21.960852] Epoch: [3]  [720/781]  eta: 0:00:10  lr: 0.000196  training_loss: 3.4671 (3.4735)  mae_loss: 3.8550 (3.8235)  classification_loss: 2.1908 (2.2075)  loss_mask: 0.6407 (0.6330)  time: 0.1726  data: 0.0002  max mem: 6052
[05:58:25.417030] Epoch: [3]  [740/781]  eta: 0:00:07  lr: 0.000197  training_loss: 3.3439 (3.4709)  mae_loss: 3.8847 (3.8255)  classification_loss: 2.1939 (2.2072)  loss_mask: 0.5713 (0.6319)  time: 0.1727  data: 0.0003  max mem: 6052
[05:58:28.880800] Epoch: [3]  [760/781]  eta: 0:00:03  lr: 0.000199  training_loss: 3.3896 (3.4685)  mae_loss: 3.8366 (3.8258)  classification_loss: 2.1813 (2.2066)  loss_mask: 0.5829 (0.6309)  time: 0.1731  data: 0.0003  max mem: 6052
[05:58:32.337216] Epoch: [3]  [780/781]  eta: 0:00:00  lr: 0.000200  training_loss: 3.3887 (3.4671)  mae_loss: 3.8520 (3.8266)  classification_loss: 2.1784 (2.2064)  loss_mask: 0.5992 (0.6304)  time: 0.1727  data: 0.0003  max mem: 6052
[05:58:32.490816] Epoch: [3] Total time: 0:02:15 (0.1740 s / it)
[05:58:32.492243] Averaged stats: lr: 0.000200  training_loss: 3.3887 (3.4671)  mae_loss: 3.8520 (3.8266)  classification_loss: 2.1784 (2.2064)  loss_mask: 0.5992 (0.6304)
[05:58:33.189974] Test:  [  0/157]  eta: 0:01:48  testing_loss: 1.8622 (1.8622)  acc1: 39.0625 (39.0625)  acc5: 85.9375 (85.9375)  time: 0.6939  data: 0.6626  max mem: 6052
[05:58:33.476118] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.9988 (1.9798)  acc1: 29.6875 (30.6818)  acc5: 81.2500 (81.8182)  time: 0.0889  data: 0.0604  max mem: 6052
[05:58:33.759854] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.9837 (1.9810)  acc1: 32.8125 (31.9940)  acc5: 81.2500 (81.1756)  time: 0.0283  data: 0.0002  max mem: 6052
[05:58:34.044890] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.9540 (1.9722)  acc1: 32.8125 (32.3589)  acc5: 81.2500 (81.6532)  time: 0.0283  data: 0.0002  max mem: 6052
[05:58:34.341549] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.9573 (1.9751)  acc1: 32.8125 (32.2409)  acc5: 81.2500 (81.3643)  time: 0.0289  data: 0.0002  max mem: 6052
[05:58:34.630946] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.9750 (1.9759)  acc1: 29.6875 (31.8321)  acc5: 81.2500 (81.4338)  time: 0.0291  data: 0.0002  max mem: 6052
[05:58:34.919004] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.9544 (1.9726)  acc1: 29.6875 (31.5574)  acc5: 82.8125 (81.8648)  time: 0.0287  data: 0.0002  max mem: 6052
[05:58:35.208827] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.9505 (1.9742)  acc1: 29.6875 (31.4701)  acc5: 84.3750 (81.9982)  time: 0.0288  data: 0.0002  max mem: 6052
[05:58:35.492632] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.9682 (1.9713)  acc1: 31.2500 (31.6551)  acc5: 85.9375 (82.3110)  time: 0.0286  data: 0.0002  max mem: 6052
[05:58:35.776624] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.9674 (1.9723)  acc1: 32.8125 (31.5934)  acc5: 81.2500 (82.4176)  time: 0.0283  data: 0.0002  max mem: 6052
[05:58:36.060482] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.9709 (1.9741)  acc1: 31.2500 (31.6832)  acc5: 79.6875 (82.2092)  time: 0.0283  data: 0.0002  max mem: 6052
[05:58:36.343732] Test:  [110/157]  eta: 0:00:01  testing_loss: 2.0002 (1.9757)  acc1: 31.2500 (31.7286)  acc5: 81.2500 (82.0805)  time: 0.0282  data: 0.0002  max mem: 6052
[05:58:36.628049] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.9547 (1.9720)  acc1: 32.8125 (31.8827)  acc5: 82.8125 (82.3218)  time: 0.0283  data: 0.0002  max mem: 6052
[05:58:36.912533] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.9546 (1.9728)  acc1: 31.2500 (31.8583)  acc5: 84.3750 (82.3235)  time: 0.0283  data: 0.0002  max mem: 6052
[05:58:37.195534] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.9954 (1.9728)  acc1: 31.2500 (31.9481)  acc5: 81.2500 (82.3692)  time: 0.0283  data: 0.0001  max mem: 6052
[05:58:37.476435] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.9592 (1.9713)  acc1: 31.2500 (31.8709)  acc5: 82.8125 (82.3158)  time: 0.0281  data: 0.0001  max mem: 6052
[05:58:37.627701] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.9552 (1.9708)  acc1: 29.6875 (31.6900)  acc5: 82.8125 (82.3600)  time: 0.0271  data: 0.0001  max mem: 6052
[05:58:37.804384] Test: Total time: 0:00:05 (0.0338 s / it)
[05:58:37.804829] * Acc@1 31.690 Acc@5 82.360 loss 1.971
[05:58:37.805157] Accuracy of the network on the 10000 test images: 31.7%
[05:58:37.805341] Max accuracy: 31.70%
[05:58:37.993850] log_dir: ./output_dir
[05:58:38.941330] Epoch: [4]  [  0/781]  eta: 0:12:18  lr: 0.000200  training_loss: 3.3153 (3.3153)  mae_loss: 3.9729 (3.9729)  classification_loss: 2.1877 (2.1877)  loss_mask: 0.5638 (0.5638)  time: 0.9454  data: 0.7262  max mem: 6052
[05:58:42.386685] Epoch: [4]  [ 20/781]  eta: 0:02:39  lr: 0.000201  training_loss: 3.3929 (3.3539)  mae_loss: 3.8675 (3.8712)  classification_loss: 2.2004 (2.1896)  loss_mask: 0.6025 (0.5821)  time: 0.1721  data: 0.0002  max mem: 6052
[05:58:45.870175] Epoch: [4]  [ 40/781]  eta: 0:02:22  lr: 0.000203  training_loss: 3.3887 (3.3702)  mae_loss: 3.8876 (3.8830)  classification_loss: 2.1917 (2.1944)  loss_mask: 0.5900 (0.5879)  time: 0.1741  data: 0.0002  max mem: 6052
[05:58:49.361624] Epoch: [4]  [ 60/781]  eta: 0:02:14  lr: 0.000204  training_loss: 3.4291 (3.3796)  mae_loss: 3.8905 (3.8827)  classification_loss: 2.1773 (2.1919)  loss_mask: 0.5923 (0.5938)  time: 0.1745  data: 0.0003  max mem: 6052
[05:58:52.810894] Epoch: [4]  [ 80/781]  eta: 0:02:08  lr: 0.000205  training_loss: 3.2718 (3.3455)  mae_loss: 3.7790 (3.8726)  classification_loss: 2.2103 (2.1980)  loss_mask: 0.5132 (0.5738)  time: 0.1724  data: 0.0002  max mem: 6052
[05:58:56.302004] Epoch: [4]  [100/781]  eta: 0:02:03  lr: 0.000206  training_loss: 3.3007 (3.3469)  mae_loss: 3.7767 (3.8563)  classification_loss: 2.2047 (2.1995)  loss_mask: 0.5545 (0.5737)  time: 0.1745  data: 0.0002  max mem: 6052
[05:58:59.764363] Epoch: [4]  [120/781]  eta: 0:01:58  lr: 0.000208  training_loss: 3.6965 (3.4090)  mae_loss: 3.9495 (3.8779)  classification_loss: 2.1861 (2.1995)  loss_mask: 0.7284 (0.6048)  time: 0.1730  data: 0.0002  max mem: 6052
[05:59:03.207513] Epoch: [4]  [140/781]  eta: 0:01:54  lr: 0.000209  training_loss: 3.3058 (3.4029)  mae_loss: 3.9547 (3.8881)  classification_loss: 2.1730 (2.1963)  loss_mask: 0.5778 (0.6033)  time: 0.1721  data: 0.0002  max mem: 6052
[05:59:06.664979] Epoch: [4]  [160/781]  eta: 0:01:50  lr: 0.000210  training_loss: 3.3618 (3.4061)  mae_loss: 3.8520 (3.8876)  classification_loss: 2.1543 (2.1933)  loss_mask: 0.6204 (0.6064)  time: 0.1728  data: 0.0002  max mem: 6052
[05:59:10.140214] Epoch: [4]  [180/781]  eta: 0:01:46  lr: 0.000212  training_loss: 3.5650 (3.4274)  mae_loss: 3.8499 (3.8859)  classification_loss: 2.2068 (2.1957)  loss_mask: 0.6612 (0.6159)  time: 0.1736  data: 0.0003  max mem: 6052
[05:59:13.586648] Epoch: [4]  [200/781]  eta: 0:01:42  lr: 0.000213  training_loss: 3.4547 (3.4347)  mae_loss: 3.9587 (3.8960)  classification_loss: 2.1925 (2.1968)  loss_mask: 0.6316 (0.6189)  time: 0.1722  data: 0.0003  max mem: 6052
[05:59:17.063601] Epoch: [4]  [220/781]  eta: 0:01:39  lr: 0.000214  training_loss: 3.2756 (3.4187)  mae_loss: 3.8943 (3.8978)  classification_loss: 2.1840 (2.1956)  loss_mask: 0.5578 (0.6116)  time: 0.1738  data: 0.0006  max mem: 6052
[05:59:20.521166] Epoch: [4]  [240/781]  eta: 0:01:35  lr: 0.000215  training_loss: 3.2298 (3.4028)  mae_loss: 3.8895 (3.8973)  classification_loss: 2.1652 (2.1941)  loss_mask: 0.4980 (0.6044)  time: 0.1728  data: 0.0002  max mem: 6052
[05:59:23.990035] Epoch: [4]  [260/781]  eta: 0:01:31  lr: 0.000217  training_loss: 3.4788 (3.4101)  mae_loss: 3.8176 (3.8922)  classification_loss: 2.1869 (2.1930)  loss_mask: 0.6637 (0.6086)  time: 0.1734  data: 0.0002  max mem: 6052
[05:59:27.438434] Epoch: [4]  [280/781]  eta: 0:01:28  lr: 0.000218  training_loss: 3.3305 (3.4084)  mae_loss: 4.0239 (3.9026)  classification_loss: 2.1810 (2.1935)  loss_mask: 0.5818 (0.6074)  time: 0.1723  data: 0.0002  max mem: 6052
[05:59:30.884458] Epoch: [4]  [300/781]  eta: 0:01:24  lr: 0.000219  training_loss: 3.2396 (3.3994)  mae_loss: 3.9013 (3.9049)  classification_loss: 2.2134 (2.1939)  loss_mask: 0.5198 (0.6028)  time: 0.1722  data: 0.0002  max mem: 6052
[05:59:34.350649] Epoch: [4]  [320/781]  eta: 0:01:20  lr: 0.000220  training_loss: 3.1952 (3.3888)  mae_loss: 3.9086 (3.9031)  classification_loss: 2.1868 (2.1933)  loss_mask: 0.5368 (0.5977)  time: 0.1732  data: 0.0002  max mem: 6052
[05:59:37.803852] Epoch: [4]  [340/781]  eta: 0:01:17  lr: 0.000222  training_loss: 3.2674 (3.3847)  mae_loss: 3.8193 (3.8991)  classification_loss: 2.1910 (2.1938)  loss_mask: 0.5519 (0.5954)  time: 0.1726  data: 0.0002  max mem: 6052
[05:59:41.264097] Epoch: [4]  [360/781]  eta: 0:01:13  lr: 0.000223  training_loss: 3.4250 (3.3855)  mae_loss: 3.9176 (3.8990)  classification_loss: 2.1956 (2.1946)  loss_mask: 0.5776 (0.5954)  time: 0.1729  data: 0.0002  max mem: 6052
[05:59:44.708789] Epoch: [4]  [380/781]  eta: 0:01:10  lr: 0.000224  training_loss: 3.1416 (3.3773)  mae_loss: 3.9319 (3.9019)  classification_loss: 2.1950 (2.1945)  loss_mask: 0.4905 (0.5914)  time: 0.1721  data: 0.0002  max mem: 6052
[05:59:48.151841] Epoch: [4]  [400/781]  eta: 0:01:06  lr: 0.000226  training_loss: 3.3405 (3.3776)  mae_loss: 3.8877 (3.9017)  classification_loss: 2.1783 (2.1937)  loss_mask: 0.5798 (0.5919)  time: 0.1720  data: 0.0002  max mem: 6052
[05:59:51.612395] Epoch: [4]  [420/781]  eta: 0:01:03  lr: 0.000227  training_loss: 3.2519 (3.3759)  mae_loss: 3.9335 (3.9034)  classification_loss: 2.1647 (2.1927)  loss_mask: 0.5489 (0.5916)  time: 0.1729  data: 0.0002  max mem: 6052
[05:59:55.078372] Epoch: [4]  [440/781]  eta: 0:00:59  lr: 0.000228  training_loss: 3.2779 (3.3725)  mae_loss: 3.8820 (3.9032)  classification_loss: 2.1769 (2.1921)  loss_mask: 0.5426 (0.5902)  time: 0.1732  data: 0.0002  max mem: 6052
[05:59:58.545024] Epoch: [4]  [460/781]  eta: 0:00:56  lr: 0.000229  training_loss: 3.0908 (3.3634)  mae_loss: 3.8432 (3.9020)  classification_loss: 2.1765 (2.1915)  loss_mask: 0.4750 (0.5860)  time: 0.1732  data: 0.0003  max mem: 6052
[06:00:01.986190] Epoch: [4]  [480/781]  eta: 0:00:52  lr: 0.000231  training_loss: 3.4196 (3.3656)  mae_loss: 3.7816 (3.8985)  classification_loss: 2.2242 (2.1923)  loss_mask: 0.6154 (0.5866)  time: 0.1720  data: 0.0002  max mem: 6052
[06:00:05.457190] Epoch: [4]  [500/781]  eta: 0:00:49  lr: 0.000232  training_loss: 3.4421 (3.3700)  mae_loss: 3.9403 (3.9004)  classification_loss: 2.1678 (2.1912)  loss_mask: 0.6487 (0.5894)  time: 0.1735  data: 0.0002  max mem: 6052
[06:00:08.947361] Epoch: [4]  [520/781]  eta: 0:00:45  lr: 0.000233  training_loss: 3.2708 (3.3676)  mae_loss: 3.8745 (3.9023)  classification_loss: 2.1480 (2.1901)  loss_mask: 0.5539 (0.5888)  time: 0.1744  data: 0.0003  max mem: 6052
[06:00:12.381884] Epoch: [4]  [540/781]  eta: 0:00:42  lr: 0.000235  training_loss: 3.2534 (3.3651)  mae_loss: 3.8965 (3.9021)  classification_loss: 2.1939 (2.1902)  loss_mask: 0.5323 (0.5874)  time: 0.1716  data: 0.0003  max mem: 6052
[06:00:15.874285] Epoch: [4]  [560/781]  eta: 0:00:38  lr: 0.000236  training_loss: 3.1832 (3.3597)  mae_loss: 3.7855 (3.8987)  classification_loss: 2.1453 (2.1890)  loss_mask: 0.4972 (0.5853)  time: 0.1745  data: 0.0002  max mem: 6052
[06:00:19.341630] Epoch: [4]  [580/781]  eta: 0:00:35  lr: 0.000237  training_loss: 3.2576 (3.3573)  mae_loss: 3.8655 (3.8965)  classification_loss: 2.1564 (2.1881)  loss_mask: 0.5515 (0.5846)  time: 0.1733  data: 0.0002  max mem: 6052
[06:00:22.803823] Epoch: [4]  [600/781]  eta: 0:00:31  lr: 0.000238  training_loss: 3.3903 (3.3594)  mae_loss: 3.8903 (3.8961)  classification_loss: 2.1718 (2.1881)  loss_mask: 0.6127 (0.5857)  time: 0.1730  data: 0.0002  max mem: 6052
[06:00:26.261127] Epoch: [4]  [620/781]  eta: 0:00:28  lr: 0.000240  training_loss: 3.3091 (3.3594)  mae_loss: 4.0663 (3.9022)  classification_loss: 2.1547 (2.1872)  loss_mask: 0.5583 (0.5861)  time: 0.1728  data: 0.0003  max mem: 6052
[06:00:29.717712] Epoch: [4]  [640/781]  eta: 0:00:24  lr: 0.000241  training_loss: 3.2849 (3.3569)  mae_loss: 3.9603 (3.9056)  classification_loss: 2.1689 (2.1866)  loss_mask: 0.5580 (0.5852)  time: 0.1727  data: 0.0003  max mem: 6052
[06:00:33.151866] Epoch: [4]  [660/781]  eta: 0:00:21  lr: 0.000242  training_loss: 3.1749 (3.3519)  mae_loss: 3.9382 (3.9057)  classification_loss: 2.1611 (2.1859)  loss_mask: 0.5027 (0.5830)  time: 0.1716  data: 0.0002  max mem: 6052
[06:00:36.598515] Epoch: [4]  [680/781]  eta: 0:00:17  lr: 0.000244  training_loss: 3.2509 (3.3491)  mae_loss: 3.9170 (3.9053)  classification_loss: 2.1730 (2.1858)  loss_mask: 0.5453 (0.5816)  time: 0.1722  data: 0.0002  max mem: 6052
[06:00:40.064581] Epoch: [4]  [700/781]  eta: 0:00:14  lr: 0.000245  training_loss: 3.6181 (3.3586)  mae_loss: 3.9522 (3.9064)  classification_loss: 2.1398 (2.1860)  loss_mask: 0.7159 (0.5863)  time: 0.1732  data: 0.0002  max mem: 6052
[06:00:43.531319] Epoch: [4]  [720/781]  eta: 0:00:10  lr: 0.000246  training_loss: 3.3239 (3.3573)  mae_loss: 4.1160 (3.9108)  classification_loss: 2.2126 (2.1866)  loss_mask: 0.5643 (0.5854)  time: 0.1732  data: 0.0002  max mem: 6052
[06:00:46.996382] Epoch: [4]  [740/781]  eta: 0:00:07  lr: 0.000247  training_loss: 3.2458 (3.3541)  mae_loss: 3.8583 (3.9108)  classification_loss: 2.1970 (2.1872)  loss_mask: 0.4953 (0.5834)  time: 0.1732  data: 0.0002  max mem: 6052
[06:00:50.454296] Epoch: [4]  [760/781]  eta: 0:00:03  lr: 0.000249  training_loss: 3.2439 (3.3513)  mae_loss: 3.8081 (3.9083)  classification_loss: 2.2043 (2.1877)  loss_mask: 0.5185 (0.5818)  time: 0.1728  data: 0.0003  max mem: 6052
[06:00:53.893284] Epoch: [4]  [780/781]  eta: 0:00:00  lr: 0.000250  training_loss: 3.1796 (3.3468)  mae_loss: 3.8502 (3.9074)  classification_loss: 2.1979 (2.1884)  loss_mask: 0.4645 (0.5792)  time: 0.1719  data: 0.0002  max mem: 6052
[06:00:54.034224] Epoch: [4] Total time: 0:02:16 (0.1742 s / it)
[06:00:54.034677] Averaged stats: lr: 0.000250  training_loss: 3.1796 (3.3468)  mae_loss: 3.8502 (3.9074)  classification_loss: 2.1979 (2.1884)  loss_mask: 0.4645 (0.5792)
[06:00:54.631947] Test:  [  0/157]  eta: 0:01:32  testing_loss: 1.9131 (1.9131)  acc1: 32.8125 (32.8125)  acc5: 84.3750 (84.3750)  time: 0.5922  data: 0.5581  max mem: 6052
[06:00:54.921396] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 1.9927 (1.9974)  acc1: 31.2500 (30.3977)  acc5: 79.6875 (80.5398)  time: 0.0799  data: 0.0509  max mem: 6052
[06:00:55.204656] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.9913 (1.9898)  acc1: 31.2500 (31.2500)  acc5: 79.6875 (80.9524)  time: 0.0285  data: 0.0002  max mem: 6052
[06:00:55.488919] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 1.9736 (1.9823)  acc1: 32.8125 (31.6532)  acc5: 81.2500 (81.3004)  time: 0.0282  data: 0.0002  max mem: 6052
[06:00:55.772395] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 1.9653 (1.9848)  acc1: 31.2500 (31.3262)  acc5: 81.2500 (80.9451)  time: 0.0282  data: 0.0002  max mem: 6052
[06:00:56.056889] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.9773 (1.9827)  acc1: 28.1250 (31.3113)  acc5: 81.2500 (81.0662)  time: 0.0282  data: 0.0002  max mem: 6052
[06:00:56.342346] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.9540 (1.9789)  acc1: 29.6875 (31.2500)  acc5: 81.2500 (81.4293)  time: 0.0284  data: 0.0002  max mem: 6052
[06:00:56.626604] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.9573 (1.9809)  acc1: 28.1250 (30.5018)  acc5: 82.8125 (81.5361)  time: 0.0284  data: 0.0002  max mem: 6052
[06:00:56.914897] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.9678 (1.9786)  acc1: 28.1250 (30.4205)  acc5: 82.8125 (81.9252)  time: 0.0285  data: 0.0003  max mem: 6052
[06:00:57.207213] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.9695 (1.9796)  acc1: 31.2500 (30.3571)  acc5: 82.8125 (81.8853)  time: 0.0289  data: 0.0003  max mem: 6052
[06:00:57.494869] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.9918 (1.9810)  acc1: 31.2500 (30.2599)  acc5: 81.2500 (81.9462)  time: 0.0288  data: 0.0002  max mem: 6052
[06:00:57.780595] Test:  [110/157]  eta: 0:00:01  testing_loss: 2.0006 (1.9832)  acc1: 28.1250 (29.9268)  acc5: 81.2500 (81.7286)  time: 0.0285  data: 0.0002  max mem: 6052
[06:00:58.068114] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.9657 (1.9800)  acc1: 28.1250 (30.2040)  acc5: 81.2500 (81.9473)  time: 0.0285  data: 0.0002  max mem: 6052
[06:00:58.357893] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.9664 (1.9811)  acc1: 31.2500 (30.2481)  acc5: 82.8125 (81.8702)  time: 0.0287  data: 0.0002  max mem: 6052
[06:00:58.646706] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.9937 (1.9816)  acc1: 29.6875 (30.3524)  acc5: 81.2500 (81.9814)  time: 0.0288  data: 0.0003  max mem: 6052
[06:00:58.929742] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.9664 (1.9807)  acc1: 31.2500 (30.3704)  acc5: 82.8125 (82.0468)  time: 0.0284  data: 0.0003  max mem: 6052
[06:00:59.081812] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.9646 (1.9804)  acc1: 31.2500 (30.2900)  acc5: 82.8125 (82.0800)  time: 0.0273  data: 0.0001  max mem: 6052
[06:00:59.244810] Test: Total time: 0:00:05 (0.0332 s / it)
[06:00:59.245411] * Acc@1 30.290 Acc@5 82.080 loss 1.980
[06:00:59.245930] Accuracy of the network on the 10000 test images: 30.3%
[06:00:59.246170] Max accuracy: 31.70%
[06:00:59.366213] log_dir: ./output_dir
[06:01:00.150667] Epoch: [5]  [  0/781]  eta: 0:10:11  lr: 0.000250  training_loss: 3.0831 (3.0831)  mae_loss: 3.8588 (3.8588)  classification_loss: 2.1557 (2.1557)  loss_mask: 0.4637 (0.4637)  time: 0.7827  data: 0.5838  max mem: 6052
[06:01:03.600382] Epoch: [5]  [ 20/781]  eta: 0:02:33  lr: 0.000250  training_loss: 3.1888 (3.2215)  mae_loss: 3.8964 (3.8985)  classification_loss: 2.1874 (2.1786)  loss_mask: 0.5156 (0.5215)  time: 0.1724  data: 0.0002  max mem: 6052
[06:01:07.074248] Epoch: [5]  [ 40/781]  eta: 0:02:19  lr: 0.000250  training_loss: 3.5111 (3.3946)  mae_loss: 3.9227 (3.8964)  classification_loss: 2.2201 (2.1969)  loss_mask: 0.6558 (0.5989)  time: 0.1736  data: 0.0002  max mem: 6052
[06:01:10.560600] Epoch: [5]  [ 60/781]  eta: 0:02:12  lr: 0.000250  training_loss: 3.1443 (3.3299)  mae_loss: 4.0351 (3.9406)  classification_loss: 2.1703 (2.1865)  loss_mask: 0.4959 (0.5717)  time: 0.1742  data: 0.0005  max mem: 6052
[06:01:14.019461] Epoch: [5]  [ 80/781]  eta: 0:02:06  lr: 0.000250  training_loss: 3.0443 (3.2701)  mae_loss: 3.8047 (3.9082)  classification_loss: 2.2053 (2.1897)  loss_mask: 0.4334 (0.5402)  time: 0.1729  data: 0.0002  max mem: 6052
[06:01:17.484222] Epoch: [5]  [100/781]  eta: 0:02:02  lr: 0.000250  training_loss: 3.1953 (3.2665)  mae_loss: 3.7744 (3.8836)  classification_loss: 2.1747 (2.1877)  loss_mask: 0.5147 (0.5394)  time: 0.1731  data: 0.0002  max mem: 6052
[06:01:20.931889] Epoch: [5]  [120/781]  eta: 0:01:57  lr: 0.000250  training_loss: 3.0995 (3.2437)  mae_loss: 3.8778 (3.8830)  classification_loss: 2.1647 (2.1839)  loss_mask: 0.4611 (0.5299)  time: 0.1723  data: 0.0002  max mem: 6052
[06:01:24.389534] Epoch: [5]  [140/781]  eta: 0:01:53  lr: 0.000250  training_loss: 3.0986 (3.2245)  mae_loss: 3.8575 (3.8859)  classification_loss: 2.1555 (2.1797)  loss_mask: 0.4626 (0.5224)  time: 0.1728  data: 0.0002  max mem: 6052
[06:01:27.853710] Epoch: [5]  [160/781]  eta: 0:01:49  lr: 0.000250  training_loss: 3.0529 (3.2093)  mae_loss: 3.9155 (3.8949)  classification_loss: 2.1759 (2.1800)  loss_mask: 0.4324 (0.5146)  time: 0.1731  data: 0.0002  max mem: 6052
[06:01:31.313887] Epoch: [5]  [180/781]  eta: 0:01:46  lr: 0.000250  training_loss: 3.3977 (3.2409)  mae_loss: 3.9385 (3.8952)  classification_loss: 2.1890 (2.1808)  loss_mask: 0.6091 (0.5300)  time: 0.1729  data: 0.0003  max mem: 6052
[06:01:34.783385] Epoch: [5]  [200/781]  eta: 0:01:42  lr: 0.000250  training_loss: 3.4197 (3.2646)  mae_loss: 4.0070 (3.9082)  classification_loss: 2.1892 (2.1816)  loss_mask: 0.5942 (0.5415)  time: 0.1734  data: 0.0002  max mem: 6052
[06:01:38.246535] Epoch: [5]  [220/781]  eta: 0:01:38  lr: 0.000250  training_loss: 3.1610 (3.2532)  mae_loss: 3.9502 (3.9167)  classification_loss: 2.1949 (2.1824)  loss_mask: 0.4764 (0.5354)  time: 0.1731  data: 0.0002  max mem: 6052
[06:01:41.694110] Epoch: [5]  [240/781]  eta: 0:01:34  lr: 0.000250  training_loss: 3.0890 (3.2431)  mae_loss: 3.8693 (3.9144)  classification_loss: 2.1834 (2.1819)  loss_mask: 0.4604 (0.5306)  time: 0.1723  data: 0.0003  max mem: 6052
[06:01:45.170884] Epoch: [5]  [260/781]  eta: 0:01:31  lr: 0.000250  training_loss: 3.1161 (3.2388)  mae_loss: 3.7679 (3.9054)  classification_loss: 2.1335 (2.1797)  loss_mask: 0.4749 (0.5295)  time: 0.1737  data: 0.0002  max mem: 6052
[06:01:48.661100] Epoch: [5]  [280/781]  eta: 0:01:27  lr: 0.000250  training_loss: 3.0864 (3.2380)  mae_loss: 3.9439 (3.9099)  classification_loss: 2.1541 (2.1793)  loss_mask: 0.4693 (0.5294)  time: 0.1744  data: 0.0002  max mem: 6052
[06:01:52.133164] Epoch: [5]  [300/781]  eta: 0:01:24  lr: 0.000250  training_loss: 3.1815 (3.2384)  mae_loss: 3.9735 (3.9149)  classification_loss: 2.1966 (2.1800)  loss_mask: 0.5209 (0.5292)  time: 0.1735  data: 0.0003  max mem: 6052
[06:01:55.602237] Epoch: [5]  [320/781]  eta: 0:01:20  lr: 0.000250  training_loss: 3.0609 (3.2287)  mae_loss: 3.9710 (3.9204)  classification_loss: 2.1904 (2.1803)  loss_mask: 0.4372 (0.5242)  time: 0.1734  data: 0.0002  max mem: 6052
[06:01:59.059269] Epoch: [5]  [340/781]  eta: 0:01:17  lr: 0.000250  training_loss: 3.0669 (3.2229)  mae_loss: 3.8039 (3.9142)  classification_loss: 2.1772 (2.1803)  loss_mask: 0.4199 (0.5213)  time: 0.1728  data: 0.0002  max mem: 6052
[06:02:02.509979] Epoch: [5]  [360/781]  eta: 0:01:13  lr: 0.000250  training_loss: 3.1639 (3.2252)  mae_loss: 3.9075 (3.9148)  classification_loss: 2.2034 (2.1810)  loss_mask: 0.5065 (0.5221)  time: 0.1725  data: 0.0002  max mem: 6052
[06:02:05.969621] Epoch: [5]  [380/781]  eta: 0:01:10  lr: 0.000250  training_loss: 3.5268 (3.2392)  mae_loss: 3.9185 (3.9169)  classification_loss: 2.1899 (2.1813)  loss_mask: 0.6291 (0.5290)  time: 0.1729  data: 0.0003  max mem: 6052
[06:02:09.439853] Epoch: [5]  [400/781]  eta: 0:01:06  lr: 0.000250  training_loss: 3.2444 (3.2442)  mae_loss: 4.0436 (3.9219)  classification_loss: 2.1782 (2.1816)  loss_mask: 0.5261 (0.5313)  time: 0.1734  data: 0.0003  max mem: 6052
[06:02:12.894895] Epoch: [5]  [420/781]  eta: 0:01:03  lr: 0.000250  training_loss: 3.1174 (3.2402)  mae_loss: 4.0087 (3.9260)  classification_loss: 2.1905 (2.1820)  loss_mask: 0.4631 (0.5291)  time: 0.1727  data: 0.0002  max mem: 6052
[06:02:16.384030] Epoch: [5]  [440/781]  eta: 0:00:59  lr: 0.000250  training_loss: 3.0642 (3.2341)  mae_loss: 4.0474 (3.9318)  classification_loss: 2.1558 (2.1808)  loss_mask: 0.4622 (0.5266)  time: 0.1744  data: 0.0002  max mem: 6052
[06:02:19.864588] Epoch: [5]  [460/781]  eta: 0:00:56  lr: 0.000250  training_loss: 2.9919 (3.2234)  mae_loss: 3.8861 (3.9320)  classification_loss: 2.1434 (2.1788)  loss_mask: 0.4312 (0.5223)  time: 0.1739  data: 0.0002  max mem: 6052
[06:02:23.337629] Epoch: [5]  [480/781]  eta: 0:00:52  lr: 0.000250  training_loss: 3.0908 (3.2194)  mae_loss: 3.8701 (3.9295)  classification_loss: 2.1763 (2.1797)  loss_mask: 0.4529 (0.5199)  time: 0.1736  data: 0.0005  max mem: 6052
[06:02:26.790648] Epoch: [5]  [500/781]  eta: 0:00:49  lr: 0.000250  training_loss: 3.2499 (3.2214)  mae_loss: 3.8077 (3.9265)  classification_loss: 2.1674 (2.1789)  loss_mask: 0.5454 (0.5213)  time: 0.1726  data: 0.0002  max mem: 6052
[06:02:30.285483] Epoch: [5]  [520/781]  eta: 0:00:45  lr: 0.000250  training_loss: 3.0960 (3.2186)  mae_loss: 3.9299 (3.9265)  classification_loss: 2.1542 (2.1783)  loss_mask: 0.4544 (0.5201)  time: 0.1746  data: 0.0002  max mem: 6052
[06:02:33.759931] Epoch: [5]  [540/781]  eta: 0:00:42  lr: 0.000250  training_loss: 3.0473 (3.2133)  mae_loss: 3.9507 (3.9273)  classification_loss: 2.1697 (2.1785)  loss_mask: 0.4345 (0.5174)  time: 0.1736  data: 0.0002  max mem: 6052
[06:02:37.247689] Epoch: [5]  [560/781]  eta: 0:00:38  lr: 0.000250  training_loss: 3.0978 (3.2094)  mae_loss: 4.0185 (3.9296)  classification_loss: 2.1467 (2.1774)  loss_mask: 0.4783 (0.5160)  time: 0.1743  data: 0.0002  max mem: 6052
[06:02:40.711501] Epoch: [5]  [580/781]  eta: 0:00:35  lr: 0.000250  training_loss: 3.0266 (3.2034)  mae_loss: 3.8951 (3.9281)  classification_loss: 2.1463 (2.1768)  loss_mask: 0.4469 (0.5133)  time: 0.1731  data: 0.0002  max mem: 6052
[06:02:44.172347] Epoch: [5]  [600/781]  eta: 0:00:31  lr: 0.000250  training_loss: 3.0321 (3.1984)  mae_loss: 4.0100 (3.9314)  classification_loss: 2.1804 (2.1770)  loss_mask: 0.4247 (0.5107)  time: 0.1729  data: 0.0003  max mem: 6052
[06:02:47.651207] Epoch: [5]  [620/781]  eta: 0:00:28  lr: 0.000250  training_loss: 2.8250 (3.1874)  mae_loss: 3.8716 (3.9309)  classification_loss: 2.1497 (2.1761)  loss_mask: 0.3513 (0.5057)  time: 0.1739  data: 0.0003  max mem: 6052
[06:02:51.140685] Epoch: [5]  [640/781]  eta: 0:00:24  lr: 0.000250  training_loss: 3.0077 (3.1847)  mae_loss: 3.8262 (3.9268)  classification_loss: 2.1683 (2.1757)  loss_mask: 0.4196 (0.5045)  time: 0.1744  data: 0.0002  max mem: 6052

[06:02:54.595308] Epoch: [5]  [660/781]  eta: 0:00:21  lr: 0.000250  training_loss: 3.0933 (3.1840)  mae_loss: 3.8013 (3.9228)  classification_loss: 2.1523 (2.1754)  loss_mask: 0.4570 (0.5043)  time: 0.1726  data: 0.0002  max mem: 6052
[06:02:58.081946] Epoch: [5]  [680/781]  eta: 0:00:17  lr: 0.000250  training_loss: 3.1732 (3.1865)  mae_loss: 4.0013 (3.9254)  classification_loss: 2.1559 (2.1749)  loss_mask: 0.4793 (0.5058)  time: 0.1743  data: 0.0002  max mem: 6052
[06:03:01.573030] Epoch: [5]  [700/781]  eta: 0:00:14  lr: 0.000250  training_loss: 3.0362 (3.1834)  mae_loss: 4.0581 (3.9297)  classification_loss: 2.1454 (2.1742)  loss_mask: 0.4576 (0.5046)  time: 0.1744  data: 0.0002  max mem: 6052
[06:03:05.026005] Epoch: [5]  [720/781]  eta: 0:00:10  lr: 0.000250  training_loss: 3.0096 (3.1791)  mae_loss: 3.8897 (3.9291)  classification_loss: 2.1452 (2.1736)  loss_mask: 0.4162 (0.5028)  time: 0.1726  data: 0.0003  max mem: 6052
[06:03:08.498417] Epoch: [5]  [740/781]  eta: 0:00:07  lr: 0.000250  training_loss: 2.9361 (3.1732)  mae_loss: 3.9103 (3.9280)  classification_loss: 2.1608 (2.1732)  loss_mask: 0.3935 (0.5000)  time: 0.1735  data: 0.0002  max mem: 6052
[06:03:11.970052] Epoch: [5]  [760/781]  eta: 0:00:03  lr: 0.000250  training_loss: 3.1272 (3.1722)  mae_loss: 3.9731 (3.9283)  classification_loss: 2.1773 (2.1730)  loss_mask: 0.4918 (0.4996)  time: 0.1735  data: 0.0004  max mem: 6052
[06:03:15.485828] Epoch: [5]  [780/781]  eta: 0:00:00  lr: 0.000250  training_loss: 3.0423 (3.1710)  mae_loss: 3.9712 (3.9294)  classification_loss: 2.1586 (2.1727)  loss_mask: 0.4325 (0.4991)  time: 0.1757  data: 0.0002  max mem: 6052
[06:03:15.636128] Epoch: [5] Total time: 0:02:16 (0.1745 s / it)
[06:03:15.637060] Averaged stats: lr: 0.000250  training_loss: 3.0423 (3.1710)  mae_loss: 3.9712 (3.9294)  classification_loss: 2.1586 (2.1727)  loss_mask: 0.4325 (0.4991)
[06:03:16.210871] Test:  [  0/157]  eta: 0:01:29  testing_loss: 1.8511 (1.8511)  acc1: 40.6250 (40.6250)  acc5: 87.5000 (87.5000)  time: 0.5683  data: 0.5200  max mem: 6052
[06:03:16.497758] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 1.9245 (1.9115)  acc1: 34.3750 (33.8068)  acc5: 85.9375 (84.9432)  time: 0.0776  data: 0.0474  max mem: 6052
[06:03:16.784335] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.8995 (1.9006)  acc1: 35.9375 (35.8631)  acc5: 85.9375 (85.6399)  time: 0.0285  data: 0.0002  max mem: 6052
[06:03:17.077136] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 1.8691 (1.8949)  acc1: 37.5000 (36.4415)  acc5: 85.9375 (85.5847)  time: 0.0288  data: 0.0002  max mem: 6052
[06:03:17.369534] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 1.8727 (1.8969)  acc1: 35.9375 (36.5091)  acc5: 82.8125 (84.9466)  time: 0.0291  data: 0.0002  max mem: 6052
[06:03:17.658219] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.8916 (1.8971)  acc1: 32.8125 (35.9988)  acc5: 82.8125 (84.9877)  time: 0.0289  data: 0.0002  max mem: 6052
[06:03:17.945977] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.8653 (1.8927)  acc1: 32.8125 (36.0400)  acc5: 84.3750 (85.2715)  time: 0.0287  data: 0.0002  max mem: 6052
[06:03:18.232174] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.8631 (1.8920)  acc1: 35.9375 (35.4974)  acc5: 87.5000 (85.6734)  time: 0.0286  data: 0.0002  max mem: 6052
[06:03:18.513579] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.8807 (1.8918)  acc1: 34.3750 (35.3395)  acc5: 89.0625 (85.8410)  time: 0.0283  data: 0.0001  max mem: 6052
[06:03:18.798945] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.9101 (1.8952)  acc1: 32.8125 (35.0103)  acc5: 85.9375 (85.8860)  time: 0.0282  data: 0.0002  max mem: 6052
[06:03:19.083947] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.9266 (1.8974)  acc1: 31.2500 (34.9783)  acc5: 82.8125 (85.5353)  time: 0.0284  data: 0.0002  max mem: 6052
[06:03:19.372054] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.9260 (1.8988)  acc1: 31.2500 (34.7551)  acc5: 82.8125 (85.3885)  time: 0.0285  data: 0.0002  max mem: 6052
[06:03:19.658310] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.8838 (1.8960)  acc1: 32.8125 (34.7882)  acc5: 85.9375 (85.6018)  time: 0.0286  data: 0.0002  max mem: 6052
[06:03:19.948284] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.8895 (1.8968)  acc1: 34.3750 (34.8402)  acc5: 87.5000 (85.5916)  time: 0.0286  data: 0.0002  max mem: 6052
[06:03:20.237426] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.9016 (1.8964)  acc1: 34.3750 (34.7739)  acc5: 85.9375 (85.6937)  time: 0.0288  data: 0.0002  max mem: 6052
[06:03:20.520332] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.9016 (1.8956)  acc1: 32.8125 (34.8406)  acc5: 85.9375 (85.7512)  time: 0.0285  data: 0.0002  max mem: 6052
[06:03:20.675393] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.8997 (1.8948)  acc1: 34.3750 (34.7200)  acc5: 84.3750 (85.7300)  time: 0.0274  data: 0.0002  max mem: 6052
[06:03:20.830420] Test: Total time: 0:00:05 (0.0330 s / it)
[06:03:20.831154] * Acc@1 34.720 Acc@5 85.730 loss 1.895
[06:03:20.831488] Accuracy of the network on the 10000 test images: 34.7%
[06:03:20.831689] Max accuracy: 34.72%
[06:03:21.007244] log_dir: ./output_dir
[06:03:21.837624] Epoch: [6]  [  0/781]  eta: 0:10:47  lr: 0.000250  training_loss: 2.9350 (2.9350)  mae_loss: 3.9055 (3.9055)  classification_loss: 2.2074 (2.2074)  loss_mask: 0.3638 (0.3638)  time: 0.8287  data: 0.6098  max mem: 6052
[06:03:25.293204] Epoch: [6]  [ 20/781]  eta: 0:02:35  lr: 0.000250  training_loss: 2.9670 (2.9765)  mae_loss: 4.0777 (4.0414)  classification_loss: 2.1579 (2.1597)  loss_mask: 0.3862 (0.4084)  time: 0.1727  data: 0.0002  max mem: 6052
[06:03:28.762858] Epoch: [6]  [ 40/781]  eta: 0:02:20  lr: 0.000250  training_loss: 2.8581 (2.9256)  mae_loss: 4.0479 (4.0347)  classification_loss: 2.1273 (2.1514)  loss_mask: 0.3602 (0.3871)  time: 0.1734  data: 0.0002  max mem: 6052
[06:03:32.220160] Epoch: [6]  [ 60/781]  eta: 0:02:12  lr: 0.000250  training_loss: 2.8321 (2.9048)  mae_loss: 3.8784 (3.9895)  classification_loss: 2.1766 (2.1516)  loss_mask: 0.3579 (0.3766)  time: 0.1728  data: 0.0002  max mem: 6052
[06:03:35.686217] Epoch: [6]  [ 80/781]  eta: 0:02:06  lr: 0.000250  training_loss: 3.0366 (2.9670)  mae_loss: 3.9316 (3.9733)  classification_loss: 2.1921 (2.1606)  loss_mask: 0.4377 (0.4032)  time: 0.1732  data: 0.0002  max mem: 6052
[06:03:39.139302] Epoch: [6]  [100/781]  eta: 0:02:02  lr: 0.000250  training_loss: 3.0367 (2.9961)  mae_loss: 4.0149 (3.9813)  classification_loss: 2.1537 (2.1616)  loss_mask: 0.4615 (0.4173)  time: 0.1726  data: 0.0002  max mem: 6052
[06:03:42.595256] Epoch: [6]  [120/781]  eta: 0:01:57  lr: 0.000250  training_loss: 2.9401 (2.9940)  mae_loss: 4.0054 (3.9847)  classification_loss: 2.1406 (2.1601)  loss_mask: 0.3904 (0.4169)  time: 0.1727  data: 0.0003  max mem: 6052
[06:03:46.041178] Epoch: [6]  [140/781]  eta: 0:01:53  lr: 0.000250  training_loss: 2.7988 (2.9721)  mae_loss: 3.9578 (3.9876)  classification_loss: 2.1461 (2.1579)  loss_mask: 0.3304 (0.4071)  time: 0.1722  data: 0.0003  max mem: 6052
[06:03:49.507670] Epoch: [6]  [160/781]  eta: 0:01:49  lr: 0.000250  training_loss: 2.8775 (2.9607)  mae_loss: 3.8987 (3.9775)  classification_loss: 2.1989 (2.1614)  loss_mask: 0.3411 (0.3996)  time: 0.1732  data: 0.0002  max mem: 6052
[06:03:52.971574] Epoch: [6]  [180/781]  eta: 0:01:46  lr: 0.000250  training_loss: 2.9271 (2.9581)  mae_loss: 3.8055 (3.9626)  classification_loss: 2.1468 (2.1614)  loss_mask: 0.3727 (0.3984)  time: 0.1731  data: 0.0002  max mem: 6052
[06:03:56.418477] Epoch: [6]  [200/781]  eta: 0:01:42  lr: 0.000250  training_loss: 2.9484 (2.9600)  mae_loss: 3.9021 (3.9556)  classification_loss: 2.1727 (2.1626)  loss_mask: 0.3955 (0.3987)  time: 0.1723  data: 0.0002  max mem: 6052
[06:03:59.871260] Epoch: [6]  [220/781]  eta: 0:01:38  lr: 0.000250  training_loss: 2.8893 (2.9623)  mae_loss: 3.8251 (3.9443)  classification_loss: 2.1472 (2.1621)  loss_mask: 0.3921 (0.4001)  time: 0.1725  data: 0.0003  max mem: 6052
[06:04:03.334162] Epoch: [6]  [240/781]  eta: 0:01:34  lr: 0.000250  training_loss: 2.8180 (2.9508)  mae_loss: 3.8424 (3.9413)  classification_loss: 2.1559 (2.1624)  loss_mask: 0.3148 (0.3942)  time: 0.1730  data: 0.0002  max mem: 6052
[06:04:06.813474] Epoch: [6]  [260/781]  eta: 0:01:31  lr: 0.000250  training_loss: 2.8247 (2.9396)  mae_loss: 3.8219 (3.9343)  classification_loss: 2.1268 (2.1601)  loss_mask: 0.3314 (0.3898)  time: 0.1739  data: 0.0002  max mem: 6052
[06:04:10.285241] Epoch: [6]  [280/781]  eta: 0:01:27  lr: 0.000250  training_loss: 2.8195 (2.9311)  mae_loss: 3.8878 (3.9316)  classification_loss: 2.1286 (2.1592)  loss_mask: 0.3471 (0.3860)  time: 0.1735  data: 0.0002  max mem: 6052
[06:04:13.739985] Epoch: [6]  [300/781]  eta: 0:01:24  lr: 0.000250  training_loss: 2.8120 (2.9267)  mae_loss: 3.8917 (3.9301)  classification_loss: 2.1621 (2.1594)  loss_mask: 0.3387 (0.3836)  time: 0.1727  data: 0.0002  max mem: 6052
[06:04:17.194621] Epoch: [6]  [320/781]  eta: 0:01:20  lr: 0.000250  training_loss: 2.8913 (2.9265)  mae_loss: 3.9077 (3.9299)  classification_loss: 2.1694 (2.1587)  loss_mask: 0.3642 (0.3839)  time: 0.1726  data: 0.0002  max mem: 6052
[06:04:20.663436] Epoch: [6]  [340/781]  eta: 0:01:17  lr: 0.000250  training_loss: 2.8993 (2.9246)  mae_loss: 3.8033 (3.9248)  classification_loss: 2.1220 (2.1565)  loss_mask: 0.3886 (0.3840)  time: 0.1734  data: 0.0005  max mem: 6052
[06:04:24.129116] Epoch: [6]  [360/781]  eta: 0:01:13  lr: 0.000250  training_loss: 3.2982 (2.9570)  mae_loss: 4.0138 (3.9285)  classification_loss: 2.1548 (2.1570)  loss_mask: 0.5670 (0.4000)  time: 0.1732  data: 0.0003  max mem: 6052
[06:04:27.571406] Epoch: [6]  [380/781]  eta: 0:01:10  lr: 0.000250  training_loss: 3.0897 (2.9642)  mae_loss: 4.0750 (3.9365)  classification_loss: 2.1711 (2.1582)  loss_mask: 0.4464 (0.4030)  time: 0.1720  data: 0.0002  max mem: 6052
[06:04:31.033474] Epoch: [6]  [400/781]  eta: 0:01:06  lr: 0.000250  training_loss: 2.8689 (2.9620)  mae_loss: 4.1491 (3.9473)  classification_loss: 2.1530 (2.1583)  loss_mask: 0.3718 (0.4019)  time: 0.1730  data: 0.0002  max mem: 6052
[06:04:34.492354] Epoch: [6]  [420/781]  eta: 0:01:02  lr: 0.000250  training_loss: 2.8309 (2.9560)  mae_loss: 4.0448 (3.9536)  classification_loss: 2.1403 (2.1580)  loss_mask: 0.3502 (0.3990)  time: 0.1728  data: 0.0002  max mem: 6052
[06:04:37.950085] Epoch: [6]  [440/781]  eta: 0:00:59  lr: 0.000250  training_loss: 2.6898 (2.9450)  mae_loss: 4.0094 (3.9576)  classification_loss: 2.1146 (2.1565)  loss_mask: 0.2849 (0.3942)  time: 0.1728  data: 0.0002  max mem: 6052
[06:04:41.425852] Epoch: [6]  [460/781]  eta: 0:00:55  lr: 0.000250  training_loss: 2.7191 (2.9379)  mae_loss: 3.8815 (3.9545)  classification_loss: 2.1106 (2.1545)  loss_mask: 0.3229 (0.3917)  time: 0.1737  data: 0.0003  max mem: 6052
[06:04:44.889778] Epoch: [6]  [480/781]  eta: 0:00:52  lr: 0.000250  training_loss: 2.8775 (2.9360)  mae_loss: 3.9315 (3.9536)  classification_loss: 2.1432 (2.1548)  loss_mask: 0.3355 (0.3906)  time: 0.1731  data: 0.0002  max mem: 6052
[06:04:48.349884] Epoch: [6]  [500/781]  eta: 0:00:48  lr: 0.000250  training_loss: 2.8357 (2.9332)  mae_loss: 3.9688 (3.9551)  classification_loss: 2.1655 (2.1552)  loss_mask: 0.3370 (0.3890)  time: 0.1729  data: 0.0002  max mem: 6052
[06:04:51.816483] Epoch: [6]  [520/781]  eta: 0:00:45  lr: 0.000250  training_loss: 2.6801 (2.9243)  mae_loss: 3.9368 (3.9545)  classification_loss: 2.1394 (2.1550)  loss_mask: 0.2534 (0.3846)  time: 0.1733  data: 0.0003  max mem: 6052
[06:04:55.324481] Epoch: [6]  [540/781]  eta: 0:00:41  lr: 0.000250  training_loss: 2.7074 (2.9177)  mae_loss: 3.8647 (3.9519)  classification_loss: 2.1380 (2.1547)  loss_mask: 0.2946 (0.3815)  time: 0.1753  data: 0.0002  max mem: 6052
[06:04:58.773770] Epoch: [6]  [560/781]  eta: 0:00:38  lr: 0.000250  training_loss: 2.6939 (2.9103)  mae_loss: 3.8729 (3.9491)  classification_loss: 2.1312 (2.1540)  loss_mask: 0.2528 (0.3781)  time: 0.1724  data: 0.0002  max mem: 6052
[06:05:02.237529] Epoch: [6]  [580/781]  eta: 0:00:35  lr: 0.000250  training_loss: 2.6778 (2.9037)  mae_loss: 3.8553 (3.9460)  classification_loss: 2.1246 (2.1532)  loss_mask: 0.2755 (0.3752)  time: 0.1731  data: 0.0002  max mem: 6052
[06:05:05.674934] Epoch: [6]  [600/781]  eta: 0:00:31  lr: 0.000250  training_loss: 2.7115 (2.9002)  mae_loss: 3.8331 (3.9428)  classification_loss: 2.1506 (2.1530)  loss_mask: 0.3043 (0.3736)  time: 0.1718  data: 0.0002  max mem: 6052
[06:05:09.141992] Epoch: [6]  [620/781]  eta: 0:00:28  lr: 0.000250  training_loss: 2.7553 (2.8962)  mae_loss: 4.0004 (3.9441)  classification_loss: 2.1302 (2.1523)  loss_mask: 0.3208 (0.3719)  time: 0.1733  data: 0.0002  max mem: 6052
[06:05:12.643895] Epoch: [6]  [640/781]  eta: 0:00:24  lr: 0.000250  training_loss: 2.7573 (2.8940)  mae_loss: 4.0563 (3.9470)  classification_loss: 2.1783 (2.1531)  loss_mask: 0.3081 (0.3704)  time: 0.1750  data: 0.0003  max mem: 6052
[06:05:16.099117] Epoch: [6]  [660/781]  eta: 0:00:21  lr: 0.000250  training_loss: 2.7647 (2.8907)  mae_loss: 3.9452 (3.9464)  classification_loss: 2.1788 (2.1539)  loss_mask: 0.2838 (0.3684)  time: 0.1727  data: 0.0003  max mem: 6052
[06:05:19.570623] Epoch: [6]  [680/781]  eta: 0:00:17  lr: 0.000250  training_loss: 2.6980 (2.8863)  mae_loss: 3.9624 (3.9467)  classification_loss: 2.1121 (2.1532)  loss_mask: 0.2828 (0.3666)  time: 0.1735  data: 0.0002  max mem: 6052
[06:05:23.031157] Epoch: [6]  [700/781]  eta: 0:00:14  lr: 0.000250  training_loss: 2.6868 (2.8819)  mae_loss: 3.9521 (3.9481)  classification_loss: 2.1519 (2.1532)  loss_mask: 0.2790 (0.3644)  time: 0.1729  data: 0.0002  max mem: 6052
[06:05:26.510607] Epoch: [6]  [720/781]  eta: 0:00:10  lr: 0.000250  training_loss: 2.8296 (2.8812)  mae_loss: 3.9332 (3.9476)  classification_loss: 2.1635 (2.1534)  loss_mask: 0.3210 (0.3639)  time: 0.1739  data: 0.0003  max mem: 6052
[06:05:29.982100] Epoch: [6]  [740/781]  eta: 0:00:07  lr: 0.000250  training_loss: 2.7700 (2.8781)  mae_loss: 3.9240 (3.9474)  classification_loss: 2.1365 (2.1532)  loss_mask: 0.2948 (0.3624)  time: 0.1735  data: 0.0002  max mem: 6052
[06:05:33.444472] Epoch: [6]  [760/781]  eta: 0:00:03  lr: 0.000250  training_loss: 2.7965 (2.8783)  mae_loss: 4.0186 (3.9495)  classification_loss: 2.1331 (2.1528)  loss_mask: 0.3280 (0.3628)  time: 0.1730  data: 0.0002  max mem: 6052
[06:05:36.880368] Epoch: [6]  [780/781]  eta: 0:00:00  lr: 0.000250  training_loss: 2.7024 (2.8758)  mae_loss: 3.9490 (3.9509)  classification_loss: 2.1477 (2.1525)  loss_mask: 0.3063 (0.3616)  time: 0.1717  data: 0.0002  max mem: 6052
[06:05:37.052977] Epoch: [6] Total time: 0:02:16 (0.1742 s / it)
[06:05:37.053480] Averaged stats: lr: 0.000250  training_loss: 2.7024 (2.8758)  mae_loss: 3.9490 (3.9509)  classification_loss: 2.1477 (2.1525)  loss_mask: 0.3063 (0.3616)
[06:05:37.663512] Test:  [  0/157]  eta: 0:01:35  testing_loss: 1.8523 (1.8523)  acc1: 40.6250 (40.6250)  acc5: 89.0625 (89.0625)  time: 0.6057  data: 0.5764  max mem: 6052
[06:05:37.948201] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 1.9139 (1.9158)  acc1: 34.3750 (34.2330)  acc5: 82.8125 (83.6648)  time: 0.0807  data: 0.0525  max mem: 6052
[06:05:38.231989] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.9101 (1.9022)  acc1: 34.3750 (34.6726)  acc5: 82.8125 (84.3006)  time: 0.0282  data: 0.0002  max mem: 6052
[06:05:38.515344] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 1.8629 (1.8916)  acc1: 35.9375 (35.0806)  acc5: 84.3750 (84.6270)  time: 0.0282  data: 0.0003  max mem: 6052
[06:05:38.803324] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 1.8655 (1.8952)  acc1: 34.3750 (34.7561)  acc5: 84.3750 (84.4893)  time: 0.0284  data: 0.0002  max mem: 6052
[06:05:39.091189] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.8808 (1.8940)  acc1: 31.2500 (34.5282)  acc5: 85.9375 (84.7733)  time: 0.0286  data: 0.0002  max mem: 6052
[06:05:39.377880] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.8598 (1.8884)  acc1: 32.8125 (34.7080)  acc5: 85.9375 (85.0154)  time: 0.0285  data: 0.0002  max mem: 6052
[06:05:39.662229] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.8709 (1.8897)  acc1: 32.8125 (34.1329)  acc5: 87.5000 (85.2333)  time: 0.0284  data: 0.0002  max mem: 6052
[06:05:39.951208] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.8804 (1.8883)  acc1: 32.8125 (34.1242)  acc5: 85.9375 (85.3781)  time: 0.0285  data: 0.0002  max mem: 6052
[06:05:40.238482] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.9030 (1.8895)  acc1: 32.8125 (33.8427)  acc5: 84.3750 (85.4224)  time: 0.0287  data: 0.0002  max mem: 6052
[06:05:40.525508] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.9123 (1.8925)  acc1: 31.2500 (33.6170)  acc5: 84.3750 (85.3032)  time: 0.0285  data: 0.0002  max mem: 6052
[06:05:40.811515] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.9183 (1.8951)  acc1: 29.6875 (33.2207)  acc5: 84.3750 (85.2759)  time: 0.0285  data: 0.0002  max mem: 6052
[06:05:41.100759] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.8794 (1.8921)  acc1: 31.2500 (33.3678)  acc5: 85.9375 (85.4210)  time: 0.0286  data: 0.0002  max mem: 6052
[06:05:41.389590] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.8805 (1.8931)  acc1: 35.9375 (33.5281)  acc5: 85.9375 (85.3769)  time: 0.0287  data: 0.0002  max mem: 6052
[06:05:41.682520] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.9039 (1.8935)  acc1: 32.8125 (33.5550)  acc5: 85.9375 (85.4610)  time: 0.0289  data: 0.0005  max mem: 6052
[06:05:41.962704] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.8909 (1.8930)  acc1: 32.8125 (33.4954)  acc5: 85.9375 (85.3891)  time: 0.0285  data: 0.0005  max mem: 6052
[06:05:42.113660] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.8846 (1.8926)  acc1: 34.3750 (33.3600)  acc5: 85.9375 (85.4200)  time: 0.0271  data: 0.0001  max mem: 6052
[06:05:42.281260] Test: Total time: 0:00:05 (0.0333 s / it)
[06:05:42.281734] * Acc@1 33.360 Acc@5 85.420 loss 1.893
[06:05:42.282020] Accuracy of the network on the 10000 test images: 33.4%
[06:05:42.282196] Max accuracy: 34.72%
[06:05:42.410612] log_dir: ./output_dir
[06:05:43.203331] Epoch: [7]  [  0/781]  eta: 0:10:17  lr: 0.000250  training_loss: 2.6627 (2.6627)  mae_loss: 4.2551 (4.2551)  classification_loss: 2.1914 (2.1914)  loss_mask: 0.2357 (0.2357)  time: 0.7910  data: 0.5868  max mem: 6052
[06:05:46.674582] Epoch: [7]  [ 20/781]  eta: 0:02:34  lr: 0.000250  training_loss: 2.6881 (2.7155)  mae_loss: 4.0635 (4.0722)  classification_loss: 2.1034 (2.1314)  loss_mask: 0.2805 (0.2920)  time: 0.1735  data: 0.0002  max mem: 6052
[06:05:50.129321] Epoch: [7]  [ 40/781]  eta: 0:02:19  lr: 0.000250  training_loss: 2.6753 (2.7018)  mae_loss: 4.0545 (4.0662)  classification_loss: 2.1516 (2.1395)  loss_mask: 0.2496 (0.2812)  time: 0.1727  data: 0.0002  max mem: 6052
[06:05:53.560202] Epoch: [7]  [ 60/781]  eta: 0:02:11  lr: 0.000250  training_loss: 2.6528 (2.7014)  mae_loss: 4.0631 (4.0614)  classification_loss: 2.1596 (2.1472)  loss_mask: 0.2404 (0.2771)  time: 0.1714  data: 0.0002  max mem: 6052
[06:05:57.007852] Epoch: [7]  [ 80/781]  eta: 0:02:06  lr: 0.000250  training_loss: 2.5978 (2.6761)  mae_loss: 3.8902 (4.0162)  classification_loss: 2.1376 (2.1441)  loss_mask: 0.2291 (0.2660)  time: 0.1723  data: 0.0003  max mem: 6052
[06:06:00.474565] Epoch: [7]  [100/781]  eta: 0:02:01  lr: 0.000250  training_loss: 2.5905 (2.6642)  mae_loss: 3.8374 (3.9802)  classification_loss: 2.1340 (2.1434)  loss_mask: 0.2160 (0.2604)  time: 0.1732  data: 0.0002  max mem: 6052
[06:06:03.940192] Epoch: [7]  [120/781]  eta: 0:01:57  lr: 0.000250  training_loss: 2.5859 (2.6549)  mae_loss: 4.0276 (3.9845)  classification_loss: 2.1263 (2.1400)  loss_mask: 0.2271 (0.2575)  time: 0.1732  data: 0.0003  max mem: 6052
[06:06:07.386568] Epoch: [7]  [140/781]  eta: 0:01:53  lr: 0.000250  training_loss: 2.6899 (2.6832)  mae_loss: 3.9553 (3.9812)  classification_loss: 2.1180 (2.1374)  loss_mask: 0.3064 (0.2729)  time: 0.1722  data: 0.0002  max mem: 6052
[06:06:10.870865] Epoch: [7]  [160/781]  eta: 0:01:49  lr: 0.000250  training_loss: 2.6702 (2.6825)  mae_loss: 4.0328 (3.9908)  classification_loss: 2.1391 (2.1392)  loss_mask: 0.2552 (0.2717)  time: 0.1741  data: 0.0002  max mem: 6052
[06:06:14.326702] Epoch: [7]  [180/781]  eta: 0:01:45  lr: 0.000250  training_loss: 2.5877 (2.6769)  mae_loss: 3.9439 (3.9861)  classification_loss: 2.1435 (2.1402)  loss_mask: 0.2312 (0.2683)  time: 0.1727  data: 0.0002  max mem: 6052
[06:06:17.784895] Epoch: [7]  [200/781]  eta: 0:01:42  lr: 0.000250  training_loss: 2.5687 (2.6666)  mae_loss: 3.9403 (3.9791)  classification_loss: 2.1472 (2.1410)  loss_mask: 0.1994 (0.2628)  time: 0.1728  data: 0.0002  max mem: 6052
[06:06:21.236878] Epoch: [7]  [220/781]  eta: 0:01:38  lr: 0.000250  training_loss: 2.5698 (2.6613)  mae_loss: 3.9472 (3.9770)  classification_loss: 2.1171 (2.1401)  loss_mask: 0.2391 (0.2606)  time: 0.1725  data: 0.0002  max mem: 6052
[06:06:24.716337] Epoch: [7]  [240/781]  eta: 0:01:34  lr: 0.000250  training_loss: 2.4987 (2.6516)  mae_loss: 4.0465 (3.9867)  classification_loss: 2.1126 (2.1388)  loss_mask: 0.1956 (0.2564)  time: 0.1739  data: 0.0002  max mem: 6052
[06:06:28.190639] Epoch: [7]  [260/781]  eta: 0:01:31  lr: 0.000250  training_loss: 2.7319 (2.6699)  mae_loss: 3.9319 (3.9832)  classification_loss: 2.1029 (2.1368)  loss_mask: 0.2787 (0.2665)  time: 0.1736  data: 0.0002  max mem: 6052
[06:06:31.679010] Epoch: [7]  [280/781]  eta: 0:01:27  lr: 0.000250  training_loss: 2.8799 (2.6838)  mae_loss: 4.0865 (3.9914)  classification_loss: 2.1112 (2.1358)  loss_mask: 0.3708 (0.2740)  time: 0.1743  data: 0.0001  max mem: 6052
[06:06:35.139963] Epoch: [7]  [300/781]  eta: 0:01:24  lr: 0.000250  training_loss: 2.7860 (2.6935)  mae_loss: 4.0008 (3.9935)  classification_loss: 2.1330 (2.1358)  loss_mask: 0.3248 (0.2789)  time: 0.1729  data: 0.0003  max mem: 6052
[06:06:38.609013] Epoch: [7]  [320/781]  eta: 0:01:20  lr: 0.000250  training_loss: 2.5869 (2.6888)  mae_loss: 4.1266 (4.0023)  classification_loss: 2.1271 (2.1349)  loss_mask: 0.2364 (0.2770)  time: 0.1734  data: 0.0002  max mem: 6052
[06:06:42.060610] Epoch: [7]  [340/781]  eta: 0:01:17  lr: 0.000250  training_loss: 2.5266 (2.6800)  mae_loss: 3.9996 (4.0027)  classification_loss: 2.1063 (2.1333)  loss_mask: 0.2069 (0.2733)  time: 0.1725  data: 0.0002  max mem: 6052
[06:06:45.520172] Epoch: [7]  [360/781]  eta: 0:01:13  lr: 0.000250  training_loss: 2.6016 (2.6760)  mae_loss: 4.0551 (4.0042)  classification_loss: 2.1113 (2.1323)  loss_mask: 0.2393 (0.2718)  time: 0.1729  data: 0.0002  max mem: 6052
[06:06:49.014583] Epoch: [7]  [380/781]  eta: 0:01:10  lr: 0.000250  training_loss: 2.5589 (2.6720)  mae_loss: 4.0295 (4.0052)  classification_loss: 2.1202 (2.1319)  loss_mask: 0.2108 (0.2700)  time: 0.1746  data: 0.0002  max mem: 6052
[06:06:52.502457] Epoch: [7]  [400/781]  eta: 0:01:06  lr: 0.000250  training_loss: 2.6620 (2.6750)  mae_loss: 4.0222 (4.0083)  classification_loss: 2.1240 (2.1315)  loss_mask: 0.2713 (0.2717)  time: 0.1743  data: 0.0002  max mem: 6052
[06:06:55.961637] Epoch: [7]  [420/781]  eta: 0:01:03  lr: 0.000250  training_loss: 2.5112 (2.6687)  mae_loss: 4.0862 (4.0097)  classification_loss: 2.1033 (2.1316)  loss_mask: 0.1821 (0.2686)  time: 0.1729  data: 0.0003  max mem: 6052
[06:06:59.411085] Epoch: [7]  [440/781]  eta: 0:00:59  lr: 0.000250  training_loss: 2.5027 (2.6602)  mae_loss: 3.9321 (4.0072)  classification_loss: 2.0983 (2.1312)  loss_mask: 0.1826 (0.2645)  time: 0.1724  data: 0.0004  max mem: 6052
[06:07:02.860313] Epoch: [7]  [460/781]  eta: 0:00:55  lr: 0.000250  training_loss: 2.4968 (2.6551)  mae_loss: 3.9392 (4.0042)  classification_loss: 2.1181 (2.1308)  loss_mask: 0.1928 (0.2622)  time: 0.1724  data: 0.0002  max mem: 6052
[06:07:06.316219] Epoch: [7]  [480/781]  eta: 0:00:52  lr: 0.000250  training_loss: 2.5469 (2.6527)  mae_loss: 3.9521 (4.0014)  classification_loss: 2.1275 (2.1306)  loss_mask: 0.2172 (0.2610)  time: 0.1727  data: 0.0002  max mem: 6052
[06:07:09.763696] Epoch: [7]  [500/781]  eta: 0:00:48  lr: 0.000250  training_loss: 2.5544 (2.6497)  mae_loss: 3.8977 (3.9973)  classification_loss: 2.1251 (2.1303)  loss_mask: 0.2218 (0.2597)  time: 0.1723  data: 0.0002  max mem: 6052
[06:07:13.223515] Epoch: [7]  [520/781]  eta: 0:00:45  lr: 0.000250  training_loss: 2.5273 (2.6445)  mae_loss: 3.8518 (3.9911)  classification_loss: 2.1381 (2.1306)  loss_mask: 0.1850 (0.2569)  time: 0.1729  data: 0.0002  max mem: 6052
[06:07:16.663415] Epoch: [7]  [540/781]  eta: 0:00:41  lr: 0.000250  training_loss: 2.4521 (2.6380)  mae_loss: 3.8660 (3.9868)  classification_loss: 2.1210 (2.1303)  loss_mask: 0.1715 (0.2538)  time: 0.1719  data: 0.0002  max mem: 6052
[06:07:20.124928] Epoch: [7]  [560/781]  eta: 0:00:38  lr: 0.000249  training_loss: 2.5477 (2.6367)  mae_loss: 3.9345 (3.9852)  classification_loss: 2.1240 (2.1304)  loss_mask: 0.2186 (0.2531)  time: 0.1730  data: 0.0002  max mem: 6052
[06:07:23.595982] Epoch: [7]  [580/781]  eta: 0:00:34  lr: 0.000249  training_loss: 2.4499 (2.6315)  mae_loss: 3.9028 (3.9842)  classification_loss: 2.1180 (2.1303)  loss_mask: 0.1741 (0.2506)  time: 0.1735  data: 0.0002  max mem: 6052
[06:07:27.040430] Epoch: [7]  [600/781]  eta: 0:00:31  lr: 0.000249  training_loss: 2.4311 (2.6249)  mae_loss: 3.8511 (3.9809)  classification_loss: 2.0896 (2.1296)  loss_mask: 0.1604 (0.2476)  time: 0.1721  data: 0.0002  max mem: 6052
[06:07:30.511599] Epoch: [7]  [620/781]  eta: 0:00:28  lr: 0.000249  training_loss: 2.3994 (2.6187)  mae_loss: 3.8433 (3.9764)  classification_loss: 2.1055 (2.1288)  loss_mask: 0.1509 (0.2450)  time: 0.1735  data: 0.0003  max mem: 6052
[06:07:33.987163] Epoch: [7]  [640/781]  eta: 0:00:24  lr: 0.000249  training_loss: 2.3428 (2.6111)  mae_loss: 3.8676 (3.9744)  classification_loss: 2.1234 (2.1288)  loss_mask: 0.1179 (0.2412)  time: 0.1737  data: 0.0003  max mem: 6052
[06:07:37.423557] Epoch: [7]  [660/781]  eta: 0:00:21  lr: 0.000249  training_loss: 2.4050 (2.6077)  mae_loss: 3.8917 (3.9723)  classification_loss: 2.1265 (2.1289)  loss_mask: 0.1444 (0.2394)  time: 0.1717  data: 0.0003  max mem: 6052
[06:07:40.882701] Epoch: [7]  [680/781]  eta: 0:00:17  lr: 0.000249  training_loss: 2.6253 (2.6118)  mae_loss: 3.9380 (3.9719)  classification_loss: 2.1267 (2.1290)  loss_mask: 0.2885 (0.2414)  time: 0.1729  data: 0.0003  max mem: 6052
[06:07:44.356812] Epoch: [7]  [700/781]  eta: 0:00:14  lr: 0.000249  training_loss: 2.4641 (2.6080)  mae_loss: 3.9211 (3.9715)  classification_loss: 2.1232 (2.1287)  loss_mask: 0.1546 (0.2397)  time: 0.1736  data: 0.0002  max mem: 6052
[06:07:47.822390] Epoch: [7]  [720/781]  eta: 0:00:10  lr: 0.000249  training_loss: 2.4290 (2.6040)  mae_loss: 3.8428 (3.9691)  classification_loss: 2.1351 (2.1291)  loss_mask: 0.1599 (0.2375)  time: 0.1732  data: 0.0002  max mem: 6052
[06:07:51.298582] Epoch: [7]  [740/781]  eta: 0:00:07  lr: 0.000249  training_loss: 2.3981 (2.5991)  mae_loss: 3.8952 (3.9668)  classification_loss: 2.1248 (2.1290)  loss_mask: 0.1462 (0.2351)  time: 0.1737  data: 0.0002  max mem: 6052
[06:07:54.774649] Epoch: [7]  [760/781]  eta: 0:00:03  lr: 0.000249  training_loss: 2.4485 (2.5961)  mae_loss: 3.8725 (3.9648)  classification_loss: 2.1393 (2.1294)  loss_mask: 0.1566 (0.2334)  time: 0.1737  data: 0.0002  max mem: 6052
[06:07:58.206030] Epoch: [7]  [780/781]  eta: 0:00:00  lr: 0.000249  training_loss: 2.4404 (2.5938)  mae_loss: 3.8498 (3.9621)  classification_loss: 2.1240 (2.1291)  loss_mask: 0.1724 (0.2323)  time: 0.1715  data: 0.0002  max mem: 6052
[06:07:58.369442] Epoch: [7] Total time: 0:02:15 (0.1741 s / it)
[06:07:58.370170] Averaged stats: lr: 0.000249  training_loss: 2.4404 (2.5938)  mae_loss: 3.8498 (3.9621)  classification_loss: 2.1240 (2.1291)  loss_mask: 0.1724 (0.2323)
[06:07:59.033967] Test:  [  0/157]  eta: 0:01:43  testing_loss: 1.8610 (1.8610)  acc1: 34.3750 (34.3750)  acc5: 87.5000 (87.5000)  time: 0.6586  data: 0.6252  max mem: 6052
[06:07:59.324394] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.8817 (1.8837)  acc1: 34.3750 (34.3750)  acc5: 87.5000 (87.7841)  time: 0.0861  data: 0.0570  max mem: 6052
[06:07:59.615006] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.8708 (1.8642)  acc1: 34.3750 (36.5327)  acc5: 89.0625 (88.0952)  time: 0.0289  data: 0.0002  max mem: 6052
[06:07:59.902006] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.8372 (1.8582)  acc1: 39.0625 (37.4496)  acc5: 89.0625 (87.7520)  time: 0.0287  data: 0.0002  max mem: 6052
[06:08:00.191027] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.8446 (1.8635)  acc1: 37.5000 (37.4619)  acc5: 87.5000 (87.4238)  time: 0.0287  data: 0.0002  max mem: 6052
[06:08:00.478379] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.8623 (1.8629)  acc1: 35.9375 (37.6225)  acc5: 87.5000 (87.4694)  time: 0.0286  data: 0.0002  max mem: 6052
[06:08:00.763482] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.8410 (1.8592)  acc1: 37.5000 (37.7561)  acc5: 87.5000 (87.6281)  time: 0.0284  data: 0.0002  max mem: 6052
[06:08:01.048459] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.8341 (1.8580)  acc1: 37.5000 (37.7861)  acc5: 89.0625 (87.8961)  time: 0.0284  data: 0.0003  max mem: 6052
[06:08:01.334176] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.8385 (1.8581)  acc1: 35.9375 (37.6543)  acc5: 90.6250 (88.1173)  time: 0.0284  data: 0.0003  max mem: 6052
[06:08:01.622107] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.8784 (1.8598)  acc1: 34.3750 (37.3283)  acc5: 87.5000 (88.0666)  time: 0.0285  data: 0.0002  max mem: 6052
[06:08:01.911153] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.8835 (1.8624)  acc1: 34.3750 (37.2679)  acc5: 85.9375 (87.8249)  time: 0.0287  data: 0.0002  max mem: 6052
[06:08:02.199357] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.8862 (1.8643)  acc1: 35.9375 (37.1903)  acc5: 87.5000 (87.8801)  time: 0.0287  data: 0.0002  max mem: 6052
[06:08:02.489882] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.8582 (1.8623)  acc1: 35.9375 (37.3709)  acc5: 89.0625 (88.0682)  time: 0.0288  data: 0.0002  max mem: 6052
[06:08:02.787702] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.8586 (1.8632)  acc1: 37.5000 (37.4284)  acc5: 89.0625 (88.0129)  time: 0.0292  data: 0.0002  max mem: 6052
[06:08:03.077544] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.8668 (1.8631)  acc1: 35.9375 (37.3338)  acc5: 87.5000 (88.1095)  time: 0.0292  data: 0.0002  max mem: 6052
[06:08:03.364699] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.8516 (1.8627)  acc1: 37.5000 (37.3655)  acc5: 87.5000 (88.0070)  time: 0.0287  data: 0.0002  max mem: 6052
[06:08:03.518179] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.8516 (1.8622)  acc1: 37.5000 (37.3000)  acc5: 87.5000 (87.9700)  time: 0.0276  data: 0.0002  max mem: 6052
[06:08:03.696141] Test: Total time: 0:00:05 (0.0339 s / it)
[06:08:03.696933] * Acc@1 37.300 Acc@5 87.970 loss 1.862
[06:08:03.697223] Accuracy of the network on the 10000 test images: 37.3%
[06:08:03.697489] Max accuracy: 37.30%
[06:08:03.870059] log_dir: ./output_dir
[06:08:04.726968] Epoch: [8]  [  0/781]  eta: 0:11:07  lr: 0.000249  training_loss: 2.4539 (2.4539)  mae_loss: 3.7851 (3.7851)  classification_loss: 2.0802 (2.0802)  loss_mask: 0.1868 (0.1868)  time: 0.8551  data: 0.6602  max mem: 6052
[06:08:08.223646] Epoch: [8]  [ 20/781]  eta: 0:02:37  lr: 0.000249  training_loss: 2.5133 (2.5891)  mae_loss: 3.9850 (3.9489)  classification_loss: 2.1097 (2.1202)  loss_mask: 0.2069 (0.2345)  time: 0.1747  data: 0.0002  max mem: 6052
[06:08:11.700552] Epoch: [8]  [ 40/781]  eta: 0:02:21  lr: 0.000249  training_loss: 2.4703 (2.5538)  mae_loss: 3.8998 (3.9391)  classification_loss: 2.1044 (2.1199)  loss_mask: 0.1771 (0.2169)  time: 0.1738  data: 0.0002  max mem: 6052
[06:08:15.161704] Epoch: [8]  [ 60/781]  eta: 0:02:13  lr: 0.000249  training_loss: 2.4353 (2.5173)  mae_loss: 3.9566 (3.9632)  classification_loss: 2.0596 (2.1113)  loss_mask: 0.1656 (0.2030)  time: 0.1730  data: 0.0002  max mem: 6052
[06:08:18.620936] Epoch: [8]  [ 80/781]  eta: 0:02:07  lr: 0.000249  training_loss: 2.4011 (2.4971)  mae_loss: 3.8966 (3.9343)  classification_loss: 2.1298 (2.1170)  loss_mask: 0.1351 (0.1900)  time: 0.1729  data: 0.0002  max mem: 6052
[06:08:22.068928] Epoch: [8]  [100/781]  eta: 0:02:02  lr: 0.000249  training_loss: 2.4068 (2.4810)  mae_loss: 3.7854 (3.9082)  classification_loss: 2.1267 (2.1195)  loss_mask: 0.1257 (0.1807)  time: 0.1723  data: 0.0002  max mem: 6052
[06:08:25.547166] Epoch: [8]  [120/781]  eta: 0:01:58  lr: 0.000249  training_loss: 2.3479 (2.4651)  mae_loss: 3.8708 (3.8999)  classification_loss: 2.0943 (2.1162)  loss_mask: 0.1215 (0.1744)  time: 0.1738  data: 0.0002  max mem: 6052
[06:08:28.998482] Epoch: [8]  [140/781]  eta: 0:01:54  lr: 0.000249  training_loss: 2.4469 (2.4642)  mae_loss: 3.8778 (3.8919)  classification_loss: 2.0825 (2.1109)  loss_mask: 0.1729 (0.1767)  time: 0.1725  data: 0.0002  max mem: 6052
[06:08:32.455326] Epoch: [8]  [160/781]  eta: 0:01:50  lr: 0.000249  training_loss: 2.4279 (2.4593)  mae_loss: 3.9267 (3.9001)  classification_loss: 2.1341 (2.1128)  loss_mask: 0.1354 (0.1732)  time: 0.1728  data: 0.0002  max mem: 6052
[06:08:35.912399] Epoch: [8]  [180/781]  eta: 0:01:46  lr: 0.000249  training_loss: 2.4192 (2.4546)  mae_loss: 3.9624 (3.9094)  classification_loss: 2.1287 (2.1122)  loss_mask: 0.1401 (0.1712)  time: 0.1728  data: 0.0002  max mem: 6052
[06:08:39.399354] Epoch: [8]  [200/781]  eta: 0:01:42  lr: 0.000249  training_loss: 2.4536 (2.4583)  mae_loss: 3.8034 (3.9051)  classification_loss: 2.0949 (2.1113)  loss_mask: 0.1800 (0.1735)  time: 0.1743  data: 0.0004  max mem: 6052
[06:08:42.844500] Epoch: [8]  [220/781]  eta: 0:01:38  lr: 0.000249  training_loss: 2.5214 (2.4735)  mae_loss: 3.9670 (3.9064)  classification_loss: 2.0777 (2.1104)  loss_mask: 0.2254 (0.1816)  time: 0.1721  data: 0.0002  max mem: 6052
[06:08:46.295223] Epoch: [8]  [240/781]  eta: 0:01:35  lr: 0.000249  training_loss: 2.5362 (2.4856)  mae_loss: 3.9503 (3.9087)  classification_loss: 2.1178 (2.1111)  loss_mask: 0.1915 (0.1872)  time: 0.1725  data: 0.0002  max mem: 6052
[06:08:49.761233] Epoch: [8]  [260/781]  eta: 0:01:31  lr: 0.000249  training_loss: 2.4280 (2.4808)  mae_loss: 3.9244 (3.9135)  classification_loss: 2.0991 (2.1103)  loss_mask: 0.1528 (0.1853)  time: 0.1732  data: 0.0002  max mem: 6052
[06:08:53.235801] Epoch: [8]  [280/781]  eta: 0:01:27  lr: 0.000249  training_loss: 2.3742 (2.4755)  mae_loss: 3.9368 (3.9124)  classification_loss: 2.0832 (2.1091)  loss_mask: 0.1498 (0.1832)  time: 0.1736  data: 0.0002  max mem: 6052
[06:08:56.731371] Epoch: [8]  [300/781]  eta: 0:01:24  lr: 0.000249  training_loss: 2.4082 (2.4739)  mae_loss: 4.0412 (3.9201)  classification_loss: 2.0904 (2.1088)  loss_mask: 0.1403 (0.1825)  time: 0.1747  data: 0.0002  max mem: 6052
[06:09:00.216056] Epoch: [8]  [320/781]  eta: 0:01:20  lr: 0.000249  training_loss: 2.3935 (2.4697)  mae_loss: 3.8757 (3.9184)  classification_loss: 2.1101 (2.1087)  loss_mask: 0.1340 (0.1805)  time: 0.1741  data: 0.0002  max mem: 6052
[06:09:03.719786] Epoch: [8]  [340/781]  eta: 0:01:17  lr: 0.000249  training_loss: 2.3848 (2.4639)  mae_loss: 3.7831 (3.9119)  classification_loss: 2.0946 (2.1077)  loss_mask: 0.1209 (0.1781)  time: 0.1751  data: 0.0002  max mem: 6052
[06:09:07.171911] Epoch: [8]  [360/781]  eta: 0:01:13  lr: 0.000249  training_loss: 2.3379 (2.4561)  mae_loss: 3.9073 (3.9123)  classification_loss: 2.0950 (2.1074)  loss_mask: 0.1053 (0.1743)  time: 0.1725  data: 0.0002  max mem: 6052
[06:09:10.659039] Epoch: [8]  [380/781]  eta: 0:01:10  lr: 0.000249  training_loss: 2.3664 (2.4518)  mae_loss: 3.8683 (3.9107)  classification_loss: 2.0959 (2.1069)  loss_mask: 0.1313 (0.1724)  time: 0.1743  data: 0.0002  max mem: 6052
[06:09:14.136723] Epoch: [8]  [400/781]  eta: 0:01:06  lr: 0.000249  training_loss: 2.3651 (2.4493)  mae_loss: 3.8358 (3.9079)  classification_loss: 2.1086 (2.1079)  loss_mask: 0.1046 (0.1707)  time: 0.1738  data: 0.0003  max mem: 6052
[06:09:17.593283] Epoch: [8]  [420/781]  eta: 0:01:03  lr: 0.000249  training_loss: 2.2939 (2.4440)  mae_loss: 3.8495 (3.9051)  classification_loss: 2.0787 (2.1071)  loss_mask: 0.1008 (0.1684)  time: 0.1727  data: 0.0002  max mem: 6052
[06:09:21.069593] Epoch: [8]  [440/781]  eta: 0:00:59  lr: 0.000249  training_loss: 2.3433 (2.4414)  mae_loss: 3.8936 (3.9030)  classification_loss: 2.0925 (2.1064)  loss_mask: 0.1330 (0.1675)  time: 0.1737  data: 0.0004  max mem: 6052
[06:09:24.587901] Epoch: [8]  [460/781]  eta: 0:00:56  lr: 0.000249  training_loss: 2.4196 (2.4416)  mae_loss: 3.8401 (3.9009)  classification_loss: 2.0971 (2.1063)  loss_mask: 0.1553 (0.1676)  time: 0.1758  data: 0.0003  max mem: 6052
[06:09:28.041370] Epoch: [8]  [480/781]  eta: 0:00:52  lr: 0.000249  training_loss: 2.3419 (2.4394)  mae_loss: 3.8782 (3.8992)  classification_loss: 2.1003 (2.1062)  loss_mask: 0.1238 (0.1666)  time: 0.1726  data: 0.0002  max mem: 6052
[06:09:31.508737] Epoch: [8]  [500/781]  eta: 0:00:49  lr: 0.000249  training_loss: 2.5235 (2.4430)  mae_loss: 3.8168 (3.8984)  classification_loss: 2.0886 (2.1061)  loss_mask: 0.2009 (0.1684)  time: 0.1733  data: 0.0003  max mem: 6052
[06:09:34.967434] Epoch: [8]  [520/781]  eta: 0:00:45  lr: 0.000249  training_loss: 2.3543 (2.4401)  mae_loss: 3.9263 (3.8977)  classification_loss: 2.0864 (2.1056)  loss_mask: 0.1234 (0.1673)  time: 0.1728  data: 0.0002  max mem: 6052
[06:09:38.419161] Epoch: [8]  [540/781]  eta: 0:00:42  lr: 0.000249  training_loss: 2.3866 (2.4404)  mae_loss: 3.8152 (3.8957)  classification_loss: 2.0861 (2.1054)  loss_mask: 0.1170 (0.1675)  time: 0.1725  data: 0.0002  max mem: 6052
[06:09:41.860559] Epoch: [8]  [560/781]  eta: 0:00:38  lr: 0.000249  training_loss: 2.3282 (2.4372)  mae_loss: 3.9617 (3.8990)  classification_loss: 2.1006 (2.1055)  loss_mask: 0.1084 (0.1658)  time: 0.1720  data: 0.0002  max mem: 6052
[06:09:45.324493] Epoch: [8]  [580/781]  eta: 0:00:35  lr: 0.000249  training_loss: 2.3175 (2.4337)  mae_loss: 3.8462 (3.8989)  classification_loss: 2.1069 (2.1059)  loss_mask: 0.0997 (0.1639)  time: 0.1731  data: 0.0002  max mem: 6052
[06:09:48.774612] Epoch: [8]  [600/781]  eta: 0:00:31  lr: 0.000249  training_loss: 2.3106 (2.4297)  mae_loss: 3.7185 (3.8944)  classification_loss: 2.0998 (2.1057)  loss_mask: 0.0948 (0.1620)  time: 0.1724  data: 0.0002  max mem: 6052
[06:09:52.234423] Epoch: [8]  [620/781]  eta: 0:00:28  lr: 0.000249  training_loss: 2.3114 (2.4267)  mae_loss: 3.8427 (3.8931)  classification_loss: 2.0802 (2.1050)  loss_mask: 0.1128 (0.1608)  time: 0.1729  data: 0.0002  max mem: 6052
[06:09:55.708423] Epoch: [8]  [640/781]  eta: 0:00:24  lr: 0.000249  training_loss: 2.3685 (2.4243)  mae_loss: 3.9958 (3.8964)  classification_loss: 2.0885 (2.1046)  loss_mask: 0.1241 (0.1598)  time: 0.1736  data: 0.0003  max mem: 6052
[06:09:59.201040] Epoch: [8]  [660/781]  eta: 0:00:21  lr: 0.000249  training_loss: 2.4604 (2.4274)  mae_loss: 3.8271 (3.8951)  classification_loss: 2.0726 (2.1041)  loss_mask: 0.2056 (0.1617)  time: 0.1745  data: 0.0002  max mem: 6052
[06:10:02.670138] Epoch: [8]  [680/781]  eta: 0:00:17  lr: 0.000249  training_loss: 2.3120 (2.4249)  mae_loss: 3.8376 (3.8934)  classification_loss: 2.0873 (2.1036)  loss_mask: 0.1241 (0.1607)  time: 0.1734  data: 0.0002  max mem: 6052
[06:10:06.138938] Epoch: [8]  [700/781]  eta: 0:00:14  lr: 0.000249  training_loss: 2.2948 (2.4235)  mae_loss: 3.8089 (3.8910)  classification_loss: 2.1170 (2.1036)  loss_mask: 0.1241 (0.1600)  time: 0.1734  data: 0.0002  max mem: 6052
[06:10:09.598602] Epoch: [8]  [720/781]  eta: 0:00:10  lr: 0.000249  training_loss: 2.4061 (2.4238)  mae_loss: 3.8593 (3.8904)  classification_loss: 2.1100 (2.1038)  loss_mask: 0.1269 (0.1600)  time: 0.1729  data: 0.0003  max mem: 6052
[06:10:13.058849] Epoch: [8]  [740/781]  eta: 0:00:07  lr: 0.000249  training_loss: 2.3403 (2.4209)  mae_loss: 3.7860 (3.8890)  classification_loss: 2.1038 (2.1036)  loss_mask: 0.1069 (0.1587)  time: 0.1729  data: 0.0003  max mem: 6052
[06:10:16.546791] Epoch: [8]  [760/781]  eta: 0:00:03  lr: 0.000249  training_loss: 2.3551 (2.4189)  mae_loss: 3.9193 (3.8896)  classification_loss: 2.1034 (2.1037)  loss_mask: 0.1020 (0.1576)  time: 0.1743  data: 0.0002  max mem: 6052
[06:10:20.012137] Epoch: [8]  [780/781]  eta: 0:00:00  lr: 0.000249  training_loss: 2.3199 (2.4185)  mae_loss: 3.8716 (3.8898)  classification_loss: 2.0802 (2.1036)  loss_mask: 0.1204 (0.1575)  time: 0.1731  data: 0.0002  max mem: 6052
[06:10:20.189174] Epoch: [8] Total time: 0:02:16 (0.1745 s / it)
[06:10:20.189662] Averaged stats: lr: 0.000249  training_loss: 2.3199 (2.4185)  mae_loss: 3.8716 (3.8898)  classification_loss: 2.0802 (2.1036)  loss_mask: 0.1204 (0.1575)
[06:10:20.789953] Test:  [  0/157]  eta: 0:01:33  testing_loss: 1.7740 (1.7740)  acc1: 42.1875 (42.1875)  acc5: 89.0625 (89.0625)  time: 0.5960  data: 0.5643  max mem: 6052
[06:10:21.080281] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 1.7719 (1.7680)  acc1: 39.0625 (38.7784)  acc5: 89.0625 (89.9148)  time: 0.0804  data: 0.0515  max mem: 6052
[06:10:21.366200] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.7278 (1.7421)  acc1: 40.6250 (41.0714)  acc5: 89.0625 (90.2530)  time: 0.0286  data: 0.0003  max mem: 6052
[06:10:21.652886] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 1.7160 (1.7334)  acc1: 43.7500 (41.8851)  acc5: 89.0625 (90.0202)  time: 0.0285  data: 0.0003  max mem: 6052
[06:10:21.950026] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 1.7270 (1.7411)  acc1: 40.6250 (41.4253)  acc5: 89.0625 (89.6723)  time: 0.0291  data: 0.0002  max mem: 6052
[06:10:22.237949] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.7380 (1.7394)  acc1: 40.6250 (41.8505)  acc5: 89.0625 (89.6752)  time: 0.0291  data: 0.0004  max mem: 6052
[06:10:22.524002] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.7210 (1.7360)  acc1: 40.6250 (41.8801)  acc5: 89.0625 (89.6516)  time: 0.0286  data: 0.0003  max mem: 6052
[06:10:22.810073] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.7174 (1.7359)  acc1: 40.6250 (41.5713)  acc5: 90.6250 (89.8548)  time: 0.0285  data: 0.0002  max mem: 6052
[06:10:23.096632] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.7333 (1.7361)  acc1: 39.0625 (41.4931)  acc5: 92.1875 (89.9691)  time: 0.0285  data: 0.0002  max mem: 6052
[06:10:23.385545] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.7414 (1.7375)  acc1: 37.5000 (41.3462)  acc5: 90.6250 (89.9382)  time: 0.0286  data: 0.0002  max mem: 6052
[06:10:23.670779] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.7638 (1.7411)  acc1: 37.5000 (41.2438)  acc5: 90.6250 (89.8979)  time: 0.0286  data: 0.0002  max mem: 6052
[06:10:23.956741] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.7651 (1.7438)  acc1: 37.5000 (40.9488)  acc5: 90.6250 (89.9634)  time: 0.0284  data: 0.0002  max mem: 6052
[06:10:24.242150] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.7432 (1.7409)  acc1: 39.0625 (41.1544)  acc5: 90.6250 (90.0310)  time: 0.0284  data: 0.0002  max mem: 6052
[06:10:24.527208] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.7213 (1.7423)  acc1: 42.1875 (41.1856)  acc5: 89.0625 (89.8974)  time: 0.0284  data: 0.0002  max mem: 6052
[06:10:24.819287] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.7438 (1.7429)  acc1: 42.1875 (41.1458)  acc5: 89.0625 (89.8715)  time: 0.0287  data: 0.0002  max mem: 6052
[06:10:25.104208] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.7431 (1.7421)  acc1: 43.7500 (41.2976)  acc5: 89.0625 (89.7868)  time: 0.0287  data: 0.0002  max mem: 6052
[06:10:25.256460] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.7290 (1.7414)  acc1: 42.1875 (41.0900)  acc5: 89.0625 (89.7900)  time: 0.0274  data: 0.0002  max mem: 6052
[06:10:25.417830] Test: Total time: 0:00:05 (0.0333 s / it)
[06:10:25.418295] * Acc@1 41.090 Acc@5 89.790 loss 1.741
[06:10:25.418585] Accuracy of the network on the 10000 test images: 41.1%
[06:10:25.418812] Max accuracy: 41.09%
[06:10:25.561166] log_dir: ./output_dir
[06:10:26.363747] Epoch: [9]  [  0/781]  eta: 0:10:25  lr: 0.000249  training_loss: 2.4204 (2.4204)  mae_loss: 4.0135 (4.0135)  classification_loss: 2.1105 (2.1105)  loss_mask: 0.1550 (0.1550)  time: 0.8005  data: 0.6155  max mem: 6052
[06:10:29.845964] Epoch: [9]  [ 20/781]  eta: 0:02:35  lr: 0.000249  training_loss: 2.3219 (2.3593)  mae_loss: 3.8974 (3.9248)  classification_loss: 2.1040 (2.1112)  loss_mask: 0.1144 (0.1240)  time: 0.1739  data: 0.0002  max mem: 6052
[06:10:33.323428] Epoch: [9]  [ 40/781]  eta: 0:02:20  lr: 0.000249  training_loss: 2.2953 (2.3348)  mae_loss: 3.9405 (3.9090)  classification_loss: 2.0601 (2.0906)  loss_mask: 0.1124 (0.1221)  time: 0.1738  data: 0.0002  max mem: 6052
[06:10:36.808174] Epoch: [9]  [ 60/781]  eta: 0:02:12  lr: 0.000249  training_loss: 2.3208 (2.3387)  mae_loss: 3.9404 (3.9226)  classification_loss: 2.1041 (2.0899)  loss_mask: 0.1139 (0.1244)  time: 0.1741  data: 0.0002  max mem: 6052
[06:10:40.313027] Epoch: [9]  [ 80/781]  eta: 0:02:07  lr: 0.000249  training_loss: 2.3174 (2.3467)  mae_loss: 3.8782 (3.9215)  classification_loss: 2.1046 (2.0958)  loss_mask: 0.1053 (0.1255)  time: 0.1752  data: 0.0002  max mem: 6052
[06:10:43.793273] Epoch: [9]  [100/781]  eta: 0:02:02  lr: 0.000249  training_loss: 2.3109 (2.3447)  mae_loss: 3.8228 (3.9015)  classification_loss: 2.0757 (2.0940)  loss_mask: 0.1157 (0.1253)  time: 0.1739  data: 0.0002  max mem: 6052
[06:10:47.267793] Epoch: [9]  [120/781]  eta: 0:01:58  lr: 0.000249  training_loss: 2.2485 (2.3326)  mae_loss: 3.8626 (3.8956)  classification_loss: 2.0746 (2.0919)  loss_mask: 0.0912 (0.1203)  time: 0.1736  data: 0.0002  max mem: 6052
[06:10:50.735183] Epoch: [9]  [140/781]  eta: 0:01:54  lr: 0.000249  training_loss: 2.1991 (2.3183)  mae_loss: 3.8253 (3.8913)  classification_loss: 2.0223 (2.0871)  loss_mask: 0.0807 (0.1156)  time: 0.1733  data: 0.0002  max mem: 6052
[06:10:54.202728] Epoch: [9]  [160/781]  eta: 0:01:50  lr: 0.000249  training_loss: 2.2653 (2.3138)  mae_loss: 3.8122 (3.8825)  classification_loss: 2.0741 (2.0858)  loss_mask: 0.0866 (0.1140)  time: 0.1733  data: 0.0002  max mem: 6052
[06:10:57.674015] Epoch: [9]  [180/781]  eta: 0:01:46  lr: 0.000249  training_loss: 2.3548 (2.3211)  mae_loss: 3.8661 (3.8821)  classification_loss: 2.0912 (2.0862)  loss_mask: 0.1142 (0.1175)  time: 0.1734  data: 0.0003  max mem: 6052
[06:11:01.146247] Epoch: [9]  [200/781]  eta: 0:01:42  lr: 0.000249  training_loss: 2.4523 (2.3419)  mae_loss: 3.9475 (3.8893)  classification_loss: 2.1303 (2.0902)  loss_mask: 0.1682 (0.1258)  time: 0.1735  data: 0.0002  max mem: 6052
[06:11:04.652158] Epoch: [9]  [220/781]  eta: 0:01:39  lr: 0.000249  training_loss: 2.3021 (2.3380)  mae_loss: 3.7746 (3.8830)  classification_loss: 2.0635 (2.0895)  loss_mask: 0.0952 (0.1243)  time: 0.1752  data: 0.0002  max mem: 6052
[06:11:08.124441] Epoch: [9]  [240/781]  eta: 0:01:35  lr: 0.000249  training_loss: 2.3569 (2.3397)  mae_loss: 3.9399 (3.8852)  classification_loss: 2.0690 (2.0895)  loss_mask: 0.1220 (0.1251)  time: 0.1735  data: 0.0002  max mem: 6052
[06:11:11.650025] Epoch: [9]  [260/781]  eta: 0:01:31  lr: 0.000249  training_loss: 2.3731 (2.3446)  mae_loss: 3.9074 (3.8875)  classification_loss: 2.0640 (2.0873)  loss_mask: 0.1634 (0.1287)  time: 0.1762  data: 0.0002  max mem: 6052
[06:11:15.135752] Epoch: [9]  [280/781]  eta: 0:01:28  lr: 0.000249  training_loss: 2.3359 (2.3511)  mae_loss: 3.9202 (3.8910)  classification_loss: 2.1306 (2.0905)  loss_mask: 0.1277 (0.1303)  time: 0.1742  data: 0.0003  max mem: 6052
[06:11:18.626343] Epoch: [9]  [300/781]  eta: 0:01:24  lr: 0.000249  training_loss: 2.3031 (2.3475)  mae_loss: 3.7949 (3.8876)  classification_loss: 2.0523 (2.0886)  loss_mask: 0.1251 (0.1294)  time: 0.1744  data: 0.0003  max mem: 6052
[06:11:22.089984] Epoch: [9]  [320/781]  eta: 0:01:21  lr: 0.000249  training_loss: 2.3059 (2.3494)  mae_loss: 3.7917 (3.8821)  classification_loss: 2.0983 (2.0890)  loss_mask: 0.1204 (0.1302)  time: 0.1731  data: 0.0002  max mem: 6052
[06:11:25.588699] Epoch: [9]  [340/781]  eta: 0:01:17  lr: 0.000249  training_loss: 2.4042 (2.3529)  mae_loss: 3.9212 (3.8841)  classification_loss: 2.0857 (2.0892)  loss_mask: 0.1524 (0.1319)  time: 0.1748  data: 0.0002  max mem: 6052
[06:11:29.078440] Epoch: [9]  [360/781]  eta: 0:01:14  lr: 0.000249  training_loss: 2.3245 (2.3527)  mae_loss: 3.9037 (3.8844)  classification_loss: 2.1043 (2.0901)  loss_mask: 0.1118 (0.1313)  time: 0.1744  data: 0.0002  max mem: 6052
[06:11:32.557144] Epoch: [9]  [380/781]  eta: 0:01:10  lr: 0.000249  training_loss: 2.3747 (2.3550)  mae_loss: 4.0597 (3.8938)  classification_loss: 2.0706 (2.0895)  loss_mask: 0.1387 (0.1327)  time: 0.1738  data: 0.0002  max mem: 6052
[06:11:36.029565] Epoch: [9]  [400/781]  eta: 0:01:06  lr: 0.000249  training_loss: 2.2956 (2.3552)  mae_loss: 3.9404 (3.8970)  classification_loss: 2.0634 (2.0895)  loss_mask: 0.1104 (0.1329)  time: 0.1735  data: 0.0002  max mem: 6052
[06:11:39.529514] Epoch: [9]  [420/781]  eta: 0:01:03  lr: 0.000249  training_loss: 2.2907 (2.3515)  mae_loss: 3.8937 (3.8974)  classification_loss: 2.0373 (2.0877)  loss_mask: 0.1153 (0.1319)  time: 0.1749  data: 0.0003  max mem: 6052
[06:11:43.040320] Epoch: [9]  [440/781]  eta: 0:00:59  lr: 0.000249  training_loss: 2.2994 (2.3494)  mae_loss: 4.0618 (3.9056)  classification_loss: 2.0799 (2.0875)  loss_mask: 0.1051 (0.1309)  time: 0.1754  data: 0.0003  max mem: 6052
[06:11:46.538056] Epoch: [9]  [460/781]  eta: 0:00:56  lr: 0.000249  training_loss: 2.2630 (2.3474)  mae_loss: 3.8968 (3.9052)  classification_loss: 2.0669 (2.0865)  loss_mask: 0.1171 (0.1304)  time: 0.1748  data: 0.0002  max mem: 6052
[06:11:50.000972] Epoch: [9]  [480/781]  eta: 0:00:52  lr: 0.000249  training_loss: 2.4354 (2.3525)  mae_loss: 3.9503 (3.9058)  classification_loss: 2.1155 (2.0873)  loss_mask: 0.1509 (0.1326)  time: 0.1731  data: 0.0002  max mem: 6052
[06:11:53.476659] Epoch: [9]  [500/781]  eta: 0:00:49  lr: 0.000249  training_loss: 2.4228 (2.3554)  mae_loss: 3.8321 (3.9056)  classification_loss: 2.1039 (2.0879)  loss_mask: 0.1438 (0.1337)  time: 0.1737  data: 0.0006  max mem: 6052
[06:11:56.927822] Epoch: [9]  [520/781]  eta: 0:00:45  lr: 0.000249  training_loss: 2.3228 (2.3562)  mae_loss: 3.8809 (3.9057)  classification_loss: 2.0834 (2.0879)  loss_mask: 0.1414 (0.1341)  time: 0.1725  data: 0.0002  max mem: 6052
[06:12:00.404696] Epoch: [9]  [540/781]  eta: 0:00:42  lr: 0.000249  training_loss: 2.3736 (2.3574)  mae_loss: 3.8804 (3.9050)  classification_loss: 2.1343 (2.0896)  loss_mask: 0.1226 (0.1339)  time: 0.1738  data: 0.0002  max mem: 6052
[06:12:03.895387] Epoch: [9]  [560/781]  eta: 0:00:38  lr: 0.000248  training_loss: 2.2819 (2.3558)  mae_loss: 3.8803 (3.9052)  classification_loss: 2.0944 (2.0892)  loss_mask: 0.1093 (0.1333)  time: 0.1744  data: 0.0003  max mem: 6052
[06:12:07.351237] Epoch: [9]  [580/781]  eta: 0:00:35  lr: 0.000248  training_loss: 2.3896 (2.3586)  mae_loss: 4.0029 (3.9080)  classification_loss: 2.1353 (2.0908)  loss_mask: 0.1344 (0.1339)  time: 0.1727  data: 0.0002  max mem: 6052
[06:12:10.824816] Epoch: [9]  [600/781]  eta: 0:00:31  lr: 0.000248  training_loss: 2.3520 (2.3592)  mae_loss: 3.8968 (3.9088)  classification_loss: 2.0772 (2.0908)  loss_mask: 0.1276 (0.1342)  time: 0.1736  data: 0.0003  max mem: 6052
[06:12:14.295244] Epoch: [9]  [620/781]  eta: 0:00:28  lr: 0.000248  training_loss: 2.4697 (2.3636)  mae_loss: 3.8693 (3.9091)  classification_loss: 2.0588 (2.0907)  loss_mask: 0.1441 (0.1364)  time: 0.1734  data: 0.0002  max mem: 6052
[06:12:17.781328] Epoch: [9]  [640/781]  eta: 0:00:24  lr: 0.000248  training_loss: 2.3179 (2.3622)  mae_loss: 3.8463 (3.9080)  classification_loss: 2.0805 (2.0901)  loss_mask: 0.1172 (0.1360)  time: 0.1742  data: 0.0002  max mem: 6052
[06:12:21.248457] Epoch: [9]  [660/781]  eta: 0:00:21  lr: 0.000248  training_loss: 2.3644 (2.3626)  mae_loss: 3.8348 (3.9064)  classification_loss: 2.0769 (2.0895)  loss_mask: 0.1382 (0.1365)  time: 0.1733  data: 0.0002  max mem: 6052
[06:12:24.743101] Epoch: [9]  [680/781]  eta: 0:00:17  lr: 0.000248  training_loss: 2.2725 (2.3607)  mae_loss: 3.9829 (3.9081)  classification_loss: 2.0719 (2.0891)  loss_mask: 0.0975 (0.1358)  time: 0.1746  data: 0.0002  max mem: 6052
[06:12:28.201990] Epoch: [9]  [700/781]  eta: 0:00:14  lr: 0.000248  training_loss: 2.1802 (2.3569)  mae_loss: 3.8479 (3.9059)  classification_loss: 2.0507 (2.0883)  loss_mask: 0.0831 (0.1343)  time: 0.1729  data: 0.0003  max mem: 6052
[06:12:31.700684] Epoch: [9]  [720/781]  eta: 0:00:10  lr: 0.000248  training_loss: 2.2564 (2.3548)  mae_loss: 3.8270 (3.9043)  classification_loss: 2.0491 (2.0881)  loss_mask: 0.0767 (0.1334)  time: 0.1748  data: 0.0003  max mem: 6052
[06:12:35.175477] Epoch: [9]  [740/781]  eta: 0:00:07  lr: 0.000248  training_loss: 2.2643 (2.3529)  mae_loss: 3.9651 (3.9053)  classification_loss: 2.0619 (2.0877)  loss_mask: 0.0865 (0.1326)  time: 0.1736  data: 0.0003  max mem: 6052
[06:12:38.641919] Epoch: [9]  [760/781]  eta: 0:00:03  lr: 0.000248  training_loss: 2.3465 (2.3540)  mae_loss: 4.0056 (3.9089)  classification_loss: 2.0726 (2.0876)  loss_mask: 0.1166 (0.1332)  time: 0.1732  data: 0.0002  max mem: 6052
[06:12:42.115633] Epoch: [9]  [780/781]  eta: 0:00:00  lr: 0.000248  training_loss: 2.3773 (2.3565)  mae_loss: 3.9547 (3.9102)  classification_loss: 2.0724 (2.0873)  loss_mask: 0.1499 (0.1346)  time: 0.1736  data: 0.0002  max mem: 6052
[06:12:42.313538] Epoch: [9] Total time: 0:02:16 (0.1751 s / it)
[06:12:42.314183] Averaged stats: lr: 0.000248  training_loss: 2.3773 (2.3565)  mae_loss: 3.9547 (3.9102)  classification_loss: 2.0724 (2.0873)  loss_mask: 0.1499 (0.1346)
[06:12:42.954870] Test:  [  0/157]  eta: 0:01:39  testing_loss: 1.7529 (1.7529)  acc1: 37.5000 (37.5000)  acc5: 85.9375 (85.9375)  time: 0.6358  data: 0.6021  max mem: 6052
[06:12:43.274546] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.7514 (1.7594)  acc1: 39.0625 (38.7784)  acc5: 90.6250 (89.2045)  time: 0.0867  data: 0.0566  max mem: 6052
[06:12:43.558901] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.7454 (1.7400)  acc1: 40.6250 (40.7738)  acc5: 90.6250 (89.5089)  time: 0.0300  data: 0.0011  max mem: 6052
[06:12:43.844291] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.7269 (1.7296)  acc1: 43.7500 (41.5827)  acc5: 89.0625 (89.4153)  time: 0.0284  data: 0.0002  max mem: 6052
[06:12:44.132650] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.7337 (1.7391)  acc1: 42.1875 (41.3110)  acc5: 89.0625 (88.9482)  time: 0.0285  data: 0.0003  max mem: 6052
[06:12:44.431382] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.7445 (1.7376)  acc1: 39.0625 (41.5135)  acc5: 90.6250 (89.1850)  time: 0.0292  data: 0.0005  max mem: 6052
[06:12:44.722956] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.7115 (1.7340)  acc1: 42.1875 (41.7008)  acc5: 89.0625 (89.1906)  time: 0.0294  data: 0.0004  max mem: 6052
[06:12:45.013661] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.7002 (1.7332)  acc1: 42.1875 (41.3952)  acc5: 90.6250 (89.3926)  time: 0.0289  data: 0.0002  max mem: 6052
[06:12:45.303378] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.7002 (1.7319)  acc1: 40.6250 (41.3002)  acc5: 90.6250 (89.6412)  time: 0.0288  data: 0.0002  max mem: 6052
[06:12:45.592998] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.7374 (1.7345)  acc1: 37.5000 (41.0027)  acc5: 90.6250 (89.5261)  time: 0.0287  data: 0.0002  max mem: 6052
[06:12:45.882263] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.7588 (1.7376)  acc1: 37.5000 (40.7488)  acc5: 87.5000 (89.4183)  time: 0.0288  data: 0.0002  max mem: 6052
[06:12:46.178064] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.7661 (1.7408)  acc1: 35.9375 (40.4702)  acc5: 89.0625 (89.5270)  time: 0.0290  data: 0.0002  max mem: 6052
[06:12:46.470266] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.7375 (1.7394)  acc1: 35.9375 (40.3151)  acc5: 89.0625 (89.5919)  time: 0.0292  data: 0.0002  max mem: 6052
[06:12:46.764391] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.7219 (1.7404)  acc1: 39.0625 (40.4461)  acc5: 89.0625 (89.4084)  time: 0.0291  data: 0.0002  max mem: 6052
[06:12:47.054175] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.7229 (1.7405)  acc1: 42.1875 (40.3812)  acc5: 89.0625 (89.4947)  time: 0.0290  data: 0.0002  max mem: 6052
[06:12:47.338361] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.7253 (1.7401)  acc1: 42.1875 (40.5112)  acc5: 89.0625 (89.4454)  time: 0.0285  data: 0.0002  max mem: 6052
[06:12:47.491279] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.7253 (1.7402)  acc1: 39.0625 (40.4300)  acc5: 89.0625 (89.4500)  time: 0.0273  data: 0.0002  max mem: 6052
[06:12:47.646159] Test: Total time: 0:00:05 (0.0339 s / it)
[06:12:47.647280] * Acc@1 40.430 Acc@5 89.450 loss 1.740
[06:12:47.647632] Accuracy of the network on the 10000 test images: 40.4%
[06:12:47.647849] Max accuracy: 41.09%
[06:12:47.751428] log_dir: ./output_dir
[06:12:48.656716] Epoch: [10]  [  0/781]  eta: 0:11:45  lr: 0.000248  training_loss: 2.4450 (2.4450)  mae_loss: 3.9562 (3.9562)  classification_loss: 2.0729 (2.0729)  loss_mask: 0.1860 (0.1860)  time: 0.9034  data: 0.7048  max mem: 6052
[06:12:52.122249] Epoch: [10]  [ 20/781]  eta: 0:02:38  lr: 0.000248  training_loss: 2.2979 (2.3117)  mae_loss: 3.8615 (3.8876)  classification_loss: 2.0839 (2.0746)  loss_mask: 0.1108 (0.1185)  time: 0.1732  data: 0.0002  max mem: 6052
[06:12:55.578536] Epoch: [10]  [ 40/781]  eta: 0:02:21  lr: 0.000248  training_loss: 2.2088 (2.2632)  mae_loss: 3.9796 (3.9343)  classification_loss: 2.0483 (2.0653)  loss_mask: 0.0717 (0.0989)  time: 0.1727  data: 0.0002  max mem: 6052
[06:12:59.039097] Epoch: [10]  [ 60/781]  eta: 0:02:13  lr: 0.000248  training_loss: 2.3235 (2.2852)  mae_loss: 3.9007 (3.9239)  classification_loss: 2.0843 (2.0746)  loss_mask: 0.0933 (0.1053)  time: 0.1729  data: 0.0002  max mem: 6052
[06:13:02.527333] Epoch: [10]  [ 80/781]  eta: 0:02:07  lr: 0.000248  training_loss: 2.2809 (2.3020)  mae_loss: 3.8392 (3.9024)  classification_loss: 2.0805 (2.0804)  loss_mask: 0.1027 (0.1108)  time: 0.1743  data: 0.0002  max mem: 6052
[06:13:06.004113] Epoch: [10]  [100/781]  eta: 0:02:02  lr: 0.000248  training_loss: 2.2792 (2.2954)  mae_loss: 3.8473 (3.8986)  classification_loss: 2.0551 (2.0775)  loss_mask: 0.1086 (0.1090)  time: 0.1737  data: 0.0003  max mem: 6052
[06:13:09.481114] Epoch: [10]  [120/781]  eta: 0:01:58  lr: 0.000248  training_loss: 2.2131 (2.2842)  mae_loss: 3.9773 (3.9087)  classification_loss: 2.0416 (2.0748)  loss_mask: 0.0771 (0.1047)  time: 0.1738  data: 0.0005  max mem: 6052
[06:13:12.947418] Epoch: [10]  [140/781]  eta: 0:01:54  lr: 0.000248  training_loss: 2.2095 (2.2784)  mae_loss: 3.9512 (3.9132)  classification_loss: 2.0489 (2.0715)  loss_mask: 0.0888 (0.1034)  time: 0.1732  data: 0.0002  max mem: 6052
[06:13:16.402277] Epoch: [10]  [160/781]  eta: 0:01:50  lr: 0.000248  training_loss: 2.2698 (2.2773)  mae_loss: 3.8006 (3.9034)  classification_loss: 2.0472 (2.0711)  loss_mask: 0.1032 (0.1031)  time: 0.1726  data: 0.0002  max mem: 6052
[06:13:19.870180] Epoch: [10]  [180/781]  eta: 0:01:46  lr: 0.000248  training_loss: 2.2487 (2.2777)  mae_loss: 3.8901 (3.9039)  classification_loss: 2.0681 (2.0722)  loss_mask: 0.0922 (0.1028)  time: 0.1733  data: 0.0002  max mem: 6052
[06:13:23.354237] Epoch: [10]  [200/781]  eta: 0:01:42  lr: 0.000248  training_loss: 2.2732 (2.2805)  mae_loss: 3.9252 (3.9082)  classification_loss: 2.0768 (2.0716)  loss_mask: 0.0844 (0.1044)  time: 0.1741  data: 0.0003  max mem: 6052
[06:13:26.815241] Epoch: [10]  [220/781]  eta: 0:01:39  lr: 0.000248  training_loss: 2.3203 (2.2865)  mae_loss: 3.8242 (3.9032)  classification_loss: 2.0632 (2.0729)  loss_mask: 0.1221 (0.1068)  time: 0.1730  data: 0.0002  max mem: 6052
[06:13:30.295114] Epoch: [10]  [240/781]  eta: 0:01:35  lr: 0.000248  training_loss: 2.2849 (2.2889)  mae_loss: 3.8832 (3.9021)  classification_loss: 2.0797 (2.0729)  loss_mask: 0.0911 (0.1080)  time: 0.1739  data: 0.0002  max mem: 6052
[06:13:33.796095] Epoch: [10]  [260/781]  eta: 0:01:31  lr: 0.000248  training_loss: 2.3198 (2.2926)  mae_loss: 3.9428 (3.9065)  classification_loss: 2.0644 (2.0720)  loss_mask: 0.1217 (0.1103)  time: 0.1750  data: 0.0002  max mem: 6052
[06:13:37.259982] Epoch: [10]  [280/781]  eta: 0:01:28  lr: 0.000248  training_loss: 2.2311 (2.2907)  mae_loss: 3.8584 (3.9057)  classification_loss: 2.0477 (2.0712)  loss_mask: 0.0797 (0.1097)  time: 0.1731  data: 0.0002  max mem: 6052
[06:13:40.719944] Epoch: [10]  [300/781]  eta: 0:01:24  lr: 0.000248  training_loss: 2.2324 (2.2879)  mae_loss: 3.9058 (3.9105)  classification_loss: 2.0543 (2.0703)  loss_mask: 0.0887 (0.1088)  time: 0.1729  data: 0.0003  max mem: 6052
[06:13:44.182972] Epoch: [10]  [320/781]  eta: 0:01:20  lr: 0.000248  training_loss: 2.1990 (2.2823)  mae_loss: 3.9210 (3.9139)  classification_loss: 2.0442 (2.0683)  loss_mask: 0.0731 (0.1070)  time: 0.1731  data: 0.0002  max mem: 6052
[06:13:47.699439] Epoch: [10]  [340/781]  eta: 0:01:17  lr: 0.000248  training_loss: 2.2295 (2.2802)  mae_loss: 3.8091 (3.9072)  classification_loss: 2.0240 (2.0667)  loss_mask: 0.0818 (0.1068)  time: 0.1757  data: 0.0002  max mem: 6052
[06:13:51.192482] Epoch: [10]  [360/781]  eta: 0:01:13  lr: 0.000248  training_loss: 2.2143 (2.2781)  mae_loss: 3.8747 (3.9070)  classification_loss: 2.0497 (2.0668)  loss_mask: 0.0807 (0.1056)  time: 0.1746  data: 0.0002  max mem: 6052
[06:13:54.657471] Epoch: [10]  [380/781]  eta: 0:01:10  lr: 0.000248  training_loss: 2.1879 (2.2738)  mae_loss: 3.9211 (3.9059)  classification_loss: 2.0309 (2.0655)  loss_mask: 0.0703 (0.1042)  time: 0.1732  data: 0.0002  max mem: 6052
[06:13:58.130251] Epoch: [10]  [400/781]  eta: 0:01:06  lr: 0.000248  training_loss: 2.2235 (2.2725)  mae_loss: 3.8850 (3.9060)  classification_loss: 2.0383 (2.0641)  loss_mask: 0.0697 (0.1042)  time: 0.1735  data: 0.0002  max mem: 6052
[06:14:01.588558] Epoch: [10]  [420/781]  eta: 0:01:03  lr: 0.000248  training_loss: 2.2523 (2.2720)  mae_loss: 3.8606 (3.9038)  classification_loss: 2.0166 (2.0628)  loss_mask: 0.0983 (0.1046)  time: 0.1728  data: 0.0002  max mem: 6052
[06:14:05.057918] Epoch: [10]  [440/781]  eta: 0:00:59  lr: 0.000248  training_loss: 2.3045 (2.2752)  mae_loss: 3.9895 (3.9101)  classification_loss: 2.0545 (2.0627)  loss_mask: 0.1188 (0.1062)  time: 0.1733  data: 0.0003  max mem: 6052
[06:14:08.537764] Epoch: [10]  [460/781]  eta: 0:00:56  lr: 0.000248  training_loss: 2.2345 (2.2739)  mae_loss: 3.8412 (3.9078)  classification_loss: 2.0158 (2.0617)  loss_mask: 0.0894 (0.1061)  time: 0.1739  data: 0.0002  max mem: 6052
[06:14:12.002824] Epoch: [10]  [480/781]  eta: 0:00:52  lr: 0.000248  training_loss: 2.2190 (2.2721)  mae_loss: 3.8497 (3.9069)  classification_loss: 2.0360 (2.0621)  loss_mask: 0.0681 (0.1050)  time: 0.1732  data: 0.0002  max mem: 6052
[06:14:15.461151] Epoch: [10]  [500/781]  eta: 0:00:49  lr: 0.000248  training_loss: 2.1941 (2.2726)  mae_loss: 3.8524 (3.9039)  classification_loss: 2.0385 (2.0617)  loss_mask: 0.0891 (0.1054)  time: 0.1728  data: 0.0002  max mem: 6052
[06:14:18.928526] Epoch: [10]  [520/781]  eta: 0:00:45  lr: 0.000248  training_loss: 2.3481 (2.2737)  mae_loss: 3.9167 (3.9038)  classification_loss: 2.0037 (2.0604)  loss_mask: 0.1363 (0.1067)  time: 0.1733  data: 0.0003  max mem: 6052
[06:14:22.386126] Epoch: [10]  [540/781]  eta: 0:00:42  lr: 0.000248  training_loss: 2.3519 (2.2783)  mae_loss: 3.8570 (3.9015)  classification_loss: 2.0585 (2.0609)  loss_mask: 0.1479 (0.1087)  time: 0.1728  data: 0.0002  max mem: 6052
[06:14:25.828079] Epoch: [10]  [560/781]  eta: 0:00:38  lr: 0.000248  training_loss: 2.3084 (2.2786)  mae_loss: 3.8758 (3.9016)  classification_loss: 2.0940 (2.0617)  loss_mask: 0.1035 (0.1084)  time: 0.1720  data: 0.0002  max mem: 6052
[06:14:29.279430] Epoch: [10]  [580/781]  eta: 0:00:35  lr: 0.000248  training_loss: 2.2312 (2.2774)  mae_loss: 3.8317 (3.8998)  classification_loss: 2.0728 (2.0617)  loss_mask: 0.0872 (0.1079)  time: 0.1725  data: 0.0002  max mem: 6052
[06:14:32.744066] Epoch: [10]  [600/781]  eta: 0:00:31  lr: 0.000248  training_loss: 2.1904 (2.2757)  mae_loss: 3.8074 (3.8987)  classification_loss: 2.0367 (2.0611)  loss_mask: 0.0836 (0.1073)  time: 0.1731  data: 0.0002  max mem: 6052
[06:14:36.224887] Epoch: [10]  [620/781]  eta: 0:00:28  lr: 0.000248  training_loss: 2.3595 (2.2789)  mae_loss: 3.9071 (3.8997)  classification_loss: 2.0403 (2.0607)  loss_mask: 0.1358 (0.1091)  time: 0.1739  data: 0.0002  max mem: 6052
[06:14:39.701137] Epoch: [10]  [640/781]  eta: 0:00:24  lr: 0.000248  training_loss: 2.3238 (2.2802)  mae_loss: 3.8828 (3.8993)  classification_loss: 2.1167 (2.0629)  loss_mask: 0.0910 (0.1087)  time: 0.1737  data: 0.0002  max mem: 6052
[06:14:43.170224] Epoch: [10]  [660/781]  eta: 0:00:21  lr: 0.000248  training_loss: 2.2904 (2.2814)  mae_loss: 3.8666 (3.8987)  classification_loss: 2.0846 (2.0637)  loss_mask: 0.0898 (0.1089)  time: 0.1733  data: 0.0002  max mem: 6052
[06:14:46.636133] Epoch: [10]  [680/781]  eta: 0:00:17  lr: 0.000248  training_loss: 2.2310 (2.2816)  mae_loss: 3.8186 (3.8976)  classification_loss: 2.0561 (2.0634)  loss_mask: 0.1092 (0.1091)  time: 0.1731  data: 0.0004  max mem: 6052
[06:14:50.115850] Epoch: [10]  [700/781]  eta: 0:00:14  lr: 0.000248  training_loss: 2.2874 (2.2818)  mae_loss: 3.8287 (3.8962)  classification_loss: 2.0481 (2.0634)  loss_mask: 0.0970 (0.1092)  time: 0.1739  data: 0.0002  max mem: 6052
[06:14:53.589643] Epoch: [10]  [720/781]  eta: 0:00:10  lr: 0.000248  training_loss: 2.3346 (2.2856)  mae_loss: 3.8471 (3.8952)  classification_loss: 2.0495 (2.0636)  loss_mask: 0.1416 (0.1110)  time: 0.1736  data: 0.0003  max mem: 6052
[06:14:57.057776] Epoch: [10]  [740/781]  eta: 0:00:07  lr: 0.000248  training_loss: 2.2933 (2.2860)  mae_loss: 3.8708 (3.8946)  classification_loss: 2.0465 (2.0637)  loss_mask: 0.1116 (0.1112)  time: 0.1733  data: 0.0004  max mem: 6052
[06:15:00.520724] Epoch: [10]  [760/781]  eta: 0:00:03  lr: 0.000248  training_loss: 2.2553 (2.2856)  mae_loss: 3.8192 (3.8932)  classification_loss: 2.0444 (2.0636)  loss_mask: 0.0906 (0.1110)  time: 0.1731  data: 0.0003  max mem: 6052
[06:15:03.963253] Epoch: [10]  [780/781]  eta: 0:00:00  lr: 0.000248  training_loss: 2.2160 (2.2832)  mae_loss: 3.7480 (3.8903)  classification_loss: 2.0341 (2.0633)  loss_mask: 0.0584 (0.1100)  time: 0.1720  data: 0.0002  max mem: 6052
[06:15:04.146378] Epoch: [10] Total time: 0:02:16 (0.1746 s / it)
[06:15:04.147161] Averaged stats: lr: 0.000248  training_loss: 2.2160 (2.2832)  mae_loss: 3.7480 (3.8903)  classification_loss: 2.0341 (2.0633)  loss_mask: 0.0584 (0.1100)
[06:15:05.126411] Test:  [  0/157]  eta: 0:01:30  testing_loss: 1.6732 (1.6732)  acc1: 43.7500 (43.7500)  acc5: 89.0625 (89.0625)  time: 0.5778  data: 0.5389  max mem: 6052
[06:15:05.414940] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 1.6535 (1.6860)  acc1: 45.3125 (42.8977)  acc5: 92.1875 (90.7670)  time: 0.0785  data: 0.0491  max mem: 6052
[06:15:05.704254] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.6437 (1.6587)  acc1: 43.7500 (44.6429)  acc5: 92.1875 (91.3690)  time: 0.0287  data: 0.0003  max mem: 6052
[06:15:05.993927] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 1.6420 (1.6531)  acc1: 43.7500 (44.9597)  acc5: 90.6250 (90.8266)  time: 0.0288  data: 0.0003  max mem: 6052
[06:15:06.289805] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 1.6536 (1.6633)  acc1: 43.7500 (44.3216)  acc5: 89.0625 (90.4345)  time: 0.0291  data: 0.0002  max mem: 6052
[06:15:06.578309] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.6499 (1.6596)  acc1: 43.7500 (44.7304)  acc5: 90.6250 (90.5637)  time: 0.0291  data: 0.0002  max mem: 6052
[06:15:06.867568] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.6279 (1.6557)  acc1: 45.3125 (44.3648)  acc5: 90.6250 (90.6506)  time: 0.0287  data: 0.0002  max mem: 6052
[06:15:07.154447] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.6170 (1.6522)  acc1: 43.7500 (44.6523)  acc5: 92.1875 (90.8891)  time: 0.0287  data: 0.0002  max mem: 6052
[06:15:07.446697] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.6241 (1.6533)  acc1: 45.3125 (44.6952)  acc5: 92.1875 (90.9915)  time: 0.0288  data: 0.0002  max mem: 6052
[06:15:07.735375] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.6736 (1.6552)  acc1: 43.7500 (44.5055)  acc5: 89.0625 (90.8654)  time: 0.0289  data: 0.0002  max mem: 6052
[06:15:08.023307] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.6947 (1.6593)  acc1: 42.1875 (44.1213)  acc5: 89.0625 (90.5941)  time: 0.0287  data: 0.0004  max mem: 6052
[06:15:08.315438] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.6947 (1.6611)  acc1: 42.1875 (43.9611)  acc5: 89.0625 (90.5405)  time: 0.0289  data: 0.0003  max mem: 6052
[06:15:08.612275] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.6660 (1.6589)  acc1: 42.1875 (43.8017)  acc5: 90.6250 (90.6637)  time: 0.0293  data: 0.0002  max mem: 6052
[06:15:08.904017] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.6421 (1.6595)  acc1: 43.7500 (43.9051)  acc5: 90.6250 (90.6250)  time: 0.0293  data: 0.0002  max mem: 6052
[06:15:09.189281] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.6545 (1.6584)  acc1: 45.3125 (44.0270)  acc5: 90.6250 (90.7469)  time: 0.0287  data: 0.0002  max mem: 6052
[06:15:09.471639] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.6525 (1.6578)  acc1: 45.3125 (44.2777)  acc5: 90.6250 (90.6560)  time: 0.0282  data: 0.0001  max mem: 6052
[06:15:09.626323] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.6509 (1.6574)  acc1: 45.3125 (44.1700)  acc5: 89.0625 (90.6300)  time: 0.0273  data: 0.0001  max mem: 6052
[06:15:09.797765] Test: Total time: 0:00:05 (0.0334 s / it)
[06:15:09.798400] * Acc@1 44.170 Acc@5 90.630 loss 1.657
[06:15:09.798796] Accuracy of the network on the 10000 test images: 44.2%
[06:15:09.799039] Max accuracy: 44.17%
[06:15:09.932547] log_dir: ./output_dir
[06:15:10.728833] Epoch: [11]  [  0/781]  eta: 0:10:20  lr: 0.000248  training_loss: 2.0352 (2.0352)  mae_loss: 3.8075 (3.8075)  classification_loss: 1.9667 (1.9667)  loss_mask: 0.0343 (0.0343)  time: 0.7943  data: 0.6062  max mem: 6052
[06:15:14.220187] Epoch: [11]  [ 20/781]  eta: 0:02:35  lr: 0.000248  training_loss: 2.1937 (2.2223)  mae_loss: 3.7872 (3.8164)  classification_loss: 2.0173 (2.0285)  loss_mask: 0.0764 (0.0969)  time: 0.1745  data: 0.0002  max mem: 6052
[06:15:17.742161] Epoch: [11]  [ 40/781]  eta: 0:02:21  lr: 0.000248  training_loss: 2.3188 (2.2739)  mae_loss: 3.9090 (3.8822)  classification_loss: 2.0270 (2.0293)  loss_mask: 0.1271 (0.1223)  time: 0.1760  data: 0.0002  max mem: 6052
[06:15:21.223326] Epoch: [11]  [ 60/781]  eta: 0:02:13  lr: 0.000247  training_loss: 2.2168 (2.2607)  mae_loss: 3.9156 (3.8824)  classification_loss: 2.0243 (2.0308)  loss_mask: 0.0782 (0.1149)  time: 0.1740  data: 0.0002  max mem: 6052
[06:15:24.692121] Epoch: [11]  [ 80/781]  eta: 0:02:07  lr: 0.000247  training_loss: 2.2376 (2.2632)  mae_loss: 3.8836 (3.8830)  classification_loss: 2.0418 (2.0354)  loss_mask: 0.0982 (0.1139)  time: 0.1734  data: 0.0002  max mem: 6052
[06:15:28.186433] Epoch: [11]  [100/781]  eta: 0:02:03  lr: 0.000247  training_loss: 2.2247 (2.2596)  mae_loss: 3.8313 (3.8754)  classification_loss: 2.0388 (2.0381)  loss_mask: 0.0817 (0.1107)  time: 0.1746  data: 0.0002  max mem: 6052
[06:15:31.644531] Epoch: [11]  [120/781]  eta: 0:01:58  lr: 0.000247  training_loss: 2.1886 (2.2478)  mae_loss: 3.8506 (3.8765)  classification_loss: 1.9945 (2.0333)  loss_mask: 0.0844 (0.1073)  time: 0.1728  data: 0.0002  max mem: 6052
[06:15:35.112178] Epoch: [11]  [140/781]  eta: 0:01:54  lr: 0.000247  training_loss: 2.0970 (2.2314)  mae_loss: 3.8734 (3.8750)  classification_loss: 1.9980 (2.0302)  loss_mask: 0.0540 (0.1006)  time: 0.1733  data: 0.0004  max mem: 6052
[06:15:38.604467] Epoch: [11]  [160/781]  eta: 0:01:50  lr: 0.000247  training_loss: 2.2088 (2.2284)  mae_loss: 3.7832 (3.8638)  classification_loss: 2.0631 (2.0341)  loss_mask: 0.0500 (0.0971)  time: 0.1745  data: 0.0002  max mem: 6052
[06:15:42.061389] Epoch: [11]  [180/781]  eta: 0:01:46  lr: 0.000247  training_loss: 2.2175 (2.2270)  mae_loss: 3.8897 (3.8714)  classification_loss: 2.0452 (2.0350)  loss_mask: 0.0699 (0.0960)  time: 0.1728  data: 0.0002  max mem: 6052
[06:15:45.553347] Epoch: [11]  [200/781]  eta: 0:01:42  lr: 0.000247  training_loss: 2.2893 (2.2384)  mae_loss: 3.8701 (3.8729)  classification_loss: 2.0436 (2.0350)  loss_mask: 0.1086 (0.1017)  time: 0.1745  data: 0.0003  max mem: 6052
[06:15:49.012381] Epoch: [11]  [220/781]  eta: 0:01:39  lr: 0.000247  training_loss: 2.2251 (2.2374)  mae_loss: 3.8232 (3.8706)  classification_loss: 2.0277 (2.0351)  loss_mask: 0.0997 (0.1011)  time: 0.1729  data: 0.0003  max mem: 6052
[06:15:52.478474] Epoch: [11]  [240/781]  eta: 0:01:35  lr: 0.000247  training_loss: 2.1566 (2.2321)  mae_loss: 3.8178 (3.8667)  classification_loss: 2.0205 (2.0351)  loss_mask: 0.0635 (0.0985)  time: 0.1732  data: 0.0002  max mem: 6052
[06:15:55.933516] Epoch: [11]  [260/781]  eta: 0:01:31  lr: 0.000247  training_loss: 2.1579 (2.2268)  mae_loss: 3.8964 (3.8645)  classification_loss: 1.9806 (2.0314)  loss_mask: 0.0671 (0.0977)  time: 0.1726  data: 0.0003  max mem: 6052
[06:15:59.392111] Epoch: [11]  [280/781]  eta: 0:01:28  lr: 0.000247  training_loss: 2.2505 (2.2291)  mae_loss: 3.7902 (3.8589)  classification_loss: 2.0816 (2.0347)  loss_mask: 0.0860 (0.0972)  time: 0.1728  data: 0.0002  max mem: 6052
[06:16:02.852323] Epoch: [11]  [300/781]  eta: 0:01:24  lr: 0.000247  training_loss: 2.2488 (2.2301)  mae_loss: 3.8680 (3.8577)  classification_loss: 2.0406 (2.0368)  loss_mask: 0.0598 (0.0966)  time: 0.1729  data: 0.0003  max mem: 6052
[06:16:06.306343] Epoch: [11]  [320/781]  eta: 0:01:20  lr: 0.000247  training_loss: 2.2621 (2.2353)  mae_loss: 3.9069 (3.8626)  classification_loss: 2.0664 (2.0398)  loss_mask: 0.0830 (0.0978)  time: 0.1726  data: 0.0002  max mem: 6052
[06:16:09.777969] Epoch: [11]  [340/781]  eta: 0:01:17  lr: 0.000247  training_loss: 2.3070 (2.2407)  mae_loss: 3.8682 (3.8643)  classification_loss: 2.0559 (2.0408)  loss_mask: 0.1208 (0.0999)  time: 0.1735  data: 0.0003  max mem: 6052
[06:16:13.242652] Epoch: [11]  [360/781]  eta: 0:01:13  lr: 0.000247  training_loss: 2.2975 (2.2482)  mae_loss: 3.7956 (3.8616)  classification_loss: 2.0729 (2.0431)  loss_mask: 0.1229 (0.1026)  time: 0.1731  data: 0.0002  max mem: 6052
[06:16:16.732496] Epoch: [11]  [380/781]  eta: 0:01:10  lr: 0.000247  training_loss: 2.2457 (2.2480)  mae_loss: 3.9456 (3.8635)  classification_loss: 2.0392 (2.0431)  loss_mask: 0.0772 (0.1025)  time: 0.1744  data: 0.0002  max mem: 6052
[06:16:20.209651] Epoch: [11]  [400/781]  eta: 0:01:06  lr: 0.000247  training_loss: 2.2236 (2.2495)  mae_loss: 3.8906 (3.8640)  classification_loss: 2.0424 (2.0439)  loss_mask: 0.0873 (0.1028)  time: 0.1738  data: 0.0002  max mem: 6052
[06:16:23.697144] Epoch: [11]  [420/781]  eta: 0:01:03  lr: 0.000247  training_loss: 2.1406 (2.2447)  mae_loss: 3.9048 (3.8652)  classification_loss: 2.0139 (2.0430)  loss_mask: 0.0603 (0.1009)  time: 0.1743  data: 0.0002  max mem: 6052
[06:16:27.160325] Epoch: [11]  [440/781]  eta: 0:00:59  lr: 0.000247  training_loss: 2.1749 (2.2417)  mae_loss: 3.8919 (3.8658)  classification_loss: 2.0304 (2.0430)  loss_mask: 0.0657 (0.0994)  time: 0.1731  data: 0.0002  max mem: 6052
[06:16:30.608874] Epoch: [11]  [460/781]  eta: 0:00:56  lr: 0.000247  training_loss: 2.1680 (2.2390)  mae_loss: 3.8668 (3.8670)  classification_loss: 1.9856 (2.0418)  loss_mask: 0.0551 (0.0986)  time: 0.1723  data: 0.0002  max mem: 6052
[06:16:31.067920] [06:16:31.068406] [06:16:31.068579] [06:16:31.068757] [06:16:31.068968] [06:16:31.069163] [06:16:31.069323] [06:16:31.069506] [06:16:31.069662]
Traceback (most recent call last):
  File "/notebooks/CVPR2023/main_two_branch_new.py", line 370, in <module>
    main(args)
  File "/notebooks/CVPR2023/main_two_branch_new.py", line 322, in main
    train_stats = train_one_epoch(
  File "/notebooks/CVPR2023/engine_two_branch.py", line 73, in train_one_epoch
    loss_scaler(loss, optimizer, clip_grad=max_norm,
  File "/notebooks/CVPR2023/util/misc.py", line 267, in __call__
    self._scaler.step(optimizer)
  File "/usr/local/lib/python3.9/dist-packages/torch/cuda/amp/grad_scaler.py", line 338, in step
    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
  File "/usr/local/lib/python3.9/dist-packages/torch/cuda/amp/grad_scaler.py", line 284, in _maybe_opt_step
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
  File "/usr/local/lib/python3.9/dist-packages/torch/cuda/amp/grad_scaler.py", line 284, in <genexpr>
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
KeyboardInterrupt