Not using distributed mode
[02:58:23.709713] job dir: /notebooks/CVPR2023
[02:58:23.709970] Namespace(batch_size=64,
epochs=100,
accum_iter=1,
model='mae_vit_tiny',
norm_pix_loss=False,
dataset='c10',
input_size=32,
patch_size=2,
mask_ratio=0.75,
lambda_weight=0.1,
drop_path=0.1,
clip_grad=None,
weight_decay=0.05,
lr=None,
blr=0.001,
layer_decay=0.75,
min_lr=1e-06,
warmup_epochs=5,
color_jitter=None,
aa='rand-m9-mstd0.5-inc1',
smoothing=0.1,
reprob=0.25,
remode='pixel',
recount=1,
resplit=False,
mixup=0,
cutmix=0,
cutmix_minmax=None,
mixup_prob=1.0,
mixup_switch_prob=0.5,
mixup_mode='batch',
finetune='',
global_pool=True,
data_path='/datasets01/imagenet_full_size/061417/',
nb_classes=10,
output_dir='./output_dir',
log_dir='./output_dir',
device='cuda',
seed=0,
resume='',
start_epoch=0,
eval=False,
dist_eval=False,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[02:58:24.733930] Files already downloaded and verified
/usr/local/lib/python3.9/dist-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
[02:58:25.494075] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCropAndInterpolation(size=(32, 32), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic)
               RandomHorizontalFlip(p=0.5)
               RandAugment(n=2, ops=
           	AugmentOp(name=AutoContrast, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Equalize, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Invert, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Rotate, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=PosterizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeAdd, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ColorIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ContrastIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=BrightnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SharpnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearX, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearY, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateXRel, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateYRel, p=0.5, m=9, mstd=0.5))
               ToTensor()
               Normalize(mean=tensor([0.4914, 0.4822, 0.4465]), std=tensor([0.2023, 0.1994, 0.2010]))
               RandomErasing(p=0.25, mode=pixel, count=(1, 1))
           )
[02:58:25.857198] Files already downloaded and verified
[02:58:26.274787] Dataset CIFAR10
    Number of datapoints: 10000
    Root location: ./data
    Split: Test
    StandardTransform
Transform: Compose(
               Resize(size=36, interpolation=bicubic, max_size=None, antialias=None)
               CenterCrop(size=(32, 32))
               ToTensor()
               Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.201))
           )
[02:58:26.275421] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f9aac875ee0>
/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[02:58:31.280434] Model = MaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(2, 2), stride=(2, 2))
    (norm): Identity()
  )
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (7): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (8): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (9): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (10): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (11): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
  (decoder_embed): Linear(in_features=192, out_features=128, bias=True)
  (decoder_blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=128, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=128, out_features=128, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=512, out_features=128, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=128, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=128, out_features=128, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=512, out_features=128, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (decoder_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
  (decoder_pred): Linear(in_features=128, out_features=12, bias=True)
  (head): Linear(in_features=192, out_features=10, bias=True)
  (classifier_mask): Sequential(
    (0): Linear(in_features=192, out_features=5, bias=True)
    (1): LogSoftmax(dim=1)
  )
)
[02:58:31.281251] number of params (M): 5.77
[02:58:31.281455] base lr: 1.00e-03
[02:58:31.281627] actual lr: 2.50e-04
[02:58:31.281772] accumulate grad iterations: 1
[02:58:31.281933] effective batch size: 64
[02:58:31.283325] criterion = LabelSmoothingCrossEntropy()
[02:58:31.283704] Start training for 100 epochs
[02:58:31.285704] log_dir: ./output_dir
[02:58:34.006424] Epoch: [0]  [  0/781]  eta: 0:35:23  lr: 0.000000  training_loss: 4.6201 (4.6201)  classification_loss: 2.6555 (2.6555)  loss_mask: 1.9646 (1.9646)  time: 2.7190  data: 0.4958  max mem: 5042
[02:58:37.392561] Epoch: [0]  [ 20/781]  eta: 0:03:41  lr: 0.000001  training_loss: 4.5132 (4.5312)  classification_loss: 2.5805 (2.5907)  loss_mask: 1.9226 (1.9405)  time: 0.1692  data: 0.0002  max mem: 6050
[02:58:40.766145] Epoch: [0]  [ 40/781]  eta: 0:02:51  lr: 0.000003  training_loss: 4.2611 (4.4132)  classification_loss: 2.4598 (2.5277)  loss_mask: 1.8027 (1.8855)  time: 0.1686  data: 0.0002  max mem: 6050
[02:58:44.139033] Epoch: [0]  [ 60/781]  eta: 0:02:31  lr: 0.000004  training_loss: 4.0673 (4.3019)  classification_loss: 2.3610 (2.4745)  loss_mask: 1.7068 (1.8274)  time: 0.1686  data: 0.0002  max mem: 6050
[02:58:47.516742] Epoch: [0]  [ 80/781]  eta: 0:02:20  lr: 0.000005  training_loss: 4.0019 (4.2291)  classification_loss: 2.3204 (2.4363)  loss_mask: 1.6792 (1.7928)  time: 0.1688  data: 0.0003  max mem: 6050
[02:58:50.891812] Epoch: [0]  [100/781]  eta: 0:02:12  lr: 0.000006  training_loss: 3.9629 (4.1755)  classification_loss: 2.2979 (2.4090)  loss_mask: 1.6629 (1.7665)  time: 0.1687  data: 0.0002  max mem: 6050
[02:58:54.278581] Epoch: [0]  [120/781]  eta: 0:02:05  lr: 0.000008  training_loss: 3.9217 (4.1327)  classification_loss: 2.2915 (2.3899)  loss_mask: 1.6177 (1.7427)  time: 0.1692  data: 0.0003  max mem: 6050
[02:58:57.708944] Epoch: [0]  [140/781]  eta: 0:02:00  lr: 0.000009  training_loss: 3.8965 (4.1002)  classification_loss: 2.2649 (2.3732)  loss_mask: 1.6306 (1.7269)  time: 0.1714  data: 0.0002  max mem: 6050
[02:59:01.111220] Epoch: [0]  [160/781]  eta: 0:01:54  lr: 0.000010  training_loss: 3.8837 (4.0718)  classification_loss: 2.2650 (2.3597)  loss_mask: 1.6140 (1.7121)  time: 0.1700  data: 0.0002  max mem: 6050
[02:59:04.519525] Epoch: [0]  [180/781]  eta: 0:01:50  lr: 0.000012  training_loss: 3.8769 (4.0501)  classification_loss: 2.2657 (2.3496)  loss_mask: 1.5967 (1.7005)  time: 0.1703  data: 0.0002  max mem: 6050
[02:59:07.958926] Epoch: [0]  [200/781]  eta: 0:01:45  lr: 0.000013  training_loss: 3.8603 (4.0299)  classification_loss: 2.2393 (2.3393)  loss_mask: 1.5951 (1.6906)  time: 0.1719  data: 0.0002  max mem: 6050
[02:59:11.359583] Epoch: [0]  [220/781]  eta: 0:01:41  lr: 0.000014  training_loss: 3.8296 (4.0128)  classification_loss: 2.2480 (2.3316)  loss_mask: 1.5844 (1.6812)  time: 0.1700  data: 0.0003  max mem: 6050
[02:59:14.752466] Epoch: [0]  [240/781]  eta: 0:01:37  lr: 0.000015  training_loss: 3.8384 (3.9993)  classification_loss: 2.2662 (2.3262)  loss_mask: 1.5877 (1.6731)  time: 0.1696  data: 0.0001  max mem: 6050
[02:59:18.127514] Epoch: [0]  [260/781]  eta: 0:01:33  lr: 0.000017  training_loss: 3.8363 (3.9858)  classification_loss: 2.2484 (2.3204)  loss_mask: 1.5776 (1.6654)  time: 0.1687  data: 0.0002  max mem: 6050
[02:59:21.531029] Epoch: [0]  [280/781]  eta: 0:01:29  lr: 0.000018  training_loss: 3.7985 (3.9734)  classification_loss: 2.2514 (2.3154)  loss_mask: 1.5644 (1.6580)  time: 0.1701  data: 0.0002  max mem: 6050
[02:59:24.926766] Epoch: [0]  [300/781]  eta: 0:01:25  lr: 0.000019  training_loss: 3.8308 (3.9634)  classification_loss: 2.2600 (2.3116)  loss_mask: 1.5628 (1.6518)  time: 0.1697  data: 0.0003  max mem: 6050
[02:59:28.330523] Epoch: [0]  [320/781]  eta: 0:01:21  lr: 0.000020  training_loss: 3.7824 (3.9534)  classification_loss: 2.2526 (2.3079)  loss_mask: 1.5451 (1.6455)  time: 0.1701  data: 0.0003  max mem: 6050
[02:59:31.775546] Epoch: [0]  [340/781]  eta: 0:01:18  lr: 0.000022  training_loss: 3.7952 (3.9447)  classification_loss: 2.2559 (2.3042)  loss_mask: 1.5540 (1.6405)  time: 0.1722  data: 0.0003  max mem: 6050
[02:59:35.182953] Epoch: [0]  [360/781]  eta: 0:01:14  lr: 0.000023  training_loss: 3.7755 (3.9351)  classification_loss: 2.2564 (2.3008)  loss_mask: 1.5238 (1.6343)  time: 0.1703  data: 0.0003  max mem: 6050
[02:59:38.605327] Epoch: [0]  [380/781]  eta: 0:01:10  lr: 0.000024  training_loss: 3.7675 (3.9276)  classification_loss: 2.2526 (2.2981)  loss_mask: 1.5533 (1.6294)  time: 0.1710  data: 0.0002  max mem: 6050
[02:59:42.007816] Epoch: [0]  [400/781]  eta: 0:01:07  lr: 0.000026  training_loss: 3.7250 (3.9187)  classification_loss: 2.2314 (2.2948)  loss_mask: 1.5103 (1.6239)  time: 0.1701  data: 0.0002  max mem: 6050
[02:59:45.422598] Epoch: [0]  [420/781]  eta: 0:01:03  lr: 0.000027  training_loss: 3.7659 (3.9112)  classification_loss: 2.2555 (2.2933)  loss_mask: 1.4914 (1.6179)  time: 0.1707  data: 0.0003  max mem: 6050
[02:59:48.849830] Epoch: [0]  [440/781]  eta: 0:00:59  lr: 0.000028  training_loss: 3.7297 (3.9037)  classification_loss: 2.2476 (2.2908)  loss_mask: 1.5078 (1.6129)  time: 0.1713  data: 0.0003  max mem: 6050
[02:59:52.260493] Epoch: [0]  [460/781]  eta: 0:00:56  lr: 0.000029  training_loss: 3.6862 (3.8944)  classification_loss: 2.2009 (2.2871)  loss_mask: 1.4724 (1.6073)  time: 0.1705  data: 0.0002  max mem: 6050
[02:59:55.673608] Epoch: [0]  [480/781]  eta: 0:00:52  lr: 0.000031  training_loss: 3.7133 (3.8873)  classification_loss: 2.2468 (2.2858)  loss_mask: 1.4595 (1.6016)  time: 0.1706  data: 0.0003  max mem: 6050
[02:59:59.081553] Epoch: [0]  [500/781]  eta: 0:00:49  lr: 0.000032  training_loss: 3.6705 (3.8792)  classification_loss: 2.2396 (2.2838)  loss_mask: 1.4212 (1.5954)  time: 0.1703  data: 0.0003  max mem: 6050
[03:00:02.504319] Epoch: [0]  [520/781]  eta: 0:00:45  lr: 0.000033  training_loss: 3.6118 (3.8689)  classification_loss: 2.2396 (2.2822)  loss_mask: 1.3724 (1.5867)  time: 0.1711  data: 0.0002  max mem: 6050
[03:00:05.902554] Epoch: [0]  [540/781]  eta: 0:00:42  lr: 0.000035  training_loss: 3.6180 (3.8593)  classification_loss: 2.2332 (2.2809)  loss_mask: 1.3530 (1.5784)  time: 0.1698  data: 0.0002  max mem: 6050
[03:00:09.450695] Epoch: [0]  [560/781]  eta: 0:00:38  lr: 0.000036  training_loss: 3.5007 (3.8474)  classification_loss: 2.2338 (2.2794)  loss_mask: 1.2801 (1.5681)  time: 0.1773  data: 0.0003  max mem: 6050
[03:00:12.983886] Epoch: [0]  [580/781]  eta: 0:00:35  lr: 0.000037  training_loss: 3.4949 (3.8349)  classification_loss: 2.2186 (2.2774)  loss_mask: 1.2412 (1.5575)  time: 0.1766  data: 0.0003  max mem: 6050
[03:00:16.439674] Epoch: [0]  [600/781]  eta: 0:00:31  lr: 0.000038  training_loss: 3.4504 (3.8227)  classification_loss: 2.2054 (2.2754)  loss_mask: 1.2608 (1.5473)  time: 0.1727  data: 0.0003  max mem: 6050
[03:00:19.847677] Epoch: [0]  [620/781]  eta: 0:00:28  lr: 0.000040  training_loss: 3.3766 (3.8088)  classification_loss: 2.2252 (2.2739)  loss_mask: 1.1608 (1.5349)  time: 0.1703  data: 0.0002  max mem: 6050
[03:00:23.250298] Epoch: [0]  [640/781]  eta: 0:00:24  lr: 0.000041  training_loss: 3.3254 (3.7943)  classification_loss: 2.2145 (2.2727)  loss_mask: 1.1036 (1.5216)  time: 0.1701  data: 0.0002  max mem: 6050
[03:00:26.665894] Epoch: [0]  [660/781]  eta: 0:00:21  lr: 0.000042  training_loss: 3.3039 (3.7801)  classification_loss: 2.2172 (2.2713)  loss_mask: 1.1007 (1.5088)  time: 0.1707  data: 0.0003  max mem: 6050
[03:00:30.101909] Epoch: [0]  [680/781]  eta: 0:00:17  lr: 0.000044  training_loss: 3.3442 (3.7677)  classification_loss: 2.2040 (2.2698)  loss_mask: 1.1069 (1.4979)  time: 0.1717  data: 0.0003  max mem: 6050
[03:00:33.500773] Epoch: [0]  [700/781]  eta: 0:00:14  lr: 0.000045  training_loss: 3.3259 (3.7550)  classification_loss: 2.2000 (2.2681)  loss_mask: 1.0898 (1.4868)  time: 0.1699  data: 0.0002  max mem: 6050
[03:00:36.893421] Epoch: [0]  [720/781]  eta: 0:00:10  lr: 0.000046  training_loss: 3.2601 (3.7418)  classification_loss: 2.2422 (2.2673)  loss_mask: 1.0307 (1.4745)  time: 0.1696  data: 0.0003  max mem: 6050
[03:00:40.281541] Epoch: [0]  [740/781]  eta: 0:00:07  lr: 0.000047  training_loss: 3.2234 (3.7285)  classification_loss: 2.2249 (2.2662)  loss_mask: 1.0224 (1.4624)  time: 0.1693  data: 0.0003  max mem: 6050
[03:00:43.685044] Epoch: [0]  [760/781]  eta: 0:00:03  lr: 0.000049  training_loss: 3.1916 (3.7141)  classification_loss: 2.2167 (2.2651)  loss_mask: 0.9548 (1.4490)  time: 0.1701  data: 0.0002  max mem: 6050
[03:00:47.077574] Epoch: [0]  [780/781]  eta: 0:00:00  lr: 0.000050  training_loss: 3.2043 (3.7001)  classification_loss: 2.2118 (2.2639)  loss_mask: 0.9263 (1.4362)  time: 0.1696  data: 0.0002  max mem: 6050
[03:00:47.188999] Epoch: [0] Total time: 0:02:15 (0.1740 s / it)
[03:00:47.189483] Averaged stats: lr: 0.000050  training_loss: 3.2043 (3.7001)  classification_loss: 2.2118 (2.2639)  loss_mask: 0.9263 (1.4362)
[03:00:48.644611] Test:  [  0/157]  eta: 0:01:45  testing_loss: 1.9531 (1.9531)  acc1: 37.5000 (37.5000)  acc5: 89.0625 (89.0625)  time: 0.6707  data: 0.6403  max mem: 6050
[03:00:48.925854] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 2.0367 (2.0287)  acc1: 28.1250 (27.9830)  acc5: 82.8125 (83.0966)  time: 0.0864  data: 0.0583  max mem: 6050
[03:00:49.207776] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 2.0351 (2.0251)  acc1: 28.1250 (29.4643)  acc5: 82.8125 (82.0685)  time: 0.0280  data: 0.0001  max mem: 6050
[03:00:49.497205] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 2.0043 (2.0159)  acc1: 31.2500 (30.1411)  acc5: 81.2500 (81.8548)  time: 0.0285  data: 0.0001  max mem: 6050
[03:00:49.780548] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 2.0049 (2.0162)  acc1: 29.6875 (29.8780)  acc5: 79.6875 (81.6692)  time: 0.0285  data: 0.0001  max mem: 6050
[03:00:50.060212] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 2.0210 (2.0168)  acc1: 29.6875 (29.8100)  acc5: 81.2500 (82.0159)  time: 0.0280  data: 0.0001  max mem: 6050
[03:00:50.340812] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 2.0132 (2.0161)  acc1: 29.6875 (29.9436)  acc5: 84.3750 (82.5051)  time: 0.0279  data: 0.0001  max mem: 6050
[03:00:50.625478] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 2.0084 (2.0146)  acc1: 29.6875 (29.7095)  acc5: 84.3750 (82.6364)  time: 0.0281  data: 0.0002  max mem: 6050
[03:00:50.912125] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 2.0045 (2.0138)  acc1: 28.1250 (29.5910)  acc5: 84.3750 (83.1597)  time: 0.0285  data: 0.0002  max mem: 6050
[03:00:51.201181] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 2.0375 (2.0168)  acc1: 28.1250 (29.3956)  acc5: 84.3750 (83.0357)  time: 0.0287  data: 0.0001  max mem: 6050
[03:00:51.484940] Test:  [100/157]  eta: 0:00:01  testing_loss: 2.0358 (2.0170)  acc1: 28.1250 (29.3781)  acc5: 84.3750 (83.1374)  time: 0.0285  data: 0.0001  max mem: 6050
[03:00:51.769294] Test:  [110/157]  eta: 0:00:01  testing_loss: 2.0263 (2.0175)  acc1: 28.1250 (29.3778)  acc5: 84.3750 (83.0800)  time: 0.0283  data: 0.0002  max mem: 6050
[03:00:52.051551] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.9985 (2.0147)  acc1: 29.6875 (29.4292)  acc5: 82.8125 (83.1612)  time: 0.0282  data: 0.0002  max mem: 6050
[03:00:52.336336] Test:  [130/157]  eta: 0:00:00  testing_loss: 2.0135 (2.0160)  acc1: 29.6875 (29.5086)  acc5: 82.8125 (82.9676)  time: 0.0282  data: 0.0001  max mem: 6050
[03:00:52.617863] Test:  [140/157]  eta: 0:00:00  testing_loss: 2.0244 (2.0155)  acc1: 29.6875 (29.4880)  acc5: 81.2500 (83.0120)  time: 0.0282  data: 0.0001  max mem: 6050
[03:00:52.896853] Test:  [150/157]  eta: 0:00:00  testing_loss: 2.0080 (2.0146)  acc1: 29.6875 (29.5737)  acc5: 82.8125 (83.1850)  time: 0.0279  data: 0.0001  max mem: 6050
[03:00:53.186499] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.9912 (2.0139)  acc1: 29.6875 (29.5700)  acc5: 85.9375 (83.3200)  time: 0.0339  data: 0.0001  max mem: 6050
[03:00:53.352797] Test: Total time: 0:00:05 (0.0343 s / it)
[03:00:53.353305] * Acc@1 29.570 Acc@5 83.320 loss 2.014
[03:00:53.353651] Accuracy of the network on the 10000 test images: 29.6%
[03:00:53.353912] Max accuracy: 29.57%
[03:00:53.529356] log_dir: ./output_dir
[03:00:54.427611] Epoch: [1]  [  0/781]  eta: 0:11:40  lr: 0.000050  training_loss: 3.2933 (3.2933)  classification_loss: 2.2184 (2.2184)  loss_mask: 1.0749 (1.0749)  time: 0.8963  data: 0.6766  max mem: 6050
[03:00:57.853959] Epoch: [1]  [ 20/781]  eta: 0:02:36  lr: 0.000051  training_loss: 3.2074 (3.2149)  classification_loss: 2.2088 (2.2161)  loss_mask: 1.0021 (0.9989)  time: 0.1712  data: 0.0002  max mem: 6052
[03:01:01.296352] Epoch: [1]  [ 40/781]  eta: 0:02:20  lr: 0.000053  training_loss: 3.1520 (3.1846)  classification_loss: 2.2113 (2.2222)  loss_mask: 0.9088 (0.9624)  time: 0.1720  data: 0.0004  max mem: 6052
[03:01:04.722548] Epoch: [1]  [ 60/781]  eta: 0:02:12  lr: 0.000054  training_loss: 3.0926 (3.1663)  classification_loss: 2.2456 (2.2259)  loss_mask: 0.8832 (0.9404)  time: 0.1712  data: 0.0002  max mem: 6052
[03:01:08.155249] Epoch: [1]  [ 80/781]  eta: 0:02:06  lr: 0.000055  training_loss: 3.1488 (3.1687)  classification_loss: 2.2162 (2.2264)  loss_mask: 0.9006 (0.9423)  time: 0.1716  data: 0.0002  max mem: 6052
[03:01:11.568641] Epoch: [1]  [100/781]  eta: 0:02:01  lr: 0.000056  training_loss: 3.1988 (3.1737)  classification_loss: 2.2167 (2.2238)  loss_mask: 0.9601 (0.9499)  time: 0.1706  data: 0.0001  max mem: 6052
[03:01:14.994239] Epoch: [1]  [120/781]  eta: 0:01:57  lr: 0.000058  training_loss: 3.1000 (3.1654)  classification_loss: 2.2281 (2.2228)  loss_mask: 0.8598 (0.9426)  time: 0.1712  data: 0.0002  max mem: 6052
[03:01:18.416174] Epoch: [1]  [140/781]  eta: 0:01:53  lr: 0.000059  training_loss: 3.0361 (3.1482)  classification_loss: 2.1826 (2.2190)  loss_mask: 0.8350 (0.9291)  time: 0.1710  data: 0.0002  max mem: 6052
[03:01:21.823746] Epoch: [1]  [160/781]  eta: 0:01:49  lr: 0.000060  training_loss: 3.1731 (3.1554)  classification_loss: 2.2162 (2.2180)  loss_mask: 0.9578 (0.9374)  time: 0.1703  data: 0.0002  max mem: 6052
[03:01:25.228626] Epoch: [1]  [180/781]  eta: 0:01:45  lr: 0.000062  training_loss: 3.1205 (3.1493)  classification_loss: 2.1934 (2.2177)  loss_mask: 0.8893 (0.9316)  time: 0.1702  data: 0.0003  max mem: 6052
[03:01:28.676604] Epoch: [1]  [200/781]  eta: 0:01:41  lr: 0.000063  training_loss: 3.0670 (3.1397)  classification_loss: 2.2092 (2.2164)  loss_mask: 0.8485 (0.9233)  time: 0.1723  data: 0.0002  max mem: 6052
[03:01:32.088023] Epoch: [1]  [220/781]  eta: 0:01:37  lr: 0.000064  training_loss: 3.0363 (3.1335)  classification_loss: 2.1961 (2.2157)  loss_mask: 0.8388 (0.9178)  time: 0.1705  data: 0.0002  max mem: 6052
[03:01:35.527279] Epoch: [1]  [240/781]  eta: 0:01:34  lr: 0.000065  training_loss: 3.1054 (3.1305)  classification_loss: 2.1984 (2.2150)  loss_mask: 0.8626 (0.9155)  time: 0.1719  data: 0.0002  max mem: 6052
[03:01:38.951024] Epoch: [1]  [260/781]  eta: 0:01:30  lr: 0.000067  training_loss: 3.0448 (3.1230)  classification_loss: 2.2063 (2.2139)  loss_mask: 0.8389 (0.9091)  time: 0.1711  data: 0.0002  max mem: 6052
[03:01:42.457767] Epoch: [1]  [280/781]  eta: 0:01:27  lr: 0.000068  training_loss: 3.0875 (3.1187)  classification_loss: 2.2306 (2.2148)  loss_mask: 0.8255 (0.9038)  time: 0.1752  data: 0.0002  max mem: 6052
[03:01:45.880969] Epoch: [1]  [300/781]  eta: 0:01:23  lr: 0.000069  training_loss: 2.9635 (3.1095)  classification_loss: 2.2063 (2.2144)  loss_mask: 0.7566 (0.8951)  time: 0.1711  data: 0.0002  max mem: 6052
[03:01:49.295332] Epoch: [1]  [320/781]  eta: 0:01:20  lr: 0.000070  training_loss: 2.9605 (3.1018)  classification_loss: 2.1944 (2.2137)  loss_mask: 0.7862 (0.8881)  time: 0.1707  data: 0.0002  max mem: 6052
[03:01:52.766837] Epoch: [1]  [340/781]  eta: 0:01:16  lr: 0.000072  training_loss: 3.0146 (3.0964)  classification_loss: 2.2361 (2.2138)  loss_mask: 0.8035 (0.8826)  time: 0.1735  data: 0.0002  max mem: 6052
[03:01:56.186713] Epoch: [1]  [360/781]  eta: 0:01:13  lr: 0.000073  training_loss: 3.0682 (3.0939)  classification_loss: 2.2329 (2.2141)  loss_mask: 0.8227 (0.8798)  time: 0.1709  data: 0.0002  max mem: 6052
[03:01:59.609813] Epoch: [1]  [380/781]  eta: 0:01:09  lr: 0.000074  training_loss: 3.0587 (3.0912)  classification_loss: 2.2151 (2.2147)  loss_mask: 0.7883 (0.8765)  time: 0.1711  data: 0.0003  max mem: 6052
[03:02:03.032605] Epoch: [1]  [400/781]  eta: 0:01:06  lr: 0.000076  training_loss: 2.9333 (3.0839)  classification_loss: 2.2092 (2.2143)  loss_mask: 0.7104 (0.8697)  time: 0.1711  data: 0.0003  max mem: 6052
[03:02:06.489870] Epoch: [1]  [420/781]  eta: 0:01:02  lr: 0.000077  training_loss: 3.0133 (3.0811)  classification_loss: 2.1978 (2.2134)  loss_mask: 0.8240 (0.8677)  time: 0.1728  data: 0.0005  max mem: 6052
[03:02:09.909019] Epoch: [1]  [440/781]  eta: 0:00:59  lr: 0.000078  training_loss: 2.9471 (3.0764)  classification_loss: 2.1897 (2.2128)  loss_mask: 0.7573 (0.8636)  time: 0.1709  data: 0.0003  max mem: 6052
[03:02:13.332518] Epoch: [1]  [460/781]  eta: 0:00:55  lr: 0.000079  training_loss: 2.9199 (3.0698)  classification_loss: 2.1820 (2.2119)  loss_mask: 0.7260 (0.8579)  time: 0.1711  data: 0.0003  max mem: 6052
[03:02:16.760107] Epoch: [1]  [480/781]  eta: 0:00:52  lr: 0.000081  training_loss: 3.0330 (3.0700)  classification_loss: 2.2223 (2.2127)  loss_mask: 0.7909 (0.8572)  time: 0.1713  data: 0.0002  max mem: 6052
[03:02:20.200905] Epoch: [1]  [500/781]  eta: 0:00:48  lr: 0.000082  training_loss: 3.1255 (3.0712)  classification_loss: 2.1807 (2.2120)  loss_mask: 0.9210 (0.8591)  time: 0.1720  data: 0.0002  max mem: 6052
[03:02:23.605039] Epoch: [1]  [520/781]  eta: 0:00:45  lr: 0.000083  training_loss: 2.9602 (3.0676)  classification_loss: 2.2089 (2.2119)  loss_mask: 0.7590 (0.8557)  time: 0.1701  data: 0.0002  max mem: 6052
[03:02:27.007413] Epoch: [1]  [540/781]  eta: 0:00:41  lr: 0.000085  training_loss: 2.9575 (3.0642)  classification_loss: 2.2078 (2.2121)  loss_mask: 0.7382 (0.8522)  time: 0.1700  data: 0.0001  max mem: 6052
[03:02:30.413979] Epoch: [1]  [560/781]  eta: 0:00:38  lr: 0.000086  training_loss: 2.8992 (3.0599)  classification_loss: 2.2019 (2.2117)  loss_mask: 0.7120 (0.8482)  time: 0.1702  data: 0.0002  max mem: 6052
[03:02:33.827411] Epoch: [1]  [580/781]  eta: 0:00:34  lr: 0.000087  training_loss: 3.0079 (3.0588)  classification_loss: 2.1934 (2.2114)  loss_mask: 0.8147 (0.8474)  time: 0.1706  data: 0.0002  max mem: 6052
[03:02:37.240132] Epoch: [1]  [600/781]  eta: 0:00:31  lr: 0.000088  training_loss: 3.0552 (3.0591)  classification_loss: 2.1919 (2.2110)  loss_mask: 0.8513 (0.8481)  time: 0.1706  data: 0.0001  max mem: 6052
[03:02:40.649551] Epoch: [1]  [620/781]  eta: 0:00:27  lr: 0.000090  training_loss: 3.0115 (3.0577)  classification_loss: 2.1953 (2.2104)  loss_mask: 0.8200 (0.8473)  time: 0.1704  data: 0.0002  max mem: 6052
[03:02:44.060174] Epoch: [1]  [640/781]  eta: 0:00:24  lr: 0.000091  training_loss: 2.8999 (3.0539)  classification_loss: 2.1682 (2.2098)  loss_mask: 0.7451 (0.8441)  time: 0.1705  data: 0.0001  max mem: 6052
[03:02:47.482482] Epoch: [1]  [660/781]  eta: 0:00:20  lr: 0.000092  training_loss: 2.9712 (3.0521)  classification_loss: 2.1759 (2.2093)  loss_mask: 0.7784 (0.8428)  time: 0.1710  data: 0.0002  max mem: 6052
[03:02:50.897710] Epoch: [1]  [680/781]  eta: 0:00:17  lr: 0.000094  training_loss: 2.9053 (3.0491)  classification_loss: 2.2077 (2.2094)  loss_mask: 0.7245 (0.8396)  time: 0.1707  data: 0.0002  max mem: 6052
[03:02:54.308811] Epoch: [1]  [700/781]  eta: 0:00:13  lr: 0.000095  training_loss: 2.9554 (3.0467)  classification_loss: 2.1920 (2.2091)  loss_mask: 0.7639 (0.8376)  time: 0.1705  data: 0.0002  max mem: 6052
[03:02:57.731428] Epoch: [1]  [720/781]  eta: 0:00:10  lr: 0.000096  training_loss: 2.9813 (3.0448)  classification_loss: 2.2160 (2.2093)  loss_mask: 0.7686 (0.8356)  time: 0.1710  data: 0.0002  max mem: 6052
[03:03:01.141929] Epoch: [1]  [740/781]  eta: 0:00:07  lr: 0.000097  training_loss: 3.0049 (3.0443)  classification_loss: 2.2187 (2.2095)  loss_mask: 0.7835 (0.8348)  time: 0.1705  data: 0.0002  max mem: 6052
[03:03:04.546354] Epoch: [1]  [760/781]  eta: 0:00:03  lr: 0.000099  training_loss: 2.9317 (3.0419)  classification_loss: 2.1892 (2.2092)  loss_mask: 0.7322 (0.8327)  time: 0.1701  data: 0.0002  max mem: 6052
[03:03:07.950884] Epoch: [1]  [780/781]  eta: 0:00:00  lr: 0.000100  training_loss: 2.9176 (3.0394)  classification_loss: 2.2097 (2.2096)  loss_mask: 0.6911 (0.8297)  time: 0.1702  data: 0.0003  max mem: 6052
[03:03:08.117251] Epoch: [1] Total time: 0:02:14 (0.1723 s / it)
[03:03:08.117707] Averaged stats: lr: 0.000100  training_loss: 2.9176 (3.0394)  classification_loss: 2.2097 (2.2096)  loss_mask: 0.6911 (0.8297)
[03:03:08.783278] Test:  [  0/157]  eta: 0:01:43  testing_loss: 1.8957 (1.8957)  acc1: 39.0625 (39.0625)  acc5: 85.9375 (85.9375)  time: 0.6615  data: 0.6319  max mem: 6052
[03:03:09.067947] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 2.0244 (1.9880)  acc1: 29.6875 (31.5341)  acc5: 84.3750 (83.2386)  time: 0.0859  data: 0.0578  max mem: 6052
[03:03:09.349776] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.9861 (1.9888)  acc1: 32.8125 (32.7381)  acc5: 82.8125 (82.5893)  time: 0.0282  data: 0.0003  max mem: 6052
[03:03:09.630525] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.9749 (1.9807)  acc1: 32.8125 (33.0141)  acc5: 82.8125 (83.1653)  time: 0.0280  data: 0.0002  max mem: 6052
[03:03:09.911317] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.9692 (1.9806)  acc1: 32.8125 (33.4985)  acc5: 84.3750 (82.8506)  time: 0.0280  data: 0.0002  max mem: 6052
[03:03:10.192040] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.9846 (1.9803)  acc1: 32.8125 (33.5172)  acc5: 84.3750 (83.1495)  time: 0.0280  data: 0.0002  max mem: 6052
[03:03:10.479640] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.9747 (1.9786)  acc1: 34.3750 (33.6578)  acc5: 82.8125 (83.2223)  time: 0.0283  data: 0.0002  max mem: 6052
[03:03:10.761526] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.9639 (1.9793)  acc1: 31.2500 (33.1206)  acc5: 82.8125 (83.1866)  time: 0.0283  data: 0.0002  max mem: 6052
[03:03:11.042164] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.9630 (1.9773)  acc1: 34.3750 (33.4105)  acc5: 84.3750 (83.4298)  time: 0.0280  data: 0.0002  max mem: 6052
[03:03:11.323231] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.9825 (1.9798)  acc1: 34.3750 (33.1044)  acc5: 82.8125 (83.2589)  time: 0.0279  data: 0.0002  max mem: 6052
[03:03:11.606329] Test:  [100/157]  eta: 0:00:01  testing_loss: 2.0076 (1.9808)  acc1: 31.2500 (33.1683)  acc5: 79.6875 (83.2302)  time: 0.0281  data: 0.0002  max mem: 6052
[03:03:11.887808] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.9890 (1.9817)  acc1: 32.8125 (33.1081)  acc5: 82.8125 (83.3052)  time: 0.0281  data: 0.0002  max mem: 6052
[03:03:12.168786] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.9686 (1.9780)  acc1: 32.8125 (33.2128)  acc5: 85.9375 (83.5486)  time: 0.0280  data: 0.0001  max mem: 6052
[03:03:12.449555] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.9692 (1.9790)  acc1: 34.3750 (33.2657)  acc5: 84.3750 (83.4685)  time: 0.0279  data: 0.0001  max mem: 6052
[03:03:12.730115] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.9944 (1.9785)  acc1: 34.3750 (33.4552)  acc5: 84.3750 (83.5106)  time: 0.0279  data: 0.0001  max mem: 6052
[03:03:13.008745] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.9820 (1.9773)  acc1: 34.3750 (33.4023)  acc5: 84.3750 (83.5989)  time: 0.0278  data: 0.0001  max mem: 6052
[03:03:13.158247] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.9623 (1.9764)  acc1: 34.3750 (33.2700)  acc5: 84.3750 (83.6600)  time: 0.0268  data: 0.0001  max mem: 6052
[03:03:13.332730] Test: Total time: 0:00:05 (0.0332 s / it)
[03:03:13.333168] * Acc@1 33.270 Acc@5 83.660 loss 1.976
[03:03:13.333449] Accuracy of the network on the 10000 test images: 33.3%
[03:03:13.333623] Max accuracy: 33.27%
[03:03:13.531222] log_dir: ./output_dir
[03:03:14.393965] Epoch: [2]  [  0/781]  eta: 0:11:12  lr: 0.000100  training_loss: 2.8280 (2.8280)  classification_loss: 2.2121 (2.2121)  loss_mask: 0.6159 (0.6159)  time: 0.8606  data: 0.6747  max mem: 6052

[03:03:17.811324] Epoch: [2]  [ 20/781]  eta: 0:02:34  lr: 0.000101  training_loss: 2.9203 (2.9084)  classification_loss: 2.1977 (2.1885)  loss_mask: 0.7119 (0.7199)  time: 0.1708  data: 0.0002  max mem: 6052
[03:03:21.233480] Epoch: [2]  [ 40/781]  eta: 0:02:19  lr: 0.000103  training_loss: 2.8779 (2.8951)  classification_loss: 2.2040 (2.2012)  loss_mask: 0.6347 (0.6939)  time: 0.1710  data: 0.0002  max mem: 6052
[03:03:24.665848] Epoch: [2]  [ 60/781]  eta: 0:02:11  lr: 0.000104  training_loss: 2.9201 (2.9099)  classification_loss: 2.2014 (2.2055)  loss_mask: 0.7391 (0.7045)  time: 0.1715  data: 0.0002  max mem: 6052
[03:03:28.072134] Epoch: [2]  [ 80/781]  eta: 0:02:05  lr: 0.000105  training_loss: 2.9403 (2.9229)  classification_loss: 2.2221 (2.2111)  loss_mask: 0.7156 (0.7118)  time: 0.1702  data: 0.0003  max mem: 6052
[03:03:31.494344] Epoch: [2]  [100/781]  eta: 0:02:01  lr: 0.000106  training_loss: 2.8979 (2.9207)  classification_loss: 2.1985 (2.2072)  loss_mask: 0.6939 (0.7134)  time: 0.1710  data: 0.0002  max mem: 6052
[03:03:34.914136] Epoch: [2]  [120/781]  eta: 0:01:56  lr: 0.000108  training_loss: 2.9794 (2.9350)  classification_loss: 2.2114 (2.2090)  loss_mask: 0.7430 (0.7261)  time: 0.1709  data: 0.0003  max mem: 6052
[03:03:38.344418] Epoch: [2]  [140/781]  eta: 0:01:52  lr: 0.000109  training_loss: 2.9084 (2.9336)  classification_loss: 2.2019 (2.2066)  loss_mask: 0.7035 (0.7270)  time: 0.1714  data: 0.0004  max mem: 6052
[03:03:41.764067] Epoch: [2]  [160/781]  eta: 0:01:48  lr: 0.000110  training_loss: 2.9680 (2.9361)  classification_loss: 2.1855 (2.2053)  loss_mask: 0.7430 (0.7308)  time: 0.1709  data: 0.0002  max mem: 6052
[03:03:45.175157] Epoch: [2]  [180/781]  eta: 0:01:45  lr: 0.000112  training_loss: 2.8906 (2.9348)  classification_loss: 2.1975 (2.2050)  loss_mask: 0.7184 (0.7298)  time: 0.1705  data: 0.0002  max mem: 6052
[03:03:48.594748] Epoch: [2]  [200/781]  eta: 0:01:41  lr: 0.000113  training_loss: 2.8435 (2.9262)  classification_loss: 2.1994 (2.2034)  loss_mask: 0.6661 (0.7228)  time: 0.1709  data: 0.0002  max mem: 6052
[03:03:52.005716] Epoch: [2]  [220/781]  eta: 0:01:37  lr: 0.000114  training_loss: 2.8131 (2.9176)  classification_loss: 2.1557 (2.2003)  loss_mask: 0.6342 (0.7173)  time: 0.1705  data: 0.0002  max mem: 6052
[03:03:55.438973] Epoch: [2]  [240/781]  eta: 0:01:34  lr: 0.000115  training_loss: 2.9569 (2.9192)  classification_loss: 2.1967 (2.1997)  loss_mask: 0.7615 (0.7195)  time: 0.1715  data: 0.0002  max mem: 6052
[03:03:58.854734] Epoch: [2]  [260/781]  eta: 0:01:30  lr: 0.000117  training_loss: 2.9517 (2.9198)  classification_loss: 2.1586 (2.1984)  loss_mask: 0.7265 (0.7214)  time: 0.1707  data: 0.0002  max mem: 6052
[03:04:02.283899] Epoch: [2]  [280/781]  eta: 0:01:26  lr: 0.000118  training_loss: 2.9272 (2.9202)  classification_loss: 2.1908 (2.1983)  loss_mask: 0.7239 (0.7219)  time: 0.1713  data: 0.0003  max mem: 6052
[03:04:05.747329] Epoch: [2]  [300/781]  eta: 0:01:23  lr: 0.000119  training_loss: 2.9611 (2.9233)  classification_loss: 2.1865 (2.1977)  loss_mask: 0.7624 (0.7257)  time: 0.1731  data: 0.0003  max mem: 6052
[03:04:09.176183] Epoch: [2]  [320/781]  eta: 0:01:19  lr: 0.000120  training_loss: 2.8905 (2.9219)  classification_loss: 2.1767 (2.1969)  loss_mask: 0.6969 (0.7250)  time: 0.1713  data: 0.0002  max mem: 6052
[03:04:12.591384] Epoch: [2]  [340/781]  eta: 0:01:16  lr: 0.000122  training_loss: 2.8675 (2.9196)  classification_loss: 2.1944 (2.1967)  loss_mask: 0.7028 (0.7229)  time: 0.1707  data: 0.0002  max mem: 6052
[03:04:16.035294] Epoch: [2]  [360/781]  eta: 0:01:12  lr: 0.000123  training_loss: 2.8612 (2.9161)  classification_loss: 2.1815 (2.1964)  loss_mask: 0.6501 (0.7197)  time: 0.1721  data: 0.0002  max mem: 6052
[03:04:19.458671] Epoch: [2]  [380/781]  eta: 0:01:09  lr: 0.000124  training_loss: 2.9442 (2.9187)  classification_loss: 2.2037 (2.1968)  loss_mask: 0.7398 (0.7219)  time: 0.1711  data: 0.0002  max mem: 6052
[03:04:22.882078] Epoch: [2]  [400/781]  eta: 0:01:05  lr: 0.000126  training_loss: 2.8782 (2.9182)  classification_loss: 2.1694 (2.1959)  loss_mask: 0.7185 (0.7223)  time: 0.1711  data: 0.0003  max mem: 6052
[03:04:26.308467] Epoch: [2]  [420/781]  eta: 0:01:02  lr: 0.000127  training_loss: 2.8392 (2.9142)  classification_loss: 2.1804 (2.1952)  loss_mask: 0.6344 (0.7190)  time: 0.1712  data: 0.0002  max mem: 6052
[03:04:29.722169] Epoch: [2]  [440/781]  eta: 0:00:58  lr: 0.000128  training_loss: 2.9208 (2.9144)  classification_loss: 2.1654 (2.1944)  loss_mask: 0.7183 (0.7200)  time: 0.1706  data: 0.0001  max mem: 6052
[03:04:33.154859] Epoch: [2]  [460/781]  eta: 0:00:55  lr: 0.000129  training_loss: 2.8076 (2.9101)  classification_loss: 2.1565 (2.1927)  loss_mask: 0.6468 (0.7174)  time: 0.1715  data: 0.0002  max mem: 6052
[03:04:36.560429] Epoch: [2]  [480/781]  eta: 0:00:51  lr: 0.000131  training_loss: 2.8923 (2.9090)  classification_loss: 2.2017 (2.1932)  loss_mask: 0.6670 (0.7158)  time: 0.1702  data: 0.0001  max mem: 6052
[03:04:39.972243] Epoch: [2]  [500/781]  eta: 0:00:48  lr: 0.000132  training_loss: 2.8859 (2.9091)  classification_loss: 2.1797 (2.1925)  loss_mask: 0.7352 (0.7166)  time: 0.1705  data: 0.0002  max mem: 6052
[03:04:43.401840] Epoch: [2]  [520/781]  eta: 0:00:44  lr: 0.000133  training_loss: 2.9319 (2.9100)  classification_loss: 2.1678 (2.1920)  loss_mask: 0.7492 (0.7180)  time: 0.1714  data: 0.0003  max mem: 6052
[03:04:46.824240] Epoch: [2]  [540/781]  eta: 0:00:41  lr: 0.000135  training_loss: 2.8713 (2.9085)  classification_loss: 2.1783 (2.1919)  loss_mask: 0.6900 (0.7166)  time: 0.1710  data: 0.0001  max mem: 6052
[03:04:50.231200] Epoch: [2]  [560/781]  eta: 0:00:38  lr: 0.000136  training_loss: 2.8045 (2.9056)  classification_loss: 2.1681 (2.1909)  loss_mask: 0.6705 (0.7148)  time: 0.1703  data: 0.0002  max mem: 6052
[03:04:53.688143] Epoch: [2]  [580/781]  eta: 0:00:34  lr: 0.000137  training_loss: 2.8339 (2.9029)  classification_loss: 2.1491 (2.1898)  loss_mask: 0.6613 (0.7131)  time: 0.1728  data: 0.0002  max mem: 6052
[03:04:57.122990] Epoch: [2]  [600/781]  eta: 0:00:31  lr: 0.000138  training_loss: 2.8610 (2.9016)  classification_loss: 2.1905 (2.1897)  loss_mask: 0.6682 (0.7119)  time: 0.1717  data: 0.0002  max mem: 6052
[03:05:00.538988] Epoch: [2]  [620/781]  eta: 0:00:27  lr: 0.000140  training_loss: 2.7867 (2.8986)  classification_loss: 2.1811 (2.1891)  loss_mask: 0.6573 (0.7094)  time: 0.1707  data: 0.0002  max mem: 6052
[03:05:03.987552] Epoch: [2]  [640/781]  eta: 0:00:24  lr: 0.000141  training_loss: 2.8047 (2.8964)  classification_loss: 2.1602 (2.1883)  loss_mask: 0.6373 (0.7081)  time: 0.1723  data: 0.0002  max mem: 6052
[03:05:07.413407] Epoch: [2]  [660/781]  eta: 0:00:20  lr: 0.000142  training_loss: 2.8320 (2.8947)  classification_loss: 2.1684 (2.1877)  loss_mask: 0.6552 (0.7071)  time: 0.1712  data: 0.0004  max mem: 6052
[03:05:10.838623] Epoch: [2]  [680/781]  eta: 0:00:17  lr: 0.000144  training_loss: 2.8199 (2.8935)  classification_loss: 2.1680 (2.1870)  loss_mask: 0.6874 (0.7065)  time: 0.1712  data: 0.0003  max mem: 6052
[03:05:14.244701] Epoch: [2]  [700/781]  eta: 0:00:13  lr: 0.000145  training_loss: 2.9098 (2.8940)  classification_loss: 2.1596 (2.1865)  loss_mask: 0.7006 (0.7075)  time: 0.1702  data: 0.0002  max mem: 6052
[03:05:17.654809] Epoch: [2]  [720/781]  eta: 0:00:10  lr: 0.000146  training_loss: 2.9067 (2.8951)  classification_loss: 2.1731 (2.1864)  loss_mask: 0.7154 (0.7087)  time: 0.1704  data: 0.0003  max mem: 6052
[03:05:21.052656] Epoch: [2]  [740/781]  eta: 0:00:07  lr: 0.000147  training_loss: 2.8513 (2.8943)  classification_loss: 2.1676 (2.1863)  loss_mask: 0.6542 (0.7079)  time: 0.1698  data: 0.0002  max mem: 6052
[03:05:24.450686] Epoch: [2]  [760/781]  eta: 0:00:03  lr: 0.000149  training_loss: 2.8892 (2.8943)  classification_loss: 2.1869 (2.1862)  loss_mask: 0.7108 (0.7081)  time: 0.1698  data: 0.0003  max mem: 6052
[03:05:27.871397] Epoch: [2]  [780/781]  eta: 0:00:00  lr: 0.000150  training_loss: 2.8723 (2.8948)  classification_loss: 2.1493 (2.1855)  loss_mask: 0.7141 (0.7093)  time: 0.1709  data: 0.0001  max mem: 6052
[03:05:28.026318] Epoch: [2] Total time: 0:02:14 (0.1722 s / it)
[03:05:28.026788] Averaged stats: lr: 0.000150  training_loss: 2.8723 (2.8948)  classification_loss: 2.1493 (2.1855)  loss_mask: 0.7141 (0.7093)
[03:05:28.672718] Test:  [  0/157]  eta: 0:01:40  testing_loss: 1.8038 (1.8038)  acc1: 45.3125 (45.3125)  acc5: 87.5000 (87.5000)  time: 0.6415  data: 0.6122  max mem: 6052
[03:05:28.971793] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.9057 (1.8886)  acc1: 34.3750 (35.6534)  acc5: 85.9375 (85.9375)  time: 0.0853  data: 0.0564  max mem: 6052
[03:05:29.252906] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.8753 (1.8855)  acc1: 35.9375 (36.9048)  acc5: 85.9375 (85.1935)  time: 0.0289  data: 0.0005  max mem: 6052
[03:05:29.533719] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.8520 (1.8755)  acc1: 35.9375 (36.8952)  acc5: 84.3750 (84.9798)  time: 0.0280  data: 0.0001  max mem: 6052
[03:05:29.814481] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.8564 (1.8779)  acc1: 35.9375 (36.9665)  acc5: 84.3750 (84.7180)  time: 0.0280  data: 0.0001  max mem: 6052
[03:05:30.096685] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.8851 (1.8798)  acc1: 35.9375 (36.5196)  acc5: 84.3750 (84.6814)  time: 0.0280  data: 0.0002  max mem: 6052
[03:05:30.386084] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.8599 (1.8752)  acc1: 35.9375 (36.6803)  acc5: 84.3750 (84.8873)  time: 0.0285  data: 0.0003  max mem: 6052
[03:05:30.677455] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.8573 (1.8750)  acc1: 37.5000 (36.6637)  acc5: 85.9375 (85.1232)  time: 0.0289  data: 0.0002  max mem: 6052
[03:05:30.957143] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.8607 (1.8727)  acc1: 35.9375 (36.5934)  acc5: 87.5000 (85.4360)  time: 0.0284  data: 0.0002  max mem: 6052
[03:05:31.237438] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.8770 (1.8761)  acc1: 35.9375 (36.3668)  acc5: 85.9375 (85.4224)  time: 0.0279  data: 0.0002  max mem: 6052
[03:05:31.520368] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.9035 (1.8777)  acc1: 34.3750 (36.3088)  acc5: 84.3750 (85.3342)  time: 0.0280  data: 0.0001  max mem: 6052
[03:05:31.809206] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.8939 (1.8791)  acc1: 35.9375 (36.3035)  acc5: 84.3750 (85.2759)  time: 0.0284  data: 0.0002  max mem: 6052
[03:05:32.096668] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.8571 (1.8750)  acc1: 35.9375 (36.4411)  acc5: 85.9375 (85.5243)  time: 0.0287  data: 0.0003  max mem: 6052
[03:05:32.378431] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.8557 (1.8759)  acc1: 34.3750 (36.3430)  acc5: 87.5000 (85.4962)  time: 0.0283  data: 0.0004  max mem: 6052
[03:05:32.667723] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.8805 (1.8760)  acc1: 35.9375 (36.3586)  acc5: 85.9375 (85.5053)  time: 0.0284  data: 0.0002  max mem: 6052
[03:05:32.947223] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.8800 (1.8745)  acc1: 37.5000 (36.4652)  acc5: 87.5000 (85.5546)  time: 0.0282  data: 0.0001  max mem: 6052
[03:05:33.096725] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.8569 (1.8737)  acc1: 37.5000 (36.3000)  acc5: 87.5000 (85.5900)  time: 0.0268  data: 0.0001  max mem: 6052
[03:05:33.251714] Test: Total time: 0:00:05 (0.0333 s / it)
[03:05:33.252141] * Acc@1 36.300 Acc@5 85.590 loss 1.874
[03:05:33.252427] Accuracy of the network on the 10000 test images: 36.3%
[03:05:33.252682] Max accuracy: 36.30%
[03:05:33.557039] log_dir: ./output_dir
[03:05:34.367977] Epoch: [3]  [  0/781]  eta: 0:10:31  lr: 0.000150  training_loss: 2.7584 (2.7584)  classification_loss: 2.0554 (2.0554)  loss_mask: 0.7029 (0.7029)  time: 0.8089  data: 0.6140  max mem: 6052
[03:05:37.782808] Epoch: [3]  [ 20/781]  eta: 0:02:32  lr: 0.000151  training_loss: 2.8467 (2.8724)  classification_loss: 2.1417 (2.1388)  loss_mask: 0.7139 (0.7336)  time: 0.1706  data: 0.0003  max mem: 6052
[03:05:41.193960] Epoch: [3]  [ 40/781]  eta: 0:02:17  lr: 0.000153  training_loss: 2.9273 (2.8953)  classification_loss: 2.2302 (2.1678)  loss_mask: 0.7099 (0.7276)  time: 0.1705  data: 0.0002  max mem: 6052
[03:05:44.601458] Epoch: [3]  [ 60/781]  eta: 0:02:10  lr: 0.000154  training_loss: 2.8146 (2.8645)  classification_loss: 2.1576 (2.1667)  loss_mask: 0.6520 (0.6978)  time: 0.1703  data: 0.0002  max mem: 6052
[03:05:48.034054] Epoch: [3]  [ 80/781]  eta: 0:02:05  lr: 0.000155  training_loss: 2.8278 (2.8692)  classification_loss: 2.1849 (2.1718)  loss_mask: 0.6696 (0.6975)  time: 0.1716  data: 0.0004  max mem: 6052
[03:05:51.476876] Epoch: [3]  [100/781]  eta: 0:02:00  lr: 0.000156  training_loss: 2.8772 (2.8747)  classification_loss: 2.1842 (2.1749)  loss_mask: 0.7143 (0.6998)  time: 0.1721  data: 0.0002  max mem: 6052
[03:05:54.906665] Epoch: [3]  [120/781]  eta: 0:01:56  lr: 0.000158  training_loss: 2.8517 (2.8742)  classification_loss: 2.1638 (2.1738)  loss_mask: 0.6879 (0.7004)  time: 0.1714  data: 0.0004  max mem: 6052
[03:05:58.313620] Epoch: [3]  [140/781]  eta: 0:01:52  lr: 0.000159  training_loss: 2.7602 (2.8599)  classification_loss: 2.1648 (2.1733)  loss_mask: 0.5975 (0.6866)  time: 0.1702  data: 0.0002  max mem: 6052
[03:06:01.713755] Epoch: [3]  [160/781]  eta: 0:01:48  lr: 0.000160  training_loss: 2.7541 (2.8482)  classification_loss: 2.1605 (2.1732)  loss_mask: 0.5785 (0.6750)  time: 0.1699  data: 0.0003  max mem: 6052
[03:06:05.217249] Epoch: [3]  [180/781]  eta: 0:01:45  lr: 0.000162  training_loss: 2.8049 (2.8444)  classification_loss: 2.1768 (2.1732)  loss_mask: 0.6365 (0.6713)  time: 0.1751  data: 0.0002  max mem: 6052
[03:06:08.674363] Epoch: [3]  [200/781]  eta: 0:01:41  lr: 0.000163  training_loss: 2.7964 (2.8435)  classification_loss: 2.1805 (2.1736)  loss_mask: 0.6516 (0.6699)  time: 0.1728  data: 0.0002  max mem: 6052
[03:06:12.090988] Epoch: [3]  [220/781]  eta: 0:01:37  lr: 0.000164  training_loss: 2.7639 (2.8423)  classification_loss: 2.1460 (2.1716)  loss_mask: 0.6624 (0.6707)  time: 0.1708  data: 0.0002  max mem: 6052
[03:06:15.499840] Epoch: [3]  [240/781]  eta: 0:01:34  lr: 0.000165  training_loss: 2.7757 (2.8388)  classification_loss: 2.1612 (2.1709)  loss_mask: 0.6119 (0.6679)  time: 0.1703  data: 0.0004  max mem: 6052
[03:06:18.915308] Epoch: [3]  [260/781]  eta: 0:01:30  lr: 0.000167  training_loss: 2.7810 (2.8330)  classification_loss: 2.1381 (2.1683)  loss_mask: 0.6262 (0.6647)  time: 0.1707  data: 0.0003  max mem: 6052
[03:06:22.324018] Epoch: [3]  [280/781]  eta: 0:01:26  lr: 0.000168  training_loss: 2.8965 (2.8430)  classification_loss: 2.1852 (2.1702)  loss_mask: 0.7435 (0.6727)  time: 0.1704  data: 0.0003  max mem: 6052
[03:06:25.718612] Epoch: [3]  [300/781]  eta: 0:01:23  lr: 0.000169  training_loss: 2.8817 (2.8458)  classification_loss: 2.1849 (2.1711)  loss_mask: 0.6787 (0.6747)  time: 0.1697  data: 0.0003  max mem: 6052
[03:06:29.124500] Epoch: [3]  [320/781]  eta: 0:01:19  lr: 0.000170  training_loss: 2.8067 (2.8426)  classification_loss: 2.1464 (2.1693)  loss_mask: 0.6252 (0.6733)  time: 0.1702  data: 0.0002  max mem: 6052
[03:06:32.532804] Epoch: [3]  [340/781]  eta: 0:01:16  lr: 0.000172  training_loss: 2.8192 (2.8420)  classification_loss: 2.1451 (2.1681)  loss_mask: 0.6433 (0.6739)  time: 0.1703  data: 0.0001  max mem: 6052
[03:06:35.947304] Epoch: [3]  [360/781]  eta: 0:01:12  lr: 0.000173  training_loss: 2.7571 (2.8389)  classification_loss: 2.1522 (2.1680)  loss_mask: 0.6193 (0.6710)  time: 0.1707  data: 0.0003  max mem: 6052
[03:06:39.360631] Epoch: [3]  [380/781]  eta: 0:01:09  lr: 0.000174  training_loss: 2.8222 (2.8397)  classification_loss: 2.1689 (2.1673)  loss_mask: 0.6706 (0.6724)  time: 0.1706  data: 0.0002  max mem: 6052
[03:06:42.767118] Epoch: [3]  [400/781]  eta: 0:01:05  lr: 0.000176  training_loss: 2.8396 (2.8397)  classification_loss: 2.1771 (2.1674)  loss_mask: 0.6301 (0.6724)  time: 0.1703  data: 0.0002  max mem: 6052
[03:06:46.194106] Epoch: [3]  [420/781]  eta: 0:01:02  lr: 0.000177  training_loss: 2.8193 (2.8386)  classification_loss: 2.1285 (2.1666)  loss_mask: 0.6477 (0.6720)  time: 0.1713  data: 0.0002  max mem: 6052
[03:06:49.622849] Epoch: [3]  [440/781]  eta: 0:00:58  lr: 0.000178  training_loss: 2.8318 (2.8392)  classification_loss: 2.1490 (2.1652)  loss_mask: 0.6822 (0.6740)  time: 0.1713  data: 0.0002  max mem: 6052
[03:06:53.023850] Epoch: [3]  [460/781]  eta: 0:00:55  lr: 0.000179  training_loss: 2.7679 (2.8362)  classification_loss: 2.1396 (2.1647)  loss_mask: 0.6146 (0.6715)  time: 0.1699  data: 0.0002  max mem: 6052
[03:06:56.427592] Epoch: [3]  [480/781]  eta: 0:00:51  lr: 0.000181  training_loss: 2.7853 (2.8335)  classification_loss: 2.1688 (2.1646)  loss_mask: 0.6027 (0.6690)  time: 0.1701  data: 0.0002  max mem: 6052
[03:06:59.817065] Epoch: [3]  [500/781]  eta: 0:00:48  lr: 0.000182  training_loss: 2.7374 (2.8297)  classification_loss: 2.1380 (2.1638)  loss_mask: 0.5888 (0.6659)  time: 0.1694  data: 0.0002  max mem: 6052
[03:07:03.203788] Epoch: [3]  [520/781]  eta: 0:00:44  lr: 0.000183  training_loss: 2.7623 (2.8275)  classification_loss: 2.1626 (2.1636)  loss_mask: 0.6132 (0.6639)  time: 0.1693  data: 0.0001  max mem: 6052
[03:07:06.607729] Epoch: [3]  [540/781]  eta: 0:00:41  lr: 0.000185  training_loss: 2.8034 (2.8275)  classification_loss: 2.1572 (2.1642)  loss_mask: 0.6273 (0.6633)  time: 0.1701  data: 0.0002  max mem: 6052
[03:07:10.018529] Epoch: [3]  [560/781]  eta: 0:00:37  lr: 0.000186  training_loss: 2.7040 (2.8237)  classification_loss: 2.1279 (2.1631)  loss_mask: 0.6069 (0.6606)  time: 0.1705  data: 0.0006  max mem: 6052
[03:07:13.468336] Epoch: [3]  [580/781]  eta: 0:00:34  lr: 0.000187  training_loss: 2.7663 (2.8224)  classification_loss: 2.1618 (2.1632)  loss_mask: 0.5838 (0.6592)  time: 0.1724  data: 0.0002  max mem: 6052
[03:07:16.897933] Epoch: [3]  [600/781]  eta: 0:00:31  lr: 0.000188  training_loss: 2.8031 (2.8227)  classification_loss: 2.1623 (2.1631)  loss_mask: 0.6552 (0.6596)  time: 0.1714  data: 0.0003  max mem: 6052
[03:07:20.308651] Epoch: [3]  [620/781]  eta: 0:00:27  lr: 0.000190  training_loss: 2.7457 (2.8204)  classification_loss: 2.1340 (2.1626)  loss_mask: 0.5962 (0.6578)  time: 0.1704  data: 0.0002  max mem: 6052
[03:07:23.742324] Epoch: [3]  [640/781]  eta: 0:00:24  lr: 0.000191  training_loss: 2.7335 (2.8185)  classification_loss: 2.1409 (2.1622)  loss_mask: 0.5826 (0.6563)  time: 0.1716  data: 0.0003  max mem: 6052
[03:07:27.134282] Epoch: [3]  [660/781]  eta: 0:00:20  lr: 0.000192  training_loss: 2.7059 (2.8151)  classification_loss: 2.1395 (2.1620)  loss_mask: 0.5414 (0.6532)  time: 0.1695  data: 0.0003  max mem: 6052
[03:07:30.530029] Epoch: [3]  [680/781]  eta: 0:00:17  lr: 0.000194  training_loss: 2.7425 (2.8139)  classification_loss: 2.1308 (2.1612)  loss_mask: 0.6070 (0.6528)  time: 0.1697  data: 0.0003  max mem: 6052
[03:07:33.938375] Epoch: [3]  [700/781]  eta: 0:00:13  lr: 0.000195  training_loss: 2.7394 (2.8137)  classification_loss: 2.1347 (2.1604)  loss_mask: 0.6175 (0.6534)  time: 0.1700  data: 0.0002  max mem: 6052
[03:07:37.358400] Epoch: [3]  [720/781]  eta: 0:00:10  lr: 0.000196  training_loss: 2.7537 (2.8141)  classification_loss: 2.1334 (2.1598)  loss_mask: 0.6361 (0.6543)  time: 0.1709  data: 0.0002  max mem: 6052
[03:07:40.749953] Epoch: [3]  [740/781]  eta: 0:00:07  lr: 0.000197  training_loss: 2.7328 (2.8128)  classification_loss: 2.1320 (2.1594)  loss_mask: 0.6065 (0.6534)  time: 0.1695  data: 0.0001  max mem: 6052
[03:07:44.170805] Epoch: [3]  [760/781]  eta: 0:00:03  lr: 0.000199  training_loss: 2.7572 (2.8116)  classification_loss: 2.1453 (2.1589)  loss_mask: 0.6403 (0.6527)  time: 0.1709  data: 0.0003  max mem: 6052
[03:07:47.555158] Epoch: [3]  [780/781]  eta: 0:00:00  lr: 0.000200  training_loss: 2.7382 (2.8105)  classification_loss: 2.1404 (2.1584)  loss_mask: 0.6346 (0.6520)  time: 0.1691  data: 0.0001  max mem: 6052
[03:07:47.725967] Epoch: [3] Total time: 0:02:14 (0.1718 s / it)
[03:07:47.726433] Averaged stats: lr: 0.000200  training_loss: 2.7382 (2.8105)  classification_loss: 2.1404 (2.1584)  loss_mask: 0.6346 (0.6520)
[03:07:48.404199] Test:  [  0/157]  eta: 0:01:45  testing_loss: 1.7327 (1.7327)  acc1: 46.8750 (46.8750)  acc5: 90.6250 (90.6250)  time: 0.6736  data: 0.6441  max mem: 6052
[03:07:48.688568] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.8370 (1.8391)  acc1: 35.9375 (38.4943)  acc5: 84.3750 (85.0852)  time: 0.0869  data: 0.0587  max mem: 6052
[03:07:48.968425] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.8236 (1.8293)  acc1: 37.5000 (39.9554)  acc5: 85.9375 (86.3095)  time: 0.0281  data: 0.0002  max mem: 6052
[03:07:49.248308] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.8117 (1.8219)  acc1: 39.0625 (39.3649)  acc5: 85.9375 (86.1391)  time: 0.0279  data: 0.0002  max mem: 6052
[03:07:49.528136] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.8055 (1.8271)  acc1: 39.0625 (39.4055)  acc5: 84.3750 (85.5564)  time: 0.0279  data: 0.0002  max mem: 6052
[03:07:49.808573] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.8247 (1.8278)  acc1: 39.0625 (38.9400)  acc5: 85.9375 (85.8456)  time: 0.0279  data: 0.0002  max mem: 6052
[03:07:50.089144] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.7990 (1.8231)  acc1: 37.5000 (38.9600)  acc5: 87.5000 (86.0912)  time: 0.0279  data: 0.0002  max mem: 6052
[03:07:50.370044] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.7859 (1.8235)  acc1: 39.0625 (38.6444)  acc5: 87.5000 (86.1796)  time: 0.0280  data: 0.0002  max mem: 6052
[03:07:50.650713] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.8140 (1.8210)  acc1: 37.5000 (38.5610)  acc5: 89.0625 (86.4390)  time: 0.0280  data: 0.0002  max mem: 6052
[03:07:50.931890] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.8271 (1.8232)  acc1: 37.5000 (38.5474)  acc5: 85.9375 (86.4183)  time: 0.0280  data: 0.0001  max mem: 6052
[03:07:51.215802] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.8433 (1.8258)  acc1: 39.0625 (38.6293)  acc5: 85.9375 (86.3552)  time: 0.0281  data: 0.0001  max mem: 6052
[03:07:51.497583] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.8502 (1.8280)  acc1: 39.0625 (38.5135)  acc5: 84.3750 (86.2472)  time: 0.0282  data: 0.0002  max mem: 6052
[03:07:51.787276] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.8209 (1.8247)  acc1: 39.0625 (38.6235)  acc5: 87.5000 (86.4153)  time: 0.0284  data: 0.0002  max mem: 6052
[03:07:52.068336] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.8191 (1.8258)  acc1: 37.5000 (38.4900)  acc5: 89.0625 (86.4742)  time: 0.0283  data: 0.0001  max mem: 6052
[03:07:52.350100] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.8421 (1.8266)  acc1: 37.5000 (38.4973)  acc5: 85.9375 (86.4583)  time: 0.0280  data: 0.0001  max mem: 6052
[03:07:52.627922] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.8149 (1.8252)  acc1: 37.5000 (38.5141)  acc5: 85.9375 (86.4549)  time: 0.0279  data: 0.0001  max mem: 6052
[03:07:52.779360] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.8050 (1.8246)  acc1: 40.6250 (38.3600)  acc5: 85.9375 (86.4800)  time: 0.0269  data: 0.0001  max mem: 6052
[03:07:52.934731] Test: Total time: 0:00:05 (0.0332 s / it)
[03:07:52.935194] * Acc@1 38.360 Acc@5 86.480 loss 1.825
[03:07:52.935527] Accuracy of the network on the 10000 test images: 38.4%
[03:07:52.935712] Max accuracy: 38.36%
[03:07:53.248029] log_dir: ./output_dir
[03:07:54.130932] Epoch: [4]  [  0/781]  eta: 0:11:28  lr: 0.000200  training_loss: 2.7242 (2.7242)  classification_loss: 2.1180 (2.1180)  loss_mask: 0.6062 (0.6062)  time: 0.8812  data: 0.6854  max mem: 6052
[03:07:57.627041] Epoch: [4]  [ 20/781]  eta: 0:02:38  lr: 0.000201  training_loss: 2.7460 (2.7484)  classification_loss: 2.1258 (2.1222)  loss_mask: 0.6572 (0.6262)  time: 0.1747  data: 0.0002  max mem: 6052
[03:08:01.040274] Epoch: [4]  [ 40/781]  eta: 0:02:20  lr: 0.000203  training_loss: 2.7606 (2.7652)  classification_loss: 2.1352 (2.1344)  loss_mask: 0.6268 (0.6308)  time: 0.1706  data: 0.0002  max mem: 6052
[03:08:04.446602] Epoch: [4]  [ 60/781]  eta: 0:02:12  lr: 0.000204  training_loss: 2.7888 (2.7656)  classification_loss: 2.1342 (2.1353)  loss_mask: 0.6133 (0.6303)  time: 0.1702  data: 0.0003  max mem: 6052
[03:08:07.863625] Epoch: [4]  [ 80/781]  eta: 0:02:06  lr: 0.000205  training_loss: 2.6761 (2.7440)  classification_loss: 2.1483 (2.1374)  loss_mask: 0.5193 (0.6066)  time: 0.1708  data: 0.0004  max mem: 6052
[03:08:11.309526] Epoch: [4]  [100/781]  eta: 0:02:01  lr: 0.000206  training_loss: 2.7308 (2.7470)  classification_loss: 2.1404 (2.1413)  loss_mask: 0.5634 (0.6056)  time: 0.1722  data: 0.0003  max mem: 6052
[03:08:14.754890] Epoch: [4]  [120/781]  eta: 0:01:57  lr: 0.000208  training_loss: 2.9276 (2.7765)  classification_loss: 2.1469 (2.1405)  loss_mask: 0.7535 (0.6359)  time: 0.1722  data: 0.0002  max mem: 6052
[03:08:18.175133] Epoch: [4]  [140/781]  eta: 0:01:53  lr: 0.000209  training_loss: 2.6904 (2.7692)  classification_loss: 2.0896 (2.1366)  loss_mask: 0.5804 (0.6326)  time: 0.1709  data: 0.0003  max mem: 6052
[03:08:21.589272] Epoch: [4]  [160/781]  eta: 0:01:49  lr: 0.000210  training_loss: 2.7207 (2.7672)  classification_loss: 2.0980 (2.1329)  loss_mask: 0.6357 (0.6343)  time: 0.1706  data: 0.0002  max mem: 6052
[03:08:25.034550] Epoch: [4]  [180/781]  eta: 0:01:45  lr: 0.000212  training_loss: 2.8538 (2.7818)  classification_loss: 2.1765 (2.1384)  loss_mask: 0.6883 (0.6435)  time: 0.1722  data: 0.0002  max mem: 6052
[03:08:28.459645] Epoch: [4]  [200/781]  eta: 0:01:41  lr: 0.000213  training_loss: 2.8140 (2.7875)  classification_loss: 2.1509 (2.1422)  loss_mask: 0.6733 (0.6453)  time: 0.1712  data: 0.0003  max mem: 6052
[03:08:31.913288] Epoch: [4]  [220/781]  eta: 0:01:38  lr: 0.000214  training_loss: 2.6884 (2.7796)  classification_loss: 2.1241 (2.1416)  loss_mask: 0.5725 (0.6380)  time: 0.1726  data: 0.0002  max mem: 6052
[03:08:35.402659] Epoch: [4]  [240/781]  eta: 0:01:34  lr: 0.000215  training_loss: 2.6902 (2.7706)  classification_loss: 2.1041 (2.1397)  loss_mask: 0.5413 (0.6309)  time: 0.1744  data: 0.0002  max mem: 6052
[03:08:38.828033] Epoch: [4]  [260/781]  eta: 0:01:30  lr: 0.000217  training_loss: 2.7913 (2.7722)  classification_loss: 2.1242 (2.1380)  loss_mask: 0.6467 (0.6341)  time: 0.1712  data: 0.0002  max mem: 6052
[03:08:42.233382] Epoch: [4]  [280/781]  eta: 0:01:27  lr: 0.000218  training_loss: 2.7535 (2.7728)  classification_loss: 2.1379 (2.1383)  loss_mask: 0.6136 (0.6345)  time: 0.1702  data: 0.0002  max mem: 6052
[03:08:45.650834] Epoch: [4]  [300/781]  eta: 0:01:23  lr: 0.000219  training_loss: 2.7308 (2.7704)  classification_loss: 2.1722 (2.1392)  loss_mask: 0.5509 (0.6312)  time: 0.1708  data: 0.0003  max mem: 6052
[03:08:49.087104] Epoch: [4]  [320/781]  eta: 0:01:20  lr: 0.000220  training_loss: 2.6563 (2.7638)  classification_loss: 2.1213 (2.1378)  loss_mask: 0.5339 (0.6260)  time: 0.1717  data: 0.0002  max mem: 6052
[03:08:52.494778] Epoch: [4]  [340/781]  eta: 0:01:16  lr: 0.000222  training_loss: 2.6855 (2.7603)  classification_loss: 2.1092 (2.1376)  loss_mask: 0.5616 (0.6228)  time: 0.1703  data: 0.0002  max mem: 6052
[03:08:55.896541] Epoch: [4]  [360/781]  eta: 0:01:13  lr: 0.000223  training_loss: 2.7607 (2.7607)  classification_loss: 2.1349 (2.1377)  loss_mask: 0.6264 (0.6231)  time: 0.1700  data: 0.0002  max mem: 6052
[03:08:59.331476] Epoch: [4]  [380/781]  eta: 0:01:09  lr: 0.000224  training_loss: 2.6408 (2.7566)  classification_loss: 2.1426 (2.1368)  loss_mask: 0.5123 (0.6198)  time: 0.1716  data: 0.0002  max mem: 6052
[03:09:02.742349] Epoch: [4]  [400/781]  eta: 0:01:05  lr: 0.000226  training_loss: 2.7111 (2.7566)  classification_loss: 2.0954 (2.1355)  loss_mask: 0.5999 (0.6211)  time: 0.1705  data: 0.0002  max mem: 6052
[03:09:06.177063] Epoch: [4]  [420/781]  eta: 0:01:02  lr: 0.000227  training_loss: 2.6588 (2.7535)  classification_loss: 2.0992 (2.1343)  loss_mask: 0.5771 (0.6192)  time: 0.1716  data: 0.0003  max mem: 6052
[03:09:09.578006] Epoch: [4]  [440/781]  eta: 0:00:58  lr: 0.000228  training_loss: 2.7150 (2.7515)  classification_loss: 2.1071 (2.1331)  loss_mask: 0.5835 (0.6185)  time: 0.1700  data: 0.0002  max mem: 6052
[03:09:12.979587] Epoch: [4]  [460/781]  eta: 0:00:55  lr: 0.000229  training_loss: 2.6369 (2.7467)  classification_loss: 2.0951 (2.1321)  loss_mask: 0.5173 (0.6146)  time: 0.1700  data: 0.0003  max mem: 6052
[03:09:16.397450] Epoch: [4]  [480/781]  eta: 0:00:52  lr: 0.000231  training_loss: 2.7578 (2.7469)  classification_loss: 2.1459 (2.1327)  loss_mask: 0.6102 (0.6142)  time: 0.1708  data: 0.0003  max mem: 6052
[03:09:19.812308] Epoch: [4]  [500/781]  eta: 0:00:48  lr: 0.000232  training_loss: 2.7436 (2.7477)  classification_loss: 2.0975 (2.1316)  loss_mask: 0.6298 (0.6161)  time: 0.1707  data: 0.0003  max mem: 6052
[03:09:23.223598] Epoch: [4]  [520/781]  eta: 0:00:45  lr: 0.000233  training_loss: 2.6918 (2.7462)  classification_loss: 2.1116 (2.1306)  loss_mask: 0.5915 (0.6155)  time: 0.1705  data: 0.0002  max mem: 6052
[03:09:26.630236] Epoch: [4]  [540/781]  eta: 0:00:41  lr: 0.000235  training_loss: 2.6765 (2.7450)  classification_loss: 2.1263 (2.1306)  loss_mask: 0.5847 (0.6144)  time: 0.1703  data: 0.0002  max mem: 6052
[03:09:30.048842] Epoch: [4]  [560/781]  eta: 0:00:38  lr: 0.000236  training_loss: 2.6138 (2.7414)  classification_loss: 2.0974 (2.1295)  loss_mask: 0.5276 (0.6119)  time: 0.1708  data: 0.0002  max mem: 6052
[03:09:33.454640] Epoch: [4]  [580/781]  eta: 0:00:34  lr: 0.000237  training_loss: 2.6614 (2.7397)  classification_loss: 2.1007 (2.1286)  loss_mask: 0.5577 (0.6111)  time: 0.1702  data: 0.0002  max mem: 6052
[03:09:36.877350] Epoch: [4]  [600/781]  eta: 0:00:31  lr: 0.000238  training_loss: 2.7797 (2.7417)  classification_loss: 2.1152 (2.1284)  loss_mask: 0.6838 (0.6133)  time: 0.1710  data: 0.0002  max mem: 6052
[03:09:40.314417] Epoch: [4]  [620/781]  eta: 0:00:27  lr: 0.000240  training_loss: 2.7168 (2.7407)  classification_loss: 2.0789 (2.1270)  loss_mask: 0.5870 (0.6136)  time: 0.1718  data: 0.0002  max mem: 6052
[03:09:43.730926] Epoch: [4]  [640/781]  eta: 0:00:24  lr: 0.000241  training_loss: 2.6916 (2.7392)  classification_loss: 2.0883 (2.1263)  loss_mask: 0.5983 (0.6130)  time: 0.1708  data: 0.0002  max mem: 6052
[03:09:47.144537] Epoch: [4]  [660/781]  eta: 0:00:20  lr: 0.000242  training_loss: 2.6680 (2.7370)  classification_loss: 2.1143 (2.1254)  loss_mask: 0.5468 (0.6115)  time: 0.1706  data: 0.0002  max mem: 6052
[03:09:50.576371] Epoch: [4]  [680/781]  eta: 0:00:17  lr: 0.000244  training_loss: 2.7023 (2.7356)  classification_loss: 2.1247 (2.1252)  loss_mask: 0.5717 (0.6105)  time: 0.1715  data: 0.0002  max mem: 6052
[03:09:54.008523] Epoch: [4]  [700/781]  eta: 0:00:13  lr: 0.000245  training_loss: 2.8320 (2.7404)  classification_loss: 2.1010 (2.1254)  loss_mask: 0.7447 (0.6150)  time: 0.1715  data: 0.0003  max mem: 6052
[03:09:57.438978] Epoch: [4]  [720/781]  eta: 0:00:10  lr: 0.000246  training_loss: 2.7716 (2.7410)  classification_loss: 2.1548 (2.1265)  loss_mask: 0.5934 (0.6144)  time: 0.1714  data: 0.0003  max mem: 6052
[03:10:00.877214] Epoch: [4]  [740/781]  eta: 0:00:07  lr: 0.000247  training_loss: 2.7256 (2.7401)  classification_loss: 2.1322 (2.1268)  loss_mask: 0.5657 (0.6133)  time: 0.1718  data: 0.0002  max mem: 6052
[03:10:04.298412] Epoch: [4]  [760/781]  eta: 0:00:03  lr: 0.000249  training_loss: 2.7020 (2.7388)  classification_loss: 2.1187 (2.1269)  loss_mask: 0.5722 (0.6118)  time: 0.1710  data: 0.0002  max mem: 6052
[03:10:07.699954] Epoch: [4]  [780/781]  eta: 0:00:00  lr: 0.000250  training_loss: 2.6320 (2.7359)  classification_loss: 2.1064 (2.1267)  loss_mask: 0.5081 (0.6092)  time: 0.1700  data: 0.0002  max mem: 6052
[03:10:07.861464] Epoch: [4] Total time: 0:02:14 (0.1724 s / it)
[03:10:07.862481] Averaged stats: lr: 0.000250  training_loss: 2.6320 (2.7359)  classification_loss: 2.1064 (2.1267)  loss_mask: 0.5081 (0.6092)
[03:10:08.507754] Test:  [  0/157]  eta: 0:01:40  testing_loss: 1.6899 (1.6899)  acc1: 39.0625 (39.0625)  acc5: 87.5000 (87.5000)  time: 0.6402  data: 0.6109  max mem: 6052
[03:10:08.790997] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.7615 (1.7560)  acc1: 43.7500 (40.9091)  acc5: 90.6250 (90.1989)  time: 0.0837  data: 0.0557  max mem: 6052
[03:10:09.072040] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.7321 (1.7338)  acc1: 43.7500 (41.6667)  acc5: 90.6250 (89.5089)  time: 0.0280  data: 0.0002  max mem: 6052
[03:10:09.352366] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.7128 (1.7276)  acc1: 43.7500 (42.7419)  acc5: 89.0625 (89.5665)  time: 0.0280  data: 0.0002  max mem: 6052
[03:10:09.631858] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.7277 (1.7345)  acc1: 42.1875 (41.8826)  acc5: 89.0625 (89.5960)  time: 0.0279  data: 0.0002  max mem: 6052
[03:10:09.911457] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.7372 (1.7355)  acc1: 42.1875 (41.8199)  acc5: 90.6250 (89.8897)  time: 0.0278  data: 0.0002  max mem: 6052
[03:10:10.190958] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.7277 (1.7318)  acc1: 43.7500 (42.1875)  acc5: 89.0625 (89.9078)  time: 0.0278  data: 0.0002  max mem: 6052
[03:10:10.476488] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.7122 (1.7305)  acc1: 42.1875 (41.8134)  acc5: 90.6250 (89.9868)  time: 0.0281  data: 0.0002  max mem: 6052
[03:10:10.755950] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.7271 (1.7321)  acc1: 42.1875 (41.7631)  acc5: 89.0625 (89.8727)  time: 0.0281  data: 0.0002  max mem: 6052
[03:10:11.035753] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.7558 (1.7352)  acc1: 42.1875 (41.7582)  acc5: 87.5000 (89.7493)  time: 0.0279  data: 0.0002  max mem: 6052
[03:10:11.315224] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.7577 (1.7377)  acc1: 42.1875 (41.6770)  acc5: 87.5000 (89.6040)  time: 0.0278  data: 0.0002  max mem: 6052
[03:10:11.594390] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.7755 (1.7391)  acc1: 40.6250 (41.5822)  acc5: 87.5000 (89.4566)  time: 0.0278  data: 0.0002  max mem: 6052
[03:10:11.873838] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.7150 (1.7361)  acc1: 40.6250 (41.5160)  acc5: 89.0625 (89.6049)  time: 0.0278  data: 0.0002  max mem: 6052
[03:10:12.155091] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.7425 (1.7382)  acc1: 40.6250 (41.6627)  acc5: 89.0625 (89.4323)  time: 0.0279  data: 0.0002  max mem: 6052
[03:10:12.435997] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.7473 (1.7371)  acc1: 42.1875 (41.9105)  acc5: 89.0625 (89.4725)  time: 0.0280  data: 0.0002  max mem: 6052
[03:10:12.713801] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.7335 (1.7365)  acc1: 45.3125 (42.0737)  acc5: 89.0625 (89.4764)  time: 0.0278  data: 0.0001  max mem: 6052
[03:10:12.862365] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.7127 (1.7362)  acc1: 43.7500 (41.9400)  acc5: 89.0625 (89.4900)  time: 0.0268  data: 0.0001  max mem: 6052
[03:10:13.029119] Test: Total time: 0:00:05 (0.0329 s / it)
[03:10:13.029548] * Acc@1 41.940 Acc@5 89.490 loss 1.736
[03:10:13.029872] Accuracy of the network on the 10000 test images: 41.9%
[03:10:13.030049] Max accuracy: 41.94%
[03:10:13.338644] log_dir: ./output_dir
[03:10:14.216756] Epoch: [5]  [  0/781]  eta: 0:11:24  lr: 0.000250  training_loss: 2.6011 (2.6011)  classification_loss: 2.0323 (2.0323)  loss_mask: 0.5688 (0.5688)  time: 0.8765  data: 0.6819  max mem: 6052
[03:10:17.634397] Epoch: [5]  [ 20/781]  eta: 0:02:35  lr: 0.000250  training_loss: 2.6425 (2.6547)  classification_loss: 2.0592 (2.0618)  loss_mask: 0.5991 (0.5929)  time: 0.1708  data: 0.0002  max mem: 6052
[03:10:21.048503] Epoch: [5]  [ 40/781]  eta: 0:02:19  lr: 0.000250  training_loss: 2.7809 (2.7345)  classification_loss: 2.1652 (2.1100)  loss_mask: 0.6176 (0.6245)  time: 0.1706  data: 0.0003  max mem: 6052
[03:10:24.447898] Epoch: [5]  [ 60/781]  eta: 0:02:11  lr: 0.000250  training_loss: 2.6200 (2.6996)  classification_loss: 2.0946 (2.1077)  loss_mask: 0.4972 (0.5919)  time: 0.1699  data: 0.0003  max mem: 6052
[03:10:27.849791] Epoch: [5]  [ 80/781]  eta: 0:02:05  lr: 0.000250  training_loss: 2.5677 (2.6683)  classification_loss: 2.1122 (2.1058)  loss_mask: 0.4517 (0.5625)  time: 0.1700  data: 0.0003  max mem: 6052
[03:10:31.261577] Epoch: [5]  [100/781]  eta: 0:02:00  lr: 0.000250  training_loss: 2.6803 (2.6750)  classification_loss: 2.1016 (2.1064)  loss_mask: 0.5637 (0.5686)  time: 0.1705  data: 0.0002  max mem: 6052
[03:10:34.673734] Epoch: [5]  [120/781]  eta: 0:01:56  lr: 0.000250  training_loss: 2.6111 (2.6683)  classification_loss: 2.0818 (2.1037)  loss_mask: 0.5405 (0.5646)  time: 0.1705  data: 0.0003  max mem: 6052
[03:10:38.100616] Epoch: [5]  [140/781]  eta: 0:01:52  lr: 0.000250  training_loss: 2.5797 (2.6558)  classification_loss: 2.0562 (2.0996)  loss_mask: 0.4729 (0.5562)  time: 0.1713  data: 0.0002  max mem: 6052
[03:10:41.530065] Epoch: [5]  [160/781]  eta: 0:01:48  lr: 0.000250  training_loss: 2.5737 (2.6489)  classification_loss: 2.1050 (2.1012)  loss_mask: 0.4573 (0.5477)  time: 0.1714  data: 0.0002  max mem: 6052
[03:10:44.929011] Epoch: [5]  [180/781]  eta: 0:01:44  lr: 0.000250  training_loss: 2.7857 (2.6629)  classification_loss: 2.1235 (2.1026)  loss_mask: 0.6320 (0.5603)  time: 0.1699  data: 0.0002  max mem: 6052
[03:10:48.342409] Epoch: [5]  [200/781]  eta: 0:01:41  lr: 0.000250  training_loss: 2.6884 (2.6705)  classification_loss: 2.0848 (2.1018)  loss_mask: 0.5827 (0.5686)  time: 0.1706  data: 0.0003  max mem: 6052
[03:10:51.741266] Epoch: [5]  [220/781]  eta: 0:01:37  lr: 0.000250  training_loss: 2.5993 (2.6660)  classification_loss: 2.1113 (2.1029)  loss_mask: 0.5007 (0.5631)  time: 0.1699  data: 0.0003  max mem: 6052
[03:10:55.134603] Epoch: [5]  [240/781]  eta: 0:01:33  lr: 0.000250  training_loss: 2.5710 (2.6604)  classification_loss: 2.1137 (2.1036)  loss_mask: 0.4765 (0.5567)  time: 0.1696  data: 0.0003  max mem: 6052
[03:10:58.540567] Epoch: [5]  [260/781]  eta: 0:01:30  lr: 0.000250  training_loss: 2.6053 (2.6569)  classification_loss: 2.0914 (2.1019)  loss_mask: 0.5123 (0.5550)  time: 0.1702  data: 0.0002  max mem: 6052
[03:11:02.017405] Epoch: [5]  [280/781]  eta: 0:01:26  lr: 0.000250  training_loss: 2.6242 (2.6567)  classification_loss: 2.0610 (2.1005)  loss_mask: 0.5263 (0.5562)  time: 0.1738  data: 0.0002  max mem: 6052
[03:11:05.430090] Epoch: [5]  [300/781]  eta: 0:01:23  lr: 0.000250  training_loss: 2.7058 (2.6621)  classification_loss: 2.1288 (2.1021)  loss_mask: 0.5913 (0.5599)  time: 0.1706  data: 0.0002  max mem: 6052
[03:11:08.852685] Epoch: [5]  [320/781]  eta: 0:01:19  lr: 0.000250  training_loss: 2.6252 (2.6592)  classification_loss: 2.1201 (2.1027)  loss_mask: 0.4916 (0.5565)  time: 0.1711  data: 0.0002  max mem: 6052
[03:11:12.253432] Epoch: [5]  [340/781]  eta: 0:01:16  lr: 0.000250  training_loss: 2.5162 (2.6521)  classification_loss: 2.0747 (2.1015)  loss_mask: 0.4291 (0.5506)  time: 0.1700  data: 0.0002  max mem: 6052
[03:11:15.680940] Epoch: [5]  [360/781]  eta: 0:01:12  lr: 0.000250  training_loss: 2.6143 (2.6503)  classification_loss: 2.0988 (2.1012)  loss_mask: 0.5309 (0.5491)  time: 0.1713  data: 0.0002  max mem: 6052
[03:11:19.093601] Epoch: [5]  [380/781]  eta: 0:01:09  lr: 0.000250  training_loss: 2.6826 (2.6546)  classification_loss: 2.0654 (2.0998)  loss_mask: 0.6317 (0.5548)  time: 0.1705  data: 0.0003  max mem: 6052
[03:11:22.497297] Epoch: [5]  [400/781]  eta: 0:01:05  lr: 0.000250  training_loss: 2.6618 (2.6551)  classification_loss: 2.0899 (2.0996)  loss_mask: 0.5487 (0.5555)  time: 0.1701  data: 0.0002  max mem: 6052
[03:11:25.917411] Epoch: [5]  [420/781]  eta: 0:01:02  lr: 0.000250  training_loss: 2.6111 (2.6525)  classification_loss: 2.0896 (2.0995)  loss_mask: 0.5083 (0.5529)  time: 0.1709  data: 0.0002  max mem: 6052
[03:11:29.331824] Epoch: [5]  [440/781]  eta: 0:00:58  lr: 0.000250  training_loss: 2.5825 (2.6502)  classification_loss: 2.0538 (2.0980)  loss_mask: 0.5426 (0.5522)  time: 0.1707  data: 0.0002  max mem: 6052
[03:11:32.744245] Epoch: [5]  [460/781]  eta: 0:00:55  lr: 0.000250  training_loss: 2.5600 (2.6465)  classification_loss: 2.0589 (2.0967)  loss_mask: 0.5066 (0.5498)  time: 0.1706  data: 0.0002  max mem: 6052
[03:11:36.157389] Epoch: [5]  [480/781]  eta: 0:00:51  lr: 0.000250  training_loss: 2.6014 (2.6442)  classification_loss: 2.1088 (2.0973)  loss_mask: 0.4756 (0.5469)  time: 0.1706  data: 0.0002  max mem: 6052
[03:11:39.588758] Epoch: [5]  [500/781]  eta: 0:00:48  lr: 0.000250  training_loss: 2.6323 (2.6436)  classification_loss: 2.0611 (2.0965)  loss_mask: 0.5482 (0.5471)  time: 0.1715  data: 0.0002  max mem: 6052
[03:11:43.013347] Epoch: [5]  [520/781]  eta: 0:00:44  lr: 0.000250  training_loss: 2.6220 (2.6435)  classification_loss: 2.0523 (2.0952)  loss_mask: 0.5698 (0.5483)  time: 0.1712  data: 0.0002  max mem: 6052
[03:11:46.414204] Epoch: [5]  [540/781]  eta: 0:00:41  lr: 0.000250  training_loss: 2.5900 (2.6416)  classification_loss: 2.0739 (2.0952)  loss_mask: 0.4836 (0.5465)  time: 0.1699  data: 0.0002  max mem: 6052
[03:11:49.824608] Epoch: [5]  [560/781]  eta: 0:00:37  lr: 0.000250  training_loss: 2.5557 (2.6389)  classification_loss: 2.0386 (2.0938)  loss_mask: 0.4949 (0.5451)  time: 0.1704  data: 0.0003  max mem: 6052
[03:11:53.224168] Epoch: [5]  [580/781]  eta: 0:00:34  lr: 0.000250  training_loss: 2.5272 (2.6355)  classification_loss: 2.0730 (2.0933)  loss_mask: 0.4422 (0.5422)  time: 0.1699  data: 0.0003  max mem: 6052
[03:11:56.644691] Epoch: [5]  [600/781]  eta: 0:00:31  lr: 0.000250  training_loss: 2.5988 (2.6346)  classification_loss: 2.0903 (2.0934)  loss_mask: 0.4535 (0.5412)  time: 0.1709  data: 0.0003  max mem: 6052
[03:12:00.071720] Epoch: [5]  [620/781]  eta: 0:00:27  lr: 0.000250  training_loss: 2.4476 (2.6294)  classification_loss: 2.0382 (2.0922)  loss_mask: 0.3824 (0.5372)  time: 0.1713  data: 0.0002  max mem: 6052
[03:12:03.496522] Epoch: [5]  [640/781]  eta: 0:00:24  lr: 0.000250  training_loss: 2.5913 (2.6300)  classification_loss: 2.0761 (2.0914)  loss_mask: 0.5194 (0.5386)  time: 0.1712  data: 0.0002  max mem: 6052
[03:12:06.923335] Epoch: [5]  [660/781]  eta: 0:00:20  lr: 0.000250  training_loss: 2.5856 (2.6287)  classification_loss: 2.0681 (2.0905)  loss_mask: 0.4998 (0.5382)  time: 0.1713  data: 0.0003  max mem: 6052
[03:12:10.347798] Epoch: [5]  [680/781]  eta: 0:00:17  lr: 0.000250  training_loss: 2.5185 (2.6270)  classification_loss: 2.0456 (2.0896)  loss_mask: 0.4791 (0.5375)  time: 0.1711  data: 0.0003  max mem: 6052
[03:12:13.755529] Epoch: [5]  [700/781]  eta: 0:00:13  lr: 0.000250  training_loss: 2.4841 (2.6241)  classification_loss: 2.0707 (2.0889)  loss_mask: 0.4404 (0.5352)  time: 0.1703  data: 0.0002  max mem: 6052
[03:12:17.182004] Epoch: [5]  [720/781]  eta: 0:00:10  lr: 0.000250  training_loss: 2.5578 (2.6220)  classification_loss: 2.0758 (2.0879)  loss_mask: 0.4835 (0.5341)  time: 0.1712  data: 0.0002  max mem: 6052
[03:12:20.610830] Epoch: [5]  [740/781]  eta: 0:00:07  lr: 0.000250  training_loss: 2.5090 (2.6187)  classification_loss: 2.0893 (2.0877)  loss_mask: 0.4137 (0.5310)  time: 0.1713  data: 0.0003  max mem: 6052
[03:12:24.058470] Epoch: [5]  [760/781]  eta: 0:00:03  lr: 0.000250  training_loss: 2.6147 (2.6184)  classification_loss: 2.0751 (2.0872)  loss_mask: 0.5105 (0.5313)  time: 0.1723  data: 0.0002  max mem: 6052
[03:12:27.480936] Epoch: [5]  [780/781]  eta: 0:00:00  lr: 0.000250  training_loss: 2.5939 (2.6191)  classification_loss: 2.0550 (2.0869)  loss_mask: 0.5023 (0.5322)  time: 0.1710  data: 0.0002  max mem: 6052
[03:12:27.633298] Epoch: [5] Total time: 0:02:14 (0.1720 s / it)
[03:12:27.634649] Averaged stats: lr: 0.000250  training_loss: 2.5939 (2.6191)  classification_loss: 2.0550 (2.0869)  loss_mask: 0.5023 (0.5322)
[03:12:28.289273] Test:  [  0/157]  eta: 0:01:42  testing_loss: 1.6226 (1.6226)  acc1: 46.8750 (46.8750)  acc5: 85.9375 (85.9375)  time: 0.6505  data: 0.6151  max mem: 6052
[03:12:28.571667] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.6382 (1.6650)  acc1: 43.7500 (44.0341)  acc5: 92.1875 (91.0511)  time: 0.0847  data: 0.0561  max mem: 6052
[03:12:28.852750] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.6338 (1.6407)  acc1: 46.8750 (46.8750)  acc5: 92.1875 (91.5923)  time: 0.0280  data: 0.0002  max mem: 6052
[03:12:29.137861] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.6114 (1.6371)  acc1: 50.0000 (47.1270)  acc5: 92.1875 (91.5827)  time: 0.0282  data: 0.0002  max mem: 6052
[03:12:29.421099] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.6178 (1.6462)  acc1: 48.4375 (46.5320)  acc5: 90.6250 (91.0061)  time: 0.0283  data: 0.0002  max mem: 6052
[03:12:29.709025] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.6497 (1.6448)  acc1: 48.4375 (46.6299)  acc5: 90.6250 (91.1765)  time: 0.0284  data: 0.0001  max mem: 6052
[03:12:29.997000] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.6173 (1.6395)  acc1: 48.4375 (46.8238)  acc5: 90.6250 (91.2910)  time: 0.0287  data: 0.0001  max mem: 6052
[03:12:30.284784] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.6083 (1.6369)  acc1: 48.4375 (47.0070)  acc5: 93.7500 (91.5053)  time: 0.0287  data: 0.0001  max mem: 6052
[03:12:30.569089] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.6167 (1.6374)  acc1: 46.8750 (46.9522)  acc5: 93.7500 (91.6088)  time: 0.0285  data: 0.0002  max mem: 6052
[03:12:30.850004] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.6585 (1.6416)  acc1: 45.3125 (46.7720)  acc5: 90.6250 (91.4835)  time: 0.0281  data: 0.0001  max mem: 6052
[03:12:31.130406] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.6945 (1.6449)  acc1: 43.7500 (46.3800)  acc5: 90.6250 (91.3366)  time: 0.0280  data: 0.0001  max mem: 6052
[03:12:31.412025] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.6886 (1.6475)  acc1: 42.1875 (46.1712)  acc5: 90.6250 (91.2021)  time: 0.0280  data: 0.0002  max mem: 6052
[03:12:31.693434] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.6222 (1.6449)  acc1: 45.3125 (46.3456)  acc5: 90.6250 (91.2965)  time: 0.0280  data: 0.0002  max mem: 6052
[03:12:31.977016] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.6218 (1.6472)  acc1: 46.8750 (46.1951)  acc5: 92.1875 (91.1021)  time: 0.0281  data: 0.0002  max mem: 6052
[03:12:32.263087] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.6418 (1.6474)  acc1: 45.3125 (46.2988)  acc5: 90.6250 (91.1458)  time: 0.0284  data: 0.0001  max mem: 6052
[03:12:32.546033] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.6370 (1.6469)  acc1: 48.4375 (46.4507)  acc5: 92.1875 (91.1217)  time: 0.0283  data: 0.0001  max mem: 6052
[03:12:32.695044] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.6185 (1.6461)  acc1: 48.4375 (46.3700)  acc5: 90.6250 (91.1100)  time: 0.0270  data: 0.0001  max mem: 6052
[03:12:32.846539] Test: Total time: 0:00:05 (0.0332 s / it)
[03:12:32.846980] * Acc@1 46.370 Acc@5 91.110 loss 1.646
[03:12:32.847270] Accuracy of the network on the 10000 test images: 46.4%
[03:12:32.847447] Max accuracy: 46.37%
[03:12:33.048642] log_dir: ./output_dir
[03:12:33.917557] Epoch: [6]  [  0/781]  eta: 0:11:17  lr: 0.000250  training_loss: 2.5813 (2.5813)  classification_loss: 2.0913 (2.0913)  loss_mask: 0.4900 (0.4900)  time: 0.8674  data: 0.6673  max mem: 6052
[03:12:37.354819] Epoch: [6]  [ 20/781]  eta: 0:02:35  lr: 0.000250  training_loss: 2.5399 (2.5479)  classification_loss: 2.0574 (2.0630)  loss_mask: 0.4820 (0.4849)  time: 0.1718  data: 0.0002  max mem: 6052
[03:12:40.763241] Epoch: [6]  [ 40/781]  eta: 0:02:19  lr: 0.000250  training_loss: 2.4489 (2.5092)  classification_loss: 2.0302 (2.0565)  loss_mask: 0.4332 (0.4528)  time: 0.1703  data: 0.0003  max mem: 6052
[03:12:44.175838] Epoch: [6]  [ 60/781]  eta: 0:02:11  lr: 0.000250  training_loss: 2.4883 (2.4989)  classification_loss: 2.0467 (2.0585)  loss_mask: 0.4031 (0.4404)  time: 0.1705  data: 0.0002  max mem: 6052
[03:12:47.589176] Epoch: [6]  [ 80/781]  eta: 0:02:05  lr: 0.000250  training_loss: 2.5660 (2.5236)  classification_loss: 2.0740 (2.0636)  loss_mask: 0.4915 (0.4600)  time: 0.1706  data: 0.0002  max mem: 6052
[03:12:51.002247] Epoch: [6]  [100/781]  eta: 0:02:00  lr: 0.000250  training_loss: 2.5564 (2.5369)  classification_loss: 2.0548 (2.0656)  loss_mask: 0.5331 (0.4712)  time: 0.1706  data: 0.0002  max mem: 6052
[03:12:54.415327] Epoch: [6]  [120/781]  eta: 0:01:56  lr: 0.000250  training_loss: 2.5020 (2.5343)  classification_loss: 2.0292 (2.0615)  loss_mask: 0.4544 (0.4728)  time: 0.1706  data: 0.0002  max mem: 6052
[03:12:57.906749] Epoch: [6]  [140/781]  eta: 0:01:52  lr: 0.000250  training_loss: 2.4381 (2.5228)  classification_loss: 2.0669 (2.0595)  loss_mask: 0.3946 (0.4634)  time: 0.1745  data: 0.0002  max mem: 6052
[03:13:01.380772] Epoch: [6]  [160/781]  eta: 0:01:49  lr: 0.000250  training_loss: 2.5298 (2.5225)  classification_loss: 2.0943 (2.0642)  loss_mask: 0.4353 (0.4583)  time: 0.1736  data: 0.0002  max mem: 6052
[03:13:04.856296] Epoch: [6]  [180/781]  eta: 0:01:45  lr: 0.000250  training_loss: 2.5050 (2.5196)  classification_loss: 2.0364 (2.0629)  loss_mask: 0.4436 (0.4567)  time: 0.1737  data: 0.0002  max mem: 6052
[03:13:08.381087] Epoch: [6]  [200/781]  eta: 0:01:42  lr: 0.000250  training_loss: 2.5135 (2.5274)  classification_loss: 2.0830 (2.0653)  loss_mask: 0.4855 (0.4621)  time: 0.1762  data: 0.0002  max mem: 6052
[03:13:11.798661] Epoch: [6]  [220/781]  eta: 0:01:38  lr: 0.000250  training_loss: 2.5366 (2.5310)  classification_loss: 2.0795 (2.0648)  loss_mask: 0.4550 (0.4662)  time: 0.1708  data: 0.0002  max mem: 6052
[03:13:15.214362] Epoch: [6]  [240/781]  eta: 0:01:34  lr: 0.000250  training_loss: 2.5266 (2.5308)  classification_loss: 2.0846 (2.0670)  loss_mask: 0.4213 (0.4637)  time: 0.1707  data: 0.0002  max mem: 6052

[03:13:18.680128] Epoch: [6]  [260/781]  eta: 0:01:31  lr: 0.000250  training_loss: 2.4413 (2.5259)  classification_loss: 2.0192 (2.0640)  loss_mask: 0.4360 (0.4619)  time: 0.1732  data: 0.0003  max mem: 6052
[03:13:22.113110] Epoch: [6]  [280/781]  eta: 0:01:27  lr: 0.000250  training_loss: 2.4300 (2.5202)  classification_loss: 2.0104 (2.0623)  loss_mask: 0.4145 (0.4578)  time: 0.1716  data: 0.0002  max mem: 6052
[03:13:25.524841] Epoch: [6]  [300/781]  eta: 0:01:23  lr: 0.000250  training_loss: 2.4362 (2.5163)  classification_loss: 2.0617 (2.0623)  loss_mask: 0.3850 (0.4541)  time: 0.1705  data: 0.0002  max mem: 6052
[03:13:28.922037] Epoch: [6]  [320/781]  eta: 0:01:20  lr: 0.000250  training_loss: 2.4544 (2.5146)  classification_loss: 2.0183 (2.0615)  loss_mask: 0.3823 (0.4531)  time: 0.1698  data: 0.0002  max mem: 6052
[03:13:32.329673] Epoch: [6]  [340/781]  eta: 0:01:16  lr: 0.000250  training_loss: 2.5294 (2.5164)  classification_loss: 2.0283 (2.0582)  loss_mask: 0.5198 (0.4582)  time: 0.1703  data: 0.0002  max mem: 6052
[03:13:35.737133] Epoch: [6]  [360/781]  eta: 0:01:13  lr: 0.000250  training_loss: 2.6958 (2.5276)  classification_loss: 2.0673 (2.0593)  loss_mask: 0.5413 (0.4683)  time: 0.1703  data: 0.0003  max mem: 6052
[03:13:39.155231] Epoch: [6]  [380/781]  eta: 0:01:09  lr: 0.000250  training_loss: 2.5194 (2.5268)  classification_loss: 2.0721 (2.0604)  loss_mask: 0.4194 (0.4663)  time: 0.1708  data: 0.0002  max mem: 6052
[03:13:42.563086] Epoch: [6]  [400/781]  eta: 0:01:06  lr: 0.000250  training_loss: 2.4738 (2.5243)  classification_loss: 2.0529 (2.0604)  loss_mask: 0.4267 (0.4638)  time: 0.1703  data: 0.0003  max mem: 6052
[03:13:45.974185] Epoch: [6]  [420/781]  eta: 0:01:02  lr: 0.000250  training_loss: 2.4696 (2.5205)  classification_loss: 2.0434 (2.0598)  loss_mask: 0.3766 (0.4607)  time: 0.1705  data: 0.0001  max mem: 6052
[03:13:49.390641] Epoch: [6]  [440/781]  eta: 0:00:59  lr: 0.000250  training_loss: 2.3380 (2.5133)  classification_loss: 2.0168 (2.0578)  loss_mask: 0.3445 (0.4556)  time: 0.1708  data: 0.0003  max mem: 6052
[03:13:52.823371] Epoch: [6]  [460/781]  eta: 0:00:55  lr: 0.000250  training_loss: 2.3970 (2.5086)  classification_loss: 2.0142 (2.0558)  loss_mask: 0.4025 (0.4528)  time: 0.1716  data: 0.0002  max mem: 6052
[03:13:56.309669] Epoch: [6]  [480/781]  eta: 0:00:52  lr: 0.000250  training_loss: 2.4711 (2.5068)  classification_loss: 2.0601 (2.0553)  loss_mask: 0.4373 (0.4515)  time: 0.1742  data: 0.0002  max mem: 6052
[03:13:59.744859] Epoch: [6]  [500/781]  eta: 0:00:48  lr: 0.000250  training_loss: 2.5372 (2.5076)  classification_loss: 2.0431 (2.0552)  loss_mask: 0.4364 (0.4524)  time: 0.1716  data: 0.0003  max mem: 6052
[03:14:03.312295] Epoch: [6]  [520/781]  eta: 0:00:45  lr: 0.000250  training_loss: 2.3780 (2.5041)  classification_loss: 1.9943 (2.0546)  loss_mask: 0.3571 (0.4495)  time: 0.1783  data: 0.0002  max mem: 6052
[03:14:06.823445] Epoch: [6]  [540/781]  eta: 0:00:41  lr: 0.000250  training_loss: 2.4304 (2.5016)  classification_loss: 2.0615 (2.0547)  loss_mask: 0.3570 (0.4469)  time: 0.1755  data: 0.0002  max mem: 6052
[03:14:10.237720] Epoch: [6]  [560/781]  eta: 0:00:38  lr: 0.000250  training_loss: 2.3590 (2.4972)  classification_loss: 2.0327 (2.0540)  loss_mask: 0.3374 (0.4433)  time: 0.1706  data: 0.0002  max mem: 6052
[03:14:13.663757] Epoch: [6]  [580/781]  eta: 0:00:34  lr: 0.000250  training_loss: 2.3768 (2.4936)  classification_loss: 2.0413 (2.0531)  loss_mask: 0.3476 (0.4405)  time: 0.1712  data: 0.0002  max mem: 6052
[03:14:17.084826] Epoch: [6]  [600/781]  eta: 0:00:31  lr: 0.000250  training_loss: 2.3876 (2.4901)  classification_loss: 2.0324 (2.0519)  loss_mask: 0.3309 (0.4382)  time: 0.1710  data: 0.0002  max mem: 6052
[03:14:20.569274] Epoch: [6]  [620/781]  eta: 0:00:27  lr: 0.000250  training_loss: 2.3945 (2.4869)  classification_loss: 2.0162 (2.0504)  loss_mask: 0.3753 (0.4364)  time: 0.1741  data: 0.0002  max mem: 6052
[03:14:23.983583] Epoch: [6]  [640/781]  eta: 0:00:24  lr: 0.000250  training_loss: 2.4181 (2.4858)  classification_loss: 2.0673 (2.0505)  loss_mask: 0.3912 (0.4353)  time: 0.1706  data: 0.0003  max mem: 6052
[03:14:27.394180] Epoch: [6]  [660/781]  eta: 0:00:20  lr: 0.000250  training_loss: 2.4410 (2.4838)  classification_loss: 2.0682 (2.0509)  loss_mask: 0.3425 (0.4329)  time: 0.1704  data: 0.0003  max mem: 6052
[03:14:30.813415] Epoch: [6]  [680/781]  eta: 0:00:17  lr: 0.000250  training_loss: 2.3780 (2.4813)  classification_loss: 2.0300 (2.0503)  loss_mask: 0.3712 (0.4310)  time: 0.1709  data: 0.0002  max mem: 6052
[03:14:34.238861] Epoch: [6]  [700/781]  eta: 0:00:13  lr: 0.000250  training_loss: 2.3434 (2.4777)  classification_loss: 2.0355 (2.0501)  loss_mask: 0.3012 (0.4276)  time: 0.1712  data: 0.0002  max mem: 6052
[03:14:37.664177] Epoch: [6]  [720/781]  eta: 0:00:10  lr: 0.000250  training_loss: 2.3794 (2.4755)  classification_loss: 2.0442 (2.0502)  loss_mask: 0.3371 (0.4253)  time: 0.1712  data: 0.0002  max mem: 6052
[03:14:41.131594] Epoch: [6]  [740/781]  eta: 0:00:07  lr: 0.000250  training_loss: 2.3103 (2.4708)  classification_loss: 2.0266 (2.0493)  loss_mask: 0.2741 (0.4216)  time: 0.1733  data: 0.0002  max mem: 6052
[03:14:44.537484] Epoch: [6]  [760/781]  eta: 0:00:03  lr: 0.000250  training_loss: 2.3777 (2.4699)  classification_loss: 2.0092 (2.0489)  loss_mask: 0.3642 (0.4210)  time: 0.1702  data: 0.0003  max mem: 6052
[03:14:48.016679] Epoch: [6]  [780/781]  eta: 0:00:00  lr: 0.000250  training_loss: 2.4000 (2.4695)  classification_loss: 2.0338 (2.0488)  loss_mask: 0.3865 (0.4207)  time: 0.1739  data: 0.0003  max mem: 6052
[03:14:48.163106] Epoch: [6] Total time: 0:02:15 (0.1730 s / it)
[03:14:48.163540] Averaged stats: lr: 0.000250  training_loss: 2.4000 (2.4695)  classification_loss: 2.0338 (2.0488)  loss_mask: 0.3865 (0.4207)
[03:14:48.818143] Test:  [  0/157]  eta: 0:01:41  testing_loss: 1.5943 (1.5943)  acc1: 46.8750 (46.8750)  acc5: 87.5000 (87.5000)  time: 0.6467  data: 0.6175  max mem: 6052
[03:14:49.107799] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.6500 (1.6648)  acc1: 40.6250 (43.1818)  acc5: 89.0625 (88.7784)  time: 0.0850  data: 0.0563  max mem: 6052
[03:14:49.393047] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.6323 (1.6433)  acc1: 43.7500 (44.9405)  acc5: 90.6250 (89.9554)  time: 0.0286  data: 0.0001  max mem: 6052
[03:14:49.676565] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.5985 (1.6272)  acc1: 46.8750 (46.1190)  acc5: 90.6250 (90.5242)  time: 0.0283  data: 0.0002  max mem: 6052
[03:14:49.963611] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.5892 (1.6381)  acc1: 46.8750 (45.9223)  acc5: 90.6250 (89.9009)  time: 0.0284  data: 0.0002  max mem: 6052
[03:14:50.244942] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.6233 (1.6363)  acc1: 46.8750 (45.9252)  acc5: 89.0625 (90.0429)  time: 0.0283  data: 0.0001  max mem: 6052
[03:14:50.529654] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.6115 (1.6295)  acc1: 46.8750 (46.6189)  acc5: 90.6250 (90.2920)  time: 0.0282  data: 0.0001  max mem: 6052
[03:14:50.809358] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.5903 (1.6293)  acc1: 48.4375 (46.7870)  acc5: 92.1875 (90.4710)  time: 0.0281  data: 0.0002  max mem: 6052
[03:14:51.089778] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.6148 (1.6284)  acc1: 46.8750 (46.8171)  acc5: 90.6250 (90.6443)  time: 0.0279  data: 0.0002  max mem: 6052
[03:14:51.369328] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.6435 (1.6297)  acc1: 46.8750 (46.8578)  acc5: 90.6250 (90.5391)  time: 0.0279  data: 0.0002  max mem: 6052
[03:14:51.649153] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.6652 (1.6340)  acc1: 46.8750 (46.5192)  acc5: 90.6250 (90.4858)  time: 0.0279  data: 0.0002  max mem: 6052
[03:14:51.928874] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.6700 (1.6377)  acc1: 42.1875 (46.2275)  acc5: 90.6250 (90.4561)  time: 0.0279  data: 0.0002  max mem: 6052
[03:14:52.211103] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.6522 (1.6340)  acc1: 43.7500 (46.3843)  acc5: 90.6250 (90.5863)  time: 0.0280  data: 0.0002  max mem: 6052
[03:14:52.498305] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.6089 (1.6370)  acc1: 45.3125 (46.1713)  acc5: 90.6250 (90.3984)  time: 0.0284  data: 0.0002  max mem: 6052
[03:14:52.780261] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.6490 (1.6369)  acc1: 45.3125 (46.2434)  acc5: 89.0625 (90.4255)  time: 0.0283  data: 0.0002  max mem: 6052
[03:14:53.060307] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.6120 (1.6356)  acc1: 46.8750 (46.3266)  acc5: 89.0625 (90.4077)  time: 0.0280  data: 0.0001  max mem: 6052
[03:14:53.215130] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.6075 (1.6357)  acc1: 46.8750 (46.3500)  acc5: 90.6250 (90.4500)  time: 0.0271  data: 0.0001  max mem: 6052
[03:14:53.389951] Test: Total time: 0:00:05 (0.0333 s / it)
[03:14:53.390855] * Acc@1 46.350 Acc@5 90.450 loss 1.636
[03:14:53.391170] Accuracy of the network on the 10000 test images: 46.4%
[03:14:53.391372] Max accuracy: 46.37%
[03:14:53.619976] log_dir: ./output_dir
[03:14:54.490070] Epoch: [7]  [  0/781]  eta: 0:11:18  lr: 0.000250  training_loss: 2.4006 (2.4006)  classification_loss: 2.0645 (2.0645)  loss_mask: 0.3361 (0.3361)  time: 0.8684  data: 0.6843  max mem: 6052
[03:14:57.923541] Epoch: [7]  [ 20/781]  eta: 0:02:35  lr: 0.000250  training_loss: 2.3266 (2.3601)  classification_loss: 1.9950 (2.0218)  loss_mask: 0.3324 (0.3383)  time: 0.1716  data: 0.0004  max mem: 6052
[03:15:01.345214] Epoch: [7]  [ 40/781]  eta: 0:02:19  lr: 0.000250  training_loss: 2.3146 (2.3392)  classification_loss: 2.0522 (2.0312)  loss_mask: 0.2969 (0.3080)  time: 0.1710  data: 0.0002  max mem: 6052
[03:15:04.751388] Epoch: [7]  [ 60/781]  eta: 0:02:11  lr: 0.000250  training_loss: 2.3641 (2.3546)  classification_loss: 2.0432 (2.0400)  loss_mask: 0.2897 (0.3146)  time: 0.1702  data: 0.0001  max mem: 6052
[03:15:08.167827] Epoch: [7]  [ 80/781]  eta: 0:02:05  lr: 0.000250  training_loss: 2.2729 (2.3399)  classification_loss: 1.9837 (2.0310)  loss_mask: 0.2818 (0.3089)  time: 0.1707  data: 0.0003  max mem: 6052
[03:15:11.592579] Epoch: [7]  [100/781]  eta: 0:02:01  lr: 0.000250  training_loss: 2.2577 (2.3288)  classification_loss: 1.9859 (2.0298)  loss_mask: 0.2548 (0.2989)  time: 0.1711  data: 0.0002  max mem: 6052
[03:15:15.038721] Epoch: [7]  [120/781]  eta: 0:01:56  lr: 0.000250  training_loss: 2.3624 (2.3312)  classification_loss: 1.9862 (2.0236)  loss_mask: 0.3317 (0.3076)  time: 0.1722  data: 0.0002  max mem: 6052
[03:15:18.519268] Epoch: [7]  [140/781]  eta: 0:01:53  lr: 0.000250  training_loss: 2.2569 (2.3306)  classification_loss: 1.9502 (2.0185)  loss_mask: 0.3175 (0.3122)  time: 0.1740  data: 0.0002  max mem: 6052
[03:15:21.947897] Epoch: [7]  [160/781]  eta: 0:01:49  lr: 0.000250  training_loss: 2.3562 (2.3357)  classification_loss: 2.0188 (2.0190)  loss_mask: 0.2991 (0.3166)  time: 0.1714  data: 0.0002  max mem: 6052
[03:15:25.350237] Epoch: [7]  [180/781]  eta: 0:01:45  lr: 0.000250  training_loss: 2.3330 (2.3375)  classification_loss: 2.0247 (2.0204)  loss_mask: 0.3100 (0.3172)  time: 0.1700  data: 0.0002  max mem: 6052
[03:15:28.747049] Epoch: [7]  [200/781]  eta: 0:01:41  lr: 0.000250  training_loss: 2.2883 (2.3334)  classification_loss: 2.0387 (2.0212)  loss_mask: 0.2849 (0.3122)  time: 0.1698  data: 0.0002  max mem: 6052
[03:15:32.141525] Epoch: [7]  [220/781]  eta: 0:01:37  lr: 0.000250  training_loss: 2.2419 (2.3264)  classification_loss: 1.9955 (2.0202)  loss_mask: 0.2475 (0.3062)  time: 0.1697  data: 0.0003  max mem: 6052
[03:15:35.535105] Epoch: [7]  [240/781]  eta: 0:01:34  lr: 0.000250  training_loss: 2.2773 (2.3216)  classification_loss: 2.0136 (2.0199)  loss_mask: 0.2343 (0.3017)  time: 0.1696  data: 0.0003  max mem: 6052
[03:15:38.965061] Epoch: [7]  [260/781]  eta: 0:01:30  lr: 0.000250  training_loss: 2.2332 (2.3215)  classification_loss: 2.0289 (2.0202)  loss_mask: 0.2660 (0.3013)  time: 0.1714  data: 0.0002  max mem: 6052
[03:15:42.358063] Epoch: [7]  [280/781]  eta: 0:01:26  lr: 0.000250  training_loss: 2.3981 (2.3273)  classification_loss: 2.0178 (2.0200)  loss_mask: 0.3920 (0.3073)  time: 0.1696  data: 0.0002  max mem: 6052
[03:15:45.746629] Epoch: [7]  [300/781]  eta: 0:01:23  lr: 0.000250  training_loss: 2.2773 (2.3263)  classification_loss: 2.0091 (2.0204)  loss_mask: 0.2782 (0.3060)  time: 0.1693  data: 0.0001  max mem: 6052
[03:15:49.148279] Epoch: [7]  [320/781]  eta: 0:01:19  lr: 0.000250  training_loss: 2.2481 (2.3213)  classification_loss: 2.0168 (2.0197)  loss_mask: 0.2435 (0.3017)  time: 0.1700  data: 0.0001  max mem: 6052
[03:15:52.565277] Epoch: [7]  [340/781]  eta: 0:01:16  lr: 0.000250  training_loss: 2.2924 (2.3203)  classification_loss: 1.9981 (2.0185)  loss_mask: 0.2842 (0.3018)  time: 0.1708  data: 0.0002  max mem: 6052
[03:15:55.993842] Epoch: [7]  [360/781]  eta: 0:01:12  lr: 0.000250  training_loss: 2.2942 (2.3196)  classification_loss: 1.9937 (2.0177)  loss_mask: 0.2790 (0.3019)  time: 0.1713  data: 0.0002  max mem: 6052
[03:15:59.430728] Epoch: [7]  [380/781]  eta: 0:01:09  lr: 0.000250  training_loss: 2.2513 (2.3167)  classification_loss: 2.0193 (2.0174)  loss_mask: 0.2388 (0.2993)  time: 0.1717  data: 0.0002  max mem: 6052
[03:16:02.848916] Epoch: [7]  [400/781]  eta: 0:01:05  lr: 0.000250  training_loss: 2.2242 (2.3149)  classification_loss: 1.9555 (2.0154)  loss_mask: 0.2513 (0.2995)  time: 0.1708  data: 0.0003  max mem: 6052
[03:16:06.275344] Epoch: [7]  [420/781]  eta: 0:01:02  lr: 0.000250  training_loss: 2.2271 (2.3122)  classification_loss: 1.9775 (2.0148)  loss_mask: 0.2218 (0.2974)  time: 0.1712  data: 0.0003  max mem: 6052
[03:16:09.705969] Epoch: [7]  [440/781]  eta: 0:00:58  lr: 0.000250  training_loss: 2.1935 (2.3085)  classification_loss: 1.9858 (2.0147)  loss_mask: 0.2199 (0.2938)  time: 0.1715  data: 0.0002  max mem: 6052
[03:16:13.117698] Epoch: [7]  [460/781]  eta: 0:00:55  lr: 0.000250  training_loss: 2.2332 (2.3059)  classification_loss: 1.9904 (2.0144)  loss_mask: 0.2397 (0.2914)  time: 0.1705  data: 0.0002  max mem: 6052
[03:16:16.534477] Epoch: [7]  [480/781]  eta: 0:00:51  lr: 0.000250  training_loss: 2.2199 (2.3034)  classification_loss: 2.0155 (2.0145)  loss_mask: 0.2176 (0.2889)  time: 0.1707  data: 0.0002  max mem: 6052
[03:16:19.956711] Epoch: [7]  [500/781]  eta: 0:00:48  lr: 0.000250  training_loss: 2.1900 (2.2999)  classification_loss: 1.9966 (2.0139)  loss_mask: 0.2207 (0.2860)  time: 0.1710  data: 0.0002  max mem: 6052
[03:16:23.376234] Epoch: [7]  [520/781]  eta: 0:00:44  lr: 0.000250  training_loss: 2.2341 (2.2975)  classification_loss: 1.9755 (2.0132)  loss_mask: 0.2255 (0.2842)  time: 0.1709  data: 0.0003  max mem: 6052
[03:16:26.771354] Epoch: [7]  [540/781]  eta: 0:00:41  lr: 0.000250  training_loss: 2.2090 (2.2941)  classification_loss: 2.0100 (2.0127)  loss_mask: 0.1771 (0.2813)  time: 0.1697  data: 0.0002  max mem: 6052
[03:16:30.186574] Epoch: [7]  [560/781]  eta: 0:00:38  lr: 0.000249  training_loss: 2.2121 (2.2913)  classification_loss: 2.0067 (2.0132)  loss_mask: 0.1848 (0.2781)  time: 0.1707  data: 0.0002  max mem: 6052
[03:16:33.591488] Epoch: [7]  [580/781]  eta: 0:00:34  lr: 0.000249  training_loss: 2.2252 (2.2903)  classification_loss: 1.9884 (2.0127)  loss_mask: 0.2480 (0.2776)  time: 0.1702  data: 0.0003  max mem: 6052
[03:16:37.010723] Epoch: [7]  [600/781]  eta: 0:00:31  lr: 0.000249  training_loss: 2.3069 (2.2920)  classification_loss: 1.9922 (2.0125)  loss_mask: 0.3103 (0.2795)  time: 0.1709  data: 0.0002  max mem: 6052
[03:16:40.467923] Epoch: [7]  [620/781]  eta: 0:00:27  lr: 0.000249  training_loss: 2.1817 (2.2893)  classification_loss: 1.9626 (2.0117)  loss_mask: 0.2115 (0.2776)  time: 0.1728  data: 0.0003  max mem: 6052
[03:16:43.878594] Epoch: [7]  [640/781]  eta: 0:00:24  lr: 0.000249  training_loss: 2.1696 (2.2858)  classification_loss: 2.0003 (2.0114)  loss_mask: 0.1731 (0.2744)  time: 0.1705  data: 0.0002  max mem: 6052
[03:16:47.317906] Epoch: [7]  [660/781]  eta: 0:00:20  lr: 0.000249  training_loss: 2.1594 (2.2829)  classification_loss: 2.0016 (2.0113)  loss_mask: 0.1582 (0.2716)  time: 0.1719  data: 0.0003  max mem: 6052
[03:16:50.744808] Epoch: [7]  [680/781]  eta: 0:00:17  lr: 0.000249  training_loss: 2.2486 (2.2820)  classification_loss: 2.0041 (2.0114)  loss_mask: 0.2410 (0.2707)  time: 0.1713  data: 0.0003  max mem: 6052
[03:16:54.160684] Epoch: [7]  [700/781]  eta: 0:00:13  lr: 0.000249  training_loss: 2.1676 (2.2788)  classification_loss: 1.9753 (2.0110)  loss_mask: 0.1721 (0.2678)  time: 0.1707  data: 0.0003  max mem: 6052
[03:16:57.564299] Epoch: [7]  [720/781]  eta: 0:00:10  lr: 0.000249  training_loss: 2.1979 (2.2763)  classification_loss: 2.0203 (2.0113)  loss_mask: 0.1685 (0.2650)  time: 0.1701  data: 0.0002  max mem: 6052
[03:17:00.964867] Epoch: [7]  [740/781]  eta: 0:00:07  lr: 0.000249  training_loss: 2.1914 (2.2738)  classification_loss: 1.9989 (2.0108)  loss_mask: 0.1901 (0.2631)  time: 0.1700  data: 0.0001  max mem: 6052
[03:17:04.387433] Epoch: [7]  [760/781]  eta: 0:00:03  lr: 0.000249  training_loss: 2.2155 (2.2723)  classification_loss: 2.0336 (2.0109)  loss_mask: 0.1642 (0.2614)  time: 0.1711  data: 0.0002  max mem: 6052
[03:17:07.787664] Epoch: [7]  [780/781]  eta: 0:00:00  lr: 0.000249  training_loss: 2.1840 (2.2709)  classification_loss: 1.9676 (2.0102)  loss_mask: 0.2174 (0.2607)  time: 0.1699  data: 0.0002  max mem: 6052
[03:17:07.950832] Epoch: [7] Total time: 0:02:14 (0.1720 s / it)
[03:17:07.952281] Averaged stats: lr: 0.000249  training_loss: 2.1840 (2.2709)  classification_loss: 1.9676 (2.0102)  loss_mask: 0.2174 (0.2607)
[03:17:08.644543] Test:  [  0/157]  eta: 0:01:47  testing_loss: 1.4465 (1.4465)  acc1: 57.8125 (57.8125)  acc5: 90.6250 (90.6250)  time: 0.6858  data: 0.6556  max mem: 6052
[03:17:08.942429] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.5243 (1.5511)  acc1: 48.4375 (47.3011)  acc5: 92.1875 (92.1875)  time: 0.0892  data: 0.0598  max mem: 6052
[03:17:09.228114] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.5021 (1.5179)  acc1: 50.0000 (49.9256)  acc5: 92.1875 (92.8571)  time: 0.0290  data: 0.0002  max mem: 6052
[03:17:09.513877] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.4858 (1.5126)  acc1: 53.1250 (51.4113)  acc5: 92.1875 (92.7419)  time: 0.0285  data: 0.0002  max mem: 6052
[03:17:09.795375] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.5141 (1.5231)  acc1: 50.0000 (50.0000)  acc5: 92.1875 (91.8064)  time: 0.0282  data: 0.0002  max mem: 6052
[03:17:10.079322] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.5099 (1.5216)  acc1: 50.0000 (50.6127)  acc5: 92.1875 (92.0343)  time: 0.0281  data: 0.0002  max mem: 6052
[03:17:10.362331] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.4751 (1.5150)  acc1: 51.5625 (50.6148)  acc5: 92.1875 (92.2387)  time: 0.0282  data: 0.0002  max mem: 6052
[03:17:10.645086] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.4733 (1.5117)  acc1: 51.5625 (50.5502)  acc5: 93.7500 (92.6276)  time: 0.0282  data: 0.0002  max mem: 6052
[03:17:10.929182] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.4733 (1.5112)  acc1: 51.5625 (50.7330)  acc5: 95.3125 (92.6119)  time: 0.0282  data: 0.0001  max mem: 6052
[03:17:11.214190] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.5253 (1.5148)  acc1: 53.1250 (50.7898)  acc5: 92.1875 (92.5824)  time: 0.0283  data: 0.0002  max mem: 6052
[03:17:11.495679] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.5359 (1.5190)  acc1: 50.0000 (50.4486)  acc5: 92.1875 (92.4969)  time: 0.0282  data: 0.0002  max mem: 6052
[03:17:11.778437] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.5675 (1.5237)  acc1: 45.3125 (50.0422)  acc5: 92.1875 (92.5535)  time: 0.0281  data: 0.0001  max mem: 6052
[03:17:12.065445] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.5245 (1.5202)  acc1: 48.4375 (50.2454)  acc5: 93.7500 (92.6782)  time: 0.0283  data: 0.0002  max mem: 6052
[03:17:12.348429] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.4902 (1.5220)  acc1: 51.5625 (50.0477)  acc5: 92.1875 (92.6288)  time: 0.0284  data: 0.0002  max mem: 6052
[03:17:12.631173] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.5288 (1.5234)  acc1: 50.0000 (50.0443)  acc5: 92.1875 (92.5754)  time: 0.0282  data: 0.0002  max mem: 6052
[03:17:12.914283] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.5288 (1.5211)  acc1: 51.5625 (50.2587)  acc5: 92.1875 (92.5600)  time: 0.0281  data: 0.0001  max mem: 6052
[03:17:13.067314] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.5077 (1.5215)  acc1: 51.5625 (50.2600)  acc5: 92.1875 (92.5700)  time: 0.0272  data: 0.0001  max mem: 6052
[03:17:13.228049] Test: Total time: 0:00:05 (0.0336 s / it)
[03:17:13.228514] * Acc@1 50.260 Acc@5 92.570 loss 1.521
[03:17:13.229198] Accuracy of the network on the 10000 test images: 50.3%
[03:17:13.229378] Max accuracy: 50.26%
[03:17:13.509857] log_dir: ./output_dir
[03:17:14.387957] Epoch: [8]  [  0/781]  eta: 0:11:24  lr: 0.000249  training_loss: 2.2695 (2.2695)  classification_loss: 1.8741 (1.8741)  loss_mask: 0.3954 (0.3954)  time: 0.8764  data: 0.6906  max mem: 6052
[03:17:17.816962] Epoch: [8]  [ 20/781]  eta: 0:02:35  lr: 0.000249  training_loss: 2.1899 (2.1998)  classification_loss: 1.9880 (1.9834)  loss_mask: 0.1735 (0.2164)  time: 0.1714  data: 0.0002  max mem: 6052
[03:17:21.255558] Epoch: [8]  [ 40/781]  eta: 0:02:19  lr: 0.000249  training_loss: 2.1908 (2.1970)  classification_loss: 2.0098 (1.9921)  loss_mask: 0.1659 (0.2049)  time: 0.1718  data: 0.0002  max mem: 6052
[03:17:24.696361] Epoch: [8]  [ 60/781]  eta: 0:02:12  lr: 0.000249  training_loss: 2.1174 (2.1711)  classification_loss: 1.9461 (1.9784)  loss_mask: 0.1584 (0.1926)  time: 0.1720  data: 0.0002  max mem: 6052
[03:17:28.121009] Epoch: [8]  [ 80/781]  eta: 0:02:06  lr: 0.000249  training_loss: 2.1354 (2.1724)  classification_loss: 1.9996 (1.9894)  loss_mask: 0.1338 (0.1829)  time: 0.1711  data: 0.0003  max mem: 6052
[03:17:31.539425] Epoch: [8]  [100/781]  eta: 0:02:01  lr: 0.000249  training_loss: 2.1790 (2.1725)  classification_loss: 2.0248 (1.9958)  loss_mask: 0.1381 (0.1767)  time: 0.1708  data: 0.0002  max mem: 6052
[03:17:34.968005] Epoch: [8]  [120/781]  eta: 0:01:57  lr: 0.000249  training_loss: 2.1204 (2.1622)  classification_loss: 1.9140 (1.9879)  loss_mask: 0.1371 (0.1743)  time: 0.1714  data: 0.0002  max mem: 6052
[03:17:38.413618] Epoch: [8]  [140/781]  eta: 0:01:53  lr: 0.000249  training_loss: 2.1584 (2.1626)  classification_loss: 1.9383 (1.9794)  loss_mask: 0.2200 (0.1832)  time: 0.1721  data: 0.0002  max mem: 6052
[03:17:41.813944] Epoch: [8]  [160/781]  eta: 0:01:49  lr: 0.000249  training_loss: 2.1383 (2.1596)  classification_loss: 1.9783 (1.9794)  loss_mask: 0.1445 (0.1802)  time: 0.1699  data: 0.0002  max mem: 6052
[03:17:45.223265] Epoch: [8]  [180/781]  eta: 0:01:45  lr: 0.000249  training_loss: 2.0853 (2.1538)  classification_loss: 1.9255 (1.9762)  loss_mask: 0.1417 (0.1776)  time: 0.1704  data: 0.0002  max mem: 6052
[03:17:48.653401] Epoch: [8]  [200/781]  eta: 0:01:41  lr: 0.000249  training_loss: 2.0832 (2.1495)  classification_loss: 1.9670 (1.9750)  loss_mask: 0.1357 (0.1745)  time: 0.1714  data: 0.0002  max mem: 6052
[03:17:52.136063] Epoch: [8]  [220/781]  eta: 0:01:37  lr: 0.000249  training_loss: 2.1357 (2.1487)  classification_loss: 1.9255 (1.9724)  loss_mask: 0.1647 (0.1762)  time: 0.1741  data: 0.0003  max mem: 6052
[03:17:55.564554] Epoch: [8]  [240/781]  eta: 0:01:34  lr: 0.000249  training_loss: 2.1479 (2.1478)  classification_loss: 1.9776 (1.9739)  loss_mask: 0.1425 (0.1739)  time: 0.1713  data: 0.0002  max mem: 6052
[03:17:59.070873] Epoch: [8]  [260/781]  eta: 0:01:30  lr: 0.000249  training_loss: 2.1400 (2.1490)  classification_loss: 1.9768 (1.9748)  loss_mask: 0.1650 (0.1742)  time: 0.1752  data: 0.0002  max mem: 6052
[03:18:02.542432] Epoch: [8]  [280/781]  eta: 0:01:27  lr: 0.000249  training_loss: 2.1330 (2.1483)  classification_loss: 1.9631 (1.9746)  loss_mask: 0.1644 (0.1737)  time: 0.1735  data: 0.0003  max mem: 6052
[03:18:05.999503] Epoch: [8]  [300/781]  eta: 0:01:23  lr: 0.000249  training_loss: 2.1433 (2.1485)  classification_loss: 1.9930 (1.9761)  loss_mask: 0.1372 (0.1724)  time: 0.1728  data: 0.0002  max mem: 6052
[03:18:09.420618] Epoch: [8]  [320/781]  eta: 0:01:20  lr: 0.000249  training_loss: 2.1046 (2.1464)  classification_loss: 1.9563 (1.9757)  loss_mask: 0.1309 (0.1707)  time: 0.1710  data: 0.0002  max mem: 6052
[03:18:12.836574] Epoch: [8]  [340/781]  eta: 0:01:16  lr: 0.000249  training_loss: 2.1124 (2.1451)  classification_loss: 1.9873 (1.9764)  loss_mask: 0.1203 (0.1688)  time: 0.1707  data: 0.0002  max mem: 6052
[03:18:16.263457] Epoch: [8]  [360/781]  eta: 0:01:13  lr: 0.000249  training_loss: 2.0917 (2.1430)  classification_loss: 1.9706 (1.9768)  loss_mask: 0.1187 (0.1663)  time: 0.1713  data: 0.0002  max mem: 6052
[03:18:19.704642] Epoch: [8]  [380/781]  eta: 0:01:09  lr: 0.000249  training_loss: 2.1184 (2.1432)  classification_loss: 1.9646 (1.9769)  loss_mask: 0.1534 (0.1663)  time: 0.1720  data: 0.0002  max mem: 6052
[03:18:23.128533] Epoch: [8]  [400/781]  eta: 0:01:06  lr: 0.000249  training_loss: 2.1036 (2.1424)  classification_loss: 1.9688 (1.9780)  loss_mask: 0.1186 (0.1644)  time: 0.1711  data: 0.0003  max mem: 6052
[03:18:26.556985] Epoch: [8]  [420/781]  eta: 0:01:02  lr: 0.000249  training_loss: 2.0855 (2.1397)  classification_loss: 1.9619 (1.9774)  loss_mask: 0.1277 (0.1624)  time: 0.1713  data: 0.0002  max mem: 6052
[03:18:29.963348] Epoch: [8]  [440/781]  eta: 0:00:59  lr: 0.000249  training_loss: 2.0979 (2.1382)  classification_loss: 1.9418 (1.9758)  loss_mask: 0.1457 (0.1624)  time: 0.1702  data: 0.0003  max mem: 6052
[03:18:33.385928] Epoch: [8]  [460/781]  eta: 0:00:55  lr: 0.000249  training_loss: 2.1328 (2.1382)  classification_loss: 1.9470 (1.9757)  loss_mask: 0.1350 (0.1625)  time: 0.1711  data: 0.0002  max mem: 6052
[03:18:36.804376] Epoch: [8]  [480/781]  eta: 0:00:52  lr: 0.000249  training_loss: 2.1742 (2.1401)  classification_loss: 1.9491 (1.9749)  loss_mask: 0.1936 (0.1652)  time: 0.1709  data: 0.0002  max mem: 6052
[03:18:40.234363] Epoch: [8]  [500/781]  eta: 0:00:48  lr: 0.000249  training_loss: 2.1243 (2.1400)  classification_loss: 1.9393 (1.9740)  loss_mask: 0.1818 (0.1660)  time: 0.1714  data: 0.0002  max mem: 6052
[03:18:43.643303] Epoch: [8]  [520/781]  eta: 0:00:45  lr: 0.000249  training_loss: 2.1002 (2.1385)  classification_loss: 1.9399 (1.9729)  loss_mask: 0.1377 (0.1656)  time: 0.1704  data: 0.0003  max mem: 6052
[03:18:47.091299] Epoch: [8]  [540/781]  eta: 0:00:41  lr: 0.000249  training_loss: 2.1301 (2.1385)  classification_loss: 1.9090 (1.9717)  loss_mask: 0.1499 (0.1669)  time: 0.1723  data: 0.0002  max mem: 6052
[03:18:50.525157] Epoch: [8]  [560/781]  eta: 0:00:38  lr: 0.000249  training_loss: 2.0338 (2.1356)  classification_loss: 1.9266 (1.9699)  loss_mask: 0.1142 (0.1656)  time: 0.1716  data: 0.0002  max mem: 6052
[03:18:53.956287] Epoch: [8]  [580/781]  eta: 0:00:34  lr: 0.000249  training_loss: 2.1015 (2.1334)  classification_loss: 1.9298 (1.9690)  loss_mask: 0.1117 (0.1644)  time: 0.1715  data: 0.0002  max mem: 6052
[03:18:57.420693] Epoch: [8]  [600/781]  eta: 0:00:31  lr: 0.000249  training_loss: 2.0887 (2.1323)  classification_loss: 1.9672 (1.9690)  loss_mask: 0.1197 (0.1633)  time: 0.1731  data: 0.0004  max mem: 6052
[03:19:00.844874] Epoch: [8]  [620/781]  eta: 0:00:27  lr: 0.000249  training_loss: 2.1082 (2.1322)  classification_loss: 1.9307 (1.9681)  loss_mask: 0.1441 (0.1640)  time: 0.1711  data: 0.0002  max mem: 6052
[03:19:04.260814] Epoch: [8]  [640/781]  eta: 0:00:24  lr: 0.000249  training_loss: 2.0582 (2.1298)  classification_loss: 1.9403 (1.9672)  loss_mask: 0.1084 (0.1626)  time: 0.1707  data: 0.0002  max mem: 6052
[03:19:07.678963] Epoch: [8]  [660/781]  eta: 0:00:20  lr: 0.000249  training_loss: 2.0615 (2.1280)  classification_loss: 1.9292 (1.9668)  loss_mask: 0.1049 (0.1612)  time: 0.1708  data: 0.0003  max mem: 6052
[03:19:11.086893] Epoch: [8]  [680/781]  eta: 0:00:17  lr: 0.000249  training_loss: 2.0433 (2.1259)  classification_loss: 1.9168 (1.9656)  loss_mask: 0.1249 (0.1603)  time: 0.1703  data: 0.0003  max mem: 6052
[03:19:14.487972] Epoch: [8]  [700/781]  eta: 0:00:13  lr: 0.000249  training_loss: 2.0451 (2.1242)  classification_loss: 1.9518 (1.9652)  loss_mask: 0.0931 (0.1590)  time: 0.1700  data: 0.0002  max mem: 6052
[03:19:17.886667] Epoch: [8]  [720/781]  eta: 0:00:10  lr: 0.000249  training_loss: 2.1470 (2.1247)  classification_loss: 1.9581 (1.9654)  loss_mask: 0.1580 (0.1592)  time: 0.1699  data: 0.0002  max mem: 6052
[03:19:21.333179] Epoch: [8]  [740/781]  eta: 0:00:07  lr: 0.000249  training_loss: 2.1208 (2.1245)  classification_loss: 1.9351 (1.9645)  loss_mask: 0.1665 (0.1600)  time: 0.1723  data: 0.0002  max mem: 6052
[03:19:24.734170] Epoch: [8]  [760/781]  eta: 0:00:03  lr: 0.000249  training_loss: 2.1069 (2.1243)  classification_loss: 1.9361 (1.9643)  loss_mask: 0.1510 (0.1600)  time: 0.1700  data: 0.0002  max mem: 6052
[03:19:28.157009] Epoch: [8]  [780/781]  eta: 0:00:00  lr: 0.000249  training_loss: 2.0311 (2.1224)  classification_loss: 1.9196 (1.9633)  loss_mask: 0.1015 (0.1591)  time: 0.1711  data: 0.0002  max mem: 6052
[03:19:28.322775] Epoch: [8] Total time: 0:02:14 (0.1726 s / it)
[03:19:28.323490] Averaged stats: lr: 0.000249  training_loss: 2.0311 (2.1224)  classification_loss: 1.9196 (1.9633)  loss_mask: 0.1015 (0.1591)
[03:19:29.000087] Test:  [  0/157]  eta: 0:01:45  testing_loss: 1.3247 (1.3247)  acc1: 56.2500 (56.2500)  acc5: 93.7500 (93.7500)  time: 0.6715  data: 0.6407  max mem: 6052
[03:19:29.285883] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.3913 (1.4197)  acc1: 54.6875 (52.4148)  acc5: 93.7500 (93.0398)  time: 0.0868  data: 0.0584  max mem: 6052
[03:19:29.573272] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.3916 (1.4022)  acc1: 54.6875 (53.4970)  acc5: 93.7500 (93.1548)  time: 0.0285  data: 0.0002  max mem: 6052
[03:19:29.858157] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.4000 (1.4040)  acc1: 53.1250 (53.5282)  acc5: 93.7500 (93.2964)  time: 0.0285  data: 0.0002  max mem: 6052
[03:19:30.141630] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.4044 (1.4163)  acc1: 51.5625 (52.8201)  acc5: 93.7500 (92.7591)  time: 0.0283  data: 0.0002  max mem: 6052
[03:19:30.429605] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.3881 (1.4112)  acc1: 51.5625 (53.0331)  acc5: 92.1875 (92.9534)  time: 0.0284  data: 0.0003  max mem: 6052
[03:19:30.716060] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.3861 (1.4033)  acc1: 54.6875 (53.1506)  acc5: 93.7500 (93.1096)  time: 0.0286  data: 0.0004  max mem: 6052
[03:19:30.999138] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.3678 (1.4004)  acc1: 54.6875 (53.3451)  acc5: 93.7500 (93.2879)  time: 0.0283  data: 0.0002  max mem: 6052
[03:19:31.283075] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.3678 (1.4000)  acc1: 53.1250 (53.2793)  acc5: 93.7500 (93.2677)  time: 0.0282  data: 0.0001  max mem: 6052
[03:19:31.565655] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.3953 (1.4027)  acc1: 53.1250 (53.3139)  acc5: 93.7500 (93.2521)  time: 0.0282  data: 0.0001  max mem: 6052
[03:19:31.848065] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.4107 (1.4070)  acc1: 53.1250 (53.0322)  acc5: 93.7500 (93.1776)  time: 0.0281  data: 0.0001  max mem: 6052
[03:19:32.131994] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.4599 (1.4114)  acc1: 50.0000 (52.7168)  acc5: 93.7500 (93.3136)  time: 0.0282  data: 0.0002  max mem: 6052
[03:19:32.417000] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.4117 (1.4077)  acc1: 51.5625 (52.8151)  acc5: 95.3125 (93.4401)  time: 0.0283  data: 0.0002  max mem: 6052
[03:19:32.709230] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.3806 (1.4101)  acc1: 53.1250 (52.6956)  acc5: 93.7500 (93.3802)  time: 0.0287  data: 0.0005  max mem: 6052
[03:19:32.990911] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.4014 (1.4104)  acc1: 51.5625 (52.8923)  acc5: 92.1875 (93.2957)  time: 0.0285  data: 0.0005  max mem: 6052
[03:19:33.269246] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.3980 (1.4082)  acc1: 54.6875 (53.0526)  acc5: 92.1875 (93.2223)  time: 0.0279  data: 0.0001  max mem: 6052
[03:19:33.418803] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.4256 (1.4093)  acc1: 54.6875 (53.0000)  acc5: 92.1875 (93.1900)  time: 0.0268  data: 0.0001  max mem: 6052
[03:19:33.587761] Test: Total time: 0:00:05 (0.0335 s / it)
[03:19:33.588208] * Acc@1 53.000 Acc@5 93.190 loss 1.409
[03:19:33.588502] Accuracy of the network on the 10000 test images: 53.0%
[03:19:33.588745] Max accuracy: 53.00%
[03:19:33.785874] log_dir: ./output_dir
[03:19:34.642135] Epoch: [9]  [  0/781]  eta: 0:11:07  lr: 0.000249  training_loss: 2.0655 (2.0655)  classification_loss: 1.9453 (1.9453)  loss_mask: 0.1202 (0.1202)  time: 0.8547  data: 0.6726  max mem: 6052
[03:19:38.069980] Epoch: [9]  [ 20/781]  eta: 0:02:35  lr: 0.000249  training_loss: 2.0361 (2.0647)  classification_loss: 1.9272 (1.9560)  loss_mask: 0.1046 (0.1087)  time: 0.1713  data: 0.0003  max mem: 6052
[03:19:41.512406] Epoch: [9]  [ 40/781]  eta: 0:02:19  lr: 0.000249  training_loss: 2.0157 (2.0469)  classification_loss: 1.9146 (1.9380)  loss_mask: 0.1047 (0.1089)  time: 0.1720  data: 0.0003  max mem: 6052
[03:19:44.944237] Epoch: [9]  [ 60/781]  eta: 0:02:11  lr: 0.000249  training_loss: 2.1128 (2.0715)  classification_loss: 1.9583 (1.9465)  loss_mask: 0.1406 (0.1250)  time: 0.1715  data: 0.0002  max mem: 6052
[03:19:48.383735] Epoch: [9]  [ 80/781]  eta: 0:02:06  lr: 0.000249  training_loss: 2.0331 (2.0696)  classification_loss: 1.9554 (1.9468)  loss_mask: 0.0999 (0.1229)  time: 0.1719  data: 0.0003  max mem: 6052
[03:19:51.795551] Epoch: [9]  [100/781]  eta: 0:02:01  lr: 0.000249  training_loss: 2.0705 (2.0761)  classification_loss: 1.9061 (1.9420)  loss_mask: 0.1759 (0.1341)  time: 0.1705  data: 0.0003  max mem: 6052
[03:19:55.201255] Epoch: [9]  [120/781]  eta: 0:01:56  lr: 0.000249  training_loss: 2.0567 (2.0750)  classification_loss: 1.9080 (1.9385)  loss_mask: 0.1509 (0.1364)  time: 0.1702  data: 0.0002  max mem: 6052
[03:19:58.619830] Epoch: [9]  [140/781]  eta: 0:01:52  lr: 0.000249  training_loss: 2.0117 (2.0667)  classification_loss: 1.8980 (1.9346)  loss_mask: 0.0912 (0.1321)  time: 0.1709  data: 0.0002  max mem: 6052
[03:20:02.031300] Epoch: [9]  [160/781]  eta: 0:01:48  lr: 0.000249  training_loss: 2.0228 (2.0633)  classification_loss: 1.9004 (1.9323)  loss_mask: 0.1098 (0.1310)  time: 0.1705  data: 0.0002  max mem: 6052
[03:20:05.460632] Epoch: [9]  [180/781]  eta: 0:01:45  lr: 0.000249  training_loss: 2.0203 (2.0587)  classification_loss: 1.9170 (1.9312)  loss_mask: 0.0971 (0.1275)  time: 0.1714  data: 0.0002  max mem: 6052
[03:20:08.880502] Epoch: [9]  [200/781]  eta: 0:01:41  lr: 0.000249  training_loss: 2.0665 (2.0634)  classification_loss: 1.9186 (1.9323)  loss_mask: 0.1272 (0.1311)  time: 0.1709  data: 0.0002  max mem: 6052
[03:20:12.325466] Epoch: [9]  [220/781]  eta: 0:01:37  lr: 0.000249  training_loss: 2.0086 (2.0594)  classification_loss: 1.9430 (1.9318)  loss_mask: 0.0951 (0.1276)  time: 0.1722  data: 0.0002  max mem: 6052
[03:20:15.733142] Epoch: [9]  [240/781]  eta: 0:01:34  lr: 0.000249  training_loss: 2.0667 (2.0605)  classification_loss: 1.9123 (1.9307)  loss_mask: 0.1435 (0.1298)  time: 0.1703  data: 0.0002  max mem: 6052
[03:20:19.150915] Epoch: [9]  [260/781]  eta: 0:01:30  lr: 0.000249  training_loss: 2.0874 (2.0608)  classification_loss: 1.9141 (1.9292)  loss_mask: 0.1414 (0.1317)  time: 0.1708  data: 0.0002  max mem: 6052
[03:20:22.568502] Epoch: [9]  [280/781]  eta: 0:01:26  lr: 0.000249  training_loss: 2.1265 (2.0673)  classification_loss: 1.9762 (1.9319)  loss_mask: 0.1561 (0.1354)  time: 0.1708  data: 0.0002  max mem: 6052
[03:20:25.985750] Epoch: [9]  [300/781]  eta: 0:01:23  lr: 0.000249  training_loss: 2.0066 (2.0640)  classification_loss: 1.9020 (1.9306)  loss_mask: 0.0992 (0.1334)  time: 0.1708  data: 0.0002  max mem: 6052
[03:20:29.401415] Epoch: [9]  [320/781]  eta: 0:01:19  lr: 0.000249  training_loss: 2.0253 (2.0642)  classification_loss: 1.9135 (1.9300)  loss_mask: 0.1333 (0.1342)  time: 0.1707  data: 0.0003  max mem: 6052
[03:20:32.835238] Epoch: [9]  [340/781]  eta: 0:01:16  lr: 0.000249  training_loss: 2.0770 (2.0668)  classification_loss: 1.8923 (1.9295)  loss_mask: 0.1621 (0.1373)  time: 0.1716  data: 0.0002  max mem: 6052
[03:20:36.259094] Epoch: [9]  [360/781]  eta: 0:01:12  lr: 0.000249  training_loss: 2.0389 (2.0664)  classification_loss: 1.9242 (1.9294)  loss_mask: 0.1198 (0.1370)  time: 0.1711  data: 0.0002  max mem: 6052
[03:20:39.679892] Epoch: [9]  [380/781]  eta: 0:01:09  lr: 0.000249  training_loss: 2.0340 (2.0652)  classification_loss: 1.8929 (1.9287)  loss_mask: 0.1247 (0.1365)  time: 0.1710  data: 0.0002  max mem: 6052
[03:20:43.093525] Epoch: [9]  [400/781]  eta: 0:01:05  lr: 0.000249  training_loss: 2.0874 (2.0655)  classification_loss: 1.8806 (1.9277)  loss_mask: 0.1578 (0.1378)  time: 0.1706  data: 0.0002  max mem: 6052
[03:20:46.512897] Epoch: [9]  [420/781]  eta: 0:01:02  lr: 0.000249  training_loss: 1.9914 (2.0628)  classification_loss: 1.8722 (1.9255)  loss_mask: 0.1228 (0.1373)  time: 0.1709  data: 0.0002  max mem: 6052
[03:20:49.972677] Epoch: [9]  [440/781]  eta: 0:00:58  lr: 0.000249  training_loss: 1.9748 (2.0600)  classification_loss: 1.8815 (1.9237)  loss_mask: 0.0900 (0.1364)  time: 0.1729  data: 0.0002  max mem: 6052
[03:20:53.389908] Epoch: [9]  [460/781]  eta: 0:00:55  lr: 0.000249  training_loss: 2.0119 (2.0595)  classification_loss: 1.9209 (1.9233)  loss_mask: 0.1242 (0.1362)  time: 0.1708  data: 0.0002  max mem: 6052
[03:20:56.820765] Epoch: [9]  [480/781]  eta: 0:00:51  lr: 0.000249  training_loss: 2.0875 (2.0606)  classification_loss: 1.8989 (1.9233)  loss_mask: 0.1408 (0.1374)  time: 0.1714  data: 0.0002  max mem: 6052
[03:21:00.278352] Epoch: [9]  [500/781]  eta: 0:00:48  lr: 0.000249  training_loss: 2.0158 (2.0608)  classification_loss: 1.9228 (1.9234)  loss_mask: 0.1147 (0.1374)  time: 0.1728  data: 0.0002  max mem: 6052
[03:21:03.740770] Epoch: [9]  [520/781]  eta: 0:00:45  lr: 0.000249  training_loss: 2.0144 (2.0597)  classification_loss: 1.9068 (1.9234)  loss_mask: 0.0967 (0.1362)  time: 0.1731  data: 0.0002  max mem: 6052
[03:21:07.177920] Epoch: [9]  [540/781]  eta: 0:00:41  lr: 0.000249  training_loss: 2.0533 (2.0587)  classification_loss: 1.9451 (1.9239)  loss_mask: 0.0959 (0.1349)  time: 0.1718  data: 0.0002  max mem: 6052
[03:21:10.615458] Epoch: [9]  [560/781]  eta: 0:00:38  lr: 0.000248  training_loss: 1.9835 (2.0566)  classification_loss: 1.8682 (1.9230)  loss_mask: 0.0870 (0.1337)  time: 0.1718  data: 0.0002  max mem: 6052
[03:21:14.048538] Epoch: [9]  [580/781]  eta: 0:00:34  lr: 0.000248  training_loss: 2.0339 (2.0561)  classification_loss: 1.9031 (1.9227)  loss_mask: 0.1164 (0.1334)  time: 0.1716  data: 0.0003  max mem: 6052
[03:21:17.475263] Epoch: [9]  [600/781]  eta: 0:00:31  lr: 0.000248  training_loss: 1.9908 (2.0537)  classification_loss: 1.8927 (1.9211)  loss_mask: 0.0928 (0.1326)  time: 0.1713  data: 0.0003  max mem: 6052
[03:21:20.878864] Epoch: [9]  [620/781]  eta: 0:00:27  lr: 0.000248  training_loss: 1.9729 (2.0521)  classification_loss: 1.8696 (1.9195)  loss_mask: 0.1202 (0.1325)  time: 0.1701  data: 0.0002  max mem: 6052
[03:21:24.293105] Epoch: [9]  [640/781]  eta: 0:00:24  lr: 0.000248  training_loss: 1.9673 (2.0497)  classification_loss: 1.8853 (1.9186)  loss_mask: 0.0779 (0.1311)  time: 0.1706  data: 0.0002  max mem: 6052
[03:21:27.739000] Epoch: [9]  [660/781]  eta: 0:00:20  lr: 0.000248  training_loss: 2.0896 (2.0508)  classification_loss: 1.9026 (1.9181)  loss_mask: 0.1359 (0.1327)  time: 0.1722  data: 0.0003  max mem: 6052
[03:21:31.155953] Epoch: [9]  [680/781]  eta: 0:00:17  lr: 0.000248  training_loss: 2.0343 (2.0510)  classification_loss: 1.8789 (1.9173)  loss_mask: 0.1553 (0.1336)  time: 0.1708  data: 0.0003  max mem: 6052
[03:21:34.579441] Epoch: [9]  [700/781]  eta: 0:00:13  lr: 0.000248  training_loss: 1.9972 (2.0508)  classification_loss: 1.8945 (1.9167)  loss_mask: 0.1333 (0.1341)  time: 0.1711  data: 0.0002  max mem: 6052
[03:21:38.041650] Epoch: [9]  [720/781]  eta: 0:00:10  lr: 0.000248  training_loss: 2.0432 (2.0501)  classification_loss: 1.9046 (1.9166)  loss_mask: 0.1061 (0.1336)  time: 0.1730  data: 0.0002  max mem: 6052
[03:21:41.437317] Epoch: [9]  [740/781]  eta: 0:00:07  lr: 0.000248  training_loss: 2.0360 (2.0495)  classification_loss: 1.8897 (1.9161)  loss_mask: 0.1200 (0.1333)  time: 0.1697  data: 0.0003  max mem: 6052
[03:21:44.839658] Epoch: [9]  [760/781]  eta: 0:00:03  lr: 0.000248  training_loss: 2.0652 (2.0501)  classification_loss: 1.9302 (1.9166)  loss_mask: 0.1110 (0.1335)  time: 0.1700  data: 0.0003  max mem: 6052
[03:21:48.254803] Epoch: [9]  [780/781]  eta: 0:00:00  lr: 0.000248  training_loss: 2.0451 (2.0518)  classification_loss: 1.8894 (1.9166)  loss_mask: 0.1343 (0.1352)  time: 0.1706  data: 0.0002  max mem: 6052
[03:21:48.424666] Epoch: [9] Total time: 0:02:14 (0.1724 s / it)
[03:21:48.425159] Averaged stats: lr: 0.000248  training_loss: 2.0451 (2.0518)  classification_loss: 1.8894 (1.9166)  loss_mask: 0.1343 (0.1352)
[03:21:49.077157] Test:  [  0/157]  eta: 0:01:41  testing_loss: 1.3384 (1.3384)  acc1: 56.2500 (56.2500)  acc5: 92.1875 (92.1875)  time: 0.6478  data: 0.6182  max mem: 6052
[03:21:49.373294] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.4127 (1.4113)  acc1: 53.1250 (51.5625)  acc5: 93.7500 (94.0341)  time: 0.0855  data: 0.0566  max mem: 6052
[03:21:49.656902] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.3533 (1.3834)  acc1: 53.1250 (53.6458)  acc5: 93.7500 (94.4940)  time: 0.0288  data: 0.0003  max mem: 6052
[03:21:49.939256] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.3879 (1.3884)  acc1: 53.1250 (53.6290)  acc5: 93.7500 (93.9516)  time: 0.0282  data: 0.0002  max mem: 6052
[03:21:50.224974] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.4090 (1.3964)  acc1: 53.1250 (53.4680)  acc5: 92.1875 (93.4832)  time: 0.0283  data: 0.0001  max mem: 6052
[03:21:50.506917] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.3837 (1.3917)  acc1: 54.6875 (54.0135)  acc5: 93.7500 (93.6581)  time: 0.0283  data: 0.0001  max mem: 6052
[03:21:50.787917] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.3540 (1.3861)  acc1: 56.2500 (54.4057)  acc5: 95.3125 (93.8012)  time: 0.0280  data: 0.0002  max mem: 6052
[03:21:51.069915] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.3390 (1.3835)  acc1: 54.6875 (54.4454)  acc5: 95.3125 (94.1021)  time: 0.0280  data: 0.0001  max mem: 6052
[03:21:51.350536] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.3434 (1.3855)  acc1: 54.6875 (54.4367)  acc5: 93.7500 (94.0779)  time: 0.0280  data: 0.0001  max mem: 6052
[03:21:51.631613] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.3974 (1.3866)  acc1: 54.6875 (54.2411)  acc5: 95.3125 (94.2136)  time: 0.0280  data: 0.0001  max mem: 6052
[03:21:51.912930] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.3991 (1.3900)  acc1: 53.1250 (53.9449)  acc5: 95.3125 (94.1368)  time: 0.0280  data: 0.0001  max mem: 6052
[03:21:52.194186] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.4406 (1.3942)  acc1: 48.4375 (53.4488)  acc5: 93.7500 (94.1723)  time: 0.0280  data: 0.0001  max mem: 6052
[03:21:52.475732] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.4245 (1.3916)  acc1: 50.0000 (53.6415)  acc5: 95.3125 (94.2794)  time: 0.0280  data: 0.0001  max mem: 6052
[03:21:52.759185] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.3692 (1.3940)  acc1: 54.6875 (53.4948)  acc5: 95.3125 (94.2867)  time: 0.0281  data: 0.0001  max mem: 6052
[03:21:53.039728] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.3834 (1.3938)  acc1: 54.6875 (53.7899)  acc5: 93.7500 (94.1933)  time: 0.0281  data: 0.0001  max mem: 6052
[03:21:53.318468] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.3831 (1.3923)  acc1: 54.6875 (53.8390)  acc5: 93.7500 (94.1950)  time: 0.0279  data: 0.0001  max mem: 6052
[03:21:53.470498] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.3644 (1.3934)  acc1: 54.6875 (53.7300)  acc5: 93.7500 (94.2300)  time: 0.0270  data: 0.0001  max mem: 6052
[03:21:53.643417] Test: Total time: 0:00:05 (0.0332 s / it)
[03:21:53.644009] * Acc@1 53.730 Acc@5 94.230 loss 1.393
[03:21:53.644350] Accuracy of the network on the 10000 test images: 53.7%
[03:21:53.644569] Max accuracy: 53.73%
[03:21:53.824745] log_dir: ./output_dir
[03:21:54.630945] Epoch: [10]  [  0/781]  eta: 0:10:28  lr: 0.000248  training_loss: 1.9217 (1.9217)  classification_loss: 1.8064 (1.8064)  loss_mask: 0.1153 (0.1153)  time: 0.8046  data: 0.6274  max mem: 6052
[03:21:58.083949] Epoch: [10]  [ 20/781]  eta: 0:02:34  lr: 0.000248  training_loss: 2.0163 (2.0175)  classification_loss: 1.8892 (1.8950)  loss_mask: 0.1064 (0.1226)  time: 0.1725  data: 0.0002  max mem: 6052
[03:22:01.499520] Epoch: [10]  [ 40/781]  eta: 0:02:18  lr: 0.000248  training_loss: 1.9904 (2.0082)  classification_loss: 1.9134 (1.9053)  loss_mask: 0.0781 (0.1029)  time: 0.1707  data: 0.0003  max mem: 6052
[03:22:04.909655] Epoch: [10]  [ 60/781]  eta: 0:02:10  lr: 0.000248  training_loss: 2.0812 (2.0276)  classification_loss: 1.9539 (1.9225)  loss_mask: 0.1042 (0.1051)  time: 0.1704  data: 0.0002  max mem: 6052
[03:22:08.467214] Epoch: [10]  [ 80/781]  eta: 0:02:06  lr: 0.000248  training_loss: 2.0763 (2.0359)  classification_loss: 1.9296 (1.9238)  loss_mask: 0.1101 (0.1121)  time: 0.1778  data: 0.0002  max mem: 6052
[03:22:12.038999] Epoch: [10]  [100/781]  eta: 0:02:02  lr: 0.000248  training_loss: 2.0222 (2.0322)  classification_loss: 1.9168 (1.9218)  loss_mask: 0.1062 (0.1104)  time: 0.1785  data: 0.0002  max mem: 6052
[03:22:15.611457] Epoch: [10]  [120/781]  eta: 0:01:58  lr: 0.000248  training_loss: 1.9987 (2.0251)  classification_loss: 1.9015 (1.9175)  loss_mask: 0.0856 (0.1076)  time: 0.1786  data: 0.0002  max mem: 6052
[03:22:19.131654] Epoch: [10]  [140/781]  eta: 0:01:54  lr: 0.000248  training_loss: 2.0359 (2.0257)  classification_loss: 1.8767 (1.9086)  loss_mask: 0.1472 (0.1171)  time: 0.1759  data: 0.0002  max mem: 6052
[03:22:22.545248] Epoch: [10]  [160/781]  eta: 0:01:50  lr: 0.000248  training_loss: 2.1356 (2.0438)  classification_loss: 1.9414 (1.9131)  loss_mask: 0.1797 (0.1307)  time: 0.1706  data: 0.0004  max mem: 6052
[03:22:25.965530] Epoch: [10]  [180/781]  eta: 0:01:46  lr: 0.000248  training_loss: 2.2083 (2.0627)  classification_loss: 1.9815 (1.9219)  loss_mask: 0.2027 (0.1408)  time: 0.1709  data: 0.0001  max mem: 6052
[03:22:29.400493] Epoch: [10]  [200/781]  eta: 0:01:42  lr: 0.000248  training_loss: 2.1790 (2.0846)  classification_loss: 1.9764 (1.9309)  loss_mask: 0.1741 (0.1537)  time: 0.1716  data: 0.0003  max mem: 6052
[03:22:32.831180] Epoch: [10]  [220/781]  eta: 0:01:38  lr: 0.000248  training_loss: 2.4493 (2.1230)  classification_loss: 2.2404 (1.9587)  loss_mask: 0.2140 (0.1644)  time: 0.1714  data: 0.0002  max mem: 6052
[03:22:36.244113] Epoch: [10]  [240/781]  eta: 0:01:35  lr: 0.000248  training_loss: 2.5488 (2.1729)  classification_loss: 2.2240 (1.9796)  loss_mask: 0.3492 (0.1933)  time: 0.1706  data: 0.0002  max mem: 6052
[03:22:39.658640] Epoch: [10]  [260/781]  eta: 0:01:31  lr: 0.000248  training_loss: 2.3849 (2.1893)  classification_loss: 2.1635 (1.9929)  loss_mask: 0.2227 (0.1964)  time: 0.1706  data: 0.0001  max mem: 6052
[03:22:43.050210] Epoch: [10]  [280/781]  eta: 0:01:27  lr: 0.000248  training_loss: 2.2405 (2.1933)  classification_loss: 2.0513 (1.9980)  loss_mask: 0.1592 (0.1953)  time: 0.1695  data: 0.0002  max mem: 6052
[03:22:46.472005] Epoch: [10]  [300/781]  eta: 0:01:24  lr: 0.000248  training_loss: 2.1251 (2.1890)  classification_loss: 1.9793 (1.9975)  loss_mask: 0.1421 (0.1915)  time: 0.1710  data: 0.0003  max mem: 6052
[03:22:49.894917] Epoch: [10]  [320/781]  eta: 0:01:20  lr: 0.000248  training_loss: 2.1033 (2.1821)  classification_loss: 1.9757 (1.9954)  loss_mask: 0.1036 (0.1867)  time: 0.1711  data: 0.0002  max mem: 6052
[03:22:53.329827] Epoch: [10]  [340/781]  eta: 0:01:16  lr: 0.000248  training_loss: 2.0760 (2.1776)  classification_loss: 1.9572 (1.9931)  loss_mask: 0.1342 (0.1845)  time: 0.1717  data: 0.0002  max mem: 6052
[03:22:56.751887] Epoch: [10]  [360/781]  eta: 0:01:13  lr: 0.000248  training_loss: 2.0854 (2.1737)  classification_loss: 1.9657 (1.9926)  loss_mask: 0.1191 (0.1810)  time: 0.1710  data: 0.0003  max mem: 6052
[03:23:00.172296] Epoch: [10]  [380/781]  eta: 0:01:09  lr: 0.000248  training_loss: 2.0489 (2.1666)  classification_loss: 1.9153 (1.9889)  loss_mask: 0.1073 (0.1777)  time: 0.1709  data: 0.0002  max mem: 6052
[03:23:03.592758] Epoch: [10]  [400/781]  eta: 0:01:06  lr: 0.000248  training_loss: 2.0350 (2.1599)  classification_loss: 1.9335 (1.9859)  loss_mask: 0.0800 (0.1740)  time: 0.1709  data: 0.0002  max mem: 6052
[03:23:07.018902] Epoch: [10]  [420/781]  eta: 0:01:02  lr: 0.000248  training_loss: 1.9983 (2.1526)  classification_loss: 1.9023 (1.9820)  loss_mask: 0.0874 (0.1705)  time: 0.1712  data: 0.0002  max mem: 6052
[03:23:10.438195] Epoch: [10]  [440/781]  eta: 0:00:59  lr: 0.000248  training_loss: 2.0110 (2.1471)  classification_loss: 1.9063 (1.9794)  loss_mask: 0.0912 (0.1677)  time: 0.1709  data: 0.0002  max mem: 6052
[03:23:13.855575] Epoch: [10]  [460/781]  eta: 0:00:55  lr: 0.000248  training_loss: 1.9972 (2.1413)  classification_loss: 1.8595 (1.9752)  loss_mask: 0.1123 (0.1661)  time: 0.1708  data: 0.0002  max mem: 6052
[03:23:17.304425] Epoch: [10]  [480/781]  eta: 0:00:52  lr: 0.000248  training_loss: 2.0507 (2.1386)  classification_loss: 1.9251 (1.9731)  loss_mask: 0.1256 (0.1654)  time: 0.1724  data: 0.0002  max mem: 6052
[03:23:20.751889] Epoch: [10]  [500/781]  eta: 0:00:48  lr: 0.000248  training_loss: 2.0164 (2.1331)  classification_loss: 1.9282 (1.9705)  loss_mask: 0.0943 (0.1626)  time: 0.1723  data: 0.0002  max mem: 6052
[03:23:24.167224] Epoch: [10]  [520/781]  eta: 0:00:45  lr: 0.000248  training_loss: 1.9581 (2.1268)  classification_loss: 1.8487 (1.9665)  loss_mask: 0.0802 (0.1603)  time: 0.1707  data: 0.0002  max mem: 6052
[03:23:27.640260] Epoch: [10]  [540/781]  eta: 0:00:41  lr: 0.000248  training_loss: 2.0529 (2.1256)  classification_loss: 1.9262 (1.9657)  loss_mask: 0.1535 (0.1599)  time: 0.1736  data: 0.0002  max mem: 6052
[03:23:31.050107] Epoch: [10]  [560/781]  eta: 0:00:38  lr: 0.000248  training_loss: 2.0201 (2.1220)  classification_loss: 1.9175 (1.9639)  loss_mask: 0.0989 (0.1581)  time: 0.1704  data: 0.0002  max mem: 6052
[03:23:34.475894] Epoch: [10]  [580/781]  eta: 0:00:34  lr: 0.000248  training_loss: 1.9426 (2.1166)  classification_loss: 1.8736 (1.9609)  loss_mask: 0.0843 (0.1557)  time: 0.1712  data: 0.0002  max mem: 6052
[03:23:37.890252] Epoch: [10]  [600/781]  eta: 0:00:31  lr: 0.000248  training_loss: 1.9677 (2.1121)  classification_loss: 1.8800 (1.9586)  loss_mask: 0.0896 (0.1535)  time: 0.1706  data: 0.0002  max mem: 6052
[03:23:41.305810] Epoch: [10]  [620/781]  eta: 0:00:27  lr: 0.000248  training_loss: 2.0079 (2.1086)  classification_loss: 1.8379 (1.9552)  loss_mask: 0.1595 (0.1534)  time: 0.1706  data: 0.0002  max mem: 6052
[03:23:44.738018] Epoch: [10]  [640/781]  eta: 0:00:24  lr: 0.000248  training_loss: 1.9484 (2.1039)  classification_loss: 1.8672 (1.9529)  loss_mask: 0.0722 (0.1510)  time: 0.1715  data: 0.0002  max mem: 6052
[03:23:48.179858] Epoch: [10]  [660/781]  eta: 0:00:20  lr: 0.000248  training_loss: 1.9593 (2.1006)  classification_loss: 1.8808 (1.9517)  loss_mask: 0.0770 (0.1489)  time: 0.1720  data: 0.0002  max mem: 6052
[03:23:51.601381] Epoch: [10]  [680/781]  eta: 0:00:17  lr: 0.000248  training_loss: 1.9659 (2.0963)  classification_loss: 1.8955 (1.9495)  loss_mask: 0.0776 (0.1468)  time: 0.1710  data: 0.0002  max mem: 6052
[03:23:55.017962] Epoch: [10]  [700/781]  eta: 0:00:13  lr: 0.000248  training_loss: 1.9137 (2.0921)  classification_loss: 1.8361 (1.9467)  loss_mask: 0.0779 (0.1454)  time: 0.1708  data: 0.0002  max mem: 6052
[03:23:58.441534] Epoch: [10]  [720/781]  eta: 0:00:10  lr: 0.000248  training_loss: 2.0872 (2.0925)  classification_loss: 1.8592 (1.9448)  loss_mask: 0.1660 (0.1478)  time: 0.1711  data: 0.0002  max mem: 6052
[03:24:01.859039] Epoch: [10]  [740/781]  eta: 0:00:07  lr: 0.000248  training_loss: 2.0170 (2.0906)  classification_loss: 1.8520 (1.9431)  loss_mask: 0.1135 (0.1475)  time: 0.1708  data: 0.0002  max mem: 6052
[03:24:05.281660] Epoch: [10]  [760/781]  eta: 0:00:03  lr: 0.000248  training_loss: 1.9679 (2.0877)  classification_loss: 1.8658 (1.9416)  loss_mask: 0.0877 (0.1461)  time: 0.1711  data: 0.0003  max mem: 6052
[03:24:08.683470] Epoch: [10]  [780/781]  eta: 0:00:00  lr: 0.000248  training_loss: 1.9740 (2.0844)  classification_loss: 1.8595 (1.9400)  loss_mask: 0.0859 (0.1444)  time: 0.1700  data: 0.0001  max mem: 6052
[03:24:08.844902] Epoch: [10] Total time: 0:02:15 (0.1729 s / it)
[03:24:08.845360] Averaged stats: lr: 0.000248  training_loss: 1.9740 (2.0844)  classification_loss: 1.8595 (1.9400)  loss_mask: 0.0859 (0.1444)
[03:24:10.288114] Test:  [  0/157]  eta: 0:01:39  testing_loss: 1.2621 (1.2621)  acc1: 53.1250 (53.1250)  acc5: 93.7500 (93.7500)  time: 0.6361  data: 0.6048  max mem: 6052
[03:24:10.575308] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.3465 (1.3670)  acc1: 50.0000 (50.8523)  acc5: 93.7500 (93.1818)  time: 0.0837  data: 0.0551  max mem: 6052
[03:24:10.859700] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.2943 (1.3227)  acc1: 54.6875 (54.6131)  acc5: 93.7500 (93.4524)  time: 0.0284  data: 0.0002  max mem: 6052
[03:24:11.149493] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.3078 (1.3298)  acc1: 56.2500 (53.9819)  acc5: 93.7500 (93.5484)  time: 0.0286  data: 0.0001  max mem: 6052
[03:24:11.429893] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.3342 (1.3358)  acc1: 51.5625 (53.7348)  acc5: 93.7500 (93.3308)  time: 0.0284  data: 0.0001  max mem: 6052
[03:24:11.710941] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.3147 (1.3322)  acc1: 54.6875 (54.0748)  acc5: 93.7500 (93.5968)  time: 0.0280  data: 0.0001  max mem: 6052
[03:24:11.992682] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.2791 (1.3236)  acc1: 56.2500 (54.4570)  acc5: 95.3125 (93.9293)  time: 0.0280  data: 0.0001  max mem: 6052
[03:24:12.282482] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.2765 (1.3199)  acc1: 57.8125 (54.9736)  acc5: 95.3125 (93.9921)  time: 0.0284  data: 0.0002  max mem: 6052
[03:24:12.573128] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.3016 (1.3223)  acc1: 54.6875 (54.7261)  acc5: 95.3125 (94.0394)  time: 0.0289  data: 0.0002  max mem: 6052
[03:24:12.859926] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.3653 (1.3263)  acc1: 53.1250 (54.5158)  acc5: 93.7500 (94.0247)  time: 0.0287  data: 0.0002  max mem: 6052
[03:24:13.149017] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.3961 (1.3320)  acc1: 51.5625 (54.1925)  acc5: 93.7500 (94.0439)  time: 0.0286  data: 0.0002  max mem: 6052
[03:24:13.433635] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.4027 (1.3368)  acc1: 51.5625 (54.0822)  acc5: 93.7500 (93.9752)  time: 0.0285  data: 0.0002  max mem: 6052
[03:24:13.719255] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.3425 (1.3337)  acc1: 54.6875 (54.1968)  acc5: 95.3125 (94.1632)  time: 0.0284  data: 0.0002  max mem: 6052
[03:24:14.004065] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.3210 (1.3347)  acc1: 56.2500 (54.2104)  acc5: 95.3125 (94.1198)  time: 0.0283  data: 0.0002  max mem: 6052
[03:24:14.287664] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.3398 (1.3346)  acc1: 56.2500 (54.4880)  acc5: 93.7500 (94.0714)  time: 0.0283  data: 0.0002  max mem: 6052
[03:24:14.568959] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.3166 (1.3315)  acc1: 57.8125 (54.7496)  acc5: 92.1875 (94.0501)  time: 0.0281  data: 0.0001  max mem: 6052
[03:24:14.719849] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.3098 (1.3318)  acc1: 54.6875 (54.6900)  acc5: 93.7500 (94.0900)  time: 0.0270  data: 0.0001  max mem: 6052
[03:24:14.873862] Test: Total time: 0:00:05 (0.0333 s / it)
[03:24:14.874329] * Acc@1 54.690 Acc@5 94.090 loss 1.332
[03:24:14.874656] Accuracy of the network on the 10000 test images: 54.7%
[03:24:14.874860] Max accuracy: 54.69%
[03:24:15.037966] log_dir: ./output_dir
[03:24:15.942232] Epoch: [11]  [  0/781]  eta: 0:11:44  lr: 0.000248  training_loss: 1.8427 (1.8427)  classification_loss: 1.8097 (1.8097)  loss_mask: 0.0330 (0.0330)  time: 0.9024  data: 0.7266  max mem: 6052
[03:24:19.370830] Epoch: [11]  [ 20/781]  eta: 0:02:36  lr: 0.000248  training_loss: 1.9611 (1.9719)  classification_loss: 1.8880 (1.8584)  loss_mask: 0.0851 (0.1135)  time: 0.1713  data: 0.0002  max mem: 6052
[03:24:22.804477] Epoch: [11]  [ 40/781]  eta: 0:02:20  lr: 0.000248  training_loss: 2.0515 (2.0098)  classification_loss: 1.8962 (1.8708)  loss_mask: 0.1447 (0.1391)  time: 0.1716  data: 0.0003  max mem: 6052
[03:24:26.225888] Epoch: [11]  [ 60/781]  eta: 0:02:12  lr: 0.000247  training_loss: 2.0272 (2.0140)  classification_loss: 1.8783 (1.8757)  loss_mask: 0.1031 (0.1383)  time: 0.1710  data: 0.0002  max mem: 6052
[03:24:29.656336] Epoch: [11]  [ 80/781]  eta: 0:02:06  lr: 0.000247  training_loss: 1.9856 (2.0124)  classification_loss: 1.8589 (1.8782)  loss_mask: 0.0986 (0.1342)  time: 0.1715  data: 0.0003  max mem: 6052
[03:24:33.161858] Epoch: [11]  [100/781]  eta: 0:02:02  lr: 0.000247  training_loss: 1.9690 (2.0028)  classification_loss: 1.8820 (1.8787)  loss_mask: 0.0719 (0.1240)  time: 0.1752  data: 0.0002  max mem: 6052
[03:24:36.587017] Epoch: [11]  [120/781]  eta: 0:01:57  lr: 0.000247  training_loss: 1.9645 (1.9964)  classification_loss: 1.8582 (1.8772)  loss_mask: 0.0881 (0.1192)  time: 0.1712  data: 0.0002  max mem: 6052
[03:24:40.015969] Epoch: [11]  [140/781]  eta: 0:01:53  lr: 0.000247  training_loss: 1.9407 (1.9868)  classification_loss: 1.8463 (1.8716)  loss_mask: 0.0718 (0.1152)  time: 0.1714  data: 0.0002  max mem: 6052
[03:24:43.411840] Epoch: [11]  [160/781]  eta: 0:01:49  lr: 0.000247  training_loss: 1.9635 (1.9848)  classification_loss: 1.8617 (1.8729)  loss_mask: 0.0765 (0.1119)  time: 0.1697  data: 0.0002  max mem: 6052
[03:24:46.828917] Epoch: [11]  [180/781]  eta: 0:01:45  lr: 0.000247  training_loss: 1.9433 (1.9773)  classification_loss: 1.8356 (1.8704)  loss_mask: 0.0650 (0.1069)  time: 0.1708  data: 0.0002  max mem: 6052
[03:24:50.222448] Epoch: [11]  [200/781]  eta: 0:01:41  lr: 0.000247  training_loss: 1.9512 (1.9767)  classification_loss: 1.8445 (1.8688)  loss_mask: 0.0997 (0.1079)  time: 0.1696  data: 0.0002  max mem: 6052

[03:24:53.684528] Epoch: [11]  [220/781]  eta: 0:01:38  lr: 0.000247  training_loss: 1.9309 (1.9731)  classification_loss: 1.8507 (1.8680)  loss_mask: 0.0688 (0.1050)  time: 0.1730  data: 0.0002  max mem: 6052
[03:24:57.155993] Epoch: [11]  [240/781]  eta: 0:01:34  lr: 0.000247  training_loss: 1.9792 (1.9736)  classification_loss: 1.9080 (1.8703)  loss_mask: 0.0784 (0.1034)  time: 0.1735  data: 0.0002  max mem: 6052
[03:25:00.563771] Epoch: [11]  [260/781]  eta: 0:01:30  lr: 0.000247  training_loss: 1.8898 (1.9671)  classification_loss: 1.8055 (1.8641)  loss_mask: 0.0732 (0.1030)  time: 0.1703  data: 0.0002  max mem: 6052
[03:25:04.010043] Epoch: [11]  [280/781]  eta: 0:01:27  lr: 0.000247  training_loss: 1.9103 (1.9641)  classification_loss: 1.8691 (1.8636)  loss_mask: 0.0522 (0.1005)  time: 0.1722  data: 0.0002  max mem: 6052
[03:25:07.444628] Epoch: [11]  [300/781]  eta: 0:01:23  lr: 0.000247  training_loss: 1.9747 (1.9646)  classification_loss: 1.8209 (1.8621)  loss_mask: 0.1162 (0.1025)  time: 0.1717  data: 0.0003  max mem: 6052
[03:25:10.872740] Epoch: [11]  [320/781]  eta: 0:01:20  lr: 0.000247  training_loss: 1.9506 (1.9627)  classification_loss: 1.8616 (1.8617)  loss_mask: 0.0646 (0.1010)  time: 0.1713  data: 0.0002  max mem: 6052
[03:25:14.301550] Epoch: [11]  [340/781]  eta: 0:01:16  lr: 0.000247  training_loss: 1.9307 (1.9609)  classification_loss: 1.8568 (1.8608)  loss_mask: 0.0746 (0.1001)  time: 0.1714  data: 0.0002  max mem: 6052
[03:25:17.717744] Epoch: [11]  [360/781]  eta: 0:01:13  lr: 0.000247  training_loss: 1.9851 (1.9636)  classification_loss: 1.8790 (1.8616)  loss_mask: 0.1170 (0.1020)  time: 0.1707  data: 0.0002  max mem: 6052
[03:25:21.136042] Epoch: [11]  [380/781]  eta: 0:01:09  lr: 0.000247  training_loss: 2.0292 (1.9671)  classification_loss: 1.8588 (1.8618)  loss_mask: 0.1237 (0.1053)  time: 0.1708  data: 0.0002  max mem: 6052
[03:25:24.555514] Epoch: [11]  [400/781]  eta: 0:01:06  lr: 0.000247  training_loss: 1.9210 (1.9669)  classification_loss: 1.8317 (1.8612)  loss_mask: 0.0881 (0.1057)  time: 0.1709  data: 0.0002  max mem: 6052
[03:25:27.947579] Epoch: [11]  [420/781]  eta: 0:01:02  lr: 0.000247  training_loss: 1.9178 (1.9637)  classification_loss: 1.8246 (1.8595)  loss_mask: 0.0626 (0.1042)  time: 0.1695  data: 0.0001  max mem: 6052
[03:25:31.365429] Epoch: [11]  [440/781]  eta: 0:00:58  lr: 0.000247  training_loss: 1.9484 (1.9629)  classification_loss: 1.8644 (1.8598)  loss_mask: 0.0681 (0.1030)  time: 0.1708  data: 0.0002  max mem: 6052
[03:25:34.765615] Epoch: [11]  [460/781]  eta: 0:00:55  lr: 0.000247  training_loss: 1.9052 (1.9599)  classification_loss: 1.8129 (1.8582)  loss_mask: 0.0622 (0.1017)  time: 0.1699  data: 0.0002  max mem: 6052
[03:25:38.214483] Epoch: [11]  [480/781]  eta: 0:00:52  lr: 0.000247  training_loss: 1.9597 (1.9599)  classification_loss: 1.8884 (1.8596)  loss_mask: 0.0534 (0.1003)  time: 0.1724  data: 0.0002  max mem: 6052
[03:25:41.633841] Epoch: [11]  [500/781]  eta: 0:00:48  lr: 0.000247  training_loss: 1.9450 (1.9606)  classification_loss: 1.8558 (1.8598)  loss_mask: 0.1040 (0.1008)  time: 0.1709  data: 0.0002  max mem: 6052
[03:25:45.057774] Epoch: [11]  [520/781]  eta: 0:00:45  lr: 0.000247  training_loss: 1.9663 (1.9621)  classification_loss: 1.8505 (1.8598)  loss_mask: 0.1119 (0.1023)  time: 0.1711  data: 0.0002  max mem: 6052
[03:25:48.450634] Epoch: [11]  [540/781]  eta: 0:00:41  lr: 0.000247  training_loss: 1.9310 (1.9613)  classification_loss: 1.8237 (1.8598)  loss_mask: 0.0672 (0.1015)  time: 0.1696  data: 0.0002  max mem: 6052
[03:25:51.854227] Epoch: [11]  [560/781]  eta: 0:00:38  lr: 0.000247  training_loss: 1.8663 (1.9598)  classification_loss: 1.8283 (1.8598)  loss_mask: 0.0627 (0.1000)  time: 0.1701  data: 0.0002  max mem: 6052
[03:25:55.261186] Epoch: [11]  [580/781]  eta: 0:00:34  lr: 0.000247  training_loss: 1.9267 (1.9592)  classification_loss: 1.7910 (1.8587)  loss_mask: 0.0761 (0.1005)  time: 0.1703  data: 0.0001  max mem: 6052
[03:25:58.665310] Epoch: [11]  [600/781]  eta: 0:00:31  lr: 0.000247  training_loss: 1.9700 (1.9598)  classification_loss: 1.8502 (1.8583)  loss_mask: 0.0819 (0.1015)  time: 0.1701  data: 0.0001  max mem: 6052
[03:26:02.083134] Epoch: [11]  [620/781]  eta: 0:00:27  lr: 0.000247  training_loss: 1.9209 (1.9588)  classification_loss: 1.8362 (1.8582)  loss_mask: 0.0607 (0.1006)  time: 0.1708  data: 0.0003  max mem: 6052
[03:26:05.511385] Epoch: [11]  [640/781]  eta: 0:00:24  lr: 0.000247  training_loss: 1.9130 (1.9573)  classification_loss: 1.8375 (1.8577)  loss_mask: 0.0614 (0.0996)  time: 0.1713  data: 0.0002  max mem: 6052
[03:26:08.916282] Epoch: [11]  [660/781]  eta: 0:00:20  lr: 0.000247  training_loss: 1.9250 (1.9567)  classification_loss: 1.8560 (1.8577)  loss_mask: 0.0613 (0.0990)  time: 0.1702  data: 0.0002  max mem: 6052
[03:26:12.335051] Epoch: [11]  [680/781]  eta: 0:00:17  lr: 0.000247  training_loss: 1.9213 (1.9560)  classification_loss: 1.8163 (1.8574)  loss_mask: 0.0722 (0.0987)  time: 0.1709  data: 0.0003  max mem: 6052
[03:26:15.754622] Epoch: [11]  [700/781]  eta: 0:00:13  lr: 0.000247  training_loss: 1.9267 (1.9555)  classification_loss: 1.8346 (1.8571)  loss_mask: 0.0873 (0.0984)  time: 0.1709  data: 0.0002  max mem: 6052
[03:26:19.185947] Epoch: [11]  [720/781]  eta: 0:00:10  lr: 0.000247  training_loss: 1.9832 (1.9564)  classification_loss: 1.8551 (1.8577)  loss_mask: 0.0987 (0.0987)  time: 0.1715  data: 0.0002  max mem: 6052
[03:26:22.650350] Epoch: [11]  [740/781]  eta: 0:00:07  lr: 0.000247  training_loss: 1.9999 (1.9576)  classification_loss: 1.8311 (1.8569)  loss_mask: 0.1395 (0.1007)  time: 0.1731  data: 0.0002  max mem: 6052
[03:26:26.058636] Epoch: [11]  [760/781]  eta: 0:00:03  lr: 0.000247  training_loss: 1.9460 (1.9578)  classification_loss: 1.8430 (1.8566)  loss_mask: 0.1122 (0.1011)  time: 0.1703  data: 0.0002  max mem: 6052
[03:26:29.459866] Epoch: [11]  [780/781]  eta: 0:00:00  lr: 0.000247  training_loss: 1.9174 (1.9572)  classification_loss: 1.8482 (1.8568)  loss_mask: 0.0732 (0.1004)  time: 0.1700  data: 0.0002  max mem: 6052
[03:26:29.623251] Epoch: [11] Total time: 0:02:14 (0.1723 s / it)
[03:26:29.623717] Averaged stats: lr: 0.000247  training_loss: 1.9174 (1.9572)  classification_loss: 1.8482 (1.8568)  loss_mask: 0.0732 (0.1004)
[03:26:30.264648] Test:  [  0/157]  eta: 0:01:39  testing_loss: 1.2644 (1.2644)  acc1: 57.8125 (57.8125)  acc5: 92.1875 (92.1875)  time: 0.6366  data: 0.6003  max mem: 6052
[03:26:30.546527] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.2781 (1.3017)  acc1: 51.5625 (53.4091)  acc5: 95.3125 (95.3125)  time: 0.0833  data: 0.0547  max mem: 6052
[03:26:30.828558] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.2338 (1.2650)  acc1: 54.6875 (56.0268)  acc5: 95.3125 (95.5357)  time: 0.0280  data: 0.0002  max mem: 6052
[03:26:31.109184] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.2474 (1.2721)  acc1: 57.8125 (56.3508)  acc5: 95.3125 (95.1109)  time: 0.0280  data: 0.0002  max mem: 6052
[03:26:31.390059] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.2776 (1.2805)  acc1: 54.6875 (56.4405)  acc5: 93.7500 (94.8171)  time: 0.0280  data: 0.0002  max mem: 6052
[03:26:31.670353] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.2497 (1.2742)  acc1: 56.2500 (56.7402)  acc5: 95.3125 (94.9449)  time: 0.0280  data: 0.0002  max mem: 6052
[03:26:31.950259] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.2463 (1.2671)  acc1: 59.3750 (57.2490)  acc5: 95.3125 (94.9283)  time: 0.0279  data: 0.0001  max mem: 6052
[03:26:32.229802] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.2299 (1.2617)  acc1: 59.3750 (57.5264)  acc5: 95.3125 (95.1144)  time: 0.0279  data: 0.0001  max mem: 6052
[03:26:32.511474] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.2447 (1.2646)  acc1: 59.3750 (57.6196)  acc5: 95.3125 (95.0810)  time: 0.0280  data: 0.0002  max mem: 6052
[03:26:32.792368] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.2706 (1.2682)  acc1: 54.6875 (57.2974)  acc5: 95.3125 (95.1923)  time: 0.0280  data: 0.0002  max mem: 6052
[03:26:33.073060] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.3294 (1.2728)  acc1: 53.1250 (56.9307)  acc5: 96.8750 (95.1887)  time: 0.0280  data: 0.0002  max mem: 6052
[03:26:33.353029] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.3275 (1.2761)  acc1: 54.6875 (56.7708)  acc5: 95.3125 (95.1295)  time: 0.0279  data: 0.0002  max mem: 6052
[03:26:33.638441] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.3011 (1.2734)  acc1: 54.6875 (56.7924)  acc5: 95.3125 (95.1963)  time: 0.0282  data: 0.0001  max mem: 6052
[03:26:33.918720] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.2813 (1.2761)  acc1: 54.6875 (56.6555)  acc5: 95.3125 (95.0501)  time: 0.0282  data: 0.0001  max mem: 6052
[03:26:34.198978] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.2745 (1.2759)  acc1: 56.2500 (56.7376)  acc5: 93.7500 (95.0022)  time: 0.0279  data: 0.0001  max mem: 6052
[03:26:34.479088] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.2607 (1.2734)  acc1: 56.2500 (56.8605)  acc5: 95.3125 (95.0642)  time: 0.0279  data: 0.0001  max mem: 6052
[03:26:34.630554] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.2317 (1.2743)  acc1: 57.8125 (56.8300)  acc5: 95.3125 (95.1100)  time: 0.0270  data: 0.0001  max mem: 6052
[03:26:34.796263] Test: Total time: 0:00:05 (0.0329 s / it)
[03:26:34.796753] * Acc@1 56.830 Acc@5 95.110 loss 1.274
[03:26:34.797063] Accuracy of the network on the 10000 test images: 56.8%
[03:26:34.797248] Max accuracy: 56.83%
[03:26:35.015117] log_dir: ./output_dir
[03:26:35.833320] Epoch: [12]  [  0/781]  eta: 0:10:37  lr: 0.000247  training_loss: 1.7523 (1.7523)  classification_loss: 1.6976 (1.6976)  loss_mask: 0.0547 (0.0547)  time: 0.8166  data: 0.6381  max mem: 6052
[03:26:39.262415] Epoch: [12]  [ 20/781]  eta: 0:02:33  lr: 0.000247  training_loss: 1.9153 (1.9308)  classification_loss: 1.8321 (1.8328)  loss_mask: 0.0859 (0.0980)  time: 0.1714  data: 0.0002  max mem: 6052
[03:26:42.695730] Epoch: [12]  [ 40/781]  eta: 0:02:18  lr: 0.000247  training_loss: 1.9476 (1.9438)  classification_loss: 1.8340 (1.8385)  loss_mask: 0.0825 (0.1053)  time: 0.1716  data: 0.0002  max mem: 6052
[03:26:46.200805] Epoch: [12]  [ 60/781]  eta: 0:02:12  lr: 0.000247  training_loss: 1.9236 (1.9396)  classification_loss: 1.8531 (1.8390)  loss_mask: 0.0708 (0.1006)  time: 0.1752  data: 0.0002  max mem: 6052
[03:26:49.648410] Epoch: [12]  [ 80/781]  eta: 0:02:06  lr: 0.000247  training_loss: 1.9096 (1.9345)  classification_loss: 1.8473 (1.8428)  loss_mask: 0.0607 (0.0918)  time: 0.1723  data: 0.0002  max mem: 6052
[03:26:53.069544] Epoch: [12]  [100/781]  eta: 0:02:01  lr: 0.000247  training_loss: 1.8758 (1.9294)  classification_loss: 1.8259 (1.8441)  loss_mask: 0.0440 (0.0853)  time: 0.1710  data: 0.0002  max mem: 6052
[03:26:56.479396] Epoch: [12]  [120/781]  eta: 0:01:57  lr: 0.000247  training_loss: 1.8993 (1.9243)  classification_loss: 1.8252 (1.8416)  loss_mask: 0.0731 (0.0826)  time: 0.1704  data: 0.0003  max mem: 6052
[03:26:59.918631] Epoch: [12]  [140/781]  eta: 0:01:53  lr: 0.000247  training_loss: 1.9154 (1.9221)  classification_loss: 1.8242 (1.8398)  loss_mask: 0.0662 (0.0823)  time: 0.1719  data: 0.0002  max mem: 6052
[03:27:03.430062] Epoch: [12]  [160/781]  eta: 0:01:49  lr: 0.000246  training_loss: 1.9059 (1.9235)  classification_loss: 1.8312 (1.8418)  loss_mask: 0.0791 (0.0817)  time: 0.1755  data: 0.0002  max mem: 6052
[03:27:06.853631] Epoch: [12]  [180/781]  eta: 0:01:45  lr: 0.000246  training_loss: 1.8925 (1.9204)  classification_loss: 1.8080 (1.8383)  loss_mask: 0.0647 (0.0820)  time: 0.1711  data: 0.0002  max mem: 6052
[03:27:10.274685] Epoch: [12]  [200/781]  eta: 0:01:41  lr: 0.000246  training_loss: 1.9681 (1.9237)  classification_loss: 1.8445 (1.8391)  loss_mask: 0.0937 (0.0846)  time: 0.1710  data: 0.0002  max mem: 6052
[03:27:13.691714] Epoch: [12]  [220/781]  eta: 0:01:38  lr: 0.000246  training_loss: 1.9976 (1.9267)  classification_loss: 1.8609 (1.8391)  loss_mask: 0.0865 (0.0876)  time: 0.1708  data: 0.0002  max mem: 6052
[03:27:17.127070] Epoch: [12]  [240/781]  eta: 0:01:34  lr: 0.000246  training_loss: 1.9103 (1.9251)  classification_loss: 1.8243 (1.8382)  loss_mask: 0.0713 (0.0870)  time: 0.1717  data: 0.0003  max mem: 6052
[03:27:20.553655] Epoch: [12]  [260/781]  eta: 0:01:30  lr: 0.000246  training_loss: 1.9104 (1.9244)  classification_loss: 1.7837 (1.8363)  loss_mask: 0.0865 (0.0881)  time: 0.1713  data: 0.0004  max mem: 6052
[03:27:23.972211] Epoch: [12]  [280/781]  eta: 0:01:27  lr: 0.000246  training_loss: 1.9505 (1.9269)  classification_loss: 1.8484 (1.8369)  loss_mask: 0.1050 (0.0899)  time: 0.1709  data: 0.0003  max mem: 6052
[03:27:27.389679] Epoch: [12]  [300/781]  eta: 0:01:23  lr: 0.000246  training_loss: 1.8997 (1.9274)  classification_loss: 1.8230 (1.8381)  loss_mask: 0.0497 (0.0893)  time: 0.1708  data: 0.0002  max mem: 6052
[03:27:30.811215] Epoch: [12]  [320/781]  eta: 0:01:20  lr: 0.000246  training_loss: 1.9254 (1.9297)  classification_loss: 1.8222 (1.8389)  loss_mask: 0.0955 (0.0908)  time: 0.1710  data: 0.0002  max mem: 6052
[03:27:34.241426] Epoch: [12]  [340/781]  eta: 0:01:16  lr: 0.000246  training_loss: 1.9348 (1.9319)  classification_loss: 1.8372 (1.8377)  loss_mask: 0.1218 (0.0942)  time: 0.1714  data: 0.0002  max mem: 6052
[03:27:37.675903] Epoch: [12]  [360/781]  eta: 0:01:13  lr: 0.000246  training_loss: 2.0115 (1.9386)  classification_loss: 1.8876 (1.8403)  loss_mask: 0.1146 (0.0983)  time: 0.1716  data: 0.0002  max mem: 6052
[03:27:41.110087] Epoch: [12]  [380/781]  eta: 0:01:09  lr: 0.000246  training_loss: 1.9491 (1.9388)  classification_loss: 1.8458 (1.8403)  loss_mask: 0.0801 (0.0985)  time: 0.1716  data: 0.0003  max mem: 6052
[03:27:44.544506] Epoch: [12]  [400/781]  eta: 0:01:06  lr: 0.000246  training_loss: 1.9217 (1.9389)  classification_loss: 1.8282 (1.8404)  loss_mask: 0.0777 (0.0985)  time: 0.1716  data: 0.0002  max mem: 6052
[03:27:48.004453] Epoch: [12]  [420/781]  eta: 0:01:02  lr: 0.000246  training_loss: 1.8894 (1.9377)  classification_loss: 1.8134 (1.8398)  loss_mask: 0.0895 (0.0979)  time: 0.1729  data: 0.0003  max mem: 6052
[03:27:51.434703] Epoch: [12]  [440/781]  eta: 0:00:59  lr: 0.000246  training_loss: 1.8934 (1.9358)  classification_loss: 1.8434 (1.8392)  loss_mask: 0.0636 (0.0966)  time: 0.1714  data: 0.0002  max mem: 6052
[03:27:54.901541] Epoch: [12]  [460/781]  eta: 0:00:55  lr: 0.000246  training_loss: 1.8542 (1.9326)  classification_loss: 1.7847 (1.8370)  loss_mask: 0.0649 (0.0957)  time: 0.1733  data: 0.0002  max mem: 6052
[03:27:58.362071] Epoch: [12]  [480/781]  eta: 0:00:52  lr: 0.000246  training_loss: 1.8793 (1.9310)  classification_loss: 1.8250 (1.8360)  loss_mask: 0.0689 (0.0950)  time: 0.1729  data: 0.0002  max mem: 6052
[03:28:01.818147] Epoch: [12]  [500/781]  eta: 0:00:48  lr: 0.000246  training_loss: 1.8951 (1.9290)  classification_loss: 1.8027 (1.8345)  loss_mask: 0.0715 (0.0945)  time: 0.1727  data: 0.0002  max mem: 6052
[03:28:05.232821] Epoch: [12]  [520/781]  eta: 0:00:45  lr: 0.000246  training_loss: 1.8916 (1.9284)  classification_loss: 1.8118 (1.8339)  loss_mask: 0.0775 (0.0945)  time: 0.1707  data: 0.0002  max mem: 6052
[03:28:08.653549] Epoch: [12]  [540/781]  eta: 0:00:41  lr: 0.000246  training_loss: 1.9752 (1.9299)  classification_loss: 1.7751 (1.8333)  loss_mask: 0.1243 (0.0967)  time: 0.1710  data: 0.0002  max mem: 6052
[03:28:12.089485] Epoch: [12]  [560/781]  eta: 0:00:38  lr: 0.000246  training_loss: 2.0004 (1.9323)  classification_loss: 1.8499 (1.8341)  loss_mask: 0.1049 (0.0982)  time: 0.1717  data: 0.0002  max mem: 6052
[03:28:15.515100] Epoch: [12]  [580/781]  eta: 0:00:34  lr: 0.000246  training_loss: 1.9017 (1.9315)  classification_loss: 1.7843 (1.8330)  loss_mask: 0.0859 (0.0985)  time: 0.1712  data: 0.0002  max mem: 6052
[03:28:18.936644] Epoch: [12]  [600/781]  eta: 0:00:31  lr: 0.000246  training_loss: 1.8029 (1.9294)  classification_loss: 1.7572 (1.8320)  loss_mask: 0.0583 (0.0974)  time: 0.1710  data: 0.0001  max mem: 6052
[03:28:22.368322] Epoch: [12]  [620/781]  eta: 0:00:27  lr: 0.000246  training_loss: 1.8321 (1.9268)  classification_loss: 1.7661 (1.8305)  loss_mask: 0.0532 (0.0964)  time: 0.1715  data: 0.0002  max mem: 6052
[03:28:25.782556] Epoch: [12]  [640/781]  eta: 0:00:24  lr: 0.000246  training_loss: 1.8837 (1.9258)  classification_loss: 1.8010 (1.8293)  loss_mask: 0.0969 (0.0965)  time: 0.1706  data: 0.0002  max mem: 6052
[03:28:29.220075] Epoch: [12]  [660/781]  eta: 0:00:20  lr: 0.000246  training_loss: 1.9337 (1.9252)  classification_loss: 1.8495 (1.8293)  loss_mask: 0.0741 (0.0960)  time: 0.1718  data: 0.0002  max mem: 6052
[03:28:32.628609] Epoch: [12]  [680/781]  eta: 0:00:17  lr: 0.000246  training_loss: 1.9358 (1.9258)  classification_loss: 1.8192 (1.8293)  loss_mask: 0.1061 (0.0965)  time: 0.1703  data: 0.0001  max mem: 6052
[03:28:36.027030] Epoch: [12]  [700/781]  eta: 0:00:13  lr: 0.000246  training_loss: 1.9002 (1.9245)  classification_loss: 1.8117 (1.8287)  loss_mask: 0.0660 (0.0958)  time: 0.1698  data: 0.0001  max mem: 6052
[03:28:39.428257] Epoch: [12]  [720/781]  eta: 0:00:10  lr: 0.000246  training_loss: 1.8874 (1.9241)  classification_loss: 1.8315 (1.8293)  loss_mask: 0.0526 (0.0948)  time: 0.1700  data: 0.0001  max mem: 6052
[03:28:42.842334] Epoch: [12]  [740/781]  eta: 0:00:07  lr: 0.000246  training_loss: 1.9075 (1.9233)  classification_loss: 1.8292 (1.8292)  loss_mask: 0.0616 (0.0942)  time: 0.1706  data: 0.0002  max mem: 6052
[03:28:46.260518] Epoch: [12]  [760/781]  eta: 0:00:03  lr: 0.000246  training_loss: 1.9502 (1.9240)  classification_loss: 1.8155 (1.8291)  loss_mask: 0.1025 (0.0949)  time: 0.1708  data: 0.0002  max mem: 6052
[03:28:49.660766] Epoch: [12]  [780/781]  eta: 0:00:00  lr: 0.000246  training_loss: 1.9150 (1.9238)  classification_loss: 1.8122 (1.8286)  loss_mask: 0.0892 (0.0951)  time: 0.1699  data: 0.0001  max mem: 6052
[03:28:49.810692] Epoch: [12] Total time: 0:02:14 (0.1726 s / it)
[03:28:49.811164] Averaged stats: lr: 0.000246  training_loss: 1.9150 (1.9238)  classification_loss: 1.8122 (1.8286)  loss_mask: 0.0892 (0.0951)
[03:28:50.493952] Test:  [  0/157]  eta: 0:01:46  testing_loss: 1.1352 (1.1352)  acc1: 67.1875 (67.1875)  acc5: 92.1875 (92.1875)  time: 0.6783  data: 0.6289  max mem: 6052
[03:28:50.798629] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.2348 (1.2428)  acc1: 56.2500 (55.9659)  acc5: 96.8750 (95.7386)  time: 0.0892  data: 0.0575  max mem: 6052
[03:28:51.085018] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.2043 (1.2094)  acc1: 56.2500 (57.5893)  acc5: 96.8750 (96.1310)  time: 0.0294  data: 0.0003  max mem: 6052
[03:28:51.369159] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.2132 (1.2234)  acc1: 59.3750 (58.2157)  acc5: 95.3125 (95.7157)  time: 0.0284  data: 0.0002  max mem: 6052
[03:28:51.650762] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.2299 (1.2298)  acc1: 59.3750 (58.1936)  acc5: 93.7500 (95.3125)  time: 0.0282  data: 0.0001  max mem: 6052
[03:28:51.932044] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.2093 (1.2244)  acc1: 57.8125 (58.2721)  acc5: 95.3125 (95.5576)  time: 0.0280  data: 0.0001  max mem: 6052
[03:28:52.215177] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.1848 (1.2153)  acc1: 57.8125 (58.2992)  acc5: 96.8750 (95.6199)  time: 0.0281  data: 0.0001  max mem: 6052
[03:28:52.499143] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.1610 (1.2109)  acc1: 57.8125 (58.2526)  acc5: 95.3125 (95.6426)  time: 0.0282  data: 0.0002  max mem: 6052
[03:28:52.783544] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.1811 (1.2124)  acc1: 56.2500 (57.9475)  acc5: 96.8750 (95.5633)  time: 0.0283  data: 0.0002  max mem: 6052
[03:28:53.065187] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.2104 (1.2166)  acc1: 56.2500 (57.6751)  acc5: 96.8750 (95.5185)  time: 0.0281  data: 0.0002  max mem: 6052
[03:28:53.345804] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.2679 (1.2224)  acc1: 53.1250 (57.3948)  acc5: 95.3125 (95.4981)  time: 0.0280  data: 0.0001  max mem: 6052
[03:28:53.625820] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.2873 (1.2254)  acc1: 53.1250 (57.2072)  acc5: 95.3125 (95.4110)  time: 0.0279  data: 0.0001  max mem: 6052
[03:28:53.905902] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.2388 (1.2220)  acc1: 57.8125 (57.4380)  acc5: 95.3125 (95.5062)  time: 0.0279  data: 0.0001  max mem: 6052
[03:28:54.187049] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.2078 (1.2239)  acc1: 57.8125 (57.3115)  acc5: 95.3125 (95.4914)  time: 0.0280  data: 0.0001  max mem: 6052
[03:28:54.466590] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.2078 (1.2228)  acc1: 59.3750 (57.6795)  acc5: 95.3125 (95.4566)  time: 0.0279  data: 0.0001  max mem: 6052
[03:28:54.744937] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.2005 (1.2198)  acc1: 60.9375 (57.7918)  acc5: 95.3125 (95.4263)  time: 0.0278  data: 0.0001  max mem: 6052
[03:28:54.896409] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.2017 (1.2210)  acc1: 57.8125 (57.6300)  acc5: 95.3125 (95.4600)  time: 0.0269  data: 0.0001  max mem: 6052
[03:28:55.066426] Test: Total time: 0:00:05 (0.0334 s / it)
[03:28:55.066866] * Acc@1 57.630 Acc@5 95.460 loss 1.221
[03:28:55.067303] Accuracy of the network on the 10000 test images: 57.6%
[03:28:55.067535] Max accuracy: 57.63%
[03:28:55.241780] log_dir: ./output_dir
[03:28:56.081967] Epoch: [13]  [  0/781]  eta: 0:10:54  lr: 0.000246  training_loss: 1.9670 (1.9670)  classification_loss: 1.8852 (1.8852)  loss_mask: 0.0817 (0.0817)  time: 0.8386  data: 0.6603  max mem: 6052
[03:28:59.501039] Epoch: [13]  [ 20/781]  eta: 0:02:34  lr: 0.000246  training_loss: 1.9493 (1.9540)  classification_loss: 1.7938 (1.7987)  loss_mask: 0.1341 (0.1553)  time: 0.1709  data: 0.0002  max mem: 6052
[03:29:02.988064] Epoch: [13]  [ 40/781]  eta: 0:02:19  lr: 0.000246  training_loss: 1.8987 (1.9368)  classification_loss: 1.8230 (1.8221)  loss_mask: 0.0684 (0.1147)  time: 0.1743  data: 0.0002  max mem: 6052
[03:29:06.420277] Epoch: [13]  [ 60/781]  eta: 0:02:12  lr: 0.000246  training_loss: 1.9259 (1.9322)  classification_loss: 1.8816 (1.8327)  loss_mask: 0.0648 (0.0995)  time: 0.1715  data: 0.0003  max mem: 6052
[03:29:09.867846] Epoch: [13]  [ 80/781]  eta: 0:02:06  lr: 0.000246  training_loss: 1.9303 (1.9240)  classification_loss: 1.8232 (1.8295)  loss_mask: 0.0737 (0.0945)  time: 0.1722  data: 0.0003  max mem: 6052
[03:29:13.304812] Epoch: [13]  [100/781]  eta: 0:02:01  lr: 0.000246  training_loss: 1.8901 (1.9216)  classification_loss: 1.8447 (1.8327)  loss_mask: 0.0500 (0.0889)  time: 0.1718  data: 0.0002  max mem: 6052
[03:29:16.767571] Epoch: [13]  [120/781]  eta: 0:01:57  lr: 0.000246  training_loss: 1.8752 (1.9158)  classification_loss: 1.7730 (1.8278)  loss_mask: 0.0746 (0.0880)  time: 0.1731  data: 0.0002  max mem: 6052
[03:29:20.201563] Epoch: [13]  [140/781]  eta: 0:01:53  lr: 0.000245  training_loss: 1.9278 (1.9239)  classification_loss: 1.7803 (1.8216)  loss_mask: 0.1324 (0.1023)  time: 0.1716  data: 0.0002  max mem: 6052
[03:29:23.740177] Epoch: [13]  [160/781]  eta: 0:01:49  lr: 0.000245  training_loss: 1.9667 (1.9310)  classification_loss: 1.8208 (1.8249)  loss_mask: 0.1020 (0.1061)  time: 0.1769  data: 0.0002  max mem: 6052
[03:29:27.247351] Epoch: [13]  [180/781]  eta: 0:01:46  lr: 0.000245  training_loss: 1.8428 (1.9254)  classification_loss: 1.7714 (1.8195)  loss_mask: 0.1010 (0.1059)  time: 0.1753  data: 0.0003  max mem: 6052
[03:29:30.743979] Epoch: [13]  [200/781]  eta: 0:01:42  lr: 0.000245  training_loss: 1.8723 (1.9218)  classification_loss: 1.8082 (1.8182)  loss_mask: 0.0725 (0.1036)  time: 0.1748  data: 0.0002  max mem: 6052
[03:29:34.203641] Epoch: [13]  [220/781]  eta: 0:01:38  lr: 0.000245  training_loss: 1.9491 (1.9208)  classification_loss: 1.8460 (1.8192)  loss_mask: 0.0730 (0.1016)  time: 0.1729  data: 0.0002  max mem: 6052
[03:29:37.663301] Epoch: [13]  [240/781]  eta: 0:01:35  lr: 0.000245  training_loss: 1.8758 (1.9187)  classification_loss: 1.8079 (1.8193)  loss_mask: 0.0751 (0.0994)  time: 0.1729  data: 0.0004  max mem: 6052
[03:29:41.134909] Epoch: [13]  [260/781]  eta: 0:01:31  lr: 0.000245  training_loss: 1.9125 (1.9167)  classification_loss: 1.8085 (1.8178)  loss_mask: 0.0703 (0.0990)  time: 0.1735  data: 0.0002  max mem: 6052
[03:29:44.533421] Epoch: [13]  [280/781]  eta: 0:01:27  lr: 0.000245  training_loss: 1.8891 (1.9155)  classification_loss: 1.8238 (1.8180)  loss_mask: 0.0701 (0.0976)  time: 0.1698  data: 0.0003  max mem: 6052
[03:29:47.940608] Epoch: [13]  [300/781]  eta: 0:01:24  lr: 0.000245  training_loss: 1.8775 (1.9133)  classification_loss: 1.7944 (1.8152)  loss_mask: 0.0857 (0.0981)  time: 0.1703  data: 0.0003  max mem: 6052
[03:29:51.367208] Epoch: [13]  [320/781]  eta: 0:01:20  lr: 0.000245  training_loss: 1.9767 (1.9176)  classification_loss: 1.8336 (1.8168)  loss_mask: 0.1226 (0.1008)  time: 0.1713  data: 0.0002  max mem: 6052
[03:29:54.758326] Epoch: [13]  [340/781]  eta: 0:01:16  lr: 0.000245  training_loss: 1.8666 (1.9145)  classification_loss: 1.7736 (1.8150)  loss_mask: 0.0624 (0.0995)  time: 0.1695  data: 0.0002  max mem: 6052
[03:29:58.170094] Epoch: [13]  [360/781]  eta: 0:01:13  lr: 0.000245  training_loss: 1.8588 (1.9112)  classification_loss: 1.7916 (1.8137)  loss_mask: 0.0527 (0.0975)  time: 0.1705  data: 0.0002  max mem: 6052
[03:30:01.562154] Epoch: [13]  [380/781]  eta: 0:01:09  lr: 0.000245  training_loss: 1.8807 (1.9096)  classification_loss: 1.8326 (1.8141)  loss_mask: 0.0501 (0.0955)  time: 0.1695  data: 0.0002  max mem: 6052
[03:30:04.976677] Epoch: [13]  [400/781]  eta: 0:01:06  lr: 0.000245  training_loss: 1.8621 (1.9073)  classification_loss: 1.7982 (1.8136)  loss_mask: 0.0558 (0.0937)  time: 0.1707  data: 0.0003  max mem: 6052
[03:30:08.371041] Epoch: [13]  [420/781]  eta: 0:01:02  lr: 0.000245  training_loss: 1.8406 (1.9042)  classification_loss: 1.7622 (1.8119)  loss_mask: 0.0566 (0.0923)  time: 0.1696  data: 0.0001  max mem: 6052
[03:30:11.798613] Epoch: [13]  [440/781]  eta: 0:00:59  lr: 0.000245  training_loss: 1.8401 (1.9005)  classification_loss: 1.7716 (1.8097)  loss_mask: 0.0527 (0.0908)  time: 0.1713  data: 0.0004  max mem: 6052
[03:30:15.212723] Epoch: [13]  [460/781]  eta: 0:00:55  lr: 0.000245  training_loss: 1.8952 (1.9010)  classification_loss: 1.8118 (1.8086)  loss_mask: 0.1073 (0.0924)  time: 0.1706  data: 0.0002  max mem: 6052
[03:30:18.620271] Epoch: [13]  [480/781]  eta: 0:00:52  lr: 0.000245  training_loss: 1.9462 (1.9025)  classification_loss: 1.8058 (1.8088)  loss_mask: 0.1142 (0.0937)  time: 0.1703  data: 0.0002  max mem: 6052
[03:30:22.036467] Epoch: [13]  [500/781]  eta: 0:00:48  lr: 0.000245  training_loss: 1.9222 (1.9033)  classification_loss: 1.8388 (1.8095)  loss_mask: 0.0669 (0.0938)  time: 0.1707  data: 0.0002  max mem: 6052
[03:30:25.463430] Epoch: [13]  [520/781]  eta: 0:00:45  lr: 0.000245  training_loss: 1.8766 (1.9033)  classification_loss: 1.8138 (1.8091)  loss_mask: 0.0837 (0.0942)  time: 0.1712  data: 0.0003  max mem: 6052
[03:30:28.955965] Epoch: [13]  [540/781]  eta: 0:00:41  lr: 0.000245  training_loss: 1.8577 (1.9030)  classification_loss: 1.8001 (1.8101)  loss_mask: 0.0577 (0.0929)  time: 0.1745  data: 0.0002  max mem: 6052
[03:30:32.369744] Epoch: [13]  [560/781]  eta: 0:00:38  lr: 0.000245  training_loss: 1.8457 (1.9015)  classification_loss: 1.8171 (1.8098)  loss_mask: 0.0560 (0.0917)  time: 0.1706  data: 0.0004  max mem: 6052
[03:30:35.782105] Epoch: [13]  [580/781]  eta: 0:00:34  lr: 0.000245  training_loss: 1.8991 (1.9024)  classification_loss: 1.8142 (1.8099)  loss_mask: 0.0938 (0.0925)  time: 0.1705  data: 0.0003  max mem: 6052
[03:30:39.218205] Epoch: [13]  [600/781]  eta: 0:00:31  lr: 0.000245  training_loss: 1.8429 (1.9007)  classification_loss: 1.7524 (1.8085)  loss_mask: 0.0746 (0.0922)  time: 0.1717  data: 0.0002  max mem: 6052
[03:30:42.691050] Epoch: [13]  [620/781]  eta: 0:00:27  lr: 0.000245  training_loss: 1.8408 (1.8992)  classification_loss: 1.7432 (1.8065)  loss_mask: 0.0728 (0.0927)  time: 0.1736  data: 0.0003  max mem: 6052
[03:30:46.185097] Epoch: [13]  [640/781]  eta: 0:00:24  lr: 0.000245  training_loss: 1.8619 (1.8987)  classification_loss: 1.7770 (1.8061)  loss_mask: 0.0791 (0.0926)  time: 0.1746  data: 0.0002  max mem: 6052
[03:30:49.594828] Epoch: [13]  [660/781]  eta: 0:00:20  lr: 0.000245  training_loss: 1.8564 (1.8980)  classification_loss: 1.7566 (1.8054)  loss_mask: 0.0784 (0.0926)  time: 0.1704  data: 0.0002  max mem: 6052
[03:30:53.001445] Epoch: [13]  [680/781]  eta: 0:00:17  lr: 0.000245  training_loss: 1.8451 (1.8968)  classification_loss: 1.7973 (1.8057)  loss_mask: 0.0349 (0.0910)  time: 0.1703  data: 0.0002  max mem: 6052
[03:30:56.450161] Epoch: [13]  [700/781]  eta: 0:00:13  lr: 0.000245  training_loss: 1.8013 (1.8955)  classification_loss: 1.7594 (1.8054)  loss_mask: 0.0454 (0.0900)  time: 0.1724  data: 0.0002  max mem: 6052
[03:30:59.905182] Epoch: [13]  [720/781]  eta: 0:00:10  lr: 0.000245  training_loss: 1.8539 (1.8950)  classification_loss: 1.7701 (1.8049)  loss_mask: 0.0821 (0.0900)  time: 0.1727  data: 0.0002  max mem: 6052
[03:31:03.327415] Epoch: [13]  [740/781]  eta: 0:00:07  lr: 0.000245  training_loss: 1.8238 (1.8931)  classification_loss: 1.7975 (1.8041)  loss_mask: 0.0471 (0.0891)  time: 0.1710  data: 0.0002  max mem: 6052
[03:31:06.758228] Epoch: [13]  [760/781]  eta: 0:00:03  lr: 0.000245  training_loss: 1.8839 (1.8935)  classification_loss: 1.7994 (1.8046)  loss_mask: 0.0664 (0.0888)  time: 0.1715  data: 0.0003  max mem: 6052
[03:31:10.183387] Epoch: [13]  [780/781]  eta: 0:00:00  lr: 0.000245  training_loss: 1.9177 (1.8937)  classification_loss: 1.8091 (1.8048)  loss_mask: 0.0615 (0.0889)  time: 0.1712  data: 0.0002  max mem: 6052
[03:31:10.334468] Epoch: [13] Total time: 0:02:15 (0.1730 s / it)
[03:31:10.335187] Averaged stats: lr: 0.000245  training_loss: 1.9177 (1.8937)  classification_loss: 1.8091 (1.8048)  loss_mask: 0.0615 (0.0889)
[03:31:10.962980] Test:  [  0/157]  eta: 0:01:37  testing_loss: 1.1277 (1.1277)  acc1: 62.5000 (62.5000)  acc5: 92.1875 (92.1875)  time: 0.6184  data: 0.5859  max mem: 6052
[03:31:11.248359] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.2037 (1.2253)  acc1: 56.2500 (56.9602)  acc5: 95.3125 (95.4545)  time: 0.0819  data: 0.0534  max mem: 6052
[03:31:11.531095] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.1631 (1.1808)  acc1: 59.3750 (59.0774)  acc5: 95.3125 (95.9821)  time: 0.0282  data: 0.0002  max mem: 6052
[03:31:11.810380] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 1.1794 (1.1862)  acc1: 59.3750 (58.8710)  acc5: 96.8750 (96.0181)  time: 0.0280  data: 0.0002  max mem: 6052
[03:31:12.089873] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 1.1927 (1.1885)  acc1: 59.3750 (59.4512)  acc5: 95.3125 (95.8460)  time: 0.0278  data: 0.0002  max mem: 6052
[03:31:12.369397] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.1681 (1.1862)  acc1: 60.9375 (59.7120)  acc5: 95.3125 (95.9865)  time: 0.0278  data: 0.0002  max mem: 6052
[03:31:12.648652] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.1480 (1.1804)  acc1: 60.9375 (59.8873)  acc5: 96.8750 (96.0297)  time: 0.0278  data: 0.0002  max mem: 6052
[03:31:12.928259] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.1234 (1.1786)  acc1: 60.9375 (59.8151)  acc5: 96.8750 (96.1048)  time: 0.0278  data: 0.0002  max mem: 6052
[03:31:13.208540] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.1726 (1.1813)  acc1: 57.8125 (59.4715)  acc5: 96.8750 (96.0648)  time: 0.0279  data: 0.0002  max mem: 6052
[03:31:13.489590] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.1726 (1.1831)  acc1: 57.8125 (59.4437)  acc5: 96.8750 (96.1710)  time: 0.0280  data: 0.0002  max mem: 6052
[03:31:13.770054] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.2173 (1.1875)  acc1: 57.8125 (59.3595)  acc5: 96.8750 (96.1943)  time: 0.0280  data: 0.0002  max mem: 6052
[03:31:14.053143] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.2251 (1.1901)  acc1: 56.2500 (59.1639)  acc5: 95.3125 (96.1149)  time: 0.0280  data: 0.0002  max mem: 6052
[03:31:14.341346] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.2008 (1.1865)  acc1: 60.9375 (59.3879)  acc5: 96.8750 (96.1777)  time: 0.0284  data: 0.0002  max mem: 6052
[03:31:14.629173] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.1911 (1.1897)  acc1: 60.9375 (59.1961)  acc5: 96.8750 (96.1116)  time: 0.0287  data: 0.0002  max mem: 6052
[03:31:14.912447] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.1971 (1.1899)  acc1: 59.3750 (59.2753)  acc5: 95.3125 (96.0550)  time: 0.0284  data: 0.0001  max mem: 6052
[03:31:15.190500] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.1666 (1.1869)  acc1: 60.9375 (59.4267)  acc5: 95.3125 (96.0679)  time: 0.0279  data: 0.0001  max mem: 6052
[03:31:15.340135] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.1457 (1.1878)  acc1: 60.9375 (59.3100)  acc5: 95.3125 (96.0700)  time: 0.0268  data: 0.0001  max mem: 6052
[03:31:15.513353] Test: Total time: 0:00:05 (0.0330 s / it)
[03:31:15.513813] * Acc@1 59.310 Acc@5 96.070 loss 1.188
[03:31:15.514107] Accuracy of the network on the 10000 test images: 59.3%
[03:31:15.514286] Max accuracy: 59.31%
[03:31:15.685136] log_dir: ./output_dir
[03:31:16.550319] Epoch: [14]  [  0/781]  eta: 0:11:14  lr: 0.000245  training_loss: 1.8117 (1.8117)  classification_loss: 1.7027 (1.7027)  loss_mask: 0.1090 (0.1090)  time: 0.8636  data: 0.6722  max mem: 6052
[03:31:20.002695] Epoch: [14]  [ 20/781]  eta: 0:02:36  lr: 0.000244  training_loss: 1.8895 (1.8911)  classification_loss: 1.7446 (1.7539)  loss_mask: 0.1227 (0.1372)  time: 0.1725  data: 0.0002  max mem: 6052
[03:31:23.425033] Epoch: [14]  [ 40/781]  eta: 0:02:19  lr: 0.000244  training_loss: 1.8553 (1.8818)  classification_loss: 1.7486 (1.7666)  loss_mask: 0.0870 (0.1152)  time: 0.1710  data: 0.0002  max mem: 6052
[03:31:26.838117] Epoch: [14]  [ 60/781]  eta: 0:02:11  lr: 0.000244  training_loss: 1.8826 (1.8824)  classification_loss: 1.8005 (1.7794)  loss_mask: 0.0743 (0.1030)  time: 0.1706  data: 0.0002  max mem: 6052
[03:31:30.254961] Epoch: [14]  [ 80/781]  eta: 0:02:06  lr: 0.000244  training_loss: 1.8638 (1.8898)  classification_loss: 1.7866 (1.7916)  loss_mask: 0.0538 (0.0981)  time: 0.1708  data: 0.0002  max mem: 6052
[03:31:33.701321] Epoch: [14]  [100/781]  eta: 0:02:01  lr: 0.000244  training_loss: 1.9103 (1.8962)  classification_loss: 1.8332 (1.8013)  loss_mask: 0.0698 (0.0949)  time: 0.1722  data: 0.0003  max mem: 6052
[03:31:37.118478] Epoch: [14]  [120/781]  eta: 0:01:57  lr: 0.000244  training_loss: 1.8302 (1.8839)  classification_loss: 1.7697 (1.7941)  loss_mask: 0.0606 (0.0898)  time: 0.1708  data: 0.0002  max mem: 6052
[03:31:40.527845] Epoch: [14]  [140/781]  eta: 0:01:52  lr: 0.000244  training_loss: 1.7956 (1.8712)  classification_loss: 1.7364 (1.7852)  loss_mask: 0.0433 (0.0860)  time: 0.1704  data: 0.0002  max mem: 6052
[03:31:43.981388] Epoch: [14]  [160/781]  eta: 0:01:49  lr: 0.000244  training_loss: 1.8008 (1.8654)  classification_loss: 1.7440 (1.7818)  loss_mask: 0.0548 (0.0836)  time: 0.1726  data: 0.0001  max mem: 6052
[03:31:47.421056] Epoch: [14]  [180/781]  eta: 0:01:45  lr: 0.000244  training_loss: 1.7793 (1.8573)  classification_loss: 1.7504 (1.7785)  loss_mask: 0.0377 (0.0789)  time: 0.1719  data: 0.0002  max mem: 6052
[03:31:50.855591] Epoch: [14]  [200/781]  eta: 0:01:41  lr: 0.000244  training_loss: 1.7837 (1.8525)  classification_loss: 1.7518 (1.7772)  loss_mask: 0.0398 (0.0753)  time: 0.1717  data: 0.0002  max mem: 6052
[03:31:54.263050] Epoch: [14]  [220/781]  eta: 0:01:37  lr: 0.000244  training_loss: 1.8397 (1.8519)  classification_loss: 1.7623 (1.7760)  loss_mask: 0.0668 (0.0759)  time: 0.1703  data: 0.0002  max mem: 6052
[03:31:57.684739] Epoch: [14]  [240/781]  eta: 0:01:34  lr: 0.000244  training_loss: 1.8767 (1.8543)  classification_loss: 1.7577 (1.7766)  loss_mask: 0.0667 (0.0777)  time: 0.1710  data: 0.0005  max mem: 6052
[03:32:01.099819] Epoch: [14]  [260/781]  eta: 0:01:30  lr: 0.000244  training_loss: 1.9461 (1.8593)  classification_loss: 1.7865 (1.7784)  loss_mask: 0.0967 (0.0809)  time: 0.1707  data: 0.0002  max mem: 6052
[03:32:04.513358] Epoch: [14]  [280/781]  eta: 0:01:27  lr: 0.000244  training_loss: 1.9130 (1.8617)  classification_loss: 1.7650 (1.7791)  loss_mask: 0.0661 (0.0825)  time: 0.1706  data: 0.0002  max mem: 6052
[03:32:07.904020] Epoch: [14]  [300/781]  eta: 0:01:23  lr: 0.000244  training_loss: 1.8518 (1.8615)  classification_loss: 1.7903 (1.7797)  loss_mask: 0.0492 (0.0818)  time: 0.1695  data: 0.0001  max mem: 6052
[03:32:11.306240] Epoch: [14]  [320/781]  eta: 0:01:19  lr: 0.000244  training_loss: 1.9080 (1.8627)  classification_loss: 1.8144 (1.7805)  loss_mask: 0.0729 (0.0822)  time: 0.1700  data: 0.0002  max mem: 6052
[03:32:14.709598] Epoch: [14]  [340/781]  eta: 0:01:16  lr: 0.000244  training_loss: 1.8805 (1.8639)  classification_loss: 1.7969 (1.7809)  loss_mask: 0.0716 (0.0830)  time: 0.1701  data: 0.0003  max mem: 6052
[03:32:18.125788] Epoch: [14]  [360/781]  eta: 0:01:12  lr: 0.000244  training_loss: 1.8355 (1.8645)  classification_loss: 1.7658 (1.7822)  loss_mask: 0.0650 (0.0824)  time: 0.1707  data: 0.0002  max mem: 6052
[03:32:21.531383] Epoch: [14]  [380/781]  eta: 0:01:09  lr: 0.000244  training_loss: 1.8485 (1.8657)  classification_loss: 1.7727 (1.7819)  loss_mask: 0.0835 (0.0839)  time: 0.1702  data: 0.0003  max mem: 6052
[03:32:24.971989] Epoch: [14]  [400/781]  eta: 0:01:05  lr: 0.000244  training_loss: 1.9012 (1.8676)  classification_loss: 1.7789 (1.7824)  loss_mask: 0.0964 (0.0853)  time: 0.1719  data: 0.0002  max mem: 6052
[03:32:28.384400] Epoch: [14]  [420/781]  eta: 0:01:02  lr: 0.000244  training_loss: 1.7916 (1.8655)  classification_loss: 1.7340 (1.7809)  loss_mask: 0.0672 (0.0846)  time: 0.1705  data: 0.0002  max mem: 6052
[03:32:31.795968] Epoch: [14]  [440/781]  eta: 0:00:58  lr: 0.000244  training_loss: 1.7984 (1.8629)  classification_loss: 1.7571 (1.7798)  loss_mask: 0.0453 (0.0831)  time: 0.1705  data: 0.0002  max mem: 6052
[03:32:35.224416] Epoch: [14]  [460/781]  eta: 0:00:55  lr: 0.000244  training_loss: 1.8252 (1.8623)  classification_loss: 1.7702 (1.7798)  loss_mask: 0.0580 (0.0825)  time: 0.1714  data: 0.0006  max mem: 6052
[03:32:38.664229] Epoch: [14]  [480/781]  eta: 0:00:51  lr: 0.000244  training_loss: 1.8472 (1.8625)  classification_loss: 1.8051 (1.7809)  loss_mask: 0.0447 (0.0817)  time: 0.1719  data: 0.0002  max mem: 6052
[03:32:42.076409] Epoch: [14]  [500/781]  eta: 0:00:48  lr: 0.000244  training_loss: 1.8206 (1.8611)  classification_loss: 1.7500 (1.7798)  loss_mask: 0.0653 (0.0813)  time: 0.1705  data: 0.0002  max mem: 6052
[03:32:45.493783] Epoch: [14]  [520/781]  eta: 0:00:44  lr: 0.000244  training_loss: 1.8103 (1.8607)  classification_loss: 1.7356 (1.7786)  loss_mask: 0.0811 (0.0821)  time: 0.1708  data: 0.0002  max mem: 6052
[03:32:48.936218] Epoch: [14]  [540/781]  eta: 0:00:41  lr: 0.000244  training_loss: 2.0037 (1.8686)  classification_loss: 1.7799 (1.7797)  loss_mask: 0.1940 (0.0889)  time: 0.1720  data: 0.0002  max mem: 6052
[03:32:52.368971] Epoch: [14]  [560/781]  eta: 0:00:38  lr: 0.000244  training_loss: 1.8693 (1.8696)  classification_loss: 1.7598 (1.7791)  loss_mask: 0.1207 (0.0905)  time: 0.1716  data: 0.0002  max mem: 6052
[03:32:55.854898] Epoch: [14]  [580/781]  eta: 0:00:34  lr: 0.000244  training_loss: 1.8838 (1.8702)  classification_loss: 1.7783 (1.7799)  loss_mask: 0.0665 (0.0903)  time: 0.1742  data: 0.0003  max mem: 6052
[03:32:59.283010] Epoch: [14]  [600/781]  eta: 0:00:31  lr: 0.000244  training_loss: 1.7797 (1.8677)  classification_loss: 1.7310 (1.7785)  loss_mask: 0.0514 (0.0892)  time: 0.1713  data: 0.0002  max mem: 6052
[03:33:02.701645] Epoch: [14]  [620/781]  eta: 0:00:27  lr: 0.000244  training_loss: 1.8305 (1.8661)  classification_loss: 1.7344 (1.7773)  loss_mask: 0.0638 (0.0888)  time: 0.1709  data: 0.0002  max mem: 6052
[03:33:06.139584] Epoch: [14]  [640/781]  eta: 0:00:24  lr: 0.000243  training_loss: 1.7476 (1.8633)  classification_loss: 1.6851 (1.7755)  loss_mask: 0.0486 (0.0879)  time: 0.1718  data: 0.0002  max mem: 6052
[03:33:09.559355] Epoch: [14]  [660/781]  eta: 0:00:20  lr: 0.000243  training_loss: 1.8038 (1.8622)  classification_loss: 1.7629 (1.7756)  loss_mask: 0.0414 (0.0866)  time: 0.1709  data: 0.0004  max mem: 6052
[03:33:12.994674] Epoch: [14]  [680/781]  eta: 0:00:17  lr: 0.000243  training_loss: 1.8121 (1.8610)  classification_loss: 1.7441 (1.7749)  loss_mask: 0.0632 (0.0861)  time: 0.1717  data: 0.0002  max mem: 6052
[03:33:16.414940] Epoch: [14]  [700/781]  eta: 0:00:13  lr: 0.000243  training_loss: 1.8614 (1.8603)  classification_loss: 1.7337 (1.7742)  loss_mask: 0.0610 (0.0862)  time: 0.1709  data: 0.0002  max mem: 6052
[03:33:19.834241] Epoch: [14]  [720/781]  eta: 0:00:10  lr: 0.000243  training_loss: 1.8492 (1.8607)  classification_loss: 1.8061 (1.7755)  loss_mask: 0.0390 (0.0851)  time: 0.1709  data: 0.0002  max mem: 6052
[03:33:23.286292] Epoch: [14]  [740/781]  eta: 0:00:07  lr: 0.000243  training_loss: 1.8344 (1.8597)  classification_loss: 1.7894 (1.7756)  loss_mask: 0.0408 (0.0841)  time: 0.1725  data: 0.0002  max mem: 6052
[03:33:26.726066] Epoch: [14]  [760/781]  eta: 0:00:03  lr: 0.000243  training_loss: 1.8833 (1.8603)  classification_loss: 1.7838 (1.7756)  loss_mask: 0.0794 (0.0847)  time: 0.1719  data: 0.0002  max mem: 6052
[03:33:30.115048] Epoch: [14]  [780/781]  eta: 0:00:00  lr: 0.000243  training_loss: 1.8080 (1.8593)  classification_loss: 1.7492 (1.7753)  loss_mask: 0.0475 (0.0841)  time: 0.1694  data: 0.0002  max mem: 6052
[03:33:30.304392] Epoch: [14] Total time: 0:02:14 (0.1724 s / it)
[03:33:30.305182] Averaged stats: lr: 0.000243  training_loss: 1.8080 (1.8593)  classification_loss: 1.7492 (1.7753)  loss_mask: 0.0475 (0.0841)
[03:33:30.977007] Test:  [  0/157]  eta: 0:01:44  testing_loss: 1.1147 (1.1147)  acc1: 60.9375 (60.9375)  acc5: 96.8750 (96.8750)  time: 0.6673  data: 0.6290  max mem: 6052
[03:33:31.261897] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.2614 (1.2089)  acc1: 59.3750 (57.6705)  acc5: 96.8750 (97.1591)  time: 0.0864  data: 0.0575  max mem: 6052
[03:33:31.549085] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.1278 (1.1605)  acc1: 60.9375 (60.4911)  acc5: 98.4375 (97.6190)  time: 0.0285  data: 0.0003  max mem: 6052
[03:33:31.836822] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.1325 (1.1667)  acc1: 62.5000 (60.7863)  acc5: 96.8750 (96.9254)  time: 0.0286  data: 0.0002  max mem: 6052
[03:33:32.121669] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.1592 (1.1689)  acc1: 60.9375 (61.0137)  acc5: 96.8750 (96.8750)  time: 0.0285  data: 0.0002  max mem: 6052
[03:33:32.409125] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.1518 (1.1649)  acc1: 62.5000 (61.4890)  acc5: 96.8750 (96.8444)  time: 0.0285  data: 0.0002  max mem: 6052
[03:33:32.696732] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.1456 (1.1584)  acc1: 62.5000 (61.7316)  acc5: 96.8750 (96.7725)  time: 0.0286  data: 0.0002  max mem: 6052
[03:33:32.981323] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.1086 (1.1542)  acc1: 62.5000 (61.9278)  acc5: 96.8750 (96.6769)  time: 0.0285  data: 0.0001  max mem: 6052
[03:33:33.265346] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.1282 (1.1559)  acc1: 60.9375 (61.8827)  acc5: 95.3125 (96.5278)  time: 0.0283  data: 0.0001  max mem: 6052
[03:33:33.552389] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.1660 (1.1567)  acc1: 59.3750 (61.7788)  acc5: 95.3125 (96.5488)  time: 0.0284  data: 0.0002  max mem: 6052
[03:33:33.835572] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.1800 (1.1596)  acc1: 59.3750 (61.6801)  acc5: 96.8750 (96.5656)  time: 0.0284  data: 0.0002  max mem: 6052
[03:33:34.114249] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.1857 (1.1616)  acc1: 60.9375 (61.5428)  acc5: 95.3125 (96.4809)  time: 0.0280  data: 0.0001  max mem: 6052
[03:33:34.393475] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.1561 (1.1577)  acc1: 60.9375 (61.7639)  acc5: 96.8750 (96.4876)  time: 0.0278  data: 0.0001  max mem: 6052
[03:33:34.673009] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.1560 (1.1599)  acc1: 62.5000 (61.5458)  acc5: 96.8750 (96.4814)  time: 0.0278  data: 0.0001  max mem: 6052
[03:33:34.957826] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.1716 (1.1587)  acc1: 62.5000 (61.7132)  acc5: 95.3125 (96.4539)  time: 0.0281  data: 0.0001  max mem: 6052
[03:33:35.235472] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.1194 (1.1556)  acc1: 64.0625 (61.7964)  acc5: 96.8750 (96.5232)  time: 0.0280  data: 0.0001  max mem: 6052
[03:33:35.385335] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.1132 (1.1579)  acc1: 64.0625 (61.7600)  acc5: 96.8750 (96.5600)  time: 0.0268  data: 0.0001  max mem: 6052
[03:33:35.538563] Test: Total time: 0:00:05 (0.0333 s / it)
[03:33:35.539077] * Acc@1 61.760 Acc@5 96.560 loss 1.158
[03:33:35.539381] Accuracy of the network on the 10000 test images: 61.8%
[03:33:35.539567] Max accuracy: 61.76%
[03:33:35.710273] log_dir: ./output_dir
[03:33:36.572717] Epoch: [15]  [  0/781]  eta: 0:11:12  lr: 0.000243  training_loss: 1.8740 (1.8740)  classification_loss: 1.8400 (1.8400)  loss_mask: 0.0340 (0.0340)  time: 0.8609  data: 0.6798  max mem: 6052
[03:33:40.020437] Epoch: [15]  [ 20/781]  eta: 0:02:36  lr: 0.000243  training_loss: 1.8220 (1.8322)  classification_loss: 1.7595 (1.7562)  loss_mask: 0.0627 (0.0760)  time: 0.1723  data: 0.0002  max mem: 6052
[03:33:43.422893] Epoch: [15]  [ 40/781]  eta: 0:02:19  lr: 0.000243  training_loss: 1.8246 (1.8280)  classification_loss: 1.7595 (1.7615)  loss_mask: 0.0466 (0.0665)  time: 0.1701  data: 0.0002  max mem: 6052
[03:33:46.872046] Epoch: [15]  [ 60/781]  eta: 0:02:11  lr: 0.000243  training_loss: 1.8645 (1.8499)  classification_loss: 1.7861 (1.7701)  loss_mask: 0.0794 (0.0798)  time: 0.1724  data: 0.0002  max mem: 6052
[03:33:50.308319] Epoch: [15]  [ 80/781]  eta: 0:02:06  lr: 0.000243  training_loss: 1.8304 (1.8560)  classification_loss: 1.7738 (1.7731)  loss_mask: 0.0867 (0.0829)  time: 0.1717  data: 0.0004  max mem: 6052
[03:33:53.804395] Epoch: [15]  [100/781]  eta: 0:02:01  lr: 0.000243  training_loss: 1.8289 (1.8544)  classification_loss: 1.7643 (1.7730)  loss_mask: 0.0562 (0.0814)  time: 0.1747  data: 0.0002  max mem: 6052
[03:33:57.221858] Epoch: [15]  [120/781]  eta: 0:01:57  lr: 0.000243  training_loss: 2.0167 (1.8796)  classification_loss: 1.8031 (1.7811)  loss_mask: 0.1355 (0.0985)  time: 0.1708  data: 0.0003  max mem: 6052
[03:34:00.648180] Epoch: [15]  [140/781]  eta: 0:01:53  lr: 0.000243  training_loss: 1.9186 (1.8874)  classification_loss: 1.8172 (1.7882)  loss_mask: 0.0972 (0.0992)  time: 0.1712  data: 0.0003  max mem: 6052
[03:34:04.070860] Epoch: [15]  [160/781]  eta: 0:01:49  lr: 0.000243  training_loss: 1.9671 (1.8974)  classification_loss: 1.8099 (1.7932)  loss_mask: 0.1264 (0.1042)  time: 0.1710  data: 0.0002  max mem: 6052
[03:34:07.492383] Epoch: [15]  [180/781]  eta: 0:01:45  lr: 0.000243  training_loss: 1.8678 (1.8948)  classification_loss: 1.7849 (1.7921)  loss_mask: 0.0903 (0.1027)  time: 0.1709  data: 0.0002  max mem: 6052
[03:34:10.926058] Epoch: [15]  [200/781]  eta: 0:01:41  lr: 0.000243  training_loss: 1.8251 (1.8894)  classification_loss: 1.7793 (1.7908)  loss_mask: 0.0562 (0.0986)  time: 0.1716  data: 0.0002  max mem: 6052
[03:34:14.389624] Epoch: [15]  [220/781]  eta: 0:01:38  lr: 0.000243  training_loss: 1.8404 (1.8851)  classification_loss: 1.8015 (1.7905)  loss_mask: 0.0455 (0.0946)  time: 0.1731  data: 0.0002  max mem: 6052
[03:34:17.807756] Epoch: [15]  [240/781]  eta: 0:01:34  lr: 0.000243  training_loss: 1.8300 (1.8838)  classification_loss: 1.7729 (1.7914)  loss_mask: 0.0522 (0.0925)  time: 0.1708  data: 0.0003  max mem: 6052
[03:34:21.243355] Epoch: [15]  [260/781]  eta: 0:01:30  lr: 0.000243  training_loss: 1.8186 (1.8806)  classification_loss: 1.7215 (1.7879)  loss_mask: 0.0796 (0.0927)  time: 0.1717  data: 0.0003  max mem: 6052
[03:34:24.666156] Epoch: [15]  [280/781]  eta: 0:01:27  lr: 0.000243  training_loss: 1.8558 (1.8791)  classification_loss: 1.8229 (1.7880)  loss_mask: 0.0643 (0.0911)  time: 0.1711  data: 0.0003  max mem: 6052
[03:34:28.091432] Epoch: [15]  [300/781]  eta: 0:01:23  lr: 0.000243  training_loss: 1.8397 (1.8768)  classification_loss: 1.7365 (1.7858)  loss_mask: 0.0642 (0.0909)  time: 0.1712  data: 0.0003  max mem: 6052
[03:34:31.519650] Epoch: [15]  [320/781]  eta: 0:01:20  lr: 0.000243  training_loss: 1.8450 (1.8750)  classification_loss: 1.8046 (1.7864)  loss_mask: 0.0453 (0.0886)  time: 0.1713  data: 0.0002  max mem: 6052
[03:34:34.936931] Epoch: [15]  [340/781]  eta: 0:01:16  lr: 0.000243  training_loss: 1.7901 (1.8711)  classification_loss: 1.7483 (1.7844)  loss_mask: 0.0453 (0.0867)  time: 0.1708  data: 0.0003  max mem: 6052
[03:34:38.369737] Epoch: [15]  [360/781]  eta: 0:01:13  lr: 0.000243  training_loss: 1.8273 (1.8710)  classification_loss: 1.7722 (1.7847)  loss_mask: 0.0502 (0.0863)  time: 0.1716  data: 0.0002  max mem: 6052
[03:34:41.777541] Epoch: [15]  [380/781]  eta: 0:01:09  lr: 0.000243  training_loss: 1.8105 (1.8684)  classification_loss: 1.7474 (1.7835)  loss_mask: 0.0585 (0.0849)  time: 0.1703  data: 0.0002  max mem: 6052
[03:34:45.186334] Epoch: [15]  [400/781]  eta: 0:01:05  lr: 0.000243  training_loss: 1.8213 (1.8665)  classification_loss: 1.7720 (1.7838)  loss_mask: 0.0360 (0.0827)  time: 0.1704  data: 0.0002  max mem: 6052
[03:34:48.602046] Epoch: [15]  [420/781]  eta: 0:01:02  lr: 0.000243  training_loss: 1.7829 (1.8622)  classification_loss: 1.7479 (1.7815)  loss_mask: 0.0293 (0.0808)  time: 0.1707  data: 0.0002  max mem: 6052
[03:34:52.026530] Epoch: [15]  [440/781]  eta: 0:00:58  lr: 0.000242  training_loss: 1.7896 (1.8608)  classification_loss: 1.7570 (1.7808)  loss_mask: 0.0600 (0.0800)  time: 0.1711  data: 0.0004  max mem: 6052
[03:34:55.439477] Epoch: [15]  [460/781]  eta: 0:00:55  lr: 0.000242  training_loss: 1.7619 (1.8575)  classification_loss: 1.6882 (1.7783)  loss_mask: 0.0461 (0.0793)  time: 0.1706  data: 0.0002  max mem: 6052
[03:34:58.861148] Epoch: [15]  [480/781]  eta: 0:00:52  lr: 0.000242  training_loss: 1.8282 (1.8565)  classification_loss: 1.7845 (1.7776)  loss_mask: 0.0635 (0.0789)  time: 0.1710  data: 0.0002  max mem: 6052
[03:35:02.297219] Epoch: [15]  [500/781]  eta: 0:00:48  lr: 0.000242  training_loss: 1.8169 (1.8547)  classification_loss: 1.7472 (1.7767)  loss_mask: 0.0468 (0.0780)  time: 0.1717  data: 0.0002  max mem: 6052
[03:35:05.714307] Epoch: [15]  [520/781]  eta: 0:00:45  lr: 0.000242  training_loss: 1.7844 (1.8532)  classification_loss: 1.7439 (1.7757)  loss_mask: 0.0417 (0.0775)  time: 0.1708  data: 0.0002  max mem: 6052
[03:35:09.158988] Epoch: [15]  [540/781]  eta: 0:00:41  lr: 0.000242  training_loss: 1.8680 (1.8531)  classification_loss: 1.7929 (1.7763)  loss_mask: 0.0523 (0.0767)  time: 0.1721  data: 0.0003  max mem: 6052
[03:35:12.644914] Epoch: [15]  [560/781]  eta: 0:00:38  lr: 0.000242  training_loss: 1.7575 (1.8519)  classification_loss: 1.7250 (1.7764)  loss_mask: 0.0335 (0.0756)  time: 0.1742  data: 0.0002  max mem: 6052
[03:35:16.095945] Epoch: [15]  [580/781]  eta: 0:00:34  lr: 0.000242  training_loss: 1.7957 (1.8498)  classification_loss: 1.7185 (1.7751)  loss_mask: 0.0499 (0.0747)  time: 0.1724  data: 0.0003  max mem: 6052
[03:35:19.527424] Epoch: [15]  [600/781]  eta: 0:00:31  lr: 0.000242  training_loss: 1.7949 (1.8484)  classification_loss: 1.7226 (1.7732)  loss_mask: 0.0468 (0.0752)  time: 0.1715  data: 0.0003  max mem: 6052
[03:35:22.940533] Epoch: [15]  [620/781]  eta: 0:00:27  lr: 0.000242  training_loss: 1.8004 (1.8469)  classification_loss: 1.7461 (1.7725)  loss_mask: 0.0471 (0.0743)  time: 0.1706  data: 0.0002  max mem: 6052
[03:35:26.345715] Epoch: [15]  [640/781]  eta: 0:00:24  lr: 0.000242  training_loss: 1.8179 (1.8467)  classification_loss: 1.7803 (1.7719)  loss_mask: 0.0455 (0.0748)  time: 0.1702  data: 0.0002  max mem: 6052
[03:35:29.776318] Epoch: [15]  [660/781]  eta: 0:00:20  lr: 0.000242  training_loss: 1.8083 (1.8456)  classification_loss: 1.7468 (1.7709)  loss_mask: 0.0515 (0.0747)  time: 0.1715  data: 0.0002  max mem: 6052
[03:35:33.198810] Epoch: [15]  [680/781]  eta: 0:00:17  lr: 0.000242  training_loss: 1.7960 (1.8447)  classification_loss: 1.7162 (1.7701)  loss_mask: 0.0658 (0.0746)  time: 0.1710  data: 0.0002  max mem: 6052
[03:35:36.629525] Epoch: [15]  [700/781]  eta: 0:00:13  lr: 0.000242  training_loss: 1.8191 (1.8447)  classification_loss: 1.7310 (1.7695)  loss_mask: 0.0951 (0.0753)  time: 0.1715  data: 0.0002  max mem: 6052
[03:35:40.077847] Epoch: [15]  [720/781]  eta: 0:00:10  lr: 0.000242  training_loss: 1.8821 (1.8461)  classification_loss: 1.7416 (1.7692)  loss_mask: 0.0909 (0.0768)  time: 0.1723  data: 0.0002  max mem: 6052
[03:35:43.501886] Epoch: [15]  [740/781]  eta: 0:00:07  lr: 0.000242  training_loss: 1.7824 (1.8447)  classification_loss: 1.7124 (1.7682)  loss_mask: 0.0508 (0.0764)  time: 0.1711  data: 0.0002  max mem: 6052
[03:35:46.914367] Epoch: [15]  [760/781]  eta: 0:00:03  lr: 0.000242  training_loss: 1.8324 (1.8442)  classification_loss: 1.8006 (1.7684)  loss_mask: 0.0460 (0.0759)  time: 0.1705  data: 0.0002  max mem: 6052
[03:35:50.305407] Epoch: [15]  [780/781]  eta: 0:00:00  lr: 0.000242  training_loss: 1.8346 (1.8443)  classification_loss: 1.7766 (1.7689)  loss_mask: 0.0483 (0.0754)  time: 0.1695  data: 0.0001  max mem: 6052
[03:35:50.472374] Epoch: [15] Total time: 0:02:14 (0.1726 s / it)
[03:35:50.473134] Averaged stats: lr: 0.000242  training_loss: 1.8346 (1.8443)  classification_loss: 1.7766 (1.7689)  loss_mask: 0.0483 (0.0754)
[03:35:51.114035] Test:  [  0/157]  eta: 0:01:38  testing_loss: 1.0021 (1.0021)  acc1: 65.6250 (65.6250)  acc5: 95.3125 (95.3125)  time: 0.6306  data: 0.6010  max mem: 6052
[03:35:51.410904] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.1756 (1.1667)  acc1: 59.3750 (58.5227)  acc5: 96.8750 (96.5909)  time: 0.0840  data: 0.0552  max mem: 6052
[03:35:51.702737] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.1070 (1.1316)  acc1: 60.9375 (61.1607)  acc5: 96.8750 (97.0982)  time: 0.0292  data: 0.0004  max mem: 6052
[03:35:51.992519] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.1070 (1.1330)  acc1: 64.0625 (61.7944)  acc5: 96.8750 (96.7238)  time: 0.0289  data: 0.0002  max mem: 6052
[03:35:52.275575] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.1152 (1.1342)  acc1: 62.5000 (62.2713)  acc5: 95.3125 (96.4939)  time: 0.0285  data: 0.0002  max mem: 6052
[03:35:52.561191] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.1275 (1.1340)  acc1: 62.5000 (62.1017)  acc5: 95.3125 (96.3848)  time: 0.0283  data: 0.0001  max mem: 6052
[03:35:52.844093] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.1414 (1.1325)  acc1: 60.9375 (61.9621)  acc5: 95.3125 (96.3371)  time: 0.0283  data: 0.0002  max mem: 6052
[03:35:53.129161] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.1185 (1.1280)  acc1: 62.5000 (62.2139)  acc5: 96.8750 (96.4129)  time: 0.0283  data: 0.0002  max mem: 6052
[03:35:53.412051] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.1237 (1.1313)  acc1: 62.5000 (62.2299)  acc5: 96.8750 (96.4120)  time: 0.0283  data: 0.0002  max mem: 6052
[03:35:53.699334] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.1547 (1.1307)  acc1: 60.9375 (62.2424)  acc5: 96.8750 (96.4973)  time: 0.0284  data: 0.0003  max mem: 6052
[03:35:53.992027] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.1553 (1.1342)  acc1: 59.3750 (62.0050)  acc5: 96.8750 (96.5192)  time: 0.0289  data: 0.0004  max mem: 6052
[03:35:54.277208] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.1736 (1.1378)  acc1: 59.3750 (61.7539)  acc5: 96.8750 (96.5090)  time: 0.0288  data: 0.0002  max mem: 6052
[03:35:54.561675] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.1366 (1.1335)  acc1: 64.0625 (62.0480)  acc5: 96.8750 (96.5522)  time: 0.0284  data: 0.0001  max mem: 6052
[03:35:54.849418] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.1366 (1.1359)  acc1: 64.0625 (62.0229)  acc5: 96.8750 (96.5530)  time: 0.0285  data: 0.0002  max mem: 6052
[03:35:55.132350] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.1404 (1.1366)  acc1: 60.9375 (61.9681)  acc5: 96.8750 (96.5315)  time: 0.0284  data: 0.0002  max mem: 6052
[03:35:55.412087] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.0983 (1.1330)  acc1: 62.5000 (62.1378)  acc5: 96.8750 (96.6060)  time: 0.0280  data: 0.0001  max mem: 6052
[03:35:55.561312] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.0871 (1.1343)  acc1: 64.0625 (62.0200)  acc5: 96.8750 (96.6200)  time: 0.0269  data: 0.0001  max mem: 6052
[03:35:55.719366] Test: Total time: 0:00:05 (0.0334 s / it)
[03:35:55.719940] * Acc@1 62.020 Acc@5 96.620 loss 1.134
[03:35:55.720245] Accuracy of the network on the 10000 test images: 62.0%
[03:35:55.720422] Max accuracy: 62.02%
[03:35:55.903227] log_dir: ./output_dir
[03:35:56.745713] Epoch: [16]  [  0/781]  eta: 0:10:56  lr: 0.000242  training_loss: 1.8719 (1.8719)  classification_loss: 1.7496 (1.7496)  loss_mask: 0.1223 (0.1223)  time: 0.8409  data: 0.6542  max mem: 6052
[03:36:00.202681] Epoch: [16]  [ 20/781]  eta: 0:02:35  lr: 0.000242  training_loss: 1.7962 (1.8131)  classification_loss: 1.7192 (1.7528)  loss_mask: 0.0522 (0.0603)  time: 0.1728  data: 0.0002  max mem: 6052
[03:36:03.648026] Epoch: [16]  [ 40/781]  eta: 0:02:19  lr: 0.000242  training_loss: 1.8702 (1.8549)  classification_loss: 1.7628 (1.7477)  loss_mask: 0.0844 (0.1072)  time: 0.1722  data: 0.0003  max mem: 6052
[03:36:07.075882] Epoch: [16]  [ 60/781]  eta: 0:02:11  lr: 0.000242  training_loss: 1.9238 (1.8845)  classification_loss: 1.7228 (1.7480)  loss_mask: 0.2163 (0.1365)  time: 0.1713  data: 0.0002  max mem: 6052
[03:36:10.494576] Epoch: [16]  [ 80/781]  eta: 0:02:06  lr: 0.000242  training_loss: 1.8762 (1.8775)  classification_loss: 1.7655 (1.7494)  loss_mask: 0.1107 (0.1281)  time: 0.1709  data: 0.0002  max mem: 6052
[03:36:14.028853] Epoch: [16]  [100/781]  eta: 0:02:02  lr: 0.000242  training_loss: 1.8142 (1.8675)  classification_loss: 1.7384 (1.7489)  loss_mask: 0.0735 (0.1186)  time: 0.1766  data: 0.0003  max mem: 6052
[03:36:17.449796] Epoch: [16]  [120/781]  eta: 0:01:57  lr: 0.000242  training_loss: 1.8358 (1.8658)  classification_loss: 1.7621 (1.7520)  loss_mask: 0.0747 (0.1138)  time: 0.1710  data: 0.0002  max mem: 6052
[03:36:20.855069] Epoch: [16]  [140/781]  eta: 0:01:53  lr: 0.000242  training_loss: 1.7676 (1.8565)  classification_loss: 1.7053 (1.7451)  loss_mask: 0.0835 (0.1114)  time: 0.1702  data: 0.0003  max mem: 6052
[03:36:24.276037] Epoch: [16]  [160/781]  eta: 0:01:49  lr: 0.000242  training_loss: 1.8427 (1.8552)  classification_loss: 1.7536 (1.7480)  loss_mask: 0.0679 (0.1072)  time: 0.1710  data: 0.0003  max mem: 6052
[03:36:27.693904] Epoch: [16]  [180/781]  eta: 0:01:45  lr: 0.000242  training_loss: 1.7825 (1.8457)  classification_loss: 1.7195 (1.7458)  loss_mask: 0.0391 (0.0999)  time: 0.1708  data: 0.0003  max mem: 6052
[03:36:31.104297] Epoch: [16]  [200/781]  eta: 0:01:41  lr: 0.000241  training_loss: 1.7710 (1.8407)  classification_loss: 1.7302 (1.7467)  loss_mask: 0.0342 (0.0940)  time: 0.1705  data: 0.0003  max mem: 6052
[03:36:34.523698] Epoch: [16]  [220/781]  eta: 0:01:37  lr: 0.000241  training_loss: 1.7804 (1.8355)  classification_loss: 1.7327 (1.7459)  loss_mask: 0.0442 (0.0896)  time: 0.1709  data: 0.0002  max mem: 6052
[03:36:37.933738] Epoch: [16]  [240/781]  eta: 0:01:34  lr: 0.000241  training_loss: 1.7702 (1.8304)  classification_loss: 1.7347 (1.7439)  loss_mask: 0.0425 (0.0864)  time: 0.1704  data: 0.0002  max mem: 6052
[03:36:41.359693] Epoch: [16]  [260/781]  eta: 0:01:30  lr: 0.000241  training_loss: 1.8926 (1.8350)  classification_loss: 1.7518 (1.7447)  loss_mask: 0.1243 (0.0904)  time: 0.1712  data: 0.0002  max mem: 6052
[03:36:44.820336] Epoch: [16]  [280/781]  eta: 0:01:27  lr: 0.000241  training_loss: 1.8047 (1.8353)  classification_loss: 1.7242 (1.7450)  loss_mask: 0.0698 (0.0904)  time: 0.1729  data: 0.0002  max mem: 6052
[03:36:48.248998] Epoch: [16]  [300/781]  eta: 0:01:23  lr: 0.000241  training_loss: 1.7765 (1.8331)  classification_loss: 1.7331 (1.7457)  loss_mask: 0.0402 (0.0875)  time: 0.1714  data: 0.0002  max mem: 6052
[03:36:51.712452] Epoch: [16]  [320/781]  eta: 0:01:20  lr: 0.000241  training_loss: 1.7799 (1.8306)  classification_loss: 1.7280 (1.7451)  loss_mask: 0.0448 (0.0854)  time: 0.1731  data: 0.0002  max mem: 6052
[03:36:55.153339] Epoch: [16]  [340/781]  eta: 0:01:16  lr: 0.000241  training_loss: 1.7187 (1.8240)  classification_loss: 1.6547 (1.7403)  loss_mask: 0.0423 (0.0837)  time: 0.1720  data: 0.0002  max mem: 6052
[03:36:58.606913] Epoch: [16]  [360/781]  eta: 0:01:13  lr: 0.000241  training_loss: 1.8281 (1.8243)  classification_loss: 1.7301 (1.7408)  loss_mask: 0.0680 (0.0835)  time: 0.1726  data: 0.0003  max mem: 6052
[03:37:02.024637] Epoch: [16]  [380/781]  eta: 0:01:09  lr: 0.000241  training_loss: 1.7775 (1.8222)  classification_loss: 1.7105 (1.7405)  loss_mask: 0.0429 (0.0817)  time: 0.1708  data: 0.0002  max mem: 6052
[03:37:05.448274] Epoch: [16]  [400/781]  eta: 0:01:06  lr: 0.000241  training_loss: 1.8387 (1.8223)  classification_loss: 1.7880 (1.7410)  loss_mask: 0.0600 (0.0813)  time: 0.1711  data: 0.0002  max mem: 6052
[03:37:08.868536] Epoch: [16]  [420/781]  eta: 0:01:02  lr: 0.000241  training_loss: 1.8638 (1.8248)  classification_loss: 1.7654 (1.7432)  loss_mask: 0.0703 (0.0816)  time: 0.1709  data: 0.0002  max mem: 6052
[03:37:12.304418] Epoch: [16]  [440/781]  eta: 0:00:59  lr: 0.000241  training_loss: 1.8383 (1.8250)  classification_loss: 1.7600 (1.7434)  loss_mask: 0.0506 (0.0816)  time: 0.1717  data: 0.0003  max mem: 6052
[03:37:15.735272] Epoch: [16]  [460/781]  eta: 0:00:55  lr: 0.000241  training_loss: 1.8236 (1.8256)  classification_loss: 1.6757 (1.7413)  loss_mask: 0.0784 (0.0843)  time: 0.1713  data: 0.0002  max mem: 6052
[03:37:19.164062] Epoch: [16]  [480/781]  eta: 0:00:52  lr: 0.000241  training_loss: 1.7952 (1.8244)  classification_loss: 1.7443 (1.7414)  loss_mask: 0.0501 (0.0830)  time: 0.1714  data: 0.0002  max mem: 6052
[03:37:22.573163] Epoch: [16]  [500/781]  eta: 0:00:48  lr: 0.000241  training_loss: 1.7348 (1.8217)  classification_loss: 1.6937 (1.7403)  loss_mask: 0.0383 (0.0814)  time: 0.1704  data: 0.0003  max mem: 6052
[03:37:25.986511] Epoch: [16]  [520/781]  eta: 0:00:45  lr: 0.000241  training_loss: 1.7608 (1.8208)  classification_loss: 1.6989 (1.7396)  loss_mask: 0.0729 (0.0812)  time: 0.1706  data: 0.0002  max mem: 6052
[03:37:29.399495] Epoch: [16]  [540/781]  eta: 0:00:41  lr: 0.000241  training_loss: 1.7917 (1.8196)  classification_loss: 1.7262 (1.7392)  loss_mask: 0.0573 (0.0804)  time: 0.1706  data: 0.0003  max mem: 6052
[03:37:32.813669] Epoch: [16]  [560/781]  eta: 0:00:38  lr: 0.000241  training_loss: 1.7692 (1.8189)  classification_loss: 1.7261 (1.7394)  loss_mask: 0.0619 (0.0795)  time: 0.1706  data: 0.0004  max mem: 6052
[03:37:36.315709] Epoch: [16]  [580/781]  eta: 0:00:34  lr: 0.000241  training_loss: 1.8021 (1.8187)  classification_loss: 1.7478 (1.7403)  loss_mask: 0.0415 (0.0784)  time: 0.1750  data: 0.0002  max mem: 6052
[03:37:39.735145] Epoch: [16]  [600/781]  eta: 0:00:31  lr: 0.000241  training_loss: 1.7870 (1.8177)  classification_loss: 1.7408 (1.7401)  loss_mask: 0.0384 (0.0775)  time: 0.1709  data: 0.0002  max mem: 6052
[03:37:43.150227] Epoch: [16]  [620/781]  eta: 0:00:27  lr: 0.000241  training_loss: 1.7611 (1.8162)  classification_loss: 1.6874 (1.7393)  loss_mask: 0.0391 (0.0768)  time: 0.1707  data: 0.0002  max mem: 6052
[03:37:46.564556] Epoch: [16]  [640/781]  eta: 0:00:24  lr: 0.000241  training_loss: 1.8117 (1.8153)  classification_loss: 1.7637 (1.7396)  loss_mask: 0.0403 (0.0757)  time: 0.1706  data: 0.0003  max mem: 6052
[03:37:50.023857] Epoch: [16]  [660/781]  eta: 0:00:20  lr: 0.000241  training_loss: 1.8318 (1.8160)  classification_loss: 1.7577 (1.7397)  loss_mask: 0.0741 (0.0763)  time: 0.1729  data: 0.0003  max mem: 6052
[03:37:53.451803] Epoch: [16]  [680/781]  eta: 0:00:17  lr: 0.000241  training_loss: 1.7588 (1.8141)  classification_loss: 1.7120 (1.7386)  loss_mask: 0.0373 (0.0754)  time: 0.1713  data: 0.0003  max mem: 6052
[03:37:56.860464] Epoch: [16]  [700/781]  eta: 0:00:13  lr: 0.000240  training_loss: 1.7623 (1.8129)  classification_loss: 1.6980 (1.7379)  loss_mask: 0.0531 (0.0750)  time: 0.1704  data: 0.0002  max mem: 6052
[03:38:00.314796] Epoch: [16]  [720/781]  eta: 0:00:10  lr: 0.000240  training_loss: 1.7391 (1.8115)  classification_loss: 1.6968 (1.7373)  loss_mask: 0.0411 (0.0742)  time: 0.1726  data: 0.0003  max mem: 6052
[03:38:03.857941] Epoch: [16]  [740/781]  eta: 0:00:07  lr: 0.000240  training_loss: 1.7687 (1.8098)  classification_loss: 1.6948 (1.7361)  loss_mask: 0.0458 (0.0737)  time: 0.1771  data: 0.0003  max mem: 6052
[03:38:07.268861] Epoch: [16]  [760/781]  eta: 0:00:03  lr: 0.000240  training_loss: 1.7930 (1.8098)  classification_loss: 1.7490 (1.7363)  loss_mask: 0.0481 (0.0735)  time: 0.1705  data: 0.0002  max mem: 6052
[03:38:10.676707] Epoch: [16]  [780/781]  eta: 0:00:00  lr: 0.000240  training_loss: 1.8345 (1.8106)  classification_loss: 1.7154 (1.7367)  loss_mask: 0.0652 (0.0740)  time: 0.1703  data: 0.0002  max mem: 6052
[03:38:10.842742] Epoch: [16] Total time: 0:02:14 (0.1728 s / it)
[03:38:10.843186] Averaged stats: lr: 0.000240  training_loss: 1.8345 (1.8106)  classification_loss: 1.7154 (1.7367)  loss_mask: 0.0652 (0.0740)
[03:38:11.509504] Test:  [  0/157]  eta: 0:01:43  testing_loss: 0.9828 (0.9828)  acc1: 65.6250 (65.6250)  acc5: 96.8750 (96.8750)  time: 0.6615  data: 0.6106  max mem: 6052
[03:38:11.792276] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.1689 (1.1309)  acc1: 59.3750 (59.2330)  acc5: 96.8750 (96.7330)  time: 0.0856  data: 0.0557  max mem: 6052
[03:38:12.079620] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.0729 (1.0974)  acc1: 59.3750 (61.2351)  acc5: 98.4375 (97.7679)  time: 0.0283  data: 0.0003  max mem: 6052
[03:38:12.380006] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.1059 (1.1155)  acc1: 62.5000 (61.4415)  acc5: 96.8750 (96.9758)  time: 0.0292  data: 0.0003  max mem: 6052
[03:38:12.662888] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.1155 (1.1171)  acc1: 62.5000 (61.5473)  acc5: 95.3125 (96.7988)  time: 0.0290  data: 0.0003  max mem: 6052
[03:38:12.945182] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.1107 (1.1164)  acc1: 64.0625 (62.0404)  acc5: 95.3125 (96.5993)  time: 0.0281  data: 0.0003  max mem: 6052
[03:38:13.228574] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.1247 (1.1151)  acc1: 62.5000 (62.1158)  acc5: 95.3125 (96.5164)  time: 0.0282  data: 0.0002  max mem: 6052
[03:38:13.510022] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.0816 (1.1075)  acc1: 62.5000 (62.2359)  acc5: 96.8750 (96.7650)  time: 0.0281  data: 0.0002  max mem: 6052
[03:38:13.790064] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.0816 (1.1096)  acc1: 60.9375 (62.1335)  acc5: 96.8750 (96.5856)  time: 0.0279  data: 0.0001  max mem: 6052
[03:38:14.072950] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.1059 (1.1094)  acc1: 60.9375 (62.1566)  acc5: 96.8750 (96.6518)  time: 0.0280  data: 0.0002  max mem: 6052
[03:38:14.355875] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.1432 (1.1123)  acc1: 60.9375 (62.0050)  acc5: 96.8750 (96.6739)  time: 0.0281  data: 0.0002  max mem: 6052
[03:38:14.635530] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.1759 (1.1150)  acc1: 57.8125 (61.7117)  acc5: 95.3125 (96.6075)  time: 0.0280  data: 0.0002  max mem: 6052
[03:38:14.920896] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.1247 (1.1107)  acc1: 60.9375 (61.9576)  acc5: 96.8750 (96.6684)  time: 0.0280  data: 0.0002  max mem: 6052
[03:38:15.209072] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.1247 (1.1134)  acc1: 60.9375 (61.7247)  acc5: 96.8750 (96.5649)  time: 0.0284  data: 0.0002  max mem: 6052
[03:38:15.488422] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.1356 (1.1134)  acc1: 59.3750 (61.8351)  acc5: 96.8750 (96.5758)  time: 0.0282  data: 0.0002  max mem: 6052
[03:38:15.765828] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.0686 (1.1093)  acc1: 64.0625 (62.1068)  acc5: 96.8750 (96.6060)  time: 0.0277  data: 0.0001  max mem: 6052
[03:38:15.915928] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.0575 (1.1112)  acc1: 65.6250 (62.0600)  acc5: 96.8750 (96.6300)  time: 0.0267  data: 0.0001  max mem: 6052
[03:38:16.088578] Test: Total time: 0:00:05 (0.0334 s / it)
[03:38:16.089865] * Acc@1 62.060 Acc@5 96.630 loss 1.111
[03:38:16.090577] Accuracy of the network on the 10000 test images: 62.1%
[03:38:16.091253] Max accuracy: 62.06%
[03:38:16.334996] log_dir: ./output_dir
[03:38:17.215519] Epoch: [17]  [  0/781]  eta: 0:11:25  lr: 0.000240  training_loss: 1.7079 (1.7079)  classification_loss: 1.6130 (1.6130)  loss_mask: 0.0950 (0.0950)  time: 0.8781  data: 0.6866  max mem: 6052
[03:38:20.653964] Epoch: [17]  [ 20/781]  eta: 0:02:36  lr: 0.000240  training_loss: 1.7881 (1.7907)  classification_loss: 1.7218 (1.7177)  loss_mask: 0.0731 (0.0731)  time: 0.1718  data: 0.0002  max mem: 6052
[03:38:24.065076] Epoch: [17]  [ 40/781]  eta: 0:02:19  lr: 0.000240  training_loss: 1.8315 (1.8116)  classification_loss: 1.7493 (1.7330)  loss_mask: 0.0561 (0.0786)  time: 0.1705  data: 0.0002  max mem: 6052
[03:38:27.487178] Epoch: [17]  [ 60/781]  eta: 0:02:11  lr: 0.000240  training_loss: 1.8018 (1.8145)  classification_loss: 1.7268 (1.7348)  loss_mask: 0.0531 (0.0797)  time: 0.1710  data: 0.0002  max mem: 6052
[03:38:30.939510] Epoch: [17]  [ 80/781]  eta: 0:02:06  lr: 0.000240  training_loss: 1.7803 (1.8040)  classification_loss: 1.7339 (1.7344)  loss_mask: 0.0339 (0.0696)  time: 0.1725  data: 0.0002  max mem: 6052
[03:38:34.377490] Epoch: [17]  [100/781]  eta: 0:02:01  lr: 0.000240  training_loss: 1.7583 (1.7940)  classification_loss: 1.7032 (1.7308)  loss_mask: 0.0247 (0.0631)  time: 0.1718  data: 0.0002  max mem: 6052
[03:38:37.848335] Epoch: [17]  [120/781]  eta: 0:01:57  lr: 0.000240  training_loss: 1.7424 (1.7883)  classification_loss: 1.7015 (1.7269)  loss_mask: 0.0420 (0.0615)  time: 0.1735  data: 0.0003  max mem: 6052
[03:38:41.289769] Epoch: [17]  [140/781]  eta: 0:01:53  lr: 0.000240  training_loss: 1.7353 (1.7822)  classification_loss: 1.6653 (1.7213)  loss_mask: 0.0467 (0.0610)  time: 0.1719  data: 0.0004  max mem: 6052
[03:38:44.703320] Epoch: [17]  [160/781]  eta: 0:01:49  lr: 0.000240  training_loss: 1.7631 (1.7831)  classification_loss: 1.7106 (1.7210)  loss_mask: 0.0536 (0.0621)  time: 0.1706  data: 0.0002  max mem: 6052
[03:38:48.122498] Epoch: [17]  [180/781]  eta: 0:01:45  lr: 0.000240  training_loss: 1.6903 (1.7746)  classification_loss: 1.6234 (1.7141)  loss_mask: 0.0415 (0.0605)  time: 0.1709  data: 0.0003  max mem: 6052
[03:38:51.530287] Epoch: [17]  [200/781]  eta: 0:01:41  lr: 0.000240  training_loss: 1.7973 (1.7743)  classification_loss: 1.7201 (1.7143)  loss_mask: 0.0320 (0.0600)  time: 0.1703  data: 0.0002  max mem: 6052
[03:38:54.944577] Epoch: [17]  [220/781]  eta: 0:01:37  lr: 0.000240  training_loss: 1.7268 (1.7726)  classification_loss: 1.7021 (1.7151)  loss_mask: 0.0296 (0.0575)  time: 0.1706  data: 0.0002  max mem: 6052
[03:38:58.379629] Epoch: [17]  [240/781]  eta: 0:01:34  lr: 0.000240  training_loss: 1.7792 (1.7730)  classification_loss: 1.7520 (1.7176)  loss_mask: 0.0271 (0.0554)  time: 0.1717  data: 0.0002  max mem: 6052
[03:39:01.803709] Epoch: [17]  [260/781]  eta: 0:01:30  lr: 0.000240  training_loss: 1.7347 (1.7708)  classification_loss: 1.6964 (1.7161)  loss_mask: 0.0287 (0.0547)  time: 0.1711  data: 0.0002  max mem: 6052
[03:39:05.214614] Epoch: [17]  [280/781]  eta: 0:01:27  lr: 0.000240  training_loss: 1.8340 (1.7783)  classification_loss: 1.7490 (1.7180)  loss_mask: 0.0650 (0.0603)  time: 0.1705  data: 0.0002  max mem: 6052
[03:39:08.645423] Epoch: [17]  [300/781]  eta: 0:01:23  lr: 0.000240  training_loss: 1.7619 (1.7778)  classification_loss: 1.6989 (1.7175)  loss_mask: 0.0493 (0.0604)  time: 0.1714  data: 0.0002  max mem: 6052
[03:39:12.076840] Epoch: [17]  [320/781]  eta: 0:01:20  lr: 0.000240  training_loss: 1.8124 (1.7777)  classification_loss: 1.7409 (1.7175)  loss_mask: 0.0439 (0.0602)  time: 0.1715  data: 0.0002  max mem: 6052
[03:39:15.492169] Epoch: [17]  [340/781]  eta: 0:01:16  lr: 0.000240  training_loss: 1.7691 (1.7776)  classification_loss: 1.7117 (1.7177)  loss_mask: 0.0445 (0.0599)  time: 0.1707  data: 0.0002  max mem: 6052
[03:39:18.901938] Epoch: [17]  [360/781]  eta: 0:01:12  lr: 0.000240  training_loss: 1.7748 (1.7783)  classification_loss: 1.7073 (1.7187)  loss_mask: 0.0432 (0.0595)  time: 0.1704  data: 0.0002  max mem: 6052
[03:39:22.316755] Epoch: [17]  [380/781]  eta: 0:01:09  lr: 0.000240  training_loss: 1.7693 (1.7786)  classification_loss: 1.7287 (1.7198)  loss_mask: 0.0306 (0.0588)  time: 0.1707  data: 0.0002  max mem: 6052
[03:39:25.724712] Epoch: [17]  [400/781]  eta: 0:01:05  lr: 0.000239  training_loss: 1.7723 (1.7787)  classification_loss: 1.6768 (1.7186)  loss_mask: 0.0563 (0.0601)  time: 0.1703  data: 0.0002  max mem: 6052
[03:39:29.142003] Epoch: [17]  [420/781]  eta: 0:01:02  lr: 0.000239  training_loss: 1.7619 (1.7782)  classification_loss: 1.7100 (1.7189)  loss_mask: 0.0409 (0.0593)  time: 0.1708  data: 0.0002  max mem: 6052
[03:39:32.561301] Epoch: [17]  [440/781]  eta: 0:00:58  lr: 0.000239  training_loss: 1.7226 (1.7762)  classification_loss: 1.6875 (1.7175)  loss_mask: 0.0367 (0.0587)  time: 0.1709  data: 0.0002  max mem: 6052
[03:39:35.966258] Epoch: [17]  [460/781]  eta: 0:00:55  lr: 0.000239  training_loss: 1.7298 (1.7740)  classification_loss: 1.6931 (1.7167)  loss_mask: 0.0219 (0.0573)  time: 0.1702  data: 0.0001  max mem: 6052
[03:39:39.374899] Epoch: [17]  [480/781]  eta: 0:00:51  lr: 0.000239  training_loss: 1.7470 (1.7729)  classification_loss: 1.7065 (1.7160)  loss_mask: 0.0502 (0.0569)  time: 0.1703  data: 0.0002  max mem: 6052
[03:39:42.792200] Epoch: [17]  [500/781]  eta: 0:00:48  lr: 0.000239  training_loss: 1.7750 (1.7746)  classification_loss: 1.7052 (1.7161)  loss_mask: 0.0688 (0.0585)  time: 0.1708  data: 0.0003  max mem: 6052
[03:39:46.213686] Epoch: [17]  [520/781]  eta: 0:00:45  lr: 0.000239  training_loss: 1.7948 (1.7771)  classification_loss: 1.6771 (1.7153)  loss_mask: 0.1177 (0.0618)  time: 0.1710  data: 0.0003  max mem: 6052
[03:39:49.628000] Epoch: [17]  [540/781]  eta: 0:00:41  lr: 0.000239  training_loss: 1.8645 (1.7807)  classification_loss: 1.7360 (1.7169)  loss_mask: 0.0887 (0.0638)  time: 0.1706  data: 0.0002  max mem: 6052
[03:39:53.030045] Epoch: [17]  [560/781]  eta: 0:00:38  lr: 0.000239  training_loss: 1.7631 (1.7823)  classification_loss: 1.7012 (1.7167)  loss_mask: 0.1030 (0.0656)  time: 0.1700  data: 0.0002  max mem: 6052
[03:39:56.460002] Epoch: [17]  [580/781]  eta: 0:00:34  lr: 0.000239  training_loss: 1.8518 (1.7838)  classification_loss: 1.6818 (1.7167)  loss_mask: 0.0853 (0.0672)  time: 0.1714  data: 0.0003  max mem: 6052
[03:39:59.855363] Epoch: [17]  [600/781]  eta: 0:00:31  lr: 0.000239  training_loss: 1.7549 (1.7825)  classification_loss: 1.6841 (1.7155)  loss_mask: 0.0561 (0.0670)  time: 0.1697  data: 0.0003  max mem: 6052
[03:40:03.266967] Epoch: [17]  [620/781]  eta: 0:00:27  lr: 0.000239  training_loss: 1.6904 (1.7807)  classification_loss: 1.6618 (1.7141)  loss_mask: 0.0490 (0.0666)  time: 0.1705  data: 0.0003  max mem: 6052
[03:40:06.687185] Epoch: [17]  [640/781]  eta: 0:00:24  lr: 0.000239  training_loss: 1.7412 (1.7802)  classification_loss: 1.7000 (1.7141)  loss_mask: 0.0447 (0.0660)  time: 0.1709  data: 0.0002  max mem: 6052
[03:40:10.096100] Epoch: [17]  [660/781]  eta: 0:00:20  lr: 0.000239  training_loss: 1.7732 (1.7804)  classification_loss: 1.7279 (1.7148)  loss_mask: 0.0449 (0.0656)  time: 0.1704  data: 0.0003  max mem: 6052
[03:40:13.513721] Epoch: [17]  [680/781]  eta: 0:00:17  lr: 0.000239  training_loss: 1.7262 (1.7791)  classification_loss: 1.7031 (1.7143)  loss_mask: 0.0342 (0.0648)  time: 0.1708  data: 0.0002  max mem: 6052
[03:40:16.917177] Epoch: [17]  [700/781]  eta: 0:00:13  lr: 0.000239  training_loss: 1.7206 (1.7776)  classification_loss: 1.6867 (1.7136)  loss_mask: 0.0346 (0.0640)  time: 0.1701  data: 0.0002  max mem: 6052
[03:40:20.337604] Epoch: [17]  [720/781]  eta: 0:00:10  lr: 0.000239  training_loss: 1.7293 (1.7765)  classification_loss: 1.7010 (1.7133)  loss_mask: 0.0267 (0.0632)  time: 0.1709  data: 0.0002  max mem: 6052
[03:40:23.747773] Epoch: [17]  [740/781]  eta: 0:00:07  lr: 0.000239  training_loss: 1.6914 (1.7743)  classification_loss: 1.6632 (1.7119)  loss_mask: 0.0255 (0.0624)  time: 0.1704  data: 0.0002  max mem: 6052
[03:40:27.151234] Epoch: [17]  [760/781]  eta: 0:00:03  lr: 0.000239  training_loss: 1.7425 (1.7734)  classification_loss: 1.7086 (1.7118)  loss_mask: 0.0291 (0.0616)  time: 0.1701  data: 0.0003  max mem: 6052
[03:40:30.549161] Epoch: [17]  [780/781]  eta: 0:00:00  lr: 0.000239  training_loss: 1.7302 (1.7729)  classification_loss: 1.7018 (1.7117)  loss_mask: 0.0294 (0.0611)  time: 0.1698  data: 0.0002  max mem: 6052
[03:40:30.711312] Epoch: [17] Total time: 0:02:14 (0.1721 s / it)
[03:40:30.711781] Averaged stats: lr: 0.000239  training_loss: 1.7302 (1.7729)  classification_loss: 1.7018 (1.7117)  loss_mask: 0.0294 (0.0611)
[03:40:31.357795] Test:  [  0/157]  eta: 0:01:40  testing_loss: 0.8695 (0.8695)  acc1: 75.0000 (75.0000)  acc5: 95.3125 (95.3125)  time: 0.6397  data: 0.6103  max mem: 6052
[03:40:31.644840] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.1174 (1.0771)  acc1: 59.3750 (62.0739)  acc5: 96.8750 (96.4489)  time: 0.0841  data: 0.0558  max mem: 6052
[03:40:31.929018] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.9978 (1.0290)  acc1: 62.5000 (64.0625)  acc5: 98.4375 (97.3958)  time: 0.0284  data: 0.0002  max mem: 6052
[03:40:32.212820] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.0326 (1.0378)  acc1: 65.6250 (64.5161)  acc5: 98.4375 (96.8750)  time: 0.0282  data: 0.0001  max mem: 6052
[03:40:32.511221] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.0563 (1.0467)  acc1: 65.6250 (63.9863)  acc5: 96.8750 (96.9512)  time: 0.0289  data: 0.0003  max mem: 6052
[03:40:32.798259] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.0563 (1.0497)  acc1: 64.0625 (64.3995)  acc5: 96.8750 (96.8137)  time: 0.0291  data: 0.0006  max mem: 6052
[03:40:33.077965] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.0631 (1.0497)  acc1: 62.5000 (64.1137)  acc5: 96.8750 (96.6957)  time: 0.0282  data: 0.0004  max mem: 6052
[03:40:33.363867] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.0200 (1.0422)  acc1: 62.5000 (64.1945)  acc5: 96.8750 (96.8530)  time: 0.0281  data: 0.0002  max mem: 6052
[03:40:33.657263] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.0172 (1.0423)  acc1: 64.0625 (64.1397)  acc5: 96.8750 (96.7785)  time: 0.0288  data: 0.0004  max mem: 6052
[03:40:33.945206] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.0490 (1.0425)  acc1: 64.0625 (64.2342)  acc5: 98.4375 (96.8235)  time: 0.0289  data: 0.0003  max mem: 6052
[03:40:34.226367] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.0965 (1.0489)  acc1: 64.0625 (63.9542)  acc5: 98.4375 (96.9369)  time: 0.0283  data: 0.0001  max mem: 6052
[03:40:34.511036] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.1289 (1.0511)  acc1: 62.5000 (63.9217)  acc5: 96.8750 (96.9313)  time: 0.0282  data: 0.0003  max mem: 6052
[03:40:34.794502] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.0359 (1.0476)  acc1: 65.6250 (64.0754)  acc5: 96.8750 (96.9267)  time: 0.0282  data: 0.0003  max mem: 6052
[03:40:35.076619] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.0359 (1.0496)  acc1: 64.0625 (63.9790)  acc5: 98.4375 (96.9585)  time: 0.0281  data: 0.0001  max mem: 6052
[03:40:35.356438] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.0332 (1.0477)  acc1: 64.0625 (64.0736)  acc5: 96.8750 (96.9637)  time: 0.0280  data: 0.0001  max mem: 6052
[03:40:35.635388] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.0139 (1.0438)  acc1: 67.1875 (64.3626)  acc5: 96.8750 (96.9578)  time: 0.0278  data: 0.0001  max mem: 6052
[03:40:35.785015] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.9892 (1.0461)  acc1: 65.6250 (64.2700)  acc5: 98.4375 (97.0100)  time: 0.0269  data: 0.0001  max mem: 6052
[03:40:35.964303] Test: Total time: 0:00:05 (0.0334 s / it)
[03:40:35.964891] * Acc@1 64.270 Acc@5 97.010 loss 1.046
[03:40:35.965233] Accuracy of the network on the 10000 test images: 64.3%
[03:40:35.965466] Max accuracy: 64.27%
[03:40:36.325487] log_dir: ./output_dir
[03:40:37.210034] Epoch: [18]  [  0/781]  eta: 0:11:29  lr: 0.000239  training_loss: 1.6048 (1.6048)  classification_loss: 1.5800 (1.5800)  loss_mask: 0.0248 (0.0248)  time: 0.8830  data: 0.6961  max mem: 6052
[03:40:40.635426] Epoch: [18]  [ 20/781]  eta: 0:02:36  lr: 0.000239  training_loss: 1.6616 (1.7012)  classification_loss: 1.6457 (1.6513)  loss_mask: 0.0205 (0.0499)  time: 0.1712  data: 0.0002  max mem: 6052
[03:40:44.055724] Epoch: [18]  [ 40/781]  eta: 0:02:19  lr: 0.000239  training_loss: 1.9519 (1.8239)  classification_loss: 1.7190 (1.6940)  loss_mask: 0.1616 (0.1299)  time: 0.1709  data: 0.0002  max mem: 6052
[03:40:47.481785] Epoch: [18]  [ 60/781]  eta: 0:02:11  lr: 0.000239  training_loss: 1.7812 (1.8176)  classification_loss: 1.7047 (1.6989)  loss_mask: 0.0706 (0.1187)  time: 0.1712  data: 0.0002  max mem: 6052
[03:40:50.900889] Epoch: [18]  [ 80/781]  eta: 0:02:06  lr: 0.000238  training_loss: 1.8193 (1.8223)  classification_loss: 1.7639 (1.7059)  loss_mask: 0.0792 (0.1164)  time: 0.1709  data: 0.0002  max mem: 6052
[03:40:54.331721] Epoch: [18]  [100/781]  eta: 0:02:01  lr: 0.000238  training_loss: 1.8193 (1.8141)  classification_loss: 1.7231 (1.7062)  loss_mask: 0.0672 (0.1078)  time: 0.1715  data: 0.0002  max mem: 6052
[03:40:57.754249] Epoch: [18]  [120/781]  eta: 0:01:56  lr: 0.000238  training_loss: 1.7552 (1.8034)  classification_loss: 1.7027 (1.7038)  loss_mask: 0.0541 (0.0996)  time: 0.1711  data: 0.0002  max mem: 6052

[03:41:01.172208] Epoch: [18]  [140/781]  eta: 0:01:52  lr: 0.000238  training_loss: 1.7825 (1.8029)  classification_loss: 1.6753 (1.7035)  loss_mask: 0.0995 (0.0994)  time: 0.1708  data: 0.0002  max mem: 6052
[03:41:04.620185] Epoch: [18]  [160/781]  eta: 0:01:49  lr: 0.000238  training_loss: 1.7466 (1.7965)  classification_loss: 1.6850 (1.7021)  loss_mask: 0.0515 (0.0944)  time: 0.1723  data: 0.0002  max mem: 6052
[03:41:08.065965] Epoch: [18]  [180/781]  eta: 0:01:45  lr: 0.000238  training_loss: 1.8002 (1.7990)  classification_loss: 1.6448 (1.6988)  loss_mask: 0.1309 (0.1001)  time: 0.1722  data: 0.0002  max mem: 6052
[03:41:11.500174] Epoch: [18]  [200/781]  eta: 0:01:41  lr: 0.000238  training_loss: 1.8019 (1.7996)  classification_loss: 1.6816 (1.6975)  loss_mask: 0.0796 (0.1021)  time: 0.1716  data: 0.0002  max mem: 6052
[03:41:14.913144] Epoch: [18]  [220/781]  eta: 0:01:37  lr: 0.000238  training_loss: 1.7885 (1.7988)  classification_loss: 1.7164 (1.6996)  loss_mask: 0.0608 (0.0992)  time: 0.1705  data: 0.0002  max mem: 6052
[03:41:18.329329] Epoch: [18]  [240/781]  eta: 0:01:34  lr: 0.000238  training_loss: 1.7689 (1.7982)  classification_loss: 1.7140 (1.7024)  loss_mask: 0.0349 (0.0958)  time: 0.1707  data: 0.0003  max mem: 6052
[03:41:21.752644] Epoch: [18]  [260/781]  eta: 0:01:30  lr: 0.000238  training_loss: 1.7675 (1.7969)  classification_loss: 1.7379 (1.7043)  loss_mask: 0.0397 (0.0926)  time: 0.1711  data: 0.0003  max mem: 6052
[03:41:25.156943] Epoch: [18]  [280/781]  eta: 0:01:27  lr: 0.000238  training_loss: 1.8090 (1.7990)  classification_loss: 1.7516 (1.7081)  loss_mask: 0.0626 (0.0909)  time: 0.1701  data: 0.0002  max mem: 6052
[03:41:28.583947] Epoch: [18]  [300/781]  eta: 0:01:23  lr: 0.000238  training_loss: 1.8120 (1.8000)  classification_loss: 1.7238 (1.7097)  loss_mask: 0.0543 (0.0903)  time: 0.1713  data: 0.0003  max mem: 6052
[03:41:32.006602] Epoch: [18]  [320/781]  eta: 0:01:19  lr: 0.000238  training_loss: 1.8041 (1.8003)  classification_loss: 1.7306 (1.7113)  loss_mask: 0.0564 (0.0889)  time: 0.1711  data: 0.0002  max mem: 6052
[03:41:35.408912] Epoch: [18]  [340/781]  eta: 0:01:16  lr: 0.000238  training_loss: 1.7822 (1.7992)  classification_loss: 1.6810 (1.7108)  loss_mask: 0.0680 (0.0883)  time: 0.1700  data: 0.0001  max mem: 6052
[03:41:38.858892] Epoch: [18]  [360/781]  eta: 0:01:12  lr: 0.000238  training_loss: 1.7994 (1.7996)  classification_loss: 1.7384 (1.7112)  loss_mask: 0.0725 (0.0884)  time: 0.1724  data: 0.0002  max mem: 6052
[03:41:42.274987] Epoch: [18]  [380/781]  eta: 0:01:09  lr: 0.000238  training_loss: 1.7844 (1.7984)  classification_loss: 1.6937 (1.7113)  loss_mask: 0.0504 (0.0870)  time: 0.1707  data: 0.0002  max mem: 6052
[03:41:45.668966] Epoch: [18]  [400/781]  eta: 0:01:05  lr: 0.000238  training_loss: 1.7647 (1.7963)  classification_loss: 1.6957 (1.7110)  loss_mask: 0.0535 (0.0853)  time: 0.1696  data: 0.0002  max mem: 6052
[03:41:49.070984] Epoch: [18]  [420/781]  eta: 0:01:02  lr: 0.000238  training_loss: 1.7149 (1.7919)  classification_loss: 1.6725 (1.7086)  loss_mask: 0.0324 (0.0832)  time: 0.1700  data: 0.0002  max mem: 6052
[03:41:52.487071] Epoch: [18]  [440/781]  eta: 0:00:58  lr: 0.000238  training_loss: 1.7084 (1.7887)  classification_loss: 1.6563 (1.7070)  loss_mask: 0.0340 (0.0817)  time: 0.1707  data: 0.0003  max mem: 6052
[03:41:55.932914] Epoch: [18]  [460/781]  eta: 0:00:55  lr: 0.000238  training_loss: 1.7132 (1.7860)  classification_loss: 1.6740 (1.7049)  loss_mask: 0.0568 (0.0811)  time: 0.1722  data: 0.0003  max mem: 6052
[03:41:59.347185] Epoch: [18]  [480/781]  eta: 0:00:51  lr: 0.000238  training_loss: 1.7523 (1.7849)  classification_loss: 1.6729 (1.7045)  loss_mask: 0.0626 (0.0804)  time: 0.1706  data: 0.0003  max mem: 6052
[03:42:02.774213] Epoch: [18]  [500/781]  eta: 0:00:48  lr: 0.000238  training_loss: 1.7171 (1.7828)  classification_loss: 1.6897 (1.7040)  loss_mask: 0.0305 (0.0788)  time: 0.1713  data: 0.0002  max mem: 6052
[03:42:06.184499] Epoch: [18]  [520/781]  eta: 0:00:44  lr: 0.000238  training_loss: 1.7329 (1.7800)  classification_loss: 1.7000 (1.7032)  loss_mask: 0.0258 (0.0768)  time: 0.1704  data: 0.0001  max mem: 6052
[03:42:09.614451] Epoch: [18]  [540/781]  eta: 0:00:41  lr: 0.000237  training_loss: 1.7358 (1.7790)  classification_loss: 1.6904 (1.7024)  loss_mask: 0.0481 (0.0766)  time: 0.1714  data: 0.0002  max mem: 6052
[03:42:13.057004] Epoch: [18]  [560/781]  eta: 0:00:38  lr: 0.000237  training_loss: 1.7811 (1.7813)  classification_loss: 1.7043 (1.7033)  loss_mask: 0.0768 (0.0780)  time: 0.1721  data: 0.0002  max mem: 6052
[03:42:16.471946] Epoch: [18]  [580/781]  eta: 0:00:34  lr: 0.000237  training_loss: 1.6812 (1.7791)  classification_loss: 1.6474 (1.7019)  loss_mask: 0.0440 (0.0772)  time: 0.1706  data: 0.0002  max mem: 6052
[03:42:19.878424] Epoch: [18]  [600/781]  eta: 0:00:31  lr: 0.000237  training_loss: 1.8238 (1.7803)  classification_loss: 1.6890 (1.7020)  loss_mask: 0.0707 (0.0783)  time: 0.1702  data: 0.0002  max mem: 6052
[03:42:23.306322] Epoch: [18]  [620/781]  eta: 0:00:27  lr: 0.000237  training_loss: 1.7719 (1.7799)  classification_loss: 1.6888 (1.7017)  loss_mask: 0.0706 (0.0782)  time: 0.1713  data: 0.0003  max mem: 6052
[03:42:26.733011] Epoch: [18]  [640/781]  eta: 0:00:24  lr: 0.000237  training_loss: 1.7457 (1.7788)  classification_loss: 1.6717 (1.7011)  loss_mask: 0.0512 (0.0777)  time: 0.1713  data: 0.0001  max mem: 6052
[03:42:30.135195] Epoch: [18]  [660/781]  eta: 0:00:20  lr: 0.000237  training_loss: 1.7445 (1.7778)  classification_loss: 1.6734 (1.7006)  loss_mask: 0.0582 (0.0772)  time: 0.1700  data: 0.0002  max mem: 6052
[03:42:33.574441] Epoch: [18]  [680/781]  eta: 0:00:17  lr: 0.000237  training_loss: 1.7681 (1.7777)  classification_loss: 1.6877 (1.7008)  loss_mask: 0.0540 (0.0769)  time: 0.1719  data: 0.0002  max mem: 6052
[03:42:37.003369] Epoch: [18]  [700/781]  eta: 0:00:13  lr: 0.000237  training_loss: 1.7220 (1.7764)  classification_loss: 1.6982 (1.7005)  loss_mask: 0.0313 (0.0759)  time: 0.1714  data: 0.0003  max mem: 6052
[03:42:40.426921] Epoch: [18]  [720/781]  eta: 0:00:10  lr: 0.000237  training_loss: 1.7247 (1.7755)  classification_loss: 1.6744 (1.7009)  loss_mask: 0.0293 (0.0747)  time: 0.1711  data: 0.0002  max mem: 6052
[03:42:43.847182] Epoch: [18]  [740/781]  eta: 0:00:07  lr: 0.000237  training_loss: 1.6943 (1.7732)  classification_loss: 1.6594 (1.6995)  loss_mask: 0.0325 (0.0737)  time: 0.1709  data: 0.0002  max mem: 6052
[03:42:47.282643] Epoch: [18]  [760/781]  eta: 0:00:03  lr: 0.000237  training_loss: 1.7253 (1.7720)  classification_loss: 1.6954 (1.6992)  loss_mask: 0.0349 (0.0728)  time: 0.1717  data: 0.0002  max mem: 6052
[03:42:50.693460] Epoch: [18]  [780/781]  eta: 0:00:00  lr: 0.000237  training_loss: 1.7268 (1.7719)  classification_loss: 1.6676 (1.6991)  loss_mask: 0.0608 (0.0728)  time: 0.1704  data: 0.0003  max mem: 6052
[03:42:50.839647] Epoch: [18] Total time: 0:02:14 (0.1722 s / it)
[03:42:50.840199] Averaged stats: lr: 0.000237  training_loss: 1.7268 (1.7719)  classification_loss: 1.6676 (1.6991)  loss_mask: 0.0608 (0.0728)
[03:42:51.513846] Test:  [  0/157]  eta: 0:01:45  testing_loss: 0.9539 (0.9539)  acc1: 70.3125 (70.3125)  acc5: 95.3125 (95.3125)  time: 0.6693  data: 0.6393  max mem: 6052
[03:42:51.796717] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.0881 (1.0791)  acc1: 60.9375 (63.0682)  acc5: 96.8750 (97.1591)  time: 0.0864  data: 0.0583  max mem: 6052
[03:42:52.077045] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.0310 (1.0402)  acc1: 65.6250 (65.5506)  acc5: 96.8750 (97.6935)  time: 0.0280  data: 0.0001  max mem: 6052
[03:42:52.357533] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.0310 (1.0521)  acc1: 65.6250 (64.8690)  acc5: 96.8750 (97.1774)  time: 0.0279  data: 0.0001  max mem: 6052
[03:42:52.642485] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.0541 (1.0555)  acc1: 65.6250 (64.4817)  acc5: 96.8750 (97.0274)  time: 0.0281  data: 0.0001  max mem: 6052
[03:42:52.931309] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.0510 (1.0558)  acc1: 65.6250 (64.4914)  acc5: 96.8750 (96.8444)  time: 0.0285  data: 0.0002  max mem: 6052
[03:42:53.213129] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.0505 (1.0493)  acc1: 65.6250 (64.6773)  acc5: 96.8750 (96.8238)  time: 0.0284  data: 0.0002  max mem: 6052
[03:42:53.493500] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.0034 (1.0424)  acc1: 67.1875 (64.9648)  acc5: 96.8750 (96.8970)  time: 0.0280  data: 0.0002  max mem: 6052
[03:42:53.778666] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.9863 (1.0447)  acc1: 67.1875 (65.0656)  acc5: 96.8750 (96.8557)  time: 0.0282  data: 0.0001  max mem: 6052
[03:42:54.059216] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.0518 (1.0456)  acc1: 64.0625 (64.9725)  acc5: 96.8750 (96.8750)  time: 0.0282  data: 0.0001  max mem: 6052
[03:42:54.339392] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.0483 (1.0482)  acc1: 62.5000 (64.8205)  acc5: 96.8750 (96.8750)  time: 0.0279  data: 0.0001  max mem: 6052
[03:42:54.620667] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.0995 (1.0513)  acc1: 60.9375 (64.6537)  acc5: 96.8750 (96.9313)  time: 0.0280  data: 0.0001  max mem: 6052
[03:42:54.901823] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.0163 (1.0478)  acc1: 65.6250 (64.7598)  acc5: 96.8750 (96.9137)  time: 0.0280  data: 0.0002  max mem: 6052
[03:42:55.182383] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.0120 (1.0497)  acc1: 64.0625 (64.6589)  acc5: 96.8750 (96.8989)  time: 0.0280  data: 0.0002  max mem: 6052
[03:42:55.462366] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.0683 (1.0490)  acc1: 65.6250 (64.8493)  acc5: 96.8750 (96.8861)  time: 0.0279  data: 0.0001  max mem: 6052
[03:42:55.740636] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.9981 (1.0448)  acc1: 68.7500 (65.1180)  acc5: 96.8750 (96.9060)  time: 0.0278  data: 0.0001  max mem: 6052
[03:42:55.890860] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.9728 (1.0474)  acc1: 68.7500 (65.0400)  acc5: 96.8750 (96.9200)  time: 0.0269  data: 0.0001  max mem: 6052
[03:42:56.045457] Test: Total time: 0:00:05 (0.0331 s / it)
[03:42:56.045959] * Acc@1 65.040 Acc@5 96.920 loss 1.047
[03:42:56.046288] Accuracy of the network on the 10000 test images: 65.0%
[03:42:56.046549] Max accuracy: 65.04%
[03:42:56.331719] log_dir: ./output_dir
[03:42:57.182936] Epoch: [19]  [  0/781]  eta: 0:11:03  lr: 0.000237  training_loss: 1.8061 (1.8061)  classification_loss: 1.6884 (1.6884)  loss_mask: 0.1177 (0.1177)  time: 0.8496  data: 0.6554  max mem: 6052
[03:43:00.601088] Epoch: [19]  [ 20/781]  eta: 0:02:34  lr: 0.000237  training_loss: 1.6943 (1.7319)  classification_loss: 1.6582 (1.6567)  loss_mask: 0.0496 (0.0752)  time: 0.1708  data: 0.0003  max mem: 6052
[03:43:04.042072] Epoch: [19]  [ 40/781]  eta: 0:02:19  lr: 0.000237  training_loss: 1.7214 (1.7114)  classification_loss: 1.6725 (1.6526)  loss_mask: 0.0351 (0.0588)  time: 0.1720  data: 0.0002  max mem: 6052
[03:43:07.517621] Epoch: [19]  [ 60/781]  eta: 0:02:12  lr: 0.000237  training_loss: 1.7360 (1.7268)  classification_loss: 1.6876 (1.6692)  loss_mask: 0.0395 (0.0575)  time: 0.1737  data: 0.0003  max mem: 6052
[03:43:10.939083] Epoch: [19]  [ 80/781]  eta: 0:02:06  lr: 0.000237  training_loss: 1.7630 (1.7313)  classification_loss: 1.7294 (1.6763)  loss_mask: 0.0339 (0.0551)  time: 0.1710  data: 0.0003  max mem: 6052
[03:43:14.366245] Epoch: [19]  [100/781]  eta: 0:02:01  lr: 0.000237  training_loss: 1.7888 (1.7410)  classification_loss: 1.6906 (1.6775)  loss_mask: 0.0727 (0.0635)  time: 0.1713  data: 0.0002  max mem: 6052
[03:43:17.767173] Epoch: [19]  [120/781]  eta: 0:01:57  lr: 0.000237  training_loss: 1.7002 (1.7327)  classification_loss: 1.6537 (1.6709)  loss_mask: 0.0338 (0.0618)  time: 0.1700  data: 0.0003  max mem: 6052
[03:43:21.159516] Epoch: [19]  [140/781]  eta: 0:01:52  lr: 0.000237  training_loss: 1.6861 (1.7252)  classification_loss: 1.6394 (1.6640)  loss_mask: 0.0382 (0.0612)  time: 0.1695  data: 0.0003  max mem: 6052
[03:43:24.575362] Epoch: [19]  [160/781]  eta: 0:01:48  lr: 0.000237  training_loss: 1.7573 (1.7289)  classification_loss: 1.6627 (1.6621)  loss_mask: 0.0913 (0.0668)  time: 0.1707  data: 0.0002  max mem: 6052
[03:43:27.990906] Epoch: [19]  [180/781]  eta: 0:01:45  lr: 0.000236  training_loss: 1.7115 (1.7260)  classification_loss: 1.6557 (1.6612)  loss_mask: 0.0404 (0.0648)  time: 0.1707  data: 0.0003  max mem: 6052
[03:43:31.435647] Epoch: [19]  [200/781]  eta: 0:01:41  lr: 0.000236  training_loss: 1.7342 (1.7277)  classification_loss: 1.7209 (1.6661)  loss_mask: 0.0197 (0.0615)  time: 0.1722  data: 0.0002  max mem: 6052
[03:43:34.850553] Epoch: [19]  [220/781]  eta: 0:01:37  lr: 0.000236  training_loss: 1.6949 (1.7264)  classification_loss: 1.6516 (1.6677)  loss_mask: 0.0300 (0.0587)  time: 0.1707  data: 0.0001  max mem: 6052
[03:43:38.264571] Epoch: [19]  [240/781]  eta: 0:01:34  lr: 0.000236  training_loss: 1.7887 (1.7297)  classification_loss: 1.7063 (1.6708)  loss_mask: 0.0539 (0.0589)  time: 0.1706  data: 0.0002  max mem: 6052
[03:43:41.688876] Epoch: [19]  [260/781]  eta: 0:01:30  lr: 0.000236  training_loss: 1.7514 (1.7292)  classification_loss: 1.6968 (1.6712)  loss_mask: 0.0415 (0.0580)  time: 0.1711  data: 0.0002  max mem: 6052
[03:43:45.100888] Epoch: [19]  [280/781]  eta: 0:01:26  lr: 0.000236  training_loss: 1.7626 (1.7306)  classification_loss: 1.6946 (1.6738)  loss_mask: 0.0345 (0.0568)  time: 0.1705  data: 0.0001  max mem: 6052
[03:43:48.513212] Epoch: [19]  [300/781]  eta: 0:01:23  lr: 0.000236  training_loss: 1.7423 (1.7338)  classification_loss: 1.6604 (1.6753)  loss_mask: 0.0598 (0.0584)  time: 0.1705  data: 0.0001  max mem: 6052
[03:43:51.974484] Epoch: [19]  [320/781]  eta: 0:01:19  lr: 0.000236  training_loss: 1.7256 (1.7335)  classification_loss: 1.6654 (1.6746)  loss_mask: 0.0475 (0.0589)  time: 0.1730  data: 0.0002  max mem: 6052
[03:43:55.405641] Epoch: [19]  [340/781]  eta: 0:01:16  lr: 0.000236  training_loss: 1.6852 (1.7306)  classification_loss: 1.6237 (1.6726)  loss_mask: 0.0432 (0.0581)  time: 0.1715  data: 0.0003  max mem: 6052
[03:43:58.829202] Epoch: [19]  [360/781]  eta: 0:01:12  lr: 0.000236  training_loss: 1.7208 (1.7302)  classification_loss: 1.6747 (1.6737)  loss_mask: 0.0269 (0.0565)  time: 0.1711  data: 0.0002  max mem: 6052
[03:44:02.244639] Epoch: [19]  [380/781]  eta: 0:01:09  lr: 0.000236  training_loss: 1.6457 (1.7279)  classification_loss: 1.6316 (1.6720)  loss_mask: 0.0255 (0.0559)  time: 0.1707  data: 0.0002  max mem: 6052
[03:44:05.678383] Epoch: [19]  [400/781]  eta: 0:01:05  lr: 0.000236  training_loss: 1.6900 (1.7262)  classification_loss: 1.6270 (1.6707)  loss_mask: 0.0376 (0.0554)  time: 0.1716  data: 0.0002  max mem: 6052
[03:44:09.176606] Epoch: [19]  [420/781]  eta: 0:01:02  lr: 0.000236  training_loss: 1.7410 (1.7267)  classification_loss: 1.6875 (1.6717)  loss_mask: 0.0406 (0.0549)  time: 0.1748  data: 0.0002  max mem: 6052
[03:44:12.589737] Epoch: [19]  [440/781]  eta: 0:00:58  lr: 0.000236  training_loss: 1.7553 (1.7280)  classification_loss: 1.6843 (1.6736)  loss_mask: 0.0327 (0.0543)  time: 0.1706  data: 0.0002  max mem: 6052
[03:44:16.005554] Epoch: [19]  [460/781]  eta: 0:00:55  lr: 0.000236  training_loss: 1.7075 (1.7290)  classification_loss: 1.6123 (1.6725)  loss_mask: 0.0886 (0.0565)  time: 0.1707  data: 0.0002  max mem: 6052
[03:44:19.414480] Epoch: [19]  [480/781]  eta: 0:00:51  lr: 0.000236  training_loss: 1.8333 (1.7332)  classification_loss: 1.6988 (1.6737)  loss_mask: 0.0945 (0.0595)  time: 0.1703  data: 0.0003  max mem: 6052
[03:44:22.823156] Epoch: [19]  [500/781]  eta: 0:00:48  lr: 0.000236  training_loss: 1.7536 (1.7336)  classification_loss: 1.7038 (1.6739)  loss_mask: 0.0479 (0.0596)  time: 0.1704  data: 0.0002  max mem: 6052
[03:44:26.222324] Epoch: [19]  [520/781]  eta: 0:00:45  lr: 0.000236  training_loss: 1.7478 (1.7342)  classification_loss: 1.6800 (1.6747)  loss_mask: 0.0455 (0.0595)  time: 0.1699  data: 0.0002  max mem: 6052
[03:44:29.617146] Epoch: [19]  [540/781]  eta: 0:00:41  lr: 0.000236  training_loss: 1.7547 (1.7351)  classification_loss: 1.6911 (1.6753)  loss_mask: 0.0494 (0.0598)  time: 0.1697  data: 0.0001  max mem: 6052
[03:44:33.030069] Epoch: [19]  [560/781]  eta: 0:00:38  lr: 0.000236  training_loss: 1.7195 (1.7354)  classification_loss: 1.6537 (1.6750)  loss_mask: 0.0538 (0.0604)  time: 0.1706  data: 0.0006  max mem: 6052
[03:44:36.421731] Epoch: [19]  [580/781]  eta: 0:00:34  lr: 0.000235  training_loss: 1.6982 (1.7349)  classification_loss: 1.6600 (1.6745)  loss_mask: 0.0557 (0.0604)  time: 0.1695  data: 0.0002  max mem: 6052
[03:44:39.813338] Epoch: [19]  [600/781]  eta: 0:00:31  lr: 0.000235  training_loss: 1.6596 (1.7329)  classification_loss: 1.6147 (1.6734)  loss_mask: 0.0286 (0.0595)  time: 0.1695  data: 0.0002  max mem: 6052
[03:44:43.251667] Epoch: [19]  [620/781]  eta: 0:00:27  lr: 0.000235  training_loss: 1.7147 (1.7324)  classification_loss: 1.6833 (1.6737)  loss_mask: 0.0303 (0.0588)  time: 0.1718  data: 0.0002  max mem: 6052
[03:44:46.679763] Epoch: [19]  [640/781]  eta: 0:00:24  lr: 0.000235  training_loss: 1.7631 (1.7341)  classification_loss: 1.6809 (1.6743)  loss_mask: 0.0628 (0.0598)  time: 0.1713  data: 0.0003  max mem: 6052
[03:44:50.091819] Epoch: [19]  [660/781]  eta: 0:00:20  lr: 0.000235  training_loss: 1.7711 (1.7350)  classification_loss: 1.6991 (1.6748)  loss_mask: 0.0606 (0.0603)  time: 0.1705  data: 0.0002  max mem: 6052
[03:44:53.516075] Epoch: [19]  [680/781]  eta: 0:00:17  lr: 0.000235  training_loss: 1.7776 (1.7358)  classification_loss: 1.6562 (1.6746)  loss_mask: 0.0793 (0.0613)  time: 0.1711  data: 0.0003  max mem: 6052
[03:44:56.932063] Epoch: [19]  [700/781]  eta: 0:00:13  lr: 0.000235  training_loss: 1.7797 (1.7360)  classification_loss: 1.6720 (1.6740)  loss_mask: 0.0561 (0.0619)  time: 0.1707  data: 0.0002  max mem: 6052
[03:45:00.354552] Epoch: [19]  [720/781]  eta: 0:00:10  lr: 0.000235  training_loss: 1.6928 (1.7350)  classification_loss: 1.6758 (1.6737)  loss_mask: 0.0396 (0.0613)  time: 0.1711  data: 0.0002  max mem: 6052
[03:45:03.796468] Epoch: [19]  [740/781]  eta: 0:00:07  lr: 0.000235  training_loss: 1.6699 (1.7330)  classification_loss: 1.6256 (1.6726)  loss_mask: 0.0218 (0.0604)  time: 0.1720  data: 0.0002  max mem: 6052
[03:45:07.213702] Epoch: [19]  [760/781]  eta: 0:00:03  lr: 0.000235  training_loss: 1.7459 (1.7334)  classification_loss: 1.7321 (1.6740)  loss_mask: 0.0163 (0.0593)  time: 0.1708  data: 0.0002  max mem: 6052
[03:45:10.627604] Epoch: [19]  [780/781]  eta: 0:00:00  lr: 0.000235  training_loss: 1.6918 (1.7326)  classification_loss: 1.6534 (1.6739)  loss_mask: 0.0303 (0.0586)  time: 0.1706  data: 0.0002  max mem: 6052
[03:45:10.795718] Epoch: [19] Total time: 0:02:14 (0.1722 s / it)
[03:45:10.796178] Averaged stats: lr: 0.000235  training_loss: 1.6918 (1.7326)  classification_loss: 1.6534 (1.6739)  loss_mask: 0.0303 (0.0586)
[03:45:11.431105] Test:  [  0/157]  eta: 0:01:38  testing_loss: 0.9674 (0.9674)  acc1: 67.1875 (67.1875)  acc5: 95.3125 (95.3125)  time: 0.6286  data: 0.5996  max mem: 6052
[03:45:11.715294] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.0107 (1.0130)  acc1: 65.6250 (65.0568)  acc5: 98.4375 (98.1534)  time: 0.0829  data: 0.0546  max mem: 6052
[03:45:11.996773] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.9748 (0.9755)  acc1: 65.6250 (66.8155)  acc5: 98.4375 (98.5863)  time: 0.0282  data: 0.0001  max mem: 6052
[03:45:12.280867] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.9716 (0.9880)  acc1: 67.1875 (66.6835)  acc5: 98.4375 (97.6310)  time: 0.0282  data: 0.0001  max mem: 6052
[03:45:12.568886] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.9892 (0.9906)  acc1: 67.1875 (66.5777)  acc5: 96.8750 (97.6372)  time: 0.0285  data: 0.0001  max mem: 6052
[03:45:12.854292] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.9821 (0.9893)  acc1: 65.6250 (66.4828)  acc5: 96.8750 (97.3958)  time: 0.0286  data: 0.0002  max mem: 6052
[03:45:13.141016] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.9722 (0.9818)  acc1: 67.1875 (66.8289)  acc5: 96.8750 (97.2592)  time: 0.0285  data: 0.0002  max mem: 6052
[03:45:13.427659] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.9205 (0.9775)  acc1: 67.1875 (66.8574)  acc5: 96.8750 (97.2711)  time: 0.0286  data: 0.0001  max mem: 6052
[03:45:13.712834] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.0056 (0.9825)  acc1: 65.6250 (66.6667)  acc5: 96.8750 (97.3187)  time: 0.0284  data: 0.0002  max mem: 6052
[03:45:13.997205] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.0056 (0.9845)  acc1: 65.6250 (66.5350)  acc5: 98.4375 (97.2527)  time: 0.0283  data: 0.0001  max mem: 6052
[03:45:14.280895] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.0044 (0.9885)  acc1: 65.6250 (66.4913)  acc5: 98.4375 (97.3082)  time: 0.0283  data: 0.0001  max mem: 6052
[03:45:14.565429] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.0413 (0.9907)  acc1: 64.0625 (66.4555)  acc5: 98.4375 (97.3677)  time: 0.0283  data: 0.0001  max mem: 6052
[03:45:14.847820] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.9636 (0.9862)  acc1: 64.0625 (66.6710)  acc5: 96.8750 (97.3528)  time: 0.0282  data: 0.0002  max mem: 6052
[03:45:15.129014] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.9636 (0.9870)  acc1: 67.1875 (66.6746)  acc5: 96.8750 (97.3044)  time: 0.0281  data: 0.0001  max mem: 6052
[03:45:15.408475] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.0196 (0.9873)  acc1: 65.6250 (66.6888)  acc5: 96.8750 (97.3183)  time: 0.0279  data: 0.0001  max mem: 6052
[03:45:15.686014] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.9708 (0.9850)  acc1: 67.1875 (66.8667)  acc5: 96.8750 (97.3406)  time: 0.0277  data: 0.0001  max mem: 6052
[03:45:15.835984] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.9573 (0.9874)  acc1: 67.1875 (66.6900)  acc5: 96.8750 (97.3500)  time: 0.0268  data: 0.0001  max mem: 6052
[03:45:15.995962] Test: Total time: 0:00:05 (0.0331 s / it)
[03:45:15.996517] * Acc@1 66.690 Acc@5 97.350 loss 0.987
[03:45:15.996861] Accuracy of the network on the 10000 test images: 66.7%
[03:45:15.997042] Max accuracy: 66.69%
[03:45:16.283530] log_dir: ./output_dir
[03:45:17.171570] Epoch: [20]  [  0/781]  eta: 0:11:32  lr: 0.000235  training_loss: 1.6376 (1.6376)  classification_loss: 1.5775 (1.5775)  loss_mask: 0.0601 (0.0601)  time: 0.8865  data: 0.7120  max mem: 6052
[03:45:20.585481] Epoch: [20]  [ 20/781]  eta: 0:02:35  lr: 0.000235  training_loss: 1.7088 (1.7001)  classification_loss: 1.6404 (1.6575)  loss_mask: 0.0343 (0.0426)  time: 0.1706  data: 0.0001  max mem: 6052
[03:45:24.019499] Epoch: [20]  [ 40/781]  eta: 0:02:19  lr: 0.000235  training_loss: 1.7065 (1.7103)  classification_loss: 1.6680 (1.6670)  loss_mask: 0.0338 (0.0433)  time: 0.1716  data: 0.0002  max mem: 6052
[03:45:27.446704] Epoch: [20]  [ 60/781]  eta: 0:02:11  lr: 0.000235  training_loss: 1.7262 (1.7094)  classification_loss: 1.6902 (1.6725)  loss_mask: 0.0218 (0.0369)  time: 0.1713  data: 0.0002  max mem: 6052
[03:45:30.882378] Epoch: [20]  [ 80/781]  eta: 0:02:06  lr: 0.000235  training_loss: 1.7141 (1.7096)  classification_loss: 1.6685 (1.6664)  loss_mask: 0.0316 (0.0432)  time: 0.1717  data: 0.0002  max mem: 6052
[03:45:34.324320] Epoch: [20]  [100/781]  eta: 0:02:01  lr: 0.000235  training_loss: 1.9112 (1.7555)  classification_loss: 1.6652 (1.6691)  loss_mask: 0.2294 (0.0863)  time: 0.1720  data: 0.0003  max mem: 6052
[03:45:37.781279] Epoch: [20]  [120/781]  eta: 0:01:57  lr: 0.000235  training_loss: 1.7313 (1.7539)  classification_loss: 1.6257 (1.6629)  loss_mask: 0.1049 (0.0910)  time: 0.1728  data: 0.0002  max mem: 6052
[03:45:41.201413] Epoch: [20]  [140/781]  eta: 0:01:53  lr: 0.000235  training_loss: 1.7588 (1.7536)  classification_loss: 1.6467 (1.6638)  loss_mask: 0.0682 (0.0898)  time: 0.1709  data: 0.0002  max mem: 6052
[03:45:44.691707] Epoch: [20]  [160/781]  eta: 0:01:49  lr: 0.000235  training_loss: 1.7521 (1.7541)  classification_loss: 1.7042 (1.6695)  loss_mask: 0.0443 (0.0846)  time: 0.1744  data: 0.0002  max mem: 6052
[03:45:48.115636] Epoch: [20]  [180/781]  eta: 0:01:45  lr: 0.000235  training_loss: 1.7261 (1.7537)  classification_loss: 1.6747 (1.6701)  loss_mask: 0.0362 (0.0836)  time: 0.1711  data: 0.0002  max mem: 6052
[03:45:51.538747] Epoch: [20]  [200/781]  eta: 0:01:41  lr: 0.000234  training_loss: 1.7856 (1.7594)  classification_loss: 1.7069 (1.6750)  loss_mask: 0.0559 (0.0844)  time: 0.1711  data: 0.0002  max mem: 6052
[03:45:54.954804] Epoch: [20]  [220/781]  eta: 0:01:38  lr: 0.000234  training_loss: 1.8068 (1.7622)  classification_loss: 1.7533 (1.6802)  loss_mask: 0.0499 (0.0820)  time: 0.1707  data: 0.0002  max mem: 6052
[03:45:58.372487] Epoch: [20]  [240/781]  eta: 0:01:34  lr: 0.000234  training_loss: 1.7939 (1.7636)  classification_loss: 1.6975 (1.6823)  loss_mask: 0.0580 (0.0813)  time: 0.1708  data: 0.0004  max mem: 6052
[03:46:01.784952] Epoch: [20]  [260/781]  eta: 0:01:30  lr: 0.000234  training_loss: 1.6955 (1.7576)  classification_loss: 1.6541 (1.6786)  loss_mask: 0.0413 (0.0790)  time: 0.1705  data: 0.0003  max mem: 6052
[03:46:05.233756] Epoch: [20]  [280/781]  eta: 0:01:27  lr: 0.000234  training_loss: 1.7408 (1.7611)  classification_loss: 1.7031 (1.6805)  loss_mask: 0.0680 (0.0805)  time: 0.1724  data: 0.0002  max mem: 6052
[03:46:08.639988] Epoch: [20]  [300/781]  eta: 0:01:23  lr: 0.000234  training_loss: 1.7715 (1.7623)  classification_loss: 1.7042 (1.6831)  loss_mask: 0.0509 (0.0792)  time: 0.1702  data: 0.0003  max mem: 6052
[03:46:12.139175] Epoch: [20]  [320/781]  eta: 0:01:20  lr: 0.000234  training_loss: 1.7629 (1.7611)  classification_loss: 1.7165 (1.6842)  loss_mask: 0.0373 (0.0769)  time: 0.1749  data: 0.0002  max mem: 6052
[03:46:15.592771] Epoch: [20]  [340/781]  eta: 0:01:16  lr: 0.000234  training_loss: 1.6800 (1.7569)  classification_loss: 1.6291 (1.6814)  loss_mask: 0.0366 (0.0755)  time: 0.1726  data: 0.0002  max mem: 6052
[03:46:19.007368] Epoch: [20]  [360/781]  eta: 0:01:13  lr: 0.000234  training_loss: 1.7001 (1.7540)  classification_loss: 1.6645 (1.6806)  loss_mask: 0.0277 (0.0734)  time: 0.1707  data: 0.0003  max mem: 6052
[03:46:22.422092] Epoch: [20]  [380/781]  eta: 0:01:09  lr: 0.000234  training_loss: 1.7315 (1.7537)  classification_loss: 1.6567 (1.6794)  loss_mask: 0.0707 (0.0743)  time: 0.1707  data: 0.0003  max mem: 6052
[03:46:25.846127] Epoch: [20]  [400/781]  eta: 0:01:06  lr: 0.000234  training_loss: 1.7669 (1.7542)  classification_loss: 1.7392 (1.6813)  loss_mask: 0.0388 (0.0729)  time: 0.1711  data: 0.0002  max mem: 6052
[03:46:29.248542] Epoch: [20]  [420/781]  eta: 0:01:02  lr: 0.000234  training_loss: 1.7499 (1.7538)  classification_loss: 1.6763 (1.6815)  loss_mask: 0.0568 (0.0723)  time: 0.1701  data: 0.0002  max mem: 6052
[03:46:32.673921] Epoch: [20]  [440/781]  eta: 0:00:59  lr: 0.000234  training_loss: 1.6717 (1.7519)  classification_loss: 1.6329 (1.6802)  loss_mask: 0.0453 (0.0717)  time: 0.1712  data: 0.0002  max mem: 6052
[03:46:36.116898] Epoch: [20]  [460/781]  eta: 0:00:55  lr: 0.000234  training_loss: 1.6378 (1.7485)  classification_loss: 1.6001 (1.6779)  loss_mask: 0.0399 (0.0706)  time: 0.1721  data: 0.0002  max mem: 6052
[03:46:39.540664] Epoch: [20]  [480/781]  eta: 0:00:52  lr: 0.000234  training_loss: 1.6883 (1.7463)  classification_loss: 1.6591 (1.6773)  loss_mask: 0.0246 (0.0690)  time: 0.1711  data: 0.0003  max mem: 6052
[03:46:42.973713] Epoch: [20]  [500/781]  eta: 0:00:48  lr: 0.000234  training_loss: 1.6919 (1.7444)  classification_loss: 1.6709 (1.6770)  loss_mask: 0.0214 (0.0674)  time: 0.1716  data: 0.0002  max mem: 6052
[03:46:46.393341] Epoch: [20]  [520/781]  eta: 0:00:45  lr: 0.000234  training_loss: 1.6844 (1.7422)  classification_loss: 1.6479 (1.6760)  loss_mask: 0.0366 (0.0662)  time: 0.1709  data: 0.0002  max mem: 6052
[03:46:49.802586] Epoch: [20]  [540/781]  eta: 0:00:41  lr: 0.000234  training_loss: 1.6527 (1.7389)  classification_loss: 1.5990 (1.6737)  loss_mask: 0.0325 (0.0652)  time: 0.1704  data: 0.0002  max mem: 6052
[03:46:53.215924] Epoch: [20]  [560/781]  eta: 0:00:38  lr: 0.000234  training_loss: 1.8072 (1.7425)  classification_loss: 1.6466 (1.6734)  loss_mask: 0.1138 (0.0690)  time: 0.1706  data: 0.0002  max mem: 6052
[03:46:56.642498] Epoch: [20]  [580/781]  eta: 0:00:34  lr: 0.000234  training_loss: 1.7800 (1.7442)  classification_loss: 1.6993 (1.6735)  loss_mask: 0.0812 (0.0708)  time: 0.1712  data: 0.0002  max mem: 6052
[03:47:00.072169] Epoch: [20]  [600/781]  eta: 0:00:31  lr: 0.000233  training_loss: 1.6831 (1.7422)  classification_loss: 1.6051 (1.6720)  loss_mask: 0.0530 (0.0702)  time: 0.1714  data: 0.0002  max mem: 6052
[03:47:03.485201] Epoch: [20]  [620/781]  eta: 0:00:27  lr: 0.000233  training_loss: 1.6980 (1.7410)  classification_loss: 1.6554 (1.6718)  loss_mask: 0.0318 (0.0691)  time: 0.1706  data: 0.0002  max mem: 6052
[03:47:06.905411] Epoch: [20]  [640/781]  eta: 0:00:24  lr: 0.000233  training_loss: 1.7283 (1.7412)  classification_loss: 1.7058 (1.6732)  loss_mask: 0.0252 (0.0679)  time: 0.1709  data: 0.0002  max mem: 6052
[03:47:10.323977] Epoch: [20]  [660/781]  eta: 0:00:20  lr: 0.000233  training_loss: 1.6808 (1.7395)  classification_loss: 1.6401 (1.6722)  loss_mask: 0.0439 (0.0673)  time: 0.1708  data: 0.0002  max mem: 6052
[03:47:13.728975] Epoch: [20]  [680/781]  eta: 0:00:17  lr: 0.000233  training_loss: 1.6962 (1.7382)  classification_loss: 1.6590 (1.6721)  loss_mask: 0.0263 (0.0661)  time: 0.1702  data: 0.0002  max mem: 6052
[03:47:17.130942] Epoch: [20]  [700/781]  eta: 0:00:13  lr: 0.000233  training_loss: 1.6897 (1.7372)  classification_loss: 1.6519 (1.6719)  loss_mask: 0.0361 (0.0653)  time: 0.1700  data: 0.0002  max mem: 6052
[03:47:20.539445] Epoch: [20]  [720/781]  eta: 0:00:10  lr: 0.000233  training_loss: 1.7079 (1.7363)  classification_loss: 1.6675 (1.6717)  loss_mask: 0.0378 (0.0646)  time: 0.1703  data: 0.0003  max mem: 6052
[03:47:23.941938] Epoch: [20]  [740/781]  eta: 0:00:07  lr: 0.000233  training_loss: 1.7082 (1.7364)  classification_loss: 1.6355 (1.6711)  loss_mask: 0.0470 (0.0654)  time: 0.1700  data: 0.0003  max mem: 6052
[03:47:27.362582] Epoch: [20]  [760/781]  eta: 0:00:03  lr: 0.000233  training_loss: 1.7968 (1.7384)  classification_loss: 1.6493 (1.6708)  loss_mask: 0.1098 (0.0676)  time: 0.1710  data: 0.0003  max mem: 6052
[03:47:30.760960] Epoch: [20]  [780/781]  eta: 0:00:00  lr: 0.000233  training_loss: 1.7038 (1.7379)  classification_loss: 1.6409 (1.6707)  loss_mask: 0.0475 (0.0672)  time: 0.1698  data: 0.0002  max mem: 6052
[03:47:30.915016] Epoch: [20] Total time: 0:02:14 (0.1724 s / it)
[03:47:30.915473] Averaged stats: lr: 0.000233  training_loss: 1.7038 (1.7379)  classification_loss: 1.6409 (1.6707)  loss_mask: 0.0475 (0.0672)
[03:47:32.373367] Test:  [  0/157]  eta: 0:01:49  testing_loss: 0.8596 (0.8596)  acc1: 71.8750 (71.8750)  acc5: 98.4375 (98.4375)  time: 0.6990  data: 0.6667  max mem: 6052
[03:47:32.661185] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.0133 (1.0243)  acc1: 62.5000 (63.7784)  acc5: 98.4375 (97.3011)  time: 0.0896  data: 0.0607  max mem: 6052
[03:47:32.950262] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.9893 (0.9797)  acc1: 65.6250 (66.6667)  acc5: 98.4375 (97.9167)  time: 0.0287  data: 0.0001  max mem: 6052
[03:47:33.237354] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.9803 (0.9897)  acc1: 70.3125 (66.9355)  acc5: 96.8750 (97.3286)  time: 0.0287  data: 0.0002  max mem: 6052
[03:47:33.520105] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.9976 (0.9948)  acc1: 68.7500 (66.7302)  acc5: 96.8750 (97.2180)  time: 0.0284  data: 0.0001  max mem: 6052
[03:47:33.809136] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.9934 (0.9919)  acc1: 67.1875 (66.8505)  acc5: 96.8750 (97.3652)  time: 0.0284  data: 0.0002  max mem: 6052
[03:47:34.095020] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.9583 (0.9883)  acc1: 67.1875 (66.8801)  acc5: 96.8750 (97.2080)  time: 0.0286  data: 0.0002  max mem: 6052
[03:47:34.377872] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.9392 (0.9861)  acc1: 67.1875 (66.7033)  acc5: 95.3125 (97.2491)  time: 0.0283  data: 0.0002  max mem: 6052
[03:47:34.659990] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.9615 (0.9903)  acc1: 68.7500 (66.8403)  acc5: 96.8750 (97.1644)  time: 0.0281  data: 0.0001  max mem: 6052
[03:47:34.945243] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.9785 (0.9910)  acc1: 68.7500 (66.9128)  acc5: 96.8750 (97.1669)  time: 0.0282  data: 0.0002  max mem: 6052
[03:47:35.232951] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.0276 (0.9961)  acc1: 64.0625 (66.5996)  acc5: 98.4375 (97.2308)  time: 0.0285  data: 0.0002  max mem: 6052
[03:47:35.514395] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.0645 (0.9980)  acc1: 62.5000 (66.3851)  acc5: 98.4375 (97.2410)  time: 0.0283  data: 0.0002  max mem: 6052
[03:47:35.801023] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.0031 (0.9941)  acc1: 65.6250 (66.4385)  acc5: 96.8750 (97.2495)  time: 0.0283  data: 0.0001  max mem: 6052
[03:47:36.083615] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.9983 (0.9965)  acc1: 65.6250 (66.2452)  acc5: 96.8750 (97.2328)  time: 0.0283  data: 0.0002  max mem: 6052
[03:47:36.367051] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.0035 (0.9962)  acc1: 65.6250 (66.4007)  acc5: 98.4375 (97.2074)  time: 0.0282  data: 0.0001  max mem: 6052
[03:47:36.644780] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.9637 (0.9933)  acc1: 68.7500 (66.5873)  acc5: 98.4375 (97.2475)  time: 0.0280  data: 0.0001  max mem: 6052
[03:47:36.794700] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.9634 (0.9956)  acc1: 68.7500 (66.4800)  acc5: 98.4375 (97.2700)  time: 0.0268  data: 0.0001  max mem: 6052
[03:47:36.961617] Test: Total time: 0:00:05 (0.0337 s / it)
[03:47:36.962531] * Acc@1 66.480 Acc@5 97.270 loss 0.996
[03:47:36.963465] Accuracy of the network on the 10000 test images: 66.5%
[03:47:36.963712] Max accuracy: 66.69%
[03:47:37.122864] log_dir: ./output_dir
[03:47:38.002436] Epoch: [21]  [  0/781]  eta: 0:11:25  lr: 0.000233  training_loss: 1.5572 (1.5572)  classification_loss: 1.5177 (1.5177)  loss_mask: 0.0396 (0.0396)  time: 0.8772  data: 0.6836  max mem: 6052
[03:47:41.464479] Epoch: [21]  [ 20/781]  eta: 0:02:37  lr: 0.000233  training_loss: 1.6293 (1.6584)  classification_loss: 1.5796 (1.5932)  loss_mask: 0.0607 (0.0652)  time: 0.1730  data: 0.0002  max mem: 6052
[03:47:44.881704] Epoch: [21]  [ 40/781]  eta: 0:02:20  lr: 0.000233  training_loss: 1.7008 (1.6921)  classification_loss: 1.6041 (1.6094)  loss_mask: 0.0725 (0.0826)  time: 0.1708  data: 0.0002  max mem: 6052
[03:47:48.307063] Epoch: [21]  [ 60/781]  eta: 0:02:12  lr: 0.000233  training_loss: 1.7875 (1.7270)  classification_loss: 1.7098 (1.6356)  loss_mask: 0.1113 (0.0914)  time: 0.1712  data: 0.0002  max mem: 6052
[03:47:51.725605] Epoch: [21]  [ 80/781]  eta: 0:02:06  lr: 0.000233  training_loss: 1.7717 (1.7310)  classification_loss: 1.6564 (1.6424)  loss_mask: 0.0683 (0.0886)  time: 0.1708  data: 0.0002  max mem: 6052
[03:47:55.148105] Epoch: [21]  [100/781]  eta: 0:02:01  lr: 0.000233  training_loss: 1.6676 (1.7239)  classification_loss: 1.6425 (1.6454)  loss_mask: 0.0313 (0.0785)  time: 0.1711  data: 0.0002  max mem: 6052
[03:47:58.584850] Epoch: [21]  [120/781]  eta: 0:01:57  lr: 0.000233  training_loss: 1.6842 (1.7175)  classification_loss: 1.6372 (1.6452)  loss_mask: 0.0344 (0.0723)  time: 0.1718  data: 0.0002  max mem: 6052
[03:48:01.998461] Epoch: [21]  [140/781]  eta: 0:01:53  lr: 0.000233  training_loss: 1.6721 (1.7125)  classification_loss: 1.6280 (1.6408)  loss_mask: 0.0645 (0.0717)  time: 0.1706  data: 0.0002  max mem: 6052
[03:48:05.424630] Epoch: [21]  [160/781]  eta: 0:01:49  lr: 0.000233  training_loss: 1.6704 (1.7090)  classification_loss: 1.6562 (1.6412)  loss_mask: 0.0313 (0.0678)  time: 0.1712  data: 0.0002  max mem: 6052
[03:48:08.843169] Epoch: [21]  [180/781]  eta: 0:01:45  lr: 0.000232  training_loss: 1.6433 (1.7023)  classification_loss: 1.5989 (1.6380)  loss_mask: 0.0342 (0.0643)  time: 0.1709  data: 0.0002  max mem: 6052
[03:48:12.248800] Epoch: [21]  [200/781]  eta: 0:01:41  lr: 0.000232  training_loss: 1.6955 (1.7019)  classification_loss: 1.6558 (1.6402)  loss_mask: 0.0355 (0.0617)  time: 0.1702  data: 0.0002  max mem: 6052
[03:48:15.681360] Epoch: [21]  [220/781]  eta: 0:01:37  lr: 0.000232  training_loss: 1.6516 (1.7003)  classification_loss: 1.6257 (1.6408)  loss_mask: 0.0273 (0.0595)  time: 0.1716  data: 0.0002  max mem: 6052
[03:48:19.147935] Epoch: [21]  [240/781]  eta: 0:01:34  lr: 0.000232  training_loss: 1.6103 (1.6961)  classification_loss: 1.5807 (1.6388)  loss_mask: 0.0267 (0.0573)  time: 0.1733  data: 0.0003  max mem: 6052
[03:48:22.558896] Epoch: [21]  [260/781]  eta: 0:01:30  lr: 0.000232  training_loss: 1.6923 (1.6984)  classification_loss: 1.6750 (1.6410)  loss_mask: 0.0511 (0.0573)  time: 0.1705  data: 0.0003  max mem: 6052
[03:48:25.988878] Epoch: [21]  [280/781]  eta: 0:01:27  lr: 0.000232  training_loss: 1.7343 (1.6995)  classification_loss: 1.7034 (1.6443)  loss_mask: 0.0276 (0.0552)  time: 0.1714  data: 0.0002  max mem: 6052
[03:48:29.394803] Epoch: [21]  [300/781]  eta: 0:01:23  lr: 0.000232  training_loss: 1.6554 (1.6988)  classification_loss: 1.6210 (1.6448)  loss_mask: 0.0236 (0.0540)  time: 0.1702  data: 0.0002  max mem: 6052
[03:48:32.800118] Epoch: [21]  [320/781]  eta: 0:01:19  lr: 0.000232  training_loss: 1.7683 (1.7016)  classification_loss: 1.6521 (1.6464)  loss_mask: 0.0498 (0.0552)  time: 0.1702  data: 0.0002  max mem: 6052
[03:48:36.272677] Epoch: [21]  [340/781]  eta: 0:01:16  lr: 0.000232  training_loss: 1.6736 (1.6998)  classification_loss: 1.6269 (1.6449)  loss_mask: 0.0304 (0.0549)  time: 0.1735  data: 0.0002  max mem: 6052
[03:48:39.694835] Epoch: [21]  [360/781]  eta: 0:01:12  lr: 0.000232  training_loss: 1.7352 (1.7020)  classification_loss: 1.6486 (1.6460)  loss_mask: 0.0564 (0.0560)  time: 0.1710  data: 0.0003  max mem: 6052
[03:48:43.107280] Epoch: [21]  [380/781]  eta: 0:01:09  lr: 0.000232  training_loss: 1.7153 (1.7031)  classification_loss: 1.6825 (1.6481)  loss_mask: 0.0291 (0.0550)  time: 0.1706  data: 0.0002  max mem: 6052
[03:48:46.520789] Epoch: [21]  [400/781]  eta: 0:01:05  lr: 0.000232  training_loss: 1.6518 (1.7015)  classification_loss: 1.6231 (1.6482)  loss_mask: 0.0171 (0.0533)  time: 0.1706  data: 0.0003  max mem: 6052
[03:48:49.941405] Epoch: [21]  [420/781]  eta: 0:01:02  lr: 0.000232  training_loss: 1.6737 (1.6999)  classification_loss: 1.6630 (1.6482)  loss_mask: 0.0142 (0.0516)  time: 0.1710  data: 0.0003  max mem: 6052
[03:48:53.355319] Epoch: [21]  [440/781]  eta: 0:00:58  lr: 0.000232  training_loss: 1.6252 (1.6968)  classification_loss: 1.5728 (1.6460)  loss_mask: 0.0190 (0.0508)  time: 0.1706  data: 0.0002  max mem: 6052
[03:48:56.793111] Epoch: [21]  [460/781]  eta: 0:00:55  lr: 0.000232  training_loss: 1.6827 (1.6971)  classification_loss: 1.6059 (1.6458)  loss_mask: 0.0306 (0.0513)  time: 0.1718  data: 0.0002  max mem: 6052
[03:49:00.237964] Epoch: [21]  [480/781]  eta: 0:00:51  lr: 0.000232  training_loss: 1.7469 (1.7011)  classification_loss: 1.6541 (1.6464)  loss_mask: 0.0872 (0.0548)  time: 0.1722  data: 0.0002  max mem: 6052
[03:49:03.662967] Epoch: [21]  [500/781]  eta: 0:00:48  lr: 0.000232  training_loss: 1.7317 (1.7018)  classification_loss: 1.6782 (1.6472)  loss_mask: 0.0323 (0.0546)  time: 0.1712  data: 0.0002  max mem: 6052
[03:49:07.077649] Epoch: [21]  [520/781]  eta: 0:00:45  lr: 0.000232  training_loss: 1.6685 (1.7011)  classification_loss: 1.6172 (1.6463)  loss_mask: 0.0496 (0.0548)  time: 0.1707  data: 0.0002  max mem: 6052
[03:49:10.500400] Epoch: [21]  [540/781]  eta: 0:00:41  lr: 0.000232  training_loss: 1.7362 (1.7026)  classification_loss: 1.6523 (1.6476)  loss_mask: 0.0580 (0.0550)  time: 0.1711  data: 0.0002  max mem: 6052
[03:49:13.905304] Epoch: [21]  [560/781]  eta: 0:00:38  lr: 0.000231  training_loss: 1.7089 (1.7029)  classification_loss: 1.6370 (1.6472)  loss_mask: 0.0522 (0.0557)  time: 0.1702  data: 0.0002  max mem: 6052
[03:49:17.325474] Epoch: [21]  [580/781]  eta: 0:00:34  lr: 0.000231  training_loss: 1.6538 (1.7019)  classification_loss: 1.6311 (1.6468)  loss_mask: 0.0327 (0.0551)  time: 0.1709  data: 0.0002  max mem: 6052
[03:49:20.756535] Epoch: [21]  [600/781]  eta: 0:00:31  lr: 0.000231  training_loss: 1.5985 (1.7002)  classification_loss: 1.5828 (1.6462)  loss_mask: 0.0190 (0.0540)  time: 0.1715  data: 0.0002  max mem: 6052
[03:49:24.178305] Epoch: [21]  [620/781]  eta: 0:00:27  lr: 0.000231  training_loss: 1.6035 (1.6975)  classification_loss: 1.5796 (1.6447)  loss_mask: 0.0163 (0.0529)  time: 0.1710  data: 0.0002  max mem: 6052
[03:49:27.581006] Epoch: [21]  [640/781]  eta: 0:00:24  lr: 0.000231  training_loss: 1.5986 (1.6951)  classification_loss: 1.5869 (1.6434)  loss_mask: 0.0112 (0.0516)  time: 0.1701  data: 0.0002  max mem: 6052
[03:49:30.999407] Epoch: [21]  [660/781]  eta: 0:00:20  lr: 0.000231  training_loss: 1.6990 (1.6954)  classification_loss: 1.6825 (1.6447)  loss_mask: 0.0163 (0.0507)  time: 0.1708  data: 0.0002  max mem: 6052
[03:49:34.431071] Epoch: [21]  [680/781]  eta: 0:00:17  lr: 0.000231  training_loss: 1.6566 (1.6947)  classification_loss: 1.6247 (1.6448)  loss_mask: 0.0169 (0.0499)  time: 0.1715  data: 0.0003  max mem: 6052
[03:49:37.855525] Epoch: [21]  [700/781]  eta: 0:00:13  lr: 0.000231  training_loss: 1.6787 (1.6944)  classification_loss: 1.6596 (1.6450)  loss_mask: 0.0298 (0.0495)  time: 0.1711  data: 0.0002  max mem: 6052
[03:49:41.262682] Epoch: [21]  [720/781]  eta: 0:00:10  lr: 0.000231  training_loss: 1.7297 (1.6951)  classification_loss: 1.6320 (1.6448)  loss_mask: 0.0594 (0.0503)  time: 0.1703  data: 0.0002  max mem: 6052
[03:49:44.665304] Epoch: [21]  [740/781]  eta: 0:00:07  lr: 0.000231  training_loss: 1.6626 (1.6944)  classification_loss: 1.6248 (1.6440)  loss_mask: 0.0387 (0.0504)  time: 0.1701  data: 0.0002  max mem: 6052
[03:49:48.093018] Epoch: [21]  [760/781]  eta: 0:00:03  lr: 0.000231  training_loss: 1.7207 (1.6956)  classification_loss: 1.6315 (1.6437)  loss_mask: 0.0874 (0.0519)  time: 0.1713  data: 0.0003  max mem: 6052
[03:49:51.503268] Epoch: [21]  [780/781]  eta: 0:00:00  lr: 0.000231  training_loss: 1.7913 (1.6977)  classification_loss: 1.6513 (1.6442)  loss_mask: 0.0857 (0.0534)  time: 0.1704  data: 0.0002  max mem: 6052
[03:49:51.671616] Epoch: [21] Total time: 0:02:14 (0.1723 s / it)
[03:49:51.672071] Averaged stats: lr: 0.000231  training_loss: 1.7913 (1.6977)  classification_loss: 1.6513 (1.6442)  loss_mask: 0.0857 (0.0534)
[03:49:52.317066] Test:  [  0/157]  eta: 0:01:40  testing_loss: 0.8590 (0.8590)  acc1: 73.4375 (73.4375)  acc5: 96.8750 (96.8750)  time: 0.6389  data: 0.6017  max mem: 6052
[03:49:52.608944] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.0380 (1.0017)  acc1: 67.1875 (66.4773)  acc5: 96.8750 (97.7273)  time: 0.0842  data: 0.0549  max mem: 6052
[03:49:52.900993] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.9664 (0.9791)  acc1: 67.1875 (68.0804)  acc5: 98.4375 (98.0655)  time: 0.0289  data: 0.0002  max mem: 6052
[03:49:53.182322] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.9789 (0.9850)  acc1: 70.3125 (68.0948)  acc5: 98.4375 (97.5302)  time: 0.0285  data: 0.0001  max mem: 6052
[03:49:53.469477] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.9789 (0.9853)  acc1: 68.7500 (67.8354)  acc5: 96.8750 (97.2561)  time: 0.0283  data: 0.0002  max mem: 6052
[03:49:53.751223] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.9414 (0.9813)  acc1: 67.1875 (67.7696)  acc5: 96.8750 (97.0895)  time: 0.0283  data: 0.0002  max mem: 6052
[03:49:54.034092] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.9544 (0.9791)  acc1: 67.1875 (67.6742)  acc5: 96.8750 (97.1311)  time: 0.0281  data: 0.0002  max mem: 6052
[03:49:54.319853] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.9372 (0.9734)  acc1: 67.1875 (67.7377)  acc5: 98.4375 (97.2271)  time: 0.0283  data: 0.0002  max mem: 6052
[03:49:54.605346] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.9521 (0.9773)  acc1: 67.1875 (67.4961)  acc5: 96.8750 (97.1451)  time: 0.0284  data: 0.0002  max mem: 6052
[03:49:54.889491] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.9893 (0.9811)  acc1: 67.1875 (67.3935)  acc5: 96.8750 (97.0639)  time: 0.0284  data: 0.0002  max mem: 6052
[03:49:55.176511] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.0025 (0.9842)  acc1: 67.1875 (67.4041)  acc5: 96.8750 (97.1380)  time: 0.0284  data: 0.0002  max mem: 6052
[03:49:55.462008] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.0293 (0.9863)  acc1: 67.1875 (67.2579)  acc5: 98.4375 (97.1847)  time: 0.0285  data: 0.0002  max mem: 6052
[03:49:55.744989] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.9856 (0.9829)  acc1: 65.6250 (67.3166)  acc5: 96.8750 (97.1978)  time: 0.0283  data: 0.0002  max mem: 6052
[03:49:56.027516] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.9731 (0.9846)  acc1: 67.1875 (67.1636)  acc5: 96.8750 (97.1851)  time: 0.0282  data: 0.0001  max mem: 6052
[03:49:56.310274] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.9754 (0.9828)  acc1: 67.1875 (67.2983)  acc5: 98.4375 (97.2185)  time: 0.0282  data: 0.0001  max mem: 6052
[03:49:56.588644] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.9620 (0.9791)  acc1: 68.7500 (67.5186)  acc5: 96.8750 (97.2372)  time: 0.0279  data: 0.0001  max mem: 6052
[03:49:56.738009] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.9544 (0.9805)  acc1: 68.7500 (67.4100)  acc5: 98.4375 (97.2600)  time: 0.0268  data: 0.0001  max mem: 6052
[03:49:56.914138] Test: Total time: 0:00:05 (0.0334 s / it)
[03:49:56.914726] * Acc@1 67.410 Acc@5 97.260 loss 0.980
[03:49:56.915243] Accuracy of the network on the 10000 test images: 67.4%
[03:49:56.915643] Max accuracy: 67.41%
[03:49:57.067379] log_dir: ./output_dir
[03:49:57.956769] Epoch: [22]  [  0/781]  eta: 0:11:32  lr: 0.000231  training_loss: 1.5818 (1.5818)  classification_loss: 1.4923 (1.4923)  loss_mask: 0.0895 (0.0895)  time: 0.8868  data: 0.7087  max mem: 6052
[03:50:01.386861] Epoch: [22]  [ 20/781]  eta: 0:02:36  lr: 0.000231  training_loss: 1.6609 (1.6593)  classification_loss: 1.6001 (1.6087)  loss_mask: 0.0421 (0.0506)  time: 0.1714  data: 0.0003  max mem: 6052
[03:50:04.820507] Epoch: [22]  [ 40/781]  eta: 0:02:20  lr: 0.000231  training_loss: 1.6431 (1.6562)  classification_loss: 1.6170 (1.6169)  loss_mask: 0.0223 (0.0393)  time: 0.1716  data: 0.0003  max mem: 6052
[03:50:08.257308] Epoch: [22]  [ 60/781]  eta: 0:02:12  lr: 0.000231  training_loss: 1.7145 (1.6879)  classification_loss: 1.6449 (1.6273)  loss_mask: 0.0611 (0.0606)  time: 0.1718  data: 0.0003  max mem: 6052
[03:50:11.680685] Epoch: [22]  [ 80/781]  eta: 0:02:06  lr: 0.000231  training_loss: 1.7743 (1.7247)  classification_loss: 1.6302 (1.6335)  loss_mask: 0.1676 (0.0912)  time: 0.1711  data: 0.0002  max mem: 6052
[03:50:15.112168] Epoch: [22]  [100/781]  eta: 0:02:01  lr: 0.000231  training_loss: 1.7545 (1.7347)  classification_loss: 1.6148 (1.6321)  loss_mask: 0.1013 (0.1027)  time: 0.1715  data: 0.0002  max mem: 6052
[03:50:18.525377] Epoch: [22]  [120/781]  eta: 0:01:57  lr: 0.000231  training_loss: 1.6544 (1.7257)  classification_loss: 1.6093 (1.6303)  loss_mask: 0.0528 (0.0954)  time: 0.1706  data: 0.0003  max mem: 6052
[03:50:21.936284] Epoch: [22]  [140/781]  eta: 0:01:52  lr: 0.000230  training_loss: 1.6259 (1.7139)  classification_loss: 1.5939 (1.6253)  loss_mask: 0.0396 (0.0886)  time: 0.1705  data: 0.0003  max mem: 6052
[03:50:25.352563] Epoch: [22]  [160/781]  eta: 0:01:49  lr: 0.000230  training_loss: 1.6759 (1.7089)  classification_loss: 1.6289 (1.6263)  loss_mask: 0.0342 (0.0825)  time: 0.1707  data: 0.0002  max mem: 6052
[03:50:28.786970] Epoch: [22]  [180/781]  eta: 0:01:45  lr: 0.000230  training_loss: 1.6495 (1.7038)  classification_loss: 1.6064 (1.6258)  loss_mask: 0.0333 (0.0780)  time: 0.1716  data: 0.0002  max mem: 6052
[03:50:32.198510] Epoch: [22]  [200/781]  eta: 0:01:41  lr: 0.000230  training_loss: 1.6626 (1.7018)  classification_loss: 1.5622 (1.6217)  loss_mask: 0.0695 (0.0800)  time: 0.1705  data: 0.0002  max mem: 6052
[03:50:35.628560] Epoch: [22]  [220/781]  eta: 0:01:37  lr: 0.000230  training_loss: 1.7350 (1.7075)  classification_loss: 1.6836 (1.6252)  loss_mask: 0.0941 (0.0823)  time: 0.1714  data: 0.0002  max mem: 6052
[03:50:39.113900] Epoch: [22]  [240/781]  eta: 0:01:34  lr: 0.000230  training_loss: 1.7330 (1.7080)  classification_loss: 1.6349 (1.6260)  loss_mask: 0.0463 (0.0820)  time: 0.1742  data: 0.0002  max mem: 6052
[03:50:42.525342] Epoch: [22]  [260/781]  eta: 0:01:30  lr: 0.000230  training_loss: 1.7004 (1.7067)  classification_loss: 1.6089 (1.6258)  loss_mask: 0.0508 (0.0809)  time: 0.1705  data: 0.0002  max mem: 6052
[03:50:45.942971] Epoch: [22]  [280/781]  eta: 0:01:27  lr: 0.000230  training_loss: 1.6517 (1.7040)  classification_loss: 1.6190 (1.6261)  loss_mask: 0.0356 (0.0779)  time: 0.1708  data: 0.0002  max mem: 6052
[03:50:49.374644] Epoch: [22]  [300/781]  eta: 0:01:23  lr: 0.000230  training_loss: 1.6889 (1.7027)  classification_loss: 1.6487 (1.6278)  loss_mask: 0.0241 (0.0749)  time: 0.1715  data: 0.0002  max mem: 6052
[03:50:52.806670] Epoch: [22]  [320/781]  eta: 0:01:20  lr: 0.000230  training_loss: 1.6460 (1.7005)  classification_loss: 1.6289 (1.6287)  loss_mask: 0.0216 (0.0718)  time: 0.1715  data: 0.0002  max mem: 6052
[03:50:56.236919] Epoch: [22]  [340/781]  eta: 0:01:16  lr: 0.000230  training_loss: 1.6358 (1.6971)  classification_loss: 1.6093 (1.6276)  loss_mask: 0.0242 (0.0695)  time: 0.1714  data: 0.0002  max mem: 6052
[03:50:59.787134] Epoch: [22]  [360/781]  eta: 0:01:13  lr: 0.000230  training_loss: 1.6682 (1.6955)  classification_loss: 1.6218 (1.6280)  loss_mask: 0.0281 (0.0675)  time: 0.1774  data: 0.0003  max mem: 6052
[03:51:03.346547] Epoch: [22]  [380/781]  eta: 0:01:09  lr: 0.000230  training_loss: 1.6221 (1.6925)  classification_loss: 1.6032 (1.6272)  loss_mask: 0.0238 (0.0653)  time: 0.1779  data: 0.0003  max mem: 6052
[03:51:06.779364] Epoch: [22]  [400/781]  eta: 0:01:06  lr: 0.000230  training_loss: 1.6903 (1.6941)  classification_loss: 1.5938 (1.6258)  loss_mask: 0.0362 (0.0683)  time: 0.1716  data: 0.0003  max mem: 6052
[03:51:10.213514] Epoch: [22]  [420/781]  eta: 0:01:02  lr: 0.000230  training_loss: 1.8503 (1.7037)  classification_loss: 1.6782 (1.6291)  loss_mask: 0.1331 (0.0745)  time: 0.1716  data: 0.0002  max mem: 6052
[03:51:13.647032] Epoch: [22]  [440/781]  eta: 0:00:59  lr: 0.000230  training_loss: 1.7669 (1.7086)  classification_loss: 1.7166 (1.6326)  loss_mask: 0.0527 (0.0760)  time: 0.1716  data: 0.0003  max mem: 6052
[03:51:17.069611] Epoch: [22]  [460/781]  eta: 0:00:55  lr: 0.000230  training_loss: 1.7168 (1.7097)  classification_loss: 1.6788 (1.6346)  loss_mask: 0.0408 (0.0750)  time: 0.1711  data: 0.0002  max mem: 6052
[03:51:20.483954] Epoch: [22]  [480/781]  eta: 0:00:52  lr: 0.000229  training_loss: 1.7276 (1.7104)  classification_loss: 1.6632 (1.6364)  loss_mask: 0.0365 (0.0740)  time: 0.1707  data: 0.0002  max mem: 6052
[03:51:23.905577] Epoch: [22]  [500/781]  eta: 0:00:48  lr: 0.000229  training_loss: 1.6545 (1.7083)  classification_loss: 1.6028 (1.6359)  loss_mask: 0.0277 (0.0724)  time: 0.1710  data: 0.0002  max mem: 6052
[03:51:27.347189] Epoch: [22]  [520/781]  eta: 0:00:45  lr: 0.000229  training_loss: 1.6796 (1.7073)  classification_loss: 1.5896 (1.6358)  loss_mask: 0.0493 (0.0715)  time: 0.1720  data: 0.0003  max mem: 6052
[03:51:30.780862] Epoch: [22]  [540/781]  eta: 0:00:41  lr: 0.000229  training_loss: 1.7143 (1.7080)  classification_loss: 1.6913 (1.6376)  loss_mask: 0.0264 (0.0704)  time: 0.1716  data: 0.0002  max mem: 6052
[03:51:34.202461] Epoch: [22]  [560/781]  eta: 0:00:38  lr: 0.000229  training_loss: 1.6918 (1.7068)  classification_loss: 1.6255 (1.6371)  loss_mask: 0.0484 (0.0697)  time: 0.1710  data: 0.0002  max mem: 6052
[03:51:37.627595] Epoch: [22]  [580/781]  eta: 0:00:34  lr: 0.000229  training_loss: 1.6896 (1.7070)  classification_loss: 1.6311 (1.6370)  loss_mask: 0.0518 (0.0700)  time: 0.1712  data: 0.0003  max mem: 6052
[03:51:41.039980] Epoch: [22]  [600/781]  eta: 0:00:31  lr: 0.000229  training_loss: 1.6697 (1.7066)  classification_loss: 1.6033 (1.6368)  loss_mask: 0.0436 (0.0698)  time: 0.1705  data: 0.0002  max mem: 6052
[03:51:44.472559] Epoch: [22]  [620/781]  eta: 0:00:27  lr: 0.000229  training_loss: 1.6424 (1.7053)  classification_loss: 1.6130 (1.6368)  loss_mask: 0.0259 (0.0685)  time: 0.1715  data: 0.0002  max mem: 6052
[03:51:47.917525] Epoch: [22]  [640/781]  eta: 0:00:24  lr: 0.000229  training_loss: 1.7518 (1.7072)  classification_loss: 1.6685 (1.6383)  loss_mask: 0.0618 (0.0689)  time: 0.1721  data: 0.0002  max mem: 6052
[03:51:51.350145] Epoch: [22]  [660/781]  eta: 0:00:20  lr: 0.000229  training_loss: 1.7837 (1.7088)  classification_loss: 1.6484 (1.6390)  loss_mask: 0.0797 (0.0698)  time: 0.1715  data: 0.0003  max mem: 6052
[03:51:54.773443] Epoch: [22]  [680/781]  eta: 0:00:17  lr: 0.000229  training_loss: 1.6572 (1.7069)  classification_loss: 1.6208 (1.6380)  loss_mask: 0.0333 (0.0690)  time: 0.1711  data: 0.0002  max mem: 6052
[03:51:58.183139] Epoch: [22]  [700/781]  eta: 0:00:13  lr: 0.000229  training_loss: 1.6512 (1.7058)  classification_loss: 1.6219 (1.6377)  loss_mask: 0.0321 (0.0681)  time: 0.1704  data: 0.0002  max mem: 6052
[03:52:01.588475] Epoch: [22]  [720/781]  eta: 0:00:10  lr: 0.000229  training_loss: 1.7017 (1.7059)  classification_loss: 1.5942 (1.6373)  loss_mask: 0.0575 (0.0686)  time: 0.1702  data: 0.0002  max mem: 6052
[03:52:05.002636] Epoch: [22]  [740/781]  eta: 0:00:07  lr: 0.000229  training_loss: 1.7225 (1.7073)  classification_loss: 1.5840 (1.6365)  loss_mask: 0.1376 (0.0709)  time: 0.1706  data: 0.0002  max mem: 6052
[03:52:08.405056] Epoch: [22]  [760/781]  eta: 0:00:03  lr: 0.000229  training_loss: 1.7084 (1.7084)  classification_loss: 1.6318 (1.6367)  loss_mask: 0.0662 (0.0717)  time: 0.1700  data: 0.0001  max mem: 6052
[03:52:11.827040] Epoch: [22]  [780/781]  eta: 0:00:00  lr: 0.000229  training_loss: 1.6951 (1.7088)  classification_loss: 1.6491 (1.6374)  loss_mask: 0.0509 (0.0714)  time: 0.1710  data: 0.0002  max mem: 6052
[03:52:12.006640] Epoch: [22] Total time: 0:02:14 (0.1728 s / it)
[03:52:12.007103] Averaged stats: lr: 0.000229  training_loss: 1.6951 (1.7088)  classification_loss: 1.6491 (1.6374)  loss_mask: 0.0509 (0.0714)
[03:52:12.695853] Test:  [  0/157]  eta: 0:01:47  testing_loss: 0.9285 (0.9285)  acc1: 73.4375 (73.4375)  acc5: 93.7500 (93.7500)  time: 0.6849  data: 0.6548  max mem: 6052
[03:52:12.984824] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.9936 (0.9883)  acc1: 65.6250 (66.3352)  acc5: 96.8750 (97.4432)  time: 0.0884  data: 0.0603  max mem: 6052
[03:52:13.272221] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.9594 (0.9519)  acc1: 67.1875 (67.7083)  acc5: 96.8750 (97.6935)  time: 0.0287  data: 0.0005  max mem: 6052
[03:52:13.557826] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.9349 (0.9536)  acc1: 67.1875 (67.4899)  acc5: 96.8750 (97.2782)  time: 0.0285  data: 0.0002  max mem: 6052
[03:52:13.839593] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.9337 (0.9526)  acc1: 68.7500 (67.9497)  acc5: 96.8750 (97.3704)  time: 0.0282  data: 0.0002  max mem: 6052
[03:52:14.125263] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.9301 (0.9534)  acc1: 68.7500 (67.8615)  acc5: 98.4375 (97.4877)  time: 0.0282  data: 0.0001  max mem: 6052
[03:52:14.410972] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.9300 (0.9510)  acc1: 68.7500 (67.9559)  acc5: 96.8750 (97.4898)  time: 0.0284  data: 0.0002  max mem: 6052
[03:52:14.694025] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.9052 (0.9420)  acc1: 70.3125 (68.3099)  acc5: 96.8750 (97.5572)  time: 0.0283  data: 0.0002  max mem: 6052
[03:52:14.978395] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.9152 (0.9450)  acc1: 70.3125 (68.2870)  acc5: 98.4375 (97.5309)  time: 0.0283  data: 0.0002  max mem: 6052
[03:52:15.261024] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.9804 (0.9464)  acc1: 67.1875 (68.2692)  acc5: 98.4375 (97.5446)  time: 0.0282  data: 0.0002  max mem: 6052
[03:52:15.542658] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.9763 (0.9507)  acc1: 65.6250 (68.1157)  acc5: 98.4375 (97.6021)  time: 0.0281  data: 0.0002  max mem: 6052
[03:52:15.823864] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.9914 (0.9539)  acc1: 65.6250 (67.9336)  acc5: 98.4375 (97.5788)  time: 0.0280  data: 0.0002  max mem: 6052
[03:52:16.104728] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.9571 (0.9509)  acc1: 65.6250 (67.9494)  acc5: 96.8750 (97.5465)  time: 0.0280  data: 0.0002  max mem: 6052
[03:52:16.388294] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.9450 (0.9520)  acc1: 68.7500 (67.9628)  acc5: 98.4375 (97.5549)  time: 0.0281  data: 0.0001  max mem: 6052
[03:52:16.672423] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.9522 (0.9505)  acc1: 68.7500 (68.1516)  acc5: 98.4375 (97.5953)  time: 0.0283  data: 0.0001  max mem: 6052
[03:52:16.950440] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.9630 (0.9476)  acc1: 70.3125 (68.2740)  acc5: 96.8750 (97.5476)  time: 0.0280  data: 0.0001  max mem: 6052
[03:52:17.099720] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.9326 (0.9488)  acc1: 70.3125 (68.2800)  acc5: 96.8750 (97.5500)  time: 0.0268  data: 0.0001  max mem: 6052
[03:52:17.261280] Test: Total time: 0:00:05 (0.0334 s / it)
[03:52:17.261900] * Acc@1 68.280 Acc@5 97.550 loss 0.949
[03:52:17.262193] Accuracy of the network on the 10000 test images: 68.3%
[03:52:17.262385] Max accuracy: 68.28%
[03:52:17.379573] log_dir: ./output_dir
[03:52:18.250806] Epoch: [23]  [  0/781]  eta: 0:11:19  lr: 0.000229  training_loss: 1.6346 (1.6346)  classification_loss: 1.5888 (1.5888)  loss_mask: 0.0459 (0.0459)  time: 0.8696  data: 0.6921  max mem: 6052
[03:52:21.676999] Epoch: [23]  [ 20/781]  eta: 0:02:35  lr: 0.000229  training_loss: 1.6620 (1.6515)  classification_loss: 1.6243 (1.6181)  loss_mask: 0.0267 (0.0334)  time: 0.1712  data: 0.0004  max mem: 6052
[03:52:25.112700] Epoch: [23]  [ 40/781]  eta: 0:02:19  lr: 0.000228  training_loss: 1.6129 (1.6516)  classification_loss: 1.6000 (1.6204)  loss_mask: 0.0226 (0.0312)  time: 0.1717  data: 0.0006  max mem: 6052
[03:52:28.528130] Epoch: [23]  [ 60/781]  eta: 0:02:11  lr: 0.000228  training_loss: 1.6566 (1.6587)  classification_loss: 1.6286 (1.6315)  loss_mask: 0.0160 (0.0272)  time: 0.1707  data: 0.0002  max mem: 6052
[03:52:31.956001] Epoch: [23]  [ 80/781]  eta: 0:02:06  lr: 0.000228  training_loss: 1.6251 (1.6526)  classification_loss: 1.6122 (1.6275)  loss_mask: 0.0152 (0.0251)  time: 0.1713  data: 0.0003  max mem: 6052
[03:52:35.431898] Epoch: [23]  [100/781]  eta: 0:02:01  lr: 0.000228  training_loss: 1.6240 (1.6495)  classification_loss: 1.6100 (1.6245)  loss_mask: 0.0171 (0.0250)  time: 0.1737  data: 0.0002  max mem: 6052
[03:52:38.995513] Epoch: [23]  [120/781]  eta: 0:01:58  lr: 0.000228  training_loss: 1.6174 (1.6456)  classification_loss: 1.5921 (1.6205)  loss_mask: 0.0254 (0.0251)  time: 0.1781  data: 0.0003  max mem: 6052
[03:52:42.519937] Epoch: [23]  [140/781]  eta: 0:01:54  lr: 0.000228  training_loss: 1.6147 (1.6437)  classification_loss: 1.5911 (1.6183)  loss_mask: 0.0249 (0.0255)  time: 0.1761  data: 0.0003  max mem: 6052
[03:52:45.955934] Epoch: [23]  [160/781]  eta: 0:01:50  lr: 0.000228  training_loss: 1.7089 (1.6496)  classification_loss: 1.6512 (1.6234)  loss_mask: 0.0194 (0.0262)  time: 0.1717  data: 0.0003  max mem: 6052
[03:52:49.363735] Epoch: [23]  [180/781]  eta: 0:01:46  lr: 0.000228  training_loss: 1.6735 (1.6522)  classification_loss: 1.5888 (1.6228)  loss_mask: 0.0400 (0.0295)  time: 0.1703  data: 0.0002  max mem: 6052
[03:52:52.852458] Epoch: [23]  [200/781]  eta: 0:01:42  lr: 0.000228  training_loss: 1.6290 (1.6489)  classification_loss: 1.5960 (1.6187)  loss_mask: 0.0210 (0.0301)  time: 0.1743  data: 0.0002  max mem: 6052
[03:52:56.265134] Epoch: [23]  [220/781]  eta: 0:01:38  lr: 0.000228  training_loss: 1.6287 (1.6504)  classification_loss: 1.6173 (1.6200)  loss_mask: 0.0197 (0.0304)  time: 0.1706  data: 0.0003  max mem: 6052
[03:52:59.678261] Epoch: [23]  [240/781]  eta: 0:01:34  lr: 0.000228  training_loss: 1.6639 (1.6522)  classification_loss: 1.6358 (1.6224)  loss_mask: 0.0222 (0.0298)  time: 0.1706  data: 0.0002  max mem: 6052
[03:53:03.152503] Epoch: [23]  [260/781]  eta: 0:01:31  lr: 0.000228  training_loss: 1.6611 (1.6534)  classification_loss: 1.6262 (1.6227)  loss_mask: 0.0347 (0.0307)  time: 0.1736  data: 0.0005  max mem: 6052
[03:53:06.637367] Epoch: [23]  [280/781]  eta: 0:01:27  lr: 0.000228  training_loss: 1.6243 (1.6520)  classification_loss: 1.5893 (1.6216)  loss_mask: 0.0179 (0.0303)  time: 0.1742  data: 0.0003  max mem: 6052
[03:53:10.166903] Epoch: [23]  [300/781]  eta: 0:01:24  lr: 0.000228  training_loss: 1.6634 (1.6538)  classification_loss: 1.6363 (1.6230)  loss_mask: 0.0271 (0.0308)  time: 0.1764  data: 0.0002  max mem: 6052
[03:53:13.656825] Epoch: [23]  [320/781]  eta: 0:01:20  lr: 0.000228  training_loss: 1.7773 (1.6601)  classification_loss: 1.6048 (1.6234)  loss_mask: 0.0759 (0.0366)  time: 0.1744  data: 0.0003  max mem: 6052
[03:53:17.144845] Epoch: [23]  [340/781]  eta: 0:01:17  lr: 0.000228  training_loss: 1.6286 (1.6578)  classification_loss: 1.5848 (1.6209)  loss_mask: 0.0347 (0.0369)  time: 0.1743  data: 0.0002  max mem: 6052
[03:53:20.586726] Epoch: [23]  [360/781]  eta: 0:01:13  lr: 0.000228  training_loss: 1.6523 (1.6577)  classification_loss: 1.6184 (1.6211)  loss_mask: 0.0234 (0.0365)  time: 0.1720  data: 0.0005  max mem: 6052
[03:53:24.008451] Epoch: [23]  [380/781]  eta: 0:01:10  lr: 0.000227  training_loss: 1.6139 (1.6558)  classification_loss: 1.5889 (1.6201)  loss_mask: 0.0178 (0.0357)  time: 0.1710  data: 0.0003  max mem: 6052
[03:53:27.418760] Epoch: [23]  [400/781]  eta: 0:01:06  lr: 0.000227  training_loss: 1.6204 (1.6541)  classification_loss: 1.5749 (1.6188)  loss_mask: 0.0215 (0.0354)  time: 0.1704  data: 0.0002  max mem: 6052
[03:53:30.846578] Epoch: [23]  [420/781]  eta: 0:01:02  lr: 0.000227  training_loss: 1.6506 (1.6540)  classification_loss: 1.6069 (1.6185)  loss_mask: 0.0297 (0.0354)  time: 0.1713  data: 0.0002  max mem: 6052
[03:53:34.312326] Epoch: [23]  [440/781]  eta: 0:00:59  lr: 0.000227  training_loss: 1.6831 (1.6553)  classification_loss: 1.6471 (1.6195)  loss_mask: 0.0393 (0.0358)  time: 0.1732  data: 0.0002  max mem: 6052
[03:53:37.726223] Epoch: [23]  [460/781]  eta: 0:00:55  lr: 0.000227  training_loss: 1.6830 (1.6555)  classification_loss: 1.6255 (1.6194)  loss_mask: 0.0397 (0.0361)  time: 0.1706  data: 0.0002  max mem: 6052
[03:53:41.126471] Epoch: [23]  [480/781]  eta: 0:00:52  lr: 0.000227  training_loss: 1.6203 (1.6549)  classification_loss: 1.5977 (1.6195)  loss_mask: 0.0166 (0.0354)  time: 0.1699  data: 0.0003  max mem: 6052
[03:53:44.526073] Epoch: [23]  [500/781]  eta: 0:00:48  lr: 0.000227  training_loss: 1.6198 (1.6542)  classification_loss: 1.6119 (1.6197)  loss_mask: 0.0115 (0.0345)  time: 0.1699  data: 0.0003  max mem: 6052
[03:53:47.986292] Epoch: [23]  [520/781]  eta: 0:00:45  lr: 0.000227  training_loss: 1.6280 (1.6530)  classification_loss: 1.5961 (1.6191)  loss_mask: 0.0110 (0.0339)  time: 0.1729  data: 0.0002  max mem: 6052
[03:53:51.409733] Epoch: [23]  [540/781]  eta: 0:00:41  lr: 0.000227  training_loss: 1.6349 (1.6529)  classification_loss: 1.6051 (1.6194)  loss_mask: 0.0112 (0.0334)  time: 0.1711  data: 0.0002  max mem: 6052
[03:53:54.832011] Epoch: [23]  [560/781]  eta: 0:00:38  lr: 0.000227  training_loss: 1.6132 (1.6523)  classification_loss: 1.5767 (1.6186)  loss_mask: 0.0316 (0.0338)  time: 0.1710  data: 0.0002  max mem: 6052
[03:53:58.244473] Epoch: [23]  [580/781]  eta: 0:00:34  lr: 0.000227  training_loss: 1.6232 (1.6519)  classification_loss: 1.5863 (1.6179)  loss_mask: 0.0294 (0.0340)  time: 0.1706  data: 0.0002  max mem: 6052
[03:54:01.655478] Epoch: [23]  [600/781]  eta: 0:00:31  lr: 0.000227  training_loss: 1.5863 (1.6518)  classification_loss: 1.5668 (1.6175)  loss_mask: 0.0350 (0.0343)  time: 0.1705  data: 0.0003  max mem: 6052
[03:54:05.075059] Epoch: [23]  [620/781]  eta: 0:00:27  lr: 0.000227  training_loss: 1.6731 (1.6530)  classification_loss: 1.6298 (1.6179)  loss_mask: 0.0236 (0.0351)  time: 0.1709  data: 0.0002  max mem: 6052
[03:54:08.512009] Epoch: [23]  [640/781]  eta: 0:00:24  lr: 0.000227  training_loss: 1.6763 (1.6549)  classification_loss: 1.5982 (1.6176)  loss_mask: 0.0726 (0.0372)  time: 0.1718  data: 0.0003  max mem: 6052
[03:54:11.952934] Epoch: [23]  [660/781]  eta: 0:00:20  lr: 0.000227  training_loss: 1.6451 (1.6554)  classification_loss: 1.5962 (1.6175)  loss_mask: 0.0407 (0.0379)  time: 0.1720  data: 0.0005  max mem: 6052
[03:54:15.449963] Epoch: [23]  [680/781]  eta: 0:00:17  lr: 0.000227  training_loss: 1.6281 (1.6550)  classification_loss: 1.5925 (1.6171)  loss_mask: 0.0341 (0.0379)  time: 0.1747  data: 0.0002  max mem: 6052
[03:54:18.909319] Epoch: [23]  [700/781]  eta: 0:00:14  lr: 0.000226  training_loss: 1.6039 (1.6543)  classification_loss: 1.5675 (1.6167)  loss_mask: 0.0191 (0.0375)  time: 0.1729  data: 0.0002  max mem: 6052
[03:54:22.343217] Epoch: [23]  [720/781]  eta: 0:00:10  lr: 0.000226  training_loss: 1.6246 (1.6538)  classification_loss: 1.6100 (1.6167)  loss_mask: 0.0147 (0.0370)  time: 0.1716  data: 0.0003  max mem: 6052
[03:54:25.743634] Epoch: [23]  [740/781]  eta: 0:00:07  lr: 0.000226  training_loss: 1.5990 (1.6522)  classification_loss: 1.5895 (1.6157)  loss_mask: 0.0112 (0.0364)  time: 0.1699  data: 0.0002  max mem: 6052
[03:54:29.170895] Epoch: [23]  [760/781]  eta: 0:00:03  lr: 0.000226  training_loss: 1.6218 (1.6515)  classification_loss: 1.6094 (1.6157)  loss_mask: 0.0108 (0.0358)  time: 0.1713  data: 0.0002  max mem: 6052
[03:54:32.554780] Epoch: [23]  [780/781]  eta: 0:00:00  lr: 0.000226  training_loss: 1.6300 (1.6513)  classification_loss: 1.6173 (1.6160)  loss_mask: 0.0116 (0.0353)  time: 0.1691  data: 0.0001  max mem: 6052
[03:54:32.723743] Epoch: [23] Total time: 0:02:15 (0.1733 s / it)
[03:54:32.724205] Averaged stats: lr: 0.000226  training_loss: 1.6300 (1.6513)  classification_loss: 1.6173 (1.6160)  loss_mask: 0.0116 (0.0353)
[03:54:33.404893] Test:  [  0/157]  eta: 0:01:45  testing_loss: 0.9114 (0.9114)  acc1: 71.8750 (71.8750)  acc5: 93.7500 (93.7500)  time: 0.6736  data: 0.6431  max mem: 6052
[03:54:33.689327] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.9630 (0.9554)  acc1: 67.1875 (67.3295)  acc5: 98.4375 (98.4375)  time: 0.0869  data: 0.0587  max mem: 6052
[03:54:33.969924] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.9602 (0.9288)  acc1: 67.1875 (67.9315)  acc5: 98.4375 (98.6607)  time: 0.0281  data: 0.0002  max mem: 6052
[03:54:34.249821] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.9264 (0.9380)  acc1: 68.7500 (67.4395)  acc5: 98.4375 (98.0847)  time: 0.0279  data: 0.0002  max mem: 6052
[03:54:34.533268] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.9254 (0.9363)  acc1: 67.1875 (67.6067)  acc5: 98.4375 (98.1326)  time: 0.0281  data: 0.0001  max mem: 6052
[03:54:34.815644] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.9254 (0.9342)  acc1: 67.1875 (68.0760)  acc5: 98.4375 (98.0699)  time: 0.0282  data: 0.0002  max mem: 6052
[03:54:35.097555] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.9312 (0.9312)  acc1: 67.1875 (68.1609)  acc5: 96.8750 (97.8996)  time: 0.0281  data: 0.0002  max mem: 6052
[03:54:35.382959] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.9180 (0.9248)  acc1: 70.3125 (68.7280)  acc5: 96.8750 (97.8433)  time: 0.0282  data: 0.0002  max mem: 6052
[03:54:35.665063] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.9212 (0.9309)  acc1: 70.3125 (68.6728)  acc5: 96.8750 (97.8588)  time: 0.0282  data: 0.0002  max mem: 6052
[03:54:35.945417] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.9656 (0.9320)  acc1: 67.1875 (68.5096)  acc5: 98.4375 (97.9052)  time: 0.0280  data: 0.0002  max mem: 6052
[03:54:36.225562] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.9515 (0.9374)  acc1: 67.1875 (68.0538)  acc5: 98.4375 (97.9579)  time: 0.0279  data: 0.0002  max mem: 6052
[03:54:36.506464] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.9742 (0.9383)  acc1: 67.1875 (68.0743)  acc5: 98.4375 (97.8885)  time: 0.0279  data: 0.0002  max mem: 6052
[03:54:36.786559] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.8745 (0.9334)  acc1: 68.7500 (68.3497)  acc5: 98.4375 (97.9081)  time: 0.0279  data: 0.0002  max mem: 6052
[03:54:37.068301] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.8972 (0.9364)  acc1: 70.3125 (68.1894)  acc5: 98.4375 (97.9127)  time: 0.0280  data: 0.0002  max mem: 6052
[03:54:37.348861] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.9585 (0.9368)  acc1: 65.6250 (68.2402)  acc5: 98.4375 (97.8723)  time: 0.0280  data: 0.0002  max mem: 6052
[03:54:37.626058] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.9585 (0.9339)  acc1: 68.7500 (68.4706)  acc5: 96.8750 (97.7959)  time: 0.0278  data: 0.0001  max mem: 6052
[03:54:37.775506] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.9441 (0.9354)  acc1: 67.1875 (68.4000)  acc5: 96.8750 (97.8100)  time: 0.0268  data: 0.0001  max mem: 6052
[03:54:37.931088] Test: Total time: 0:00:05 (0.0331 s / it)
[03:54:37.931556] * Acc@1 68.400 Acc@5 97.810 loss 0.935
[03:54:37.931864] Accuracy of the network on the 10000 test images: 68.4%
[03:54:37.932056] Max accuracy: 68.40%
[03:54:38.122361] log_dir: ./output_dir
[03:54:38.963855] Epoch: [24]  [  0/781]  eta: 0:10:55  lr: 0.000226  training_loss: 1.4904 (1.4904)  classification_loss: 1.4697 (1.4697)  loss_mask: 0.0207 (0.0207)  time: 0.8399  data: 0.6436  max mem: 6052
[03:54:42.368759] Epoch: [24]  [ 20/781]  eta: 0:02:33  lr: 0.000226  training_loss: 1.6074 (1.6293)  classification_loss: 1.5676 (1.5846)  loss_mask: 0.0305 (0.0447)  time: 0.1702  data: 0.0001  max mem: 6052
[03:54:45.766560] Epoch: [24]  [ 40/781]  eta: 0:02:18  lr: 0.000226  training_loss: 1.6830 (1.6580)  classification_loss: 1.6142 (1.5947)  loss_mask: 0.0677 (0.0633)  time: 0.1698  data: 0.0002  max mem: 6052
[03:54:49.182905] Epoch: [24]  [ 60/781]  eta: 0:02:10  lr: 0.000226  training_loss: 1.6686 (1.6635)  classification_loss: 1.6394 (1.6098)  loss_mask: 0.0273 (0.0537)  time: 0.1707  data: 0.0002  max mem: 6052
[03:54:52.581380] Epoch: [24]  [ 80/781]  eta: 0:02:05  lr: 0.000226  training_loss: 1.6870 (1.6596)  classification_loss: 1.6020 (1.6103)  loss_mask: 0.0227 (0.0494)  time: 0.1699  data: 0.0002  max mem: 6052
[03:54:55.978891] Epoch: [24]  [100/781]  eta: 0:02:00  lr: 0.000226  training_loss: 1.6580 (1.6612)  classification_loss: 1.6053 (1.6122)  loss_mask: 0.0240 (0.0490)  time: 0.1698  data: 0.0002  max mem: 6052
[03:54:59.401427] Epoch: [24]  [120/781]  eta: 0:01:56  lr: 0.000226  training_loss: 1.5929 (1.6526)  classification_loss: 1.5674 (1.6053)  loss_mask: 0.0150 (0.0473)  time: 0.1710  data: 0.0002  max mem: 6052
[03:55:02.819439] Epoch: [24]  [140/781]  eta: 0:01:52  lr: 0.000226  training_loss: 1.5898 (1.6448)  classification_loss: 1.5611 (1.5998)  loss_mask: 0.0215 (0.0450)  time: 0.1708  data: 0.0003  max mem: 6052
[03:55:06.254608] Epoch: [24]  [160/781]  eta: 0:01:48  lr: 0.000226  training_loss: 1.5851 (1.6383)  classification_loss: 1.5794 (1.5966)  loss_mask: 0.0138 (0.0417)  time: 0.1717  data: 0.0002  max mem: 6052
[03:55:09.692377] Epoch: [24]  [180/781]  eta: 0:01:44  lr: 0.000226  training_loss: 1.5779 (1.6368)  classification_loss: 1.5510 (1.5953)  loss_mask: 0.0190 (0.0414)  time: 0.1718  data: 0.0002  max mem: 6052
[03:55:13.118689] Epoch: [24]  [200/781]  eta: 0:01:41  lr: 0.000226  training_loss: 1.7039 (1.6462)  classification_loss: 1.5985 (1.5962)  loss_mask: 0.1132 (0.0500)  time: 0.1712  data: 0.0003  max mem: 6052
[03:55:16.539787] Epoch: [24]  [220/781]  eta: 0:01:37  lr: 0.000226  training_loss: 1.7176 (1.6524)  classification_loss: 1.6412 (1.6012)  loss_mask: 0.0682 (0.0512)  time: 0.1710  data: 0.0003  max mem: 6052
[03:55:19.951612] Epoch: [24]  [240/781]  eta: 0:01:33  lr: 0.000225  training_loss: 1.6500 (1.6538)  classification_loss: 1.5918 (1.6036)  loss_mask: 0.0354 (0.0501)  time: 0.1705  data: 0.0002  max mem: 6052
[03:55:23.369671] Epoch: [24]  [260/781]  eta: 0:01:30  lr: 0.000225  training_loss: 1.6287 (1.6532)  classification_loss: 1.6070 (1.6045)  loss_mask: 0.0222 (0.0487)  time: 0.1708  data: 0.0002  max mem: 6052
[03:55:26.797270] Epoch: [24]  [280/781]  eta: 0:01:26  lr: 0.000225  training_loss: 1.6534 (1.6583)  classification_loss: 1.5734 (1.6027)  loss_mask: 0.0606 (0.0557)  time: 0.1713  data: 0.0002  max mem: 6052
[03:55:30.213432] Epoch: [24]  [300/781]  eta: 0:01:23  lr: 0.000225  training_loss: 1.8392 (1.6699)  classification_loss: 1.6928 (1.6088)  loss_mask: 0.1326 (0.0612)  time: 0.1707  data: 0.0002  max mem: 6052
[03:55:33.641636] Epoch: [24]  [320/781]  eta: 0:01:19  lr: 0.000225  training_loss: 1.7816 (1.6785)  classification_loss: 1.6766 (1.6131)  loss_mask: 0.0886 (0.0654)  time: 0.1713  data: 0.0002  max mem: 6052
[03:55:37.066954] Epoch: [24]  [340/781]  eta: 0:01:16  lr: 0.000225  training_loss: 1.7923 (1.6820)  classification_loss: 1.6915 (1.6174)  loss_mask: 0.0496 (0.0646)  time: 0.1712  data: 0.0004  max mem: 6052
[03:55:40.523999] Epoch: [24]  [360/781]  eta: 0:01:12  lr: 0.000225  training_loss: 1.7718 (1.6882)  classification_loss: 1.7165 (1.6228)  loss_mask: 0.0277 (0.0654)  time: 0.1728  data: 0.0004  max mem: 6052
[03:55:43.916515] Epoch: [24]  [380/781]  eta: 0:01:09  lr: 0.000225  training_loss: 1.8136 (1.6980)  classification_loss: 1.6526 (1.6251)  loss_mask: 0.1433 (0.0729)  time: 0.1695  data: 0.0002  max mem: 6052
[03:55:47.320881] Epoch: [24]  [400/781]  eta: 0:01:05  lr: 0.000225  training_loss: 1.7217 (1.6996)  classification_loss: 1.6622 (1.6263)  loss_mask: 0.0690 (0.0733)  time: 0.1701  data: 0.0003  max mem: 6052
[03:55:50.741750] Epoch: [24]  [420/781]  eta: 0:01:02  lr: 0.000225  training_loss: 1.6520 (1.6980)  classification_loss: 1.5965 (1.6258)  loss_mask: 0.0475 (0.0722)  time: 0.1710  data: 0.0003  max mem: 6052
[03:55:54.164401] Epoch: [24]  [440/781]  eta: 0:00:58  lr: 0.000225  training_loss: 1.6691 (1.6963)  classification_loss: 1.6276 (1.6258)  loss_mask: 0.0304 (0.0705)  time: 0.1710  data: 0.0002  max mem: 6052
[03:55:57.591401] Epoch: [24]  [460/781]  eta: 0:00:55  lr: 0.000225  training_loss: 1.6792 (1.6951)  classification_loss: 1.6289 (1.6255)  loss_mask: 0.0435 (0.0696)  time: 0.1713  data: 0.0002  max mem: 6052
[03:56:01.040173] Epoch: [24]  [480/781]  eta: 0:00:51  lr: 0.000225  training_loss: 1.6737 (1.6959)  classification_loss: 1.6063 (1.6268)  loss_mask: 0.0556 (0.0692)  time: 0.1723  data: 0.0003  max mem: 6052
[03:56:04.486774] Epoch: [24]  [500/781]  eta: 0:00:48  lr: 0.000225  training_loss: 1.6591 (1.6941)  classification_loss: 1.6423 (1.6265)  loss_mask: 0.0245 (0.0675)  time: 0.1723  data: 0.0002  max mem: 6052
[03:56:07.892354] Epoch: [24]  [520/781]  eta: 0:00:44  lr: 0.000225  training_loss: 1.6327 (1.6918)  classification_loss: 1.6157 (1.6258)  loss_mask: 0.0252 (0.0661)  time: 0.1702  data: 0.0002  max mem: 6052
[03:56:11.342778] Epoch: [24]  [540/781]  eta: 0:00:41  lr: 0.000225  training_loss: 1.6280 (1.6899)  classification_loss: 1.6173 (1.6252)  loss_mask: 0.0273 (0.0648)  time: 0.1725  data: 0.0002  max mem: 6052
[03:56:14.741775] Epoch: [24]  [560/781]  eta: 0:00:38  lr: 0.000224  training_loss: 1.6286 (1.6880)  classification_loss: 1.5893 (1.6243)  loss_mask: 0.0305 (0.0637)  time: 0.1699  data: 0.0003  max mem: 6052
[03:56:18.196233] Epoch: [24]  [580/781]  eta: 0:00:34  lr: 0.000224  training_loss: 1.6115 (1.6862)  classification_loss: 1.5972 (1.6237)  loss_mask: 0.0249 (0.0625)  time: 0.1726  data: 0.0003  max mem: 6052
[03:56:21.617727] Epoch: [24]  [600/781]  eta: 0:00:31  lr: 0.000224  training_loss: 1.5690 (1.6840)  classification_loss: 1.5528 (1.6225)  loss_mask: 0.0249 (0.0615)  time: 0.1710  data: 0.0002  max mem: 6052
[03:56:25.036772] Epoch: [24]  [620/781]  eta: 0:00:27  lr: 0.000224  training_loss: 1.6103 (1.6830)  classification_loss: 1.5949 (1.6218)  loss_mask: 0.0285 (0.0612)  time: 0.1709  data: 0.0002  max mem: 6052
[03:56:28.453229] Epoch: [24]  [640/781]  eta: 0:00:24  lr: 0.000224  training_loss: 1.6824 (1.6834)  classification_loss: 1.5472 (1.6199)  loss_mask: 0.1196 (0.0635)  time: 0.1707  data: 0.0002  max mem: 6052
[03:56:31.875921] Epoch: [24]  [660/781]  eta: 0:00:20  lr: 0.000224  training_loss: 1.6731 (1.6838)  classification_loss: 1.6414 (1.6205)  loss_mask: 0.0501 (0.0633)  time: 0.1710  data: 0.0002  max mem: 6052
[03:56:35.299976] Epoch: [24]  [680/781]  eta: 0:00:17  lr: 0.000224  training_loss: 1.6558 (1.6831)  classification_loss: 1.6193 (1.6207)  loss_mask: 0.0272 (0.0624)  time: 0.1711  data: 0.0002  max mem: 6052
[03:56:38.721873] Epoch: [24]  [700/781]  eta: 0:00:13  lr: 0.000224  training_loss: 1.6109 (1.6818)  classification_loss: 1.5831 (1.6203)  loss_mask: 0.0202 (0.0615)  time: 0.1710  data: 0.0004  max mem: 6052
[03:56:42.142587] Epoch: [24]  [720/781]  eta: 0:00:10  lr: 0.000224  training_loss: 1.6518 (1.6806)  classification_loss: 1.6236 (1.6200)  loss_mask: 0.0172 (0.0605)  time: 0.1710  data: 0.0003  max mem: 6052
[03:56:45.554611] Epoch: [24]  [740/781]  eta: 0:00:07  lr: 0.000224  training_loss: 1.6449 (1.6807)  classification_loss: 1.5656 (1.6192)  loss_mask: 0.0481 (0.0615)  time: 0.1705  data: 0.0003  max mem: 6052
[03:56:48.973550] Epoch: [24]  [760/781]  eta: 0:00:03  lr: 0.000224  training_loss: 1.7513 (1.6820)  classification_loss: 1.5782 (1.6189)  loss_mask: 0.0930 (0.0632)  time: 0.1709  data: 0.0002  max mem: 6052
[03:56:52.377967] Epoch: [24]  [780/781]  eta: 0:00:00  lr: 0.000224  training_loss: 1.6503 (1.6817)  classification_loss: 1.6230 (1.6188)  loss_mask: 0.0439 (0.0628)  time: 0.1701  data: 0.0003  max mem: 6052
[03:56:52.557895] Epoch: [24] Total time: 0:02:14 (0.1721 s / it)
[03:56:52.558339] Averaged stats: lr: 0.000224  training_loss: 1.6503 (1.6817)  classification_loss: 1.6230 (1.6188)  loss_mask: 0.0439 (0.0628)
[03:56:53.233598] Test:  [  0/157]  eta: 0:01:45  testing_loss: 0.8864 (0.8864)  acc1: 71.8750 (71.8750)  acc5: 95.3125 (95.3125)  time: 0.6713  data: 0.6359  max mem: 6052
[03:56:53.516663] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.0287 (0.9849)  acc1: 68.7500 (65.4830)  acc5: 98.4375 (97.4432)  time: 0.0866  data: 0.0580  max mem: 6052
[03:56:53.797786] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.9429 (0.9399)  acc1: 68.7500 (67.7827)  acc5: 98.4375 (97.7679)  time: 0.0281  data: 0.0002  max mem: 6052
[03:56:54.084948] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.9337 (0.9446)  acc1: 67.1875 (67.5907)  acc5: 96.8750 (97.3790)  time: 0.0283  data: 0.0002  max mem: 6052
[03:56:54.367581] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.9103 (0.9394)  acc1: 68.7500 (68.3308)  acc5: 96.8750 (97.3323)  time: 0.0283  data: 0.0003  max mem: 6052
[03:56:54.648808] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.9139 (0.9364)  acc1: 70.3125 (68.5662)  acc5: 96.8750 (97.3039)  time: 0.0280  data: 0.0002  max mem: 6052
[03:56:54.931098] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.9170 (0.9331)  acc1: 68.7500 (68.4682)  acc5: 96.8750 (97.3361)  time: 0.0280  data: 0.0001  max mem: 6052
[03:56:55.217008] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.8917 (0.9252)  acc1: 68.7500 (68.8160)  acc5: 98.4375 (97.4032)  time: 0.0282  data: 0.0001  max mem: 6052
[03:56:55.504148] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.9016 (0.9288)  acc1: 70.3125 (68.7886)  acc5: 98.4375 (97.5116)  time: 0.0285  data: 0.0002  max mem: 6052
[03:56:55.786545] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.9187 (0.9286)  acc1: 68.7500 (68.8359)  acc5: 98.4375 (97.5618)  time: 0.0283  data: 0.0002  max mem: 6052
[03:56:56.067968] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.9187 (0.9338)  acc1: 67.1875 (68.5798)  acc5: 98.4375 (97.6021)  time: 0.0281  data: 0.0002  max mem: 6052
[03:56:56.349337] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.9960 (0.9363)  acc1: 65.6250 (68.4262)  acc5: 98.4375 (97.5648)  time: 0.0280  data: 0.0002  max mem: 6052
[03:56:56.645290] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.9483 (0.9330)  acc1: 67.1875 (68.4013)  acc5: 96.8750 (97.5723)  time: 0.0287  data: 0.0004  max mem: 6052
[03:56:56.925305] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.9483 (0.9358)  acc1: 65.6250 (68.1775)  acc5: 98.4375 (97.5906)  time: 0.0287  data: 0.0004  max mem: 6052
[03:56:57.205688] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.9502 (0.9350)  acc1: 65.6250 (68.2513)  acc5: 98.4375 (97.5842)  time: 0.0279  data: 0.0001  max mem: 6052
[03:56:57.485271] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.9376 (0.9322)  acc1: 67.1875 (68.2947)  acc5: 98.4375 (97.6304)  time: 0.0279  data: 0.0001  max mem: 6052
[03:56:57.636180] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.8949 (0.9330)  acc1: 68.7500 (68.2000)  acc5: 98.4375 (97.6600)  time: 0.0269  data: 0.0001  max mem: 6052
[03:56:57.801784] Test: Total time: 0:00:05 (0.0334 s / it)
[03:56:57.802535] * Acc@1 68.200 Acc@5 97.660 loss 0.933
[03:56:57.802851] Accuracy of the network on the 10000 test images: 68.2%
[03:56:57.803061] Max accuracy: 68.40%
[03:56:57.929395] log_dir: ./output_dir
[03:56:58.843870] Epoch: [25]  [  0/781]  eta: 0:11:52  lr: 0.000224  training_loss: 1.5361 (1.5361)  classification_loss: 1.4967 (1.4967)  loss_mask: 0.0394 (0.0394)  time: 0.9127  data: 0.7342  max mem: 6052
[03:57:02.324668] Epoch: [25]  [ 20/781]  eta: 0:02:39  lr: 0.000224  training_loss: 1.6207 (1.6096)  classification_loss: 1.5794 (1.5821)  loss_mask: 0.0188 (0.0274)  time: 0.1739  data: 0.0003  max mem: 6052
[03:57:05.763230] Epoch: [25]  [ 40/781]  eta: 0:02:21  lr: 0.000224  training_loss: 1.6388 (1.6239)  classification_loss: 1.6133 (1.5939)  loss_mask: 0.0228 (0.0300)  time: 0.1719  data: 0.0002  max mem: 6052
[03:57:09.185654] Epoch: [25]  [ 60/781]  eta: 0:02:12  lr: 0.000224  training_loss: 1.7794 (1.6676)  classification_loss: 1.6465 (1.6067)  loss_mask: 0.1116 (0.0609)  time: 0.1710  data: 0.0002  max mem: 6052
[03:57:12.604174] Epoch: [25]  [ 80/781]  eta: 0:02:06  lr: 0.000223  training_loss: 1.6791 (1.6744)  classification_loss: 1.5964 (1.6126)  loss_mask: 0.0579 (0.0618)  time: 0.1709  data: 0.0003  max mem: 6052
[03:57:16.070004] Epoch: [25]  [100/781]  eta: 0:02:02  lr: 0.000223  training_loss: 1.5988 (1.6656)  classification_loss: 1.5733 (1.6099)  loss_mask: 0.0284 (0.0557)  time: 0.1732  data: 0.0002  max mem: 6052
[03:57:19.583478] Epoch: [25]  [120/781]  eta: 0:01:58  lr: 0.000223  training_loss: 1.5785 (1.6542)  classification_loss: 1.5583 (1.6023)  loss_mask: 0.0260 (0.0519)  time: 0.1756  data: 0.0002  max mem: 6052
[03:57:23.007800] Epoch: [25]  [140/781]  eta: 0:01:53  lr: 0.000223  training_loss: 1.6095 (1.6500)  classification_loss: 1.5763 (1.5982)  loss_mask: 0.0356 (0.0518)  time: 0.1711  data: 0.0004  max mem: 6052
[03:57:26.494226] Epoch: [25]  [160/781]  eta: 0:01:50  lr: 0.000223  training_loss: 1.6256 (1.6463)  classification_loss: 1.6026 (1.5970)  loss_mask: 0.0228 (0.0493)  time: 0.1742  data: 0.0002  max mem: 6052
[03:57:29.945545] Epoch: [25]  [180/781]  eta: 0:01:46  lr: 0.000223  training_loss: 1.5772 (1.6421)  classification_loss: 1.5483 (1.5948)  loss_mask: 0.0203 (0.0474)  time: 0.1725  data: 0.0003  max mem: 6052
[03:57:33.354764] Epoch: [25]  [200/781]  eta: 0:01:42  lr: 0.000223  training_loss: 1.5412 (1.6362)  classification_loss: 1.5066 (1.5900)  loss_mask: 0.0281 (0.0462)  time: 0.1704  data: 0.0002  max mem: 6052
[03:57:36.784566] Epoch: [25]  [220/781]  eta: 0:01:38  lr: 0.000223  training_loss: 1.6513 (1.6391)  classification_loss: 1.5919 (1.5898)  loss_mask: 0.0769 (0.0493)  time: 0.1714  data: 0.0003  max mem: 6052
[03:57:40.300809] Epoch: [25]  [240/781]  eta: 0:01:35  lr: 0.000223  training_loss: 1.6168 (1.6392)  classification_loss: 1.5693 (1.5890)  loss_mask: 0.0475 (0.0502)  time: 0.1757  data: 0.0002  max mem: 6052
[03:57:43.717140] Epoch: [25]  [260/781]  eta: 0:01:31  lr: 0.000223  training_loss: 1.6403 (1.6390)  classification_loss: 1.5712 (1.5888)  loss_mask: 0.0277 (0.0502)  time: 0.1707  data: 0.0002  max mem: 6052
[03:57:47.144126] Epoch: [25]  [280/781]  eta: 0:01:27  lr: 0.000223  training_loss: 1.5965 (1.6360)  classification_loss: 1.5494 (1.5862)  loss_mask: 0.0347 (0.0497)  time: 0.1712  data: 0.0002  max mem: 6052
[03:57:50.566843] Epoch: [25]  [300/781]  eta: 0:01:24  lr: 0.000223  training_loss: 1.6053 (1.6356)  classification_loss: 1.5839 (1.5874)  loss_mask: 0.0210 (0.0482)  time: 0.1711  data: 0.0003  max mem: 6052
[03:57:54.008955] Epoch: [25]  [320/781]  eta: 0:01:20  lr: 0.000223  training_loss: 1.6716 (1.6380)  classification_loss: 1.6029 (1.5889)  loss_mask: 0.0369 (0.0491)  time: 0.1720  data: 0.0002  max mem: 6052
[03:57:57.453902] Epoch: [25]  [340/781]  eta: 0:01:16  lr: 0.000223  training_loss: 1.5919 (1.6350)  classification_loss: 1.5588 (1.5872)  loss_mask: 0.0189 (0.0477)  time: 0.1722  data: 0.0002  max mem: 6052
[03:58:00.871620] Epoch: [25]  [360/781]  eta: 0:01:13  lr: 0.000223  training_loss: 1.6068 (1.6339)  classification_loss: 1.5942 (1.5880)  loss_mask: 0.0145 (0.0459)  time: 0.1708  data: 0.0002  max mem: 6052
[03:58:04.279079] Epoch: [25]  [380/781]  eta: 0:01:09  lr: 0.000223  training_loss: 1.6029 (1.6341)  classification_loss: 1.5980 (1.5901)  loss_mask: 0.0084 (0.0440)  time: 0.1703  data: 0.0002  max mem: 6052
[03:58:07.705529] Epoch: [25]  [400/781]  eta: 0:01:06  lr: 0.000222  training_loss: 1.5564 (1.6309)  classification_loss: 1.5474 (1.5886)  loss_mask: 0.0072 (0.0423)  time: 0.1713  data: 0.0002  max mem: 6052
[03:58:11.131071] Epoch: [25]  [420/781]  eta: 0:01:02  lr: 0.000222  training_loss: 1.5607 (1.6286)  classification_loss: 1.5392 (1.5877)  loss_mask: 0.0117 (0.0410)  time: 0.1712  data: 0.0003  max mem: 6052
[03:58:14.542828] Epoch: [25]  [440/781]  eta: 0:00:59  lr: 0.000222  training_loss: 1.5917 (1.6270)  classification_loss: 1.5738 (1.5867)  loss_mask: 0.0138 (0.0403)  time: 0.1705  data: 0.0002  max mem: 6052
[03:58:17.954535] Epoch: [25]  [460/781]  eta: 0:00:55  lr: 0.000222  training_loss: 1.6136 (1.6259)  classification_loss: 1.5541 (1.5862)  loss_mask: 0.0102 (0.0397)  time: 0.1705  data: 0.0002  max mem: 6052
[03:58:21.364555] Epoch: [25]  [480/781]  eta: 0:00:52  lr: 0.000222  training_loss: 1.6716 (1.6267)  classification_loss: 1.6007 (1.5862)  loss_mask: 0.0463 (0.0405)  time: 0.1704  data: 0.0002  max mem: 6052
[03:58:24.767770] Epoch: [25]  [500/781]  eta: 0:00:48  lr: 0.000222  training_loss: 1.5471 (1.6248)  classification_loss: 1.5355 (1.5854)  loss_mask: 0.0117 (0.0395)  time: 0.1701  data: 0.0003  max mem: 6052
[03:58:28.194811] Epoch: [25]  [520/781]  eta: 0:00:45  lr: 0.000222  training_loss: 1.5658 (1.6233)  classification_loss: 1.5586 (1.5849)  loss_mask: 0.0080 (0.0383)  time: 0.1712  data: 0.0002  max mem: 6052
[03:58:31.591169] Epoch: [25]  [540/781]  eta: 0:00:41  lr: 0.000222  training_loss: 1.5759 (1.6222)  classification_loss: 1.5723 (1.5849)  loss_mask: 0.0054 (0.0373)  time: 0.1697  data: 0.0003  max mem: 6052
[03:58:35.019643] Epoch: [25]  [560/781]  eta: 0:00:38  lr: 0.000222  training_loss: 1.6147 (1.6215)  classification_loss: 1.6029 (1.5852)  loss_mask: 0.0060 (0.0363)  time: 0.1713  data: 0.0002  max mem: 6052
[03:58:38.447140] Epoch: [25]  [580/781]  eta: 0:00:34  lr: 0.000222  training_loss: 1.5834 (1.6207)  classification_loss: 1.5774 (1.5854)  loss_mask: 0.0060 (0.0353)  time: 0.1713  data: 0.0002  max mem: 6052
[03:58:41.865221] Epoch: [25]  [600/781]  eta: 0:00:31  lr: 0.000222  training_loss: 1.5393 (1.6190)  classification_loss: 1.5316 (1.5845)  loss_mask: 0.0077 (0.0345)  time: 0.1708  data: 0.0002  max mem: 6052
[03:58:45.272073] Epoch: [25]  [620/781]  eta: 0:00:27  lr: 0.000222  training_loss: 1.5903 (1.6183)  classification_loss: 1.5831 (1.5845)  loss_mask: 0.0071 (0.0337)  time: 0.1703  data: 0.0002  max mem: 6052
[03:58:48.685760] Epoch: [25]  [640/781]  eta: 0:00:24  lr: 0.000222  training_loss: 1.5633 (1.6172)  classification_loss: 1.5453 (1.5839)  loss_mask: 0.0156 (0.0333)  time: 0.1706  data: 0.0002  max mem: 6052
[03:58:52.079122] Epoch: [25]  [660/781]  eta: 0:00:20  lr: 0.000222  training_loss: 1.6446 (1.6173)  classification_loss: 1.6036 (1.5845)  loss_mask: 0.0090 (0.0328)  time: 0.1696  data: 0.0002  max mem: 6052
[03:58:55.475353] Epoch: [25]  [680/781]  eta: 0:00:17  lr: 0.000222  training_loss: 1.6648 (1.6198)  classification_loss: 1.5920 (1.5853)  loss_mask: 0.0209 (0.0344)  time: 0.1697  data: 0.0003  max mem: 6052
[03:58:58.897264] Epoch: [25]  [700/781]  eta: 0:00:13  lr: 0.000221  training_loss: 1.6724 (1.6218)  classification_loss: 1.5686 (1.5851)  loss_mask: 0.0897 (0.0367)  time: 0.1710  data: 0.0002  max mem: 6052
[03:59:02.322660] Epoch: [25]  [720/781]  eta: 0:00:10  lr: 0.000221  training_loss: 1.6450 (1.6224)  classification_loss: 1.6242 (1.5857)  loss_mask: 0.0293 (0.0367)  time: 0.1712  data: 0.0002  max mem: 6052
[03:59:05.804078] Epoch: [25]  [740/781]  eta: 0:00:07  lr: 0.000221  training_loss: 1.5615 (1.6210)  classification_loss: 1.5320 (1.5846)  loss_mask: 0.0219 (0.0364)  time: 0.1739  data: 0.0003  max mem: 6052
[03:59:09.194134] Epoch: [25]  [760/781]  eta: 0:00:03  lr: 0.000221  training_loss: 1.6275 (1.6213)  classification_loss: 1.5594 (1.5842)  loss_mask: 0.0523 (0.0371)  time: 0.1694  data: 0.0003  max mem: 6052
[03:59:12.582636] Epoch: [25]  [780/781]  eta: 0:00:00  lr: 0.000221  training_loss: 1.5920 (1.6205)  classification_loss: 1.5563 (1.5834)  loss_mask: 0.0226 (0.0371)  time: 0.1693  data: 0.0002  max mem: 6052
[03:59:12.735182] Epoch: [25] Total time: 0:02:14 (0.1726 s / it)
[03:59:12.735624] Averaged stats: lr: 0.000221  training_loss: 1.5920 (1.6205)  classification_loss: 1.5563 (1.5834)  loss_mask: 0.0226 (0.0371)
[03:59:13.376503] Test:  [  0/157]  eta: 0:01:39  testing_loss: 0.8067 (0.8067)  acc1: 73.4375 (73.4375)  acc5: 95.3125 (95.3125)  time: 0.6368  data: 0.6070  max mem: 6052
[03:59:13.659593] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.8770 (0.9048)  acc1: 70.3125 (68.6080)  acc5: 98.4375 (98.4375)  time: 0.0834  data: 0.0553  max mem: 6052
[03:59:13.940768] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.8770 (0.8734)  acc1: 68.7500 (70.3869)  acc5: 98.4375 (98.6607)  time: 0.0280  data: 0.0001  max mem: 6052
[03:59:14.223689] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.8611 (0.8722)  acc1: 71.8750 (71.0685)  acc5: 98.4375 (98.3871)  time: 0.0281  data: 0.0001  max mem: 6052
[03:59:14.504142] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.8576 (0.8777)  acc1: 70.3125 (70.6936)  acc5: 96.8750 (98.1707)  time: 0.0281  data: 0.0001  max mem: 6052
[03:59:14.785395] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.8576 (0.8777)  acc1: 70.3125 (70.6189)  acc5: 98.4375 (98.1311)  time: 0.0280  data: 0.0001  max mem: 6052
[03:59:15.066848] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.8802 (0.8786)  acc1: 70.3125 (70.3893)  acc5: 98.4375 (98.0020)  time: 0.0280  data: 0.0001  max mem: 6052
[03:59:15.348006] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.8532 (0.8725)  acc1: 68.7500 (70.4665)  acc5: 96.8750 (97.9754)  time: 0.0280  data: 0.0001  max mem: 6052
[03:59:15.628722] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.8518 (0.8781)  acc1: 70.3125 (70.2353)  acc5: 96.8750 (97.8395)  time: 0.0280  data: 0.0001  max mem: 6052
[03:59:15.909224] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.8694 (0.8765)  acc1: 68.7500 (70.2266)  acc5: 96.8750 (97.9052)  time: 0.0279  data: 0.0001  max mem: 6052
[03:59:16.189272] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.8645 (0.8779)  acc1: 70.3125 (70.2661)  acc5: 98.4375 (97.9115)  time: 0.0279  data: 0.0001  max mem: 6052
[03:59:16.470702] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.8988 (0.8804)  acc1: 70.3125 (70.1014)  acc5: 96.8750 (97.8463)  time: 0.0279  data: 0.0001  max mem: 6052
[03:59:16.751896] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.8500 (0.8765)  acc1: 70.3125 (70.1575)  acc5: 98.4375 (97.8951)  time: 0.0280  data: 0.0001  max mem: 6052
[03:59:17.032426] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.8500 (0.8777)  acc1: 71.8750 (70.2171)  acc5: 98.4375 (97.9365)  time: 0.0280  data: 0.0001  max mem: 6052
[03:59:17.316406] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.8690 (0.8757)  acc1: 71.8750 (70.3014)  acc5: 98.4375 (97.9832)  time: 0.0281  data: 0.0001  max mem: 6052
[03:59:17.595998] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.8835 (0.8747)  acc1: 71.8750 (70.4263)  acc5: 98.4375 (97.9615)  time: 0.0280  data: 0.0001  max mem: 6052
[03:59:17.745346] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.8311 (0.8760)  acc1: 71.8750 (70.3400)  acc5: 98.4375 (97.9800)  time: 0.0269  data: 0.0001  max mem: 6052
[03:59:17.912860] Test: Total time: 0:00:05 (0.0330 s / it)
[03:59:17.913427] * Acc@1 70.340 Acc@5 97.980 loss 0.876
[03:59:17.913889] Accuracy of the network on the 10000 test images: 70.3%
[03:59:17.914108] Max accuracy: 70.34%
[03:59:18.037609] log_dir: ./output_dir
[03:59:18.945948] Epoch: [26]  [  0/781]  eta: 0:11:48  lr: 0.000221  training_loss: 1.4541 (1.4541)  classification_loss: 1.4439 (1.4439)  loss_mask: 0.0103 (0.0103)  time: 0.9068  data: 0.7246  max mem: 6052
[03:59:22.383723] Epoch: [26]  [ 20/781]  eta: 0:02:37  lr: 0.000221  training_loss: 1.5489 (1.5652)  classification_loss: 1.5276 (1.5423)  loss_mask: 0.0213 (0.0228)  time: 0.1718  data: 0.0002  max mem: 6052
[03:59:25.810491] Epoch: [26]  [ 40/781]  eta: 0:02:20  lr: 0.000221  training_loss: 1.5968 (1.5853)  classification_loss: 1.5699 (1.5445)  loss_mask: 0.0403 (0.0408)  time: 0.1713  data: 0.0002  max mem: 6052
[03:59:29.258908] Epoch: [26]  [ 60/781]  eta: 0:02:12  lr: 0.000221  training_loss: 1.7990 (1.6659)  classification_loss: 1.6588 (1.5809)  loss_mask: 0.1429 (0.0850)  time: 0.1724  data: 0.0003  max mem: 6052
[03:59:32.680505] Epoch: [26]  [ 80/781]  eta: 0:02:06  lr: 0.000221  training_loss: 1.6672 (1.6745)  classification_loss: 1.5835 (1.5812)  loss_mask: 0.0546 (0.0933)  time: 0.1710  data: 0.0004  max mem: 6052
[03:59:36.107310] Epoch: [26]  [100/781]  eta: 0:02:01  lr: 0.000221  training_loss: 1.6706 (1.6746)  classification_loss: 1.5837 (1.5859)  loss_mask: 0.0694 (0.0886)  time: 0.1713  data: 0.0003  max mem: 6052
[03:59:39.532241] Epoch: [26]  [120/781]  eta: 0:01:57  lr: 0.000221  training_loss: 1.5840 (1.6600)  classification_loss: 1.5673 (1.5800)  loss_mask: 0.0307 (0.0801)  time: 0.1712  data: 0.0002  max mem: 6052
[03:59:42.970377] Epoch: [26]  [140/781]  eta: 0:01:53  lr: 0.000221  training_loss: 1.5329 (1.6418)  classification_loss: 1.5079 (1.5693)  loss_mask: 0.0214 (0.0725)  time: 0.1718  data: 0.0003  max mem: 6052
[03:59:46.386798] Epoch: [26]  [160/781]  eta: 0:01:49  lr: 0.000221  training_loss: 1.5379 (1.6337)  classification_loss: 1.5203 (1.5675)  loss_mask: 0.0177 (0.0661)  time: 0.1707  data: 0.0002  max mem: 6052
[03:59:49.799033] Epoch: [26]  [180/781]  eta: 0:01:45  lr: 0.000221  training_loss: 1.5960 (1.6300)  classification_loss: 1.5763 (1.5694)  loss_mask: 0.0138 (0.0607)  time: 0.1705  data: 0.0002  max mem: 6052
[03:59:53.234576] Epoch: [26]  [200/781]  eta: 0:01:41  lr: 0.000220  training_loss: 1.6235 (1.6270)  classification_loss: 1.5742 (1.5693)  loss_mask: 0.0143 (0.0577)  time: 0.1717  data: 0.0002  max mem: 6052
[03:59:56.644089] Epoch: [26]  [220/781]  eta: 0:01:37  lr: 0.000220  training_loss: 1.7223 (1.6359)  classification_loss: 1.5845 (1.5718)  loss_mask: 0.1017 (0.0640)  time: 0.1704  data: 0.0002  max mem: 6052
[04:00:00.065150] Epoch: [26]  [240/781]  eta: 0:01:34  lr: 0.000220  training_loss: 1.6165 (1.6389)  classification_loss: 1.5679 (1.5721)  loss_mask: 0.0500 (0.0668)  time: 0.1710  data: 0.0003  max mem: 6052
[04:00:03.521262] Epoch: [26]  [260/781]  eta: 0:01:30  lr: 0.000220  training_loss: 1.6091 (1.6377)  classification_loss: 1.5498 (1.5721)  loss_mask: 0.0405 (0.0655)  time: 0.1727  data: 0.0002  max mem: 6052
[04:00:06.930476] Epoch: [26]  [280/781]  eta: 0:01:27  lr: 0.000220  training_loss: 1.5993 (1.6359)  classification_loss: 1.5665 (1.5734)  loss_mask: 0.0202 (0.0625)  time: 0.1704  data: 0.0002  max mem: 6052
[04:00:10.373149] Epoch: [26]  [300/781]  eta: 0:01:23  lr: 0.000220  training_loss: 1.5758 (1.6336)  classification_loss: 1.5675 (1.5742)  loss_mask: 0.0123 (0.0594)  time: 0.1719  data: 0.0002  max mem: 6052
[04:00:13.774311] Epoch: [26]  [320/781]  eta: 0:01:20  lr: 0.000220  training_loss: 1.5866 (1.6308)  classification_loss: 1.5483 (1.5739)  loss_mask: 0.0135 (0.0570)  time: 0.1700  data: 0.0002  max mem: 6052
[04:00:17.186651] Epoch: [26]  [340/781]  eta: 0:01:16  lr: 0.000220  training_loss: 1.5225 (1.6285)  classification_loss: 1.5131 (1.5734)  loss_mask: 0.0161 (0.0550)  time: 0.1705  data: 0.0002  max mem: 6052
[04:00:20.599945] Epoch: [26]  [360/781]  eta: 0:01:12  lr: 0.000220  training_loss: 1.6325 (1.6287)  classification_loss: 1.5932 (1.5749)  loss_mask: 0.0261 (0.0538)  time: 0.1706  data: 0.0002  max mem: 6052
[04:00:24.011366] Epoch: [26]  [380/781]  eta: 0:01:09  lr: 0.000220  training_loss: 1.5461 (1.6255)  classification_loss: 1.5282 (1.5732)  loss_mask: 0.0210 (0.0523)  time: 0.1705  data: 0.0002  max mem: 6052
[04:00:27.427337] Epoch: [26]  [400/781]  eta: 0:01:05  lr: 0.000220  training_loss: 1.5588 (1.6231)  classification_loss: 1.5255 (1.5717)  loss_mask: 0.0234 (0.0514)  time: 0.1707  data: 0.0003  max mem: 6052
[04:00:30.843970] Epoch: [26]  [420/781]  eta: 0:01:02  lr: 0.000220  training_loss: 1.6241 (1.6240)  classification_loss: 1.5714 (1.5716)  loss_mask: 0.0383 (0.0524)  time: 0.1707  data: 0.0003  max mem: 6052
[04:00:34.271679] Epoch: [26]  [440/781]  eta: 0:00:58  lr: 0.000220  training_loss: 1.5866 (1.6217)  classification_loss: 1.5429 (1.5702)  loss_mask: 0.0288 (0.0515)  time: 0.1712  data: 0.0003  max mem: 6052
[04:00:37.703652] Epoch: [26]  [460/781]  eta: 0:00:55  lr: 0.000220  training_loss: 1.5663 (1.6203)  classification_loss: 1.5364 (1.5693)  loss_mask: 0.0293 (0.0509)  time: 0.1715  data: 0.0002  max mem: 6052
[04:00:41.125885] Epoch: [26]  [480/781]  eta: 0:00:51  lr: 0.000220  training_loss: 1.5771 (1.6201)  classification_loss: 1.5449 (1.5693)  loss_mask: 0.0322 (0.0508)  time: 0.1710  data: 0.0003  max mem: 6052
[04:00:44.535384] Epoch: [26]  [500/781]  eta: 0:00:48  lr: 0.000219  training_loss: 1.5561 (1.6189)  classification_loss: 1.5449 (1.5683)  loss_mask: 0.0260 (0.0506)  time: 0.1704  data: 0.0002  max mem: 6052
[04:00:47.954903] Epoch: [26]  [520/781]  eta: 0:00:45  lr: 0.000219  training_loss: 1.6110 (1.6198)  classification_loss: 1.5729 (1.5683)  loss_mask: 0.0310 (0.0515)  time: 0.1709  data: 0.0002  max mem: 6052
[04:00:51.365564] Epoch: [26]  [540/781]  eta: 0:00:41  lr: 0.000219  training_loss: 1.6921 (1.6243)  classification_loss: 1.5976 (1.5698)  loss_mask: 0.1079 (0.0546)  time: 0.1704  data: 0.0002  max mem: 6052
[04:00:54.773674] Epoch: [26]  [560/781]  eta: 0:00:38  lr: 0.000219  training_loss: 1.5657 (1.6229)  classification_loss: 1.5421 (1.5693)  loss_mask: 0.0271 (0.0537)  time: 0.1703  data: 0.0002  max mem: 6052
[04:00:58.183215] Epoch: [26]  [580/781]  eta: 0:00:34  lr: 0.000219  training_loss: 1.6208 (1.6225)  classification_loss: 1.5885 (1.5698)  loss_mask: 0.0203 (0.0526)  time: 0.1704  data: 0.0002  max mem: 6052
[04:01:01.582104] Epoch: [26]  [600/781]  eta: 0:00:31  lr: 0.000219  training_loss: 1.5590 (1.6218)  classification_loss: 1.5474 (1.5697)  loss_mask: 0.0156 (0.0520)  time: 0.1699  data: 0.0002  max mem: 6052
[04:01:04.994305] Epoch: [26]  [620/781]  eta: 0:00:27  lr: 0.000219  training_loss: 1.6434 (1.6236)  classification_loss: 1.5475 (1.5700)  loss_mask: 0.0774 (0.0536)  time: 0.1705  data: 0.0002  max mem: 6052
[04:01:08.432731] Epoch: [26]  [640/781]  eta: 0:00:24  lr: 0.000219  training_loss: 1.6329 (1.6244)  classification_loss: 1.5827 (1.5704)  loss_mask: 0.0458 (0.0540)  time: 0.1718  data: 0.0002  max mem: 6052
[04:01:11.865170] Epoch: [26]  [660/781]  eta: 0:00:20  lr: 0.000219  training_loss: 1.6046 (1.6235)  classification_loss: 1.5579 (1.5701)  loss_mask: 0.0245 (0.0534)  time: 0.1715  data: 0.0003  max mem: 6052
[04:01:15.297576] Epoch: [26]  [680/781]  eta: 0:00:17  lr: 0.000219  training_loss: 1.6133 (1.6235)  classification_loss: 1.5926 (1.5711)  loss_mask: 0.0161 (0.0524)  time: 0.1715  data: 0.0002  max mem: 6052
[04:01:18.710497] Epoch: [26]  [700/781]  eta: 0:00:13  lr: 0.000219  training_loss: 1.5709 (1.6230)  classification_loss: 1.5444 (1.5714)  loss_mask: 0.0161 (0.0516)  time: 0.1706  data: 0.0002  max mem: 6052
[04:01:22.147885] Epoch: [26]  [720/781]  eta: 0:00:10  lr: 0.000219  training_loss: 1.5526 (1.6219)  classification_loss: 1.5256 (1.5709)  loss_mask: 0.0223 (0.0510)  time: 0.1718  data: 0.0002  max mem: 6052
[04:01:25.569784] Epoch: [26]  [740/781]  eta: 0:00:07  lr: 0.000219  training_loss: 1.5749 (1.6212)  classification_loss: 1.5454 (1.5702)  loss_mask: 0.0319 (0.0511)  time: 0.1710  data: 0.0003  max mem: 6052
[04:01:28.986953] Epoch: [26]  [760/781]  eta: 0:00:03  lr: 0.000219  training_loss: 1.6219 (1.6215)  classification_loss: 1.5805 (1.5707)  loss_mask: 0.0320 (0.0508)  time: 0.1708  data: 0.0003  max mem: 6052
[04:01:32.387465] Epoch: [26]  [780/781]  eta: 0:00:00  lr: 0.000218  training_loss: 1.5646 (1.6207)  classification_loss: 1.5508 (1.5707)  loss_mask: 0.0160 (0.0500)  time: 0.1699  data: 0.0001  max mem: 6052
[04:01:32.541455] Epoch: [26] Total time: 0:02:14 (0.1722 s / it)
[04:01:32.541908] Averaged stats: lr: 0.000218  training_loss: 1.5646 (1.6207)  classification_loss: 1.5508 (1.5707)  loss_mask: 0.0160 (0.0500)
[04:01:33.165832] Test:  [  0/157]  eta: 0:01:37  testing_loss: 0.8676 (0.8676)  acc1: 73.4375 (73.4375)  acc5: 96.8750 (96.8750)  time: 0.6191  data: 0.5896  max mem: 6052
[04:01:33.463680] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.9480 (0.9515)  acc1: 67.1875 (67.1875)  acc5: 98.4375 (98.1534)  time: 0.0832  data: 0.0547  max mem: 6052
[04:01:33.745060] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.9059 (0.9169)  acc1: 68.7500 (68.0060)  acc5: 98.4375 (98.2143)  time: 0.0288  data: 0.0007  max mem: 6052
[04:01:34.029090] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.9043 (0.9249)  acc1: 68.7500 (68.5484)  acc5: 98.4375 (97.6815)  time: 0.0282  data: 0.0001  max mem: 6052
[04:01:34.314583] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.9043 (0.9222)  acc1: 67.1875 (68.7881)  acc5: 98.4375 (97.6372)  time: 0.0283  data: 0.0002  max mem: 6052
[04:01:34.597881] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.9029 (0.9172)  acc1: 68.7500 (68.9645)  acc5: 98.4375 (97.6409)  time: 0.0283  data: 0.0002  max mem: 6052
[04:01:34.881188] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.9104 (0.9179)  acc1: 70.3125 (69.0061)  acc5: 96.8750 (97.5666)  time: 0.0282  data: 0.0002  max mem: 6052
[04:01:35.161473] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.8568 (0.9102)  acc1: 70.3125 (69.4762)  acc5: 96.8750 (97.6012)  time: 0.0280  data: 0.0002  max mem: 6052
[04:01:35.445185] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.8992 (0.9150)  acc1: 70.3125 (69.4252)  acc5: 98.4375 (97.5502)  time: 0.0281  data: 0.0001  max mem: 6052
[04:01:35.730101] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.9293 (0.9152)  acc1: 70.3125 (69.4540)  acc5: 98.4375 (97.5446)  time: 0.0283  data: 0.0002  max mem: 6052
[04:01:36.013255] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.9237 (0.9212)  acc1: 67.1875 (69.1213)  acc5: 98.4375 (97.5402)  time: 0.0283  data: 0.0002  max mem: 6052
[04:01:36.295421] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.9863 (0.9233)  acc1: 65.6250 (69.0878)  acc5: 98.4375 (97.5788)  time: 0.0281  data: 0.0002  max mem: 6052
[04:01:36.577800] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.9348 (0.9224)  acc1: 68.7500 (69.0857)  acc5: 96.8750 (97.5852)  time: 0.0281  data: 0.0002  max mem: 6052
[04:01:36.858440] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.9251 (0.9247)  acc1: 70.3125 (68.9766)  acc5: 96.8750 (97.6264)  time: 0.0280  data: 0.0002  max mem: 6052
[04:01:37.140295] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.9251 (0.9228)  acc1: 70.3125 (69.1711)  acc5: 98.4375 (97.6285)  time: 0.0280  data: 0.0001  max mem: 6052
[04:01:37.418115] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.9024 (0.9189)  acc1: 70.3125 (69.3605)  acc5: 98.4375 (97.6304)  time: 0.0279  data: 0.0001  max mem: 6052
[04:01:37.567387] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.8976 (0.9201)  acc1: 70.3125 (69.2900)  acc5: 96.8750 (97.6300)  time: 0.0268  data: 0.0001  max mem: 6052
[04:01:37.729562] Test: Total time: 0:00:05 (0.0330 s / it)
[04:01:37.730067] * Acc@1 69.290 Acc@5 97.630 loss 0.920
[04:01:37.730409] Accuracy of the network on the 10000 test images: 69.3%
[04:01:37.730642] Max accuracy: 70.34%
[04:01:37.909943] log_dir: ./output_dir
[04:01:38.791345] Epoch: [27]  [  0/781]  eta: 0:11:26  lr: 0.000218  training_loss: 1.6284 (1.6284)  classification_loss: 1.6049 (1.6049)  loss_mask: 0.0235 (0.0235)  time: 0.8796  data: 0.6917  max mem: 6052
[04:01:42.204193] Epoch: [27]  [ 20/781]  eta: 0:02:35  lr: 0.000218  training_loss: 1.5729 (1.5739)  classification_loss: 1.5668 (1.5577)  loss_mask: 0.0152 (0.0162)  time: 0.1705  data: 0.0002  max mem: 6052
[04:01:45.612227] Epoch: [27]  [ 40/781]  eta: 0:02:19  lr: 0.000218  training_loss: 1.5821 (1.5855)  classification_loss: 1.5713 (1.5712)  loss_mask: 0.0099 (0.0143)  time: 0.1703  data: 0.0002  max mem: 6052
[04:01:49.036801] Epoch: [27]  [ 60/781]  eta: 0:02:11  lr: 0.000218  training_loss: 1.5423 (1.5746)  classification_loss: 1.5326 (1.5623)  loss_mask: 0.0067 (0.0123)  time: 0.1712  data: 0.0002  max mem: 6052
[04:01:52.461724] Epoch: [27]  [ 80/781]  eta: 0:02:05  lr: 0.000218  training_loss: 1.6235 (1.5834)  classification_loss: 1.6177 (1.5721)  loss_mask: 0.0070 (0.0113)  time: 0.1712  data: 0.0002  max mem: 6052
[04:01:55.873703] Epoch: [27]  [100/781]  eta: 0:02:01  lr: 0.000218  training_loss: 1.5968 (1.5835)  classification_loss: 1.5458 (1.5652)  loss_mask: 0.0227 (0.0183)  time: 0.1703  data: 0.0002  max mem: 6052
[04:01:59.313200] Epoch: [27]  [120/781]  eta: 0:01:56  lr: 0.000218  training_loss: 1.6542 (1.5975)  classification_loss: 1.5439 (1.5626)  loss_mask: 0.0972 (0.0349)  time: 0.1719  data: 0.0003  max mem: 6052
[04:02:02.725303] Epoch: [27]  [140/781]  eta: 0:01:52  lr: 0.000218  training_loss: 1.6208 (1.6051)  classification_loss: 1.5875 (1.5665)  loss_mask: 0.0571 (0.0386)  time: 0.1705  data: 0.0003  max mem: 6052
[04:02:06.150400] Epoch: [27]  [160/781]  eta: 0:01:48  lr: 0.000218  training_loss: 1.5769 (1.6008)  classification_loss: 1.5477 (1.5612)  loss_mask: 0.0341 (0.0396)  time: 0.1712  data: 0.0002  max mem: 6052
[04:02:09.569603] Epoch: [27]  [180/781]  eta: 0:01:45  lr: 0.000218  training_loss: 1.6004 (1.6002)  classification_loss: 1.5296 (1.5606)  loss_mask: 0.0298 (0.0396)  time: 0.1709  data: 0.0002  max mem: 6052
[04:02:12.986454] Epoch: [27]  [200/781]  eta: 0:01:41  lr: 0.000218  training_loss: 1.6395 (1.6033)  classification_loss: 1.6042 (1.5641)  loss_mask: 0.0267 (0.0391)  time: 0.1708  data: 0.0002  max mem: 6052
[04:02:16.393547] Epoch: [27]  [220/781]  eta: 0:01:37  lr: 0.000218  training_loss: 1.6215 (1.6043)  classification_loss: 1.5588 (1.5636)  loss_mask: 0.0436 (0.0407)  time: 0.1703  data: 0.0001  max mem: 6052
[04:02:19.816369] Epoch: [27]  [240/781]  eta: 0:01:34  lr: 0.000218  training_loss: 1.5528 (1.6027)  classification_loss: 1.5427 (1.5641)  loss_mask: 0.0106 (0.0386)  time: 0.1711  data: 0.0003  max mem: 6052
[04:02:23.243432] Epoch: [27]  [260/781]  eta: 0:01:30  lr: 0.000218  training_loss: 1.5709 (1.6004)  classification_loss: 1.5620 (1.5639)  loss_mask: 0.0099 (0.0365)  time: 0.1713  data: 0.0003  max mem: 6052
[04:02:26.654759] Epoch: [27]  [280/781]  eta: 0:01:26  lr: 0.000217  training_loss: 1.5373 (1.5984)  classification_loss: 1.5212 (1.5639)  loss_mask: 0.0075 (0.0345)  time: 0.1705  data: 0.0002  max mem: 6052
[04:02:30.072974] Epoch: [27]  [300/781]  eta: 0:01:23  lr: 0.000217  training_loss: 1.6230 (1.6002)  classification_loss: 1.6052 (1.5669)  loss_mask: 0.0112 (0.0333)  time: 0.1708  data: 0.0002  max mem: 6052
[04:02:33.528344] Epoch: [27]  [320/781]  eta: 0:01:19  lr: 0.000217  training_loss: 1.5750 (1.5981)  classification_loss: 1.5622 (1.5660)  loss_mask: 0.0106 (0.0320)  time: 0.1727  data: 0.0002  max mem: 6052
[04:02:36.957900] Epoch: [27]  [340/781]  eta: 0:01:16  lr: 0.000217  training_loss: 1.5189 (1.5950)  classification_loss: 1.5139 (1.5638)  loss_mask: 0.0136 (0.0312)  time: 0.1714  data: 0.0004  max mem: 6052
[04:02:40.389297] Epoch: [27]  [360/781]  eta: 0:01:12  lr: 0.000217  training_loss: 1.5846 (1.5944)  classification_loss: 1.5670 (1.5644)  loss_mask: 0.0061 (0.0300)  time: 0.1715  data: 0.0002  max mem: 6052
[04:02:43.801255] Epoch: [27]  [380/781]  eta: 0:01:09  lr: 0.000217  training_loss: 1.5505 (1.5940)  classification_loss: 1.5266 (1.5647)  loss_mask: 0.0107 (0.0293)  time: 0.1705  data: 0.0002  max mem: 6052
[04:02:47.226712] Epoch: [27]  [400/781]  eta: 0:01:05  lr: 0.000217  training_loss: 1.5271 (1.5911)  classification_loss: 1.5174 (1.5618)  loss_mask: 0.0156 (0.0293)  time: 0.1712  data: 0.0003  max mem: 6052
[04:02:50.684362] Epoch: [27]  [420/781]  eta: 0:01:02  lr: 0.000217  training_loss: 1.5721 (1.5910)  classification_loss: 1.5469 (1.5618)  loss_mask: 0.0180 (0.0291)  time: 0.1728  data: 0.0002  max mem: 6052
[04:02:54.103792] Epoch: [27]  [440/781]  eta: 0:00:58  lr: 0.000217  training_loss: 1.6723 (1.5984)  classification_loss: 1.5756 (1.5623)  loss_mask: 0.0924 (0.0361)  time: 0.1709  data: 0.0002  max mem: 6052
[04:02:57.526583] Epoch: [27]  [460/781]  eta: 0:00:55  lr: 0.000217  training_loss: 1.7369 (1.6043)  classification_loss: 1.5575 (1.5623)  loss_mask: 0.1170 (0.0420)  time: 0.1711  data: 0.0002  max mem: 6052
[04:03:00.949492] Epoch: [27]  [480/781]  eta: 0:00:51  lr: 0.000217  training_loss: 1.6433 (1.6062)  classification_loss: 1.5857 (1.5639)  loss_mask: 0.0391 (0.0423)  time: 0.1711  data: 0.0002  max mem: 6052
[04:03:04.440920] Epoch: [27]  [500/781]  eta: 0:00:48  lr: 0.000217  training_loss: 1.5427 (1.6052)  classification_loss: 1.5067 (1.5633)  loss_mask: 0.0253 (0.0419)  time: 0.1745  data: 0.0002  max mem: 6052
[04:03:07.994379] Epoch: [27]  [520/781]  eta: 0:00:45  lr: 0.000217  training_loss: 1.5672 (1.6044)  classification_loss: 1.5474 (1.5634)  loss_mask: 0.0156 (0.0410)  time: 0.1776  data: 0.0003  max mem: 6052
[04:03:11.404968] Epoch: [27]  [540/781]  eta: 0:00:41  lr: 0.000217  training_loss: 1.5992 (1.6036)  classification_loss: 1.5829 (1.5635)  loss_mask: 0.0147 (0.0401)  time: 0.1705  data: 0.0002  max mem: 6052
[04:03:14.817706] Epoch: [27]  [560/781]  eta: 0:00:38  lr: 0.000216  training_loss: 1.5204 (1.6017)  classification_loss: 1.4799 (1.5616)  loss_mask: 0.0313 (0.0401)  time: 0.1706  data: 0.0002  max mem: 6052
[04:03:18.245154] Epoch: [27]  [580/781]  eta: 0:00:34  lr: 0.000216  training_loss: 1.6109 (1.6014)  classification_loss: 1.5880 (1.5619)  loss_mask: 0.0196 (0.0395)  time: 0.1713  data: 0.0002  max mem: 6052
[04:03:21.700186] Epoch: [27]  [600/781]  eta: 0:00:31  lr: 0.000216  training_loss: 1.5133 (1.5992)  classification_loss: 1.4988 (1.5605)  loss_mask: 0.0143 (0.0387)  time: 0.1727  data: 0.0002  max mem: 6052
[04:03:25.122303] Epoch: [27]  [620/781]  eta: 0:00:27  lr: 0.000216  training_loss: 1.5304 (1.5976)  classification_loss: 1.5241 (1.5598)  loss_mask: 0.0064 (0.0377)  time: 0.1710  data: 0.0003  max mem: 6052
[04:03:28.544724] Epoch: [27]  [640/781]  eta: 0:00:24  lr: 0.000216  training_loss: 1.4990 (1.5956)  classification_loss: 1.4942 (1.5584)  loss_mask: 0.0080 (0.0372)  time: 0.1711  data: 0.0003  max mem: 6052
[04:03:31.953485] Epoch: [27]  [660/781]  eta: 0:00:20  lr: 0.000216  training_loss: 1.5938 (1.5953)  classification_loss: 1.5496 (1.5582)  loss_mask: 0.0286 (0.0371)  time: 0.1704  data: 0.0003  max mem: 6052
[04:03:35.380663] Epoch: [27]  [680/781]  eta: 0:00:17  lr: 0.000216  training_loss: 1.5924 (1.5946)  classification_loss: 1.5859 (1.5582)  loss_mask: 0.0079 (0.0364)  time: 0.1713  data: 0.0002  max mem: 6052
[04:03:38.847013] Epoch: [27]  [700/781]  eta: 0:00:13  lr: 0.000216  training_loss: 1.5426 (1.5945)  classification_loss: 1.5359 (1.5589)  loss_mask: 0.0069 (0.0356)  time: 0.1732  data: 0.0002  max mem: 6052
[04:03:42.250421] Epoch: [27]  [720/781]  eta: 0:00:10  lr: 0.000216  training_loss: 1.5359 (1.5941)  classification_loss: 1.5306 (1.5585)  loss_mask: 0.0127 (0.0357)  time: 0.1701  data: 0.0003  max mem: 6052
[04:03:45.643297] Epoch: [27]  [740/781]  eta: 0:00:07  lr: 0.000216  training_loss: 1.5436 (1.5928)  classification_loss: 1.4985 (1.5573)  loss_mask: 0.0145 (0.0355)  time: 0.1696  data: 0.0003  max mem: 6052
[04:03:49.048525] Epoch: [27]  [760/781]  eta: 0:00:03  lr: 0.000216  training_loss: 1.6323 (1.5945)  classification_loss: 1.5214 (1.5575)  loss_mask: 0.0490 (0.0370)  time: 0.1702  data: 0.0004  max mem: 6052
[04:03:52.451003] Epoch: [27]  [780/781]  eta: 0:00:00  lr: 0.000216  training_loss: 1.6850 (1.5975)  classification_loss: 1.5543 (1.5578)  loss_mask: 0.1038 (0.0397)  time: 0.1700  data: 0.0001  max mem: 6052
[04:03:52.600323] Epoch: [27] Total time: 0:02:14 (0.1725 s / it)
[04:03:52.601049] Averaged stats: lr: 0.000216  training_loss: 1.6850 (1.5975)  classification_loss: 1.5543 (1.5578)  loss_mask: 0.1038 (0.0397)
[04:03:53.263808] Test:  [  0/157]  eta: 0:01:43  testing_loss: 0.8582 (0.8582)  acc1: 78.1250 (78.1250)  acc5: 93.7500 (93.7500)  time: 0.6584  data: 0.6282  max mem: 6052
[04:03:53.549488] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.8715 (0.9201)  acc1: 68.7500 (68.4659)  acc5: 98.4375 (98.0114)  time: 0.0855  data: 0.0573  max mem: 6052
[04:03:53.833242] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.8655 (0.8753)  acc1: 70.3125 (70.3125)  acc5: 98.4375 (98.4375)  time: 0.0282  data: 0.0002  max mem: 6052
[04:03:54.116769] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.8871 (0.8857)  acc1: 71.8750 (70.6149)  acc5: 98.4375 (98.0343)  time: 0.0282  data: 0.0002  max mem: 6052
[04:03:54.397175] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.9075 (0.8893)  acc1: 71.8750 (70.3125)  acc5: 98.4375 (98.0564)  time: 0.0280  data: 0.0002  max mem: 6052
[04:03:54.678305] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.8871 (0.8845)  acc1: 71.8750 (71.0172)  acc5: 98.4375 (97.9779)  time: 0.0280  data: 0.0002  max mem: 6052
[04:03:54.959587] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.8432 (0.8795)  acc1: 71.8750 (70.8504)  acc5: 96.8750 (97.8227)  time: 0.0280  data: 0.0002  max mem: 6052
[04:03:55.240032] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.8260 (0.8719)  acc1: 71.8750 (71.1928)  acc5: 96.8750 (97.7993)  time: 0.0280  data: 0.0002  max mem: 6052
[04:03:55.527377] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.8609 (0.8794)  acc1: 71.8750 (70.8912)  acc5: 96.8750 (97.7238)  time: 0.0283  data: 0.0003  max mem: 6052
[04:03:55.814512] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.8765 (0.8825)  acc1: 70.3125 (70.8276)  acc5: 98.4375 (97.8022)  time: 0.0286  data: 0.0003  max mem: 6052
[04:03:56.100719] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.8805 (0.8892)  acc1: 70.3125 (70.6374)  acc5: 98.4375 (97.8187)  time: 0.0286  data: 0.0002  max mem: 6052
[04:03:56.381579] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.9349 (0.8911)  acc1: 68.7500 (70.6503)  acc5: 98.4375 (97.8463)  time: 0.0282  data: 0.0001  max mem: 6052
[04:03:56.663065] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.8549 (0.8854)  acc1: 70.3125 (70.8290)  acc5: 98.4375 (97.8564)  time: 0.0280  data: 0.0001  max mem: 6052
[04:03:56.942831] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.8706 (0.8875)  acc1: 70.3125 (70.7300)  acc5: 98.4375 (97.8769)  time: 0.0279  data: 0.0001  max mem: 6052
[04:03:57.221638] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.9081 (0.8862)  acc1: 70.3125 (70.9220)  acc5: 98.4375 (97.8391)  time: 0.0278  data: 0.0001  max mem: 6052
[04:03:57.499023] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.8723 (0.8847)  acc1: 73.4375 (71.0058)  acc5: 96.8750 (97.8270)  time: 0.0277  data: 0.0001  max mem: 6052
[04:03:57.647880] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.8723 (0.8859)  acc1: 70.3125 (70.9600)  acc5: 98.4375 (97.8300)  time: 0.0268  data: 0.0001  max mem: 6052
[04:03:57.814997] Test: Total time: 0:00:05 (0.0332 s / it)
[04:03:57.816231] * Acc@1 70.960 Acc@5 97.830 loss 0.886
[04:03:57.816978] Accuracy of the network on the 10000 test images: 71.0%
[04:03:57.817595] Max accuracy: 70.96%
[04:03:57.986900] log_dir: ./output_dir
[04:03:58.854840] Epoch: [28]  [  0/781]  eta: 0:11:15  lr: 0.000216  training_loss: 1.5336 (1.5336)  classification_loss: 1.5044 (1.5044)  loss_mask: 0.0292 (0.0292)  time: 0.8650  data: 0.6687  max mem: 6052
[04:04:02.276156] Epoch: [28]  [ 20/781]  eta: 0:02:35  lr: 0.000216  training_loss: 1.6033 (1.6123)  classification_loss: 1.5415 (1.5659)  loss_mask: 0.0362 (0.0463)  time: 0.1709  data: 0.0004  max mem: 6052
[04:04:05.691016] Epoch: [28]  [ 40/781]  eta: 0:02:19  lr: 0.000216  training_loss: 1.5629 (1.6013)  classification_loss: 1.5516 (1.5646)  loss_mask: 0.0156 (0.0367)  time: 0.1707  data: 0.0002  max mem: 6052
[04:04:09.102411] Epoch: [28]  [ 60/781]  eta: 0:02:11  lr: 0.000215  training_loss: 1.5810 (1.5943)  classification_loss: 1.5489 (1.5639)  loss_mask: 0.0108 (0.0304)  time: 0.1705  data: 0.0002  max mem: 6052
[04:04:12.532729] Epoch: [28]  [ 80/781]  eta: 0:02:05  lr: 0.000215  training_loss: 1.5107 (1.5816)  classification_loss: 1.5056 (1.5561)  loss_mask: 0.0099 (0.0255)  time: 0.1714  data: 0.0003  max mem: 6052
[04:04:15.969436] Epoch: [28]  [100/781]  eta: 0:02:01  lr: 0.000215  training_loss: 1.5802 (1.5805)  classification_loss: 1.5777 (1.5586)  loss_mask: 0.0060 (0.0218)  time: 0.1718  data: 0.0003  max mem: 6052
[04:04:19.375715] Epoch: [28]  [120/781]  eta: 0:01:56  lr: 0.000215  training_loss: 1.5431 (1.5723)  classification_loss: 1.5360 (1.5530)  loss_mask: 0.0056 (0.0193)  time: 0.1702  data: 0.0002  max mem: 6052
[04:04:22.831067] Epoch: [28]  [140/781]  eta: 0:01:52  lr: 0.000215  training_loss: 1.5414 (1.5705)  classification_loss: 1.5372 (1.5531)  loss_mask: 0.0048 (0.0174)  time: 0.1727  data: 0.0003  max mem: 6052
[04:04:26.252548] Epoch: [28]  [160/781]  eta: 0:01:48  lr: 0.000215  training_loss: 1.5650 (1.5750)  classification_loss: 1.5405 (1.5539)  loss_mask: 0.0056 (0.0211)  time: 0.1709  data: 0.0002  max mem: 6052
[04:04:29.703159] Epoch: [28]  [180/781]  eta: 0:01:45  lr: 0.000215  training_loss: 1.6261 (1.5811)  classification_loss: 1.5715 (1.5527)  loss_mask: 0.0452 (0.0283)  time: 0.1724  data: 0.0002  max mem: 6052
[04:04:33.121446] Epoch: [28]  [200/781]  eta: 0:01:41  lr: 0.000215  training_loss: 1.5917 (1.5840)  classification_loss: 1.5565 (1.5536)  loss_mask: 0.0411 (0.0304)  time: 0.1708  data: 0.0002  max mem: 6052
[04:04:36.528029] Epoch: [28]  [220/781]  eta: 0:01:37  lr: 0.000215  training_loss: 1.5835 (1.5852)  classification_loss: 1.5364 (1.5531)  loss_mask: 0.0376 (0.0321)  time: 0.1702  data: 0.0002  max mem: 6052
[04:04:39.939438] Epoch: [28]  [240/781]  eta: 0:01:34  lr: 0.000215  training_loss: 1.6111 (1.5879)  classification_loss: 1.5948 (1.5564)  loss_mask: 0.0175 (0.0315)  time: 0.1705  data: 0.0002  max mem: 6052
[04:04:43.362518] Epoch: [28]  [260/781]  eta: 0:01:30  lr: 0.000215  training_loss: 1.5823 (1.5856)  classification_loss: 1.5706 (1.5558)  loss_mask: 0.0098 (0.0298)  time: 0.1711  data: 0.0003  max mem: 6052
[04:04:46.783821] Epoch: [28]  [280/781]  eta: 0:01:26  lr: 0.000215  training_loss: 1.5549 (1.5827)  classification_loss: 1.5289 (1.5543)  loss_mask: 0.0074 (0.0285)  time: 0.1710  data: 0.0002  max mem: 6052
[04:04:50.218464] Epoch: [28]  [300/781]  eta: 0:01:23  lr: 0.000215  training_loss: 1.5374 (1.5806)  classification_loss: 1.5331 (1.5535)  loss_mask: 0.0062 (0.0271)  time: 0.1716  data: 0.0002  max mem: 6052
[04:04:53.653426] Epoch: [28]  [320/781]  eta: 0:01:19  lr: 0.000215  training_loss: 1.5354 (1.5775)  classification_loss: 1.5225 (1.5517)  loss_mask: 0.0055 (0.0258)  time: 0.1717  data: 0.0002  max mem: 6052
[04:04:57.068459] Epoch: [28]  [340/781]  eta: 0:01:16  lr: 0.000214  training_loss: 1.5149 (1.5756)  classification_loss: 1.5040 (1.5502)  loss_mask: 0.0102 (0.0254)  time: 0.1707  data: 0.0004  max mem: 6052
[04:05:00.508742] Epoch: [28]  [360/781]  eta: 0:01:12  lr: 0.000214  training_loss: 1.5450 (1.5766)  classification_loss: 1.5327 (1.5518)  loss_mask: 0.0118 (0.0248)  time: 0.1719  data: 0.0004  max mem: 6052
[04:05:03.923172] Epoch: [28]  [380/781]  eta: 0:01:09  lr: 0.000214  training_loss: 1.5375 (1.5765)  classification_loss: 1.5302 (1.5523)  loss_mask: 0.0066 (0.0242)  time: 0.1706  data: 0.0002  max mem: 6052
[04:05:07.348308] Epoch: [28]  [400/781]  eta: 0:01:05  lr: 0.000214  training_loss: 1.5267 (1.5744)  classification_loss: 1.5223 (1.5508)  loss_mask: 0.0063 (0.0235)  time: 0.1712  data: 0.0002  max mem: 6052
[04:05:10.779687] Epoch: [28]  [420/781]  eta: 0:01:02  lr: 0.000214  training_loss: 1.5784 (1.5735)  classification_loss: 1.5549 (1.5504)  loss_mask: 0.0103 (0.0231)  time: 0.1715  data: 0.0003  max mem: 6052
[04:05:14.198471] Epoch: [28]  [440/781]  eta: 0:00:58  lr: 0.000214  training_loss: 1.5913 (1.5746)  classification_loss: 1.5808 (1.5522)  loss_mask: 0.0054 (0.0224)  time: 0.1709  data: 0.0003  max mem: 6052
[04:05:17.618921] Epoch: [28]  [460/781]  eta: 0:00:55  lr: 0.000214  training_loss: 1.5518 (1.5732)  classification_loss: 1.5491 (1.5515)  loss_mask: 0.0045 (0.0217)  time: 0.1709  data: 0.0003  max mem: 6052
[04:05:21.047083] Epoch: [28]  [480/781]  eta: 0:00:51  lr: 0.000214  training_loss: 1.5789 (1.5729)  classification_loss: 1.5753 (1.5520)  loss_mask: 0.0026 (0.0209)  time: 0.1713  data: 0.0002  max mem: 6052
[04:05:24.468649] Epoch: [28]  [500/781]  eta: 0:00:48  lr: 0.000214  training_loss: 1.5809 (1.5731)  classification_loss: 1.5794 (1.5530)  loss_mask: 0.0017 (0.0201)  time: 0.1710  data: 0.0002  max mem: 6052
[04:05:27.877641] Epoch: [28]  [520/781]  eta: 0:00:45  lr: 0.000214  training_loss: 1.5592 (1.5727)  classification_loss: 1.5578 (1.5532)  loss_mask: 0.0018 (0.0194)  time: 0.1704  data: 0.0003  max mem: 6052
[04:05:31.320422] Epoch: [28]  [540/781]  eta: 0:00:41  lr: 0.000214  training_loss: 1.5479 (1.5723)  classification_loss: 1.5457 (1.5535)  loss_mask: 0.0022 (0.0188)  time: 0.1721  data: 0.0002  max mem: 6052
[04:05:34.777067] Epoch: [28]  [560/781]  eta: 0:00:38  lr: 0.000214  training_loss: 1.5405 (1.5715)  classification_loss: 1.5090 (1.5525)  loss_mask: 0.0025 (0.0190)  time: 0.1727  data: 0.0004  max mem: 6052
[04:05:38.215167] Epoch: [28]  [580/781]  eta: 0:00:34  lr: 0.000214  training_loss: 1.5774 (1.5730)  classification_loss: 1.5373 (1.5524)  loss_mask: 0.0286 (0.0206)  time: 0.1718  data: 0.0002  max mem: 6052
[04:05:41.692459] Epoch: [28]  [600/781]  eta: 0:00:31  lr: 0.000213  training_loss: 1.7352 (1.5790)  classification_loss: 1.5245 (1.5526)  loss_mask: 0.1546 (0.0264)  time: 0.1738  data: 0.0002  max mem: 6052
[04:05:45.117431] Epoch: [28]  [620/781]  eta: 0:00:27  lr: 0.000213  training_loss: 1.6014 (1.5804)  classification_loss: 1.5245 (1.5518)  loss_mask: 0.0642 (0.0286)  time: 0.1712  data: 0.0002  max mem: 6052
[04:05:48.540539] Epoch: [28]  [640/781]  eta: 0:00:24  lr: 0.000213  training_loss: 1.5676 (1.5803)  classification_loss: 1.5418 (1.5516)  loss_mask: 0.0275 (0.0287)  time: 0.1711  data: 0.0002  max mem: 6052
[04:05:51.975880] Epoch: [28]  [660/781]  eta: 0:00:20  lr: 0.000213  training_loss: 1.5735 (1.5799)  classification_loss: 1.5534 (1.5515)  loss_mask: 0.0182 (0.0284)  time: 0.1717  data: 0.0003  max mem: 6052
[04:05:55.396179] Epoch: [28]  [680/781]  eta: 0:00:17  lr: 0.000213  training_loss: 1.5785 (1.5804)  classification_loss: 1.5563 (1.5521)  loss_mask: 0.0120 (0.0283)  time: 0.1709  data: 0.0002  max mem: 6052
[04:05:58.819198] Epoch: [28]  [700/781]  eta: 0:00:13  lr: 0.000213  training_loss: 1.5844 (1.5816)  classification_loss: 1.5636 (1.5531)  loss_mask: 0.0208 (0.0285)  time: 0.1711  data: 0.0002  max mem: 6052
[04:06:02.249462] Epoch: [28]  [720/781]  eta: 0:00:10  lr: 0.000213  training_loss: 1.5558 (1.5813)  classification_loss: 1.5307 (1.5530)  loss_mask: 0.0157 (0.0283)  time: 0.1714  data: 0.0002  max mem: 6052
[04:06:05.664607] Epoch: [28]  [740/781]  eta: 0:00:07  lr: 0.000213  training_loss: 1.5397 (1.5802)  classification_loss: 1.5296 (1.5524)  loss_mask: 0.0083 (0.0278)  time: 0.1707  data: 0.0003  max mem: 6052
[04:06:09.076304] Epoch: [28]  [760/781]  eta: 0:00:03  lr: 0.000213  training_loss: 1.5823 (1.5802)  classification_loss: 1.5717 (1.5529)  loss_mask: 0.0060 (0.0272)  time: 0.1705  data: 0.0003  max mem: 6052
[04:06:12.617170] Epoch: [28]  [780/781]  eta: 0:00:00  lr: 0.000213  training_loss: 1.5218 (1.5797)  classification_loss: 1.5194 (1.5530)  loss_mask: 0.0033 (0.0268)  time: 0.1769  data: 0.0002  max mem: 6052
[04:06:12.785660] Epoch: [28] Total time: 0:02:14 (0.1726 s / it)
[04:06:12.786175] Averaged stats: lr: 0.000213  training_loss: 1.5218 (1.5797)  classification_loss: 1.5194 (1.5530)  loss_mask: 0.0033 (0.0268)
[04:06:13.456563] Test:  [  0/157]  eta: 0:01:43  testing_loss: 0.8652 (0.8652)  acc1: 75.0000 (75.0000)  acc5: 100.0000 (100.0000)  time: 0.6595  data: 0.6274  max mem: 6052
[04:06:13.744218] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.8652 (0.8955)  acc1: 73.4375 (71.4489)  acc5: 98.4375 (98.7216)  time: 0.0860  data: 0.0572  max mem: 6052
[04:06:14.027658] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.8532 (0.8569)  acc1: 73.4375 (72.9911)  acc5: 98.4375 (98.8095)  time: 0.0284  data: 0.0002  max mem: 6052
[04:06:14.313042] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.8570 (0.8656)  acc1: 73.4375 (72.6815)  acc5: 98.4375 (98.1351)  time: 0.0283  data: 0.0001  max mem: 6052
[04:06:14.595829] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.8570 (0.8650)  acc1: 71.8750 (72.4848)  acc5: 98.4375 (98.0945)  time: 0.0283  data: 0.0001  max mem: 6052
[04:06:14.881395] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.8228 (0.8584)  acc1: 71.8750 (72.5184)  acc5: 98.4375 (98.1311)  time: 0.0283  data: 0.0003  max mem: 6052
[04:06:15.164504] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.8228 (0.8575)  acc1: 70.3125 (72.2592)  acc5: 98.4375 (98.0277)  time: 0.0283  data: 0.0003  max mem: 6052
[04:06:15.449162] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.8041 (0.8505)  acc1: 70.3125 (72.4252)  acc5: 96.8750 (97.9754)  time: 0.0283  data: 0.0002  max mem: 6052
[04:06:15.736297] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.8510 (0.8572)  acc1: 71.8750 (72.0486)  acc5: 96.8750 (97.8974)  time: 0.0285  data: 0.0002  max mem: 6052
[04:06:16.017773] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.8619 (0.8587)  acc1: 70.3125 (71.9437)  acc5: 98.4375 (97.9911)  time: 0.0283  data: 0.0002  max mem: 6052
[04:06:16.304680] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.8641 (0.8626)  acc1: 70.3125 (71.8595)  acc5: 98.4375 (97.9270)  time: 0.0283  data: 0.0001  max mem: 6052
[04:06:16.589191] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.9220 (0.8654)  acc1: 70.3125 (71.7202)  acc5: 96.8750 (97.8885)  time: 0.0284  data: 0.0001  max mem: 6052
[04:06:16.873114] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.8569 (0.8626)  acc1: 71.8750 (71.8492)  acc5: 98.4375 (97.8822)  time: 0.0283  data: 0.0002  max mem: 6052
[04:06:17.157162] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.8569 (0.8638)  acc1: 71.8750 (71.8034)  acc5: 98.4375 (97.9127)  time: 0.0283  data: 0.0002  max mem: 6052
[04:06:17.435807] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.8663 (0.8612)  acc1: 70.3125 (71.7199)  acc5: 98.4375 (97.9499)  time: 0.0280  data: 0.0001  max mem: 6052
[04:06:17.713749] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.8684 (0.8598)  acc1: 71.8750 (71.7405)  acc5: 98.4375 (97.9408)  time: 0.0277  data: 0.0001  max mem: 6052
[04:06:17.862436] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.8112 (0.8616)  acc1: 73.4375 (71.5900)  acc5: 98.4375 (97.9400)  time: 0.0268  data: 0.0001  max mem: 6052
[04:06:18.017689] Test: Total time: 0:00:05 (0.0333 s / it)
[04:06:18.018156] * Acc@1 71.590 Acc@5 97.940 loss 0.862
[04:06:18.018452] Accuracy of the network on the 10000 test images: 71.6%
[04:06:18.018642] Max accuracy: 71.59%
[04:06:18.158699] log_dir: ./output_dir
[04:06:19.064427] Epoch: [29]  [  0/781]  eta: 0:11:46  lr: 0.000213  training_loss: 1.4542 (1.4542)  classification_loss: 1.4291 (1.4291)  loss_mask: 0.0251 (0.0251)  time: 0.9042  data: 0.7161  max mem: 6052
[04:06:22.483501] Epoch: [29]  [ 20/781]  eta: 0:02:36  lr: 0.000213  training_loss: 1.5988 (1.6061)  classification_loss: 1.4772 (1.4870)  loss_mask: 0.0446 (0.1192)  time: 0.1709  data: 0.0002  max mem: 6052
[04:06:25.915932] Epoch: [29]  [ 40/781]  eta: 0:02:20  lr: 0.000213  training_loss: 1.5871 (1.6026)  classification_loss: 1.5806 (1.5269)  loss_mask: 0.0228 (0.0757)  time: 0.1715  data: 0.0004  max mem: 6052
[04:06:29.317759] Epoch: [29]  [ 60/781]  eta: 0:02:11  lr: 0.000213  training_loss: 1.5461 (1.5964)  classification_loss: 1.5213 (1.5350)  loss_mask: 0.0176 (0.0614)  time: 0.1700  data: 0.0003  max mem: 6052
[04:06:32.735218] Epoch: [29]  [ 80/781]  eta: 0:02:06  lr: 0.000213  training_loss: 1.6266 (1.6060)  classification_loss: 1.5632 (1.5415)  loss_mask: 0.0541 (0.0645)  time: 0.1708  data: 0.0002  max mem: 6052
[04:06:36.173187] Epoch: [29]  [100/781]  eta: 0:02:01  lr: 0.000212  training_loss: 1.5881 (1.6045)  classification_loss: 1.5206 (1.5435)  loss_mask: 0.0207 (0.0610)  time: 0.1718  data: 0.0003  max mem: 6052
[04:06:39.597269] Epoch: [29]  [120/781]  eta: 0:01:57  lr: 0.000212  training_loss: 1.6243 (1.6122)  classification_loss: 1.6084 (1.5502)  loss_mask: 0.0576 (0.0620)  time: 0.1711  data: 0.0002  max mem: 6052
[04:06:43.018777] Epoch: [29]  [140/781]  eta: 0:01:52  lr: 0.000212  training_loss: 1.5539 (1.6013)  classification_loss: 1.4965 (1.5415)  loss_mask: 0.0326 (0.0598)  time: 0.1710  data: 0.0002  max mem: 6052
[04:06:46.428758] Epoch: [29]  [160/781]  eta: 0:01:48  lr: 0.000212  training_loss: 1.5195 (1.5958)  classification_loss: 1.4960 (1.5394)  loss_mask: 0.0212 (0.0564)  time: 0.1704  data: 0.0003  max mem: 6052
[04:06:49.835771] Epoch: [29]  [180/781]  eta: 0:01:45  lr: 0.000212  training_loss: 1.5644 (1.5925)  classification_loss: 1.5139 (1.5366)  loss_mask: 0.0435 (0.0559)  time: 0.1703  data: 0.0002  max mem: 6052
[04:06:53.228921] Epoch: [29]  [200/781]  eta: 0:01:41  lr: 0.000212  training_loss: 1.5632 (1.5903)  classification_loss: 1.5376 (1.5371)  loss_mask: 0.0188 (0.0532)  time: 0.1696  data: 0.0003  max mem: 6052
[04:06:56.632806] Epoch: [29]  [220/781]  eta: 0:01:37  lr: 0.000212  training_loss: 1.5128 (1.5831)  classification_loss: 1.4822 (1.5323)  loss_mask: 0.0182 (0.0508)  time: 0.1701  data: 0.0003  max mem: 6052
[04:07:00.029369] Epoch: [29]  [240/781]  eta: 0:01:33  lr: 0.000212  training_loss: 1.5947 (1.5819)  classification_loss: 1.5000 (1.5323)  loss_mask: 0.0226 (0.0496)  time: 0.1697  data: 0.0001  max mem: 6052
[04:07:03.414231] Epoch: [29]  [260/781]  eta: 0:01:30  lr: 0.000212  training_loss: 1.5443 (1.5811)  classification_loss: 1.5172 (1.5313)  loss_mask: 0.0271 (0.0498)  time: 0.1692  data: 0.0003  max mem: 6052
[04:07:06.824025] Epoch: [29]  [280/781]  eta: 0:01:26  lr: 0.000212  training_loss: 1.6129 (1.5833)  classification_loss: 1.5207 (1.5308)  loss_mask: 0.0438 (0.0525)  time: 0.1704  data: 0.0003  max mem: 6052
[04:07:10.244155] Epoch: [29]  [300/781]  eta: 0:01:23  lr: 0.000212  training_loss: 1.5842 (1.5835)  classification_loss: 1.5521 (1.5314)  loss_mask: 0.0360 (0.0521)  time: 0.1709  data: 0.0001  max mem: 6052
[04:07:13.669198] Epoch: [29]  [320/781]  eta: 0:01:19  lr: 0.000212  training_loss: 1.5479 (1.5817)  classification_loss: 1.5315 (1.5312)  loss_mask: 0.0210 (0.0505)  time: 0.1712  data: 0.0003  max mem: 6052
[04:07:17.072985] Epoch: [29]  [340/781]  eta: 0:01:16  lr: 0.000212  training_loss: 1.5704 (1.5825)  classification_loss: 1.5238 (1.5317)  loss_mask: 0.0161 (0.0508)  time: 0.1701  data: 0.0003  max mem: 6052
[04:07:20.507256] Epoch: [29]  [360/781]  eta: 0:01:12  lr: 0.000211  training_loss: 1.6406 (1.5860)  classification_loss: 1.5200 (1.5325)  loss_mask: 0.0843 (0.0534)  time: 0.1716  data: 0.0003  max mem: 6052
[04:07:23.919052] Epoch: [29]  [380/781]  eta: 0:01:09  lr: 0.000211  training_loss: 1.5800 (1.5865)  classification_loss: 1.5512 (1.5343)  loss_mask: 0.0249 (0.0522)  time: 0.1705  data: 0.0002  max mem: 6052
[04:07:27.346561] Epoch: [29]  [400/781]  eta: 0:01:05  lr: 0.000211  training_loss: 1.5505 (1.5855)  classification_loss: 1.5265 (1.5350)  loss_mask: 0.0168 (0.0505)  time: 0.1713  data: 0.0002  max mem: 6052
[04:07:30.769791] Epoch: [29]  [420/781]  eta: 0:01:02  lr: 0.000211  training_loss: 1.5313 (1.5831)  classification_loss: 1.5185 (1.5345)  loss_mask: 0.0089 (0.0487)  time: 0.1711  data: 0.0002  max mem: 6052
[04:07:34.194640] Epoch: [29]  [440/781]  eta: 0:00:58  lr: 0.000211  training_loss: 1.5046 (1.5813)  classification_loss: 1.4892 (1.5337)  loss_mask: 0.0144 (0.0477)  time: 0.1712  data: 0.0002  max mem: 6052
[04:07:37.604702] Epoch: [29]  [460/781]  eta: 0:00:55  lr: 0.000211  training_loss: 1.6248 (1.5864)  classification_loss: 1.4953 (1.5325)  loss_mask: 0.0792 (0.0539)  time: 0.1704  data: 0.0002  max mem: 6052
[04:07:41.030389] Epoch: [29]  [480/781]  eta: 0:00:51  lr: 0.000211  training_loss: 1.6331 (1.5881)  classification_loss: 1.5736 (1.5338)  loss_mask: 0.0497 (0.0543)  time: 0.1712  data: 0.0002  max mem: 6052
[04:07:44.447217] Epoch: [29]  [500/781]  eta: 0:00:48  lr: 0.000211  training_loss: 1.5170 (1.5871)  classification_loss: 1.4942 (1.5334)  loss_mask: 0.0370 (0.0537)  time: 0.1708  data: 0.0002  max mem: 6052
[04:07:47.854623] Epoch: [29]  [520/781]  eta: 0:00:44  lr: 0.000211  training_loss: 1.5811 (1.5865)  classification_loss: 1.5512 (1.5336)  loss_mask: 0.0239 (0.0528)  time: 0.1703  data: 0.0002  max mem: 6052
[04:07:51.281731] Epoch: [29]  [540/781]  eta: 0:00:41  lr: 0.000211  training_loss: 1.5703 (1.5850)  classification_loss: 1.5439 (1.5334)  loss_mask: 0.0162 (0.0516)  time: 0.1713  data: 0.0002  max mem: 6052
[04:07:54.712930] Epoch: [29]  [560/781]  eta: 0:00:38  lr: 0.000211  training_loss: 1.5125 (1.5827)  classification_loss: 1.5002 (1.5323)  loss_mask: 0.0146 (0.0503)  time: 0.1715  data: 0.0002  max mem: 6052
[04:07:58.142279] Epoch: [29]  [580/781]  eta: 0:00:34  lr: 0.000211  training_loss: 1.5105 (1.5808)  classification_loss: 1.4951 (1.5316)  loss_mask: 0.0154 (0.0491)  time: 0.1714  data: 0.0002  max mem: 6052
[04:08:01.566508] Epoch: [29]  [600/781]  eta: 0:00:31  lr: 0.000211  training_loss: 1.5161 (1.5789)  classification_loss: 1.5054 (1.5311)  loss_mask: 0.0102 (0.0478)  time: 0.1711  data: 0.0002  max mem: 6052
[04:08:04.985547] Epoch: [29]  [620/781]  eta: 0:00:27  lr: 0.000210  training_loss: 1.5853 (1.5787)  classification_loss: 1.5193 (1.5306)  loss_mask: 0.0388 (0.0480)  time: 0.1709  data: 0.0002  max mem: 6052
[04:08:08.411317] Epoch: [29]  [640/781]  eta: 0:00:24  lr: 0.000210  training_loss: 1.5683 (1.5790)  classification_loss: 1.5082 (1.5306)  loss_mask: 0.0359 (0.0484)  time: 0.1712  data: 0.0002  max mem: 6052
[04:08:11.822792] Epoch: [29]  [660/781]  eta: 0:00:20  lr: 0.000210  training_loss: 1.5611 (1.5800)  classification_loss: 1.4931 (1.5303)  loss_mask: 0.0532 (0.0497)  time: 0.1705  data: 0.0002  max mem: 6052
[04:08:15.251403] Epoch: [29]  [680/781]  eta: 0:00:17  lr: 0.000210  training_loss: 1.5886 (1.5802)  classification_loss: 1.5486 (1.5308)  loss_mask: 0.0280 (0.0494)  time: 0.1714  data: 0.0002  max mem: 6052
[04:08:18.670282] Epoch: [29]  [700/781]  eta: 0:00:13  lr: 0.000210  training_loss: 1.5353 (1.5793)  classification_loss: 1.5120 (1.5308)  loss_mask: 0.0177 (0.0485)  time: 0.1709  data: 0.0003  max mem: 6052
[04:08:22.084036] Epoch: [29]  [720/781]  eta: 0:00:10  lr: 0.000210  training_loss: 1.5197 (1.5782)  classification_loss: 1.5073 (1.5306)  loss_mask: 0.0128 (0.0476)  time: 0.1706  data: 0.0003  max mem: 6052

[04:08:25.500736] Epoch: [29]  [740/781]  eta: 0:00:07  lr: 0.000210  training_loss: 1.5301 (1.5771)  classification_loss: 1.4976 (1.5303)  loss_mask: 0.0129 (0.0467)  time: 0.1707  data: 0.0002  max mem: 6052
[04:08:28.935069] Epoch: [29]  [760/781]  eta: 0:00:03  lr: 0.000210  training_loss: 1.5630 (1.5770)  classification_loss: 1.5425 (1.5310)  loss_mask: 0.0143 (0.0460)  time: 0.1716  data: 0.0002  max mem: 6052
[04:08:32.350878] Epoch: [29]  [780/781]  eta: 0:00:00  lr: 0.000210  training_loss: 1.5506 (1.5763)  classification_loss: 1.5468 (1.5312)  loss_mask: 0.0109 (0.0451)  time: 0.1707  data: 0.0002  max mem: 6052
[04:08:32.508810] Epoch: [29] Total time: 0:02:14 (0.1720 s / it)
[04:08:32.509356] Averaged stats: lr: 0.000210  training_loss: 1.5506 (1.5763)  classification_loss: 1.5468 (1.5312)  loss_mask: 0.0109 (0.0451)
[04:08:33.168743] Test:  [  0/157]  eta: 0:01:42  testing_loss: 0.8146 (0.8146)  acc1: 73.4375 (73.4375)  acc5: 100.0000 (100.0000)  time: 0.6551  data: 0.6256  max mem: 6052
[04:08:33.449458] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.8856 (0.9133)  acc1: 67.1875 (67.3295)  acc5: 98.4375 (99.0057)  time: 0.0849  data: 0.0570  max mem: 6052
[04:08:33.728977] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.8795 (0.8667)  acc1: 68.7500 (70.0893)  acc5: 98.4375 (99.0327)  time: 0.0279  data: 0.0001  max mem: 6052
[04:08:34.009217] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.8741 (0.8728)  acc1: 70.3125 (70.3629)  acc5: 98.4375 (98.2863)  time: 0.0279  data: 0.0001  max mem: 6052
[04:08:34.289324] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.8417 (0.8666)  acc1: 71.8750 (70.8841)  acc5: 96.8750 (98.0945)  time: 0.0279  data: 0.0001  max mem: 6052
[04:08:34.568843] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.8089 (0.8581)  acc1: 73.4375 (71.5074)  acc5: 98.4375 (98.0086)  time: 0.0279  data: 0.0001  max mem: 6052
[04:08:34.853923] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.8183 (0.8536)  acc1: 70.3125 (71.3115)  acc5: 96.8750 (98.0020)  time: 0.0281  data: 0.0001  max mem: 6052
[04:08:35.135745] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7947 (0.8477)  acc1: 70.3125 (71.4789)  acc5: 98.4375 (97.9754)  time: 0.0282  data: 0.0001  max mem: 6052
[04:08:35.416402] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7947 (0.8485)  acc1: 73.4375 (71.4892)  acc5: 96.8750 (97.8588)  time: 0.0280  data: 0.0001  max mem: 6052
[04:08:35.696246] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.8385 (0.8486)  acc1: 71.8750 (71.5144)  acc5: 96.8750 (97.9052)  time: 0.0279  data: 0.0002  max mem: 6052
[04:08:35.979920] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.8742 (0.8519)  acc1: 71.8750 (71.4264)  acc5: 98.4375 (97.9579)  time: 0.0281  data: 0.0002  max mem: 6052
[04:08:36.264351] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.8943 (0.8552)  acc1: 70.3125 (71.4105)  acc5: 98.4375 (97.9307)  time: 0.0283  data: 0.0002  max mem: 6052
[04:08:36.549048] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.8014 (0.8493)  acc1: 71.8750 (71.6942)  acc5: 98.4375 (97.9726)  time: 0.0283  data: 0.0002  max mem: 6052
[04:08:36.829935] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.7939 (0.8497)  acc1: 71.8750 (71.6722)  acc5: 98.4375 (97.9723)  time: 0.0282  data: 0.0002  max mem: 6052
[04:08:37.108774] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.8034 (0.8466)  acc1: 71.8750 (71.8639)  acc5: 98.4375 (97.9610)  time: 0.0279  data: 0.0001  max mem: 6052
[04:08:37.386631] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7846 (0.8432)  acc1: 73.4375 (71.9785)  acc5: 98.4375 (97.9512)  time: 0.0277  data: 0.0001  max mem: 6052
[04:08:37.536102] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7796 (0.8447)  acc1: 71.8750 (71.9000)  acc5: 98.4375 (97.9500)  time: 0.0268  data: 0.0001  max mem: 6052
[04:08:37.709079] Test: Total time: 0:00:05 (0.0331 s / it)
[04:08:37.709570] * Acc@1 71.900 Acc@5 97.950 loss 0.845
[04:08:37.709878] Accuracy of the network on the 10000 test images: 71.9%
[04:08:37.710061] Max accuracy: 71.90%
[04:08:37.792751] log_dir: ./output_dir
[04:08:38.664543] Epoch: [30]  [  0/781]  eta: 0:11:19  lr: 0.000210  training_loss: 1.3884 (1.3884)  classification_loss: 1.3825 (1.3825)  loss_mask: 0.0059 (0.0059)  time: 0.8699  data: 0.6676  max mem: 6052
[04:08:42.112468] Epoch: [30]  [ 20/781]  eta: 0:02:36  lr: 0.000210  training_loss: 1.4869 (1.5233)  classification_loss: 1.4820 (1.5154)  loss_mask: 0.0056 (0.0079)  time: 0.1723  data: 0.0002  max mem: 6052
[04:08:45.525476] Epoch: [30]  [ 40/781]  eta: 0:02:19  lr: 0.000210  training_loss: 1.5424 (1.5393)  classification_loss: 1.5319 (1.5335)  loss_mask: 0.0034 (0.0058)  time: 0.1706  data: 0.0002  max mem: 6052
[04:08:48.943794] Epoch: [30]  [ 60/781]  eta: 0:02:11  lr: 0.000210  training_loss: 1.5466 (1.5439)  classification_loss: 1.5397 (1.5389)  loss_mask: 0.0027 (0.0050)  time: 0.1709  data: 0.0002  max mem: 6052
[04:08:52.363528] Epoch: [30]  [ 80/781]  eta: 0:02:06  lr: 0.000210  training_loss: 1.4806 (1.5342)  classification_loss: 1.4770 (1.5298)  loss_mask: 0.0022 (0.0043)  time: 0.1709  data: 0.0003  max mem: 6052
[04:08:55.782397] Epoch: [30]  [100/781]  eta: 0:02:01  lr: 0.000209  training_loss: 1.5197 (1.5328)  classification_loss: 1.4961 (1.5283)  loss_mask: 0.0024 (0.0044)  time: 0.1709  data: 0.0003  max mem: 6052
[04:08:59.192373] Epoch: [30]  [120/781]  eta: 0:01:56  lr: 0.000209  training_loss: 1.4956 (1.5318)  classification_loss: 1.4740 (1.5197)  loss_mask: 0.0174 (0.0121)  time: 0.1704  data: 0.0003  max mem: 6052
[04:09:02.620658] Epoch: [30]  [140/781]  eta: 0:01:52  lr: 0.000209  training_loss: 1.5098 (1.5322)  classification_loss: 1.4542 (1.5136)  loss_mask: 0.0604 (0.0186)  time: 0.1713  data: 0.0002  max mem: 6052
[04:09:06.083529] Epoch: [30]  [160/781]  eta: 0:01:49  lr: 0.000209  training_loss: 1.5222 (1.5376)  classification_loss: 1.5021 (1.5149)  loss_mask: 0.0351 (0.0227)  time: 0.1730  data: 0.0002  max mem: 6052
[04:09:09.576214] Epoch: [30]  [180/781]  eta: 0:01:45  lr: 0.000209  training_loss: 1.5688 (1.5406)  classification_loss: 1.5332 (1.5168)  loss_mask: 0.0223 (0.0239)  time: 0.1746  data: 0.0003  max mem: 6052
[04:09:12.999791] Epoch: [30]  [200/781]  eta: 0:01:41  lr: 0.000209  training_loss: 1.5482 (1.5402)  classification_loss: 1.5142 (1.5170)  loss_mask: 0.0140 (0.0232)  time: 0.1711  data: 0.0002  max mem: 6052
[04:09:16.434694] Epoch: [30]  [220/781]  eta: 0:01:38  lr: 0.000209  training_loss: 1.5381 (1.5400)  classification_loss: 1.5252 (1.5180)  loss_mask: 0.0062 (0.0219)  time: 0.1716  data: 0.0003  max mem: 6052
[04:09:19.861715] Epoch: [30]  [240/781]  eta: 0:01:34  lr: 0.000209  training_loss: 1.5309 (1.5379)  classification_loss: 1.5217 (1.5172)  loss_mask: 0.0045 (0.0207)  time: 0.1713  data: 0.0002  max mem: 6052
[04:09:23.280536] Epoch: [30]  [260/781]  eta: 0:01:30  lr: 0.000209  training_loss: 1.5639 (1.5388)  classification_loss: 1.5553 (1.5194)  loss_mask: 0.0036 (0.0194)  time: 0.1709  data: 0.0002  max mem: 6052
[04:09:26.696143] Epoch: [30]  [280/781]  eta: 0:01:27  lr: 0.000209  training_loss: 1.5374 (1.5383)  classification_loss: 1.5344 (1.5201)  loss_mask: 0.0024 (0.0182)  time: 0.1707  data: 0.0003  max mem: 6052
[04:09:30.114060] Epoch: [30]  [300/781]  eta: 0:01:23  lr: 0.000209  training_loss: 1.5214 (1.5383)  classification_loss: 1.5182 (1.5211)  loss_mask: 0.0027 (0.0172)  time: 0.1708  data: 0.0002  max mem: 6052
[04:09:33.534991] Epoch: [30]  [320/781]  eta: 0:01:20  lr: 0.000209  training_loss: 1.5424 (1.5387)  classification_loss: 1.5397 (1.5224)  loss_mask: 0.0019 (0.0162)  time: 0.1710  data: 0.0003  max mem: 6052
[04:09:36.956094] Epoch: [30]  [340/781]  eta: 0:01:16  lr: 0.000208  training_loss: 1.5313 (1.5384)  classification_loss: 1.5056 (1.5220)  loss_mask: 0.0037 (0.0165)  time: 0.1710  data: 0.0003  max mem: 6052
[04:09:40.378685] Epoch: [30]  [360/781]  eta: 0:01:12  lr: 0.000208  training_loss: 1.7407 (1.5721)  classification_loss: 1.6071 (1.5307)  loss_mask: 0.1624 (0.0414)  time: 0.1710  data: 0.0002  max mem: 6052
[04:09:43.782463] Epoch: [30]  [380/781]  eta: 0:01:09  lr: 0.000208  training_loss: 2.2462 (1.6155)  classification_loss: 1.8687 (1.5491)  loss_mask: 0.3786 (0.0664)  time: 0.1701  data: 0.0003  max mem: 6052
[04:09:47.203081] Epoch: [30]  [400/781]  eta: 0:01:05  lr: 0.000208  training_loss: 1.9241 (1.6311)  classification_loss: 1.7152 (1.5582)  loss_mask: 0.1874 (0.0729)  time: 0.1710  data: 0.0003  max mem: 6052
[04:09:50.617405] Epoch: [30]  [420/781]  eta: 0:01:02  lr: 0.000208  training_loss: 1.8286 (1.6410)  classification_loss: 1.7076 (1.5663)  loss_mask: 0.1040 (0.0747)  time: 0.1706  data: 0.0003  max mem: 6052
[04:09:54.079656] Epoch: [30]  [440/781]  eta: 0:00:58  lr: 0.000208  training_loss: 1.7394 (1.6463)  classification_loss: 1.6628 (1.5709)  loss_mask: 0.0821 (0.0754)  time: 0.1730  data: 0.0002  max mem: 6052
[04:09:57.501664] Epoch: [30]  [460/781]  eta: 0:00:55  lr: 0.000208  training_loss: 1.6591 (1.6460)  classification_loss: 1.5997 (1.5714)  loss_mask: 0.0529 (0.0746)  time: 0.1710  data: 0.0003  max mem: 6052
[04:10:00.934008] Epoch: [30]  [480/781]  eta: 0:00:52  lr: 0.000208  training_loss: 1.6764 (1.6483)  classification_loss: 1.6125 (1.5741)  loss_mask: 0.0575 (0.0742)  time: 0.1715  data: 0.0003  max mem: 6052
[04:10:04.343582] Epoch: [30]  [500/781]  eta: 0:00:48  lr: 0.000208  training_loss: 1.6750 (1.6514)  classification_loss: 1.5930 (1.5750)  loss_mask: 0.0875 (0.0764)  time: 0.1704  data: 0.0002  max mem: 6052
[04:10:07.750575] Epoch: [30]  [520/781]  eta: 0:00:45  lr: 0.000208  training_loss: 1.6630 (1.6507)  classification_loss: 1.5714 (1.5742)  loss_mask: 0.0682 (0.0765)  time: 0.1703  data: 0.0002  max mem: 6052
[04:10:11.163860] Epoch: [30]  [540/781]  eta: 0:00:41  lr: 0.000208  training_loss: 1.6532 (1.6508)  classification_loss: 1.5966 (1.5745)  loss_mask: 0.0704 (0.0762)  time: 0.1706  data: 0.0001  max mem: 6052
[04:10:14.616433] Epoch: [30]  [560/781]  eta: 0:00:38  lr: 0.000208  training_loss: 1.6122 (1.6491)  classification_loss: 1.5808 (1.5737)  loss_mask: 0.0523 (0.0754)  time: 0.1725  data: 0.0002  max mem: 6052
[04:10:18.039770] Epoch: [30]  [580/781]  eta: 0:00:34  lr: 0.000208  training_loss: 1.5938 (1.6481)  classification_loss: 1.5315 (1.5731)  loss_mask: 0.0477 (0.0749)  time: 0.1711  data: 0.0004  max mem: 6052
[04:10:21.445196] Epoch: [30]  [600/781]  eta: 0:00:31  lr: 0.000207  training_loss: 1.5958 (1.6471)  classification_loss: 1.5376 (1.5726)  loss_mask: 0.0466 (0.0745)  time: 0.1702  data: 0.0002  max mem: 6052
[04:10:24.878100] Epoch: [30]  [620/781]  eta: 0:00:27  lr: 0.000207  training_loss: 1.6180 (1.6461)  classification_loss: 1.5764 (1.5724)  loss_mask: 0.0415 (0.0737)  time: 0.1716  data: 0.0004  max mem: 6052
[04:10:28.314802] Epoch: [30]  [640/781]  eta: 0:00:24  lr: 0.000207  training_loss: 1.5584 (1.6440)  classification_loss: 1.4832 (1.5705)  loss_mask: 0.0435 (0.0735)  time: 0.1718  data: 0.0002  max mem: 6052
[04:10:31.739305] Epoch: [30]  [660/781]  eta: 0:00:20  lr: 0.000207  training_loss: 1.5626 (1.6427)  classification_loss: 1.5167 (1.5700)  loss_mask: 0.0438 (0.0727)  time: 0.1712  data: 0.0002  max mem: 6052
[04:10:35.156109] Epoch: [30]  [680/781]  eta: 0:00:17  lr: 0.000207  training_loss: 1.5965 (1.6404)  classification_loss: 1.5169 (1.5680)  loss_mask: 0.0408 (0.0724)  time: 0.1708  data: 0.0001  max mem: 6052
[04:10:38.584057] Epoch: [30]  [700/781]  eta: 0:00:13  lr: 0.000207  training_loss: 1.6272 (1.6397)  classification_loss: 1.5538 (1.5676)  loss_mask: 0.0432 (0.0721)  time: 0.1713  data: 0.0003  max mem: 6052
[04:10:42.012485] Epoch: [30]  [720/781]  eta: 0:00:10  lr: 0.000207  training_loss: 1.6001 (1.6389)  classification_loss: 1.5592 (1.5673)  loss_mask: 0.0447 (0.0716)  time: 0.1713  data: 0.0003  max mem: 6052
[04:10:45.420790] Epoch: [30]  [740/781]  eta: 0:00:07  lr: 0.000207  training_loss: 1.6142 (1.6384)  classification_loss: 1.5393 (1.5665)  loss_mask: 0.0594 (0.0719)  time: 0.1703  data: 0.0002  max mem: 6052
[04:10:48.824699] Epoch: [30]  [760/781]  eta: 0:00:03  lr: 0.000207  training_loss: 1.5955 (1.6380)  classification_loss: 1.5447 (1.5663)  loss_mask: 0.0560 (0.0717)  time: 0.1701  data: 0.0003  max mem: 6052
[04:10:52.228075] Epoch: [30]  [780/781]  eta: 0:00:00  lr: 0.000207  training_loss: 1.6193 (1.6373)  classification_loss: 1.5327 (1.5657)  loss_mask: 0.0696 (0.0716)  time: 0.1701  data: 0.0002  max mem: 6052
[04:10:52.391269] Epoch: [30] Total time: 0:02:14 (0.1723 s / it)
[04:10:52.391785] Averaged stats: lr: 0.000207  training_loss: 1.6193 (1.6373)  classification_loss: 1.5327 (1.5657)  loss_mask: 0.0696 (0.0716)
[04:10:54.370652] Test:  [  0/157]  eta: 0:01:47  testing_loss: 0.8614 (0.8614)  acc1: 70.3125 (70.3125)  acc5: 96.8750 (96.8750)  time: 0.6829  data: 0.6504  max mem: 6052
[04:10:54.669585] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.8614 (0.8680)  acc1: 70.3125 (71.1648)  acc5: 98.4375 (98.2955)  time: 0.0890  data: 0.0593  max mem: 6052
[04:10:54.960867] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.8080 (0.8385)  acc1: 71.8750 (72.1726)  acc5: 100.0000 (98.8095)  time: 0.0293  data: 0.0002  max mem: 6052
[04:10:55.243594] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.8328 (0.8557)  acc1: 70.3125 (71.9758)  acc5: 98.4375 (98.3871)  time: 0.0285  data: 0.0002  max mem: 6052
[04:10:55.528454] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.8328 (0.8591)  acc1: 70.3125 (71.8750)  acc5: 98.4375 (98.2851)  time: 0.0282  data: 0.0002  max mem: 6052
[04:10:55.808968] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.8541 (0.8579)  acc1: 70.3125 (72.0895)  acc5: 98.4375 (98.1005)  time: 0.0281  data: 0.0002  max mem: 6052
[04:10:56.091241] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.8541 (0.8580)  acc1: 70.3125 (71.6701)  acc5: 98.4375 (98.0533)  time: 0.0280  data: 0.0002  max mem: 6052
[04:10:56.377039] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.8237 (0.8502)  acc1: 71.8750 (71.8750)  acc5: 96.8750 (97.9313)  time: 0.0283  data: 0.0004  max mem: 6052
[04:10:56.664774] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.8411 (0.8531)  acc1: 71.8750 (71.6821)  acc5: 98.4375 (97.9360)  time: 0.0286  data: 0.0005  max mem: 6052
[04:10:56.946062] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.8635 (0.8565)  acc1: 70.3125 (71.5316)  acc5: 98.4375 (97.9567)  time: 0.0283  data: 0.0003  max mem: 6052
[04:10:57.228764] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.8799 (0.8589)  acc1: 68.7500 (71.3490)  acc5: 98.4375 (97.9579)  time: 0.0281  data: 0.0002  max mem: 6052
[04:10:57.512635] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.9069 (0.8606)  acc1: 68.7500 (71.2697)  acc5: 96.8750 (97.8604)  time: 0.0282  data: 0.0001  max mem: 6052
[04:10:57.801904] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.8439 (0.8582)  acc1: 68.7500 (71.4747)  acc5: 98.4375 (97.8951)  time: 0.0285  data: 0.0001  max mem: 6052
[04:10:58.084573] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.8457 (0.8599)  acc1: 71.8750 (71.4098)  acc5: 98.4375 (97.9485)  time: 0.0284  data: 0.0001  max mem: 6052
[04:10:58.364784] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.8835 (0.8586)  acc1: 71.8750 (71.5758)  acc5: 98.4375 (97.9388)  time: 0.0279  data: 0.0001  max mem: 6052
[04:10:58.641887] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.8491 (0.8567)  acc1: 73.4375 (71.5646)  acc5: 98.4375 (97.9305)  time: 0.0277  data: 0.0001  max mem: 6052
[04:10:58.790551] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.8372 (0.8585)  acc1: 73.4375 (71.4700)  acc5: 98.4375 (97.9400)  time: 0.0267  data: 0.0001  max mem: 6052
[04:10:58.942568] Test: Total time: 0:00:05 (0.0335 s / it)
[04:10:58.943103] * Acc@1 71.470 Acc@5 97.940 loss 0.858
[04:10:58.943431] Accuracy of the network on the 10000 test images: 71.5%
[04:10:58.943634] Max accuracy: 71.90%
[04:10:59.295533] log_dir: ./output_dir
[04:11:00.164493] Epoch: [31]  [  0/781]  eta: 0:11:17  lr: 0.000207  training_loss: 1.6417 (1.6417)  classification_loss: 1.5325 (1.5325)  loss_mask: 0.1091 (0.1091)  time: 0.8674  data: 0.6767  max mem: 6052
[04:11:03.565316] Epoch: [31]  [ 20/781]  eta: 0:02:34  lr: 0.000207  training_loss: 1.5895 (1.6165)  classification_loss: 1.5549 (1.5500)  loss_mask: 0.0361 (0.0665)  time: 0.1699  data: 0.0003  max mem: 6052
[04:11:06.994775] Epoch: [31]  [ 40/781]  eta: 0:02:19  lr: 0.000207  training_loss: 1.6864 (1.6440)  classification_loss: 1.5336 (1.5573)  loss_mask: 0.0717 (0.0868)  time: 0.1714  data: 0.0003  max mem: 6052
[04:11:10.432914] Epoch: [31]  [ 60/781]  eta: 0:02:11  lr: 0.000207  training_loss: 1.7362 (1.6878)  classification_loss: 1.6706 (1.5937)  loss_mask: 0.0655 (0.0941)  time: 0.1718  data: 0.0002  max mem: 6052
[04:11:13.847448] Epoch: [31]  [ 80/781]  eta: 0:02:05  lr: 0.000206  training_loss: 1.6779 (1.6870)  classification_loss: 1.6289 (1.6023)  loss_mask: 0.0421 (0.0848)  time: 0.1707  data: 0.0002  max mem: 6052
[04:11:17.285101] Epoch: [31]  [100/781]  eta: 0:02:01  lr: 0.000206  training_loss: 1.6651 (1.6845)  classification_loss: 1.6389 (1.6065)  loss_mask: 0.0343 (0.0780)  time: 0.1718  data: 0.0002  max mem: 6052
[04:11:20.715402] Epoch: [31]  [120/781]  eta: 0:01:56  lr: 0.000206  training_loss: 1.6178 (1.6761)  classification_loss: 1.5344 (1.6010)  loss_mask: 0.0443 (0.0751)  time: 0.1714  data: 0.0002  max mem: 6052
[04:11:24.141311] Epoch: [31]  [140/781]  eta: 0:01:52  lr: 0.000206  training_loss: 1.5945 (1.6656)  classification_loss: 1.5686 (1.5931)  loss_mask: 0.0340 (0.0724)  time: 0.1712  data: 0.0002  max mem: 6052
[04:11:27.703320] Epoch: [31]  [160/781]  eta: 0:01:49  lr: 0.000206  training_loss: 1.6452 (1.6623)  classification_loss: 1.5871 (1.5935)  loss_mask: 0.0347 (0.0688)  time: 0.1780  data: 0.0002  max mem: 6052
[04:11:31.264768] Epoch: [31]  [180/781]  eta: 0:01:46  lr: 0.000206  training_loss: 1.5508 (1.6540)  classification_loss: 1.4893 (1.5868)  loss_mask: 0.0394 (0.0672)  time: 0.1780  data: 0.0003  max mem: 6052
[04:11:34.826018] Epoch: [31]  [200/781]  eta: 0:01:42  lr: 0.000206  training_loss: 1.6593 (1.6523)  classification_loss: 1.6082 (1.5862)  loss_mask: 0.0360 (0.0661)  time: 0.1780  data: 0.0003  max mem: 6052
[04:11:38.246024] Epoch: [31]  [220/781]  eta: 0:01:38  lr: 0.000206  training_loss: 1.6569 (1.6537)  classification_loss: 1.5753 (1.5851)  loss_mask: 0.0793 (0.0685)  time: 0.1709  data: 0.0003  max mem: 6052
[04:11:41.636089] Epoch: [31]  [240/781]  eta: 0:01:35  lr: 0.000206  training_loss: 1.5617 (1.6491)  classification_loss: 1.5005 (1.5808)  loss_mask: 0.0457 (0.0682)  time: 0.1694  data: 0.0003  max mem: 6052
[04:11:45.042284] Epoch: [31]  [260/781]  eta: 0:01:31  lr: 0.000206  training_loss: 1.5665 (1.6435)  classification_loss: 1.5364 (1.5777)  loss_mask: 0.0302 (0.0658)  time: 0.1702  data: 0.0002  max mem: 6052
[04:11:48.503821] Epoch: [31]  [280/781]  eta: 0:01:27  lr: 0.000206  training_loss: 1.5404 (1.6374)  classification_loss: 1.5041 (1.5726)  loss_mask: 0.0382 (0.0648)  time: 0.1730  data: 0.0002  max mem: 6052
[04:11:51.955584] Epoch: [31]  [300/781]  eta: 0:01:24  lr: 0.000206  training_loss: 1.6212 (1.6391)  classification_loss: 1.5529 (1.5717)  loss_mask: 0.0571 (0.0673)  time: 0.1725  data: 0.0003  max mem: 6052
[04:11:55.359037] Epoch: [31]  [320/781]  eta: 0:01:20  lr: 0.000205  training_loss: 1.5639 (1.6337)  classification_loss: 1.5175 (1.5676)  loss_mask: 0.0432 (0.0661)  time: 0.1701  data: 0.0002  max mem: 6052
[04:11:58.760707] Epoch: [31]  [340/781]  eta: 0:01:16  lr: 0.000205  training_loss: 1.4972 (1.6268)  classification_loss: 1.4662 (1.5627)  loss_mask: 0.0276 (0.0641)  time: 0.1700  data: 0.0002  max mem: 6052
[04:12:02.161365] Epoch: [31]  [360/781]  eta: 0:01:13  lr: 0.000205  training_loss: 1.5976 (1.6244)  classification_loss: 1.5611 (1.5627)  loss_mask: 0.0183 (0.0617)  time: 0.1699  data: 0.0002  max mem: 6052
[04:12:05.618046] Epoch: [31]  [380/781]  eta: 0:01:09  lr: 0.000205  training_loss: 1.4918 (1.6193)  classification_loss: 1.4698 (1.5595)  loss_mask: 0.0211 (0.0598)  time: 0.1727  data: 0.0002  max mem: 6052
[04:12:09.071870] Epoch: [31]  [400/781]  eta: 0:01:06  lr: 0.000205  training_loss: 1.5073 (1.6151)  classification_loss: 1.4632 (1.5552)  loss_mask: 0.0376 (0.0599)  time: 0.1726  data: 0.0002  max mem: 6052
[04:12:12.504928] Epoch: [31]  [420/781]  eta: 0:01:02  lr: 0.000205  training_loss: 1.5937 (1.6146)  classification_loss: 1.5519 (1.5543)  loss_mask: 0.0561 (0.0604)  time: 0.1716  data: 0.0002  max mem: 6052
[04:12:15.923137] Epoch: [31]  [440/781]  eta: 0:00:59  lr: 0.000205  training_loss: 1.5270 (1.6109)  classification_loss: 1.4507 (1.5500)  loss_mask: 0.0525 (0.0609)  time: 0.1708  data: 0.0002  max mem: 6052
[04:12:19.348635] Epoch: [31]  [460/781]  eta: 0:00:55  lr: 0.000205  training_loss: 1.5057 (1.6075)  classification_loss: 1.4599 (1.5468)  loss_mask: 0.0586 (0.0607)  time: 0.1712  data: 0.0002  max mem: 6052
[04:12:22.759596] Epoch: [31]  [480/781]  eta: 0:00:52  lr: 0.000205  training_loss: 1.5294 (1.6059)  classification_loss: 1.4999 (1.5467)  loss_mask: 0.0221 (0.0592)  time: 0.1705  data: 0.0002  max mem: 6052
[04:12:26.214648] Epoch: [31]  [500/781]  eta: 0:00:48  lr: 0.000205  training_loss: 1.5197 (1.6026)  classification_loss: 1.5160 (1.5450)  loss_mask: 0.0176 (0.0576)  time: 0.1726  data: 0.0003  max mem: 6052
[04:12:29.637932] Epoch: [31]  [520/781]  eta: 0:00:45  lr: 0.000205  training_loss: 1.5447 (1.6013)  classification_loss: 1.5256 (1.5447)  loss_mask: 0.0232 (0.0566)  time: 0.1711  data: 0.0002  max mem: 6052
[04:12:33.079233] Epoch: [31]  [540/781]  eta: 0:00:41  lr: 0.000205  training_loss: 1.5352 (1.5989)  classification_loss: 1.5047 (1.5437)  loss_mask: 0.0147 (0.0552)  time: 0.1720  data: 0.0002  max mem: 6052
[04:12:36.482912] Epoch: [31]  [560/781]  eta: 0:00:38  lr: 0.000204  training_loss: 1.5035 (1.5951)  classification_loss: 1.4733 (1.5411)  loss_mask: 0.0124 (0.0540)  time: 0.1701  data: 0.0003  max mem: 6052
[04:12:39.957041] Epoch: [31]  [580/781]  eta: 0:00:34  lr: 0.000204  training_loss: 1.5608 (1.5927)  classification_loss: 1.5426 (1.5400)  loss_mask: 0.0116 (0.0526)  time: 0.1736  data: 0.0003  max mem: 6052
[04:12:43.415622] Epoch: [31]  [600/781]  eta: 0:00:31  lr: 0.000204  training_loss: 1.4717 (1.5902)  classification_loss: 1.4586 (1.5386)  loss_mask: 0.0120 (0.0516)  time: 0.1729  data: 0.0004  max mem: 6052
[04:12:46.872321] Epoch: [31]  [620/781]  eta: 0:00:27  lr: 0.000204  training_loss: 1.5305 (1.5887)  classification_loss: 1.4959 (1.5372)  loss_mask: 0.0184 (0.0515)  time: 0.1728  data: 0.0002  max mem: 6052
[04:12:50.350003] Epoch: [31]  [640/781]  eta: 0:00:24  lr: 0.000204  training_loss: 1.5065 (1.5868)  classification_loss: 1.4619 (1.5354)  loss_mask: 0.0504 (0.0514)  time: 0.1737  data: 0.0002  max mem: 6052
[04:12:53.776483] Epoch: [31]  [660/781]  eta: 0:00:20  lr: 0.000204  training_loss: 1.5567 (1.5859)  classification_loss: 1.5349 (1.5353)  loss_mask: 0.0203 (0.0507)  time: 0.1713  data: 0.0002  max mem: 6052
[04:12:57.215251] Epoch: [31]  [680/781]  eta: 0:00:17  lr: 0.000204  training_loss: 1.5560 (1.5854)  classification_loss: 1.4935 (1.5339)  loss_mask: 0.0674 (0.0515)  time: 0.1719  data: 0.0002  max mem: 6052
[04:13:00.636670] Epoch: [31]  [700/781]  eta: 0:00:14  lr: 0.000204  training_loss: 1.5395 (1.5846)  classification_loss: 1.4881 (1.5334)  loss_mask: 0.0345 (0.0512)  time: 0.1710  data: 0.0002  max mem: 6052
[04:13:04.067896] Epoch: [31]  [720/781]  eta: 0:00:10  lr: 0.000204  training_loss: 1.5341 (1.5830)  classification_loss: 1.5053 (1.5322)  loss_mask: 0.0238 (0.0508)  time: 0.1714  data: 0.0003  max mem: 6052
[04:13:07.504483] Epoch: [31]  [740/781]  eta: 0:00:07  lr: 0.000204  training_loss: 1.5260 (1.5803)  classification_loss: 1.5084 (1.5302)  loss_mask: 0.0156 (0.0501)  time: 0.1718  data: 0.0002  max mem: 6052
[04:13:10.931133] Epoch: [31]  [760/781]  eta: 0:00:03  lr: 0.000204  training_loss: 1.5532 (1.5796)  classification_loss: 1.5339 (1.5305)  loss_mask: 0.0122 (0.0491)  time: 0.1713  data: 0.0002  max mem: 6052
[04:13:14.407746] Epoch: [31]  [780/781]  eta: 0:00:00  lr: 0.000204  training_loss: 1.5504 (1.5788)  classification_loss: 1.5444 (1.5308)  loss_mask: 0.0081 (0.0481)  time: 0.1737  data: 0.0002  max mem: 6052
[04:13:14.569708] Epoch: [31] Total time: 0:02:15 (0.1732 s / it)
[04:13:14.570139] Averaged stats: lr: 0.000204  training_loss: 1.5504 (1.5788)  classification_loss: 1.5444 (1.5308)  loss_mask: 0.0081 (0.0481)
[04:13:15.261140] Test:  [  0/157]  eta: 0:01:47  testing_loss: 0.8079 (0.8079)  acc1: 75.0000 (75.0000)  acc5: 95.3125 (95.3125)  time: 0.6860  data: 0.6551  max mem: 6052
[04:13:15.546404] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.8349 (0.8725)  acc1: 71.8750 (70.5966)  acc5: 98.4375 (98.5795)  time: 0.0881  data: 0.0597  max mem: 6052
[04:13:15.831149] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.8156 (0.8333)  acc1: 71.8750 (72.6190)  acc5: 98.4375 (98.6607)  time: 0.0283  data: 0.0001  max mem: 6052
[04:13:16.117190] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.8263 (0.8423)  acc1: 73.4375 (72.2278)  acc5: 98.4375 (98.3367)  time: 0.0284  data: 0.0001  max mem: 6052
[04:13:16.405386] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.8258 (0.8359)  acc1: 73.4375 (72.6372)  acc5: 98.4375 (98.2088)  time: 0.0286  data: 0.0002  max mem: 6052
[04:13:16.701162] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.8144 (0.8296)  acc1: 73.4375 (72.9779)  acc5: 98.4375 (98.1618)  time: 0.0290  data: 0.0002  max mem: 6052
[04:13:16.985097] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.8126 (0.8272)  acc1: 73.4375 (72.8484)  acc5: 98.4375 (98.0277)  time: 0.0288  data: 0.0002  max mem: 6052
[04:13:17.269142] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.8037 (0.8207)  acc1: 73.4375 (73.0414)  acc5: 98.4375 (98.0194)  time: 0.0283  data: 0.0003  max mem: 6052
[04:13:17.549665] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.8102 (0.8298)  acc1: 73.4375 (72.7238)  acc5: 98.4375 (97.9552)  time: 0.0281  data: 0.0003  max mem: 6052
[04:13:17.830289] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.8066 (0.8259)  acc1: 73.4375 (72.9567)  acc5: 98.4375 (97.9739)  time: 0.0279  data: 0.0001  max mem: 6052
[04:13:18.112406] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.8195 (0.8303)  acc1: 71.8750 (72.8032)  acc5: 98.4375 (97.9889)  time: 0.0280  data: 0.0001  max mem: 6052
[04:13:18.397001] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.8380 (0.8309)  acc1: 71.8750 (72.8744)  acc5: 96.8750 (98.0011)  time: 0.0282  data: 0.0002  max mem: 6052
[04:13:18.680948] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.7997 (0.8266)  acc1: 75.0000 (73.1276)  acc5: 98.4375 (98.0114)  time: 0.0283  data: 0.0002  max mem: 6052
[04:13:18.968141] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.8192 (0.8285)  acc1: 73.4375 (73.0916)  acc5: 98.4375 (97.9604)  time: 0.0284  data: 0.0002  max mem: 6052
[04:13:19.252491] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.8662 (0.8281)  acc1: 70.3125 (73.0496)  acc5: 98.4375 (97.9388)  time: 0.0285  data: 0.0001  max mem: 6052
[04:13:19.531393] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.8330 (0.8269)  acc1: 71.8750 (73.0857)  acc5: 96.8750 (97.8891)  time: 0.0280  data: 0.0001  max mem: 6052
[04:13:19.683728] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.8178 (0.8282)  acc1: 73.4375 (73.0300)  acc5: 98.4375 (97.9100)  time: 0.0270  data: 0.0001  max mem: 6052
[04:13:19.827594] Test: Total time: 0:00:05 (0.0335 s / it)
[04:13:19.828142] * Acc@1 73.030 Acc@5 97.910 loss 0.828
[04:13:19.828442] Accuracy of the network on the 10000 test images: 73.0%
[04:13:19.828652] Max accuracy: 73.03%
[04:13:20.080676] log_dir: ./output_dir
[04:13:20.979377] Epoch: [32]  [  0/781]  eta: 0:11:40  lr: 0.000204  training_loss: 1.3703 (1.3703)  classification_loss: 1.3538 (1.3538)  loss_mask: 0.0164 (0.0164)  time: 0.8972  data: 0.7182  max mem: 6052
[04:13:24.410996] Epoch: [32]  [ 20/781]  eta: 0:02:36  lr: 0.000204  training_loss: 1.5435 (1.5133)  classification_loss: 1.5396 (1.5054)  loss_mask: 0.0062 (0.0079)  time: 0.1715  data: 0.0001  max mem: 6052
[04:13:27.825644] Epoch: [32]  [ 40/781]  eta: 0:02:19  lr: 0.000203  training_loss: 1.5061 (1.5028)  classification_loss: 1.5004 (1.4927)  loss_mask: 0.0050 (0.0102)  time: 0.1707  data: 0.0002  max mem: 6052
[04:13:31.281346] Epoch: [32]  [ 60/781]  eta: 0:02:12  lr: 0.000203  training_loss: 1.5453 (1.5214)  classification_loss: 1.5127 (1.5030)  loss_mask: 0.0282 (0.0184)  time: 0.1727  data: 0.0002  max mem: 6052
[04:13:34.699920] Epoch: [32]  [ 80/781]  eta: 0:02:06  lr: 0.000203  training_loss: 1.5514 (1.5245)  classification_loss: 1.5195 (1.5061)  loss_mask: 0.0153 (0.0183)  time: 0.1708  data: 0.0002  max mem: 6052
[04:13:38.137132] Epoch: [32]  [100/781]  eta: 0:02:01  lr: 0.000203  training_loss: 1.5292 (1.5268)  classification_loss: 1.5100 (1.5097)  loss_mask: 0.0088 (0.0171)  time: 0.1718  data: 0.0002  max mem: 6052
[04:13:41.545784] Epoch: [32]  [120/781]  eta: 0:01:57  lr: 0.000203  training_loss: 1.5596 (1.5290)  classification_loss: 1.4982 (1.5105)  loss_mask: 0.0086 (0.0185)  time: 0.1704  data: 0.0002  max mem: 6052
[04:13:44.972874] Epoch: [32]  [140/781]  eta: 0:01:53  lr: 0.000203  training_loss: 1.6575 (1.5537)  classification_loss: 1.5268 (1.5158)  loss_mask: 0.1721 (0.0379)  time: 0.1713  data: 0.0002  max mem: 6052
[04:13:48.384657] Epoch: [32]  [160/781]  eta: 0:01:49  lr: 0.000203  training_loss: 1.5779 (1.5578)  classification_loss: 1.5266 (1.5166)  loss_mask: 0.0487 (0.0412)  time: 0.1705  data: 0.0001  max mem: 6052
[04:13:51.796574] Epoch: [32]  [180/781]  eta: 0:01:45  lr: 0.000203  training_loss: 1.5687 (1.5613)  classification_loss: 1.5029 (1.5182)  loss_mask: 0.0404 (0.0430)  time: 0.1705  data: 0.0002  max mem: 6052
[04:13:55.246058] Epoch: [32]  [200/781]  eta: 0:01:41  lr: 0.000203  training_loss: 1.5896 (1.5654)  classification_loss: 1.5252 (1.5183)  loss_mask: 0.0587 (0.0471)  time: 0.1724  data: 0.0005  max mem: 6052
[04:13:58.676341] Epoch: [32]  [220/781]  eta: 0:01:37  lr: 0.000203  training_loss: 1.5333 (1.5637)  classification_loss: 1.5120 (1.5180)  loss_mask: 0.0242 (0.0457)  time: 0.1714  data: 0.0002  max mem: 6052
[04:14:02.117782] Epoch: [32]  [240/781]  eta: 0:01:34  lr: 0.000203  training_loss: 1.5517 (1.5620)  classification_loss: 1.5248 (1.5181)  loss_mask: 0.0214 (0.0439)  time: 0.1720  data: 0.0003  max mem: 6052
[04:14:05.571833] Epoch: [32]  [260/781]  eta: 0:01:30  lr: 0.000203  training_loss: 1.5320 (1.5589)  classification_loss: 1.5189 (1.5168)  loss_mask: 0.0179 (0.0422)  time: 0.1726  data: 0.0003  max mem: 6052
[04:14:08.996793] Epoch: [32]  [280/781]  eta: 0:01:27  lr: 0.000202  training_loss: 1.5594 (1.5581)  classification_loss: 1.5467 (1.5183)  loss_mask: 0.0097 (0.0398)  time: 0.1712  data: 0.0003  max mem: 6052
[04:14:12.417790] Epoch: [32]  [300/781]  eta: 0:01:23  lr: 0.000202  training_loss: 1.5013 (1.5551)  classification_loss: 1.4929 (1.5174)  loss_mask: 0.0060 (0.0376)  time: 0.1710  data: 0.0002  max mem: 6052
[04:14:15.828609] Epoch: [32]  [320/781]  eta: 0:01:20  lr: 0.000202  training_loss: 1.4705 (1.5505)  classification_loss: 1.4633 (1.5149)  loss_mask: 0.0056 (0.0357)  time: 0.1705  data: 0.0002  max mem: 6052
[04:14:19.233841] Epoch: [32]  [340/781]  eta: 0:01:16  lr: 0.000202  training_loss: 1.5231 (1.5497)  classification_loss: 1.5195 (1.5158)  loss_mask: 0.0046 (0.0339)  time: 0.1701  data: 0.0002  max mem: 6052
[04:14:22.629773] Epoch: [32]  [360/781]  eta: 0:01:12  lr: 0.000202  training_loss: 1.4605 (1.5457)  classification_loss: 1.4553 (1.5131)  loss_mask: 0.0057 (0.0326)  time: 0.1697  data: 0.0002  max mem: 6052
[04:14:26.037761] Epoch: [32]  [380/781]  eta: 0:01:09  lr: 0.000202  training_loss: 1.5282 (1.5448)  classification_loss: 1.5056 (1.5128)  loss_mask: 0.0124 (0.0320)  time: 0.1703  data: 0.0002  max mem: 6052
[04:14:29.444291] Epoch: [32]  [400/781]  eta: 0:01:05  lr: 0.000202  training_loss: 1.5385 (1.5460)  classification_loss: 1.4935 (1.5123)  loss_mask: 0.0533 (0.0336)  time: 0.1703  data: 0.0002  max mem: 6052
[04:14:32.870405] Epoch: [32]  [420/781]  eta: 0:01:02  lr: 0.000202  training_loss: 1.5241 (1.5460)  classification_loss: 1.4973 (1.5126)  loss_mask: 0.0193 (0.0333)  time: 0.1712  data: 0.0002  max mem: 6052
[04:14:36.273496] Epoch: [32]  [440/781]  eta: 0:00:58  lr: 0.000202  training_loss: 1.5103 (1.5440)  classification_loss: 1.4921 (1.5115)  loss_mask: 0.0103 (0.0325)  time: 0.1701  data: 0.0001  max mem: 6052
[04:14:39.706503] Epoch: [32]  [460/781]  eta: 0:00:55  lr: 0.000202  training_loss: 1.4450 (1.5406)  classification_loss: 1.4435 (1.5093)  loss_mask: 0.0049 (0.0313)  time: 0.1716  data: 0.0002  max mem: 6052
[04:14:43.145051] Epoch: [32]  [480/781]  eta: 0:00:51  lr: 0.000202  training_loss: 1.5347 (1.5402)  classification_loss: 1.5321 (1.5100)  loss_mask: 0.0038 (0.0302)  time: 0.1719  data: 0.0002  max mem: 6052
[04:14:46.598834] Epoch: [32]  [500/781]  eta: 0:00:48  lr: 0.000202  training_loss: 1.5031 (1.5392)  classification_loss: 1.5021 (1.5100)  loss_mask: 0.0042 (0.0292)  time: 0.1726  data: 0.0002  max mem: 6052
[04:14:50.009763] Epoch: [32]  [520/781]  eta: 0:00:45  lr: 0.000201  training_loss: 1.4955 (1.5380)  classification_loss: 1.4919 (1.5098)  loss_mask: 0.0022 (0.0282)  time: 0.1705  data: 0.0002  max mem: 6052
[04:14:53.450300] Epoch: [32]  [540/781]  eta: 0:00:41  lr: 0.000201  training_loss: 1.5016 (1.5364)  classification_loss: 1.4983 (1.5092)  loss_mask: 0.0025 (0.0272)  time: 0.1719  data: 0.0002  max mem: 6052
[04:14:56.906250] Epoch: [32]  [560/781]  eta: 0:00:38  lr: 0.000201  training_loss: 1.4905 (1.5352)  classification_loss: 1.4869 (1.5085)  loss_mask: 0.0059 (0.0267)  time: 0.1727  data: 0.0002  max mem: 6052
[04:15:00.346285] Epoch: [32]  [580/781]  eta: 0:00:34  lr: 0.000201  training_loss: 1.5262 (1.5351)  classification_loss: 1.5183 (1.5083)  loss_mask: 0.0228 (0.0268)  time: 0.1719  data: 0.0004  max mem: 6052
[04:15:03.770511] Epoch: [32]  [600/781]  eta: 0:00:31  lr: 0.000201  training_loss: 1.4789 (1.5343)  classification_loss: 1.4718 (1.5080)  loss_mask: 0.0093 (0.0263)  time: 0.1711  data: 0.0002  max mem: 6052
[04:15:07.199338] Epoch: [32]  [620/781]  eta: 0:00:27  lr: 0.000201  training_loss: 1.5412 (1.5346)  classification_loss: 1.4556 (1.5068)  loss_mask: 0.0374 (0.0278)  time: 0.1714  data: 0.0003  max mem: 6052
[04:15:10.623540] Epoch: [32]  [640/781]  eta: 0:00:24  lr: 0.000201  training_loss: 1.4918 (1.5332)  classification_loss: 1.4700 (1.5057)  loss_mask: 0.0149 (0.0275)  time: 0.1711  data: 0.0003  max mem: 6052
[04:15:14.039933] Epoch: [32]  [660/781]  eta: 0:00:20  lr: 0.000201  training_loss: 1.5772 (1.5336)  classification_loss: 1.5602 (1.5064)  loss_mask: 0.0135 (0.0272)  time: 0.1707  data: 0.0002  max mem: 6052
[04:15:17.463818] Epoch: [32]  [680/781]  eta: 0:00:17  lr: 0.000201  training_loss: 1.4708 (1.5332)  classification_loss: 1.4375 (1.5055)  loss_mask: 0.0261 (0.0278)  time: 0.1711  data: 0.0002  max mem: 6052
[04:15:20.871395] Epoch: [32]  [700/781]  eta: 0:00:13  lr: 0.000201  training_loss: 1.5958 (1.5353)  classification_loss: 1.4978 (1.5056)  loss_mask: 0.0627 (0.0298)  time: 0.1703  data: 0.0002  max mem: 6052
[04:15:24.309904] Epoch: [32]  [720/781]  eta: 0:00:10  lr: 0.000201  training_loss: 1.5570 (1.5351)  classification_loss: 1.5096 (1.5053)  loss_mask: 0.0274 (0.0297)  time: 0.1718  data: 0.0002  max mem: 6052
[04:15:27.736859] Epoch: [32]  [740/781]  eta: 0:00:07  lr: 0.000201  training_loss: 1.4670 (1.5326)  classification_loss: 1.4417 (1.5032)  loss_mask: 0.0128 (0.0294)  time: 0.1713  data: 0.0002  max mem: 6052
[04:15:31.189497] Epoch: [32]  [760/781]  eta: 0:00:03  lr: 0.000200  training_loss: 1.5514 (1.5329)  classification_loss: 1.5198 (1.5039)  loss_mask: 0.0114 (0.0290)  time: 0.1726  data: 0.0002  max mem: 6052
[04:15:34.578970] Epoch: [32]  [780/781]  eta: 0:00:00  lr: 0.000200  training_loss: 1.5415 (1.5328)  classification_loss: 1.5401 (1.5043)  loss_mask: 0.0059 (0.0285)  time: 0.1694  data: 0.0001  max mem: 6052
[04:15:34.764926] Epoch: [32] Total time: 0:02:14 (0.1725 s / it)
[04:15:34.765467] Averaged stats: lr: 0.000200  training_loss: 1.5415 (1.5328)  classification_loss: 1.5401 (1.5043)  loss_mask: 0.0059 (0.0285)
[04:15:35.458328] Test:  [  0/157]  eta: 0:01:47  testing_loss: 0.8357 (0.8357)  acc1: 75.0000 (75.0000)  acc5: 98.4375 (98.4375)  time: 0.6848  data: 0.6438  max mem: 6052
[04:15:35.743761] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.8339 (0.8512)  acc1: 75.0000 (71.5909)  acc5: 98.4375 (98.5795)  time: 0.0881  data: 0.0587  max mem: 6052
[04:15:36.030487] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.7931 (0.8070)  acc1: 75.0000 (73.2143)  acc5: 98.4375 (98.7351)  time: 0.0285  data: 0.0002  max mem: 6052
[04:15:36.315279] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.8013 (0.8214)  acc1: 71.8750 (72.2782)  acc5: 98.4375 (98.4375)  time: 0.0284  data: 0.0002  max mem: 6052
[04:15:36.597072] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.8013 (0.8193)  acc1: 71.8750 (72.2561)  acc5: 96.8750 (98.2470)  time: 0.0282  data: 0.0002  max mem: 6052
[04:15:36.878824] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.7701 (0.8109)  acc1: 71.8750 (72.8248)  acc5: 98.4375 (98.1311)  time: 0.0281  data: 0.0002  max mem: 6052
[04:15:37.160460] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.8171 (0.8109)  acc1: 70.3125 (72.5410)  acc5: 98.4375 (98.1045)  time: 0.0281  data: 0.0002  max mem: 6052
[04:15:37.441900] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7591 (0.7997)  acc1: 71.8750 (73.0634)  acc5: 98.4375 (98.1074)  time: 0.0280  data: 0.0002  max mem: 6052
[04:15:37.722833] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7561 (0.8053)  acc1: 73.4375 (72.8202)  acc5: 98.4375 (98.1867)  time: 0.0280  data: 0.0002  max mem: 6052
[04:15:38.004183] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.8138 (0.8044)  acc1: 73.4375 (72.9052)  acc5: 98.4375 (98.2830)  time: 0.0280  data: 0.0001  max mem: 6052
[04:15:38.287338] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.7953 (0.8068)  acc1: 73.4375 (72.9115)  acc5: 98.4375 (98.2519)  time: 0.0281  data: 0.0002  max mem: 6052
[04:15:38.575500] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.8425 (0.8096)  acc1: 71.8750 (72.7196)  acc5: 98.4375 (98.2686)  time: 0.0284  data: 0.0002  max mem: 6052
[04:15:38.860681] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.7855 (0.8057)  acc1: 71.8750 (72.8951)  acc5: 98.4375 (98.3084)  time: 0.0285  data: 0.0002  max mem: 6052
[04:15:39.140810] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.7708 (0.8045)  acc1: 73.4375 (72.9365)  acc5: 98.4375 (98.3302)  time: 0.0281  data: 0.0001  max mem: 6052
[04:15:39.420973] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7763 (0.8027)  acc1: 73.4375 (73.0829)  acc5: 98.4375 (98.3378)  time: 0.0279  data: 0.0001  max mem: 6052
[04:15:39.699214] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7976 (0.8012)  acc1: 76.5625 (73.1581)  acc5: 98.4375 (98.2926)  time: 0.0278  data: 0.0001  max mem: 6052
[04:15:39.848374] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7983 (0.8023)  acc1: 73.4375 (73.1100)  acc5: 98.4375 (98.2800)  time: 0.0268  data: 0.0001  max mem: 6052
[04:15:40.016657] Test: Total time: 0:00:05 (0.0334 s / it)
[04:15:40.017198] * Acc@1 73.110 Acc@5 98.280 loss 0.802
[04:15:40.017506] Accuracy of the network on the 10000 test images: 73.1%
[04:15:40.017700] Max accuracy: 73.11%
[04:15:40.392796] log_dir: ./output_dir
[04:15:41.250685] Epoch: [33]  [  0/781]  eta: 0:11:08  lr: 0.000200  training_loss: 1.4618 (1.4618)  classification_loss: 1.4598 (1.4598)  loss_mask: 0.0020 (0.0020)  time: 0.8560  data: 0.6502  max mem: 6052
[04:15:44.662598] Epoch: [33]  [ 20/781]  eta: 0:02:34  lr: 0.000200  training_loss: 1.4468 (1.4695)  classification_loss: 1.4406 (1.4656)  loss_mask: 0.0033 (0.0039)  time: 0.1705  data: 0.0002  max mem: 6052
[04:15:48.064077] Epoch: [33]  [ 40/781]  eta: 0:02:18  lr: 0.000200  training_loss: 1.4680 (1.4715)  classification_loss: 1.4656 (1.4673)  loss_mask: 0.0029 (0.0042)  time: 0.1700  data: 0.0002  max mem: 6052
[04:15:51.487262] Epoch: [33]  [ 60/781]  eta: 0:02:11  lr: 0.000200  training_loss: 1.4555 (1.4795)  classification_loss: 1.4536 (1.4758)  loss_mask: 0.0020 (0.0037)  time: 0.1711  data: 0.0003  max mem: 6052
[04:15:54.894935] Epoch: [33]  [ 80/781]  eta: 0:02:05  lr: 0.000200  training_loss: 1.4937 (1.4884)  classification_loss: 1.4927 (1.4852)  loss_mask: 0.0013 (0.0032)  time: 0.1703  data: 0.0002  max mem: 6052
[04:15:58.315967] Epoch: [33]  [100/781]  eta: 0:02:00  lr: 0.000200  training_loss: 1.4922 (1.4905)  classification_loss: 1.4906 (1.4875)  loss_mask: 0.0017 (0.0029)  time: 0.1710  data: 0.0002  max mem: 6052
[04:16:01.744361] Epoch: [33]  [120/781]  eta: 0:01:56  lr: 0.000200  training_loss: 1.4414 (1.4861)  classification_loss: 1.4397 (1.4834)  loss_mask: 0.0017 (0.0027)  time: 0.1713  data: 0.0003  max mem: 6052
[04:16:05.173589] Epoch: [33]  [140/781]  eta: 0:01:52  lr: 0.000200  training_loss: 1.4923 (1.4873)  classification_loss: 1.4916 (1.4847)  loss_mask: 0.0012 (0.0026)  time: 0.1714  data: 0.0002  max mem: 6052
[04:16:08.653248] Epoch: [33]  [160/781]  eta: 0:01:48  lr: 0.000200  training_loss: 1.4506 (1.4864)  classification_loss: 1.4492 (1.4840)  loss_mask: 0.0015 (0.0024)  time: 0.1739  data: 0.0002  max mem: 6052
[04:16:12.085718] Epoch: [33]  [180/781]  eta: 0:01:45  lr: 0.000200  training_loss: 1.4270 (1.4825)  classification_loss: 1.4260 (1.4802)  loss_mask: 0.0010 (0.0023)  time: 0.1715  data: 0.0002  max mem: 6052
[04:16:15.516609] Epoch: [33]  [200/781]  eta: 0:01:41  lr: 0.000199  training_loss: 1.4613 (1.4831)  classification_loss: 1.4605 (1.4809)  loss_mask: 0.0009 (0.0022)  time: 0.1715  data: 0.0002  max mem: 6052
[04:16:18.989870] Epoch: [33]  [220/781]  eta: 0:01:37  lr: 0.000199  training_loss: 1.4755 (1.4817)  classification_loss: 1.4743 (1.4795)  loss_mask: 0.0012 (0.0022)  time: 0.1736  data: 0.0002  max mem: 6052
[04:16:22.389904] Epoch: [33]  [240/781]  eta: 0:01:34  lr: 0.000199  training_loss: 1.4398 (1.4815)  classification_loss: 1.4382 (1.4794)  loss_mask: 0.0015 (0.0021)  time: 0.1699  data: 0.0001  max mem: 6052
[04:16:25.792617] Epoch: [33]  [260/781]  eta: 0:01:30  lr: 0.000199  training_loss: 1.4571 (1.4813)  classification_loss: 1.4554 (1.4793)  loss_mask: 0.0012 (0.0021)  time: 0.1701  data: 0.0001  max mem: 6052
[04:16:29.319021] Epoch: [33]  [280/781]  eta: 0:01:27  lr: 0.000199  training_loss: 1.5079 (1.4834)  classification_loss: 1.5068 (1.4814)  loss_mask: 0.0008 (0.0020)  time: 0.1763  data: 0.0003  max mem: 6052
[04:16:32.883048] Epoch: [33]  [300/781]  eta: 0:01:23  lr: 0.000199  training_loss: 1.5132 (1.4852)  classification_loss: 1.5124 (1.4833)  loss_mask: 0.0008 (0.0019)  time: 0.1781  data: 0.0003  max mem: 6052
[04:16:36.426020] Epoch: [33]  [320/781]  eta: 0:01:20  lr: 0.000199  training_loss: 1.5087 (1.4871)  classification_loss: 1.5081 (1.4852)  loss_mask: 0.0006 (0.0018)  time: 0.1771  data: 0.0003  max mem: 6052
[04:16:39.861917] Epoch: [33]  [340/781]  eta: 0:01:16  lr: 0.000199  training_loss: 1.4585 (1.4880)  classification_loss: 1.4184 (1.4848)  loss_mask: 0.0040 (0.0032)  time: 0.1717  data: 0.0002  max mem: 6052
[04:16:43.263028] Epoch: [33]  [360/781]  eta: 0:01:13  lr: 0.000199  training_loss: 1.6094 (1.4938)  classification_loss: 1.5317 (1.4868)  loss_mask: 0.0330 (0.0070)  time: 0.1700  data: 0.0002  max mem: 6052
[04:16:46.667504] Epoch: [33]  [380/781]  eta: 0:01:09  lr: 0.000199  training_loss: 1.5150 (1.4960)  classification_loss: 1.4642 (1.4864)  loss_mask: 0.0464 (0.0096)  time: 0.1701  data: 0.0002  max mem: 6052
[04:16:50.079347] Epoch: [33]  [400/781]  eta: 0:01:06  lr: 0.000199  training_loss: 1.5357 (1.4975)  classification_loss: 1.4818 (1.4855)  loss_mask: 0.0282 (0.0120)  time: 0.1705  data: 0.0003  max mem: 6052
[04:16:53.489158] Epoch: [33]  [420/781]  eta: 0:01:02  lr: 0.000199  training_loss: 1.5462 (1.5004)  classification_loss: 1.4921 (1.4854)  loss_mask: 0.0517 (0.0151)  time: 0.1704  data: 0.0003  max mem: 6052
[04:16:56.937626] Epoch: [33]  [440/781]  eta: 0:00:59  lr: 0.000198  training_loss: 1.5233 (1.5004)  classification_loss: 1.4856 (1.4850)  loss_mask: 0.0196 (0.0154)  time: 0.1723  data: 0.0002  max mem: 6052
[04:17:00.391323] Epoch: [33]  [460/781]  eta: 0:00:55  lr: 0.000198  training_loss: 1.4598 (1.4993)  classification_loss: 1.4454 (1.4842)  loss_mask: 0.0065 (0.0151)  time: 0.1726  data: 0.0003  max mem: 6052
[04:17:03.825126] Epoch: [33]  [480/781]  eta: 0:00:52  lr: 0.000198  training_loss: 1.5430 (1.5009)  classification_loss: 1.5349 (1.4862)  loss_mask: 0.0044 (0.0147)  time: 0.1716  data: 0.0002  max mem: 6052
[04:17:07.236958] Epoch: [33]  [500/781]  eta: 0:00:48  lr: 0.000198  training_loss: 1.5049 (1.5009)  classification_loss: 1.5033 (1.4866)  loss_mask: 0.0027 (0.0143)  time: 0.1705  data: 0.0002  max mem: 6052
[04:17:10.674457] Epoch: [33]  [520/781]  eta: 0:00:45  lr: 0.000198  training_loss: 1.5014 (1.5003)  classification_loss: 1.4996 (1.4864)  loss_mask: 0.0022 (0.0138)  time: 0.1718  data: 0.0002  max mem: 6052
[04:17:14.091995] Epoch: [33]  [540/781]  eta: 0:00:41  lr: 0.000198  training_loss: 1.4990 (1.5012)  classification_loss: 1.4977 (1.4878)  loss_mask: 0.0020 (0.0134)  time: 0.1708  data: 0.0003  max mem: 6052
[04:17:17.509058] Epoch: [33]  [560/781]  eta: 0:00:38  lr: 0.000198  training_loss: 1.4874 (1.5013)  classification_loss: 1.4702 (1.4877)  loss_mask: 0.0050 (0.0136)  time: 0.1708  data: 0.0003  max mem: 6052
[04:17:20.930004] Epoch: [33]  [580/781]  eta: 0:00:34  lr: 0.000198  training_loss: 1.5340 (1.5026)  classification_loss: 1.4708 (1.4876)  loss_mask: 0.0310 (0.0150)  time: 0.1710  data: 0.0002  max mem: 6052
[04:17:24.365869] Epoch: [33]  [600/781]  eta: 0:00:31  lr: 0.000198  training_loss: 1.6093 (1.5066)  classification_loss: 1.4755 (1.4882)  loss_mask: 0.0491 (0.0184)  time: 0.1717  data: 0.0002  max mem: 6052
[04:17:27.766256] Epoch: [33]  [620/781]  eta: 0:00:27  lr: 0.000198  training_loss: 1.5129 (1.5082)  classification_loss: 1.4348 (1.4878)  loss_mask: 0.0613 (0.0203)  time: 0.1699  data: 0.0001  max mem: 6052
[04:17:31.192969] Epoch: [33]  [640/781]  eta: 0:00:24  lr: 0.000198  training_loss: 1.4293 (1.5061)  classification_loss: 1.4185 (1.4858)  loss_mask: 0.0164 (0.0203)  time: 0.1713  data: 0.0003  max mem: 6052
[04:17:34.641112] Epoch: [33]  [660/781]  eta: 0:00:20  lr: 0.000198  training_loss: 1.5274 (1.5066)  classification_loss: 1.4747 (1.4862)  loss_mask: 0.0128 (0.0205)  time: 0.1723  data: 0.0002  max mem: 6052
[04:17:38.073590] Epoch: [33]  [680/781]  eta: 0:00:17  lr: 0.000197  training_loss: 1.5217 (1.5076)  classification_loss: 1.4833 (1.4866)  loss_mask: 0.0317 (0.0210)  time: 0.1715  data: 0.0002  max mem: 6052
[04:17:41.487688] Epoch: [33]  [700/781]  eta: 0:00:13  lr: 0.000197  training_loss: 1.4920 (1.5083)  classification_loss: 1.4750 (1.4873)  loss_mask: 0.0164 (0.0210)  time: 0.1706  data: 0.0002  max mem: 6052
[04:17:44.900019] Epoch: [33]  [720/781]  eta: 0:00:10  lr: 0.000197  training_loss: 1.5044 (1.5087)  classification_loss: 1.4957 (1.4879)  loss_mask: 0.0099 (0.0208)  time: 0.1705  data: 0.0001  max mem: 6052
[04:17:48.342311] Epoch: [33]  [740/781]  eta: 0:00:07  lr: 0.000197  training_loss: 1.4753 (1.5077)  classification_loss: 1.4324 (1.4863)  loss_mask: 0.0218 (0.0214)  time: 0.1720  data: 0.0002  max mem: 6052
[04:17:51.750161] Epoch: [33]  [760/781]  eta: 0:00:03  lr: 0.000197  training_loss: 1.5922 (1.5102)  classification_loss: 1.4843 (1.4869)  loss_mask: 0.0616 (0.0233)  time: 0.1703  data: 0.0002  max mem: 6052
[04:17:55.147166] Epoch: [33]  [780/781]  eta: 0:00:00  lr: 0.000197  training_loss: 1.5470 (1.5115)  classification_loss: 1.4760 (1.4871)  loss_mask: 0.0527 (0.0244)  time: 0.1698  data: 0.0001  max mem: 6052
[04:17:55.312734] Epoch: [33] Total time: 0:02:14 (0.1728 s / it)
[04:17:55.313405] Averaged stats: lr: 0.000197  training_loss: 1.5470 (1.5115)  classification_loss: 1.4760 (1.4871)  loss_mask: 0.0527 (0.0244)
[04:17:55.997452] Test:  [  0/157]  eta: 0:01:45  testing_loss: 0.8142 (0.8142)  acc1: 75.0000 (75.0000)  acc5: 98.4375 (98.4375)  time: 0.6740  data: 0.6438  max mem: 6052
[04:17:56.283491] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.7875 (0.8125)  acc1: 73.4375 (72.4432)  acc5: 98.4375 (98.5795)  time: 0.0870  data: 0.0588  max mem: 6052
[04:17:56.563495] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.7590 (0.7752)  acc1: 75.0000 (74.4792)  acc5: 98.4375 (98.5863)  time: 0.0281  data: 0.0002  max mem: 6052
[04:17:56.850412] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.7677 (0.7903)  acc1: 73.4375 (73.8407)  acc5: 98.4375 (98.2359)  time: 0.0282  data: 0.0003  max mem: 6052
[04:17:57.138488] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.7783 (0.7843)  acc1: 73.4375 (74.1235)  acc5: 98.4375 (98.3613)  time: 0.0286  data: 0.0003  max mem: 6052
[04:17:57.423756] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.7395 (0.7798)  acc1: 75.0000 (74.5404)  acc5: 98.4375 (98.2843)  time: 0.0285  data: 0.0002  max mem: 6052
[04:17:57.704418] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.7836 (0.7813)  acc1: 73.4375 (74.1291)  acc5: 98.4375 (98.2582)  time: 0.0282  data: 0.0001  max mem: 6052
[04:17:57.985752] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7596 (0.7738)  acc1: 73.4375 (74.3618)  acc5: 98.4375 (98.2394)  time: 0.0280  data: 0.0001  max mem: 6052
[04:17:58.268103] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7503 (0.7774)  acc1: 75.0000 (74.4213)  acc5: 98.4375 (98.2639)  time: 0.0281  data: 0.0002  max mem: 6052
[04:17:58.549672] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7403 (0.7752)  acc1: 75.0000 (74.5879)  acc5: 98.4375 (98.3345)  time: 0.0281  data: 0.0002  max mem: 6052
[04:17:58.834753] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.7403 (0.7786)  acc1: 75.0000 (74.6132)  acc5: 98.4375 (98.3447)  time: 0.0282  data: 0.0002  max mem: 6052
[04:17:59.115653] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.7825 (0.7802)  acc1: 75.0000 (74.6199)  acc5: 98.4375 (98.3390)  time: 0.0282  data: 0.0002  max mem: 6052
[04:17:59.395896] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.7681 (0.7774)  acc1: 73.4375 (74.7159)  acc5: 98.4375 (98.2955)  time: 0.0279  data: 0.0002  max mem: 6052
[04:17:59.676435] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.7613 (0.7768)  acc1: 73.4375 (74.7853)  acc5: 98.4375 (98.3182)  time: 0.0279  data: 0.0002  max mem: 6052
[04:17:59.956767] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7746 (0.7755)  acc1: 75.0000 (74.8449)  acc5: 98.4375 (98.3267)  time: 0.0279  data: 0.0002  max mem: 6052
[04:18:00.234720] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7746 (0.7745)  acc1: 76.5625 (74.9069)  acc5: 98.4375 (98.3030)  time: 0.0278  data: 0.0001  max mem: 6052
[04:18:00.383718] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7714 (0.7766)  acc1: 75.0000 (74.7400)  acc5: 98.4375 (98.3000)  time: 0.0268  data: 0.0001  max mem: 6052
[04:18:00.542076] Test: Total time: 0:00:05 (0.0333 s / it)
[04:18:00.542537] * Acc@1 74.740 Acc@5 98.300 loss 0.777
[04:18:00.542830] Accuracy of the network on the 10000 test images: 74.7%
[04:18:00.543026] Max accuracy: 74.74%
[04:18:00.929420] log_dir: ./output_dir
[04:18:01.816793] Epoch: [34]  [  0/781]  eta: 0:11:31  lr: 0.000197  training_loss: 1.4811 (1.4811)  classification_loss: 1.3422 (1.3422)  loss_mask: 0.1389 (0.1389)  time: 0.8853  data: 0.6920  max mem: 6052
[04:18:05.242526] Epoch: [34]  [ 20/781]  eta: 0:02:36  lr: 0.000197  training_loss: 1.4578 (1.4819)  classification_loss: 1.4309 (1.4339)  loss_mask: 0.0356 (0.0480)  time: 0.1712  data: 0.0002  max mem: 6052
[04:18:08.669092] Epoch: [34]  [ 40/781]  eta: 0:02:19  lr: 0.000197  training_loss: 1.5005 (1.4916)  classification_loss: 1.4883 (1.4581)  loss_mask: 0.0131 (0.0336)  time: 0.1712  data: 0.0004  max mem: 6052
[04:18:12.092281] Epoch: [34]  [ 60/781]  eta: 0:02:11  lr: 0.000197  training_loss: 1.4755 (1.4921)  classification_loss: 1.4674 (1.4663)  loss_mask: 0.0106 (0.0258)  time: 0.1711  data: 0.0002  max mem: 6052
[04:18:15.524473] Epoch: [34]  [ 80/781]  eta: 0:02:06  lr: 0.000197  training_loss: 1.5240 (1.4982)  classification_loss: 1.5155 (1.4771)  loss_mask: 0.0057 (0.0211)  time: 0.1715  data: 0.0003  max mem: 6052
[04:18:18.932765] Epoch: [34]  [100/781]  eta: 0:02:01  lr: 0.000197  training_loss: 1.5189 (1.5000)  classification_loss: 1.5160 (1.4823)  loss_mask: 0.0038 (0.0177)  time: 0.1703  data: 0.0003  max mem: 6052
[04:18:22.352814] Epoch: [34]  [120/781]  eta: 0:01:56  lr: 0.000196  training_loss: 1.5078 (1.5030)  classification_loss: 1.4724 (1.4830)  loss_mask: 0.0217 (0.0200)  time: 0.1709  data: 0.0003  max mem: 6052
[04:18:25.760975] Epoch: [34]  [140/781]  eta: 0:01:52  lr: 0.000196  training_loss: 1.4781 (1.5023)  classification_loss: 1.4411 (1.4809)  loss_mask: 0.0105 (0.0214)  time: 0.1703  data: 0.0002  max mem: 6052
[04:18:29.190942] Epoch: [34]  [160/781]  eta: 0:01:48  lr: 0.000196  training_loss: 1.5263 (1.5062)  classification_loss: 1.4915 (1.4797)  loss_mask: 0.0357 (0.0265)  time: 0.1714  data: 0.0002  max mem: 6052
[04:18:32.624884] Epoch: [34]  [180/781]  eta: 0:01:45  lr: 0.000196  training_loss: 1.5537 (1.5121)  classification_loss: 1.4942 (1.4792)  loss_mask: 0.0516 (0.0329)  time: 0.1716  data: 0.0002  max mem: 6052
[04:18:36.046039] Epoch: [34]  [200/781]  eta: 0:01:41  lr: 0.000196  training_loss: 1.5850 (1.5164)  classification_loss: 1.5348 (1.4818)  loss_mask: 0.0308 (0.0347)  time: 0.1710  data: 0.0002  max mem: 6052
[04:18:39.493558] Epoch: [34]  [220/781]  eta: 0:01:37  lr: 0.000196  training_loss: 1.6420 (1.5229)  classification_loss: 1.5320 (1.4835)  loss_mask: 0.0769 (0.0394)  time: 0.1723  data: 0.0002  max mem: 6052
[04:18:42.922103] Epoch: [34]  [240/781]  eta: 0:01:34  lr: 0.000196  training_loss: 1.6260 (1.5296)  classification_loss: 1.4942 (1.4838)  loss_mask: 0.1068 (0.0459)  time: 0.1714  data: 0.0002  max mem: 6052
[04:18:46.342804] Epoch: [34]  [260/781]  eta: 0:01:30  lr: 0.000196  training_loss: 1.5707 (1.5326)  classification_loss: 1.4526 (1.4814)  loss_mask: 0.0990 (0.0512)  time: 0.1709  data: 0.0002  max mem: 6052
[04:18:49.756356] Epoch: [34]  [280/781]  eta: 0:01:27  lr: 0.000196  training_loss: 1.5344 (1.5318)  classification_loss: 1.4818 (1.4809)  loss_mask: 0.0396 (0.0509)  time: 0.1706  data: 0.0003  max mem: 6052
[04:18:53.184658] Epoch: [34]  [300/781]  eta: 0:01:23  lr: 0.000196  training_loss: 1.5536 (1.5320)  classification_loss: 1.5304 (1.4820)  loss_mask: 0.0295 (0.0499)  time: 0.1713  data: 0.0002  max mem: 6052
[04:18:56.615501] Epoch: [34]  [320/781]  eta: 0:01:19  lr: 0.000196  training_loss: 1.5455 (1.5310)  classification_loss: 1.4631 (1.4818)  loss_mask: 0.0212 (0.0492)  time: 0.1714  data: 0.0003  max mem: 6052
[04:19:00.044144] Epoch: [34]  [340/781]  eta: 0:01:16  lr: 0.000196  training_loss: 1.4552 (1.5266)  classification_loss: 1.4376 (1.4796)  loss_mask: 0.0119 (0.0470)  time: 0.1713  data: 0.0004  max mem: 6052
[04:19:03.484479] Epoch: [34]  [360/781]  eta: 0:01:12  lr: 0.000195  training_loss: 1.5188 (1.5253)  classification_loss: 1.4997 (1.4803)  loss_mask: 0.0079 (0.0450)  time: 0.1719  data: 0.0002  max mem: 6052
[04:19:06.892112] Epoch: [34]  [380/781]  eta: 0:01:09  lr: 0.000195  training_loss: 1.5344 (1.5268)  classification_loss: 1.4783 (1.4802)  loss_mask: 0.0494 (0.0466)  time: 0.1703  data: 0.0003  max mem: 6052
[04:19:10.315310] Epoch: [34]  [400/781]  eta: 0:01:05  lr: 0.000195  training_loss: 1.6253 (1.5332)  classification_loss: 1.4933 (1.4808)  loss_mask: 0.1000 (0.0523)  time: 0.1711  data: 0.0002  max mem: 6052
[04:19:13.747918] Epoch: [34]  [420/781]  eta: 0:01:02  lr: 0.000195  training_loss: 1.5515 (1.5323)  classification_loss: 1.4765 (1.4800)  loss_mask: 0.0510 (0.0523)  time: 0.1716  data: 0.0003  max mem: 6052
[04:19:17.174944] Epoch: [34]  [440/781]  eta: 0:00:58  lr: 0.000195  training_loss: 1.5038 (1.5320)  classification_loss: 1.4427 (1.4797)  loss_mask: 0.0485 (0.0523)  time: 0.1713  data: 0.0002  max mem: 6052
[04:19:20.599309] Epoch: [34]  [460/781]  eta: 0:00:55  lr: 0.000195  training_loss: 1.5031 (1.5305)  classification_loss: 1.4846 (1.4793)  loss_mask: 0.0272 (0.0512)  time: 0.1711  data: 0.0002  max mem: 6052
[04:19:24.091774] Epoch: [34]  [480/781]  eta: 0:00:52  lr: 0.000195  training_loss: 1.5141 (1.5293)  classification_loss: 1.4911 (1.4794)  loss_mask: 0.0184 (0.0499)  time: 0.1746  data: 0.0002  max mem: 6052
[04:19:27.512953] Epoch: [34]  [500/781]  eta: 0:00:48  lr: 0.000195  training_loss: 1.4853 (1.5286)  classification_loss: 1.4653 (1.4800)  loss_mask: 0.0147 (0.0486)  time: 0.1710  data: 0.0003  max mem: 6052
[04:19:30.924713] Epoch: [34]  [520/781]  eta: 0:00:45  lr: 0.000195  training_loss: 1.4904 (1.5282)  classification_loss: 1.4620 (1.4808)  loss_mask: 0.0184 (0.0474)  time: 0.1705  data: 0.0002  max mem: 6052
[04:19:34.346594] Epoch: [34]  [540/781]  eta: 0:00:41  lr: 0.000195  training_loss: 1.5282 (1.5295)  classification_loss: 1.4995 (1.4816)  loss_mask: 0.0268 (0.0479)  time: 0.1710  data: 0.0002  max mem: 6052
[04:19:37.762441] Epoch: [34]  [560/781]  eta: 0:00:38  lr: 0.000195  training_loss: 1.5721 (1.5303)  classification_loss: 1.4585 (1.4815)  loss_mask: 0.0352 (0.0488)  time: 0.1707  data: 0.0002  max mem: 6052
[04:19:41.233064] Epoch: [34]  [580/781]  eta: 0:00:34  lr: 0.000194  training_loss: 1.4730 (1.5294)  classification_loss: 1.4479 (1.4811)  loss_mask: 0.0251 (0.0484)  time: 0.1734  data: 0.0003  max mem: 6052
[04:19:44.657328] Epoch: [34]  [600/781]  eta: 0:00:31  lr: 0.000194  training_loss: 1.4648 (1.5278)  classification_loss: 1.4550 (1.4804)  loss_mask: 0.0166 (0.0474)  time: 0.1711  data: 0.0002  max mem: 6052
[04:19:48.078383] Epoch: [34]  [620/781]  eta: 0:00:27  lr: 0.000194  training_loss: 1.4902 (1.5276)  classification_loss: 1.4749 (1.4812)  loss_mask: 0.0130 (0.0464)  time: 0.1710  data: 0.0001  max mem: 6052
[04:19:51.508153] Epoch: [34]  [640/781]  eta: 0:00:24  lr: 0.000194  training_loss: 1.5274 (1.5276)  classification_loss: 1.5156 (1.4823)  loss_mask: 0.0096 (0.0452)  time: 0.1714  data: 0.0002  max mem: 6052
[04:19:54.915373] Epoch: [34]  [660/781]  eta: 0:00:20  lr: 0.000194  training_loss: 1.5110 (1.5272)  classification_loss: 1.5047 (1.4831)  loss_mask: 0.0077 (0.0441)  time: 0.1703  data: 0.0002  max mem: 6052
[04:19:58.336985] Epoch: [34]  [680/781]  eta: 0:00:17  lr: 0.000194  training_loss: 1.4826 (1.5263)  classification_loss: 1.4722 (1.4833)  loss_mask: 0.0057 (0.0430)  time: 0.1710  data: 0.0003  max mem: 6052
[04:20:01.761486] Epoch: [34]  [700/781]  eta: 0:00:13  lr: 0.000194  training_loss: 1.4951 (1.5254)  classification_loss: 1.4916 (1.4833)  loss_mask: 0.0072 (0.0420)  time: 0.1711  data: 0.0004  max mem: 6052
[04:20:05.174814] Epoch: [34]  [720/781]  eta: 0:00:10  lr: 0.000194  training_loss: 1.4890 (1.5245)  classification_loss: 1.4853 (1.4835)  loss_mask: 0.0053 (0.0410)  time: 0.1706  data: 0.0003  max mem: 6052
[04:20:08.583805] Epoch: [34]  [740/781]  eta: 0:00:07  lr: 0.000194  training_loss: 1.4167 (1.5222)  classification_loss: 1.4136 (1.4821)  loss_mask: 0.0039 (0.0400)  time: 0.1704  data: 0.0002  max mem: 6052
[04:20:12.033783] Epoch: [34]  [760/781]  eta: 0:00:03  lr: 0.000194  training_loss: 1.5244 (1.5220)  classification_loss: 1.5218 (1.4829)  loss_mask: 0.0035 (0.0391)  time: 0.1724  data: 0.0002  max mem: 6052
[04:20:15.441504] Epoch: [34]  [780/781]  eta: 0:00:00  lr: 0.000194  training_loss: 1.4960 (1.5215)  classification_loss: 1.4913 (1.4833)  loss_mask: 0.0028 (0.0381)  time: 0.1703  data: 0.0002  max mem: 6052
[04:20:15.605748] Epoch: [34] Total time: 0:02:14 (0.1724 s / it)
[04:20:15.606204] Averaged stats: lr: 0.000194  training_loss: 1.4960 (1.5215)  classification_loss: 1.4913 (1.4833)  loss_mask: 0.0028 (0.0381)
[04:20:16.263787] Test:  [  0/157]  eta: 0:01:42  testing_loss: 0.7570 (0.7570)  acc1: 71.8750 (71.8750)  acc5: 96.8750 (96.8750)  time: 0.6529  data: 0.6215  max mem: 6052
[04:20:16.556787] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.7570 (0.7993)  acc1: 73.4375 (74.5739)  acc5: 100.0000 (99.1477)  time: 0.0857  data: 0.0566  max mem: 6052
[04:20:16.841986] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.7459 (0.7680)  acc1: 78.1250 (75.8185)  acc5: 100.0000 (99.2560)  time: 0.0287  data: 0.0002  max mem: 6052
[04:20:17.123958] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.7619 (0.7821)  acc1: 73.4375 (74.6976)  acc5: 100.0000 (98.9415)  time: 0.0282  data: 0.0001  max mem: 6052
[04:20:17.407931] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.7681 (0.7846)  acc1: 73.4375 (74.3140)  acc5: 98.4375 (98.9329)  time: 0.0281  data: 0.0001  max mem: 6052
[04:20:17.696059] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.7543 (0.7785)  acc1: 75.0000 (74.6936)  acc5: 98.4375 (98.8051)  time: 0.0284  data: 0.0002  max mem: 6052
[04:20:17.981367] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.7591 (0.7799)  acc1: 73.4375 (74.3596)  acc5: 98.4375 (98.7193)  time: 0.0285  data: 0.0002  max mem: 6052
[04:20:18.265377] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7742 (0.7755)  acc1: 73.4375 (74.4938)  acc5: 98.4375 (98.6356)  time: 0.0283  data: 0.0003  max mem: 6052
[04:20:18.550610] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7699 (0.7822)  acc1: 75.0000 (74.3827)  acc5: 98.4375 (98.5340)  time: 0.0283  data: 0.0003  max mem: 6052
[04:20:18.832527] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7699 (0.7811)  acc1: 75.0000 (74.4849)  acc5: 98.4375 (98.5577)  time: 0.0282  data: 0.0002  max mem: 6052
[04:20:19.118047] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.7684 (0.7864)  acc1: 73.4375 (74.3348)  acc5: 98.4375 (98.5458)  time: 0.0282  data: 0.0002  max mem: 6052
[04:20:19.401302] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.8238 (0.7879)  acc1: 73.4375 (74.2680)  acc5: 98.4375 (98.5220)  time: 0.0283  data: 0.0002  max mem: 6052
[04:20:19.688966] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.8039 (0.7845)  acc1: 73.4375 (74.3673)  acc5: 98.4375 (98.4892)  time: 0.0284  data: 0.0001  max mem: 6052
[04:20:19.976673] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.7487 (0.7849)  acc1: 75.0000 (74.3798)  acc5: 98.4375 (98.4494)  time: 0.0286  data: 0.0002  max mem: 6052
[04:20:20.259585] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7671 (0.7830)  acc1: 75.0000 (74.5013)  acc5: 98.4375 (98.4707)  time: 0.0284  data: 0.0002  max mem: 6052
[04:20:20.537885] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7671 (0.7813)  acc1: 75.0000 (74.5550)  acc5: 98.4375 (98.4272)  time: 0.0280  data: 0.0001  max mem: 6052
[04:20:20.688481] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7737 (0.7843)  acc1: 73.4375 (74.4400)  acc5: 98.4375 (98.4400)  time: 0.0268  data: 0.0001  max mem: 6052
[04:20:20.849099] Test: Total time: 0:00:05 (0.0334 s / it)
[04:20:20.849556] * Acc@1 74.440 Acc@5 98.440 loss 0.784
[04:20:20.849867] Accuracy of the network on the 10000 test images: 74.4%
[04:20:20.850070] Max accuracy: 74.74%
[04:20:21.052869] log_dir: ./output_dir
[04:20:21.911121] Epoch: [35]  [  0/781]  eta: 0:11:08  lr: 0.000194  training_loss: 1.3183 (1.3183)  classification_loss: 1.3147 (1.3147)  loss_mask: 0.0036 (0.0036)  time: 0.8562  data: 0.6591  max mem: 6052
[04:20:25.340043] Epoch: [35]  [ 20/781]  eta: 0:02:35  lr: 0.000194  training_loss: 1.4241 (1.4337)  classification_loss: 1.4217 (1.4296)  loss_mask: 0.0037 (0.0041)  time: 0.1714  data: 0.0002  max mem: 6052
[04:20:28.794708] Epoch: [35]  [ 40/781]  eta: 0:02:19  lr: 0.000193  training_loss: 1.4796 (1.4441)  classification_loss: 1.4749 (1.4404)  loss_mask: 0.0030 (0.0037)  time: 0.1727  data: 0.0002  max mem: 6052
[04:20:32.217459] Epoch: [35]  [ 60/781]  eta: 0:02:11  lr: 0.000193  training_loss: 1.5223 (1.4691)  classification_loss: 1.4893 (1.4625)  loss_mask: 0.0037 (0.0066)  time: 0.1711  data: 0.0002  max mem: 6052
[04:20:35.641950] Epoch: [35]  [ 80/781]  eta: 0:02:06  lr: 0.000193  training_loss: 1.4868 (1.4747)  classification_loss: 1.4713 (1.4666)  loss_mask: 0.0091 (0.0081)  time: 0.1711  data: 0.0002  max mem: 6052
[04:20:39.057500] Epoch: [35]  [100/781]  eta: 0:02:01  lr: 0.000193  training_loss: 1.4736 (1.4741)  classification_loss: 1.4540 (1.4654)  loss_mask: 0.0060 (0.0086)  time: 0.1707  data: 0.0002  max mem: 6052
[04:20:42.471052] Epoch: [35]  [120/781]  eta: 0:01:56  lr: 0.000193  training_loss: 1.6047 (1.5319)  classification_loss: 1.4634 (1.4637)  loss_mask: 0.0761 (0.0682)  time: 0.1706  data: 0.0003  max mem: 6052
[04:20:45.889711] Epoch: [35]  [140/781]  eta: 0:01:52  lr: 0.000193  training_loss: 1.5715 (1.5396)  classification_loss: 1.4367 (1.4620)  loss_mask: 0.1048 (0.0776)  time: 0.1709  data: 0.0004  max mem: 6052
[04:20:49.286968] Epoch: [35]  [160/781]  eta: 0:01:48  lr: 0.000193  training_loss: 1.5094 (1.5354)  classification_loss: 1.4526 (1.4597)  loss_mask: 0.0626 (0.0757)  time: 0.1698  data: 0.0001  max mem: 6052
[04:20:52.693579] Epoch: [35]  [180/781]  eta: 0:01:45  lr: 0.000193  training_loss: 1.4940 (1.5300)  classification_loss: 1.4393 (1.4571)  loss_mask: 0.0486 (0.0730)  time: 0.1702  data: 0.0001  max mem: 6052
[04:20:56.116193] Epoch: [35]  [200/781]  eta: 0:01:41  lr: 0.000193  training_loss: 1.5348 (1.5294)  classification_loss: 1.5037 (1.4599)  loss_mask: 0.0370 (0.0695)  time: 0.1711  data: 0.0002  max mem: 6052
[04:20:59.519375] Epoch: [35]  [220/781]  eta: 0:01:37  lr: 0.000193  training_loss: 1.4912 (1.5251)  classification_loss: 1.4605 (1.4592)  loss_mask: 0.0241 (0.0659)  time: 0.1701  data: 0.0002  max mem: 6052
[04:21:02.931089] Epoch: [35]  [240/781]  eta: 0:01:33  lr: 0.000193  training_loss: 1.5222 (1.5250)  classification_loss: 1.5021 (1.4623)  loss_mask: 0.0244 (0.0627)  time: 0.1705  data: 0.0002  max mem: 6052
[04:21:06.350531] Epoch: [35]  [260/781]  eta: 0:01:30  lr: 0.000192  training_loss: 1.4936 (1.5235)  classification_loss: 1.4777 (1.4639)  loss_mask: 0.0203 (0.0596)  time: 0.1709  data: 0.0001  max mem: 6052
[04:21:09.760769] Epoch: [35]  [280/781]  eta: 0:01:26  lr: 0.000192  training_loss: 1.4976 (1.5207)  classification_loss: 1.4711 (1.4637)  loss_mask: 0.0210 (0.0570)  time: 0.1704  data: 0.0002  max mem: 6052
[04:21:13.174197] Epoch: [35]  [300/781]  eta: 0:01:23  lr: 0.000192  training_loss: 1.5168 (1.5217)  classification_loss: 1.4845 (1.4650)  loss_mask: 0.0308 (0.0566)  time: 0.1706  data: 0.0002  max mem: 6052
[04:21:16.568080] Epoch: [35]  [320/781]  eta: 0:01:19  lr: 0.000192  training_loss: 1.4854 (1.5199)  classification_loss: 1.4602 (1.4646)  loss_mask: 0.0274 (0.0553)  time: 0.1696  data: 0.0002  max mem: 6052
[04:21:19.968824] Epoch: [35]  [340/781]  eta: 0:01:16  lr: 0.000192  training_loss: 1.4601 (1.5159)  classification_loss: 1.4334 (1.4625)  loss_mask: 0.0204 (0.0534)  time: 0.1700  data: 0.0002  max mem: 6052
[04:21:23.378982] Epoch: [35]  [360/781]  eta: 0:01:12  lr: 0.000192  training_loss: 1.5124 (1.5161)  classification_loss: 1.4967 (1.4646)  loss_mask: 0.0148 (0.0515)  time: 0.1704  data: 0.0003  max mem: 6052
[04:21:26.788386] Epoch: [35]  [380/781]  eta: 0:01:09  lr: 0.000192  training_loss: 1.4795 (1.5151)  classification_loss: 1.4552 (1.4652)  loss_mask: 0.0113 (0.0499)  time: 0.1704  data: 0.0002  max mem: 6052
[04:21:30.192006] Epoch: [35]  [400/781]  eta: 0:01:05  lr: 0.000192  training_loss: 1.4620 (1.5125)  classification_loss: 1.4497 (1.4640)  loss_mask: 0.0132 (0.0485)  time: 0.1701  data: 0.0002  max mem: 6052
[04:21:33.661408] Epoch: [35]  [420/781]  eta: 0:01:02  lr: 0.000192  training_loss: 1.5002 (1.5112)  classification_loss: 1.4871 (1.4644)  loss_mask: 0.0096 (0.0468)  time: 0.1734  data: 0.0002  max mem: 6052
[04:21:37.149345] Epoch: [35]  [440/781]  eta: 0:00:58  lr: 0.000192  training_loss: 1.4242 (1.5076)  classification_loss: 1.4165 (1.4626)  loss_mask: 0.0076 (0.0450)  time: 0.1743  data: 0.0003  max mem: 6052
[04:21:40.668459] Epoch: [35]  [460/781]  eta: 0:00:55  lr: 0.000192  training_loss: 1.4621 (1.5054)  classification_loss: 1.4591 (1.4622)  loss_mask: 0.0038 (0.0432)  time: 0.1759  data: 0.0003  max mem: 6052
[04:21:44.098122] Epoch: [35]  [480/781]  eta: 0:00:51  lr: 0.000191  training_loss: 1.4629 (1.5042)  classification_loss: 1.4590 (1.4625)  loss_mask: 0.0046 (0.0417)  time: 0.1714  data: 0.0002  max mem: 6052
[04:21:47.524712] Epoch: [35]  [500/781]  eta: 0:00:48  lr: 0.000191  training_loss: 1.5434 (1.5055)  classification_loss: 1.5395 (1.4652)  loss_mask: 0.0050 (0.0403)  time: 0.1712  data: 0.0003  max mem: 6052
[04:21:50.965733] Epoch: [35]  [520/781]  eta: 0:00:45  lr: 0.000191  training_loss: 1.4566 (1.5037)  classification_loss: 1.4544 (1.4648)  loss_mask: 0.0030 (0.0389)  time: 0.1719  data: 0.0003  max mem: 6052
[04:21:54.444909] Epoch: [35]  [540/781]  eta: 0:00:41  lr: 0.000191  training_loss: 1.5046 (1.5038)  classification_loss: 1.5011 (1.4661)  loss_mask: 0.0034 (0.0377)  time: 0.1739  data: 0.0002  max mem: 6052
[04:21:57.871158] Epoch: [35]  [560/781]  eta: 0:00:38  lr: 0.000191  training_loss: 1.4369 (1.5024)  classification_loss: 1.4309 (1.4660)  loss_mask: 0.0030 (0.0365)  time: 0.1712  data: 0.0002  max mem: 6052
[04:22:01.323216] Epoch: [35]  [580/781]  eta: 0:00:34  lr: 0.000191  training_loss: 1.4860 (1.5028)  classification_loss: 1.4842 (1.4673)  loss_mask: 0.0033 (0.0355)  time: 0.1725  data: 0.0002  max mem: 6052
[04:22:04.797423] Epoch: [35]  [600/781]  eta: 0:00:31  lr: 0.000191  training_loss: 1.4863 (1.5025)  classification_loss: 1.4759 (1.4679)  loss_mask: 0.0044 (0.0345)  time: 0.1736  data: 0.0003  max mem: 6052
[04:22:08.202261] Epoch: [35]  [620/781]  eta: 0:00:27  lr: 0.000191  training_loss: 1.4218 (1.5005)  classification_loss: 1.4059 (1.4667)  loss_mask: 0.0091 (0.0339)  time: 0.1702  data: 0.0002  max mem: 6052
[04:22:11.605711] Epoch: [35]  [640/781]  eta: 0:00:24  lr: 0.000191  training_loss: 1.5125 (1.5007)  classification_loss: 1.4985 (1.4674)  loss_mask: 0.0047 (0.0334)  time: 0.1701  data: 0.0002  max mem: 6052
[04:22:15.021743] Epoch: [35]  [660/781]  eta: 0:00:20  lr: 0.000191  training_loss: 1.5041 (1.5010)  classification_loss: 1.4901 (1.4680)  loss_mask: 0.0120 (0.0330)  time: 0.1707  data: 0.0003  max mem: 6052
[04:22:18.462901] Epoch: [35]  [680/781]  eta: 0:00:17  lr: 0.000191  training_loss: 1.5413 (1.5028)  classification_loss: 1.5021 (1.4686)  loss_mask: 0.0488 (0.0342)  time: 0.1719  data: 0.0002  max mem: 6052
[04:22:21.957739] Epoch: [35]  [700/781]  eta: 0:00:13  lr: 0.000190  training_loss: 1.5736 (1.5051)  classification_loss: 1.4561 (1.4694)  loss_mask: 0.0655 (0.0358)  time: 0.1746  data: 0.0002  max mem: 6052
[04:22:25.369806] Epoch: [35]  [720/781]  eta: 0:00:10  lr: 0.000190  training_loss: 1.4863 (1.5050)  classification_loss: 1.4579 (1.4692)  loss_mask: 0.0281 (0.0359)  time: 0.1705  data: 0.0002  max mem: 6052
[04:22:28.838870] Epoch: [35]  [740/781]  eta: 0:00:07  lr: 0.000190  training_loss: 1.4567 (1.5040)  classification_loss: 1.4394 (1.4684)  loss_mask: 0.0173 (0.0356)  time: 0.1733  data: 0.0002  max mem: 6052
[04:22:32.251440] Epoch: [35]  [760/781]  eta: 0:00:03  lr: 0.000190  training_loss: 1.4871 (1.5038)  classification_loss: 1.4814 (1.4689)  loss_mask: 0.0071 (0.0349)  time: 0.1705  data: 0.0002  max mem: 6052
[04:22:35.730845] Epoch: [35]  [780/781]  eta: 0:00:00  lr: 0.000190  training_loss: 1.4627 (1.5030)  classification_loss: 1.4568 (1.4689)  loss_mask: 0.0043 (0.0341)  time: 0.1739  data: 0.0002  max mem: 6052
[04:22:35.893098] Epoch: [35] Total time: 0:02:14 (0.1727 s / it)
[04:22:35.893816] Averaged stats: lr: 0.000190  training_loss: 1.4627 (1.5030)  classification_loss: 1.4568 (1.4689)  loss_mask: 0.0043 (0.0341)
[04:22:36.581810] Test:  [  0/157]  eta: 0:01:47  testing_loss: 0.7075 (0.7075)  acc1: 76.5625 (76.5625)  acc5: 100.0000 (100.0000)  time: 0.6841  data: 0.6488  max mem: 6052
[04:22:36.872387] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.7701 (0.7692)  acc1: 73.4375 (74.1477)  acc5: 100.0000 (99.0057)  time: 0.0884  data: 0.0591  max mem: 6052
[04:22:37.160503] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.7385 (0.7330)  acc1: 76.5625 (76.6369)  acc5: 98.4375 (98.9583)  time: 0.0287  data: 0.0001  max mem: 6052
[04:22:37.445111] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.7385 (0.7473)  acc1: 78.1250 (76.0585)  acc5: 98.4375 (98.8407)  time: 0.0285  data: 0.0001  max mem: 6052
[04:22:37.729008] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.7313 (0.7451)  acc1: 76.5625 (76.3338)  acc5: 98.4375 (98.5137)  time: 0.0283  data: 0.0002  max mem: 6052
[04:22:38.012654] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.7157 (0.7388)  acc1: 76.5625 (76.3174)  acc5: 98.4375 (98.4681)  time: 0.0283  data: 0.0002  max mem: 6052
[04:22:38.294808] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.7238 (0.7437)  acc1: 75.0000 (76.0502)  acc5: 98.4375 (98.4119)  time: 0.0282  data: 0.0001  max mem: 6052
[04:22:38.581231] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7277 (0.7406)  acc1: 75.0000 (76.1004)  acc5: 98.4375 (98.3935)  time: 0.0282  data: 0.0001  max mem: 6052
[04:22:38.865003] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7616 (0.7501)  acc1: 73.4375 (75.6559)  acc5: 98.4375 (98.3025)  time: 0.0282  data: 0.0001  max mem: 6052
[04:22:39.149286] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7578 (0.7483)  acc1: 73.4375 (75.6868)  acc5: 98.4375 (98.3345)  time: 0.0283  data: 0.0001  max mem: 6052
[04:22:39.430477] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.7187 (0.7502)  acc1: 76.5625 (75.5724)  acc5: 100.0000 (98.3911)  time: 0.0282  data: 0.0001  max mem: 6052
[04:22:39.715945] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.7790 (0.7531)  acc1: 73.4375 (75.4927)  acc5: 98.4375 (98.2967)  time: 0.0282  data: 0.0002  max mem: 6052
[04:22:40.001332] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.7511 (0.7499)  acc1: 73.4375 (75.6198)  acc5: 98.4375 (98.3084)  time: 0.0284  data: 0.0002  max mem: 6052
[04:22:40.289129] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.7137 (0.7501)  acc1: 75.0000 (75.5606)  acc5: 98.4375 (98.2824)  time: 0.0285  data: 0.0002  max mem: 6052
[04:22:40.570281] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7655 (0.7484)  acc1: 75.0000 (75.6760)  acc5: 98.4375 (98.2602)  time: 0.0283  data: 0.0001  max mem: 6052
[04:22:40.848716] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7640 (0.7458)  acc1: 75.0000 (75.7243)  acc5: 98.4375 (98.2719)  time: 0.0279  data: 0.0001  max mem: 6052
[04:22:41.000706] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7425 (0.7468)  acc1: 75.0000 (75.6800)  acc5: 98.4375 (98.2900)  time: 0.0269  data: 0.0001  max mem: 6052
[04:22:41.153964] Test: Total time: 0:00:05 (0.0335 s / it)
[04:22:41.155645] * Acc@1 75.680 Acc@5 98.290 loss 0.747
[04:22:41.155988] Accuracy of the network on the 10000 test images: 75.7%
[04:22:41.156170] Max accuracy: 75.68%
[04:22:41.293476] log_dir: ./output_dir
[04:22:42.178586] Epoch: [36]  [  0/781]  eta: 0:11:29  lr: 0.000190  training_loss: 1.4514 (1.4514)  classification_loss: 1.4476 (1.4476)  loss_mask: 0.0038 (0.0038)  time: 0.8835  data: 0.6845  max mem: 6052
[04:22:45.604571] Epoch: [36]  [ 20/781]  eta: 0:02:36  lr: 0.000190  training_loss: 1.5029 (1.4911)  classification_loss: 1.4994 (1.4874)  loss_mask: 0.0035 (0.0037)  time: 0.1712  data: 0.0001  max mem: 6052
[04:22:49.028831] Epoch: [36]  [ 40/781]  eta: 0:02:19  lr: 0.000190  training_loss: 1.4979 (1.4920)  classification_loss: 1.4939 (1.4889)  loss_mask: 0.0024 (0.0032)  time: 0.1711  data: 0.0002  max mem: 6052
[04:22:52.449303] Epoch: [36]  [ 60/781]  eta: 0:02:11  lr: 0.000190  training_loss: 1.4767 (1.4867)  classification_loss: 1.4747 (1.4837)  loss_mask: 0.0021 (0.0030)  time: 0.1709  data: 0.0002  max mem: 6052
[04:22:55.873099] Epoch: [36]  [ 80/781]  eta: 0:02:06  lr: 0.000190  training_loss: 1.4955 (1.4925)  classification_loss: 1.4721 (1.4864)  loss_mask: 0.0061 (0.0060)  time: 0.1711  data: 0.0002  max mem: 6052
[04:22:59.286442] Epoch: [36]  [100/781]  eta: 0:02:01  lr: 0.000190  training_loss: 1.4498 (1.4822)  classification_loss: 1.4480 (1.4766)  loss_mask: 0.0027 (0.0056)  time: 0.1706  data: 0.0002  max mem: 6052
[04:23:02.751482] Epoch: [36]  [120/781]  eta: 0:01:57  lr: 0.000190  training_loss: 1.4701 (1.4809)  classification_loss: 1.4667 (1.4757)  loss_mask: 0.0031 (0.0052)  time: 0.1732  data: 0.0003  max mem: 6052
[04:23:06.184842] Epoch: [36]  [140/781]  eta: 0:01:53  lr: 0.000189  training_loss: 1.4490 (1.4745)  classification_loss: 1.4465 (1.4695)  loss_mask: 0.0032 (0.0050)  time: 0.1716  data: 0.0002  max mem: 6052
[04:23:09.691547] Epoch: [36]  [160/781]  eta: 0:01:49  lr: 0.000189  training_loss: 1.4856 (1.4727)  classification_loss: 1.4768 (1.4672)  loss_mask: 0.0044 (0.0055)  time: 0.1753  data: 0.0002  max mem: 6052
[04:23:13.125909] Epoch: [36]  [180/781]  eta: 0:01:45  lr: 0.000189  training_loss: 1.4611 (1.4738)  classification_loss: 1.4380 (1.4654)  loss_mask: 0.0237 (0.0084)  time: 0.1716  data: 0.0003  max mem: 6052
[04:23:16.543747] Epoch: [36]  [200/781]  eta: 0:01:41  lr: 0.000189  training_loss: 1.5023 (1.4779)  classification_loss: 1.4871 (1.4651)  loss_mask: 0.0211 (0.0128)  time: 0.1708  data: 0.0002  max mem: 6052
[04:23:19.952698] Epoch: [36]  [220/781]  eta: 0:01:38  lr: 0.000189  training_loss: 1.5509 (1.4857)  classification_loss: 1.4832 (1.4667)  loss_mask: 0.0460 (0.0190)  time: 0.1704  data: 0.0001  max mem: 6052
[04:23:23.377787] Epoch: [36]  [240/781]  eta: 0:01:34  lr: 0.000189  training_loss: 1.4179 (1.4833)  classification_loss: 1.3921 (1.4643)  loss_mask: 0.0180 (0.0191)  time: 0.1712  data: 0.0002  max mem: 6052
[04:23:26.817680] Epoch: [36]  [260/781]  eta: 0:01:30  lr: 0.000189  training_loss: 1.5352 (1.4848)  classification_loss: 1.5274 (1.4663)  loss_mask: 0.0095 (0.0185)  time: 0.1719  data: 0.0002  max mem: 6052
[04:23:30.228244] Epoch: [36]  [280/781]  eta: 0:01:27  lr: 0.000189  training_loss: 1.4420 (1.4822)  classification_loss: 1.4388 (1.4638)  loss_mask: 0.0108 (0.0184)  time: 0.1705  data: 0.0003  max mem: 6052
[04:23:33.654650] Epoch: [36]  [300/781]  eta: 0:01:23  lr: 0.000189  training_loss: 1.4831 (1.4834)  classification_loss: 1.4787 (1.4658)  loss_mask: 0.0048 (0.0176)  time: 0.1712  data: 0.0003  max mem: 6052
[04:23:37.123335] Epoch: [36]  [320/781]  eta: 0:01:20  lr: 0.000189  training_loss: 1.4191 (1.4810)  classification_loss: 1.4158 (1.4642)  loss_mask: 0.0033 (0.0167)  time: 0.1733  data: 0.0002  max mem: 6052
[04:23:40.525509] Epoch: [36]  [340/781]  eta: 0:01:16  lr: 0.000189  training_loss: 1.4743 (1.4814)  classification_loss: 1.4600 (1.4651)  loss_mask: 0.0056 (0.0163)  time: 0.1700  data: 0.0002  max mem: 6052
[04:23:43.938359] Epoch: [36]  [360/781]  eta: 0:01:13  lr: 0.000188  training_loss: 1.4868 (1.4823)  classification_loss: 1.4816 (1.4667)  loss_mask: 0.0030 (0.0156)  time: 0.1706  data: 0.0002  max mem: 6052
[04:23:47.353371] Epoch: [36]  [380/781]  eta: 0:01:09  lr: 0.000188  training_loss: 1.4775 (1.4836)  classification_loss: 1.4763 (1.4687)  loss_mask: 0.0018 (0.0149)  time: 0.1707  data: 0.0002  max mem: 6052
[04:23:50.755036] Epoch: [36]  [400/781]  eta: 0:01:05  lr: 0.000188  training_loss: 1.4272 (1.4817)  classification_loss: 1.4258 (1.4675)  loss_mask: 0.0014 (0.0142)  time: 0.1700  data: 0.0003  max mem: 6052
[04:23:54.194230] Epoch: [36]  [420/781]  eta: 0:01:02  lr: 0.000188  training_loss: 1.4694 (1.4818)  classification_loss: 1.4665 (1.4678)  loss_mask: 0.0017 (0.0140)  time: 0.1719  data: 0.0003  max mem: 6052
[04:23:57.614808] Epoch: [36]  [440/781]  eta: 0:00:58  lr: 0.000188  training_loss: 1.5164 (1.4853)  classification_loss: 1.4409 (1.4676)  loss_mask: 0.0436 (0.0177)  time: 0.1709  data: 0.0002  max mem: 6052
[04:24:01.018716] Epoch: [36]  [460/781]  eta: 0:00:55  lr: 0.000188  training_loss: 1.4848 (1.4877)  classification_loss: 1.4024 (1.4656)  loss_mask: 0.0758 (0.0221)  time: 0.1701  data: 0.0002  max mem: 6052
[04:24:04.459156] Epoch: [36]  [480/781]  eta: 0:00:52  lr: 0.000188  training_loss: 1.5071 (1.4883)  classification_loss: 1.4905 (1.4654)  loss_mask: 0.0365 (0.0228)  time: 0.1719  data: 0.0002  max mem: 6052
[04:24:07.883366] Epoch: [36]  [500/781]  eta: 0:00:48  lr: 0.000188  training_loss: 1.5339 (1.4904)  classification_loss: 1.4955 (1.4664)  loss_mask: 0.0384 (0.0240)  time: 0.1711  data: 0.0002  max mem: 6052
[04:24:11.331060] Epoch: [36]  [520/781]  eta: 0:00:45  lr: 0.000188  training_loss: 1.5646 (1.4936)  classification_loss: 1.5306 (1.4691)  loss_mask: 0.0268 (0.0245)  time: 0.1723  data: 0.0002  max mem: 6052
[04:24:14.778678] Epoch: [36]  [540/781]  eta: 0:00:41  lr: 0.000188  training_loss: 1.5143 (1.4944)  classification_loss: 1.4708 (1.4701)  loss_mask: 0.0109 (0.0243)  time: 0.1723  data: 0.0002  max mem: 6052
[04:24:18.256854] Epoch: [36]  [560/781]  eta: 0:00:38  lr: 0.000188  training_loss: 1.4843 (1.4947)  classification_loss: 1.4804 (1.4709)  loss_mask: 0.0068 (0.0238)  time: 0.1738  data: 0.0002  max mem: 6052
[04:24:21.697056] Epoch: [36]  [580/781]  eta: 0:00:34  lr: 0.000187  training_loss: 1.4555 (1.4937)  classification_loss: 1.4334 (1.4702)  loss_mask: 0.0101 (0.0235)  time: 0.1719  data: 0.0003  max mem: 6052
[04:24:25.120275] Epoch: [36]  [600/781]  eta: 0:00:31  lr: 0.000187  training_loss: 1.4575 (1.4931)  classification_loss: 1.4537 (1.4701)  loss_mask: 0.0046 (0.0230)  time: 0.1711  data: 0.0002  max mem: 6052
[04:24:28.585110] Epoch: [36]  [620/781]  eta: 0:00:27  lr: 0.000187  training_loss: 1.5122 (1.4933)  classification_loss: 1.5095 (1.4709)  loss_mask: 0.0039 (0.0224)  time: 0.1731  data: 0.0002  max mem: 6052
[04:24:32.013438] Epoch: [36]  [640/781]  eta: 0:00:24  lr: 0.000187  training_loss: 1.4575 (1.4923)  classification_loss: 1.4544 (1.4704)  loss_mask: 0.0037 (0.0219)  time: 0.1713  data: 0.0003  max mem: 6052
[04:24:35.535222] Epoch: [36]  [660/781]  eta: 0:00:20  lr: 0.000187  training_loss: 1.4926 (1.4926)  classification_loss: 1.4902 (1.4711)  loss_mask: 0.0039 (0.0215)  time: 0.1760  data: 0.0002  max mem: 6052
[04:24:38.939645] Epoch: [36]  [680/781]  eta: 0:00:17  lr: 0.000187  training_loss: 1.4838 (1.4921)  classification_loss: 1.4788 (1.4709)  loss_mask: 0.0030 (0.0213)  time: 0.1701  data: 0.0002  max mem: 6052
[04:24:42.356243] Epoch: [36]  [700/781]  eta: 0:00:13  lr: 0.000187  training_loss: 1.4645 (1.4925)  classification_loss: 1.4353 (1.4712)  loss_mask: 0.0117 (0.0214)  time: 0.1707  data: 0.0003  max mem: 6052
[04:24:45.756002] Epoch: [36]  [720/781]  eta: 0:00:10  lr: 0.000187  training_loss: 1.4989 (1.4925)  classification_loss: 1.4705 (1.4710)  loss_mask: 0.0165 (0.0214)  time: 0.1699  data: 0.0002  max mem: 6052
[04:24:49.198224] Epoch: [36]  [740/781]  eta: 0:00:07  lr: 0.000187  training_loss: 1.4711 (1.4915)  classification_loss: 1.4574 (1.4704)  loss_mask: 0.0056 (0.0212)  time: 0.1720  data: 0.0002  max mem: 6052
[04:24:52.618999] Epoch: [36]  [760/781]  eta: 0:00:03  lr: 0.000187  training_loss: 1.5028 (1.4924)  classification_loss: 1.4584 (1.4703)  loss_mask: 0.0148 (0.0222)  time: 0.1710  data: 0.0003  max mem: 6052
[04:24:56.012225] Epoch: [36]  [780/781]  eta: 0:00:00  lr: 0.000187  training_loss: 1.5131 (1.4928)  classification_loss: 1.4456 (1.4701)  loss_mask: 0.0263 (0.0226)  time: 0.1696  data: 0.0001  max mem: 6052
[04:24:56.178321] Epoch: [36] Total time: 0:02:14 (0.1727 s / it)
[04:24:56.178786] Averaged stats: lr: 0.000187  training_loss: 1.5131 (1.4928)  classification_loss: 1.4456 (1.4701)  loss_mask: 0.0263 (0.0226)
[04:24:56.833377] Test:  [  0/157]  eta: 0:01:42  testing_loss: 0.7610 (0.7610)  acc1: 73.4375 (73.4375)  acc5: 98.4375 (98.4375)  time: 0.6502  data: 0.6205  max mem: 6052
[04:24:57.122224] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.7610 (0.7748)  acc1: 76.5625 (75.2841)  acc5: 98.4375 (98.8636)  time: 0.0852  data: 0.0569  max mem: 6052
[04:24:57.409384] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.7210 (0.7394)  acc1: 76.5625 (76.4881)  acc5: 98.4375 (98.8839)  time: 0.0287  data: 0.0003  max mem: 6052
[04:24:57.695773] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.7210 (0.7533)  acc1: 76.5625 (75.8569)  acc5: 98.4375 (98.6391)  time: 0.0286  data: 0.0002  max mem: 6052
[04:24:57.978167] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.7416 (0.7586)  acc1: 76.5625 (75.6479)  acc5: 98.4375 (98.5137)  time: 0.0283  data: 0.0002  max mem: 6052
[04:24:58.259880] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.7249 (0.7521)  acc1: 76.5625 (76.0417)  acc5: 98.4375 (98.4681)  time: 0.0281  data: 0.0002  max mem: 6052
[04:24:58.541547] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.7249 (0.7515)  acc1: 75.0000 (75.6660)  acc5: 98.4375 (98.4631)  time: 0.0281  data: 0.0002  max mem: 6052
[04:24:58.821781] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7198 (0.7474)  acc1: 75.0000 (75.8583)  acc5: 98.4375 (98.3495)  time: 0.0280  data: 0.0002  max mem: 6052
[04:24:59.103143] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7510 (0.7568)  acc1: 73.4375 (75.4630)  acc5: 98.4375 (98.3025)  time: 0.0280  data: 0.0001  max mem: 6052
[04:24:59.384260] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7399 (0.7522)  acc1: 78.1250 (75.8929)  acc5: 98.4375 (98.3345)  time: 0.0280  data: 0.0002  max mem: 6052
[04:24:59.665775] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.7399 (0.7582)  acc1: 76.5625 (75.4950)  acc5: 98.4375 (98.3137)  time: 0.0280  data: 0.0001  max mem: 6052
[04:24:59.946431] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.7802 (0.7588)  acc1: 73.4375 (75.4364)  acc5: 98.4375 (98.2967)  time: 0.0280  data: 0.0001  max mem: 6052
[04:25:00.226976] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.7209 (0.7543)  acc1: 75.0000 (75.5553)  acc5: 98.4375 (98.2825)  time: 0.0280  data: 0.0001  max mem: 6052
[04:25:00.508818] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.7320 (0.7560)  acc1: 76.5625 (75.5010)  acc5: 98.4375 (98.2944)  time: 0.0280  data: 0.0002  max mem: 6052
[04:25:00.788431] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7798 (0.7561)  acc1: 75.0000 (75.5984)  acc5: 98.4375 (98.2824)  time: 0.0280  data: 0.0001  max mem: 6052
[04:25:01.066046] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7798 (0.7549)  acc1: 75.0000 (75.6726)  acc5: 98.4375 (98.2512)  time: 0.0278  data: 0.0001  max mem: 6052
[04:25:01.215234] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7781 (0.7559)  acc1: 75.0000 (75.5900)  acc5: 98.4375 (98.2800)  time: 0.0268  data: 0.0001  max mem: 6052
[04:25:01.387528] Test: Total time: 0:00:05 (0.0332 s / it)
[04:25:01.388625] * Acc@1 75.590 Acc@5 98.280 loss 0.756
[04:25:01.388947] Accuracy of the network on the 10000 test images: 75.6%
[04:25:01.389156] Max accuracy: 75.68%
[04:25:01.719859] log_dir: ./output_dir
[04:25:02.632655] Epoch: [37]  [  0/781]  eta: 0:11:51  lr: 0.000187  training_loss: 1.4775 (1.4775)  classification_loss: 1.4467 (1.4467)  loss_mask: 0.0308 (0.0308)  time: 0.9112  data: 0.6934  max mem: 6052
[04:25:06.065167] Epoch: [37]  [ 20/781]  eta: 0:02:37  lr: 0.000186  training_loss: 1.4205 (1.4318)  classification_loss: 1.4158 (1.4228)  loss_mask: 0.0062 (0.0090)  time: 0.1715  data: 0.0002  max mem: 6052
[04:25:09.495701] Epoch: [37]  [ 40/781]  eta: 0:02:20  lr: 0.000186  training_loss: 1.4639 (1.4548)  classification_loss: 1.4582 (1.4477)  loss_mask: 0.0047 (0.0071)  time: 0.1715  data: 0.0003  max mem: 6052
[04:25:12.921440] Epoch: [37]  [ 60/781]  eta: 0:02:12  lr: 0.000186  training_loss: 1.4881 (1.4653)  classification_loss: 1.4758 (1.4580)  loss_mask: 0.0039 (0.0074)  time: 0.1712  data: 0.0002  max mem: 6052
[04:25:16.343524] Epoch: [37]  [ 80/781]  eta: 0:02:06  lr: 0.000186  training_loss: 1.5671 (1.4907)  classification_loss: 1.5551 (1.4785)  loss_mask: 0.0118 (0.0123)  time: 0.1710  data: 0.0003  max mem: 6052
[04:25:19.784018] Epoch: [37]  [100/781]  eta: 0:02:01  lr: 0.000186  training_loss: 1.4935 (1.4943)  classification_loss: 1.4727 (1.4775)  loss_mask: 0.0144 (0.0168)  time: 0.1720  data: 0.0002  max mem: 6052
[04:25:23.277558] Epoch: [37]  [120/781]  eta: 0:01:57  lr: 0.000186  training_loss: 1.4496 (1.4891)  classification_loss: 1.4468 (1.4734)  loss_mask: 0.0040 (0.0157)  time: 0.1746  data: 0.0002  max mem: 6052
[04:25:26.691497] Epoch: [37]  [140/781]  eta: 0:01:53  lr: 0.000186  training_loss: 1.5267 (1.4901)  classification_loss: 1.5159 (1.4755)  loss_mask: 0.0057 (0.0146)  time: 0.1706  data: 0.0002  max mem: 6052
[04:25:30.155655] Epoch: [37]  [160/781]  eta: 0:01:49  lr: 0.000186  training_loss: 1.5024 (1.4872)  classification_loss: 1.4828 (1.4735)  loss_mask: 0.0029 (0.0136)  time: 0.1731  data: 0.0002  max mem: 6052
[04:25:33.568485] Epoch: [37]  [180/781]  eta: 0:01:45  lr: 0.000186  training_loss: 1.4655 (1.4875)  classification_loss: 1.4619 (1.4750)  loss_mask: 0.0020 (0.0124)  time: 0.1705  data: 0.0002  max mem: 6052
[04:25:36.991048] Epoch: [37]  [200/781]  eta: 0:01:41  lr: 0.000186  training_loss: 1.4767 (1.4871)  classification_loss: 1.4761 (1.4757)  loss_mask: 0.0014 (0.0114)  time: 0.1710  data: 0.0002  max mem: 6052
[04:25:40.397907] Epoch: [37]  [220/781]  eta: 0:01:38  lr: 0.000186  training_loss: 1.4957 (1.4871)  classification_loss: 1.4565 (1.4756)  loss_mask: 0.0066 (0.0115)  time: 0.1703  data: 0.0002  max mem: 6052
[04:25:43.846713] Epoch: [37]  [240/781]  eta: 0:01:34  lr: 0.000185  training_loss: 1.4820 (1.4878)  classification_loss: 1.4440 (1.4745)  loss_mask: 0.0261 (0.0133)  time: 0.1724  data: 0.0003  max mem: 6052
[04:25:47.261083] Epoch: [37]  [260/781]  eta: 0:01:30  lr: 0.000185  training_loss: 1.4726 (1.4869)  classification_loss: 1.4598 (1.4742)  loss_mask: 0.0032 (0.0128)  time: 0.1706  data: 0.0004  max mem: 6052
[04:25:50.695574] Epoch: [37]  [280/781]  eta: 0:01:27  lr: 0.000185  training_loss: 1.4628 (1.4857)  classification_loss: 1.4612 (1.4737)  loss_mask: 0.0020 (0.0121)  time: 0.1717  data: 0.0002  max mem: 6052
[04:25:54.107265] Epoch: [37]  [300/781]  eta: 0:01:23  lr: 0.000185  training_loss: 1.4428 (1.4856)  classification_loss: 1.4403 (1.4735)  loss_mask: 0.0025 (0.0121)  time: 0.1705  data: 0.0002  max mem: 6052
[04:25:57.539575] Epoch: [37]  [320/781]  eta: 0:01:20  lr: 0.000185  training_loss: 1.4649 (1.4857)  classification_loss: 1.4460 (1.4739)  loss_mask: 0.0043 (0.0118)  time: 0.1715  data: 0.0002  max mem: 6052
[04:26:00.993529] Epoch: [37]  [340/781]  eta: 0:01:16  lr: 0.000185  training_loss: 1.4505 (1.4846)  classification_loss: 1.4477 (1.4733)  loss_mask: 0.0020 (0.0112)  time: 0.1726  data: 0.0005  max mem: 6052
[04:26:04.425137] Epoch: [37]  [360/781]  eta: 0:01:13  lr: 0.000185  training_loss: 1.4905 (1.4849)  classification_loss: 1.4890 (1.4741)  loss_mask: 0.0016 (0.0107)  time: 0.1715  data: 0.0002  max mem: 6052
[04:26:07.874664] Epoch: [37]  [380/781]  eta: 0:01:09  lr: 0.000185  training_loss: 1.4363 (1.4831)  classification_loss: 1.4355 (1.4729)  loss_mask: 0.0010 (0.0102)  time: 0.1724  data: 0.0002  max mem: 6052
[04:26:11.289971] Epoch: [37]  [400/781]  eta: 0:01:06  lr: 0.000185  training_loss: 1.4661 (1.4826)  classification_loss: 1.4639 (1.4728)  loss_mask: 0.0009 (0.0098)  time: 0.1707  data: 0.0003  max mem: 6052
[04:26:14.703304] Epoch: [37]  [420/781]  eta: 0:01:02  lr: 0.000185  training_loss: 1.4395 (1.4817)  classification_loss: 1.4389 (1.4723)  loss_mask: 0.0008 (0.0094)  time: 0.1705  data: 0.0002  max mem: 6052
[04:26:18.122541] Epoch: [37]  [440/781]  eta: 0:00:59  lr: 0.000185  training_loss: 1.4435 (1.4801)  classification_loss: 1.4431 (1.4712)  loss_mask: 0.0006 (0.0090)  time: 0.1709  data: 0.0002  max mem: 6052
[04:26:21.546473] Epoch: [37]  [460/781]  eta: 0:00:55  lr: 0.000184  training_loss: 1.4231 (1.4784)  classification_loss: 1.4225 (1.4698)  loss_mask: 0.0006 (0.0086)  time: 0.1711  data: 0.0002  max mem: 6052
[04:26:24.983149] Epoch: [37]  [480/781]  eta: 0:00:52  lr: 0.000184  training_loss: 1.4254 (1.4772)  classification_loss: 1.4249 (1.4689)  loss_mask: 0.0006 (0.0083)  time: 0.1718  data: 0.0004  max mem: 6052
[04:26:28.399060] Epoch: [37]  [500/781]  eta: 0:00:48  lr: 0.000184  training_loss: 1.4503 (1.4764)  classification_loss: 1.4495 (1.4684)  loss_mask: 0.0005 (0.0080)  time: 0.1707  data: 0.0002  max mem: 6052
[04:26:31.823025] Epoch: [37]  [520/781]  eta: 0:00:45  lr: 0.000184  training_loss: 1.4211 (1.4755)  classification_loss: 1.4206 (1.4678)  loss_mask: 0.0005 (0.0077)  time: 0.1711  data: 0.0003  max mem: 6052
[04:26:35.240283] Epoch: [37]  [540/781]  eta: 0:00:41  lr: 0.000184  training_loss: 1.4702 (1.4759)  classification_loss: 1.4699 (1.4684)  loss_mask: 0.0005 (0.0074)  time: 0.1708  data: 0.0002  max mem: 6052
[04:26:38.655088] Epoch: [37]  [560/781]  eta: 0:00:38  lr: 0.000184  training_loss: 1.4357 (1.4743)  classification_loss: 1.4352 (1.4671)  loss_mask: 0.0006 (0.0072)  time: 0.1707  data: 0.0004  max mem: 6052
[04:26:42.114115] Epoch: [37]  [580/781]  eta: 0:00:34  lr: 0.000184  training_loss: 1.4557 (1.4747)  classification_loss: 1.4551 (1.4677)  loss_mask: 0.0006 (0.0070)  time: 0.1729  data: 0.0002  max mem: 6052
[04:26:45.538228] Epoch: [37]  [600/781]  eta: 0:00:31  lr: 0.000184  training_loss: 1.4053 (1.4742)  classification_loss: 1.4047 (1.4674)  loss_mask: 0.0004 (0.0068)  time: 0.1711  data: 0.0002  max mem: 6052
[04:26:48.964519] Epoch: [37]  [620/781]  eta: 0:00:27  lr: 0.000184  training_loss: 1.4286 (1.4730)  classification_loss: 1.4278 (1.4664)  loss_mask: 0.0004 (0.0066)  time: 0.1712  data: 0.0002  max mem: 6052
[04:26:52.380963] Epoch: [37]  [640/781]  eta: 0:00:24  lr: 0.000184  training_loss: 1.5196 (1.4740)  classification_loss: 1.5052 (1.4675)  loss_mask: 0.0005 (0.0065)  time: 0.1708  data: 0.0003  max mem: 6052
[04:26:55.789797] Epoch: [37]  [660/781]  eta: 0:00:20  lr: 0.000184  training_loss: 1.5425 (1.4766)  classification_loss: 1.4939 (1.4673)  loss_mask: 0.0488 (0.0092)  time: 0.1704  data: 0.0002  max mem: 6052
[04:26:59.206146] Epoch: [37]  [680/781]  eta: 0:00:17  lr: 0.000183  training_loss: 1.5268 (1.4781)  classification_loss: 1.5121 (1.4681)  loss_mask: 0.0289 (0.0100)  time: 0.1707  data: 0.0002  max mem: 6052
[04:27:02.633967] Epoch: [37]  [700/781]  eta: 0:00:13  lr: 0.000183  training_loss: 1.5533 (1.4796)  classification_loss: 1.5158 (1.4689)  loss_mask: 0.0210 (0.0107)  time: 0.1713  data: 0.0002  max mem: 6052
[04:27:06.063258] Epoch: [37]  [720/781]  eta: 0:00:10  lr: 0.000183  training_loss: 1.4964 (1.4802)  classification_loss: 1.4684 (1.4688)  loss_mask: 0.0211 (0.0114)  time: 0.1714  data: 0.0002  max mem: 6052
[04:27:09.520285] Epoch: [37]  [740/781]  eta: 0:00:07  lr: 0.000183  training_loss: 1.5705 (1.4831)  classification_loss: 1.4842 (1.4690)  loss_mask: 0.0608 (0.0141)  time: 0.1728  data: 0.0002  max mem: 6052
[04:27:13.013539] Epoch: [37]  [760/781]  eta: 0:00:03  lr: 0.000183  training_loss: 1.5625 (1.4854)  classification_loss: 1.4847 (1.4694)  loss_mask: 0.0699 (0.0160)  time: 0.1746  data: 0.0002  max mem: 6052
[04:27:16.409584] Epoch: [37]  [780/781]  eta: 0:00:00  lr: 0.000183  training_loss: 1.4782 (1.4857)  classification_loss: 1.4201 (1.4681)  loss_mask: 0.0730 (0.0176)  time: 0.1697  data: 0.0002  max mem: 6052
[04:27:16.575252] Epoch: [37] Total time: 0:02:14 (0.1727 s / it)
[04:27:16.576312] Averaged stats: lr: 0.000183  training_loss: 1.4782 (1.4857)  classification_loss: 1.4201 (1.4681)  loss_mask: 0.0730 (0.0176)
[04:27:17.250642] Test:  [  0/157]  eta: 0:01:44  testing_loss: 0.6956 (0.6956)  acc1: 76.5625 (76.5625)  acc5: 100.0000 (100.0000)  time: 0.6687  data: 0.6386  max mem: 6052
[04:27:17.542059] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.7159 (0.7675)  acc1: 76.5625 (76.4205)  acc5: 98.4375 (98.7216)  time: 0.0867  data: 0.0583  max mem: 6052
[04:27:17.823564] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.7117 (0.7386)  acc1: 76.5625 (77.0833)  acc5: 98.4375 (98.7351)  time: 0.0283  data: 0.0002  max mem: 6052
[04:27:18.111358] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.7115 (0.7490)  acc1: 76.5625 (76.9657)  acc5: 98.4375 (98.7903)  time: 0.0284  data: 0.0002  max mem: 6052
[04:27:18.397149] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.7484 (0.7513)  acc1: 76.5625 (76.8674)  acc5: 98.4375 (98.6662)  time: 0.0285  data: 0.0002  max mem: 6052
[04:27:18.680550] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.7121 (0.7441)  acc1: 76.5625 (76.8076)  acc5: 98.4375 (98.5294)  time: 0.0282  data: 0.0002  max mem: 6052
[04:27:18.967177] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6890 (0.7451)  acc1: 75.0000 (76.3064)  acc5: 98.4375 (98.4887)  time: 0.0283  data: 0.0002  max mem: 6052
[04:27:19.248076] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7059 (0.7413)  acc1: 75.0000 (76.2544)  acc5: 98.4375 (98.4595)  time: 0.0282  data: 0.0002  max mem: 6052
[04:27:19.537325] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7356 (0.7525)  acc1: 75.0000 (75.8295)  acc5: 96.8750 (98.3025)  time: 0.0284  data: 0.0002  max mem: 6052
[04:27:19.821251] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7814 (0.7502)  acc1: 75.0000 (75.8929)  acc5: 98.4375 (98.3345)  time: 0.0285  data: 0.0002  max mem: 6052
[04:27:20.113303] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.7539 (0.7546)  acc1: 76.5625 (75.7426)  acc5: 98.4375 (98.2983)  time: 0.0287  data: 0.0001  max mem: 6052
[04:27:20.403698] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.8225 (0.7600)  acc1: 73.4375 (75.4645)  acc5: 98.4375 (98.2545)  time: 0.0290  data: 0.0002  max mem: 6052
[04:27:20.687988] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.7295 (0.7559)  acc1: 73.4375 (75.5682)  acc5: 98.4375 (98.2825)  time: 0.0286  data: 0.0002  max mem: 6052
[04:27:20.972204] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.7161 (0.7554)  acc1: 76.5625 (75.6083)  acc5: 98.4375 (98.3182)  time: 0.0283  data: 0.0002  max mem: 6052
[04:27:21.255510] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7684 (0.7551)  acc1: 76.5625 (75.6095)  acc5: 98.4375 (98.2934)  time: 0.0282  data: 0.0001  max mem: 6052
[04:27:21.533835] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7567 (0.7543)  acc1: 73.4375 (75.5484)  acc5: 98.4375 (98.2719)  time: 0.0279  data: 0.0001  max mem: 6052
[04:27:21.684131] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7506 (0.7566)  acc1: 75.0000 (75.4700)  acc5: 98.4375 (98.2700)  time: 0.0269  data: 0.0001  max mem: 6052
[04:27:21.841224] Test: Total time: 0:00:05 (0.0335 s / it)
[04:27:21.841717] * Acc@1 75.470 Acc@5 98.270 loss 0.757
[04:27:21.842019] Accuracy of the network on the 10000 test images: 75.5%
[04:27:21.842227] Max accuracy: 75.68%
[04:27:22.096045] log_dir: ./output_dir
[04:27:22.967526] Epoch: [38]  [  0/781]  eta: 0:11:19  lr: 0.000183  training_loss: 1.1000 (1.1000)  classification_loss: 1.0804 (1.0804)  loss_mask: 0.0196 (0.0196)  time: 0.8697  data: 0.6923  max mem: 6052
[04:27:26.417906] Epoch: [38]  [ 20/781]  eta: 0:02:36  lr: 0.000183  training_loss: 1.4929 (1.4792)  classification_loss: 1.4551 (1.4309)  loss_mask: 0.0379 (0.0482)  time: 0.1724  data: 0.0003  max mem: 6052
[04:27:29.826482] Epoch: [38]  [ 40/781]  eta: 0:02:19  lr: 0.000183  training_loss: 1.4887 (1.4797)  classification_loss: 1.4591 (1.4482)  loss_mask: 0.0123 (0.0316)  time: 0.1704  data: 0.0003  max mem: 6052
[04:27:33.245349] Epoch: [38]  [ 60/781]  eta: 0:02:11  lr: 0.000183  training_loss: 1.4889 (1.4848)  classification_loss: 1.4640 (1.4591)  loss_mask: 0.0108 (0.0257)  time: 0.1709  data: 0.0003  max mem: 6052
[04:27:36.645104] Epoch: [38]  [ 80/781]  eta: 0:02:05  lr: 0.000183  training_loss: 1.4270 (1.4801)  classification_loss: 1.4087 (1.4560)  loss_mask: 0.0129 (0.0242)  time: 0.1699  data: 0.0003  max mem: 6052
[04:27:40.047299] Epoch: [38]  [100/781]  eta: 0:02:00  lr: 0.000182  training_loss: 1.4696 (1.4812)  classification_loss: 1.4522 (1.4597)  loss_mask: 0.0107 (0.0215)  time: 0.1700  data: 0.0002  max mem: 6052
[04:27:43.458730] Epoch: [38]  [120/781]  eta: 0:01:56  lr: 0.000182  training_loss: 1.4304 (1.4723)  classification_loss: 1.4252 (1.4523)  loss_mask: 0.0087 (0.0199)  time: 0.1705  data: 0.0002  max mem: 6052
[04:27:46.913417] Epoch: [38]  [140/781]  eta: 0:01:52  lr: 0.000182  training_loss: 1.4281 (1.4644)  classification_loss: 1.4209 (1.4467)  loss_mask: 0.0031 (0.0177)  time: 0.1726  data: 0.0002  max mem: 6052
[04:27:50.475638] Epoch: [38]  [160/781]  eta: 0:01:49  lr: 0.000182  training_loss: 1.4352 (1.4622)  classification_loss: 1.4325 (1.4464)  loss_mask: 0.0021 (0.0158)  time: 0.1780  data: 0.0002  max mem: 6052
[04:27:53.906108] Epoch: [38]  [180/781]  eta: 0:01:45  lr: 0.000182  training_loss: 1.4056 (1.4598)  classification_loss: 1.4029 (1.4456)  loss_mask: 0.0018 (0.0143)  time: 0.1714  data: 0.0003  max mem: 6052
[04:27:57.367024] Epoch: [38]  [200/781]  eta: 0:01:41  lr: 0.000182  training_loss: 1.4471 (1.4615)  classification_loss: 1.4464 (1.4485)  loss_mask: 0.0016 (0.0130)  time: 0.1730  data: 0.0002  max mem: 6052
[04:28:00.790732] Epoch: [38]  [220/781]  eta: 0:01:38  lr: 0.000182  training_loss: 1.4396 (1.4586)  classification_loss: 1.4367 (1.4466)  loss_mask: 0.0013 (0.0120)  time: 0.1711  data: 0.0002  max mem: 6052
[04:28:04.209754] Epoch: [38]  [240/781]  eta: 0:01:34  lr: 0.000182  training_loss: 1.4480 (1.4597)  classification_loss: 1.4466 (1.4486)  loss_mask: 0.0015 (0.0111)  time: 0.1709  data: 0.0002  max mem: 6052
[04:28:07.613092] Epoch: [38]  [260/781]  eta: 0:01:30  lr: 0.000182  training_loss: 1.4051 (1.4564)  classification_loss: 1.4043 (1.4460)  loss_mask: 0.0008 (0.0104)  time: 0.1701  data: 0.0002  max mem: 6052
[04:28:11.034957] Epoch: [38]  [280/781]  eta: 0:01:27  lr: 0.000182  training_loss: 1.4965 (1.4586)  classification_loss: 1.4917 (1.4483)  loss_mask: 0.0016 (0.0103)  time: 0.1710  data: 0.0002  max mem: 6052
[04:28:14.455061] Epoch: [38]  [300/781]  eta: 0:01:23  lr: 0.000182  training_loss: 1.4611 (1.4591)  classification_loss: 1.4600 (1.4492)  loss_mask: 0.0035 (0.0100)  time: 0.1709  data: 0.0002  max mem: 6052
[04:28:17.865757] Epoch: [38]  [320/781]  eta: 0:01:20  lr: 0.000181  training_loss: 1.5464 (1.4657)  classification_loss: 1.4662 (1.4504)  loss_mask: 0.0141 (0.0153)  time: 0.1705  data: 0.0002  max mem: 6052
[04:28:21.285727] Epoch: [38]  [340/781]  eta: 0:01:16  lr: 0.000181  training_loss: 1.5175 (1.4688)  classification_loss: 1.4165 (1.4486)  loss_mask: 0.0696 (0.0202)  time: 0.1709  data: 0.0002  max mem: 6052
[04:28:24.711769] Epoch: [38]  [360/781]  eta: 0:01:12  lr: 0.000181  training_loss: 1.4842 (1.4701)  classification_loss: 1.4565 (1.4488)  loss_mask: 0.0283 (0.0213)  time: 0.1712  data: 0.0002  max mem: 6052
[04:28:28.115005] Epoch: [38]  [380/781]  eta: 0:01:09  lr: 0.000181  training_loss: 1.4548 (1.4692)  classification_loss: 1.4239 (1.4471)  loss_mask: 0.0262 (0.0221)  time: 0.1701  data: 0.0003  max mem: 6052
[04:28:31.516474] Epoch: [38]  [400/781]  eta: 0:01:05  lr: 0.000181  training_loss: 1.4412 (1.4673)  classification_loss: 1.4186 (1.4455)  loss_mask: 0.0139 (0.0217)  time: 0.1700  data: 0.0002  max mem: 6052
[04:28:34.937781] Epoch: [38]  [420/781]  eta: 0:01:02  lr: 0.000181  training_loss: 1.4241 (1.4658)  classification_loss: 1.4067 (1.4444)  loss_mask: 0.0136 (0.0215)  time: 0.1710  data: 0.0002  max mem: 6052
[04:28:38.377775] Epoch: [38]  [440/781]  eta: 0:00:58  lr: 0.000181  training_loss: 1.4860 (1.4664)  classification_loss: 1.4618 (1.4453)  loss_mask: 0.0105 (0.0211)  time: 0.1719  data: 0.0002  max mem: 6052
[04:28:41.841555] Epoch: [38]  [460/781]  eta: 0:00:55  lr: 0.000181  training_loss: 1.4533 (1.4662)  classification_loss: 1.4433 (1.4455)  loss_mask: 0.0070 (0.0206)  time: 0.1731  data: 0.0003  max mem: 6052
[04:28:45.270595] Epoch: [38]  [480/781]  eta: 0:00:52  lr: 0.000181  training_loss: 1.4617 (1.4665)  classification_loss: 1.4499 (1.4459)  loss_mask: 0.0129 (0.0206)  time: 0.1714  data: 0.0002  max mem: 6052
[04:28:48.716199] Epoch: [38]  [500/781]  eta: 0:00:48  lr: 0.000181  training_loss: 1.4389 (1.4656)  classification_loss: 1.4345 (1.4455)  loss_mask: 0.0037 (0.0201)  time: 0.1722  data: 0.0002  max mem: 6052
[04:28:52.260015] Epoch: [38]  [520/781]  eta: 0:00:45  lr: 0.000180  training_loss: 1.3863 (1.4639)  classification_loss: 1.3835 (1.4444)  loss_mask: 0.0027 (0.0194)  time: 0.1771  data: 0.0002  max mem: 6052
[04:28:55.679501] Epoch: [38]  [540/781]  eta: 0:00:41  lr: 0.000180  training_loss: 1.4778 (1.4642)  classification_loss: 1.4766 (1.4454)  loss_mask: 0.0022 (0.0188)  time: 0.1709  data: 0.0002  max mem: 6052
[04:28:59.086849] Epoch: [38]  [560/781]  eta: 0:00:38  lr: 0.000180  training_loss: 1.4505 (1.4629)  classification_loss: 1.4478 (1.4446)  loss_mask: 0.0029 (0.0182)  time: 0.1703  data: 0.0002  max mem: 6052
[04:29:02.533019] Epoch: [38]  [580/781]  eta: 0:00:34  lr: 0.000180  training_loss: 1.4632 (1.4632)  classification_loss: 1.4612 (1.4455)  loss_mask: 0.0020 (0.0177)  time: 0.1722  data: 0.0003  max mem: 6052
[04:29:05.940223] Epoch: [38]  [600/781]  eta: 0:00:31  lr: 0.000180  training_loss: 1.4741 (1.4634)  classification_loss: 1.4713 (1.4463)  loss_mask: 0.0012 (0.0171)  time: 0.1703  data: 0.0001  max mem: 6052
[04:29:09.359460] Epoch: [38]  [620/781]  eta: 0:00:27  lr: 0.000180  training_loss: 1.3792 (1.4618)  classification_loss: 1.3785 (1.4452)  loss_mask: 0.0011 (0.0166)  time: 0.1709  data: 0.0003  max mem: 6052
[04:29:12.818592] Epoch: [38]  [640/781]  eta: 0:00:24  lr: 0.000180  training_loss: 1.4735 (1.4620)  classification_loss: 1.4728 (1.4459)  loss_mask: 0.0010 (0.0161)  time: 0.1729  data: 0.0002  max mem: 6052
[04:29:16.239469] Epoch: [38]  [660/781]  eta: 0:00:20  lr: 0.000180  training_loss: 1.5179 (1.4631)  classification_loss: 1.5174 (1.4474)  loss_mask: 0.0009 (0.0157)  time: 0.1710  data: 0.0002  max mem: 6052
[04:29:19.662841] Epoch: [38]  [680/781]  eta: 0:00:17  lr: 0.000180  training_loss: 1.4798 (1.4636)  classification_loss: 1.4783 (1.4483)  loss_mask: 0.0011 (0.0153)  time: 0.1711  data: 0.0002  max mem: 6052
[04:29:23.075735] Epoch: [38]  [700/781]  eta: 0:00:13  lr: 0.000180  training_loss: 1.5130 (1.4642)  classification_loss: 1.4948 (1.4488)  loss_mask: 0.0012 (0.0154)  time: 0.1706  data: 0.0003  max mem: 6052
[04:29:26.501848] Epoch: [38]  [720/781]  eta: 0:00:10  lr: 0.000180  training_loss: 1.6412 (1.4692)  classification_loss: 1.4426 (1.4502)  loss_mask: 0.1047 (0.0189)  time: 0.1712  data: 0.0002  max mem: 6052
[04:29:29.927021] Epoch: [38]  [740/781]  eta: 0:00:07  lr: 0.000179  training_loss: 1.5790 (1.4718)  classification_loss: 1.4399 (1.4504)  loss_mask: 0.0793 (0.0214)  time: 0.1712  data: 0.0003  max mem: 6052
[04:29:33.343891] Epoch: [38]  [760/781]  eta: 0:00:03  lr: 0.000179  training_loss: 1.5740 (1.4747)  classification_loss: 1.5076 (1.4519)  loss_mask: 0.0512 (0.0228)  time: 0.1707  data: 0.0002  max mem: 6052
[04:29:36.756034] Epoch: [38]  [780/781]  eta: 0:00:00  lr: 0.000179  training_loss: 1.4785 (1.4749)  classification_loss: 1.4545 (1.4519)  loss_mask: 0.0242 (0.0230)  time: 0.1705  data: 0.0002  max mem: 6052
[04:29:36.928805] Epoch: [38] Total time: 0:02:14 (0.1726 s / it)
[04:29:36.929259] Averaged stats: lr: 0.000179  training_loss: 1.4785 (1.4749)  classification_loss: 1.4545 (1.4519)  loss_mask: 0.0242 (0.0230)
[04:29:37.595297] Test:  [  0/157]  eta: 0:01:43  testing_loss: 0.6964 (0.6964)  acc1: 78.1250 (78.1250)  acc5: 100.0000 (100.0000)  time: 0.6608  data: 0.6298  max mem: 6052
[04:29:37.880577] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.7255 (0.7307)  acc1: 75.0000 (75.7102)  acc5: 100.0000 (99.1477)  time: 0.0858  data: 0.0575  max mem: 6052
[04:29:38.162186] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.6999 (0.7110)  acc1: 78.1250 (76.7113)  acc5: 100.0000 (99.1815)  time: 0.0282  data: 0.0003  max mem: 6052
[04:29:38.444984] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.7243 (0.7294)  acc1: 76.5625 (76.2097)  acc5: 100.0000 (98.9415)  time: 0.0281  data: 0.0002  max mem: 6052
[04:29:38.736740] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.7367 (0.7321)  acc1: 75.0000 (75.8384)  acc5: 98.4375 (98.8567)  time: 0.0286  data: 0.0001  max mem: 6052
[04:29:39.020010] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6838 (0.7227)  acc1: 76.5625 (76.4706)  acc5: 98.4375 (98.7439)  time: 0.0286  data: 0.0001  max mem: 6052
[04:29:39.305433] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6943 (0.7203)  acc1: 76.5625 (76.1783)  acc5: 98.4375 (98.7961)  time: 0.0283  data: 0.0003  max mem: 6052
[04:29:39.589352] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7083 (0.7157)  acc1: 76.5625 (76.4525)  acc5: 98.4375 (98.7896)  time: 0.0284  data: 0.0003  max mem: 6052
[04:29:39.873042] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7127 (0.7243)  acc1: 76.5625 (76.0610)  acc5: 98.4375 (98.6497)  time: 0.0283  data: 0.0001  max mem: 6052
[04:29:40.155958] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7383 (0.7233)  acc1: 75.0000 (76.0130)  acc5: 98.4375 (98.6951)  time: 0.0282  data: 0.0001  max mem: 6052
[04:29:40.441841] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.7520 (0.7289)  acc1: 73.4375 (75.7426)  acc5: 98.4375 (98.6541)  time: 0.0283  data: 0.0002  max mem: 6052
[04:29:40.724934] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.7764 (0.7309)  acc1: 75.0000 (75.8587)  acc5: 100.0000 (98.6768)  time: 0.0283  data: 0.0002  max mem: 6052
[04:29:41.008937] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.7086 (0.7273)  acc1: 76.5625 (76.0589)  acc5: 98.4375 (98.6570)  time: 0.0282  data: 0.0001  max mem: 6052
[04:29:41.293988] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.7086 (0.7282)  acc1: 75.0000 (75.9900)  acc5: 98.4375 (98.6760)  time: 0.0283  data: 0.0002  max mem: 6052
[04:29:41.574132] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7562 (0.7282)  acc1: 75.0000 (76.1303)  acc5: 98.4375 (98.6702)  time: 0.0281  data: 0.0001  max mem: 6052
[04:29:41.853733] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7725 (0.7271)  acc1: 78.1250 (76.2521)  acc5: 98.4375 (98.6445)  time: 0.0279  data: 0.0001  max mem: 6052
[04:29:42.003420] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7113 (0.7285)  acc1: 76.5625 (76.2300)  acc5: 98.4375 (98.6500)  time: 0.0269  data: 0.0001  max mem: 6052
[04:29:42.164996] Test: Total time: 0:00:05 (0.0333 s / it)
[04:29:42.165620] * Acc@1 76.230 Acc@5 98.650 loss 0.729
[04:29:42.165928] Accuracy of the network on the 10000 test images: 76.2%
[04:29:42.166126] Max accuracy: 76.23%
[04:29:42.524136] log_dir: ./output_dir
[04:29:43.395656] Epoch: [39]  [  0/781]  eta: 0:11:19  lr: 0.000179  training_loss: 1.2092 (1.2092)  classification_loss: 1.1982 (1.1982)  loss_mask: 0.0110 (0.0110)  time: 0.8695  data: 0.6568  max mem: 6052
[04:29:46.828785] Epoch: [39]  [ 20/781]  eta: 0:02:35  lr: 0.000179  training_loss: 1.4040 (1.4308)  classification_loss: 1.3973 (1.4139)  loss_mask: 0.0157 (0.0169)  time: 0.1715  data: 0.0002  max mem: 6052
[04:29:50.314222] Epoch: [39]  [ 40/781]  eta: 0:02:20  lr: 0.000179  training_loss: 1.4876 (1.4561)  classification_loss: 1.4560 (1.4345)  loss_mask: 0.0133 (0.0215)  time: 0.1742  data: 0.0002  max mem: 6052
[04:29:53.726259] Epoch: [39]  [ 60/781]  eta: 0:02:12  lr: 0.000179  training_loss: 1.4450 (1.4626)  classification_loss: 1.4285 (1.4422)  loss_mask: 0.0154 (0.0204)  time: 0.1705  data: 0.0002  max mem: 6052
[04:29:57.151886] Epoch: [39]  [ 80/781]  eta: 0:02:06  lr: 0.000179  training_loss: 1.4543 (1.4610)  classification_loss: 1.4391 (1.4431)  loss_mask: 0.0098 (0.0180)  time: 0.1712  data: 0.0002  max mem: 6052
[04:30:00.574099] Epoch: [39]  [100/781]  eta: 0:02:01  lr: 0.000179  training_loss: 1.4344 (1.4603)  classification_loss: 1.4296 (1.4443)  loss_mask: 0.0077 (0.0160)  time: 0.1710  data: 0.0003  max mem: 6052
[04:30:03.991080] Epoch: [39]  [120/781]  eta: 0:01:57  lr: 0.000179  training_loss: 1.3837 (1.4501)  classification_loss: 1.3793 (1.4358)  loss_mask: 0.0048 (0.0143)  time: 0.1708  data: 0.0003  max mem: 6052
[04:30:07.406093] Epoch: [39]  [140/781]  eta: 0:01:53  lr: 0.000179  training_loss: 1.4192 (1.4459)  classification_loss: 1.4166 (1.4329)  loss_mask: 0.0045 (0.0129)  time: 0.1707  data: 0.0002  max mem: 6052
[04:30:10.834943] Epoch: [39]  [160/781]  eta: 0:01:49  lr: 0.000178  training_loss: 1.4276 (1.4414)  classification_loss: 1.4205 (1.4295)  loss_mask: 0.0039 (0.0118)  time: 0.1714  data: 0.0002  max mem: 6052
[04:30:14.251084] Epoch: [39]  [180/781]  eta: 0:01:45  lr: 0.000178  training_loss: 1.4343 (1.4388)  classification_loss: 1.4329 (1.4279)  loss_mask: 0.0026 (0.0109)  time: 0.1707  data: 0.0002  max mem: 6052
[04:30:17.673636] Epoch: [39]  [200/781]  eta: 0:01:41  lr: 0.000178  training_loss: 1.4515 (1.4408)  classification_loss: 1.4480 (1.4307)  loss_mask: 0.0029 (0.0101)  time: 0.1711  data: 0.0003  max mem: 6052
[04:30:21.129925] Epoch: [39]  [220/781]  eta: 0:01:37  lr: 0.000178  training_loss: 1.4652 (1.4437)  classification_loss: 1.4629 (1.4344)  loss_mask: 0.0017 (0.0093)  time: 0.1727  data: 0.0002  max mem: 6052
[04:30:24.602081] Epoch: [39]  [240/781]  eta: 0:01:34  lr: 0.000178  training_loss: 1.4703 (1.4463)  classification_loss: 1.4687 (1.4376)  loss_mask: 0.0015 (0.0087)  time: 0.1735  data: 0.0002  max mem: 6052
[04:30:28.054712] Epoch: [39]  [260/781]  eta: 0:01:30  lr: 0.000178  training_loss: 1.4254 (1.4467)  classification_loss: 1.4246 (1.4386)  loss_mask: 0.0012 (0.0081)  time: 0.1726  data: 0.0003  max mem: 6052
[04:30:31.506721] Epoch: [39]  [280/781]  eta: 0:01:27  lr: 0.000178  training_loss: 1.5143 (1.4493)  classification_loss: 1.5133 (1.4416)  loss_mask: 0.0010 (0.0076)  time: 0.1725  data: 0.0002  max mem: 6052
[04:30:34.935407] Epoch: [39]  [300/781]  eta: 0:01:23  lr: 0.000178  training_loss: 1.4249 (1.4473)  classification_loss: 1.4245 (1.4401)  loss_mask: 0.0010 (0.0072)  time: 0.1713  data: 0.0003  max mem: 6052
[04:30:38.355020] Epoch: [39]  [320/781]  eta: 0:01:20  lr: 0.000178  training_loss: 1.4695 (1.4480)  classification_loss: 1.4676 (1.4410)  loss_mask: 0.0011 (0.0070)  time: 0.1709  data: 0.0002  max mem: 6052
[04:30:41.749090] Epoch: [39]  [340/781]  eta: 0:01:16  lr: 0.000178  training_loss: 1.6629 (1.4645)  classification_loss: 1.4205 (1.4400)  loss_mask: 0.2286 (0.0245)  time: 0.1696  data: 0.0001  max mem: 6052
[04:30:45.142874] Epoch: [39]  [360/781]  eta: 0:01:12  lr: 0.000178  training_loss: 1.5589 (1.4716)  classification_loss: 1.4678 (1.4410)  loss_mask: 0.1191 (0.0306)  time: 0.1696  data: 0.0002  max mem: 6052
[04:30:48.558805] Epoch: [39]  [380/781]  eta: 0:01:09  lr: 0.000177  training_loss: 1.4499 (1.4725)  classification_loss: 1.3937 (1.4409)  loss_mask: 0.0432 (0.0316)  time: 0.1705  data: 0.0002  max mem: 6052
[04:30:51.972292] Epoch: [39]  [400/781]  eta: 0:01:05  lr: 0.000177  training_loss: 1.4594 (1.4732)  classification_loss: 1.4390 (1.4421)  loss_mask: 0.0166 (0.0311)  time: 0.1706  data: 0.0002  max mem: 6052
[04:30:55.410566] Epoch: [39]  [420/781]  eta: 0:01:02  lr: 0.000177  training_loss: 1.4704 (1.4732)  classification_loss: 1.4409 (1.4419)  loss_mask: 0.0299 (0.0313)  time: 0.1718  data: 0.0002  max mem: 6052
[04:30:58.814552] Epoch: [39]  [440/781]  eta: 0:00:58  lr: 0.000177  training_loss: 1.4605 (1.4736)  classification_loss: 1.4148 (1.4414)  loss_mask: 0.0401 (0.0322)  time: 0.1701  data: 0.0002  max mem: 6052
[04:31:02.234476] Epoch: [39]  [460/781]  eta: 0:00:55  lr: 0.000177  training_loss: 1.4264 (1.4729)  classification_loss: 1.4029 (1.4408)  loss_mask: 0.0299 (0.0321)  time: 0.1709  data: 0.0002  max mem: 6052
[04:31:05.664237] Epoch: [39]  [480/781]  eta: 0:00:51  lr: 0.000177  training_loss: 1.4898 (1.4746)  classification_loss: 1.4456 (1.4420)  loss_mask: 0.0267 (0.0327)  time: 0.1714  data: 0.0002  max mem: 6052
[04:31:09.062555] Epoch: [39]  [500/781]  eta: 0:00:48  lr: 0.000177  training_loss: 1.4643 (1.4747)  classification_loss: 1.4362 (1.4417)  loss_mask: 0.0302 (0.0330)  time: 0.1698  data: 0.0002  max mem: 6052
[04:31:12.518244] Epoch: [39]  [520/781]  eta: 0:00:45  lr: 0.000177  training_loss: 1.4475 (1.4734)  classification_loss: 1.4235 (1.4412)  loss_mask: 0.0119 (0.0322)  time: 0.1727  data: 0.0003  max mem: 6052
[04:31:15.926535] Epoch: [39]  [540/781]  eta: 0:00:41  lr: 0.000177  training_loss: 1.4521 (1.4725)  classification_loss: 1.4445 (1.4410)  loss_mask: 0.0104 (0.0315)  time: 0.1703  data: 0.0002  max mem: 6052
[04:31:19.323949] Epoch: [39]  [560/781]  eta: 0:00:38  lr: 0.000177  training_loss: 1.4722 (1.4722)  classification_loss: 1.4571 (1.4414)  loss_mask: 0.0097 (0.0308)  time: 0.1698  data: 0.0002  max mem: 6052
[04:31:22.746617] Epoch: [39]  [580/781]  eta: 0:00:34  lr: 0.000176  training_loss: 1.4348 (1.4713)  classification_loss: 1.4271 (1.4412)  loss_mask: 0.0069 (0.0301)  time: 0.1711  data: 0.0002  max mem: 6052
[04:31:26.163694] Epoch: [39]  [600/781]  eta: 0:00:31  lr: 0.000176  training_loss: 1.4048 (1.4690)  classification_loss: 1.3991 (1.4397)  loss_mask: 0.0046 (0.0293)  time: 0.1708  data: 0.0002  max mem: 6052
[04:31:29.681069] Epoch: [39]  [620/781]  eta: 0:00:27  lr: 0.000176  training_loss: 1.4591 (1.4690)  classification_loss: 1.4564 (1.4405)  loss_mask: 0.0048 (0.0285)  time: 0.1758  data: 0.0002  max mem: 6052
[04:31:33.215054] Epoch: [39]  [640/781]  eta: 0:00:24  lr: 0.000176  training_loss: 1.4187 (1.4675)  classification_loss: 1.4135 (1.4397)  loss_mask: 0.0028 (0.0277)  time: 0.1766  data: 0.0004  max mem: 6052
[04:31:36.719453] Epoch: [39]  [660/781]  eta: 0:00:20  lr: 0.000176  training_loss: 1.4370 (1.4673)  classification_loss: 1.4351 (1.4404)  loss_mask: 0.0034 (0.0270)  time: 0.1752  data: 0.0002  max mem: 6052
[04:31:40.276836] Epoch: [39]  [680/781]  eta: 0:00:17  lr: 0.000176  training_loss: 1.4610 (1.4675)  classification_loss: 1.4599 (1.4410)  loss_mask: 0.0047 (0.0265)  time: 0.1778  data: 0.0001  max mem: 6052
[04:31:43.696204] Epoch: [39]  [700/781]  eta: 0:00:13  lr: 0.000176  training_loss: 1.4738 (1.4675)  classification_loss: 1.4609 (1.4416)  loss_mask: 0.0041 (0.0259)  time: 0.1709  data: 0.0002  max mem: 6052
[04:31:47.120103] Epoch: [39]  [720/781]  eta: 0:00:10  lr: 0.000176  training_loss: 1.4345 (1.4672)  classification_loss: 1.4124 (1.4414)  loss_mask: 0.0088 (0.0257)  time: 0.1711  data: 0.0002  max mem: 6052
[04:31:50.530297] Epoch: [39]  [740/781]  eta: 0:00:07  lr: 0.000176  training_loss: 1.4153 (1.4662)  classification_loss: 1.3378 (1.4395)  loss_mask: 0.0311 (0.0267)  time: 0.1704  data: 0.0002  max mem: 6052
[04:31:53.938108] Epoch: [39]  [760/781]  eta: 0:00:03  lr: 0.000176  training_loss: 1.4913 (1.4669)  classification_loss: 1.4469 (1.4399)  loss_mask: 0.0235 (0.0270)  time: 0.1703  data: 0.0002  max mem: 6052
[04:31:57.346429] Epoch: [39]  [780/781]  eta: 0:00:00  lr: 0.000176  training_loss: 1.4901 (1.4675)  classification_loss: 1.4575 (1.4404)  loss_mask: 0.0141 (0.0270)  time: 0.1703  data: 0.0002  max mem: 6052
[04:31:57.525280] Epoch: [39] Total time: 0:02:15 (0.1729 s / it)
[04:31:57.525885] Averaged stats: lr: 0.000176  training_loss: 1.4901 (1.4675)  classification_loss: 1.4575 (1.4404)  loss_mask: 0.0141 (0.0270)
[04:31:58.187688] Test:  [  0/157]  eta: 0:01:42  testing_loss: 0.7064 (0.7064)  acc1: 79.6875 (79.6875)  acc5: 98.4375 (98.4375)  time: 0.6560  data: 0.6258  max mem: 6052
[04:31:58.479622] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.8064 (0.7912)  acc1: 75.0000 (74.2898)  acc5: 98.4375 (99.0057)  time: 0.0859  data: 0.0572  max mem: 6052
[04:31:58.773111] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.7748 (0.7671)  acc1: 75.0000 (75.0744)  acc5: 98.4375 (98.8095)  time: 0.0291  data: 0.0002  max mem: 6052
[04:31:59.061596] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.7748 (0.7847)  acc1: 73.4375 (74.5464)  acc5: 98.4375 (98.2863)  time: 0.0290  data: 0.0002  max mem: 6052
[04:31:59.343588] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.7839 (0.7846)  acc1: 75.0000 (74.7332)  acc5: 96.8750 (98.1326)  time: 0.0284  data: 0.0002  max mem: 6052
[04:31:59.629891] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.7391 (0.7747)  acc1: 76.5625 (74.9387)  acc5: 98.4375 (98.1924)  time: 0.0283  data: 0.0003  max mem: 6052
[04:31:59.913020] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.7184 (0.7715)  acc1: 73.4375 (74.7951)  acc5: 98.4375 (98.2070)  time: 0.0284  data: 0.0003  max mem: 6052
[04:32:00.196741] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7467 (0.7691)  acc1: 75.0000 (74.8460)  acc5: 98.4375 (98.2174)  time: 0.0282  data: 0.0001  max mem: 6052
[04:32:00.486948] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7665 (0.7773)  acc1: 73.4375 (74.6142)  acc5: 96.8750 (98.1481)  time: 0.0286  data: 0.0001  max mem: 6052
[04:32:00.768172] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7948 (0.7756)  acc1: 73.4375 (74.6566)  acc5: 98.4375 (98.1799)  time: 0.0285  data: 0.0002  max mem: 6052
[04:32:01.056689] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.7919 (0.7818)  acc1: 71.8750 (74.2265)  acc5: 98.4375 (98.1900)  time: 0.0284  data: 0.0002  max mem: 6052
[04:32:01.341465] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.8251 (0.7822)  acc1: 70.3125 (74.1554)  acc5: 98.4375 (98.1982)  time: 0.0285  data: 0.0002  max mem: 6052
[04:32:01.627375] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.7465 (0.7789)  acc1: 75.0000 (74.2898)  acc5: 98.4375 (98.2051)  time: 0.0284  data: 0.0002  max mem: 6052
[04:32:01.908767] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.7405 (0.7782)  acc1: 75.0000 (74.2366)  acc5: 98.4375 (98.2228)  time: 0.0283  data: 0.0001  max mem: 6052
[04:32:02.193000] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7573 (0.7775)  acc1: 75.0000 (74.3019)  acc5: 98.4375 (98.2491)  time: 0.0282  data: 0.0001  max mem: 6052
[04:32:02.471429] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7928 (0.7762)  acc1: 71.8750 (74.2446)  acc5: 98.4375 (98.2512)  time: 0.0280  data: 0.0001  max mem: 6052
[04:32:02.620425] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7948 (0.7774)  acc1: 71.8750 (74.0800)  acc5: 98.4375 (98.2900)  time: 0.0268  data: 0.0001  max mem: 6052
[04:32:02.796057] Test: Total time: 0:00:05 (0.0335 s / it)
[04:32:02.796485] * Acc@1 74.080 Acc@5 98.290 loss 0.777
[04:32:02.796816] Accuracy of the network on the 10000 test images: 74.1%
[04:32:02.796987] Max accuracy: 76.23%
[04:32:03.199338] log_dir: ./output_dir
[04:32:04.048401] Epoch: [40]  [  0/781]  eta: 0:11:01  lr: 0.000176  training_loss: 1.3203 (1.3203)  classification_loss: 1.3003 (1.3003)  loss_mask: 0.0200 (0.0200)  time: 0.8474  data: 0.6690  max mem: 6052
[04:32:07.470632] Epoch: [40]  [ 20/781]  eta: 0:02:34  lr: 0.000175  training_loss: 1.3855 (1.4082)  classification_loss: 1.3733 (1.3916)  loss_mask: 0.0122 (0.0166)  time: 0.1710  data: 0.0002  max mem: 6052
[04:32:10.901846] Epoch: [40]  [ 40/781]  eta: 0:02:19  lr: 0.000175  training_loss: 1.4671 (1.4427)  classification_loss: 1.4602 (1.4242)  loss_mask: 0.0092 (0.0185)  time: 0.1715  data: 0.0002  max mem: 6052
[04:32:14.332748] Epoch: [40]  [ 60/781]  eta: 0:02:11  lr: 0.000175  training_loss: 1.4889 (1.4530)  classification_loss: 1.4153 (1.4265)  loss_mask: 0.0285 (0.0265)  time: 0.1715  data: 0.0003  max mem: 6052
[04:32:17.755556] Epoch: [40]  [ 80/781]  eta: 0:02:05  lr: 0.000175  training_loss: 1.5421 (1.4820)  classification_loss: 1.4355 (1.4264)  loss_mask: 0.1330 (0.0556)  time: 0.1711  data: 0.0002  max mem: 6052
[04:32:21.172080] Epoch: [40]  [100/781]  eta: 0:02:01  lr: 0.000175  training_loss: 1.5513 (1.4988)  classification_loss: 1.4699 (1.4351)  loss_mask: 0.0814 (0.0637)  time: 0.1708  data: 0.0002  max mem: 6052
[04:32:24.648732] Epoch: [40]  [120/781]  eta: 0:01:57  lr: 0.000175  training_loss: 1.4730 (1.4947)  classification_loss: 1.4155 (1.4330)  loss_mask: 0.0358 (0.0617)  time: 0.1737  data: 0.0003  max mem: 6052
[04:32:28.061513] Epoch: [40]  [140/781]  eta: 0:01:52  lr: 0.000175  training_loss: 1.4526 (1.4881)  classification_loss: 1.4202 (1.4306)  loss_mask: 0.0282 (0.0575)  time: 0.1706  data: 0.0002  max mem: 6052
[04:32:31.478734] Epoch: [40]  [160/781]  eta: 0:01:49  lr: 0.000175  training_loss: 1.4422 (1.4801)  classification_loss: 1.4154 (1.4281)  loss_mask: 0.0108 (0.0520)  time: 0.1707  data: 0.0001  max mem: 6052
[04:32:34.894675] Epoch: [40]  [180/781]  eta: 0:01:45  lr: 0.000175  training_loss: 1.4014 (1.4762)  classification_loss: 1.3955 (1.4291)  loss_mask: 0.0070 (0.0471)  time: 0.1707  data: 0.0001  max mem: 6052
[04:32:38.290093] Epoch: [40]  [200/781]  eta: 0:01:41  lr: 0.000175  training_loss: 1.4195 (1.4739)  classification_loss: 1.4158 (1.4308)  loss_mask: 0.0057 (0.0430)  time: 0.1697  data: 0.0002  max mem: 6052
[04:32:41.691385] Epoch: [40]  [220/781]  eta: 0:01:37  lr: 0.000174  training_loss: 1.3647 (1.4687)  classification_loss: 1.3545 (1.4290)  loss_mask: 0.0054 (0.0396)  time: 0.1700  data: 0.0002  max mem: 6052
[04:32:45.125115] Epoch: [40]  [240/781]  eta: 0:01:34  lr: 0.000174  training_loss: 1.3972 (1.4669)  classification_loss: 1.3961 (1.4302)  loss_mask: 0.0037 (0.0367)  time: 0.1716  data: 0.0002  max mem: 6052
[04:32:48.545387] Epoch: [40]  [260/781]  eta: 0:01:30  lr: 0.000174  training_loss: 1.4413 (1.4667)  classification_loss: 1.4379 (1.4325)  loss_mask: 0.0033 (0.0342)  time: 0.1709  data: 0.0003  max mem: 6052
[04:32:51.983068] Epoch: [40]  [280/781]  eta: 0:01:26  lr: 0.000174  training_loss: 1.4342 (1.4635)  classification_loss: 1.4313 (1.4315)  loss_mask: 0.0032 (0.0320)  time: 0.1718  data: 0.0003  max mem: 6052
[04:32:55.491716] Epoch: [40]  [300/781]  eta: 0:01:23  lr: 0.000174  training_loss: 1.4852 (1.4659)  classification_loss: 1.4805 (1.4359)  loss_mask: 0.0027 (0.0301)  time: 0.1754  data: 0.0002  max mem: 6052
[04:32:58.947578] Epoch: [40]  [320/781]  eta: 0:01:20  lr: 0.000174  training_loss: 1.4027 (1.4639)  classification_loss: 1.4002 (1.4355)  loss_mask: 0.0020 (0.0283)  time: 0.1727  data: 0.0002  max mem: 6052
[04:33:02.404491] Epoch: [40]  [340/781]  eta: 0:01:16  lr: 0.000174  training_loss: 1.3826 (1.4602)  classification_loss: 1.3803 (1.4334)  loss_mask: 0.0016 (0.0268)  time: 0.1728  data: 0.0002  max mem: 6052
[04:33:05.817932] Epoch: [40]  [360/781]  eta: 0:01:12  lr: 0.000174  training_loss: 1.4470 (1.4590)  classification_loss: 1.4444 (1.4335)  loss_mask: 0.0020 (0.0255)  time: 0.1706  data: 0.0002  max mem: 6052
[04:33:09.211790] Epoch: [40]  [380/781]  eta: 0:01:09  lr: 0.000174  training_loss: 1.4015 (1.4583)  classification_loss: 1.3936 (1.4336)  loss_mask: 0.0057 (0.0247)  time: 0.1696  data: 0.0002  max mem: 6052
[04:33:12.613485] Epoch: [40]  [400/781]  eta: 0:01:05  lr: 0.000174  training_loss: 1.4363 (1.4580)  classification_loss: 1.4287 (1.4343)  loss_mask: 0.0025 (0.0237)  time: 0.1700  data: 0.0002  max mem: 6052
[04:33:16.033608] Epoch: [40]  [420/781]  eta: 0:01:02  lr: 0.000173  training_loss: 1.4212 (1.4567)  classification_loss: 1.4186 (1.4340)  loss_mask: 0.0020 (0.0227)  time: 0.1709  data: 0.0003  max mem: 6052
[04:33:19.470008] Epoch: [40]  [440/781]  eta: 0:00:58  lr: 0.000173  training_loss: 1.4088 (1.4558)  classification_loss: 1.4080 (1.4341)  loss_mask: 0.0015 (0.0217)  time: 0.1717  data: 0.0002  max mem: 6052
[04:33:22.882170] Epoch: [40]  [460/781]  eta: 0:00:55  lr: 0.000173  training_loss: 1.4258 (1.4539)  classification_loss: 1.4250 (1.4330)  loss_mask: 0.0013 (0.0209)  time: 0.1705  data: 0.0002  max mem: 6052
[04:33:26.307725] Epoch: [40]  [480/781]  eta: 0:00:51  lr: 0.000173  training_loss: 1.4198 (1.4524)  classification_loss: 1.4188 (1.4324)  loss_mask: 0.0010 (0.0200)  time: 0.1712  data: 0.0002  max mem: 6052
[04:33:29.737876] Epoch: [40]  [500/781]  eta: 0:00:48  lr: 0.000173  training_loss: 1.4060 (1.4515)  classification_loss: 1.4041 (1.4322)  loss_mask: 0.0008 (0.0193)  time: 0.1714  data: 0.0002  max mem: 6052
[04:33:33.160396] Epoch: [40]  [520/781]  eta: 0:00:45  lr: 0.000173  training_loss: 1.4247 (1.4511)  classification_loss: 1.4224 (1.4325)  loss_mask: 0.0012 (0.0186)  time: 0.1711  data: 0.0003  max mem: 6052
[04:33:36.574763] Epoch: [40]  [540/781]  eta: 0:00:41  lr: 0.000173  training_loss: 1.4160 (1.4500)  classification_loss: 1.4025 (1.4319)  loss_mask: 0.0008 (0.0181)  time: 0.1706  data: 0.0003  max mem: 6052
[04:33:39.985546] Epoch: [40]  [560/781]  eta: 0:00:38  lr: 0.000173  training_loss: 1.4329 (1.4495)  classification_loss: 1.4151 (1.4312)  loss_mask: 0.0063 (0.0183)  time: 0.1704  data: 0.0002  max mem: 6052
[04:33:43.466115] Epoch: [40]  [580/781]  eta: 0:00:34  lr: 0.000173  training_loss: 1.5326 (1.4533)  classification_loss: 1.4185 (1.4308)  loss_mask: 0.1062 (0.0224)  time: 0.1739  data: 0.0002  max mem: 6052
[04:33:46.879680] Epoch: [40]  [600/781]  eta: 0:00:31  lr: 0.000173  training_loss: 1.4906 (1.4552)  classification_loss: 1.4318 (1.4309)  loss_mask: 0.0692 (0.0243)  time: 0.1706  data: 0.0002  max mem: 6052
[04:33:50.316170] Epoch: [40]  [620/781]  eta: 0:00:27  lr: 0.000173  training_loss: 1.4595 (1.4559)  classification_loss: 1.4228 (1.4307)  loss_mask: 0.0391 (0.0252)  time: 0.1717  data: 0.0002  max mem: 6052
[04:33:53.791706] Epoch: [40]  [640/781]  eta: 0:00:24  lr: 0.000172  training_loss: 1.4562 (1.4559)  classification_loss: 1.4298 (1.4305)  loss_mask: 0.0169 (0.0254)  time: 0.1737  data: 0.0002  max mem: 6052
[04:33:57.275789] Epoch: [40]  [660/781]  eta: 0:00:20  lr: 0.000172  training_loss: 1.4695 (1.4568)  classification_loss: 1.4351 (1.4306)  loss_mask: 0.0419 (0.0262)  time: 0.1741  data: 0.0002  max mem: 6052
[04:34:00.689361] Epoch: [40]  [680/781]  eta: 0:00:17  lr: 0.000172  training_loss: 1.4959 (1.4581)  classification_loss: 1.4635 (1.4316)  loss_mask: 0.0222 (0.0265)  time: 0.1706  data: 0.0001  max mem: 6052
[04:34:04.105043] Epoch: [40]  [700/781]  eta: 0:00:13  lr: 0.000172  training_loss: 1.4703 (1.4587)  classification_loss: 1.4627 (1.4325)  loss_mask: 0.0164 (0.0262)  time: 0.1707  data: 0.0001  max mem: 6052
[04:34:07.525636] Epoch: [40]  [720/781]  eta: 0:00:10  lr: 0.000172  training_loss: 1.3971 (1.4579)  classification_loss: 1.3925 (1.4322)  loss_mask: 0.0064 (0.0257)  time: 0.1709  data: 0.0002  max mem: 6052
[04:34:10.950381] Epoch: [40]  [740/781]  eta: 0:00:07  lr: 0.000172  training_loss: 1.3888 (1.4568)  classification_loss: 1.3842 (1.4317)  loss_mask: 0.0044 (0.0252)  time: 0.1711  data: 0.0002  max mem: 6052
[04:34:14.369625] Epoch: [40]  [760/781]  eta: 0:00:03  lr: 0.000172  training_loss: 1.4238 (1.4570)  classification_loss: 1.4202 (1.4323)  loss_mask: 0.0035 (0.0247)  time: 0.1709  data: 0.0003  max mem: 6052
[04:34:17.775137] Epoch: [40]  [780/781]  eta: 0:00:00  lr: 0.000172  training_loss: 1.4273 (1.4564)  classification_loss: 1.4136 (1.4321)  loss_mask: 0.0036 (0.0242)  time: 0.1701  data: 0.0003  max mem: 6052
[04:34:17.923340] Epoch: [40] Total time: 0:02:14 (0.1725 s / it)
[04:34:17.924066] Averaged stats: lr: 0.000172  training_loss: 1.4273 (1.4564)  classification_loss: 1.4136 (1.4321)  loss_mask: 0.0036 (0.0242)
[04:34:19.268686] Test:  [  0/157]  eta: 0:01:41  testing_loss: 0.6929 (0.6929)  acc1: 79.6875 (79.6875)  acc5: 100.0000 (100.0000)  time: 0.6490  data: 0.6178  max mem: 6052
[04:34:19.561059] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.7278 (0.7454)  acc1: 76.5625 (75.1420)  acc5: 98.4375 (99.0057)  time: 0.0854  data: 0.0566  max mem: 6052
[04:34:19.844539] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.6881 (0.7246)  acc1: 76.5625 (75.7440)  acc5: 98.4375 (98.8839)  time: 0.0286  data: 0.0004  max mem: 6052
[04:34:20.132150] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.7048 (0.7404)  acc1: 76.5625 (75.6552)  acc5: 98.4375 (98.8407)  time: 0.0284  data: 0.0004  max mem: 6052
[04:34:20.420875] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.7397 (0.7377)  acc1: 76.5625 (75.9527)  acc5: 98.4375 (98.7424)  time: 0.0286  data: 0.0004  max mem: 6052
[04:34:20.705079] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6986 (0.7324)  acc1: 76.5625 (76.1336)  acc5: 98.4375 (98.7439)  time: 0.0285  data: 0.0005  max mem: 6052
[04:34:20.985323] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.7140 (0.7293)  acc1: 76.5625 (76.0246)  acc5: 100.0000 (98.7961)  time: 0.0281  data: 0.0003  max mem: 6052
[04:34:21.276731] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7186 (0.7273)  acc1: 78.1250 (76.2104)  acc5: 98.4375 (98.7016)  time: 0.0285  data: 0.0004  max mem: 6052
[04:34:21.557916] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7287 (0.7312)  acc1: 76.5625 (76.1767)  acc5: 98.4375 (98.5725)  time: 0.0285  data: 0.0005  max mem: 6052
[04:34:21.852902] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7287 (0.7314)  acc1: 76.5625 (76.3049)  acc5: 98.4375 (98.5749)  time: 0.0287  data: 0.0002  max mem: 6052
[04:34:22.132572] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.7373 (0.7367)  acc1: 75.0000 (75.9592)  acc5: 98.4375 (98.5303)  time: 0.0286  data: 0.0001  max mem: 6052
[04:34:22.412070] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.7849 (0.7392)  acc1: 73.4375 (75.9572)  acc5: 98.4375 (98.4797)  time: 0.0278  data: 0.0001  max mem: 6052
[04:34:22.691531] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.7337 (0.7357)  acc1: 76.5625 (76.0589)  acc5: 98.4375 (98.4892)  time: 0.0278  data: 0.0001  max mem: 6052
[04:34:22.972338] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.7232 (0.7347)  acc1: 76.5625 (76.1450)  acc5: 98.4375 (98.4971)  time: 0.0279  data: 0.0001  max mem: 6052
[04:34:23.253107] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7248 (0.7317)  acc1: 76.5625 (76.2965)  acc5: 98.4375 (98.5151)  time: 0.0280  data: 0.0001  max mem: 6052
[04:34:23.530981] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7255 (0.7315)  acc1: 76.5625 (76.2728)  acc5: 98.4375 (98.4892)  time: 0.0278  data: 0.0001  max mem: 6052
[04:34:23.680450] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7156 (0.7315)  acc1: 76.5625 (76.3200)  acc5: 98.4375 (98.5000)  time: 0.0268  data: 0.0001  max mem: 6052
[04:34:23.842840] Test: Total time: 0:00:05 (0.0333 s / it)
[04:34:23.843393] * Acc@1 76.320 Acc@5 98.500 loss 0.731
[04:34:23.843692] Accuracy of the network on the 10000 test images: 76.3%
[04:34:23.843865] Max accuracy: 76.32%
[04:34:24.119795] log_dir: ./output_dir
[04:34:24.966726] Epoch: [41]  [  0/781]  eta: 0:11:00  lr: 0.000172  training_loss: 1.3319 (1.3319)  classification_loss: 1.3294 (1.3294)  loss_mask: 0.0025 (0.0025)  time: 0.8453  data: 0.6520  max mem: 6052
[04:34:28.374333] Epoch: [41]  [ 20/781]  eta: 0:02:34  lr: 0.000172  training_loss: 1.4069 (1.3983)  classification_loss: 1.4018 (1.3953)  loss_mask: 0.0027 (0.0030)  time: 0.1703  data: 0.0002  max mem: 6052
[04:34:31.804548] Epoch: [41]  [ 40/781]  eta: 0:02:18  lr: 0.000172  training_loss: 1.4595 (1.4256)  classification_loss: 1.4548 (1.4216)  loss_mask: 0.0039 (0.0040)  time: 0.1714  data: 0.0002  max mem: 6052
[04:34:35.228208] Epoch: [41]  [ 60/781]  eta: 0:02:11  lr: 0.000171  training_loss: 1.4492 (1.4339)  classification_loss: 1.4461 (1.4291)  loss_mask: 0.0024 (0.0048)  time: 0.1711  data: 0.0002  max mem: 6052
[04:34:38.672488] Epoch: [41]  [ 80/781]  eta: 0:02:05  lr: 0.000171  training_loss: 1.4763 (1.4440)  classification_loss: 1.4271 (1.4279)  loss_mask: 0.0294 (0.0161)  time: 0.1721  data: 0.0002  max mem: 6052
[04:34:42.086461] Epoch: [41]  [100/781]  eta: 0:02:01  lr: 0.000171  training_loss: 1.4414 (1.4482)  classification_loss: 1.3909 (1.4275)  loss_mask: 0.0267 (0.0207)  time: 0.1706  data: 0.0002  max mem: 6052
[04:34:45.522146] Epoch: [41]  [120/781]  eta: 0:01:56  lr: 0.000171  training_loss: 1.3779 (1.4434)  classification_loss: 1.3698 (1.4247)  loss_mask: 0.0058 (0.0187)  time: 0.1717  data: 0.0002  max mem: 6052
[04:34:48.966461] Epoch: [41]  [140/781]  eta: 0:01:52  lr: 0.000171  training_loss: 1.4228 (1.4398)  classification_loss: 1.4194 (1.4232)  loss_mask: 0.0031 (0.0167)  time: 0.1721  data: 0.0002  max mem: 6052
[04:34:52.385296] Epoch: [41]  [160/781]  eta: 0:01:48  lr: 0.000171  training_loss: 1.4384 (1.4378)  classification_loss: 1.4369 (1.4229)  loss_mask: 0.0018 (0.0149)  time: 0.1709  data: 0.0002  max mem: 6052
[04:34:55.791286] Epoch: [41]  [180/781]  eta: 0:01:45  lr: 0.000171  training_loss: 1.3306 (1.4290)  classification_loss: 1.3290 (1.4156)  loss_mask: 0.0017 (0.0134)  time: 0.1702  data: 0.0002  max mem: 6052
[04:34:59.224809] Epoch: [41]  [200/781]  eta: 0:01:41  lr: 0.000171  training_loss: 1.4814 (1.4328)  classification_loss: 1.4799 (1.4205)  loss_mask: 0.0018 (0.0123)  time: 0.1716  data: 0.0003  max mem: 6052
[04:35:02.643456] Epoch: [41]  [220/781]  eta: 0:01:37  lr: 0.000171  training_loss: 1.4754 (1.4333)  classification_loss: 1.4728 (1.4219)  loss_mask: 0.0024 (0.0115)  time: 0.1708  data: 0.0002  max mem: 6052
[04:35:06.048604] Epoch: [41]  [240/781]  eta: 0:01:34  lr: 0.000171  training_loss: 1.3647 (1.4316)  classification_loss: 1.3630 (1.4209)  loss_mask: 0.0015 (0.0107)  time: 0.1702  data: 0.0002  max mem: 6052
[04:35:09.447730] Epoch: [41]  [260/781]  eta: 0:01:30  lr: 0.000170  training_loss: 1.3868 (1.4295)  classification_loss: 1.3857 (1.4195)  loss_mask: 0.0015 (0.0100)  time: 0.1699  data: 0.0002  max mem: 6052
[04:35:12.851816] Epoch: [41]  [280/781]  eta: 0:01:26  lr: 0.000170  training_loss: 1.4365 (1.4291)  classification_loss: 1.4352 (1.4197)  loss_mask: 0.0009 (0.0093)  time: 0.1701  data: 0.0002  max mem: 6052
[04:35:16.252674] Epoch: [41]  [300/781]  eta: 0:01:23  lr: 0.000170  training_loss: 1.3919 (1.4280)  classification_loss: 1.3908 (1.4192)  loss_mask: 0.0011 (0.0088)  time: 0.1700  data: 0.0003  max mem: 6052
[04:35:19.706067] Epoch: [41]  [320/781]  eta: 0:01:19  lr: 0.000170  training_loss: 1.4253 (1.4282)  classification_loss: 1.4242 (1.4199)  loss_mask: 0.0008 (0.0083)  time: 0.1726  data: 0.0002  max mem: 6052
[04:35:23.109716] Epoch: [41]  [340/781]  eta: 0:01:16  lr: 0.000170  training_loss: 1.4176 (1.4287)  classification_loss: 1.4171 (1.4208)  loss_mask: 0.0008 (0.0079)  time: 0.1701  data: 0.0002  max mem: 6052
[04:35:26.501425] Epoch: [41]  [360/781]  eta: 0:01:12  lr: 0.000170  training_loss: 1.4105 (1.4299)  classification_loss: 1.4079 (1.4224)  loss_mask: 0.0011 (0.0075)  time: 0.1695  data: 0.0003  max mem: 6052
[04:35:29.919837] Epoch: [41]  [380/781]  eta: 0:01:09  lr: 0.000170  training_loss: 1.4206 (1.4298)  classification_loss: 1.4195 (1.4227)  loss_mask: 0.0008 (0.0072)  time: 0.1708  data: 0.0004  max mem: 6052
[04:35:33.318812] Epoch: [41]  [400/781]  eta: 0:01:05  lr: 0.000170  training_loss: 1.4440 (1.4303)  classification_loss: 1.4384 (1.4234)  loss_mask: 0.0010 (0.0069)  time: 0.1699  data: 0.0002  max mem: 6052
[04:35:36.716366] Epoch: [41]  [420/781]  eta: 0:01:02  lr: 0.000170  training_loss: 1.4329 (1.4307)  classification_loss: 1.4325 (1.4240)  loss_mask: 0.0009 (0.0066)  time: 0.1698  data: 0.0004  max mem: 6052
[04:35:40.122203] Epoch: [41]  [440/781]  eta: 0:00:58  lr: 0.000170  training_loss: 1.5651 (1.4439)  classification_loss: 1.4246 (1.4243)  loss_mask: 0.1282 (0.0196)  time: 0.1702  data: 0.0003  max mem: 6052
[04:35:43.536792] Epoch: [41]  [460/781]  eta: 0:00:55  lr: 0.000169  training_loss: 1.5400 (1.4528)  classification_loss: 1.3513 (1.4226)  loss_mask: 0.1657 (0.0301)  time: 0.1707  data: 0.0003  max mem: 6052
[04:35:46.921186] Epoch: [41]  [480/781]  eta: 0:00:51  lr: 0.000169  training_loss: 1.5042 (1.4552)  classification_loss: 1.4391 (1.4235)  loss_mask: 0.0580 (0.0318)  time: 0.1691  data: 0.0003  max mem: 6052
[04:35:50.331229] Epoch: [41]  [500/781]  eta: 0:00:48  lr: 0.000169  training_loss: 1.4842 (1.4562)  classification_loss: 1.4364 (1.4239)  loss_mask: 0.0407 (0.0322)  time: 0.1704  data: 0.0002  max mem: 6052
[04:35:53.771074] Epoch: [41]  [520/781]  eta: 0:00:44  lr: 0.000169  training_loss: 1.4293 (1.4557)  classification_loss: 1.4104 (1.4235)  loss_mask: 0.0334 (0.0322)  time: 0.1719  data: 0.0003  max mem: 6052
[04:35:57.174342] Epoch: [41]  [540/781]  eta: 0:00:41  lr: 0.000169  training_loss: 1.4908 (1.4565)  classification_loss: 1.4566 (1.4245)  loss_mask: 0.0260 (0.0321)  time: 0.1701  data: 0.0003  max mem: 6052
[04:36:00.573181] Epoch: [41]  [560/781]  eta: 0:00:37  lr: 0.000169  training_loss: 1.4668 (1.4568)  classification_loss: 1.4441 (1.4251)  loss_mask: 0.0194 (0.0316)  time: 0.1699  data: 0.0002  max mem: 6052
[04:36:04.003480] Epoch: [41]  [580/781]  eta: 0:00:34  lr: 0.000169  training_loss: 1.4311 (1.4564)  classification_loss: 1.4228 (1.4252)  loss_mask: 0.0170 (0.0312)  time: 0.1714  data: 0.0003  max mem: 6052
[04:36:07.411716] Epoch: [41]  [600/781]  eta: 0:00:31  lr: 0.000169  training_loss: 1.4363 (1.4558)  classification_loss: 1.4138 (1.4247)  loss_mask: 0.0116 (0.0310)  time: 0.1703  data: 0.0002  max mem: 6052
[04:36:10.828495] Epoch: [41]  [620/781]  eta: 0:00:27  lr: 0.000169  training_loss: 1.4298 (1.4550)  classification_loss: 1.4080 (1.4245)  loss_mask: 0.0140 (0.0305)  time: 0.1708  data: 0.0002  max mem: 6052
[04:36:14.296408] Epoch: [41]  [640/781]  eta: 0:00:24  lr: 0.000169  training_loss: 1.4146 (1.4536)  classification_loss: 1.4011 (1.4237)  loss_mask: 0.0113 (0.0299)  time: 0.1733  data: 0.0003  max mem: 6052
[04:36:17.796534] Epoch: [41]  [660/781]  eta: 0:00:20  lr: 0.000168  training_loss: 1.4697 (1.4530)  classification_loss: 1.4659 (1.4238)  loss_mask: 0.0066 (0.0292)  time: 0.1749  data: 0.0002  max mem: 6052
[04:36:21.248184] Epoch: [41]  [680/781]  eta: 0:00:17  lr: 0.000168  training_loss: 1.4560 (1.4534)  classification_loss: 1.4517 (1.4247)  loss_mask: 0.0085 (0.0287)  time: 0.1725  data: 0.0002  max mem: 6052
[04:36:24.678046] Epoch: [41]  [700/781]  eta: 0:00:13  lr: 0.000168  training_loss: 1.4369 (1.4527)  classification_loss: 1.4309 (1.4246)  loss_mask: 0.0044 (0.0280)  time: 0.1714  data: 0.0002  max mem: 6052
[04:36:28.102861] Epoch: [41]  [720/781]  eta: 0:00:10  lr: 0.000168  training_loss: 1.4235 (1.4520)  classification_loss: 1.4165 (1.4243)  loss_mask: 0.0053 (0.0277)  time: 0.1711  data: 0.0002  max mem: 6052
[04:36:31.525427] Epoch: [41]  [740/781]  eta: 0:00:07  lr: 0.000168  training_loss: 1.3811 (1.4511)  classification_loss: 1.3721 (1.4239)  loss_mask: 0.0065 (0.0272)  time: 0.1711  data: 0.0002  max mem: 6052
[04:36:34.944401] Epoch: [41]  [760/781]  eta: 0:00:03  lr: 0.000168  training_loss: 1.4397 (1.4510)  classification_loss: 1.4187 (1.4243)  loss_mask: 0.0048 (0.0267)  time: 0.1709  data: 0.0002  max mem: 6052
[04:36:38.330278] Epoch: [41]  [780/781]  eta: 0:00:00  lr: 0.000168  training_loss: 1.4271 (1.4512)  classification_loss: 1.4216 (1.4249)  loss_mask: 0.0064 (0.0263)  time: 0.1692  data: 0.0001  max mem: 6052
[04:36:38.513347] Epoch: [41] Total time: 0:02:14 (0.1721 s / it)
[04:36:38.513859] Averaged stats: lr: 0.000168  training_loss: 1.4271 (1.4512)  classification_loss: 1.4216 (1.4249)  loss_mask: 0.0064 (0.0263)
[04:36:39.197995] Test:  [  0/157]  eta: 0:01:46  testing_loss: 0.6528 (0.6528)  acc1: 79.6875 (79.6875)  acc5: 100.0000 (100.0000)  time: 0.6792  data: 0.6481  max mem: 6052
[04:36:39.480744] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.6986 (0.7256)  acc1: 76.5625 (76.4205)  acc5: 100.0000 (99.1477)  time: 0.0872  data: 0.0591  max mem: 6052
[04:36:39.760416] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.6733 (0.7005)  acc1: 76.5625 (77.6042)  acc5: 98.4375 (99.0327)  time: 0.0279  data: 0.0002  max mem: 6052
[04:36:40.040024] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6856 (0.7164)  acc1: 76.5625 (76.9153)  acc5: 98.4375 (98.8911)  time: 0.0278  data: 0.0002  max mem: 6052
[04:36:40.319517] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.7003 (0.7209)  acc1: 76.5625 (76.4863)  acc5: 98.4375 (98.7805)  time: 0.0278  data: 0.0002  max mem: 6052
[04:36:40.600545] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6784 (0.7167)  acc1: 76.5625 (76.6544)  acc5: 98.4375 (98.7132)  time: 0.0279  data: 0.0002  max mem: 6052
[04:36:40.887976] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6859 (0.7151)  acc1: 76.5625 (76.5113)  acc5: 98.4375 (98.7193)  time: 0.0283  data: 0.0002  max mem: 6052
[04:36:41.169122] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6898 (0.7084)  acc1: 76.5625 (76.7165)  acc5: 98.4375 (98.7676)  time: 0.0283  data: 0.0002  max mem: 6052
[04:36:41.451996] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7076 (0.7164)  acc1: 75.0000 (76.3889)  acc5: 98.4375 (98.6497)  time: 0.0281  data: 0.0002  max mem: 6052
[04:36:41.735374] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7148 (0.7150)  acc1: 75.0000 (76.4080)  acc5: 98.4375 (98.7294)  time: 0.0282  data: 0.0002  max mem: 6052
[04:36:42.021487] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.7148 (0.7220)  acc1: 76.5625 (76.0829)  acc5: 100.0000 (98.6850)  time: 0.0284  data: 0.0002  max mem: 6052
[04:36:42.302618] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.7377 (0.7224)  acc1: 76.5625 (76.1402)  acc5: 98.4375 (98.6205)  time: 0.0282  data: 0.0002  max mem: 6052
[04:36:42.584013] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.7198 (0.7172)  acc1: 76.5625 (76.3171)  acc5: 98.4375 (98.6829)  time: 0.0280  data: 0.0002  max mem: 6052
[04:36:42.864838] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6473 (0.7171)  acc1: 76.5625 (76.3955)  acc5: 98.4375 (98.6522)  time: 0.0280  data: 0.0002  max mem: 6052
[04:36:43.143982] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7490 (0.7172)  acc1: 76.5625 (76.4849)  acc5: 98.4375 (98.6370)  time: 0.0279  data: 0.0001  max mem: 6052
[04:36:43.421420] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7050 (0.7169)  acc1: 76.5625 (76.5211)  acc5: 98.4375 (98.5824)  time: 0.0277  data: 0.0001  max mem: 6052
[04:36:43.572480] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7119 (0.7184)  acc1: 75.0000 (76.4800)  acc5: 98.4375 (98.5900)  time: 0.0269  data: 0.0001  max mem: 6052
[04:36:43.735405] Test: Total time: 0:00:05 (0.0332 s / it)
[04:36:43.735898] * Acc@1 76.480 Acc@5 98.590 loss 0.718
[04:36:43.736193] Accuracy of the network on the 10000 test images: 76.5%
[04:36:43.736368] Max accuracy: 76.48%
[04:36:44.185416] log_dir: ./output_dir
[04:36:45.074255] Epoch: [42]  [  0/781]  eta: 0:11:32  lr: 0.000168  training_loss: 1.4285 (1.4285)  classification_loss: 1.4226 (1.4226)  loss_mask: 0.0059 (0.0059)  time: 0.8868  data: 0.6979  max mem: 6052
[04:36:48.516097] Epoch: [42]  [ 20/781]  eta: 0:02:36  lr: 0.000168  training_loss: 1.4036 (1.4517)  classification_loss: 1.3912 (1.4405)  loss_mask: 0.0044 (0.0112)  time: 0.1720  data: 0.0002  max mem: 6052
[04:36:51.971715] Epoch: [42]  [ 40/781]  eta: 0:02:20  lr: 0.000168  training_loss: 1.4965 (1.4785)  classification_loss: 1.4331 (1.4397)  loss_mask: 0.0445 (0.0388)  time: 0.1727  data: 0.0002  max mem: 6052
[04:36:55.395949] Epoch: [42]  [ 60/781]  eta: 0:02:12  lr: 0.000168  training_loss: 1.4929 (1.4824)  classification_loss: 1.4468 (1.4415)  loss_mask: 0.0386 (0.0410)  time: 0.1711  data: 0.0003  max mem: 6052
[04:36:58.836157] Epoch: [42]  [ 80/781]  eta: 0:02:06  lr: 0.000167  training_loss: 1.3868 (1.4583)  classification_loss: 1.3780 (1.4248)  loss_mask: 0.0091 (0.0334)  time: 0.1719  data: 0.0003  max mem: 6052
[04:37:02.257909] Epoch: [42]  [100/781]  eta: 0:02:01  lr: 0.000167  training_loss: 1.4940 (1.4651)  classification_loss: 1.4856 (1.4369)  loss_mask: 0.0056 (0.0282)  time: 0.1710  data: 0.0002  max mem: 6052
[04:37:05.679473] Epoch: [42]  [120/781]  eta: 0:01:57  lr: 0.000167  training_loss: 1.4004 (1.4531)  classification_loss: 1.3950 (1.4286)  loss_mask: 0.0045 (0.0244)  time: 0.1710  data: 0.0002  max mem: 6052
[04:37:09.112375] Epoch: [42]  [140/781]  eta: 0:01:53  lr: 0.000167  training_loss: 1.3983 (1.4457)  classification_loss: 1.3949 (1.4239)  loss_mask: 0.0047 (0.0218)  time: 0.1716  data: 0.0003  max mem: 6052
[04:37:12.543793] Epoch: [42]  [160/781]  eta: 0:01:49  lr: 0.000167  training_loss: 1.3903 (1.4409)  classification_loss: 1.3867 (1.4211)  loss_mask: 0.0040 (0.0198)  time: 0.1715  data: 0.0003  max mem: 6052
[04:37:16.003766] Epoch: [42]  [180/781]  eta: 0:01:45  lr: 0.000167  training_loss: 1.4084 (1.4380)  classification_loss: 1.4047 (1.4200)  loss_mask: 0.0035 (0.0180)  time: 0.1729  data: 0.0002  max mem: 6052
[04:37:19.404573] Epoch: [42]  [200/781]  eta: 0:01:41  lr: 0.000167  training_loss: 1.3746 (1.4339)  classification_loss: 1.3732 (1.4175)  loss_mask: 0.0023 (0.0164)  time: 0.1700  data: 0.0001  max mem: 6052
[04:37:22.817359] Epoch: [42]  [220/781]  eta: 0:01:38  lr: 0.000167  training_loss: 1.4069 (1.4310)  classification_loss: 1.4024 (1.4158)  loss_mask: 0.0021 (0.0152)  time: 0.1706  data: 0.0002  max mem: 6052
[04:37:26.219729] Epoch: [42]  [240/781]  eta: 0:01:34  lr: 0.000167  training_loss: 1.3992 (1.4281)  classification_loss: 1.3973 (1.4140)  loss_mask: 0.0015 (0.0141)  time: 0.1700  data: 0.0002  max mem: 6052
[04:37:29.628017] Epoch: [42]  [260/781]  eta: 0:01:30  lr: 0.000167  training_loss: 1.3958 (1.4281)  classification_loss: 1.3919 (1.4149)  loss_mask: 0.0015 (0.0132)  time: 0.1703  data: 0.0002  max mem: 6052
[04:37:33.041327] Epoch: [42]  [280/781]  eta: 0:01:27  lr: 0.000166  training_loss: 1.4106 (1.4261)  classification_loss: 1.4093 (1.4136)  loss_mask: 0.0020 (0.0124)  time: 0.1706  data: 0.0003  max mem: 6052
[04:37:36.454664] Epoch: [42]  [300/781]  eta: 0:01:23  lr: 0.000166  training_loss: 1.4072 (1.4249)  classification_loss: 1.4047 (1.4132)  loss_mask: 0.0011 (0.0117)  time: 0.1706  data: 0.0002  max mem: 6052
[04:37:39.866920] Epoch: [42]  [320/781]  eta: 0:01:19  lr: 0.000166  training_loss: 1.3902 (1.4240)  classification_loss: 1.3875 (1.4127)  loss_mask: 0.0016 (0.0112)  time: 0.1705  data: 0.0003  max mem: 6052
[04:37:43.298783] Epoch: [42]  [340/781]  eta: 0:01:16  lr: 0.000166  training_loss: 1.4894 (1.4269)  classification_loss: 1.4252 (1.4125)  loss_mask: 0.0227 (0.0144)  time: 0.1715  data: 0.0002  max mem: 6052
[04:37:46.755111] Epoch: [42]  [360/781]  eta: 0:01:12  lr: 0.000166  training_loss: 1.4395 (1.4291)  classification_loss: 1.4371 (1.4129)  loss_mask: 0.0357 (0.0162)  time: 0.1727  data: 0.0002  max mem: 6052
[04:37:50.236981] Epoch: [42]  [380/781]  eta: 0:01:09  lr: 0.000166  training_loss: 1.4070 (1.4305)  classification_loss: 1.3716 (1.4140)  loss_mask: 0.0109 (0.0164)  time: 0.1740  data: 0.0003  max mem: 6052
[04:37:53.653192] Epoch: [42]  [400/781]  eta: 0:01:05  lr: 0.000166  training_loss: 1.4087 (1.4298)  classification_loss: 1.3959 (1.4139)  loss_mask: 0.0051 (0.0159)  time: 0.1707  data: 0.0002  max mem: 6052
[04:37:57.072419] Epoch: [42]  [420/781]  eta: 0:01:02  lr: 0.000166  training_loss: 1.4146 (1.4289)  classification_loss: 1.4107 (1.4135)  loss_mask: 0.0041 (0.0154)  time: 0.1709  data: 0.0002  max mem: 6052
[04:38:00.472273] Epoch: [42]  [440/781]  eta: 0:00:58  lr: 0.000166  training_loss: 1.4544 (1.4308)  classification_loss: 1.4527 (1.4159)  loss_mask: 0.0035 (0.0148)  time: 0.1699  data: 0.0002  max mem: 6052
[04:38:03.873088] Epoch: [42]  [460/781]  eta: 0:00:55  lr: 0.000166  training_loss: 1.3444 (1.4286)  classification_loss: 1.3433 (1.4143)  loss_mask: 0.0012 (0.0143)  time: 0.1700  data: 0.0002  max mem: 6052
[04:38:07.331015] Epoch: [42]  [480/781]  eta: 0:00:52  lr: 0.000165  training_loss: 1.4205 (1.4291)  classification_loss: 1.4196 (1.4154)  loss_mask: 0.0011 (0.0137)  time: 0.1728  data: 0.0003  max mem: 6052
[04:38:10.731002] Epoch: [42]  [500/781]  eta: 0:00:48  lr: 0.000165  training_loss: 1.4236 (1.4293)  classification_loss: 1.4229 (1.4158)  loss_mask: 0.0014 (0.0135)  time: 0.1699  data: 0.0002  max mem: 6052
[04:38:14.129098] Epoch: [42]  [520/781]  eta: 0:00:45  lr: 0.000165  training_loss: 1.4071 (1.4289)  classification_loss: 1.4049 (1.4158)  loss_mask: 0.0020 (0.0131)  time: 0.1698  data: 0.0003  max mem: 6052
[04:38:17.545156] Epoch: [42]  [540/781]  eta: 0:00:41  lr: 0.000165  training_loss: 1.4264 (1.4292)  classification_loss: 1.4242 (1.4165)  loss_mask: 0.0012 (0.0127)  time: 0.1707  data: 0.0003  max mem: 6052
[04:38:20.958697] Epoch: [42]  [560/781]  eta: 0:00:38  lr: 0.000165  training_loss: 1.4422 (1.4282)  classification_loss: 1.4419 (1.4160)  loss_mask: 0.0010 (0.0122)  time: 0.1706  data: 0.0003  max mem: 6052
[04:38:24.372609] Epoch: [42]  [580/781]  eta: 0:00:34  lr: 0.000165  training_loss: 1.3910 (1.4271)  classification_loss: 1.3892 (1.4152)  loss_mask: 0.0011 (0.0119)  time: 0.1706  data: 0.0002  max mem: 6052
[04:38:27.779537] Epoch: [42]  [600/781]  eta: 0:00:31  lr: 0.000165  training_loss: 1.3620 (1.4258)  classification_loss: 1.3492 (1.4139)  loss_mask: 0.0012 (0.0118)  time: 0.1703  data: 0.0002  max mem: 6052
[04:38:31.196159] Epoch: [42]  [620/781]  eta: 0:00:27  lr: 0.000165  training_loss: 1.4528 (1.4296)  classification_loss: 1.4301 (1.4148)  loss_mask: 0.0055 (0.0148)  time: 0.1707  data: 0.0002  max mem: 6052
[04:38:34.607853] Epoch: [42]  [640/781]  eta: 0:00:24  lr: 0.000165  training_loss: 1.4567 (1.4307)  classification_loss: 1.4137 (1.4149)  loss_mask: 0.0296 (0.0158)  time: 0.1705  data: 0.0002  max mem: 6052
[04:38:38.021398] Epoch: [42]  [660/781]  eta: 0:00:20  lr: 0.000165  training_loss: 1.4481 (1.4311)  classification_loss: 1.4336 (1.4152)  loss_mask: 0.0136 (0.0158)  time: 0.1706  data: 0.0002  max mem: 6052
[04:38:41.433966] Epoch: [42]  [680/781]  eta: 0:00:17  lr: 0.000164  training_loss: 1.4350 (1.4303)  classification_loss: 1.4280 (1.4146)  loss_mask: 0.0089 (0.0156)  time: 0.1705  data: 0.0002  max mem: 6052
[04:38:44.832509] Epoch: [42]  [700/781]  eta: 0:00:13  lr: 0.000164  training_loss: 1.4313 (1.4304)  classification_loss: 1.4268 (1.4150)  loss_mask: 0.0054 (0.0153)  time: 0.1698  data: 0.0002  max mem: 6052
[04:38:48.247708] Epoch: [42]  [720/781]  eta: 0:00:10  lr: 0.000164  training_loss: 1.4117 (1.4296)  classification_loss: 1.4093 (1.4145)  loss_mask: 0.0044 (0.0151)  time: 0.1707  data: 0.0002  max mem: 6052
[04:38:51.666482] Epoch: [42]  [740/781]  eta: 0:00:07  lr: 0.000164  training_loss: 1.4071 (1.4288)  classification_loss: 1.3990 (1.4140)  loss_mask: 0.0040 (0.0148)  time: 0.1709  data: 0.0002  max mem: 6052
[04:38:55.074219] Epoch: [42]  [760/781]  eta: 0:00:03  lr: 0.000164  training_loss: 1.4698 (1.4298)  classification_loss: 1.4660 (1.4152)  loss_mask: 0.0040 (0.0145)  time: 0.1703  data: 0.0002  max mem: 6052
[04:38:58.510506] Epoch: [42]  [780/781]  eta: 0:00:00  lr: 0.000164  training_loss: 1.3617 (1.4288)  classification_loss: 1.3605 (1.4145)  loss_mask: 0.0030 (0.0142)  time: 0.1717  data: 0.0002  max mem: 6052
[04:38:58.678972] Epoch: [42] Total time: 0:02:14 (0.1722 s / it)
[04:38:58.679414] Averaged stats: lr: 0.000164  training_loss: 1.3617 (1.4288)  classification_loss: 1.3605 (1.4145)  loss_mask: 0.0030 (0.0142)
[04:38:59.329107] Test:  [  0/157]  eta: 0:01:41  testing_loss: 0.6016 (0.6016)  acc1: 81.2500 (81.2500)  acc5: 98.4375 (98.4375)  time: 0.6458  data: 0.6130  max mem: 6052
[04:38:59.611582] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.6987 (0.6942)  acc1: 78.1250 (77.1307)  acc5: 100.0000 (99.1477)  time: 0.0841  data: 0.0559  max mem: 6052
[04:38:59.892225] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.6645 (0.6783)  acc1: 76.5625 (77.3065)  acc5: 100.0000 (99.2560)  time: 0.0279  data: 0.0001  max mem: 6052
[04:39:00.171989] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6793 (0.6910)  acc1: 76.5625 (76.9657)  acc5: 98.4375 (98.9415)  time: 0.0279  data: 0.0001  max mem: 6052
[04:39:00.457757] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6904 (0.6923)  acc1: 78.1250 (77.2866)  acc5: 98.4375 (98.7805)  time: 0.0282  data: 0.0002  max mem: 6052
[04:39:00.739491] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6690 (0.6866)  acc1: 78.1250 (77.7880)  acc5: 98.4375 (98.8051)  time: 0.0283  data: 0.0002  max mem: 6052
[04:39:01.018679] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6620 (0.6886)  acc1: 76.5625 (77.3309)  acc5: 100.0000 (98.8217)  time: 0.0279  data: 0.0001  max mem: 6052
[04:39:01.297633] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6447 (0.6806)  acc1: 78.1250 (77.8169)  acc5: 100.0000 (98.8336)  time: 0.0278  data: 0.0001  max mem: 6052
[04:39:01.577034] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6624 (0.6912)  acc1: 79.6875 (77.5849)  acc5: 98.4375 (98.7847)  time: 0.0278  data: 0.0001  max mem: 6052
[04:39:01.858996] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7140 (0.6914)  acc1: 76.5625 (77.5412)  acc5: 98.4375 (98.8152)  time: 0.0280  data: 0.0001  max mem: 6052
[04:39:02.141651] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.7074 (0.6949)  acc1: 76.5625 (77.3205)  acc5: 98.4375 (98.8243)  time: 0.0281  data: 0.0002  max mem: 6052
[04:39:02.429419] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.7283 (0.6963)  acc1: 76.5625 (77.3367)  acc5: 98.4375 (98.7894)  time: 0.0284  data: 0.0001  max mem: 6052
[04:39:02.719532] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6954 (0.6925)  acc1: 78.1250 (77.5439)  acc5: 98.4375 (98.7603)  time: 0.0288  data: 0.0001  max mem: 6052
[04:39:03.004936] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6478 (0.6915)  acc1: 79.6875 (77.7195)  acc5: 98.4375 (98.7834)  time: 0.0287  data: 0.0001  max mem: 6052
[04:39:03.285295] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7163 (0.6911)  acc1: 79.6875 (77.8036)  acc5: 98.4375 (98.7810)  time: 0.0282  data: 0.0001  max mem: 6052
[04:39:03.562941] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7019 (0.6885)  acc1: 76.5625 (77.8767)  acc5: 98.4375 (98.7686)  time: 0.0278  data: 0.0001  max mem: 6052
[04:39:03.711916] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6548 (0.6901)  acc1: 76.5625 (77.8300)  acc5: 98.4375 (98.7800)  time: 0.0268  data: 0.0001  max mem: 6052
[04:39:03.887192] Test: Total time: 0:00:05 (0.0332 s / it)
[04:39:03.887780] * Acc@1 77.830 Acc@5 98.780 loss 0.690
[04:39:03.888075] Accuracy of the network on the 10000 test images: 77.8%
[04:39:03.888244] Max accuracy: 77.83%
[04:39:04.315597] log_dir: ./output_dir
[04:39:05.226060] Epoch: [43]  [  0/781]  eta: 0:11:49  lr: 0.000164  training_loss: 1.3421 (1.3421)  classification_loss: 1.3401 (1.3401)  loss_mask: 0.0020 (0.0020)  time: 0.9087  data: 0.6957  max mem: 6052
[04:39:08.636952] Epoch: [43]  [ 20/781]  eta: 0:02:36  lr: 0.000164  training_loss: 1.3499 (1.3793)  classification_loss: 1.3486 (1.3742)  loss_mask: 0.0023 (0.0052)  time: 0.1705  data: 0.0002  max mem: 6052
[04:39:12.056995] Epoch: [43]  [ 40/781]  eta: 0:02:19  lr: 0.000164  training_loss: 1.5378 (1.4736)  classification_loss: 1.4217 (1.3969)  loss_mask: 0.0604 (0.0767)  time: 0.1709  data: 0.0002  max mem: 6052
[04:39:15.482780] Epoch: [43]  [ 60/781]  eta: 0:02:11  lr: 0.000164  training_loss: 1.5586 (1.5207)  classification_loss: 1.4333 (1.4220)  loss_mask: 0.1119 (0.0987)  time: 0.1712  data: 0.0002  max mem: 6052
[04:39:18.911460] Epoch: [43]  [ 80/781]  eta: 0:02:06  lr: 0.000164  training_loss: 1.4180 (1.4995)  classification_loss: 1.3877 (1.4154)  loss_mask: 0.0373 (0.0841)  time: 0.1714  data: 0.0002  max mem: 6052
[04:39:22.320737] Epoch: [43]  [100/781]  eta: 0:02:01  lr: 0.000163  training_loss: 1.4754 (1.4933)  classification_loss: 1.4316 (1.4196)  loss_mask: 0.0195 (0.0738)  time: 0.1704  data: 0.0002  max mem: 6052
[04:39:25.729667] Epoch: [43]  [120/781]  eta: 0:01:56  lr: 0.000163  training_loss: 1.4501 (1.4856)  classification_loss: 1.4249 (1.4206)  loss_mask: 0.0164 (0.0651)  time: 0.1704  data: 0.0002  max mem: 6052
[04:39:29.144737] Epoch: [43]  [140/781]  eta: 0:01:52  lr: 0.000163  training_loss: 1.4213 (1.4796)  classification_loss: 1.4006 (1.4202)  loss_mask: 0.0191 (0.0594)  time: 0.1707  data: 0.0003  max mem: 6052
[04:39:32.576166] Epoch: [43]  [160/781]  eta: 0:01:48  lr: 0.000163  training_loss: 1.3882 (1.4711)  classification_loss: 1.3655 (1.4152)  loss_mask: 0.0139 (0.0558)  time: 0.1715  data: 0.0003  max mem: 6052
[04:39:35.981426] Epoch: [43]  [180/781]  eta: 0:01:45  lr: 0.000163  training_loss: 1.4377 (1.4652)  classification_loss: 1.4157 (1.4120)  loss_mask: 0.0243 (0.0532)  time: 0.1702  data: 0.0003  max mem: 6052
[04:39:39.418706] Epoch: [43]  [200/781]  eta: 0:01:41  lr: 0.000163  training_loss: 1.4648 (1.4624)  classification_loss: 1.4310 (1.4109)  loss_mask: 0.0137 (0.0515)  time: 0.1718  data: 0.0002  max mem: 6052
[04:39:42.810148] Epoch: [43]  [220/781]  eta: 0:01:37  lr: 0.000163  training_loss: 1.3856 (1.4610)  classification_loss: 1.3785 (1.4118)  loss_mask: 0.0200 (0.0492)  time: 0.1695  data: 0.0002  max mem: 6052
[04:39:46.219278] Epoch: [43]  [240/781]  eta: 0:01:34  lr: 0.000163  training_loss: 1.3875 (1.4570)  classification_loss: 1.3752 (1.4110)  loss_mask: 0.0076 (0.0460)  time: 0.1704  data: 0.0002  max mem: 6052
[04:39:49.628460] Epoch: [43]  [260/781]  eta: 0:01:30  lr: 0.000163  training_loss: 1.3868 (1.4551)  classification_loss: 1.3843 (1.4122)  loss_mask: 0.0039 (0.0429)  time: 0.1704  data: 0.0002  max mem: 6052
[04:39:53.026139] Epoch: [43]  [280/781]  eta: 0:01:26  lr: 0.000163  training_loss: 1.3832 (1.4514)  classification_loss: 1.3776 (1.4112)  loss_mask: 0.0045 (0.0402)  time: 0.1698  data: 0.0002  max mem: 6052
[04:39:56.431846] Epoch: [43]  [300/781]  eta: 0:01:23  lr: 0.000162  training_loss: 1.3880 (1.4484)  classification_loss: 1.3806 (1.4106)  loss_mask: 0.0031 (0.0378)  time: 0.1702  data: 0.0002  max mem: 6052
[04:39:59.827792] Epoch: [43]  [320/781]  eta: 0:01:19  lr: 0.000162  training_loss: 1.4080 (1.4462)  classification_loss: 1.4053 (1.4105)  loss_mask: 0.0033 (0.0356)  time: 0.1697  data: 0.0001  max mem: 6052
[04:40:03.228470] Epoch: [43]  [340/781]  eta: 0:01:16  lr: 0.000162  training_loss: 1.3613 (1.4430)  classification_loss: 1.3574 (1.4093)  loss_mask: 0.0027 (0.0337)  time: 0.1700  data: 0.0003  max mem: 6052
[04:40:06.644075] Epoch: [43]  [360/781]  eta: 0:01:12  lr: 0.000162  training_loss: 1.3796 (1.4428)  classification_loss: 1.3748 (1.4108)  loss_mask: 0.0022 (0.0320)  time: 0.1707  data: 0.0002  max mem: 6052
[04:40:10.051987] Epoch: [43]  [380/781]  eta: 0:01:09  lr: 0.000162  training_loss: 1.3797 (1.4406)  classification_loss: 1.3781 (1.4101)  loss_mask: 0.0022 (0.0304)  time: 0.1703  data: 0.0002  max mem: 6052
[04:40:13.466871] Epoch: [43]  [400/781]  eta: 0:01:05  lr: 0.000162  training_loss: 1.4170 (1.4399)  classification_loss: 1.4159 (1.4109)  loss_mask: 0.0016 (0.0290)  time: 0.1706  data: 0.0004  max mem: 6052
[04:40:16.869970] Epoch: [43]  [420/781]  eta: 0:01:02  lr: 0.000162  training_loss: 1.3735 (1.4381)  classification_loss: 1.3722 (1.4104)  loss_mask: 0.0015 (0.0277)  time: 0.1701  data: 0.0002  max mem: 6052
[04:40:20.336221] Epoch: [43]  [440/781]  eta: 0:00:58  lr: 0.000162  training_loss: 1.3852 (1.4365)  classification_loss: 1.3825 (1.4100)  loss_mask: 0.0020 (0.0265)  time: 0.1732  data: 0.0002  max mem: 6052
[04:40:23.751199] Epoch: [43]  [460/781]  eta: 0:00:55  lr: 0.000162  training_loss: 1.3356 (1.4332)  classification_loss: 1.3346 (1.4077)  loss_mask: 0.0014 (0.0255)  time: 0.1706  data: 0.0002  max mem: 6052
[04:40:27.161816] Epoch: [43]  [480/781]  eta: 0:00:51  lr: 0.000162  training_loss: 1.4600 (1.4337)  classification_loss: 1.4588 (1.4092)  loss_mask: 0.0015 (0.0245)  time: 0.1705  data: 0.0002  max mem: 6052
[04:40:30.553816] Epoch: [43]  [500/781]  eta: 0:00:48  lr: 0.000161  training_loss: 1.4043 (1.4327)  classification_loss: 1.4028 (1.4091)  loss_mask: 0.0013 (0.0236)  time: 0.1695  data: 0.0003  max mem: 6052
[04:40:33.967558] Epoch: [43]  [520/781]  eta: 0:00:44  lr: 0.000161  training_loss: 1.4250 (1.4327)  classification_loss: 1.4239 (1.4100)  loss_mask: 0.0011 (0.0227)  time: 0.1706  data: 0.0002  max mem: 6052
[04:40:37.380441] Epoch: [43]  [540/781]  eta: 0:00:41  lr: 0.000161  training_loss: 1.3676 (1.4305)  classification_loss: 1.3667 (1.4086)  loss_mask: 0.0012 (0.0219)  time: 0.1706  data: 0.0003  max mem: 6052
[04:40:40.802568] Epoch: [43]  [560/781]  eta: 0:00:37  lr: 0.000161  training_loss: 1.3871 (1.4299)  classification_loss: 1.3865 (1.4087)  loss_mask: 0.0010 (0.0212)  time: 0.1710  data: 0.0002  max mem: 6052
[04:40:44.208507] Epoch: [43]  [580/781]  eta: 0:00:34  lr: 0.000161  training_loss: 1.4068 (1.4292)  classification_loss: 1.4062 (1.4088)  loss_mask: 0.0008 (0.0205)  time: 0.1702  data: 0.0002  max mem: 6052
[04:40:47.644312] Epoch: [43]  [600/781]  eta: 0:00:31  lr: 0.000161  training_loss: 1.4069 (1.4289)  classification_loss: 1.4063 (1.4090)  loss_mask: 0.0010 (0.0199)  time: 0.1717  data: 0.0002  max mem: 6052
[04:40:51.039150] Epoch: [43]  [620/781]  eta: 0:00:27  lr: 0.000161  training_loss: 1.4118 (1.4285)  classification_loss: 1.4093 (1.4090)  loss_mask: 0.0025 (0.0194)  time: 0.1697  data: 0.0002  max mem: 6052
[04:40:54.432230] Epoch: [43]  [640/781]  eta: 0:00:24  lr: 0.000161  training_loss: 1.3914 (1.4271)  classification_loss: 1.3833 (1.4080)  loss_mask: 0.0040 (0.0191)  time: 0.1696  data: 0.0001  max mem: 6052
[04:40:57.877077] Epoch: [43]  [660/781]  eta: 0:00:20  lr: 0.000161  training_loss: 1.4493 (1.4289)  classification_loss: 1.4167 (1.4087)  loss_mask: 0.0215 (0.0202)  time: 0.1722  data: 0.0004  max mem: 6052
[04:41:01.298545] Epoch: [43]  [680/781]  eta: 0:00:17  lr: 0.000161  training_loss: 1.4749 (1.4284)  classification_loss: 1.4161 (1.4076)  loss_mask: 0.0282 (0.0208)  time: 0.1710  data: 0.0002  max mem: 6052
[04:41:04.708516] Epoch: [43]  [700/781]  eta: 0:00:13  lr: 0.000160  training_loss: 1.5206 (1.4313)  classification_loss: 1.4044 (1.4074)  loss_mask: 0.0800 (0.0239)  time: 0.1704  data: 0.0002  max mem: 6052
[04:41:08.112534] Epoch: [43]  [720/781]  eta: 0:00:10  lr: 0.000160  training_loss: 1.4987 (1.4335)  classification_loss: 1.3424 (1.4062)  loss_mask: 0.1264 (0.0273)  time: 0.1701  data: 0.0002  max mem: 6052
[04:41:11.524837] Epoch: [43]  [740/781]  eta: 0:00:07  lr: 0.000160  training_loss: 1.3965 (1.4325)  classification_loss: 1.3638 (1.4049)  loss_mask: 0.0280 (0.0276)  time: 0.1705  data: 0.0001  max mem: 6052
[04:41:14.944144] Epoch: [43]  [760/781]  eta: 0:00:03  lr: 0.000160  training_loss: 1.4677 (1.4334)  classification_loss: 1.4420 (1.4060)  loss_mask: 0.0129 (0.0274)  time: 0.1709  data: 0.0002  max mem: 6052
[04:41:18.388674] Epoch: [43]  [780/781]  eta: 0:00:00  lr: 0.000160  training_loss: 1.3747 (1.4326)  classification_loss: 1.3588 (1.4056)  loss_mask: 0.0126 (0.0270)  time: 0.1721  data: 0.0002  max mem: 6052
[04:41:18.531426] Epoch: [43] Total time: 0:02:14 (0.1719 s / it)
[04:41:18.532017] Averaged stats: lr: 0.000160  training_loss: 1.3747 (1.4326)  classification_loss: 1.3588 (1.4056)  loss_mask: 0.0126 (0.0270)
[04:41:19.192347] Test:  [  0/157]  eta: 0:01:42  testing_loss: 0.5648 (0.5648)  acc1: 85.9375 (85.9375)  acc5: 100.0000 (100.0000)  time: 0.6556  data: 0.6134  max mem: 6052
[04:41:19.477196] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.6850 (0.7080)  acc1: 76.5625 (77.8409)  acc5: 100.0000 (99.2898)  time: 0.0853  data: 0.0559  max mem: 6052
[04:41:19.765193] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.6771 (0.6894)  acc1: 76.5625 (78.4226)  acc5: 98.4375 (99.0327)  time: 0.0285  data: 0.0002  max mem: 6052
[04:41:20.054426] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6996 (0.7031)  acc1: 76.5625 (77.4194)  acc5: 98.4375 (98.6391)  time: 0.0287  data: 0.0002  max mem: 6052
[04:41:20.336056] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.7119 (0.7092)  acc1: 76.5625 (77.4390)  acc5: 98.4375 (98.5137)  time: 0.0284  data: 0.0002  max mem: 6052
[04:41:20.625365] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6899 (0.7031)  acc1: 78.1250 (77.9412)  acc5: 96.8750 (98.4375)  time: 0.0284  data: 0.0002  max mem: 6052
[04:41:20.909158] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6843 (0.7029)  acc1: 78.1250 (77.6383)  acc5: 98.4375 (98.5143)  time: 0.0285  data: 0.0002  max mem: 6052
[04:41:21.191982] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6771 (0.6989)  acc1: 78.1250 (77.6849)  acc5: 98.4375 (98.5915)  time: 0.0282  data: 0.0001  max mem: 6052
[04:41:21.473789] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6828 (0.7054)  acc1: 76.5625 (77.3534)  acc5: 98.4375 (98.4761)  time: 0.0281  data: 0.0001  max mem: 6052
[04:41:21.756350] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6726 (0.7009)  acc1: 76.5625 (77.4897)  acc5: 98.4375 (98.5234)  time: 0.0281  data: 0.0001  max mem: 6052
[04:41:22.041652] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.7097 (0.7052)  acc1: 76.5625 (77.2587)  acc5: 98.4375 (98.5149)  time: 0.0283  data: 0.0001  max mem: 6052
[04:41:22.332315] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.7285 (0.7064)  acc1: 75.0000 (77.1819)  acc5: 98.4375 (98.5360)  time: 0.0287  data: 0.0002  max mem: 6052
[04:41:22.615981] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6799 (0.7019)  acc1: 76.5625 (77.2727)  acc5: 98.4375 (98.5279)  time: 0.0286  data: 0.0002  max mem: 6052
[04:41:22.901186] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6600 (0.7021)  acc1: 76.5625 (77.2424)  acc5: 98.4375 (98.5568)  time: 0.0283  data: 0.0002  max mem: 6052
[04:41:23.184693] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6922 (0.6997)  acc1: 78.1250 (77.3825)  acc5: 98.4375 (98.5594)  time: 0.0283  data: 0.0001  max mem: 6052
[04:41:23.464152] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6930 (0.6985)  acc1: 76.5625 (77.3593)  acc5: 98.4375 (98.5927)  time: 0.0280  data: 0.0001  max mem: 6052
[04:41:23.615207] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7252 (0.6993)  acc1: 76.5625 (77.3200)  acc5: 98.4375 (98.6000)  time: 0.0269  data: 0.0001  max mem: 6052
[04:41:23.786325] Test: Total time: 0:00:05 (0.0334 s / it)
[04:41:23.786770] * Acc@1 77.320 Acc@5 98.600 loss 0.699
[04:41:23.787107] Accuracy of the network on the 10000 test images: 77.3%
[04:41:23.787311] Max accuracy: 77.83%
[04:41:23.977908] log_dir: ./output_dir
[04:41:24.853357] Epoch: [44]  [  0/781]  eta: 0:11:22  lr: 0.000160  training_loss: 1.1302 (1.1302)  classification_loss: 1.1218 (1.1218)  loss_mask: 0.0084 (0.0084)  time: 0.8736  data: 0.6888  max mem: 6052
[04:41:28.255680] Epoch: [44]  [ 20/781]  eta: 0:02:34  lr: 0.000160  training_loss: 1.3788 (1.3837)  classification_loss: 1.3620 (1.3742)  loss_mask: 0.0072 (0.0095)  time: 0.1700  data: 0.0002  max mem: 6052
[04:41:31.669724] Epoch: [44]  [ 40/781]  eta: 0:02:18  lr: 0.000160  training_loss: 1.3964 (1.3858)  classification_loss: 1.3882 (1.3774)  loss_mask: 0.0065 (0.0084)  time: 0.1706  data: 0.0002  max mem: 6052
[04:41:35.101054] Epoch: [44]  [ 60/781]  eta: 0:02:11  lr: 0.000160  training_loss: 1.4094 (1.3953)  classification_loss: 1.4057 (1.3873)  loss_mask: 0.0071 (0.0080)  time: 0.1714  data: 0.0002  max mem: 6052
[04:41:38.511699] Epoch: [44]  [ 80/781]  eta: 0:02:05  lr: 0.000160  training_loss: 1.4368 (1.4020)  classification_loss: 1.4317 (1.3949)  loss_mask: 0.0039 (0.0071)  time: 0.1704  data: 0.0002  max mem: 6052
[04:41:41.910357] Epoch: [44]  [100/781]  eta: 0:02:00  lr: 0.000160  training_loss: 1.3934 (1.3989)  classification_loss: 1.3909 (1.3925)  loss_mask: 0.0032 (0.0064)  time: 0.1699  data: 0.0002  max mem: 6052
[04:41:45.311999] Epoch: [44]  [120/781]  eta: 0:01:56  lr: 0.000159  training_loss: 1.3926 (1.3997)  classification_loss: 1.3883 (1.3936)  loss_mask: 0.0032 (0.0061)  time: 0.1700  data: 0.0001  max mem: 6052
[04:41:48.839475] Epoch: [44]  [140/781]  eta: 0:01:52  lr: 0.000159  training_loss: 1.3702 (1.3962)  classification_loss: 1.3679 (1.3904)  loss_mask: 0.0036 (0.0058)  time: 0.1763  data: 0.0003  max mem: 6052
[04:41:52.368646] Epoch: [44]  [160/781]  eta: 0:01:49  lr: 0.000159  training_loss: 1.3877 (1.3942)  classification_loss: 1.3838 (1.3888)  loss_mask: 0.0022 (0.0054)  time: 0.1764  data: 0.0003  max mem: 6052
[04:41:55.864773] Epoch: [44]  [180/781]  eta: 0:01:45  lr: 0.000159  training_loss: 1.3375 (1.3909)  classification_loss: 1.3348 (1.3858)  loss_mask: 0.0017 (0.0050)  time: 0.1747  data: 0.0002  max mem: 6052
[04:41:59.336795] Epoch: [44]  [200/781]  eta: 0:01:42  lr: 0.000159  training_loss: 1.3702 (1.3904)  classification_loss: 1.3682 (1.3856)  loss_mask: 0.0015 (0.0047)  time: 0.1733  data: 0.0003  max mem: 6052
[04:42:02.768633] Epoch: [44]  [220/781]  eta: 0:01:38  lr: 0.000159  training_loss: 1.3979 (1.3909)  classification_loss: 1.3965 (1.3865)  loss_mask: 0.0014 (0.0044)  time: 0.1715  data: 0.0002  max mem: 6052
[04:42:06.212924] Epoch: [44]  [240/781]  eta: 0:01:34  lr: 0.000159  training_loss: 1.3696 (1.3903)  classification_loss: 1.3683 (1.3861)  loss_mask: 0.0013 (0.0042)  time: 0.1721  data: 0.0003  max mem: 6052
[04:42:09.658110] Epoch: [44]  [260/781]  eta: 0:01:31  lr: 0.000159  training_loss: 1.3827 (1.3918)  classification_loss: 1.3817 (1.3878)  loss_mask: 0.0010 (0.0039)  time: 0.1722  data: 0.0002  max mem: 6052
[04:42:13.096000] Epoch: [44]  [280/781]  eta: 0:01:27  lr: 0.000159  training_loss: 1.3830 (1.3919)  classification_loss: 1.3820 (1.3882)  loss_mask: 0.0012 (0.0038)  time: 0.1718  data: 0.0002  max mem: 6052
[04:42:16.560858] Epoch: [44]  [300/781]  eta: 0:01:23  lr: 0.000159  training_loss: 1.3926 (1.3920)  classification_loss: 1.3917 (1.3884)  loss_mask: 0.0009 (0.0036)  time: 0.1732  data: 0.0002  max mem: 6052
[04:42:19.984687] Epoch: [44]  [320/781]  eta: 0:01:20  lr: 0.000158  training_loss: 1.3956 (1.3935)  classification_loss: 1.3942 (1.3901)  loss_mask: 0.0008 (0.0034)  time: 0.1711  data: 0.0002  max mem: 6052
[04:42:23.422470] Epoch: [44]  [340/781]  eta: 0:01:16  lr: 0.000158  training_loss: 1.3386 (1.3924)  classification_loss: 1.3380 (1.3892)  loss_mask: 0.0008 (0.0033)  time: 0.1718  data: 0.0004  max mem: 6052
[04:42:26.865198] Epoch: [44]  [360/781]  eta: 0:01:13  lr: 0.000158  training_loss: 1.4087 (1.3948)  classification_loss: 1.4080 (1.3916)  loss_mask: 0.0010 (0.0031)  time: 0.1721  data: 0.0002  max mem: 6052
[04:42:30.297422] Epoch: [44]  [380/781]  eta: 0:01:09  lr: 0.000158  training_loss: 1.3839 (1.3952)  classification_loss: 1.3821 (1.3922)  loss_mask: 0.0008 (0.0030)  time: 0.1715  data: 0.0002  max mem: 6052
[04:42:33.741163] Epoch: [44]  [400/781]  eta: 0:01:06  lr: 0.000158  training_loss: 1.3955 (1.3960)  classification_loss: 1.3949 (1.3931)  loss_mask: 0.0008 (0.0029)  time: 0.1721  data: 0.0002  max mem: 6052
[04:42:37.158499] Epoch: [44]  [420/781]  eta: 0:01:02  lr: 0.000158  training_loss: 1.4009 (1.3957)  classification_loss: 1.4002 (1.3929)  loss_mask: 0.0006 (0.0028)  time: 0.1707  data: 0.0001  max mem: 6052
[04:42:40.578076] Epoch: [44]  [440/781]  eta: 0:00:59  lr: 0.000158  training_loss: 1.3662 (1.3939)  classification_loss: 1.3658 (1.3912)  loss_mask: 0.0006 (0.0027)  time: 0.1709  data: 0.0002  max mem: 6052
[04:42:44.005476] Epoch: [44]  [460/781]  eta: 0:00:55  lr: 0.000158  training_loss: 1.3646 (1.3927)  classification_loss: 1.3643 (1.3901)  loss_mask: 0.0005 (0.0026)  time: 0.1713  data: 0.0002  max mem: 6052
[04:42:47.415454] Epoch: [44]  [480/781]  eta: 0:00:52  lr: 0.000158  training_loss: 1.4087 (1.3938)  classification_loss: 1.4079 (1.3913)  loss_mask: 0.0004 (0.0025)  time: 0.1704  data: 0.0002  max mem: 6052
[04:42:50.835294] Epoch: [44]  [500/781]  eta: 0:00:48  lr: 0.000157  training_loss: 1.3861 (1.3942)  classification_loss: 1.3857 (1.3917)  loss_mask: 0.0004 (0.0024)  time: 0.1709  data: 0.0004  max mem: 6052
[04:42:54.252002] Epoch: [44]  [520/781]  eta: 0:00:45  lr: 0.000157  training_loss: 1.3585 (1.3937)  classification_loss: 1.3579 (1.3914)  loss_mask: 0.0006 (0.0024)  time: 0.1708  data: 0.0002  max mem: 6052
[04:42:57.696052] Epoch: [44]  [540/781]  eta: 0:00:41  lr: 0.000157  training_loss: 1.3896 (1.3937)  classification_loss: 1.3892 (1.3914)  loss_mask: 0.0005 (0.0023)  time: 0.1721  data: 0.0002  max mem: 6052
[04:43:01.108077] Epoch: [44]  [560/781]  eta: 0:00:38  lr: 0.000157  training_loss: 1.4049 (1.3950)  classification_loss: 1.4044 (1.3927)  loss_mask: 0.0005 (0.0022)  time: 0.1705  data: 0.0003  max mem: 6052
[04:43:04.531307] Epoch: [44]  [580/781]  eta: 0:00:34  lr: 0.000157  training_loss: 1.3921 (1.3955)  classification_loss: 1.3917 (1.3933)  loss_mask: 0.0005 (0.0022)  time: 0.1711  data: 0.0003  max mem: 6052
[04:43:07.941881] Epoch: [44]  [600/781]  eta: 0:00:31  lr: 0.000157  training_loss: 1.3239 (1.3939)  classification_loss: 1.3237 (1.3918)  loss_mask: 0.0005 (0.0021)  time: 0.1704  data: 0.0002  max mem: 6052
[04:43:11.353020] Epoch: [44]  [620/781]  eta: 0:00:27  lr: 0.000157  training_loss: 1.3579 (1.3937)  classification_loss: 1.3569 (1.3916)  loss_mask: 0.0004 (0.0021)  time: 0.1705  data: 0.0002  max mem: 6052
[04:43:14.752301] Epoch: [44]  [640/781]  eta: 0:00:24  lr: 0.000157  training_loss: 1.4250 (1.3947)  classification_loss: 1.4245 (1.3927)  loss_mask: 0.0005 (0.0020)  time: 0.1699  data: 0.0003  max mem: 6052
[04:43:18.188818] Epoch: [44]  [660/781]  eta: 0:00:20  lr: 0.000157  training_loss: 1.3509 (1.3937)  classification_loss: 1.3504 (1.3918)  loss_mask: 0.0004 (0.0020)  time: 0.1718  data: 0.0002  max mem: 6052
[04:43:21.596719] Epoch: [44]  [680/781]  eta: 0:00:17  lr: 0.000157  training_loss: 1.4174 (1.3949)  classification_loss: 1.4164 (1.3929)  loss_mask: 0.0003 (0.0019)  time: 0.1703  data: 0.0002  max mem: 6052
[04:43:25.008949] Epoch: [44]  [700/781]  eta: 0:00:13  lr: 0.000156  training_loss: 1.3910 (1.3950)  classification_loss: 1.3907 (1.3931)  loss_mask: 0.0004 (0.0019)  time: 0.1705  data: 0.0002  max mem: 6052
[04:43:28.430226] Epoch: [44]  [720/781]  eta: 0:00:10  lr: 0.000156  training_loss: 1.3640 (1.3949)  classification_loss: 1.3636 (1.3931)  loss_mask: 0.0004 (0.0018)  time: 0.1710  data: 0.0003  max mem: 6052
[04:43:31.837835] Epoch: [44]  [740/781]  eta: 0:00:07  lr: 0.000156  training_loss: 1.3371 (1.3940)  classification_loss: 1.3369 (1.3922)  loss_mask: 0.0004 (0.0018)  time: 0.1703  data: 0.0002  max mem: 6052
[04:43:35.252061] Epoch: [44]  [760/781]  eta: 0:00:03  lr: 0.000156  training_loss: 1.4145 (1.3946)  classification_loss: 1.4144 (1.3928)  loss_mask: 0.0004 (0.0018)  time: 0.1706  data: 0.0008  max mem: 6052
[04:43:38.662958] Epoch: [44]  [780/781]  eta: 0:00:00  lr: 0.000156  training_loss: 1.4295 (1.3951)  classification_loss: 1.4290 (1.3933)  loss_mask: 0.0004 (0.0017)  time: 0.1705  data: 0.0001  max mem: 6052
[04:43:38.817852] Epoch: [44] Total time: 0:02:14 (0.1726 s / it)
[04:43:38.818310] Averaged stats: lr: 0.000156  training_loss: 1.4295 (1.3951)  classification_loss: 1.4290 (1.3933)  loss_mask: 0.0004 (0.0017)
[04:43:39.490339] Test:  [  0/157]  eta: 0:01:44  testing_loss: 0.6217 (0.6217)  acc1: 84.3750 (84.3750)  acc5: 98.4375 (98.4375)  time: 0.6674  data: 0.6368  max mem: 6052
[04:43:39.776140] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.6988 (0.6914)  acc1: 78.1250 (78.9773)  acc5: 98.4375 (98.7216)  time: 0.0864  data: 0.0581  max mem: 6052
[04:43:40.061747] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.6430 (0.6697)  acc1: 78.1250 (79.9107)  acc5: 98.4375 (98.8839)  time: 0.0283  data: 0.0002  max mem: 6052
[04:43:40.345351] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6525 (0.6796)  acc1: 79.6875 (79.3851)  acc5: 98.4375 (98.8911)  time: 0.0283  data: 0.0002  max mem: 6052
[04:43:40.629053] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6770 (0.6831)  acc1: 79.6875 (79.4588)  acc5: 98.4375 (98.8186)  time: 0.0282  data: 0.0002  max mem: 6052
[04:43:40.911196] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6485 (0.6736)  acc1: 81.2500 (79.8713)  acc5: 98.4375 (98.8971)  time: 0.0282  data: 0.0001  max mem: 6052
[04:43:41.196879] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6454 (0.6748)  acc1: 79.6875 (79.4826)  acc5: 100.0000 (98.8986)  time: 0.0283  data: 0.0001  max mem: 6052
[04:43:41.479787] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6541 (0.6728)  acc1: 79.6875 (79.5555)  acc5: 98.4375 (98.7896)  time: 0.0283  data: 0.0001  max mem: 6052
[04:43:41.763470] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6685 (0.6819)  acc1: 79.6875 (79.2824)  acc5: 98.4375 (98.7461)  time: 0.0282  data: 0.0001  max mem: 6052
[04:43:42.048982] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6724 (0.6804)  acc1: 78.1250 (79.2582)  acc5: 100.0000 (98.8496)  time: 0.0283  data: 0.0001  max mem: 6052
[04:43:42.334094] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.6724 (0.6836)  acc1: 79.6875 (79.1151)  acc5: 100.0000 (98.7469)  time: 0.0284  data: 0.0001  max mem: 6052
[04:43:42.615996] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6938 (0.6839)  acc1: 76.5625 (79.0400)  acc5: 98.4375 (98.7753)  time: 0.0282  data: 0.0001  max mem: 6052
[04:43:42.899371] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6625 (0.6796)  acc1: 78.1250 (79.1322)  acc5: 98.4375 (98.7603)  time: 0.0282  data: 0.0001  max mem: 6052
[04:43:43.185564] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6341 (0.6794)  acc1: 79.6875 (79.1269)  acc5: 98.4375 (98.8073)  time: 0.0284  data: 0.0002  max mem: 6052
[04:43:43.465864] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6886 (0.6779)  acc1: 76.5625 (79.1113)  acc5: 98.4375 (98.7810)  time: 0.0282  data: 0.0002  max mem: 6052
[04:43:43.744617] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6885 (0.6768)  acc1: 76.5625 (79.1701)  acc5: 98.4375 (98.7583)  time: 0.0278  data: 0.0001  max mem: 6052
[04:43:43.894531] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6770 (0.6779)  acc1: 79.6875 (79.1400)  acc5: 98.4375 (98.7600)  time: 0.0268  data: 0.0001  max mem: 6052
[04:43:44.042787] Test: Total time: 0:00:05 (0.0333 s / it)
[04:43:44.043229] * Acc@1 79.140 Acc@5 98.760 loss 0.678
[04:43:44.043543] Accuracy of the network on the 10000 test images: 79.1%
[04:43:44.043723] Max accuracy: 79.14%
[04:43:44.211637] log_dir: ./output_dir
[04:43:45.082278] Epoch: [45]  [  0/781]  eta: 0:11:18  lr: 0.000156  training_loss: 1.3279 (1.3279)  classification_loss: 1.3274 (1.3274)  loss_mask: 0.0004 (0.0004)  time: 0.8688  data: 0.6867  max mem: 6052
[04:43:48.508871] Epoch: [45]  [ 20/781]  eta: 0:02:35  lr: 0.000156  training_loss: 1.3866 (1.3942)  classification_loss: 1.3856 (1.3938)  loss_mask: 0.0004 (0.0004)  time: 0.1712  data: 0.0002  max mem: 6052
[04:43:51.943134] Epoch: [45]  [ 40/781]  eta: 0:02:19  lr: 0.000156  training_loss: 1.3874 (1.3722)  classification_loss: 1.3870 (1.3718)  loss_mask: 0.0004 (0.0004)  time: 0.1716  data: 0.0003  max mem: 6052
[04:43:55.402159] Epoch: [45]  [ 60/781]  eta: 0:02:12  lr: 0.000156  training_loss: 1.3930 (1.3873)  classification_loss: 1.3928 (1.3869)  loss_mask: 0.0003 (0.0004)  time: 0.1729  data: 0.0002  max mem: 6052
[04:43:58.825967] Epoch: [45]  [ 80/781]  eta: 0:02:06  lr: 0.000156  training_loss: 1.3643 (1.3872)  classification_loss: 1.3641 (1.3868)  loss_mask: 0.0003 (0.0004)  time: 0.1711  data: 0.0002  max mem: 6052
[04:44:02.245692] Epoch: [45]  [100/781]  eta: 0:02:01  lr: 0.000156  training_loss: 1.4098 (1.3912)  classification_loss: 1.4095 (1.3908)  loss_mask: 0.0003 (0.0004)  time: 0.1709  data: 0.0002  max mem: 6052
[04:44:05.673114] Epoch: [45]  [120/781]  eta: 0:01:57  lr: 0.000155  training_loss: 1.2972 (1.3813)  classification_loss: 1.2968 (1.3803)  loss_mask: 0.0005 (0.0009)  time: 0.1713  data: 0.0003  max mem: 6052
[04:44:09.074941] Epoch: [45]  [140/781]  eta: 0:01:52  lr: 0.000155  training_loss: 1.4557 (1.4118)  classification_loss: 1.3788 (1.3809)  loss_mask: 0.0830 (0.0309)  time: 0.1700  data: 0.0003  max mem: 6052
[04:44:12.498107] Epoch: [45]  [160/781]  eta: 0:01:49  lr: 0.000155  training_loss: 1.4457 (1.4225)  classification_loss: 1.3629 (1.3843)  loss_mask: 0.0581 (0.0382)  time: 0.1711  data: 0.0003  max mem: 6052
[04:44:15.910319] Epoch: [45]  [180/781]  eta: 0:01:45  lr: 0.000155  training_loss: 1.4599 (1.4257)  classification_loss: 1.3411 (1.3815)  loss_mask: 0.0634 (0.0441)  time: 0.1705  data: 0.0003  max mem: 6052
[04:44:19.335756] Epoch: [45]  [200/781]  eta: 0:01:41  lr: 0.000155  training_loss: 1.3773 (1.4226)  classification_loss: 1.3348 (1.3788)  loss_mask: 0.0298 (0.0438)  time: 0.1712  data: 0.0002  max mem: 6052
[04:44:22.798098] Epoch: [45]  [220/781]  eta: 0:01:37  lr: 0.000155  training_loss: 1.4091 (1.4204)  classification_loss: 1.4028 (1.3785)  loss_mask: 0.0158 (0.0420)  time: 0.1730  data: 0.0002  max mem: 6052
[04:44:26.243891] Epoch: [45]  [240/781]  eta: 0:01:34  lr: 0.000155  training_loss: 1.3676 (1.4202)  classification_loss: 1.3608 (1.3807)  loss_mask: 0.0090 (0.0395)  time: 0.1722  data: 0.0003  max mem: 6052
[04:44:29.667339] Epoch: [45]  [260/781]  eta: 0:01:30  lr: 0.000155  training_loss: 1.3838 (1.4188)  classification_loss: 1.3805 (1.3817)  loss_mask: 0.0061 (0.0371)  time: 0.1711  data: 0.0002  max mem: 6052
[04:44:33.083286] Epoch: [45]  [280/781]  eta: 0:01:27  lr: 0.000155  training_loss: 1.3574 (1.4148)  classification_loss: 1.3450 (1.3796)  loss_mask: 0.0073 (0.0352)  time: 0.1707  data: 0.0002  max mem: 6052
[04:44:36.527110] Epoch: [45]  [300/781]  eta: 0:01:23  lr: 0.000155  training_loss: 1.3851 (1.4154)  classification_loss: 1.3781 (1.3821)  loss_mask: 0.0052 (0.0333)  time: 0.1721  data: 0.0002  max mem: 6052
[04:44:40.050772] Epoch: [45]  [320/781]  eta: 0:01:20  lr: 0.000154  training_loss: 1.3849 (1.4147)  classification_loss: 1.3769 (1.3831)  loss_mask: 0.0048 (0.0315)  time: 0.1761  data: 0.0002  max mem: 6052
[04:44:43.616439] Epoch: [45]  [340/781]  eta: 0:01:16  lr: 0.000154  training_loss: 1.3147 (1.4111)  classification_loss: 1.3121 (1.3813)  loss_mask: 0.0028 (0.0299)  time: 0.1782  data: 0.0003  max mem: 6052
[04:44:47.087569] Epoch: [45]  [360/781]  eta: 0:01:13  lr: 0.000154  training_loss: 1.3619 (1.4102)  classification_loss: 1.3595 (1.3818)  loss_mask: 0.0021 (0.0283)  time: 0.1735  data: 0.0004  max mem: 6052
[04:44:50.550753] Epoch: [45]  [380/781]  eta: 0:01:09  lr: 0.000154  training_loss: 1.3491 (1.4082)  classification_loss: 1.3475 (1.3809)  loss_mask: 0.0021 (0.0273)  time: 0.1731  data: 0.0003  max mem: 6052
[04:44:54.003838] Epoch: [45]  [400/781]  eta: 0:01:06  lr: 0.000154  training_loss: 1.3959 (1.4076)  classification_loss: 1.3893 (1.3816)  loss_mask: 0.0018 (0.0260)  time: 0.1726  data: 0.0002  max mem: 6052
[04:44:57.414725] Epoch: [45]  [420/781]  eta: 0:01:02  lr: 0.000154  training_loss: 1.3977 (1.4064)  classification_loss: 1.3971 (1.3816)  loss_mask: 0.0012 (0.0249)  time: 0.1705  data: 0.0001  max mem: 6052
[04:45:00.857226] Epoch: [45]  [440/781]  eta: 0:00:59  lr: 0.000154  training_loss: 1.3720 (1.4065)  classification_loss: 1.3702 (1.3824)  loss_mask: 0.0018 (0.0241)  time: 0.1720  data: 0.0002  max mem: 6052
[04:45:04.287143] Epoch: [45]  [460/781]  eta: 0:00:55  lr: 0.000154  training_loss: 1.3397 (1.4048)  classification_loss: 1.3374 (1.3816)  loss_mask: 0.0022 (0.0231)  time: 0.1714  data: 0.0003  max mem: 6052
[04:45:07.712441] Epoch: [45]  [480/781]  eta: 0:00:52  lr: 0.000154  training_loss: 1.3719 (1.4026)  classification_loss: 1.3714 (1.3804)  loss_mask: 0.0012 (0.0222)  time: 0.1712  data: 0.0002  max mem: 6052
[04:45:11.127873] Epoch: [45]  [500/781]  eta: 0:00:48  lr: 0.000154  training_loss: 1.3779 (1.4027)  classification_loss: 1.3768 (1.3813)  loss_mask: 0.0009 (0.0214)  time: 0.1707  data: 0.0002  max mem: 6052
[04:45:14.545528] Epoch: [45]  [520/781]  eta: 0:00:45  lr: 0.000153  training_loss: 1.3540 (1.4017)  classification_loss: 1.3533 (1.3811)  loss_mask: 0.0008 (0.0206)  time: 0.1707  data: 0.0002  max mem: 6052
[04:45:17.978311] Epoch: [45]  [540/781]  eta: 0:00:41  lr: 0.000153  training_loss: 1.3648 (1.4016)  classification_loss: 1.3635 (1.3817)  loss_mask: 0.0008 (0.0199)  time: 0.1715  data: 0.0001  max mem: 6052
[04:45:21.378224] Epoch: [45]  [560/781]  eta: 0:00:38  lr: 0.000153  training_loss: 1.3827 (1.4007)  classification_loss: 1.3817 (1.3815)  loss_mask: 0.0008 (0.0192)  time: 0.1699  data: 0.0002  max mem: 6052
[04:45:24.789687] Epoch: [45]  [580/781]  eta: 0:00:34  lr: 0.000153  training_loss: 1.3691 (1.3995)  classification_loss: 1.3684 (1.3810)  loss_mask: 0.0007 (0.0186)  time: 0.1705  data: 0.0001  max mem: 6052
[04:45:28.205754] Epoch: [45]  [600/781]  eta: 0:00:31  lr: 0.000153  training_loss: 1.3689 (1.3981)  classification_loss: 1.3676 (1.3802)  loss_mask: 0.0006 (0.0180)  time: 0.1707  data: 0.0002  max mem: 6052
[04:45:31.620656] Epoch: [45]  [620/781]  eta: 0:00:27  lr: 0.000153  training_loss: 1.3921 (1.3989)  classification_loss: 1.3915 (1.3815)  loss_mask: 0.0007 (0.0174)  time: 0.1707  data: 0.0002  max mem: 6052
[04:45:35.023312] Epoch: [45]  [640/781]  eta: 0:00:24  lr: 0.000153  training_loss: 1.3610 (1.3985)  classification_loss: 1.3601 (1.3816)  loss_mask: 0.0006 (0.0169)  time: 0.1701  data: 0.0002  max mem: 6052
[04:45:38.443321] Epoch: [45]  [660/781]  eta: 0:00:20  lr: 0.000153  training_loss: 1.4036 (1.3987)  classification_loss: 1.4024 (1.3823)  loss_mask: 0.0006 (0.0164)  time: 0.1709  data: 0.0003  max mem: 6052
[04:45:41.888428] Epoch: [45]  [680/781]  eta: 0:00:17  lr: 0.000153  training_loss: 1.3850 (1.3989)  classification_loss: 1.3840 (1.3830)  loss_mask: 0.0006 (0.0159)  time: 0.1722  data: 0.0004  max mem: 6052
[04:45:45.293628] Epoch: [45]  [700/781]  eta: 0:00:13  lr: 0.000152  training_loss: 1.4140 (1.3988)  classification_loss: 1.4137 (1.3833)  loss_mask: 0.0006 (0.0155)  time: 0.1702  data: 0.0003  max mem: 6052
[04:45:48.712545] Epoch: [45]  [720/781]  eta: 0:00:10  lr: 0.000152  training_loss: 1.3823 (1.3988)  classification_loss: 1.3816 (1.3837)  loss_mask: 0.0004 (0.0151)  time: 0.1708  data: 0.0004  max mem: 6052
[04:45:52.140268] Epoch: [45]  [740/781]  eta: 0:00:07  lr: 0.000152  training_loss: 1.3511 (1.3983)  classification_loss: 1.3507 (1.3836)  loss_mask: 0.0004 (0.0147)  time: 0.1712  data: 0.0003  max mem: 6052
[04:45:55.585067] Epoch: [45]  [760/781]  eta: 0:00:03  lr: 0.000152  training_loss: 1.4229 (1.3979)  classification_loss: 1.4224 (1.3836)  loss_mask: 0.0005 (0.0143)  time: 0.1722  data: 0.0003  max mem: 6052
[04:45:58.987231] Epoch: [45]  [780/781]  eta: 0:00:00  lr: 0.000152  training_loss: 1.3542 (1.3978)  classification_loss: 1.3539 (1.3839)  loss_mask: 0.0004 (0.0140)  time: 0.1700  data: 0.0002  max mem: 6052
[04:45:59.144318] Epoch: [45] Total time: 0:02:14 (0.1728 s / it)
[04:45:59.144822] Averaged stats: lr: 0.000152  training_loss: 1.3542 (1.3978)  classification_loss: 1.3539 (1.3839)  loss_mask: 0.0004 (0.0140)
[04:45:59.817821] Test:  [  0/157]  eta: 0:01:44  testing_loss: 0.5737 (0.5737)  acc1: 84.3750 (84.3750)  acc5: 98.4375 (98.4375)  time: 0.6679  data: 0.6371  max mem: 6052
[04:46:00.103053] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.6620 (0.6678)  acc1: 81.2500 (79.1193)  acc5: 98.4375 (99.0057)  time: 0.0864  data: 0.0580  max mem: 6052
[04:46:00.389412] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.6239 (0.6462)  acc1: 79.6875 (79.6875)  acc5: 98.4375 (99.1071)  time: 0.0284  data: 0.0001  max mem: 6052
[04:46:00.673312] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6454 (0.6573)  acc1: 78.1250 (79.0323)  acc5: 98.4375 (98.9415)  time: 0.0284  data: 0.0002  max mem: 6052
[04:46:00.958601] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6486 (0.6622)  acc1: 79.6875 (79.0396)  acc5: 98.4375 (98.8186)  time: 0.0283  data: 0.0002  max mem: 6052
[04:46:01.241749] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6318 (0.6545)  acc1: 81.2500 (79.5650)  acc5: 98.4375 (98.8051)  time: 0.0283  data: 0.0002  max mem: 6052
[04:46:01.531415] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6229 (0.6539)  acc1: 79.6875 (79.5082)  acc5: 100.0000 (98.8730)  time: 0.0285  data: 0.0002  max mem: 6052
[04:46:01.815255] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6347 (0.6509)  acc1: 79.6875 (79.6215)  acc5: 98.4375 (98.7236)  time: 0.0285  data: 0.0001  max mem: 6052
[04:46:02.097082] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6603 (0.6584)  acc1: 79.6875 (79.3789)  acc5: 98.4375 (98.6690)  time: 0.0281  data: 0.0001  max mem: 6052
[04:46:02.377371] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6393 (0.6573)  acc1: 78.1250 (79.2411)  acc5: 98.4375 (98.7294)  time: 0.0280  data: 0.0001  max mem: 6052
[04:46:02.658219] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.6393 (0.6601)  acc1: 78.1250 (79.0687)  acc5: 98.4375 (98.6850)  time: 0.0279  data: 0.0001  max mem: 6052
[04:46:02.941624] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6720 (0.6604)  acc1: 78.1250 (79.0822)  acc5: 98.4375 (98.6909)  time: 0.0281  data: 0.0001  max mem: 6052
[04:46:03.226536] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6482 (0.6575)  acc1: 78.1250 (79.1839)  acc5: 98.4375 (98.7216)  time: 0.0283  data: 0.0002  max mem: 6052
[04:46:03.512016] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6462 (0.6575)  acc1: 78.1250 (79.2104)  acc5: 98.4375 (98.6880)  time: 0.0284  data: 0.0002  max mem: 6052
[04:46:03.793399] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6706 (0.6556)  acc1: 79.6875 (79.2332)  acc5: 100.0000 (98.7367)  time: 0.0282  data: 0.0001  max mem: 6052
[04:46:04.071715] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6686 (0.6551)  acc1: 79.6875 (79.2425)  acc5: 100.0000 (98.7686)  time: 0.0278  data: 0.0001  max mem: 6052
[04:46:04.220478] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6686 (0.6560)  acc1: 78.1250 (79.2300)  acc5: 100.0000 (98.7700)  time: 0.0268  data: 0.0001  max mem: 6052
[04:46:04.384523] Test: Total time: 0:00:05 (0.0334 s / it)
[04:46:04.385109] * Acc@1 79.230 Acc@5 98.770 loss 0.656
[04:46:04.385465] Accuracy of the network on the 10000 test images: 79.2%
[04:46:04.385670] Max accuracy: 79.23%
[04:46:04.842874] log_dir: ./output_dir
[04:46:05.705002] Epoch: [46]  [  0/781]  eta: 0:11:11  lr: 0.000152  training_loss: 1.2789 (1.2789)  classification_loss: 1.2783 (1.2783)  loss_mask: 0.0007 (0.0007)  time: 0.8602  data: 0.6472  max mem: 6052
[04:46:09.149338] Epoch: [46]  [ 20/781]  eta: 0:02:35  lr: 0.000152  training_loss: 1.3468 (1.3527)  classification_loss: 1.3463 (1.3523)  loss_mask: 0.0004 (0.0004)  time: 0.1721  data: 0.0002  max mem: 6052
[04:46:12.673045] Epoch: [46]  [ 40/781]  eta: 0:02:21  lr: 0.000152  training_loss: 1.3910 (1.3584)  classification_loss: 1.3907 (1.3579)  loss_mask: 0.0004 (0.0004)  time: 0.1761  data: 0.0002  max mem: 6052
[04:46:16.083468] Epoch: [46]  [ 60/781]  eta: 0:02:12  lr: 0.000152  training_loss: 1.3917 (1.3697)  classification_loss: 1.3914 (1.3693)  loss_mask: 0.0004 (0.0004)  time: 0.1704  data: 0.0003  max mem: 6052
[04:46:19.517709] Epoch: [46]  [ 80/781]  eta: 0:02:06  lr: 0.000152  training_loss: 1.3611 (1.3698)  classification_loss: 1.3608 (1.3694)  loss_mask: 0.0004 (0.0004)  time: 0.1716  data: 0.0002  max mem: 6052
[04:46:22.932365] Epoch: [46]  [100/781]  eta: 0:02:01  lr: 0.000152  training_loss: 1.3423 (1.3671)  classification_loss: 1.3420 (1.3667)  loss_mask: 0.0003 (0.0004)  time: 0.1707  data: 0.0002  max mem: 6052
[04:46:26.349243] Epoch: [46]  [120/781]  eta: 0:01:57  lr: 0.000151  training_loss: 1.4206 (1.3731)  classification_loss: 1.4202 (1.3727)  loss_mask: 0.0003 (0.0004)  time: 0.1708  data: 0.0002  max mem: 6052
[04:46:29.764618] Epoch: [46]  [140/781]  eta: 0:01:53  lr: 0.000151  training_loss: 1.4243 (1.3774)  classification_loss: 1.4240 (1.3770)  loss_mask: 0.0003 (0.0004)  time: 0.1707  data: 0.0003  max mem: 6052
[04:46:33.237173] Epoch: [46]  [160/781]  eta: 0:01:49  lr: 0.000151  training_loss: 1.3431 (1.3756)  classification_loss: 1.3429 (1.3752)  loss_mask: 0.0003 (0.0004)  time: 0.1736  data: 0.0003  max mem: 6052
[04:46:36.653494] Epoch: [46]  [180/781]  eta: 0:01:45  lr: 0.000151  training_loss: 1.3375 (1.3739)  classification_loss: 1.3374 (1.3735)  loss_mask: 0.0003 (0.0004)  time: 0.1707  data: 0.0002  max mem: 6052
[04:46:40.116904] Epoch: [46]  [200/781]  eta: 0:01:41  lr: 0.000151  training_loss: 1.4000 (1.3758)  classification_loss: 1.3995 (1.3754)  loss_mask: 0.0003 (0.0004)  time: 0.1731  data: 0.0002  max mem: 6052
[04:46:43.538214] Epoch: [46]  [220/781]  eta: 0:01:38  lr: 0.000151  training_loss: 1.3692 (1.3779)  classification_loss: 1.3687 (1.3775)  loss_mask: 0.0004 (0.0004)  time: 0.1710  data: 0.0002  max mem: 6052
[04:46:46.954065] Epoch: [46]  [240/781]  eta: 0:01:34  lr: 0.000151  training_loss: 1.5259 (1.3951)  classification_loss: 1.3781 (1.3793)  loss_mask: 0.0939 (0.0158)  time: 0.1707  data: 0.0002  max mem: 6052
[04:46:50.391866] Epoch: [46]  [260/781]  eta: 0:01:30  lr: 0.000151  training_loss: 1.4408 (1.4018)  classification_loss: 1.3601 (1.3789)  loss_mask: 0.0319 (0.0229)  time: 0.1718  data: 0.0002  max mem: 6052
[04:46:53.798925] Epoch: [46]  [280/781]  eta: 0:01:27  lr: 0.000151  training_loss: 1.4090 (1.4008)  classification_loss: 1.3840 (1.3778)  loss_mask: 0.0198 (0.0230)  time: 0.1703  data: 0.0002  max mem: 6052
[04:46:57.242726] Epoch: [46]  [300/781]  eta: 0:01:23  lr: 0.000151  training_loss: 1.3681 (1.3989)  classification_loss: 1.3349 (1.3757)  loss_mask: 0.0223 (0.0233)  time: 0.1721  data: 0.0003  max mem: 6052
[04:47:00.659642] Epoch: [46]  [320/781]  eta: 0:01:20  lr: 0.000150  training_loss: 1.3836 (1.3980)  classification_loss: 1.3746 (1.3754)  loss_mask: 0.0115 (0.0226)  time: 0.1708  data: 0.0003  max mem: 6052
[04:47:04.088076] Epoch: [46]  [340/781]  eta: 0:01:16  lr: 0.000150  training_loss: 1.3920 (1.3973)  classification_loss: 1.3524 (1.3734)  loss_mask: 0.0245 (0.0239)  time: 0.1713  data: 0.0002  max mem: 6052
[04:47:07.510107] Epoch: [46]  [360/781]  eta: 0:01:13  lr: 0.000150  training_loss: 1.4838 (1.4014)  classification_loss: 1.3922 (1.3757)  loss_mask: 0.0375 (0.0258)  time: 0.1710  data: 0.0002  max mem: 6052
[04:47:10.933349] Epoch: [46]  [380/781]  eta: 0:01:09  lr: 0.000150  training_loss: 1.3958 (1.4015)  classification_loss: 1.3690 (1.3758)  loss_mask: 0.0176 (0.0257)  time: 0.1711  data: 0.0002  max mem: 6052
[04:47:14.354053] Epoch: [46]  [400/781]  eta: 0:01:06  lr: 0.000150  training_loss: 1.3849 (1.4011)  classification_loss: 1.3649 (1.3758)  loss_mask: 0.0137 (0.0253)  time: 0.1710  data: 0.0002  max mem: 6052
[04:47:17.804111] Epoch: [46]  [420/781]  eta: 0:01:02  lr: 0.000150  training_loss: 1.4012 (1.3993)  classification_loss: 1.3924 (1.3749)  loss_mask: 0.0057 (0.0244)  time: 0.1724  data: 0.0002  max mem: 6052
[04:47:21.214347] Epoch: [46]  [440/781]  eta: 0:00:59  lr: 0.000150  training_loss: 1.3800 (1.3982)  classification_loss: 1.3747 (1.3747)  loss_mask: 0.0052 (0.0235)  time: 0.1704  data: 0.0002  max mem: 6052
[04:47:24.620563] Epoch: [46]  [460/781]  eta: 0:00:55  lr: 0.000150  training_loss: 1.3457 (1.3961)  classification_loss: 1.3438 (1.3734)  loss_mask: 0.0030 (0.0227)  time: 0.1702  data: 0.0003  max mem: 6052
[04:47:28.043251] Epoch: [46]  [480/781]  eta: 0:00:52  lr: 0.000150  training_loss: 1.4270 (1.3974)  classification_loss: 1.4245 (1.3755)  loss_mask: 0.0033 (0.0219)  time: 0.1711  data: 0.0002  max mem: 6052
[04:47:31.453783] Epoch: [46]  [500/781]  eta: 0:00:48  lr: 0.000149  training_loss: 1.3939 (1.3980)  classification_loss: 1.3914 (1.3768)  loss_mask: 0.0025 (0.0211)  time: 0.1704  data: 0.0001  max mem: 6052
[04:47:34.870444] Epoch: [46]  [520/781]  eta: 0:00:45  lr: 0.000149  training_loss: 1.3659 (1.3968)  classification_loss: 1.3620 (1.3763)  loss_mask: 0.0032 (0.0205)  time: 0.1708  data: 0.0001  max mem: 6052
[04:47:38.307820] Epoch: [46]  [540/781]  eta: 0:00:41  lr: 0.000149  training_loss: 1.3880 (1.3968)  classification_loss: 1.3855 (1.3770)  loss_mask: 0.0025 (0.0198)  time: 0.1718  data: 0.0002  max mem: 6052
[04:47:41.722273] Epoch: [46]  [560/781]  eta: 0:00:38  lr: 0.000149  training_loss: 1.3022 (1.3945)  classification_loss: 1.3011 (1.3754)  loss_mask: 0.0016 (0.0192)  time: 0.1706  data: 0.0002  max mem: 6052
[04:47:45.132390] Epoch: [46]  [580/781]  eta: 0:00:34  lr: 0.000149  training_loss: 1.3927 (1.3948)  classification_loss: 1.3895 (1.3762)  loss_mask: 0.0023 (0.0186)  time: 0.1704  data: 0.0002  max mem: 6052
[04:47:48.532576] Epoch: [46]  [600/781]  eta: 0:00:31  lr: 0.000149  training_loss: 1.3581 (1.3942)  classification_loss: 1.3550 (1.3762)  loss_mask: 0.0016 (0.0180)  time: 0.1699  data: 0.0001  max mem: 6052
[04:47:51.936471] Epoch: [46]  [620/781]  eta: 0:00:27  lr: 0.000149  training_loss: 1.3831 (1.3943)  classification_loss: 1.3800 (1.3768)  loss_mask: 0.0023 (0.0175)  time: 0.1701  data: 0.0002  max mem: 6052
[04:47:55.359464] Epoch: [46]  [640/781]  eta: 0:00:24  lr: 0.000149  training_loss: 1.3636 (1.3942)  classification_loss: 1.3622 (1.3771)  loss_mask: 0.0015 (0.0170)  time: 0.1711  data: 0.0004  max mem: 6052
[04:47:58.770378] Epoch: [46]  [660/781]  eta: 0:00:20  lr: 0.000149  training_loss: 1.3318 (1.3936)  classification_loss: 1.3305 (1.3771)  loss_mask: 0.0014 (0.0166)  time: 0.1705  data: 0.0002  max mem: 6052
[04:48:02.231855] Epoch: [46]  [680/781]  eta: 0:00:17  lr: 0.000149  training_loss: 1.3953 (1.3936)  classification_loss: 1.3951 (1.3775)  loss_mask: 0.0012 (0.0161)  time: 0.1730  data: 0.0003  max mem: 6052
[04:48:05.649957] Epoch: [46]  [700/781]  eta: 0:00:13  lr: 0.000148  training_loss: 1.3696 (1.3931)  classification_loss: 1.3686 (1.3774)  loss_mask: 0.0011 (0.0157)  time: 0.1708  data: 0.0003  max mem: 6052
[04:48:09.081285] Epoch: [46]  [720/781]  eta: 0:00:10  lr: 0.000148  training_loss: 1.4023 (1.3929)  classification_loss: 1.4007 (1.3776)  loss_mask: 0.0010 (0.0153)  time: 0.1715  data: 0.0002  max mem: 6052
[04:48:12.537394] Epoch: [46]  [740/781]  eta: 0:00:07  lr: 0.000148  training_loss: 1.3729 (1.3924)  classification_loss: 1.3718 (1.3775)  loss_mask: 0.0011 (0.0149)  time: 0.1727  data: 0.0003  max mem: 6052
[04:48:15.962370] Epoch: [46]  [760/781]  eta: 0:00:03  lr: 0.000148  training_loss: 1.4113 (1.3935)  classification_loss: 1.4111 (1.3789)  loss_mask: 0.0011 (0.0146)  time: 0.1712  data: 0.0002  max mem: 6052
[04:48:19.356242] Epoch: [46]  [780/781]  eta: 0:00:00  lr: 0.000148  training_loss: 1.4262 (1.3944)  classification_loss: 1.4252 (1.3802)  loss_mask: 0.0010 (0.0142)  time: 0.1696  data: 0.0002  max mem: 6052
[04:48:19.521067] Epoch: [46] Total time: 0:02:14 (0.1724 s / it)
[04:48:19.521635] Averaged stats: lr: 0.000148  training_loss: 1.4262 (1.3944)  classification_loss: 1.4252 (1.3802)  loss_mask: 0.0010 (0.0142)
[04:48:20.188688] Test:  [  0/157]  eta: 0:01:44  testing_loss: 0.6413 (0.6413)  acc1: 82.8125 (82.8125)  acc5: 100.0000 (100.0000)  time: 0.6630  data: 0.6338  max mem: 6052
[04:48:20.471318] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.6413 (0.6667)  acc1: 79.6875 (78.8352)  acc5: 100.0000 (99.4318)  time: 0.0858  data: 0.0578  max mem: 6052
[04:48:20.754973] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.6319 (0.6509)  acc1: 79.6875 (79.3899)  acc5: 98.4375 (99.2560)  time: 0.0282  data: 0.0002  max mem: 6052
[04:48:21.042258] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6616 (0.6553)  acc1: 79.6875 (79.5867)  acc5: 98.4375 (99.1431)  time: 0.0284  data: 0.0002  max mem: 6052
[04:48:21.323835] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6616 (0.6645)  acc1: 78.1250 (79.0015)  acc5: 98.4375 (98.8567)  time: 0.0283  data: 0.0002  max mem: 6052
[04:48:21.605079] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6583 (0.6577)  acc1: 78.1250 (79.3199)  acc5: 98.4375 (98.8358)  time: 0.0280  data: 0.0001  max mem: 6052
[04:48:21.885239] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6679 (0.6632)  acc1: 78.1250 (79.0727)  acc5: 98.4375 (98.8473)  time: 0.0280  data: 0.0001  max mem: 6052
[04:48:22.166369] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6576 (0.6594)  acc1: 79.6875 (79.3794)  acc5: 98.4375 (98.8336)  time: 0.0280  data: 0.0001  max mem: 6052
[04:48:22.450532] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6517 (0.6665)  acc1: 78.1250 (79.0702)  acc5: 98.4375 (98.7654)  time: 0.0281  data: 0.0002  max mem: 6052
[04:48:22.733678] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6545 (0.6639)  acc1: 78.1250 (79.2754)  acc5: 100.0000 (98.8668)  time: 0.0282  data: 0.0002  max mem: 6052
[04:48:23.015543] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.6495 (0.6695)  acc1: 79.6875 (79.0068)  acc5: 100.0000 (98.8088)  time: 0.0281  data: 0.0001  max mem: 6052
[04:48:23.301630] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.7154 (0.6715)  acc1: 76.5625 (78.8007)  acc5: 100.0000 (98.8316)  time: 0.0283  data: 0.0002  max mem: 6052
[04:48:23.584464] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6352 (0.6684)  acc1: 76.5625 (78.8869)  acc5: 100.0000 (98.8249)  time: 0.0283  data: 0.0002  max mem: 6052
[04:48:23.867430] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6337 (0.6687)  acc1: 79.6875 (78.9003)  acc5: 98.4375 (98.8430)  time: 0.0282  data: 0.0002  max mem: 6052
[04:48:24.149104] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6682 (0.6672)  acc1: 79.6875 (79.0337)  acc5: 98.4375 (98.8364)  time: 0.0281  data: 0.0002  max mem: 6052
[04:48:24.428216] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6682 (0.6661)  acc1: 78.1250 (79.0459)  acc5: 98.4375 (98.8514)  time: 0.0279  data: 0.0001  max mem: 6052
[04:48:24.579649] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6729 (0.6663)  acc1: 78.1250 (79.0000)  acc5: 98.4375 (98.8800)  time: 0.0270  data: 0.0001  max mem: 6052
[04:48:24.728730] Test: Total time: 0:00:05 (0.0331 s / it)
[04:48:24.729192] * Acc@1 79.000 Acc@5 98.880 loss 0.666
[04:48:24.729511] Accuracy of the network on the 10000 test images: 79.0%
[04:48:24.729742] Max accuracy: 79.23%
[04:48:25.134201] log_dir: ./output_dir
[04:48:26.013789] Epoch: [47]  [  0/781]  eta: 0:11:25  lr: 0.000148  training_loss: 1.3893 (1.3893)  classification_loss: 1.3886 (1.3886)  loss_mask: 0.0007 (0.0007)  time: 0.8778  data: 0.6705  max mem: 6052
[04:48:29.451080] Epoch: [47]  [ 20/781]  eta: 0:02:36  lr: 0.000148  training_loss: 1.3402 (1.3734)  classification_loss: 1.3398 (1.3725)  loss_mask: 0.0008 (0.0009)  time: 0.1718  data: 0.0002  max mem: 6052
[04:48:32.900676] Epoch: [47]  [ 40/781]  eta: 0:02:20  lr: 0.000148  training_loss: 1.3831 (1.3901)  classification_loss: 1.3820 (1.3892)  loss_mask: 0.0008 (0.0009)  time: 0.1724  data: 0.0002  max mem: 6052
[04:48:36.346265] Epoch: [47]  [ 60/781]  eta: 0:02:12  lr: 0.000148  training_loss: 1.3901 (1.3922)  classification_loss: 1.3900 (1.3914)  loss_mask: 0.0005 (0.0008)  time: 0.1722  data: 0.0003  max mem: 6052
[04:48:39.793425] Epoch: [47]  [ 80/781]  eta: 0:02:06  lr: 0.000148  training_loss: 1.3358 (1.3818)  classification_loss: 1.3354 (1.3809)  loss_mask: 0.0011 (0.0009)  time: 0.1722  data: 0.0003  max mem: 6052
[04:48:43.286900] Epoch: [47]  [100/781]  eta: 0:02:02  lr: 0.000148  training_loss: 1.3918 (1.3825)  classification_loss: 1.3903 (1.3817)  loss_mask: 0.0006 (0.0009)  time: 0.1746  data: 0.0003  max mem: 6052
[04:48:46.723071] Epoch: [47]  [120/781]  eta: 0:01:57  lr: 0.000147  training_loss: 1.3177 (1.3740)  classification_loss: 1.3172 (1.3732)  loss_mask: 0.0007 (0.0008)  time: 0.1717  data: 0.0002  max mem: 6052
[04:48:50.156366] Epoch: [47]  [140/781]  eta: 0:01:53  lr: 0.000147  training_loss: 1.3693 (1.3707)  classification_loss: 1.3687 (1.3699)  loss_mask: 0.0006 (0.0008)  time: 0.1716  data: 0.0003  max mem: 6052
[04:48:53.583054] Epoch: [47]  [160/781]  eta: 0:01:49  lr: 0.000147  training_loss: 1.3635 (1.3706)  classification_loss: 1.3625 (1.3698)  loss_mask: 0.0007 (0.0008)  time: 0.1713  data: 0.0002  max mem: 6052
[04:48:56.998737] Epoch: [47]  [180/781]  eta: 0:01:45  lr: 0.000147  training_loss: 1.3615 (1.3718)  classification_loss: 1.3609 (1.3710)  loss_mask: 0.0006 (0.0008)  time: 0.1707  data: 0.0002  max mem: 6052
[04:49:00.422199] Epoch: [47]  [200/781]  eta: 0:01:41  lr: 0.000147  training_loss: 1.3254 (1.3703)  classification_loss: 1.3249 (1.3696)  loss_mask: 0.0005 (0.0008)  time: 0.1711  data: 0.0002  max mem: 6052
[04:49:03.842559] Epoch: [47]  [220/781]  eta: 0:01:38  lr: 0.000147  training_loss: 1.4145 (1.3723)  classification_loss: 1.4144 (1.3715)  loss_mask: 0.0006 (0.0008)  time: 0.1709  data: 0.0003  max mem: 6052
[04:49:07.250836] Epoch: [47]  [240/781]  eta: 0:01:34  lr: 0.000147  training_loss: 1.3656 (1.3748)  classification_loss: 1.3646 (1.3741)  loss_mask: 0.0005 (0.0008)  time: 0.1703  data: 0.0002  max mem: 6052
[04:49:10.691576] Epoch: [47]  [260/781]  eta: 0:01:30  lr: 0.000147  training_loss: 1.3736 (1.3752)  classification_loss: 1.3731 (1.3744)  loss_mask: 0.0005 (0.0007)  time: 0.1719  data: 0.0002  max mem: 6052
[04:49:14.102436] Epoch: [47]  [280/781]  eta: 0:01:27  lr: 0.000147  training_loss: 1.3265 (1.3729)  classification_loss: 1.3256 (1.3722)  loss_mask: 0.0005 (0.0007)  time: 0.1705  data: 0.0002  max mem: 6052
[04:49:17.511227] Epoch: [47]  [300/781]  eta: 0:01:23  lr: 0.000146  training_loss: 1.4278 (1.3757)  classification_loss: 1.4271 (1.3750)  loss_mask: 0.0005 (0.0007)  time: 0.1704  data: 0.0002  max mem: 6052
[04:49:20.973762] Epoch: [47]  [320/781]  eta: 0:01:20  lr: 0.000146  training_loss: 1.2992 (1.3720)  classification_loss: 1.2866 (1.3710)  loss_mask: 0.0007 (0.0009)  time: 0.1730  data: 0.0002  max mem: 6052
[04:49:24.430626] Epoch: [47]  [340/781]  eta: 0:01:16  lr: 0.000146  training_loss: 1.4274 (1.3814)  classification_loss: 1.3967 (1.3712)  loss_mask: 0.0219 (0.0101)  time: 0.1728  data: 0.0002  max mem: 6052
[04:49:27.849835] Epoch: [47]  [360/781]  eta: 0:01:13  lr: 0.000146  training_loss: 1.6589 (1.3996)  classification_loss: 1.3580 (1.3717)  loss_mask: 0.2272 (0.0279)  time: 0.1709  data: 0.0003  max mem: 6052
[04:49:31.271491] Epoch: [47]  [380/781]  eta: 0:01:09  lr: 0.000146  training_loss: 1.3876 (1.3998)  classification_loss: 1.3485 (1.3704)  loss_mask: 0.0375 (0.0294)  time: 0.1710  data: 0.0002  max mem: 6052
[04:49:34.758447] Epoch: [47]  [400/781]  eta: 0:01:06  lr: 0.000146  training_loss: 1.3388 (1.3962)  classification_loss: 1.3192 (1.3673)  loss_mask: 0.0194 (0.0289)  time: 0.1743  data: 0.0002  max mem: 6052
[04:49:38.185757] Epoch: [47]  [420/781]  eta: 0:01:02  lr: 0.000146  training_loss: 1.3927 (1.3961)  classification_loss: 1.3908 (1.3677)  loss_mask: 0.0181 (0.0285)  time: 0.1713  data: 0.0003  max mem: 6052
[04:49:41.597853] Epoch: [47]  [440/781]  eta: 0:00:59  lr: 0.000146  training_loss: 1.3653 (1.3953)  classification_loss: 1.3417 (1.3670)  loss_mask: 0.0167 (0.0283)  time: 0.1705  data: 0.0002  max mem: 6052
[04:49:45.012627] Epoch: [47]  [460/781]  eta: 0:00:55  lr: 0.000146  training_loss: 1.3465 (1.3933)  classification_loss: 1.3400 (1.3657)  loss_mask: 0.0107 (0.0276)  time: 0.1707  data: 0.0002  max mem: 6052
[04:49:48.443263] Epoch: [47]  [480/781]  eta: 0:00:52  lr: 0.000146  training_loss: 1.4323 (1.3941)  classification_loss: 1.4216 (1.3674)  loss_mask: 0.0068 (0.0267)  time: 0.1715  data: 0.0002  max mem: 6052
[04:49:51.863583] Epoch: [47]  [500/781]  eta: 0:00:48  lr: 0.000145  training_loss: 1.3747 (1.3945)  classification_loss: 1.3666 (1.3686)  loss_mask: 0.0063 (0.0259)  time: 0.1709  data: 0.0002  max mem: 6052
[04:49:55.289956] Epoch: [47]  [520/781]  eta: 0:00:45  lr: 0.000145  training_loss: 1.3596 (1.3937)  classification_loss: 1.3555 (1.3686)  loss_mask: 0.0056 (0.0252)  time: 0.1712  data: 0.0002  max mem: 6052
[04:49:58.687091] Epoch: [47]  [540/781]  eta: 0:00:41  lr: 0.000145  training_loss: 1.3275 (1.3938)  classification_loss: 1.3249 (1.3694)  loss_mask: 0.0038 (0.0244)  time: 0.1698  data: 0.0003  max mem: 6052
[04:50:02.088777] Epoch: [47]  [560/781]  eta: 0:00:38  lr: 0.000145  training_loss: 1.3576 (1.3938)  classification_loss: 1.3539 (1.3702)  loss_mask: 0.0031 (0.0236)  time: 0.1700  data: 0.0003  max mem: 6052
[04:50:05.501751] Epoch: [47]  [580/781]  eta: 0:00:34  lr: 0.000145  training_loss: 1.3969 (1.3936)  classification_loss: 1.3937 (1.3707)  loss_mask: 0.0030 (0.0229)  time: 0.1706  data: 0.0003  max mem: 6052
[04:50:08.908049] Epoch: [47]  [600/781]  eta: 0:00:31  lr: 0.000145  training_loss: 1.3923 (1.3954)  classification_loss: 1.3876 (1.3717)  loss_mask: 0.0080 (0.0237)  time: 0.1702  data: 0.0003  max mem: 6052
[04:50:12.325744] Epoch: [47]  [620/781]  eta: 0:00:27  lr: 0.000145  training_loss: 1.4615 (1.3991)  classification_loss: 1.4278 (1.3740)  loss_mask: 0.0279 (0.0251)  time: 0.1708  data: 0.0002  max mem: 6052
[04:50:15.741386] Epoch: [47]  [640/781]  eta: 0:00:24  lr: 0.000145  training_loss: 1.4925 (1.4013)  classification_loss: 1.4292 (1.3757)  loss_mask: 0.0350 (0.0256)  time: 0.1707  data: 0.0003  max mem: 6052
[04:50:19.170708] Epoch: [47]  [660/781]  eta: 0:00:20  lr: 0.000145  training_loss: 1.3920 (1.4013)  classification_loss: 1.3657 (1.3758)  loss_mask: 0.0187 (0.0256)  time: 0.1714  data: 0.0004  max mem: 6052
[04:50:22.623661] Epoch: [47]  [680/781]  eta: 0:00:17  lr: 0.000144  training_loss: 1.3852 (1.4016)  classification_loss: 1.3576 (1.3765)  loss_mask: 0.0099 (0.0252)  time: 0.1726  data: 0.0002  max mem: 6052
[04:50:26.030001] Epoch: [47]  [700/781]  eta: 0:00:13  lr: 0.000144  training_loss: 1.3983 (1.4028)  classification_loss: 1.3812 (1.3778)  loss_mask: 0.0092 (0.0250)  time: 0.1702  data: 0.0003  max mem: 6052
[04:50:29.444795] Epoch: [47]  [720/781]  eta: 0:00:10  lr: 0.000144  training_loss: 1.3628 (1.4022)  classification_loss: 1.3570 (1.3778)  loss_mask: 0.0044 (0.0244)  time: 0.1706  data: 0.0002  max mem: 6052
[04:50:32.864904] Epoch: [47]  [740/781]  eta: 0:00:07  lr: 0.000144  training_loss: 1.3376 (1.4012)  classification_loss: 1.3305 (1.3773)  loss_mask: 0.0045 (0.0239)  time: 0.1709  data: 0.0003  max mem: 6052
[04:50:36.276598] Epoch: [47]  [760/781]  eta: 0:00:03  lr: 0.000144  training_loss: 1.3919 (1.4007)  classification_loss: 1.3867 (1.3773)  loss_mask: 0.0046 (0.0234)  time: 0.1705  data: 0.0002  max mem: 6052
[04:50:39.666085] Epoch: [47]  [780/781]  eta: 0:00:00  lr: 0.000144  training_loss: 1.3674 (1.4002)  classification_loss: 1.3647 (1.3773)  loss_mask: 0.0025 (0.0229)  time: 0.1694  data: 0.0002  max mem: 6052
[04:50:39.822574] Epoch: [47] Total time: 0:02:14 (0.1725 s / it)
[04:50:39.824130] Averaged stats: lr: 0.000144  training_loss: 1.3674 (1.4002)  classification_loss: 1.3647 (1.3773)  loss_mask: 0.0025 (0.0229)
[04:50:40.499857] Test:  [  0/157]  eta: 0:01:45  testing_loss: 0.5924 (0.5924)  acc1: 82.8125 (82.8125)  acc5: 98.4375 (98.4375)  time: 0.6708  data: 0.6409  max mem: 6052
[04:50:40.787014] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.6750 (0.6725)  acc1: 79.6875 (78.1250)  acc5: 98.4375 (98.8636)  time: 0.0869  data: 0.0584  max mem: 6052
[04:50:41.069739] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.6454 (0.6502)  acc1: 79.6875 (78.7202)  acc5: 98.4375 (98.8839)  time: 0.0283  data: 0.0002  max mem: 6052
[04:50:41.351301] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6454 (0.6610)  acc1: 79.6875 (78.7298)  acc5: 98.4375 (98.7399)  time: 0.0281  data: 0.0002  max mem: 6052
[04:50:41.633337] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6432 (0.6656)  acc1: 78.1250 (78.9253)  acc5: 98.4375 (98.5137)  time: 0.0281  data: 0.0001  max mem: 6052
[04:50:41.915358] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6270 (0.6540)  acc1: 79.6875 (79.6875)  acc5: 98.4375 (98.4988)  time: 0.0281  data: 0.0001  max mem: 6052
[04:50:42.197232] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6137 (0.6500)  acc1: 81.2500 (79.7387)  acc5: 98.4375 (98.4887)  time: 0.0281  data: 0.0002  max mem: 6052
[04:50:42.479839] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6326 (0.6446)  acc1: 79.6875 (79.8856)  acc5: 98.4375 (98.4815)  time: 0.0281  data: 0.0001  max mem: 6052
[04:50:42.761327] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6506 (0.6535)  acc1: 78.1250 (79.4946)  acc5: 98.4375 (98.4182)  time: 0.0281  data: 0.0001  max mem: 6052
[04:50:43.042506] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6437 (0.6484)  acc1: 79.6875 (79.8077)  acc5: 98.4375 (98.4890)  time: 0.0280  data: 0.0001  max mem: 6052
[04:50:43.324459] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.6283 (0.6520)  acc1: 79.6875 (79.5173)  acc5: 100.0000 (98.5149)  time: 0.0280  data: 0.0001  max mem: 6052
[04:50:43.606423] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6521 (0.6525)  acc1: 78.1250 (79.5467)  acc5: 100.0000 (98.5501)  time: 0.0281  data: 0.0001  max mem: 6052
[04:50:43.887498] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6239 (0.6487)  acc1: 81.2500 (79.6875)  acc5: 98.4375 (98.5537)  time: 0.0280  data: 0.0001  max mem: 6052
[04:50:44.169440] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6147 (0.6473)  acc1: 79.6875 (79.8187)  acc5: 98.4375 (98.5806)  time: 0.0280  data: 0.0001  max mem: 6052
[04:50:44.450916] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6458 (0.6455)  acc1: 79.6875 (79.8537)  acc5: 100.0000 (98.6480)  time: 0.0280  data: 0.0002  max mem: 6052
[04:50:44.730613] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6359 (0.6440)  acc1: 78.1250 (79.8013)  acc5: 100.0000 (98.6548)  time: 0.0279  data: 0.0001  max mem: 6052
[04:50:44.884608] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6184 (0.6447)  acc1: 81.2500 (79.7400)  acc5: 98.4375 (98.6800)  time: 0.0271  data: 0.0001  max mem: 6052
[04:50:45.038522] Test: Total time: 0:00:05 (0.0332 s / it)
[04:50:45.038969] * Acc@1 79.740 Acc@5 98.680 loss 0.645
[04:50:45.039298] Accuracy of the network on the 10000 test images: 79.7%
[04:50:45.039510] Max accuracy: 79.74%
[04:50:45.352828] log_dir: ./output_dir
[04:50:46.239227] Epoch: [48]  [  0/781]  eta: 0:11:30  lr: 0.000144  training_loss: 1.3328 (1.3328)  classification_loss: 1.3279 (1.3279)  loss_mask: 0.0049 (0.0049)  time: 0.8845  data: 0.6683  max mem: 6052
[04:50:49.692871] Epoch: [48]  [ 20/781]  eta: 0:02:37  lr: 0.000144  training_loss: 1.3506 (1.3607)  classification_loss: 1.3498 (1.3585)  loss_mask: 0.0020 (0.0022)  time: 0.1726  data: 0.0002  max mem: 6052
[04:50:53.106065] Epoch: [48]  [ 40/781]  eta: 0:02:20  lr: 0.000144  training_loss: 1.4259 (1.3768)  classification_loss: 1.4249 (1.3748)  loss_mask: 0.0017 (0.0021)  time: 0.1706  data: 0.0002  max mem: 6052
[04:50:56.581448] Epoch: [48]  [ 60/781]  eta: 0:02:12  lr: 0.000144  training_loss: 1.3994 (1.3939)  classification_loss: 1.3716 (1.3863)  loss_mask: 0.0035 (0.0076)  time: 0.1737  data: 0.0002  max mem: 6052
[04:51:00.099692] Epoch: [48]  [ 80/781]  eta: 0:02:07  lr: 0.000144  training_loss: 1.3982 (1.3901)  classification_loss: 1.3693 (1.3792)  loss_mask: 0.0059 (0.0109)  time: 0.1758  data: 0.0003  max mem: 6052
[04:51:03.533438] Epoch: [48]  [100/781]  eta: 0:02:02  lr: 0.000143  training_loss: 1.3717 (1.3892)  classification_loss: 1.3628 (1.3781)  loss_mask: 0.0083 (0.0111)  time: 0.1716  data: 0.0002  max mem: 6052
[04:51:06.970637] Epoch: [48]  [120/781]  eta: 0:01:58  lr: 0.000143  training_loss: 1.3184 (1.3808)  classification_loss: 1.3154 (1.3706)  loss_mask: 0.0037 (0.0101)  time: 0.1718  data: 0.0002  max mem: 6052
[04:51:10.395485] Epoch: [48]  [140/781]  eta: 0:01:53  lr: 0.000143  training_loss: 1.3448 (1.3735)  classification_loss: 1.3407 (1.3644)  loss_mask: 0.0019 (0.0091)  time: 0.1712  data: 0.0003  max mem: 6052
[04:51:13.807218] Epoch: [48]  [160/781]  eta: 0:01:49  lr: 0.000143  training_loss: 1.3295 (1.3688)  classification_loss: 1.3267 (1.3606)  loss_mask: 0.0022 (0.0082)  time: 0.1705  data: 0.0002  max mem: 6052
[04:51:17.220349] Epoch: [48]  [180/781]  eta: 0:01:45  lr: 0.000143  training_loss: 1.3397 (1.3691)  classification_loss: 1.3363 (1.3615)  loss_mask: 0.0018 (0.0076)  time: 0.1706  data: 0.0002  max mem: 6052
[04:51:20.626837] Epoch: [48]  [200/781]  eta: 0:01:41  lr: 0.000143  training_loss: 1.3340 (1.3674)  classification_loss: 1.3328 (1.3605)  loss_mask: 0.0012 (0.0070)  time: 0.1703  data: 0.0002  max mem: 6052
[04:51:24.069771] Epoch: [48]  [220/781]  eta: 0:01:38  lr: 0.000143  training_loss: 1.3527 (1.3676)  classification_loss: 1.3509 (1.3611)  loss_mask: 0.0011 (0.0064)  time: 0.1721  data: 0.0002  max mem: 6052
[04:51:27.483082] Epoch: [48]  [240/781]  eta: 0:01:34  lr: 0.000143  training_loss: 1.2738 (1.3639)  classification_loss: 1.2727 (1.3579)  loss_mask: 0.0011 (0.0060)  time: 0.1706  data: 0.0002  max mem: 6052
[04:51:30.902578] Epoch: [48]  [260/781]  eta: 0:01:30  lr: 0.000143  training_loss: 1.3373 (1.3622)  classification_loss: 1.3354 (1.3565)  loss_mask: 0.0014 (0.0057)  time: 0.1709  data: 0.0002  max mem: 6052
[04:51:34.332896] Epoch: [48]  [280/781]  eta: 0:01:27  lr: 0.000142  training_loss: 1.3865 (1.3623)  classification_loss: 1.3851 (1.3569)  loss_mask: 0.0019 (0.0054)  time: 0.1714  data: 0.0002  max mem: 6052
[04:51:37.746930] Epoch: [48]  [300/781]  eta: 0:01:23  lr: 0.000142  training_loss: 1.3871 (1.3629)  classification_loss: 1.3867 (1.3577)  loss_mask: 0.0009 (0.0051)  time: 0.1706  data: 0.0003  max mem: 6052
[04:51:41.165522] Epoch: [48]  [320/781]  eta: 0:01:20  lr: 0.000142  training_loss: 1.3530 (1.3639)  classification_loss: 1.3524 (1.3590)  loss_mask: 0.0008 (0.0049)  time: 0.1709  data: 0.0002  max mem: 6052
[04:51:44.575229] Epoch: [48]  [340/781]  eta: 0:01:16  lr: 0.000142  training_loss: 1.3659 (1.3644)  classification_loss: 1.3653 (1.3597)  loss_mask: 0.0010 (0.0046)  time: 0.1704  data: 0.0002  max mem: 6052
[04:51:47.997576] Epoch: [48]  [360/781]  eta: 0:01:13  lr: 0.000142  training_loss: 1.3516 (1.3643)  classification_loss: 1.3508 (1.3599)  loss_mask: 0.0009 (0.0044)  time: 0.1710  data: 0.0002  max mem: 6052
[04:51:51.429526] Epoch: [48]  [380/781]  eta: 0:01:09  lr: 0.000142  training_loss: 1.3493 (1.3634)  classification_loss: 1.3487 (1.3592)  loss_mask: 0.0006 (0.0042)  time: 0.1715  data: 0.0002  max mem: 6052
[04:51:54.852626] Epoch: [48]  [400/781]  eta: 0:01:05  lr: 0.000142  training_loss: 1.3063 (1.3624)  classification_loss: 1.3058 (1.3583)  loss_mask: 0.0005 (0.0041)  time: 0.1711  data: 0.0002  max mem: 6052
[04:51:58.309036] Epoch: [48]  [420/781]  eta: 0:01:02  lr: 0.000142  training_loss: 1.4307 (1.3655)  classification_loss: 1.4302 (1.3616)  loss_mask: 0.0006 (0.0039)  time: 0.1727  data: 0.0002  max mem: 6052
[04:52:01.721125] Epoch: [48]  [440/781]  eta: 0:00:59  lr: 0.000142  training_loss: 1.4135 (1.3669)  classification_loss: 1.3352 (1.3610)  loss_mask: 0.0064 (0.0058)  time: 0.1705  data: 0.0002  max mem: 6052
[04:52:05.130052] Epoch: [48]  [460/781]  eta: 0:00:55  lr: 0.000142  training_loss: 1.4663 (1.3722)  classification_loss: 1.3004 (1.3590)  loss_mask: 0.1333 (0.0133)  time: 0.1704  data: 0.0003  max mem: 6052
[04:52:08.580314] Epoch: [48]  [480/781]  eta: 0:00:52  lr: 0.000141  training_loss: 1.6221 (1.3825)  classification_loss: 1.3362 (1.3599)  loss_mask: 0.1817 (0.0226)  time: 0.1724  data: 0.0002  max mem: 6052
[04:52:12.009607] Epoch: [48]  [500/781]  eta: 0:00:48  lr: 0.000141  training_loss: 1.4110 (1.3847)  classification_loss: 1.3316 (1.3600)  loss_mask: 0.0574 (0.0247)  time: 0.1714  data: 0.0002  max mem: 6052
[04:52:15.414295] Epoch: [48]  [520/781]  eta: 0:00:45  lr: 0.000141  training_loss: 1.4017 (1.3853)  classification_loss: 1.3540 (1.3604)  loss_mask: 0.0246 (0.0249)  time: 0.1702  data: 0.0002  max mem: 6052
[04:52:18.829918] Epoch: [48]  [540/781]  eta: 0:00:41  lr: 0.000141  training_loss: 1.3910 (1.3857)  classification_loss: 1.3738 (1.3610)  loss_mask: 0.0162 (0.0247)  time: 0.1707  data: 0.0002  max mem: 6052
[04:52:22.251003] Epoch: [48]  [560/781]  eta: 0:00:38  lr: 0.000141  training_loss: 1.3943 (1.3847)  classification_loss: 1.3810 (1.3602)  loss_mask: 0.0151 (0.0245)  time: 0.1710  data: 0.0003  max mem: 6052
[04:52:25.658686] Epoch: [48]  [580/781]  eta: 0:00:34  lr: 0.000141  training_loss: 1.3389 (1.3841)  classification_loss: 1.3272 (1.3601)  loss_mask: 0.0117 (0.0240)  time: 0.1703  data: 0.0002  max mem: 6052
[04:52:29.064418] Epoch: [48]  [600/781]  eta: 0:00:31  lr: 0.000141  training_loss: 1.3317 (1.3826)  classification_loss: 1.3240 (1.3591)  loss_mask: 0.0064 (0.0235)  time: 0.1702  data: 0.0003  max mem: 6052
[04:52:32.465795] Epoch: [48]  [620/781]  eta: 0:00:27  lr: 0.000141  training_loss: 1.3860 (1.3827)  classification_loss: 1.3841 (1.3596)  loss_mask: 0.0073 (0.0231)  time: 0.1700  data: 0.0002  max mem: 6052
[04:52:35.872801] Epoch: [48]  [640/781]  eta: 0:00:24  lr: 0.000141  training_loss: 1.3045 (1.3812)  classification_loss: 1.2993 (1.3586)  loss_mask: 0.0052 (0.0226)  time: 0.1703  data: 0.0002  max mem: 6052
[04:52:39.286623] Epoch: [48]  [660/781]  eta: 0:00:20  lr: 0.000141  training_loss: 1.3800 (1.3812)  classification_loss: 1.3743 (1.3591)  loss_mask: 0.0058 (0.0221)  time: 0.1706  data: 0.0002  max mem: 6052
[04:52:42.693098] Epoch: [48]  [680/781]  eta: 0:00:17  lr: 0.000140  training_loss: 1.3569 (1.3810)  classification_loss: 1.3519 (1.3594)  loss_mask: 0.0038 (0.0216)  time: 0.1702  data: 0.0002  max mem: 6052
[04:52:46.097610] Epoch: [48]  [700/781]  eta: 0:00:13  lr: 0.000140  training_loss: 1.3647 (1.3807)  classification_loss: 1.3187 (1.3591)  loss_mask: 0.0102 (0.0215)  time: 0.1701  data: 0.0002  max mem: 6052
[04:52:49.507764] Epoch: [48]  [720/781]  eta: 0:00:10  lr: 0.000140  training_loss: 1.4477 (1.3817)  classification_loss: 1.4208 (1.3604)  loss_mask: 0.0054 (0.0212)  time: 0.1704  data: 0.0003  max mem: 6052
[04:52:52.919702] Epoch: [48]  [740/781]  eta: 0:00:07  lr: 0.000140  training_loss: 1.3515 (1.3807)  classification_loss: 1.3459 (1.3599)  loss_mask: 0.0048 (0.0209)  time: 0.1705  data: 0.0005  max mem: 6052
[04:52:56.343879] Epoch: [48]  [760/781]  eta: 0:00:03  lr: 0.000140  training_loss: 1.4016 (1.3808)  classification_loss: 1.3941 (1.3603)  loss_mask: 0.0073 (0.0205)  time: 0.1711  data: 0.0002  max mem: 6052
[04:52:59.756677] Epoch: [48]  [780/781]  eta: 0:00:00  lr: 0.000140  training_loss: 1.3809 (1.3813)  classification_loss: 1.3730 (1.3610)  loss_mask: 0.0042 (0.0204)  time: 0.1706  data: 0.0002  max mem: 6052
[04:52:59.913086] Epoch: [48] Total time: 0:02:14 (0.1723 s / it)
[04:52:59.913555] Averaged stats: lr: 0.000140  training_loss: 1.3809 (1.3813)  classification_loss: 1.3730 (1.3610)  loss_mask: 0.0042 (0.0204)
[04:53:00.576942] Test:  [  0/157]  eta: 0:01:43  testing_loss: 0.5736 (0.5736)  acc1: 84.3750 (84.3750)  acc5: 96.8750 (96.8750)  time: 0.6589  data: 0.6253  max mem: 6052
[04:53:00.865356] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.6272 (0.6483)  acc1: 79.6875 (80.3977)  acc5: 98.4375 (98.8636)  time: 0.0858  data: 0.0570  max mem: 6052
[04:53:01.153902] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.6294 (0.6316)  acc1: 78.1250 (80.5060)  acc5: 98.4375 (99.0327)  time: 0.0286  data: 0.0002  max mem: 6052
[04:53:01.441395] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6294 (0.6443)  acc1: 79.6875 (79.8387)  acc5: 100.0000 (98.9919)  time: 0.0287  data: 0.0001  max mem: 6052
[04:53:01.723156] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6371 (0.6493)  acc1: 78.1250 (79.4588)  acc5: 98.4375 (98.8567)  time: 0.0283  data: 0.0001  max mem: 6052
[04:53:02.003893] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5990 (0.6389)  acc1: 79.6875 (80.0551)  acc5: 98.4375 (98.8664)  time: 0.0280  data: 0.0001  max mem: 6052
[04:53:02.285854] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5921 (0.6417)  acc1: 79.6875 (79.8668)  acc5: 98.4375 (98.8730)  time: 0.0280  data: 0.0001  max mem: 6052
[04:53:02.569445] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6177 (0.6391)  acc1: 79.6875 (79.9296)  acc5: 98.4375 (98.8336)  time: 0.0282  data: 0.0001  max mem: 6052
[04:53:02.850375] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6397 (0.6451)  acc1: 79.6875 (79.7647)  acc5: 98.4375 (98.7654)  time: 0.0281  data: 0.0001  max mem: 6052
[04:53:03.133132] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6377 (0.6398)  acc1: 79.6875 (80.0137)  acc5: 98.4375 (98.7981)  time: 0.0281  data: 0.0001  max mem: 6052
[04:53:03.414005] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.6319 (0.6432)  acc1: 79.6875 (79.9660)  acc5: 98.4375 (98.7933)  time: 0.0281  data: 0.0001  max mem: 6052
[04:53:03.694614] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6347 (0.6435)  acc1: 79.6875 (80.0113)  acc5: 100.0000 (98.8316)  time: 0.0280  data: 0.0001  max mem: 6052
[04:53:03.975809] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6134 (0.6398)  acc1: 81.2500 (80.1007)  acc5: 100.0000 (98.8765)  time: 0.0280  data: 0.0001  max mem: 6052
[04:53:04.256670] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5967 (0.6380)  acc1: 81.2500 (80.2600)  acc5: 100.0000 (98.9265)  time: 0.0280  data: 0.0001  max mem: 6052
[04:53:04.536947] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5940 (0.6348)  acc1: 82.8125 (80.4078)  acc5: 100.0000 (98.9473)  time: 0.0279  data: 0.0001  max mem: 6052
[04:53:04.816958] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6221 (0.6342)  acc1: 79.6875 (80.4015)  acc5: 100.0000 (98.9549)  time: 0.0279  data: 0.0001  max mem: 6052
[04:53:04.969218] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6414 (0.6339)  acc1: 79.6875 (80.4700)  acc5: 100.0000 (98.9700)  time: 0.0271  data: 0.0001  max mem: 6052
[04:53:05.127889] Test: Total time: 0:00:05 (0.0332 s / it)
[04:53:05.128365] * Acc@1 80.470 Acc@5 98.970 loss 0.634
[04:53:05.128705] Accuracy of the network on the 10000 test images: 80.5%
[04:53:05.128907] Max accuracy: 80.47%
[04:53:05.290756] log_dir: ./output_dir
[04:53:06.195946] Epoch: [49]  [  0/781]  eta: 0:11:45  lr: 0.000140  training_loss: 1.3516 (1.3516)  classification_loss: 1.3254 (1.3254)  loss_mask: 0.0262 (0.0262)  time: 0.9036  data: 0.7163  max mem: 6052
[04:53:09.648509] Epoch: [49]  [ 20/781]  eta: 0:02:37  lr: 0.000140  training_loss: 1.3699 (1.3684)  classification_loss: 1.3631 (1.3617)  loss_mask: 0.0044 (0.0067)  time: 0.1725  data: 0.0003  max mem: 6052
[04:53:13.123995] Epoch: [49]  [ 40/781]  eta: 0:02:21  lr: 0.000140  training_loss: 1.3524 (1.3724)  classification_loss: 1.3498 (1.3674)  loss_mask: 0.0025 (0.0050)  time: 0.1737  data: 0.0002  max mem: 6052
[04:53:16.572623] Epoch: [49]  [ 60/781]  eta: 0:02:13  lr: 0.000140  training_loss: 1.3333 (1.3656)  classification_loss: 1.3315 (1.3616)  loss_mask: 0.0017 (0.0040)  time: 0.1724  data: 0.0002  max mem: 6052
[04:53:20.023653] Epoch: [49]  [ 80/781]  eta: 0:02:07  lr: 0.000139  training_loss: 1.3452 (1.3606)  classification_loss: 1.3435 (1.3571)  loss_mask: 0.0017 (0.0035)  time: 0.1725  data: 0.0003  max mem: 6052
[04:53:23.431227] Epoch: [49]  [100/781]  eta: 0:02:02  lr: 0.000139  training_loss: 1.3164 (1.3557)  classification_loss: 1.3144 (1.3526)  loss_mask: 0.0016 (0.0031)  time: 0.1703  data: 0.0002  max mem: 6052
[04:53:26.874713] Epoch: [49]  [120/781]  eta: 0:01:57  lr: 0.000139  training_loss: 1.3154 (1.3519)  classification_loss: 1.3133 (1.3489)  loss_mask: 0.0018 (0.0029)  time: 0.1720  data: 0.0002  max mem: 6052
[04:53:30.289017] Epoch: [49]  [140/781]  eta: 0:01:53  lr: 0.000139  training_loss: 1.4021 (1.3565)  classification_loss: 1.3814 (1.3524)  loss_mask: 0.0015 (0.0040)  time: 0.1706  data: 0.0002  max mem: 6052
[04:53:33.702088] Epoch: [49]  [160/781]  eta: 0:01:49  lr: 0.000139  training_loss: 1.3537 (1.3559)  classification_loss: 1.3469 (1.3519)  loss_mask: 0.0021 (0.0039)  time: 0.1706  data: 0.0003  max mem: 6052
[04:53:37.117888] Epoch: [49]  [180/781]  eta: 0:01:45  lr: 0.000139  training_loss: 1.3218 (1.3545)  classification_loss: 1.3196 (1.3508)  loss_mask: 0.0017 (0.0037)  time: 0.1707  data: 0.0003  max mem: 6052
[04:53:40.566440] Epoch: [49]  [200/781]  eta: 0:01:41  lr: 0.000139  training_loss: 1.3137 (1.3538)  classification_loss: 1.3119 (1.3503)  loss_mask: 0.0010 (0.0035)  time: 0.1724  data: 0.0003  max mem: 6052
[04:53:43.974512] Epoch: [49]  [220/781]  eta: 0:01:38  lr: 0.000139  training_loss: 1.2889 (1.3535)  classification_loss: 1.2885 (1.3502)  loss_mask: 0.0009 (0.0032)  time: 0.1703  data: 0.0002  max mem: 6052
[04:53:47.406454] Epoch: [49]  [240/781]  eta: 0:01:34  lr: 0.000139  training_loss: 1.3884 (1.3549)  classification_loss: 1.3876 (1.3518)  loss_mask: 0.0008 (0.0031)  time: 0.1715  data: 0.0002  max mem: 6052
[04:53:50.877538] Epoch: [49]  [260/781]  eta: 0:01:30  lr: 0.000139  training_loss: 1.3645 (1.3562)  classification_loss: 1.3639 (1.3532)  loss_mask: 0.0013 (0.0030)  time: 0.1735  data: 0.0001  max mem: 6052
[04:53:54.300498] Epoch: [49]  [280/781]  eta: 0:01:27  lr: 0.000138  training_loss: 1.3597 (1.3579)  classification_loss: 1.3524 (1.3546)  loss_mask: 0.0030 (0.0033)  time: 0.1711  data: 0.0002  max mem: 6052
[04:53:57.716019] Epoch: [49]  [300/781]  eta: 0:01:23  lr: 0.000138  training_loss: 1.3242 (1.3588)  classification_loss: 1.3213 (1.3554)  loss_mask: 0.0034 (0.0033)  time: 0.1707  data: 0.0002  max mem: 6052
[04:54:01.129537] Epoch: [49]  [320/781]  eta: 0:01:20  lr: 0.000138  training_loss: 1.3739 (1.3590)  classification_loss: 1.3728 (1.3555)  loss_mask: 0.0015 (0.0035)  time: 0.1706  data: 0.0002  max mem: 6052
[04:54:04.532229] Epoch: [49]  [340/781]  eta: 0:01:16  lr: 0.000138  training_loss: 1.3060 (1.3564)  classification_loss: 1.3033 (1.3529)  loss_mask: 0.0022 (0.0034)  time: 0.1700  data: 0.0003  max mem: 6052
[04:54:07.945795] Epoch: [49]  [360/781]  eta: 0:01:13  lr: 0.000138  training_loss: 1.3515 (1.3563)  classification_loss: 1.3507 (1.3530)  loss_mask: 0.0010 (0.0033)  time: 0.1705  data: 0.0002  max mem: 6052
[04:54:11.419657] Epoch: [49]  [380/781]  eta: 0:01:09  lr: 0.000138  training_loss: 1.3339 (1.3555)  classification_loss: 1.3335 (1.3524)  loss_mask: 0.0005 (0.0032)  time: 0.1736  data: 0.0002  max mem: 6052
[04:54:14.852081] Epoch: [49]  [400/781]  eta: 0:01:06  lr: 0.000138  training_loss: 1.3904 (1.3563)  classification_loss: 1.3899 (1.3532)  loss_mask: 0.0005 (0.0030)  time: 0.1715  data: 0.0003  max mem: 6052
[04:54:18.276176] Epoch: [49]  [420/781]  eta: 0:01:02  lr: 0.000138  training_loss: 1.3528 (1.3567)  classification_loss: 1.3523 (1.3538)  loss_mask: 0.0006 (0.0029)  time: 0.1711  data: 0.0002  max mem: 6052
[04:54:21.736399] Epoch: [49]  [440/781]  eta: 0:00:59  lr: 0.000138  training_loss: 1.3145 (1.3563)  classification_loss: 1.3143 (1.3535)  loss_mask: 0.0005 (0.0028)  time: 0.1729  data: 0.0002  max mem: 6052
[04:54:25.171341] Epoch: [49]  [460/781]  eta: 0:00:55  lr: 0.000137  training_loss: 1.3147 (1.3566)  classification_loss: 1.3138 (1.3536)  loss_mask: 0.0010 (0.0030)  time: 0.1717  data: 0.0003  max mem: 6052
[04:54:28.571792] Epoch: [49]  [480/781]  eta: 0:00:52  lr: 0.000137  training_loss: 1.3808 (1.3598)  classification_loss: 1.3476 (1.3542)  loss_mask: 0.0098 (0.0056)  time: 0.1700  data: 0.0002  max mem: 6052
[04:54:31.977975] Epoch: [49]  [500/781]  eta: 0:00:48  lr: 0.000137  training_loss: 1.4826 (1.3659)  classification_loss: 1.3278 (1.3530)  loss_mask: 0.1313 (0.0129)  time: 0.1702  data: 0.0002  max mem: 6052
[04:54:35.385250] Epoch: [49]  [520/781]  eta: 0:00:45  lr: 0.000137  training_loss: 1.4464 (1.3686)  classification_loss: 1.3537 (1.3539)  loss_mask: 0.0445 (0.0148)  time: 0.1703  data: 0.0002  max mem: 6052
[04:54:38.803505] Epoch: [49]  [540/781]  eta: 0:00:41  lr: 0.000137  training_loss: 1.3742 (1.3693)  classification_loss: 1.3563 (1.3543)  loss_mask: 0.0151 (0.0150)  time: 0.1708  data: 0.0002  max mem: 6052
[04:54:42.195882] Epoch: [49]  [560/781]  eta: 0:00:38  lr: 0.000137  training_loss: 1.3681 (1.3702)  classification_loss: 1.3620 (1.3554)  loss_mask: 0.0083 (0.0148)  time: 0.1695  data: 0.0002  max mem: 6052
[04:54:45.603954] Epoch: [49]  [580/781]  eta: 0:00:34  lr: 0.000137  training_loss: 1.3430 (1.3694)  classification_loss: 1.3361 (1.3549)  loss_mask: 0.0058 (0.0145)  time: 0.1703  data: 0.0002  max mem: 6052
[04:54:49.003523] Epoch: [49]  [600/781]  eta: 0:00:31  lr: 0.000137  training_loss: 1.3129 (1.3682)  classification_loss: 1.3122 (1.3540)  loss_mask: 0.0036 (0.0142)  time: 0.1699  data: 0.0002  max mem: 6052
[04:54:52.450233] Epoch: [49]  [620/781]  eta: 0:00:27  lr: 0.000137  training_loss: 1.3106 (1.3673)  classification_loss: 1.3058 (1.3535)  loss_mask: 0.0035 (0.0138)  time: 0.1723  data: 0.0003  max mem: 6052
[04:54:55.894954] Epoch: [49]  [640/781]  eta: 0:00:24  lr: 0.000137  training_loss: 1.3371 (1.3670)  classification_loss: 1.3346 (1.3536)  loss_mask: 0.0026 (0.0135)  time: 0.1721  data: 0.0002  max mem: 6052
[04:54:59.411177] Epoch: [49]  [660/781]  eta: 0:00:20  lr: 0.000136  training_loss: 1.3943 (1.3676)  classification_loss: 1.3927 (1.3545)  loss_mask: 0.0018 (0.0131)  time: 0.1757  data: 0.0003  max mem: 6052
[04:55:02.869903] Epoch: [49]  [680/781]  eta: 0:00:17  lr: 0.000136  training_loss: 1.3713 (1.3678)  classification_loss: 1.3696 (1.3548)  loss_mask: 0.0036 (0.0130)  time: 0.1729  data: 0.0003  max mem: 6052
[04:55:06.357163] Epoch: [49]  [700/781]  eta: 0:00:13  lr: 0.000136  training_loss: 1.3716 (1.3680)  classification_loss: 1.3698 (1.3552)  loss_mask: 0.0054 (0.0129)  time: 0.1743  data: 0.0004  max mem: 6052
[04:55:09.815652] Epoch: [49]  [720/781]  eta: 0:00:10  lr: 0.000136  training_loss: 1.3554 (1.3679)  classification_loss: 1.3471 (1.3553)  loss_mask: 0.0042 (0.0126)  time: 0.1729  data: 0.0003  max mem: 6052
[04:55:13.241123] Epoch: [49]  [740/781]  eta: 0:00:07  lr: 0.000136  training_loss: 1.3219 (1.3671)  classification_loss: 1.3208 (1.3547)  loss_mask: 0.0014 (0.0124)  time: 0.1712  data: 0.0002  max mem: 6052
[04:55:16.714520] Epoch: [49]  [760/781]  eta: 0:00:03  lr: 0.000136  training_loss: 1.3701 (1.3675)  classification_loss: 1.3693 (1.3554)  loss_mask: 0.0011 (0.0121)  time: 0.1736  data: 0.0002  max mem: 6052
[04:55:20.144109] Epoch: [49]  [780/781]  eta: 0:00:00  lr: 0.000136  training_loss: 1.3648 (1.3678)  classification_loss: 1.3636 (1.3560)  loss_mask: 0.0013 (0.0118)  time: 0.1714  data: 0.0002  max mem: 6052
[04:55:20.316174] Epoch: [49] Total time: 0:02:15 (0.1729 s / it)
[04:55:20.316840] Averaged stats: lr: 0.000136  training_loss: 1.3648 (1.3678)  classification_loss: 1.3636 (1.3560)  loss_mask: 0.0013 (0.0118)
[04:55:20.979695] Test:  [  0/157]  eta: 0:01:43  testing_loss: 0.6286 (0.6286)  acc1: 79.6875 (79.6875)  acc5: 96.8750 (96.8750)  time: 0.6584  data: 0.6293  max mem: 6052
[04:55:21.266354] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.6525 (0.6694)  acc1: 79.6875 (78.9773)  acc5: 98.4375 (98.7216)  time: 0.0857  data: 0.0576  max mem: 6052
[04:55:21.552673] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.6411 (0.6482)  acc1: 79.6875 (79.7619)  acc5: 98.4375 (98.7351)  time: 0.0284  data: 0.0004  max mem: 6052
[04:55:21.837092] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6515 (0.6561)  acc1: 79.6875 (79.7883)  acc5: 98.4375 (98.7903)  time: 0.0284  data: 0.0003  max mem: 6052
[04:55:22.118956] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6584 (0.6616)  acc1: 79.6875 (79.6494)  acc5: 98.4375 (98.6662)  time: 0.0282  data: 0.0003  max mem: 6052
[04:55:22.406186] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6239 (0.6533)  acc1: 79.6875 (79.9939)  acc5: 98.4375 (98.7132)  time: 0.0283  data: 0.0004  max mem: 6052
[04:55:22.689347] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6093 (0.6515)  acc1: 81.2500 (79.8924)  acc5: 98.4375 (98.7449)  time: 0.0284  data: 0.0004  max mem: 6052
[04:55:22.977303] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6347 (0.6480)  acc1: 79.6875 (79.7315)  acc5: 98.4375 (98.7676)  time: 0.0284  data: 0.0001  max mem: 6052
[04:55:23.265102] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6437 (0.6602)  acc1: 76.5625 (79.1281)  acc5: 98.4375 (98.7269)  time: 0.0287  data: 0.0001  max mem: 6052
[04:55:23.548842] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6703 (0.6557)  acc1: 76.5625 (79.3098)  acc5: 98.4375 (98.8152)  time: 0.0285  data: 0.0001  max mem: 6052
[04:55:23.833419] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.6121 (0.6592)  acc1: 79.6875 (79.1615)  acc5: 100.0000 (98.8243)  time: 0.0283  data: 0.0001  max mem: 6052
[04:55:24.118372] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6763 (0.6603)  acc1: 78.1250 (79.1807)  acc5: 100.0000 (98.8316)  time: 0.0283  data: 0.0002  max mem: 6052
[04:55:24.402413] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6460 (0.6563)  acc1: 79.6875 (79.3776)  acc5: 100.0000 (98.8507)  time: 0.0283  data: 0.0002  max mem: 6052
[04:55:24.689131] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6419 (0.6555)  acc1: 79.6875 (79.2820)  acc5: 100.0000 (98.8788)  time: 0.0284  data: 0.0002  max mem: 6052
[04:55:24.971287] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6522 (0.6540)  acc1: 76.5625 (79.3107)  acc5: 100.0000 (98.9029)  time: 0.0283  data: 0.0002  max mem: 6052
[04:55:25.248385] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6660 (0.6527)  acc1: 79.6875 (79.3771)  acc5: 98.4375 (98.8721)  time: 0.0279  data: 0.0001  max mem: 6052
[04:55:25.399373] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6554 (0.6527)  acc1: 79.6875 (79.2600)  acc5: 98.4375 (98.8700)  time: 0.0268  data: 0.0001  max mem: 6052
[04:55:25.570429] Test: Total time: 0:00:05 (0.0334 s / it)
[04:55:25.570873] * Acc@1 79.260 Acc@5 98.870 loss 0.653
[04:55:25.571160] Accuracy of the network on the 10000 test images: 79.3%
[04:55:25.571336] Max accuracy: 80.47%
[04:55:25.740399] log_dir: ./output_dir
[04:55:26.627505] Epoch: [50]  [  0/781]  eta: 0:11:31  lr: 0.000136  training_loss: 1.1769 (1.1769)  classification_loss: 1.1760 (1.1760)  loss_mask: 0.0009 (0.0009)  time: 0.8853  data: 0.6731  max mem: 6052
[04:55:30.040545] Epoch: [50]  [ 20/781]  eta: 0:02:35  lr: 0.000136  training_loss: 1.2765 (1.2960)  classification_loss: 1.2750 (1.2948)  loss_mask: 0.0010 (0.0012)  time: 0.1706  data: 0.0002  max mem: 6052
[04:55:33.469260] Epoch: [50]  [ 40/781]  eta: 0:02:19  lr: 0.000136  training_loss: 1.3450 (1.3293)  classification_loss: 1.3291 (1.3228)  loss_mask: 0.0014 (0.0065)  time: 0.1714  data: 0.0002  max mem: 6052
[04:55:36.877703] Epoch: [50]  [ 60/781]  eta: 0:02:11  lr: 0.000135  training_loss: 1.4343 (1.3606)  classification_loss: 1.3412 (1.3311)  loss_mask: 0.0369 (0.0295)  time: 0.1704  data: 0.0003  max mem: 6052
[04:55:40.305643] Epoch: [50]  [ 80/781]  eta: 0:02:05  lr: 0.000135  training_loss: 1.3700 (1.3677)  classification_loss: 1.3263 (1.3356)  loss_mask: 0.0242 (0.0321)  time: 0.1713  data: 0.0002  max mem: 6052
[04:55:43.712267] Epoch: [50]  [100/781]  eta: 0:02:01  lr: 0.000135  training_loss: 1.3918 (1.3743)  classification_loss: 1.3762 (1.3449)  loss_mask: 0.0104 (0.0294)  time: 0.1703  data: 0.0002  max mem: 6052
[04:55:47.142636] Epoch: [50]  [120/781]  eta: 0:01:56  lr: 0.000135  training_loss: 1.3722 (1.3768)  classification_loss: 1.3407 (1.3485)  loss_mask: 0.0065 (0.0283)  time: 0.1714  data: 0.0002  max mem: 6052
[04:55:50.563619] Epoch: [50]  [140/781]  eta: 0:01:52  lr: 0.000135  training_loss: 1.3376 (1.3697)  classification_loss: 1.3339 (1.3445)  loss_mask: 0.0040 (0.0251)  time: 0.1710  data: 0.0002  max mem: 6052
[04:55:53.973826] Epoch: [50]  [160/781]  eta: 0:01:48  lr: 0.000135  training_loss: 1.2985 (1.3651)  classification_loss: 1.2959 (1.3428)  loss_mask: 0.0021 (0.0223)  time: 0.1704  data: 0.0001  max mem: 6052
[04:55:57.401503] Epoch: [50]  [180/781]  eta: 0:01:45  lr: 0.000135  training_loss: 1.2918 (1.3605)  classification_loss: 1.2904 (1.3405)  loss_mask: 0.0017 (0.0200)  time: 0.1713  data: 0.0002  max mem: 6052
[04:56:00.823393] Epoch: [50]  [200/781]  eta: 0:01:41  lr: 0.000135  training_loss: 1.3710 (1.3607)  classification_loss: 1.3685 (1.3424)  loss_mask: 0.0021 (0.0182)  time: 0.1710  data: 0.0003  max mem: 6052
[04:56:04.282557] Epoch: [50]  [220/781]  eta: 0:01:37  lr: 0.000135  training_loss: 1.3012 (1.3551)  classification_loss: 1.3001 (1.3383)  loss_mask: 0.0017 (0.0167)  time: 0.1729  data: 0.0003  max mem: 6052
[04:56:07.697003] Epoch: [50]  [240/781]  eta: 0:01:34  lr: 0.000135  training_loss: 1.3547 (1.3533)  classification_loss: 1.3530 (1.3378)  loss_mask: 0.0014 (0.0155)  time: 0.1706  data: 0.0002  max mem: 6052
[04:56:11.161743] Epoch: [50]  [260/781]  eta: 0:01:30  lr: 0.000134  training_loss: 1.3842 (1.3550)  classification_loss: 1.3834 (1.3406)  loss_mask: 0.0011 (0.0144)  time: 0.1732  data: 0.0002  max mem: 6052
[04:56:14.676279] Epoch: [50]  [280/781]  eta: 0:01:27  lr: 0.000134  training_loss: 1.3210 (1.3544)  classification_loss: 1.3202 (1.3409)  loss_mask: 0.0011 (0.0134)  time: 0.1756  data: 0.0002  max mem: 6052
[04:56:18.077854] Epoch: [50]  [300/781]  eta: 0:01:23  lr: 0.000134  training_loss: 1.3341 (1.3546)  classification_loss: 1.3337 (1.3420)  loss_mask: 0.0007 (0.0126)  time: 0.1700  data: 0.0002  max mem: 6052
[04:56:21.489395] Epoch: [50]  [320/781]  eta: 0:01:20  lr: 0.000134  training_loss: 1.3285 (1.3539)  classification_loss: 1.3272 (1.3420)  loss_mask: 0.0009 (0.0119)  time: 0.1705  data: 0.0002  max mem: 6052
[04:56:24.901815] Epoch: [50]  [340/781]  eta: 0:01:16  lr: 0.000134  training_loss: 1.3236 (1.3525)  classification_loss: 1.3182 (1.3410)  loss_mask: 0.0009 (0.0115)  time: 0.1706  data: 0.0002  max mem: 6052
[04:56:28.373531] Epoch: [50]  [360/781]  eta: 0:01:13  lr: 0.000134  training_loss: 1.3077 (1.3530)  classification_loss: 1.3061 (1.3416)  loss_mask: 0.0014 (0.0113)  time: 0.1735  data: 0.0002  max mem: 6052
[04:56:31.786003] Epoch: [50]  [380/781]  eta: 0:01:09  lr: 0.000134  training_loss: 1.3624 (1.3536)  classification_loss: 1.3564 (1.3419)  loss_mask: 0.0082 (0.0117)  time: 0.1705  data: 0.0002  max mem: 6052
[04:56:35.199631] Epoch: [50]  [400/781]  eta: 0:01:05  lr: 0.000134  training_loss: 1.4062 (1.3560)  classification_loss: 1.3951 (1.3437)  loss_mask: 0.0059 (0.0123)  time: 0.1706  data: 0.0002  max mem: 6052
[04:56:38.623132] Epoch: [50]  [420/781]  eta: 0:01:02  lr: 0.000134  training_loss: 1.3354 (1.3550)  classification_loss: 1.3288 (1.3422)  loss_mask: 0.0066 (0.0128)  time: 0.1711  data: 0.0002  max mem: 6052
[04:56:42.032695] Epoch: [50]  [440/781]  eta: 0:00:58  lr: 0.000133  training_loss: 1.2914 (1.3535)  classification_loss: 1.2856 (1.3410)  loss_mask: 0.0052 (0.0125)  time: 0.1704  data: 0.0002  max mem: 6052
[04:56:45.439624] Epoch: [50]  [460/781]  eta: 0:00:55  lr: 0.000133  training_loss: 1.2916 (1.3524)  classification_loss: 1.2876 (1.3401)  loss_mask: 0.0036 (0.0122)  time: 0.1703  data: 0.0001  max mem: 6052
[04:56:48.862202] Epoch: [50]  [480/781]  eta: 0:00:51  lr: 0.000133  training_loss: 1.4157 (1.3535)  classification_loss: 1.4002 (1.3414)  loss_mask: 0.0037 (0.0121)  time: 0.1711  data: 0.0002  max mem: 6052
[04:56:52.273776] Epoch: [50]  [500/781]  eta: 0:00:48  lr: 0.000133  training_loss: 1.5012 (1.3581)  classification_loss: 1.3542 (1.3423)  loss_mask: 0.0553 (0.0158)  time: 0.1705  data: 0.0002  max mem: 6052
[04:56:55.688371] Epoch: [50]  [520/781]  eta: 0:00:45  lr: 0.000133  training_loss: 1.3498 (1.3591)  classification_loss: 1.3216 (1.3417)  loss_mask: 0.0419 (0.0174)  time: 0.1707  data: 0.0002  max mem: 6052
[04:56:59.102346] Epoch: [50]  [540/781]  eta: 0:00:41  lr: 0.000133  training_loss: 1.4056 (1.3611)  classification_loss: 1.3854 (1.3431)  loss_mask: 0.0130 (0.0180)  time: 0.1706  data: 0.0002  max mem: 6052
[04:57:02.558396] Epoch: [50]  [560/781]  eta: 0:00:38  lr: 0.000133  training_loss: 1.3855 (1.3621)  classification_loss: 1.3497 (1.3434)  loss_mask: 0.0120 (0.0187)  time: 0.1727  data: 0.0003  max mem: 6052
[04:57:06.020292] Epoch: [50]  [580/781]  eta: 0:00:34  lr: 0.000133  training_loss: 1.3324 (1.3619)  classification_loss: 1.3219 (1.3436)  loss_mask: 0.0044 (0.0183)  time: 0.1730  data: 0.0003  max mem: 6052
[04:57:09.476471] Epoch: [50]  [600/781]  eta: 0:00:31  lr: 0.000133  training_loss: 1.2817 (1.3590)  classification_loss: 1.2787 (1.3412)  loss_mask: 0.0039 (0.0178)  time: 0.1727  data: 0.0002  max mem: 6052
[04:57:12.917998] Epoch: [50]  [620/781]  eta: 0:00:27  lr: 0.000133  training_loss: 1.3155 (1.3574)  classification_loss: 1.3125 (1.3401)  loss_mask: 0.0030 (0.0174)  time: 0.1720  data: 0.0003  max mem: 6052
[04:57:16.320578] Epoch: [50]  [640/781]  eta: 0:00:24  lr: 0.000132  training_loss: 1.3313 (1.3567)  classification_loss: 1.3288 (1.3398)  loss_mask: 0.0021 (0.0169)  time: 0.1701  data: 0.0002  max mem: 6052
[04:57:19.747748] Epoch: [50]  [660/781]  eta: 0:00:20  lr: 0.000132  training_loss: 1.3282 (1.3561)  classification_loss: 1.3266 (1.3397)  loss_mask: 0.0016 (0.0164)  time: 0.1713  data: 0.0002  max mem: 6052
[04:57:23.160543] Epoch: [50]  [680/781]  eta: 0:00:17  lr: 0.000132  training_loss: 1.3469 (1.3564)  classification_loss: 1.3454 (1.3404)  loss_mask: 0.0015 (0.0160)  time: 0.1706  data: 0.0002  max mem: 6052
[04:57:26.581748] Epoch: [50]  [700/781]  eta: 0:00:13  lr: 0.000132  training_loss: 1.3559 (1.3563)  classification_loss: 1.3547 (1.3407)  loss_mask: 0.0013 (0.0156)  time: 0.1710  data: 0.0002  max mem: 6052
[04:57:29.994997] Epoch: [50]  [720/781]  eta: 0:00:10  lr: 0.000132  training_loss: 1.3624 (1.3565)  classification_loss: 1.3589 (1.3413)  loss_mask: 0.0013 (0.0152)  time: 0.1706  data: 0.0002  max mem: 6052

[04:57:33.394128] Epoch: [50]  [740/781]  eta: 0:00:07  lr: 0.000132  training_loss: 1.3277 (1.3562)  classification_loss: 1.3265 (1.3414)  loss_mask: 0.0012 (0.0148)  time: 0.1698  data: 0.0002  max mem: 6052
[04:57:36.825562] Epoch: [50]  [760/781]  eta: 0:00:03  lr: 0.000132  training_loss: 1.3631 (1.3559)  classification_loss: 1.3620 (1.3414)  loss_mask: 0.0011 (0.0145)  time: 0.1715  data: 0.0002  max mem: 6052
[04:57:40.229513] Epoch: [50]  [780/781]  eta: 0:00:00  lr: 0.000132  training_loss: 1.3861 (1.3570)  classification_loss: 1.3858 (1.3429)  loss_mask: 0.0014 (0.0141)  time: 0.1701  data: 0.0002  max mem: 6052
[04:57:40.400349] Epoch: [50] Total time: 0:02:14 (0.1724 s / it)
[04:57:40.400850] Averaged stats: lr: 0.000132  training_loss: 1.3861 (1.3570)  classification_loss: 1.3858 (1.3429)  loss_mask: 0.0014 (0.0141)
[04:57:41.632488] Test:  [  0/157]  eta: 0:01:40  testing_loss: 0.5621 (0.5621)  acc1: 81.2500 (81.2500)  acc5: 100.0000 (100.0000)  time: 0.6396  data: 0.5993  max mem: 6052
[04:57:41.916342] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.6227 (0.6555)  acc1: 78.1250 (78.8352)  acc5: 100.0000 (99.4318)  time: 0.0838  data: 0.0546  max mem: 6052
[04:57:42.200645] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.6052 (0.6259)  acc1: 79.6875 (79.9107)  acc5: 100.0000 (99.2560)  time: 0.0283  data: 0.0002  max mem: 6052
[04:57:42.488147] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6331 (0.6417)  acc1: 79.6875 (79.4859)  acc5: 98.4375 (98.9919)  time: 0.0284  data: 0.0002  max mem: 6052
[04:57:42.773322] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6675 (0.6490)  acc1: 78.1250 (79.6113)  acc5: 98.4375 (99.0091)  time: 0.0285  data: 0.0002  max mem: 6052
[04:57:43.053729] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5992 (0.6382)  acc1: 81.2500 (79.9939)  acc5: 98.4375 (98.8664)  time: 0.0282  data: 0.0002  max mem: 6052
[04:57:43.332531] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6004 (0.6390)  acc1: 79.6875 (79.9180)  acc5: 100.0000 (98.9242)  time: 0.0278  data: 0.0002  max mem: 6052
[04:57:43.612508] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6083 (0.6342)  acc1: 79.6875 (80.1717)  acc5: 98.4375 (98.8776)  time: 0.0278  data: 0.0002  max mem: 6052
[04:57:43.892767] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6259 (0.6434)  acc1: 79.6875 (79.6682)  acc5: 98.4375 (98.9198)  time: 0.0279  data: 0.0002  max mem: 6052
[04:57:44.174770] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6255 (0.6390)  acc1: 79.6875 (79.8935)  acc5: 100.0000 (98.9183)  time: 0.0280  data: 0.0002  max mem: 6052
[04:57:44.455941] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.6255 (0.6418)  acc1: 81.2500 (79.7030)  acc5: 100.0000 (98.9325)  time: 0.0280  data: 0.0002  max mem: 6052
[04:57:44.735648] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6825 (0.6419)  acc1: 78.1250 (79.7720)  acc5: 100.0000 (98.9583)  time: 0.0279  data: 0.0002  max mem: 6052
[04:57:45.015574] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6083 (0.6375)  acc1: 81.2500 (79.9199)  acc5: 100.0000 (98.9928)  time: 0.0279  data: 0.0002  max mem: 6052
[04:57:45.295063] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6083 (0.6370)  acc1: 81.2500 (79.9260)  acc5: 100.0000 (98.9981)  time: 0.0278  data: 0.0001  max mem: 6052
[04:57:45.573956] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6105 (0.6352)  acc1: 79.6875 (80.0421)  acc5: 98.4375 (98.9694)  time: 0.0278  data: 0.0001  max mem: 6052
[04:57:45.851227] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6219 (0.6351)  acc1: 79.6875 (80.0393)  acc5: 98.4375 (98.9549)  time: 0.0277  data: 0.0001  max mem: 6052
[04:57:46.001060] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6219 (0.6343)  acc1: 81.2500 (80.0700)  acc5: 98.4375 (98.9500)  time: 0.0268  data: 0.0001  max mem: 6052
[04:57:46.165649] Test: Total time: 0:00:05 (0.0330 s / it)
[04:57:46.166604] * Acc@1 80.070 Acc@5 98.950 loss 0.634
[04:57:46.166999] Accuracy of the network on the 10000 test images: 80.1%
[04:57:46.167177] Max accuracy: 80.47%
[04:57:46.370319] log_dir: ./output_dir
[04:57:47.243658] Epoch: [51]  [  0/781]  eta: 0:11:20  lr: 0.000132  training_loss: 1.2872 (1.2872)  classification_loss: 1.2861 (1.2861)  loss_mask: 0.0011 (0.0011)  time: 0.8714  data: 0.6937  max mem: 6052
[04:57:50.669547] Epoch: [51]  [ 20/781]  eta: 0:02:35  lr: 0.000132  training_loss: 1.2954 (1.3058)  classification_loss: 1.2942 (1.3045)  loss_mask: 0.0012 (0.0014)  time: 0.1712  data: 0.0002  max mem: 6052
[04:57:54.134845] Epoch: [51]  [ 40/781]  eta: 0:02:20  lr: 0.000131  training_loss: 1.3790 (1.3443)  classification_loss: 1.3771 (1.3430)  loss_mask: 0.0010 (0.0013)  time: 0.1732  data: 0.0002  max mem: 6052
[04:57:57.540163] Epoch: [51]  [ 60/781]  eta: 0:02:11  lr: 0.000131  training_loss: 1.3177 (1.3319)  classification_loss: 1.3158 (1.3307)  loss_mask: 0.0009 (0.0012)  time: 0.1702  data: 0.0002  max mem: 6052
[04:58:00.996407] Epoch: [51]  [ 80/781]  eta: 0:02:06  lr: 0.000131  training_loss: 1.2813 (1.3323)  classification_loss: 1.2807 (1.3312)  loss_mask: 0.0008 (0.0011)  time: 0.1727  data: 0.0003  max mem: 6052
[04:58:04.460459] Epoch: [51]  [100/781]  eta: 0:02:01  lr: 0.000131  training_loss: 1.3374 (1.3326)  classification_loss: 1.3368 (1.3316)  loss_mask: 0.0007 (0.0010)  time: 0.1731  data: 0.0002  max mem: 6052
[04:58:07.887608] Epoch: [51]  [120/781]  eta: 0:01:57  lr: 0.000131  training_loss: 1.3185 (1.3327)  classification_loss: 1.3179 (1.3317)  loss_mask: 0.0009 (0.0010)  time: 0.1713  data: 0.0002  max mem: 6052
[04:58:11.319587] Epoch: [51]  [140/781]  eta: 0:01:53  lr: 0.000131  training_loss: 1.3279 (1.3324)  classification_loss: 1.3276 (1.3314)  loss_mask: 0.0005 (0.0010)  time: 0.1715  data: 0.0001  max mem: 6052
[04:58:14.724897] Epoch: [51]  [160/781]  eta: 0:01:49  lr: 0.000131  training_loss: 1.2883 (1.3309)  classification_loss: 1.2882 (1.3300)  loss_mask: 0.0007 (0.0010)  time: 0.1702  data: 0.0002  max mem: 6052
[04:58:18.138473] Epoch: [51]  [180/781]  eta: 0:01:45  lr: 0.000131  training_loss: 1.3462 (1.3333)  classification_loss: 1.3455 (1.3324)  loss_mask: 0.0006 (0.0009)  time: 0.1706  data: 0.0003  max mem: 6052
[04:58:21.555486] Epoch: [51]  [200/781]  eta: 0:01:41  lr: 0.000131  training_loss: 1.3000 (1.3319)  classification_loss: 1.2993 (1.3310)  loss_mask: 0.0006 (0.0009)  time: 0.1708  data: 0.0004  max mem: 6052
[04:58:24.983424] Epoch: [51]  [220/781]  eta: 0:01:37  lr: 0.000131  training_loss: 1.4096 (1.3352)  classification_loss: 1.4087 (1.3343)  loss_mask: 0.0006 (0.0009)  time: 0.1713  data: 0.0002  max mem: 6052
[04:58:28.391965] Epoch: [51]  [240/781]  eta: 0:01:34  lr: 0.000130  training_loss: 1.3432 (1.3366)  classification_loss: 1.3420 (1.3350)  loss_mask: 0.0008 (0.0017)  time: 0.1703  data: 0.0001  max mem: 6052
[04:58:31.885548] Epoch: [51]  [260/781]  eta: 0:01:30  lr: 0.000130  training_loss: 1.3174 (1.3361)  classification_loss: 1.3168 (1.3345)  loss_mask: 0.0005 (0.0016)  time: 0.1746  data: 0.0003  max mem: 6052
[04:58:35.324756] Epoch: [51]  [280/781]  eta: 0:01:27  lr: 0.000130  training_loss: 1.3087 (1.3343)  classification_loss: 1.3082 (1.3327)  loss_mask: 0.0006 (0.0015)  time: 0.1719  data: 0.0003  max mem: 6052
[04:58:38.736841] Epoch: [51]  [300/781]  eta: 0:01:23  lr: 0.000130  training_loss: 1.3015 (1.3334)  classification_loss: 1.3003 (1.3319)  loss_mask: 0.0006 (0.0015)  time: 0.1705  data: 0.0002  max mem: 6052
[04:58:42.175512] Epoch: [51]  [320/781]  eta: 0:01:20  lr: 0.000130  training_loss: 1.2691 (1.3311)  classification_loss: 1.2687 (1.3297)  loss_mask: 0.0006 (0.0014)  time: 0.1719  data: 0.0002  max mem: 6052
[04:58:45.570289] Epoch: [51]  [340/781]  eta: 0:01:16  lr: 0.000130  training_loss: 1.2783 (1.3295)  classification_loss: 1.2760 (1.3277)  loss_mask: 0.0013 (0.0018)  time: 0.1697  data: 0.0003  max mem: 6052
[04:58:48.995271] Epoch: [51]  [360/781]  eta: 0:01:12  lr: 0.000130  training_loss: 1.3657 (1.3313)  classification_loss: 1.3608 (1.3294)  loss_mask: 0.0021 (0.0019)  time: 0.1712  data: 0.0003  max mem: 6052
[04:58:52.436285] Epoch: [51]  [380/781]  eta: 0:01:09  lr: 0.000130  training_loss: 1.3593 (1.3327)  classification_loss: 1.3566 (1.3309)  loss_mask: 0.0012 (0.0019)  time: 0.1720  data: 0.0002  max mem: 6052
[04:58:55.905494] Epoch: [51]  [400/781]  eta: 0:01:06  lr: 0.000130  training_loss: 1.3289 (1.3322)  classification_loss: 1.3277 (1.3303)  loss_mask: 0.0009 (0.0018)  time: 0.1733  data: 0.0002  max mem: 6052
[04:58:59.317074] Epoch: [51]  [420/781]  eta: 0:01:02  lr: 0.000129  training_loss: 1.2929 (1.3312)  classification_loss: 1.2925 (1.3294)  loss_mask: 0.0005 (0.0018)  time: 0.1705  data: 0.0002  max mem: 6052
[04:59:02.729976] Epoch: [51]  [440/781]  eta: 0:00:59  lr: 0.000129  training_loss: 1.2869 (1.3302)  classification_loss: 1.2862 (1.3284)  loss_mask: 0.0006 (0.0017)  time: 0.1705  data: 0.0003  max mem: 6052
[04:59:06.143253] Epoch: [51]  [460/781]  eta: 0:00:55  lr: 0.000129  training_loss: 1.2763 (1.3290)  classification_loss: 1.2758 (1.3273)  loss_mask: 0.0006 (0.0017)  time: 0.1706  data: 0.0002  max mem: 6052
[04:59:09.584340] Epoch: [51]  [480/781]  eta: 0:00:52  lr: 0.000129  training_loss: 1.3621 (1.3302)  classification_loss: 1.3618 (1.3286)  loss_mask: 0.0007 (0.0016)  time: 0.1720  data: 0.0002  max mem: 6052
[04:59:12.995196] Epoch: [51]  [500/781]  eta: 0:00:48  lr: 0.000129  training_loss: 1.3412 (1.3310)  classification_loss: 1.3407 (1.3294)  loss_mask: 0.0003 (0.0016)  time: 0.1705  data: 0.0002  max mem: 6052
[04:59:16.410762] Epoch: [51]  [520/781]  eta: 0:00:45  lr: 0.000129  training_loss: 1.3593 (1.3325)  classification_loss: 1.3588 (1.3309)  loss_mask: 0.0005 (0.0016)  time: 0.1707  data: 0.0002  max mem: 6052
[04:59:19.838403] Epoch: [51]  [540/781]  eta: 0:00:41  lr: 0.000129  training_loss: 1.3414 (1.3327)  classification_loss: 1.3407 (1.3311)  loss_mask: 0.0004 (0.0015)  time: 0.1713  data: 0.0002  max mem: 6052
[04:59:23.341287] Epoch: [51]  [560/781]  eta: 0:00:38  lr: 0.000129  training_loss: 1.3027 (1.3321)  classification_loss: 1.3025 (1.3306)  loss_mask: 0.0004 (0.0015)  time: 0.1751  data: 0.0002  max mem: 6052
[04:59:26.790968] Epoch: [51]  [580/781]  eta: 0:00:34  lr: 0.000129  training_loss: 1.3104 (1.3324)  classification_loss: 1.3097 (1.3306)  loss_mask: 0.0007 (0.0018)  time: 0.1724  data: 0.0003  max mem: 6052
[04:59:30.216293] Epoch: [51]  [600/781]  eta: 0:00:31  lr: 0.000129  training_loss: 1.3602 (1.3327)  classification_loss: 1.3544 (1.3306)  loss_mask: 0.0014 (0.0020)  time: 0.1712  data: 0.0003  max mem: 6052
[04:59:33.629727] Epoch: [51]  [620/781]  eta: 0:00:27  lr: 0.000128  training_loss: 1.3882 (1.3363)  classification_loss: 1.3335 (1.3310)  loss_mask: 0.0245 (0.0053)  time: 0.1706  data: 0.0002  max mem: 6052
[04:59:37.050616] Epoch: [51]  [640/781]  eta: 0:00:24  lr: 0.000128  training_loss: 1.4715 (1.3422)  classification_loss: 1.3352 (1.3311)  loss_mask: 0.1287 (0.0112)  time: 0.1710  data: 0.0002  max mem: 6052
[04:59:40.507863] Epoch: [51]  [660/781]  eta: 0:00:20  lr: 0.000128  training_loss: 1.3962 (1.3439)  classification_loss: 1.3502 (1.3316)  loss_mask: 0.0335 (0.0124)  time: 0.1728  data: 0.0003  max mem: 6052
[04:59:43.926070] Epoch: [51]  [680/781]  eta: 0:00:17  lr: 0.000128  training_loss: 1.3605 (1.3448)  classification_loss: 1.3517 (1.3322)  loss_mask: 0.0128 (0.0127)  time: 0.1708  data: 0.0003  max mem: 6052
[04:59:47.332893] Epoch: [51]  [700/781]  eta: 0:00:13  lr: 0.000128  training_loss: 1.3671 (1.3456)  classification_loss: 1.3531 (1.3329)  loss_mask: 0.0077 (0.0127)  time: 0.1703  data: 0.0002  max mem: 6052
[04:59:50.753212] Epoch: [51]  [720/781]  eta: 0:00:10  lr: 0.000128  training_loss: 1.3759 (1.3464)  classification_loss: 1.3672 (1.3337)  loss_mask: 0.0088 (0.0127)  time: 0.1709  data: 0.0003  max mem: 6052
[04:59:54.185033] Epoch: [51]  [740/781]  eta: 0:00:07  lr: 0.000128  training_loss: 1.3093 (1.3454)  classification_loss: 1.3043 (1.3330)  loss_mask: 0.0043 (0.0124)  time: 0.1715  data: 0.0002  max mem: 6052
[04:59:57.632134] Epoch: [51]  [760/781]  eta: 0:00:03  lr: 0.000128  training_loss: 1.3334 (1.3459)  classification_loss: 1.3313 (1.3337)  loss_mask: 0.0024 (0.0122)  time: 0.1723  data: 0.0002  max mem: 6052
[05:00:01.037907] Epoch: [51]  [780/781]  eta: 0:00:00  lr: 0.000128  training_loss: 1.3530 (1.3462)  classification_loss: 1.3516 (1.3343)  loss_mask: 0.0022 (0.0120)  time: 0.1702  data: 0.0002  max mem: 6052
[05:00:01.209051] Epoch: [51] Total time: 0:02:14 (0.1726 s / it)
[05:00:01.209621] Averaged stats: lr: 0.000128  training_loss: 1.3530 (1.3462)  classification_loss: 1.3516 (1.3343)  loss_mask: 0.0022 (0.0120)
[05:00:01.867229] Test:  [  0/157]  eta: 0:01:42  testing_loss: 0.5364 (0.5364)  acc1: 82.8125 (82.8125)  acc5: 98.4375 (98.4375)  time: 0.6535  data: 0.6225  max mem: 6052
[05:00:02.160724] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.6138 (0.6160)  acc1: 81.2500 (80.6818)  acc5: 100.0000 (99.4318)  time: 0.0859  data: 0.0576  max mem: 6052
[05:00:02.448649] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.6075 (0.5998)  acc1: 81.2500 (81.3244)  acc5: 100.0000 (99.4048)  time: 0.0289  data: 0.0006  max mem: 6052
[05:00:02.731701] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5897 (0.6098)  acc1: 81.2500 (81.0484)  acc5: 100.0000 (99.2944)  time: 0.0284  data: 0.0001  max mem: 6052
[05:00:03.020809] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5937 (0.6163)  acc1: 81.2500 (81.0595)  acc5: 98.4375 (99.1235)  time: 0.0284  data: 0.0002  max mem: 6052
[05:00:03.305348] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5774 (0.6088)  acc1: 82.8125 (81.5257)  acc5: 98.4375 (99.0196)  time: 0.0285  data: 0.0002  max mem: 6052
[05:00:03.597880] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5660 (0.6083)  acc1: 82.8125 (81.4037)  acc5: 98.4375 (99.0010)  time: 0.0287  data: 0.0001  max mem: 6052
[05:00:03.889074] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5849 (0.6051)  acc1: 81.2500 (81.5801)  acc5: 100.0000 (98.9657)  time: 0.0290  data: 0.0002  max mem: 6052
[05:00:04.170814] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6131 (0.6156)  acc1: 79.6875 (81.1150)  acc5: 98.4375 (98.9005)  time: 0.0285  data: 0.0002  max mem: 6052
[05:00:04.451511] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6137 (0.6107)  acc1: 79.6875 (81.2500)  acc5: 98.4375 (98.9526)  time: 0.0280  data: 0.0002  max mem: 6052
[05:00:04.732907] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.6129 (0.6138)  acc1: 81.2500 (81.1108)  acc5: 100.0000 (98.9790)  time: 0.0280  data: 0.0001  max mem: 6052
[05:00:05.017867] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6247 (0.6145)  acc1: 79.6875 (81.0670)  acc5: 100.0000 (98.9865)  time: 0.0282  data: 0.0002  max mem: 6052
[05:00:05.301785] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5628 (0.6089)  acc1: 81.2500 (81.3404)  acc5: 98.4375 (98.9799)  time: 0.0283  data: 0.0002  max mem: 6052
[05:00:05.586600] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5498 (0.6088)  acc1: 82.8125 (81.4289)  acc5: 98.4375 (98.9742)  time: 0.0283  data: 0.0002  max mem: 6052
[05:00:05.865077] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5498 (0.6055)  acc1: 81.2500 (81.5381)  acc5: 100.0000 (98.9694)  time: 0.0281  data: 0.0001  max mem: 6052
[05:00:06.143209] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6197 (0.6042)  acc1: 81.2500 (81.4776)  acc5: 98.4375 (98.9756)  time: 0.0277  data: 0.0001  max mem: 6052
[05:00:06.292231] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6197 (0.6063)  acc1: 79.6875 (81.3700)  acc5: 98.4375 (98.9900)  time: 0.0268  data: 0.0001  max mem: 6052
[05:00:06.471560] Test: Total time: 0:00:05 (0.0335 s / it)
[05:00:06.472830] * Acc@1 81.370 Acc@5 98.990 loss 0.606
[05:00:06.473141] Accuracy of the network on the 10000 test images: 81.4%
[05:00:06.473310] Max accuracy: 81.37%
[05:00:06.825215] log_dir: ./output_dir
[05:00:07.666479] Epoch: [52]  [  0/781]  eta: 0:10:55  lr: 0.000128  training_loss: 1.2148 (1.2148)  classification_loss: 1.2137 (1.2137)  loss_mask: 0.0011 (0.0011)  time: 0.8397  data: 0.6499  max mem: 6052
[05:00:11.083442] Epoch: [52]  [ 20/781]  eta: 0:02:34  lr: 0.000127  training_loss: 1.3446 (1.3229)  classification_loss: 1.3434 (1.3211)  loss_mask: 0.0019 (0.0018)  time: 0.1707  data: 0.0003  max mem: 6052
[05:00:14.465242] Epoch: [52]  [ 40/781]  eta: 0:02:17  lr: 0.000127  training_loss: 1.3302 (1.3236)  classification_loss: 1.3283 (1.3218)  loss_mask: 0.0017 (0.0018)  time: 0.1690  data: 0.0002  max mem: 6052
[05:00:17.848480] Epoch: [52]  [ 60/781]  eta: 0:02:10  lr: 0.000127  training_loss: 1.2953 (1.3176)  classification_loss: 1.2931 (1.3159)  loss_mask: 0.0014 (0.0017)  time: 0.1691  data: 0.0002  max mem: 6052
[05:00:21.234735] Epoch: [52]  [ 80/781]  eta: 0:02:04  lr: 0.000127  training_loss: 1.2885 (1.3137)  classification_loss: 1.2866 (1.3121)  loss_mask: 0.0012 (0.0016)  time: 0.1692  data: 0.0002  max mem: 6052
[05:00:24.643572] Epoch: [52]  [100/781]  eta: 0:02:00  lr: 0.000127  training_loss: 1.3692 (1.3211)  classification_loss: 1.3684 (1.3196)  loss_mask: 0.0012 (0.0015)  time: 0.1704  data: 0.0002  max mem: 6052
[05:00:28.041288] Epoch: [52]  [120/781]  eta: 0:01:55  lr: 0.000127  training_loss: 1.2909 (1.3188)  classification_loss: 1.2898 (1.3173)  loss_mask: 0.0010 (0.0015)  time: 0.1698  data: 0.0002  max mem: 6052
[05:00:31.434344] Epoch: [52]  [140/781]  eta: 0:01:51  lr: 0.000127  training_loss: 1.2866 (1.3155)  classification_loss: 1.2854 (1.3141)  loss_mask: 0.0009 (0.0014)  time: 0.1696  data: 0.0001  max mem: 6052
[05:00:34.836487] Epoch: [52]  [160/781]  eta: 0:01:47  lr: 0.000127  training_loss: 1.3108 (1.3155)  classification_loss: 1.3101 (1.3142)  loss_mask: 0.0007 (0.0013)  time: 0.1700  data: 0.0002  max mem: 6052
[05:00:38.236024] Epoch: [52]  [180/781]  eta: 0:01:44  lr: 0.000127  training_loss: 1.3618 (1.3214)  classification_loss: 1.3613 (1.3201)  loss_mask: 0.0009 (0.0013)  time: 0.1699  data: 0.0002  max mem: 6052
[05:00:41.672803] Epoch: [52]  [200/781]  eta: 0:01:40  lr: 0.000127  training_loss: 1.3383 (1.3210)  classification_loss: 1.3367 (1.3198)  loss_mask: 0.0008 (0.0012)  time: 0.1717  data: 0.0003  max mem: 6052
[05:00:45.075961] Epoch: [52]  [220/781]  eta: 0:01:37  lr: 0.000126  training_loss: 1.3511 (1.3231)  classification_loss: 1.3501 (1.3219)  loss_mask: 0.0007 (0.0012)  time: 0.1701  data: 0.0002  max mem: 6052
[05:00:48.500453] Epoch: [52]  [240/781]  eta: 0:01:33  lr: 0.000126  training_loss: 1.3100 (1.3233)  classification_loss: 1.3094 (1.3222)  loss_mask: 0.0006 (0.0011)  time: 0.1711  data: 0.0002  max mem: 6052
[05:00:51.915221] Epoch: [52]  [260/781]  eta: 0:01:29  lr: 0.000126  training_loss: 1.3575 (1.3253)  classification_loss: 1.3570 (1.3242)  loss_mask: 0.0008 (0.0011)  time: 0.1706  data: 0.0002  max mem: 6052
[05:00:55.338166] Epoch: [52]  [280/781]  eta: 0:01:26  lr: 0.000126  training_loss: 1.3496 (1.3267)  classification_loss: 1.3480 (1.3257)  loss_mask: 0.0005 (0.0011)  time: 0.1711  data: 0.0003  max mem: 6052
[05:00:58.741714] Epoch: [52]  [300/781]  eta: 0:01:22  lr: 0.000126  training_loss: 1.3000 (1.3274)  classification_loss: 1.2993 (1.3263)  loss_mask: 0.0006 (0.0011)  time: 0.1701  data: 0.0003  max mem: 6052
[05:01:02.149464] Epoch: [52]  [320/781]  eta: 0:01:19  lr: 0.000126  training_loss: 1.3079 (1.3260)  classification_loss: 1.3070 (1.3250)  loss_mask: 0.0005 (0.0010)  time: 0.1703  data: 0.0002  max mem: 6052
[05:01:05.573820] Epoch: [52]  [340/781]  eta: 0:01:15  lr: 0.000126  training_loss: 1.3429 (1.3284)  classification_loss: 1.3425 (1.3274)  loss_mask: 0.0005 (0.0010)  time: 0.1711  data: 0.0002  max mem: 6052
[05:01:08.992662] Epoch: [52]  [360/781]  eta: 0:01:12  lr: 0.000126  training_loss: 1.3851 (1.3304)  classification_loss: 1.3846 (1.3295)  loss_mask: 0.0007 (0.0010)  time: 0.1709  data: 0.0002  max mem: 6052
[05:01:12.407450] Epoch: [52]  [380/781]  eta: 0:01:08  lr: 0.000126  training_loss: 1.3007 (1.3286)  classification_loss: 1.3002 (1.3276)  loss_mask: 0.0004 (0.0010)  time: 0.1706  data: 0.0002  max mem: 6052
[05:01:15.833448] Epoch: [52]  [400/781]  eta: 0:01:05  lr: 0.000125  training_loss: 1.3342 (1.3302)  classification_loss: 1.3335 (1.3293)  loss_mask: 0.0004 (0.0009)  time: 0.1712  data: 0.0002  max mem: 6052
[05:01:19.271702] Epoch: [52]  [420/781]  eta: 0:01:02  lr: 0.000125  training_loss: 1.2781 (1.3288)  classification_loss: 1.2778 (1.3278)  loss_mask: 0.0005 (0.0009)  time: 0.1718  data: 0.0003  max mem: 6052
[05:01:22.713265] Epoch: [52]  [440/781]  eta: 0:00:58  lr: 0.000125  training_loss: 1.2573 (1.3266)  classification_loss: 1.2565 (1.3257)  loss_mask: 0.0006 (0.0009)  time: 0.1720  data: 0.0002  max mem: 6052
[05:01:26.127413] Epoch: [52]  [460/781]  eta: 0:00:55  lr: 0.000125  training_loss: 1.3416 (1.3273)  classification_loss: 1.3412 (1.3264)  loss_mask: 0.0004 (0.0009)  time: 0.1706  data: 0.0002  max mem: 6052
[05:01:29.558744] Epoch: [52]  [480/781]  eta: 0:00:51  lr: 0.000125  training_loss: 1.3270 (1.3277)  classification_loss: 1.3129 (1.3262)  loss_mask: 0.0043 (0.0015)  time: 0.1715  data: 0.0002  max mem: 6052
[05:01:32.974207] Epoch: [52]  [500/781]  eta: 0:00:48  lr: 0.000125  training_loss: 1.3697 (1.3331)  classification_loss: 1.2963 (1.3251)  loss_mask: 0.0407 (0.0080)  time: 0.1707  data: 0.0003  max mem: 6052
[05:01:36.383796] Epoch: [52]  [520/781]  eta: 0:00:44  lr: 0.000125  training_loss: 1.5744 (1.3416)  classification_loss: 1.3407 (1.3254)  loss_mask: 0.1502 (0.0161)  time: 0.1704  data: 0.0002  max mem: 6052
[05:01:39.810813] Epoch: [52]  [540/781]  eta: 0:00:41  lr: 0.000125  training_loss: 1.4015 (1.3434)  classification_loss: 1.3773 (1.3263)  loss_mask: 0.0381 (0.0171)  time: 0.1712  data: 0.0003  max mem: 6052
[05:01:43.213461] Epoch: [52]  [560/781]  eta: 0:00:37  lr: 0.000125  training_loss: 1.3210 (1.3431)  classification_loss: 1.2975 (1.3257)  loss_mask: 0.0235 (0.0175)  time: 0.1700  data: 0.0003  max mem: 6052
[05:01:46.640705] Epoch: [52]  [580/781]  eta: 0:00:34  lr: 0.000125  training_loss: 1.3595 (1.3439)  classification_loss: 1.3461 (1.3267)  loss_mask: 0.0088 (0.0172)  time: 0.1713  data: 0.0003  max mem: 6052
[05:01:50.052025] Epoch: [52]  [600/781]  eta: 0:00:31  lr: 0.000124  training_loss: 1.3580 (1.3439)  classification_loss: 1.3524 (1.3269)  loss_mask: 0.0067 (0.0170)  time: 0.1705  data: 0.0002  max mem: 6052
[05:01:53.469334] Epoch: [52]  [620/781]  eta: 0:00:27  lr: 0.000124  training_loss: 1.3418 (1.3439)  classification_loss: 1.3368 (1.3272)  loss_mask: 0.0050 (0.0166)  time: 0.1708  data: 0.0003  max mem: 6052
[05:01:56.908399] Epoch: [52]  [640/781]  eta: 0:00:24  lr: 0.000124  training_loss: 1.3393 (1.3434)  classification_loss: 1.3353 (1.3272)  loss_mask: 0.0039 (0.0163)  time: 0.1719  data: 0.0004  max mem: 6052
[05:02:00.336871] Epoch: [52]  [660/781]  eta: 0:00:20  lr: 0.000124  training_loss: 1.3999 (1.3452)  classification_loss: 1.3816 (1.3291)  loss_mask: 0.0058 (0.0161)  time: 0.1713  data: 0.0003  max mem: 6052
[05:02:03.754741] Epoch: [52]  [680/781]  eta: 0:00:17  lr: 0.000124  training_loss: 1.3165 (1.3441)  classification_loss: 1.3146 (1.3284)  loss_mask: 0.0035 (0.0157)  time: 0.1708  data: 0.0003  max mem: 6052
[05:02:07.185464] Epoch: [52]  [700/781]  eta: 0:00:13  lr: 0.000124  training_loss: 1.3466 (1.3442)  classification_loss: 1.3436 (1.3288)  loss_mask: 0.0033 (0.0153)  time: 0.1714  data: 0.0003  max mem: 6052
[05:02:10.609304] Epoch: [52]  [720/781]  eta: 0:00:10  lr: 0.000124  training_loss: 1.3675 (1.3451)  classification_loss: 1.3650 (1.3300)  loss_mask: 0.0030 (0.0151)  time: 0.1711  data: 0.0003  max mem: 6052
[05:02:14.016279] Epoch: [52]  [740/781]  eta: 0:00:07  lr: 0.000124  training_loss: 1.3259 (1.3448)  classification_loss: 1.3212 (1.3299)  loss_mask: 0.0046 (0.0149)  time: 0.1703  data: 0.0002  max mem: 6052
[05:02:17.429802] Epoch: [52]  [760/781]  eta: 0:00:03  lr: 0.000124  training_loss: 1.3528 (1.3453)  classification_loss: 1.3478 (1.3306)  loss_mask: 0.0046 (0.0146)  time: 0.1706  data: 0.0003  max mem: 6052
[05:02:20.834823] Epoch: [52]  [780/781]  eta: 0:00:00  lr: 0.000123  training_loss: 1.3653 (1.3458)  classification_loss: 1.2907 (1.3311)  loss_mask: 0.0026 (0.0147)  time: 0.1702  data: 0.0002  max mem: 6052
[05:02:20.992234] Epoch: [52] Total time: 0:02:14 (0.1718 s / it)
[05:02:20.993023] Averaged stats: lr: 0.000123  training_loss: 1.3653 (1.3458)  classification_loss: 1.2907 (1.3311)  loss_mask: 0.0026 (0.0147)
[05:02:21.657474] Test:  [  0/157]  eta: 0:01:43  testing_loss: 0.5376 (0.5376)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.6599  data: 0.6219  max mem: 6052
[05:02:21.941681] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.6301 (0.6418)  acc1: 81.2500 (79.5455)  acc5: 100.0000 (99.4318)  time: 0.0856  data: 0.0568  max mem: 6052
[05:02:22.221947] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.5953 (0.6173)  acc1: 79.6875 (80.2827)  acc5: 100.0000 (99.1815)  time: 0.0281  data: 0.0002  max mem: 6052
[05:02:22.501948] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6017 (0.6348)  acc1: 79.6875 (79.9899)  acc5: 98.4375 (99.0423)  time: 0.0279  data: 0.0001  max mem: 6052
[05:02:22.781957] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6354 (0.6398)  acc1: 79.6875 (80.0305)  acc5: 98.4375 (98.8948)  time: 0.0279  data: 0.0001  max mem: 6052
[05:02:23.062548] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5949 (0.6277)  acc1: 82.8125 (80.6373)  acc5: 98.4375 (98.8971)  time: 0.0279  data: 0.0001  max mem: 6052
[05:02:23.341881] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5859 (0.6265)  acc1: 82.8125 (80.5328)  acc5: 100.0000 (98.9242)  time: 0.0279  data: 0.0001  max mem: 6052
[05:02:23.622122] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5978 (0.6232)  acc1: 79.6875 (80.6338)  acc5: 100.0000 (98.8996)  time: 0.0279  data: 0.0001  max mem: 6052
[05:02:23.902516] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6272 (0.6313)  acc1: 79.6875 (80.2855)  acc5: 98.4375 (98.8426)  time: 0.0279  data: 0.0001  max mem: 6052
[05:02:24.182571] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6108 (0.6277)  acc1: 81.2500 (80.5460)  acc5: 98.4375 (98.8839)  time: 0.0279  data: 0.0001  max mem: 6052
[05:02:24.462583] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.6031 (0.6310)  acc1: 81.2500 (80.4920)  acc5: 100.0000 (98.9171)  time: 0.0279  data: 0.0001  max mem: 6052
[05:02:24.747100] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6312 (0.6309)  acc1: 82.8125 (80.7010)  acc5: 100.0000 (98.9443)  time: 0.0281  data: 0.0002  max mem: 6052
[05:02:25.032158] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6234 (0.6290)  acc1: 82.8125 (80.7851)  acc5: 100.0000 (98.9669)  time: 0.0283  data: 0.0002  max mem: 6052
[05:02:25.317582] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6269 (0.6313)  acc1: 81.2500 (80.6298)  acc5: 100.0000 (98.9862)  time: 0.0283  data: 0.0002  max mem: 6052
[05:02:25.600744] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6528 (0.6312)  acc1: 79.6875 (80.5962)  acc5: 100.0000 (98.9916)  time: 0.0283  data: 0.0002  max mem: 6052
[05:02:25.881762] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6499 (0.6311)  acc1: 79.6875 (80.5360)  acc5: 100.0000 (99.0066)  time: 0.0281  data: 0.0001  max mem: 6052
[05:02:26.032527] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6390 (0.6312)  acc1: 79.6875 (80.5500)  acc5: 100.0000 (99.0300)  time: 0.0270  data: 0.0001  max mem: 6052
[05:02:26.199013] Test: Total time: 0:00:05 (0.0331 s / it)
[05:02:26.199473] * Acc@1 80.550 Acc@5 99.030 loss 0.631
[05:02:26.199788] Accuracy of the network on the 10000 test images: 80.5%
[05:02:26.200023] Max accuracy: 81.37%
[05:02:26.496098] log_dir: ./output_dir
[05:02:27.383407] Epoch: [53]  [  0/781]  eta: 0:11:31  lr: 0.000123  training_loss: 1.3339 (1.3339)  classification_loss: 1.3313 (1.3313)  loss_mask: 0.0026 (0.0026)  time: 0.8849  data: 0.6900  max mem: 6052
[05:02:30.812067] Epoch: [53]  [ 20/781]  eta: 0:02:36  lr: 0.000123  training_loss: 1.2814 (1.3000)  classification_loss: 1.2789 (1.2982)  loss_mask: 0.0017 (0.0018)  time: 0.1713  data: 0.0002  max mem: 6052
[05:02:34.227568] Epoch: [53]  [ 40/781]  eta: 0:02:19  lr: 0.000123  training_loss: 1.3254 (1.3193)  classification_loss: 1.3249 (1.3178)  loss_mask: 0.0012 (0.0016)  time: 0.1707  data: 0.0002  max mem: 6052
[05:02:37.669009] Epoch: [53]  [ 60/781]  eta: 0:02:11  lr: 0.000123  training_loss: 1.3468 (1.3256)  classification_loss: 1.3454 (1.3241)  loss_mask: 0.0013 (0.0015)  time: 0.1720  data: 0.0002  max mem: 6052
[05:02:41.097982] Epoch: [53]  [ 80/781]  eta: 0:02:06  lr: 0.000123  training_loss: 1.2904 (1.3213)  classification_loss: 1.2894 (1.3198)  loss_mask: 0.0012 (0.0014)  time: 0.1714  data: 0.0004  max mem: 6052
[05:02:44.517583] Epoch: [53]  [100/781]  eta: 0:02:01  lr: 0.000123  training_loss: 1.3161 (1.3230)  classification_loss: 1.3153 (1.3216)  loss_mask: 0.0012 (0.0014)  time: 0.1709  data: 0.0001  max mem: 6052
[05:02:47.931927] Epoch: [53]  [120/781]  eta: 0:01:57  lr: 0.000123  training_loss: 1.3570 (1.3268)  classification_loss: 1.3559 (1.3254)  loss_mask: 0.0009 (0.0014)  time: 0.1706  data: 0.0001  max mem: 6052
[05:02:51.348608] Epoch: [53]  [140/781]  eta: 0:01:52  lr: 0.000123  training_loss: 1.2780 (1.3196)  classification_loss: 1.2769 (1.3183)  loss_mask: 0.0010 (0.0013)  time: 0.1707  data: 0.0002  max mem: 6052
[05:02:54.747706] Epoch: [53]  [160/781]  eta: 0:01:48  lr: 0.000123  training_loss: 1.2855 (1.3191)  classification_loss: 1.2848 (1.3178)  loss_mask: 0.0010 (0.0013)  time: 0.1699  data: 0.0002  max mem: 6052
[05:02:58.160101] Epoch: [53]  [180/781]  eta: 0:01:45  lr: 0.000122  training_loss: 1.2973 (1.3207)  classification_loss: 1.2965 (1.3195)  loss_mask: 0.0011 (0.0013)  time: 0.1706  data: 0.0002  max mem: 6052
[05:03:01.588853] Epoch: [53]  [200/781]  eta: 0:01:41  lr: 0.000122  training_loss: 1.2755 (1.3187)  classification_loss: 1.2751 (1.3175)  loss_mask: 0.0008 (0.0012)  time: 0.1714  data: 0.0002  max mem: 6052
[05:03:05.011201] Epoch: [53]  [220/781]  eta: 0:01:37  lr: 0.000122  training_loss: 1.3481 (1.3186)  classification_loss: 1.3481 (1.3174)  loss_mask: 0.0007 (0.0012)  time: 0.1710  data: 0.0003  max mem: 6052
[05:03:08.443172] Epoch: [53]  [240/781]  eta: 0:01:34  lr: 0.000122  training_loss: 1.3034 (1.3174)  classification_loss: 1.3025 (1.3162)  loss_mask: 0.0009 (0.0012)  time: 0.1715  data: 0.0002  max mem: 6052
[05:03:11.847456] Epoch: [53]  [260/781]  eta: 0:01:30  lr: 0.000122  training_loss: 1.2844 (1.3166)  classification_loss: 1.2839 (1.3155)  loss_mask: 0.0005 (0.0011)  time: 0.1701  data: 0.0002  max mem: 6052
[05:03:15.287284] Epoch: [53]  [280/781]  eta: 0:01:26  lr: 0.000122  training_loss: 1.3381 (1.3152)  classification_loss: 1.3378 (1.3141)  loss_mask: 0.0007 (0.0011)  time: 0.1719  data: 0.0003  max mem: 6052
[05:03:18.702859] Epoch: [53]  [300/781]  eta: 0:01:23  lr: 0.000122  training_loss: 1.2890 (1.3156)  classification_loss: 1.2883 (1.3145)  loss_mask: 0.0007 (0.0011)  time: 0.1707  data: 0.0003  max mem: 6052
[05:03:22.118230] Epoch: [53]  [320/781]  eta: 0:01:19  lr: 0.000122  training_loss: 1.2881 (1.3160)  classification_loss: 1.2875 (1.3150)  loss_mask: 0.0007 (0.0011)  time: 0.1707  data: 0.0003  max mem: 6052
[05:03:25.533600] Epoch: [53]  [340/781]  eta: 0:01:16  lr: 0.000122  training_loss: 1.3115 (1.3144)  classification_loss: 1.3099 (1.3133)  loss_mask: 0.0009 (0.0011)  time: 0.1707  data: 0.0003  max mem: 6052
[05:03:29.013783] Epoch: [53]  [360/781]  eta: 0:01:12  lr: 0.000122  training_loss: 1.3657 (1.3163)  classification_loss: 1.3644 (1.3152)  loss_mask: 0.0006 (0.0011)  time: 0.1739  data: 0.0002  max mem: 6052
[05:03:32.572678] Epoch: [53]  [380/781]  eta: 0:01:09  lr: 0.000121  training_loss: 1.2911 (1.3166)  classification_loss: 1.2907 (1.3155)  loss_mask: 0.0006 (0.0010)  time: 0.1779  data: 0.0002  max mem: 6052
[05:03:36.006188] Epoch: [53]  [400/781]  eta: 0:01:06  lr: 0.000121  training_loss: 1.3111 (1.3169)  classification_loss: 1.3108 (1.3159)  loss_mask: 0.0006 (0.0010)  time: 0.1716  data: 0.0003  max mem: 6052
[05:03:39.410061] Epoch: [53]  [420/781]  eta: 0:01:02  lr: 0.000121  training_loss: 1.2755 (1.3150)  classification_loss: 1.2752 (1.3140)  loss_mask: 0.0006 (0.0010)  time: 0.1701  data: 0.0002  max mem: 6052
[05:03:42.823223] Epoch: [53]  [440/781]  eta: 0:00:58  lr: 0.000121  training_loss: 1.2784 (1.3149)  classification_loss: 1.2775 (1.3139)  loss_mask: 0.0004 (0.0010)  time: 0.1706  data: 0.0003  max mem: 6052
[05:03:46.231623] Epoch: [53]  [460/781]  eta: 0:00:55  lr: 0.000121  training_loss: 1.2894 (1.3139)  classification_loss: 1.2885 (1.3130)  loss_mask: 0.0005 (0.0010)  time: 0.1703  data: 0.0002  max mem: 6052
[05:03:49.652109] Epoch: [53]  [480/781]  eta: 0:00:52  lr: 0.000121  training_loss: 1.3861 (1.3186)  classification_loss: 1.3491 (1.3144)  loss_mask: 0.0007 (0.0042)  time: 0.1709  data: 0.0002  max mem: 6052
[05:03:53.061020] Epoch: [53]  [500/781]  eta: 0:00:48  lr: 0.000121  training_loss: 1.5131 (1.3271)  classification_loss: 1.3408 (1.3159)  loss_mask: 0.1560 (0.0112)  time: 0.1704  data: 0.0002  max mem: 6052
[05:03:56.490151] Epoch: [53]  [520/781]  eta: 0:00:45  lr: 0.000121  training_loss: 1.4320 (1.3324)  classification_loss: 1.3155 (1.3171)  loss_mask: 0.0588 (0.0153)  time: 0.1714  data: 0.0002  max mem: 6052
[05:03:59.910621] Epoch: [53]  [540/781]  eta: 0:00:41  lr: 0.000121  training_loss: 1.3537 (1.3340)  classification_loss: 1.3242 (1.3174)  loss_mask: 0.0265 (0.0165)  time: 0.1709  data: 0.0003  max mem: 6052
[05:04:03.323649] Epoch: [53]  [560/781]  eta: 0:00:38  lr: 0.000120  training_loss: 1.3321 (1.3338)  classification_loss: 1.3138 (1.3175)  loss_mask: 0.0110 (0.0164)  time: 0.1706  data: 0.0003  max mem: 6052
[05:04:06.747215] Epoch: [53]  [580/781]  eta: 0:00:34  lr: 0.000120  training_loss: 1.2996 (1.3331)  classification_loss: 1.2910 (1.3170)  loss_mask: 0.0094 (0.0162)  time: 0.1711  data: 0.0002  max mem: 6052
[05:04:10.166823] Epoch: [53]  [600/781]  eta: 0:00:31  lr: 0.000120  training_loss: 1.3118 (1.3327)  classification_loss: 1.3044 (1.3168)  loss_mask: 0.0067 (0.0159)  time: 0.1709  data: 0.0002  max mem: 6052
[05:04:13.584264] Epoch: [53]  [620/781]  eta: 0:00:27  lr: 0.000120  training_loss: 1.2749 (1.3312)  classification_loss: 1.2689 (1.3156)  loss_mask: 0.0060 (0.0156)  time: 0.1708  data: 0.0003  max mem: 6052
[05:04:16.996570] Epoch: [53]  [640/781]  eta: 0:00:24  lr: 0.000120  training_loss: 1.2864 (1.3303)  classification_loss: 1.2823 (1.3150)  loss_mask: 0.0041 (0.0153)  time: 0.1705  data: 0.0002  max mem: 6052
[05:04:20.468732] Epoch: [53]  [660/781]  eta: 0:00:20  lr: 0.000120  training_loss: 1.3176 (1.3302)  classification_loss: 1.3142 (1.3153)  loss_mask: 0.0035 (0.0149)  time: 0.1735  data: 0.0002  max mem: 6052
[05:04:23.884732] Epoch: [53]  [680/781]  eta: 0:00:17  lr: 0.000120  training_loss: 1.3492 (1.3303)  classification_loss: 1.3430 (1.3157)  loss_mask: 0.0025 (0.0146)  time: 0.1707  data: 0.0003  max mem: 6052
[05:04:27.300390] Epoch: [53]  [700/781]  eta: 0:00:13  lr: 0.000120  training_loss: 1.3403 (1.3310)  classification_loss: 1.3378 (1.3168)  loss_mask: 0.0025 (0.0142)  time: 0.1707  data: 0.0002  max mem: 6052
[05:04:30.720941] Epoch: [53]  [720/781]  eta: 0:00:10  lr: 0.000120  training_loss: 1.3599 (1.3316)  classification_loss: 1.3578 (1.3177)  loss_mask: 0.0022 (0.0139)  time: 0.1710  data: 0.0002  max mem: 6052
[05:04:34.145942] Epoch: [53]  [740/781]  eta: 0:00:07  lr: 0.000120  training_loss: 1.2815 (1.3303)  classification_loss: 1.2797 (1.3167)  loss_mask: 0.0018 (0.0136)  time: 0.1712  data: 0.0002  max mem: 6052
[05:04:37.559564] Epoch: [53]  [760/781]  eta: 0:00:03  lr: 0.000119  training_loss: 1.3233 (1.3312)  classification_loss: 1.3217 (1.3179)  loss_mask: 0.0017 (0.0133)  time: 0.1706  data: 0.0002  max mem: 6052
[05:04:40.955587] Epoch: [53]  [780/781]  eta: 0:00:00  lr: 0.000119  training_loss: 1.3115 (1.3310)  classification_loss: 1.3095 (1.3180)  loss_mask: 0.0017 (0.0130)  time: 0.1697  data: 0.0001  max mem: 6052
[05:04:41.121008] Epoch: [53] Total time: 0:02:14 (0.1724 s / it)
[05:04:41.121842] Averaged stats: lr: 0.000119  training_loss: 1.3115 (1.3310)  classification_loss: 1.3095 (1.3180)  loss_mask: 0.0017 (0.0130)
[05:04:41.811496] Test:  [  0/157]  eta: 0:01:47  testing_loss: 0.5087 (0.5087)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.6858  data: 0.6568  max mem: 6052
[05:04:42.093841] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.6023 (0.6112)  acc1: 81.2500 (80.6818)  acc5: 100.0000 (99.2898)  time: 0.0879  data: 0.0598  max mem: 6052
[05:04:42.375924] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5584 (0.5717)  acc1: 82.8125 (82.0685)  acc5: 100.0000 (99.4048)  time: 0.0281  data: 0.0001  max mem: 6052
[05:04:42.656989] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5397 (0.5845)  acc1: 84.3750 (81.8044)  acc5: 100.0000 (99.0927)  time: 0.0280  data: 0.0001  max mem: 6052
[05:04:42.936929] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5792 (0.5898)  acc1: 82.8125 (81.9360)  acc5: 98.4375 (99.0473)  time: 0.0279  data: 0.0001  max mem: 6052
[05:04:43.216963] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5583 (0.5798)  acc1: 82.8125 (82.3836)  acc5: 100.0000 (99.1115)  time: 0.0279  data: 0.0001  max mem: 6052
[05:04:43.496558] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5447 (0.5782)  acc1: 82.8125 (82.4027)  acc5: 100.0000 (99.1291)  time: 0.0279  data: 0.0001  max mem: 6052
[05:04:43.776818] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5570 (0.5757)  acc1: 82.8125 (82.4604)  acc5: 100.0000 (99.1197)  time: 0.0279  data: 0.0001  max mem: 6052
[05:04:44.057082] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5881 (0.5835)  acc1: 81.2500 (82.1566)  acc5: 98.4375 (99.1127)  time: 0.0279  data: 0.0001  max mem: 6052
[05:04:44.337968] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5826 (0.5788)  acc1: 82.8125 (82.3489)  acc5: 100.0000 (99.1587)  time: 0.0280  data: 0.0001  max mem: 6052
[05:04:44.618368] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5826 (0.5840)  acc1: 81.2500 (82.1937)  acc5: 100.0000 (99.1337)  time: 0.0279  data: 0.0001  max mem: 6052
[05:04:44.899759] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5960 (0.5851)  acc1: 81.2500 (82.1931)  acc5: 100.0000 (99.1695)  time: 0.0280  data: 0.0001  max mem: 6052
[05:04:45.180391] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5646 (0.5848)  acc1: 82.8125 (82.0635)  acc5: 100.0000 (99.1477)  time: 0.0280  data: 0.0001  max mem: 6052
[05:04:45.460797] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5455 (0.5829)  acc1: 81.2500 (82.1923)  acc5: 100.0000 (99.1651)  time: 0.0279  data: 0.0001  max mem: 6052
[05:04:45.739931] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5651 (0.5803)  acc1: 84.3750 (82.3138)  acc5: 100.0000 (99.2021)  time: 0.0279  data: 0.0001  max mem: 6052
[05:04:46.017553] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5758 (0.5803)  acc1: 82.8125 (82.3055)  acc5: 100.0000 (99.2032)  time: 0.0277  data: 0.0001  max mem: 6052
[05:04:46.166331] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5758 (0.5809)  acc1: 82.8125 (82.2700)  acc5: 100.0000 (99.2300)  time: 0.0267  data: 0.0001  max mem: 6052
[05:04:46.326630] Test: Total time: 0:00:05 (0.0331 s / it)
[05:04:46.327409] * Acc@1 82.270 Acc@5 99.230 loss 0.581
[05:04:46.327741] Accuracy of the network on the 10000 test images: 82.3%
[05:04:46.328078] Max accuracy: 82.27%
[05:04:46.543899] log_dir: ./output_dir
[05:04:47.412951] Epoch: [54]  [  0/781]  eta: 0:11:17  lr: 0.000119  training_loss: 1.2582 (1.2582)  classification_loss: 1.2572 (1.2572)  loss_mask: 0.0010 (0.0010)  time: 0.8672  data: 0.6658  max mem: 6052
[05:04:50.865970] Epoch: [54]  [ 20/781]  eta: 0:02:36  lr: 0.000119  training_loss: 1.2975 (1.2927)  classification_loss: 1.2931 (1.2883)  loss_mask: 0.0025 (0.0044)  time: 0.1725  data: 0.0002  max mem: 6052
[05:04:54.297487] Epoch: [54]  [ 40/781]  eta: 0:02:20  lr: 0.000119  training_loss: 1.3743 (1.3180)  classification_loss: 1.3705 (1.3136)  loss_mask: 0.0029 (0.0044)  time: 0.1715  data: 0.0003  max mem: 6052
[05:04:57.713365] Epoch: [54]  [ 60/781]  eta: 0:02:11  lr: 0.000119  training_loss: 1.3436 (1.3288)  classification_loss: 1.3423 (1.3250)  loss_mask: 0.0020 (0.0038)  time: 0.1707  data: 0.0003  max mem: 6052
[05:05:01.115465] Epoch: [54]  [ 80/781]  eta: 0:02:06  lr: 0.000119  training_loss: 1.3063 (1.3242)  classification_loss: 1.3033 (1.3207)  loss_mask: 0.0020 (0.0036)  time: 0.1700  data: 0.0002  max mem: 6052
[05:05:04.531526] Epoch: [54]  [100/781]  eta: 0:02:01  lr: 0.000119  training_loss: 1.2635 (1.3195)  classification_loss: 1.2578 (1.3159)  loss_mask: 0.0024 (0.0036)  time: 0.1707  data: 0.0003  max mem: 6052
[05:05:07.966936] Epoch: [54]  [120/781]  eta: 0:01:56  lr: 0.000119  training_loss: 1.3231 (1.3223)  classification_loss: 1.3021 (1.3166)  loss_mask: 0.0081 (0.0058)  time: 0.1717  data: 0.0002  max mem: 6052
[05:05:11.378326] Epoch: [54]  [140/781]  eta: 0:01:52  lr: 0.000119  training_loss: 1.3249 (1.3243)  classification_loss: 1.2847 (1.3137)  loss_mask: 0.0199 (0.0106)  time: 0.1705  data: 0.0003  max mem: 6052
[05:05:14.803908] Epoch: [54]  [160/781]  eta: 0:01:48  lr: 0.000118  training_loss: 1.3994 (1.3303)  classification_loss: 1.3305 (1.3139)  loss_mask: 0.0388 (0.0163)  time: 0.1712  data: 0.0002  max mem: 6052
[05:05:18.222338] Epoch: [54]  [180/781]  eta: 0:01:45  lr: 0.000118  training_loss: 1.3235 (1.3305)  classification_loss: 1.2959 (1.3130)  loss_mask: 0.0117 (0.0175)  time: 0.1708  data: 0.0003  max mem: 6052
[05:05:21.638989] Epoch: [54]  [200/781]  eta: 0:01:41  lr: 0.000118  training_loss: 1.3457 (1.3313)  classification_loss: 1.3388 (1.3150)  loss_mask: 0.0041 (0.0163)  time: 0.1708  data: 0.0002  max mem: 6052
[05:05:25.055066] Epoch: [54]  [220/781]  eta: 0:01:37  lr: 0.000118  training_loss: 1.3871 (1.3358)  classification_loss: 1.3837 (1.3207)  loss_mask: 0.0029 (0.0151)  time: 0.1707  data: 0.0002  max mem: 6052
[05:05:28.496062] Epoch: [54]  [240/781]  eta: 0:01:34  lr: 0.000118  training_loss: 1.2998 (1.3352)  classification_loss: 1.2975 (1.3212)  loss_mask: 0.0017 (0.0140)  time: 0.1720  data: 0.0003  max mem: 6052
[05:05:31.907800] Epoch: [54]  [260/781]  eta: 0:01:30  lr: 0.000118  training_loss: 1.3343 (1.3348)  classification_loss: 1.3328 (1.3217)  loss_mask: 0.0017 (0.0131)  time: 0.1705  data: 0.0003  max mem: 6052
[05:05:35.326215] Epoch: [54]  [280/781]  eta: 0:01:26  lr: 0.000118  training_loss: 1.3241 (1.3327)  classification_loss: 1.3228 (1.3205)  loss_mask: 0.0013 (0.0122)  time: 0.1709  data: 0.0002  max mem: 6052
[05:05:38.753543] Epoch: [54]  [300/781]  eta: 0:01:23  lr: 0.000118  training_loss: 1.3206 (1.3322)  classification_loss: 1.3177 (1.3207)  loss_mask: 0.0012 (0.0115)  time: 0.1713  data: 0.0002  max mem: 6052
[05:05:42.172799] Epoch: [54]  [320/781]  eta: 0:01:19  lr: 0.000118  training_loss: 1.3266 (1.3313)  classification_loss: 1.3253 (1.3204)  loss_mask: 0.0012 (0.0109)  time: 0.1709  data: 0.0002  max mem: 6052
[05:05:45.596190] Epoch: [54]  [340/781]  eta: 0:01:16  lr: 0.000118  training_loss: 1.2971 (1.3287)  classification_loss: 1.2961 (1.3184)  loss_mask: 0.0011 (0.0103)  time: 0.1711  data: 0.0004  max mem: 6052
[05:05:49.017180] Epoch: [54]  [360/781]  eta: 0:01:12  lr: 0.000117  training_loss: 1.3212 (1.3288)  classification_loss: 1.3200 (1.3190)  loss_mask: 0.0009 (0.0098)  time: 0.1710  data: 0.0003  max mem: 6052
[05:05:52.417821] Epoch: [54]  [380/781]  eta: 0:01:09  lr: 0.000117  training_loss: 1.2915 (1.3281)  classification_loss: 1.2900 (1.3182)  loss_mask: 0.0011 (0.0099)  time: 0.1700  data: 0.0002  max mem: 6052
[05:05:55.853573] Epoch: [54]  [400/781]  eta: 0:01:05  lr: 0.000117  training_loss: 1.2970 (1.3278)  classification_loss: 1.2965 (1.3184)  loss_mask: 0.0009 (0.0094)  time: 0.1717  data: 0.0002  max mem: 6052
[05:05:59.302607] Epoch: [54]  [420/781]  eta: 0:01:02  lr: 0.000117  training_loss: 1.2952 (1.3277)  classification_loss: 1.2948 (1.3186)  loss_mask: 0.0011 (0.0090)  time: 0.1723  data: 0.0002  max mem: 6052
[05:06:02.723062] Epoch: [54]  [440/781]  eta: 0:00:58  lr: 0.000117  training_loss: 1.2975 (1.3262)  classification_loss: 1.2966 (1.3176)  loss_mask: 0.0007 (0.0087)  time: 0.1709  data: 0.0002  max mem: 6052
[05:06:06.146977] Epoch: [54]  [460/781]  eta: 0:00:55  lr: 0.000117  training_loss: 1.2607 (1.3236)  classification_loss: 1.2598 (1.3153)  loss_mask: 0.0008 (0.0083)  time: 0.1711  data: 0.0003  max mem: 6052
[05:06:09.594019] Epoch: [54]  [480/781]  eta: 0:00:51  lr: 0.000117  training_loss: 1.3122 (1.3244)  classification_loss: 1.3110 (1.3164)  loss_mask: 0.0008 (0.0080)  time: 0.1723  data: 0.0002  max mem: 6052
[05:06:13.003287] Epoch: [54]  [500/781]  eta: 0:00:48  lr: 0.000117  training_loss: 1.2899 (1.3237)  classification_loss: 1.2892 (1.3160)  loss_mask: 0.0007 (0.0077)  time: 0.1704  data: 0.0002  max mem: 6052
[05:06:16.408473] Epoch: [54]  [520/781]  eta: 0:00:44  lr: 0.000117  training_loss: 1.2845 (1.3226)  classification_loss: 1.2841 (1.3151)  loss_mask: 0.0008 (0.0075)  time: 0.1702  data: 0.0002  max mem: 6052
[05:06:19.822990] Epoch: [54]  [540/781]  eta: 0:00:41  lr: 0.000116  training_loss: 1.3138 (1.3233)  classification_loss: 1.3118 (1.3161)  loss_mask: 0.0006 (0.0072)  time: 0.1706  data: 0.0002  max mem: 6052
[05:06:23.255176] Epoch: [54]  [560/781]  eta: 0:00:38  lr: 0.000116  training_loss: 1.2956 (1.3222)  classification_loss: 1.2949 (1.3152)  loss_mask: 0.0006 (0.0070)  time: 0.1715  data: 0.0003  max mem: 6052
[05:06:26.676869] Epoch: [54]  [580/781]  eta: 0:00:34  lr: 0.000116  training_loss: 1.3354 (1.3223)  classification_loss: 1.3349 (1.3156)  loss_mask: 0.0006 (0.0068)  time: 0.1710  data: 0.0004  max mem: 6052
[05:06:30.095496] Epoch: [54]  [600/781]  eta: 0:00:31  lr: 0.000116  training_loss: 1.2796 (1.3213)  classification_loss: 1.2789 (1.3147)  loss_mask: 0.0006 (0.0066)  time: 0.1708  data: 0.0003  max mem: 6052
[05:06:33.532952] Epoch: [54]  [620/781]  eta: 0:00:27  lr: 0.000116  training_loss: 1.2671 (1.3202)  classification_loss: 1.2661 (1.3138)  loss_mask: 0.0005 (0.0064)  time: 0.1718  data: 0.0005  max mem: 6052
[05:06:36.957655] Epoch: [54]  [640/781]  eta: 0:00:24  lr: 0.000116  training_loss: 1.3132 (1.3198)  classification_loss: 1.3127 (1.3136)  loss_mask: 0.0005 (0.0062)  time: 0.1712  data: 0.0002  max mem: 6052
[05:06:40.375720] Epoch: [54]  [660/781]  eta: 0:00:20  lr: 0.000116  training_loss: 1.3439 (1.3206)  classification_loss: 1.3433 (1.3144)  loss_mask: 0.0005 (0.0062)  time: 0.1708  data: 0.0004  max mem: 6052
[05:06:43.793931] Epoch: [54]  [680/781]  eta: 0:00:17  lr: 0.000116  training_loss: 1.3516 (1.3219)  classification_loss: 1.3515 (1.3157)  loss_mask: 0.0010 (0.0061)  time: 0.1708  data: 0.0003  max mem: 6052
[05:06:47.215902] Epoch: [54]  [700/781]  eta: 0:00:13  lr: 0.000116  training_loss: 1.2644 (1.3209)  classification_loss: 1.2641 (1.3149)  loss_mask: 0.0007 (0.0060)  time: 0.1710  data: 0.0004  max mem: 6052
[05:06:50.635253] Epoch: [54]  [720/781]  eta: 0:00:10  lr: 0.000116  training_loss: 1.3203 (1.3215)  classification_loss: 1.3194 (1.3157)  loss_mask: 0.0007 (0.0058)  time: 0.1709  data: 0.0002  max mem: 6052
[05:06:54.047835] Epoch: [54]  [740/781]  eta: 0:00:07  lr: 0.000115  training_loss: 1.2903 (1.3213)  classification_loss: 1.2898 (1.3156)  loss_mask: 0.0005 (0.0057)  time: 0.1706  data: 0.0003  max mem: 6052
[05:06:57.461376] Epoch: [54]  [760/781]  eta: 0:00:03  lr: 0.000115  training_loss: 1.3544 (1.3220)  classification_loss: 1.3540 (1.3165)  loss_mask: 0.0005 (0.0055)  time: 0.1706  data: 0.0002  max mem: 6052
[05:07:00.885928] Epoch: [54]  [780/781]  eta: 0:00:00  lr: 0.000115  training_loss: 1.3453 (1.3223)  classification_loss: 1.3449 (1.3169)  loss_mask: 0.0004 (0.0054)  time: 0.1711  data: 0.0002  max mem: 6052
[05:07:01.043803] Epoch: [54] Total time: 0:02:14 (0.1722 s / it)
[05:07:01.044235] Averaged stats: lr: 0.000115  training_loss: 1.3453 (1.3223)  classification_loss: 1.3449 (1.3169)  loss_mask: 0.0004 (0.0054)
[05:07:01.708180] Test:  [  0/157]  eta: 0:01:43  testing_loss: 0.5109 (0.5109)  acc1: 84.3750 (84.3750)  acc5: 96.8750 (96.8750)  time: 0.6586  data: 0.6286  max mem: 6052
[05:07:01.997134] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.6053 (0.6258)  acc1: 79.6875 (79.8295)  acc5: 98.4375 (98.7216)  time: 0.0860  data: 0.0577  max mem: 6052
[05:07:02.281688] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5919 (0.5940)  acc1: 81.2500 (81.3988)  acc5: 98.4375 (98.8095)  time: 0.0285  data: 0.0004  max mem: 6052
[05:07:02.565283] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5703 (0.6066)  acc1: 82.8125 (81.2500)  acc5: 98.4375 (98.6895)  time: 0.0283  data: 0.0002  max mem: 6052
[05:07:02.849209] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6038 (0.6150)  acc1: 79.6875 (80.7927)  acc5: 100.0000 (98.8186)  time: 0.0282  data: 0.0002  max mem: 6052
[05:07:03.132844] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5870 (0.6041)  acc1: 79.6875 (81.1887)  acc5: 100.0000 (98.8664)  time: 0.0282  data: 0.0001  max mem: 6052
[05:07:03.420702] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5567 (0.6013)  acc1: 81.2500 (81.1732)  acc5: 100.0000 (98.8730)  time: 0.0284  data: 0.0001  max mem: 6052
[05:07:03.703177] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5804 (0.5963)  acc1: 81.2500 (81.3600)  acc5: 98.4375 (98.8336)  time: 0.0284  data: 0.0002  max mem: 6052
[05:07:03.988875] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6064 (0.6057)  acc1: 79.6875 (81.0185)  acc5: 98.4375 (98.8233)  time: 0.0283  data: 0.0002  max mem: 6052
[05:07:04.270637] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5975 (0.6015)  acc1: 79.6875 (81.2328)  acc5: 98.4375 (98.8839)  time: 0.0282  data: 0.0001  max mem: 6052
[05:07:04.552883] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5603 (0.6060)  acc1: 81.2500 (81.1262)  acc5: 100.0000 (98.8552)  time: 0.0281  data: 0.0001  max mem: 6052
[05:07:04.839109] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6046 (0.6055)  acc1: 81.2500 (81.1374)  acc5: 100.0000 (98.8880)  time: 0.0283  data: 0.0002  max mem: 6052
[05:07:05.119463] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5932 (0.6024)  acc1: 81.2500 (81.2887)  acc5: 100.0000 (98.8895)  time: 0.0282  data: 0.0002  max mem: 6052
[05:07:05.411811] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5871 (0.6028)  acc1: 82.8125 (81.3216)  acc5: 100.0000 (98.9385)  time: 0.0285  data: 0.0002  max mem: 6052
[05:07:05.693970] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5625 (0.5995)  acc1: 82.8125 (81.4495)  acc5: 100.0000 (98.9694)  time: 0.0286  data: 0.0002  max mem: 6052
[05:07:05.972325] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5602 (0.5978)  acc1: 82.8125 (81.4466)  acc5: 100.0000 (98.9859)  time: 0.0279  data: 0.0001  max mem: 6052
[05:07:06.121422] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6092 (0.5985)  acc1: 81.2500 (81.3900)  acc5: 100.0000 (99.0200)  time: 0.0268  data: 0.0001  max mem: 6052
[05:07:06.282499] Test: Total time: 0:00:05 (0.0333 s / it)
[05:07:06.282956] * Acc@1 81.390 Acc@5 99.020 loss 0.599
[05:07:06.283246] Accuracy of the network on the 10000 test images: 81.4%
[05:07:06.283421] Max accuracy: 82.27%
[05:07:06.404031] log_dir: ./output_dir
[05:07:07.271832] Epoch: [55]  [  0/781]  eta: 0:11:16  lr: 0.000115  training_loss: 1.2856 (1.2856)  classification_loss: 1.2847 (1.2847)  loss_mask: 0.0008 (0.0008)  time: 0.8663  data: 0.6537  max mem: 6052
[05:07:10.714757] Epoch: [55]  [ 20/781]  eta: 0:02:36  lr: 0.000115  training_loss: 1.2679 (1.3043)  classification_loss: 1.2677 (1.3038)  loss_mask: 0.0004 (0.0006)  time: 0.1720  data: 0.0002  max mem: 6052
[05:07:14.164783] Epoch: [55]  [ 40/781]  eta: 0:02:20  lr: 0.000115  training_loss: 1.3431 (1.3118)  classification_loss: 1.3427 (1.3113)  loss_mask: 0.0004 (0.0005)  time: 0.1724  data: 0.0002  max mem: 6052
[05:07:17.584833] Epoch: [55]  [ 60/781]  eta: 0:02:12  lr: 0.000115  training_loss: 1.2567 (1.3103)  classification_loss: 1.2557 (1.3098)  loss_mask: 0.0004 (0.0005)  time: 0.1709  data: 0.0003  max mem: 6052
[05:07:21.016721] Epoch: [55]  [ 80/781]  eta: 0:02:06  lr: 0.000115  training_loss: 1.2618 (1.3021)  classification_loss: 1.2614 (1.3015)  loss_mask: 0.0004 (0.0005)  time: 0.1715  data: 0.0002  max mem: 6052
[05:07:24.450406] Epoch: [55]  [100/781]  eta: 0:02:01  lr: 0.000115  training_loss: 1.2963 (1.3024)  classification_loss: 1.2955 (1.3019)  loss_mask: 0.0004 (0.0005)  time: 0.1716  data: 0.0002  max mem: 6052
[05:07:27.863299] Epoch: [55]  [120/781]  eta: 0:01:57  lr: 0.000115  training_loss: 1.3051 (1.3063)  classification_loss: 1.3046 (1.3058)  loss_mask: 0.0004 (0.0005)  time: 0.1706  data: 0.0003  max mem: 6052
[05:07:31.276865] Epoch: [55]  [140/781]  eta: 0:01:53  lr: 0.000114  training_loss: 1.2716 (1.3043)  classification_loss: 1.2712 (1.3038)  loss_mask: 0.0003 (0.0005)  time: 0.1706  data: 0.0002  max mem: 6052
[05:07:34.708005] Epoch: [55]  [160/781]  eta: 0:01:49  lr: 0.000114  training_loss: 1.2813 (1.3038)  classification_loss: 1.2810 (1.3034)  loss_mask: 0.0003 (0.0004)  time: 0.1714  data: 0.0002  max mem: 6052
[05:07:38.126335] Epoch: [55]  [180/781]  eta: 0:01:45  lr: 0.000114  training_loss: 1.2719 (1.3019)  classification_loss: 1.2715 (1.3015)  loss_mask: 0.0003 (0.0004)  time: 0.1708  data: 0.0003  max mem: 6052
[05:07:41.540201] Epoch: [55]  [200/781]  eta: 0:01:41  lr: 0.000114  training_loss: 1.3296 (1.3038)  classification_loss: 1.3292 (1.3033)  loss_mask: 0.0003 (0.0004)  time: 0.1706  data: 0.0002  max mem: 6052
[05:07:44.957110] Epoch: [55]  [220/781]  eta: 0:01:37  lr: 0.000114  training_loss: 1.3103 (1.3069)  classification_loss: 1.3099 (1.3065)  loss_mask: 0.0003 (0.0004)  time: 0.1707  data: 0.0002  max mem: 6052
[05:07:48.374814] Epoch: [55]  [240/781]  eta: 0:01:34  lr: 0.000114  training_loss: 1.3104 (1.3064)  classification_loss: 1.3102 (1.3060)  loss_mask: 0.0003 (0.0004)  time: 0.1708  data: 0.0002  max mem: 6052
[05:07:51.786793] Epoch: [55]  [260/781]  eta: 0:01:30  lr: 0.000114  training_loss: 1.3088 (1.3071)  classification_loss: 1.3077 (1.3067)  loss_mask: 0.0003 (0.0004)  time: 0.1705  data: 0.0002  max mem: 6052
[05:07:55.195613] Epoch: [55]  [280/781]  eta: 0:01:26  lr: 0.000114  training_loss: 1.2853 (1.3050)  classification_loss: 1.2849 (1.3046)  loss_mask: 0.0003 (0.0004)  time: 0.1703  data: 0.0003  max mem: 6052
[05:07:58.635845] Epoch: [55]  [300/781]  eta: 0:01:23  lr: 0.000114  training_loss: 1.2752 (1.3051)  classification_loss: 1.2750 (1.3047)  loss_mask: 0.0003 (0.0004)  time: 0.1719  data: 0.0003  max mem: 6052
[05:08:02.193281] Epoch: [55]  [320/781]  eta: 0:01:20  lr: 0.000114  training_loss: 1.3113 (1.3050)  classification_loss: 1.3110 (1.3046)  loss_mask: 0.0003 (0.0004)  time: 0.1778  data: 0.0003  max mem: 6052
[05:08:05.653223] Epoch: [55]  [340/781]  eta: 0:01:16  lr: 0.000113  training_loss: 1.2617 (1.3033)  classification_loss: 1.2615 (1.3029)  loss_mask: 0.0002 (0.0004)  time: 0.1729  data: 0.0002  max mem: 6052
[05:08:09.076873] Epoch: [55]  [360/781]  eta: 0:01:13  lr: 0.000113  training_loss: 1.2996 (1.3045)  classification_loss: 1.2993 (1.3041)  loss_mask: 0.0003 (0.0004)  time: 0.1711  data: 0.0003  max mem: 6052
[05:08:12.492452] Epoch: [55]  [380/781]  eta: 0:01:09  lr: 0.000113  training_loss: 1.2882 (1.3048)  classification_loss: 1.2880 (1.3044)  loss_mask: 0.0002 (0.0004)  time: 0.1707  data: 0.0003  max mem: 6052
[05:08:15.897912] Epoch: [55]  [400/781]  eta: 0:01:05  lr: 0.000113  training_loss: 1.2822 (1.3043)  classification_loss: 1.2819 (1.3040)  loss_mask: 0.0003 (0.0004)  time: 0.1702  data: 0.0004  max mem: 6052
[05:08:19.321850] Epoch: [55]  [420/781]  eta: 0:01:02  lr: 0.000113  training_loss: 1.2990 (1.3045)  classification_loss: 1.2987 (1.3042)  loss_mask: 0.0003 (0.0004)  time: 0.1711  data: 0.0002  max mem: 6052
[05:08:22.740784] Epoch: [55]  [440/781]  eta: 0:00:58  lr: 0.000113  training_loss: 1.2640 (1.3038)  classification_loss: 1.2638 (1.3035)  loss_mask: 0.0002 (0.0004)  time: 0.1709  data: 0.0002  max mem: 6052
[05:08:26.160500] Epoch: [55]  [460/781]  eta: 0:00:55  lr: 0.000113  training_loss: 1.2759 (1.3023)  classification_loss: 1.2758 (1.3020)  loss_mask: 0.0002 (0.0004)  time: 0.1709  data: 0.0002  max mem: 6052
[05:08:29.600024] Epoch: [55]  [480/781]  eta: 0:00:52  lr: 0.000113  training_loss: 1.3295 (1.3048)  classification_loss: 1.3291 (1.3044)  loss_mask: 0.0003 (0.0004)  time: 0.1719  data: 0.0002  max mem: 6052
[05:08:33.047255] Epoch: [55]  [500/781]  eta: 0:00:48  lr: 0.000113  training_loss: 1.2827 (1.3045)  classification_loss: 1.2821 (1.3042)  loss_mask: 0.0002 (0.0004)  time: 0.1723  data: 0.0002  max mem: 6052
[05:08:36.516875] Epoch: [55]  [520/781]  eta: 0:00:45  lr: 0.000112  training_loss: 1.3058 (1.3043)  classification_loss: 1.3056 (1.3039)  loss_mask: 0.0002 (0.0003)  time: 0.1734  data: 0.0002  max mem: 6052
[05:08:39.935055] Epoch: [55]  [540/781]  eta: 0:00:41  lr: 0.000112  training_loss: 1.3434 (1.3056)  classification_loss: 1.3430 (1.3052)  loss_mask: 0.0002 (0.0003)  time: 0.1708  data: 0.0003  max mem: 6052
[05:08:43.338880] Epoch: [55]  [560/781]  eta: 0:00:38  lr: 0.000112  training_loss: 1.2808 (1.3048)  classification_loss: 1.2797 (1.3045)  loss_mask: 0.0003 (0.0003)  time: 0.1701  data: 0.0001  max mem: 6052
[05:08:46.766901] Epoch: [55]  [580/781]  eta: 0:00:34  lr: 0.000112  training_loss: 1.2933 (1.3053)  classification_loss: 1.2930 (1.3049)  loss_mask: 0.0002 (0.0003)  time: 0.1713  data: 0.0002  max mem: 6052
[05:08:50.242085] Epoch: [55]  [600/781]  eta: 0:00:31  lr: 0.000112  training_loss: 1.2688 (1.3040)  classification_loss: 1.2685 (1.3037)  loss_mask: 0.0002 (0.0003)  time: 0.1737  data: 0.0003  max mem: 6052
[05:08:53.663169] Epoch: [55]  [620/781]  eta: 0:00:27  lr: 0.000112  training_loss: 1.3142 (1.3045)  classification_loss: 1.3139 (1.3042)  loss_mask: 0.0002 (0.0003)  time: 0.1710  data: 0.0002  max mem: 6052
[05:08:57.112262] Epoch: [55]  [640/781]  eta: 0:00:24  lr: 0.000112  training_loss: 1.2929 (1.3050)  classification_loss: 1.2926 (1.3047)  loss_mask: 0.0002 (0.0003)  time: 0.1724  data: 0.0003  max mem: 6052
[05:09:00.530732] Epoch: [55]  [660/781]  eta: 0:00:20  lr: 0.000112  training_loss: 1.3258 (1.3059)  classification_loss: 1.3255 (1.3056)  loss_mask: 0.0002 (0.0003)  time: 0.1708  data: 0.0002  max mem: 6052
[05:09:03.946198] Epoch: [55]  [680/781]  eta: 0:00:17  lr: 0.000112  training_loss: 1.3030 (1.3058)  classification_loss: 1.3028 (1.3055)  loss_mask: 0.0002 (0.0003)  time: 0.1707  data: 0.0002  max mem: 6052
[05:09:07.373519] Epoch: [55]  [700/781]  eta: 0:00:13  lr: 0.000112  training_loss: 1.3244 (1.3061)  classification_loss: 1.3242 (1.3058)  loss_mask: 0.0002 (0.0003)  time: 0.1713  data: 0.0003  max mem: 6052
[05:09:10.794188] Epoch: [55]  [720/781]  eta: 0:00:10  lr: 0.000111  training_loss: 1.3676 (1.3079)  classification_loss: 1.3674 (1.3076)  loss_mask: 0.0002 (0.0003)  time: 0.1710  data: 0.0003  max mem: 6052
[05:09:14.220764] Epoch: [55]  [740/781]  eta: 0:00:07  lr: 0.000111  training_loss: 1.2237 (1.3064)  classification_loss: 1.2235 (1.3061)  loss_mask: 0.0002 (0.0003)  time: 0.1713  data: 0.0003  max mem: 6052
[05:09:17.645204] Epoch: [55]  [760/781]  eta: 0:00:03  lr: 0.000111  training_loss: 1.3177 (1.3070)  classification_loss: 1.3035 (1.3062)  loss_mask: 0.0013 (0.0008)  time: 0.1711  data: 0.0002  max mem: 6052
[05:09:21.038439] Epoch: [55]  [780/781]  eta: 0:00:00  lr: 0.000111  training_loss: 1.4156 (1.3095)  classification_loss: 1.3191 (1.3060)  loss_mask: 0.0802 (0.0035)  time: 0.1696  data: 0.0001  max mem: 6052
[05:09:21.217871] Epoch: [55] Total time: 0:02:14 (0.1726 s / it)
[05:09:21.218390] Averaged stats: lr: 0.000111  training_loss: 1.4156 (1.3095)  classification_loss: 1.3191 (1.3060)  loss_mask: 0.0802 (0.0035)
[05:09:21.868427] Test:  [  0/157]  eta: 0:01:41  testing_loss: 0.5084 (0.5084)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.6455  data: 0.6160  max mem: 6052
[05:09:22.150348] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5565 (0.6082)  acc1: 81.2500 (79.9716)  acc5: 98.4375 (99.0057)  time: 0.0841  data: 0.0562  max mem: 6052
[05:09:22.430133] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.5481 (0.5745)  acc1: 81.2500 (81.8452)  acc5: 100.0000 (99.1071)  time: 0.0279  data: 0.0002  max mem: 6052
[05:09:22.711137] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5676 (0.5961)  acc1: 81.2500 (80.8468)  acc5: 100.0000 (99.1431)  time: 0.0279  data: 0.0002  max mem: 6052
[05:09:22.991857] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6220 (0.6069)  acc1: 81.2500 (80.7546)  acc5: 100.0000 (99.0473)  time: 0.0280  data: 0.0002  max mem: 6052
[05:09:23.272806] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6051 (0.5944)  acc1: 81.2500 (81.4951)  acc5: 98.4375 (99.0502)  time: 0.0279  data: 0.0002  max mem: 6052
[05:09:23.554508] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5508 (0.5895)  acc1: 82.8125 (81.4037)  acc5: 100.0000 (99.0523)  time: 0.0280  data: 0.0002  max mem: 6052
[05:09:23.837016] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5508 (0.5828)  acc1: 82.8125 (81.8222)  acc5: 100.0000 (99.1197)  time: 0.0281  data: 0.0002  max mem: 6052
[05:09:24.117701] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5741 (0.5917)  acc1: 81.2500 (81.4429)  acc5: 98.4375 (99.0741)  time: 0.0280  data: 0.0002  max mem: 6052
[05:09:24.398362] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5716 (0.5889)  acc1: 81.2500 (81.6277)  acc5: 98.4375 (99.1071)  time: 0.0280  data: 0.0002  max mem: 6052
[05:09:24.679053] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5743 (0.5935)  acc1: 81.2500 (81.3428)  acc5: 100.0000 (99.0873)  time: 0.0279  data: 0.0002  max mem: 6052
[05:09:24.960594] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5941 (0.5937)  acc1: 79.6875 (81.3626)  acc5: 100.0000 (99.1132)  time: 0.0280  data: 0.0001  max mem: 6052
[05:09:25.240980] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5757 (0.5915)  acc1: 81.2500 (81.4566)  acc5: 100.0000 (99.1090)  time: 0.0280  data: 0.0001  max mem: 6052
[05:09:25.523020] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5948 (0.5921)  acc1: 81.2500 (81.4528)  acc5: 100.0000 (99.1293)  time: 0.0280  data: 0.0001  max mem: 6052
[05:09:25.803428] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5483 (0.5884)  acc1: 81.2500 (81.5492)  acc5: 100.0000 (99.1467)  time: 0.0280  data: 0.0001  max mem: 6052
[05:09:26.081653] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5608 (0.5870)  acc1: 81.2500 (81.5915)  acc5: 98.4375 (99.0894)  time: 0.0278  data: 0.0001  max mem: 6052
[05:09:26.231187] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5641 (0.5881)  acc1: 81.2500 (81.5200)  acc5: 98.4375 (99.0900)  time: 0.0268  data: 0.0001  max mem: 6052
[05:09:26.393541] Test: Total time: 0:00:05 (0.0329 s / it)
[05:09:26.393987] * Acc@1 81.520 Acc@5 99.090 loss 0.588
[05:09:26.394291] Accuracy of the network on the 10000 test images: 81.5%
[05:09:26.394489] Max accuracy: 82.27%
[05:09:26.651578] log_dir: ./output_dir
[05:09:27.479812] Epoch: [56]  [  0/781]  eta: 0:10:45  lr: 0.000111  training_loss: 1.5635 (1.5635)  classification_loss: 1.1328 (1.1328)  loss_mask: 0.4306 (0.4306)  time: 0.8265  data: 0.6398  max mem: 6052
[05:09:30.896528] Epoch: [56]  [ 20/781]  eta: 0:02:33  lr: 0.000111  training_loss: 1.3501 (1.3957)  classification_loss: 1.2312 (1.2584)  loss_mask: 0.0632 (0.1373)  time: 0.1707  data: 0.0002  max mem: 6052
[05:09:34.327029] Epoch: [56]  [ 40/781]  eta: 0:02:18  lr: 0.000111  training_loss: 1.3474 (1.3721)  classification_loss: 1.3103 (1.2803)  loss_mask: 0.0191 (0.0917)  time: 0.1714  data: 0.0002  max mem: 6052
[05:09:37.762977] Epoch: [56]  [ 60/781]  eta: 0:02:11  lr: 0.000111  training_loss: 1.3341 (1.3568)  classification_loss: 1.3104 (1.2897)  loss_mask: 0.0123 (0.0671)  time: 0.1717  data: 0.0002  max mem: 6052
[05:09:41.196663] Epoch: [56]  [ 80/781]  eta: 0:02:05  lr: 0.000111  training_loss: 1.3173 (1.3369)  classification_loss: 1.3112 (1.2850)  loss_mask: 0.0047 (0.0519)  time: 0.1716  data: 0.0003  max mem: 6052
[05:09:44.622751] Epoch: [56]  [100/781]  eta: 0:02:01  lr: 0.000111  training_loss: 1.3366 (1.3355)  classification_loss: 1.3324 (1.2930)  loss_mask: 0.0036 (0.0424)  time: 0.1712  data: 0.0002  max mem: 6052
[05:09:48.040080] Epoch: [56]  [120/781]  eta: 0:01:56  lr: 0.000110  training_loss: 1.3072 (1.3287)  classification_loss: 1.3039 (1.2929)  loss_mask: 0.0023 (0.0358)  time: 0.1708  data: 0.0002  max mem: 6052
[05:09:51.504231] Epoch: [56]  [140/781]  eta: 0:01:52  lr: 0.000110  training_loss: 1.2470 (1.3212)  classification_loss: 1.2443 (1.2893)  loss_mask: 0.0026 (0.0319)  time: 0.1731  data: 0.0002  max mem: 6052
[05:09:54.938345] Epoch: [56]  [160/781]  eta: 0:01:49  lr: 0.000110  training_loss: 1.2832 (1.3163)  classification_loss: 1.2706 (1.2877)  loss_mask: 0.0034 (0.0286)  time: 0.1716  data: 0.0003  max mem: 6052
[05:09:58.368793] Epoch: [56]  [180/781]  eta: 0:01:45  lr: 0.000110  training_loss: 1.2631 (1.3128)  classification_loss: 1.2622 (1.2871)  loss_mask: 0.0018 (0.0257)  time: 0.1714  data: 0.0002  max mem: 6052
[05:10:01.858443] Epoch: [56]  [200/781]  eta: 0:01:41  lr: 0.000110  training_loss: 1.3111 (1.3143)  classification_loss: 1.3101 (1.2910)  loss_mask: 0.0012 (0.0233)  time: 0.1744  data: 0.0003  max mem: 6052
[05:10:05.274843] Epoch: [56]  [220/781]  eta: 0:01:37  lr: 0.000110  training_loss: 1.2962 (1.3136)  classification_loss: 1.2948 (1.2923)  loss_mask: 0.0012 (0.0213)  time: 0.1707  data: 0.0002  max mem: 6052
[05:10:08.683524] Epoch: [56]  [240/781]  eta: 0:01:34  lr: 0.000110  training_loss: 1.2702 (1.3103)  classification_loss: 1.2693 (1.2907)  loss_mask: 0.0008 (0.0196)  time: 0.1704  data: 0.0003  max mem: 6052
[05:10:12.092108] Epoch: [56]  [260/781]  eta: 0:01:30  lr: 0.000110  training_loss: 1.2667 (1.3073)  classification_loss: 1.2660 (1.2892)  loss_mask: 0.0007 (0.0181)  time: 0.1704  data: 0.0002  max mem: 6052
[05:10:15.505429] Epoch: [56]  [280/781]  eta: 0:01:27  lr: 0.000110  training_loss: 1.2829 (1.3081)  classification_loss: 1.2826 (1.2912)  loss_mask: 0.0008 (0.0169)  time: 0.1706  data: 0.0002  max mem: 6052
[05:10:18.919004] Epoch: [56]  [300/781]  eta: 0:01:23  lr: 0.000110  training_loss: 1.2427 (1.3043)  classification_loss: 1.2423 (1.2884)  loss_mask: 0.0005 (0.0158)  time: 0.1706  data: 0.0003  max mem: 6052
[05:10:22.372946] Epoch: [56]  [320/781]  eta: 0:01:19  lr: 0.000109  training_loss: 1.2537 (1.3030)  classification_loss: 1.2531 (1.2881)  loss_mask: 0.0004 (0.0149)  time: 0.1726  data: 0.0002  max mem: 6052
[05:10:25.792249] Epoch: [56]  [340/781]  eta: 0:01:16  lr: 0.000109  training_loss: 1.2708 (1.3006)  classification_loss: 1.2471 (1.2861)  loss_mask: 0.0006 (0.0145)  time: 0.1709  data: 0.0002  max mem: 6052
[05:10:29.199854] Epoch: [56]  [360/781]  eta: 0:01:12  lr: 0.000109  training_loss: 1.3838 (1.3059)  classification_loss: 1.3159 (1.2882)  loss_mask: 0.0086 (0.0176)  time: 0.1703  data: 0.0002  max mem: 6052
[05:10:32.617119] Epoch: [56]  [380/781]  eta: 0:01:09  lr: 0.000109  training_loss: 1.3113 (1.3074)  classification_loss: 1.2504 (1.2882)  loss_mask: 0.0320 (0.0192)  time: 0.1708  data: 0.0003  max mem: 6052
[05:10:36.017976] Epoch: [56]  [400/781]  eta: 0:01:05  lr: 0.000109  training_loss: 1.4374 (1.3150)  classification_loss: 1.2989 (1.2888)  loss_mask: 0.0774 (0.0262)  time: 0.1700  data: 0.0002  max mem: 6052
[05:10:39.431712] Epoch: [56]  [420/781]  eta: 0:01:02  lr: 0.000109  training_loss: 1.3418 (1.3179)  classification_loss: 1.2664 (1.2891)  loss_mask: 0.0586 (0.0288)  time: 0.1706  data: 0.0002  max mem: 6052
[05:10:42.851669] Epoch: [56]  [440/781]  eta: 0:00:58  lr: 0.000109  training_loss: 1.3255 (1.3184)  classification_loss: 1.3087 (1.2900)  loss_mask: 0.0180 (0.0284)  time: 0.1709  data: 0.0002  max mem: 6052
[05:10:46.258509] Epoch: [56]  [460/781]  eta: 0:00:55  lr: 0.000109  training_loss: 1.2736 (1.3169)  classification_loss: 1.2665 (1.2892)  loss_mask: 0.0105 (0.0277)  time: 0.1703  data: 0.0002  max mem: 6052
[05:10:49.732108] Epoch: [56]  [480/781]  eta: 0:00:51  lr: 0.000109  training_loss: 1.2882 (1.3164)  classification_loss: 1.2808 (1.2897)  loss_mask: 0.0052 (0.0268)  time: 0.1736  data: 0.0002  max mem: 6052
[05:10:53.150854] Epoch: [56]  [500/781]  eta: 0:00:48  lr: 0.000109  training_loss: 1.3361 (1.3175)  classification_loss: 1.3308 (1.2916)  loss_mask: 0.0037 (0.0259)  time: 0.1709  data: 0.0003  max mem: 6052
[05:10:56.581000] Epoch: [56]  [520/781]  eta: 0:00:45  lr: 0.000108  training_loss: 1.3212 (1.3180)  classification_loss: 1.3182 (1.2930)  loss_mask: 0.0031 (0.0250)  time: 0.1714  data: 0.0002  max mem: 6052
[05:10:59.997183] Epoch: [56]  [540/781]  eta: 0:00:41  lr: 0.000108  training_loss: 1.3129 (1.3180)  classification_loss: 1.3098 (1.2939)  loss_mask: 0.0032 (0.0242)  time: 0.1707  data: 0.0003  max mem: 6052
[05:11:03.404800] Epoch: [56]  [560/781]  eta: 0:00:38  lr: 0.000108  training_loss: 1.2504 (1.3155)  classification_loss: 1.2480 (1.2922)  loss_mask: 0.0018 (0.0234)  time: 0.1703  data: 0.0002  max mem: 6052
[05:11:06.820826] Epoch: [56]  [580/781]  eta: 0:00:34  lr: 0.000108  training_loss: 1.2780 (1.3142)  classification_loss: 1.2765 (1.2915)  loss_mask: 0.0018 (0.0227)  time: 0.1707  data: 0.0002  max mem: 6052
[05:11:10.242719] Epoch: [56]  [600/781]  eta: 0:00:31  lr: 0.000108  training_loss: 1.3045 (1.3144)  classification_loss: 1.3017 (1.2924)  loss_mask: 0.0022 (0.0220)  time: 0.1710  data: 0.0002  max mem: 6052
[05:11:13.660337] Epoch: [56]  [620/781]  eta: 0:00:27  lr: 0.000108  training_loss: 1.3042 (1.3138)  classification_loss: 1.3038 (1.2925)  loss_mask: 0.0020 (0.0213)  time: 0.1708  data: 0.0002  max mem: 6052
[05:11:17.081367] Epoch: [56]  [640/781]  eta: 0:00:24  lr: 0.000108  training_loss: 1.2499 (1.3125)  classification_loss: 1.2490 (1.2918)  loss_mask: 0.0014 (0.0207)  time: 0.1710  data: 0.0004  max mem: 6052
[05:11:20.503050] Epoch: [56]  [660/781]  eta: 0:00:20  lr: 0.000108  training_loss: 1.2645 (1.3118)  classification_loss: 1.2617 (1.2917)  loss_mask: 0.0012 (0.0201)  time: 0.1710  data: 0.0003  max mem: 6052
[05:11:23.919313] Epoch: [56]  [680/781]  eta: 0:00:17  lr: 0.000108  training_loss: 1.2760 (1.3114)  classification_loss: 1.2751 (1.2918)  loss_mask: 0.0011 (0.0196)  time: 0.1707  data: 0.0003  max mem: 6052
[05:11:27.339407] Epoch: [56]  [700/781]  eta: 0:00:13  lr: 0.000107  training_loss: 1.2814 (1.3110)  classification_loss: 1.2811 (1.2920)  loss_mask: 0.0011 (0.0191)  time: 0.1709  data: 0.0002  max mem: 6052
[05:11:30.747936] Epoch: [56]  [720/781]  eta: 0:00:10  lr: 0.000107  training_loss: 1.3367 (1.3120)  classification_loss: 1.3354 (1.2935)  loss_mask: 0.0010 (0.0186)  time: 0.1704  data: 0.0002  max mem: 6052
[05:11:34.150202] Epoch: [56]  [740/781]  eta: 0:00:07  lr: 0.000107  training_loss: 1.2995 (1.3114)  classification_loss: 1.2989 (1.2933)  loss_mask: 0.0008 (0.0181)  time: 0.1700  data: 0.0003  max mem: 6052
[05:11:37.559384] Epoch: [56]  [760/781]  eta: 0:00:03  lr: 0.000107  training_loss: 1.3269 (1.3119)  classification_loss: 1.3262 (1.2943)  loss_mask: 0.0008 (0.0176)  time: 0.1704  data: 0.0002  max mem: 6052
[05:11:40.967967] Epoch: [56]  [780/781]  eta: 0:00:00  lr: 0.000107  training_loss: 1.2879 (1.3123)  classification_loss: 1.2870 (1.2951)  loss_mask: 0.0006 (0.0172)  time: 0.1703  data: 0.0002  max mem: 6052
[05:11:41.127281] Epoch: [56] Total time: 0:02:14 (0.1722 s / it)
[05:11:41.127742] Averaged stats: lr: 0.000107  training_loss: 1.2879 (1.3123)  classification_loss: 1.2870 (1.2951)  loss_mask: 0.0006 (0.0172)
[05:11:41.795902] Test:  [  0/157]  eta: 0:01:44  testing_loss: 0.5304 (0.5304)  acc1: 82.8125 (82.8125)  acc5: 100.0000 (100.0000)  time: 0.6631  data: 0.6302  max mem: 6052
[05:11:42.077732] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5832 (0.5744)  acc1: 82.8125 (81.9602)  acc5: 100.0000 (99.5739)  time: 0.0857  data: 0.0574  max mem: 6052
[05:11:42.359234] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.5336 (0.5416)  acc1: 82.8125 (83.5565)  acc5: 100.0000 (99.4792)  time: 0.0280  data: 0.0001  max mem: 6052
[05:11:42.649236] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5308 (0.5497)  acc1: 82.8125 (82.9637)  acc5: 100.0000 (99.1935)  time: 0.0285  data: 0.0002  max mem: 6052
[05:11:42.937552] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5573 (0.5629)  acc1: 82.8125 (82.6601)  acc5: 98.4375 (98.9710)  time: 0.0288  data: 0.0002  max mem: 6052
[05:11:43.222325] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5503 (0.5575)  acc1: 81.2500 (82.7512)  acc5: 98.4375 (99.0809)  time: 0.0285  data: 0.0002  max mem: 6052
[05:11:43.511581] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5377 (0.5572)  acc1: 81.2500 (82.5820)  acc5: 100.0000 (99.0779)  time: 0.0286  data: 0.0002  max mem: 6052
[05:11:43.797430] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5380 (0.5527)  acc1: 82.8125 (82.7025)  acc5: 100.0000 (99.1417)  time: 0.0286  data: 0.0003  max mem: 6052
[05:11:44.081936] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5494 (0.5616)  acc1: 82.8125 (82.4653)  acc5: 100.0000 (99.1127)  time: 0.0284  data: 0.0003  max mem: 6052
[05:11:44.372390] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5570 (0.5562)  acc1: 82.8125 (82.7266)  acc5: 98.4375 (99.1243)  time: 0.0286  data: 0.0002  max mem: 6052
[05:11:44.657050] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5288 (0.5600)  acc1: 82.8125 (82.5495)  acc5: 100.0000 (99.1646)  time: 0.0286  data: 0.0002  max mem: 6052
[05:11:44.940398] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5701 (0.5592)  acc1: 81.2500 (82.5450)  acc5: 100.0000 (99.1695)  time: 0.0283  data: 0.0002  max mem: 6052
[05:11:45.227538] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5334 (0.5574)  acc1: 81.2500 (82.6317)  acc5: 98.4375 (99.1477)  time: 0.0284  data: 0.0002  max mem: 6052
[05:11:45.513667] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5245 (0.5575)  acc1: 82.8125 (82.6574)  acc5: 98.4375 (99.1770)  time: 0.0285  data: 0.0003  max mem: 6052
[05:11:45.793330] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5245 (0.5549)  acc1: 82.8125 (82.7128)  acc5: 100.0000 (99.1800)  time: 0.0282  data: 0.0002  max mem: 6052
[05:11:46.072188] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5544 (0.5546)  acc1: 82.8125 (82.7194)  acc5: 100.0000 (99.1722)  time: 0.0278  data: 0.0001  max mem: 6052
[05:11:46.221417] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5562 (0.5558)  acc1: 82.8125 (82.7500)  acc5: 98.4375 (99.1700)  time: 0.0268  data: 0.0001  max mem: 6052
[05:11:46.403304] Test: Total time: 0:00:05 (0.0336 s / it)
[05:11:46.403879] * Acc@1 82.750 Acc@5 99.170 loss 0.556
[05:11:46.404182] Accuracy of the network on the 10000 test images: 82.8%
[05:11:46.404359] Max accuracy: 82.75%
[05:11:46.551589] log_dir: ./output_dir
[05:11:47.451292] Epoch: [57]  [  0/781]  eta: 0:11:41  lr: 0.000107  training_loss: 1.3842 (1.3842)  classification_loss: 1.3829 (1.3829)  loss_mask: 0.0013 (0.0013)  time: 0.8979  data: 0.6750  max mem: 6052
[05:11:50.871133] Epoch: [57]  [ 20/781]  eta: 0:02:36  lr: 0.000107  training_loss: 1.3130 (1.2886)  classification_loss: 1.3127 (1.2878)  loss_mask: 0.0007 (0.0009)  time: 0.1709  data: 0.0002  max mem: 6052
[05:11:54.282224] Epoch: [57]  [ 40/781]  eta: 0:02:19  lr: 0.000107  training_loss: 1.2605 (1.2902)  classification_loss: 1.2597 (1.2893)  loss_mask: 0.0008 (0.0009)  time: 0.1705  data: 0.0003  max mem: 6052
[05:11:57.710338] Epoch: [57]  [ 60/781]  eta: 0:02:11  lr: 0.000107  training_loss: 1.3170 (1.3073)  classification_loss: 1.3166 (1.3065)  loss_mask: 0.0006 (0.0008)  time: 0.1713  data: 0.0003  max mem: 6052
[05:12:01.131848] Epoch: [57]  [ 80/781]  eta: 0:02:06  lr: 0.000107  training_loss: 1.2314 (1.2982)  classification_loss: 1.2311 (1.2975)  loss_mask: 0.0007 (0.0008)  time: 0.1710  data: 0.0002  max mem: 6052
[05:12:04.546005] Epoch: [57]  [100/781]  eta: 0:02:01  lr: 0.000107  training_loss: 1.2648 (1.3000)  classification_loss: 1.2645 (1.2986)  loss_mask: 0.0012 (0.0014)  time: 0.1706  data: 0.0002  max mem: 6052
[05:12:07.950852] Epoch: [57]  [120/781]  eta: 0:01:56  lr: 0.000106  training_loss: 1.2727 (1.2977)  classification_loss: 1.2691 (1.2962)  loss_mask: 0.0014 (0.0014)  time: 0.1702  data: 0.0002  max mem: 6052
[05:12:11.391932] Epoch: [57]  [140/781]  eta: 0:01:52  lr: 0.000106  training_loss: 1.2235 (1.2928)  classification_loss: 1.2225 (1.2914)  loss_mask: 0.0009 (0.0014)  time: 0.1720  data: 0.0003  max mem: 6052
[05:12:14.805509] Epoch: [57]  [160/781]  eta: 0:01:48  lr: 0.000106  training_loss: 1.3095 (1.2959)  classification_loss: 1.3087 (1.2946)  loss_mask: 0.0006 (0.0013)  time: 0.1706  data: 0.0003  max mem: 6052
[05:12:18.224160] Epoch: [57]  [180/781]  eta: 0:01:45  lr: 0.000106  training_loss: 1.2876 (1.2965)  classification_loss: 1.2870 (1.2953)  loss_mask: 0.0006 (0.0012)  time: 0.1709  data: 0.0002  max mem: 6052
[05:12:21.637508] Epoch: [57]  [200/781]  eta: 0:01:41  lr: 0.000106  training_loss: 1.2668 (1.2940)  classification_loss: 1.2661 (1.2928)  loss_mask: 0.0006 (0.0012)  time: 0.1706  data: 0.0002  max mem: 6052
[05:12:25.059307] Epoch: [57]  [220/781]  eta: 0:01:37  lr: 0.000106  training_loss: 1.2653 (1.2915)  classification_loss: 1.2648 (1.2903)  loss_mask: 0.0006 (0.0011)  time: 0.1710  data: 0.0002  max mem: 6052
[05:12:28.495313] Epoch: [57]  [240/781]  eta: 0:01:34  lr: 0.000106  training_loss: 1.2682 (1.2903)  classification_loss: 1.2676 (1.2892)  loss_mask: 0.0004 (0.0011)  time: 0.1717  data: 0.0003  max mem: 6052
[05:12:31.908465] Epoch: [57]  [260/781]  eta: 0:01:30  lr: 0.000106  training_loss: 1.2685 (1.2895)  classification_loss: 1.2683 (1.2885)  loss_mask: 0.0005 (0.0010)  time: 0.1705  data: 0.0001  max mem: 6052
[05:12:35.347698] Epoch: [57]  [280/781]  eta: 0:01:26  lr: 0.000106  training_loss: 1.2775 (1.2892)  classification_loss: 1.2774 (1.2883)  loss_mask: 0.0005 (0.0010)  time: 0.1718  data: 0.0002  max mem: 6052
[05:12:38.756244] Epoch: [57]  [300/781]  eta: 0:01:23  lr: 0.000105  training_loss: 1.3143 (1.2922)  classification_loss: 1.3137 (1.2913)  loss_mask: 0.0005 (0.0010)  time: 0.1703  data: 0.0002  max mem: 6052
[05:12:42.173077] Epoch: [57]  [320/781]  eta: 0:01:19  lr: 0.000105  training_loss: 1.2920 (1.2927)  classification_loss: 1.2914 (1.2917)  loss_mask: 0.0004 (0.0009)  time: 0.1707  data: 0.0002  max mem: 6052
[05:12:45.629447] Epoch: [57]  [340/781]  eta: 0:01:16  lr: 0.000105  training_loss: 1.2571 (1.2927)  classification_loss: 1.2565 (1.2918)  loss_mask: 0.0004 (0.0009)  time: 0.1727  data: 0.0002  max mem: 6052
[05:12:49.042040] Epoch: [57]  [360/781]  eta: 0:01:12  lr: 0.000105  training_loss: 1.3139 (1.2942)  classification_loss: 1.3136 (1.2933)  loss_mask: 0.0004 (0.0009)  time: 0.1706  data: 0.0004  max mem: 6052
[05:12:52.463309] Epoch: [57]  [380/781]  eta: 0:01:09  lr: 0.000105  training_loss: 1.2871 (1.2949)  classification_loss: 1.2867 (1.2940)  loss_mask: 0.0004 (0.0008)  time: 0.1710  data: 0.0002  max mem: 6052
[05:12:55.884474] Epoch: [57]  [400/781]  eta: 0:01:05  lr: 0.000105  training_loss: 1.2888 (1.2953)  classification_loss: 1.2887 (1.2945)  loss_mask: 0.0004 (0.0008)  time: 0.1710  data: 0.0003  max mem: 6052
[05:12:59.290212] Epoch: [57]  [420/781]  eta: 0:01:02  lr: 0.000105  training_loss: 1.2575 (1.2949)  classification_loss: 1.2571 (1.2941)  loss_mask: 0.0004 (0.0008)  time: 0.1702  data: 0.0003  max mem: 6052
[05:13:02.719420] Epoch: [57]  [440/781]  eta: 0:00:58  lr: 0.000105  training_loss: 1.2785 (1.2950)  classification_loss: 1.2780 (1.2942)  loss_mask: 0.0003 (0.0008)  time: 0.1714  data: 0.0003  max mem: 6052
[05:13:06.156400] Epoch: [57]  [460/781]  eta: 0:00:55  lr: 0.000105  training_loss: 1.2488 (1.2942)  classification_loss: 1.2481 (1.2934)  loss_mask: 0.0004 (0.0008)  time: 0.1718  data: 0.0002  max mem: 6052
[05:13:09.584820] Epoch: [57]  [480/781]  eta: 0:00:51  lr: 0.000105  training_loss: 1.3328 (1.2945)  classification_loss: 1.3206 (1.2932)  loss_mask: 0.0004 (0.0013)  time: 0.1714  data: 0.0002  max mem: 6052
[05:13:13.004062] Epoch: [57]  [500/781]  eta: 0:00:48  lr: 0.000104  training_loss: 1.5558 (1.3082)  classification_loss: 1.3087 (1.2941)  loss_mask: 0.2647 (0.0141)  time: 0.1709  data: 0.0003  max mem: 6052
[05:13:16.426941] Epoch: [57]  [520/781]  eta: 0:00:45  lr: 0.000104  training_loss: 1.3769 (1.3136)  classification_loss: 1.2895 (1.2941)  loss_mask: 0.0874 (0.0194)  time: 0.1711  data: 0.0002  max mem: 6052
[05:13:19.843636] Epoch: [57]  [540/781]  eta: 0:00:41  lr: 0.000104  training_loss: 1.3857 (1.3158)  classification_loss: 1.3426 (1.2948)  loss_mask: 0.0493 (0.0210)  time: 0.1707  data: 0.0002  max mem: 6052
[05:13:23.267157] Epoch: [57]  [560/781]  eta: 0:00:38  lr: 0.000104  training_loss: 1.3231 (1.3167)  classification_loss: 1.2651 (1.2939)  loss_mask: 0.0560 (0.0229)  time: 0.1711  data: 0.0002  max mem: 6052
[05:13:26.697255] Epoch: [57]  [580/781]  eta: 0:00:34  lr: 0.000104  training_loss: 1.3016 (1.3160)  classification_loss: 1.2846 (1.2930)  loss_mask: 0.0235 (0.0230)  time: 0.1714  data: 0.0003  max mem: 6052
[05:13:30.133130] Epoch: [57]  [600/781]  eta: 0:00:31  lr: 0.000104  training_loss: 1.2344 (1.3135)  classification_loss: 1.2186 (1.2908)  loss_mask: 0.0121 (0.0227)  time: 0.1717  data: 0.0002  max mem: 6052
[05:13:33.554487] Epoch: [57]  [620/781]  eta: 0:00:27  lr: 0.000104  training_loss: 1.3072 (1.3136)  classification_loss: 1.2996 (1.2913)  loss_mask: 0.0092 (0.0223)  time: 0.1710  data: 0.0001  max mem: 6052
[05:13:36.980111] Epoch: [57]  [640/781]  eta: 0:00:24  lr: 0.000104  training_loss: 1.3429 (1.3141)  classification_loss: 1.3354 (1.2922)  loss_mask: 0.0066 (0.0219)  time: 0.1712  data: 0.0002  max mem: 6052
[05:13:40.395325] Epoch: [57]  [660/781]  eta: 0:00:20  lr: 0.000104  training_loss: 1.2865 (1.3143)  classification_loss: 1.2826 (1.2930)  loss_mask: 0.0050 (0.0214)  time: 0.1707  data: 0.0002  max mem: 6052
[05:13:43.806825] Epoch: [57]  [680/781]  eta: 0:00:17  lr: 0.000104  training_loss: 1.2828 (1.3146)  classification_loss: 1.2812 (1.2937)  loss_mask: 0.0034 (0.0208)  time: 0.1705  data: 0.0002  max mem: 6052
[05:13:47.223714] Epoch: [57]  [700/781]  eta: 0:00:13  lr: 0.000103  training_loss: 1.2854 (1.3132)  classification_loss: 1.2809 (1.2929)  loss_mask: 0.0038 (0.0203)  time: 0.1708  data: 0.0002  max mem: 6052
[05:13:50.666377] Epoch: [57]  [720/781]  eta: 0:00:10  lr: 0.000103  training_loss: 1.2944 (1.3129)  classification_loss: 1.2906 (1.2930)  loss_mask: 0.0038 (0.0199)  time: 0.1721  data: 0.0003  max mem: 6052
[05:13:54.091708] Epoch: [57]  [740/781]  eta: 0:00:07  lr: 0.000103  training_loss: 1.2384 (1.3113)  classification_loss: 1.2242 (1.2916)  loss_mask: 0.0052 (0.0197)  time: 0.1712  data: 0.0004  max mem: 6052
[05:13:57.514667] Epoch: [57]  [760/781]  eta: 0:00:03  lr: 0.000103  training_loss: 1.3533 (1.3122)  classification_loss: 1.3357 (1.2927)  loss_mask: 0.0059 (0.0195)  time: 0.1711  data: 0.0002  max mem: 6052
[05:14:00.923377] Epoch: [57]  [780/781]  eta: 0:00:00  lr: 0.000103  training_loss: 1.3253 (1.3125)  classification_loss: 1.3208 (1.2932)  loss_mask: 0.0061 (0.0192)  time: 0.1704  data: 0.0002  max mem: 6052
[05:14:01.097439] Epoch: [57] Total time: 0:02:14 (0.1723 s / it)
[05:14:01.097884] Averaged stats: lr: 0.000103  training_loss: 1.3253 (1.3125)  classification_loss: 1.3208 (1.2932)  loss_mask: 0.0061 (0.0192)
[05:14:01.766640] Test:  [  0/157]  eta: 0:01:44  testing_loss: 0.4664 (0.4664)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.6647  data: 0.6337  max mem: 6052
[05:14:02.057530] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5578 (0.5777)  acc1: 81.2500 (81.5341)  acc5: 100.0000 (99.4318)  time: 0.0867  data: 0.0578  max mem: 6052
[05:14:02.340099] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5491 (0.5464)  acc1: 81.2500 (82.8125)  acc5: 100.0000 (99.4792)  time: 0.0285  data: 0.0002  max mem: 6052
[05:14:02.624078] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5491 (0.5610)  acc1: 84.3750 (82.2077)  acc5: 100.0000 (99.1935)  time: 0.0282  data: 0.0001  max mem: 6052
[05:14:02.915195] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5664 (0.5648)  acc1: 82.8125 (82.3552)  acc5: 98.4375 (98.9710)  time: 0.0286  data: 0.0001  max mem: 6052
[05:14:03.198416] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5397 (0.5579)  acc1: 82.8125 (82.7819)  acc5: 98.4375 (99.0196)  time: 0.0286  data: 0.0002  max mem: 6052
[05:14:03.485411] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5190 (0.5566)  acc1: 82.8125 (82.7100)  acc5: 100.0000 (99.0266)  time: 0.0284  data: 0.0002  max mem: 6052
[05:14:03.768899] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5422 (0.5525)  acc1: 84.3750 (82.9665)  acc5: 100.0000 (99.0977)  time: 0.0284  data: 0.0001  max mem: 6052
[05:14:04.057349] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5665 (0.5610)  acc1: 82.8125 (82.5231)  acc5: 100.0000 (99.0741)  time: 0.0284  data: 0.0002  max mem: 6052
[05:14:04.340758] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6099 (0.5593)  acc1: 79.6875 (82.6236)  acc5: 98.4375 (99.0728)  time: 0.0284  data: 0.0002  max mem: 6052
[05:14:04.629422] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5741 (0.5629)  acc1: 81.2500 (82.4876)  acc5: 100.0000 (99.0718)  time: 0.0284  data: 0.0002  max mem: 6052
[05:14:04.915013] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5780 (0.5625)  acc1: 82.8125 (82.6436)  acc5: 100.0000 (99.0709)  time: 0.0286  data: 0.0003  max mem: 6052
[05:14:05.198134] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5380 (0.5608)  acc1: 84.3750 (82.7092)  acc5: 100.0000 (99.0832)  time: 0.0283  data: 0.0002  max mem: 6052
[05:14:05.490262] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5537 (0.5615)  acc1: 82.8125 (82.6097)  acc5: 100.0000 (99.1054)  time: 0.0285  data: 0.0002  max mem: 6052
[05:14:05.772393] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5309 (0.5574)  acc1: 82.8125 (82.8014)  acc5: 100.0000 (99.1135)  time: 0.0285  data: 0.0002  max mem: 6052
[05:14:06.051158] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5458 (0.5575)  acc1: 82.8125 (82.7090)  acc5: 100.0000 (99.1308)  time: 0.0279  data: 0.0001  max mem: 6052
[05:14:06.200187] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5488 (0.5577)  acc1: 81.2500 (82.6500)  acc5: 100.0000 (99.1500)  time: 0.0268  data: 0.0001  max mem: 6052
[05:14:06.368000] Test: Total time: 0:00:05 (0.0335 s / it)
[05:14:06.369241] * Acc@1 82.650 Acc@5 99.150 loss 0.558
[05:14:06.370031] Accuracy of the network on the 10000 test images: 82.7%
[05:14:06.370576] Max accuracy: 82.75%
[05:14:06.481965] log_dir: ./output_dir
[05:14:07.358186] Epoch: [58]  [  0/781]  eta: 0:11:22  lr: 0.000103  training_loss: 1.2541 (1.2541)  classification_loss: 1.2519 (1.2519)  loss_mask: 0.0023 (0.0023)  time: 0.8744  data: 0.6794  max mem: 6052
[05:14:10.823190] Epoch: [58]  [ 20/781]  eta: 0:02:37  lr: 0.000103  training_loss: 1.2440 (1.2639)  classification_loss: 1.2369 (1.2597)  loss_mask: 0.0039 (0.0042)  time: 0.1732  data: 0.0002  max mem: 6052
[05:14:14.313606] Epoch: [58]  [ 40/781]  eta: 0:02:21  lr: 0.000103  training_loss: 1.3085 (1.2812)  classification_loss: 1.3056 (1.2777)  loss_mask: 0.0023 (0.0034)  time: 0.1744  data: 0.0002  max mem: 6052
[05:14:17.732147] Epoch: [58]  [ 60/781]  eta: 0:02:12  lr: 0.000103  training_loss: 1.3221 (1.2999)  classification_loss: 1.3189 (1.2968)  loss_mask: 0.0021 (0.0030)  time: 0.1709  data: 0.0002  max mem: 6052
[05:14:21.153434] Epoch: [58]  [ 80/781]  eta: 0:02:06  lr: 0.000103  training_loss: 1.2771 (1.2905)  classification_loss: 1.2758 (1.2878)  loss_mask: 0.0014 (0.0026)  time: 0.1709  data: 0.0002  max mem: 6052
[05:14:24.565367] Epoch: [58]  [100/781]  eta: 0:02:01  lr: 0.000102  training_loss: 1.2817 (1.2898)  classification_loss: 1.2803 (1.2874)  loss_mask: 0.0014 (0.0024)  time: 0.1705  data: 0.0002  max mem: 6052
[05:14:27.986321] Epoch: [58]  [120/781]  eta: 0:01:57  lr: 0.000102  training_loss: 1.2894 (1.2905)  classification_loss: 1.2880 (1.2883)  loss_mask: 0.0010 (0.0022)  time: 0.1709  data: 0.0002  max mem: 6052
[05:14:31.403096] Epoch: [58]  [140/781]  eta: 0:01:53  lr: 0.000102  training_loss: 1.2331 (1.2876)  classification_loss: 1.2315 (1.2856)  loss_mask: 0.0015 (0.0021)  time: 0.1708  data: 0.0005  max mem: 6052
[05:14:34.827827] Epoch: [58]  [160/781]  eta: 0:01:49  lr: 0.000102  training_loss: 1.2571 (1.2846)  classification_loss: 1.2556 (1.2826)  loss_mask: 0.0015 (0.0020)  time: 0.1712  data: 0.0006  max mem: 6052
[05:14:38.252229] Epoch: [58]  [180/781]  eta: 0:01:45  lr: 0.000102  training_loss: 1.2660 (1.2827)  classification_loss: 1.2654 (1.2808)  loss_mask: 0.0010 (0.0019)  time: 0.1712  data: 0.0002  max mem: 6052
[05:14:41.680155] Epoch: [58]  [200/781]  eta: 0:01:41  lr: 0.000102  training_loss: 1.2718 (1.2852)  classification_loss: 1.2706 (1.2834)  loss_mask: 0.0010 (0.0018)  time: 0.1713  data: 0.0002  max mem: 6052
[05:14:45.129001] Epoch: [58]  [220/781]  eta: 0:01:38  lr: 0.000102  training_loss: 1.2706 (1.2864)  classification_loss: 1.2694 (1.2847)  loss_mask: 0.0010 (0.0017)  time: 0.1724  data: 0.0004  max mem: 6052
[05:14:48.558497] Epoch: [58]  [240/781]  eta: 0:01:34  lr: 0.000102  training_loss: 1.2308 (1.2852)  classification_loss: 1.2303 (1.2835)  loss_mask: 0.0007 (0.0017)  time: 0.1714  data: 0.0002  max mem: 6052
[05:14:51.979042] Epoch: [58]  [260/781]  eta: 0:01:30  lr: 0.000102  training_loss: 1.2970 (1.2879)  classification_loss: 1.2957 (1.2863)  loss_mask: 0.0008 (0.0016)  time: 0.1709  data: 0.0002  max mem: 6052
[05:14:55.400750] Epoch: [58]  [280/781]  eta: 0:01:27  lr: 0.000102  training_loss: 1.2882 (1.2868)  classification_loss: 1.2870 (1.2852)  loss_mask: 0.0008 (0.0016)  time: 0.1710  data: 0.0003  max mem: 6052
[05:14:58.823352] Epoch: [58]  [300/781]  eta: 0:01:23  lr: 0.000101  training_loss: 1.2773 (1.2853)  classification_loss: 1.2769 (1.2838)  loss_mask: 0.0010 (0.0015)  time: 0.1710  data: 0.0003  max mem: 6052
[05:15:02.248926] Epoch: [58]  [320/781]  eta: 0:01:20  lr: 0.000101  training_loss: 1.2997 (1.2857)  classification_loss: 1.2996 (1.2842)  loss_mask: 0.0007 (0.0015)  time: 0.1712  data: 0.0002  max mem: 6052
[05:15:05.664183] Epoch: [58]  [340/781]  eta: 0:01:16  lr: 0.000101  training_loss: 1.2843 (1.2854)  classification_loss: 1.2835 (1.2839)  loss_mask: 0.0008 (0.0014)  time: 0.1706  data: 0.0002  max mem: 6052
[05:15:09.077373] Epoch: [58]  [360/781]  eta: 0:01:12  lr: 0.000101  training_loss: 1.2854 (1.2856)  classification_loss: 1.2845 (1.2842)  loss_mask: 0.0007 (0.0014)  time: 0.1706  data: 0.0002  max mem: 6052
[05:15:12.486825] Epoch: [58]  [380/781]  eta: 0:01:09  lr: 0.000101  training_loss: 1.2405 (1.2861)  classification_loss: 1.2398 (1.2847)  loss_mask: 0.0005 (0.0014)  time: 0.1704  data: 0.0002  max mem: 6052
[05:15:15.912660] Epoch: [58]  [400/781]  eta: 0:01:05  lr: 0.000101  training_loss: 1.3467 (1.2944)  classification_loss: 1.2559 (1.2846)  loss_mask: 0.0810 (0.0098)  time: 0.1712  data: 0.0002  max mem: 6052
[05:15:19.316258] Epoch: [58]  [420/781]  eta: 0:01:02  lr: 0.000101  training_loss: 1.3469 (1.3011)  classification_loss: 1.2589 (1.2843)  loss_mask: 0.0832 (0.0168)  time: 0.1701  data: 0.0002  max mem: 6052
[05:15:22.719462] Epoch: [58]  [440/781]  eta: 0:00:58  lr: 0.000101  training_loss: 1.3422 (1.3017)  classification_loss: 1.3093 (1.2840)  loss_mask: 0.0285 (0.0177)  time: 0.1701  data: 0.0002  max mem: 6052
[05:15:26.133403] Epoch: [58]  [460/781]  eta: 0:00:55  lr: 0.000101  training_loss: 1.3460 (1.3048)  classification_loss: 1.2888 (1.2853)  loss_mask: 0.0412 (0.0195)  time: 0.1706  data: 0.0002  max mem: 6052
[05:15:29.577911] Epoch: [58]  [480/781]  eta: 0:00:51  lr: 0.000100  training_loss: 1.3195 (1.3051)  classification_loss: 1.2888 (1.2851)  loss_mask: 0.0218 (0.0200)  time: 0.1721  data: 0.0003  max mem: 6052
[05:15:32.999494] Epoch: [58]  [500/781]  eta: 0:00:48  lr: 0.000100  training_loss: 1.3204 (1.3053)  classification_loss: 1.2943 (1.2853)  loss_mask: 0.0162 (0.0200)  time: 0.1710  data: 0.0002  max mem: 6052
[05:15:36.427270] Epoch: [58]  [520/781]  eta: 0:00:45  lr: 0.000100  training_loss: 1.3117 (1.3055)  classification_loss: 1.3024 (1.2859)  loss_mask: 0.0086 (0.0196)  time: 0.1713  data: 0.0002  max mem: 6052
[05:15:39.939406] Epoch: [58]  [540/781]  eta: 0:00:41  lr: 0.000100  training_loss: 1.2890 (1.3050)  classification_loss: 1.2783 (1.2859)  loss_mask: 0.0049 (0.0191)  time: 0.1755  data: 0.0002  max mem: 6052
[05:15:43.368602] Epoch: [58]  [560/781]  eta: 0:00:38  lr: 0.000100  training_loss: 1.2374 (1.3033)  classification_loss: 1.2356 (1.2847)  loss_mask: 0.0042 (0.0186)  time: 0.1714  data: 0.0003  max mem: 6052
[05:15:46.814622] Epoch: [58]  [580/781]  eta: 0:00:34  lr: 0.000100  training_loss: 1.2783 (1.3023)  classification_loss: 1.2748 (1.2843)  loss_mask: 0.0036 (0.0181)  time: 0.1722  data: 0.0003  max mem: 6052
[05:15:50.224519] Epoch: [58]  [600/781]  eta: 0:00:31  lr: 0.000100  training_loss: 1.2292 (1.3003)  classification_loss: 1.2274 (1.2828)  loss_mask: 0.0026 (0.0176)  time: 0.1704  data: 0.0002  max mem: 6052
[05:15:53.645979] Epoch: [58]  [620/781]  eta: 0:00:27  lr: 0.000100  training_loss: 1.2365 (1.2984)  classification_loss: 1.2348 (1.2813)  loss_mask: 0.0026 (0.0171)  time: 0.1710  data: 0.0002  max mem: 6052
[05:15:57.073861] Epoch: [58]  [640/781]  eta: 0:00:24  lr: 0.000100  training_loss: 1.2657 (1.2977)  classification_loss: 1.2645 (1.2810)  loss_mask: 0.0022 (0.0166)  time: 0.1713  data: 0.0002  max mem: 6052
[05:16:00.498159] Epoch: [58]  [660/781]  eta: 0:00:20  lr: 0.000100  training_loss: 1.2891 (1.2980)  classification_loss: 1.2791 (1.2815)  loss_mask: 0.0040 (0.0165)  time: 0.1711  data: 0.0001  max mem: 6052
[05:16:03.903916] Epoch: [58]  [680/781]  eta: 0:00:17  lr: 0.000099  training_loss: 1.3031 (1.2983)  classification_loss: 1.2959 (1.2822)  loss_mask: 0.0042 (0.0162)  time: 0.1702  data: 0.0002  max mem: 6052
[05:16:07.324738] Epoch: [58]  [700/781]  eta: 0:00:13  lr: 0.000099  training_loss: 1.3050 (1.2993)  classification_loss: 1.3032 (1.2836)  loss_mask: 0.0027 (0.0158)  time: 0.1710  data: 0.0002  max mem: 6052
[05:16:10.744736] Epoch: [58]  [720/781]  eta: 0:00:10  lr: 0.000099  training_loss: 1.3052 (1.2998)  classification_loss: 1.3035 (1.2844)  loss_mask: 0.0016 (0.0154)  time: 0.1709  data: 0.0003  max mem: 6052
[05:16:14.160354] Epoch: [58]  [740/781]  eta: 0:00:07  lr: 0.000099  training_loss: 1.2479 (1.2987)  classification_loss: 1.2454 (1.2837)  loss_mask: 0.0015 (0.0150)  time: 0.1707  data: 0.0002  max mem: 6052
[05:16:17.578601] Epoch: [58]  [760/781]  eta: 0:00:03  lr: 0.000099  training_loss: 1.2501 (1.2987)  classification_loss: 1.2483 (1.2841)  loss_mask: 0.0012 (0.0147)  time: 0.1708  data: 0.0002  max mem: 6052
[05:16:20.959147] Epoch: [58]  [780/781]  eta: 0:00:00  lr: 0.000099  training_loss: 1.3035 (1.2989)  classification_loss: 1.3019 (1.2846)  loss_mask: 0.0013 (0.0143)  time: 0.1689  data: 0.0001  max mem: 6052
[05:16:21.110378] Epoch: [58] Total time: 0:02:14 (0.1724 s / it)
[05:16:21.110875] Averaged stats: lr: 0.000099  training_loss: 1.3035 (1.2989)  classification_loss: 1.3019 (1.2846)  loss_mask: 0.0013 (0.0143)
[05:16:21.769385] Test:  [  0/157]  eta: 0:01:42  testing_loss: 0.5388 (0.5388)  acc1: 84.3750 (84.3750)  acc5: 100.0000 (100.0000)  time: 0.6542  data: 0.6249  max mem: 6052
[05:16:22.065074] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5534 (0.5823)  acc1: 82.8125 (80.9659)  acc5: 100.0000 (99.4318)  time: 0.0862  data: 0.0570  max mem: 6052
[05:16:22.373095] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5380 (0.5503)  acc1: 82.8125 (82.2917)  acc5: 100.0000 (99.4792)  time: 0.0300  data: 0.0002  max mem: 6052
[05:16:22.654558] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5477 (0.5627)  acc1: 82.8125 (82.1573)  acc5: 100.0000 (99.2944)  time: 0.0294  data: 0.0002  max mem: 6052
[05:16:22.934733] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5700 (0.5697)  acc1: 81.2500 (82.1646)  acc5: 98.4375 (99.0854)  time: 0.0280  data: 0.0001  max mem: 6052
[05:16:23.214845] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5531 (0.5619)  acc1: 82.8125 (82.5980)  acc5: 98.4375 (99.0502)  time: 0.0279  data: 0.0001  max mem: 6052
[05:16:23.495537] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5531 (0.5641)  acc1: 82.8125 (82.3770)  acc5: 98.4375 (99.0523)  time: 0.0279  data: 0.0001  max mem: 6052
[05:16:23.776601] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5344 (0.5581)  acc1: 82.8125 (82.5484)  acc5: 100.0000 (99.0977)  time: 0.0280  data: 0.0001  max mem: 6052
[05:16:24.057260] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5437 (0.5666)  acc1: 82.8125 (82.2145)  acc5: 100.0000 (98.9969)  time: 0.0280  data: 0.0001  max mem: 6052
[05:16:24.338773] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5623 (0.5641)  acc1: 81.2500 (82.4691)  acc5: 100.0000 (99.0556)  time: 0.0280  data: 0.0001  max mem: 6052
[05:16:24.621237] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5609 (0.5691)  acc1: 81.2500 (82.2092)  acc5: 100.0000 (99.0408)  time: 0.0280  data: 0.0001  max mem: 6052
[05:16:24.905623] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5839 (0.5708)  acc1: 79.6875 (82.1791)  acc5: 100.0000 (99.0850)  time: 0.0282  data: 0.0001  max mem: 6052
[05:16:25.192279] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5668 (0.5711)  acc1: 79.6875 (82.1023)  acc5: 100.0000 (99.0961)  time: 0.0284  data: 0.0001  max mem: 6052
[05:16:25.480622] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5668 (0.5702)  acc1: 82.8125 (82.2758)  acc5: 100.0000 (99.1054)  time: 0.0286  data: 0.0001  max mem: 6052
[05:16:25.764206] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5664 (0.5672)  acc1: 82.8125 (82.4136)  acc5: 100.0000 (99.1246)  time: 0.0283  data: 0.0001  max mem: 6052
[05:16:26.044059] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5701 (0.5662)  acc1: 82.8125 (82.4400)  acc5: 98.4375 (99.1101)  time: 0.0280  data: 0.0001  max mem: 6052
[05:16:26.193702] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5414 (0.5661)  acc1: 82.8125 (82.4200)  acc5: 98.4375 (99.1200)  time: 0.0269  data: 0.0001  max mem: 6052
[05:16:26.352945] Test: Total time: 0:00:05 (0.0334 s / it)
[05:16:26.353414] * Acc@1 82.420 Acc@5 99.120 loss 0.566
[05:16:26.353712] Accuracy of the network on the 10000 test images: 82.4%
[05:16:26.353914] Max accuracy: 82.75%
[05:16:26.553044] log_dir: ./output_dir
[05:16:27.445940] Epoch: [59]  [  0/781]  eta: 0:11:36  lr: 0.000099  training_loss: 0.9857 (0.9857)  classification_loss: 0.9840 (0.9840)  loss_mask: 0.0017 (0.0017)  time: 0.8914  data: 0.7091  max mem: 6052
[05:16:30.901934] Epoch: [59]  [ 20/781]  eta: 0:02:37  lr: 0.000099  training_loss: 1.2245 (1.2257)  classification_loss: 1.2231 (1.2244)  loss_mask: 0.0012 (0.0012)  time: 0.1726  data: 0.0002  max mem: 6052
[05:16:34.304344] Epoch: [59]  [ 40/781]  eta: 0:02:19  lr: 0.000099  training_loss: 1.2601 (1.2526)  classification_loss: 1.2585 (1.2514)  loss_mask: 0.0011 (0.0013)  time: 0.1700  data: 0.0003  max mem: 6052
[05:16:37.712104] Epoch: [59]  [ 60/781]  eta: 0:02:11  lr: 0.000099  training_loss: 1.2656 (1.2645)  classification_loss: 1.2641 (1.2632)  loss_mask: 0.0010 (0.0012)  time: 0.1703  data: 0.0003  max mem: 6052
[05:16:41.204432] Epoch: [59]  [ 80/781]  eta: 0:02:06  lr: 0.000099  training_loss: 1.2742 (1.2629)  classification_loss: 1.2734 (1.2617)  loss_mask: 0.0011 (0.0012)  time: 0.1745  data: 0.0002  max mem: 6052
[05:16:44.670446] Epoch: [59]  [100/781]  eta: 0:02:02  lr: 0.000098  training_loss: 1.3121 (1.2701)  classification_loss: 1.3119 (1.2689)  loss_mask: 0.0011 (0.0012)  time: 0.1732  data: 0.0002  max mem: 6052
[05:16:48.099584] Epoch: [59]  [120/781]  eta: 0:01:57  lr: 0.000098  training_loss: 1.2317 (1.2680)  classification_loss: 1.2301 (1.2668)  loss_mask: 0.0009 (0.0012)  time: 0.1714  data: 0.0003  max mem: 6052
[05:16:51.520502] Epoch: [59]  [140/781]  eta: 0:01:53  lr: 0.000098  training_loss: 1.2187 (1.2655)  classification_loss: 1.2175 (1.2644)  loss_mask: 0.0008 (0.0011)  time: 0.1710  data: 0.0002  max mem: 6052
[05:16:54.940826] Epoch: [59]  [160/781]  eta: 0:01:49  lr: 0.000098  training_loss: 1.2686 (1.2646)  classification_loss: 1.2675 (1.2635)  loss_mask: 0.0008 (0.0011)  time: 0.1709  data: 0.0002  max mem: 6052
[05:16:58.353828] Epoch: [59]  [180/781]  eta: 0:01:45  lr: 0.000098  training_loss: 1.2657 (1.2668)  classification_loss: 1.2650 (1.2657)  loss_mask: 0.0008 (0.0011)  time: 0.1706  data: 0.0002  max mem: 6052
[05:17:01.787433] Epoch: [59]  [200/781]  eta: 0:01:41  lr: 0.000098  training_loss: 1.2136 (1.2654)  classification_loss: 1.2124 (1.2637)  loss_mask: 0.0014 (0.0017)  time: 0.1716  data: 0.0002  max mem: 6052
[05:17:05.196109] Epoch: [59]  [220/781]  eta: 0:01:38  lr: 0.000098  training_loss: 1.4123 (1.2791)  classification_loss: 1.2920 (1.2690)  loss_mask: 0.0353 (0.0102)  time: 0.1704  data: 0.0002  max mem: 6052
[05:17:08.637764] Epoch: [59]  [240/781]  eta: 0:01:34  lr: 0.000098  training_loss: 1.2744 (1.2830)  classification_loss: 1.2409 (1.2688)  loss_mask: 0.0358 (0.0142)  time: 0.1720  data: 0.0002  max mem: 6052
[05:17:12.077878] Epoch: [59]  [260/781]  eta: 0:01:30  lr: 0.000098  training_loss: 1.3222 (1.2863)  classification_loss: 1.3014 (1.2710)  loss_mask: 0.0156 (0.0153)  time: 0.1719  data: 0.0003  max mem: 6052
[05:17:15.502397] Epoch: [59]  [280/781]  eta: 0:01:27  lr: 0.000098  training_loss: 1.2797 (1.2861)  classification_loss: 1.2759 (1.2709)  loss_mask: 0.0067 (0.0152)  time: 0.1712  data: 0.0002  max mem: 6052
[05:17:18.925218] Epoch: [59]  [300/781]  eta: 0:01:23  lr: 0.000097  training_loss: 1.3174 (1.2885)  classification_loss: 1.3108 (1.2741)  loss_mask: 0.0038 (0.0144)  time: 0.1710  data: 0.0002  max mem: 6052
[05:17:22.316621] Epoch: [59]  [320/781]  eta: 0:01:20  lr: 0.000097  training_loss: 1.2726 (1.2880)  classification_loss: 1.2722 (1.2742)  loss_mask: 0.0027 (0.0137)  time: 0.1695  data: 0.0002  max mem: 6052
[05:17:25.712091] Epoch: [59]  [340/781]  eta: 0:01:16  lr: 0.000097  training_loss: 1.2517 (1.2851)  classification_loss: 1.2490 (1.2720)  loss_mask: 0.0021 (0.0131)  time: 0.1697  data: 0.0002  max mem: 6052
[05:17:29.118071] Epoch: [59]  [360/781]  eta: 0:01:12  lr: 0.000097  training_loss: 1.2628 (1.2850)  classification_loss: 1.2613 (1.2726)  loss_mask: 0.0019 (0.0125)  time: 0.1702  data: 0.0003  max mem: 6052
[05:17:32.556995] Epoch: [59]  [380/781]  eta: 0:01:09  lr: 0.000097  training_loss: 1.2963 (1.2848)  classification_loss: 1.2955 (1.2730)  loss_mask: 0.0014 (0.0119)  time: 0.1719  data: 0.0003  max mem: 6052
[05:17:35.973726] Epoch: [59]  [400/781]  eta: 0:01:05  lr: 0.000097  training_loss: 1.2691 (1.2850)  classification_loss: 1.2672 (1.2737)  loss_mask: 0.0012 (0.0114)  time: 0.1707  data: 0.0002  max mem: 6052
[05:17:39.392954] Epoch: [59]  [420/781]  eta: 0:01:02  lr: 0.000097  training_loss: 1.3173 (1.2854)  classification_loss: 1.3055 (1.2733)  loss_mask: 0.0041 (0.0121)  time: 0.1709  data: 0.0002  max mem: 6052
[05:17:42.823355] Epoch: [59]  [440/781]  eta: 0:00:58  lr: 0.000097  training_loss: 1.2331 (1.2839)  classification_loss: 1.2235 (1.2719)  loss_mask: 0.0070 (0.0119)  time: 0.1714  data: 0.0002  max mem: 6052
[05:17:46.243977] Epoch: [59]  [460/781]  eta: 0:00:55  lr: 0.000097  training_loss: 1.2275 (1.2813)  classification_loss: 1.2255 (1.2697)  loss_mask: 0.0026 (0.0116)  time: 0.1710  data: 0.0001  max mem: 6052
[05:17:49.659227] Epoch: [59]  [480/781]  eta: 0:00:51  lr: 0.000096  training_loss: 1.2717 (1.2804)  classification_loss: 1.2705 (1.2692)  loss_mask: 0.0017 (0.0112)  time: 0.1707  data: 0.0002  max mem: 6052
[05:17:53.061793] Epoch: [59]  [500/781]  eta: 0:00:48  lr: 0.000096  training_loss: 1.2823 (1.2818)  classification_loss: 1.2810 (1.2710)  loss_mask: 0.0013 (0.0108)  time: 0.1701  data: 0.0003  max mem: 6052
[05:17:56.471674] Epoch: [59]  [520/781]  eta: 0:00:45  lr: 0.000096  training_loss: 1.2755 (1.2825)  classification_loss: 1.2747 (1.2720)  loss_mask: 0.0012 (0.0104)  time: 0.1704  data: 0.0002  max mem: 6052
[05:17:59.871929] Epoch: [59]  [540/781]  eta: 0:00:41  lr: 0.000096  training_loss: 1.2963 (1.2838)  classification_loss: 1.2950 (1.2737)  loss_mask: 0.0010 (0.0101)  time: 0.1699  data: 0.0002  max mem: 6052
[05:18:03.282728] Epoch: [59]  [560/781]  eta: 0:00:38  lr: 0.000096  training_loss: 1.2611 (1.2834)  classification_loss: 1.2603 (1.2736)  loss_mask: 0.0008 (0.0098)  time: 0.1704  data: 0.0003  max mem: 6052
[05:18:06.706002] Epoch: [59]  [580/781]  eta: 0:00:34  lr: 0.000096  training_loss: 1.2557 (1.2821)  classification_loss: 1.2544 (1.2726)  loss_mask: 0.0008 (0.0095)  time: 0.1711  data: 0.0002  max mem: 6052
[05:18:10.116807] Epoch: [59]  [600/781]  eta: 0:00:31  lr: 0.000096  training_loss: 1.2525 (1.2816)  classification_loss: 1.2520 (1.2724)  loss_mask: 0.0006 (0.0092)  time: 0.1705  data: 0.0003  max mem: 6052
[05:18:13.519733] Epoch: [59]  [620/781]  eta: 0:00:27  lr: 0.000096  training_loss: 1.2530 (1.2807)  classification_loss: 1.2525 (1.2719)  loss_mask: 0.0007 (0.0089)  time: 0.1701  data: 0.0003  max mem: 6052
[05:18:16.908278] Epoch: [59]  [640/781]  eta: 0:00:24  lr: 0.000096  training_loss: 1.2766 (1.2818)  classification_loss: 1.2756 (1.2732)  loss_mask: 0.0007 (0.0086)  time: 0.1694  data: 0.0003  max mem: 6052
[05:18:20.332753] Epoch: [59]  [660/781]  eta: 0:00:20  lr: 0.000096  training_loss: 1.2570 (1.2816)  classification_loss: 1.2565 (1.2732)  loss_mask: 0.0007 (0.0084)  time: 0.1712  data: 0.0003  max mem: 6052
[05:18:23.736316] Epoch: [59]  [680/781]  eta: 0:00:17  lr: 0.000095  training_loss: 1.2566 (1.2813)  classification_loss: 1.2510 (1.2729)  loss_mask: 0.0021 (0.0084)  time: 0.1701  data: 0.0002  max mem: 6052
[05:18:27.142413] Epoch: [59]  [700/781]  eta: 0:00:13  lr: 0.000095  training_loss: 1.2883 (1.2814)  classification_loss: 1.2868 (1.2731)  loss_mask: 0.0013 (0.0082)  time: 0.1702  data: 0.0003  max mem: 6052
[05:18:30.555485] Epoch: [59]  [720/781]  eta: 0:00:10  lr: 0.000095  training_loss: 1.2779 (1.2818)  classification_loss: 1.2769 (1.2738)  loss_mask: 0.0007 (0.0080)  time: 0.1706  data: 0.0002  max mem: 6052
[05:18:33.970314] Epoch: [59]  [740/781]  eta: 0:00:07  lr: 0.000095  training_loss: 1.2277 (1.2807)  classification_loss: 1.2272 (1.2729)  loss_mask: 0.0007 (0.0078)  time: 0.1707  data: 0.0002  max mem: 6052
[05:18:37.374424] Epoch: [59]  [760/781]  eta: 0:00:03  lr: 0.000095  training_loss: 1.2981 (1.2811)  classification_loss: 1.2970 (1.2734)  loss_mask: 0.0008 (0.0077)  time: 0.1701  data: 0.0002  max mem: 6052
[05:18:40.779229] Epoch: [59]  [780/781]  eta: 0:00:00  lr: 0.000095  training_loss: 1.2231 (1.2800)  classification_loss: 1.2222 (1.2725)  loss_mask: 0.0007 (0.0075)  time: 0.1702  data: 0.0002  max mem: 6052
[05:18:40.930143] Epoch: [59] Total time: 0:02:14 (0.1721 s / it)
[05:18:40.930615] Averaged stats: lr: 0.000095  training_loss: 1.2231 (1.2800)  classification_loss: 1.2222 (1.2725)  loss_mask: 0.0007 (0.0075)
[05:18:41.594696] Test:  [  0/157]  eta: 0:01:43  testing_loss: 0.4926 (0.4926)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.6588  data: 0.6264  max mem: 6052
[05:18:41.892267] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5810 (0.5758)  acc1: 82.8125 (81.5341)  acc5: 100.0000 (99.5739)  time: 0.0867  data: 0.0573  max mem: 6052
[05:18:42.180949] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5373 (0.5414)  acc1: 82.8125 (83.4077)  acc5: 100.0000 (99.6280)  time: 0.0291  data: 0.0003  max mem: 6052
[05:18:42.468939] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5393 (0.5473)  acc1: 82.8125 (83.3669)  acc5: 100.0000 (99.3952)  time: 0.0287  data: 0.0002  max mem: 6052
[05:18:42.751698] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5406 (0.5559)  acc1: 82.8125 (83.0412)  acc5: 98.4375 (99.2378)  time: 0.0284  data: 0.0001  max mem: 6052
[05:18:43.038919] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5382 (0.5475)  acc1: 84.3750 (83.3640)  acc5: 98.4375 (99.2034)  time: 0.0283  data: 0.0001  max mem: 6052
[05:18:43.320921] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5237 (0.5466)  acc1: 84.3750 (83.2736)  acc5: 100.0000 (99.1803)  time: 0.0283  data: 0.0002  max mem: 6052
[05:18:43.603245] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5237 (0.5413)  acc1: 84.3750 (83.5607)  acc5: 100.0000 (99.1417)  time: 0.0280  data: 0.0002  max mem: 6052
[05:18:43.891833] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5429 (0.5468)  acc1: 82.8125 (83.2369)  acc5: 98.4375 (99.1319)  time: 0.0284  data: 0.0002  max mem: 6052
[05:18:44.174122] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5442 (0.5431)  acc1: 82.8125 (83.4135)  acc5: 100.0000 (99.1587)  time: 0.0284  data: 0.0002  max mem: 6052
[05:18:44.461746] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5506 (0.5467)  acc1: 84.3750 (83.2611)  acc5: 100.0000 (99.1646)  time: 0.0284  data: 0.0001  max mem: 6052
[05:18:44.744249] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5746 (0.5477)  acc1: 81.2500 (83.2066)  acc5: 100.0000 (99.1695)  time: 0.0284  data: 0.0002  max mem: 6052
[05:18:45.027849] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5098 (0.5461)  acc1: 82.8125 (83.2257)  acc5: 100.0000 (99.1606)  time: 0.0282  data: 0.0002  max mem: 6052
[05:18:45.311536] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5742 (0.5478)  acc1: 82.8125 (83.2180)  acc5: 100.0000 (99.1651)  time: 0.0282  data: 0.0001  max mem: 6052
[05:18:45.595552] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5686 (0.5451)  acc1: 82.8125 (83.3666)  acc5: 100.0000 (99.2021)  time: 0.0282  data: 0.0001  max mem: 6052
[05:18:45.873844] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5436 (0.5447)  acc1: 82.8125 (83.3402)  acc5: 100.0000 (99.2550)  time: 0.0280  data: 0.0001  max mem: 6052
[05:18:46.023823] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5436 (0.5449)  acc1: 84.3750 (83.3300)  acc5: 100.0000 (99.2500)  time: 0.0269  data: 0.0001  max mem: 6052
[05:18:46.194070] Test: Total time: 0:00:05 (0.0335 s / it)
[05:18:46.194806] * Acc@1 83.330 Acc@5 99.250 loss 0.545
[05:18:46.195212] Accuracy of the network on the 10000 test images: 83.3%
[05:18:46.195400] Max accuracy: 83.33%
[05:18:46.383151] log_dir: ./output_dir
[05:18:47.239590] Epoch: [60]  [  0/781]  eta: 0:11:07  lr: 0.000095  training_loss: 1.1349 (1.1349)  classification_loss: 1.1345 (1.1345)  loss_mask: 0.0004 (0.0004)  time: 0.8549  data: 0.6797  max mem: 6052
[05:18:50.703129] Epoch: [60]  [ 20/781]  eta: 0:02:36  lr: 0.000095  training_loss: 1.2202 (1.2525)  classification_loss: 1.2196 (1.2519)  loss_mask: 0.0006 (0.0006)  time: 0.1731  data: 0.0002  max mem: 6052
[05:18:54.164873] Epoch: [60]  [ 40/781]  eta: 0:02:20  lr: 0.000095  training_loss: 1.2390 (1.2620)  classification_loss: 1.2379 (1.2614)  loss_mask: 0.0005 (0.0006)  time: 0.1729  data: 0.0002  max mem: 6052
[05:18:57.645575] Epoch: [60]  [ 60/781]  eta: 0:02:13  lr: 0.000095  training_loss: 1.2940 (1.2763)  classification_loss: 1.2936 (1.2758)  loss_mask: 0.0004 (0.0006)  time: 0.1739  data: 0.0002  max mem: 6052
[05:19:01.053538] Epoch: [60]  [ 80/781]  eta: 0:02:06  lr: 0.000095  training_loss: 1.2509 (1.2734)  classification_loss: 1.2506 (1.2729)  loss_mask: 0.0004 (0.0006)  time: 0.1703  data: 0.0002  max mem: 6052
[05:19:04.468731] Epoch: [60]  [100/781]  eta: 0:02:01  lr: 0.000094  training_loss: 1.2711 (1.2727)  classification_loss: 1.2707 (1.2721)  loss_mask: 0.0007 (0.0006)  time: 0.1707  data: 0.0003  max mem: 6052
[05:19:07.881091] Epoch: [60]  [120/781]  eta: 0:01:57  lr: 0.000094  training_loss: 1.2811 (1.2714)  classification_loss: 1.2806 (1.2708)  loss_mask: 0.0004 (0.0005)  time: 0.1705  data: 0.0002  max mem: 6052
[05:19:11.286601] Epoch: [60]  [140/781]  eta: 0:01:53  lr: 0.000094  training_loss: 1.2719 (1.2743)  classification_loss: 1.2714 (1.2737)  loss_mask: 0.0005 (0.0005)  time: 0.1702  data: 0.0001  max mem: 6052
[05:19:14.710399] Epoch: [60]  [160/781]  eta: 0:01:49  lr: 0.000094  training_loss: 1.2363 (1.2690)  classification_loss: 1.2359 (1.2685)  loss_mask: 0.0004 (0.0005)  time: 0.1711  data: 0.0001  max mem: 6052
[05:19:18.129055] Epoch: [60]  [180/781]  eta: 0:01:45  lr: 0.000094  training_loss: 1.2479 (1.2687)  classification_loss: 1.2475 (1.2682)  loss_mask: 0.0004 (0.0005)  time: 0.1708  data: 0.0001  max mem: 6052
[05:19:21.540125] Epoch: [60]  [200/781]  eta: 0:01:41  lr: 0.000094  training_loss: 1.2681 (1.2706)  classification_loss: 1.2677 (1.2701)  loss_mask: 0.0004 (0.0005)  time: 0.1705  data: 0.0003  max mem: 6052
[05:19:24.969174] Epoch: [60]  [220/781]  eta: 0:01:37  lr: 0.000094  training_loss: 1.2758 (1.2712)  classification_loss: 1.2754 (1.2707)  loss_mask: 0.0004 (0.0005)  time: 0.1714  data: 0.0002  max mem: 6052
[05:19:28.383766] Epoch: [60]  [240/781]  eta: 0:01:34  lr: 0.000094  training_loss: 1.3232 (1.2751)  classification_loss: 1.3225 (1.2746)  loss_mask: 0.0006 (0.0005)  time: 0.1706  data: 0.0002  max mem: 6052
[05:19:31.792702] Epoch: [60]  [260/781]  eta: 0:01:30  lr: 0.000094  training_loss: 1.3101 (1.2769)  classification_loss: 1.3044 (1.2758)  loss_mask: 0.0006 (0.0012)  time: 0.1704  data: 0.0002  max mem: 6052
[05:19:35.200310] Epoch: [60]  [280/781]  eta: 0:01:26  lr: 0.000094  training_loss: 1.2557 (1.2770)  classification_loss: 1.2549 (1.2758)  loss_mask: 0.0007 (0.0011)  time: 0.1703  data: 0.0002  max mem: 6052
[05:19:38.620032] Epoch: [60]  [300/781]  eta: 0:01:23  lr: 0.000093  training_loss: 1.2877 (1.2779)  classification_loss: 1.2863 (1.2768)  loss_mask: 0.0005 (0.0011)  time: 0.1709  data: 0.0003  max mem: 6052
[05:19:42.031276] Epoch: [60]  [320/781]  eta: 0:01:19  lr: 0.000093  training_loss: 1.2602 (1.2774)  classification_loss: 1.2583 (1.2763)  loss_mask: 0.0005 (0.0011)  time: 0.1705  data: 0.0002  max mem: 6052
[05:19:45.444792] Epoch: [60]  [340/781]  eta: 0:01:16  lr: 0.000093  training_loss: 1.2259 (1.2752)  classification_loss: 1.2253 (1.2742)  loss_mask: 0.0006 (0.0011)  time: 0.1706  data: 0.0002  max mem: 6052
[05:19:48.875221] Epoch: [60]  [360/781]  eta: 0:01:12  lr: 0.000093  training_loss: 1.2242 (1.2733)  classification_loss: 1.2235 (1.2723)  loss_mask: 0.0004 (0.0010)  time: 0.1715  data: 0.0003  max mem: 6052
[05:19:52.296236] Epoch: [60]  [380/781]  eta: 0:01:09  lr: 0.000093  training_loss: 1.2945 (1.2747)  classification_loss: 1.2940 (1.2737)  loss_mask: 0.0003 (0.0010)  time: 0.1710  data: 0.0002  max mem: 6052
[05:19:55.743649] Epoch: [60]  [400/781]  eta: 0:01:05  lr: 0.000093  training_loss: 1.2890 (1.2756)  classification_loss: 1.2881 (1.2744)  loss_mask: 0.0006 (0.0012)  time: 0.1722  data: 0.0002  max mem: 6052
[05:19:59.189717] Epoch: [60]  [420/781]  eta: 0:01:02  lr: 0.000093  training_loss: 1.4149 (1.2812)  classification_loss: 1.2509 (1.2744)  loss_mask: 0.0455 (0.0068)  time: 0.1722  data: 0.0002  max mem: 6052
[05:20:02.624359] Epoch: [60]  [440/781]  eta: 0:00:58  lr: 0.000093  training_loss: 1.3617 (1.2849)  classification_loss: 1.2635 (1.2746)  loss_mask: 0.0741 (0.0102)  time: 0.1717  data: 0.0002  max mem: 6052
[05:20:06.050246] Epoch: [60]  [460/781]  eta: 0:00:55  lr: 0.000093  training_loss: 1.2901 (1.2842)  classification_loss: 1.2344 (1.2736)  loss_mask: 0.0103 (0.0106)  time: 0.1712  data: 0.0003  max mem: 6052
[05:20:09.464713] Epoch: [60]  [480/781]  eta: 0:00:51  lr: 0.000092  training_loss: 1.2867 (1.2850)  classification_loss: 1.2759 (1.2746)  loss_mask: 0.0065 (0.0104)  time: 0.1707  data: 0.0002  max mem: 6052
[05:20:12.926357] Epoch: [60]  [500/781]  eta: 0:00:48  lr: 0.000092  training_loss: 1.2255 (1.2833)  classification_loss: 1.2210 (1.2732)  loss_mask: 0.0029 (0.0101)  time: 0.1730  data: 0.0002  max mem: 6052
[05:20:16.346998] Epoch: [60]  [520/781]  eta: 0:00:45  lr: 0.000092  training_loss: 1.2389 (1.2819)  classification_loss: 1.2342 (1.2721)  loss_mask: 0.0022 (0.0098)  time: 0.1709  data: 0.0002  max mem: 6052
[05:20:19.801051] Epoch: [60]  [540/781]  eta: 0:00:41  lr: 0.000092  training_loss: 1.2405 (1.2813)  classification_loss: 1.2398 (1.2718)  loss_mask: 0.0018 (0.0095)  time: 0.1726  data: 0.0002  max mem: 6052
[05:20:23.223750] Epoch: [60]  [560/781]  eta: 0:00:38  lr: 0.000092  training_loss: 1.2234 (1.2798)  classification_loss: 1.2218 (1.2705)  loss_mask: 0.0017 (0.0093)  time: 0.1711  data: 0.0002  max mem: 6052
[05:20:26.643829] Epoch: [60]  [580/781]  eta: 0:00:34  lr: 0.000092  training_loss: 1.2820 (1.2801)  classification_loss: 1.2811 (1.2711)  loss_mask: 0.0014 (0.0090)  time: 0.1709  data: 0.0002  max mem: 6052
[05:20:30.077961] Epoch: [60]  [600/781]  eta: 0:00:31  lr: 0.000092  training_loss: 1.2727 (1.2793)  classification_loss: 1.2712 (1.2706)  loss_mask: 0.0013 (0.0087)  time: 0.1716  data: 0.0002  max mem: 6052
[05:20:33.504309] Epoch: [60]  [620/781]  eta: 0:00:27  lr: 0.000092  training_loss: 1.2783 (1.2797)  classification_loss: 1.2775 (1.2712)  loss_mask: 0.0010 (0.0085)  time: 0.1712  data: 0.0002  max mem: 6052
[05:20:36.922708] Epoch: [60]  [640/781]  eta: 0:00:24  lr: 0.000092  training_loss: 1.2559 (1.2791)  classification_loss: 1.2554 (1.2709)  loss_mask: 0.0009 (0.0083)  time: 0.1708  data: 0.0003  max mem: 6052
[05:20:40.334571] Epoch: [60]  [660/781]  eta: 0:00:20  lr: 0.000092  training_loss: 1.2362 (1.2784)  classification_loss: 1.2355 (1.2703)  loss_mask: 0.0008 (0.0080)  time: 0.1705  data: 0.0002  max mem: 6052
[05:20:43.742781] Epoch: [60]  [680/781]  eta: 0:00:17  lr: 0.000091  training_loss: 1.2780 (1.2783)  classification_loss: 1.2773 (1.2705)  loss_mask: 0.0010 (0.0078)  time: 0.1703  data: 0.0002  max mem: 6052
[05:20:47.161262] Epoch: [60]  [700/781]  eta: 0:00:13  lr: 0.000091  training_loss: 1.2482 (1.2779)  classification_loss: 1.2469 (1.2703)  loss_mask: 0.0008 (0.0076)  time: 0.1708  data: 0.0002  max mem: 6052
[05:20:50.574450] Epoch: [60]  [720/781]  eta: 0:00:10  lr: 0.000091  training_loss: 1.3205 (1.2793)  classification_loss: 1.3196 (1.2718)  loss_mask: 0.0007 (0.0074)  time: 0.1706  data: 0.0002  max mem: 6052
[05:20:54.000537] Epoch: [60]  [740/781]  eta: 0:00:07  lr: 0.000091  training_loss: 1.2469 (1.2792)  classification_loss: 1.2464 (1.2719)  loss_mask: 0.0005 (0.0073)  time: 0.1712  data: 0.0002  max mem: 6052
[05:20:57.457108] Epoch: [60]  [760/781]  eta: 0:00:03  lr: 0.000091  training_loss: 1.2968 (1.2798)  classification_loss: 1.2960 (1.2727)  loss_mask: 0.0008 (0.0071)  time: 0.1727  data: 0.0002  max mem: 6052
[05:21:00.953359] Epoch: [60]  [780/781]  eta: 0:00:00  lr: 0.000091  training_loss: 1.2510 (1.2796)  classification_loss: 1.2505 (1.2726)  loss_mask: 0.0006 (0.0069)  time: 0.1747  data: 0.0002  max mem: 6052
[05:21:01.107363] Epoch: [60] Total time: 0:02:14 (0.1725 s / it)
[05:21:01.107864] Averaged stats: lr: 0.000091  training_loss: 1.2510 (1.2796)  classification_loss: 1.2505 (1.2726)  loss_mask: 0.0006 (0.0069)
[05:21:02.188871] Test:  [  0/157]  eta: 0:01:43  testing_loss: 0.4554 (0.4554)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (100.0000)  time: 0.6568  data: 0.6275  max mem: 6052
[05:21:02.480407] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5793 (0.5755)  acc1: 79.6875 (81.3920)  acc5: 100.0000 (99.4318)  time: 0.0860  data: 0.0572  max mem: 6052
[05:21:02.764928] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5596 (0.5510)  acc1: 82.8125 (83.1101)  acc5: 100.0000 (99.4048)  time: 0.0286  data: 0.0002  max mem: 6052
[05:21:03.047328] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5443 (0.5549)  acc1: 84.3750 (83.0141)  acc5: 100.0000 (99.1935)  time: 0.0282  data: 0.0002  max mem: 6052
[05:21:03.333235] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5534 (0.5624)  acc1: 82.8125 (82.5457)  acc5: 98.4375 (99.0854)  time: 0.0283  data: 0.0002  max mem: 6052
[05:21:03.618881] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5422 (0.5517)  acc1: 81.2500 (82.9657)  acc5: 98.4375 (99.1115)  time: 0.0284  data: 0.0002  max mem: 6052
[05:21:03.907399] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5250 (0.5504)  acc1: 84.3750 (83.0430)  acc5: 100.0000 (99.1035)  time: 0.0285  data: 0.0002  max mem: 6052
[05:21:04.194251] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5250 (0.5468)  acc1: 82.8125 (83.1866)  acc5: 100.0000 (99.1857)  time: 0.0286  data: 0.0003  max mem: 6052
[05:21:04.481462] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5427 (0.5549)  acc1: 81.2500 (82.9090)  acc5: 100.0000 (99.1319)  time: 0.0286  data: 0.0004  max mem: 6052
[05:21:04.770208] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5493 (0.5512)  acc1: 84.3750 (83.2074)  acc5: 100.0000 (99.1930)  time: 0.0287  data: 0.0003  max mem: 6052
[05:21:05.059818] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5493 (0.5575)  acc1: 81.2500 (82.8125)  acc5: 100.0000 (99.1491)  time: 0.0288  data: 0.0003  max mem: 6052
[05:21:05.345773] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5800 (0.5584)  acc1: 81.2500 (82.8407)  acc5: 98.4375 (99.1413)  time: 0.0286  data: 0.0002  max mem: 6052
[05:21:05.626122] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5266 (0.5554)  acc1: 84.3750 (82.9287)  acc5: 98.4375 (99.1219)  time: 0.0282  data: 0.0002  max mem: 6052
[05:21:05.906950] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5090 (0.5560)  acc1: 84.3750 (82.9556)  acc5: 98.4375 (99.1054)  time: 0.0279  data: 0.0001  max mem: 6052
[05:21:06.186695] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5164 (0.5532)  acc1: 82.8125 (83.0230)  acc5: 100.0000 (99.1246)  time: 0.0279  data: 0.0001  max mem: 6052
[05:21:06.464497] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5481 (0.5534)  acc1: 81.2500 (82.9367)  acc5: 100.0000 (99.1308)  time: 0.0278  data: 0.0001  max mem: 6052
[05:21:06.614613] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5464 (0.5530)  acc1: 81.2500 (82.9100)  acc5: 100.0000 (99.1500)  time: 0.0268  data: 0.0001  max mem: 6052
[05:21:06.764373] Test: Total time: 0:00:05 (0.0333 s / it)
[05:21:06.764920] * Acc@1 82.910 Acc@5 99.150 loss 0.553
[05:21:06.765206] Accuracy of the network on the 10000 test images: 82.9%
[05:21:06.765389] Max accuracy: 83.33%
[05:21:06.919893] log_dir: ./output_dir
[05:21:07.777364] Epoch: [61]  [  0/781]  eta: 0:11:08  lr: 0.000091  training_loss: 1.0599 (1.0599)  classification_loss: 1.0591 (1.0591)  loss_mask: 0.0008 (0.0008)  time: 0.8555  data: 0.6616  max mem: 6052
[05:21:11.241256] Epoch: [61]  [ 20/781]  eta: 0:02:36  lr: 0.000091  training_loss: 1.2580 (1.2666)  classification_loss: 1.2572 (1.2660)  loss_mask: 0.0006 (0.0006)  time: 0.1731  data: 0.0003  max mem: 6052
[05:21:14.694622] Epoch: [61]  [ 40/781]  eta: 0:02:20  lr: 0.000091  training_loss: 1.3059 (1.2704)  classification_loss: 1.3055 (1.2698)  loss_mask: 0.0006 (0.0006)  time: 0.1726  data: 0.0002  max mem: 6052
[05:21:18.267607] Epoch: [61]  [ 60/781]  eta: 0:02:14  lr: 0.000091  training_loss: 1.2651 (1.2735)  classification_loss: 1.2644 (1.2729)  loss_mask: 0.0005 (0.0006)  time: 0.1786  data: 0.0002  max mem: 6052
[05:21:21.764413] Epoch: [61]  [ 80/781]  eta: 0:02:08  lr: 0.000091  training_loss: 1.2454 (1.2727)  classification_loss: 1.2451 (1.2721)  loss_mask: 0.0005 (0.0006)  time: 0.1748  data: 0.0002  max mem: 6052
[05:21:25.183775] Epoch: [61]  [100/781]  eta: 0:02:03  lr: 0.000090  training_loss: 1.2751 (1.2766)  classification_loss: 1.2747 (1.2761)  loss_mask: 0.0004 (0.0006)  time: 0.1709  data: 0.0003  max mem: 6052
[05:21:28.593564] Epoch: [61]  [120/781]  eta: 0:01:58  lr: 0.000090  training_loss: 1.2568 (1.2747)  classification_loss: 1.2558 (1.2742)  loss_mask: 0.0004 (0.0005)  time: 0.1704  data: 0.0003  max mem: 6052
[05:21:31.999093] Epoch: [61]  [140/781]  eta: 0:01:53  lr: 0.000090  training_loss: 1.2750 (1.2724)  classification_loss: 1.2746 (1.2719)  loss_mask: 0.0004 (0.0005)  time: 0.1702  data: 0.0002  max mem: 6052
[05:21:35.435050] Epoch: [61]  [160/781]  eta: 0:01:49  lr: 0.000090  training_loss: 1.2120 (1.2669)  classification_loss: 1.2117 (1.2664)  loss_mask: 0.0004 (0.0005)  time: 0.1717  data: 0.0002  max mem: 6052
[05:21:38.842190] Epoch: [61]  [180/781]  eta: 0:01:45  lr: 0.000090  training_loss: 1.2428 (1.2655)  classification_loss: 1.2425 (1.2650)  loss_mask: 0.0004 (0.0005)  time: 0.1703  data: 0.0002  max mem: 6052
[05:21:42.238061] Epoch: [61]  [200/781]  eta: 0:01:42  lr: 0.000090  training_loss: 1.2267 (1.2656)  classification_loss: 1.2252 (1.2644)  loss_mask: 0.0010 (0.0011)  time: 0.1697  data: 0.0002  max mem: 6052
[05:21:45.644052] Epoch: [61]  [220/781]  eta: 0:01:38  lr: 0.000090  training_loss: 1.2175 (1.2621)  classification_loss: 1.2167 (1.2610)  loss_mask: 0.0008 (0.0011)  time: 0.1702  data: 0.0001  max mem: 6052
[05:21:49.062064] Epoch: [61]  [240/781]  eta: 0:01:34  lr: 0.000090  training_loss: 1.2287 (1.2611)  classification_loss: 1.2282 (1.2601)  loss_mask: 0.0004 (0.0011)  time: 0.1708  data: 0.0003  max mem: 6052
[05:21:52.480112] Epoch: [61]  [260/781]  eta: 0:01:30  lr: 0.000090  training_loss: 1.2335 (1.2603)  classification_loss: 1.2331 (1.2593)  loss_mask: 0.0003 (0.0010)  time: 0.1708  data: 0.0002  max mem: 6052
[05:21:55.878722] Epoch: [61]  [280/781]  eta: 0:01:27  lr: 0.000090  training_loss: 1.2226 (1.2584)  classification_loss: 1.2223 (1.2574)  loss_mask: 0.0004 (0.0010)  time: 0.1698  data: 0.0002  max mem: 6052
[05:21:59.279187] Epoch: [61]  [300/781]  eta: 0:01:23  lr: 0.000089  training_loss: 1.3017 (1.2602)  classification_loss: 1.3014 (1.2592)  loss_mask: 0.0004 (0.0009)  time: 0.1700  data: 0.0002  max mem: 6052
[05:22:02.686806] Epoch: [61]  [320/781]  eta: 0:01:20  lr: 0.000089  training_loss: 1.2842 (1.2612)  classification_loss: 1.2839 (1.2603)  loss_mask: 0.0004 (0.0009)  time: 0.1703  data: 0.0002  max mem: 6052
[05:22:06.085630] Epoch: [61]  [340/781]  eta: 0:01:16  lr: 0.000089  training_loss: 1.2667 (1.2617)  classification_loss: 1.2665 (1.2608)  loss_mask: 0.0003 (0.0009)  time: 0.1699  data: 0.0002  max mem: 6052
[05:22:09.526679] Epoch: [61]  [360/781]  eta: 0:01:12  lr: 0.000089  training_loss: 1.2392 (1.2609)  classification_loss: 1.2389 (1.2600)  loss_mask: 0.0003 (0.0008)  time: 0.1720  data: 0.0002  max mem: 6052
[05:22:13.000185] Epoch: [61]  [380/781]  eta: 0:01:09  lr: 0.000089  training_loss: 1.2397 (1.2606)  classification_loss: 1.2396 (1.2597)  loss_mask: 0.0003 (0.0008)  time: 0.1736  data: 0.0002  max mem: 6052
[05:22:16.429386] Epoch: [61]  [400/781]  eta: 0:01:06  lr: 0.000089  training_loss: 1.2548 (1.2620)  classification_loss: 1.2544 (1.2612)  loss_mask: 0.0003 (0.0008)  time: 0.1714  data: 0.0002  max mem: 6052
[05:22:19.849640] Epoch: [61]  [420/781]  eta: 0:01:02  lr: 0.000089  training_loss: 1.2565 (1.2625)  classification_loss: 1.2562 (1.2617)  loss_mask: 0.0003 (0.0008)  time: 0.1709  data: 0.0003  max mem: 6052
[05:22:23.329903] Epoch: [61]  [440/781]  eta: 0:00:59  lr: 0.000089  training_loss: 1.2455 (1.2613)  classification_loss: 1.2453 (1.2606)  loss_mask: 0.0002 (0.0007)  time: 0.1739  data: 0.0003  max mem: 6052
[05:22:26.754720] Epoch: [61]  [460/781]  eta: 0:00:55  lr: 0.000089  training_loss: 1.2439 (1.2601)  classification_loss: 1.2438 (1.2593)  loss_mask: 0.0003 (0.0007)  time: 0.1712  data: 0.0003  max mem: 6052
[05:22:30.264676] Epoch: [61]  [480/781]  eta: 0:00:52  lr: 0.000089  training_loss: 1.2436 (1.2604)  classification_loss: 1.2434 (1.2597)  loss_mask: 0.0002 (0.0007)  time: 0.1754  data: 0.0002  max mem: 6052
[05:22:33.681978] Epoch: [61]  [500/781]  eta: 0:00:48  lr: 0.000088  training_loss: 1.2709 (1.2601)  classification_loss: 1.2707 (1.2594)  loss_mask: 0.0003 (0.0007)  time: 0.1708  data: 0.0003  max mem: 6052
[05:22:37.104446] Epoch: [61]  [520/781]  eta: 0:00:45  lr: 0.000088  training_loss: 1.1870 (1.2585)  classification_loss: 1.1867 (1.2578)  loss_mask: 0.0002 (0.0007)  time: 0.1711  data: 0.0004  max mem: 6052
[05:22:40.522259] Epoch: [61]  [540/781]  eta: 0:00:41  lr: 0.000088  training_loss: 1.2301 (1.2570)  classification_loss: 1.2297 (1.2563)  loss_mask: 0.0003 (0.0007)  time: 0.1708  data: 0.0002  max mem: 6052
[05:22:43.916852] Epoch: [61]  [560/781]  eta: 0:00:38  lr: 0.000088  training_loss: 1.2233 (1.2565)  classification_loss: 1.2228 (1.2558)  loss_mask: 0.0003 (0.0006)  time: 0.1697  data: 0.0002  max mem: 6052
[05:22:47.334153] Epoch: [61]  [580/781]  eta: 0:00:34  lr: 0.000088  training_loss: 1.2197 (1.2553)  classification_loss: 1.2194 (1.2547)  loss_mask: 0.0002 (0.0006)  time: 0.1708  data: 0.0002  max mem: 6052
[05:22:50.739502] Epoch: [61]  [600/781]  eta: 0:00:31  lr: 0.000088  training_loss: 1.2120 (1.2541)  classification_loss: 1.2118 (1.2535)  loss_mask: 0.0002 (0.0006)  time: 0.1702  data: 0.0002  max mem: 6052
[05:22:54.175767] Epoch: [61]  [620/781]  eta: 0:00:27  lr: 0.000088  training_loss: 1.2846 (1.2555)  classification_loss: 1.2844 (1.2549)  loss_mask: 0.0002 (0.0006)  time: 0.1717  data: 0.0003  max mem: 6052
[05:22:57.597696] Epoch: [61]  [640/781]  eta: 0:00:24  lr: 0.000088  training_loss: 1.2557 (1.2558)  classification_loss: 1.2555 (1.2552)  loss_mask: 0.0002 (0.0006)  time: 0.1710  data: 0.0003  max mem: 6052
[05:23:01.033127] Epoch: [61]  [660/781]  eta: 0:00:20  lr: 0.000088  training_loss: 1.2244 (1.2554)  classification_loss: 1.2241 (1.2548)  loss_mask: 0.0002 (0.0006)  time: 0.1717  data: 0.0002  max mem: 6052
[05:23:04.443969] Epoch: [61]  [680/781]  eta: 0:00:17  lr: 0.000088  training_loss: 1.2261 (1.2553)  classification_loss: 1.2257 (1.2547)  loss_mask: 0.0002 (0.0006)  time: 0.1705  data: 0.0002  max mem: 6052
[05:23:07.855116] Epoch: [61]  [700/781]  eta: 0:00:13  lr: 0.000087  training_loss: 1.2573 (1.2552)  classification_loss: 1.2571 (1.2546)  loss_mask: 0.0002 (0.0006)  time: 0.1705  data: 0.0002  max mem: 6052
[05:23:11.262854] Epoch: [61]  [720/781]  eta: 0:00:10  lr: 0.000087  training_loss: 1.3073 (1.2567)  classification_loss: 1.3070 (1.2561)  loss_mask: 0.0002 (0.0006)  time: 0.1703  data: 0.0002  max mem: 6052
[05:23:14.674306] Epoch: [61]  [740/781]  eta: 0:00:07  lr: 0.000087  training_loss: 1.2932 (1.2567)  classification_loss: 1.2930 (1.2562)  loss_mask: 0.0002 (0.0006)  time: 0.1705  data: 0.0002  max mem: 6052
[05:23:18.124796] Epoch: [61]  [760/781]  eta: 0:00:03  lr: 0.000087  training_loss: 1.2852 (1.2565)  classification_loss: 1.2851 (1.2560)  loss_mask: 0.0002 (0.0005)  time: 0.1724  data: 0.0003  max mem: 6052
[05:23:21.525593] Epoch: [61]  [780/781]  eta: 0:00:00  lr: 0.000087  training_loss: 1.2247 (1.2558)  classification_loss: 1.2245 (1.2553)  loss_mask: 0.0002 (0.0005)  time: 0.1700  data: 0.0002  max mem: 6052
[05:23:21.683917] Epoch: [61] Total time: 0:02:14 (0.1726 s / it)
[05:23:21.684455] Averaged stats: lr: 0.000087  training_loss: 1.2247 (1.2558)  classification_loss: 1.2245 (1.2553)  loss_mask: 0.0002 (0.0005)
[05:23:22.340544] Test:  [  0/157]  eta: 0:01:42  testing_loss: 0.4686 (0.4686)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.6517  data: 0.6125  max mem: 6052
[05:23:22.635278] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5572 (0.5618)  acc1: 82.8125 (82.3864)  acc5: 98.4375 (99.1477)  time: 0.0854  data: 0.0562  max mem: 6052
[05:23:22.921191] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.5572 (0.5513)  acc1: 81.2500 (83.0357)  acc5: 100.0000 (99.2560)  time: 0.0286  data: 0.0004  max mem: 6052
[05:23:23.210084] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5751 (0.5689)  acc1: 81.2500 (82.4093)  acc5: 100.0000 (99.0423)  time: 0.0286  data: 0.0001  max mem: 6052
[05:23:23.495416] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5890 (0.5699)  acc1: 81.2500 (82.3552)  acc5: 98.4375 (99.0091)  time: 0.0286  data: 0.0002  max mem: 6052
[05:23:23.782610] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5380 (0.5588)  acc1: 82.8125 (82.7512)  acc5: 98.4375 (98.9890)  time: 0.0285  data: 0.0002  max mem: 6052
[05:23:24.075048] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5211 (0.5527)  acc1: 84.3750 (83.1967)  acc5: 100.0000 (99.0523)  time: 0.0288  data: 0.0002  max mem: 6052
[05:23:24.358805] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5211 (0.5497)  acc1: 84.3750 (83.2526)  acc5: 100.0000 (99.0757)  time: 0.0287  data: 0.0001  max mem: 6052
[05:23:24.640316] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5492 (0.5578)  acc1: 81.2500 (83.0440)  acc5: 98.4375 (99.0355)  time: 0.0282  data: 0.0001  max mem: 6052
[05:23:24.925234] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5494 (0.5557)  acc1: 82.8125 (83.1387)  acc5: 98.4375 (99.0385)  time: 0.0282  data: 0.0001  max mem: 6052
[05:23:25.211298] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5494 (0.5596)  acc1: 81.2500 (82.9053)  acc5: 98.4375 (99.0408)  time: 0.0284  data: 0.0002  max mem: 6052
[05:23:25.492626] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5529 (0.5581)  acc1: 81.2500 (83.0800)  acc5: 100.0000 (99.0428)  time: 0.0283  data: 0.0002  max mem: 6052
[05:23:25.775163] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4996 (0.5575)  acc1: 85.9375 (83.2128)  acc5: 98.4375 (99.0444)  time: 0.0281  data: 0.0002  max mem: 6052
[05:23:26.057417] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5545 (0.5602)  acc1: 84.3750 (83.1226)  acc5: 98.4375 (99.0219)  time: 0.0281  data: 0.0002  max mem: 6052
[05:23:26.340298] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5517 (0.5573)  acc1: 82.8125 (83.1671)  acc5: 98.4375 (99.0248)  time: 0.0281  data: 0.0001  max mem: 6052
[05:23:26.619313] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5517 (0.5579)  acc1: 84.3750 (83.1126)  acc5: 98.4375 (99.0273)  time: 0.0280  data: 0.0001  max mem: 6052
[05:23:26.769460] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5589 (0.5592)  acc1: 82.8125 (83.0400)  acc5: 98.4375 (99.0200)  time: 0.0269  data: 0.0001  max mem: 6052
[05:23:26.925825] Test: Total time: 0:00:05 (0.0334 s / it)
[05:23:26.927043] * Acc@1 83.040 Acc@5 99.020 loss 0.559
[05:23:26.927719] Accuracy of the network on the 10000 test images: 83.0%
[05:23:26.928256] Max accuracy: 83.33%
[05:23:27.104345] log_dir: ./output_dir
[05:23:27.968849] Epoch: [62]  [  0/781]  eta: 0:11:13  lr: 0.000087  training_loss: 1.2074 (1.2074)  classification_loss: 1.2072 (1.2072)  loss_mask: 0.0002 (0.0002)  time: 0.8627  data: 0.6857  max mem: 6052
[05:23:31.393853] Epoch: [62]  [ 20/781]  eta: 0:02:35  lr: 0.000087  training_loss: 1.1698 (1.2085)  classification_loss: 1.1697 (1.2083)  loss_mask: 0.0002 (0.0002)  time: 0.1711  data: 0.0002  max mem: 6052
[05:23:34.812706] Epoch: [62]  [ 40/781]  eta: 0:02:19  lr: 0.000087  training_loss: 1.2455 (1.2309)  classification_loss: 1.2452 (1.2307)  loss_mask: 0.0002 (0.0002)  time: 0.1709  data: 0.0003  max mem: 6052
[05:23:38.237680] Epoch: [62]  [ 60/781]  eta: 0:02:11  lr: 0.000087  training_loss: 1.2151 (1.2328)  classification_loss: 1.2149 (1.2326)  loss_mask: 0.0002 (0.0002)  time: 0.1712  data: 0.0002  max mem: 6052
[05:23:41.663854] Epoch: [62]  [ 80/781]  eta: 0:02:05  lr: 0.000087  training_loss: 1.2283 (1.2331)  classification_loss: 1.2281 (1.2328)  loss_mask: 0.0002 (0.0002)  time: 0.1712  data: 0.0002  max mem: 6052
[05:23:45.080120] Epoch: [62]  [100/781]  eta: 0:02:01  lr: 0.000087  training_loss: 1.2871 (1.2437)  classification_loss: 1.2484 (1.2372)  loss_mask: 0.0022 (0.0066)  time: 0.1707  data: 0.0002  max mem: 6052
[05:23:48.472311] Epoch: [62]  [120/781]  eta: 0:01:56  lr: 0.000086  training_loss: 1.3371 (1.2576)  classification_loss: 1.2636 (1.2442)  loss_mask: 0.0371 (0.0134)  time: 0.1695  data: 0.0002  max mem: 6052
[05:23:51.880566] Epoch: [62]  [140/781]  eta: 0:01:52  lr: 0.000086  training_loss: 1.3610 (1.2763)  classification_loss: 1.2202 (1.2428)  loss_mask: 0.0684 (0.0334)  time: 0.1703  data: 0.0002  max mem: 6052
[05:23:55.283605] Epoch: [62]  [160/781]  eta: 0:01:48  lr: 0.000086  training_loss: 1.4243 (1.2964)  classification_loss: 1.2688 (1.2452)  loss_mask: 0.0847 (0.0511)  time: 0.1701  data: 0.0001  max mem: 6052
[05:23:58.696703] Epoch: [62]  [180/781]  eta: 0:01:44  lr: 0.000086  training_loss: 1.2902 (1.2979)  classification_loss: 1.2370 (1.2468)  loss_mask: 0.0271 (0.0511)  time: 0.1706  data: 0.0004  max mem: 6052
[05:24:02.082073] Epoch: [62]  [200/781]  eta: 0:01:41  lr: 0.000086  training_loss: 1.2820 (1.2988)  classification_loss: 1.2662 (1.2509)  loss_mask: 0.0125 (0.0479)  time: 0.1692  data: 0.0002  max mem: 6052
[05:24:05.485154] Epoch: [62]  [220/781]  eta: 0:01:37  lr: 0.000086  training_loss: 1.2512 (1.2930)  classification_loss: 1.2424 (1.2486)  loss_mask: 0.0079 (0.0444)  time: 0.1701  data: 0.0002  max mem: 6052
[05:24:08.897857] Epoch: [62]  [240/781]  eta: 0:01:33  lr: 0.000086  training_loss: 1.2489 (1.2905)  classification_loss: 1.2439 (1.2495)  loss_mask: 0.0040 (0.0410)  time: 0.1706  data: 0.0002  max mem: 6052
[05:24:12.321328] Epoch: [62]  [260/781]  eta: 0:01:30  lr: 0.000086  training_loss: 1.2431 (1.2886)  classification_loss: 1.2420 (1.2505)  loss_mask: 0.0028 (0.0381)  time: 0.1711  data: 0.0002  max mem: 6052
[05:24:15.739206] Epoch: [62]  [280/781]  eta: 0:01:26  lr: 0.000086  training_loss: 1.2577 (1.2856)  classification_loss: 1.2540 (1.2500)  loss_mask: 0.0023 (0.0356)  time: 0.1708  data: 0.0003  max mem: 6052
[05:24:19.162799] Epoch: [62]  [300/781]  eta: 0:01:23  lr: 0.000086  training_loss: 1.2790 (1.2860)  classification_loss: 1.2752 (1.2526)  loss_mask: 0.0024 (0.0334)  time: 0.1711  data: 0.0003  max mem: 6052
[05:24:22.582309] Epoch: [62]  [320/781]  eta: 0:01:19  lr: 0.000085  training_loss: 1.2369 (1.2839)  classification_loss: 1.2347 (1.2525)  loss_mask: 0.0018 (0.0314)  time: 0.1709  data: 0.0003  max mem: 6052
[05:24:26.005003] Epoch: [62]  [340/781]  eta: 0:01:16  lr: 0.000085  training_loss: 1.2343 (1.2807)  classification_loss: 1.2333 (1.2510)  loss_mask: 0.0016 (0.0297)  time: 0.1710  data: 0.0003  max mem: 6052
[05:24:29.422692] Epoch: [62]  [360/781]  eta: 0:01:12  lr: 0.000085  training_loss: 1.2230 (1.2787)  classification_loss: 1.2221 (1.2506)  loss_mask: 0.0016 (0.0281)  time: 0.1708  data: 0.0002  max mem: 6052
[05:24:32.843191] Epoch: [62]  [380/781]  eta: 0:01:09  lr: 0.000085  training_loss: 1.2891 (1.2787)  classification_loss: 1.2877 (1.2520)  loss_mask: 0.0014 (0.0267)  time: 0.1709  data: 0.0002  max mem: 6052
[05:24:36.271451] Epoch: [62]  [400/781]  eta: 0:01:05  lr: 0.000085  training_loss: 1.2558 (1.2796)  classification_loss: 1.2543 (1.2541)  loss_mask: 0.0013 (0.0255)  time: 0.1713  data: 0.0002  max mem: 6052
[05:24:39.684330] Epoch: [62]  [420/781]  eta: 0:01:02  lr: 0.000085  training_loss: 1.2189 (1.2775)  classification_loss: 1.2181 (1.2532)  loss_mask: 0.0011 (0.0243)  time: 0.1706  data: 0.0002  max mem: 6052
[05:24:43.112563] Epoch: [62]  [440/781]  eta: 0:00:58  lr: 0.000085  training_loss: 1.2112 (1.2758)  classification_loss: 1.2101 (1.2525)  loss_mask: 0.0011 (0.0233)  time: 0.1713  data: 0.0002  max mem: 6052
[05:24:46.550000] Epoch: [62]  [460/781]  eta: 0:00:55  lr: 0.000085  training_loss: 1.1774 (1.2726)  classification_loss: 1.1759 (1.2503)  loss_mask: 0.0009 (0.0223)  time: 0.1718  data: 0.0002  max mem: 6052
[05:24:49.970638] Epoch: [62]  [480/781]  eta: 0:00:51  lr: 0.000085  training_loss: 1.2731 (1.2728)  classification_loss: 1.2720 (1.2514)  loss_mask: 0.0009 (0.0214)  time: 0.1710  data: 0.0004  max mem: 6052
[05:24:53.393541] Epoch: [62]  [500/781]  eta: 0:00:48  lr: 0.000085  training_loss: 1.2406 (1.2714)  classification_loss: 1.2403 (1.2508)  loss_mask: 0.0007 (0.0206)  time: 0.1711  data: 0.0002  max mem: 6052
[05:24:56.809317] Epoch: [62]  [520/781]  eta: 0:00:44  lr: 0.000084  training_loss: 1.2067 (1.2695)  classification_loss: 1.2057 (1.2497)  loss_mask: 0.0008 (0.0198)  time: 0.1707  data: 0.0002  max mem: 6052
[05:25:00.232275] Epoch: [62]  [540/781]  eta: 0:00:41  lr: 0.000084  training_loss: 1.2356 (1.2683)  classification_loss: 1.2349 (1.2491)  loss_mask: 0.0007 (0.0191)  time: 0.1711  data: 0.0002  max mem: 6052
[05:25:03.662257] Epoch: [62]  [560/781]  eta: 0:00:38  lr: 0.000084  training_loss: 1.2746 (1.2685)  classification_loss: 1.2736 (1.2501)  loss_mask: 0.0008 (0.0185)  time: 0.1714  data: 0.0002  max mem: 6052
[05:25:07.094677] Epoch: [62]  [580/781]  eta: 0:00:34  lr: 0.000084  training_loss: 1.2655 (1.2687)  classification_loss: 1.2641 (1.2508)  loss_mask: 0.0007 (0.0179)  time: 0.1715  data: 0.0002  max mem: 6052
[05:25:10.533547] Epoch: [62]  [600/781]  eta: 0:00:31  lr: 0.000084  training_loss: 1.2310 (1.2677)  classification_loss: 1.2306 (1.2504)  loss_mask: 0.0007 (0.0173)  time: 0.1719  data: 0.0002  max mem: 6052
[05:25:13.975896] Epoch: [62]  [620/781]  eta: 0:00:27  lr: 0.000084  training_loss: 1.2328 (1.2671)  classification_loss: 1.2322 (1.2503)  loss_mask: 0.0007 (0.0168)  time: 0.1720  data: 0.0002  max mem: 6052
[05:25:17.404685] Epoch: [62]  [640/781]  eta: 0:00:24  lr: 0.000084  training_loss: 1.2231 (1.2665)  classification_loss: 1.2222 (1.2502)  loss_mask: 0.0005 (0.0163)  time: 0.1714  data: 0.0003  max mem: 6052
[05:25:20.825630] Epoch: [62]  [660/781]  eta: 0:00:20  lr: 0.000084  training_loss: 1.2363 (1.2664)  classification_loss: 1.2357 (1.2506)  loss_mask: 0.0006 (0.0158)  time: 0.1710  data: 0.0003  max mem: 6052
[05:25:24.276950] Epoch: [62]  [680/781]  eta: 0:00:17  lr: 0.000084  training_loss: 1.2328 (1.2660)  classification_loss: 1.2319 (1.2507)  loss_mask: 0.0005 (0.0153)  time: 0.1725  data: 0.0002  max mem: 6052
[05:25:27.692358] Epoch: [62]  [700/781]  eta: 0:00:13  lr: 0.000084  training_loss: 1.2504 (1.2661)  classification_loss: 1.2468 (1.2512)  loss_mask: 0.0007 (0.0149)  time: 0.1707  data: 0.0003  max mem: 6052
[05:25:31.109616] Epoch: [62]  [720/781]  eta: 0:00:10  lr: 0.000083  training_loss: 1.2546 (1.2661)  classification_loss: 1.2537 (1.2516)  loss_mask: 0.0006 (0.0145)  time: 0.1708  data: 0.0003  max mem: 6052
[05:25:34.515337] Epoch: [62]  [740/781]  eta: 0:00:07  lr: 0.000083  training_loss: 1.2537 (1.2663)  classification_loss: 1.2528 (1.2522)  loss_mask: 0.0005 (0.0141)  time: 0.1702  data: 0.0003  max mem: 6052
[05:25:37.930404] Epoch: [62]  [760/781]  eta: 0:00:03  lr: 0.000083  training_loss: 1.2550 (1.2660)  classification_loss: 1.2546 (1.2522)  loss_mask: 0.0005 (0.0138)  time: 0.1707  data: 0.0003  max mem: 6052
[05:25:41.323782] Epoch: [62]  [780/781]  eta: 0:00:00  lr: 0.000083  training_loss: 1.2440 (1.2657)  classification_loss: 1.2434 (1.2523)  loss_mask: 0.0006 (0.0134)  time: 0.1696  data: 0.0001  max mem: 6052
[05:25:41.476391] Epoch: [62] Total time: 0:02:14 (0.1721 s / it)
[05:25:41.476892] Averaged stats: lr: 0.000083  training_loss: 1.2440 (1.2657)  classification_loss: 1.2434 (1.2523)  loss_mask: 0.0006 (0.0134)
[05:25:42.140542] Test:  [  0/157]  eta: 0:01:43  testing_loss: 0.4256 (0.4256)  acc1: 89.0625 (89.0625)  acc5: 96.8750 (96.8750)  time: 0.6597  data: 0.6283  max mem: 6052
[05:25:42.428155] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5842 (0.5802)  acc1: 82.8125 (81.8182)  acc5: 100.0000 (99.5739)  time: 0.0860  data: 0.0573  max mem: 6052
[05:25:42.714553] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5442 (0.5506)  acc1: 82.8125 (83.0357)  acc5: 100.0000 (99.3304)  time: 0.0285  data: 0.0002  max mem: 6052
[05:25:43.001098] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5441 (0.5612)  acc1: 82.8125 (82.5101)  acc5: 100.0000 (99.2440)  time: 0.0284  data: 0.0001  max mem: 6052
[05:25:43.291028] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5594 (0.5636)  acc1: 82.8125 (82.5457)  acc5: 98.4375 (99.1235)  time: 0.0286  data: 0.0001  max mem: 6052
[05:25:43.577240] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5184 (0.5530)  acc1: 84.3750 (83.2108)  acc5: 98.4375 (99.1728)  time: 0.0286  data: 0.0002  max mem: 6052
[05:25:43.861568] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5184 (0.5506)  acc1: 85.9375 (83.3760)  acc5: 100.0000 (99.1547)  time: 0.0284  data: 0.0003  max mem: 6052
[05:25:44.151062] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5289 (0.5471)  acc1: 84.3750 (83.5827)  acc5: 100.0000 (99.1857)  time: 0.0286  data: 0.0002  max mem: 6052
[05:25:44.432655] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5430 (0.5547)  acc1: 82.8125 (83.3140)  acc5: 100.0000 (99.1898)  time: 0.0284  data: 0.0001  max mem: 6052
[05:25:44.720703] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5370 (0.5549)  acc1: 82.8125 (83.2246)  acc5: 100.0000 (99.2273)  time: 0.0284  data: 0.0001  max mem: 6052
[05:25:45.009127] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5467 (0.5599)  acc1: 81.2500 (82.9517)  acc5: 100.0000 (99.2110)  time: 0.0287  data: 0.0002  max mem: 6052
[05:25:45.291898] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5509 (0.5614)  acc1: 81.2500 (82.9533)  acc5: 100.0000 (99.1976)  time: 0.0284  data: 0.0002  max mem: 6052
[05:25:45.585184] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5503 (0.5596)  acc1: 82.8125 (82.9804)  acc5: 100.0000 (99.1865)  time: 0.0286  data: 0.0002  max mem: 6052
[05:25:45.873343] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5723 (0.5621)  acc1: 81.2500 (82.9318)  acc5: 100.0000 (99.1770)  time: 0.0289  data: 0.0001  max mem: 6052
[05:25:46.156096] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5338 (0.5594)  acc1: 82.8125 (83.0230)  acc5: 100.0000 (99.1689)  time: 0.0284  data: 0.0001  max mem: 6052
[05:25:46.434728] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5342 (0.5578)  acc1: 82.8125 (83.0195)  acc5: 100.0000 (99.1825)  time: 0.0280  data: 0.0001  max mem: 6052
[05:25:46.583704] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5172 (0.5566)  acc1: 82.8125 (83.0400)  acc5: 100.0000 (99.1900)  time: 0.0268  data: 0.0001  max mem: 6052
[05:25:46.741348] Test: Total time: 0:00:05 (0.0335 s / it)
[05:25:46.741780] * Acc@1 83.040 Acc@5 99.190 loss 0.557
[05:25:46.742065] Accuracy of the network on the 10000 test images: 83.0%
[05:25:46.742262] Max accuracy: 83.33%
[05:25:46.985544] log_dir: ./output_dir
[05:25:47.829024] Epoch: [63]  [  0/781]  eta: 0:10:57  lr: 0.000083  training_loss: 1.1809 (1.1809)  classification_loss: 1.1807 (1.1807)  loss_mask: 0.0003 (0.0003)  time: 0.8417  data: 0.6631  max mem: 6052
[05:25:51.332237] Epoch: [63]  [ 20/781]  eta: 0:02:37  lr: 0.000083  training_loss: 1.2111 (1.2163)  classification_loss: 1.2108 (1.2158)  loss_mask: 0.0004 (0.0005)  time: 0.1750  data: 0.0002  max mem: 6052
[05:25:54.784634] Epoch: [63]  [ 40/781]  eta: 0:02:20  lr: 0.000083  training_loss: 1.2548 (1.2444)  classification_loss: 1.2541 (1.2438)  loss_mask: 0.0005 (0.0005)  time: 0.1725  data: 0.0002  max mem: 6052
[05:25:58.212726] Epoch: [63]  [ 60/781]  eta: 0:02:12  lr: 0.000083  training_loss: 1.3029 (1.2548)  classification_loss: 1.3026 (1.2543)  loss_mask: 0.0005 (0.0005)  time: 0.1713  data: 0.0004  max mem: 6052
[05:26:01.615169] Epoch: [63]  [ 80/781]  eta: 0:02:06  lr: 0.000083  training_loss: 1.2533 (1.2558)  classification_loss: 1.2526 (1.2553)  loss_mask: 0.0005 (0.0005)  time: 0.1700  data: 0.0002  max mem: 6052
[05:26:05.101916] Epoch: [63]  [100/781]  eta: 0:02:02  lr: 0.000083  training_loss: 1.3167 (1.2620)  classification_loss: 1.3161 (1.2614)  loss_mask: 0.0005 (0.0005)  time: 0.1743  data: 0.0002  max mem: 6052
[05:26:08.524752] Epoch: [63]  [120/781]  eta: 0:01:57  lr: 0.000083  training_loss: 1.2095 (1.2587)  classification_loss: 1.2091 (1.2582)  loss_mask: 0.0004 (0.0005)  time: 0.1711  data: 0.0002  max mem: 6052
[05:26:11.953801] Epoch: [63]  [140/781]  eta: 0:01:53  lr: 0.000082  training_loss: 1.2384 (1.2577)  classification_loss: 1.2382 (1.2572)  loss_mask: 0.0004 (0.0005)  time: 0.1713  data: 0.0002  max mem: 6052
[05:26:15.437576] Epoch: [63]  [160/781]  eta: 0:01:49  lr: 0.000082  training_loss: 1.1951 (1.2541)  classification_loss: 1.1946 (1.2536)  loss_mask: 0.0003 (0.0005)  time: 0.1741  data: 0.0002  max mem: 6052
[05:26:18.869936] Epoch: [63]  [180/781]  eta: 0:01:45  lr: 0.000082  training_loss: 1.2042 (1.2545)  classification_loss: 1.2037 (1.2541)  loss_mask: 0.0003 (0.0005)  time: 0.1715  data: 0.0002  max mem: 6052
[05:26:22.295023] Epoch: [63]  [200/781]  eta: 0:01:42  lr: 0.000082  training_loss: 1.2151 (1.2514)  classification_loss: 1.2146 (1.2510)  loss_mask: 0.0003 (0.0005)  time: 0.1712  data: 0.0003  max mem: 6052
[05:26:25.736748] Epoch: [63]  [220/781]  eta: 0:01:38  lr: 0.000082  training_loss: 1.2000 (1.2503)  classification_loss: 1.1998 (1.2499)  loss_mask: 0.0003 (0.0005)  time: 0.1720  data: 0.0003  max mem: 6052
[05:26:29.173192] Epoch: [63]  [240/781]  eta: 0:01:34  lr: 0.000082  training_loss: 1.2318 (1.2496)  classification_loss: 1.2315 (1.2492)  loss_mask: 0.0003 (0.0004)  time: 0.1717  data: 0.0003  max mem: 6052
[05:26:32.585732] Epoch: [63]  [260/781]  eta: 0:01:30  lr: 0.000082  training_loss: 1.2355 (1.2481)  classification_loss: 1.2352 (1.2477)  loss_mask: 0.0003 (0.0004)  time: 0.1706  data: 0.0002  max mem: 6052
[05:26:36.008831] Epoch: [63]  [280/781]  eta: 0:01:27  lr: 0.000082  training_loss: 1.2468 (1.2484)  classification_loss: 1.2464 (1.2480)  loss_mask: 0.0003 (0.0004)  time: 0.1711  data: 0.0003  max mem: 6052
[05:26:39.418759] Epoch: [63]  [300/781]  eta: 0:01:23  lr: 0.000082  training_loss: 1.2404 (1.2497)  classification_loss: 1.2403 (1.2493)  loss_mask: 0.0003 (0.0004)  time: 0.1704  data: 0.0002  max mem: 6052
[05:26:42.854163] Epoch: [63]  [320/781]  eta: 0:01:20  lr: 0.000082  training_loss: 1.2221 (1.2484)  classification_loss: 1.2220 (1.2480)  loss_mask: 0.0003 (0.0004)  time: 0.1717  data: 0.0002  max mem: 6052
[05:26:46.287370] Epoch: [63]  [340/781]  eta: 0:01:16  lr: 0.000081  training_loss: 1.2109 (1.2469)  classification_loss: 1.2107 (1.2465)  loss_mask: 0.0004 (0.0004)  time: 0.1716  data: 0.0003  max mem: 6052
[05:26:49.701836] Epoch: [63]  [360/781]  eta: 0:01:13  lr: 0.000081  training_loss: 1.2656 (1.2470)  classification_loss: 1.2653 (1.2466)  loss_mask: 0.0003 (0.0004)  time: 0.1706  data: 0.0002  max mem: 6052
[05:26:53.134218] Epoch: [63]  [380/781]  eta: 0:01:09  lr: 0.000081  training_loss: 1.1902 (1.2456)  classification_loss: 1.1895 (1.2452)  loss_mask: 0.0003 (0.0004)  time: 0.1715  data: 0.0002  max mem: 6052
[05:26:56.548268] Epoch: [63]  [400/781]  eta: 0:01:06  lr: 0.000081  training_loss: 1.2460 (1.2464)  classification_loss: 1.2452 (1.2460)  loss_mask: 0.0003 (0.0004)  time: 0.1706  data: 0.0002  max mem: 6052
[05:26:59.969655] Epoch: [63]  [420/781]  eta: 0:01:02  lr: 0.000081  training_loss: 1.2187 (1.2453)  classification_loss: 1.2184 (1.2449)  loss_mask: 0.0003 (0.0004)  time: 0.1710  data: 0.0002  max mem: 6052
[05:27:03.383192] Epoch: [63]  [440/781]  eta: 0:00:59  lr: 0.000081  training_loss: 1.2263 (1.2454)  classification_loss: 1.2258 (1.2450)  loss_mask: 0.0003 (0.0004)  time: 0.1706  data: 0.0002  max mem: 6052
[05:27:06.808808] Epoch: [63]  [460/781]  eta: 0:00:55  lr: 0.000081  training_loss: 1.2529 (1.2461)  classification_loss: 1.2527 (1.2457)  loss_mask: 0.0002 (0.0004)  time: 0.1712  data: 0.0002  max mem: 6052
[05:27:10.236794] Epoch: [63]  [480/781]  eta: 0:00:52  lr: 0.000081  training_loss: 1.2743 (1.2473)  classification_loss: 1.2741 (1.2469)  loss_mask: 0.0003 (0.0004)  time: 0.1713  data: 0.0002  max mem: 6052
[05:27:13.689368] Epoch: [63]  [500/781]  eta: 0:00:48  lr: 0.000081  training_loss: 1.2627 (1.2482)  classification_loss: 1.2623 (1.2478)  loss_mask: 0.0003 (0.0004)  time: 0.1726  data: 0.0002  max mem: 6052
[05:27:17.103767] Epoch: [63]  [520/781]  eta: 0:00:45  lr: 0.000081  training_loss: 1.2478 (1.2479)  classification_loss: 1.2475 (1.2475)  loss_mask: 0.0003 (0.0004)  time: 0.1706  data: 0.0002  max mem: 6052
[05:27:20.532227] Epoch: [63]  [540/781]  eta: 0:00:41  lr: 0.000080  training_loss: 1.2348 (1.2489)  classification_loss: 1.2345 (1.2486)  loss_mask: 0.0003 (0.0004)  time: 0.1713  data: 0.0002  max mem: 6052
[05:27:23.930868] Epoch: [63]  [560/781]  eta: 0:00:38  lr: 0.000080  training_loss: 1.2717 (1.2496)  classification_loss: 1.2716 (1.2492)  loss_mask: 0.0003 (0.0004)  time: 0.1698  data: 0.0002  max mem: 6052
[05:27:27.340746] Epoch: [63]  [580/781]  eta: 0:00:34  lr: 0.000080  training_loss: 1.2644 (1.2505)  classification_loss: 1.2642 (1.2501)  loss_mask: 0.0003 (0.0004)  time: 0.1704  data: 0.0002  max mem: 6052
[05:27:30.751446] Epoch: [63]  [600/781]  eta: 0:00:31  lr: 0.000080  training_loss: 1.2577 (1.2511)  classification_loss: 1.2573 (1.2507)  loss_mask: 0.0003 (0.0004)  time: 0.1705  data: 0.0002  max mem: 6052
[05:27:34.167778] Epoch: [63]  [620/781]  eta: 0:00:27  lr: 0.000080  training_loss: 1.2280 (1.2511)  classification_loss: 1.2278 (1.2508)  loss_mask: 0.0002 (0.0004)  time: 0.1707  data: 0.0001  max mem: 6052
[05:27:37.574553] Epoch: [63]  [640/781]  eta: 0:00:24  lr: 0.000080  training_loss: 1.2348 (1.2508)  classification_loss: 1.2345 (1.2504)  loss_mask: 0.0002 (0.0004)  time: 0.1703  data: 0.0003  max mem: 6052
[05:27:40.993870] Epoch: [63]  [660/781]  eta: 0:00:20  lr: 0.000080  training_loss: 1.1796 (1.2497)  classification_loss: 1.1795 (1.2494)  loss_mask: 0.0003 (0.0004)  time: 0.1709  data: 0.0003  max mem: 6052
[05:27:44.422367] Epoch: [63]  [680/781]  eta: 0:00:17  lr: 0.000080  training_loss: 1.2268 (1.2499)  classification_loss: 1.2265 (1.2495)  loss_mask: 0.0002 (0.0004)  time: 0.1713  data: 0.0003  max mem: 6052
[05:27:47.844626] Epoch: [63]  [700/781]  eta: 0:00:13  lr: 0.000080  training_loss: 1.2611 (1.2504)  classification_loss: 1.2608 (1.2501)  loss_mask: 0.0003 (0.0004)  time: 0.1710  data: 0.0002  max mem: 6052
[05:27:51.251978] Epoch: [63]  [720/781]  eta: 0:00:10  lr: 0.000080  training_loss: 1.2664 (1.2509)  classification_loss: 1.2662 (1.2506)  loss_mask: 0.0002 (0.0004)  time: 0.1703  data: 0.0002  max mem: 6052
[05:27:54.680761] Epoch: [63]  [740/781]  eta: 0:00:07  lr: 0.000079  training_loss: 1.1700 (1.2497)  classification_loss: 1.1700 (1.2493)  loss_mask: 0.0003 (0.0004)  time: 0.1714  data: 0.0002  max mem: 6052
[05:27:58.100292] Epoch: [63]  [760/781]  eta: 0:00:03  lr: 0.000079  training_loss: 1.2294 (1.2499)  classification_loss: 1.2291 (1.2496)  loss_mask: 0.0002 (0.0003)  time: 0.1709  data: 0.0002  max mem: 6052
[05:28:01.495025] Epoch: [63]  [780/781]  eta: 0:00:00  lr: 0.000079  training_loss: 1.2399 (1.2499)  classification_loss: 1.2395 (1.2495)  loss_mask: 0.0002 (0.0003)  time: 0.1697  data: 0.0001  max mem: 6052
[05:28:01.663835] Epoch: [63] Total time: 0:02:14 (0.1724 s / it)
[05:28:01.665388] Averaged stats: lr: 0.000079  training_loss: 1.2399 (1.2499)  classification_loss: 1.2395 (1.2495)  loss_mask: 0.0002 (0.0003)
[05:28:02.339530] Test:  [  0/157]  eta: 0:01:45  testing_loss: 0.4085 (0.4085)  acc1: 90.6250 (90.6250)  acc5: 98.4375 (98.4375)  time: 0.6702  data: 0.6401  max mem: 6052
[05:28:02.626845] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5594 (0.5605)  acc1: 82.8125 (82.6705)  acc5: 100.0000 (99.1477)  time: 0.0869  data: 0.0583  max mem: 6052
[05:28:02.908702] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5149 (0.5226)  acc1: 84.3750 (84.5238)  acc5: 100.0000 (99.4048)  time: 0.0283  data: 0.0001  max mem: 6052
[05:28:03.193270] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5052 (0.5316)  acc1: 84.3750 (84.0222)  acc5: 100.0000 (99.2440)  time: 0.0282  data: 0.0001  max mem: 6052
[05:28:03.477653] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5375 (0.5360)  acc1: 84.3750 (84.0320)  acc5: 100.0000 (99.1997)  time: 0.0283  data: 0.0002  max mem: 6052
[05:28:03.761456] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5045 (0.5279)  acc1: 84.3750 (84.4363)  acc5: 100.0000 (99.2647)  time: 0.0283  data: 0.0002  max mem: 6052
[05:28:04.044687] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4845 (0.5262)  acc1: 84.3750 (84.3494)  acc5: 100.0000 (99.2572)  time: 0.0282  data: 0.0002  max mem: 6052
[05:28:04.330457] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4824 (0.5229)  acc1: 85.9375 (84.5511)  acc5: 100.0000 (99.2518)  time: 0.0283  data: 0.0002  max mem: 6052
[05:28:04.616844] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5465 (0.5324)  acc1: 84.3750 (84.2207)  acc5: 98.4375 (99.2091)  time: 0.0285  data: 0.0001  max mem: 6052
[05:28:04.902992] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5670 (0.5316)  acc1: 82.8125 (84.3407)  acc5: 100.0000 (99.2102)  time: 0.0285  data: 0.0002  max mem: 6052
[05:28:05.187095] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5035 (0.5364)  acc1: 82.8125 (84.0347)  acc5: 100.0000 (99.2110)  time: 0.0284  data: 0.0002  max mem: 6052
[05:28:05.471426] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5234 (0.5375)  acc1: 82.8125 (84.0653)  acc5: 100.0000 (99.2539)  time: 0.0283  data: 0.0002  max mem: 6052
[05:28:05.757163] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5185 (0.5377)  acc1: 82.8125 (83.8585)  acc5: 100.0000 (99.2510)  time: 0.0283  data: 0.0002  max mem: 6052
[05:28:06.040553] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5408 (0.5387)  acc1: 81.2500 (83.7428)  acc5: 100.0000 (99.2486)  time: 0.0283  data: 0.0002  max mem: 6052
[05:28:06.322783] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5089 (0.5347)  acc1: 82.8125 (83.8320)  acc5: 100.0000 (99.2575)  time: 0.0281  data: 0.0002  max mem: 6052
[05:28:06.601636] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5020 (0.5341)  acc1: 84.3750 (83.7541)  acc5: 98.4375 (99.2343)  time: 0.0278  data: 0.0001  max mem: 6052
[05:28:06.751744] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5020 (0.5334)  acc1: 84.3750 (83.7600)  acc5: 100.0000 (99.2500)  time: 0.0269  data: 0.0001  max mem: 6052
[05:28:06.915564] Test: Total time: 0:00:05 (0.0334 s / it)
[05:28:06.916261] * Acc@1 83.760 Acc@5 99.250 loss 0.533
[05:28:06.916574] Accuracy of the network on the 10000 test images: 83.8%
[05:28:06.916808] Max accuracy: 83.76%
[05:28:07.270177] log_dir: ./output_dir
[05:28:08.129161] Epoch: [64]  [  0/781]  eta: 0:11:09  lr: 0.000079  training_loss: 1.0224 (1.0224)  classification_loss: 1.0221 (1.0221)  loss_mask: 0.0003 (0.0003)  time: 0.8574  data: 0.6482  max mem: 6052
[05:28:11.532217] Epoch: [64]  [ 20/781]  eta: 0:02:34  lr: 0.000079  training_loss: 1.2362 (1.2107)  classification_loss: 1.2360 (1.2105)  loss_mask: 0.0002 (0.0002)  time: 0.1701  data: 0.0001  max mem: 6052
[05:28:14.946761] Epoch: [64]  [ 40/781]  eta: 0:02:18  lr: 0.000079  training_loss: 1.1981 (1.2166)  classification_loss: 1.1980 (1.2164)  loss_mask: 0.0002 (0.0002)  time: 0.1707  data: 0.0002  max mem: 6052
[05:28:18.354091] Epoch: [64]  [ 60/781]  eta: 0:02:10  lr: 0.000079  training_loss: 1.2328 (1.2288)  classification_loss: 1.2326 (1.2286)  loss_mask: 0.0002 (0.0002)  time: 0.1703  data: 0.0001  max mem: 6052
[05:28:21.753625] Epoch: [64]  [ 80/781]  eta: 0:02:05  lr: 0.000079  training_loss: 1.2261 (1.2255)  classification_loss: 1.2259 (1.2253)  loss_mask: 0.0002 (0.0002)  time: 0.1699  data: 0.0001  max mem: 6052
[05:28:25.204521] Epoch: [64]  [100/781]  eta: 0:02:00  lr: 0.000079  training_loss: 1.2324 (1.2290)  classification_loss: 1.2322 (1.2288)  loss_mask: 0.0002 (0.0002)  time: 0.1725  data: 0.0002  max mem: 6052
[05:28:28.691390] Epoch: [64]  [120/781]  eta: 0:01:56  lr: 0.000079  training_loss: 1.1937 (1.2263)  classification_loss: 1.1936 (1.2261)  loss_mask: 0.0002 (0.0002)  time: 0.1743  data: 0.0003  max mem: 6052
[05:28:32.172702] Epoch: [64]  [140/781]  eta: 0:01:53  lr: 0.000079  training_loss: 1.2800 (1.2300)  classification_loss: 1.2799 (1.2298)  loss_mask: 0.0002 (0.0002)  time: 0.1740  data: 0.0002  max mem: 6052
[05:28:35.615877] Epoch: [64]  [160/781]  eta: 0:01:49  lr: 0.000079  training_loss: 1.2201 (1.2316)  classification_loss: 1.2200 (1.2314)  loss_mask: 0.0002 (0.0002)  time: 0.1721  data: 0.0002  max mem: 6052
[05:28:39.016982] Epoch: [64]  [180/781]  eta: 0:01:45  lr: 0.000078  training_loss: 1.1969 (1.2310)  classification_loss: 1.1967 (1.2308)  loss_mask: 0.0002 (0.0002)  time: 0.1700  data: 0.0003  max mem: 6052
[05:28:42.441916] Epoch: [64]  [200/781]  eta: 0:01:41  lr: 0.000078  training_loss: 1.2578 (1.2321)  classification_loss: 1.2576 (1.2319)  loss_mask: 0.0002 (0.0002)  time: 0.1712  data: 0.0002  max mem: 6052
[05:28:45.862503] Epoch: [64]  [220/781]  eta: 0:01:37  lr: 0.000078  training_loss: 1.2348 (1.2323)  classification_loss: 1.2347 (1.2321)  loss_mask: 0.0002 (0.0002)  time: 0.1709  data: 0.0002  max mem: 6052
[05:28:49.283649] Epoch: [64]  [240/781]  eta: 0:01:34  lr: 0.000078  training_loss: 1.2226 (1.2338)  classification_loss: 1.2224 (1.2336)  loss_mask: 0.0001 (0.0002)  time: 0.1710  data: 0.0004  max mem: 6052
[05:28:52.711542] Epoch: [64]  [260/781]  eta: 0:01:30  lr: 0.000078  training_loss: 1.2464 (1.2339)  classification_loss: 1.2463 (1.2337)  loss_mask: 0.0002 (0.0002)  time: 0.1713  data: 0.0003  max mem: 6052
[05:28:56.181863] Epoch: [64]  [280/781]  eta: 0:01:27  lr: 0.000078  training_loss: 1.1899 (1.2311)  classification_loss: 1.1898 (1.2309)  loss_mask: 0.0002 (0.0002)  time: 0.1734  data: 0.0003  max mem: 6052
[05:28:59.627871] Epoch: [64]  [300/781]  eta: 0:01:23  lr: 0.000078  training_loss: 1.2488 (1.2339)  classification_loss: 1.2487 (1.2337)  loss_mask: 0.0002 (0.0002)  time: 0.1722  data: 0.0002  max mem: 6052
[05:29:03.044182] Epoch: [64]  [320/781]  eta: 0:01:20  lr: 0.000078  training_loss: 1.2434 (1.2344)  classification_loss: 1.2432 (1.2337)  loss_mask: 0.0002 (0.0007)  time: 0.1707  data: 0.0002  max mem: 6052
[05:29:06.470565] Epoch: [64]  [340/781]  eta: 0:01:16  lr: 0.000078  training_loss: 1.2012 (1.2356)  classification_loss: 1.1724 (1.2309)  loss_mask: 0.0288 (0.0047)  time: 0.1712  data: 0.0004  max mem: 6052
[05:29:09.893748] Epoch: [64]  [360/781]  eta: 0:01:12  lr: 0.000078  training_loss: 1.2839 (1.2389)  classification_loss: 1.2415 (1.2324)  loss_mask: 0.0218 (0.0065)  time: 0.1711  data: 0.0001  max mem: 6052
[05:29:13.328741] Epoch: [64]  [380/781]  eta: 0:01:09  lr: 0.000077  training_loss: 1.3059 (1.2443)  classification_loss: 1.2850 (1.2351)  loss_mask: 0.0310 (0.0093)  time: 0.1717  data: 0.0002  max mem: 6052
[05:29:16.740485] Epoch: [64]  [400/781]  eta: 0:01:05  lr: 0.000077  training_loss: 1.2149 (1.2442)  classification_loss: 1.2063 (1.2346)  loss_mask: 0.0082 (0.0096)  time: 0.1705  data: 0.0002  max mem: 6052
[05:29:20.155976] Epoch: [64]  [420/781]  eta: 0:01:02  lr: 0.000077  training_loss: 1.2968 (1.2457)  classification_loss: 1.2352 (1.2354)  loss_mask: 0.0124 (0.0103)  time: 0.1707  data: 0.0003  max mem: 6052
[05:29:23.568760] Epoch: [64]  [440/781]  eta: 0:00:58  lr: 0.000077  training_loss: 1.2592 (1.2461)  classification_loss: 1.2204 (1.2353)  loss_mask: 0.0044 (0.0108)  time: 0.1706  data: 0.0002  max mem: 6052
[05:29:26.967174] Epoch: [64]  [460/781]  eta: 0:00:55  lr: 0.000077  training_loss: 1.2174 (1.2448)  classification_loss: 1.2126 (1.2342)  loss_mask: 0.0034 (0.0107)  time: 0.1698  data: 0.0002  max mem: 6052
[05:29:30.364989] Epoch: [64]  [480/781]  eta: 0:00:51  lr: 0.000077  training_loss: 1.1968 (1.2433)  classification_loss: 1.1951 (1.2330)  loss_mask: 0.0016 (0.0103)  time: 0.1698  data: 0.0003  max mem: 6052
[05:29:33.761832] Epoch: [64]  [500/781]  eta: 0:00:48  lr: 0.000077  training_loss: 1.2814 (1.2443)  classification_loss: 1.2798 (1.2343)  loss_mask: 0.0009 (0.0100)  time: 0.1698  data: 0.0003  max mem: 6052
[05:29:37.165058] Epoch: [64]  [520/781]  eta: 0:00:45  lr: 0.000077  training_loss: 1.2141 (1.2435)  classification_loss: 1.2134 (1.2339)  loss_mask: 0.0009 (0.0096)  time: 0.1701  data: 0.0003  max mem: 6052
[05:29:40.584973] Epoch: [64]  [540/781]  eta: 0:00:41  lr: 0.000077  training_loss: 1.2387 (1.2438)  classification_loss: 1.2382 (1.2345)  loss_mask: 0.0007 (0.0093)  time: 0.1708  data: 0.0002  max mem: 6052
[05:29:44.014053] Epoch: [64]  [560/781]  eta: 0:00:38  lr: 0.000077  training_loss: 1.2115 (1.2428)  classification_loss: 1.2109 (1.2338)  loss_mask: 0.0007 (0.0090)  time: 0.1714  data: 0.0002  max mem: 6052
[05:29:47.432415] Epoch: [64]  [580/781]  eta: 0:00:34  lr: 0.000076  training_loss: 1.2465 (1.2434)  classification_loss: 1.2455 (1.2348)  loss_mask: 0.0006 (0.0087)  time: 0.1708  data: 0.0002  max mem: 6052
[05:29:50.864048] Epoch: [64]  [600/781]  eta: 0:00:31  lr: 0.000076  training_loss: 1.2470 (1.2440)  classification_loss: 1.2464 (1.2355)  loss_mask: 0.0006 (0.0084)  time: 0.1715  data: 0.0002  max mem: 6052
[05:29:54.284045] Epoch: [64]  [620/781]  eta: 0:00:27  lr: 0.000076  training_loss: 1.2720 (1.2439)  classification_loss: 1.2716 (1.2357)  loss_mask: 0.0005 (0.0082)  time: 0.1709  data: 0.0002  max mem: 6052
[05:29:57.722315] Epoch: [64]  [640/781]  eta: 0:00:24  lr: 0.000076  training_loss: 1.2149 (1.2429)  classification_loss: 1.2143 (1.2350)  loss_mask: 0.0005 (0.0079)  time: 0.1718  data: 0.0002  max mem: 6052
[05:30:01.143727] Epoch: [64]  [660/781]  eta: 0:00:20  lr: 0.000076  training_loss: 1.2490 (1.2431)  classification_loss: 1.2486 (1.2353)  loss_mask: 0.0006 (0.0077)  time: 0.1710  data: 0.0002  max mem: 6052
[05:30:04.560095] Epoch: [64]  [680/781]  eta: 0:00:17  lr: 0.000076  training_loss: 1.2649 (1.2438)  classification_loss: 1.2645 (1.2363)  loss_mask: 0.0004 (0.0075)  time: 0.1707  data: 0.0002  max mem: 6052
[05:30:07.999632] Epoch: [64]  [700/781]  eta: 0:00:13  lr: 0.000076  training_loss: 1.2654 (1.2450)  classification_loss: 1.2650 (1.2377)  loss_mask: 0.0004 (0.0073)  time: 0.1719  data: 0.0002  max mem: 6052
[05:30:11.414557] Epoch: [64]  [720/781]  eta: 0:00:10  lr: 0.000076  training_loss: 1.2502 (1.2462)  classification_loss: 1.2496 (1.2391)  loss_mask: 0.0004 (0.0071)  time: 0.1707  data: 0.0003  max mem: 6052
[05:30:14.838055] Epoch: [64]  [740/781]  eta: 0:00:07  lr: 0.000076  training_loss: 1.1712 (1.2454)  classification_loss: 1.1709 (1.2385)  loss_mask: 0.0004 (0.0069)  time: 0.1711  data: 0.0003  max mem: 6052
[05:30:18.265499] Epoch: [64]  [760/781]  eta: 0:00:03  lr: 0.000076  training_loss: 1.2253 (1.2453)  classification_loss: 1.2251 (1.2385)  loss_mask: 0.0003 (0.0068)  time: 0.1713  data: 0.0002  max mem: 6052
[05:30:21.666104] Epoch: [64]  [780/781]  eta: 0:00:00  lr: 0.000075  training_loss: 1.2279 (1.2450)  classification_loss: 1.2278 (1.2385)  loss_mask: 0.0005 (0.0066)  time: 0.1700  data: 0.0002  max mem: 6052
[05:30:21.822929] Epoch: [64] Total time: 0:02:14 (0.1723 s / it)
[05:30:21.823610] Averaged stats: lr: 0.000075  training_loss: 1.2279 (1.2450)  classification_loss: 1.2278 (1.2385)  loss_mask: 0.0005 (0.0066)
[05:30:22.506824] Test:  [  0/157]  eta: 0:01:46  testing_loss: 0.4166 (0.4166)  acc1: 90.6250 (90.6250)  acc5: 100.0000 (100.0000)  time: 0.6781  data: 0.6466  max mem: 6052
[05:30:22.792844] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5417 (0.5444)  acc1: 84.3750 (83.3807)  acc5: 100.0000 (99.5739)  time: 0.0875  data: 0.0589  max mem: 6052
[05:30:23.081258] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5056 (0.5273)  acc1: 82.8125 (83.5565)  acc5: 100.0000 (99.5536)  time: 0.0286  data: 0.0001  max mem: 6052
[05:30:23.363149] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5186 (0.5423)  acc1: 82.8125 (83.4173)  acc5: 100.0000 (99.3448)  time: 0.0284  data: 0.0001  max mem: 6052
[05:30:23.653237] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5302 (0.5425)  acc1: 84.3750 (83.5747)  acc5: 100.0000 (99.2378)  time: 0.0284  data: 0.0001  max mem: 6052
[05:30:23.937347] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5103 (0.5339)  acc1: 84.3750 (83.9154)  acc5: 100.0000 (99.2647)  time: 0.0286  data: 0.0001  max mem: 6052
[05:30:24.226217] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4903 (0.5318)  acc1: 84.3750 (84.0420)  acc5: 100.0000 (99.2316)  time: 0.0285  data: 0.0001  max mem: 6052
[05:30:24.511759] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4966 (0.5292)  acc1: 84.3750 (84.2210)  acc5: 100.0000 (99.2738)  time: 0.0286  data: 0.0001  max mem: 6052
[05:30:24.800346] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5473 (0.5373)  acc1: 84.3750 (83.9506)  acc5: 100.0000 (99.2670)  time: 0.0286  data: 0.0003  max mem: 6052
[05:30:25.092046] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5364 (0.5354)  acc1: 82.8125 (84.0831)  acc5: 98.4375 (99.1930)  time: 0.0289  data: 0.0003  max mem: 6052
[05:30:25.381460] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5364 (0.5400)  acc1: 82.8125 (83.8026)  acc5: 98.4375 (99.1801)  time: 0.0289  data: 0.0002  max mem: 6052
[05:30:25.663075] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5405 (0.5407)  acc1: 84.3750 (83.7979)  acc5: 100.0000 (99.1976)  time: 0.0284  data: 0.0002  max mem: 6052
[05:30:25.947889] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5202 (0.5402)  acc1: 84.3750 (83.7681)  acc5: 100.0000 (99.2381)  time: 0.0282  data: 0.0002  max mem: 6052
[05:30:26.233183] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5559 (0.5418)  acc1: 82.8125 (83.7071)  acc5: 100.0000 (99.2605)  time: 0.0284  data: 0.0001  max mem: 6052
[05:30:26.515816] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5553 (0.5405)  acc1: 82.8125 (83.6990)  acc5: 100.0000 (99.2354)  time: 0.0283  data: 0.0001  max mem: 6052
[05:30:26.795022] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5523 (0.5403)  acc1: 82.8125 (83.5886)  acc5: 100.0000 (99.2446)  time: 0.0280  data: 0.0001  max mem: 6052
[05:30:26.944881] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5285 (0.5407)  acc1: 82.8125 (83.5100)  acc5: 100.0000 (99.2500)  time: 0.0268  data: 0.0001  max mem: 6052
[05:30:27.116462] Test: Total time: 0:00:05 (0.0337 s / it)
[05:30:27.116937] * Acc@1 83.510 Acc@5 99.250 loss 0.541
[05:30:27.117222] Accuracy of the network on the 10000 test images: 83.5%
[05:30:27.117392] Max accuracy: 83.76%
[05:30:27.291199] log_dir: ./output_dir
[05:30:28.152281] Epoch: [65]  [  0/781]  eta: 0:11:11  lr: 0.000075  training_loss: 1.1933 (1.1933)  classification_loss: 1.1929 (1.1929)  loss_mask: 0.0004 (0.0004)  time: 0.8596  data: 0.6378  max mem: 6052
[05:30:31.564888] Epoch: [65]  [ 20/781]  eta: 0:02:34  lr: 0.000075  training_loss: 1.1789 (1.1977)  classification_loss: 1.1784 (1.1973)  loss_mask: 0.0003 (0.0004)  time: 0.1705  data: 0.0002  max mem: 6052
[05:30:34.995672] Epoch: [65]  [ 40/781]  eta: 0:02:19  lr: 0.000075  training_loss: 1.2550 (1.2243)  classification_loss: 1.2545 (1.2239)  loss_mask: 0.0004 (0.0004)  time: 0.1715  data: 0.0002  max mem: 6052
[05:30:38.409530] Epoch: [65]  [ 60/781]  eta: 0:02:11  lr: 0.000075  training_loss: 1.2394 (1.2361)  classification_loss: 1.2392 (1.2357)  loss_mask: 0.0003 (0.0004)  time: 0.1706  data: 0.0002  max mem: 6052
[05:30:41.818740] Epoch: [65]  [ 80/781]  eta: 0:02:05  lr: 0.000075  training_loss: 1.2391 (1.2443)  classification_loss: 1.2388 (1.2439)  loss_mask: 0.0003 (0.0004)  time: 0.1704  data: 0.0001  max mem: 6052
[05:30:45.231038] Epoch: [65]  [100/781]  eta: 0:02:00  lr: 0.000075  training_loss: 1.2391 (1.2458)  classification_loss: 1.2384 (1.2454)  loss_mask: 0.0004 (0.0004)  time: 0.1705  data: 0.0002  max mem: 6052
[05:30:48.659517] Epoch: [65]  [120/781]  eta: 0:01:56  lr: 0.000075  training_loss: 1.2015 (1.2421)  classification_loss: 1.2009 (1.2417)  loss_mask: 0.0003 (0.0004)  time: 0.1713  data: 0.0002  max mem: 6052
[05:30:52.065453] Epoch: [65]  [140/781]  eta: 0:01:52  lr: 0.000075  training_loss: 1.1679 (1.2348)  classification_loss: 1.1676 (1.2344)  loss_mask: 0.0003 (0.0004)  time: 0.1701  data: 0.0002  max mem: 6052
[05:30:55.496117] Epoch: [65]  [160/781]  eta: 0:01:48  lr: 0.000075  training_loss: 1.1955 (1.2326)  classification_loss: 1.1950 (1.2322)  loss_mask: 0.0003 (0.0004)  time: 0.1715  data: 0.0002  max mem: 6052
[05:30:58.904637] Epoch: [65]  [180/781]  eta: 0:01:44  lr: 0.000075  training_loss: 1.1824 (1.2298)  classification_loss: 1.1820 (1.2295)  loss_mask: 0.0003 (0.0004)  time: 0.1703  data: 0.0004  max mem: 6052
[05:31:02.325288] Epoch: [65]  [200/781]  eta: 0:01:41  lr: 0.000075  training_loss: 1.1978 (1.2291)  classification_loss: 1.1974 (1.2288)  loss_mask: 0.0003 (0.0004)  time: 0.1710  data: 0.0002  max mem: 6052
[05:31:05.734406] Epoch: [65]  [220/781]  eta: 0:01:37  lr: 0.000074  training_loss: 1.2083 (1.2289)  classification_loss: 1.2080 (1.2286)  loss_mask: 0.0003 (0.0004)  time: 0.1704  data: 0.0002  max mem: 6052
[05:31:09.233514] Epoch: [65]  [240/781]  eta: 0:01:34  lr: 0.000074  training_loss: 1.1643 (1.2277)  classification_loss: 1.1632 (1.2273)  loss_mask: 0.0002 (0.0004)  time: 0.1749  data: 0.0003  max mem: 6052
[05:31:12.650585] Epoch: [65]  [260/781]  eta: 0:01:30  lr: 0.000074  training_loss: 1.2749 (1.2312)  classification_loss: 1.2746 (1.2309)  loss_mask: 0.0003 (0.0003)  time: 0.1708  data: 0.0004  max mem: 6052
[05:31:16.059306] Epoch: [65]  [280/781]  eta: 0:01:26  lr: 0.000074  training_loss: 1.2036 (1.2298)  classification_loss: 1.2014 (1.2295)  loss_mask: 0.0003 (0.0003)  time: 0.1704  data: 0.0002  max mem: 6052
[05:31:19.500032] Epoch: [65]  [300/781]  eta: 0:01:23  lr: 0.000074  training_loss: 1.2636 (1.2328)  classification_loss: 1.2631 (1.2325)  loss_mask: 0.0002 (0.0003)  time: 0.1720  data: 0.0003  max mem: 6052
[05:31:22.921639] Epoch: [65]  [320/781]  eta: 0:01:19  lr: 0.000074  training_loss: 1.2153 (1.2333)  classification_loss: 1.2149 (1.2329)  loss_mask: 0.0003 (0.0004)  time: 0.1710  data: 0.0001  max mem: 6052
[05:31:26.334145] Epoch: [65]  [340/781]  eta: 0:01:16  lr: 0.000074  training_loss: 1.2358 (1.2321)  classification_loss: 1.2355 (1.2318)  loss_mask: 0.0002 (0.0003)  time: 0.1705  data: 0.0001  max mem: 6052
[05:31:29.734559] Epoch: [65]  [360/781]  eta: 0:01:12  lr: 0.000074  training_loss: 1.2285 (1.2327)  classification_loss: 1.2284 (1.2322)  loss_mask: 0.0003 (0.0005)  time: 0.1699  data: 0.0002  max mem: 6052
[05:31:33.159425] Epoch: [65]  [380/781]  eta: 0:01:09  lr: 0.000074  training_loss: 1.1891 (1.2319)  classification_loss: 1.1888 (1.2314)  loss_mask: 0.0002 (0.0004)  time: 0.1712  data: 0.0002  max mem: 6052
[05:31:36.579376] Epoch: [65]  [400/781]  eta: 0:01:05  lr: 0.000074  training_loss: 1.2320 (1.2312)  classification_loss: 1.2317 (1.2308)  loss_mask: 0.0002 (0.0004)  time: 0.1709  data: 0.0003  max mem: 6052
[05:31:40.053877] Epoch: [65]  [420/781]  eta: 0:01:02  lr: 0.000073  training_loss: 1.2224 (1.2314)  classification_loss: 1.2221 (1.2309)  loss_mask: 0.0003 (0.0004)  time: 0.1736  data: 0.0002  max mem: 6052
[05:31:43.476112] Epoch: [65]  [440/781]  eta: 0:00:58  lr: 0.000073  training_loss: 1.2595 (1.2326)  classification_loss: 1.2583 (1.2321)  loss_mask: 0.0003 (0.0004)  time: 0.1710  data: 0.0002  max mem: 6052
[05:31:46.890542] Epoch: [65]  [460/781]  eta: 0:00:55  lr: 0.000073  training_loss: 1.1536 (1.2295)  classification_loss: 1.1535 (1.2291)  loss_mask: 0.0002 (0.0004)  time: 0.1707  data: 0.0003  max mem: 6052
[05:31:50.305309] Epoch: [65]  [480/781]  eta: 0:00:51  lr: 0.000073  training_loss: 1.2050 (1.2297)  classification_loss: 1.2046 (1.2293)  loss_mask: 0.0002 (0.0004)  time: 0.1705  data: 0.0002  max mem: 6052
[05:31:53.733589] Epoch: [65]  [500/781]  eta: 0:00:48  lr: 0.000073  training_loss: 1.1763 (1.2296)  classification_loss: 1.1737 (1.2289)  loss_mask: 0.0005 (0.0007)  time: 0.1713  data: 0.0002  max mem: 6052
[05:31:57.147516] Epoch: [65]  [520/781]  eta: 0:00:44  lr: 0.000073  training_loss: 1.2772 (1.2312)  classification_loss: 1.2502 (1.2301)  loss_mask: 0.0006 (0.0011)  time: 0.1706  data: 0.0002  max mem: 6052
[05:32:00.569627] Epoch: [65]  [540/781]  eta: 0:00:41  lr: 0.000073  training_loss: 1.2129 (1.2317)  classification_loss: 1.2127 (1.2306)  loss_mask: 0.0003 (0.0011)  time: 0.1710  data: 0.0001  max mem: 6052
[05:32:03.984624] Epoch: [65]  [560/781]  eta: 0:00:38  lr: 0.000073  training_loss: 1.2090 (1.2313)  classification_loss: 1.2089 (1.2303)  loss_mask: 0.0004 (0.0011)  time: 0.1707  data: 0.0002  max mem: 6052
[05:32:07.403240] Epoch: [65]  [580/781]  eta: 0:00:34  lr: 0.000073  training_loss: 1.2445 (1.2318)  classification_loss: 1.2443 (1.2307)  loss_mask: 0.0002 (0.0011)  time: 0.1708  data: 0.0003  max mem: 6052
[05:32:10.815959] Epoch: [65]  [600/781]  eta: 0:00:31  lr: 0.000073  training_loss: 1.1970 (1.2317)  classification_loss: 1.1949 (1.2307)  loss_mask: 0.0002 (0.0010)  time: 0.1706  data: 0.0002  max mem: 6052
[05:32:14.221765] Epoch: [65]  [620/781]  eta: 0:00:27  lr: 0.000073  training_loss: 1.2629 (1.2324)  classification_loss: 1.2623 (1.2314)  loss_mask: 0.0002 (0.0010)  time: 0.1702  data: 0.0002  max mem: 6052
[05:32:17.615373] Epoch: [65]  [640/781]  eta: 0:00:24  lr: 0.000072  training_loss: 1.2649 (1.2332)  classification_loss: 1.2647 (1.2322)  loss_mask: 0.0002 (0.0010)  time: 0.1696  data: 0.0003  max mem: 6052
[05:32:21.035485] Epoch: [65]  [660/781]  eta: 0:00:20  lr: 0.000072  training_loss: 1.2753 (1.2346)  classification_loss: 1.2749 (1.2336)  loss_mask: 0.0002 (0.0010)  time: 0.1709  data: 0.0002  max mem: 6052
[05:32:24.442121] Epoch: [65]  [680/781]  eta: 0:00:17  lr: 0.000072  training_loss: 1.2363 (1.2354)  classification_loss: 1.2362 (1.2344)  loss_mask: 0.0002 (0.0009)  time: 0.1703  data: 0.0003  max mem: 6052
[05:32:27.858553] Epoch: [65]  [700/781]  eta: 0:00:13  lr: 0.000072  training_loss: 1.2287 (1.2357)  classification_loss: 1.2284 (1.2347)  loss_mask: 0.0002 (0.0009)  time: 0.1708  data: 0.0002  max mem: 6052
[05:32:31.274330] Epoch: [65]  [720/781]  eta: 0:00:10  lr: 0.000072  training_loss: 1.2516 (1.2365)  classification_loss: 1.2514 (1.2355)  loss_mask: 0.0002 (0.0009)  time: 0.1707  data: 0.0002  max mem: 6052
[05:32:34.705922] Epoch: [65]  [740/781]  eta: 0:00:07  lr: 0.000072  training_loss: 1.1947 (1.2358)  classification_loss: 1.1945 (1.2349)  loss_mask: 0.0002 (0.0009)  time: 0.1715  data: 0.0002  max mem: 6052
[05:32:38.120860] Epoch: [65]  [760/781]  eta: 0:00:03  lr: 0.000072  training_loss: 1.2323 (1.2361)  classification_loss: 1.2322 (1.2352)  loss_mask: 0.0002 (0.0009)  time: 0.1707  data: 0.0002  max mem: 6052
[05:32:41.533863] Epoch: [65]  [780/781]  eta: 0:00:00  lr: 0.000072  training_loss: 1.2312 (1.2357)  classification_loss: 1.2311 (1.2349)  loss_mask: 0.0002 (0.0009)  time: 0.1705  data: 0.0002  max mem: 6052
[05:32:41.706890] Epoch: [65] Total time: 0:02:14 (0.1721 s / it)
[05:32:41.707633] Averaged stats: lr: 0.000072  training_loss: 1.2312 (1.2357)  classification_loss: 1.2311 (1.2349)  loss_mask: 0.0002 (0.0009)
[05:32:42.380333] Test:  [  0/157]  eta: 0:01:44  testing_loss: 0.4075 (0.4075)  acc1: 85.9375 (85.9375)  acc5: 100.0000 (100.0000)  time: 0.6674  data: 0.6377  max mem: 6052
[05:32:42.668682] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5608 (0.5327)  acc1: 84.3750 (82.9545)  acc5: 100.0000 (99.4318)  time: 0.0866  data: 0.0583  max mem: 6052
[05:32:42.951461] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4999 (0.5074)  acc1: 84.3750 (84.7470)  acc5: 100.0000 (99.4792)  time: 0.0284  data: 0.0002  max mem: 6052
[05:32:43.240200] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4951 (0.5172)  acc1: 84.3750 (84.2742)  acc5: 100.0000 (99.2944)  time: 0.0283  data: 0.0003  max mem: 6052
[05:32:43.521311] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5018 (0.5229)  acc1: 84.3750 (84.1082)  acc5: 100.0000 (99.1997)  time: 0.0283  data: 0.0003  max mem: 6052
[05:32:43.807891] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4946 (0.5159)  acc1: 85.9375 (84.6507)  acc5: 100.0000 (99.2647)  time: 0.0282  data: 0.0001  max mem: 6052
[05:32:44.095846] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4908 (0.5155)  acc1: 85.9375 (84.5287)  acc5: 100.0000 (99.2572)  time: 0.0285  data: 0.0001  max mem: 6052
[05:32:44.389570] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4861 (0.5105)  acc1: 84.3750 (84.7491)  acc5: 100.0000 (99.2738)  time: 0.0289  data: 0.0002  max mem: 6052
[05:32:44.672856] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5079 (0.5195)  acc1: 84.3750 (84.3750)  acc5: 100.0000 (99.2670)  time: 0.0287  data: 0.0002  max mem: 6052
[05:32:44.957176] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5283 (0.5178)  acc1: 84.3750 (84.4265)  acc5: 100.0000 (99.2445)  time: 0.0283  data: 0.0002  max mem: 6052
[05:32:45.239780] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5027 (0.5210)  acc1: 84.3750 (84.3286)  acc5: 100.0000 (99.2574)  time: 0.0282  data: 0.0002  max mem: 6052
[05:32:45.521359] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5187 (0.5205)  acc1: 84.3750 (84.4032)  acc5: 100.0000 (99.2117)  time: 0.0280  data: 0.0001  max mem: 6052
[05:32:45.801843] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5126 (0.5212)  acc1: 84.3750 (84.3363)  acc5: 100.0000 (99.2252)  time: 0.0279  data: 0.0002  max mem: 6052
[05:32:46.082995] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4989 (0.5206)  acc1: 84.3750 (84.4227)  acc5: 100.0000 (99.2366)  time: 0.0280  data: 0.0002  max mem: 6052
[05:32:46.363960] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5121 (0.5195)  acc1: 82.8125 (84.3861)  acc5: 100.0000 (99.2575)  time: 0.0280  data: 0.0001  max mem: 6052
[05:32:46.643886] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5283 (0.5185)  acc1: 82.8125 (84.3750)  acc5: 100.0000 (99.2653)  time: 0.0279  data: 0.0001  max mem: 6052
[05:32:46.794020] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5008 (0.5191)  acc1: 84.3750 (84.3100)  acc5: 100.0000 (99.2800)  time: 0.0270  data: 0.0001  max mem: 6052
[05:32:46.953033] Test: Total time: 0:00:05 (0.0334 s / it)
[05:32:46.953466] * Acc@1 84.310 Acc@5 99.280 loss 0.519
[05:32:46.953771] Accuracy of the network on the 10000 test images: 84.3%
[05:32:46.953979] Max accuracy: 84.31%
[05:32:47.238366] log_dir: ./output_dir
[05:32:48.134014] Epoch: [66]  [  0/781]  eta: 0:11:38  lr: 0.000072  training_loss: 1.0800 (1.0800)  classification_loss: 1.0800 (1.0800)  loss_mask: 0.0000 (0.0000)  time: 0.8939  data: 0.6780  max mem: 6052
[05:32:51.565574] Epoch: [66]  [ 20/781]  eta: 0:02:36  lr: 0.000072  training_loss: 1.2049 (1.1852)  classification_loss: 1.2046 (1.1849)  loss_mask: 0.0002 (0.0003)  time: 0.1715  data: 0.0003  max mem: 6052
[05:32:54.995561] Epoch: [66]  [ 40/781]  eta: 0:02:20  lr: 0.000072  training_loss: 1.2166 (1.2094)  classification_loss: 1.2165 (1.2091)  loss_mask: 0.0001 (0.0002)  time: 0.1714  data: 0.0002  max mem: 6052
[05:32:58.409123] Epoch: [66]  [ 60/781]  eta: 0:02:11  lr: 0.000071  training_loss: 1.2689 (1.2262)  classification_loss: 1.2687 (1.2259)  loss_mask: 0.0002 (0.0002)  time: 0.1706  data: 0.0003  max mem: 6052
[05:33:01.818637] Epoch: [66]  [ 80/781]  eta: 0:02:06  lr: 0.000071  training_loss: 1.2242 (1.2281)  classification_loss: 1.2240 (1.2279)  loss_mask: 0.0002 (0.0002)  time: 0.1704  data: 0.0003  max mem: 6052
[05:33:05.235748] Epoch: [66]  [100/781]  eta: 0:02:01  lr: 0.000071  training_loss: 1.2156 (1.2240)  classification_loss: 1.2154 (1.2238)  loss_mask: 0.0002 (0.0002)  time: 0.1708  data: 0.0002  max mem: 6052
[05:33:08.650645] Epoch: [66]  [120/781]  eta: 0:01:56  lr: 0.000071  training_loss: 1.2029 (1.2222)  classification_loss: 1.2028 (1.2220)  loss_mask: 0.0002 (0.0002)  time: 0.1707  data: 0.0002  max mem: 6052
[05:33:12.062442] Epoch: [66]  [140/781]  eta: 0:01:52  lr: 0.000071  training_loss: 1.1706 (1.2214)  classification_loss: 1.1702 (1.2212)  loss_mask: 0.0002 (0.0002)  time: 0.1705  data: 0.0003  max mem: 6052
[05:33:15.475861] Epoch: [66]  [160/781]  eta: 0:01:48  lr: 0.000071  training_loss: 1.2612 (1.2237)  classification_loss: 1.2610 (1.2235)  loss_mask: 0.0002 (0.0002)  time: 0.1706  data: 0.0002  max mem: 6052
[05:33:18.899200] Epoch: [66]  [180/781]  eta: 0:01:45  lr: 0.000071  training_loss: 1.2050 (1.2223)  classification_loss: 1.2048 (1.2221)  loss_mask: 0.0001 (0.0002)  time: 0.1711  data: 0.0002  max mem: 6052
[05:33:22.315103] Epoch: [66]  [200/781]  eta: 0:01:41  lr: 0.000071  training_loss: 1.2418 (1.2241)  classification_loss: 1.2417 (1.2239)  loss_mask: 0.0002 (0.0002)  time: 0.1707  data: 0.0002  max mem: 6052
[05:33:25.767914] Epoch: [66]  [220/781]  eta: 0:01:37  lr: 0.000071  training_loss: 1.2365 (1.2237)  classification_loss: 1.2364 (1.2235)  loss_mask: 0.0002 (0.0002)  time: 0.1726  data: 0.0002  max mem: 6052
[05:33:29.190351] Epoch: [66]  [240/781]  eta: 0:01:34  lr: 0.000071  training_loss: 1.2357 (1.2255)  classification_loss: 1.2355 (1.2253)  loss_mask: 0.0002 (0.0002)  time: 0.1710  data: 0.0003  max mem: 6052
[05:33:32.592053] Epoch: [66]  [260/781]  eta: 0:01:30  lr: 0.000071  training_loss: 1.2101 (1.2254)  classification_loss: 1.2099 (1.2252)  loss_mask: 0.0001 (0.0002)  time: 0.1700  data: 0.0002  max mem: 6052
[05:33:36.021235] Epoch: [66]  [280/781]  eta: 0:01:26  lr: 0.000070  training_loss: 1.2230 (1.2259)  classification_loss: 1.2229 (1.2257)  loss_mask: 0.0001 (0.0002)  time: 0.1713  data: 0.0002  max mem: 6052
[05:33:39.466732] Epoch: [66]  [300/781]  eta: 0:01:23  lr: 0.000070  training_loss: 1.2990 (1.2305)  classification_loss: 1.2988 (1.2303)  loss_mask: 0.0001 (0.0002)  time: 0.1722  data: 0.0003  max mem: 6052
[05:33:42.889860] Epoch: [66]  [320/781]  eta: 0:01:19  lr: 0.000070  training_loss: 1.2298 (1.2300)  classification_loss: 1.2298 (1.2298)  loss_mask: 0.0001 (0.0002)  time: 0.1711  data: 0.0002  max mem: 6052
[05:33:46.345178] Epoch: [66]  [340/781]  eta: 0:01:16  lr: 0.000070  training_loss: 1.1691 (1.2292)  classification_loss: 1.1689 (1.2290)  loss_mask: 0.0002 (0.0002)  time: 0.1727  data: 0.0002  max mem: 6052
[05:33:49.800378] Epoch: [66]  [360/781]  eta: 0:01:12  lr: 0.000070  training_loss: 1.2306 (1.2285)  classification_loss: 1.2304 (1.2283)  loss_mask: 0.0001 (0.0002)  time: 0.1727  data: 0.0002  max mem: 6052
[05:33:53.226225] Epoch: [66]  [380/781]  eta: 0:01:09  lr: 0.000070  training_loss: 1.1931 (1.2260)  classification_loss: 1.1931 (1.2258)  loss_mask: 0.0002 (0.0002)  time: 0.1712  data: 0.0002  max mem: 6052
[05:33:56.640292] Epoch: [66]  [400/781]  eta: 0:01:05  lr: 0.000070  training_loss: 1.2206 (1.2268)  classification_loss: 1.2204 (1.2266)  loss_mask: 0.0001 (0.0002)  time: 0.1706  data: 0.0003  max mem: 6052

[05:34:00.084944] Epoch: [66]  [420/781]  eta: 0:01:02  lr: 0.000070  training_loss: 1.2039 (1.2256)  classification_loss: 1.2037 (1.2254)  loss_mask: 0.0002 (0.0002)  time: 0.1722  data: 0.0004  max mem: 6052
[05:34:03.508837] Epoch: [66]  [440/781]  eta: 0:00:58  lr: 0.000070  training_loss: 1.2078 (1.2260)  classification_loss: 1.2076 (1.2258)  loss_mask: 0.0001 (0.0002)  time: 0.1711  data: 0.0003  max mem: 6052
[05:34:06.935348] Epoch: [66]  [460/781]  eta: 0:00:55  lr: 0.000070  training_loss: 1.2009 (1.2258)  classification_loss: 1.2007 (1.2256)  loss_mask: 0.0001 (0.0002)  time: 0.1712  data: 0.0003  max mem: 6052
[05:34:10.365222] Epoch: [66]  [480/781]  eta: 0:00:51  lr: 0.000069  training_loss: 1.2132 (1.2253)  classification_loss: 1.2131 (1.2251)  loss_mask: 0.0001 (0.0002)  time: 0.1714  data: 0.0002  max mem: 6052
[05:34:13.818033] Epoch: [66]  [500/781]  eta: 0:00:48  lr: 0.000069  training_loss: 1.2476 (1.2264)  classification_loss: 1.2474 (1.2263)  loss_mask: 0.0001 (0.0002)  time: 0.1725  data: 0.0002  max mem: 6052
[05:34:17.217593] Epoch: [66]  [520/781]  eta: 0:00:45  lr: 0.000069  training_loss: 1.2705 (1.2284)  classification_loss: 1.2284 (1.2272)  loss_mask: 0.0003 (0.0011)  time: 0.1699  data: 0.0003  max mem: 6052
[05:34:20.638973] Epoch: [66]  [540/781]  eta: 0:00:41  lr: 0.000069  training_loss: 1.2681 (1.2302)  classification_loss: 1.2416 (1.2283)  loss_mask: 0.0051 (0.0019)  time: 0.1710  data: 0.0002  max mem: 6052
[05:34:24.050420] Epoch: [66]  [560/781]  eta: 0:00:38  lr: 0.000069  training_loss: 1.2316 (1.2305)  classification_loss: 1.2250 (1.2282)  loss_mask: 0.0032 (0.0023)  time: 0.1705  data: 0.0002  max mem: 6052
[05:34:27.463598] Epoch: [66]  [580/781]  eta: 0:00:34  lr: 0.000069  training_loss: 1.2028 (1.2301)  classification_loss: 1.2021 (1.2278)  loss_mask: 0.0007 (0.0023)  time: 0.1706  data: 0.0002  max mem: 6052
[05:34:30.868252] Epoch: [66]  [600/781]  eta: 0:00:31  lr: 0.000069  training_loss: 1.2208 (1.2305)  classification_loss: 1.2202 (1.2283)  loss_mask: 0.0005 (0.0022)  time: 0.1702  data: 0.0002  max mem: 6052
[05:34:34.282716] Epoch: [66]  [620/781]  eta: 0:00:27  lr: 0.000069  training_loss: 1.1879 (1.2301)  classification_loss: 1.1877 (1.2280)  loss_mask: 0.0003 (0.0022)  time: 0.1706  data: 0.0003  max mem: 6052
[05:34:37.714124] Epoch: [66]  [640/781]  eta: 0:00:24  lr: 0.000069  training_loss: 1.2143 (1.2299)  classification_loss: 1.2139 (1.2277)  loss_mask: 0.0003 (0.0021)  time: 0.1715  data: 0.0003  max mem: 6052
[05:34:41.154850] Epoch: [66]  [660/781]  eta: 0:00:20  lr: 0.000069  training_loss: 1.2285 (1.2306)  classification_loss: 1.2283 (1.2285)  loss_mask: 0.0003 (0.0021)  time: 0.1719  data: 0.0002  max mem: 6052
[05:34:44.567986] Epoch: [66]  [680/781]  eta: 0:00:17  lr: 0.000069  training_loss: 1.1922 (1.2306)  classification_loss: 1.1704 (1.2280)  loss_mask: 0.0038 (0.0027)  time: 0.1706  data: 0.0002  max mem: 6052
[05:34:47.988845] Epoch: [66]  [700/781]  eta: 0:00:13  lr: 0.000068  training_loss: 1.2754 (1.2321)  classification_loss: 1.2466 (1.2287)  loss_mask: 0.0048 (0.0035)  time: 0.1709  data: 0.0002  max mem: 6052
[05:34:51.400794] Epoch: [66]  [720/781]  eta: 0:00:10  lr: 0.000068  training_loss: 1.2729 (1.2342)  classification_loss: 1.2257 (1.2294)  loss_mask: 0.0183 (0.0048)  time: 0.1705  data: 0.0002  max mem: 6052
[05:34:54.820668] Epoch: [66]  [740/781]  eta: 0:00:07  lr: 0.000068  training_loss: 1.1831 (1.2340)  classification_loss: 1.1671 (1.2288)  loss_mask: 0.0052 (0.0053)  time: 0.1709  data: 0.0002  max mem: 6052
[05:34:58.234994] Epoch: [66]  [760/781]  eta: 0:00:03  lr: 0.000068  training_loss: 1.2667 (1.2365)  classification_loss: 1.2653 (1.2309)  loss_mask: 0.0037 (0.0055)  time: 0.1706  data: 0.0002  max mem: 6052
[05:35:01.635856] Epoch: [66]  [780/781]  eta: 0:00:00  lr: 0.000068  training_loss: 1.2472 (1.2368)  classification_loss: 1.2357 (1.2311)  loss_mask: 0.0017 (0.0058)  time: 0.1700  data: 0.0002  max mem: 6052
[05:35:01.810723] Epoch: [66] Total time: 0:02:14 (0.1723 s / it)
[05:35:01.811729] Averaged stats: lr: 0.000068  training_loss: 1.2472 (1.2368)  classification_loss: 1.2357 (1.2311)  loss_mask: 0.0017 (0.0058)
[05:35:02.474033] Test:  [  0/157]  eta: 0:01:42  testing_loss: 0.4387 (0.4387)  acc1: 90.6250 (90.6250)  acc5: 100.0000 (100.0000)  time: 0.6557  data: 0.6155  max mem: 6052
[05:35:02.762295] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5522 (0.5245)  acc1: 84.3750 (83.5227)  acc5: 100.0000 (99.5739)  time: 0.0856  data: 0.0561  max mem: 6052
[05:35:03.042187] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4593 (0.4868)  acc1: 85.9375 (85.3423)  acc5: 100.0000 (99.5536)  time: 0.0282  data: 0.0001  max mem: 6052
[05:35:03.326154] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4593 (0.5040)  acc1: 85.9375 (84.3750)  acc5: 100.0000 (99.3952)  time: 0.0281  data: 0.0001  max mem: 6052
[05:35:03.616835] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4988 (0.5109)  acc1: 84.3750 (84.2226)  acc5: 98.4375 (99.2759)  time: 0.0286  data: 0.0002  max mem: 6052
[05:35:03.898015] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4948 (0.5055)  acc1: 85.9375 (84.8346)  acc5: 100.0000 (99.2953)  time: 0.0285  data: 0.0002  max mem: 6052
[05:35:04.181390] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4777 (0.5066)  acc1: 85.9375 (84.5799)  acc5: 100.0000 (99.2316)  time: 0.0281  data: 0.0001  max mem: 6052
[05:35:04.465953] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4738 (0.5001)  acc1: 84.3750 (84.7271)  acc5: 100.0000 (99.3178)  time: 0.0282  data: 0.0002  max mem: 6052
[05:35:04.757228] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4853 (0.5078)  acc1: 84.3750 (84.5486)  acc5: 100.0000 (99.2477)  time: 0.0286  data: 0.0002  max mem: 6052
[05:35:05.045290] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4853 (0.5055)  acc1: 84.3750 (84.5982)  acc5: 98.4375 (99.1930)  time: 0.0287  data: 0.0002  max mem: 6052
[05:35:05.330685] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5084 (0.5102)  acc1: 84.3750 (84.2822)  acc5: 100.0000 (99.2265)  time: 0.0285  data: 0.0002  max mem: 6052
[05:35:05.616225] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5328 (0.5109)  acc1: 82.8125 (84.3187)  acc5: 100.0000 (99.2258)  time: 0.0284  data: 0.0002  max mem: 6052
[05:35:05.898403] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4897 (0.5104)  acc1: 82.8125 (84.2588)  acc5: 100.0000 (99.2639)  time: 0.0283  data: 0.0001  max mem: 6052
[05:35:06.183110] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4889 (0.5104)  acc1: 84.3750 (84.4585)  acc5: 100.0000 (99.2844)  time: 0.0282  data: 0.0001  max mem: 6052
[05:35:06.464086] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4724 (0.5072)  acc1: 87.5000 (84.6299)  acc5: 100.0000 (99.2908)  time: 0.0282  data: 0.0001  max mem: 6052
[05:35:06.742273] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4731 (0.5057)  acc1: 85.9375 (84.6440)  acc5: 100.0000 (99.3067)  time: 0.0278  data: 0.0001  max mem: 6052
[05:35:06.891433] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4779 (0.5064)  acc1: 85.9375 (84.5800)  acc5: 100.0000 (99.3000)  time: 0.0268  data: 0.0001  max mem: 6052
[05:35:07.058073] Test: Total time: 0:00:05 (0.0334 s / it)
[05:35:07.058697] * Acc@1 84.580 Acc@5 99.300 loss 0.506
[05:35:07.059033] Accuracy of the network on the 10000 test images: 84.6%
[05:35:07.059210] Max accuracy: 84.58%
[05:35:07.412553] log_dir: ./output_dir
[05:35:08.260272] Epoch: [67]  [  0/781]  eta: 0:11:00  lr: 0.000068  training_loss: 1.1353 (1.1353)  classification_loss: 1.1313 (1.1313)  loss_mask: 0.0040 (0.0040)  time: 0.8460  data: 0.6512  max mem: 6052
[05:35:11.693826] Epoch: [67]  [ 20/781]  eta: 0:02:35  lr: 0.000068  training_loss: 1.1759 (1.2153)  classification_loss: 1.1653 (1.2073)  loss_mask: 0.0012 (0.0081)  time: 0.1716  data: 0.0002  max mem: 6052
[05:35:15.141886] Epoch: [67]  [ 40/781]  eta: 0:02:19  lr: 0.000068  training_loss: 1.2108 (1.2212)  classification_loss: 1.1981 (1.2133)  loss_mask: 0.0019 (0.0079)  time: 0.1723  data: 0.0002  max mem: 6052
[05:35:18.572461] Epoch: [67]  [ 60/781]  eta: 0:02:11  lr: 0.000068  training_loss: 1.2211 (1.2249)  classification_loss: 1.2198 (1.2192)  loss_mask: 0.0011 (0.0057)  time: 0.1714  data: 0.0002  max mem: 6052
[05:35:22.050838] Epoch: [67]  [ 80/781]  eta: 0:02:06  lr: 0.000068  training_loss: 1.1845 (1.2200)  classification_loss: 1.1839 (1.2156)  loss_mask: 0.0005 (0.0044)  time: 0.1738  data: 0.0003  max mem: 6052
[05:35:25.588322] Epoch: [67]  [100/781]  eta: 0:02:02  lr: 0.000068  training_loss: 1.1690 (1.2179)  classification_loss: 1.1687 (1.2143)  loss_mask: 0.0005 (0.0036)  time: 0.1768  data: 0.0003  max mem: 6052
[05:35:29.154574] Epoch: [67]  [120/781]  eta: 0:01:58  lr: 0.000068  training_loss: 1.2383 (1.2165)  classification_loss: 1.2376 (1.2134)  loss_mask: 0.0003 (0.0031)  time: 0.1782  data: 0.0003  max mem: 6052
[05:35:32.700363] Epoch: [67]  [140/781]  eta: 0:01:54  lr: 0.000067  training_loss: 1.2156 (1.2173)  classification_loss: 1.2154 (1.2145)  loss_mask: 0.0004 (0.0027)  time: 0.1772  data: 0.0002  max mem: 6052
[05:35:36.119991] Epoch: [67]  [160/781]  eta: 0:01:50  lr: 0.000067  training_loss: 1.1660 (1.2160)  classification_loss: 1.1654 (1.2136)  loss_mask: 0.0004 (0.0024)  time: 0.1709  data: 0.0002  max mem: 6052
[05:35:39.543885] Epoch: [67]  [180/781]  eta: 0:01:46  lr: 0.000067  training_loss: 1.1877 (1.2155)  classification_loss: 1.1873 (1.2133)  loss_mask: 0.0004 (0.0022)  time: 0.1711  data: 0.0002  max mem: 6052
[05:35:42.950251] Epoch: [67]  [200/781]  eta: 0:01:42  lr: 0.000067  training_loss: 1.1681 (1.2136)  classification_loss: 1.1679 (1.2115)  loss_mask: 0.0005 (0.0021)  time: 0.1702  data: 0.0002  max mem: 6052
[05:35:46.371115] Epoch: [67]  [220/781]  eta: 0:01:38  lr: 0.000067  training_loss: 1.2632 (1.2168)  classification_loss: 1.2629 (1.2148)  loss_mask: 0.0004 (0.0019)  time: 0.1710  data: 0.0002  max mem: 6052
[05:35:49.792373] Epoch: [67]  [240/781]  eta: 0:01:35  lr: 0.000067  training_loss: 1.2124 (1.2177)  classification_loss: 1.2121 (1.2159)  loss_mask: 0.0003 (0.0018)  time: 0.1710  data: 0.0002  max mem: 6052
[05:35:53.196905] Epoch: [67]  [260/781]  eta: 0:01:31  lr: 0.000067  training_loss: 1.1886 (1.2181)  classification_loss: 1.1883 (1.2164)  loss_mask: 0.0003 (0.0017)  time: 0.1701  data: 0.0003  max mem: 6052
[05:35:56.611332] Epoch: [67]  [280/781]  eta: 0:01:27  lr: 0.000067  training_loss: 1.2183 (1.2182)  classification_loss: 1.2181 (1.2166)  loss_mask: 0.0003 (0.0016)  time: 0.1706  data: 0.0003  max mem: 6052
[05:35:59.997515] Epoch: [67]  [300/781]  eta: 0:01:23  lr: 0.000067  training_loss: 1.2278 (1.2185)  classification_loss: 1.2275 (1.2170)  loss_mask: 0.0003 (0.0015)  time: 0.1692  data: 0.0003  max mem: 6052
[05:36:03.383344] Epoch: [67]  [320/781]  eta: 0:01:20  lr: 0.000067  training_loss: 1.2339 (1.2181)  classification_loss: 1.2335 (1.2167)  loss_mask: 0.0002 (0.0014)  time: 0.1692  data: 0.0003  max mem: 6052
[05:36:06.791257] Epoch: [67]  [340/781]  eta: 0:01:16  lr: 0.000066  training_loss: 1.1879 (1.2176)  classification_loss: 1.1877 (1.2163)  loss_mask: 0.0002 (0.0014)  time: 0.1703  data: 0.0003  max mem: 6052
[05:36:10.220038] Epoch: [67]  [360/781]  eta: 0:01:13  lr: 0.000066  training_loss: 1.1651 (1.2150)  classification_loss: 1.1617 (1.2129)  loss_mask: 0.0003 (0.0020)  time: 0.1714  data: 0.0003  max mem: 6052
[05:36:13.612055] Epoch: [67]  [380/781]  eta: 0:01:09  lr: 0.000066  training_loss: 1.1612 (1.2151)  classification_loss: 1.1609 (1.2130)  loss_mask: 0.0014 (0.0021)  time: 0.1695  data: 0.0002  max mem: 6052
[05:36:17.034940] Epoch: [67]  [400/781]  eta: 0:01:06  lr: 0.000066  training_loss: 1.2328 (1.2163)  classification_loss: 1.2321 (1.2142)  loss_mask: 0.0004 (0.0020)  time: 0.1711  data: 0.0003  max mem: 6052
[05:36:20.444741] Epoch: [67]  [420/781]  eta: 0:01:02  lr: 0.000066  training_loss: 1.1803 (1.2151)  classification_loss: 1.1799 (1.2131)  loss_mask: 0.0003 (0.0020)  time: 0.1704  data: 0.0002  max mem: 6052
[05:36:23.844311] Epoch: [67]  [440/781]  eta: 0:00:59  lr: 0.000066  training_loss: 1.2212 (1.2151)  classification_loss: 1.2211 (1.2132)  loss_mask: 0.0003 (0.0019)  time: 0.1699  data: 0.0002  max mem: 6052
[05:36:27.273170] Epoch: [67]  [460/781]  eta: 0:00:55  lr: 0.000066  training_loss: 1.1840 (1.2142)  classification_loss: 1.1833 (1.2123)  loss_mask: 0.0003 (0.0018)  time: 0.1714  data: 0.0004  max mem: 6052
[05:36:30.687146] Epoch: [67]  [480/781]  eta: 0:00:52  lr: 0.000066  training_loss: 1.2360 (1.2146)  classification_loss: 1.2357 (1.2128)  loss_mask: 0.0003 (0.0018)  time: 0.1706  data: 0.0002  max mem: 6052
[05:36:34.109239] Epoch: [67]  [500/781]  eta: 0:00:48  lr: 0.000066  training_loss: 1.2217 (1.2148)  classification_loss: 1.2213 (1.2131)  loss_mask: 0.0003 (0.0017)  time: 0.1710  data: 0.0003  max mem: 6052
[05:36:37.524416] Epoch: [67]  [520/781]  eta: 0:00:45  lr: 0.000066  training_loss: 1.1779 (1.2143)  classification_loss: 1.1776 (1.2126)  loss_mask: 0.0003 (0.0016)  time: 0.1707  data: 0.0002  max mem: 6052
[05:36:40.931942] Epoch: [67]  [540/781]  eta: 0:00:41  lr: 0.000066  training_loss: 1.2265 (1.2151)  classification_loss: 1.2263 (1.2135)  loss_mask: 0.0002 (0.0016)  time: 0.1703  data: 0.0002  max mem: 6052
[05:36:44.343340] Epoch: [67]  [560/781]  eta: 0:00:38  lr: 0.000065  training_loss: 1.1641 (1.2145)  classification_loss: 1.1638 (1.2130)  loss_mask: 0.0003 (0.0015)  time: 0.1705  data: 0.0003  max mem: 6052
[05:36:47.741630] Epoch: [67]  [580/781]  eta: 0:00:34  lr: 0.000065  training_loss: 1.1969 (1.2148)  classification_loss: 1.1850 (1.2131)  loss_mask: 0.0002 (0.0017)  time: 0.1698  data: 0.0003  max mem: 6052
[05:36:51.156638] Epoch: [67]  [600/781]  eta: 0:00:31  lr: 0.000065  training_loss: 1.1700 (1.2147)  classification_loss: 1.1677 (1.2129)  loss_mask: 0.0016 (0.0018)  time: 0.1707  data: 0.0002  max mem: 6052
[05:36:54.630225] Epoch: [67]  [620/781]  eta: 0:00:27  lr: 0.000065  training_loss: 1.2154 (1.2151)  classification_loss: 1.2151 (1.2133)  loss_mask: 0.0003 (0.0018)  time: 0.1736  data: 0.0002  max mem: 6052
[05:36:58.060504] Epoch: [67]  [640/781]  eta: 0:00:24  lr: 0.000065  training_loss: 1.1920 (1.2153)  classification_loss: 1.1911 (1.2136)  loss_mask: 0.0003 (0.0017)  time: 0.1714  data: 0.0002  max mem: 6052
[05:37:01.477063] Epoch: [67]  [660/781]  eta: 0:00:20  lr: 0.000065  training_loss: 1.2236 (1.2153)  classification_loss: 1.2233 (1.2136)  loss_mask: 0.0002 (0.0017)  time: 0.1708  data: 0.0002  max mem: 6052
[05:37:04.890368] Epoch: [67]  [680/781]  eta: 0:00:17  lr: 0.000065  training_loss: 1.2024 (1.2151)  classification_loss: 1.2023 (1.2134)  loss_mask: 0.0002 (0.0016)  time: 0.1706  data: 0.0002  max mem: 6052
[05:37:08.293260] Epoch: [67]  [700/781]  eta: 0:00:13  lr: 0.000065  training_loss: 1.2271 (1.2162)  classification_loss: 1.2269 (1.2146)  loss_mask: 0.0002 (0.0016)  time: 0.1699  data: 0.0002  max mem: 6052
[05:37:11.700924] Epoch: [67]  [720/781]  eta: 0:00:10  lr: 0.000065  training_loss: 1.2217 (1.2161)  classification_loss: 1.2215 (1.2146)  loss_mask: 0.0002 (0.0016)  time: 0.1702  data: 0.0002  max mem: 6052
[05:37:15.105493] Epoch: [67]  [740/781]  eta: 0:00:07  lr: 0.000065  training_loss: 1.1654 (1.2160)  classification_loss: 1.1652 (1.2145)  loss_mask: 0.0002 (0.0015)  time: 0.1701  data: 0.0002  max mem: 6052
[05:37:18.540139] Epoch: [67]  [760/781]  eta: 0:00:03  lr: 0.000065  training_loss: 1.2138 (1.2166)  classification_loss: 1.2137 (1.2151)  loss_mask: 0.0002 (0.0015)  time: 0.1716  data: 0.0002  max mem: 6052
[05:37:21.939529] Epoch: [67]  [780/781]  eta: 0:00:00  lr: 0.000064  training_loss: 1.2304 (1.2173)  classification_loss: 1.2304 (1.2158)  loss_mask: 0.0002 (0.0015)  time: 0.1699  data: 0.0002  max mem: 6052
[05:37:22.110704] Epoch: [67] Total time: 0:02:14 (0.1725 s / it)
[05:37:22.111173] Averaged stats: lr: 0.000064  training_loss: 1.2304 (1.2173)  classification_loss: 1.2304 (1.2158)  loss_mask: 0.0002 (0.0015)
[05:37:22.754247] Test:  [  0/157]  eta: 0:01:40  testing_loss: 0.4645 (0.4645)  acc1: 84.3750 (84.3750)  acc5: 100.0000 (100.0000)  time: 0.6387  data: 0.6032  max mem: 6052
[05:37:23.042458] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4951 (0.5245)  acc1: 82.8125 (83.2386)  acc5: 100.0000 (99.4318)  time: 0.0840  data: 0.0550  max mem: 6052
[05:37:23.326269] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4708 (0.4823)  acc1: 84.3750 (84.8958)  acc5: 100.0000 (99.5536)  time: 0.0284  data: 0.0003  max mem: 6052
[05:37:23.613068] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4652 (0.4998)  acc1: 85.9375 (84.5766)  acc5: 100.0000 (99.3448)  time: 0.0284  data: 0.0002  max mem: 6052
[05:37:23.895252] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5204 (0.5054)  acc1: 84.3750 (84.7942)  acc5: 98.4375 (99.3140)  time: 0.0283  data: 0.0001  max mem: 6052
[05:37:24.177763] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5110 (0.5030)  acc1: 85.9375 (84.8958)  acc5: 100.0000 (99.2953)  time: 0.0281  data: 0.0002  max mem: 6052
[05:37:24.457685] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4678 (0.5011)  acc1: 84.3750 (84.9898)  acc5: 100.0000 (99.2828)  time: 0.0280  data: 0.0002  max mem: 6052
[05:37:24.738365] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4818 (0.4967)  acc1: 84.3750 (85.1232)  acc5: 100.0000 (99.2958)  time: 0.0279  data: 0.0002  max mem: 6052
[05:37:25.027437] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4867 (0.5025)  acc1: 84.3750 (84.8765)  acc5: 100.0000 (99.2863)  time: 0.0284  data: 0.0002  max mem: 6052
[05:37:25.317136] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4742 (0.4985)  acc1: 82.8125 (84.8901)  acc5: 98.4375 (99.2617)  time: 0.0288  data: 0.0002  max mem: 6052
[05:37:25.605159] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4857 (0.5000)  acc1: 82.8125 (84.7927)  acc5: 100.0000 (99.2729)  time: 0.0287  data: 0.0002  max mem: 6052
[05:37:25.889820] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5016 (0.5018)  acc1: 82.8125 (84.7551)  acc5: 100.0000 (99.2821)  time: 0.0285  data: 0.0001  max mem: 6052
[05:37:26.177035] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4783 (0.4996)  acc1: 84.3750 (84.8011)  acc5: 100.0000 (99.2898)  time: 0.0285  data: 0.0001  max mem: 6052
[05:37:26.456695] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4875 (0.5005)  acc1: 85.9375 (84.8998)  acc5: 100.0000 (99.3082)  time: 0.0282  data: 0.0002  max mem: 6052
[05:37:26.735556] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4956 (0.4981)  acc1: 85.9375 (84.9623)  acc5: 100.0000 (99.3240)  time: 0.0278  data: 0.0001  max mem: 6052
[05:37:27.012889] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5112 (0.4992)  acc1: 82.8125 (84.8406)  acc5: 100.0000 (99.3067)  time: 0.0277  data: 0.0001  max mem: 6052
[05:37:27.163374] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4981 (0.4996)  acc1: 85.9375 (84.8800)  acc5: 100.0000 (99.3200)  time: 0.0268  data: 0.0001  max mem: 6052
[05:37:27.312145] Test: Total time: 0:00:05 (0.0331 s / it)
[05:37:27.312616] * Acc@1 84.880 Acc@5 99.320 loss 0.500
[05:37:27.312939] Accuracy of the network on the 10000 test images: 84.9%
[05:37:27.313236] Max accuracy: 84.88%
[05:37:27.618974] log_dir: ./output_dir
[05:37:28.486552] Epoch: [68]  [  0/781]  eta: 0:11:16  lr: 0.000064  training_loss: 1.1179 (1.1179)  classification_loss: 1.1177 (1.1177)  loss_mask: 0.0002 (0.0002)  time: 0.8660  data: 0.6493  max mem: 6052
[05:37:32.019828] Epoch: [68]  [ 20/781]  eta: 0:02:39  lr: 0.000064  training_loss: 1.2105 (1.2305)  classification_loss: 1.2104 (1.2303)  loss_mask: 0.0002 (0.0002)  time: 0.1766  data: 0.0003  max mem: 6052
[05:37:35.439364] Epoch: [68]  [ 40/781]  eta: 0:02:21  lr: 0.000064  training_loss: 1.2263 (1.2289)  classification_loss: 1.2260 (1.2287)  loss_mask: 0.0002 (0.0002)  time: 0.1709  data: 0.0002  max mem: 6052
[05:37:38.863855] Epoch: [68]  [ 60/781]  eta: 0:02:12  lr: 0.000064  training_loss: 1.2580 (1.2382)  classification_loss: 1.2579 (1.2380)  loss_mask: 0.0001 (0.0002)  time: 0.1711  data: 0.0002  max mem: 6052
[05:37:42.297322] Epoch: [68]  [ 80/781]  eta: 0:02:06  lr: 0.000064  training_loss: 1.1883 (1.2269)  classification_loss: 1.1881 (1.2267)  loss_mask: 0.0002 (0.0002)  time: 0.1716  data: 0.0001  max mem: 6052
[05:37:45.714528] Epoch: [68]  [100/781]  eta: 0:02:01  lr: 0.000064  training_loss: 1.2164 (1.2236)  classification_loss: 1.2162 (1.2234)  loss_mask: 0.0002 (0.0002)  time: 0.1708  data: 0.0002  max mem: 6052
[05:37:49.145538] Epoch: [68]  [120/781]  eta: 0:01:57  lr: 0.000064  training_loss: 1.1992 (1.2230)  classification_loss: 1.1990 (1.2228)  loss_mask: 0.0002 (0.0002)  time: 0.1715  data: 0.0003  max mem: 6052
[05:37:52.563922] Epoch: [68]  [140/781]  eta: 0:01:53  lr: 0.000064  training_loss: 1.1849 (1.2213)  classification_loss: 1.1847 (1.2211)  loss_mask: 0.0002 (0.0002)  time: 0.1708  data: 0.0002  max mem: 6052
[05:37:55.987899] Epoch: [68]  [160/781]  eta: 0:01:49  lr: 0.000064  training_loss: 1.2423 (1.2246)  classification_loss: 1.2422 (1.2244)  loss_mask: 0.0002 (0.0002)  time: 0.1711  data: 0.0002  max mem: 6052
[05:37:59.396183] Epoch: [68]  [180/781]  eta: 0:01:45  lr: 0.000064  training_loss: 1.2615 (1.2259)  classification_loss: 1.2613 (1.2257)  loss_mask: 0.0001 (0.0002)  time: 0.1703  data: 0.0001  max mem: 6052
[05:38:02.810609] Epoch: [68]  [200/781]  eta: 0:01:41  lr: 0.000064  training_loss: 1.1930 (1.2233)  classification_loss: 1.1928 (1.2231)  loss_mask: 0.0002 (0.0002)  time: 0.1706  data: 0.0002  max mem: 6052
[05:38:06.236823] Epoch: [68]  [220/781]  eta: 0:01:37  lr: 0.000063  training_loss: 1.2398 (1.2224)  classification_loss: 1.2396 (1.2222)  loss_mask: 0.0001 (0.0002)  time: 0.1712  data: 0.0002  max mem: 6052
[05:38:09.657823] Epoch: [68]  [240/781]  eta: 0:01:34  lr: 0.000063  training_loss: 1.2307 (1.2211)  classification_loss: 1.2306 (1.2209)  loss_mask: 0.0001 (0.0002)  time: 0.1710  data: 0.0003  max mem: 6052
[05:38:13.099803] Epoch: [68]  [260/781]  eta: 0:01:30  lr: 0.000063  training_loss: 1.1821 (1.2189)  classification_loss: 1.1820 (1.2187)  loss_mask: 0.0002 (0.0002)  time: 0.1720  data: 0.0002  max mem: 6052
[05:38:16.510699] Epoch: [68]  [280/781]  eta: 0:01:27  lr: 0.000063  training_loss: 1.2530 (1.2217)  classification_loss: 1.2528 (1.2215)  loss_mask: 0.0001 (0.0002)  time: 0.1705  data: 0.0001  max mem: 6052
[05:38:20.006928] Epoch: [68]  [300/781]  eta: 0:01:23  lr: 0.000063  training_loss: 1.2030 (1.2197)  classification_loss: 1.2030 (1.2195)  loss_mask: 0.0002 (0.0002)  time: 0.1747  data: 0.0003  max mem: 6052
[05:38:23.423789] Epoch: [68]  [320/781]  eta: 0:01:20  lr: 0.000063  training_loss: 1.1601 (1.2198)  classification_loss: 1.1599 (1.2196)  loss_mask: 0.0001 (0.0002)  time: 0.1708  data: 0.0002  max mem: 6052
[05:38:26.825999] Epoch: [68]  [340/781]  eta: 0:01:16  lr: 0.000063  training_loss: 1.2141 (1.2184)  classification_loss: 1.2140 (1.2182)  loss_mask: 0.0001 (0.0002)  time: 0.1700  data: 0.0002  max mem: 6052
[05:38:30.241942] Epoch: [68]  [360/781]  eta: 0:01:12  lr: 0.000063  training_loss: 1.2428 (1.2195)  classification_loss: 1.2427 (1.2193)  loss_mask: 0.0001 (0.0002)  time: 0.1707  data: 0.0003  max mem: 6052
[05:38:33.686558] Epoch: [68]  [380/781]  eta: 0:01:09  lr: 0.000063  training_loss: 1.2235 (1.2198)  classification_loss: 1.2233 (1.2196)  loss_mask: 0.0001 (0.0002)  time: 0.1722  data: 0.0002  max mem: 6052
[05:38:37.111351] Epoch: [68]  [400/781]  eta: 0:01:05  lr: 0.000063  training_loss: 1.2221 (1.2202)  classification_loss: 1.2220 (1.2195)  loss_mask: 0.0006 (0.0006)  time: 0.1712  data: 0.0002  max mem: 6052
[05:38:40.519783] Epoch: [68]  [420/781]  eta: 0:01:02  lr: 0.000063  training_loss: 1.2702 (1.2221)  classification_loss: 1.2700 (1.2214)  loss_mask: 0.0003 (0.0007)  time: 0.1703  data: 0.0002  max mem: 6052
[05:38:43.932088] Epoch: [68]  [440/781]  eta: 0:00:58  lr: 0.000062  training_loss: 1.2256 (1.2224)  classification_loss: 1.2255 (1.2217)  loss_mask: 0.0003 (0.0007)  time: 0.1705  data: 0.0002  max mem: 6052
[05:38:47.395174] Epoch: [68]  [460/781]  eta: 0:00:55  lr: 0.000062  training_loss: 1.1831 (1.2212)  classification_loss: 1.1830 (1.2206)  loss_mask: 0.0002 (0.0007)  time: 0.1731  data: 0.0002  max mem: 6052
[05:38:50.817037] Epoch: [68]  [480/781]  eta: 0:00:52  lr: 0.000062  training_loss: 1.2082 (1.2202)  classification_loss: 1.2080 (1.2196)  loss_mask: 0.0001 (0.0006)  time: 0.1710  data: 0.0001  max mem: 6052
[05:38:54.227149] Epoch: [68]  [500/781]  eta: 0:00:48  lr: 0.000062  training_loss: 1.2160 (1.2201)  classification_loss: 1.2158 (1.2194)  loss_mask: 0.0004 (0.0007)  time: 0.1704  data: 0.0002  max mem: 6052
[05:38:57.660988] Epoch: [68]  [520/781]  eta: 0:00:45  lr: 0.000062  training_loss: 1.1615 (1.2188)  classification_loss: 1.1614 (1.2182)  loss_mask: 0.0002 (0.0006)  time: 0.1716  data: 0.0002  max mem: 6052
[05:39:01.093442] Epoch: [68]  [540/781]  eta: 0:00:41  lr: 0.000062  training_loss: 1.2298 (1.2201)  classification_loss: 1.2297 (1.2195)  loss_mask: 0.0001 (0.0006)  time: 0.1716  data: 0.0002  max mem: 6052
[05:39:04.520639] Epoch: [68]  [560/781]  eta: 0:00:38  lr: 0.000062  training_loss: 1.2723 (1.2211)  classification_loss: 1.2579 (1.2199)  loss_mask: 0.0001 (0.0012)  time: 0.1713  data: 0.0004  max mem: 6052
[05:39:07.930051] Epoch: [68]  [580/781]  eta: 0:00:34  lr: 0.000062  training_loss: 1.2299 (1.2212)  classification_loss: 1.2299 (1.2201)  loss_mask: 0.0002 (0.0011)  time: 0.1704  data: 0.0002  max mem: 6052
[05:39:11.352321] Epoch: [68]  [600/781]  eta: 0:00:31  lr: 0.000062  training_loss: 1.2176 (1.2213)  classification_loss: 1.2175 (1.2202)  loss_mask: 0.0001 (0.0011)  time: 0.1710  data: 0.0002  max mem: 6052
[05:39:14.789582] Epoch: [68]  [620/781]  eta: 0:00:27  lr: 0.000062  training_loss: 1.1929 (1.2209)  classification_loss: 1.1928 (1.2198)  loss_mask: 0.0001 (0.0011)  time: 0.1718  data: 0.0001  max mem: 6052
[05:39:18.205889] Epoch: [68]  [640/781]  eta: 0:00:24  lr: 0.000062  training_loss: 1.2026 (1.2198)  classification_loss: 1.2025 (1.2188)  loss_mask: 0.0001 (0.0010)  time: 0.1707  data: 0.0003  max mem: 6052
[05:39:21.626901] Epoch: [68]  [660/781]  eta: 0:00:20  lr: 0.000061  training_loss: 1.2241 (1.2202)  classification_loss: 1.2240 (1.2192)  loss_mask: 0.0001 (0.0010)  time: 0.1710  data: 0.0002  max mem: 6052
[05:39:25.050341] Epoch: [68]  [680/781]  eta: 0:00:17  lr: 0.000061  training_loss: 1.1974 (1.2201)  classification_loss: 1.1973 (1.2191)  loss_mask: 0.0001 (0.0010)  time: 0.1711  data: 0.0002  max mem: 6052
[05:39:28.476055] Epoch: [68]  [700/781]  eta: 0:00:13  lr: 0.000061  training_loss: 1.2003 (1.2202)  classification_loss: 1.2002 (1.2192)  loss_mask: 0.0001 (0.0010)  time: 0.1712  data: 0.0002  max mem: 6052
[05:39:31.904008] Epoch: [68]  [720/781]  eta: 0:00:10  lr: 0.000061  training_loss: 1.2499 (1.2211)  classification_loss: 1.2497 (1.2201)  loss_mask: 0.0001 (0.0009)  time: 0.1713  data: 0.0001  max mem: 6052
[05:39:35.323376] Epoch: [68]  [740/781]  eta: 0:00:07  lr: 0.000061  training_loss: 1.1850 (1.2203)  classification_loss: 1.1849 (1.2193)  loss_mask: 0.0001 (0.0009)  time: 0.1709  data: 0.0001  max mem: 6052
[05:39:38.727006] Epoch: [68]  [760/781]  eta: 0:00:03  lr: 0.000061  training_loss: 1.2107 (1.2206)  classification_loss: 1.2103 (1.2197)  loss_mask: 0.0001 (0.0009)  time: 0.1701  data: 0.0002  max mem: 6052
[05:39:42.129669] Epoch: [68]  [780/781]  eta: 0:00:00  lr: 0.000061  training_loss: 1.1932 (1.2204)  classification_loss: 1.1930 (1.2196)  loss_mask: 0.0001 (0.0009)  time: 0.1701  data: 0.0001  max mem: 6052
[05:39:42.306355] Epoch: [68] Total time: 0:02:14 (0.1725 s / it)
[05:39:42.307224] Averaged stats: lr: 0.000061  training_loss: 1.1932 (1.2204)  classification_loss: 1.1930 (1.2196)  loss_mask: 0.0001 (0.0009)
[05:39:42.972664] Test:  [  0/157]  eta: 0:01:43  testing_loss: 0.4674 (0.4674)  acc1: 87.5000 (87.5000)  acc5: 96.8750 (96.8750)  time: 0.6609  data: 0.6303  max mem: 6052
[05:39:43.255966] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5454 (0.5388)  acc1: 84.3750 (83.9489)  acc5: 98.4375 (98.8636)  time: 0.0857  data: 0.0575  max mem: 6052
[05:39:43.536094] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.5034 (0.4963)  acc1: 85.9375 (85.9375)  acc5: 100.0000 (99.2560)  time: 0.0280  data: 0.0002  max mem: 6052
[05:39:43.816476] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4730 (0.5055)  acc1: 87.5000 (85.2319)  acc5: 100.0000 (99.2440)  time: 0.0279  data: 0.0001  max mem: 6052
[05:39:44.097409] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4860 (0.5103)  acc1: 84.3750 (85.0991)  acc5: 98.4375 (99.1616)  time: 0.0279  data: 0.0002  max mem: 6052
[05:39:44.378314] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4960 (0.5078)  acc1: 85.9375 (85.0797)  acc5: 98.4375 (99.1422)  time: 0.0280  data: 0.0002  max mem: 6052
[05:39:44.664677] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4878 (0.5047)  acc1: 85.9375 (85.1691)  acc5: 100.0000 (99.1291)  time: 0.0283  data: 0.0001  max mem: 6052
[05:39:44.952775] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4833 (0.5008)  acc1: 85.9375 (85.3213)  acc5: 100.0000 (99.1417)  time: 0.0286  data: 0.0002  max mem: 6052
[05:39:45.236874] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5169 (0.5062)  acc1: 84.3750 (84.9923)  acc5: 100.0000 (99.1705)  time: 0.0285  data: 0.0002  max mem: 6052
[05:39:45.520497] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5079 (0.5037)  acc1: 84.3750 (85.0446)  acc5: 100.0000 (99.1758)  time: 0.0283  data: 0.0002  max mem: 6052
[05:39:45.801187] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5079 (0.5093)  acc1: 82.8125 (84.6999)  acc5: 100.0000 (99.2110)  time: 0.0281  data: 0.0002  max mem: 6052
[05:39:46.085251] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5139 (0.5100)  acc1: 82.8125 (84.6425)  acc5: 100.0000 (99.2258)  time: 0.0281  data: 0.0002  max mem: 6052
[05:39:46.369415] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4676 (0.5075)  acc1: 85.9375 (84.7366)  acc5: 100.0000 (99.2252)  time: 0.0283  data: 0.0002  max mem: 6052
[05:39:46.649877] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4917 (0.5086)  acc1: 85.9375 (84.7090)  acc5: 98.4375 (99.1889)  time: 0.0281  data: 0.0002  max mem: 6052
[05:39:46.929843] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4763 (0.5051)  acc1: 85.9375 (84.8293)  acc5: 98.4375 (99.2132)  time: 0.0279  data: 0.0001  max mem: 6052
[05:39:47.207673] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4881 (0.5046)  acc1: 84.3750 (84.7993)  acc5: 100.0000 (99.2239)  time: 0.0278  data: 0.0001  max mem: 6052
[05:39:47.356768] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4881 (0.5049)  acc1: 84.3750 (84.8100)  acc5: 100.0000 (99.2300)  time: 0.0268  data: 0.0001  max mem: 6052
[05:39:47.515080] Test: Total time: 0:00:05 (0.0332 s / it)
[05:39:47.515813] * Acc@1 84.810 Acc@5 99.230 loss 0.505
[05:39:47.516167] Accuracy of the network on the 10000 test images: 84.8%
[05:39:47.516402] Max accuracy: 84.88%
[05:39:47.810732] log_dir: ./output_dir
[05:39:48.709134] Epoch: [69]  [  0/781]  eta: 0:11:40  lr: 0.000061  training_loss: 1.1034 (1.1034)  classification_loss: 1.1032 (1.1032)  loss_mask: 0.0001 (0.0001)  time: 0.8966  data: 0.7208  max mem: 6052
[05:39:52.120971] Epoch: [69]  [ 20/781]  eta: 0:02:36  lr: 0.000061  training_loss: 1.2078 (1.2143)  classification_loss: 1.2077 (1.2141)  loss_mask: 0.0001 (0.0001)  time: 0.1705  data: 0.0001  max mem: 6052
[05:39:55.540122] Epoch: [69]  [ 40/781]  eta: 0:02:19  lr: 0.000061  training_loss: 1.2195 (1.2178)  classification_loss: 1.2194 (1.2177)  loss_mask: 0.0001 (0.0001)  time: 0.1709  data: 0.0002  max mem: 6052
[05:39:58.962447] Epoch: [69]  [ 60/781]  eta: 0:02:11  lr: 0.000061  training_loss: 1.1931 (1.2190)  classification_loss: 1.1930 (1.2189)  loss_mask: 0.0001 (0.0001)  time: 0.1710  data: 0.0002  max mem: 6052
[05:40:02.396388] Epoch: [69]  [ 80/781]  eta: 0:02:06  lr: 0.000061  training_loss: 1.1791 (1.2142)  classification_loss: 1.1790 (1.2140)  loss_mask: 0.0001 (0.0001)  time: 0.1716  data: 0.0003  max mem: 6052
[05:40:05.818962] Epoch: [69]  [100/781]  eta: 0:02:01  lr: 0.000060  training_loss: 1.1987 (1.2121)  classification_loss: 1.1987 (1.2120)  loss_mask: 0.0001 (0.0001)  time: 0.1711  data: 0.0003  max mem: 6052
[05:40:09.238252] Epoch: [69]  [120/781]  eta: 0:01:56  lr: 0.000060  training_loss: 1.2156 (1.2122)  classification_loss: 1.2154 (1.2121)  loss_mask: 0.0001 (0.0001)  time: 0.1709  data: 0.0002  max mem: 6052
[05:40:12.763562] Epoch: [69]  [140/781]  eta: 0:01:53  lr: 0.000060  training_loss: 1.1822 (1.2104)  classification_loss: 1.1822 (1.2103)  loss_mask: 0.0001 (0.0001)  time: 0.1762  data: 0.0002  max mem: 6052
[05:40:16.184094] Epoch: [69]  [160/781]  eta: 0:01:49  lr: 0.000060  training_loss: 1.1696 (1.2080)  classification_loss: 1.1695 (1.2078)  loss_mask: 0.0001 (0.0001)  time: 0.1710  data: 0.0002  max mem: 6052
[05:40:19.646612] Epoch: [69]  [180/781]  eta: 0:01:45  lr: 0.000060  training_loss: 1.1956 (1.2057)  classification_loss: 1.1955 (1.2055)  loss_mask: 0.0001 (0.0001)  time: 0.1730  data: 0.0003  max mem: 6052
[05:40:23.105067] Epoch: [69]  [200/781]  eta: 0:01:41  lr: 0.000060  training_loss: 1.2263 (1.2079)  classification_loss: 1.2261 (1.2078)  loss_mask: 0.0001 (0.0001)  time: 0.1729  data: 0.0003  max mem: 6052
[05:40:26.506632] Epoch: [69]  [220/781]  eta: 0:01:38  lr: 0.000060  training_loss: 1.2032 (1.2054)  classification_loss: 1.2031 (1.2052)  loss_mask: 0.0001 (0.0001)  time: 0.1700  data: 0.0001  max mem: 6052
[05:40:29.911342] Epoch: [69]  [240/781]  eta: 0:01:34  lr: 0.000060  training_loss: 1.2336 (1.2060)  classification_loss: 1.2335 (1.2059)  loss_mask: 0.0001 (0.0001)  time: 0.1702  data: 0.0002  max mem: 6052
[05:40:33.353917] Epoch: [69]  [260/781]  eta: 0:01:30  lr: 0.000060  training_loss: 1.1705 (1.2059)  classification_loss: 1.1705 (1.2058)  loss_mask: 0.0001 (0.0001)  time: 0.1721  data: 0.0002  max mem: 6052
[05:40:36.779629] Epoch: [69]  [280/781]  eta: 0:01:27  lr: 0.000060  training_loss: 1.2244 (1.2071)  classification_loss: 1.2243 (1.2070)  loss_mask: 0.0001 (0.0001)  time: 0.1712  data: 0.0002  max mem: 6052
[05:40:40.199366] Epoch: [69]  [300/781]  eta: 0:01:23  lr: 0.000060  training_loss: 1.2136 (1.2068)  classification_loss: 1.2135 (1.2066)  loss_mask: 0.0001 (0.0001)  time: 0.1709  data: 0.0003  max mem: 6052
[05:40:43.607576] Epoch: [69]  [320/781]  eta: 0:01:20  lr: 0.000059  training_loss: 1.2094 (1.2066)  classification_loss: 1.2093 (1.2065)  loss_mask: 0.0001 (0.0001)  time: 0.1703  data: 0.0003  max mem: 6052
[05:40:47.026259] Epoch: [69]  [340/781]  eta: 0:01:16  lr: 0.000059  training_loss: 1.1668 (1.2060)  classification_loss: 1.1667 (1.2059)  loss_mask: 0.0001 (0.0001)  time: 0.1709  data: 0.0002  max mem: 6052
[05:40:50.447999] Epoch: [69]  [360/781]  eta: 0:01:13  lr: 0.000059  training_loss: 1.2443 (1.2081)  classification_loss: 1.2442 (1.2080)  loss_mask: 0.0001 (0.0001)  time: 0.1710  data: 0.0002  max mem: 6052
[05:40:53.865975] Epoch: [69]  [380/781]  eta: 0:01:09  lr: 0.000059  training_loss: 1.2294 (1.2111)  classification_loss: 1.2292 (1.2110)  loss_mask: 0.0001 (0.0001)  time: 0.1708  data: 0.0003  max mem: 6052
[05:40:57.280209] Epoch: [69]  [400/781]  eta: 0:01:05  lr: 0.000059  training_loss: 1.2066 (1.2115)  classification_loss: 1.2065 (1.2114)  loss_mask: 0.0001 (0.0001)  time: 0.1706  data: 0.0004  max mem: 6052

[05:41:00.693042] Epoch: [69]  [420/781]  eta: 0:01:02  lr: 0.000059  training_loss: 1.1860 (1.2107)  classification_loss: 1.1859 (1.2106)  loss_mask: 0.0001 (0.0001)  time: 0.1706  data: 0.0003  max mem: 6052
[05:41:04.113773] Epoch: [69]  [440/781]  eta: 0:00:58  lr: 0.000059  training_loss: 1.2011 (1.2098)  classification_loss: 1.2010 (1.2097)  loss_mask: 0.0001 (0.0001)  time: 0.1710  data: 0.0003  max mem: 6052
[05:41:07.538721] Epoch: [69]  [460/781]  eta: 0:00:55  lr: 0.000059  training_loss: 1.1769 (1.2090)  classification_loss: 1.1767 (1.2088)  loss_mask: 0.0001 (0.0001)  time: 0.1712  data: 0.0002  max mem: 6052
[05:41:10.966266] Epoch: [69]  [480/781]  eta: 0:00:52  lr: 0.000059  training_loss: 1.2028 (1.2100)  classification_loss: 1.2027 (1.2099)  loss_mask: 0.0001 (0.0001)  time: 0.1713  data: 0.0001  max mem: 6052
[05:41:14.400660] Epoch: [69]  [500/781]  eta: 0:00:48  lr: 0.000059  training_loss: 1.2174 (1.2111)  classification_loss: 1.2173 (1.2110)  loss_mask: 0.0001 (0.0001)  time: 0.1717  data: 0.0002  max mem: 6052
[05:41:17.818891] Epoch: [69]  [520/781]  eta: 0:00:45  lr: 0.000059  training_loss: 1.2179 (1.2110)  classification_loss: 1.2178 (1.2109)  loss_mask: 0.0001 (0.0001)  time: 0.1708  data: 0.0002  max mem: 6052
[05:41:21.220055] Epoch: [69]  [540/781]  eta: 0:00:41  lr: 0.000058  training_loss: 1.1911 (1.2107)  classification_loss: 1.1911 (1.2106)  loss_mask: 0.0001 (0.0001)  time: 0.1700  data: 0.0001  max mem: 6052
[05:41:24.648695] Epoch: [69]  [560/781]  eta: 0:00:38  lr: 0.000058  training_loss: 1.1824 (1.2099)  classification_loss: 1.1823 (1.2098)  loss_mask: 0.0001 (0.0001)  time: 0.1714  data: 0.0003  max mem: 6052
[05:41:28.095621] Epoch: [69]  [580/781]  eta: 0:00:34  lr: 0.000058  training_loss: 1.2328 (1.2109)  classification_loss: 1.2327 (1.2108)  loss_mask: 0.0001 (0.0001)  time: 0.1723  data: 0.0003  max mem: 6052
[05:41:31.534766] Epoch: [69]  [600/781]  eta: 0:00:31  lr: 0.000058  training_loss: 1.2531 (1.2113)  classification_loss: 1.2531 (1.2112)  loss_mask: 0.0001 (0.0001)  time: 0.1719  data: 0.0002  max mem: 6052
[05:41:34.952297] Epoch: [69]  [620/781]  eta: 0:00:27  lr: 0.000058  training_loss: 1.2399 (1.2119)  classification_loss: 1.2399 (1.2118)  loss_mask: 0.0001 (0.0001)  time: 0.1708  data: 0.0003  max mem: 6052
[05:41:38.369976] Epoch: [69]  [640/781]  eta: 0:00:24  lr: 0.000058  training_loss: 1.1682 (1.2109)  classification_loss: 1.1682 (1.2108)  loss_mask: 0.0001 (0.0001)  time: 0.1708  data: 0.0003  max mem: 6052
[05:41:41.779880] Epoch: [69]  [660/781]  eta: 0:00:20  lr: 0.000058  training_loss: 1.2046 (1.2117)  classification_loss: 1.2046 (1.2116)  loss_mask: 0.0001 (0.0001)  time: 0.1704  data: 0.0001  max mem: 6052
[05:41:45.200434] Epoch: [69]  [680/781]  eta: 0:00:17  lr: 0.000058  training_loss: 1.2505 (1.2119)  classification_loss: 1.2504 (1.2118)  loss_mask: 0.0001 (0.0001)  time: 0.1710  data: 0.0002  max mem: 6052
[05:41:48.614573] Epoch: [69]  [700/781]  eta: 0:00:13  lr: 0.000058  training_loss: 1.2310 (1.2126)  classification_loss: 1.2309 (1.2124)  loss_mask: 0.0001 (0.0001)  time: 0.1706  data: 0.0002  max mem: 6052
[05:41:52.026768] Epoch: [69]  [720/781]  eta: 0:00:10  lr: 0.000058  training_loss: 1.2312 (1.2132)  classification_loss: 1.2310 (1.2131)  loss_mask: 0.0001 (0.0001)  time: 0.1705  data: 0.0002  max mem: 6052
[05:41:55.438978] Epoch: [69]  [740/781]  eta: 0:00:07  lr: 0.000058  training_loss: 1.1966 (1.2130)  classification_loss: 1.1965 (1.2129)  loss_mask: 0.0001 (0.0001)  time: 0.1705  data: 0.0002  max mem: 6052
[05:41:58.860285] Epoch: [69]  [760/781]  eta: 0:00:03  lr: 0.000057  training_loss: 1.2343 (1.2134)  classification_loss: 1.2341 (1.2133)  loss_mask: 0.0001 (0.0001)  time: 0.1710  data: 0.0002  max mem: 6052
[05:42:02.259574] Epoch: [69]  [780/781]  eta: 0:00:00  lr: 0.000057  training_loss: 1.2288 (1.2147)  classification_loss: 1.2287 (1.2146)  loss_mask: 0.0001 (0.0001)  time: 0.1699  data: 0.0001  max mem: 6052
[05:42:02.426670] Epoch: [69] Total time: 0:02:14 (0.1724 s / it)
[05:42:02.427392] Averaged stats: lr: 0.000057  training_loss: 1.2288 (1.2147)  classification_loss: 1.2287 (1.2146)  loss_mask: 0.0001 (0.0001)
[05:42:03.105289] Test:  [  0/157]  eta: 0:01:45  testing_loss: 0.4810 (0.4810)  acc1: 84.3750 (84.3750)  acc5: 100.0000 (100.0000)  time: 0.6741  data: 0.6427  max mem: 6052
[05:42:03.387685] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5195 (0.5295)  acc1: 82.8125 (82.3864)  acc5: 100.0000 (99.7159)  time: 0.0867  data: 0.0586  max mem: 6052
[05:42:03.668872] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4930 (0.4843)  acc1: 84.3750 (84.8214)  acc5: 100.0000 (99.7024)  time: 0.0280  data: 0.0002  max mem: 6052
[05:42:03.951131] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4780 (0.4978)  acc1: 84.3750 (84.3750)  acc5: 100.0000 (99.5464)  time: 0.0280  data: 0.0002  max mem: 6052
[05:42:04.249351] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5113 (0.5017)  acc1: 84.3750 (84.4893)  acc5: 98.4375 (99.4284)  time: 0.0289  data: 0.0003  max mem: 6052
[05:42:04.529773] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4764 (0.4995)  acc1: 85.9375 (84.8652)  acc5: 98.4375 (99.3260)  time: 0.0288  data: 0.0002  max mem: 6052
[05:42:04.812318] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4740 (0.4955)  acc1: 85.9375 (85.0410)  acc5: 100.0000 (99.3084)  time: 0.0280  data: 0.0002  max mem: 6052
[05:42:05.093523] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4755 (0.4915)  acc1: 85.9375 (85.1232)  acc5: 100.0000 (99.3618)  time: 0.0281  data: 0.0003  max mem: 6052
[05:42:05.373767] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4883 (0.4980)  acc1: 84.3750 (84.8765)  acc5: 100.0000 (99.3441)  time: 0.0279  data: 0.0002  max mem: 6052
[05:42:05.654408] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4822 (0.4941)  acc1: 84.3750 (85.1305)  acc5: 100.0000 (99.3647)  time: 0.0279  data: 0.0001  max mem: 6052
[05:42:05.934849] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4784 (0.4980)  acc1: 84.3750 (84.9010)  acc5: 100.0000 (99.3812)  time: 0.0279  data: 0.0002  max mem: 6052
[05:42:06.215274] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4828 (0.4976)  acc1: 82.8125 (84.8818)  acc5: 100.0000 (99.4229)  time: 0.0279  data: 0.0002  max mem: 6052
[05:42:06.495981] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4624 (0.4962)  acc1: 84.3750 (84.9174)  acc5: 100.0000 (99.4060)  time: 0.0279  data: 0.0001  max mem: 6052
[05:42:06.776811] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4930 (0.4978)  acc1: 85.9375 (84.9117)  acc5: 100.0000 (99.4156)  time: 0.0280  data: 0.0002  max mem: 6052
[05:42:07.061004] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4925 (0.4954)  acc1: 85.9375 (85.0953)  acc5: 100.0000 (99.4127)  time: 0.0281  data: 0.0001  max mem: 6052
[05:42:07.338622] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4905 (0.4942)  acc1: 85.9375 (85.0993)  acc5: 100.0000 (99.4205)  time: 0.0280  data: 0.0001  max mem: 6052
[05:42:07.488434] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4774 (0.4943)  acc1: 85.9375 (85.1100)  acc5: 100.0000 (99.4400)  time: 0.0268  data: 0.0001  max mem: 6052
[05:42:07.666735] Test: Total time: 0:00:05 (0.0334 s / it)
[05:42:07.668033] * Acc@1 85.110 Acc@5 99.440 loss 0.494
[05:42:07.668770] Accuracy of the network on the 10000 test images: 85.1%
[05:42:07.669464] Max accuracy: 85.11%
[05:42:07.992247] log_dir: ./output_dir
[05:42:08.879696] Epoch: [70]  [  0/781]  eta: 0:11:31  lr: 0.000057  training_loss: 1.0366 (1.0366)  classification_loss: 1.0365 (1.0365)  loss_mask: 0.0000 (0.0000)  time: 0.8856  data: 0.6872  max mem: 6052
[05:42:12.383196] Epoch: [70]  [ 20/781]  eta: 0:02:38  lr: 0.000057  training_loss: 1.1733 (1.1838)  classification_loss: 1.1732 (1.1837)  loss_mask: 0.0001 (0.0001)  time: 0.1751  data: 0.0002  max mem: 6052
[05:42:15.789353] Epoch: [70]  [ 40/781]  eta: 0:02:20  lr: 0.000057  training_loss: 1.1800 (1.1799)  classification_loss: 1.1799 (1.1798)  loss_mask: 0.0001 (0.0001)  time: 0.1702  data: 0.0002  max mem: 6052
[05:42:19.227983] Epoch: [70]  [ 60/781]  eta: 0:02:12  lr: 0.000057  training_loss: 1.2058 (1.1936)  classification_loss: 1.2058 (1.1935)  loss_mask: 0.0001 (0.0001)  time: 0.1719  data: 0.0003  max mem: 6052
[05:42:22.694076] Epoch: [70]  [ 80/781]  eta: 0:02:07  lr: 0.000057  training_loss: 1.1893 (1.1918)  classification_loss: 1.1891 (1.1917)  loss_mask: 0.0001 (0.0001)  time: 0.1732  data: 0.0003  max mem: 6052
[05:42:26.117441] Epoch: [70]  [100/781]  eta: 0:02:02  lr: 0.000057  training_loss: 1.2135 (1.1930)  classification_loss: 1.2135 (1.1929)  loss_mask: 0.0001 (0.0001)  time: 0.1711  data: 0.0003  max mem: 6052
[05:42:29.540205] Epoch: [70]  [120/781]  eta: 0:01:57  lr: 0.000057  training_loss: 1.1976 (1.1943)  classification_loss: 1.1976 (1.1943)  loss_mask: 0.0001 (0.0001)  time: 0.1711  data: 0.0004  max mem: 6052
[05:42:32.964383] Epoch: [70]  [140/781]  eta: 0:01:53  lr: 0.000057  training_loss: 1.1572 (1.1918)  classification_loss: 1.1571 (1.1917)  loss_mask: 0.0001 (0.0001)  time: 0.1711  data: 0.0002  max mem: 6052
[05:42:36.371673] Epoch: [70]  [160/781]  eta: 0:01:49  lr: 0.000057  training_loss: 1.1894 (1.1946)  classification_loss: 1.1893 (1.1945)  loss_mask: 0.0001 (0.0001)  time: 0.1703  data: 0.0002  max mem: 6052
[05:42:39.773003] Epoch: [70]  [180/781]  eta: 0:01:45  lr: 0.000057  training_loss: 1.1738 (1.1958)  classification_loss: 1.1736 (1.1957)  loss_mask: 0.0001 (0.0001)  time: 0.1700  data: 0.0002  max mem: 6052
[05:42:43.170696] Epoch: [70]  [200/781]  eta: 0:01:41  lr: 0.000057  training_loss: 1.1472 (1.1931)  classification_loss: 1.1471 (1.1930)  loss_mask: 0.0001 (0.0001)  time: 0.1698  data: 0.0002  max mem: 6052
[05:42:46.571378] Epoch: [70]  [220/781]  eta: 0:01:37  lr: 0.000056  training_loss: 1.2207 (1.1988)  classification_loss: 1.1835 (1.1929)  loss_mask: 0.0001 (0.0059)  time: 0.1700  data: 0.0002  max mem: 6052
[05:42:49.975261] Epoch: [70]  [240/781]  eta: 0:01:34  lr: 0.000056  training_loss: 1.2452 (1.2038)  classification_loss: 1.1667 (1.1922)  loss_mask: 0.0534 (0.0116)  time: 0.1701  data: 0.0002  max mem: 6052
[05:42:53.388315] Epoch: [70]  [260/781]  eta: 0:01:30  lr: 0.000056  training_loss: 1.2647 (1.2091)  classification_loss: 1.1920 (1.1931)  loss_mask: 0.0529 (0.0160)  time: 0.1706  data: 0.0002  max mem: 6052
[05:42:56.799147] Epoch: [70]  [280/781]  eta: 0:01:26  lr: 0.000056  training_loss: 1.1684 (1.2064)  classification_loss: 1.1621 (1.1909)  loss_mask: 0.0047 (0.0155)  time: 0.1705  data: 0.0002  max mem: 6052
[05:43:00.198162] Epoch: [70]  [300/781]  eta: 0:01:23  lr: 0.000056  training_loss: 1.2232 (1.2096)  classification_loss: 1.1747 (1.1915)  loss_mask: 0.0073 (0.0181)  time: 0.1699  data: 0.0001  max mem: 6052
[05:43:03.601242] Epoch: [70]  [320/781]  eta: 0:01:19  lr: 0.000056  training_loss: 1.2468 (1.2105)  classification_loss: 1.2434 (1.1931)  loss_mask: 0.0039 (0.0174)  time: 0.1701  data: 0.0001  max mem: 6052
[05:43:07.012440] Epoch: [70]  [340/781]  eta: 0:01:16  lr: 0.000056  training_loss: 1.1607 (1.2085)  classification_loss: 1.1589 (1.1919)  loss_mask: 0.0016 (0.0165)  time: 0.1705  data: 0.0002  max mem: 6052
[05:43:10.426320] Epoch: [70]  [360/781]  eta: 0:01:12  lr: 0.000056  training_loss: 1.2325 (1.2105)  classification_loss: 1.2313 (1.1948)  loss_mask: 0.0010 (0.0157)  time: 0.1706  data: 0.0002  max mem: 6052
[05:43:13.853164] Epoch: [70]  [380/781]  eta: 0:01:09  lr: 0.000056  training_loss: 1.2171 (1.2116)  classification_loss: 1.2168 (1.1968)  loss_mask: 0.0007 (0.0149)  time: 0.1712  data: 0.0003  max mem: 6052
[05:43:17.289347] Epoch: [70]  [400/781]  eta: 0:01:05  lr: 0.000056  training_loss: 1.1547 (1.2105)  classification_loss: 1.1542 (1.1963)  loss_mask: 0.0005 (0.0142)  time: 0.1717  data: 0.0002  max mem: 6052
[05:43:20.729564] Epoch: [70]  [420/781]  eta: 0:01:02  lr: 0.000056  training_loss: 1.1725 (1.2095)  classification_loss: 1.1720 (1.1957)  loss_mask: 0.0020 (0.0138)  time: 0.1719  data: 0.0004  max mem: 6052
[05:43:24.160780] Epoch: [70]  [440/781]  eta: 0:00:58  lr: 0.000055  training_loss: 1.1840 (1.2089)  classification_loss: 1.1835 (1.1956)  loss_mask: 0.0006 (0.0132)  time: 0.1715  data: 0.0004  max mem: 6052
[05:43:27.566234] Epoch: [70]  [460/781]  eta: 0:00:55  lr: 0.000055  training_loss: 1.1698 (1.2076)  classification_loss: 1.1690 (1.1949)  loss_mask: 0.0006 (0.0127)  time: 0.1702  data: 0.0003  max mem: 6052
[05:43:31.008798] Epoch: [70]  [480/781]  eta: 0:00:51  lr: 0.000055  training_loss: 1.2094 (1.2078)  classification_loss: 1.2087 (1.1956)  loss_mask: 0.0004 (0.0122)  time: 0.1720  data: 0.0002  max mem: 6052
[05:43:34.416458] Epoch: [70]  [500/781]  eta: 0:00:48  lr: 0.000055  training_loss: 1.1961 (1.2077)  classification_loss: 1.1955 (1.1960)  loss_mask: 0.0004 (0.0117)  time: 0.1703  data: 0.0002  max mem: 6052
[05:43:37.834704] Epoch: [70]  [520/781]  eta: 0:00:44  lr: 0.000055  training_loss: 1.1795 (1.2071)  classification_loss: 1.1788 (1.1958)  loss_mask: 0.0003 (0.0113)  time: 0.1708  data: 0.0002  max mem: 6052
[05:43:41.299203] Epoch: [70]  [540/781]  eta: 0:00:41  lr: 0.000055  training_loss: 1.2065 (1.2074)  classification_loss: 1.2060 (1.1966)  loss_mask: 0.0004 (0.0109)  time: 0.1731  data: 0.0002  max mem: 6052
[05:43:44.742222] Epoch: [70]  [560/781]  eta: 0:00:38  lr: 0.000055  training_loss: 1.1607 (1.2069)  classification_loss: 1.1604 (1.1964)  loss_mask: 0.0003 (0.0105)  time: 0.1721  data: 0.0004  max mem: 6052
[05:43:48.184205] Epoch: [70]  [580/781]  eta: 0:00:34  lr: 0.000055  training_loss: 1.1634 (1.2055)  classification_loss: 1.1629 (1.1953)  loss_mask: 0.0003 (0.0101)  time: 0.1720  data: 0.0003  max mem: 6052
[05:43:51.593736] Epoch: [70]  [600/781]  eta: 0:00:31  lr: 0.000055  training_loss: 1.1772 (1.2046)  classification_loss: 1.1768 (1.1948)  loss_mask: 0.0004 (0.0098)  time: 0.1704  data: 0.0002  max mem: 6052
[05:43:55.033748] Epoch: [70]  [620/781]  eta: 0:00:27  lr: 0.000055  training_loss: 1.2131 (1.2048)  classification_loss: 1.2127 (1.1953)  loss_mask: 0.0004 (0.0095)  time: 0.1719  data: 0.0004  max mem: 6052
[05:43:58.464682] Epoch: [70]  [640/781]  eta: 0:00:24  lr: 0.000055  training_loss: 1.1878 (1.2045)  classification_loss: 1.1876 (1.1953)  loss_mask: 0.0003 (0.0092)  time: 0.1715  data: 0.0004  max mem: 6052
[05:44:01.871191] Epoch: [70]  [660/781]  eta: 0:00:20  lr: 0.000055  training_loss: 1.2052 (1.2041)  classification_loss: 1.2042 (1.1952)  loss_mask: 0.0003 (0.0090)  time: 0.1703  data: 0.0003  max mem: 6052
[05:44:05.292007] Epoch: [70]  [680/781]  eta: 0:00:17  lr: 0.000054  training_loss: 1.1935 (1.2039)  classification_loss: 1.1932 (1.1952)  loss_mask: 0.0003 (0.0087)  time: 0.1710  data: 0.0001  max mem: 6052
[05:44:08.707899] Epoch: [70]  [700/781]  eta: 0:00:13  lr: 0.000054  training_loss: 1.2009 (1.2050)  classification_loss: 1.2007 (1.1965)  loss_mask: 0.0003 (0.0085)  time: 0.1707  data: 0.0003  max mem: 6052
[05:44:12.126720] Epoch: [70]  [720/781]  eta: 0:00:10  lr: 0.000054  training_loss: 1.2355 (1.2060)  classification_loss: 1.2351 (1.1978)  loss_mask: 0.0002 (0.0082)  time: 0.1709  data: 0.0003  max mem: 6052
[05:44:15.549642] Epoch: [70]  [740/781]  eta: 0:00:07  lr: 0.000054  training_loss: 1.2057 (1.2059)  classification_loss: 1.2055 (1.1978)  loss_mask: 0.0003 (0.0080)  time: 0.1711  data: 0.0004  max mem: 6052
[05:44:19.011304] Epoch: [70]  [760/781]  eta: 0:00:03  lr: 0.000054  training_loss: 1.1979 (1.2063)  classification_loss: 1.1974 (1.1985)  loss_mask: 0.0002 (0.0078)  time: 0.1730  data: 0.0003  max mem: 6052
[05:44:22.524367] Epoch: [70]  [780/781]  eta: 0:00:00  lr: 0.000054  training_loss: 1.1723 (1.2059)  classification_loss: 1.1718 (1.1983)  loss_mask: 0.0003 (0.0076)  time: 0.1756  data: 0.0003  max mem: 6052
[05:44:22.692908] Epoch: [70] Total time: 0:02:14 (0.1725 s / it)
[05:44:22.693448] Averaged stats: lr: 0.000054  training_loss: 1.1723 (1.2059)  classification_loss: 1.1718 (1.1983)  loss_mask: 0.0003 (0.0076)
[05:44:24.002056] Test:  [  0/157]  eta: 0:01:43  testing_loss: 0.4729 (0.4729)  acc1: 85.9375 (85.9375)  acc5: 100.0000 (100.0000)  time: 0.6602  data: 0.6300  max mem: 6052
[05:44:24.293363] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4729 (0.5029)  acc1: 84.3750 (83.6648)  acc5: 100.0000 (99.5739)  time: 0.0862  data: 0.0575  max mem: 6052
[05:44:24.576038] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4608 (0.4745)  acc1: 84.3750 (84.7470)  acc5: 100.0000 (99.6280)  time: 0.0285  data: 0.0002  max mem: 6052
[05:44:24.857751] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4467 (0.4847)  acc1: 85.9375 (84.7278)  acc5: 100.0000 (99.3448)  time: 0.0281  data: 0.0002  max mem: 6052
[05:44:25.137989] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4901 (0.4913)  acc1: 85.9375 (85.0610)  acc5: 98.4375 (99.1616)  time: 0.0280  data: 0.0002  max mem: 6052
[05:44:25.418414] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4619 (0.4862)  acc1: 85.9375 (85.3554)  acc5: 98.4375 (99.1728)  time: 0.0279  data: 0.0001  max mem: 6052
[05:44:25.698914] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4527 (0.4830)  acc1: 85.9375 (85.5020)  acc5: 100.0000 (99.1803)  time: 0.0279  data: 0.0001  max mem: 6052
[05:44:25.978863] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4658 (0.4801)  acc1: 85.9375 (85.5414)  acc5: 100.0000 (99.2298)  time: 0.0279  data: 0.0001  max mem: 6052
[05:44:26.261202] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4845 (0.4874)  acc1: 84.3750 (85.3009)  acc5: 100.0000 (99.2863)  time: 0.0280  data: 0.0002  max mem: 6052
[05:44:26.542429] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4632 (0.4838)  acc1: 85.9375 (85.6113)  acc5: 100.0000 (99.2788)  time: 0.0281  data: 0.0002  max mem: 6052
[05:44:26.822847] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4651 (0.4882)  acc1: 85.9375 (85.3032)  acc5: 100.0000 (99.3038)  time: 0.0280  data: 0.0002  max mem: 6052
[05:44:27.103428] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4935 (0.4879)  acc1: 82.8125 (85.3041)  acc5: 100.0000 (99.3243)  time: 0.0279  data: 0.0002  max mem: 6052
[05:44:27.389721] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4587 (0.4863)  acc1: 84.3750 (85.3177)  acc5: 100.0000 (99.3156)  time: 0.0282  data: 0.0001  max mem: 6052
[05:44:27.670991] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4778 (0.4880)  acc1: 84.3750 (85.4008)  acc5: 100.0000 (99.3201)  time: 0.0283  data: 0.0002  max mem: 6052
[05:44:27.950172] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4727 (0.4848)  acc1: 84.3750 (85.4832)  acc5: 100.0000 (99.3240)  time: 0.0279  data: 0.0001  max mem: 6052
[05:44:28.228384] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4728 (0.4851)  acc1: 84.3750 (85.3891)  acc5: 100.0000 (99.3481)  time: 0.0278  data: 0.0001  max mem: 6052
[05:44:28.377407] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4674 (0.4849)  acc1: 85.9375 (85.3900)  acc5: 100.0000 (99.3500)  time: 0.0268  data: 0.0001  max mem: 6052
[05:44:28.536952] Test: Total time: 0:00:05 (0.0331 s / it)
[05:44:28.537381] * Acc@1 85.390 Acc@5 99.350 loss 0.485
[05:44:28.537669] Accuracy of the network on the 10000 test images: 85.4%
[05:44:28.537865] Max accuracy: 85.39%
[05:44:28.741663] log_dir: ./output_dir
[05:44:29.568370] Epoch: [71]  [  0/781]  eta: 0:10:44  lr: 0.000054  training_loss: 1.1235 (1.1235)  classification_loss: 1.1231 (1.1231)  loss_mask: 0.0005 (0.0005)  time: 0.8252  data: 0.6197  max mem: 6052
[05:44:33.001954] Epoch: [71]  [ 20/781]  eta: 0:02:34  lr: 0.000054  training_loss: 1.1962 (1.1763)  classification_loss: 1.1959 (1.1761)  loss_mask: 0.0002 (0.0003)  time: 0.1716  data: 0.0005  max mem: 6052
[05:44:36.436300] Epoch: [71]  [ 40/781]  eta: 0:02:18  lr: 0.000054  training_loss: 1.2246 (1.2047)  classification_loss: 1.2243 (1.2044)  loss_mask: 0.0002 (0.0003)  time: 0.1716  data: 0.0002  max mem: 6052
[05:44:39.854962] Epoch: [71]  [ 60/781]  eta: 0:02:11  lr: 0.000054  training_loss: 1.1907 (1.2025)  classification_loss: 1.1903 (1.2023)  loss_mask: 0.0003 (0.0003)  time: 0.1708  data: 0.0002  max mem: 6052
[05:44:43.278991] Epoch: [71]  [ 80/781]  eta: 0:02:05  lr: 0.000054  training_loss: 1.1809 (1.2004)  classification_loss: 1.1808 (1.2001)  loss_mask: 0.0002 (0.0002)  time: 0.1711  data: 0.0002  max mem: 6052
[05:44:46.691474] Epoch: [71]  [100/781]  eta: 0:02:00  lr: 0.000054  training_loss: 1.1681 (1.1944)  classification_loss: 1.1679 (1.1942)  loss_mask: 0.0002 (0.0002)  time: 0.1705  data: 0.0003  max mem: 6052
[05:44:50.147933] Epoch: [71]  [120/781]  eta: 0:01:56  lr: 0.000053  training_loss: 1.2431 (1.2018)  classification_loss: 1.2428 (1.2015)  loss_mask: 0.0002 (0.0002)  time: 0.1727  data: 0.0002  max mem: 6052
[05:44:53.588567] Epoch: [71]  [140/781]  eta: 0:01:52  lr: 0.000053  training_loss: 1.1734 (1.1969)  classification_loss: 1.1730 (1.1967)  loss_mask: 0.0002 (0.0002)  time: 0.1719  data: 0.0002  max mem: 6052
[05:44:57.023184] Epoch: [71]  [160/781]  eta: 0:01:49  lr: 0.000053  training_loss: 1.2026 (1.1988)  classification_loss: 1.2022 (1.1985)  loss_mask: 0.0002 (0.0002)  time: 0.1716  data: 0.0002  max mem: 6052
[05:45:00.431144] Epoch: [71]  [180/781]  eta: 0:01:45  lr: 0.000053  training_loss: 1.1937 (1.1992)  classification_loss: 1.1937 (1.1990)  loss_mask: 0.0002 (0.0002)  time: 0.1702  data: 0.0002  max mem: 6052
[05:45:03.851628] Epoch: [71]  [200/781]  eta: 0:01:41  lr: 0.000053  training_loss: 1.1556 (1.1951)  classification_loss: 1.1553 (1.1949)  loss_mask: 0.0002 (0.0002)  time: 0.1709  data: 0.0002  max mem: 6052
[05:45:07.260607] Epoch: [71]  [220/781]  eta: 0:01:37  lr: 0.000053  training_loss: 1.1690 (1.1949)  classification_loss: 1.1688 (1.1947)  loss_mask: 0.0002 (0.0002)  time: 0.1704  data: 0.0003  max mem: 6052
[05:45:10.697256] Epoch: [71]  [240/781]  eta: 0:01:34  lr: 0.000053  training_loss: 1.1783 (1.1954)  classification_loss: 1.1780 (1.1952)  loss_mask: 0.0002 (0.0002)  time: 0.1718  data: 0.0005  max mem: 6052
[05:45:14.118269] Epoch: [71]  [260/781]  eta: 0:01:30  lr: 0.000053  training_loss: 1.1982 (1.1958)  classification_loss: 1.1980 (1.1955)  loss_mask: 0.0002 (0.0002)  time: 0.1709  data: 0.0002  max mem: 6052
[05:45:17.575402] Epoch: [71]  [280/781]  eta: 0:01:27  lr: 0.000053  training_loss: 1.1654 (1.1968)  classification_loss: 1.1653 (1.1965)  loss_mask: 0.0002 (0.0002)  time: 0.1728  data: 0.0002  max mem: 6052
[05:45:21.024757] Epoch: [71]  [300/781]  eta: 0:01:23  lr: 0.000053  training_loss: 1.1696 (1.1970)  classification_loss: 1.1693 (1.1967)  loss_mask: 0.0002 (0.0002)  time: 0.1723  data: 0.0002  max mem: 6052
[05:45:24.443414] Epoch: [71]  [320/781]  eta: 0:01:19  lr: 0.000053  training_loss: 1.1656 (1.1947)  classification_loss: 1.1651 (1.1945)  loss_mask: 0.0002 (0.0002)  time: 0.1709  data: 0.0002  max mem: 6052
[05:45:27.866160] Epoch: [71]  [340/781]  eta: 0:01:16  lr: 0.000053  training_loss: 1.1514 (1.1931)  classification_loss: 1.1510 (1.1929)  loss_mask: 0.0002 (0.0002)  time: 0.1711  data: 0.0003  max mem: 6052
[05:45:31.286625] Epoch: [71]  [360/781]  eta: 0:01:12  lr: 0.000052  training_loss: 1.2024 (1.1938)  classification_loss: 1.2023 (1.1935)  loss_mask: 0.0002 (0.0002)  time: 0.1710  data: 0.0003  max mem: 6052
[05:45:34.680786] Epoch: [71]  [380/781]  eta: 0:01:09  lr: 0.000052  training_loss: 1.1637 (1.1939)  classification_loss: 1.1635 (1.1937)  loss_mask: 0.0002 (0.0002)  time: 0.1696  data: 0.0003  max mem: 6052
[05:45:38.084688] Epoch: [71]  [400/781]  eta: 0:01:05  lr: 0.000052  training_loss: 1.2140 (1.1956)  classification_loss: 1.2138 (1.1954)  loss_mask: 0.0002 (0.0002)  time: 0.1701  data: 0.0003  max mem: 6052
[05:45:41.513691] Epoch: [71]  [420/781]  eta: 0:01:02  lr: 0.000052  training_loss: 1.1942 (1.1953)  classification_loss: 1.1941 (1.1951)  loss_mask: 0.0001 (0.0002)  time: 0.1714  data: 0.0003  max mem: 6052
[05:45:44.930690] Epoch: [71]  [440/781]  eta: 0:00:58  lr: 0.000052  training_loss: 1.1853 (1.1956)  classification_loss: 1.1849 (1.1953)  loss_mask: 0.0002 (0.0003)  time: 0.1708  data: 0.0002  max mem: 6052
[05:45:48.376847] Epoch: [71]  [460/781]  eta: 0:00:55  lr: 0.000052  training_loss: 1.1985 (1.1943)  classification_loss: 1.1983 (1.1941)  loss_mask: 0.0002 (0.0003)  time: 0.1722  data: 0.0001  max mem: 6052
[05:45:51.798173] Epoch: [71]  [480/781]  eta: 0:00:51  lr: 0.000052  training_loss: 1.1639 (1.1947)  classification_loss: 1.1637 (1.1944)  loss_mask: 0.0001 (0.0003)  time: 0.1710  data: 0.0002  max mem: 6052
[05:45:55.215376] Epoch: [71]  [500/781]  eta: 0:00:48  lr: 0.000052  training_loss: 1.2143 (1.1950)  classification_loss: 1.2141 (1.1948)  loss_mask: 0.0001 (0.0003)  time: 0.1707  data: 0.0002  max mem: 6052
[05:45:58.657708] Epoch: [71]  [520/781]  eta: 0:00:45  lr: 0.000052  training_loss: 1.2226 (1.1958)  classification_loss: 1.2225 (1.1955)  loss_mask: 0.0002 (0.0003)  time: 0.1720  data: 0.0002  max mem: 6052
[05:46:02.068675] Epoch: [71]  [540/781]  eta: 0:00:41  lr: 0.000052  training_loss: 1.2043 (1.1966)  classification_loss: 1.2041 (1.1964)  loss_mask: 0.0002 (0.0002)  time: 0.1705  data: 0.0002  max mem: 6052
[05:46:05.518085] Epoch: [71]  [560/781]  eta: 0:00:38  lr: 0.000052  training_loss: 1.1717 (1.1960)  classification_loss: 1.1716 (1.1957)  loss_mask: 0.0002 (0.0002)  time: 0.1724  data: 0.0003  max mem: 6052
[05:46:08.939269] Epoch: [71]  [580/781]  eta: 0:00:34  lr: 0.000052  training_loss: 1.1792 (1.1962)  classification_loss: 1.1791 (1.1959)  loss_mask: 0.0001 (0.0002)  time: 0.1708  data: 0.0003  max mem: 6052
[05:46:12.366462] Epoch: [71]  [600/781]  eta: 0:00:31  lr: 0.000051  training_loss: 1.2180 (1.1965)  classification_loss: 1.2177 (1.1963)  loss_mask: 0.0002 (0.0002)  time: 0.1712  data: 0.0005  max mem: 6052
[05:46:15.797413] Epoch: [71]  [620/781]  eta: 0:00:27  lr: 0.000051  training_loss: 1.1842 (1.1962)  classification_loss: 1.1840 (1.1960)  loss_mask: 0.0002 (0.0002)  time: 0.1715  data: 0.0002  max mem: 6052
[05:46:19.204198] Epoch: [71]  [640/781]  eta: 0:00:24  lr: 0.000051  training_loss: 1.1767 (1.1966)  classification_loss: 1.1766 (1.1963)  loss_mask: 0.0001 (0.0002)  time: 0.1703  data: 0.0002  max mem: 6052
[05:46:22.617734] Epoch: [71]  [660/781]  eta: 0:00:20  lr: 0.000051  training_loss: 1.1682 (1.1958)  classification_loss: 1.1678 (1.1956)  loss_mask: 0.0002 (0.0002)  time: 0.1706  data: 0.0003  max mem: 6052
[05:46:26.033574] Epoch: [71]  [680/781]  eta: 0:00:17  lr: 0.000051  training_loss: 1.1758 (1.1958)  classification_loss: 1.1757 (1.1956)  loss_mask: 0.0002 (0.0002)  time: 0.1707  data: 0.0002  max mem: 6052
[05:46:29.444481] Epoch: [71]  [700/781]  eta: 0:00:13  lr: 0.000051  training_loss: 1.1990 (1.1962)  classification_loss: 1.1989 (1.1960)  loss_mask: 0.0002 (0.0002)  time: 0.1704  data: 0.0001  max mem: 6052
[05:46:32.867390] Epoch: [71]  [720/781]  eta: 0:00:10  lr: 0.000051  training_loss: 1.2054 (1.1969)  classification_loss: 1.2051 (1.1967)  loss_mask: 0.0002 (0.0002)  time: 0.1711  data: 0.0002  max mem: 6052
[05:46:36.284436] Epoch: [71]  [740/781]  eta: 0:00:07  lr: 0.000051  training_loss: 1.1912 (1.1966)  classification_loss: 1.1911 (1.1964)  loss_mask: 0.0002 (0.0002)  time: 0.1708  data: 0.0003  max mem: 6052
[05:46:39.685662] Epoch: [71]  [760/781]  eta: 0:00:03  lr: 0.000051  training_loss: 1.2644 (1.1975)  classification_loss: 1.2384 (1.1971)  loss_mask: 0.0002 (0.0004)  time: 0.1700  data: 0.0002  max mem: 6052
[05:46:43.088524] Epoch: [71]  [780/781]  eta: 0:00:00  lr: 0.000051  training_loss: 1.2023 (1.1976)  classification_loss: 1.2011 (1.1972)  loss_mask: 0.0004 (0.0004)  time: 0.1701  data: 0.0001  max mem: 6052
[05:46:43.243788] Epoch: [71] Total time: 0:02:14 (0.1722 s / it)
[05:46:43.244243] Averaged stats: lr: 0.000051  training_loss: 1.2023 (1.1976)  classification_loss: 1.2011 (1.1972)  loss_mask: 0.0004 (0.0004)
[05:46:43.922576] Test:  [  0/157]  eta: 0:01:45  testing_loss: 0.4634 (0.4634)  acc1: 85.9375 (85.9375)  acc5: 100.0000 (100.0000)  time: 0.6742  data: 0.6440  max mem: 6052
[05:46:44.209467] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4999 (0.5131)  acc1: 84.3750 (83.9489)  acc5: 100.0000 (99.5739)  time: 0.0872  data: 0.0587  max mem: 6052
[05:46:44.489899] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4630 (0.4759)  acc1: 84.3750 (85.0446)  acc5: 100.0000 (99.6280)  time: 0.0282  data: 0.0001  max mem: 6052
[05:46:44.773323] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4619 (0.4837)  acc1: 84.3750 (84.9294)  acc5: 100.0000 (99.5968)  time: 0.0281  data: 0.0002  max mem: 6052
[05:46:45.061101] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4994 (0.4898)  acc1: 84.3750 (84.9848)  acc5: 100.0000 (99.3902)  time: 0.0284  data: 0.0002  max mem: 6052
[05:46:45.348180] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5064 (0.4866)  acc1: 84.3750 (85.3554)  acc5: 98.4375 (99.3260)  time: 0.0286  data: 0.0002  max mem: 6052
[05:46:45.634158] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4487 (0.4845)  acc1: 85.9375 (85.2971)  acc5: 98.4375 (99.3084)  time: 0.0285  data: 0.0002  max mem: 6052
[05:46:45.918586] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4487 (0.4794)  acc1: 85.9375 (85.6294)  acc5: 100.0000 (99.3398)  time: 0.0283  data: 0.0002  max mem: 6052
[05:46:46.205209] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4804 (0.4879)  acc1: 84.3750 (85.3781)  acc5: 100.0000 (99.3248)  time: 0.0284  data: 0.0004  max mem: 6052
[05:46:46.485967] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4804 (0.4854)  acc1: 84.3750 (85.4739)  acc5: 98.4375 (99.2788)  time: 0.0283  data: 0.0004  max mem: 6052
[05:46:46.768886] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4620 (0.4881)  acc1: 87.5000 (85.5353)  acc5: 98.4375 (99.2729)  time: 0.0281  data: 0.0001  max mem: 6052
[05:46:47.056835] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4801 (0.4884)  acc1: 85.9375 (85.4730)  acc5: 100.0000 (99.2962)  time: 0.0284  data: 0.0001  max mem: 6052
[05:46:47.339631] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4780 (0.4875)  acc1: 85.9375 (85.4985)  acc5: 100.0000 (99.3156)  time: 0.0284  data: 0.0002  max mem: 6052
[05:46:47.625345] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4780 (0.4889)  acc1: 85.9375 (85.5320)  acc5: 100.0000 (99.3082)  time: 0.0283  data: 0.0004  max mem: 6052
[05:46:47.907318] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4489 (0.4851)  acc1: 85.9375 (85.5829)  acc5: 100.0000 (99.3351)  time: 0.0282  data: 0.0004  max mem: 6052
[05:46:48.186022] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4501 (0.4845)  acc1: 84.3750 (85.5546)  acc5: 100.0000 (99.3377)  time: 0.0278  data: 0.0001  max mem: 6052
[05:46:48.335570] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4583 (0.4851)  acc1: 84.3750 (85.4900)  acc5: 100.0000 (99.3400)  time: 0.0268  data: 0.0001  max mem: 6052
[05:46:48.515023] Test: Total time: 0:00:05 (0.0336 s / it)
[05:46:48.515947] * Acc@1 85.490 Acc@5 99.340 loss 0.485
[05:46:48.516258] Accuracy of the network on the 10000 test images: 85.5%
[05:46:48.516479] Max accuracy: 85.49%
[05:46:48.766536] log_dir: ./output_dir
[05:46:49.661117] Epoch: [72]  [  0/781]  eta: 0:11:37  lr: 0.000051  training_loss: 1.0506 (1.0506)  classification_loss: 1.0504 (1.0504)  loss_mask: 0.0002 (0.0002)  time: 0.8931  data: 0.7120  max mem: 6052
[05:46:53.081860] Epoch: [72]  [ 20/781]  eta: 0:02:36  lr: 0.000051  training_loss: 1.1543 (1.1510)  classification_loss: 1.1541 (1.1507)  loss_mask: 0.0002 (0.0003)  time: 0.1709  data: 0.0002  max mem: 6052
[05:46:56.534885] Epoch: [72]  [ 40/781]  eta: 0:02:20  lr: 0.000050  training_loss: 1.1852 (1.1791)  classification_loss: 1.1852 (1.1789)  loss_mask: 0.0002 (0.0002)  time: 0.1726  data: 0.0002  max mem: 6052
[05:46:59.943876] Epoch: [72]  [ 60/781]  eta: 0:02:12  lr: 0.000050  training_loss: 1.1857 (1.1878)  classification_loss: 1.1855 (1.1876)  loss_mask: 0.0001 (0.0002)  time: 0.1704  data: 0.0002  max mem: 6052
[05:47:03.364408] Epoch: [72]  [ 80/781]  eta: 0:02:06  lr: 0.000050  training_loss: 1.2184 (1.1953)  classification_loss: 1.2182 (1.1951)  loss_mask: 0.0002 (0.0002)  time: 0.1710  data: 0.0003  max mem: 6052
[05:47:06.787948] Epoch: [72]  [100/781]  eta: 0:02:01  lr: 0.000050  training_loss: 1.1921 (1.2018)  classification_loss: 1.1920 (1.2017)  loss_mask: 0.0001 (0.0002)  time: 0.1711  data: 0.0003  max mem: 6052
[05:47:10.221078] Epoch: [72]  [120/781]  eta: 0:01:57  lr: 0.000050  training_loss: 1.1834 (1.1980)  classification_loss: 1.1833 (1.1979)  loss_mask: 0.0001 (0.0002)  time: 0.1716  data: 0.0002  max mem: 6052
[05:47:13.643874] Epoch: [72]  [140/781]  eta: 0:01:53  lr: 0.000050  training_loss: 1.1943 (1.1977)  classification_loss: 1.1941 (1.1975)  loss_mask: 0.0001 (0.0002)  time: 0.1711  data: 0.0002  max mem: 6052
[05:47:17.065030] Epoch: [72]  [160/781]  eta: 0:01:49  lr: 0.000050  training_loss: 1.1967 (1.1949)  classification_loss: 1.1965 (1.1948)  loss_mask: 0.0002 (0.0002)  time: 0.1710  data: 0.0002  max mem: 6052
[05:47:20.485764] Epoch: [72]  [180/781]  eta: 0:01:45  lr: 0.000050  training_loss: 1.1734 (1.1910)  classification_loss: 1.1733 (1.1908)  loss_mask: 0.0001 (0.0002)  time: 0.1710  data: 0.0002  max mem: 6052
[05:47:23.900545] Epoch: [72]  [200/781]  eta: 0:01:41  lr: 0.000050  training_loss: 1.2413 (1.1941)  classification_loss: 1.2412 (1.1939)  loss_mask: 0.0001 (0.0002)  time: 0.1706  data: 0.0002  max mem: 6052
[05:47:27.397344] Epoch: [72]  [220/781]  eta: 0:01:38  lr: 0.000050  training_loss: 1.1883 (1.1948)  classification_loss: 1.1883 (1.1947)  loss_mask: 0.0001 (0.0002)  time: 0.1748  data: 0.0003  max mem: 6052
[05:47:30.862541] Epoch: [72]  [240/781]  eta: 0:01:34  lr: 0.000050  training_loss: 1.2037 (1.1960)  classification_loss: 1.2033 (1.1959)  loss_mask: 0.0002 (0.0002)  time: 0.1732  data: 0.0003  max mem: 6052
[05:47:34.291551] Epoch: [72]  [260/781]  eta: 0:01:30  lr: 0.000050  training_loss: 1.2083 (1.1966)  classification_loss: 1.2082 (1.1965)  loss_mask: 0.0001 (0.0002)  time: 0.1714  data: 0.0003  max mem: 6052
[05:47:37.849243] Epoch: [72]  [280/781]  eta: 0:01:27  lr: 0.000049  training_loss: 1.1600 (1.1952)  classification_loss: 1.1599 (1.1951)  loss_mask: 0.0001 (0.0002)  time: 0.1778  data: 0.0002  max mem: 6052
[05:47:41.345832] Epoch: [72]  [300/781]  eta: 0:01:23  lr: 0.000049  training_loss: 1.2068 (1.1960)  classification_loss: 1.2067 (1.1959)  loss_mask: 0.0001 (0.0002)  time: 0.1748  data: 0.0002  max mem: 6052
[05:47:44.762179] Epoch: [72]  [320/781]  eta: 0:01:20  lr: 0.000049  training_loss: 1.1773 (1.1950)  classification_loss: 1.1772 (1.1948)  loss_mask: 0.0001 (0.0002)  time: 0.1707  data: 0.0002  max mem: 6052
[05:47:48.207413] Epoch: [72]  [340/781]  eta: 0:01:16  lr: 0.000049  training_loss: 1.1398 (1.1931)  classification_loss: 1.1397 (1.1929)  loss_mask: 0.0001 (0.0002)  time: 0.1722  data: 0.0002  max mem: 6052
[05:47:51.632650] Epoch: [72]  [360/781]  eta: 0:01:13  lr: 0.000049  training_loss: 1.1937 (1.1937)  classification_loss: 1.1936 (1.1935)  loss_mask: 0.0001 (0.0002)  time: 0.1712  data: 0.0002  max mem: 6052
[05:47:55.051165] Epoch: [72]  [380/781]  eta: 0:01:09  lr: 0.000049  training_loss: 1.2164 (1.1949)  classification_loss: 1.2162 (1.1947)  loss_mask: 0.0001 (0.0002)  time: 0.1708  data: 0.0003  max mem: 6052
[05:47:58.486490] Epoch: [72]  [400/781]  eta: 0:01:06  lr: 0.000049  training_loss: 1.1518 (1.1944)  classification_loss: 1.1516 (1.1942)  loss_mask: 0.0001 (0.0002)  time: 0.1717  data: 0.0003  max mem: 6052
[05:48:01.916392] Epoch: [72]  [420/781]  eta: 0:01:02  lr: 0.000049  training_loss: 1.1971 (1.1943)  classification_loss: 1.1970 (1.1941)  loss_mask: 0.0001 (0.0002)  time: 0.1714  data: 0.0003  max mem: 6052
[05:48:05.344849] Epoch: [72]  [440/781]  eta: 0:00:59  lr: 0.000049  training_loss: 1.1864 (1.1936)  classification_loss: 1.1863 (1.1934)  loss_mask: 0.0001 (0.0002)  time: 0.1713  data: 0.0002  max mem: 6052
[05:48:08.765070] Epoch: [72]  [460/781]  eta: 0:00:55  lr: 0.000049  training_loss: 1.1652 (1.1928)  classification_loss: 1.1649 (1.1926)  loss_mask: 0.0002 (0.0002)  time: 0.1709  data: 0.0002  max mem: 6052
[05:48:12.203970] Epoch: [72]  [480/781]  eta: 0:00:52  lr: 0.000049  training_loss: 1.2236 (1.1933)  classification_loss: 1.2234 (1.1931)  loss_mask: 0.0001 (0.0002)  time: 0.1719  data: 0.0002  max mem: 6052
[05:48:15.690590] Epoch: [72]  [500/781]  eta: 0:00:48  lr: 0.000049  training_loss: 1.1510 (1.1927)  classification_loss: 1.1508 (1.1925)  loss_mask: 0.0001 (0.0002)  time: 0.1742  data: 0.0002  max mem: 6052
[05:48:19.089863] Epoch: [72]  [520/781]  eta: 0:00:45  lr: 0.000048  training_loss: 1.1695 (1.1922)  classification_loss: 1.1694 (1.1920)  loss_mask: 0.0001 (0.0002)  time: 0.1699  data: 0.0003  max mem: 6052
[05:48:22.534638] Epoch: [72]  [540/781]  eta: 0:00:41  lr: 0.000048  training_loss: 1.1663 (1.1910)  classification_loss: 1.1662 (1.1908)  loss_mask: 0.0001 (0.0002)  time: 0.1722  data: 0.0002  max mem: 6052
[05:48:25.944450] Epoch: [72]  [560/781]  eta: 0:00:38  lr: 0.000048  training_loss: 1.1756 (1.1911)  classification_loss: 1.1756 (1.1909)  loss_mask: 0.0001 (0.0002)  time: 0.1704  data: 0.0002  max mem: 6052
[05:48:29.406669] Epoch: [72]  [580/781]  eta: 0:00:34  lr: 0.000048  training_loss: 1.1740 (1.1902)  classification_loss: 1.1738 (1.1900)  loss_mask: 0.0001 (0.0002)  time: 0.1730  data: 0.0004  max mem: 6052
[05:48:32.844577] Epoch: [72]  [600/781]  eta: 0:00:31  lr: 0.000048  training_loss: 1.2385 (1.1910)  classification_loss: 1.2383 (1.1909)  loss_mask: 0.0001 (0.0002)  time: 0.1718  data: 0.0002  max mem: 6052
[05:48:36.256283] Epoch: [72]  [620/781]  eta: 0:00:27  lr: 0.000048  training_loss: 1.1807 (1.1905)  classification_loss: 1.1807 (1.1904)  loss_mask: 0.0001 (0.0002)  time: 0.1705  data: 0.0002  max mem: 6052
[05:48:39.668787] Epoch: [72]  [640/781]  eta: 0:00:24  lr: 0.000048  training_loss: 1.1871 (1.1904)  classification_loss: 1.1870 (1.1902)  loss_mask: 0.0001 (0.0002)  time: 0.1705  data: 0.0003  max mem: 6052
[05:48:43.091105] Epoch: [72]  [660/781]  eta: 0:00:20  lr: 0.000048  training_loss: 1.2160 (1.1914)  classification_loss: 1.2159 (1.1912)  loss_mask: 0.0001 (0.0002)  time: 0.1710  data: 0.0003  max mem: 6052
[05:48:46.508567] Epoch: [72]  [680/781]  eta: 0:00:17  lr: 0.000048  training_loss: 1.1873 (1.1914)  classification_loss: 1.1872 (1.1913)  loss_mask: 0.0001 (0.0002)  time: 0.1708  data: 0.0002  max mem: 6052
[05:48:49.911063] Epoch: [72]  [700/781]  eta: 0:00:13  lr: 0.000048  training_loss: 1.2049 (1.1917)  classification_loss: 1.2049 (1.1916)  loss_mask: 0.0001 (0.0002)  time: 0.1701  data: 0.0002  max mem: 6052
[05:48:53.321382] Epoch: [72]  [720/781]  eta: 0:00:10  lr: 0.000048  training_loss: 1.2001 (1.1921)  classification_loss: 1.2000 (1.1919)  loss_mask: 0.0001 (0.0002)  time: 0.1704  data: 0.0003  max mem: 6052
[05:48:56.735220] Epoch: [72]  [740/781]  eta: 0:00:07  lr: 0.000048  training_loss: 1.1507 (1.1915)  classification_loss: 1.1505 (1.1914)  loss_mask: 0.0001 (0.0001)  time: 0.1706  data: 0.0003  max mem: 6052
[05:49:00.149203] Epoch: [72]  [760/781]  eta: 0:00:03  lr: 0.000048  training_loss: 1.1876 (1.1916)  classification_loss: 1.1874 (1.1915)  loss_mask: 0.0001 (0.0001)  time: 0.1706  data: 0.0002  max mem: 6052
[05:49:03.549140] Epoch: [72]  [780/781]  eta: 0:00:00  lr: 0.000047  training_loss: 1.2080 (1.1920)  classification_loss: 1.2079 (1.1918)  loss_mask: 0.0001 (0.0001)  time: 0.1699  data: 0.0003  max mem: 6052
[05:49:03.714581] Epoch: [72] Total time: 0:02:14 (0.1728 s / it)
[05:49:03.715303] Averaged stats: lr: 0.000047  training_loss: 1.2080 (1.1920)  classification_loss: 1.2079 (1.1918)  loss_mask: 0.0001 (0.0001)
[05:49:04.388832] Test:  [  0/157]  eta: 0:01:45  testing_loss: 0.4688 (0.4688)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (100.0000)  time: 0.6696  data: 0.6403  max mem: 6052
[05:49:04.672107] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4825 (0.5090)  acc1: 84.3750 (84.3750)  acc5: 100.0000 (99.4318)  time: 0.0865  data: 0.0583  max mem: 6052
[05:49:04.955523] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4825 (0.4755)  acc1: 84.3750 (85.4911)  acc5: 100.0000 (99.4792)  time: 0.0282  data: 0.0002  max mem: 6052
[05:49:05.237527] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4806 (0.4866)  acc1: 85.9375 (85.3327)  acc5: 100.0000 (99.5464)  time: 0.0281  data: 0.0002  max mem: 6052
[05:49:05.517946] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4840 (0.4881)  acc1: 85.9375 (85.4040)  acc5: 100.0000 (99.3140)  time: 0.0280  data: 0.0002  max mem: 6052
[05:49:05.798696] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4770 (0.4845)  acc1: 85.9375 (85.6005)  acc5: 98.4375 (99.3260)  time: 0.0280  data: 0.0001  max mem: 6052
[05:49:06.080368] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4484 (0.4798)  acc1: 87.5000 (85.8094)  acc5: 100.0000 (99.2828)  time: 0.0280  data: 0.0001  max mem: 6052
[05:49:06.361048] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4510 (0.4765)  acc1: 85.9375 (85.9155)  acc5: 100.0000 (99.2738)  time: 0.0280  data: 0.0001  max mem: 6052
[05:49:06.642729] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5006 (0.4841)  acc1: 85.9375 (85.6096)  acc5: 100.0000 (99.2477)  time: 0.0280  data: 0.0002  max mem: 6052
[05:49:06.924068] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4571 (0.4810)  acc1: 85.9375 (85.7658)  acc5: 100.0000 (99.2445)  time: 0.0280  data: 0.0002  max mem: 6052
[05:49:07.217140] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4702 (0.4861)  acc1: 85.9375 (85.6900)  acc5: 100.0000 (99.2265)  time: 0.0286  data: 0.0001  max mem: 6052
[05:49:07.501001] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5046 (0.4871)  acc1: 85.9375 (85.6841)  acc5: 100.0000 (99.2258)  time: 0.0287  data: 0.0002  max mem: 6052
[05:49:07.785850] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4800 (0.4856)  acc1: 85.9375 (85.6663)  acc5: 100.0000 (99.2639)  time: 0.0283  data: 0.0002  max mem: 6052
[05:49:08.071495] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4478 (0.4866)  acc1: 85.9375 (85.7109)  acc5: 100.0000 (99.2844)  time: 0.0284  data: 0.0002  max mem: 6052
[05:49:08.352662] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4445 (0.4830)  acc1: 85.9375 (85.8710)  acc5: 100.0000 (99.3240)  time: 0.0282  data: 0.0001  max mem: 6052
[05:49:08.630410] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4573 (0.4819)  acc1: 85.9375 (85.8030)  acc5: 100.0000 (99.3377)  time: 0.0278  data: 0.0001  max mem: 6052
[05:49:08.780782] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4588 (0.4830)  acc1: 84.3750 (85.6800)  acc5: 100.0000 (99.3400)  time: 0.0269  data: 0.0001  max mem: 6052
[05:49:08.932642] Test: Total time: 0:00:05 (0.0332 s / it)
[05:49:08.933084] * Acc@1 85.680 Acc@5 99.340 loss 0.483
[05:49:08.933380] Accuracy of the network on the 10000 test images: 85.7%
[05:49:08.933593] Max accuracy: 85.68%
[05:49:09.199252] log_dir: ./output_dir
[05:49:10.050496] Epoch: [73]  [  0/781]  eta: 0:11:03  lr: 0.000047  training_loss: 1.1523 (1.1523)  classification_loss: 1.1522 (1.1522)  loss_mask: 0.0001 (0.0001)  time: 0.8497  data: 0.6557  max mem: 6052
[05:49:13.466700] Epoch: [73]  [ 20/781]  eta: 0:02:34  lr: 0.000047  training_loss: 1.1304 (1.1559)  classification_loss: 1.1303 (1.1558)  loss_mask: 0.0001 (0.0001)  time: 0.1707  data: 0.0002  max mem: 6052
[05:49:16.905471] Epoch: [73]  [ 40/781]  eta: 0:02:19  lr: 0.000047  training_loss: 1.1438 (1.1599)  classification_loss: 1.1436 (1.1598)  loss_mask: 0.0001 (0.0001)  time: 0.1719  data: 0.0002  max mem: 6052
[05:49:20.321767] Epoch: [73]  [ 60/781]  eta: 0:02:11  lr: 0.000047  training_loss: 1.2264 (1.1759)  classification_loss: 1.2262 (1.1757)  loss_mask: 0.0002 (0.0002)  time: 0.1707  data: 0.0002  max mem: 6052
[05:49:23.732183] Epoch: [73]  [ 80/781]  eta: 0:02:05  lr: 0.000047  training_loss: 1.1418 (1.1749)  classification_loss: 1.1417 (1.1747)  loss_mask: 0.0001 (0.0002)  time: 0.1705  data: 0.0002  max mem: 6052
[05:49:27.190403] Epoch: [73]  [100/781]  eta: 0:02:01  lr: 0.000047  training_loss: 1.1233 (1.1709)  classification_loss: 1.1232 (1.1707)  loss_mask: 0.0001 (0.0002)  time: 0.1728  data: 0.0003  max mem: 6052
[05:49:30.708927] Epoch: [73]  [120/781]  eta: 0:01:57  lr: 0.000047  training_loss: 1.1349 (1.1646)  classification_loss: 1.1348 (1.1645)  loss_mask: 0.0001 (0.0002)  time: 0.1758  data: 0.0002  max mem: 6052
[05:49:34.274771] Epoch: [73]  [140/781]  eta: 0:01:53  lr: 0.000047  training_loss: 1.1873 (1.1696)  classification_loss: 1.1872 (1.1695)  loss_mask: 0.0001 (0.0002)  time: 0.1782  data: 0.0002  max mem: 6052
[05:49:37.839274] Epoch: [73]  [160/781]  eta: 0:01:50  lr: 0.000047  training_loss: 1.1832 (1.1729)  classification_loss: 1.1830 (1.1727)  loss_mask: 0.0001 (0.0002)  time: 0.1781  data: 0.0003  max mem: 6052
[05:49:41.402493] Epoch: [73]  [180/781]  eta: 0:01:46  lr: 0.000047  training_loss: 1.1968 (1.1764)  classification_loss: 1.1967 (1.1762)  loss_mask: 0.0001 (0.0001)  time: 0.1781  data: 0.0002  max mem: 6052
[05:49:44.965785] Epoch: [73]  [200/781]  eta: 0:01:43  lr: 0.000047  training_loss: 1.1978 (1.1761)  classification_loss: 1.1977 (1.1760)  loss_mask: 0.0001 (0.0001)  time: 0.1781  data: 0.0002  max mem: 6052
[05:49:48.511791] Epoch: [73]  [220/781]  eta: 0:01:39  lr: 0.000047  training_loss: 1.1933 (1.1785)  classification_loss: 1.1932 (1.1783)  loss_mask: 0.0002 (0.0002)  time: 0.1772  data: 0.0002  max mem: 6052
[05:49:51.934482] Epoch: [73]  [240/781]  eta: 0:01:35  lr: 0.000046  training_loss: 1.1923 (1.1804)  classification_loss: 1.1922 (1.1802)  loss_mask: 0.0001 (0.0002)  time: 0.1710  data: 0.0002  max mem: 6052
[05:49:55.338714] Epoch: [73]  [260/781]  eta: 0:01:32  lr: 0.000046  training_loss: 1.1615 (1.1790)  classification_loss: 1.1614 (1.1788)  loss_mask: 0.0001 (0.0002)  time: 0.1701  data: 0.0002  max mem: 6052
[05:49:58.766632] Epoch: [73]  [280/781]  eta: 0:01:28  lr: 0.000046  training_loss: 1.1365 (1.1764)  classification_loss: 1.1364 (1.1763)  loss_mask: 0.0001 (0.0002)  time: 0.1713  data: 0.0003  max mem: 6052
[05:50:02.199851] Epoch: [73]  [300/781]  eta: 0:01:24  lr: 0.000046  training_loss: 1.1811 (1.1781)  classification_loss: 1.1810 (1.1780)  loss_mask: 0.0001 (0.0002)  time: 0.1716  data: 0.0002  max mem: 6052
[05:50:05.624414] Epoch: [73]  [320/781]  eta: 0:01:20  lr: 0.000046  training_loss: 1.2101 (1.1804)  classification_loss: 1.2099 (1.1802)  loss_mask: 0.0001 (0.0002)  time: 0.1712  data: 0.0003  max mem: 6052
[05:50:09.039208] Epoch: [73]  [340/781]  eta: 0:01:17  lr: 0.000046  training_loss: 1.0979 (1.1774)  classification_loss: 1.0979 (1.1772)  loss_mask: 0.0001 (0.0002)  time: 0.1707  data: 0.0002  max mem: 6052
[05:50:12.451042] Epoch: [73]  [360/781]  eta: 0:01:13  lr: 0.000046  training_loss: 1.1759 (1.1772)  classification_loss: 1.1757 (1.1771)  loss_mask: 0.0001 (0.0001)  time: 0.1705  data: 0.0002  max mem: 6052
[05:50:15.854648] Epoch: [73]  [380/781]  eta: 0:01:10  lr: 0.000046  training_loss: 1.1902 (1.1785)  classification_loss: 1.1901 (1.1783)  loss_mask: 0.0001 (0.0001)  time: 0.1701  data: 0.0001  max mem: 6052
[05:50:19.273406] Epoch: [73]  [400/781]  eta: 0:01:06  lr: 0.000046  training_loss: 1.1914 (1.1794)  classification_loss: 1.1912 (1.1792)  loss_mask: 0.0001 (0.0001)  time: 0.1708  data: 0.0002  max mem: 6052
[05:50:22.680685] Epoch: [73]  [420/781]  eta: 0:01:02  lr: 0.000046  training_loss: 1.1976 (1.1791)  classification_loss: 1.1975 (1.1790)  loss_mask: 0.0001 (0.0001)  time: 0.1703  data: 0.0003  max mem: 6052
[05:50:26.097718] Epoch: [73]  [440/781]  eta: 0:00:59  lr: 0.000046  training_loss: 1.1740 (1.1793)  classification_loss: 1.1738 (1.1791)  loss_mask: 0.0001 (0.0001)  time: 0.1708  data: 0.0003  max mem: 6052
[05:50:29.514494] Epoch: [73]  [460/781]  eta: 0:00:55  lr: 0.000046  training_loss: 1.1543 (1.1778)  classification_loss: 1.1542 (1.1776)  loss_mask: 0.0001 (0.0001)  time: 0.1707  data: 0.0002  max mem: 6052
[05:50:32.928024] Epoch: [73]  [480/781]  eta: 0:00:52  lr: 0.000045  training_loss: 1.1466 (1.1771)  classification_loss: 1.1466 (1.1769)  loss_mask: 0.0001 (0.0001)  time: 0.1706  data: 0.0002  max mem: 6052
[05:50:36.330104] Epoch: [73]  [500/781]  eta: 0:00:48  lr: 0.000045  training_loss: 1.1690 (1.1773)  classification_loss: 1.1689 (1.1772)  loss_mask: 0.0001 (0.0001)  time: 0.1700  data: 0.0002  max mem: 6052
[05:50:39.775728] Epoch: [73]  [520/781]  eta: 0:00:45  lr: 0.000045  training_loss: 1.1723 (1.1776)  classification_loss: 1.1722 (1.1774)  loss_mask: 0.0001 (0.0001)  time: 0.1722  data: 0.0003  max mem: 6052
[05:50:43.226902] Epoch: [73]  [540/781]  eta: 0:00:41  lr: 0.000045  training_loss: 1.1611 (1.1772)  classification_loss: 1.1610 (1.1771)  loss_mask: 0.0001 (0.0001)  time: 0.1725  data: 0.0002  max mem: 6052
[05:50:46.649243] Epoch: [73]  [560/781]  eta: 0:00:38  lr: 0.000045  training_loss: 1.1683 (1.1774)  classification_loss: 1.1655 (1.1772)  loss_mask: 0.0002 (0.0003)  time: 0.1710  data: 0.0004  max mem: 6052
[05:50:50.073245] Epoch: [73]  [580/781]  eta: 0:00:34  lr: 0.000045  training_loss: 1.2165 (1.1788)  classification_loss: 1.1385 (1.1775)  loss_mask: 0.0037 (0.0014)  time: 0.1711  data: 0.0002  max mem: 6052
[05:50:53.502503] Epoch: [73]  [600/781]  eta: 0:00:31  lr: 0.000045  training_loss: 1.2109 (1.1803)  classification_loss: 1.1617 (1.1772)  loss_mask: 0.0187 (0.0031)  time: 0.1714  data: 0.0002  max mem: 6052
[05:50:56.986037] Epoch: [73]  [620/781]  eta: 0:00:27  lr: 0.000045  training_loss: 1.1515 (1.1796)  classification_loss: 1.1505 (1.1764)  loss_mask: 0.0028 (0.0031)  time: 0.1741  data: 0.0003  max mem: 6052
[05:51:00.424175] Epoch: [73]  [640/781]  eta: 0:00:24  lr: 0.000045  training_loss: 1.1445 (1.1797)  classification_loss: 1.1436 (1.1766)  loss_mask: 0.0010 (0.0031)  time: 0.1718  data: 0.0002  max mem: 6052
[05:51:03.834146] Epoch: [73]  [660/781]  eta: 0:00:20  lr: 0.000045  training_loss: 1.2124 (1.1802)  classification_loss: 1.2117 (1.1772)  loss_mask: 0.0006 (0.0030)  time: 0.1704  data: 0.0002  max mem: 6052
[05:51:07.278384] Epoch: [73]  [680/781]  eta: 0:00:17  lr: 0.000045  training_loss: 1.1792 (1.1802)  classification_loss: 1.1781 (1.1772)  loss_mask: 0.0006 (0.0030)  time: 0.1721  data: 0.0002  max mem: 6052
[05:51:10.731354] Epoch: [73]  [700/781]  eta: 0:00:14  lr: 0.000045  training_loss: 1.2085 (1.1810)  classification_loss: 1.2074 (1.1781)  loss_mask: 0.0004 (0.0029)  time: 0.1726  data: 0.0002  max mem: 6052
[05:51:14.139951] Epoch: [73]  [720/781]  eta: 0:00:10  lr: 0.000044  training_loss: 1.2368 (1.1822)  classification_loss: 1.2364 (1.1794)  loss_mask: 0.0003 (0.0028)  time: 0.1704  data: 0.0002  max mem: 6052
[05:51:17.536126] Epoch: [73]  [740/781]  eta: 0:00:07  lr: 0.000044  training_loss: 1.1657 (1.1822)  classification_loss: 1.1653 (1.1795)  loss_mask: 0.0004 (0.0028)  time: 0.1697  data: 0.0003  max mem: 6052
[05:51:20.950048] Epoch: [73]  [760/781]  eta: 0:00:03  lr: 0.000044  training_loss: 1.2175 (1.1830)  classification_loss: 1.2170 (1.1803)  loss_mask: 0.0004 (0.0027)  time: 0.1706  data: 0.0002  max mem: 6052
[05:51:24.343071] Epoch: [73]  [780/781]  eta: 0:00:00  lr: 0.000044  training_loss: 1.1637 (1.1827)  classification_loss: 1.1632 (1.1800)  loss_mask: 0.0003 (0.0026)  time: 0.1696  data: 0.0002  max mem: 6052
[05:51:24.509774] Epoch: [73] Total time: 0:02:15 (0.1733 s / it)
[05:51:24.510459] Averaged stats: lr: 0.000044  training_loss: 1.1637 (1.1827)  classification_loss: 1.1632 (1.1800)  loss_mask: 0.0003 (0.0026)
[05:51:25.197281] Test:  [  0/157]  eta: 0:01:47  testing_loss: 0.4192 (0.4192)  acc1: 89.0625 (89.0625)  acc5: 96.8750 (96.8750)  time: 0.6824  data: 0.6505  max mem: 6052
[05:51:25.484950] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4981 (0.5018)  acc1: 82.8125 (84.9432)  acc5: 100.0000 (99.4318)  time: 0.0880  data: 0.0593  max mem: 6052
[05:51:25.770202] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4596 (0.4743)  acc1: 85.9375 (86.0119)  acc5: 100.0000 (99.4048)  time: 0.0285  data: 0.0001  max mem: 6052
[05:51:26.053116] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4573 (0.4821)  acc1: 85.9375 (85.4839)  acc5: 100.0000 (99.2944)  time: 0.0283  data: 0.0001  max mem: 6052
[05:51:26.337368] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4928 (0.4860)  acc1: 85.9375 (85.4040)  acc5: 98.4375 (99.0854)  time: 0.0282  data: 0.0001  max mem: 6052
[05:51:26.620623] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4915 (0.4813)  acc1: 85.9375 (85.8456)  acc5: 98.4375 (99.1728)  time: 0.0282  data: 0.0001  max mem: 6052
[05:51:26.905158] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4405 (0.4783)  acc1: 85.9375 (85.7838)  acc5: 100.0000 (99.1803)  time: 0.0282  data: 0.0002  max mem: 6052
[05:51:27.188184] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4571 (0.4747)  acc1: 85.9375 (85.8935)  acc5: 100.0000 (99.2298)  time: 0.0283  data: 0.0001  max mem: 6052
[05:51:27.473907] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4668 (0.4825)  acc1: 85.9375 (85.6096)  acc5: 100.0000 (99.2670)  time: 0.0283  data: 0.0001  max mem: 6052
[05:51:27.769202] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4629 (0.4795)  acc1: 85.9375 (85.7315)  acc5: 100.0000 (99.2617)  time: 0.0289  data: 0.0002  max mem: 6052
[05:51:28.053379] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4509 (0.4814)  acc1: 87.5000 (85.6745)  acc5: 100.0000 (99.2884)  time: 0.0288  data: 0.0002  max mem: 6052
[05:51:28.335484] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4662 (0.4809)  acc1: 85.9375 (85.6700)  acc5: 100.0000 (99.2680)  time: 0.0282  data: 0.0002  max mem: 6052
[05:51:28.617099] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4650 (0.4805)  acc1: 85.9375 (85.6921)  acc5: 100.0000 (99.2639)  time: 0.0280  data: 0.0002  max mem: 6052
[05:51:28.898790] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4866 (0.4818)  acc1: 85.9375 (85.6870)  acc5: 100.0000 (99.2844)  time: 0.0281  data: 0.0002  max mem: 6052
[05:51:29.179535] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4540 (0.4774)  acc1: 85.9375 (85.8710)  acc5: 100.0000 (99.3240)  time: 0.0280  data: 0.0001  max mem: 6052
[05:51:29.459527] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4565 (0.4778)  acc1: 85.9375 (85.7926)  acc5: 100.0000 (99.3377)  time: 0.0279  data: 0.0001  max mem: 6052
[05:51:29.611875] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4596 (0.4780)  acc1: 85.9375 (85.7500)  acc5: 100.0000 (99.3400)  time: 0.0270  data: 0.0001  max mem: 6052
[05:51:29.787023] Test: Total time: 0:00:05 (0.0336 s / it)
[05:51:29.787765] * Acc@1 85.750 Acc@5 99.340 loss 0.478
[05:51:29.788071] Accuracy of the network on the 10000 test images: 85.8%
[05:51:29.788302] Max accuracy: 85.75%
[05:51:30.040395] log_dir: ./output_dir
[05:51:30.942167] Epoch: [74]  [  0/781]  eta: 0:11:42  lr: 0.000044  training_loss: 1.1304 (1.1304)  classification_loss: 1.1302 (1.1302)  loss_mask: 0.0002 (0.0002)  time: 0.9000  data: 0.6859  max mem: 6052
[05:51:34.367861] Epoch: [74]  [ 20/781]  eta: 0:02:36  lr: 0.000044  training_loss: 1.1869 (1.1836)  classification_loss: 1.1867 (1.1833)  loss_mask: 0.0003 (0.0003)  time: 0.1712  data: 0.0003  max mem: 6052
[05:51:37.825159] Epoch: [74]  [ 40/781]  eta: 0:02:20  lr: 0.000044  training_loss: 1.1995 (1.1940)  classification_loss: 1.1992 (1.1937)  loss_mask: 0.0003 (0.0003)  time: 0.1728  data: 0.0002  max mem: 6052
[05:51:41.247088] Epoch: [74]  [ 60/781]  eta: 0:02:12  lr: 0.000044  training_loss: 1.2100 (1.1951)  classification_loss: 1.2096 (1.1948)  loss_mask: 0.0002 (0.0003)  time: 0.1710  data: 0.0003  max mem: 6052
[05:51:44.766075] Epoch: [74]  [ 80/781]  eta: 0:02:07  lr: 0.000044  training_loss: 1.1586 (1.1910)  classification_loss: 1.1584 (1.1907)  loss_mask: 0.0002 (0.0003)  time: 0.1759  data: 0.0003  max mem: 6052
[05:51:48.232865] Epoch: [74]  [100/781]  eta: 0:02:02  lr: 0.000044  training_loss: 1.1671 (1.1879)  classification_loss: 1.1669 (1.1876)  loss_mask: 0.0002 (0.0002)  time: 0.1732  data: 0.0003  max mem: 6052
[05:51:51.639525] Epoch: [74]  [120/781]  eta: 0:01:57  lr: 0.000044  training_loss: 1.1725 (1.1828)  classification_loss: 1.1721 (1.1825)  loss_mask: 0.0003 (0.0002)  time: 0.1702  data: 0.0003  max mem: 6052
[05:51:55.053232] Epoch: [74]  [140/781]  eta: 0:01:53  lr: 0.000044  training_loss: 1.0981 (1.1759)  classification_loss: 1.0980 (1.1757)  loss_mask: 0.0001 (0.0002)  time: 0.1706  data: 0.0003  max mem: 6052
[05:51:58.471632] Epoch: [74]  [160/781]  eta: 0:01:49  lr: 0.000044  training_loss: 1.1556 (1.1749)  classification_loss: 1.1554 (1.1746)  loss_mask: 0.0002 (0.0002)  time: 0.1708  data: 0.0003  max mem: 6052
[05:52:01.879551] Epoch: [74]  [180/781]  eta: 0:01:45  lr: 0.000044  training_loss: 1.2356 (1.1806)  classification_loss: 1.2091 (1.1795)  loss_mask: 0.0002 (0.0011)  time: 0.1703  data: 0.0002  max mem: 6052
[05:52:05.351788] Epoch: [74]  [200/781]  eta: 0:01:42  lr: 0.000043  training_loss: 1.2153 (1.1819)  classification_loss: 1.2017 (1.1799)  loss_mask: 0.0008 (0.0020)  time: 0.1735  data: 0.0004  max mem: 6052
[05:52:08.771725] Epoch: [74]  [220/781]  eta: 0:01:38  lr: 0.000043  training_loss: 1.1876 (1.1832)  classification_loss: 1.1842 (1.1810)  loss_mask: 0.0012 (0.0021)  time: 0.1709  data: 0.0002  max mem: 6052
[05:52:12.212352] Epoch: [74]  [240/781]  eta: 0:01:34  lr: 0.000043  training_loss: 1.1746 (1.1829)  classification_loss: 1.1745 (1.1809)  loss_mask: 0.0003 (0.0020)  time: 0.1719  data: 0.0002  max mem: 6052
[05:52:15.624944] Epoch: [74]  [260/781]  eta: 0:01:30  lr: 0.000043  training_loss: 1.1508 (1.1816)  classification_loss: 1.1504 (1.1798)  loss_mask: 0.0004 (0.0019)  time: 0.1705  data: 0.0003  max mem: 6052
[05:52:19.050218] Epoch: [74]  [280/781]  eta: 0:01:27  lr: 0.000043  training_loss: 1.1619 (1.1801)  classification_loss: 1.1618 (1.1783)  loss_mask: 0.0003 (0.0018)  time: 0.1712  data: 0.0004  max mem: 6052
[05:52:22.455229] Epoch: [74]  [300/781]  eta: 0:01:23  lr: 0.000043  training_loss: 1.1990 (1.1818)  classification_loss: 1.1987 (1.1802)  loss_mask: 0.0002 (0.0017)  time: 0.1702  data: 0.0003  max mem: 6052
[05:52:25.862868] Epoch: [74]  [320/781]  eta: 0:01:20  lr: 0.000043  training_loss: 1.2340 (1.1839)  classification_loss: 1.2334 (1.1823)  loss_mask: 0.0002 (0.0016)  time: 0.1703  data: 0.0002  max mem: 6052
[05:52:29.277188] Epoch: [74]  [340/781]  eta: 0:01:16  lr: 0.000043  training_loss: 1.1522 (1.1834)  classification_loss: 1.1519 (1.1819)  loss_mask: 0.0002 (0.0015)  time: 0.1706  data: 0.0003  max mem: 6052
[05:52:32.697904] Epoch: [74]  [360/781]  eta: 0:01:13  lr: 0.000043  training_loss: 1.1608 (1.1832)  classification_loss: 1.1607 (1.1815)  loss_mask: 0.0005 (0.0018)  time: 0.1710  data: 0.0002  max mem: 6052
[05:52:36.124804] Epoch: [74]  [380/781]  eta: 0:01:09  lr: 0.000043  training_loss: 1.2054 (1.1847)  classification_loss: 1.2044 (1.1829)  loss_mask: 0.0004 (0.0017)  time: 0.1713  data: 0.0002  max mem: 6052
[05:52:39.555554] Epoch: [74]  [400/781]  eta: 0:01:06  lr: 0.000043  training_loss: 1.1085 (1.1828)  classification_loss: 1.1083 (1.1811)  loss_mask: 0.0002 (0.0016)  time: 0.1715  data: 0.0002  max mem: 6052
[05:52:43.007390] Epoch: [74]  [420/781]  eta: 0:01:02  lr: 0.000043  training_loss: 1.1732 (1.1830)  classification_loss: 1.1730 (1.1814)  loss_mask: 0.0001 (0.0016)  time: 0.1725  data: 0.0002  max mem: 6052
[05:52:46.422533] Epoch: [74]  [440/781]  eta: 0:00:59  lr: 0.000043  training_loss: 1.1519 (1.1822)  classification_loss: 1.1517 (1.1806)  loss_mask: 0.0001 (0.0015)  time: 0.1707  data: 0.0002  max mem: 6052
[05:52:49.919502] Epoch: [74]  [460/781]  eta: 0:00:55  lr: 0.000042  training_loss: 1.0887 (1.1800)  classification_loss: 1.0885 (1.1786)  loss_mask: 0.0001 (0.0015)  time: 0.1748  data: 0.0002  max mem: 6052
[05:52:53.393936] Epoch: [74]  [480/781]  eta: 0:00:52  lr: 0.000042  training_loss: 1.2337 (1.1806)  classification_loss: 1.2335 (1.1791)  loss_mask: 0.0002 (0.0014)  time: 0.1736  data: 0.0002  max mem: 6052
[05:52:56.871580] Epoch: [74]  [500/781]  eta: 0:00:48  lr: 0.000042  training_loss: 1.1331 (1.1792)  classification_loss: 1.1327 (1.1778)  loss_mask: 0.0001 (0.0014)  time: 0.1738  data: 0.0002  max mem: 6052
[05:53:00.291056] Epoch: [74]  [520/781]  eta: 0:00:45  lr: 0.000042  training_loss: 1.2013 (1.1794)  classification_loss: 1.2012 (1.1781)  loss_mask: 0.0002 (0.0013)  time: 0.1709  data: 0.0005  max mem: 6052
[05:53:03.705077] Epoch: [74]  [540/781]  eta: 0:00:41  lr: 0.000042  training_loss: 1.2036 (1.1805)  classification_loss: 1.1962 (1.1791)  loss_mask: 0.0002 (0.0014)  time: 0.1706  data: 0.0003  max mem: 6052
[05:53:07.168257] Epoch: [74]  [560/781]  eta: 0:00:38  lr: 0.000042  training_loss: 1.1782 (1.1807)  classification_loss: 1.1778 (1.1792)  loss_mask: 0.0007 (0.0014)  time: 0.1731  data: 0.0004  max mem: 6052
[05:53:10.598108] Epoch: [74]  [580/781]  eta: 0:00:34  lr: 0.000042  training_loss: 1.1851 (1.1807)  classification_loss: 1.1849 (1.1793)  loss_mask: 0.0004 (0.0014)  time: 0.1714  data: 0.0002  max mem: 6052
[05:53:14.007900] Epoch: [74]  [600/781]  eta: 0:00:31  lr: 0.000042  training_loss: 1.1750 (1.1811)  classification_loss: 1.1749 (1.1797)  loss_mask: 0.0001 (0.0014)  time: 0.1704  data: 0.0002  max mem: 6052
[05:53:17.427597] Epoch: [74]  [620/781]  eta: 0:00:27  lr: 0.000042  training_loss: 1.1979 (1.1819)  classification_loss: 1.1976 (1.1806)  loss_mask: 0.0001 (0.0013)  time: 0.1709  data: 0.0002  max mem: 6052
[05:53:20.845321] Epoch: [74]  [640/781]  eta: 0:00:24  lr: 0.000042  training_loss: 1.2244 (1.1822)  classification_loss: 1.2243 (1.1809)  loss_mask: 0.0001 (0.0013)  time: 0.1708  data: 0.0002  max mem: 6052
[05:53:24.256711] Epoch: [74]  [660/781]  eta: 0:00:20  lr: 0.000042  training_loss: 1.1463 (1.1819)  classification_loss: 1.1461 (1.1807)  loss_mask: 0.0001 (0.0013)  time: 0.1705  data: 0.0002  max mem: 6052
[05:53:27.692625] Epoch: [74]  [680/781]  eta: 0:00:17  lr: 0.000042  training_loss: 1.2238 (1.1828)  classification_loss: 1.2236 (1.1816)  loss_mask: 0.0001 (0.0012)  time: 0.1717  data: 0.0002  max mem: 6052
[05:53:31.107092] Epoch: [74]  [700/781]  eta: 0:00:13  lr: 0.000041  training_loss: 1.1523 (1.1824)  classification_loss: 1.1522 (1.1812)  loss_mask: 0.0001 (0.0012)  time: 0.1706  data: 0.0002  max mem: 6052
[05:53:34.517728] Epoch: [74]  [720/781]  eta: 0:00:10  lr: 0.000041  training_loss: 1.1376 (1.1816)  classification_loss: 1.1375 (1.1804)  loss_mask: 0.0001 (0.0012)  time: 0.1705  data: 0.0003  max mem: 6052
[05:53:37.935890] Epoch: [74]  [740/781]  eta: 0:00:07  lr: 0.000041  training_loss: 1.1221 (1.1810)  classification_loss: 1.1220 (1.1799)  loss_mask: 0.0001 (0.0011)  time: 0.1708  data: 0.0004  max mem: 6052
[05:53:41.432438] Epoch: [74]  [760/781]  eta: 0:00:03  lr: 0.000041  training_loss: 1.1751 (1.1812)  classification_loss: 1.1749 (1.1801)  loss_mask: 0.0001 (0.0011)  time: 0.1747  data: 0.0003  max mem: 6052
[05:53:44.885800] Epoch: [74]  [780/781]  eta: 0:00:00  lr: 0.000041  training_loss: 1.2032 (1.1820)  classification_loss: 1.2029 (1.1809)  loss_mask: 0.0001 (0.0011)  time: 0.1726  data: 0.0002  max mem: 6052
[05:53:45.058584] Epoch: [74] Total time: 0:02:15 (0.1729 s / it)
[05:53:45.059028] Averaged stats: lr: 0.000041  training_loss: 1.2032 (1.1820)  classification_loss: 1.2029 (1.1809)  loss_mask: 0.0001 (0.0011)
[05:53:45.732856] Test:  [  0/157]  eta: 0:01:45  testing_loss: 0.4307 (0.4307)  acc1: 92.1875 (92.1875)  acc5: 100.0000 (100.0000)  time: 0.6697  data: 0.6393  max mem: 6052
[05:53:46.020300] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4893 (0.4965)  acc1: 85.9375 (85.5114)  acc5: 100.0000 (99.4318)  time: 0.0868  data: 0.0583  max mem: 6052
[05:53:46.314033] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4741 (0.4699)  acc1: 85.9375 (86.0119)  acc5: 100.0000 (99.4792)  time: 0.0289  data: 0.0002  max mem: 6052
[05:53:46.595847] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4679 (0.4791)  acc1: 87.5000 (85.5847)  acc5: 100.0000 (99.3952)  time: 0.0287  data: 0.0001  max mem: 6052
[05:53:46.881551] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4880 (0.4818)  acc1: 85.9375 (85.7088)  acc5: 100.0000 (99.2759)  time: 0.0283  data: 0.0002  max mem: 6052
[05:53:47.166269] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4771 (0.4740)  acc1: 85.9375 (86.0907)  acc5: 98.4375 (99.2341)  time: 0.0284  data: 0.0001  max mem: 6052
[05:53:47.450598] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4294 (0.4708)  acc1: 87.5000 (86.1168)  acc5: 100.0000 (99.2316)  time: 0.0283  data: 0.0001  max mem: 6052
[05:53:47.736016] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4405 (0.4681)  acc1: 85.9375 (86.2896)  acc5: 100.0000 (99.2738)  time: 0.0284  data: 0.0001  max mem: 6052
[05:53:48.021519] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4801 (0.4776)  acc1: 85.9375 (85.8218)  acc5: 100.0000 (99.2670)  time: 0.0284  data: 0.0002  max mem: 6052
[05:53:48.305757] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4817 (0.4754)  acc1: 84.3750 (85.9375)  acc5: 98.4375 (99.2102)  time: 0.0284  data: 0.0002  max mem: 6052
[05:53:48.588951] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4529 (0.4766)  acc1: 85.9375 (85.8447)  acc5: 100.0000 (99.2729)  time: 0.0283  data: 0.0002  max mem: 6052
[05:53:48.873242] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4712 (0.4775)  acc1: 85.9375 (85.8671)  acc5: 100.0000 (99.2680)  time: 0.0283  data: 0.0001  max mem: 6052
[05:53:49.156981] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4719 (0.4771)  acc1: 85.9375 (85.8600)  acc5: 100.0000 (99.2639)  time: 0.0283  data: 0.0001  max mem: 6052
[05:53:49.449277] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4709 (0.4780)  acc1: 85.9375 (85.9136)  acc5: 100.0000 (99.2486)  time: 0.0287  data: 0.0001  max mem: 6052
[05:53:49.731480] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4422 (0.4741)  acc1: 89.0625 (86.1148)  acc5: 100.0000 (99.2908)  time: 0.0286  data: 0.0001  max mem: 6052
[05:53:50.008938] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4674 (0.4745)  acc1: 85.9375 (86.0099)  acc5: 100.0000 (99.3377)  time: 0.0279  data: 0.0001  max mem: 6052
[05:53:50.157810] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5024 (0.4757)  acc1: 84.3750 (85.9500)  acc5: 100.0000 (99.3400)  time: 0.0268  data: 0.0001  max mem: 6052
[05:53:50.313530] Test: Total time: 0:00:05 (0.0334 s / it)
[05:53:50.314164] * Acc@1 85.950 Acc@5 99.340 loss 0.476
[05:53:50.314456] Accuracy of the network on the 10000 test images: 86.0%
[05:53:50.314628] Max accuracy: 85.95%
[05:53:50.573409] log_dir: ./output_dir
[05:53:51.460040] Epoch: [75]  [  0/781]  eta: 0:11:31  lr: 0.000041  training_loss: 1.2313 (1.2313)  classification_loss: 1.2312 (1.2312)  loss_mask: 0.0000 (0.0000)  time: 0.8850  data: 0.6910  max mem: 6052
[05:53:54.890366] Epoch: [75]  [ 20/781]  eta: 0:02:36  lr: 0.000041  training_loss: 1.1467 (1.1558)  classification_loss: 1.1467 (1.1557)  loss_mask: 0.0001 (0.0001)  time: 0.1714  data: 0.0002  max mem: 6052
[05:53:58.353262] Epoch: [75]  [ 40/781]  eta: 0:02:20  lr: 0.000041  training_loss: 1.1956 (1.1663)  classification_loss: 1.1955 (1.1662)  loss_mask: 0.0001 (0.0001)  time: 0.1731  data: 0.0002  max mem: 6052
[05:54:01.761570] Epoch: [75]  [ 60/781]  eta: 0:02:12  lr: 0.000041  training_loss: 1.1654 (1.1710)  classification_loss: 1.1654 (1.1709)  loss_mask: 0.0001 (0.0001)  time: 0.1703  data: 0.0002  max mem: 6052
[05:54:05.188459] Epoch: [75]  [ 80/781]  eta: 0:02:06  lr: 0.000041  training_loss: 1.1672 (1.1681)  classification_loss: 1.1670 (1.1680)  loss_mask: 0.0001 (0.0001)  time: 0.1713  data: 0.0002  max mem: 6052
[05:54:08.607557] Epoch: [75]  [100/781]  eta: 0:02:01  lr: 0.000041  training_loss: 1.1717 (1.1762)  classification_loss: 1.1715 (1.1704)  loss_mask: 0.0001 (0.0058)  time: 0.1708  data: 0.0002  max mem: 6052
[05:54:12.018901] Epoch: [75]  [120/781]  eta: 0:01:57  lr: 0.000041  training_loss: 1.2303 (1.1920)  classification_loss: 1.1917 (1.1746)  loss_mask: 0.0111 (0.0174)  time: 0.1705  data: 0.0002  max mem: 6052
[05:54:15.436525] Epoch: [75]  [140/781]  eta: 0:01:52  lr: 0.000041  training_loss: 1.1585 (1.1863)  classification_loss: 1.1532 (1.1706)  loss_mask: 0.0015 (0.0157)  time: 0.1708  data: 0.0002  max mem: 6052
[05:54:18.845463] Epoch: [75]  [160/781]  eta: 0:01:48  lr: 0.000041  training_loss: 1.1477 (1.1849)  classification_loss: 1.1472 (1.1710)  loss_mask: 0.0006 (0.0139)  time: 0.1704  data: 0.0002  max mem: 6052
[05:54:22.256232] Epoch: [75]  [180/781]  eta: 0:01:45  lr: 0.000040  training_loss: 1.1451 (1.1834)  classification_loss: 1.1426 (1.1709)  loss_mask: 0.0005 (0.0124)  time: 0.1704  data: 0.0003  max mem: 6052
[05:54:25.676127] Epoch: [75]  [200/781]  eta: 0:01:41  lr: 0.000040  training_loss: 1.1686 (1.1815)  classification_loss: 1.1681 (1.1703)  loss_mask: 0.0003 (0.0112)  time: 0.1709  data: 0.0002  max mem: 6052
[05:54:29.103786] Epoch: [75]  [220/781]  eta: 0:01:37  lr: 0.000040  training_loss: 1.1923 (1.1832)  classification_loss: 1.1922 (1.1729)  loss_mask: 0.0004 (0.0103)  time: 0.1713  data: 0.0002  max mem: 6052
[05:54:32.551138] Epoch: [75]  [240/781]  eta: 0:01:34  lr: 0.000040  training_loss: 1.1849 (1.1844)  classification_loss: 1.1846 (1.1750)  loss_mask: 0.0003 (0.0094)  time: 0.1722  data: 0.0003  max mem: 6052
[05:54:35.965947] Epoch: [75]  [260/781]  eta: 0:01:30  lr: 0.000040  training_loss: 1.1720 (1.1849)  classification_loss: 1.1715 (1.1762)  loss_mask: 0.0003 (0.0087)  time: 0.1707  data: 0.0002  max mem: 6052
[05:54:39.374746] Epoch: [75]  [280/781]  eta: 0:01:26  lr: 0.000040  training_loss: 1.1970 (1.1838)  classification_loss: 1.1968 (1.1756)  loss_mask: 0.0002 (0.0081)  time: 0.1704  data: 0.0002  max mem: 6052
[05:54:42.811318] Epoch: [75]  [300/781]  eta: 0:01:23  lr: 0.000040  training_loss: 1.1675 (1.1853)  classification_loss: 1.1672 (1.1777)  loss_mask: 0.0003 (0.0076)  time: 0.1718  data: 0.0001  max mem: 6052
[05:54:46.301198] Epoch: [75]  [320/781]  eta: 0:01:19  lr: 0.000040  training_loss: 1.1560 (1.1848)  classification_loss: 1.1558 (1.1776)  loss_mask: 0.0002 (0.0072)  time: 0.1744  data: 0.0002  max mem: 6052
[05:54:49.716498] Epoch: [75]  [340/781]  eta: 0:01:16  lr: 0.000040  training_loss: 1.1401 (1.1823)  classification_loss: 1.1399 (1.1756)  loss_mask: 0.0002 (0.0068)  time: 0.1707  data: 0.0002  max mem: 6052
[05:54:53.130364] Epoch: [75]  [360/781]  eta: 0:01:12  lr: 0.000040  training_loss: 1.1643 (1.1821)  classification_loss: 1.1640 (1.1757)  loss_mask: 0.0002 (0.0064)  time: 0.1706  data: 0.0002  max mem: 6052
[05:54:56.591558] Epoch: [75]  [380/781]  eta: 0:01:09  lr: 0.000040  training_loss: 1.1949 (1.1837)  classification_loss: 1.1947 (1.1777)  loss_mask: 0.0002 (0.0061)  time: 0.1730  data: 0.0002  max mem: 6052
[05:55:00.016796] Epoch: [75]  [400/781]  eta: 0:01:05  lr: 0.000040  training_loss: 1.1617 (1.1840)  classification_loss: 1.1615 (1.1782)  loss_mask: 0.0002 (0.0058)  time: 0.1712  data: 0.0003  max mem: 6052
[05:55:03.433756] Epoch: [75]  [420/781]  eta: 0:01:02  lr: 0.000040  training_loss: 1.1723 (1.1841)  classification_loss: 1.1721 (1.1786)  loss_mask: 0.0002 (0.0055)  time: 0.1708  data: 0.0003  max mem: 6052
[05:55:06.838000] Epoch: [75]  [440/781]  eta: 0:00:58  lr: 0.000039  training_loss: 1.1577 (1.1820)  classification_loss: 1.1575 (1.1767)  loss_mask: 0.0002 (0.0053)  time: 0.1701  data: 0.0002  max mem: 6052
[05:55:10.263233] Epoch: [75]  [460/781]  eta: 0:00:55  lr: 0.000039  training_loss: 1.1124 (1.1798)  classification_loss: 1.1121 (1.1747)  loss_mask: 0.0002 (0.0051)  time: 0.1712  data: 0.0002  max mem: 6052
[05:55:13.685906] Epoch: [75]  [480/781]  eta: 0:00:51  lr: 0.000039  training_loss: 1.1475 (1.1789)  classification_loss: 1.1473 (1.1741)  loss_mask: 0.0002 (0.0049)  time: 0.1710  data: 0.0003  max mem: 6052
[05:55:17.097784] Epoch: [75]  [500/781]  eta: 0:00:48  lr: 0.000039  training_loss: 1.1768 (1.1789)  classification_loss: 1.1768 (1.1742)  loss_mask: 0.0002 (0.0047)  time: 0.1705  data: 0.0002  max mem: 6052
[05:55:20.551852] Epoch: [75]  [520/781]  eta: 0:00:45  lr: 0.000039  training_loss: 1.1502 (1.1787)  classification_loss: 1.1500 (1.1742)  loss_mask: 0.0002 (0.0045)  time: 0.1726  data: 0.0002  max mem: 6052
[05:55:23.966521] Epoch: [75]  [540/781]  eta: 0:00:41  lr: 0.000039  training_loss: 1.1622 (1.1784)  classification_loss: 1.1620 (1.1740)  loss_mask: 0.0002 (0.0043)  time: 0.1707  data: 0.0002  max mem: 6052
[05:55:27.382958] Epoch: [75]  [560/781]  eta: 0:00:38  lr: 0.000039  training_loss: 1.2094 (1.1786)  classification_loss: 1.2094 (1.1744)  loss_mask: 0.0001 (0.0042)  time: 0.1707  data: 0.0004  max mem: 6052
[05:55:30.795871] Epoch: [75]  [580/781]  eta: 0:00:34  lr: 0.000039  training_loss: 1.1642 (1.1778)  classification_loss: 1.1641 (1.1737)  loss_mask: 0.0001 (0.0041)  time: 0.1706  data: 0.0002  max mem: 6052
[05:55:34.213860] Epoch: [75]  [600/781]  eta: 0:00:31  lr: 0.000039  training_loss: 1.1709 (1.1778)  classification_loss: 1.1707 (1.1739)  loss_mask: 0.0002 (0.0039)  time: 0.1708  data: 0.0002  max mem: 6052
[05:55:37.629376] Epoch: [75]  [620/781]  eta: 0:00:27  lr: 0.000039  training_loss: 1.1661 (1.1789)  classification_loss: 1.1658 (1.1751)  loss_mask: 0.0002 (0.0038)  time: 0.1707  data: 0.0002  max mem: 6052
[05:55:41.042023] Epoch: [75]  [640/781]  eta: 0:00:24  lr: 0.000039  training_loss: 1.1633 (1.1782)  classification_loss: 1.1631 (1.1745)  loss_mask: 0.0002 (0.0037)  time: 0.1706  data: 0.0003  max mem: 6052
[05:55:44.483381] Epoch: [75]  [660/781]  eta: 0:00:20  lr: 0.000039  training_loss: 1.1592 (1.1777)  classification_loss: 1.1590 (1.1741)  loss_mask: 0.0002 (0.0036)  time: 0.1720  data: 0.0003  max mem: 6052
[05:55:47.904918] Epoch: [75]  [680/781]  eta: 0:00:17  lr: 0.000039  training_loss: 1.1870 (1.1784)  classification_loss: 1.1866 (1.1749)  loss_mask: 0.0002 (0.0035)  time: 0.1710  data: 0.0003  max mem: 6052
[05:55:51.343757] Epoch: [75]  [700/781]  eta: 0:00:13  lr: 0.000039  training_loss: 1.1437 (1.1780)  classification_loss: 1.1434 (1.1746)  loss_mask: 0.0002 (0.0034)  time: 0.1719  data: 0.0002  max mem: 6052
[05:55:54.799556] Epoch: [75]  [720/781]  eta: 0:00:10  lr: 0.000038  training_loss: 1.2222 (1.1791)  classification_loss: 1.2219 (1.1758)  loss_mask: 0.0001 (0.0033)  time: 0.1727  data: 0.0003  max mem: 6052
[05:55:58.222017] Epoch: [75]  [740/781]  eta: 0:00:07  lr: 0.000038  training_loss: 1.1764 (1.1788)  classification_loss: 1.1762 (1.1756)  loss_mask: 0.0001 (0.0032)  time: 0.1710  data: 0.0002  max mem: 6052
[05:56:01.670122] Epoch: [75]  [760/781]  eta: 0:00:03  lr: 0.000038  training_loss: 1.1703 (1.1789)  classification_loss: 1.1702 (1.1758)  loss_mask: 0.0001 (0.0031)  time: 0.1723  data: 0.0002  max mem: 6052
[05:56:05.057891] Epoch: [75]  [780/781]  eta: 0:00:00  lr: 0.000038  training_loss: 1.1781 (1.1796)  classification_loss: 1.1780 (1.1766)  loss_mask: 0.0001 (0.0031)  time: 0.1693  data: 0.0001  max mem: 6052
[05:56:05.218820] Epoch: [75] Total time: 0:02:14 (0.1724 s / it)
[05:56:05.220346] Averaged stats: lr: 0.000038  training_loss: 1.1781 (1.1796)  classification_loss: 1.1780 (1.1766)  loss_mask: 0.0001 (0.0031)
[05:56:05.902290] Test:  [  0/157]  eta: 0:01:46  testing_loss: 0.4385 (0.4385)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.6770  data: 0.6473  max mem: 6052
[05:56:06.189700] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5115 (0.4918)  acc1: 82.8125 (84.2330)  acc5: 100.0000 (99.5739)  time: 0.0875  data: 0.0590  max mem: 6052
[05:56:06.471332] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4449 (0.4633)  acc1: 84.3750 (85.3423)  acc5: 100.0000 (99.6280)  time: 0.0283  data: 0.0001  max mem: 6052
[05:56:06.752693] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4361 (0.4707)  acc1: 85.9375 (85.1815)  acc5: 100.0000 (99.4960)  time: 0.0280  data: 0.0001  max mem: 6052
[05:56:07.036916] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4866 (0.4742)  acc1: 85.9375 (85.3659)  acc5: 98.4375 (99.2759)  time: 0.0282  data: 0.0002  max mem: 6052
[05:56:07.317949] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4635 (0.4728)  acc1: 85.9375 (85.7230)  acc5: 98.4375 (99.2034)  time: 0.0281  data: 0.0002  max mem: 6052
[05:56:07.598734] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4597 (0.4704)  acc1: 85.9375 (85.8350)  acc5: 100.0000 (99.2059)  time: 0.0279  data: 0.0001  max mem: 6052
[05:56:07.879006] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4490 (0.4661)  acc1: 85.9375 (86.0475)  acc5: 100.0000 (99.2518)  time: 0.0279  data: 0.0001  max mem: 6052
[05:56:08.160183] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4924 (0.4748)  acc1: 84.3750 (85.6481)  acc5: 100.0000 (99.2670)  time: 0.0280  data: 0.0001  max mem: 6052
[05:56:08.441966] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4670 (0.4701)  acc1: 85.9375 (85.8516)  acc5: 100.0000 (99.2788)  time: 0.0280  data: 0.0001  max mem: 6052
[05:56:08.728688] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4560 (0.4741)  acc1: 85.9375 (85.6900)  acc5: 100.0000 (99.3038)  time: 0.0283  data: 0.0002  max mem: 6052
[05:56:09.008622] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4740 (0.4731)  acc1: 84.3750 (85.7686)  acc5: 100.0000 (99.2821)  time: 0.0282  data: 0.0001  max mem: 6052
[05:56:09.288422] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4452 (0.4716)  acc1: 84.3750 (85.7825)  acc5: 100.0000 (99.3027)  time: 0.0279  data: 0.0001  max mem: 6052
[05:56:09.569564] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4301 (0.4725)  acc1: 84.3750 (85.7944)  acc5: 100.0000 (99.3082)  time: 0.0279  data: 0.0001  max mem: 6052
[05:56:09.848485] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4339 (0.4699)  acc1: 85.9375 (85.9153)  acc5: 100.0000 (99.3351)  time: 0.0279  data: 0.0001  max mem: 6052
[05:56:10.125934] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4577 (0.4702)  acc1: 85.9375 (85.8651)  acc5: 100.0000 (99.3377)  time: 0.0277  data: 0.0001  max mem: 6052
[05:56:10.274427] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4657 (0.4709)  acc1: 85.9375 (85.8400)  acc5: 100.0000 (99.3400)  time: 0.0267  data: 0.0001  max mem: 6052
[05:56:10.429925] Test: Total time: 0:00:05 (0.0332 s / it)
[05:56:10.430473] * Acc@1 85.840 Acc@5 99.340 loss 0.471
[05:56:10.430764] Accuracy of the network on the 10000 test images: 85.8%
[05:56:10.430943] Max accuracy: 85.95%
[05:56:10.636526] log_dir: ./output_dir
[05:56:11.507916] Epoch: [76]  [  0/781]  eta: 0:11:18  lr: 0.000038  training_loss: 1.1721 (1.1721)  classification_loss: 1.1719 (1.1719)  loss_mask: 0.0001 (0.0001)  time: 0.8693  data: 0.6657  max mem: 6052
[05:56:14.923806] Epoch: [76]  [ 20/781]  eta: 0:02:35  lr: 0.000038  training_loss: 1.0973 (1.1385)  classification_loss: 1.0971 (1.1384)  loss_mask: 0.0001 (0.0002)  time: 0.1707  data: 0.0003  max mem: 6052
[05:56:18.350774] Epoch: [76]  [ 40/781]  eta: 0:02:19  lr: 0.000038  training_loss: 1.1751 (1.1607)  classification_loss: 1.1749 (1.1605)  loss_mask: 0.0001 (0.0002)  time: 0.1713  data: 0.0002  max mem: 6052
[05:56:21.765185] Epoch: [76]  [ 60/781]  eta: 0:02:11  lr: 0.000038  training_loss: 1.1761 (1.1644)  classification_loss: 1.1760 (1.1642)  loss_mask: 0.0001 (0.0001)  time: 0.1706  data: 0.0002  max mem: 6052
[05:56:25.190717] Epoch: [76]  [ 80/781]  eta: 0:02:05  lr: 0.000038  training_loss: 1.1535 (1.1613)  classification_loss: 1.1533 (1.1611)  loss_mask: 0.0001 (0.0001)  time: 0.1712  data: 0.0002  max mem: 6052
[05:56:28.605151] Epoch: [76]  [100/781]  eta: 0:02:01  lr: 0.000038  training_loss: 1.1569 (1.1649)  classification_loss: 1.1502 (1.1632)  loss_mask: 0.0002 (0.0017)  time: 0.1707  data: 0.0002  max mem: 6052
[05:56:32.031516] Epoch: [76]  [120/781]  eta: 0:01:56  lr: 0.000038  training_loss: 1.1749 (1.1675)  classification_loss: 1.1737 (1.1661)  loss_mask: 0.0002 (0.0015)  time: 0.1712  data: 0.0003  max mem: 6052
[05:56:35.457881] Epoch: [76]  [140/781]  eta: 0:01:52  lr: 0.000038  training_loss: 1.2123 (1.1725)  classification_loss: 1.2119 (1.1712)  loss_mask: 0.0002 (0.0013)  time: 0.1712  data: 0.0003  max mem: 6052
[05:56:38.900740] Epoch: [76]  [160/781]  eta: 0:01:48  lr: 0.000038  training_loss: 1.1335 (1.1680)  classification_loss: 1.1334 (1.1668)  loss_mask: 0.0002 (0.0012)  time: 0.1721  data: 0.0003  max mem: 6052
[05:56:42.362002] Epoch: [76]  [180/781]  eta: 0:01:45  lr: 0.000038  training_loss: 1.1479 (1.1691)  classification_loss: 1.1477 (1.1681)  loss_mask: 0.0001 (0.0011)  time: 0.1730  data: 0.0003  max mem: 6052
[05:56:45.801314] Epoch: [76]  [200/781]  eta: 0:01:41  lr: 0.000037  training_loss: 1.1362 (1.1690)  classification_loss: 1.1361 (1.1681)  loss_mask: 0.0002 (0.0010)  time: 0.1719  data: 0.0002  max mem: 6052
[05:56:49.259823] Epoch: [76]  [220/781]  eta: 0:01:37  lr: 0.000037  training_loss: 1.1392 (1.1682)  classification_loss: 1.1391 (1.1673)  loss_mask: 0.0001 (0.0009)  time: 0.1728  data: 0.0002  max mem: 6052
[05:56:52.691027] Epoch: [76]  [240/781]  eta: 0:01:34  lr: 0.000037  training_loss: 1.1463 (1.1688)  classification_loss: 1.1463 (1.1679)  loss_mask: 0.0002 (0.0008)  time: 0.1715  data: 0.0002  max mem: 6052
[05:56:56.141615] Epoch: [76]  [260/781]  eta: 0:01:30  lr: 0.000037  training_loss: 1.1846 (1.1694)  classification_loss: 1.1845 (1.1686)  loss_mask: 0.0001 (0.0008)  time: 0.1724  data: 0.0002  max mem: 6052
[05:56:59.559687] Epoch: [76]  [280/781]  eta: 0:01:27  lr: 0.000037  training_loss: 1.1423 (1.1682)  classification_loss: 1.1422 (1.1675)  loss_mask: 0.0001 (0.0007)  time: 0.1708  data: 0.0002  max mem: 6052
[05:57:03.003636] Epoch: [76]  [300/781]  eta: 0:01:23  lr: 0.000037  training_loss: 1.1747 (1.1683)  classification_loss: 1.1745 (1.1676)  loss_mask: 0.0001 (0.0007)  time: 0.1721  data: 0.0004  max mem: 6052
[05:57:06.441500] Epoch: [76]  [320/781]  eta: 0:01:20  lr: 0.000037  training_loss: 1.1448 (1.1682)  classification_loss: 1.1446 (1.1675)  loss_mask: 0.0001 (0.0007)  time: 0.1718  data: 0.0002  max mem: 6052
[05:57:09.862166] Epoch: [76]  [340/781]  eta: 0:01:16  lr: 0.000037  training_loss: 1.1386 (1.1661)  classification_loss: 1.1382 (1.1655)  loss_mask: 0.0001 (0.0006)  time: 0.1710  data: 0.0002  max mem: 6052
[05:57:13.285659] Epoch: [76]  [360/781]  eta: 0:01:13  lr: 0.000037  training_loss: 1.1475 (1.1665)  classification_loss: 1.1473 (1.1659)  loss_mask: 0.0001 (0.0006)  time: 0.1711  data: 0.0002  max mem: 6052
[05:57:16.728793] Epoch: [76]  [380/781]  eta: 0:01:09  lr: 0.000037  training_loss: 1.1647 (1.1668)  classification_loss: 1.1646 (1.1663)  loss_mask: 0.0001 (0.0006)  time: 0.1721  data: 0.0002  max mem: 6052
[05:57:20.154179] Epoch: [76]  [400/781]  eta: 0:01:06  lr: 0.000037  training_loss: 1.1920 (1.1679)  classification_loss: 1.1918 (1.1673)  loss_mask: 0.0001 (0.0006)  time: 0.1712  data: 0.0003  max mem: 6052
[05:57:23.561633] Epoch: [76]  [420/781]  eta: 0:01:02  lr: 0.000037  training_loss: 1.1418 (1.1676)  classification_loss: 1.1417 (1.1671)  loss_mask: 0.0001 (0.0005)  time: 0.1703  data: 0.0003  max mem: 6052
[05:57:27.006766] Epoch: [76]  [440/781]  eta: 0:00:59  lr: 0.000037  training_loss: 1.1686 (1.1668)  classification_loss: 1.1685 (1.1663)  loss_mask: 0.0001 (0.0005)  time: 0.1722  data: 0.0003  max mem: 6052
[05:57:30.418239] Epoch: [76]  [460/781]  eta: 0:00:55  lr: 0.000036  training_loss: 1.1080 (1.1652)  classification_loss: 1.1079 (1.1647)  loss_mask: 0.0001 (0.0005)  time: 0.1705  data: 0.0003  max mem: 6052
[05:57:33.834140] Epoch: [76]  [480/781]  eta: 0:00:52  lr: 0.000036  training_loss: 1.1859 (1.1677)  classification_loss: 1.1857 (1.1672)  loss_mask: 0.0001 (0.0005)  time: 0.1707  data: 0.0002  max mem: 6052
[05:57:37.250090] Epoch: [76]  [500/781]  eta: 0:00:48  lr: 0.000036  training_loss: 1.1950 (1.1680)  classification_loss: 1.1950 (1.1675)  loss_mask: 0.0001 (0.0005)  time: 0.1707  data: 0.0003  max mem: 6052
[05:57:40.678915] Epoch: [76]  [520/781]  eta: 0:00:45  lr: 0.000036  training_loss: 1.1631 (1.1686)  classification_loss: 1.1630 (1.1681)  loss_mask: 0.0001 (0.0005)  time: 0.1714  data: 0.0003  max mem: 6052
[05:57:44.087538] Epoch: [76]  [540/781]  eta: 0:00:41  lr: 0.000036  training_loss: 1.1544 (1.1691)  classification_loss: 1.1543 (1.1687)  loss_mask: 0.0001 (0.0004)  time: 0.1703  data: 0.0002  max mem: 6052
[05:57:47.516149] Epoch: [76]  [560/781]  eta: 0:00:38  lr: 0.000036  training_loss: 1.1816 (1.1691)  classification_loss: 1.1815 (1.1687)  loss_mask: 0.0001 (0.0004)  time: 0.1714  data: 0.0003  max mem: 6052
[05:57:50.934331] Epoch: [76]  [580/781]  eta: 0:00:34  lr: 0.000036  training_loss: 1.1777 (1.1689)  classification_loss: 1.1777 (1.1685)  loss_mask: 0.0001 (0.0004)  time: 0.1708  data: 0.0002  max mem: 6052
[05:57:54.371980] Epoch: [76]  [600/781]  eta: 0:00:31  lr: 0.000036  training_loss: 1.1050 (1.1676)  classification_loss: 1.1049 (1.1672)  loss_mask: 0.0001 (0.0004)  time: 0.1718  data: 0.0002  max mem: 6052
[05:57:57.821433] Epoch: [76]  [620/781]  eta: 0:00:27  lr: 0.000036  training_loss: 1.1533 (1.1673)  classification_loss: 1.1532 (1.1669)  loss_mask: 0.0001 (0.0004)  time: 0.1724  data: 0.0002  max mem: 6052
[05:58:01.247017] Epoch: [76]  [640/781]  eta: 0:00:24  lr: 0.000036  training_loss: 1.1146 (1.1667)  classification_loss: 1.1145 (1.1663)  loss_mask: 0.0001 (0.0004)  time: 0.1712  data: 0.0002  max mem: 6052
[05:58:04.659149] Epoch: [76]  [660/781]  eta: 0:00:20  lr: 0.000036  training_loss: 1.1727 (1.1666)  classification_loss: 1.1727 (1.1662)  loss_mask: 0.0001 (0.0004)  time: 0.1705  data: 0.0002  max mem: 6052
[05:58:08.067175] Epoch: [76]  [680/781]  eta: 0:00:17  lr: 0.000036  training_loss: 1.1643 (1.1672)  classification_loss: 1.1641 (1.1668)  loss_mask: 0.0001 (0.0004)  time: 0.1703  data: 0.0003  max mem: 6052
[05:58:11.490707] Epoch: [76]  [700/781]  eta: 0:00:13  lr: 0.000036  training_loss: 1.1751 (1.1670)  classification_loss: 1.1748 (1.1666)  loss_mask: 0.0001 (0.0004)  time: 0.1711  data: 0.0002  max mem: 6052
[05:58:14.887764] Epoch: [76]  [720/781]  eta: 0:00:10  lr: 0.000036  training_loss: 1.2196 (1.1678)  classification_loss: 1.2196 (1.1675)  loss_mask: 0.0001 (0.0004)  time: 0.1698  data: 0.0003  max mem: 6052
[05:58:18.297962] Epoch: [76]  [740/781]  eta: 0:00:07  lr: 0.000035  training_loss: 1.1696 (1.1676)  classification_loss: 1.1696 (1.1672)  loss_mask: 0.0001 (0.0004)  time: 0.1704  data: 0.0002  max mem: 6052
[05:58:21.719381] Epoch: [76]  [760/781]  eta: 0:00:03  lr: 0.000035  training_loss: 1.1809 (1.1673)  classification_loss: 1.1808 (1.1670)  loss_mask: 0.0001 (0.0003)  time: 0.1710  data: 0.0002  max mem: 6052
[05:58:25.149772] Epoch: [76]  [780/781]  eta: 0:00:00  lr: 0.000035  training_loss: 1.1264 (1.1670)  classification_loss: 1.1263 (1.1666)  loss_mask: 0.0001 (0.0003)  time: 0.1714  data: 0.0002  max mem: 6052
[05:58:25.311172] Epoch: [76] Total time: 0:02:14 (0.1724 s / it)
[05:58:25.311612] Averaged stats: lr: 0.000035  training_loss: 1.1264 (1.1670)  classification_loss: 1.1263 (1.1666)  loss_mask: 0.0001 (0.0003)
[05:58:26.004980] Test:  [  0/157]  eta: 0:01:48  testing_loss: 0.4624 (0.4624)  acc1: 85.9375 (85.9375)  acc5: 100.0000 (100.0000)  time: 0.6886  data: 0.6580  max mem: 6052
[05:58:26.288077] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4953 (0.4959)  acc1: 84.3750 (84.6591)  acc5: 100.0000 (99.8580)  time: 0.0882  data: 0.0600  max mem: 6052
[05:58:26.568937] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4627 (0.4635)  acc1: 85.9375 (86.2351)  acc5: 100.0000 (99.4792)  time: 0.0280  data: 0.0002  max mem: 6052
[05:58:26.856371] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4511 (0.4734)  acc1: 85.9375 (85.9879)  acc5: 100.0000 (99.4960)  time: 0.0283  data: 0.0001  max mem: 6052
[05:58:27.137486] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4761 (0.4800)  acc1: 85.9375 (85.8613)  acc5: 100.0000 (99.2759)  time: 0.0283  data: 0.0001  max mem: 6052
[05:58:27.420851] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4691 (0.4766)  acc1: 84.3750 (85.9681)  acc5: 100.0000 (99.2647)  time: 0.0281  data: 0.0001  max mem: 6052
[05:58:27.703836] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4539 (0.4726)  acc1: 85.9375 (86.2449)  acc5: 100.0000 (99.2572)  time: 0.0282  data: 0.0001  max mem: 6052
[05:58:27.988673] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4536 (0.4692)  acc1: 85.9375 (86.0695)  acc5: 100.0000 (99.2738)  time: 0.0283  data: 0.0002  max mem: 6052
[05:58:28.272923] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4649 (0.4751)  acc1: 84.3750 (85.7446)  acc5: 100.0000 (99.2670)  time: 0.0283  data: 0.0002  max mem: 6052
[05:58:28.560991] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4524 (0.4726)  acc1: 85.9375 (85.8173)  acc5: 100.0000 (99.2617)  time: 0.0285  data: 0.0003  max mem: 6052
[05:58:28.845775] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4524 (0.4751)  acc1: 85.9375 (85.7209)  acc5: 100.0000 (99.3038)  time: 0.0285  data: 0.0003  max mem: 6052
[05:58:29.127874] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4831 (0.4762)  acc1: 85.9375 (85.7264)  acc5: 100.0000 (99.3102)  time: 0.0282  data: 0.0002  max mem: 6052
[05:58:29.409182] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4487 (0.4746)  acc1: 85.9375 (85.7825)  acc5: 100.0000 (99.3156)  time: 0.0281  data: 0.0001  max mem: 6052
[05:58:29.696870] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4397 (0.4756)  acc1: 85.9375 (85.7705)  acc5: 100.0000 (99.3201)  time: 0.0283  data: 0.0003  max mem: 6052
[05:58:29.975588] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4458 (0.4735)  acc1: 85.9375 (85.8932)  acc5: 100.0000 (99.3351)  time: 0.0282  data: 0.0003  max mem: 6052
[05:58:30.252375] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4794 (0.4743)  acc1: 85.9375 (85.8547)  acc5: 100.0000 (99.3688)  time: 0.0277  data: 0.0001  max mem: 6052
[05:58:30.401662] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4973 (0.4756)  acc1: 85.9375 (85.8000)  acc5: 100.0000 (99.3500)  time: 0.0267  data: 0.0001  max mem: 6052
[05:58:30.562924] Test: Total time: 0:00:05 (0.0334 s / it)
[05:58:30.564190] * Acc@1 85.800 Acc@5 99.350 loss 0.476
[05:58:30.565030] Accuracy of the network on the 10000 test images: 85.8%
[05:58:30.565639] Max accuracy: 85.95%
[05:58:30.819752] log_dir: ./output_dir
[05:58:31.666936] Epoch: [77]  [  0/781]  eta: 0:10:59  lr: 0.000035  training_loss: 1.1417 (1.1417)  classification_loss: 1.1416 (1.1416)  loss_mask: 0.0001 (0.0001)  time: 0.8448  data: 0.6340  max mem: 6052
[05:58:35.063973] Epoch: [77]  [ 20/781]  eta: 0:02:33  lr: 0.000035  training_loss: 1.1283 (1.1397)  classification_loss: 1.1281 (1.1396)  loss_mask: 0.0001 (0.0001)  time: 0.1698  data: 0.0002  max mem: 6052
[05:58:38.463438] Epoch: [77]  [ 40/781]  eta: 0:02:18  lr: 0.000035  training_loss: 1.1801 (1.1555)  classification_loss: 1.1800 (1.1554)  loss_mask: 0.0001 (0.0001)  time: 0.1699  data: 0.0002  max mem: 6052
[05:58:41.883363] Epoch: [77]  [ 60/781]  eta: 0:02:10  lr: 0.000035  training_loss: 1.1705 (1.1632)  classification_loss: 1.1704 (1.1631)  loss_mask: 0.0001 (0.0001)  time: 0.1709  data: 0.0002  max mem: 6052
[05:58:45.389547] Epoch: [77]  [ 80/781]  eta: 0:02:06  lr: 0.000035  training_loss: 1.1548 (1.1632)  classification_loss: 1.1548 (1.1631)  loss_mask: 0.0001 (0.0001)  time: 0.1752  data: 0.0002  max mem: 6052
[05:58:48.782398] Epoch: [77]  [100/781]  eta: 0:02:01  lr: 0.000035  training_loss: 1.1546 (1.1668)  classification_loss: 1.1544 (1.1667)  loss_mask: 0.0001 (0.0001)  time: 0.1696  data: 0.0001  max mem: 6052
[05:58:52.232880] Epoch: [77]  [120/781]  eta: 0:01:56  lr: 0.000035  training_loss: 1.1664 (1.1717)  classification_loss: 1.1663 (1.1716)  loss_mask: 0.0001 (0.0001)  time: 0.1724  data: 0.0002  max mem: 6052
[05:58:55.635989] Epoch: [77]  [140/781]  eta: 0:01:52  lr: 0.000035  training_loss: 1.1776 (1.1718)  classification_loss: 1.1775 (1.1717)  loss_mask: 0.0001 (0.0001)  time: 0.1701  data: 0.0002  max mem: 6052
[05:58:59.038508] Epoch: [77]  [160/781]  eta: 0:01:48  lr: 0.000035  training_loss: 1.1841 (1.1748)  classification_loss: 1.1840 (1.1747)  loss_mask: 0.0001 (0.0001)  time: 0.1700  data: 0.0001  max mem: 6052
[05:59:02.429192] Epoch: [77]  [180/781]  eta: 0:01:44  lr: 0.000035  training_loss: 1.1711 (1.1746)  classification_loss: 1.1710 (1.1745)  loss_mask: 0.0001 (0.0001)  time: 0.1694  data: 0.0002  max mem: 6052
[05:59:05.837860] Epoch: [77]  [200/781]  eta: 0:01:41  lr: 0.000035  training_loss: 1.1073 (1.1694)  classification_loss: 1.1072 (1.1693)  loss_mask: 0.0001 (0.0001)  time: 0.1703  data: 0.0003  max mem: 6052
[05:59:09.246996] Epoch: [77]  [220/781]  eta: 0:01:37  lr: 0.000035  training_loss: 1.1370 (1.1661)  classification_loss: 1.1370 (1.1660)  loss_mask: 0.0001 (0.0001)  time: 0.1704  data: 0.0002  max mem: 6052
[05:59:12.664009] Epoch: [77]  [240/781]  eta: 0:01:33  lr: 0.000034  training_loss: 1.0918 (1.1638)  classification_loss: 1.0917 (1.1637)  loss_mask: 0.0001 (0.0001)  time: 0.1708  data: 0.0002  max mem: 6052
[05:59:16.073424] Epoch: [77]  [260/781]  eta: 0:01:30  lr: 0.000034  training_loss: 1.1784 (1.1658)  classification_loss: 1.1784 (1.1657)  loss_mask: 0.0001 (0.0001)  time: 0.1704  data: 0.0002  max mem: 6052
[05:59:19.470428] Epoch: [77]  [280/781]  eta: 0:01:26  lr: 0.000034  training_loss: 1.1287 (1.1650)  classification_loss: 1.1286 (1.1649)  loss_mask: 0.0001 (0.0001)  time: 0.1698  data: 0.0003  max mem: 6052
[05:59:22.873781] Epoch: [77]  [300/781]  eta: 0:01:23  lr: 0.000034  training_loss: 1.2025 (1.1665)  classification_loss: 1.2024 (1.1664)  loss_mask: 0.0001 (0.0001)  time: 0.1701  data: 0.0002  max mem: 6052
[05:59:26.307653] Epoch: [77]  [320/781]  eta: 0:01:19  lr: 0.000034  training_loss: 1.1359 (1.1656)  classification_loss: 1.1358 (1.1655)  loss_mask: 0.0001 (0.0001)  time: 0.1716  data: 0.0002  max mem: 6052
[05:59:29.724463] Epoch: [77]  [340/781]  eta: 0:01:16  lr: 0.000034  training_loss: 1.0995 (1.1642)  classification_loss: 1.0994 (1.1641)  loss_mask: 0.0001 (0.0001)  time: 0.1708  data: 0.0003  max mem: 6052
[05:59:33.137521] Epoch: [77]  [360/781]  eta: 0:01:12  lr: 0.000034  training_loss: 1.1547 (1.1647)  classification_loss: 1.1546 (1.1646)  loss_mask: 0.0001 (0.0001)  time: 0.1706  data: 0.0002  max mem: 6052
[05:59:36.536916] Epoch: [77]  [380/781]  eta: 0:01:09  lr: 0.000034  training_loss: 1.2331 (1.1670)  classification_loss: 1.2331 (1.1669)  loss_mask: 0.0001 (0.0001)  time: 0.1699  data: 0.0003  max mem: 6052
[05:59:39.934415] Epoch: [77]  [400/781]  eta: 0:01:05  lr: 0.000034  training_loss: 1.1693 (1.1668)  classification_loss: 1.1692 (1.1667)  loss_mask: 0.0001 (0.0001)  time: 0.1698  data: 0.0002  max mem: 6052
[05:59:43.373162] Epoch: [77]  [420/781]  eta: 0:01:02  lr: 0.000034  training_loss: 1.1585 (1.1661)  classification_loss: 1.1584 (1.1660)  loss_mask: 0.0001 (0.0001)  time: 0.1718  data: 0.0003  max mem: 6052
[05:59:46.768317] Epoch: [77]  [440/781]  eta: 0:00:58  lr: 0.000034  training_loss: 1.1281 (1.1636)  classification_loss: 1.1281 (1.1635)  loss_mask: 0.0001 (0.0001)  time: 0.1697  data: 0.0002  max mem: 6052
[05:59:50.174449] Epoch: [77]  [460/781]  eta: 0:00:55  lr: 0.000034  training_loss: 1.1477 (1.1630)  classification_loss: 1.1476 (1.1629)  loss_mask: 0.0001 (0.0001)  time: 0.1702  data: 0.0002  max mem: 6052
[05:59:53.594512] Epoch: [77]  [480/781]  eta: 0:00:51  lr: 0.000034  training_loss: 1.1396 (1.1629)  classification_loss: 1.1395 (1.1629)  loss_mask: 0.0001 (0.0001)  time: 0.1709  data: 0.0002  max mem: 6052
[05:59:57.013402] Epoch: [77]  [500/781]  eta: 0:00:48  lr: 0.000034  training_loss: 1.1661 (1.1639)  classification_loss: 1.1661 (1.1638)  loss_mask: 0.0001 (0.0001)  time: 0.1709  data: 0.0002  max mem: 6052
[06:00:00.429613] Epoch: [77]  [520/781]  eta: 0:00:44  lr: 0.000033  training_loss: 1.1768 (1.1642)  classification_loss: 1.1767 (1.1641)  loss_mask: 0.0001 (0.0001)  time: 0.1707  data: 0.0003  max mem: 6052
[06:00:03.839663] Epoch: [77]  [540/781]  eta: 0:00:41  lr: 0.000033  training_loss: 1.1931 (1.1653)  classification_loss: 1.1931 (1.1652)  loss_mask: 0.0001 (0.0001)  time: 0.1704  data: 0.0002  max mem: 6052
[06:00:07.342938] Epoch: [77]  [560/781]  eta: 0:00:38  lr: 0.000033  training_loss: 1.1434 (1.1647)  classification_loss: 1.1433 (1.1646)  loss_mask: 0.0001 (0.0001)  time: 0.1751  data: 0.0002  max mem: 6052
[06:00:10.774287] Epoch: [77]  [580/781]  eta: 0:00:34  lr: 0.000033  training_loss: 1.1534 (1.1642)  classification_loss: 1.1533 (1.1641)  loss_mask: 0.0001 (0.0001)  time: 0.1714  data: 0.0002  max mem: 6052
[06:00:14.165388] Epoch: [77]  [600/781]  eta: 0:00:31  lr: 0.000033  training_loss: 1.1645 (1.1641)  classification_loss: 1.1644 (1.1640)  loss_mask: 0.0001 (0.0001)  time: 0.1695  data: 0.0003  max mem: 6052
[06:00:17.560537] Epoch: [77]  [620/781]  eta: 0:00:27  lr: 0.000033  training_loss: 1.1557 (1.1648)  classification_loss: 1.1557 (1.1647)  loss_mask: 0.0001 (0.0001)  time: 0.1697  data: 0.0002  max mem: 6052
[06:00:21.072087] Epoch: [77]  [640/781]  eta: 0:00:24  lr: 0.000033  training_loss: 1.1418 (1.1643)  classification_loss: 1.1418 (1.1642)  loss_mask: 0.0002 (0.0001)  time: 0.1755  data: 0.0002  max mem: 6052
[06:00:24.493392] Epoch: [77]  [660/781]  eta: 0:00:20  lr: 0.000033  training_loss: 1.1566 (1.1648)  classification_loss: 1.1564 (1.1647)  loss_mask: 0.0002 (0.0001)  time: 0.1710  data: 0.0002  max mem: 6052
[06:00:28.033914] Epoch: [77]  [680/781]  eta: 0:00:17  lr: 0.000033  training_loss: 1.1419 (1.1646)  classification_loss: 1.1419 (1.1645)  loss_mask: 0.0001 (0.0001)  time: 0.1770  data: 0.0002  max mem: 6052
[06:00:31.447068] Epoch: [77]  [700/781]  eta: 0:00:13  lr: 0.000033  training_loss: 1.1383 (1.1650)  classification_loss: 1.1382 (1.1649)  loss_mask: 0.0001 (0.0001)  time: 0.1706  data: 0.0003  max mem: 6052
[06:00:34.893839] Epoch: [77]  [720/781]  eta: 0:00:10  lr: 0.000033  training_loss: 1.2022 (1.1659)  classification_loss: 1.2021 (1.1658)  loss_mask: 0.0001 (0.0001)  time: 0.1723  data: 0.0002  max mem: 6052
[06:00:38.318467] Epoch: [77]  [740/781]  eta: 0:00:07  lr: 0.000033  training_loss: 1.1513 (1.1660)  classification_loss: 1.1512 (1.1659)  loss_mask: 0.0001 (0.0001)  time: 0.1711  data: 0.0002  max mem: 6052
[06:00:41.731658] Epoch: [77]  [760/781]  eta: 0:00:03  lr: 0.000033  training_loss: 1.1741 (1.1663)  classification_loss: 1.1740 (1.1662)  loss_mask: 0.0001 (0.0001)  time: 0.1706  data: 0.0002  max mem: 6052
[06:00:45.142811] Epoch: [77]  [780/781]  eta: 0:00:00  lr: 0.000033  training_loss: 1.1321 (1.1659)  classification_loss: 1.1320 (1.1658)  loss_mask: 0.0001 (0.0001)  time: 0.1705  data: 0.0002  max mem: 6052
[06:00:45.315534] Epoch: [77] Total time: 0:02:14 (0.1722 s / it)
[06:00:45.315972] Averaged stats: lr: 0.000033  training_loss: 1.1321 (1.1659)  classification_loss: 1.1320 (1.1658)  loss_mask: 0.0001 (0.0001)
[06:00:45.971048] Test:  [  0/157]  eta: 0:01:42  testing_loss: 0.4975 (0.4975)  acc1: 84.3750 (84.3750)  acc5: 98.4375 (98.4375)  time: 0.6512  data: 0.6222  max mem: 6052
[06:00:46.260318] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4677 (0.4864)  acc1: 84.3750 (85.3693)  acc5: 100.0000 (99.7159)  time: 0.0853  data: 0.0567  max mem: 6052
[06:00:46.540879] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4544 (0.4640)  acc1: 85.9375 (86.3095)  acc5: 100.0000 (99.6280)  time: 0.0283  data: 0.0001  max mem: 6052
[06:00:46.824261] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4554 (0.4757)  acc1: 87.5000 (86.1895)  acc5: 100.0000 (99.4960)  time: 0.0281  data: 0.0001  max mem: 6052
[06:00:47.105230] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4703 (0.4785)  acc1: 87.5000 (86.2043)  acc5: 98.4375 (99.2759)  time: 0.0281  data: 0.0001  max mem: 6052
[06:00:47.390480] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4680 (0.4756)  acc1: 87.5000 (86.3664)  acc5: 98.4375 (99.2034)  time: 0.0282  data: 0.0002  max mem: 6052
[06:00:47.670726] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4551 (0.4746)  acc1: 85.9375 (86.3986)  acc5: 100.0000 (99.2059)  time: 0.0282  data: 0.0002  max mem: 6052
[06:00:47.950823] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4596 (0.4718)  acc1: 85.9375 (86.3776)  acc5: 100.0000 (99.2738)  time: 0.0279  data: 0.0001  max mem: 6052
[06:00:48.236918] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4773 (0.4790)  acc1: 84.3750 (85.9954)  acc5: 100.0000 (99.2863)  time: 0.0282  data: 0.0002  max mem: 6052
[06:00:48.524739] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4621 (0.4767)  acc1: 84.3750 (86.1092)  acc5: 100.0000 (99.2273)  time: 0.0286  data: 0.0002  max mem: 6052
[06:00:48.813777] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4558 (0.4791)  acc1: 87.5000 (86.0149)  acc5: 98.4375 (99.2110)  time: 0.0287  data: 0.0002  max mem: 6052
[06:00:49.093984] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4594 (0.4783)  acc1: 85.9375 (86.0360)  acc5: 100.0000 (99.2117)  time: 0.0283  data: 0.0001  max mem: 6052
[06:00:49.375116] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4576 (0.4762)  acc1: 85.9375 (86.1183)  acc5: 100.0000 (99.2381)  time: 0.0280  data: 0.0001  max mem: 6052
[06:00:49.657153] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4382 (0.4772)  acc1: 84.3750 (86.2118)  acc5: 100.0000 (99.2486)  time: 0.0280  data: 0.0001  max mem: 6052
[06:00:49.937039] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4349 (0.4736)  acc1: 87.5000 (86.3364)  acc5: 100.0000 (99.2575)  time: 0.0280  data: 0.0001  max mem: 6052
[06:00:50.215645] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4502 (0.4741)  acc1: 87.5000 (86.2272)  acc5: 100.0000 (99.2550)  time: 0.0278  data: 0.0001  max mem: 6052
[06:00:50.365026] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4678 (0.4760)  acc1: 85.9375 (86.1600)  acc5: 100.0000 (99.2600)  time: 0.0269  data: 0.0001  max mem: 6052
[06:00:50.528254] Test: Total time: 0:00:05 (0.0332 s / it)
[06:00:50.528982] * Acc@1 86.160 Acc@5 99.260 loss 0.476
[06:00:50.529274] Accuracy of the network on the 10000 test images: 86.2%
[06:00:50.529465] Max accuracy: 86.16%
[06:00:50.706075] log_dir: ./output_dir
[06:00:51.579885] Epoch: [78]  [  0/781]  eta: 0:11:21  lr: 0.000033  training_loss: 1.2227 (1.2227)  classification_loss: 1.2227 (1.2227)  loss_mask: 0.0001 (0.0001)  time: 0.8721  data: 0.6873  max mem: 6052
[06:00:54.989742] Epoch: [78]  [ 20/781]  eta: 0:02:35  lr: 0.000032  training_loss: 1.1513 (1.1729)  classification_loss: 1.1512 (1.1728)  loss_mask: 0.0001 (0.0001)  time: 0.1704  data: 0.0002  max mem: 6052
[06:00:58.405940] Epoch: [78]  [ 40/781]  eta: 0:02:19  lr: 0.000032  training_loss: 1.1810 (1.1732)  classification_loss: 1.1809 (1.1731)  loss_mask: 0.0001 (0.0001)  time: 0.1707  data: 0.0003  max mem: 6052
[06:01:01.810587] Epoch: [78]  [ 60/781]  eta: 0:02:11  lr: 0.000032  training_loss: 1.1693 (1.1822)  classification_loss: 1.1692 (1.1821)  loss_mask: 0.0001 (0.0001)  time: 0.1702  data: 0.0003  max mem: 6052
[06:01:05.218034] Epoch: [78]  [ 80/781]  eta: 0:02:05  lr: 0.000032  training_loss: 1.1709 (1.1816)  classification_loss: 1.1708 (1.1815)  loss_mask: 0.0001 (0.0001)  time: 0.1703  data: 0.0002  max mem: 6052
[06:01:08.634194] Epoch: [78]  [100/781]  eta: 0:02:00  lr: 0.000032  training_loss: 1.2033 (1.1849)  classification_loss: 1.2033 (1.1848)  loss_mask: 0.0001 (0.0001)  time: 0.1707  data: 0.0003  max mem: 6052
[06:01:12.055751] Epoch: [78]  [120/781]  eta: 0:01:56  lr: 0.000032  training_loss: 1.1284 (1.1774)  classification_loss: 1.1283 (1.1773)  loss_mask: 0.0001 (0.0001)  time: 0.1710  data: 0.0002  max mem: 6052
[06:01:15.473174] Epoch: [78]  [140/781]  eta: 0:01:52  lr: 0.000032  training_loss: 1.1197 (1.1703)  classification_loss: 1.1196 (1.1702)  loss_mask: 0.0001 (0.0001)  time: 0.1708  data: 0.0004  max mem: 6052
[06:01:18.902377] Epoch: [78]  [160/781]  eta: 0:01:48  lr: 0.000032  training_loss: 1.0990 (1.1642)  classification_loss: 1.0989 (1.1641)  loss_mask: 0.0001 (0.0001)  time: 0.1714  data: 0.0004  max mem: 6052
[06:01:22.322605] Epoch: [78]  [180/781]  eta: 0:01:44  lr: 0.000032  training_loss: 1.1596 (1.1647)  classification_loss: 1.1595 (1.1647)  loss_mask: 0.0001 (0.0001)  time: 0.1709  data: 0.0002  max mem: 6052
[06:01:25.748144] Epoch: [78]  [200/781]  eta: 0:01:41  lr: 0.000032  training_loss: 1.1923 (1.1640)  classification_loss: 1.1922 (1.1639)  loss_mask: 0.0001 (0.0001)  time: 0.1712  data: 0.0002  max mem: 6052
[06:01:29.169570] Epoch: [78]  [220/781]  eta: 0:01:37  lr: 0.000032  training_loss: 1.1562 (1.1638)  classification_loss: 1.1561 (1.1638)  loss_mask: 0.0001 (0.0001)  time: 0.1710  data: 0.0003  max mem: 6052
[06:01:32.579368] Epoch: [78]  [240/781]  eta: 0:01:33  lr: 0.000032  training_loss: 1.1591 (1.1643)  classification_loss: 1.1590 (1.1642)  loss_mask: 0.0001 (0.0001)  time: 0.1704  data: 0.0002  max mem: 6052
[06:01:36.009013] Epoch: [78]  [260/781]  eta: 0:01:30  lr: 0.000032  training_loss: 1.1589 (1.1661)  classification_loss: 1.1588 (1.1660)  loss_mask: 0.0001 (0.0001)  time: 0.1714  data: 0.0002  max mem: 6052
[06:01:39.414045] Epoch: [78]  [280/781]  eta: 0:01:26  lr: 0.000032  training_loss: 1.1329 (1.1650)  classification_loss: 1.1329 (1.1649)  loss_mask: 0.0001 (0.0001)  time: 0.1702  data: 0.0001  max mem: 6052
[06:01:42.834106] Epoch: [78]  [300/781]  eta: 0:01:23  lr: 0.000031  training_loss: 1.1610 (1.1642)  classification_loss: 1.1610 (1.1642)  loss_mask: 0.0001 (0.0001)  time: 0.1709  data: 0.0002  max mem: 6052
[06:01:46.267077] Epoch: [78]  [320/781]  eta: 0:01:19  lr: 0.000031  training_loss: 1.1428 (1.1638)  classification_loss: 1.1427 (1.1637)  loss_mask: 0.0001 (0.0001)  time: 0.1716  data: 0.0001  max mem: 6052
[06:01:49.685851] Epoch: [78]  [340/781]  eta: 0:01:16  lr: 0.000031  training_loss: 1.0855 (1.1599)  classification_loss: 1.0854 (1.1598)  loss_mask: 0.0001 (0.0001)  time: 0.1708  data: 0.0002  max mem: 6052
[06:01:53.092631] Epoch: [78]  [360/781]  eta: 0:01:12  lr: 0.000031  training_loss: 1.1748 (1.1605)  classification_loss: 1.1747 (1.1604)  loss_mask: 0.0001 (0.0001)  time: 0.1703  data: 0.0002  max mem: 6052
[06:01:56.560707] Epoch: [78]  [380/781]  eta: 0:01:09  lr: 0.000031  training_loss: 1.1712 (1.1616)  classification_loss: 1.1712 (1.1616)  loss_mask: 0.0001 (0.0001)  time: 0.1733  data: 0.0002  max mem: 6052
[06:01:59.975850] Epoch: [78]  [400/781]  eta: 0:01:05  lr: 0.000031  training_loss: 1.1173 (1.1601)  classification_loss: 1.1172 (1.1601)  loss_mask: 0.0001 (0.0001)  time: 0.1707  data: 0.0002  max mem: 6052
[06:02:03.382318] Epoch: [78]  [420/781]  eta: 0:01:02  lr: 0.000031  training_loss: 1.1548 (1.1594)  classification_loss: 1.1547 (1.1594)  loss_mask: 0.0001 (0.0001)  time: 0.1702  data: 0.0002  max mem: 6052
[06:02:06.796703] Epoch: [78]  [440/781]  eta: 0:00:58  lr: 0.000031  training_loss: 1.1589 (1.1607)  classification_loss: 1.1588 (1.1607)  loss_mask: 0.0001 (0.0001)  time: 0.1707  data: 0.0002  max mem: 6052
[06:02:10.204073] Epoch: [78]  [460/781]  eta: 0:00:55  lr: 0.000031  training_loss: 1.1211 (1.1591)  classification_loss: 1.1210 (1.1591)  loss_mask: 0.0001 (0.0001)  time: 0.1703  data: 0.0001  max mem: 6052
[06:02:13.604122] Epoch: [78]  [480/781]  eta: 0:00:51  lr: 0.000031  training_loss: 1.1294 (1.1593)  classification_loss: 1.1293 (1.1592)  loss_mask: 0.0001 (0.0001)  time: 0.1699  data: 0.0002  max mem: 6052
[06:02:17.021310] Epoch: [78]  [500/781]  eta: 0:00:48  lr: 0.000031  training_loss: 1.1698 (1.1589)  classification_loss: 1.1698 (1.1589)  loss_mask: 0.0001 (0.0001)  time: 0.1708  data: 0.0002  max mem: 6052
[06:02:20.424900] Epoch: [78]  [520/781]  eta: 0:00:44  lr: 0.000031  training_loss: 1.1340 (1.1593)  classification_loss: 1.1339 (1.1592)  loss_mask: 0.0001 (0.0001)  time: 0.1701  data: 0.0003  max mem: 6052
[06:02:23.845143] Epoch: [78]  [540/781]  eta: 0:00:41  lr: 0.000031  training_loss: 1.1806 (1.1600)  classification_loss: 1.1805 (1.1599)  loss_mask: 0.0001 (0.0001)  time: 0.1709  data: 0.0002  max mem: 6052
[06:02:27.263881] Epoch: [78]  [560/781]  eta: 0:00:38  lr: 0.000031  training_loss: 1.1684 (1.1596)  classification_loss: 1.1683 (1.1596)  loss_mask: 0.0001 (0.0001)  time: 0.1709  data: 0.0003  max mem: 6052
[06:02:30.680404] Epoch: [78]  [580/781]  eta: 0:00:34  lr: 0.000031  training_loss: 1.1518 (1.1601)  classification_loss: 1.1518 (1.1600)  loss_mask: 0.0001 (0.0001)  time: 0.1707  data: 0.0002  max mem: 6052
[06:02:34.096041] Epoch: [78]  [600/781]  eta: 0:00:31  lr: 0.000030  training_loss: 1.1632 (1.1598)  classification_loss: 1.1632 (1.1598)  loss_mask: 0.0001 (0.0001)  time: 0.1707  data: 0.0002  max mem: 6052
[06:02:37.509350] Epoch: [78]  [620/781]  eta: 0:00:27  lr: 0.000030  training_loss: 1.1189 (1.1588)  classification_loss: 1.1188 (1.1588)  loss_mask: 0.0001 (0.0001)  time: 0.1706  data: 0.0002  max mem: 6052
[06:02:40.901726] Epoch: [78]  [640/781]  eta: 0:00:24  lr: 0.000030  training_loss: 1.1436 (1.1585)  classification_loss: 1.1435 (1.1584)  loss_mask: 0.0001 (0.0002)  time: 0.1696  data: 0.0002  max mem: 6052
[06:02:44.301942] Epoch: [78]  [660/781]  eta: 0:00:20  lr: 0.000030  training_loss: 1.1798 (1.1590)  classification_loss: 1.1798 (1.1588)  loss_mask: 0.0000 (0.0002)  time: 0.1699  data: 0.0002  max mem: 6052
[06:02:47.708810] Epoch: [78]  [680/781]  eta: 0:00:17  lr: 0.000030  training_loss: 1.1832 (1.1602)  classification_loss: 1.1831 (1.1601)  loss_mask: 0.0001 (0.0002)  time: 0.1703  data: 0.0002  max mem: 6052
[06:02:51.119580] Epoch: [78]  [700/781]  eta: 0:00:13  lr: 0.000030  training_loss: 1.1785 (1.1608)  classification_loss: 1.1784 (1.1606)  loss_mask: 0.0001 (0.0002)  time: 0.1705  data: 0.0002  max mem: 6052
[06:02:54.529135] Epoch: [78]  [720/781]  eta: 0:00:10  lr: 0.000030  training_loss: 1.1444 (1.1607)  classification_loss: 1.1444 (1.1605)  loss_mask: 0.0000 (0.0002)  time: 0.1704  data: 0.0002  max mem: 6052
[06:02:57.937316] Epoch: [78]  [740/781]  eta: 0:00:07  lr: 0.000030  training_loss: 1.0908 (1.1592)  classification_loss: 1.0907 (1.1590)  loss_mask: 0.0001 (0.0002)  time: 0.1703  data: 0.0002  max mem: 6052
[06:03:01.343833] Epoch: [78]  [760/781]  eta: 0:00:03  lr: 0.000030  training_loss: 1.1344 (1.1598)  classification_loss: 1.1344 (1.1596)  loss_mask: 0.0001 (0.0002)  time: 0.1702  data: 0.0002  max mem: 6052
[06:03:04.750421] Epoch: [78]  [780/781]  eta: 0:00:00  lr: 0.000030  training_loss: 1.1790 (1.1600)  classification_loss: 1.1790 (1.1599)  loss_mask: 0.0000 (0.0002)  time: 0.1703  data: 0.0002  max mem: 6052
[06:03:04.915889] Epoch: [78] Total time: 0:02:14 (0.1718 s / it)
[06:03:04.916538] Averaged stats: lr: 0.000030  training_loss: 1.1790 (1.1600)  classification_loss: 1.1790 (1.1599)  loss_mask: 0.0000 (0.0002)
[06:03:05.587743] Test:  [  0/157]  eta: 0:01:44  testing_loss: 0.4063 (0.4063)  acc1: 85.9375 (85.9375)  acc5: 100.0000 (100.0000)  time: 0.6668  data: 0.6132  max mem: 6052
[06:03:05.872089] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4842 (0.4895)  acc1: 84.3750 (84.3750)  acc5: 100.0000 (99.8580)  time: 0.0863  data: 0.0559  max mem: 6052
[06:03:06.161137] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4733 (0.4601)  acc1: 85.9375 (85.5655)  acc5: 100.0000 (99.6280)  time: 0.0285  data: 0.0003  max mem: 6052
[06:03:06.446301] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4338 (0.4677)  acc1: 87.5000 (86.1391)  acc5: 100.0000 (99.4960)  time: 0.0286  data: 0.0003  max mem: 6052
[06:03:06.737249] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4542 (0.4740)  acc1: 87.5000 (85.9756)  acc5: 98.4375 (99.2759)  time: 0.0287  data: 0.0002  max mem: 6052
[06:03:07.025356] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4501 (0.4723)  acc1: 85.9375 (86.2439)  acc5: 98.4375 (99.2341)  time: 0.0288  data: 0.0002  max mem: 6052
[06:03:07.308734] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4417 (0.4689)  acc1: 87.5000 (86.3730)  acc5: 100.0000 (99.2572)  time: 0.0284  data: 0.0002  max mem: 6052
[06:03:07.594612] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4696 (0.4670)  acc1: 87.5000 (86.4217)  acc5: 100.0000 (99.2958)  time: 0.0283  data: 0.0002  max mem: 6052
[06:03:07.874912] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4791 (0.4745)  acc1: 82.8125 (86.0340)  acc5: 100.0000 (99.2863)  time: 0.0282  data: 0.0001  max mem: 6052
[06:03:08.157826] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4662 (0.4715)  acc1: 85.9375 (86.1607)  acc5: 100.0000 (99.2788)  time: 0.0280  data: 0.0001  max mem: 6052
[06:03:08.448861] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4641 (0.4727)  acc1: 85.9375 (86.1077)  acc5: 98.4375 (99.2729)  time: 0.0286  data: 0.0001  max mem: 6052
[06:03:08.733319] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4701 (0.4726)  acc1: 85.9375 (86.1064)  acc5: 100.0000 (99.2680)  time: 0.0286  data: 0.0002  max mem: 6052
[06:03:09.014082] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4392 (0.4721)  acc1: 87.5000 (86.1829)  acc5: 100.0000 (99.2769)  time: 0.0281  data: 0.0002  max mem: 6052
[06:03:09.295235] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4504 (0.4728)  acc1: 87.5000 (86.2715)  acc5: 100.0000 (99.2486)  time: 0.0280  data: 0.0001  max mem: 6052
[06:03:09.574412] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4422 (0.4700)  acc1: 87.5000 (86.4694)  acc5: 100.0000 (99.2575)  time: 0.0279  data: 0.0001  max mem: 6052
[06:03:09.852123] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4539 (0.4703)  acc1: 87.5000 (86.4549)  acc5: 100.0000 (99.2550)  time: 0.0277  data: 0.0001  max mem: 6052
[06:03:10.001070] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4929 (0.4719)  acc1: 85.9375 (86.3500)  acc5: 100.0000 (99.2400)  time: 0.0268  data: 0.0001  max mem: 6052
[06:03:10.149450] Test: Total time: 0:00:05 (0.0333 s / it)
[06:03:10.150795] * Acc@1 86.350 Acc@5 99.240 loss 0.472
[06:03:10.151194] Accuracy of the network on the 10000 test images: 86.3%
[06:03:10.151549] Max accuracy: 86.35%
[06:03:10.481212] log_dir: ./output_dir
[06:03:11.358630] Epoch: [79]  [  0/781]  eta: 0:11:23  lr: 0.000030  training_loss: 1.0095 (1.0095)  classification_loss: 1.0095 (1.0095)  loss_mask: 0.0000 (0.0000)  time: 0.8757  data: 0.6861  max mem: 6052
[06:03:14.817440] Epoch: [79]  [ 20/781]  eta: 0:02:37  lr: 0.000030  training_loss: 1.1296 (1.1367)  classification_loss: 1.1295 (1.1366)  loss_mask: 0.0000 (0.0001)  time: 0.1728  data: 0.0002  max mem: 6052
[06:03:18.232899] Epoch: [79]  [ 40/781]  eta: 0:02:20  lr: 0.000030  training_loss: 1.1471 (1.1428)  classification_loss: 1.1471 (1.1428)  loss_mask: 0.0001 (0.0001)  time: 0.1707  data: 0.0002  max mem: 6052
[06:03:21.633201] Epoch: [79]  [ 60/781]  eta: 0:02:11  lr: 0.000030  training_loss: 1.1550 (1.1494)  classification_loss: 1.1550 (1.1494)  loss_mask: 0.0001 (0.0001)  time: 0.1699  data: 0.0002  max mem: 6052
[06:03:25.035273] Epoch: [79]  [ 80/781]  eta: 0:02:05  lr: 0.000030  training_loss: 1.0961 (1.1403)  classification_loss: 1.0960 (1.1402)  loss_mask: 0.0001 (0.0001)  time: 0.1700  data: 0.0002  max mem: 6052
[06:03:28.444473] Epoch: [79]  [100/781]  eta: 0:02:01  lr: 0.000029  training_loss: 1.1846 (1.1484)  classification_loss: 1.1846 (1.1483)  loss_mask: 0.0001 (0.0001)  time: 0.1703  data: 0.0002  max mem: 6052
[06:03:31.874242] Epoch: [79]  [120/781]  eta: 0:01:56  lr: 0.000029  training_loss: 1.1590 (1.1503)  classification_loss: 1.1590 (1.1502)  loss_mask: 0.0001 (0.0001)  time: 0.1714  data: 0.0003  max mem: 6052
[06:03:35.314278] Epoch: [79]  [140/781]  eta: 0:01:52  lr: 0.000029  training_loss: 1.1108 (1.1458)  classification_loss: 1.1108 (1.1457)  loss_mask: 0.0001 (0.0001)  time: 0.1719  data: 0.0004  max mem: 6052
[06:03:38.731427] Epoch: [79]  [160/781]  eta: 0:01:48  lr: 0.000029  training_loss: 1.1412 (1.1487)  classification_loss: 1.1411 (1.1486)  loss_mask: 0.0000 (0.0001)  time: 0.1708  data: 0.0002  max mem: 6052
[06:03:42.141784] Epoch: [79]  [180/781]  eta: 0:01:45  lr: 0.000029  training_loss: 1.1310 (1.1489)  classification_loss: 1.1310 (1.1488)  loss_mask: 0.0000 (0.0001)  time: 0.1704  data: 0.0004  max mem: 6052
[06:03:45.552962] Epoch: [79]  [200/781]  eta: 0:01:41  lr: 0.000029  training_loss: 1.1919 (1.1519)  classification_loss: 1.1919 (1.1518)  loss_mask: 0.0000 (0.0001)  time: 0.1705  data: 0.0002  max mem: 6052
[06:03:48.950514] Epoch: [79]  [220/781]  eta: 0:01:37  lr: 0.000029  training_loss: 1.1387 (1.1541)  classification_loss: 1.1386 (1.1541)  loss_mask: 0.0001 (0.0001)  time: 0.1698  data: 0.0002  max mem: 6052
[06:03:52.404512] Epoch: [79]  [240/781]  eta: 0:01:34  lr: 0.000029  training_loss: 1.1358 (1.1541)  classification_loss: 1.1357 (1.1541)  loss_mask: 0.0000 (0.0001)  time: 0.1726  data: 0.0002  max mem: 6052
[06:03:55.830292] Epoch: [79]  [260/781]  eta: 0:01:30  lr: 0.000029  training_loss: 1.1296 (1.1545)  classification_loss: 1.1296 (1.1544)  loss_mask: 0.0001 (0.0001)  time: 0.1712  data: 0.0004  max mem: 6052
[06:03:59.275365] Epoch: [79]  [280/781]  eta: 0:01:26  lr: 0.000029  training_loss: 1.1126 (1.1533)  classification_loss: 1.1126 (1.1532)  loss_mask: 0.0000 (0.0001)  time: 0.1722  data: 0.0002  max mem: 6052
[06:04:02.712395] Epoch: [79]  [300/781]  eta: 0:01:23  lr: 0.000029  training_loss: 1.1586 (1.1545)  classification_loss: 1.1585 (1.1544)  loss_mask: 0.0000 (0.0001)  time: 0.1718  data: 0.0002  max mem: 6052
[06:04:06.118929] Epoch: [79]  [320/781]  eta: 0:01:19  lr: 0.000029  training_loss: 1.2063 (1.1568)  classification_loss: 1.2063 (1.1567)  loss_mask: 0.0001 (0.0001)  time: 0.1702  data: 0.0001  max mem: 6052
[06:04:09.578647] Epoch: [79]  [340/781]  eta: 0:01:16  lr: 0.000029  training_loss: 1.1266 (1.1573)  classification_loss: 1.1265 (1.1572)  loss_mask: 0.0000 (0.0001)  time: 0.1729  data: 0.0002  max mem: 6052
[06:04:12.956870] Epoch: [79]  [360/781]  eta: 0:01:12  lr: 0.000029  training_loss: 1.1336 (1.1578)  classification_loss: 1.1336 (1.1577)  loss_mask: 0.0001 (0.0001)  time: 0.1688  data: 0.0001  max mem: 6052
[06:04:16.352857] Epoch: [79]  [380/781]  eta: 0:01:09  lr: 0.000029  training_loss: 1.1287 (1.1599)  classification_loss: 1.1286 (1.1598)  loss_mask: 0.0001 (0.0001)  time: 0.1697  data: 0.0002  max mem: 6052
[06:04:19.763626] Epoch: [79]  [400/781]  eta: 0:01:05  lr: 0.000028  training_loss: 1.1654 (1.1597)  classification_loss: 1.1653 (1.1597)  loss_mask: 0.0000 (0.0001)  time: 0.1704  data: 0.0002  max mem: 6052
[06:04:23.176519] Epoch: [79]  [420/781]  eta: 0:01:02  lr: 0.000028  training_loss: 1.1750 (1.1611)  classification_loss: 1.1750 (1.1611)  loss_mask: 0.0001 (0.0001)  time: 0.1706  data: 0.0002  max mem: 6052
[06:04:26.603738] Epoch: [79]  [440/781]  eta: 0:00:58  lr: 0.000028  training_loss: 1.1912 (1.1621)  classification_loss: 1.1911 (1.1621)  loss_mask: 0.0001 (0.0001)  time: 0.1713  data: 0.0002  max mem: 6052
[06:04:30.028881] Epoch: [79]  [460/781]  eta: 0:00:55  lr: 0.000028  training_loss: 1.1140 (1.1614)  classification_loss: 1.1139 (1.1613)  loss_mask: 0.0001 (0.0001)  time: 0.1712  data: 0.0003  max mem: 6052
[06:04:33.430787] Epoch: [79]  [480/781]  eta: 0:00:51  lr: 0.000028  training_loss: 1.1888 (1.1622)  classification_loss: 1.1888 (1.1621)  loss_mask: 0.0001 (0.0001)  time: 0.1700  data: 0.0001  max mem: 6052

[06:04:36.861406] Epoch: [79]  [500/781]  eta: 0:00:48  lr: 0.000028  training_loss: 1.1749 (1.1630)  classification_loss: 1.1748 (1.1629)  loss_mask: 0.0000 (0.0001)  time: 0.1714  data: 0.0002  max mem: 6052
[06:04:40.273010] Epoch: [79]  [520/781]  eta: 0:00:44  lr: 0.000028  training_loss: 1.1396 (1.1627)  classification_loss: 1.1395 (1.1626)  loss_mask: 0.0000 (0.0001)  time: 0.1705  data: 0.0003  max mem: 6052
[06:04:43.686999] Epoch: [79]  [540/781]  eta: 0:00:41  lr: 0.000028  training_loss: 1.1656 (1.1623)  classification_loss: 1.1656 (1.1622)  loss_mask: 0.0001 (0.0001)  time: 0.1706  data: 0.0002  max mem: 6052
[06:04:47.105605] Epoch: [79]  [560/781]  eta: 0:00:38  lr: 0.000028  training_loss: 1.1736 (1.1648)  classification_loss: 1.1642 (1.1626)  loss_mask: 0.0018 (0.0021)  time: 0.1709  data: 0.0002  max mem: 6052
[06:04:50.510460] Epoch: [79]  [580/781]  eta: 0:00:34  lr: 0.000028  training_loss: 1.2034 (1.1664)  classification_loss: 1.1377 (1.1631)  loss_mask: 0.0091 (0.0033)  time: 0.1702  data: 0.0002  max mem: 6052
[06:04:53.958188] Epoch: [79]  [600/781]  eta: 0:00:31  lr: 0.000028  training_loss: 1.1635 (1.1667)  classification_loss: 1.1581 (1.1633)  loss_mask: 0.0011 (0.0034)  time: 0.1723  data: 0.0002  max mem: 6052
[06:04:57.374729] Epoch: [79]  [620/781]  eta: 0:00:27  lr: 0.000028  training_loss: 1.1313 (1.1659)  classification_loss: 1.1310 (1.1626)  loss_mask: 0.0003 (0.0033)  time: 0.1707  data: 0.0003  max mem: 6052
[06:05:00.775902] Epoch: [79]  [640/781]  eta: 0:00:24  lr: 0.000028  training_loss: 1.1471 (1.1653)  classification_loss: 1.1470 (1.1621)  loss_mask: 0.0001 (0.0032)  time: 0.1699  data: 0.0002  max mem: 6052
[06:05:04.189096] Epoch: [79]  [660/781]  eta: 0:00:20  lr: 0.000028  training_loss: 1.0904 (1.1641)  classification_loss: 1.0903 (1.1610)  loss_mask: 0.0002 (0.0031)  time: 0.1706  data: 0.0002  max mem: 6052
[06:05:07.600304] Epoch: [79]  [680/781]  eta: 0:00:17  lr: 0.000028  training_loss: 1.1575 (1.1644)  classification_loss: 1.1574 (1.1614)  loss_mask: 0.0001 (0.0030)  time: 0.1705  data: 0.0002  max mem: 6052
[06:05:11.026808] Epoch: [79]  [700/781]  eta: 0:00:13  lr: 0.000028  training_loss: 1.1611 (1.1649)  classification_loss: 1.1610 (1.1620)  loss_mask: 0.0002 (0.0030)  time: 0.1712  data: 0.0002  max mem: 6052
[06:05:14.457827] Epoch: [79]  [720/781]  eta: 0:00:10  lr: 0.000027  training_loss: 1.1409 (1.1641)  classification_loss: 1.1408 (1.1612)  loss_mask: 0.0001 (0.0029)  time: 0.1714  data: 0.0002  max mem: 6052
[06:05:17.891184] Epoch: [79]  [740/781]  eta: 0:00:07  lr: 0.000027  training_loss: 1.1232 (1.1638)  classification_loss: 1.1230 (1.1610)  loss_mask: 0.0001 (0.0028)  time: 0.1716  data: 0.0002  max mem: 6052
[06:05:21.322496] Epoch: [79]  [760/781]  eta: 0:00:03  lr: 0.000027  training_loss: 1.1390 (1.1641)  classification_loss: 1.1390 (1.1614)  loss_mask: 0.0001 (0.0027)  time: 0.1715  data: 0.0003  max mem: 6052
[06:05:24.702559] Epoch: [79]  [780/781]  eta: 0:00:00  lr: 0.000027  training_loss: 1.1540 (1.1639)  classification_loss: 1.1539 (1.1613)  loss_mask: 0.0001 (0.0027)  time: 0.1689  data: 0.0002  max mem: 6052
[06:05:24.872501] Epoch: [79] Total time: 0:02:14 (0.1721 s / it)
[06:05:24.873639] Averaged stats: lr: 0.000027  training_loss: 1.1540 (1.1639)  classification_loss: 1.1539 (1.1613)  loss_mask: 0.0001 (0.0027)
[06:05:25.525954] Test:  [  0/157]  eta: 0:01:41  testing_loss: 0.4198 (0.4198)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (100.0000)  time: 0.6469  data: 0.6168  max mem: 6052
[06:05:25.831278] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4663 (0.4875)  acc1: 82.8125 (83.9489)  acc5: 100.0000 (99.8580)  time: 0.0863  data: 0.0562  max mem: 6052
[06:05:26.113176] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4567 (0.4563)  acc1: 85.9375 (85.8631)  acc5: 100.0000 (99.7768)  time: 0.0292  data: 0.0002  max mem: 6052
[06:05:26.397504] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4203 (0.4639)  acc1: 85.9375 (85.9375)  acc5: 100.0000 (99.5968)  time: 0.0282  data: 0.0002  max mem: 6052
[06:05:26.682023] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4968 (0.4703)  acc1: 85.9375 (85.8613)  acc5: 98.4375 (99.2759)  time: 0.0283  data: 0.0002  max mem: 6052
[06:05:26.974227] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4328 (0.4658)  acc1: 85.9375 (86.3051)  acc5: 98.4375 (99.2341)  time: 0.0287  data: 0.0004  max mem: 6052
[06:05:27.258191] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4269 (0.4626)  acc1: 87.5000 (86.3986)  acc5: 100.0000 (99.2572)  time: 0.0286  data: 0.0003  max mem: 6052
[06:05:27.548848] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4434 (0.4589)  acc1: 85.9375 (86.4217)  acc5: 100.0000 (99.2958)  time: 0.0286  data: 0.0002  max mem: 6052
[06:05:27.834458] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4709 (0.4656)  acc1: 85.9375 (86.2076)  acc5: 100.0000 (99.3248)  time: 0.0287  data: 0.0001  max mem: 6052
[06:05:28.120972] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4505 (0.4617)  acc1: 85.9375 (86.3668)  acc5: 100.0000 (99.3132)  time: 0.0285  data: 0.0001  max mem: 6052
[06:05:28.402454] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4321 (0.4621)  acc1: 89.0625 (86.3707)  acc5: 100.0000 (99.3348)  time: 0.0283  data: 0.0001  max mem: 6052
[06:05:28.685691] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4422 (0.4613)  acc1: 87.5000 (86.4020)  acc5: 100.0000 (99.3384)  time: 0.0281  data: 0.0001  max mem: 6052
[06:05:28.972522] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4310 (0.4605)  acc1: 87.5000 (86.4024)  acc5: 100.0000 (99.3543)  time: 0.0284  data: 0.0002  max mem: 6052
[06:05:29.257122] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4231 (0.4609)  acc1: 85.9375 (86.4981)  acc5: 100.0000 (99.3678)  time: 0.0284  data: 0.0001  max mem: 6052
[06:05:29.540431] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4066 (0.4579)  acc1: 87.5000 (86.5913)  acc5: 100.0000 (99.4016)  time: 0.0283  data: 0.0001  max mem: 6052
[06:05:29.818193] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4372 (0.4573)  acc1: 85.9375 (86.5791)  acc5: 100.0000 (99.3998)  time: 0.0279  data: 0.0001  max mem: 6052
[06:05:29.967233] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4637 (0.4588)  acc1: 87.5000 (86.5100)  acc5: 100.0000 (99.3900)  time: 0.0268  data: 0.0001  max mem: 6052
[06:05:30.124433] Test: Total time: 0:00:05 (0.0334 s / it)
[06:05:30.125020] * Acc@1 86.510 Acc@5 99.390 loss 0.459
[06:05:30.125314] Accuracy of the network on the 10000 test images: 86.5%
[06:05:30.125619] Max accuracy: 86.51%
[06:05:30.392913] log_dir: ./output_dir
[06:05:31.253681] Epoch: [80]  [  0/781]  eta: 0:11:10  lr: 0.000027  training_loss: 1.1285 (1.1285)  classification_loss: 1.1283 (1.1283)  loss_mask: 0.0002 (0.0002)  time: 0.8591  data: 0.6714  max mem: 6052
[06:05:34.686697] Epoch: [80]  [ 20/781]  eta: 0:02:35  lr: 0.000027  training_loss: 1.1035 (1.1411)  classification_loss: 1.1034 (1.1409)  loss_mask: 0.0001 (0.0001)  time: 0.1715  data: 0.0002  max mem: 6052
[06:05:38.086030] Epoch: [80]  [ 40/781]  eta: 0:02:18  lr: 0.000027  training_loss: 1.1437 (1.1562)  classification_loss: 1.1437 (1.1560)  loss_mask: 0.0001 (0.0001)  time: 0.1699  data: 0.0002  max mem: 6052
[06:05:41.505039] Epoch: [80]  [ 60/781]  eta: 0:02:11  lr: 0.000027  training_loss: 1.1578 (1.1676)  classification_loss: 1.1576 (1.1674)  loss_mask: 0.0001 (0.0001)  time: 0.1709  data: 0.0003  max mem: 6052
[06:05:44.916919] Epoch: [80]  [ 80/781]  eta: 0:02:05  lr: 0.000027  training_loss: 1.1383 (1.1626)  classification_loss: 1.1382 (1.1625)  loss_mask: 0.0001 (0.0001)  time: 0.1705  data: 0.0003  max mem: 6052
[06:05:48.354810] Epoch: [80]  [100/781]  eta: 0:02:01  lr: 0.000027  training_loss: 1.1135 (1.1557)  classification_loss: 1.1135 (1.1556)  loss_mask: 0.0001 (0.0001)  time: 0.1718  data: 0.0003  max mem: 6052
[06:05:51.807967] Epoch: [80]  [120/781]  eta: 0:01:56  lr: 0.000027  training_loss: 1.1612 (1.1593)  classification_loss: 1.1611 (1.1592)  loss_mask: 0.0001 (0.0001)  time: 0.1726  data: 0.0002  max mem: 6052
[06:05:55.261709] Epoch: [80]  [140/781]  eta: 0:01:52  lr: 0.000027  training_loss: 1.1047 (1.1553)  classification_loss: 1.1046 (1.1552)  loss_mask: 0.0001 (0.0001)  time: 0.1726  data: 0.0002  max mem: 6052
[06:05:58.696153] Epoch: [80]  [160/781]  eta: 0:01:49  lr: 0.000027  training_loss: 1.1285 (1.1519)  classification_loss: 1.1284 (1.1518)  loss_mask: 0.0001 (0.0001)  time: 0.1716  data: 0.0002  max mem: 6052
[06:06:02.112534] Epoch: [80]  [180/781]  eta: 0:01:45  lr: 0.000027  training_loss: 1.1388 (1.1537)  classification_loss: 1.1387 (1.1536)  loss_mask: 0.0001 (0.0001)  time: 0.1707  data: 0.0002  max mem: 6052
[06:06:05.534716] Epoch: [80]  [200/781]  eta: 0:01:41  lr: 0.000027  training_loss: 1.0952 (1.1503)  classification_loss: 1.0951 (1.1502)  loss_mask: 0.0001 (0.0001)  time: 0.1710  data: 0.0002  max mem: 6052
[06:06:08.938863] Epoch: [80]  [220/781]  eta: 0:01:37  lr: 0.000027  training_loss: 1.1247 (1.1488)  classification_loss: 1.1246 (1.1487)  loss_mask: 0.0001 (0.0001)  time: 0.1701  data: 0.0001  max mem: 6052
[06:06:12.348402] Epoch: [80]  [240/781]  eta: 0:01:34  lr: 0.000026  training_loss: 1.1029 (1.1479)  classification_loss: 1.1029 (1.1478)  loss_mask: 0.0001 (0.0001)  time: 0.1704  data: 0.0002  max mem: 6052
[06:06:15.814353] Epoch: [80]  [260/781]  eta: 0:01:30  lr: 0.000026  training_loss: 1.1451 (1.1497)  classification_loss: 1.1450 (1.1496)  loss_mask: 0.0001 (0.0001)  time: 0.1732  data: 0.0002  max mem: 6052
[06:06:19.226138] Epoch: [80]  [280/781]  eta: 0:01:27  lr: 0.000026  training_loss: 1.1141 (1.1471)  classification_loss: 1.1140 (1.1470)  loss_mask: 0.0001 (0.0001)  time: 0.1705  data: 0.0002  max mem: 6052
[06:06:22.642946] Epoch: [80]  [300/781]  eta: 0:01:23  lr: 0.000026  training_loss: 1.1637 (1.1482)  classification_loss: 1.1637 (1.1481)  loss_mask: 0.0001 (0.0001)  time: 0.1708  data: 0.0002  max mem: 6052
[06:06:26.085608] Epoch: [80]  [320/781]  eta: 0:01:19  lr: 0.000026  training_loss: 1.1823 (1.1499)  classification_loss: 1.1822 (1.1497)  loss_mask: 0.0001 (0.0001)  time: 0.1721  data: 0.0003  max mem: 6052
[06:06:29.500954] Epoch: [80]  [340/781]  eta: 0:01:16  lr: 0.000026  training_loss: 1.1205 (1.1488)  classification_loss: 1.1205 (1.1487)  loss_mask: 0.0001 (0.0001)  time: 0.1707  data: 0.0002  max mem: 6052
[06:06:32.931975] Epoch: [80]  [360/781]  eta: 0:01:12  lr: 0.000026  training_loss: 1.1288 (1.1474)  classification_loss: 1.1287 (1.1473)  loss_mask: 0.0001 (0.0001)  time: 0.1715  data: 0.0002  max mem: 6052
[06:06:36.350497] Epoch: [80]  [380/781]  eta: 0:01:09  lr: 0.000026  training_loss: 1.1106 (1.1469)  classification_loss: 1.1106 (1.1468)  loss_mask: 0.0001 (0.0001)  time: 0.1709  data: 0.0003  max mem: 6052
[06:06:39.778507] Epoch: [80]  [400/781]  eta: 0:01:05  lr: 0.000026  training_loss: 1.0976 (1.1458)  classification_loss: 1.0975 (1.1457)  loss_mask: 0.0001 (0.0001)  time: 0.1713  data: 0.0002  max mem: 6052
[06:06:43.195769] Epoch: [80]  [420/781]  eta: 0:01:02  lr: 0.000026  training_loss: 1.1579 (1.1466)  classification_loss: 1.1579 (1.1465)  loss_mask: 0.0001 (0.0001)  time: 0.1708  data: 0.0005  max mem: 6052
[06:06:46.602906] Epoch: [80]  [440/781]  eta: 0:00:58  lr: 0.000026  training_loss: 1.1419 (1.1475)  classification_loss: 1.1418 (1.1474)  loss_mask: 0.0001 (0.0001)  time: 0.1703  data: 0.0002  max mem: 6052
[06:06:50.013455] Epoch: [80]  [460/781]  eta: 0:00:55  lr: 0.000026  training_loss: 1.1013 (1.1471)  classification_loss: 1.1012 (1.1470)  loss_mask: 0.0001 (0.0001)  time: 0.1705  data: 0.0001  max mem: 6052
[06:06:53.467784] Epoch: [80]  [480/781]  eta: 0:00:51  lr: 0.000026  training_loss: 1.0979 (1.1466)  classification_loss: 1.0978 (1.1465)  loss_mask: 0.0001 (0.0001)  time: 0.1726  data: 0.0002  max mem: 6052
[06:06:56.961954] Epoch: [80]  [500/781]  eta: 0:00:48  lr: 0.000026  training_loss: 1.1447 (1.1464)  classification_loss: 1.1446 (1.1463)  loss_mask: 0.0001 (0.0001)  time: 0.1746  data: 0.0002  max mem: 6052
[06:07:00.389240] Epoch: [80]  [520/781]  eta: 0:00:45  lr: 0.000026  training_loss: 1.1803 (1.1463)  classification_loss: 1.1802 (1.1462)  loss_mask: 0.0001 (0.0001)  time: 0.1713  data: 0.0002  max mem: 6052
[06:07:03.855592] Epoch: [80]  [540/781]  eta: 0:00:41  lr: 0.000026  training_loss: 1.1651 (1.1465)  classification_loss: 1.1650 (1.1464)  loss_mask: 0.0001 (0.0001)  time: 0.1732  data: 0.0002  max mem: 6052
[06:07:07.264093] Epoch: [80]  [560/781]  eta: 0:00:38  lr: 0.000025  training_loss: 1.1256 (1.1461)  classification_loss: 1.1255 (1.1460)  loss_mask: 0.0001 (0.0001)  time: 0.1704  data: 0.0002  max mem: 6052
[06:07:10.687278] Epoch: [80]  [580/781]  eta: 0:00:34  lr: 0.000025  training_loss: 1.1621 (1.1470)  classification_loss: 1.1620 (1.1469)  loss_mask: 0.0001 (0.0001)  time: 0.1711  data: 0.0002  max mem: 6052
[06:07:14.103077] Epoch: [80]  [600/781]  eta: 0:00:31  lr: 0.000025  training_loss: 1.1375 (1.1467)  classification_loss: 1.1373 (1.1466)  loss_mask: 0.0001 (0.0001)  time: 0.1707  data: 0.0002  max mem: 6052
[06:07:17.518799] Epoch: [80]  [620/781]  eta: 0:00:27  lr: 0.000025  training_loss: 1.1615 (1.1466)  classification_loss: 1.1615 (1.1465)  loss_mask: 0.0001 (0.0001)  time: 0.1707  data: 0.0002  max mem: 6052
[06:07:20.934601] Epoch: [80]  [640/781]  eta: 0:00:24  lr: 0.000025  training_loss: 1.1018 (1.1464)  classification_loss: 1.1018 (1.1463)  loss_mask: 0.0001 (0.0001)  time: 0.1707  data: 0.0002  max mem: 6052
[06:07:24.348294] Epoch: [80]  [660/781]  eta: 0:00:20  lr: 0.000025  training_loss: 1.1450 (1.1463)  classification_loss: 1.1449 (1.1462)  loss_mask: 0.0001 (0.0001)  time: 0.1706  data: 0.0002  max mem: 6052
[06:07:27.766296] Epoch: [80]  [680/781]  eta: 0:00:17  lr: 0.000025  training_loss: 1.1796 (1.1470)  classification_loss: 1.1794 (1.1469)  loss_mask: 0.0001 (0.0001)  time: 0.1708  data: 0.0004  max mem: 6052
[06:07:31.202835] Epoch: [80]  [700/781]  eta: 0:00:13  lr: 0.000025  training_loss: 1.1938 (1.1487)  classification_loss: 1.1937 (1.1486)  loss_mask: 0.0001 (0.0001)  time: 0.1717  data: 0.0004  max mem: 6052
[06:07:34.619959] Epoch: [80]  [720/781]  eta: 0:00:10  lr: 0.000025  training_loss: 1.1459 (1.1492)  classification_loss: 1.1459 (1.1491)  loss_mask: 0.0001 (0.0001)  time: 0.1708  data: 0.0002  max mem: 6052
[06:07:38.039596] Epoch: [80]  [740/781]  eta: 0:00:07  lr: 0.000025  training_loss: 1.1107 (1.1485)  classification_loss: 1.1106 (1.1484)  loss_mask: 0.0001 (0.0001)  time: 0.1709  data: 0.0003  max mem: 6052
[06:07:41.457169] Epoch: [80]  [760/781]  eta: 0:00:03  lr: 0.000025  training_loss: 1.1723 (1.1486)  classification_loss: 1.1723 (1.1485)  loss_mask: 0.0001 (0.0001)  time: 0.1708  data: 0.0003  max mem: 6052
[06:07:44.872143] Epoch: [80]  [780/781]  eta: 0:00:00  lr: 0.000025  training_loss: 1.1405 (1.1487)  classification_loss: 1.1405 (1.1486)  loss_mask: 0.0001 (0.0001)  time: 0.1707  data: 0.0002  max mem: 6052
[06:07:45.035656] Epoch: [80] Total time: 0:02:14 (0.1724 s / it)
[06:07:45.036113] Averaged stats: lr: 0.000025  training_loss: 1.1405 (1.1487)  classification_loss: 1.1405 (1.1486)  loss_mask: 0.0001 (0.0001)
[06:07:46.201669] Test:  [  0/157]  eta: 0:01:37  testing_loss: 0.4200 (0.4200)  acc1: 89.0625 (89.0625)  acc5: 100.0000 (100.0000)  time: 0.6189  data: 0.5886  max mem: 6052
[06:07:46.490267] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4762 (0.4895)  acc1: 85.9375 (84.9432)  acc5: 100.0000 (100.0000)  time: 0.0823  data: 0.0541  max mem: 6052
[06:07:46.777537] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4579 (0.4642)  acc1: 85.9375 (86.0119)  acc5: 100.0000 (99.7024)  time: 0.0286  data: 0.0004  max mem: 6052
[06:07:47.070657] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4370 (0.4744)  acc1: 87.5000 (85.7359)  acc5: 100.0000 (99.5464)  time: 0.0289  data: 0.0003  max mem: 6052
[06:07:47.350583] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4825 (0.4779)  acc1: 84.3750 (85.6326)  acc5: 98.4375 (99.3140)  time: 0.0285  data: 0.0003  max mem: 6052
[06:07:47.630874] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4535 (0.4736)  acc1: 85.9375 (86.0600)  acc5: 98.4375 (99.3260)  time: 0.0279  data: 0.0001  max mem: 6052
[06:07:47.916855] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4388 (0.4684)  acc1: 87.5000 (86.2449)  acc5: 100.0000 (99.3596)  time: 0.0282  data: 0.0001  max mem: 6052
[06:07:48.199691] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4414 (0.4651)  acc1: 87.5000 (86.2896)  acc5: 100.0000 (99.3838)  time: 0.0283  data: 0.0002  max mem: 6052
[06:07:48.488945] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4789 (0.4714)  acc1: 85.9375 (86.0532)  acc5: 100.0000 (99.4020)  time: 0.0285  data: 0.0002  max mem: 6052
[06:07:48.772949] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4769 (0.4702)  acc1: 85.9375 (86.1435)  acc5: 100.0000 (99.3475)  time: 0.0285  data: 0.0002  max mem: 6052
[06:07:49.062502] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4489 (0.4717)  acc1: 87.5000 (86.0303)  acc5: 100.0000 (99.3812)  time: 0.0286  data: 0.0002  max mem: 6052
[06:07:49.344782] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4489 (0.4705)  acc1: 87.5000 (86.1486)  acc5: 100.0000 (99.3806)  time: 0.0285  data: 0.0002  max mem: 6052
[06:07:49.628740] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4522 (0.4699)  acc1: 87.5000 (86.1570)  acc5: 100.0000 (99.3673)  time: 0.0282  data: 0.0002  max mem: 6052
[06:07:49.916955] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4577 (0.4707)  acc1: 85.9375 (86.1999)  acc5: 100.0000 (99.3678)  time: 0.0284  data: 0.0002  max mem: 6052
[06:07:50.195827] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4367 (0.4675)  acc1: 89.0625 (86.3808)  acc5: 100.0000 (99.3794)  time: 0.0282  data: 0.0001  max mem: 6052
[06:07:50.474738] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4450 (0.4672)  acc1: 87.5000 (86.3514)  acc5: 100.0000 (99.3791)  time: 0.0277  data: 0.0001  max mem: 6052
[06:07:50.623587] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4677 (0.4689)  acc1: 85.9375 (86.2900)  acc5: 100.0000 (99.3700)  time: 0.0268  data: 0.0001  max mem: 6052
[06:07:50.786218] Test: Total time: 0:00:05 (0.0332 s / it)
[06:07:50.786761] * Acc@1 86.290 Acc@5 99.370 loss 0.469
[06:07:50.787061] Accuracy of the network on the 10000 test images: 86.3%
[06:07:50.787250] Max accuracy: 86.51%
[06:07:51.009360] log_dir: ./output_dir
[06:07:51.885467] Epoch: [81]  [  0/781]  eta: 0:11:22  lr: 0.000025  training_loss: 0.9583 (0.9583)  classification_loss: 0.9582 (0.9582)  loss_mask: 0.0001 (0.0001)  time: 0.8743  data: 0.6911  max mem: 6052
[06:07:55.313866] Epoch: [81]  [ 20/781]  eta: 0:02:35  lr: 0.000025  training_loss: 1.1207 (1.1246)  classification_loss: 1.1205 (1.1245)  loss_mask: 0.0001 (0.0001)  time: 0.1713  data: 0.0002  max mem: 6052
[06:07:58.723587] Epoch: [81]  [ 40/781]  eta: 0:02:19  lr: 0.000025  training_loss: 1.1202 (1.1168)  classification_loss: 1.1201 (1.1167)  loss_mask: 0.0001 (0.0001)  time: 0.1704  data: 0.0002  max mem: 6052
[06:08:02.151708] Epoch: [81]  [ 60/781]  eta: 0:02:11  lr: 0.000025  training_loss: 1.1366 (1.1298)  classification_loss: 1.1366 (1.1297)  loss_mask: 0.0001 (0.0001)  time: 0.1713  data: 0.0002  max mem: 6052
[06:08:05.571525] Epoch: [81]  [ 80/781]  eta: 0:02:05  lr: 0.000025  training_loss: 1.0785 (1.1234)  classification_loss: 1.0783 (1.1233)  loss_mask: 0.0001 (0.0001)  time: 0.1709  data: 0.0002  max mem: 6052
[06:08:08.989774] Epoch: [81]  [100/781]  eta: 0:02:01  lr: 0.000024  training_loss: 1.1458 (1.1315)  classification_loss: 1.1457 (1.1314)  loss_mask: 0.0001 (0.0001)  time: 0.1708  data: 0.0002  max mem: 6052
[06:08:12.413449] Epoch: [81]  [120/781]  eta: 0:01:56  lr: 0.000024  training_loss: 1.1141 (1.1292)  classification_loss: 1.1140 (1.1292)  loss_mask: 0.0001 (0.0001)  time: 0.1711  data: 0.0002  max mem: 6052
[06:08:15.825504] Epoch: [81]  [140/781]  eta: 0:01:52  lr: 0.000024  training_loss: 1.1560 (1.1328)  classification_loss: 1.1560 (1.1327)  loss_mask: 0.0001 (0.0001)  time: 0.1705  data: 0.0002  max mem: 6052
[06:08:19.248562] Epoch: [81]  [160/781]  eta: 0:01:48  lr: 0.000024  training_loss: 1.1352 (1.1338)  classification_loss: 1.1352 (1.1338)  loss_mask: 0.0001 (0.0001)  time: 0.1711  data: 0.0003  max mem: 6052
[06:08:22.668925] Epoch: [81]  [180/781]  eta: 0:01:45  lr: 0.000024  training_loss: 1.1373 (1.1359)  classification_loss: 1.1373 (1.1358)  loss_mask: 0.0001 (0.0001)  time: 0.1709  data: 0.0003  max mem: 6052
[06:08:26.109942] Epoch: [81]  [200/781]  eta: 0:01:41  lr: 0.000024  training_loss: 1.1038 (1.1345)  classification_loss: 1.1037 (1.1344)  loss_mask: 0.0001 (0.0001)  time: 0.1720  data: 0.0003  max mem: 6052
[06:08:29.540680] Epoch: [81]  [220/781]  eta: 0:01:37  lr: 0.000024  training_loss: 1.1336 (1.1348)  classification_loss: 1.1336 (1.1347)  loss_mask: 0.0001 (0.0001)  time: 0.1714  data: 0.0005  max mem: 6052
[06:08:32.930078] Epoch: [81]  [240/781]  eta: 0:01:34  lr: 0.000024  training_loss: 1.1624 (1.1370)  classification_loss: 1.1623 (1.1370)  loss_mask: 0.0001 (0.0001)  time: 0.1694  data: 0.0002  max mem: 6052
[06:08:36.351811] Epoch: [81]  [260/781]  eta: 0:01:30  lr: 0.000024  training_loss: 1.1305 (1.1379)  classification_loss: 1.1304 (1.1379)  loss_mask: 0.0001 (0.0001)  time: 0.1710  data: 0.0002  max mem: 6052
[06:08:39.814252] Epoch: [81]  [280/781]  eta: 0:01:26  lr: 0.000024  training_loss: 1.0798 (1.1360)  classification_loss: 1.0797 (1.1359)  loss_mask: 0.0001 (0.0001)  time: 0.1730  data: 0.0002  max mem: 6052
[06:08:43.228994] Epoch: [81]  [300/781]  eta: 0:01:23  lr: 0.000024  training_loss: 1.1902 (1.1406)  classification_loss: 1.1902 (1.1405)  loss_mask: 0.0001 (0.0001)  time: 0.1707  data: 0.0003  max mem: 6052
[06:08:46.658038] Epoch: [81]  [320/781]  eta: 0:01:19  lr: 0.000024  training_loss: 1.1497 (1.1427)  classification_loss: 1.1496 (1.1426)  loss_mask: 0.0001 (0.0001)  time: 0.1714  data: 0.0002  max mem: 6052
[06:08:50.066537] Epoch: [81]  [340/781]  eta: 0:01:16  lr: 0.000024  training_loss: 1.1370 (1.1441)  classification_loss: 1.1369 (1.1440)  loss_mask: 0.0001 (0.0001)  time: 0.1703  data: 0.0002  max mem: 6052
[06:08:53.474771] Epoch: [81]  [360/781]  eta: 0:01:12  lr: 0.000024  training_loss: 1.1391 (1.1454)  classification_loss: 1.1391 (1.1453)  loss_mask: 0.0001 (0.0001)  time: 0.1703  data: 0.0003  max mem: 6052
[06:08:56.891673] Epoch: [81]  [380/781]  eta: 0:01:09  lr: 0.000024  training_loss: 1.1409 (1.1473)  classification_loss: 1.1409 (1.1472)  loss_mask: 0.0001 (0.0001)  time: 0.1708  data: 0.0003  max mem: 6052
[06:09:00.305816] Epoch: [81]  [400/781]  eta: 0:01:05  lr: 0.000024  training_loss: 1.1482 (1.1470)  classification_loss: 1.1481 (1.1469)  loss_mask: 0.0001 (0.0001)  time: 0.1706  data: 0.0002  max mem: 6052
[06:09:03.723447] Epoch: [81]  [420/781]  eta: 0:01:02  lr: 0.000023  training_loss: 1.1321 (1.1469)  classification_loss: 1.1321 (1.1468)  loss_mask: 0.0001 (0.0001)  time: 0.1708  data: 0.0003  max mem: 6052
[06:09:07.133663] Epoch: [81]  [440/781]  eta: 0:00:58  lr: 0.000023  training_loss: 1.0972 (1.1455)  classification_loss: 1.0972 (1.1454)  loss_mask: 0.0001 (0.0001)  time: 0.1704  data: 0.0003  max mem: 6052
[06:09:10.538062] Epoch: [81]  [460/781]  eta: 0:00:55  lr: 0.000023  training_loss: 1.0735 (1.1435)  classification_loss: 1.0734 (1.1434)  loss_mask: 0.0001 (0.0001)  time: 0.1701  data: 0.0003  max mem: 6052
[06:09:13.937892] Epoch: [81]  [480/781]  eta: 0:00:51  lr: 0.000023  training_loss: 1.1529 (1.1446)  classification_loss: 1.1529 (1.1445)  loss_mask: 0.0001 (0.0001)  time: 0.1699  data: 0.0003  max mem: 6052
[06:09:17.348833] Epoch: [81]  [500/781]  eta: 0:00:48  lr: 0.000023  training_loss: 1.1813 (1.1458)  classification_loss: 1.1812 (1.1457)  loss_mask: 0.0000 (0.0001)  time: 0.1705  data: 0.0003  max mem: 6052
[06:09:20.741333] Epoch: [81]  [520/781]  eta: 0:00:44  lr: 0.000023  training_loss: 1.1262 (1.1450)  classification_loss: 1.1262 (1.1450)  loss_mask: 0.0001 (0.0001)  time: 0.1695  data: 0.0001  max mem: 6052
[06:09:24.144308] Epoch: [81]  [540/781]  eta: 0:00:41  lr: 0.000023  training_loss: 1.1835 (1.1464)  classification_loss: 1.1833 (1.1464)  loss_mask: 0.0001 (0.0001)  time: 0.1701  data: 0.0002  max mem: 6052
[06:09:27.560318] Epoch: [81]  [560/781]  eta: 0:00:38  lr: 0.000023  training_loss: 1.1179 (1.1461)  classification_loss: 1.1178 (1.1460)  loss_mask: 0.0001 (0.0001)  time: 0.1707  data: 0.0002  max mem: 6052
[06:09:30.957339] Epoch: [81]  [580/781]  eta: 0:00:34  lr: 0.000023  training_loss: 1.1343 (1.1451)  classification_loss: 1.1343 (1.1451)  loss_mask: 0.0001 (0.0001)  time: 0.1698  data: 0.0002  max mem: 6052
[06:09:34.356890] Epoch: [81]  [600/781]  eta: 0:00:31  lr: 0.000023  training_loss: 1.1037 (1.1445)  classification_loss: 1.1037 (1.1444)  loss_mask: 0.0001 (0.0001)  time: 0.1699  data: 0.0002  max mem: 6052
[06:09:37.761194] Epoch: [81]  [620/781]  eta: 0:00:27  lr: 0.000023  training_loss: 1.1482 (1.1447)  classification_loss: 1.1481 (1.1446)  loss_mask: 0.0001 (0.0001)  time: 0.1701  data: 0.0002  max mem: 6052
[06:09:41.150830] Epoch: [81]  [640/781]  eta: 0:00:24  lr: 0.000023  training_loss: 1.1124 (1.1446)  classification_loss: 1.1123 (1.1445)  loss_mask: 0.0001 (0.0001)  time: 0.1694  data: 0.0002  max mem: 6052
[06:09:44.580064] Epoch: [81]  [660/781]  eta: 0:00:20  lr: 0.000023  training_loss: 1.0898 (1.1436)  classification_loss: 1.0897 (1.1435)  loss_mask: 0.0001 (0.0001)  time: 0.1714  data: 0.0002  max mem: 6052
[06:09:47.992851] Epoch: [81]  [680/781]  eta: 0:00:17  lr: 0.000023  training_loss: 1.1561 (1.1442)  classification_loss: 1.1560 (1.1441)  loss_mask: 0.0001 (0.0001)  time: 0.1706  data: 0.0002  max mem: 6052
[06:09:51.404883] Epoch: [81]  [700/781]  eta: 0:00:13  lr: 0.000023  training_loss: 1.1452 (1.1442)  classification_loss: 1.1452 (1.1441)  loss_mask: 0.0001 (0.0001)  time: 0.1705  data: 0.0004  max mem: 6052
[06:09:54.820556] Epoch: [81]  [720/781]  eta: 0:00:10  lr: 0.000023  training_loss: 1.1058 (1.1434)  classification_loss: 1.1057 (1.1433)  loss_mask: 0.0001 (0.0001)  time: 0.1707  data: 0.0002  max mem: 6052
[06:09:58.264569] Epoch: [81]  [740/781]  eta: 0:00:07  lr: 0.000023  training_loss: 1.1146 (1.1429)  classification_loss: 1.1145 (1.1428)  loss_mask: 0.0001 (0.0001)  time: 0.1721  data: 0.0003  max mem: 6052
[06:10:01.823230] Epoch: [81]  [760/781]  eta: 0:00:03  lr: 0.000022  training_loss: 1.1037 (1.1426)  classification_loss: 1.1037 (1.1425)  loss_mask: 0.0000 (0.0001)  time: 0.1779  data: 0.0003  max mem: 6052
[06:10:05.228441] Epoch: [81]  [780/781]  eta: 0:00:00  lr: 0.000022  training_loss: 1.1315 (1.1426)  classification_loss: 1.1314 (1.1426)  loss_mask: 0.0001 (0.0001)  time: 0.1702  data: 0.0002  max mem: 6052
[06:10:05.391570] Epoch: [81] Total time: 0:02:14 (0.1721 s / it)
[06:10:05.392040] Averaged stats: lr: 0.000022  training_loss: 1.1315 (1.1426)  classification_loss: 1.1314 (1.1426)  loss_mask: 0.0001 (0.0001)
[06:10:06.068227] Test:  [  0/157]  eta: 0:01:45  testing_loss: 0.4360 (0.4360)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (100.0000)  time: 0.6705  data: 0.6298  max mem: 6052
[06:10:06.359016] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4491 (0.4738)  acc1: 87.5000 (86.0795)  acc5: 100.0000 (99.8580)  time: 0.0872  data: 0.0580  max mem: 6052
[06:10:06.648622] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4491 (0.4512)  acc1: 85.9375 (86.7560)  acc5: 100.0000 (99.6280)  time: 0.0289  data: 0.0006  max mem: 6052
[06:10:06.929184] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4332 (0.4592)  acc1: 85.9375 (86.1895)  acc5: 100.0000 (99.4456)  time: 0.0284  data: 0.0003  max mem: 6052
[06:10:07.217648] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4757 (0.4662)  acc1: 85.9375 (86.0899)  acc5: 98.4375 (99.2378)  time: 0.0283  data: 0.0002  max mem: 6052
[06:10:07.505783] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4436 (0.4630)  acc1: 87.5000 (86.3664)  acc5: 100.0000 (99.2953)  time: 0.0287  data: 0.0002  max mem: 6052
[06:10:07.786876] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4276 (0.4605)  acc1: 87.5000 (86.5523)  acc5: 100.0000 (99.3084)  time: 0.0283  data: 0.0001  max mem: 6052
[06:10:08.069263] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4425 (0.4577)  acc1: 85.9375 (86.5757)  acc5: 100.0000 (99.3618)  time: 0.0281  data: 0.0001  max mem: 6052
[06:10:08.350050] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4686 (0.4638)  acc1: 85.9375 (86.3426)  acc5: 100.0000 (99.3248)  time: 0.0280  data: 0.0001  max mem: 6052
[06:10:08.630192] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4522 (0.4607)  acc1: 87.5000 (86.4183)  acc5: 100.0000 (99.3132)  time: 0.0279  data: 0.0001  max mem: 6052
[06:10:08.912211] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4440 (0.4630)  acc1: 87.5000 (86.2160)  acc5: 100.0000 (99.3657)  time: 0.0280  data: 0.0001  max mem: 6052
[06:10:09.193361] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4403 (0.4608)  acc1: 85.9375 (86.3739)  acc5: 100.0000 (99.3666)  time: 0.0280  data: 0.0001  max mem: 6052
[06:10:09.474537] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4403 (0.4604)  acc1: 87.5000 (86.4024)  acc5: 100.0000 (99.3673)  time: 0.0280  data: 0.0001  max mem: 6052
[06:10:09.755264] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4548 (0.4612)  acc1: 84.3750 (86.4385)  acc5: 100.0000 (99.3917)  time: 0.0280  data: 0.0001  max mem: 6052
[06:10:10.036145] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4486 (0.4592)  acc1: 87.5000 (86.5802)  acc5: 100.0000 (99.3905)  time: 0.0280  data: 0.0001  max mem: 6052
[06:10:10.316614] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4582 (0.4599)  acc1: 87.5000 (86.5066)  acc5: 100.0000 (99.4205)  time: 0.0280  data: 0.0001  max mem: 6052
[06:10:10.466859] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4859 (0.4619)  acc1: 85.9375 (86.4000)  acc5: 100.0000 (99.4200)  time: 0.0270  data: 0.0001  max mem: 6052
[06:10:10.619795] Test: Total time: 0:00:05 (0.0333 s / it)
[06:10:10.620400] * Acc@1 86.400 Acc@5 99.420 loss 0.462
[06:10:10.620738] Accuracy of the network on the 10000 test images: 86.4%
[06:10:10.620926] Max accuracy: 86.51%
[06:10:10.884750] log_dir: ./output_dir
[06:10:11.764849] Epoch: [82]  [  0/781]  eta: 0:11:26  lr: 0.000022  training_loss: 1.0207 (1.0207)  classification_loss: 1.0206 (1.0206)  loss_mask: 0.0001 (0.0001)  time: 0.8785  data: 0.6975  max mem: 6052
[06:10:15.172439] Epoch: [82]  [ 20/781]  eta: 0:02:35  lr: 0.000022  training_loss: 1.1577 (1.1493)  classification_loss: 1.1576 (1.1492)  loss_mask: 0.0001 (0.0001)  time: 0.1703  data: 0.0003  max mem: 6052
[06:10:18.591249] Epoch: [82]  [ 40/781]  eta: 0:02:19  lr: 0.000022  training_loss: 1.1825 (1.1733)  classification_loss: 1.1823 (1.1733)  loss_mask: 0.0001 (0.0001)  time: 0.1709  data: 0.0003  max mem: 6052
[06:10:22.005309] Epoch: [82]  [ 60/781]  eta: 0:02:11  lr: 0.000022  training_loss: 1.1481 (1.1629)  classification_loss: 1.1480 (1.1628)  loss_mask: 0.0001 (0.0001)  time: 0.1706  data: 0.0002  max mem: 6052
[06:10:25.413152] Epoch: [82]  [ 80/781]  eta: 0:02:05  lr: 0.000022  training_loss: 1.1684 (1.1605)  classification_loss: 1.1683 (1.1604)  loss_mask: 0.0001 (0.0001)  time: 0.1703  data: 0.0002  max mem: 6052
[06:10:28.850306] Epoch: [82]  [100/781]  eta: 0:02:01  lr: 0.000022  training_loss: 1.1359 (1.1582)  classification_loss: 1.1358 (1.1581)  loss_mask: 0.0001 (0.0001)  time: 0.1718  data: 0.0002  max mem: 6052
[06:10:32.293665] Epoch: [82]  [120/781]  eta: 0:01:56  lr: 0.000022  training_loss: 1.1685 (1.1553)  classification_loss: 1.1685 (1.1553)  loss_mask: 0.0001 (0.0001)  time: 0.1720  data: 0.0003  max mem: 6052
[06:10:35.741860] Epoch: [82]  [140/781]  eta: 0:01:52  lr: 0.000022  training_loss: 1.1270 (1.1529)  classification_loss: 1.1270 (1.1529)  loss_mask: 0.0001 (0.0001)  time: 0.1723  data: 0.0002  max mem: 6052
[06:10:39.171685] Epoch: [82]  [160/781]  eta: 0:01:49  lr: 0.000022  training_loss: 1.1487 (1.1522)  classification_loss: 1.1487 (1.1521)  loss_mask: 0.0001 (0.0001)  time: 0.1714  data: 0.0002  max mem: 6052
[06:10:42.583262] Epoch: [82]  [180/781]  eta: 0:01:45  lr: 0.000022  training_loss: 1.1013 (1.1507)  classification_loss: 1.1013 (1.1506)  loss_mask: 0.0001 (0.0001)  time: 0.1705  data: 0.0003  max mem: 6052
[06:10:45.998935] Epoch: [82]  [200/781]  eta: 0:01:41  lr: 0.000022  training_loss: 1.1574 (1.1513)  classification_loss: 1.1574 (1.1513)  loss_mask: 0.0001 (0.0001)  time: 0.1707  data: 0.0002  max mem: 6052
[06:10:49.426416] Epoch: [82]  [220/781]  eta: 0:01:37  lr: 0.000022  training_loss: 1.1603 (1.1517)  classification_loss: 1.1602 (1.1517)  loss_mask: 0.0001 (0.0001)  time: 0.1713  data: 0.0002  max mem: 6052
[06:10:52.846472] Epoch: [82]  [240/781]  eta: 0:01:34  lr: 0.000022  training_loss: 1.1140 (1.1509)  classification_loss: 1.1139 (1.1508)  loss_mask: 0.0001 (0.0001)  time: 0.1709  data: 0.0002  max mem: 6052
[06:10:56.276147] Epoch: [82]  [260/781]  eta: 0:01:30  lr: 0.000022  training_loss: 1.1432 (1.1489)  classification_loss: 1.1431 (1.1489)  loss_mask: 0.0001 (0.0001)  time: 0.1714  data: 0.0002  max mem: 6052
[06:10:59.711941] Epoch: [82]  [280/781]  eta: 0:01:27  lr: 0.000022  training_loss: 1.1149 (1.1463)  classification_loss: 1.1149 (1.1462)  loss_mask: 0.0000 (0.0001)  time: 0.1717  data: 0.0003  max mem: 6052
[06:11:03.127314] Epoch: [82]  [300/781]  eta: 0:01:23  lr: 0.000022  training_loss: 1.1450 (1.1468)  classification_loss: 1.1449 (1.1467)  loss_mask: 0.0001 (0.0001)  time: 0.1707  data: 0.0002  max mem: 6052
[06:11:06.525728] Epoch: [82]  [320/781]  eta: 0:01:19  lr: 0.000021  training_loss: 1.1144 (1.1456)  classification_loss: 1.1143 (1.1455)  loss_mask: 0.0001 (0.0001)  time: 0.1698  data: 0.0001  max mem: 6052
[06:11:09.946484] Epoch: [82]  [340/781]  eta: 0:01:16  lr: 0.000021  training_loss: 1.1413 (1.1445)  classification_loss: 1.1412 (1.1444)  loss_mask: 0.0001 (0.0001)  time: 0.1710  data: 0.0002  max mem: 6052
[06:11:13.361877] Epoch: [82]  [360/781]  eta: 0:01:12  lr: 0.000021  training_loss: 1.1843 (1.1472)  classification_loss: 1.1842 (1.1471)  loss_mask: 0.0001 (0.0001)  time: 0.1707  data: 0.0002  max mem: 6052
[06:11:16.785105] Epoch: [82]  [380/781]  eta: 0:01:09  lr: 0.000021  training_loss: 1.1484 (1.1474)  classification_loss: 1.1483 (1.1474)  loss_mask: 0.0001 (0.0001)  time: 0.1711  data: 0.0004  max mem: 6052
[06:11:20.202835] Epoch: [82]  [400/781]  eta: 0:01:05  lr: 0.000021  training_loss: 1.1665 (1.1479)  classification_loss: 1.1664 (1.1478)  loss_mask: 0.0000 (0.0001)  time: 0.1708  data: 0.0002  max mem: 6052
[06:11:23.619015] Epoch: [82]  [420/781]  eta: 0:01:02  lr: 0.000021  training_loss: 1.1475 (1.1478)  classification_loss: 1.1475 (1.1478)  loss_mask: 0.0000 (0.0001)  time: 0.1707  data: 0.0002  max mem: 6052
[06:11:27.039031] Epoch: [82]  [440/781]  eta: 0:00:58  lr: 0.000021  training_loss: 1.1566 (1.1471)  classification_loss: 1.1565 (1.1471)  loss_mask: 0.0001 (0.0001)  time: 0.1709  data: 0.0002  max mem: 6052
[06:11:30.460535] Epoch: [82]  [460/781]  eta: 0:00:55  lr: 0.000021  training_loss: 1.1057 (1.1462)  classification_loss: 1.1057 (1.1461)  loss_mask: 0.0001 (0.0001)  time: 0.1710  data: 0.0002  max mem: 6052
[06:11:33.888992] Epoch: [82]  [480/781]  eta: 0:00:51  lr: 0.000021  training_loss: 1.1663 (1.1474)  classification_loss: 1.1662 (1.1474)  loss_mask: 0.0000 (0.0001)  time: 0.1713  data: 0.0004  max mem: 6052
[06:11:37.312213] Epoch: [82]  [500/781]  eta: 0:00:48  lr: 0.000021  training_loss: 1.1236 (1.1471)  classification_loss: 1.1235 (1.1470)  loss_mask: 0.0000 (0.0001)  time: 0.1711  data: 0.0002  max mem: 6052
[06:11:40.726001] Epoch: [82]  [520/781]  eta: 0:00:44  lr: 0.000021  training_loss: 1.1605 (1.1472)  classification_loss: 1.1605 (1.1471)  loss_mask: 0.0000 (0.0001)  time: 0.1706  data: 0.0002  max mem: 6052
[06:11:44.132118] Epoch: [82]  [540/781]  eta: 0:00:41  lr: 0.000021  training_loss: 1.1234 (1.1472)  classification_loss: 1.1234 (1.1471)  loss_mask: 0.0001 (0.0001)  time: 0.1702  data: 0.0003  max mem: 6052
[06:11:47.538599] Epoch: [82]  [560/781]  eta: 0:00:38  lr: 0.000021  training_loss: 1.1252 (1.1466)  classification_loss: 1.1251 (1.1466)  loss_mask: 0.0000 (0.0001)  time: 0.1703  data: 0.0003  max mem: 6052
[06:11:50.949481] Epoch: [82]  [580/781]  eta: 0:00:34  lr: 0.000021  training_loss: 1.1091 (1.1466)  classification_loss: 1.1091 (1.1465)  loss_mask: 0.0001 (0.0001)  time: 0.1705  data: 0.0002  max mem: 6052
[06:11:54.371651] Epoch: [82]  [600/781]  eta: 0:00:31  lr: 0.000021  training_loss: 1.1126 (1.1465)  classification_loss: 1.1126 (1.1464)  loss_mask: 0.0001 (0.0001)  time: 0.1710  data: 0.0002  max mem: 6052
[06:11:57.794441] Epoch: [82]  [620/781]  eta: 0:00:27  lr: 0.000021  training_loss: 1.1326 (1.1463)  classification_loss: 1.1325 (1.1463)  loss_mask: 0.0001 (0.0001)  time: 0.1711  data: 0.0002  max mem: 6052
[06:12:01.305671] Epoch: [82]  [640/781]  eta: 0:00:24  lr: 0.000021  training_loss: 1.1062 (1.1450)  classification_loss: 1.1061 (1.1449)  loss_mask: 0.0001 (0.0001)  time: 0.1755  data: 0.0002  max mem: 6052
[06:12:04.732110] Epoch: [82]  [660/781]  eta: 0:00:20  lr: 0.000021  training_loss: 1.1004 (1.1450)  classification_loss: 1.1004 (1.1449)  loss_mask: 0.0000 (0.0001)  time: 0.1712  data: 0.0004  max mem: 6052
[06:12:08.146524] Epoch: [82]  [680/781]  eta: 0:00:17  lr: 0.000020  training_loss: 1.1776 (1.1457)  classification_loss: 1.1775 (1.1457)  loss_mask: 0.0000 (0.0001)  time: 0.1706  data: 0.0002  max mem: 6052
[06:12:11.571800] Epoch: [82]  [700/781]  eta: 0:00:13  lr: 0.000020  training_loss: 1.1896 (1.1465)  classification_loss: 1.1896 (1.1464)  loss_mask: 0.0001 (0.0001)  time: 0.1712  data: 0.0002  max mem: 6052
[06:12:15.036413] Epoch: [82]  [720/781]  eta: 0:00:10  lr: 0.000020  training_loss: 1.1258 (1.1467)  classification_loss: 1.1257 (1.1466)  loss_mask: 0.0001 (0.0001)  time: 0.1731  data: 0.0003  max mem: 6052
[06:12:18.471399] Epoch: [82]  [740/781]  eta: 0:00:07  lr: 0.000020  training_loss: 1.1698 (1.1469)  classification_loss: 1.1698 (1.1469)  loss_mask: 0.0001 (0.0001)  time: 0.1717  data: 0.0003  max mem: 6052
[06:12:21.876185] Epoch: [82]  [760/781]  eta: 0:00:03  lr: 0.000020  training_loss: 1.1728 (1.1480)  classification_loss: 1.1728 (1.1480)  loss_mask: 0.0001 (0.0001)  time: 0.1701  data: 0.0003  max mem: 6052
[06:12:25.258831] Epoch: [82]  [780/781]  eta: 0:00:00  lr: 0.000020  training_loss: 1.1985 (1.1491)  classification_loss: 1.1985 (1.1490)  loss_mask: 0.0001 (0.0001)  time: 0.1691  data: 0.0002  max mem: 6052
[06:12:25.423559] Epoch: [82] Total time: 0:02:14 (0.1723 s / it)
[06:12:25.424065] Averaged stats: lr: 0.000020  training_loss: 1.1985 (1.1491)  classification_loss: 1.1985 (1.1490)  loss_mask: 0.0001 (0.0001)
[06:12:26.099693] Test:  [  0/157]  eta: 0:01:45  testing_loss: 0.4629 (0.4629)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.6710  data: 0.6381  max mem: 6052
[06:12:26.383772] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4473 (0.4688)  acc1: 87.5000 (86.0795)  acc5: 100.0000 (99.7159)  time: 0.0867  data: 0.0582  max mem: 6052
[06:12:26.669526] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4301 (0.4440)  acc1: 87.5000 (86.8304)  acc5: 100.0000 (99.6280)  time: 0.0283  data: 0.0002  max mem: 6052
[06:12:26.951008] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4184 (0.4555)  acc1: 85.9375 (86.3407)  acc5: 100.0000 (99.4456)  time: 0.0282  data: 0.0002  max mem: 6052
[06:12:27.232780] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4690 (0.4604)  acc1: 85.9375 (86.0899)  acc5: 98.4375 (99.2378)  time: 0.0280  data: 0.0002  max mem: 6052
[06:12:27.517048] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4317 (0.4565)  acc1: 85.9375 (86.3971)  acc5: 98.4375 (99.2647)  time: 0.0281  data: 0.0001  max mem: 6052
[06:12:27.802488] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4167 (0.4552)  acc1: 87.5000 (86.4242)  acc5: 100.0000 (99.2828)  time: 0.0283  data: 0.0002  max mem: 6052
[06:12:28.087534] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4301 (0.4518)  acc1: 87.5000 (86.5097)  acc5: 100.0000 (99.3178)  time: 0.0284  data: 0.0001  max mem: 6052
[06:12:28.371577] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4557 (0.4574)  acc1: 85.9375 (86.4005)  acc5: 100.0000 (99.3634)  time: 0.0283  data: 0.0001  max mem: 6052
[06:12:28.656154] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4465 (0.4554)  acc1: 87.5000 (86.5728)  acc5: 100.0000 (99.3819)  time: 0.0283  data: 0.0002  max mem: 6052
[06:12:28.939590] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4429 (0.4572)  acc1: 87.5000 (86.5254)  acc5: 100.0000 (99.3657)  time: 0.0283  data: 0.0002  max mem: 6052
[06:12:29.224528] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4404 (0.4566)  acc1: 85.9375 (86.5850)  acc5: 100.0000 (99.3666)  time: 0.0282  data: 0.0002  max mem: 6052
[06:12:29.504412] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4399 (0.4567)  acc1: 87.5000 (86.5832)  acc5: 100.0000 (99.3543)  time: 0.0281  data: 0.0001  max mem: 6052
[06:12:29.784093] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4338 (0.4577)  acc1: 87.5000 (86.5935)  acc5: 100.0000 (99.3798)  time: 0.0279  data: 0.0001  max mem: 6052
[06:12:30.063269] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4188 (0.4540)  acc1: 87.5000 (86.7575)  acc5: 100.0000 (99.4016)  time: 0.0278  data: 0.0001  max mem: 6052
[06:12:30.341144] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4190 (0.4535)  acc1: 87.5000 (86.7757)  acc5: 100.0000 (99.4205)  time: 0.0277  data: 0.0001  max mem: 6052
[06:12:30.491418] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4680 (0.4550)  acc1: 87.5000 (86.7300)  acc5: 100.0000 (99.4300)  time: 0.0268  data: 0.0001  max mem: 6052
[06:12:30.646565] Test: Total time: 0:00:05 (0.0332 s / it)
[06:12:30.647340] * Acc@1 86.730 Acc@5 99.430 loss 0.455
[06:12:30.647651] Accuracy of the network on the 10000 test images: 86.7%
[06:12:30.647829] Max accuracy: 86.73%
[06:12:30.882694] log_dir: ./output_dir
[06:12:31.740454] Epoch: [83]  [  0/781]  eta: 0:11:08  lr: 0.000020  training_loss: 0.9057 (0.9057)  classification_loss: 0.9056 (0.9056)  loss_mask: 0.0001 (0.0001)  time: 0.8557  data: 0.6441  max mem: 6052
[06:12:35.165535] Epoch: [83]  [ 20/781]  eta: 0:02:35  lr: 0.000020  training_loss: 1.1082 (1.1269)  classification_loss: 1.1081 (1.1269)  loss_mask: 0.0000 (0.0001)  time: 0.1712  data: 0.0003  max mem: 6052
[06:12:38.626766] Epoch: [83]  [ 40/781]  eta: 0:02:19  lr: 0.000020  training_loss: 1.1521 (1.1460)  classification_loss: 1.1519 (1.1460)  loss_mask: 0.0001 (0.0001)  time: 0.1730  data: 0.0002  max mem: 6052
[06:12:42.093593] Epoch: [83]  [ 60/781]  eta: 0:02:12  lr: 0.000020  training_loss: 1.1391 (1.1504)  classification_loss: 1.1373 (1.1477)  loss_mask: 0.0001 (0.0026)  time: 0.1732  data: 0.0002  max mem: 6052
[06:12:45.514544] Epoch: [83]  [ 80/781]  eta: 0:02:06  lr: 0.000020  training_loss: 1.1396 (1.1503)  classification_loss: 1.1396 (1.1479)  loss_mask: 0.0005 (0.0024)  time: 0.1710  data: 0.0002  max mem: 6052
[06:12:48.964501] Epoch: [83]  [100/781]  eta: 0:02:01  lr: 0.000020  training_loss: 1.1767 (1.1528)  classification_loss: 1.1766 (1.1509)  loss_mask: 0.0002 (0.0020)  time: 0.1724  data: 0.0002  max mem: 6052
[06:12:52.379285] Epoch: [83]  [120/781]  eta: 0:01:57  lr: 0.000020  training_loss: 1.1011 (1.1493)  classification_loss: 1.1010 (1.1476)  loss_mask: 0.0001 (0.0017)  time: 0.1707  data: 0.0002  max mem: 6052
[06:12:55.795373] Epoch: [83]  [140/781]  eta: 0:01:53  lr: 0.000020  training_loss: 1.0995 (1.1427)  classification_loss: 1.0995 (1.1412)  loss_mask: 0.0001 (0.0014)  time: 0.1707  data: 0.0002  max mem: 6052
[06:12:59.226845] Epoch: [83]  [160/781]  eta: 0:01:49  lr: 0.000020  training_loss: 1.1197 (1.1409)  classification_loss: 1.1148 (1.1392)  loss_mask: 0.0001 (0.0017)  time: 0.1714  data: 0.0002  max mem: 6052
[06:13:02.633942] Epoch: [83]  [180/781]  eta: 0:01:45  lr: 0.000020  training_loss: 1.1303 (1.1407)  classification_loss: 1.1276 (1.1375)  loss_mask: 0.0008 (0.0032)  time: 0.1703  data: 0.0002  max mem: 6052
[06:13:06.068327] Epoch: [83]  [200/781]  eta: 0:01:41  lr: 0.000020  training_loss: 1.1204 (1.1436)  classification_loss: 1.0810 (1.1380)  loss_mask: 0.0057 (0.0056)  time: 0.1716  data: 0.0002  max mem: 6052
[06:13:09.516879] Epoch: [83]  [220/781]  eta: 0:01:38  lr: 0.000020  training_loss: 1.1469 (1.1442)  classification_loss: 1.1469 (1.1388)  loss_mask: 0.0004 (0.0054)  time: 0.1723  data: 0.0003  max mem: 6052
[06:13:12.935285] Epoch: [83]  [240/781]  eta: 0:01:34  lr: 0.000019  training_loss: 1.1471 (1.1435)  classification_loss: 1.1468 (1.1386)  loss_mask: 0.0002 (0.0050)  time: 0.1709  data: 0.0002  max mem: 6052
[06:13:16.359176] Epoch: [83]  [260/781]  eta: 0:01:30  lr: 0.000019  training_loss: 1.1929 (1.1453)  classification_loss: 1.1927 (1.1407)  loss_mask: 0.0001 (0.0046)  time: 0.1711  data: 0.0002  max mem: 6052
[06:13:19.784141] Epoch: [83]  [280/781]  eta: 0:01:27  lr: 0.000019  training_loss: 1.1184 (1.1433)  classification_loss: 1.1184 (1.1390)  loss_mask: 0.0001 (0.0043)  time: 0.1712  data: 0.0002  max mem: 6052
[06:13:23.186743] Epoch: [83]  [300/781]  eta: 0:01:23  lr: 0.000019  training_loss: 1.1362 (1.1436)  classification_loss: 1.1361 (1.1395)  loss_mask: 0.0001 (0.0040)  time: 0.1700  data: 0.0003  max mem: 6052
[06:13:26.574564] Epoch: [83]  [320/781]  eta: 0:01:19  lr: 0.000019  training_loss: 1.1191 (1.1420)  classification_loss: 1.1190 (1.1383)  loss_mask: 0.0001 (0.0038)  time: 0.1693  data: 0.0003  max mem: 6052
[06:13:29.975098] Epoch: [83]  [340/781]  eta: 0:01:16  lr: 0.000019  training_loss: 1.1396 (1.1414)  classification_loss: 1.1395 (1.1378)  loss_mask: 0.0001 (0.0036)  time: 0.1700  data: 0.0002  max mem: 6052
[06:13:33.410559] Epoch: [83]  [360/781]  eta: 0:01:12  lr: 0.000019  training_loss: 1.1393 (1.1412)  classification_loss: 1.1392 (1.1379)  loss_mask: 0.0001 (0.0034)  time: 0.1717  data: 0.0002  max mem: 6052
[06:13:36.808563] Epoch: [83]  [380/781]  eta: 0:01:09  lr: 0.000019  training_loss: 1.1268 (1.1411)  classification_loss: 1.1267 (1.1379)  loss_mask: 0.0001 (0.0032)  time: 0.1698  data: 0.0003  max mem: 6052
[06:13:40.224296] Epoch: [83]  [400/781]  eta: 0:01:05  lr: 0.000019  training_loss: 1.1339 (1.1403)  classification_loss: 1.1338 (1.1373)  loss_mask: 0.0001 (0.0030)  time: 0.1707  data: 0.0003  max mem: 6052
[06:13:43.640409] Epoch: [83]  [420/781]  eta: 0:01:02  lr: 0.000019  training_loss: 1.1722 (1.1420)  classification_loss: 1.1721 (1.1391)  loss_mask: 0.0001 (0.0029)  time: 0.1707  data: 0.0002  max mem: 6052
[06:13:47.052287] Epoch: [83]  [440/781]  eta: 0:00:58  lr: 0.000019  training_loss: 1.1381 (1.1411)  classification_loss: 1.1380 (1.1383)  loss_mask: 0.0001 (0.0028)  time: 0.1705  data: 0.0003  max mem: 6052
[06:13:50.465992] Epoch: [83]  [460/781]  eta: 0:00:55  lr: 0.000019  training_loss: 1.1431 (1.1412)  classification_loss: 1.1429 (1.1385)  loss_mask: 0.0001 (0.0027)  time: 0.1706  data: 0.0002  max mem: 6052
[06:13:53.872326] Epoch: [83]  [480/781]  eta: 0:00:51  lr: 0.000019  training_loss: 1.1365 (1.1414)  classification_loss: 1.1364 (1.1389)  loss_mask: 0.0001 (0.0026)  time: 0.1703  data: 0.0002  max mem: 6052
[06:13:57.284524] Epoch: [83]  [500/781]  eta: 0:00:48  lr: 0.000019  training_loss: 1.1459 (1.1424)  classification_loss: 1.1458 (1.1399)  loss_mask: 0.0001 (0.0025)  time: 0.1705  data: 0.0002  max mem: 6052
[06:14:00.702733] Epoch: [83]  [520/781]  eta: 0:00:44  lr: 0.000019  training_loss: 1.1191 (1.1420)  classification_loss: 1.1191 (1.1397)  loss_mask: 0.0001 (0.0024)  time: 0.1708  data: 0.0003  max mem: 6052
[06:14:04.113059] Epoch: [83]  [540/781]  eta: 0:00:41  lr: 0.000019  training_loss: 1.1439 (1.1418)  classification_loss: 1.1438 (1.1395)  loss_mask: 0.0001 (0.0023)  time: 0.1704  data: 0.0002  max mem: 6052
[06:14:07.501636] Epoch: [83]  [560/781]  eta: 0:00:38  lr: 0.000019  training_loss: 1.0692 (1.1408)  classification_loss: 1.0691 (1.1386)  loss_mask: 0.0001 (0.0022)  time: 0.1694  data: 0.0003  max mem: 6052
[06:14:10.920866] Epoch: [83]  [580/781]  eta: 0:00:34  lr: 0.000019  training_loss: 1.1128 (1.1402)  classification_loss: 1.1124 (1.1380)  loss_mask: 0.0001 (0.0021)  time: 0.1708  data: 0.0002  max mem: 6052
[06:14:14.354608] Epoch: [83]  [600/781]  eta: 0:00:31  lr: 0.000019  training_loss: 1.1142 (1.1398)  classification_loss: 1.1142 (1.1377)  loss_mask: 0.0001 (0.0021)  time: 0.1716  data: 0.0002  max mem: 6052
[06:14:17.792211] Epoch: [83]  [620/781]  eta: 0:00:27  lr: 0.000018  training_loss: 1.1418 (1.1396)  classification_loss: 1.1417 (1.1376)  loss_mask: 0.0001 (0.0020)  time: 0.1718  data: 0.0002  max mem: 6052
[06:14:21.207543] Epoch: [83]  [640/781]  eta: 0:00:24  lr: 0.000018  training_loss: 1.1392 (1.1393)  classification_loss: 1.1391 (1.1374)  loss_mask: 0.0001 (0.0019)  time: 0.1707  data: 0.0003  max mem: 6052
[06:14:24.622332] Epoch: [83]  [660/781]  eta: 0:00:20  lr: 0.000018  training_loss: 1.1356 (1.1394)  classification_loss: 1.1355 (1.1375)  loss_mask: 0.0001 (0.0019)  time: 0.1706  data: 0.0003  max mem: 6052
[06:14:28.025057] Epoch: [83]  [680/781]  eta: 0:00:17  lr: 0.000018  training_loss: 1.1292 (1.1397)  classification_loss: 1.1291 (1.1379)  loss_mask: 0.0001 (0.0018)  time: 0.1700  data: 0.0002  max mem: 6052
[06:14:31.446596] Epoch: [83]  [700/781]  eta: 0:00:13  lr: 0.000018  training_loss: 1.1922 (1.1408)  classification_loss: 1.1921 (1.1390)  loss_mask: 0.0001 (0.0018)  time: 0.1710  data: 0.0001  max mem: 6052
[06:14:34.895192] Epoch: [83]  [720/781]  eta: 0:00:10  lr: 0.000018  training_loss: 1.1915 (1.1422)  classification_loss: 1.1914 (1.1404)  loss_mask: 0.0001 (0.0017)  time: 0.1723  data: 0.0002  max mem: 6052
[06:14:38.341887] Epoch: [83]  [740/781]  eta: 0:00:07  lr: 0.000018  training_loss: 1.1067 (1.1418)  classification_loss: 1.1066 (1.1401)  loss_mask: 0.0001 (0.0017)  time: 0.1723  data: 0.0003  max mem: 6052

[06:14:41.758325] Epoch: [83]  [760/781]  eta: 0:00:03  lr: 0.000018  training_loss: 1.1694 (1.1422)  classification_loss: 1.1693 (1.1406)  loss_mask: 0.0001 (0.0016)  time: 0.1707  data: 0.0002  max mem: 6052
[06:14:45.305517] Epoch: [83]  [780/781]  eta: 0:00:00  lr: 0.000018  training_loss: 1.1581 (1.1434)  classification_loss: 1.1580 (1.1418)  loss_mask: 0.0001 (0.0016)  time: 0.1773  data: 0.0002  max mem: 6052
[06:14:45.477182] Epoch: [83] Total time: 0:02:14 (0.1723 s / it)
[06:14:45.477721] Averaged stats: lr: 0.000018  training_loss: 1.1581 (1.1434)  classification_loss: 1.1580 (1.1418)  loss_mask: 0.0001 (0.0016)
[06:14:46.116421] Test:  [  0/157]  eta: 0:01:39  testing_loss: 0.4534 (0.4534)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.6345  data: 0.5972  max mem: 6052
[06:14:46.409473] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4532 (0.4685)  acc1: 85.9375 (86.2216)  acc5: 100.0000 (99.4318)  time: 0.0837  data: 0.0545  max mem: 6052
[06:14:46.690978] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4278 (0.4426)  acc1: 85.9375 (87.2024)  acc5: 100.0000 (99.4048)  time: 0.0283  data: 0.0002  max mem: 6052
[06:14:46.972491] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4275 (0.4527)  acc1: 85.9375 (86.8448)  acc5: 100.0000 (99.3448)  time: 0.0280  data: 0.0001  max mem: 6052
[06:14:47.256363] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4822 (0.4567)  acc1: 87.5000 (86.8902)  acc5: 98.4375 (99.1997)  time: 0.0282  data: 0.0002  max mem: 6052
[06:14:47.548240] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4422 (0.4530)  acc1: 87.5000 (87.0711)  acc5: 98.4375 (99.2341)  time: 0.0287  data: 0.0004  max mem: 6052
[06:14:47.834827] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4240 (0.4518)  acc1: 85.9375 (87.0133)  acc5: 100.0000 (99.2572)  time: 0.0288  data: 0.0003  max mem: 6052
[06:14:48.118385] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4234 (0.4482)  acc1: 85.9375 (87.1039)  acc5: 100.0000 (99.3178)  time: 0.0284  data: 0.0002  max mem: 6052
[06:14:48.404895] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4553 (0.4537)  acc1: 85.9375 (86.8634)  acc5: 100.0000 (99.3248)  time: 0.0284  data: 0.0002  max mem: 6052
[06:14:48.692873] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4457 (0.4511)  acc1: 87.5000 (86.9677)  acc5: 100.0000 (99.3132)  time: 0.0286  data: 0.0001  max mem: 6052
[06:14:48.975172] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4361 (0.4540)  acc1: 87.5000 (86.9276)  acc5: 100.0000 (99.3502)  time: 0.0284  data: 0.0001  max mem: 6052
[06:14:49.257239] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4355 (0.4536)  acc1: 87.5000 (86.9651)  acc5: 100.0000 (99.3525)  time: 0.0281  data: 0.0002  max mem: 6052
[06:14:49.537712] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4355 (0.4549)  acc1: 85.9375 (86.8802)  acc5: 100.0000 (99.3156)  time: 0.0280  data: 0.0002  max mem: 6052
[06:14:49.820812] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4519 (0.4560)  acc1: 85.9375 (86.8678)  acc5: 100.0000 (99.3082)  time: 0.0281  data: 0.0001  max mem: 6052
[06:14:50.103957] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4063 (0.4526)  acc1: 89.0625 (87.0124)  acc5: 100.0000 (99.3240)  time: 0.0282  data: 0.0001  max mem: 6052
[06:14:50.381310] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4136 (0.4529)  acc1: 85.9375 (86.8895)  acc5: 100.0000 (99.3688)  time: 0.0279  data: 0.0001  max mem: 6052
[06:14:50.531609] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4655 (0.4546)  acc1: 85.9375 (86.8600)  acc5: 100.0000 (99.3600)  time: 0.0268  data: 0.0001  max mem: 6052
[06:14:50.689734] Test: Total time: 0:00:05 (0.0332 s / it)
[06:14:50.690220] * Acc@1 86.860 Acc@5 99.360 loss 0.455
[06:14:50.690526] Accuracy of the network on the 10000 test images: 86.9%
[06:14:50.690768] Max accuracy: 86.86%
[06:14:50.890576] log_dir: ./output_dir
[06:14:51.760109] Epoch: [84]  [  0/781]  eta: 0:11:17  lr: 0.000018  training_loss: 1.0614 (1.0614)  classification_loss: 1.0613 (1.0613)  loss_mask: 0.0002 (0.0002)  time: 0.8672  data: 0.6767  max mem: 6052
[06:14:55.178120] Epoch: [84]  [ 20/781]  eta: 0:02:35  lr: 0.000018  training_loss: 1.0609 (1.0725)  classification_loss: 1.0609 (1.0724)  loss_mask: 0.0001 (0.0001)  time: 0.1708  data: 0.0003  max mem: 6052
[06:14:58.595819] Epoch: [84]  [ 40/781]  eta: 0:02:19  lr: 0.000018  training_loss: 1.1039 (1.0947)  classification_loss: 1.1038 (1.0946)  loss_mask: 0.0001 (0.0001)  time: 0.1708  data: 0.0003  max mem: 6052
[06:15:02.047282] Epoch: [84]  [ 60/781]  eta: 0:02:11  lr: 0.000018  training_loss: 1.1750 (1.1107)  classification_loss: 1.1749 (1.1106)  loss_mask: 0.0000 (0.0001)  time: 0.1725  data: 0.0003  max mem: 6052
[06:15:05.521920] Epoch: [84]  [ 80/781]  eta: 0:02:06  lr: 0.000018  training_loss: 1.1531 (1.1210)  classification_loss: 1.1531 (1.1210)  loss_mask: 0.0001 (0.0001)  time: 0.1737  data: 0.0002  max mem: 6052
[06:15:08.926848] Epoch: [84]  [100/781]  eta: 0:02:01  lr: 0.000018  training_loss: 1.1335 (1.1294)  classification_loss: 1.1334 (1.1293)  loss_mask: 0.0001 (0.0001)  time: 0.1702  data: 0.0002  max mem: 6052
[06:15:12.340462] Epoch: [84]  [120/781]  eta: 0:01:57  lr: 0.000018  training_loss: 1.1569 (1.1354)  classification_loss: 1.1569 (1.1354)  loss_mask: 0.0000 (0.0001)  time: 0.1706  data: 0.0002  max mem: 6052
[06:15:15.858928] Epoch: [84]  [140/781]  eta: 0:01:53  lr: 0.000018  training_loss: 1.1454 (1.1336)  classification_loss: 1.1454 (1.1335)  loss_mask: 0.0001 (0.0001)  time: 0.1758  data: 0.0002  max mem: 6052
[06:15:19.298556] Epoch: [84]  [160/781]  eta: 0:01:49  lr: 0.000018  training_loss: 1.1142 (1.1363)  classification_loss: 1.1142 (1.1363)  loss_mask: 0.0001 (0.0001)  time: 0.1719  data: 0.0002  max mem: 6052
[06:15:22.708332] Epoch: [84]  [180/781]  eta: 0:01:45  lr: 0.000018  training_loss: 1.1566 (1.1390)  classification_loss: 1.1565 (1.1390)  loss_mask: 0.0001 (0.0001)  time: 0.1704  data: 0.0002  max mem: 6052
[06:15:26.130748] Epoch: [84]  [200/781]  eta: 0:01:41  lr: 0.000017  training_loss: 1.1396 (1.1405)  classification_loss: 1.1396 (1.1405)  loss_mask: 0.0001 (0.0001)  time: 0.1710  data: 0.0002  max mem: 6052
[06:15:29.538780] Epoch: [84]  [220/781]  eta: 0:01:38  lr: 0.000017  training_loss: 1.1564 (1.1405)  classification_loss: 1.1563 (1.1404)  loss_mask: 0.0001 (0.0001)  time: 0.1703  data: 0.0003  max mem: 6052
[06:15:32.960015] Epoch: [84]  [240/781]  eta: 0:01:34  lr: 0.000017  training_loss: 1.1364 (1.1392)  classification_loss: 1.1363 (1.1392)  loss_mask: 0.0001 (0.0001)  time: 0.1710  data: 0.0003  max mem: 6052
[06:15:36.381308] Epoch: [84]  [260/781]  eta: 0:01:30  lr: 0.000017  training_loss: 1.1309 (1.1402)  classification_loss: 1.1308 (1.1401)  loss_mask: 0.0001 (0.0001)  time: 0.1710  data: 0.0002  max mem: 6052
[06:15:39.790616] Epoch: [84]  [280/781]  eta: 0:01:27  lr: 0.000017  training_loss: 1.1359 (1.1406)  classification_loss: 1.1358 (1.1406)  loss_mask: 0.0001 (0.0001)  time: 0.1704  data: 0.0002  max mem: 6052
[06:15:43.202333] Epoch: [84]  [300/781]  eta: 0:01:23  lr: 0.000017  training_loss: 1.1179 (1.1393)  classification_loss: 1.1178 (1.1393)  loss_mask: 0.0001 (0.0001)  time: 0.1705  data: 0.0003  max mem: 6052
[06:15:46.608635] Epoch: [84]  [320/781]  eta: 0:01:19  lr: 0.000017  training_loss: 1.1253 (1.1393)  classification_loss: 1.1252 (1.1392)  loss_mask: 0.0001 (0.0001)  time: 0.1702  data: 0.0003  max mem: 6052
[06:15:50.012297] Epoch: [84]  [340/781]  eta: 0:01:16  lr: 0.000017  training_loss: 1.1202 (1.1375)  classification_loss: 1.1200 (1.1374)  loss_mask: 0.0001 (0.0001)  time: 0.1701  data: 0.0002  max mem: 6052
[06:15:53.442260] Epoch: [84]  [360/781]  eta: 0:01:12  lr: 0.000017  training_loss: 1.1335 (1.1376)  classification_loss: 1.1335 (1.1375)  loss_mask: 0.0001 (0.0001)  time: 0.1714  data: 0.0003  max mem: 6052
[06:15:56.860873] Epoch: [84]  [380/781]  eta: 0:01:09  lr: 0.000017  training_loss: 1.1399 (1.1373)  classification_loss: 1.1399 (1.1373)  loss_mask: 0.0000 (0.0001)  time: 0.1709  data: 0.0003  max mem: 6052
[06:16:00.331648] Epoch: [84]  [400/781]  eta: 0:01:05  lr: 0.000017  training_loss: 1.0921 (1.1361)  classification_loss: 1.0921 (1.1360)  loss_mask: 0.0001 (0.0001)  time: 0.1734  data: 0.0002  max mem: 6052
[06:16:03.771668] Epoch: [84]  [420/781]  eta: 0:01:02  lr: 0.000017  training_loss: 1.1048 (1.1356)  classification_loss: 1.1047 (1.1355)  loss_mask: 0.0001 (0.0001)  time: 0.1719  data: 0.0002  max mem: 6052
[06:16:07.205554] Epoch: [84]  [440/781]  eta: 0:00:58  lr: 0.000017  training_loss: 1.0923 (1.1340)  classification_loss: 1.0922 (1.1339)  loss_mask: 0.0001 (0.0001)  time: 0.1716  data: 0.0002  max mem: 6052
[06:16:10.654238] Epoch: [84]  [460/781]  eta: 0:00:55  lr: 0.000017  training_loss: 1.1096 (1.1335)  classification_loss: 1.1096 (1.1334)  loss_mask: 0.0001 (0.0001)  time: 0.1724  data: 0.0002  max mem: 6052
[06:16:14.076897] Epoch: [84]  [480/781]  eta: 0:00:52  lr: 0.000017  training_loss: 1.1406 (1.1338)  classification_loss: 1.1406 (1.1338)  loss_mask: 0.0001 (0.0001)  time: 0.1711  data: 0.0002  max mem: 6052
[06:16:17.482757] Epoch: [84]  [500/781]  eta: 0:00:48  lr: 0.000017  training_loss: 1.1706 (1.1352)  classification_loss: 1.1705 (1.1351)  loss_mask: 0.0001 (0.0001)  time: 0.1702  data: 0.0003  max mem: 6052
[06:16:20.885259] Epoch: [84]  [520/781]  eta: 0:00:45  lr: 0.000017  training_loss: 1.1174 (1.1349)  classification_loss: 1.1174 (1.1348)  loss_mask: 0.0000 (0.0001)  time: 0.1701  data: 0.0001  max mem: 6052
[06:16:24.305637] Epoch: [84]  [540/781]  eta: 0:00:41  lr: 0.000017  training_loss: 1.0964 (1.1347)  classification_loss: 1.0964 (1.1347)  loss_mask: 0.0001 (0.0001)  time: 0.1709  data: 0.0002  max mem: 6052
[06:16:27.826324] Epoch: [84]  [560/781]  eta: 0:00:38  lr: 0.000017  training_loss: 1.0933 (1.1341)  classification_loss: 1.0932 (1.1340)  loss_mask: 0.0001 (0.0001)  time: 0.1760  data: 0.0002  max mem: 6052
[06:16:31.254055] Epoch: [84]  [580/781]  eta: 0:00:34  lr: 0.000017  training_loss: 1.1111 (1.1335)  classification_loss: 1.1110 (1.1334)  loss_mask: 0.0001 (0.0001)  time: 0.1713  data: 0.0002  max mem: 6052
[06:16:34.648840] Epoch: [84]  [600/781]  eta: 0:00:31  lr: 0.000016  training_loss: 1.0621 (1.1312)  classification_loss: 1.0620 (1.1311)  loss_mask: 0.0001 (0.0001)  time: 0.1697  data: 0.0003  max mem: 6052
[06:16:38.068629] Epoch: [84]  [620/781]  eta: 0:00:27  lr: 0.000016  training_loss: 1.0737 (1.1300)  classification_loss: 1.0737 (1.1299)  loss_mask: 0.0000 (0.0001)  time: 0.1709  data: 0.0002  max mem: 6052
[06:16:41.502824] Epoch: [84]  [640/781]  eta: 0:00:24  lr: 0.000016  training_loss: 1.1298 (1.1307)  classification_loss: 1.1297 (1.1306)  loss_mask: 0.0001 (0.0001)  time: 0.1716  data: 0.0002  max mem: 6052
[06:16:44.907523] Epoch: [84]  [660/781]  eta: 0:00:20  lr: 0.000016  training_loss: 1.1560 (1.1314)  classification_loss: 1.1560 (1.1313)  loss_mask: 0.0001 (0.0001)  time: 0.1701  data: 0.0003  max mem: 6052
[06:16:48.387055] Epoch: [84]  [680/781]  eta: 0:00:17  lr: 0.000016  training_loss: 1.1362 (1.1319)  classification_loss: 1.1361 (1.1318)  loss_mask: 0.0000 (0.0001)  time: 0.1739  data: 0.0002  max mem: 6052
[06:16:51.810660] Epoch: [84]  [700/781]  eta: 0:00:13  lr: 0.000016  training_loss: 1.1061 (1.1314)  classification_loss: 1.1061 (1.1313)  loss_mask: 0.0001 (0.0001)  time: 0.1711  data: 0.0002  max mem: 6052
[06:16:55.226992] Epoch: [84]  [720/781]  eta: 0:00:10  lr: 0.000016  training_loss: 1.1622 (1.1329)  classification_loss: 1.1622 (1.1329)  loss_mask: 0.0001 (0.0001)  time: 0.1707  data: 0.0002  max mem: 6052
[06:16:58.653197] Epoch: [84]  [740/781]  eta: 0:00:07  lr: 0.000016  training_loss: 1.1266 (1.1329)  classification_loss: 1.1266 (1.1328)  loss_mask: 0.0001 (0.0001)  time: 0.1712  data: 0.0002  max mem: 6052
[06:17:02.074309] Epoch: [84]  [760/781]  eta: 0:00:03  lr: 0.000016  training_loss: 1.1282 (1.1329)  classification_loss: 1.1282 (1.1329)  loss_mask: 0.0001 (0.0001)  time: 0.1710  data: 0.0003  max mem: 6052
[06:17:05.488313] Epoch: [84]  [780/781]  eta: 0:00:00  lr: 0.000016  training_loss: 1.1432 (1.1335)  classification_loss: 1.1431 (1.1334)  loss_mask: 0.0000 (0.0001)  time: 0.1706  data: 0.0002  max mem: 6052
[06:17:05.648738] Epoch: [84] Total time: 0:02:14 (0.1725 s / it)
[06:17:05.649181] Averaged stats: lr: 0.000016  training_loss: 1.1432 (1.1335)  classification_loss: 1.1431 (1.1334)  loss_mask: 0.0000 (0.0001)
[06:17:06.315719] Test:  [  0/157]  eta: 0:01:44  testing_loss: 0.4457 (0.4457)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (100.0000)  time: 0.6626  data: 0.6334  max mem: 6052
[06:17:06.616687] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4457 (0.4720)  acc1: 85.9375 (86.3636)  acc5: 100.0000 (99.5739)  time: 0.0870  data: 0.0578  max mem: 6052
[06:17:06.901787] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4376 (0.4472)  acc1: 87.5000 (87.9464)  acc5: 100.0000 (99.4792)  time: 0.0289  data: 0.0002  max mem: 6052
[06:17:07.184749] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4340 (0.4536)  acc1: 87.5000 (87.5504)  acc5: 100.0000 (99.3952)  time: 0.0283  data: 0.0001  max mem: 6052
[06:17:07.471810] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4700 (0.4604)  acc1: 87.5000 (87.1570)  acc5: 98.4375 (99.1616)  time: 0.0284  data: 0.0002  max mem: 6052
[06:17:07.755346] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4562 (0.4567)  acc1: 87.5000 (87.2855)  acc5: 98.4375 (99.2341)  time: 0.0284  data: 0.0002  max mem: 6052
[06:17:08.038345] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4383 (0.4562)  acc1: 87.5000 (87.1670)  acc5: 100.0000 (99.2572)  time: 0.0281  data: 0.0002  max mem: 6052
[06:17:08.332052] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4454 (0.4535)  acc1: 87.5000 (87.1039)  acc5: 100.0000 (99.3178)  time: 0.0287  data: 0.0002  max mem: 6052
[06:17:08.613161] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4614 (0.4596)  acc1: 87.5000 (86.9599)  acc5: 100.0000 (99.2477)  time: 0.0286  data: 0.0001  max mem: 6052
[06:17:08.900044] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4508 (0.4556)  acc1: 87.5000 (87.1051)  acc5: 98.4375 (99.2617)  time: 0.0282  data: 0.0001  max mem: 6052
[06:17:09.183382] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4359 (0.4578)  acc1: 87.5000 (86.8967)  acc5: 100.0000 (99.2884)  time: 0.0284  data: 0.0002  max mem: 6052
[06:17:09.467422] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4359 (0.4566)  acc1: 85.9375 (86.9510)  acc5: 100.0000 (99.2821)  time: 0.0283  data: 0.0001  max mem: 6052
[06:17:09.753251] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4211 (0.4566)  acc1: 87.5000 (86.9318)  acc5: 100.0000 (99.2639)  time: 0.0284  data: 0.0002  max mem: 6052
[06:17:10.034835] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4194 (0.4574)  acc1: 87.5000 (86.9275)  acc5: 100.0000 (99.2724)  time: 0.0283  data: 0.0002  max mem: 6052
[06:17:10.315909] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4149 (0.4548)  acc1: 87.5000 (87.0678)  acc5: 100.0000 (99.2908)  time: 0.0280  data: 0.0001  max mem: 6052
[06:17:10.596312] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4248 (0.4552)  acc1: 85.9375 (86.9516)  acc5: 100.0000 (99.3274)  time: 0.0279  data: 0.0001  max mem: 6052
[06:17:10.745223] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4710 (0.4567)  acc1: 85.9375 (86.8800)  acc5: 100.0000 (99.3400)  time: 0.0269  data: 0.0001  max mem: 6052
[06:17:10.912096] Test: Total time: 0:00:05 (0.0335 s / it)
[06:17:10.912755] * Acc@1 86.880 Acc@5 99.340 loss 0.457
[06:17:10.913130] Accuracy of the network on the 10000 test images: 86.9%
[06:17:10.913572] Max accuracy: 86.88%
[06:17:11.229127] log_dir: ./output_dir
[06:17:12.148023] Epoch: [85]  [  0/781]  eta: 0:11:56  lr: 0.000016  training_loss: 1.0339 (1.0339)  classification_loss: 1.0338 (1.0338)  loss_mask: 0.0001 (0.0001)  time: 0.9171  data: 0.7275  max mem: 6052
[06:17:15.563335] Epoch: [85]  [ 20/781]  eta: 0:02:36  lr: 0.000016  training_loss: 1.0907 (1.1232)  classification_loss: 1.0907 (1.1232)  loss_mask: 0.0001 (0.0001)  time: 0.1707  data: 0.0002  max mem: 6052
[06:17:19.029304] Epoch: [85]  [ 40/781]  eta: 0:02:20  lr: 0.000016  training_loss: 1.1380 (1.1378)  classification_loss: 1.1379 (1.1377)  loss_mask: 0.0001 (0.0001)  time: 0.1732  data: 0.0002  max mem: 6052
[06:17:22.467187] Epoch: [85]  [ 60/781]  eta: 0:02:12  lr: 0.000016  training_loss: 1.0828 (1.1368)  classification_loss: 1.0827 (1.1367)  loss_mask: 0.0001 (0.0001)  time: 0.1718  data: 0.0002  max mem: 6052
[06:17:25.882752] Epoch: [85]  [ 80/781]  eta: 0:02:06  lr: 0.000016  training_loss: 1.1015 (1.1291)  classification_loss: 1.1015 (1.1291)  loss_mask: 0.0000 (0.0001)  time: 0.1707  data: 0.0002  max mem: 6052
[06:17:29.294408] Epoch: [85]  [100/781]  eta: 0:02:01  lr: 0.000016  training_loss: 1.1382 (1.1332)  classification_loss: 1.1381 (1.1331)  loss_mask: 0.0001 (0.0001)  time: 0.1705  data: 0.0003  max mem: 6052
[06:17:32.703837] Epoch: [85]  [120/781]  eta: 0:01:57  lr: 0.000016  training_loss: 1.0945 (1.1285)  classification_loss: 1.0945 (1.1284)  loss_mask: 0.0001 (0.0001)  time: 0.1704  data: 0.0002  max mem: 6052
[06:17:36.130802] Epoch: [85]  [140/781]  eta: 0:01:53  lr: 0.000016  training_loss: 1.0793 (1.1243)  classification_loss: 1.0793 (1.1242)  loss_mask: 0.0000 (0.0001)  time: 0.1713  data: 0.0002  max mem: 6052
[06:17:39.541726] Epoch: [85]  [160/781]  eta: 0:01:49  lr: 0.000016  training_loss: 1.1058 (1.1235)  classification_loss: 1.1058 (1.1234)  loss_mask: 0.0001 (0.0001)  time: 0.1705  data: 0.0002  max mem: 6052
[06:17:42.947485] Epoch: [85]  [180/781]  eta: 0:01:45  lr: 0.000016  training_loss: 1.1361 (1.1253)  classification_loss: 1.1361 (1.1252)  loss_mask: 0.0000 (0.0001)  time: 0.1702  data: 0.0002  max mem: 6052
[06:17:46.362468] Epoch: [85]  [200/781]  eta: 0:01:41  lr: 0.000016  training_loss: 1.1270 (1.1255)  classification_loss: 1.1269 (1.1254)  loss_mask: 0.0001 (0.0001)  time: 0.1707  data: 0.0003  max mem: 6052
[06:17:49.795171] Epoch: [85]  [220/781]  eta: 0:01:37  lr: 0.000015  training_loss: 1.1071 (1.1227)  classification_loss: 1.1070 (1.1226)  loss_mask: 0.0001 (0.0001)  time: 0.1715  data: 0.0003  max mem: 6052
[06:17:53.210893] Epoch: [85]  [240/781]  eta: 0:01:34  lr: 0.000015  training_loss: 1.1685 (1.1251)  classification_loss: 1.1684 (1.1250)  loss_mask: 0.0001 (0.0001)  time: 0.1707  data: 0.0002  max mem: 6052
[06:17:56.626970] Epoch: [85]  [260/781]  eta: 0:01:30  lr: 0.000015  training_loss: 1.1160 (1.1268)  classification_loss: 1.1160 (1.1268)  loss_mask: 0.0001 (0.0001)  time: 0.1707  data: 0.0002  max mem: 6052
[06:18:00.098717] Epoch: [85]  [280/781]  eta: 0:01:27  lr: 0.000015  training_loss: 1.1541 (1.1289)  classification_loss: 1.1540 (1.1289)  loss_mask: 0.0000 (0.0001)  time: 0.1735  data: 0.0002  max mem: 6052
[06:18:03.538061] Epoch: [85]  [300/781]  eta: 0:01:23  lr: 0.000015  training_loss: 1.1265 (1.1291)  classification_loss: 1.1265 (1.1291)  loss_mask: 0.0000 (0.0001)  time: 0.1719  data: 0.0002  max mem: 6052
[06:18:07.022335] Epoch: [85]  [320/781]  eta: 0:01:20  lr: 0.000015  training_loss: 1.0823 (1.1263)  classification_loss: 1.0822 (1.1263)  loss_mask: 0.0000 (0.0001)  time: 0.1741  data: 0.0003  max mem: 6052
[06:18:10.456051] Epoch: [85]  [340/781]  eta: 0:01:16  lr: 0.000015  training_loss: 1.0924 (1.1257)  classification_loss: 1.0924 (1.1256)  loss_mask: 0.0001 (0.0001)  time: 0.1716  data: 0.0002  max mem: 6052
[06:18:13.932027] Epoch: [85]  [360/781]  eta: 0:01:13  lr: 0.000015  training_loss: 1.1307 (1.1264)  classification_loss: 1.1306 (1.1264)  loss_mask: 0.0000 (0.0001)  time: 0.1737  data: 0.0003  max mem: 6052
[06:18:17.343648] Epoch: [85]  [380/781]  eta: 0:01:09  lr: 0.000015  training_loss: 1.1645 (1.1272)  classification_loss: 1.1644 (1.1271)  loss_mask: 0.0001 (0.0001)  time: 0.1705  data: 0.0002  max mem: 6052
[06:18:20.774122] Epoch: [85]  [400/781]  eta: 0:01:06  lr: 0.000015  training_loss: 1.1475 (1.1284)  classification_loss: 1.1475 (1.1284)  loss_mask: 0.0000 (0.0001)  time: 0.1714  data: 0.0002  max mem: 6052
[06:18:24.181116] Epoch: [85]  [420/781]  eta: 0:01:02  lr: 0.000015  training_loss: 1.1456 (1.1292)  classification_loss: 1.1455 (1.1292)  loss_mask: 0.0001 (0.0001)  time: 0.1703  data: 0.0002  max mem: 6052

[06:18:27.597536] Epoch: [85]  [440/781]  eta: 0:00:59  lr: 0.000015  training_loss: 1.1160 (1.1294)  classification_loss: 1.1160 (1.1293)  loss_mask: 0.0000 (0.0001)  time: 0.1707  data: 0.0002  max mem: 6052
[06:18:31.021129] Epoch: [85]  [460/781]  eta: 0:00:55  lr: 0.000015  training_loss: 1.1392 (1.1301)  classification_loss: 1.1391 (1.1300)  loss_mask: 0.0000 (0.0001)  time: 0.1711  data: 0.0003  max mem: 6052
[06:18:34.418959] Epoch: [85]  [480/781]  eta: 0:00:52  lr: 0.000015  training_loss: 1.1389 (1.1307)  classification_loss: 1.1389 (1.1306)  loss_mask: 0.0001 (0.0001)  time: 0.1698  data: 0.0002  max mem: 6052
[06:18:37.836552] Epoch: [85]  [500/781]  eta: 0:00:48  lr: 0.000015  training_loss: 1.1508 (1.1317)  classification_loss: 1.1508 (1.1317)  loss_mask: 0.0000 (0.0001)  time: 0.1708  data: 0.0008  max mem: 6052
[06:18:41.252391] Epoch: [85]  [520/781]  eta: 0:00:45  lr: 0.000015  training_loss: 1.1101 (1.1314)  classification_loss: 1.1100 (1.1314)  loss_mask: 0.0001 (0.0001)  time: 0.1707  data: 0.0003  max mem: 6052
[06:18:44.679846] Epoch: [85]  [540/781]  eta: 0:00:41  lr: 0.000015  training_loss: 1.1478 (1.1321)  classification_loss: 1.1478 (1.1320)  loss_mask: 0.0000 (0.0001)  time: 0.1713  data: 0.0005  max mem: 6052
[06:18:48.095888] Epoch: [85]  [560/781]  eta: 0:00:38  lr: 0.000015  training_loss: 1.0981 (1.1303)  classification_loss: 1.0980 (1.1303)  loss_mask: 0.0000 (0.0001)  time: 0.1707  data: 0.0004  max mem: 6052
[06:18:51.511898] Epoch: [85]  [580/781]  eta: 0:00:34  lr: 0.000015  training_loss: 1.1299 (1.1307)  classification_loss: 1.1298 (1.1306)  loss_mask: 0.0000 (0.0001)  time: 0.1707  data: 0.0002  max mem: 6052
[06:18:54.929265] Epoch: [85]  [600/781]  eta: 0:00:31  lr: 0.000015  training_loss: 1.0904 (1.1301)  classification_loss: 1.0903 (1.1301)  loss_mask: 0.0000 (0.0001)  time: 0.1708  data: 0.0002  max mem: 6052
[06:18:58.354154] Epoch: [85]  [620/781]  eta: 0:00:27  lr: 0.000014  training_loss: 1.1054 (1.1301)  classification_loss: 1.1053 (1.1300)  loss_mask: 0.0001 (0.0001)  time: 0.1712  data: 0.0004  max mem: 6052
[06:19:01.761423] Epoch: [85]  [640/781]  eta: 0:00:24  lr: 0.000014  training_loss: 1.1227 (1.1300)  classification_loss: 1.1227 (1.1300)  loss_mask: 0.0000 (0.0001)  time: 0.1702  data: 0.0002  max mem: 6052
[06:19:05.186383] Epoch: [85]  [660/781]  eta: 0:00:20  lr: 0.000014  training_loss: 1.1437 (1.1303)  classification_loss: 1.1436 (1.1302)  loss_mask: 0.0000 (0.0001)  time: 0.1711  data: 0.0002  max mem: 6052
[06:19:08.600654] Epoch: [85]  [680/781]  eta: 0:00:17  lr: 0.000014  training_loss: 1.1420 (1.1309)  classification_loss: 1.1420 (1.1308)  loss_mask: 0.0000 (0.0001)  time: 0.1706  data: 0.0002  max mem: 6052
[06:19:12.015750] Epoch: [85]  [700/781]  eta: 0:00:13  lr: 0.000014  training_loss: 1.1326 (1.1313)  classification_loss: 1.1325 (1.1313)  loss_mask: 0.0001 (0.0001)  time: 0.1707  data: 0.0002  max mem: 6052
[06:19:15.429879] Epoch: [85]  [720/781]  eta: 0:00:10  lr: 0.000014  training_loss: 1.1162 (1.1314)  classification_loss: 1.1162 (1.1314)  loss_mask: 0.0000 (0.0001)  time: 0.1706  data: 0.0003  max mem: 6052
[06:19:18.879657] Epoch: [85]  [740/781]  eta: 0:00:07  lr: 0.000014  training_loss: 1.0823 (1.1308)  classification_loss: 1.0822 (1.1308)  loss_mask: 0.0001 (0.0001)  time: 0.1724  data: 0.0004  max mem: 6052
[06:19:22.317685] Epoch: [85]  [760/781]  eta: 0:00:03  lr: 0.000014  training_loss: 1.1235 (1.1312)  classification_loss: 1.1235 (1.1311)  loss_mask: 0.0000 (0.0001)  time: 0.1718  data: 0.0002  max mem: 6052
[06:19:25.717252] Epoch: [85]  [780/781]  eta: 0:00:00  lr: 0.000014  training_loss: 1.1452 (1.1316)  classification_loss: 1.1452 (1.1316)  loss_mask: 0.0000 (0.0001)  time: 0.1699  data: 0.0002  max mem: 6052
[06:19:25.882961] Epoch: [85] Total time: 0:02:14 (0.1724 s / it)
[06:19:25.883710] Averaged stats: lr: 0.000014  training_loss: 1.1452 (1.1316)  classification_loss: 1.1452 (1.1316)  loss_mask: 0.0000 (0.0001)
[06:19:26.556137] Test:  [  0/157]  eta: 0:01:44  testing_loss: 0.4356 (0.4356)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.6681  data: 0.6293  max mem: 6052
[06:19:26.838150] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4451 (0.4598)  acc1: 85.9375 (86.0795)  acc5: 100.0000 (99.5739)  time: 0.0861  data: 0.0574  max mem: 6052
[06:19:27.118684] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4300 (0.4352)  acc1: 85.9375 (87.2768)  acc5: 100.0000 (99.5536)  time: 0.0278  data: 0.0002  max mem: 6052
[06:19:27.407944] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4269 (0.4441)  acc1: 87.5000 (87.1472)  acc5: 100.0000 (99.4456)  time: 0.0282  data: 0.0002  max mem: 6052
[06:19:27.697051] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4460 (0.4497)  acc1: 87.5000 (86.9284)  acc5: 98.4375 (99.2759)  time: 0.0287  data: 0.0002  max mem: 6052
[06:19:27.979000] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4491 (0.4469)  acc1: 87.5000 (87.0404)  acc5: 100.0000 (99.2953)  time: 0.0283  data: 0.0002  max mem: 6052
[06:19:28.259577] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4238 (0.4452)  acc1: 87.5000 (87.0902)  acc5: 100.0000 (99.3340)  time: 0.0279  data: 0.0002  max mem: 6052
[06:19:28.546289] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4251 (0.4422)  acc1: 87.5000 (87.0819)  acc5: 100.0000 (99.3398)  time: 0.0282  data: 0.0002  max mem: 6052
[06:19:28.828435] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4537 (0.4493)  acc1: 85.9375 (86.9213)  acc5: 100.0000 (99.3248)  time: 0.0282  data: 0.0002  max mem: 6052
[06:19:29.108871] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4373 (0.4468)  acc1: 87.5000 (87.0021)  acc5: 100.0000 (99.3304)  time: 0.0279  data: 0.0001  max mem: 6052
[06:19:29.389156] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4354 (0.4486)  acc1: 85.9375 (86.8657)  acc5: 100.0000 (99.3657)  time: 0.0278  data: 0.0001  max mem: 6052
[06:19:29.670747] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4311 (0.4475)  acc1: 85.9375 (86.9229)  acc5: 100.0000 (99.3666)  time: 0.0278  data: 0.0001  max mem: 6052
[06:19:29.951717] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4257 (0.4472)  acc1: 87.5000 (86.8931)  acc5: 100.0000 (99.3673)  time: 0.0279  data: 0.0002  max mem: 6052
[06:19:30.233069] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4007 (0.4481)  acc1: 87.5000 (86.9036)  acc5: 100.0000 (99.3798)  time: 0.0279  data: 0.0001  max mem: 6052
[06:19:30.514653] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3909 (0.4450)  acc1: 89.0625 (87.0789)  acc5: 100.0000 (99.4016)  time: 0.0279  data: 0.0001  max mem: 6052
[06:19:30.794065] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4442 (0.4455)  acc1: 87.5000 (87.0033)  acc5: 100.0000 (99.4102)  time: 0.0278  data: 0.0001  max mem: 6052
[06:19:30.944164] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4614 (0.4475)  acc1: 85.9375 (86.9400)  acc5: 100.0000 (99.4100)  time: 0.0268  data: 0.0001  max mem: 6052
[06:19:31.123322] Test: Total time: 0:00:05 (0.0334 s / it)
[06:19:31.124526] * Acc@1 86.940 Acc@5 99.410 loss 0.447
[06:19:31.125250] Accuracy of the network on the 10000 test images: 86.9%
[06:19:31.125823] Max accuracy: 86.94%
[06:19:31.355648] log_dir: ./output_dir
[06:19:32.241100] Epoch: [86]  [  0/781]  eta: 0:11:29  lr: 0.000014  training_loss: 1.0929 (1.0929)  classification_loss: 1.0928 (1.0928)  loss_mask: 0.0000 (0.0000)  time: 0.8831  data: 0.6702  max mem: 6052
[06:19:35.667093] Epoch: [86]  [ 20/781]  eta: 0:02:36  lr: 0.000014  training_loss: 1.0778 (1.1148)  classification_loss: 1.0777 (1.1148)  loss_mask: 0.0000 (0.0000)  time: 0.1712  data: 0.0002  max mem: 6052
[06:19:39.086823] Epoch: [86]  [ 40/781]  eta: 0:02:19  lr: 0.000014  training_loss: 1.1159 (1.1266)  classification_loss: 1.1158 (1.1265)  loss_mask: 0.0000 (0.0000)  time: 0.1709  data: 0.0002  max mem: 6052
[06:19:42.516085] Epoch: [86]  [ 60/781]  eta: 0:02:11  lr: 0.000014  training_loss: 1.1496 (1.1417)  classification_loss: 1.1496 (1.1416)  loss_mask: 0.0000 (0.0000)  time: 0.1714  data: 0.0002  max mem: 6052
[06:19:45.942857] Epoch: [86]  [ 80/781]  eta: 0:02:06  lr: 0.000014  training_loss: 1.0985 (1.1411)  classification_loss: 1.0985 (1.1410)  loss_mask: 0.0000 (0.0000)  time: 0.1712  data: 0.0002  max mem: 6052
[06:19:49.365953] Epoch: [86]  [100/781]  eta: 0:02:01  lr: 0.000014  training_loss: 1.1499 (1.1429)  classification_loss: 1.1499 (1.1428)  loss_mask: 0.0000 (0.0000)  time: 0.1711  data: 0.0002  max mem: 6052
[06:19:52.785143] Epoch: [86]  [120/781]  eta: 0:01:56  lr: 0.000014  training_loss: 1.0853 (1.1399)  classification_loss: 1.0852 (1.1399)  loss_mask: 0.0000 (0.0000)  time: 0.1709  data: 0.0002  max mem: 6052
[06:19:56.223071] Epoch: [86]  [140/781]  eta: 0:01:52  lr: 0.000014  training_loss: 1.1077 (1.1357)  classification_loss: 1.1076 (1.1356)  loss_mask: 0.0001 (0.0000)  time: 0.1718  data: 0.0002  max mem: 6052
[06:19:59.635383] Epoch: [86]  [160/781]  eta: 0:01:49  lr: 0.000014  training_loss: 1.0884 (1.1287)  classification_loss: 1.0883 (1.1286)  loss_mask: 0.0000 (0.0000)  time: 0.1705  data: 0.0001  max mem: 6052
[06:20:03.033680] Epoch: [86]  [180/781]  eta: 0:01:45  lr: 0.000014  training_loss: 1.1366 (1.1275)  classification_loss: 1.1366 (1.1275)  loss_mask: 0.0000 (0.0001)  time: 0.1698  data: 0.0001  max mem: 6052
[06:20:06.435656] Epoch: [86]  [200/781]  eta: 0:01:41  lr: 0.000014  training_loss: 1.1280 (1.1273)  classification_loss: 1.1280 (1.1273)  loss_mask: 0.0001 (0.0001)  time: 0.1700  data: 0.0001  max mem: 6052
[06:20:09.852026] Epoch: [86]  [220/781]  eta: 0:01:37  lr: 0.000014  training_loss: 1.1415 (1.1283)  classification_loss: 1.1414 (1.1282)  loss_mask: 0.0000 (0.0001)  time: 0.1708  data: 0.0002  max mem: 6052
[06:20:13.269079] Epoch: [86]  [240/781]  eta: 0:01:34  lr: 0.000014  training_loss: 1.1419 (1.1316)  classification_loss: 1.1418 (1.1316)  loss_mask: 0.0000 (0.0001)  time: 0.1708  data: 0.0002  max mem: 6052
[06:20:16.685243] Epoch: [86]  [260/781]  eta: 0:01:30  lr: 0.000014  training_loss: 1.1625 (1.1331)  classification_loss: 1.1625 (1.1330)  loss_mask: 0.0000 (0.0001)  time: 0.1707  data: 0.0001  max mem: 6052
[06:20:20.101517] Epoch: [86]  [280/781]  eta: 0:01:26  lr: 0.000013  training_loss: 1.1206 (1.1298)  classification_loss: 1.1205 (1.1298)  loss_mask: 0.0000 (0.0001)  time: 0.1707  data: 0.0002  max mem: 6052
[06:20:23.528979] Epoch: [86]  [300/781]  eta: 0:01:23  lr: 0.000013  training_loss: 1.1016 (1.1278)  classification_loss: 1.1016 (1.1278)  loss_mask: 0.0001 (0.0001)  time: 0.1713  data: 0.0002  max mem: 6052
[06:20:26.945426] Epoch: [86]  [320/781]  eta: 0:01:19  lr: 0.000013  training_loss: 1.0968 (1.1277)  classification_loss: 1.0967 (1.1276)  loss_mask: 0.0000 (0.0001)  time: 0.1707  data: 0.0002  max mem: 6052
[06:20:30.355953] Epoch: [86]  [340/781]  eta: 0:01:16  lr: 0.000013  training_loss: 1.1021 (1.1269)  classification_loss: 1.1020 (1.1268)  loss_mask: 0.0000 (0.0001)  time: 0.1705  data: 0.0002  max mem: 6052
[06:20:33.778170] Epoch: [86]  [360/781]  eta: 0:01:12  lr: 0.000013  training_loss: 1.1070 (1.1262)  classification_loss: 1.1070 (1.1261)  loss_mask: 0.0000 (0.0000)  time: 0.1710  data: 0.0002  max mem: 6052
[06:20:37.200930] Epoch: [86]  [380/781]  eta: 0:01:09  lr: 0.000013  training_loss: 1.1176 (1.1263)  classification_loss: 1.1175 (1.1263)  loss_mask: 0.0000 (0.0001)  time: 0.1711  data: 0.0002  max mem: 6052
[06:20:40.623436] Epoch: [86]  [400/781]  eta: 0:01:05  lr: 0.000013  training_loss: 1.1625 (1.1274)  classification_loss: 1.1624 (1.1274)  loss_mask: 0.0000 (0.0001)  time: 0.1711  data: 0.0002  max mem: 6052
[06:20:44.066541] Epoch: [86]  [420/781]  eta: 0:01:02  lr: 0.000013  training_loss: 1.1449 (1.1283)  classification_loss: 1.1449 (1.1283)  loss_mask: 0.0000 (0.0001)  time: 0.1720  data: 0.0003  max mem: 6052
[06:20:47.482926] Epoch: [86]  [440/781]  eta: 0:00:58  lr: 0.000013  training_loss: 1.0977 (1.1287)  classification_loss: 1.0977 (1.1287)  loss_mask: 0.0000 (0.0001)  time: 0.1708  data: 0.0003  max mem: 6052
[06:20:50.908252] Epoch: [86]  [460/781]  eta: 0:00:55  lr: 0.000013  training_loss: 1.1058 (1.1282)  classification_loss: 1.1058 (1.1281)  loss_mask: 0.0001 (0.0001)  time: 0.1712  data: 0.0002  max mem: 6052
[06:20:54.323311] Epoch: [86]  [480/781]  eta: 0:00:51  lr: 0.000013  training_loss: 1.1087 (1.1274)  classification_loss: 1.1087 (1.1274)  loss_mask: 0.0000 (0.0001)  time: 0.1707  data: 0.0002  max mem: 6052
[06:20:57.749265] Epoch: [86]  [500/781]  eta: 0:00:48  lr: 0.000013  training_loss: 1.0981 (1.1275)  classification_loss: 1.0980 (1.1274)  loss_mask: 0.0000 (0.0001)  time: 0.1712  data: 0.0002  max mem: 6052
[06:21:01.188406] Epoch: [86]  [520/781]  eta: 0:00:44  lr: 0.000013  training_loss: 1.1222 (1.1277)  classification_loss: 1.1221 (1.1277)  loss_mask: 0.0000 (0.0001)  time: 0.1719  data: 0.0002  max mem: 6052
[06:21:04.613767] Epoch: [86]  [540/781]  eta: 0:00:41  lr: 0.000013  training_loss: 1.1385 (1.1287)  classification_loss: 1.1384 (1.1286)  loss_mask: 0.0000 (0.0001)  time: 0.1712  data: 0.0002  max mem: 6052
[06:21:08.037167] Epoch: [86]  [560/781]  eta: 0:00:38  lr: 0.000013  training_loss: 1.1332 (1.1285)  classification_loss: 1.1332 (1.1285)  loss_mask: 0.0000 (0.0001)  time: 0.1711  data: 0.0002  max mem: 6052
[06:21:11.474839] Epoch: [86]  [580/781]  eta: 0:00:34  lr: 0.000013  training_loss: 1.0813 (1.1277)  classification_loss: 1.0812 (1.1276)  loss_mask: 0.0000 (0.0001)  time: 0.1718  data: 0.0002  max mem: 6052
[06:21:14.935465] Epoch: [86]  [600/781]  eta: 0:00:31  lr: 0.000013  training_loss: 1.1554 (1.1284)  classification_loss: 1.1553 (1.1283)  loss_mask: 0.0000 (0.0001)  time: 0.1729  data: 0.0003  max mem: 6052
[06:21:18.353186] Epoch: [86]  [620/781]  eta: 0:00:27  lr: 0.000013  training_loss: 1.1363 (1.1285)  classification_loss: 1.1362 (1.1284)  loss_mask: 0.0000 (0.0001)  time: 0.1708  data: 0.0002  max mem: 6052
[06:21:21.775377] Epoch: [86]  [640/781]  eta: 0:00:24  lr: 0.000013  training_loss: 1.1266 (1.1279)  classification_loss: 1.1266 (1.1278)  loss_mask: 0.0000 (0.0000)  time: 0.1710  data: 0.0002  max mem: 6052
[06:21:25.184099] Epoch: [86]  [660/781]  eta: 0:00:20  lr: 0.000013  training_loss: 1.1075 (1.1278)  classification_loss: 1.1075 (1.1277)  loss_mask: 0.0000 (0.0000)  time: 0.1704  data: 0.0003  max mem: 6052
[06:21:28.590802] Epoch: [86]  [680/781]  eta: 0:00:17  lr: 0.000013  training_loss: 1.0982 (1.1268)  classification_loss: 1.0981 (1.1268)  loss_mask: 0.0000 (0.0000)  time: 0.1702  data: 0.0003  max mem: 6052
[06:21:32.039038] Epoch: [86]  [700/781]  eta: 0:00:13  lr: 0.000013  training_loss: 1.1626 (1.1275)  classification_loss: 1.1626 (1.1275)  loss_mask: 0.0000 (0.0000)  time: 0.1723  data: 0.0003  max mem: 6052
[06:21:35.465686] Epoch: [86]  [720/781]  eta: 0:00:10  lr: 0.000012  training_loss: 1.1258 (1.1282)  classification_loss: 1.1257 (1.1282)  loss_mask: 0.0000 (0.0000)  time: 0.1713  data: 0.0003  max mem: 6052
[06:21:38.905227] Epoch: [86]  [740/781]  eta: 0:00:07  lr: 0.000012  training_loss: 1.1317 (1.1287)  classification_loss: 1.1316 (1.1286)  loss_mask: 0.0000 (0.0000)  time: 0.1719  data: 0.0002  max mem: 6052
[06:21:42.347844] Epoch: [86]  [760/781]  eta: 0:00:03  lr: 0.000012  training_loss: 1.1219 (1.1290)  classification_loss: 1.1219 (1.1289)  loss_mask: 0.0000 (0.0000)  time: 0.1721  data: 0.0002  max mem: 6052
[06:21:45.747541] Epoch: [86]  [780/781]  eta: 0:00:00  lr: 0.000012  training_loss: 1.1001 (1.1288)  classification_loss: 1.1001 (1.1287)  loss_mask: 0.0000 (0.0000)  time: 0.1699  data: 0.0002  max mem: 6052
[06:21:45.915776] Epoch: [86] Total time: 0:02:14 (0.1723 s / it)
[06:21:45.916227] Averaged stats: lr: 0.000012  training_loss: 1.1001 (1.1288)  classification_loss: 1.1001 (1.1287)  loss_mask: 0.0000 (0.0000)
[06:21:46.571567] Test:  [  0/157]  eta: 0:01:42  testing_loss: 0.4292 (0.4292)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (100.0000)  time: 0.6510  data: 0.6218  max mem: 6052
[06:21:46.865339] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4299 (0.4583)  acc1: 87.5000 (86.5057)  acc5: 100.0000 (99.8580)  time: 0.0857  data: 0.0567  max mem: 6052
[06:21:47.148947] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4299 (0.4276)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (99.5536)  time: 0.0287  data: 0.0002  max mem: 6052
[06:21:47.434232] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4373 (0.4421)  acc1: 87.5000 (87.3488)  acc5: 100.0000 (99.3952)  time: 0.0283  data: 0.0001  max mem: 6052
[06:21:47.718311] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4541 (0.4483)  acc1: 87.5000 (87.1951)  acc5: 98.4375 (99.2378)  time: 0.0283  data: 0.0002  max mem: 6052
[06:21:48.000967] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4351 (0.4438)  acc1: 87.5000 (87.4694)  acc5: 98.4375 (99.2647)  time: 0.0282  data: 0.0002  max mem: 6052
[06:21:48.285389] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4176 (0.4423)  acc1: 87.5000 (87.3719)  acc5: 100.0000 (99.3084)  time: 0.0281  data: 0.0002  max mem: 6052
[06:21:48.569219] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4305 (0.4396)  acc1: 87.5000 (87.3680)  acc5: 100.0000 (99.3618)  time: 0.0282  data: 0.0002  max mem: 6052
[06:21:48.856646] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4499 (0.4472)  acc1: 85.9375 (87.0756)  acc5: 100.0000 (99.3441)  time: 0.0284  data: 0.0002  max mem: 6052
[06:21:49.140809] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4499 (0.4444)  acc1: 87.5000 (87.0879)  acc5: 100.0000 (99.3304)  time: 0.0285  data: 0.0001  max mem: 6052
[06:21:49.424639] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4276 (0.4472)  acc1: 87.5000 (86.9585)  acc5: 100.0000 (99.3348)  time: 0.0283  data: 0.0001  max mem: 6052
[06:21:49.711692] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4260 (0.4462)  acc1: 87.5000 (87.0355)  acc5: 100.0000 (99.3243)  time: 0.0284  data: 0.0001  max mem: 6052
[06:21:49.995985] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4078 (0.4457)  acc1: 87.5000 (86.9576)  acc5: 100.0000 (99.3285)  time: 0.0285  data: 0.0002  max mem: 6052
[06:21:50.277039] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.3907 (0.4465)  acc1: 85.9375 (86.9633)  acc5: 100.0000 (99.3440)  time: 0.0281  data: 0.0002  max mem: 6052
[06:21:50.559463] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3968 (0.4437)  acc1: 87.5000 (87.1454)  acc5: 100.0000 (99.3794)  time: 0.0280  data: 0.0001  max mem: 6052
[06:21:50.837263] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4243 (0.4440)  acc1: 87.5000 (87.0757)  acc5: 100.0000 (99.3895)  time: 0.0279  data: 0.0001  max mem: 6052
[06:21:50.987021] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4707 (0.4464)  acc1: 87.5000 (87.0000)  acc5: 100.0000 (99.3800)  time: 0.0268  data: 0.0001  max mem: 6052
[06:21:51.137629] Test: Total time: 0:00:05 (0.0332 s / it)
[06:21:51.138894] * Acc@1 87.000 Acc@5 99.380 loss 0.446
[06:21:51.139709] Accuracy of the network on the 10000 test images: 87.0%
[06:21:51.140261] Max accuracy: 87.00%
[06:21:51.355691] log_dir: ./output_dir
[06:21:52.230907] Epoch: [87]  [  0/781]  eta: 0:11:21  lr: 0.000012  training_loss: 1.0784 (1.0784)  classification_loss: 1.0783 (1.0783)  loss_mask: 0.0001 (0.0001)  time: 0.8728  data: 0.6753  max mem: 6052
[06:21:55.662856] Epoch: [87]  [ 20/781]  eta: 0:02:35  lr: 0.000012  training_loss: 1.0723 (1.0821)  classification_loss: 1.0723 (1.0821)  loss_mask: 0.0000 (0.0000)  time: 0.1715  data: 0.0001  max mem: 6052
[06:21:59.090042] Epoch: [87]  [ 40/781]  eta: 0:02:19  lr: 0.000012  training_loss: 1.1135 (1.1100)  classification_loss: 1.1134 (1.1100)  loss_mask: 0.0000 (0.0000)  time: 0.1713  data: 0.0002  max mem: 6052
[06:22:02.526927] Epoch: [87]  [ 60/781]  eta: 0:02:11  lr: 0.000012  training_loss: 1.1060 (1.1092)  classification_loss: 1.1060 (1.1091)  loss_mask: 0.0000 (0.0000)  time: 0.1718  data: 0.0002  max mem: 6052
[06:22:05.954845] Epoch: [87]  [ 80/781]  eta: 0:02:06  lr: 0.000012  training_loss: 1.0973 (1.1079)  classification_loss: 1.0973 (1.1079)  loss_mask: 0.0000 (0.0000)  time: 0.1713  data: 0.0002  max mem: 6052
[06:22:09.381270] Epoch: [87]  [100/781]  eta: 0:02:01  lr: 0.000012  training_loss: 1.1310 (1.1163)  classification_loss: 1.1309 (1.1163)  loss_mask: 0.0000 (0.0000)  time: 0.1713  data: 0.0003  max mem: 6052
[06:22:12.821961] Epoch: [87]  [120/781]  eta: 0:01:57  lr: 0.000012  training_loss: 1.1281 (1.1202)  classification_loss: 1.1281 (1.1201)  loss_mask: 0.0000 (0.0000)  time: 0.1720  data: 0.0002  max mem: 6052
[06:22:16.262130] Epoch: [87]  [140/781]  eta: 0:01:53  lr: 0.000012  training_loss: 1.1612 (1.1205)  classification_loss: 1.1612 (1.1205)  loss_mask: 0.0000 (0.0000)  time: 0.1719  data: 0.0002  max mem: 6052
[06:22:19.712049] Epoch: [87]  [160/781]  eta: 0:01:49  lr: 0.000012  training_loss: 1.0443 (1.1167)  classification_loss: 1.0443 (1.1159)  loss_mask: 0.0001 (0.0008)  time: 0.1724  data: 0.0002  max mem: 6052
[06:22:23.132253] Epoch: [87]  [180/781]  eta: 0:01:45  lr: 0.000012  training_loss: 1.1141 (1.1182)  classification_loss: 1.1141 (1.1174)  loss_mask: 0.0001 (0.0008)  time: 0.1709  data: 0.0002  max mem: 6052
[06:22:26.552686] Epoch: [87]  [200/781]  eta: 0:01:41  lr: 0.000012  training_loss: 1.1116 (1.1193)  classification_loss: 1.1111 (1.1186)  loss_mask: 0.0002 (0.0007)  time: 0.1709  data: 0.0002  max mem: 6052
[06:22:29.970098] Epoch: [87]  [220/781]  eta: 0:01:37  lr: 0.000012  training_loss: 1.1441 (1.1220)  classification_loss: 1.1440 (1.1213)  loss_mask: 0.0000 (0.0007)  time: 0.1708  data: 0.0002  max mem: 6052
[06:22:33.409173] Epoch: [87]  [240/781]  eta: 0:01:34  lr: 0.000012  training_loss: 1.1122 (1.1235)  classification_loss: 1.1121 (1.1229)  loss_mask: 0.0000 (0.0006)  time: 0.1719  data: 0.0002  max mem: 6052
[06:22:36.835995] Epoch: [87]  [260/781]  eta: 0:01:30  lr: 0.000012  training_loss: 1.0751 (1.1235)  classification_loss: 1.0751 (1.1229)  loss_mask: 0.0000 (0.0006)  time: 0.1713  data: 0.0002  max mem: 6052
[06:22:40.254101] Epoch: [87]  [280/781]  eta: 0:01:27  lr: 0.000012  training_loss: 1.1326 (1.1242)  classification_loss: 1.1326 (1.1236)  loss_mask: 0.0000 (0.0005)  time: 0.1708  data: 0.0002  max mem: 6052
[06:22:43.703901] Epoch: [87]  [300/781]  eta: 0:01:23  lr: 0.000012  training_loss: 1.1106 (1.1231)  classification_loss: 1.1106 (1.1226)  loss_mask: 0.0000 (0.0005)  time: 0.1724  data: 0.0002  max mem: 6052
[06:22:47.140867] Epoch: [87]  [320/781]  eta: 0:01:20  lr: 0.000012  training_loss: 1.1365 (1.1241)  classification_loss: 1.1364 (1.1236)  loss_mask: 0.0000 (0.0005)  time: 0.1717  data: 0.0003  max mem: 6052
[06:22:50.553362] Epoch: [87]  [340/781]  eta: 0:01:16  lr: 0.000012  training_loss: 1.1570 (1.1248)  classification_loss: 1.1570 (1.1244)  loss_mask: 0.0000 (0.0005)  time: 0.1705  data: 0.0003  max mem: 6052
[06:22:53.973725] Epoch: [87]  [360/781]  eta: 0:01:12  lr: 0.000012  training_loss: 1.0963 (1.1256)  classification_loss: 1.0962 (1.1252)  loss_mask: 0.0000 (0.0004)  time: 0.1709  data: 0.0003  max mem: 6052
[06:22:57.402786] Epoch: [87]  [380/781]  eta: 0:01:09  lr: 0.000012  training_loss: 1.0814 (1.1255)  classification_loss: 1.0814 (1.1251)  loss_mask: 0.0000 (0.0004)  time: 0.1714  data: 0.0002  max mem: 6052
[06:23:00.811779] Epoch: [87]  [400/781]  eta: 0:01:05  lr: 0.000011  training_loss: 1.0947 (1.1257)  classification_loss: 1.0946 (1.1253)  loss_mask: 0.0000 (0.0004)  time: 0.1704  data: 0.0002  max mem: 6052
[06:23:04.277029] Epoch: [87]  [420/781]  eta: 0:01:02  lr: 0.000011  training_loss: 1.1356 (1.1254)  classification_loss: 1.1356 (1.1250)  loss_mask: 0.0000 (0.0004)  time: 0.1732  data: 0.0002  max mem: 6052
[06:23:07.699013] Epoch: [87]  [440/781]  eta: 0:00:59  lr: 0.000011  training_loss: 1.1468 (1.1259)  classification_loss: 1.1467 (1.1255)  loss_mask: 0.0000 (0.0004)  time: 0.1710  data: 0.0002  max mem: 6052
[06:23:11.113247] Epoch: [87]  [460/781]  eta: 0:00:55  lr: 0.000011  training_loss: 1.0918 (1.1256)  classification_loss: 1.0918 (1.1252)  loss_mask: 0.0000 (0.0003)  time: 0.1706  data: 0.0002  max mem: 6052
[06:23:14.533066] Epoch: [87]  [480/781]  eta: 0:00:52  lr: 0.000011  training_loss: 1.1625 (1.1257)  classification_loss: 1.1625 (1.1253)  loss_mask: 0.0000 (0.0003)  time: 0.1709  data: 0.0002  max mem: 6052
[06:23:17.988611] Epoch: [87]  [500/781]  eta: 0:00:48  lr: 0.000011  training_loss: 1.1355 (1.1265)  classification_loss: 1.1354 (1.1261)  loss_mask: 0.0000 (0.0003)  time: 0.1727  data: 0.0002  max mem: 6052
[06:23:21.527525] Epoch: [87]  [520/781]  eta: 0:00:45  lr: 0.000011  training_loss: 1.1449 (1.1268)  classification_loss: 1.1448 (1.1265)  loss_mask: 0.0000 (0.0003)  time: 0.1769  data: 0.0003  max mem: 6052
[06:23:24.930463] Epoch: [87]  [540/781]  eta: 0:00:41  lr: 0.000011  training_loss: 1.1273 (1.1274)  classification_loss: 1.1273 (1.1271)  loss_mask: 0.0000 (0.0003)  time: 0.1701  data: 0.0001  max mem: 6052
[06:23:28.335144] Epoch: [87]  [560/781]  eta: 0:00:38  lr: 0.000011  training_loss: 1.1083 (1.1271)  classification_loss: 1.1083 (1.1268)  loss_mask: 0.0000 (0.0003)  time: 0.1702  data: 0.0003  max mem: 6052
[06:23:31.732150] Epoch: [87]  [580/781]  eta: 0:00:34  lr: 0.000011  training_loss: 1.1301 (1.1276)  classification_loss: 1.1301 (1.1273)  loss_mask: 0.0000 (0.0003)  time: 0.1698  data: 0.0002  max mem: 6052
[06:23:35.126719] Epoch: [87]  [600/781]  eta: 0:00:31  lr: 0.000011  training_loss: 1.1014 (1.1277)  classification_loss: 1.1013 (1.1274)  loss_mask: 0.0000 (0.0003)  time: 0.1696  data: 0.0002  max mem: 6052
[06:23:38.530441] Epoch: [87]  [620/781]  eta: 0:00:27  lr: 0.000011  training_loss: 1.1284 (1.1279)  classification_loss: 1.1283 (1.1276)  loss_mask: 0.0000 (0.0003)  time: 0.1701  data: 0.0002  max mem: 6052
[06:23:41.937734] Epoch: [87]  [640/781]  eta: 0:00:24  lr: 0.000011  training_loss: 1.1020 (1.1276)  classification_loss: 1.1019 (1.1274)  loss_mask: 0.0000 (0.0003)  time: 0.1703  data: 0.0003  max mem: 6052
[06:23:45.350392] Epoch: [87]  [660/781]  eta: 0:00:20  lr: 0.000011  training_loss: 1.1190 (1.1270)  classification_loss: 1.1190 (1.1268)  loss_mask: 0.0000 (0.0003)  time: 0.1706  data: 0.0004  max mem: 6052
[06:23:48.767123] Epoch: [87]  [680/781]  eta: 0:00:17  lr: 0.000011  training_loss: 1.1334 (1.1279)  classification_loss: 1.1334 (1.1276)  loss_mask: 0.0000 (0.0002)  time: 0.1708  data: 0.0002  max mem: 6052
[06:23:52.169389] Epoch: [87]  [700/781]  eta: 0:00:13  lr: 0.000011  training_loss: 1.0942 (1.1279)  classification_loss: 1.0942 (1.1276)  loss_mask: 0.0000 (0.0002)  time: 0.1700  data: 0.0002  max mem: 6052
[06:23:55.668537] Epoch: [87]  [720/781]  eta: 0:00:10  lr: 0.000011  training_loss: 1.1183 (1.1284)  classification_loss: 1.1182 (1.1282)  loss_mask: 0.0000 (0.0002)  time: 0.1749  data: 0.0003  max mem: 6052
[06:23:59.205278] Epoch: [87]  [740/781]  eta: 0:00:07  lr: 0.000011  training_loss: 1.1163 (1.1287)  classification_loss: 1.1162 (1.1285)  loss_mask: 0.0000 (0.0002)  time: 0.1768  data: 0.0002  max mem: 6052
[06:24:02.669174] Epoch: [87]  [760/781]  eta: 0:00:03  lr: 0.000011  training_loss: 1.1497 (1.1288)  classification_loss: 1.1496 (1.1286)  loss_mask: 0.0000 (0.0002)  time: 0.1731  data: 0.0002  max mem: 6052
[06:24:06.064472] Epoch: [87]  [780/781]  eta: 0:00:00  lr: 0.000011  training_loss: 1.1487 (1.1294)  classification_loss: 1.1487 (1.1292)  loss_mask: 0.0000 (0.0002)  time: 0.1697  data: 0.0002  max mem: 6052
[06:24:06.224986] Epoch: [87] Total time: 0:02:14 (0.1727 s / it)
[06:24:06.226458] Averaged stats: lr: 0.000011  training_loss: 1.1487 (1.1294)  classification_loss: 1.1487 (1.1292)  loss_mask: 0.0000 (0.0002)
[06:24:06.892142] Test:  [  0/157]  eta: 0:01:43  testing_loss: 0.4496 (0.4496)  acc1: 89.0625 (89.0625)  acc5: 98.4375 (98.4375)  time: 0.6610  data: 0.6157  max mem: 6052
[06:24:07.184806] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4370 (0.4564)  acc1: 87.5000 (86.9318)  acc5: 100.0000 (99.7159)  time: 0.0865  data: 0.0561  max mem: 6052
[06:24:07.472719] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4287 (0.4324)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (99.5536)  time: 0.0288  data: 0.0001  max mem: 6052
[06:24:07.762389] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4287 (0.4418)  acc1: 87.5000 (87.4496)  acc5: 100.0000 (99.4456)  time: 0.0287  data: 0.0002  max mem: 6052
[06:24:08.046127] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4532 (0.4464)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (99.2759)  time: 0.0285  data: 0.0002  max mem: 6052
[06:24:08.332941] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4409 (0.4426)  acc1: 87.5000 (87.6225)  acc5: 100.0000 (99.3566)  time: 0.0284  data: 0.0002  max mem: 6052
[06:24:08.616525] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4211 (0.4434)  acc1: 87.5000 (87.3975)  acc5: 100.0000 (99.3852)  time: 0.0284  data: 0.0002  max mem: 6052
[06:24:08.898291] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4405 (0.4408)  acc1: 87.5000 (87.4120)  acc5: 100.0000 (99.4058)  time: 0.0281  data: 0.0001  max mem: 6052
[06:24:09.185237] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4538 (0.4486)  acc1: 87.5000 (87.0949)  acc5: 100.0000 (99.4020)  time: 0.0283  data: 0.0001  max mem: 6052
[06:24:09.476264] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4538 (0.4461)  acc1: 87.5000 (87.1394)  acc5: 100.0000 (99.3819)  time: 0.0288  data: 0.0002  max mem: 6052
[06:24:09.763295] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4353 (0.4486)  acc1: 87.5000 (87.0050)  acc5: 100.0000 (99.3502)  time: 0.0288  data: 0.0002  max mem: 6052
[06:24:10.045003] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4148 (0.4470)  acc1: 87.5000 (87.1481)  acc5: 100.0000 (99.3384)  time: 0.0283  data: 0.0001  max mem: 6052
[06:24:10.332574] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4116 (0.4463)  acc1: 87.5000 (87.1772)  acc5: 100.0000 (99.3285)  time: 0.0283  data: 0.0001  max mem: 6052
[06:24:10.618044] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4109 (0.4473)  acc1: 87.5000 (87.2018)  acc5: 100.0000 (99.3559)  time: 0.0285  data: 0.0001  max mem: 6052
[06:24:10.897729] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4109 (0.4442)  acc1: 87.5000 (87.3449)  acc5: 100.0000 (99.3794)  time: 0.0281  data: 0.0001  max mem: 6052
[06:24:11.175418] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4363 (0.4445)  acc1: 87.5000 (87.2517)  acc5: 100.0000 (99.3998)  time: 0.0277  data: 0.0001  max mem: 6052
[06:24:11.326097] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4507 (0.4463)  acc1: 84.3750 (87.1400)  acc5: 100.0000 (99.4100)  time: 0.0268  data: 0.0001  max mem: 6052
[06:24:11.489242] Test: Total time: 0:00:05 (0.0335 s / it)
[06:24:11.489718] * Acc@1 87.140 Acc@5 99.410 loss 0.446
[06:24:11.490061] Accuracy of the network on the 10000 test images: 87.1%
[06:24:11.490267] Max accuracy: 87.14%
[06:24:11.790500] log_dir: ./output_dir
[06:24:12.649101] Epoch: [88]  [  0/781]  eta: 0:11:09  lr: 0.000011  training_loss: 0.9654 (0.9654)  classification_loss: 0.9654 (0.9654)  loss_mask: 0.0000 (0.0000)  time: 0.8570  data: 0.6730  max mem: 6052
[06:24:16.074158] Epoch: [88]  [ 20/781]  eta: 0:02:35  lr: 0.000011  training_loss: 1.0751 (1.0985)  classification_loss: 1.0751 (1.0984)  loss_mask: 0.0000 (0.0000)  time: 0.1712  data: 0.0002  max mem: 6052
[06:24:19.510350] Epoch: [88]  [ 40/781]  eta: 0:02:19  lr: 0.000011  training_loss: 1.0859 (1.1044)  classification_loss: 1.0858 (1.1043)  loss_mask: 0.0000 (0.0000)  time: 0.1717  data: 0.0002  max mem: 6052
[06:24:22.928956] Epoch: [88]  [ 60/781]  eta: 0:02:11  lr: 0.000011  training_loss: 1.1102 (1.1165)  classification_loss: 1.1102 (1.1165)  loss_mask: 0.0000 (0.0000)  time: 0.1708  data: 0.0002  max mem: 6052
[06:24:26.382109] Epoch: [88]  [ 80/781]  eta: 0:02:06  lr: 0.000011  training_loss: 1.1396 (1.1179)  classification_loss: 1.1396 (1.1179)  loss_mask: 0.0000 (0.0000)  time: 0.1726  data: 0.0002  max mem: 6052
[06:24:29.808414] Epoch: [88]  [100/781]  eta: 0:02:01  lr: 0.000010  training_loss: 1.1324 (1.1206)  classification_loss: 1.1324 (1.1205)  loss_mask: 0.0000 (0.0000)  time: 0.1712  data: 0.0003  max mem: 6052
[06:24:33.207653] Epoch: [88]  [120/781]  eta: 0:01:56  lr: 0.000010  training_loss: 1.0769 (1.1165)  classification_loss: 1.0768 (1.1165)  loss_mask: 0.0000 (0.0000)  time: 0.1699  data: 0.0002  max mem: 6052
[06:24:36.632908] Epoch: [88]  [140/781]  eta: 0:01:52  lr: 0.000010  training_loss: 1.0899 (1.1121)  classification_loss: 1.0899 (1.1120)  loss_mask: 0.0000 (0.0000)  time: 0.1712  data: 0.0002  max mem: 6052
[06:24:40.044518] Epoch: [88]  [160/781]  eta: 0:01:48  lr: 0.000010  training_loss: 1.1025 (1.1132)  classification_loss: 1.1025 (1.1131)  loss_mask: 0.0000 (0.0000)  time: 0.1705  data: 0.0002  max mem: 6052
[06:24:43.450628] Epoch: [88]  [180/781]  eta: 0:01:45  lr: 0.000010  training_loss: 1.0723 (1.1115)  classification_loss: 1.0723 (1.1115)  loss_mask: 0.0000 (0.0000)  time: 0.1702  data: 0.0002  max mem: 6052
[06:24:46.869896] Epoch: [88]  [200/781]  eta: 0:01:41  lr: 0.000010  training_loss: 1.0944 (1.1113)  classification_loss: 1.0943 (1.1112)  loss_mask: 0.0000 (0.0000)  time: 0.1709  data: 0.0002  max mem: 6052
[06:24:50.312455] Epoch: [88]  [220/781]  eta: 0:01:37  lr: 0.000010  training_loss: 1.1012 (1.1107)  classification_loss: 1.1012 (1.1106)  loss_mask: 0.0000 (0.0000)  time: 0.1721  data: 0.0003  max mem: 6052
[06:24:53.742815] Epoch: [88]  [240/781]  eta: 0:01:34  lr: 0.000010  training_loss: 1.1006 (1.1108)  classification_loss: 1.1006 (1.1108)  loss_mask: 0.0000 (0.0000)  time: 0.1714  data: 0.0002  max mem: 6052
[06:24:57.207757] Epoch: [88]  [260/781]  eta: 0:01:30  lr: 0.000010  training_loss: 1.1517 (1.1148)  classification_loss: 1.1516 (1.1148)  loss_mask: 0.0000 (0.0000)  time: 0.1732  data: 0.0002  max mem: 6052
[06:25:00.667869] Epoch: [88]  [280/781]  eta: 0:01:27  lr: 0.000010  training_loss: 1.1112 (1.1140)  classification_loss: 1.1112 (1.1139)  loss_mask: 0.0000 (0.0000)  time: 0.1729  data: 0.0005  max mem: 6052
[06:25:04.083921] Epoch: [88]  [300/781]  eta: 0:01:23  lr: 0.000010  training_loss: 1.1430 (1.1139)  classification_loss: 1.1430 (1.1139)  loss_mask: 0.0000 (0.0000)  time: 0.1707  data: 0.0002  max mem: 6052
[06:25:07.562579] Epoch: [88]  [320/781]  eta: 0:01:20  lr: 0.000010  training_loss: 1.0670 (1.1114)  classification_loss: 1.0670 (1.1113)  loss_mask: 0.0000 (0.0000)  time: 0.1739  data: 0.0002  max mem: 6052
[06:25:11.077188] Epoch: [88]  [340/781]  eta: 0:01:16  lr: 0.000010  training_loss: 1.0775 (1.1101)  classification_loss: 1.0775 (1.1101)  loss_mask: 0.0000 (0.0000)  time: 0.1756  data: 0.0003  max mem: 6052
[06:25:14.632481] Epoch: [88]  [360/781]  eta: 0:01:13  lr: 0.000010  training_loss: 1.0894 (1.1108)  classification_loss: 1.0894 (1.1107)  loss_mask: 0.0000 (0.0000)  time: 0.1777  data: 0.0002  max mem: 6052
[06:25:18.069022] Epoch: [88]  [380/781]  eta: 0:01:09  lr: 0.000010  training_loss: 1.1972 (1.1151)  classification_loss: 1.1971 (1.1150)  loss_mask: 0.0000 (0.0000)  time: 0.1717  data: 0.0003  max mem: 6052
[06:25:21.529583] Epoch: [88]  [400/781]  eta: 0:01:06  lr: 0.000010  training_loss: 1.0769 (1.1151)  classification_loss: 1.0768 (1.1150)  loss_mask: 0.0000 (0.0000)  time: 0.1730  data: 0.0002  max mem: 6052
[06:25:24.939807] Epoch: [88]  [420/781]  eta: 0:01:02  lr: 0.000010  training_loss: 1.1411 (1.1160)  classification_loss: 1.1411 (1.1159)  loss_mask: 0.0000 (0.0000)  time: 0.1704  data: 0.0003  max mem: 6052
[06:25:28.344391] Epoch: [88]  [440/781]  eta: 0:00:59  lr: 0.000010  training_loss: 1.0871 (1.1152)  classification_loss: 1.0871 (1.1152)  loss_mask: 0.0000 (0.0000)  time: 0.1702  data: 0.0002  max mem: 6052
[06:25:31.770858] Epoch: [88]  [460/781]  eta: 0:00:55  lr: 0.000010  training_loss: 1.0946 (1.1158)  classification_loss: 1.0945 (1.1158)  loss_mask: 0.0000 (0.0000)  time: 0.1713  data: 0.0002  max mem: 6052
[06:25:35.191491] Epoch: [88]  [480/781]  eta: 0:00:52  lr: 0.000010  training_loss: 1.1288 (1.1163)  classification_loss: 1.1288 (1.1163)  loss_mask: 0.0000 (0.0000)  time: 0.1710  data: 0.0002  max mem: 6052
[06:25:38.596724] Epoch: [88]  [500/781]  eta: 0:00:48  lr: 0.000010  training_loss: 1.1380 (1.1177)  classification_loss: 1.1380 (1.1177)  loss_mask: 0.0000 (0.0000)  time: 0.1702  data: 0.0003  max mem: 6052
[06:25:42.006728] Epoch: [88]  [520/781]  eta: 0:00:45  lr: 0.000010  training_loss: 1.1083 (1.1180)  classification_loss: 1.1082 (1.1180)  loss_mask: 0.0000 (0.0000)  time: 0.1704  data: 0.0002  max mem: 6052
[06:25:45.420855] Epoch: [88]  [540/781]  eta: 0:00:41  lr: 0.000010  training_loss: 1.0833 (1.1175)  classification_loss: 1.0833 (1.1175)  loss_mask: 0.0001 (0.0000)  time: 0.1706  data: 0.0003  max mem: 6052
[06:25:48.824759] Epoch: [88]  [560/781]  eta: 0:00:38  lr: 0.000010  training_loss: 1.1114 (1.1177)  classification_loss: 1.1114 (1.1177)  loss_mask: 0.0000 (0.0000)  time: 0.1701  data: 0.0003  max mem: 6052
[06:25:52.240618] Epoch: [88]  [580/781]  eta: 0:00:34  lr: 0.000010  training_loss: 1.0836 (1.1175)  classification_loss: 1.0836 (1.1174)  loss_mask: 0.0001 (0.0000)  time: 0.1707  data: 0.0002  max mem: 6052
[06:25:55.668970] Epoch: [88]  [600/781]  eta: 0:00:31  lr: 0.000009  training_loss: 1.0907 (1.1166)  classification_loss: 1.0905 (1.1166)  loss_mask: 0.0000 (0.0000)  time: 0.1713  data: 0.0002  max mem: 6052
[06:25:59.106317] Epoch: [88]  [620/781]  eta: 0:00:27  lr: 0.000009  training_loss: 1.1256 (1.1170)  classification_loss: 1.1255 (1.1169)  loss_mask: 0.0000 (0.0000)  time: 0.1718  data: 0.0002  max mem: 6052
[06:26:02.512328] Epoch: [88]  [640/781]  eta: 0:00:24  lr: 0.000009  training_loss: 1.1240 (1.1168)  classification_loss: 1.1240 (1.1168)  loss_mask: 0.0000 (0.0000)  time: 0.1702  data: 0.0003  max mem: 6052
[06:26:05.925994] Epoch: [88]  [660/781]  eta: 0:00:20  lr: 0.000009  training_loss: 1.1538 (1.1176)  classification_loss: 1.1538 (1.1175)  loss_mask: 0.0000 (0.0000)  time: 0.1706  data: 0.0002  max mem: 6052
[06:26:09.337906] Epoch: [88]  [680/781]  eta: 0:00:17  lr: 0.000009  training_loss: 1.1523 (1.1182)  classification_loss: 1.1523 (1.1182)  loss_mask: 0.0000 (0.0000)  time: 0.1705  data: 0.0002  max mem: 6052
[06:26:12.754170] Epoch: [88]  [700/781]  eta: 0:00:13  lr: 0.000009  training_loss: 1.1842 (1.1200)  classification_loss: 1.1841 (1.1200)  loss_mask: 0.0000 (0.0000)  time: 0.1707  data: 0.0002  max mem: 6052
[06:26:16.172234] Epoch: [88]  [720/781]  eta: 0:00:10  lr: 0.000009  training_loss: 1.1597 (1.1207)  classification_loss: 1.1597 (1.1207)  loss_mask: 0.0000 (0.0000)  time: 0.1708  data: 0.0002  max mem: 6052
[06:26:19.596137] Epoch: [88]  [740/781]  eta: 0:00:07  lr: 0.000009  training_loss: 1.0772 (1.1203)  classification_loss: 1.0772 (1.1202)  loss_mask: 0.0000 (0.0000)  time: 0.1711  data: 0.0002  max mem: 6052
[06:26:23.028715] Epoch: [88]  [760/781]  eta: 0:00:03  lr: 0.000009  training_loss: 1.1404 (1.1211)  classification_loss: 1.1404 (1.1211)  loss_mask: 0.0000 (0.0000)  time: 0.1716  data: 0.0002  max mem: 6052
[06:26:26.429291] Epoch: [88]  [780/781]  eta: 0:00:00  lr: 0.000009  training_loss: 1.1538 (1.1217)  classification_loss: 1.1538 (1.1216)  loss_mask: 0.0000 (0.0000)  time: 0.1699  data: 0.0002  max mem: 6052
[06:26:26.607168] Epoch: [88] Total time: 0:02:14 (0.1726 s / it)
[06:26:26.607639] Averaged stats: lr: 0.000009  training_loss: 1.1538 (1.1217)  classification_loss: 1.1538 (1.1216)  loss_mask: 0.0000 (0.0000)
[06:26:27.270085] Test:  [  0/157]  eta: 0:01:43  testing_loss: 0.4446 (0.4446)  acc1: 89.0625 (89.0625)  acc5: 98.4375 (98.4375)  time: 0.6568  data: 0.6276  max mem: 6052
[06:26:27.553720] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4383 (0.4563)  acc1: 87.5000 (86.2216)  acc5: 100.0000 (99.5739)  time: 0.0853  data: 0.0572  max mem: 6052
[06:26:27.834436] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4191 (0.4330)  acc1: 87.5000 (87.3512)  acc5: 100.0000 (99.4792)  time: 0.0280  data: 0.0002  max mem: 6052
[06:26:28.117192] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4132 (0.4416)  acc1: 87.5000 (87.1472)  acc5: 100.0000 (99.3448)  time: 0.0280  data: 0.0002  max mem: 6052
[06:26:28.398643] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4507 (0.4478)  acc1: 87.5000 (86.8521)  acc5: 98.4375 (99.2378)  time: 0.0281  data: 0.0002  max mem: 6052
[06:26:28.680542] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4198 (0.4434)  acc1: 87.5000 (86.9485)  acc5: 100.0000 (99.2647)  time: 0.0280  data: 0.0002  max mem: 6052
[06:26:28.963919] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4194 (0.4424)  acc1: 87.5000 (86.9877)  acc5: 100.0000 (99.2828)  time: 0.0282  data: 0.0002  max mem: 6052
[06:26:29.249024] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4341 (0.4390)  acc1: 87.5000 (87.0379)  acc5: 100.0000 (99.2958)  time: 0.0283  data: 0.0002  max mem: 6052
[06:26:29.529667] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4561 (0.4460)  acc1: 87.5000 (86.9213)  acc5: 100.0000 (99.3441)  time: 0.0282  data: 0.0002  max mem: 6052
[06:26:29.809692] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4418 (0.4430)  acc1: 87.5000 (87.0192)  acc5: 100.0000 (99.3647)  time: 0.0279  data: 0.0002  max mem: 6052
[06:26:30.095586] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4246 (0.4453)  acc1: 87.5000 (86.9585)  acc5: 100.0000 (99.3812)  time: 0.0282  data: 0.0002  max mem: 6052
[06:26:30.381710] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4253 (0.4440)  acc1: 87.5000 (87.0918)  acc5: 100.0000 (99.3666)  time: 0.0285  data: 0.0002  max mem: 6052
[06:26:30.668767] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4132 (0.4440)  acc1: 87.5000 (87.0610)  acc5: 100.0000 (99.3673)  time: 0.0285  data: 0.0003  max mem: 6052
[06:26:30.956807] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.3980 (0.4447)  acc1: 87.5000 (87.0945)  acc5: 100.0000 (99.3559)  time: 0.0286  data: 0.0002  max mem: 6052
[06:26:31.240834] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3980 (0.4420)  acc1: 87.5000 (87.2230)  acc5: 100.0000 (99.3684)  time: 0.0285  data: 0.0002  max mem: 6052
[06:26:31.518682] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4240 (0.4418)  acc1: 87.5000 (87.1482)  acc5: 100.0000 (99.3791)  time: 0.0280  data: 0.0001  max mem: 6052
[06:26:31.668234] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4406 (0.4432)  acc1: 87.5000 (87.0900)  acc5: 100.0000 (99.3800)  time: 0.0268  data: 0.0001  max mem: 6052
[06:26:31.822259] Test: Total time: 0:00:05 (0.0332 s / it)
[06:26:31.823529] * Acc@1 87.090 Acc@5 99.380 loss 0.443
[06:26:31.824426] Accuracy of the network on the 10000 test images: 87.1%
[06:26:31.825148] Max accuracy: 87.14%
[06:26:32.006860] log_dir: ./output_dir
[06:26:32.908343] Epoch: [89]  [  0/781]  eta: 0:11:42  lr: 0.000009  training_loss: 0.9554 (0.9554)  classification_loss: 0.9554 (0.9554)  loss_mask: 0.0000 (0.0000)  time: 0.9000  data: 0.6704  max mem: 6052
[06:26:36.357835] Epoch: [89]  [ 20/781]  eta: 0:02:37  lr: 0.000009  training_loss: 1.0987 (1.1027)  classification_loss: 1.0986 (1.1026)  loss_mask: 0.0000 (0.0000)  time: 0.1724  data: 0.0002  max mem: 6052
[06:26:39.821951] Epoch: [89]  [ 40/781]  eta: 0:02:21  lr: 0.000009  training_loss: 1.1329 (1.1217)  classification_loss: 1.1328 (1.1217)  loss_mask: 0.0000 (0.0000)  time: 0.1731  data: 0.0002  max mem: 6052
[06:26:43.241803] Epoch: [89]  [ 60/781]  eta: 0:02:12  lr: 0.000009  training_loss: 1.1640 (1.1380)  classification_loss: 1.1639 (1.1380)  loss_mask: 0.0000 (0.0000)  time: 0.1709  data: 0.0003  max mem: 6052
[06:26:46.660764] Epoch: [89]  [ 80/781]  eta: 0:02:06  lr: 0.000009  training_loss: 1.1113 (1.1331)  classification_loss: 1.1113 (1.1331)  loss_mask: 0.0000 (0.0000)  time: 0.1709  data: 0.0002  max mem: 6052
[06:26:50.084199] Epoch: [89]  [100/781]  eta: 0:02:01  lr: 0.000009  training_loss: 1.0921 (1.1298)  classification_loss: 1.0920 (1.1298)  loss_mask: 0.0000 (0.0000)  time: 0.1711  data: 0.0002  max mem: 6052
[06:26:53.512543] Epoch: [89]  [120/781]  eta: 0:01:57  lr: 0.000009  training_loss: 1.0822 (1.1256)  classification_loss: 1.0821 (1.1255)  loss_mask: 0.0000 (0.0000)  time: 0.1713  data: 0.0002  max mem: 6052
[06:26:56.939119] Epoch: [89]  [140/781]  eta: 0:01:53  lr: 0.000009  training_loss: 1.1161 (1.1249)  classification_loss: 1.1160 (1.1249)  loss_mask: 0.0000 (0.0000)  time: 0.1713  data: 0.0002  max mem: 6052
[06:27:00.356148] Epoch: [89]  [160/781]  eta: 0:01:49  lr: 0.000009  training_loss: 1.1140 (1.1228)  classification_loss: 1.1140 (1.1228)  loss_mask: 0.0000 (0.0000)  time: 0.1708  data: 0.0003  max mem: 6052
[06:27:03.816815] Epoch: [89]  [180/781]  eta: 0:01:45  lr: 0.000009  training_loss: 1.1314 (1.1250)  classification_loss: 1.1314 (1.1249)  loss_mask: 0.0000 (0.0000)  time: 0.1730  data: 0.0002  max mem: 6052
[06:27:07.351515] Epoch: [89]  [200/781]  eta: 0:01:42  lr: 0.000009  training_loss: 1.1775 (1.1285)  classification_loss: 1.1774 (1.1285)  loss_mask: 0.0000 (0.0000)  time: 0.1767  data: 0.0002  max mem: 6052
[06:27:10.764350] Epoch: [89]  [220/781]  eta: 0:01:38  lr: 0.000009  training_loss: 1.1167 (1.1282)  classification_loss: 1.1166 (1.1282)  loss_mask: 0.0000 (0.0000)  time: 0.1706  data: 0.0001  max mem: 6052
[06:27:14.153066] Epoch: [89]  [240/781]  eta: 0:01:34  lr: 0.000009  training_loss: 1.1245 (1.1291)  classification_loss: 1.1244 (1.1290)  loss_mask: 0.0000 (0.0000)  time: 0.1693  data: 0.0002  max mem: 6052
[06:27:17.554105] Epoch: [89]  [260/781]  eta: 0:01:30  lr: 0.000009  training_loss: 1.1104 (1.1279)  classification_loss: 1.1104 (1.1279)  loss_mask: 0.0000 (0.0000)  time: 0.1700  data: 0.0002  max mem: 6052
[06:27:20.969154] Epoch: [89]  [280/781]  eta: 0:01:27  lr: 0.000009  training_loss: 1.1069 (1.1275)  classification_loss: 1.1069 (1.1275)  loss_mask: 0.0000 (0.0000)  time: 0.1707  data: 0.0003  max mem: 6052
[06:27:24.437878] Epoch: [89]  [300/781]  eta: 0:01:23  lr: 0.000009  training_loss: 1.1101 (1.1272)  classification_loss: 1.1101 (1.1272)  loss_mask: 0.0000 (0.0000)  time: 0.1734  data: 0.0003  max mem: 6052
[06:27:27.842567] Epoch: [89]  [320/781]  eta: 0:01:20  lr: 0.000009  training_loss: 1.0948 (1.1266)  classification_loss: 1.0947 (1.1266)  loss_mask: 0.0000 (0.0000)  time: 0.1702  data: 0.0002  max mem: 6052
[06:27:31.247286] Epoch: [89]  [340/781]  eta: 0:01:16  lr: 0.000009  training_loss: 1.0923 (1.1249)  classification_loss: 1.0923 (1.1248)  loss_mask: 0.0000 (0.0000)  time: 0.1702  data: 0.0003  max mem: 6052
[06:27:34.656782] Epoch: [89]  [360/781]  eta: 0:01:13  lr: 0.000008  training_loss: 1.1178 (1.1257)  classification_loss: 1.1177 (1.1256)  loss_mask: 0.0000 (0.0000)  time: 0.1704  data: 0.0002  max mem: 6052
[06:27:38.068272] Epoch: [89]  [380/781]  eta: 0:01:09  lr: 0.000008  training_loss: 1.0786 (1.1242)  classification_loss: 1.0785 (1.1241)  loss_mask: 0.0000 (0.0000)  time: 0.1705  data: 0.0001  max mem: 6052
[06:27:41.476650] Epoch: [89]  [400/781]  eta: 0:01:05  lr: 0.000008  training_loss: 1.1044 (1.1233)  classification_loss: 1.1044 (1.1232)  loss_mask: 0.0000 (0.0000)  time: 0.1703  data: 0.0003  max mem: 6052
[06:27:44.897788] Epoch: [89]  [420/781]  eta: 0:01:02  lr: 0.000008  training_loss: 1.1148 (1.1225)  classification_loss: 1.1147 (1.1224)  loss_mask: 0.0000 (0.0000)  time: 0.1710  data: 0.0003  max mem: 6052
[06:27:48.334501] Epoch: [89]  [440/781]  eta: 0:00:58  lr: 0.000008  training_loss: 1.1359 (1.1216)  classification_loss: 1.1358 (1.1216)  loss_mask: 0.0000 (0.0000)  time: 0.1718  data: 0.0001  max mem: 6052
[06:27:51.746179] Epoch: [89]  [460/781]  eta: 0:00:55  lr: 0.000008  training_loss: 1.0959 (1.1209)  classification_loss: 1.0959 (1.1209)  loss_mask: 0.0000 (0.0000)  time: 0.1705  data: 0.0001  max mem: 6052
[06:27:55.156647] Epoch: [89]  [480/781]  eta: 0:00:52  lr: 0.000008  training_loss: 1.1303 (1.1218)  classification_loss: 1.1302 (1.1218)  loss_mask: 0.0000 (0.0000)  time: 0.1704  data: 0.0002  max mem: 6052
[06:27:58.620212] Epoch: [89]  [500/781]  eta: 0:00:48  lr: 0.000008  training_loss: 1.0642 (1.1205)  classification_loss: 1.0642 (1.1205)  loss_mask: 0.0000 (0.0000)  time: 0.1731  data: 0.0002  max mem: 6052
[06:28:02.056837] Epoch: [89]  [520/781]  eta: 0:00:45  lr: 0.000008  training_loss: 1.1466 (1.1209)  classification_loss: 1.1465 (1.1208)  loss_mask: 0.0000 (0.0000)  time: 0.1717  data: 0.0003  max mem: 6052
[06:28:05.515584] Epoch: [89]  [540/781]  eta: 0:00:41  lr: 0.000008  training_loss: 1.1135 (1.1204)  classification_loss: 1.1135 (1.1204)  loss_mask: 0.0000 (0.0000)  time: 0.1729  data: 0.0002  max mem: 6052
[06:28:09.008929] Epoch: [89]  [560/781]  eta: 0:00:38  lr: 0.000008  training_loss: 1.0679 (1.1192)  classification_loss: 1.0679 (1.1192)  loss_mask: 0.0000 (0.0000)  time: 0.1746  data: 0.0002  max mem: 6052
[06:28:12.421306] Epoch: [89]  [580/781]  eta: 0:00:34  lr: 0.000008  training_loss: 1.1308 (1.1195)  classification_loss: 1.1308 (1.1194)  loss_mask: 0.0000 (0.0000)  time: 0.1705  data: 0.0002  max mem: 6052
[06:28:15.831060] Epoch: [89]  [600/781]  eta: 0:00:31  lr: 0.000008  training_loss: 1.0600 (1.1186)  classification_loss: 1.0599 (1.1185)  loss_mask: 0.0000 (0.0000)  time: 0.1704  data: 0.0003  max mem: 6052
[06:28:19.240092] Epoch: [89]  [620/781]  eta: 0:00:27  lr: 0.000008  training_loss: 1.1011 (1.1185)  classification_loss: 1.1011 (1.1185)  loss_mask: 0.0000 (0.0000)  time: 0.1704  data: 0.0003  max mem: 6052
[06:28:22.665863] Epoch: [89]  [640/781]  eta: 0:00:24  lr: 0.000008  training_loss: 1.1298 (1.1187)  classification_loss: 1.1298 (1.1186)  loss_mask: 0.0000 (0.0000)  time: 0.1712  data: 0.0002  max mem: 6052
[06:28:26.091048] Epoch: [89]  [660/781]  eta: 0:00:20  lr: 0.000008  training_loss: 1.1656 (1.1198)  classification_loss: 1.1656 (1.1197)  loss_mask: 0.0000 (0.0000)  time: 0.1712  data: 0.0002  max mem: 6052
[06:28:29.520970] Epoch: [89]  [680/781]  eta: 0:00:17  lr: 0.000008  training_loss: 1.1198 (1.1198)  classification_loss: 1.1198 (1.1197)  loss_mask: 0.0000 (0.0000)  time: 0.1714  data: 0.0002  max mem: 6052
[06:28:32.953712] Epoch: [89]  [700/781]  eta: 0:00:13  lr: 0.000008  training_loss: 1.1098 (1.1204)  classification_loss: 1.1098 (1.1204)  loss_mask: 0.0000 (0.0000)  time: 0.1716  data: 0.0003  max mem: 6052
[06:28:36.368263] Epoch: [89]  [720/781]  eta: 0:00:10  lr: 0.000008  training_loss: 1.0917 (1.1207)  classification_loss: 1.0916 (1.1206)  loss_mask: 0.0000 (0.0000)  time: 0.1706  data: 0.0002  max mem: 6052
[06:28:39.772774] Epoch: [89]  [740/781]  eta: 0:00:07  lr: 0.000008  training_loss: 1.0929 (1.1203)  classification_loss: 1.0929 (1.1203)  loss_mask: 0.0000 (0.0000)  time: 0.1702  data: 0.0003  max mem: 6052
[06:28:43.195106] Epoch: [89]  [760/781]  eta: 0:00:03  lr: 0.000008  training_loss: 1.1236 (1.1201)  classification_loss: 1.1235 (1.1201)  loss_mask: 0.0000 (0.0000)  time: 0.1710  data: 0.0002  max mem: 6052
[06:28:46.591683] Epoch: [89]  [780/781]  eta: 0:00:00  lr: 0.000008  training_loss: 1.1112 (1.1206)  classification_loss: 1.1112 (1.1205)  loss_mask: 0.0000 (0.0000)  time: 0.1697  data: 0.0002  max mem: 6052
[06:28:46.754573] Epoch: [89] Total time: 0:02:14 (0.1725 s / it)
[06:28:46.755797] Averaged stats: lr: 0.000008  training_loss: 1.1112 (1.1206)  classification_loss: 1.1112 (1.1205)  loss_mask: 0.0000 (0.0000)
[06:28:47.413362] Test:  [  0/157]  eta: 0:01:42  testing_loss: 0.4468 (0.4468)  acc1: 89.0625 (89.0625)  acc5: 98.4375 (98.4375)  time: 0.6510  data: 0.6169  max mem: 6052
[06:28:47.698951] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4307 (0.4501)  acc1: 87.5000 (87.2159)  acc5: 100.0000 (99.7159)  time: 0.0850  data: 0.0562  max mem: 6052
[06:28:47.980564] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4177 (0.4284)  acc1: 87.5000 (87.9464)  acc5: 100.0000 (99.5536)  time: 0.0282  data: 0.0001  max mem: 6052
[06:28:48.263349] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4150 (0.4350)  acc1: 87.5000 (87.5504)  acc5: 100.0000 (99.4960)  time: 0.0281  data: 0.0001  max mem: 6052
[06:28:48.544479] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4522 (0.4412)  acc1: 87.5000 (87.5762)  acc5: 98.4375 (99.3521)  time: 0.0281  data: 0.0001  max mem: 6052
[06:28:48.825451] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4217 (0.4373)  acc1: 89.0625 (87.7451)  acc5: 100.0000 (99.3260)  time: 0.0280  data: 0.0001  max mem: 6052
[06:28:49.106370] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4144 (0.4373)  acc1: 85.9375 (87.4744)  acc5: 100.0000 (99.2828)  time: 0.0280  data: 0.0001  max mem: 6052
[06:28:49.386976] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4161 (0.4331)  acc1: 85.9375 (87.4560)  acc5: 100.0000 (99.2958)  time: 0.0280  data: 0.0001  max mem: 6052
[06:28:49.668728] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4429 (0.4402)  acc1: 87.5000 (87.2878)  acc5: 100.0000 (99.3248)  time: 0.0280  data: 0.0001  max mem: 6052
[06:28:49.949423] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4445 (0.4369)  acc1: 87.5000 (87.4141)  acc5: 100.0000 (99.3304)  time: 0.0280  data: 0.0001  max mem: 6052
[06:28:50.230148] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4263 (0.4404)  acc1: 87.5000 (87.2061)  acc5: 100.0000 (99.3348)  time: 0.0280  data: 0.0001  max mem: 6052
[06:28:50.511420] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4271 (0.4391)  acc1: 87.5000 (87.3311)  acc5: 100.0000 (99.3102)  time: 0.0280  data: 0.0001  max mem: 6052
[06:28:50.793084] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.3902 (0.4390)  acc1: 89.0625 (87.3450)  acc5: 100.0000 (99.3027)  time: 0.0280  data: 0.0001  max mem: 6052
[06:28:51.073148] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.3854 (0.4395)  acc1: 87.5000 (87.3569)  acc5: 100.0000 (99.2963)  time: 0.0280  data: 0.0001  max mem: 6052
[06:28:51.355823] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4035 (0.4365)  acc1: 87.5000 (87.5111)  acc5: 100.0000 (99.3351)  time: 0.0280  data: 0.0001  max mem: 6052
[06:28:51.633463] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4193 (0.4361)  acc1: 87.5000 (87.4690)  acc5: 100.0000 (99.3584)  time: 0.0279  data: 0.0001  max mem: 6052
[06:28:51.782193] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4457 (0.4373)  acc1: 87.5000 (87.3800)  acc5: 100.0000 (99.3700)  time: 0.0268  data: 0.0001  max mem: 6052
[06:28:51.933730] Test: Total time: 0:00:05 (0.0330 s / it)
[06:28:51.935083] * Acc@1 87.380 Acc@5 99.370 loss 0.437
[06:28:51.935761] Accuracy of the network on the 10000 test images: 87.4%
[06:28:51.936419] Max accuracy: 87.38%
[06:28:52.177376] log_dir: ./output_dir
[06:28:53.069947] Epoch: [90]  [  0/781]  eta: 0:11:35  lr: 0.000008  training_loss: 1.0586 (1.0586)  classification_loss: 1.0585 (1.0585)  loss_mask: 0.0001 (0.0001)  time: 0.8903  data: 0.6898  max mem: 6052
[06:28:56.486527] Epoch: [90]  [ 20/781]  eta: 0:02:36  lr: 0.000008  training_loss: 1.0895 (1.0894)  classification_loss: 1.0895 (1.0894)  loss_mask: 0.0000 (0.0000)  time: 0.1707  data: 0.0002  max mem: 6052
[06:28:59.911209] Epoch: [90]  [ 40/781]  eta: 0:02:19  lr: 0.000008  training_loss: 1.1056 (1.0984)  classification_loss: 1.1056 (1.0984)  loss_mask: 0.0000 (0.0000)  time: 0.1712  data: 0.0003  max mem: 6052
[06:29:03.332609] Epoch: [90]  [ 60/781]  eta: 0:02:11  lr: 0.000008  training_loss: 1.1525 (1.1142)  classification_loss: 1.1525 (1.1142)  loss_mask: 0.0000 (0.0000)  time: 0.1710  data: 0.0002  max mem: 6052
[06:29:06.745617] Epoch: [90]  [ 80/781]  eta: 0:02:06  lr: 0.000008  training_loss: 1.0689 (1.1067)  classification_loss: 1.0688 (1.1067)  loss_mask: 0.0000 (0.0000)  time: 0.1706  data: 0.0003  max mem: 6052
[06:29:10.228046] Epoch: [90]  [100/781]  eta: 0:02:01  lr: 0.000008  training_loss: 1.1453 (1.1172)  classification_loss: 1.1453 (1.1171)  loss_mask: 0.0000 (0.0000)  time: 0.1740  data: 0.0002  max mem: 6052
[06:29:13.672272] Epoch: [90]  [120/781]  eta: 0:01:57  lr: 0.000008  training_loss: 1.1070 (1.1172)  classification_loss: 1.1070 (1.1171)  loss_mask: 0.0000 (0.0000)  time: 0.1721  data: 0.0003  max mem: 6052
[06:29:17.153544] Epoch: [90]  [140/781]  eta: 0:01:53  lr: 0.000008  training_loss: 1.1741 (1.1212)  classification_loss: 1.1741 (1.1211)  loss_mask: 0.0000 (0.0000)  time: 0.1740  data: 0.0002  max mem: 6052
[06:29:20.579032] Epoch: [90]  [160/781]  eta: 0:01:49  lr: 0.000007  training_loss: 1.1252 (1.1233)  classification_loss: 1.1218 (1.1223)  loss_mask: 0.0002 (0.0010)  time: 0.1712  data: 0.0002  max mem: 6052
[06:29:23.995581] Epoch: [90]  [180/781]  eta: 0:01:45  lr: 0.000007  training_loss: 1.1167 (1.1222)  classification_loss: 1.1162 (1.1212)  loss_mask: 0.0001 (0.0010)  time: 0.1708  data: 0.0003  max mem: 6052
[06:29:27.417225] Epoch: [90]  [200/781]  eta: 0:01:41  lr: 0.000007  training_loss: 1.1039 (1.1199)  classification_loss: 1.1039 (1.1191)  loss_mask: 0.0001 (0.0009)  time: 0.1710  data: 0.0003  max mem: 6052
[06:29:30.854793] Epoch: [90]  [220/781]  eta: 0:01:38  lr: 0.000007  training_loss: 1.1320 (1.1189)  classification_loss: 1.1319 (1.1181)  loss_mask: 0.0001 (0.0008)  time: 0.1717  data: 0.0003  max mem: 6052
[06:29:34.270673] Epoch: [90]  [240/781]  eta: 0:01:34  lr: 0.000007  training_loss: 1.0801 (1.1186)  classification_loss: 1.0800 (1.1178)  loss_mask: 0.0000 (0.0007)  time: 0.1707  data: 0.0002  max mem: 6052
[06:29:37.787433] Epoch: [90]  [260/781]  eta: 0:01:30  lr: 0.000007  training_loss: 1.1290 (1.1195)  classification_loss: 1.1290 (1.1188)  loss_mask: 0.0000 (0.0007)  time: 0.1757  data: 0.0002  max mem: 6052
[06:29:41.204403] Epoch: [90]  [280/781]  eta: 0:01:27  lr: 0.000007  training_loss: 1.0752 (1.1168)  classification_loss: 1.0751 (1.1162)  loss_mask: 0.0000 (0.0006)  time: 0.1708  data: 0.0002  max mem: 6052
[06:29:44.608403] Epoch: [90]  [300/781]  eta: 0:01:23  lr: 0.000007  training_loss: 1.1734 (1.1198)  classification_loss: 1.1733 (1.1192)  loss_mask: 0.0000 (0.0006)  time: 0.1701  data: 0.0002  max mem: 6052
[06:29:48.023328] Epoch: [90]  [320/781]  eta: 0:01:20  lr: 0.000007  training_loss: 1.0885 (1.1193)  classification_loss: 1.0884 (1.1187)  loss_mask: 0.0000 (0.0006)  time: 0.1706  data: 0.0002  max mem: 6052
[06:29:51.431573] Epoch: [90]  [340/781]  eta: 0:01:16  lr: 0.000007  training_loss: 1.0946 (1.1174)  classification_loss: 1.0946 (1.1169)  loss_mask: 0.0000 (0.0005)  time: 0.1703  data: 0.0004  max mem: 6052
[06:29:54.845514] Epoch: [90]  [360/781]  eta: 0:01:13  lr: 0.000007  training_loss: 1.0939 (1.1170)  classification_loss: 1.0938 (1.1165)  loss_mask: 0.0000 (0.0005)  time: 0.1706  data: 0.0003  max mem: 6052
[06:29:58.262561] Epoch: [90]  [380/781]  eta: 0:01:09  lr: 0.000007  training_loss: 1.1500 (1.1196)  classification_loss: 1.1500 (1.1191)  loss_mask: 0.0000 (0.0005)  time: 0.1708  data: 0.0003  max mem: 6052
[06:30:01.683933] Epoch: [90]  [400/781]  eta: 0:01:06  lr: 0.000007  training_loss: 1.0635 (1.1176)  classification_loss: 1.0635 (1.1171)  loss_mask: 0.0000 (0.0005)  time: 0.1710  data: 0.0001  max mem: 6052
[06:30:05.132315] Epoch: [90]  [420/781]  eta: 0:01:02  lr: 0.000007  training_loss: 1.0875 (1.1174)  classification_loss: 1.0874 (1.1170)  loss_mask: 0.0000 (0.0004)  time: 0.1723  data: 0.0003  max mem: 6052
[06:30:08.543983] Epoch: [90]  [440/781]  eta: 0:00:59  lr: 0.000007  training_loss: 1.1170 (1.1175)  classification_loss: 1.1170 (1.1170)  loss_mask: 0.0000 (0.0004)  time: 0.1705  data: 0.0002  max mem: 6052
[06:30:11.964763] Epoch: [90]  [460/781]  eta: 0:00:55  lr: 0.000007  training_loss: 1.0683 (1.1168)  classification_loss: 1.0682 (1.1164)  loss_mask: 0.0000 (0.0004)  time: 0.1710  data: 0.0001  max mem: 6052
[06:30:15.396799] Epoch: [90]  [480/781]  eta: 0:00:52  lr: 0.000007  training_loss: 1.1041 (1.1160)  classification_loss: 1.1041 (1.1156)  loss_mask: 0.0000 (0.0004)  time: 0.1715  data: 0.0002  max mem: 6052
[06:30:18.835050] Epoch: [90]  [500/781]  eta: 0:00:48  lr: 0.000007  training_loss: 1.0922 (1.1157)  classification_loss: 1.0922 (1.1153)  loss_mask: 0.0000 (0.0004)  time: 0.1718  data: 0.0002  max mem: 6052
[06:30:22.254188] Epoch: [90]  [520/781]  eta: 0:00:45  lr: 0.000007  training_loss: 1.1074 (1.1155)  classification_loss: 1.1074 (1.1152)  loss_mask: 0.0000 (0.0004)  time: 0.1709  data: 0.0002  max mem: 6052
[06:30:25.673166] Epoch: [90]  [540/781]  eta: 0:00:41  lr: 0.000007  training_loss: 1.1129 (1.1156)  classification_loss: 1.1128 (1.1152)  loss_mask: 0.0000 (0.0004)  time: 0.1708  data: 0.0003  max mem: 6052
[06:30:29.078050] Epoch: [90]  [560/781]  eta: 0:00:38  lr: 0.000007  training_loss: 1.1378 (1.1168)  classification_loss: 1.1378 (1.1165)  loss_mask: 0.0000 (0.0003)  time: 0.1702  data: 0.0001  max mem: 6052
[06:30:32.485770] Epoch: [90]  [580/781]  eta: 0:00:34  lr: 0.000007  training_loss: 1.1468 (1.1174)  classification_loss: 1.1468 (1.1171)  loss_mask: 0.0000 (0.0003)  time: 0.1703  data: 0.0003  max mem: 6052
[06:30:35.898306] Epoch: [90]  [600/781]  eta: 0:00:31  lr: 0.000007  training_loss: 1.1041 (1.1169)  classification_loss: 1.1040 (1.1166)  loss_mask: 0.0000 (0.0003)  time: 0.1706  data: 0.0002  max mem: 6052
[06:30:39.311264] Epoch: [90]  [620/781]  eta: 0:00:27  lr: 0.000007  training_loss: 1.1043 (1.1164)  classification_loss: 1.1043 (1.1160)  loss_mask: 0.0000 (0.0003)  time: 0.1706  data: 0.0002  max mem: 6052
[06:30:42.833235] Epoch: [90]  [640/781]  eta: 0:00:24  lr: 0.000007  training_loss: 1.1131 (1.1164)  classification_loss: 1.1131 (1.1161)  loss_mask: 0.0000 (0.0003)  time: 0.1760  data: 0.0003  max mem: 6052
[06:30:46.245806] Epoch: [90]  [660/781]  eta: 0:00:20  lr: 0.000007  training_loss: 1.0811 (1.1160)  classification_loss: 1.0811 (1.1157)  loss_mask: 0.0000 (0.0003)  time: 0.1706  data: 0.0002  max mem: 6052
[06:30:49.667082] Epoch: [90]  [680/781]  eta: 0:00:17  lr: 0.000007  training_loss: 1.0795 (1.1157)  classification_loss: 1.0794 (1.1154)  loss_mask: 0.0000 (0.0003)  time: 0.1709  data: 0.0003  max mem: 6052
[06:30:53.077734] Epoch: [90]  [700/781]  eta: 0:00:13  lr: 0.000007  training_loss: 1.1047 (1.1161)  classification_loss: 1.1046 (1.1158)  loss_mask: 0.0000 (0.0003)  time: 0.1704  data: 0.0003  max mem: 6052
[06:30:56.522139] Epoch: [90]  [720/781]  eta: 0:00:10  lr: 0.000007  training_loss: 1.0803 (1.1153)  classification_loss: 1.0803 (1.1150)  loss_mask: 0.0000 (0.0003)  time: 0.1721  data: 0.0003  max mem: 6052
[06:30:59.981248] Epoch: [90]  [740/781]  eta: 0:00:07  lr: 0.000007  training_loss: 1.1137 (1.1150)  classification_loss: 1.1136 (1.1147)  loss_mask: 0.0000 (0.0003)  time: 0.1728  data: 0.0002  max mem: 6052
[06:31:03.402456] Epoch: [90]  [760/781]  eta: 0:00:03  lr: 0.000007  training_loss: 1.1752 (1.1162)  classification_loss: 1.1752 (1.1160)  loss_mask: 0.0000 (0.0003)  time: 0.1710  data: 0.0004  max mem: 6052
[06:31:06.859613] Epoch: [90]  [780/781]  eta: 0:00:00  lr: 0.000006  training_loss: 1.1722 (1.1176)  classification_loss: 1.1722 (1.1174)  loss_mask: 0.0000 (0.0003)  time: 0.1728  data: 0.0002  max mem: 6052
[06:31:07.029595] Epoch: [90] Total time: 0:02:14 (0.1727 s / it)
[06:31:07.030034] Averaged stats: lr: 0.000006  training_loss: 1.1722 (1.1176)  classification_loss: 1.1722 (1.1174)  loss_mask: 0.0000 (0.0003)
[06:31:08.090501] Test:  [  0/157]  eta: 0:01:41  testing_loss: 0.4410 (0.4410)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (100.0000)  time: 0.6457  data: 0.6141  max mem: 6052
[06:31:08.375627] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4273 (0.4503)  acc1: 87.5000 (86.9318)  acc5: 100.0000 (99.7159)  time: 0.0843  data: 0.0560  max mem: 6052
[06:31:08.661408] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4113 (0.4247)  acc1: 87.5000 (87.6488)  acc5: 100.0000 (99.6280)  time: 0.0283  data: 0.0002  max mem: 6052
[06:31:08.944037] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4113 (0.4304)  acc1: 87.5000 (87.6008)  acc5: 100.0000 (99.5464)  time: 0.0283  data: 0.0002  max mem: 6052
[06:31:09.225549] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4396 (0.4371)  acc1: 87.5000 (87.4619)  acc5: 98.4375 (99.3902)  time: 0.0281  data: 0.0001  max mem: 6052
[06:31:09.509149] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4299 (0.4351)  acc1: 87.5000 (87.6838)  acc5: 98.4375 (99.3566)  time: 0.0281  data: 0.0001  max mem: 6052
[06:31:09.790439] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4169 (0.4340)  acc1: 89.0625 (87.5768)  acc5: 100.0000 (99.3340)  time: 0.0281  data: 0.0001  max mem: 6052
[06:31:10.071703] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4219 (0.4314)  acc1: 87.5000 (87.5440)  acc5: 100.0000 (99.3398)  time: 0.0280  data: 0.0001  max mem: 6052
[06:31:10.352853] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4380 (0.4386)  acc1: 87.5000 (87.4228)  acc5: 100.0000 (99.3248)  time: 0.0280  data: 0.0001  max mem: 6052
[06:31:10.639162] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4380 (0.4359)  acc1: 89.0625 (87.5000)  acc5: 100.0000 (99.3304)  time: 0.0282  data: 0.0002  max mem: 6052
[06:31:10.919823] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4182 (0.4389)  acc1: 89.0625 (87.3917)  acc5: 100.0000 (99.3502)  time: 0.0282  data: 0.0002  max mem: 6052
[06:31:11.200698] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4216 (0.4379)  acc1: 87.5000 (87.4437)  acc5: 100.0000 (99.3806)  time: 0.0279  data: 0.0001  max mem: 6052
[06:31:11.486310] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4068 (0.4384)  acc1: 89.0625 (87.4354)  acc5: 100.0000 (99.3673)  time: 0.0282  data: 0.0001  max mem: 6052
[06:31:11.767179] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4068 (0.4388)  acc1: 87.5000 (87.4642)  acc5: 100.0000 (99.3798)  time: 0.0282  data: 0.0001  max mem: 6052
[06:31:12.046783] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3943 (0.4363)  acc1: 87.5000 (87.5997)  acc5: 100.0000 (99.4016)  time: 0.0279  data: 0.0001  max mem: 6052
[06:31:12.325808] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4229 (0.4361)  acc1: 87.5000 (87.5207)  acc5: 100.0000 (99.4205)  time: 0.0278  data: 0.0001  max mem: 6052
[06:31:12.475102] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4505 (0.4378)  acc1: 85.9375 (87.4100)  acc5: 100.0000 (99.4200)  time: 0.0268  data: 0.0001  max mem: 6052
[06:31:12.628760] Test: Total time: 0:00:05 (0.0330 s / it)
[06:31:12.629222] * Acc@1 87.410 Acc@5 99.420 loss 0.438
[06:31:12.629510] Accuracy of the network on the 10000 test images: 87.4%
[06:31:12.629694] Max accuracy: 87.41%
[06:31:12.798281] log_dir: ./output_dir
[06:31:13.651078] Epoch: [91]  [  0/781]  eta: 0:11:04  lr: 0.000006  training_loss: 0.9926 (0.9926)  classification_loss: 0.9926 (0.9926)  loss_mask: 0.0000 (0.0000)  time: 0.8513  data: 0.6735  max mem: 6052
[06:31:17.070538] Epoch: [91]  [ 20/781]  eta: 0:02:34  lr: 0.000006  training_loss: 1.0694 (1.1015)  classification_loss: 1.0694 (1.1015)  loss_mask: 0.0000 (0.0000)  time: 0.1709  data: 0.0002  max mem: 6052
[06:31:20.514841] Epoch: [91]  [ 40/781]  eta: 0:02:19  lr: 0.000006  training_loss: 1.1115 (1.1203)  classification_loss: 1.1115 (1.1202)  loss_mask: 0.0000 (0.0000)  time: 0.1721  data: 0.0002  max mem: 6052
[06:31:23.919897] Epoch: [91]  [ 60/781]  eta: 0:02:11  lr: 0.000006  training_loss: 1.1157 (1.1225)  classification_loss: 1.1157 (1.1225)  loss_mask: 0.0000 (0.0000)  time: 0.1702  data: 0.0002  max mem: 6052
[06:31:27.336521] Epoch: [91]  [ 80/781]  eta: 0:02:05  lr: 0.000006  training_loss: 1.0969 (1.1180)  classification_loss: 1.0969 (1.1180)  loss_mask: 0.0000 (0.0000)  time: 0.1708  data: 0.0003  max mem: 6052
[06:31:30.750358] Epoch: [91]  [100/781]  eta: 0:02:00  lr: 0.000006  training_loss: 1.1047 (1.1186)  classification_loss: 1.1046 (1.1186)  loss_mask: 0.0000 (0.0000)  time: 0.1706  data: 0.0002  max mem: 6052
[06:31:34.155934] Epoch: [91]  [120/781]  eta: 0:01:56  lr: 0.000006  training_loss: 1.0922 (1.1174)  classification_loss: 1.0921 (1.1174)  loss_mask: 0.0000 (0.0000)  time: 0.1702  data: 0.0002  max mem: 6052
[06:31:37.560940] Epoch: [91]  [140/781]  eta: 0:01:52  lr: 0.000006  training_loss: 1.0965 (1.1143)  classification_loss: 1.0964 (1.1142)  loss_mask: 0.0000 (0.0000)  time: 0.1702  data: 0.0002  max mem: 6052
[06:31:40.989257] Epoch: [91]  [160/781]  eta: 0:01:48  lr: 0.000006  training_loss: 1.0963 (1.1121)  classification_loss: 1.0963 (1.1120)  loss_mask: 0.0000 (0.0000)  time: 0.1713  data: 0.0002  max mem: 6052
[06:31:44.415596] Epoch: [91]  [180/781]  eta: 0:01:44  lr: 0.000006  training_loss: 1.1223 (1.1142)  classification_loss: 1.1223 (1.1142)  loss_mask: 0.0000 (0.0000)  time: 0.1712  data: 0.0002  max mem: 6052
[06:31:47.839919] Epoch: [91]  [200/781]  eta: 0:01:41  lr: 0.000006  training_loss: 1.0967 (1.1142)  classification_loss: 1.0966 (1.1142)  loss_mask: 0.0000 (0.0000)  time: 0.1711  data: 0.0005  max mem: 6052
[06:31:51.334836] Epoch: [91]  [220/781]  eta: 0:01:37  lr: 0.000006  training_loss: 1.1552 (1.1159)  classification_loss: 1.1551 (1.1159)  loss_mask: 0.0000 (0.0000)  time: 0.1746  data: 0.0002  max mem: 6052
[06:31:54.759460] Epoch: [91]  [240/781]  eta: 0:01:34  lr: 0.000006  training_loss: 1.0767 (1.1160)  classification_loss: 1.0767 (1.1160)  loss_mask: 0.0000 (0.0000)  time: 0.1711  data: 0.0002  max mem: 6052
[06:31:58.170944] Epoch: [91]  [260/781]  eta: 0:01:30  lr: 0.000006  training_loss: 1.0744 (1.1162)  classification_loss: 1.0744 (1.1162)  loss_mask: 0.0000 (0.0000)  time: 0.1705  data: 0.0002  max mem: 6052
[06:32:01.594169] Epoch: [91]  [280/781]  eta: 0:01:26  lr: 0.000006  training_loss: 1.1136 (1.1164)  classification_loss: 1.1135 (1.1163)  loss_mask: 0.0000 (0.0002)  time: 0.1711  data: 0.0003  max mem: 6052
[06:32:05.021176] Epoch: [91]  [300/781]  eta: 0:01:23  lr: 0.000006  training_loss: 1.1536 (1.1190)  classification_loss: 1.1533 (1.1188)  loss_mask: 0.0001 (0.0002)  time: 0.1712  data: 0.0003  max mem: 6052
[06:32:08.447485] Epoch: [91]  [320/781]  eta: 0:01:19  lr: 0.000006  training_loss: 1.0534 (1.1172)  classification_loss: 1.0533 (1.1170)  loss_mask: 0.0001 (0.0002)  time: 0.1712  data: 0.0002  max mem: 6052
[06:32:11.868832] Epoch: [91]  [340/781]  eta: 0:01:16  lr: 0.000006  training_loss: 1.0854 (1.1166)  classification_loss: 1.0854 (1.1165)  loss_mask: 0.0000 (0.0001)  time: 0.1710  data: 0.0002  max mem: 6052
[06:32:15.280771] Epoch: [91]  [360/781]  eta: 0:01:12  lr: 0.000006  training_loss: 1.1164 (1.1172)  classification_loss: 1.1163 (1.1170)  loss_mask: 0.0000 (0.0001)  time: 0.1705  data: 0.0001  max mem: 6052
[06:32:18.704757] Epoch: [91]  [380/781]  eta: 0:01:09  lr: 0.000006  training_loss: 1.1856 (1.1195)  classification_loss: 1.1856 (1.1194)  loss_mask: 0.0000 (0.0001)  time: 0.1711  data: 0.0002  max mem: 6052
[06:32:22.127488] Epoch: [91]  [400/781]  eta: 0:01:05  lr: 0.000006  training_loss: 1.1358 (1.1200)  classification_loss: 1.1357 (1.1198)  loss_mask: 0.0000 (0.0001)  time: 0.1711  data: 0.0002  max mem: 6052
[06:32:25.537349] Epoch: [91]  [420/781]  eta: 0:01:02  lr: 0.000006  training_loss: 1.1569 (1.1208)  classification_loss: 1.1569 (1.1207)  loss_mask: 0.0000 (0.0001)  time: 0.1704  data: 0.0002  max mem: 6052
[06:32:28.954003] Epoch: [91]  [440/781]  eta: 0:00:58  lr: 0.000006  training_loss: 1.1200 (1.1198)  classification_loss: 1.1199 (1.1197)  loss_mask: 0.0000 (0.0001)  time: 0.1708  data: 0.0002  max mem: 6052
[06:32:32.366369] Epoch: [91]  [460/781]  eta: 0:00:55  lr: 0.000006  training_loss: 1.0821 (1.1185)  classification_loss: 1.0821 (1.1183)  loss_mask: 0.0000 (0.0001)  time: 0.1705  data: 0.0002  max mem: 6052
[06:32:35.778834] Epoch: [91]  [480/781]  eta: 0:00:51  lr: 0.000006  training_loss: 1.1343 (1.1193)  classification_loss: 1.1343 (1.1192)  loss_mask: 0.0000 (0.0001)  time: 0.1705  data: 0.0003  max mem: 6052
[06:32:39.189660] Epoch: [91]  [500/781]  eta: 0:00:48  lr: 0.000006  training_loss: 1.1057 (1.1192)  classification_loss: 1.1057 (1.1191)  loss_mask: 0.0000 (0.0001)  time: 0.1705  data: 0.0002  max mem: 6052
[06:32:42.623688] Epoch: [91]  [520/781]  eta: 0:00:44  lr: 0.000006  training_loss: 1.1047 (1.1190)  classification_loss: 1.1047 (1.1189)  loss_mask: 0.0000 (0.0001)  time: 0.1716  data: 0.0002  max mem: 6052
[06:32:46.047750] Epoch: [91]  [540/781]  eta: 0:00:41  lr: 0.000006  training_loss: 1.1482 (1.1195)  classification_loss: 1.1481 (1.1194)  loss_mask: 0.0000 (0.0001)  time: 0.1711  data: 0.0002  max mem: 6052
[06:32:49.452747] Epoch: [91]  [560/781]  eta: 0:00:38  lr: 0.000006  training_loss: 1.1157 (1.1195)  classification_loss: 1.1156 (1.1194)  loss_mask: 0.0000 (0.0001)  time: 0.1702  data: 0.0002  max mem: 6052
[06:32:52.875053] Epoch: [91]  [580/781]  eta: 0:00:34  lr: 0.000006  training_loss: 1.0688 (1.1188)  classification_loss: 1.0688 (1.1187)  loss_mask: 0.0000 (0.0001)  time: 0.1710  data: 0.0003  max mem: 6052
[06:32:56.313669] Epoch: [91]  [600/781]  eta: 0:00:31  lr: 0.000006  training_loss: 1.0953 (1.1177)  classification_loss: 1.0951 (1.1176)  loss_mask: 0.0000 (0.0001)  time: 0.1717  data: 0.0002  max mem: 6052
[06:32:59.739080] Epoch: [91]  [620/781]  eta: 0:00:27  lr: 0.000006  training_loss: 1.1196 (1.1178)  classification_loss: 1.1196 (1.1177)  loss_mask: 0.0000 (0.0001)  time: 0.1712  data: 0.0002  max mem: 6052
[06:33:03.160154] Epoch: [91]  [640/781]  eta: 0:00:24  lr: 0.000006  training_loss: 1.0576 (1.1173)  classification_loss: 1.0576 (1.1172)  loss_mask: 0.0000 (0.0001)  time: 0.1710  data: 0.0002  max mem: 6052
[06:33:06.612875] Epoch: [91]  [660/781]  eta: 0:00:20  lr: 0.000005  training_loss: 1.0813 (1.1178)  classification_loss: 1.0813 (1.1177)  loss_mask: 0.0000 (0.0001)  time: 0.1725  data: 0.0002  max mem: 6052
[06:33:10.045060] Epoch: [91]  [680/781]  eta: 0:00:17  lr: 0.000005  training_loss: 1.1075 (1.1182)  classification_loss: 1.1075 (1.1181)  loss_mask: 0.0000 (0.0001)  time: 0.1715  data: 0.0003  max mem: 6052
[06:33:13.502862] Epoch: [91]  [700/781]  eta: 0:00:13  lr: 0.000005  training_loss: 1.0998 (1.1176)  classification_loss: 1.0998 (1.1175)  loss_mask: 0.0000 (0.0001)  time: 0.1728  data: 0.0002  max mem: 6052
[06:33:16.985240] Epoch: [91]  [720/781]  eta: 0:00:10  lr: 0.000005  training_loss: 1.1097 (1.1175)  classification_loss: 1.1097 (1.1174)  loss_mask: 0.0000 (0.0001)  time: 0.1740  data: 0.0002  max mem: 6052
[06:33:20.394325] Epoch: [91]  [740/781]  eta: 0:00:07  lr: 0.000005  training_loss: 1.1194 (1.1175)  classification_loss: 1.1194 (1.1174)  loss_mask: 0.0000 (0.0001)  time: 0.1704  data: 0.0002  max mem: 6052
[06:33:23.816291] Epoch: [91]  [760/781]  eta: 0:00:03  lr: 0.000005  training_loss: 1.1321 (1.1186)  classification_loss: 1.1321 (1.1185)  loss_mask: 0.0000 (0.0001)  time: 0.1710  data: 0.0002  max mem: 6052
[06:33:27.304112] Epoch: [91]  [780/781]  eta: 0:00:00  lr: 0.000005  training_loss: 1.0811 (1.1185)  classification_loss: 1.0811 (1.1184)  loss_mask: 0.0000 (0.0001)  time: 0.1743  data: 0.0002  max mem: 6052
[06:33:27.465087] Epoch: [91] Total time: 0:02:14 (0.1724 s / it)
[06:33:27.465562] Averaged stats: lr: 0.000005  training_loss: 1.0811 (1.1185)  classification_loss: 1.0811 (1.1184)  loss_mask: 0.0000 (0.0001)
[06:33:28.149186] Test:  [  0/157]  eta: 0:01:46  testing_loss: 0.4507 (0.4507)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.6788  data: 0.6494  max mem: 6052
[06:33:28.432305] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4334 (0.4501)  acc1: 87.5000 (86.2216)  acc5: 100.0000 (99.5739)  time: 0.0873  data: 0.0592  max mem: 6052
[06:33:28.716431] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4169 (0.4251)  acc1: 87.5000 (87.6488)  acc5: 100.0000 (99.6280)  time: 0.0282  data: 0.0002  max mem: 6052
[06:33:28.999969] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4130 (0.4314)  acc1: 87.5000 (87.3992)  acc5: 100.0000 (99.5464)  time: 0.0282  data: 0.0001  max mem: 6052
[06:33:29.284833] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4566 (0.4379)  acc1: 87.5000 (87.2332)  acc5: 98.4375 (99.3902)  time: 0.0282  data: 0.0002  max mem: 6052
[06:33:29.569358] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4356 (0.4349)  acc1: 87.5000 (87.4694)  acc5: 98.4375 (99.3566)  time: 0.0283  data: 0.0002  max mem: 6052
[06:33:29.851209] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4127 (0.4340)  acc1: 87.5000 (87.3719)  acc5: 100.0000 (99.3340)  time: 0.0282  data: 0.0002  max mem: 6052
[06:33:30.132047] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4157 (0.4303)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (99.3398)  time: 0.0280  data: 0.0002  max mem: 6052
[06:33:30.414785] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4285 (0.4374)  acc1: 87.5000 (87.4228)  acc5: 100.0000 (99.3634)  time: 0.0280  data: 0.0002  max mem: 6052
[06:33:30.697536] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4357 (0.4351)  acc1: 89.0625 (87.5000)  acc5: 100.0000 (99.3647)  time: 0.0281  data: 0.0002  max mem: 6052
[06:33:30.978762] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4102 (0.4382)  acc1: 89.0625 (87.3762)  acc5: 100.0000 (99.3812)  time: 0.0281  data: 0.0002  max mem: 6052
[06:33:31.259156] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4282 (0.4373)  acc1: 87.5000 (87.4015)  acc5: 100.0000 (99.3525)  time: 0.0280  data: 0.0001  max mem: 6052
[06:33:31.539551] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.3947 (0.4372)  acc1: 87.5000 (87.4096)  acc5: 100.0000 (99.3414)  time: 0.0279  data: 0.0001  max mem: 6052
[06:33:31.821280] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.3818 (0.4374)  acc1: 87.5000 (87.4523)  acc5: 100.0000 (99.3440)  time: 0.0280  data: 0.0002  max mem: 6052
[06:33:32.101105] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3889 (0.4351)  acc1: 87.5000 (87.5776)  acc5: 100.0000 (99.3684)  time: 0.0280  data: 0.0001  max mem: 6052
[06:33:32.379620] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4236 (0.4348)  acc1: 87.5000 (87.5621)  acc5: 100.0000 (99.3998)  time: 0.0278  data: 0.0001  max mem: 6052
[06:33:32.528346] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4442 (0.4362)  acc1: 87.5000 (87.4500)  acc5: 100.0000 (99.4100)  time: 0.0268  data: 0.0001  max mem: 6052
[06:33:32.696483] Test: Total time: 0:00:05 (0.0333 s / it)
[06:33:32.696942] * Acc@1 87.450 Acc@5 99.410 loss 0.436
[06:33:32.697258] Accuracy of the network on the 10000 test images: 87.5%
[06:33:32.697451] Max accuracy: 87.45%
[06:33:32.923774] log_dir: ./output_dir
[06:33:33.723982] Epoch: [92]  [  0/781]  eta: 0:10:23  lr: 0.000005  training_loss: 0.9416 (0.9416)  classification_loss: 0.9416 (0.9416)  loss_mask: 0.0000 (0.0000)  time: 0.7987  data: 0.6213  max mem: 6052
[06:33:37.154015] Epoch: [92]  [ 20/781]  eta: 0:02:33  lr: 0.000005  training_loss: 1.0986 (1.0742)  classification_loss: 1.0986 (1.0742)  loss_mask: 0.0000 (0.0000)  time: 0.1714  data: 0.0001  max mem: 6052
[06:33:40.598358] Epoch: [92]  [ 40/781]  eta: 0:02:18  lr: 0.000005  training_loss: 1.1286 (1.0969)  classification_loss: 1.1286 (1.0969)  loss_mask: 0.0000 (0.0000)  time: 0.1721  data: 0.0002  max mem: 6052
[06:33:43.993191] Epoch: [92]  [ 60/781]  eta: 0:02:10  lr: 0.000005  training_loss: 1.0907 (1.1049)  classification_loss: 1.0907 (1.1048)  loss_mask: 0.0000 (0.0000)  time: 0.1697  data: 0.0001  max mem: 6052
[06:33:47.405759] Epoch: [92]  [ 80/781]  eta: 0:02:05  lr: 0.000005  training_loss: 1.1317 (1.1104)  classification_loss: 1.1317 (1.1103)  loss_mask: 0.0000 (0.0000)  time: 0.1706  data: 0.0003  max mem: 6052
[06:33:50.818905] Epoch: [92]  [100/781]  eta: 0:02:00  lr: 0.000005  training_loss: 1.1395 (1.1167)  classification_loss: 1.1394 (1.1167)  loss_mask: 0.0000 (0.0000)  time: 0.1706  data: 0.0002  max mem: 6052
[06:33:54.241473] Epoch: [92]  [120/781]  eta: 0:01:56  lr: 0.000005  training_loss: 1.1955 (1.1240)  classification_loss: 1.1955 (1.1239)  loss_mask: 0.0000 (0.0000)  time: 0.1711  data: 0.0002  max mem: 6052
[06:33:57.661393] Epoch: [92]  [140/781]  eta: 0:01:52  lr: 0.000005  training_loss: 1.1031 (1.1225)  classification_loss: 1.1031 (1.1225)  loss_mask: 0.0000 (0.0000)  time: 0.1709  data: 0.0002  max mem: 6052
[06:34:01.091687] Epoch: [92]  [160/781]  eta: 0:01:48  lr: 0.000005  training_loss: 1.1080 (1.1235)  classification_loss: 1.1079 (1.1234)  loss_mask: 0.0000 (0.0000)  time: 0.1714  data: 0.0002  max mem: 6052
[06:34:04.515778] Epoch: [92]  [180/781]  eta: 0:01:44  lr: 0.000005  training_loss: 1.1053 (1.1228)  classification_loss: 1.1052 (1.1228)  loss_mask: 0.0000 (0.0000)  time: 0.1711  data: 0.0004  max mem: 6052
[06:34:07.933374] Epoch: [92]  [200/781]  eta: 0:01:41  lr: 0.000005  training_loss: 1.1115 (1.1218)  classification_loss: 1.1115 (1.1217)  loss_mask: 0.0000 (0.0000)  time: 0.1708  data: 0.0002  max mem: 6052
[06:34:11.377756] Epoch: [92]  [220/781]  eta: 0:01:37  lr: 0.000005  training_loss: 1.1155 (1.1201)  classification_loss: 1.1155 (1.1200)  loss_mask: 0.0000 (0.0000)  time: 0.1722  data: 0.0002  max mem: 6052
[06:34:14.775910] Epoch: [92]  [240/781]  eta: 0:01:33  lr: 0.000005  training_loss: 1.0803 (1.1211)  classification_loss: 1.0803 (1.1211)  loss_mask: 0.0000 (0.0000)  time: 0.1698  data: 0.0002  max mem: 6052
[06:34:18.203691] Epoch: [92]  [260/781]  eta: 0:01:30  lr: 0.000005  training_loss: 1.1205 (1.1205)  classification_loss: 1.1204 (1.1204)  loss_mask: 0.0000 (0.0000)  time: 0.1713  data: 0.0002  max mem: 6052
[06:34:21.645987] Epoch: [92]  [280/781]  eta: 0:01:26  lr: 0.000005  training_loss: 1.1134 (1.1206)  classification_loss: 1.1133 (1.1205)  loss_mask: 0.0000 (0.0000)  time: 0.1720  data: 0.0002  max mem: 6052
[06:34:25.064116] Epoch: [92]  [300/781]  eta: 0:01:23  lr: 0.000005  training_loss: 1.0805 (1.1212)  classification_loss: 1.0804 (1.1211)  loss_mask: 0.0000 (0.0000)  time: 0.1708  data: 0.0004  max mem: 6052
[06:34:28.473021] Epoch: [92]  [320/781]  eta: 0:01:19  lr: 0.000005  training_loss: 1.1101 (1.1198)  classification_loss: 1.1101 (1.1197)  loss_mask: 0.0000 (0.0000)  time: 0.1704  data: 0.0002  max mem: 6052
[06:34:31.897072] Epoch: [92]  [340/781]  eta: 0:01:16  lr: 0.000005  training_loss: 1.1055 (1.1186)  classification_loss: 1.1055 (1.1186)  loss_mask: 0.0000 (0.0000)  time: 0.1711  data: 0.0002  max mem: 6052
[06:34:35.315985] Epoch: [92]  [360/781]  eta: 0:01:12  lr: 0.000005  training_loss: 1.1165 (1.1182)  classification_loss: 1.1165 (1.1182)  loss_mask: 0.0000 (0.0000)  time: 0.1709  data: 0.0003  max mem: 6052
[06:34:38.767510] Epoch: [92]  [380/781]  eta: 0:01:09  lr: 0.000005  training_loss: 1.1439 (1.1190)  classification_loss: 1.1439 (1.1190)  loss_mask: 0.0000 (0.0000)  time: 0.1725  data: 0.0002  max mem: 6052
[06:34:42.167832] Epoch: [92]  [400/781]  eta: 0:01:05  lr: 0.000005  training_loss: 1.1404 (1.1199)  classification_loss: 1.1404 (1.1198)  loss_mask: 0.0000 (0.0000)  time: 0.1699  data: 0.0002  max mem: 6052
[06:34:45.559399] Epoch: [92]  [420/781]  eta: 0:01:02  lr: 0.000005  training_loss: 1.0957 (1.1202)  classification_loss: 1.0957 (1.1201)  loss_mask: 0.0000 (0.0000)  time: 0.1695  data: 0.0002  max mem: 6052
[06:34:48.947849] Epoch: [92]  [440/781]  eta: 0:00:58  lr: 0.000005  training_loss: 1.0372 (1.1180)  classification_loss: 1.0371 (1.1179)  loss_mask: 0.0000 (0.0000)  time: 0.1693  data: 0.0001  max mem: 6052
[06:34:52.343785] Epoch: [92]  [460/781]  eta: 0:00:55  lr: 0.000005  training_loss: 1.1184 (1.1177)  classification_loss: 1.1184 (1.1177)  loss_mask: 0.0000 (0.0000)  time: 0.1697  data: 0.0002  max mem: 6052
[06:34:55.732154] Epoch: [92]  [480/781]  eta: 0:00:51  lr: 0.000005  training_loss: 1.1675 (1.1196)  classification_loss: 1.1675 (1.1196)  loss_mask: 0.0000 (0.0000)  time: 0.1694  data: 0.0002  max mem: 6052
[06:34:59.156370] Epoch: [92]  [500/781]  eta: 0:00:48  lr: 0.000005  training_loss: 1.1245 (1.1202)  classification_loss: 1.1244 (1.1201)  loss_mask: 0.0000 (0.0000)  time: 0.1711  data: 0.0003  max mem: 6052
[06:35:02.582857] Epoch: [92]  [520/781]  eta: 0:00:44  lr: 0.000005  training_loss: 1.1243 (1.1206)  classification_loss: 1.1242 (1.1206)  loss_mask: 0.0000 (0.0000)  time: 0.1712  data: 0.0001  max mem: 6052
[06:35:06.008755] Epoch: [92]  [540/781]  eta: 0:00:41  lr: 0.000005  training_loss: 1.1719 (1.1225)  classification_loss: 1.1718 (1.1225)  loss_mask: 0.0000 (0.0000)  time: 0.1712  data: 0.0002  max mem: 6052
[06:35:09.521200] Epoch: [92]  [560/781]  eta: 0:00:38  lr: 0.000005  training_loss: 1.1008 (1.1213)  classification_loss: 1.1007 (1.1213)  loss_mask: 0.0000 (0.0000)  time: 0.1756  data: 0.0003  max mem: 6052
[06:35:12.933133] Epoch: [92]  [580/781]  eta: 0:00:34  lr: 0.000005  training_loss: 1.1143 (1.1213)  classification_loss: 1.1143 (1.1213)  loss_mask: 0.0000 (0.0000)  time: 0.1705  data: 0.0003  max mem: 6052
[06:35:16.336120] Epoch: [92]  [600/781]  eta: 0:00:31  lr: 0.000005  training_loss: 1.1165 (1.1216)  classification_loss: 1.1165 (1.1216)  loss_mask: 0.0000 (0.0000)  time: 0.1701  data: 0.0002  max mem: 6052
[06:35:19.730108] Epoch: [92]  [620/781]  eta: 0:00:27  lr: 0.000005  training_loss: 1.1045 (1.1213)  classification_loss: 1.1045 (1.1213)  loss_mask: 0.0000 (0.0000)  time: 0.1696  data: 0.0002  max mem: 6052
[06:35:23.134588] Epoch: [92]  [640/781]  eta: 0:00:24  lr: 0.000004  training_loss: 1.1095 (1.1209)  classification_loss: 1.1095 (1.1207)  loss_mask: 0.0000 (0.0002)  time: 0.1702  data: 0.0002  max mem: 6052
[06:35:26.553462] Epoch: [92]  [660/781]  eta: 0:00:20  lr: 0.000004  training_loss: 1.1509 (1.1215)  classification_loss: 1.1508 (1.1213)  loss_mask: 0.0000 (0.0002)  time: 0.1708  data: 0.0003  max mem: 6052
[06:35:30.005380] Epoch: [92]  [680/781]  eta: 0:00:17  lr: 0.000004  training_loss: 1.1852 (1.1228)  classification_loss: 1.1852 (1.1226)  loss_mask: 0.0000 (0.0002)  time: 0.1725  data: 0.0002  max mem: 6052
[06:35:33.425753] Epoch: [92]  [700/781]  eta: 0:00:13  lr: 0.000004  training_loss: 1.1174 (1.1224)  classification_loss: 1.1173 (1.1223)  loss_mask: 0.0000 (0.0002)  time: 0.1709  data: 0.0002  max mem: 6052
[06:35:36.869696] Epoch: [92]  [720/781]  eta: 0:00:10  lr: 0.000004  training_loss: 1.1126 (1.1222)  classification_loss: 1.1126 (1.1220)  loss_mask: 0.0000 (0.0002)  time: 0.1720  data: 0.0002  max mem: 6052
[06:35:40.307806] Epoch: [92]  [740/781]  eta: 0:00:07  lr: 0.000004  training_loss: 1.0838 (1.1214)  classification_loss: 1.0837 (1.1212)  loss_mask: 0.0000 (0.0002)  time: 0.1718  data: 0.0002  max mem: 6052
[06:35:43.745690] Epoch: [92]  [760/781]  eta: 0:00:03  lr: 0.000004  training_loss: 1.1496 (1.1222)  classification_loss: 1.1495 (1.1220)  loss_mask: 0.0000 (0.0002)  time: 0.1718  data: 0.0002  max mem: 6052
[06:35:47.153044] Epoch: [92]  [780/781]  eta: 0:00:00  lr: 0.000004  training_loss: 1.1466 (1.1225)  classification_loss: 1.1466 (1.1223)  loss_mask: 0.0000 (0.0002)  time: 0.1703  data: 0.0002  max mem: 6052
[06:35:47.311494] Epoch: [92] Total time: 0:02:14 (0.1721 s / it)
[06:35:47.311952] Averaged stats: lr: 0.000004  training_loss: 1.1466 (1.1225)  classification_loss: 1.1466 (1.1223)  loss_mask: 0.0000 (0.0002)
[06:35:47.972721] Test:  [  0/157]  eta: 0:01:43  testing_loss: 0.4366 (0.4366)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.6565  data: 0.6262  max mem: 6052
[06:35:48.262206] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4165 (0.4429)  acc1: 89.0625 (87.0739)  acc5: 100.0000 (99.7159)  time: 0.0858  data: 0.0573  max mem: 6052
[06:35:48.547629] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4107 (0.4228)  acc1: 89.0625 (87.7232)  acc5: 100.0000 (99.6280)  time: 0.0286  data: 0.0003  max mem: 6052
[06:35:48.830862] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4107 (0.4302)  acc1: 87.5000 (87.4496)  acc5: 100.0000 (99.5464)  time: 0.0283  data: 0.0001  max mem: 6052
[06:35:49.113344] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4441 (0.4366)  acc1: 87.5000 (87.3095)  acc5: 98.4375 (99.3902)  time: 0.0282  data: 0.0001  max mem: 6052
[06:35:49.399182] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4332 (0.4335)  acc1: 87.5000 (87.5306)  acc5: 100.0000 (99.3873)  time: 0.0283  data: 0.0002  max mem: 6052
[06:35:49.681918] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4107 (0.4325)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (99.3596)  time: 0.0283  data: 0.0002  max mem: 6052
[06:35:49.968979] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4122 (0.4287)  acc1: 87.5000 (87.6320)  acc5: 100.0000 (99.3838)  time: 0.0284  data: 0.0002  max mem: 6052
[06:35:50.256791] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4355 (0.4356)  acc1: 85.9375 (87.4035)  acc5: 100.0000 (99.4020)  time: 0.0286  data: 0.0002  max mem: 6052
[06:35:50.540818] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4400 (0.4331)  acc1: 87.5000 (87.4828)  acc5: 100.0000 (99.3990)  time: 0.0284  data: 0.0003  max mem: 6052
[06:35:50.828757] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4175 (0.4364)  acc1: 89.0625 (87.3917)  acc5: 100.0000 (99.4121)  time: 0.0284  data: 0.0003  max mem: 6052
[06:35:51.112885] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4300 (0.4356)  acc1: 87.5000 (87.4437)  acc5: 100.0000 (99.3806)  time: 0.0284  data: 0.0002  max mem: 6052
[06:35:51.394968] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4128 (0.4356)  acc1: 87.5000 (87.4225)  acc5: 100.0000 (99.3673)  time: 0.0282  data: 0.0002  max mem: 6052
[06:35:51.676761] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.3768 (0.4357)  acc1: 85.9375 (87.5000)  acc5: 100.0000 (99.3678)  time: 0.0280  data: 0.0002  max mem: 6052
[06:35:51.959765] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3897 (0.4331)  acc1: 89.0625 (87.5997)  acc5: 100.0000 (99.3905)  time: 0.0281  data: 0.0002  max mem: 6052
[06:35:52.239091] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4135 (0.4329)  acc1: 85.9375 (87.5310)  acc5: 100.0000 (99.4102)  time: 0.0280  data: 0.0001  max mem: 6052
[06:35:52.389210] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4342 (0.4344)  acc1: 85.9375 (87.4300)  acc5: 100.0000 (99.4100)  time: 0.0269  data: 0.0001  max mem: 6052
[06:35:52.539198] Test: Total time: 0:00:05 (0.0333 s / it)
[06:35:52.539635] * Acc@1 87.430 Acc@5 99.410 loss 0.434
[06:35:52.539998] Accuracy of the network on the 10000 test images: 87.4%
[06:35:52.540224] Max accuracy: 87.45%
[06:35:52.739746] log_dir: ./output_dir
[06:35:53.598885] Epoch: [93]  [  0/781]  eta: 0:11:09  lr: 0.000004  training_loss: 0.9551 (0.9551)  classification_loss: 0.9550 (0.9550)  loss_mask: 0.0000 (0.0000)  time: 0.8573  data: 0.6617  max mem: 6052
[06:35:57.019037] Epoch: [93]  [ 20/781]  eta: 0:02:34  lr: 0.000004  training_loss: 1.0417 (1.0607)  classification_loss: 1.0417 (1.0607)  loss_mask: 0.0000 (0.0000)  time: 0.1709  data: 0.0002  max mem: 6052
[06:36:00.411546] Epoch: [93]  [ 40/781]  eta: 0:02:18  lr: 0.000004  training_loss: 1.0806 (1.0950)  classification_loss: 1.0805 (1.0950)  loss_mask: 0.0000 (0.0000)  time: 0.1695  data: 0.0002  max mem: 6052
[06:36:03.817106] Epoch: [93]  [ 60/781]  eta: 0:02:10  lr: 0.000004  training_loss: 1.1200 (1.1010)  classification_loss: 1.1200 (1.1010)  loss_mask: 0.0000 (0.0000)  time: 0.1702  data: 0.0001  max mem: 6052
[06:36:07.229012] Epoch: [93]  [ 80/781]  eta: 0:02:05  lr: 0.000004  training_loss: 1.1053 (1.0995)  classification_loss: 1.1053 (1.0994)  loss_mask: 0.0000 (0.0000)  time: 0.1705  data: 0.0003  max mem: 6052
[06:36:10.643620] Epoch: [93]  [100/781]  eta: 0:02:00  lr: 0.000004  training_loss: 1.1280 (1.1056)  classification_loss: 1.1279 (1.1055)  loss_mask: 0.0000 (0.0000)  time: 0.1706  data: 0.0002  max mem: 6052
[06:36:14.056464] Epoch: [93]  [120/781]  eta: 0:01:56  lr: 0.000004  training_loss: 1.1008 (1.1035)  classification_loss: 1.1008 (1.1035)  loss_mask: 0.0000 (0.0000)  time: 0.1705  data: 0.0003  max mem: 6052
[06:36:17.455141] Epoch: [93]  [140/781]  eta: 0:01:52  lr: 0.000004  training_loss: 1.0649 (1.1015)  classification_loss: 1.0648 (1.1015)  loss_mask: 0.0000 (0.0000)  time: 0.1699  data: 0.0002  max mem: 6052
[06:36:20.844255] Epoch: [93]  [160/781]  eta: 0:01:48  lr: 0.000004  training_loss: 1.1057 (1.0998)  classification_loss: 1.1057 (1.0997)  loss_mask: 0.0000 (0.0000)  time: 0.1694  data: 0.0002  max mem: 6052
[06:36:24.244186] Epoch: [93]  [180/781]  eta: 0:01:44  lr: 0.000004  training_loss: 1.0853 (1.0989)  classification_loss: 1.0853 (1.0988)  loss_mask: 0.0000 (0.0000)  time: 0.1699  data: 0.0002  max mem: 6052
[06:36:27.658755] Epoch: [93]  [200/781]  eta: 0:01:40  lr: 0.000004  training_loss: 1.0630 (1.0989)  classification_loss: 1.0630 (1.0989)  loss_mask: 0.0000 (0.0000)  time: 0.1707  data: 0.0002  max mem: 6052
[06:36:31.074513] Epoch: [93]  [220/781]  eta: 0:01:37  lr: 0.000004  training_loss: 1.0988 (1.0979)  classification_loss: 1.0988 (1.0979)  loss_mask: 0.0000 (0.0000)  time: 0.1707  data: 0.0002  max mem: 6052
[06:36:34.474041] Epoch: [93]  [240/781]  eta: 0:01:33  lr: 0.000004  training_loss: 1.0965 (1.0984)  classification_loss: 1.0964 (1.0983)  loss_mask: 0.0000 (0.0000)  time: 0.1699  data: 0.0002  max mem: 6052
[06:36:37.914734] Epoch: [93]  [260/781]  eta: 0:01:30  lr: 0.000004  training_loss: 1.0929 (1.1004)  classification_loss: 1.0928 (1.1004)  loss_mask: 0.0000 (0.0000)  time: 0.1719  data: 0.0003  max mem: 6052
[06:36:41.332901] Epoch: [93]  [280/781]  eta: 0:01:26  lr: 0.000004  training_loss: 1.0808 (1.1011)  classification_loss: 1.0807 (1.1011)  loss_mask: 0.0000 (0.0000)  time: 0.1708  data: 0.0002  max mem: 6052
[06:36:44.763535] Epoch: [93]  [300/781]  eta: 0:01:23  lr: 0.000004  training_loss: 1.0989 (1.1020)  classification_loss: 1.0989 (1.1019)  loss_mask: 0.0000 (0.0000)  time: 0.1714  data: 0.0002  max mem: 6052
[06:36:48.190048] Epoch: [93]  [320/781]  eta: 0:01:19  lr: 0.000004  training_loss: 1.0768 (1.1032)  classification_loss: 1.0767 (1.1032)  loss_mask: 0.0000 (0.0000)  time: 0.1712  data: 0.0002  max mem: 6052
[06:36:51.637267] Epoch: [93]  [340/781]  eta: 0:01:16  lr: 0.000004  training_loss: 1.0702 (1.1015)  classification_loss: 1.0702 (1.1015)  loss_mask: 0.0000 (0.0000)  time: 0.1723  data: 0.0002  max mem: 6052
[06:36:55.051608] Epoch: [93]  [360/781]  eta: 0:01:12  lr: 0.000004  training_loss: 1.1314 (1.1035)  classification_loss: 1.1313 (1.1034)  loss_mask: 0.0000 (0.0000)  time: 0.1706  data: 0.0003  max mem: 6052
[06:36:58.462823] Epoch: [93]  [380/781]  eta: 0:01:09  lr: 0.000004  training_loss: 1.1034 (1.1036)  classification_loss: 1.1034 (1.1036)  loss_mask: 0.0000 (0.0000)  time: 0.1705  data: 0.0003  max mem: 6052
[06:37:01.876009] Epoch: [93]  [400/781]  eta: 0:01:05  lr: 0.000004  training_loss: 1.1442 (1.1046)  classification_loss: 1.1442 (1.1046)  loss_mask: 0.0000 (0.0000)  time: 0.1706  data: 0.0002  max mem: 6052
[06:37:05.307032] Epoch: [93]  [420/781]  eta: 0:01:02  lr: 0.000004  training_loss: 1.1034 (1.1047)  classification_loss: 1.1034 (1.1047)  loss_mask: 0.0000 (0.0000)  time: 0.1715  data: 0.0002  max mem: 6052
[06:37:08.713037] Epoch: [93]  [440/781]  eta: 0:00:58  lr: 0.000004  training_loss: 1.1610 (1.1063)  classification_loss: 1.1610 (1.1063)  loss_mask: 0.0000 (0.0000)  time: 0.1702  data: 0.0002  max mem: 6052
[06:37:12.127869] Epoch: [93]  [460/781]  eta: 0:00:55  lr: 0.000004  training_loss: 1.0440 (1.1048)  classification_loss: 1.0439 (1.1048)  loss_mask: 0.0000 (0.0000)  time: 0.1707  data: 0.0002  max mem: 6052
[06:37:15.524432] Epoch: [93]  [480/781]  eta: 0:00:51  lr: 0.000004  training_loss: 1.1630 (1.1073)  classification_loss: 1.1629 (1.1072)  loss_mask: 0.0000 (0.0000)  time: 0.1698  data: 0.0002  max mem: 6052
[06:37:18.910996] Epoch: [93]  [500/781]  eta: 0:00:48  lr: 0.000004  training_loss: 1.1308 (1.1081)  classification_loss: 1.1308 (1.1080)  loss_mask: 0.0000 (0.0000)  time: 0.1692  data: 0.0002  max mem: 6052
[06:37:22.319830] Epoch: [93]  [520/781]  eta: 0:00:44  lr: 0.000004  training_loss: 1.1187 (1.1081)  classification_loss: 1.1187 (1.1081)  loss_mask: 0.0000 (0.0000)  time: 0.1704  data: 0.0002  max mem: 6052
[06:37:25.729554] Epoch: [93]  [540/781]  eta: 0:00:41  lr: 0.000004  training_loss: 1.1484 (1.1090)  classification_loss: 1.1484 (1.1090)  loss_mask: 0.0000 (0.0000)  time: 0.1704  data: 0.0002  max mem: 6052
[06:37:29.145611] Epoch: [93]  [560/781]  eta: 0:00:37  lr: 0.000004  training_loss: 1.1325 (1.1100)  classification_loss: 1.1325 (1.1100)  loss_mask: 0.0000 (0.0000)  time: 0.1707  data: 0.0002  max mem: 6052
[06:37:32.566665] Epoch: [93]  [580/781]  eta: 0:00:34  lr: 0.000004  training_loss: 1.0629 (1.1088)  classification_loss: 1.0629 (1.1088)  loss_mask: 0.0000 (0.0000)  time: 0.1710  data: 0.0003  max mem: 6052
[06:37:36.015045] Epoch: [93]  [600/781]  eta: 0:00:31  lr: 0.000004  training_loss: 1.0657 (1.1081)  classification_loss: 1.0656 (1.1081)  loss_mask: 0.0000 (0.0000)  time: 0.1723  data: 0.0002  max mem: 6052
[06:37:39.438952] Epoch: [93]  [620/781]  eta: 0:00:27  lr: 0.000004  training_loss: 1.1204 (1.1094)  classification_loss: 1.1203 (1.1093)  loss_mask: 0.0000 (0.0000)  time: 0.1711  data: 0.0001  max mem: 6052
[06:37:42.894179] Epoch: [93]  [640/781]  eta: 0:00:24  lr: 0.000004  training_loss: 1.1545 (1.1106)  classification_loss: 1.1544 (1.1106)  loss_mask: 0.0000 (0.0000)  time: 0.1727  data: 0.0002  max mem: 6052
[06:37:46.314112] Epoch: [93]  [660/781]  eta: 0:00:20  lr: 0.000004  training_loss: 1.0921 (1.1106)  classification_loss: 1.0921 (1.1105)  loss_mask: 0.0000 (0.0000)  time: 0.1709  data: 0.0002  max mem: 6052
[06:37:49.714852] Epoch: [93]  [680/781]  eta: 0:00:17  lr: 0.000004  training_loss: 1.0951 (1.1113)  classification_loss: 1.0951 (1.1113)  loss_mask: 0.0000 (0.0000)  time: 0.1700  data: 0.0002  max mem: 6052
[06:37:53.103866] Epoch: [93]  [700/781]  eta: 0:00:13  lr: 0.000004  training_loss: 1.0686 (1.1110)  classification_loss: 1.0685 (1.1110)  loss_mask: 0.0000 (0.0000)  time: 0.1693  data: 0.0001  max mem: 6052
[06:37:56.538508] Epoch: [93]  [720/781]  eta: 0:00:10  lr: 0.000004  training_loss: 1.1794 (1.1121)  classification_loss: 1.1794 (1.1121)  loss_mask: 0.0000 (0.0000)  time: 0.1716  data: 0.0002  max mem: 6052
[06:37:59.944841] Epoch: [93]  [740/781]  eta: 0:00:07  lr: 0.000003  training_loss: 1.1090 (1.1119)  classification_loss: 1.1090 (1.1119)  loss_mask: 0.0000 (0.0000)  time: 0.1702  data: 0.0002  max mem: 6052
[06:38:03.409393] Epoch: [93]  [760/781]  eta: 0:00:03  lr: 0.000003  training_loss: 1.1157 (1.1122)  classification_loss: 1.1157 (1.1122)  loss_mask: 0.0000 (0.0000)  time: 0.1732  data: 0.0007  max mem: 6052
[06:38:06.817414] Epoch: [93]  [780/781]  eta: 0:00:00  lr: 0.000003  training_loss: 1.1012 (1.1118)  classification_loss: 1.1012 (1.1118)  loss_mask: 0.0000 (0.0000)  time: 0.1703  data: 0.0002  max mem: 6052
[06:38:06.995186] Epoch: [93] Total time: 0:02:14 (0.1719 s / it)
[06:38:06.995640] Averaged stats: lr: 0.000003  training_loss: 1.1012 (1.1118)  classification_loss: 1.1012 (1.1118)  loss_mask: 0.0000 (0.0000)
[06:38:07.658041] Test:  [  0/157]  eta: 0:01:43  testing_loss: 0.4368 (0.4368)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.6578  data: 0.6202  max mem: 6052
[06:38:07.945400] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4183 (0.4440)  acc1: 89.0625 (86.9318)  acc5: 100.0000 (99.5739)  time: 0.0857  data: 0.0566  max mem: 6052
[06:38:08.236194] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4151 (0.4229)  acc1: 89.0625 (87.7976)  acc5: 100.0000 (99.6280)  time: 0.0287  data: 0.0003  max mem: 6052
[06:38:08.517845] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4151 (0.4308)  acc1: 87.5000 (87.5504)  acc5: 100.0000 (99.5464)  time: 0.0285  data: 0.0003  max mem: 6052
[06:38:08.800066] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4430 (0.4374)  acc1: 87.5000 (87.4619)  acc5: 98.4375 (99.3902)  time: 0.0280  data: 0.0002  max mem: 6052
[06:38:09.080973] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4273 (0.4345)  acc1: 87.5000 (87.6225)  acc5: 100.0000 (99.4179)  time: 0.0280  data: 0.0001  max mem: 6052
[06:38:09.360971] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4165 (0.4339)  acc1: 89.0625 (87.5256)  acc5: 100.0000 (99.3596)  time: 0.0279  data: 0.0001  max mem: 6052
[06:38:09.640997] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4165 (0.4305)  acc1: 85.9375 (87.6100)  acc5: 100.0000 (99.3838)  time: 0.0279  data: 0.0001  max mem: 6052
[06:38:09.920849] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4444 (0.4377)  acc1: 85.9375 (87.4228)  acc5: 100.0000 (99.4020)  time: 0.0279  data: 0.0001  max mem: 6052
[06:38:10.201900] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4409 (0.4350)  acc1: 87.5000 (87.4828)  acc5: 100.0000 (99.3990)  time: 0.0279  data: 0.0001  max mem: 6052
[06:38:10.483581] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4138 (0.4376)  acc1: 87.5000 (87.3453)  acc5: 100.0000 (99.3967)  time: 0.0280  data: 0.0001  max mem: 6052
[06:38:10.770208] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4260 (0.4362)  acc1: 87.5000 (87.4859)  acc5: 100.0000 (99.3947)  time: 0.0283  data: 0.0002  max mem: 6052
[06:38:11.050440] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4040 (0.4364)  acc1: 89.0625 (87.4871)  acc5: 100.0000 (99.3802)  time: 0.0282  data: 0.0001  max mem: 6052
[06:38:11.331408] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.3849 (0.4365)  acc1: 87.5000 (87.5239)  acc5: 100.0000 (99.3917)  time: 0.0279  data: 0.0001  max mem: 6052
[06:38:11.610704] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3925 (0.4341)  acc1: 87.5000 (87.6551)  acc5: 100.0000 (99.4127)  time: 0.0279  data: 0.0001  max mem: 6052
[06:38:11.888299] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4211 (0.4339)  acc1: 87.5000 (87.6138)  acc5: 100.0000 (99.4205)  time: 0.0277  data: 0.0001  max mem: 6052
[06:38:12.038085] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4475 (0.4356)  acc1: 87.5000 (87.5300)  acc5: 100.0000 (99.4200)  time: 0.0268  data: 0.0001  max mem: 6052
[06:38:12.203023] Test: Total time: 0:00:05 (0.0331 s / it)
[06:38:12.203760] * Acc@1 87.530 Acc@5 99.420 loss 0.436
[06:38:12.204078] Accuracy of the network on the 10000 test images: 87.5%
[06:38:12.204275] Max accuracy: 87.53%
[06:38:12.441805] log_dir: ./output_dir
[06:38:13.345008] Epoch: [94]  [  0/781]  eta: 0:11:43  lr: 0.000003  training_loss: 1.0938 (1.0938)  classification_loss: 1.0938 (1.0938)  loss_mask: 0.0000 (0.0000)  time: 0.9012  data: 0.7148  max mem: 6052
[06:38:16.761605] Epoch: [94]  [ 20/781]  eta: 0:02:36  lr: 0.000003  training_loss: 1.0441 (1.0861)  classification_loss: 1.0441 (1.0861)  loss_mask: 0.0000 (0.0000)  time: 0.1707  data: 0.0003  max mem: 6052
[06:38:20.203184] Epoch: [94]  [ 40/781]  eta: 0:02:20  lr: 0.000003  training_loss: 1.1037 (1.1042)  classification_loss: 1.1036 (1.1041)  loss_mask: 0.0000 (0.0000)  time: 0.1720  data: 0.0003  max mem: 6052
[06:38:23.613317] Epoch: [94]  [ 60/781]  eta: 0:02:11  lr: 0.000003  training_loss: 1.0923 (1.1109)  classification_loss: 1.0922 (1.1108)  loss_mask: 0.0000 (0.0000)  time: 0.1704  data: 0.0003  max mem: 6052
[06:38:27.033469] Epoch: [94]  [ 80/781]  eta: 0:02:06  lr: 0.000003  training_loss: 1.1123 (1.1046)  classification_loss: 1.1123 (1.1046)  loss_mask: 0.0000 (0.0000)  time: 0.1709  data: 0.0002  max mem: 6052
[06:38:30.443753] Epoch: [94]  [100/781]  eta: 0:02:01  lr: 0.000003  training_loss: 1.1122 (1.1067)  classification_loss: 1.1121 (1.1067)  loss_mask: 0.0000 (0.0000)  time: 0.1704  data: 0.0003  max mem: 6052
[06:38:33.866784] Epoch: [94]  [120/781]  eta: 0:01:56  lr: 0.000003  training_loss: 1.0891 (1.1065)  classification_loss: 1.0890 (1.1064)  loss_mask: 0.0000 (0.0000)  time: 0.1711  data: 0.0002  max mem: 6052
[06:38:37.322915] Epoch: [94]  [140/781]  eta: 0:01:53  lr: 0.000003  training_loss: 1.0965 (1.1069)  classification_loss: 1.0965 (1.1068)  loss_mask: 0.0000 (0.0000)  time: 0.1727  data: 0.0002  max mem: 6052
[06:38:40.733033] Epoch: [94]  [160/781]  eta: 0:01:49  lr: 0.000003  training_loss: 1.0546 (1.1006)  classification_loss: 1.0545 (1.1006)  loss_mask: 0.0000 (0.0000)  time: 0.1704  data: 0.0003  max mem: 6052
[06:38:44.135990] Epoch: [94]  [180/781]  eta: 0:01:45  lr: 0.000003  training_loss: 1.1410 (1.1054)  classification_loss: 1.1410 (1.1053)  loss_mask: 0.0000 (0.0000)  time: 0.1701  data: 0.0003  max mem: 6052
[06:38:47.537901] Epoch: [94]  [200/781]  eta: 0:01:41  lr: 0.000003  training_loss: 1.0822 (1.1043)  classification_loss: 1.0821 (1.1043)  loss_mask: 0.0000 (0.0000)  time: 0.1700  data: 0.0003  max mem: 6052
[06:38:50.934397] Epoch: [94]  [220/781]  eta: 0:01:37  lr: 0.000003  training_loss: 1.1300 (1.1098)  classification_loss: 1.1300 (1.1098)  loss_mask: 0.0000 (0.0000)  time: 0.1698  data: 0.0002  max mem: 6052
[06:38:54.389915] Epoch: [94]  [240/781]  eta: 0:01:34  lr: 0.000003  training_loss: 1.1067 (1.1121)  classification_loss: 1.1066 (1.1121)  loss_mask: 0.0000 (0.0000)  time: 0.1727  data: 0.0008  max mem: 6052
[06:38:57.800001] Epoch: [94]  [260/781]  eta: 0:01:30  lr: 0.000003  training_loss: 1.1331 (1.1134)  classification_loss: 1.1331 (1.1133)  loss_mask: 0.0000 (0.0000)  time: 0.1704  data: 0.0002  max mem: 6052
[06:39:01.207746] Epoch: [94]  [280/781]  eta: 0:01:26  lr: 0.000003  training_loss: 1.0739 (1.1118)  classification_loss: 1.0739 (1.1117)  loss_mask: 0.0000 (0.0000)  time: 0.1703  data: 0.0001  max mem: 6052
[06:39:04.629078] Epoch: [94]  [300/781]  eta: 0:01:23  lr: 0.000003  training_loss: 1.0879 (1.1107)  classification_loss: 1.0879 (1.1107)  loss_mask: 0.0000 (0.0000)  time: 0.1710  data: 0.0002  max mem: 6052
[06:39:08.042112] Epoch: [94]  [320/781]  eta: 0:01:19  lr: 0.000003  training_loss: 1.1093 (1.1115)  classification_loss: 1.1093 (1.1115)  loss_mask: 0.0000 (0.0000)  time: 0.1706  data: 0.0002  max mem: 6052
[06:39:11.529205] Epoch: [94]  [340/781]  eta: 0:01:16  lr: 0.000003  training_loss: 1.0351 (1.1078)  classification_loss: 1.0351 (1.1078)  loss_mask: 0.0000 (0.0000)  time: 0.1743  data: 0.0002  max mem: 6052
[06:39:14.922430] Epoch: [94]  [360/781]  eta: 0:01:12  lr: 0.000003  training_loss: 1.0994 (1.1070)  classification_loss: 1.0994 (1.1069)  loss_mask: 0.0000 (0.0000)  time: 0.1696  data: 0.0002  max mem: 6052
[06:39:18.365704] Epoch: [94]  [380/781]  eta: 0:01:09  lr: 0.000003  training_loss: 1.1048 (1.1073)  classification_loss: 1.1048 (1.1073)  loss_mask: 0.0000 (0.0000)  time: 0.1721  data: 0.0002  max mem: 6052
[06:39:21.804538] Epoch: [94]  [400/781]  eta: 0:01:05  lr: 0.000003  training_loss: 1.0694 (1.1071)  classification_loss: 1.0693 (1.1070)  loss_mask: 0.0000 (0.0000)  time: 0.1719  data: 0.0002  max mem: 6052
[06:39:25.226624] Epoch: [94]  [420/781]  eta: 0:01:02  lr: 0.000003  training_loss: 1.0780 (1.1063)  classification_loss: 1.0779 (1.1063)  loss_mask: 0.0000 (0.0000)  time: 0.1710  data: 0.0003  max mem: 6052
[06:39:28.665647] Epoch: [94]  [440/781]  eta: 0:00:58  lr: 0.000003  training_loss: 1.1031 (1.1068)  classification_loss: 1.1030 (1.1068)  loss_mask: 0.0000 (0.0000)  time: 0.1719  data: 0.0002  max mem: 6052
[06:39:32.075873] Epoch: [94]  [460/781]  eta: 0:00:55  lr: 0.000003  training_loss: 1.0579 (1.1057)  classification_loss: 1.0578 (1.1057)  loss_mask: 0.0000 (0.0000)  time: 0.1704  data: 0.0002  max mem: 6052
[06:39:35.488753] Epoch: [94]  [480/781]  eta: 0:00:51  lr: 0.000003  training_loss: 1.0920 (1.1064)  classification_loss: 1.0920 (1.1064)  loss_mask: 0.0000 (0.0000)  time: 0.1706  data: 0.0002  max mem: 6052
[06:39:38.909361] Epoch: [94]  [500/781]  eta: 0:00:48  lr: 0.000003  training_loss: 1.0885 (1.1057)  classification_loss: 1.0885 (1.1056)  loss_mask: 0.0000 (0.0000)  time: 0.1710  data: 0.0003  max mem: 6052
[06:39:42.341782] Epoch: [94]  [520/781]  eta: 0:00:45  lr: 0.000003  training_loss: 1.1087 (1.1064)  classification_loss: 1.1087 (1.1063)  loss_mask: 0.0000 (0.0000)  time: 0.1716  data: 0.0003  max mem: 6052
[06:39:45.757820] Epoch: [94]  [540/781]  eta: 0:00:41  lr: 0.000003  training_loss: 1.1095 (1.1065)  classification_loss: 1.1095 (1.1065)  loss_mask: 0.0000 (0.0000)  time: 0.1707  data: 0.0002  max mem: 6052
[06:39:49.163401] Epoch: [94]  [560/781]  eta: 0:00:38  lr: 0.000003  training_loss: 1.1054 (1.1066)  classification_loss: 1.1054 (1.1066)  loss_mask: 0.0000 (0.0000)  time: 0.1702  data: 0.0002  max mem: 6052
[06:39:52.569550] Epoch: [94]  [580/781]  eta: 0:00:34  lr: 0.000003  training_loss: 1.1040 (1.1076)  classification_loss: 1.1040 (1.1076)  loss_mask: 0.0000 (0.0000)  time: 0.1702  data: 0.0003  max mem: 6052
[06:39:56.003659] Epoch: [94]  [600/781]  eta: 0:00:31  lr: 0.000003  training_loss: 1.0897 (1.1072)  classification_loss: 1.0896 (1.1072)  loss_mask: 0.0000 (0.0000)  time: 0.1716  data: 0.0002  max mem: 6052
[06:39:59.477447] Epoch: [94]  [620/781]  eta: 0:00:27  lr: 0.000003  training_loss: 1.0626 (1.1077)  classification_loss: 1.0626 (1.1077)  loss_mask: 0.0000 (0.0000)  time: 0.1736  data: 0.0002  max mem: 6052
[06:40:02.890154] Epoch: [94]  [640/781]  eta: 0:00:24  lr: 0.000003  training_loss: 1.1152 (1.1086)  classification_loss: 1.1152 (1.1086)  loss_mask: 0.0000 (0.0000)  time: 0.1705  data: 0.0002  max mem: 6052
[06:40:06.314502] Epoch: [94]  [660/781]  eta: 0:00:20  lr: 0.000003  training_loss: 1.1255 (1.1097)  classification_loss: 1.1255 (1.1096)  loss_mask: 0.0000 (0.0000)  time: 0.1710  data: 0.0004  max mem: 6052
[06:40:09.774525] Epoch: [94]  [680/781]  eta: 0:00:17  lr: 0.000003  training_loss: 1.0984 (1.1099)  classification_loss: 1.0984 (1.1098)  loss_mask: 0.0000 (0.0000)  time: 0.1729  data: 0.0003  max mem: 6052
[06:40:13.200832] Epoch: [94]  [700/781]  eta: 0:00:13  lr: 0.000003  training_loss: 1.1604 (1.1112)  classification_loss: 1.1604 (1.1112)  loss_mask: 0.0000 (0.0000)  time: 0.1712  data: 0.0003  max mem: 6052
[06:40:16.657107] Epoch: [94]  [720/781]  eta: 0:00:10  lr: 0.000003  training_loss: 1.0909 (1.1111)  classification_loss: 1.0909 (1.1111)  loss_mask: 0.0000 (0.0000)  time: 0.1727  data: 0.0003  max mem: 6052
[06:40:20.100202] Epoch: [94]  [740/781]  eta: 0:00:07  lr: 0.000003  training_loss: 1.0617 (1.1100)  classification_loss: 1.0617 (1.1100)  loss_mask: 0.0000 (0.0000)  time: 0.1721  data: 0.0004  max mem: 6052
[06:40:23.521188] Epoch: [94]  [760/781]  eta: 0:00:03  lr: 0.000003  training_loss: 1.1002 (1.1104)  classification_loss: 1.1002 (1.1104)  loss_mask: 0.0000 (0.0000)  time: 0.1710  data: 0.0002  max mem: 6052
[06:40:26.922028] Epoch: [94]  [780/781]  eta: 0:00:00  lr: 0.000003  training_loss: 1.0739 (1.1105)  classification_loss: 1.0739 (1.1105)  loss_mask: 0.0000 (0.0000)  time: 0.1700  data: 0.0002  max mem: 6052
[06:40:27.106019] Epoch: [94] Total time: 0:02:14 (0.1724 s / it)
[06:40:27.106548] Averaged stats: lr: 0.000003  training_loss: 1.0739 (1.1105)  classification_loss: 1.0739 (1.1105)  loss_mask: 0.0000 (0.0000)
[06:40:27.770753] Test:  [  0/157]  eta: 0:01:43  testing_loss: 0.4428 (0.4428)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.6597  data: 0.6304  max mem: 6052
[06:40:28.057187] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4242 (0.4488)  acc1: 87.5000 (86.9318)  acc5: 100.0000 (99.7159)  time: 0.0859  data: 0.0575  max mem: 6052
[06:40:28.340690] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4210 (0.4262)  acc1: 87.5000 (87.7232)  acc5: 100.0000 (99.5536)  time: 0.0284  data: 0.0003  max mem: 6052
[06:40:28.622060] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4227 (0.4322)  acc1: 87.5000 (87.2984)  acc5: 100.0000 (99.4960)  time: 0.0281  data: 0.0003  max mem: 6052
[06:40:28.905175] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4411 (0.4379)  acc1: 87.5000 (87.3476)  acc5: 98.4375 (99.3521)  time: 0.0281  data: 0.0001  max mem: 6052
[06:40:29.192376] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4372 (0.4351)  acc1: 87.5000 (87.4694)  acc5: 100.0000 (99.3873)  time: 0.0284  data: 0.0002  max mem: 6052
[06:40:29.473351] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4141 (0.4340)  acc1: 87.5000 (87.4232)  acc5: 100.0000 (99.3596)  time: 0.0283  data: 0.0002  max mem: 6052
[06:40:29.757316] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4145 (0.4302)  acc1: 85.9375 (87.3900)  acc5: 100.0000 (99.3618)  time: 0.0281  data: 0.0002  max mem: 6052
[06:40:30.041310] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4368 (0.4375)  acc1: 85.9375 (87.2878)  acc5: 100.0000 (99.3827)  time: 0.0283  data: 0.0002  max mem: 6052
[06:40:30.341431] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4354 (0.4344)  acc1: 87.5000 (87.3626)  acc5: 100.0000 (99.3819)  time: 0.0291  data: 0.0002  max mem: 6052
[06:40:30.623043] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4185 (0.4370)  acc1: 87.5000 (87.2370)  acc5: 100.0000 (99.3657)  time: 0.0290  data: 0.0002  max mem: 6052
[06:40:30.906612] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4222 (0.4357)  acc1: 87.5000 (87.3170)  acc5: 100.0000 (99.3525)  time: 0.0281  data: 0.0002  max mem: 6052
[06:40:31.192776] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4077 (0.4355)  acc1: 87.5000 (87.3450)  acc5: 100.0000 (99.3414)  time: 0.0283  data: 0.0002  max mem: 6052
[06:40:31.473502] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.3835 (0.4359)  acc1: 87.5000 (87.3927)  acc5: 100.0000 (99.3440)  time: 0.0282  data: 0.0002  max mem: 6052
[06:40:31.752025] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3910 (0.4333)  acc1: 89.0625 (87.5443)  acc5: 100.0000 (99.3684)  time: 0.0279  data: 0.0001  max mem: 6052
[06:40:32.032977] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4131 (0.4335)  acc1: 87.5000 (87.4897)  acc5: 100.0000 (99.3998)  time: 0.0279  data: 0.0001  max mem: 6052
[06:40:32.185345] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4516 (0.4351)  acc1: 87.5000 (87.3800)  acc5: 100.0000 (99.4000)  time: 0.0270  data: 0.0001  max mem: 6052
[06:40:32.351721] Test: Total time: 0:00:05 (0.0334 s / it)
[06:40:32.353360] * Acc@1 87.380 Acc@5 99.400 loss 0.435
[06:40:32.353830] Accuracy of the network on the 10000 test images: 87.4%
[06:40:32.354032] Max accuracy: 87.53%
[06:40:32.467479] log_dir: ./output_dir
[06:40:33.345069] Epoch: [95]  [  0/781]  eta: 0:11:23  lr: 0.000003  training_loss: 1.0699 (1.0699)  classification_loss: 1.0699 (1.0699)  loss_mask: 0.0000 (0.0000)  time: 0.8757  data: 0.6994  max mem: 6052
[06:40:36.781056] Epoch: [95]  [ 20/781]  eta: 0:02:36  lr: 0.000003  training_loss: 1.1069 (1.0805)  classification_loss: 1.1069 (1.0804)  loss_mask: 0.0000 (0.0000)  time: 0.1717  data: 0.0003  max mem: 6052
[06:40:40.192977] Epoch: [95]  [ 40/781]  eta: 0:02:19  lr: 0.000003  training_loss: 1.1311 (1.1128)  classification_loss: 1.1311 (1.1128)  loss_mask: 0.0000 (0.0000)  time: 0.1705  data: 0.0002  max mem: 6052
[06:40:43.601549] Epoch: [95]  [ 60/781]  eta: 0:02:11  lr: 0.000003  training_loss: 1.1321 (1.1198)  classification_loss: 1.1321 (1.1198)  loss_mask: 0.0000 (0.0000)  time: 0.1704  data: 0.0003  max mem: 6052
[06:40:47.056156] Epoch: [95]  [ 80/781]  eta: 0:02:06  lr: 0.000003  training_loss: 1.1195 (1.1191)  classification_loss: 1.1195 (1.1190)  loss_mask: 0.0000 (0.0000)  time: 0.1727  data: 0.0004  max mem: 6052
[06:40:50.508899] Epoch: [95]  [100/781]  eta: 0:02:01  lr: 0.000003  training_loss: 1.0924 (1.1150)  classification_loss: 1.0924 (1.1150)  loss_mask: 0.0000 (0.0000)  time: 0.1725  data: 0.0002  max mem: 6052
[06:40:54.053662] Epoch: [95]  [120/781]  eta: 0:01:57  lr: 0.000003  training_loss: 1.1015 (1.1125)  classification_loss: 1.1015 (1.1125)  loss_mask: 0.0000 (0.0000)  time: 0.1771  data: 0.0003  max mem: 6052
[06:40:57.473914] Epoch: [95]  [140/781]  eta: 0:01:53  lr: 0.000003  training_loss: 1.1093 (1.1139)  classification_loss: 1.1092 (1.1139)  loss_mask: 0.0000 (0.0000)  time: 0.1709  data: 0.0003  max mem: 6052
[06:41:00.892947] Epoch: [95]  [160/781]  eta: 0:01:49  lr: 0.000003  training_loss: 1.0746 (1.1128)  classification_loss: 1.0745 (1.1127)  loss_mask: 0.0000 (0.0000)  time: 0.1708  data: 0.0001  max mem: 6052
[06:41:04.309335] Epoch: [95]  [180/781]  eta: 0:01:45  lr: 0.000003  training_loss: 1.1441 (1.1120)  classification_loss: 1.1441 (1.1120)  loss_mask: 0.0000 (0.0000)  time: 0.1707  data: 0.0002  max mem: 6052
[06:41:07.714187] Epoch: [95]  [200/781]  eta: 0:01:41  lr: 0.000003  training_loss: 1.1178 (1.1122)  classification_loss: 1.1178 (1.1122)  loss_mask: 0.0000 (0.0000)  time: 0.1702  data: 0.0002  max mem: 6052
[06:41:11.120792] Epoch: [95]  [220/781]  eta: 0:01:38  lr: 0.000003  training_loss: 1.0982 (1.1102)  classification_loss: 1.0982 (1.1102)  loss_mask: 0.0000 (0.0000)  time: 0.1703  data: 0.0001  max mem: 6052
[06:41:14.529948] Epoch: [95]  [240/781]  eta: 0:01:34  lr: 0.000002  training_loss: 1.1262 (1.1130)  classification_loss: 1.1262 (1.1130)  loss_mask: 0.0000 (0.0000)  time: 0.1704  data: 0.0002  max mem: 6052
[06:41:17.971074] Epoch: [95]  [260/781]  eta: 0:01:30  lr: 0.000002  training_loss: 1.1381 (1.1131)  classification_loss: 1.1381 (1.1131)  loss_mask: 0.0000 (0.0000)  time: 0.1720  data: 0.0002  max mem: 6052
[06:41:21.398165] Epoch: [95]  [280/781]  eta: 0:01:27  lr: 0.000002  training_loss: 1.1272 (1.1133)  classification_loss: 1.1272 (1.1133)  loss_mask: 0.0000 (0.0000)  time: 0.1713  data: 0.0002  max mem: 6052
[06:41:24.835406] Epoch: [95]  [300/781]  eta: 0:01:23  lr: 0.000002  training_loss: 1.1132 (1.1141)  classification_loss: 1.1132 (1.1140)  loss_mask: 0.0000 (0.0000)  time: 0.1718  data: 0.0002  max mem: 6052
[06:41:28.242709] Epoch: [95]  [320/781]  eta: 0:01:20  lr: 0.000002  training_loss: 1.1405 (1.1152)  classification_loss: 1.1405 (1.1152)  loss_mask: 0.0000 (0.0000)  time: 0.1703  data: 0.0002  max mem: 6052
[06:41:31.651110] Epoch: [95]  [340/781]  eta: 0:01:16  lr: 0.000002  training_loss: 1.0273 (1.1124)  classification_loss: 1.0273 (1.1123)  loss_mask: 0.0000 (0.0000)  time: 0.1703  data: 0.0002  max mem: 6052
[06:41:35.062730] Epoch: [95]  [360/781]  eta: 0:01:12  lr: 0.000002  training_loss: 1.1603 (1.1144)  classification_loss: 1.1602 (1.1144)  loss_mask: 0.0000 (0.0000)  time: 0.1705  data: 0.0002  max mem: 6052
[06:41:38.474528] Epoch: [95]  [380/781]  eta: 0:01:09  lr: 0.000002  training_loss: 1.1335 (1.1143)  classification_loss: 1.1335 (1.1142)  loss_mask: 0.0000 (0.0000)  time: 0.1705  data: 0.0002  max mem: 6052
[06:41:41.905742] Epoch: [95]  [400/781]  eta: 0:01:05  lr: 0.000002  training_loss: 1.0833 (1.1135)  classification_loss: 1.0833 (1.1134)  loss_mask: 0.0000 (0.0000)  time: 0.1715  data: 0.0002  max mem: 6052
[06:41:45.386901] Epoch: [95]  [420/781]  eta: 0:01:02  lr: 0.000002  training_loss: 1.0675 (1.1121)  classification_loss: 1.0674 (1.1121)  loss_mask: 0.0000 (0.0000)  time: 0.1740  data: 0.0002  max mem: 6052
[06:41:48.803060] Epoch: [95]  [440/781]  eta: 0:00:58  lr: 0.000002  training_loss: 1.0855 (1.1110)  classification_loss: 1.0855 (1.1109)  loss_mask: 0.0000 (0.0000)  time: 0.1707  data: 0.0002  max mem: 6052
[06:41:52.300737] Epoch: [95]  [460/781]  eta: 0:00:55  lr: 0.000002  training_loss: 1.1099 (1.1105)  classification_loss: 1.1099 (1.1105)  loss_mask: 0.0000 (0.0000)  time: 0.1748  data: 0.0002  max mem: 6052
[06:41:55.708775] Epoch: [95]  [480/781]  eta: 0:00:52  lr: 0.000002  training_loss: 1.1241 (1.1110)  classification_loss: 1.1241 (1.1110)  loss_mask: 0.0000 (0.0000)  time: 0.1703  data: 0.0002  max mem: 6052
[06:41:59.123717] Epoch: [95]  [500/781]  eta: 0:00:48  lr: 0.000002  training_loss: 1.0805 (1.1103)  classification_loss: 1.0805 (1.1102)  loss_mask: 0.0000 (0.0000)  time: 0.1707  data: 0.0003  max mem: 6052
[06:42:02.524945] Epoch: [95]  [520/781]  eta: 0:00:45  lr: 0.000002  training_loss: 1.1518 (1.1121)  classification_loss: 1.1517 (1.1120)  loss_mask: 0.0000 (0.0000)  time: 0.1700  data: 0.0002  max mem: 6052
[06:42:05.948618] Epoch: [95]  [540/781]  eta: 0:00:41  lr: 0.000002  training_loss: 1.1343 (1.1134)  classification_loss: 1.1343 (1.1133)  loss_mask: 0.0000 (0.0000)  time: 0.1711  data: 0.0003  max mem: 6052
[06:42:09.384976] Epoch: [95]  [560/781]  eta: 0:00:38  lr: 0.000002  training_loss: 1.1072 (1.1141)  classification_loss: 1.1072 (1.1141)  loss_mask: 0.0000 (0.0000)  time: 0.1717  data: 0.0002  max mem: 6052
[06:42:12.809520] Epoch: [95]  [580/781]  eta: 0:00:34  lr: 0.000002  training_loss: 1.1282 (1.1144)  classification_loss: 1.1281 (1.1143)  loss_mask: 0.0000 (0.0000)  time: 0.1711  data: 0.0003  max mem: 6052
[06:42:16.223113] Epoch: [95]  [600/781]  eta: 0:00:31  lr: 0.000002  training_loss: 1.0935 (1.1139)  classification_loss: 1.0935 (1.1139)  loss_mask: 0.0000 (0.0000)  time: 0.1706  data: 0.0002  max mem: 6052
[06:42:19.665634] Epoch: [95]  [620/781]  eta: 0:00:27  lr: 0.000002  training_loss: 1.0870 (1.1137)  classification_loss: 1.0869 (1.1137)  loss_mask: 0.0000 (0.0000)  time: 0.1721  data: 0.0003  max mem: 6052
[06:42:23.083815] Epoch: [95]  [640/781]  eta: 0:00:24  lr: 0.000002  training_loss: 1.1054 (1.1135)  classification_loss: 1.1054 (1.1135)  loss_mask: 0.0000 (0.0000)  time: 0.1708  data: 0.0003  max mem: 6052
[06:42:26.498853] Epoch: [95]  [660/781]  eta: 0:00:20  lr: 0.000002  training_loss: 1.0949 (1.1134)  classification_loss: 1.0949 (1.1133)  loss_mask: 0.0000 (0.0000)  time: 0.1707  data: 0.0002  max mem: 6052
[06:42:29.907760] Epoch: [95]  [680/781]  eta: 0:00:17  lr: 0.000002  training_loss: 1.1048 (1.1135)  classification_loss: 1.1048 (1.1135)  loss_mask: 0.0000 (0.0000)  time: 0.1703  data: 0.0002  max mem: 6052
[06:42:33.306767] Epoch: [95]  [700/781]  eta: 0:00:13  lr: 0.000002  training_loss: 1.1244 (1.1137)  classification_loss: 1.1244 (1.1136)  loss_mask: 0.0000 (0.0000)  time: 0.1699  data: 0.0002  max mem: 6052
[06:42:36.745344] Epoch: [95]  [720/781]  eta: 0:00:10  lr: 0.000002  training_loss: 1.0699 (1.1130)  classification_loss: 1.0698 (1.1129)  loss_mask: 0.0000 (0.0000)  time: 0.1718  data: 0.0002  max mem: 6052
[06:42:40.180301] Epoch: [95]  [740/781]  eta: 0:00:07  lr: 0.000002  training_loss: 1.1007 (1.1125)  classification_loss: 1.1007 (1.1125)  loss_mask: 0.0000 (0.0000)  time: 0.1717  data: 0.0002  max mem: 6052
[06:42:43.639783] Epoch: [95]  [760/781]  eta: 0:00:03  lr: 0.000002  training_loss: 1.0920 (1.1128)  classification_loss: 1.0920 (1.1128)  loss_mask: 0.0000 (0.0000)  time: 0.1729  data: 0.0002  max mem: 6052
[06:42:47.035181] Epoch: [95]  [780/781]  eta: 0:00:00  lr: 0.000002  training_loss: 1.1242 (1.1132)  classification_loss: 1.1242 (1.1132)  loss_mask: 0.0000 (0.0000)  time: 0.1697  data: 0.0002  max mem: 6052
[06:42:47.219260] Epoch: [95] Total time: 0:02:14 (0.1725 s / it)
[06:42:47.220005] Averaged stats: lr: 0.000002  training_loss: 1.1242 (1.1132)  classification_loss: 1.1242 (1.1132)  loss_mask: 0.0000 (0.0000)
[06:42:47.871688] Test:  [  0/157]  eta: 0:01:41  testing_loss: 0.4404 (0.4404)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.6473  data: 0.6059  max mem: 6052
[06:42:48.156623] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4173 (0.4446)  acc1: 87.5000 (87.2159)  acc5: 100.0000 (99.7159)  time: 0.0845  data: 0.0552  max mem: 6052
[06:42:48.448794] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4134 (0.4220)  acc1: 87.5000 (87.7232)  acc5: 100.0000 (99.5536)  time: 0.0287  data: 0.0002  max mem: 6052
[06:42:48.730207] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4134 (0.4278)  acc1: 87.5000 (87.6512)  acc5: 100.0000 (99.4960)  time: 0.0286  data: 0.0002  max mem: 6052
[06:42:49.010437] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4467 (0.4332)  acc1: 87.5000 (87.6143)  acc5: 98.4375 (99.3521)  time: 0.0280  data: 0.0002  max mem: 6052
[06:42:49.307793] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4224 (0.4307)  acc1: 89.0625 (87.7145)  acc5: 100.0000 (99.3566)  time: 0.0288  data: 0.0001  max mem: 6052
[06:42:49.593178] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4071 (0.4297)  acc1: 89.0625 (87.6793)  acc5: 100.0000 (99.3340)  time: 0.0290  data: 0.0002  max mem: 6052
[06:42:49.874463] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4071 (0.4260)  acc1: 89.0625 (87.7641)  acc5: 100.0000 (99.3618)  time: 0.0282  data: 0.0002  max mem: 6052
[06:42:50.169231] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4405 (0.4336)  acc1: 87.5000 (87.5772)  acc5: 100.0000 (99.3827)  time: 0.0286  data: 0.0002  max mem: 6052
[06:42:50.453820] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4346 (0.4307)  acc1: 87.5000 (87.6374)  acc5: 100.0000 (99.3819)  time: 0.0288  data: 0.0002  max mem: 6052
[06:42:50.734929] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4163 (0.4334)  acc1: 87.5000 (87.4691)  acc5: 100.0000 (99.3812)  time: 0.0282  data: 0.0002  max mem: 6052
[06:42:51.015526] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4169 (0.4324)  acc1: 87.5000 (87.5845)  acc5: 100.0000 (99.3806)  time: 0.0280  data: 0.0002  max mem: 6052
[06:42:51.295737] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4073 (0.4325)  acc1: 89.0625 (87.5904)  acc5: 100.0000 (99.3673)  time: 0.0279  data: 0.0002  max mem: 6052
[06:42:51.576557] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.3836 (0.4328)  acc1: 87.5000 (87.6193)  acc5: 100.0000 (99.3678)  time: 0.0279  data: 0.0002  max mem: 6052
[06:42:51.856056] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3888 (0.4301)  acc1: 89.0625 (87.7549)  acc5: 100.0000 (99.3905)  time: 0.0279  data: 0.0001  max mem: 6052
[06:42:52.134212] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4105 (0.4299)  acc1: 87.5000 (87.6863)  acc5: 100.0000 (99.4205)  time: 0.0278  data: 0.0001  max mem: 6052
[06:42:52.290014] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4491 (0.4317)  acc1: 85.9375 (87.6100)  acc5: 100.0000 (99.4200)  time: 0.0271  data: 0.0001  max mem: 6052
[06:42:52.444782] Test: Total time: 0:00:05 (0.0333 s / it)
[06:42:52.445341] * Acc@1 87.610 Acc@5 99.420 loss 0.432
[06:42:52.445658] Accuracy of the network on the 10000 test images: 87.6%
[06:42:52.445872] Max accuracy: 87.61%
[06:42:52.570728] log_dir: ./output_dir
[06:42:53.465100] Epoch: [96]  [  0/781]  eta: 0:11:37  lr: 0.000002  training_loss: 0.9620 (0.9620)  classification_loss: 0.9620 (0.9620)  loss_mask: 0.0000 (0.0000)  time: 0.8928  data: 0.7029  max mem: 6052
[06:42:56.886779] Epoch: [96]  [ 20/781]  eta: 0:02:36  lr: 0.000002  training_loss: 1.1222 (1.0999)  classification_loss: 1.1222 (1.0999)  loss_mask: 0.0000 (0.0000)  time: 0.1710  data: 0.0002  max mem: 6052
[06:43:00.316050] Epoch: [96]  [ 40/781]  eta: 0:02:19  lr: 0.000002  training_loss: 1.1022 (1.1018)  classification_loss: 1.1021 (1.1018)  loss_mask: 0.0000 (0.0001)  time: 0.1714  data: 0.0003  max mem: 6052
[06:43:03.743664] Epoch: [96]  [ 60/781]  eta: 0:02:11  lr: 0.000002  training_loss: 1.1195 (1.1157)  classification_loss: 1.1194 (1.1156)  loss_mask: 0.0000 (0.0000)  time: 0.1713  data: 0.0002  max mem: 6052
[06:43:07.153523] Epoch: [96]  [ 80/781]  eta: 0:02:06  lr: 0.000002  training_loss: 1.1053 (1.1161)  classification_loss: 1.1053 (1.1160)  loss_mask: 0.0000 (0.0000)  time: 0.1704  data: 0.0003  max mem: 6052
[06:43:10.572386] Epoch: [96]  [100/781]  eta: 0:02:01  lr: 0.000002  training_loss: 1.1025 (1.1123)  classification_loss: 1.1025 (1.1122)  loss_mask: 0.0000 (0.0000)  time: 0.1708  data: 0.0002  max mem: 6052
[06:43:13.975626] Epoch: [96]  [120/781]  eta: 0:01:56  lr: 0.000002  training_loss: 1.0999 (1.1072)  classification_loss: 1.0998 (1.1072)  loss_mask: 0.0000 (0.0000)  time: 0.1701  data: 0.0003  max mem: 6052
[06:43:17.386524] Epoch: [96]  [140/781]  eta: 0:01:52  lr: 0.000002  training_loss: 1.0439 (1.1013)  classification_loss: 1.0438 (1.1013)  loss_mask: 0.0000 (0.0000)  time: 0.1705  data: 0.0002  max mem: 6052
[06:43:20.798622] Epoch: [96]  [160/781]  eta: 0:01:48  lr: 0.000002  training_loss: 1.0768 (1.1048)  classification_loss: 1.0768 (1.1048)  loss_mask: 0.0000 (0.0000)  time: 0.1705  data: 0.0003  max mem: 6052
[06:43:24.229556] Epoch: [96]  [180/781]  eta: 0:01:45  lr: 0.000002  training_loss: 1.0823 (1.1051)  classification_loss: 1.0823 (1.1051)  loss_mask: 0.0000 (0.0000)  time: 0.1714  data: 0.0002  max mem: 6052
[06:43:27.666333] Epoch: [96]  [200/781]  eta: 0:01:41  lr: 0.000002  training_loss: 1.1186 (1.1075)  classification_loss: 1.1186 (1.1074)  loss_mask: 0.0000 (0.0000)  time: 0.1718  data: 0.0002  max mem: 6052
[06:43:31.092450] Epoch: [96]  [220/781]  eta: 0:01:37  lr: 0.000002  training_loss: 1.1044 (1.1074)  classification_loss: 1.1044 (1.1073)  loss_mask: 0.0000 (0.0000)  time: 0.1712  data: 0.0002  max mem: 6052
[06:43:34.542413] Epoch: [96]  [240/781]  eta: 0:01:34  lr: 0.000002  training_loss: 1.1569 (1.1108)  classification_loss: 1.1569 (1.1108)  loss_mask: 0.0000 (0.0000)  time: 0.1724  data: 0.0002  max mem: 6052
[06:43:37.974068] Epoch: [96]  [260/781]  eta: 0:01:30  lr: 0.000002  training_loss: 1.0993 (1.1117)  classification_loss: 1.0992 (1.1116)  loss_mask: 0.0000 (0.0000)  time: 0.1715  data: 0.0004  max mem: 6052
[06:43:41.396870] Epoch: [96]  [280/781]  eta: 0:01:27  lr: 0.000002  training_loss: 1.1049 (1.1126)  classification_loss: 1.1049 (1.1126)  loss_mask: 0.0000 (0.0000)  time: 0.1711  data: 0.0001  max mem: 6052
[06:43:44.813686] Epoch: [96]  [300/781]  eta: 0:01:23  lr: 0.000002  training_loss: 1.1226 (1.1137)  classification_loss: 1.1225 (1.1136)  loss_mask: 0.0000 (0.0000)  time: 0.1708  data: 0.0002  max mem: 6052
[06:43:48.222369] Epoch: [96]  [320/781]  eta: 0:01:19  lr: 0.000002  training_loss: 1.1357 (1.1151)  classification_loss: 1.1356 (1.1151)  loss_mask: 0.0000 (0.0000)  time: 0.1704  data: 0.0002  max mem: 6052
[06:43:51.650438] Epoch: [96]  [340/781]  eta: 0:01:16  lr: 0.000002  training_loss: 1.0837 (1.1145)  classification_loss: 1.0836 (1.1144)  loss_mask: 0.0000 (0.0000)  time: 0.1713  data: 0.0002  max mem: 6052
[06:43:55.056411] Epoch: [96]  [360/781]  eta: 0:01:12  lr: 0.000002  training_loss: 1.0752 (1.1139)  classification_loss: 1.0751 (1.1139)  loss_mask: 0.0000 (0.0000)  time: 0.1702  data: 0.0002  max mem: 6052
[06:43:58.485710] Epoch: [96]  [380/781]  eta: 0:01:09  lr: 0.000002  training_loss: 1.0954 (1.1136)  classification_loss: 1.0954 (1.1135)  loss_mask: 0.0000 (0.0000)  time: 0.1714  data: 0.0002  max mem: 6052
[06:44:01.911435] Epoch: [96]  [400/781]  eta: 0:01:05  lr: 0.000002  training_loss: 1.1185 (1.1133)  classification_loss: 1.1185 (1.1133)  loss_mask: 0.0000 (0.0000)  time: 0.1712  data: 0.0002  max mem: 6052
[06:44:05.331787] Epoch: [96]  [420/781]  eta: 0:01:02  lr: 0.000002  training_loss: 1.0931 (1.1129)  classification_loss: 1.0931 (1.1129)  loss_mask: 0.0000 (0.0000)  time: 0.1710  data: 0.0002  max mem: 6052
[06:44:08.753319] Epoch: [96]  [440/781]  eta: 0:00:58  lr: 0.000002  training_loss: 1.0813 (1.1120)  classification_loss: 1.0812 (1.1119)  loss_mask: 0.0000 (0.0000)  time: 0.1710  data: 0.0002  max mem: 6052
[06:44:12.265704] Epoch: [96]  [460/781]  eta: 0:00:55  lr: 0.000002  training_loss: 1.0795 (1.1118)  classification_loss: 1.0795 (1.1118)  loss_mask: 0.0000 (0.0000)  time: 0.1755  data: 0.0002  max mem: 6052
[06:44:15.675245] Epoch: [96]  [480/781]  eta: 0:00:51  lr: 0.000002  training_loss: 1.0984 (1.1118)  classification_loss: 1.0984 (1.1118)  loss_mask: 0.0000 (0.0000)  time: 0.1704  data: 0.0002  max mem: 6052
[06:44:19.124109] Epoch: [96]  [500/781]  eta: 0:00:48  lr: 0.000002  training_loss: 1.1370 (1.1122)  classification_loss: 1.1370 (1.1121)  loss_mask: 0.0000 (0.0000)  time: 0.1724  data: 0.0002  max mem: 6052
[06:44:22.517078] Epoch: [96]  [520/781]  eta: 0:00:45  lr: 0.000002  training_loss: 1.1817 (1.1139)  classification_loss: 1.1816 (1.1139)  loss_mask: 0.0000 (0.0000)  time: 0.1696  data: 0.0001  max mem: 6052
[06:44:25.944087] Epoch: [96]  [540/781]  eta: 0:00:41  lr: 0.000002  training_loss: 1.1134 (1.1136)  classification_loss: 1.1134 (1.1136)  loss_mask: 0.0000 (0.0000)  time: 0.1713  data: 0.0002  max mem: 6052
[06:44:29.373247] Epoch: [96]  [560/781]  eta: 0:00:38  lr: 0.000002  training_loss: 1.1010 (1.1127)  classification_loss: 1.1010 (1.1127)  loss_mask: 0.0000 (0.0000)  time: 0.1714  data: 0.0003  max mem: 6052
[06:44:32.814847] Epoch: [96]  [580/781]  eta: 0:00:34  lr: 0.000002  training_loss: 1.0886 (1.1115)  classification_loss: 1.0886 (1.1115)  loss_mask: 0.0000 (0.0000)  time: 0.1720  data: 0.0002  max mem: 6052
[06:44:36.220347] Epoch: [96]  [600/781]  eta: 0:00:31  lr: 0.000002  training_loss: 1.0806 (1.1106)  classification_loss: 1.0806 (1.1106)  loss_mask: 0.0000 (0.0000)  time: 0.1702  data: 0.0003  max mem: 6052
[06:44:39.657768] Epoch: [96]  [620/781]  eta: 0:00:27  lr: 0.000002  training_loss: 1.0994 (1.1106)  classification_loss: 1.0994 (1.1106)  loss_mask: 0.0000 (0.0000)  time: 0.1718  data: 0.0002  max mem: 6052
[06:44:43.070127] Epoch: [96]  [640/781]  eta: 0:00:24  lr: 0.000002  training_loss: 1.1374 (1.1112)  classification_loss: 1.1374 (1.1111)  loss_mask: 0.0000 (0.0000)  time: 0.1705  data: 0.0002  max mem: 6052
[06:44:46.559200] Epoch: [96]  [660/781]  eta: 0:00:20  lr: 0.000002  training_loss: 1.0869 (1.1114)  classification_loss: 1.0869 (1.1114)  loss_mask: 0.0000 (0.0000)  time: 0.1744  data: 0.0003  max mem: 6052
[06:44:49.988961] Epoch: [96]  [680/781]  eta: 0:00:17  lr: 0.000002  training_loss: 1.1487 (1.1122)  classification_loss: 1.1487 (1.1122)  loss_mask: 0.0000 (0.0000)  time: 0.1714  data: 0.0002  max mem: 6052
[06:44:53.398562] Epoch: [96]  [700/781]  eta: 0:00:13  lr: 0.000002  training_loss: 1.0818 (1.1123)  classification_loss: 1.0818 (1.1122)  loss_mask: 0.0000 (0.0000)  time: 0.1704  data: 0.0002  max mem: 6052
[06:44:56.932183] Epoch: [96]  [720/781]  eta: 0:00:10  lr: 0.000002  training_loss: 1.1644 (1.1133)  classification_loss: 1.1644 (1.1133)  loss_mask: 0.0000 (0.0000)  time: 0.1766  data: 0.0002  max mem: 6052
[06:45:00.468519] Epoch: [96]  [740/781]  eta: 0:00:07  lr: 0.000002  training_loss: 1.0872 (1.1127)  classification_loss: 1.0872 (1.1127)  loss_mask: 0.0000 (0.0000)  time: 0.1767  data: 0.0002  max mem: 6052
[06:45:03.906469] Epoch: [96]  [760/781]  eta: 0:00:03  lr: 0.000002  training_loss: 1.1334 (1.1134)  classification_loss: 1.1334 (1.1133)  loss_mask: 0.0000 (0.0000)  time: 0.1718  data: 0.0003  max mem: 6052
[06:45:07.315222] Epoch: [96]  [780/781]  eta: 0:00:00  lr: 0.000002  training_loss: 1.1457 (1.1140)  classification_loss: 1.1456 (1.1139)  loss_mask: 0.0000 (0.0000)  time: 0.1703  data: 0.0002  max mem: 6052
[06:45:07.482469] Epoch: [96] Total time: 0:02:14 (0.1727 s / it)
[06:45:07.483688] Averaged stats: lr: 0.000002  training_loss: 1.1457 (1.1140)  classification_loss: 1.1456 (1.1139)  loss_mask: 0.0000 (0.0000)
[06:45:08.131334] Test:  [  0/157]  eta: 0:01:40  testing_loss: 0.4389 (0.4389)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.6412  data: 0.6068  max mem: 6052
[06:45:08.416117] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4224 (0.4442)  acc1: 87.5000 (86.7898)  acc5: 100.0000 (99.7159)  time: 0.0840  data: 0.0553  max mem: 6052
[06:45:08.697086] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4053 (0.4222)  acc1: 89.0625 (88.0952)  acc5: 100.0000 (99.5536)  time: 0.0281  data: 0.0001  max mem: 6052
[06:45:08.977810] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4024 (0.4283)  acc1: 89.0625 (87.8024)  acc5: 100.0000 (99.4960)  time: 0.0280  data: 0.0001  max mem: 6052
[06:45:09.258031] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4447 (0.4344)  acc1: 87.5000 (87.6143)  acc5: 98.4375 (99.3521)  time: 0.0279  data: 0.0001  max mem: 6052
[06:45:09.539492] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4327 (0.4315)  acc1: 87.5000 (87.7451)  acc5: 100.0000 (99.3566)  time: 0.0280  data: 0.0001  max mem: 6052
[06:45:09.819648] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4080 (0.4303)  acc1: 87.5000 (87.6537)  acc5: 100.0000 (99.3340)  time: 0.0280  data: 0.0001  max mem: 6052
[06:45:10.099845] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4102 (0.4267)  acc1: 87.5000 (87.7201)  acc5: 100.0000 (99.3618)  time: 0.0279  data: 0.0001  max mem: 6052
[06:45:10.380704] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4383 (0.4336)  acc1: 85.9375 (87.5579)  acc5: 100.0000 (99.4020)  time: 0.0279  data: 0.0001  max mem: 6052
[06:45:10.661530] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4322 (0.4308)  acc1: 87.5000 (87.5687)  acc5: 100.0000 (99.3990)  time: 0.0280  data: 0.0001  max mem: 6052
[06:45:10.942067] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4103 (0.4336)  acc1: 87.5000 (87.4536)  acc5: 100.0000 (99.3967)  time: 0.0280  data: 0.0001  max mem: 6052
[06:45:11.224068] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4110 (0.4326)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (99.3947)  time: 0.0280  data: 0.0001  max mem: 6052
[06:45:11.507667] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4087 (0.4329)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (99.3802)  time: 0.0282  data: 0.0001  max mem: 6052
[06:45:11.787683] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.3819 (0.4332)  acc1: 87.5000 (87.5477)  acc5: 100.0000 (99.3798)  time: 0.0281  data: 0.0001  max mem: 6052
[06:45:12.067042] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3872 (0.4309)  acc1: 87.5000 (87.6773)  acc5: 100.0000 (99.4016)  time: 0.0279  data: 0.0001  max mem: 6052
[06:45:12.345033] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4188 (0.4310)  acc1: 87.5000 (87.6242)  acc5: 100.0000 (99.4102)  time: 0.0278  data: 0.0001  max mem: 6052
[06:45:12.493770] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4468 (0.4327)  acc1: 87.5000 (87.5200)  acc5: 100.0000 (99.4100)  time: 0.0268  data: 0.0001  max mem: 6052
[06:45:12.645673] Test: Total time: 0:00:05 (0.0329 s / it)
[06:45:12.646101] * Acc@1 87.520 Acc@5 99.410 loss 0.433
[06:45:12.646412] Accuracy of the network on the 10000 test images: 87.5%
[06:45:12.646587] Max accuracy: 87.61%
[06:45:12.934170] log_dir: ./output_dir
[06:45:13.791385] Epoch: [97]  [  0/781]  eta: 0:11:08  lr: 0.000002  training_loss: 0.8403 (0.8403)  classification_loss: 0.8402 (0.8402)  loss_mask: 0.0000 (0.0000)  time: 0.8556  data: 0.6454  max mem: 6052
[06:45:17.231365] Epoch: [97]  [ 20/781]  eta: 0:02:35  lr: 0.000002  training_loss: 1.1089 (1.0786)  classification_loss: 1.1089 (1.0786)  loss_mask: 0.0000 (0.0000)  time: 0.1719  data: 0.0002  max mem: 6052
[06:45:20.648493] Epoch: [97]  [ 40/781]  eta: 0:02:19  lr: 0.000002  training_loss: 1.1207 (1.1074)  classification_loss: 1.1207 (1.1074)  loss_mask: 0.0000 (0.0000)  time: 0.1708  data: 0.0003  max mem: 6052
[06:45:24.069811] Epoch: [97]  [ 60/781]  eta: 0:02:11  lr: 0.000002  training_loss: 1.1045 (1.1117)  classification_loss: 1.1045 (1.1116)  loss_mask: 0.0000 (0.0000)  time: 0.1710  data: 0.0003  max mem: 6052
[06:45:27.534743] Epoch: [97]  [ 80/781]  eta: 0:02:06  lr: 0.000002  training_loss: 1.1039 (1.1140)  classification_loss: 1.1038 (1.1140)  loss_mask: 0.0000 (0.0000)  time: 0.1732  data: 0.0003  max mem: 6052
[06:45:31.024668] Epoch: [97]  [100/781]  eta: 0:02:01  lr: 0.000002  training_loss: 1.0755 (1.1108)  classification_loss: 1.0755 (1.1108)  loss_mask: 0.0000 (0.0000)  time: 0.1744  data: 0.0002  max mem: 6052
[06:45:34.476640] Epoch: [97]  [120/781]  eta: 0:01:57  lr: 0.000002  training_loss: 1.0615 (1.1072)  classification_loss: 1.0614 (1.1071)  loss_mask: 0.0000 (0.0000)  time: 0.1725  data: 0.0003  max mem: 6052
[06:45:37.908509] Epoch: [97]  [140/781]  eta: 0:01:53  lr: 0.000002  training_loss: 1.1046 (1.1033)  classification_loss: 1.1046 (1.1033)  loss_mask: 0.0000 (0.0000)  time: 0.1715  data: 0.0002  max mem: 6052
[06:45:41.331200] Epoch: [97]  [160/781]  eta: 0:01:49  lr: 0.000002  training_loss: 1.1077 (1.1058)  classification_loss: 1.1076 (1.1058)  loss_mask: 0.0000 (0.0000)  time: 0.1710  data: 0.0002  max mem: 6052
[06:45:44.735262] Epoch: [97]  [180/781]  eta: 0:01:45  lr: 0.000002  training_loss: 1.1026 (1.1068)  classification_loss: 1.1026 (1.1068)  loss_mask: 0.0000 (0.0000)  time: 0.1701  data: 0.0002  max mem: 6052
[06:45:48.154805] Epoch: [97]  [200/781]  eta: 0:01:41  lr: 0.000002  training_loss: 1.0718 (1.1057)  classification_loss: 1.0718 (1.1057)  loss_mask: 0.0000 (0.0000)  time: 0.1709  data: 0.0002  max mem: 6052
[06:45:51.568399] Epoch: [97]  [220/781]  eta: 0:01:38  lr: 0.000002  training_loss: 1.1156 (1.1066)  classification_loss: 1.1156 (1.1066)  loss_mask: 0.0000 (0.0000)  time: 0.1706  data: 0.0001  max mem: 6052
[06:45:55.037991] Epoch: [97]  [240/781]  eta: 0:01:34  lr: 0.000001  training_loss: 1.0519 (1.1067)  classification_loss: 1.0519 (1.1067)  loss_mask: 0.0000 (0.0000)  time: 0.1734  data: 0.0002  max mem: 6052
[06:45:58.480737] Epoch: [97]  [260/781]  eta: 0:01:30  lr: 0.000001  training_loss: 1.1027 (1.1074)  classification_loss: 1.1027 (1.1074)  loss_mask: 0.0000 (0.0000)  time: 0.1720  data: 0.0002  max mem: 6052
[06:46:01.965763] Epoch: [97]  [280/781]  eta: 0:01:27  lr: 0.000001  training_loss: 1.0721 (1.1069)  classification_loss: 1.0721 (1.1069)  loss_mask: 0.0000 (0.0000)  time: 0.1742  data: 0.0002  max mem: 6052
[06:46:05.405274] Epoch: [97]  [300/781]  eta: 0:01:23  lr: 0.000001  training_loss: 1.1129 (1.1067)  classification_loss: 1.1129 (1.1066)  loss_mask: 0.0000 (0.0000)  time: 0.1719  data: 0.0002  max mem: 6052
[06:46:08.827624] Epoch: [97]  [320/781]  eta: 0:01:20  lr: 0.000001  training_loss: 1.0960 (1.1070)  classification_loss: 1.0960 (1.1070)  loss_mask: 0.0000 (0.0000)  time: 0.1710  data: 0.0002  max mem: 6052
[06:46:12.244961] Epoch: [97]  [340/781]  eta: 0:01:16  lr: 0.000001  training_loss: 1.0627 (1.1038)  classification_loss: 1.0627 (1.1038)  loss_mask: 0.0000 (0.0000)  time: 0.1707  data: 0.0003  max mem: 6052
[06:46:15.678904] Epoch: [97]  [360/781]  eta: 0:01:13  lr: 0.000001  training_loss: 1.0628 (1.1022)  classification_loss: 1.0627 (1.1021)  loss_mask: 0.0000 (0.0000)  time: 0.1716  data: 0.0001  max mem: 6052
[06:46:19.106199] Epoch: [97]  [380/781]  eta: 0:01:09  lr: 0.000001  training_loss: 1.1038 (1.1031)  classification_loss: 1.1038 (1.1031)  loss_mask: 0.0000 (0.0000)  time: 0.1713  data: 0.0003  max mem: 6052
[06:46:22.514905] Epoch: [97]  [400/781]  eta: 0:01:06  lr: 0.000001  training_loss: 1.0887 (1.1024)  classification_loss: 1.0886 (1.1024)  loss_mask: 0.0000 (0.0000)  time: 0.1704  data: 0.0002  max mem: 6052
[06:46:25.949101] Epoch: [97]  [420/781]  eta: 0:01:02  lr: 0.000001  training_loss: 1.1125 (1.1040)  classification_loss: 1.1125 (1.1039)  loss_mask: 0.0000 (0.0000)  time: 0.1716  data: 0.0002  max mem: 6052
[06:46:29.383605] Epoch: [97]  [440/781]  eta: 0:00:59  lr: 0.000001  training_loss: 1.0948 (1.1041)  classification_loss: 1.0948 (1.1040)  loss_mask: 0.0000 (0.0000)  time: 0.1717  data: 0.0002  max mem: 6052
[06:46:32.819746] Epoch: [97]  [460/781]  eta: 0:00:55  lr: 0.000001  training_loss: 1.0992 (1.1033)  classification_loss: 1.0992 (1.1033)  loss_mask: 0.0000 (0.0000)  time: 0.1717  data: 0.0002  max mem: 6052
[06:46:36.308524] Epoch: [97]  [480/781]  eta: 0:00:52  lr: 0.000001  training_loss: 1.1142 (1.1040)  classification_loss: 1.1142 (1.1039)  loss_mask: 0.0000 (0.0000)  time: 0.1743  data: 0.0002  max mem: 6052
[06:46:39.765861] Epoch: [97]  [500/781]  eta: 0:00:48  lr: 0.000001  training_loss: 1.1041 (1.1045)  classification_loss: 1.1040 (1.1045)  loss_mask: 0.0000 (0.0000)  time: 0.1727  data: 0.0002  max mem: 6052
[06:46:43.197701] Epoch: [97]  [520/781]  eta: 0:00:45  lr: 0.000001  training_loss: 1.0709 (1.1043)  classification_loss: 1.0709 (1.1042)  loss_mask: 0.0000 (0.0000)  time: 0.1715  data: 0.0002  max mem: 6052
[06:46:46.633084] Epoch: [97]  [540/781]  eta: 0:00:41  lr: 0.000001  training_loss: 1.0901 (1.1045)  classification_loss: 1.0901 (1.1045)  loss_mask: 0.0000 (0.0000)  time: 0.1717  data: 0.0002  max mem: 6052
[06:46:50.043565] Epoch: [97]  [560/781]  eta: 0:00:38  lr: 0.000001  training_loss: 1.1197 (1.1046)  classification_loss: 1.1197 (1.1046)  loss_mask: 0.0000 (0.0000)  time: 0.1704  data: 0.0002  max mem: 6052
[06:46:53.470072] Epoch: [97]  [580/781]  eta: 0:00:34  lr: 0.000001  training_loss: 1.0976 (1.1051)  classification_loss: 1.0976 (1.1051)  loss_mask: 0.0000 (0.0000)  time: 0.1713  data: 0.0002  max mem: 6052
[06:46:56.942880] Epoch: [97]  [600/781]  eta: 0:00:31  lr: 0.000001  training_loss: 1.0879 (1.1057)  classification_loss: 1.0879 (1.1057)  loss_mask: 0.0000 (0.0000)  time: 0.1736  data: 0.0002  max mem: 6052
[06:47:00.495412] Epoch: [97]  [620/781]  eta: 0:00:27  lr: 0.000001  training_loss: 1.1045 (1.1059)  classification_loss: 1.1044 (1.1058)  loss_mask: 0.0000 (0.0000)  time: 0.1776  data: 0.0002  max mem: 6052
[06:47:03.901411] Epoch: [97]  [640/781]  eta: 0:00:24  lr: 0.000001  training_loss: 1.0592 (1.1060)  classification_loss: 1.0591 (1.1060)  loss_mask: 0.0000 (0.0000)  time: 0.1702  data: 0.0003  max mem: 6052
[06:47:07.369369] Epoch: [97]  [660/781]  eta: 0:00:20  lr: 0.000001  training_loss: 1.1139 (1.1064)  classification_loss: 1.1139 (1.1064)  loss_mask: 0.0000 (0.0000)  time: 0.1733  data: 0.0002  max mem: 6052
[06:47:10.787352] Epoch: [97]  [680/781]  eta: 0:00:17  lr: 0.000001  training_loss: 1.1255 (1.1068)  classification_loss: 1.1255 (1.1068)  loss_mask: 0.0000 (0.0000)  time: 0.1708  data: 0.0002  max mem: 6052

[06:47:14.193188] Epoch: [97]  [700/781]  eta: 0:00:14  lr: 0.000001  training_loss: 1.1224 (1.1068)  classification_loss: 1.1224 (1.1067)  loss_mask: 0.0000 (0.0000)  time: 0.1702  data: 0.0002  max mem: 6052
[06:47:17.619046] Epoch: [97]  [720/781]  eta: 0:00:10  lr: 0.000001  training_loss: 1.0870 (1.1073)  classification_loss: 1.0870 (1.1073)  loss_mask: 0.0000 (0.0000)  time: 0.1712  data: 0.0002  max mem: 6052
[06:47:21.031826] Epoch: [97]  [740/781]  eta: 0:00:07  lr: 0.000001  training_loss: 1.0888 (1.1073)  classification_loss: 1.0888 (1.1072)  loss_mask: 0.0000 (0.0000)  time: 0.1706  data: 0.0002  max mem: 6052
[06:47:24.439502] Epoch: [97]  [760/781]  eta: 0:00:03  lr: 0.000001  training_loss: 1.0946 (1.1077)  classification_loss: 1.0946 (1.1077)  loss_mask: 0.0000 (0.0000)  time: 0.1703  data: 0.0003  max mem: 6052
[06:47:27.848344] Epoch: [97]  [780/781]  eta: 0:00:00  lr: 0.000001  training_loss: 1.1387 (1.1087)  classification_loss: 1.1387 (1.1087)  loss_mask: 0.0000 (0.0000)  time: 0.1704  data: 0.0002  max mem: 6052
[06:47:28.023978] Epoch: [97] Total time: 0:02:15 (0.1730 s / it)
[06:47:28.024449] Averaged stats: lr: 0.000001  training_loss: 1.1387 (1.1087)  classification_loss: 1.1387 (1.1087)  loss_mask: 0.0000 (0.0000)
[06:47:28.725014] Test:  [  0/157]  eta: 0:01:49  testing_loss: 0.4493 (0.4493)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.6949  data: 0.6629  max mem: 6052
[06:47:29.013538] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.4143 (0.4422)  acc1: 89.0625 (87.0739)  acc5: 100.0000 (99.7159)  time: 0.0893  data: 0.0605  max mem: 6052
[06:47:29.301314] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4094 (0.4195)  acc1: 89.0625 (88.3929)  acc5: 100.0000 (99.5536)  time: 0.0287  data: 0.0002  max mem: 6052
[06:47:29.588946] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4055 (0.4265)  acc1: 89.0625 (88.2056)  acc5: 100.0000 (99.4960)  time: 0.0287  data: 0.0001  max mem: 6052
[06:47:29.873420] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4467 (0.4333)  acc1: 87.5000 (87.9573)  acc5: 98.4375 (99.3521)  time: 0.0285  data: 0.0001  max mem: 6052
[06:47:30.158187] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4338 (0.4299)  acc1: 87.5000 (88.0208)  acc5: 100.0000 (99.3873)  time: 0.0283  data: 0.0001  max mem: 6052
[06:47:30.441242] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4089 (0.4292)  acc1: 87.5000 (87.8586)  acc5: 100.0000 (99.3852)  time: 0.0283  data: 0.0001  max mem: 6052
[06:47:30.737608] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4102 (0.4257)  acc1: 87.5000 (87.9181)  acc5: 100.0000 (99.4058)  time: 0.0289  data: 0.0002  max mem: 6052
[06:47:31.022350] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4345 (0.4323)  acc1: 87.5000 (87.7701)  acc5: 100.0000 (99.4213)  time: 0.0289  data: 0.0002  max mem: 6052
[06:47:31.310460] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4345 (0.4297)  acc1: 87.5000 (87.8091)  acc5: 100.0000 (99.4162)  time: 0.0285  data: 0.0002  max mem: 6052
[06:47:31.601209] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.4115 (0.4324)  acc1: 87.5000 (87.6547)  acc5: 100.0000 (99.4121)  time: 0.0288  data: 0.0002  max mem: 6052
[06:47:31.884121] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4146 (0.4314)  acc1: 87.5000 (87.7111)  acc5: 100.0000 (99.3947)  time: 0.0286  data: 0.0001  max mem: 6052
[06:47:32.173640] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4070 (0.4317)  acc1: 87.5000 (87.7066)  acc5: 100.0000 (99.3802)  time: 0.0285  data: 0.0001  max mem: 6052
[06:47:32.463676] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.3843 (0.4321)  acc1: 87.5000 (87.7266)  acc5: 100.0000 (99.3798)  time: 0.0287  data: 0.0002  max mem: 6052
[06:47:32.744009] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3887 (0.4296)  acc1: 87.5000 (87.8435)  acc5: 100.0000 (99.4016)  time: 0.0283  data: 0.0001  max mem: 6052
[06:47:33.021725] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4115 (0.4297)  acc1: 87.5000 (87.7690)  acc5: 100.0000 (99.4309)  time: 0.0278  data: 0.0001  max mem: 6052
[06:47:33.170545] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4605 (0.4316)  acc1: 87.5000 (87.6800)  acc5: 100.0000 (99.4300)  time: 0.0268  data: 0.0001  max mem: 6052
[06:47:33.332685] Test: Total time: 0:00:05 (0.0338 s / it)
[06:47:33.333252] * Acc@1 87.680 Acc@5 99.430 loss 0.432
[06:47:33.333541] Accuracy of the network on the 10000 test images: 87.7%
[06:47:33.333713] Max accuracy: 87.68%
[06:47:33.484805] log_dir: ./output_dir
[06:47:34.353583] Epoch: [98]  [  0/781]  eta: 0:11:17  lr: 0.000001  training_loss: 1.0577 (1.0577)  classification_loss: 1.0577 (1.0577)  loss_mask: 0.0000 (0.0000)  time: 0.8671  data: 0.6891  max mem: 6052
[06:47:37.788276] Epoch: [98]  [ 20/781]  eta: 0:02:35  lr: 0.000001  training_loss: 1.0827 (1.0910)  classification_loss: 1.0826 (1.0910)  loss_mask: 0.0000 (0.0000)  time: 0.1717  data: 0.0002  max mem: 6052
[06:47:41.230049] Epoch: [98]  [ 40/781]  eta: 0:02:19  lr: 0.000001  training_loss: 1.1020 (1.1023)  classification_loss: 1.1019 (1.1023)  loss_mask: 0.0000 (0.0000)  time: 0.1720  data: 0.0004  max mem: 6052
[06:47:44.721768] Epoch: [98]  [ 60/781]  eta: 0:02:12  lr: 0.000001  training_loss: 1.0827 (1.1051)  classification_loss: 1.0827 (1.1051)  loss_mask: 0.0000 (0.0000)  time: 0.1745  data: 0.0002  max mem: 6052
[06:47:48.167874] Epoch: [98]  [ 80/781]  eta: 0:02:06  lr: 0.000001  training_loss: 1.0594 (1.0988)  classification_loss: 1.0594 (1.0988)  loss_mask: 0.0000 (0.0000)  time: 0.1722  data: 0.0003  max mem: 6052
[06:47:51.594797] Epoch: [98]  [100/781]  eta: 0:02:02  lr: 0.000001  training_loss: 1.1301 (1.1043)  classification_loss: 1.1301 (1.1043)  loss_mask: 0.0000 (0.0000)  time: 0.1713  data: 0.0002  max mem: 6052
[06:47:55.027665] Epoch: [98]  [120/781]  eta: 0:01:57  lr: 0.000001  training_loss: 1.1416 (1.1058)  classification_loss: 1.1416 (1.1058)  loss_mask: 0.0000 (0.0000)  time: 0.1716  data: 0.0002  max mem: 6052
[06:47:58.445570] Epoch: [98]  [140/781]  eta: 0:01:53  lr: 0.000001  training_loss: 1.0947 (1.1074)  classification_loss: 1.0946 (1.1073)  loss_mask: 0.0000 (0.0000)  time: 0.1708  data: 0.0003  max mem: 6052
[06:48:01.863289] Epoch: [98]  [160/781]  eta: 0:01:49  lr: 0.000001  training_loss: 1.0917 (1.1074)  classification_loss: 1.0917 (1.1074)  loss_mask: 0.0000 (0.0000)  time: 0.1708  data: 0.0002  max mem: 6052
[06:48:05.272255] Epoch: [98]  [180/781]  eta: 0:01:45  lr: 0.000001  training_loss: 1.1426 (1.1099)  classification_loss: 1.1426 (1.1099)  loss_mask: 0.0000 (0.0000)  time: 0.1704  data: 0.0002  max mem: 6052
[06:48:08.695737] Epoch: [98]  [200/781]  eta: 0:01:41  lr: 0.000001  training_loss: 1.0971 (1.1107)  classification_loss: 1.0971 (1.1107)  loss_mask: 0.0000 (0.0000)  time: 0.1711  data: 0.0002  max mem: 6052
[06:48:12.134639] Epoch: [98]  [220/781]  eta: 0:01:38  lr: 0.000001  training_loss: 1.1007 (1.1097)  classification_loss: 1.1007 (1.1097)  loss_mask: 0.0000 (0.0000)  time: 0.1719  data: 0.0002  max mem: 6052
[06:48:15.636608] Epoch: [98]  [240/781]  eta: 0:01:34  lr: 0.000001  training_loss: 1.1065 (1.1112)  classification_loss: 1.1065 (1.1112)  loss_mask: 0.0000 (0.0000)  time: 0.1750  data: 0.0002  max mem: 6052
[06:48:19.145897] Epoch: [98]  [260/781]  eta: 0:01:31  lr: 0.000001  training_loss: 1.1372 (1.1119)  classification_loss: 1.1371 (1.1119)  loss_mask: 0.0000 (0.0000)  time: 0.1754  data: 0.0002  max mem: 6052
[06:48:22.645119] Epoch: [98]  [280/781]  eta: 0:01:27  lr: 0.000001  training_loss: 1.0717 (1.1119)  classification_loss: 1.0717 (1.1119)  loss_mask: 0.0000 (0.0000)  time: 0.1749  data: 0.0003  max mem: 6052
[06:48:26.070284] Epoch: [98]  [300/781]  eta: 0:01:23  lr: 0.000001  training_loss: 1.0998 (1.1120)  classification_loss: 1.0998 (1.1120)  loss_mask: 0.0000 (0.0000)  time: 0.1712  data: 0.0002  max mem: 6052
[06:48:29.492283] Epoch: [98]  [320/781]  eta: 0:01:20  lr: 0.000001  training_loss: 1.1190 (1.1120)  classification_loss: 1.1190 (1.1120)  loss_mask: 0.0000 (0.0000)  time: 0.1710  data: 0.0002  max mem: 6052
[06:48:32.913790] Epoch: [98]  [340/781]  eta: 0:01:16  lr: 0.000001  training_loss: 1.1343 (1.1114)  classification_loss: 1.1343 (1.1114)  loss_mask: 0.0000 (0.0000)  time: 0.1710  data: 0.0002  max mem: 6052
[06:48:36.385595] Epoch: [98]  [360/781]  eta: 0:01:13  lr: 0.000001  training_loss: 1.1268 (1.1123)  classification_loss: 1.1267 (1.1123)  loss_mask: 0.0000 (0.0000)  time: 0.1735  data: 0.0002  max mem: 6052
[06:48:39.827515] Epoch: [98]  [380/781]  eta: 0:01:09  lr: 0.000001  training_loss: 1.0820 (1.1123)  classification_loss: 1.0820 (1.1123)  loss_mask: 0.0000 (0.0000)  time: 0.1720  data: 0.0003  max mem: 6052
[06:48:43.263601] Epoch: [98]  [400/781]  eta: 0:01:06  lr: 0.000001  training_loss: 1.1572 (1.1149)  classification_loss: 1.1572 (1.1148)  loss_mask: 0.0000 (0.0000)  time: 0.1717  data: 0.0002  max mem: 6052
[06:48:46.689673] Epoch: [98]  [420/781]  eta: 0:01:02  lr: 0.000001  training_loss: 1.1293 (1.1149)  classification_loss: 1.1293 (1.1148)  loss_mask: 0.0000 (0.0000)  time: 0.1712  data: 0.0002  max mem: 6052
[06:48:50.102629] Epoch: [98]  [440/781]  eta: 0:00:59  lr: 0.000001  training_loss: 1.0334 (1.1135)  classification_loss: 1.0333 (1.1134)  loss_mask: 0.0000 (0.0000)  time: 0.1706  data: 0.0002  max mem: 6052
[06:48:53.518295] Epoch: [98]  [460/781]  eta: 0:00:55  lr: 0.000001  training_loss: 1.0956 (1.1126)  classification_loss: 1.0955 (1.1126)  loss_mask: 0.0000 (0.0000)  time: 0.1707  data: 0.0002  max mem: 6052
[06:48:56.971239] Epoch: [98]  [480/781]  eta: 0:00:52  lr: 0.000001  training_loss: 1.1135 (1.1135)  classification_loss: 1.1135 (1.1134)  loss_mask: 0.0000 (0.0000)  time: 0.1726  data: 0.0002  max mem: 6052
[06:49:00.404287] Epoch: [98]  [500/781]  eta: 0:00:48  lr: 0.000001  training_loss: 1.0753 (1.1127)  classification_loss: 1.0753 (1.1127)  loss_mask: 0.0000 (0.0000)  time: 0.1715  data: 0.0002  max mem: 6052
[06:49:03.835025] Epoch: [98]  [520/781]  eta: 0:00:45  lr: 0.000001  training_loss: 1.1214 (1.1125)  classification_loss: 1.1214 (1.1124)  loss_mask: 0.0000 (0.0000)  time: 0.1715  data: 0.0002  max mem: 6052
[06:49:07.253815] Epoch: [98]  [540/781]  eta: 0:00:41  lr: 0.000001  training_loss: 1.1756 (1.1137)  classification_loss: 1.1755 (1.1137)  loss_mask: 0.0000 (0.0000)  time: 0.1708  data: 0.0002  max mem: 6052
[06:49:10.665838] Epoch: [98]  [560/781]  eta: 0:00:38  lr: 0.000001  training_loss: 1.1201 (1.1138)  classification_loss: 1.1200 (1.1138)  loss_mask: 0.0000 (0.0000)  time: 0.1704  data: 0.0003  max mem: 6052
[06:49:14.073725] Epoch: [98]  [580/781]  eta: 0:00:34  lr: 0.000001  training_loss: 1.0859 (1.1135)  classification_loss: 1.0859 (1.1135)  loss_mask: 0.0000 (0.0000)  time: 0.1703  data: 0.0002  max mem: 6052
[06:49:17.534435] Epoch: [98]  [600/781]  eta: 0:00:31  lr: 0.000001  training_loss: 1.0852 (1.1129)  classification_loss: 1.0852 (1.1129)  loss_mask: 0.0000 (0.0000)  time: 0.1730  data: 0.0003  max mem: 6052
[06:49:21.021630] Epoch: [98]  [620/781]  eta: 0:00:27  lr: 0.000001  training_loss: 1.1028 (1.1127)  classification_loss: 1.1028 (1.1127)  loss_mask: 0.0000 (0.0000)  time: 0.1742  data: 0.0002  max mem: 6052
[06:49:24.510907] Epoch: [98]  [640/781]  eta: 0:00:24  lr: 0.000001  training_loss: 1.0748 (1.1130)  classification_loss: 1.0748 (1.1129)  loss_mask: 0.0000 (0.0000)  time: 0.1744  data: 0.0002  max mem: 6052
[06:49:27.915227] Epoch: [98]  [660/781]  eta: 0:00:20  lr: 0.000001  training_loss: 1.1129 (1.1129)  classification_loss: 1.1129 (1.1129)  loss_mask: 0.0000 (0.0000)  time: 0.1701  data: 0.0002  max mem: 6052
[06:49:31.354262] Epoch: [98]  [680/781]  eta: 0:00:17  lr: 0.000001  training_loss: 1.1125 (1.1133)  classification_loss: 1.1125 (1.1133)  loss_mask: 0.0000 (0.0000)  time: 0.1719  data: 0.0004  max mem: 6052
[06:49:34.770729] Epoch: [98]  [700/781]  eta: 0:00:14  lr: 0.000001  training_loss: 1.1017 (1.1126)  classification_loss: 1.1017 (1.1126)  loss_mask: 0.0000 (0.0000)  time: 0.1707  data: 0.0001  max mem: 6052
[06:49:38.196398] Epoch: [98]  [720/781]  eta: 0:00:10  lr: 0.000001  training_loss: 1.1044 (1.1129)  classification_loss: 1.1044 (1.1129)  loss_mask: 0.0000 (0.0000)  time: 0.1711  data: 0.0003  max mem: 6052
[06:49:41.649956] Epoch: [98]  [740/781]  eta: 0:00:07  lr: 0.000001  training_loss: 1.0922 (1.1123)  classification_loss: 1.0922 (1.1123)  loss_mask: 0.0000 (0.0000)  time: 0.1726  data: 0.0002  max mem: 6052
[06:49:45.118811] Epoch: [98]  [760/781]  eta: 0:00:03  lr: 0.000001  training_loss: 1.1072 (1.1132)  classification_loss: 1.1072 (1.1131)  loss_mask: 0.0000 (0.0000)  time: 0.1733  data: 0.0003  max mem: 6052
[06:49:48.536647] Epoch: [98]  [780/781]  eta: 0:00:00  lr: 0.000001  training_loss: 1.1330 (1.1129)  classification_loss: 1.1330 (1.1129)  loss_mask: 0.0000 (0.0000)  time: 0.1708  data: 0.0001  max mem: 6052
[06:49:48.707350] Epoch: [98] Total time: 0:02:15 (0.1731 s / it)
[06:49:48.707839] Averaged stats: lr: 0.000001  training_loss: 1.1330 (1.1129)  classification_loss: 1.1330 (1.1129)  loss_mask: 0.0000 (0.0000)
[06:49:49.402709] Test:  [  0/157]  eta: 0:01:48  testing_loss: 0.4483 (0.4483)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.6899  data: 0.6585  max mem: 6052
[06:49:49.691332] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.4201 (0.4423)  acc1: 87.5000 (87.0739)  acc5: 100.0000 (99.7159)  time: 0.0888  data: 0.0600  max mem: 6052
[06:49:49.981044] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4030 (0.4190)  acc1: 89.0625 (88.1696)  acc5: 100.0000 (99.5536)  time: 0.0287  data: 0.0002  max mem: 6052
[06:49:50.265328] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4014 (0.4263)  acc1: 89.0625 (87.8528)  acc5: 100.0000 (99.4960)  time: 0.0286  data: 0.0002  max mem: 6052
[06:49:50.553710] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4434 (0.4330)  acc1: 87.5000 (87.6905)  acc5: 98.4375 (99.3521)  time: 0.0285  data: 0.0002  max mem: 6052
[06:49:50.837707] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4314 (0.4299)  acc1: 89.0625 (87.8370)  acc5: 100.0000 (99.3566)  time: 0.0285  data: 0.0002  max mem: 6052
[06:49:51.121003] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4037 (0.4290)  acc1: 89.0625 (87.6793)  acc5: 100.0000 (99.3596)  time: 0.0282  data: 0.0002  max mem: 6052
[06:49:51.408197] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4037 (0.4255)  acc1: 87.5000 (87.7201)  acc5: 100.0000 (99.3838)  time: 0.0284  data: 0.0002  max mem: 6052
[06:49:51.689463] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4367 (0.4324)  acc1: 87.5000 (87.5965)  acc5: 100.0000 (99.4020)  time: 0.0283  data: 0.0002  max mem: 6052
[06:49:51.975498] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4367 (0.4296)  acc1: 87.5000 (87.6889)  acc5: 100.0000 (99.3990)  time: 0.0282  data: 0.0002  max mem: 6052
[06:49:52.261160] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4093 (0.4325)  acc1: 87.5000 (87.5464)  acc5: 100.0000 (99.3967)  time: 0.0285  data: 0.0002  max mem: 6052
[06:49:52.545536] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4189 (0.4315)  acc1: 87.5000 (87.6126)  acc5: 100.0000 (99.3806)  time: 0.0283  data: 0.0002  max mem: 6052
[06:49:52.828824] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.3997 (0.4318)  acc1: 87.5000 (87.6291)  acc5: 100.0000 (99.3673)  time: 0.0282  data: 0.0002  max mem: 6052
[06:49:53.110262] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.3799 (0.4321)  acc1: 87.5000 (87.6431)  acc5: 100.0000 (99.3678)  time: 0.0281  data: 0.0002  max mem: 6052
[06:49:53.390983] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3880 (0.4296)  acc1: 87.5000 (87.7660)  acc5: 100.0000 (99.3905)  time: 0.0280  data: 0.0001  max mem: 6052
[06:49:53.671099] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4121 (0.4296)  acc1: 87.5000 (87.7070)  acc5: 100.0000 (99.4205)  time: 0.0279  data: 0.0001  max mem: 6052
[06:49:53.825095] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4510 (0.4313)  acc1: 87.5000 (87.6400)  acc5: 100.0000 (99.4200)  time: 0.0270  data: 0.0001  max mem: 6052
[06:49:53.996357] Test: Total time: 0:00:05 (0.0337 s / it)
[06:49:53.997716] * Acc@1 87.640 Acc@5 99.420 loss 0.431
[06:49:53.998547] Accuracy of the network on the 10000 test images: 87.6%
[06:49:53.999223] Max accuracy: 87.68%
[06:49:54.138609] log_dir: ./output_dir
[06:49:54.973941] Epoch: [99]  [  0/781]  eta: 0:10:50  lr: 0.000001  training_loss: 0.9243 (0.9243)  classification_loss: 0.9243 (0.9243)  loss_mask: 0.0000 (0.0000)  time: 0.8334  data: 0.6468  max mem: 6052
[06:49:58.416881] Epoch: [99]  [ 20/781]  eta: 0:02:34  lr: 0.000001  training_loss: 1.1053 (1.0960)  classification_loss: 1.1053 (1.0960)  loss_mask: 0.0000 (0.0000)  time: 0.1721  data: 0.0003  max mem: 6052
[06:50:01.844987] Epoch: [99]  [ 40/781]  eta: 0:02:19  lr: 0.000001  training_loss: 1.0658 (1.0889)  classification_loss: 1.0657 (1.0889)  loss_mask: 0.0000 (0.0000)  time: 0.1713  data: 0.0002  max mem: 6052
[06:50:05.276826] Epoch: [99]  [ 60/781]  eta: 0:02:11  lr: 0.000001  training_loss: 1.1266 (1.1065)  classification_loss: 1.1266 (1.1065)  loss_mask: 0.0000 (0.0000)  time: 0.1715  data: 0.0002  max mem: 6052
[06:50:08.699194] Epoch: [99]  [ 80/781]  eta: 0:02:05  lr: 0.000001  training_loss: 1.0643 (1.0989)  classification_loss: 1.0643 (1.0989)  loss_mask: 0.0000 (0.0000)  time: 0.1710  data: 0.0002  max mem: 6052
[06:50:12.127806] Epoch: [99]  [100/781]  eta: 0:02:01  lr: 0.000001  training_loss: 1.0932 (1.1019)  classification_loss: 1.0932 (1.1019)  loss_mask: 0.0000 (0.0000)  time: 0.1714  data: 0.0002  max mem: 6052
[06:50:15.542372] Epoch: [99]  [120/781]  eta: 0:01:56  lr: 0.000001  training_loss: 1.0904 (1.1068)  classification_loss: 1.0903 (1.1068)  loss_mask: 0.0000 (0.0000)  time: 0.1707  data: 0.0003  max mem: 6052
[06:50:18.981496] Epoch: [99]  [140/781]  eta: 0:01:52  lr: 0.000001  training_loss: 1.0751 (1.1073)  classification_loss: 1.0751 (1.1073)  loss_mask: 0.0000 (0.0000)  time: 0.1719  data: 0.0002  max mem: 6052
[06:50:22.403642] Epoch: [99]  [160/781]  eta: 0:01:48  lr: 0.000001  training_loss: 1.0537 (1.1032)  classification_loss: 1.0537 (1.1031)  loss_mask: 0.0000 (0.0000)  time: 0.1710  data: 0.0002  max mem: 6052
[06:50:25.821885] Epoch: [99]  [180/781]  eta: 0:01:45  lr: 0.000001  training_loss: 1.0809 (1.1051)  classification_loss: 1.0809 (1.1050)  loss_mask: 0.0000 (0.0000)  time: 0.1708  data: 0.0002  max mem: 6052
[06:50:29.232224] Epoch: [99]  [200/781]  eta: 0:01:41  lr: 0.000001  training_loss: 1.0856 (1.1041)  classification_loss: 1.0855 (1.1040)  loss_mask: 0.0000 (0.0000)  time: 0.1704  data: 0.0002  max mem: 6052
[06:50:32.664934] Epoch: [99]  [220/781]  eta: 0:01:37  lr: 0.000001  training_loss: 1.0742 (1.1027)  classification_loss: 1.0741 (1.1027)  loss_mask: 0.0000 (0.0000)  time: 0.1715  data: 0.0002  max mem: 6052
[06:50:36.085852] Epoch: [99]  [240/781]  eta: 0:01:34  lr: 0.000001  training_loss: 1.1044 (1.1057)  classification_loss: 1.1044 (1.1057)  loss_mask: 0.0000 (0.0000)  time: 0.1709  data: 0.0003  max mem: 6052
[06:50:39.496565] Epoch: [99]  [260/781]  eta: 0:01:30  lr: 0.000001  training_loss: 1.1017 (1.1064)  classification_loss: 1.1017 (1.1064)  loss_mask: 0.0000 (0.0000)  time: 0.1704  data: 0.0002  max mem: 6052
[06:50:42.913733] Epoch: [99]  [280/781]  eta: 0:01:26  lr: 0.000001  training_loss: 1.1153 (1.1056)  classification_loss: 1.1153 (1.1056)  loss_mask: 0.0000 (0.0000)  time: 0.1708  data: 0.0002  max mem: 6052
[06:50:46.343976] Epoch: [99]  [300/781]  eta: 0:01:23  lr: 0.000001  training_loss: 1.0815 (1.1042)  classification_loss: 1.0815 (1.1042)  loss_mask: 0.0000 (0.0000)  time: 0.1714  data: 0.0003  max mem: 6052
[06:50:49.785200] Epoch: [99]  [320/781]  eta: 0:01:19  lr: 0.000001  training_loss: 1.1338 (1.1072)  classification_loss: 1.1337 (1.1072)  loss_mask: 0.0000 (0.0000)  time: 0.1720  data: 0.0003  max mem: 6052
[06:50:53.258590] Epoch: [99]  [340/781]  eta: 0:01:16  lr: 0.000001  training_loss: 1.0625 (1.1059)  classification_loss: 1.0625 (1.1058)  loss_mask: 0.0000 (0.0000)  time: 0.1736  data: 0.0002  max mem: 6052
[06:50:56.691232] Epoch: [99]  [360/781]  eta: 0:01:12  lr: 0.000001  training_loss: 1.1636 (1.1079)  classification_loss: 1.1635 (1.1078)  loss_mask: 0.0000 (0.0000)  time: 0.1716  data: 0.0003  max mem: 6052
[06:51:00.098404] Epoch: [99]  [380/781]  eta: 0:01:09  lr: 0.000001  training_loss: 1.1440 (1.1093)  classification_loss: 1.1440 (1.1092)  loss_mask: 0.0000 (0.0000)  time: 0.1703  data: 0.0002  max mem: 6052
[06:51:03.502129] Epoch: [99]  [400/781]  eta: 0:01:05  lr: 0.000001  training_loss: 1.0987 (1.1097)  classification_loss: 1.0987 (1.1097)  loss_mask: 0.0000 (0.0000)  time: 0.1701  data: 0.0003  max mem: 6052
[06:51:06.965282] Epoch: [99]  [420/781]  eta: 0:01:02  lr: 0.000001  training_loss: 1.1104 (1.1106)  classification_loss: 1.1104 (1.1106)  loss_mask: 0.0000 (0.0000)  time: 0.1730  data: 0.0002  max mem: 6052
[06:51:10.424370] Epoch: [99]  [440/781]  eta: 0:00:58  lr: 0.000001  training_loss: 1.0929 (1.1102)  classification_loss: 1.0929 (1.1101)  loss_mask: 0.0000 (0.0000)  time: 0.1728  data: 0.0002  max mem: 6052
[06:51:13.823071] Epoch: [99]  [460/781]  eta: 0:00:55  lr: 0.000001  training_loss: 1.0850 (1.1096)  classification_loss: 1.0741 (1.1094)  loss_mask: 0.0000 (0.0002)  time: 0.1698  data: 0.0002  max mem: 6052
[06:51:17.242814] Epoch: [99]  [480/781]  eta: 0:00:51  lr: 0.000001  training_loss: 1.1177 (1.1102)  classification_loss: 1.1177 (1.1100)  loss_mask: 0.0000 (0.0002)  time: 0.1709  data: 0.0002  max mem: 6052
[06:51:20.673747] Epoch: [99]  [500/781]  eta: 0:00:48  lr: 0.000001  training_loss: 1.0433 (1.1091)  classification_loss: 1.0433 (1.1089)  loss_mask: 0.0000 (0.0002)  time: 0.1715  data: 0.0002  max mem: 6052
[06:51:24.093577] Epoch: [99]  [520/781]  eta: 0:00:45  lr: 0.000001  training_loss: 1.1368 (1.1097)  classification_loss: 1.1368 (1.1095)  loss_mask: 0.0000 (0.0002)  time: 0.1709  data: 0.0003  max mem: 6052
[06:51:27.505548] Epoch: [99]  [540/781]  eta: 0:00:41  lr: 0.000001  training_loss: 1.0654 (1.1084)  classification_loss: 1.0654 (1.1082)  loss_mask: 0.0000 (0.0002)  time: 0.1705  data: 0.0002  max mem: 6052
[06:51:30.901655] Epoch: [99]  [560/781]  eta: 0:00:38  lr: 0.000001  training_loss: 1.0824 (1.1080)  classification_loss: 1.0824 (1.1078)  loss_mask: 0.0000 (0.0002)  time: 0.1697  data: 0.0003  max mem: 6052
[06:51:34.325153] Epoch: [99]  [580/781]  eta: 0:00:34  lr: 0.000001  training_loss: 1.0733 (1.1074)  classification_loss: 1.0733 (1.1072)  loss_mask: 0.0000 (0.0002)  time: 0.1711  data: 0.0002  max mem: 6052
[06:51:37.748749] Epoch: [99]  [600/781]  eta: 0:00:31  lr: 0.000001  training_loss: 1.0946 (1.1073)  classification_loss: 1.0945 (1.1071)  loss_mask: 0.0000 (0.0002)  time: 0.1711  data: 0.0003  max mem: 6052
[06:51:41.160621] Epoch: [99]  [620/781]  eta: 0:00:27  lr: 0.000001  training_loss: 1.0973 (1.1079)  classification_loss: 1.0973 (1.1078)  loss_mask: 0.0000 (0.0002)  time: 0.1705  data: 0.0003  max mem: 6052
[06:51:44.570829] Epoch: [99]  [640/781]  eta: 0:00:24  lr: 0.000001  training_loss: 1.1075 (1.1088)  classification_loss: 1.1074 (1.1087)  loss_mask: 0.0000 (0.0002)  time: 0.1704  data: 0.0003  max mem: 6052
[06:51:47.997482] Epoch: [99]  [660/781]  eta: 0:00:20  lr: 0.000001  training_loss: 1.1162 (1.1086)  classification_loss: 1.1161 (1.1084)  loss_mask: 0.0000 (0.0002)  time: 0.1712  data: 0.0002  max mem: 6052
[06:51:51.418975] Epoch: [99]  [680/781]  eta: 0:00:17  lr: 0.000001  training_loss: 1.1423 (1.1091)  classification_loss: 1.1423 (1.1089)  loss_mask: 0.0000 (0.0002)  time: 0.1710  data: 0.0003  max mem: 6052
[06:51:54.838568] Epoch: [99]  [700/781]  eta: 0:00:13  lr: 0.000001  training_loss: 1.1283 (1.1100)  classification_loss: 1.1283 (1.1098)  loss_mask: 0.0000 (0.0002)  time: 0.1709  data: 0.0003  max mem: 6052
[06:51:58.275323] Epoch: [99]  [720/781]  eta: 0:00:10  lr: 0.000001  training_loss: 1.1405 (1.1116)  classification_loss: 1.1405 (1.1114)  loss_mask: 0.0000 (0.0002)  time: 0.1717  data: 0.0002  max mem: 6052
[06:52:01.687401] Epoch: [99]  [740/781]  eta: 0:00:07  lr: 0.000001  training_loss: 1.1299 (1.1121)  classification_loss: 1.1299 (1.1119)  loss_mask: 0.0000 (0.0001)  time: 0.1705  data: 0.0002  max mem: 6052
[06:52:05.101426] Epoch: [99]  [760/781]  eta: 0:00:03  lr: 0.000001  training_loss: 1.1080 (1.1122)  classification_loss: 1.1080 (1.1120)  loss_mask: 0.0000 (0.0001)  time: 0.1706  data: 0.0002  max mem: 6052
[06:52:08.513358] Epoch: [99]  [780/781]  eta: 0:00:00  lr: 0.000001  training_loss: 1.1155 (1.1127)  classification_loss: 1.1155 (1.1126)  loss_mask: 0.0000 (0.0001)  time: 0.1705  data: 0.0001  max mem: 6052
[06:52:08.681097] Epoch: [99] Total time: 0:02:14 (0.1723 s / it)
[06:52:08.681557] Averaged stats: lr: 0.000001  training_loss: 1.1155 (1.1127)  classification_loss: 1.1155 (1.1126)  loss_mask: 0.0000 (0.0001)
[06:52:09.367099] Test:  [  0/157]  eta: 0:01:46  testing_loss: 0.4442 (0.4442)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.6811  data: 0.6512  max mem: 6052
[06:52:09.649030] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4208 (0.4431)  acc1: 89.0625 (87.0739)  acc5: 100.0000 (99.7159)  time: 0.0874  data: 0.0594  max mem: 6052
[06:52:09.930776] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4102 (0.4210)  acc1: 89.0625 (88.0208)  acc5: 100.0000 (99.5536)  time: 0.0280  data: 0.0001  max mem: 6052
[06:52:10.212112] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4042 (0.4274)  acc1: 89.0625 (87.6008)  acc5: 100.0000 (99.4960)  time: 0.0280  data: 0.0002  max mem: 6052
[06:52:10.496081] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4455 (0.4342)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (99.3521)  time: 0.0281  data: 0.0002  max mem: 6052
[06:52:10.800971] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4320 (0.4312)  acc1: 89.0625 (87.6225)  acc5: 100.0000 (99.3873)  time: 0.0293  data: 0.0002  max mem: 6052
[06:52:11.087131] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4073 (0.4302)  acc1: 87.5000 (87.4744)  acc5: 100.0000 (99.3596)  time: 0.0294  data: 0.0002  max mem: 6052
[06:52:11.377199] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4073 (0.4264)  acc1: 87.5000 (87.5660)  acc5: 100.0000 (99.3618)  time: 0.0287  data: 0.0003  max mem: 6052
[06:52:11.658008] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4313 (0.4333)  acc1: 87.5000 (87.4614)  acc5: 100.0000 (99.4020)  time: 0.0284  data: 0.0003  max mem: 6052
[06:52:11.938928] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4326 (0.4304)  acc1: 87.5000 (87.5172)  acc5: 100.0000 (99.3990)  time: 0.0280  data: 0.0002  max mem: 6052
[06:52:12.219539] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4120 (0.4333)  acc1: 87.5000 (87.3762)  acc5: 100.0000 (99.3967)  time: 0.0280  data: 0.0002  max mem: 6052
[06:52:12.500156] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4215 (0.4321)  acc1: 87.5000 (87.4578)  acc5: 100.0000 (99.3806)  time: 0.0280  data: 0.0002  max mem: 6052
[06:52:12.780630] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4025 (0.4324)  acc1: 87.5000 (87.4871)  acc5: 100.0000 (99.3673)  time: 0.0279  data: 0.0002  max mem: 6052
[06:52:13.061200] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.3799 (0.4327)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (99.3678)  time: 0.0279  data: 0.0002  max mem: 6052
[06:52:13.340318] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3863 (0.4302)  acc1: 87.5000 (87.6219)  acc5: 100.0000 (99.3905)  time: 0.0279  data: 0.0001  max mem: 6052
[06:52:13.618296] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4112 (0.4302)  acc1: 87.5000 (87.5621)  acc5: 100.0000 (99.4205)  time: 0.0277  data: 0.0001  max mem: 6052
[06:52:13.768036] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4546 (0.4319)  acc1: 87.5000 (87.4800)  acc5: 100.0000 (99.4200)  time: 0.0268  data: 0.0001  max mem: 6052
[06:52:13.936702] Test: Total time: 0:00:05 (0.0335 s / it)
[06:52:13.937213] * Acc@1 87.480 Acc@5 99.420 loss 0.432
[06:52:13.937650] Accuracy of the network on the 10000 test images: 87.5%
[06:52:13.937850] Max accuracy: 87.68%
[06:52:14.165071] Training time 3:53:42