Not using distributed mode
[17:29:46.376382] job dir: /notebooks/CVPR2023
[17:29:46.376727] Namespace(batch_size=64,
epochs=100,
accum_iter=1,
model='mae_vit_tiny',
norm_pix_loss=False,
dataset='c10',
input_size=32,
patch_size=2,
mask_ratio=0.75,
lambda_weight=0.1,
drop_path=0.1,
clip_grad=None,
weight_decay=0.05,
lr=None,
blr=0.001,
layer_decay=0.75,
min_lr=1e-06,
warmup_epochs=5,
color_jitter=None,
aa='rand-m9-mstd0.5-inc1',
smoothing=0.1,
reprob=0.25,
remode='pixel',
recount=1,
resplit=False,
mixup=0,
cutmix=0,
cutmix_minmax=None,
mixup_prob=1.0,
mixup_switch_prob=0.5,
mixup_mode='batch',
finetune='',
global_pool=True,
data_path='/datasets01/imagenet_full_size/061417/',
nb_classes=10,
output_dir='./output_dir',
log_dir='./output_dir',
device='cuda',
seed=0,
resume='',
start_epoch=0,
eval=False,
dist_eval=False,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[17:29:46.808357] Files already downloaded and verified
[17:29:47.659486] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCropAndInterpolation(size=(32, 32), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic)
               RandomHorizontalFlip(p=0.5)
               RandAugment(n=2, ops=
           	AugmentOp(name=AutoContrast, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Equalize, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Invert, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Rotate, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=PosterizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeAdd, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ColorIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ContrastIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=BrightnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SharpnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearX, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearY, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateXRel, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateYRel, p=0.5, m=9, mstd=0.5))
               ToTensor()
               Normalize(mean=tensor([0.4914, 0.4822, 0.4465]), std=tensor([0.2023, 0.1994, 0.2010]))
               RandomErasing(p=0.25, mode=pixel, count=(1, 1))
           )
/usr/local/lib/python3.9/dist-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
[17:29:48.055489] Files already downloaded and verified
[17:29:48.509017] Dataset CIFAR10
    Number of datapoints: 10000
    Root location: ./data
    Split: Test
    StandardTransform
Transform: Compose(
               Resize(size=36, interpolation=bicubic, max_size=None, antialias=None)
               CenterCrop(size=(32, 32))
               ToTensor()
               Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.201))
           )
[17:29:48.509700] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7fd67d72d160>
/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[17:29:54.444598] Model = MaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(2, 2), stride=(2, 2))
    (norm): Identity()
  )
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (7): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (8): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (9): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (10): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (11): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
  (decoder_embed): Linear(in_features=192, out_features=128, bias=True)
  (decoder_blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=128, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=128, out_features=128, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=512, out_features=128, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=128, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=128, out_features=128, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=512, out_features=128, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (decoder_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
  (decoder_pred): Linear(in_features=128, out_features=12, bias=True)
  (head): Linear(in_features=192, out_features=10, bias=True)
  (classifier_mask): Sequential(
    (0): Linear(in_features=192, out_features=5, bias=True)
    (1): LogSoftmax(dim=1)
  )
)
[17:29:54.445393] number of params (M): 5.77
[17:29:54.445669] base lr: 1.00e-03
[17:29:54.445882] actual lr: 2.50e-04
[17:29:54.446070] accumulate grad iterations: 1
[17:29:54.446271] effective batch size: 64
[17:29:54.448035] criterion = LabelSmoothingCrossEntropy()
[17:29:54.448442] Start training for 100 epochs
[17:29:54.450091] log_dir: ./output_dir
[17:29:57.422356] Epoch: [0]  [  0/781]  eta: 0:38:40  lr: 0.000000  training_loss: 4.6201 (4.6201)  classification_loss: 2.6555 (2.6555)  loss_mask: 1.9646 (1.9646)  time: 2.9706  data: 0.6022  max mem: 4070
[17:30:00.725326] Epoch: [0]  [ 20/781]  eta: 0:03:47  lr: 0.000001  training_loss: 4.5130 (4.5311)  classification_loss: 2.5804 (2.5906)  loss_mask: 1.9226 (1.9405)  time: 0.1651  data: 0.0003  max mem: 4130
[17:30:04.025180] Epoch: [0]  [ 40/781]  eta: 0:02:52  lr: 0.000003  training_loss: 4.2610 (4.4132)  classification_loss: 2.4598 (2.5276)  loss_mask: 1.8026 (1.8855)  time: 0.1649  data: 0.0003  max mem: 4130
[17:30:07.330788] Epoch: [0]  [ 60/781]  eta: 0:02:32  lr: 0.000004  training_loss: 4.0671 (4.3019)  classification_loss: 2.3609 (2.4745)  loss_mask: 1.7067 (1.8274)  time: 0.1652  data: 0.0003  max mem: 4130
[17:30:10.615246] Epoch: [0]  [ 80/781]  eta: 0:02:19  lr: 0.000005  training_loss: 4.0020 (4.2290)  classification_loss: 2.3204 (2.4362)  loss_mask: 1.6793 (1.7928)  time: 0.1641  data: 0.0002  max mem: 4130
[17:30:13.911894] Epoch: [0]  [100/781]  eta: 0:02:11  lr: 0.000006  training_loss: 3.9629 (4.1755)  classification_loss: 2.2978 (2.4090)  loss_mask: 1.6630 (1.7665)  time: 0.1647  data: 0.0005  max mem: 4130
[17:30:17.215247] Epoch: [0]  [120/781]  eta: 0:02:04  lr: 0.000008  training_loss: 3.9215 (4.1327)  classification_loss: 2.2914 (2.3899)  loss_mask: 1.6177 (1.7428)  time: 0.1650  data: 0.0003  max mem: 4130
[17:30:20.531950] Epoch: [0]  [140/781]  eta: 0:01:58  lr: 0.000009  training_loss: 3.8962 (4.1001)  classification_loss: 2.2650 (2.3732)  loss_mask: 1.6306 (1.7269)  time: 0.1657  data: 0.0002  max mem: 4130
[17:30:23.832304] Epoch: [0]  [160/781]  eta: 0:01:53  lr: 0.000010  training_loss: 3.8840 (4.0718)  classification_loss: 2.2648 (2.3597)  loss_mask: 1.6142 (1.7121)  time: 0.1649  data: 0.0003  max mem: 4130
[17:30:27.172253] Epoch: [0]  [180/781]  eta: 0:01:48  lr: 0.000012  training_loss: 3.8771 (4.0501)  classification_loss: 2.2657 (2.3496)  loss_mask: 1.5966 (1.7005)  time: 0.1669  data: 0.0003  max mem: 4130
[17:30:30.488749] Epoch: [0]  [200/781]  eta: 0:01:44  lr: 0.000013  training_loss: 3.8603 (4.0298)  classification_loss: 2.2394 (2.3393)  loss_mask: 1.5951 (1.6906)  time: 0.1657  data: 0.0002  max mem: 4130
[17:30:33.808069] Epoch: [0]  [220/781]  eta: 0:01:39  lr: 0.000014  training_loss: 3.8296 (4.0128)  classification_loss: 2.2479 (2.3316)  loss_mask: 1.5846 (1.6812)  time: 0.1659  data: 0.0003  max mem: 4130
[17:30:37.106299] Epoch: [0]  [240/781]  eta: 0:01:35  lr: 0.000015  training_loss: 3.8385 (3.9993)  classification_loss: 2.2661 (2.3262)  loss_mask: 1.5878 (1.6731)  time: 0.1648  data: 0.0003  max mem: 4130
[17:30:40.445331] Epoch: [0]  [260/781]  eta: 0:01:31  lr: 0.000017  training_loss: 3.8364 (3.9858)  classification_loss: 2.2483 (2.3204)  loss_mask: 1.5776 (1.6654)  time: 0.1669  data: 0.0003  max mem: 4130
[17:30:43.750193] Epoch: [0]  [280/781]  eta: 0:01:27  lr: 0.000018  training_loss: 3.7986 (3.9734)  classification_loss: 2.2514 (2.3154)  loss_mask: 1.5646 (1.6580)  time: 0.1651  data: 0.0003  max mem: 4130
[17:30:47.048020] Epoch: [0]  [300/781]  eta: 0:01:24  lr: 0.000019  training_loss: 3.8307 (3.9634)  classification_loss: 2.2600 (2.3116)  loss_mask: 1.5631 (1.6518)  time: 0.1648  data: 0.0003  max mem: 4130
[17:30:50.359327] Epoch: [0]  [320/781]  eta: 0:01:20  lr: 0.000020  training_loss: 3.7824 (3.9534)  classification_loss: 2.2527 (2.3079)  loss_mask: 1.5451 (1.6455)  time: 0.1655  data: 0.0003  max mem: 4130
[17:30:53.702189] Epoch: [0]  [340/781]  eta: 0:01:16  lr: 0.000022  training_loss: 3.7953 (3.9447)  classification_loss: 2.2558 (2.3042)  loss_mask: 1.5541 (1.6405)  time: 0.1670  data: 0.0003  max mem: 4130
[17:30:57.024176] Epoch: [0]  [360/781]  eta: 0:01:12  lr: 0.000023  training_loss: 3.7753 (3.9351)  classification_loss: 2.2565 (2.3008)  loss_mask: 1.5238 (1.6343)  time: 0.1660  data: 0.0003  max mem: 4130
[17:31:00.314409] Epoch: [0]  [380/781]  eta: 0:01:09  lr: 0.000024  training_loss: 3.7676 (3.9276)  classification_loss: 2.2525 (2.2981)  loss_mask: 1.5534 (1.6294)  time: 0.1644  data: 0.0003  max mem: 4130
[17:31:03.702411] Epoch: [0]  [400/781]  eta: 0:01:05  lr: 0.000026  training_loss: 3.7248 (3.9187)  classification_loss: 2.2314 (2.2948)  loss_mask: 1.5101 (1.6239)  time: 0.1693  data: 0.0004  max mem: 4130
[17:31:07.056586] Epoch: [0]  [420/781]  eta: 0:01:02  lr: 0.000027  training_loss: 3.7660 (3.9112)  classification_loss: 2.2554 (2.2933)  loss_mask: 1.4913 (1.6179)  time: 0.1676  data: 0.0004  max mem: 4130
[17:31:10.335941] Epoch: [0]  [440/781]  eta: 0:00:58  lr: 0.000028  training_loss: 3.7294 (3.9037)  classification_loss: 2.2476 (2.2908)  loss_mask: 1.5075 (1.6129)  time: 0.1639  data: 0.0003  max mem: 4130
[17:31:13.621999] Epoch: [0]  [460/781]  eta: 0:00:55  lr: 0.000029  training_loss: 3.6863 (3.8944)  classification_loss: 2.2010 (2.2871)  loss_mask: 1.4726 (1.6073)  time: 0.1642  data: 0.0002  max mem: 4130
[17:31:16.920727] Epoch: [0]  [480/781]  eta: 0:00:51  lr: 0.000031  training_loss: 3.7134 (3.8873)  classification_loss: 2.2467 (2.2857)  loss_mask: 1.4597 (1.6016)  time: 0.1649  data: 0.0003  max mem: 4130
[17:31:20.232900] Epoch: [0]  [500/781]  eta: 0:00:48  lr: 0.000032  training_loss: 3.6705 (3.8792)  classification_loss: 2.2395 (2.2838)  loss_mask: 1.4212 (1.5954)  time: 0.1655  data: 0.0003  max mem: 4130
[17:31:23.539207] Epoch: [0]  [520/781]  eta: 0:00:44  lr: 0.000033  training_loss: 3.6118 (3.8688)  classification_loss: 2.2397 (2.2822)  loss_mask: 1.3724 (1.5867)  time: 0.1652  data: 0.0004  max mem: 4130
[17:31:26.879776] Epoch: [0]  [540/781]  eta: 0:00:41  lr: 0.000035  training_loss: 3.6180 (3.8593)  classification_loss: 2.2332 (2.2809)  loss_mask: 1.3528 (1.5784)  time: 0.1669  data: 0.0003  max mem: 4130
[17:31:30.187532] Epoch: [0]  [560/781]  eta: 0:00:37  lr: 0.000036  training_loss: 3.5008 (3.8474)  classification_loss: 2.2339 (2.2794)  loss_mask: 1.2803 (1.5680)  time: 0.1653  data: 0.0003  max mem: 4130
[17:31:33.512239] Epoch: [0]  [580/781]  eta: 0:00:34  lr: 0.000037  training_loss: 3.4951 (3.8348)  classification_loss: 2.2186 (2.2774)  loss_mask: 1.2415 (1.5575)  time: 0.1661  data: 0.0003  max mem: 4130
[17:31:36.832360] Epoch: [0]  [600/781]  eta: 0:00:30  lr: 0.000038  training_loss: 3.4505 (3.8227)  classification_loss: 2.2054 (2.2754)  loss_mask: 1.2608 (1.5473)  time: 0.1658  data: 0.0003  max mem: 4130
[17:31:40.151910] Epoch: [0]  [620/781]  eta: 0:00:27  lr: 0.000040  training_loss: 3.3766 (3.8088)  classification_loss: 2.2251 (2.2739)  loss_mask: 1.1608 (1.5349)  time: 0.1659  data: 0.0003  max mem: 4130
[17:31:43.437798] Epoch: [0]  [640/781]  eta: 0:00:23  lr: 0.000041  training_loss: 3.3257 (3.7943)  classification_loss: 2.2145 (2.2727)  loss_mask: 1.1033 (1.5216)  time: 0.1641  data: 0.0004  max mem: 4130
[17:31:46.719516] Epoch: [0]  [660/781]  eta: 0:00:20  lr: 0.000042  training_loss: 3.3043 (3.7801)  classification_loss: 2.2172 (2.2713)  loss_mask: 1.0999 (1.5088)  time: 0.1639  data: 0.0003  max mem: 4130
[17:31:50.041228] Epoch: [0]  [680/781]  eta: 0:00:17  lr: 0.000044  training_loss: 3.3457 (3.7677)  classification_loss: 2.2040 (2.2698)  loss_mask: 1.1086 (1.4979)  time: 0.1660  data: 0.0003  max mem: 4130
[17:31:53.366761] Epoch: [0]  [700/781]  eta: 0:00:13  lr: 0.000045  training_loss: 3.3272 (3.7550)  classification_loss: 2.2001 (2.2681)  loss_mask: 1.0911 (1.4868)  time: 0.1662  data: 0.0003  max mem: 4130
[17:31:56.676097] Epoch: [0]  [720/781]  eta: 0:00:10  lr: 0.000046  training_loss: 3.2590 (3.7418)  classification_loss: 2.2423 (2.2673)  loss_mask: 1.0309 (1.4745)  time: 0.1654  data: 0.0003  max mem: 4130
[17:31:59.966448] Epoch: [0]  [740/781]  eta: 0:00:06  lr: 0.000047  training_loss: 3.2228 (3.7285)  classification_loss: 2.2248 (2.2661)  loss_mask: 1.0229 (1.4624)  time: 0.1644  data: 0.0003  max mem: 4130
[17:32:03.329792] Epoch: [0]  [760/781]  eta: 0:00:03  lr: 0.000049  training_loss: 3.1935 (3.7141)  classification_loss: 2.2167 (2.2651)  loss_mask: 0.9564 (1.4491)  time: 0.1681  data: 0.0003  max mem: 4130
[17:32:06.611366] Epoch: [0]  [780/781]  eta: 0:00:00  lr: 0.000050  training_loss: 3.2053 (3.7002)  classification_loss: 2.2118 (2.2639)  loss_mask: 0.9355 (1.4363)  time: 0.1640  data: 0.0003  max mem: 4130
[17:32:06.737033] Epoch: [0] Total time: 0:02:12 (0.1694 s / it)
[17:32:06.737517] Averaged stats: lr: 0.000050  training_loss: 3.2053 (3.7002)  classification_loss: 2.2118 (2.2639)  loss_mask: 0.9355 (1.4363)
[17:32:08.157377] Test:  [  0/157]  eta: 0:01:57  testing_loss: 1.9529 (1.9529)  acc1: 37.5000 (37.5000)  acc5: 89.0625 (89.0625)  time: 0.7454  data: 0.7108  max mem: 4130
[17:32:08.453269] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 2.0365 (2.0286)  acc1: 28.1250 (27.9830)  acc5: 82.8125 (83.0966)  time: 0.0943  data: 0.0649  max mem: 4130
[17:32:08.743285] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 2.0347 (2.0249)  acc1: 28.1250 (29.3899)  acc5: 82.8125 (82.0685)  time: 0.0290  data: 0.0003  max mem: 4130
[17:32:09.031441] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 2.0040 (2.0156)  acc1: 31.2500 (30.0403)  acc5: 81.2500 (81.8548)  time: 0.0287  data: 0.0002  max mem: 4130
[17:32:09.322274] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 2.0048 (2.0160)  acc1: 29.6875 (29.8018)  acc5: 79.6875 (81.6692)  time: 0.0288  data: 0.0002  max mem: 4130
[17:32:09.611758] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 2.0204 (2.0166)  acc1: 29.6875 (29.7488)  acc5: 81.2500 (82.0159)  time: 0.0289  data: 0.0002  max mem: 4130
[17:32:09.900929] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 2.0131 (2.0158)  acc1: 29.6875 (29.8924)  acc5: 85.9375 (82.5051)  time: 0.0288  data: 0.0002  max mem: 4130
[17:32:10.190116] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 2.0081 (2.0143)  acc1: 29.6875 (29.6215)  acc5: 84.3750 (82.6364)  time: 0.0288  data: 0.0002  max mem: 4130
[17:32:10.481738] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 2.0042 (2.0136)  acc1: 28.1250 (29.5139)  acc5: 84.3750 (83.1597)  time: 0.0288  data: 0.0002  max mem: 4130
[17:32:10.772541] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 2.0371 (2.0165)  acc1: 28.1250 (29.3269)  acc5: 84.3750 (83.0357)  time: 0.0289  data: 0.0002  max mem: 4130
[17:32:11.067581] Test:  [100/157]  eta: 0:00:02  testing_loss: 2.0353 (2.0167)  acc1: 28.1250 (29.3162)  acc5: 84.3750 (83.1374)  time: 0.0290  data: 0.0003  max mem: 4130
[17:32:11.360834] Test:  [110/157]  eta: 0:00:01  testing_loss: 2.0260 (2.0172)  acc1: 28.1250 (29.3074)  acc5: 82.8125 (83.0659)  time: 0.0292  data: 0.0003  max mem: 4130
[17:32:11.651526] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.9984 (2.0144)  acc1: 29.6875 (29.3518)  acc5: 82.8125 (83.1741)  time: 0.0290  data: 0.0003  max mem: 4130
[17:32:11.940189] Test:  [130/157]  eta: 0:00:00  testing_loss: 2.0129 (2.0157)  acc1: 29.6875 (29.4370)  acc5: 82.8125 (82.9914)  time: 0.0288  data: 0.0003  max mem: 4130
[17:32:12.228389] Test:  [140/157]  eta: 0:00:00  testing_loss: 2.0240 (2.0152)  acc1: 29.6875 (29.4215)  acc5: 81.2500 (83.0341)  time: 0.0287  data: 0.0003  max mem: 4130
[17:32:12.515278] Test:  [150/157]  eta: 0:00:00  testing_loss: 2.0079 (2.0144)  acc1: 29.6875 (29.5012)  acc5: 82.8125 (83.2057)  time: 0.0286  data: 0.0002  max mem: 4130
[17:32:12.782585] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.9907 (2.0136)  acc1: 29.6875 (29.5000)  acc5: 85.9375 (83.3400)  time: 0.0332  data: 0.0002  max mem: 4130
[17:32:12.948311] Test: Total time: 0:00:05 (0.0353 s / it)
[17:32:12.948899] * Acc@1 29.500 Acc@5 83.340 loss 2.014
[17:32:12.949205] Accuracy of the network on the 10000 test images: 29.5%
[17:32:12.949386] Max accuracy: 29.50%
[17:32:13.178290] log_dir: ./output_dir
[17:32:14.099892] Epoch: [1]  [  0/781]  eta: 0:11:58  lr: 0.000050  training_loss: 3.2975 (3.2975)  classification_loss: 2.2184 (2.2184)  loss_mask: 1.0791 (1.0791)  time: 0.9199  data: 0.7027  max mem: 4132
[17:32:17.370421] Epoch: [1]  [ 20/781]  eta: 0:02:31  lr: 0.000051  training_loss: 3.2051 (3.2156)  classification_loss: 2.2090 (2.2160)  loss_mask: 0.9945 (0.9996)  time: 0.1634  data: 0.0002  max mem: 4132
[17:32:20.668439] Epoch: [1]  [ 40/781]  eta: 0:02:15  lr: 0.000053  training_loss: 3.1539 (3.1844)  classification_loss: 2.2111 (2.2222)  loss_mask: 0.9062 (0.9622)  time: 0.1648  data: 0.0003  max mem: 4132
[17:32:23.997937] Epoch: [1]  [ 60/781]  eta: 0:02:07  lr: 0.000054  training_loss: 3.0910 (3.1661)  classification_loss: 2.2456 (2.2259)  loss_mask: 0.8846 (0.9403)  time: 0.1664  data: 0.0003  max mem: 4132
[17:32:27.289156] Epoch: [1]  [ 80/781]  eta: 0:02:02  lr: 0.000055  training_loss: 3.1504 (3.1683)  classification_loss: 2.2162 (2.2264)  loss_mask: 0.9000 (0.9420)  time: 0.1644  data: 0.0004  max mem: 4132
[17:32:30.610128] Epoch: [1]  [100/781]  eta: 0:01:57  lr: 0.000056  training_loss: 3.1967 (3.1730)  classification_loss: 2.2167 (2.2238)  loss_mask: 0.9588 (0.9492)  time: 0.1660  data: 0.0003  max mem: 4132
[17:32:33.908974] Epoch: [1]  [120/781]  eta: 0:01:53  lr: 0.000058  training_loss: 3.0982 (3.1648)  classification_loss: 2.2282 (2.2228)  loss_mask: 0.8579 (0.9420)  time: 0.1649  data: 0.0003  max mem: 4132
[17:32:37.215512] Epoch: [1]  [140/781]  eta: 0:01:49  lr: 0.000059  training_loss: 3.0365 (3.1476)  classification_loss: 2.1823 (2.2190)  loss_mask: 0.8353 (0.9286)  time: 0.1652  data: 0.0003  max mem: 4132
[17:32:40.509009] Epoch: [1]  [160/781]  eta: 0:01:45  lr: 0.000060  training_loss: 3.1736 (3.1549)  classification_loss: 2.2163 (2.2180)  loss_mask: 0.9578 (0.9370)  time: 0.1646  data: 0.0002  max mem: 4132
[17:32:43.820401] Epoch: [1]  [180/781]  eta: 0:01:41  lr: 0.000062  training_loss: 3.1208 (3.1490)  classification_loss: 2.1933 (2.2177)  loss_mask: 0.8922 (0.9313)  time: 0.1655  data: 0.0003  max mem: 4132
[17:32:47.133761] Epoch: [1]  [200/781]  eta: 0:01:38  lr: 0.000063  training_loss: 3.0665 (3.1394)  classification_loss: 2.2092 (2.2164)  loss_mask: 0.8481 (0.9231)  time: 0.1655  data: 0.0002  max mem: 4132
[17:32:50.460445] Epoch: [1]  [220/781]  eta: 0:01:34  lr: 0.000064  training_loss: 3.0382 (3.1333)  classification_loss: 2.1962 (2.2157)  loss_mask: 0.8394 (0.9176)  time: 0.1661  data: 0.0003  max mem: 4132
[17:32:53.776122] Epoch: [1]  [240/781]  eta: 0:01:31  lr: 0.000065  training_loss: 3.1049 (3.1303)  classification_loss: 2.1984 (2.2150)  loss_mask: 0.8623 (0.9154)  time: 0.1657  data: 0.0003  max mem: 4132
[17:32:57.106266] Epoch: [1]  [260/781]  eta: 0:01:27  lr: 0.000067  training_loss: 3.0459 (3.1228)  classification_loss: 2.2062 (2.2139)  loss_mask: 0.8380 (0.9089)  time: 0.1664  data: 0.0003  max mem: 4132
[17:33:00.405490] Epoch: [1]  [280/781]  eta: 0:01:24  lr: 0.000068  training_loss: 3.0877 (3.1185)  classification_loss: 2.2305 (2.2148)  loss_mask: 0.8250 (0.9036)  time: 0.1649  data: 0.0003  max mem: 4132
[17:33:03.710440] Epoch: [1]  [300/781]  eta: 0:01:20  lr: 0.000069  training_loss: 2.9637 (3.1093)  classification_loss: 2.2062 (2.2144)  loss_mask: 0.7559 (0.8950)  time: 0.1651  data: 0.0002  max mem: 4132
[17:33:07.010892] Epoch: [1]  [320/781]  eta: 0:01:17  lr: 0.000070  training_loss: 2.9615 (3.1016)  classification_loss: 2.1945 (2.2137)  loss_mask: 0.7867 (0.8880)  time: 0.1649  data: 0.0003  max mem: 4132
[17:33:10.353076] Epoch: [1]  [340/781]  eta: 0:01:13  lr: 0.000072  training_loss: 3.0153 (3.0963)  classification_loss: 2.2361 (2.2138)  loss_mask: 0.8050 (0.8825)  time: 0.1670  data: 0.0003  max mem: 4132
[17:33:13.632146] Epoch: [1]  [360/781]  eta: 0:01:10  lr: 0.000073  training_loss: 3.0668 (3.0938)  classification_loss: 2.2330 (2.2141)  loss_mask: 0.8233 (0.8797)  time: 0.1639  data: 0.0002  max mem: 4132
[17:33:16.917256] Epoch: [1]  [380/781]  eta: 0:01:07  lr: 0.000074  training_loss: 3.0584 (3.0911)  classification_loss: 2.2152 (2.2147)  loss_mask: 0.7876 (0.8764)  time: 0.1642  data: 0.0002  max mem: 4132
[17:33:20.218276] Epoch: [1]  [400/781]  eta: 0:01:03  lr: 0.000076  training_loss: 2.9330 (3.0838)  classification_loss: 2.2090 (2.2143)  loss_mask: 0.7091 (0.8695)  time: 0.1650  data: 0.0002  max mem: 4132
[17:33:23.548530] Epoch: [1]  [420/781]  eta: 0:01:00  lr: 0.000077  training_loss: 3.0138 (3.0810)  classification_loss: 2.1979 (2.2134)  loss_mask: 0.8244 (0.8676)  time: 0.1664  data: 0.0003  max mem: 4132
[17:33:26.869499] Epoch: [1]  [440/781]  eta: 0:00:56  lr: 0.000078  training_loss: 2.9465 (3.0763)  classification_loss: 2.1896 (2.2128)  loss_mask: 0.7576 (0.8635)  time: 0.1660  data: 0.0003  max mem: 4132
[17:33:30.178347] Epoch: [1]  [460/781]  eta: 0:00:53  lr: 0.000079  training_loss: 2.9196 (3.0698)  classification_loss: 2.1820 (2.2119)  loss_mask: 0.7267 (0.8579)  time: 0.1653  data: 0.0003  max mem: 4132
[17:33:33.535210] Epoch: [1]  [480/781]  eta: 0:00:50  lr: 0.000081  training_loss: 3.0293 (3.0699)  classification_loss: 2.2223 (2.2127)  loss_mask: 0.7898 (0.8571)  time: 0.1677  data: 0.0003  max mem: 4132
[17:33:36.859491] Epoch: [1]  [500/781]  eta: 0:00:46  lr: 0.000082  training_loss: 3.1262 (3.0711)  classification_loss: 2.1807 (2.2120)  loss_mask: 0.9219 (0.8590)  time: 0.1661  data: 0.0004  max mem: 4132
[17:33:40.229170] Epoch: [1]  [520/781]  eta: 0:00:43  lr: 0.000083  training_loss: 2.9594 (3.0675)  classification_loss: 2.2090 (2.2118)  loss_mask: 0.7598 (0.8557)  time: 0.1683  data: 0.0004  max mem: 4132
[17:33:43.562412] Epoch: [1]  [540/781]  eta: 0:00:40  lr: 0.000085  training_loss: 2.9570 (3.0642)  classification_loss: 2.2077 (2.2121)  loss_mask: 0.7393 (0.8521)  time: 0.1666  data: 0.0003  max mem: 4132
[17:33:46.843421] Epoch: [1]  [560/781]  eta: 0:00:36  lr: 0.000086  training_loss: 2.8996 (3.0598)  classification_loss: 2.2019 (2.2117)  loss_mask: 0.7124 (0.8482)  time: 0.1639  data: 0.0003  max mem: 4132
[17:33:50.112535] Epoch: [1]  [580/781]  eta: 0:00:33  lr: 0.000087  training_loss: 3.0079 (3.0587)  classification_loss: 2.1935 (2.2114)  loss_mask: 0.8147 (0.8473)  time: 0.1634  data: 0.0002  max mem: 4132
[17:33:53.436071] Epoch: [1]  [600/781]  eta: 0:00:30  lr: 0.000088  training_loss: 3.0568 (3.0590)  classification_loss: 2.1920 (2.2110)  loss_mask: 0.8498 (0.8480)  time: 0.1661  data: 0.0004  max mem: 4132
[17:33:56.769917] Epoch: [1]  [620/781]  eta: 0:00:26  lr: 0.000090  training_loss: 3.0122 (3.0576)  classification_loss: 2.1953 (2.2104)  loss_mask: 0.8201 (0.8472)  time: 0.1666  data: 0.0003  max mem: 4132
[17:34:00.130771] Epoch: [1]  [640/781]  eta: 0:00:23  lr: 0.000091  training_loss: 2.8993 (3.0538)  classification_loss: 2.1683 (2.2098)  loss_mask: 0.7438 (0.8440)  time: 0.1680  data: 0.0004  max mem: 4132
[17:34:03.431662] Epoch: [1]  [660/781]  eta: 0:00:20  lr: 0.000092  training_loss: 2.9719 (3.0520)  classification_loss: 2.1757 (2.2093)  loss_mask: 0.7791 (0.8427)  time: 0.1650  data: 0.0004  max mem: 4132
[17:34:06.749678] Epoch: [1]  [680/781]  eta: 0:00:16  lr: 0.000094  training_loss: 2.9058 (3.0490)  classification_loss: 2.2076 (2.2094)  loss_mask: 0.7240 (0.8396)  time: 0.1658  data: 0.0003  max mem: 4132
[17:34:10.074519] Epoch: [1]  [700/781]  eta: 0:00:13  lr: 0.000095  training_loss: 2.9549 (3.0467)  classification_loss: 2.1920 (2.2091)  loss_mask: 0.7641 (0.8375)  time: 0.1661  data: 0.0003  max mem: 4132
[17:34:13.427567] Epoch: [1]  [720/781]  eta: 0:00:10  lr: 0.000096  training_loss: 2.9815 (3.0448)  classification_loss: 2.2158 (2.2093)  loss_mask: 0.7681 (0.8355)  time: 0.1675  data: 0.0003  max mem: 4132
[17:34:16.768224] Epoch: [1]  [740/781]  eta: 0:00:06  lr: 0.000097  training_loss: 3.0044 (3.0442)  classification_loss: 2.2186 (2.2095)  loss_mask: 0.7843 (0.8347)  time: 0.1669  data: 0.0003  max mem: 4132
[17:34:20.053247] Epoch: [1]  [760/781]  eta: 0:00:03  lr: 0.000099  training_loss: 2.9321 (3.0419)  classification_loss: 2.1893 (2.2092)  loss_mask: 0.7331 (0.8326)  time: 0.1642  data: 0.0003  max mem: 4132
[17:34:23.354374] Epoch: [1]  [780/781]  eta: 0:00:00  lr: 0.000100  training_loss: 2.9175 (3.0393)  classification_loss: 2.2097 (2.2096)  loss_mask: 0.6920 (0.8297)  time: 0.1650  data: 0.0002  max mem: 4132
[17:34:23.531962] Epoch: [1] Total time: 0:02:10 (0.1669 s / it)
[17:34:23.532534] Averaged stats: lr: 0.000100  training_loss: 2.9175 (3.0393)  classification_loss: 2.2097 (2.2096)  loss_mask: 0.6920 (0.8297)
[17:34:24.295373] Test:  [  0/157]  eta: 0:01:59  testing_loss: 1.8956 (1.8956)  acc1: 39.0625 (39.0625)  acc5: 85.9375 (85.9375)  time: 0.7587  data: 0.7290  max mem: 4132
[17:34:24.624509] Test:  [ 10/157]  eta: 0:00:14  testing_loss: 2.0244 (1.9880)  acc1: 29.6875 (31.5341)  acc5: 84.3750 (83.2386)  time: 0.0976  data: 0.0665  max mem: 4132
[17:34:24.922157] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.9861 (1.9889)  acc1: 32.8125 (32.7381)  acc5: 82.8125 (82.5893)  time: 0.0305  data: 0.0004  max mem: 4132
[17:34:25.218405] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.9751 (1.9808)  acc1: 32.8125 (33.0141)  acc5: 82.8125 (83.1653)  time: 0.0295  data: 0.0004  max mem: 4132
[17:34:25.513511] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.9693 (1.9806)  acc1: 32.8125 (33.5366)  acc5: 84.3750 (82.8506)  time: 0.0294  data: 0.0003  max mem: 4132
[17:34:25.804184] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.9846 (1.9804)  acc1: 32.8125 (33.5478)  acc5: 84.3750 (83.1495)  time: 0.0291  data: 0.0003  max mem: 4132
[17:34:26.096236] Test:  [ 60/157]  eta: 0:00:04  testing_loss: 1.9750 (1.9787)  acc1: 34.3750 (33.6834)  acc5: 82.8125 (83.2223)  time: 0.0289  data: 0.0003  max mem: 4132
[17:34:26.389773] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.9638 (1.9793)  acc1: 31.2500 (33.1426)  acc5: 82.8125 (83.1866)  time: 0.0291  data: 0.0002  max mem: 4132
[17:34:26.677349] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.9631 (1.9774)  acc1: 34.3750 (33.4298)  acc5: 84.3750 (83.4298)  time: 0.0289  data: 0.0002  max mem: 4132
[17:34:26.970191] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.9827 (1.9798)  acc1: 34.3750 (33.1216)  acc5: 82.8125 (83.2589)  time: 0.0288  data: 0.0004  max mem: 4132
[17:34:27.261449] Test:  [100/157]  eta: 0:00:02  testing_loss: 2.0076 (1.9809)  acc1: 31.2500 (33.1838)  acc5: 79.6875 (83.2147)  time: 0.0290  data: 0.0004  max mem: 4132
[17:34:27.548630] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.9891 (1.9817)  acc1: 32.8125 (33.1222)  acc5: 82.8125 (83.2770)  time: 0.0288  data: 0.0002  max mem: 4132
[17:34:27.837686] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.9686 (1.9780)  acc1: 32.8125 (33.2128)  acc5: 84.3750 (83.5227)  time: 0.0287  data: 0.0002  max mem: 4132
[17:34:28.133282] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.9691 (1.9791)  acc1: 34.3750 (33.2657)  acc5: 84.3750 (83.4447)  time: 0.0291  data: 0.0002  max mem: 4132
[17:34:28.420328] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.9943 (1.9786)  acc1: 34.3750 (33.4663)  acc5: 84.3750 (83.4885)  time: 0.0290  data: 0.0002  max mem: 4132
[17:34:28.705768] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.9821 (1.9774)  acc1: 34.3750 (33.4127)  acc5: 84.3750 (83.5782)  time: 0.0284  data: 0.0002  max mem: 4132
[17:34:28.860044] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.9625 (1.9764)  acc1: 34.3750 (33.2900)  acc5: 84.3750 (83.6400)  time: 0.0275  data: 0.0002  max mem: 4132
[17:34:29.038701] Test: Total time: 0:00:05 (0.0351 s / it)
[17:34:29.039187] * Acc@1 33.290 Acc@5 83.640 loss 1.976
[17:34:29.039534] Accuracy of the network on the 10000 test images: 33.3%
[17:34:29.039814] Max accuracy: 33.29%
[17:34:31.535578] log_dir: ./output_dir
[17:34:32.519943] Epoch: [2]  [  0/781]  eta: 0:12:47  lr: 0.000100  training_loss: 2.8285 (2.8285)  classification_loss: 2.2122 (2.2122)  loss_mask: 0.6163 (0.6163)  time: 0.9823  data: 0.8053  max mem: 4132
[17:34:35.857440] Epoch: [2]  [ 20/781]  eta: 0:02:36  lr: 0.000101  training_loss: 2.9200 (2.9086)  classification_loss: 2.1976 (2.1885)  loss_mask: 0.7126 (0.7201)  time: 0.1668  data: 0.0003  max mem: 4132
[17:34:39.147419] Epoch: [2]  [ 40/781]  eta: 0:02:17  lr: 0.000103  training_loss: 2.8773 (2.8950)  classification_loss: 2.2039 (2.2012)  loss_mask: 0.6340 (0.6939)  time: 0.1644  data: 0.0002  max mem: 4132
[17:34:42.452887] Epoch: [2]  [ 60/781]  eta: 0:02:08  lr: 0.000104  training_loss: 2.9204 (2.9098)  classification_loss: 2.2014 (2.2055)  loss_mask: 0.7393 (0.7044)  time: 0.1652  data: 0.0003  max mem: 4132
[17:34:45.807929] Epoch: [2]  [ 80/781]  eta: 0:02:03  lr: 0.000105  training_loss: 2.9405 (2.9228)  classification_loss: 2.2220 (2.2111)  loss_mask: 0.7155 (0.7117)  time: 0.1677  data: 0.0004  max mem: 4132
[17:34:49.080011] Epoch: [2]  [100/781]  eta: 0:01:58  lr: 0.000106  training_loss: 2.8976 (2.9206)  classification_loss: 2.1985 (2.2072)  loss_mask: 0.6928 (0.7133)  time: 0.1635  data: 0.0002  max mem: 4132
[17:34:52.328340] Epoch: [2]  [120/781]  eta: 0:01:53  lr: 0.000108  training_loss: 2.9793 (2.9349)  classification_loss: 2.2113 (2.2090)  loss_mask: 0.7434 (0.7259)  time: 0.1623  data: 0.0002  max mem: 4132
[17:34:55.637551] Epoch: [2]  [140/781]  eta: 0:01:49  lr: 0.000109  training_loss: 2.9074 (2.9335)  classification_loss: 2.2018 (2.2066)  loss_mask: 0.7031 (0.7268)  time: 0.1654  data: 0.0002  max mem: 4132
[17:34:59.026922] Epoch: [2]  [160/781]  eta: 0:01:45  lr: 0.000110  training_loss: 2.9684 (2.9360)  classification_loss: 2.1854 (2.2053)  loss_mask: 0.7435 (0.7307)  time: 0.1694  data: 0.0003  max mem: 4132
[17:35:02.353141] Epoch: [2]  [180/781]  eta: 0:01:42  lr: 0.000112  training_loss: 2.8905 (2.9347)  classification_loss: 2.1974 (2.2050)  loss_mask: 0.7187 (0.7297)  time: 0.1662  data: 0.0003  max mem: 4132
[17:35:05.673534] Epoch: [2]  [200/781]  eta: 0:01:38  lr: 0.000113  training_loss: 2.8430 (2.9262)  classification_loss: 2.1993 (2.2034)  loss_mask: 0.6657 (0.7228)  time: 0.1659  data: 0.0004  max mem: 4132
[17:35:08.981536] Epoch: [2]  [220/781]  eta: 0:01:34  lr: 0.000114  training_loss: 2.8135 (2.9176)  classification_loss: 2.1555 (2.2003)  loss_mask: 0.6340 (0.7173)  time: 0.1653  data: 0.0002  max mem: 4132
[17:35:12.308198] Epoch: [2]  [240/781]  eta: 0:01:31  lr: 0.000115  training_loss: 2.9582 (2.9192)  classification_loss: 2.1966 (2.1997)  loss_mask: 0.7625 (0.7195)  time: 0.1662  data: 0.0003  max mem: 4132
[17:35:15.619547] Epoch: [2]  [260/781]  eta: 0:01:27  lr: 0.000117  training_loss: 2.9526 (2.9198)  classification_loss: 2.1586 (2.1983)  loss_mask: 0.7269 (0.7215)  time: 0.1655  data: 0.0003  max mem: 4132
[17:35:18.939245] Epoch: [2]  [280/781]  eta: 0:01:24  lr: 0.000118  training_loss: 2.9267 (2.9202)  classification_loss: 2.1907 (2.1983)  loss_mask: 0.7248 (0.7219)  time: 0.1659  data: 0.0003  max mem: 4132
[17:35:22.247067] Epoch: [2]  [300/781]  eta: 0:01:20  lr: 0.000119  training_loss: 2.9619 (2.9234)  classification_loss: 2.1866 (2.1977)  loss_mask: 0.7629 (0.7257)  time: 0.1653  data: 0.0003  max mem: 4132
[17:35:25.548923] Epoch: [2]  [320/781]  eta: 0:01:17  lr: 0.000120  training_loss: 2.8912 (2.9219)  classification_loss: 2.1767 (2.1969)  loss_mask: 0.6972 (0.7250)  time: 0.1650  data: 0.0002  max mem: 4132
[17:35:28.878391] Epoch: [2]  [340/781]  eta: 0:01:14  lr: 0.000122  training_loss: 2.8667 (2.9196)  classification_loss: 2.1943 (2.1967)  loss_mask: 0.7027 (0.7229)  time: 0.1663  data: 0.0003  max mem: 4132
[17:35:32.179446] Epoch: [2]  [360/781]  eta: 0:01:10  lr: 0.000123  training_loss: 2.8615 (2.9161)  classification_loss: 2.1814 (2.1963)  loss_mask: 0.6491 (0.7197)  time: 0.1650  data: 0.0003  max mem: 4132
[17:35:35.517232] Epoch: [2]  [380/781]  eta: 0:01:07  lr: 0.000124  training_loss: 2.9449 (2.9187)  classification_loss: 2.2039 (2.1968)  loss_mask: 0.7406 (0.7219)  time: 0.1668  data: 0.0004  max mem: 4132
[17:35:38.807737] Epoch: [2]  [400/781]  eta: 0:01:03  lr: 0.000126  training_loss: 2.8780 (2.9182)  classification_loss: 2.1694 (2.1958)  loss_mask: 0.7186 (0.7223)  time: 0.1644  data: 0.0002  max mem: 4132
[17:35:42.116240] Epoch: [2]  [420/781]  eta: 0:01:00  lr: 0.000127  training_loss: 2.8393 (2.9142)  classification_loss: 2.1805 (2.1952)  loss_mask: 0.6339 (0.7190)  time: 0.1653  data: 0.0003  max mem: 4132
[17:35:45.431272] Epoch: [2]  [440/781]  eta: 0:00:57  lr: 0.000128  training_loss: 2.9212 (2.9145)  classification_loss: 2.1654 (2.1944)  loss_mask: 0.7186 (0.7201)  time: 0.1657  data: 0.0003  max mem: 4132
[17:35:48.750020] Epoch: [2]  [460/781]  eta: 0:00:53  lr: 0.000129  training_loss: 2.8077 (2.9102)  classification_loss: 2.1564 (2.1927)  loss_mask: 0.6467 (0.7174)  time: 0.1658  data: 0.0003  max mem: 4132
[17:35:52.052035] Epoch: [2]  [480/781]  eta: 0:00:50  lr: 0.000131  training_loss: 2.8927 (2.9090)  classification_loss: 2.2015 (2.1932)  loss_mask: 0.6673 (0.7159)  time: 0.1650  data: 0.0003  max mem: 4132
[17:35:55.355929] Epoch: [2]  [500/781]  eta: 0:00:46  lr: 0.000132  training_loss: 2.8870 (2.9091)  classification_loss: 2.1797 (2.1925)  loss_mask: 0.7340 (0.7166)  time: 0.1651  data: 0.0002  max mem: 4132
[17:35:58.644615] Epoch: [2]  [520/781]  eta: 0:00:43  lr: 0.000133  training_loss: 2.9305 (2.9100)  classification_loss: 2.1678 (2.1920)  loss_mask: 0.7484 (0.7180)  time: 0.1643  data: 0.0002  max mem: 4132
[17:36:01.965145] Epoch: [2]  [540/781]  eta: 0:00:40  lr: 0.000135  training_loss: 2.8715 (2.9085)  classification_loss: 2.1783 (2.1919)  loss_mask: 0.6901 (0.7166)  time: 0.1659  data: 0.0003  max mem: 4132
[17:36:05.323097] Epoch: [2]  [560/781]  eta: 0:00:36  lr: 0.000136  training_loss: 2.8047 (2.9057)  classification_loss: 2.1681 (2.1909)  loss_mask: 0.6696 (0.7148)  time: 0.1678  data: 0.0003  max mem: 4132
[17:36:08.658552] Epoch: [2]  [580/781]  eta: 0:00:33  lr: 0.000137  training_loss: 2.8328 (2.9029)  classification_loss: 2.1491 (2.1898)  loss_mask: 0.6615 (0.7131)  time: 0.1667  data: 0.0005  max mem: 4132
[17:36:11.957554] Epoch: [2]  [600/781]  eta: 0:00:30  lr: 0.000138  training_loss: 2.8613 (2.9016)  classification_loss: 2.1905 (2.1897)  loss_mask: 0.6686 (0.7119)  time: 0.1649  data: 0.0003  max mem: 4132
[17:36:15.263847] Epoch: [2]  [620/781]  eta: 0:00:26  lr: 0.000140  training_loss: 2.7867 (2.8986)  classification_loss: 2.1810 (2.1891)  loss_mask: 0.6571 (0.7094)  time: 0.1652  data: 0.0002  max mem: 4132
[17:36:18.587619] Epoch: [2]  [640/781]  eta: 0:00:23  lr: 0.000141  training_loss: 2.8041 (2.8964)  classification_loss: 2.1603 (2.1883)  loss_mask: 0.6374 (0.7081)  time: 0.1661  data: 0.0003  max mem: 4132
[17:36:21.904477] Epoch: [2]  [660/781]  eta: 0:00:20  lr: 0.000142  training_loss: 2.8318 (2.8947)  classification_loss: 2.1684 (2.1877)  loss_mask: 0.6554 (0.7070)  time: 0.1657  data: 0.0003  max mem: 4132
[17:36:25.222436] Epoch: [2]  [680/781]  eta: 0:00:16  lr: 0.000144  training_loss: 2.8206 (2.8935)  classification_loss: 2.1680 (2.1870)  loss_mask: 0.6871 (0.7065)  time: 0.1658  data: 0.0003  max mem: 4132
[17:36:28.542619] Epoch: [2]  [700/781]  eta: 0:00:13  lr: 0.000145  training_loss: 2.9102 (2.8940)  classification_loss: 2.1598 (2.1865)  loss_mask: 0.7011 (0.7075)  time: 0.1658  data: 0.0002  max mem: 4132
[17:36:31.864507] Epoch: [2]  [720/781]  eta: 0:00:10  lr: 0.000146  training_loss: 2.9074 (2.8951)  classification_loss: 2.1730 (2.1864)  loss_mask: 0.7163 (0.7087)  time: 0.1660  data: 0.0003  max mem: 4132
[17:36:35.202695] Epoch: [2]  [740/781]  eta: 0:00:06  lr: 0.000147  training_loss: 2.8514 (2.8943)  classification_loss: 2.1676 (2.1863)  loss_mask: 0.6536 (0.7080)  time: 0.1668  data: 0.0004  max mem: 4132
[17:36:38.530171] Epoch: [2]  [760/781]  eta: 0:00:03  lr: 0.000149  training_loss: 2.8891 (2.8944)  classification_loss: 2.1866 (2.1862)  loss_mask: 0.7108 (0.7081)  time: 0.1662  data: 0.0004  max mem: 4132
[17:36:41.809392] Epoch: [2]  [780/781]  eta: 0:00:00  lr: 0.000150  training_loss: 2.8725 (2.8948)  classification_loss: 2.1492 (2.1855)  loss_mask: 0.7142 (0.7093)  time: 0.1639  data: 0.0002  max mem: 4132
[17:36:41.974202] Epoch: [2] Total time: 0:02:10 (0.1670 s / it)
[17:36:41.975047] Averaged stats: lr: 0.000150  training_loss: 2.8725 (2.8948)  classification_loss: 2.1492 (2.1855)  loss_mask: 0.7142 (0.7093)
[17:36:42.675679] Test:  [  0/157]  eta: 0:01:49  testing_loss: 1.8038 (1.8038)  acc1: 45.3125 (45.3125)  acc5: 87.5000 (87.5000)  time: 0.6957  data: 0.6660  max mem: 4132
[17:36:42.971945] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.9058 (1.8886)  acc1: 34.3750 (35.7955)  acc5: 85.9375 (85.9375)  time: 0.0900  data: 0.0610  max mem: 4132
[17:36:43.264252] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.8754 (1.8856)  acc1: 35.9375 (36.9792)  acc5: 85.9375 (85.1935)  time: 0.0293  data: 0.0005  max mem: 4132
[17:36:43.553349] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.8521 (1.8755)  acc1: 35.9375 (36.9456)  acc5: 84.3750 (84.9798)  time: 0.0289  data: 0.0003  max mem: 4132
[17:36:43.844132] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.8563 (1.8779)  acc1: 35.9375 (37.0046)  acc5: 84.3750 (84.7180)  time: 0.0287  data: 0.0002  max mem: 4132
[17:36:44.134205] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.8849 (1.8798)  acc1: 35.9375 (36.5502)  acc5: 84.3750 (84.6814)  time: 0.0288  data: 0.0003  max mem: 4132
[17:36:44.428874] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.8597 (1.8752)  acc1: 35.9375 (36.7059)  acc5: 84.3750 (84.8361)  time: 0.0290  data: 0.0003  max mem: 4132
[17:36:44.728059] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.8573 (1.8750)  acc1: 37.5000 (36.7077)  acc5: 85.9375 (85.0792)  time: 0.0295  data: 0.0002  max mem: 4132
[17:36:45.023287] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.8606 (1.8727)  acc1: 35.9375 (36.5934)  acc5: 87.5000 (85.3974)  time: 0.0296  data: 0.0004  max mem: 4132
[17:36:45.313191] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.8770 (1.8761)  acc1: 35.9375 (36.3668)  acc5: 85.9375 (85.3880)  time: 0.0291  data: 0.0004  max mem: 4132
[17:36:45.606290] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.9035 (1.8777)  acc1: 34.3750 (36.3088)  acc5: 84.3750 (85.3032)  time: 0.0290  data: 0.0002  max mem: 4132
[17:36:45.897597] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.8939 (1.8791)  acc1: 35.9375 (36.3035)  acc5: 84.3750 (85.2618)  time: 0.0291  data: 0.0002  max mem: 4132
[17:36:46.190846] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.8570 (1.8750)  acc1: 35.9375 (36.4282)  acc5: 85.9375 (85.5114)  time: 0.0291  data: 0.0002  max mem: 4132
[17:36:46.494152] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.8559 (1.8759)  acc1: 34.3750 (36.3550)  acc5: 87.5000 (85.4843)  time: 0.0296  data: 0.0003  max mem: 4132
[17:36:46.785942] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.8804 (1.8760)  acc1: 35.9375 (36.3586)  acc5: 85.9375 (85.4942)  time: 0.0294  data: 0.0003  max mem: 4132
[17:36:47.072281] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.8801 (1.8745)  acc1: 37.5000 (36.4652)  acc5: 87.5000 (85.5443)  time: 0.0287  data: 0.0002  max mem: 4132
[17:36:47.227162] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.8567 (1.8737)  acc1: 37.5000 (36.3000)  acc5: 87.5000 (85.5800)  time: 0.0276  data: 0.0002  max mem: 4132
[17:36:47.467050] Test: Total time: 0:00:05 (0.0350 s / it)
[17:36:47.468451] * Acc@1 36.300 Acc@5 85.580 loss 1.874
[17:36:47.468871] Accuracy of the network on the 10000 test images: 36.3%
[17:36:47.469083] Max accuracy: 36.30%
[17:36:47.829774] log_dir: ./output_dir
[17:36:48.805525] Epoch: [3]  [  0/781]  eta: 0:12:40  lr: 0.000150  training_loss: 2.7577 (2.7577)  classification_loss: 2.0554 (2.0554)  loss_mask: 0.7023 (0.7023)  time: 0.9736  data: 0.7447  max mem: 4132
[17:36:52.115508] Epoch: [3]  [ 20/781]  eta: 0:02:35  lr: 0.000151  training_loss: 2.8464 (2.8723)  classification_loss: 2.1417 (2.1387)  loss_mask: 0.7146 (0.7336)  time: 0.1654  data: 0.0004  max mem: 4132
[17:36:55.440486] Epoch: [3]  [ 40/781]  eta: 0:02:17  lr: 0.000153  training_loss: 2.9271 (2.8952)  classification_loss: 2.2301 (2.1678)  loss_mask: 0.7098 (0.7275)  time: 0.1662  data: 0.0003  max mem: 4132
[17:36:58.777347] Epoch: [3]  [ 60/781]  eta: 0:02:09  lr: 0.000154  training_loss: 2.8147 (2.8645)  classification_loss: 2.1577 (2.1667)  loss_mask: 0.6523 (0.6977)  time: 0.1668  data: 0.0003  max mem: 4132
[17:37:02.081492] Epoch: [3]  [ 80/781]  eta: 0:02:03  lr: 0.000155  training_loss: 2.8267 (2.8692)  classification_loss: 2.1849 (2.1718)  loss_mask: 0.6703 (0.6974)  time: 0.1651  data: 0.0002  max mem: 4132
[17:37:05.417022] Epoch: [3]  [100/781]  eta: 0:01:58  lr: 0.000156  training_loss: 2.8775 (2.8747)  classification_loss: 2.1842 (2.1749)  loss_mask: 0.7129 (0.6997)  time: 0.1667  data: 0.0003  max mem: 4132
[17:37:08.736995] Epoch: [3]  [120/781]  eta: 0:01:54  lr: 0.000158  training_loss: 2.8514 (2.8741)  classification_loss: 2.1638 (2.1738)  loss_mask: 0.6874 (0.7003)  time: 0.1659  data: 0.0004  max mem: 4132
[17:37:12.068833] Epoch: [3]  [140/781]  eta: 0:01:50  lr: 0.000159  training_loss: 2.7604 (2.8598)  classification_loss: 2.1648 (2.1733)  loss_mask: 0.5982 (0.6865)  time: 0.1665  data: 0.0004  max mem: 4132
[17:37:15.375044] Epoch: [3]  [160/781]  eta: 0:01:46  lr: 0.000160  training_loss: 2.7546 (2.8481)  classification_loss: 2.1606 (2.1732)  loss_mask: 0.5778 (0.6749)  time: 0.1652  data: 0.0003  max mem: 4132
[17:37:18.710336] Epoch: [3]  [180/781]  eta: 0:01:42  lr: 0.000162  training_loss: 2.8052 (2.8444)  classification_loss: 2.1764 (2.1732)  loss_mask: 0.6371 (0.6712)  time: 0.1667  data: 0.0003  max mem: 4132
[17:37:22.016540] Epoch: [3]  [200/781]  eta: 0:01:38  lr: 0.000163  training_loss: 2.7960 (2.8435)  classification_loss: 2.1806 (2.1736)  loss_mask: 0.6511 (0.6698)  time: 0.1652  data: 0.0003  max mem: 4132
[17:37:25.337333] Epoch: [3]  [220/781]  eta: 0:01:35  lr: 0.000164  training_loss: 2.7638 (2.8422)  classification_loss: 2.1458 (2.1716)  loss_mask: 0.6623 (0.6706)  time: 0.1659  data: 0.0003  max mem: 4132
[17:37:28.647992] Epoch: [3]  [240/781]  eta: 0:01:31  lr: 0.000165  training_loss: 2.7757 (2.8387)  classification_loss: 2.1611 (2.1709)  loss_mask: 0.6122 (0.6678)  time: 0.1654  data: 0.0003  max mem: 4132
[17:37:31.909518] Epoch: [3]  [260/781]  eta: 0:01:27  lr: 0.000167  training_loss: 2.7814 (2.8330)  classification_loss: 2.1381 (2.1683)  loss_mask: 0.6255 (0.6647)  time: 0.1630  data: 0.0002  max mem: 4132
[17:37:35.201041] Epoch: [3]  [280/781]  eta: 0:01:24  lr: 0.000168  training_loss: 2.8964 (2.8429)  classification_loss: 2.1854 (2.1702)  loss_mask: 0.7428 (0.6727)  time: 0.1644  data: 0.0003  max mem: 4132
[17:37:38.504367] Epoch: [3]  [300/781]  eta: 0:01:20  lr: 0.000169  training_loss: 2.8819 (2.8458)  classification_loss: 2.1849 (2.1711)  loss_mask: 0.6792 (0.6747)  time: 0.1651  data: 0.0003  max mem: 4132
[17:37:41.845038] Epoch: [3]  [320/781]  eta: 0:01:17  lr: 0.000170  training_loss: 2.8063 (2.8426)  classification_loss: 2.1463 (2.1693)  loss_mask: 0.6250 (0.6733)  time: 0.1669  data: 0.0006  max mem: 4132
[17:37:45.163662] Epoch: [3]  [340/781]  eta: 0:01:14  lr: 0.000172  training_loss: 2.8193 (2.8420)  classification_loss: 2.1452 (2.1681)  loss_mask: 0.6430 (0.6739)  time: 0.1658  data: 0.0003  max mem: 4132
[17:37:48.467620] Epoch: [3]  [360/781]  eta: 0:01:10  lr: 0.000173  training_loss: 2.7569 (2.8389)  classification_loss: 2.1522 (2.1679)  loss_mask: 0.6195 (0.6710)  time: 0.1651  data: 0.0003  max mem: 4132
[17:37:51.797511] Epoch: [3]  [380/781]  eta: 0:01:07  lr: 0.000174  training_loss: 2.8216 (2.8396)  classification_loss: 2.1690 (2.1673)  loss_mask: 0.6709 (0.6724)  time: 0.1664  data: 0.0003  max mem: 4132
[17:37:55.160493] Epoch: [3]  [400/781]  eta: 0:01:03  lr: 0.000176  training_loss: 2.8390 (2.8397)  classification_loss: 2.1771 (2.1674)  loss_mask: 0.6308 (0.6724)  time: 0.1680  data: 0.0003  max mem: 4132
[17:37:58.443409] Epoch: [3]  [420/781]  eta: 0:01:00  lr: 0.000177  training_loss: 2.8197 (2.8386)  classification_loss: 2.1285 (2.1666)  loss_mask: 0.6478 (0.6720)  time: 0.1641  data: 0.0003  max mem: 4132
[17:38:01.751993] Epoch: [3]  [440/781]  eta: 0:00:57  lr: 0.000178  training_loss: 2.8323 (2.8392)  classification_loss: 2.1490 (2.1652)  loss_mask: 0.6821 (0.6740)  time: 0.1653  data: 0.0002  max mem: 4132
[17:38:05.035623] Epoch: [3]  [460/781]  eta: 0:00:53  lr: 0.000179  training_loss: 2.7674 (2.8362)  classification_loss: 2.1396 (2.1647)  loss_mask: 0.6149 (0.6715)  time: 0.1641  data: 0.0002  max mem: 4132
[17:38:08.363330] Epoch: [3]  [480/781]  eta: 0:00:50  lr: 0.000181  training_loss: 2.7854 (2.8335)  classification_loss: 2.1687 (2.1645)  loss_mask: 0.6024 (0.6690)  time: 0.1663  data: 0.0003  max mem: 4132
[17:38:11.718718] Epoch: [3]  [500/781]  eta: 0:00:47  lr: 0.000182  training_loss: 2.7375 (2.8297)  classification_loss: 2.1378 (2.1638)  loss_mask: 0.5878 (0.6659)  time: 0.1677  data: 0.0003  max mem: 4132
[17:38:15.032656] Epoch: [3]  [520/781]  eta: 0:00:43  lr: 0.000183  training_loss: 2.7613 (2.8275)  classification_loss: 2.1625 (2.1636)  loss_mask: 0.6134 (0.6639)  time: 0.1656  data: 0.0003  max mem: 4132
[17:38:18.344496] Epoch: [3]  [540/781]  eta: 0:00:40  lr: 0.000185  training_loss: 2.8028 (2.8274)  classification_loss: 2.1572 (2.1642)  loss_mask: 0.6261 (0.6633)  time: 0.1655  data: 0.0003  max mem: 4132
[17:38:21.687610] Epoch: [3]  [560/781]  eta: 0:00:36  lr: 0.000186  training_loss: 2.7038 (2.8236)  classification_loss: 2.1280 (2.1630)  loss_mask: 0.6046 (0.6606)  time: 0.1671  data: 0.0005  max mem: 4132
[17:38:24.998309] Epoch: [3]  [580/781]  eta: 0:00:33  lr: 0.000187  training_loss: 2.7658 (2.8224)  classification_loss: 2.1618 (2.1632)  loss_mask: 0.5848 (0.6591)  time: 0.1654  data: 0.0004  max mem: 4132
[17:38:28.333186] Epoch: [3]  [600/781]  eta: 0:00:30  lr: 0.000188  training_loss: 2.8028 (2.8226)  classification_loss: 2.1622 (2.1631)  loss_mask: 0.6550 (0.6595)  time: 0.1666  data: 0.0004  max mem: 4132
[17:38:31.612317] Epoch: [3]  [620/781]  eta: 0:00:26  lr: 0.000190  training_loss: 2.7453 (2.8203)  classification_loss: 2.1340 (2.1626)  loss_mask: 0.5962 (0.6577)  time: 0.1639  data: 0.0002  max mem: 4132
[17:38:34.884892] Epoch: [3]  [640/781]  eta: 0:00:23  lr: 0.000191  training_loss: 2.7338 (2.8184)  classification_loss: 2.1410 (2.1622)  loss_mask: 0.5819 (0.6562)  time: 0.1636  data: 0.0003  max mem: 4132
[17:38:38.163791] Epoch: [3]  [660/781]  eta: 0:00:20  lr: 0.000192  training_loss: 2.7054 (2.8151)  classification_loss: 2.1396 (2.1620)  loss_mask: 0.5415 (0.6531)  time: 0.1638  data: 0.0002  max mem: 4132
[17:38:41.488273] Epoch: [3]  [680/781]  eta: 0:00:16  lr: 0.000194  training_loss: 2.7420 (2.8139)  classification_loss: 2.1308 (2.1611)  loss_mask: 0.6068 (0.6527)  time: 0.1661  data: 0.0003  max mem: 4132
[17:38:44.793610] Epoch: [3]  [700/781]  eta: 0:00:13  lr: 0.000195  training_loss: 2.7388 (2.8137)  classification_loss: 2.1347 (2.1604)  loss_mask: 0.6168 (0.6533)  time: 0.1652  data: 0.0003  max mem: 4132
[17:38:48.103486] Epoch: [3]  [720/781]  eta: 0:00:10  lr: 0.000196  training_loss: 2.7558 (2.8140)  classification_loss: 2.1335 (2.1598)  loss_mask: 0.6341 (0.6542)  time: 0.1654  data: 0.0003  max mem: 4132
[17:38:51.443659] Epoch: [3]  [740/781]  eta: 0:00:06  lr: 0.000197  training_loss: 2.7323 (2.8127)  classification_loss: 2.1321 (2.1594)  loss_mask: 0.6071 (0.6533)  time: 0.1669  data: 0.0003  max mem: 4132
[17:38:54.752482] Epoch: [3]  [760/781]  eta: 0:00:03  lr: 0.000199  training_loss: 2.7567 (2.8115)  classification_loss: 2.1453 (2.1589)  loss_mask: 0.6364 (0.6525)  time: 0.1653  data: 0.0004  max mem: 4132
[17:38:58.080395] Epoch: [3]  [780/781]  eta: 0:00:00  lr: 0.000200  training_loss: 2.7414 (2.8103)  classification_loss: 2.1405 (2.1584)  loss_mask: 0.6343 (0.6518)  time: 0.1663  data: 0.0002  max mem: 4132
[17:38:58.276426] Epoch: [3] Total time: 0:02:10 (0.1670 s / it)
[17:38:58.277938] Averaged stats: lr: 0.000200  training_loss: 2.7414 (2.8103)  classification_loss: 2.1405 (2.1584)  loss_mask: 0.6343 (0.6518)
[17:38:59.017794] Test:  [  0/157]  eta: 0:01:55  testing_loss: 1.7324 (1.7324)  acc1: 46.8750 (46.8750)  acc5: 90.6250 (90.6250)  time: 0.7359  data: 0.6856  max mem: 4132
[17:38:59.313538] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.8371 (1.8390)  acc1: 35.9375 (38.2102)  acc5: 84.3750 (85.0852)  time: 0.0936  data: 0.0626  max mem: 4132
[17:38:59.603786] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.8236 (1.8291)  acc1: 37.5000 (39.8065)  acc5: 85.9375 (86.3095)  time: 0.0291  data: 0.0003  max mem: 4132
[17:38:59.901102] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.8114 (1.8219)  acc1: 39.0625 (39.3145)  acc5: 85.9375 (86.1391)  time: 0.0292  data: 0.0003  max mem: 4132
[17:39:00.193926] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.8053 (1.8270)  acc1: 40.6250 (39.3674)  acc5: 84.3750 (85.5564)  time: 0.0293  data: 0.0003  max mem: 4132
[17:39:00.489692] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.8247 (1.8277)  acc1: 39.0625 (38.9400)  acc5: 85.9375 (85.8456)  time: 0.0291  data: 0.0003  max mem: 4132
[17:39:00.785121] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.7988 (1.8231)  acc1: 37.5000 (38.9600)  acc5: 87.5000 (86.0912)  time: 0.0293  data: 0.0003  max mem: 4132
[17:39:01.077063] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.7860 (1.8234)  acc1: 39.0625 (38.6444)  acc5: 87.5000 (86.2016)  time: 0.0292  data: 0.0003  max mem: 4132
[17:39:01.369006] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.8138 (1.8209)  acc1: 37.5000 (38.5610)  acc5: 89.0625 (86.4583)  time: 0.0290  data: 0.0003  max mem: 4132
[17:39:01.662153] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.8271 (1.8231)  acc1: 37.5000 (38.5474)  acc5: 85.9375 (86.4183)  time: 0.0291  data: 0.0003  max mem: 4132
[17:39:01.955445] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.8434 (1.8257)  acc1: 39.0625 (38.6603)  acc5: 84.3750 (86.3397)  time: 0.0291  data: 0.0003  max mem: 4132
[17:39:02.245052] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.8504 (1.8279)  acc1: 39.0625 (38.5557)  acc5: 84.3750 (86.2331)  time: 0.0289  data: 0.0003  max mem: 4132
[17:39:02.539500] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.8208 (1.8246)  acc1: 39.0625 (38.6751)  acc5: 87.5000 (86.4024)  time: 0.0290  data: 0.0003  max mem: 4132
[17:39:02.828370] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.8189 (1.8257)  acc1: 39.0625 (38.5735)  acc5: 89.0625 (86.4623)  time: 0.0290  data: 0.0003  max mem: 4132
[17:39:03.114263] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.8420 (1.8265)  acc1: 37.5000 (38.5860)  acc5: 85.9375 (86.4473)  time: 0.0286  data: 0.0002  max mem: 4132
[17:39:03.399380] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.8146 (1.8251)  acc1: 39.0625 (38.5865)  acc5: 85.9375 (86.4445)  time: 0.0284  data: 0.0002  max mem: 4132
[17:39:03.551604] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.8048 (1.8245)  acc1: 39.0625 (38.4300)  acc5: 85.9375 (86.4700)  time: 0.0274  data: 0.0002  max mem: 4132
[17:39:03.733625] Test: Total time: 0:00:05 (0.0347 s / it)
[17:39:03.734299] * Acc@1 38.430 Acc@5 86.470 loss 1.824
[17:39:03.734755] Accuracy of the network on the 10000 test images: 38.4%
[17:39:03.735059] Max accuracy: 38.43%
[17:39:04.068760] log_dir: ./output_dir
[17:39:05.019066] Epoch: [4]  [  0/781]  eta: 0:12:20  lr: 0.000200  training_loss: 2.7235 (2.7235)  classification_loss: 2.1179 (2.1179)  loss_mask: 0.6056 (0.6056)  time: 0.9483  data: 0.7519  max mem: 4132
[17:39:08.314451] Epoch: [4]  [ 20/781]  eta: 0:02:33  lr: 0.000201  training_loss: 2.7449 (2.7483)  classification_loss: 2.1259 (2.1222)  loss_mask: 0.6577 (0.6261)  time: 0.1647  data: 0.0002  max mem: 4132
[17:39:11.594868] Epoch: [4]  [ 40/781]  eta: 0:02:15  lr: 0.000203  training_loss: 2.7616 (2.7653)  classification_loss: 2.1354 (2.1344)  loss_mask: 0.6288 (0.6309)  time: 0.1639  data: 0.0003  max mem: 4132

[17:39:14.943660] Epoch: [4]  [ 60/781]  eta: 0:02:08  lr: 0.000204  training_loss: 2.7887 (2.7658)  classification_loss: 2.1343 (2.1353)  loss_mask: 0.6141 (0.6304)  time: 0.1673  data: 0.0003  max mem: 4132
[17:39:18.250780] Epoch: [4]  [ 80/781]  eta: 0:02:02  lr: 0.000205  training_loss: 2.6767 (2.7439)  classification_loss: 2.1482 (2.1374)  loss_mask: 0.5184 (0.6065)  time: 0.1653  data: 0.0003  max mem: 4132
[17:39:21.615165] Epoch: [4]  [100/781]  eta: 0:01:58  lr: 0.000206  training_loss: 2.7311 (2.7470)  classification_loss: 2.1405 (2.1414)  loss_mask: 0.5643 (0.6056)  time: 0.1681  data: 0.0002  max mem: 4132
[17:39:24.920169] Epoch: [4]  [120/781]  eta: 0:01:53  lr: 0.000208  training_loss: 2.9306 (2.7767)  classification_loss: 2.1472 (2.1406)  loss_mask: 0.7566 (0.6362)  time: 0.1652  data: 0.0003  max mem: 4132
[17:39:28.248838] Epoch: [4]  [140/781]  eta: 0:01:49  lr: 0.000209  training_loss: 2.6907 (2.7695)  classification_loss: 2.0896 (2.1366)  loss_mask: 0.5818 (0.6329)  time: 0.1664  data: 0.0003  max mem: 4132
[17:39:31.610961] Epoch: [4]  [160/781]  eta: 0:01:46  lr: 0.000210  training_loss: 2.7216 (2.7675)  classification_loss: 2.0983 (2.1329)  loss_mask: 0.6364 (0.6346)  time: 0.1680  data: 0.0003  max mem: 4132
[17:39:34.937029] Epoch: [4]  [180/781]  eta: 0:01:42  lr: 0.000212  training_loss: 2.8535 (2.7821)  classification_loss: 2.1767 (2.1384)  loss_mask: 0.6880 (0.6437)  time: 0.1662  data: 0.0003  max mem: 4132
[17:39:38.202841] Epoch: [4]  [200/781]  eta: 0:01:38  lr: 0.000213  training_loss: 2.8138 (2.7876)  classification_loss: 2.1510 (2.1422)  loss_mask: 0.6731 (0.6454)  time: 0.1632  data: 0.0002  max mem: 4132
[17:39:41.533184] Epoch: [4]  [220/781]  eta: 0:01:35  lr: 0.000214  training_loss: 2.6879 (2.7798)  classification_loss: 2.1241 (2.1416)  loss_mask: 0.5721 (0.6381)  time: 0.1664  data: 0.0003  max mem: 4132
[17:39:44.853977] Epoch: [4]  [240/781]  eta: 0:01:31  lr: 0.000215  training_loss: 2.6896 (2.7707)  classification_loss: 2.1041 (2.1397)  loss_mask: 0.5412 (0.6310)  time: 0.1659  data: 0.0004  max mem: 4132
[17:39:48.184490] Epoch: [4]  [260/781]  eta: 0:01:28  lr: 0.000217  training_loss: 2.7918 (2.7723)  classification_loss: 2.1241 (2.1380)  loss_mask: 0.6473 (0.6342)  time: 0.1664  data: 0.0003  max mem: 4132
[17:39:51.502110] Epoch: [4]  [280/781]  eta: 0:01:24  lr: 0.000218  training_loss: 2.7537 (2.7729)  classification_loss: 2.1380 (2.1383)  loss_mask: 0.6123 (0.6346)  time: 0.1658  data: 0.0004  max mem: 4132
[17:39:54.845627] Epoch: [4]  [300/781]  eta: 0:01:21  lr: 0.000219  training_loss: 2.7311 (2.7705)  classification_loss: 2.1723 (2.1392)  loss_mask: 0.5513 (0.6313)  time: 0.1671  data: 0.0003  max mem: 4132
[17:39:58.223445] Epoch: [4]  [320/781]  eta: 0:01:17  lr: 0.000220  training_loss: 2.6576 (2.7639)  classification_loss: 2.1213 (2.1378)  loss_mask: 0.5353 (0.6261)  time: 0.1687  data: 0.0004  max mem: 4132
[17:40:01.570360] Epoch: [4]  [340/781]  eta: 0:01:14  lr: 0.000222  training_loss: 2.6861 (2.7605)  classification_loss: 2.1092 (2.1376)  loss_mask: 0.5629 (0.6229)  time: 0.1672  data: 0.0003  max mem: 4132
[17:40:04.908875] Epoch: [4]  [360/781]  eta: 0:01:10  lr: 0.000223  training_loss: 2.7607 (2.7609)  classification_loss: 2.1352 (2.1377)  loss_mask: 0.6270 (0.6232)  time: 0.1668  data: 0.0003  max mem: 4132
[17:40:08.253816] Epoch: [4]  [380/781]  eta: 0:01:07  lr: 0.000224  training_loss: 2.6404 (2.7567)  classification_loss: 2.1425 (2.1368)  loss_mask: 0.5126 (0.6199)  time: 0.1671  data: 0.0003  max mem: 4132
[17:40:11.564094] Epoch: [4]  [400/781]  eta: 0:01:04  lr: 0.000226  training_loss: 2.7095 (2.7567)  classification_loss: 2.0958 (2.1356)  loss_mask: 0.6004 (0.6211)  time: 0.1654  data: 0.0003  max mem: 4132
[17:40:14.889173] Epoch: [4]  [420/781]  eta: 0:01:00  lr: 0.000227  training_loss: 2.6561 (2.7535)  classification_loss: 2.0995 (2.1343)  loss_mask: 0.5767 (0.6193)  time: 0.1661  data: 0.0003  max mem: 4132
[17:40:18.238684] Epoch: [4]  [440/781]  eta: 0:00:57  lr: 0.000228  training_loss: 2.7144 (2.7516)  classification_loss: 2.1074 (2.1331)  loss_mask: 0.5849 (0.6185)  time: 0.1673  data: 0.0003  max mem: 4132
[17:40:21.545359] Epoch: [4]  [460/781]  eta: 0:00:53  lr: 0.000229  training_loss: 2.6359 (2.7467)  classification_loss: 2.0944 (2.1321)  loss_mask: 0.5184 (0.6146)  time: 0.1652  data: 0.0003  max mem: 4132
[17:40:24.930108] Epoch: [4]  [480/781]  eta: 0:00:50  lr: 0.000231  training_loss: 2.7602 (2.7469)  classification_loss: 2.1432 (2.1327)  loss_mask: 0.6099 (0.6142)  time: 0.1691  data: 0.0004  max mem: 4132
[17:40:28.267846] Epoch: [4]  [500/781]  eta: 0:00:47  lr: 0.000232  training_loss: 2.7441 (2.7477)  classification_loss: 2.0973 (2.1316)  loss_mask: 0.6302 (0.6161)  time: 0.1668  data: 0.0004  max mem: 4132
[17:40:31.589480] Epoch: [4]  [520/781]  eta: 0:00:43  lr: 0.000233  training_loss: 2.6927 (2.7462)  classification_loss: 2.1098 (2.1307)  loss_mask: 0.5905 (0.6156)  time: 0.1660  data: 0.0003  max mem: 4132
[17:40:34.902263] Epoch: [4]  [540/781]  eta: 0:00:40  lr: 0.000235  training_loss: 2.6714 (2.7450)  classification_loss: 2.1194 (2.1306)  loss_mask: 0.5843 (0.6144)  time: 0.1655  data: 0.0003  max mem: 4132
[17:40:38.263187] Epoch: [4]  [560/781]  eta: 0:00:37  lr: 0.000236  training_loss: 2.6167 (2.7414)  classification_loss: 2.1005 (2.1294)  loss_mask: 0.5255 (0.6120)  time: 0.1679  data: 0.0004  max mem: 4132
[17:40:41.567043] Epoch: [4]  [580/781]  eta: 0:00:33  lr: 0.000237  training_loss: 2.6643 (2.7398)  classification_loss: 2.1126 (2.1286)  loss_mask: 0.5610 (0.6112)  time: 0.1651  data: 0.0003  max mem: 4132
[17:40:44.860749] Epoch: [4]  [600/781]  eta: 0:00:30  lr: 0.000238  training_loss: 2.7747 (2.7419)  classification_loss: 2.1164 (2.1284)  loss_mask: 0.6867 (0.6135)  time: 0.1646  data: 0.0003  max mem: 4132
[17:40:48.183970] Epoch: [4]  [620/781]  eta: 0:00:26  lr: 0.000240  training_loss: 2.7234 (2.7408)  classification_loss: 2.0784 (2.1271)  loss_mask: 0.5838 (0.6138)  time: 0.1661  data: 0.0003  max mem: 4132
[17:40:51.497496] Epoch: [4]  [640/781]  eta: 0:00:23  lr: 0.000241  training_loss: 2.6935 (2.7393)  classification_loss: 2.0835 (2.1263)  loss_mask: 0.5960 (0.6131)  time: 0.1656  data: 0.0004  max mem: 4132
[17:40:54.828846] Epoch: [4]  [660/781]  eta: 0:00:20  lr: 0.000242  training_loss: 2.6672 (2.7370)  classification_loss: 2.1156 (2.1254)  loss_mask: 0.5411 (0.6116)  time: 0.1665  data: 0.0005  max mem: 4132
[17:40:58.167083] Epoch: [4]  [680/781]  eta: 0:00:16  lr: 0.000244  training_loss: 2.7103 (2.7357)  classification_loss: 2.1274 (2.1252)  loss_mask: 0.5782 (0.6106)  time: 0.1668  data: 0.0003  max mem: 4132
[17:41:01.506584] Epoch: [4]  [700/781]  eta: 0:00:13  lr: 0.000245  training_loss: 2.8388 (2.7404)  classification_loss: 2.1001 (2.1254)  loss_mask: 0.7391 (0.6150)  time: 0.1668  data: 0.0003  max mem: 4132
[17:41:04.837484] Epoch: [4]  [720/781]  eta: 0:00:10  lr: 0.000246  training_loss: 2.7639 (2.7408)  classification_loss: 2.1551 (2.1264)  loss_mask: 0.5893 (0.6144)  time: 0.1664  data: 0.0003  max mem: 4132
[17:41:08.145730] Epoch: [4]  [740/781]  eta: 0:00:06  lr: 0.000247  training_loss: 2.7262 (2.7401)  classification_loss: 2.1383 (2.1268)  loss_mask: 0.5725 (0.6133)  time: 0.1653  data: 0.0003  max mem: 4132
[17:41:11.496980] Epoch: [4]  [760/781]  eta: 0:00:03  lr: 0.000249  training_loss: 2.7206 (2.7387)  classification_loss: 2.1164 (2.1269)  loss_mask: 0.5720 (0.6118)  time: 0.1674  data: 0.0004  max mem: 4132
[17:41:14.781330] Epoch: [4]  [780/781]  eta: 0:00:00  lr: 0.000250  training_loss: 2.6372 (2.7359)  classification_loss: 2.1125 (2.1267)  loss_mask: 0.5078 (0.6092)  time: 0.1641  data: 0.0003  max mem: 4132
[17:41:14.980028] Epoch: [4] Total time: 0:02:10 (0.1676 s / it)
[17:41:14.980973] Averaged stats: lr: 0.000250  training_loss: 2.6372 (2.7359)  classification_loss: 2.1125 (2.1267)  loss_mask: 0.5078 (0.6092)
[17:41:15.678440] Test:  [  0/157]  eta: 0:01:48  testing_loss: 1.6939 (1.6939)  acc1: 43.7500 (43.7500)  acc5: 87.5000 (87.5000)  time: 0.6929  data: 0.6635  max mem: 4132
[17:41:15.988728] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.7597 (1.7551)  acc1: 40.6250 (40.6250)  acc5: 90.6250 (89.9148)  time: 0.0908  data: 0.0613  max mem: 4132
[17:41:16.275029] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.7351 (1.7324)  acc1: 40.6250 (41.7411)  acc5: 89.0625 (89.2857)  time: 0.0295  data: 0.0007  max mem: 4132
[17:41:16.560426] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.7113 (1.7265)  acc1: 46.8750 (43.2460)  acc5: 89.0625 (89.2137)  time: 0.0284  data: 0.0002  max mem: 4132
[17:41:16.845287] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.7198 (1.7336)  acc1: 45.3125 (42.4162)  acc5: 89.0625 (89.2530)  time: 0.0284  data: 0.0002  max mem: 4132
[17:41:17.134396] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.7425 (1.7351)  acc1: 42.1875 (42.1569)  acc5: 89.0625 (89.4914)  time: 0.0286  data: 0.0002  max mem: 4132
[17:41:17.423051] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.7428 (1.7314)  acc1: 43.7500 (42.5973)  acc5: 89.0625 (89.4723)  time: 0.0287  data: 0.0003  max mem: 4132
[17:41:17.721433] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.7098 (1.7303)  acc1: 43.7500 (42.2975)  acc5: 90.6250 (89.6787)  time: 0.0292  data: 0.0003  max mem: 4132
[17:41:18.008521] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.7266 (1.7314)  acc1: 40.6250 (42.1296)  acc5: 90.6250 (89.7184)  time: 0.0291  data: 0.0002  max mem: 4132
[17:41:18.296109] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.7509 (1.7345)  acc1: 42.1875 (42.1360)  acc5: 89.0625 (89.5604)  time: 0.0286  data: 0.0002  max mem: 4132
[17:41:18.583848] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.7541 (1.7369)  acc1: 42.1875 (41.8936)  acc5: 87.5000 (89.3255)  time: 0.0286  data: 0.0003  max mem: 4132
[17:41:18.870292] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.7775 (1.7386)  acc1: 40.6250 (41.8497)  acc5: 85.9375 (89.1892)  time: 0.0285  data: 0.0002  max mem: 4132
[17:41:19.158383] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.7208 (1.7357)  acc1: 40.6250 (41.7872)  acc5: 89.0625 (89.3595)  time: 0.0286  data: 0.0002  max mem: 4132
[17:41:19.468764] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.7237 (1.7374)  acc1: 40.6250 (41.8058)  acc5: 90.6250 (89.2295)  time: 0.0298  data: 0.0005  max mem: 4132
[17:41:19.760068] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.7473 (1.7361)  acc1: 42.1875 (41.9659)  acc5: 87.5000 (89.2841)  time: 0.0299  data: 0.0005  max mem: 4132
[17:41:20.045487] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.7283 (1.7356)  acc1: 43.7500 (42.0323)  acc5: 89.0625 (89.2695)  time: 0.0286  data: 0.0002  max mem: 4132
[17:41:20.203726] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.7087 (1.7352)  acc1: 43.7500 (41.9100)  acc5: 87.5000 (89.2800)  time: 0.0279  data: 0.0002  max mem: 4132
[17:41:20.393658] Test: Total time: 0:00:05 (0.0345 s / it)
[17:41:20.394198] * Acc@1 41.910 Acc@5 89.280 loss 1.735
[17:41:20.394546] Accuracy of the network on the 10000 test images: 41.9%
[17:41:20.394894] Max accuracy: 41.91%
[17:41:20.777218] log_dir: ./output_dir
[17:41:21.721427] Epoch: [5]  [  0/781]  eta: 0:12:15  lr: 0.000250  training_loss: 2.5935 (2.5935)  classification_loss: 2.0244 (2.0244)  loss_mask: 0.5691 (0.5691)  time: 0.9423  data: 0.7315  max mem: 4132
[17:41:25.034093] Epoch: [5]  [ 20/781]  eta: 0:02:34  lr: 0.000250  training_loss: 2.6494 (2.6572)  classification_loss: 2.0549 (2.0617)  loss_mask: 0.6098 (0.5955)  time: 0.1655  data: 0.0002  max mem: 4132
[17:41:28.344109] Epoch: [5]  [ 40/781]  eta: 0:02:16  lr: 0.000250  training_loss: 2.7801 (2.7346)  classification_loss: 2.1618 (2.1090)  loss_mask: 0.6063 (0.6256)  time: 0.1654  data: 0.0003  max mem: 4132
[17:41:31.607936] Epoch: [5]  [ 60/781]  eta: 0:02:07  lr: 0.000250  training_loss: 2.6125 (2.6978)  classification_loss: 2.0880 (2.1052)  loss_mask: 0.5002 (0.5926)  time: 0.1631  data: 0.0002  max mem: 4132
[17:41:34.879112] Epoch: [5]  [ 80/781]  eta: 0:02:01  lr: 0.000250  training_loss: 2.5723 (2.6676)  classification_loss: 2.1074 (2.1030)  loss_mask: 0.4552 (0.5646)  time: 0.1635  data: 0.0003  max mem: 4132
[17:41:38.210610] Epoch: [5]  [100/781]  eta: 0:01:57  lr: 0.000250  training_loss: 2.6840 (2.6745)  classification_loss: 2.0993 (2.1039)  loss_mask: 0.5631 (0.5706)  time: 0.1665  data: 0.0006  max mem: 4132
[17:41:41.578532] Epoch: [5]  [120/781]  eta: 0:01:53  lr: 0.000250  training_loss: 2.6015 (2.6670)  classification_loss: 2.0839 (2.1018)  loss_mask: 0.5416 (0.5652)  time: 0.1683  data: 0.0003  max mem: 4132
[17:41:44.851257] Epoch: [5]  [140/781]  eta: 0:01:49  lr: 0.000250  training_loss: 2.5622 (2.6534)  classification_loss: 2.0507 (2.0976)  loss_mask: 0.4654 (0.5558)  time: 0.1635  data: 0.0003  max mem: 4132
[17:41:48.182719] Epoch: [5]  [160/781]  eta: 0:01:45  lr: 0.000250  training_loss: 2.5788 (2.6467)  classification_loss: 2.1028 (2.0998)  loss_mask: 0.4575 (0.5469)  time: 0.1665  data: 0.0002  max mem: 4132
[17:41:51.492224] Epoch: [5]  [180/781]  eta: 0:01:41  lr: 0.000250  training_loss: 2.7508 (2.6591)  classification_loss: 2.1237 (2.1009)  loss_mask: 0.6297 (0.5583)  time: 0.1654  data: 0.0003  max mem: 4132
[17:41:54.796759] Epoch: [5]  [200/781]  eta: 0:01:38  lr: 0.000250  training_loss: 2.6787 (2.6656)  classification_loss: 2.0872 (2.1002)  loss_mask: 0.5809 (0.5653)  time: 0.1651  data: 0.0003  max mem: 4132
[17:41:58.107028] Epoch: [5]  [220/781]  eta: 0:01:34  lr: 0.000250  training_loss: 2.6054 (2.6617)  classification_loss: 2.1181 (2.1015)  loss_mask: 0.4943 (0.5602)  time: 0.1654  data: 0.0003  max mem: 4132
[17:42:01.420728] Epoch: [5]  [240/781]  eta: 0:01:31  lr: 0.000250  training_loss: 2.5682 (2.6563)  classification_loss: 2.1130 (2.1022)  loss_mask: 0.4698 (0.5541)  time: 0.1656  data: 0.0004  max mem: 4132
[17:42:04.728687] Epoch: [5]  [260/781]  eta: 0:01:27  lr: 0.000250  training_loss: 2.6052 (2.6540)  classification_loss: 2.0864 (2.1007)  loss_mask: 0.5277 (0.5533)  time: 0.1653  data: 0.0003  max mem: 4132
[17:42:08.046574] Epoch: [5]  [280/781]  eta: 0:01:24  lr: 0.000250  training_loss: 2.6511 (2.6545)  classification_loss: 2.0637 (2.0995)  loss_mask: 0.5210 (0.5550)  time: 0.1658  data: 0.0003  max mem: 4132
[17:42:11.346730] Epoch: [5]  [300/781]  eta: 0:01:20  lr: 0.000250  training_loss: 2.6931 (2.6592)  classification_loss: 2.1289 (2.1011)  loss_mask: 0.5747 (0.5580)  time: 0.1649  data: 0.0003  max mem: 4132
[17:42:14.633243] Epoch: [5]  [320/781]  eta: 0:01:17  lr: 0.000250  training_loss: 2.6160 (2.6563)  classification_loss: 2.1139 (2.1017)  loss_mask: 0.4949 (0.5546)  time: 0.1642  data: 0.0002  max mem: 4132
[17:42:17.882246] Epoch: [5]  [340/781]  eta: 0:01:13  lr: 0.000250  training_loss: 2.5068 (2.6493)  classification_loss: 2.0807 (2.1007)  loss_mask: 0.4252 (0.5486)  time: 0.1623  data: 0.0002  max mem: 4132
[17:42:21.160887] Epoch: [5]  [360/781]  eta: 0:01:10  lr: 0.000250  training_loss: 2.5862 (2.6470)  classification_loss: 2.0907 (2.1004)  loss_mask: 0.5195 (0.5466)  time: 0.1639  data: 0.0003  max mem: 4132
[17:42:24.451425] Epoch: [5]  [380/781]  eta: 0:01:06  lr: 0.000250  training_loss: 2.6070 (2.6494)  classification_loss: 2.0636 (2.0990)  loss_mask: 0.5610 (0.5504)  time: 0.1644  data: 0.0003  max mem: 4132
[17:42:27.723532] Epoch: [5]  [400/781]  eta: 0:01:03  lr: 0.000250  training_loss: 2.6293 (2.6483)  classification_loss: 2.0949 (2.0991)  loss_mask: 0.5083 (0.5493)  time: 0.1635  data: 0.0002  max mem: 4132
[17:42:30.969658] Epoch: [5]  [420/781]  eta: 0:01:00  lr: 0.000250  training_loss: 2.5808 (2.6443)  classification_loss: 2.0859 (2.0991)  loss_mask: 0.4516 (0.5452)  time: 0.1622  data: 0.0002  max mem: 4132
[17:42:34.258604] Epoch: [5]  [440/781]  eta: 0:00:56  lr: 0.000250  training_loss: 2.6236 (2.6428)  classification_loss: 2.0521 (2.0974)  loss_mask: 0.5444 (0.5454)  time: 0.1644  data: 0.0002  max mem: 4132
[17:42:37.585968] Epoch: [5]  [460/781]  eta: 0:00:53  lr: 0.000250  training_loss: 2.5428 (2.6400)  classification_loss: 2.0631 (2.0962)  loss_mask: 0.5118 (0.5438)  time: 0.1662  data: 0.0002  max mem: 4132
[17:42:40.898634] Epoch: [5]  [480/781]  eta: 0:00:50  lr: 0.000250  training_loss: 2.5657 (2.6378)  classification_loss: 2.1125 (2.0968)  loss_mask: 0.4691 (0.5410)  time: 0.1655  data: 0.0003  max mem: 4132
[17:42:44.200372] Epoch: [5]  [500/781]  eta: 0:00:46  lr: 0.000250  training_loss: 2.6255 (2.6367)  classification_loss: 2.0576 (2.0959)  loss_mask: 0.5375 (0.5407)  time: 0.1650  data: 0.0003  max mem: 4132
[17:42:47.488997] Epoch: [5]  [520/781]  eta: 0:00:43  lr: 0.000250  training_loss: 2.6379 (2.6373)  classification_loss: 2.0458 (2.0947)  loss_mask: 0.5920 (0.5426)  time: 0.1643  data: 0.0002  max mem: 4132
[17:42:50.802489] Epoch: [5]  [540/781]  eta: 0:00:40  lr: 0.000250  training_loss: 2.6048 (2.6367)  classification_loss: 2.0726 (2.0947)  loss_mask: 0.5131 (0.5419)  time: 0.1656  data: 0.0002  max mem: 4132
[17:42:54.132830] Epoch: [5]  [560/781]  eta: 0:00:36  lr: 0.000250  training_loss: 2.5517 (2.6339)  classification_loss: 2.0541 (2.0933)  loss_mask: 0.5050 (0.5406)  time: 0.1664  data: 0.0003  max mem: 4132
[17:42:57.429203] Epoch: [5]  [580/781]  eta: 0:00:33  lr: 0.000250  training_loss: 2.5153 (2.6307)  classification_loss: 2.0798 (2.0928)  loss_mask: 0.4537 (0.5379)  time: 0.1647  data: 0.0003  max mem: 4132
[17:43:00.754775] Epoch: [5]  [600/781]  eta: 0:00:30  lr: 0.000250  training_loss: 2.5705 (2.6293)  classification_loss: 2.0807 (2.0928)  loss_mask: 0.4520 (0.5365)  time: 0.1662  data: 0.0003  max mem: 4132
[17:43:04.028159] Epoch: [5]  [620/781]  eta: 0:00:26  lr: 0.000250  training_loss: 2.4359 (2.6240)  classification_loss: 2.0428 (2.0916)  loss_mask: 0.3809 (0.5324)  time: 0.1636  data: 0.0002  max mem: 4132
[17:43:07.299121] Epoch: [5]  [640/781]  eta: 0:00:23  lr: 0.000250  training_loss: 2.5617 (2.6234)  classification_loss: 2.0706 (2.0909)  loss_mask: 0.5069 (0.5325)  time: 0.1635  data: 0.0002  max mem: 4132
[17:43:10.610188] Epoch: [5]  [660/781]  eta: 0:00:20  lr: 0.000250  training_loss: 2.5974 (2.6225)  classification_loss: 2.0811 (2.0901)  loss_mask: 0.5080 (0.5324)  time: 0.1654  data: 0.0003  max mem: 4132
[17:43:13.895056] Epoch: [5]  [680/781]  eta: 0:00:16  lr: 0.000250  training_loss: 2.5627 (2.6225)  classification_loss: 2.0404 (2.0890)  loss_mask: 0.4952 (0.5335)  time: 0.1642  data: 0.0003  max mem: 4132
[17:43:17.168015] Epoch: [5]  [700/781]  eta: 0:00:13  lr: 0.000250  training_loss: 2.5232 (2.6209)  classification_loss: 2.0761 (2.0885)  loss_mask: 0.4881 (0.5325)  time: 0.1636  data: 0.0003  max mem: 4132
[17:43:20.440760] Epoch: [5]  [720/781]  eta: 0:00:10  lr: 0.000250  training_loss: 2.5348 (2.6186)  classification_loss: 2.0849 (2.0877)  loss_mask: 0.4412 (0.5309)  time: 0.1635  data: 0.0003  max mem: 4132
[17:43:23.732561] Epoch: [5]  [740/781]  eta: 0:00:06  lr: 0.000250  training_loss: 2.5072 (2.6159)  classification_loss: 2.0926 (2.0875)  loss_mask: 0.4165 (0.5283)  time: 0.1645  data: 0.0002  max mem: 4132
[17:43:27.012096] Epoch: [5]  [760/781]  eta: 0:00:03  lr: 0.000250  training_loss: 2.6871 (2.6165)  classification_loss: 2.0728 (2.0869)  loss_mask: 0.5964 (0.5296)  time: 0.1639  data: 0.0003  max mem: 4132
[17:43:30.288848] Epoch: [5]  [780/781]  eta: 0:00:00  lr: 0.000250  training_loss: 2.6017 (2.6178)  classification_loss: 2.0614 (2.0866)  loss_mask: 0.5299 (0.5311)  time: 0.1637  data: 0.0002  max mem: 4132
[17:43:30.459054] Epoch: [5] Total time: 0:02:09 (0.1660 s / it)
[17:43:30.459571] Averaged stats: lr: 0.000250  training_loss: 2.6017 (2.6178)  classification_loss: 2.0614 (2.0866)  loss_mask: 0.5299 (0.5311)
[17:43:31.154264] Test:  [  0/157]  eta: 0:01:48  testing_loss: 1.6224 (1.6224)  acc1: 50.0000 (50.0000)  acc5: 87.5000 (87.5000)  time: 0.6901  data: 0.6587  max mem: 4132
[17:43:31.453550] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.6395 (1.6665)  acc1: 42.1875 (44.6023)  acc5: 90.6250 (90.6250)  time: 0.0898  data: 0.0601  max mem: 4132
[17:43:31.739360] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.6369 (1.6427)  acc1: 46.8750 (46.8750)  acc5: 90.6250 (91.2202)  time: 0.0291  data: 0.0002  max mem: 4132
[17:43:32.029297] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.6104 (1.6378)  acc1: 50.0000 (47.4798)  acc5: 90.6250 (91.4315)  time: 0.0286  data: 0.0002  max mem: 4132
[17:43:32.319326] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.6195 (1.6464)  acc1: 50.0000 (46.7607)  acc5: 90.6250 (90.9680)  time: 0.0288  data: 0.0002  max mem: 4132
[17:43:32.610124] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.6433 (1.6449)  acc1: 48.4375 (46.6912)  acc5: 90.6250 (91.2071)  time: 0.0289  data: 0.0002  max mem: 4132
[17:43:32.899860] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.6297 (1.6406)  acc1: 46.8750 (46.8494)  acc5: 90.6250 (91.2654)  time: 0.0289  data: 0.0002  max mem: 4132
[17:43:33.190927] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.6084 (1.6384)  acc1: 46.8750 (47.1611)  acc5: 93.7500 (91.5273)  time: 0.0289  data: 0.0002  max mem: 4132
[17:43:33.484389] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.6141 (1.6390)  acc1: 46.8750 (47.1451)  acc5: 93.7500 (91.5702)  time: 0.0290  data: 0.0002  max mem: 4132
[17:43:33.787934] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.6629 (1.6428)  acc1: 46.8750 (47.0295)  acc5: 90.6250 (91.4320)  time: 0.0297  data: 0.0002  max mem: 4132
[17:43:34.085645] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.6927 (1.6460)  acc1: 45.3125 (46.7203)  acc5: 90.6250 (91.2902)  time: 0.0298  data: 0.0002  max mem: 4132
[17:43:34.387089] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.6927 (1.6485)  acc1: 40.6250 (46.3682)  acc5: 90.6250 (91.1740)  time: 0.0298  data: 0.0004  max mem: 4132
[17:43:34.675401] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.6247 (1.6458)  acc1: 45.3125 (46.4489)  acc5: 90.6250 (91.2448)  time: 0.0293  data: 0.0004  max mem: 4132
[17:43:34.977400] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.6235 (1.6482)  acc1: 45.3125 (46.2190)  acc5: 90.6250 (91.0782)  time: 0.0294  data: 0.0002  max mem: 4132
[17:43:35.265240] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.6529 (1.6483)  acc1: 45.3125 (46.3763)  acc5: 90.6250 (91.1015)  time: 0.0293  data: 0.0002  max mem: 4132
[17:43:35.548727] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.6334 (1.6476)  acc1: 48.4375 (46.5853)  acc5: 92.1875 (91.0596)  time: 0.0284  data: 0.0002  max mem: 4132
[17:43:35.706797] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.6119 (1.6469)  acc1: 48.4375 (46.4900)  acc5: 92.1875 (91.0700)  time: 0.0276  data: 0.0002  max mem: 4132
[17:43:35.862573] Test: Total time: 0:00:05 (0.0344 s / it)
[17:43:35.863367] * Acc@1 46.490 Acc@5 91.070 loss 1.647
[17:43:35.863678] Accuracy of the network on the 10000 test images: 46.5%
[17:43:35.863922] Max accuracy: 46.49%
[17:43:36.198868] log_dir: ./output_dir
[17:43:37.150374] Epoch: [6]  [  0/781]  eta: 0:12:21  lr: 0.000250  training_loss: 2.5324 (2.5324)  classification_loss: 2.0812 (2.0812)  loss_mask: 0.4511 (0.4511)  time: 0.9494  data: 0.7565  max mem: 4132
[17:43:40.458684] Epoch: [6]  [ 20/781]  eta: 0:02:34  lr: 0.000250  training_loss: 2.5372 (2.5272)  classification_loss: 2.0463 (2.0615)  loss_mask: 0.4809 (0.4658)  time: 0.1653  data: 0.0002  max mem: 4132
[17:43:43.732686] Epoch: [6]  [ 40/781]  eta: 0:02:16  lr: 0.000250  training_loss: 2.4373 (2.4923)  classification_loss: 2.0242 (2.0554)  loss_mask: 0.4302 (0.4369)  time: 0.1636  data: 0.0003  max mem: 4132
[17:43:47.009798] Epoch: [6]  [ 60/781]  eta: 0:02:07  lr: 0.000250  training_loss: 2.4860 (2.4845)  classification_loss: 2.0504 (2.0570)  loss_mask: 0.4041 (0.4275)  time: 0.1637  data: 0.0003  max mem: 4132
[17:43:50.297668] Epoch: [6]  [ 80/781]  eta: 0:02:01  lr: 0.000250  training_loss: 2.5241 (2.5082)  classification_loss: 2.0728 (2.0617)  loss_mask: 0.4676 (0.4465)  time: 0.1643  data: 0.0003  max mem: 4132
[17:43:53.586811] Epoch: [6]  [100/781]  eta: 0:01:57  lr: 0.000250  training_loss: 2.5833 (2.5230)  classification_loss: 2.0509 (2.0636)  loss_mask: 0.5301 (0.4594)  time: 0.1643  data: 0.0002  max mem: 4132
[17:43:56.882451] Epoch: [6]  [120/781]  eta: 0:01:52  lr: 0.000250  training_loss: 2.4831 (2.5211)  classification_loss: 2.0279 (2.0596)  loss_mask: 0.4543 (0.4615)  time: 0.1647  data: 0.0003  max mem: 4132
[17:44:00.194125] Epoch: [6]  [140/781]  eta: 0:01:49  lr: 0.000250  training_loss: 2.4471 (2.5117)  classification_loss: 2.0696 (2.0581)  loss_mask: 0.3969 (0.4536)  time: 0.1655  data: 0.0002  max mem: 4132
[17:44:03.466587] Epoch: [6]  [160/781]  eta: 0:01:45  lr: 0.000250  training_loss: 2.5171 (2.5123)  classification_loss: 2.0963 (2.0628)  loss_mask: 0.4345 (0.4495)  time: 0.1635  data: 0.0002  max mem: 4132
[17:44:06.746742] Epoch: [6]  [180/781]  eta: 0:01:41  lr: 0.000250  training_loss: 2.4768 (2.5089)  classification_loss: 2.0492 (2.0615)  loss_mask: 0.4221 (0.4474)  time: 0.1639  data: 0.0002  max mem: 4132
[17:44:10.049900] Epoch: [6]  [200/781]  eta: 0:01:37  lr: 0.000250  training_loss: 2.5293 (2.5135)  classification_loss: 2.0848 (2.0642)  loss_mask: 0.4488 (0.4493)  time: 0.1651  data: 0.0003  max mem: 4132
[17:44:13.365964] Epoch: [6]  [220/781]  eta: 0:01:34  lr: 0.000250  training_loss: 2.4881 (2.5141)  classification_loss: 2.0805 (2.0639)  loss_mask: 0.4279 (0.4502)  time: 0.1657  data: 0.0003  max mem: 4132
[17:44:16.663337] Epoch: [6]  [240/781]  eta: 0:01:30  lr: 0.000250  training_loss: 2.4755 (2.5124)  classification_loss: 2.0725 (2.0659)  loss_mask: 0.4056 (0.4465)  time: 0.1648  data: 0.0003  max mem: 4132
[17:44:19.979006] Epoch: [6]  [260/781]  eta: 0:01:27  lr: 0.000250  training_loss: 2.4238 (2.5048)  classification_loss: 2.0313 (2.0631)  loss_mask: 0.3768 (0.4417)  time: 0.1657  data: 0.0003  max mem: 4132
[17:44:23.291437] Epoch: [6]  [280/781]  eta: 0:01:23  lr: 0.000250  training_loss: 2.3999 (2.4987)  classification_loss: 2.0101 (2.0613)  loss_mask: 0.3794 (0.4374)  time: 0.1655  data: 0.0003  max mem: 4132
[17:44:26.592061] Epoch: [6]  [300/781]  eta: 0:01:20  lr: 0.000250  training_loss: 2.4266 (2.4956)  classification_loss: 2.0639 (2.0610)  loss_mask: 0.3932 (0.4346)  time: 0.1649  data: 0.0002  max mem: 4132
[17:44:29.891009] Epoch: [6]  [320/781]  eta: 0:01:17  lr: 0.000250  training_loss: 2.4815 (2.5006)  classification_loss: 2.0217 (2.0603)  loss_mask: 0.4307 (0.4403)  time: 0.1648  data: 0.0003  max mem: 4132
[17:44:33.223328] Epoch: [6]  [340/781]  eta: 0:01:13  lr: 0.000250  training_loss: 2.4818 (2.5013)  classification_loss: 2.0247 (2.0573)  loss_mask: 0.5031 (0.4440)  time: 0.1665  data: 0.0003  max mem: 4132
[17:44:36.556650] Epoch: [6]  [360/781]  eta: 0:01:10  lr: 0.000250  training_loss: 2.6408 (2.5087)  classification_loss: 2.0644 (2.0581)  loss_mask: 0.5475 (0.4507)  time: 0.1666  data: 0.0002  max mem: 4132
[17:44:39.813333] Epoch: [6]  [380/781]  eta: 0:01:06  lr: 0.000250  training_loss: 2.5236 (2.5110)  classification_loss: 2.0770 (2.0593)  loss_mask: 0.4471 (0.4517)  time: 0.1627  data: 0.0002  max mem: 4132
[17:44:43.100227] Epoch: [6]  [400/781]  eta: 0:01:03  lr: 0.000250  training_loss: 2.5022 (2.5100)  classification_loss: 2.0700 (2.0593)  loss_mask: 0.4172 (0.4507)  time: 0.1643  data: 0.0003  max mem: 4132
[17:44:46.425741] Epoch: [6]  [420/781]  eta: 0:01:00  lr: 0.000250  training_loss: 2.4340 (2.5061)  classification_loss: 2.0435 (2.0587)  loss_mask: 0.3494 (0.4475)  time: 0.1662  data: 0.0002  max mem: 4132
[17:44:49.697144] Epoch: [6]  [440/781]  eta: 0:00:56  lr: 0.000250  training_loss: 2.3529 (2.4997)  classification_loss: 2.0243 (2.0568)  loss_mask: 0.3272 (0.4429)  time: 0.1635  data: 0.0002  max mem: 4132
[17:44:52.973295] Epoch: [6]  [460/781]  eta: 0:00:53  lr: 0.000250  training_loss: 2.3880 (2.4950)  classification_loss: 2.0046 (2.0547)  loss_mask: 0.3924 (0.4403)  time: 0.1637  data: 0.0002  max mem: 4132
[17:44:56.290040] Epoch: [6]  [480/781]  eta: 0:00:50  lr: 0.000250  training_loss: 2.4928 (2.4948)  classification_loss: 2.0614 (2.0542)  loss_mask: 0.4434 (0.4406)  time: 0.1657  data: 0.0003  max mem: 4132
[17:44:59.571878] Epoch: [6]  [500/781]  eta: 0:00:46  lr: 0.000250  training_loss: 2.5357 (2.4967)  classification_loss: 2.0427 (2.0543)  loss_mask: 0.4536 (0.4424)  time: 0.1640  data: 0.0002  max mem: 4132
[17:45:02.884702] Epoch: [6]  [520/781]  eta: 0:00:43  lr: 0.000250  training_loss: 2.3880 (2.4935)  classification_loss: 2.0015 (2.0539)  loss_mask: 0.3747 (0.4396)  time: 0.1656  data: 0.0003  max mem: 4132
[17:45:06.181231] Epoch: [6]  [540/781]  eta: 0:00:40  lr: 0.000250  training_loss: 2.4049 (2.4908)  classification_loss: 2.0556 (2.0541)  loss_mask: 0.3349 (0.4368)  time: 0.1647  data: 0.0002  max mem: 4132
[17:45:09.481965] Epoch: [6]  [560/781]  eta: 0:00:36  lr: 0.000250  training_loss: 2.3281 (2.4863)  classification_loss: 2.0335 (2.0534)  loss_mask: 0.3123 (0.4330)  time: 0.1649  data: 0.0002  max mem: 4132
[17:45:12.790338] Epoch: [6]  [580/781]  eta: 0:00:33  lr: 0.000250  training_loss: 2.3591 (2.4826)  classification_loss: 2.0408 (2.0525)  loss_mask: 0.3332 (0.4302)  time: 0.1653  data: 0.0004  max mem: 4132
[17:45:16.104275] Epoch: [6]  [600/781]  eta: 0:00:30  lr: 0.000250  training_loss: 2.3777 (2.4797)  classification_loss: 2.0284 (2.0513)  loss_mask: 0.3520 (0.4284)  time: 0.1656  data: 0.0003  max mem: 4132
[17:45:19.410151] Epoch: [6]  [620/781]  eta: 0:00:26  lr: 0.000250  training_loss: 2.3863 (2.4769)  classification_loss: 2.0111 (2.0498)  loss_mask: 0.3792 (0.4271)  time: 0.1651  data: 0.0002  max mem: 4132
[17:45:22.751452] Epoch: [6]  [640/781]  eta: 0:00:23  lr: 0.000250  training_loss: 2.3938 (2.4754)  classification_loss: 2.0597 (2.0498)  loss_mask: 0.3653 (0.4256)  time: 0.1670  data: 0.0003  max mem: 4132
[17:45:26.023379] Epoch: [6]  [660/781]  eta: 0:00:20  lr: 0.000250  training_loss: 2.3832 (2.4727)  classification_loss: 2.0564 (2.0500)  loss_mask: 0.3152 (0.4227)  time: 0.1634  data: 0.0003  max mem: 4132
[17:45:29.327002] Epoch: [6]  [680/781]  eta: 0:00:16  lr: 0.000250  training_loss: 2.3308 (2.4692)  classification_loss: 2.0319 (2.0493)  loss_mask: 0.3032 (0.4199)  time: 0.1651  data: 0.0003  max mem: 4132
[17:45:32.623249] Epoch: [6]  [700/781]  eta: 0:00:13  lr: 0.000250  training_loss: 2.2728 (2.4646)  classification_loss: 2.0355 (2.0492)  loss_mask: 0.2624 (0.4155)  time: 0.1647  data: 0.0002  max mem: 4132
[17:45:35.907581] Epoch: [6]  [720/781]  eta: 0:00:10  lr: 0.000250  training_loss: 2.3767 (2.4625)  classification_loss: 2.0511 (2.0494)  loss_mask: 0.3451 (0.4131)  time: 0.1641  data: 0.0003  max mem: 4132
[17:45:39.201890] Epoch: [6]  [740/781]  eta: 0:00:06  lr: 0.000250  training_loss: 2.3184 (2.4586)  classification_loss: 2.0235 (2.0485)  loss_mask: 0.2764 (0.4101)  time: 0.1646  data: 0.0003  max mem: 4132
[17:45:42.476296] Epoch: [6]  [760/781]  eta: 0:00:03  lr: 0.000250  training_loss: 2.3528 (2.4564)  classification_loss: 2.0257 (2.0483)  loss_mask: 0.3244 (0.4082)  time: 0.1636  data: 0.0002  max mem: 4132
[17:45:45.736758] Epoch: [6]  [780/781]  eta: 0:00:00  lr: 0.000250  training_loss: 2.4147 (2.4551)  classification_loss: 2.0417 (2.0481)  loss_mask: 0.3586 (0.4070)  time: 0.1629  data: 0.0002  max mem: 4132
[17:45:45.925929] Epoch: [6] Total time: 0:02:09 (0.1661 s / it)
[17:45:45.926414] Averaged stats: lr: 0.000250  training_loss: 2.4147 (2.4551)  classification_loss: 2.0417 (2.0481)  loss_mask: 0.3586 (0.4070)
[17:45:46.641648] Test:  [  0/157]  eta: 0:01:51  testing_loss: 1.5839 (1.5839)  acc1: 54.6875 (54.6875)  acc5: 92.1875 (92.1875)  time: 0.7096  data: 0.6777  max mem: 4132
[17:45:46.934254] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.6554 (1.6630)  acc1: 43.7500 (43.8920)  acc5: 90.6250 (89.7727)  time: 0.0909  data: 0.0618  max mem: 4132
[17:45:47.233110] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.6299 (1.6416)  acc1: 45.3125 (46.0565)  acc5: 90.6250 (90.3274)  time: 0.0294  data: 0.0003  max mem: 4132
[17:45:47.520290] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.5962 (1.6250)  acc1: 48.4375 (47.4294)  acc5: 90.6250 (90.7258)  time: 0.0292  data: 0.0003  max mem: 4132
[17:45:47.807657] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.5908 (1.6357)  acc1: 48.4375 (46.9512)  acc5: 90.6250 (90.4345)  time: 0.0286  data: 0.0002  max mem: 4132
[17:45:48.092847] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.6306 (1.6350)  acc1: 45.3125 (46.9363)  acc5: 89.0625 (90.3186)  time: 0.0285  data: 0.0002  max mem: 4132
[17:45:48.381524] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.6125 (1.6289)  acc1: 50.0000 (47.4385)  acc5: 90.6250 (90.3945)  time: 0.0286  data: 0.0002  max mem: 4132
[17:45:48.677111] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.5936 (1.6288)  acc1: 48.4375 (47.5572)  acc5: 90.6250 (90.5590)  time: 0.0291  data: 0.0002  max mem: 4132
[17:45:48.969559] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.6149 (1.6283)  acc1: 45.3125 (47.4730)  acc5: 92.1875 (90.7215)  time: 0.0292  data: 0.0004  max mem: 4132
[17:45:49.264605] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.6448 (1.6295)  acc1: 46.8750 (47.5103)  acc5: 90.6250 (90.6078)  time: 0.0292  data: 0.0004  max mem: 4132
[17:45:49.556035] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.6796 (1.6338)  acc1: 45.3125 (47.1225)  acc5: 90.6250 (90.5786)  time: 0.0291  data: 0.0002  max mem: 4132
[17:45:49.846609] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.6713 (1.6372)  acc1: 40.6250 (46.7905)  acc5: 90.6250 (90.5405)  time: 0.0289  data: 0.0002  max mem: 4132
[17:45:50.130664] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.6469 (1.6332)  acc1: 45.3125 (46.9912)  acc5: 92.1875 (90.7025)  time: 0.0286  data: 0.0002  max mem: 4132
[17:45:50.414006] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.6116 (1.6364)  acc1: 45.3125 (46.7199)  acc5: 90.6250 (90.5296)  time: 0.0282  data: 0.0002  max mem: 4132
[17:45:50.699991] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.6532 (1.6365)  acc1: 43.7500 (46.7974)  acc5: 90.6250 (90.5363)  time: 0.0283  data: 0.0002  max mem: 4132
[17:45:50.980935] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.6189 (1.6352)  acc1: 45.3125 (46.8957)  acc5: 90.6250 (90.5422)  time: 0.0282  data: 0.0001  max mem: 4132
[17:45:51.134030] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.6078 (1.6352)  acc1: 50.0000 (46.9400)  acc5: 90.6250 (90.5900)  time: 0.0272  data: 0.0001  max mem: 4132
[17:45:51.299371] Test: Total time: 0:00:05 (0.0342 s / it)
[17:45:51.299896] * Acc@1 46.940 Acc@5 90.590 loss 1.635
[17:45:51.300199] Accuracy of the network on the 10000 test images: 46.9%
[17:45:51.300381] Max accuracy: 46.94%
[17:45:51.619909] log_dir: ./output_dir
[17:45:52.491579] Epoch: [7]  [  0/781]  eta: 0:11:19  lr: 0.000250  training_loss: 2.3445 (2.3445)  classification_loss: 2.0623 (2.0623)  loss_mask: 0.2822 (0.2822)  time: 0.8700  data: 0.6953  max mem: 4132
[17:45:55.792229] Epoch: [7]  [ 20/781]  eta: 0:02:31  lr: 0.000250  training_loss: 2.3819 (2.3867)  classification_loss: 1.9934 (2.0189)  loss_mask: 0.3688 (0.3678)  time: 0.1649  data: 0.0002  max mem: 4132
[17:45:59.097640] Epoch: [7]  [ 40/781]  eta: 0:02:15  lr: 0.000250  training_loss: 2.3442 (2.3861)  classification_loss: 2.0407 (2.0261)  loss_mask: 0.3076 (0.3600)  time: 0.1652  data: 0.0003  max mem: 4132
[17:46:02.437662] Epoch: [7]  [ 60/781]  eta: 0:02:07  lr: 0.000250  training_loss: 2.3644 (2.3793)  classification_loss: 2.0668 (2.0370)  loss_mask: 0.2831 (0.3423)  time: 0.1669  data: 0.0004  max mem: 4132
[17:46:05.757396] Epoch: [7]  [ 80/781]  eta: 0:02:02  lr: 0.000250  training_loss: 2.2572 (2.3525)  classification_loss: 1.9807 (2.0290)  loss_mask: 0.2683 (0.3236)  time: 0.1659  data: 0.0003  max mem: 4132
[17:46:09.048576] Epoch: [7]  [100/781]  eta: 0:01:57  lr: 0.000250  training_loss: 2.2515 (2.3424)  classification_loss: 1.9874 (2.0275)  loss_mask: 0.2603 (0.3149)  time: 0.1645  data: 0.0003  max mem: 4132
[17:46:12.357140] Epoch: [7]  [120/781]  eta: 0:01:53  lr: 0.000250  training_loss: 2.2808 (2.3325)  classification_loss: 1.9834 (2.0220)  loss_mask: 0.2722 (0.3105)  time: 0.1653  data: 0.0002  max mem: 4132
[17:46:15.720746] Epoch: [7]  [140/781]  eta: 0:01:49  lr: 0.000250  training_loss: 2.2537 (2.3299)  classification_loss: 1.9499 (2.0168)  loss_mask: 0.3006 (0.3131)  time: 0.1681  data: 0.0003  max mem: 4132
[17:46:19.035661] Epoch: [7]  [160/781]  eta: 0:01:45  lr: 0.000250  training_loss: 2.3547 (2.3373)  classification_loss: 2.0114 (2.0180)  loss_mask: 0.3390 (0.3193)  time: 0.1656  data: 0.0003  max mem: 4132
[17:46:22.307523] Epoch: [7]  [180/781]  eta: 0:01:41  lr: 0.000250  training_loss: 2.2964 (2.3361)  classification_loss: 2.0242 (2.0191)  loss_mask: 0.2673 (0.3170)  time: 0.1635  data: 0.0003  max mem: 4132
[17:46:25.578739] Epoch: [7]  [200/781]  eta: 0:01:38  lr: 0.000250  training_loss: 2.2889 (2.3313)  classification_loss: 2.0281 (2.0201)  loss_mask: 0.2486 (0.3112)  time: 0.1635  data: 0.0002  max mem: 4132
[17:46:28.878524] Epoch: [7]  [220/781]  eta: 0:01:34  lr: 0.000250  training_loss: 2.2277 (2.3239)  classification_loss: 1.9969 (2.0197)  loss_mask: 0.2339 (0.3043)  time: 0.1649  data: 0.0002  max mem: 4132
[17:46:32.195887] Epoch: [7]  [240/781]  eta: 0:01:31  lr: 0.000250  training_loss: 2.2592 (2.3183)  classification_loss: 2.0234 (2.0194)  loss_mask: 0.2246 (0.2989)  time: 0.1658  data: 0.0004  max mem: 4132
[17:46:35.532730] Epoch: [7]  [260/781]  eta: 0:01:27  lr: 0.000250  training_loss: 2.2619 (2.3206)  classification_loss: 2.0193 (2.0196)  loss_mask: 0.2887 (0.3010)  time: 0.1668  data: 0.0007  max mem: 4132
[17:46:38.821579] Epoch: [7]  [280/781]  eta: 0:01:24  lr: 0.000250  training_loss: 2.3794 (2.3268)  classification_loss: 2.0161 (2.0194)  loss_mask: 0.3505 (0.3075)  time: 0.1644  data: 0.0005  max mem: 4132
[17:46:42.113554] Epoch: [7]  [300/781]  eta: 0:01:20  lr: 0.000250  training_loss: 2.2859 (2.3265)  classification_loss: 2.0091 (2.0195)  loss_mask: 0.2793 (0.3070)  time: 0.1645  data: 0.0002  max mem: 4132
[17:46:45.402982] Epoch: [7]  [320/781]  eta: 0:01:17  lr: 0.000250  training_loss: 2.2732 (2.3223)  classification_loss: 2.0126 (2.0188)  loss_mask: 0.2500 (0.3035)  time: 0.1644  data: 0.0003  max mem: 4132
[17:46:48.701573] Epoch: [7]  [340/781]  eta: 0:01:13  lr: 0.000250  training_loss: 2.2789 (2.3197)  classification_loss: 1.9884 (2.0176)  loss_mask: 0.2646 (0.3021)  time: 0.1648  data: 0.0003  max mem: 4132
[17:46:51.990551] Epoch: [7]  [360/781]  eta: 0:01:10  lr: 0.000250  training_loss: 2.2942 (2.3190)  classification_loss: 1.9917 (2.0168)  loss_mask: 0.2810 (0.3022)  time: 0.1644  data: 0.0003  max mem: 4132
[17:46:55.264968] Epoch: [7]  [380/781]  eta: 0:01:06  lr: 0.000250  training_loss: 2.2480 (2.3171)  classification_loss: 2.0157 (2.0165)  loss_mask: 0.2453 (0.3005)  time: 0.1636  data: 0.0002  max mem: 4132
[17:46:58.578355] Epoch: [7]  [400/781]  eta: 0:01:03  lr: 0.000250  training_loss: 2.2924 (2.3166)  classification_loss: 1.9602 (2.0145)  loss_mask: 0.2888 (0.3021)  time: 0.1656  data: 0.0003  max mem: 4132
[17:47:01.860730] Epoch: [7]  [420/781]  eta: 0:01:00  lr: 0.000250  training_loss: 2.2195 (2.3130)  classification_loss: 1.9745 (2.0139)  loss_mask: 0.1950 (0.2991)  time: 0.1640  data: 0.0002  max mem: 4132
[17:47:05.145577] Epoch: [7]  [440/781]  eta: 0:00:56  lr: 0.000250  training_loss: 2.1796 (2.3085)  classification_loss: 1.9785 (2.0137)  loss_mask: 0.2006 (0.2947)  time: 0.1642  data: 0.0002  max mem: 4132
[17:47:08.452896] Epoch: [7]  [460/781]  eta: 0:00:53  lr: 0.000250  training_loss: 2.1875 (2.3047)  classification_loss: 1.9803 (2.0134)  loss_mask: 0.2027 (0.2913)  time: 0.1653  data: 0.0002  max mem: 4132
[17:47:11.742652] Epoch: [7]  [480/781]  eta: 0:00:50  lr: 0.000250  training_loss: 2.2435 (2.3028)  classification_loss: 2.0000 (2.0135)  loss_mask: 0.2196 (0.2893)  time: 0.1644  data: 0.0002  max mem: 4132
[17:47:15.011421] Epoch: [7]  [500/781]  eta: 0:00:46  lr: 0.000250  training_loss: 2.2083 (2.2995)  classification_loss: 2.0021 (2.0131)  loss_mask: 0.2033 (0.2863)  time: 0.1634  data: 0.0002  max mem: 4132
[17:47:18.284646] Epoch: [7]  [520/781]  eta: 0:00:43  lr: 0.000250  training_loss: 2.2545 (2.2975)  classification_loss: 1.9831 (2.0125)  loss_mask: 0.2353 (0.2850)  time: 0.1636  data: 0.0002  max mem: 4132
[17:47:21.559365] Epoch: [7]  [540/781]  eta: 0:00:40  lr: 0.000250  training_loss: 2.2546 (2.2972)  classification_loss: 2.0134 (2.0121)  loss_mask: 0.2412 (0.2850)  time: 0.1636  data: 0.0002  max mem: 4132
[17:47:24.891203] Epoch: [7]  [560/781]  eta: 0:00:36  lr: 0.000249  training_loss: 2.2289 (2.2951)  classification_loss: 1.9982 (2.0124)  loss_mask: 0.2179 (0.2826)  time: 0.1665  data: 0.0003  max mem: 4132
[17:47:28.142116] Epoch: [7]  [580/781]  eta: 0:00:33  lr: 0.000249  training_loss: 2.1593 (2.2905)  classification_loss: 1.9842 (2.0118)  loss_mask: 0.1694 (0.2787)  time: 0.1625  data: 0.0003  max mem: 4132
[17:47:31.384298] Epoch: [7]  [600/781]  eta: 0:00:30  lr: 0.000249  training_loss: 2.2407 (2.2890)  classification_loss: 1.9766 (2.0115)  loss_mask: 0.2248 (0.2775)  time: 0.1620  data: 0.0003  max mem: 4132
[17:47:34.680342] Epoch: [7]  [620/781]  eta: 0:00:26  lr: 0.000249  training_loss: 2.1722 (2.2859)  classification_loss: 1.9666 (2.0107)  loss_mask: 0.1831 (0.2752)  time: 0.1646  data: 0.0002  max mem: 4132
[17:47:38.003995] Epoch: [7]  [640/781]  eta: 0:00:23  lr: 0.000249  training_loss: 2.1394 (2.2817)  classification_loss: 2.0015 (2.0103)  loss_mask: 0.1383 (0.2714)  time: 0.1661  data: 0.0006  max mem: 4132
[17:47:41.344939] Epoch: [7]  [660/781]  eta: 0:00:20  lr: 0.000249  training_loss: 2.1262 (2.2787)  classification_loss: 2.0072 (2.0102)  loss_mask: 0.1551 (0.2685)  time: 0.1670  data: 0.0003  max mem: 4132
[17:47:44.677181] Epoch: [7]  [680/781]  eta: 0:00:16  lr: 0.000249  training_loss: 2.2292 (2.2780)  classification_loss: 1.9969 (2.0104)  loss_mask: 0.2011 (0.2676)  time: 0.1665  data: 0.0006  max mem: 4132
[17:47:47.999562] Epoch: [7]  [700/781]  eta: 0:00:13  lr: 0.000249  training_loss: 2.1968 (2.2752)  classification_loss: 1.9778 (2.0099)  loss_mask: 0.1810 (0.2653)  time: 0.1660  data: 0.0003  max mem: 4132
[17:47:51.358631] Epoch: [7]  [720/781]  eta: 0:00:10  lr: 0.000249  training_loss: 2.1778 (2.2729)  classification_loss: 2.0306 (2.0103)  loss_mask: 0.1687 (0.2626)  time: 0.1678  data: 0.0003  max mem: 4132
[17:47:54.674862] Epoch: [7]  [740/781]  eta: 0:00:06  lr: 0.000249  training_loss: 2.1585 (2.2700)  classification_loss: 1.9985 (2.0099)  loss_mask: 0.1612 (0.2601)  time: 0.1657  data: 0.0003  max mem: 4132
[17:47:57.974521] Epoch: [7]  [760/781]  eta: 0:00:03  lr: 0.000249  training_loss: 2.2028 (2.2684)  classification_loss: 2.0294 (2.0101)  loss_mask: 0.1525 (0.2583)  time: 0.1649  data: 0.0003  max mem: 4132
[17:48:01.221877] Epoch: [7]  [780/781]  eta: 0:00:00  lr: 0.000249  training_loss: 2.1353 (2.2659)  classification_loss: 1.9702 (2.0094)  loss_mask: 0.1756 (0.2565)  time: 0.1623  data: 0.0002  max mem: 4132
[17:48:01.395242] Epoch: [7] Total time: 0:02:09 (0.1662 s / it)
[17:48:01.395903] Averaged stats: lr: 0.000249  training_loss: 2.1353 (2.2659)  classification_loss: 1.9702 (2.0094)  loss_mask: 0.1756 (0.2565)
[17:48:02.103040] Test:  [  0/157]  eta: 0:01:50  testing_loss: 1.4578 (1.4578)  acc1: 59.3750 (59.3750)  acc5: 95.3125 (95.3125)  time: 0.7035  data: 0.6730  max mem: 4132
[17:48:02.397470] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.5437 (1.5620)  acc1: 48.4375 (47.3011)  acc5: 92.1875 (92.1875)  time: 0.0905  data: 0.0615  max mem: 4132
[17:48:02.687172] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.5067 (1.5287)  acc1: 50.0000 (49.2560)  acc5: 92.1875 (92.7827)  time: 0.0290  data: 0.0003  max mem: 4132
[17:48:02.972237] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.5021 (1.5232)  acc1: 51.5625 (50.2016)  acc5: 92.1875 (92.5403)  time: 0.0286  data: 0.0002  max mem: 4132
[17:48:03.266726] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.5221 (1.5344)  acc1: 48.4375 (49.1616)  acc5: 90.6250 (91.6921)  time: 0.0288  data: 0.0003  max mem: 4132
[17:48:03.569466] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.5194 (1.5334)  acc1: 48.4375 (49.6630)  acc5: 92.1875 (91.8505)  time: 0.0297  data: 0.0003  max mem: 4132
[17:48:03.873792] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.4976 (1.5268)  acc1: 50.0000 (49.6414)  acc5: 92.1875 (91.9826)  time: 0.0302  data: 0.0006  max mem: 4132
[17:48:04.165107] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.4908 (1.5234)  acc1: 50.0000 (49.7359)  acc5: 93.7500 (92.3636)  time: 0.0296  data: 0.0005  max mem: 4132
[17:48:04.457922] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.4927 (1.5237)  acc1: 51.5625 (49.9035)  acc5: 93.7500 (92.3225)  time: 0.0289  data: 0.0003  max mem: 4132
[17:48:04.749817] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.5403 (1.5272)  acc1: 51.5625 (50.0000)  acc5: 92.1875 (92.3077)  time: 0.0290  data: 0.0002  max mem: 4132
[17:48:05.036291] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.5524 (1.5315)  acc1: 48.4375 (49.7215)  acc5: 92.1875 (92.2184)  time: 0.0288  data: 0.0002  max mem: 4132
[17:48:05.328136] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.5825 (1.5361)  acc1: 46.8750 (49.2962)  acc5: 92.1875 (92.2438)  time: 0.0288  data: 0.0002  max mem: 4132
[17:48:05.619910] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.5434 (1.5328)  acc1: 48.4375 (49.5610)  acc5: 93.7500 (92.3554)  time: 0.0290  data: 0.0003  max mem: 4132
[17:48:05.909695] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.5076 (1.5345)  acc1: 51.5625 (49.3917)  acc5: 92.1875 (92.2948)  time: 0.0289  data: 0.0003  max mem: 4132
[17:48:06.195087] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.5379 (1.5357)  acc1: 48.4375 (49.3905)  acc5: 90.6250 (92.2318)  time: 0.0286  data: 0.0002  max mem: 4132
[17:48:06.478546] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.5360 (1.5338)  acc1: 50.0000 (49.5757)  acc5: 90.6250 (92.2392)  time: 0.0283  data: 0.0002  max mem: 4132
[17:48:06.631661] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.5152 (1.5344)  acc1: 50.0000 (49.5800)  acc5: 92.1875 (92.2300)  time: 0.0273  data: 0.0001  max mem: 4132
[17:48:06.788038] Test: Total time: 0:00:05 (0.0343 s / it)
[17:48:06.788601] * Acc@1 49.580 Acc@5 92.230 loss 1.534
[17:48:06.789109] Accuracy of the network on the 10000 test images: 49.6%
[17:48:06.789396] Max accuracy: 49.58%
[17:48:07.074143] log_dir: ./output_dir
[17:48:07.944329] Epoch: [8]  [  0/781]  eta: 0:11:17  lr: 0.000249  training_loss: 2.0501 (2.0501)  classification_loss: 1.8790 (1.8790)  loss_mask: 0.1711 (0.1711)  time: 0.8680  data: 0.6857  max mem: 4132
[17:48:11.214999] Epoch: [8]  [ 20/781]  eta: 0:02:29  lr: 0.000249  training_loss: 2.1906 (2.2521)  classification_loss: 1.9835 (1.9841)  loss_mask: 0.1985 (0.2680)  time: 0.1634  data: 0.0002  max mem: 4132
[17:48:14.506666] Epoch: [8]  [ 40/781]  eta: 0:02:14  lr: 0.000249  training_loss: 2.2180 (2.2431)  classification_loss: 2.0200 (1.9946)  loss_mask: 0.1894 (0.2485)  time: 0.1645  data: 0.0003  max mem: 4132
[17:48:17.782146] Epoch: [8]  [ 60/781]  eta: 0:02:06  lr: 0.000249  training_loss: 2.1232 (2.2055)  classification_loss: 1.9500 (1.9800)  loss_mask: 0.1683 (0.2256)  time: 0.1637  data: 0.0002  max mem: 4132
[17:48:21.081533] Epoch: [8]  [ 80/781]  eta: 0:02:01  lr: 0.000249  training_loss: 2.1665 (2.1972)  classification_loss: 1.9855 (1.9890)  loss_mask: 0.1442 (0.2082)  time: 0.1649  data: 0.0003  max mem: 4132
[17:48:24.374236] Epoch: [8]  [100/781]  eta: 0:01:56  lr: 0.000249  training_loss: 2.1856 (2.1936)  classification_loss: 2.0039 (1.9939)  loss_mask: 0.1434 (0.1998)  time: 0.1646  data: 0.0003  max mem: 4132
[17:48:27.667100] Epoch: [8]  [120/781]  eta: 0:01:52  lr: 0.000249  training_loss: 2.0903 (2.1788)  classification_loss: 1.9179 (1.9857)  loss_mask: 0.1510 (0.1931)  time: 0.1645  data: 0.0003  max mem: 4132
[17:48:30.967907] Epoch: [8]  [140/781]  eta: 0:01:48  lr: 0.000249  training_loss: 2.0878 (2.1698)  classification_loss: 1.9299 (1.9773)  loss_mask: 0.1435 (0.1924)  time: 0.1649  data: 0.0002  max mem: 4132
[17:48:34.261492] Epoch: [8]  [160/781]  eta: 0:01:44  lr: 0.000249  training_loss: 2.1434 (2.1633)  classification_loss: 1.9831 (1.9779)  loss_mask: 0.1278 (0.1854)  time: 0.1646  data: 0.0002  max mem: 4132
[17:48:37.625561] Epoch: [8]  [180/781]  eta: 0:01:41  lr: 0.000249  training_loss: 2.0657 (2.1534)  classification_loss: 1.9123 (1.9745)  loss_mask: 0.1288 (0.1789)  time: 0.1681  data: 0.0003  max mem: 4132
[17:48:40.898606] Epoch: [8]  [200/781]  eta: 0:01:37  lr: 0.000249  training_loss: 2.0703 (2.1459)  classification_loss: 1.9425 (1.9730)  loss_mask: 0.1134 (0.1729)  time: 0.1636  data: 0.0003  max mem: 4132
[17:48:44.169450] Epoch: [8]  [220/781]  eta: 0:01:34  lr: 0.000249  training_loss: 2.0843 (2.1410)  classification_loss: 1.9308 (1.9709)  loss_mask: 0.1295 (0.1701)  time: 0.1635  data: 0.0003  max mem: 4132
[17:48:47.446738] Epoch: [8]  [240/781]  eta: 0:01:30  lr: 0.000249  training_loss: 2.1031 (2.1376)  classification_loss: 1.9718 (1.9725)  loss_mask: 0.1016 (0.1651)  time: 0.1638  data: 0.0002  max mem: 4132
[17:48:50.732704] Epoch: [8]  [260/781]  eta: 0:01:27  lr: 0.000249  training_loss: 2.1023 (2.1359)  classification_loss: 1.9836 (1.9734)  loss_mask: 0.1173 (0.1624)  time: 0.1642  data: 0.0003  max mem: 4132
[17:48:54.023850] Epoch: [8]  [280/781]  eta: 0:01:23  lr: 0.000249  training_loss: 2.1056 (2.1331)  classification_loss: 1.9788 (1.9731)  loss_mask: 0.1227 (0.1600)  time: 0.1645  data: 0.0003  max mem: 4132
[17:48:57.305426] Epoch: [8]  [300/781]  eta: 0:01:20  lr: 0.000249  training_loss: 2.1234 (2.1352)  classification_loss: 1.9835 (1.9748)  loss_mask: 0.1340 (0.1604)  time: 0.1640  data: 0.0003  max mem: 4132
[17:49:00.659484] Epoch: [8]  [320/781]  eta: 0:01:16  lr: 0.000249  training_loss: 2.1409 (2.1401)  classification_loss: 1.9723 (1.9745)  loss_mask: 0.1779 (0.1656)  time: 0.1676  data: 0.0003  max mem: 4132
[17:49:03.948867] Epoch: [8]  [340/781]  eta: 0:01:13  lr: 0.000249  training_loss: 2.0964 (2.1380)  classification_loss: 1.9709 (1.9742)  loss_mask: 0.1237 (0.1638)  time: 0.1644  data: 0.0003  max mem: 4132
[17:49:07.220743] Epoch: [8]  [360/781]  eta: 0:01:10  lr: 0.000249  training_loss: 2.0926 (2.1368)  classification_loss: 1.9735 (1.9743)  loss_mask: 0.1191 (0.1625)  time: 0.1635  data: 0.0002  max mem: 4132
[17:49:10.513834] Epoch: [8]  [380/781]  eta: 0:01:06  lr: 0.000249  training_loss: 2.0966 (2.1363)  classification_loss: 1.9497 (1.9741)  loss_mask: 0.1574 (0.1622)  time: 0.1645  data: 0.0003  max mem: 4132
[17:49:13.870877] Epoch: [8]  [400/781]  eta: 0:01:03  lr: 0.000249  training_loss: 2.1128 (2.1355)  classification_loss: 1.9797 (1.9752)  loss_mask: 0.1235 (0.1603)  time: 0.1678  data: 0.0004  max mem: 4132
[17:49:17.194079] Epoch: [8]  [420/781]  eta: 0:01:00  lr: 0.000249  training_loss: 2.1060 (2.1331)  classification_loss: 1.9575 (1.9743)  loss_mask: 0.1186 (0.1587)  time: 0.1661  data: 0.0004  max mem: 4132
[17:49:20.483655] Epoch: [8]  [440/781]  eta: 0:00:56  lr: 0.000249  training_loss: 2.1125 (2.1342)  classification_loss: 1.9428 (1.9729)  loss_mask: 0.1622 (0.1613)  time: 0.1643  data: 0.0003  max mem: 4132
[17:49:23.812423] Epoch: [8]  [460/781]  eta: 0:00:53  lr: 0.000249  training_loss: 2.2119 (2.1379)  classification_loss: 1.9456 (1.9725)  loss_mask: 0.2173 (0.1654)  time: 0.1663  data: 0.0003  max mem: 4132
[17:49:27.121291] Epoch: [8]  [480/781]  eta: 0:00:50  lr: 0.000249  training_loss: 2.0994 (2.1367)  classification_loss: 1.9323 (1.9716)  loss_mask: 0.1415 (0.1651)  time: 0.1653  data: 0.0003  max mem: 4132
[17:49:30.428017] Epoch: [8]  [500/781]  eta: 0:00:46  lr: 0.000249  training_loss: 2.1178 (2.1371)  classification_loss: 1.9518 (1.9706)  loss_mask: 0.1753 (0.1665)  time: 0.1652  data: 0.0003  max mem: 4132
[17:49:33.746275] Epoch: [8]  [520/781]  eta: 0:00:43  lr: 0.000249  training_loss: 2.0930 (2.1356)  classification_loss: 1.9346 (1.9693)  loss_mask: 0.1393 (0.1663)  time: 0.1658  data: 0.0003  max mem: 4132
[17:49:37.038992] Epoch: [8]  [540/781]  eta: 0:00:40  lr: 0.000249  training_loss: 2.1403 (2.1362)  classification_loss: 1.9253 (1.9683)  loss_mask: 0.1599 (0.1680)  time: 0.1645  data: 0.0007  max mem: 4132
[17:49:40.297986] Epoch: [8]  [560/781]  eta: 0:00:36  lr: 0.000249  training_loss: 2.0509 (2.1333)  classification_loss: 1.9229 (1.9665)  loss_mask: 0.1335 (0.1668)  time: 0.1629  data: 0.0003  max mem: 4132
[17:49:43.567915] Epoch: [8]  [580/781]  eta: 0:00:33  lr: 0.000249  training_loss: 2.1354 (2.1321)  classification_loss: 1.9656 (1.9658)  loss_mask: 0.1354 (0.1663)  time: 0.1634  data: 0.0003  max mem: 4132
[17:49:46.867048] Epoch: [8]  [600/781]  eta: 0:00:30  lr: 0.000249  training_loss: 2.0921 (2.1309)  classification_loss: 1.9616 (1.9660)  loss_mask: 0.1242 (0.1650)  time: 0.1648  data: 0.0003  max mem: 4132
[17:49:50.191417] Epoch: [8]  [620/781]  eta: 0:00:26  lr: 0.000249  training_loss: 2.0678 (2.1293)  classification_loss: 1.9270 (1.9651)  loss_mask: 0.1190 (0.1643)  time: 0.1661  data: 0.0004  max mem: 4132
[17:49:53.489611] Epoch: [8]  [640/781]  eta: 0:00:23  lr: 0.000249  training_loss: 2.0465 (2.1267)  classification_loss: 1.9392 (1.9642)  loss_mask: 0.1072 (0.1625)  time: 0.1648  data: 0.0003  max mem: 4132
[17:49:56.792915] Epoch: [8]  [660/781]  eta: 0:00:20  lr: 0.000249  training_loss: 2.1076 (2.1268)  classification_loss: 1.9455 (1.9640)  loss_mask: 0.1518 (0.1629)  time: 0.1650  data: 0.0003  max mem: 4132
[17:50:00.062689] Epoch: [8]  [680/781]  eta: 0:00:16  lr: 0.000249  training_loss: 2.0666 (2.1250)  classification_loss: 1.9272 (1.9629)  loss_mask: 0.1091 (0.1621)  time: 0.1634  data: 0.0002  max mem: 4132
[17:50:03.419499] Epoch: [8]  [700/781]  eta: 0:00:13  lr: 0.000249  training_loss: 2.0719 (2.1239)  classification_loss: 1.9662 (1.9625)  loss_mask: 0.1291 (0.1613)  time: 0.1678  data: 0.0003  max mem: 4132
[17:50:06.687114] Epoch: [8]  [720/781]  eta: 0:00:10  lr: 0.000249  training_loss: 2.1353 (2.1241)  classification_loss: 1.9674 (1.9627)  loss_mask: 0.1306 (0.1615)  time: 0.1633  data: 0.0002  max mem: 4132
[17:50:09.988904] Epoch: [8]  [740/781]  eta: 0:00:06  lr: 0.000249  training_loss: 2.0875 (2.1227)  classification_loss: 1.9393 (1.9619)  loss_mask: 0.1363 (0.1608)  time: 0.1650  data: 0.0003  max mem: 4132
[17:50:13.300831] Epoch: [8]  [760/781]  eta: 0:00:03  lr: 0.000249  training_loss: 2.0958 (2.1227)  classification_loss: 1.9346 (1.9616)  loss_mask: 0.1712 (0.1611)  time: 0.1655  data: 0.0004  max mem: 4132
[17:50:16.573527] Epoch: [8]  [780/781]  eta: 0:00:00  lr: 0.000249  training_loss: 2.1343 (2.1228)  classification_loss: 1.9254 (1.9608)  loss_mask: 0.1493 (0.1621)  time: 0.1635  data: 0.0002  max mem: 4132
[17:50:16.736076] Epoch: [8] Total time: 0:02:09 (0.1660 s / it)
[17:50:16.736608] Averaged stats: lr: 0.000249  training_loss: 2.1343 (2.1228)  classification_loss: 1.9254 (1.9608)  loss_mask: 0.1493 (0.1621)
[17:50:17.474283] Test:  [  0/157]  eta: 0:01:55  testing_loss: 1.3405 (1.3405)  acc1: 59.3750 (59.3750)  acc5: 92.1875 (92.1875)  time: 0.7336  data: 0.7010  max mem: 4132
[17:50:17.771376] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.3818 (1.4229)  acc1: 51.5625 (51.7045)  acc5: 93.7500 (93.3239)  time: 0.0935  data: 0.0640  max mem: 4132
[17:50:18.066085] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.3818 (1.4041)  acc1: 54.6875 (53.7946)  acc5: 93.7500 (93.3036)  time: 0.0294  data: 0.0003  max mem: 4132
[17:50:18.361226] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.3914 (1.4065)  acc1: 54.6875 (54.0323)  acc5: 93.7500 (93.2460)  time: 0.0293  data: 0.0003  max mem: 4132
[17:50:18.650895] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.4064 (1.4196)  acc1: 51.5625 (52.8582)  acc5: 92.1875 (92.4924)  time: 0.0291  data: 0.0003  max mem: 4132
[17:50:18.937095] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.3951 (1.4154)  acc1: 51.5625 (52.9105)  acc5: 90.6250 (92.6777)  time: 0.0286  data: 0.0002  max mem: 4132
[17:50:19.223738] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.3871 (1.4078)  acc1: 53.1250 (53.0225)  acc5: 92.1875 (92.6998)  time: 0.0285  data: 0.0002  max mem: 4132
[17:50:19.510290] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.3637 (1.4040)  acc1: 54.6875 (53.1470)  acc5: 93.7500 (93.0018)  time: 0.0285  data: 0.0002  max mem: 4132
[17:50:19.807512] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.3743 (1.4037)  acc1: 53.1250 (53.1250)  acc5: 93.7500 (92.9977)  time: 0.0291  data: 0.0002  max mem: 4132
[17:50:20.098317] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.4061 (1.4064)  acc1: 53.1250 (53.0563)  acc5: 92.1875 (92.9773)  time: 0.0293  data: 0.0002  max mem: 4132
[17:50:20.387624] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.4290 (1.4109)  acc1: 51.5625 (52.7382)  acc5: 92.1875 (92.8991)  time: 0.0288  data: 0.0002  max mem: 4132
[17:50:20.674617] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.4740 (1.4150)  acc1: 48.4375 (52.3649)  acc5: 93.7500 (93.0884)  time: 0.0287  data: 0.0002  max mem: 4132
[17:50:20.961486] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.4147 (1.4117)  acc1: 50.0000 (52.5439)  acc5: 93.7500 (93.2076)  time: 0.0286  data: 0.0002  max mem: 4132
[17:50:21.255988] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.3887 (1.4140)  acc1: 51.5625 (52.3736)  acc5: 93.7500 (93.1536)  time: 0.0289  data: 0.0002  max mem: 4132
[17:50:21.543304] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.4026 (1.4146)  acc1: 51.5625 (52.4601)  acc5: 92.1875 (93.0629)  time: 0.0289  data: 0.0002  max mem: 4132
[17:50:21.827312] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.4019 (1.4122)  acc1: 53.1250 (52.6490)  acc5: 92.1875 (93.0153)  time: 0.0284  data: 0.0002  max mem: 4132
[17:50:21.982259] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.4037 (1.4136)  acc1: 53.1250 (52.5700)  acc5: 92.1875 (92.9800)  time: 0.0274  data: 0.0002  max mem: 4132
[17:50:22.169440] Test: Total time: 0:00:05 (0.0346 s / it)
[17:50:22.169918] * Acc@1 52.570 Acc@5 92.980 loss 1.414
[17:50:22.170268] Accuracy of the network on the 10000 test images: 52.6%
[17:50:22.170496] Max accuracy: 52.57%
[17:50:22.424695] log_dir: ./output_dir
[17:50:23.325496] Epoch: [9]  [  0/781]  eta: 0:11:40  lr: 0.000249  training_loss: 2.0924 (2.0924)  classification_loss: 1.9547 (1.9547)  loss_mask: 0.1377 (0.1377)  time: 0.8975  data: 0.6898  max mem: 4132
[17:50:26.612910] Epoch: [9]  [ 20/781]  eta: 0:02:31  lr: 0.000249  training_loss: 2.0783 (2.0834)  classification_loss: 1.9236 (1.9512)  loss_mask: 0.1182 (0.1322)  time: 0.1643  data: 0.0002  max mem: 4132
[17:50:29.892314] Epoch: [9]  [ 40/781]  eta: 0:02:14  lr: 0.000249  training_loss: 2.0321 (2.0599)  classification_loss: 1.9146 (1.9346)  loss_mask: 0.1063 (0.1253)  time: 0.1639  data: 0.0003  max mem: 4132
[17:50:33.216409] Epoch: [9]  [ 60/781]  eta: 0:02:07  lr: 0.000249  training_loss: 2.0904 (2.0760)  classification_loss: 1.9567 (1.9437)  loss_mask: 0.1419 (0.1324)  time: 0.1661  data: 0.0003  max mem: 4132
[17:50:36.570099] Epoch: [9]  [ 80/781]  eta: 0:02:02  lr: 0.000249  training_loss: 2.0347 (2.0814)  classification_loss: 1.9413 (1.9447)  loss_mask: 0.1236 (0.1367)  time: 0.1676  data: 0.0004  max mem: 4132
[17:50:39.892869] Epoch: [9]  [100/781]  eta: 0:01:57  lr: 0.000249  training_loss: 2.0495 (2.0782)  classification_loss: 1.9069 (1.9388)  loss_mask: 0.1295 (0.1394)  time: 0.1661  data: 0.0003  max mem: 4132
[17:50:43.175469] Epoch: [9]  [120/781]  eta: 0:01:53  lr: 0.000249  training_loss: 2.0244 (2.0698)  classification_loss: 1.9097 (1.9360)  loss_mask: 0.0920 (0.1338)  time: 0.1640  data: 0.0002  max mem: 4132
[17:50:46.490559] Epoch: [9]  [140/781]  eta: 0:01:49  lr: 0.000249  training_loss: 1.9882 (2.0591)  classification_loss: 1.8862 (1.9311)  loss_mask: 0.0763 (0.1280)  time: 0.1656  data: 0.0002  max mem: 4132
[17:50:49.840728] Epoch: [9]  [160/781]  eta: 0:01:45  lr: 0.000249  training_loss: 2.0337 (2.0554)  classification_loss: 1.9046 (1.9298)  loss_mask: 0.0932 (0.1256)  time: 0.1674  data: 0.0003  max mem: 4132
[17:50:53.190502] Epoch: [9]  [180/781]  eta: 0:01:42  lr: 0.000249  training_loss: 2.0229 (2.0530)  classification_loss: 1.9101 (1.9287)  loss_mask: 0.1002 (0.1243)  time: 0.1674  data: 0.0003  max mem: 4132
[17:50:56.490696] Epoch: [9]  [200/781]  eta: 0:01:38  lr: 0.000249  training_loss: 2.0990 (2.0571)  classification_loss: 1.9155 (1.9299)  loss_mask: 0.1181 (0.1272)  time: 0.1649  data: 0.0003  max mem: 4132
[17:50:59.821016] Epoch: [9]  [220/781]  eta: 0:01:34  lr: 0.000249  training_loss: 2.0455 (2.0557)  classification_loss: 1.9365 (1.9300)  loss_mask: 0.0919 (0.1257)  time: 0.1664  data: 0.0003  max mem: 4132
[17:51:03.123357] Epoch: [9]  [240/781]  eta: 0:01:31  lr: 0.000249  training_loss: 2.0302 (2.0556)  classification_loss: 1.8989 (1.9291)  loss_mask: 0.1149 (0.1265)  time: 0.1650  data: 0.0003  max mem: 4132
[17:51:06.425572] Epoch: [9]  [260/781]  eta: 0:01:27  lr: 0.000249  training_loss: 2.0396 (2.0551)  classification_loss: 1.9140 (1.9273)  loss_mask: 0.0973 (0.1277)  time: 0.1650  data: 0.0003  max mem: 4132
[17:51:09.742589] Epoch: [9]  [280/781]  eta: 0:01:24  lr: 0.000249  training_loss: 2.1386 (2.0636)  classification_loss: 1.9761 (1.9304)  loss_mask: 0.1815 (0.1332)  time: 0.1658  data: 0.0003  max mem: 4132
[17:51:13.008071] Epoch: [9]  [300/781]  eta: 0:01:20  lr: 0.000249  training_loss: 2.0321 (2.0615)  classification_loss: 1.8942 (1.9295)  loss_mask: 0.1208 (0.1319)  time: 0.1632  data: 0.0004  max mem: 4132
[17:51:16.286489] Epoch: [9]  [320/781]  eta: 0:01:17  lr: 0.000249  training_loss: 2.0271 (2.0600)  classification_loss: 1.8998 (1.9291)  loss_mask: 0.1088 (0.1309)  time: 0.1638  data: 0.0002  max mem: 4132
[17:51:19.589104] Epoch: [9]  [340/781]  eta: 0:01:13  lr: 0.000249  training_loss: 2.0073 (2.0585)  classification_loss: 1.9231 (1.9289)  loss_mask: 0.1035 (0.1296)  time: 0.1650  data: 0.0002  max mem: 4132
[17:51:22.870728] Epoch: [9]  [360/781]  eta: 0:01:10  lr: 0.000249  training_loss: 1.9847 (2.0561)  classification_loss: 1.9214 (1.9285)  loss_mask: 0.0890 (0.1276)  time: 0.1640  data: 0.0003  max mem: 4132
[17:51:26.174718] Epoch: [9]  [380/781]  eta: 0:01:07  lr: 0.000249  training_loss: 2.0762 (2.0587)  classification_loss: 1.8948 (1.9276)  loss_mask: 0.1717 (0.1311)  time: 0.1651  data: 0.0004  max mem: 4132
[17:51:29.462095] Epoch: [9]  [400/781]  eta: 0:01:03  lr: 0.000249  training_loss: 2.0423 (2.0590)  classification_loss: 1.8823 (1.9264)  loss_mask: 0.1430 (0.1326)  time: 0.1643  data: 0.0003  max mem: 4132
[17:51:32.736162] Epoch: [9]  [420/781]  eta: 0:01:00  lr: 0.000249  training_loss: 1.9783 (2.0562)  classification_loss: 1.8687 (1.9243)  loss_mask: 0.1104 (0.1319)  time: 0.1636  data: 0.0002  max mem: 4132
[17:51:36.053182] Epoch: [9]  [440/781]  eta: 0:00:56  lr: 0.000249  training_loss: 1.9726 (2.0532)  classification_loss: 1.8794 (1.9224)  loss_mask: 0.0967 (0.1308)  time: 0.1658  data: 0.0002  max mem: 4132
[17:51:39.341151] Epoch: [9]  [460/781]  eta: 0:00:53  lr: 0.000249  training_loss: 2.0352 (2.0531)  classification_loss: 1.9210 (1.9223)  loss_mask: 0.1067 (0.1308)  time: 0.1643  data: 0.0003  max mem: 4132
[17:51:42.605919] Epoch: [9]  [480/781]  eta: 0:00:50  lr: 0.000249  training_loss: 2.1373 (2.0561)  classification_loss: 1.8844 (1.9223)  loss_mask: 0.1665 (0.1338)  time: 0.1632  data: 0.0002  max mem: 4132
[17:51:45.903799] Epoch: [9]  [500/781]  eta: 0:00:46  lr: 0.000249  training_loss: 2.0437 (2.0573)  classification_loss: 1.9221 (1.9223)  loss_mask: 0.1440 (0.1350)  time: 0.1648  data: 0.0003  max mem: 4132
[17:51:49.217052] Epoch: [9]  [520/781]  eta: 0:00:43  lr: 0.000249  training_loss: 2.0390 (2.0577)  classification_loss: 1.9099 (1.9224)  loss_mask: 0.1264 (0.1352)  time: 0.1656  data: 0.0003  max mem: 4132
[17:51:52.482621] Epoch: [9]  [540/781]  eta: 0:00:40  lr: 0.000249  training_loss: 2.0964 (2.0591)  classification_loss: 1.9396 (1.9229)  loss_mask: 0.1387 (0.1362)  time: 0.1632  data: 0.0002  max mem: 4132
[17:51:55.814957] Epoch: [9]  [560/781]  eta: 0:00:36  lr: 0.000248  training_loss: 1.9953 (2.0574)  classification_loss: 1.8836 (1.9219)  loss_mask: 0.1141 (0.1355)  time: 0.1665  data: 0.0003  max mem: 4132
[17:51:59.075212] Epoch: [9]  [580/781]  eta: 0:00:33  lr: 0.000248  training_loss: 2.0096 (2.0565)  classification_loss: 1.8980 (1.9215)  loss_mask: 0.1188 (0.1350)  time: 0.1629  data: 0.0002  max mem: 4132
[17:52:02.372802] Epoch: [9]  [600/781]  eta: 0:00:30  lr: 0.000248  training_loss: 1.9908 (2.0546)  classification_loss: 1.8965 (1.9199)  loss_mask: 0.1158 (0.1347)  time: 0.1648  data: 0.0003  max mem: 4132
[17:52:05.660932] Epoch: [9]  [620/781]  eta: 0:00:26  lr: 0.000248  training_loss: 2.0227 (2.0541)  classification_loss: 1.8636 (1.9183)  loss_mask: 0.1271 (0.1358)  time: 0.1643  data: 0.0003  max mem: 4132
[17:52:08.947926] Epoch: [9]  [640/781]  eta: 0:00:23  lr: 0.000248  training_loss: 1.9666 (2.0520)  classification_loss: 1.8747 (1.9175)  loss_mask: 0.0922 (0.1346)  time: 0.1643  data: 0.0003  max mem: 4132
[17:52:12.242427] Epoch: [9]  [660/781]  eta: 0:00:20  lr: 0.000248  training_loss: 2.0301 (2.0524)  classification_loss: 1.8989 (1.9171)  loss_mask: 0.1413 (0.1353)  time: 0.1646  data: 0.0002  max mem: 4132
[17:52:15.524765] Epoch: [9]  [680/781]  eta: 0:00:16  lr: 0.000248  training_loss: 2.0127 (2.0523)  classification_loss: 1.8789 (1.9165)  loss_mask: 0.1351 (0.1358)  time: 0.1640  data: 0.0002  max mem: 4132
[17:52:18.808727] Epoch: [9]  [700/781]  eta: 0:00:13  lr: 0.000248  training_loss: 1.9649 (2.0508)  classification_loss: 1.8619 (1.9153)  loss_mask: 0.1141 (0.1355)  time: 0.1641  data: 0.0003  max mem: 4132
[17:52:22.125681] Epoch: [9]  [720/781]  eta: 0:00:10  lr: 0.000248  training_loss: 2.1367 (2.0526)  classification_loss: 1.9587 (1.9157)  loss_mask: 0.1757 (0.1369)  time: 0.1658  data: 0.0003  max mem: 4132
[17:52:25.428586] Epoch: [9]  [740/781]  eta: 0:00:06  lr: 0.000248  training_loss: 2.0874 (2.0532)  classification_loss: 1.9580 (1.9164)  loss_mask: 0.1250 (0.1368)  time: 0.1650  data: 0.0002  max mem: 4132
[17:52:28.749939] Epoch: [9]  [760/781]  eta: 0:00:03  lr: 0.000248  training_loss: 2.1109 (2.0547)  classification_loss: 1.9595 (1.9173)  loss_mask: 0.1365 (0.1374)  time: 0.1660  data: 0.0003  max mem: 4132
[17:52:32.050874] Epoch: [9]  [780/781]  eta: 0:00:00  lr: 0.000248  training_loss: 2.0834 (2.0564)  classification_loss: 1.9168 (1.9177)  loss_mask: 0.1219 (0.1387)  time: 0.1650  data: 0.0002  max mem: 4132
[17:52:32.246694] Epoch: [9] Total time: 0:02:09 (0.1662 s / it)
[17:52:32.247254] Averaged stats: lr: 0.000248  training_loss: 2.0834 (2.0564)  classification_loss: 1.9168 (1.9177)  loss_mask: 0.1219 (0.1387)
[17:52:32.975371] Test:  [  0/157]  eta: 0:01:53  testing_loss: 1.3081 (1.3081)  acc1: 53.1250 (53.1250)  acc5: 92.1875 (92.1875)  time: 0.7233  data: 0.6935  max mem: 4132
[17:52:33.272170] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.3739 (1.4111)  acc1: 50.0000 (50.0000)  acc5: 95.3125 (93.7500)  time: 0.0926  data: 0.0633  max mem: 4132
[17:52:33.559185] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.3547 (1.3812)  acc1: 51.5625 (52.9018)  acc5: 95.3125 (94.7173)  time: 0.0290  data: 0.0003  max mem: 4132
[17:52:33.846479] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.3581 (1.3853)  acc1: 56.2500 (53.7802)  acc5: 95.3125 (94.6069)  time: 0.0286  data: 0.0002  max mem: 4132
[17:52:34.140277] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.3939 (1.3938)  acc1: 54.6875 (53.6204)  acc5: 93.7500 (94.1692)  time: 0.0289  data: 0.0004  max mem: 4132
[17:52:34.433056] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.3893 (1.3897)  acc1: 54.6875 (54.1973)  acc5: 92.1875 (94.2096)  time: 0.0292  data: 0.0004  max mem: 4132
[17:52:34.720332] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.3658 (1.3849)  acc1: 57.8125 (54.5594)  acc5: 93.7500 (94.2111)  time: 0.0289  data: 0.0003  max mem: 4132
[17:52:35.007600] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.3603 (1.3838)  acc1: 56.2500 (54.5775)  acc5: 93.7500 (94.4322)  time: 0.0286  data: 0.0002  max mem: 4132
[17:52:35.294320] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.3674 (1.3843)  acc1: 54.6875 (54.6103)  acc5: 95.3125 (94.4830)  time: 0.0286  data: 0.0002  max mem: 4132
[17:52:35.582873] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.3740 (1.3848)  acc1: 54.6875 (54.6875)  acc5: 95.3125 (94.4712)  time: 0.0286  data: 0.0002  max mem: 4132
[17:52:35.872273] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.4171 (1.3894)  acc1: 53.1250 (54.4709)  acc5: 93.7500 (94.3843)  time: 0.0288  data: 0.0003  max mem: 4132
[17:52:36.161876] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.4334 (1.3927)  acc1: 48.4375 (53.9133)  acc5: 95.3125 (94.4961)  time: 0.0288  data: 0.0003  max mem: 4132
[17:52:36.451329] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.4243 (1.3897)  acc1: 50.0000 (53.9773)  acc5: 95.3125 (94.6539)  time: 0.0288  data: 0.0003  max mem: 4132
[17:52:36.742434] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.3905 (1.3921)  acc1: 53.1250 (53.9838)  acc5: 95.3125 (94.5611)  time: 0.0289  data: 0.0002  max mem: 4132
[17:52:37.031179] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.3842 (1.3918)  acc1: 54.6875 (54.1999)  acc5: 92.1875 (94.4814)  time: 0.0288  data: 0.0002  max mem: 4132
[17:52:37.316887] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.3747 (1.3905)  acc1: 54.6875 (54.2115)  acc5: 92.1875 (94.4329)  time: 0.0286  data: 0.0002  max mem: 4132
[17:52:37.471603] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.3842 (1.3919)  acc1: 53.1250 (54.1200)  acc5: 93.7500 (94.4600)  time: 0.0275  data: 0.0002  max mem: 4132
[17:52:37.650061] Test: Total time: 0:00:05 (0.0344 s / it)
[17:52:37.650675] * Acc@1 54.120 Acc@5 94.460 loss 1.392
[17:52:37.651013] Accuracy of the network on the 10000 test images: 54.1%
[17:52:37.651212] Max accuracy: 54.12%
[17:52:37.983955] log_dir: ./output_dir
[17:52:38.941167] Epoch: [10]  [  0/781]  eta: 0:12:25  lr: 0.000248  training_loss: 2.0505 (2.0505)  classification_loss: 1.7789 (1.7789)  loss_mask: 0.2716 (0.2716)  time: 0.9550  data: 0.7577  max mem: 4132
[17:52:42.285984] Epoch: [10]  [ 20/781]  eta: 0:02:35  lr: 0.000248  training_loss: 2.0258 (2.0384)  classification_loss: 1.8834 (1.8978)  loss_mask: 0.1266 (0.1405)  time: 0.1671  data: 0.0003  max mem: 4132
[17:52:45.631826] Epoch: [10]  [ 40/781]  eta: 0:02:18  lr: 0.000248  training_loss: 1.9815 (2.0168)  classification_loss: 1.9037 (1.9014)  loss_mask: 0.0817 (0.1154)  time: 0.1672  data: 0.0003  max mem: 4132
[17:52:48.920245] Epoch: [10]  [ 60/781]  eta: 0:02:09  lr: 0.000248  training_loss: 2.1002 (2.0432)  classification_loss: 1.9468 (1.9232)  loss_mask: 0.1131 (0.1199)  time: 0.1643  data: 0.0002  max mem: 4132
[17:52:52.271254] Epoch: [10]  [ 80/781]  eta: 0:02:03  lr: 0.000248  training_loss: 2.0097 (2.0441)  classification_loss: 1.9213 (1.9260)  loss_mask: 0.0927 (0.1182)  time: 0.1675  data: 0.0004  max mem: 4132
[17:52:55.597590] Epoch: [10]  [100/781]  eta: 0:01:58  lr: 0.000248  training_loss: 2.0062 (2.0388)  classification_loss: 1.8973 (1.9244)  loss_mask: 0.0966 (0.1144)  time: 0.1662  data: 0.0003  max mem: 4132
[17:52:58.909600] Epoch: [10]  [120/781]  eta: 0:01:54  lr: 0.000248  training_loss: 1.9798 (2.0292)  classification_loss: 1.8830 (1.9198)  loss_mask: 0.0738 (0.1095)  time: 0.1655  data: 0.0003  max mem: 4132
[17:53:02.179836] Epoch: [10]  [140/781]  eta: 0:01:49  lr: 0.000248  training_loss: 1.9433 (2.0161)  classification_loss: 1.8613 (1.9087)  loss_mask: 0.0795 (0.1074)  time: 0.1634  data: 0.0002  max mem: 4132
[17:53:05.451878] Epoch: [10]  [160/781]  eta: 0:01:45  lr: 0.000248  training_loss: 2.0027 (2.0156)  classification_loss: 1.8856 (1.9078)  loss_mask: 0.1014 (0.1078)  time: 0.1635  data: 0.0002  max mem: 4132
[17:53:08.744143] Epoch: [10]  [180/781]  eta: 0:01:42  lr: 0.000248  training_loss: 2.0039 (2.0160)  classification_loss: 1.8931 (1.9081)  loss_mask: 0.0908 (0.1079)  time: 0.1645  data: 0.0002  max mem: 4132
[17:53:12.040579] Epoch: [10]  [200/781]  eta: 0:01:38  lr: 0.000248  training_loss: 2.0396 (2.0186)  classification_loss: 1.9132 (1.9086)  loss_mask: 0.1082 (0.1099)  time: 0.1647  data: 0.0003  max mem: 4132
[17:53:15.338743] Epoch: [10]  [220/781]  eta: 0:01:34  lr: 0.000248  training_loss: 2.0553 (2.0240)  classification_loss: 1.8992 (1.9077)  loss_mask: 0.1366 (0.1164)  time: 0.1648  data: 0.0003  max mem: 4132
[17:53:18.637137] Epoch: [10]  [240/781]  eta: 0:01:31  lr: 0.000248  training_loss: 1.9906 (2.0250)  classification_loss: 1.8915 (1.9079)  loss_mask: 0.1043 (0.1171)  time: 0.1648  data: 0.0004  max mem: 4132
[17:53:21.918178] Epoch: [10]  [260/781]  eta: 0:01:27  lr: 0.000248  training_loss: 1.9887 (2.0218)  classification_loss: 1.8728 (1.9058)  loss_mask: 0.0968 (0.1160)  time: 0.1640  data: 0.0003  max mem: 4132
[17:53:25.195859] Epoch: [10]  [280/781]  eta: 0:01:24  lr: 0.000248  training_loss: 1.9605 (2.0169)  classification_loss: 1.8955 (1.9040)  loss_mask: 0.0699 (0.1129)  time: 0.1638  data: 0.0003  max mem: 4132
[17:53:28.509465] Epoch: [10]  [300/781]  eta: 0:01:20  lr: 0.000248  training_loss: 1.9431 (2.0135)  classification_loss: 1.8861 (1.9029)  loss_mask: 0.0676 (0.1106)  time: 0.1656  data: 0.0003  max mem: 4132
[17:53:31.796866] Epoch: [10]  [320/781]  eta: 0:01:17  lr: 0.000248  training_loss: 1.9637 (2.0108)  classification_loss: 1.8566 (1.9005)  loss_mask: 0.0989 (0.1103)  time: 0.1643  data: 0.0002  max mem: 4132
[17:53:35.118242] Epoch: [10]  [340/781]  eta: 0:01:13  lr: 0.000248  training_loss: 2.0268 (2.0122)  classification_loss: 1.8869 (1.8998)  loss_mask: 0.1153 (0.1124)  time: 0.1660  data: 0.0002  max mem: 4132
[17:53:38.402293] Epoch: [10]  [360/781]  eta: 0:01:10  lr: 0.000248  training_loss: 1.9968 (2.0119)  classification_loss: 1.9054 (1.9009)  loss_mask: 0.0884 (0.1111)  time: 0.1641  data: 0.0002  max mem: 4132
[17:53:41.704627] Epoch: [10]  [380/781]  eta: 0:01:07  lr: 0.000248  training_loss: 1.9523 (2.0091)  classification_loss: 1.8813 (1.8997)  loss_mask: 0.0712 (0.1094)  time: 0.1650  data: 0.0002  max mem: 4132
[17:53:45.023692] Epoch: [10]  [400/781]  eta: 0:01:03  lr: 0.000248  training_loss: 2.0155 (2.0095)  classification_loss: 1.8965 (1.8990)  loss_mask: 0.0737 (0.1105)  time: 0.1659  data: 0.0003  max mem: 4132
[17:53:48.320510] Epoch: [10]  [420/781]  eta: 0:01:00  lr: 0.000248  training_loss: 1.9904 (2.0081)  classification_loss: 1.8651 (1.8969)  loss_mask: 0.1014 (0.1113)  time: 0.1648  data: 0.0003  max mem: 4132
[17:53:51.621933] Epoch: [10]  [440/781]  eta: 0:00:56  lr: 0.000248  training_loss: 2.0237 (2.0083)  classification_loss: 1.8779 (1.8968)  loss_mask: 0.1061 (0.1114)  time: 0.1650  data: 0.0002  max mem: 4132
[17:53:54.877119] Epoch: [10]  [460/781]  eta: 0:00:53  lr: 0.000248  training_loss: 1.9043 (2.0059)  classification_loss: 1.8325 (1.8949)  loss_mask: 0.0886 (0.1110)  time: 0.1627  data: 0.0002  max mem: 4132
[17:53:58.152613] Epoch: [10]  [480/781]  eta: 0:00:50  lr: 0.000248  training_loss: 1.9722 (2.0055)  classification_loss: 1.8869 (1.8949)  loss_mask: 0.0927 (0.1106)  time: 0.1637  data: 0.0003  max mem: 4132
[17:54:01.476139] Epoch: [10]  [500/781]  eta: 0:00:46  lr: 0.000248  training_loss: 1.9574 (2.0032)  classification_loss: 1.8766 (1.8938)  loss_mask: 0.0720 (0.1094)  time: 0.1661  data: 0.0004  max mem: 4132
[17:54:04.786988] Epoch: [10]  [520/781]  eta: 0:00:43  lr: 0.000248  training_loss: 1.9278 (2.0018)  classification_loss: 1.8454 (1.8921)  loss_mask: 0.0845 (0.1097)  time: 0.1655  data: 0.0003  max mem: 4132
[17:54:08.099582] Epoch: [10]  [540/781]  eta: 0:00:40  lr: 0.000248  training_loss: 2.0752 (2.0047)  classification_loss: 1.8953 (1.8928)  loss_mask: 0.1820 (0.1119)  time: 0.1655  data: 0.0003  max mem: 4132
[17:54:11.407377] Epoch: [10]  [560/781]  eta: 0:00:36  lr: 0.000248  training_loss: 1.9692 (2.0041)  classification_loss: 1.8868 (1.8928)  loss_mask: 0.0839 (0.1113)  time: 0.1653  data: 0.0002  max mem: 4132
[17:54:14.701345] Epoch: [10]  [580/781]  eta: 0:00:33  lr: 0.000248  training_loss: 1.9398 (2.0017)  classification_loss: 1.8546 (1.8915)  loss_mask: 0.0810 (0.1102)  time: 0.1646  data: 0.0003  max mem: 4132
[17:54:18.026641] Epoch: [10]  [600/781]  eta: 0:00:30  lr: 0.000248  training_loss: 1.9391 (2.0000)  classification_loss: 1.8513 (1.8909)  loss_mask: 0.0776 (0.1091)  time: 0.1662  data: 0.0004  max mem: 4132
[17:54:21.340108] Epoch: [10]  [620/781]  eta: 0:00:26  lr: 0.000248  training_loss: 2.0298 (2.0016)  classification_loss: 1.8522 (1.8895)  loss_mask: 0.1638 (0.1120)  time: 0.1656  data: 0.0002  max mem: 4132
[17:54:24.646537] Epoch: [10]  [640/781]  eta: 0:00:23  lr: 0.000248  training_loss: 1.9500 (2.0002)  classification_loss: 1.8384 (1.8884)  loss_mask: 0.0823 (0.1118)  time: 0.1652  data: 0.0003  max mem: 4132
[17:54:27.919504] Epoch: [10]  [660/781]  eta: 0:00:20  lr: 0.000248  training_loss: 1.9547 (1.9999)  classification_loss: 1.8888 (1.8885)  loss_mask: 0.0983 (0.1113)  time: 0.1633  data: 0.0003  max mem: 4132
[17:54:31.219565] Epoch: [10]  [680/781]  eta: 0:00:16  lr: 0.000248  training_loss: 1.9302 (1.9979)  classification_loss: 1.8645 (1.8874)  loss_mask: 0.0679 (0.1105)  time: 0.1649  data: 0.0002  max mem: 4132
[17:54:34.532414] Epoch: [10]  [700/781]  eta: 0:00:13  lr: 0.000248  training_loss: 1.8971 (1.9954)  classification_loss: 1.8408 (1.8858)  loss_mask: 0.0766 (0.1096)  time: 0.1656  data: 0.0003  max mem: 4132
[17:54:37.826319] Epoch: [10]  [720/781]  eta: 0:00:10  lr: 0.000248  training_loss: 2.0411 (1.9966)  classification_loss: 1.8580 (1.8853)  loss_mask: 0.1833 (0.1113)  time: 0.1646  data: 0.0003  max mem: 4132
[17:54:41.127972] Epoch: [10]  [740/781]  eta: 0:00:06  lr: 0.000248  training_loss: 2.0110 (1.9968)  classification_loss: 1.8502 (1.8846)  loss_mask: 0.1371 (0.1123)  time: 0.1649  data: 0.0003  max mem: 4132
[17:54:44.427632] Epoch: [10]  [760/781]  eta: 0:00:03  lr: 0.000248  training_loss: 2.0517 (1.9982)  classification_loss: 1.8445 (1.8841)  loss_mask: 0.1325 (0.1141)  time: 0.1649  data: 0.0003  max mem: 4132
[17:54:47.685728] Epoch: [10]  [780/781]  eta: 0:00:00  lr: 0.000248  training_loss: 1.9358 (1.9971)  classification_loss: 1.8472 (1.8836)  loss_mask: 0.0827 (0.1135)  time: 0.1628  data: 0.0002  max mem: 4132
[17:54:47.858197] Epoch: [10] Total time: 0:02:09 (0.1663 s / it)
[17:54:47.858674] Averaged stats: lr: 0.000248  training_loss: 1.9358 (1.9971)  classification_loss: 1.8472 (1.8836)  loss_mask: 0.0827 (0.1135)
[17:54:51.444178] Test:  [  0/157]  eta: 0:01:55  testing_loss: 1.2856 (1.2856)  acc1: 51.5625 (51.5625)  acc5: 92.1875 (92.1875)  time: 0.7358  data: 0.6945  max mem: 4132
[17:54:51.745798] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.3437 (1.3584)  acc1: 51.5625 (50.4261)  acc5: 95.3125 (94.3182)  time: 0.0941  data: 0.0633  max mem: 4132
[17:54:52.043123] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.3088 (1.3185)  acc1: 54.6875 (53.6458)  acc5: 95.3125 (94.4940)  time: 0.0297  data: 0.0002  max mem: 4132
[17:54:52.331445] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.3088 (1.3219)  acc1: 56.2500 (53.7298)  acc5: 93.7500 (94.3548)  time: 0.0291  data: 0.0002  max mem: 4132
[17:54:52.624872] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.3371 (1.3265)  acc1: 56.2500 (54.2302)  acc5: 93.7500 (94.4360)  time: 0.0289  data: 0.0002  max mem: 4132
[17:54:52.916933] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.2922 (1.3236)  acc1: 56.2500 (54.7794)  acc5: 93.7500 (94.4853)  time: 0.0291  data: 0.0003  max mem: 4132
[17:54:53.207024] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.2907 (1.3176)  acc1: 56.2500 (55.1486)  acc5: 93.7500 (94.4928)  time: 0.0290  data: 0.0003  max mem: 4132
[17:54:53.496498] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.2908 (1.3135)  acc1: 57.8125 (55.4798)  acc5: 95.3125 (94.6083)  time: 0.0288  data: 0.0002  max mem: 4132
[17:54:53.788395] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.2986 (1.3149)  acc1: 56.2500 (55.3627)  acc5: 95.3125 (94.5023)  time: 0.0289  data: 0.0002  max mem: 4132
[17:54:54.079969] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.3351 (1.3178)  acc1: 51.5625 (55.1683)  acc5: 93.7500 (94.5742)  time: 0.0289  data: 0.0002  max mem: 4132
[17:54:54.369376] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.3481 (1.3227)  acc1: 51.5625 (54.9814)  acc5: 93.7500 (94.4926)  time: 0.0288  data: 0.0002  max mem: 4132
[17:54:54.661233] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.3875 (1.3275)  acc1: 51.5625 (54.7720)  acc5: 93.7500 (94.5242)  time: 0.0289  data: 0.0002  max mem: 4132
[17:54:54.951626] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.3296 (1.3237)  acc1: 51.5625 (54.9329)  acc5: 95.3125 (94.6798)  time: 0.0289  data: 0.0003  max mem: 4132
[17:54:55.241374] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.3006 (1.3247)  acc1: 54.6875 (54.7948)  acc5: 95.3125 (94.6326)  time: 0.0288  data: 0.0002  max mem: 4132
[17:54:55.528412] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.3203 (1.3240)  acc1: 56.2500 (55.1197)  acc5: 93.7500 (94.6365)  time: 0.0286  data: 0.0002  max mem: 4132
[17:54:55.812288] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.2980 (1.3211)  acc1: 57.8125 (55.2463)  acc5: 95.3125 (94.6192)  time: 0.0283  data: 0.0002  max mem: 4132
[17:54:55.966653] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.2980 (1.3220)  acc1: 56.2500 (55.1500)  acc5: 95.3125 (94.6300)  time: 0.0274  data: 0.0002  max mem: 4132
[17:54:56.141234] Test: Total time: 0:00:05 (0.0346 s / it)
[17:54:56.142259] * Acc@1 55.150 Acc@5 94.630 loss 1.322
[17:54:56.142703] Accuracy of the network on the 10000 test images: 55.1%
[17:54:56.142931] Max accuracy: 55.15%
[17:54:56.365089] log_dir: ./output_dir
[17:54:57.364454] Epoch: [11]  [  0/781]  eta: 0:12:58  lr: 0.000248  training_loss: 1.8485 (1.8485)  classification_loss: 1.8111 (1.8111)  loss_mask: 0.0374 (0.0374)  time: 0.9972  data: 0.8092  max mem: 4132
[17:55:00.655073] Epoch: [11]  [ 20/781]  eta: 0:02:35  lr: 0.000248  training_loss: 1.9554 (1.9418)  classification_loss: 1.8671 (1.8475)  loss_mask: 0.0850 (0.0943)  time: 0.1644  data: 0.0002  max mem: 4132
[17:55:03.939744] Epoch: [11]  [ 40/781]  eta: 0:02:16  lr: 0.000248  training_loss: 2.0061 (1.9720)  classification_loss: 1.8933 (1.8643)  loss_mask: 0.1109 (0.1078)  time: 0.1641  data: 0.0002  max mem: 4132
[17:55:07.234412] Epoch: [11]  [ 60/781]  eta: 0:02:08  lr: 0.000247  training_loss: 1.9781 (1.9751)  classification_loss: 1.8601 (1.8656)  loss_mask: 0.0817 (0.1095)  time: 0.1647  data: 0.0003  max mem: 4132
[17:55:10.539033] Epoch: [11]  [ 80/781]  eta: 0:02:02  lr: 0.000247  training_loss: 1.9945 (1.9805)  classification_loss: 1.8421 (1.8647)  loss_mask: 0.1152 (0.1157)  time: 0.1651  data: 0.0003  max mem: 4132
[17:55:13.827030] Epoch: [11]  [100/781]  eta: 0:01:57  lr: 0.000247  training_loss: 2.0819 (2.0082)  classification_loss: 1.9295 (1.8813)  loss_mask: 0.1486 (0.1270)  time: 0.1643  data: 0.0003  max mem: 4132
[17:55:17.104483] Epoch: [11]  [120/781]  eta: 0:01:53  lr: 0.000247  training_loss: 2.1444 (2.0267)  classification_loss: 1.9401 (1.8918)  loss_mask: 0.1734 (0.1350)  time: 0.1638  data: 0.0003  max mem: 4132
[17:55:20.415399] Epoch: [11]  [140/781]  eta: 0:01:49  lr: 0.000247  training_loss: 2.0128 (2.0262)  classification_loss: 1.8663 (1.8907)  loss_mask: 0.1354 (0.1355)  time: 0.1655  data: 0.0004  max mem: 4132
[17:55:23.700943] Epoch: [11]  [160/781]  eta: 0:01:45  lr: 0.000247  training_loss: 2.0215 (2.0279)  classification_loss: 1.8758 (1.8934)  loss_mask: 0.1052 (0.1345)  time: 0.1638  data: 0.0003  max mem: 4132
[17:55:26.984298] Epoch: [11]  [180/781]  eta: 0:01:41  lr: 0.000247  training_loss: 1.9779 (2.0225)  classification_loss: 1.8683 (1.8903)  loss_mask: 0.0935 (0.1322)  time: 0.1641  data: 0.0002  max mem: 4132
[17:55:30.304940] Epoch: [11]  [200/781]  eta: 0:01:38  lr: 0.000247  training_loss: 1.9629 (2.0199)  classification_loss: 1.8485 (1.8870)  loss_mask: 0.0995 (0.1328)  time: 0.1659  data: 0.0005  max mem: 4132
[17:55:33.604869] Epoch: [11]  [220/781]  eta: 0:01:34  lr: 0.000247  training_loss: 1.9279 (2.0136)  classification_loss: 1.8493 (1.8840)  loss_mask: 0.0894 (0.1296)  time: 0.1649  data: 0.0002  max mem: 4132
[17:55:36.894991] Epoch: [11]  [240/781]  eta: 0:01:30  lr: 0.000247  training_loss: 1.9614 (2.0098)  classification_loss: 1.9015 (1.8842)  loss_mask: 0.0655 (0.1256)  time: 0.1644  data: 0.0002  max mem: 4132
[17:55:40.241262] Epoch: [11]  [260/781]  eta: 0:01:27  lr: 0.000247  training_loss: 1.8543 (2.0005)  classification_loss: 1.7745 (1.8774)  loss_mask: 0.0738 (0.1231)  time: 0.1672  data: 0.0006  max mem: 4132
[17:55:43.605843] Epoch: [11]  [280/781]  eta: 0:01:24  lr: 0.000247  training_loss: 1.8943 (1.9951)  classification_loss: 1.8406 (1.8759)  loss_mask: 0.0649 (0.1193)  time: 0.1681  data: 0.0005  max mem: 4132
[17:55:46.957801] Epoch: [11]  [300/781]  eta: 0:01:20  lr: 0.000247  training_loss: 1.9821 (1.9931)  classification_loss: 1.8509 (1.8735)  loss_mask: 0.1098 (0.1195)  time: 0.1675  data: 0.0004  max mem: 4132
[17:55:50.315439] Epoch: [11]  [320/781]  eta: 0:01:17  lr: 0.000247  training_loss: 1.9439 (1.9904)  classification_loss: 1.8510 (1.8716)  loss_mask: 0.0904 (0.1188)  time: 0.1678  data: 0.0003  max mem: 4132
[17:55:53.621541] Epoch: [11]  [340/781]  eta: 0:01:13  lr: 0.000247  training_loss: 1.9486 (1.9874)  classification_loss: 1.8510 (1.8701)  loss_mask: 0.0887 (0.1173)  time: 0.1652  data: 0.0003  max mem: 4132
[17:55:56.959289] Epoch: [11]  [360/781]  eta: 0:01:10  lr: 0.000247  training_loss: 2.0210 (1.9915)  classification_loss: 1.8897 (1.8709)  loss_mask: 0.1336 (0.1206)  time: 0.1668  data: 0.0003  max mem: 4132
[17:56:00.285039] Epoch: [11]  [380/781]  eta: 0:01:07  lr: 0.000247  training_loss: 2.0307 (1.9936)  classification_loss: 1.8560 (1.8702)  loss_mask: 0.1527 (0.1234)  time: 0.1662  data: 0.0003  max mem: 4132
[17:56:03.578243] Epoch: [11]  [400/781]  eta: 0:01:03  lr: 0.000247  training_loss: 1.9402 (1.9927)  classification_loss: 1.8230 (1.8692)  loss_mask: 0.1037 (0.1235)  time: 0.1646  data: 0.0003  max mem: 4132
[17:56:06.902482] Epoch: [11]  [420/781]  eta: 0:01:00  lr: 0.000247  training_loss: 1.9355 (1.9899)  classification_loss: 1.8382 (1.8687)  loss_mask: 0.0769 (0.1212)  time: 0.1661  data: 0.0003  max mem: 4132
[17:56:10.176537] Epoch: [11]  [440/781]  eta: 0:00:57  lr: 0.000247  training_loss: 1.9693 (1.9894)  classification_loss: 1.8883 (1.8702)  loss_mask: 0.0726 (0.1192)  time: 0.1636  data: 0.0003  max mem: 4132
[17:56:13.451350] Epoch: [11]  [460/781]  eta: 0:00:53  lr: 0.000247  training_loss: 1.9383 (1.9870)  classification_loss: 1.8522 (1.8693)  loss_mask: 0.0769 (0.1177)  time: 0.1637  data: 0.0002  max mem: 4132
[17:56:16.746660] Epoch: [11]  [480/781]  eta: 0:00:50  lr: 0.000247  training_loss: 1.9791 (1.9858)  classification_loss: 1.8929 (1.8698)  loss_mask: 0.0775 (0.1160)  time: 0.1647  data: 0.0002  max mem: 4132
[17:56:20.020415] Epoch: [11]  [500/781]  eta: 0:00:46  lr: 0.000247  training_loss: 2.0364 (1.9880)  classification_loss: 1.8830 (1.8705)  loss_mask: 0.1303 (0.1175)  time: 0.1636  data: 0.0002  max mem: 4132
[17:56:23.324994] Epoch: [11]  [520/781]  eta: 0:00:43  lr: 0.000247  training_loss: 2.0143 (1.9905)  classification_loss: 1.8754 (1.8717)  loss_mask: 0.1117 (0.1188)  time: 0.1651  data: 0.0003  max mem: 4132
[17:56:26.616950] Epoch: [11]  [540/781]  eta: 0:00:40  lr: 0.000247  training_loss: 1.9212 (1.9897)  classification_loss: 1.8199 (1.8718)  loss_mask: 0.0881 (0.1179)  time: 0.1645  data: 0.0003  max mem: 4132
[17:56:29.908465] Epoch: [11]  [560/781]  eta: 0:00:36  lr: 0.000247  training_loss: 1.9056 (1.9883)  classification_loss: 1.8378 (1.8712)  loss_mask: 0.0764 (0.1171)  time: 0.1645  data: 0.0003  max mem: 4132
[17:56:33.235171] Epoch: [11]  [580/781]  eta: 0:00:33  lr: 0.000247  training_loss: 2.0261 (1.9894)  classification_loss: 1.8236 (1.8701)  loss_mask: 0.1299 (0.1193)  time: 0.1662  data: 0.0003  max mem: 4132
[17:56:36.516354] Epoch: [11]  [600/781]  eta: 0:00:30  lr: 0.000247  training_loss: 1.9734 (1.9900)  classification_loss: 1.8499 (1.8690)  loss_mask: 0.1412 (0.1210)  time: 0.1640  data: 0.0003  max mem: 4132
[17:56:39.854432] Epoch: [11]  [620/781]  eta: 0:00:26  lr: 0.000247  training_loss: 1.9370 (1.9885)  classification_loss: 1.8593 (1.8682)  loss_mask: 0.0938 (0.1203)  time: 0.1668  data: 0.0003  max mem: 4132
[17:56:43.124079] Epoch: [11]  [640/781]  eta: 0:00:23  lr: 0.000247  training_loss: 1.8871 (1.9858)  classification_loss: 1.8141 (1.8669)  loss_mask: 0.0650 (0.1189)  time: 0.1634  data: 0.0002  max mem: 4132
[17:56:46.388884] Epoch: [11]  [660/781]  eta: 0:00:20  lr: 0.000247  training_loss: 1.9374 (1.9844)  classification_loss: 1.8417 (1.8665)  loss_mask: 0.0799 (0.1179)  time: 0.1632  data: 0.0003  max mem: 4132
[17:56:49.653792] Epoch: [11]  [680/781]  eta: 0:00:16  lr: 0.000247  training_loss: 1.9234 (1.9831)  classification_loss: 1.8431 (1.8659)  loss_mask: 0.0814 (0.1172)  time: 0.1632  data: 0.0002  max mem: 4132
[17:56:52.983806] Epoch: [11]  [700/781]  eta: 0:00:13  lr: 0.000247  training_loss: 1.9119 (1.9823)  classification_loss: 1.8156 (1.8651)  loss_mask: 0.0953 (0.1171)  time: 0.1664  data: 0.0003  max mem: 4132
[17:56:56.312007] Epoch: [11]  [720/781]  eta: 0:00:10  lr: 0.000247  training_loss: 1.9744 (1.9825)  classification_loss: 1.8559 (1.8653)  loss_mask: 0.1055 (0.1171)  time: 0.1663  data: 0.0003  max mem: 4132
[17:56:59.611158] Epoch: [11]  [740/781]  eta: 0:00:06  lr: 0.000247  training_loss: 1.9407 (1.9810)  classification_loss: 1.8374 (1.8641)  loss_mask: 0.0877 (0.1169)  time: 0.1649  data: 0.0003  max mem: 4132
[17:57:02.888731] Epoch: [11]  [760/781]  eta: 0:00:03  lr: 0.000247  training_loss: 1.9214 (1.9797)  classification_loss: 1.8516 (1.8637)  loss_mask: 0.0765 (0.1160)  time: 0.1638  data: 0.0003  max mem: 4132
[17:57:06.219964] Epoch: [11]  [780/781]  eta: 0:00:00  lr: 0.000247  training_loss: 1.9151 (1.9784)  classification_loss: 1.8462 (1.8635)  loss_mask: 0.0614 (0.1149)  time: 0.1664  data: 0.0002  max mem: 4132
[17:57:06.427399] Epoch: [11] Total time: 0:02:10 (0.1665 s / it)
[17:57:06.428003] Averaged stats: lr: 0.000247  training_loss: 1.9151 (1.9784)  classification_loss: 1.8462 (1.8635)  loss_mask: 0.0614 (0.1149)
[17:57:07.175495] Test:  [  0/157]  eta: 0:01:56  testing_loss: 1.2638 (1.2638)  acc1: 57.8125 (57.8125)  acc5: 92.1875 (92.1875)  time: 0.7405  data: 0.7106  max mem: 4132
[17:57:07.462483] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.2645 (1.2941)  acc1: 54.6875 (53.9773)  acc5: 95.3125 (94.8864)  time: 0.0932  data: 0.0649  max mem: 4132
[17:57:07.750236] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.2430 (1.2609)  acc1: 54.6875 (55.8036)  acc5: 95.3125 (94.7917)  time: 0.0286  data: 0.0003  max mem: 4132
[17:57:08.046105] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.2430 (1.2674)  acc1: 59.3750 (56.6028)  acc5: 95.3125 (94.3548)  time: 0.0290  data: 0.0003  max mem: 4132
[17:57:08.337876] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.2680 (1.2735)  acc1: 59.3750 (57.0122)  acc5: 93.7500 (94.0930)  time: 0.0292  data: 0.0003  max mem: 4132
[17:57:08.627309] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.2409 (1.2691)  acc1: 57.8125 (57.1078)  acc5: 93.7500 (94.3015)  time: 0.0289  data: 0.0003  max mem: 4132
[17:57:08.918128] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.2369 (1.2615)  acc1: 57.8125 (57.5307)  acc5: 95.3125 (94.3904)  time: 0.0289  data: 0.0003  max mem: 4132
[17:57:09.202083] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.1946 (1.2547)  acc1: 59.3750 (57.7685)  acc5: 95.3125 (94.6303)  time: 0.0286  data: 0.0002  max mem: 4132
[17:57:09.489757] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.2267 (1.2574)  acc1: 57.8125 (57.6968)  acc5: 95.3125 (94.5023)  time: 0.0284  data: 0.0003  max mem: 4132
[17:57:09.774051] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.2700 (1.2598)  acc1: 56.2500 (57.5206)  acc5: 93.7500 (94.6772)  time: 0.0285  data: 0.0003  max mem: 4132
[17:57:10.065833] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.2972 (1.2638)  acc1: 53.1250 (57.1937)  acc5: 96.8750 (94.8175)  time: 0.0286  data: 0.0004  max mem: 4132
[17:57:10.353430] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.3318 (1.2693)  acc1: 53.1250 (56.8271)  acc5: 95.3125 (94.7635)  time: 0.0288  data: 0.0004  max mem: 4132
[17:57:10.639056] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.2609 (1.2657)  acc1: 53.1250 (56.9215)  acc5: 95.3125 (94.8735)  time: 0.0285  data: 0.0002  max mem: 4132
[17:57:10.924710] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.2581 (1.2681)  acc1: 57.8125 (56.8941)  acc5: 95.3125 (94.7281)  time: 0.0284  data: 0.0003  max mem: 4132
[17:57:11.209931] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.2609 (1.2682)  acc1: 57.8125 (57.1365)  acc5: 93.7500 (94.7252)  time: 0.0284  data: 0.0002  max mem: 4132
[17:57:11.493589] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.2457 (1.2655)  acc1: 59.3750 (57.3055)  acc5: 95.3125 (94.8262)  time: 0.0283  data: 0.0001  max mem: 4132
[17:57:11.646341] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.2423 (1.2656)  acc1: 57.8125 (57.2900)  acc5: 95.3125 (94.9100)  time: 0.0273  data: 0.0001  max mem: 4132
[17:57:11.832200] Test: Total time: 0:00:05 (0.0344 s / it)
[17:57:11.832954] * Acc@1 57.290 Acc@5 94.910 loss 1.266
[17:57:11.833289] Accuracy of the network on the 10000 test images: 57.3%
[17:57:11.833629] Max accuracy: 57.29%
[17:57:12.223024] log_dir: ./output_dir
[17:57:13.151499] Epoch: [12]  [  0/781]  eta: 0:12:03  lr: 0.000247  training_loss: 1.7599 (1.7599)  classification_loss: 1.6931 (1.6931)  loss_mask: 0.0668 (0.0668)  time: 0.9262  data: 0.7182  max mem: 4132
[17:57:16.489090] Epoch: [12]  [ 20/781]  eta: 0:02:34  lr: 0.000247  training_loss: 1.8994 (1.9032)  classification_loss: 1.8281 (1.8261)  loss_mask: 0.0704 (0.0771)  time: 0.1667  data: 0.0005  max mem: 4132
[17:57:19.840194] Epoch: [12]  [ 40/781]  eta: 0:02:17  lr: 0.000247  training_loss: 1.9469 (1.9169)  classification_loss: 1.8304 (1.8326)  loss_mask: 0.0869 (0.0843)  time: 0.1675  data: 0.0004  max mem: 4132
[17:57:23.159520] Epoch: [12]  [ 60/781]  eta: 0:02:09  lr: 0.000247  training_loss: 1.9282 (1.9249)  classification_loss: 1.8374 (1.8343)  loss_mask: 0.0819 (0.0905)  time: 0.1659  data: 0.0003  max mem: 4132
[17:57:26.473377] Epoch: [12]  [ 80/781]  eta: 0:02:03  lr: 0.000247  training_loss: 1.9369 (1.9321)  classification_loss: 1.8473 (1.8426)  loss_mask: 0.0731 (0.0894)  time: 0.1656  data: 0.0003  max mem: 4132
[17:57:29.820309] Epoch: [12]  [100/781]  eta: 0:01:58  lr: 0.000247  training_loss: 1.8893 (1.9291)  classification_loss: 1.8251 (1.8433)  loss_mask: 0.0536 (0.0859)  time: 0.1673  data: 0.0002  max mem: 4132
[17:57:33.171161] Epoch: [12]  [120/781]  eta: 0:01:54  lr: 0.000247  training_loss: 1.9264 (1.9273)  classification_loss: 1.8355 (1.8406)  loss_mask: 0.0793 (0.0868)  time: 0.1675  data: 0.0003  max mem: 4132
[17:57:36.481250] Epoch: [12]  [140/781]  eta: 0:01:50  lr: 0.000247  training_loss: 1.9021 (1.9225)  classification_loss: 1.7992 (1.8365)  loss_mask: 0.0770 (0.0860)  time: 0.1654  data: 0.0003  max mem: 4132
[17:57:39.808541] Epoch: [12]  [160/781]  eta: 0:01:46  lr: 0.000246  training_loss: 1.8837 (1.9231)  classification_loss: 1.7905 (1.8375)  loss_mask: 0.0766 (0.0856)  time: 0.1663  data: 0.0003  max mem: 4132
[17:57:43.124920] Epoch: [12]  [180/781]  eta: 0:01:42  lr: 0.000246  training_loss: 1.8799 (1.9199)  classification_loss: 1.8113 (1.8353)  loss_mask: 0.0747 (0.0846)  time: 0.1657  data: 0.0002  max mem: 4132
[17:57:46.433806] Epoch: [12]  [200/781]  eta: 0:01:38  lr: 0.000246  training_loss: 1.9304 (1.9206)  classification_loss: 1.8424 (1.8346)  loss_mask: 0.0903 (0.0860)  time: 0.1654  data: 0.0003  max mem: 4132
[17:57:49.728307] Epoch: [12]  [220/781]  eta: 0:01:35  lr: 0.000246  training_loss: 1.9924 (1.9251)  classification_loss: 1.8353 (1.8345)  loss_mask: 0.0939 (0.0906)  time: 0.1646  data: 0.0003  max mem: 4132
[17:57:53.062810] Epoch: [12]  [240/781]  eta: 0:01:31  lr: 0.000246  training_loss: 1.9158 (1.9233)  classification_loss: 1.8452 (1.8337)  loss_mask: 0.0755 (0.0896)  time: 0.1666  data: 0.0003  max mem: 4132
[17:57:56.373940] Epoch: [12]  [260/781]  eta: 0:01:28  lr: 0.000246  training_loss: 1.9266 (1.9237)  classification_loss: 1.7897 (1.8310)  loss_mask: 0.0986 (0.0927)  time: 0.1655  data: 0.0003  max mem: 4132
[17:57:59.688445] Epoch: [12]  [280/781]  eta: 0:01:24  lr: 0.000246  training_loss: 1.9313 (1.9246)  classification_loss: 1.8232 (1.8302)  loss_mask: 0.1187 (0.0945)  time: 0.1656  data: 0.0003  max mem: 4132
[17:58:02.995239] Epoch: [12]  [300/781]  eta: 0:01:21  lr: 0.000246  training_loss: 1.8949 (1.9236)  classification_loss: 1.8165 (1.8313)  loss_mask: 0.0554 (0.0923)  time: 0.1653  data: 0.0003  max mem: 4132
[17:58:06.286602] Epoch: [12]  [320/781]  eta: 0:01:17  lr: 0.000246  training_loss: 1.8730 (1.9213)  classification_loss: 1.7914 (1.8307)  loss_mask: 0.0529 (0.0905)  time: 0.1645  data: 0.0005  max mem: 4132
[17:58:09.569789] Epoch: [12]  [340/781]  eta: 0:01:14  lr: 0.000246  training_loss: 1.9030 (1.9194)  classification_loss: 1.7926 (1.8285)  loss_mask: 0.0837 (0.0910)  time: 0.1641  data: 0.0002  max mem: 4132
[17:58:12.852279] Epoch: [12]  [360/781]  eta: 0:01:10  lr: 0.000246  training_loss: 1.9607 (1.9225)  classification_loss: 1.8635 (1.8296)  loss_mask: 0.0876 (0.0930)  time: 0.1640  data: 0.0002  max mem: 4132
[17:58:16.162511] Epoch: [12]  [380/781]  eta: 0:01:07  lr: 0.000246  training_loss: 1.9402 (1.9244)  classification_loss: 1.8247 (1.8293)  loss_mask: 0.1138 (0.0951)  time: 0.1654  data: 0.0003  max mem: 4132
[17:58:19.451595] Epoch: [12]  [400/781]  eta: 0:01:03  lr: 0.000246  training_loss: 1.9452 (1.9265)  classification_loss: 1.8652 (1.8299)  loss_mask: 0.1014 (0.0966)  time: 0.1644  data: 0.0002  max mem: 4132
[17:58:22.734322] Epoch: [12]  [420/781]  eta: 0:01:00  lr: 0.000246  training_loss: 1.8881 (1.9254)  classification_loss: 1.8148 (1.8292)  loss_mask: 0.0786 (0.0961)  time: 0.1641  data: 0.0004  max mem: 4132
[17:58:26.044655] Epoch: [12]  [440/781]  eta: 0:00:57  lr: 0.000246  training_loss: 1.9198 (1.9250)  classification_loss: 1.8467 (1.8291)  loss_mask: 0.0756 (0.0959)  time: 0.1654  data: 0.0003  max mem: 4132
[17:58:29.321498] Epoch: [12]  [460/781]  eta: 0:00:53  lr: 0.000246  training_loss: 1.8825 (1.9231)  classification_loss: 1.7662 (1.8273)  loss_mask: 0.0792 (0.0958)  time: 0.1638  data: 0.0002  max mem: 4132
[17:58:32.628373] Epoch: [12]  [480/781]  eta: 0:00:50  lr: 0.000246  training_loss: 1.8728 (1.9221)  classification_loss: 1.8094 (1.8267)  loss_mask: 0.0704 (0.0954)  time: 0.1653  data: 0.0003  max mem: 4132
[17:58:35.956770] Epoch: [12]  [500/781]  eta: 0:00:46  lr: 0.000246  training_loss: 1.8959 (1.9202)  classification_loss: 1.7692 (1.8249)  loss_mask: 0.0774 (0.0953)  time: 0.1663  data: 0.0002  max mem: 4132
[17:58:39.242222] Epoch: [12]  [520/781]  eta: 0:00:43  lr: 0.000246  training_loss: 1.8497 (1.9188)  classification_loss: 1.7845 (1.8240)  loss_mask: 0.0709 (0.0948)  time: 0.1642  data: 0.0003  max mem: 4132
[17:58:42.528639] Epoch: [12]  [540/781]  eta: 0:00:40  lr: 0.000246  training_loss: 1.9451 (1.9197)  classification_loss: 1.7539 (1.8228)  loss_mask: 0.1187 (0.0969)  time: 0.1641  data: 0.0003  max mem: 4132
[17:58:45.818875] Epoch: [12]  [560/781]  eta: 0:00:36  lr: 0.000246  training_loss: 1.9536 (1.9213)  classification_loss: 1.8296 (1.8235)  loss_mask: 0.0961 (0.0978)  time: 0.1644  data: 0.0002  max mem: 4132
[17:58:49.062597] Epoch: [12]  [580/781]  eta: 0:00:33  lr: 0.000246  training_loss: 1.8788 (1.9202)  classification_loss: 1.7746 (1.8223)  loss_mask: 0.0797 (0.0979)  time: 0.1621  data: 0.0002  max mem: 4132
[17:58:52.313291] Epoch: [12]  [600/781]  eta: 0:00:30  lr: 0.000246  training_loss: 1.7909 (1.9175)  classification_loss: 1.7596 (1.8208)  loss_mask: 0.0541 (0.0967)  time: 0.1624  data: 0.0002  max mem: 4132
[17:58:55.586873] Epoch: [12]  [620/781]  eta: 0:00:26  lr: 0.000246  training_loss: 1.8014 (1.9142)  classification_loss: 1.7435 (1.8190)  loss_mask: 0.0469 (0.0953)  time: 0.1636  data: 0.0002  max mem: 4132
[17:58:58.844607] Epoch: [12]  [640/781]  eta: 0:00:23  lr: 0.000246  training_loss: 1.8244 (1.9125)  classification_loss: 1.7789 (1.8179)  loss_mask: 0.0643 (0.0947)  time: 0.1628  data: 0.0002  max mem: 4132
[17:59:02.122250] Epoch: [12]  [660/781]  eta: 0:00:20  lr: 0.000246  training_loss: 1.8966 (1.9117)  classification_loss: 1.8206 (1.8177)  loss_mask: 0.0713 (0.0940)  time: 0.1638  data: 0.0006  max mem: 4132
[17:59:05.440012] Epoch: [12]  [680/781]  eta: 0:00:16  lr: 0.000246  training_loss: 1.9352 (1.9128)  classification_loss: 1.8266 (1.8185)  loss_mask: 0.0732 (0.0943)  time: 0.1658  data: 0.0002  max mem: 4132
[17:59:08.755816] Epoch: [12]  [700/781]  eta: 0:00:13  lr: 0.000246  training_loss: 1.9302 (1.9126)  classification_loss: 1.8402 (1.8182)  loss_mask: 0.0982 (0.0944)  time: 0.1657  data: 0.0003  max mem: 4132
[17:59:12.049301] Epoch: [12]  [720/781]  eta: 0:00:10  lr: 0.000246  training_loss: 1.8663 (1.9119)  classification_loss: 1.7999 (1.8183)  loss_mask: 0.0587 (0.0936)  time: 0.1646  data: 0.0003  max mem: 4132
[17:59:15.333851] Epoch: [12]  [740/781]  eta: 0:00:06  lr: 0.000246  training_loss: 1.8914 (1.9111)  classification_loss: 1.8308 (1.8182)  loss_mask: 0.0594 (0.0928)  time: 0.1641  data: 0.0003  max mem: 4132
[17:59:18.625682] Epoch: [12]  [760/781]  eta: 0:00:03  lr: 0.000246  training_loss: 1.8762 (1.9104)  classification_loss: 1.8151 (1.8180)  loss_mask: 0.0692 (0.0923)  time: 0.1645  data: 0.0003  max mem: 4132
[17:59:21.935424] Epoch: [12]  [780/781]  eta: 0:00:00  lr: 0.000246  training_loss: 1.8525 (1.9092)  classification_loss: 1.8212 (1.8176)  loss_mask: 0.0577 (0.0916)  time: 0.1654  data: 0.0003  max mem: 4132
[17:59:22.102397] Epoch: [12] Total time: 0:02:09 (0.1663 s / it)
[17:59:22.103238] Averaged stats: lr: 0.000246  training_loss: 1.8525 (1.9092)  classification_loss: 1.8212 (1.8176)  loss_mask: 0.0577 (0.0916)
[17:59:22.826821] Test:  [  0/157]  eta: 0:01:52  testing_loss: 1.1255 (1.1255)  acc1: 59.3750 (59.3750)  acc5: 93.7500 (93.7500)  time: 0.7175  data: 0.6862  max mem: 4132
[17:59:23.122709] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.2253 (1.2419)  acc1: 57.8125 (55.9659)  acc5: 95.3125 (95.3125)  time: 0.0919  data: 0.0626  max mem: 4132
[17:59:23.414056] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.2036 (1.2064)  acc1: 56.2500 (58.0357)  acc5: 96.8750 (95.9821)  time: 0.0291  data: 0.0002  max mem: 4132
[17:59:23.704767] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.2007 (1.2108)  acc1: 57.8125 (58.4173)  acc5: 95.3125 (95.7157)  time: 0.0290  data: 0.0003  max mem: 4132
[17:59:23.998112] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.2007 (1.2142)  acc1: 57.8125 (58.3841)  acc5: 93.7500 (95.3887)  time: 0.0291  data: 0.0003  max mem: 4132
[17:59:24.290934] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.1865 (1.2088)  acc1: 60.9375 (59.3137)  acc5: 95.3125 (95.6495)  time: 0.0292  data: 0.0003  max mem: 4132
[17:59:24.585423] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.1595 (1.2010)  acc1: 62.5000 (59.5287)  acc5: 96.8750 (95.6455)  time: 0.0292  data: 0.0003  max mem: 4132
[17:59:24.879907] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.1408 (1.1952)  acc1: 60.9375 (59.5951)  acc5: 96.8750 (95.7086)  time: 0.0292  data: 0.0002  max mem: 4132
[17:59:25.169640] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.1996 (1.1972)  acc1: 59.3750 (59.2014)  acc5: 95.3125 (95.6404)  time: 0.0289  data: 0.0002  max mem: 4132
[17:59:25.458190] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.2169 (1.2006)  acc1: 57.8125 (59.2033)  acc5: 95.3125 (95.6731)  time: 0.0287  data: 0.0002  max mem: 4132
[17:59:25.749318] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.2468 (1.2062)  acc1: 56.2500 (58.7407)  acc5: 95.3125 (95.6374)  time: 0.0288  data: 0.0002  max mem: 4132
[17:59:26.046460] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.2626 (1.2094)  acc1: 56.2500 (58.6008)  acc5: 95.3125 (95.6222)  time: 0.0293  data: 0.0002  max mem: 4132
[17:59:26.337873] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.2030 (1.2057)  acc1: 57.8125 (58.6906)  acc5: 95.3125 (95.6999)  time: 0.0293  data: 0.0002  max mem: 4132
[17:59:26.624078] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.1807 (1.2067)  acc1: 57.8125 (58.5401)  acc5: 95.3125 (95.6584)  time: 0.0287  data: 0.0002  max mem: 4132
[17:59:26.909712] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.1850 (1.2058)  acc1: 60.9375 (58.9207)  acc5: 95.3125 (95.6228)  time: 0.0285  data: 0.0002  max mem: 4132
[17:59:27.194579] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.1755 (1.2027)  acc1: 62.5000 (59.1370)  acc5: 95.3125 (95.6643)  time: 0.0284  data: 0.0002  max mem: 4132
[17:59:27.352483] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.1755 (1.2039)  acc1: 59.3750 (59.0300)  acc5: 96.8750 (95.6800)  time: 0.0277  data: 0.0002  max mem: 4132
[17:59:27.524434] Test: Total time: 0:00:05 (0.0345 s / it)
[17:59:27.524951] * Acc@1 59.030 Acc@5 95.680 loss 1.204
[17:59:27.525286] Accuracy of the network on the 10000 test images: 59.0%
[17:59:27.525526] Max accuracy: 59.03%
[17:59:27.718983] log_dir: ./output_dir
[17:59:28.626444] Epoch: [13]  [  0/781]  eta: 0:11:46  lr: 0.000246  training_loss: 1.8788 (1.8788)  classification_loss: 1.8245 (1.8245)  loss_mask: 0.0542 (0.0542)  time: 0.9050  data: 0.7045  max mem: 4132
[17:59:31.921745] Epoch: [13]  [ 20/781]  eta: 0:02:32  lr: 0.000246  training_loss: 1.9041 (1.8966)  classification_loss: 1.7851 (1.7853)  loss_mask: 0.0803 (0.1113)  time: 0.1647  data: 0.0002  max mem: 4132
[17:59:35.194943] Epoch: [13]  [ 40/781]  eta: 0:02:15  lr: 0.000246  training_loss: 1.9085 (1.9103)  classification_loss: 1.8253 (1.8138)  loss_mask: 0.0734 (0.0965)  time: 0.1636  data: 0.0002  max mem: 4132
[17:59:38.497805] Epoch: [13]  [ 60/781]  eta: 0:02:07  lr: 0.000246  training_loss: 1.8933 (1.9076)  classification_loss: 1.8353 (1.8251)  loss_mask: 0.0480 (0.0825)  time: 0.1651  data: 0.0003  max mem: 4132
[17:59:41.788850] Epoch: [13]  [ 80/781]  eta: 0:02:01  lr: 0.000246  training_loss: 1.8719 (1.8979)  classification_loss: 1.8225 (1.8224)  loss_mask: 0.0494 (0.0755)  time: 0.1645  data: 0.0003  max mem: 4132
[17:59:45.078583] Epoch: [13]  [100/781]  eta: 0:01:56  lr: 0.000246  training_loss: 1.9062 (1.8988)  classification_loss: 1.8340 (1.8250)  loss_mask: 0.0679 (0.0738)  time: 0.1644  data: 0.0003  max mem: 4132
[17:59:48.360786] Epoch: [13]  [120/781]  eta: 0:01:52  lr: 0.000246  training_loss: 2.0218 (1.9171)  classification_loss: 1.7832 (1.8234)  loss_mask: 0.1037 (0.0937)  time: 0.1640  data: 0.0003  max mem: 4132
[17:59:51.634392] Epoch: [13]  [140/781]  eta: 0:01:48  lr: 0.000245  training_loss: 1.9838 (1.9312)  classification_loss: 1.7718 (1.8171)  loss_mask: 0.1998 (0.1141)  time: 0.1636  data: 0.0002  max mem: 4132
[17:59:54.939071] Epoch: [13]  [160/781]  eta: 0:01:44  lr: 0.000245  training_loss: 1.9525 (1.9326)  classification_loss: 1.7855 (1.8168)  loss_mask: 0.1121 (0.1158)  time: 0.1652  data: 0.0002  max mem: 4132
[17:59:58.214190] Epoch: [13]  [180/781]  eta: 0:01:41  lr: 0.000245  training_loss: 1.8481 (1.9263)  classification_loss: 1.7311 (1.8101)  loss_mask: 0.1048 (0.1161)  time: 0.1637  data: 0.0003  max mem: 4132
[18:00:01.475420] Epoch: [13]  [200/781]  eta: 0:01:37  lr: 0.000245  training_loss: 1.9126 (1.9265)  classification_loss: 1.7707 (1.8087)  loss_mask: 0.1149 (0.1178)  time: 0.1630  data: 0.0002  max mem: 4132
[18:00:04.794906] Epoch: [13]  [220/781]  eta: 0:01:34  lr: 0.000245  training_loss: 1.9202 (1.9238)  classification_loss: 1.8335 (1.8093)  loss_mask: 0.0794 (0.1145)  time: 0.1659  data: 0.0002  max mem: 4132
[18:00:08.078629] Epoch: [13]  [240/781]  eta: 0:01:30  lr: 0.000245  training_loss: 1.8991 (1.9210)  classification_loss: 1.8298 (1.8098)  loss_mask: 0.0798 (0.1111)  time: 0.1641  data: 0.0002  max mem: 4132
[18:00:11.373074] Epoch: [13]  [260/781]  eta: 0:01:27  lr: 0.000245  training_loss: 1.8646 (1.9164)  classification_loss: 1.7785 (1.8070)  loss_mask: 0.0732 (0.1094)  time: 0.1646  data: 0.0003  max mem: 4132
[18:00:14.683350] Epoch: [13]  [280/781]  eta: 0:01:23  lr: 0.000245  training_loss: 1.8658 (1.9131)  classification_loss: 1.8009 (1.8065)  loss_mask: 0.0666 (0.1067)  time: 0.1654  data: 0.0002  max mem: 4132
[18:00:17.948326] Epoch: [13]  [300/781]  eta: 0:01:20  lr: 0.000245  training_loss: 1.8161 (1.9070)  classification_loss: 1.7680 (1.8034)  loss_mask: 0.0579 (0.1036)  time: 0.1632  data: 0.0003  max mem: 4132
[18:00:21.238396] Epoch: [13]  [320/781]  eta: 0:01:16  lr: 0.000245  training_loss: 1.9209 (1.9079)  classification_loss: 1.8237 (1.8046)  loss_mask: 0.0912 (0.1033)  time: 0.1644  data: 0.0002  max mem: 4132
[18:00:24.501024] Epoch: [13]  [340/781]  eta: 0:01:13  lr: 0.000245  training_loss: 1.8426 (1.9034)  classification_loss: 1.7800 (1.8032)  loss_mask: 0.0441 (0.1003)  time: 0.1630  data: 0.0002  max mem: 4132
[18:00:27.831825] Epoch: [13]  [360/781]  eta: 0:01:10  lr: 0.000245  training_loss: 1.8926 (1.9014)  classification_loss: 1.7822 (1.8025)  loss_mask: 0.0572 (0.0989)  time: 0.1664  data: 0.0003  max mem: 4132
[18:00:31.118393] Epoch: [13]  [380/781]  eta: 0:01:06  lr: 0.000245  training_loss: 1.8849 (1.9004)  classification_loss: 1.8240 (1.8030)  loss_mask: 0.0609 (0.0974)  time: 0.1642  data: 0.0003  max mem: 4132
[18:00:34.405962] Epoch: [13]  [400/781]  eta: 0:01:03  lr: 0.000245  training_loss: 1.8439 (1.8979)  classification_loss: 1.7604 (1.8022)  loss_mask: 0.0582 (0.0956)  time: 0.1643  data: 0.0003  max mem: 4132
[18:00:37.647044] Epoch: [13]  [420/781]  eta: 0:00:59  lr: 0.000245  training_loss: 1.8437 (1.8948)  classification_loss: 1.7970 (1.8013)  loss_mask: 0.0536 (0.0935)  time: 0.1620  data: 0.0003  max mem: 4132
[18:00:40.977976] Epoch: [13]  [440/781]  eta: 0:00:56  lr: 0.000245  training_loss: 1.8493 (1.8913)  classification_loss: 1.7917 (1.7991)  loss_mask: 0.0576 (0.0922)  time: 0.1665  data: 0.0003  max mem: 4132
[18:00:44.357304] Epoch: [13]  [460/781]  eta: 0:00:53  lr: 0.000245  training_loss: 1.8983 (1.8930)  classification_loss: 1.7891 (1.7982)  loss_mask: 0.1119 (0.0947)  time: 0.1688  data: 0.0004  max mem: 4132
[18:00:47.762105] Epoch: [13]  [480/781]  eta: 0:00:50  lr: 0.000245  training_loss: 1.9244 (1.8943)  classification_loss: 1.7979 (1.7984)  loss_mask: 0.1001 (0.0959)  time: 0.1701  data: 0.0004  max mem: 4132
[18:00:51.077802] Epoch: [13]  [500/781]  eta: 0:00:46  lr: 0.000245  training_loss: 1.9373 (1.8955)  classification_loss: 1.8016 (1.7985)  loss_mask: 0.0948 (0.0971)  time: 0.1656  data: 0.0003  max mem: 4132
[18:00:54.397286] Epoch: [13]  [520/781]  eta: 0:00:43  lr: 0.000245  training_loss: 1.8592 (1.8948)  classification_loss: 1.7591 (1.7975)  loss_mask: 0.0938 (0.0973)  time: 0.1659  data: 0.0003  max mem: 4132
[18:00:57.687195] Epoch: [13]  [540/781]  eta: 0:00:40  lr: 0.000245  training_loss: 1.8888 (1.8942)  classification_loss: 1.8260 (1.7986)  loss_mask: 0.0502 (0.0956)  time: 0.1644  data: 0.0003  max mem: 4132
[18:01:00.985790] Epoch: [13]  [560/781]  eta: 0:00:36  lr: 0.000245  training_loss: 1.8287 (1.8926)  classification_loss: 1.7966 (1.7981)  loss_mask: 0.0539 (0.0944)  time: 0.1648  data: 0.0003  max mem: 4132
[18:01:04.339363] Epoch: [13]  [580/781]  eta: 0:00:33  lr: 0.000245  training_loss: 1.9331 (1.8948)  classification_loss: 1.7840 (1.7979)  loss_mask: 0.1124 (0.0969)  time: 0.1676  data: 0.0004  max mem: 4132
[18:01:07.651979] Epoch: [13]  [600/781]  eta: 0:00:30  lr: 0.000245  training_loss: 1.8801 (1.8934)  classification_loss: 1.7518 (1.7968)  loss_mask: 0.0857 (0.0967)  time: 0.1655  data: 0.0002  max mem: 4132
[18:01:10.922428] Epoch: [13]  [620/781]  eta: 0:00:26  lr: 0.000245  training_loss: 1.8159 (1.8906)  classification_loss: 1.7252 (1.7951)  loss_mask: 0.0527 (0.0955)  time: 0.1634  data: 0.0002  max mem: 4132
[18:01:14.190749] Epoch: [13]  [640/781]  eta: 0:00:23  lr: 0.000245  training_loss: 1.8709 (1.8898)  classification_loss: 1.7745 (1.7946)  loss_mask: 0.0686 (0.0952)  time: 0.1633  data: 0.0003  max mem: 4132
[18:01:17.482199] Epoch: [13]  [660/781]  eta: 0:00:20  lr: 0.000245  training_loss: 1.8602 (1.8892)  classification_loss: 1.7722 (1.7944)  loss_mask: 0.0689 (0.0948)  time: 0.1645  data: 0.0002  max mem: 4132
[18:01:20.756781] Epoch: [13]  [680/781]  eta: 0:00:16  lr: 0.000245  training_loss: 1.8416 (1.8878)  classification_loss: 1.8047 (1.7945)  loss_mask: 0.0388 (0.0933)  time: 0.1636  data: 0.0002  max mem: 4132
[18:01:24.041973] Epoch: [13]  [700/781]  eta: 0:00:13  lr: 0.000245  training_loss: 1.7924 (1.8864)  classification_loss: 1.7612 (1.7943)  loss_mask: 0.0421 (0.0921)  time: 0.1642  data: 0.0003  max mem: 4132
[18:01:27.339615] Epoch: [13]  [720/781]  eta: 0:00:10  lr: 0.000245  training_loss: 1.8264 (1.8856)  classification_loss: 1.7555 (1.7936)  loss_mask: 0.0860 (0.0920)  time: 0.1648  data: 0.0003  max mem: 4132
[18:01:30.625660] Epoch: [13]  [740/781]  eta: 0:00:06  lr: 0.000245  training_loss: 1.8150 (1.8835)  classification_loss: 1.7842 (1.7925)  loss_mask: 0.0529 (0.0910)  time: 0.1642  data: 0.0002  max mem: 4132
[18:01:33.890146] Epoch: [13]  [760/781]  eta: 0:00:03  lr: 0.000245  training_loss: 1.9079 (1.8840)  classification_loss: 1.7751 (1.7931)  loss_mask: 0.0581 (0.0908)  time: 0.1631  data: 0.0002  max mem: 4132
[18:01:37.145384] Epoch: [13]  [780/781]  eta: 0:00:00  lr: 0.000245  training_loss: 1.9774 (1.8855)  classification_loss: 1.8329 (1.7942)  loss_mask: 0.0787 (0.0913)  time: 0.1627  data: 0.0003  max mem: 4132
[18:01:37.307756] Epoch: [13] Total time: 0:02:09 (0.1659 s / it)
[18:01:37.308303] Averaged stats: lr: 0.000245  training_loss: 1.9774 (1.8855)  classification_loss: 1.8329 (1.7942)  loss_mask: 0.0787 (0.0913)
[18:01:38.009665] Test:  [  0/157]  eta: 0:01:49  testing_loss: 1.0891 (1.0891)  acc1: 60.9375 (60.9375)  acc5: 95.3125 (95.3125)  time: 0.6953  data: 0.6643  max mem: 4132
[18:01:38.299765] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.1999 (1.2411)  acc1: 56.2500 (56.6761)  acc5: 96.8750 (95.5966)  time: 0.0893  data: 0.0606  max mem: 4132
[18:01:38.585822] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.1809 (1.2014)  acc1: 56.2500 (58.1101)  acc5: 96.8750 (95.9821)  time: 0.0286  data: 0.0002  max mem: 4132
[18:01:38.872508] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.1813 (1.2042)  acc1: 59.3750 (58.6694)  acc5: 95.3125 (95.6653)  time: 0.0285  data: 0.0002  max mem: 4132
[18:01:39.161618] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.2051 (1.2061)  acc1: 60.9375 (59.0701)  acc5: 93.7500 (95.3506)  time: 0.0286  data: 0.0002  max mem: 4132
[18:01:39.449340] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.2073 (1.2047)  acc1: 60.9375 (59.3750)  acc5: 95.3125 (95.4963)  time: 0.0287  data: 0.0002  max mem: 4132
[18:01:39.737607] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.1869 (1.1976)  acc1: 60.9375 (59.8617)  acc5: 96.8750 (95.4406)  time: 0.0287  data: 0.0002  max mem: 4132
[18:01:40.022670] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.1567 (1.1950)  acc1: 60.9375 (59.7271)  acc5: 95.3125 (95.4665)  time: 0.0285  data: 0.0002  max mem: 4132
[18:01:40.328906] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.1753 (1.1966)  acc1: 57.8125 (59.5679)  acc5: 95.3125 (95.4668)  time: 0.0294  data: 0.0003  max mem: 4132
[18:01:40.625897] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.1948 (1.1986)  acc1: 59.3750 (59.6154)  acc5: 96.8750 (95.6216)  time: 0.0299  data: 0.0003  max mem: 4132
[18:01:40.918925] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.2235 (1.2028)  acc1: 57.8125 (59.2512)  acc5: 96.8750 (95.6838)  time: 0.0292  data: 0.0004  max mem: 4132
[18:01:41.216389] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.2445 (1.2056)  acc1: 54.6875 (59.0372)  acc5: 95.3125 (95.6785)  time: 0.0294  data: 0.0004  max mem: 4132
[18:01:41.509198] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.2176 (1.2032)  acc1: 57.8125 (59.0780)  acc5: 95.3125 (95.7515)  time: 0.0293  data: 0.0004  max mem: 4132
[18:01:41.802992] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.2071 (1.2059)  acc1: 57.8125 (58.8263)  acc5: 95.3125 (95.7657)  time: 0.0291  data: 0.0004  max mem: 4132
[18:01:42.088393] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.2169 (1.2055)  acc1: 57.8125 (58.9761)  acc5: 95.3125 (95.7225)  time: 0.0288  data: 0.0002  max mem: 4132
[18:01:42.372098] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.1667 (1.2030)  acc1: 62.5000 (59.1991)  acc5: 95.3125 (95.6540)  time: 0.0283  data: 0.0002  max mem: 4132
[18:01:42.526979] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.1528 (1.2039)  acc1: 59.3750 (59.1300)  acc5: 95.3125 (95.6600)  time: 0.0274  data: 0.0002  max mem: 4132
[18:01:42.694954] Test: Total time: 0:00:05 (0.0343 s / it)
[18:01:42.695475] * Acc@1 59.130 Acc@5 95.660 loss 1.204
[18:01:42.695922] Accuracy of the network on the 10000 test images: 59.1%
[18:01:42.696182] Max accuracy: 59.13%
[18:01:42.912169] log_dir: ./output_dir
[18:01:43.835980] Epoch: [14]  [  0/781]  eta: 0:11:59  lr: 0.000245  training_loss: 1.8226 (1.8226)  classification_loss: 1.7259 (1.7259)  loss_mask: 0.0967 (0.0967)  time: 0.9216  data: 0.7443  max mem: 4132
[18:01:47.154961] Epoch: [14]  [ 20/781]  eta: 0:02:33  lr: 0.000244  training_loss: 1.8354 (1.8513)  classification_loss: 1.7545 (1.7421)  loss_mask: 0.0809 (0.1093)  time: 0.1658  data: 0.0004  max mem: 4132
[18:01:50.444644] Epoch: [14]  [ 40/781]  eta: 0:02:16  lr: 0.000244  training_loss: 2.0610 (2.0165)  classification_loss: 1.9491 (1.8721)  loss_mask: 0.1089 (0.1444)  time: 0.1644  data: 0.0003  max mem: 4132
[18:01:53.717302] Epoch: [14]  [ 60/781]  eta: 0:02:07  lr: 0.000244  training_loss: 2.1670 (2.0740)  classification_loss: 2.0622 (1.9348)  loss_mask: 0.1320 (0.1392)  time: 0.1635  data: 0.0003  max mem: 4132
[18:01:57.048217] Epoch: [14]  [ 80/781]  eta: 0:02:02  lr: 0.000244  training_loss: 2.1226 (2.1045)  classification_loss: 1.9579 (1.9473)  loss_mask: 0.1677 (0.1572)  time: 0.1665  data: 0.0002  max mem: 4132
[18:02:00.345658] Epoch: [14]  [100/781]  eta: 0:01:57  lr: 0.000244  training_loss: 2.0667 (2.1003)  classification_loss: 1.9486 (1.9456)  loss_mask: 0.1182 (0.1547)  time: 0.1648  data: 0.0002  max mem: 4132
[18:02:03.616761] Epoch: [14]  [120/781]  eta: 0:01:53  lr: 0.000244  training_loss: 1.9071 (2.0708)  classification_loss: 1.8106 (1.9254)  loss_mask: 0.0792 (0.1454)  time: 0.1635  data: 0.0002  max mem: 4132
[18:02:06.923308] Epoch: [14]  [140/781]  eta: 0:01:49  lr: 0.000244  training_loss: 1.8660 (2.0406)  classification_loss: 1.7873 (1.9050)  loss_mask: 0.0639 (0.1356)  time: 0.1652  data: 0.0003  max mem: 4132
[18:02:10.203658] Epoch: [14]  [160/781]  eta: 0:01:45  lr: 0.000244  training_loss: 1.8305 (2.0178)  classification_loss: 1.7689 (1.8909)  loss_mask: 0.0492 (0.1269)  time: 0.1638  data: 0.0002  max mem: 4132
[18:02:13.543746] Epoch: [14]  [180/781]  eta: 0:01:41  lr: 0.000244  training_loss: 1.8476 (1.9983)  classification_loss: 1.7602 (1.8777)  loss_mask: 0.0627 (0.1206)  time: 0.1669  data: 0.0002  max mem: 4132
[18:02:16.860946] Epoch: [14]  [200/781]  eta: 0:01:38  lr: 0.000244  training_loss: 1.8435 (1.9848)  classification_loss: 1.7761 (1.8677)  loss_mask: 0.0772 (0.1171)  time: 0.1657  data: 0.0003  max mem: 4132
[18:02:20.167448] Epoch: [14]  [220/781]  eta: 0:01:34  lr: 0.000244  training_loss: 1.8410 (1.9710)  classification_loss: 1.7861 (1.8584)  loss_mask: 0.0657 (0.1126)  time: 0.1652  data: 0.0003  max mem: 4132
[18:02:23.486147] Epoch: [14]  [240/781]  eta: 0:01:31  lr: 0.000244  training_loss: 1.8661 (1.9628)  classification_loss: 1.7613 (1.8530)  loss_mask: 0.0713 (0.1099)  time: 0.1658  data: 0.0004  max mem: 4132
[18:02:26.820921] Epoch: [14]  [260/781]  eta: 0:01:27  lr: 0.000244  training_loss: 1.8929 (1.9568)  classification_loss: 1.7989 (1.8492)  loss_mask: 0.0610 (0.1076)  time: 0.1666  data: 0.0003  max mem: 4132
[18:02:30.100548] Epoch: [14]  [280/781]  eta: 0:01:24  lr: 0.000244  training_loss: 1.8447 (1.9495)  classification_loss: 1.7354 (1.8451)  loss_mask: 0.0584 (0.1044)  time: 0.1639  data: 0.0003  max mem: 4132
[18:02:33.392794] Epoch: [14]  [300/781]  eta: 0:01:20  lr: 0.000244  training_loss: 1.8277 (1.9422)  classification_loss: 1.7693 (1.8410)  loss_mask: 0.0476 (0.1012)  time: 0.1645  data: 0.0003  max mem: 4132
[18:02:36.687924] Epoch: [14]  [320/781]  eta: 0:01:17  lr: 0.000244  training_loss: 1.8567 (1.9366)  classification_loss: 1.8016 (1.8382)  loss_mask: 0.0500 (0.0983)  time: 0.1647  data: 0.0003  max mem: 4132
[18:02:40.003149] Epoch: [14]  [340/781]  eta: 0:01:13  lr: 0.000244  training_loss: 1.8442 (1.9318)  classification_loss: 1.7522 (1.8349)  loss_mask: 0.0567 (0.0969)  time: 0.1657  data: 0.0003  max mem: 4132
[18:02:43.297380] Epoch: [14]  [360/781]  eta: 0:01:10  lr: 0.000244  training_loss: 1.8190 (1.9280)  classification_loss: 1.7820 (1.8331)  loss_mask: 0.0491 (0.0949)  time: 0.1646  data: 0.0003  max mem: 4132
[18:02:46.561915] Epoch: [14]  [380/781]  eta: 0:01:06  lr: 0.000244  training_loss: 1.9067 (1.9279)  classification_loss: 1.7569 (1.8301)  loss_mask: 0.1092 (0.0978)  time: 0.1631  data: 0.0002  max mem: 4132
[18:02:49.836808] Epoch: [14]  [400/781]  eta: 0:01:03  lr: 0.000244  training_loss: 1.8791 (1.9268)  classification_loss: 1.7925 (1.8281)  loss_mask: 0.0772 (0.0987)  time: 0.1637  data: 0.0003  max mem: 4132
[18:02:53.098538] Epoch: [14]  [420/781]  eta: 0:01:00  lr: 0.000244  training_loss: 1.7729 (1.9219)  classification_loss: 1.7183 (1.8244)  loss_mask: 0.0724 (0.0974)  time: 0.1630  data: 0.0003  max mem: 4132
[18:02:56.397899] Epoch: [14]  [440/781]  eta: 0:00:56  lr: 0.000244  training_loss: 1.7453 (1.9172)  classification_loss: 1.7044 (1.8207)  loss_mask: 0.0611 (0.0965)  time: 0.1649  data: 0.0002  max mem: 4132
[18:02:59.679871] Epoch: [14]  [460/781]  eta: 0:00:53  lr: 0.000244  training_loss: 1.8357 (1.9143)  classification_loss: 1.7569 (1.8186)  loss_mask: 0.0579 (0.0958)  time: 0.1640  data: 0.0002  max mem: 4132
[18:03:02.959663] Epoch: [14]  [480/781]  eta: 0:00:50  lr: 0.000244  training_loss: 1.8424 (1.9122)  classification_loss: 1.7888 (1.8175)  loss_mask: 0.0560 (0.0947)  time: 0.1638  data: 0.0003  max mem: 4132
[18:03:06.260414] Epoch: [14]  [500/781]  eta: 0:00:46  lr: 0.000244  training_loss: 1.8292 (1.9092)  classification_loss: 1.7232 (1.8147)  loss_mask: 0.0680 (0.0946)  time: 0.1650  data: 0.0002  max mem: 4132
[18:03:09.536503] Epoch: [14]  [520/781]  eta: 0:00:43  lr: 0.000244  training_loss: 1.8884 (1.9097)  classification_loss: 1.7482 (1.8123)  loss_mask: 0.1611 (0.0974)  time: 0.1637  data: 0.0002  max mem: 4132
[18:03:12.821378] Epoch: [14]  [540/781]  eta: 0:00:40  lr: 0.000244  training_loss: 1.8602 (1.9101)  classification_loss: 1.7797 (1.8119)  loss_mask: 0.0720 (0.0983)  time: 0.1642  data: 0.0002  max mem: 4132
[18:03:16.122453] Epoch: [14]  [560/781]  eta: 0:00:36  lr: 0.000244  training_loss: 1.8293 (1.9070)  classification_loss: 1.7530 (1.8099)  loss_mask: 0.0573 (0.0971)  time: 0.1650  data: 0.0003  max mem: 4132
[18:03:19.434580] Epoch: [14]  [580/781]  eta: 0:00:33  lr: 0.000244  training_loss: 1.8715 (1.9081)  classification_loss: 1.7891 (1.8096)  loss_mask: 0.0989 (0.0985)  time: 0.1655  data: 0.0003  max mem: 4132
[18:03:22.748112] Epoch: [14]  [600/781]  eta: 0:00:30  lr: 0.000244  training_loss: 1.8060 (1.9052)  classification_loss: 1.6948 (1.8061)  loss_mask: 0.0846 (0.0991)  time: 0.1656  data: 0.0003  max mem: 4132
[18:03:26.021208] Epoch: [14]  [620/781]  eta: 0:00:26  lr: 0.000244  training_loss: 1.8263 (1.9030)  classification_loss: 1.7605 (1.8047)  loss_mask: 0.0691 (0.0984)  time: 0.1636  data: 0.0002  max mem: 4132
[18:03:29.303409] Epoch: [14]  [640/781]  eta: 0:00:23  lr: 0.000243  training_loss: 1.7822 (1.8991)  classification_loss: 1.7171 (1.8020)  loss_mask: 0.0499 (0.0971)  time: 0.1640  data: 0.0003  max mem: 4132
[18:03:32.611566] Epoch: [14]  [660/781]  eta: 0:00:20  lr: 0.000243  training_loss: 1.7798 (1.8962)  classification_loss: 1.7410 (1.8008)  loss_mask: 0.0388 (0.0954)  time: 0.1653  data: 0.0003  max mem: 4132
[18:03:35.894835] Epoch: [14]  [680/781]  eta: 0:00:16  lr: 0.000243  training_loss: 1.7748 (1.8934)  classification_loss: 1.7291 (1.7997)  loss_mask: 0.0347 (0.0937)  time: 0.1640  data: 0.0003  max mem: 4132
[18:03:39.186491] Epoch: [14]  [700/781]  eta: 0:00:13  lr: 0.000243  training_loss: 1.8025 (1.8908)  classification_loss: 1.7308 (1.7980)  loss_mask: 0.0466 (0.0928)  time: 0.1645  data: 0.0003  max mem: 4132
[18:03:42.470789] Epoch: [14]  [720/781]  eta: 0:00:10  lr: 0.000243  training_loss: 1.8439 (1.8904)  classification_loss: 1.8004 (1.7983)  loss_mask: 0.0630 (0.0921)  time: 0.1641  data: 0.0003  max mem: 4132
[18:03:45.771958] Epoch: [14]  [740/781]  eta: 0:00:06  lr: 0.000243  training_loss: 1.8778 (1.8902)  classification_loss: 1.7626 (1.7973)  loss_mask: 0.0756 (0.0928)  time: 0.1649  data: 0.0004  max mem: 4132
[18:03:49.038237] Epoch: [14]  [760/781]  eta: 0:00:03  lr: 0.000243  training_loss: 1.8904 (1.8901)  classification_loss: 1.7800 (1.7967)  loss_mask: 0.1007 (0.0934)  time: 0.1632  data: 0.0002  max mem: 4132
[18:03:52.308312] Epoch: [14]  [780/781]  eta: 0:00:00  lr: 0.000243  training_loss: 1.7907 (1.8882)  classification_loss: 1.7596 (1.7957)  loss_mask: 0.0559 (0.0925)  time: 0.1634  data: 0.0002  max mem: 4132
[18:03:52.476549] Epoch: [14] Total time: 0:02:09 (0.1659 s / it)
[18:03:52.477050] Averaged stats: lr: 0.000243  training_loss: 1.7907 (1.8882)  classification_loss: 1.7596 (1.7957)  loss_mask: 0.0559 (0.0925)
[18:03:53.196488] Test:  [  0/157]  eta: 0:01:52  testing_loss: 1.0496 (1.0496)  acc1: 64.0625 (64.0625)  acc5: 95.3125 (95.3125)  time: 0.7142  data: 0.6826  max mem: 4132
[18:03:53.494149] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.1872 (1.1750)  acc1: 59.3750 (58.3807)  acc5: 96.8750 (96.7330)  time: 0.0918  data: 0.0622  max mem: 4132
[18:03:53.790150] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.1355 (1.1359)  acc1: 60.9375 (61.3839)  acc5: 96.8750 (97.3214)  time: 0.0295  data: 0.0003  max mem: 4132
[18:03:54.085739] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.1231 (1.1443)  acc1: 64.0625 (61.0887)  acc5: 96.8750 (96.6230)  time: 0.0294  data: 0.0005  max mem: 4132
[18:03:54.374234] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.1461 (1.1429)  acc1: 60.9375 (61.2805)  acc5: 95.3125 (96.2271)  time: 0.0290  data: 0.0004  max mem: 4132
[18:03:54.671613] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.1270 (1.1386)  acc1: 64.0625 (62.1017)  acc5: 96.8750 (96.4767)  time: 0.0290  data: 0.0003  max mem: 4132
[18:03:54.960719] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.1270 (1.1338)  acc1: 62.5000 (62.1414)  acc5: 96.8750 (96.3627)  time: 0.0291  data: 0.0003  max mem: 4132
[18:03:55.251247] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.0839 (1.1298)  acc1: 62.5000 (62.2579)  acc5: 95.3125 (96.3028)  time: 0.0288  data: 0.0002  max mem: 4132
[18:03:55.544026] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.1026 (1.1302)  acc1: 62.5000 (62.2878)  acc5: 95.3125 (96.2963)  time: 0.0290  data: 0.0002  max mem: 4132
[18:03:55.832692] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.1082 (1.1315)  acc1: 59.3750 (62.1051)  acc5: 96.8750 (96.3255)  time: 0.0288  data: 0.0002  max mem: 4132
[18:03:56.126513] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.1643 (1.1353)  acc1: 60.9375 (62.0668)  acc5: 96.8750 (96.4109)  time: 0.0290  data: 0.0002  max mem: 4132
[18:03:56.417609] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.1678 (1.1379)  acc1: 60.9375 (61.9088)  acc5: 96.8750 (96.3823)  time: 0.0291  data: 0.0002  max mem: 4132
[18:03:56.703735] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.1136 (1.1334)  acc1: 62.5000 (62.0868)  acc5: 96.8750 (96.4230)  time: 0.0287  data: 0.0002  max mem: 4132
[18:03:56.993518] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.1164 (1.1353)  acc1: 60.9375 (61.8201)  acc5: 96.8750 (96.4814)  time: 0.0287  data: 0.0002  max mem: 4132
[18:03:57.280614] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.1483 (1.1346)  acc1: 60.9375 (62.0013)  acc5: 96.8750 (96.4871)  time: 0.0287  data: 0.0003  max mem: 4132
[18:03:57.565895] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.1131 (1.1319)  acc1: 62.5000 (62.0447)  acc5: 96.8750 (96.5128)  time: 0.0285  data: 0.0002  max mem: 4132
[18:03:57.722196] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.0952 (1.1327)  acc1: 62.5000 (62.0300)  acc5: 96.8750 (96.5700)  time: 0.0276  data: 0.0002  max mem: 4132
[18:03:57.891742] Test: Total time: 0:00:05 (0.0345 s / it)
[18:03:57.892418] * Acc@1 62.030 Acc@5 96.570 loss 1.133
[18:03:57.892863] Accuracy of the network on the 10000 test images: 62.0%
[18:03:57.893329] Max accuracy: 62.03%
[18:03:58.144959] log_dir: ./output_dir
[18:03:59.194592] Epoch: [15]  [  0/781]  eta: 0:13:37  lr: 0.000243  training_loss: 1.8374 (1.8374)  classification_loss: 1.8121 (1.8121)  loss_mask: 0.0253 (0.0253)  time: 1.0474  data: 0.8289  max mem: 4132
[18:04:02.524117] Epoch: [15]  [ 20/781]  eta: 0:02:38  lr: 0.000243  training_loss: 1.8307 (1.8453)  classification_loss: 1.7334 (1.7487)  loss_mask: 0.0907 (0.0965)  time: 0.1664  data: 0.0003  max mem: 4132
[18:04:05.853039] Epoch: [15]  [ 40/781]  eta: 0:02:19  lr: 0.000243  training_loss: 1.8177 (1.8318)  classification_loss: 1.7268 (1.7455)  loss_mask: 0.0526 (0.0863)  time: 0.1663  data: 0.0003  max mem: 4132
[18:04:09.158899] Epoch: [15]  [ 60/781]  eta: 0:02:10  lr: 0.000243  training_loss: 1.8828 (1.8483)  classification_loss: 1.7547 (1.7532)  loss_mask: 0.0719 (0.0952)  time: 0.1652  data: 0.0003  max mem: 4132
[18:04:12.469790] Epoch: [15]  [ 80/781]  eta: 0:02:03  lr: 0.000243  training_loss: 1.8428 (1.8413)  classification_loss: 1.7685 (1.7567)  loss_mask: 0.0516 (0.0846)  time: 0.1655  data: 0.0003  max mem: 4132
[18:04:15.773306] Epoch: [15]  [100/781]  eta: 0:01:58  lr: 0.000243  training_loss: 1.8300 (1.8473)  classification_loss: 1.7577 (1.7580)  loss_mask: 0.0490 (0.0892)  time: 0.1651  data: 0.0003  max mem: 4132
[18:04:19.048984] Epoch: [15]  [120/781]  eta: 0:01:54  lr: 0.000243  training_loss: 1.8431 (1.8478)  classification_loss: 1.7625 (1.7560)  loss_mask: 0.0857 (0.0918)  time: 0.1637  data: 0.0002  max mem: 4132
[18:04:22.305435] Epoch: [15]  [140/781]  eta: 0:01:49  lr: 0.000243  training_loss: 1.8482 (1.8444)  classification_loss: 1.7662 (1.7541)  loss_mask: 0.0692 (0.0903)  time: 0.1627  data: 0.0002  max mem: 4132
[18:04:25.586182] Epoch: [15]  [160/781]  eta: 0:01:45  lr: 0.000243  training_loss: 1.7971 (1.8414)  classification_loss: 1.7592 (1.7554)  loss_mask: 0.0545 (0.0860)  time: 0.1639  data: 0.0002  max mem: 4132
[18:04:28.858468] Epoch: [15]  [180/781]  eta: 0:01:41  lr: 0.000243  training_loss: 1.7582 (1.8353)  classification_loss: 1.6981 (1.7525)  loss_mask: 0.0558 (0.0829)  time: 0.1635  data: 0.0003  max mem: 4132
[18:04:32.135454] Epoch: [15]  [200/781]  eta: 0:01:38  lr: 0.000243  training_loss: 1.7867 (1.8299)  classification_loss: 1.7275 (1.7502)  loss_mask: 0.0543 (0.0797)  time: 0.1638  data: 0.0003  max mem: 4132
[18:04:35.409743] Epoch: [15]  [220/781]  eta: 0:01:34  lr: 0.000243  training_loss: 1.8192 (1.8285)  classification_loss: 1.7718 (1.7512)  loss_mask: 0.0471 (0.0773)  time: 0.1636  data: 0.0002  max mem: 4132
[18:04:38.712286] Epoch: [15]  [240/781]  eta: 0:01:31  lr: 0.000243  training_loss: 1.7776 (1.8280)  classification_loss: 1.7171 (1.7524)  loss_mask: 0.0473 (0.0756)  time: 0.1650  data: 0.0003  max mem: 4132
[18:04:42.004808] Epoch: [15]  [260/781]  eta: 0:01:27  lr: 0.000243  training_loss: 1.7796 (1.8269)  classification_loss: 1.7084 (1.7500)  loss_mask: 0.0702 (0.0769)  time: 0.1645  data: 0.0003  max mem: 4132
[18:04:45.281248] Epoch: [15]  [280/781]  eta: 0:01:23  lr: 0.000243  training_loss: 1.8478 (1.8292)  classification_loss: 1.7475 (1.7514)  loss_mask: 0.0787 (0.0778)  time: 0.1637  data: 0.0003  max mem: 4132
[18:04:48.569070] Epoch: [15]  [300/781]  eta: 0:01:20  lr: 0.000243  training_loss: 1.8197 (1.8291)  classification_loss: 1.7132 (1.7501)  loss_mask: 0.0688 (0.0790)  time: 0.1643  data: 0.0003  max mem: 4132
[18:04:51.855434] Epoch: [15]  [320/781]  eta: 0:01:17  lr: 0.000243  training_loss: 1.7973 (1.8283)  classification_loss: 1.7719 (1.7516)  loss_mask: 0.0337 (0.0767)  time: 0.1642  data: 0.0002  max mem: 4132
[18:04:55.141794] Epoch: [15]  [340/781]  eta: 0:01:13  lr: 0.000243  training_loss: 1.7813 (1.8246)  classification_loss: 1.7327 (1.7500)  loss_mask: 0.0314 (0.0746)  time: 0.1642  data: 0.0003  max mem: 4132
[18:04:58.421474] Epoch: [15]  [360/781]  eta: 0:01:10  lr: 0.000243  training_loss: 1.8176 (1.8262)  classification_loss: 1.7811 (1.7522)  loss_mask: 0.0415 (0.0740)  time: 0.1639  data: 0.0003  max mem: 4132
[18:05:01.698068] Epoch: [15]  [380/781]  eta: 0:01:06  lr: 0.000243  training_loss: 1.8384 (1.8264)  classification_loss: 1.7446 (1.7527)  loss_mask: 0.0610 (0.0737)  time: 0.1637  data: 0.0003  max mem: 4132
[18:05:04.979155] Epoch: [15]  [400/781]  eta: 0:01:03  lr: 0.000243  training_loss: 1.8311 (1.8263)  classification_loss: 1.7614 (1.7534)  loss_mask: 0.0423 (0.0729)  time: 0.1640  data: 0.0002  max mem: 4132
[18:05:08.281413] Epoch: [15]  [420/781]  eta: 0:01:00  lr: 0.000243  training_loss: 1.7516 (1.8237)  classification_loss: 1.7148 (1.7524)  loss_mask: 0.0373 (0.0714)  time: 0.1650  data: 0.0003  max mem: 4132
[18:05:11.579431] Epoch: [15]  [440/781]  eta: 0:00:56  lr: 0.000242  training_loss: 1.7920 (1.8231)  classification_loss: 1.7173 (1.7522)  loss_mask: 0.0537 (0.0709)  time: 0.1648  data: 0.0002  max mem: 4132
[18:05:14.875709] Epoch: [15]  [460/781]  eta: 0:00:53  lr: 0.000242  training_loss: 1.7239 (1.8202)  classification_loss: 1.6988 (1.7503)  loss_mask: 0.0357 (0.0699)  time: 0.1647  data: 0.0004  max mem: 4132
[18:05:18.164731] Epoch: [15]  [480/781]  eta: 0:00:50  lr: 0.000242  training_loss: 1.8166 (1.8193)  classification_loss: 1.7536 (1.7506)  loss_mask: 0.0300 (0.0687)  time: 0.1644  data: 0.0004  max mem: 4132
[18:05:21.474972] Epoch: [15]  [500/781]  eta: 0:00:46  lr: 0.000242  training_loss: 1.8054 (1.8178)  classification_loss: 1.7236 (1.7501)  loss_mask: 0.0342 (0.0677)  time: 0.1654  data: 0.0002  max mem: 4132
[18:05:24.771039] Epoch: [15]  [520/781]  eta: 0:00:43  lr: 0.000242  training_loss: 1.7778 (1.8166)  classification_loss: 1.7293 (1.7492)  loss_mask: 0.0438 (0.0674)  time: 0.1647  data: 0.0003  max mem: 4132
[18:05:28.025076] Epoch: [15]  [540/781]  eta: 0:00:40  lr: 0.000242  training_loss: 1.8835 (1.8191)  classification_loss: 1.7763 (1.7508)  loss_mask: 0.0544 (0.0683)  time: 0.1626  data: 0.0003  max mem: 4132
[18:05:31.275236] Epoch: [15]  [560/781]  eta: 0:00:36  lr: 0.000242  training_loss: 1.7403 (1.8178)  classification_loss: 1.7225 (1.7508)  loss_mask: 0.0322 (0.0671)  time: 0.1624  data: 0.0002  max mem: 4132
[18:05:34.513151] Epoch: [15]  [580/781]  eta: 0:00:33  lr: 0.000242  training_loss: 1.8587 (1.8201)  classification_loss: 1.7228 (1.7501)  loss_mask: 0.1222 (0.0700)  time: 0.1618  data: 0.0002  max mem: 4132
[18:05:37.814134] Epoch: [15]  [600/781]  eta: 0:00:29  lr: 0.000242  training_loss: 1.7674 (1.8187)  classification_loss: 1.7282 (1.7489)  loss_mask: 0.0541 (0.0698)  time: 0.1650  data: 0.0003  max mem: 4132
[18:05:41.126905] Epoch: [15]  [620/781]  eta: 0:00:26  lr: 0.000242  training_loss: 1.7946 (1.8176)  classification_loss: 1.7599 (1.7485)  loss_mask: 0.0440 (0.0691)  time: 0.1655  data: 0.0003  max mem: 4132
[18:05:44.486707] Epoch: [15]  [640/781]  eta: 0:00:23  lr: 0.000242  training_loss: 1.8183 (1.8184)  classification_loss: 1.7533 (1.7486)  loss_mask: 0.0516 (0.0698)  time: 0.1678  data: 0.0003  max mem: 4132
[18:05:47.810080] Epoch: [15]  [660/781]  eta: 0:00:20  lr: 0.000242  training_loss: 1.8195 (1.8184)  classification_loss: 1.7590 (1.7485)  loss_mask: 0.0678 (0.0699)  time: 0.1661  data: 0.0003  max mem: 4132
[18:05:51.123079] Epoch: [15]  [680/781]  eta: 0:00:16  lr: 0.000242  training_loss: 1.8136 (1.8184)  classification_loss: 1.7307 (1.7482)  loss_mask: 0.0699 (0.0702)  time: 0.1655  data: 0.0003  max mem: 4132
[18:05:54.488655] Epoch: [15]  [700/781]  eta: 0:00:13  lr: 0.000242  training_loss: 1.8347 (1.8183)  classification_loss: 1.7503 (1.7482)  loss_mask: 0.0630 (0.0702)  time: 0.1682  data: 0.0006  max mem: 4132
[18:05:57.784883] Epoch: [15]  [720/781]  eta: 0:00:10  lr: 0.000242  training_loss: 1.8464 (1.8194)  classification_loss: 1.7425 (1.7481)  loss_mask: 0.0654 (0.0713)  time: 0.1647  data: 0.0003  max mem: 4132
[18:06:01.062249] Epoch: [15]  [740/781]  eta: 0:00:06  lr: 0.000242  training_loss: 1.7964 (1.8189)  classification_loss: 1.7229 (1.7480)  loss_mask: 0.0453 (0.0709)  time: 0.1637  data: 0.0003  max mem: 4132
[18:06:04.354940] Epoch: [15]  [760/781]  eta: 0:00:03  lr: 0.000242  training_loss: 1.7619 (1.8185)  classification_loss: 1.7258 (1.7481)  loss_mask: 0.0384 (0.0703)  time: 0.1645  data: 0.0002  max mem: 4132
[18:06:07.636207] Epoch: [15]  [780/781]  eta: 0:00:00  lr: 0.000242  training_loss: 1.7916 (1.8179)  classification_loss: 1.7345 (1.7482)  loss_mask: 0.0364 (0.0697)  time: 0.1640  data: 0.0002  max mem: 4132
[18:06:07.797690] Epoch: [15] Total time: 0:02:09 (0.1660 s / it)
[18:06:07.798176] Averaged stats: lr: 0.000242  training_loss: 1.7916 (1.8179)  classification_loss: 1.7345 (1.7482)  loss_mask: 0.0364 (0.0697)
[18:06:08.529127] Test:  [  0/157]  eta: 0:01:53  testing_loss: 0.9728 (0.9728)  acc1: 67.1875 (67.1875)  acc5: 95.3125 (95.3125)  time: 0.7225  data: 0.6811  max mem: 4132
[18:06:08.827485] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.1033 (1.1400)  acc1: 57.8125 (58.3807)  acc5: 96.8750 (96.7330)  time: 0.0924  data: 0.0621  max mem: 4132
[18:06:09.114610] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.0885 (1.0977)  acc1: 64.0625 (62.0536)  acc5: 96.8750 (97.3214)  time: 0.0290  data: 0.0002  max mem: 4132
[18:06:09.401878] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.0768 (1.1027)  acc1: 64.0625 (62.2480)  acc5: 96.8750 (96.6734)  time: 0.0285  data: 0.0002  max mem: 4132
[18:06:09.690399] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.0864 (1.1064)  acc1: 64.0625 (62.5381)  acc5: 95.3125 (96.3796)  time: 0.0286  data: 0.0004  max mem: 4132
[18:06:09.976605] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.0900 (1.1052)  acc1: 62.5000 (62.9902)  acc5: 96.8750 (96.4154)  time: 0.0286  data: 0.0004  max mem: 4132
[18:06:10.261442] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.1047 (1.1042)  acc1: 62.5000 (62.8586)  acc5: 96.8750 (96.3115)  time: 0.0284  data: 0.0002  max mem: 4132
[18:06:10.547093] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.0678 (1.0967)  acc1: 64.0625 (63.2042)  acc5: 96.8750 (96.3688)  time: 0.0284  data: 0.0002  max mem: 4132
[18:06:10.834959] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.0827 (1.0990)  acc1: 64.0625 (63.0787)  acc5: 96.8750 (96.3927)  time: 0.0285  data: 0.0002  max mem: 4132
[18:06:11.121813] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.1194 (1.0995)  acc1: 59.3750 (62.9808)  acc5: 96.8750 (96.4114)  time: 0.0286  data: 0.0002  max mem: 4132
[18:06:11.412117] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.1343 (1.1045)  acc1: 59.3750 (62.7166)  acc5: 96.8750 (96.4882)  time: 0.0287  data: 0.0002  max mem: 4132
[18:06:11.699343] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.1453 (1.1083)  acc1: 57.8125 (62.5282)  acc5: 96.8750 (96.5231)  time: 0.0287  data: 0.0002  max mem: 4132
[18:06:11.986540] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.1094 (1.1038)  acc1: 62.5000 (62.5258)  acc5: 96.8750 (96.5263)  time: 0.0286  data: 0.0003  max mem: 4132
[18:06:12.277062] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.1250 (1.1068)  acc1: 62.5000 (62.4284)  acc5: 96.8750 (96.5410)  time: 0.0287  data: 0.0003  max mem: 4132
[18:06:12.563483] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.1250 (1.1066)  acc1: 64.0625 (62.5111)  acc5: 96.8750 (96.5536)  time: 0.0287  data: 0.0002  max mem: 4132
[18:06:12.852072] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.0624 (1.1029)  acc1: 64.0625 (62.6138)  acc5: 96.8750 (96.5956)  time: 0.0286  data: 0.0002  max mem: 4132
[18:06:13.006423] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.0549 (1.1038)  acc1: 64.0625 (62.5400)  acc5: 96.8750 (96.6200)  time: 0.0277  data: 0.0002  max mem: 4132
[18:06:13.183076] Test: Total time: 0:00:05 (0.0343 s / it)
[18:06:13.183644] * Acc@1 62.540 Acc@5 96.620 loss 1.104
[18:06:13.183990] Accuracy of the network on the 10000 test images: 62.5%
[18:06:13.184161] Max accuracy: 62.54%
[18:06:13.476264] log_dir: ./output_dir
[18:06:14.396658] Epoch: [16]  [  0/781]  eta: 0:11:57  lr: 0.000242  training_loss: 1.7995 (1.7995)  classification_loss: 1.7515 (1.7515)  loss_mask: 0.0480 (0.0480)  time: 0.9183  data: 0.7331  max mem: 4132
[18:06:17.665831] Epoch: [16]  [ 20/781]  eta: 0:02:31  lr: 0.000242  training_loss: 1.7311 (1.7640)  classification_loss: 1.7009 (1.7202)  loss_mask: 0.0328 (0.0438)  time: 0.1634  data: 0.0003  max mem: 4132
[18:06:20.974783] Epoch: [16]  [ 40/781]  eta: 0:02:15  lr: 0.000242  training_loss: 1.8076 (1.8170)  classification_loss: 1.7539 (1.7332)  loss_mask: 0.0788 (0.0838)  time: 0.1654  data: 0.0002  max mem: 4132
[18:06:24.277744] Epoch: [16]  [ 60/781]  eta: 0:02:07  lr: 0.000242  training_loss: 1.8441 (1.8358)  classification_loss: 1.7309 (1.7366)  loss_mask: 0.1099 (0.0992)  time: 0.1650  data: 0.0002  max mem: 4132
[18:06:27.584709] Epoch: [16]  [ 80/781]  eta: 0:02:02  lr: 0.000242  training_loss: 1.8877 (1.8490)  classification_loss: 1.7644 (1.7410)  loss_mask: 0.1165 (0.1079)  time: 0.1653  data: 0.0003  max mem: 4132
[18:06:30.902962] Epoch: [16]  [100/781]  eta: 0:01:57  lr: 0.000242  training_loss: 1.8289 (1.8422)  classification_loss: 1.7460 (1.7395)  loss_mask: 0.0740 (0.1027)  time: 0.1658  data: 0.0002  max mem: 4132
[18:06:34.257330] Epoch: [16]  [120/781]  eta: 0:01:53  lr: 0.000242  training_loss: 1.8211 (1.8389)  classification_loss: 1.7233 (1.7382)  loss_mask: 0.0762 (0.1006)  time: 0.1676  data: 0.0003  max mem: 4132
[18:06:37.566706] Epoch: [16]  [140/781]  eta: 0:01:49  lr: 0.000242  training_loss: 1.7457 (1.8299)  classification_loss: 1.7007 (1.7313)  loss_mask: 0.0800 (0.0985)  time: 0.1653  data: 0.0005  max mem: 4132
[18:06:40.877918] Epoch: [16]  [160/781]  eta: 0:01:45  lr: 0.000242  training_loss: 1.8027 (1.8279)  classification_loss: 1.7336 (1.7348)  loss_mask: 0.0497 (0.0931)  time: 0.1655  data: 0.0002  max mem: 4132
[18:06:44.149194] Epoch: [16]  [180/781]  eta: 0:01:41  lr: 0.000242  training_loss: 1.7432 (1.8178)  classification_loss: 1.7058 (1.7301)  loss_mask: 0.0385 (0.0877)  time: 0.1635  data: 0.0003  max mem: 4132
[18:06:47.445238] Epoch: [16]  [200/781]  eta: 0:01:38  lr: 0.000241  training_loss: 1.7749 (1.8165)  classification_loss: 1.7339 (1.7339)  loss_mask: 0.0355 (0.0827)  time: 0.1647  data: 0.0003  max mem: 4132
[18:06:50.751941] Epoch: [16]  [220/781]  eta: 0:01:34  lr: 0.000241  training_loss: 1.7848 (1.8128)  classification_loss: 1.7310 (1.7339)  loss_mask: 0.0318 (0.0789)  time: 0.1653  data: 0.0004  max mem: 4132
[18:06:54.023497] Epoch: [16]  [240/781]  eta: 0:01:30  lr: 0.000241  training_loss: 1.8355 (1.8130)  classification_loss: 1.7022 (1.7330)  loss_mask: 0.0697 (0.0800)  time: 0.1635  data: 0.0003  max mem: 4132
[18:06:57.295905] Epoch: [16]  [260/781]  eta: 0:01:27  lr: 0.000241  training_loss: 1.9354 (1.8257)  classification_loss: 1.7365 (1.7332)  loss_mask: 0.1811 (0.0925)  time: 0.1635  data: 0.0002  max mem: 4132
[18:07:00.568777] Epoch: [16]  [280/781]  eta: 0:01:23  lr: 0.000241  training_loss: 1.8649 (1.8323)  classification_loss: 1.7337 (1.7350)  loss_mask: 0.1047 (0.0973)  time: 0.1636  data: 0.0002  max mem: 4132
[18:07:03.880047] Epoch: [16]  [300/781]  eta: 0:01:20  lr: 0.000241  training_loss: 1.8228 (1.8327)  classification_loss: 1.7607 (1.7374)  loss_mask: 0.0527 (0.0954)  time: 0.1655  data: 0.0003  max mem: 4132
[18:07:07.150782] Epoch: [16]  [320/781]  eta: 0:01:17  lr: 0.000241  training_loss: 1.8390 (1.8334)  classification_loss: 1.7358 (1.7387)  loss_mask: 0.0539 (0.0947)  time: 0.1634  data: 0.0002  max mem: 4132
[18:07:10.405157] Epoch: [16]  [340/781]  eta: 0:01:13  lr: 0.000241  training_loss: 1.7770 (1.8303)  classification_loss: 1.6980 (1.7373)  loss_mask: 0.0626 (0.0930)  time: 0.1626  data: 0.0004  max mem: 4132
[18:07:13.706410] Epoch: [16]  [360/781]  eta: 0:01:10  lr: 0.000241  training_loss: 1.7937 (1.8303)  classification_loss: 1.7247 (1.7388)  loss_mask: 0.0673 (0.0915)  time: 0.1650  data: 0.0002  max mem: 4132
[18:07:16.997706] Epoch: [16]  [380/781]  eta: 0:01:06  lr: 0.000241  training_loss: 1.7587 (1.8291)  classification_loss: 1.7321 (1.7400)  loss_mask: 0.0438 (0.0892)  time: 0.1645  data: 0.0003  max mem: 4132
[18:07:20.329196] Epoch: [16]  [400/781]  eta: 0:01:03  lr: 0.000241  training_loss: 1.8067 (1.8279)  classification_loss: 1.7267 (1.7402)  loss_mask: 0.0516 (0.0877)  time: 0.1665  data: 0.0003  max mem: 4132
[18:07:23.649208] Epoch: [16]  [420/781]  eta: 0:01:00  lr: 0.000241  training_loss: 1.8401 (1.8282)  classification_loss: 1.7347 (1.7410)  loss_mask: 0.0625 (0.0872)  time: 0.1659  data: 0.0003  max mem: 4132
[18:07:26.998368] Epoch: [16]  [440/781]  eta: 0:00:56  lr: 0.000241  training_loss: 1.7751 (1.8273)  classification_loss: 1.7225 (1.7403)  loss_mask: 0.0563 (0.0870)  time: 0.1674  data: 0.0003  max mem: 4132
[18:07:30.290072] Epoch: [16]  [460/781]  eta: 0:00:53  lr: 0.000241  training_loss: 1.7663 (1.8261)  classification_loss: 1.6833 (1.7377)  loss_mask: 0.0649 (0.0884)  time: 0.1645  data: 0.0002  max mem: 4132
[18:07:33.598046] Epoch: [16]  [480/781]  eta: 0:00:50  lr: 0.000241  training_loss: 1.8252 (1.8256)  classification_loss: 1.7615 (1.7383)  loss_mask: 0.0508 (0.0873)  time: 0.1653  data: 0.0003  max mem: 4132
[18:07:36.917222] Epoch: [16]  [500/781]  eta: 0:00:46  lr: 0.000241  training_loss: 1.7352 (1.8235)  classification_loss: 1.6814 (1.7370)  loss_mask: 0.0613 (0.0865)  time: 0.1659  data: 0.0003  max mem: 4132
[18:07:40.220063] Epoch: [16]  [520/781]  eta: 0:00:43  lr: 0.000241  training_loss: 1.7778 (1.8219)  classification_loss: 1.6980 (1.7367)  loss_mask: 0.0469 (0.0853)  time: 0.1650  data: 0.0003  max mem: 4132
[18:07:43.490070] Epoch: [16]  [540/781]  eta: 0:00:40  lr: 0.000241  training_loss: 1.7374 (1.8199)  classification_loss: 1.7182 (1.7363)  loss_mask: 0.0369 (0.0836)  time: 0.1634  data: 0.0002  max mem: 4132
[18:07:46.761305] Epoch: [16]  [560/781]  eta: 0:00:36  lr: 0.000241  training_loss: 1.7645 (1.8185)  classification_loss: 1.6913 (1.7360)  loss_mask: 0.0468 (0.0825)  time: 0.1635  data: 0.0002  max mem: 4132
[18:07:50.042464] Epoch: [16]  [580/781]  eta: 0:00:33  lr: 0.000241  training_loss: 1.8007 (1.8180)  classification_loss: 1.7639 (1.7367)  loss_mask: 0.0486 (0.0813)  time: 0.1640  data: 0.0003  max mem: 4132
[18:07:53.304655] Epoch: [16]  [600/781]  eta: 0:00:30  lr: 0.000241  training_loss: 1.8090 (1.8171)  classification_loss: 1.7467 (1.7367)  loss_mask: 0.0497 (0.0803)  time: 0.1630  data: 0.0003  max mem: 4132
[18:07:56.577468] Epoch: [16]  [620/781]  eta: 0:00:26  lr: 0.000241  training_loss: 1.7500 (1.8155)  classification_loss: 1.7065 (1.7359)  loss_mask: 0.0442 (0.0796)  time: 0.1636  data: 0.0002  max mem: 4132
[18:07:59.840181] Epoch: [16]  [640/781]  eta: 0:00:23  lr: 0.000241  training_loss: 1.7708 (1.8142)  classification_loss: 1.7331 (1.7359)  loss_mask: 0.0305 (0.0783)  time: 0.1630  data: 0.0003  max mem: 4132
[18:08:03.139813] Epoch: [16]  [660/781]  eta: 0:00:20  lr: 0.000241  training_loss: 1.7834 (1.8137)  classification_loss: 1.7279 (1.7360)  loss_mask: 0.0408 (0.0777)  time: 0.1649  data: 0.0003  max mem: 4132
[18:08:06.449807] Epoch: [16]  [680/781]  eta: 0:00:16  lr: 0.000241  training_loss: 1.7643 (1.8118)  classification_loss: 1.7222 (1.7351)  loss_mask: 0.0405 (0.0768)  time: 0.1654  data: 0.0003  max mem: 4132
[18:08:09.770447] Epoch: [16]  [700/781]  eta: 0:00:13  lr: 0.000240  training_loss: 1.7421 (1.8106)  classification_loss: 1.7076 (1.7340)  loss_mask: 0.0588 (0.0766)  time: 0.1659  data: 0.0003  max mem: 4132
[18:08:13.063505] Epoch: [16]  [720/781]  eta: 0:00:10  lr: 0.000240  training_loss: 1.7362 (1.8092)  classification_loss: 1.6911 (1.7332)  loss_mask: 0.0470 (0.0760)  time: 0.1646  data: 0.0003  max mem: 4132
[18:08:16.353900] Epoch: [16]  [740/781]  eta: 0:00:06  lr: 0.000240  training_loss: 1.7753 (1.8077)  classification_loss: 1.7124 (1.7321)  loss_mask: 0.0421 (0.0757)  time: 0.1644  data: 0.0003  max mem: 4132
[18:08:19.622976] Epoch: [16]  [760/781]  eta: 0:00:03  lr: 0.000240  training_loss: 1.7578 (1.8066)  classification_loss: 1.7273 (1.7321)  loss_mask: 0.0296 (0.0745)  time: 0.1634  data: 0.0003  max mem: 4132
[18:08:22.881076] Epoch: [16]  [780/781]  eta: 0:00:00  lr: 0.000240  training_loss: 1.7965 (1.8064)  classification_loss: 1.7297 (1.7323)  loss_mask: 0.0515 (0.0742)  time: 0.1628  data: 0.0002  max mem: 4132
[18:08:23.059836] Epoch: [16] Total time: 0:02:09 (0.1659 s / it)
[18:08:23.060909] Averaged stats: lr: 0.000240  training_loss: 1.7965 (1.8064)  classification_loss: 1.7297 (1.7323)  loss_mask: 0.0515 (0.0742)
[18:08:23.771800] Test:  [  0/157]  eta: 0:01:50  testing_loss: 0.9321 (0.9321)  acc1: 70.3125 (70.3125)  acc5: 96.8750 (96.8750)  time: 0.7041  data: 0.6667  max mem: 4132
[18:08:24.063253] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.0619 (1.0635)  acc1: 64.0625 (62.0739)  acc5: 96.8750 (97.5852)  time: 0.0902  data: 0.0608  max mem: 4132
[18:08:24.351118] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.0382 (1.0419)  acc1: 64.0625 (64.5833)  acc5: 98.4375 (97.7679)  time: 0.0287  data: 0.0002  max mem: 4132
[18:08:24.648815] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.0686 (1.0643)  acc1: 62.5000 (63.4073)  acc5: 96.8750 (97.0262)  time: 0.0291  data: 0.0002  max mem: 4132
[18:08:24.940222] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.0768 (1.0701)  acc1: 62.5000 (62.9573)  acc5: 95.3125 (96.9893)  time: 0.0293  data: 0.0002  max mem: 4132
[18:08:25.229133] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.0665 (1.0663)  acc1: 64.0625 (63.6336)  acc5: 96.8750 (96.9669)  time: 0.0289  data: 0.0002  max mem: 4132
[18:08:25.515592] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.0484 (1.0658)  acc1: 64.0625 (63.4734)  acc5: 96.8750 (96.7982)  time: 0.0286  data: 0.0002  max mem: 4132
[18:08:25.801602] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.0329 (1.0569)  acc1: 65.6250 (63.7764)  acc5: 96.8750 (97.0290)  time: 0.0285  data: 0.0002  max mem: 4132
[18:08:26.093059] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.0210 (1.0590)  acc1: 64.0625 (63.7153)  acc5: 98.4375 (97.0100)  time: 0.0287  data: 0.0003  max mem: 4132
[18:08:26.384458] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.0654 (1.0600)  acc1: 64.0625 (63.7534)  acc5: 96.8750 (97.0810)  time: 0.0290  data: 0.0002  max mem: 4132
[18:08:26.674973] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.0776 (1.0642)  acc1: 62.5000 (63.6139)  acc5: 98.4375 (97.0916)  time: 0.0290  data: 0.0002  max mem: 4132
[18:08:26.964737] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.1279 (1.0661)  acc1: 60.9375 (63.5698)  acc5: 96.8750 (97.1143)  time: 0.0289  data: 0.0002  max mem: 4132
[18:08:27.255069] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.0828 (1.0622)  acc1: 62.5000 (63.7526)  acc5: 98.4375 (97.1333)  time: 0.0289  data: 0.0002  max mem: 4132
[18:08:27.545276] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.0587 (1.0635)  acc1: 62.5000 (63.6927)  acc5: 96.8750 (97.1016)  time: 0.0289  data: 0.0002  max mem: 4132
[18:08:27.831729] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.1070 (1.0637)  acc1: 62.5000 (63.6857)  acc5: 96.8750 (97.0966)  time: 0.0287  data: 0.0002  max mem: 4132
[18:08:28.115348] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.0325 (1.0608)  acc1: 64.0625 (63.9487)  acc5: 96.8750 (97.0302)  time: 0.0284  data: 0.0002  max mem: 4132
[18:08:28.268325] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.0285 (1.0615)  acc1: 65.6250 (63.9200)  acc5: 96.8750 (97.0600)  time: 0.0273  data: 0.0001  max mem: 4132
[18:08:28.436780] Test: Total time: 0:00:05 (0.0342 s / it)
[18:08:28.437321] * Acc@1 63.920 Acc@5 97.060 loss 1.062
[18:08:28.437667] Accuracy of the network on the 10000 test images: 63.9%
[18:08:28.437876] Max accuracy: 63.92%
[18:08:28.723145] log_dir: ./output_dir
[18:08:29.599747] Epoch: [17]  [  0/781]  eta: 0:11:23  lr: 0.000240  training_loss: 1.6360 (1.6360)  classification_loss: 1.5940 (1.5940)  loss_mask: 0.0420 (0.0420)  time: 0.8748  data: 0.6859  max mem: 4132
[18:08:32.859478] Epoch: [17]  [ 20/781]  eta: 0:02:29  lr: 0.000240  training_loss: 1.7682 (1.7636)  classification_loss: 1.6860 (1.7090)  loss_mask: 0.0412 (0.0546)  time: 0.1628  data: 0.0002  max mem: 4132
[18:08:36.147999] Epoch: [17]  [ 40/781]  eta: 0:02:14  lr: 0.000240  training_loss: 1.7774 (1.7784)  classification_loss: 1.7383 (1.7244)  loss_mask: 0.0418 (0.0540)  time: 0.1643  data: 0.0002  max mem: 4132
[18:08:39.419518] Epoch: [17]  [ 60/781]  eta: 0:02:06  lr: 0.000240  training_loss: 1.7761 (1.7844)  classification_loss: 1.7175 (1.7301)  loss_mask: 0.0432 (0.0544)  time: 0.1635  data: 0.0002  max mem: 4132
[18:08:42.740467] Epoch: [17]  [ 80/781]  eta: 0:02:01  lr: 0.000240  training_loss: 1.7435 (1.7864)  classification_loss: 1.7112 (1.7275)  loss_mask: 0.0609 (0.0589)  time: 0.1660  data: 0.0003  max mem: 4132
[18:08:46.002398] Epoch: [17]  [100/781]  eta: 0:01:56  lr: 0.000240  training_loss: 1.8101 (1.7899)  classification_loss: 1.7196 (1.7258)  loss_mask: 0.0513 (0.0640)  time: 0.1630  data: 0.0002  max mem: 4132
[18:08:49.267254] Epoch: [17]  [120/781]  eta: 0:01:52  lr: 0.000240  training_loss: 1.8224 (1.7991)  classification_loss: 1.7465 (1.7281)  loss_mask: 0.0695 (0.0710)  time: 0.1632  data: 0.0002  max mem: 4132
[18:08:52.533295] Epoch: [17]  [140/781]  eta: 0:01:48  lr: 0.000240  training_loss: 1.8145 (1.8014)  classification_loss: 1.6810 (1.7239)  loss_mask: 0.0860 (0.0775)  time: 0.1632  data: 0.0002  max mem: 4132
[18:08:55.868617] Epoch: [17]  [160/781]  eta: 0:01:44  lr: 0.000240  training_loss: 1.7932 (1.8032)  classification_loss: 1.7066 (1.7245)  loss_mask: 0.0910 (0.0787)  time: 0.1667  data: 0.0003  max mem: 4132
[18:08:59.170762] Epoch: [17]  [180/781]  eta: 0:01:41  lr: 0.000240  training_loss: 1.7428 (1.7980)  classification_loss: 1.6694 (1.7202)  loss_mask: 0.0485 (0.0777)  time: 0.1650  data: 0.0002  max mem: 4132
[18:09:02.506619] Epoch: [17]  [200/781]  eta: 0:01:37  lr: 0.000240  training_loss: 1.7665 (1.7962)  classification_loss: 1.6966 (1.7201)  loss_mask: 0.0358 (0.0761)  time: 0.1667  data: 0.0002  max mem: 4132
[18:09:05.806807] Epoch: [17]  [220/781]  eta: 0:01:34  lr: 0.000240  training_loss: 1.7597 (1.7952)  classification_loss: 1.7149 (1.7199)  loss_mask: 0.0582 (0.0753)  time: 0.1649  data: 0.0003  max mem: 4132
[18:09:09.096519] Epoch: [17]  [240/781]  eta: 0:01:30  lr: 0.000240  training_loss: 1.7866 (1.7953)  classification_loss: 1.7485 (1.7216)  loss_mask: 0.0494 (0.0737)  time: 0.1644  data: 0.0002  max mem: 4132
[18:09:12.429408] Epoch: [17]  [260/781]  eta: 0:01:27  lr: 0.000240  training_loss: 1.8055 (1.7956)  classification_loss: 1.7244 (1.7202)  loss_mask: 0.0610 (0.0754)  time: 0.1666  data: 0.0003  max mem: 4132
[18:09:15.720944] Epoch: [17]  [280/781]  eta: 0:01:23  lr: 0.000240  training_loss: 1.8977 (1.8054)  classification_loss: 1.7320 (1.7231)  loss_mask: 0.1066 (0.0823)  time: 0.1645  data: 0.0003  max mem: 4132
[18:09:19.036406] Epoch: [17]  [300/781]  eta: 0:01:20  lr: 0.000240  training_loss: 1.7874 (1.8041)  classification_loss: 1.7012 (1.7228)  loss_mask: 0.0616 (0.0813)  time: 0.1657  data: 0.0004  max mem: 4132
[18:09:22.321234] Epoch: [17]  [320/781]  eta: 0:01:16  lr: 0.000240  training_loss: 1.7411 (1.8023)  classification_loss: 1.6973 (1.7222)  loss_mask: 0.0613 (0.0802)  time: 0.1641  data: 0.0002  max mem: 4132
[18:09:25.636711] Epoch: [17]  [340/781]  eta: 0:01:13  lr: 0.000240  training_loss: 1.7496 (1.8012)  classification_loss: 1.7125 (1.7220)  loss_mask: 0.0457 (0.0792)  time: 0.1656  data: 0.0003  max mem: 4132
[18:09:28.934380] Epoch: [17]  [360/781]  eta: 0:01:10  lr: 0.000240  training_loss: 1.9382 (1.8137)  classification_loss: 1.8256 (1.7297)  loss_mask: 0.0936 (0.0839)  time: 0.1648  data: 0.0003  max mem: 4132
[18:09:32.200127] Epoch: [17]  [380/781]  eta: 0:01:06  lr: 0.000240  training_loss: 2.2725 (1.8405)  classification_loss: 2.1002 (1.7495)  loss_mask: 0.1480 (0.0910)  time: 0.1632  data: 0.0002  max mem: 4132
[18:09:35.485823] Epoch: [17]  [400/781]  eta: 0:01:03  lr: 0.000239  training_loss: 2.0168 (1.8514)  classification_loss: 1.9045 (1.7579)  loss_mask: 0.1306 (0.0935)  time: 0.1642  data: 0.0003  max mem: 4132
[18:09:38.767109] Epoch: [17]  [420/781]  eta: 0:01:00  lr: 0.000239  training_loss: 2.2772 (1.8739)  classification_loss: 2.0741 (1.7733)  loss_mask: 0.1436 (0.1007)  time: 0.1640  data: 0.0002  max mem: 4132
[18:09:42.036251] Epoch: [17]  [440/781]  eta: 0:00:56  lr: 0.000239  training_loss: 2.2424 (1.8906)  classification_loss: 2.0418 (1.7853)  loss_mask: 0.1775 (0.1053)  time: 0.1634  data: 0.0003  max mem: 4132
[18:09:45.309000] Epoch: [17]  [460/781]  eta: 0:00:53  lr: 0.000239  training_loss: 2.2893 (1.9111)  classification_loss: 2.0927 (1.7983)  loss_mask: 0.2075 (0.1129)  time: 0.1636  data: 0.0003  max mem: 4132
[18:09:48.575831] Epoch: [17]  [480/781]  eta: 0:00:49  lr: 0.000239  training_loss: 2.1539 (1.9223)  classification_loss: 1.9983 (1.8066)  loss_mask: 0.1555 (0.1157)  time: 0.1632  data: 0.0003  max mem: 4132
[18:09:51.862868] Epoch: [17]  [500/781]  eta: 0:00:46  lr: 0.000239  training_loss: 2.0522 (1.9280)  classification_loss: 1.9214 (1.8112)  loss_mask: 0.1247 (0.1168)  time: 0.1643  data: 0.0003  max mem: 4132
[18:09:55.149672] Epoch: [17]  [520/781]  eta: 0:00:43  lr: 0.000239  training_loss: 1.9368 (1.9296)  classification_loss: 1.8490 (1.8130)  loss_mask: 0.0932 (0.1166)  time: 0.1642  data: 0.0003  max mem: 4132
[18:09:58.427737] Epoch: [17]  [540/781]  eta: 0:00:39  lr: 0.000239  training_loss: 1.9144 (1.9301)  classification_loss: 1.8227 (1.8141)  loss_mask: 0.1057 (0.1160)  time: 0.1638  data: 0.0003  max mem: 4132
[18:10:01.746477] Epoch: [17]  [560/781]  eta: 0:00:36  lr: 0.000239  training_loss: 1.9517 (1.9316)  classification_loss: 1.7556 (1.8134)  loss_mask: 0.1461 (0.1182)  time: 0.1659  data: 0.0003  max mem: 4132
[18:10:05.027603] Epoch: [17]  [580/781]  eta: 0:00:33  lr: 0.000239  training_loss: 1.9059 (1.9310)  classification_loss: 1.8025 (1.8128)  loss_mask: 0.0996 (0.1183)  time: 0.1640  data: 0.0004  max mem: 4132
[18:10:08.305809] Epoch: [17]  [600/781]  eta: 0:00:29  lr: 0.000239  training_loss: 1.8078 (1.9270)  classification_loss: 1.7388 (1.8102)  loss_mask: 0.0676 (0.1168)  time: 0.1638  data: 0.0002  max mem: 4132
[18:10:11.614209] Epoch: [17]  [620/781]  eta: 0:00:26  lr: 0.000239  training_loss: 1.7971 (1.9228)  classification_loss: 1.6881 (1.8076)  loss_mask: 0.0539 (0.1151)  time: 0.1653  data: 0.0003  max mem: 4132
[18:10:14.889930] Epoch: [17]  [640/781]  eta: 0:00:23  lr: 0.000239  training_loss: 1.8416 (1.9201)  classification_loss: 1.7424 (1.8060)  loss_mask: 0.0578 (0.1141)  time: 0.1637  data: 0.0002  max mem: 4132
[18:10:18.152888] Epoch: [17]  [660/781]  eta: 0:00:20  lr: 0.000239  training_loss: 1.8919 (1.9201)  classification_loss: 1.8310 (1.8069)  loss_mask: 0.0766 (0.1132)  time: 0.1631  data: 0.0003  max mem: 4132
[18:10:21.420698] Epoch: [17]  [680/781]  eta: 0:00:16  lr: 0.000239  training_loss: 1.8174 (1.9175)  classification_loss: 1.7642 (1.8057)  loss_mask: 0.0599 (0.1118)  time: 0.1633  data: 0.0003  max mem: 4132
[18:10:24.690445] Epoch: [17]  [700/781]  eta: 0:00:13  lr: 0.000239  training_loss: 1.7942 (1.9142)  classification_loss: 1.7420 (1.8037)  loss_mask: 0.0605 (0.1104)  time: 0.1634  data: 0.0002  max mem: 4132
[18:10:27.954038] Epoch: [17]  [720/781]  eta: 0:00:10  lr: 0.000239  training_loss: 1.8002 (1.9113)  classification_loss: 1.7282 (1.8020)  loss_mask: 0.0668 (0.1093)  time: 0.1631  data: 0.0003  max mem: 4132
[18:10:31.255480] Epoch: [17]  [740/781]  eta: 0:00:06  lr: 0.000239  training_loss: 1.7590 (1.9088)  classification_loss: 1.6801 (1.7996)  loss_mask: 0.0945 (0.1092)  time: 0.1649  data: 0.0003  max mem: 4132
[18:10:34.574028] Epoch: [17]  [760/781]  eta: 0:00:03  lr: 0.000239  training_loss: 1.8468 (1.9069)  classification_loss: 1.7359 (1.7983)  loss_mask: 0.0849 (0.1086)  time: 0.1658  data: 0.0003  max mem: 4132
[18:10:37.844952] Epoch: [17]  [780/781]  eta: 0:00:00  lr: 0.000239  training_loss: 1.7978 (1.9046)  classification_loss: 1.7348 (1.7968)  loss_mask: 0.0630 (0.1077)  time: 0.1635  data: 0.0002  max mem: 4132
[18:10:38.039270] Epoch: [17] Total time: 0:02:09 (0.1656 s / it)
[18:10:38.040051] Averaged stats: lr: 0.000239  training_loss: 1.7978 (1.9046)  classification_loss: 1.7348 (1.7968)  loss_mask: 0.0630 (0.1077)
[18:10:38.790057] Test:  [  0/157]  eta: 0:01:56  testing_loss: 0.9720 (0.9720)  acc1: 71.8750 (71.8750)  acc5: 96.8750 (96.8750)  time: 0.7451  data: 0.7133  max mem: 4132
[18:10:39.085067] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.1148 (1.1154)  acc1: 60.9375 (61.9318)  acc5: 96.8750 (96.7330)  time: 0.0942  data: 0.0652  max mem: 4132
[18:10:39.382101] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.0803 (1.0765)  acc1: 62.5000 (63.0952)  acc5: 96.8750 (97.0982)  time: 0.0293  data: 0.0003  max mem: 4132
[18:10:39.677185] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.0609 (1.0790)  acc1: 64.0625 (63.2560)  acc5: 96.8750 (96.4718)  time: 0.0294  data: 0.0003  max mem: 4132
[18:10:39.966711] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.0744 (1.0838)  acc1: 62.5000 (63.0716)  acc5: 95.3125 (96.5320)  time: 0.0291  data: 0.0003  max mem: 4132
[18:10:40.253207] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.0816 (1.0866)  acc1: 62.5000 (63.1740)  acc5: 96.8750 (96.3848)  time: 0.0287  data: 0.0002  max mem: 4132
[18:10:40.548771] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.1022 (1.0857)  acc1: 62.5000 (62.8330)  acc5: 95.3125 (96.2602)  time: 0.0290  data: 0.0003  max mem: 4132
[18:10:40.837397] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.0129 (1.0761)  acc1: 64.0625 (63.0722)  acc5: 96.8750 (96.4789)  time: 0.0291  data: 0.0003  max mem: 4132
[18:10:41.126548] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.0373 (1.0787)  acc1: 65.6250 (63.0787)  acc5: 96.8750 (96.5856)  time: 0.0287  data: 0.0003  max mem: 4132
[18:10:41.424057] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.0684 (1.0810)  acc1: 62.5000 (63.0495)  acc5: 96.8750 (96.6690)  time: 0.0291  data: 0.0003  max mem: 4132
[18:10:41.718042] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.1112 (1.0865)  acc1: 59.3750 (62.8094)  acc5: 96.8750 (96.6584)  time: 0.0293  data: 0.0003  max mem: 4132
[18:10:42.005112] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.1401 (1.0893)  acc1: 59.3750 (62.7675)  acc5: 96.8750 (96.7061)  time: 0.0289  data: 0.0003  max mem: 4132
[18:10:42.294609] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.0770 (1.0866)  acc1: 62.5000 (62.8874)  acc5: 98.4375 (96.7975)  time: 0.0287  data: 0.0002  max mem: 4132
[18:10:42.583103] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.0564 (1.0877)  acc1: 62.5000 (62.8101)  acc5: 96.8750 (96.7557)  time: 0.0287  data: 0.0002  max mem: 4132
[18:10:42.868166] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.0918 (1.0865)  acc1: 62.5000 (62.8214)  acc5: 96.8750 (96.7199)  time: 0.0285  data: 0.0002  max mem: 4132
[18:10:43.151922] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.0516 (1.0831)  acc1: 62.5000 (62.9760)  acc5: 96.8750 (96.7508)  time: 0.0283  data: 0.0002  max mem: 4132
[18:10:43.305776] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.0482 (1.0842)  acc1: 62.5000 (62.8600)  acc5: 96.8750 (96.7900)  time: 0.0274  data: 0.0002  max mem: 4132
[18:10:43.485428] Test: Total time: 0:00:05 (0.0347 s / it)
[18:10:43.486046] * Acc@1 62.860 Acc@5 96.790 loss 1.084
[18:10:43.486394] Accuracy of the network on the 10000 test images: 62.9%
[18:10:43.486608] Max accuracy: 63.92%
[18:10:43.693831] log_dir: ./output_dir
[18:10:44.692377] Epoch: [18]  [  0/781]  eta: 0:12:58  lr: 0.000239  training_loss: 1.6340 (1.6340)  classification_loss: 1.5716 (1.5716)  loss_mask: 0.0624 (0.0624)  time: 0.9964  data: 0.7624  max mem: 4132
[18:10:48.017641] Epoch: [18]  [ 20/781]  eta: 0:02:36  lr: 0.000239  training_loss: 1.7313 (1.7327)  classification_loss: 1.6661 (1.6651)  loss_mask: 0.0542 (0.0677)  time: 0.1662  data: 0.0003  max mem: 4132
[18:10:51.309078] Epoch: [18]  [ 40/781]  eta: 0:02:17  lr: 0.000239  training_loss: 1.8173 (1.7622)  classification_loss: 1.7513 (1.6954)  loss_mask: 0.0595 (0.0668)  time: 0.1644  data: 0.0003  max mem: 4132
[18:10:54.635243] Epoch: [18]  [ 60/781]  eta: 0:02:09  lr: 0.000239  training_loss: 1.7882 (1.7704)  classification_loss: 1.7367 (1.7086)  loss_mask: 0.0343 (0.0618)  time: 0.1662  data: 0.0003  max mem: 4132
[18:10:57.911541] Epoch: [18]  [ 80/781]  eta: 0:02:02  lr: 0.000238  training_loss: 1.8342 (1.7846)  classification_loss: 1.7565 (1.7199)  loss_mask: 0.0576 (0.0647)  time: 0.1637  data: 0.0003  max mem: 4132
[18:11:01.230450] Epoch: [18]  [100/781]  eta: 0:01:58  lr: 0.000238  training_loss: 1.7501 (1.7776)  classification_loss: 1.6906 (1.7152)  loss_mask: 0.0464 (0.0624)  time: 0.1659  data: 0.0003  max mem: 4132
[18:11:04.519545] Epoch: [18]  [120/781]  eta: 0:01:53  lr: 0.000238  training_loss: 1.7150 (1.7750)  classification_loss: 1.6815 (1.7119)  loss_mask: 0.0564 (0.0631)  time: 0.1644  data: 0.0003  max mem: 4132
[18:11:07.833874] Epoch: [18]  [140/781]  eta: 0:01:49  lr: 0.000238  training_loss: 1.7764 (1.7777)  classification_loss: 1.6929 (1.7111)  loss_mask: 0.0618 (0.0667)  time: 0.1656  data: 0.0004  max mem: 4132
[18:11:11.133153] Epoch: [18]  [160/781]  eta: 0:01:45  lr: 0.000238  training_loss: 1.7514 (1.7753)  classification_loss: 1.7026 (1.7114)  loss_mask: 0.0384 (0.0640)  time: 0.1649  data: 0.0002  max mem: 4132
[18:11:14.383248] Epoch: [18]  [180/781]  eta: 0:01:41  lr: 0.000238  training_loss: 1.7066 (1.7692)  classification_loss: 1.6515 (1.7072)  loss_mask: 0.0358 (0.0621)  time: 0.1624  data: 0.0002  max mem: 4132
[18:11:17.678811] Epoch: [18]  [200/781]  eta: 0:01:38  lr: 0.000238  training_loss: 1.7954 (1.7694)  classification_loss: 1.6881 (1.7069)  loss_mask: 0.0584 (0.0625)  time: 0.1647  data: 0.0003  max mem: 4132
[18:11:20.968646] Epoch: [18]  [220/781]  eta: 0:01:34  lr: 0.000238  training_loss: 1.8128 (1.7730)  classification_loss: 1.7413 (1.7095)  loss_mask: 0.0663 (0.0635)  time: 0.1644  data: 0.0002  max mem: 4132
[18:11:24.267134] Epoch: [18]  [240/781]  eta: 0:01:31  lr: 0.000238  training_loss: 1.8283 (1.7784)  classification_loss: 1.7515 (1.7113)  loss_mask: 0.0826 (0.0671)  time: 0.1648  data: 0.0003  max mem: 4132
[18:11:27.549195] Epoch: [18]  [260/781]  eta: 0:01:27  lr: 0.000238  training_loss: 1.7503 (1.7769)  classification_loss: 1.6993 (1.7110)  loss_mask: 0.0475 (0.0659)  time: 0.1640  data: 0.0002  max mem: 4132
[18:11:30.819774] Epoch: [18]  [280/781]  eta: 0:01:23  lr: 0.000238  training_loss: 1.7572 (1.7771)  classification_loss: 1.7016 (1.7121)  loss_mask: 0.0446 (0.0650)  time: 0.1634  data: 0.0003  max mem: 4132
[18:11:34.105759] Epoch: [18]  [300/781]  eta: 0:01:20  lr: 0.000238  training_loss: 1.7512 (1.7750)  classification_loss: 1.7004 (1.7118)  loss_mask: 0.0295 (0.0633)  time: 0.1642  data: 0.0003  max mem: 4132
[18:11:37.411106] Epoch: [18]  [320/781]  eta: 0:01:17  lr: 0.000238  training_loss: 1.7494 (1.7737)  classification_loss: 1.6819 (1.7105)  loss_mask: 0.0461 (0.0631)  time: 0.1651  data: 0.0004  max mem: 4132
[18:11:40.701569] Epoch: [18]  [340/781]  eta: 0:01:13  lr: 0.000238  training_loss: 1.7939 (1.7743)  classification_loss: 1.6609 (1.7089)  loss_mask: 0.0872 (0.0653)  time: 0.1644  data: 0.0003  max mem: 4132
[18:11:44.017617] Epoch: [18]  [360/781]  eta: 0:01:10  lr: 0.000238  training_loss: 1.7690 (1.7756)  classification_loss: 1.7067 (1.7085)  loss_mask: 0.0696 (0.0670)  time: 0.1657  data: 0.0003  max mem: 4132
[18:11:47.281896] Epoch: [18]  [380/781]  eta: 0:01:06  lr: 0.000238  training_loss: 1.7728 (1.7763)  classification_loss: 1.7112 (1.7096)  loss_mask: 0.0555 (0.0667)  time: 0.1631  data: 0.0002  max mem: 4132
[18:11:50.553730] Epoch: [18]  [400/781]  eta: 0:01:03  lr: 0.000238  training_loss: 1.7451 (1.7747)  classification_loss: 1.7011 (1.7092)  loss_mask: 0.0374 (0.0655)  time: 0.1635  data: 0.0002  max mem: 4132
[18:11:53.867338] Epoch: [18]  [420/781]  eta: 0:01:00  lr: 0.000238  training_loss: 1.7781 (1.7742)  classification_loss: 1.7133 (1.7083)  loss_mask: 0.0650 (0.0660)  time: 0.1656  data: 0.0002  max mem: 4132
[18:11:57.124833] Epoch: [18]  [440/781]  eta: 0:00:56  lr: 0.000238  training_loss: 1.7713 (1.7762)  classification_loss: 1.6784 (1.7074)  loss_mask: 0.1229 (0.0688)  time: 0.1628  data: 0.0003  max mem: 4132
[18:12:00.379408] Epoch: [18]  [460/781]  eta: 0:00:53  lr: 0.000238  training_loss: 1.7527 (1.7757)  classification_loss: 1.6530 (1.7050)  loss_mask: 0.0888 (0.0707)  time: 0.1626  data: 0.0002  max mem: 4132
[18:12:03.630527] Epoch: [18]  [480/781]  eta: 0:00:49  lr: 0.000238  training_loss: 1.7698 (1.7760)  classification_loss: 1.7112 (1.7050)  loss_mask: 0.0619 (0.0710)  time: 0.1625  data: 0.0002  max mem: 4132
[18:12:06.903192] Epoch: [18]  [500/781]  eta: 0:00:46  lr: 0.000238  training_loss: 1.7342 (1.7750)  classification_loss: 1.6893 (1.7049)  loss_mask: 0.0445 (0.0702)  time: 0.1635  data: 0.0002  max mem: 4132
[18:12:10.182679] Epoch: [18]  [520/781]  eta: 0:00:43  lr: 0.000238  training_loss: 1.7058 (1.7731)  classification_loss: 1.6608 (1.7039)  loss_mask: 0.0417 (0.0692)  time: 0.1639  data: 0.0002  max mem: 4132
[18:12:13.474820] Epoch: [18]  [540/781]  eta: 0:00:39  lr: 0.000237  training_loss: 1.7619 (1.7726)  classification_loss: 1.6723 (1.7030)  loss_mask: 0.0491 (0.0697)  time: 0.1645  data: 0.0003  max mem: 4132
[18:12:16.824542] Epoch: [18]  [560/781]  eta: 0:00:36  lr: 0.000237  training_loss: 1.7561 (1.7738)  classification_loss: 1.6810 (1.7034)  loss_mask: 0.0639 (0.0705)  time: 0.1673  data: 0.0003  max mem: 4132
[18:12:20.147569] Epoch: [18]  [580/781]  eta: 0:00:33  lr: 0.000237  training_loss: 1.7169 (1.7724)  classification_loss: 1.6527 (1.7023)  loss_mask: 0.0605 (0.0701)  time: 0.1661  data: 0.0003  max mem: 4132
[18:12:23.439517] Epoch: [18]  [600/781]  eta: 0:00:30  lr: 0.000237  training_loss: 1.8252 (1.7730)  classification_loss: 1.6841 (1.7025)  loss_mask: 0.0676 (0.0705)  time: 0.1645  data: 0.0002  max mem: 4132
[18:12:26.774770] Epoch: [18]  [620/781]  eta: 0:00:26  lr: 0.000237  training_loss: 1.7295 (1.7719)  classification_loss: 1.6957 (1.7023)  loss_mask: 0.0432 (0.0696)  time: 0.1666  data: 0.0003  max mem: 4132
[18:12:30.072238] Epoch: [18]  [640/781]  eta: 0:00:23  lr: 0.000237  training_loss: 1.7152 (1.7721)  classification_loss: 1.6620 (1.7018)  loss_mask: 0.0714 (0.0703)  time: 0.1647  data: 0.0003  max mem: 4132
[18:12:33.419758] Epoch: [18]  [660/781]  eta: 0:00:20  lr: 0.000237  training_loss: 1.7785 (1.7723)  classification_loss: 1.6830 (1.7015)  loss_mask: 0.0761 (0.0708)  time: 0.1673  data: 0.0004  max mem: 4132
[18:12:36.711296] Epoch: [18]  [680/781]  eta: 0:00:16  lr: 0.000237  training_loss: 1.7784 (1.7718)  classification_loss: 1.7204 (1.7012)  loss_mask: 0.0566 (0.0706)  time: 0.1645  data: 0.0002  max mem: 4132
[18:12:39.973808] Epoch: [18]  [700/781]  eta: 0:00:13  lr: 0.000237  training_loss: 1.7128 (1.7706)  classification_loss: 1.6403 (1.7007)  loss_mask: 0.0360 (0.0699)  time: 0.1630  data: 0.0002  max mem: 4132
[18:12:43.227501] Epoch: [18]  [720/781]  eta: 0:00:10  lr: 0.000237  training_loss: 1.7526 (1.7704)  classification_loss: 1.6952 (1.7010)  loss_mask: 0.0511 (0.0694)  time: 0.1626  data: 0.0002  max mem: 4132
[18:12:46.506072] Epoch: [18]  [740/781]  eta: 0:00:06  lr: 0.000237  training_loss: 1.6954 (1.7691)  classification_loss: 1.6443 (1.6996)  loss_mask: 0.0601 (0.0695)  time: 0.1638  data: 0.0002  max mem: 4132
[18:12:49.765923] Epoch: [18]  [760/781]  eta: 0:00:03  lr: 0.000237  training_loss: 1.7408 (1.7688)  classification_loss: 1.6747 (1.6990)  loss_mask: 0.0592 (0.0697)  time: 0.1629  data: 0.0003  max mem: 4132
[18:12:53.013814] Epoch: [18]  [780/781]  eta: 0:00:00  lr: 0.000237  training_loss: 1.6833 (1.7678)  classification_loss: 1.6441 (1.6989)  loss_mask: 0.0400 (0.0690)  time: 0.1623  data: 0.0002  max mem: 4132
[18:12:53.177515] Epoch: [18] Total time: 0:02:09 (0.1658 s / it)
[18:12:53.178418] Averaged stats: lr: 0.000237  training_loss: 1.6833 (1.7678)  classification_loss: 1.6441 (1.6989)  loss_mask: 0.0400 (0.0690)
[18:12:53.878259] Test:  [  0/157]  eta: 0:01:49  testing_loss: 0.9841 (0.9841)  acc1: 67.1875 (67.1875)  acc5: 93.7500 (93.7500)  time: 0.6957  data: 0.6591  max mem: 4132
[18:12:54.170924] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.0685 (1.0853)  acc1: 60.9375 (61.9318)  acc5: 96.8750 (96.4489)  time: 0.0897  data: 0.0601  max mem: 4132
[18:12:54.459347] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.0528 (1.0470)  acc1: 62.5000 (64.4345)  acc5: 96.8750 (97.3214)  time: 0.0289  data: 0.0002  max mem: 4132
[18:12:54.754641] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.0488 (1.0561)  acc1: 67.1875 (64.4657)  acc5: 96.8750 (96.8246)  time: 0.0290  data: 0.0002  max mem: 4132
[18:12:55.046734] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.0289 (1.0536)  acc1: 67.1875 (64.6723)  acc5: 95.3125 (96.6845)  time: 0.0292  data: 0.0002  max mem: 4132
[18:12:55.341617] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.0481 (1.0562)  acc1: 62.5000 (64.2157)  acc5: 95.3125 (96.5380)  time: 0.0291  data: 0.0003  max mem: 4132
[18:12:55.642312] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.0481 (1.0531)  acc1: 62.5000 (64.1393)  acc5: 95.3125 (96.5420)  time: 0.0295  data: 0.0003  max mem: 4132
[18:12:55.940546] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.0029 (1.0440)  acc1: 64.0625 (64.4366)  acc5: 96.8750 (96.6329)  time: 0.0298  data: 0.0003  max mem: 4132
[18:12:56.250041] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.9808 (1.0470)  acc1: 64.0625 (64.2747)  acc5: 96.8750 (96.5664)  time: 0.0302  data: 0.0002  max mem: 4132
[18:12:56.546793] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.0769 (1.0508)  acc1: 62.5000 (64.0797)  acc5: 95.3125 (96.5488)  time: 0.0302  data: 0.0002  max mem: 4132
[18:12:56.839361] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.0912 (1.0536)  acc1: 62.5000 (63.9078)  acc5: 95.3125 (96.5811)  time: 0.0293  data: 0.0002  max mem: 4132
[18:12:57.125926] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.0974 (1.0560)  acc1: 60.9375 (63.7810)  acc5: 96.8750 (96.6498)  time: 0.0288  data: 0.0003  max mem: 4132
[18:12:57.415981] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.0564 (1.0535)  acc1: 62.5000 (64.0108)  acc5: 96.8750 (96.6555)  time: 0.0287  data: 0.0003  max mem: 4132
[18:12:57.715829] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.0222 (1.0538)  acc1: 62.5000 (63.8955)  acc5: 96.8750 (96.6484)  time: 0.0293  data: 0.0003  max mem: 4132
[18:12:58.003127] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.0542 (1.0532)  acc1: 62.5000 (64.0403)  acc5: 96.8750 (96.6866)  time: 0.0292  data: 0.0002  max mem: 4132
[18:12:58.286141] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.0301 (1.0485)  acc1: 67.1875 (64.3522)  acc5: 96.8750 (96.7301)  time: 0.0284  data: 0.0002  max mem: 4132
[18:12:58.445334] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.9837 (1.0502)  acc1: 68.7500 (64.3600)  acc5: 96.8750 (96.7300)  time: 0.0276  data: 0.0002  max mem: 4132
[18:12:58.625342] Test: Total time: 0:00:05 (0.0347 s / it)
[18:12:58.625788] * Acc@1 64.360 Acc@5 96.730 loss 1.050
[18:12:58.626077] Accuracy of the network on the 10000 test images: 64.4%
[18:12:58.626281] Max accuracy: 64.36%
[18:12:58.903961] log_dir: ./output_dir
[18:12:59.850116] Epoch: [19]  [  0/781]  eta: 0:12:17  lr: 0.000237  training_loss: 1.7145 (1.7145)  classification_loss: 1.6567 (1.6567)  loss_mask: 0.0578 (0.0578)  time: 0.9438  data: 0.7560  max mem: 4132
[18:13:03.130450] Epoch: [19]  [ 20/781]  eta: 0:02:33  lr: 0.000237  training_loss: 1.6970 (1.6942)  classification_loss: 1.6484 (1.6557)  loss_mask: 0.0292 (0.0385)  time: 0.1639  data: 0.0002  max mem: 4132
[18:13:06.430793] Epoch: [19]  [ 40/781]  eta: 0:02:15  lr: 0.000237  training_loss: 1.7170 (1.7029)  classification_loss: 1.6637 (1.6572)  loss_mask: 0.0443 (0.0457)  time: 0.1649  data: 0.0003  max mem: 4132
[18:13:09.701383] Epoch: [19]  [ 60/781]  eta: 0:02:07  lr: 0.000237  training_loss: 1.7335 (1.7174)  classification_loss: 1.6907 (1.6745)  loss_mask: 0.0294 (0.0429)  time: 0.1634  data: 0.0002  max mem: 4132
[18:13:13.010249] Epoch: [19]  [ 80/781]  eta: 0:02:01  lr: 0.000237  training_loss: 1.7316 (1.7189)  classification_loss: 1.7159 (1.6776)  loss_mask: 0.0329 (0.0413)  time: 0.1653  data: 0.0002  max mem: 4132
[18:13:16.295367] Epoch: [19]  [100/781]  eta: 0:01:57  lr: 0.000237  training_loss: 1.7363 (1.7234)  classification_loss: 1.6991 (1.6823)  loss_mask: 0.0247 (0.0411)  time: 0.1641  data: 0.0003  max mem: 4132
[18:13:19.591574] Epoch: [19]  [120/781]  eta: 0:01:52  lr: 0.000237  training_loss: 1.7087 (1.7215)  classification_loss: 1.6692 (1.6799)  loss_mask: 0.0432 (0.0417)  time: 0.1647  data: 0.0003  max mem: 4132
[18:13:22.876767] Epoch: [19]  [140/781]  eta: 0:01:48  lr: 0.000237  training_loss: 1.7270 (1.7200)  classification_loss: 1.6592 (1.6739)  loss_mask: 0.0729 (0.0462)  time: 0.1642  data: 0.0003  max mem: 4132
[18:13:26.185513] Epoch: [19]  [160/781]  eta: 0:01:45  lr: 0.000237  training_loss: 1.7389 (1.7263)  classification_loss: 1.6584 (1.6720)  loss_mask: 0.0805 (0.0543)  time: 0.1654  data: 0.0002  max mem: 4132
[18:13:29.464236] Epoch: [19]  [180/781]  eta: 0:01:41  lr: 0.000236  training_loss: 1.7027 (1.7242)  classification_loss: 1.6547 (1.6698)  loss_mask: 0.0502 (0.0544)  time: 0.1639  data: 0.0002  max mem: 4132
[18:13:32.782364] Epoch: [19]  [200/781]  eta: 0:01:37  lr: 0.000236  training_loss: 1.7586 (1.7287)  classification_loss: 1.7234 (1.6740)  loss_mask: 0.0427 (0.0548)  time: 0.1658  data: 0.0003  max mem: 4132
[18:13:36.065314] Epoch: [19]  [220/781]  eta: 0:01:34  lr: 0.000236  training_loss: 1.6961 (1.7282)  classification_loss: 1.6592 (1.6755)  loss_mask: 0.0267 (0.0527)  time: 0.1641  data: 0.0002  max mem: 4132
[18:13:39.334772] Epoch: [19]  [240/781]  eta: 0:01:30  lr: 0.000236  training_loss: 1.7538 (1.7305)  classification_loss: 1.6964 (1.6769)  loss_mask: 0.0479 (0.0536)  time: 0.1634  data: 0.0003  max mem: 4132
[18:13:42.590204] Epoch: [19]  [260/781]  eta: 0:01:27  lr: 0.000236  training_loss: 1.7643 (1.7315)  classification_loss: 1.7283 (1.6783)  loss_mask: 0.0415 (0.0531)  time: 0.1627  data: 0.0002  max mem: 4132
[18:13:45.845127] Epoch: [19]  [280/781]  eta: 0:01:23  lr: 0.000236  training_loss: 1.7115 (1.7338)  classification_loss: 1.6888 (1.6797)  loss_mask: 0.0364 (0.0541)  time: 0.1627  data: 0.0002  max mem: 4132
[18:13:49.115608] Epoch: [19]  [300/781]  eta: 0:01:20  lr: 0.000236  training_loss: 1.7624 (1.7367)  classification_loss: 1.6701 (1.6799)  loss_mask: 0.0580 (0.0569)  time: 0.1634  data: 0.0002  max mem: 4132
[18:13:52.403588] Epoch: [19]  [320/781]  eta: 0:01:16  lr: 0.000236  training_loss: 1.7329 (1.7372)  classification_loss: 1.6626 (1.6797)  loss_mask: 0.0556 (0.0576)  time: 0.1643  data: 0.0003  max mem: 4132
[18:13:55.696963] Epoch: [19]  [340/781]  eta: 0:01:13  lr: 0.000236  training_loss: 1.6864 (1.7348)  classification_loss: 1.6355 (1.6775)  loss_mask: 0.0500 (0.0573)  time: 0.1646  data: 0.0003  max mem: 4132
[18:13:58.978115] Epoch: [19]  [360/781]  eta: 0:01:10  lr: 0.000236  training_loss: 1.7321 (1.7350)  classification_loss: 1.7033 (1.6785)  loss_mask: 0.0295 (0.0565)  time: 0.1640  data: 0.0003  max mem: 4132
[18:14:02.276346] Epoch: [19]  [380/781]  eta: 0:01:06  lr: 0.000236  training_loss: 1.6633 (1.7314)  classification_loss: 1.6354 (1.6760)  loss_mask: 0.0251 (0.0553)  time: 0.1648  data: 0.0003  max mem: 4132
[18:14:05.557875] Epoch: [19]  [400/781]  eta: 0:01:03  lr: 0.000236  training_loss: 1.6916 (1.7294)  classification_loss: 1.6416 (1.6749)  loss_mask: 0.0371 (0.0545)  time: 0.1640  data: 0.0003  max mem: 4132
[18:14:08.860562] Epoch: [19]  [420/781]  eta: 0:00:59  lr: 0.000236  training_loss: 1.7795 (1.7315)  classification_loss: 1.6766 (1.6751)  loss_mask: 0.0790 (0.0564)  time: 0.1650  data: 0.0003  max mem: 4132
[18:14:12.204516] Epoch: [19]  [440/781]  eta: 0:00:56  lr: 0.000236  training_loss: 1.6936 (1.7310)  classification_loss: 1.6338 (1.6744)  loss_mask: 0.0422 (0.0566)  time: 0.1671  data: 0.0003  max mem: 4132
[18:14:15.525857] Epoch: [19]  [460/781]  eta: 0:00:53  lr: 0.000236  training_loss: 1.6961 (1.7299)  classification_loss: 1.6245 (1.6728)  loss_mask: 0.0407 (0.0571)  time: 0.1660  data: 0.0003  max mem: 4132
[18:14:18.824694] Epoch: [19]  [480/781]  eta: 0:00:49  lr: 0.000236  training_loss: 1.8708 (1.7367)  classification_loss: 1.7095 (1.6736)  loss_mask: 0.1552 (0.0631)  time: 0.1649  data: 0.0003  max mem: 4132
[18:14:22.092731] Epoch: [19]  [500/781]  eta: 0:00:46  lr: 0.000236  training_loss: 1.7562 (1.7382)  classification_loss: 1.6703 (1.6740)  loss_mask: 0.0693 (0.0642)  time: 0.1633  data: 0.0004  max mem: 4132
[18:14:25.416434] Epoch: [19]  [520/781]  eta: 0:00:43  lr: 0.000236  training_loss: 1.7600 (1.7387)  classification_loss: 1.6897 (1.6743)  loss_mask: 0.0588 (0.0644)  time: 0.1661  data: 0.0003  max mem: 4132
[18:14:28.702201] Epoch: [19]  [540/781]  eta: 0:00:39  lr: 0.000236  training_loss: 1.8064 (1.7410)  classification_loss: 1.6940 (1.6750)  loss_mask: 0.0555 (0.0660)  time: 0.1642  data: 0.0003  max mem: 4132
[18:14:31.999308] Epoch: [19]  [560/781]  eta: 0:00:36  lr: 0.000236  training_loss: 1.7460 (1.7416)  classification_loss: 1.6453 (1.6745)  loss_mask: 0.0792 (0.0671)  time: 0.1648  data: 0.0003  max mem: 4132
[18:14:35.312631] Epoch: [19]  [580/781]  eta: 0:00:33  lr: 0.000235  training_loss: 1.7020 (1.7406)  classification_loss: 1.6761 (1.6745)  loss_mask: 0.0373 (0.0662)  time: 0.1656  data: 0.0004  max mem: 4132
[18:14:38.614148] Epoch: [19]  [600/781]  eta: 0:00:30  lr: 0.000235  training_loss: 1.6447 (1.7380)  classification_loss: 1.6192 (1.6731)  loss_mask: 0.0267 (0.0649)  time: 0.1650  data: 0.0003  max mem: 4132
[18:14:41.933852] Epoch: [19]  [620/781]  eta: 0:00:26  lr: 0.000235  training_loss: 1.7144 (1.7379)  classification_loss: 1.6700 (1.6730)  loss_mask: 0.0356 (0.0649)  time: 0.1658  data: 0.0003  max mem: 4132
[18:14:45.215091] Epoch: [19]  [640/781]  eta: 0:00:23  lr: 0.000235  training_loss: 1.7670 (1.7390)  classification_loss: 1.7014 (1.6739)  loss_mask: 0.0522 (0.0651)  time: 0.1640  data: 0.0002  max mem: 4132
[18:14:48.573871] Epoch: [19]  [660/781]  eta: 0:00:20  lr: 0.000235  training_loss: 1.7825 (1.7398)  classification_loss: 1.7081 (1.6746)  loss_mask: 0.0532 (0.0652)  time: 0.1679  data: 0.0003  max mem: 4132
[18:14:51.843643] Epoch: [19]  [680/781]  eta: 0:00:16  lr: 0.000235  training_loss: 1.7227 (1.7395)  classification_loss: 1.6597 (1.6744)  loss_mask: 0.0493 (0.0651)  time: 0.1634  data: 0.0002  max mem: 4132
[18:14:55.144837] Epoch: [19]  [700/781]  eta: 0:00:13  lr: 0.000235  training_loss: 1.7393 (1.7399)  classification_loss: 1.6649 (1.6744)  loss_mask: 0.0531 (0.0654)  time: 0.1650  data: 0.0002  max mem: 4132
[18:14:58.408473] Epoch: [19]  [720/781]  eta: 0:00:10  lr: 0.000235  training_loss: 1.7785 (1.7400)  classification_loss: 1.6511 (1.6740)  loss_mask: 0.0658 (0.0661)  time: 0.1631  data: 0.0003  max mem: 4132
[18:15:01.707922] Epoch: [19]  [740/781]  eta: 0:00:06  lr: 0.000235  training_loss: 1.6963 (1.7391)  classification_loss: 1.6512 (1.6732)  loss_mask: 0.0488 (0.0659)  time: 0.1649  data: 0.0003  max mem: 4132
[18:15:04.990142] Epoch: [19]  [760/781]  eta: 0:00:03  lr: 0.000235  training_loss: 1.7406 (1.7399)  classification_loss: 1.6934 (1.6744)  loss_mask: 0.0347 (0.0655)  time: 0.1640  data: 0.0003  max mem: 4132
[18:15:08.240466] Epoch: [19]  [780/781]  eta: 0:00:00  lr: 0.000235  training_loss: 1.7169 (1.7394)  classification_loss: 1.6760 (1.6746)  loss_mask: 0.0295 (0.0647)  time: 0.1624  data: 0.0003  max mem: 4132
[18:15:08.419834] Epoch: [19] Total time: 0:02:09 (0.1658 s / it)
[18:15:08.420763] Averaged stats: lr: 0.000235  training_loss: 1.7169 (1.7394)  classification_loss: 1.6760 (1.6746)  loss_mask: 0.0295 (0.0647)
[18:15:09.136597] Test:  [  0/157]  eta: 0:01:51  testing_loss: 0.9380 (0.9380)  acc1: 70.3125 (70.3125)  acc5: 95.3125 (95.3125)  time: 0.7103  data: 0.6766  max mem: 4132
[18:15:09.449569] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.0575 (1.0319)  acc1: 62.5000 (64.7727)  acc5: 98.4375 (97.4432)  time: 0.0927  data: 0.0618  max mem: 4132
[18:15:09.746359] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.0119 (0.9912)  acc1: 64.0625 (66.3690)  acc5: 98.4375 (98.2143)  time: 0.0302  data: 0.0002  max mem: 4132
[18:15:10.038612] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.9505 (0.9998)  acc1: 68.7500 (66.2298)  acc5: 98.4375 (97.6815)  time: 0.0292  data: 0.0002  max mem: 4132
[18:15:10.335778] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.9745 (0.9967)  acc1: 67.1875 (66.0823)  acc5: 96.8750 (97.5991)  time: 0.0293  data: 0.0004  max mem: 4132
[18:15:10.621375] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.9837 (0.9964)  acc1: 65.6250 (66.2684)  acc5: 96.8750 (97.5490)  time: 0.0290  data: 0.0003  max mem: 4132
[18:15:10.908477] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.9774 (0.9911)  acc1: 65.6250 (66.3166)  acc5: 96.8750 (97.4129)  time: 0.0285  data: 0.0002  max mem: 4132
[18:15:11.198238] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.9428 (0.9858)  acc1: 67.1875 (66.5713)  acc5: 98.4375 (97.5132)  time: 0.0287  data: 0.0002  max mem: 4132
[18:15:11.487170] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.9426 (0.9907)  acc1: 65.6250 (66.4159)  acc5: 98.4375 (97.5502)  time: 0.0288  data: 0.0002  max mem: 4132
[18:15:11.772526] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.9904 (0.9924)  acc1: 64.0625 (66.2775)  acc5: 98.4375 (97.5790)  time: 0.0286  data: 0.0002  max mem: 4132
[18:15:12.063084] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.0402 (0.9983)  acc1: 64.0625 (66.0272)  acc5: 98.4375 (97.6640)  time: 0.0287  data: 0.0003  max mem: 4132
[18:15:12.349542] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.0402 (1.0001)  acc1: 62.5000 (65.9910)  acc5: 98.4375 (97.6070)  time: 0.0287  data: 0.0003  max mem: 4132
[18:15:12.639020] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.9681 (0.9973)  acc1: 65.6250 (66.1157)  acc5: 96.8750 (97.6111)  time: 0.0286  data: 0.0002  max mem: 4132
[18:15:12.929335] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.9681 (0.9981)  acc1: 67.1875 (66.1617)  acc5: 98.4375 (97.5906)  time: 0.0288  data: 0.0003  max mem: 4132
[18:15:13.214631] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.0083 (0.9981)  acc1: 65.6250 (66.1791)  acc5: 96.8750 (97.5731)  time: 0.0286  data: 0.0002  max mem: 4132
[18:15:13.497984] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.0039 (0.9955)  acc1: 67.1875 (66.3700)  acc5: 96.8750 (97.5373)  time: 0.0283  data: 0.0002  max mem: 4132
[18:15:13.651212] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.9601 (0.9968)  acc1: 67.1875 (66.3800)  acc5: 96.8750 (97.5100)  time: 0.0273  data: 0.0002  max mem: 4132
[18:15:13.830285] Test: Total time: 0:00:05 (0.0344 s / it)
[18:15:13.831008] * Acc@1 66.380 Acc@5 97.510 loss 0.997
[18:15:13.831330] Accuracy of the network on the 10000 test images: 66.4%
[18:15:13.831509] Max accuracy: 66.38%
[18:15:14.084595] log_dir: ./output_dir
[18:15:15.024801] Epoch: [20]  [  0/781]  eta: 0:12:12  lr: 0.000235  training_loss: 1.5942 (1.5942)  classification_loss: 1.5578 (1.5578)  loss_mask: 0.0364 (0.0364)  time: 0.9383  data: 0.7697  max mem: 4132
[18:15:18.313404] Epoch: [20]  [ 20/781]  eta: 0:02:33  lr: 0.000235  training_loss: 1.7152 (1.7178)  classification_loss: 1.6521 (1.6460)  loss_mask: 0.0604 (0.0718)  time: 0.1643  data: 0.0003  max mem: 4132
[18:15:21.577974] Epoch: [20]  [ 40/781]  eta: 0:02:15  lr: 0.000235  training_loss: 1.8413 (1.7631)  classification_loss: 1.6811 (1.6629)  loss_mask: 0.1074 (0.1002)  time: 0.1631  data: 0.0002  max mem: 4132
[18:15:24.845434] Epoch: [20]  [ 60/781]  eta: 0:02:07  lr: 0.000235  training_loss: 1.7648 (1.7657)  classification_loss: 1.6861 (1.6659)  loss_mask: 0.0799 (0.0998)  time: 0.1633  data: 0.0002  max mem: 4132
[18:15:28.159184] Epoch: [20]  [ 80/781]  eta: 0:02:01  lr: 0.000235  training_loss: 1.7642 (1.7648)  classification_loss: 1.6928 (1.6699)  loss_mask: 0.0531 (0.0950)  time: 0.1655  data: 0.0002  max mem: 4132
[18:15:31.487427] Epoch: [20]  [100/781]  eta: 0:01:57  lr: 0.000235  training_loss: 1.8049 (1.7698)  classification_loss: 1.6764 (1.6758)  loss_mask: 0.0691 (0.0940)  time: 0.1663  data: 0.0004  max mem: 4132
[18:15:34.809395] Epoch: [20]  [120/781]  eta: 0:01:53  lr: 0.000235  training_loss: 1.7141 (1.7598)  classification_loss: 1.6501 (1.6705)  loss_mask: 0.0702 (0.0893)  time: 0.1660  data: 0.0003  max mem: 4132
[18:15:38.159389] Epoch: [20]  [140/781]  eta: 0:01:49  lr: 0.000235  training_loss: 1.7790 (1.7655)  classification_loss: 1.6720 (1.6704)  loss_mask: 0.0814 (0.0951)  time: 0.1674  data: 0.0003  max mem: 4132
[18:15:41.487511] Epoch: [20]  [160/781]  eta: 0:01:45  lr: 0.000235  training_loss: 1.7726 (1.7656)  classification_loss: 1.6623 (1.6724)  loss_mask: 0.0667 (0.0932)  time: 0.1663  data: 0.0002  max mem: 4132
[18:15:44.795994] Epoch: [20]  [180/781]  eta: 0:01:41  lr: 0.000235  training_loss: 1.7151 (1.7585)  classification_loss: 1.6505 (1.6695)  loss_mask: 0.0442 (0.0889)  time: 0.1653  data: 0.0003  max mem: 4132
[18:15:48.107854] Epoch: [20]  [200/781]  eta: 0:01:38  lr: 0.000234  training_loss: 1.7303 (1.7590)  classification_loss: 1.6851 (1.6726)  loss_mask: 0.0449 (0.0864)  time: 0.1655  data: 0.0003  max mem: 4132
[18:15:51.406496] Epoch: [20]  [220/781]  eta: 0:01:34  lr: 0.000234  training_loss: 1.7737 (1.7609)  classification_loss: 1.7216 (1.6771)  loss_mask: 0.0437 (0.0838)  time: 0.1648  data: 0.0003  max mem: 4132
[18:15:54.679037] Epoch: [20]  [240/781]  eta: 0:01:31  lr: 0.000234  training_loss: 1.7207 (1.7586)  classification_loss: 1.6857 (1.6788)  loss_mask: 0.0310 (0.0798)  time: 0.1635  data: 0.0002  max mem: 4132
[18:15:57.999429] Epoch: [20]  [260/781]  eta: 0:01:27  lr: 0.000234  training_loss: 1.7020 (1.7518)  classification_loss: 1.6306 (1.6739)  loss_mask: 0.0523 (0.0779)  time: 0.1659  data: 0.0002  max mem: 4132
[18:16:01.295647] Epoch: [20]  [280/781]  eta: 0:01:24  lr: 0.000234  training_loss: 1.7700 (1.7542)  classification_loss: 1.7142 (1.6755)  loss_mask: 0.0745 (0.0786)  time: 0.1647  data: 0.0003  max mem: 4132
[18:16:04.566505] Epoch: [20]  [300/781]  eta: 0:01:20  lr: 0.000234  training_loss: 1.7655 (1.7553)  classification_loss: 1.6908 (1.6776)  loss_mask: 0.0567 (0.0777)  time: 0.1634  data: 0.0003  max mem: 4132
[18:16:07.850584] Epoch: [20]  [320/781]  eta: 0:01:17  lr: 0.000234  training_loss: 1.7476 (1.7550)  classification_loss: 1.6950 (1.6783)  loss_mask: 0.0454 (0.0767)  time: 0.1641  data: 0.0002  max mem: 4132
[18:16:11.148618] Epoch: [20]  [340/781]  eta: 0:01:13  lr: 0.000234  training_loss: 1.7141 (1.7540)  classification_loss: 1.6569 (1.6767)  loss_mask: 0.0516 (0.0773)  time: 0.1648  data: 0.0003  max mem: 4132
[18:16:14.423950] Epoch: [20]  [360/781]  eta: 0:01:10  lr: 0.000234  training_loss: 1.7275 (1.7522)  classification_loss: 1.6716 (1.6767)  loss_mask: 0.0368 (0.0755)  time: 0.1637  data: 0.0002  max mem: 4132
[18:16:17.702344] Epoch: [20]  [380/781]  eta: 0:01:06  lr: 0.000234  training_loss: 1.6781 (1.7499)  classification_loss: 1.6279 (1.6754)  loss_mask: 0.0409 (0.0745)  time: 0.1638  data: 0.0003  max mem: 4132
[18:16:20.981941] Epoch: [20]  [400/781]  eta: 0:01:03  lr: 0.000234  training_loss: 1.7242 (1.7474)  classification_loss: 1.6954 (1.6751)  loss_mask: 0.0246 (0.0722)  time: 0.1639  data: 0.0002  max mem: 4132
[18:16:24.317019] Epoch: [20]  [420/781]  eta: 0:01:00  lr: 0.000234  training_loss: 1.7028 (1.7454)  classification_loss: 1.6738 (1.6750)  loss_mask: 0.0285 (0.0704)  time: 0.1667  data: 0.0003  max mem: 4132
[18:16:27.607193] Epoch: [20]  [440/781]  eta: 0:00:56  lr: 0.000234  training_loss: 1.7034 (1.7448)  classification_loss: 1.6588 (1.6741)  loss_mask: 0.0557 (0.0708)  time: 0.1644  data: 0.0003  max mem: 4132
[18:16:30.862593] Epoch: [20]  [460/781]  eta: 0:00:53  lr: 0.000234  training_loss: 1.6723 (1.7432)  classification_loss: 1.6022 (1.6719)  loss_mask: 0.0467 (0.0713)  time: 0.1627  data: 0.0003  max mem: 4132
[18:16:34.130729] Epoch: [20]  [480/781]  eta: 0:00:50  lr: 0.000234  training_loss: 1.7214 (1.7416)  classification_loss: 1.6725 (1.6714)  loss_mask: 0.0450 (0.0702)  time: 0.1633  data: 0.0002  max mem: 4132
[18:16:37.410944] Epoch: [20]  [500/781]  eta: 0:00:46  lr: 0.000234  training_loss: 1.6999 (1.7407)  classification_loss: 1.6201 (1.6704)  loss_mask: 0.0583 (0.0703)  time: 0.1639  data: 0.0002  max mem: 4132
[18:16:40.694069] Epoch: [20]  [520/781]  eta: 0:00:43  lr: 0.000234  training_loss: 1.7410 (1.7407)  classification_loss: 1.6588 (1.6696)  loss_mask: 0.0599 (0.0711)  time: 0.1641  data: 0.0002  max mem: 4132
[18:16:43.954418] Epoch: [20]  [540/781]  eta: 0:00:40  lr: 0.000234  training_loss: 1.6835 (1.7395)  classification_loss: 1.6141 (1.6681)  loss_mask: 0.0495 (0.0714)  time: 0.1629  data: 0.0002  max mem: 4132
[18:16:47.225483] Epoch: [20]  [560/781]  eta: 0:00:36  lr: 0.000234  training_loss: 1.7068 (1.7396)  classification_loss: 1.6238 (1.6674)  loss_mask: 0.0633 (0.0722)  time: 0.1635  data: 0.0002  max mem: 4132
[18:16:50.515493] Epoch: [20]  [580/781]  eta: 0:00:33  lr: 0.000234  training_loss: 1.7279 (1.7389)  classification_loss: 1.6937 (1.6676)  loss_mask: 0.0397 (0.0713)  time: 0.1644  data: 0.0003  max mem: 4132
[18:16:53.787431] Epoch: [20]  [600/781]  eta: 0:00:30  lr: 0.000233  training_loss: 1.6528 (1.7365)  classification_loss: 1.6262 (1.6665)  loss_mask: 0.0276 (0.0700)  time: 0.1635  data: 0.0002  max mem: 4132
[18:16:57.103919] Epoch: [20]  [620/781]  eta: 0:00:26  lr: 0.000233  training_loss: 1.7109 (1.7361)  classification_loss: 1.6740 (1.6668)  loss_mask: 0.0407 (0.0693)  time: 0.1657  data: 0.0003  max mem: 4132
[18:17:00.377090] Epoch: [20]  [640/781]  eta: 0:00:23  lr: 0.000233  training_loss: 1.7221 (1.7362)  classification_loss: 1.6979 (1.6679)  loss_mask: 0.0315 (0.0683)  time: 0.1635  data: 0.0002  max mem: 4132
[18:17:03.660287] Epoch: [20]  [660/781]  eta: 0:00:20  lr: 0.000233  training_loss: 1.6991 (1.7351)  classification_loss: 1.6486 (1.6674)  loss_mask: 0.0418 (0.0677)  time: 0.1641  data: 0.0002  max mem: 4132
[18:17:06.939616] Epoch: [20]  [680/781]  eta: 0:00:16  lr: 0.000233  training_loss: 1.7006 (1.7349)  classification_loss: 1.6606 (1.6678)  loss_mask: 0.0375 (0.0671)  time: 0.1639  data: 0.0002  max mem: 4132
[18:17:10.243416] Epoch: [20]  [700/781]  eta: 0:00:13  lr: 0.000233  training_loss: 1.7820 (1.7363)  classification_loss: 1.7013 (1.6687)  loss_mask: 0.0781 (0.0677)  time: 0.1650  data: 0.0003  max mem: 4132
[18:17:13.586951] Epoch: [20]  [720/781]  eta: 0:00:10  lr: 0.000233  training_loss: 1.7610 (1.7372)  classification_loss: 1.6836 (1.6691)  loss_mask: 0.0710 (0.0681)  time: 0.1671  data: 0.0004  max mem: 4132
[18:17:16.908616] Epoch: [20]  [740/781]  eta: 0:00:06  lr: 0.000233  training_loss: 1.8061 (1.7386)  classification_loss: 1.6875 (1.6698)  loss_mask: 0.1002 (0.0689)  time: 0.1659  data: 0.0003  max mem: 4132
[18:17:20.213325] Epoch: [20]  [760/781]  eta: 0:00:03  lr: 0.000233  training_loss: 1.7499 (1.7386)  classification_loss: 1.7164 (1.6707)  loss_mask: 0.0333 (0.0679)  time: 0.1652  data: 0.0003  max mem: 4132
[18:17:23.481701] Epoch: [20]  [780/781]  eta: 0:00:00  lr: 0.000233  training_loss: 1.7063 (1.7384)  classification_loss: 1.6549 (1.6711)  loss_mask: 0.0281 (0.0673)  time: 0.1633  data: 0.0002  max mem: 4132
[18:17:23.681110] Epoch: [20] Total time: 0:02:09 (0.1659 s / it)
[18:17:23.681623] Averaged stats: lr: 0.000233  training_loss: 1.7063 (1.7384)  classification_loss: 1.6549 (1.6711)  loss_mask: 0.0281 (0.0673)
[18:17:25.818599] Test:  [  0/157]  eta: 0:02:05  testing_loss: 0.8491 (0.8491)  acc1: 75.0000 (75.0000)  acc5: 100.0000 (100.0000)  time: 0.7987  data: 0.7452  max mem: 4132
[18:17:26.132050] Test:  [ 10/157]  eta: 0:00:14  testing_loss: 1.0586 (1.0673)  acc1: 65.6250 (63.6364)  acc5: 96.8750 (97.0170)  time: 0.1009  data: 0.0682  max mem: 4132
[18:17:26.422658] Test:  [ 20/157]  eta: 0:00:09  testing_loss: 0.9957 (1.0166)  acc1: 68.7500 (66.8899)  acc5: 98.4375 (97.8423)  time: 0.0300  data: 0.0004  max mem: 4132
[18:17:26.710862] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.9951 (1.0244)  acc1: 68.7500 (65.9274)  acc5: 98.4375 (97.4294)  time: 0.0288  data: 0.0003  max mem: 4132
[18:17:27.007154] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.0263 (1.0311)  acc1: 65.6250 (65.5107)  acc5: 96.8750 (97.4085)  time: 0.0291  data: 0.0002  max mem: 4132
[18:17:27.296491] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.0520 (1.0324)  acc1: 64.0625 (65.5025)  acc5: 96.8750 (97.3039)  time: 0.0291  data: 0.0003  max mem: 4132
[18:17:27.587074] Test:  [ 60/157]  eta: 0:00:04  testing_loss: 1.0095 (1.0276)  acc1: 64.0625 (65.5738)  acc5: 96.8750 (97.2592)  time: 0.0288  data: 0.0003  max mem: 4132
[18:17:27.876190] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.9723 (1.0214)  acc1: 67.1875 (65.6910)  acc5: 96.8750 (97.3592)  time: 0.0288  data: 0.0003  max mem: 4132
[18:17:28.167742] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.9952 (1.0242)  acc1: 67.1875 (65.7986)  acc5: 96.8750 (97.2222)  time: 0.0289  data: 0.0003  max mem: 4132
[18:17:28.456965] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.0231 (1.0241)  acc1: 67.1875 (65.9169)  acc5: 96.8750 (97.2356)  time: 0.0289  data: 0.0003  max mem: 4132
[18:17:28.745722] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.0428 (1.0298)  acc1: 64.0625 (65.5631)  acc5: 96.8750 (97.1999)  time: 0.0287  data: 0.0002  max mem: 4132
[18:17:29.038416] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.1086 (1.0337)  acc1: 60.9375 (65.2872)  acc5: 96.8750 (97.2128)  time: 0.0289  data: 0.0004  max mem: 4132
[18:17:29.324551] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.0134 (1.0303)  acc1: 62.5000 (65.4055)  acc5: 96.8750 (97.2107)  time: 0.0288  data: 0.0004  max mem: 4132
[18:17:29.612285] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.0134 (1.0320)  acc1: 67.1875 (65.3745)  acc5: 96.8750 (97.1732)  time: 0.0285  data: 0.0002  max mem: 4132
[18:17:29.898384] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.0482 (1.0310)  acc1: 64.0625 (65.4034)  acc5: 96.8750 (97.1299)  time: 0.0285  data: 0.0002  max mem: 4132
[18:17:30.183874] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.9845 (1.0280)  acc1: 67.1875 (65.5319)  acc5: 96.8750 (97.1337)  time: 0.0284  data: 0.0002  max mem: 4132
[18:17:30.337819] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.9722 (1.0301)  acc1: 67.1875 (65.4900)  acc5: 98.4375 (97.1600)  time: 0.0275  data: 0.0002  max mem: 4132
[18:17:30.506203] Test: Total time: 0:00:05 (0.0350 s / it)
[18:17:30.507287] * Acc@1 65.490 Acc@5 97.160 loss 1.030
[18:17:30.507782] Accuracy of the network on the 10000 test images: 65.5%
[18:17:30.508001] Max accuracy: 66.38%
[18:17:30.824158] log_dir: ./output_dir
[18:17:31.796030] Epoch: [21]  [  0/781]  eta: 0:12:37  lr: 0.000233  training_loss: 1.8168 (1.8168)  classification_loss: 1.5885 (1.5885)  loss_mask: 0.2283 (0.2283)  time: 0.9694  data: 0.7908  max mem: 4132
[18:17:35.063579] Epoch: [21]  [ 20/781]  eta: 0:02:33  lr: 0.000233  training_loss: 1.6721 (1.7179)  classification_loss: 1.6351 (1.6222)  loss_mask: 0.0670 (0.0957)  time: 0.1633  data: 0.0003  max mem: 4132
[18:17:38.362622] Epoch: [21]  [ 40/781]  eta: 0:02:16  lr: 0.000233  training_loss: 1.7149 (1.7116)  classification_loss: 1.6664 (1.6394)  loss_mask: 0.0367 (0.0722)  time: 0.1649  data: 0.0002  max mem: 4132
[18:17:41.659307] Epoch: [21]  [ 60/781]  eta: 0:02:07  lr: 0.000233  training_loss: 1.7273 (1.7256)  classification_loss: 1.6356 (1.6543)  loss_mask: 0.0660 (0.0713)  time: 0.1647  data: 0.0003  max mem: 4132
[18:17:44.950836] Epoch: [21]  [ 80/781]  eta: 0:02:02  lr: 0.000233  training_loss: 1.8150 (1.7458)  classification_loss: 1.6694 (1.6611)  loss_mask: 0.0938 (0.0848)  time: 0.1645  data: 0.0003  max mem: 4132
[18:17:48.228337] Epoch: [21]  [100/781]  eta: 0:01:57  lr: 0.000233  training_loss: 1.7451 (1.7483)  classification_loss: 1.6696 (1.6628)  loss_mask: 0.0706 (0.0855)  time: 0.1638  data: 0.0004  max mem: 4132
[18:17:51.496985] Epoch: [21]  [120/781]  eta: 0:01:52  lr: 0.000233  training_loss: 1.7665 (1.7493)  classification_loss: 1.6381 (1.6628)  loss_mask: 0.0698 (0.0865)  time: 0.1633  data: 0.0002  max mem: 4132

[18:17:54.791277] Epoch: [21]  [140/781]  eta: 0:01:48  lr: 0.000233  training_loss: 1.7168 (1.7491)  classification_loss: 1.6522 (1.6596)  loss_mask: 0.0786 (0.0895)  time: 0.1646  data: 0.0003  max mem: 4132
[18:17:58.096063] Epoch: [21]  [160/781]  eta: 0:01:45  lr: 0.000233  training_loss: 1.7137 (1.7441)  classification_loss: 1.6578 (1.6577)  loss_mask: 0.0619 (0.0864)  time: 0.1651  data: 0.0003  max mem: 4132
[18:18:01.391825] Epoch: [21]  [180/781]  eta: 0:01:41  lr: 0.000232  training_loss: 1.6592 (1.7346)  classification_loss: 1.6020 (1.6530)  loss_mask: 0.0277 (0.0816)  time: 0.1647  data: 0.0002  max mem: 4132
[18:18:04.674106] Epoch: [21]  [200/781]  eta: 0:01:37  lr: 0.000232  training_loss: 1.7615 (1.7365)  classification_loss: 1.6513 (1.6535)  loss_mask: 0.0728 (0.0829)  time: 0.1640  data: 0.0002  max mem: 4132
[18:18:07.975383] Epoch: [21]  [220/781]  eta: 0:01:34  lr: 0.000232  training_loss: 1.6752 (1.7337)  classification_loss: 1.6436 (1.6545)  loss_mask: 0.0418 (0.0792)  time: 0.1650  data: 0.0003  max mem: 4132
[18:18:11.233256] Epoch: [21]  [240/781]  eta: 0:01:30  lr: 0.000232  training_loss: 1.6454 (1.7283)  classification_loss: 1.6086 (1.6516)  loss_mask: 0.0291 (0.0768)  time: 0.1628  data: 0.0003  max mem: 4132
[18:18:14.491337] Epoch: [21]  [260/781]  eta: 0:01:27  lr: 0.000232  training_loss: 1.7460 (1.7298)  classification_loss: 1.6430 (1.6507)  loss_mask: 0.0740 (0.0791)  time: 0.1628  data: 0.0003  max mem: 4132
[18:18:17.760568] Epoch: [21]  [280/781]  eta: 0:01:23  lr: 0.000232  training_loss: 1.7265 (1.7300)  classification_loss: 1.6819 (1.6531)  loss_mask: 0.0346 (0.0769)  time: 0.1634  data: 0.0002  max mem: 4132
[18:18:21.096770] Epoch: [21]  [300/781]  eta: 0:01:20  lr: 0.000232  training_loss: 1.6937 (1.7278)  classification_loss: 1.6229 (1.6525)  loss_mask: 0.0467 (0.0752)  time: 0.1667  data: 0.0004  max mem: 4132
[18:18:24.383565] Epoch: [21]  [320/781]  eta: 0:01:16  lr: 0.000232  training_loss: 1.7470 (1.7290)  classification_loss: 1.6635 (1.6538)  loss_mask: 0.0578 (0.0753)  time: 0.1642  data: 0.0003  max mem: 4132
[18:18:27.665895] Epoch: [21]  [340/781]  eta: 0:01:13  lr: 0.000232  training_loss: 1.6836 (1.7259)  classification_loss: 1.6120 (1.6521)  loss_mask: 0.0440 (0.0738)  time: 0.1640  data: 0.0003  max mem: 4132
[18:18:30.935565] Epoch: [21]  [360/781]  eta: 0:01:10  lr: 0.000232  training_loss: 1.7262 (1.7256)  classification_loss: 1.6463 (1.6524)  loss_mask: 0.0524 (0.0732)  time: 0.1634  data: 0.0002  max mem: 4132
[18:18:34.202366] Epoch: [21]  [380/781]  eta: 0:01:06  lr: 0.000232  training_loss: 1.7285 (1.7263)  classification_loss: 1.6863 (1.6545)  loss_mask: 0.0365 (0.0718)  time: 0.1633  data: 0.0002  max mem: 4132
[18:18:37.529779] Epoch: [21]  [400/781]  eta: 0:01:03  lr: 0.000232  training_loss: 1.6498 (1.7237)  classification_loss: 1.6259 (1.6540)  loss_mask: 0.0232 (0.0697)  time: 0.1663  data: 0.0003  max mem: 4132
[18:18:40.793477] Epoch: [21]  [420/781]  eta: 0:00:59  lr: 0.000232  training_loss: 1.6598 (1.7213)  classification_loss: 1.6335 (1.6538)  loss_mask: 0.0190 (0.0676)  time: 0.1631  data: 0.0002  max mem: 4132
[18:18:44.072304] Epoch: [21]  [440/781]  eta: 0:00:56  lr: 0.000232  training_loss: 1.6499 (1.7179)  classification_loss: 1.5821 (1.6515)  loss_mask: 0.0340 (0.0664)  time: 0.1639  data: 0.0004  max mem: 4132
[18:18:47.354585] Epoch: [21]  [460/781]  eta: 0:00:53  lr: 0.000232  training_loss: 1.8296 (1.7220)  classification_loss: 1.6470 (1.6511)  loss_mask: 0.1521 (0.0709)  time: 0.1640  data: 0.0003  max mem: 4132
[18:18:50.634922] Epoch: [21]  [480/781]  eta: 0:00:49  lr: 0.000232  training_loss: 1.7058 (1.7216)  classification_loss: 1.6720 (1.6514)  loss_mask: 0.0372 (0.0702)  time: 0.1639  data: 0.0003  max mem: 4132
[18:18:53.894682] Epoch: [21]  [500/781]  eta: 0:00:46  lr: 0.000232  training_loss: 1.6947 (1.7200)  classification_loss: 1.6671 (1.6512)  loss_mask: 0.0287 (0.0688)  time: 0.1629  data: 0.0003  max mem: 4132
[18:18:57.220300] Epoch: [21]  [520/781]  eta: 0:00:43  lr: 0.000232  training_loss: 1.6511 (1.7183)  classification_loss: 1.5920 (1.6499)  loss_mask: 0.0390 (0.0685)  time: 0.1662  data: 0.0003  max mem: 4132
[18:19:00.557689] Epoch: [21]  [540/781]  eta: 0:00:39  lr: 0.000232  training_loss: 1.7264 (1.7204)  classification_loss: 1.6474 (1.6508)  loss_mask: 0.0850 (0.0696)  time: 0.1668  data: 0.0003  max mem: 4132
[18:19:03.875779] Epoch: [21]  [560/781]  eta: 0:00:36  lr: 0.000231  training_loss: 1.7356 (1.7206)  classification_loss: 1.5911 (1.6498)  loss_mask: 0.0899 (0.0708)  time: 0.1658  data: 0.0003  max mem: 4132
[18:19:07.190742] Epoch: [21]  [580/781]  eta: 0:00:33  lr: 0.000231  training_loss: 1.7175 (1.7197)  classification_loss: 1.6792 (1.6496)  loss_mask: 0.0460 (0.0701)  time: 0.1656  data: 0.0003  max mem: 4132
[18:19:10.502079] Epoch: [21]  [600/781]  eta: 0:00:30  lr: 0.000231  training_loss: 1.6416 (1.7177)  classification_loss: 1.6121 (1.6491)  loss_mask: 0.0242 (0.0687)  time: 0.1654  data: 0.0004  max mem: 4132
[18:19:13.836765] Epoch: [21]  [620/781]  eta: 0:00:26  lr: 0.000231  training_loss: 1.6372 (1.7152)  classification_loss: 1.6221 (1.6481)  loss_mask: 0.0186 (0.0671)  time: 0.1666  data: 0.0003  max mem: 4132
[18:19:17.168010] Epoch: [21]  [640/781]  eta: 0:00:23  lr: 0.000231  training_loss: 1.6210 (1.7123)  classification_loss: 1.6053 (1.6468)  loss_mask: 0.0143 (0.0655)  time: 0.1665  data: 0.0004  max mem: 4132
[18:19:20.530292] Epoch: [21]  [660/781]  eta: 0:00:20  lr: 0.000231  training_loss: 1.6990 (1.7112)  classification_loss: 1.6769 (1.6471)  loss_mask: 0.0164 (0.0640)  time: 0.1680  data: 0.0004  max mem: 4132
[18:19:23.848875] Epoch: [21]  [680/781]  eta: 0:00:16  lr: 0.000231  training_loss: 1.6602 (1.7104)  classification_loss: 1.6329 (1.6474)  loss_mask: 0.0208 (0.0629)  time: 0.1658  data: 0.0003  max mem: 4132
[18:19:27.213098] Epoch: [21]  [700/781]  eta: 0:00:13  lr: 0.000231  training_loss: 1.6605 (1.7094)  classification_loss: 1.6339 (1.6475)  loss_mask: 0.0209 (0.0619)  time: 0.1681  data: 0.0003  max mem: 4132
[18:19:30.579521] Epoch: [21]  [720/781]  eta: 0:00:10  lr: 0.000231  training_loss: 1.6915 (1.7086)  classification_loss: 1.6164 (1.6472)  loss_mask: 0.0303 (0.0614)  time: 0.1682  data: 0.0003  max mem: 4132
[18:19:33.969023] Epoch: [21]  [740/781]  eta: 0:00:06  lr: 0.000231  training_loss: 1.7172 (1.7093)  classification_loss: 1.6315 (1.6469)  loss_mask: 0.0659 (0.0623)  time: 0.1694  data: 0.0004  max mem: 4132
[18:19:37.300898] Epoch: [21]  [760/781]  eta: 0:00:03  lr: 0.000231  training_loss: 1.8059 (1.7113)  classification_loss: 1.6184 (1.6471)  loss_mask: 0.1041 (0.0642)  time: 0.1665  data: 0.0004  max mem: 4132
[18:19:40.583835] Epoch: [21]  [780/781]  eta: 0:00:00  lr: 0.000231  training_loss: 1.7273 (1.7119)  classification_loss: 1.6812 (1.6477)  loss_mask: 0.0504 (0.0641)  time: 0.1641  data: 0.0002  max mem: 4132
[18:19:40.768962] Epoch: [21] Total time: 0:02:09 (0.1664 s / it)
[18:19:40.769487] Averaged stats: lr: 0.000231  training_loss: 1.7273 (1.7119)  classification_loss: 1.6812 (1.6477)  loss_mask: 0.0504 (0.0641)
[18:19:41.469752] Test:  [  0/157]  eta: 0:01:49  testing_loss: 0.8792 (0.8792)  acc1: 76.5625 (76.5625)  acc5: 96.8750 (96.8750)  time: 0.6962  data: 0.6465  max mem: 4132
[18:19:41.783528] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.0145 (1.0169)  acc1: 68.7500 (67.6136)  acc5: 98.4375 (98.0114)  time: 0.0916  data: 0.0596  max mem: 4132
[18:19:42.073632] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.9813 (0.9893)  acc1: 68.7500 (68.3036)  acc5: 98.4375 (98.2887)  time: 0.0300  data: 0.0006  max mem: 4132
[18:19:42.363827] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.9614 (0.9903)  acc1: 68.7500 (68.1956)  acc5: 96.8750 (97.5806)  time: 0.0288  data: 0.0002  max mem: 4132
[18:19:42.656496] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.9676 (0.9878)  acc1: 67.1875 (67.9878)  acc5: 96.8750 (97.5229)  time: 0.0290  data: 0.0002  max mem: 4132
[18:19:42.952659] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.9676 (0.9860)  acc1: 67.1875 (68.0453)  acc5: 96.8750 (97.5184)  time: 0.0292  data: 0.0002  max mem: 4132
[18:19:43.254971] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.9676 (0.9839)  acc1: 67.1875 (68.1609)  acc5: 96.8750 (97.3361)  time: 0.0296  data: 0.0003  max mem: 4132
[18:19:43.545821] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.9296 (0.9756)  acc1: 68.7500 (68.3759)  acc5: 98.4375 (97.4692)  time: 0.0294  data: 0.0003  max mem: 4132
[18:19:43.837841] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.9242 (0.9790)  acc1: 68.7500 (68.2292)  acc5: 98.4375 (97.3958)  time: 0.0290  data: 0.0003  max mem: 4132
[18:19:44.132635] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.9826 (0.9831)  acc1: 65.6250 (68.0804)  acc5: 96.8750 (97.3901)  time: 0.0292  data: 0.0003  max mem: 4132
[18:19:44.424477] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.0245 (0.9862)  acc1: 64.0625 (67.8063)  acc5: 98.4375 (97.5093)  time: 0.0292  data: 0.0003  max mem: 4132
[18:19:44.718363] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.0245 (0.9872)  acc1: 65.6250 (67.7083)  acc5: 98.4375 (97.5366)  time: 0.0290  data: 0.0002  max mem: 4132
[18:19:45.009065] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.9667 (0.9833)  acc1: 67.1875 (67.7169)  acc5: 96.8750 (97.5207)  time: 0.0289  data: 0.0002  max mem: 4132
[18:19:45.297047] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.9647 (0.9846)  acc1: 65.6250 (67.4022)  acc5: 96.8750 (97.5191)  time: 0.0286  data: 0.0002  max mem: 4132
[18:19:45.587591] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.9813 (0.9843)  acc1: 67.1875 (67.5199)  acc5: 96.8750 (97.5177)  time: 0.0287  data: 0.0002  max mem: 4132
[18:19:45.870688] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.9813 (0.9812)  acc1: 68.7500 (67.6945)  acc5: 96.8750 (97.5062)  time: 0.0285  data: 0.0002  max mem: 4132
[18:19:46.026068] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.9749 (0.9821)  acc1: 68.7500 (67.6500)  acc5: 98.4375 (97.5200)  time: 0.0276  data: 0.0002  max mem: 4132
[18:19:46.216950] Test: Total time: 0:00:05 (0.0347 s / it)
[18:19:46.217859] * Acc@1 67.650 Acc@5 97.520 loss 0.982
[18:19:46.218190] Accuracy of the network on the 10000 test images: 67.7%
[18:19:46.218394] Max accuracy: 67.65%
[18:19:46.516225] log_dir: ./output_dir
[18:19:47.539789] Epoch: [22]  [  0/781]  eta: 0:13:17  lr: 0.000231  training_loss: 1.5433 (1.5433)  classification_loss: 1.5067 (1.5067)  loss_mask: 0.0366 (0.0366)  time: 1.0212  data: 0.8479  max mem: 4132
[18:19:50.848431] Epoch: [22]  [ 20/781]  eta: 0:02:36  lr: 0.000231  training_loss: 1.6490 (1.6608)  classification_loss: 1.5923 (1.6078)  loss_mask: 0.0364 (0.0530)  time: 0.1653  data: 0.0003  max mem: 4132
[18:19:54.175667] Epoch: [22]  [ 40/781]  eta: 0:02:18  lr: 0.000231  training_loss: 1.6672 (1.6690)  classification_loss: 1.6491 (1.6223)  loss_mask: 0.0359 (0.0467)  time: 0.1663  data: 0.0003  max mem: 4132
[18:19:57.455412] Epoch: [22]  [ 60/781]  eta: 0:02:09  lr: 0.000231  training_loss: 1.7364 (1.6926)  classification_loss: 1.6448 (1.6333)  loss_mask: 0.0324 (0.0594)  time: 0.1639  data: 0.0003  max mem: 4132
[18:20:00.746768] Epoch: [22]  [ 80/781]  eta: 0:02:03  lr: 0.000231  training_loss: 1.8053 (1.7198)  classification_loss: 1.6424 (1.6390)  loss_mask: 0.0915 (0.0808)  time: 0.1645  data: 0.0002  max mem: 4132
[18:20:04.024963] Epoch: [22]  [100/781]  eta: 0:01:57  lr: 0.000231  training_loss: 1.7263 (1.7290)  classification_loss: 1.6207 (1.6365)  loss_mask: 0.1094 (0.0925)  time: 0.1638  data: 0.0003  max mem: 4132
[18:20:07.336447] Epoch: [22]  [120/781]  eta: 0:01:53  lr: 0.000231  training_loss: 1.6696 (1.7216)  classification_loss: 1.6040 (1.6335)  loss_mask: 0.0509 (0.0881)  time: 0.1655  data: 0.0004  max mem: 4132
[18:20:10.627971] Epoch: [22]  [140/781]  eta: 0:01:49  lr: 0.000230  training_loss: 1.6446 (1.7123)  classification_loss: 1.5903 (1.6292)  loss_mask: 0.0426 (0.0831)  time: 0.1645  data: 0.0004  max mem: 4132
[18:20:13.933134] Epoch: [22]  [160/781]  eta: 0:01:45  lr: 0.000230  training_loss: 1.6573 (1.7063)  classification_loss: 1.6090 (1.6295)  loss_mask: 0.0292 (0.0768)  time: 0.1652  data: 0.0002  max mem: 4132
[18:20:17.209076] Epoch: [22]  [180/781]  eta: 0:01:41  lr: 0.000230  training_loss: 1.6203 (1.6994)  classification_loss: 1.5895 (1.6281)  loss_mask: 0.0287 (0.0713)  time: 0.1637  data: 0.0003  max mem: 4132
[18:20:20.522604] Epoch: [22]  [200/781]  eta: 0:01:38  lr: 0.000230  training_loss: 1.6704 (1.6963)  classification_loss: 1.5736 (1.6239)  loss_mask: 0.0477 (0.0723)  time: 0.1656  data: 0.0003  max mem: 4132
[18:20:23.838203] Epoch: [22]  [220/781]  eta: 0:01:34  lr: 0.000230  training_loss: 1.7186 (1.6990)  classification_loss: 1.6584 (1.6268)  loss_mask: 0.0423 (0.0722)  time: 0.1657  data: 0.0002  max mem: 4132
[18:20:27.129490] Epoch: [22]  [240/781]  eta: 0:01:31  lr: 0.000230  training_loss: 1.6936 (1.6983)  classification_loss: 1.6442 (1.6284)  loss_mask: 0.0350 (0.0699)  time: 0.1645  data: 0.0003  max mem: 4132
[18:20:30.381003] Epoch: [22]  [260/781]  eta: 0:01:27  lr: 0.000230  training_loss: 1.6717 (1.6976)  classification_loss: 1.5944 (1.6275)  loss_mask: 0.0507 (0.0701)  time: 0.1625  data: 0.0002  max mem: 4132
[18:20:33.625967] Epoch: [22]  [280/781]  eta: 0:01:23  lr: 0.000230  training_loss: 1.6748 (1.6967)  classification_loss: 1.5982 (1.6262)  loss_mask: 0.0556 (0.0705)  time: 0.1622  data: 0.0002  max mem: 4132
[18:20:36.897241] Epoch: [22]  [300/781]  eta: 0:01:20  lr: 0.000230  training_loss: 1.6986 (1.6971)  classification_loss: 1.6410 (1.6285)  loss_mask: 0.0298 (0.0686)  time: 0.1635  data: 0.0002  max mem: 4132
[18:20:40.219342] Epoch: [22]  [320/781]  eta: 0:01:17  lr: 0.000230  training_loss: 1.6470 (1.6951)  classification_loss: 1.6263 (1.6293)  loss_mask: 0.0183 (0.0657)  time: 0.1660  data: 0.0003  max mem: 4132
[18:20:43.467933] Epoch: [22]  [340/781]  eta: 0:01:13  lr: 0.000230  training_loss: 1.6450 (1.6924)  classification_loss: 1.6156 (1.6289)  loss_mask: 0.0212 (0.0634)  time: 0.1623  data: 0.0003  max mem: 4132
[18:20:46.749995] Epoch: [22]  [360/781]  eta: 0:01:10  lr: 0.000230  training_loss: 1.6705 (1.6911)  classification_loss: 1.6372 (1.6292)  loss_mask: 0.0327 (0.0618)  time: 0.1640  data: 0.0003  max mem: 4132
[18:20:50.068176] Epoch: [22]  [380/781]  eta: 0:01:06  lr: 0.000230  training_loss: 1.6682 (1.6890)  classification_loss: 1.6353 (1.6290)  loss_mask: 0.0206 (0.0600)  time: 0.1658  data: 0.0003  max mem: 4132
[18:20:53.416964] Epoch: [22]  [400/781]  eta: 0:01:03  lr: 0.000230  training_loss: 1.6429 (1.6865)  classification_loss: 1.6348 (1.6286)  loss_mask: 0.0127 (0.0578)  time: 0.1673  data: 0.0003  max mem: 4132
[18:20:56.752041] Epoch: [22]  [420/781]  eta: 0:01:00  lr: 0.000230  training_loss: 1.6350 (1.6846)  classification_loss: 1.6268 (1.6284)  loss_mask: 0.0187 (0.0562)  time: 0.1667  data: 0.0003  max mem: 4132
[18:21:00.063485] Epoch: [22]  [440/781]  eta: 0:00:56  lr: 0.000230  training_loss: 1.6400 (1.6829)  classification_loss: 1.5966 (1.6275)  loss_mask: 0.0248 (0.0555)  time: 0.1655  data: 0.0003  max mem: 4132
[18:21:03.403125] Epoch: [22]  [460/781]  eta: 0:00:53  lr: 0.000230  training_loss: 1.6612 (1.6816)  classification_loss: 1.6335 (1.6273)  loss_mask: 0.0233 (0.0543)  time: 0.1669  data: 0.0005  max mem: 4132
[18:21:06.743975] Epoch: [22]  [480/781]  eta: 0:00:50  lr: 0.000229  training_loss: 1.6754 (1.6809)  classification_loss: 1.6388 (1.6281)  loss_mask: 0.0163 (0.0529)  time: 0.1669  data: 0.0003  max mem: 4132
[18:21:10.083253] Epoch: [22]  [500/781]  eta: 0:00:46  lr: 0.000229  training_loss: 1.5958 (1.6793)  classification_loss: 1.5832 (1.6277)  loss_mask: 0.0133 (0.0516)  time: 0.1669  data: 0.0003  max mem: 4132
[18:21:13.401891] Epoch: [22]  [520/781]  eta: 0:00:43  lr: 0.000229  training_loss: 1.6768 (1.6790)  classification_loss: 1.6143 (1.6273)  loss_mask: 0.0460 (0.0517)  time: 0.1658  data: 0.0003  max mem: 4132
[18:21:16.705981] Epoch: [22]  [540/781]  eta: 0:00:40  lr: 0.000229  training_loss: 1.7390 (1.6818)  classification_loss: 1.6755 (1.6289)  loss_mask: 0.0459 (0.0529)  time: 0.1651  data: 0.0002  max mem: 4132
[18:21:19.997999] Epoch: [22]  [560/781]  eta: 0:00:36  lr: 0.000229  training_loss: 1.7282 (1.6837)  classification_loss: 1.6126 (1.6278)  loss_mask: 0.1066 (0.0559)  time: 0.1645  data: 0.0003  max mem: 4132
[18:21:23.297424] Epoch: [22]  [580/781]  eta: 0:00:33  lr: 0.000229  training_loss: 1.7239 (1.6850)  classification_loss: 1.6224 (1.6283)  loss_mask: 0.0656 (0.0567)  time: 0.1649  data: 0.0002  max mem: 4132
[18:21:26.622069] Epoch: [22]  [600/781]  eta: 0:00:30  lr: 0.000229  training_loss: 1.6986 (1.6863)  classification_loss: 1.6144 (1.6283)  loss_mask: 0.0730 (0.0580)  time: 0.1661  data: 0.0002  max mem: 4132
[18:21:29.931828] Epoch: [22]  [620/781]  eta: 0:00:26  lr: 0.000229  training_loss: 1.6137 (1.6849)  classification_loss: 1.5864 (1.6276)  loss_mask: 0.0357 (0.0573)  time: 0.1654  data: 0.0002  max mem: 4132
[18:21:33.230142] Epoch: [22]  [640/781]  eta: 0:00:23  lr: 0.000229  training_loss: 1.6507 (1.6843)  classification_loss: 1.6189 (1.6274)  loss_mask: 0.0341 (0.0568)  time: 0.1648  data: 0.0002  max mem: 4132
[18:21:36.527896] Epoch: [22]  [660/781]  eta: 0:00:20  lr: 0.000229  training_loss: 1.6618 (1.6841)  classification_loss: 1.6041 (1.6277)  loss_mask: 0.0411 (0.0564)  time: 0.1648  data: 0.0002  max mem: 4132
[18:21:39.857505] Epoch: [22]  [680/781]  eta: 0:00:16  lr: 0.000229  training_loss: 1.6463 (1.6822)  classification_loss: 1.6163 (1.6269)  loss_mask: 0.0221 (0.0553)  time: 0.1664  data: 0.0004  max mem: 4132
[18:21:43.166467] Epoch: [22]  [700/781]  eta: 0:00:13  lr: 0.000229  training_loss: 1.6610 (1.6803)  classification_loss: 1.6361 (1.6261)  loss_mask: 0.0183 (0.0543)  time: 0.1654  data: 0.0002  max mem: 4132
[18:21:46.428740] Epoch: [22]  [720/781]  eta: 0:00:10  lr: 0.000229  training_loss: 1.6527 (1.6803)  classification_loss: 1.5960 (1.6260)  loss_mask: 0.0469 (0.0543)  time: 0.1630  data: 0.0003  max mem: 4132
[18:21:49.738489] Epoch: [22]  [740/781]  eta: 0:00:06  lr: 0.000229  training_loss: 1.6900 (1.6801)  classification_loss: 1.6319 (1.6261)  loss_mask: 0.0315 (0.0540)  time: 0.1654  data: 0.0003  max mem: 4132
[18:21:53.021052] Epoch: [22]  [760/781]  eta: 0:00:03  lr: 0.000229  training_loss: 1.6390 (1.6798)  classification_loss: 1.6181 (1.6264)  loss_mask: 0.0250 (0.0534)  time: 0.1640  data: 0.0003  max mem: 4132
[18:21:56.312731] Epoch: [22]  [780/781]  eta: 0:00:00  lr: 0.000229  training_loss: 1.6346 (1.6796)  classification_loss: 1.5992 (1.6270)  loss_mask: 0.0155 (0.0526)  time: 0.1645  data: 0.0002  max mem: 4132
[18:21:56.490250] Epoch: [22] Total time: 0:02:09 (0.1664 s / it)
[18:21:56.491816] Averaged stats: lr: 0.000229  training_loss: 1.6346 (1.6796)  classification_loss: 1.5992 (1.6270)  loss_mask: 0.0155 (0.0526)
[18:21:57.217901] Test:  [  0/157]  eta: 0:01:53  testing_loss: 0.9496 (0.9496)  acc1: 68.7500 (68.7500)  acc5: 96.8750 (96.8750)  time: 0.7216  data: 0.6812  max mem: 4132
[18:21:57.511002] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.9496 (0.9668)  acc1: 68.7500 (67.6136)  acc5: 98.4375 (98.5795)  time: 0.0920  data: 0.0623  max mem: 4132
[18:21:57.799233] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.9040 (0.9353)  acc1: 68.7500 (69.1964)  acc5: 98.4375 (98.6607)  time: 0.0288  data: 0.0003  max mem: 4132
[18:21:58.092168] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.9339 (0.9533)  acc1: 68.7500 (68.2964)  acc5: 98.4375 (97.7319)  time: 0.0289  data: 0.0003  max mem: 4132
[18:21:58.381109] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.9622 (0.9552)  acc1: 68.7500 (68.4070)  acc5: 96.8750 (97.7134)  time: 0.0289  data: 0.0003  max mem: 4132
[18:21:58.677341] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.9622 (0.9562)  acc1: 70.3125 (68.5355)  acc5: 98.4375 (97.6409)  time: 0.0291  data: 0.0002  max mem: 4132
[18:21:58.969676] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.9578 (0.9566)  acc1: 70.3125 (68.4682)  acc5: 96.8750 (97.5666)  time: 0.0292  data: 0.0002  max mem: 4132
[18:21:59.261822] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.9260 (0.9483)  acc1: 70.3125 (68.8380)  acc5: 98.4375 (97.6893)  time: 0.0290  data: 0.0002  max mem: 4132
[18:21:59.551146] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.9370 (0.9543)  acc1: 70.3125 (68.6535)  acc5: 96.8750 (97.5309)  time: 0.0289  data: 0.0002  max mem: 4132
[18:21:59.838024] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.9745 (0.9579)  acc1: 67.1875 (68.3379)  acc5: 96.8750 (97.5962)  time: 0.0286  data: 0.0002  max mem: 4132
[18:22:00.129520] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.0150 (0.9624)  acc1: 65.6250 (68.1621)  acc5: 98.4375 (97.6485)  time: 0.0287  data: 0.0003  max mem: 4132
[18:22:00.428564] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.0014 (0.9643)  acc1: 67.1875 (68.0884)  acc5: 98.4375 (97.6492)  time: 0.0293  data: 0.0003  max mem: 4132
[18:22:00.716723] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.9649 (0.9599)  acc1: 67.1875 (68.1818)  acc5: 98.4375 (97.7014)  time: 0.0292  data: 0.0002  max mem: 4132
[18:22:01.005402] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.9464 (0.9610)  acc1: 68.7500 (68.0463)  acc5: 98.4375 (97.6503)  time: 0.0286  data: 0.0002  max mem: 4132
[18:22:01.291611] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.9583 (0.9591)  acc1: 68.7500 (68.1959)  acc5: 96.8750 (97.6729)  time: 0.0285  data: 0.0002  max mem: 4132
[18:22:01.576925] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.9365 (0.9555)  acc1: 70.3125 (68.4292)  acc5: 98.4375 (97.6718)  time: 0.0284  data: 0.0002  max mem: 4132
[18:22:01.731967] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.9068 (0.9579)  acc1: 70.3125 (68.3900)  acc5: 98.4375 (97.6700)  time: 0.0275  data: 0.0002  max mem: 4132
[18:22:01.885246] Test: Total time: 0:00:05 (0.0343 s / it)
[18:22:01.885903] * Acc@1 68.390 Acc@5 97.670 loss 0.958
[18:22:01.886314] Accuracy of the network on the 10000 test images: 68.4%
[18:22:01.886629] Max accuracy: 68.39%
[18:22:02.203339] log_dir: ./output_dir
[18:22:03.168916] Epoch: [23]  [  0/781]  eta: 0:12:32  lr: 0.000229  training_loss: 1.5423 (1.5423)  classification_loss: 1.5089 (1.5089)  loss_mask: 0.0334 (0.0334)  time: 0.9635  data: 0.7362  max mem: 4132
[18:22:06.455027] Epoch: [23]  [ 20/781]  eta: 0:02:33  lr: 0.000229  training_loss: 1.6505 (1.6630)  classification_loss: 1.6074 (1.6096)  loss_mask: 0.0537 (0.0534)  time: 0.1642  data: 0.0002  max mem: 4132
[18:22:09.747246] Epoch: [23]  [ 40/781]  eta: 0:02:16  lr: 0.000228  training_loss: 1.6420 (1.6593)  classification_loss: 1.6008 (1.6141)  loss_mask: 0.0260 (0.0452)  time: 0.1645  data: 0.0002  max mem: 4132
[18:22:13.054061] Epoch: [23]  [ 60/781]  eta: 0:02:08  lr: 0.000228  training_loss: 1.6508 (1.6682)  classification_loss: 1.6398 (1.6318)  loss_mask: 0.0158 (0.0363)  time: 0.1653  data: 0.0002  max mem: 4132
[18:22:16.389696] Epoch: [23]  [ 80/781]  eta: 0:02:02  lr: 0.000228  training_loss: 1.6112 (1.6577)  classification_loss: 1.5994 (1.6257)  loss_mask: 0.0155 (0.0320)  time: 0.1666  data: 0.0002  max mem: 4132
[18:22:19.663887] Epoch: [23]  [100/781]  eta: 0:01:57  lr: 0.000228  training_loss: 1.6035 (1.6538)  classification_loss: 1.5815 (1.6237)  loss_mask: 0.0167 (0.0301)  time: 0.1636  data: 0.0003  max mem: 4132
[18:22:22.947801] Epoch: [23]  [120/781]  eta: 0:01:53  lr: 0.000228  training_loss: 1.6378 (1.6494)  classification_loss: 1.6075 (1.6200)  loss_mask: 0.0188 (0.0294)  time: 0.1640  data: 0.0002  max mem: 4132
[18:22:26.230264] Epoch: [23]  [140/781]  eta: 0:01:49  lr: 0.000228  training_loss: 1.6210 (1.6460)  classification_loss: 1.6084 (1.6169)  loss_mask: 0.0123 (0.0290)  time: 0.1640  data: 0.0003  max mem: 4132
[18:22:29.519609] Epoch: [23]  [160/781]  eta: 0:01:45  lr: 0.000228  training_loss: 1.8180 (1.6649)  classification_loss: 1.6543 (1.6218)  loss_mask: 0.0741 (0.0431)  time: 0.1644  data: 0.0003  max mem: 4132
[18:22:32.804549] Epoch: [23]  [180/781]  eta: 0:01:41  lr: 0.000228  training_loss: 1.7425 (1.6716)  classification_loss: 1.6046 (1.6226)  loss_mask: 0.0358 (0.0490)  time: 0.1641  data: 0.0003  max mem: 4132
[18:22:36.078378] Epoch: [23]  [200/781]  eta: 0:01:37  lr: 0.000228  training_loss: 1.6521 (1.6730)  classification_loss: 1.6130 (1.6223)  loss_mask: 0.0465 (0.0507)  time: 0.1636  data: 0.0002  max mem: 4132
[18:22:39.424456] Epoch: [23]  [220/781]  eta: 0:01:34  lr: 0.000228  training_loss: 1.6587 (1.6720)  classification_loss: 1.6367 (1.6234)  loss_mask: 0.0222 (0.0487)  time: 0.1672  data: 0.0004  max mem: 4132
[18:22:42.749416] Epoch: [23]  [240/781]  eta: 0:01:30  lr: 0.000228  training_loss: 1.6849 (1.6718)  classification_loss: 1.6578 (1.6252)  loss_mask: 0.0225 (0.0466)  time: 0.1662  data: 0.0003  max mem: 4132
[18:22:46.094473] Epoch: [23]  [260/781]  eta: 0:01:27  lr: 0.000228  training_loss: 1.6644 (1.6693)  classification_loss: 1.6104 (1.6244)  loss_mask: 0.0178 (0.0449)  time: 0.1672  data: 0.0003  max mem: 4132
[18:22:49.390112] Epoch: [23]  [280/781]  eta: 0:01:24  lr: 0.000228  training_loss: 1.6148 (1.6674)  classification_loss: 1.6045 (1.6239)  loss_mask: 0.0152 (0.0434)  time: 0.1647  data: 0.0003  max mem: 4132
[18:22:52.672853] Epoch: [23]  [300/781]  eta: 0:01:20  lr: 0.000228  training_loss: 1.6941 (1.6701)  classification_loss: 1.6487 (1.6254)  loss_mask: 0.0209 (0.0447)  time: 0.1640  data: 0.0003  max mem: 4132
[18:22:55.987465] Epoch: [23]  [320/781]  eta: 0:01:17  lr: 0.000228  training_loss: 1.7574 (1.6760)  classification_loss: 1.6309 (1.6259)  loss_mask: 0.0957 (0.0501)  time: 0.1656  data: 0.0002  max mem: 4132
[18:22:59.301394] Epoch: [23]  [340/781]  eta: 0:01:13  lr: 0.000228  training_loss: 1.6406 (1.6747)  classification_loss: 1.5952 (1.6234)  loss_mask: 0.0517 (0.0513)  time: 0.1656  data: 0.0003  max mem: 4132
[18:23:02.618789] Epoch: [23]  [360/781]  eta: 0:01:10  lr: 0.000228  training_loss: 1.6785 (1.6748)  classification_loss: 1.6273 (1.6241)  loss_mask: 0.0336 (0.0507)  time: 0.1658  data: 0.0003  max mem: 4132
[18:23:05.894028] Epoch: [23]  [380/781]  eta: 0:01:06  lr: 0.000227  training_loss: 1.6355 (1.6736)  classification_loss: 1.6190 (1.6229)  loss_mask: 0.0396 (0.0507)  time: 0.1637  data: 0.0002  max mem: 4132
[18:23:09.218054] Epoch: [23]  [400/781]  eta: 0:01:03  lr: 0.000227  training_loss: 1.6001 (1.6719)  classification_loss: 1.5780 (1.6223)  loss_mask: 0.0270 (0.0496)  time: 0.1661  data: 0.0003  max mem: 4132
[18:23:12.527640] Epoch: [23]  [420/781]  eta: 0:01:00  lr: 0.000227  training_loss: 1.6202 (1.6701)  classification_loss: 1.6030 (1.6218)  loss_mask: 0.0161 (0.0483)  time: 0.1654  data: 0.0003  max mem: 4132
[18:23:15.813756] Epoch: [23]  [440/781]  eta: 0:00:56  lr: 0.000227  training_loss: 1.6443 (1.6691)  classification_loss: 1.6320 (1.6220)  loss_mask: 0.0200 (0.0471)  time: 0.1642  data: 0.0003  max mem: 4132
[18:23:19.088380] Epoch: [23]  [460/781]  eta: 0:00:53  lr: 0.000227  training_loss: 1.6562 (1.6688)  classification_loss: 1.6435 (1.6225)  loss_mask: 0.0189 (0.0463)  time: 0.1636  data: 0.0002  max mem: 4132
[18:23:22.375095] Epoch: [23]  [480/781]  eta: 0:00:50  lr: 0.000227  training_loss: 1.6390 (1.6679)  classification_loss: 1.5956 (1.6221)  loss_mask: 0.0203 (0.0459)  time: 0.1642  data: 0.0002  max mem: 4132
[18:23:25.684064] Epoch: [23]  [500/781]  eta: 0:00:46  lr: 0.000227  training_loss: 1.6723 (1.6688)  classification_loss: 1.6330 (1.6218)  loss_mask: 0.0388 (0.0470)  time: 0.1653  data: 0.0007  max mem: 4132
[18:23:29.006020] Epoch: [23]  [520/781]  eta: 0:00:43  lr: 0.000227  training_loss: 1.6617 (1.6694)  classification_loss: 1.6068 (1.6215)  loss_mask: 0.0429 (0.0479)  time: 0.1660  data: 0.0003  max mem: 4132
[18:23:32.293455] Epoch: [23]  [540/781]  eta: 0:00:40  lr: 0.000227  training_loss: 1.6308 (1.6685)  classification_loss: 1.6061 (1.6214)  loss_mask: 0.0249 (0.0472)  time: 0.1643  data: 0.0003  max mem: 4132
[18:23:35.604093] Epoch: [23]  [560/781]  eta: 0:00:36  lr: 0.000227  training_loss: 1.6236 (1.6680)  classification_loss: 1.5827 (1.6211)  loss_mask: 0.0372 (0.0470)  time: 0.1654  data: 0.0003  max mem: 4132
[18:23:38.874578] Epoch: [23]  [580/781]  eta: 0:00:33  lr: 0.000227  training_loss: 1.6069 (1.6663)  classification_loss: 1.5872 (1.6200)  loss_mask: 0.0196 (0.0462)  time: 0.1634  data: 0.0003  max mem: 4132
[18:23:42.192173] Epoch: [23]  [600/781]  eta: 0:00:30  lr: 0.000227  training_loss: 1.6159 (1.6653)  classification_loss: 1.5708 (1.6196)  loss_mask: 0.0169 (0.0457)  time: 0.1658  data: 0.0003  max mem: 4132
[18:23:45.483167] Epoch: [23]  [620/781]  eta: 0:00:26  lr: 0.000227  training_loss: 1.7198 (1.6673)  classification_loss: 1.6340 (1.6205)  loss_mask: 0.0730 (0.0468)  time: 0.1645  data: 0.0002  max mem: 4132
[18:23:48.770628] Epoch: [23]  [640/781]  eta: 0:00:23  lr: 0.000227  training_loss: 1.6246 (1.6664)  classification_loss: 1.5790 (1.6198)  loss_mask: 0.0248 (0.0467)  time: 0.1643  data: 0.0003  max mem: 4132
[18:23:52.061134] Epoch: [23]  [660/781]  eta: 0:00:20  lr: 0.000227  training_loss: 1.6093 (1.6653)  classification_loss: 1.5896 (1.6196)  loss_mask: 0.0166 (0.0458)  time: 0.1644  data: 0.0002  max mem: 4132
[18:23:55.349717] Epoch: [23]  [680/781]  eta: 0:00:16  lr: 0.000227  training_loss: 1.6067 (1.6641)  classification_loss: 1.5939 (1.6192)  loss_mask: 0.0142 (0.0449)  time: 0.1643  data: 0.0003  max mem: 4132
[18:23:58.640903] Epoch: [23]  [700/781]  eta: 0:00:13  lr: 0.000226  training_loss: 1.5709 (1.6628)  classification_loss: 1.5651 (1.6187)  loss_mask: 0.0109 (0.0441)  time: 0.1645  data: 0.0003  max mem: 4132
[18:24:01.902030] Epoch: [23]  [720/781]  eta: 0:00:10  lr: 0.000226  training_loss: 1.6674 (1.6630)  classification_loss: 1.6166 (1.6189)  loss_mask: 0.0338 (0.0442)  time: 0.1630  data: 0.0002  max mem: 4132
[18:24:05.191129] Epoch: [23]  [740/781]  eta: 0:00:06  lr: 0.000226  training_loss: 1.6178 (1.6617)  classification_loss: 1.5869 (1.6181)  loss_mask: 0.0169 (0.0436)  time: 0.1644  data: 0.0003  max mem: 4132
[18:24:08.464806] Epoch: [23]  [760/781]  eta: 0:00:03  lr: 0.000226  training_loss: 1.6163 (1.6608)  classification_loss: 1.6056 (1.6179)  loss_mask: 0.0137 (0.0429)  time: 0.1636  data: 0.0002  max mem: 4132
[18:24:11.713226] Epoch: [23]  [780/781]  eta: 0:00:00  lr: 0.000226  training_loss: 1.6231 (1.6605)  classification_loss: 1.6044 (1.6184)  loss_mask: 0.0106 (0.0421)  time: 0.1623  data: 0.0002  max mem: 4132
[18:24:11.881455] Epoch: [23] Total time: 0:02:09 (0.1660 s / it)
[18:24:11.882303] Averaged stats: lr: 0.000226  training_loss: 1.6231 (1.6605)  classification_loss: 1.6044 (1.6184)  loss_mask: 0.0106 (0.0421)
[18:24:12.643299] Test:  [  0/157]  eta: 0:01:58  testing_loss: 0.9554 (0.9554)  acc1: 70.3125 (70.3125)  acc5: 92.1875 (92.1875)  time: 0.7560  data: 0.7264  max mem: 4132
[18:24:12.941668] Test:  [ 10/157]  eta: 0:00:14  testing_loss: 0.9554 (0.9545)  acc1: 67.1875 (67.4716)  acc5: 96.8750 (97.4432)  time: 0.0957  data: 0.0664  max mem: 4132
[18:24:13.232365] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.9370 (0.9361)  acc1: 65.6250 (67.9315)  acc5: 98.4375 (98.1399)  time: 0.0293  data: 0.0004  max mem: 4132
[18:24:13.524160] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.9402 (0.9490)  acc1: 65.6250 (66.9355)  acc5: 98.4375 (97.7319)  time: 0.0290  data: 0.0004  max mem: 4132
[18:24:13.810190] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.9381 (0.9465)  acc1: 67.1875 (67.2637)  acc5: 96.8750 (97.6753)  time: 0.0287  data: 0.0002  max mem: 4132
[18:24:14.101239] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.9412 (0.9443)  acc1: 68.7500 (67.7390)  acc5: 96.8750 (97.4877)  time: 0.0287  data: 0.0002  max mem: 4132
[18:24:14.398534] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.9412 (0.9390)  acc1: 70.3125 (68.0072)  acc5: 96.8750 (97.4898)  time: 0.0292  data: 0.0002  max mem: 4132
[18:24:14.696237] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.8848 (0.9334)  acc1: 68.7500 (68.1338)  acc5: 98.4375 (97.6012)  time: 0.0296  data: 0.0003  max mem: 4132
[18:24:14.993946] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.9077 (0.9371)  acc1: 68.7500 (68.1713)  acc5: 96.8750 (97.5309)  time: 0.0295  data: 0.0003  max mem: 4132
[18:24:15.287988] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.9555 (0.9389)  acc1: 68.7500 (68.1490)  acc5: 96.8750 (97.5618)  time: 0.0292  data: 0.0002  max mem: 4132
[18:24:15.578748] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.9556 (0.9442)  acc1: 67.1875 (67.8218)  acc5: 98.4375 (97.6021)  time: 0.0290  data: 0.0003  max mem: 4132
[18:24:15.874486] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.9556 (0.9422)  acc1: 67.1875 (68.0462)  acc5: 98.4375 (97.6633)  time: 0.0292  data: 0.0003  max mem: 4132
[18:24:16.168829] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.8956 (0.9375)  acc1: 71.8750 (68.3497)  acc5: 98.4375 (97.7014)  time: 0.0293  data: 0.0003  max mem: 4132
[18:24:16.462069] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.9036 (0.9391)  acc1: 68.7500 (68.1775)  acc5: 98.4375 (97.6861)  time: 0.0292  data: 0.0003  max mem: 4132
[18:24:16.752077] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.9822 (0.9403)  acc1: 67.1875 (68.1627)  acc5: 96.8750 (97.6396)  time: 0.0289  data: 0.0003  max mem: 4132
[18:24:17.039236] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.9683 (0.9378)  acc1: 68.7500 (68.5017)  acc5: 96.8750 (97.5373)  time: 0.0286  data: 0.0002  max mem: 4132
[18:24:17.193742] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.9308 (0.9390)  acc1: 68.7500 (68.4500)  acc5: 96.8750 (97.5400)  time: 0.0276  data: 0.0002  max mem: 4132
[18:24:17.374376] Test: Total time: 0:00:05 (0.0350 s / it)
[18:24:17.375107] * Acc@1 68.450 Acc@5 97.540 loss 0.939
[18:24:17.375645] Accuracy of the network on the 10000 test images: 68.5%
[18:24:17.376250] Max accuracy: 68.45%
[18:24:17.592384] log_dir: ./output_dir
[18:24:18.572778] Epoch: [24]  [  0/781]  eta: 0:12:43  lr: 0.000226  training_loss: 1.5056 (1.5056)  classification_loss: 1.4913 (1.4913)  loss_mask: 0.0143 (0.0143)  time: 0.9780  data: 0.7530  max mem: 4132
[18:24:21.920611] Epoch: [24]  [ 20/781]  eta: 0:02:36  lr: 0.000226  training_loss: 1.6701 (1.6683)  classification_loss: 1.5601 (1.5927)  loss_mask: 0.0579 (0.0756)  time: 0.1673  data: 0.0003  max mem: 4132
[18:24:25.239663] Epoch: [24]  [ 40/781]  eta: 0:02:18  lr: 0.000226  training_loss: 1.7854 (1.7178)  classification_loss: 1.6112 (1.6015)  loss_mask: 0.1298 (0.1163)  time: 0.1659  data: 0.0003  max mem: 4132
[18:24:28.546580] Epoch: [24]  [ 60/781]  eta: 0:02:09  lr: 0.000226  training_loss: 1.6581 (1.7012)  classification_loss: 1.6195 (1.6093)  loss_mask: 0.0316 (0.0920)  time: 0.1652  data: 0.0003  max mem: 4132
[18:24:31.850408] Epoch: [24]  [ 80/781]  eta: 0:02:03  lr: 0.000226  training_loss: 1.6131 (1.6811)  classification_loss: 1.5873 (1.6064)  loss_mask: 0.0196 (0.0747)  time: 0.1651  data: 0.0003  max mem: 4132
[18:24:35.175159] Epoch: [24]  [100/781]  eta: 0:01:58  lr: 0.000226  training_loss: 1.6180 (1.6732)  classification_loss: 1.5936 (1.6082)  loss_mask: 0.0172 (0.0650)  time: 0.1661  data: 0.0003  max mem: 4132
[18:24:38.472893] Epoch: [24]  [120/781]  eta: 0:01:53  lr: 0.000226  training_loss: 1.6505 (1.6705)  classification_loss: 1.5624 (1.6041)  loss_mask: 0.0476 (0.0664)  time: 0.1648  data: 0.0003  max mem: 4132
[18:24:41.739398] Epoch: [24]  [140/781]  eta: 0:01:49  lr: 0.000226  training_loss: 1.6201 (1.6614)  classification_loss: 1.5801 (1.5998)  loss_mask: 0.0299 (0.0616)  time: 0.1632  data: 0.0002  max mem: 4132
[18:24:45.015544] Epoch: [24]  [160/781]  eta: 0:01:45  lr: 0.000226  training_loss: 1.6190 (1.6543)  classification_loss: 1.6037 (1.5977)  loss_mask: 0.0181 (0.0566)  time: 0.1637  data: 0.0002  max mem: 4132
[18:24:48.291228] Epoch: [24]  [180/781]  eta: 0:01:41  lr: 0.000226  training_loss: 1.5769 (1.6485)  classification_loss: 1.5365 (1.5944)  loss_mask: 0.0208 (0.0541)  time: 0.1637  data: 0.0003  max mem: 4132
[18:24:51.577085] Epoch: [24]  [200/781]  eta: 0:01:38  lr: 0.000226  training_loss: 1.6178 (1.6449)  classification_loss: 1.5857 (1.5935)  loss_mask: 0.0165 (0.0514)  time: 0.1642  data: 0.0003  max mem: 4132
[18:24:54.884192] Epoch: [24]  [220/781]  eta: 0:01:34  lr: 0.000226  training_loss: 1.7479 (1.6542)  classification_loss: 1.6499 (1.5992)  loss_mask: 0.0583 (0.0551)  time: 0.1653  data: 0.0004  max mem: 4132
[18:24:58.151827] Epoch: [24]  [240/781]  eta: 0:01:30  lr: 0.000225  training_loss: 1.6514 (1.6564)  classification_loss: 1.5901 (1.6025)  loss_mask: 0.0286 (0.0539)  time: 0.1633  data: 0.0003  max mem: 4132
[18:25:01.412421] Epoch: [24]  [260/781]  eta: 0:01:27  lr: 0.000225  training_loss: 1.6648 (1.6564)  classification_loss: 1.6041 (1.6038)  loss_mask: 0.0252 (0.0527)  time: 0.1629  data: 0.0002  max mem: 4132
[18:25:04.689964] Epoch: [24]  [280/781]  eta: 0:01:23  lr: 0.000225  training_loss: 1.6542 (1.6560)  classification_loss: 1.5768 (1.6017)  loss_mask: 0.0733 (0.0544)  time: 0.1638  data: 0.0002  max mem: 4132
[18:25:08.040413] Epoch: [24]  [300/781]  eta: 0:01:20  lr: 0.000225  training_loss: 1.6672 (1.6554)  classification_loss: 1.6407 (1.6028)  loss_mask: 0.0264 (0.0526)  time: 0.1674  data: 0.0003  max mem: 4132
[18:25:11.327049] Epoch: [24]  [320/781]  eta: 0:01:17  lr: 0.000225  training_loss: 1.6483 (1.6546)  classification_loss: 1.6140 (1.6040)  loss_mask: 0.0193 (0.0507)  time: 0.1642  data: 0.0002  max mem: 4132
[18:25:14.604566] Epoch: [24]  [340/781]  eta: 0:01:13  lr: 0.000225  training_loss: 1.6297 (1.6541)  classification_loss: 1.6208 (1.6052)  loss_mask: 0.0142 (0.0489)  time: 0.1637  data: 0.0003  max mem: 4132
[18:25:17.883881] Epoch: [24]  [360/781]  eta: 0:01:10  lr: 0.000225  training_loss: 1.5788 (1.6512)  classification_loss: 1.5686 (1.6044)  loss_mask: 0.0113 (0.0468)  time: 0.1638  data: 0.0002  max mem: 4132
[18:25:21.194350] Epoch: [24]  [380/781]  eta: 0:01:06  lr: 0.000225  training_loss: 1.6390 (1.6510)  classification_loss: 1.5723 (1.6040)  loss_mask: 0.0270 (0.0470)  time: 0.1654  data: 0.0003  max mem: 4132
[18:25:24.479792] Epoch: [24]  [400/781]  eta: 0:01:03  lr: 0.000225  training_loss: 1.6218 (1.6499)  classification_loss: 1.5834 (1.6035)  loss_mask: 0.0302 (0.0464)  time: 0.1642  data: 0.0003  max mem: 4132
[18:25:27.757041] Epoch: [24]  [420/781]  eta: 0:01:00  lr: 0.000225  training_loss: 1.6009 (1.6483)  classification_loss: 1.5582 (1.6023)  loss_mask: 0.0376 (0.0460)  time: 0.1638  data: 0.0002  max mem: 4132
[18:25:31.052808] Epoch: [24]  [440/781]  eta: 0:00:56  lr: 0.000225  training_loss: 1.6861 (1.6500)  classification_loss: 1.5590 (1.6018)  loss_mask: 0.0986 (0.0482)  time: 0.1647  data: 0.0003  max mem: 4132
[18:25:34.346217] Epoch: [24]  [460/781]  eta: 0:00:53  lr: 0.000225  training_loss: 1.7221 (1.6535)  classification_loss: 1.6358 (1.6036)  loss_mask: 0.0728 (0.0499)  time: 0.1646  data: 0.0002  max mem: 4132
[18:25:37.635539] Epoch: [24]  [480/781]  eta: 0:00:50  lr: 0.000225  training_loss: 1.7097 (1.6559)  classification_loss: 1.6408 (1.6058)  loss_mask: 0.0416 (0.0501)  time: 0.1644  data: 0.0002  max mem: 4132
[18:25:40.939455] Epoch: [24]  [500/781]  eta: 0:00:46  lr: 0.000225  training_loss: 1.6475 (1.6553)  classification_loss: 1.5993 (1.6056)  loss_mask: 0.0236 (0.0497)  time: 0.1651  data: 0.0003  max mem: 4132
[18:25:44.262449] Epoch: [24]  [520/781]  eta: 0:00:43  lr: 0.000225  training_loss: 1.6702 (1.6563)  classification_loss: 1.6410 (1.6071)  loss_mask: 0.0214 (0.0493)  time: 0.1660  data: 0.0003  max mem: 4132

[18:25:47.550939] Epoch: [24]  [540/781]  eta: 0:00:40  lr: 0.000225  training_loss: 1.6790 (1.6575)  classification_loss: 1.6225 (1.6078)  loss_mask: 0.0218 (0.0496)  time: 0.1643  data: 0.0002  max mem: 4132
[18:25:50.833030] Epoch: [24]  [560/781]  eta: 0:00:36  lr: 0.000224  training_loss: 1.6303 (1.6568)  classification_loss: 1.5766 (1.6071)  loss_mask: 0.0397 (0.0497)  time: 0.1640  data: 0.0003  max mem: 4132
[18:25:54.165021] Epoch: [24]  [580/781]  eta: 0:00:33  lr: 0.000224  training_loss: 1.6371 (1.6562)  classification_loss: 1.6207 (1.6071)  loss_mask: 0.0301 (0.0492)  time: 0.1665  data: 0.0003  max mem: 4132
[18:25:57.494731] Epoch: [24]  [600/781]  eta: 0:00:30  lr: 0.000224  training_loss: 1.6294 (1.6555)  classification_loss: 1.5559 (1.6063)  loss_mask: 0.0324 (0.0492)  time: 0.1664  data: 0.0003  max mem: 4132
[18:26:00.833759] Epoch: [24]  [620/781]  eta: 0:00:26  lr: 0.000224  training_loss: 1.6550 (1.6555)  classification_loss: 1.5832 (1.6060)  loss_mask: 0.0490 (0.0495)  time: 0.1669  data: 0.0003  max mem: 4132
[18:26:04.144732] Epoch: [24]  [640/781]  eta: 0:00:23  lr: 0.000224  training_loss: 1.6147 (1.6550)  classification_loss: 1.5619 (1.6053)  loss_mask: 0.0347 (0.0497)  time: 0.1655  data: 0.0003  max mem: 4132
[18:26:07.439874] Epoch: [24]  [660/781]  eta: 0:00:20  lr: 0.000224  training_loss: 1.7324 (1.6562)  classification_loss: 1.6529 (1.6061)  loss_mask: 0.0613 (0.0500)  time: 0.1647  data: 0.0003  max mem: 4132
[18:26:10.756185] Epoch: [24]  [680/781]  eta: 0:00:16  lr: 0.000224  training_loss: 1.6245 (1.6560)  classification_loss: 1.6024 (1.6066)  loss_mask: 0.0198 (0.0493)  time: 0.1657  data: 0.0004  max mem: 4132
[18:26:14.090523] Epoch: [24]  [700/781]  eta: 0:00:13  lr: 0.000224  training_loss: 1.6206 (1.6556)  classification_loss: 1.5931 (1.6068)  loss_mask: 0.0210 (0.0487)  time: 0.1666  data: 0.0003  max mem: 4132
[18:26:17.405390] Epoch: [24]  [720/781]  eta: 0:00:10  lr: 0.000224  training_loss: 1.6396 (1.6549)  classification_loss: 1.6179 (1.6065)  loss_mask: 0.0269 (0.0483)  time: 0.1656  data: 0.0003  max mem: 4132
[18:26:20.688134] Epoch: [24]  [740/781]  eta: 0:00:06  lr: 0.000224  training_loss: 1.6242 (1.6549)  classification_loss: 1.5784 (1.6062)  loss_mask: 0.0399 (0.0487)  time: 0.1640  data: 0.0003  max mem: 4132
[18:26:23.981385] Epoch: [24]  [760/781]  eta: 0:00:03  lr: 0.000224  training_loss: 1.6557 (1.6546)  classification_loss: 1.6105 (1.6060)  loss_mask: 0.0339 (0.0486)  time: 0.1646  data: 0.0003  max mem: 4132
[18:26:27.294559] Epoch: [24]  [780/781]  eta: 0:00:00  lr: 0.000224  training_loss: 1.6221 (1.6543)  classification_loss: 1.6024 (1.6063)  loss_mask: 0.0203 (0.0480)  time: 0.1655  data: 0.0002  max mem: 4132
[18:26:27.473961] Epoch: [24] Total time: 0:02:09 (0.1663 s / it)
[18:26:27.475361] Averaged stats: lr: 0.000224  training_loss: 1.6221 (1.6543)  classification_loss: 1.6024 (1.6063)  loss_mask: 0.0203 (0.0480)
[18:26:28.249227] Test:  [  0/157]  eta: 0:02:00  testing_loss: 0.9347 (0.9347)  acc1: 68.7500 (68.7500)  acc5: 95.3125 (95.3125)  time: 0.7684  data: 0.7387  max mem: 4132
[18:26:28.541269] Test:  [ 10/157]  eta: 0:00:14  testing_loss: 1.0022 (0.9600)  acc1: 64.0625 (65.7670)  acc5: 98.4375 (97.7273)  time: 0.0961  data: 0.0674  max mem: 4132
[18:26:28.834073] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.9150 (0.9247)  acc1: 67.1875 (67.8571)  acc5: 98.4375 (97.9911)  time: 0.0290  data: 0.0002  max mem: 4132
[18:26:29.122596] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.9242 (0.9333)  acc1: 68.7500 (68.1956)  acc5: 98.4375 (97.5302)  time: 0.0289  data: 0.0002  max mem: 4132
[18:26:29.420543] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.9018 (0.9276)  acc1: 68.7500 (68.5595)  acc5: 98.4375 (97.7896)  time: 0.0292  data: 0.0003  max mem: 4132
[18:26:29.721228] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.8888 (0.9231)  acc1: 67.1875 (68.5049)  acc5: 98.4375 (97.8554)  time: 0.0298  data: 0.0002  max mem: 4132
[18:26:30.013205] Test:  [ 60/157]  eta: 0:00:04  testing_loss: 0.8998 (0.9222)  acc1: 67.1875 (68.5707)  acc5: 98.4375 (97.8996)  time: 0.0295  data: 0.0002  max mem: 4132
[18:26:30.301120] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.8981 (0.9163)  acc1: 70.3125 (68.9261)  acc5: 98.4375 (97.8873)  time: 0.0289  data: 0.0002  max mem: 4132
[18:26:30.590542] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.8981 (0.9228)  acc1: 70.3125 (68.8850)  acc5: 98.4375 (97.8974)  time: 0.0287  data: 0.0002  max mem: 4132
[18:26:30.885345] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.9203 (0.9231)  acc1: 68.7500 (68.7672)  acc5: 98.4375 (97.9396)  time: 0.0291  data: 0.0002  max mem: 4132
[18:26:31.172798] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.9203 (0.9270)  acc1: 67.1875 (68.7500)  acc5: 98.4375 (97.9270)  time: 0.0290  data: 0.0002  max mem: 4132
[18:26:31.465288] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.9644 (0.9282)  acc1: 67.1875 (68.8485)  acc5: 98.4375 (97.9307)  time: 0.0289  data: 0.0003  max mem: 4132
[18:26:31.756839] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.9460 (0.9258)  acc1: 68.7500 (68.9824)  acc5: 98.4375 (97.9210)  time: 0.0291  data: 0.0003  max mem: 4132
[18:26:32.046983] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.9292 (0.9283)  acc1: 68.7500 (68.7381)  acc5: 96.8750 (97.8531)  time: 0.0289  data: 0.0002  max mem: 4132
[18:26:32.336820] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.9352 (0.9283)  acc1: 68.7500 (68.8054)  acc5: 96.8750 (97.8280)  time: 0.0288  data: 0.0002  max mem: 4132
[18:26:32.620506] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.9575 (0.9256)  acc1: 68.7500 (68.9052)  acc5: 98.4375 (97.8270)  time: 0.0285  data: 0.0002  max mem: 4132
[18:26:32.774604] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.8820 (0.9260)  acc1: 68.7500 (68.8900)  acc5: 96.8750 (97.8300)  time: 0.0275  data: 0.0002  max mem: 4132
[18:26:32.965811] Test: Total time: 0:00:05 (0.0349 s / it)
[18:26:32.966335] * Acc@1 68.890 Acc@5 97.830 loss 0.926
[18:26:32.966663] Accuracy of the network on the 10000 test images: 68.9%
[18:26:32.966930] Max accuracy: 68.89%
[18:26:33.250076] log_dir: ./output_dir
[18:26:34.190836] Epoch: [25]  [  0/781]  eta: 0:12:13  lr: 0.000224  training_loss: 1.6025 (1.6025)  classification_loss: 1.5757 (1.5757)  loss_mask: 0.0268 (0.0268)  time: 0.9388  data: 0.7242  max mem: 4132
[18:26:37.511847] Epoch: [25]  [ 20/781]  eta: 0:02:34  lr: 0.000224  training_loss: 1.5855 (1.6091)  classification_loss: 1.5523 (1.5617)  loss_mask: 0.0248 (0.0474)  time: 0.1659  data: 0.0003  max mem: 4132
[18:26:40.799311] Epoch: [25]  [ 40/781]  eta: 0:02:16  lr: 0.000224  training_loss: 1.6553 (1.6347)  classification_loss: 1.6171 (1.5877)  loss_mask: 0.0359 (0.0470)  time: 0.1643  data: 0.0004  max mem: 4132
[18:26:44.122538] Epoch: [25]  [ 60/781]  eta: 0:02:08  lr: 0.000224  training_loss: 1.6783 (1.6556)  classification_loss: 1.6197 (1.6001)  loss_mask: 0.0586 (0.0555)  time: 0.1661  data: 0.0003  max mem: 4132
[18:26:47.462788] Epoch: [25]  [ 80/781]  eta: 0:02:02  lr: 0.000223  training_loss: 1.7343 (1.6756)  classification_loss: 1.6387 (1.6100)  loss_mask: 0.0573 (0.0655)  time: 0.1669  data: 0.0003  max mem: 4132
[18:26:50.793642] Epoch: [25]  [100/781]  eta: 0:01:58  lr: 0.000223  training_loss: 1.6937 (1.6808)  classification_loss: 1.5866 (1.6075)  loss_mask: 0.0509 (0.0733)  time: 0.1664  data: 0.0004  max mem: 4132
[18:26:54.126746] Epoch: [25]  [120/781]  eta: 0:01:53  lr: 0.000223  training_loss: 1.6132 (1.6708)  classification_loss: 1.5367 (1.6006)  loss_mask: 0.0474 (0.0702)  time: 0.1666  data: 0.0003  max mem: 4132
[18:26:57.456343] Epoch: [25]  [140/781]  eta: 0:01:49  lr: 0.000223  training_loss: 1.5717 (1.6590)  classification_loss: 1.5388 (1.5951)  loss_mask: 0.0219 (0.0638)  time: 0.1664  data: 0.0003  max mem: 4132
[18:27:00.737478] Epoch: [25]  [160/781]  eta: 0:01:45  lr: 0.000223  training_loss: 1.6358 (1.6558)  classification_loss: 1.5841 (1.5948)  loss_mask: 0.0213 (0.0609)  time: 0.1640  data: 0.0003  max mem: 4132
[18:27:04.065679] Epoch: [25]  [180/781]  eta: 0:01:42  lr: 0.000223  training_loss: 1.6458 (1.6577)  classification_loss: 1.5937 (1.5950)  loss_mask: 0.0506 (0.0627)  time: 0.1663  data: 0.0003  max mem: 4132
[18:27:07.393243] Epoch: [25]  [200/781]  eta: 0:01:38  lr: 0.000223  training_loss: 1.5935 (1.6541)  classification_loss: 1.5203 (1.5897)  loss_mask: 0.0566 (0.0644)  time: 0.1663  data: 0.0003  max mem: 4132
[18:27:10.719378] Epoch: [25]  [220/781]  eta: 0:01:35  lr: 0.000223  training_loss: 1.6819 (1.6572)  classification_loss: 1.5798 (1.5901)  loss_mask: 0.0601 (0.0671)  time: 0.1662  data: 0.0003  max mem: 4132
[18:27:14.065448] Epoch: [25]  [240/781]  eta: 0:01:31  lr: 0.000223  training_loss: 1.6321 (1.6568)  classification_loss: 1.5485 (1.5900)  loss_mask: 0.0476 (0.0667)  time: 0.1672  data: 0.0003  max mem: 4132
[18:27:17.361342] Epoch: [25]  [260/781]  eta: 0:01:28  lr: 0.000223  training_loss: 1.7018 (1.6595)  classification_loss: 1.6006 (1.5918)  loss_mask: 0.0616 (0.0677)  time: 0.1647  data: 0.0002  max mem: 4132
[18:27:20.652747] Epoch: [25]  [280/781]  eta: 0:01:24  lr: 0.000223  training_loss: 1.6196 (1.6561)  classification_loss: 1.5886 (1.5901)  loss_mask: 0.0354 (0.0660)  time: 0.1645  data: 0.0003  max mem: 4132
[18:27:23.938539] Epoch: [25]  [300/781]  eta: 0:01:20  lr: 0.000223  training_loss: 1.5965 (1.6556)  classification_loss: 1.5881 (1.5918)  loss_mask: 0.0284 (0.0638)  time: 0.1642  data: 0.0003  max mem: 4132
[18:27:27.197796] Epoch: [25]  [320/781]  eta: 0:01:17  lr: 0.000223  training_loss: 1.6451 (1.6566)  classification_loss: 1.5988 (1.5923)  loss_mask: 0.0508 (0.0643)  time: 0.1629  data: 0.0002  max mem: 4132
[18:27:30.501953] Epoch: [25]  [340/781]  eta: 0:01:13  lr: 0.000223  training_loss: 1.5806 (1.6526)  classification_loss: 1.5279 (1.5894)  loss_mask: 0.0422 (0.0632)  time: 0.1651  data: 0.0002  max mem: 4132
[18:27:33.832465] Epoch: [25]  [360/781]  eta: 0:01:10  lr: 0.000223  training_loss: 1.5954 (1.6500)  classification_loss: 1.5829 (1.5890)  loss_mask: 0.0196 (0.0610)  time: 0.1664  data: 0.0003  max mem: 4132
[18:27:37.166055] Epoch: [25]  [380/781]  eta: 0:01:07  lr: 0.000223  training_loss: 1.5987 (1.6507)  classification_loss: 1.5667 (1.5911)  loss_mask: 0.0200 (0.0595)  time: 0.1666  data: 0.0004  max mem: 4132
[18:27:40.470431] Epoch: [25]  [400/781]  eta: 0:01:03  lr: 0.000222  training_loss: 1.6553 (1.6508)  classification_loss: 1.5712 (1.5909)  loss_mask: 0.0308 (0.0599)  time: 0.1651  data: 0.0003  max mem: 4132
[18:27:43.751134] Epoch: [25]  [420/781]  eta: 0:01:00  lr: 0.000222  training_loss: 1.6223 (1.6499)  classification_loss: 1.5700 (1.5905)  loss_mask: 0.0376 (0.0594)  time: 0.1639  data: 0.0003  max mem: 4132
[18:27:47.038271] Epoch: [25]  [440/781]  eta: 0:00:57  lr: 0.000222  training_loss: 1.6172 (1.6489)  classification_loss: 1.5894 (1.5909)  loss_mask: 0.0202 (0.0579)  time: 0.1642  data: 0.0002  max mem: 4132
[18:27:50.378828] Epoch: [25]  [460/781]  eta: 0:00:53  lr: 0.000222  training_loss: 1.5861 (1.6467)  classification_loss: 1.5679 (1.5906)  loss_mask: 0.0124 (0.0561)  time: 0.1669  data: 0.0004  max mem: 4132
[18:27:53.694087] Epoch: [25]  [480/781]  eta: 0:00:50  lr: 0.000222  training_loss: 1.6702 (1.6459)  classification_loss: 1.6071 (1.5910)  loss_mask: 0.0182 (0.0549)  time: 0.1657  data: 0.0003  max mem: 4132
[18:27:56.973045] Epoch: [25]  [500/781]  eta: 0:00:46  lr: 0.000222  training_loss: 1.5875 (1.6448)  classification_loss: 1.5535 (1.5905)  loss_mask: 0.0211 (0.0543)  time: 0.1638  data: 0.0002  max mem: 4132
[18:28:00.297519] Epoch: [25]  [520/781]  eta: 0:00:43  lr: 0.000222  training_loss: 1.6133 (1.6432)  classification_loss: 1.5624 (1.5897)  loss_mask: 0.0205 (0.0535)  time: 0.1661  data: 0.0003  max mem: 4132
[18:28:03.643179] Epoch: [25]  [540/781]  eta: 0:00:40  lr: 0.000222  training_loss: 1.6038 (1.6422)  classification_loss: 1.5672 (1.5898)  loss_mask: 0.0184 (0.0524)  time: 0.1672  data: 0.0003  max mem: 4132
[18:28:06.988439] Epoch: [25]  [560/781]  eta: 0:00:36  lr: 0.000222  training_loss: 1.5996 (1.6409)  classification_loss: 1.5939 (1.5899)  loss_mask: 0.0103 (0.0510)  time: 0.1672  data: 0.0003  max mem: 4132
[18:28:10.321277] Epoch: [25]  [580/781]  eta: 0:00:33  lr: 0.000222  training_loss: 1.5965 (1.6396)  classification_loss: 1.5868 (1.5901)  loss_mask: 0.0087 (0.0495)  time: 0.1665  data: 0.0003  max mem: 4132
[18:28:13.636792] Epoch: [25]  [600/781]  eta: 0:00:30  lr: 0.000222  training_loss: 1.5771 (1.6380)  classification_loss: 1.5686 (1.5897)  loss_mask: 0.0097 (0.0483)  time: 0.1656  data: 0.0003  max mem: 4132
[18:28:16.995188] Epoch: [25]  [620/781]  eta: 0:00:26  lr: 0.000222  training_loss: 1.5781 (1.6374)  classification_loss: 1.5640 (1.5899)  loss_mask: 0.0116 (0.0474)  time: 0.1678  data: 0.0003  max mem: 4132
[18:28:20.317347] Epoch: [25]  [640/781]  eta: 0:00:23  lr: 0.000222  training_loss: 1.5914 (1.6368)  classification_loss: 1.5703 (1.5895)  loss_mask: 0.0207 (0.0473)  time: 0.1660  data: 0.0004  max mem: 4132
[18:28:23.601784] Epoch: [25]  [660/781]  eta: 0:00:20  lr: 0.000222  training_loss: 1.6585 (1.6371)  classification_loss: 1.6474 (1.5902)  loss_mask: 0.0263 (0.0469)  time: 0.1641  data: 0.0003  max mem: 4132
[18:28:26.950626] Epoch: [25]  [680/781]  eta: 0:00:16  lr: 0.000222  training_loss: 1.6941 (1.6400)  classification_loss: 1.5968 (1.5913)  loss_mask: 0.0556 (0.0487)  time: 0.1673  data: 0.0003  max mem: 4132
[18:28:30.319780] Epoch: [25]  [700/781]  eta: 0:00:13  lr: 0.000221  training_loss: 1.6648 (1.6421)  classification_loss: 1.5954 (1.5911)  loss_mask: 0.0707 (0.0510)  time: 0.1683  data: 0.0003  max mem: 4132
[18:28:33.638526] Epoch: [25]  [720/781]  eta: 0:00:10  lr: 0.000221  training_loss: 1.6553 (1.6428)  classification_loss: 1.6156 (1.5915)  loss_mask: 0.0461 (0.0513)  time: 0.1659  data: 0.0003  max mem: 4132
[18:28:36.917379] Epoch: [25]  [740/781]  eta: 0:00:06  lr: 0.000221  training_loss: 1.6124 (1.6418)  classification_loss: 1.5461 (1.5903)  loss_mask: 0.0602 (0.0515)  time: 0.1639  data: 0.0003  max mem: 4132
[18:28:40.226816] Epoch: [25]  [760/781]  eta: 0:00:03  lr: 0.000221  training_loss: 1.5867 (1.6417)  classification_loss: 1.5598 (1.5904)  loss_mask: 0.0258 (0.0513)  time: 0.1654  data: 0.0005  max mem: 4132
[18:28:43.519380] Epoch: [25]  [780/781]  eta: 0:00:00  lr: 0.000221  training_loss: 1.5530 (1.6401)  classification_loss: 1.5330 (1.5892)  loss_mask: 0.0252 (0.0509)  time: 0.1645  data: 0.0002  max mem: 4132
[18:28:43.698828] Epoch: [25] Total time: 0:02:10 (0.1670 s / it)
[18:28:43.699303] Averaged stats: lr: 0.000221  training_loss: 1.5530 (1.6401)  classification_loss: 1.5330 (1.5892)  loss_mask: 0.0252 (0.0509)
[18:28:44.418323] Test:  [  0/157]  eta: 0:01:52  testing_loss: 0.8769 (0.8769)  acc1: 75.0000 (75.0000)  acc5: 96.8750 (96.8750)  time: 0.7146  data: 0.6850  max mem: 4132
[18:28:44.760489] Test:  [ 10/157]  eta: 0:00:14  testing_loss: 0.8769 (0.9074)  acc1: 71.8750 (69.4602)  acc5: 98.4375 (98.0114)  time: 0.0957  data: 0.0653  max mem: 4132
[18:28:45.061802] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.8416 (0.8688)  acc1: 70.3125 (70.9077)  acc5: 98.4375 (98.2887)  time: 0.0319  data: 0.0018  max mem: 4132
[18:28:45.358836] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.8395 (0.8796)  acc1: 70.3125 (70.8669)  acc5: 98.4375 (97.8327)  time: 0.0297  data: 0.0003  max mem: 4132
[18:28:45.657242] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.8610 (0.8843)  acc1: 70.3125 (70.7698)  acc5: 98.4375 (97.9040)  time: 0.0296  data: 0.0004  max mem: 4132
[18:28:45.947488] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.8855 (0.8852)  acc1: 68.7500 (70.4350)  acc5: 98.4375 (97.9167)  time: 0.0292  data: 0.0003  max mem: 4132
[18:28:46.236889] Test:  [ 60/157]  eta: 0:00:04  testing_loss: 0.8900 (0.8835)  acc1: 67.1875 (70.2613)  acc5: 96.8750 (97.8484)  time: 0.0287  data: 0.0002  max mem: 4132
[18:28:46.529220] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.8409 (0.8784)  acc1: 70.3125 (70.4665)  acc5: 96.8750 (97.8873)  time: 0.0289  data: 0.0002  max mem: 4132
[18:28:46.820684] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.8521 (0.8878)  acc1: 70.3125 (70.1968)  acc5: 98.4375 (97.7238)  time: 0.0290  data: 0.0003  max mem: 4132
[18:28:47.114580] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.8645 (0.8862)  acc1: 70.3125 (70.2095)  acc5: 98.4375 (97.7507)  time: 0.0291  data: 0.0002  max mem: 4132
[18:28:47.403757] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.8774 (0.8911)  acc1: 68.7500 (70.0495)  acc5: 98.4375 (97.7877)  time: 0.0290  data: 0.0003  max mem: 4132
[18:28:47.699923] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.9352 (0.8922)  acc1: 67.1875 (69.8480)  acc5: 98.4375 (97.8041)  time: 0.0291  data: 0.0002  max mem: 4132
[18:28:47.992156] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.8703 (0.8884)  acc1: 71.8750 (70.1446)  acc5: 98.4375 (97.8177)  time: 0.0292  data: 0.0003  max mem: 4132
[18:28:48.285317] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.8597 (0.8897)  acc1: 71.8750 (69.9785)  acc5: 98.4375 (97.8411)  time: 0.0290  data: 0.0003  max mem: 4132
[18:28:48.574302] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.8902 (0.8883)  acc1: 70.3125 (70.0576)  acc5: 98.4375 (97.8391)  time: 0.0289  data: 0.0002  max mem: 4132
[18:28:48.859165] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.8902 (0.8874)  acc1: 71.8750 (70.1262)  acc5: 96.8750 (97.7959)  time: 0.0285  data: 0.0002  max mem: 4132
[18:28:49.016885] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.8591 (0.8879)  acc1: 71.8750 (70.0300)  acc5: 96.8750 (97.8200)  time: 0.0277  data: 0.0002  max mem: 4132
[18:28:49.193732] Test: Total time: 0:00:05 (0.0350 s / it)
[18:28:49.194661] * Acc@1 70.030 Acc@5 97.820 loss 0.888
[18:28:49.194980] Accuracy of the network on the 10000 test images: 70.0%
[18:28:49.195179] Max accuracy: 70.03%
[18:28:49.425518] log_dir: ./output_dir
[18:28:50.363233] Epoch: [26]  [  0/781]  eta: 0:12:10  lr: 0.000221  training_loss: 1.4080 (1.4080)  classification_loss: 1.3780 (1.3780)  loss_mask: 0.0301 (0.0301)  time: 0.9357  data: 0.7375  max mem: 4132
[18:28:53.662147] Epoch: [26]  [ 20/781]  eta: 0:02:33  lr: 0.000221  training_loss: 1.5478 (1.5601)  classification_loss: 1.5313 (1.5356)  loss_mask: 0.0202 (0.0245)  time: 0.1648  data: 0.0002  max mem: 4132
[18:28:56.965786] Epoch: [26]  [ 40/781]  eta: 0:02:16  lr: 0.000221  training_loss: 1.5458 (1.5586)  classification_loss: 1.5345 (1.5395)  loss_mask: 0.0134 (0.0192)  time: 0.1651  data: 0.0003  max mem: 4132
[18:29:00.271962] Epoch: [26]  [ 60/781]  eta: 0:02:08  lr: 0.000221  training_loss: 1.6660 (1.5976)  classification_loss: 1.6346 (1.5756)  loss_mask: 0.0198 (0.0220)  time: 0.1652  data: 0.0003  max mem: 4132
[18:29:03.542333] Epoch: [26]  [ 80/781]  eta: 0:02:02  lr: 0.000221  training_loss: 1.6236 (1.6008)  classification_loss: 1.6074 (1.5798)  loss_mask: 0.0150 (0.0210)  time: 0.1634  data: 0.0003  max mem: 4132
[18:29:06.813940] Epoch: [26]  [100/781]  eta: 0:01:57  lr: 0.000221  training_loss: 1.6187 (1.6024)  classification_loss: 1.5999 (1.5832)  loss_mask: 0.0094 (0.0191)  time: 0.1635  data: 0.0002  max mem: 4132
[18:29:10.124004] Epoch: [26]  [120/781]  eta: 0:01:52  lr: 0.000221  training_loss: 1.6025 (1.5980)  classification_loss: 1.5839 (1.5796)  loss_mask: 0.0099 (0.0184)  time: 0.1654  data: 0.0003  max mem: 4132
[18:29:13.419248] Epoch: [26]  [140/781]  eta: 0:01:49  lr: 0.000221  training_loss: 1.5457 (1.5876)  classification_loss: 1.5354 (1.5706)  loss_mask: 0.0071 (0.0170)  time: 0.1646  data: 0.0003  max mem: 4132
[18:29:16.723831] Epoch: [26]  [160/781]  eta: 0:01:45  lr: 0.000221  training_loss: 1.5704 (1.5860)  classification_loss: 1.5579 (1.5694)  loss_mask: 0.0093 (0.0166)  time: 0.1651  data: 0.0003  max mem: 4132
[18:29:20.083385] Epoch: [26]  [180/781]  eta: 0:01:41  lr: 0.000221  training_loss: 1.6345 (1.5893)  classification_loss: 1.6104 (1.5719)  loss_mask: 0.0216 (0.0174)  time: 0.1679  data: 0.0003  max mem: 4132
[18:29:23.390430] Epoch: [26]  [200/781]  eta: 0:01:38  lr: 0.000220  training_loss: 1.6033 (1.5889)  classification_loss: 1.5963 (1.5716)  loss_mask: 0.0115 (0.0173)  time: 0.1652  data: 0.0003  max mem: 4132
[18:29:26.716677] Epoch: [26]  [220/781]  eta: 0:01:34  lr: 0.000220  training_loss: 1.5901 (1.5912)  classification_loss: 1.5629 (1.5728)  loss_mask: 0.0142 (0.0184)  time: 0.1661  data: 0.0004  max mem: 4132
[18:29:30.029662] Epoch: [26]  [240/781]  eta: 0:01:31  lr: 0.000220  training_loss: 1.5697 (1.5916)  classification_loss: 1.5638 (1.5732)  loss_mask: 0.0131 (0.0184)  time: 0.1656  data: 0.0004  max mem: 4132
[18:29:33.361297] Epoch: [26]  [260/781]  eta: 0:01:27  lr: 0.000220  training_loss: 1.5863 (1.5913)  classification_loss: 1.5725 (1.5735)  loss_mask: 0.0076 (0.0179)  time: 0.1665  data: 0.0003  max mem: 4132
[18:29:36.676453] Epoch: [26]  [280/781]  eta: 0:01:24  lr: 0.000220  training_loss: 1.5773 (1.5921)  classification_loss: 1.5704 (1.5748)  loss_mask: 0.0070 (0.0173)  time: 0.1657  data: 0.0003  max mem: 4132
[18:29:40.039488] Epoch: [26]  [300/781]  eta: 0:01:20  lr: 0.000220  training_loss: 1.5769 (1.5919)  classification_loss: 1.5702 (1.5755)  loss_mask: 0.0043 (0.0165)  time: 0.1680  data: 0.0003  max mem: 4132
[18:29:43.315067] Epoch: [26]  [320/781]  eta: 0:01:17  lr: 0.000220  training_loss: 1.5877 (1.5921)  classification_loss: 1.5820 (1.5764)  loss_mask: 0.0043 (0.0157)  time: 0.1637  data: 0.0003  max mem: 4132
[18:29:46.618630] Epoch: [26]  [340/781]  eta: 0:01:13  lr: 0.000220  training_loss: 1.5226 (1.5902)  classification_loss: 1.5193 (1.5752)  loss_mask: 0.0032 (0.0150)  time: 0.1651  data: 0.0003  max mem: 4132
[18:29:49.892792] Epoch: [26]  [360/781]  eta: 0:01:10  lr: 0.000220  training_loss: 1.5852 (1.5904)  classification_loss: 1.5826 (1.5760)  loss_mask: 0.0023 (0.0144)  time: 0.1636  data: 0.0002  max mem: 4132
[18:29:53.203405] Epoch: [26]  [380/781]  eta: 0:01:07  lr: 0.000220  training_loss: 1.5466 (1.5890)  classification_loss: 1.5419 (1.5749)  loss_mask: 0.0053 (0.0140)  time: 0.1654  data: 0.0002  max mem: 4132
[18:29:56.492812] Epoch: [26]  [400/781]  eta: 0:01:03  lr: 0.000220  training_loss: 1.5300 (1.5867)  classification_loss: 1.5293 (1.5732)  loss_mask: 0.0038 (0.0135)  time: 0.1643  data: 0.0002  max mem: 4132
[18:29:59.828528] Epoch: [26]  [420/781]  eta: 0:01:00  lr: 0.000220  training_loss: 1.6335 (1.5869)  classification_loss: 1.5902 (1.5736)  loss_mask: 0.0034 (0.0133)  time: 0.1667  data: 0.0003  max mem: 4132
[18:30:03.130364] Epoch: [26]  [440/781]  eta: 0:00:56  lr: 0.000220  training_loss: 1.6118 (1.5871)  classification_loss: 1.5979 (1.5739)  loss_mask: 0.0076 (0.0132)  time: 0.1650  data: 0.0004  max mem: 4132
[18:30:06.462762] Epoch: [26]  [460/781]  eta: 0:00:53  lr: 0.000220  training_loss: 1.6897 (1.5918)  classification_loss: 1.5580 (1.5733)  loss_mask: 0.0734 (0.0186)  time: 0.1665  data: 0.0002  max mem: 4132
[18:30:09.764262] Epoch: [26]  [480/781]  eta: 0:00:50  lr: 0.000220  training_loss: 1.6544 (1.5961)  classification_loss: 1.5784 (1.5742)  loss_mask: 0.0461 (0.0220)  time: 0.1650  data: 0.0002  max mem: 4132
[18:30:13.053950] Epoch: [26]  [500/781]  eta: 0:00:46  lr: 0.000219  training_loss: 1.6534 (1.5995)  classification_loss: 1.5537 (1.5735)  loss_mask: 0.0896 (0.0259)  time: 0.1642  data: 0.0003  max mem: 4132
[18:30:16.336271] Epoch: [26]  [520/781]  eta: 0:00:43  lr: 0.000219  training_loss: 1.5989 (1.6017)  classification_loss: 1.5695 (1.5734)  loss_mask: 0.0582 (0.0284)  time: 0.1640  data: 0.0002  max mem: 4132
[18:30:19.630317] Epoch: [26]  [540/781]  eta: 0:00:40  lr: 0.000219  training_loss: 1.6616 (1.6056)  classification_loss: 1.5666 (1.5745)  loss_mask: 0.0628 (0.0311)  time: 0.1646  data: 0.0003  max mem: 4132
[18:30:22.912264] Epoch: [26]  [560/781]  eta: 0:00:36  lr: 0.000219  training_loss: 1.5878 (1.6056)  classification_loss: 1.5591 (1.5740)  loss_mask: 0.0285 (0.0315)  time: 0.1639  data: 0.0003  max mem: 4132
[18:30:26.211529] Epoch: [26]  [580/781]  eta: 0:00:33  lr: 0.000219  training_loss: 1.6488 (1.6071)  classification_loss: 1.5960 (1.5746)  loss_mask: 0.0409 (0.0324)  time: 0.1649  data: 0.0002  max mem: 4132
[18:30:29.533702] Epoch: [26]  [600/781]  eta: 0:00:30  lr: 0.000219  training_loss: 1.6004 (1.6079)  classification_loss: 1.5624 (1.5746)  loss_mask: 0.0295 (0.0333)  time: 0.1660  data: 0.0009  max mem: 4132
[18:30:32.808094] Epoch: [26]  [620/781]  eta: 0:00:26  lr: 0.000219  training_loss: 1.5723 (1.6079)  classification_loss: 1.5421 (1.5746)  loss_mask: 0.0226 (0.0333)  time: 0.1636  data: 0.0002  max mem: 4132
[18:30:36.129656] Epoch: [26]  [640/781]  eta: 0:00:23  lr: 0.000219  training_loss: 1.5534 (1.6073)  classification_loss: 1.5285 (1.5740)  loss_mask: 0.0246 (0.0333)  time: 0.1660  data: 0.0003  max mem: 4132
[18:30:39.392790] Epoch: [26]  [660/781]  eta: 0:00:20  lr: 0.000219  training_loss: 1.5439 (1.6061)  classification_loss: 1.5029 (1.5730)  loss_mask: 0.0250 (0.0331)  time: 0.1631  data: 0.0003  max mem: 4132
[18:30:42.697051] Epoch: [26]  [680/781]  eta: 0:00:16  lr: 0.000219  training_loss: 1.6251 (1.6074)  classification_loss: 1.5726 (1.5736)  loss_mask: 0.0625 (0.0339)  time: 0.1651  data: 0.0003  max mem: 4132
[18:30:45.974589] Epoch: [26]  [700/781]  eta: 0:00:13  lr: 0.000219  training_loss: 1.6182 (1.6081)  classification_loss: 1.5608 (1.5736)  loss_mask: 0.0346 (0.0345)  time: 0.1638  data: 0.0002  max mem: 4132
[18:30:49.268105] Epoch: [26]  [720/781]  eta: 0:00:10  lr: 0.000219  training_loss: 1.5735 (1.6081)  classification_loss: 1.5233 (1.5730)  loss_mask: 0.0386 (0.0351)  time: 0.1646  data: 0.0003  max mem: 4132
[18:30:52.552849] Epoch: [26]  [740/781]  eta: 0:00:06  lr: 0.000219  training_loss: 1.5535 (1.6073)  classification_loss: 1.5437 (1.5723)  loss_mask: 0.0205 (0.0350)  time: 0.1641  data: 0.0003  max mem: 4132
[18:30:55.859301] Epoch: [26]  [760/781]  eta: 0:00:03  lr: 0.000219  training_loss: 1.6450 (1.6080)  classification_loss: 1.6133 (1.5730)  loss_mask: 0.0296 (0.0350)  time: 0.1652  data: 0.0003  max mem: 4132
[18:30:59.143772] Epoch: [26]  [780/781]  eta: 0:00:00  lr: 0.000218  training_loss: 1.6074 (1.6088)  classification_loss: 1.5566 (1.5730)  loss_mask: 0.0289 (0.0357)  time: 0.1641  data: 0.0003  max mem: 4132
[18:30:59.330388] Epoch: [26] Total time: 0:02:09 (0.1663 s / it)
[18:30:59.330914] Averaged stats: lr: 0.000218  training_loss: 1.6074 (1.6088)  classification_loss: 1.5566 (1.5730)  loss_mask: 0.0289 (0.0357)
[18:31:00.100095] Test:  [  0/157]  eta: 0:01:59  testing_loss: 0.9123 (0.9123)  acc1: 68.7500 (68.7500)  acc5: 96.8750 (96.8750)  time: 0.7623  data: 0.7312  max mem: 4132
[18:31:00.397988] Test:  [ 10/157]  eta: 0:00:14  testing_loss: 0.9365 (0.9306)  acc1: 68.7500 (67.6136)  acc5: 98.4375 (98.5795)  time: 0.0961  data: 0.0667  max mem: 4132
[18:31:00.688838] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.8756 (0.9015)  acc1: 68.7500 (68.9732)  acc5: 98.4375 (98.5119)  time: 0.0292  data: 0.0002  max mem: 4132
[18:31:00.982120] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.8756 (0.9134)  acc1: 70.3125 (69.2036)  acc5: 98.4375 (97.8831)  time: 0.0290  data: 0.0003  max mem: 4132
[18:31:01.277064] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.8907 (0.9102)  acc1: 68.7500 (69.1692)  acc5: 98.4375 (97.8659)  time: 0.0291  data: 0.0003  max mem: 4132
[18:31:01.575577] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.8907 (0.9103)  acc1: 67.1875 (68.7500)  acc5: 98.4375 (97.9167)  time: 0.0294  data: 0.0003  max mem: 4132
[18:31:01.867383] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.9156 (0.9085)  acc1: 67.1875 (68.6475)  acc5: 98.4375 (97.8227)  time: 0.0293  data: 0.0004  max mem: 4132
[18:31:02.161574] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.8919 (0.9032)  acc1: 67.1875 (68.9040)  acc5: 98.4375 (97.8433)  time: 0.0291  data: 0.0003  max mem: 4132
[18:31:02.454094] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.8964 (0.9070)  acc1: 68.7500 (68.8850)  acc5: 96.8750 (97.7238)  time: 0.0291  data: 0.0002  max mem: 4132
[18:31:02.746073] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.9202 (0.9086)  acc1: 68.7500 (68.9389)  acc5: 96.8750 (97.6820)  time: 0.0291  data: 0.0002  max mem: 4132
[18:31:03.035843] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.9458 (0.9149)  acc1: 67.1875 (68.8119)  acc5: 98.4375 (97.7104)  time: 0.0289  data: 0.0002  max mem: 4132
[18:31:03.325325] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.9906 (0.9157)  acc1: 67.1875 (68.9189)  acc5: 98.4375 (97.7196)  time: 0.0288  data: 0.0002  max mem: 4132
[18:31:03.616372] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.9068 (0.9134)  acc1: 68.7500 (68.9695)  acc5: 98.4375 (97.7014)  time: 0.0289  data: 0.0003  max mem: 4132
[18:31:03.910919] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.9064 (0.9154)  acc1: 68.7500 (68.9170)  acc5: 96.8750 (97.7457)  time: 0.0291  data: 0.0003  max mem: 4132
[18:31:04.206112] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.9327 (0.9131)  acc1: 67.1875 (69.0270)  acc5: 96.8750 (97.6950)  time: 0.0293  data: 0.0003  max mem: 4132
[18:31:04.489859] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.8747 (0.9102)  acc1: 68.7500 (69.1432)  acc5: 96.8750 (97.6511)  time: 0.0288  data: 0.0002  max mem: 4132
[18:31:04.649870] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.8746 (0.9117)  acc1: 68.7500 (69.1000)  acc5: 96.8750 (97.6300)  time: 0.0277  data: 0.0002  max mem: 4132
[18:31:04.839650] Test: Total time: 0:00:05 (0.0351 s / it)
[18:31:04.840233] * Acc@1 69.100 Acc@5 97.630 loss 0.912
[18:31:04.840574] Accuracy of the network on the 10000 test images: 69.1%
[18:31:04.840837] Max accuracy: 70.03%
[18:31:05.142409] log_dir: ./output_dir
[18:31:06.058607] Epoch: [27]  [  0/781]  eta: 0:11:54  lr: 0.000218  training_loss: 1.8371 (1.8371)  classification_loss: 1.6033 (1.6033)  loss_mask: 0.2338 (0.2338)  time: 0.9143  data: 0.7250  max mem: 4132
[18:31:09.406929] Epoch: [27]  [ 20/781]  eta: 0:02:34  lr: 0.000218  training_loss: 1.6544 (1.6729)  classification_loss: 1.5174 (1.5592)  loss_mask: 0.0814 (0.1137)  time: 0.1673  data: 0.0003  max mem: 4132
[18:31:12.701800] Epoch: [27]  [ 40/781]  eta: 0:02:16  lr: 0.000218  training_loss: 1.6463 (1.6781)  classification_loss: 1.5728 (1.5768)  loss_mask: 0.0752 (0.1013)  time: 0.1646  data: 0.0004  max mem: 4132
[18:31:16.039905] Epoch: [27]  [ 60/781]  eta: 0:02:08  lr: 0.000218  training_loss: 1.5971 (1.6512)  classification_loss: 1.5483 (1.5670)  loss_mask: 0.0373 (0.0841)  time: 0.1668  data: 0.0003  max mem: 4132
[18:31:19.332147] Epoch: [27]  [ 80/781]  eta: 0:02:02  lr: 0.000218  training_loss: 1.6221 (1.6454)  classification_loss: 1.5960 (1.5772)  loss_mask: 0.0183 (0.0682)  time: 0.1645  data: 0.0003  max mem: 4132
[18:31:22.595379] Epoch: [27]  [100/781]  eta: 0:01:57  lr: 0.000218  training_loss: 1.5862 (1.6268)  classification_loss: 1.5707 (1.5687)  loss_mask: 0.0156 (0.0581)  time: 0.1631  data: 0.0003  max mem: 4132
[18:31:25.883633] Epoch: [27]  [120/781]  eta: 0:01:53  lr: 0.000218  training_loss: 1.5933 (1.6176)  classification_loss: 1.5810 (1.5665)  loss_mask: 0.0123 (0.0511)  time: 0.1643  data: 0.0003  max mem: 4132
[18:31:29.181979] Epoch: [27]  [140/781]  eta: 0:01:49  lr: 0.000218  training_loss: 1.6096 (1.6160)  classification_loss: 1.5887 (1.5704)  loss_mask: 0.0099 (0.0456)  time: 0.1648  data: 0.0003  max mem: 4132
[18:31:32.473140] Epoch: [27]  [160/781]  eta: 0:01:45  lr: 0.000218  training_loss: 1.5066 (1.6032)  classification_loss: 1.5038 (1.5623)  loss_mask: 0.0081 (0.0410)  time: 0.1645  data: 0.0002  max mem: 4132
[18:31:35.751566] Epoch: [27]  [180/781]  eta: 0:01:41  lr: 0.000218  training_loss: 1.5459 (1.5975)  classification_loss: 1.5345 (1.5604)  loss_mask: 0.0043 (0.0370)  time: 0.1638  data: 0.0003  max mem: 4132
[18:31:39.040235] Epoch: [27]  [200/781]  eta: 0:01:37  lr: 0.000218  training_loss: 1.6080 (1.5981)  classification_loss: 1.6033 (1.5641)  loss_mask: 0.0046 (0.0340)  time: 0.1643  data: 0.0002  max mem: 4132
[18:31:42.333296] Epoch: [27]  [220/781]  eta: 0:01:34  lr: 0.000218  training_loss: 1.6114 (1.5968)  classification_loss: 1.6028 (1.5653)  loss_mask: 0.0044 (0.0314)  time: 0.1646  data: 0.0003  max mem: 4132
[18:31:45.599509] Epoch: [27]  [240/781]  eta: 0:01:30  lr: 0.000218  training_loss: 1.5656 (1.5953)  classification_loss: 1.5582 (1.5661)  loss_mask: 0.0044 (0.0292)  time: 0.1632  data: 0.0002  max mem: 4132
[18:31:48.870728] Epoch: [27]  [260/781]  eta: 0:01:27  lr: 0.000218  training_loss: 1.5649 (1.5936)  classification_loss: 1.5594 (1.5660)  loss_mask: 0.0038 (0.0277)  time: 0.1635  data: 0.0002  max mem: 4132
[18:31:52.176153] Epoch: [27]  [280/781]  eta: 0:01:23  lr: 0.000217  training_loss: 1.5733 (1.5932)  classification_loss: 1.5531 (1.5654)  loss_mask: 0.0235 (0.0278)  time: 0.1652  data: 0.0003  max mem: 4132
[18:31:55.470344] Epoch: [27]  [300/781]  eta: 0:01:20  lr: 0.000217  training_loss: 1.6324 (1.5983)  classification_loss: 1.6191 (1.5672)  loss_mask: 0.0409 (0.0310)  time: 0.1646  data: 0.0002  max mem: 4132
[18:31:58.772400] Epoch: [27]  [320/781]  eta: 0:01:16  lr: 0.000217  training_loss: 1.5861 (1.5987)  classification_loss: 1.5484 (1.5672)  loss_mask: 0.0341 (0.0314)  time: 0.1650  data: 0.0003  max mem: 4132
[18:32:02.038681] Epoch: [27]  [340/781]  eta: 0:01:13  lr: 0.000217  training_loss: 1.5332 (1.5956)  classification_loss: 1.4965 (1.5642)  loss_mask: 0.0207 (0.0314)  time: 0.1632  data: 0.0003  max mem: 4132
[18:32:05.329492] Epoch: [27]  [360/781]  eta: 0:01:10  lr: 0.000217  training_loss: 1.5951 (1.5955)  classification_loss: 1.5732 (1.5646)  loss_mask: 0.0129 (0.0309)  time: 0.1644  data: 0.0003  max mem: 4132
[18:32:08.617181] Epoch: [27]  [380/781]  eta: 0:01:06  lr: 0.000217  training_loss: 1.5500 (1.5946)  classification_loss: 1.5448 (1.5648)  loss_mask: 0.0085 (0.0298)  time: 0.1642  data: 0.0003  max mem: 4132
[18:32:11.899317] Epoch: [27]  [400/781]  eta: 0:01:03  lr: 0.000217  training_loss: 1.5055 (1.5911)  classification_loss: 1.4995 (1.5625)  loss_mask: 0.0053 (0.0286)  time: 0.1640  data: 0.0003  max mem: 4132
[18:32:15.191282] Epoch: [27]  [420/781]  eta: 0:01:00  lr: 0.000217  training_loss: 1.5419 (1.5902)  classification_loss: 1.5230 (1.5626)  loss_mask: 0.0057 (0.0276)  time: 0.1645  data: 0.0003  max mem: 4132
[18:32:18.491346] Epoch: [27]  [440/781]  eta: 0:00:56  lr: 0.000217  training_loss: 1.6314 (1.5917)  classification_loss: 1.5793 (1.5639)  loss_mask: 0.0234 (0.0278)  time: 0.1649  data: 0.0003  max mem: 4132
[18:32:21.763773] Epoch: [27]  [460/781]  eta: 0:00:53  lr: 0.000217  training_loss: 1.6316 (1.5941)  classification_loss: 1.5547 (1.5635)  loss_mask: 0.0984 (0.0306)  time: 0.1635  data: 0.0003  max mem: 4132
[18:32:25.021503] Epoch: [27]  [480/781]  eta: 0:00:49  lr: 0.000217  training_loss: 1.7210 (1.6000)  classification_loss: 1.5525 (1.5645)  loss_mask: 0.0848 (0.0355)  time: 0.1628  data: 0.0003  max mem: 4132
[18:32:28.285703] Epoch: [27]  [500/781]  eta: 0:00:46  lr: 0.000217  training_loss: 1.6137 (1.6015)  classification_loss: 1.5412 (1.5643)  loss_mask: 0.0576 (0.0372)  time: 0.1631  data: 0.0002  max mem: 4132
[18:32:31.609489] Epoch: [27]  [520/781]  eta: 0:00:43  lr: 0.000217  training_loss: 1.5847 (1.6011)  classification_loss: 1.5627 (1.5645)  loss_mask: 0.0216 (0.0366)  time: 0.1661  data: 0.0002  max mem: 4132
[18:32:34.908855] Epoch: [27]  [540/781]  eta: 0:00:39  lr: 0.000217  training_loss: 1.5792 (1.6006)  classification_loss: 1.5648 (1.5647)  loss_mask: 0.0139 (0.0358)  time: 0.1649  data: 0.0003  max mem: 4132
[18:32:38.167636] Epoch: [27]  [560/781]  eta: 0:00:36  lr: 0.000216  training_loss: 1.5187 (1.5983)  classification_loss: 1.5075 (1.5634)  loss_mask: 0.0091 (0.0349)  time: 0.1629  data: 0.0002  max mem: 4132
[18:32:41.429236] Epoch: [27]  [580/781]  eta: 0:00:33  lr: 0.000216  training_loss: 1.5670 (1.5965)  classification_loss: 1.5563 (1.5625)  loss_mask: 0.0044 (0.0340)  time: 0.1630  data: 0.0003  max mem: 4132
[18:32:44.739594] Epoch: [27]  [600/781]  eta: 0:00:29  lr: 0.000216  training_loss: 1.4910 (1.5940)  classification_loss: 1.4869 (1.5610)  loss_mask: 0.0043 (0.0330)  time: 0.1654  data: 0.0004  max mem: 4132
[18:32:48.030463] Epoch: [27]  [620/781]  eta: 0:00:26  lr: 0.000216  training_loss: 1.5482 (1.5926)  classification_loss: 1.5445 (1.5604)  loss_mask: 0.0037 (0.0322)  time: 0.1645  data: 0.0004  max mem: 4132
[18:32:51.332952] Epoch: [27]  [640/781]  eta: 0:00:23  lr: 0.000216  training_loss: 1.5319 (1.5908)  classification_loss: 1.5281 (1.5594)  loss_mask: 0.0042 (0.0314)  time: 0.1650  data: 0.0003  max mem: 4132
[18:32:54.651329] Epoch: [27]  [660/781]  eta: 0:00:20  lr: 0.000216  training_loss: 1.5617 (1.5902)  classification_loss: 1.5442 (1.5590)  loss_mask: 0.0058 (0.0312)  time: 0.1658  data: 0.0003  max mem: 4132
[18:32:57.972195] Epoch: [27]  [680/781]  eta: 0:00:16  lr: 0.000216  training_loss: 1.6175 (1.5908)  classification_loss: 1.5870 (1.5594)  loss_mask: 0.0161 (0.0314)  time: 0.1659  data: 0.0002  max mem: 4132
[18:33:01.297345] Epoch: [27]  [700/781]  eta: 0:00:13  lr: 0.000216  training_loss: 1.6697 (1.5957)  classification_loss: 1.5622 (1.5601)  loss_mask: 0.1339 (0.0355)  time: 0.1661  data: 0.0003  max mem: 4132
[18:33:04.601699] Epoch: [27]  [720/781]  eta: 0:00:10  lr: 0.000216  training_loss: 1.6049 (1.5977)  classification_loss: 1.5151 (1.5593)  loss_mask: 0.0641 (0.0384)  time: 0.1651  data: 0.0002  max mem: 4132
[18:33:07.871464] Epoch: [27]  [740/781]  eta: 0:00:06  lr: 0.000216  training_loss: 1.5496 (1.5968)  classification_loss: 1.5080 (1.5581)  loss_mask: 0.0434 (0.0387)  time: 0.1634  data: 0.0002  max mem: 4132
[18:33:11.136318] Epoch: [27]  [760/781]  eta: 0:00:03  lr: 0.000216  training_loss: 1.5872 (1.5965)  classification_loss: 1.5603 (1.5581)  loss_mask: 0.0227 (0.0384)  time: 0.1631  data: 0.0003  max mem: 4132
[18:33:14.410316] Epoch: [27]  [780/781]  eta: 0:00:00  lr: 0.000216  training_loss: 1.6007 (1.5964)  classification_loss: 1.5895 (1.5584)  loss_mask: 0.0254 (0.0381)  time: 0.1636  data: 0.0003  max mem: 4132
[18:33:14.576519] Epoch: [27] Total time: 0:02:09 (0.1657 s / it)
[18:33:14.579687] Averaged stats: lr: 0.000216  training_loss: 1.6007 (1.5964)  classification_loss: 1.5895 (1.5584)  loss_mask: 0.0254 (0.0381)
[18:33:15.319497] Test:  [  0/157]  eta: 0:01:55  testing_loss: 0.8590 (0.8590)  acc1: 75.0000 (75.0000)  acc5: 95.3125 (95.3125)  time: 0.7338  data: 0.6958  max mem: 4132
[18:33:15.625139] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.9438 (0.9211)  acc1: 65.6250 (68.1818)  acc5: 98.4375 (97.4432)  time: 0.0943  data: 0.0635  max mem: 4132
[18:33:15.916115] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.8758 (0.8827)  acc1: 68.7500 (69.6429)  acc5: 98.4375 (98.0655)  time: 0.0296  data: 0.0003  max mem: 4132
[18:33:16.206337] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.8437 (0.8843)  acc1: 70.3125 (69.6069)  acc5: 98.4375 (97.6310)  time: 0.0289  data: 0.0003  max mem: 4132
[18:33:16.501244] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.8798 (0.8813)  acc1: 70.3125 (69.7409)  acc5: 98.4375 (97.9421)  time: 0.0291  data: 0.0004  max mem: 4132
[18:33:16.796505] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.8734 (0.8802)  acc1: 70.3125 (70.0674)  acc5: 98.4375 (97.7941)  time: 0.0294  data: 0.0004  max mem: 4132
[18:33:17.087161] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.8668 (0.8791)  acc1: 68.7500 (70.1588)  acc5: 96.8750 (97.6691)  time: 0.0291  data: 0.0002  max mem: 4132
[18:33:17.374648] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.8608 (0.8719)  acc1: 73.4375 (70.5326)  acc5: 98.4375 (97.7333)  time: 0.0288  data: 0.0002  max mem: 4132
[18:33:17.662528] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.8608 (0.8805)  acc1: 71.8750 (70.2932)  acc5: 98.4375 (97.7431)  time: 0.0286  data: 0.0002  max mem: 4132
[18:33:17.949399] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.8947 (0.8821)  acc1: 68.7500 (70.2095)  acc5: 98.4375 (97.8365)  time: 0.0286  data: 0.0002  max mem: 4132
[18:33:18.247801] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.9141 (0.8889)  acc1: 67.1875 (69.8948)  acc5: 98.4375 (97.8496)  time: 0.0291  data: 0.0002  max mem: 4132
[18:33:18.544905] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.9351 (0.8904)  acc1: 67.1875 (69.9184)  acc5: 98.4375 (97.8463)  time: 0.0296  data: 0.0003  max mem: 4132
[18:33:18.839614] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.8434 (0.8849)  acc1: 71.8750 (70.1705)  acc5: 98.4375 (97.8306)  time: 0.0294  data: 0.0003  max mem: 4132
[18:33:19.131591] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.8434 (0.8863)  acc1: 71.8750 (70.1694)  acc5: 96.8750 (97.8053)  time: 0.0291  data: 0.0003  max mem: 4132
[18:33:19.420959] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.9324 (0.8869)  acc1: 71.8750 (70.2017)  acc5: 96.8750 (97.7615)  time: 0.0288  data: 0.0003  max mem: 4132
[18:33:19.707317] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.8864 (0.8855)  acc1: 71.8750 (70.2815)  acc5: 98.4375 (97.7649)  time: 0.0286  data: 0.0002  max mem: 4132
[18:33:19.862548] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.8754 (0.8865)  acc1: 68.7500 (70.2600)  acc5: 98.4375 (97.8000)  time: 0.0276  data: 0.0002  max mem: 4132
[18:33:20.051124] Test: Total time: 0:00:05 (0.0348 s / it)
[18:33:20.051921] * Acc@1 70.260 Acc@5 97.800 loss 0.886
[18:33:20.052305] Accuracy of the network on the 10000 test images: 70.3%
[18:33:20.052804] Max accuracy: 70.26%
[18:33:20.363518] log_dir: ./output_dir
[18:33:21.374183] Epoch: [28]  [  0/781]  eta: 0:13:07  lr: 0.000216  training_loss: 1.4684 (1.4684)  classification_loss: 1.4403 (1.4403)  loss_mask: 0.0281 (0.0281)  time: 1.0085  data: 0.7994  max mem: 4132
[18:33:24.661914] Epoch: [28]  [ 20/781]  eta: 0:02:35  lr: 0.000216  training_loss: 1.5926 (1.5896)  classification_loss: 1.5623 (1.5671)  loss_mask: 0.0190 (0.0225)  time: 0.1643  data: 0.0003  max mem: 4132
[18:33:27.953187] Epoch: [28]  [ 40/781]  eta: 0:02:17  lr: 0.000216  training_loss: 1.6325 (1.6214)  classification_loss: 1.5756 (1.5661)  loss_mask: 0.0200 (0.0553)  time: 0.1645  data: 0.0002  max mem: 4132
[18:33:31.231873] Epoch: [28]  [ 60/781]  eta: 0:02:08  lr: 0.000215  training_loss: 1.6134 (1.6198)  classification_loss: 1.5583 (1.5634)  loss_mask: 0.0435 (0.0565)  time: 0.1638  data: 0.0003  max mem: 4132
[18:33:34.514300] Epoch: [28]  [ 80/781]  eta: 0:02:02  lr: 0.000215  training_loss: 1.5397 (1.6054)  classification_loss: 1.5204 (1.5570)  loss_mask: 0.0219 (0.0484)  time: 0.1640  data: 0.0003  max mem: 4132
[18:33:37.826508] Epoch: [28]  [100/781]  eta: 0:01:57  lr: 0.000215  training_loss: 1.6039 (1.6073)  classification_loss: 1.5981 (1.5646)  loss_mask: 0.0133 (0.0426)  time: 0.1655  data: 0.0002  max mem: 4132
[18:33:41.137112] Epoch: [28]  [120/781]  eta: 0:01:53  lr: 0.000215  training_loss: 1.5339 (1.5966)  classification_loss: 1.5246 (1.5595)  loss_mask: 0.0086 (0.0370)  time: 0.1654  data: 0.0003  max mem: 4132
[18:33:44.436376] Epoch: [28]  [140/781]  eta: 0:01:49  lr: 0.000215  training_loss: 1.5756 (1.5928)  classification_loss: 1.5599 (1.5592)  loss_mask: 0.0078 (0.0336)  time: 0.1648  data: 0.0003  max mem: 4132
[18:33:47.730196] Epoch: [28]  [160/781]  eta: 0:01:45  lr: 0.000215  training_loss: 1.5717 (1.5906)  classification_loss: 1.5642 (1.5598)  loss_mask: 0.0095 (0.0308)  time: 0.1645  data: 0.0003  max mem: 4132
[18:33:51.052541] Epoch: [28]  [180/781]  eta: 0:01:41  lr: 0.000215  training_loss: 1.5819 (1.5868)  classification_loss: 1.5684 (1.5587)  loss_mask: 0.0043 (0.0281)  time: 0.1660  data: 0.0003  max mem: 4132
[18:33:54.383375] Epoch: [28]  [200/781]  eta: 0:01:38  lr: 0.000215  training_loss: 1.5492 (1.5852)  classification_loss: 1.5452 (1.5586)  loss_mask: 0.0044 (0.0266)  time: 0.1664  data: 0.0003  max mem: 4132
[18:33:57.693433] Epoch: [28]  [220/781]  eta: 0:01:34  lr: 0.000215  training_loss: 1.5348 (1.5834)  classification_loss: 1.5290 (1.5578)  loss_mask: 0.0060 (0.0255)  time: 0.1654  data: 0.0002  max mem: 4132
[18:34:00.988872] Epoch: [28]  [240/781]  eta: 0:01:31  lr: 0.000215  training_loss: 1.6586 (1.5907)  classification_loss: 1.5955 (1.5617)  loss_mask: 0.0540 (0.0290)  time: 0.1647  data: 0.0003  max mem: 4132
[18:34:04.302942] Epoch: [28]  [260/781]  eta: 0:01:27  lr: 0.000215  training_loss: 1.6104 (1.5926)  classification_loss: 1.5454 (1.5603)  loss_mask: 0.0385 (0.0323)  time: 0.1656  data: 0.0003  max mem: 4132
[18:34:07.594393] Epoch: [28]  [280/781]  eta: 0:01:24  lr: 0.000215  training_loss: 1.6671 (1.6011)  classification_loss: 1.5204 (1.5578)  loss_mask: 0.1507 (0.0432)  time: 0.1645  data: 0.0003  max mem: 4132
[18:34:10.896930] Epoch: [28]  [300/781]  eta: 0:01:20  lr: 0.000215  training_loss: 1.6161 (1.6017)  classification_loss: 1.5469 (1.5556)  loss_mask: 0.0738 (0.0461)  time: 0.1650  data: 0.0003  max mem: 4132
[18:34:14.213920] Epoch: [28]  [320/781]  eta: 0:01:17  lr: 0.000215  training_loss: 1.5873 (1.5999)  classification_loss: 1.5540 (1.5544)  loss_mask: 0.0317 (0.0456)  time: 0.1658  data: 0.0002  max mem: 4132
[18:34:17.489351] Epoch: [28]  [340/781]  eta: 0:01:13  lr: 0.000214  training_loss: 1.5604 (1.5973)  classification_loss: 1.5436 (1.5530)  loss_mask: 0.0194 (0.0443)  time: 0.1637  data: 0.0006  max mem: 4132
[18:34:20.784035] Epoch: [28]  [360/781]  eta: 0:01:10  lr: 0.000214  training_loss: 1.5650 (1.5966)  classification_loss: 1.5548 (1.5542)  loss_mask: 0.0103 (0.0425)  time: 0.1646  data: 0.0003  max mem: 4132
[18:34:24.108031] Epoch: [28]  [380/781]  eta: 0:01:07  lr: 0.000214  training_loss: 1.5282 (1.5950)  classification_loss: 1.5228 (1.5543)  loss_mask: 0.0081 (0.0407)  time: 0.1661  data: 0.0003  max mem: 4132
[18:34:27.421028] Epoch: [28]  [400/781]  eta: 0:01:03  lr: 0.000214  training_loss: 1.5247 (1.5922)  classification_loss: 1.5184 (1.5533)  loss_mask: 0.0055 (0.0389)  time: 0.1655  data: 0.0003  max mem: 4132
[18:34:30.729457] Epoch: [28]  [420/781]  eta: 0:01:00  lr: 0.000214  training_loss: 1.5536 (1.5909)  classification_loss: 1.5482 (1.5533)  loss_mask: 0.0047 (0.0376)  time: 0.1653  data: 0.0004  max mem: 4132
[18:34:34.052656] Epoch: [28]  [440/781]  eta: 0:00:56  lr: 0.000214  training_loss: 1.5936 (1.5910)  classification_loss: 1.5626 (1.5542)  loss_mask: 0.0157 (0.0368)  time: 0.1660  data: 0.0003  max mem: 4132
[18:34:37.383555] Epoch: [28]  [460/781]  eta: 0:00:53  lr: 0.000214  training_loss: 1.5476 (1.5891)  classification_loss: 1.5196 (1.5529)  loss_mask: 0.0183 (0.0363)  time: 0.1664  data: 0.0003  max mem: 4132
[18:34:40.719863] Epoch: [28]  [480/781]  eta: 0:00:50  lr: 0.000214  training_loss: 1.6113 (1.5892)  classification_loss: 1.5655 (1.5534)  loss_mask: 0.0168 (0.0358)  time: 0.1667  data: 0.0003  max mem: 4132
[18:34:44.046516] Epoch: [28]  [500/781]  eta: 0:00:46  lr: 0.000214  training_loss: 1.5878 (1.5899)  classification_loss: 1.5731 (1.5544)  loss_mask: 0.0148 (0.0355)  time: 0.1662  data: 0.0003  max mem: 4132
[18:34:47.349340] Epoch: [28]  [520/781]  eta: 0:00:43  lr: 0.000214  training_loss: 1.6181 (1.5945)  classification_loss: 1.5605 (1.5542)  loss_mask: 0.0876 (0.0403)  time: 0.1651  data: 0.0002  max mem: 4132
[18:34:50.655371] Epoch: [28]  [540/781]  eta: 0:00:40  lr: 0.000214  training_loss: 1.8149 (1.6036)  classification_loss: 1.5350 (1.5543)  loss_mask: 0.2188 (0.0493)  time: 0.1652  data: 0.0003  max mem: 4132
[18:34:53.942103] Epoch: [28]  [560/781]  eta: 0:00:36  lr: 0.000214  training_loss: 1.6037 (1.6045)  classification_loss: 1.5381 (1.5534)  loss_mask: 0.1043 (0.0512)  time: 0.1642  data: 0.0003  max mem: 4132
[18:34:57.256299] Epoch: [28]  [580/781]  eta: 0:00:33  lr: 0.000214  training_loss: 1.6065 (1.6046)  classification_loss: 1.5575 (1.5530)  loss_mask: 0.0606 (0.0515)  time: 0.1656  data: 0.0002  max mem: 4132
[18:35:00.537403] Epoch: [28]  [600/781]  eta: 0:00:30  lr: 0.000213  training_loss: 1.6337 (1.6066)  classification_loss: 1.5788 (1.5545)  loss_mask: 0.0423 (0.0522)  time: 0.1640  data: 0.0003  max mem: 4132
[18:35:03.859429] Epoch: [28]  [620/781]  eta: 0:00:26  lr: 0.000213  training_loss: 1.5821 (1.6058)  classification_loss: 1.5485 (1.5543)  loss_mask: 0.0243 (0.0515)  time: 0.1660  data: 0.0002  max mem: 4132
[18:35:07.156975] Epoch: [28]  [640/781]  eta: 0:00:23  lr: 0.000213  training_loss: 1.5834 (1.6050)  classification_loss: 1.5760 (1.5545)  loss_mask: 0.0155 (0.0505)  time: 0.1648  data: 0.0002  max mem: 4132
[18:35:10.446795] Epoch: [28]  [660/781]  eta: 0:00:20  lr: 0.000213  training_loss: 1.5731 (1.6041)  classification_loss: 1.5480 (1.5544)  loss_mask: 0.0174 (0.0497)  time: 0.1644  data: 0.0002  max mem: 4132
[18:35:13.768470] Epoch: [28]  [680/781]  eta: 0:00:16  lr: 0.000213  training_loss: 1.6141 (1.6053)  classification_loss: 1.5804 (1.5558)  loss_mask: 0.0242 (0.0495)  time: 0.1660  data: 0.0003  max mem: 4132
[18:35:17.076723] Epoch: [28]  [700/781]  eta: 0:00:13  lr: 0.000213  training_loss: 1.6134 (1.6070)  classification_loss: 1.5719 (1.5570)  loss_mask: 0.0597 (0.0500)  time: 0.1653  data: 0.0004  max mem: 4132
[18:35:20.377476] Epoch: [28]  [720/781]  eta: 0:00:10  lr: 0.000213  training_loss: 1.5686 (1.6063)  classification_loss: 1.5192 (1.5565)  loss_mask: 0.0315 (0.0498)  time: 0.1649  data: 0.0003  max mem: 4132
[18:35:23.687762] Epoch: [28]  [740/781]  eta: 0:00:06  lr: 0.000213  training_loss: 1.5446 (1.6050)  classification_loss: 1.5349 (1.5562)  loss_mask: 0.0111 (0.0488)  time: 0.1654  data: 0.0003  max mem: 4132
[18:35:26.997977] Epoch: [28]  [760/781]  eta: 0:00:03  lr: 0.000213  training_loss: 1.6040 (1.6048)  classification_loss: 1.5711 (1.5566)  loss_mask: 0.0183 (0.0482)  time: 0.1654  data: 0.0003  max mem: 4132
[18:35:30.285366] Epoch: [28]  [780/781]  eta: 0:00:00  lr: 0.000213  training_loss: 1.5463 (1.6036)  classification_loss: 1.5402 (1.5563)  loss_mask: 0.0097 (0.0473)  time: 0.1643  data: 0.0002  max mem: 4132
[18:35:30.470697] Epoch: [28] Total time: 0:02:10 (0.1666 s / it)
[18:35:30.472099] Averaged stats: lr: 0.000213  training_loss: 1.5463 (1.6036)  classification_loss: 1.5402 (1.5563)  loss_mask: 0.0097 (0.0473)
[18:35:31.167651] Test:  [  0/157]  eta: 0:01:48  testing_loss: 0.8654 (0.8654)  acc1: 68.7500 (68.7500)  acc5: 96.8750 (96.8750)  time: 0.6907  data: 0.6607  max mem: 4132
[18:35:31.470709] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.8664 (0.8795)  acc1: 70.3125 (70.3125)  acc5: 98.4375 (98.4375)  time: 0.0902  data: 0.0606  max mem: 4132
[18:35:31.760634] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.8506 (0.8526)  acc1: 71.8750 (71.5030)  acc5: 98.4375 (98.3631)  time: 0.0295  data: 0.0005  max mem: 4132
[18:35:32.050859] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.8445 (0.8565)  acc1: 73.4375 (71.6734)  acc5: 98.4375 (97.7319)  time: 0.0289  data: 0.0003  max mem: 4132
[18:35:32.340971] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.8333 (0.8508)  acc1: 71.8750 (72.2180)  acc5: 98.4375 (97.8277)  time: 0.0289  data: 0.0003  max mem: 4132
[18:35:32.633025] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.8308 (0.8504)  acc1: 71.8750 (71.9056)  acc5: 98.4375 (97.9167)  time: 0.0290  data: 0.0003  max mem: 4132
[18:35:32.921758] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.8383 (0.8469)  acc1: 70.3125 (71.8238)  acc5: 98.4375 (97.8227)  time: 0.0289  data: 0.0004  max mem: 4132
[18:35:33.220289] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.8009 (0.8392)  acc1: 73.4375 (72.2711)  acc5: 96.8750 (97.7773)  time: 0.0292  data: 0.0004  max mem: 4132
[18:35:33.517960] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.8267 (0.8477)  acc1: 71.8750 (71.9136)  acc5: 96.8750 (97.7431)  time: 0.0296  data: 0.0003  max mem: 4132
[18:35:33.814983] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.8575 (0.8483)  acc1: 68.7500 (71.8235)  acc5: 96.8750 (97.8194)  time: 0.0295  data: 0.0003  max mem: 4132
[18:35:34.103504] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.8575 (0.8530)  acc1: 70.3125 (71.7048)  acc5: 98.4375 (97.9115)  time: 0.0290  data: 0.0003  max mem: 4132
[18:35:34.394265] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.9273 (0.8551)  acc1: 70.3125 (71.6357)  acc5: 98.4375 (97.9167)  time: 0.0288  data: 0.0003  max mem: 4132
[18:35:34.690484] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.8889 (0.8525)  acc1: 71.8750 (71.8363)  acc5: 96.8750 (97.8693)  time: 0.0292  data: 0.0002  max mem: 4132
[18:35:34.976592] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.8376 (0.8547)  acc1: 71.8750 (71.7677)  acc5: 98.4375 (97.9008)  time: 0.0289  data: 0.0002  max mem: 4132
[18:35:35.267716] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.8456 (0.8527)  acc1: 71.8750 (71.7863)  acc5: 98.4375 (97.9167)  time: 0.0287  data: 0.0002  max mem: 4132
[18:35:35.551454] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.8456 (0.8507)  acc1: 71.8750 (71.9267)  acc5: 98.4375 (97.9408)  time: 0.0286  data: 0.0002  max mem: 4132
[18:35:35.709362] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.8374 (0.8517)  acc1: 73.4375 (71.9200)  acc5: 98.4375 (97.9700)  time: 0.0276  data: 0.0002  max mem: 4132
[18:35:35.876498] Test: Total time: 0:00:05 (0.0344 s / it)
[18:35:35.877073] * Acc@1 71.920 Acc@5 97.970 loss 0.852
[18:35:35.877403] Accuracy of the network on the 10000 test images: 71.9%
[18:35:35.877648] Max accuracy: 71.92%
[18:35:36.199258] log_dir: ./output_dir
[18:35:37.142441] Epoch: [29]  [  0/781]  eta: 0:12:14  lr: 0.000213  training_loss: 1.4331 (1.4331)  classification_loss: 1.4320 (1.4320)  loss_mask: 0.0011 (0.0011)  time: 0.9410  data: 0.7097  max mem: 4132
[18:35:40.465226] Epoch: [29]  [ 20/781]  eta: 0:02:34  lr: 0.000213  training_loss: 1.5069 (1.4980)  classification_loss: 1.4905 (1.4903)  loss_mask: 0.0070 (0.0078)  time: 0.1660  data: 0.0003  max mem: 4132
[18:35:43.767320] Epoch: [29]  [ 40/781]  eta: 0:02:16  lr: 0.000213  training_loss: 1.5905 (1.5407)  classification_loss: 1.5800 (1.5305)  loss_mask: 0.0105 (0.0101)  time: 0.1650  data: 0.0002  max mem: 4132
[18:35:47.056756] Epoch: [29]  [ 60/781]  eta: 0:02:08  lr: 0.000213  training_loss: 1.6026 (1.5941)  classification_loss: 1.5510 (1.5399)  loss_mask: 0.0328 (0.0543)  time: 0.1644  data: 0.0002  max mem: 4132
[18:35:50.328380] Epoch: [29]  [ 80/781]  eta: 0:02:02  lr: 0.000213  training_loss: 1.6203 (1.6122)  classification_loss: 1.5401 (1.5443)  loss_mask: 0.0726 (0.0679)  time: 0.1635  data: 0.0002  max mem: 4132
[18:35:53.587324] Epoch: [29]  [100/781]  eta: 0:01:57  lr: 0.000212  training_loss: 1.5513 (1.6061)  classification_loss: 1.5122 (1.5438)  loss_mask: 0.0389 (0.0623)  time: 0.1629  data: 0.0002  max mem: 4132
[18:35:56.881572] Epoch: [29]  [120/781]  eta: 0:01:52  lr: 0.000212  training_loss: 1.6022 (1.6064)  classification_loss: 1.5743 (1.5493)  loss_mask: 0.0227 (0.0571)  time: 0.1646  data: 0.0003  max mem: 4132
[18:36:00.169695] Epoch: [29]  [140/781]  eta: 0:01:48  lr: 0.000212  training_loss: 1.5411 (1.5957)  classification_loss: 1.5234 (1.5444)  loss_mask: 0.0149 (0.0513)  time: 0.1643  data: 0.0003  max mem: 4132
[18:36:03.531320] Epoch: [29]  [160/781]  eta: 0:01:45  lr: 0.000212  training_loss: 1.5133 (1.5890)  classification_loss: 1.4965 (1.5421)  loss_mask: 0.0121 (0.0469)  time: 0.1680  data: 0.0003  max mem: 4132
[18:36:06.865824] Epoch: [29]  [180/781]  eta: 0:01:41  lr: 0.000212  training_loss: 1.5418 (1.5857)  classification_loss: 1.5063 (1.5406)  loss_mask: 0.0202 (0.0451)  time: 0.1666  data: 0.0003  max mem: 4132
[18:36:10.193118] Epoch: [29]  [200/781]  eta: 0:01:38  lr: 0.000212  training_loss: 1.5624 (1.5826)  classification_loss: 1.5154 (1.5399)  loss_mask: 0.0162 (0.0427)  time: 0.1663  data: 0.0004  max mem: 4132
[18:36:13.523400] Epoch: [29]  [220/781]  eta: 0:01:34  lr: 0.000212  training_loss: 1.5290 (1.5776)  classification_loss: 1.5220 (1.5363)  loss_mask: 0.0159 (0.0413)  time: 0.1664  data: 0.0003  max mem: 4132
[18:36:16.871037] Epoch: [29]  [240/781]  eta: 0:01:31  lr: 0.000212  training_loss: 1.5578 (1.5765)  classification_loss: 1.5194 (1.5362)  loss_mask: 0.0222 (0.0403)  time: 0.1672  data: 0.0003  max mem: 4132
[18:36:20.199813] Epoch: [29]  [260/781]  eta: 0:01:27  lr: 0.000212  training_loss: 1.5364 (1.5739)  classification_loss: 1.5272 (1.5348)  loss_mask: 0.0214 (0.0390)  time: 0.1663  data: 0.0005  max mem: 4132
[18:36:23.524788] Epoch: [29]  [280/781]  eta: 0:01:24  lr: 0.000212  training_loss: 1.5340 (1.5724)  classification_loss: 1.5183 (1.5349)  loss_mask: 0.0143 (0.0375)  time: 0.1662  data: 0.0003  max mem: 4132
[18:36:26.797965] Epoch: [29]  [300/781]  eta: 0:01:20  lr: 0.000212  training_loss: 1.5702 (1.5705)  classification_loss: 1.5520 (1.5349)  loss_mask: 0.0071 (0.0356)  time: 0.1636  data: 0.0003  max mem: 4132
[18:36:30.065637] Epoch: [29]  [320/781]  eta: 0:01:17  lr: 0.000212  training_loss: 1.5219 (1.5676)  classification_loss: 1.5195 (1.5338)  loss_mask: 0.0048 (0.0338)  time: 0.1633  data: 0.0002  max mem: 4132
[18:36:33.379023] Epoch: [29]  [340/781]  eta: 0:01:13  lr: 0.000212  training_loss: 1.5036 (1.5652)  classification_loss: 1.5016 (1.5331)  loss_mask: 0.0039 (0.0321)  time: 0.1656  data: 0.0003  max mem: 4132
[18:36:36.717687] Epoch: [29]  [360/781]  eta: 0:01:10  lr: 0.000211  training_loss: 1.5291 (1.5652)  classification_loss: 1.5234 (1.5346)  loss_mask: 0.0029 (0.0306)  time: 0.1668  data: 0.0003  max mem: 4132
[18:36:40.042069] Epoch: [29]  [380/781]  eta: 0:01:07  lr: 0.000211  training_loss: 1.5400 (1.5654)  classification_loss: 1.5353 (1.5360)  loss_mask: 0.0070 (0.0294)  time: 0.1661  data: 0.0003  max mem: 4132
[18:36:43.356761] Epoch: [29]  [400/781]  eta: 0:01:03  lr: 0.000211  training_loss: 1.5323 (1.5647)  classification_loss: 1.5246 (1.5363)  loss_mask: 0.0064 (0.0284)  time: 0.1656  data: 0.0003  max mem: 4132
[18:36:46.676693] Epoch: [29]  [420/781]  eta: 0:01:00  lr: 0.000211  training_loss: 1.5307 (1.5639)  classification_loss: 1.5290 (1.5364)  loss_mask: 0.0043 (0.0275)  time: 0.1659  data: 0.0002  max mem: 4132
[18:36:49.956696] Epoch: [29]  [440/781]  eta: 0:00:56  lr: 0.000211  training_loss: 1.5113 (1.5621)  classification_loss: 1.5093 (1.5354)  loss_mask: 0.0041 (0.0267)  time: 0.1639  data: 0.0003  max mem: 4132
[18:36:53.270334] Epoch: [29]  [460/781]  eta: 0:00:53  lr: 0.000211  training_loss: 1.4442 (1.5590)  classification_loss: 1.4372 (1.5330)  loss_mask: 0.0070 (0.0260)  time: 0.1656  data: 0.0002  max mem: 4132
[18:36:56.582741] Epoch: [29]  [480/781]  eta: 0:00:50  lr: 0.000211  training_loss: 1.5688 (1.5600)  classification_loss: 1.5355 (1.5342)  loss_mask: 0.0085 (0.0258)  time: 0.1655  data: 0.0004  max mem: 4132
[18:36:59.865387] Epoch: [29]  [500/781]  eta: 0:00:46  lr: 0.000211  training_loss: 1.5731 (1.5630)  classification_loss: 1.5020 (1.5339)  loss_mask: 0.0328 (0.0291)  time: 0.1640  data: 0.0003  max mem: 4132
[18:37:03.159291] Epoch: [29]  [520/781]  eta: 0:00:43  lr: 0.000211  training_loss: 1.6301 (1.5664)  classification_loss: 1.5428 (1.5347)  loss_mask: 0.0915 (0.0317)  time: 0.1646  data: 0.0003  max mem: 4132
[18:37:06.481786] Epoch: [29]  [540/781]  eta: 0:00:40  lr: 0.000211  training_loss: 1.6072 (1.5670)  classification_loss: 1.5042 (1.5344)  loss_mask: 0.0410 (0.0327)  time: 0.1660  data: 0.0003  max mem: 4132
[18:37:09.779958] Epoch: [29]  [560/781]  eta: 0:00:36  lr: 0.000211  training_loss: 1.5068 (1.5653)  classification_loss: 1.4914 (1.5332)  loss_mask: 0.0128 (0.0321)  time: 0.1648  data: 0.0003  max mem: 4132
[18:37:13.093365] Epoch: [29]  [580/781]  eta: 0:00:33  lr: 0.000211  training_loss: 1.5203 (1.5645)  classification_loss: 1.5125 (1.5330)  loss_mask: 0.0127 (0.0315)  time: 0.1656  data: 0.0003  max mem: 4132
[18:37:16.400573] Epoch: [29]  [600/781]  eta: 0:00:30  lr: 0.000211  training_loss: 1.5174 (1.5638)  classification_loss: 1.5017 (1.5329)  loss_mask: 0.0115 (0.0309)  time: 0.1653  data: 0.0003  max mem: 4132
[18:37:19.719821] Epoch: [29]  [620/781]  eta: 0:00:26  lr: 0.000210  training_loss: 1.5464 (1.5633)  classification_loss: 1.4953 (1.5325)  loss_mask: 0.0114 (0.0308)  time: 0.1659  data: 0.0003  max mem: 4132
[18:37:22.997789] Epoch: [29]  [640/781]  eta: 0:00:23  lr: 0.000210  training_loss: 1.5894 (1.5640)  classification_loss: 1.5285 (1.5322)  loss_mask: 0.0378 (0.0318)  time: 0.1638  data: 0.0003  max mem: 4132
[18:37:26.266755] Epoch: [29]  [660/781]  eta: 0:00:20  lr: 0.000210  training_loss: 1.5666 (1.5636)  classification_loss: 1.5311 (1.5320)  loss_mask: 0.0222 (0.0316)  time: 0.1634  data: 0.0003  max mem: 4132
[18:37:29.605988] Epoch: [29]  [680/781]  eta: 0:00:16  lr: 0.000210  training_loss: 1.5905 (1.5640)  classification_loss: 1.5445 (1.5327)  loss_mask: 0.0141 (0.0313)  time: 0.1669  data: 0.0003  max mem: 4132
[18:37:32.881973] Epoch: [29]  [700/781]  eta: 0:00:13  lr: 0.000210  training_loss: 1.5415 (1.5639)  classification_loss: 1.5266 (1.5330)  loss_mask: 0.0102 (0.0309)  time: 0.1637  data: 0.0003  max mem: 4132
[18:37:36.177749] Epoch: [29]  [720/781]  eta: 0:00:10  lr: 0.000210  training_loss: 1.5164 (1.5627)  classification_loss: 1.5070 (1.5325)  loss_mask: 0.0064 (0.0302)  time: 0.1647  data: 0.0003  max mem: 4132
[18:37:39.490184] Epoch: [29]  [740/781]  eta: 0:00:06  lr: 0.000210  training_loss: 1.4703 (1.5617)  classification_loss: 1.4675 (1.5321)  loss_mask: 0.0033 (0.0296)  time: 0.1655  data: 0.0003  max mem: 4132
[18:37:42.812167] Epoch: [29]  [760/781]  eta: 0:00:03  lr: 0.000210  training_loss: 1.5623 (1.5618)  classification_loss: 1.5602 (1.5329)  loss_mask: 0.0040 (0.0289)  time: 0.1660  data: 0.0002  max mem: 4132
[18:37:46.095001] Epoch: [29]  [780/781]  eta: 0:00:00  lr: 0.000210  training_loss: 1.5293 (1.5614)  classification_loss: 1.5240 (1.5331)  loss_mask: 0.0024 (0.0283)  time: 0.1641  data: 0.0003  max mem: 4132
[18:37:46.275604] Epoch: [29] Total time: 0:02:10 (0.1666 s / it)
[18:37:46.276124] Averaged stats: lr: 0.000210  training_loss: 1.5293 (1.5614)  classification_loss: 1.5240 (1.5331)  loss_mask: 0.0024 (0.0283)
[18:37:47.016218] Test:  [  0/157]  eta: 0:01:55  testing_loss: 0.7811 (0.7811)  acc1: 73.4375 (73.4375)  acc5: 98.4375 (98.4375)  time: 0.7352  data: 0.7041  max mem: 4132
[18:37:47.315502] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.8370 (0.8771)  acc1: 73.4375 (71.1648)  acc5: 98.4375 (97.8693)  time: 0.0938  data: 0.0642  max mem: 4132
[18:37:47.604917] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.8284 (0.8371)  acc1: 73.4375 (72.0982)  acc5: 98.4375 (98.2143)  time: 0.0293  data: 0.0002  max mem: 4132
[18:37:47.897194] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.8340 (0.8519)  acc1: 73.4375 (71.7238)  acc5: 98.4375 (98.0847)  time: 0.0289  data: 0.0002  max mem: 4132
[18:37:48.188367] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.8309 (0.8434)  acc1: 71.8750 (71.8369)  acc5: 98.4375 (98.0564)  time: 0.0290  data: 0.0003  max mem: 4132
[18:37:48.481452] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.8077 (0.8443)  acc1: 73.4375 (71.9056)  acc5: 98.4375 (98.0086)  time: 0.0291  data: 0.0003  max mem: 4132
[18:37:48.772076] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.8078 (0.8423)  acc1: 71.8750 (71.6957)  acc5: 98.4375 (98.0277)  time: 0.0290  data: 0.0003  max mem: 4132
[18:37:49.065498] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7951 (0.8380)  acc1: 73.4375 (71.8750)  acc5: 98.4375 (97.9974)  time: 0.0290  data: 0.0003  max mem: 4132
[18:37:49.360395] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.8014 (0.8427)  acc1: 73.4375 (71.7785)  acc5: 96.8750 (97.9167)  time: 0.0292  data: 0.0003  max mem: 4132
[18:37:49.655316] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.8320 (0.8418)  acc1: 71.8750 (71.8063)  acc5: 98.4375 (97.9739)  time: 0.0293  data: 0.0003  max mem: 4132
[18:37:49.949322] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.8534 (0.8470)  acc1: 70.3125 (71.6584)  acc5: 98.4375 (98.0353)  time: 0.0293  data: 0.0002  max mem: 4132
[18:37:50.245469] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.9003 (0.8492)  acc1: 70.3125 (71.5231)  acc5: 98.4375 (97.9730)  time: 0.0293  data: 0.0002  max mem: 4132
[18:37:50.534060] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.8197 (0.8446)  acc1: 70.3125 (71.7200)  acc5: 96.8750 (97.9468)  time: 0.0291  data: 0.0002  max mem: 4132
[18:37:50.825589] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.8119 (0.8451)  acc1: 68.7500 (71.5887)  acc5: 98.4375 (97.9723)  time: 0.0289  data: 0.0003  max mem: 4132
[18:37:51.115209] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.8170 (0.8439)  acc1: 70.3125 (71.7974)  acc5: 98.4375 (97.9832)  time: 0.0289  data: 0.0002  max mem: 4132
[18:37:51.397924] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.8489 (0.8425)  acc1: 70.3125 (71.8233)  acc5: 98.4375 (97.9615)  time: 0.0285  data: 0.0002  max mem: 4132
[18:37:51.552190] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.8008 (0.8436)  acc1: 73.4375 (71.8400)  acc5: 98.4375 (97.9800)  time: 0.0273  data: 0.0001  max mem: 4132
[18:37:51.738199] Test: Total time: 0:00:05 (0.0348 s / it)
[18:37:51.739081] * Acc@1 71.840 Acc@5 97.980 loss 0.844
[18:37:51.739459] Accuracy of the network on the 10000 test images: 71.8%
[18:37:51.739758] Max accuracy: 71.92%
[18:37:51.919993] log_dir: ./output_dir
[18:37:52.876895] Epoch: [30]  [  0/781]  eta: 0:12:25  lr: 0.000210  training_loss: 1.4020 (1.4020)  classification_loss: 1.3997 (1.3997)  loss_mask: 0.0023 (0.0023)  time: 0.9545  data: 0.7460  max mem: 4132
[18:37:56.176354] Epoch: [30]  [ 20/781]  eta: 0:02:34  lr: 0.000210  training_loss: 1.5733 (1.5509)  classification_loss: 1.4887 (1.5050)  loss_mask: 0.0032 (0.0459)  time: 0.1649  data: 0.0002  max mem: 4132
[18:37:59.478860] Epoch: [30]  [ 40/781]  eta: 0:02:16  lr: 0.000210  training_loss: 1.6066 (1.5972)  classification_loss: 1.5678 (1.5308)  loss_mask: 0.0596 (0.0665)  time: 0.1650  data: 0.0002  max mem: 4132
[18:38:02.766314] Epoch: [30]  [ 60/781]  eta: 0:02:08  lr: 0.000210  training_loss: 1.6478 (1.6285)  classification_loss: 1.5423 (1.5381)  loss_mask: 0.1092 (0.0904)  time: 0.1643  data: 0.0003  max mem: 4132
[18:38:06.023284] Epoch: [30]  [ 80/781]  eta: 0:02:01  lr: 0.000210  training_loss: 1.5908 (1.6199)  classification_loss: 1.4831 (1.5300)  loss_mask: 0.0437 (0.0899)  time: 0.1627  data: 0.0002  max mem: 4132
[18:38:09.315054] Epoch: [30]  [100/781]  eta: 0:01:57  lr: 0.000209  training_loss: 1.5363 (1.6064)  classification_loss: 1.4862 (1.5272)  loss_mask: 0.0374 (0.0792)  time: 0.1645  data: 0.0003  max mem: 4132
[18:38:12.602423] Epoch: [30]  [120/781]  eta: 0:01:52  lr: 0.000209  training_loss: 1.5311 (1.5914)  classification_loss: 1.4588 (1.5189)  loss_mask: 0.0334 (0.0725)  time: 0.1642  data: 0.0002  max mem: 4132
[18:38:15.894708] Epoch: [30]  [140/781]  eta: 0:01:48  lr: 0.000209  training_loss: 1.5235 (1.5801)  classification_loss: 1.4948 (1.5144)  loss_mask: 0.0189 (0.0658)  time: 0.1645  data: 0.0003  max mem: 4132
[18:38:19.189597] Epoch: [30]  [160/781]  eta: 0:01:45  lr: 0.000209  training_loss: 1.5817 (1.5802)  classification_loss: 1.5046 (1.5151)  loss_mask: 0.0370 (0.0651)  time: 0.1646  data: 0.0003  max mem: 4132
[18:38:22.470783] Epoch: [30]  [180/781]  eta: 0:01:41  lr: 0.000209  training_loss: 1.6239 (1.5837)  classification_loss: 1.5672 (1.5207)  loss_mask: 0.0280 (0.0630)  time: 0.1640  data: 0.0002  max mem: 4132
[18:38:25.780660] Epoch: [30]  [200/781]  eta: 0:01:37  lr: 0.000209  training_loss: 1.5987 (1.5869)  classification_loss: 1.5434 (1.5248)  loss_mask: 0.0317 (0.0622)  time: 0.1654  data: 0.0003  max mem: 4132
[18:38:29.081425] Epoch: [30]  [220/781]  eta: 0:01:34  lr: 0.000209  training_loss: 1.7167 (1.6078)  classification_loss: 1.5803 (1.5295)  loss_mask: 0.1503 (0.0784)  time: 0.1649  data: 0.0003  max mem: 4132
[18:38:32.398113] Epoch: [30]  [240/781]  eta: 0:01:30  lr: 0.000209  training_loss: 3.2575 (1.7482)  classification_loss: 1.7651 (1.5653)  loss_mask: 1.0912 (0.1829)  time: 0.1657  data: 0.0003  max mem: 4132
[18:38:35.689489] Epoch: [30]  [260/781]  eta: 0:01:27  lr: 0.000209  training_loss: 2.7252 (1.8334)  classification_loss: 2.2976 (1.6215)  loss_mask: 0.4098 (0.2119)  time: 0.1645  data: 0.0003  max mem: 4132
[18:38:38.988915] Epoch: [30]  [280/781]  eta: 0:01:23  lr: 0.000209  training_loss: 2.5063 (1.8822)  classification_loss: 2.3064 (1.6704)  loss_mask: 0.1952 (0.2118)  time: 0.1649  data: 0.0003  max mem: 4132
[18:38:42.309329] Epoch: [30]  [300/781]  eta: 0:01:20  lr: 0.000209  training_loss: 2.4469 (1.9213)  classification_loss: 2.3008 (1.7124)  loss_mask: 0.1404 (0.2089)  time: 0.1659  data: 0.0003  max mem: 4132
[18:38:45.645701] Epoch: [30]  [320/781]  eta: 0:01:17  lr: 0.000209  training_loss: 2.3918 (1.9519)  classification_loss: 2.2959 (1.7488)  loss_mask: 0.0924 (0.2031)  time: 0.1667  data: 0.0003  max mem: 4132
[18:38:48.976162] Epoch: [30]  [340/781]  eta: 0:01:13  lr: 0.000208  training_loss: 2.3475 (1.9758)  classification_loss: 2.2694 (1.7796)  loss_mask: 0.0673 (0.1961)  time: 0.1664  data: 0.0003  max mem: 4132
[18:38:52.291516] Epoch: [30]  [360/781]  eta: 0:01:10  lr: 0.000208  training_loss: 2.4010 (2.0003)  classification_loss: 2.2767 (1.8072)  loss_mask: 0.1346 (0.1931)  time: 0.1656  data: 0.0003  max mem: 4132
[18:38:55.603193] Epoch: [30]  [380/781]  eta: 0:01:06  lr: 0.000208  training_loss: 2.3455 (2.0184)  classification_loss: 2.2468 (1.8307)  loss_mask: 0.0824 (0.1877)  time: 0.1655  data: 0.0003  max mem: 4132
[18:38:58.903363] Epoch: [30]  [400/781]  eta: 0:01:03  lr: 0.000208  training_loss: 2.2009 (2.0291)  classification_loss: 2.0997 (1.8448)  loss_mask: 0.0964 (0.1843)  time: 0.1649  data: 0.0002  max mem: 4132
[18:39:02.205889] Epoch: [30]  [420/781]  eta: 0:01:00  lr: 0.000208  training_loss: 2.0604 (2.0299)  classification_loss: 1.9591 (1.8504)  loss_mask: 0.0792 (0.1796)  time: 0.1650  data: 0.0003  max mem: 4132
[18:39:05.493571] Epoch: [30]  [440/781]  eta: 0:00:56  lr: 0.000208  training_loss: 1.8485 (2.0243)  classification_loss: 1.8162 (1.8495)  loss_mask: 0.0562 (0.1748)  time: 0.1643  data: 0.0003  max mem: 4132
[18:39:08.793647] Epoch: [30]  [460/781]  eta: 0:00:53  lr: 0.000208  training_loss: 1.9843 (2.0227)  classification_loss: 1.8783 (1.8515)  loss_mask: 0.0466 (0.1712)  time: 0.1649  data: 0.0003  max mem: 4132
[18:39:12.084603] Epoch: [30]  [480/781]  eta: 0:00:50  lr: 0.000208  training_loss: 1.9479 (2.0192)  classification_loss: 1.8603 (1.8511)  loss_mask: 0.0710 (0.1681)  time: 0.1645  data: 0.0003  max mem: 4132
[18:39:15.372351] Epoch: [30]  [500/781]  eta: 0:00:46  lr: 0.000208  training_loss: 1.8018 (2.0112)  classification_loss: 1.7151 (1.8458)  loss_mask: 0.0867 (0.1654)  time: 0.1643  data: 0.0002  max mem: 4132
[18:39:18.645257] Epoch: [30]  [520/781]  eta: 0:00:43  lr: 0.000208  training_loss: 1.7264 (2.0004)  classification_loss: 1.6814 (1.8395)  loss_mask: 0.0452 (0.1609)  time: 0.1635  data: 0.0004  max mem: 4132
[18:39:21.983541] Epoch: [30]  [540/781]  eta: 0:00:40  lr: 0.000208  training_loss: 1.7008 (1.9904)  classification_loss: 1.6709 (1.8337)  loss_mask: 0.0388 (0.1567)  time: 0.1668  data: 0.0003  max mem: 4132
[18:39:25.331796] Epoch: [30]  [560/781]  eta: 0:00:36  lr: 0.000208  training_loss: 1.6658 (1.9790)  classification_loss: 1.6110 (1.8259)  loss_mask: 0.0337 (0.1531)  time: 0.1673  data: 0.0003  max mem: 4132
[18:39:28.668618] Epoch: [30]  [580/781]  eta: 0:00:33  lr: 0.000208  training_loss: 1.6730 (1.9695)  classification_loss: 1.6123 (1.8187)  loss_mask: 0.0636 (0.1509)  time: 0.1667  data: 0.0004  max mem: 4132
[18:39:31.975809] Epoch: [30]  [600/781]  eta: 0:00:30  lr: 0.000207  training_loss: 1.6529 (1.9592)  classification_loss: 1.6057 (1.8114)  loss_mask: 0.0469 (0.1478)  time: 0.1653  data: 0.0003  max mem: 4132
[18:39:35.302308] Epoch: [30]  [620/781]  eta: 0:00:26  lr: 0.000207  training_loss: 1.6715 (1.9498)  classification_loss: 1.6315 (1.8053)  loss_mask: 0.0379 (0.1445)  time: 0.1662  data: 0.0003  max mem: 4132
[18:39:38.638782] Epoch: [30]  [640/781]  eta: 0:00:23  lr: 0.000207  training_loss: 1.6573 (1.9410)  classification_loss: 1.6022 (1.7992)  loss_mask: 0.0526 (0.1418)  time: 0.1667  data: 0.0003  max mem: 4132
[18:39:41.992440] Epoch: [30]  [660/781]  eta: 0:00:20  lr: 0.000207  training_loss: 1.6439 (1.9329)  classification_loss: 1.6096 (1.7938)  loss_mask: 0.0301 (0.1391)  time: 0.1675  data: 0.0003  max mem: 4132
[18:39:45.354486] Epoch: [30]  [680/781]  eta: 0:00:16  lr: 0.000207  training_loss: 1.6764 (1.9254)  classification_loss: 1.6083 (1.7882)  loss_mask: 0.0647 (0.1372)  time: 0.1680  data: 0.0003  max mem: 4132
[18:39:48.642208] Epoch: [30]  [700/781]  eta: 0:00:13  lr: 0.000207  training_loss: 1.6894 (1.9182)  classification_loss: 1.6366 (1.7833)  loss_mask: 0.0424 (0.1349)  time: 0.1643  data: 0.0003  max mem: 4132
[18:39:51.921659] Epoch: [30]  [720/781]  eta: 0:00:10  lr: 0.000207  training_loss: 1.6684 (1.9112)  classification_loss: 1.5898 (1.7785)  loss_mask: 0.0483 (0.1327)  time: 0.1639  data: 0.0003  max mem: 4132
[18:39:55.195229] Epoch: [30]  [740/781]  eta: 0:00:06  lr: 0.000207  training_loss: 1.6306 (1.9038)  classification_loss: 1.5784 (1.7731)  loss_mask: 0.0577 (0.1307)  time: 0.1636  data: 0.0002  max mem: 4132
[18:39:58.515205] Epoch: [30]  [760/781]  eta: 0:00:03  lr: 0.000207  training_loss: 1.6111 (1.8966)  classification_loss: 1.5727 (1.7683)  loss_mask: 0.0283 (0.1283)  time: 0.1659  data: 0.0003  max mem: 4132
[18:40:01.784321] Epoch: [30]  [780/781]  eta: 0:00:00  lr: 0.000207  training_loss: 1.5585 (1.8893)  classification_loss: 1.5436 (1.7634)  loss_mask: 0.0189 (0.1259)  time: 0.1634  data: 0.0002  max mem: 4132
[18:40:01.970141] Epoch: [30] Total time: 0:02:10 (0.1665 s / it)
[18:40:01.970601] Averaged stats: lr: 0.000207  training_loss: 1.5585 (1.8893)  classification_loss: 1.5436 (1.7634)  loss_mask: 0.0189 (0.1259)
[18:40:03.859847] Test:  [  0/157]  eta: 0:01:48  testing_loss: 0.8502 (0.8502)  acc1: 73.4375 (73.4375)  acc5: 95.3125 (95.3125)  time: 0.6939  data: 0.6571  max mem: 4132
[18:40:04.158052] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.8909 (0.9305)  acc1: 71.8750 (69.8864)  acc5: 98.4375 (97.7273)  time: 0.0897  data: 0.0599  max mem: 4132
[18:40:04.455605] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.8716 (0.8878)  acc1: 70.3125 (70.3869)  acc5: 98.4375 (98.3631)  time: 0.0294  data: 0.0002  max mem: 4132
[18:40:04.743582] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.8687 (0.8964)  acc1: 70.3125 (70.1109)  acc5: 98.4375 (98.0847)  time: 0.0291  data: 0.0002  max mem: 4132
[18:40:05.031335] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.8480 (0.8892)  acc1: 71.8750 (70.1982)  acc5: 98.4375 (98.1326)  time: 0.0286  data: 0.0002  max mem: 4132
[18:40:05.318494] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.8480 (0.8837)  acc1: 71.8750 (71.0784)  acc5: 98.4375 (97.9473)  time: 0.0286  data: 0.0002  max mem: 4132
[18:40:05.604701] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.8828 (0.8831)  acc1: 71.8750 (70.6711)  acc5: 98.4375 (97.9508)  time: 0.0285  data: 0.0002  max mem: 4132
[18:40:05.891600] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.8469 (0.8763)  acc1: 70.3125 (70.8187)  acc5: 98.4375 (97.9974)  time: 0.0285  data: 0.0002  max mem: 4132
[18:40:06.180213] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.8455 (0.8800)  acc1: 71.8750 (70.7948)  acc5: 96.8750 (97.8974)  time: 0.0286  data: 0.0002  max mem: 4132
[18:40:06.469929] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.8750 (0.8802)  acc1: 70.3125 (70.6387)  acc5: 98.4375 (97.9567)  time: 0.0287  data: 0.0002  max mem: 4132
[18:40:06.762766] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.8952 (0.8823)  acc1: 68.7500 (70.5136)  acc5: 98.4375 (98.0353)  time: 0.0289  data: 0.0002  max mem: 4132
[18:40:07.053517] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.9209 (0.8860)  acc1: 70.3125 (70.4955)  acc5: 98.4375 (97.8604)  time: 0.0290  data: 0.0002  max mem: 4132
[18:40:07.342487] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.8887 (0.8823)  acc1: 71.8750 (70.6224)  acc5: 96.8750 (97.8564)  time: 0.0288  data: 0.0002  max mem: 4132
[18:40:07.629038] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.8887 (0.8853)  acc1: 70.3125 (70.4079)  acc5: 98.4375 (97.8888)  time: 0.0286  data: 0.0002  max mem: 4132
[18:40:07.918633] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.9118 (0.8836)  acc1: 68.7500 (70.5895)  acc5: 96.8750 (97.8613)  time: 0.0287  data: 0.0002  max mem: 4132
[18:40:08.203425] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.8708 (0.8813)  acc1: 73.4375 (70.7471)  acc5: 96.8750 (97.8373)  time: 0.0286  data: 0.0002  max mem: 4132
[18:40:08.357720] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.8632 (0.8833)  acc1: 73.4375 (70.7100)  acc5: 98.4375 (97.8400)  time: 0.0275  data: 0.0002  max mem: 4132
[18:40:08.534372] Test: Total time: 0:00:05 (0.0342 s / it)
[18:40:08.534943] * Acc@1 70.710 Acc@5 97.840 loss 0.883
[18:40:08.535376] Accuracy of the network on the 10000 test images: 70.7%
[18:40:08.535596] Max accuracy: 71.92%
[18:40:08.887342] log_dir: ./output_dir
[18:40:09.840910] Epoch: [31]  [  0/781]  eta: 0:12:23  lr: 0.000207  training_loss: 1.4816 (1.4816)  classification_loss: 1.4464 (1.4464)  loss_mask: 0.0353 (0.0353)  time: 0.9515  data: 0.7313  max mem: 4132
[18:40:13.142478] Epoch: [31]  [ 20/781]  eta: 0:02:34  lr: 0.000207  training_loss: 1.6274 (1.6401)  classification_loss: 1.5695 (1.5556)  loss_mask: 0.0572 (0.0845)  time: 0.1649  data: 0.0003  max mem: 4132
[18:40:16.437160] Epoch: [31]  [ 40/781]  eta: 0:02:16  lr: 0.000207  training_loss: 1.5596 (1.6210)  classification_loss: 1.5106 (1.5547)  loss_mask: 0.0356 (0.0663)  time: 0.1646  data: 0.0003  max mem: 4132
[18:40:19.740309] Epoch: [31]  [ 60/781]  eta: 0:02:08  lr: 0.000207  training_loss: 1.6089 (1.6166)  classification_loss: 1.5815 (1.5635)  loss_mask: 0.0231 (0.0530)  time: 0.1651  data: 0.0003  max mem: 4132
[18:40:23.060421] Epoch: [31]  [ 80/781]  eta: 0:02:02  lr: 0.000206  training_loss: 1.5737 (1.6101)  classification_loss: 1.5431 (1.5643)  loss_mask: 0.0152 (0.0458)  time: 0.1659  data: 0.0002  max mem: 4132
[18:40:26.350658] Epoch: [31]  [100/781]  eta: 0:01:57  lr: 0.000206  training_loss: 1.5997 (1.6107)  classification_loss: 1.5582 (1.5660)  loss_mask: 0.0221 (0.0447)  time: 0.1644  data: 0.0003  max mem: 4132
[18:40:29.691854] Epoch: [31]  [120/781]  eta: 0:01:53  lr: 0.000206  training_loss: 1.5713 (1.6112)  classification_loss: 1.5393 (1.5611)  loss_mask: 0.0298 (0.0501)  time: 0.1670  data: 0.0003  max mem: 4132
[18:40:32.975341] Epoch: [31]  [140/781]  eta: 0:01:49  lr: 0.000206  training_loss: 1.5617 (1.6036)  classification_loss: 1.5431 (1.5554)  loss_mask: 0.0250 (0.0482)  time: 0.1641  data: 0.0002  max mem: 4132
[18:40:36.304219] Epoch: [31]  [160/781]  eta: 0:01:45  lr: 0.000206  training_loss: 1.5912 (1.6012)  classification_loss: 1.5817 (1.5570)  loss_mask: 0.0121 (0.0443)  time: 0.1663  data: 0.0003  max mem: 4132
[18:40:39.613241] Epoch: [31]  [180/781]  eta: 0:01:41  lr: 0.000206  training_loss: 1.5751 (1.5988)  classification_loss: 1.5074 (1.5551)  loss_mask: 0.0292 (0.0436)  time: 0.1654  data: 0.0003  max mem: 4132
[18:40:42.960015] Epoch: [31]  [200/781]  eta: 0:01:38  lr: 0.000206  training_loss: 1.5966 (1.5971)  classification_loss: 1.5572 (1.5533)  loss_mask: 0.0295 (0.0438)  time: 0.1673  data: 0.0003  max mem: 4132
[18:40:46.258565] Epoch: [31]  [220/781]  eta: 0:01:34  lr: 0.000206  training_loss: 1.5923 (1.5955)  classification_loss: 1.5548 (1.5532)  loss_mask: 0.0201 (0.0423)  time: 0.1648  data: 0.0003  max mem: 4132
[18:40:49.586745] Epoch: [31]  [240/781]  eta: 0:01:31  lr: 0.000206  training_loss: 1.5396 (1.5918)  classification_loss: 1.5122 (1.5514)  loss_mask: 0.0169 (0.0404)  time: 0.1663  data: 0.0003  max mem: 4132
[18:40:52.879339] Epoch: [31]  [260/781]  eta: 0:01:27  lr: 0.000206  training_loss: 1.5193 (1.5887)  classification_loss: 1.4920 (1.5501)  loss_mask: 0.0151 (0.0386)  time: 0.1645  data: 0.0002  max mem: 4132
[18:40:56.178252] Epoch: [31]  [280/781]  eta: 0:01:24  lr: 0.000206  training_loss: 1.5120 (1.5830)  classification_loss: 1.5023 (1.5464)  loss_mask: 0.0092 (0.0366)  time: 0.1648  data: 0.0002  max mem: 4132
[18:40:59.471122] Epoch: [31]  [300/781]  eta: 0:01:20  lr: 0.000206  training_loss: 1.5723 (1.5824)  classification_loss: 1.5511 (1.5470)  loss_mask: 0.0091 (0.0354)  time: 0.1645  data: 0.0003  max mem: 4132
[18:41:02.811129] Epoch: [31]  [320/781]  eta: 0:01:17  lr: 0.000205  training_loss: 1.5263 (1.5788)  classification_loss: 1.5181 (1.5449)  loss_mask: 0.0095 (0.0338)  time: 0.1669  data: 0.0004  max mem: 4132
[18:41:06.166977] Epoch: [31]  [340/781]  eta: 0:01:14  lr: 0.000205  training_loss: 1.5009 (1.5751)  classification_loss: 1.4980 (1.5427)  loss_mask: 0.0052 (0.0324)  time: 0.1677  data: 0.0003  max mem: 4132
[18:41:09.477414] Epoch: [31]  [360/781]  eta: 0:01:10  lr: 0.000205  training_loss: 1.5833 (1.5743)  classification_loss: 1.5646 (1.5422)  loss_mask: 0.0085 (0.0320)  time: 0.1654  data: 0.0004  max mem: 4132
[18:41:12.816351] Epoch: [31]  [380/781]  eta: 0:01:07  lr: 0.000205  training_loss: 1.5438 (1.5723)  classification_loss: 1.5193 (1.5411)  loss_mask: 0.0122 (0.0312)  time: 0.1668  data: 0.0003  max mem: 4132
[18:41:16.151735] Epoch: [31]  [400/781]  eta: 0:01:03  lr: 0.000205  training_loss: 1.4762 (1.5707)  classification_loss: 1.4333 (1.5385)  loss_mask: 0.0195 (0.0323)  time: 0.1667  data: 0.0003  max mem: 4132
[18:41:19.500668] Epoch: [31]  [420/781]  eta: 0:01:00  lr: 0.000205  training_loss: 1.5794 (1.5719)  classification_loss: 1.5298 (1.5382)  loss_mask: 0.0376 (0.0337)  time: 0.1673  data: 0.0006  max mem: 4132
[18:41:22.821728] Epoch: [31]  [440/781]  eta: 0:00:57  lr: 0.000205  training_loss: 1.5013 (1.5684)  classification_loss: 1.4673 (1.5349)  loss_mask: 0.0210 (0.0336)  time: 0.1660  data: 0.0003  max mem: 4132
[18:41:26.108302] Epoch: [31]  [460/781]  eta: 0:00:53  lr: 0.000205  training_loss: 1.4887 (1.5652)  classification_loss: 1.4660 (1.5324)  loss_mask: 0.0129 (0.0328)  time: 0.1642  data: 0.0003  max mem: 4132
[18:41:29.401023] Epoch: [31]  [480/781]  eta: 0:00:50  lr: 0.000205  training_loss: 1.6438 (1.5686)  classification_loss: 1.5119 (1.5330)  loss_mask: 0.0593 (0.0356)  time: 0.1645  data: 0.0002  max mem: 4132
[18:41:32.675556] Epoch: [31]  [500/781]  eta: 0:00:46  lr: 0.000205  training_loss: 1.6185 (1.5700)  classification_loss: 1.5467 (1.5328)  loss_mask: 0.0621 (0.0372)  time: 0.1636  data: 0.0003  max mem: 4132
[18:41:35.964047] Epoch: [31]  [520/781]  eta: 0:00:43  lr: 0.000205  training_loss: 1.5105 (1.5704)  classification_loss: 1.5019 (1.5332)  loss_mask: 0.0299 (0.0371)  time: 0.1643  data: 0.0003  max mem: 4132
[18:41:39.269604] Epoch: [31]  [540/781]  eta: 0:00:40  lr: 0.000205  training_loss: 1.5443 (1.5690)  classification_loss: 1.5309 (1.5328)  loss_mask: 0.0109 (0.0363)  time: 0.1652  data: 0.0003  max mem: 4132
[18:41:42.563985] Epoch: [31]  [560/781]  eta: 0:00:36  lr: 0.000204  training_loss: 1.5154 (1.5671)  classification_loss: 1.5072 (1.5318)  loss_mask: 0.0074 (0.0353)  time: 0.1646  data: 0.0003  max mem: 4132
[18:41:45.855918] Epoch: [31]  [580/781]  eta: 0:00:33  lr: 0.000204  training_loss: 1.5322 (1.5655)  classification_loss: 1.5300 (1.5310)  loss_mask: 0.0091 (0.0345)  time: 0.1645  data: 0.0003  max mem: 4132
[18:41:49.119186] Epoch: [31]  [600/781]  eta: 0:00:30  lr: 0.000204  training_loss: 1.4965 (1.5644)  classification_loss: 1.4732 (1.5300)  loss_mask: 0.0204 (0.0345)  time: 0.1631  data: 0.0002  max mem: 4132
[18:41:52.399081] Epoch: [31]  [620/781]  eta: 0:00:26  lr: 0.000204  training_loss: 1.6633 (1.5688)  classification_loss: 1.5397 (1.5314)  loss_mask: 0.1334 (0.0374)  time: 0.1639  data: 0.0003  max mem: 4132
[18:41:55.719483] Epoch: [31]  [640/781]  eta: 0:00:23  lr: 0.000204  training_loss: 1.6459 (1.5718)  classification_loss: 1.5344 (1.5324)  loss_mask: 0.1139 (0.0394)  time: 0.1659  data: 0.0003  max mem: 4132
[18:41:58.983175] Epoch: [31]  [660/781]  eta: 0:00:20  lr: 0.000204  training_loss: 1.6264 (1.5736)  classification_loss: 1.5575 (1.5333)  loss_mask: 0.0561 (0.0402)  time: 0.1631  data: 0.0003  max mem: 4132
[18:42:02.284615] Epoch: [31]  [680/781]  eta: 0:00:16  lr: 0.000204  training_loss: 1.5611 (1.5731)  classification_loss: 1.5247 (1.5332)  loss_mask: 0.0229 (0.0399)  time: 0.1650  data: 0.0004  max mem: 4132
[18:42:05.561642] Epoch: [31]  [700/781]  eta: 0:00:13  lr: 0.000204  training_loss: 1.5449 (1.5725)  classification_loss: 1.4894 (1.5328)  loss_mask: 0.0190 (0.0396)  time: 0.1638  data: 0.0003  max mem: 4132
[18:42:08.884526] Epoch: [31]  [720/781]  eta: 0:00:10  lr: 0.000204  training_loss: 1.5299 (1.5715)  classification_loss: 1.5026 (1.5322)  loss_mask: 0.0141 (0.0393)  time: 0.1660  data: 0.0003  max mem: 4132
[18:42:12.202680] Epoch: [31]  [740/781]  eta: 0:00:06  lr: 0.000204  training_loss: 1.4649 (1.5688)  classification_loss: 1.4502 (1.5304)  loss_mask: 0.0049 (0.0384)  time: 0.1658  data: 0.0003  max mem: 4132
[18:42:15.534296] Epoch: [31]  [760/781]  eta: 0:00:03  lr: 0.000204  training_loss: 1.5751 (1.5689)  classification_loss: 1.5707 (1.5311)  loss_mask: 0.0068 (0.0378)  time: 0.1665  data: 0.0003  max mem: 4132
[18:42:18.806492] Epoch: [31]  [780/781]  eta: 0:00:00  lr: 0.000204  training_loss: 1.5358 (1.5692)  classification_loss: 1.5027 (1.5313)  loss_mask: 0.0216 (0.0379)  time: 0.1635  data: 0.0002  max mem: 4132
[18:42:18.966574] Epoch: [31] Total time: 0:02:10 (0.1666 s / it)
[18:42:18.967854] Averaged stats: lr: 0.000204  training_loss: 1.5358 (1.5692)  classification_loss: 1.5027 (1.5313)  loss_mask: 0.0216 (0.0379)
[18:42:19.688653] Test:  [  0/157]  eta: 0:01:52  testing_loss: 0.7933 (0.7933)  acc1: 79.6875 (79.6875)  acc5: 96.8750 (96.8750)  time: 0.7156  data: 0.6841  max mem: 4132
[18:42:19.981804] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.8520 (0.8800)  acc1: 71.8750 (71.7330)  acc5: 98.4375 (98.1534)  time: 0.0913  data: 0.0624  max mem: 4132
[18:42:20.269156] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.8252 (0.8411)  acc1: 71.8750 (72.5446)  acc5: 98.4375 (98.6607)  time: 0.0286  data: 0.0002  max mem: 4132
[18:42:20.556986] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.8252 (0.8513)  acc1: 71.8750 (72.1270)  acc5: 98.4375 (98.4879)  time: 0.0285  data: 0.0002  max mem: 4132
[18:42:20.846101] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.8369 (0.8488)  acc1: 70.3125 (72.0274)  acc5: 98.4375 (98.3232)  time: 0.0286  data: 0.0002  max mem: 4132
[18:42:21.139252] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.8369 (0.8461)  acc1: 71.8750 (72.3652)  acc5: 98.4375 (98.1311)  time: 0.0289  data: 0.0002  max mem: 4132
[18:42:21.432042] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.8425 (0.8436)  acc1: 71.8750 (72.2592)  acc5: 96.8750 (98.1301)  time: 0.0291  data: 0.0003  max mem: 4132
[18:42:21.721641] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7921 (0.8354)  acc1: 75.0000 (72.7993)  acc5: 98.4375 (98.1514)  time: 0.0289  data: 0.0003  max mem: 4132
[18:42:22.020605] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7921 (0.8412)  acc1: 75.0000 (72.6659)  acc5: 98.4375 (98.0517)  time: 0.0292  data: 0.0002  max mem: 4132
[18:42:22.317723] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.8261 (0.8382)  acc1: 73.4375 (72.7850)  acc5: 98.4375 (98.1456)  time: 0.0295  data: 0.0002  max mem: 4132
[18:42:22.608692] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.8574 (0.8425)  acc1: 73.4375 (72.5557)  acc5: 98.4375 (98.1590)  time: 0.0292  data: 0.0002  max mem: 4132
[18:42:22.897154] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.8861 (0.8442)  acc1: 70.3125 (72.3958)  acc5: 98.4375 (98.0434)  time: 0.0288  data: 0.0003  max mem: 4132
[18:42:23.189100] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.8197 (0.8405)  acc1: 70.3125 (72.4819)  acc5: 98.4375 (98.0630)  time: 0.0289  data: 0.0002  max mem: 4132
[18:42:23.478698] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.8246 (0.8431)  acc1: 68.7500 (72.3163)  acc5: 98.4375 (97.9723)  time: 0.0289  data: 0.0002  max mem: 4132
[18:42:23.774008] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.8626 (0.8427)  acc1: 68.7500 (72.4291)  acc5: 96.8750 (97.9610)  time: 0.0290  data: 0.0003  max mem: 4132
[18:42:24.060508] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.8592 (0.8411)  acc1: 73.4375 (72.5373)  acc5: 98.4375 (97.9822)  time: 0.0289  data: 0.0002  max mem: 4132
[18:42:24.218653] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.8274 (0.8435)  acc1: 73.4375 (72.4600)  acc5: 98.4375 (97.9800)  time: 0.0277  data: 0.0002  max mem: 4132
[18:42:24.409110] Test: Total time: 0:00:05 (0.0346 s / it)
[18:42:24.409997] * Acc@1 72.460 Acc@5 97.980 loss 0.843
[18:42:24.410321] Accuracy of the network on the 10000 test images: 72.5%
[18:42:24.410500] Max accuracy: 72.46%
[18:42:24.618100] log_dir: ./output_dir
[18:42:25.558006] Epoch: [32]  [  0/781]  eta: 0:12:12  lr: 0.000204  training_loss: 1.4801 (1.4801)  classification_loss: 1.4290 (1.4290)  loss_mask: 0.0511 (0.0511)  time: 0.9380  data: 0.7351  max mem: 4132
[18:42:28.835377] Epoch: [32]  [ 20/781]  eta: 0:02:32  lr: 0.000204  training_loss: 1.5624 (1.5530)  classification_loss: 1.5463 (1.5160)  loss_mask: 0.0146 (0.0370)  time: 0.1638  data: 0.0002  max mem: 4132
[18:42:32.115346] Epoch: [32]  [ 40/781]  eta: 0:02:15  lr: 0.000203  training_loss: 1.5493 (1.5428)  classification_loss: 1.5131 (1.5021)  loss_mask: 0.0271 (0.0407)  time: 0.1639  data: 0.0002  max mem: 4132
[18:42:35.414755] Epoch: [32]  [ 60/781]  eta: 0:02:07  lr: 0.000203  training_loss: 1.6180 (1.5580)  classification_loss: 1.5871 (1.5189)  loss_mask: 0.0252 (0.0391)  time: 0.1649  data: 0.0002  max mem: 4132
[18:42:38.694506] Epoch: [32]  [ 80/781]  eta: 0:02:01  lr: 0.000203  training_loss: 1.5193 (1.5529)  classification_loss: 1.5107 (1.5198)  loss_mask: 0.0139 (0.0331)  time: 0.1639  data: 0.0003  max mem: 4132
[18:42:41.999056] Epoch: [32]  [100/781]  eta: 0:01:57  lr: 0.000203  training_loss: 1.5541 (1.5570)  classification_loss: 1.5406 (1.5241)  loss_mask: 0.0128 (0.0329)  time: 0.1651  data: 0.0003  max mem: 4132
[18:42:45.357965] Epoch: [32]  [120/781]  eta: 0:01:53  lr: 0.000203  training_loss: 1.6140 (1.5634)  classification_loss: 1.5187 (1.5219)  loss_mask: 0.0690 (0.0415)  time: 0.1679  data: 0.0003  max mem: 4132
[18:42:48.692076] Epoch: [32]  [140/781]  eta: 0:01:49  lr: 0.000203  training_loss: 1.5844 (1.5681)  classification_loss: 1.5471 (1.5226)  loss_mask: 0.0398 (0.0455)  time: 0.1666  data: 0.0003  max mem: 4132
[18:42:52.020513] Epoch: [32]  [160/781]  eta: 0:01:45  lr: 0.000203  training_loss: 1.5228 (1.5649)  classification_loss: 1.4887 (1.5210)  loss_mask: 0.0243 (0.0439)  time: 0.1663  data: 0.0005  max mem: 4132
[18:42:55.341067] Epoch: [32]  [180/781]  eta: 0:01:41  lr: 0.000203  training_loss: 1.5331 (1.5624)  classification_loss: 1.4930 (1.5195)  loss_mask: 0.0234 (0.0429)  time: 0.1659  data: 0.0003  max mem: 4132
[18:42:58.689592] Epoch: [32]  [200/781]  eta: 0:01:38  lr: 0.000203  training_loss: 1.6074 (1.5663)  classification_loss: 1.5438 (1.5198)  loss_mask: 0.0507 (0.0465)  time: 0.1673  data: 0.0003  max mem: 4132
[18:43:02.029969] Epoch: [32]  [220/781]  eta: 0:01:34  lr: 0.000203  training_loss: 1.5420 (1.5651)  classification_loss: 1.5020 (1.5200)  loss_mask: 0.0269 (0.0450)  time: 0.1669  data: 0.0005  max mem: 4132
[18:43:05.343297] Epoch: [32]  [240/781]  eta: 0:01:31  lr: 0.000203  training_loss: 1.6026 (1.5672)  classification_loss: 1.5068 (1.5205)  loss_mask: 0.0466 (0.0467)  time: 0.1656  data: 0.0003  max mem: 4132
[18:43:08.615666] Epoch: [32]  [260/781]  eta: 0:01:27  lr: 0.000203  training_loss: 1.5415 (1.5668)  classification_loss: 1.5118 (1.5198)  loss_mask: 0.0417 (0.0471)  time: 0.1635  data: 0.0003  max mem: 4132
[18:43:11.904163] Epoch: [32]  [280/781]  eta: 0:01:24  lr: 0.000202  training_loss: 1.5766 (1.5674)  classification_loss: 1.5413 (1.5222)  loss_mask: 0.0171 (0.0452)  time: 0.1643  data: 0.0003  max mem: 4132
[18:43:15.177394] Epoch: [32]  [300/781]  eta: 0:01:20  lr: 0.000202  training_loss: 1.5234 (1.5662)  classification_loss: 1.4972 (1.5222)  loss_mask: 0.0162 (0.0440)  time: 0.1636  data: 0.0003  max mem: 4132
[18:43:18.484173] Epoch: [32]  [320/781]  eta: 0:01:17  lr: 0.000202  training_loss: 1.4978 (1.5625)  classification_loss: 1.4499 (1.5191)  loss_mask: 0.0183 (0.0434)  time: 0.1652  data: 0.0002  max mem: 4132
[18:43:21.807150] Epoch: [32]  [340/781]  eta: 0:01:13  lr: 0.000202  training_loss: 1.5398 (1.5621)  classification_loss: 1.5092 (1.5193)  loss_mask: 0.0286 (0.0428)  time: 0.1661  data: 0.0003  max mem: 4132
[18:43:25.106188] Epoch: [32]  [360/781]  eta: 0:01:10  lr: 0.000202  training_loss: 1.5107 (1.5595)  classification_loss: 1.4944 (1.5173)  loss_mask: 0.0166 (0.0422)  time: 0.1649  data: 0.0003  max mem: 4132
[18:43:28.393148] Epoch: [32]  [380/781]  eta: 0:01:07  lr: 0.000202  training_loss: 1.5273 (1.5577)  classification_loss: 1.5104 (1.5169)  loss_mask: 0.0128 (0.0408)  time: 0.1642  data: 0.0003  max mem: 4132
[18:43:31.704622] Epoch: [32]  [400/781]  eta: 0:01:03  lr: 0.000202  training_loss: 1.5292 (1.5566)  classification_loss: 1.5188 (1.5173)  loss_mask: 0.0059 (0.0393)  time: 0.1655  data: 0.0003  max mem: 4132
[18:43:35.007984] Epoch: [32]  [420/781]  eta: 0:01:00  lr: 0.000202  training_loss: 1.5686 (1.5597)  classification_loss: 1.5029 (1.5176)  loss_mask: 0.0536 (0.0421)  time: 0.1651  data: 0.0003  max mem: 4132
[18:43:38.318808] Epoch: [32]  [440/781]  eta: 0:00:56  lr: 0.000202  training_loss: 1.5425 (1.5590)  classification_loss: 1.5185 (1.5173)  loss_mask: 0.0227 (0.0417)  time: 0.1654  data: 0.0002  max mem: 4132
[18:43:41.623095] Epoch: [32]  [460/781]  eta: 0:00:53  lr: 0.000202  training_loss: 1.5024 (1.5572)  classification_loss: 1.4598 (1.5157)  loss_mask: 0.0273 (0.0415)  time: 0.1651  data: 0.0002  max mem: 4132
[18:43:44.915447] Epoch: [32]  [480/781]  eta: 0:00:50  lr: 0.000202  training_loss: 1.5945 (1.5581)  classification_loss: 1.5512 (1.5162)  loss_mask: 0.0324 (0.0419)  time: 0.1645  data: 0.0003  max mem: 4132
[18:43:48.214100] Epoch: [32]  [500/781]  eta: 0:00:46  lr: 0.000202  training_loss: 1.5477 (1.5572)  classification_loss: 1.5176 (1.5161)  loss_mask: 0.0180 (0.0411)  time: 0.1648  data: 0.0003  max mem: 4132
[18:43:51.529690] Epoch: [32]  [520/781]  eta: 0:00:43  lr: 0.000201  training_loss: 1.4952 (1.5560)  classification_loss: 1.4905 (1.5161)  loss_mask: 0.0075 (0.0399)  time: 0.1657  data: 0.0005  max mem: 4132
[18:43:54.840067] Epoch: [32]  [540/781]  eta: 0:00:40  lr: 0.000201  training_loss: 1.5054 (1.5539)  classification_loss: 1.4983 (1.5153)  loss_mask: 0.0043 (0.0386)  time: 0.1654  data: 0.0002  max mem: 4132
[18:43:58.129840] Epoch: [32]  [560/781]  eta: 0:00:36  lr: 0.000201  training_loss: 1.5038 (1.5519)  classification_loss: 1.5001 (1.5144)  loss_mask: 0.0043 (0.0374)  time: 0.1644  data: 0.0003  max mem: 4132
[18:44:01.471090] Epoch: [32]  [580/781]  eta: 0:00:33  lr: 0.000201  training_loss: 1.4838 (1.5502)  classification_loss: 1.4800 (1.5139)  loss_mask: 0.0035 (0.0363)  time: 0.1669  data: 0.0003  max mem: 4132
[18:44:04.814195] Epoch: [32]  [600/781]  eta: 0:00:30  lr: 0.000201  training_loss: 1.5005 (1.5487)  classification_loss: 1.4959 (1.5135)  loss_mask: 0.0030 (0.0352)  time: 0.1671  data: 0.0003  max mem: 4132
[18:44:08.127800] Epoch: [32]  [620/781]  eta: 0:00:26  lr: 0.000201  training_loss: 1.4814 (1.5471)  classification_loss: 1.4771 (1.5130)  loss_mask: 0.0023 (0.0341)  time: 0.1656  data: 0.0003  max mem: 4132
[18:44:11.450963] Epoch: [32]  [640/781]  eta: 0:00:23  lr: 0.000201  training_loss: 1.5066 (1.5453)  classification_loss: 1.5045 (1.5121)  loss_mask: 0.0024 (0.0331)  time: 0.1661  data: 0.0003  max mem: 4132
[18:44:14.759844] Epoch: [32]  [660/781]  eta: 0:00:20  lr: 0.000201  training_loss: 1.4984 (1.5451)  classification_loss: 1.4955 (1.5129)  loss_mask: 0.0022 (0.0322)  time: 0.1653  data: 0.0002  max mem: 4132
[18:44:18.020754] Epoch: [32]  [680/781]  eta: 0:00:16  lr: 0.000201  training_loss: 1.4564 (1.5430)  classification_loss: 1.4554 (1.5117)  loss_mask: 0.0014 (0.0313)  time: 0.1630  data: 0.0003  max mem: 4132
[18:44:21.336113] Epoch: [32]  [700/781]  eta: 0:00:13  lr: 0.000201  training_loss: 1.5047 (1.5424)  classification_loss: 1.5036 (1.5119)  loss_mask: 0.0017 (0.0305)  time: 0.1657  data: 0.0003  max mem: 4132
[18:44:24.655148] Epoch: [32]  [720/781]  eta: 0:00:10  lr: 0.000201  training_loss: 1.5351 (1.5414)  classification_loss: 1.5324 (1.5117)  loss_mask: 0.0014 (0.0297)  time: 0.1658  data: 0.0004  max mem: 4132
[18:44:27.992073] Epoch: [32]  [740/781]  eta: 0:00:06  lr: 0.000201  training_loss: 1.4484 (1.5385)  classification_loss: 1.4472 (1.5096)  loss_mask: 0.0013 (0.0289)  time: 0.1667  data: 0.0006  max mem: 4132
[18:44:31.316167] Epoch: [32]  [760/781]  eta: 0:00:03  lr: 0.000200  training_loss: 1.5126 (1.5384)  classification_loss: 1.5118 (1.5101)  loss_mask: 0.0017 (0.0283)  time: 0.1661  data: 0.0004  max mem: 4132
[18:44:34.612433] Epoch: [32]  [780/781]  eta: 0:00:00  lr: 0.000200  training_loss: 1.5762 (1.5391)  classification_loss: 1.5357 (1.5101)  loss_mask: 0.0129 (0.0290)  time: 0.1647  data: 0.0002  max mem: 4132
[18:44:34.795488] Epoch: [32] Total time: 0:02:10 (0.1667 s / it)
[18:44:34.796556] Averaged stats: lr: 0.000200  training_loss: 1.5762 (1.5391)  classification_loss: 1.5357 (1.5101)  loss_mask: 0.0129 (0.0290)
[18:44:35.564591] Test:  [  0/157]  eta: 0:01:57  testing_loss: 0.8462 (0.8462)  acc1: 71.8750 (71.8750)  acc5: 95.3125 (95.3125)  time: 0.7455  data: 0.7040  max mem: 4132
[18:44:35.860840] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.8281 (0.8584)  acc1: 71.8750 (72.0170)  acc5: 98.4375 (98.1534)  time: 0.0945  data: 0.0645  max mem: 4132
[18:44:36.150659] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.8025 (0.8138)  acc1: 73.4375 (73.2143)  acc5: 98.4375 (98.3631)  time: 0.0291  data: 0.0004  max mem: 4132
[18:44:36.444227] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.7968 (0.8277)  acc1: 73.4375 (72.7823)  acc5: 98.4375 (97.9839)  time: 0.0289  data: 0.0003  max mem: 4132
[18:44:36.735412] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.7919 (0.8230)  acc1: 75.0000 (72.9802)  acc5: 96.8750 (97.9421)  time: 0.0290  data: 0.0003  max mem: 4132
[18:44:37.026304] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.7828 (0.8145)  acc1: 75.0000 (73.4375)  acc5: 98.4375 (97.9779)  time: 0.0289  data: 0.0003  max mem: 4132
[18:44:37.313127] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.7828 (0.8160)  acc1: 73.4375 (73.1301)  acc5: 98.4375 (98.0533)  time: 0.0287  data: 0.0003  max mem: 4132
[18:44:37.599948] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.8040 (0.8086)  acc1: 73.4375 (73.4155)  acc5: 98.4375 (98.1074)  time: 0.0285  data: 0.0002  max mem: 4132
[18:44:37.891814] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.8100 (0.8151)  acc1: 73.4375 (73.2446)  acc5: 98.4375 (97.9938)  time: 0.0288  data: 0.0002  max mem: 4132
[18:44:38.184251] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.8462 (0.8148)  acc1: 73.4375 (73.1799)  acc5: 96.8750 (98.0082)  time: 0.0291  data: 0.0003  max mem: 4132
[18:44:38.477316] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.8513 (0.8184)  acc1: 71.8750 (72.9734)  acc5: 98.4375 (98.0507)  time: 0.0291  data: 0.0003  max mem: 4132
[18:44:38.764456] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.8715 (0.8207)  acc1: 71.8750 (72.9589)  acc5: 98.4375 (98.0574)  time: 0.0289  data: 0.0003  max mem: 4132
[18:44:39.052540] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.8188 (0.8184)  acc1: 71.8750 (72.9210)  acc5: 98.4375 (98.0243)  time: 0.0286  data: 0.0002  max mem: 4132
[18:44:39.339563] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.8126 (0.8199)  acc1: 71.8750 (72.8531)  acc5: 96.8750 (98.0200)  time: 0.0286  data: 0.0002  max mem: 4132
[18:44:39.626871] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.8332 (0.8193)  acc1: 70.3125 (72.8613)  acc5: 98.4375 (98.0496)  time: 0.0286  data: 0.0002  max mem: 4132
[18:44:39.911260] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.8357 (0.8184)  acc1: 70.3125 (72.8684)  acc5: 98.4375 (97.9925)  time: 0.0284  data: 0.0002  max mem: 4132
[18:44:40.072110] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.8389 (0.8203)  acc1: 70.3125 (72.7600)  acc5: 98.4375 (98.0000)  time: 0.0278  data: 0.0002  max mem: 4132
[18:44:40.280201] Test: Total time: 0:00:05 (0.0349 s / it)
[18:44:40.280899] * Acc@1 72.760 Acc@5 98.000 loss 0.820
[18:44:40.281256] Accuracy of the network on the 10000 test images: 72.8%
[18:44:40.281428] Max accuracy: 72.76%
[18:44:40.588641] log_dir: ./output_dir
[18:44:41.603436] Epoch: [33]  [  0/781]  eta: 0:13:10  lr: 0.000200  training_loss: 1.4213 (1.4213)  classification_loss: 1.4133 (1.4133)  loss_mask: 0.0079 (0.0079)  time: 1.0127  data: 0.7897  max mem: 4132
[18:44:44.922221] Epoch: [33]  [ 20/781]  eta: 0:02:36  lr: 0.000200  training_loss: 1.5648 (1.5771)  classification_loss: 1.4773 (1.4699)  loss_mask: 0.0875 (0.1072)  time: 0.1658  data: 0.0002  max mem: 4132
[18:44:48.215266] Epoch: [33]  [ 40/781]  eta: 0:02:17  lr: 0.000200  training_loss: 1.5171 (1.5481)  classification_loss: 1.4760 (1.4636)  loss_mask: 0.0430 (0.0844)  time: 0.1646  data: 0.0002  max mem: 4132
[18:44:51.496552] Epoch: [33]  [ 60/781]  eta: 0:02:08  lr: 0.000200  training_loss: 1.5181 (1.5426)  classification_loss: 1.4687 (1.4773)  loss_mask: 0.0226 (0.0653)  time: 0.1640  data: 0.0003  max mem: 4132
[18:44:54.890393] Epoch: [33]  [ 80/781]  eta: 0:02:03  lr: 0.000200  training_loss: 1.5455 (1.5393)  classification_loss: 1.5173 (1.4867)  loss_mask: 0.0102 (0.0525)  time: 0.1695  data: 0.0003  max mem: 4132
[18:44:58.203181] Epoch: [33]  [100/781]  eta: 0:01:58  lr: 0.000200  training_loss: 1.5167 (1.5342)  classification_loss: 1.5137 (1.4907)  loss_mask: 0.0063 (0.0435)  time: 0.1655  data: 0.0002  max mem: 4132
[18:45:01.512049] Epoch: [33]  [120/781]  eta: 0:01:54  lr: 0.000200  training_loss: 1.4767 (1.5244)  classification_loss: 1.4738 (1.4872)  loss_mask: 0.0046 (0.0372)  time: 0.1654  data: 0.0003  max mem: 4132
[18:45:04.843033] Epoch: [33]  [140/781]  eta: 0:01:50  lr: 0.000200  training_loss: 1.4864 (1.5205)  classification_loss: 1.4839 (1.4882)  loss_mask: 0.0029 (0.0323)  time: 0.1665  data: 0.0004  max mem: 4132
[18:45:08.126865] Epoch: [33]  [160/781]  eta: 0:01:46  lr: 0.000200  training_loss: 1.4642 (1.5144)  classification_loss: 1.4618 (1.4858)  loss_mask: 0.0024 (0.0286)  time: 0.1641  data: 0.0003  max mem: 4132
[18:45:11.414486] Epoch: [33]  [180/781]  eta: 0:01:42  lr: 0.000200  training_loss: 1.4425 (1.5075)  classification_loss: 1.4407 (1.4817)  loss_mask: 0.0023 (0.0258)  time: 0.1643  data: 0.0002  max mem: 4132
[18:45:14.749297] Epoch: [33]  [200/781]  eta: 0:01:38  lr: 0.000199  training_loss: 1.4414 (1.5061)  classification_loss: 1.4402 (1.4827)  loss_mask: 0.0015 (0.0234)  time: 0.1666  data: 0.0004  max mem: 4132
[18:45:18.073131] Epoch: [33]  [220/781]  eta: 0:01:35  lr: 0.000199  training_loss: 1.4793 (1.5018)  classification_loss: 1.4774 (1.4804)  loss_mask: 0.0016 (0.0215)  time: 0.1661  data: 0.0003  max mem: 4132
[18:45:21.363593] Epoch: [33]  [240/781]  eta: 0:01:31  lr: 0.000199  training_loss: 1.4496 (1.5010)  classification_loss: 1.4482 (1.4811)  loss_mask: 0.0015 (0.0198)  time: 0.1644  data: 0.0003  max mem: 4132
[18:45:24.659099] Epoch: [33]  [260/781]  eta: 0:01:27  lr: 0.000199  training_loss: 1.4595 (1.5003)  classification_loss: 1.4575 (1.4818)  loss_mask: 0.0016 (0.0185)  time: 0.1647  data: 0.0003  max mem: 4132
[18:45:27.954487] Epoch: [33]  [280/781]  eta: 0:01:24  lr: 0.000199  training_loss: 1.6528 (1.5156)  classification_loss: 1.4854 (1.4834)  loss_mask: 0.1189 (0.0322)  time: 0.1647  data: 0.0004  max mem: 4132
[18:45:31.275783] Epoch: [33]  [300/781]  eta: 0:01:20  lr: 0.000199  training_loss: 1.6322 (1.5234)  classification_loss: 1.5027 (1.4847)  loss_mask: 0.1230 (0.0388)  time: 0.1660  data: 0.0003  max mem: 4132
[18:45:34.613927] Epoch: [33]  [320/781]  eta: 0:01:17  lr: 0.000199  training_loss: 1.6300 (1.5305)  classification_loss: 1.5084 (1.4856)  loss_mask: 0.0946 (0.0448)  time: 0.1668  data: 0.0003  max mem: 4132
[18:45:37.892085] Epoch: [33]  [340/781]  eta: 0:01:14  lr: 0.000199  training_loss: 1.5887 (1.5346)  classification_loss: 1.4352 (1.4858)  loss_mask: 0.0896 (0.0488)  time: 0.1638  data: 0.0006  max mem: 4132
[18:45:41.213258] Epoch: [33]  [360/781]  eta: 0:01:10  lr: 0.000199  training_loss: 1.6046 (1.5382)  classification_loss: 1.5336 (1.4876)  loss_mask: 0.0642 (0.0506)  time: 0.1660  data: 0.0003  max mem: 4132
[18:45:44.544564] Epoch: [33]  [380/781]  eta: 0:01:07  lr: 0.000199  training_loss: 1.5130 (1.5369)  classification_loss: 1.4877 (1.4872)  loss_mask: 0.0314 (0.0497)  time: 0.1664  data: 0.0003  max mem: 4132
[18:45:47.864372] Epoch: [33]  [400/781]  eta: 0:01:03  lr: 0.000199  training_loss: 1.4885 (1.5348)  classification_loss: 1.4595 (1.4867)  loss_mask: 0.0163 (0.0481)  time: 0.1659  data: 0.0004  max mem: 4132
[18:45:51.155314] Epoch: [33]  [420/781]  eta: 0:01:00  lr: 0.000199  training_loss: 1.5054 (1.5329)  classification_loss: 1.4894 (1.4865)  loss_mask: 0.0113 (0.0464)  time: 0.1645  data: 0.0003  max mem: 4132
[18:45:54.424139] Epoch: [33]  [440/781]  eta: 0:00:57  lr: 0.000198  training_loss: 1.5295 (1.5317)  classification_loss: 1.5110 (1.4868)  loss_mask: 0.0128 (0.0449)  time: 0.1634  data: 0.0002  max mem: 4132
[18:45:57.705905] Epoch: [33]  [460/781]  eta: 0:00:53  lr: 0.000198  training_loss: 1.4537 (1.5300)  classification_loss: 1.4404 (1.4866)  loss_mask: 0.0067 (0.0434)  time: 0.1640  data: 0.0002  max mem: 4132
[18:46:01.020979] Epoch: [33]  [480/781]  eta: 0:00:50  lr: 0.000198  training_loss: 1.5032 (1.5304)  classification_loss: 1.5023 (1.4886)  loss_mask: 0.0053 (0.0418)  time: 0.1657  data: 0.0002  max mem: 4132
[18:46:04.399909] Epoch: [33]  [500/781]  eta: 0:00:46  lr: 0.000198  training_loss: 1.4689 (1.5287)  classification_loss: 1.4608 (1.4883)  loss_mask: 0.0051 (0.0404)  time: 0.1689  data: 0.0003  max mem: 4132
[18:46:07.741614] Epoch: [33]  [520/781]  eta: 0:00:43  lr: 0.000198  training_loss: 1.4977 (1.5272)  classification_loss: 1.4946 (1.4882)  loss_mask: 0.0055 (0.0390)  time: 0.1670  data: 0.0004  max mem: 4132
[18:46:11.027475] Epoch: [33]  [540/781]  eta: 0:00:40  lr: 0.000198  training_loss: 1.5441 (1.5282)  classification_loss: 1.5385 (1.4901)  loss_mask: 0.0079 (0.0381)  time: 0.1642  data: 0.0002  max mem: 4132
[18:46:14.354937] Epoch: [33]  [560/781]  eta: 0:00:36  lr: 0.000198  training_loss: 1.6458 (1.5349)  classification_loss: 1.4826 (1.4903)  loss_mask: 0.1510 (0.0446)  time: 0.1663  data: 0.0002  max mem: 4132
[18:46:17.647754] Epoch: [33]  [580/781]  eta: 0:00:33  lr: 0.000198  training_loss: 1.6178 (1.5381)  classification_loss: 1.5687 (1.4922)  loss_mask: 0.0711 (0.0459)  time: 0.1645  data: 0.0003  max mem: 4132
[18:46:21.003632] Epoch: [33]  [600/781]  eta: 0:00:30  lr: 0.000198  training_loss: 1.5366 (1.5389)  classification_loss: 1.5075 (1.4933)  loss_mask: 0.0341 (0.0456)  time: 0.1677  data: 0.0005  max mem: 4132
[18:46:24.292974] Epoch: [33]  [620/781]  eta: 0:00:26  lr: 0.000198  training_loss: 1.5156 (1.5384)  classification_loss: 1.4918 (1.4935)  loss_mask: 0.0211 (0.0449)  time: 0.1644  data: 0.0003  max mem: 4132
[18:46:27.618582] Epoch: [33]  [640/781]  eta: 0:00:23  lr: 0.000198  training_loss: 1.4098 (1.5360)  classification_loss: 1.3953 (1.4919)  loss_mask: 0.0145 (0.0440)  time: 0.1662  data: 0.0004  max mem: 4132
[18:46:30.907879] Epoch: [33]  [660/781]  eta: 0:00:20  lr: 0.000198  training_loss: 1.4817 (1.5354)  classification_loss: 1.4685 (1.4921)  loss_mask: 0.0125 (0.0433)  time: 0.1644  data: 0.0003  max mem: 4132
[18:46:34.184763] Epoch: [33]  [680/781]  eta: 0:00:16  lr: 0.000197  training_loss: 1.5493 (1.5355)  classification_loss: 1.5276 (1.4930)  loss_mask: 0.0143 (0.0425)  time: 0.1638  data: 0.0002  max mem: 4132
[18:46:37.481507] Epoch: [33]  [700/781]  eta: 0:00:13  lr: 0.000197  training_loss: 1.5086 (1.5351)  classification_loss: 1.4998 (1.4935)  loss_mask: 0.0093 (0.0416)  time: 0.1647  data: 0.0003  max mem: 4132
[18:46:40.785909] Epoch: [33]  [720/781]  eta: 0:00:10  lr: 0.000197  training_loss: 1.5260 (1.5352)  classification_loss: 1.5215 (1.4942)  loss_mask: 0.0112 (0.0410)  time: 0.1651  data: 0.0002  max mem: 4132
[18:46:44.074189] Epoch: [33]  [740/781]  eta: 0:00:06  lr: 0.000197  training_loss: 1.5122 (1.5344)  classification_loss: 1.4347 (1.4930)  loss_mask: 0.0313 (0.0414)  time: 0.1643  data: 0.0003  max mem: 4132
[18:46:47.388250] Epoch: [33]  [760/781]  eta: 0:00:03  lr: 0.000197  training_loss: 1.5373 (1.5353)  classification_loss: 1.4855 (1.4936)  loss_mask: 0.0320 (0.0417)  time: 0.1656  data: 0.0004  max mem: 4132
[18:46:50.707906] Epoch: [33]  [780/781]  eta: 0:00:00  lr: 0.000197  training_loss: 1.4926 (1.5349)  classification_loss: 1.4729 (1.4937)  loss_mask: 0.0191 (0.0412)  time: 0.1659  data: 0.0003  max mem: 4132
[18:46:50.880606] Epoch: [33] Total time: 0:02:10 (0.1668 s / it)
[18:46:50.881901] Averaged stats: lr: 0.000197  training_loss: 1.4926 (1.5349)  classification_loss: 1.4729 (1.4937)  loss_mask: 0.0191 (0.0412)
[18:46:51.617672] Test:  [  0/157]  eta: 0:01:54  testing_loss: 0.7999 (0.7999)  acc1: 73.4375 (73.4375)  acc5: 95.3125 (95.3125)  time: 0.7315  data: 0.6851  max mem: 4132
[18:46:51.907925] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.8102 (0.8359)  acc1: 73.4375 (71.8750)  acc5: 98.4375 (98.5795)  time: 0.0927  data: 0.0625  max mem: 4132
[18:46:52.199227] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.7619 (0.7964)  acc1: 73.4375 (73.6607)  acc5: 98.4375 (98.4375)  time: 0.0288  data: 0.0002  max mem: 4132
[18:46:52.491300] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.7809 (0.8154)  acc1: 73.4375 (73.0847)  acc5: 98.4375 (98.2863)  time: 0.0289  data: 0.0002  max mem: 4132
[18:46:52.782122] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.7934 (0.8081)  acc1: 73.4375 (73.0945)  acc5: 98.4375 (98.2851)  time: 0.0290  data: 0.0003  max mem: 4132
[18:46:53.074882] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.7549 (0.7973)  acc1: 75.0000 (73.7439)  acc5: 98.4375 (98.2537)  time: 0.0290  data: 0.0003  max mem: 4132
[18:46:53.374886] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.7783 (0.7970)  acc1: 71.8750 (73.3607)  acc5: 98.4375 (98.2070)  time: 0.0294  data: 0.0003  max mem: 4132
[18:46:53.668891] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7783 (0.7903)  acc1: 73.4375 (73.7016)  acc5: 98.4375 (98.2614)  time: 0.0295  data: 0.0003  max mem: 4132
[18:46:53.961454] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7632 (0.7951)  acc1: 75.0000 (73.5918)  acc5: 98.4375 (98.1481)  time: 0.0292  data: 0.0003  max mem: 4132
[18:46:54.257309] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7638 (0.7922)  acc1: 75.0000 (73.7981)  acc5: 98.4375 (98.2830)  time: 0.0292  data: 0.0003  max mem: 4132
[18:46:54.550467] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.8094 (0.7965)  acc1: 73.4375 (73.6696)  acc5: 100.0000 (98.3756)  time: 0.0291  data: 0.0003  max mem: 4132
[18:46:54.847930] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.8462 (0.7979)  acc1: 71.8750 (73.6768)  acc5: 98.4375 (98.3671)  time: 0.0293  data: 0.0003  max mem: 4132
[18:46:55.140831] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.7868 (0.7953)  acc1: 71.8750 (73.6958)  acc5: 98.4375 (98.3342)  time: 0.0293  data: 0.0003  max mem: 4132
[18:46:55.440672] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.7766 (0.7962)  acc1: 71.8750 (73.6403)  acc5: 98.4375 (98.3182)  time: 0.0295  data: 0.0002  max mem: 4132
[18:46:55.733676] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7782 (0.7947)  acc1: 73.4375 (73.8143)  acc5: 98.4375 (98.3267)  time: 0.0295  data: 0.0003  max mem: 4132
[18:46:56.019853] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7626 (0.7934)  acc1: 75.0000 (73.9342)  acc5: 98.4375 (98.3237)  time: 0.0288  data: 0.0002  max mem: 4132
[18:46:56.174606] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.8005 (0.7958)  acc1: 73.4375 (73.8800)  acc5: 98.4375 (98.3400)  time: 0.0276  data: 0.0002  max mem: 4132
[18:46:56.372567] Test: Total time: 0:00:05 (0.0350 s / it)
[18:46:56.373903] * Acc@1 73.880 Acc@5 98.340 loss 0.796
[18:46:56.374205] Accuracy of the network on the 10000 test images: 73.9%
[18:46:56.374424] Max accuracy: 73.88%
[18:46:56.573137] log_dir: ./output_dir
[18:46:57.499659] Epoch: [34]  [  0/781]  eta: 0:12:01  lr: 0.000197  training_loss: 1.3163 (1.3163)  classification_loss: 1.2948 (1.2948)  loss_mask: 0.0215 (0.0215)  time: 0.9242  data: 0.7408  max mem: 4132
[18:47:00.829348] Epoch: [34]  [ 20/781]  eta: 0:02:34  lr: 0.000197  training_loss: 1.4575 (1.4548)  classification_loss: 1.4277 (1.4400)  loss_mask: 0.0081 (0.0148)  time: 0.1664  data: 0.0003  max mem: 4132
[18:47:04.126174] Epoch: [34]  [ 40/781]  eta: 0:02:16  lr: 0.000197  training_loss: 1.4816 (1.4784)  classification_loss: 1.4760 (1.4572)  loss_mask: 0.0136 (0.0212)  time: 0.1647  data: 0.0003  max mem: 4132
[18:47:07.453695] Epoch: [34]  [ 60/781]  eta: 0:02:08  lr: 0.000197  training_loss: 1.5101 (1.4917)  classification_loss: 1.4768 (1.4687)  loss_mask: 0.0199 (0.0230)  time: 0.1663  data: 0.0002  max mem: 4132
[18:47:10.750405] Epoch: [34]  [ 80/781]  eta: 0:02:02  lr: 0.000197  training_loss: 1.5164 (1.4977)  classification_loss: 1.5130 (1.4763)  loss_mask: 0.0069 (0.0214)  time: 0.1647  data: 0.0003  max mem: 4132
[18:47:14.065725] Epoch: [34]  [100/781]  eta: 0:01:57  lr: 0.000197  training_loss: 1.5284 (1.5076)  classification_loss: 1.5013 (1.4814)  loss_mask: 0.0231 (0.0262)  time: 0.1657  data: 0.0003  max mem: 4132
[18:47:17.343664] Epoch: [34]  [120/781]  eta: 0:01:53  lr: 0.000196  training_loss: 1.5942 (1.5228)  classification_loss: 1.4761 (1.4812)  loss_mask: 0.0884 (0.0416)  time: 0.1638  data: 0.0003  max mem: 4132
[18:47:20.664221] Epoch: [34]  [140/781]  eta: 0:01:49  lr: 0.000196  training_loss: 1.4792 (1.5189)  classification_loss: 1.4596 (1.4779)  loss_mask: 0.0351 (0.0411)  time: 0.1659  data: 0.0003  max mem: 4132
[18:47:23.975271] Epoch: [34]  [160/781]  eta: 0:01:45  lr: 0.000196  training_loss: 1.5155 (1.5172)  classification_loss: 1.4934 (1.4779)  loss_mask: 0.0283 (0.0394)  time: 0.1655  data: 0.0003  max mem: 4132
[18:47:27.273668] Epoch: [34]  [180/781]  eta: 0:01:41  lr: 0.000196  training_loss: 1.5071 (1.5155)  classification_loss: 1.4843 (1.4781)  loss_mask: 0.0162 (0.0374)  time: 0.1648  data: 0.0003  max mem: 4132
[18:47:30.589597] Epoch: [34]  [200/781]  eta: 0:01:38  lr: 0.000196  training_loss: 1.5216 (1.5169)  classification_loss: 1.4988 (1.4803)  loss_mask: 0.0228 (0.0366)  time: 0.1657  data: 0.0003  max mem: 4132
[18:47:33.892384] Epoch: [34]  [220/781]  eta: 0:01:34  lr: 0.000196  training_loss: 1.5279 (1.5165)  classification_loss: 1.5029 (1.4812)  loss_mask: 0.0130 (0.0353)  time: 0.1651  data: 0.0003  max mem: 4132
[18:47:37.181674] Epoch: [34]  [240/781]  eta: 0:01:31  lr: 0.000196  training_loss: 1.5091 (1.5179)  classification_loss: 1.4673 (1.4814)  loss_mask: 0.0212 (0.0365)  time: 0.1644  data: 0.0003  max mem: 4132
[18:47:40.489258] Epoch: [34]  [260/781]  eta: 0:01:27  lr: 0.000196  training_loss: 1.5045 (1.5181)  classification_loss: 1.4862 (1.4805)  loss_mask: 0.0433 (0.0376)  time: 0.1652  data: 0.0003  max mem: 4132
[18:47:43.809470] Epoch: [34]  [280/781]  eta: 0:01:24  lr: 0.000196  training_loss: 1.5495 (1.5192)  classification_loss: 1.5082 (1.4805)  loss_mask: 0.0326 (0.0387)  time: 0.1659  data: 0.0003  max mem: 4132
[18:47:47.178481] Epoch: [34]  [300/781]  eta: 0:01:20  lr: 0.000196  training_loss: 1.5464 (1.5190)  classification_loss: 1.5289 (1.4818)  loss_mask: 0.0113 (0.0372)  time: 0.1683  data: 0.0003  max mem: 4132
[18:47:50.476630] Epoch: [34]  [320/781]  eta: 0:01:17  lr: 0.000196  training_loss: 1.4833 (1.5170)  classification_loss: 1.4790 (1.4817)  loss_mask: 0.0061 (0.0353)  time: 0.1648  data: 0.0003  max mem: 4132
[18:47:53.846579] Epoch: [34]  [340/781]  eta: 0:01:14  lr: 0.000196  training_loss: 1.4518 (1.5126)  classification_loss: 1.4485 (1.4791)  loss_mask: 0.0057 (0.0335)  time: 0.1684  data: 0.0004  max mem: 4132

[18:47:57.188993] Epoch: [34]  [360/781]  eta: 0:01:10  lr: 0.000195  training_loss: 1.4954 (1.5122)  classification_loss: 1.4925 (1.4802)  loss_mask: 0.0035 (0.0319)  time: 0.1670  data: 0.0004  max mem: 4132
[18:48:00.523269] Epoch: [34]  [380/781]  eta: 0:01:07  lr: 0.000195  training_loss: 1.4894 (1.5114)  classification_loss: 1.4729 (1.4800)  loss_mask: 0.0083 (0.0313)  time: 0.1666  data: 0.0004  max mem: 4132
[18:48:03.828679] Epoch: [34]  [400/781]  eta: 0:01:03  lr: 0.000195  training_loss: 1.6438 (1.5184)  classification_loss: 1.5119 (1.4802)  loss_mask: 0.1203 (0.0383)  time: 0.1652  data: 0.0003  max mem: 4132
[18:48:07.115070] Epoch: [34]  [420/781]  eta: 0:01:00  lr: 0.000195  training_loss: 1.5390 (1.5197)  classification_loss: 1.4920 (1.4798)  loss_mask: 0.0499 (0.0398)  time: 0.1642  data: 0.0003  max mem: 4132
[18:48:10.405219] Epoch: [34]  [440/781]  eta: 0:00:57  lr: 0.000195  training_loss: 1.4808 (1.5188)  classification_loss: 1.4659 (1.4797)  loss_mask: 0.0207 (0.0391)  time: 0.1644  data: 0.0004  max mem: 4132
[18:48:13.724715] Epoch: [34]  [460/781]  eta: 0:00:53  lr: 0.000195  training_loss: 1.5177 (1.5170)  classification_loss: 1.5093 (1.4792)  loss_mask: 0.0099 (0.0378)  time: 0.1659  data: 0.0003  max mem: 4132
[18:48:17.032641] Epoch: [34]  [480/781]  eta: 0:00:50  lr: 0.000195  training_loss: 1.4551 (1.5152)  classification_loss: 1.4461 (1.4787)  loss_mask: 0.0064 (0.0365)  time: 0.1653  data: 0.0003  max mem: 4132
[18:48:20.322900] Epoch: [34]  [500/781]  eta: 0:00:46  lr: 0.000195  training_loss: 1.4786 (1.5142)  classification_loss: 1.4744 (1.4790)  loss_mask: 0.0038 (0.0352)  time: 0.1644  data: 0.0003  max mem: 4132
[18:48:23.608339] Epoch: [34]  [520/781]  eta: 0:00:43  lr: 0.000195  training_loss: 1.4998 (1.5144)  classification_loss: 1.4984 (1.4803)  loss_mask: 0.0044 (0.0341)  time: 0.1642  data: 0.0004  max mem: 4132
[18:48:26.899916] Epoch: [34]  [540/781]  eta: 0:00:40  lr: 0.000195  training_loss: 1.4755 (1.5139)  classification_loss: 1.4718 (1.4810)  loss_mask: 0.0032 (0.0329)  time: 0.1645  data: 0.0003  max mem: 4132
[18:48:30.180171] Epoch: [34]  [560/781]  eta: 0:00:36  lr: 0.000195  training_loss: 1.4776 (1.5128)  classification_loss: 1.4739 (1.4809)  loss_mask: 0.0030 (0.0319)  time: 0.1639  data: 0.0003  max mem: 4132
[18:48:33.499470] Epoch: [34]  [580/781]  eta: 0:00:33  lr: 0.000194  training_loss: 1.4424 (1.5111)  classification_loss: 1.4401 (1.4802)  loss_mask: 0.0030 (0.0310)  time: 0.1658  data: 0.0003  max mem: 4132
[18:48:36.797046] Epoch: [34]  [600/781]  eta: 0:00:30  lr: 0.000194  training_loss: 1.4640 (1.5094)  classification_loss: 1.4625 (1.4793)  loss_mask: 0.0025 (0.0300)  time: 0.1648  data: 0.0003  max mem: 4132
[18:48:40.066413] Epoch: [34]  [620/781]  eta: 0:00:26  lr: 0.000194  training_loss: 1.4761 (1.5091)  classification_loss: 1.4722 (1.4799)  loss_mask: 0.0031 (0.0292)  time: 0.1634  data: 0.0003  max mem: 4132
[18:48:43.337022] Epoch: [34]  [640/781]  eta: 0:00:23  lr: 0.000194  training_loss: 1.5182 (1.5098)  classification_loss: 1.5158 (1.4814)  loss_mask: 0.0024 (0.0284)  time: 0.1634  data: 0.0003  max mem: 4132
[18:48:46.620533] Epoch: [34]  [660/781]  eta: 0:00:20  lr: 0.000194  training_loss: 1.4793 (1.5098)  classification_loss: 1.4778 (1.4822)  loss_mask: 0.0018 (0.0276)  time: 0.1641  data: 0.0003  max mem: 4132
[18:48:49.914295] Epoch: [34]  [680/781]  eta: 0:00:16  lr: 0.000194  training_loss: 1.4974 (1.5093)  classification_loss: 1.4957 (1.4824)  loss_mask: 0.0018 (0.0268)  time: 0.1646  data: 0.0003  max mem: 4132
[18:48:53.235876] Epoch: [34]  [700/781]  eta: 0:00:13  lr: 0.000194  training_loss: 1.4772 (1.5090)  classification_loss: 1.4736 (1.4829)  loss_mask: 0.0015 (0.0261)  time: 0.1660  data: 0.0002  max mem: 4132
[18:48:56.525012] Epoch: [34]  [720/781]  eta: 0:00:10  lr: 0.000194  training_loss: 1.4847 (1.5087)  classification_loss: 1.4828 (1.4832)  loss_mask: 0.0017 (0.0255)  time: 0.1644  data: 0.0004  max mem: 4132
[18:48:59.797054] Epoch: [34]  [740/781]  eta: 0:00:06  lr: 0.000194  training_loss: 1.4034 (1.5064)  classification_loss: 1.4018 (1.4815)  loss_mask: 0.0014 (0.0248)  time: 0.1635  data: 0.0003  max mem: 4132
[18:49:03.069538] Epoch: [34]  [760/781]  eta: 0:00:03  lr: 0.000194  training_loss: 1.5013 (1.5065)  classification_loss: 1.5007 (1.4823)  loss_mask: 0.0012 (0.0242)  time: 0.1635  data: 0.0002  max mem: 4132
[18:49:06.408427] Epoch: [34]  [780/781]  eta: 0:00:00  lr: 0.000194  training_loss: 1.5155 (1.5068)  classification_loss: 1.5149 (1.4831)  loss_mask: 0.0014 (0.0236)  time: 0.1668  data: 0.0003  max mem: 4132
[18:49:06.598734] Epoch: [34] Total time: 0:02:10 (0.1665 s / it)
[18:49:06.599367] Averaged stats: lr: 0.000194  training_loss: 1.5155 (1.5068)  classification_loss: 1.5149 (1.4831)  loss_mask: 0.0014 (0.0236)
[18:49:07.333976] Test:  [  0/157]  eta: 0:01:54  testing_loss: 0.8141 (0.8141)  acc1: 73.4375 (73.4375)  acc5: 95.3125 (95.3125)  time: 0.7305  data: 0.7001  max mem: 4132
[18:49:07.637082] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.8141 (0.8087)  acc1: 73.4375 (73.7216)  acc5: 100.0000 (98.5795)  time: 0.0938  data: 0.0643  max mem: 4132
[18:49:07.928428] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.7608 (0.7783)  acc1: 76.5625 (74.6280)  acc5: 98.4375 (98.8095)  time: 0.0295  data: 0.0005  max mem: 4132
[18:49:08.225059] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.7683 (0.8018)  acc1: 75.0000 (74.2944)  acc5: 98.4375 (98.6895)  time: 0.0293  data: 0.0002  max mem: 4132
[18:49:08.516420] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.7683 (0.7966)  acc1: 75.0000 (74.5046)  acc5: 98.4375 (98.6280)  time: 0.0293  data: 0.0002  max mem: 4132
[18:49:08.805517] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.7680 (0.7923)  acc1: 75.0000 (74.9387)  acc5: 98.4375 (98.5907)  time: 0.0289  data: 0.0002  max mem: 4132
[18:49:09.089634] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.7684 (0.7921)  acc1: 75.0000 (75.0512)  acc5: 96.8750 (98.3607)  time: 0.0285  data: 0.0002  max mem: 4132
[18:49:09.374349] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7560 (0.7845)  acc1: 75.0000 (75.3741)  acc5: 98.4375 (98.4155)  time: 0.0283  data: 0.0002  max mem: 4132
[18:49:09.658018] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7708 (0.7945)  acc1: 75.0000 (74.9614)  acc5: 98.4375 (98.2639)  time: 0.0283  data: 0.0002  max mem: 4132
[18:49:09.944286] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.8017 (0.7931)  acc1: 75.0000 (75.0687)  acc5: 98.4375 (98.3345)  time: 0.0284  data: 0.0002  max mem: 4132
[18:49:10.232969] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.7904 (0.7975)  acc1: 76.5625 (74.8453)  acc5: 100.0000 (98.3756)  time: 0.0286  data: 0.0002  max mem: 4132
[18:49:10.517138] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.8047 (0.7986)  acc1: 75.0000 (74.8170)  acc5: 98.4375 (98.3812)  time: 0.0285  data: 0.0002  max mem: 4132
[18:49:10.808927] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.7920 (0.7954)  acc1: 73.4375 (74.9225)  acc5: 98.4375 (98.3600)  time: 0.0287  data: 0.0002  max mem: 4132
[18:49:11.100940] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.7920 (0.7960)  acc1: 73.4375 (74.9165)  acc5: 98.4375 (98.3421)  time: 0.0291  data: 0.0002  max mem: 4132
[18:49:11.387122] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.8172 (0.7951)  acc1: 71.8750 (74.9003)  acc5: 98.4375 (98.3710)  time: 0.0288  data: 0.0002  max mem: 4132
[18:49:11.669932] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.8131 (0.7942)  acc1: 73.4375 (74.9069)  acc5: 98.4375 (98.3547)  time: 0.0283  data: 0.0002  max mem: 4132
[18:49:11.825066] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.8176 (0.7990)  acc1: 73.4375 (74.7000)  acc5: 98.4375 (98.3600)  time: 0.0274  data: 0.0001  max mem: 4132
[18:49:11.998577] Test: Total time: 0:00:05 (0.0344 s / it)
[18:49:11.999651] * Acc@1 74.700 Acc@5 98.360 loss 0.799
[18:49:11.999983] Accuracy of the network on the 10000 test images: 74.7%
[18:49:12.000290] Max accuracy: 74.70%
[18:49:12.326435] log_dir: ./output_dir
[18:49:13.177133] Epoch: [35]  [  0/781]  eta: 0:11:02  lr: 0.000194  training_loss: 1.3019 (1.3019)  classification_loss: 1.2998 (1.2998)  loss_mask: 0.0022 (0.0022)  time: 0.8488  data: 0.6651  max mem: 4132
[18:49:16.459145] Epoch: [35]  [ 20/781]  eta: 0:02:29  lr: 0.000194  training_loss: 1.4279 (1.4384)  classification_loss: 1.4267 (1.4372)  loss_mask: 0.0010 (0.0012)  time: 0.1640  data: 0.0003  max mem: 4132
[18:49:19.828844] Epoch: [35]  [ 40/781]  eta: 0:02:15  lr: 0.000193  training_loss: 1.4798 (1.4483)  classification_loss: 1.4789 (1.4471)  loss_mask: 0.0009 (0.0012)  time: 0.1684  data: 0.0003  max mem: 4132
[18:49:23.124895] Epoch: [35]  [ 60/781]  eta: 0:02:07  lr: 0.000193  training_loss: 1.4933 (1.4647)  classification_loss: 1.4925 (1.4635)  loss_mask: 0.0010 (0.0011)  time: 0.1647  data: 0.0003  max mem: 4132
[18:49:26.434701] Epoch: [35]  [ 80/781]  eta: 0:02:02  lr: 0.000193  training_loss: 1.4662 (1.4722)  classification_loss: 1.4656 (1.4711)  loss_mask: 0.0011 (0.0012)  time: 0.1654  data: 0.0003  max mem: 4132
[18:49:29.732180] Epoch: [35]  [100/781]  eta: 0:01:57  lr: 0.000193  training_loss: 1.4558 (1.4716)  classification_loss: 1.4551 (1.4705)  loss_mask: 0.0009 (0.0011)  time: 0.1648  data: 0.0003  max mem: 4132
[18:49:33.025847] Epoch: [35]  [120/781]  eta: 0:01:53  lr: 0.000193  training_loss: 1.4713 (1.4696)  classification_loss: 1.4707 (1.4685)  loss_mask: 0.0008 (0.0011)  time: 0.1646  data: 0.0004  max mem: 4132
[18:49:36.404040] Epoch: [35]  [140/781]  eta: 0:01:49  lr: 0.000193  training_loss: 1.4462 (1.4658)  classification_loss: 1.4453 (1.4648)  loss_mask: 0.0008 (0.0010)  time: 0.1688  data: 0.0003  max mem: 4132
[18:49:39.710391] Epoch: [35]  [160/781]  eta: 0:01:45  lr: 0.000193  training_loss: 1.4213 (1.4619)  classification_loss: 1.4206 (1.4609)  loss_mask: 0.0007 (0.0010)  time: 0.1652  data: 0.0003  max mem: 4132
[18:49:43.032771] Epoch: [35]  [180/781]  eta: 0:01:41  lr: 0.000193  training_loss: 1.4769 (1.4607)  classification_loss: 1.4759 (1.4597)  loss_mask: 0.0007 (0.0010)  time: 0.1660  data: 0.0004  max mem: 4132
[18:49:46.345878] Epoch: [35]  [200/781]  eta: 0:01:38  lr: 0.000193  training_loss: 1.5074 (1.4633)  classification_loss: 1.5057 (1.4623)  loss_mask: 0.0006 (0.0010)  time: 0.1655  data: 0.0004  max mem: 4132
[18:49:49.622950] Epoch: [35]  [220/781]  eta: 0:01:34  lr: 0.000193  training_loss: 1.4471 (1.4638)  classification_loss: 1.4460 (1.4629)  loss_mask: 0.0005 (0.0009)  time: 0.1638  data: 0.0003  max mem: 4132
[18:49:52.923608] Epoch: [35]  [240/781]  eta: 0:01:31  lr: 0.000193  training_loss: 1.4576 (1.4650)  classification_loss: 1.4573 (1.4641)  loss_mask: 0.0006 (0.0009)  time: 0.1649  data: 0.0003  max mem: 4132
[18:49:56.217480] Epoch: [35]  [260/781]  eta: 0:01:27  lr: 0.000192  training_loss: 1.5627 (1.4765)  classification_loss: 1.4470 (1.4643)  loss_mask: 0.0546 (0.0122)  time: 0.1645  data: 0.0003  max mem: 4132
[18:49:59.499699] Epoch: [35]  [280/781]  eta: 0:01:24  lr: 0.000192  training_loss: 1.5560 (1.4817)  classification_loss: 1.4952 (1.4653)  loss_mask: 0.0569 (0.0164)  time: 0.1640  data: 0.0003  max mem: 4132
[18:50:02.838315] Epoch: [35]  [300/781]  eta: 0:01:20  lr: 0.000192  training_loss: 1.5535 (1.4849)  classification_loss: 1.4827 (1.4672)  loss_mask: 0.0302 (0.0177)  time: 0.1668  data: 0.0003  max mem: 4132
[18:50:06.129646] Epoch: [35]  [320/781]  eta: 0:01:17  lr: 0.000192  training_loss: 1.4956 (1.4859)  classification_loss: 1.4598 (1.4666)  loss_mask: 0.0351 (0.0193)  time: 0.1645  data: 0.0003  max mem: 4132
[18:50:09.439129] Epoch: [35]  [340/781]  eta: 0:01:13  lr: 0.000192  training_loss: 1.4212 (1.4835)  classification_loss: 1.3937 (1.4636)  loss_mask: 0.0201 (0.0199)  time: 0.1654  data: 0.0003  max mem: 4132
[18:50:12.730787] Epoch: [35]  [360/781]  eta: 0:01:10  lr: 0.000192  training_loss: 1.4903 (1.4848)  classification_loss: 1.4834 (1.4649)  loss_mask: 0.0174 (0.0199)  time: 0.1645  data: 0.0002  max mem: 4132
[18:50:16.022181] Epoch: [35]  [380/781]  eta: 0:01:06  lr: 0.000192  training_loss: 1.4560 (1.4846)  classification_loss: 1.4444 (1.4649)  loss_mask: 0.0114 (0.0197)  time: 0.1645  data: 0.0003  max mem: 4132
[18:50:19.329075] Epoch: [35]  [400/781]  eta: 0:01:03  lr: 0.000192  training_loss: 1.4951 (1.4842)  classification_loss: 1.4816 (1.4644)  loss_mask: 0.0147 (0.0198)  time: 0.1652  data: 0.0003  max mem: 4132
[18:50:22.653515] Epoch: [35]  [420/781]  eta: 0:01:00  lr: 0.000192  training_loss: 1.4685 (1.4844)  classification_loss: 1.4656 (1.4650)  loss_mask: 0.0082 (0.0194)  time: 0.1661  data: 0.0003  max mem: 4132
[18:50:25.935871] Epoch: [35]  [440/781]  eta: 0:00:56  lr: 0.000192  training_loss: 1.4066 (1.4813)  classification_loss: 1.4024 (1.4625)  loss_mask: 0.0035 (0.0188)  time: 0.1640  data: 0.0003  max mem: 4132
[18:50:29.281616] Epoch: [35]  [460/781]  eta: 0:00:53  lr: 0.000192  training_loss: 1.4311 (1.4801)  classification_loss: 1.4267 (1.4620)  loss_mask: 0.0023 (0.0181)  time: 0.1672  data: 0.0004  max mem: 4132
[18:50:32.600054] Epoch: [35]  [480/781]  eta: 0:00:50  lr: 0.000191  training_loss: 1.4488 (1.4799)  classification_loss: 1.4438 (1.4625)  loss_mask: 0.0033 (0.0175)  time: 0.1658  data: 0.0004  max mem: 4132
[18:50:35.897288] Epoch: [35]  [500/781]  eta: 0:00:46  lr: 0.000191  training_loss: 1.5136 (1.4814)  classification_loss: 1.5123 (1.4642)  loss_mask: 0.0032 (0.0172)  time: 0.1648  data: 0.0003  max mem: 4132
[18:50:39.196791] Epoch: [35]  [520/781]  eta: 0:00:43  lr: 0.000191  training_loss: 1.4625 (1.4806)  classification_loss: 1.4618 (1.4639)  loss_mask: 0.0016 (0.0167)  time: 0.1649  data: 0.0003  max mem: 4132
[18:50:42.504222] Epoch: [35]  [540/781]  eta: 0:00:40  lr: 0.000191  training_loss: 1.5007 (1.4809)  classification_loss: 1.4965 (1.4647)  loss_mask: 0.0026 (0.0161)  time: 0.1653  data: 0.0003  max mem: 4132
[18:50:45.806208] Epoch: [35]  [560/781]  eta: 0:00:36  lr: 0.000191  training_loss: 1.4685 (1.4801)  classification_loss: 1.4679 (1.4645)  loss_mask: 0.0012 (0.0156)  time: 0.1650  data: 0.0002  max mem: 4132
[18:50:49.137821] Epoch: [35]  [580/781]  eta: 0:00:33  lr: 0.000191  training_loss: 1.4545 (1.4807)  classification_loss: 1.4531 (1.4656)  loss_mask: 0.0012 (0.0151)  time: 0.1665  data: 0.0003  max mem: 4132
[18:50:52.475303] Epoch: [35]  [600/781]  eta: 0:00:30  lr: 0.000191  training_loss: 1.4833 (1.4809)  classification_loss: 1.4822 (1.4663)  loss_mask: 0.0011 (0.0147)  time: 0.1668  data: 0.0002  max mem: 4132
[18:50:55.765739] Epoch: [35]  [620/781]  eta: 0:00:26  lr: 0.000191  training_loss: 1.4564 (1.4796)  classification_loss: 1.4500 (1.4652)  loss_mask: 0.0026 (0.0145)  time: 0.1644  data: 0.0003  max mem: 4132
[18:50:59.069854] Epoch: [35]  [640/781]  eta: 0:00:23  lr: 0.000191  training_loss: 1.4953 (1.4800)  classification_loss: 1.4868 (1.4658)  loss_mask: 0.0041 (0.0142)  time: 0.1651  data: 0.0003  max mem: 4132
[18:51:02.416938] Epoch: [35]  [660/781]  eta: 0:00:20  lr: 0.000191  training_loss: 1.4930 (1.4805)  classification_loss: 1.4899 (1.4667)  loss_mask: 0.0017 (0.0138)  time: 0.1673  data: 0.0003  max mem: 4132
[18:51:05.742005] Epoch: [35]  [680/781]  eta: 0:00:16  lr: 0.000191  training_loss: 1.5096 (1.4813)  classification_loss: 1.5083 (1.4679)  loss_mask: 0.0008 (0.0134)  time: 0.1661  data: 0.0003  max mem: 4132
[18:51:09.075630] Epoch: [35]  [700/781]  eta: 0:00:13  lr: 0.000190  training_loss: 1.4655 (1.4816)  classification_loss: 1.4653 (1.4685)  loss_mask: 0.0008 (0.0131)  time: 0.1666  data: 0.0003  max mem: 4132
[18:51:12.376737] Epoch: [35]  [720/781]  eta: 0:00:10  lr: 0.000190  training_loss: 1.4460 (1.4815)  classification_loss: 1.4451 (1.4688)  loss_mask: 0.0006 (0.0127)  time: 0.1649  data: 0.0005  max mem: 4132
[18:51:15.695192] Epoch: [35]  [740/781]  eta: 0:00:06  lr: 0.000190  training_loss: 1.4441 (1.4805)  classification_loss: 1.4436 (1.4681)  loss_mask: 0.0005 (0.0124)  time: 0.1658  data: 0.0003  max mem: 4132
[18:51:19.014423] Epoch: [35]  [760/781]  eta: 0:00:03  lr: 0.000190  training_loss: 1.5068 (1.4807)  classification_loss: 1.5062 (1.4686)  loss_mask: 0.0005 (0.0121)  time: 0.1659  data: 0.0003  max mem: 4132
[18:51:22.320563] Epoch: [35]  [780/781]  eta: 0:00:00  lr: 0.000190  training_loss: 1.4657 (1.4807)  classification_loss: 1.4652 (1.4689)  loss_mask: 0.0005 (0.0118)  time: 0.1652  data: 0.0003  max mem: 4132
[18:51:22.485693] Epoch: [35] Total time: 0:02:10 (0.1667 s / it)
[18:51:22.486169] Averaged stats: lr: 0.000190  training_loss: 1.4657 (1.4807)  classification_loss: 1.4652 (1.4689)  loss_mask: 0.0005 (0.0118)
[18:51:23.168508] Test:  [  0/157]  eta: 0:01:46  testing_loss: 0.6760 (0.6760)  acc1: 79.6875 (79.6875)  acc5: 98.4375 (98.4375)  time: 0.6779  data: 0.6484  max mem: 4132
[18:51:23.520195] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.7608 (0.7622)  acc1: 76.5625 (75.8523)  acc5: 98.4375 (98.8636)  time: 0.0931  data: 0.0613  max mem: 4132
[18:51:23.804613] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.7329 (0.7270)  acc1: 76.5625 (76.7857)  acc5: 98.4375 (99.0327)  time: 0.0314  data: 0.0014  max mem: 4132
[18:51:24.094381] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.7052 (0.7464)  acc1: 75.0000 (75.9577)  acc5: 98.4375 (98.7903)  time: 0.0286  data: 0.0002  max mem: 4132
[18:51:24.381580] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.7175 (0.7434)  acc1: 76.5625 (76.2195)  acc5: 98.4375 (98.6280)  time: 0.0287  data: 0.0002  max mem: 4132
[18:51:24.670105] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.7175 (0.7390)  acc1: 76.5625 (76.3174)  acc5: 98.4375 (98.4988)  time: 0.0286  data: 0.0002  max mem: 4132
[18:51:24.957524] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.7152 (0.7405)  acc1: 75.0000 (76.0758)  acc5: 98.4375 (98.4887)  time: 0.0287  data: 0.0002  max mem: 4132
[18:51:25.244703] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7037 (0.7348)  acc1: 75.0000 (76.2324)  acc5: 98.4375 (98.4375)  time: 0.0286  data: 0.0002  max mem: 4132
[18:51:25.528489] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7231 (0.7430)  acc1: 75.0000 (75.8873)  acc5: 98.4375 (98.3410)  time: 0.0284  data: 0.0002  max mem: 4132
[18:51:25.815082] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7231 (0.7413)  acc1: 75.0000 (76.0474)  acc5: 98.4375 (98.4032)  time: 0.0284  data: 0.0002  max mem: 4132
[18:51:26.105177] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.7353 (0.7461)  acc1: 76.5625 (75.7426)  acc5: 98.4375 (98.4994)  time: 0.0286  data: 0.0002  max mem: 4132
[18:51:26.393958] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.7778 (0.7470)  acc1: 73.4375 (75.6194)  acc5: 98.4375 (98.4234)  time: 0.0287  data: 0.0002  max mem: 4132
[18:51:26.686630] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.7454 (0.7452)  acc1: 76.5625 (75.7748)  acc5: 98.4375 (98.4375)  time: 0.0288  data: 0.0003  max mem: 4132
[18:51:26.974008] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.7249 (0.7469)  acc1: 75.0000 (75.6799)  acc5: 98.4375 (98.4375)  time: 0.0288  data: 0.0002  max mem: 4132
[18:51:27.262159] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7600 (0.7454)  acc1: 73.4375 (75.7425)  acc5: 98.4375 (98.4043)  time: 0.0286  data: 0.0002  max mem: 4132
[18:51:27.546308] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7261 (0.7439)  acc1: 73.4375 (75.7347)  acc5: 98.4375 (98.4065)  time: 0.0284  data: 0.0002  max mem: 4132
[18:51:27.705708] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7261 (0.7453)  acc1: 75.0000 (75.7000)  acc5: 98.4375 (98.4100)  time: 0.0277  data: 0.0002  max mem: 4132
[18:51:27.878456] Test: Total time: 0:00:05 (0.0343 s / it)
[18:51:27.878939] * Acc@1 75.700 Acc@5 98.410 loss 0.745
[18:51:27.879261] Accuracy of the network on the 10000 test images: 75.7%
[18:51:27.879496] Max accuracy: 75.70%
[18:51:28.279368] log_dir: ./output_dir
[18:51:29.225393] Epoch: [36]  [  0/781]  eta: 0:12:17  lr: 0.000190  training_loss: 1.4397 (1.4397)  classification_loss: 1.4393 (1.4393)  loss_mask: 0.0004 (0.0004)  time: 0.9441  data: 0.7719  max mem: 4132
[18:51:32.541402] Epoch: [36]  [ 20/781]  eta: 0:02:34  lr: 0.000190  training_loss: 1.4632 (1.4863)  classification_loss: 1.4628 (1.4857)  loss_mask: 0.0006 (0.0006)  time: 0.1657  data: 0.0003  max mem: 4132
[18:51:35.909241] Epoch: [36]  [ 40/781]  eta: 0:02:17  lr: 0.000190  training_loss: 1.5101 (1.4904)  classification_loss: 1.5097 (1.4898)  loss_mask: 0.0004 (0.0005)  time: 0.1683  data: 0.0003  max mem: 4132
[18:51:39.228871] Epoch: [36]  [ 60/781]  eta: 0:02:09  lr: 0.000190  training_loss: 1.4740 (1.4861)  classification_loss: 1.4737 (1.4856)  loss_mask: 0.0004 (0.0005)  time: 0.1658  data: 0.0003  max mem: 4132
[18:51:42.567888] Epoch: [36]  [ 80/781]  eta: 0:02:03  lr: 0.000190  training_loss: 1.5517 (1.5183)  classification_loss: 1.5146 (1.4885)  loss_mask: 0.0280 (0.0298)  time: 0.1669  data: 0.0003  max mem: 4132
[18:51:45.918729] Epoch: [36]  [100/781]  eta: 0:01:58  lr: 0.000190  training_loss: 1.6512 (1.5435)  classification_loss: 1.4443 (1.4810)  loss_mask: 0.1281 (0.0625)  time: 0.1674  data: 0.0005  max mem: 4132
[18:51:49.244593] Epoch: [36]  [120/781]  eta: 0:01:54  lr: 0.000190  training_loss: 1.5095 (1.5392)  classification_loss: 1.4555 (1.4761)  loss_mask: 0.0418 (0.0630)  time: 0.1662  data: 0.0004  max mem: 4132
[18:51:52.586727] Epoch: [36]  [140/781]  eta: 0:01:50  lr: 0.000189  training_loss: 1.4830 (1.5312)  classification_loss: 1.4479 (1.4717)  loss_mask: 0.0342 (0.0594)  time: 0.1670  data: 0.0003  max mem: 4132
[18:51:55.871423] Epoch: [36]  [160/781]  eta: 0:01:46  lr: 0.000189  training_loss: 1.4907 (1.5254)  classification_loss: 1.4630 (1.4693)  loss_mask: 0.0296 (0.0562)  time: 0.1641  data: 0.0003  max mem: 4132
[18:51:59.156370] Epoch: [36]  [180/781]  eta: 0:01:42  lr: 0.000189  training_loss: 1.4373 (1.5200)  classification_loss: 1.4209 (1.4677)  loss_mask: 0.0161 (0.0523)  time: 0.1642  data: 0.0003  max mem: 4132
[18:52:02.489539] Epoch: [36]  [200/781]  eta: 0:01:38  lr: 0.000189  training_loss: 1.4826 (1.5155)  classification_loss: 1.4778 (1.4675)  loss_mask: 0.0071 (0.0480)  time: 0.1666  data: 0.0003  max mem: 4132
[18:52:05.802113] Epoch: [36]  [220/781]  eta: 0:01:35  lr: 0.000189  training_loss: 1.4946 (1.5130)  classification_loss: 1.4791 (1.4685)  loss_mask: 0.0055 (0.0445)  time: 0.1655  data: 0.0003  max mem: 4132
[18:52:09.087544] Epoch: [36]  [240/781]  eta: 0:01:31  lr: 0.000189  training_loss: 1.4512 (1.5125)  classification_loss: 1.3996 (1.4671)  loss_mask: 0.0123 (0.0454)  time: 0.1642  data: 0.0002  max mem: 4132
[18:52:12.378010] Epoch: [36]  [260/781]  eta: 0:01:27  lr: 0.000189  training_loss: 1.5136 (1.5178)  classification_loss: 1.4600 (1.4706)  loss_mask: 0.0460 (0.0473)  time: 0.1644  data: 0.0004  max mem: 4132
[18:52:15.658327] Epoch: [36]  [280/781]  eta: 0:01:24  lr: 0.000189  training_loss: 1.4861 (1.5149)  classification_loss: 1.4658 (1.4695)  loss_mask: 0.0168 (0.0455)  time: 0.1639  data: 0.0003  max mem: 4132
[18:52:18.987959] Epoch: [36]  [300/781]  eta: 0:01:20  lr: 0.000189  training_loss: 1.4862 (1.5136)  classification_loss: 1.4823 (1.4707)  loss_mask: 0.0062 (0.0429)  time: 0.1663  data: 0.0003  max mem: 4132
[18:52:22.309887] Epoch: [36]  [320/781]  eta: 0:01:17  lr: 0.000189  training_loss: 1.4391 (1.5102)  classification_loss: 1.4325 (1.4694)  loss_mask: 0.0055 (0.0408)  time: 0.1660  data: 0.0004  max mem: 4132
[18:52:25.601329] Epoch: [36]  [340/781]  eta: 0:01:14  lr: 0.000189  training_loss: 1.4865 (1.5092)  classification_loss: 1.4793 (1.4705)  loss_mask: 0.0042 (0.0387)  time: 0.1645  data: 0.0003  max mem: 4132
[18:52:28.852098] Epoch: [36]  [360/781]  eta: 0:01:10  lr: 0.000188  training_loss: 1.4859 (1.5080)  classification_loss: 1.4816 (1.4713)  loss_mask: 0.0023 (0.0367)  time: 0.1624  data: 0.0002  max mem: 4132
[18:52:32.151487] Epoch: [36]  [380/781]  eta: 0:01:07  lr: 0.000188  training_loss: 1.4603 (1.5065)  classification_loss: 1.4586 (1.4716)  loss_mask: 0.0019 (0.0349)  time: 0.1649  data: 0.0002  max mem: 4132
[18:52:35.491325] Epoch: [36]  [400/781]  eta: 0:01:03  lr: 0.000188  training_loss: 1.4224 (1.5032)  classification_loss: 1.4214 (1.4699)  loss_mask: 0.0014 (0.0333)  time: 0.1669  data: 0.0002  max mem: 4132
[18:52:38.843500] Epoch: [36]  [420/781]  eta: 0:01:00  lr: 0.000188  training_loss: 1.4631 (1.5017)  classification_loss: 1.4624 (1.4700)  loss_mask: 0.0015 (0.0317)  time: 0.1675  data: 0.0004  max mem: 4132
[18:52:42.195651] Epoch: [36]  [440/781]  eta: 0:00:57  lr: 0.000188  training_loss: 1.4780 (1.5001)  classification_loss: 1.4772 (1.4697)  loss_mask: 0.0016 (0.0304)  time: 0.1675  data: 0.0004  max mem: 4132
[18:52:45.574048] Epoch: [36]  [460/781]  eta: 0:00:53  lr: 0.000188  training_loss: 1.3889 (1.4964)  classification_loss: 1.3875 (1.4673)  loss_mask: 0.0010 (0.0291)  time: 0.1688  data: 0.0002  max mem: 4132
[18:52:48.910467] Epoch: [36]  [480/781]  eta: 0:00:50  lr: 0.000188  training_loss: 1.4593 (1.4955)  classification_loss: 1.4446 (1.4672)  loss_mask: 0.0020 (0.0283)  time: 0.1667  data: 0.0003  max mem: 4132
[18:52:52.236992] Epoch: [36]  [500/781]  eta: 0:00:47  lr: 0.000188  training_loss: 1.5440 (1.4981)  classification_loss: 1.5166 (1.4685)  loss_mask: 0.0330 (0.0297)  time: 0.1662  data: 0.0003  max mem: 4132
[18:52:55.590556] Epoch: [36]  [520/781]  eta: 0:00:43  lr: 0.000188  training_loss: 1.5373 (1.5016)  classification_loss: 1.4761 (1.4686)  loss_mask: 0.0672 (0.0330)  time: 0.1676  data: 0.0004  max mem: 4132
[18:52:58.907222] Epoch: [36]  [540/781]  eta: 0:00:40  lr: 0.000188  training_loss: 1.5334 (1.5042)  classification_loss: 1.4507 (1.4686)  loss_mask: 0.0633 (0.0356)  time: 0.1657  data: 0.0003  max mem: 4132
[18:53:02.206250] Epoch: [36]  [560/781]  eta: 0:00:36  lr: 0.000188  training_loss: 1.4872 (1.5044)  classification_loss: 1.4392 (1.4685)  loss_mask: 0.0392 (0.0359)  time: 0.1649  data: 0.0003  max mem: 4132
[18:53:05.481971] Epoch: [36]  [580/781]  eta: 0:00:33  lr: 0.000187  training_loss: 1.4566 (1.5033)  classification_loss: 1.4424 (1.4678)  loss_mask: 0.0188 (0.0354)  time: 0.1637  data: 0.0002  max mem: 4132
[18:53:08.760878] Epoch: [36]  [600/781]  eta: 0:00:30  lr: 0.000187  training_loss: 1.4451 (1.5020)  classification_loss: 1.4284 (1.4672)  loss_mask: 0.0136 (0.0348)  time: 0.1639  data: 0.0003  max mem: 4132
[18:53:12.033637] Epoch: [36]  [620/781]  eta: 0:00:26  lr: 0.000187  training_loss: 1.5155 (1.5024)  classification_loss: 1.5078 (1.4679)  loss_mask: 0.0203 (0.0345)  time: 0.1636  data: 0.0002  max mem: 4132
[18:53:15.343359] Epoch: [36]  [640/781]  eta: 0:00:23  lr: 0.000187  training_loss: 1.4712 (1.5016)  classification_loss: 1.4483 (1.4676)  loss_mask: 0.0112 (0.0341)  time: 0.1654  data: 0.0003  max mem: 4132
[18:53:18.652438] Epoch: [36]  [660/781]  eta: 0:00:20  lr: 0.000187  training_loss: 1.4691 (1.5015)  classification_loss: 1.4542 (1.4681)  loss_mask: 0.0085 (0.0334)  time: 0.1654  data: 0.0002  max mem: 4132
[18:53:21.961442] Epoch: [36]  [680/781]  eta: 0:00:16  lr: 0.000187  training_loss: 1.4454 (1.5006)  classification_loss: 1.4393 (1.4679)  loss_mask: 0.0049 (0.0327)  time: 0.1653  data: 0.0003  max mem: 4132
[18:53:25.245956] Epoch: [36]  [700/781]  eta: 0:00:13  lr: 0.000187  training_loss: 1.4384 (1.5000)  classification_loss: 1.4271 (1.4682)  loss_mask: 0.0030 (0.0318)  time: 0.1641  data: 0.0003  max mem: 4132
[18:53:28.561823] Epoch: [36]  [720/781]  eta: 0:00:10  lr: 0.000187  training_loss: 1.4522 (1.4986)  classification_loss: 1.4498 (1.4675)  loss_mask: 0.0027 (0.0311)  time: 0.1657  data: 0.0003  max mem: 4132
[18:53:31.865241] Epoch: [36]  [740/781]  eta: 0:00:06  lr: 0.000187  training_loss: 1.4212 (1.4973)  classification_loss: 1.4193 (1.4670)  loss_mask: 0.0019 (0.0303)  time: 0.1651  data: 0.0003  max mem: 4132
[18:53:35.165647] Epoch: [36]  [760/781]  eta: 0:00:03  lr: 0.000187  training_loss: 1.4877 (1.4969)  classification_loss: 1.4849 (1.4673)  loss_mask: 0.0021 (0.0296)  time: 0.1649  data: 0.0003  max mem: 4132
[18:53:38.434040] Epoch: [36]  [780/781]  eta: 0:00:00  lr: 0.000187  training_loss: 1.4529 (1.4963)  classification_loss: 1.4510 (1.4675)  loss_mask: 0.0015 (0.0289)  time: 0.1633  data: 0.0002  max mem: 4132
[18:53:38.602562] Epoch: [36] Total time: 0:02:10 (0.1669 s / it)
[18:53:38.603178] Averaged stats: lr: 0.000187  training_loss: 1.4529 (1.4963)  classification_loss: 1.4510 (1.4675)  loss_mask: 0.0015 (0.0289)
[18:53:39.308096] Test:  [  0/157]  eta: 0:01:49  testing_loss: 0.7109 (0.7109)  acc1: 79.6875 (79.6875)  acc5: 96.8750 (96.8750)  time: 0.7003  data: 0.6705  max mem: 4132
[18:53:39.605732] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.7739 (0.7741)  acc1: 75.0000 (74.4318)  acc5: 100.0000 (99.0057)  time: 0.0905  data: 0.0617  max mem: 4132
[18:53:39.912318] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.7438 (0.7550)  acc1: 75.0000 (75.2976)  acc5: 98.4375 (98.7351)  time: 0.0298  data: 0.0009  max mem: 4132
[18:53:40.209208] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.7551 (0.7780)  acc1: 73.4375 (73.9919)  acc5: 98.4375 (98.4879)  time: 0.0298  data: 0.0007  max mem: 4132
[18:53:40.505386] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.7904 (0.7826)  acc1: 71.8750 (73.5518)  acc5: 98.4375 (98.3613)  time: 0.0294  data: 0.0004  max mem: 4132
[18:53:40.796702] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.7618 (0.7793)  acc1: 71.8750 (73.9583)  acc5: 98.4375 (98.4069)  time: 0.0291  data: 0.0003  max mem: 4132
[18:53:41.085395] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.7413 (0.7770)  acc1: 73.4375 (73.6936)  acc5: 98.4375 (98.3350)  time: 0.0288  data: 0.0003  max mem: 4132
[18:53:41.379194] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7192 (0.7694)  acc1: 75.0000 (74.0537)  acc5: 98.4375 (98.3935)  time: 0.0290  data: 0.0003  max mem: 4132
[18:53:41.671025] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7322 (0.7771)  acc1: 75.0000 (73.8426)  acc5: 98.4375 (98.2832)  time: 0.0291  data: 0.0003  max mem: 4132
[18:53:41.968939] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7890 (0.7732)  acc1: 73.4375 (74.1243)  acc5: 98.4375 (98.3001)  time: 0.0293  data: 0.0002  max mem: 4132
[18:53:42.264660] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.7952 (0.7767)  acc1: 73.4375 (74.1646)  acc5: 98.4375 (98.1900)  time: 0.0295  data: 0.0004  max mem: 4132
[18:53:42.574312] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.8094 (0.7781)  acc1: 73.4375 (74.2399)  acc5: 96.8750 (98.1419)  time: 0.0301  data: 0.0004  max mem: 4132
[18:53:42.870396] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.7628 (0.7741)  acc1: 76.5625 (74.4706)  acc5: 98.4375 (98.1534)  time: 0.0301  data: 0.0003  max mem: 4132
[18:53:43.162583] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.7628 (0.7769)  acc1: 76.5625 (74.4036)  acc5: 98.4375 (98.1512)  time: 0.0293  data: 0.0002  max mem: 4132
[18:53:43.452606] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.8023 (0.7765)  acc1: 71.8750 (74.4016)  acc5: 98.4375 (98.1161)  time: 0.0289  data: 0.0002  max mem: 4132
[18:53:43.739690] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7960 (0.7747)  acc1: 71.8750 (74.4930)  acc5: 98.4375 (98.1167)  time: 0.0287  data: 0.0002  max mem: 4132
[18:53:43.896164] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7574 (0.7758)  acc1: 78.1250 (74.5900)  acc5: 98.4375 (98.1400)  time: 0.0277  data: 0.0002  max mem: 4132
[18:53:44.089577] Test: Total time: 0:00:05 (0.0349 s / it)
[18:53:44.090882] * Acc@1 74.590 Acc@5 98.140 loss 0.776
[18:53:44.091222] Accuracy of the network on the 10000 test images: 74.6%
[18:53:44.091432] Max accuracy: 75.70%
[18:53:44.283207] log_dir: ./output_dir
[18:53:45.318208] Epoch: [37]  [  0/781]  eta: 0:13:26  lr: 0.000187  training_loss: 1.5110 (1.5110)  classification_loss: 1.5098 (1.5098)  loss_mask: 0.0012 (0.0012)  time: 1.0327  data: 0.7903  max mem: 4132
[18:53:48.631402] Epoch: [37]  [ 20/781]  eta: 0:02:37  lr: 0.000186  training_loss: 1.3935 (1.4278)  classification_loss: 1.3913 (1.4263)  loss_mask: 0.0013 (0.0015)  time: 0.1655  data: 0.0002  max mem: 4132
[18:53:51.919156] Epoch: [37]  [ 40/781]  eta: 0:02:17  lr: 0.000186  training_loss: 1.4526 (1.4494)  classification_loss: 1.4482 (1.4477)  loss_mask: 0.0017 (0.0017)  time: 0.1643  data: 0.0002  max mem: 4132
[18:53:55.221327] Epoch: [37]  [ 60/781]  eta: 0:02:09  lr: 0.000186  training_loss: 1.4688 (1.4589)  classification_loss: 1.4678 (1.4572)  loss_mask: 0.0010 (0.0017)  time: 0.1650  data: 0.0002  max mem: 4132
[18:53:58.510458] Epoch: [37]  [ 80/781]  eta: 0:02:03  lr: 0.000186  training_loss: 1.4912 (1.4610)  classification_loss: 1.4452 (1.4570)  loss_mask: 0.0021 (0.0040)  time: 0.1644  data: 0.0003  max mem: 4132
[18:54:01.791840] Epoch: [37]  [100/781]  eta: 0:01:57  lr: 0.000186  training_loss: 1.4625 (1.4658)  classification_loss: 1.4532 (1.4588)  loss_mask: 0.0084 (0.0070)  time: 0.1640  data: 0.0002  max mem: 4132
[18:54:05.080938] Epoch: [37]  [120/781]  eta: 0:01:53  lr: 0.000186  training_loss: 1.4741 (1.4696)  classification_loss: 1.3966 (1.4498)  loss_mask: 0.0218 (0.0198)  time: 0.1644  data: 0.0004  max mem: 4132
[18:54:08.407361] Epoch: [37]  [140/781]  eta: 0:01:49  lr: 0.000186  training_loss: 1.5071 (1.4756)  classification_loss: 1.4865 (1.4539)  loss_mask: 0.0197 (0.0217)  time: 0.1662  data: 0.0003  max mem: 4132
[18:54:11.691989] Epoch: [37]  [160/781]  eta: 0:01:45  lr: 0.000186  training_loss: 1.6051 (1.5003)  classification_loss: 1.4556 (1.4533)  loss_mask: 0.1618 (0.0469)  time: 0.1641  data: 0.0002  max mem: 4132
[18:54:14.975428] Epoch: [37]  [180/781]  eta: 0:01:41  lr: 0.000186  training_loss: 1.5544 (1.5138)  classification_loss: 1.4312 (1.4535)  loss_mask: 0.1353 (0.0603)  time: 0.1641  data: 0.0002  max mem: 4132
[18:54:18.262614] Epoch: [37]  [200/781]  eta: 0:01:38  lr: 0.000186  training_loss: 1.5436 (1.5170)  classification_loss: 1.4822 (1.4583)  loss_mask: 0.0321 (0.0587)  time: 0.1643  data: 0.0003  max mem: 4132
[18:54:21.520918] Epoch: [37]  [220/781]  eta: 0:01:34  lr: 0.000186  training_loss: 1.5263 (1.5159)  classification_loss: 1.4957 (1.4598)  loss_mask: 0.0306 (0.0561)  time: 0.1628  data: 0.0002  max mem: 4132
[18:54:24.813810] Epoch: [37]  [240/781]  eta: 0:01:30  lr: 0.000185  training_loss: 1.4761 (1.5139)  classification_loss: 1.4214 (1.4599)  loss_mask: 0.0254 (0.0540)  time: 0.1645  data: 0.0003  max mem: 4132
[18:54:28.142079] Epoch: [37]  [260/781]  eta: 0:01:27  lr: 0.000185  training_loss: 1.4622 (1.5113)  classification_loss: 1.4458 (1.4605)  loss_mask: 0.0104 (0.0508)  time: 0.1663  data: 0.0004  max mem: 4132
[18:54:31.459238] Epoch: [37]  [280/781]  eta: 0:01:24  lr: 0.000185  training_loss: 1.4436 (1.5069)  classification_loss: 1.4392 (1.4591)  loss_mask: 0.0078 (0.0478)  time: 0.1658  data: 0.0003  max mem: 4132
[18:54:34.794661] Epoch: [37]  [300/781]  eta: 0:01:20  lr: 0.000185  training_loss: 1.4907 (1.5056)  classification_loss: 1.4677 (1.4597)  loss_mask: 0.0079 (0.0459)  time: 0.1667  data: 0.0004  max mem: 4132
[18:54:38.090667] Epoch: [37]  [320/781]  eta: 0:01:17  lr: 0.000185  training_loss: 1.5136 (1.5078)  classification_loss: 1.4567 (1.4602)  loss_mask: 0.0415 (0.0476)  time: 0.1647  data: 0.0003  max mem: 4132
[18:54:41.391964] Epoch: [37]  [340/781]  eta: 0:01:13  lr: 0.000185  training_loss: 1.4794 (1.5064)  classification_loss: 1.4576 (1.4597)  loss_mask: 0.0272 (0.0467)  time: 0.1650  data: 0.0002  max mem: 4132
[18:54:44.737715] Epoch: [37]  [360/781]  eta: 0:01:10  lr: 0.000185  training_loss: 1.5087 (1.5058)  classification_loss: 1.4899 (1.4608)  loss_mask: 0.0125 (0.0450)  time: 0.1672  data: 0.0003  max mem: 4132
[18:54:48.084127] Epoch: [37]  [380/781]  eta: 0:01:07  lr: 0.000185  training_loss: 1.4807 (1.5066)  classification_loss: 1.4479 (1.4613)  loss_mask: 0.0250 (0.0453)  time: 0.1672  data: 0.0003  max mem: 4132
[18:54:51.365029] Epoch: [37]  [400/781]  eta: 0:01:03  lr: 0.000185  training_loss: 1.5353 (1.5105)  classification_loss: 1.5237 (1.4642)  loss_mask: 0.0289 (0.0463)  time: 0.1639  data: 0.0002  max mem: 4132
[18:54:54.675474] Epoch: [37]  [420/781]  eta: 0:01:00  lr: 0.000185  training_loss: 1.4776 (1.5126)  classification_loss: 1.4364 (1.4646)  loss_mask: 0.0602 (0.0481)  time: 0.1654  data: 0.0002  max mem: 4132
[18:54:57.997404] Epoch: [37]  [440/781]  eta: 0:00:56  lr: 0.000185  training_loss: 1.4980 (1.5124)  classification_loss: 1.4780 (1.4652)  loss_mask: 0.0226 (0.0472)  time: 0.1660  data: 0.0008  max mem: 4132
[18:55:01.300100] Epoch: [37]  [460/781]  eta: 0:00:53  lr: 0.000184  training_loss: 1.4687 (1.5100)  classification_loss: 1.4611 (1.4643)  loss_mask: 0.0086 (0.0457)  time: 0.1650  data: 0.0002  max mem: 4132
[18:55:04.594289] Epoch: [37]  [480/781]  eta: 0:00:50  lr: 0.000184  training_loss: 1.5097 (1.5091)  classification_loss: 1.4981 (1.4649)  loss_mask: 0.0084 (0.0442)  time: 0.1646  data: 0.0003  max mem: 4132
[18:55:07.904692] Epoch: [37]  [500/781]  eta: 0:00:46  lr: 0.000184  training_loss: 1.4626 (1.5078)  classification_loss: 1.4546 (1.4652)  loss_mask: 0.0044 (0.0426)  time: 0.1654  data: 0.0002  max mem: 4132
[18:55:11.204274] Epoch: [37]  [520/781]  eta: 0:00:43  lr: 0.000184  training_loss: 1.4390 (1.5065)  classification_loss: 1.4363 (1.4654)  loss_mask: 0.0026 (0.0411)  time: 0.1649  data: 0.0003  max mem: 4132
[18:55:14.509072] Epoch: [37]  [540/781]  eta: 0:00:40  lr: 0.000184  training_loss: 1.4934 (1.5059)  classification_loss: 1.4880 (1.4661)  loss_mask: 0.0048 (0.0399)  time: 0.1651  data: 0.0003  max mem: 4132
[18:55:17.812696] Epoch: [37]  [560/781]  eta: 0:00:36  lr: 0.000184  training_loss: 1.4378 (1.5037)  classification_loss: 1.4278 (1.4650)  loss_mask: 0.0063 (0.0387)  time: 0.1651  data: 0.0003  max mem: 4132
[18:55:21.107597] Epoch: [37]  [580/781]  eta: 0:00:33  lr: 0.000184  training_loss: 1.5263 (1.5058)  classification_loss: 1.4516 (1.4651)  loss_mask: 0.0216 (0.0407)  time: 0.1647  data: 0.0003  max mem: 4132
[18:55:24.385167] Epoch: [37]  [600/781]  eta: 0:00:30  lr: 0.000184  training_loss: 1.5230 (1.5069)  classification_loss: 1.4208 (1.4648)  loss_mask: 0.0573 (0.0421)  time: 0.1638  data: 0.0002  max mem: 4132
[18:55:27.652859] Epoch: [37]  [620/781]  eta: 0:00:26  lr: 0.000184  training_loss: 1.4751 (1.5057)  classification_loss: 1.4451 (1.4635)  loss_mask: 0.0412 (0.0423)  time: 0.1633  data: 0.0002  max mem: 4132
[18:55:30.958140] Epoch: [37]  [640/781]  eta: 0:00:23  lr: 0.000184  training_loss: 1.5297 (1.5062)  classification_loss: 1.5058 (1.4642)  loss_mask: 0.0197 (0.0420)  time: 0.1652  data: 0.0002  max mem: 4132
[18:55:34.236076] Epoch: [37]  [660/781]  eta: 0:00:20  lr: 0.000184  training_loss: 1.5268 (1.5057)  classification_loss: 1.5216 (1.4647)  loss_mask: 0.0098 (0.0410)  time: 0.1638  data: 0.0002  max mem: 4132
[18:55:37.519899] Epoch: [37]  [680/781]  eta: 0:00:16  lr: 0.000183  training_loss: 1.5277 (1.5061)  classification_loss: 1.5228 (1.4660)  loss_mask: 0.0074 (0.0401)  time: 0.1641  data: 0.0005  max mem: 4132
[18:55:40.821161] Epoch: [37]  [700/781]  eta: 0:00:13  lr: 0.000183  training_loss: 1.5438 (1.5056)  classification_loss: 1.5404 (1.4665)  loss_mask: 0.0053 (0.0391)  time: 0.1650  data: 0.0003  max mem: 4132
[18:55:44.122752] Epoch: [37]  [720/781]  eta: 0:00:10  lr: 0.000183  training_loss: 1.4640 (1.5043)  classification_loss: 1.4607 (1.4661)  loss_mask: 0.0032 (0.0382)  time: 0.1650  data: 0.0003  max mem: 4132
[18:55:47.420509] Epoch: [37]  [740/781]  eta: 0:00:06  lr: 0.000183  training_loss: 1.4529 (1.5036)  classification_loss: 1.4487 (1.4662)  loss_mask: 0.0040 (0.0374)  time: 0.1648  data: 0.0003  max mem: 4132
[18:55:50.713559] Epoch: [37]  [760/781]  eta: 0:00:03  lr: 0.000183  training_loss: 1.4628 (1.5029)  classification_loss: 1.4571 (1.4664)  loss_mask: 0.0046 (0.0365)  time: 0.1646  data: 0.0003  max mem: 4132
[18:55:53.975675] Epoch: [37]  [780/781]  eta: 0:00:00  lr: 0.000183  training_loss: 1.4172 (1.5005)  classification_loss: 1.4090 (1.4648)  loss_mask: 0.0033 (0.0357)  time: 0.1630  data: 0.0002  max mem: 4132
[18:55:54.152334] Epoch: [37] Total time: 0:02:09 (0.1663 s / it)
[18:55:54.154173] Averaged stats: lr: 0.000183  training_loss: 1.4172 (1.5005)  classification_loss: 1.4090 (1.4648)  loss_mask: 0.0033 (0.0357)
[18:55:54.940452] Test:  [  0/157]  eta: 0:02:02  testing_loss: 0.6765 (0.6765)  acc1: 81.2500 (81.2500)  acc5: 100.0000 (100.0000)  time: 0.7819  data: 0.7461  max mem: 4132
[18:55:55.237160] Test:  [ 10/157]  eta: 0:00:14  testing_loss: 0.6987 (0.7439)  acc1: 76.5625 (75.1420)  acc5: 100.0000 (99.2898)  time: 0.0979  data: 0.0684  max mem: 4132
[18:55:55.525712] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.7120 (0.7301)  acc1: 76.5625 (75.5208)  acc5: 98.4375 (99.1071)  time: 0.0291  data: 0.0004  max mem: 4132
[18:55:55.812230] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.7205 (0.7451)  acc1: 75.0000 (75.2520)  acc5: 98.4375 (98.8911)  time: 0.0286  data: 0.0002  max mem: 4132
[18:55:56.098258] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.7205 (0.7392)  acc1: 75.0000 (75.7241)  acc5: 98.4375 (98.8186)  time: 0.0285  data: 0.0002  max mem: 4132
[18:55:56.389582] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6934 (0.7336)  acc1: 76.5625 (75.9498)  acc5: 98.4375 (98.7439)  time: 0.0287  data: 0.0002  max mem: 4132
[18:55:56.675072] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6914 (0.7330)  acc1: 76.5625 (75.8965)  acc5: 98.4375 (98.7705)  time: 0.0287  data: 0.0002  max mem: 4132
[18:55:56.961587] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7013 (0.7281)  acc1: 76.5625 (76.0563)  acc5: 98.4375 (98.7456)  time: 0.0285  data: 0.0002  max mem: 4132
[18:55:57.249277] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7196 (0.7397)  acc1: 76.5625 (75.7716)  acc5: 98.4375 (98.5532)  time: 0.0286  data: 0.0002  max mem: 4132
[18:55:57.538674] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7828 (0.7383)  acc1: 75.0000 (75.8070)  acc5: 98.4375 (98.5577)  time: 0.0286  data: 0.0002  max mem: 4132
[18:55:57.824421] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.7389 (0.7430)  acc1: 75.0000 (75.6343)  acc5: 98.4375 (98.5613)  time: 0.0285  data: 0.0002  max mem: 4132
[18:55:58.110587] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.8032 (0.7435)  acc1: 75.0000 (75.6475)  acc5: 98.4375 (98.5220)  time: 0.0284  data: 0.0002  max mem: 4132
[18:55:58.395882] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.7356 (0.7404)  acc1: 76.5625 (75.8264)  acc5: 98.4375 (98.5150)  time: 0.0284  data: 0.0002  max mem: 4132
[18:55:58.681653] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.7117 (0.7397)  acc1: 76.5625 (75.8707)  acc5: 98.4375 (98.5329)  time: 0.0284  data: 0.0002  max mem: 4132
[18:55:58.967329] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7436 (0.7401)  acc1: 75.0000 (75.9087)  acc5: 98.4375 (98.4929)  time: 0.0284  data: 0.0002  max mem: 4132
[18:55:59.249900] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7635 (0.7389)  acc1: 76.5625 (75.9106)  acc5: 98.4375 (98.4478)  time: 0.0283  data: 0.0002  max mem: 4132
[18:55:59.402768] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7120 (0.7404)  acc1: 76.5625 (75.8900)  acc5: 98.4375 (98.4300)  time: 0.0273  data: 0.0001  max mem: 4132
[18:55:59.575290] Test: Total time: 0:00:05 (0.0345 s / it)
[18:55:59.575894] * Acc@1 75.890 Acc@5 98.430 loss 0.740
[18:55:59.576210] Accuracy of the network on the 10000 test images: 75.9%
[18:55:59.576421] Max accuracy: 75.89%
[18:55:59.876919] log_dir: ./output_dir
[18:56:00.815763] Epoch: [38]  [  0/781]  eta: 0:12:11  lr: 0.000183  training_loss: 1.0855 (1.0855)  classification_loss: 1.0842 (1.0842)  loss_mask: 0.0013 (0.0013)  time: 0.9366  data: 0.7601  max mem: 4132
[18:56:04.116985] Epoch: [38]  [ 20/781]  eta: 0:02:33  lr: 0.000183  training_loss: 1.4439 (1.4358)  classification_loss: 1.4399 (1.4329)  loss_mask: 0.0024 (0.0029)  time: 0.1649  data: 0.0003  max mem: 4132
[18:56:07.454572] Epoch: [38]  [ 40/781]  eta: 0:02:16  lr: 0.000183  training_loss: 1.4403 (1.4421)  classification_loss: 1.4386 (1.4394)  loss_mask: 0.0020 (0.0026)  time: 0.1668  data: 0.0003  max mem: 4132
[18:56:10.777090] Epoch: [38]  [ 60/781]  eta: 0:02:08  lr: 0.000183  training_loss: 1.4546 (1.4531)  classification_loss: 1.4529 (1.4506)  loss_mask: 0.0018 (0.0024)  time: 0.1660  data: 0.0004  max mem: 4132
[18:56:14.102351] Epoch: [38]  [ 80/781]  eta: 0:02:03  lr: 0.000183  training_loss: 1.4401 (1.4571)  classification_loss: 1.4393 (1.4548)  loss_mask: 0.0017 (0.0023)  time: 0.1662  data: 0.0003  max mem: 4132
[18:56:17.454345] Epoch: [38]  [100/781]  eta: 0:01:58  lr: 0.000182  training_loss: 1.4461 (1.4577)  classification_loss: 1.4456 (1.4556)  loss_mask: 0.0015 (0.0021)  time: 0.1675  data: 0.0003  max mem: 4132
[18:56:20.779173] Epoch: [38]  [120/781]  eta: 0:01:54  lr: 0.000182  training_loss: 1.4172 (1.4529)  classification_loss: 1.4162 (1.4509)  loss_mask: 0.0014 (0.0021)  time: 0.1662  data: 0.0003  max mem: 4132
[18:56:24.105917] Epoch: [38]  [140/781]  eta: 0:01:50  lr: 0.000182  training_loss: 1.4235 (1.4487)  classification_loss: 1.4218 (1.4467)  loss_mask: 0.0012 (0.0020)  time: 0.1662  data: 0.0004  max mem: 4132
[18:56:27.442902] Epoch: [38]  [160/781]  eta: 0:01:46  lr: 0.000182  training_loss: 1.3987 (1.4482)  classification_loss: 1.3971 (1.4464)  loss_mask: 0.0013 (0.0019)  time: 0.1668  data: 0.0003  max mem: 4132
[18:56:30.722167] Epoch: [38]  [180/781]  eta: 0:01:42  lr: 0.000182  training_loss: 1.4504 (1.4487)  classification_loss: 1.4491 (1.4469)  loss_mask: 0.0012 (0.0018)  time: 0.1638  data: 0.0002  max mem: 4132
[18:56:34.031758] Epoch: [38]  [200/781]  eta: 0:01:38  lr: 0.000182  training_loss: 1.4543 (1.4522)  classification_loss: 1.4523 (1.4504)  loss_mask: 0.0011 (0.0018)  time: 0.1654  data: 0.0003  max mem: 4132
[18:56:37.311615] Epoch: [38]  [220/781]  eta: 0:01:34  lr: 0.000182  training_loss: 1.4158 (1.4493)  classification_loss: 1.4154 (1.4476)  loss_mask: 0.0010 (0.0017)  time: 0.1639  data: 0.0002  max mem: 4132
[18:56:40.594079] Epoch: [38]  [240/781]  eta: 0:01:31  lr: 0.000182  training_loss: 1.4440 (1.4506)  classification_loss: 1.4422 (1.4490)  loss_mask: 0.0011 (0.0016)  time: 0.1640  data: 0.0003  max mem: 4132
[18:56:43.878817] Epoch: [38]  [260/781]  eta: 0:01:27  lr: 0.000182  training_loss: 1.4234 (1.4496)  classification_loss: 1.4161 (1.4475)  loss_mask: 0.0007 (0.0020)  time: 0.1642  data: 0.0003  max mem: 4132
[18:56:47.184383] Epoch: [38]  [280/781]  eta: 0:01:24  lr: 0.000182  training_loss: 1.5456 (1.4558)  classification_loss: 1.4917 (1.4500)  loss_mask: 0.0135 (0.0058)  time: 0.1652  data: 0.0005  max mem: 4132
[18:56:50.455863] Epoch: [38]  [300/781]  eta: 0:01:20  lr: 0.000182  training_loss: 1.5436 (1.4657)  classification_loss: 1.4584 (1.4515)  loss_mask: 0.0718 (0.0142)  time: 0.1635  data: 0.0004  max mem: 4132
[18:56:53.740305] Epoch: [38]  [320/781]  eta: 0:01:17  lr: 0.000181  training_loss: 1.5394 (1.4735)  classification_loss: 1.4478 (1.4518)  loss_mask: 0.0751 (0.0217)  time: 0.1641  data: 0.0003  max mem: 4132
[18:56:57.028282] Epoch: [38]  [340/781]  eta: 0:01:13  lr: 0.000181  training_loss: 1.4863 (1.4744)  classification_loss: 1.4379 (1.4514)  loss_mask: 0.0301 (0.0230)  time: 0.1643  data: 0.0003  max mem: 4132
[18:57:00.319991] Epoch: [38]  [360/781]  eta: 0:01:10  lr: 0.000181  training_loss: 1.5345 (1.4766)  classification_loss: 1.4663 (1.4519)  loss_mask: 0.0337 (0.0247)  time: 0.1645  data: 0.0003  max mem: 4132
[18:57:03.610305] Epoch: [38]  [380/781]  eta: 0:01:07  lr: 0.000181  training_loss: 1.4260 (1.4751)  classification_loss: 1.3746 (1.4491)  loss_mask: 0.0365 (0.0260)  time: 0.1644  data: 0.0003  max mem: 4132
[18:57:06.901263] Epoch: [38]  [400/781]  eta: 0:01:03  lr: 0.000181  training_loss: 1.4342 (1.4727)  classification_loss: 1.4092 (1.4468)  loss_mask: 0.0204 (0.0259)  time: 0.1645  data: 0.0003  max mem: 4132
[18:57:10.194851] Epoch: [38]  [420/781]  eta: 0:01:00  lr: 0.000181  training_loss: 1.4181 (1.4707)  classification_loss: 1.3940 (1.4453)  loss_mask: 0.0134 (0.0254)  time: 0.1646  data: 0.0003  max mem: 4132
[18:57:13.471521] Epoch: [38]  [440/781]  eta: 0:00:56  lr: 0.000181  training_loss: 1.5159 (1.4722)  classification_loss: 1.4681 (1.4463)  loss_mask: 0.0240 (0.0260)  time: 0.1638  data: 0.0002  max mem: 4132
[18:57:16.764796] Epoch: [38]  [460/781]  eta: 0:00:53  lr: 0.000181  training_loss: 1.4664 (1.4728)  classification_loss: 1.4419 (1.4464)  loss_mask: 0.0200 (0.0264)  time: 0.1646  data: 0.0003  max mem: 4132
[18:57:20.060514] Epoch: [38]  [480/781]  eta: 0:00:50  lr: 0.000181  training_loss: 1.5143 (1.4755)  classification_loss: 1.4548 (1.4474)  loss_mask: 0.0400 (0.0281)  time: 0.1647  data: 0.0003  max mem: 4132
[18:57:23.359382] Epoch: [38]  [500/781]  eta: 0:00:46  lr: 0.000181  training_loss: 1.4385 (1.4749)  classification_loss: 1.4181 (1.4470)  loss_mask: 0.0172 (0.0279)  time: 0.1649  data: 0.0003  max mem: 4132
[18:57:26.701057] Epoch: [38]  [520/781]  eta: 0:00:43  lr: 0.000180  training_loss: 1.3808 (1.4729)  classification_loss: 1.3584 (1.4454)  loss_mask: 0.0143 (0.0275)  time: 0.1670  data: 0.0002  max mem: 4132
[18:57:30.072761] Epoch: [38]  [540/781]  eta: 0:00:40  lr: 0.000180  training_loss: 1.4795 (1.4738)  classification_loss: 1.4758 (1.4470)  loss_mask: 0.0081 (0.0268)  time: 0.1685  data: 0.0004  max mem: 4132
[18:57:33.405614] Epoch: [38]  [560/781]  eta: 0:00:36  lr: 0.000180  training_loss: 1.4582 (1.4728)  classification_loss: 1.4461 (1.4462)  loss_mask: 0.0117 (0.0266)  time: 0.1665  data: 0.0003  max mem: 4132
[18:57:36.737656] Epoch: [38]  [580/781]  eta: 0:00:33  lr: 0.000180  training_loss: 1.4761 (1.4730)  classification_loss: 1.4416 (1.4463)  loss_mask: 0.0184 (0.0267)  time: 0.1665  data: 0.0003  max mem: 4132
[18:57:40.032873] Epoch: [38]  [600/781]  eta: 0:00:30  lr: 0.000180  training_loss: 1.5171 (1.4746)  classification_loss: 1.4729 (1.4471)  loss_mask: 0.0467 (0.0275)  time: 0.1647  data: 0.0003  max mem: 4132
[18:57:43.299996] Epoch: [38]  [620/781]  eta: 0:00:26  lr: 0.000180  training_loss: 1.4211 (1.4740)  classification_loss: 1.4021 (1.4462)  loss_mask: 0.0261 (0.0278)  time: 0.1632  data: 0.0002  max mem: 4132
[18:57:46.579382] Epoch: [38]  [640/781]  eta: 0:00:23  lr: 0.000180  training_loss: 1.4640 (1.4741)  classification_loss: 1.4306 (1.4463)  loss_mask: 0.0153 (0.0277)  time: 0.1639  data: 0.0002  max mem: 4132
[18:57:49.879403] Epoch: [38]  [660/781]  eta: 0:00:20  lr: 0.000180  training_loss: 1.5037 (1.4748)  classification_loss: 1.4984 (1.4477)  loss_mask: 0.0053 (0.0271)  time: 0.1649  data: 0.0002  max mem: 4132
[18:57:53.184262] Epoch: [38]  [680/781]  eta: 0:00:16  lr: 0.000180  training_loss: 1.4702 (1.4747)  classification_loss: 1.4402 (1.4481)  loss_mask: 0.0041 (0.0267)  time: 0.1651  data: 0.0003  max mem: 4132
[18:57:56.484775] Epoch: [38]  [700/781]  eta: 0:00:13  lr: 0.000180  training_loss: 1.5103 (1.4751)  classification_loss: 1.5056 (1.4485)  loss_mask: 0.0040 (0.0266)  time: 0.1649  data: 0.0002  max mem: 4132
[18:57:59.835541] Epoch: [38]  [720/781]  eta: 0:00:10  lr: 0.000180  training_loss: 1.4779 (1.4760)  classification_loss: 1.4745 (1.4498)  loss_mask: 0.0046 (0.0262)  time: 0.1674  data: 0.0003  max mem: 4132
[18:58:03.160456] Epoch: [38]  [740/781]  eta: 0:00:06  lr: 0.000179  training_loss: 1.4614 (1.4760)  classification_loss: 1.4572 (1.4499)  loss_mask: 0.0058 (0.0260)  time: 0.1661  data: 0.0003  max mem: 4132
[18:58:06.521362] Epoch: [38]  [760/781]  eta: 0:00:03  lr: 0.000179  training_loss: 1.5872 (1.4787)  classification_loss: 1.4925 (1.4509)  loss_mask: 0.0404 (0.0278)  time: 0.1680  data: 0.0004  max mem: 4132
[18:58:09.824299] Epoch: [38]  [780/781]  eta: 0:00:00  lr: 0.000179  training_loss: 1.5242 (1.4795)  classification_loss: 1.4906 (1.4512)  loss_mask: 0.0391 (0.0284)  time: 0.1650  data: 0.0002  max mem: 4132
[18:58:09.995984] Epoch: [38] Total time: 0:02:10 (0.1666 s / it)
[18:58:09.996835] Averaged stats: lr: 0.000179  training_loss: 1.5242 (1.4795)  classification_loss: 1.4906 (1.4512)  loss_mask: 0.0391 (0.0284)
[18:58:10.735626] Test:  [  0/157]  eta: 0:01:55  testing_loss: 0.7454 (0.7454)  acc1: 75.0000 (75.0000)  acc5: 95.3125 (95.3125)  time: 0.7338  data: 0.6972  max mem: 4132
[18:58:11.041996] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.7692 (0.7699)  acc1: 75.0000 (75.1420)  acc5: 98.4375 (98.5795)  time: 0.0943  data: 0.0637  max mem: 4132
[18:58:11.332904] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.7348 (0.7414)  acc1: 76.5625 (76.1905)  acc5: 98.4375 (98.7351)  time: 0.0296  data: 0.0003  max mem: 4132
[18:58:11.628232] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.7348 (0.7552)  acc1: 75.0000 (75.4536)  acc5: 98.4375 (98.4375)  time: 0.0291  data: 0.0002  max mem: 4132
[18:58:11.922284] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.7355 (0.7550)  acc1: 75.0000 (75.4573)  acc5: 98.4375 (98.3613)  time: 0.0293  data: 0.0002  max mem: 4132
[18:58:12.214883] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.7241 (0.7495)  acc1: 76.5625 (76.1029)  acc5: 98.4375 (98.3150)  time: 0.0291  data: 0.0002  max mem: 4132
[18:58:12.502971] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.7411 (0.7522)  acc1: 76.5625 (75.7428)  acc5: 96.8750 (98.1814)  time: 0.0288  data: 0.0002  max mem: 4132
[18:58:12.793085] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7411 (0.7470)  acc1: 76.5625 (75.9023)  acc5: 96.8750 (98.1954)  time: 0.0287  data: 0.0002  max mem: 4132
[18:58:13.085709] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7419 (0.7526)  acc1: 75.0000 (75.6559)  acc5: 98.4375 (98.1481)  time: 0.0290  data: 0.0003  max mem: 4132
[18:58:13.384989] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7760 (0.7528)  acc1: 75.0000 (75.7555)  acc5: 98.4375 (98.2143)  time: 0.0294  data: 0.0003  max mem: 4132
[18:58:13.681390] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.7549 (0.7576)  acc1: 73.4375 (75.5569)  acc5: 98.4375 (98.2209)  time: 0.0295  data: 0.0003  max mem: 4132
[18:58:13.979224] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.7760 (0.7563)  acc1: 73.4375 (75.5771)  acc5: 100.0000 (98.2545)  time: 0.0295  data: 0.0003  max mem: 4132
[18:58:14.272880] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.7408 (0.7534)  acc1: 76.5625 (75.6715)  acc5: 98.4375 (98.3084)  time: 0.0294  data: 0.0003  max mem: 4132
[18:58:14.567499] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.7029 (0.7533)  acc1: 76.5625 (75.6918)  acc5: 98.4375 (98.3421)  time: 0.0292  data: 0.0003  max mem: 4132
[18:58:14.857352] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7863 (0.7544)  acc1: 73.4375 (75.6760)  acc5: 98.4375 (98.3599)  time: 0.0290  data: 0.0002  max mem: 4132
[18:58:15.141994] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7862 (0.7536)  acc1: 73.4375 (75.5898)  acc5: 98.4375 (98.3237)  time: 0.0286  data: 0.0002  max mem: 4132
[18:58:15.297348] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7249 (0.7543)  acc1: 73.4375 (75.5200)  acc5: 98.4375 (98.3500)  time: 0.0276  data: 0.0002  max mem: 4132
[18:58:15.482487] Test: Total time: 0:00:05 (0.0349 s / it)
[18:58:15.483029] * Acc@1 75.520 Acc@5 98.350 loss 0.754
[18:58:15.483416] Accuracy of the network on the 10000 test images: 75.5%
[18:58:15.483653] Max accuracy: 75.89%
[18:58:15.867859] log_dir: ./output_dir
[18:58:16.908749] Epoch: [39]  [  0/781]  eta: 0:13:30  lr: 0.000179  training_loss: 1.2566 (1.2566)  classification_loss: 1.2316 (1.2316)  loss_mask: 0.0249 (0.0249)  time: 1.0380  data: 0.8202  max mem: 4132
[18:58:20.269031] Epoch: [39]  [ 20/781]  eta: 0:02:39  lr: 0.000179  training_loss: 1.4415 (1.4416)  classification_loss: 1.4305 (1.4210)  loss_mask: 0.0155 (0.0206)  time: 0.1678  data: 0.0003  max mem: 4132
[18:58:23.588022] Epoch: [39]  [ 40/781]  eta: 0:02:19  lr: 0.000179  training_loss: 1.4904 (1.4859)  classification_loss: 1.4632 (1.4316)  loss_mask: 0.0343 (0.0543)  time: 0.1658  data: 0.0003  max mem: 4132
[18:58:26.891442] Epoch: [39]  [ 60/781]  eta: 0:02:10  lr: 0.000179  training_loss: 1.4661 (1.4999)  classification_loss: 1.4293 (1.4419)  loss_mask: 0.0525 (0.0580)  time: 0.1651  data: 0.0003  max mem: 4132
[18:58:30.199635] Epoch: [39]  [ 80/781]  eta: 0:02:03  lr: 0.000179  training_loss: 1.4699 (1.4977)  classification_loss: 1.4190 (1.4420)  loss_mask: 0.0281 (0.0557)  time: 0.1653  data: 0.0003  max mem: 4132
[18:58:33.525343] Epoch: [39]  [100/781]  eta: 0:01:58  lr: 0.000179  training_loss: 1.4927 (1.4963)  classification_loss: 1.4598 (1.4441)  loss_mask: 0.0258 (0.0522)  time: 0.1662  data: 0.0003  max mem: 4132
[18:58:36.874939] Epoch: [39]  [120/781]  eta: 0:01:54  lr: 0.000179  training_loss: 1.3916 (1.4862)  classification_loss: 1.3870 (1.4386)  loss_mask: 0.0238 (0.0476)  time: 0.1674  data: 0.0004  max mem: 4132
[18:58:40.186740] Epoch: [39]  [140/781]  eta: 0:01:50  lr: 0.000179  training_loss: 1.4348 (1.4777)  classification_loss: 1.4233 (1.4351)  loss_mask: 0.0101 (0.0426)  time: 0.1655  data: 0.0003  max mem: 4132
[18:58:43.497622] Epoch: [39]  [160/781]  eta: 0:01:46  lr: 0.000178  training_loss: 1.4225 (1.4694)  classification_loss: 1.4107 (1.4303)  loss_mask: 0.0117 (0.0392)  time: 0.1654  data: 0.0003  max mem: 4132
[18:58:46.820585] Epoch: [39]  [180/781]  eta: 0:01:42  lr: 0.000178  training_loss: 1.4069 (1.4640)  classification_loss: 1.4005 (1.4282)  loss_mask: 0.0042 (0.0358)  time: 0.1660  data: 0.0003  max mem: 4132
[18:58:50.138590] Epoch: [39]  [200/781]  eta: 0:01:38  lr: 0.000178  training_loss: 1.4425 (1.4629)  classification_loss: 1.4386 (1.4304)  loss_mask: 0.0033 (0.0326)  time: 0.1658  data: 0.0003  max mem: 4132
[18:58:53.442912] Epoch: [39]  [220/781]  eta: 0:01:35  lr: 0.000178  training_loss: 1.4148 (1.4620)  classification_loss: 1.4102 (1.4321)  loss_mask: 0.0024 (0.0299)  time: 0.1651  data: 0.0003  max mem: 4132
[18:58:56.756235] Epoch: [39]  [240/781]  eta: 0:01:31  lr: 0.000178  training_loss: 1.4760 (1.4628)  classification_loss: 1.4725 (1.4353)  loss_mask: 0.0019 (0.0276)  time: 0.1656  data: 0.0002  max mem: 4132
[18:59:00.073811] Epoch: [39]  [260/781]  eta: 0:01:28  lr: 0.000178  training_loss: 1.4386 (1.4628)  classification_loss: 1.4368 (1.4373)  loss_mask: 0.0016 (0.0256)  time: 0.1658  data: 0.0002  max mem: 4132
[18:59:03.390822] Epoch: [39]  [280/781]  eta: 0:01:24  lr: 0.000178  training_loss: 1.4662 (1.4644)  classification_loss: 1.4646 (1.4405)  loss_mask: 0.0012 (0.0239)  time: 0.1657  data: 0.0003  max mem: 4132
[18:59:06.685031] Epoch: [39]  [300/781]  eta: 0:01:21  lr: 0.000178  training_loss: 1.3822 (1.4613)  classification_loss: 1.3799 (1.4390)  loss_mask: 0.0013 (0.0224)  time: 0.1646  data: 0.0003  max mem: 4132
[18:59:09.974205] Epoch: [39]  [320/781]  eta: 0:01:17  lr: 0.000178  training_loss: 1.4687 (1.4608)  classification_loss: 1.4682 (1.4398)  loss_mask: 0.0014 (0.0210)  time: 0.1644  data: 0.0002  max mem: 4132
[18:59:13.263203] Epoch: [39]  [340/781]  eta: 0:01:14  lr: 0.000178  training_loss: 1.4403 (1.4598)  classification_loss: 1.4370 (1.4399)  loss_mask: 0.0012 (0.0199)  time: 0.1643  data: 0.0003  max mem: 4132
[18:59:16.594134] Epoch: [39]  [360/781]  eta: 0:01:10  lr: 0.000178  training_loss: 1.4188 (1.4581)  classification_loss: 1.4183 (1.4393)  loss_mask: 0.0010 (0.0189)  time: 0.1664  data: 0.0004  max mem: 4132
[18:59:19.878860] Epoch: [39]  [380/781]  eta: 0:01:07  lr: 0.000177  training_loss: 1.4313 (1.4575)  classification_loss: 1.4294 (1.4391)  loss_mask: 0.0025 (0.0184)  time: 0.1641  data: 0.0003  max mem: 4132
[18:59:23.157379] Epoch: [39]  [400/781]  eta: 0:01:03  lr: 0.000177  training_loss: 1.4979 (1.4592)  classification_loss: 1.4508 (1.4408)  loss_mask: 0.0108 (0.0184)  time: 0.1638  data: 0.0002  max mem: 4132
[18:59:26.445220] Epoch: [39]  [420/781]  eta: 0:01:00  lr: 0.000177  training_loss: 1.5917 (1.4658)  classification_loss: 1.4365 (1.4404)  loss_mask: 0.1764 (0.0254)  time: 0.1643  data: 0.0002  max mem: 4132
[18:59:29.745982] Epoch: [39]  [440/781]  eta: 0:00:57  lr: 0.000177  training_loss: 1.5366 (1.4697)  classification_loss: 1.4141 (1.4394)  loss_mask: 0.1247 (0.0303)  time: 0.1649  data: 0.0003  max mem: 4132
[18:59:33.035791] Epoch: [39]  [460/781]  eta: 0:00:53  lr: 0.000177  training_loss: 1.5112 (1.4729)  classification_loss: 1.4274 (1.4396)  loss_mask: 0.0768 (0.0332)  time: 0.1644  data: 0.0002  max mem: 4132
[18:59:36.301107] Epoch: [39]  [480/781]  eta: 0:00:50  lr: 0.000177  training_loss: 1.4883 (1.4752)  classification_loss: 1.4425 (1.4409)  loss_mask: 0.0401 (0.0343)  time: 0.1632  data: 0.0002  max mem: 4132
[18:59:39.616020] Epoch: [39]  [500/781]  eta: 0:00:46  lr: 0.000177  training_loss: 1.4306 (1.4740)  classification_loss: 1.4104 (1.4400)  loss_mask: 0.0236 (0.0340)  time: 0.1656  data: 0.0003  max mem: 4132
[18:59:42.934273] Epoch: [39]  [520/781]  eta: 0:00:43  lr: 0.000177  training_loss: 1.4572 (1.4731)  classification_loss: 1.4443 (1.4399)  loss_mask: 0.0117 (0.0332)  time: 0.1658  data: 0.0003  max mem: 4132
[18:59:46.248057] Epoch: [39]  [540/781]  eta: 0:00:40  lr: 0.000177  training_loss: 1.4706 (1.4724)  classification_loss: 1.4544 (1.4402)  loss_mask: 0.0073 (0.0323)  time: 0.1656  data: 0.0003  max mem: 4132
[18:59:49.591963] Epoch: [39]  [560/781]  eta: 0:00:36  lr: 0.000177  training_loss: 1.4611 (1.4718)  classification_loss: 1.4563 (1.4405)  loss_mask: 0.0058 (0.0314)  time: 0.1671  data: 0.0005  max mem: 4132
[18:59:52.894932] Epoch: [39]  [580/781]  eta: 0:00:33  lr: 0.000176  training_loss: 1.4646 (1.4707)  classification_loss: 1.4569 (1.4402)  loss_mask: 0.0056 (0.0305)  time: 0.1651  data: 0.0003  max mem: 4132
[18:59:56.207528] Epoch: [39]  [600/781]  eta: 0:00:30  lr: 0.000176  training_loss: 1.4010 (1.4684)  classification_loss: 1.3973 (1.4387)  loss_mask: 0.0029 (0.0296)  time: 0.1655  data: 0.0004  max mem: 4132
[18:59:59.533361] Epoch: [39]  [620/781]  eta: 0:00:26  lr: 0.000176  training_loss: 1.4732 (1.4681)  classification_loss: 1.4705 (1.4393)  loss_mask: 0.0022 (0.0288)  time: 0.1662  data: 0.0004  max mem: 4132
[19:00:02.877630] Epoch: [39]  [640/781]  eta: 0:00:23  lr: 0.000176  training_loss: 1.4099 (1.4663)  classification_loss: 1.4075 (1.4383)  loss_mask: 0.0020 (0.0279)  time: 0.1671  data: 0.0003  max mem: 4132
[19:00:06.155420] Epoch: [39]  [660/781]  eta: 0:00:20  lr: 0.000176  training_loss: 1.4357 (1.4656)  classification_loss: 1.4338 (1.4384)  loss_mask: 0.0017 (0.0272)  time: 0.1638  data: 0.0003  max mem: 4132
[19:00:09.487400] Epoch: [39]  [680/781]  eta: 0:00:16  lr: 0.000176  training_loss: 1.4658 (1.4660)  classification_loss: 1.4589 (1.4389)  loss_mask: 0.0092 (0.0271)  time: 0.1665  data: 0.0002  max mem: 4132
[19:00:12.848984] Epoch: [39]  [700/781]  eta: 0:00:13  lr: 0.000176  training_loss: 1.4665 (1.4662)  classification_loss: 1.4543 (1.4396)  loss_mask: 0.0073 (0.0266)  time: 0.1680  data: 0.0003  max mem: 4132
[19:00:16.154902] Epoch: [39]  [720/781]  eta: 0:00:10  lr: 0.000176  training_loss: 1.4313 (1.4656)  classification_loss: 1.4269 (1.4395)  loss_mask: 0.0058 (0.0261)  time: 0.1652  data: 0.0003  max mem: 4132
[19:00:19.460697] Epoch: [39]  [740/781]  eta: 0:00:06  lr: 0.000176  training_loss: 1.3903 (1.4640)  classification_loss: 1.3879 (1.4383)  loss_mask: 0.0069 (0.0256)  time: 0.1652  data: 0.0003  max mem: 4132
[19:00:22.753522] Epoch: [39]  [760/781]  eta: 0:00:03  lr: 0.000176  training_loss: 1.4695 (1.4638)  classification_loss: 1.4585 (1.4386)  loss_mask: 0.0049 (0.0252)  time: 0.1646  data: 0.0003  max mem: 4132
[19:00:26.009697] Epoch: [39]  [780/781]  eta: 0:00:00  lr: 0.000176  training_loss: 1.5005 (1.4649)  classification_loss: 1.4661 (1.4396)  loss_mask: 0.0052 (0.0253)  time: 0.1627  data: 0.0002  max mem: 4132
[19:00:26.206522] Epoch: [39] Total time: 0:02:10 (0.1669 s / it)
[19:00:26.207301] Averaged stats: lr: 0.000176  training_loss: 1.5005 (1.4649)  classification_loss: 1.4661 (1.4396)  loss_mask: 0.0052 (0.0253)
[19:00:26.919387] Test:  [  0/157]  eta: 0:01:50  testing_loss: 0.6986 (0.6986)  acc1: 76.5625 (76.5625)  acc5: 95.3125 (95.3125)  time: 0.7068  data: 0.6756  max mem: 4132
[19:00:27.212272] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.7938 (0.7856)  acc1: 75.0000 (73.4375)  acc5: 98.4375 (98.8636)  time: 0.0906  data: 0.0616  max mem: 4132
[19:00:27.506156] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.7377 (0.7577)  acc1: 75.0000 (75.0000)  acc5: 100.0000 (98.8839)  time: 0.0291  data: 0.0002  max mem: 4132
[19:00:27.803319] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.7434 (0.7762)  acc1: 73.4375 (74.5464)  acc5: 100.0000 (98.6895)  time: 0.0294  data: 0.0002  max mem: 4132
[19:00:28.094456] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.7457 (0.7768)  acc1: 73.4375 (74.4665)  acc5: 98.4375 (98.4375)  time: 0.0293  data: 0.0002  max mem: 4132
[19:00:28.381835] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.7373 (0.7664)  acc1: 75.0000 (75.0919)  acc5: 98.4375 (98.4375)  time: 0.0288  data: 0.0002  max mem: 4132
[19:00:28.667666] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.7327 (0.7622)  acc1: 75.0000 (75.0256)  acc5: 98.4375 (98.3607)  time: 0.0285  data: 0.0002  max mem: 4132
[19:00:28.954545] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7345 (0.7578)  acc1: 75.0000 (75.1981)  acc5: 98.4375 (98.3715)  time: 0.0285  data: 0.0002  max mem: 4132
[19:00:29.241938] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7500 (0.7663)  acc1: 75.0000 (75.1350)  acc5: 98.4375 (98.2060)  time: 0.0286  data: 0.0002  max mem: 4132
[19:00:29.529556] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7680 (0.7630)  acc1: 75.0000 (75.3777)  acc5: 98.4375 (98.2658)  time: 0.0286  data: 0.0002  max mem: 4132
[19:00:29.818237] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.8029 (0.7705)  acc1: 75.0000 (75.0155)  acc5: 98.4375 (98.2828)  time: 0.0287  data: 0.0003  max mem: 4132
[19:00:30.105067] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.8033 (0.7707)  acc1: 71.8750 (74.9296)  acc5: 98.4375 (98.3390)  time: 0.0286  data: 0.0002  max mem: 4132
[19:00:30.391144] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.7587 (0.7682)  acc1: 75.0000 (75.0775)  acc5: 98.4375 (98.3988)  time: 0.0285  data: 0.0002  max mem: 4132
[19:00:30.679072] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.7399 (0.7705)  acc1: 75.0000 (74.9046)  acc5: 98.4375 (98.3898)  time: 0.0286  data: 0.0003  max mem: 4132
[19:00:30.965493] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7961 (0.7707)  acc1: 71.8750 (74.9335)  acc5: 98.4375 (98.4375)  time: 0.0286  data: 0.0002  max mem: 4132
[19:00:31.249327] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7961 (0.7691)  acc1: 75.0000 (74.8655)  acc5: 98.4375 (98.4375)  time: 0.0284  data: 0.0002  max mem: 4132
[19:00:31.402624] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7992 (0.7708)  acc1: 71.8750 (74.7200)  acc5: 98.4375 (98.4600)  time: 0.0274  data: 0.0002  max mem: 4132
[19:00:31.588734] Test: Total time: 0:00:05 (0.0343 s / it)
[19:00:31.590063] * Acc@1 74.720 Acc@5 98.460 loss 0.771
[19:00:31.590740] Accuracy of the network on the 10000 test images: 74.7%
[19:00:31.591376] Max accuracy: 75.89%
[19:00:31.923783] log_dir: ./output_dir
[19:00:32.861641] Epoch: [40]  [  0/781]  eta: 0:12:09  lr: 0.000176  training_loss: 1.4479 (1.4479)  classification_loss: 1.4322 (1.4322)  loss_mask: 0.0157 (0.0157)  time: 0.9345  data: 0.7367  max mem: 4132
[19:00:36.136033] Epoch: [40]  [ 20/781]  eta: 0:02:32  lr: 0.000175  training_loss: 1.4256 (1.4139)  classification_loss: 1.4102 (1.4037)  loss_mask: 0.0077 (0.0101)  time: 0.1636  data: 0.0003  max mem: 4132
[19:00:39.454620] Epoch: [40]  [ 40/781]  eta: 0:02:15  lr: 0.000175  training_loss: 1.4483 (1.4340)  classification_loss: 1.4467 (1.4274)  loss_mask: 0.0022 (0.0066)  time: 0.1658  data: 0.0003  max mem: 4132
[19:00:42.748636] Epoch: [40]  [ 60/781]  eta: 0:02:07  lr: 0.000175  training_loss: 1.4630 (1.4387)  classification_loss: 1.4591 (1.4333)  loss_mask: 0.0022 (0.0054)  time: 0.1646  data: 0.0002  max mem: 4132
[19:00:46.055816] Epoch: [40]  [ 80/781]  eta: 0:02:02  lr: 0.000175  training_loss: 1.4364 (1.4347)  classification_loss: 1.4294 (1.4288)  loss_mask: 0.0028 (0.0059)  time: 0.1653  data: 0.0003  max mem: 4132
[19:00:49.389112] Epoch: [40]  [100/781]  eta: 0:01:57  lr: 0.000175  training_loss: 1.4753 (1.4439)  classification_loss: 1.4548 (1.4378)  loss_mask: 0.0031 (0.0061)  time: 0.1666  data: 0.0005  max mem: 4132
[19:00:52.677712] Epoch: [40]  [120/781]  eta: 0:01:53  lr: 0.000175  training_loss: 1.4232 (1.4405)  classification_loss: 1.4219 (1.4352)  loss_mask: 0.0011 (0.0053)  time: 0.1643  data: 0.0003  max mem: 4132
[19:00:55.986327] Epoch: [40]  [140/781]  eta: 0:01:49  lr: 0.000175  training_loss: 1.4104 (1.4364)  classification_loss: 1.4089 (1.4317)  loss_mask: 0.0015 (0.0048)  time: 0.1653  data: 0.0003  max mem: 4132
[19:00:59.290804] Epoch: [40]  [160/781]  eta: 0:01:45  lr: 0.000175  training_loss: 1.4072 (1.4343)  classification_loss: 1.4065 (1.4299)  loss_mask: 0.0013 (0.0044)  time: 0.1651  data: 0.0002  max mem: 4132
[19:01:02.572386] Epoch: [40]  [180/781]  eta: 0:01:41  lr: 0.000175  training_loss: 1.3884 (1.4349)  classification_loss: 1.3857 (1.4306)  loss_mask: 0.0022 (0.0043)  time: 0.1640  data: 0.0003  max mem: 4132
[19:01:05.860295] Epoch: [40]  [200/781]  eta: 0:01:38  lr: 0.000175  training_loss: 1.4367 (1.4386)  classification_loss: 1.4288 (1.4335)  loss_mask: 0.0024 (0.0050)  time: 0.1642  data: 0.0003  max mem: 4132
[19:01:09.134556] Epoch: [40]  [220/781]  eta: 0:01:34  lr: 0.000174  training_loss: 1.4321 (1.4367)  classification_loss: 1.4196 (1.4309)  loss_mask: 0.0093 (0.0058)  time: 0.1636  data: 0.0002  max mem: 4132
[19:01:12.395385] Epoch: [40]  [240/781]  eta: 0:01:30  lr: 0.000174  training_loss: 1.5000 (1.4413)  classification_loss: 1.4321 (1.4325)  loss_mask: 0.0330 (0.0088)  time: 0.1630  data: 0.0002  max mem: 4132
[19:01:15.661145] Epoch: [40]  [260/781]  eta: 0:01:27  lr: 0.000174  training_loss: 1.4817 (1.4458)  classification_loss: 1.4423 (1.4342)  loss_mask: 0.0186 (0.0115)  time: 0.1632  data: 0.0002  max mem: 4132
[19:01:18.931065] Epoch: [40]  [280/781]  eta: 0:01:23  lr: 0.000174  training_loss: 1.4771 (1.4460)  classification_loss: 1.4621 (1.4342)  loss_mask: 0.0078 (0.0118)  time: 0.1634  data: 0.0002  max mem: 4132
[19:01:22.186860] Epoch: [40]  [300/781]  eta: 0:01:20  lr: 0.000174  training_loss: 1.4763 (1.4497)  classification_loss: 1.4706 (1.4382)  loss_mask: 0.0056 (0.0115)  time: 0.1627  data: 0.0002  max mem: 4132
[19:01:25.466633] Epoch: [40]  [320/781]  eta: 0:01:16  lr: 0.000174  training_loss: 1.4173 (1.4490)  classification_loss: 1.4123 (1.4377)  loss_mask: 0.0053 (0.0113)  time: 0.1639  data: 0.0002  max mem: 4132
[19:01:28.724527] Epoch: [40]  [340/781]  eta: 0:01:13  lr: 0.000174  training_loss: 1.4319 (1.4481)  classification_loss: 1.3971 (1.4355)  loss_mask: 0.0158 (0.0126)  time: 0.1628  data: 0.0002  max mem: 4132
[19:01:32.019634] Epoch: [40]  [360/781]  eta: 0:01:10  lr: 0.000174  training_loss: 1.4430 (1.4477)  classification_loss: 1.4383 (1.4344)  loss_mask: 0.0117 (0.0132)  time: 0.1647  data: 0.0003  max mem: 4132
[19:01:35.328349] Epoch: [40]  [380/781]  eta: 0:01:06  lr: 0.000174  training_loss: 1.5309 (1.4528)  classification_loss: 1.3868 (1.4338)  loss_mask: 0.0259 (0.0190)  time: 0.1653  data: 0.0003  max mem: 4132
[19:01:38.620541] Epoch: [40]  [400/781]  eta: 0:01:03  lr: 0.000174  training_loss: 1.4903 (1.4555)  classification_loss: 1.4445 (1.4345)  loss_mask: 0.0452 (0.0210)  time: 0.1645  data: 0.0003  max mem: 4132
[19:01:41.931060] Epoch: [40]  [420/781]  eta: 0:00:59  lr: 0.000173  training_loss: 1.4211 (1.4550)  classification_loss: 1.4079 (1.4341)  loss_mask: 0.0162 (0.0208)  time: 0.1654  data: 0.0002  max mem: 4132
[19:01:45.227616] Epoch: [40]  [440/781]  eta: 0:00:56  lr: 0.000173  training_loss: 1.4188 (1.4541)  classification_loss: 1.4103 (1.4338)  loss_mask: 0.0084 (0.0203)  time: 0.1647  data: 0.0003  max mem: 4132
[19:01:48.551850] Epoch: [40]  [460/781]  eta: 0:00:53  lr: 0.000173  training_loss: 1.4502 (1.4530)  classification_loss: 1.4467 (1.4334)  loss_mask: 0.0039 (0.0196)  time: 0.1661  data: 0.0003  max mem: 4132
[19:01:51.865049] Epoch: [40]  [480/781]  eta: 0:00:49  lr: 0.000173  training_loss: 1.4058 (1.4514)  classification_loss: 1.4020 (1.4325)  loss_mask: 0.0031 (0.0189)  time: 0.1656  data: 0.0003  max mem: 4132
[19:01:55.156025] Epoch: [40]  [500/781]  eta: 0:00:46  lr: 0.000173  training_loss: 1.4244 (1.4503)  classification_loss: 1.4231 (1.4321)  loss_mask: 0.0014 (0.0182)  time: 0.1645  data: 0.0003  max mem: 4132
[19:01:58.442836] Epoch: [40]  [520/781]  eta: 0:00:43  lr: 0.000173  training_loss: 1.4266 (1.4500)  classification_loss: 1.4245 (1.4324)  loss_mask: 0.0019 (0.0176)  time: 0.1643  data: 0.0003  max mem: 4132
[19:02:01.749115] Epoch: [40]  [540/781]  eta: 0:00:39  lr: 0.000173  training_loss: 1.4037 (1.4485)  classification_loss: 1.4021 (1.4315)  loss_mask: 0.0012 (0.0170)  time: 0.1652  data: 0.0003  max mem: 4132
[19:02:05.043306] Epoch: [40]  [560/781]  eta: 0:00:36  lr: 0.000173  training_loss: 1.3908 (1.4465)  classification_loss: 1.3903 (1.4300)  loss_mask: 0.0013 (0.0165)  time: 0.1646  data: 0.0002  max mem: 4132
[19:02:08.347213] Epoch: [40]  [580/781]  eta: 0:00:33  lr: 0.000173  training_loss: 1.4111 (1.4452)  classification_loss: 1.4100 (1.4293)  loss_mask: 0.0013 (0.0160)  time: 0.1651  data: 0.0002  max mem: 4132
[19:02:11.628092] Epoch: [40]  [600/781]  eta: 0:00:30  lr: 0.000173  training_loss: 1.4313 (1.4449)  classification_loss: 1.4303 (1.4294)  loss_mask: 0.0010 (0.0155)  time: 0.1639  data: 0.0004  max mem: 4132
[19:02:14.907446] Epoch: [40]  [620/781]  eta: 0:00:26  lr: 0.000173  training_loss: 1.4185 (1.4440)  classification_loss: 1.4176 (1.4290)  loss_mask: 0.0009 (0.0150)  time: 0.1639  data: 0.0003  max mem: 4132
[19:02:18.197824] Epoch: [40]  [640/781]  eta: 0:00:23  lr: 0.000172  training_loss: 1.4318 (1.4432)  classification_loss: 1.4309 (1.4286)  loss_mask: 0.0007 (0.0146)  time: 0.1644  data: 0.0003  max mem: 4132
[19:02:21.499860] Epoch: [40]  [660/781]  eta: 0:00:20  lr: 0.000172  training_loss: 1.4153 (1.4426)  classification_loss: 1.4118 (1.4284)  loss_mask: 0.0018 (0.0142)  time: 0.1650  data: 0.0002  max mem: 4132
[19:02:24.787299] Epoch: [40]  [680/781]  eta: 0:00:16  lr: 0.000172  training_loss: 1.4602 (1.4432)  classification_loss: 1.4560 (1.4291)  loss_mask: 0.0010 (0.0141)  time: 0.1643  data: 0.0003  max mem: 4132
[19:02:28.074979] Epoch: [40]  [700/781]  eta: 0:00:13  lr: 0.000172  training_loss: 1.6173 (1.4515)  classification_loss: 1.4639 (1.4303)  loss_mask: 0.1514 (0.0212)  time: 0.1643  data: 0.0002  max mem: 4132
[19:02:31.349706] Epoch: [40]  [720/781]  eta: 0:00:10  lr: 0.000172  training_loss: 1.4929 (1.4525)  classification_loss: 1.3978 (1.4300)  loss_mask: 0.0541 (0.0225)  time: 0.1636  data: 0.0002  max mem: 4132
[19:02:34.645828] Epoch: [40]  [740/781]  eta: 0:00:06  lr: 0.000172  training_loss: 1.4337 (1.4525)  classification_loss: 1.4047 (1.4295)  loss_mask: 0.0347 (0.0231)  time: 0.1647  data: 0.0003  max mem: 4132
[19:02:37.953043] Epoch: [40]  [760/781]  eta: 0:00:03  lr: 0.000172  training_loss: 1.4567 (1.4524)  classification_loss: 1.4272 (1.4291)  loss_mask: 0.0257 (0.0232)  time: 0.1653  data: 0.0004  max mem: 4132
[19:02:41.215805] Epoch: [40]  [780/781]  eta: 0:00:00  lr: 0.000172  training_loss: 1.4432 (1.4523)  classification_loss: 1.4345 (1.4294)  loss_mask: 0.0120 (0.0230)  time: 0.1630  data: 0.0003  max mem: 4132
[19:02:41.378531] Epoch: [40] Total time: 0:02:09 (0.1658 s / it)
[19:02:41.379233] Averaged stats: lr: 0.000172  training_loss: 1.4432 (1.4523)  classification_loss: 1.4345 (1.4294)  loss_mask: 0.0120 (0.0230)
[19:02:43.176857] Test:  [  0/157]  eta: 0:01:54  testing_loss: 0.7137 (0.7137)  acc1: 76.5625 (76.5625)  acc5: 98.4375 (98.4375)  time: 0.7278  data: 0.6887  max mem: 4132
[19:02:43.480233] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.7137 (0.7295)  acc1: 76.5625 (76.8466)  acc5: 100.0000 (99.1477)  time: 0.0935  data: 0.0628  max mem: 4132
[19:02:43.765967] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.6991 (0.7207)  acc1: 76.5625 (77.0833)  acc5: 98.4375 (98.6607)  time: 0.0293  data: 0.0002  max mem: 4132
[19:02:44.058557] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.7062 (0.7352)  acc1: 78.1250 (76.7137)  acc5: 98.4375 (98.4879)  time: 0.0287  data: 0.0002  max mem: 4132
[19:02:44.347578] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6880 (0.7285)  acc1: 76.5625 (76.9055)  acc5: 98.4375 (98.3232)  time: 0.0288  data: 0.0003  max mem: 4132
[19:02:44.637548] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.7014 (0.7248)  acc1: 78.1250 (77.2672)  acc5: 98.4375 (98.2537)  time: 0.0287  data: 0.0003  max mem: 4132
[19:02:44.938082] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.7127 (0.7220)  acc1: 76.5625 (76.9211)  acc5: 98.4375 (98.3094)  time: 0.0293  data: 0.0003  max mem: 4132
[19:02:45.232731] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7027 (0.7163)  acc1: 76.5625 (77.2007)  acc5: 98.4375 (98.3935)  time: 0.0296  data: 0.0003  max mem: 4132
[19:02:45.532524] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7107 (0.7228)  acc1: 78.1250 (77.1798)  acc5: 98.4375 (98.3218)  time: 0.0295  data: 0.0003  max mem: 4132
[19:02:45.826807] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7265 (0.7216)  acc1: 76.5625 (77.1291)  acc5: 98.4375 (98.3345)  time: 0.0295  data: 0.0003  max mem: 4132
[19:02:46.116386] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.7333 (0.7258)  acc1: 75.0000 (76.8100)  acc5: 98.4375 (98.3756)  time: 0.0290  data: 0.0002  max mem: 4132
[19:02:46.409162] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.7590 (0.7267)  acc1: 73.4375 (76.7033)  acc5: 98.4375 (98.3812)  time: 0.0289  data: 0.0002  max mem: 4132
[19:02:46.701630] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6919 (0.7228)  acc1: 78.1250 (76.8724)  acc5: 98.4375 (98.3988)  time: 0.0290  data: 0.0002  max mem: 4132
[19:02:46.992055] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6895 (0.7233)  acc1: 76.5625 (76.7772)  acc5: 98.4375 (98.4375)  time: 0.0289  data: 0.0003  max mem: 4132
[19:02:47.278743] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7605 (0.7222)  acc1: 75.0000 (76.7952)  acc5: 98.4375 (98.4707)  time: 0.0286  data: 0.0003  max mem: 4132
[19:02:47.563119] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7324 (0.7210)  acc1: 76.5625 (76.8419)  acc5: 98.4375 (98.4685)  time: 0.0284  data: 0.0002  max mem: 4132
[19:02:47.717497] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6841 (0.7208)  acc1: 76.5625 (76.7900)  acc5: 98.4375 (98.4800)  time: 0.0274  data: 0.0002  max mem: 4132
[19:02:47.899257] Test: Total time: 0:00:05 (0.0347 s / it)
[19:02:47.899947] * Acc@1 76.790 Acc@5 98.480 loss 0.721
[19:02:47.900428] Accuracy of the network on the 10000 test images: 76.8%
[19:02:47.900712] Max accuracy: 76.79%
[19:02:48.111856] log_dir: ./output_dir
[19:02:49.150903] Epoch: [41]  [  0/781]  eta: 0:13:29  lr: 0.000172  training_loss: 1.3576 (1.3576)  classification_loss: 1.3456 (1.3456)  loss_mask: 0.0120 (0.0120)  time: 1.0371  data: 0.8453  max mem: 4132
[19:02:52.470456] Epoch: [41]  [ 20/781]  eta: 0:02:37  lr: 0.000172  training_loss: 1.4021 (1.4132)  classification_loss: 1.3926 (1.4037)  loss_mask: 0.0089 (0.0096)  time: 0.1659  data: 0.0002  max mem: 4132
[19:02:55.783451] Epoch: [41]  [ 40/781]  eta: 0:02:18  lr: 0.000172  training_loss: 1.4258 (1.4216)  classification_loss: 1.4190 (1.4130)  loss_mask: 0.0074 (0.0086)  time: 0.1656  data: 0.0003  max mem: 4132
[19:02:59.076995] Epoch: [41]  [ 60/781]  eta: 0:02:09  lr: 0.000171  training_loss: 1.4454 (1.4300)  classification_loss: 1.4420 (1.4225)  loss_mask: 0.0043 (0.0074)  time: 0.1646  data: 0.0002  max mem: 4132
[19:03:02.361251] Epoch: [41]  [ 80/781]  eta: 0:02:03  lr: 0.000171  training_loss: 1.4140 (1.4276)  classification_loss: 1.4087 (1.4208)  loss_mask: 0.0038 (0.0068)  time: 0.1641  data: 0.0003  max mem: 4132
[19:03:05.641407] Epoch: [41]  [100/781]  eta: 0:01:58  lr: 0.000171  training_loss: 1.4664 (1.4291)  classification_loss: 1.4611 (1.4224)  loss_mask: 0.0040 (0.0066)  time: 0.1639  data: 0.0003  max mem: 4132
[19:03:08.937478] Epoch: [41]  [120/781]  eta: 0:01:53  lr: 0.000171  training_loss: 1.3922 (1.4253)  classification_loss: 1.3911 (1.4193)  loss_mask: 0.0023 (0.0060)  time: 0.1647  data: 0.0003  max mem: 4132
[19:03:12.274993] Epoch: [41]  [140/781]  eta: 0:01:49  lr: 0.000171  training_loss: 1.4189 (1.4234)  classification_loss: 1.4170 (1.4180)  loss_mask: 0.0016 (0.0054)  time: 0.1668  data: 0.0003  max mem: 4132
[19:03:15.630014] Epoch: [41]  [160/781]  eta: 0:01:46  lr: 0.000171  training_loss: 1.4256 (1.4231)  classification_loss: 1.4223 (1.4182)  loss_mask: 0.0014 (0.0049)  time: 0.1677  data: 0.0003  max mem: 4132
[19:03:18.977428] Epoch: [41]  [180/781]  eta: 0:01:42  lr: 0.000171  training_loss: 1.3259 (1.4161)  classification_loss: 1.3239 (1.4116)  loss_mask: 0.0012 (0.0045)  time: 0.1672  data: 0.0003  max mem: 4132
[19:03:22.286567] Epoch: [41]  [200/781]  eta: 0:01:38  lr: 0.000171  training_loss: 1.4290 (1.4193)  classification_loss: 1.4285 (1.4151)  loss_mask: 0.0011 (0.0042)  time: 0.1653  data: 0.0003  max mem: 4132
[19:03:25.621097] Epoch: [41]  [220/781]  eta: 0:01:35  lr: 0.000171  training_loss: 1.4054 (1.4196)  classification_loss: 1.4044 (1.4157)  loss_mask: 0.0010 (0.0039)  time: 0.1666  data: 0.0005  max mem: 4132
[19:03:28.977956] Epoch: [41]  [240/781]  eta: 0:01:31  lr: 0.000171  training_loss: 1.4263 (1.4189)  classification_loss: 1.4112 (1.4144)  loss_mask: 0.0009 (0.0045)  time: 0.1677  data: 0.0004  max mem: 4132
[19:03:32.334065] Epoch: [41]  [260/781]  eta: 0:01:28  lr: 0.000170  training_loss: 1.4603 (1.4219)  classification_loss: 1.4328 (1.4145)  loss_mask: 0.0320 (0.0074)  time: 0.1677  data: 0.0002  max mem: 4132
[19:03:35.639154] Epoch: [41]  [280/781]  eta: 0:01:24  lr: 0.000170  training_loss: 1.5152 (1.4288)  classification_loss: 1.4190 (1.4149)  loss_mask: 0.0907 (0.0139)  time: 0.1651  data: 0.0002  max mem: 4132
[19:03:38.940890] Epoch: [41]  [300/781]  eta: 0:01:21  lr: 0.000170  training_loss: 1.4694 (1.4333)  classification_loss: 1.3986 (1.4139)  loss_mask: 0.0659 (0.0194)  time: 0.1650  data: 0.0003  max mem: 4132
[19:03:42.210617] Epoch: [41]  [320/781]  eta: 0:01:17  lr: 0.000170  training_loss: 1.4716 (1.4375)  classification_loss: 1.4210 (1.4155)  loss_mask: 0.0369 (0.0220)  time: 0.1634  data: 0.0003  max mem: 4132
[19:03:45.510451] Epoch: [41]  [340/781]  eta: 0:01:14  lr: 0.000170  training_loss: 1.4201 (1.4386)  classification_loss: 1.4048 (1.4172)  loss_mask: 0.0111 (0.0215)  time: 0.1649  data: 0.0004  max mem: 4132
[19:03:48.892589] Epoch: [41]  [360/781]  eta: 0:01:10  lr: 0.000170  training_loss: 1.4180 (1.4384)  classification_loss: 1.4097 (1.4176)  loss_mask: 0.0081 (0.0207)  time: 0.1690  data: 0.0003  max mem: 4132
[19:03:52.221259] Epoch: [41]  [380/781]  eta: 0:01:07  lr: 0.000170  training_loss: 1.4427 (1.4380)  classification_loss: 1.4394 (1.4181)  loss_mask: 0.0046 (0.0199)  time: 0.1663  data: 0.0003  max mem: 4132
[19:03:55.506168] Epoch: [41]  [400/781]  eta: 0:01:03  lr: 0.000170  training_loss: 1.4357 (1.4381)  classification_loss: 1.4332 (1.4191)  loss_mask: 0.0028 (0.0191)  time: 0.1642  data: 0.0003  max mem: 4132
[19:03:58.843174] Epoch: [41]  [420/781]  eta: 0:01:00  lr: 0.000170  training_loss: 1.4278 (1.4370)  classification_loss: 1.4242 (1.4187)  loss_mask: 0.0029 (0.0183)  time: 0.1667  data: 0.0003  max mem: 4132
[19:04:02.193408] Epoch: [41]  [440/781]  eta: 0:00:57  lr: 0.000170  training_loss: 1.4120 (1.4362)  classification_loss: 1.4097 (1.4187)  loss_mask: 0.0017 (0.0176)  time: 0.1674  data: 0.0003  max mem: 4132
[19:04:05.491303] Epoch: [41]  [460/781]  eta: 0:00:53  lr: 0.000169  training_loss: 1.3824 (1.4338)  classification_loss: 1.3803 (1.4169)  loss_mask: 0.0018 (0.0169)  time: 0.1647  data: 0.0002  max mem: 4132
[19:04:08.769881] Epoch: [41]  [480/781]  eta: 0:00:50  lr: 0.000169  training_loss: 1.4501 (1.4346)  classification_loss: 1.4483 (1.4184)  loss_mask: 0.0015 (0.0163)  time: 0.1638  data: 0.0002  max mem: 4132
[19:04:12.064815] Epoch: [41]  [500/781]  eta: 0:00:47  lr: 0.000169  training_loss: 1.4379 (1.4350)  classification_loss: 1.4363 (1.4193)  loss_mask: 0.0014 (0.0157)  time: 0.1646  data: 0.0002  max mem: 4132
[19:04:15.361622] Epoch: [41]  [520/781]  eta: 0:00:43  lr: 0.000169  training_loss: 1.3873 (1.4338)  classification_loss: 1.3852 (1.4187)  loss_mask: 0.0011 (0.0151)  time: 0.1647  data: 0.0003  max mem: 4132
[19:04:18.660172] Epoch: [41]  [540/781]  eta: 0:00:40  lr: 0.000169  training_loss: 1.4375 (1.4345)  classification_loss: 1.4371 (1.4199)  loss_mask: 0.0009 (0.0146)  time: 0.1648  data: 0.0003  max mem: 4132
[19:04:21.953139] Epoch: [41]  [560/781]  eta: 0:00:36  lr: 0.000169  training_loss: 1.4501 (1.4346)  classification_loss: 1.4487 (1.4205)  loss_mask: 0.0014 (0.0142)  time: 0.1645  data: 0.0003  max mem: 4132
[19:04:25.246629] Epoch: [41]  [580/781]  eta: 0:00:33  lr: 0.000169  training_loss: 1.4939 (1.4377)  classification_loss: 1.4201 (1.4207)  loss_mask: 0.0372 (0.0170)  time: 0.1646  data: 0.0003  max mem: 4132
[19:04:28.528437] Epoch: [41]  [600/781]  eta: 0:00:30  lr: 0.000169  training_loss: 1.5190 (1.4411)  classification_loss: 1.4214 (1.4204)  loss_mask: 0.0676 (0.0207)  time: 0.1640  data: 0.0003  max mem: 4132
[19:04:31.829146] Epoch: [41]  [620/781]  eta: 0:00:26  lr: 0.000169  training_loss: 1.5053 (1.4437)  classification_loss: 1.4157 (1.4207)  loss_mask: 0.0521 (0.0230)  time: 0.1649  data: 0.0002  max mem: 4132
[19:04:35.132024] Epoch: [41]  [640/781]  eta: 0:00:23  lr: 0.000169  training_loss: 1.4370 (1.4431)  classification_loss: 1.4101 (1.4199)  loss_mask: 0.0245 (0.0232)  time: 0.1650  data: 0.0002  max mem: 4132
[19:04:38.424660] Epoch: [41]  [660/781]  eta: 0:00:20  lr: 0.000168  training_loss: 1.4216 (1.4425)  classification_loss: 1.4087 (1.4198)  loss_mask: 0.0088 (0.0227)  time: 0.1645  data: 0.0003  max mem: 4132
[19:04:41.693048] Epoch: [41]  [680/781]  eta: 0:00:16  lr: 0.000168  training_loss: 1.4151 (1.4426)  classification_loss: 1.4088 (1.4204)  loss_mask: 0.0059 (0.0223)  time: 0.1632  data: 0.0002  max mem: 4132
[19:04:44.995662] Epoch: [41]  [700/781]  eta: 0:00:13  lr: 0.000168  training_loss: 1.3817 (1.4420)  classification_loss: 1.3776 (1.4203)  loss_mask: 0.0048 (0.0218)  time: 0.1650  data: 0.0002  max mem: 4132
[19:04:48.302008] Epoch: [41]  [720/781]  eta: 0:00:10  lr: 0.000168  training_loss: 1.3864 (1.4415)  classification_loss: 1.3833 (1.4202)  loss_mask: 0.0029 (0.0212)  time: 0.1652  data: 0.0003  max mem: 4132
[19:04:51.614600] Epoch: [41]  [740/781]  eta: 0:00:06  lr: 0.000168  training_loss: 1.4082 (1.4408)  classification_loss: 1.4058 (1.4201)  loss_mask: 0.0024 (0.0207)  time: 0.1655  data: 0.0004  max mem: 4132
[19:04:54.961917] Epoch: [41]  [760/781]  eta: 0:00:03  lr: 0.000168  training_loss: 1.4451 (1.4411)  classification_loss: 1.4375 (1.4208)  loss_mask: 0.0024 (0.0203)  time: 0.1673  data: 0.0003  max mem: 4132
[19:04:58.265822] Epoch: [41]  [780/781]  eta: 0:00:00  lr: 0.000168  training_loss: 1.4381 (1.4412)  classification_loss: 1.4367 (1.4213)  loss_mask: 0.0026 (0.0199)  time: 0.1650  data: 0.0002  max mem: 4132
[19:04:58.476807] Epoch: [41] Total time: 0:02:10 (0.1669 s / it)
[19:04:58.477320] Averaged stats: lr: 0.000168  training_loss: 1.4381 (1.4412)  classification_loss: 1.4367 (1.4213)  loss_mask: 0.0026 (0.0199)
[19:04:59.240598] Test:  [  0/157]  eta: 0:01:58  testing_loss: 0.6201 (0.6201)  acc1: 82.8125 (82.8125)  acc5: 98.4375 (98.4375)  time: 0.7577  data: 0.7015  max mem: 4132
[19:04:59.576826] Test:  [ 10/157]  eta: 0:00:14  testing_loss: 0.6698 (0.7102)  acc1: 76.5625 (76.9886)  acc5: 100.0000 (99.1477)  time: 0.0990  data: 0.0642  max mem: 4132
[19:04:59.874484] Test:  [ 20/157]  eta: 0:00:09  testing_loss: 0.6455 (0.6849)  acc1: 78.1250 (77.8274)  acc5: 98.4375 (99.0327)  time: 0.0313  data: 0.0004  max mem: 4132
[19:05:00.172747] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6470 (0.6983)  acc1: 78.1250 (76.9657)  acc5: 98.4375 (98.8911)  time: 0.0296  data: 0.0003  max mem: 4132
[19:05:00.475076] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6747 (0.7002)  acc1: 76.5625 (76.7530)  acc5: 98.4375 (98.8186)  time: 0.0298  data: 0.0003  max mem: 4132
[19:05:00.772643] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6728 (0.6931)  acc1: 76.5625 (77.2059)  acc5: 98.4375 (98.8051)  time: 0.0298  data: 0.0002  max mem: 4132
[19:05:01.085646] Test:  [ 60/157]  eta: 0:00:04  testing_loss: 0.6654 (0.6937)  acc1: 76.5625 (77.0236)  acc5: 98.4375 (98.7449)  time: 0.0304  data: 0.0003  max mem: 4132
[19:05:01.381120] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6444 (0.6867)  acc1: 76.5625 (77.1787)  acc5: 98.4375 (98.7676)  time: 0.0302  data: 0.0003  max mem: 4132
[19:05:01.678767] Test:  [ 80/157]  eta: 0:00:03  testing_loss: 0.6683 (0.6957)  acc1: 76.5625 (76.7361)  acc5: 98.4375 (98.6883)  time: 0.0294  data: 0.0003  max mem: 4132
[19:05:01.972722] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6842 (0.6950)  acc1: 76.5625 (76.8372)  acc5: 100.0000 (98.7294)  time: 0.0294  data: 0.0003  max mem: 4132
[19:05:02.279491] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.7029 (0.7007)  acc1: 76.5625 (76.5470)  acc5: 100.0000 (98.7624)  time: 0.0299  data: 0.0003  max mem: 4132
[19:05:02.577970] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.7159 (0.6985)  acc1: 75.0000 (76.7033)  acc5: 98.4375 (98.7613)  time: 0.0301  data: 0.0002  max mem: 4132
[19:05:02.868331] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6516 (0.6937)  acc1: 78.1250 (76.9886)  acc5: 98.4375 (98.7474)  time: 0.0293  data: 0.0002  max mem: 4132
[19:05:03.157495] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6500 (0.6946)  acc1: 76.5625 (76.9680)  acc5: 98.4375 (98.7715)  time: 0.0288  data: 0.0002  max mem: 4132
[19:05:03.446426] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7277 (0.6958)  acc1: 76.5625 (76.8728)  acc5: 98.4375 (98.8032)  time: 0.0288  data: 0.0002  max mem: 4132
[19:05:03.732249] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7134 (0.6947)  acc1: 76.5625 (76.9557)  acc5: 98.4375 (98.8100)  time: 0.0286  data: 0.0002  max mem: 4132
[19:05:03.886559] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7118 (0.6953)  acc1: 78.1250 (76.9800)  acc5: 98.4375 (98.8300)  time: 0.0275  data: 0.0002  max mem: 4132
[19:05:04.066360] Test: Total time: 0:00:05 (0.0356 s / it)
[19:05:04.066848] * Acc@1 76.980 Acc@5 98.830 loss 0.695
[19:05:04.067172] Accuracy of the network on the 10000 test images: 77.0%
[19:05:04.067382] Max accuracy: 76.98%
[19:05:04.168732] log_dir: ./output_dir
[19:05:05.141451] Epoch: [42]  [  0/781]  eta: 0:12:38  lr: 0.000168  training_loss: 1.4022 (1.4022)  classification_loss: 1.3904 (1.3904)  loss_mask: 0.0118 (0.0118)  time: 0.9706  data: 0.7588  max mem: 4132
[19:05:08.494982] Epoch: [42]  [ 20/781]  eta: 0:02:36  lr: 0.000168  training_loss: 1.4359 (1.4340)  classification_loss: 1.3926 (1.4264)  loss_mask: 0.0041 (0.0076)  time: 0.1675  data: 0.0003  max mem: 4132
[19:05:11.837594] Epoch: [42]  [ 40/781]  eta: 0:02:18  lr: 0.000168  training_loss: 1.5703 (1.5021)  classification_loss: 1.4498 (1.4362)  loss_mask: 0.0923 (0.0659)  time: 0.1670  data: 0.0003  max mem: 4132
[19:05:15.177548] Epoch: [42]  [ 60/781]  eta: 0:02:10  lr: 0.000168  training_loss: 1.5100 (1.4975)  classification_loss: 1.4622 (1.4424)  loss_mask: 0.0338 (0.0551)  time: 0.1669  data: 0.0003  max mem: 4132
[19:05:18.459547] Epoch: [42]  [ 80/781]  eta: 0:02:03  lr: 0.000167  training_loss: 1.4260 (1.4784)  classification_loss: 1.3717 (1.4252)  loss_mask: 0.0261 (0.0532)  time: 0.1639  data: 0.0002  max mem: 4132
[19:05:21.718637] Epoch: [42]  [100/781]  eta: 0:01:58  lr: 0.000167  training_loss: 1.5493 (1.4900)  classification_loss: 1.4919 (1.4399)  loss_mask: 0.0265 (0.0501)  time: 0.1629  data: 0.0002  max mem: 4132
[19:05:25.028290] Epoch: [42]  [120/781]  eta: 0:01:53  lr: 0.000167  training_loss: 1.4191 (1.4771)  classification_loss: 1.3853 (1.4326)  loss_mask: 0.0123 (0.0445)  time: 0.1654  data: 0.0003  max mem: 4132
[19:05:28.317682] Epoch: [42]  [140/781]  eta: 0:01:49  lr: 0.000167  training_loss: 1.4354 (1.4698)  classification_loss: 1.4249 (1.4302)  loss_mask: 0.0094 (0.0396)  time: 0.1644  data: 0.0003  max mem: 4132
[19:05:31.600644] Epoch: [42]  [160/781]  eta: 0:01:45  lr: 0.000167  training_loss: 1.4118 (1.4606)  classification_loss: 1.4069 (1.4252)  loss_mask: 0.0044 (0.0353)  time: 0.1641  data: 0.0003  max mem: 4132
[19:05:34.905281] Epoch: [42]  [180/781]  eta: 0:01:41  lr: 0.000167  training_loss: 1.4108 (1.4559)  classification_loss: 1.4071 (1.4241)  loss_mask: 0.0030 (0.0318)  time: 0.1651  data: 0.0003  max mem: 4132
[19:05:38.214021] Epoch: [42]  [200/781]  eta: 0:01:38  lr: 0.000167  training_loss: 1.3450 (1.4489)  classification_loss: 1.3428 (1.4200)  loss_mask: 0.0023 (0.0289)  time: 0.1654  data: 0.0002  max mem: 4132
[19:05:41.508156] Epoch: [42]  [220/781]  eta: 0:01:34  lr: 0.000167  training_loss: 1.4058 (1.4447)  classification_loss: 1.4030 (1.4176)  loss_mask: 0.0052 (0.0271)  time: 0.1646  data: 0.0002  max mem: 4132
[19:05:44.843003] Epoch: [42]  [240/781]  eta: 0:01:31  lr: 0.000167  training_loss: 1.3484 (1.4422)  classification_loss: 1.3468 (1.4169)  loss_mask: 0.0041 (0.0253)  time: 0.1666  data: 0.0002  max mem: 4132
[19:05:48.160682] Epoch: [42]  [260/781]  eta: 0:01:27  lr: 0.000167  training_loss: 1.3847 (1.4400)  classification_loss: 1.3835 (1.4164)  loss_mask: 0.0016 (0.0235)  time: 0.1658  data: 0.0003  max mem: 4132
[19:05:51.451570] Epoch: [42]  [280/781]  eta: 0:01:24  lr: 0.000166  training_loss: 1.3956 (1.4374)  classification_loss: 1.3941 (1.4153)  loss_mask: 0.0021 (0.0221)  time: 0.1645  data: 0.0003  max mem: 4132
[19:05:54.751055] Epoch: [42]  [300/781]  eta: 0:01:20  lr: 0.000166  training_loss: 1.4795 (1.4411)  classification_loss: 1.4184 (1.4167)  loss_mask: 0.0215 (0.0244)  time: 0.1649  data: 0.0003  max mem: 4132
[19:05:58.018274] Epoch: [42]  [320/781]  eta: 0:01:17  lr: 0.000166  training_loss: 1.4406 (1.4428)  classification_loss: 1.4081 (1.4168)  loss_mask: 0.0367 (0.0260)  time: 0.1633  data: 0.0002  max mem: 4132
[19:06:01.324307] Epoch: [42]  [340/781]  eta: 0:01:13  lr: 0.000166  training_loss: 1.4020 (1.4418)  classification_loss: 1.3783 (1.4161)  loss_mask: 0.0124 (0.0257)  time: 0.1652  data: 0.0003  max mem: 4132
[19:06:04.613862] Epoch: [42]  [360/781]  eta: 0:01:10  lr: 0.000166  training_loss: 1.4310 (1.4428)  classification_loss: 1.4176 (1.4174)  loss_mask: 0.0101 (0.0254)  time: 0.1643  data: 0.0002  max mem: 4132
[19:06:07.905286] Epoch: [42]  [380/781]  eta: 0:01:07  lr: 0.000166  training_loss: 1.3945 (1.4427)  classification_loss: 1.3895 (1.4181)  loss_mask: 0.0059 (0.0246)  time: 0.1645  data: 0.0005  max mem: 4132
[19:06:11.177251] Epoch: [42]  [400/781]  eta: 0:01:03  lr: 0.000166  training_loss: 1.4371 (1.4416)  classification_loss: 1.4337 (1.4177)  loss_mask: 0.0034 (0.0239)  time: 0.1635  data: 0.0003  max mem: 4132
[19:06:14.515021] Epoch: [42]  [420/781]  eta: 0:01:00  lr: 0.000166  training_loss: 1.4159 (1.4405)  classification_loss: 1.4126 (1.4171)  loss_mask: 0.0084 (0.0233)  time: 0.1668  data: 0.0004  max mem: 4132
[19:06:17.788923] Epoch: [42]  [440/781]  eta: 0:00:56  lr: 0.000166  training_loss: 1.4751 (1.4437)  classification_loss: 1.4388 (1.4185)  loss_mask: 0.0293 (0.0252)  time: 0.1636  data: 0.0002  max mem: 4132
[19:06:21.081671] Epoch: [42]  [460/781]  eta: 0:00:53  lr: 0.000166  training_loss: 1.3684 (1.4417)  classification_loss: 1.3555 (1.4166)  loss_mask: 0.0134 (0.0251)  time: 0.1645  data: 0.0002  max mem: 4132
[19:06:24.350671] Epoch: [42]  [480/781]  eta: 0:00:50  lr: 0.000165  training_loss: 1.4920 (1.4433)  classification_loss: 1.3952 (1.4171)  loss_mask: 0.0406 (0.0262)  time: 0.1634  data: 0.0003  max mem: 4132
[19:06:27.603491] Epoch: [42]  [500/781]  eta: 0:00:46  lr: 0.000165  training_loss: 1.4737 (1.4441)  classification_loss: 1.4498 (1.4182)  loss_mask: 0.0133 (0.0259)  time: 0.1626  data: 0.0002  max mem: 4132
[19:06:30.892735] Epoch: [42]  [520/781]  eta: 0:00:43  lr: 0.000165  training_loss: 1.4420 (1.4442)  classification_loss: 1.4115 (1.4187)  loss_mask: 0.0111 (0.0256)  time: 0.1644  data: 0.0002  max mem: 4132
[19:06:34.192793] Epoch: [42]  [540/781]  eta: 0:00:40  lr: 0.000165  training_loss: 1.4281 (1.4438)  classification_loss: 1.4210 (1.4188)  loss_mask: 0.0048 (0.0249)  time: 0.1649  data: 0.0003  max mem: 4132
[19:06:37.527432] Epoch: [42]  [560/781]  eta: 0:00:36  lr: 0.000165  training_loss: 1.4034 (1.4417)  classification_loss: 1.4004 (1.4175)  loss_mask: 0.0028 (0.0242)  time: 0.1666  data: 0.0003  max mem: 4132
[19:06:40.841189] Epoch: [42]  [580/781]  eta: 0:00:33  lr: 0.000165  training_loss: 1.4086 (1.4402)  classification_loss: 1.4061 (1.4168)  loss_mask: 0.0023 (0.0234)  time: 0.1656  data: 0.0002  max mem: 4132
[19:06:44.163747] Epoch: [42]  [600/781]  eta: 0:00:30  lr: 0.000165  training_loss: 1.3637 (1.4381)  classification_loss: 1.3625 (1.4154)  loss_mask: 0.0017 (0.0227)  time: 0.1660  data: 0.0003  max mem: 4132
[19:06:47.476962] Epoch: [42]  [620/781]  eta: 0:00:26  lr: 0.000165  training_loss: 1.4445 (1.4385)  classification_loss: 1.4433 (1.4165)  loss_mask: 0.0011 (0.0220)  time: 0.1655  data: 0.0003  max mem: 4132
[19:06:50.827420] Epoch: [42]  [640/781]  eta: 0:00:23  lr: 0.000165  training_loss: 1.3976 (1.4383)  classification_loss: 1.3961 (1.4170)  loss_mask: 0.0011 (0.0213)  time: 0.1674  data: 0.0004  max mem: 4132
[19:06:54.156520] Epoch: [42]  [660/781]  eta: 0:00:20  lr: 0.000165  training_loss: 1.3984 (1.4377)  classification_loss: 1.3977 (1.4170)  loss_mask: 0.0009 (0.0207)  time: 0.1664  data: 0.0003  max mem: 4132
[19:06:57.423725] Epoch: [42]  [680/781]  eta: 0:00:16  lr: 0.000164  training_loss: 1.3899 (1.4362)  classification_loss: 1.3877 (1.4161)  loss_mask: 0.0010 (0.0202)  time: 0.1633  data: 0.0002  max mem: 4132
[19:07:00.736152] Epoch: [42]  [700/781]  eta: 0:00:13  lr: 0.000164  training_loss: 1.4075 (1.4361)  classification_loss: 1.4065 (1.4165)  loss_mask: 0.0012 (0.0196)  time: 0.1655  data: 0.0003  max mem: 4132
[19:07:04.019050] Epoch: [42]  [720/781]  eta: 0:00:10  lr: 0.000164  training_loss: 1.4137 (1.4357)  classification_loss: 1.4125 (1.4165)  loss_mask: 0.0012 (0.0191)  time: 0.1641  data: 0.0003  max mem: 4132
[19:07:07.310328] Epoch: [42]  [740/781]  eta: 0:00:06  lr: 0.000164  training_loss: 1.3949 (1.4347)  classification_loss: 1.3936 (1.4161)  loss_mask: 0.0010 (0.0186)  time: 0.1645  data: 0.0003  max mem: 4132
[19:07:10.599961] Epoch: [42]  [760/781]  eta: 0:00:03  lr: 0.000164  training_loss: 1.4463 (1.4350)  classification_loss: 1.4454 (1.4168)  loss_mask: 0.0009 (0.0182)  time: 0.1644  data: 0.0004  max mem: 4132
[19:07:13.869824] Epoch: [42]  [780/781]  eta: 0:00:00  lr: 0.000164  training_loss: 1.3860 (1.4340)  classification_loss: 1.3853 (1.4163)  loss_mask: 0.0007 (0.0177)  time: 0.1634  data: 0.0002  max mem: 4132
[19:07:14.057645] Epoch: [42] Total time: 0:02:09 (0.1663 s / it)
[19:07:14.058433] Averaged stats: lr: 0.000164  training_loss: 1.3860 (1.4340)  classification_loss: 1.3853 (1.4163)  loss_mask: 0.0007 (0.0177)
[19:07:14.764635] Test:  [  0/157]  eta: 0:01:49  testing_loss: 0.6601 (0.6601)  acc1: 79.6875 (79.6875)  acc5: 96.8750 (96.8750)  time: 0.6999  data: 0.6702  max mem: 4132
[19:07:15.064559] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.7349 (0.7310)  acc1: 76.5625 (76.2784)  acc5: 98.4375 (98.8636)  time: 0.0906  data: 0.0612  max mem: 4132
[19:07:15.357486] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.6786 (0.7057)  acc1: 76.5625 (77.2321)  acc5: 98.4375 (98.7351)  time: 0.0294  data: 0.0002  max mem: 4132
[19:07:15.646390] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6861 (0.7173)  acc1: 76.5625 (76.7137)  acc5: 98.4375 (98.7903)  time: 0.0289  data: 0.0003  max mem: 4132
[19:07:15.934352] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.7024 (0.7181)  acc1: 75.0000 (76.9055)  acc5: 98.4375 (98.7043)  time: 0.0287  data: 0.0003  max mem: 4132
[19:07:16.221725] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6865 (0.7138)  acc1: 78.1250 (76.9914)  acc5: 98.4375 (98.6520)  time: 0.0286  data: 0.0002  max mem: 4132
[19:07:16.508183] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6831 (0.7147)  acc1: 76.5625 (76.9980)  acc5: 98.4375 (98.6424)  time: 0.0285  data: 0.0002  max mem: 4132
[19:07:16.795104] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6521 (0.7027)  acc1: 79.6875 (77.6629)  acc5: 98.4375 (98.6796)  time: 0.0285  data: 0.0002  max mem: 4132
[19:07:17.088938] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6521 (0.7109)  acc1: 78.1250 (77.2569)  acc5: 98.4375 (98.5725)  time: 0.0289  data: 0.0003  max mem: 4132
[19:07:17.384144] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7264 (0.7101)  acc1: 75.0000 (77.1291)  acc5: 98.4375 (98.6435)  time: 0.0293  data: 0.0003  max mem: 4132
[19:07:17.673061] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.7264 (0.7157)  acc1: 76.5625 (76.9647)  acc5: 100.0000 (98.6696)  time: 0.0290  data: 0.0002  max mem: 4132
[19:07:17.967086] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.7285 (0.7146)  acc1: 78.1250 (77.1256)  acc5: 100.0000 (98.7050)  time: 0.0290  data: 0.0002  max mem: 4132
[19:07:18.257208] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6858 (0.7111)  acc1: 79.6875 (77.2469)  acc5: 98.4375 (98.7087)  time: 0.0290  data: 0.0002  max mem: 4132
[19:07:18.544570] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6915 (0.7097)  acc1: 78.1250 (77.2662)  acc5: 98.4375 (98.7238)  time: 0.0287  data: 0.0002  max mem: 4132
[19:07:18.829903] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7221 (0.7113)  acc1: 76.5625 (77.1831)  acc5: 98.4375 (98.7589)  time: 0.0285  data: 0.0002  max mem: 4132
[19:07:19.114902] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7240 (0.7090)  acc1: 75.0000 (77.2041)  acc5: 100.0000 (98.7583)  time: 0.0284  data: 0.0002  max mem: 4132
[19:07:19.270199] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6880 (0.7104)  acc1: 76.5625 (77.1700)  acc5: 98.4375 (98.7500)  time: 0.0275  data: 0.0002  max mem: 4132
[19:07:19.448682] Test: Total time: 0:00:05 (0.0343 s / it)
[19:07:19.449171] * Acc@1 77.170 Acc@5 98.750 loss 0.710
[19:07:19.449514] Accuracy of the network on the 10000 test images: 77.2%
[19:07:19.449672] Max accuracy: 77.17%
[19:07:19.611179] log_dir: ./output_dir
[19:07:20.512941] Epoch: [43]  [  0/781]  eta: 0:11:42  lr: 0.000164  training_loss: 1.3590 (1.3590)  classification_loss: 1.3586 (1.3586)  loss_mask: 0.0004 (0.0004)  time: 0.8997  data: 0.7061  max mem: 4132
[19:07:23.828709] Epoch: [43]  [ 20/781]  eta: 0:02:32  lr: 0.000164  training_loss: 1.3289 (1.3722)  classification_loss: 1.3279 (1.3716)  loss_mask: 0.0006 (0.0006)  time: 0.1657  data: 0.0002  max mem: 4132
[19:07:27.114214] Epoch: [43]  [ 40/781]  eta: 0:02:15  lr: 0.000164  training_loss: 1.4026 (1.3917)  classification_loss: 1.4016 (1.3910)  loss_mask: 0.0006 (0.0007)  time: 0.1642  data: 0.0004  max mem: 4132
[19:07:30.435724] Epoch: [43]  [ 60/781]  eta: 0:02:07  lr: 0.000164  training_loss: 1.4573 (1.4190)  classification_loss: 1.4570 (1.4153)  loss_mask: 0.0008 (0.0037)  time: 0.1660  data: 0.0003  max mem: 4132
[19:07:33.724851] Epoch: [43]  [ 80/781]  eta: 0:02:02  lr: 0.000164  training_loss: 1.4346 (1.4341)  classification_loss: 1.3794 (1.4081)  loss_mask: 0.0603 (0.0261)  time: 0.1643  data: 0.0004  max mem: 4132
[19:07:37.014129] Epoch: [43]  [100/781]  eta: 0:01:57  lr: 0.000163  training_loss: 1.4682 (1.4415)  classification_loss: 1.4443 (1.4128)  loss_mask: 0.0177 (0.0288)  time: 0.1644  data: 0.0003  max mem: 4132
[19:07:40.316167] Epoch: [43]  [120/781]  eta: 0:01:53  lr: 0.000163  training_loss: 1.4407 (1.4416)  classification_loss: 1.3850 (1.4095)  loss_mask: 0.0237 (0.0321)  time: 0.1650  data: 0.0003  max mem: 4132
[19:07:43.605889] Epoch: [43]  [140/781]  eta: 0:01:49  lr: 0.000163  training_loss: 1.4717 (1.4427)  classification_loss: 1.4422 (1.4106)  loss_mask: 0.0185 (0.0322)  time: 0.1644  data: 0.0003  max mem: 4132
[19:07:46.908243] Epoch: [43]  [160/781]  eta: 0:01:45  lr: 0.000163  training_loss: 1.3627 (1.4340)  classification_loss: 1.3432 (1.4042)  loss_mask: 0.0116 (0.0297)  time: 0.1650  data: 0.0003  max mem: 4132
[19:07:50.174219] Epoch: [43]  [180/781]  eta: 0:01:41  lr: 0.000163  training_loss: 1.3687 (1.4303)  classification_loss: 1.3570 (1.4030)  loss_mask: 0.0047 (0.0274)  time: 0.1631  data: 0.0002  max mem: 4132
[19:07:53.477995] Epoch: [43]  [200/781]  eta: 0:01:37  lr: 0.000163  training_loss: 1.4519 (1.4279)  classification_loss: 1.4447 (1.4022)  loss_mask: 0.0037 (0.0258)  time: 0.1651  data: 0.0002  max mem: 4132
[19:07:56.760111] Epoch: [43]  [220/781]  eta: 0:01:34  lr: 0.000163  training_loss: 1.4530 (1.4272)  classification_loss: 1.4512 (1.4032)  loss_mask: 0.0044 (0.0240)  time: 0.1640  data: 0.0003  max mem: 4132
[19:08:00.022990] Epoch: [43]  [240/781]  eta: 0:01:30  lr: 0.000163  training_loss: 1.3974 (1.4270)  classification_loss: 1.3883 (1.4042)  loss_mask: 0.0057 (0.0229)  time: 0.1631  data: 0.0003  max mem: 4132
[19:08:03.299609] Epoch: [43]  [260/781]  eta: 0:01:27  lr: 0.000163  training_loss: 1.4663 (1.4287)  classification_loss: 1.4661 (1.4070)  loss_mask: 0.0045 (0.0216)  time: 0.1637  data: 0.0003  max mem: 4132
[19:08:06.585810] Epoch: [43]  [280/781]  eta: 0:01:23  lr: 0.000163  training_loss: 1.3765 (1.4272)  classification_loss: 1.3619 (1.4067)  loss_mask: 0.0027 (0.0205)  time: 0.1642  data: 0.0002  max mem: 4132
[19:08:09.874713] Epoch: [43]  [300/781]  eta: 0:01:20  lr: 0.000162  training_loss: 1.3986 (1.4267)  classification_loss: 1.3978 (1.4075)  loss_mask: 0.0014 (0.0192)  time: 0.1644  data: 0.0002  max mem: 4132
[19:08:13.171758] Epoch: [43]  [320/781]  eta: 0:01:16  lr: 0.000162  training_loss: 1.3842 (1.4254)  classification_loss: 1.3832 (1.4069)  loss_mask: 0.0015 (0.0185)  time: 0.1647  data: 0.0003  max mem: 4132
[19:08:16.501098] Epoch: [43]  [340/781]  eta: 0:01:13  lr: 0.000162  training_loss: 1.4444 (1.4325)  classification_loss: 1.3468 (1.4054)  loss_mask: 0.0598 (0.0271)  time: 0.1664  data: 0.0002  max mem: 4132
[19:08:19.847020] Epoch: [43]  [360/781]  eta: 0:01:10  lr: 0.000162  training_loss: 1.5505 (1.4388)  classification_loss: 1.4011 (1.4067)  loss_mask: 0.0804 (0.0321)  time: 0.1672  data: 0.0003  max mem: 4132
[19:08:23.154847] Epoch: [43]  [380/781]  eta: 0:01:06  lr: 0.000162  training_loss: 1.4577 (1.4401)  classification_loss: 1.3794 (1.4062)  loss_mask: 0.0563 (0.0340)  time: 0.1653  data: 0.0003  max mem: 4132
[19:08:26.455391] Epoch: [43]  [400/781]  eta: 0:01:03  lr: 0.000162  training_loss: 1.4426 (1.4408)  classification_loss: 1.4074 (1.4070)  loss_mask: 0.0241 (0.0337)  time: 0.1649  data: 0.0003  max mem: 4132
[19:08:29.739563] Epoch: [43]  [420/781]  eta: 0:01:00  lr: 0.000162  training_loss: 1.4144 (1.4393)  classification_loss: 1.4048 (1.4064)  loss_mask: 0.0147 (0.0330)  time: 0.1641  data: 0.0003  max mem: 4132
[19:08:33.045925] Epoch: [43]  [440/781]  eta: 0:00:56  lr: 0.000162  training_loss: 1.3875 (1.4373)  classification_loss: 1.3815 (1.4055)  loss_mask: 0.0075 (0.0318)  time: 0.1652  data: 0.0004  max mem: 4132
[19:08:36.367478] Epoch: [43]  [460/781]  eta: 0:00:53  lr: 0.000162  training_loss: 1.3363 (1.4339)  classification_loss: 1.3337 (1.4032)  loss_mask: 0.0036 (0.0306)  time: 0.1660  data: 0.0003  max mem: 4132
[19:08:39.654263] Epoch: [43]  [480/781]  eta: 0:00:50  lr: 0.000162  training_loss: 1.4506 (1.4347)  classification_loss: 1.4487 (1.4052)  loss_mask: 0.0035 (0.0295)  time: 0.1642  data: 0.0002  max mem: 4132
[19:08:42.995594] Epoch: [43]  [500/781]  eta: 0:00:46  lr: 0.000161  training_loss: 1.3895 (1.4342)  classification_loss: 1.3878 (1.4057)  loss_mask: 0.0027 (0.0285)  time: 0.1670  data: 0.0003  max mem: 4132
[19:08:46.302111] Epoch: [43]  [520/781]  eta: 0:00:43  lr: 0.000161  training_loss: 1.4578 (1.4337)  classification_loss: 1.4570 (1.4063)  loss_mask: 0.0024 (0.0275)  time: 0.1652  data: 0.0004  max mem: 4132
[19:08:49.610510] Epoch: [43]  [540/781]  eta: 0:00:40  lr: 0.000161  training_loss: 1.3937 (1.4321)  classification_loss: 1.3903 (1.4056)  loss_mask: 0.0022 (0.0265)  time: 0.1653  data: 0.0003  max mem: 4132
[19:08:52.899865] Epoch: [43]  [560/781]  eta: 0:00:36  lr: 0.000161  training_loss: 1.3891 (1.4306)  classification_loss: 1.3853 (1.4049)  loss_mask: 0.0020 (0.0257)  time: 0.1644  data: 0.0003  max mem: 4132
[19:08:56.198372] Epoch: [43]  [580/781]  eta: 0:00:33  lr: 0.000161  training_loss: 1.4309 (1.4299)  classification_loss: 1.4301 (1.4050)  loss_mask: 0.0017 (0.0248)  time: 0.1648  data: 0.0003  max mem: 4132
[19:08:59.489309] Epoch: [43]  [600/781]  eta: 0:00:30  lr: 0.000161  training_loss: 1.4265 (1.4299)  classification_loss: 1.4248 (1.4058)  loss_mask: 0.0018 (0.0241)  time: 0.1644  data: 0.0002  max mem: 4132
[19:09:02.782209] Epoch: [43]  [620/781]  eta: 0:00:26  lr: 0.000161  training_loss: 1.3949 (1.4294)  classification_loss: 1.3881 (1.4058)  loss_mask: 0.0033 (0.0236)  time: 0.1645  data: 0.0002  max mem: 4132
[19:09:06.083714] Epoch: [43]  [640/781]  eta: 0:00:23  lr: 0.000161  training_loss: 1.3828 (1.4279)  classification_loss: 1.3800 (1.4050)  loss_mask: 0.0015 (0.0229)  time: 0.1650  data: 0.0002  max mem: 4132
[19:09:09.382644] Epoch: [43]  [660/781]  eta: 0:00:20  lr: 0.000161  training_loss: 1.4268 (1.4276)  classification_loss: 1.4244 (1.4054)  loss_mask: 0.0015 (0.0223)  time: 0.1648  data: 0.0003  max mem: 4132
[19:09:12.712769] Epoch: [43]  [680/781]  eta: 0:00:16  lr: 0.000161  training_loss: 1.3786 (1.4258)  classification_loss: 1.3774 (1.4041)  loss_mask: 0.0014 (0.0217)  time: 0.1664  data: 0.0002  max mem: 4132
[19:09:16.022555] Epoch: [43]  [700/781]  eta: 0:00:13  lr: 0.000160  training_loss: 1.4238 (1.4254)  classification_loss: 1.4233 (1.4043)  loss_mask: 0.0011 (0.0211)  time: 0.1654  data: 0.0002  max mem: 4132
[19:09:19.315013] Epoch: [43]  [720/781]  eta: 0:00:10  lr: 0.000160  training_loss: 1.3550 (1.4237)  classification_loss: 1.3544 (1.4032)  loss_mask: 0.0009 (0.0205)  time: 0.1645  data: 0.0002  max mem: 4132
[19:09:22.611600] Epoch: [43]  [740/781]  eta: 0:00:06  lr: 0.000160  training_loss: 1.3531 (1.4218)  classification_loss: 1.3525 (1.4018)  loss_mask: 0.0009 (0.0200)  time: 0.1647  data: 0.0002  max mem: 4132
[19:09:25.946948] Epoch: [43]  [760/781]  eta: 0:00:03  lr: 0.000160  training_loss: 1.4310 (1.4225)  classification_loss: 1.4301 (1.4030)  loss_mask: 0.0008 (0.0195)  time: 0.1666  data: 0.0003  max mem: 4132
[19:09:29.309400] Epoch: [43]  [780/781]  eta: 0:00:00  lr: 0.000160  training_loss: 1.3403 (1.4216)  classification_loss: 1.3395 (1.4025)  loss_mask: 0.0011 (0.0190)  time: 0.1680  data: 0.0002  max mem: 4132
[19:09:29.485196] Epoch: [43] Total time: 0:02:09 (0.1663 s / it)
[19:09:29.485955] Averaged stats: lr: 0.000160  training_loss: 1.3403 (1.4216)  classification_loss: 1.3395 (1.4025)  loss_mask: 0.0011 (0.0190)
[19:09:30.157881] Test:  [  0/157]  eta: 0:01:44  testing_loss: 0.6178 (0.6178)  acc1: 79.6875 (79.6875)  acc5: 100.0000 (100.0000)  time: 0.6649  data: 0.6349  max mem: 4132
[19:09:30.470975] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.7165 (0.7150)  acc1: 76.5625 (76.4205)  acc5: 100.0000 (99.2898)  time: 0.0886  data: 0.0599  max mem: 4132
[19:09:30.755613] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.6933 (0.6908)  acc1: 78.1250 (77.7530)  acc5: 100.0000 (99.1071)  time: 0.0297  data: 0.0013  max mem: 4132
[19:09:31.040984] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6960 (0.7060)  acc1: 78.1250 (77.0161)  acc5: 98.4375 (99.0423)  time: 0.0284  data: 0.0002  max mem: 4132
[19:09:31.328933] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6960 (0.7080)  acc1: 76.5625 (76.9436)  acc5: 98.4375 (98.9329)  time: 0.0285  data: 0.0002  max mem: 4132
[19:09:31.620210] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6906 (0.7044)  acc1: 76.5625 (77.2978)  acc5: 98.4375 (98.9277)  time: 0.0288  data: 0.0002  max mem: 4132
[19:09:31.908204] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6916 (0.7029)  acc1: 78.1250 (77.1004)  acc5: 98.4375 (98.8473)  time: 0.0288  data: 0.0002  max mem: 4132
[19:09:32.196359] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6672 (0.6981)  acc1: 78.1250 (77.1787)  acc5: 98.4375 (98.8556)  time: 0.0287  data: 0.0003  max mem: 4132
[19:09:32.485775] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6672 (0.7009)  acc1: 78.1250 (77.1219)  acc5: 98.4375 (98.7269)  time: 0.0287  data: 0.0003  max mem: 4132
[19:09:32.771709] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6681 (0.6964)  acc1: 79.6875 (77.3352)  acc5: 100.0000 (98.7981)  time: 0.0286  data: 0.0002  max mem: 4132
[19:09:33.057030] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.6818 (0.6996)  acc1: 76.5625 (77.2123)  acc5: 100.0000 (98.7933)  time: 0.0284  data: 0.0002  max mem: 4132
[19:09:33.341210] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.7363 (0.7022)  acc1: 76.5625 (77.0693)  acc5: 98.4375 (98.7613)  time: 0.0283  data: 0.0002  max mem: 4132
[19:09:33.626592] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6855 (0.6993)  acc1: 76.5625 (76.9886)  acc5: 98.4375 (98.6958)  time: 0.0283  data: 0.0002  max mem: 4132
[19:09:33.914445] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6734 (0.6989)  acc1: 76.5625 (76.9442)  acc5: 98.4375 (98.7238)  time: 0.0285  data: 0.0002  max mem: 4132
[19:09:34.199941] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7138 (0.6984)  acc1: 76.5625 (77.0833)  acc5: 98.4375 (98.6813)  time: 0.0285  data: 0.0002  max mem: 4132
[19:09:34.483665] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7144 (0.6959)  acc1: 76.5625 (77.1730)  acc5: 98.4375 (98.6858)  time: 0.0283  data: 0.0002  max mem: 4132
[19:09:34.636538] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6932 (0.6962)  acc1: 76.5625 (77.1800)  acc5: 98.4375 (98.7000)  time: 0.0273  data: 0.0001  max mem: 4132
[19:09:34.799033] Test: Total time: 0:00:05 (0.0338 s / it)
[19:09:34.800446] * Acc@1 77.180 Acc@5 98.700 loss 0.696
[19:09:34.801150] Accuracy of the network on the 10000 test images: 77.2%
[19:09:34.801682] Max accuracy: 77.18%
[19:09:34.906879] log_dir: ./output_dir
[19:09:35.841078] Epoch: [44]  [  0/781]  eta: 0:12:07  lr: 0.000160  training_loss: 1.1898 (1.1898)  classification_loss: 1.1893 (1.1893)  loss_mask: 0.0006 (0.0006)  time: 0.9309  data: 0.7357  max mem: 4132
[19:09:39.121701] Epoch: [44]  [ 20/781]  eta: 0:02:32  lr: 0.000160  training_loss: 1.3644 (1.3791)  classification_loss: 1.3639 (1.3707)  loss_mask: 0.0009 (0.0084)  time: 0.1639  data: 0.0002  max mem: 4132
[19:09:42.383537] Epoch: [44]  [ 40/781]  eta: 0:02:14  lr: 0.000160  training_loss: 1.3711 (1.3817)  classification_loss: 1.3692 (1.3766)  loss_mask: 0.0009 (0.0052)  time: 0.1630  data: 0.0003  max mem: 4132
[19:09:45.660223] Epoch: [44]  [ 60/781]  eta: 0:02:06  lr: 0.000160  training_loss: 1.3884 (1.3893)  classification_loss: 1.3869 (1.3854)  loss_mask: 0.0016 (0.0040)  time: 0.1638  data: 0.0004  max mem: 4132
[19:09:48.966531] Epoch: [44]  [ 80/781]  eta: 0:02:01  lr: 0.000160  training_loss: 1.4090 (1.3947)  classification_loss: 1.4075 (1.3914)  loss_mask: 0.0008 (0.0033)  time: 0.1652  data: 0.0003  max mem: 4132
[19:09:52.238973] Epoch: [44]  [100/781]  eta: 0:01:56  lr: 0.000160  training_loss: 1.3885 (1.3938)  classification_loss: 1.3879 (1.3910)  loss_mask: 0.0006 (0.0028)  time: 0.1635  data: 0.0003  max mem: 4132
[19:09:55.510901] Epoch: [44]  [120/781]  eta: 0:01:52  lr: 0.000159  training_loss: 1.3613 (1.3927)  classification_loss: 1.3607 (1.3903)  loss_mask: 0.0008 (0.0025)  time: 0.1635  data: 0.0002  max mem: 4132
[19:09:58.801982] Epoch: [44]  [140/781]  eta: 0:01:48  lr: 0.000159  training_loss: 1.3491 (1.3899)  classification_loss: 1.3481 (1.3877)  loss_mask: 0.0007 (0.0022)  time: 0.1645  data: 0.0002  max mem: 4132
[19:10:02.105021] Epoch: [44]  [160/781]  eta: 0:01:44  lr: 0.000159  training_loss: 1.3516 (1.3876)  classification_loss: 1.3510 (1.3855)  loss_mask: 0.0006 (0.0020)  time: 0.1651  data: 0.0003  max mem: 4132
[19:10:05.485020] Epoch: [44]  [180/781]  eta: 0:01:41  lr: 0.000159  training_loss: 1.3776 (1.3847)  classification_loss: 1.3766 (1.3828)  loss_mask: 0.0008 (0.0019)  time: 0.1689  data: 0.0004  max mem: 4132
[19:10:08.800229] Epoch: [44]  [200/781]  eta: 0:01:37  lr: 0.000159  training_loss: 1.3952 (1.3855)  classification_loss: 1.3943 (1.3838)  loss_mask: 0.0007 (0.0018)  time: 0.1657  data: 0.0003  max mem: 4132
[19:10:12.120800] Epoch: [44]  [220/781]  eta: 0:01:34  lr: 0.000159  training_loss: 1.3938 (1.3865)  classification_loss: 1.3933 (1.3848)  loss_mask: 0.0006 (0.0017)  time: 0.1659  data: 0.0004  max mem: 4132
[19:10:15.436863] Epoch: [44]  [240/781]  eta: 0:01:30  lr: 0.000159  training_loss: 1.3592 (1.3855)  classification_loss: 1.3587 (1.3840)  loss_mask: 0.0005 (0.0016)  time: 0.1657  data: 0.0003  max mem: 4132
[19:10:18.740364] Epoch: [44]  [260/781]  eta: 0:01:27  lr: 0.000159  training_loss: 1.3820 (1.3866)  classification_loss: 1.3818 (1.3851)  loss_mask: 0.0004 (0.0015)  time: 0.1650  data: 0.0003  max mem: 4132
[19:10:22.061087] Epoch: [44]  [280/781]  eta: 0:01:24  lr: 0.000159  training_loss: 1.3860 (1.3875)  classification_loss: 1.3855 (1.3860)  loss_mask: 0.0004 (0.0014)  time: 0.1659  data: 0.0004  max mem: 4132
[19:10:25.391316] Epoch: [44]  [300/781]  eta: 0:01:20  lr: 0.000159  training_loss: 1.3890 (1.3885)  classification_loss: 1.3885 (1.3871)  loss_mask: 0.0006 (0.0014)  time: 0.1664  data: 0.0003  max mem: 4132
[19:10:28.685527] Epoch: [44]  [320/781]  eta: 0:01:17  lr: 0.000158  training_loss: 1.3917 (1.3894)  classification_loss: 1.3915 (1.3881)  loss_mask: 0.0004 (0.0013)  time: 0.1646  data: 0.0003  max mem: 4132
[19:10:32.013467] Epoch: [44]  [340/781]  eta: 0:01:13  lr: 0.000158  training_loss: 1.3799 (1.3887)  classification_loss: 1.3773 (1.3875)  loss_mask: 0.0006 (0.0013)  time: 0.1663  data: 0.0003  max mem: 4132
[19:10:35.296709] Epoch: [44]  [360/781]  eta: 0:01:10  lr: 0.000158  training_loss: 1.4042 (1.3906)  classification_loss: 1.4041 (1.3894)  loss_mask: 0.0005 (0.0012)  time: 0.1641  data: 0.0003  max mem: 4132
[19:10:38.587661] Epoch: [44]  [380/781]  eta: 0:01:06  lr: 0.000158  training_loss: 1.4970 (1.3977)  classification_loss: 1.4104 (1.3910)  loss_mask: 0.0766 (0.0067)  time: 0.1645  data: 0.0003  max mem: 4132
[19:10:41.878665] Epoch: [44]  [400/781]  eta: 0:01:03  lr: 0.000158  training_loss: 1.5287 (1.4059)  classification_loss: 1.4407 (1.3938)  loss_mask: 0.0659 (0.0121)  time: 0.1644  data: 0.0003  max mem: 4132
[19:10:45.179738] Epoch: [44]  [420/781]  eta: 0:01:00  lr: 0.000158  training_loss: 1.6465 (1.4224)  classification_loss: 1.4839 (1.3994)  loss_mask: 0.1006 (0.0230)  time: 0.1649  data: 0.0003  max mem: 4132
[19:10:48.475578] Epoch: [44]  [440/781]  eta: 0:00:56  lr: 0.000158  training_loss: 2.6291 (1.4841)  classification_loss: 1.7997 (1.4152)  loss_mask: 0.6919 (0.0689)  time: 0.1647  data: 0.0003  max mem: 4132
[19:10:51.781028] Epoch: [44]  [460/781]  eta: 0:00:53  lr: 0.000158  training_loss: 2.4817 (1.5317)  classification_loss: 2.2209 (1.4501)  loss_mask: 0.2566 (0.0816)  time: 0.1651  data: 0.0003  max mem: 4132
[19:10:55.073349] Epoch: [44]  [480/781]  eta: 0:00:50  lr: 0.000158  training_loss: 2.4714 (1.5732)  classification_loss: 2.1744 (1.4807)  loss_mask: 0.3076 (0.0925)  time: 0.1645  data: 0.0002  max mem: 4132
[19:10:58.369051] Epoch: [44]  [500/781]  eta: 0:00:46  lr: 0.000157  training_loss: 2.2372 (1.5997)  classification_loss: 1.9213 (1.4994)  loss_mask: 0.2800 (0.1003)  time: 0.1647  data: 0.0003  max mem: 4132
[19:11:01.675481] Epoch: [44]  [520/781]  eta: 0:00:43  lr: 0.000157  training_loss: 2.0696 (1.6205)  classification_loss: 1.7765 (1.5108)  loss_mask: 0.2723 (0.1097)  time: 0.1652  data: 0.0003  max mem: 4132
[19:11:04.995586] Epoch: [44]  [540/781]  eta: 0:00:40  lr: 0.000157  training_loss: 2.6267 (1.6568)  classification_loss: 1.9229 (1.5268)  loss_mask: 0.6824 (0.1300)  time: 0.1659  data: 0.0003  max mem: 4132
[19:11:08.288776] Epoch: [44]  [560/781]  eta: 0:00:36  lr: 0.000157  training_loss: 2.5125 (1.6874)  classification_loss: 2.1184 (1.5481)  loss_mask: 0.3708 (0.1392)  time: 0.1646  data: 0.0003  max mem: 4132
[19:11:11.579867] Epoch: [44]  [580/781]  eta: 0:00:33  lr: 0.000157  training_loss: 2.4523 (1.7137)  classification_loss: 2.0881 (1.5662)  loss_mask: 0.3368 (0.1475)  time: 0.1645  data: 0.0002  max mem: 4132
[19:11:14.867122] Epoch: [44]  [600/781]  eta: 0:00:30  lr: 0.000157  training_loss: 2.5486 (1.7407)  classification_loss: 1.9363 (1.5786)  loss_mask: 0.6203 (0.1621)  time: 0.1643  data: 0.0003  max mem: 4132
[19:11:18.152833] Epoch: [44]  [620/781]  eta: 0:00:26  lr: 0.000157  training_loss: 2.2134 (1.7557)  classification_loss: 1.9337 (1.5898)  loss_mask: 0.2612 (0.1659)  time: 0.1642  data: 0.0003  max mem: 4132
[19:11:21.455651] Epoch: [44]  [640/781]  eta: 0:00:23  lr: 0.000157  training_loss: 2.1398 (1.7683)  classification_loss: 1.9860 (1.6023)  loss_mask: 0.1575 (0.1660)  time: 0.1651  data: 0.0003  max mem: 4132
[19:11:24.766590] Epoch: [44]  [660/781]  eta: 0:00:20  lr: 0.000157  training_loss: 2.1233 (1.7792)  classification_loss: 2.0000 (1.6150)  loss_mask: 0.1065 (0.1642)  time: 0.1655  data: 0.0003  max mem: 4132
[19:11:28.048087] Epoch: [44]  [680/781]  eta: 0:00:16  lr: 0.000157  training_loss: 2.0560 (1.7868)  classification_loss: 1.9851 (1.6252)  loss_mask: 0.0659 (0.1616)  time: 0.1640  data: 0.0002  max mem: 4132
[19:11:31.330070] Epoch: [44]  [700/781]  eta: 0:00:13  lr: 0.000156  training_loss: 1.9440 (1.7918)  classification_loss: 1.8733 (1.6329)  loss_mask: 0.0569 (0.1589)  time: 0.1640  data: 0.0002  max mem: 4132
[19:11:34.637557] Epoch: [44]  [720/781]  eta: 0:00:10  lr: 0.000156  training_loss: 1.8654 (1.7952)  classification_loss: 1.8187 (1.6390)  loss_mask: 0.0541 (0.1562)  time: 0.1652  data: 0.0002  max mem: 4132
[19:11:37.901508] Epoch: [44]  [740/781]  eta: 0:00:06  lr: 0.000156  training_loss: 1.8381 (1.7968)  classification_loss: 1.7909 (1.6436)  loss_mask: 0.0462 (0.1533)  time: 0.1630  data: 0.0003  max mem: 4132
[19:11:41.199357] Epoch: [44]  [760/781]  eta: 0:00:03  lr: 0.000156  training_loss: 1.8531 (1.7985)  classification_loss: 1.7969 (1.6480)  loss_mask: 0.0376 (0.1505)  time: 0.1648  data: 0.0003  max mem: 4132
[19:11:44.484984] Epoch: [44]  [780/781]  eta: 0:00:00  lr: 0.000156  training_loss: 1.8340 (1.7996)  classification_loss: 1.7743 (1.6510)  loss_mask: 0.0593 (0.1486)  time: 0.1641  data: 0.0003  max mem: 4132
[19:11:44.662780] Epoch: [44] Total time: 0:02:09 (0.1661 s / it)
[19:11:44.663954] Averaged stats: lr: 0.000156  training_loss: 1.8340 (1.7996)  classification_loss: 1.7743 (1.6510)  loss_mask: 0.0593 (0.1486)
[19:11:45.366030] Test:  [  0/157]  eta: 0:01:49  testing_loss: 0.9271 (0.9271)  acc1: 70.3125 (70.3125)  acc5: 96.8750 (96.8750)  time: 0.6964  data: 0.6665  max mem: 4132
[19:11:45.696310] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.0501 (1.0491)  acc1: 65.6250 (65.9091)  acc5: 96.8750 (97.4432)  time: 0.0929  data: 0.0617  max mem: 4132
[19:11:45.991386] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.0182 (1.0300)  acc1: 65.6250 (66.0714)  acc5: 96.8750 (97.5446)  time: 0.0309  data: 0.0007  max mem: 4132
[19:11:46.282586] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.0100 (1.0340)  acc1: 65.6250 (66.2298)  acc5: 96.8750 (97.0766)  time: 0.0292  data: 0.0003  max mem: 4132
[19:11:46.574535] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.0286 (1.0364)  acc1: 65.6250 (66.8445)  acc5: 96.8750 (97.1418)  time: 0.0290  data: 0.0003  max mem: 4132
[19:11:46.864721] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.0377 (1.0360)  acc1: 65.6250 (66.9730)  acc5: 96.8750 (97.1201)  time: 0.0290  data: 0.0003  max mem: 4132
[19:11:47.154490] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.0377 (1.0323)  acc1: 65.6250 (66.9570)  acc5: 96.8750 (97.1568)  time: 0.0288  data: 0.0003  max mem: 4132
[19:11:47.445973] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.9958 (1.0273)  acc1: 68.7500 (67.0995)  acc5: 96.8750 (97.2271)  time: 0.0289  data: 0.0003  max mem: 4132
[19:11:47.737311] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.0101 (1.0313)  acc1: 67.1875 (66.9174)  acc5: 96.8750 (97.2415)  time: 0.0290  data: 0.0003  max mem: 4132
[19:11:48.029434] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.0643 (1.0308)  acc1: 67.1875 (66.9986)  acc5: 98.4375 (97.3386)  time: 0.0290  data: 0.0003  max mem: 4132
[19:11:48.316846] Test:  [100/157]  eta: 0:00:02  testing_loss: 1.0531 (1.0336)  acc1: 68.7500 (67.0173)  acc5: 98.4375 (97.3855)  time: 0.0288  data: 0.0003  max mem: 4132
[19:11:48.605011] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.0531 (1.0363)  acc1: 65.6250 (66.8919)  acc5: 98.4375 (97.3395)  time: 0.0286  data: 0.0003  max mem: 4132
[19:11:48.901812] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.0439 (1.0331)  acc1: 65.6250 (67.0584)  acc5: 96.8750 (97.4174)  time: 0.0291  data: 0.0003  max mem: 4132
[19:11:49.195147] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.0345 (1.0359)  acc1: 65.6250 (66.8655)  acc5: 96.8750 (97.3760)  time: 0.0294  data: 0.0003  max mem: 4132
[19:11:49.485468] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.0322 (1.0350)  acc1: 67.1875 (66.9105)  acc5: 96.8750 (97.3626)  time: 0.0290  data: 0.0003  max mem: 4132
[19:11:49.770412] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.0445 (1.0328)  acc1: 68.7500 (67.0219)  acc5: 96.8750 (97.2993)  time: 0.0286  data: 0.0002  max mem: 4132
[19:11:49.925374] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.0289 (1.0340)  acc1: 68.7500 (66.8300)  acc5: 96.8750 (97.3500)  time: 0.0275  data: 0.0002  max mem: 4132
[19:11:50.093902] Test: Total time: 0:00:05 (0.0346 s / it)
[19:11:50.094452] * Acc@1 66.830 Acc@5 97.350 loss 1.034
[19:11:50.095001] Accuracy of the network on the 10000 test images: 66.8%
[19:11:50.095369] Max accuracy: 77.18%
[19:11:50.207648] log_dir: ./output_dir
[19:11:51.182913] Epoch: [45]  [  0/781]  eta: 0:12:40  lr: 0.000156  training_loss: 1.7296 (1.7296)  classification_loss: 1.6697 (1.6697)  loss_mask: 0.0599 (0.0599)  time: 0.9733  data: 0.7616  max mem: 4132
[19:11:54.506716] Epoch: [45]  [ 20/781]  eta: 0:02:35  lr: 0.000156  training_loss: 1.7856 (1.8179)  classification_loss: 1.7511 (1.7641)  loss_mask: 0.0444 (0.0538)  time: 0.1661  data: 0.0003  max mem: 4132
[19:11:57.821379] Epoch: [45]  [ 40/781]  eta: 0:02:17  lr: 0.000156  training_loss: 1.7495 (1.8082)  classification_loss: 1.6835 (1.7312)  loss_mask: 0.0880 (0.0769)  time: 0.1656  data: 0.0003  max mem: 4132
[19:12:01.158977] Epoch: [45]  [ 60/781]  eta: 0:02:09  lr: 0.000156  training_loss: 1.7972 (1.8073)  classification_loss: 1.7201 (1.7300)  loss_mask: 0.0619 (0.0773)  time: 0.1668  data: 0.0004  max mem: 4132
[19:12:04.507238] Epoch: [45]  [ 80/781]  eta: 0:02:03  lr: 0.000156  training_loss: 1.7651 (1.7949)  classification_loss: 1.6929 (1.7229)  loss_mask: 0.0389 (0.0720)  time: 0.1673  data: 0.0003  max mem: 4132
[19:12:07.826569] Epoch: [45]  [100/781]  eta: 0:01:58  lr: 0.000156  training_loss: 1.7292 (1.7786)  classification_loss: 1.6877 (1.7121)  loss_mask: 0.0377 (0.0665)  time: 0.1659  data: 0.0003  max mem: 4132
[19:12:11.103532] Epoch: [45]  [120/781]  eta: 0:01:54  lr: 0.000155  training_loss: 1.6388 (1.7597)  classification_loss: 1.5840 (1.6952)  loss_mask: 0.0394 (0.0645)  time: 0.1638  data: 0.0003  max mem: 4132
[19:12:14.404414] Epoch: [45]  [140/781]  eta: 0:01:49  lr: 0.000155  training_loss: 2.0422 (1.8006)  classification_loss: 1.6621 (1.6914)  loss_mask: 0.2664 (0.1092)  time: 0.1649  data: 0.0003  max mem: 4132
[19:12:17.693651] Epoch: [45]  [160/781]  eta: 0:01:45  lr: 0.000155  training_loss: 2.7275 (1.9208)  classification_loss: 1.7669 (1.7073)  loss_mask: 0.9368 (0.2135)  time: 0.1644  data: 0.0002  max mem: 4132
[19:12:21.026927] Epoch: [45]  [180/781]  eta: 0:01:42  lr: 0.000155  training_loss: 2.1399 (1.9439)  classification_loss: 1.7706 (1.7170)  loss_mask: 0.3453 (0.2268)  time: 0.1666  data: 0.0003  max mem: 4132
[19:12:24.346798] Epoch: [45]  [200/781]  eta: 0:01:38  lr: 0.000155  training_loss: 1.9741 (1.9498)  classification_loss: 1.8464 (1.7331)  loss_mask: 0.1225 (0.2168)  time: 0.1659  data: 0.0002  max mem: 4132
[19:12:27.640128] Epoch: [45]  [220/781]  eta: 0:01:34  lr: 0.000155  training_loss: 1.8528 (1.9437)  classification_loss: 1.7992 (1.7385)  loss_mask: 0.0721 (0.2052)  time: 0.1645  data: 0.0004  max mem: 4132
[19:12:30.938257] Epoch: [45]  [240/781]  eta: 0:01:31  lr: 0.000155  training_loss: 1.8231 (1.9324)  classification_loss: 1.7557 (1.7397)  loss_mask: 0.0526 (0.1926)  time: 0.1648  data: 0.0003  max mem: 4132
[19:12:34.215207] Epoch: [45]  [260/781]  eta: 0:01:27  lr: 0.000155  training_loss: 1.6982 (1.9170)  classification_loss: 1.6561 (1.7348)  loss_mask: 0.0375 (0.1822)  time: 0.1638  data: 0.0003  max mem: 4132
[19:12:37.490722] Epoch: [45]  [280/781]  eta: 0:01:24  lr: 0.000155  training_loss: 1.6102 (1.8985)  classification_loss: 1.5638 (1.7257)  loss_mask: 0.0369 (0.1729)  time: 0.1637  data: 0.0002  max mem: 4132
[19:12:40.799377] Epoch: [45]  [300/781]  eta: 0:01:20  lr: 0.000155  training_loss: 1.6501 (1.8837)  classification_loss: 1.6194 (1.7201)  loss_mask: 0.0289 (0.1636)  time: 0.1653  data: 0.0002  max mem: 4132
[19:12:44.088028] Epoch: [45]  [320/781]  eta: 0:01:17  lr: 0.000154  training_loss: 1.6100 (1.8676)  classification_loss: 1.5876 (1.7123)  loss_mask: 0.0244 (0.1553)  time: 0.1643  data: 0.0003  max mem: 4132
[19:12:47.398363] Epoch: [45]  [340/781]  eta: 0:01:13  lr: 0.000154  training_loss: 1.5842 (1.8512)  classification_loss: 1.5474 (1.7029)  loss_mask: 0.0271 (0.1482)  time: 0.1654  data: 0.0003  max mem: 4132
[19:12:50.679833] Epoch: [45]  [360/781]  eta: 0:01:10  lr: 0.000154  training_loss: 1.6380 (1.8396)  classification_loss: 1.5444 (1.6953)  loss_mask: 0.0618 (0.1443)  time: 0.1640  data: 0.0002  max mem: 4132
[19:12:53.981845] Epoch: [45]  [380/781]  eta: 0:01:07  lr: 0.000154  training_loss: 1.5884 (1.8270)  classification_loss: 1.5129 (1.6865)  loss_mask: 0.0554 (0.1405)  time: 0.1650  data: 0.0002  max mem: 4132
[19:12:57.292798] Epoch: [45]  [400/781]  eta: 0:01:03  lr: 0.000154  training_loss: 1.5656 (1.8144)  classification_loss: 1.5360 (1.6792)  loss_mask: 0.0263 (0.1352)  time: 0.1655  data: 0.0003  max mem: 4132
[19:13:00.555655] Epoch: [45]  [420/781]  eta: 0:01:00  lr: 0.000154  training_loss: 1.5266 (1.8016)  classification_loss: 1.4977 (1.6717)  loss_mask: 0.0207 (0.1299)  time: 0.1630  data: 0.0003  max mem: 4132
[19:13:03.841576] Epoch: [45]  [440/781]  eta: 0:00:56  lr: 0.000154  training_loss: 1.5559 (1.7915)  classification_loss: 1.5290 (1.6658)  loss_mask: 0.0165 (0.1257)  time: 0.1642  data: 0.0003  max mem: 4132
[19:13:07.120641] Epoch: [45]  [460/781]  eta: 0:00:53  lr: 0.000154  training_loss: 1.5327 (1.7805)  classification_loss: 1.5214 (1.6591)  loss_mask: 0.0135 (0.1215)  time: 0.1639  data: 0.0003  max mem: 4132
[19:13:10.398402] Epoch: [45]  [480/781]  eta: 0:00:50  lr: 0.000154  training_loss: 1.4930 (1.7706)  classification_loss: 1.4733 (1.6516)  loss_mask: 0.0485 (0.1190)  time: 0.1638  data: 0.0003  max mem: 4132
[19:13:13.687883] Epoch: [45]  [500/781]  eta: 0:00:46  lr: 0.000154  training_loss: 1.5886 (1.7630)  classification_loss: 1.5197 (1.6465)  loss_mask: 0.0504 (0.1165)  time: 0.1644  data: 0.0003  max mem: 4132
[19:13:16.941880] Epoch: [45]  [520/781]  eta: 0:00:43  lr: 0.000153  training_loss: 1.5209 (1.7538)  classification_loss: 1.4598 (1.6401)  loss_mask: 0.0407 (0.1137)  time: 0.1626  data: 0.0003  max mem: 4132
[19:13:20.225369] Epoch: [45]  [540/781]  eta: 0:00:40  lr: 0.000153  training_loss: 1.5095 (1.7458)  classification_loss: 1.4912 (1.6351)  loss_mask: 0.0201 (0.1107)  time: 0.1641  data: 0.0003  max mem: 4132
[19:13:23.581491] Epoch: [45]  [560/781]  eta: 0:00:36  lr: 0.000153  training_loss: 1.5019 (1.7378)  classification_loss: 1.4974 (1.6300)  loss_mask: 0.0157 (0.1078)  time: 0.1677  data: 0.0004  max mem: 4132
[19:13:26.893222] Epoch: [45]  [580/781]  eta: 0:00:33  lr: 0.000153  training_loss: 1.5085 (1.7305)  classification_loss: 1.4569 (1.6246)  loss_mask: 0.0424 (0.1058)  time: 0.1653  data: 0.0003  max mem: 4132
[19:13:30.226709] Epoch: [45]  [600/781]  eta: 0:00:30  lr: 0.000153  training_loss: 1.5472 (1.7247)  classification_loss: 1.4766 (1.6200)  loss_mask: 0.0485 (0.1047)  time: 0.1666  data: 0.0003  max mem: 4132
[19:13:33.522779] Epoch: [45]  [620/781]  eta: 0:00:26  lr: 0.000153  training_loss: 1.5348 (1.7188)  classification_loss: 1.4808 (1.6160)  loss_mask: 0.0292 (0.1029)  time: 0.1647  data: 0.0003  max mem: 4132
[19:13:36.851139] Epoch: [45]  [640/781]  eta: 0:00:23  lr: 0.000153  training_loss: 1.4712 (1.7117)  classification_loss: 1.4496 (1.6114)  loss_mask: 0.0175 (0.1003)  time: 0.1663  data: 0.0003  max mem: 4132
[19:13:40.154876] Epoch: [45]  [660/781]  eta: 0:00:20  lr: 0.000153  training_loss: 1.4904 (1.7051)  classification_loss: 1.4763 (1.6075)  loss_mask: 0.0083 (0.0976)  time: 0.1651  data: 0.0003  max mem: 4132
[19:13:43.455615] Epoch: [45]  [680/781]  eta: 0:00:16  lr: 0.000153  training_loss: 1.5466 (1.6999)  classification_loss: 1.5162 (1.6041)  loss_mask: 0.0205 (0.0957)  time: 0.1649  data: 0.0004  max mem: 4132
[19:13:46.771434] Epoch: [45]  [700/781]  eta: 0:00:13  lr: 0.000152  training_loss: 1.5294 (1.6953)  classification_loss: 1.5086 (1.6014)  loss_mask: 0.0267 (0.0939)  time: 0.1657  data: 0.0002  max mem: 4132
[19:13:50.050963] Epoch: [45]  [720/781]  eta: 0:00:10  lr: 0.000152  training_loss: 1.5495 (1.6917)  classification_loss: 1.4681 (1.5979)  loss_mask: 0.0654 (0.0938)  time: 0.1639  data: 0.0002  max mem: 4132
[19:13:53.332055] Epoch: [45]  [740/781]  eta: 0:00:06  lr: 0.000152  training_loss: 1.5192 (1.6873)  classification_loss: 1.4706 (1.5946)  loss_mask: 0.0448 (0.0927)  time: 0.1640  data: 0.0003  max mem: 4132
[19:13:56.630457] Epoch: [45]  [760/781]  eta: 0:00:03  lr: 0.000152  training_loss: 1.5472 (1.6840)  classification_loss: 1.4813 (1.5918)  loss_mask: 0.0659 (0.0922)  time: 0.1648  data: 0.0003  max mem: 4132
[19:13:59.924896] Epoch: [45]  [780/781]  eta: 0:00:00  lr: 0.000152  training_loss: 1.5686 (1.6804)  classification_loss: 1.4563 (1.5887)  loss_mask: 0.0661 (0.0917)  time: 0.1646  data: 0.0002  max mem: 4132
[19:14:00.085237] Epoch: [45] Total time: 0:02:09 (0.1663 s / it)
[19:14:00.085722] Averaged stats: lr: 0.000152  training_loss: 1.5686 (1.6804)  classification_loss: 1.4563 (1.5887)  loss_mask: 0.0661 (0.0917)
[19:14:00.819857] Test:  [  0/157]  eta: 0:01:54  testing_loss: 0.6524 (0.6524)  acc1: 79.6875 (79.6875)  acc5: 95.3125 (95.3125)  time: 0.7301  data: 0.7004  max mem: 4132
[19:14:01.121017] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.7237 (0.7124)  acc1: 76.5625 (78.4091)  acc5: 100.0000 (99.0057)  time: 0.0936  data: 0.0642  max mem: 4132
[19:14:01.414307] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.7089 (0.7065)  acc1: 78.1250 (78.2738)  acc5: 100.0000 (98.9583)  time: 0.0296  data: 0.0004  max mem: 4132
[19:14:01.705937] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.7121 (0.7270)  acc1: 78.1250 (77.3185)  acc5: 98.4375 (98.9919)  time: 0.0291  data: 0.0003  max mem: 4132
[19:14:01.998310] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.7062 (0.7202)  acc1: 76.5625 (77.6296)  acc5: 98.4375 (98.8567)  time: 0.0290  data: 0.0003  max mem: 4132
[19:14:02.291540] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6738 (0.7185)  acc1: 79.6875 (77.8799)  acc5: 98.4375 (98.7132)  time: 0.0290  data: 0.0003  max mem: 4132
[19:14:02.582872] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6818 (0.7174)  acc1: 78.1250 (77.4078)  acc5: 98.4375 (98.6424)  time: 0.0290  data: 0.0002  max mem: 4132
[19:14:02.871110] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6799 (0.7116)  acc1: 78.1250 (77.7729)  acc5: 98.4375 (98.5695)  time: 0.0288  data: 0.0002  max mem: 4132
[19:14:03.157314] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6945 (0.7186)  acc1: 79.6875 (77.5463)  acc5: 98.4375 (98.4568)  time: 0.0286  data: 0.0003  max mem: 4132
[19:14:03.441859] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7395 (0.7177)  acc1: 76.5625 (77.4897)  acc5: 98.4375 (98.4375)  time: 0.0284  data: 0.0002  max mem: 4132
[19:14:03.726796] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.7324 (0.7221)  acc1: 76.5625 (77.2432)  acc5: 98.4375 (98.4375)  time: 0.0283  data: 0.0002  max mem: 4132
[19:14:04.015429] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.7577 (0.7228)  acc1: 75.0000 (77.0693)  acc5: 98.4375 (98.4234)  time: 0.0285  data: 0.0003  max mem: 4132
[19:14:04.309513] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.7156 (0.7201)  acc1: 75.0000 (77.1307)  acc5: 98.4375 (98.4375)  time: 0.0290  data: 0.0002  max mem: 4132
[19:14:04.599435] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.7227 (0.7204)  acc1: 75.0000 (77.0754)  acc5: 98.4375 (98.4136)  time: 0.0290  data: 0.0002  max mem: 4132
[19:14:04.889625] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7342 (0.7200)  acc1: 76.5625 (77.0944)  acc5: 98.4375 (98.4597)  time: 0.0289  data: 0.0002  max mem: 4132
[19:14:05.173517] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7269 (0.7192)  acc1: 76.5625 (77.1316)  acc5: 98.4375 (98.4685)  time: 0.0286  data: 0.0002  max mem: 4132
[19:14:05.327823] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7177 (0.7207)  acc1: 76.5625 (77.0900)  acc5: 98.4375 (98.4700)  time: 0.0274  data: 0.0002  max mem: 4132
[19:14:05.504576] Test: Total time: 0:00:05 (0.0345 s / it)
[19:14:05.505114] * Acc@1 77.090 Acc@5 98.470 loss 0.721
[19:14:05.505537] Accuracy of the network on the 10000 test images: 77.1%
[19:14:05.505874] Max accuracy: 77.18%
[19:14:05.584031] log_dir: ./output_dir
[19:14:06.534619] Epoch: [46]  [  0/781]  eta: 0:12:20  lr: 0.000152  training_loss: 1.3255 (1.3255)  classification_loss: 1.3059 (1.3059)  loss_mask: 0.0196 (0.0196)  time: 0.9487  data: 0.7407  max mem: 4132
[19:14:09.832476] Epoch: [46]  [ 20/781]  eta: 0:02:33  lr: 0.000152  training_loss: 1.5081 (1.4790)  classification_loss: 1.4404 (1.4157)  loss_mask: 0.0513 (0.0633)  time: 0.1648  data: 0.0002  max mem: 4132
[19:14:13.130774] Epoch: [46]  [ 40/781]  eta: 0:02:16  lr: 0.000152  training_loss: 1.4546 (1.4784)  classification_loss: 1.4041 (1.4161)  loss_mask: 0.0516 (0.0623)  time: 0.1648  data: 0.0003  max mem: 4132
[19:14:16.443274] Epoch: [46]  [ 60/781]  eta: 0:02:08  lr: 0.000152  training_loss: 1.4990 (1.4904)  classification_loss: 1.4593 (1.4358)  loss_mask: 0.0330 (0.0546)  time: 0.1655  data: 0.0002  max mem: 4132
[19:14:19.734111] Epoch: [46]  [ 80/781]  eta: 0:02:02  lr: 0.000152  training_loss: 1.5044 (1.4982)  classification_loss: 1.4173 (1.4345)  loss_mask: 0.0767 (0.0637)  time: 0.1645  data: 0.0002  max mem: 4132
[19:14:23.010870] Epoch: [46]  [100/781]  eta: 0:01:57  lr: 0.000152  training_loss: 1.5047 (1.5032)  classification_loss: 1.4566 (1.4408)  loss_mask: 0.0288 (0.0624)  time: 0.1637  data: 0.0002  max mem: 4132
[19:14:26.318614] Epoch: [46]  [120/781]  eta: 0:01:53  lr: 0.000151  training_loss: 1.5216 (1.5112)  classification_loss: 1.5035 (1.4511)  loss_mask: 0.0294 (0.0601)  time: 0.1653  data: 0.0002  max mem: 4132
[19:14:29.601870] Epoch: [46]  [140/781]  eta: 0:01:49  lr: 0.000151  training_loss: 1.5141 (1.5102)  classification_loss: 1.4412 (1.4501)  loss_mask: 0.0502 (0.0601)  time: 0.1641  data: 0.0002  max mem: 4132
[19:14:32.878440] Epoch: [46]  [160/781]  eta: 0:01:45  lr: 0.000151  training_loss: 1.4645 (1.5064)  classification_loss: 1.3831 (1.4460)  loss_mask: 0.0493 (0.0604)  time: 0.1637  data: 0.0002  max mem: 4132
[19:14:36.149236] Epoch: [46]  [180/781]  eta: 0:01:41  lr: 0.000151  training_loss: 1.5149 (1.5085)  classification_loss: 1.4364 (1.4455)  loss_mask: 0.0652 (0.0630)  time: 0.1634  data: 0.0002  max mem: 4132
[19:14:39.436870] Epoch: [46]  [200/781]  eta: 0:01:37  lr: 0.000151  training_loss: 1.6100 (1.5213)  classification_loss: 1.4356 (1.4486)  loss_mask: 0.1304 (0.0727)  time: 0.1643  data: 0.0003  max mem: 4132
[19:14:42.735791] Epoch: [46]  [220/781]  eta: 0:01:34  lr: 0.000151  training_loss: 1.4675 (1.5210)  classification_loss: 1.3864 (1.4466)  loss_mask: 0.0706 (0.0745)  time: 0.1648  data: 0.0004  max mem: 4132
[19:14:46.006660] Epoch: [46]  [240/781]  eta: 0:01:30  lr: 0.000151  training_loss: 1.5346 (1.5200)  classification_loss: 1.4449 (1.4466)  loss_mask: 0.0570 (0.0734)  time: 0.1635  data: 0.0002  max mem: 4132
[19:14:49.283269] Epoch: [46]  [260/781]  eta: 0:01:27  lr: 0.000151  training_loss: 1.4812 (1.5155)  classification_loss: 1.4313 (1.4448)  loss_mask: 0.0321 (0.0707)  time: 0.1637  data: 0.0003  max mem: 4132
[19:14:52.594765] Epoch: [46]  [280/781]  eta: 0:01:23  lr: 0.000151  training_loss: 1.4747 (1.5140)  classification_loss: 1.4537 (1.4432)  loss_mask: 0.0614 (0.0708)  time: 0.1655  data: 0.0003  max mem: 4132
[19:14:55.894375] Epoch: [46]  [300/781]  eta: 0:01:20  lr: 0.000151  training_loss: 1.5556 (1.5168)  classification_loss: 1.4278 (1.4422)  loss_mask: 0.1003 (0.0745)  time: 0.1648  data: 0.0003  max mem: 4132
[19:14:59.184334] Epoch: [46]  [320/781]  eta: 0:01:16  lr: 0.000150  training_loss: 1.5298 (1.5190)  classification_loss: 1.4589 (1.4433)  loss_mask: 0.0858 (0.0757)  time: 0.1644  data: 0.0002  max mem: 4132
[19:15:02.523914] Epoch: [46]  [340/781]  eta: 0:01:13  lr: 0.000150  training_loss: 1.4946 (1.5188)  classification_loss: 1.4642 (1.4451)  loss_mask: 0.0325 (0.0737)  time: 0.1669  data: 0.0004  max mem: 4132
[19:15:05.862178] Epoch: [46]  [360/781]  eta: 0:01:10  lr: 0.000150  training_loss: 1.5234 (1.5218)  classification_loss: 1.5187 (1.4510)  loss_mask: 0.0183 (0.0708)  time: 0.1668  data: 0.0006  max mem: 4132
[19:15:09.236304] Epoch: [46]  [380/781]  eta: 0:01:06  lr: 0.000150  training_loss: 1.4496 (1.5203)  classification_loss: 1.4462 (1.4522)  loss_mask: 0.0125 (0.0681)  time: 0.1686  data: 0.0005  max mem: 4132
[19:15:12.571086] Epoch: [46]  [400/781]  eta: 0:01:03  lr: 0.000150  training_loss: 1.4784 (1.5190)  classification_loss: 1.4523 (1.4527)  loss_mask: 0.0228 (0.0663)  time: 0.1666  data: 0.0003  max mem: 4132
[19:15:15.870075] Epoch: [46]  [420/781]  eta: 0:01:00  lr: 0.000150  training_loss: 1.4597 (1.5166)  classification_loss: 1.4379 (1.4524)  loss_mask: 0.0175 (0.0641)  time: 0.1648  data: 0.0002  max mem: 4132
[19:15:19.187953] Epoch: [46]  [440/781]  eta: 0:00:56  lr: 0.000150  training_loss: 1.4786 (1.5156)  classification_loss: 1.4495 (1.4520)  loss_mask: 0.0195 (0.0636)  time: 0.1658  data: 0.0003  max mem: 4132
[19:15:22.517084] Epoch: [46]  [460/781]  eta: 0:00:53  lr: 0.000150  training_loss: 1.4850 (1.5146)  classification_loss: 1.4083 (1.4514)  loss_mask: 0.0336 (0.0632)  time: 0.1664  data: 0.0003  max mem: 4132
[19:15:25.856349] Epoch: [46]  [480/781]  eta: 0:00:50  lr: 0.000150  training_loss: 1.5434 (1.5163)  classification_loss: 1.5143 (1.4541)  loss_mask: 0.0266 (0.0622)  time: 0.1669  data: 0.0003  max mem: 4132
[19:15:29.167009] Epoch: [46]  [500/781]  eta: 0:00:46  lr: 0.000149  training_loss: 1.5573 (1.5176)  classification_loss: 1.4607 (1.4551)  loss_mask: 0.0549 (0.0625)  time: 0.1654  data: 0.0003  max mem: 4132
[19:15:32.441737] Epoch: [46]  [520/781]  eta: 0:00:43  lr: 0.000149  training_loss: 1.4086 (1.5150)  classification_loss: 1.3789 (1.4534)  loss_mask: 0.0332 (0.0616)  time: 0.1636  data: 0.0002  max mem: 4132
[19:15:35.772886] Epoch: [46]  [540/781]  eta: 0:00:40  lr: 0.000149  training_loss: 1.5067 (1.5140)  classification_loss: 1.4770 (1.4537)  loss_mask: 0.0087 (0.0603)  time: 0.1665  data: 0.0003  max mem: 4132
[19:15:39.084179] Epoch: [46]  [560/781]  eta: 0:00:36  lr: 0.000149  training_loss: 1.4560 (1.5122)  classification_loss: 1.4449 (1.4532)  loss_mask: 0.0132 (0.0590)  time: 0.1655  data: 0.0003  max mem: 4132
[19:15:42.359874] Epoch: [46]  [580/781]  eta: 0:00:33  lr: 0.000149  training_loss: 1.4890 (1.5122)  classification_loss: 1.4578 (1.4546)  loss_mask: 0.0120 (0.0576)  time: 0.1637  data: 0.0004  max mem: 4132
[19:15:45.667077] Epoch: [46]  [600/781]  eta: 0:00:30  lr: 0.000149  training_loss: 1.4758 (1.5107)  classification_loss: 1.4372 (1.4542)  loss_mask: 0.0099 (0.0565)  time: 0.1653  data: 0.0003  max mem: 4132
[19:15:48.968312] Epoch: [46]  [620/781]  eta: 0:00:26  lr: 0.000149  training_loss: 1.5887 (1.5121)  classification_loss: 1.4441 (1.4542)  loss_mask: 0.0559 (0.0580)  time: 0.1650  data: 0.0003  max mem: 4132
[19:15:52.287084] Epoch: [46]  [640/781]  eta: 0:00:23  lr: 0.000149  training_loss: 1.4618 (1.5117)  classification_loss: 1.4171 (1.4539)  loss_mask: 0.0375 (0.0578)  time: 0.1658  data: 0.0003  max mem: 4132
[19:15:55.584232] Epoch: [46]  [660/781]  eta: 0:00:20  lr: 0.000149  training_loss: 1.5008 (1.5115)  classification_loss: 1.4821 (1.4546)  loss_mask: 0.0216 (0.0569)  time: 0.1648  data: 0.0002  max mem: 4132
[19:15:58.890193] Epoch: [46]  [680/781]  eta: 0:00:16  lr: 0.000149  training_loss: 1.5098 (1.5120)  classification_loss: 1.4799 (1.4555)  loss_mask: 0.0300 (0.0565)  time: 0.1652  data: 0.0003  max mem: 4132
[19:16:02.169405] Epoch: [46]  [700/781]  eta: 0:00:13  lr: 0.000148  training_loss: 1.4964 (1.5116)  classification_loss: 1.4630 (1.4557)  loss_mask: 0.0207 (0.0560)  time: 0.1639  data: 0.0003  max mem: 4132
[19:16:05.461543] Epoch: [46]  [720/781]  eta: 0:00:10  lr: 0.000148  training_loss: 1.5813 (1.5129)  classification_loss: 1.4610 (1.4560)  loss_mask: 0.0687 (0.0569)  time: 0.1645  data: 0.0003  max mem: 4132
[19:16:08.751593] Epoch: [46]  [740/781]  eta: 0:00:06  lr: 0.000148  training_loss: 1.5572 (1.5137)  classification_loss: 1.4852 (1.4563)  loss_mask: 0.0730 (0.0574)  time: 0.1644  data: 0.0003  max mem: 4132
[19:16:12.047749] Epoch: [46]  [760/781]  eta: 0:00:03  lr: 0.000148  training_loss: 1.5315 (1.5146)  classification_loss: 1.5003 (1.4572)  loss_mask: 0.0475 (0.0573)  time: 0.1647  data: 0.0002  max mem: 4132
[19:16:15.371461] Epoch: [46]  [780/781]  eta: 0:00:00  lr: 0.000148  training_loss: 1.5078 (1.5148)  classification_loss: 1.4837 (1.4581)  loss_mask: 0.0195 (0.0567)  time: 0.1661  data: 0.0003  max mem: 4132
[19:16:15.580452] Epoch: [46] Total time: 0:02:09 (0.1664 s / it)
[19:16:15.580977] Averaged stats: lr: 0.000148  training_loss: 1.5078 (1.5148)  classification_loss: 1.4837 (1.4581)  loss_mask: 0.0195 (0.0567)
[19:16:16.358631] Test:  [  0/157]  eta: 0:02:01  testing_loss: 0.6799 (0.6799)  acc1: 76.5625 (76.5625)  acc5: 96.8750 (96.8750)  time: 0.7736  data: 0.7401  max mem: 4132
[19:16:16.649579] Test:  [ 10/157]  eta: 0:00:14  testing_loss: 0.6866 (0.7051)  acc1: 78.1250 (77.4148)  acc5: 100.0000 (99.4318)  time: 0.0966  data: 0.0675  max mem: 4132
[19:16:16.944593] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.6494 (0.6844)  acc1: 79.6875 (78.0506)  acc5: 100.0000 (99.2560)  time: 0.0291  data: 0.0004  max mem: 4132
[19:16:17.233724] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6919 (0.7030)  acc1: 76.5625 (77.4698)  acc5: 98.4375 (99.0927)  time: 0.0291  data: 0.0004  max mem: 4132
[19:16:17.523622] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6919 (0.7007)  acc1: 76.5625 (77.7439)  acc5: 98.4375 (98.9329)  time: 0.0288  data: 0.0002  max mem: 4132
[19:16:17.821268] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6722 (0.7004)  acc1: 78.1250 (77.9412)  acc5: 98.4375 (98.8358)  time: 0.0292  data: 0.0002  max mem: 4132
[19:16:18.108326] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6957 (0.6979)  acc1: 76.5625 (77.7920)  acc5: 98.4375 (98.7961)  time: 0.0291  data: 0.0002  max mem: 4132
[19:16:18.400462] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6667 (0.6931)  acc1: 78.1250 (77.9269)  acc5: 98.4375 (98.7896)  time: 0.0288  data: 0.0002  max mem: 4132
[19:16:18.687104] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6670 (0.6998)  acc1: 78.1250 (77.6813)  acc5: 98.4375 (98.6690)  time: 0.0288  data: 0.0002  max mem: 4132
[19:16:18.977399] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7135 (0.6980)  acc1: 76.5625 (77.7129)  acc5: 98.4375 (98.6951)  time: 0.0287  data: 0.0002  max mem: 4132
[19:16:19.271861] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.7291 (0.7028)  acc1: 76.5625 (77.6300)  acc5: 98.4375 (98.6850)  time: 0.0291  data: 0.0003  max mem: 4132
[19:16:19.560916] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.7291 (0.7052)  acc1: 76.5625 (77.5901)  acc5: 98.4375 (98.6486)  time: 0.0290  data: 0.0003  max mem: 4132
[19:16:19.858964] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6850 (0.7016)  acc1: 78.1250 (77.6730)  acc5: 98.4375 (98.6570)  time: 0.0292  data: 0.0003  max mem: 4132
[19:16:20.149938] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6963 (0.7017)  acc1: 78.1250 (77.7075)  acc5: 98.4375 (98.6760)  time: 0.0293  data: 0.0002  max mem: 4132
[19:16:20.437863] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7030 (0.7006)  acc1: 78.1250 (77.7815)  acc5: 98.4375 (98.7035)  time: 0.0288  data: 0.0002  max mem: 4132
[19:16:20.721520] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7039 (0.7006)  acc1: 76.5625 (77.7939)  acc5: 98.4375 (98.6858)  time: 0.0284  data: 0.0002  max mem: 4132
[19:16:20.875184] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7221 (0.7031)  acc1: 76.5625 (77.7400)  acc5: 98.4375 (98.7000)  time: 0.0274  data: 0.0002  max mem: 4132
[19:16:21.054331] Test: Total time: 0:00:05 (0.0348 s / it)
[19:16:21.054785] * Acc@1 77.740 Acc@5 98.700 loss 0.703
[19:16:21.055082] Accuracy of the network on the 10000 test images: 77.7%
[19:16:21.055264] Max accuracy: 77.74%
[19:16:21.192742] log_dir: ./output_dir
[19:16:22.134541] Epoch: [47]  [  0/781]  eta: 0:12:14  lr: 0.000148  training_loss: 1.4538 (1.4538)  classification_loss: 1.4372 (1.4372)  loss_mask: 0.0165 (0.0165)  time: 0.9400  data: 0.7484  max mem: 4132
[19:16:25.444436] Epoch: [47]  [ 20/781]  eta: 0:02:33  lr: 0.000148  training_loss: 1.3996 (1.4339)  classification_loss: 1.3883 (1.4177)  loss_mask: 0.0131 (0.0162)  time: 0.1654  data: 0.0002  max mem: 4132
[19:16:28.749981] Epoch: [47]  [ 40/781]  eta: 0:02:16  lr: 0.000148  training_loss: 1.4645 (1.4483)  classification_loss: 1.4552 (1.4345)  loss_mask: 0.0092 (0.0138)  time: 0.1652  data: 0.0004  max mem: 4132
[19:16:32.028917] Epoch: [47]  [ 60/781]  eta: 0:02:07  lr: 0.000148  training_loss: 1.4461 (1.4485)  classification_loss: 1.4386 (1.4358)  loss_mask: 0.0047 (0.0127)  time: 0.1639  data: 0.0002  max mem: 4132
[19:16:35.301261] Epoch: [47]  [ 80/781]  eta: 0:02:02  lr: 0.000148  training_loss: 1.4091 (1.4363)  classification_loss: 1.4042 (1.4246)  loss_mask: 0.0046 (0.0117)  time: 0.1636  data: 0.0002  max mem: 4132
[19:16:38.568307] Epoch: [47]  [100/781]  eta: 0:01:57  lr: 0.000148  training_loss: 1.4401 (1.4373)  classification_loss: 1.4373 (1.4271)  loss_mask: 0.0033 (0.0102)  time: 0.1632  data: 0.0003  max mem: 4132
[19:16:41.842559] Epoch: [47]  [120/781]  eta: 0:01:52  lr: 0.000147  training_loss: 1.3978 (1.4284)  classification_loss: 1.3948 (1.4191)  loss_mask: 0.0033 (0.0094)  time: 0.1636  data: 0.0003  max mem: 4132
[19:16:45.141417] Epoch: [47]  [140/781]  eta: 0:01:48  lr: 0.000147  training_loss: 1.4117 (1.4236)  classification_loss: 1.4057 (1.4145)  loss_mask: 0.0042 (0.0091)  time: 0.1648  data: 0.0003  max mem: 4132
[19:16:48.466369] Epoch: [47]  [160/781]  eta: 0:01:45  lr: 0.000147  training_loss: 1.4353 (1.4261)  classification_loss: 1.4328 (1.4175)  loss_mask: 0.0030 (0.0085)  time: 0.1661  data: 0.0003  max mem: 4132
[19:16:51.834236] Epoch: [47]  [180/781]  eta: 0:01:41  lr: 0.000147  training_loss: 1.3726 (1.4261)  classification_loss: 1.3699 (1.4172)  loss_mask: 0.0032 (0.0089)  time: 0.1683  data: 0.0003  max mem: 4132
[19:16:55.187132] Epoch: [47]  [200/781]  eta: 0:01:38  lr: 0.000147  training_loss: 1.3711 (1.4239)  classification_loss: 1.3542 (1.4140)  loss_mask: 0.0093 (0.0099)  time: 0.1675  data: 0.0003  max mem: 4132
[19:16:58.515365] Epoch: [47]  [220/781]  eta: 0:01:34  lr: 0.000147  training_loss: 1.4523 (1.4272)  classification_loss: 1.4373 (1.4164)  loss_mask: 0.0078 (0.0108)  time: 0.1663  data: 0.0003  max mem: 4132
[19:17:01.837679] Epoch: [47]  [240/781]  eta: 0:01:31  lr: 0.000147  training_loss: 1.4440 (1.4302)  classification_loss: 1.4368 (1.4190)  loss_mask: 0.0074 (0.0112)  time: 0.1660  data: 0.0003  max mem: 4132
[19:17:05.155720] Epoch: [47]  [260/781]  eta: 0:01:27  lr: 0.000147  training_loss: 1.4028 (1.4294)  classification_loss: 1.3871 (1.4184)  loss_mask: 0.0063 (0.0110)  time: 0.1658  data: 0.0003  max mem: 4132
[19:17:08.512398] Epoch: [47]  [280/781]  eta: 0:01:24  lr: 0.000147  training_loss: 1.4329 (1.4309)  classification_loss: 1.3808 (1.4156)  loss_mask: 0.0361 (0.0154)  time: 0.1677  data: 0.0003  max mem: 4132
[19:17:11.790212] Epoch: [47]  [300/781]  eta: 0:01:20  lr: 0.000146  training_loss: 1.4661 (1.4342)  classification_loss: 1.4448 (1.4177)  loss_mask: 0.0178 (0.0166)  time: 0.1638  data: 0.0003  max mem: 4132
[19:17:15.074914] Epoch: [47]  [320/781]  eta: 0:01:17  lr: 0.000146  training_loss: 1.3991 (1.4315)  classification_loss: 1.3140 (1.4140)  loss_mask: 0.0126 (0.0175)  time: 0.1641  data: 0.0003  max mem: 4132
[19:17:18.381126] Epoch: [47]  [340/781]  eta: 0:01:13  lr: 0.000146  training_loss: 1.4317 (1.4309)  classification_loss: 1.4086 (1.4131)  loss_mask: 0.0156 (0.0178)  time: 0.1652  data: 0.0004  max mem: 4132
[19:17:21.709361] Epoch: [47]  [360/781]  eta: 0:01:10  lr: 0.000146  training_loss: 1.4689 (1.4323)  classification_loss: 1.4431 (1.4146)  loss_mask: 0.0071 (0.0176)  time: 0.1663  data: 0.0003  max mem: 4132
[19:17:24.994644] Epoch: [47]  [380/781]  eta: 0:01:07  lr: 0.000146  training_loss: 1.4016 (1.4305)  classification_loss: 1.3972 (1.4136)  loss_mask: 0.0033 (0.0169)  time: 0.1642  data: 0.0003  max mem: 4132
[19:17:28.329034] Epoch: [47]  [400/781]  eta: 0:01:03  lr: 0.000146  training_loss: 1.3714 (1.4273)  classification_loss: 1.3641 (1.4110)  loss_mask: 0.0040 (0.0163)  time: 0.1666  data: 0.0003  max mem: 4132
[19:17:31.633379] Epoch: [47]  [420/781]  eta: 0:01:00  lr: 0.000146  training_loss: 1.4144 (1.4269)  classification_loss: 1.4132 (1.4111)  loss_mask: 0.0024 (0.0158)  time: 0.1651  data: 0.0005  max mem: 4132
[19:17:34.937917] Epoch: [47]  [440/781]  eta: 0:00:56  lr: 0.000146  training_loss: 1.3679 (1.4238)  classification_loss: 1.3649 (1.4087)  loss_mask: 0.0016 (0.0152)  time: 0.1651  data: 0.0003  max mem: 4132
[19:17:38.230038] Epoch: [47]  [460/781]  eta: 0:00:53  lr: 0.000146  training_loss: 1.3985 (1.4221)  classification_loss: 1.3911 (1.4072)  loss_mask: 0.0029 (0.0150)  time: 0.1645  data: 0.0003  max mem: 4132
[19:17:41.520686] Epoch: [47]  [480/781]  eta: 0:00:50  lr: 0.000146  training_loss: 1.4999 (1.4236)  classification_loss: 1.4789 (1.4086)  loss_mask: 0.0044 (0.0150)  time: 0.1644  data: 0.0003  max mem: 4132
[19:17:44.836487] Epoch: [47]  [500/781]  eta: 0:00:46  lr: 0.000145  training_loss: 1.4364 (1.4240)  classification_loss: 1.4352 (1.4094)  loss_mask: 0.0031 (0.0145)  time: 0.1657  data: 0.0003  max mem: 4132
[19:17:48.105633] Epoch: [47]  [520/781]  eta: 0:00:43  lr: 0.000145  training_loss: 1.3585 (1.4227)  classification_loss: 1.3570 (1.4086)  loss_mask: 0.0018 (0.0141)  time: 0.1633  data: 0.0002  max mem: 4132
[19:17:51.413780] Epoch: [47]  [540/781]  eta: 0:00:40  lr: 0.000145  training_loss: 1.4043 (1.4225)  classification_loss: 1.3775 (1.4087)  loss_mask: 0.0013 (0.0138)  time: 0.1653  data: 0.0002  max mem: 4132
[19:17:54.715690] Epoch: [47]  [560/781]  eta: 0:00:36  lr: 0.000145  training_loss: 1.4017 (1.4230)  classification_loss: 1.3879 (1.4092)  loss_mask: 0.0078 (0.0139)  time: 0.1650  data: 0.0003  max mem: 4132
[19:17:58.041663] Epoch: [47]  [580/781]  eta: 0:00:33  lr: 0.000145  training_loss: 1.5172 (1.4264)  classification_loss: 1.4071 (1.4099)  loss_mask: 0.0535 (0.0165)  time: 0.1662  data: 0.0003  max mem: 4132
[19:18:01.353868] Epoch: [47]  [600/781]  eta: 0:00:30  lr: 0.000145  training_loss: 1.4679 (1.4284)  classification_loss: 1.3902 (1.4094)  loss_mask: 0.0866 (0.0191)  time: 0.1655  data: 0.0003  max mem: 4132
[19:18:04.646597] Epoch: [47]  [620/781]  eta: 0:00:26  lr: 0.000145  training_loss: 1.3813 (1.4280)  classification_loss: 1.3337 (1.4082)  loss_mask: 0.0365 (0.0197)  time: 0.1646  data: 0.0002  max mem: 4132
[19:18:07.984527] Epoch: [47]  [640/781]  eta: 0:00:23  lr: 0.000145  training_loss: 1.4199 (1.4285)  classification_loss: 1.4100 (1.4088)  loss_mask: 0.0114 (0.0197)  time: 0.1668  data: 0.0006  max mem: 4132
[19:18:11.278070] Epoch: [47]  [660/781]  eta: 0:00:20  lr: 0.000145  training_loss: 1.3912 (1.4275)  classification_loss: 1.3647 (1.4081)  loss_mask: 0.0077 (0.0194)  time: 0.1646  data: 0.0003  max mem: 4132
[19:18:14.561445] Epoch: [47]  [680/781]  eta: 0:00:16  lr: 0.000144  training_loss: 1.4357 (1.4272)  classification_loss: 1.4265 (1.4081)  loss_mask: 0.0051 (0.0190)  time: 0.1641  data: 0.0003  max mem: 4132
[19:18:17.828501] Epoch: [47]  [700/781]  eta: 0:00:13  lr: 0.000144  training_loss: 1.4253 (1.4279)  classification_loss: 1.4217 (1.4092)  loss_mask: 0.0038 (0.0187)  time: 0.1633  data: 0.0002  max mem: 4132
[19:18:21.108625] Epoch: [47]  [720/781]  eta: 0:00:10  lr: 0.000144  training_loss: 1.3904 (1.4275)  classification_loss: 1.3814 (1.4090)  loss_mask: 0.0046 (0.0185)  time: 0.1639  data: 0.0002  max mem: 4132
[19:18:24.393036] Epoch: [47]  [740/781]  eta: 0:00:06  lr: 0.000144  training_loss: 1.3657 (1.4265)  classification_loss: 1.3595 (1.4082)  loss_mask: 0.0038 (0.0182)  time: 0.1641  data: 0.0003  max mem: 4132
[19:18:27.712791] Epoch: [47]  [760/781]  eta: 0:00:03  lr: 0.000144  training_loss: 1.4109 (1.4256)  classification_loss: 1.4085 (1.4078)  loss_mask: 0.0031 (0.0178)  time: 0.1658  data: 0.0003  max mem: 4132
[19:18:31.003104] Epoch: [47]  [780/781]  eta: 0:00:00  lr: 0.000144  training_loss: 1.4000 (1.4252)  classification_loss: 1.3988 (1.4078)  loss_mask: 0.0016 (0.0174)  time: 0.1644  data: 0.0003  max mem: 4132
[19:18:31.201692] Epoch: [47] Total time: 0:02:10 (0.1665 s / it)
[19:18:31.202693] Averaged stats: lr: 0.000144  training_loss: 1.4000 (1.4252)  classification_loss: 1.3988 (1.4078)  loss_mask: 0.0016 (0.0174)
[19:18:31.968836] Test:  [  0/157]  eta: 0:01:59  testing_loss: 0.6017 (0.6017)  acc1: 82.8125 (82.8125)  acc5: 98.4375 (98.4375)  time: 0.7615  data: 0.6998  max mem: 4132
[19:18:32.264857] Test:  [ 10/157]  eta: 0:00:14  testing_loss: 0.6526 (0.6910)  acc1: 81.2500 (78.9773)  acc5: 100.0000 (99.2898)  time: 0.0959  data: 0.0639  max mem: 4132
[19:18:32.559632] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.6161 (0.6640)  acc1: 79.6875 (79.2411)  acc5: 100.0000 (99.2560)  time: 0.0293  data: 0.0003  max mem: 4132
[19:18:32.853802] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6699 (0.6815)  acc1: 79.6875 (78.9315)  acc5: 98.4375 (98.8911)  time: 0.0292  data: 0.0003  max mem: 4132
[19:18:33.150294] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6714 (0.6789)  acc1: 79.6875 (78.8491)  acc5: 98.4375 (98.7805)  time: 0.0294  data: 0.0003  max mem: 4132
[19:18:33.447594] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6463 (0.6722)  acc1: 81.2500 (79.4730)  acc5: 98.4375 (98.8051)  time: 0.0295  data: 0.0003  max mem: 4132
[19:18:33.741713] Test:  [ 60/157]  eta: 0:00:04  testing_loss: 0.6530 (0.6702)  acc1: 81.2500 (79.3801)  acc5: 98.4375 (98.7961)  time: 0.0293  data: 0.0003  max mem: 4132
[19:18:34.033966] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6201 (0.6612)  acc1: 79.6875 (79.5995)  acc5: 98.4375 (98.8336)  time: 0.0291  data: 0.0003  max mem: 4132
[19:18:34.329477] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6261 (0.6695)  acc1: 78.1250 (79.2631)  acc5: 98.4375 (98.7076)  time: 0.0292  data: 0.0003  max mem: 4132
[19:18:34.623790] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6873 (0.6676)  acc1: 78.1250 (79.2239)  acc5: 98.4375 (98.7466)  time: 0.0293  data: 0.0003  max mem: 4132
[19:18:34.922272] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.6873 (0.6737)  acc1: 78.1250 (78.8521)  acc5: 98.4375 (98.7005)  time: 0.0294  data: 0.0003  max mem: 4132
[19:18:35.223020] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.7247 (0.6736)  acc1: 75.0000 (78.8711)  acc5: 98.4375 (98.7190)  time: 0.0298  data: 0.0003  max mem: 4132
[19:18:35.532386] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6724 (0.6705)  acc1: 78.1250 (78.9256)  acc5: 98.4375 (98.7087)  time: 0.0303  data: 0.0003  max mem: 4132
[19:18:35.821689] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6456 (0.6695)  acc1: 78.1250 (78.8287)  acc5: 98.4375 (98.7595)  time: 0.0298  data: 0.0003  max mem: 4132
[19:18:36.112869] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6690 (0.6696)  acc1: 79.6875 (78.9450)  acc5: 100.0000 (98.7921)  time: 0.0288  data: 0.0003  max mem: 4132
[19:18:36.399001] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6690 (0.6682)  acc1: 79.6875 (79.1080)  acc5: 100.0000 (98.8307)  time: 0.0287  data: 0.0002  max mem: 4132
[19:18:36.554982] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6614 (0.6684)  acc1: 81.2500 (79.1100)  acc5: 100.0000 (98.8600)  time: 0.0277  data: 0.0002  max mem: 4132
[19:18:36.769245] Test: Total time: 0:00:05 (0.0354 s / it)
[19:18:36.770120] * Acc@1 79.110 Acc@5 98.860 loss 0.668
[19:18:36.770624] Accuracy of the network on the 10000 test images: 79.1%
[19:18:36.770950] Max accuracy: 79.11%
[19:18:36.922134] log_dir: ./output_dir
[19:18:37.919382] Epoch: [48]  [  0/781]  eta: 0:12:56  lr: 0.000144  training_loss: 1.3163 (1.3163)  classification_loss: 1.3088 (1.3088)  loss_mask: 0.0076 (0.0076)  time: 0.9944  data: 0.8246  max mem: 4132
[19:18:41.281940] Epoch: [48]  [ 20/781]  eta: 0:02:37  lr: 0.000144  training_loss: 1.3965 (1.3834)  classification_loss: 1.3945 (1.3815)  loss_mask: 0.0014 (0.0019)  time: 0.1680  data: 0.0004  max mem: 4132
[19:18:44.631667] Epoch: [48]  [ 40/781]  eta: 0:02:19  lr: 0.000144  training_loss: 1.4717 (1.4040)  classification_loss: 1.4704 (1.4024)  loss_mask: 0.0009 (0.0016)  time: 0.1674  data: 0.0003  max mem: 4132
[19:18:47.938047] Epoch: [48]  [ 60/781]  eta: 0:02:10  lr: 0.000144  training_loss: 1.3874 (1.4105)  classification_loss: 1.3858 (1.4089)  loss_mask: 0.0011 (0.0015)  time: 0.1652  data: 0.0003  max mem: 4132
[19:18:51.219083] Epoch: [48]  [ 80/781]  eta: 0:02:03  lr: 0.000144  training_loss: 1.3518 (1.4028)  classification_loss: 1.3501 (1.4014)  loss_mask: 0.0011 (0.0014)  time: 0.1640  data: 0.0003  max mem: 4132
[19:18:54.524680] Epoch: [48]  [100/781]  eta: 0:01:58  lr: 0.000143  training_loss: 1.3537 (1.3975)  classification_loss: 1.3529 (1.3954)  loss_mask: 0.0016 (0.0021)  time: 0.1652  data: 0.0003  max mem: 4132
[19:18:57.818456] Epoch: [48]  [120/781]  eta: 0:01:54  lr: 0.000143  training_loss: 1.3742 (1.3939)  classification_loss: 1.3732 (1.3919)  loss_mask: 0.0012 (0.0020)  time: 0.1646  data: 0.0002  max mem: 4132
[19:19:01.178143] Epoch: [48]  [140/781]  eta: 0:01:50  lr: 0.000143  training_loss: 1.3511 (1.3873)  classification_loss: 1.3500 (1.3848)  loss_mask: 0.0011 (0.0025)  time: 0.1679  data: 0.0003  max mem: 4132
[19:19:04.480043] Epoch: [48]  [160/781]  eta: 0:01:46  lr: 0.000143  training_loss: 1.3534 (1.3846)  classification_loss: 1.3367 (1.3800)  loss_mask: 0.0114 (0.0045)  time: 0.1650  data: 0.0003  max mem: 4132
[19:19:07.805093] Epoch: [48]  [180/781]  eta: 0:01:42  lr: 0.000143  training_loss: 1.4638 (1.4004)  classification_loss: 1.3573 (1.3827)  loss_mask: 0.0813 (0.0177)  time: 0.1662  data: 0.0003  max mem: 4132
[19:19:11.073853] Epoch: [48]  [200/781]  eta: 0:01:38  lr: 0.000143  training_loss: 1.4245 (1.4031)  classification_loss: 1.3893 (1.3818)  loss_mask: 0.0574 (0.0214)  time: 0.1633  data: 0.0002  max mem: 4132
[19:19:14.424337] Epoch: [48]  [220/781]  eta: 0:01:35  lr: 0.000143  training_loss: 1.4422 (1.4034)  classification_loss: 1.3746 (1.3813)  loss_mask: 0.0274 (0.0221)  time: 0.1674  data: 0.0002  max mem: 4132
[19:19:17.723185] Epoch: [48]  [240/781]  eta: 0:01:31  lr: 0.000143  training_loss: 1.3637 (1.4000)  classification_loss: 1.3605 (1.3788)  loss_mask: 0.0104 (0.0213)  time: 0.1649  data: 0.0002  max mem: 4132
[19:19:21.029440] Epoch: [48]  [260/781]  eta: 0:01:27  lr: 0.000143  training_loss: 1.3700 (1.3976)  classification_loss: 1.3575 (1.3773)  loss_mask: 0.0077 (0.0203)  time: 0.1652  data: 0.0002  max mem: 4132
[19:19:24.392823] Epoch: [48]  [280/781]  eta: 0:01:24  lr: 0.000142  training_loss: 1.3517 (1.3964)  classification_loss: 1.3440 (1.3770)  loss_mask: 0.0038 (0.0194)  time: 0.1681  data: 0.0004  max mem: 4132
[19:19:27.732370] Epoch: [48]  [300/781]  eta: 0:01:21  lr: 0.000142  training_loss: 1.3844 (1.3955)  classification_loss: 1.3833 (1.3772)  loss_mask: 0.0023 (0.0183)  time: 0.1669  data: 0.0003  max mem: 4132
[19:19:31.030197] Epoch: [48]  [320/781]  eta: 0:01:17  lr: 0.000142  training_loss: 1.3635 (1.3943)  classification_loss: 1.3622 (1.3767)  loss_mask: 0.0017 (0.0176)  time: 0.1648  data: 0.0002  max mem: 4132
[19:19:34.319286] Epoch: [48]  [340/781]  eta: 0:01:14  lr: 0.000142  training_loss: 1.3924 (1.3957)  classification_loss: 1.3908 (1.3784)  loss_mask: 0.0046 (0.0172)  time: 0.1644  data: 0.0003  max mem: 4132
[19:19:37.613460] Epoch: [48]  [360/781]  eta: 0:01:10  lr: 0.000142  training_loss: 1.4095 (1.3973)  classification_loss: 1.4031 (1.3806)  loss_mask: 0.0050 (0.0168)  time: 0.1646  data: 0.0002  max mem: 4132
[19:19:40.904145] Epoch: [48]  [380/781]  eta: 0:01:07  lr: 0.000142  training_loss: 1.3678 (1.3961)  classification_loss: 1.3625 (1.3798)  loss_mask: 0.0032 (0.0163)  time: 0.1644  data: 0.0002  max mem: 4132
[19:19:44.208463] Epoch: [48]  [400/781]  eta: 0:01:03  lr: 0.000142  training_loss: 1.3914 (1.3966)  classification_loss: 1.3766 (1.3797)  loss_mask: 0.0058 (0.0169)  time: 0.1651  data: 0.0003  max mem: 4132
[19:19:47.495440] Epoch: [48]  [420/781]  eta: 0:01:00  lr: 0.000142  training_loss: 1.4690 (1.4004)  classification_loss: 1.4112 (1.3824)  loss_mask: 0.0280 (0.0180)  time: 0.1643  data: 0.0003  max mem: 4132
[19:19:50.775809] Epoch: [48]  [440/781]  eta: 0:00:57  lr: 0.000142  training_loss: 1.3942 (1.4001)  classification_loss: 1.3513 (1.3815)  loss_mask: 0.0134 (0.0185)  time: 0.1639  data: 0.0004  max mem: 4132
[19:19:54.086036] Epoch: [48]  [460/781]  eta: 0:00:53  lr: 0.000142  training_loss: 1.4139 (1.4016)  classification_loss: 1.3345 (1.3794)  loss_mask: 0.0270 (0.0222)  time: 0.1654  data: 0.0003  max mem: 4132
[19:19:57.376892] Epoch: [48]  [480/781]  eta: 0:00:50  lr: 0.000141  training_loss: 1.4617 (1.4039)  classification_loss: 1.4281 (1.3807)  loss_mask: 0.0261 (0.0232)  time: 0.1645  data: 0.0002  max mem: 4132
[19:20:00.644644] Epoch: [48]  [500/781]  eta: 0:00:46  lr: 0.000141  training_loss: 1.4066 (1.4040)  classification_loss: 1.3776 (1.3808)  loss_mask: 0.0188 (0.0232)  time: 0.1633  data: 0.0003  max mem: 4132
[19:20:03.937214] Epoch: [48]  [520/781]  eta: 0:00:43  lr: 0.000141  training_loss: 1.3925 (1.4045)  classification_loss: 1.3847 (1.3815)  loss_mask: 0.0078 (0.0229)  time: 0.1645  data: 0.0002  max mem: 4132
[19:20:07.274391] Epoch: [48]  [540/781]  eta: 0:00:40  lr: 0.000141  training_loss: 1.4123 (1.4045)  classification_loss: 1.4008 (1.3820)  loss_mask: 0.0087 (0.0224)  time: 0.1668  data: 0.0005  max mem: 4132
[19:20:10.634211] Epoch: [48]  [560/781]  eta: 0:00:36  lr: 0.000141  training_loss: 1.3951 (1.4032)  classification_loss: 1.3915 (1.3811)  loss_mask: 0.0042 (0.0221)  time: 0.1679  data: 0.0005  max mem: 4132
[19:20:13.987805] Epoch: [48]  [580/781]  eta: 0:00:33  lr: 0.000141  training_loss: 1.3683 (1.4028)  classification_loss: 1.3595 (1.3810)  loss_mask: 0.0038 (0.0218)  time: 0.1676  data: 0.0003  max mem: 4132
[19:20:17.301777] Epoch: [48]  [600/781]  eta: 0:00:30  lr: 0.000141  training_loss: 1.3217 (1.4012)  classification_loss: 1.3194 (1.3796)  loss_mask: 0.0044 (0.0216)  time: 0.1656  data: 0.0003  max mem: 4132
[19:20:20.602607] Epoch: [48]  [620/781]  eta: 0:00:26  lr: 0.000141  training_loss: 1.3697 (1.4014)  classification_loss: 1.3667 (1.3804)  loss_mask: 0.0031 (0.0210)  time: 0.1649  data: 0.0003  max mem: 4132
[19:20:23.939728] Epoch: [48]  [640/781]  eta: 0:00:23  lr: 0.000141  training_loss: 1.3974 (1.4014)  classification_loss: 1.3717 (1.3807)  loss_mask: 0.0024 (0.0206)  time: 0.1668  data: 0.0003  max mem: 4132
[19:20:27.232393] Epoch: [48]  [660/781]  eta: 0:00:20  lr: 0.000141  training_loss: 1.4288 (1.4021)  classification_loss: 1.4142 (1.3818)  loss_mask: 0.0020 (0.0203)  time: 0.1645  data: 0.0003  max mem: 4132
[19:20:30.546401] Epoch: [48]  [680/781]  eta: 0:00:16  lr: 0.000140  training_loss: 1.4015 (1.4027)  classification_loss: 1.3919 (1.3826)  loss_mask: 0.0033 (0.0201)  time: 0.1656  data: 0.0003  max mem: 4132
[19:20:33.831974] Epoch: [48]  [700/781]  eta: 0:00:13  lr: 0.000140  training_loss: 1.4279 (1.4038)  classification_loss: 1.3707 (1.3826)  loss_mask: 0.0356 (0.0212)  time: 0.1642  data: 0.0002  max mem: 4132
[19:20:37.126866] Epoch: [48]  [720/781]  eta: 0:00:10  lr: 0.000140  training_loss: 1.4930 (1.4059)  classification_loss: 1.4197 (1.3843)  loss_mask: 0.0142 (0.0217)  time: 0.1646  data: 0.0003  max mem: 4132
[19:20:40.452455] Epoch: [48]  [740/781]  eta: 0:00:06  lr: 0.000140  training_loss: 1.3872 (1.4056)  classification_loss: 1.3801 (1.3841)  loss_mask: 0.0094 (0.0216)  time: 0.1662  data: 0.0004  max mem: 4132
[19:20:43.763080] Epoch: [48]  [760/781]  eta: 0:00:03  lr: 0.000140  training_loss: 1.4480 (1.4063)  classification_loss: 1.4271 (1.3846)  loss_mask: 0.0140 (0.0217)  time: 0.1654  data: 0.0002  max mem: 4132
[19:20:47.028061] Epoch: [48]  [780/781]  eta: 0:00:00  lr: 0.000140  training_loss: 1.4159 (1.4073)  classification_loss: 1.4101 (1.3855)  loss_mask: 0.0120 (0.0218)  time: 0.1632  data: 0.0003  max mem: 4132
[19:20:47.192053] Epoch: [48] Total time: 0:02:10 (0.1668 s / it)
[19:20:47.192570] Averaged stats: lr: 0.000140  training_loss: 1.4159 (1.4073)  classification_loss: 1.4101 (1.3855)  loss_mask: 0.0120 (0.0218)
[19:20:47.916235] Test:  [  0/157]  eta: 0:01:52  testing_loss: 0.6722 (0.6722)  acc1: 81.2500 (81.2500)  acc5: 95.3125 (95.3125)  time: 0.7192  data: 0.6661  max mem: 4132
[19:20:48.208896] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.6570 (0.6532)  acc1: 79.6875 (78.6932)  acc5: 100.0000 (99.0057)  time: 0.0917  data: 0.0608  max mem: 4132
[19:20:48.495527] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5993 (0.6357)  acc1: 79.6875 (79.6875)  acc5: 100.0000 (99.0327)  time: 0.0287  data: 0.0002  max mem: 4132
[19:20:48.782896] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6672 (0.6599)  acc1: 79.6875 (78.8306)  acc5: 98.4375 (98.8407)  time: 0.0285  data: 0.0002  max mem: 4132
[19:20:49.069928] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6668 (0.6607)  acc1: 78.1250 (79.0015)  acc5: 98.4375 (98.7043)  time: 0.0286  data: 0.0002  max mem: 4132
[19:20:49.355877] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6519 (0.6564)  acc1: 79.6875 (79.3811)  acc5: 98.4375 (98.6826)  time: 0.0285  data: 0.0002  max mem: 4132
[19:20:49.641900] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6344 (0.6537)  acc1: 79.6875 (79.2777)  acc5: 98.4375 (98.7961)  time: 0.0285  data: 0.0002  max mem: 4132
[19:20:49.928532] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6193 (0.6483)  acc1: 79.6875 (79.5114)  acc5: 100.0000 (98.8116)  time: 0.0285  data: 0.0002  max mem: 4132
[19:20:50.214799] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6365 (0.6576)  acc1: 79.6875 (79.2438)  acc5: 98.4375 (98.6690)  time: 0.0285  data: 0.0002  max mem: 4132
[19:20:50.500193] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7043 (0.6565)  acc1: 79.6875 (79.3613)  acc5: 98.4375 (98.6435)  time: 0.0284  data: 0.0002  max mem: 4132
[19:20:50.787113] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.6937 (0.6641)  acc1: 76.5625 (78.9295)  acc5: 98.4375 (98.6077)  time: 0.0284  data: 0.0002  max mem: 4132
[19:20:51.072762] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6937 (0.6642)  acc1: 76.5625 (78.9696)  acc5: 98.4375 (98.6205)  time: 0.0284  data: 0.0002  max mem: 4132
[19:20:51.358603] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6541 (0.6619)  acc1: 79.6875 (79.0031)  acc5: 98.4375 (98.6183)  time: 0.0284  data: 0.0002  max mem: 4132
[19:20:51.646446] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6285 (0.6603)  acc1: 79.6875 (79.0196)  acc5: 98.4375 (98.6283)  time: 0.0286  data: 0.0002  max mem: 4132
[19:20:51.931503] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6543 (0.6599)  acc1: 79.6875 (79.0891)  acc5: 98.4375 (98.6702)  time: 0.0285  data: 0.0002  max mem: 4132
[19:20:52.215588] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6609 (0.6586)  acc1: 78.1250 (79.0666)  acc5: 98.4375 (98.6858)  time: 0.0283  data: 0.0002  max mem: 4132
[19:20:52.369663] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6362 (0.6583)  acc1: 78.1250 (79.0800)  acc5: 98.4375 (98.6800)  time: 0.0274  data: 0.0002  max mem: 4132
[19:20:52.546787] Test: Total time: 0:00:05 (0.0341 s / it)
[19:20:52.547309] * Acc@1 79.080 Acc@5 98.680 loss 0.658
[19:20:52.547588] Accuracy of the network on the 10000 test images: 79.1%
[19:20:52.547778] Max accuracy: 79.11%
[19:20:52.684264] log_dir: ./output_dir
[19:20:53.677201] Epoch: [49]  [  0/781]  eta: 0:12:53  lr: 0.000140  training_loss: 1.3745 (1.3745)  classification_loss: 1.3476 (1.3476)  loss_mask: 0.0269 (0.0269)  time: 0.9909  data: 0.7910  max mem: 4132
[19:20:57.012281] Epoch: [49]  [ 20/781]  eta: 0:02:36  lr: 0.000140  training_loss: 1.3846 (1.3848)  classification_loss: 1.3485 (1.3646)  loss_mask: 0.0053 (0.0202)  time: 0.1666  data: 0.0003  max mem: 4132
[19:21:00.327713] Epoch: [49]  [ 40/781]  eta: 0:02:18  lr: 0.000140  training_loss: 1.3904 (1.3950)  classification_loss: 1.3795 (1.3739)  loss_mask: 0.0161 (0.0211)  time: 0.1657  data: 0.0003  max mem: 4132
[19:21:03.624023] Epoch: [49]  [ 60/781]  eta: 0:02:09  lr: 0.000140  training_loss: 1.3813 (1.4012)  classification_loss: 1.3696 (1.3835)  loss_mask: 0.0061 (0.0177)  time: 0.1647  data: 0.0002  max mem: 4132
[19:21:06.915428] Epoch: [49]  [ 80/781]  eta: 0:02:03  lr: 0.000139  training_loss: 1.3853 (1.3986)  classification_loss: 1.3842 (1.3846)  loss_mask: 0.0017 (0.0140)  time: 0.1645  data: 0.0003  max mem: 4132
[19:21:10.209001] Epoch: [49]  [100/781]  eta: 0:01:58  lr: 0.000139  training_loss: 1.3744 (1.3938)  classification_loss: 1.3450 (1.3801)  loss_mask: 0.0020 (0.0137)  time: 0.1646  data: 0.0002  max mem: 4132
[19:21:13.519643] Epoch: [49]  [120/781]  eta: 0:01:53  lr: 0.000139  training_loss: 1.3537 (1.3899)  classification_loss: 1.3492 (1.3774)  loss_mask: 0.0034 (0.0125)  time: 0.1654  data: 0.0004  max mem: 4132
[19:21:16.847750] Epoch: [49]  [140/781]  eta: 0:01:49  lr: 0.000139  training_loss: 1.4095 (1.3916)  classification_loss: 1.3559 (1.3786)  loss_mask: 0.0049 (0.0131)  time: 0.1663  data: 0.0003  max mem: 4132
[19:21:20.175623] Epoch: [49]  [160/781]  eta: 0:01:45  lr: 0.000139  training_loss: 1.3880 (1.3890)  classification_loss: 1.3732 (1.3770)  loss_mask: 0.0031 (0.0120)  time: 0.1663  data: 0.0004  max mem: 4132
[19:21:23.491007] Epoch: [49]  [180/781]  eta: 0:01:42  lr: 0.000139  training_loss: 1.3734 (1.3895)  classification_loss: 1.3712 (1.3778)  loss_mask: 0.0022 (0.0117)  time: 0.1657  data: 0.0003  max mem: 4132
[19:21:26.799310] Epoch: [49]  [200/781]  eta: 0:01:38  lr: 0.000139  training_loss: 1.3976 (1.3947)  classification_loss: 1.3461 (1.3775)  loss_mask: 0.0391 (0.0172)  time: 0.1653  data: 0.0003  max mem: 4132
[19:21:30.105177] Epoch: [49]  [220/781]  eta: 0:01:34  lr: 0.000139  training_loss: 1.3788 (1.3950)  classification_loss: 1.3608 (1.3775)  loss_mask: 0.0133 (0.0175)  time: 0.1652  data: 0.0002  max mem: 4132
[19:21:33.390085] Epoch: [49]  [240/781]  eta: 0:01:31  lr: 0.000139  training_loss: 1.4390 (1.3972)  classification_loss: 1.4340 (1.3804)  loss_mask: 0.0057 (0.0168)  time: 0.1642  data: 0.0004  max mem: 4132
[19:21:36.663247] Epoch: [49]  [260/781]  eta: 0:01:27  lr: 0.000139  training_loss: 1.3764 (1.3977)  classification_loss: 1.3715 (1.3811)  loss_mask: 0.0048 (0.0166)  time: 0.1636  data: 0.0002  max mem: 4132
[19:21:40.002500] Epoch: [49]  [280/781]  eta: 0:01:24  lr: 0.000138  training_loss: 1.4200 (1.3993)  classification_loss: 1.3925 (1.3818)  loss_mask: 0.0072 (0.0175)  time: 0.1669  data: 0.0003  max mem: 4132
[19:21:43.316629] Epoch: [49]  [300/781]  eta: 0:01:20  lr: 0.000138  training_loss: 3.2876 (1.5186)  classification_loss: 1.7276 (1.4030)  loss_mask: 1.5481 (0.1156)  time: 0.1656  data: 0.0002  max mem: 4132
[19:21:46.613600] Epoch: [49]  [320/781]  eta: 0:01:17  lr: 0.000138  training_loss: 3.1300 (1.6204)  classification_loss: 1.7159 (1.4207)  loss_mask: 1.4414 (0.1998)  time: 0.1648  data: 0.0002  max mem: 4132
[19:21:49.926497] Epoch: [49]  [340/781]  eta: 0:01:13  lr: 0.000138  training_loss: 3.0072 (1.7001)  classification_loss: 1.6061 (1.4318)  loss_mask: 1.3561 (0.2684)  time: 0.1655  data: 0.0003  max mem: 4132
[19:21:53.258420] Epoch: [49]  [360/781]  eta: 0:01:10  lr: 0.000138  training_loss: 2.7614 (1.7610)  classification_loss: 1.5330 (1.4382)  loss_mask: 1.2446 (0.3228)  time: 0.1665  data: 0.0003  max mem: 4132
[19:21:56.602579] Epoch: [49]  [380/781]  eta: 0:01:07  lr: 0.000138  training_loss: 2.5457 (1.8049)  classification_loss: 1.4570 (1.4405)  loss_mask: 1.1202 (0.3644)  time: 0.1671  data: 0.0003  max mem: 4132
[19:21:59.897033] Epoch: [49]  [400/781]  eta: 0:01:03  lr: 0.000138  training_loss: 2.4403 (1.8376)  classification_loss: 1.4703 (1.4424)  loss_mask: 0.9807 (0.3952)  time: 0.1646  data: 0.0003  max mem: 4132
[19:22:03.215081] Epoch: [49]  [420/781]  eta: 0:01:00  lr: 0.000138  training_loss: 2.3688 (1.8640)  classification_loss: 1.4759 (1.4433)  loss_mask: 0.8890 (0.4207)  time: 0.1658  data: 0.0003  max mem: 4132
[19:22:06.532186] Epoch: [49]  [440/781]  eta: 0:00:57  lr: 0.000138  training_loss: 2.2297 (1.8828)  classification_loss: 1.4104 (1.4435)  loss_mask: 0.8278 (0.4393)  time: 0.1658  data: 0.0005  max mem: 4132
[19:22:09.874050] Epoch: [49]  [460/781]  eta: 0:00:53  lr: 0.000137  training_loss: 2.1290 (1.8949)  classification_loss: 1.3908 (1.4419)  loss_mask: 0.7396 (0.4530)  time: 0.1670  data: 0.0003  max mem: 4132
[19:22:13.181610] Epoch: [49]  [480/781]  eta: 0:00:50  lr: 0.000137  training_loss: 2.1616 (1.9074)  classification_loss: 1.4167 (1.4420)  loss_mask: 0.7353 (0.4654)  time: 0.1653  data: 0.0003  max mem: 4132
[19:22:16.490976] Epoch: [49]  [500/781]  eta: 0:00:46  lr: 0.000137  training_loss: 1.9773 (1.9113)  classification_loss: 1.3812 (1.4404)  loss_mask: 0.5939 (0.4709)  time: 0.1654  data: 0.0002  max mem: 4132
[19:22:19.808434] Epoch: [49]  [520/781]  eta: 0:00:43  lr: 0.000137  training_loss: 1.9057 (1.9134)  classification_loss: 1.4316 (1.4405)  loss_mask: 0.5217 (0.4729)  time: 0.1658  data: 0.0003  max mem: 4132
[19:22:23.091852] Epoch: [49]  [540/781]  eta: 0:00:40  lr: 0.000137  training_loss: 1.8933 (1.9145)  classification_loss: 1.4085 (1.4404)  loss_mask: 0.4493 (0.4741)  time: 0.1641  data: 0.0003  max mem: 4132
[19:22:26.384161] Epoch: [49]  [560/781]  eta: 0:00:36  lr: 0.000137  training_loss: 1.8194 (1.9126)  classification_loss: 1.4341 (1.4399)  loss_mask: 0.4050 (0.4727)  time: 0.1645  data: 0.0002  max mem: 4132
[19:22:29.663323] Epoch: [49]  [580/781]  eta: 0:00:33  lr: 0.000137  training_loss: 1.7739 (1.9098)  classification_loss: 1.4003 (1.4390)  loss_mask: 0.3952 (0.4708)  time: 0.1638  data: 0.0002  max mem: 4132
[19:22:32.967752] Epoch: [49]  [600/781]  eta: 0:00:30  lr: 0.000137  training_loss: 1.8225 (1.9068)  classification_loss: 1.3979 (1.4375)  loss_mask: 0.3970 (0.4694)  time: 0.1651  data: 0.0003  max mem: 4132
[19:22:36.282352] Epoch: [49]  [620/781]  eta: 0:00:26  lr: 0.000137  training_loss: 1.7348 (1.9012)  classification_loss: 1.3875 (1.4360)  loss_mask: 0.3401 (0.4652)  time: 0.1656  data: 0.0002  max mem: 4132
[19:22:39.569725] Epoch: [49]  [640/781]  eta: 0:00:23  lr: 0.000137  training_loss: 1.7700 (1.8978)  classification_loss: 1.4102 (1.4352)  loss_mask: 0.3345 (0.4626)  time: 0.1643  data: 0.0003  max mem: 4132
[19:22:42.832861] Epoch: [49]  [660/781]  eta: 0:00:20  lr: 0.000136  training_loss: 1.7030 (1.8918)  classification_loss: 1.4304 (1.4351)  loss_mask: 0.2582 (0.4567)  time: 0.1630  data: 0.0002  max mem: 4132
[19:22:46.091835] Epoch: [49]  [680/781]  eta: 0:00:16  lr: 0.000136  training_loss: 1.6176 (1.8839)  classification_loss: 1.4050 (1.4341)  loss_mask: 0.2025 (0.4498)  time: 0.1629  data: 0.0004  max mem: 4132
[19:22:49.332363] Epoch: [49]  [700/781]  eta: 0:00:13  lr: 0.000136  training_loss: 1.7101 (1.8782)  classification_loss: 1.4160 (1.4335)  loss_mask: 0.2592 (0.4447)  time: 0.1619  data: 0.0002  max mem: 4132
[19:22:52.616874] Epoch: [49]  [720/781]  eta: 0:00:10  lr: 0.000136  training_loss: 1.5857 (1.8710)  classification_loss: 1.3736 (1.4321)  loss_mask: 0.1851 (0.4389)  time: 0.1642  data: 0.0002  max mem: 4132
[19:22:55.890730] Epoch: [49]  [740/781]  eta: 0:00:06  lr: 0.000136  training_loss: 1.6166 (1.8643)  classification_loss: 1.3860 (1.4311)  loss_mask: 0.2306 (0.4332)  time: 0.1636  data: 0.0002  max mem: 4132
[19:22:59.170973] Epoch: [49]  [760/781]  eta: 0:00:03  lr: 0.000136  training_loss: 1.5662 (1.8572)  classification_loss: 1.3566 (1.4300)  loss_mask: 0.1876 (0.4271)  time: 0.1639  data: 0.0002  max mem: 4132
[19:23:02.410516] Epoch: [49]  [780/781]  eta: 0:00:00  lr: 0.000136  training_loss: 1.5561 (1.8507)  classification_loss: 1.4080 (1.4299)  loss_mask: 0.1651 (0.4208)  time: 0.1619  data: 0.0002  max mem: 4132
[19:23:02.585437] Epoch: [49] Total time: 0:02:09 (0.1663 s / it)
[19:23:02.585926] Averaged stats: lr: 0.000136  training_loss: 1.5561 (1.8507)  classification_loss: 1.4080 (1.4299)  loss_mask: 0.1651 (0.4208)
[19:23:03.281795] Test:  [  0/157]  eta: 0:01:48  testing_loss: 0.6468 (0.6468)  acc1: 84.3750 (84.3750)  acc5: 96.8750 (96.8750)  time: 0.6909  data: 0.6586  max mem: 4132
[19:23:03.572788] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.6468 (0.6711)  acc1: 79.6875 (79.6875)  acc5: 98.4375 (98.8636)  time: 0.0890  data: 0.0601  max mem: 4132
[19:23:03.860747] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.6388 (0.6486)  acc1: 79.6875 (80.0595)  acc5: 100.0000 (98.9583)  time: 0.0287  data: 0.0002  max mem: 4132
[19:23:04.146431] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6608 (0.6682)  acc1: 79.6875 (79.2843)  acc5: 98.4375 (98.8911)  time: 0.0285  data: 0.0002  max mem: 4132
[19:23:04.437396] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6728 (0.6680)  acc1: 78.1250 (79.4588)  acc5: 98.4375 (98.8186)  time: 0.0287  data: 0.0002  max mem: 4132
[19:23:04.725540] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6635 (0.6673)  acc1: 79.6875 (79.6569)  acc5: 98.4375 (98.7745)  time: 0.0287  data: 0.0002  max mem: 4132
[19:23:05.010618] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6689 (0.6631)  acc1: 79.6875 (79.5338)  acc5: 98.4375 (98.8473)  time: 0.0285  data: 0.0002  max mem: 4132
[19:23:05.293810] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6314 (0.6572)  acc1: 78.1250 (79.6435)  acc5: 100.0000 (98.9437)  time: 0.0283  data: 0.0002  max mem: 4132
[19:23:05.577674] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6460 (0.6676)  acc1: 78.1250 (79.2245)  acc5: 100.0000 (98.8812)  time: 0.0282  data: 0.0002  max mem: 4132
[19:23:05.868314] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6780 (0.6650)  acc1: 78.1250 (79.3784)  acc5: 98.4375 (98.9183)  time: 0.0286  data: 0.0002  max mem: 4132
[19:23:06.167954] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.6780 (0.6709)  acc1: 78.1250 (79.1925)  acc5: 98.4375 (98.9016)  time: 0.0293  data: 0.0002  max mem: 4132
[19:23:06.457177] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6809 (0.6723)  acc1: 78.1250 (79.0822)  acc5: 98.4375 (98.8739)  time: 0.0292  data: 0.0002  max mem: 4132
[19:23:06.743724] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6446 (0.6679)  acc1: 79.6875 (79.2355)  acc5: 100.0000 (98.9282)  time: 0.0286  data: 0.0002  max mem: 4132
[19:23:07.031102] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6310 (0.6672)  acc1: 79.6875 (79.0553)  acc5: 100.0000 (98.9504)  time: 0.0285  data: 0.0002  max mem: 4132
[19:23:07.318196] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6715 (0.6668)  acc1: 78.1250 (79.0115)  acc5: 100.0000 (98.9583)  time: 0.0286  data: 0.0002  max mem: 4132
[19:23:07.602560] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6689 (0.6653)  acc1: 78.1250 (79.0977)  acc5: 98.4375 (98.9445)  time: 0.0284  data: 0.0002  max mem: 4132
[19:23:07.760983] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6623 (0.6655)  acc1: 78.1250 (79.0400)  acc5: 100.0000 (98.9700)  time: 0.0277  data: 0.0002  max mem: 4132
[19:23:07.980500] Test: Total time: 0:00:05 (0.0343 s / it)
[19:23:07.982302] * Acc@1 79.040 Acc@5 98.970 loss 0.666
[19:23:07.983527] Accuracy of the network on the 10000 test images: 79.0%
[19:23:07.984304] Max accuracy: 79.11%
[19:23:08.090961] log_dir: ./output_dir
[19:23:08.994832] Epoch: [50]  [  0/781]  eta: 0:11:44  lr: 0.000136  training_loss: 1.3634 (1.3634)  classification_loss: 1.1757 (1.1757)  loss_mask: 0.1877 (0.1877)  time: 0.9017  data: 0.7032  max mem: 4132
[19:23:12.276638] Epoch: [50]  [ 20/781]  eta: 0:02:31  lr: 0.000136  training_loss: 1.4972 (1.4748)  classification_loss: 1.2869 (1.3215)  loss_mask: 0.1411 (0.1533)  time: 0.1640  data: 0.0004  max mem: 4132
[19:23:15.582212] Epoch: [50]  [ 40/781]  eta: 0:02:15  lr: 0.000136  training_loss: 1.5645 (1.5311)  classification_loss: 1.3704 (1.3530)  loss_mask: 0.1691 (0.1781)  time: 0.1652  data: 0.0004  max mem: 4132
[19:23:18.845965] Epoch: [50]  [ 60/781]  eta: 0:02:07  lr: 0.000135  training_loss: 1.5516 (1.5363)  classification_loss: 1.3713 (1.3634)  loss_mask: 0.1570 (0.1729)  time: 0.1631  data: 0.0002  max mem: 4132
[19:23:22.160553] Epoch: [50]  [ 80/781]  eta: 0:02:01  lr: 0.000135  training_loss: 1.5057 (1.5258)  classification_loss: 1.3851 (1.3663)  loss_mask: 0.1063 (0.1594)  time: 0.1656  data: 0.0003  max mem: 4132
[19:23:25.427873] Epoch: [50]  [100/781]  eta: 0:01:56  lr: 0.000135  training_loss: 1.4718 (1.5239)  classification_loss: 1.4049 (1.3734)  loss_mask: 0.0975 (0.1505)  time: 0.1633  data: 0.0002  max mem: 4132
[19:23:28.763201] Epoch: [50]  [120/781]  eta: 0:01:52  lr: 0.000135  training_loss: 1.4978 (1.5238)  classification_loss: 1.3605 (1.3762)  loss_mask: 0.1169 (0.1476)  time: 0.1667  data: 0.0003  max mem: 4132
[19:23:32.073575] Epoch: [50]  [140/781]  eta: 0:01:48  lr: 0.000135  training_loss: 1.4556 (1.5180)  classification_loss: 1.3689 (1.3735)  loss_mask: 0.1148 (0.1445)  time: 0.1654  data: 0.0003  max mem: 4132
[19:23:35.390597] Epoch: [50]  [160/781]  eta: 0:01:45  lr: 0.000135  training_loss: 1.5090 (1.5200)  classification_loss: 1.3383 (1.3718)  loss_mask: 0.1312 (0.1483)  time: 0.1658  data: 0.0002  max mem: 4132
[19:23:38.697116] Epoch: [50]  [180/781]  eta: 0:01:41  lr: 0.000135  training_loss: 1.4633 (1.5165)  classification_loss: 1.3434 (1.3703)  loss_mask: 0.1129 (0.1462)  time: 0.1652  data: 0.0002  max mem: 4132
[19:23:42.015247] Epoch: [50]  [200/781]  eta: 0:01:38  lr: 0.000135  training_loss: 1.4596 (1.5142)  classification_loss: 1.3639 (1.3717)  loss_mask: 0.1106 (0.1425)  time: 0.1658  data: 0.0002  max mem: 4132
[19:23:45.339861] Epoch: [50]  [220/781]  eta: 0:01:34  lr: 0.000135  training_loss: 1.4104 (1.5079)  classification_loss: 1.3186 (1.3665)  loss_mask: 0.1235 (0.1414)  time: 0.1661  data: 0.0003  max mem: 4132
[19:23:48.636215] Epoch: [50]  [240/781]  eta: 0:01:30  lr: 0.000135  training_loss: 1.4745 (1.5056)  classification_loss: 1.3533 (1.3663)  loss_mask: 0.1187 (0.1393)  time: 0.1647  data: 0.0003  max mem: 4132
[19:23:51.942452] Epoch: [50]  [260/781]  eta: 0:01:27  lr: 0.000134  training_loss: 1.4603 (1.5021)  classification_loss: 1.3852 (1.3672)  loss_mask: 0.0770 (0.1349)  time: 0.1652  data: 0.0003  max mem: 4132
[19:23:55.230372] Epoch: [50]  [280/781]  eta: 0:01:23  lr: 0.000134  training_loss: 1.4498 (1.4989)  classification_loss: 1.3375 (1.3667)  loss_mask: 0.0907 (0.1322)  time: 0.1643  data: 0.0002  max mem: 4132
[19:23:58.506615] Epoch: [50]  [300/781]  eta: 0:01:20  lr: 0.000134  training_loss: 1.4931 (1.4987)  classification_loss: 1.3788 (1.3682)  loss_mask: 0.0896 (0.1305)  time: 0.1637  data: 0.0002  max mem: 4132
[19:24:01.799948] Epoch: [50]  [320/781]  eta: 0:01:17  lr: 0.000134  training_loss: 1.4930 (1.4998)  classification_loss: 1.3645 (1.3687)  loss_mask: 0.1098 (0.1311)  time: 0.1646  data: 0.0002  max mem: 4132
[19:24:05.095850] Epoch: [50]  [340/781]  eta: 0:01:13  lr: 0.000134  training_loss: 1.4577 (1.4997)  classification_loss: 1.3342 (1.3679)  loss_mask: 0.1265 (0.1318)  time: 0.1647  data: 0.0002  max mem: 4132
[19:24:08.366112] Epoch: [50]  [360/781]  eta: 0:01:10  lr: 0.000134  training_loss: 1.5091 (1.5010)  classification_loss: 1.3602 (1.3672)  loss_mask: 0.1643 (0.1339)  time: 0.1634  data: 0.0002  max mem: 4132
[19:24:11.648765] Epoch: [50]  [380/781]  eta: 0:01:06  lr: 0.000134  training_loss: 1.4782 (1.5018)  classification_loss: 1.3406 (1.3665)  loss_mask: 0.1471 (0.1353)  time: 0.1640  data: 0.0002  max mem: 4132
[19:24:14.939273] Epoch: [50]  [400/781]  eta: 0:01:03  lr: 0.000134  training_loss: 1.5231 (1.5022)  classification_loss: 1.4341 (1.3685)  loss_mask: 0.1010 (0.1336)  time: 0.1644  data: 0.0003  max mem: 4132
[19:24:18.228770] Epoch: [50]  [420/781]  eta: 0:01:00  lr: 0.000134  training_loss: 1.3947 (1.4965)  classification_loss: 1.3413 (1.3660)  loss_mask: 0.0619 (0.1305)  time: 0.1644  data: 0.0003  max mem: 4132
[19:24:21.549170] Epoch: [50]  [440/781]  eta: 0:00:56  lr: 0.000133  training_loss: 1.4225 (1.4933)  classification_loss: 1.3531 (1.3655)  loss_mask: 0.0593 (0.1278)  time: 0.1659  data: 0.0003  max mem: 4132
[19:24:24.828626] Epoch: [50]  [460/781]  eta: 0:00:53  lr: 0.000133  training_loss: 1.4187 (1.4906)  classification_loss: 1.2935 (1.3641)  loss_mask: 0.0969 (0.1265)  time: 0.1639  data: 0.0003  max mem: 4132
[19:24:28.105256] Epoch: [50]  [480/781]  eta: 0:00:50  lr: 0.000133  training_loss: 1.4955 (1.4904)  classification_loss: 1.3850 (1.3651)  loss_mask: 0.0878 (0.1252)  time: 0.1637  data: 0.0003  max mem: 4132
[19:24:31.390562] Epoch: [50]  [500/781]  eta: 0:00:46  lr: 0.000133  training_loss: 1.4679 (1.4897)  classification_loss: 1.3854 (1.3660)  loss_mask: 0.0799 (0.1237)  time: 0.1642  data: 0.0002  max mem: 4132
[19:24:34.641117] Epoch: [50]  [520/781]  eta: 0:00:43  lr: 0.000133  training_loss: 1.4215 (1.4872)  classification_loss: 1.3581 (1.3653)  loss_mask: 0.0722 (0.1218)  time: 0.1625  data: 0.0002  max mem: 4132
[19:24:37.910399] Epoch: [50]  [540/781]  eta: 0:00:39  lr: 0.000133  training_loss: 1.5118 (1.4885)  classification_loss: 1.3947 (1.3666)  loss_mask: 0.1154 (0.1219)  time: 0.1633  data: 0.0002  max mem: 4132
[19:24:41.181070] Epoch: [50]  [560/781]  eta: 0:00:36  lr: 0.000133  training_loss: 1.4716 (1.4892)  classification_loss: 1.3470 (1.3669)  loss_mask: 0.1201 (0.1223)  time: 0.1634  data: 0.0002  max mem: 4132
[19:24:44.503962] Epoch: [50]  [580/781]  eta: 0:00:33  lr: 0.000133  training_loss: 1.5231 (1.4900)  classification_loss: 1.3400 (1.3667)  loss_mask: 0.1038 (0.1233)  time: 0.1661  data: 0.0002  max mem: 4132
[19:24:47.809352] Epoch: [50]  [600/781]  eta: 0:00:30  lr: 0.000133  training_loss: 1.3708 (1.4860)  classification_loss: 1.2968 (1.3645)  loss_mask: 0.0627 (0.1215)  time: 0.1652  data: 0.0002  max mem: 4132
[19:24:51.079576] Epoch: [50]  [620/781]  eta: 0:00:26  lr: 0.000133  training_loss: 1.3926 (1.4827)  classification_loss: 1.3102 (1.3633)  loss_mask: 0.0516 (0.1194)  time: 0.1634  data: 0.0002  max mem: 4132
[19:24:54.356871] Epoch: [50]  [640/781]  eta: 0:00:23  lr: 0.000132  training_loss: 1.3531 (1.4795)  classification_loss: 1.3039 (1.3625)  loss_mask: 0.0388 (0.1171)  time: 0.1638  data: 0.0002  max mem: 4132
[19:24:57.651593] Epoch: [50]  [660/781]  eta: 0:00:20  lr: 0.000132  training_loss: 1.4494 (1.4783)  classification_loss: 1.3717 (1.3625)  loss_mask: 0.0677 (0.1158)  time: 0.1647  data: 0.0002  max mem: 4132
[19:25:00.912145] Epoch: [50]  [680/781]  eta: 0:00:16  lr: 0.000132  training_loss: 1.4412 (1.4779)  classification_loss: 1.3364 (1.3630)  loss_mask: 0.0704 (0.1149)  time: 0.1629  data: 0.0002  max mem: 4132
[19:25:04.188207] Epoch: [50]  [700/781]  eta: 0:00:13  lr: 0.000132  training_loss: 1.4695 (1.4773)  classification_loss: 1.4052 (1.3636)  loss_mask: 0.0666 (0.1137)  time: 0.1637  data: 0.0002  max mem: 4132
[19:25:07.463409] Epoch: [50]  [720/781]  eta: 0:00:10  lr: 0.000132  training_loss: 1.4374 (1.4768)  classification_loss: 1.3681 (1.3641)  loss_mask: 0.0670 (0.1127)  time: 0.1636  data: 0.0002  max mem: 4132
[19:25:10.762233] Epoch: [50]  [740/781]  eta: 0:00:06  lr: 0.000132  training_loss: 1.3873 (1.4752)  classification_loss: 1.3174 (1.3635)  loss_mask: 0.0517 (0.1117)  time: 0.1648  data: 0.0003  max mem: 4132
[19:25:14.077026] Epoch: [50]  [760/781]  eta: 0:00:03  lr: 0.000132  training_loss: 1.4361 (1.4741)  classification_loss: 1.4016 (1.3638)  loss_mask: 0.0555 (0.1104)  time: 0.1657  data: 0.0002  max mem: 4132
[19:25:17.331314] Epoch: [50]  [780/781]  eta: 0:00:00  lr: 0.000132  training_loss: 1.4741 (1.4742)  classification_loss: 1.3979 (1.3647)  loss_mask: 0.0778 (0.1095)  time: 0.1626  data: 0.0003  max mem: 4132
[19:25:17.490381] Epoch: [50] Total time: 0:02:09 (0.1657 s / it)
[19:25:17.490854] Averaged stats: lr: 0.000132  training_loss: 1.4741 (1.4742)  classification_loss: 1.3979 (1.3647)  loss_mask: 0.0778 (0.1095)
[19:25:19.016890] Test:  [  0/157]  eta: 0:01:45  testing_loss: 0.6418 (0.6418)  acc1: 82.8125 (82.8125)  acc5: 98.4375 (98.4375)  time: 0.6746  data: 0.6445  max mem: 4132
[19:25:19.310000] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.6418 (0.6438)  acc1: 79.6875 (79.8295)  acc5: 100.0000 (99.4318)  time: 0.0878  data: 0.0588  max mem: 4132
[19:25:19.596737] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.6197 (0.6330)  acc1: 81.2500 (80.7292)  acc5: 98.4375 (99.1071)  time: 0.0288  data: 0.0002  max mem: 4132
[19:25:19.884590] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6345 (0.6584)  acc1: 81.2500 (79.7379)  acc5: 98.4375 (98.7399)  time: 0.0286  data: 0.0003  max mem: 4132
[19:25:20.181625] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6541 (0.6571)  acc1: 78.1250 (79.5351)  acc5: 98.4375 (98.6280)  time: 0.0291  data: 0.0005  max mem: 4132
[19:25:20.469713] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6453 (0.6513)  acc1: 81.2500 (79.9020)  acc5: 98.4375 (98.5600)  time: 0.0291  data: 0.0004  max mem: 4132
[19:25:20.766592] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6169 (0.6472)  acc1: 81.2500 (79.8668)  acc5: 98.4375 (98.6168)  time: 0.0290  data: 0.0002  max mem: 4132
[19:25:21.057997] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6016 (0.6395)  acc1: 81.2500 (80.1496)  acc5: 100.0000 (98.7236)  time: 0.0292  data: 0.0002  max mem: 4132
[19:25:21.349471] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6075 (0.6469)  acc1: 81.2500 (79.8804)  acc5: 98.4375 (98.7269)  time: 0.0290  data: 0.0002  max mem: 4132
[19:25:21.639289] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6523 (0.6466)  acc1: 78.1250 (79.9107)  acc5: 98.4375 (98.7122)  time: 0.0289  data: 0.0002  max mem: 4132
[19:25:21.931637] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.6712 (0.6532)  acc1: 78.1250 (79.5019)  acc5: 98.4375 (98.7005)  time: 0.0290  data: 0.0002  max mem: 4132
[19:25:22.217321] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6712 (0.6519)  acc1: 78.1250 (79.4764)  acc5: 98.4375 (98.7190)  time: 0.0288  data: 0.0002  max mem: 4132
[19:25:22.504171] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6217 (0.6480)  acc1: 79.6875 (79.6488)  acc5: 98.4375 (98.7474)  time: 0.0285  data: 0.0002  max mem: 4132
[19:25:22.789258] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6138 (0.6480)  acc1: 81.2500 (79.7471)  acc5: 100.0000 (98.8073)  time: 0.0285  data: 0.0002  max mem: 4132
[19:25:23.070957] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6500 (0.6486)  acc1: 81.2500 (79.6875)  acc5: 100.0000 (98.8254)  time: 0.0282  data: 0.0001  max mem: 4132
[19:25:23.352259] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6500 (0.6471)  acc1: 79.6875 (79.7185)  acc5: 98.4375 (98.8204)  time: 0.0280  data: 0.0001  max mem: 4132
[19:25:23.503640] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6344 (0.6462)  acc1: 79.6875 (79.7600)  acc5: 98.4375 (98.8400)  time: 0.0271  data: 0.0001  max mem: 4132
[19:25:23.668363] Test: Total time: 0:00:05 (0.0339 s / it)
[19:25:23.669057] * Acc@1 79.760 Acc@5 98.840 loss 0.646
[19:25:23.669321] Accuracy of the network on the 10000 test images: 79.8%
[19:25:23.669491] Max accuracy: 79.76%
[19:25:23.798593] log_dir: ./output_dir
[19:25:24.756712] Epoch: [51]  [  0/781]  eta: 0:12:26  lr: 0.000132  training_loss: 1.3349 (1.3349)  classification_loss: 1.2728 (1.2728)  loss_mask: 0.0621 (0.0621)  time: 0.9556  data: 0.7626  max mem: 4132
[19:25:28.039439] Epoch: [51]  [ 20/781]  eta: 0:02:33  lr: 0.000132  training_loss: 1.3749 (1.3956)  classification_loss: 1.3212 (1.3359)  loss_mask: 0.0505 (0.0597)  time: 0.1640  data: 0.0004  max mem: 4132
[19:25:31.318718] Epoch: [51]  [ 40/781]  eta: 0:02:15  lr: 0.000131  training_loss: 1.4438 (1.4315)  classification_loss: 1.3554 (1.3609)  loss_mask: 0.0685 (0.0705)  time: 0.1639  data: 0.0002  max mem: 4132
[19:25:34.651252] Epoch: [51]  [ 60/781]  eta: 0:02:08  lr: 0.000131  training_loss: 1.3914 (1.4260)  classification_loss: 1.3209 (1.3588)  loss_mask: 0.0517 (0.0672)  time: 0.1665  data: 0.0003  max mem: 4132
[19:25:37.944671] Epoch: [51]  [ 80/781]  eta: 0:02:02  lr: 0.000131  training_loss: 1.3723 (1.4158)  classification_loss: 1.3178 (1.3561)  loss_mask: 0.0312 (0.0597)  time: 0.1646  data: 0.0004  max mem: 4132
[19:25:41.250314] Epoch: [51]  [100/781]  eta: 0:01:57  lr: 0.000131  training_loss: 1.4332 (1.4196)  classification_loss: 1.3303 (1.3534)  loss_mask: 0.0688 (0.0662)  time: 0.1652  data: 0.0003  max mem: 4132
[19:25:44.530455] Epoch: [51]  [120/781]  eta: 0:01:53  lr: 0.000131  training_loss: 1.4167 (1.4227)  classification_loss: 1.3300 (1.3534)  loss_mask: 0.0821 (0.0693)  time: 0.1639  data: 0.0002  max mem: 4132
[19:25:47.828586] Epoch: [51]  [140/781]  eta: 0:01:49  lr: 0.000131  training_loss: 1.3698 (1.4200)  classification_loss: 1.3230 (1.3529)  loss_mask: 0.0485 (0.0671)  time: 0.1648  data: 0.0002  max mem: 4132
[19:25:51.090343] Epoch: [51]  [160/781]  eta: 0:01:45  lr: 0.000131  training_loss: 1.3555 (1.4175)  classification_loss: 1.3044 (1.3515)  loss_mask: 0.0511 (0.0660)  time: 0.1630  data: 0.0002  max mem: 4132
[19:25:54.372437] Epoch: [51]  [180/781]  eta: 0:01:41  lr: 0.000131  training_loss: 1.3881 (1.4169)  classification_loss: 1.3440 (1.3522)  loss_mask: 0.0435 (0.0647)  time: 0.1640  data: 0.0005  max mem: 4132
[19:25:57.664160] Epoch: [51]  [200/781]  eta: 0:01:37  lr: 0.000131  training_loss: 1.3932 (1.4189)  classification_loss: 1.3008 (1.3494)  loss_mask: 0.0970 (0.0695)  time: 0.1645  data: 0.0003  max mem: 4132
[19:26:00.949633] Epoch: [51]  [220/781]  eta: 0:01:34  lr: 0.000131  training_loss: 1.5151 (1.4254)  classification_loss: 1.3932 (1.3504)  loss_mask: 0.0990 (0.0750)  time: 0.1642  data: 0.0002  max mem: 4132
[19:26:04.216024] Epoch: [51]  [240/781]  eta: 0:01:30  lr: 0.000130  training_loss: 1.4524 (1.4282)  classification_loss: 1.3674 (1.3512)  loss_mask: 0.0726 (0.0770)  time: 0.1632  data: 0.0002  max mem: 4132
[19:26:07.486731] Epoch: [51]  [260/781]  eta: 0:01:27  lr: 0.000130  training_loss: 1.4035 (1.4271)  classification_loss: 1.3347 (1.3509)  loss_mask: 0.0454 (0.0762)  time: 0.1634  data: 0.0002  max mem: 4132
[19:26:10.755617] Epoch: [51]  [280/781]  eta: 0:01:23  lr: 0.000130  training_loss: 1.3652 (1.4230)  classification_loss: 1.3228 (1.3486)  loss_mask: 0.0491 (0.0744)  time: 0.1634  data: 0.0003  max mem: 4132
[19:26:14.046593] Epoch: [51]  [300/781]  eta: 0:01:20  lr: 0.000130  training_loss: 1.3912 (1.4219)  classification_loss: 1.3367 (1.3490)  loss_mask: 0.0476 (0.0729)  time: 0.1645  data: 0.0003  max mem: 4132
[19:26:17.323062] Epoch: [51]  [320/781]  eta: 0:01:16  lr: 0.000130  training_loss: 1.3572 (1.4188)  classification_loss: 1.3090 (1.3478)  loss_mask: 0.0367 (0.0709)  time: 0.1637  data: 0.0004  max mem: 4132
[19:26:20.600725] Epoch: [51]  [340/781]  eta: 0:01:13  lr: 0.000130  training_loss: 1.3742 (1.4158)  classification_loss: 1.2945 (1.3453)  loss_mask: 0.0617 (0.0705)  time: 0.1638  data: 0.0003  max mem: 4132
[19:26:23.880526] Epoch: [51]  [360/781]  eta: 0:01:10  lr: 0.000130  training_loss: 1.4167 (1.4176)  classification_loss: 1.3421 (1.3468)  loss_mask: 0.0635 (0.0708)  time: 0.1639  data: 0.0002  max mem: 4132
[19:26:27.174004] Epoch: [51]  [380/781]  eta: 0:01:06  lr: 0.000130  training_loss: 1.4617 (1.4199)  classification_loss: 1.3655 (1.3482)  loss_mask: 0.0790 (0.0717)  time: 0.1646  data: 0.0003  max mem: 4132
[19:26:30.468298] Epoch: [51]  [400/781]  eta: 0:01:03  lr: 0.000130  training_loss: 1.4263 (1.4200)  classification_loss: 1.3606 (1.3480)  loss_mask: 0.0523 (0.0719)  time: 0.1646  data: 0.0003  max mem: 4132
[19:26:33.737659] Epoch: [51]  [420/781]  eta: 0:00:59  lr: 0.000129  training_loss: 1.3606 (1.4186)  classification_loss: 1.3113 (1.3469)  loss_mask: 0.0518 (0.0716)  time: 0.1634  data: 0.0003  max mem: 4132
[19:26:37.019090] Epoch: [51]  [440/781]  eta: 0:00:56  lr: 0.000129  training_loss: 1.3779 (1.4164)  classification_loss: 1.3247 (1.3455)  loss_mask: 0.0420 (0.0709)  time: 0.1640  data: 0.0003  max mem: 4132
[19:26:40.294047] Epoch: [51]  [460/781]  eta: 0:00:53  lr: 0.000129  training_loss: 1.3494 (1.4155)  classification_loss: 1.2906 (1.3447)  loss_mask: 0.0415 (0.0709)  time: 0.1637  data: 0.0003  max mem: 4132
[19:26:43.629105] Epoch: [51]  [480/781]  eta: 0:00:49  lr: 0.000129  training_loss: 1.4300 (1.4156)  classification_loss: 1.3767 (1.3459)  loss_mask: 0.0339 (0.0696)  time: 0.1667  data: 0.0003  max mem: 4132
[19:26:46.885220] Epoch: [51]  [500/781]  eta: 0:00:46  lr: 0.000129  training_loss: 1.3684 (1.4140)  classification_loss: 1.3466 (1.3462)  loss_mask: 0.0238 (0.0678)  time: 0.1627  data: 0.0002  max mem: 4132
[19:26:50.164233] Epoch: [51]  [520/781]  eta: 0:00:43  lr: 0.000129  training_loss: 1.4332 (1.4144)  classification_loss: 1.3828 (1.3473)  loss_mask: 0.0344 (0.0671)  time: 0.1639  data: 0.0003  max mem: 4132
[19:26:53.426679] Epoch: [51]  [540/781]  eta: 0:00:39  lr: 0.000129  training_loss: 1.3981 (1.4139)  classification_loss: 1.3585 (1.3475)  loss_mask: 0.0403 (0.0664)  time: 0.1630  data: 0.0002  max mem: 4132
[19:26:56.734428] Epoch: [51]  [560/781]  eta: 0:00:36  lr: 0.000129  training_loss: 1.3151 (1.4117)  classification_loss: 1.3086 (1.3465)  loss_mask: 0.0224 (0.0652)  time: 0.1653  data: 0.0002  max mem: 4132
[19:26:59.997224] Epoch: [51]  [580/781]  eta: 0:00:33  lr: 0.000129  training_loss: 1.3338 (1.4098)  classification_loss: 1.3098 (1.3461)  loss_mask: 0.0232 (0.0637)  time: 0.1631  data: 0.0003  max mem: 4132
[19:27:03.257783] Epoch: [51]  [600/781]  eta: 0:00:29  lr: 0.000129  training_loss: 1.3914 (1.4093)  classification_loss: 1.3573 (1.3466)  loss_mask: 0.0318 (0.0627)  time: 0.1630  data: 0.0003  max mem: 4132
[19:27:06.535848] Epoch: [51]  [620/781]  eta: 0:00:26  lr: 0.000128  training_loss: 1.3787 (1.4089)  classification_loss: 1.3437 (1.3469)  loss_mask: 0.0350 (0.0620)  time: 0.1638  data: 0.0002  max mem: 4132
[19:27:09.789750] Epoch: [51]  [640/781]  eta: 0:00:23  lr: 0.000128  training_loss: 1.3743 (1.4082)  classification_loss: 1.3517 (1.3471)  loss_mask: 0.0258 (0.0611)  time: 0.1626  data: 0.0003  max mem: 4132
[19:27:13.065688] Epoch: [51]  [660/781]  eta: 0:00:19  lr: 0.000128  training_loss: 1.3638 (1.4074)  classification_loss: 1.3444 (1.3476)  loss_mask: 0.0158 (0.0599)  time: 0.1637  data: 0.0003  max mem: 4132
[19:27:16.346039] Epoch: [51]  [680/781]  eta: 0:00:16  lr: 0.000128  training_loss: 1.3977 (1.4076)  classification_loss: 1.3597 (1.3481)  loss_mask: 0.0190 (0.0595)  time: 0.1639  data: 0.0003  max mem: 4132
[19:27:19.652865] Epoch: [51]  [700/781]  eta: 0:00:13  lr: 0.000128  training_loss: 1.4956 (1.4093)  classification_loss: 1.3258 (1.3487)  loss_mask: 0.0733 (0.0606)  time: 0.1653  data: 0.0003  max mem: 4132
[19:27:22.975831] Epoch: [51]  [720/781]  eta: 0:00:10  lr: 0.000128  training_loss: 1.4721 (1.4109)  classification_loss: 1.3922 (1.3496)  loss_mask: 0.0712 (0.0614)  time: 0.1661  data: 0.0003  max mem: 4132
[19:27:26.282263] Epoch: [51]  [740/781]  eta: 0:00:06  lr: 0.000128  training_loss: 1.3896 (1.4098)  classification_loss: 1.3225 (1.3486)  loss_mask: 0.0459 (0.0613)  time: 0.1651  data: 0.0003  max mem: 4132
[19:27:29.569058] Epoch: [51]  [760/781]  eta: 0:00:03  lr: 0.000128  training_loss: 1.4291 (1.4100)  classification_loss: 1.3939 (1.3493)  loss_mask: 0.0329 (0.0607)  time: 0.1643  data: 0.0003  max mem: 4132
[19:27:32.859510] Epoch: [51]  [780/781]  eta: 0:00:00  lr: 0.000128  training_loss: 1.4140 (1.4102)  classification_loss: 1.3698 (1.3501)  loss_mask: 0.0286 (0.0601)  time: 0.1644  data: 0.0002  max mem: 4132
[19:27:33.033088] Epoch: [51] Total time: 0:02:09 (0.1655 s / it)
[19:27:33.033924] Averaged stats: lr: 0.000128  training_loss: 1.4140 (1.4102)  classification_loss: 1.3698 (1.3501)  loss_mask: 0.0286 (0.0601)
[19:27:33.784987] Test:  [  0/157]  eta: 0:01:57  testing_loss: 0.6107 (0.6107)  acc1: 82.8125 (82.8125)  acc5: 98.4375 (98.4375)  time: 0.7459  data: 0.7111  max mem: 4132
[19:27:34.079015] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.6261 (0.6456)  acc1: 81.2500 (80.2557)  acc5: 100.0000 (99.5739)  time: 0.0942  data: 0.0649  max mem: 4132
[19:27:34.367176] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.6261 (0.6329)  acc1: 79.6875 (80.2827)  acc5: 100.0000 (99.1815)  time: 0.0288  data: 0.0003  max mem: 4132
[19:27:34.655826] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6336 (0.6439)  acc1: 81.2500 (80.3931)  acc5: 98.4375 (98.8407)  time: 0.0286  data: 0.0003  max mem: 4132
[19:27:34.947514] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6221 (0.6441)  acc1: 81.2500 (80.3735)  acc5: 98.4375 (98.7424)  time: 0.0288  data: 0.0003  max mem: 4132
[19:27:35.239829] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6187 (0.6390)  acc1: 81.2500 (80.4534)  acc5: 98.4375 (98.7439)  time: 0.0290  data: 0.0003  max mem: 4132
[19:27:35.531022] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6187 (0.6357)  acc1: 79.6875 (80.4559)  acc5: 98.4375 (98.8217)  time: 0.0290  data: 0.0003  max mem: 4132
[19:27:35.824489] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5993 (0.6308)  acc1: 79.6875 (80.3917)  acc5: 100.0000 (98.8556)  time: 0.0290  data: 0.0003  max mem: 4132
[19:27:36.112367] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6077 (0.6388)  acc1: 78.1250 (80.0733)  acc5: 98.4375 (98.7461)  time: 0.0288  data: 0.0002  max mem: 4132
[19:27:36.402336] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5954 (0.6357)  acc1: 79.6875 (80.2713)  acc5: 98.4375 (98.7466)  time: 0.0286  data: 0.0002  max mem: 4132
[19:27:36.694012] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.6297 (0.6387)  acc1: 78.1250 (80.1671)  acc5: 98.4375 (98.7314)  time: 0.0289  data: 0.0003  max mem: 4132
[19:27:36.980014] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6575 (0.6384)  acc1: 78.1250 (80.1802)  acc5: 100.0000 (98.7753)  time: 0.0287  data: 0.0003  max mem: 4132
[19:27:37.265896] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6144 (0.6340)  acc1: 81.2500 (80.4106)  acc5: 100.0000 (98.7732)  time: 0.0284  data: 0.0002  max mem: 4132
[19:27:37.551036] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6052 (0.6338)  acc1: 82.8125 (80.4986)  acc5: 98.4375 (98.7834)  time: 0.0284  data: 0.0002  max mem: 4132
[19:27:37.835175] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6442 (0.6335)  acc1: 79.6875 (80.4410)  acc5: 100.0000 (98.8143)  time: 0.0283  data: 0.0002  max mem: 4132
[19:27:38.118440] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6450 (0.6324)  acc1: 79.6875 (80.4739)  acc5: 100.0000 (98.8411)  time: 0.0282  data: 0.0002  max mem: 4132
[19:27:38.275636] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6431 (0.6328)  acc1: 79.6875 (80.5100)  acc5: 100.0000 (98.8400)  time: 0.0275  data: 0.0002  max mem: 4132
[19:27:38.477138] Test: Total time: 0:00:05 (0.0346 s / it)
[19:27:38.477638] * Acc@1 80.510 Acc@5 98.840 loss 0.633
[19:27:38.477984] Accuracy of the network on the 10000 test images: 80.5%
[19:27:38.478253] Max accuracy: 80.51%
[19:27:38.652662] log_dir: ./output_dir
[19:27:39.582400] Epoch: [52]  [  0/781]  eta: 0:12:04  lr: 0.000128  training_loss: 1.2809 (1.2809)  classification_loss: 1.2621 (1.2621)  loss_mask: 0.0189 (0.0189)  time: 0.9278  data: 0.7111  max mem: 4132
[19:27:42.853136] Epoch: [52]  [ 20/781]  eta: 0:02:32  lr: 0.000127  training_loss: 1.4038 (1.3728)  classification_loss: 1.3517 (1.3422)  loss_mask: 0.0233 (0.0305)  time: 0.1634  data: 0.0002  max mem: 4132
[19:27:46.168414] Epoch: [52]  [ 40/781]  eta: 0:02:15  lr: 0.000127  training_loss: 1.3780 (1.3857)  classification_loss: 1.3197 (1.3476)  loss_mask: 0.0481 (0.0382)  time: 0.1657  data: 0.0003  max mem: 4132
[19:27:49.449117] Epoch: [52]  [ 60/781]  eta: 0:02:07  lr: 0.000127  training_loss: 1.3871 (1.3842)  classification_loss: 1.3050 (1.3433)  loss_mask: 0.0311 (0.0409)  time: 0.1639  data: 0.0002  max mem: 4132
[19:27:52.819134] Epoch: [52]  [ 80/781]  eta: 0:02:02  lr: 0.000127  training_loss: 1.4548 (1.4032)  classification_loss: 1.3349 (1.3402)  loss_mask: 0.0900 (0.0630)  time: 0.1683  data: 0.0003  max mem: 4132
[19:27:56.133164] Epoch: [52]  [100/781]  eta: 0:01:57  lr: 0.000127  training_loss: 1.4060 (1.4073)  classification_loss: 1.3481 (1.3436)  loss_mask: 0.0573 (0.0637)  time: 0.1656  data: 0.0003  max mem: 4132
[19:27:59.456091] Epoch: [52]  [120/781]  eta: 0:01:53  lr: 0.000127  training_loss: 1.3372 (1.4013)  classification_loss: 1.3025 (1.3404)  loss_mask: 0.0385 (0.0609)  time: 0.1660  data: 0.0003  max mem: 4132
[19:28:02.769323] Epoch: [52]  [140/781]  eta: 0:01:49  lr: 0.000127  training_loss: 1.3865 (1.3972)  classification_loss: 1.3367 (1.3397)  loss_mask: 0.0345 (0.0575)  time: 0.1656  data: 0.0004  max mem: 4132
[19:28:06.042569] Epoch: [52]  [160/781]  eta: 0:01:45  lr: 0.000127  training_loss: 1.3754 (1.3962)  classification_loss: 1.3514 (1.3410)  loss_mask: 0.0261 (0.0552)  time: 0.1636  data: 0.0003  max mem: 4132
[19:28:09.342047] Epoch: [52]  [180/781]  eta: 0:01:41  lr: 0.000127  training_loss: 1.4071 (1.3979)  classification_loss: 1.3654 (1.3446)  loss_mask: 0.0272 (0.0532)  time: 0.1649  data: 0.0002  max mem: 4132
[19:28:12.636194] Epoch: [52]  [200/781]  eta: 0:01:38  lr: 0.000127  training_loss: 1.3820 (1.4021)  classification_loss: 1.3224 (1.3442)  loss_mask: 0.0639 (0.0579)  time: 0.1646  data: 0.0003  max mem: 4132
[19:28:15.962916] Epoch: [52]  [220/781]  eta: 0:01:34  lr: 0.000126  training_loss: 1.4051 (1.4003)  classification_loss: 1.3434 (1.3437)  loss_mask: 0.0368 (0.0566)  time: 0.1663  data: 0.0003  max mem: 4132
[19:28:19.255715] Epoch: [52]  [240/781]  eta: 0:01:31  lr: 0.000126  training_loss: 1.3519 (1.3963)  classification_loss: 1.3131 (1.3425)  loss_mask: 0.0209 (0.0538)  time: 0.1645  data: 0.0002  max mem: 4132
[19:28:22.532388] Epoch: [52]  [260/781]  eta: 0:01:27  lr: 0.000126  training_loss: 1.3842 (1.3942)  classification_loss: 1.3774 (1.3429)  loss_mask: 0.0153 (0.0513)  time: 0.1637  data: 0.0002  max mem: 4132
[19:28:25.813356] Epoch: [52]  [280/781]  eta: 0:01:24  lr: 0.000126  training_loss: 1.3665 (1.3926)  classification_loss: 1.3532 (1.3438)  loss_mask: 0.0131 (0.0487)  time: 0.1640  data: 0.0002  max mem: 4132
[19:28:29.125245] Epoch: [52]  [300/781]  eta: 0:01:20  lr: 0.000126  training_loss: 1.3040 (1.3893)  classification_loss: 1.2922 (1.3431)  loss_mask: 0.0104 (0.0462)  time: 0.1655  data: 0.0003  max mem: 4132
[19:28:32.421329] Epoch: [52]  [320/781]  eta: 0:01:17  lr: 0.000126  training_loss: 1.3553 (1.3861)  classification_loss: 1.2894 (1.3419)  loss_mask: 0.0089 (0.0442)  time: 0.1647  data: 0.0002  max mem: 4132
[19:28:35.732839] Epoch: [52]  [340/781]  eta: 0:01:13  lr: 0.000126  training_loss: 1.4019 (1.3861)  classification_loss: 1.3834 (1.3438)  loss_mask: 0.0090 (0.0424)  time: 0.1655  data: 0.0002  max mem: 4132
[19:28:39.021073] Epoch: [52]  [360/781]  eta: 0:01:10  lr: 0.000126  training_loss: 1.3908 (1.3863)  classification_loss: 1.3862 (1.3457)  loss_mask: 0.0091 (0.0406)  time: 0.1643  data: 0.0003  max mem: 4132
[19:28:42.331612] Epoch: [52]  [380/781]  eta: 0:01:06  lr: 0.000126  training_loss: 1.3250 (1.3827)  classification_loss: 1.3148 (1.3437)  loss_mask: 0.0102 (0.0391)  time: 0.1654  data: 0.0003  max mem: 4132
[19:28:45.657411] Epoch: [52]  [400/781]  eta: 0:01:03  lr: 0.000125  training_loss: 1.3867 (1.3836)  classification_loss: 1.3200 (1.3449)  loss_mask: 0.0117 (0.0387)  time: 0.1662  data: 0.0004  max mem: 4132
[19:28:48.964321] Epoch: [52]  [420/781]  eta: 0:01:00  lr: 0.000125  training_loss: 1.4638 (1.3870)  classification_loss: 1.3331 (1.3445)  loss_mask: 0.0494 (0.0424)  time: 0.1653  data: 0.0003  max mem: 4132
[19:28:52.256730] Epoch: [52]  [440/781]  eta: 0:00:56  lr: 0.000125  training_loss: 1.3467 (1.3854)  classification_loss: 1.2781 (1.3428)  loss_mask: 0.0405 (0.0426)  time: 0.1645  data: 0.0003  max mem: 4132
[19:28:55.536067] Epoch: [52]  [460/781]  eta: 0:00:53  lr: 0.000125  training_loss: 1.3850 (1.3861)  classification_loss: 1.3444 (1.3432)  loss_mask: 0.0437 (0.0429)  time: 0.1639  data: 0.0003  max mem: 4132
[19:28:58.827551] Epoch: [52]  [480/781]  eta: 0:00:50  lr: 0.000125  training_loss: 1.3382 (1.3850)  classification_loss: 1.3268 (1.3428)  loss_mask: 0.0197 (0.0421)  time: 0.1645  data: 0.0003  max mem: 4132
[19:29:02.133913] Epoch: [52]  [500/781]  eta: 0:00:46  lr: 0.000125  training_loss: 1.3254 (1.3830)  classification_loss: 1.3072 (1.3416)  loss_mask: 0.0157 (0.0414)  time: 0.1652  data: 0.0002  max mem: 4132
[19:29:05.409705] Epoch: [52]  [520/781]  eta: 0:00:43  lr: 0.000125  training_loss: 1.3214 (1.3815)  classification_loss: 1.3124 (1.3409)  loss_mask: 0.0160 (0.0405)  time: 0.1637  data: 0.0003  max mem: 4132
[19:29:08.698879] Epoch: [52]  [540/781]  eta: 0:00:40  lr: 0.000125  training_loss: 1.4158 (1.3814)  classification_loss: 1.3751 (1.3416)  loss_mask: 0.0163 (0.0397)  time: 0.1643  data: 0.0002  max mem: 4132
[19:29:12.022795] Epoch: [52]  [560/781]  eta: 0:00:36  lr: 0.000125  training_loss: 1.3616 (1.3808)  classification_loss: 1.3249 (1.3412)  loss_mask: 0.0310 (0.0396)  time: 0.1661  data: 0.0003  max mem: 4132
[19:29:15.331626] Epoch: [52]  [580/781]  eta: 0:00:33  lr: 0.000125  training_loss: 1.3971 (1.3808)  classification_loss: 1.3563 (1.3418)  loss_mask: 0.0155 (0.0390)  time: 0.1654  data: 0.0003  max mem: 4132
[19:29:18.618922] Epoch: [52]  [600/781]  eta: 0:00:30  lr: 0.000124  training_loss: 1.3864 (1.3816)  classification_loss: 1.3625 (1.3423)  loss_mask: 0.0306 (0.0393)  time: 0.1643  data: 0.0002  max mem: 4132
[19:29:21.899610] Epoch: [52]  [620/781]  eta: 0:00:26  lr: 0.000124  training_loss: 1.3967 (1.3821)  classification_loss: 1.3460 (1.3429)  loss_mask: 0.0295 (0.0392)  time: 0.1640  data: 0.0002  max mem: 4132
[19:29:25.187633] Epoch: [52]  [640/781]  eta: 0:00:23  lr: 0.000124  training_loss: 1.3743 (1.3816)  classification_loss: 1.3372 (1.3424)  loss_mask: 0.0278 (0.0391)  time: 0.1643  data: 0.0003  max mem: 4132
[19:29:28.471385] Epoch: [52]  [660/781]  eta: 0:00:20  lr: 0.000124  training_loss: 1.4092 (1.3820)  classification_loss: 1.3927 (1.3436)  loss_mask: 0.0132 (0.0384)  time: 0.1641  data: 0.0003  max mem: 4132
[19:29:31.757620] Epoch: [52]  [680/781]  eta: 0:00:16  lr: 0.000124  training_loss: 1.3407 (1.3806)  classification_loss: 1.3360 (1.3431)  loss_mask: 0.0062 (0.0375)  time: 0.1642  data: 0.0003  max mem: 4132
[19:29:35.038363] Epoch: [52]  [700/781]  eta: 0:00:13  lr: 0.000124  training_loss: 1.4264 (1.3814)  classification_loss: 1.4205 (1.3447)  loss_mask: 0.0078 (0.0366)  time: 0.1639  data: 0.0003  max mem: 4132
[19:29:38.317297] Epoch: [52]  [720/781]  eta: 0:00:10  lr: 0.000124  training_loss: 1.3929 (1.3812)  classification_loss: 1.3896 (1.3454)  loss_mask: 0.0041 (0.0357)  time: 0.1638  data: 0.0002  max mem: 4132
[19:29:41.610088] Epoch: [52]  [740/781]  eta: 0:00:06  lr: 0.000124  training_loss: 1.3163 (1.3798)  classification_loss: 1.3127 (1.3449)  loss_mask: 0.0036 (0.0349)  time: 0.1645  data: 0.0003  max mem: 4132
[19:29:44.884233] Epoch: [52]  [760/781]  eta: 0:00:03  lr: 0.000124  training_loss: 1.3747 (1.3798)  classification_loss: 1.3572 (1.3455)  loss_mask: 0.0063 (0.0343)  time: 0.1636  data: 0.0002  max mem: 4132
[19:29:48.149421] Epoch: [52]  [780/781]  eta: 0:00:00  lr: 0.000123  training_loss: 1.3533 (1.3801)  classification_loss: 1.3424 (1.3463)  loss_mask: 0.0087 (0.0338)  time: 0.1632  data: 0.0003  max mem: 4132
[19:29:48.337232] Epoch: [52] Total time: 0:02:09 (0.1660 s / it)
[19:29:48.337849] Averaged stats: lr: 0.000123  training_loss: 1.3533 (1.3801)  classification_loss: 1.3424 (1.3463)  loss_mask: 0.0087 (0.0338)
[19:29:49.059340] Test:  [  0/157]  eta: 0:01:52  testing_loss: 0.6169 (0.6169)  acc1: 82.8125 (82.8125)  acc5: 95.3125 (95.3125)  time: 0.7153  data: 0.6712  max mem: 4132
[19:29:49.360690] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.6170 (0.6438)  acc1: 79.6875 (79.6875)  acc5: 100.0000 (99.0057)  time: 0.0919  data: 0.0613  max mem: 4132
[19:29:49.657342] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.6087 (0.6154)  acc1: 79.6875 (81.3244)  acc5: 100.0000 (98.9583)  time: 0.0295  data: 0.0003  max mem: 4132
[19:29:49.952529] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6277 (0.6370)  acc1: 81.2500 (80.3931)  acc5: 98.4375 (98.8407)  time: 0.0293  data: 0.0003  max mem: 4132
[19:29:50.239521] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6410 (0.6365)  acc1: 79.6875 (80.4116)  acc5: 98.4375 (98.7805)  time: 0.0288  data: 0.0003  max mem: 4132
[19:29:50.525972] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6015 (0.6313)  acc1: 82.8125 (81.2500)  acc5: 100.0000 (98.8664)  time: 0.0285  data: 0.0002  max mem: 4132
[19:29:50.812693] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6023 (0.6310)  acc1: 82.8125 (80.9682)  acc5: 100.0000 (98.8986)  time: 0.0285  data: 0.0002  max mem: 4132
[19:29:51.102064] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5890 (0.6243)  acc1: 81.2500 (81.2280)  acc5: 100.0000 (98.9657)  time: 0.0287  data: 0.0002  max mem: 4132
[19:29:51.390251] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5929 (0.6323)  acc1: 79.6875 (80.8835)  acc5: 98.4375 (98.8619)  time: 0.0287  data: 0.0003  max mem: 4132
[19:29:51.677818] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6233 (0.6295)  acc1: 79.6875 (81.0268)  acc5: 98.4375 (98.8839)  time: 0.0286  data: 0.0003  max mem: 4132
[19:29:51.962869] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.6229 (0.6329)  acc1: 81.2500 (80.8632)  acc5: 98.4375 (98.8707)  time: 0.0285  data: 0.0002  max mem: 4132
[19:29:52.249038] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6316 (0.6321)  acc1: 81.2500 (80.9262)  acc5: 100.0000 (98.9161)  time: 0.0284  data: 0.0002  max mem: 4132
[19:29:52.537478] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5931 (0.6284)  acc1: 82.8125 (81.0305)  acc5: 100.0000 (98.9669)  time: 0.0285  data: 0.0002  max mem: 4132
[19:29:52.828003] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6081 (0.6292)  acc1: 79.6875 (80.7610)  acc5: 100.0000 (98.9981)  time: 0.0288  data: 0.0003  max mem: 4132
[19:29:53.129976] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6668 (0.6303)  acc1: 79.6875 (80.6848)  acc5: 100.0000 (99.0137)  time: 0.0295  data: 0.0002  max mem: 4132
[19:29:53.411943] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6668 (0.6290)  acc1: 79.6875 (80.6602)  acc5: 100.0000 (99.0170)  time: 0.0291  data: 0.0002  max mem: 4132
[19:29:53.573241] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6120 (0.6286)  acc1: 79.6875 (80.6300)  acc5: 100.0000 (99.0400)  time: 0.0277  data: 0.0002  max mem: 4132
[19:29:53.806228] Test: Total time: 0:00:05 (0.0348 s / it)
[19:29:53.806746] * Acc@1 80.630 Acc@5 99.040 loss 0.629
[19:29:53.807158] Accuracy of the network on the 10000 test images: 80.6%
[19:29:53.807367] Max accuracy: 80.63%
[19:29:53.963769] log_dir: ./output_dir
[19:29:54.945208] Epoch: [53]  [  0/781]  eta: 0:12:45  lr: 0.000123  training_loss: 1.3623 (1.3623)  classification_loss: 1.3588 (1.3588)  loss_mask: 0.0035 (0.0035)  time: 0.9796  data: 0.7451  max mem: 4132
[19:29:58.300497] Epoch: [53]  [ 20/781]  eta: 0:02:36  lr: 0.000123  training_loss: 1.3214 (1.3299)  classification_loss: 1.2862 (1.3117)  loss_mask: 0.0109 (0.0181)  time: 0.1676  data: 0.0003  max mem: 4132
[19:30:01.584154] Epoch: [53]  [ 40/781]  eta: 0:02:17  lr: 0.000123  training_loss: 1.3620 (1.3474)  classification_loss: 1.3437 (1.3279)  loss_mask: 0.0155 (0.0195)  time: 0.1641  data: 0.0003  max mem: 4132
[19:30:04.866978] Epoch: [53]  [ 60/781]  eta: 0:02:08  lr: 0.000123  training_loss: 1.3726 (1.3613)  classification_loss: 1.3537 (1.3386)  loss_mask: 0.0235 (0.0227)  time: 0.1641  data: 0.0003  max mem: 4132
[19:30:08.158503] Epoch: [53]  [ 80/781]  eta: 0:02:02  lr: 0.000123  training_loss: 1.3357 (1.3582)  classification_loss: 1.3133 (1.3358)  loss_mask: 0.0118 (0.0224)  time: 0.1645  data: 0.0004  max mem: 4132
[19:30:11.490625] Epoch: [53]  [100/781]  eta: 0:01:58  lr: 0.000123  training_loss: 1.3745 (1.3608)  classification_loss: 1.3486 (1.3386)  loss_mask: 0.0116 (0.0222)  time: 0.1665  data: 0.0003  max mem: 4132
[19:30:14.807367] Epoch: [53]  [120/781]  eta: 0:01:53  lr: 0.000123  training_loss: 1.3890 (1.3647)  classification_loss: 1.3540 (1.3408)  loss_mask: 0.0168 (0.0240)  time: 0.1657  data: 0.0003  max mem: 4132
[19:30:18.093515] Epoch: [53]  [140/781]  eta: 0:01:49  lr: 0.000123  training_loss: 1.3409 (1.3613)  classification_loss: 1.3085 (1.3344)  loss_mask: 0.0232 (0.0269)  time: 0.1642  data: 0.0003  max mem: 4132
[19:30:21.368959] Epoch: [53]  [160/781]  eta: 0:01:45  lr: 0.000123  training_loss: 1.3811 (1.3666)  classification_loss: 1.3082 (1.3339)  loss_mask: 0.0387 (0.0327)  time: 0.1637  data: 0.0003  max mem: 4132
[19:30:24.657623] Epoch: [53]  [180/781]  eta: 0:01:41  lr: 0.000122  training_loss: 1.3692 (1.3685)  classification_loss: 1.3391 (1.3363)  loss_mask: 0.0239 (0.0322)  time: 0.1643  data: 0.0002  max mem: 4132
[19:30:27.983849] Epoch: [53]  [200/781]  eta: 0:01:38  lr: 0.000122  training_loss: 1.3902 (1.3666)  classification_loss: 1.3537 (1.3356)  loss_mask: 0.0125 (0.0309)  time: 0.1662  data: 0.0003  max mem: 4132
[19:30:31.315899] Epoch: [53]  [220/781]  eta: 0:01:34  lr: 0.000122  training_loss: 1.3466 (1.3638)  classification_loss: 1.3410 (1.3345)  loss_mask: 0.0109 (0.0294)  time: 0.1665  data: 0.0003  max mem: 4132
[19:30:34.626359] Epoch: [53]  [240/781]  eta: 0:01:31  lr: 0.000122  training_loss: 1.3201 (1.3609)  classification_loss: 1.3105 (1.3332)  loss_mask: 0.0076 (0.0277)  time: 0.1654  data: 0.0003  max mem: 4132
[19:30:37.982449] Epoch: [53]  [260/781]  eta: 0:01:27  lr: 0.000122  training_loss: 1.3464 (1.3602)  classification_loss: 1.3200 (1.3332)  loss_mask: 0.0069 (0.0270)  time: 0.1677  data: 0.0004  max mem: 4132
[19:30:41.318971] Epoch: [53]  [280/781]  eta: 0:01:24  lr: 0.000122  training_loss: 1.4642 (1.3704)  classification_loss: 1.3447 (1.3332)  loss_mask: 0.1141 (0.0373)  time: 0.1667  data: 0.0003  max mem: 4132
[19:30:44.618954] Epoch: [53]  [300/781]  eta: 0:01:20  lr: 0.000122  training_loss: 1.4431 (1.3759)  classification_loss: 1.3205 (1.3335)  loss_mask: 0.0869 (0.0424)  time: 0.1649  data: 0.0002  max mem: 4132
[19:30:47.896156] Epoch: [53]  [320/781]  eta: 0:01:17  lr: 0.000122  training_loss: 1.3673 (1.3764)  classification_loss: 1.3057 (1.3326)  loss_mask: 0.0523 (0.0438)  time: 0.1638  data: 0.0002  max mem: 4132
[19:30:51.161114] Epoch: [53]  [340/781]  eta: 0:01:13  lr: 0.000122  training_loss: 1.3706 (1.3757)  classification_loss: 1.3423 (1.3315)  loss_mask: 0.0379 (0.0441)  time: 0.1632  data: 0.0003  max mem: 4132
[19:30:54.405965] Epoch: [53]  [360/781]  eta: 0:01:10  lr: 0.000122  training_loss: 1.3729 (1.3769)  classification_loss: 1.3508 (1.3339)  loss_mask: 0.0204 (0.0429)  time: 0.1622  data: 0.0002  max mem: 4132
[19:30:57.681333] Epoch: [53]  [380/781]  eta: 0:01:07  lr: 0.000121  training_loss: 1.3578 (1.3765)  classification_loss: 1.3420 (1.3349)  loss_mask: 0.0151 (0.0416)  time: 0.1637  data: 0.0002  max mem: 4132
[19:31:00.952856] Epoch: [53]  [400/781]  eta: 0:01:03  lr: 0.000121  training_loss: 1.3363 (1.3749)  classification_loss: 1.3210 (1.3349)  loss_mask: 0.0072 (0.0400)  time: 0.1635  data: 0.0003  max mem: 4132
[19:31:04.211003] Epoch: [53]  [420/781]  eta: 0:01:00  lr: 0.000121  training_loss: 1.3211 (1.3727)  classification_loss: 1.3174 (1.3343)  loss_mask: 0.0048 (0.0384)  time: 0.1628  data: 0.0002  max mem: 4132
[19:31:07.480858] Epoch: [53]  [440/781]  eta: 0:00:56  lr: 0.000121  training_loss: 1.3204 (1.3707)  classification_loss: 1.3155 (1.3339)  loss_mask: 0.0038 (0.0368)  time: 0.1634  data: 0.0002  max mem: 4132
[19:31:10.788183] Epoch: [53]  [460/781]  eta: 0:00:53  lr: 0.000121  training_loss: 1.2975 (1.3688)  classification_loss: 1.2929 (1.3334)  loss_mask: 0.0034 (0.0354)  time: 0.1652  data: 0.0002  max mem: 4132
[19:31:14.080174] Epoch: [53]  [480/781]  eta: 0:00:50  lr: 0.000121  training_loss: 1.3489 (1.3688)  classification_loss: 1.3459 (1.3346)  loss_mask: 0.0039 (0.0342)  time: 0.1645  data: 0.0003  max mem: 4132
[19:31:17.377220] Epoch: [53]  [500/781]  eta: 0:00:46  lr: 0.000121  training_loss: 1.3454 (1.3690)  classification_loss: 1.3429 (1.3361)  loss_mask: 0.0024 (0.0330)  time: 0.1648  data: 0.0002  max mem: 4132
[19:31:20.655966] Epoch: [53]  [520/781]  eta: 0:00:43  lr: 0.000121  training_loss: 1.3616 (1.3694)  classification_loss: 1.3560 (1.3375)  loss_mask: 0.0029 (0.0319)  time: 0.1638  data: 0.0002  max mem: 4132
[19:31:23.952055] Epoch: [53]  [540/781]  eta: 0:00:40  lr: 0.000121  training_loss: 1.3484 (1.3689)  classification_loss: 1.3012 (1.3378)  loss_mask: 0.0053 (0.0312)  time: 0.1647  data: 0.0003  max mem: 4132
[19:31:27.262242] Epoch: [53]  [560/781]  eta: 0:00:36  lr: 0.000120  training_loss: 1.3112 (1.3684)  classification_loss: 1.3061 (1.3377)  loss_mask: 0.0097 (0.0307)  time: 0.1654  data: 0.0003  max mem: 4132
[19:31:30.579109] Epoch: [53]  [580/781]  eta: 0:00:33  lr: 0.000120  training_loss: 1.3361 (1.3681)  classification_loss: 1.3050 (1.3370)  loss_mask: 0.0219 (0.0311)  time: 0.1657  data: 0.0003  max mem: 4132
[19:31:33.858976] Epoch: [53]  [600/781]  eta: 0:00:30  lr: 0.000120  training_loss: 1.3718 (1.3682)  classification_loss: 1.3321 (1.3367)  loss_mask: 0.0247 (0.0315)  time: 0.1639  data: 0.0003  max mem: 4132
[19:31:37.149904] Epoch: [53]  [620/781]  eta: 0:00:26  lr: 0.000120  training_loss: 1.2982 (1.3662)  classification_loss: 1.2926 (1.3354)  loss_mask: 0.0088 (0.0308)  time: 0.1644  data: 0.0005  max mem: 4132
[19:31:40.453019] Epoch: [53]  [640/781]  eta: 0:00:23  lr: 0.000120  training_loss: 1.2969 (1.3646)  classification_loss: 1.2915 (1.3345)  loss_mask: 0.0056 (0.0301)  time: 0.1651  data: 0.0003  max mem: 4132
[19:31:43.724327] Epoch: [53]  [660/781]  eta: 0:00:20  lr: 0.000120  training_loss: 1.3686 (1.3643)  classification_loss: 1.3655 (1.3348)  loss_mask: 0.0065 (0.0295)  time: 0.1635  data: 0.0003  max mem: 4132
[19:31:47.036613] Epoch: [53]  [680/781]  eta: 0:00:16  lr: 0.000120  training_loss: 1.3502 (1.3642)  classification_loss: 1.3362 (1.3354)  loss_mask: 0.0035 (0.0288)  time: 0.1655  data: 0.0004  max mem: 4132
[19:31:50.328791] Epoch: [53]  [700/781]  eta: 0:00:13  lr: 0.000120  training_loss: 1.4858 (1.3674)  classification_loss: 1.3398 (1.3364)  loss_mask: 0.0692 (0.0310)  time: 0.1645  data: 0.0003  max mem: 4132
[19:31:53.625988] Epoch: [53]  [720/781]  eta: 0:00:10  lr: 0.000120  training_loss: 1.4346 (1.3696)  classification_loss: 1.3734 (1.3375)  loss_mask: 0.0467 (0.0321)  time: 0.1648  data: 0.0003  max mem: 4132
[19:31:56.913356] Epoch: [53]  [740/781]  eta: 0:00:06  lr: 0.000120  training_loss: 1.3431 (1.3687)  classification_loss: 1.3130 (1.3366)  loss_mask: 0.0274 (0.0321)  time: 0.1643  data: 0.0003  max mem: 4132
[19:32:00.187475] Epoch: [53]  [760/781]  eta: 0:00:03  lr: 0.000119  training_loss: 1.4109 (1.3699)  classification_loss: 1.3587 (1.3378)  loss_mask: 0.0206 (0.0321)  time: 0.1636  data: 0.0003  max mem: 4132
[19:32:03.464980] Epoch: [53]  [780/781]  eta: 0:00:00  lr: 0.000119  training_loss: 1.3491 (1.3699)  classification_loss: 1.3311 (1.3377)  loss_mask: 0.0273 (0.0322)  time: 0.1637  data: 0.0003  max mem: 4132
[19:32:03.672926] Epoch: [53] Total time: 0:02:09 (0.1661 s / it)
[19:32:03.678837] Averaged stats: lr: 0.000119  training_loss: 1.3491 (1.3699)  classification_loss: 1.3311 (1.3377)  loss_mask: 0.0273 (0.0322)
[19:32:04.377299] Test:  [  0/157]  eta: 0:01:47  testing_loss: 0.5699 (0.5699)  acc1: 81.2500 (81.2500)  acc5: 96.8750 (96.8750)  time: 0.6873  data: 0.6575  max mem: 4132
[19:32:04.713466] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.6148 (0.6371)  acc1: 78.1250 (79.2614)  acc5: 100.0000 (99.4318)  time: 0.0928  data: 0.0604  max mem: 4132
[19:32:05.005148] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5854 (0.5905)  acc1: 81.2500 (81.3244)  acc5: 100.0000 (99.3304)  time: 0.0312  data: 0.0005  max mem: 4132
[19:32:05.297353] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5932 (0.6103)  acc1: 81.2500 (80.4435)  acc5: 100.0000 (99.1935)  time: 0.0290  data: 0.0003  max mem: 4132
[19:32:05.581793] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6277 (0.6144)  acc1: 79.6875 (80.2591)  acc5: 100.0000 (99.1616)  time: 0.0287  data: 0.0002  max mem: 4132
[19:32:05.866753] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6120 (0.6087)  acc1: 81.2500 (80.8211)  acc5: 100.0000 (99.1728)  time: 0.0283  data: 0.0002  max mem: 4132
[19:32:06.162401] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5788 (0.6076)  acc1: 81.2500 (80.7633)  acc5: 100.0000 (99.1803)  time: 0.0289  data: 0.0002  max mem: 4132
[19:32:06.454302] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5696 (0.6014)  acc1: 82.8125 (81.2060)  acc5: 100.0000 (99.1637)  time: 0.0292  data: 0.0002  max mem: 4132
[19:32:06.746978] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6017 (0.6106)  acc1: 81.2500 (80.8642)  acc5: 98.4375 (99.0741)  time: 0.0291  data: 0.0002  max mem: 4132
[19:32:07.032358] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6242 (0.6068)  acc1: 79.6875 (80.9924)  acc5: 98.4375 (99.1071)  time: 0.0287  data: 0.0002  max mem: 4132
[19:32:07.318270] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.5963 (0.6091)  acc1: 81.2500 (80.9715)  acc5: 98.4375 (99.0873)  time: 0.0284  data: 0.0002  max mem: 4132
[19:32:07.610437] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5963 (0.6099)  acc1: 81.2500 (80.8981)  acc5: 98.4375 (99.0569)  time: 0.0287  data: 0.0002  max mem: 4132
[19:32:07.897287] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5777 (0.6073)  acc1: 81.2500 (80.9788)  acc5: 98.4375 (99.0057)  time: 0.0288  data: 0.0002  max mem: 4132
[19:32:08.183614] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5806 (0.6062)  acc1: 81.2500 (80.9757)  acc5: 100.0000 (99.0577)  time: 0.0285  data: 0.0002  max mem: 4132
[19:32:08.468255] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6222 (0.6045)  acc1: 79.6875 (81.0173)  acc5: 100.0000 (99.0581)  time: 0.0284  data: 0.0002  max mem: 4132
[19:32:08.749980] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6222 (0.6038)  acc1: 79.6875 (81.0327)  acc5: 100.0000 (99.0584)  time: 0.0282  data: 0.0002  max mem: 4132
[19:32:08.900739] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6196 (0.6044)  acc1: 79.6875 (80.9800)  acc5: 100.0000 (99.0800)  time: 0.0272  data: 0.0001  max mem: 4132
[19:32:09.073197] Test: Total time: 0:00:05 (0.0343 s / it)
[19:32:09.074063] * Acc@1 80.980 Acc@5 99.080 loss 0.604
[19:32:09.074803] Accuracy of the network on the 10000 test images: 81.0%
[19:32:09.075199] Max accuracy: 80.98%
[19:32:09.235492] log_dir: ./output_dir
[19:32:10.118059] Epoch: [54]  [  0/781]  eta: 0:11:27  lr: 0.000119  training_loss: 1.2841 (1.2841)  classification_loss: 1.2591 (1.2591)  loss_mask: 0.0250 (0.0250)  time: 0.8807  data: 0.6980  max mem: 4132
[19:32:13.379359] Epoch: [54]  [ 20/781]  eta: 0:02:30  lr: 0.000119  training_loss: 1.3588 (1.3733)  classification_loss: 1.3327 (1.3313)  loss_mask: 0.0321 (0.0420)  time: 0.1629  data: 0.0002  max mem: 4132
[19:32:16.666690] Epoch: [54]  [ 40/781]  eta: 0:02:14  lr: 0.000119  training_loss: 1.3613 (1.3867)  classification_loss: 1.3274 (1.3465)  loss_mask: 0.0340 (0.0402)  time: 0.1643  data: 0.0002  max mem: 4132
[19:32:19.949308] Epoch: [54]  [ 60/781]  eta: 0:02:06  lr: 0.000119  training_loss: 1.3953 (1.3849)  classification_loss: 1.3717 (1.3500)  loss_mask: 0.0198 (0.0349)  time: 0.1640  data: 0.0002  max mem: 4132
[19:32:23.235174] Epoch: [54]  [ 80/781]  eta: 0:02:01  lr: 0.000119  training_loss: 1.3871 (1.3802)  classification_loss: 1.3177 (1.3451)  loss_mask: 0.0121 (0.0352)  time: 0.1642  data: 0.0003  max mem: 4132
[19:32:26.550553] Epoch: [54]  [100/781]  eta: 0:01:56  lr: 0.000119  training_loss: 1.3672 (1.3814)  classification_loss: 1.3316 (1.3441)  loss_mask: 0.0326 (0.0373)  time: 0.1657  data: 0.0003  max mem: 4132
[19:32:29.853158] Epoch: [54]  [120/781]  eta: 0:01:52  lr: 0.000119  training_loss: 1.4184 (1.3886)  classification_loss: 1.3423 (1.3430)  loss_mask: 0.0749 (0.0456)  time: 0.1650  data: 0.0003  max mem: 4132
[19:32:33.172435] Epoch: [54]  [140/781]  eta: 0:01:48  lr: 0.000119  training_loss: 1.3789 (1.3870)  classification_loss: 1.3547 (1.3408)  loss_mask: 0.0377 (0.0462)  time: 0.1659  data: 0.0002  max mem: 4132
[19:32:36.469486] Epoch: [54]  [160/781]  eta: 0:01:44  lr: 0.000118  training_loss: 1.3672 (1.3823)  classification_loss: 1.3474 (1.3381)  loss_mask: 0.0190 (0.0442)  time: 0.1648  data: 0.0003  max mem: 4132
[19:32:39.765104] Epoch: [54]  [180/781]  eta: 0:01:41  lr: 0.000118  training_loss: 1.3489 (1.3799)  classification_loss: 1.3422 (1.3390)  loss_mask: 0.0104 (0.0409)  time: 0.1647  data: 0.0004  max mem: 4132
[19:32:43.030729] Epoch: [54]  [200/781]  eta: 0:01:37  lr: 0.000118  training_loss: 1.3636 (1.3783)  classification_loss: 1.3607 (1.3407)  loss_mask: 0.0054 (0.0376)  time: 0.1632  data: 0.0002  max mem: 4132
[19:32:46.336352] Epoch: [54]  [220/781]  eta: 0:01:34  lr: 0.000118  training_loss: 1.3860 (1.3801)  classification_loss: 1.3787 (1.3449)  loss_mask: 0.0051 (0.0352)  time: 0.1652  data: 0.0003  max mem: 4132
[19:32:49.638358] Epoch: [54]  [240/781]  eta: 0:01:30  lr: 0.000118  training_loss: 1.3318 (1.3785)  classification_loss: 1.3273 (1.3459)  loss_mask: 0.0032 (0.0327)  time: 0.1650  data: 0.0003  max mem: 4132
[19:32:52.925718] Epoch: [54]  [260/781]  eta: 0:01:27  lr: 0.000118  training_loss: 1.3715 (1.3770)  classification_loss: 1.3699 (1.3466)  loss_mask: 0.0025 (0.0304)  time: 0.1643  data: 0.0003  max mem: 4132
[19:32:56.197216] Epoch: [54]  [280/781]  eta: 0:01:23  lr: 0.000118  training_loss: 1.3418 (1.3741)  classification_loss: 1.3257 (1.3453)  loss_mask: 0.0031 (0.0289)  time: 0.1635  data: 0.0003  max mem: 4132
[19:32:59.486230] Epoch: [54]  [300/781]  eta: 0:01:20  lr: 0.000118  training_loss: 1.3360 (1.3722)  classification_loss: 1.3224 (1.3446)  loss_mask: 0.0073 (0.0276)  time: 0.1644  data: 0.0003  max mem: 4132
[19:33:02.808712] Epoch: [54]  [320/781]  eta: 0:01:16  lr: 0.000118  training_loss: 1.3656 (1.3705)  classification_loss: 1.3617 (1.3443)  loss_mask: 0.0043 (0.0262)  time: 0.1660  data: 0.0003  max mem: 4132
[19:33:06.080722] Epoch: [54]  [340/781]  eta: 0:01:13  lr: 0.000118  training_loss: 1.3539 (1.3684)  classification_loss: 1.3526 (1.3436)  loss_mask: 0.0020 (0.0248)  time: 0.1635  data: 0.0003  max mem: 4132
[19:33:09.353245] Epoch: [54]  [360/781]  eta: 0:01:10  lr: 0.000117  training_loss: 1.3247 (1.3676)  classification_loss: 1.3223 (1.3441)  loss_mask: 0.0015 (0.0235)  time: 0.1635  data: 0.0003  max mem: 4132
[19:33:12.646076] Epoch: [54]  [380/781]  eta: 0:01:06  lr: 0.000117  training_loss: 1.3252 (1.3659)  classification_loss: 1.3241 (1.3435)  loss_mask: 0.0015 (0.0223)  time: 0.1646  data: 0.0003  max mem: 4132
[19:33:15.947351] Epoch: [54]  [400/781]  eta: 0:01:03  lr: 0.000117  training_loss: 1.3316 (1.3645)  classification_loss: 1.3300 (1.3429)  loss_mask: 0.0017 (0.0215)  time: 0.1650  data: 0.0003  max mem: 4132
[19:33:19.230706] Epoch: [54]  [420/781]  eta: 0:00:59  lr: 0.000117  training_loss: 1.3533 (1.3635)  classification_loss: 1.3512 (1.3429)  loss_mask: 0.0019 (0.0206)  time: 0.1641  data: 0.0003  max mem: 4132
[19:33:22.506858] Epoch: [54]  [440/781]  eta: 0:00:56  lr: 0.000117  training_loss: 1.3221 (1.3618)  classification_loss: 1.3215 (1.3420)  loss_mask: 0.0012 (0.0198)  time: 0.1637  data: 0.0002  max mem: 4132
[19:33:25.764257] Epoch: [54]  [460/781]  eta: 0:00:53  lr: 0.000117  training_loss: 1.2814 (1.3586)  classification_loss: 1.2802 (1.3396)  loss_mask: 0.0012 (0.0190)  time: 0.1627  data: 0.0002  max mem: 4132
[19:33:29.040934] Epoch: [54]  [480/781]  eta: 0:00:49  lr: 0.000117  training_loss: 1.3631 (1.3597)  classification_loss: 1.3617 (1.3414)  loss_mask: 0.0014 (0.0182)  time: 0.1638  data: 0.0003  max mem: 4132
[19:33:32.347915] Epoch: [54]  [500/781]  eta: 0:00:46  lr: 0.000117  training_loss: 1.3619 (1.3594)  classification_loss: 1.3607 (1.3417)  loss_mask: 0.0019 (0.0177)  time: 0.1653  data: 0.0003  max mem: 4132
[19:33:35.624564] Epoch: [54]  [520/781]  eta: 0:00:43  lr: 0.000117  training_loss: 1.2936 (1.3580)  classification_loss: 1.2923 (1.3406)  loss_mask: 0.0024 (0.0174)  time: 0.1637  data: 0.0002  max mem: 4132
[19:33:38.958608] Epoch: [54]  [540/781]  eta: 0:00:39  lr: 0.000116  training_loss: 1.4204 (1.3613)  classification_loss: 1.3327 (1.3414)  loss_mask: 0.0577 (0.0199)  time: 0.1666  data: 0.0004  max mem: 4132
[19:33:42.237220] Epoch: [54]  [560/781]  eta: 0:00:36  lr: 0.000116  training_loss: 1.3867 (1.3619)  classification_loss: 1.3239 (1.3408)  loss_mask: 0.0555 (0.0211)  time: 0.1638  data: 0.0002  max mem: 4132
[19:33:45.515637] Epoch: [54]  [580/781]  eta: 0:00:33  lr: 0.000116  training_loss: 1.3918 (1.3623)  classification_loss: 1.3467 (1.3407)  loss_mask: 0.0266 (0.0216)  time: 0.1638  data: 0.0002  max mem: 4132
[19:33:48.858104] Epoch: [54]  [600/781]  eta: 0:00:29  lr: 0.000116  training_loss: 1.3345 (1.3609)  classification_loss: 1.2990 (1.3393)  loss_mask: 0.0134 (0.0217)  time: 0.1670  data: 0.0006  max mem: 4132
[19:33:52.152841] Epoch: [54]  [620/781]  eta: 0:00:26  lr: 0.000116  training_loss: 1.3200 (1.3606)  classification_loss: 1.2947 (1.3383)  loss_mask: 0.0254 (0.0223)  time: 0.1646  data: 0.0005  max mem: 4132
[19:33:55.420297] Epoch: [54]  [640/781]  eta: 0:00:23  lr: 0.000116  training_loss: 1.3962 (1.3623)  classification_loss: 1.3509 (1.3381)  loss_mask: 0.0548 (0.0242)  time: 0.1633  data: 0.0004  max mem: 4132
[19:33:58.728632] Epoch: [54]  [660/781]  eta: 0:00:20  lr: 0.000116  training_loss: 1.3869 (1.3631)  classification_loss: 1.3704 (1.3389)  loss_mask: 0.0168 (0.0242)  time: 0.1653  data: 0.0003  max mem: 4132
[19:34:02.056171] Epoch: [54]  [680/781]  eta: 0:00:16  lr: 0.000116  training_loss: 1.4125 (1.3642)  classification_loss: 1.3800 (1.3402)  loss_mask: 0.0088 (0.0240)  time: 0.1663  data: 0.0003  max mem: 4132
[19:34:05.350526] Epoch: [54]  [700/781]  eta: 0:00:13  lr: 0.000116  training_loss: 1.3118 (1.3631)  classification_loss: 1.3003 (1.3396)  loss_mask: 0.0078 (0.0235)  time: 0.1646  data: 0.0003  max mem: 4132
[19:34:08.619103] Epoch: [54]  [720/781]  eta: 0:00:10  lr: 0.000116  training_loss: 1.3399 (1.3634)  classification_loss: 1.3298 (1.3404)  loss_mask: 0.0055 (0.0230)  time: 0.1633  data: 0.0003  max mem: 4132
[19:34:11.903864] Epoch: [54]  [740/781]  eta: 0:00:06  lr: 0.000115  training_loss: 1.4406 (1.3656)  classification_loss: 1.3079 (1.3400)  loss_mask: 0.0436 (0.0256)  time: 0.1641  data: 0.0003  max mem: 4132
[19:34:15.156214] Epoch: [54]  [760/781]  eta: 0:00:03  lr: 0.000115  training_loss: 1.4310 (1.3682)  classification_loss: 1.3488 (1.3411)  loss_mask: 0.0613 (0.0272)  time: 0.1625  data: 0.0003  max mem: 4132
[19:34:18.408758] Epoch: [54]  [780/781]  eta: 0:00:00  lr: 0.000115  training_loss: 1.3784 (1.3688)  classification_loss: 1.3354 (1.3413)  loss_mask: 0.0430 (0.0274)  time: 0.1625  data: 0.0002  max mem: 4132
[19:34:18.572513] Epoch: [54] Total time: 0:02:09 (0.1656 s / it)
[19:34:18.573377] Averaged stats: lr: 0.000115  training_loss: 1.3784 (1.3688)  classification_loss: 1.3354 (1.3413)  loss_mask: 0.0430 (0.0274)
[19:34:19.290323] Test:  [  0/157]  eta: 0:01:51  testing_loss: 0.5697 (0.5697)  acc1: 82.8125 (82.8125)  acc5: 96.8750 (96.8750)  time: 0.7121  data: 0.6830  max mem: 4132
[19:34:19.584595] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.5971 (0.6153)  acc1: 81.2500 (80.5398)  acc5: 100.0000 (99.4318)  time: 0.0913  data: 0.0627  max mem: 4132
[19:34:19.871354] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5900 (0.5908)  acc1: 81.2500 (81.6220)  acc5: 100.0000 (99.2560)  time: 0.0289  data: 0.0004  max mem: 4132
[19:34:20.166216] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5926 (0.6090)  acc1: 82.8125 (81.2500)  acc5: 100.0000 (99.0423)  time: 0.0289  data: 0.0002  max mem: 4132
[19:34:20.459169] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6232 (0.6163)  acc1: 79.6875 (80.6402)  acc5: 98.4375 (99.0473)  time: 0.0292  data: 0.0003  max mem: 4132
[19:34:20.748863] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6097 (0.6107)  acc1: 81.2500 (81.1581)  acc5: 100.0000 (99.0809)  time: 0.0290  data: 0.0002  max mem: 4132
[19:34:21.036539] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5954 (0.6120)  acc1: 81.2500 (80.9170)  acc5: 100.0000 (99.1035)  time: 0.0287  data: 0.0002  max mem: 4132
[19:34:21.325083] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5715 (0.6049)  acc1: 81.2500 (81.3380)  acc5: 98.4375 (99.0757)  time: 0.0287  data: 0.0002  max mem: 4132
[19:34:21.616820] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6048 (0.6150)  acc1: 81.2500 (80.8835)  acc5: 98.4375 (99.0162)  time: 0.0289  data: 0.0002  max mem: 4132
[19:34:21.907789] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6400 (0.6133)  acc1: 79.6875 (80.9238)  acc5: 98.4375 (99.0041)  time: 0.0290  data: 0.0002  max mem: 4132
[19:34:22.199119] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.6112 (0.6165)  acc1: 79.6875 (80.8014)  acc5: 98.4375 (98.9325)  time: 0.0289  data: 0.0002  max mem: 4132
[19:34:22.492855] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6356 (0.6161)  acc1: 79.6875 (80.7714)  acc5: 100.0000 (98.9583)  time: 0.0291  data: 0.0003  max mem: 4132
[19:34:22.786674] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5833 (0.6127)  acc1: 81.2500 (80.8755)  acc5: 98.4375 (98.9153)  time: 0.0292  data: 0.0006  max mem: 4132
[19:34:23.076601] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5833 (0.6117)  acc1: 81.2500 (80.9280)  acc5: 100.0000 (98.9862)  time: 0.0290  data: 0.0005  max mem: 4132
[19:34:23.367848] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6175 (0.6107)  acc1: 81.2500 (81.0616)  acc5: 100.0000 (98.9916)  time: 0.0289  data: 0.0002  max mem: 4132
[19:34:23.652165] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6222 (0.6101)  acc1: 81.2500 (81.0534)  acc5: 100.0000 (99.0066)  time: 0.0286  data: 0.0002  max mem: 4132
[19:34:23.807282] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6163 (0.6117)  acc1: 79.6875 (80.9800)  acc5: 100.0000 (99.0200)  time: 0.0275  data: 0.0002  max mem: 4132
[19:34:23.985143] Test: Total time: 0:00:05 (0.0344 s / it)
[19:34:23.985784] * Acc@1 80.980 Acc@5 99.020 loss 0.612
[19:34:23.986221] Accuracy of the network on the 10000 test images: 81.0%
[19:34:23.986532] Max accuracy: 80.98%
[19:34:24.115731] log_dir: ./output_dir
[19:34:24.975367] Epoch: [55]  [  0/781]  eta: 0:11:09  lr: 0.000115  training_loss: 1.3125 (1.3125)  classification_loss: 1.2915 (1.2915)  loss_mask: 0.0210 (0.0210)  time: 0.8575  data: 0.6804  max mem: 4132
[19:34:28.306617] Epoch: [55]  [ 20/781]  eta: 0:02:31  lr: 0.000115  training_loss: 1.3184 (1.3493)  classification_loss: 1.3053 (1.3283)  loss_mask: 0.0138 (0.0210)  time: 0.1664  data: 0.0002  max mem: 4132
[19:34:31.592598] Epoch: [55]  [ 40/781]  eta: 0:02:15  lr: 0.000115  training_loss: 1.3717 (1.3514)  classification_loss: 1.3595 (1.3336)  loss_mask: 0.0102 (0.0178)  time: 0.1642  data: 0.0003  max mem: 4132
[19:34:34.864606] Epoch: [55]  [ 60/781]  eta: 0:02:06  lr: 0.000115  training_loss: 1.3663 (1.3564)  classification_loss: 1.3563 (1.3313)  loss_mask: 0.0179 (0.0251)  time: 0.1634  data: 0.0002  max mem: 4132
[19:34:38.157231] Epoch: [55]  [ 80/781]  eta: 0:02:01  lr: 0.000115  training_loss: 1.3210 (1.3537)  classification_loss: 1.2861 (1.3286)  loss_mask: 0.0155 (0.0252)  time: 0.1645  data: 0.0003  max mem: 4132

[19:34:41.424977] Epoch: [55]  [100/781]  eta: 0:01:56  lr: 0.000115  training_loss: 1.3194 (1.3489)  classification_loss: 1.3166 (1.3270)  loss_mask: 0.0067 (0.0219)  time: 0.1633  data: 0.0002  max mem: 4132
[19:34:44.727928] Epoch: [55]  [120/781]  eta: 0:01:52  lr: 0.000115  training_loss: 1.3237 (1.3485)  classification_loss: 1.3182 (1.3295)  loss_mask: 0.0040 (0.0190)  time: 0.1650  data: 0.0003  max mem: 4132
[19:34:48.025055] Epoch: [55]  [140/781]  eta: 0:01:48  lr: 0.000114  training_loss: 1.3154 (1.3443)  classification_loss: 1.3110 (1.3275)  loss_mask: 0.0023 (0.0168)  time: 0.1648  data: 0.0003  max mem: 4132
[19:34:51.342345] Epoch: [55]  [160/781]  eta: 0:01:44  lr: 0.000114  training_loss: 1.2767 (1.3387)  classification_loss: 1.2745 (1.3237)  loss_mask: 0.0024 (0.0150)  time: 0.1658  data: 0.0002  max mem: 4132
[19:34:54.659590] Epoch: [55]  [180/781]  eta: 0:01:41  lr: 0.000114  training_loss: 1.2823 (1.3359)  classification_loss: 1.2803 (1.3224)  loss_mask: 0.0018 (0.0136)  time: 0.1658  data: 0.0003  max mem: 4132
[19:34:58.011332] Epoch: [55]  [200/781]  eta: 0:01:37  lr: 0.000114  training_loss: 1.3308 (1.3350)  classification_loss: 1.3295 (1.3227)  loss_mask: 0.0017 (0.0124)  time: 0.1675  data: 0.0003  max mem: 4132
[19:35:01.320280] Epoch: [55]  [220/781]  eta: 0:01:34  lr: 0.000114  training_loss: 1.3342 (1.3386)  classification_loss: 1.3315 (1.3273)  loss_mask: 0.0015 (0.0114)  time: 0.1653  data: 0.0003  max mem: 4132
[19:35:04.644157] Epoch: [55]  [240/781]  eta: 0:01:30  lr: 0.000114  training_loss: 1.3002 (1.3357)  classification_loss: 1.2992 (1.3251)  loss_mask: 0.0015 (0.0106)  time: 0.1661  data: 0.0003  max mem: 4132
[19:35:07.980725] Epoch: [55]  [260/781]  eta: 0:01:27  lr: 0.000114  training_loss: 1.3247 (1.3355)  classification_loss: 1.3236 (1.3256)  loss_mask: 0.0011 (0.0099)  time: 0.1667  data: 0.0003  max mem: 4132
[19:35:11.257292] Epoch: [55]  [280/781]  eta: 0:01:23  lr: 0.000114  training_loss: 1.3039 (1.3337)  classification_loss: 1.3032 (1.3245)  loss_mask: 0.0010 (0.0092)  time: 0.1637  data: 0.0002  max mem: 4132
[19:35:14.554250] Epoch: [55]  [300/781]  eta: 0:01:20  lr: 0.000114  training_loss: 1.3240 (1.3343)  classification_loss: 1.3233 (1.3256)  loss_mask: 0.0012 (0.0087)  time: 0.1648  data: 0.0003  max mem: 4132
[19:35:17.848664] Epoch: [55]  [320/781]  eta: 0:01:17  lr: 0.000114  training_loss: 1.3139 (1.3343)  classification_loss: 1.3133 (1.3260)  loss_mask: 0.0011 (0.0083)  time: 0.1646  data: 0.0004  max mem: 4132
[19:35:21.129194] Epoch: [55]  [340/781]  eta: 0:01:13  lr: 0.000113  training_loss: 1.3294 (1.3334)  classification_loss: 1.3281 (1.3255)  loss_mask: 0.0011 (0.0078)  time: 0.1639  data: 0.0003  max mem: 4132
[19:35:24.447129] Epoch: [55]  [360/781]  eta: 0:01:10  lr: 0.000113  training_loss: 1.3481 (1.3342)  classification_loss: 1.3476 (1.3267)  loss_mask: 0.0008 (0.0074)  time: 0.1658  data: 0.0003  max mem: 4132
[19:35:27.736493] Epoch: [55]  [380/781]  eta: 0:01:06  lr: 0.000113  training_loss: 1.3207 (1.3341)  classification_loss: 1.3194 (1.3270)  loss_mask: 0.0015 (0.0071)  time: 0.1644  data: 0.0003  max mem: 4132
[19:35:31.029638] Epoch: [55]  [400/781]  eta: 0:01:03  lr: 0.000113  training_loss: 1.2988 (1.3331)  classification_loss: 1.2959 (1.3263)  loss_mask: 0.0009 (0.0068)  time: 0.1645  data: 0.0003  max mem: 4132
[19:35:34.335266] Epoch: [55]  [420/781]  eta: 0:01:00  lr: 0.000113  training_loss: 1.3323 (1.3333)  classification_loss: 1.3318 (1.3268)  loss_mask: 0.0008 (0.0066)  time: 0.1652  data: 0.0003  max mem: 4132
[19:35:37.654964] Epoch: [55]  [440/781]  eta: 0:00:56  lr: 0.000113  training_loss: 1.2808 (1.3321)  classification_loss: 1.2798 (1.3258)  loss_mask: 0.0008 (0.0063)  time: 0.1659  data: 0.0003  max mem: 4132
[19:35:40.972733] Epoch: [55]  [460/781]  eta: 0:00:53  lr: 0.000113  training_loss: 1.2962 (1.3305)  classification_loss: 1.2955 (1.3244)  loss_mask: 0.0008 (0.0061)  time: 0.1658  data: 0.0003  max mem: 4132
[19:35:44.275378] Epoch: [55]  [480/781]  eta: 0:00:50  lr: 0.000113  training_loss: 1.3719 (1.3329)  classification_loss: 1.3694 (1.3270)  loss_mask: 0.0008 (0.0059)  time: 0.1650  data: 0.0003  max mem: 4132
[19:35:47.544346] Epoch: [55]  [500/781]  eta: 0:00:46  lr: 0.000113  training_loss: 1.3430 (1.3330)  classification_loss: 1.3424 (1.3274)  loss_mask: 0.0007 (0.0056)  time: 0.1634  data: 0.0003  max mem: 4132
[19:35:50.793532] Epoch: [55]  [520/781]  eta: 0:00:43  lr: 0.000112  training_loss: 1.3042 (1.3321)  classification_loss: 1.3036 (1.3267)  loss_mask: 0.0006 (0.0055)  time: 0.1624  data: 0.0002  max mem: 4132
[19:35:54.084779] Epoch: [55]  [540/781]  eta: 0:00:40  lr: 0.000112  training_loss: 1.3669 (1.3331)  classification_loss: 1.3663 (1.3278)  loss_mask: 0.0007 (0.0053)  time: 0.1644  data: 0.0002  max mem: 4132
[19:35:57.388446] Epoch: [55]  [560/781]  eta: 0:00:36  lr: 0.000112  training_loss: 1.2977 (1.3323)  classification_loss: 1.2970 (1.3272)  loss_mask: 0.0008 (0.0051)  time: 0.1651  data: 0.0003  max mem: 4132
[19:36:00.682007] Epoch: [55]  [580/781]  eta: 0:00:33  lr: 0.000112  training_loss: 1.3294 (1.3327)  classification_loss: 1.3289 (1.3277)  loss_mask: 0.0006 (0.0050)  time: 0.1646  data: 0.0003  max mem: 4132
[19:36:03.987875] Epoch: [55]  [600/781]  eta: 0:00:30  lr: 0.000112  training_loss: 1.2838 (1.3314)  classification_loss: 1.2769 (1.3265)  loss_mask: 0.0009 (0.0049)  time: 0.1652  data: 0.0003  max mem: 4132
[19:36:07.288115] Epoch: [55]  [620/781]  eta: 0:00:26  lr: 0.000112  training_loss: 1.3553 (1.3340)  classification_loss: 1.3225 (1.3273)  loss_mask: 0.0172 (0.0067)  time: 0.1649  data: 0.0002  max mem: 4132
[19:36:10.579719] Epoch: [55]  [640/781]  eta: 0:00:23  lr: 0.000112  training_loss: 1.4949 (1.3409)  classification_loss: 1.3387 (1.3283)  loss_mask: 0.1018 (0.0126)  time: 0.1644  data: 0.0003  max mem: 4132
[19:36:13.869251] Epoch: [55]  [660/781]  eta: 0:00:20  lr: 0.000112  training_loss: 1.4213 (1.3436)  classification_loss: 1.3229 (1.3292)  loss_mask: 0.0637 (0.0144)  time: 0.1644  data: 0.0002  max mem: 4132
[19:36:17.162826] Epoch: [55]  [680/781]  eta: 0:00:16  lr: 0.000112  training_loss: 1.3658 (1.3442)  classification_loss: 1.3305 (1.3293)  loss_mask: 0.0309 (0.0149)  time: 0.1646  data: 0.0003  max mem: 4132
[19:36:20.442139] Epoch: [55]  [700/781]  eta: 0:00:13  lr: 0.000112  training_loss: 1.3472 (1.3443)  classification_loss: 1.3238 (1.3291)  loss_mask: 0.0199 (0.0152)  time: 0.1639  data: 0.0003  max mem: 4132
[19:36:23.723760] Epoch: [55]  [720/781]  eta: 0:00:10  lr: 0.000111  training_loss: 1.4763 (1.3481)  classification_loss: 1.3679 (1.3307)  loss_mask: 0.0252 (0.0173)  time: 0.1640  data: 0.0003  max mem: 4132
[19:36:27.024155] Epoch: [55]  [740/781]  eta: 0:00:06  lr: 0.000111  training_loss: 1.3994 (1.3493)  classification_loss: 1.2680 (1.3296)  loss_mask: 0.0736 (0.0197)  time: 0.1649  data: 0.0004  max mem: 4132
[19:36:30.348922] Epoch: [55]  [760/781]  eta: 0:00:03  lr: 0.000111  training_loss: 1.3436 (1.3501)  classification_loss: 1.2972 (1.3295)  loss_mask: 0.0384 (0.0206)  time: 0.1662  data: 0.0003  max mem: 4132
[19:36:33.664705] Epoch: [55]  [780/781]  eta: 0:00:00  lr: 0.000111  training_loss: 1.3364 (1.3499)  classification_loss: 1.3235 (1.3291)  loss_mask: 0.0252 (0.0209)  time: 0.1657  data: 0.0003  max mem: 4132
[19:36:33.852621] Epoch: [55] Total time: 0:02:09 (0.1661 s / it)
[19:36:33.853104] Averaged stats: lr: 0.000111  training_loss: 1.3364 (1.3499)  classification_loss: 1.3235 (1.3291)  loss_mask: 0.0252 (0.0209)
[19:36:34.594851] Test:  [  0/157]  eta: 0:01:55  testing_loss: 0.5913 (0.5913)  acc1: 85.9375 (85.9375)  acc5: 96.8750 (96.8750)  time: 0.7363  data: 0.6975  max mem: 4132
[19:36:34.894617] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.5913 (0.6312)  acc1: 81.2500 (80.1136)  acc5: 100.0000 (99.2898)  time: 0.0928  data: 0.0636  max mem: 4132
[19:36:35.180429] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5759 (0.5869)  acc1: 82.8125 (81.6964)  acc5: 100.0000 (99.2560)  time: 0.0285  data: 0.0002  max mem: 4132
[19:36:35.469754] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6052 (0.6144)  acc1: 81.2500 (80.3931)  acc5: 98.4375 (99.0423)  time: 0.0286  data: 0.0003  max mem: 4132
[19:36:35.755840] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6355 (0.6169)  acc1: 79.6875 (80.3354)  acc5: 98.4375 (98.8186)  time: 0.0286  data: 0.0003  max mem: 4132
[19:36:36.041366] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6127 (0.6097)  acc1: 81.2500 (80.9130)  acc5: 98.4375 (98.8664)  time: 0.0284  data: 0.0002  max mem: 4132
[19:36:36.329441] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6033 (0.6077)  acc1: 81.2500 (80.5328)  acc5: 100.0000 (98.8986)  time: 0.0285  data: 0.0003  max mem: 4132
[19:36:36.622850] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5665 (0.6021)  acc1: 81.2500 (80.8099)  acc5: 98.4375 (98.9437)  time: 0.0289  data: 0.0003  max mem: 4132
[19:36:36.910319] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6017 (0.6106)  acc1: 79.6875 (80.4784)  acc5: 98.4375 (98.9005)  time: 0.0289  data: 0.0003  max mem: 4132
[19:36:37.196848] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6268 (0.6092)  acc1: 79.6875 (80.4945)  acc5: 98.4375 (98.9011)  time: 0.0286  data: 0.0002  max mem: 4132
[19:36:37.487461] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.6376 (0.6148)  acc1: 78.1250 (80.1980)  acc5: 98.4375 (98.9016)  time: 0.0287  data: 0.0002  max mem: 4132
[19:36:37.777584] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6376 (0.6137)  acc1: 78.1250 (80.3773)  acc5: 100.0000 (98.8880)  time: 0.0289  data: 0.0002  max mem: 4132
[19:36:38.068112] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5873 (0.6115)  acc1: 81.2500 (80.4881)  acc5: 100.0000 (98.8895)  time: 0.0289  data: 0.0002  max mem: 4132
[19:36:38.354043] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5906 (0.6113)  acc1: 81.2500 (80.5344)  acc5: 100.0000 (98.9265)  time: 0.0287  data: 0.0003  max mem: 4132
[19:36:38.640187] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6178 (0.6107)  acc1: 79.6875 (80.5297)  acc5: 100.0000 (98.9583)  time: 0.0284  data: 0.0002  max mem: 4132
[19:36:38.921954] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6091 (0.6083)  acc1: 79.6875 (80.6084)  acc5: 100.0000 (98.9652)  time: 0.0282  data: 0.0002  max mem: 4132
[19:36:39.073474] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5769 (0.6085)  acc1: 81.2500 (80.6400)  acc5: 100.0000 (98.9900)  time: 0.0272  data: 0.0001  max mem: 4132
[19:36:39.250161] Test: Total time: 0:00:05 (0.0344 s / it)
[19:36:39.250620] * Acc@1 80.640 Acc@5 98.990 loss 0.609
[19:36:39.250924] Accuracy of the network on the 10000 test images: 80.6%
[19:36:39.251124] Max accuracy: 80.98%
[19:36:39.335953] log_dir: ./output_dir
[19:36:40.286343] Epoch: [56]  [  0/781]  eta: 0:12:20  lr: 0.000111  training_loss: 1.1715 (1.1715)  classification_loss: 1.1459 (1.1459)  loss_mask: 0.0257 (0.0257)  time: 0.9482  data: 0.7705  max mem: 4132
[19:36:43.597225] Epoch: [56]  [ 20/781]  eta: 0:02:34  lr: 0.000111  training_loss: 1.2767 (1.3103)  classification_loss: 1.2463 (1.2817)  loss_mask: 0.0189 (0.0286)  time: 0.1654  data: 0.0003  max mem: 4132
[19:36:46.890603] Epoch: [56]  [ 40/781]  eta: 0:02:16  lr: 0.000111  training_loss: 1.3338 (1.3201)  classification_loss: 1.3131 (1.2972)  loss_mask: 0.0132 (0.0229)  time: 0.1646  data: 0.0003  max mem: 4132
[19:36:50.213866] Epoch: [56]  [ 60/781]  eta: 0:02:08  lr: 0.000111  training_loss: 1.3937 (1.3524)  classification_loss: 1.3479 (1.3065)  loss_mask: 0.0498 (0.0459)  time: 0.1661  data: 0.0003  max mem: 4132
[19:36:53.543111] Epoch: [56]  [ 80/781]  eta: 0:02:02  lr: 0.000111  training_loss: 1.3564 (1.3506)  classification_loss: 1.3145 (1.3054)  loss_mask: 0.0365 (0.0452)  time: 0.1663  data: 0.0003  max mem: 4132
[19:36:56.806541] Epoch: [56]  [100/781]  eta: 0:01:57  lr: 0.000111  training_loss: 1.3645 (1.3503)  classification_loss: 1.3203 (1.3074)  loss_mask: 0.0213 (0.0430)  time: 0.1631  data: 0.0003  max mem: 4132
[19:37:00.087571] Epoch: [56]  [120/781]  eta: 0:01:53  lr: 0.000110  training_loss: 1.3137 (1.3432)  classification_loss: 1.3042 (1.3058)  loss_mask: 0.0086 (0.0375)  time: 0.1639  data: 0.0002  max mem: 4132
[19:37:03.350656] Epoch: [56]  [140/781]  eta: 0:01:49  lr: 0.000110  training_loss: 1.3071 (1.3390)  classification_loss: 1.2998 (1.3060)  loss_mask: 0.0056 (0.0330)  time: 0.1630  data: 0.0002  max mem: 4132
[19:37:06.632871] Epoch: [56]  [160/781]  eta: 0:01:45  lr: 0.000110  training_loss: 1.2971 (1.3326)  classification_loss: 1.2935 (1.3032)  loss_mask: 0.0032 (0.0294)  time: 0.1640  data: 0.0002  max mem: 4132
[19:37:09.931084] Epoch: [56]  [180/781]  eta: 0:01:41  lr: 0.000110  training_loss: 1.2853 (1.3307)  classification_loss: 1.2784 (1.3043)  loss_mask: 0.0023 (0.0265)  time: 0.1648  data: 0.0003  max mem: 4132
[19:37:13.254795] Epoch: [56]  [200/781]  eta: 0:01:37  lr: 0.000110  training_loss: 1.3492 (1.3339)  classification_loss: 1.3465 (1.3098)  loss_mask: 0.0021 (0.0241)  time: 0.1661  data: 0.0004  max mem: 4132
[19:37:16.538260] Epoch: [56]  [220/781]  eta: 0:01:34  lr: 0.000110  training_loss: 1.2929 (1.3319)  classification_loss: 1.2889 (1.3099)  loss_mask: 0.0017 (0.0221)  time: 0.1641  data: 0.0003  max mem: 4132
[19:37:19.841988] Epoch: [56]  [240/781]  eta: 0:01:30  lr: 0.000110  training_loss: 1.3132 (1.3300)  classification_loss: 1.3127 (1.3094)  loss_mask: 0.0014 (0.0205)  time: 0.1651  data: 0.0003  max mem: 4132
[19:37:23.135933] Epoch: [56]  [260/781]  eta: 0:01:27  lr: 0.000110  training_loss: 1.3196 (1.3282)  classification_loss: 1.3064 (1.3088)  loss_mask: 0.0022 (0.0194)  time: 0.1646  data: 0.0003  max mem: 4132
[19:37:26.404025] Epoch: [56]  [280/781]  eta: 0:01:23  lr: 0.000110  training_loss: 1.3571 (1.3332)  classification_loss: 1.3117 (1.3116)  loss_mask: 0.0123 (0.0216)  time: 0.1633  data: 0.0003  max mem: 4132
[19:37:29.681535] Epoch: [56]  [300/781]  eta: 0:01:20  lr: 0.000110  training_loss: 1.3281 (1.3324)  classification_loss: 1.2816 (1.3098)  loss_mask: 0.0270 (0.0226)  time: 0.1637  data: 0.0003  max mem: 4132
[19:37:33.025030] Epoch: [56]  [320/781]  eta: 0:01:17  lr: 0.000109  training_loss: 1.2813 (1.3303)  classification_loss: 1.2644 (1.3083)  loss_mask: 0.0123 (0.0220)  time: 0.1670  data: 0.0003  max mem: 4132
[19:37:36.325304] Epoch: [56]  [340/781]  eta: 0:01:13  lr: 0.000109  training_loss: 1.2975 (1.3285)  classification_loss: 1.2930 (1.3075)  loss_mask: 0.0053 (0.0210)  time: 0.1649  data: 0.0003  max mem: 4132
[19:37:39.647046] Epoch: [56]  [360/781]  eta: 0:01:10  lr: 0.000109  training_loss: 1.3006 (1.3301)  classification_loss: 1.2978 (1.3096)  loss_mask: 0.0035 (0.0205)  time: 0.1660  data: 0.0003  max mem: 4132
[19:37:42.973737] Epoch: [56]  [380/781]  eta: 0:01:06  lr: 0.000109  training_loss: 1.3073 (1.3289)  classification_loss: 1.3005 (1.3093)  loss_mask: 0.0028 (0.0197)  time: 0.1662  data: 0.0004  max mem: 4132
[19:37:46.270451] Epoch: [56]  [400/781]  eta: 0:01:03  lr: 0.000109  training_loss: 1.3040 (1.3288)  classification_loss: 1.2920 (1.3098)  loss_mask: 0.0028 (0.0189)  time: 0.1647  data: 0.0002  max mem: 4132
[19:37:49.559735] Epoch: [56]  [420/781]  eta: 0:01:00  lr: 0.000109  training_loss: 1.3011 (1.3286)  classification_loss: 1.2999 (1.3104)  loss_mask: 0.0027 (0.0182)  time: 0.1644  data: 0.0002  max mem: 4132
[19:37:52.853217] Epoch: [56]  [440/781]  eta: 0:00:56  lr: 0.000109  training_loss: 1.3025 (1.3284)  classification_loss: 1.3016 (1.3109)  loss_mask: 0.0011 (0.0175)  time: 0.1646  data: 0.0003  max mem: 4132
[19:37:56.153671] Epoch: [56]  [460/781]  eta: 0:00:53  lr: 0.000109  training_loss: 1.2878 (1.3276)  classification_loss: 1.2871 (1.3108)  loss_mask: 0.0010 (0.0168)  time: 0.1649  data: 0.0003  max mem: 4132
[19:37:59.440922] Epoch: [56]  [480/781]  eta: 0:00:50  lr: 0.000109  training_loss: 1.3064 (1.3276)  classification_loss: 1.3054 (1.3115)  loss_mask: 0.0010 (0.0161)  time: 0.1643  data: 0.0003  max mem: 4132
[19:38:02.736771] Epoch: [56]  [500/781]  eta: 0:00:46  lr: 0.000109  training_loss: 1.3277 (1.3286)  classification_loss: 1.3273 (1.3131)  loss_mask: 0.0009 (0.0155)  time: 0.1646  data: 0.0003  max mem: 4132
[19:38:06.066902] Epoch: [56]  [520/781]  eta: 0:00:43  lr: 0.000108  training_loss: 1.3278 (1.3296)  classification_loss: 1.3250 (1.3144)  loss_mask: 0.0028 (0.0152)  time: 0.1663  data: 0.0003  max mem: 4132
[19:38:09.329679] Epoch: [56]  [540/781]  eta: 0:00:40  lr: 0.000108  training_loss: 1.3342 (1.3304)  classification_loss: 1.3293 (1.3153)  loss_mask: 0.0061 (0.0151)  time: 0.1630  data: 0.0002  max mem: 4132
[19:38:12.640628] Epoch: [56]  [560/781]  eta: 0:00:36  lr: 0.000108  training_loss: 1.2702 (1.3283)  classification_loss: 1.2692 (1.3137)  loss_mask: 0.0014 (0.0146)  time: 0.1654  data: 0.0003  max mem: 4132
[19:38:15.918582] Epoch: [56]  [580/781]  eta: 0:00:33  lr: 0.000108  training_loss: 1.2869 (1.3273)  classification_loss: 1.2864 (1.3130)  loss_mask: 0.0028 (0.0143)  time: 0.1638  data: 0.0002  max mem: 4132
[19:38:19.205142] Epoch: [56]  [600/781]  eta: 0:00:30  lr: 0.000108  training_loss: 1.3820 (1.3313)  classification_loss: 1.3152 (1.3144)  loss_mask: 0.0512 (0.0169)  time: 0.1642  data: 0.0002  max mem: 4132
[19:38:22.482636] Epoch: [56]  [620/781]  eta: 0:00:26  lr: 0.000108  training_loss: 1.4948 (1.3387)  classification_loss: 1.3625 (1.3153)  loss_mask: 0.1452 (0.0234)  time: 0.1638  data: 0.0002  max mem: 4132
[19:38:25.777455] Epoch: [56]  [640/781]  eta: 0:00:23  lr: 0.000108  training_loss: 1.3570 (1.3403)  classification_loss: 1.2835 (1.3147)  loss_mask: 0.0643 (0.0256)  time: 0.1647  data: 0.0004  max mem: 4132
[19:38:29.054929] Epoch: [56]  [660/781]  eta: 0:00:20  lr: 0.000108  training_loss: 1.3266 (1.3406)  classification_loss: 1.2787 (1.3144)  loss_mask: 0.0383 (0.0262)  time: 0.1638  data: 0.0003  max mem: 4132
[19:38:32.310410] Epoch: [56]  [680/781]  eta: 0:00:16  lr: 0.000108  training_loss: 1.3627 (1.3410)  classification_loss: 1.3360 (1.3149)  loss_mask: 0.0211 (0.0261)  time: 0.1627  data: 0.0002  max mem: 4132
[19:38:35.612035] Epoch: [56]  [700/781]  eta: 0:00:13  lr: 0.000107  training_loss: 1.3148 (1.3403)  classification_loss: 1.3052 (1.3146)  loss_mask: 0.0119 (0.0257)  time: 0.1650  data: 0.0003  max mem: 4132
[19:38:38.904131] Epoch: [56]  [720/781]  eta: 0:00:10  lr: 0.000107  training_loss: 1.3818 (1.3414)  classification_loss: 1.3756 (1.3163)  loss_mask: 0.0061 (0.0252)  time: 0.1645  data: 0.0002  max mem: 4132
[19:38:42.203959] Epoch: [56]  [740/781]  eta: 0:00:06  lr: 0.000107  training_loss: 1.3245 (1.3407)  classification_loss: 1.3192 (1.3161)  loss_mask: 0.0036 (0.0246)  time: 0.1649  data: 0.0002  max mem: 4132
[19:38:45.510107] Epoch: [56]  [760/781]  eta: 0:00:03  lr: 0.000107  training_loss: 1.4127 (1.3422)  classification_loss: 1.3960 (1.3179)  loss_mask: 0.0032 (0.0243)  time: 0.1652  data: 0.0003  max mem: 4132
[19:38:48.814993] Epoch: [56]  [780/781]  eta: 0:00:00  lr: 0.000107  training_loss: 1.3528 (1.3431)  classification_loss: 1.3475 (1.3194)  loss_mask: 0.0021 (0.0238)  time: 0.1652  data: 0.0002  max mem: 4132
[19:38:48.988370] Epoch: [56] Total time: 0:02:09 (0.1660 s / it)
[19:38:48.988871] Averaged stats: lr: 0.000107  training_loss: 1.3528 (1.3431)  classification_loss: 1.3475 (1.3194)  loss_mask: 0.0021 (0.0238)
[19:38:49.713781] Test:  [  0/157]  eta: 0:01:52  testing_loss: 0.5740 (0.5740)  acc1: 84.3750 (84.3750)  acc5: 96.8750 (96.8750)  time: 0.7189  data: 0.6653  max mem: 4132
[19:38:50.004424] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.5998 (0.6041)  acc1: 81.2500 (80.9659)  acc5: 100.0000 (99.2898)  time: 0.0915  data: 0.0607  max mem: 4132
[19:38:50.296348] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5640 (0.5839)  acc1: 81.2500 (81.8452)  acc5: 100.0000 (99.3304)  time: 0.0289  data: 0.0002  max mem: 4132
[19:38:50.596643] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5859 (0.6023)  acc1: 82.8125 (81.2500)  acc5: 98.4375 (98.9919)  time: 0.0295  data: 0.0003  max mem: 4132
[19:38:50.884829] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6204 (0.6060)  acc1: 81.2500 (81.1357)  acc5: 98.4375 (98.8567)  time: 0.0292  data: 0.0003  max mem: 4132
[19:38:51.180233] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5835 (0.6006)  acc1: 81.2500 (81.5564)  acc5: 98.4375 (98.8971)  time: 0.0290  data: 0.0003  max mem: 4132
[19:38:51.471984] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5667 (0.5995)  acc1: 81.2500 (81.4293)  acc5: 100.0000 (98.9242)  time: 0.0291  data: 0.0003  max mem: 4132
[19:38:51.769332] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5606 (0.5939)  acc1: 82.8125 (81.6461)  acc5: 98.4375 (98.9657)  time: 0.0292  data: 0.0004  max mem: 4132
[19:38:52.057885] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5797 (0.6037)  acc1: 79.6875 (81.1343)  acc5: 98.4375 (98.8619)  time: 0.0290  data: 0.0004  max mem: 4132
[19:38:52.345851] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5846 (0.5988)  acc1: 81.2500 (81.3702)  acc5: 98.4375 (98.9011)  time: 0.0286  data: 0.0003  max mem: 4132
[19:38:52.635432] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.6073 (0.6044)  acc1: 81.2500 (81.1572)  acc5: 100.0000 (98.9016)  time: 0.0287  data: 0.0003  max mem: 4132
[19:38:52.924263] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6302 (0.6029)  acc1: 79.6875 (81.2500)  acc5: 100.0000 (98.9583)  time: 0.0287  data: 0.0003  max mem: 4132
[19:38:53.215097] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5615 (0.5991)  acc1: 82.8125 (81.4050)  acc5: 100.0000 (98.9669)  time: 0.0288  data: 0.0003  max mem: 4132
[19:38:53.501933] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5729 (0.5999)  acc1: 81.2500 (81.4170)  acc5: 100.0000 (99.0100)  time: 0.0287  data: 0.0002  max mem: 4132
[19:38:53.787412] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6333 (0.5999)  acc1: 79.6875 (81.5049)  acc5: 100.0000 (99.0248)  time: 0.0285  data: 0.0002  max mem: 4132
[19:38:54.072721] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6333 (0.5999)  acc1: 79.6875 (81.5501)  acc5: 98.4375 (98.9963)  time: 0.0284  data: 0.0002  max mem: 4132
[19:38:54.228588] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5889 (0.5995)  acc1: 82.8125 (81.5600)  acc5: 98.4375 (99.0100)  time: 0.0276  data: 0.0002  max mem: 4132
[19:38:54.391514] Test: Total time: 0:00:05 (0.0344 s / it)
[19:38:54.392048] * Acc@1 81.560 Acc@5 99.010 loss 0.599
[19:38:54.392374] Accuracy of the network on the 10000 test images: 81.6%
[19:38:54.392552] Max accuracy: 81.56%
[19:38:54.510523] log_dir: ./output_dir
[19:38:55.451290] Epoch: [57]  [  0/781]  eta: 0:12:13  lr: 0.000107  training_loss: 1.4640 (1.4640)  classification_loss: 1.4618 (1.4618)  loss_mask: 0.0022 (0.0022)  time: 0.9386  data: 0.7269  max mem: 4132
[19:38:58.778631] Epoch: [57]  [ 20/781]  eta: 0:02:34  lr: 0.000107  training_loss: 1.3054 (1.3384)  classification_loss: 1.3034 (1.3344)  loss_mask: 0.0021 (0.0039)  time: 0.1662  data: 0.0003  max mem: 4132
[19:39:02.099651] Epoch: [57]  [ 40/781]  eta: 0:02:17  lr: 0.000107  training_loss: 1.3261 (1.3326)  classification_loss: 1.3197 (1.3273)  loss_mask: 0.0032 (0.0053)  time: 0.1660  data: 0.0003  max mem: 4132
[19:39:05.390686] Epoch: [57]  [ 60/781]  eta: 0:02:08  lr: 0.000107  training_loss: 1.3678 (1.3439)  classification_loss: 1.3578 (1.3385)  loss_mask: 0.0022 (0.0054)  time: 0.1645  data: 0.0002  max mem: 4132
[19:39:08.687879] Epoch: [57]  [ 80/781]  eta: 0:02:02  lr: 0.000107  training_loss: 1.2883 (1.3324)  classification_loss: 1.2810 (1.3272)  loss_mask: 0.0026 (0.0052)  time: 0.1648  data: 0.0003  max mem: 4132
[19:39:12.001802] Epoch: [57]  [100/781]  eta: 0:01:57  lr: 0.000107  training_loss: 1.3548 (1.3332)  classification_loss: 1.3538 (1.3287)  loss_mask: 0.0010 (0.0044)  time: 0.1656  data: 0.0003  max mem: 4132
[19:39:15.293319] Epoch: [57]  [120/781]  eta: 0:01:53  lr: 0.000106  training_loss: 1.3145 (1.3288)  classification_loss: 1.3123 (1.3249)  loss_mask: 0.0015 (0.0039)  time: 0.1645  data: 0.0003  max mem: 4132
[19:39:18.566593] Epoch: [57]  [140/781]  eta: 0:01:49  lr: 0.000106  training_loss: 1.2786 (1.3224)  classification_loss: 1.2781 (1.3183)  loss_mask: 0.0010 (0.0041)  time: 0.1636  data: 0.0003  max mem: 4132
[19:39:21.885942] Epoch: [57]  [160/781]  eta: 0:01:45  lr: 0.000106  training_loss: 1.3493 (1.3276)  classification_loss: 1.3463 (1.3233)  loss_mask: 0.0032 (0.0043)  time: 0.1659  data: 0.0004  max mem: 4132
[19:39:25.160749] Epoch: [57]  [180/781]  eta: 0:01:41  lr: 0.000106  training_loss: 1.3241 (1.3306)  classification_loss: 1.3142 (1.3252)  loss_mask: 0.0100 (0.0053)  time: 0.1637  data: 0.0002  max mem: 4132
[19:39:28.461029] Epoch: [57]  [200/781]  eta: 0:01:38  lr: 0.000106  training_loss: 1.3009 (1.3299)  classification_loss: 1.2970 (1.3244)  loss_mask: 0.0035 (0.0055)  time: 0.1649  data: 0.0002  max mem: 4132
[19:39:31.742444] Epoch: [57]  [220/781]  eta: 0:01:34  lr: 0.000106  training_loss: 1.3055 (1.3272)  classification_loss: 1.3042 (1.3219)  loss_mask: 0.0020 (0.0053)  time: 0.1640  data: 0.0002  max mem: 4132
[19:39:35.031379] Epoch: [57]  [240/781]  eta: 0:01:30  lr: 0.000106  training_loss: 1.2807 (1.3238)  classification_loss: 1.2801 (1.3188)  loss_mask: 0.0009 (0.0050)  time: 0.1643  data: 0.0003  max mem: 4132
[19:39:38.332072] Epoch: [57]  [260/781]  eta: 0:01:27  lr: 0.000106  training_loss: 1.3290 (1.3231)  classification_loss: 1.3269 (1.3184)  loss_mask: 0.0010 (0.0047)  time: 0.1649  data: 0.0003  max mem: 4132
[19:39:41.608802] Epoch: [57]  [280/781]  eta: 0:01:23  lr: 0.000106  training_loss: 1.3147 (1.3224)  classification_loss: 1.3140 (1.3180)  loss_mask: 0.0009 (0.0044)  time: 0.1637  data: 0.0003  max mem: 4132
[19:39:44.919621] Epoch: [57]  [300/781]  eta: 0:01:20  lr: 0.000105  training_loss: 1.3362 (1.3248)  classification_loss: 1.3357 (1.3206)  loss_mask: 0.0006 (0.0042)  time: 0.1655  data: 0.0003  max mem: 4132
[19:39:48.203432] Epoch: [57]  [320/781]  eta: 0:01:17  lr: 0.000105  training_loss: 1.3186 (1.3237)  classification_loss: 1.3177 (1.3196)  loss_mask: 0.0011 (0.0041)  time: 0.1641  data: 0.0003  max mem: 4132
[19:39:51.492724] Epoch: [57]  [340/781]  eta: 0:01:13  lr: 0.000105  training_loss: 1.3236 (1.3236)  classification_loss: 1.3230 (1.3196)  loss_mask: 0.0009 (0.0040)  time: 0.1644  data: 0.0003  max mem: 4132
[19:39:54.804175] Epoch: [57]  [360/781]  eta: 0:01:10  lr: 0.000105  training_loss: 1.3711 (1.3253)  classification_loss: 1.3701 (1.3215)  loss_mask: 0.0005 (0.0038)  time: 0.1654  data: 0.0003  max mem: 4132
[19:39:58.110618] Epoch: [57]  [380/781]  eta: 0:01:06  lr: 0.000105  training_loss: 1.3047 (1.3247)  classification_loss: 1.3044 (1.3211)  loss_mask: 0.0004 (0.0036)  time: 0.1652  data: 0.0003  max mem: 4132
[19:40:01.405546] Epoch: [57]  [400/781]  eta: 0:01:03  lr: 0.000105  training_loss: 1.3214 (1.3250)  classification_loss: 1.3210 (1.3215)  loss_mask: 0.0003 (0.0035)  time: 0.1647  data: 0.0003  max mem: 4132
[19:40:04.710545] Epoch: [57]  [420/781]  eta: 0:01:00  lr: 0.000105  training_loss: 1.2869 (1.3242)  classification_loss: 1.2862 (1.3208)  loss_mask: 0.0003 (0.0033)  time: 0.1652  data: 0.0003  max mem: 4132
[19:40:08.016981] Epoch: [57]  [440/781]  eta: 0:00:56  lr: 0.000105  training_loss: 1.2914 (1.3229)  classification_loss: 1.2904 (1.3197)  loss_mask: 0.0004 (0.0032)  time: 0.1651  data: 0.0003  max mem: 4132
[19:40:11.320183] Epoch: [57]  [460/781]  eta: 0:00:53  lr: 0.000105  training_loss: 1.2796 (1.3223)  classification_loss: 1.2792 (1.3192)  loss_mask: 0.0003 (0.0031)  time: 0.1651  data: 0.0004  max mem: 4132
[19:40:14.612423] Epoch: [57]  [480/781]  eta: 0:00:50  lr: 0.000105  training_loss: 1.3177 (1.3221)  classification_loss: 1.3173 (1.3191)  loss_mask: 0.0003 (0.0030)  time: 0.1645  data: 0.0003  max mem: 4132
[19:40:17.895519] Epoch: [57]  [500/781]  eta: 0:00:46  lr: 0.000104  training_loss: 1.2922 (1.3226)  classification_loss: 1.2920 (1.3197)  loss_mask: 0.0004 (0.0029)  time: 0.1640  data: 0.0003  max mem: 4132
[19:40:21.192804] Epoch: [57]  [520/781]  eta: 0:00:43  lr: 0.000104  training_loss: 1.3193 (1.3231)  classification_loss: 1.3189 (1.3203)  loss_mask: 0.0003 (0.0028)  time: 0.1648  data: 0.0003  max mem: 4132
[19:40:24.479967] Epoch: [57]  [540/781]  eta: 0:00:40  lr: 0.000104  training_loss: 1.3297 (1.3230)  classification_loss: 1.3294 (1.3203)  loss_mask: 0.0003 (0.0027)  time: 0.1642  data: 0.0002  max mem: 4132
[19:40:27.739378] Epoch: [57]  [560/781]  eta: 0:00:36  lr: 0.000104  training_loss: 1.2763 (1.3218)  classification_loss: 1.2762 (1.3192)  loss_mask: 0.0003 (0.0026)  time: 0.1629  data: 0.0003  max mem: 4132
[19:40:31.034766] Epoch: [57]  [580/781]  eta: 0:00:33  lr: 0.000104  training_loss: 1.2752 (1.3204)  classification_loss: 1.2748 (1.3179)  loss_mask: 0.0003 (0.0025)  time: 0.1647  data: 0.0003  max mem: 4132
[19:40:34.310951] Epoch: [57]  [600/781]  eta: 0:00:30  lr: 0.000104  training_loss: 1.2208 (1.3185)  classification_loss: 1.2205 (1.3160)  loss_mask: 0.0003 (0.0024)  time: 0.1637  data: 0.0002  max mem: 4132
[19:40:37.588634] Epoch: [57]  [620/781]  eta: 0:00:26  lr: 0.000104  training_loss: 1.3507 (1.3193)  classification_loss: 1.3503 (1.3170)  loss_mask: 0.0003 (0.0024)  time: 0.1638  data: 0.0002  max mem: 4132
[19:40:40.902406] Epoch: [57]  [640/781]  eta: 0:00:23  lr: 0.000104  training_loss: 1.3212 (1.3196)  classification_loss: 1.3191 (1.3173)  loss_mask: 0.0003 (0.0023)  time: 0.1656  data: 0.0003  max mem: 4132
[19:40:44.176161] Epoch: [57]  [660/781]  eta: 0:00:20  lr: 0.000104  training_loss: 1.3340 (1.3207)  classification_loss: 1.3335 (1.3185)  loss_mask: 0.0002 (0.0022)  time: 0.1636  data: 0.0003  max mem: 4132
[19:40:47.455458] Epoch: [57]  [680/781]  eta: 0:00:16  lr: 0.000104  training_loss: 1.3397 (1.3214)  classification_loss: 1.3394 (1.3193)  loss_mask: 0.0002 (0.0022)  time: 0.1639  data: 0.0002  max mem: 4132
[19:40:50.729228] Epoch: [57]  [700/781]  eta: 0:00:13  lr: 0.000103  training_loss: 1.2945 (1.3205)  classification_loss: 1.2944 (1.3184)  loss_mask: 0.0002 (0.0021)  time: 0.1636  data: 0.0002  max mem: 4132
[19:40:54.041414] Epoch: [57]  [720/781]  eta: 0:00:10  lr: 0.000103  training_loss: 1.3152 (1.3207)  classification_loss: 1.3149 (1.3186)  loss_mask: 0.0003 (0.0021)  time: 0.1655  data: 0.0003  max mem: 4132
[19:40:57.363979] Epoch: [57]  [740/781]  eta: 0:00:06  lr: 0.000103  training_loss: 1.2398 (1.3193)  classification_loss: 1.2395 (1.3173)  loss_mask: 0.0003 (0.0020)  time: 0.1660  data: 0.0003  max mem: 4132
[19:41:00.673666] Epoch: [57]  [760/781]  eta: 0:00:03  lr: 0.000103  training_loss: 1.3711 (1.3205)  classification_loss: 1.3709 (1.3186)  loss_mask: 0.0002 (0.0020)  time: 0.1654  data: 0.0003  max mem: 4132
[19:41:03.963840] Epoch: [57]  [780/781]  eta: 0:00:00  lr: 0.000103  training_loss: 1.2797 (1.3203)  classification_loss: 1.2795 (1.3184)  loss_mask: 0.0003 (0.0019)  time: 0.1644  data: 0.0003  max mem: 4132
[19:41:04.154190] Epoch: [57] Total time: 0:02:09 (0.1660 s / it)
[19:41:04.154979] Averaged stats: lr: 0.000103  training_loss: 1.2797 (1.3203)  classification_loss: 1.2795 (1.3184)  loss_mask: 0.0003 (0.0019)
[19:41:05.036294] Test:  [  0/157]  eta: 0:02:17  testing_loss: 0.5273 (0.5273)  acc1: 84.3750 (84.3750)  acc5: 98.4375 (98.4375)  time: 0.8733  data: 0.8312  max mem: 4132
[19:41:05.328154] Test:  [ 10/157]  eta: 0:00:15  testing_loss: 0.5716 (0.5936)  acc1: 79.6875 (78.9773)  acc5: 98.4375 (99.0057)  time: 0.1056  data: 0.0759  max mem: 4132
[19:41:05.613748] Test:  [ 20/157]  eta: 0:00:09  testing_loss: 0.5621 (0.5555)  acc1: 81.2500 (81.3988)  acc5: 100.0000 (99.3304)  time: 0.0286  data: 0.0003  max mem: 4132
[19:41:05.907385] Test:  [ 30/157]  eta: 0:00:07  testing_loss: 0.5787 (0.5817)  acc1: 82.8125 (80.8468)  acc5: 100.0000 (99.0927)  time: 0.0287  data: 0.0003  max mem: 4132
[19:41:06.195453] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5931 (0.5851)  acc1: 81.2500 (81.0595)  acc5: 98.4375 (99.1235)  time: 0.0289  data: 0.0003  max mem: 4132
[19:41:06.480576] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5768 (0.5791)  acc1: 82.8125 (81.4338)  acc5: 98.4375 (99.0809)  time: 0.0285  data: 0.0002  max mem: 4132
[19:41:06.769233] Test:  [ 60/157]  eta: 0:00:04  testing_loss: 0.5386 (0.5777)  acc1: 82.8125 (81.2756)  acc5: 98.4375 (99.0779)  time: 0.0285  data: 0.0002  max mem: 4132
[19:41:07.059595] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5499 (0.5736)  acc1: 82.8125 (81.4921)  acc5: 100.0000 (99.0977)  time: 0.0287  data: 0.0002  max mem: 4132
[19:41:07.347374] Test:  [ 80/157]  eta: 0:00:03  testing_loss: 0.5636 (0.5820)  acc1: 79.6875 (81.1150)  acc5: 100.0000 (99.1319)  time: 0.0287  data: 0.0003  max mem: 4132
[19:41:07.635122] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6049 (0.5808)  acc1: 79.6875 (81.2843)  acc5: 100.0000 (99.1587)  time: 0.0286  data: 0.0003  max mem: 4132
[19:41:07.922263] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.5931 (0.5847)  acc1: 81.2500 (81.2036)  acc5: 98.4375 (99.1182)  time: 0.0286  data: 0.0002  max mem: 4132
[19:41:08.209506] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5931 (0.5844)  acc1: 79.6875 (81.1655)  acc5: 98.4375 (99.1273)  time: 0.0286  data: 0.0002  max mem: 4132
[19:41:08.496247] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5500 (0.5811)  acc1: 79.6875 (81.2629)  acc5: 98.4375 (99.1219)  time: 0.0286  data: 0.0002  max mem: 4132
[19:41:08.781400] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5822 (0.5819)  acc1: 79.6875 (81.1665)  acc5: 100.0000 (99.1412)  time: 0.0284  data: 0.0002  max mem: 4132
[19:41:09.066032] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5476 (0.5782)  acc1: 82.8125 (81.3497)  acc5: 100.0000 (99.1689)  time: 0.0283  data: 0.0002  max mem: 4132
[19:41:09.349358] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5430 (0.5770)  acc1: 82.8125 (81.4259)  acc5: 100.0000 (99.1722)  time: 0.0283  data: 0.0002  max mem: 4132
[19:41:09.503665] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5430 (0.5764)  acc1: 82.8125 (81.4600)  acc5: 100.0000 (99.1900)  time: 0.0274  data: 0.0002  max mem: 4132
[19:41:09.685536] Test: Total time: 0:00:05 (0.0352 s / it)
[19:41:09.686080] * Acc@1 81.460 Acc@5 99.190 loss 0.576
[19:41:09.686433] Accuracy of the network on the 10000 test images: 81.5%
[19:41:09.686632] Max accuracy: 81.56%
[19:41:09.840022] log_dir: ./output_dir
[19:41:10.733572] Epoch: [58]  [  0/781]  eta: 0:11:36  lr: 0.000103  training_loss: 1.2660 (1.2660)  classification_loss: 1.2658 (1.2658)  loss_mask: 0.0002 (0.0002)  time: 0.8912  data: 0.7019  max mem: 4132

[19:41:14.046832] Epoch: [58]  [ 20/781]  eta: 0:02:32  lr: 0.000103  training_loss: 1.2677 (1.2868)  classification_loss: 1.2674 (1.2865)  loss_mask: 0.0003 (0.0003)  time: 0.1655  data: 0.0002  max mem: 4132
[19:41:17.339952] Epoch: [58]  [ 40/781]  eta: 0:02:15  lr: 0.000103  training_loss: 1.3342 (1.3038)  classification_loss: 1.3335 (1.3035)  loss_mask: 0.0002 (0.0003)  time: 0.1645  data: 0.0003  max mem: 4132
[19:41:20.671442] Epoch: [58]  [ 60/781]  eta: 0:02:07  lr: 0.000103  training_loss: 1.3655 (1.3244)  classification_loss: 1.3653 (1.3241)  loss_mask: 0.0002 (0.0003)  time: 0.1665  data: 0.0002  max mem: 4132
[19:41:23.983341] Epoch: [58]  [ 80/781]  eta: 0:02:02  lr: 0.000103  training_loss: 1.3191 (1.3196)  classification_loss: 1.3189 (1.3194)  loss_mask: 0.0002 (0.0003)  time: 0.1655  data: 0.0003  max mem: 4132
[19:41:27.265923] Epoch: [58]  [100/781]  eta: 0:01:57  lr: 0.000102  training_loss: 1.2990 (1.3200)  classification_loss: 1.2987 (1.3198)  loss_mask: 0.0002 (0.0003)  time: 0.1640  data: 0.0003  max mem: 4132
[19:41:30.557961] Epoch: [58]  [120/781]  eta: 0:01:53  lr: 0.000102  training_loss: 1.2924 (1.3183)  classification_loss: 1.2922 (1.3181)  loss_mask: 0.0001 (0.0002)  time: 0.1645  data: 0.0003  max mem: 4132
[19:41:33.855578] Epoch: [58]  [140/781]  eta: 0:01:49  lr: 0.000102  training_loss: 1.2846 (1.3161)  classification_loss: 1.2844 (1.3159)  loss_mask: 0.0002 (0.0002)  time: 0.1648  data: 0.0003  max mem: 4132
[19:41:37.132419] Epoch: [58]  [160/781]  eta: 0:01:45  lr: 0.000102  training_loss: 1.2629 (1.3127)  classification_loss: 1.2626 (1.3124)  loss_mask: 0.0003 (0.0003)  time: 0.1637  data: 0.0002  max mem: 4132
[19:41:40.395358] Epoch: [58]  [180/781]  eta: 0:01:41  lr: 0.000102  training_loss: 1.2776 (1.3122)  classification_loss: 1.2774 (1.3119)  loss_mask: 0.0001 (0.0003)  time: 0.1630  data: 0.0003  max mem: 4132
[19:41:43.673461] Epoch: [58]  [200/781]  eta: 0:01:37  lr: 0.000102  training_loss: 1.3182 (1.3143)  classification_loss: 1.3181 (1.3141)  loss_mask: 0.0002 (0.0003)  time: 0.1638  data: 0.0002  max mem: 4132
[19:41:46.962068] Epoch: [58]  [220/781]  eta: 0:01:34  lr: 0.000102  training_loss: 1.2752 (1.3146)  classification_loss: 1.2751 (1.3143)  loss_mask: 0.0002 (0.0003)  time: 0.1643  data: 0.0002  max mem: 4132
[19:41:50.316686] Epoch: [58]  [240/781]  eta: 0:01:30  lr: 0.000102  training_loss: 1.2870 (1.3138)  classification_loss: 1.2867 (1.3136)  loss_mask: 0.0002 (0.0002)  time: 0.1676  data: 0.0003  max mem: 4132
[19:41:53.586523] Epoch: [58]  [260/781]  eta: 0:01:27  lr: 0.000102  training_loss: 1.3075 (1.3152)  classification_loss: 1.3073 (1.3150)  loss_mask: 0.0002 (0.0002)  time: 0.1633  data: 0.0002  max mem: 4132
[19:41:56.852780] Epoch: [58]  [280/781]  eta: 0:01:23  lr: 0.000102  training_loss: 1.3082 (1.3136)  classification_loss: 1.3081 (1.3133)  loss_mask: 0.0002 (0.0002)  time: 0.1632  data: 0.0003  max mem: 4132
[19:42:00.123111] Epoch: [58]  [300/781]  eta: 0:01:20  lr: 0.000101  training_loss: 1.2701 (1.3120)  classification_loss: 1.2699 (1.3118)  loss_mask: 0.0002 (0.0002)  time: 0.1634  data: 0.0002  max mem: 4132
[19:42:03.403315] Epoch: [58]  [320/781]  eta: 0:01:16  lr: 0.000101  training_loss: 1.3065 (1.3120)  classification_loss: 1.3063 (1.3117)  loss_mask: 0.0002 (0.0002)  time: 0.1639  data: 0.0002  max mem: 4132
[19:42:06.679301] Epoch: [58]  [340/781]  eta: 0:01:13  lr: 0.000101  training_loss: 1.2816 (1.3116)  classification_loss: 1.2815 (1.3114)  loss_mask: 0.0001 (0.0002)  time: 0.1637  data: 0.0003  max mem: 4132
[19:42:09.949278] Epoch: [58]  [360/781]  eta: 0:01:10  lr: 0.000101  training_loss: 1.3301 (1.3131)  classification_loss: 1.3298 (1.3129)  loss_mask: 0.0002 (0.0002)  time: 0.1634  data: 0.0003  max mem: 4132
[19:42:13.257026] Epoch: [58]  [380/781]  eta: 0:01:06  lr: 0.000101  training_loss: 1.2779 (1.3128)  classification_loss: 1.2777 (1.3126)  loss_mask: 0.0002 (0.0002)  time: 0.1653  data: 0.0004  max mem: 4132
[19:42:16.554076] Epoch: [58]  [400/781]  eta: 0:01:03  lr: 0.000101  training_loss: 1.3073 (1.3124)  classification_loss: 1.3070 (1.3122)  loss_mask: 0.0002 (0.0002)  time: 0.1648  data: 0.0003  max mem: 4132
[19:42:19.833071] Epoch: [58]  [420/781]  eta: 0:00:59  lr: 0.000101  training_loss: 1.2694 (1.3113)  classification_loss: 1.2692 (1.3111)  loss_mask: 0.0002 (0.0002)  time: 0.1639  data: 0.0003  max mem: 4132
[19:42:23.129220] Epoch: [58]  [440/781]  eta: 0:00:56  lr: 0.000101  training_loss: 1.3334 (1.3119)  classification_loss: 1.3333 (1.3116)  loss_mask: 0.0002 (0.0002)  time: 0.1647  data: 0.0004  max mem: 4132
[19:42:26.435159] Epoch: [58]  [460/781]  eta: 0:00:53  lr: 0.000101  training_loss: 1.3526 (1.3137)  classification_loss: 1.3524 (1.3134)  loss_mask: 0.0002 (0.0002)  time: 0.1652  data: 0.0003  max mem: 4132
[19:42:29.733984] Epoch: [58]  [480/781]  eta: 0:00:49  lr: 0.000100  training_loss: 1.3117 (1.3138)  classification_loss: 1.3116 (1.3135)  loss_mask: 0.0002 (0.0002)  time: 0.1648  data: 0.0002  max mem: 4132
[19:42:33.013551] Epoch: [58]  [500/781]  eta: 0:00:46  lr: 0.000100  training_loss: 1.3080 (1.3134)  classification_loss: 1.3077 (1.3132)  loss_mask: 0.0002 (0.0002)  time: 0.1639  data: 0.0004  max mem: 4132
[19:42:36.305396] Epoch: [58]  [520/781]  eta: 0:00:43  lr: 0.000100  training_loss: 1.3389 (1.3142)  classification_loss: 1.3387 (1.3140)  loss_mask: 0.0001 (0.0002)  time: 0.1645  data: 0.0004  max mem: 4132
[19:42:39.594505] Epoch: [58]  [540/781]  eta: 0:00:39  lr: 0.000100  training_loss: 1.3213 (1.3147)  classification_loss: 1.3212 (1.3145)  loss_mask: 0.0001 (0.0002)  time: 0.1644  data: 0.0003  max mem: 4132
[19:42:42.871849] Epoch: [58]  [560/781]  eta: 0:00:36  lr: 0.000100  training_loss: 1.3035 (1.3138)  classification_loss: 1.3033 (1.3135)  loss_mask: 0.0001 (0.0002)  time: 0.1638  data: 0.0003  max mem: 4132
[19:42:46.145874] Epoch: [58]  [580/781]  eta: 0:00:33  lr: 0.000100  training_loss: 1.3150 (1.3132)  classification_loss: 1.3149 (1.3130)  loss_mask: 0.0001 (0.0002)  time: 0.1636  data: 0.0003  max mem: 4132
[19:42:49.448030] Epoch: [58]  [600/781]  eta: 0:00:29  lr: 0.000100  training_loss: 1.2392 (1.3120)  classification_loss: 1.2391 (1.3118)  loss_mask: 0.0001 (0.0002)  time: 0.1650  data: 0.0004  max mem: 4132
[19:42:52.735474] Epoch: [58]  [620/781]  eta: 0:00:26  lr: 0.000100  training_loss: 1.3876 (1.3168)  classification_loss: 1.2653 (1.3100)  loss_mask: 0.1406 (0.0068)  time: 0.1643  data: 0.0003  max mem: 4132
[19:42:56.028603] Epoch: [58]  [640/781]  eta: 0:00:23  lr: 0.000100  training_loss: 1.3973 (1.3200)  classification_loss: 1.2774 (1.3099)  loss_mask: 0.1179 (0.0101)  time: 0.1646  data: 0.0002  max mem: 4132
[19:42:59.327054] Epoch: [58]  [660/781]  eta: 0:00:20  lr: 0.000100  training_loss: 1.3961 (1.3225)  classification_loss: 1.3360 (1.3107)  loss_mask: 0.0563 (0.0118)  time: 0.1648  data: 0.0003  max mem: 4132
[19:43:02.636350] Epoch: [58]  [680/781]  eta: 0:00:16  lr: 0.000099  training_loss: 1.3426 (1.3236)  classification_loss: 1.3143 (1.3113)  loss_mask: 0.0297 (0.0123)  time: 0.1654  data: 0.0003  max mem: 4132
[19:43:05.915250] Epoch: [58]  [700/781]  eta: 0:00:13  lr: 0.000099  training_loss: 1.3697 (1.3251)  classification_loss: 1.3508 (1.3125)  loss_mask: 0.0219 (0.0126)  time: 0.1638  data: 0.0003  max mem: 4132
[19:43:09.148935] Epoch: [58]  [720/781]  eta: 0:00:10  lr: 0.000099  training_loss: 1.3679 (1.3264)  classification_loss: 1.3508 (1.3138)  loss_mask: 0.0120 (0.0126)  time: 0.1616  data: 0.0002  max mem: 4132
[19:43:12.411869] Epoch: [58]  [740/781]  eta: 0:00:06  lr: 0.000099  training_loss: 1.2853 (1.3253)  classification_loss: 1.2762 (1.3129)  loss_mask: 0.0056 (0.0125)  time: 0.1631  data: 0.0004  max mem: 4132
[19:43:15.661288] Epoch: [58]  [760/781]  eta: 0:00:03  lr: 0.000099  training_loss: 1.2965 (1.3254)  classification_loss: 1.2936 (1.3132)  loss_mask: 0.0034 (0.0122)  time: 0.1624  data: 0.0003  max mem: 4132
[19:43:18.960085] Epoch: [58]  [780/781]  eta: 0:00:00  lr: 0.000099  training_loss: 1.3763 (1.3265)  classification_loss: 1.3156 (1.3139)  loss_mask: 0.0105 (0.0126)  time: 0.1649  data: 0.0002  max mem: 4132
[19:43:19.158038] Epoch: [58] Total time: 0:02:09 (0.1656 s / it)
[19:43:19.159074] Averaged stats: lr: 0.000099  training_loss: 1.3763 (1.3265)  classification_loss: 1.3156 (1.3139)  loss_mask: 0.0105 (0.0126)
[19:43:19.887282] Test:  [  0/157]  eta: 0:01:53  testing_loss: 0.6112 (0.6112)  acc1: 84.3750 (84.3750)  acc5: 98.4375 (98.4375)  time: 0.7203  data: 0.6894  max mem: 4132
[19:43:20.185748] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.6112 (0.6316)  acc1: 81.2500 (80.2557)  acc5: 100.0000 (99.4318)  time: 0.0924  data: 0.0629  max mem: 4132
[19:43:20.483908] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5654 (0.5936)  acc1: 81.2500 (81.6964)  acc5: 100.0000 (99.5536)  time: 0.0296  data: 0.0004  max mem: 4132
[19:43:20.778514] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5813 (0.6099)  acc1: 81.2500 (80.9980)  acc5: 100.0000 (99.2440)  time: 0.0294  data: 0.0004  max mem: 4132
[19:43:21.071340] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5996 (0.6110)  acc1: 81.2500 (81.0976)  acc5: 98.4375 (99.1235)  time: 0.0291  data: 0.0004  max mem: 4132
[19:43:21.361040] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6050 (0.6064)  acc1: 81.2500 (81.3725)  acc5: 100.0000 (99.1422)  time: 0.0289  data: 0.0004  max mem: 4132
[19:43:21.653999] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6016 (0.6062)  acc1: 81.2500 (81.2756)  acc5: 100.0000 (99.1547)  time: 0.0289  data: 0.0003  max mem: 4132
[19:43:21.952584] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5825 (0.6009)  acc1: 81.2500 (81.4261)  acc5: 100.0000 (99.1417)  time: 0.0294  data: 0.0003  max mem: 4132
[19:43:22.252716] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5825 (0.6105)  acc1: 81.2500 (81.0764)  acc5: 100.0000 (99.0355)  time: 0.0297  data: 0.0003  max mem: 4132
[19:43:22.550438] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6252 (0.6084)  acc1: 81.2500 (81.2672)  acc5: 98.4375 (99.0385)  time: 0.0297  data: 0.0002  max mem: 4132
[19:43:22.844958] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.6252 (0.6128)  acc1: 79.6875 (80.8478)  acc5: 98.4375 (99.0099)  time: 0.0294  data: 0.0002  max mem: 4132
[19:43:23.137485] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6254 (0.6137)  acc1: 78.1250 (80.9825)  acc5: 98.4375 (99.0428)  time: 0.0291  data: 0.0002  max mem: 4132
[19:43:23.427133] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6093 (0.6120)  acc1: 81.2500 (81.0305)  acc5: 100.0000 (99.0573)  time: 0.0289  data: 0.0002  max mem: 4132
[19:43:23.719023] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6079 (0.6105)  acc1: 81.2500 (81.1307)  acc5: 98.4375 (99.0339)  time: 0.0289  data: 0.0002  max mem: 4132
[19:43:24.016918] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5931 (0.6069)  acc1: 81.2500 (81.2611)  acc5: 98.4375 (99.0359)  time: 0.0291  data: 0.0003  max mem: 4132
[19:43:24.303693] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5907 (0.6049)  acc1: 81.2500 (81.3121)  acc5: 98.4375 (99.0066)  time: 0.0288  data: 0.0003  max mem: 4132
[19:43:24.460537] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5856 (0.6053)  acc1: 81.2500 (81.2800)  acc5: 98.4375 (99.0200)  time: 0.0277  data: 0.0002  max mem: 4132
[19:43:24.679371] Test: Total time: 0:00:05 (0.0351 s / it)
[19:43:24.679916] * Acc@1 81.280 Acc@5 99.020 loss 0.605
[19:43:24.680233] Accuracy of the network on the 10000 test images: 81.3%
[19:43:24.680476] Max accuracy: 81.56%
[19:43:24.827596] log_dir: ./output_dir
[19:43:25.797548] Epoch: [59]  [  0/781]  eta: 0:12:35  lr: 0.000099  training_loss: 1.3661 (1.3661)  classification_loss: 1.1083 (1.1083)  loss_mask: 0.2578 (0.2578)  time: 0.9674  data: 0.7443  max mem: 4132
[19:43:29.085956] Epoch: [59]  [ 20/781]  eta: 0:02:34  lr: 0.000099  training_loss: 1.3358 (1.3373)  classification_loss: 1.2960 (1.2766)  loss_mask: 0.0237 (0.0608)  time: 0.1643  data: 0.0003  max mem: 4132
[19:43:32.362686] Epoch: [59]  [ 40/781]  eta: 0:02:16  lr: 0.000099  training_loss: 1.3240 (1.3285)  classification_loss: 1.2826 (1.2880)  loss_mask: 0.0120 (0.0405)  time: 0.1637  data: 0.0003  max mem: 4132
[19:43:35.673116] Epoch: [59]  [ 60/781]  eta: 0:02:08  lr: 0.000099  training_loss: 1.2745 (1.3226)  classification_loss: 1.2667 (1.2931)  loss_mask: 0.0070 (0.0295)  time: 0.1654  data: 0.0002  max mem: 4132
[19:43:38.996284] Epoch: [59]  [ 80/781]  eta: 0:02:02  lr: 0.000099  training_loss: 1.2595 (1.3148)  classification_loss: 1.2571 (1.2917)  loss_mask: 0.0025 (0.0231)  time: 0.1661  data: 0.0003  max mem: 4132
[19:43:42.293014] Epoch: [59]  [100/781]  eta: 0:01:57  lr: 0.000098  training_loss: 1.3199 (1.3134)  classification_loss: 1.3139 (1.2937)  loss_mask: 0.0022 (0.0197)  time: 0.1647  data: 0.0003  max mem: 4132
[19:43:45.614060] Epoch: [59]  [120/781]  eta: 0:01:53  lr: 0.000098  training_loss: 1.3138 (1.3120)  classification_loss: 1.2917 (1.2934)  loss_mask: 0.0022 (0.0186)  time: 0.1659  data: 0.0003  max mem: 4132
[19:43:48.938421] Epoch: [59]  [140/781]  eta: 0:01:49  lr: 0.000098  training_loss: 1.2621 (1.3094)  classification_loss: 1.2550 (1.2924)  loss_mask: 0.0035 (0.0169)  time: 0.1661  data: 0.0004  max mem: 4132
[19:43:52.263554] Epoch: [59]  [160/781]  eta: 0:01:45  lr: 0.000098  training_loss: 1.3054 (1.3074)  classification_loss: 1.2992 (1.2915)  loss_mask: 0.0053 (0.0159)  time: 0.1662  data: 0.0003  max mem: 4132
[19:43:55.553213] Epoch: [59]  [180/781]  eta: 0:01:41  lr: 0.000098  training_loss: 1.2937 (1.3084)  classification_loss: 1.2928 (1.2935)  loss_mask: 0.0022 (0.0149)  time: 0.1644  data: 0.0003  max mem: 4132
[19:43:58.826041] Epoch: [59]  [200/781]  eta: 0:01:38  lr: 0.000098  training_loss: 1.2639 (1.3058)  classification_loss: 1.2628 (1.2922)  loss_mask: 0.0012 (0.0136)  time: 0.1635  data: 0.0002  max mem: 4132
[19:44:02.165061] Epoch: [59]  [220/781]  eta: 0:01:34  lr: 0.000098  training_loss: 1.3483 (1.3092)  classification_loss: 1.3477 (1.2967)  loss_mask: 0.0010 (0.0125)  time: 0.1669  data: 0.0003  max mem: 4132
[19:44:05.467504] Epoch: [59]  [240/781]  eta: 0:01:31  lr: 0.000098  training_loss: 1.2722 (1.3084)  classification_loss: 1.2720 (1.2969)  loss_mask: 0.0007 (0.0115)  time: 0.1650  data: 0.0003  max mem: 4132
[19:44:08.780091] Epoch: [59]  [260/781]  eta: 0:01:27  lr: 0.000098  training_loss: 1.3042 (1.3077)  classification_loss: 1.3036 (1.2970)  loss_mask: 0.0005 (0.0107)  time: 0.1655  data: 0.0003  max mem: 4132
[19:44:12.109986] Epoch: [59]  [280/781]  eta: 0:01:24  lr: 0.000098  training_loss: 1.2855 (1.3072)  classification_loss: 1.2849 (1.2973)  loss_mask: 0.0005 (0.0100)  time: 0.1664  data: 0.0003  max mem: 4132
[19:44:15.426153] Epoch: [59]  [300/781]  eta: 0:01:20  lr: 0.000097  training_loss: 1.3319 (1.3096)  classification_loss: 1.3312 (1.3002)  loss_mask: 0.0004 (0.0093)  time: 0.1657  data: 0.0003  max mem: 4132
[19:44:18.693745] Epoch: [59]  [320/781]  eta: 0:01:17  lr: 0.000097  training_loss: 1.2751 (1.3088)  classification_loss: 1.2750 (1.3000)  loss_mask: 0.0004 (0.0088)  time: 0.1633  data: 0.0002  max mem: 4132
[19:44:21.967427] Epoch: [59]  [340/781]  eta: 0:01:13  lr: 0.000097  training_loss: 1.2669 (1.3060)  classification_loss: 1.2665 (1.2977)  loss_mask: 0.0004 (0.0083)  time: 0.1636  data: 0.0003  max mem: 4132
[19:44:25.237717] Epoch: [59]  [360/781]  eta: 0:01:10  lr: 0.000097  training_loss: 1.2945 (1.3054)  classification_loss: 1.2941 (1.2975)  loss_mask: 0.0003 (0.0079)  time: 0.1634  data: 0.0002  max mem: 4132
[19:44:28.532797] Epoch: [59]  [380/781]  eta: 0:01:07  lr: 0.000097  training_loss: 1.2993 (1.3056)  classification_loss: 1.2989 (1.2981)  loss_mask: 0.0004 (0.0075)  time: 0.1647  data: 0.0003  max mem: 4132
[19:44:31.811550] Epoch: [59]  [400/781]  eta: 0:01:03  lr: 0.000097  training_loss: 1.3099 (1.3066)  classification_loss: 1.3093 (1.2995)  loss_mask: 0.0004 (0.0071)  time: 0.1639  data: 0.0004  max mem: 4132
[19:44:35.084184] Epoch: [59]  [420/781]  eta: 0:01:00  lr: 0.000097  training_loss: 1.3157 (1.3061)  classification_loss: 1.3155 (1.2991)  loss_mask: 0.0008 (0.0070)  time: 0.1635  data: 0.0002  max mem: 4132
[19:44:38.378627] Epoch: [59]  [440/781]  eta: 0:00:56  lr: 0.000097  training_loss: 1.2750 (1.3041)  classification_loss: 1.2743 (1.2974)  loss_mask: 0.0006 (0.0068)  time: 0.1646  data: 0.0003  max mem: 4132
[19:44:41.659729] Epoch: [59]  [460/781]  eta: 0:00:53  lr: 0.000097  training_loss: 1.2577 (1.3021)  classification_loss: 1.2568 (1.2956)  loss_mask: 0.0006 (0.0065)  time: 0.1640  data: 0.0002  max mem: 4132
[19:44:44.958492] Epoch: [59]  [480/781]  eta: 0:00:50  lr: 0.000096  training_loss: 1.3233 (1.3021)  classification_loss: 1.3231 (1.2958)  loss_mask: 0.0005 (0.0063)  time: 0.1649  data: 0.0002  max mem: 4132
[19:44:48.246390] Epoch: [59]  [500/781]  eta: 0:00:46  lr: 0.000096  training_loss: 1.2939 (1.3033)  classification_loss: 1.2933 (1.2972)  loss_mask: 0.0003 (0.0061)  time: 0.1642  data: 0.0003  max mem: 4132
[19:44:51.519404] Epoch: [59]  [520/781]  eta: 0:00:43  lr: 0.000096  training_loss: 1.3250 (1.3043)  classification_loss: 1.3247 (1.2984)  loss_mask: 0.0003 (0.0058)  time: 0.1636  data: 0.0003  max mem: 4132
[19:44:54.815469] Epoch: [59]  [540/781]  eta: 0:00:40  lr: 0.000096  training_loss: 1.3138 (1.3053)  classification_loss: 1.3136 (1.2997)  loss_mask: 0.0003 (0.0056)  time: 0.1647  data: 0.0003  max mem: 4132
[19:44:58.125738] Epoch: [59]  [560/781]  eta: 0:00:36  lr: 0.000096  training_loss: 1.2587 (1.3045)  classification_loss: 1.2584 (1.2991)  loss_mask: 0.0003 (0.0055)  time: 0.1654  data: 0.0002  max mem: 4132
[19:45:01.459272] Epoch: [59]  [580/781]  eta: 0:00:33  lr: 0.000096  training_loss: 1.2612 (1.3035)  classification_loss: 1.2609 (1.2982)  loss_mask: 0.0002 (0.0053)  time: 0.1666  data: 0.0003  max mem: 4132
[19:45:04.746100] Epoch: [59]  [600/781]  eta: 0:00:30  lr: 0.000096  training_loss: 1.3257 (1.3036)  classification_loss: 1.3255 (1.2985)  loss_mask: 0.0002 (0.0051)  time: 0.1642  data: 0.0003  max mem: 4132
[19:45:08.037345] Epoch: [59]  [620/781]  eta: 0:00:26  lr: 0.000096  training_loss: 1.2957 (1.3032)  classification_loss: 1.2954 (1.2982)  loss_mask: 0.0002 (0.0049)  time: 0.1645  data: 0.0003  max mem: 4132
[19:45:11.319052] Epoch: [59]  [640/781]  eta: 0:00:23  lr: 0.000096  training_loss: 1.3195 (1.3040)  classification_loss: 1.3193 (1.2992)  loss_mask: 0.0002 (0.0048)  time: 0.1640  data: 0.0003  max mem: 4132
[19:45:14.597821] Epoch: [59]  [660/781]  eta: 0:00:20  lr: 0.000096  training_loss: 1.3018 (1.3043)  classification_loss: 1.3015 (1.2996)  loss_mask: 0.0002 (0.0047)  time: 0.1639  data: 0.0003  max mem: 4132
[19:45:17.879622] Epoch: [59]  [680/781]  eta: 0:00:16  lr: 0.000095  training_loss: 1.2970 (1.3038)  classification_loss: 1.2968 (1.2993)  loss_mask: 0.0002 (0.0045)  time: 0.1640  data: 0.0003  max mem: 4132
[19:45:21.183253] Epoch: [59]  [700/781]  eta: 0:00:13  lr: 0.000095  training_loss: 1.2877 (1.3041)  classification_loss: 1.2877 (1.2997)  loss_mask: 0.0002 (0.0044)  time: 0.1651  data: 0.0002  max mem: 4132
[19:45:24.475645] Epoch: [59]  [720/781]  eta: 0:00:10  lr: 0.000095  training_loss: 1.3215 (1.3044)  classification_loss: 1.3213 (1.3001)  loss_mask: 0.0002 (0.0043)  time: 0.1645  data: 0.0003  max mem: 4132
[19:45:27.793188] Epoch: [59]  [740/781]  eta: 0:00:06  lr: 0.000095  training_loss: 1.2528 (1.3032)  classification_loss: 1.2526 (1.2990)  loss_mask: 0.0002 (0.0042)  time: 0.1658  data: 0.0003  max mem: 4132
[19:45:31.089449] Epoch: [59]  [760/781]  eta: 0:00:03  lr: 0.000095  training_loss: 1.2685 (1.3036)  classification_loss: 1.2683 (1.2996)  loss_mask: 0.0002 (0.0041)  time: 0.1647  data: 0.0002  max mem: 4132
[19:45:34.362768] Epoch: [59]  [780/781]  eta: 0:00:00  lr: 0.000095  training_loss: 1.2518 (1.3029)  classification_loss: 1.2514 (1.2989)  loss_mask: 0.0002 (0.0040)  time: 0.1636  data: 0.0002  max mem: 4132
[19:45:34.546597] Epoch: [59] Total time: 0:02:09 (0.1661 s / it)
[19:45:34.548595] Averaged stats: lr: 0.000095  training_loss: 1.2518 (1.3029)  classification_loss: 1.2514 (1.2989)  loss_mask: 0.0002 (0.0040)
[19:45:35.264918] Test:  [  0/157]  eta: 0:01:51  testing_loss: 0.5442 (0.5442)  acc1: 85.9375 (85.9375)  acc5: 96.8750 (96.8750)  time: 0.7099  data: 0.6779  max mem: 4132
[19:45:35.552474] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.5641 (0.5972)  acc1: 79.6875 (79.6875)  acc5: 100.0000 (99.4318)  time: 0.0904  data: 0.0619  max mem: 4132
[19:45:35.837155] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5641 (0.5802)  acc1: 81.2500 (80.6548)  acc5: 100.0000 (99.4048)  time: 0.0284  data: 0.0002  max mem: 4132
[19:45:36.122250] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5726 (0.5903)  acc1: 81.2500 (80.4940)  acc5: 98.4375 (99.1935)  time: 0.0283  data: 0.0002  max mem: 4132
[19:45:36.408633] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5977 (0.5914)  acc1: 81.2500 (80.6402)  acc5: 98.4375 (99.1616)  time: 0.0284  data: 0.0002  max mem: 4132
[19:45:36.696975] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5928 (0.5820)  acc1: 82.8125 (81.5257)  acc5: 100.0000 (99.2034)  time: 0.0286  data: 0.0003  max mem: 4132
[19:45:36.983632] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5624 (0.5795)  acc1: 82.8125 (81.2756)  acc5: 100.0000 (99.2059)  time: 0.0286  data: 0.0003  max mem: 4132
[19:45:37.271450] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5624 (0.5757)  acc1: 82.8125 (81.5801)  acc5: 100.0000 (99.1857)  time: 0.0285  data: 0.0002  max mem: 4132
[19:45:37.557276] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5700 (0.5842)  acc1: 79.6875 (81.2307)  acc5: 100.0000 (99.1127)  time: 0.0285  data: 0.0002  max mem: 4132
[19:45:37.842099] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5722 (0.5815)  acc1: 79.6875 (81.5419)  acc5: 98.4375 (99.0900)  time: 0.0284  data: 0.0002  max mem: 4132
[19:45:38.127097] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.6156 (0.5865)  acc1: 81.2500 (81.2036)  acc5: 98.4375 (99.0563)  time: 0.0283  data: 0.0002  max mem: 4132
[19:45:38.413503] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5951 (0.5846)  acc1: 78.1250 (81.3767)  acc5: 98.4375 (99.0850)  time: 0.0284  data: 0.0002  max mem: 4132
[19:45:38.699738] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5502 (0.5807)  acc1: 84.3750 (81.6374)  acc5: 100.0000 (99.0961)  time: 0.0285  data: 0.0003  max mem: 4132
[19:45:38.984503] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5502 (0.5808)  acc1: 82.8125 (81.7032)  acc5: 100.0000 (99.1293)  time: 0.0284  data: 0.0002  max mem: 4132
[19:45:39.269234] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5763 (0.5792)  acc1: 82.8125 (81.7930)  acc5: 100.0000 (99.1467)  time: 0.0283  data: 0.0002  max mem: 4132
[19:45:39.553934] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5585 (0.5780)  acc1: 82.8125 (81.8502)  acc5: 100.0000 (99.1618)  time: 0.0283  data: 0.0002  max mem: 4132
[19:45:39.708738] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5323 (0.5777)  acc1: 81.2500 (81.8700)  acc5: 100.0000 (99.1700)  time: 0.0274  data: 0.0002  max mem: 4132
[19:45:39.884187] Test: Total time: 0:00:05 (0.0340 s / it)
[19:45:39.884789] * Acc@1 81.870 Acc@5 99.170 loss 0.578
[19:45:39.885118] Accuracy of the network on the 10000 test images: 81.9%
[19:45:39.885388] Max accuracy: 81.87%
[19:45:40.027723] log_dir: ./output_dir
[19:45:40.910703] Epoch: [60]  [  0/781]  eta: 0:11:27  lr: 0.000095  training_loss: 1.1630 (1.1630)  classification_loss: 1.1627 (1.1627)  loss_mask: 0.0004 (0.0004)  time: 0.8809  data: 0.7000  max mem: 4132
[19:45:44.190415] Epoch: [60]  [ 20/781]  eta: 0:02:30  lr: 0.000095  training_loss: 1.2461 (1.2780)  classification_loss: 1.2459 (1.2778)  loss_mask: 0.0002 (0.0002)  time: 0.1639  data: 0.0002  max mem: 4132
[19:45:47.482016] Epoch: [60]  [ 40/781]  eta: 0:02:14  lr: 0.000095  training_loss: 1.2617 (1.2863)  classification_loss: 1.2615 (1.2861)  loss_mask: 0.0002 (0.0002)  time: 0.1645  data: 0.0002  max mem: 4132
[19:45:50.776974] Epoch: [60]  [ 60/781]  eta: 0:02:06  lr: 0.000095  training_loss: 1.3268 (1.3046)  classification_loss: 1.3264 (1.3044)  loss_mask: 0.0002 (0.0002)  time: 0.1647  data: 0.0003  max mem: 4132
[19:45:54.087507] Epoch: [60]  [ 80/781]  eta: 0:02:01  lr: 0.000095  training_loss: 1.2441 (1.2944)  classification_loss: 1.2441 (1.2942)  loss_mask: 0.0002 (0.0002)  time: 0.1654  data: 0.0003  max mem: 4132
[19:45:57.380664] Epoch: [60]  [100/781]  eta: 0:01:56  lr: 0.000094  training_loss: 1.3056 (1.2961)  classification_loss: 1.3054 (1.2959)  loss_mask: 0.0002 (0.0002)  time: 0.1646  data: 0.0003  max mem: 4132
[19:46:00.685569] Epoch: [60]  [120/781]  eta: 0:01:52  lr: 0.000094  training_loss: 1.2886 (1.2947)  classification_loss: 1.2884 (1.2945)  loss_mask: 0.0002 (0.0002)  time: 0.1652  data: 0.0003  max mem: 4132
[19:46:03.998388] Epoch: [60]  [140/781]  eta: 0:01:48  lr: 0.000094  training_loss: 1.2795 (1.2955)  classification_loss: 1.2794 (1.2953)  loss_mask: 0.0002 (0.0002)  time: 0.1655  data: 0.0003  max mem: 4132
[19:46:07.298206] Epoch: [60]  [160/781]  eta: 0:01:45  lr: 0.000094  training_loss: 1.2601 (1.2909)  classification_loss: 1.2599 (1.2906)  loss_mask: 0.0002 (0.0002)  time: 0.1649  data: 0.0003  max mem: 4132
[19:46:10.588320] Epoch: [60]  [180/781]  eta: 0:01:41  lr: 0.000094  training_loss: 1.2797 (1.2923)  classification_loss: 1.2789 (1.2920)  loss_mask: 0.0004 (0.0003)  time: 0.1644  data: 0.0003  max mem: 4132
[19:46:13.883477] Epoch: [60]  [200/781]  eta: 0:01:37  lr: 0.000094  training_loss: 1.3023 (1.2938)  classification_loss: 1.3020 (1.2935)  loss_mask: 0.0004 (0.0004)  time: 0.1646  data: 0.0003  max mem: 4132
[19:46:17.156873] Epoch: [60]  [220/781]  eta: 0:01:34  lr: 0.000094  training_loss: 1.2801 (1.2942)  classification_loss: 1.2797 (1.2939)  loss_mask: 0.0002 (0.0004)  time: 0.1636  data: 0.0003  max mem: 4132
[19:46:20.448933] Epoch: [60]  [240/781]  eta: 0:01:30  lr: 0.000094  training_loss: 1.3496 (1.2984)  classification_loss: 1.3495 (1.2980)  loss_mask: 0.0001 (0.0003)  time: 0.1645  data: 0.0002  max mem: 4132
[19:46:52.104654] Epoch: [60]  [260/781]  eta: 0:02:23  lr: 0.000094  training_loss: 1.3396 (1.3004)  classification_loss: 1.3395 (1.3000)  loss_mask: 0.0001 (0.0003)  time: 1.5827  data: 1.4160  max mem: 4132
[19:46:55.354740] Epoch: [60]  [280/781]  eta: 0:02:14  lr: 0.000094  training_loss: 1.2655 (1.2986)  classification_loss: 1.2654 (1.2983)  loss_mask: 0.0002 (0.0003)  time: 0.1624  data: 0.0002  max mem: 4132
[19:46:58.607871] Epoch: [60]  [300/781]  eta: 0:02:05  lr: 0.000093  training_loss: 1.3213 (1.3002)  classification_loss: 1.3211 (1.2999)  loss_mask: 0.0001 (0.0003)  time: 0.1626  data: 0.0002  max mem: 4132
[19:47:01.848450] Epoch: [60]  [320/781]  eta: 0:01:57  lr: 0.000093  training_loss: 1.3002 (1.3017)  classification_loss: 1.2734 (1.2993)  loss_mask: 0.0020 (0.0024)  time: 0.1619  data: 0.0001  max mem: 4132
[19:47:05.115958] Epoch: [60]  [340/781]  eta: 0:01:49  lr: 0.000093  training_loss: 1.4260 (1.3152)  classification_loss: 1.2492 (1.2974)  loss_mask: 0.1608 (0.0177)  time: 0.1633  data: 0.0002  max mem: 4132
[19:47:08.379326] Epoch: [60]  [360/781]  eta: 0:01:42  lr: 0.000093  training_loss: 1.3975 (1.3187)  classification_loss: 1.2725 (1.2970)  loss_mask: 0.0782 (0.0216)  time: 0.1630  data: 0.0003  max mem: 4132
[19:47:11.649787] Epoch: [60]  [380/781]  eta: 0:01:36  lr: 0.000093  training_loss: 1.3632 (1.3226)  classification_loss: 1.3005 (1.2986)  loss_mask: 0.0592 (0.0240)  time: 0.1634  data: 0.0002  max mem: 4132
[19:47:14.939224] Epoch: [60]  [400/781]  eta: 0:01:30  lr: 0.000093  training_loss: 1.3285 (1.3230)  classification_loss: 1.3064 (1.2984)  loss_mask: 0.0356 (0.0246)  time: 0.1644  data: 0.0003  max mem: 4132
[19:47:18.219045] Epoch: [60]  [420/781]  eta: 0:01:24  lr: 0.000093  training_loss: 1.3300 (1.3242)  classification_loss: 1.2981 (1.2991)  loss_mask: 0.0292 (0.0251)  time: 0.1639  data: 0.0003  max mem: 4132
[19:47:21.505640] Epoch: [60]  [440/781]  eta: 0:01:18  lr: 0.000093  training_loss: 1.2918 (1.3244)  classification_loss: 1.2686 (1.2996)  loss_mask: 0.0171 (0.0248)  time: 0.1642  data: 0.0003  max mem: 4132
[19:47:24.806436] Epoch: [60]  [460/781]  eta: 0:01:12  lr: 0.000093  training_loss: 1.2918 (1.3228)  classification_loss: 1.2744 (1.2986)  loss_mask: 0.0091 (0.0241)  time: 0.1650  data: 0.0002  max mem: 4132
[19:47:28.107991] Epoch: [60]  [480/781]  eta: 0:01:07  lr: 0.000092  training_loss: 1.3405 (1.3234)  classification_loss: 1.3272 (1.3000)  loss_mask: 0.0064 (0.0235)  time: 0.1650  data: 0.0003  max mem: 4132
[19:47:31.389104] Epoch: [60]  [500/781]  eta: 0:01:02  lr: 0.000092  training_loss: 1.3031 (1.3232)  classification_loss: 1.2896 (1.2997)  loss_mask: 0.0136 (0.0236)  time: 0.1639  data: 0.0003  max mem: 4132
[19:47:34.705650] Epoch: [60]  [520/781]  eta: 0:00:57  lr: 0.000092  training_loss: 1.2876 (1.3212)  classification_loss: 1.2784 (1.2982)  loss_mask: 0.0075 (0.0230)  time: 0.1657  data: 0.0003  max mem: 4132
[19:47:37.987462] Epoch: [60]  [540/781]  eta: 0:00:52  lr: 0.000092  training_loss: 1.3016 (1.3207)  classification_loss: 1.2941 (1.2984)  loss_mask: 0.0028 (0.0223)  time: 0.1640  data: 0.0003  max mem: 4132
[19:47:41.275245] Epoch: [60]  [560/781]  eta: 0:00:47  lr: 0.000092  training_loss: 1.2680 (1.3187)  classification_loss: 1.2592 (1.2971)  loss_mask: 0.0023 (0.0217)  time: 0.1642  data: 0.0003  max mem: 4132
[19:47:44.574118] Epoch: [60]  [580/781]  eta: 0:00:43  lr: 0.000092  training_loss: 1.3687 (1.3204)  classification_loss: 1.3003 (1.2972)  loss_mask: 0.0513 (0.0232)  time: 0.1648  data: 0.0003  max mem: 4132
[19:47:47.848664] Epoch: [60]  [600/781]  eta: 0:00:38  lr: 0.000092  training_loss: 1.3771 (1.3227)  classification_loss: 1.2915 (1.2972)  loss_mask: 0.0553 (0.0255)  time: 0.1636  data: 0.0003  max mem: 4132
[19:47:51.131076] Epoch: [60]  [620/781]  eta: 0:00:33  lr: 0.000092  training_loss: 1.3569 (1.3241)  classification_loss: 1.3066 (1.2979)  loss_mask: 0.0407 (0.0262)  time: 0.1640  data: 0.0003  max mem: 4132
[19:47:54.424887] Epoch: [60]  [640/781]  eta: 0:00:29  lr: 0.000092  training_loss: 1.3100 (1.3238)  classification_loss: 1.2754 (1.2976)  loss_mask: 0.0182 (0.0262)  time: 0.1646  data: 0.0003  max mem: 4132
[19:47:57.683856] Epoch: [60]  [660/781]  eta: 0:00:25  lr: 0.000092  training_loss: 1.2803 (1.3234)  classification_loss: 1.2713 (1.2976)  loss_mask: 0.0089 (0.0257)  time: 0.1629  data: 0.0002  max mem: 4132
[19:48:00.965155] Epoch: [60]  [680/781]  eta: 0:00:20  lr: 0.000091  training_loss: 1.3136 (1.3232)  classification_loss: 1.3087 (1.2980)  loss_mask: 0.0061 (0.0251)  time: 0.1640  data: 0.0003  max mem: 4132
[19:48:04.255522] Epoch: [60]  [700/781]  eta: 0:00:16  lr: 0.000091  training_loss: 1.2923 (1.3223)  classification_loss: 1.2894 (1.2978)  loss_mask: 0.0030 (0.0245)  time: 0.1644  data: 0.0002  max mem: 4132
[19:48:07.521846] Epoch: [60]  [720/781]  eta: 0:00:12  lr: 0.000091  training_loss: 1.3451 (1.3226)  classification_loss: 1.3414 (1.2987)  loss_mask: 0.0020 (0.0239)  time: 0.1632  data: 0.0002  max mem: 4132
[19:48:10.827359] Epoch: [60]  [740/781]  eta: 0:00:08  lr: 0.000091  training_loss: 1.2716 (1.3219)  classification_loss: 1.2697 (1.2986)  loss_mask: 0.0015 (0.0233)  time: 0.1652  data: 0.0003  max mem: 4132
[19:48:14.116137] Epoch: [60]  [760/781]  eta: 0:00:04  lr: 0.000091  training_loss: 1.3135 (1.3223)  classification_loss: 1.3118 (1.2996)  loss_mask: 0.0017 (0.0227)  time: 0.1643  data: 0.0002  max mem: 4132
[19:48:17.366550] Epoch: [60]  [780/781]  eta: 0:00:00  lr: 0.000091  training_loss: 1.2908 (1.3218)  classification_loss: 1.2896 (1.2996)  loss_mask: 0.0011 (0.0222)  time: 0.1624  data: 0.0002  max mem: 4132
[19:48:17.544922] Epoch: [60] Total time: 0:02:37 (0.2017 s / it)
[19:48:17.545449] Averaged stats: lr: 0.000091  training_loss: 1.2908 (1.3218)  classification_loss: 1.2896 (1.2996)  loss_mask: 0.0011 (0.0222)
[19:48:19.397130] Test:  [  0/157]  eta: 0:01:52  testing_loss: 0.4823 (0.4823)  acc1: 84.3750 (84.3750)  acc5: 98.4375 (98.4375)  time: 0.7184  data: 0.6801  max mem: 4132
[19:48:19.698176] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.5804 (0.5889)  acc1: 81.2500 (81.3920)  acc5: 100.0000 (99.2898)  time: 0.0924  data: 0.0620  max mem: 4132
[19:48:19.987813] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5372 (0.5573)  acc1: 81.2500 (82.5149)  acc5: 100.0000 (99.1815)  time: 0.0293  data: 0.0003  max mem: 4132
[19:48:20.278561] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5606 (0.5773)  acc1: 82.8125 (81.8548)  acc5: 100.0000 (99.0423)  time: 0.0289  data: 0.0003  max mem: 4132
[19:48:20.567151] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5821 (0.5827)  acc1: 79.6875 (81.5168)  acc5: 98.4375 (98.9329)  time: 0.0288  data: 0.0003  max mem: 4132
[19:48:20.858843] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5778 (0.5789)  acc1: 81.2500 (82.0772)  acc5: 98.4375 (99.0502)  time: 0.0289  data: 0.0002  max mem: 4132
[19:48:21.148237] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5718 (0.5786)  acc1: 82.8125 (81.7623)  acc5: 100.0000 (99.0779)  time: 0.0289  data: 0.0003  max mem: 4132
[19:48:21.434425] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5465 (0.5733)  acc1: 81.2500 (81.8882)  acc5: 100.0000 (99.1197)  time: 0.0286  data: 0.0003  max mem: 4132
[19:48:21.723990] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5528 (0.5834)  acc1: 81.2500 (81.6937)  acc5: 98.4375 (99.0355)  time: 0.0286  data: 0.0002  max mem: 4132
[19:48:22.015622] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5824 (0.5811)  acc1: 82.8125 (81.8338)  acc5: 98.4375 (99.0041)  time: 0.0289  data: 0.0002  max mem: 4132
[19:48:22.302929] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.5930 (0.5872)  acc1: 79.6875 (81.5130)  acc5: 98.4375 (98.9635)  time: 0.0288  data: 0.0002  max mem: 4132
[19:48:22.601122] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6027 (0.5872)  acc1: 78.1250 (81.6301)  acc5: 98.4375 (98.9724)  time: 0.0291  data: 0.0002  max mem: 4132
[19:48:22.889503] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5688 (0.5860)  acc1: 82.8125 (81.7149)  acc5: 100.0000 (99.0057)  time: 0.0292  data: 0.0002  max mem: 4132
[19:48:23.178208] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5837 (0.5850)  acc1: 79.6875 (81.6794)  acc5: 100.0000 (99.0339)  time: 0.0287  data: 0.0002  max mem: 4132
[19:48:23.464931] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5976 (0.5834)  acc1: 81.2500 (81.7376)  acc5: 100.0000 (99.0691)  time: 0.0286  data: 0.0002  max mem: 4132
[19:48:23.748686] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5829 (0.5820)  acc1: 82.8125 (81.8295)  acc5: 100.0000 (99.0791)  time: 0.0284  data: 0.0002  max mem: 4132
[19:48:23.905570] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5661 (0.5814)  acc1: 82.8125 (81.8800)  acc5: 100.0000 (99.0800)  time: 0.0275  data: 0.0002  max mem: 4132
[19:48:24.085586] Test: Total time: 0:00:05 (0.0345 s / it)
[19:48:24.086309] * Acc@1 81.880 Acc@5 99.080 loss 0.581
[19:48:24.086699] Accuracy of the network on the 10000 test images: 81.9%
[19:48:24.086904] Max accuracy: 81.88%
[19:48:24.439971] log_dir: ./output_dir
[19:48:25.356010] Epoch: [61]  [  0/781]  eta: 0:11:53  lr: 0.000091  training_loss: 1.1622 (1.1622)  classification_loss: 1.1605 (1.1605)  loss_mask: 0.0017 (0.0017)  time: 0.9139  data: 0.7230  max mem: 4132
[19:48:28.644252] Epoch: [61]  [ 20/781]  eta: 0:02:32  lr: 0.000091  training_loss: 1.3207 (1.3058)  classification_loss: 1.3206 (1.3047)  loss_mask: 0.0010 (0.0011)  time: 0.1643  data: 0.0002  max mem: 4132
[19:48:31.930981] Epoch: [61]  [ 40/781]  eta: 0:02:15  lr: 0.000091  training_loss: 1.3002 (1.3015)  classification_loss: 1.2989 (1.3004)  loss_mask: 0.0013 (0.0012)  time: 0.1643  data: 0.0003  max mem: 4132
[19:48:35.232271] Epoch: [61]  [ 60/781]  eta: 0:02:07  lr: 0.000091  training_loss: 1.2963 (1.3059)  classification_loss: 1.2954 (1.3048)  loss_mask: 0.0008 (0.0011)  time: 0.1650  data: 0.0003  max mem: 4132
[19:48:38.547104] Epoch: [61]  [ 80/781]  eta: 0:02:02  lr: 0.000091  training_loss: 1.2563 (1.3023)  classification_loss: 1.2558 (1.3012)  loss_mask: 0.0009 (0.0010)  time: 0.1656  data: 0.0003  max mem: 4132
[19:48:41.814686] Epoch: [61]  [100/781]  eta: 0:01:57  lr: 0.000090  training_loss: 1.3575 (1.3092)  classification_loss: 1.3568 (1.3083)  loss_mask: 0.0007 (0.0010)  time: 0.1633  data: 0.0002  max mem: 4132
[19:48:45.122102] Epoch: [61]  [120/781]  eta: 0:01:52  lr: 0.000090  training_loss: 1.2852 (1.3076)  classification_loss: 1.2848 (1.3067)  loss_mask: 0.0006 (0.0009)  time: 0.1653  data: 0.0003  max mem: 4132
[19:48:48.397944] Epoch: [61]  [140/781]  eta: 0:01:48  lr: 0.000090  training_loss: 1.2801 (1.3049)  classification_loss: 1.2796 (1.3040)  loss_mask: 0.0005 (0.0009)  time: 0.1637  data: 0.0003  max mem: 4132
[19:48:51.680382] Epoch: [61]  [160/781]  eta: 0:01:45  lr: 0.000090  training_loss: 1.2202 (1.2987)  classification_loss: 1.2195 (1.2979)  loss_mask: 0.0005 (0.0008)  time: 0.1640  data: 0.0003  max mem: 4132
[19:48:54.976898] Epoch: [61]  [180/781]  eta: 0:01:41  lr: 0.000090  training_loss: 1.2693 (1.2958)  classification_loss: 1.2685 (1.2950)  loss_mask: 0.0005 (0.0008)  time: 0.1647  data: 0.0003  max mem: 4132
[19:48:58.304927] Epoch: [61]  [200/781]  eta: 0:01:37  lr: 0.000090  training_loss: 1.2906 (1.2971)  classification_loss: 1.2900 (1.2957)  loss_mask: 0.0007 (0.0014)  time: 0.1663  data: 0.0003  max mem: 4132
[19:49:01.581587] Epoch: [61]  [220/781]  eta: 0:01:34  lr: 0.000090  training_loss: 1.2893 (1.2950)  classification_loss: 1.2887 (1.2937)  loss_mask: 0.0007 (0.0013)  time: 0.1638  data: 0.0003  max mem: 4132
[19:49:04.845506] Epoch: [61]  [240/781]  eta: 0:01:30  lr: 0.000090  training_loss: 1.2583 (1.2936)  classification_loss: 1.2581 (1.2923)  loss_mask: 0.0007 (0.0013)  time: 0.1631  data: 0.0003  max mem: 4132
[19:49:08.120479] Epoch: [61]  [260/781]  eta: 0:01:27  lr: 0.000090  training_loss: 1.2494 (1.2933)  classification_loss: 1.2488 (1.2921)  loss_mask: 0.0005 (0.0012)  time: 0.1637  data: 0.0003  max mem: 4132
[19:49:11.418085] Epoch: [61]  [280/781]  eta: 0:01:23  lr: 0.000090  training_loss: 1.2682 (1.2920)  classification_loss: 1.2675 (1.2908)  loss_mask: 0.0006 (0.0011)  time: 0.1648  data: 0.0003  max mem: 4132
[19:49:15.609078] Epoch: [61]  [300/781]  eta: 0:01:21  lr: 0.000089  training_loss: 1.3100 (1.2925)  classification_loss: 1.3098 (1.2914)  loss_mask: 0.0004 (0.0011)  time: 0.2095  data: 0.0460  max mem: 4132
[19:49:18.885476] Epoch: [61]  [320/781]  eta: 0:01:18  lr: 0.000089  training_loss: 1.3108 (1.2928)  classification_loss: 1.3100 (1.2917)  loss_mask: 0.0004 (0.0011)  time: 0.1637  data: 0.0003  max mem: 4132
[19:49:22.150536] Epoch: [61]  [340/781]  eta: 0:01:14  lr: 0.000089  training_loss: 1.3094 (1.2936)  classification_loss: 1.3090 (1.2925)  loss_mask: 0.0004 (0.0010)  time: 0.1632  data: 0.0002  max mem: 4132
[19:49:25.407921] Epoch: [61]  [360/781]  eta: 0:01:11  lr: 0.000089  training_loss: 1.2934 (1.2928)  classification_loss: 1.2928 (1.2919)  loss_mask: 0.0004 (0.0010)  time: 0.1628  data: 0.0002  max mem: 4132
[19:49:28.664933] Epoch: [61]  [380/781]  eta: 0:01:07  lr: 0.000089  training_loss: 1.2583 (1.2919)  classification_loss: 1.2577 (1.2909)  loss_mask: 0.0004 (0.0010)  time: 0.1628  data: 0.0002  max mem: 4132
[19:49:31.925941] Epoch: [61]  [400/781]  eta: 0:01:04  lr: 0.000089  training_loss: 1.2858 (1.2923)  classification_loss: 1.2856 (1.2914)  loss_mask: 0.0003 (0.0009)  time: 0.1629  data: 0.0003  max mem: 4132
[19:49:35.212870] Epoch: [61]  [420/781]  eta: 0:01:00  lr: 0.000089  training_loss: 1.3283 (1.2935)  classification_loss: 1.3280 (1.2926)  loss_mask: 0.0003 (0.0009)  time: 0.1643  data: 0.0003  max mem: 4132
[19:49:38.509687] Epoch: [61]  [440/781]  eta: 0:00:57  lr: 0.000089  training_loss: 1.2808 (1.2933)  classification_loss: 1.2805 (1.2923)  loss_mask: 0.0004 (0.0010)  time: 0.1647  data: 0.0003  max mem: 4132
[19:49:41.801528] Epoch: [61]  [460/781]  eta: 0:00:53  lr: 0.000089  training_loss: 1.2745 (1.2939)  classification_loss: 1.2357 (1.2911)  loss_mask: 0.0074 (0.0028)  time: 0.1645  data: 0.0003  max mem: 4132
[19:49:45.084075] Epoch: [61]  [480/781]  eta: 0:00:50  lr: 0.000089  training_loss: 1.3820 (1.2995)  classification_loss: 1.2870 (1.2909)  loss_mask: 0.0732 (0.0085)  time: 0.1640  data: 0.0007  max mem: 4132
[19:49:48.386279] Epoch: [61]  [500/781]  eta: 0:00:47  lr: 0.000088  training_loss: 1.3312 (1.3019)  classification_loss: 1.2798 (1.2905)  loss_mask: 0.0595 (0.0114)  time: 0.1650  data: 0.0003  max mem: 4132
[19:49:51.668301] Epoch: [61]  [520/781]  eta: 0:00:43  lr: 0.000088  training_loss: 1.2597 (1.3012)  classification_loss: 1.2289 (1.2889)  loss_mask: 0.0339 (0.0123)  time: 0.1640  data: 0.0002  max mem: 4132
[19:50:26.988945] Epoch: [61]  [540/781]  eta: 0:00:54  lr: 0.000088  training_loss: 1.2929 (1.3004)  classification_loss: 1.2725 (1.2877)  loss_mask: 0.0204 (0.0128)  time: 1.7659  data: 0.0003  max mem: 4132
[19:50:30.290634] Epoch: [61]  [560/781]  eta: 0:00:49  lr: 0.000088  training_loss: 1.2740 (1.2993)  classification_loss: 1.2676 (1.2866)  loss_mask: 0.0107 (0.0127)  time: 0.1650  data: 0.0003  max mem: 4132
[19:50:33.569925] Epoch: [61]  [580/781]  eta: 0:00:44  lr: 0.000088  training_loss: 1.2455 (1.2975)  classification_loss: 1.2387 (1.2849)  loss_mask: 0.0067 (0.0125)  time: 0.1639  data: 0.0003  max mem: 4132
[19:50:36.878850] Epoch: [61]  [600/781]  eta: 0:00:39  lr: 0.000088  training_loss: 1.2339 (1.2957)  classification_loss: 1.2316 (1.2834)  loss_mask: 0.0042 (0.0123)  time: 0.1653  data: 0.0004  max mem: 4132
[19:50:40.142341] Epoch: [61]  [620/781]  eta: 0:00:35  lr: 0.000088  training_loss: 1.3027 (1.2964)  classification_loss: 1.3014 (1.2845)  loss_mask: 0.0020 (0.0120)  time: 0.1631  data: 0.0002  max mem: 4132
[19:50:43.419186] Epoch: [61]  [640/781]  eta: 0:00:30  lr: 0.000088  training_loss: 1.2799 (1.2961)  classification_loss: 1.2789 (1.2844)  loss_mask: 0.0014 (0.0116)  time: 0.1637  data: 0.0002  max mem: 4132
[19:50:46.705995] Epoch: [61]  [660/781]  eta: 0:00:26  lr: 0.000088  training_loss: 1.2685 (1.2955)  classification_loss: 1.2553 (1.2840)  loss_mask: 0.0015 (0.0115)  time: 0.1642  data: 0.0002  max mem: 4132
[19:50:49.978016] Epoch: [61]  [680/781]  eta: 0:00:21  lr: 0.000088  training_loss: 1.2606 (1.2951)  classification_loss: 1.2572 (1.2839)  loss_mask: 0.0024 (0.0112)  time: 0.1635  data: 0.0003  max mem: 4132
[19:50:53.239978] Epoch: [61]  [700/781]  eta: 0:00:17  lr: 0.000087  training_loss: 1.2578 (1.2950)  classification_loss: 1.2378 (1.2839)  loss_mask: 0.0015 (0.0111)  time: 0.1630  data: 0.0002  max mem: 4132
[19:50:56.514647] Epoch: [61]  [720/781]  eta: 0:00:12  lr: 0.000087  training_loss: 1.3103 (1.2959)  classification_loss: 1.3087 (1.2850)  loss_mask: 0.0013 (0.0109)  time: 0.1636  data: 0.0003  max mem: 4132
[19:50:59.815713] Epoch: [61]  [740/781]  eta: 0:00:08  lr: 0.000087  training_loss: 1.2632 (1.2954)  classification_loss: 1.2625 (1.2847)  loss_mask: 0.0011 (0.0106)  time: 0.1650  data: 0.0003  max mem: 4132
[19:51:03.093400] Epoch: [61]  [760/781]  eta: 0:00:04  lr: 0.000087  training_loss: 1.2856 (1.2952)  classification_loss: 1.2848 (1.2848)  loss_mask: 0.0008 (0.0104)  time: 0.1638  data: 0.0003  max mem: 4132
[19:51:06.367075] Epoch: [61]  [780/781]  eta: 0:00:00  lr: 0.000087  training_loss: 1.2588 (1.2948)  classification_loss: 1.2570 (1.2847)  loss_mask: 0.0007 (0.0101)  time: 0.1636  data: 0.0003  max mem: 4132
[19:51:06.586095] Epoch: [61] Total time: 0:02:42 (0.2076 s / it)
[19:51:06.586598] Averaged stats: lr: 0.000087  training_loss: 1.2588 (1.2948)  classification_loss: 1.2570 (1.2847)  loss_mask: 0.0007 (0.0101)
[19:51:07.307886] Test:  [  0/157]  eta: 0:01:50  testing_loss: 0.5877 (0.5877)  acc1: 79.6875 (79.6875)  acc5: 98.4375 (98.4375)  time: 0.7041  data: 0.6676  max mem: 4132
[19:51:07.605203] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.6179 (0.6154)  acc1: 81.2500 (80.9659)  acc5: 98.4375 (99.0057)  time: 0.0908  data: 0.0610  max mem: 4132
[19:51:07.893457] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5584 (0.5895)  acc1: 81.2500 (81.1012)  acc5: 98.4375 (99.0327)  time: 0.0290  data: 0.0003  max mem: 4132
[19:51:08.188704] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5898 (0.6082)  acc1: 81.2500 (80.7460)  acc5: 98.4375 (98.9415)  time: 0.0290  data: 0.0003  max mem: 4132
[19:51:08.476863] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6110 (0.6134)  acc1: 81.2500 (80.6402)  acc5: 98.4375 (98.8186)  time: 0.0290  data: 0.0003  max mem: 4132
[19:51:08.773648] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5741 (0.6033)  acc1: 81.2500 (81.3419)  acc5: 98.4375 (98.9583)  time: 0.0291  data: 0.0004  max mem: 4132
[19:51:09.063927] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5494 (0.5996)  acc1: 82.8125 (81.4037)  acc5: 100.0000 (99.0266)  time: 0.0292  data: 0.0005  max mem: 4132
[19:51:09.356239] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5579 (0.5964)  acc1: 81.2500 (81.4481)  acc5: 100.0000 (99.0097)  time: 0.0289  data: 0.0004  max mem: 4132
[19:51:09.647908] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5848 (0.6035)  acc1: 81.2500 (81.0764)  acc5: 98.4375 (98.9583)  time: 0.0290  data: 0.0004  max mem: 4132
[19:51:09.936254] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5880 (0.6012)  acc1: 79.6875 (81.1470)  acc5: 98.4375 (98.9354)  time: 0.0288  data: 0.0004  max mem: 4132
[19:51:10.222773] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.5915 (0.6054)  acc1: 78.1250 (80.9561)  acc5: 98.4375 (98.9171)  time: 0.0286  data: 0.0002  max mem: 4132
[19:51:10.511220] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6366 (0.6039)  acc1: 78.1250 (80.9544)  acc5: 100.0000 (98.9583)  time: 0.0286  data: 0.0002  max mem: 4132
[19:51:10.800464] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5796 (0.6023)  acc1: 81.2500 (81.0305)  acc5: 100.0000 (98.9799)  time: 0.0287  data: 0.0002  max mem: 4132
[19:51:11.085252] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6241 (0.6058)  acc1: 81.2500 (80.9637)  acc5: 100.0000 (98.9742)  time: 0.0285  data: 0.0002  max mem: 4132
[19:51:11.369909] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5964 (0.6021)  acc1: 81.2500 (81.0616)  acc5: 100.0000 (98.9916)  time: 0.0283  data: 0.0002  max mem: 4132
[19:51:11.652285] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5813 (0.6017)  acc1: 81.2500 (81.0017)  acc5: 100.0000 (98.9963)  time: 0.0282  data: 0.0002  max mem: 4132
[19:51:11.807015] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5813 (0.6027)  acc1: 81.2500 (80.9900)  acc5: 98.4375 (98.9800)  time: 0.0273  data: 0.0002  max mem: 4132
[19:51:11.976350] Test: Total time: 0:00:05 (0.0343 s / it)
[19:51:11.976893] * Acc@1 80.990 Acc@5 98.980 loss 0.603
[19:51:11.977305] Accuracy of the network on the 10000 test images: 81.0%
[19:51:11.977750] Max accuracy: 81.88%
[19:51:12.210381] log_dir: ./output_dir
[19:51:13.178211] Epoch: [62]  [  0/781]  eta: 0:12:34  lr: 0.000087  training_loss: 1.2021 (1.2021)  classification_loss: 1.2014 (1.2014)  loss_mask: 0.0008 (0.0008)  time: 0.9659  data: 0.7558  max mem: 4132
[19:51:16.456227] Epoch: [62]  [ 20/781]  eta: 0:02:33  lr: 0.000087  training_loss: 1.2364 (1.2501)  classification_loss: 1.2360 (1.2495)  loss_mask: 0.0005 (0.0006)  time: 0.1637  data: 0.0003  max mem: 4132
[19:51:19.720897] Epoch: [62]  [ 40/781]  eta: 0:02:15  lr: 0.000087  training_loss: 1.2656 (1.2603)  classification_loss: 1.2649 (1.2593)  loss_mask: 0.0008 (0.0010)  time: 0.1631  data: 0.0003  max mem: 4132
[19:51:23.034991] Epoch: [62]  [ 60/781]  eta: 0:02:07  lr: 0.000087  training_loss: 1.2503 (1.2652)  classification_loss: 1.2490 (1.2643)  loss_mask: 0.0007 (0.0009)  time: 0.1656  data: 0.0003  max mem: 4132
[19:51:26.308732] Epoch: [62]  [ 80/781]  eta: 0:02:01  lr: 0.000087  training_loss: 1.2735 (1.2663)  classification_loss: 1.2734 (1.2654)  loss_mask: 0.0007 (0.0009)  time: 0.1636  data: 0.0003  max mem: 4132
[19:51:29.572462] Epoch: [62]  [100/781]  eta: 0:01:56  lr: 0.000087  training_loss: 1.2846 (1.2711)  classification_loss: 1.2841 (1.2703)  loss_mask: 0.0005 (0.0008)  time: 0.1631  data: 0.0002  max mem: 4132
[19:51:32.843348] Epoch: [62]  [120/781]  eta: 0:01:52  lr: 0.000086  training_loss: 1.3019 (1.2784)  classification_loss: 1.3017 (1.2777)  loss_mask: 0.0004 (0.0007)  time: 0.1635  data: 0.0003  max mem: 4132
[19:51:36.154267] Epoch: [62]  [140/781]  eta: 0:01:48  lr: 0.000086  training_loss: 1.2821 (1.2805)  classification_loss: 1.2795 (1.2787)  loss_mask: 0.0006 (0.0018)  time: 0.1655  data: 0.0004  max mem: 4132
[19:51:39.448525] Epoch: [62]  [160/781]  eta: 0:01:44  lr: 0.000086  training_loss: 1.2637 (1.2800)  classification_loss: 1.2626 (1.2783)  loss_mask: 0.0008 (0.0017)  time: 0.1646  data: 0.0003  max mem: 4132
[19:51:42.702089] Epoch: [62]  [180/781]  eta: 0:01:41  lr: 0.000086  training_loss: 1.2446 (1.2807)  classification_loss: 1.2443 (1.2792)  loss_mask: 0.0005 (0.0016)  time: 0.1626  data: 0.0002  max mem: 4132
[19:51:45.979238] Epoch: [62]  [200/781]  eta: 0:01:37  lr: 0.000086  training_loss: 1.3090 (1.2845)  classification_loss: 1.3082 (1.2830)  loss_mask: 0.0005 (0.0016)  time: 0.1638  data: 0.0003  max mem: 4132
[19:51:49.250974] Epoch: [62]  [220/781]  eta: 0:01:33  lr: 0.000086  training_loss: 1.2723 (1.2843)  classification_loss: 1.2686 (1.2828)  loss_mask: 0.0007 (0.0015)  time: 0.1635  data: 0.0003  max mem: 4132
[19:51:52.534500] Epoch: [62]  [240/781]  eta: 0:01:30  lr: 0.000086  training_loss: 1.2891 (1.2853)  classification_loss: 1.2889 (1.2838)  loss_mask: 0.0004 (0.0014)  time: 0.1641  data: 0.0003  max mem: 4132
[19:51:55.824358] Epoch: [62]  [260/781]  eta: 0:01:27  lr: 0.000086  training_loss: 1.2959 (1.2856)  classification_loss: 1.2956 (1.2843)  loss_mask: 0.0003 (0.0013)  time: 0.1644  data: 0.0002  max mem: 4132
[19:51:59.091643] Epoch: [62]  [280/781]  eta: 0:01:23  lr: 0.000086  training_loss: 1.2564 (1.2834)  classification_loss: 1.2562 (1.2822)  loss_mask: 0.0003 (0.0013)  time: 0.1632  data: 0.0003  max mem: 4132
[19:52:02.354673] Epoch: [62]  [300/781]  eta: 0:01:20  lr: 0.000086  training_loss: 1.3441 (1.2867)  classification_loss: 1.3438 (1.2855)  loss_mask: 0.0003 (0.0012)  time: 0.1631  data: 0.0003  max mem: 4132
[19:52:05.633792] Epoch: [62]  [320/781]  eta: 0:01:16  lr: 0.000085  training_loss: 1.2639 (1.2862)  classification_loss: 1.2636 (1.2851)  loss_mask: 0.0003 (0.0011)  time: 0.1639  data: 0.0002  max mem: 4132
[19:52:08.896453] Epoch: [62]  [340/781]  eta: 0:01:13  lr: 0.000085  training_loss: 1.2387 (1.2838)  classification_loss: 1.2385 (1.2827)  loss_mask: 0.0003 (0.0011)  time: 0.1630  data: 0.0003  max mem: 4132
[19:52:12.191230] Epoch: [62]  [360/781]  eta: 0:01:09  lr: 0.000085  training_loss: 1.2608 (1.2832)  classification_loss: 1.2602 (1.2821)  loss_mask: 0.0003 (0.0010)  time: 0.1647  data: 0.0003  max mem: 4132
[19:52:15.523307] Epoch: [62]  [380/781]  eta: 0:01:06  lr: 0.000085  training_loss: 1.2919 (1.2840)  classification_loss: 1.2915 (1.2830)  loss_mask: 0.0003 (0.0010)  time: 0.1665  data: 0.0003  max mem: 4132
[19:52:18.845503] Epoch: [62]  [400/781]  eta: 0:01:03  lr: 0.000085  training_loss: 1.2918 (1.2851)  classification_loss: 1.2912 (1.2841)  loss_mask: 0.0003 (0.0010)  time: 0.1660  data: 0.0004  max mem: 4132
[19:52:22.124550] Epoch: [62]  [420/781]  eta: 0:00:59  lr: 0.000085  training_loss: 1.2549 (1.2837)  classification_loss: 1.2545 (1.2827)  loss_mask: 0.0002 (0.0009)  time: 0.1638  data: 0.0003  max mem: 4132
[19:52:25.390078] Epoch: [62]  [440/781]  eta: 0:00:56  lr: 0.000085  training_loss: 1.2439 (1.2824)  classification_loss: 1.2437 (1.2815)  loss_mask: 0.0003 (0.0009)  time: 0.1632  data: 0.0003  max mem: 4132
[19:52:28.701240] Epoch: [62]  [460/781]  eta: 0:00:53  lr: 0.000085  training_loss: 1.2289 (1.2800)  classification_loss: 1.2258 (1.2790)  loss_mask: 0.0004 (0.0010)  time: 0.1655  data: 0.0002  max mem: 4132
[19:52:31.986301] Epoch: [62]  [480/781]  eta: 0:00:49  lr: 0.000085  training_loss: 1.2855 (1.2809)  classification_loss: 1.2852 (1.2799)  loss_mask: 0.0004 (0.0009)  time: 0.1642  data: 0.0005  max mem: 4132
[19:52:35.254556] Epoch: [62]  [500/781]  eta: 0:00:46  lr: 0.000085  training_loss: 1.3067 (1.2821)  classification_loss: 1.2583 (1.2801)  loss_mask: 0.0008 (0.0019)  time: 0.1633  data: 0.0003  max mem: 4132
[19:52:38.524535] Epoch: [62]  [520/781]  eta: 0:00:43  lr: 0.000084  training_loss: 1.2968 (1.2834)  classification_loss: 1.2698 (1.2796)  loss_mask: 0.0239 (0.0039)  time: 0.1634  data: 0.0003  max mem: 4132
[19:52:41.836388] Epoch: [62]  [540/781]  eta: 0:00:39  lr: 0.000084  training_loss: 1.2574 (1.2828)  classification_loss: 1.2546 (1.2787)  loss_mask: 0.0086 (0.0041)  time: 0.1655  data: 0.0008  max mem: 4132
[19:52:45.133385] Epoch: [62]  [560/781]  eta: 0:00:36  lr: 0.000084  training_loss: 1.2953 (1.2832)  classification_loss: 1.2932 (1.2792)  loss_mask: 0.0025 (0.0041)  time: 0.1648  data: 0.0003  max mem: 4132
[19:52:48.460195] Epoch: [62]  [580/781]  eta: 0:00:33  lr: 0.000084  training_loss: 1.3304 (1.2848)  classification_loss: 1.3264 (1.2808)  loss_mask: 0.0015 (0.0040)  time: 0.1663  data: 0.0003  max mem: 4132
[19:52:51.757969] Epoch: [62]  [600/781]  eta: 0:00:29  lr: 0.000084  training_loss: 1.2778 (1.2838)  classification_loss: 1.2773 (1.2799)  loss_mask: 0.0008 (0.0039)  time: 0.1648  data: 0.0005  max mem: 4132
[19:52:55.034269] Epoch: [62]  [620/781]  eta: 0:00:26  lr: 0.000084  training_loss: 1.2754 (1.2840)  classification_loss: 1.2602 (1.2799)  loss_mask: 0.0014 (0.0041)  time: 0.1637  data: 0.0002  max mem: 4132
[19:52:58.313116] Epoch: [62]  [640/781]  eta: 0:00:23  lr: 0.000084  training_loss: 1.3454 (1.2877)  classification_loss: 1.2562 (1.2801)  loss_mask: 0.0757 (0.0076)  time: 0.1639  data: 0.0005  max mem: 4132
[19:53:01.583857] Epoch: [62]  [660/781]  eta: 0:00:20  lr: 0.000084  training_loss: 1.4354 (1.2931)  classification_loss: 1.2612 (1.2806)  loss_mask: 0.1607 (0.0125)  time: 0.1635  data: 0.0004  max mem: 4132
[19:53:04.867000] Epoch: [62]  [680/781]  eta: 0:00:16  lr: 0.000084  training_loss: 1.3152 (1.2942)  classification_loss: 1.2683 (1.2807)  loss_mask: 0.0317 (0.0135)  time: 0.1641  data: 0.0004  max mem: 4132
[19:53:08.150465] Epoch: [62]  [700/781]  eta: 0:00:13  lr: 0.000084  training_loss: 1.3198 (1.2947)  classification_loss: 1.3101 (1.2810)  loss_mask: 0.0148 (0.0137)  time: 0.1641  data: 0.0003  max mem: 4132
[19:53:11.423009] Epoch: [62]  [720/781]  eta: 0:00:10  lr: 0.000083  training_loss: 1.2923 (1.2949)  classification_loss: 1.2819 (1.2811)  loss_mask: 0.0133 (0.0137)  time: 0.1635  data: 0.0002  max mem: 4132
[19:53:14.683590] Epoch: [62]  [740/781]  eta: 0:00:06  lr: 0.000083  training_loss: 1.2844 (1.2947)  classification_loss: 1.2761 (1.2810)  loss_mask: 0.0097 (0.0136)  time: 0.1629  data: 0.0003  max mem: 4132
[19:53:17.952937] Epoch: [62]  [760/781]  eta: 0:00:03  lr: 0.000083  training_loss: 1.2927 (1.2945)  classification_loss: 1.2878 (1.2812)  loss_mask: 0.0036 (0.0134)  time: 0.1634  data: 0.0002  max mem: 4132
[19:53:21.228104] Epoch: [62]  [780/781]  eta: 0:00:00  lr: 0.000083  training_loss: 1.2677 (1.2942)  classification_loss: 1.2663 (1.2812)  loss_mask: 0.0020 (0.0131)  time: 0.1637  data: 0.0002  max mem: 4132
[19:53:21.393957] Epoch: [62] Total time: 0:02:09 (0.1654 s / it)
[19:53:21.394749] Averaged stats: lr: 0.000083  training_loss: 1.2677 (1.2942)  classification_loss: 1.2663 (1.2812)  loss_mask: 0.0020 (0.0131)
[19:53:22.136330] Test:  [  0/157]  eta: 0:01:55  testing_loss: 0.5101 (0.5101)  acc1: 85.9375 (85.9375)  acc5: 96.8750 (96.8750)  time: 0.7368  data: 0.7069  max mem: 4132
[19:53:22.428992] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.5683 (0.5764)  acc1: 82.8125 (82.8125)  acc5: 98.4375 (99.0057)  time: 0.0933  data: 0.0646  max mem: 4132
[19:53:22.714717] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5555 (0.5483)  acc1: 82.8125 (83.8542)  acc5: 100.0000 (99.4048)  time: 0.0287  data: 0.0003  max mem: 4132
[19:53:23.019646] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5499 (0.5675)  acc1: 84.3750 (83.5181)  acc5: 100.0000 (99.2440)  time: 0.0293  data: 0.0004  max mem: 4132
[19:53:23.310332] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5539 (0.5678)  acc1: 84.3750 (83.4223)  acc5: 98.4375 (99.1235)  time: 0.0295  data: 0.0004  max mem: 4132
[19:53:23.601587] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5457 (0.5606)  acc1: 84.3750 (83.7316)  acc5: 98.4375 (99.1728)  time: 0.0289  data: 0.0003  max mem: 4132
[19:53:23.894809] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5450 (0.5567)  acc1: 84.3750 (83.8883)  acc5: 100.0000 (99.2059)  time: 0.0291  data: 0.0002  max mem: 4132
[19:53:24.189831] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5230 (0.5517)  acc1: 84.3750 (84.1549)  acc5: 100.0000 (99.2077)  time: 0.0292  data: 0.0002  max mem: 4132
[19:53:24.482961] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5369 (0.5606)  acc1: 82.8125 (83.7577)  acc5: 98.4375 (99.1512)  time: 0.0292  data: 0.0002  max mem: 4132
[19:53:24.777091] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5813 (0.5618)  acc1: 81.2500 (83.6882)  acc5: 98.4375 (99.0900)  time: 0.0292  data: 0.0002  max mem: 4132
[19:53:25.069764] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.5813 (0.5654)  acc1: 81.2500 (83.4777)  acc5: 98.4375 (99.0873)  time: 0.0292  data: 0.0002  max mem: 4132
[19:53:25.356316] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5688 (0.5649)  acc1: 81.2500 (83.4600)  acc5: 100.0000 (99.0991)  time: 0.0288  data: 0.0002  max mem: 4132
[19:53:25.646106] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5621 (0.5634)  acc1: 82.8125 (83.4711)  acc5: 100.0000 (99.1090)  time: 0.0287  data: 0.0002  max mem: 4132
[19:53:25.945590] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5642 (0.5640)  acc1: 82.8125 (83.3850)  acc5: 100.0000 (99.1770)  time: 0.0293  data: 0.0002  max mem: 4132
[19:53:26.233698] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5488 (0.5623)  acc1: 81.2500 (83.4441)  acc5: 100.0000 (99.1800)  time: 0.0292  data: 0.0002  max mem: 4132
[19:53:26.518167] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5235 (0.5610)  acc1: 81.2500 (83.3609)  acc5: 100.0000 (99.1722)  time: 0.0285  data: 0.0002  max mem: 4132
[19:53:26.671871] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5194 (0.5601)  acc1: 84.3750 (83.4200)  acc5: 100.0000 (99.1900)  time: 0.0274  data: 0.0002  max mem: 4132
[19:53:26.846377] Test: Total time: 0:00:05 (0.0347 s / it)
[19:53:26.847528] * Acc@1 83.420 Acc@5 99.190 loss 0.560
[19:53:26.847841] Accuracy of the network on the 10000 test images: 83.4%
[19:53:26.848038] Max accuracy: 83.42%
[19:53:27.161903] log_dir: ./output_dir
[19:53:28.125983] Epoch: [63]  [  0/781]  eta: 0:12:31  lr: 0.000083  training_loss: 1.2218 (1.2218)  classification_loss: 1.2197 (1.2197)  loss_mask: 0.0021 (0.0021)  time: 0.9617  data: 0.7644  max mem: 4132
[19:53:31.429427] Epoch: [63]  [ 20/781]  eta: 0:02:34  lr: 0.000083  training_loss: 1.2223 (1.2268)  classification_loss: 1.2204 (1.2248)  loss_mask: 0.0021 (0.0020)  time: 0.1651  data: 0.0003  max mem: 4132
[19:53:34.729424] Epoch: [63]  [ 40/781]  eta: 0:02:16  lr: 0.000083  training_loss: 1.2833 (1.2608)  classification_loss: 1.2820 (1.2591)  loss_mask: 0.0015 (0.0017)  time: 0.1649  data: 0.0003  max mem: 4132
[19:53:38.007420] Epoch: [63]  [ 60/781]  eta: 0:02:08  lr: 0.000083  training_loss: 1.2828 (1.2736)  classification_loss: 1.2817 (1.2719)  loss_mask: 0.0014 (0.0017)  time: 0.1638  data: 0.0003  max mem: 4132
[19:53:41.306985] Epoch: [63]  [ 80/781]  eta: 0:02:02  lr: 0.000083  training_loss: 1.2528 (1.2745)  classification_loss: 1.2520 (1.2729)  loss_mask: 0.0010 (0.0015)  time: 0.1649  data: 0.0003  max mem: 4132
[19:53:44.615759] Epoch: [63]  [100/781]  eta: 0:01:57  lr: 0.000083  training_loss: 1.3170 (1.2841)  classification_loss: 1.3159 (1.2826)  loss_mask: 0.0010 (0.0014)  time: 0.1653  data: 0.0003  max mem: 4132
[19:53:47.889926] Epoch: [63]  [120/781]  eta: 0:01:53  lr: 0.000083  training_loss: 1.2343 (1.2807)  classification_loss: 1.2337 (1.2794)  loss_mask: 0.0006 (0.0013)  time: 0.1636  data: 0.0003  max mem: 4132
[19:53:51.187330] Epoch: [63]  [140/781]  eta: 0:01:49  lr: 0.000082  training_loss: 1.2395 (1.2761)  classification_loss: 1.2380 (1.2749)  loss_mask: 0.0009 (0.0012)  time: 0.1647  data: 0.0003  max mem: 4132
[19:53:54.517130] Epoch: [63]  [160/781]  eta: 0:01:45  lr: 0.000082  training_loss: 1.2320 (1.2734)  classification_loss: 1.2311 (1.2722)  loss_mask: 0.0007 (0.0012)  time: 0.1664  data: 0.0002  max mem: 4132
[19:53:57.821994] Epoch: [63]  [180/781]  eta: 0:01:41  lr: 0.000082  training_loss: 1.2695 (1.2761)  classification_loss: 1.2689 (1.2749)  loss_mask: 0.0005 (0.0011)  time: 0.1651  data: 0.0003  max mem: 4132
[19:54:01.115154] Epoch: [63]  [200/781]  eta: 0:01:38  lr: 0.000082  training_loss: 1.2770 (1.2736)  classification_loss: 1.2764 (1.2725)  loss_mask: 0.0006 (0.0011)  time: 0.1646  data: 0.0003  max mem: 4132
[19:54:04.414874] Epoch: [63]  [220/781]  eta: 0:01:34  lr: 0.000082  training_loss: 1.2959 (1.2742)  classification_loss: 1.2953 (1.2731)  loss_mask: 0.0005 (0.0010)  time: 0.1649  data: 0.0002  max mem: 4132
[19:54:07.672739] Epoch: [63]  [240/781]  eta: 0:01:30  lr: 0.000082  training_loss: 1.2884 (1.2750)  classification_loss: 1.2661 (1.2735)  loss_mask: 0.0006 (0.0015)  time: 0.1628  data: 0.0003  max mem: 4132
[19:54:10.961646] Epoch: [63]  [260/781]  eta: 0:01:27  lr: 0.000082  training_loss: 1.2738 (1.2750)  classification_loss: 1.2687 (1.2726)  loss_mask: 0.0066 (0.0024)  time: 0.1643  data: 0.0003  max mem: 4132
[19:54:14.237045] Epoch: [63]  [280/781]  eta: 0:01:23  lr: 0.000082  training_loss: 1.3409 (1.2820)  classification_loss: 1.2769 (1.2734)  loss_mask: 0.0432 (0.0086)  time: 0.1637  data: 0.0003  max mem: 4132
[19:54:17.499980] Epoch: [63]  [300/781]  eta: 0:01:20  lr: 0.000082  training_loss: 1.3196 (1.2872)  classification_loss: 1.2702 (1.2743)  loss_mask: 0.0444 (0.0129)  time: 0.1630  data: 0.0003  max mem: 4132
[19:54:20.794780] Epoch: [63]  [320/781]  eta: 0:01:16  lr: 0.000082  training_loss: 1.2986 (1.2868)  classification_loss: 1.2850 (1.2735)  loss_mask: 0.0142 (0.0132)  time: 0.1647  data: 0.0003  max mem: 4132
[19:54:24.076879] Epoch: [63]  [340/781]  eta: 0:01:13  lr: 0.000081  training_loss: 1.2674 (1.2853)  classification_loss: 1.2578 (1.2723)  loss_mask: 0.0060 (0.0130)  time: 0.1640  data: 0.0002  max mem: 4132
[19:54:27.338967] Epoch: [63]  [360/781]  eta: 0:01:10  lr: 0.000081  training_loss: 1.2891 (1.2850)  classification_loss: 1.2845 (1.2725)  loss_mask: 0.0046 (0.0125)  time: 0.1630  data: 0.0003  max mem: 4132
[19:54:30.634570] Epoch: [63]  [380/781]  eta: 0:01:06  lr: 0.000081  training_loss: 1.2443 (1.2836)  classification_loss: 1.2388 (1.2716)  loss_mask: 0.0027 (0.0120)  time: 0.1647  data: 0.0003  max mem: 4132
[19:54:33.909986] Epoch: [63]  [400/781]  eta: 0:01:03  lr: 0.000081  training_loss: 1.2577 (1.2832)  classification_loss: 1.2567 (1.2717)  loss_mask: 0.0018 (0.0115)  time: 0.1637  data: 0.0003  max mem: 4132
[19:54:37.250201] Epoch: [63]  [420/781]  eta: 0:01:00  lr: 0.000081  training_loss: 1.2760 (1.2824)  classification_loss: 1.2749 (1.2714)  loss_mask: 0.0014 (0.0110)  time: 0.1669  data: 0.0004  max mem: 4132
[19:54:40.583163] Epoch: [63]  [440/781]  eta: 0:00:56  lr: 0.000081  training_loss: 1.2638 (1.2823)  classification_loss: 1.2630 (1.2717)  loss_mask: 0.0009 (0.0106)  time: 0.1665  data: 0.0004  max mem: 4132
[19:54:43.909140] Epoch: [63]  [460/781]  eta: 0:00:53  lr: 0.000081  training_loss: 1.2546 (1.2827)  classification_loss: 1.2541 (1.2725)  loss_mask: 0.0007 (0.0102)  time: 0.1662  data: 0.0003  max mem: 4132
[19:54:47.207964] Epoch: [63]  [480/781]  eta: 0:00:50  lr: 0.000081  training_loss: 1.3136 (1.2833)  classification_loss: 1.3122 (1.2735)  loss_mask: 0.0007 (0.0098)  time: 0.1648  data: 0.0002  max mem: 4132
[19:54:50.492348] Epoch: [63]  [500/781]  eta: 0:00:46  lr: 0.000081  training_loss: 1.2860 (1.2840)  classification_loss: 1.2845 (1.2746)  loss_mask: 0.0005 (0.0094)  time: 0.1641  data: 0.0003  max mem: 4132
[19:54:53.817364] Epoch: [63]  [520/781]  eta: 0:00:43  lr: 0.000081  training_loss: 1.2832 (1.2840)  classification_loss: 1.2809 (1.2749)  loss_mask: 0.0007 (0.0091)  time: 0.1662  data: 0.0005  max mem: 4132
[19:54:57.129284] Epoch: [63]  [540/781]  eta: 0:00:40  lr: 0.000080  training_loss: 1.3087 (1.2847)  classification_loss: 1.3081 (1.2759)  loss_mask: 0.0005 (0.0088)  time: 0.1655  data: 0.0003  max mem: 4132
[19:55:00.417608] Epoch: [63]  [560/781]  eta: 0:00:36  lr: 0.000080  training_loss: 1.2711 (1.2843)  classification_loss: 1.2701 (1.2758)  loss_mask: 0.0006 (0.0085)  time: 0.1643  data: 0.0003  max mem: 4132
[19:55:03.694930] Epoch: [63]  [580/781]  eta: 0:00:33  lr: 0.000080  training_loss: 1.2992 (1.2847)  classification_loss: 1.2988 (1.2765)  loss_mask: 0.0005 (0.0082)  time: 0.1638  data: 0.0003  max mem: 4132
[19:55:06.980506] Epoch: [63]  [600/781]  eta: 0:00:30  lr: 0.000080  training_loss: 1.2521 (1.2843)  classification_loss: 1.2520 (1.2764)  loss_mask: 0.0005 (0.0079)  time: 0.1642  data: 0.0003  max mem: 4132
[19:55:10.280155] Epoch: [63]  [620/781]  eta: 0:00:26  lr: 0.000080  training_loss: 1.2414 (1.2844)  classification_loss: 1.2409 (1.2767)  loss_mask: 0.0005 (0.0077)  time: 0.1649  data: 0.0003  max mem: 4132
[19:55:13.571054] Epoch: [63]  [640/781]  eta: 0:00:23  lr: 0.000080  training_loss: 1.2706 (1.2840)  classification_loss: 1.2703 (1.2765)  loss_mask: 0.0005 (0.0075)  time: 0.1645  data: 0.0003  max mem: 4132
[19:55:16.872300] Epoch: [63]  [660/781]  eta: 0:00:20  lr: 0.000080  training_loss: 1.2814 (1.2832)  classification_loss: 1.2801 (1.2759)  loss_mask: 0.0004 (0.0073)  time: 0.1650  data: 0.0003  max mem: 4132
[19:55:20.158040] Epoch: [63]  [680/781]  eta: 0:00:16  lr: 0.000080  training_loss: 1.2372 (1.2833)  classification_loss: 1.2370 (1.2762)  loss_mask: 0.0004 (0.0071)  time: 0.1642  data: 0.0003  max mem: 4132
[19:55:23.436283] Epoch: [63]  [700/781]  eta: 0:00:13  lr: 0.000080  training_loss: 1.2675 (1.2835)  classification_loss: 1.2672 (1.2767)  loss_mask: 0.0004 (0.0069)  time: 0.1638  data: 0.0003  max mem: 4132
[19:55:26.712341] Epoch: [63]  [720/781]  eta: 0:00:10  lr: 0.000080  training_loss: 1.2937 (1.2838)  classification_loss: 1.2934 (1.2771)  loss_mask: 0.0004 (0.0067)  time: 0.1637  data: 0.0002  max mem: 4132
[19:55:29.996756] Epoch: [63]  [740/781]  eta: 0:00:06  lr: 0.000079  training_loss: 1.2191 (1.2824)  classification_loss: 1.2187 (1.2758)  loss_mask: 0.0004 (0.0065)  time: 0.1641  data: 0.0003  max mem: 4132
[19:55:33.308160] Epoch: [63]  [760/781]  eta: 0:00:03  lr: 0.000079  training_loss: 1.2655 (1.2824)  classification_loss: 1.2650 (1.2760)  loss_mask: 0.0003 (0.0064)  time: 0.1655  data: 0.0003  max mem: 4132
[19:55:36.560004] Epoch: [63]  [780/781]  eta: 0:00:00  lr: 0.000079  training_loss: 1.2467 (1.2819)  classification_loss: 1.2463 (1.2757)  loss_mask: 0.0003 (0.0062)  time: 0.1625  data: 0.0002  max mem: 4132
[19:55:36.739979] Epoch: [63] Total time: 0:02:09 (0.1659 s / it)
[19:55:36.742079] Averaged stats: lr: 0.000079  training_loss: 1.2467 (1.2819)  classification_loss: 1.2463 (1.2757)  loss_mask: 0.0003 (0.0062)
[19:55:37.441128] Test:  [  0/157]  eta: 0:01:49  testing_loss: 0.5181 (0.5181)  acc1: 87.5000 (87.5000)  acc5: 96.8750 (96.8750)  time: 0.6950  data: 0.6631  max mem: 4132
[19:55:37.734622] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.5490 (0.5547)  acc1: 82.8125 (83.2386)  acc5: 100.0000 (99.2898)  time: 0.0897  data: 0.0604  max mem: 4132
[19:55:38.021386] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5370 (0.5307)  acc1: 82.8125 (84.5982)  acc5: 100.0000 (99.3304)  time: 0.0288  data: 0.0002  max mem: 4132
[19:55:38.312689] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5286 (0.5497)  acc1: 84.3750 (84.0726)  acc5: 98.4375 (99.0927)  time: 0.0287  data: 0.0002  max mem: 4132
[19:55:38.600763] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5392 (0.5539)  acc1: 82.8125 (83.8415)  acc5: 98.4375 (98.9710)  time: 0.0288  data: 0.0003  max mem: 4132
[19:55:38.891097] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5366 (0.5479)  acc1: 84.3750 (84.0993)  acc5: 98.4375 (99.0809)  time: 0.0287  data: 0.0003  max mem: 4132
[19:55:39.179252] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5278 (0.5435)  acc1: 84.3750 (84.0676)  acc5: 100.0000 (99.1035)  time: 0.0287  data: 0.0002  max mem: 4132
[19:55:39.468504] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4918 (0.5399)  acc1: 84.3750 (84.2650)  acc5: 100.0000 (99.0977)  time: 0.0287  data: 0.0002  max mem: 4132
[19:55:39.766361] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5391 (0.5483)  acc1: 84.3750 (83.8542)  acc5: 98.4375 (99.0162)  time: 0.0292  data: 0.0002  max mem: 4132
[19:55:40.055569] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5741 (0.5478)  acc1: 82.8125 (83.8771)  acc5: 98.4375 (99.0213)  time: 0.0292  data: 0.0002  max mem: 4132
[19:55:40.348502] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.5361 (0.5521)  acc1: 82.8125 (83.6479)  acc5: 100.0000 (99.0254)  time: 0.0289  data: 0.0002  max mem: 4132
[19:55:40.640909] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5513 (0.5523)  acc1: 82.8125 (83.6149)  acc5: 100.0000 (99.0850)  time: 0.0291  data: 0.0002  max mem: 4132
[19:55:40.931996] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5512 (0.5510)  acc1: 82.8125 (83.6002)  acc5: 100.0000 (99.0832)  time: 0.0290  data: 0.0003  max mem: 4132
[19:55:41.221282] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5585 (0.5514)  acc1: 82.8125 (83.5639)  acc5: 100.0000 (99.1174)  time: 0.0289  data: 0.0003  max mem: 4132
[19:55:41.509255] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5544 (0.5495)  acc1: 84.3750 (83.6325)  acc5: 100.0000 (99.1689)  time: 0.0287  data: 0.0003  max mem: 4132
[19:55:41.790716] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5421 (0.5484)  acc1: 82.8125 (83.5575)  acc5: 100.0000 (99.1618)  time: 0.0283  data: 0.0003  max mem: 4132
[19:55:41.946566] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5395 (0.5489)  acc1: 81.2500 (83.5200)  acc5: 100.0000 (99.1700)  time: 0.0273  data: 0.0001  max mem: 4132
[19:55:42.119898] Test: Total time: 0:00:05 (0.0342 s / it)
[19:55:42.121012] * Acc@1 83.520 Acc@5 99.170 loss 0.549
[19:55:42.121305] Accuracy of the network on the 10000 test images: 83.5%
[19:55:42.121478] Max accuracy: 83.52%
[19:55:42.382566] log_dir: ./output_dir
[19:55:43.267213] Epoch: [64]  [  0/781]  eta: 0:11:29  lr: 0.000079  training_loss: 1.1353 (1.1353)  classification_loss: 1.1344 (1.1344)  loss_mask: 0.0009 (0.0009)  time: 0.8827  data: 0.6976  max mem: 4132
[19:55:46.576219] Epoch: [64]  [ 20/781]  eta: 0:02:31  lr: 0.000079  training_loss: 1.2550 (1.2405)  classification_loss: 1.2548 (1.2402)  loss_mask: 0.0003 (0.0003)  time: 0.1653  data: 0.0002  max mem: 4132
[19:55:49.836627] Epoch: [64]  [ 40/781]  eta: 0:02:14  lr: 0.000079  training_loss: 1.2281 (1.2461)  classification_loss: 1.2279 (1.2457)  loss_mask: 0.0003 (0.0003)  time: 0.1629  data: 0.0002  max mem: 4132
[19:55:53.124317] Epoch: [64]  [ 60/781]  eta: 0:02:06  lr: 0.000079  training_loss: 1.2581 (1.2569)  classification_loss: 1.2578 (1.2566)  loss_mask: 0.0003 (0.0003)  time: 0.1643  data: 0.0002  max mem: 4132
[19:55:56.403938] Epoch: [64]  [ 80/781]  eta: 0:02:01  lr: 0.000079  training_loss: 1.2350 (1.2468)  classification_loss: 1.2346 (1.2465)  loss_mask: 0.0003 (0.0003)  time: 0.1639  data: 0.0003  max mem: 4132
[19:55:59.664524] Epoch: [64]  [100/781]  eta: 0:01:56  lr: 0.000079  training_loss: 1.2556 (1.2529)  classification_loss: 1.2553 (1.2526)  loss_mask: 0.0003 (0.0003)  time: 0.1629  data: 0.0002  max mem: 4132
[19:56:02.990019] Epoch: [64]  [120/781]  eta: 0:01:52  lr: 0.000079  training_loss: 1.2373 (1.2507)  classification_loss: 1.2370 (1.2504)  loss_mask: 0.0003 (0.0003)  time: 0.1662  data: 0.0003  max mem: 4132
[19:56:06.288160] Epoch: [64]  [140/781]  eta: 0:01:48  lr: 0.000079  training_loss: 1.2796 (1.2538)  classification_loss: 1.2794 (1.2534)  loss_mask: 0.0003 (0.0003)  time: 0.1648  data: 0.0003  max mem: 4132
[19:56:09.603789] Epoch: [64]  [160/781]  eta: 0:01:44  lr: 0.000079  training_loss: 1.2421 (1.2547)  classification_loss: 1.2418 (1.2544)  loss_mask: 0.0002 (0.0003)  time: 0.1657  data: 0.0003  max mem: 4132
[19:56:12.943306] Epoch: [64]  [180/781]  eta: 0:01:41  lr: 0.000078  training_loss: 1.2186 (1.2535)  classification_loss: 1.2180 (1.2532)  loss_mask: 0.0003 (0.0003)  time: 0.1669  data: 0.0003  max mem: 4132
[19:56:16.241089] Epoch: [64]  [200/781]  eta: 0:01:37  lr: 0.000078  training_loss: 1.2913 (1.2556)  classification_loss: 1.2910 (1.2553)  loss_mask: 0.0003 (0.0003)  time: 0.1647  data: 0.0003  max mem: 4132
[19:56:19.547291] Epoch: [64]  [220/781]  eta: 0:01:34  lr: 0.000078  training_loss: 1.2742 (1.2569)  classification_loss: 1.2740 (1.2566)  loss_mask: 0.0003 (0.0003)  time: 0.1652  data: 0.0003  max mem: 4132
[19:56:22.883824] Epoch: [64]  [240/781]  eta: 0:01:30  lr: 0.000078  training_loss: 1.2405 (1.2568)  classification_loss: 1.2401 (1.2565)  loss_mask: 0.0002 (0.0003)  time: 0.1667  data: 0.0003  max mem: 4132
[19:56:26.222839] Epoch: [64]  [260/781]  eta: 0:01:27  lr: 0.000078  training_loss: 1.2592 (1.2578)  classification_loss: 1.2589 (1.2575)  loss_mask: 0.0002 (0.0003)  time: 0.1669  data: 0.0003  max mem: 4132
[19:56:29.527797] Epoch: [64]  [280/781]  eta: 0:01:24  lr: 0.000078  training_loss: 1.2412 (1.2557)  classification_loss: 1.2410 (1.2554)  loss_mask: 0.0002 (0.0003)  time: 0.1651  data: 0.0003  max mem: 4132
[19:56:32.833448] Epoch: [64]  [300/781]  eta: 0:01:20  lr: 0.000078  training_loss: 1.2957 (1.2581)  classification_loss: 1.2954 (1.2578)  loss_mask: 0.0002 (0.0003)  time: 0.1652  data: 0.0003  max mem: 4132
[19:56:36.133145] Epoch: [64]  [320/781]  eta: 0:01:17  lr: 0.000078  training_loss: 1.2564 (1.2588)  classification_loss: 1.2562 (1.2585)  loss_mask: 0.0002 (0.0003)  time: 0.1649  data: 0.0003  max mem: 4132
[19:56:39.437577] Epoch: [64]  [340/781]  eta: 0:01:13  lr: 0.000078  training_loss: 1.1723 (1.2562)  classification_loss: 1.1716 (1.2559)  loss_mask: 0.0002 (0.0003)  time: 0.1651  data: 0.0002  max mem: 4132
[19:56:42.768969] Epoch: [64]  [360/781]  eta: 0:01:10  lr: 0.000078  training_loss: 1.2574 (1.2578)  classification_loss: 1.2572 (1.2575)  loss_mask: 0.0002 (0.0003)  time: 0.1665  data: 0.0003  max mem: 4132
[19:56:46.075936] Epoch: [64]  [380/781]  eta: 0:01:06  lr: 0.000077  training_loss: 1.3106 (1.2604)  classification_loss: 1.3104 (1.2601)  loss_mask: 0.0003 (0.0003)  time: 0.1653  data: 0.0002  max mem: 4132
[19:56:49.387637] Epoch: [64]  [400/781]  eta: 0:01:03  lr: 0.000077  training_loss: 1.2388 (1.2601)  classification_loss: 1.2387 (1.2598)  loss_mask: 0.0002 (0.0003)  time: 0.1655  data: 0.0002  max mem: 4132
[19:56:52.702616] Epoch: [64]  [420/781]  eta: 0:01:00  lr: 0.000077  training_loss: 1.2698 (1.2617)  classification_loss: 1.2697 (1.2614)  loss_mask: 0.0002 (0.0003)  time: 0.1657  data: 0.0002  max mem: 4132
[19:56:55.993890] Epoch: [64]  [440/781]  eta: 0:00:56  lr: 0.000077  training_loss: 1.2475 (1.2621)  classification_loss: 1.2472 (1.2618)  loss_mask: 0.0002 (0.0003)  time: 0.1645  data: 0.0003  max mem: 4132
[19:56:59.319522] Epoch: [64]  [460/781]  eta: 0:00:53  lr: 0.000077  training_loss: 1.2385 (1.2610)  classification_loss: 1.2384 (1.2607)  loss_mask: 0.0002 (0.0003)  time: 0.1662  data: 0.0003  max mem: 4132
[19:57:02.590770] Epoch: [64]  [480/781]  eta: 0:00:50  lr: 0.000077  training_loss: 1.2068 (1.2596)  classification_loss: 1.2066 (1.2593)  loss_mask: 0.0002 (0.0003)  time: 0.1634  data: 0.0002  max mem: 4132
[19:57:05.864520] Epoch: [64]  [500/781]  eta: 0:00:46  lr: 0.000077  training_loss: 1.3088 (1.2615)  classification_loss: 1.3086 (1.2612)  loss_mask: 0.0002 (0.0003)  time: 0.1636  data: 0.0003  max mem: 4132
[19:57:09.124119] Epoch: [64]  [520/781]  eta: 0:00:43  lr: 0.000077  training_loss: 1.2448 (1.2608)  classification_loss: 1.2443 (1.2606)  loss_mask: 0.0002 (0.0003)  time: 0.1629  data: 0.0003  max mem: 4132
[19:57:12.414922] Epoch: [64]  [540/781]  eta: 0:00:40  lr: 0.000077  training_loss: 1.2778 (1.2610)  classification_loss: 1.2775 (1.2607)  loss_mask: 0.0002 (0.0003)  time: 0.1644  data: 0.0003  max mem: 4132
[19:57:15.705178] Epoch: [64]  [560/781]  eta: 0:00:36  lr: 0.000077  training_loss: 1.2139 (1.2605)  classification_loss: 1.2138 (1.2602)  loss_mask: 0.0002 (0.0003)  time: 0.1642  data: 0.0003  max mem: 4132
[19:57:19.026782] Epoch: [64]  [580/781]  eta: 0:00:33  lr: 0.000076  training_loss: 1.2618 (1.2616)  classification_loss: 1.2617 (1.2614)  loss_mask: 0.0002 (0.0003)  time: 0.1660  data: 0.0003  max mem: 4132
[19:57:22.345333] Epoch: [64]  [600/781]  eta: 0:00:30  lr: 0.000076  training_loss: 1.2664 (1.2627)  classification_loss: 1.2662 (1.2624)  loss_mask: 0.0002 (0.0003)  time: 0.1658  data: 0.0003  max mem: 4132
[19:57:25.642309] Epoch: [64]  [620/781]  eta: 0:00:26  lr: 0.000076  training_loss: 1.2706 (1.2627)  classification_loss: 1.2706 (1.2625)  loss_mask: 0.0002 (0.0003)  time: 0.1647  data: 0.0003  max mem: 4132
[19:57:28.941238] Epoch: [64]  [640/781]  eta: 0:00:23  lr: 0.000076  training_loss: 1.2053 (1.2618)  classification_loss: 1.2051 (1.2616)  loss_mask: 0.0002 (0.0003)  time: 0.1648  data: 0.0003  max mem: 4132
[19:57:32.257756] Epoch: [64]  [660/781]  eta: 0:00:20  lr: 0.000076  training_loss: 1.2584 (1.2620)  classification_loss: 1.2583 (1.2618)  loss_mask: 0.0002 (0.0003)  time: 0.1657  data: 0.0003  max mem: 4132
[19:57:35.546114] Epoch: [64]  [680/781]  eta: 0:00:16  lr: 0.000076  training_loss: 1.2765 (1.2629)  classification_loss: 1.2763 (1.2626)  loss_mask: 0.0002 (0.0003)  time: 0.1643  data: 0.0003  max mem: 4132
[19:57:38.839208] Epoch: [64]  [700/781]  eta: 0:00:13  lr: 0.000076  training_loss: 1.3154 (1.2643)  classification_loss: 1.3153 (1.2641)  loss_mask: 0.0001 (0.0003)  time: 0.1646  data: 0.0003  max mem: 4132
[19:57:42.144774] Epoch: [64]  [720/781]  eta: 0:00:10  lr: 0.000076  training_loss: 1.2848 (1.2655)  classification_loss: 1.2847 (1.2653)  loss_mask: 0.0001 (0.0003)  time: 0.1652  data: 0.0003  max mem: 4132
[19:57:45.454737] Epoch: [64]  [740/781]  eta: 0:00:06  lr: 0.000076  training_loss: 1.1861 (1.2645)  classification_loss: 1.1859 (1.2642)  loss_mask: 0.0001 (0.0003)  time: 0.1654  data: 0.0002  max mem: 4132
[19:57:48.739493] Epoch: [64]  [760/781]  eta: 0:00:03  lr: 0.000076  training_loss: 1.2819 (1.2650)  classification_loss: 1.2817 (1.2648)  loss_mask: 0.0002 (0.0002)  time: 0.1642  data: 0.0002  max mem: 4132
[19:57:52.007335] Epoch: [64]  [780/781]  eta: 0:00:00  lr: 0.000075  training_loss: 1.2598 (1.2647)  classification_loss: 1.2596 (1.2644)  loss_mask: 0.0002 (0.0003)  time: 0.1633  data: 0.0002  max mem: 4132
[19:57:52.186370] Epoch: [64] Total time: 0:02:09 (0.1662 s / it)
[19:57:52.186843] Averaged stats: lr: 0.000075  training_loss: 1.2598 (1.2647)  classification_loss: 1.2596 (1.2644)  loss_mask: 0.0002 (0.0003)
[19:57:52.931994] Test:  [  0/157]  eta: 0:01:55  testing_loss: 0.4752 (0.4752)  acc1: 85.9375 (85.9375)  acc5: 96.8750 (96.8750)  time: 0.7388  data: 0.6860  max mem: 4132
[19:57:53.232252] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.5421 (0.5620)  acc1: 84.3750 (83.2386)  acc5: 98.4375 (98.7216)  time: 0.0942  data: 0.0626  max mem: 4132
[19:57:53.524266] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5152 (0.5378)  acc1: 84.3750 (83.5565)  acc5: 100.0000 (99.0327)  time: 0.0294  data: 0.0003  max mem: 4132
[19:57:53.810616] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5276 (0.5653)  acc1: 84.3750 (83.2661)  acc5: 100.0000 (98.7903)  time: 0.0288  data: 0.0003  max mem: 4132
[19:57:54.099688] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5530 (0.5658)  acc1: 82.8125 (83.1555)  acc5: 98.4375 (98.8186)  time: 0.0286  data: 0.0002  max mem: 4132
[19:57:54.395863] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5487 (0.5587)  acc1: 82.8125 (83.4252)  acc5: 98.4375 (98.9277)  time: 0.0290  data: 0.0002  max mem: 4132
[19:57:54.688674] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5359 (0.5550)  acc1: 84.3750 (83.4016)  acc5: 100.0000 (99.0523)  time: 0.0292  data: 0.0002  max mem: 4132
[19:57:54.978938] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5043 (0.5504)  acc1: 84.3750 (83.5827)  acc5: 100.0000 (99.1197)  time: 0.0290  data: 0.0003  max mem: 4132
[19:57:55.276721] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5530 (0.5582)  acc1: 82.8125 (83.1211)  acc5: 100.0000 (99.0548)  time: 0.0293  data: 0.0003  max mem: 4132
[19:57:55.564870] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5869 (0.5566)  acc1: 79.6875 (83.1044)  acc5: 98.4375 (98.9870)  time: 0.0292  data: 0.0002  max mem: 4132
[19:57:55.857854] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.5869 (0.5620)  acc1: 81.2500 (82.8434)  acc5: 98.4375 (98.9480)  time: 0.0289  data: 0.0002  max mem: 4132
[19:57:56.148662] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6083 (0.5625)  acc1: 81.2500 (82.8125)  acc5: 98.4375 (98.9724)  time: 0.0290  data: 0.0003  max mem: 4132
[19:57:56.444398] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5346 (0.5613)  acc1: 81.2500 (82.7738)  acc5: 100.0000 (98.9669)  time: 0.0291  data: 0.0003  max mem: 4132
[19:57:56.742754] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5535 (0.5620)  acc1: 81.2500 (82.7052)  acc5: 98.4375 (98.9623)  time: 0.0294  data: 0.0002  max mem: 4132
[19:57:57.037701] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5382 (0.5595)  acc1: 81.2500 (82.7571)  acc5: 100.0000 (99.0248)  time: 0.0294  data: 0.0003  max mem: 4132
[19:57:57.321219] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5382 (0.5574)  acc1: 81.2500 (82.7918)  acc5: 100.0000 (99.0273)  time: 0.0287  data: 0.0003  max mem: 4132
[19:57:57.475472] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5277 (0.5570)  acc1: 84.3750 (82.7600)  acc5: 100.0000 (99.0500)  time: 0.0274  data: 0.0002  max mem: 4132
[19:57:57.665146] Test: Total time: 0:00:05 (0.0349 s / it)
[19:57:57.665683] * Acc@1 82.760 Acc@5 99.050 loss 0.557
[19:57:57.666039] Accuracy of the network on the 10000 test images: 82.8%
[19:57:57.666289] Max accuracy: 83.52%
[19:57:57.860928] log_dir: ./output_dir
[19:57:58.853135] Epoch: [65]  [  0/781]  eta: 0:12:53  lr: 0.000075  training_loss: 1.1787 (1.1787)  classification_loss: 1.1784 (1.1784)  loss_mask: 0.0003 (0.0003)  time: 0.9898  data: 0.7782  max mem: 4132
[19:58:02.150069] Epoch: [65]  [ 20/781]  eta: 0:02:35  lr: 0.000075  training_loss: 1.2368 (1.2378)  classification_loss: 1.2367 (1.2376)  loss_mask: 0.0002 (0.0002)  time: 0.1647  data: 0.0003  max mem: 4132
[19:58:05.419057] Epoch: [65]  [ 40/781]  eta: 0:02:16  lr: 0.000075  training_loss: 1.2744 (1.2570)  classification_loss: 1.2742 (1.2568)  loss_mask: 0.0002 (0.0002)  time: 0.1634  data: 0.0003  max mem: 4132
[19:58:08.728712] Epoch: [65]  [ 60/781]  eta: 0:02:08  lr: 0.000075  training_loss: 1.3371 (1.2889)  classification_loss: 1.2607 (1.2665)  loss_mask: 0.0380 (0.0224)  time: 0.1654  data: 0.0003  max mem: 4132
[19:58:12.008087] Epoch: [65]  [ 80/781]  eta: 0:02:02  lr: 0.000075  training_loss: 1.2726 (1.2925)  classification_loss: 1.2563 (1.2677)  loss_mask: 0.0243 (0.0248)  time: 0.1639  data: 0.0003  max mem: 4132
[19:58:15.305547] Epoch: [65]  [100/781]  eta: 0:01:57  lr: 0.000075  training_loss: 1.2915 (1.2957)  classification_loss: 1.2506 (1.2701)  loss_mask: 0.0100 (0.0256)  time: 0.1648  data: 0.0003  max mem: 4132
[19:58:18.583291] Epoch: [65]  [120/781]  eta: 0:01:53  lr: 0.000075  training_loss: 1.2956 (1.2915)  classification_loss: 1.2804 (1.2671)  loss_mask: 0.0093 (0.0243)  time: 0.1638  data: 0.0003  max mem: 4132
[19:58:21.848395] Epoch: [65]  [140/781]  eta: 0:01:48  lr: 0.000075  training_loss: 1.2301 (1.2830)  classification_loss: 1.2193 (1.2608)  loss_mask: 0.0035 (0.0221)  time: 0.1632  data: 0.0004  max mem: 4132
[19:58:25.105615] Epoch: [65]  [160/781]  eta: 0:01:45  lr: 0.000075  training_loss: 1.2745 (1.2796)  classification_loss: 1.2707 (1.2599)  loss_mask: 0.0020 (0.0197)  time: 0.1628  data: 0.0002  max mem: 4132
[19:58:28.373855] Epoch: [65]  [180/781]  eta: 0:01:41  lr: 0.000075  training_loss: 1.2207 (1.2743)  classification_loss: 1.2192 (1.2566)  loss_mask: 0.0007 (0.0177)  time: 0.1633  data: 0.0002  max mem: 4132
[19:58:31.643823] Epoch: [65]  [200/781]  eta: 0:01:37  lr: 0.000075  training_loss: 1.2067 (1.2715)  classification_loss: 1.2014 (1.2545)  loss_mask: 0.0007 (0.0170)  time: 0.1634  data: 0.0005  max mem: 4132
[19:58:34.887010] Epoch: [65]  [220/781]  eta: 0:01:33  lr: 0.000074  training_loss: 1.2486 (1.2715)  classification_loss: 1.2483 (1.2557)  loss_mask: 0.0013 (0.0157)  time: 0.1620  data: 0.0002  max mem: 4132
[19:58:38.162173] Epoch: [65]  [240/781]  eta: 0:01:30  lr: 0.000074  training_loss: 1.2106 (1.2696)  classification_loss: 1.2104 (1.2551)  loss_mask: 0.0008 (0.0145)  time: 0.1637  data: 0.0002  max mem: 4132
[19:58:41.471569] Epoch: [65]  [260/781]  eta: 0:01:26  lr: 0.000074  training_loss: 1.3157 (1.2724)  classification_loss: 1.3140 (1.2589)  loss_mask: 0.0011 (0.0135)  time: 0.1654  data: 0.0002  max mem: 4132
[19:58:44.759715] Epoch: [65]  [280/781]  eta: 0:01:23  lr: 0.000074  training_loss: 1.2087 (1.2692)  classification_loss: 1.2082 (1.2566)  loss_mask: 0.0005 (0.0126)  time: 0.1643  data: 0.0003  max mem: 4132
[19:58:48.080666] Epoch: [65]  [300/781]  eta: 0:01:20  lr: 0.000074  training_loss: 1.2923 (1.2708)  classification_loss: 1.2920 (1.2590)  loss_mask: 0.0004 (0.0118)  time: 0.1659  data: 0.0003  max mem: 4132
[19:58:51.364844] Epoch: [65]  [320/781]  eta: 0:01:16  lr: 0.000074  training_loss: 1.2388 (1.2697)  classification_loss: 1.2382 (1.2586)  loss_mask: 0.0003 (0.0111)  time: 0.1641  data: 0.0003  max mem: 4132
[19:58:54.653202] Epoch: [65]  [340/781]  eta: 0:01:13  lr: 0.000074  training_loss: 1.2159 (1.2672)  classification_loss: 1.2157 (1.2568)  loss_mask: 0.0003 (0.0105)  time: 0.1643  data: 0.0003  max mem: 4132
[19:58:57.934277] Epoch: [65]  [360/781]  eta: 0:01:10  lr: 0.000074  training_loss: 1.2671 (1.2677)  classification_loss: 1.2670 (1.2578)  loss_mask: 0.0003 (0.0099)  time: 0.1640  data: 0.0003  max mem: 4132
[19:59:01.253778] Epoch: [65]  [380/781]  eta: 0:01:06  lr: 0.000074  training_loss: 1.2170 (1.2656)  classification_loss: 1.2166 (1.2562)  loss_mask: 0.0003 (0.0094)  time: 0.1659  data: 0.0003  max mem: 4132
[19:59:04.538761] Epoch: [65]  [400/781]  eta: 0:01:03  lr: 0.000074  training_loss: 1.2418 (1.2644)  classification_loss: 1.2412 (1.2555)  loss_mask: 0.0003 (0.0090)  time: 0.1641  data: 0.0003  max mem: 4132
[19:59:07.856011] Epoch: [65]  [420/781]  eta: 0:00:59  lr: 0.000073  training_loss: 1.2582 (1.2646)  classification_loss: 1.2580 (1.2560)  loss_mask: 0.0002 (0.0085)  time: 0.1658  data: 0.0003  max mem: 4132
[19:59:11.149516] Epoch: [65]  [440/781]  eta: 0:00:56  lr: 0.000073  training_loss: 1.2865 (1.2660)  classification_loss: 1.2862 (1.2578)  loss_mask: 0.0003 (0.0082)  time: 0.1646  data: 0.0002  max mem: 4132
[19:59:14.455190] Epoch: [65]  [460/781]  eta: 0:00:53  lr: 0.000073  training_loss: 1.1941 (1.2631)  classification_loss: 1.1939 (1.2553)  loss_mask: 0.0002 (0.0078)  time: 0.1652  data: 0.0004  max mem: 4132
[19:59:17.741298] Epoch: [65]  [480/781]  eta: 0:00:49  lr: 0.000073  training_loss: 1.2069 (1.2622)  classification_loss: 1.2067 (1.2546)  loss_mask: 0.0002 (0.0075)  time: 0.1642  data: 0.0003  max mem: 4132
[19:59:21.013601] Epoch: [65]  [500/781]  eta: 0:00:46  lr: 0.000073  training_loss: 1.2115 (1.2613)  classification_loss: 1.2113 (1.2541)  loss_mask: 0.0002 (0.0072)  time: 0.1635  data: 0.0003  max mem: 4132
[19:59:24.351741] Epoch: [65]  [520/781]  eta: 0:00:43  lr: 0.000073  training_loss: 1.2471 (1.2614)  classification_loss: 1.2468 (1.2545)  loss_mask: 0.0002 (0.0070)  time: 0.1668  data: 0.0003  max mem: 4132
[19:59:27.668975] Epoch: [65]  [540/781]  eta: 0:00:39  lr: 0.000073  training_loss: 1.2662 (1.2620)  classification_loss: 1.2660 (1.2553)  loss_mask: 0.0002 (0.0067)  time: 0.1658  data: 0.0003  max mem: 4132
[19:59:31.028930] Epoch: [65]  [560/781]  eta: 0:00:36  lr: 0.000073  training_loss: 1.2313 (1.2611)  classification_loss: 1.2311 (1.2546)  loss_mask: 0.0002 (0.0065)  time: 0.1679  data: 0.0002  max mem: 4132
[19:59:34.325847] Epoch: [65]  [580/781]  eta: 0:00:33  lr: 0.000073  training_loss: 1.2475 (1.2610)  classification_loss: 1.2473 (1.2547)  loss_mask: 0.0002 (0.0063)  time: 0.1648  data: 0.0003  max mem: 4132
[19:59:37.648303] Epoch: [65]  [600/781]  eta: 0:00:30  lr: 0.000073  training_loss: 1.2390 (1.2614)  classification_loss: 1.2388 (1.2553)  loss_mask: 0.0001 (0.0061)  time: 0.1660  data: 0.0002  max mem: 4132
[19:59:40.923583] Epoch: [65]  [620/781]  eta: 0:00:26  lr: 0.000073  training_loss: 1.2879 (1.2619)  classification_loss: 1.2877 (1.2561)  loss_mask: 0.0002 (0.0059)  time: 0.1637  data: 0.0003  max mem: 4132
[19:59:44.205608] Epoch: [65]  [640/781]  eta: 0:00:23  lr: 0.000072  training_loss: 1.2717 (1.2623)  classification_loss: 1.2716 (1.2566)  loss_mask: 0.0002 (0.0057)  time: 0.1640  data: 0.0003  max mem: 4132
[19:59:47.501134] Epoch: [65]  [660/781]  eta: 0:00:20  lr: 0.000072  training_loss: 1.3108 (1.2633)  classification_loss: 1.3106 (1.2578)  loss_mask: 0.0002 (0.0055)  time: 0.1647  data: 0.0002  max mem: 4132
[19:59:50.789295] Epoch: [65]  [680/781]  eta: 0:00:16  lr: 0.000072  training_loss: 1.2523 (1.2638)  classification_loss: 1.2521 (1.2585)  loss_mask: 0.0001 (0.0054)  time: 0.1643  data: 0.0003  max mem: 4132
[19:59:54.079226] Epoch: [65]  [700/781]  eta: 0:00:13  lr: 0.000072  training_loss: 1.2952 (1.2646)  classification_loss: 1.2950 (1.2594)  loss_mask: 0.0002 (0.0052)  time: 0.1644  data: 0.0003  max mem: 4132
[19:59:57.369265] Epoch: [65]  [720/781]  eta: 0:00:10  lr: 0.000072  training_loss: 1.2807 (1.2657)  classification_loss: 1.2593 (1.2600)  loss_mask: 0.0002 (0.0057)  time: 0.1644  data: 0.0003  max mem: 4132
[20:00:00.694526] Epoch: [65]  [740/781]  eta: 0:00:06  lr: 0.000072  training_loss: 1.2643 (1.2660)  classification_loss: 1.2428 (1.2595)  loss_mask: 0.0107 (0.0065)  time: 0.1662  data: 0.0003  max mem: 4132
[20:00:04.032188] Epoch: [65]  [760/781]  eta: 0:00:03  lr: 0.000072  training_loss: 1.2835 (1.2676)  classification_loss: 1.2390 (1.2598)  loss_mask: 0.0231 (0.0077)  time: 0.1668  data: 0.0003  max mem: 4132
[20:00:07.299737] Epoch: [65]  [780/781]  eta: 0:00:00  lr: 0.000072  training_loss: 1.2739 (1.2675)  classification_loss: 1.2529 (1.2593)  loss_mask: 0.0123 (0.0083)  time: 0.1633  data: 0.0002  max mem: 4132
[20:00:07.476128] Epoch: [65] Total time: 0:02:09 (0.1660 s / it)
[20:00:07.476892] Averaged stats: lr: 0.000072  training_loss: 1.2739 (1.2675)  classification_loss: 1.2529 (1.2593)  loss_mask: 0.0123 (0.0083)
[20:00:08.216018] Test:  [  0/157]  eta: 0:01:55  testing_loss: 0.5272 (0.5272)  acc1: 79.6875 (79.6875)  acc5: 98.4375 (98.4375)  time: 0.7344  data: 0.7048  max mem: 4132
[20:00:08.513587] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.5576 (0.5846)  acc1: 79.6875 (80.9659)  acc5: 100.0000 (99.2898)  time: 0.0936  data: 0.0649  max mem: 4132
[20:00:08.803239] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5387 (0.5454)  acc1: 82.8125 (83.4077)  acc5: 100.0000 (99.4792)  time: 0.0292  data: 0.0006  max mem: 4132
[20:00:09.094759] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5084 (0.5612)  acc1: 84.3750 (83.1149)  acc5: 100.0000 (99.2440)  time: 0.0289  data: 0.0002  max mem: 4132
[20:00:09.386248] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5435 (0.5567)  acc1: 82.8125 (83.3460)  acc5: 98.4375 (99.1997)  time: 0.0290  data: 0.0002  max mem: 4132
[20:00:09.678233] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5314 (0.5520)  acc1: 85.9375 (83.8848)  acc5: 100.0000 (99.2647)  time: 0.0290  data: 0.0002  max mem: 4132
[20:00:09.968209] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5131 (0.5468)  acc1: 84.3750 (83.9139)  acc5: 100.0000 (99.2828)  time: 0.0289  data: 0.0002  max mem: 4132
[20:00:10.256732] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5131 (0.5429)  acc1: 84.3750 (83.9789)  acc5: 100.0000 (99.2518)  time: 0.0287  data: 0.0002  max mem: 4132
[20:00:10.553078] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5409 (0.5508)  acc1: 82.8125 (83.6034)  acc5: 100.0000 (99.2670)  time: 0.0291  data: 0.0002  max mem: 4132
[20:00:10.839297] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5700 (0.5499)  acc1: 82.8125 (83.5337)  acc5: 100.0000 (99.2445)  time: 0.0290  data: 0.0002  max mem: 4132
[20:00:11.127237] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.5409 (0.5531)  acc1: 82.8125 (83.3385)  acc5: 100.0000 (99.2265)  time: 0.0285  data: 0.0002  max mem: 4132
[20:00:11.417469] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5760 (0.5535)  acc1: 82.8125 (83.3474)  acc5: 100.0000 (99.2117)  time: 0.0287  data: 0.0002  max mem: 4132
[20:00:11.705806] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5513 (0.5534)  acc1: 84.3750 (83.2386)  acc5: 100.0000 (99.2252)  time: 0.0287  data: 0.0002  max mem: 4132
[20:00:11.993586] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5513 (0.5519)  acc1: 84.3750 (83.2180)  acc5: 100.0000 (99.2724)  time: 0.0286  data: 0.0002  max mem: 4132
[20:00:12.284528] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5412 (0.5513)  acc1: 82.8125 (83.1893)  acc5: 100.0000 (99.3019)  time: 0.0288  data: 0.0002  max mem: 4132
[20:00:12.568387] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5456 (0.5499)  acc1: 82.8125 (83.1022)  acc5: 100.0000 (99.2964)  time: 0.0286  data: 0.0002  max mem: 4132
[20:00:12.722640] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5259 (0.5499)  acc1: 82.8125 (83.0900)  acc5: 100.0000 (99.3100)  time: 0.0274  data: 0.0002  max mem: 4132
[20:00:12.883289] Test: Total time: 0:00:05 (0.0344 s / it)
[20:00:12.883783] * Acc@1 83.090 Acc@5 99.310 loss 0.550
[20:00:12.884118] Accuracy of the network on the 10000 test images: 83.1%
[20:00:12.884398] Max accuracy: 83.52%
[20:00:13.061396] log_dir: ./output_dir
[20:00:14.068049] Epoch: [66]  [  0/781]  eta: 0:13:04  lr: 0.000072  training_loss: 1.1905 (1.1905)  classification_loss: 1.1575 (1.1575)  loss_mask: 0.0330 (0.0330)  time: 1.0047  data: 0.7788  max mem: 4132
[20:00:17.382470] Epoch: [66]  [ 20/781]  eta: 0:02:36  lr: 0.000072  training_loss: 1.2504 (1.2512)  classification_loss: 1.2177 (1.2170)  loss_mask: 0.0147 (0.0342)  time: 0.1656  data: 0.0003  max mem: 4132
[20:00:20.680262] Epoch: [66]  [ 40/781]  eta: 0:02:17  lr: 0.000072  training_loss: 1.2692 (1.2880)  classification_loss: 1.2368 (1.2470)  loss_mask: 0.0181 (0.0411)  time: 0.1648  data: 0.0002  max mem: 4132
[20:00:23.985115] Epoch: [66]  [ 60/781]  eta: 0:02:09  lr: 0.000071  training_loss: 1.2948 (1.2952)  classification_loss: 1.2745 (1.2582)  loss_mask: 0.0176 (0.0370)  time: 0.1652  data: 0.0002  max mem: 4132
[20:00:27.267019] Epoch: [66]  [ 80/781]  eta: 0:02:02  lr: 0.000071  training_loss: 1.3011 (1.2919)  classification_loss: 1.2836 (1.2618)  loss_mask: 0.0057 (0.0301)  time: 0.1640  data: 0.0003  max mem: 4132
[20:00:30.556710] Epoch: [66]  [100/781]  eta: 0:01:57  lr: 0.000071  training_loss: 1.2208 (1.2813)  classification_loss: 1.2190 (1.2568)  loss_mask: 0.0015 (0.0245)  time: 0.1643  data: 0.0003  max mem: 4132
[20:00:33.890606] Epoch: [66]  [120/781]  eta: 0:01:53  lr: 0.000071  training_loss: 1.2113 (1.2743)  classification_loss: 1.2102 (1.2536)  loss_mask: 0.0013 (0.0207)  time: 0.1666  data: 0.0003  max mem: 4132
[20:00:37.193939] Epoch: [66]  [140/781]  eta: 0:01:49  lr: 0.000071  training_loss: 1.1850 (1.2697)  classification_loss: 1.1832 (1.2518)  loss_mask: 0.0010 (0.0179)  time: 0.1651  data: 0.0003  max mem: 4132
[20:00:40.510276] Epoch: [66]  [160/781]  eta: 0:01:45  lr: 0.000071  training_loss: 1.2436 (1.2672)  classification_loss: 1.2431 (1.2514)  loss_mask: 0.0009 (0.0158)  time: 0.1657  data: 0.0003  max mem: 4132
[20:00:43.819988] Epoch: [66]  [180/781]  eta: 0:01:42  lr: 0.000071  training_loss: 1.2223 (1.2649)  classification_loss: 1.2215 (1.2507)  loss_mask: 0.0007 (0.0142)  time: 0.1654  data: 0.0003  max mem: 4132
[20:00:47.154431] Epoch: [66]  [200/781]  eta: 0:01:38  lr: 0.000071  training_loss: 1.2842 (1.2638)  classification_loss: 1.2837 (1.2510)  loss_mask: 0.0006 (0.0128)  time: 0.1666  data: 0.0003  max mem: 4132
[20:00:50.464235] Epoch: [66]  [220/781]  eta: 0:01:34  lr: 0.000071  training_loss: 1.2309 (1.2627)  classification_loss: 1.2304 (1.2509)  loss_mask: 0.0006 (0.0117)  time: 0.1654  data: 0.0003  max mem: 4132
[20:00:53.795148] Epoch: [66]  [240/781]  eta: 0:01:31  lr: 0.000071  training_loss: 1.2646 (1.2630)  classification_loss: 1.2641 (1.2522)  loss_mask: 0.0005 (0.0108)  time: 0.1665  data: 0.0003  max mem: 4132
[20:00:57.073567] Epoch: [66]  [260/781]  eta: 0:01:27  lr: 0.000071  training_loss: 1.2415 (1.2617)  classification_loss: 1.2412 (1.2516)  loss_mask: 0.0005 (0.0100)  time: 0.1638  data: 0.0002  max mem: 4132
[20:01:00.346488] Epoch: [66]  [280/781]  eta: 0:01:24  lr: 0.000070  training_loss: 1.2605 (1.2613)  classification_loss: 1.2599 (1.2520)  loss_mask: 0.0004 (0.0093)  time: 0.1636  data: 0.0002  max mem: 4132
[20:01:03.668892] Epoch: [66]  [300/781]  eta: 0:01:20  lr: 0.000070  training_loss: 1.3284 (1.2645)  classification_loss: 1.3281 (1.2558)  loss_mask: 0.0004 (0.0087)  time: 0.1661  data: 0.0003  max mem: 4132
[20:01:06.944123] Epoch: [66]  [320/781]  eta: 0:01:17  lr: 0.000070  training_loss: 1.2413 (1.2637)  classification_loss: 1.2412 (1.2555)  loss_mask: 0.0003 (0.0082)  time: 0.1637  data: 0.0002  max mem: 4132
[20:01:10.254003] Epoch: [66]  [340/781]  eta: 0:01:13  lr: 0.000070  training_loss: 1.1894 (1.2626)  classification_loss: 1.1890 (1.2548)  loss_mask: 0.0004 (0.0078)  time: 0.1654  data: 0.0003  max mem: 4132
[20:01:13.562060] Epoch: [66]  [360/781]  eta: 0:01:10  lr: 0.000070  training_loss: 1.2516 (1.2615)  classification_loss: 1.2512 (1.2542)  loss_mask: 0.0003 (0.0074)  time: 0.1652  data: 0.0003  max mem: 4132
[20:01:16.858144] Epoch: [66]  [380/781]  eta: 0:01:07  lr: 0.000070  training_loss: 1.1775 (1.2578)  classification_loss: 1.1773 (1.2508)  loss_mask: 0.0003 (0.0070)  time: 0.1647  data: 0.0003  max mem: 4132
[20:01:20.202010] Epoch: [66]  [400/781]  eta: 0:01:03  lr: 0.000070  training_loss: 1.2759 (1.2587)  classification_loss: 1.2753 (1.2520)  loss_mask: 0.0003 (0.0067)  time: 0.1671  data: 0.0005  max mem: 4132
[20:01:23.567001] Epoch: [66]  [420/781]  eta: 0:01:00  lr: 0.000070  training_loss: 1.1963 (1.2576)  classification_loss: 1.1961 (1.2512)  loss_mask: 0.0003 (0.0064)  time: 0.1682  data: 0.0005  max mem: 4132
[20:01:26.890627] Epoch: [66]  [440/781]  eta: 0:00:57  lr: 0.000070  training_loss: 1.2529 (1.2572)  classification_loss: 1.2527 (1.2511)  loss_mask: 0.0003 (0.0061)  time: 0.1661  data: 0.0003  max mem: 4132
[20:01:30.198334] Epoch: [66]  [460/781]  eta: 0:00:53  lr: 0.000070  training_loss: 1.2447 (1.2565)  classification_loss: 1.2444 (1.2506)  loss_mask: 0.0002 (0.0058)  time: 0.1653  data: 0.0003  max mem: 4132
[20:01:33.505805] Epoch: [66]  [480/781]  eta: 0:00:50  lr: 0.000069  training_loss: 1.2210 (1.2561)  classification_loss: 1.2207 (1.2505)  loss_mask: 0.0002 (0.0056)  time: 0.1653  data: 0.0003  max mem: 4132
[20:01:36.855454] Epoch: [66]  [500/781]  eta: 0:00:46  lr: 0.000069  training_loss: 1.3024 (1.2576)  classification_loss: 1.3022 (1.2523)  loss_mask: 0.0003 (0.0054)  time: 0.1673  data: 0.0003  max mem: 4132
[20:01:40.189755] Epoch: [66]  [520/781]  eta: 0:00:43  lr: 0.000069  training_loss: 1.2494 (1.2578)  classification_loss: 1.2491 (1.2526)  loss_mask: 0.0003 (0.0052)  time: 0.1666  data: 0.0003  max mem: 4132
[20:01:43.532944] Epoch: [66]  [540/781]  eta: 0:00:40  lr: 0.000069  training_loss: 1.2941 (1.2596)  classification_loss: 1.2938 (1.2546)  loss_mask: 0.0002 (0.0050)  time: 0.1671  data: 0.0003  max mem: 4132
[20:01:46.847360] Epoch: [66]  [560/781]  eta: 0:00:36  lr: 0.000069  training_loss: 1.2249 (1.2589)  classification_loss: 1.2248 (1.2541)  loss_mask: 0.0003 (0.0048)  time: 0.1656  data: 0.0003  max mem: 4132
[20:01:50.133550] Epoch: [66]  [580/781]  eta: 0:00:33  lr: 0.000069  training_loss: 1.2339 (1.2584)  classification_loss: 1.2336 (1.2537)  loss_mask: 0.0002 (0.0047)  time: 0.1642  data: 0.0002  max mem: 4132
[20:01:53.476678] Epoch: [66]  [600/781]  eta: 0:00:30  lr: 0.000069  training_loss: 1.2402 (1.2585)  classification_loss: 1.2398 (1.2540)  loss_mask: 0.0002 (0.0045)  time: 0.1671  data: 0.0004  max mem: 4132
[20:01:56.802254] Epoch: [66]  [620/781]  eta: 0:00:26  lr: 0.000069  training_loss: 1.2503 (1.2582)  classification_loss: 1.2501 (1.2538)  loss_mask: 0.0002 (0.0044)  time: 0.1662  data: 0.0003  max mem: 4132
[20:02:00.078412] Epoch: [66]  [640/781]  eta: 0:00:23  lr: 0.000069  training_loss: 1.2294 (1.2577)  classification_loss: 1.2293 (1.2535)  loss_mask: 0.0002 (0.0043)  time: 0.1637  data: 0.0003  max mem: 4132
[20:02:03.379928] Epoch: [66]  [660/781]  eta: 0:00:20  lr: 0.000069  training_loss: 1.2849 (1.2586)  classification_loss: 1.2847 (1.2544)  loss_mask: 0.0002 (0.0041)  time: 0.1650  data: 0.0003  max mem: 4132
[20:02:06.664362] Epoch: [66]  [680/781]  eta: 0:00:16  lr: 0.000069  training_loss: 1.1794 (1.2576)  classification_loss: 1.1793 (1.2535)  loss_mask: 0.0002 (0.0041)  time: 0.1641  data: 0.0002  max mem: 4132
[20:02:09.967436] Epoch: [66]  [700/781]  eta: 0:00:13  lr: 0.000068  training_loss: 1.2800 (1.2585)  classification_loss: 1.2716 (1.2540)  loss_mask: 0.0030 (0.0045)  time: 0.1651  data: 0.0003  max mem: 4132
[20:02:13.234111] Epoch: [66]  [720/781]  eta: 0:00:10  lr: 0.000068  training_loss: 1.2869 (1.2610)  classification_loss: 1.2607 (1.2546)  loss_mask: 0.0509 (0.0064)  time: 0.1632  data: 0.0002  max mem: 4132
[20:02:16.515661] Epoch: [66]  [740/781]  eta: 0:00:06  lr: 0.000068  training_loss: 1.2412 (1.2610)  classification_loss: 1.2161 (1.2541)  loss_mask: 0.0118 (0.0069)  time: 0.1640  data: 0.0003  max mem: 4132
[20:02:19.796633] Epoch: [66]  [760/781]  eta: 0:00:03  lr: 0.000068  training_loss: 1.3157 (1.2631)  classification_loss: 1.3117 (1.2560)  loss_mask: 0.0041 (0.0071)  time: 0.1639  data: 0.0002  max mem: 4132
[20:02:23.069710] Epoch: [66]  [780/781]  eta: 0:00:00  lr: 0.000068  training_loss: 1.2679 (1.2629)  classification_loss: 1.2632 (1.2559)  loss_mask: 0.0018 (0.0069)  time: 0.1636  data: 0.0002  max mem: 4132
[20:02:23.249314] Epoch: [66] Total time: 0:02:10 (0.1667 s / it)
[20:02:23.250094] Averaged stats: lr: 0.000068  training_loss: 1.2679 (1.2629)  classification_loss: 1.2632 (1.2559)  loss_mask: 0.0018 (0.0069)
[20:02:23.955749] Test:  [  0/157]  eta: 0:01:49  testing_loss: 0.5397 (0.5397)  acc1: 82.8125 (82.8125)  acc5: 96.8750 (96.8750)  time: 0.6991  data: 0.6617  max mem: 4132
[20:02:24.246248] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.5612 (0.5596)  acc1: 81.2500 (82.2443)  acc5: 98.4375 (99.0057)  time: 0.0897  data: 0.0604  max mem: 4132
[20:02:24.535286] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4985 (0.5149)  acc1: 82.8125 (84.1518)  acc5: 98.4375 (99.1071)  time: 0.0288  data: 0.0003  max mem: 4132
[20:02:24.820468] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4926 (0.5388)  acc1: 84.3750 (83.4677)  acc5: 98.4375 (99.0423)  time: 0.0286  data: 0.0003  max mem: 4132
[20:02:25.107185] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5308 (0.5384)  acc1: 84.3750 (83.7652)  acc5: 100.0000 (99.1235)  time: 0.0284  data: 0.0003  max mem: 4132
[20:02:25.393513] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5145 (0.5334)  acc1: 84.3750 (83.8542)  acc5: 100.0000 (99.1422)  time: 0.0285  data: 0.0003  max mem: 4132
[20:02:25.679844] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5039 (0.5329)  acc1: 84.3750 (83.8883)  acc5: 100.0000 (99.1803)  time: 0.0285  data: 0.0002  max mem: 4132
[20:02:25.969267] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5039 (0.5284)  acc1: 84.3750 (84.0669)  acc5: 98.4375 (99.1197)  time: 0.0286  data: 0.0002  max mem: 4132
[20:02:26.258850] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5284 (0.5365)  acc1: 82.8125 (83.8735)  acc5: 98.4375 (99.0741)  time: 0.0287  data: 0.0002  max mem: 4132
[20:02:26.545626] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5572 (0.5341)  acc1: 82.8125 (83.9973)  acc5: 98.4375 (99.0556)  time: 0.0286  data: 0.0002  max mem: 4132
[20:02:26.832258] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.5414 (0.5383)  acc1: 82.8125 (83.7717)  acc5: 100.0000 (99.0563)  time: 0.0285  data: 0.0002  max mem: 4132
[20:02:27.120033] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5433 (0.5373)  acc1: 81.2500 (83.7838)  acc5: 100.0000 (99.0850)  time: 0.0286  data: 0.0002  max mem: 4132
[20:02:27.421348] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5226 (0.5341)  acc1: 82.8125 (83.8456)  acc5: 100.0000 (99.0832)  time: 0.0293  data: 0.0002  max mem: 4132
[20:02:27.712248] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4838 (0.5336)  acc1: 82.8125 (83.8740)  acc5: 100.0000 (99.1412)  time: 0.0294  data: 0.0002  max mem: 4132
[20:02:28.000450] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5113 (0.5319)  acc1: 84.3750 (83.9650)  acc5: 100.0000 (99.1689)  time: 0.0287  data: 0.0002  max mem: 4132
[20:02:28.287461] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5138 (0.5303)  acc1: 84.3750 (83.9921)  acc5: 100.0000 (99.1618)  time: 0.0286  data: 0.0002  max mem: 4132
[20:02:28.442569] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5138 (0.5301)  acc1: 84.3750 (83.9800)  acc5: 100.0000 (99.1600)  time: 0.0276  data: 0.0002  max mem: 4132
[20:02:28.623467] Test: Total time: 0:00:05 (0.0342 s / it)
[20:02:28.623958] * Acc@1 83.980 Acc@5 99.160 loss 0.530
[20:02:28.624269] Accuracy of the network on the 10000 test images: 84.0%
[20:02:28.624459] Max accuracy: 83.98%
[20:02:28.834599] log_dir: ./output_dir
[20:02:29.784068] Epoch: [67]  [  0/781]  eta: 0:12:19  lr: 0.000068  training_loss: 1.2022 (1.2022)  classification_loss: 1.1994 (1.1994)  loss_mask: 0.0029 (0.0029)  time: 0.9472  data: 0.7544  max mem: 4132
[20:02:33.079222] Epoch: [67]  [ 20/781]  eta: 0:02:33  lr: 0.000068  training_loss: 1.2216 (1.2433)  classification_loss: 1.2196 (1.2422)  loss_mask: 0.0010 (0.0012)  time: 0.1647  data: 0.0003  max mem: 4132
[20:02:36.358329] Epoch: [67]  [ 40/781]  eta: 0:02:15  lr: 0.000068  training_loss: 1.2389 (1.2480)  classification_loss: 1.2379 (1.2469)  loss_mask: 0.0009 (0.0010)  time: 0.1638  data: 0.0003  max mem: 4132
[20:02:39.683901] Epoch: [67]  [ 60/781]  eta: 0:02:08  lr: 0.000068  training_loss: 1.2347 (1.2535)  classification_loss: 1.2342 (1.2526)  loss_mask: 0.0007 (0.0010)  time: 0.1662  data: 0.0003  max mem: 4132
[20:02:42.994596] Epoch: [67]  [ 80/781]  eta: 0:02:02  lr: 0.000068  training_loss: 1.1934 (1.2428)  classification_loss: 1.1926 (1.2419)  loss_mask: 0.0006 (0.0009)  time: 0.1653  data: 0.0003  max mem: 4132
[20:02:46.310316] Epoch: [67]  [100/781]  eta: 0:01:57  lr: 0.000068  training_loss: 1.2016 (1.2426)  classification_loss: 1.2008 (1.2418)  loss_mask: 0.0005 (0.0008)  time: 0.1657  data: 0.0003  max mem: 4132
[20:02:49.606560] Epoch: [67]  [120/781]  eta: 0:01:53  lr: 0.000068  training_loss: 1.2326 (1.2424)  classification_loss: 1.2325 (1.2417)  loss_mask: 0.0004 (0.0007)  time: 0.1647  data: 0.0003  max mem: 4132
[20:02:52.914533] Epoch: [67]  [140/781]  eta: 0:01:49  lr: 0.000067  training_loss: 1.2348 (1.2440)  classification_loss: 1.2343 (1.2433)  loss_mask: 0.0004 (0.0007)  time: 0.1653  data: 0.0002  max mem: 4132
[20:02:56.234179] Epoch: [67]  [160/781]  eta: 0:01:45  lr: 0.000067  training_loss: 1.2180 (1.2419)  classification_loss: 1.2174 (1.2413)  loss_mask: 0.0005 (0.0007)  time: 0.1659  data: 0.0003  max mem: 4132
[20:02:59.510989] Epoch: [67]  [180/781]  eta: 0:01:41  lr: 0.000067  training_loss: 1.2445 (1.2430)  classification_loss: 1.2442 (1.2424)  loss_mask: 0.0004 (0.0006)  time: 0.1638  data: 0.0002  max mem: 4132
[20:03:02.844302] Epoch: [67]  [200/781]  eta: 0:01:38  lr: 0.000067  training_loss: 1.2498 (1.2417)  classification_loss: 1.2494 (1.2410)  loss_mask: 0.0004 (0.0006)  time: 0.1666  data: 0.0005  max mem: 4132
[20:03:06.168210] Epoch: [67]  [220/781]  eta: 0:01:34  lr: 0.000067  training_loss: 1.2799 (1.2440)  classification_loss: 1.2796 (1.2434)  loss_mask: 0.0004 (0.0006)  time: 0.1661  data: 0.0003  max mem: 4132
[20:03:09.461372] Epoch: [67]  [240/781]  eta: 0:01:31  lr: 0.000067  training_loss: 1.2576 (1.2457)  classification_loss: 1.2572 (1.2451)  loss_mask: 0.0003 (0.0006)  time: 0.1645  data: 0.0003  max mem: 4132
[20:03:12.785162] Epoch: [67]  [260/781]  eta: 0:01:27  lr: 0.000067  training_loss: 1.2251 (1.2465)  classification_loss: 1.2246 (1.2459)  loss_mask: 0.0004 (0.0006)  time: 0.1661  data: 0.0003  max mem: 4132
[20:03:16.072595] Epoch: [67]  [280/781]  eta: 0:01:24  lr: 0.000067  training_loss: 1.2170 (1.2463)  classification_loss: 1.2167 (1.2458)  loss_mask: 0.0003 (0.0005)  time: 0.1643  data: 0.0003  max mem: 4132
[20:03:19.364286] Epoch: [67]  [300/781]  eta: 0:01:20  lr: 0.000067  training_loss: 1.2629 (1.2478)  classification_loss: 1.2627 (1.2473)  loss_mask: 0.0003 (0.0005)  time: 0.1645  data: 0.0003  max mem: 4132
[20:03:22.694324] Epoch: [67]  [320/781]  eta: 0:01:17  lr: 0.000067  training_loss: 1.2020 (1.2468)  classification_loss: 1.2016 (1.2462)  loss_mask: 0.0003 (0.0005)  time: 0.1663  data: 0.0003  max mem: 4132
[20:03:25.973167] Epoch: [67]  [340/781]  eta: 0:01:13  lr: 0.000066  training_loss: 1.2326 (1.2468)  classification_loss: 1.2324 (1.2463)  loss_mask: 0.0002 (0.0005)  time: 0.1639  data: 0.0002  max mem: 4132
[20:03:29.299463] Epoch: [67]  [360/781]  eta: 0:01:10  lr: 0.000066  training_loss: 1.1570 (1.2431)  classification_loss: 1.1566 (1.2426)  loss_mask: 0.0003 (0.0005)  time: 0.1662  data: 0.0002  max mem: 4132
[20:03:32.598271] Epoch: [67]  [380/781]  eta: 0:01:07  lr: 0.000066  training_loss: 1.2085 (1.2430)  classification_loss: 1.2081 (1.2425)  loss_mask: 0.0002 (0.0005)  time: 0.1648  data: 0.0003  max mem: 4132
[20:03:35.902150] Epoch: [67]  [400/781]  eta: 0:01:03  lr: 0.000066  training_loss: 1.2389 (1.2441)  classification_loss: 1.2387 (1.2436)  loss_mask: 0.0002 (0.0005)  time: 0.1651  data: 0.0003  max mem: 4132
[20:03:39.221574] Epoch: [67]  [420/781]  eta: 0:01:00  lr: 0.000066  training_loss: 1.2644 (1.2433)  classification_loss: 1.2643 (1.2428)  loss_mask: 0.0003 (0.0005)  time: 0.1659  data: 0.0003  max mem: 4132
[20:03:42.516931] Epoch: [67]  [440/781]  eta: 0:00:56  lr: 0.000066  training_loss: 1.2257 (1.2428)  classification_loss: 1.2254 (1.2423)  loss_mask: 0.0003 (0.0005)  time: 0.1647  data: 0.0002  max mem: 4132
[20:03:45.805543] Epoch: [67]  [460/781]  eta: 0:00:53  lr: 0.000066  training_loss: 1.2091 (1.2417)  classification_loss: 1.2088 (1.2412)  loss_mask: 0.0003 (0.0004)  time: 0.1643  data: 0.0003  max mem: 4132
[20:03:49.134225] Epoch: [67]  [480/781]  eta: 0:00:50  lr: 0.000066  training_loss: 1.2638 (1.2425)  classification_loss: 1.2634 (1.2420)  loss_mask: 0.0002 (0.0004)  time: 0.1663  data: 0.0002  max mem: 4132
[20:03:52.430841] Epoch: [67]  [500/781]  eta: 0:00:46  lr: 0.000066  training_loss: 1.2356 (1.2423)  classification_loss: 1.2353 (1.2419)  loss_mask: 0.0002 (0.0004)  time: 0.1647  data: 0.0003  max mem: 4132
[20:03:55.745879] Epoch: [67]  [520/781]  eta: 0:00:43  lr: 0.000066  training_loss: 1.2137 (1.2421)  classification_loss: 1.2135 (1.2416)  loss_mask: 0.0002 (0.0004)  time: 0.1657  data: 0.0004  max mem: 4132
[20:03:59.029267] Epoch: [67]  [540/781]  eta: 0:00:40  lr: 0.000066  training_loss: 1.2688 (1.2433)  classification_loss: 1.2686 (1.2429)  loss_mask: 0.0002 (0.0004)  time: 0.1641  data: 0.0002  max mem: 4132
[20:04:02.335968] Epoch: [67]  [560/781]  eta: 0:00:36  lr: 0.000065  training_loss: 1.1937 (1.2426)  classification_loss: 1.1936 (1.2422)  loss_mask: 0.0002 (0.0004)  time: 0.1652  data: 0.0005  max mem: 4132
[20:04:05.635898] Epoch: [67]  [580/781]  eta: 0:00:33  lr: 0.000065  training_loss: 1.2519 (1.2425)  classification_loss: 1.2517 (1.2421)  loss_mask: 0.0002 (0.0004)  time: 0.1649  data: 0.0003  max mem: 4132
[20:04:08.949256] Epoch: [67]  [600/781]  eta: 0:00:30  lr: 0.000065  training_loss: 1.2022 (1.2421)  classification_loss: 1.2021 (1.2417)  loss_mask: 0.0002 (0.0004)  time: 0.1656  data: 0.0002  max mem: 4132
[20:04:12.238300] Epoch: [67]  [620/781]  eta: 0:00:26  lr: 0.000065  training_loss: 1.2371 (1.2426)  classification_loss: 1.2370 (1.2422)  loss_mask: 0.0001 (0.0004)  time: 0.1644  data: 0.0003  max mem: 4132
[20:04:15.532028] Epoch: [67]  [640/781]  eta: 0:00:23  lr: 0.000065  training_loss: 1.2298 (1.2431)  classification_loss: 1.2296 (1.2427)  loss_mask: 0.0001 (0.0004)  time: 0.1646  data: 0.0003  max mem: 4132
[20:04:18.855913] Epoch: [67]  [660/781]  eta: 0:00:20  lr: 0.000065  training_loss: 1.2553 (1.2435)  classification_loss: 1.2551 (1.2431)  loss_mask: 0.0002 (0.0004)  time: 0.1661  data: 0.0004  max mem: 4132
[20:04:22.144865] Epoch: [67]  [680/781]  eta: 0:00:16  lr: 0.000065  training_loss: 1.2341 (1.2433)  classification_loss: 1.2340 (1.2429)  loss_mask: 0.0002 (0.0004)  time: 0.1644  data: 0.0003  max mem: 4132
[20:04:25.444556] Epoch: [67]  [700/781]  eta: 0:00:13  lr: 0.000065  training_loss: 1.2647 (1.2445)  classification_loss: 1.2646 (1.2442)  loss_mask: 0.0002 (0.0004)  time: 0.1649  data: 0.0003  max mem: 4132
[20:04:28.739873] Epoch: [67]  [720/781]  eta: 0:00:10  lr: 0.000065  training_loss: 1.2334 (1.2444)  classification_loss: 1.2332 (1.2440)  loss_mask: 0.0002 (0.0004)  time: 0.1646  data: 0.0003  max mem: 4132
[20:04:32.038864] Epoch: [67]  [740/781]  eta: 0:00:06  lr: 0.000065  training_loss: 1.2176 (1.2442)  classification_loss: 1.2174 (1.2439)  loss_mask: 0.0001 (0.0004)  time: 0.1649  data: 0.0003  max mem: 4132
[20:04:35.305228] Epoch: [67]  [760/781]  eta: 0:00:03  lr: 0.000065  training_loss: 1.2648 (1.2450)  classification_loss: 1.2645 (1.2446)  loss_mask: 0.0002 (0.0004)  time: 0.1632  data: 0.0004  max mem: 4132
[20:04:38.573765] Epoch: [67]  [780/781]  eta: 0:00:00  lr: 0.000064  training_loss: 1.2656 (1.2456)  classification_loss: 1.2655 (1.2453)  loss_mask: 0.0002 (0.0003)  time: 0.1633  data: 0.0002  max mem: 4132
[20:04:38.739234] Epoch: [67] Total time: 0:02:09 (0.1663 s / it)
[20:04:38.739765] Averaged stats: lr: 0.000064  training_loss: 1.2656 (1.2456)  classification_loss: 1.2655 (1.2453)  loss_mask: 0.0002 (0.0003)
[20:04:39.460660] Test:  [  0/157]  eta: 0:01:52  testing_loss: 0.5381 (0.5381)  acc1: 79.6875 (79.6875)  acc5: 95.3125 (95.3125)  time: 0.7170  data: 0.6873  max mem: 4132
[20:04:39.745752] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.5429 (0.5528)  acc1: 81.2500 (82.2443)  acc5: 98.4375 (98.8636)  time: 0.0909  data: 0.0627  max mem: 4132
[20:04:40.031362] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5144 (0.5099)  acc1: 82.8125 (83.8542)  acc5: 100.0000 (99.2560)  time: 0.0284  data: 0.0002  max mem: 4132
[20:04:40.320395] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5085 (0.5347)  acc1: 84.3750 (83.2661)  acc5: 100.0000 (99.1431)  time: 0.0286  data: 0.0002  max mem: 4132
[20:04:40.607647] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5337 (0.5348)  acc1: 84.3750 (83.4604)  acc5: 98.4375 (99.0854)  time: 0.0287  data: 0.0002  max mem: 4132
[20:04:40.900815] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5039 (0.5305)  acc1: 84.3750 (84.0074)  acc5: 98.4375 (99.0809)  time: 0.0289  data: 0.0002  max mem: 4132
[20:04:41.188421] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5008 (0.5291)  acc1: 85.9375 (84.0164)  acc5: 100.0000 (99.1547)  time: 0.0289  data: 0.0002  max mem: 4132
[20:04:41.478339] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5167 (0.5247)  acc1: 84.3750 (84.0669)  acc5: 100.0000 (99.2077)  time: 0.0287  data: 0.0002  max mem: 4132
[20:04:41.768118] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5305 (0.5328)  acc1: 84.3750 (83.8542)  acc5: 100.0000 (99.1512)  time: 0.0288  data: 0.0002  max mem: 4132
[20:04:42.061636] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5666 (0.5290)  acc1: 81.2500 (83.9457)  acc5: 98.4375 (99.1243)  time: 0.0290  data: 0.0003  max mem: 4132
[20:04:42.354655] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.5246 (0.5312)  acc1: 81.2500 (83.8335)  acc5: 98.4375 (99.0873)  time: 0.0292  data: 0.0004  max mem: 4132
[20:04:42.644372] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5357 (0.5309)  acc1: 81.2500 (83.7979)  acc5: 98.4375 (99.0991)  time: 0.0290  data: 0.0002  max mem: 4132
[20:04:42.938125] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5092 (0.5279)  acc1: 82.8125 (83.9747)  acc5: 100.0000 (99.1090)  time: 0.0290  data: 0.0002  max mem: 4132
[20:04:43.227383] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5092 (0.5278)  acc1: 84.3750 (83.9456)  acc5: 100.0000 (99.1174)  time: 0.0290  data: 0.0003  max mem: 4132
[20:04:43.517291] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5224 (0.5261)  acc1: 82.8125 (84.0426)  acc5: 100.0000 (99.1467)  time: 0.0288  data: 0.0002  max mem: 4132
[20:04:43.801501] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5200 (0.5255)  acc1: 82.8125 (83.9921)  acc5: 100.0000 (99.1618)  time: 0.0286  data: 0.0002  max mem: 4132
[20:04:43.956195] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4993 (0.5244)  acc1: 84.3750 (84.0100)  acc5: 100.0000 (99.1800)  time: 0.0275  data: 0.0002  max mem: 4132
[20:04:44.122936] Test: Total time: 0:00:05 (0.0343 s / it)
[20:04:44.123450] * Acc@1 84.010 Acc@5 99.180 loss 0.524
[20:04:44.123856] Accuracy of the network on the 10000 test images: 84.0%
[20:04:44.124084] Max accuracy: 84.01%
[20:04:44.290501] log_dir: ./output_dir
[20:04:45.190181] Epoch: [68]  [  0/781]  eta: 0:11:41  lr: 0.000064  training_loss: 1.1443 (1.1443)  classification_loss: 1.1441 (1.1441)  loss_mask: 0.0002 (0.0002)  time: 0.8978  data: 0.7261  max mem: 4132
[20:04:48.489941] Epoch: [68]  [ 20/781]  eta: 0:02:32  lr: 0.000064  training_loss: 1.2760 (1.2645)  classification_loss: 1.2758 (1.2633)  loss_mask: 0.0003 (0.0013)  time: 0.1649  data: 0.0002  max mem: 4132
[20:04:51.783173] Epoch: [68]  [ 40/781]  eta: 0:02:15  lr: 0.000064  training_loss: 1.2362 (1.2548)  classification_loss: 1.2347 (1.2537)  loss_mask: 0.0008 (0.0011)  time: 0.1646  data: 0.0002  max mem: 4132
[20:04:55.082147] Epoch: [68]  [ 60/781]  eta: 0:02:07  lr: 0.000064  training_loss: 1.2599 (1.2623)  classification_loss: 1.2596 (1.2614)  loss_mask: 0.0004 (0.0009)  time: 0.1649  data: 0.0002  max mem: 4132
[20:04:58.392680] Epoch: [68]  [ 80/781]  eta: 0:02:01  lr: 0.000064  training_loss: 1.2159 (1.2562)  classification_loss: 1.2155 (1.2555)  loss_mask: 0.0003 (0.0007)  time: 0.1654  data: 0.0002  max mem: 4132
[20:05:01.718262] Epoch: [68]  [100/781]  eta: 0:01:57  lr: 0.000064  training_loss: 1.2317 (1.2549)  classification_loss: 1.2315 (1.2543)  loss_mask: 0.0003 (0.0007)  time: 0.1662  data: 0.0004  max mem: 4132
[20:05:05.035022] Epoch: [68]  [120/781]  eta: 0:01:53  lr: 0.000064  training_loss: 1.2292 (1.2533)  classification_loss: 1.2290 (1.2527)  loss_mask: 0.0002 (0.0006)  time: 0.1657  data: 0.0002  max mem: 4132
[20:05:08.383946] Epoch: [68]  [140/781]  eta: 0:01:49  lr: 0.000064  training_loss: 1.2311 (1.2520)  classification_loss: 1.2307 (1.2515)  loss_mask: 0.0002 (0.0005)  time: 0.1673  data: 0.0003  max mem: 4132
[20:05:11.710014] Epoch: [68]  [160/781]  eta: 0:01:45  lr: 0.000064  training_loss: 1.2861 (1.2542)  classification_loss: 1.2857 (1.2537)  loss_mask: 0.0003 (0.0005)  time: 0.1662  data: 0.0002  max mem: 4132
[20:05:15.038558] Epoch: [68]  [180/781]  eta: 0:01:42  lr: 0.000064  training_loss: 1.2572 (1.2549)  classification_loss: 1.2571 (1.2544)  loss_mask: 0.0002 (0.0005)  time: 0.1663  data: 0.0003  max mem: 4132
[20:05:18.345063] Epoch: [68]  [200/781]  eta: 0:01:38  lr: 0.000064  training_loss: 1.2276 (1.2537)  classification_loss: 1.2274 (1.2532)  loss_mask: 0.0002 (0.0005)  time: 0.1652  data: 0.0004  max mem: 4132
[20:05:21.654468] Epoch: [68]  [220/781]  eta: 0:01:34  lr: 0.000063  training_loss: 1.2446 (1.2527)  classification_loss: 1.2444 (1.2523)  loss_mask: 0.0002 (0.0004)  time: 0.1654  data: 0.0003  max mem: 4132
[20:05:24.953598] Epoch: [68]  [240/781]  eta: 0:01:31  lr: 0.000063  training_loss: 1.2314 (1.2519)  classification_loss: 1.2312 (1.2514)  loss_mask: 0.0002 (0.0004)  time: 0.1649  data: 0.0002  max mem: 4132
[20:05:28.261482] Epoch: [68]  [260/781]  eta: 0:01:27  lr: 0.000063  training_loss: 1.2107 (1.2516)  classification_loss: 1.2104 (1.2512)  loss_mask: 0.0002 (0.0004)  time: 0.1653  data: 0.0002  max mem: 4132
[20:05:31.538975] Epoch: [68]  [280/781]  eta: 0:01:24  lr: 0.000063  training_loss: 1.2625 (1.2525)  classification_loss: 1.2623 (1.2521)  loss_mask: 0.0002 (0.0004)  time: 0.1638  data: 0.0002  max mem: 4132
[20:05:34.828929] Epoch: [68]  [300/781]  eta: 0:01:20  lr: 0.000063  training_loss: 1.2160 (1.2504)  classification_loss: 1.2158 (1.2501)  loss_mask: 0.0002 (0.0004)  time: 0.1644  data: 0.0003  max mem: 4132
[20:05:38.102664] Epoch: [68]  [320/781]  eta: 0:01:17  lr: 0.000063  training_loss: 1.1996 (1.2495)  classification_loss: 1.1995 (1.2491)  loss_mask: 0.0002 (0.0004)  time: 0.1636  data: 0.0002  max mem: 4132
[20:05:41.407836] Epoch: [68]  [340/781]  eta: 0:01:13  lr: 0.000063  training_loss: 1.2106 (1.2477)  classification_loss: 1.2104 (1.2474)  loss_mask: 0.0001 (0.0003)  time: 0.1652  data: 0.0003  max mem: 4132
[20:05:44.737046] Epoch: [68]  [360/781]  eta: 0:01:10  lr: 0.000063  training_loss: 1.2845 (1.2496)  classification_loss: 1.2843 (1.2493)  loss_mask: 0.0001 (0.0003)  time: 0.1664  data: 0.0003  max mem: 4132
[20:05:48.068160] Epoch: [68]  [380/781]  eta: 0:01:07  lr: 0.000063  training_loss: 1.2284 (1.2492)  classification_loss: 1.2282 (1.2489)  loss_mask: 0.0001 (0.0003)  time: 0.1664  data: 0.0004  max mem: 4132
[20:05:51.415334] Epoch: [68]  [400/781]  eta: 0:01:03  lr: 0.000063  training_loss: 1.2525 (1.2496)  classification_loss: 1.2523 (1.2493)  loss_mask: 0.0002 (0.0003)  time: 0.1673  data: 0.0002  max mem: 4132
[20:05:54.688149] Epoch: [68]  [420/781]  eta: 0:01:00  lr: 0.000063  training_loss: 1.3071 (1.2517)  classification_loss: 1.3069 (1.2514)  loss_mask: 0.0001 (0.0003)  time: 0.1636  data: 0.0002  max mem: 4132
[20:05:57.956651] Epoch: [68]  [440/781]  eta: 0:00:56  lr: 0.000062  training_loss: 1.2490 (1.2515)  classification_loss: 1.2489 (1.2512)  loss_mask: 0.0001 (0.0003)  time: 0.1633  data: 0.0002  max mem: 4132
[20:06:01.202734] Epoch: [68]  [460/781]  eta: 0:00:53  lr: 0.000062  training_loss: 1.2063 (1.2505)  classification_loss: 1.2062 (1.2502)  loss_mask: 0.0001 (0.0003)  time: 0.1622  data: 0.0002  max mem: 4132
[20:06:04.486010] Epoch: [68]  [480/781]  eta: 0:00:50  lr: 0.000062  training_loss: 1.2236 (1.2496)  classification_loss: 1.2236 (1.2494)  loss_mask: 0.0001 (0.0003)  time: 0.1641  data: 0.0003  max mem: 4132
[20:06:07.792839] Epoch: [68]  [500/781]  eta: 0:00:46  lr: 0.000062  training_loss: 1.2154 (1.2487)  classification_loss: 1.2149 (1.2484)  loss_mask: 0.0002 (0.0003)  time: 0.1653  data: 0.0003  max mem: 4132
[20:06:11.095666] Epoch: [68]  [520/781]  eta: 0:00:43  lr: 0.000062  training_loss: 1.2225 (1.2476)  classification_loss: 1.2225 (1.2473)  loss_mask: 0.0001 (0.0003)  time: 0.1651  data: 0.0003  max mem: 4132
[20:06:14.373459] Epoch: [68]  [540/781]  eta: 0:00:40  lr: 0.000062  training_loss: 1.2322 (1.2482)  classification_loss: 1.2321 (1.2479)  loss_mask: 0.0001 (0.0003)  time: 0.1638  data: 0.0003  max mem: 4132
[20:06:17.661449] Epoch: [68]  [560/781]  eta: 0:00:36  lr: 0.000062  training_loss: 1.2564 (1.2479)  classification_loss: 1.2563 (1.2476)  loss_mask: 0.0001 (0.0003)  time: 0.1643  data: 0.0003  max mem: 4132
[20:06:20.973396] Epoch: [68]  [580/781]  eta: 0:00:33  lr: 0.000062  training_loss: 1.2109 (1.2471)  classification_loss: 1.2108 (1.2469)  loss_mask: 0.0001 (0.0003)  time: 0.1655  data: 0.0003  max mem: 4132
[20:06:24.292880] Epoch: [68]  [600/781]  eta: 0:00:30  lr: 0.000062  training_loss: 1.2442 (1.2478)  classification_loss: 1.2440 (1.2475)  loss_mask: 0.0001 (0.0003)  time: 0.1658  data: 0.0003  max mem: 4132
[20:06:27.569861] Epoch: [68]  [620/781]  eta: 0:00:26  lr: 0.000062  training_loss: 1.2261 (1.2474)  classification_loss: 1.2259 (1.2472)  loss_mask: 0.0001 (0.0003)  time: 0.1638  data: 0.0002  max mem: 4132
[20:06:30.868181] Epoch: [68]  [640/781]  eta: 0:00:23  lr: 0.000062  training_loss: 1.2179 (1.2469)  classification_loss: 1.2177 (1.2466)  loss_mask: 0.0001 (0.0002)  time: 0.1648  data: 0.0002  max mem: 4132
[20:06:34.175426] Epoch: [68]  [660/781]  eta: 0:00:20  lr: 0.000061  training_loss: 1.2593 (1.2474)  classification_loss: 1.2593 (1.2472)  loss_mask: 0.0001 (0.0002)  time: 0.1653  data: 0.0003  max mem: 4132
[20:06:37.516937] Epoch: [68]  [680/781]  eta: 0:00:16  lr: 0.000061  training_loss: 1.2330 (1.2475)  classification_loss: 1.2330 (1.2473)  loss_mask: 0.0001 (0.0002)  time: 0.1670  data: 0.0003  max mem: 4132
[20:06:40.886601] Epoch: [68]  [700/781]  eta: 0:00:13  lr: 0.000061  training_loss: 1.2362 (1.2477)  classification_loss: 1.2362 (1.2475)  loss_mask: 0.0001 (0.0003)  time: 0.1684  data: 0.0003  max mem: 4132
[20:06:44.237474] Epoch: [68]  [720/781]  eta: 0:00:10  lr: 0.000061  training_loss: 1.2229 (1.2480)  classification_loss: 1.2138 (1.2477)  loss_mask: 0.0006 (0.0003)  time: 0.1674  data: 0.0003  max mem: 4132
[20:06:47.555374] Epoch: [68]  [740/781]  eta: 0:00:06  lr: 0.000061  training_loss: 1.2474 (1.2484)  classification_loss: 1.2100 (1.2474)  loss_mask: 0.0015 (0.0009)  time: 0.1658  data: 0.0003  max mem: 4132
[20:06:50.860156] Epoch: [68]  [760/781]  eta: 0:00:03  lr: 0.000061  training_loss: 1.2512 (1.2496)  classification_loss: 1.2433 (1.2483)  loss_mask: 0.0072 (0.0013)  time: 0.1652  data: 0.0003  max mem: 4132
[20:06:54.180330] Epoch: [68]  [780/781]  eta: 0:00:00  lr: 0.000061  training_loss: 1.3045 (1.2509)  classification_loss: 1.2287 (1.2484)  loss_mask: 0.0286 (0.0024)  time: 0.1659  data: 0.0003  max mem: 4132
[20:06:54.392856] Epoch: [68] Total time: 0:02:10 (0.1666 s / it)
[20:06:54.393408] Averaged stats: lr: 0.000061  training_loss: 1.3045 (1.2509)  classification_loss: 1.2287 (1.2484)  loss_mask: 0.0286 (0.0024)
[20:06:55.167260] Test:  [  0/157]  eta: 0:02:00  testing_loss: 0.5356 (0.5356)  acc1: 84.3750 (84.3750)  acc5: 96.8750 (96.8750)  time: 0.7693  data: 0.7193  max mem: 4132
[20:06:55.467163] Test:  [ 10/157]  eta: 0:00:14  testing_loss: 0.5444 (0.5538)  acc1: 82.8125 (82.9545)  acc5: 98.4375 (98.8636)  time: 0.0970  data: 0.0659  max mem: 4132
[20:06:55.765317] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5066 (0.5088)  acc1: 82.8125 (84.5982)  acc5: 100.0000 (99.1815)  time: 0.0296  data: 0.0007  max mem: 4132
[20:06:56.057914] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5088 (0.5312)  acc1: 82.8125 (83.8710)  acc5: 100.0000 (99.1935)  time: 0.0293  data: 0.0005  max mem: 4132
[20:06:56.344963] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5348 (0.5353)  acc1: 82.8125 (83.8034)  acc5: 98.4375 (99.1616)  time: 0.0288  data: 0.0002  max mem: 4132
[20:06:56.633181] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5257 (0.5329)  acc1: 84.3750 (84.0074)  acc5: 98.4375 (99.1115)  time: 0.0285  data: 0.0002  max mem: 4132
[20:06:56.918235] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5325 (0.5315)  acc1: 84.3750 (83.9652)  acc5: 100.0000 (99.1291)  time: 0.0284  data: 0.0003  max mem: 4132
[20:06:57.204478] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5132 (0.5269)  acc1: 84.3750 (84.1989)  acc5: 100.0000 (99.1637)  time: 0.0284  data: 0.0003  max mem: 4132
[20:06:57.490268] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5143 (0.5330)  acc1: 84.3750 (84.0085)  acc5: 98.4375 (99.0741)  time: 0.0284  data: 0.0003  max mem: 4132
[20:06:57.776219] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5326 (0.5300)  acc1: 84.3750 (84.1174)  acc5: 98.4375 (99.1243)  time: 0.0284  data: 0.0003  max mem: 4132
[20:06:58.065106] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.5326 (0.5337)  acc1: 82.8125 (83.9109)  acc5: 98.4375 (99.0873)  time: 0.0286  data: 0.0003  max mem: 4132
[20:06:58.356194] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5530 (0.5347)  acc1: 82.8125 (83.9105)  acc5: 98.4375 (99.0991)  time: 0.0288  data: 0.0003  max mem: 4132
[20:06:58.653343] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5169 (0.5330)  acc1: 84.3750 (83.9747)  acc5: 100.0000 (99.1090)  time: 0.0292  data: 0.0004  max mem: 4132
[20:06:58.943490] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5169 (0.5333)  acc1: 84.3750 (83.8740)  acc5: 100.0000 (99.1174)  time: 0.0292  data: 0.0004  max mem: 4132
[20:06:59.230209] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5320 (0.5322)  acc1: 82.8125 (83.9317)  acc5: 100.0000 (99.1467)  time: 0.0287  data: 0.0002  max mem: 4132
[20:06:59.517241] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5320 (0.5313)  acc1: 82.8125 (83.8680)  acc5: 100.0000 (99.1722)  time: 0.0285  data: 0.0002  max mem: 4132
[20:06:59.671839] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4857 (0.5311)  acc1: 84.3750 (83.9300)  acc5: 100.0000 (99.1800)  time: 0.0276  data: 0.0002  max mem: 4132
[20:06:59.847559] Test: Total time: 0:00:05 (0.0347 s / it)
[20:06:59.848279] * Acc@1 83.930 Acc@5 99.180 loss 0.531
[20:06:59.848590] Accuracy of the network on the 10000 test images: 83.9%
[20:06:59.848795] Max accuracy: 84.01%
[20:06:59.997361] log_dir: ./output_dir
[20:07:00.881698] Epoch: [69]  [  0/781]  eta: 0:11:29  lr: 0.000061  training_loss: 1.1861 (1.1861)  classification_loss: 1.1843 (1.1843)  loss_mask: 0.0017 (0.0017)  time: 0.8823  data: 0.7137  max mem: 4132
[20:07:04.191114] Epoch: [69]  [ 20/781]  eta: 0:02:31  lr: 0.000061  training_loss: 1.2866 (1.2504)  classification_loss: 1.2802 (1.2386)  loss_mask: 0.0047 (0.0118)  time: 0.1653  data: 0.0003  max mem: 4132
[20:07:07.479801] Epoch: [69]  [ 40/781]  eta: 0:02:15  lr: 0.000061  training_loss: 1.2778 (1.2537)  classification_loss: 1.2764 (1.2448)  loss_mask: 0.0013 (0.0089)  time: 0.1643  data: 0.0002  max mem: 4132
[20:07:10.794916] Epoch: [69]  [ 60/781]  eta: 0:02:07  lr: 0.000061  training_loss: 1.2210 (1.2562)  classification_loss: 1.2200 (1.2494)  loss_mask: 0.0010 (0.0068)  time: 0.1656  data: 0.0003  max mem: 4132
[20:07:14.092757] Epoch: [69]  [ 80/781]  eta: 0:02:01  lr: 0.000061  training_loss: 1.2137 (1.2469)  classification_loss: 1.2132 (1.2415)  loss_mask: 0.0005 (0.0053)  time: 0.1648  data: 0.0002  max mem: 4132
[20:07:17.387198] Epoch: [69]  [100/781]  eta: 0:01:57  lr: 0.000060  training_loss: 1.2642 (1.2512)  classification_loss: 1.2641 (1.2469)  loss_mask: 0.0003 (0.0044)  time: 0.1646  data: 0.0003  max mem: 4132
[20:07:20.683280] Epoch: [69]  [120/781]  eta: 0:01:52  lr: 0.000060  training_loss: 1.2098 (1.2493)  classification_loss: 1.2068 (1.2455)  loss_mask: 0.0004 (0.0037)  time: 0.1647  data: 0.0002  max mem: 4132
[20:07:23.951423] Epoch: [69]  [140/781]  eta: 0:01:48  lr: 0.000060  training_loss: 1.1885 (1.2459)  classification_loss: 1.1879 (1.2426)  loss_mask: 0.0003 (0.0033)  time: 0.1633  data: 0.0003  max mem: 4132
[20:07:27.310601] Epoch: [69]  [160/781]  eta: 0:01:45  lr: 0.000060  training_loss: 1.1819 (1.2416)  classification_loss: 1.1818 (1.2387)  loss_mask: 0.0003 (0.0029)  time: 0.1679  data: 0.0003  max mem: 4132
[20:07:30.655425] Epoch: [69]  [180/781]  eta: 0:01:41  lr: 0.000060  training_loss: 1.2428 (1.2398)  classification_loss: 1.2427 (1.2373)  loss_mask: 0.0002 (0.0026)  time: 0.1671  data: 0.0003  max mem: 4132
[20:07:33.947332] Epoch: [69]  [200/781]  eta: 0:01:38  lr: 0.000060  training_loss: 1.2843 (1.2433)  classification_loss: 1.2842 (1.2409)  loss_mask: 0.0002 (0.0024)  time: 0.1645  data: 0.0003  max mem: 4132
[20:07:37.260349] Epoch: [69]  [220/781]  eta: 0:01:34  lr: 0.000060  training_loss: 1.2375 (1.2401)  classification_loss: 1.2373 (1.2379)  loss_mask: 0.0002 (0.0022)  time: 0.1655  data: 0.0004  max mem: 4132
[20:07:40.564501] Epoch: [69]  [240/781]  eta: 0:01:30  lr: 0.000060  training_loss: 1.2593 (1.2402)  classification_loss: 1.2589 (1.2382)  loss_mask: 0.0002 (0.0020)  time: 0.1651  data: 0.0003  max mem: 4132
[20:07:43.865053] Epoch: [69]  [260/781]  eta: 0:01:27  lr: 0.000060  training_loss: 1.2115 (1.2399)  classification_loss: 1.2113 (1.2380)  loss_mask: 0.0001 (0.0019)  time: 0.1649  data: 0.0004  max mem: 4132
[20:07:47.169727] Epoch: [69]  [280/781]  eta: 0:01:24  lr: 0.000060  training_loss: 1.2386 (1.2399)  classification_loss: 1.2383 (1.2382)  loss_mask: 0.0002 (0.0017)  time: 0.1651  data: 0.0003  max mem: 4132
[20:07:50.530881] Epoch: [69]  [300/781]  eta: 0:01:20  lr: 0.000060  training_loss: 1.2368 (1.2386)  classification_loss: 1.2367 (1.2370)  loss_mask: 0.0001 (0.0016)  time: 0.1680  data: 0.0003  max mem: 4132
[20:07:53.845043] Epoch: [69]  [320/781]  eta: 0:01:17  lr: 0.000059  training_loss: 1.2309 (1.2375)  classification_loss: 1.2305 (1.2359)  loss_mask: 0.0001 (0.0015)  time: 0.1656  data: 0.0003  max mem: 4132
[20:07:57.182756] Epoch: [69]  [340/781]  eta: 0:01:13  lr: 0.000059  training_loss: 1.1744 (1.2358)  classification_loss: 1.1742 (1.2344)  loss_mask: 0.0001 (0.0015)  time: 0.1668  data: 0.0003  max mem: 4132
[20:08:00.540965] Epoch: [69]  [360/781]  eta: 0:01:10  lr: 0.000059  training_loss: 1.2711 (1.2381)  classification_loss: 1.2710 (1.2367)  loss_mask: 0.0001 (0.0014)  time: 0.1678  data: 0.0003  max mem: 4132
[20:08:03.879963] Epoch: [69]  [380/781]  eta: 0:01:07  lr: 0.000059  training_loss: 1.2449 (1.2401)  classification_loss: 1.2447 (1.2388)  loss_mask: 0.0002 (0.0013)  time: 0.1669  data: 0.0003  max mem: 4132
[20:08:07.195002] Epoch: [69]  [400/781]  eta: 0:01:03  lr: 0.000059  training_loss: 1.2509 (1.2412)  classification_loss: 1.2508 (1.2399)  loss_mask: 0.0001 (0.0013)  time: 0.1657  data: 0.0003  max mem: 4132
[20:08:10.473448] Epoch: [69]  [420/781]  eta: 0:01:00  lr: 0.000059  training_loss: 1.1915 (1.2396)  classification_loss: 1.1914 (1.2384)  loss_mask: 0.0001 (0.0012)  time: 0.1638  data: 0.0003  max mem: 4132
[20:08:13.801854] Epoch: [69]  [440/781]  eta: 0:00:57  lr: 0.000059  training_loss: 1.2215 (1.2389)  classification_loss: 1.2214 (1.2377)  loss_mask: 0.0001 (0.0012)  time: 0.1663  data: 0.0002  max mem: 4132
[20:08:17.089868] Epoch: [69]  [460/781]  eta: 0:00:53  lr: 0.000059  training_loss: 1.1984 (1.2377)  classification_loss: 1.1982 (1.2366)  loss_mask: 0.0001 (0.0011)  time: 0.1642  data: 0.0002  max mem: 4132
[20:08:20.388539] Epoch: [69]  [480/781]  eta: 0:00:50  lr: 0.000059  training_loss: 1.2538 (1.2389)  classification_loss: 1.2510 (1.2376)  loss_mask: 0.0002 (0.0014)  time: 0.1647  data: 0.0003  max mem: 4132
[20:08:23.659200] Epoch: [69]  [500/781]  eta: 0:00:46  lr: 0.000059  training_loss: 1.2917 (1.2436)  classification_loss: 1.2265 (1.2378)  loss_mask: 0.0488 (0.0058)  time: 0.1634  data: 0.0003  max mem: 4132
[20:08:26.940056] Epoch: [69]  [520/781]  eta: 0:00:43  lr: 0.000059  training_loss: 1.3320 (1.2484)  classification_loss: 1.1983 (1.2371)  loss_mask: 0.0633 (0.0113)  time: 0.1640  data: 0.0002  max mem: 4132
[20:08:30.223114] Epoch: [69]  [540/781]  eta: 0:00:40  lr: 0.000058  training_loss: 1.2831 (1.2497)  classification_loss: 1.2299 (1.2368)  loss_mask: 0.0385 (0.0129)  time: 0.1641  data: 0.0003  max mem: 4132
[20:08:33.485256] Epoch: [69]  [560/781]  eta: 0:00:36  lr: 0.000058  training_loss: 1.1940 (1.2491)  classification_loss: 1.1769 (1.2357)  loss_mask: 0.0189 (0.0134)  time: 0.1630  data: 0.0003  max mem: 4132
[20:08:36.745457] Epoch: [69]  [580/781]  eta: 0:00:33  lr: 0.000058  training_loss: 1.2559 (1.2502)  classification_loss: 1.2528 (1.2368)  loss_mask: 0.0095 (0.0134)  time: 0.1629  data: 0.0002  max mem: 4132
[20:08:40.017498] Epoch: [69]  [600/781]  eta: 0:00:30  lr: 0.000058  training_loss: 1.3041 (1.2509)  classification_loss: 1.2973 (1.2376)  loss_mask: 0.0059 (0.0133)  time: 0.1635  data: 0.0002  max mem: 4132
[20:08:43.290749] Epoch: [69]  [620/781]  eta: 0:00:26  lr: 0.000058  training_loss: 1.2556 (1.2519)  classification_loss: 1.2513 (1.2387)  loss_mask: 0.0062 (0.0131)  time: 0.1636  data: 0.0003  max mem: 4132
[20:08:46.592687] Epoch: [69]  [640/781]  eta: 0:00:23  lr: 0.000058  training_loss: 1.1929 (1.2506)  classification_loss: 1.1915 (1.2378)  loss_mask: 0.0020 (0.0128)  time: 0.1650  data: 0.0003  max mem: 4132
[20:08:49.914988] Epoch: [69]  [660/781]  eta: 0:00:20  lr: 0.000058  training_loss: 1.2662 (1.2515)  classification_loss: 1.2611 (1.2389)  loss_mask: 0.0020 (0.0126)  time: 0.1660  data: 0.0003  max mem: 4132
[20:08:53.213533] Epoch: [69]  [680/781]  eta: 0:00:16  lr: 0.000058  training_loss: 1.2060 (1.2511)  classification_loss: 1.2036 (1.2388)  loss_mask: 0.0019 (0.0123)  time: 0.1648  data: 0.0003  max mem: 4132
[20:08:56.520617] Epoch: [69]  [700/781]  eta: 0:00:13  lr: 0.000058  training_loss: 1.2661 (1.2514)  classification_loss: 1.2643 (1.2395)  loss_mask: 0.0010 (0.0120)  time: 0.1653  data: 0.0003  max mem: 4132
[20:08:59.808966] Epoch: [69]  [720/781]  eta: 0:00:10  lr: 0.000058  training_loss: 1.2495 (1.2518)  classification_loss: 1.2492 (1.2402)  loss_mask: 0.0008 (0.0117)  time: 0.1643  data: 0.0003  max mem: 4132
[20:09:03.135792] Epoch: [69]  [740/781]  eta: 0:00:06  lr: 0.000058  training_loss: 1.2191 (1.2512)  classification_loss: 1.2188 (1.2399)  loss_mask: 0.0007 (0.0114)  time: 0.1663  data: 0.0003  max mem: 4132
[20:09:06.427853] Epoch: [69]  [760/781]  eta: 0:00:03  lr: 0.000057  training_loss: 1.2577 (1.2514)  classification_loss: 1.2572 (1.2403)  loss_mask: 0.0006 (0.0111)  time: 0.1645  data: 0.0004  max mem: 4132
[20:09:09.702595] Epoch: [69]  [780/781]  eta: 0:00:00  lr: 0.000057  training_loss: 1.2607 (1.2526)  classification_loss: 1.2603 (1.2418)  loss_mask: 0.0007 (0.0108)  time: 0.1636  data: 0.0002  max mem: 4132
[20:09:09.875289] Epoch: [69] Total time: 0:02:09 (0.1663 s / it)
[20:09:09.875934] Averaged stats: lr: 0.000057  training_loss: 1.2607 (1.2526)  classification_loss: 1.2603 (1.2418)  loss_mask: 0.0007 (0.0108)
[20:09:10.615909] Test:  [  0/157]  eta: 0:01:55  testing_loss: 0.5892 (0.5892)  acc1: 79.6875 (79.6875)  acc5: 98.4375 (98.4375)  time: 0.7337  data: 0.6958  max mem: 4132
[20:09:10.928015] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.5841 (0.5551)  acc1: 81.2500 (81.6761)  acc5: 100.0000 (99.4318)  time: 0.0947  data: 0.0635  max mem: 4132
[20:09:11.218454] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5014 (0.5139)  acc1: 82.8125 (83.7054)  acc5: 100.0000 (99.6280)  time: 0.0299  data: 0.0003  max mem: 4132
[20:09:11.507578] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4950 (0.5326)  acc1: 84.3750 (83.1653)  acc5: 100.0000 (99.4960)  time: 0.0288  data: 0.0002  max mem: 4132
[20:09:11.800890] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5175 (0.5342)  acc1: 82.8125 (83.4604)  acc5: 98.4375 (99.3902)  time: 0.0289  data: 0.0004  max mem: 4132
[20:09:12.092040] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5278 (0.5303)  acc1: 84.3750 (83.8848)  acc5: 98.4375 (99.3260)  time: 0.0291  data: 0.0004  max mem: 4132
[20:09:12.379852] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5311 (0.5278)  acc1: 85.9375 (83.9908)  acc5: 100.0000 (99.3340)  time: 0.0288  data: 0.0002  max mem: 4132
[20:09:12.670364] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5221 (0.5249)  acc1: 84.3750 (84.0889)  acc5: 100.0000 (99.3398)  time: 0.0288  data: 0.0002  max mem: 4132
[20:09:12.964298] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5322 (0.5310)  acc1: 84.3750 (83.8542)  acc5: 100.0000 (99.3248)  time: 0.0291  data: 0.0004  max mem: 4132
[20:09:13.250863] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5368 (0.5264)  acc1: 84.3750 (84.1003)  acc5: 98.4375 (99.2788)  time: 0.0289  data: 0.0005  max mem: 4132
[20:09:13.542836] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.4810 (0.5269)  acc1: 84.3750 (83.9882)  acc5: 98.4375 (99.2420)  time: 0.0288  data: 0.0003  max mem: 4132
[20:09:13.833870] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5213 (0.5262)  acc1: 84.3750 (84.0794)  acc5: 100.0000 (99.2680)  time: 0.0289  data: 0.0003  max mem: 4132
[20:09:14.130311] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5040 (0.5254)  acc1: 85.9375 (84.1167)  acc5: 100.0000 (99.2510)  time: 0.0291  data: 0.0005  max mem: 4132
[20:09:14.417809] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5040 (0.5255)  acc1: 85.9375 (84.1365)  acc5: 100.0000 (99.2724)  time: 0.0290  data: 0.0005  max mem: 4132
[20:09:14.703051] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5173 (0.5228)  acc1: 85.9375 (84.2974)  acc5: 100.0000 (99.2908)  time: 0.0285  data: 0.0002  max mem: 4132
[20:09:14.985179] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4969 (0.5224)  acc1: 85.9375 (84.2715)  acc5: 100.0000 (99.2860)  time: 0.0282  data: 0.0002  max mem: 4132
[20:09:15.138667] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5051 (0.5229)  acc1: 84.3750 (84.2100)  acc5: 100.0000 (99.3100)  time: 0.0273  data: 0.0001  max mem: 4132
[20:09:15.313296] Test: Total time: 0:00:05 (0.0346 s / it)
[20:09:15.313836] * Acc@1 84.210 Acc@5 99.310 loss 0.523
[20:09:15.314316] Accuracy of the network on the 10000 test images: 84.2%
[20:09:15.314618] Max accuracy: 84.21%
[20:09:15.416247] log_dir: ./output_dir
[20:09:16.373913] Epoch: [70]  [  0/781]  eta: 0:12:26  lr: 0.000057  training_loss: 1.0377 (1.0377)  classification_loss: 1.0374 (1.0374)  loss_mask: 0.0004 (0.0004)  time: 0.9557  data: 0.7290  max mem: 4132
[20:09:19.676713] Epoch: [70]  [ 20/781]  eta: 0:02:34  lr: 0.000057  training_loss: 1.1587 (1.1928)  classification_loss: 1.1577 (1.1922)  loss_mask: 0.0006 (0.0006)  time: 0.1650  data: 0.0003  max mem: 4132
[20:09:22.969193] Epoch: [70]  [ 40/781]  eta: 0:02:16  lr: 0.000057  training_loss: 1.2050 (1.1946)  classification_loss: 1.2046 (1.1940)  loss_mask: 0.0004 (0.0005)  time: 0.1645  data: 0.0003  max mem: 4132
[20:09:26.288884] Epoch: [70]  [ 60/781]  eta: 0:02:08  lr: 0.000057  training_loss: 1.2580 (1.2151)  classification_loss: 1.2572 (1.2144)  loss_mask: 0.0006 (0.0007)  time: 0.1659  data: 0.0004  max mem: 4132
[20:09:29.609187] Epoch: [70]  [ 80/781]  eta: 0:02:02  lr: 0.000057  training_loss: 1.1893 (1.2115)  classification_loss: 1.1870 (1.2107)  loss_mask: 0.0010 (0.0008)  time: 0.1659  data: 0.0003  max mem: 4132
[20:09:32.891977] Epoch: [70]  [100/781]  eta: 0:01:57  lr: 0.000057  training_loss: 1.2235 (1.2124)  classification_loss: 1.2227 (1.2116)  loss_mask: 0.0005 (0.0008)  time: 0.1640  data: 0.0003  max mem: 4132
[20:09:36.237709] Epoch: [70]  [120/781]  eta: 0:01:53  lr: 0.000057  training_loss: 1.2323 (1.2157)  classification_loss: 1.2322 (1.2150)  loss_mask: 0.0004 (0.0007)  time: 0.1672  data: 0.0004  max mem: 4132
[20:09:39.540640] Epoch: [70]  [140/781]  eta: 0:01:49  lr: 0.000057  training_loss: 1.1854 (1.2138)  classification_loss: 1.1851 (1.2131)  loss_mask: 0.0003 (0.0007)  time: 0.1651  data: 0.0003  max mem: 4132
[20:09:42.804937] Epoch: [70]  [160/781]  eta: 0:01:45  lr: 0.000057  training_loss: 1.2175 (1.2174)  classification_loss: 1.2172 (1.2168)  loss_mask: 0.0004 (0.0006)  time: 0.1631  data: 0.0002  max mem: 4132
[20:09:46.067166] Epoch: [70]  [180/781]  eta: 0:01:41  lr: 0.000057  training_loss: 1.2206 (1.2200)  classification_loss: 1.2204 (1.2194)  loss_mask: 0.0003 (0.0006)  time: 0.1630  data: 0.0003  max mem: 4132
[20:09:49.345637] Epoch: [70]  [200/781]  eta: 0:01:38  lr: 0.000057  training_loss: 1.2113 (1.2186)  classification_loss: 1.2110 (1.2180)  loss_mask: 0.0003 (0.0006)  time: 0.1638  data: 0.0002  max mem: 4132
[20:09:52.647276] Epoch: [70]  [220/781]  eta: 0:01:34  lr: 0.000056  training_loss: 1.2074 (1.2180)  classification_loss: 1.2071 (1.2174)  loss_mask: 0.0003 (0.0006)  time: 0.1650  data: 0.0003  max mem: 4132
[20:09:55.966082] Epoch: [70]  [240/781]  eta: 0:01:30  lr: 0.000056  training_loss: 1.2195 (1.2164)  classification_loss: 1.2190 (1.2159)  loss_mask: 0.0003 (0.0005)  time: 0.1658  data: 0.0003  max mem: 4132
[20:09:59.314549] Epoch: [70]  [260/781]  eta: 0:01:27  lr: 0.000056  training_loss: 1.2128 (1.2172)  classification_loss: 1.2126 (1.2167)  loss_mask: 0.0003 (0.0005)  time: 0.1673  data: 0.0004  max mem: 4132
[20:10:02.602043] Epoch: [70]  [280/781]  eta: 0:01:24  lr: 0.000056  training_loss: 1.1536 (1.2163)  classification_loss: 1.1532 (1.2158)  loss_mask: 0.0002 (0.0005)  time: 0.1643  data: 0.0003  max mem: 4132
[20:10:05.868810] Epoch: [70]  [300/781]  eta: 0:01:20  lr: 0.000056  training_loss: 1.1911 (1.2166)  classification_loss: 1.1909 (1.2161)  loss_mask: 0.0003 (0.0005)  time: 0.1632  data: 0.0003  max mem: 4132
[20:10:09.160354] Epoch: [70]  [320/781]  eta: 0:01:17  lr: 0.000056  training_loss: 1.2491 (1.2180)  classification_loss: 1.2489 (1.2175)  loss_mask: 0.0003 (0.0005)  time: 0.1645  data: 0.0003  max mem: 4132
[20:10:12.456713] Epoch: [70]  [340/781]  eta: 0:01:13  lr: 0.000056  training_loss: 1.1814 (1.2172)  classification_loss: 1.1810 (1.2167)  loss_mask: 0.0002 (0.0005)  time: 0.1647  data: 0.0003  max mem: 4132
[20:10:15.757241] Epoch: [70]  [360/781]  eta: 0:01:10  lr: 0.000056  training_loss: 1.2593 (1.2194)  classification_loss: 1.2591 (1.2190)  loss_mask: 0.0002 (0.0005)  time: 0.1649  data: 0.0002  max mem: 4132
[20:10:19.094596] Epoch: [70]  [380/781]  eta: 0:01:06  lr: 0.000056  training_loss: 1.2459 (1.2212)  classification_loss: 1.2459 (1.2208)  loss_mask: 0.0002 (0.0004)  time: 0.1668  data: 0.0004  max mem: 4132
[20:10:22.395918] Epoch: [70]  [400/781]  eta: 0:01:03  lr: 0.000056  training_loss: 1.2034 (1.2199)  classification_loss: 1.2031 (1.2195)  loss_mask: 0.0003 (0.0004)  time: 0.1650  data: 0.0002  max mem: 4132
[20:10:25.679306] Epoch: [70]  [420/781]  eta: 0:01:00  lr: 0.000056  training_loss: 1.2286 (1.2198)  classification_loss: 1.2282 (1.2193)  loss_mask: 0.0003 (0.0004)  time: 0.1641  data: 0.0003  max mem: 4132
[20:10:28.965605] Epoch: [70]  [440/781]  eta: 0:00:56  lr: 0.000055  training_loss: 1.1858 (1.2197)  classification_loss: 1.1853 (1.2193)  loss_mask: 0.0002 (0.0004)  time: 0.1642  data: 0.0003  max mem: 4132
[20:10:32.247202] Epoch: [70]  [460/781]  eta: 0:00:53  lr: 0.000055  training_loss: 1.1885 (1.2188)  classification_loss: 1.1884 (1.2184)  loss_mask: 0.0002 (0.0004)  time: 0.1640  data: 0.0003  max mem: 4132
[20:10:35.597249] Epoch: [70]  [480/781]  eta: 0:00:50  lr: 0.000055  training_loss: 1.2756 (1.2210)  classification_loss: 1.2755 (1.2206)  loss_mask: 0.0002 (0.0004)  time: 0.1674  data: 0.0003  max mem: 4132

[20:10:38.886495] Epoch: [70]  [500/781]  eta: 0:00:46  lr: 0.000055  training_loss: 1.2261 (1.2213)  classification_loss: 1.2258 (1.2209)  loss_mask: 0.0002 (0.0004)  time: 0.1644  data: 0.0002  max mem: 4132
[20:10:42.194725] Epoch: [70]  [520/781]  eta: 0:00:43  lr: 0.000055  training_loss: 1.2036 (1.2211)  classification_loss: 1.2035 (1.2207)  loss_mask: 0.0002 (0.0004)  time: 0.1653  data: 0.0002  max mem: 4132
[20:10:45.520519] Epoch: [70]  [540/781]  eta: 0:00:40  lr: 0.000055  training_loss: 1.2047 (1.2218)  classification_loss: 1.2042 (1.2214)  loss_mask: 0.0002 (0.0004)  time: 0.1661  data: 0.0005  max mem: 4132
[20:10:48.826986] Epoch: [70]  [560/781]  eta: 0:00:36  lr: 0.000055  training_loss: 1.2319 (1.2219)  classification_loss: 1.2318 (1.2215)  loss_mask: 0.0002 (0.0004)  time: 0.1652  data: 0.0003  max mem: 4132
[20:10:52.131339] Epoch: [70]  [580/781]  eta: 0:00:33  lr: 0.000055  training_loss: 1.1634 (1.2203)  classification_loss: 1.1631 (1.2199)  loss_mask: 0.0002 (0.0004)  time: 0.1651  data: 0.0003  max mem: 4132
[20:10:55.404745] Epoch: [70]  [600/781]  eta: 0:00:30  lr: 0.000055  training_loss: 1.1961 (1.2200)  classification_loss: 1.1958 (1.2196)  loss_mask: 0.0002 (0.0004)  time: 0.1636  data: 0.0002  max mem: 4132
[20:10:58.664023] Epoch: [70]  [620/781]  eta: 0:00:26  lr: 0.000055  training_loss: 1.2104 (1.2203)  classification_loss: 1.2103 (1.2200)  loss_mask: 0.0002 (0.0004)  time: 0.1629  data: 0.0002  max mem: 4132
[20:11:01.941455] Epoch: [70]  [640/781]  eta: 0:00:23  lr: 0.000055  training_loss: 1.2115 (1.2204)  classification_loss: 1.2113 (1.2200)  loss_mask: 0.0002 (0.0004)  time: 0.1638  data: 0.0002  max mem: 4132
[20:11:05.234039] Epoch: [70]  [660/781]  eta: 0:00:20  lr: 0.000055  training_loss: 1.2158 (1.2205)  classification_loss: 1.2157 (1.2201)  loss_mask: 0.0002 (0.0004)  time: 0.1645  data: 0.0003  max mem: 4132
[20:11:08.551379] Epoch: [70]  [680/781]  eta: 0:00:16  lr: 0.000054  training_loss: 1.2227 (1.2205)  classification_loss: 1.2225 (1.2202)  loss_mask: 0.0002 (0.0003)  time: 0.1658  data: 0.0003  max mem: 4132
[20:11:11.842647] Epoch: [70]  [700/781]  eta: 0:00:13  lr: 0.000054  training_loss: 1.2507 (1.2217)  classification_loss: 1.2506 (1.2213)  loss_mask: 0.0002 (0.0003)  time: 0.1645  data: 0.0003  max mem: 4132
[20:11:15.164840] Epoch: [70]  [720/781]  eta: 0:00:10  lr: 0.000054  training_loss: 1.2640 (1.2226)  classification_loss: 1.2638 (1.2223)  loss_mask: 0.0002 (0.0003)  time: 0.1660  data: 0.0004  max mem: 4132
[20:11:18.447035] Epoch: [70]  [740/781]  eta: 0:00:06  lr: 0.000054  training_loss: 1.2411 (1.2226)  classification_loss: 1.2408 (1.2222)  loss_mask: 0.0002 (0.0003)  time: 0.1640  data: 0.0002  max mem: 4132
[20:11:21.729414] Epoch: [70]  [760/781]  eta: 0:00:03  lr: 0.000054  training_loss: 1.2179 (1.2228)  classification_loss: 1.2177 (1.2225)  loss_mask: 0.0002 (0.0003)  time: 0.1640  data: 0.0003  max mem: 4132
[20:11:25.029773] Epoch: [70]  [780/781]  eta: 0:00:00  lr: 0.000054  training_loss: 1.1808 (1.2224)  classification_loss: 1.1803 (1.2221)  loss_mask: 0.0002 (0.0003)  time: 0.1649  data: 0.0003  max mem: 4132
[20:11:25.187195] Epoch: [70] Total time: 0:02:09 (0.1662 s / it)
[20:11:25.187652] Averaged stats: lr: 0.000054  training_loss: 1.1808 (1.2224)  classification_loss: 1.1803 (1.2221)  loss_mask: 0.0002 (0.0003)
[20:11:26.761471] Test:  [  0/157]  eta: 0:02:05  testing_loss: 0.5215 (0.5215)  acc1: 84.3750 (84.3750)  acc5: 98.4375 (98.4375)  time: 0.7967  data: 0.7667  max mem: 4132
[20:11:27.054771] Test:  [ 10/157]  eta: 0:00:14  testing_loss: 0.5217 (0.5437)  acc1: 82.8125 (83.0966)  acc5: 100.0000 (99.2898)  time: 0.0989  data: 0.0699  max mem: 4132
[20:11:27.343141] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4810 (0.5070)  acc1: 85.9375 (84.5982)  acc5: 100.0000 (99.4792)  time: 0.0289  data: 0.0002  max mem: 4132
[20:11:27.630147] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4810 (0.5331)  acc1: 84.3750 (83.8206)  acc5: 100.0000 (99.1935)  time: 0.0286  data: 0.0002  max mem: 4132
[20:11:27.916743] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5149 (0.5361)  acc1: 82.8125 (83.8034)  acc5: 98.4375 (99.1235)  time: 0.0285  data: 0.0002  max mem: 4132
[20:11:28.201597] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5111 (0.5271)  acc1: 84.3750 (84.3137)  acc5: 100.0000 (99.2034)  time: 0.0284  data: 0.0002  max mem: 4132
[20:11:28.487648] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4991 (0.5224)  acc1: 85.9375 (84.5799)  acc5: 100.0000 (99.2316)  time: 0.0284  data: 0.0002  max mem: 4132
[20:11:28.779555] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4964 (0.5189)  acc1: 85.9375 (84.7271)  acc5: 100.0000 (99.1857)  time: 0.0287  data: 0.0002  max mem: 4132
[20:11:29.067258] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5077 (0.5269)  acc1: 84.3750 (84.4136)  acc5: 98.4375 (99.1319)  time: 0.0288  data: 0.0002  max mem: 4132
[20:11:29.359215] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5400 (0.5254)  acc1: 84.3750 (84.4780)  acc5: 98.4375 (99.0900)  time: 0.0288  data: 0.0002  max mem: 4132
[20:11:29.647091] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.5181 (0.5271)  acc1: 84.3750 (84.3286)  acc5: 98.4375 (99.0408)  time: 0.0288  data: 0.0002  max mem: 4132
[20:11:29.941599] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5334 (0.5269)  acc1: 84.3750 (84.4032)  acc5: 98.4375 (99.0709)  time: 0.0290  data: 0.0002  max mem: 4132
[20:11:30.227279] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5097 (0.5255)  acc1: 85.9375 (84.4267)  acc5: 100.0000 (99.0832)  time: 0.0289  data: 0.0002  max mem: 4132
[20:11:30.517182] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4949 (0.5255)  acc1: 82.8125 (84.2677)  acc5: 100.0000 (99.1412)  time: 0.0286  data: 0.0002  max mem: 4132
[20:11:30.806532] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5067 (0.5230)  acc1: 84.3750 (84.3972)  acc5: 100.0000 (99.1800)  time: 0.0288  data: 0.0003  max mem: 4132
[20:11:31.090510] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5109 (0.5225)  acc1: 84.3750 (84.3957)  acc5: 100.0000 (99.2032)  time: 0.0285  data: 0.0002  max mem: 4132
[20:11:31.244015] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4926 (0.5218)  acc1: 84.3750 (84.3900)  acc5: 100.0000 (99.2200)  time: 0.0274  data: 0.0002  max mem: 4132
[20:11:31.442204] Test: Total time: 0:00:05 (0.0349 s / it)
[20:11:31.442863] * Acc@1 84.390 Acc@5 99.220 loss 0.522
[20:11:31.443199] Accuracy of the network on the 10000 test images: 84.4%
[20:11:31.443382] Max accuracy: 84.39%
[20:11:31.650224] log_dir: ./output_dir
[20:11:32.611975] Epoch: [71]  [  0/781]  eta: 0:12:29  lr: 0.000054  training_loss: 1.1938 (1.1938)  classification_loss: 1.1935 (1.1935)  loss_mask: 0.0003 (0.0003)  time: 0.9598  data: 0.7806  max mem: 4132
[20:11:35.921864] Epoch: [71]  [ 20/781]  eta: 0:02:34  lr: 0.000054  training_loss: 1.1887 (1.1863)  classification_loss: 1.1885 (1.1861)  loss_mask: 0.0002 (0.0002)  time: 0.1654  data: 0.0003  max mem: 4132
[20:11:39.216941] Epoch: [71]  [ 40/781]  eta: 0:02:16  lr: 0.000054  training_loss: 1.2328 (1.2153)  classification_loss: 1.2326 (1.2151)  loss_mask: 0.0002 (0.0002)  time: 0.1647  data: 0.0002  max mem: 4132
[20:11:42.526698] Epoch: [71]  [ 60/781]  eta: 0:02:08  lr: 0.000054  training_loss: 1.2419 (1.2182)  classification_loss: 1.2417 (1.2180)  loss_mask: 0.0002 (0.0002)  time: 0.1654  data: 0.0003  max mem: 4132
[20:11:45.805779] Epoch: [71]  [ 80/781]  eta: 0:02:02  lr: 0.000054  training_loss: 1.1974 (1.2134)  classification_loss: 1.1973 (1.2132)  loss_mask: 0.0001 (0.0002)  time: 0.1639  data: 0.0003  max mem: 4132
[20:11:49.096227] Epoch: [71]  [100/781]  eta: 0:01:57  lr: 0.000054  training_loss: 1.2035 (1.2092)  classification_loss: 1.2033 (1.2090)  loss_mask: 0.0001 (0.0002)  time: 0.1644  data: 0.0003  max mem: 4132
[20:11:52.418220] Epoch: [71]  [120/781]  eta: 0:01:53  lr: 0.000053  training_loss: 1.2564 (1.2165)  classification_loss: 1.2562 (1.2164)  loss_mask: 0.0002 (0.0002)  time: 0.1660  data: 0.0003  max mem: 4132
[20:11:55.736033] Epoch: [71]  [140/781]  eta: 0:01:49  lr: 0.000053  training_loss: 1.2008 (1.2148)  classification_loss: 1.2008 (1.2146)  loss_mask: 0.0002 (0.0002)  time: 0.1657  data: 0.0004  max mem: 4132
[20:11:59.013083] Epoch: [71]  [160/781]  eta: 0:01:45  lr: 0.000053  training_loss: 1.1979 (1.2167)  classification_loss: 1.1978 (1.2165)  loss_mask: 0.0001 (0.0002)  time: 0.1637  data: 0.0002  max mem: 4132
[20:12:02.318158] Epoch: [71]  [180/781]  eta: 0:01:41  lr: 0.000053  training_loss: 1.2185 (1.2184)  classification_loss: 1.2185 (1.2182)  loss_mask: 0.0001 (0.0002)  time: 0.1650  data: 0.0003  max mem: 4132
[20:12:05.608594] Epoch: [71]  [200/781]  eta: 0:01:38  lr: 0.000053  training_loss: 1.1945 (1.2154)  classification_loss: 1.1944 (1.2152)  loss_mask: 0.0002 (0.0002)  time: 0.1644  data: 0.0003  max mem: 4132
[20:12:08.879494] Epoch: [71]  [220/781]  eta: 0:01:34  lr: 0.000053  training_loss: 1.1995 (1.2150)  classification_loss: 1.1993 (1.2148)  loss_mask: 0.0001 (0.0002)  time: 0.1634  data: 0.0003  max mem: 4132
[20:12:12.154110] Epoch: [71]  [240/781]  eta: 0:01:30  lr: 0.000053  training_loss: 1.1982 (1.2159)  classification_loss: 1.1979 (1.2157)  loss_mask: 0.0001 (0.0002)  time: 0.1636  data: 0.0003  max mem: 4132
[20:12:15.418603] Epoch: [71]  [260/781]  eta: 0:01:27  lr: 0.000053  training_loss: 1.2185 (1.2171)  classification_loss: 1.2184 (1.2170)  loss_mask: 0.0001 (0.0002)  time: 0.1631  data: 0.0002  max mem: 4132
[20:12:18.691309] Epoch: [71]  [280/781]  eta: 0:01:23  lr: 0.000053  training_loss: 1.2100 (1.2184)  classification_loss: 1.2099 (1.2182)  loss_mask: 0.0001 (0.0002)  time: 0.1636  data: 0.0002  max mem: 4132
[20:12:22.000672] Epoch: [71]  [300/781]  eta: 0:01:20  lr: 0.000053  training_loss: 1.2354 (1.2195)  classification_loss: 1.2353 (1.2194)  loss_mask: 0.0001 (0.0002)  time: 0.1653  data: 0.0003  max mem: 4132
[20:12:25.290629] Epoch: [71]  [320/781]  eta: 0:01:16  lr: 0.000053  training_loss: 1.1638 (1.2176)  classification_loss: 1.1636 (1.2174)  loss_mask: 0.0001 (0.0002)  time: 0.1644  data: 0.0003  max mem: 4132
[20:12:28.633130] Epoch: [71]  [340/781]  eta: 0:01:13  lr: 0.000053  training_loss: 1.2005 (1.2171)  classification_loss: 1.2003 (1.2170)  loss_mask: 0.0001 (0.0002)  time: 0.1670  data: 0.0003  max mem: 4132
[20:12:31.935222] Epoch: [71]  [360/781]  eta: 0:01:10  lr: 0.000052  training_loss: 1.2602 (1.2179)  classification_loss: 1.2601 (1.2177)  loss_mask: 0.0001 (0.0002)  time: 0.1650  data: 0.0003  max mem: 4132
[20:12:35.236736] Epoch: [71]  [380/781]  eta: 0:01:06  lr: 0.000052  training_loss: 1.1899 (1.2175)  classification_loss: 1.1897 (1.2173)  loss_mask: 0.0001 (0.0002)  time: 0.1650  data: 0.0003  max mem: 4132
[20:12:38.520418] Epoch: [71]  [400/781]  eta: 0:01:03  lr: 0.000052  training_loss: 1.2254 (1.2185)  classification_loss: 1.2253 (1.2183)  loss_mask: 0.0001 (0.0002)  time: 0.1641  data: 0.0002  max mem: 4132
[20:12:41.811860] Epoch: [71]  [420/781]  eta: 0:01:00  lr: 0.000052  training_loss: 1.2434 (1.2193)  classification_loss: 1.2412 (1.2188)  loss_mask: 0.0001 (0.0005)  time: 0.1645  data: 0.0003  max mem: 4132
[20:12:45.087116] Epoch: [71]  [440/781]  eta: 0:00:56  lr: 0.000052  training_loss: 1.2407 (1.2212)  classification_loss: 1.2247 (1.2198)  loss_mask: 0.0029 (0.0014)  time: 0.1637  data: 0.0003  max mem: 4132
[20:12:48.368405] Epoch: [71]  [460/781]  eta: 0:00:53  lr: 0.000052  training_loss: 1.2018 (1.2201)  classification_loss: 1.1899 (1.2185)  loss_mask: 0.0017 (0.0016)  time: 0.1640  data: 0.0003  max mem: 4132
[20:12:51.659182] Epoch: [71]  [480/781]  eta: 0:00:50  lr: 0.000052  training_loss: 1.2182 (1.2213)  classification_loss: 1.1933 (1.2188)  loss_mask: 0.0040 (0.0025)  time: 0.1644  data: 0.0003  max mem: 4132
[20:12:54.951223] Epoch: [71]  [500/781]  eta: 0:00:46  lr: 0.000052  training_loss: 1.2428 (1.2227)  classification_loss: 1.2369 (1.2197)  loss_mask: 0.0049 (0.0030)  time: 0.1645  data: 0.0002  max mem: 4132
[20:12:58.327478] Epoch: [71]  [520/781]  eta: 0:00:43  lr: 0.000052  training_loss: 1.2749 (1.2241)  classification_loss: 1.2609 (1.2203)  loss_mask: 0.0122 (0.0038)  time: 0.1687  data: 0.0003  max mem: 4132
[20:13:01.642517] Epoch: [71]  [540/781]  eta: 0:00:40  lr: 0.000052  training_loss: 1.2157 (1.2243)  classification_loss: 1.1988 (1.2204)  loss_mask: 0.0027 (0.0039)  time: 0.1657  data: 0.0005  max mem: 4132
[20:13:04.984916] Epoch: [71]  [560/781]  eta: 0:00:36  lr: 0.000052  training_loss: 1.2143 (1.2238)  classification_loss: 1.2140 (1.2200)  loss_mask: 0.0005 (0.0038)  time: 0.1670  data: 0.0003  max mem: 4132
[20:13:08.269193] Epoch: [71]  [580/781]  eta: 0:00:33  lr: 0.000052  training_loss: 1.2006 (1.2238)  classification_loss: 1.2002 (1.2201)  loss_mask: 0.0004 (0.0037)  time: 0.1641  data: 0.0002  max mem: 4132
[20:13:11.609607] Epoch: [71]  [600/781]  eta: 0:00:30  lr: 0.000051  training_loss: 1.2574 (1.2243)  classification_loss: 1.2569 (1.2207)  loss_mask: 0.0004 (0.0036)  time: 0.1669  data: 0.0003  max mem: 4132
[20:13:14.892669] Epoch: [71]  [620/781]  eta: 0:00:26  lr: 0.000051  training_loss: 1.2276 (1.2240)  classification_loss: 1.2274 (1.2205)  loss_mask: 0.0003 (0.0035)  time: 0.1640  data: 0.0002  max mem: 4132
[20:13:18.162974] Epoch: [71]  [640/781]  eta: 0:00:23  lr: 0.000051  training_loss: 1.2189 (1.2247)  classification_loss: 1.2188 (1.2213)  loss_mask: 0.0003 (0.0034)  time: 0.1634  data: 0.0003  max mem: 4132
[20:13:21.494533] Epoch: [71]  [660/781]  eta: 0:00:20  lr: 0.000051  training_loss: 1.2196 (1.2240)  classification_loss: 1.2193 (1.2207)  loss_mask: 0.0003 (0.0033)  time: 0.1665  data: 0.0003  max mem: 4132
[20:13:24.757365] Epoch: [71]  [680/781]  eta: 0:00:16  lr: 0.000051  training_loss: 1.1802 (1.2240)  classification_loss: 1.1800 (1.2208)  loss_mask: 0.0002 (0.0032)  time: 0.1630  data: 0.0002  max mem: 4132
[20:13:28.021577] Epoch: [71]  [700/781]  eta: 0:00:13  lr: 0.000051  training_loss: 1.2484 (1.2242)  classification_loss: 1.2480 (1.2210)  loss_mask: 0.0002 (0.0031)  time: 0.1631  data: 0.0003  max mem: 4132
[20:13:31.273488] Epoch: [71]  [720/781]  eta: 0:00:10  lr: 0.000051  training_loss: 1.2320 (1.2250)  classification_loss: 1.2318 (1.2219)  loss_mask: 0.0002 (0.0030)  time: 0.1625  data: 0.0002  max mem: 4132
[20:13:34.544554] Epoch: [71]  [740/781]  eta: 0:00:06  lr: 0.000051  training_loss: 1.1916 (1.2244)  classification_loss: 1.1914 (1.2215)  loss_mask: 0.0002 (0.0030)  time: 0.1634  data: 0.0003  max mem: 4132
[20:13:37.858589] Epoch: [71]  [760/781]  eta: 0:00:03  lr: 0.000051  training_loss: 1.2562 (1.2251)  classification_loss: 1.2558 (1.2222)  loss_mask: 0.0002 (0.0029)  time: 0.1656  data: 0.0002  max mem: 4132
[20:13:41.146501] Epoch: [71]  [780/781]  eta: 0:00:00  lr: 0.000051  training_loss: 1.2365 (1.2250)  classification_loss: 1.2364 (1.2222)  loss_mask: 0.0002 (0.0028)  time: 0.1643  data: 0.0002  max mem: 4132
[20:13:41.299545] Epoch: [71] Total time: 0:02:09 (0.1660 s / it)
[20:13:41.300096] Averaged stats: lr: 0.000051  training_loss: 1.2365 (1.2250)  classification_loss: 1.2364 (1.2222)  loss_mask: 0.0002 (0.0028)
[20:13:42.041778] Test:  [  0/157]  eta: 0:01:55  testing_loss: 0.5356 (0.5356)  acc1: 82.8125 (82.8125)  acc5: 98.4375 (98.4375)  time: 0.7371  data: 0.7055  max mem: 4132
[20:13:42.338328] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.5356 (0.5463)  acc1: 82.8125 (83.2386)  acc5: 100.0000 (99.4318)  time: 0.0938  data: 0.0643  max mem: 4132
[20:13:42.629507] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5005 (0.5037)  acc1: 84.3750 (84.5238)  acc5: 100.0000 (99.4792)  time: 0.0292  data: 0.0002  max mem: 4132
[20:13:42.916943] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4933 (0.5198)  acc1: 84.3750 (84.1734)  acc5: 100.0000 (99.2944)  time: 0.0288  data: 0.0002  max mem: 4132
[20:13:43.203032] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5139 (0.5184)  acc1: 84.3750 (84.5655)  acc5: 98.4375 (99.1616)  time: 0.0285  data: 0.0002  max mem: 4132
[20:13:43.493926] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5057 (0.5153)  acc1: 85.9375 (84.8958)  acc5: 98.4375 (99.2341)  time: 0.0287  data: 0.0003  max mem: 4132
[20:13:43.782725] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5073 (0.5129)  acc1: 84.3750 (85.0154)  acc5: 100.0000 (99.2572)  time: 0.0288  data: 0.0002  max mem: 4132
[20:13:44.070901] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4859 (0.5064)  acc1: 85.9375 (85.2553)  acc5: 100.0000 (99.2518)  time: 0.0286  data: 0.0002  max mem: 4132
[20:13:44.360906] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4897 (0.5152)  acc1: 85.9375 (84.8765)  acc5: 98.4375 (99.2091)  time: 0.0287  data: 0.0002  max mem: 4132
[20:13:44.647779] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5367 (0.5134)  acc1: 82.8125 (84.9416)  acc5: 100.0000 (99.2102)  time: 0.0287  data: 0.0002  max mem: 4132
[20:13:44.939140] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.5251 (0.5160)  acc1: 84.3750 (84.8546)  acc5: 100.0000 (99.2110)  time: 0.0287  data: 0.0003  max mem: 4132
[20:13:45.228530] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5352 (0.5163)  acc1: 84.3750 (84.8818)  acc5: 100.0000 (99.1976)  time: 0.0289  data: 0.0003  max mem: 4132
[20:13:45.518522] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4987 (0.5170)  acc1: 84.3750 (84.8140)  acc5: 100.0000 (99.1994)  time: 0.0288  data: 0.0003  max mem: 4132
[20:13:45.813634] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4959 (0.5171)  acc1: 84.3750 (84.7925)  acc5: 100.0000 (99.2128)  time: 0.0291  data: 0.0004  max mem: 4132
[20:13:46.119418] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4959 (0.5135)  acc1: 85.9375 (84.8958)  acc5: 100.0000 (99.2465)  time: 0.0299  data: 0.0004  max mem: 4132
[20:13:46.405471] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4966 (0.5122)  acc1: 84.3750 (84.8510)  acc5: 100.0000 (99.2653)  time: 0.0294  data: 0.0002  max mem: 4132
[20:13:46.560118] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4966 (0.5129)  acc1: 84.3750 (84.8400)  acc5: 100.0000 (99.2700)  time: 0.0275  data: 0.0002  max mem: 4132
[20:13:46.763907] Test: Total time: 0:00:05 (0.0348 s / it)
[20:13:46.764650] * Acc@1 84.840 Acc@5 99.270 loss 0.513
[20:13:46.765144] Accuracy of the network on the 10000 test images: 84.8%
[20:13:46.765700] Max accuracy: 84.84%
[20:13:46.984475] log_dir: ./output_dir
[20:13:47.983085] Epoch: [72]  [  0/781]  eta: 0:12:57  lr: 0.000051  training_loss: 1.1143 (1.1143)  classification_loss: 1.1142 (1.1142)  loss_mask: 0.0001 (0.0001)  time: 0.9950  data: 0.7809  max mem: 4132
[20:13:51.291593] Epoch: [72]  [ 20/781]  eta: 0:02:35  lr: 0.000051  training_loss: 1.1638 (1.1620)  classification_loss: 1.1635 (1.1618)  loss_mask: 0.0002 (0.0002)  time: 0.1653  data: 0.0002  max mem: 4132
[20:13:54.614102] Epoch: [72]  [ 40/781]  eta: 0:02:17  lr: 0.000050  training_loss: 1.2282 (1.1950)  classification_loss: 1.2280 (1.1948)  loss_mask: 0.0002 (0.0002)  time: 0.1660  data: 0.0003  max mem: 4132
[20:13:57.903652] Epoch: [72]  [ 60/781]  eta: 0:02:08  lr: 0.000050  training_loss: 1.2053 (1.2028)  classification_loss: 1.2052 (1.2026)  loss_mask: 0.0001 (0.0002)  time: 0.1644  data: 0.0003  max mem: 4132
[20:14:01.230796] Epoch: [72]  [ 80/781]  eta: 0:02:03  lr: 0.000050  training_loss: 1.2185 (1.2087)  classification_loss: 1.2183 (1.2085)  loss_mask: 0.0001 (0.0002)  time: 0.1663  data: 0.0005  max mem: 4132
[20:14:04.545258] Epoch: [72]  [100/781]  eta: 0:01:58  lr: 0.000050  training_loss: 1.2645 (1.2156)  classification_loss: 1.2645 (1.2154)  loss_mask: 0.0001 (0.0002)  time: 0.1656  data: 0.0003  max mem: 4132
[20:14:07.823305] Epoch: [72]  [120/781]  eta: 0:01:53  lr: 0.000050  training_loss: 1.2307 (1.2149)  classification_loss: 1.2305 (1.2148)  loss_mask: 0.0002 (0.0002)  time: 0.1638  data: 0.0003  max mem: 4132
[20:14:11.192450] Epoch: [72]  [140/781]  eta: 0:01:49  lr: 0.000050  training_loss: 1.2122 (1.2163)  classification_loss: 1.2121 (1.2161)  loss_mask: 0.0001 (0.0002)  time: 0.1684  data: 0.0003  max mem: 4132
[20:14:14.525330] Epoch: [72]  [160/781]  eta: 0:01:46  lr: 0.000050  training_loss: 1.1742 (1.2144)  classification_loss: 1.1741 (1.2142)  loss_mask: 0.0001 (0.0002)  time: 0.1665  data: 0.0003  max mem: 4132
[20:14:17.829397] Epoch: [72]  [180/781]  eta: 0:01:42  lr: 0.000050  training_loss: 1.2050 (1.2101)  classification_loss: 1.2049 (1.2099)  loss_mask: 0.0001 (0.0002)  time: 0.1651  data: 0.0004  max mem: 4132
[20:14:21.114150] Epoch: [72]  [200/781]  eta: 0:01:38  lr: 0.000050  training_loss: 1.2463 (1.2142)  classification_loss: 1.2458 (1.2141)  loss_mask: 0.0002 (0.0002)  time: 0.1641  data: 0.0003  max mem: 4132
[20:14:24.408752] Epoch: [72]  [220/781]  eta: 0:01:34  lr: 0.000050  training_loss: 1.2112 (1.2159)  classification_loss: 1.2110 (1.2157)  loss_mask: 0.0001 (0.0002)  time: 0.1646  data: 0.0003  max mem: 4132
[20:14:27.679129] Epoch: [72]  [240/781]  eta: 0:01:31  lr: 0.000050  training_loss: 1.2132 (1.2166)  classification_loss: 1.2131 (1.2164)  loss_mask: 0.0001 (0.0002)  time: 0.1634  data: 0.0003  max mem: 4132
[20:14:30.979959] Epoch: [72]  [260/781]  eta: 0:01:27  lr: 0.000050  training_loss: 1.2422 (1.2181)  classification_loss: 1.2421 (1.2180)  loss_mask: 0.0001 (0.0002)  time: 0.1649  data: 0.0003  max mem: 4132
[20:14:34.252863] Epoch: [72]  [280/781]  eta: 0:01:24  lr: 0.000049  training_loss: 1.2088 (1.2180)  classification_loss: 1.2087 (1.2178)  loss_mask: 0.0001 (0.0002)  time: 0.1636  data: 0.0002  max mem: 4132
[20:14:37.587054] Epoch: [72]  [300/781]  eta: 0:01:20  lr: 0.000049  training_loss: 1.2276 (1.2181)  classification_loss: 1.2275 (1.2180)  loss_mask: 0.0001 (0.0002)  time: 0.1666  data: 0.0003  max mem: 4132
[20:14:40.863463] Epoch: [72]  [320/781]  eta: 0:01:17  lr: 0.000049  training_loss: 1.2069 (1.2182)  classification_loss: 1.2068 (1.2180)  loss_mask: 0.0001 (0.0002)  time: 0.1637  data: 0.0002  max mem: 4132
[20:14:44.169856] Epoch: [72]  [340/781]  eta: 0:01:13  lr: 0.000049  training_loss: 1.1826 (1.2171)  classification_loss: 1.1825 (1.2170)  loss_mask: 0.0001 (0.0002)  time: 0.1652  data: 0.0003  max mem: 4132
[20:14:47.453461] Epoch: [72]  [360/781]  eta: 0:01:10  lr: 0.000049  training_loss: 1.2618 (1.2187)  classification_loss: 1.2618 (1.2185)  loss_mask: 0.0001 (0.0002)  time: 0.1641  data: 0.0002  max mem: 4132
[20:14:50.756186] Epoch: [72]  [380/781]  eta: 0:01:07  lr: 0.000049  training_loss: 1.2328 (1.2199)  classification_loss: 1.2328 (1.2198)  loss_mask: 0.0001 (0.0002)  time: 0.1651  data: 0.0003  max mem: 4132
[20:14:54.066622] Epoch: [72]  [400/781]  eta: 0:01:03  lr: 0.000049  training_loss: 1.2391 (1.2202)  classification_loss: 1.2390 (1.2200)  loss_mask: 0.0001 (0.0002)  time: 0.1654  data: 0.0003  max mem: 4132
[20:14:57.343938] Epoch: [72]  [420/781]  eta: 0:01:00  lr: 0.000049  training_loss: 1.2082 (1.2204)  classification_loss: 1.2080 (1.2203)  loss_mask: 0.0001 (0.0001)  time: 0.1638  data: 0.0003  max mem: 4132
[20:15:00.624490] Epoch: [72]  [440/781]  eta: 0:00:56  lr: 0.000049  training_loss: 1.2096 (1.2201)  classification_loss: 1.2094 (1.2200)  loss_mask: 0.0001 (0.0001)  time: 0.1639  data: 0.0003  max mem: 4132
[20:15:03.930143] Epoch: [72]  [460/781]  eta: 0:00:53  lr: 0.000049  training_loss: 1.1674 (1.2189)  classification_loss: 1.1673 (1.2188)  loss_mask: 0.0001 (0.0001)  time: 0.1652  data: 0.0003  max mem: 4132
[20:15:07.204963] Epoch: [72]  [480/781]  eta: 0:00:50  lr: 0.000049  training_loss: 1.2188 (1.2195)  classification_loss: 1.2187 (1.2194)  loss_mask: 0.0001 (0.0001)  time: 0.1637  data: 0.0003  max mem: 4132
[20:15:10.525317] Epoch: [72]  [500/781]  eta: 0:00:46  lr: 0.000049  training_loss: 1.2037 (1.2194)  classification_loss: 1.2036 (1.2193)  loss_mask: 0.0001 (0.0001)  time: 0.1659  data: 0.0003  max mem: 4132
[20:15:13.811962] Epoch: [72]  [520/781]  eta: 0:00:43  lr: 0.000048  training_loss: 1.1947 (1.2189)  classification_loss: 1.1946 (1.2188)  loss_mask: 0.0001 (0.0001)  time: 0.1642  data: 0.0003  max mem: 4132
[20:15:17.089293] Epoch: [72]  [540/781]  eta: 0:00:40  lr: 0.000048  training_loss: 1.1895 (1.2175)  classification_loss: 1.1894 (1.2173)  loss_mask: 0.0001 (0.0001)  time: 0.1638  data: 0.0003  max mem: 4132
[20:15:20.362447] Epoch: [72]  [560/781]  eta: 0:00:36  lr: 0.000048  training_loss: 1.1748 (1.2173)  classification_loss: 1.1747 (1.2171)  loss_mask: 0.0001 (0.0001)  time: 0.1636  data: 0.0003  max mem: 4132
[20:15:23.658158] Epoch: [72]  [580/781]  eta: 0:00:33  lr: 0.000048  training_loss: 1.1814 (1.2165)  classification_loss: 1.1814 (1.2164)  loss_mask: 0.0001 (0.0001)  time: 0.1647  data: 0.0003  max mem: 4132
[20:15:26.946601] Epoch: [72]  [600/781]  eta: 0:00:30  lr: 0.000048  training_loss: 1.2530 (1.2170)  classification_loss: 1.2529 (1.2169)  loss_mask: 0.0001 (0.0001)  time: 0.1643  data: 0.0002  max mem: 4132
[20:15:30.216192] Epoch: [72]  [620/781]  eta: 0:00:26  lr: 0.000048  training_loss: 1.1909 (1.2167)  classification_loss: 1.1908 (1.2166)  loss_mask: 0.0001 (0.0001)  time: 0.1634  data: 0.0002  max mem: 4132
[20:15:33.528239] Epoch: [72]  [640/781]  eta: 0:00:23  lr: 0.000048  training_loss: 1.1995 (1.2166)  classification_loss: 1.1992 (1.2164)  loss_mask: 0.0005 (0.0002)  time: 0.1655  data: 0.0003  max mem: 4132
[20:15:36.841098] Epoch: [72]  [660/781]  eta: 0:00:20  lr: 0.000048  training_loss: 1.2515 (1.2176)  classification_loss: 1.2513 (1.2175)  loss_mask: 0.0002 (0.0002)  time: 0.1655  data: 0.0003  max mem: 4132
[20:15:40.147944] Epoch: [72]  [680/781]  eta: 0:00:16  lr: 0.000048  training_loss: 1.2324 (1.2176)  classification_loss: 1.2321 (1.2175)  loss_mask: 0.0001 (0.0002)  time: 0.1653  data: 0.0003  max mem: 4132
[20:15:43.443526] Epoch: [72]  [700/781]  eta: 0:00:13  lr: 0.000048  training_loss: 1.2249 (1.2180)  classification_loss: 1.2249 (1.2178)  loss_mask: 0.0001 (0.0002)  time: 0.1647  data: 0.0003  max mem: 4132
[20:15:46.723028] Epoch: [72]  [720/781]  eta: 0:00:10  lr: 0.000048  training_loss: 1.2123 (1.2182)  classification_loss: 1.2122 (1.2181)  loss_mask: 0.0001 (0.0002)  time: 0.1639  data: 0.0003  max mem: 4132
[20:15:50.075198] Epoch: [72]  [740/781]  eta: 0:00:06  lr: 0.000048  training_loss: 1.2044 (1.2178)  classification_loss: 1.2043 (1.2177)  loss_mask: 0.0001 (0.0002)  time: 0.1675  data: 0.0004  max mem: 4132
[20:15:53.388385] Epoch: [72]  [760/781]  eta: 0:00:03  lr: 0.000048  training_loss: 1.2061 (1.2179)  classification_loss: 1.2060 (1.2178)  loss_mask: 0.0001 (0.0002)  time: 0.1656  data: 0.0002  max mem: 4132
[20:15:56.645650] Epoch: [72]  [780/781]  eta: 0:00:00  lr: 0.000047  training_loss: 1.2141 (1.2184)  classification_loss: 1.2140 (1.2182)  loss_mask: 0.0001 (0.0002)  time: 0.1628  data: 0.0002  max mem: 4132
[20:15:56.804493] Epoch: [72] Total time: 0:02:09 (0.1662 s / it)
[20:15:56.805831] Averaged stats: lr: 0.000047  training_loss: 1.2141 (1.2184)  classification_loss: 1.2140 (1.2182)  loss_mask: 0.0001 (0.0002)
[20:15:57.479402] Test:  [  0/157]  eta: 0:01:44  testing_loss: 0.5586 (0.5586)  acc1: 82.8125 (82.8125)  acc5: 98.4375 (98.4375)  time: 0.6688  data: 0.6392  max mem: 4132
[20:15:57.784724] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5373 (0.5289)  acc1: 82.8125 (84.3750)  acc5: 100.0000 (99.4318)  time: 0.0884  data: 0.0583  max mem: 4132
[20:15:58.078972] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4909 (0.4999)  acc1: 85.9375 (85.4167)  acc5: 100.0000 (99.5536)  time: 0.0298  data: 0.0003  max mem: 4132
[20:15:58.367247] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4998 (0.5155)  acc1: 84.3750 (84.9294)  acc5: 100.0000 (99.4456)  time: 0.0290  data: 0.0003  max mem: 4132
[20:15:58.651258] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5182 (0.5150)  acc1: 84.3750 (84.9466)  acc5: 98.4375 (99.2759)  time: 0.0285  data: 0.0002  max mem: 4132
[20:15:58.937445] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5024 (0.5097)  acc1: 84.3750 (85.1103)  acc5: 98.4375 (99.3260)  time: 0.0284  data: 0.0002  max mem: 4132
[20:15:59.228991] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4888 (0.5041)  acc1: 85.9375 (85.3996)  acc5: 100.0000 (99.3596)  time: 0.0287  data: 0.0003  max mem: 4132
[20:15:59.521385] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4847 (0.5003)  acc1: 85.9375 (85.4313)  acc5: 100.0000 (99.3178)  time: 0.0290  data: 0.0003  max mem: 4132
[20:15:59.814624] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5140 (0.5092)  acc1: 84.3750 (85.0116)  acc5: 98.4375 (99.2284)  time: 0.0291  data: 0.0003  max mem: 4132
[20:16:00.111928] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5489 (0.5080)  acc1: 84.3750 (85.0618)  acc5: 98.4375 (99.1930)  time: 0.0292  data: 0.0003  max mem: 4132
[20:16:00.400473] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.5408 (0.5136)  acc1: 82.8125 (84.7618)  acc5: 100.0000 (99.1955)  time: 0.0289  data: 0.0003  max mem: 4132
[20:16:00.686281] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5167 (0.5136)  acc1: 82.8125 (84.7269)  acc5: 100.0000 (99.1976)  time: 0.0286  data: 0.0002  max mem: 4132
[20:16:00.970112] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5127 (0.5139)  acc1: 84.3750 (84.7107)  acc5: 100.0000 (99.1994)  time: 0.0283  data: 0.0002  max mem: 4132
[20:16:01.256811] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5312 (0.5147)  acc1: 84.3750 (84.6613)  acc5: 100.0000 (99.2009)  time: 0.0284  data: 0.0002  max mem: 4132
[20:16:01.541862] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4993 (0.5120)  acc1: 84.3750 (84.7518)  acc5: 100.0000 (99.2243)  time: 0.0285  data: 0.0002  max mem: 4132
[20:16:01.823320] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4993 (0.5107)  acc1: 84.3750 (84.6544)  acc5: 100.0000 (99.2239)  time: 0.0282  data: 0.0002  max mem: 4132
[20:16:01.977218] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5049 (0.5116)  acc1: 82.8125 (84.6000)  acc5: 98.4375 (99.2100)  time: 0.0272  data: 0.0001  max mem: 4132
[20:16:02.156641] Test: Total time: 0:00:05 (0.0341 s / it)
[20:16:02.157351] * Acc@1 84.600 Acc@5 99.210 loss 0.512
[20:16:02.157636] Accuracy of the network on the 10000 test images: 84.6%
[20:16:02.157862] Max accuracy: 84.84%
[20:16:02.286403] log_dir: ./output_dir
[20:16:03.215305] Epoch: [73]  [  0/781]  eta: 0:12:03  lr: 0.000047  training_loss: 1.1687 (1.1687)  classification_loss: 1.1684 (1.1684)  loss_mask: 0.0003 (0.0003)  time: 0.9263  data: 0.7499  max mem: 4132
[20:16:06.518091] Epoch: [73]  [ 20/781]  eta: 0:02:33  lr: 0.000047  training_loss: 1.1770 (1.1770)  classification_loss: 1.1768 (1.1767)  loss_mask: 0.0002 (0.0003)  time: 0.1650  data: 0.0004  max mem: 4132
[20:16:09.855632] Epoch: [73]  [ 40/781]  eta: 0:02:16  lr: 0.000047  training_loss: 1.1875 (1.1915)  classification_loss: 1.1875 (1.1913)  loss_mask: 0.0001 (0.0002)  time: 0.1667  data: 0.0003  max mem: 4132
[20:16:13.169384] Epoch: [73]  [ 60/781]  eta: 0:02:08  lr: 0.000047  training_loss: 1.2343 (1.1973)  classification_loss: 1.2343 (1.1971)  loss_mask: 0.0001 (0.0002)  time: 0.1656  data: 0.0003  max mem: 4132
[20:16:16.486257] Epoch: [73]  [ 80/781]  eta: 0:02:02  lr: 0.000047  training_loss: 1.1818 (1.1955)  classification_loss: 1.1817 (1.1954)  loss_mask: 0.0001 (0.0002)  time: 0.1657  data: 0.0003  max mem: 4132
[20:16:19.790931] Epoch: [73]  [100/781]  eta: 0:01:57  lr: 0.000047  training_loss: 1.1644 (1.1927)  classification_loss: 1.1643 (1.1926)  loss_mask: 0.0001 (0.0002)  time: 0.1651  data: 0.0003  max mem: 4132
[20:16:23.094948] Epoch: [73]  [120/781]  eta: 0:01:53  lr: 0.000047  training_loss: 1.1530 (1.1884)  classification_loss: 1.1529 (1.1883)  loss_mask: 0.0001 (0.0001)  time: 0.1651  data: 0.0003  max mem: 4132
[20:16:26.376334] Epoch: [73]  [140/781]  eta: 0:01:49  lr: 0.000047  training_loss: 1.2320 (1.1933)  classification_loss: 1.2320 (1.1932)  loss_mask: 0.0001 (0.0001)  time: 0.1640  data: 0.0003  max mem: 4132
[20:16:29.640002] Epoch: [73]  [160/781]  eta: 0:01:45  lr: 0.000047  training_loss: 1.2263 (1.1980)  classification_loss: 1.2262 (1.1979)  loss_mask: 0.0001 (0.0001)  time: 0.1631  data: 0.0002  max mem: 4132
[20:16:32.990908] Epoch: [73]  [180/781]  eta: 0:01:41  lr: 0.000047  training_loss: 1.2324 (1.2032)  classification_loss: 1.2323 (1.2031)  loss_mask: 0.0001 (0.0001)  time: 0.1675  data: 0.0003  max mem: 4132
[20:16:36.288008] Epoch: [73]  [200/781]  eta: 0:01:38  lr: 0.000047  training_loss: 1.1971 (1.2015)  classification_loss: 1.1970 (1.2013)  loss_mask: 0.0001 (0.0002)  time: 0.1648  data: 0.0003  max mem: 4132
[20:16:39.574633] Epoch: [73]  [220/781]  eta: 0:01:34  lr: 0.000047  training_loss: 1.2227 (1.2057)  classification_loss: 1.2226 (1.2055)  loss_mask: 0.0001 (0.0002)  time: 0.1642  data: 0.0002  max mem: 4132
[20:16:42.880323] Epoch: [73]  [240/781]  eta: 0:01:31  lr: 0.000046  training_loss: 1.2038 (1.2064)  classification_loss: 1.2038 (1.2062)  loss_mask: 0.0001 (0.0002)  time: 0.1652  data: 0.0003  max mem: 4132
[20:16:46.190065] Epoch: [73]  [260/781]  eta: 0:01:27  lr: 0.000046  training_loss: 1.1768 (1.2048)  classification_loss: 1.1767 (1.2046)  loss_mask: 0.0001 (0.0002)  time: 0.1654  data: 0.0004  max mem: 4132
[20:16:49.519554] Epoch: [73]  [280/781]  eta: 0:01:24  lr: 0.000046  training_loss: 1.1491 (1.2025)  classification_loss: 1.1491 (1.2023)  loss_mask: 0.0001 (0.0002)  time: 0.1664  data: 0.0003  max mem: 4132
[20:16:52.849711] Epoch: [73]  [300/781]  eta: 0:01:20  lr: 0.000046  training_loss: 1.2147 (1.2039)  classification_loss: 1.2146 (1.2037)  loss_mask: 0.0001 (0.0002)  time: 0.1664  data: 0.0002  max mem: 4132
[20:16:56.210883] Epoch: [73]  [320/781]  eta: 0:01:17  lr: 0.000046  training_loss: 1.2016 (1.2038)  classification_loss: 1.2015 (1.2037)  loss_mask: 0.0001 (0.0002)  time: 0.1680  data: 0.0002  max mem: 4132
[20:16:59.527359] Epoch: [73]  [340/781]  eta: 0:01:13  lr: 0.000046  training_loss: 1.1519 (1.2010)  classification_loss: 1.1518 (1.2009)  loss_mask: 0.0001 (0.0002)  time: 0.1657  data: 0.0003  max mem: 4132
[20:17:02.832972] Epoch: [73]  [360/781]  eta: 0:01:10  lr: 0.000046  training_loss: 1.2179 (1.2020)  classification_loss: 1.2179 (1.2019)  loss_mask: 0.0001 (0.0002)  time: 0.1652  data: 0.0003  max mem: 4132
[20:17:06.145548] Epoch: [73]  [380/781]  eta: 0:01:07  lr: 0.000046  training_loss: 1.2084 (1.2031)  classification_loss: 1.2083 (1.2030)  loss_mask: 0.0001 (0.0002)  time: 0.1655  data: 0.0002  max mem: 4132
[20:17:09.449592] Epoch: [73]  [400/781]  eta: 0:01:03  lr: 0.000046  training_loss: 1.2398 (1.2044)  classification_loss: 1.2397 (1.2042)  loss_mask: 0.0001 (0.0002)  time: 0.1651  data: 0.0003  max mem: 4132
[20:17:12.704857] Epoch: [73]  [420/781]  eta: 0:01:00  lr: 0.000046  training_loss: 1.2203 (1.2044)  classification_loss: 1.2201 (1.2043)  loss_mask: 0.0001 (0.0002)  time: 0.1627  data: 0.0002  max mem: 4132
[20:17:16.008706] Epoch: [73]  [440/781]  eta: 0:00:56  lr: 0.000046  training_loss: 1.2349 (1.2052)  classification_loss: 1.2286 (1.2043)  loss_mask: 0.0001 (0.0008)  time: 0.1651  data: 0.0002  max mem: 4132
[20:17:19.342051] Epoch: [73]  [460/781]  eta: 0:00:53  lr: 0.000046  training_loss: 1.2272 (1.2083)  classification_loss: 1.1748 (1.2027)  loss_mask: 0.0248 (0.0056)  time: 0.1666  data: 0.0003  max mem: 4132
[20:17:22.647261] Epoch: [73]  [480/781]  eta: 0:00:50  lr: 0.000045  training_loss: 1.2856 (1.2120)  classification_loss: 1.1842 (1.2024)  loss_mask: 0.0501 (0.0096)  time: 0.1652  data: 0.0003  max mem: 4132
[20:17:25.998642] Epoch: [73]  [500/781]  eta: 0:00:46  lr: 0.000045  training_loss: 1.2460 (1.2132)  classification_loss: 1.2052 (1.2031)  loss_mask: 0.0133 (0.0101)  time: 0.1675  data: 0.0003  max mem: 4132
[20:17:29.282588] Epoch: [73]  [520/781]  eta: 0:00:43  lr: 0.000045  training_loss: 1.1893 (1.2134)  classification_loss: 1.1838 (1.2033)  loss_mask: 0.0056 (0.0100)  time: 0.1641  data: 0.0003  max mem: 4132
[20:17:32.607190] Epoch: [73]  [540/781]  eta: 0:00:40  lr: 0.000045  training_loss: 1.1803 (1.2125)  classification_loss: 1.1789 (1.2028)  loss_mask: 0.0019 (0.0098)  time: 0.1661  data: 0.0003  max mem: 4132
[20:17:35.890948] Epoch: [73]  [560/781]  eta: 0:00:36  lr: 0.000045  training_loss: 1.1734 (1.2118)  classification_loss: 1.1719 (1.2024)  loss_mask: 0.0014 (0.0095)  time: 0.1641  data: 0.0003  max mem: 4132
[20:17:39.238542] Epoch: [73]  [580/781]  eta: 0:00:33  lr: 0.000045  training_loss: 1.2057 (1.2127)  classification_loss: 1.2041 (1.2035)  loss_mask: 0.0013 (0.0092)  time: 0.1673  data: 0.0004  max mem: 4132
[20:17:42.569482] Epoch: [73]  [600/781]  eta: 0:00:30  lr: 0.000045  training_loss: 1.1953 (1.2118)  classification_loss: 1.1940 (1.2028)  loss_mask: 0.0007 (0.0089)  time: 0.1664  data: 0.0003  max mem: 4132
[20:17:45.867905] Epoch: [73]  [620/781]  eta: 0:00:26  lr: 0.000045  training_loss: 1.1743 (1.2108)  classification_loss: 1.1738 (1.2022)  loss_mask: 0.0006 (0.0086)  time: 0.1648  data: 0.0003  max mem: 4132
[20:17:49.160289] Epoch: [73]  [640/781]  eta: 0:00:23  lr: 0.000045  training_loss: 1.1896 (1.2110)  classification_loss: 1.1887 (1.2026)  loss_mask: 0.0006 (0.0084)  time: 0.1645  data: 0.0003  max mem: 4132
[20:17:52.454677] Epoch: [73]  [660/781]  eta: 0:00:20  lr: 0.000045  training_loss: 1.1992 (1.2110)  classification_loss: 1.1988 (1.2029)  loss_mask: 0.0006 (0.0082)  time: 0.1646  data: 0.0003  max mem: 4132
[20:17:55.750235] Epoch: [73]  [680/781]  eta: 0:00:16  lr: 0.000045  training_loss: 1.2319 (1.2114)  classification_loss: 1.2315 (1.2035)  loss_mask: 0.0005 (0.0079)  time: 0.1647  data: 0.0003  max mem: 4132
[20:17:59.020927] Epoch: [73]  [700/781]  eta: 0:00:13  lr: 0.000045  training_loss: 1.2293 (1.2121)  classification_loss: 1.2285 (1.2044)  loss_mask: 0.0006 (0.0077)  time: 0.1634  data: 0.0003  max mem: 4132
[20:18:02.336536] Epoch: [73]  [720/781]  eta: 0:00:10  lr: 0.000044  training_loss: 1.2222 (1.2131)  classification_loss: 1.2219 (1.2056)  loss_mask: 0.0004 (0.0075)  time: 0.1657  data: 0.0003  max mem: 4132
[20:18:05.629262] Epoch: [73]  [740/781]  eta: 0:00:06  lr: 0.000044  training_loss: 1.1740 (1.2128)  classification_loss: 1.1734 (1.2055)  loss_mask: 0.0005 (0.0073)  time: 0.1645  data: 0.0003  max mem: 4132
[20:18:08.951864] Epoch: [73]  [760/781]  eta: 0:00:03  lr: 0.000044  training_loss: 1.2345 (1.2131)  classification_loss: 1.2340 (1.2060)  loss_mask: 0.0005 (0.0072)  time: 0.1660  data: 0.0004  max mem: 4132
[20:18:12.270297] Epoch: [73]  [780/781]  eta: 0:00:00  lr: 0.000044  training_loss: 1.1932 (1.2126)  classification_loss: 1.1930 (1.2056)  loss_mask: 0.0003 (0.0070)  time: 0.1658  data: 0.0002  max mem: 4132
[20:18:12.489518] Epoch: [73] Total time: 0:02:10 (0.1667 s / it)
[20:18:12.490970] Averaged stats: lr: 0.000044  training_loss: 1.1932 (1.2126)  classification_loss: 1.1930 (1.2056)  loss_mask: 0.0003 (0.0070)
[20:18:13.211921] Test:  [  0/157]  eta: 0:01:52  testing_loss: 0.4467 (0.4467)  acc1: 89.0625 (89.0625)  acc5: 96.8750 (96.8750)  time: 0.7162  data: 0.6846  max mem: 4132
[20:18:13.503230] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.5053 (0.5160)  acc1: 84.3750 (84.6591)  acc5: 100.0000 (99.2898)  time: 0.0913  data: 0.0625  max mem: 4132
[20:18:13.791138] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4731 (0.4870)  acc1: 85.9375 (85.7143)  acc5: 100.0000 (99.4792)  time: 0.0287  data: 0.0002  max mem: 4132
[20:18:14.082689] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4669 (0.5038)  acc1: 85.9375 (85.0302)  acc5: 100.0000 (99.3952)  time: 0.0288  data: 0.0002  max mem: 4132
[20:18:14.377168] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5290 (0.5061)  acc1: 84.3750 (85.1372)  acc5: 98.4375 (99.2759)  time: 0.0291  data: 0.0003  max mem: 4132
[20:18:14.678505] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4888 (0.5024)  acc1: 85.9375 (85.3248)  acc5: 100.0000 (99.3260)  time: 0.0296  data: 0.0003  max mem: 4132
[20:18:14.970956] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4742 (0.4988)  acc1: 85.9375 (85.3227)  acc5: 100.0000 (99.3084)  time: 0.0295  data: 0.0003  max mem: 4132
[20:18:15.259249] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4771 (0.4970)  acc1: 85.9375 (85.5414)  acc5: 100.0000 (99.3178)  time: 0.0289  data: 0.0003  max mem: 4132
[20:18:15.554799] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4803 (0.5041)  acc1: 85.9375 (85.3781)  acc5: 98.4375 (99.1705)  time: 0.0290  data: 0.0003  max mem: 4132
[20:18:15.854069] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5073 (0.5025)  acc1: 85.9375 (85.4052)  acc5: 98.4375 (99.1758)  time: 0.0294  data: 0.0003  max mem: 4132
[20:18:16.148808] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.5073 (0.5044)  acc1: 85.9375 (85.3806)  acc5: 100.0000 (99.1955)  time: 0.0295  data: 0.0002  max mem: 4132
[20:18:16.438108] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5205 (0.5049)  acc1: 85.9375 (85.3604)  acc5: 100.0000 (99.1836)  time: 0.0290  data: 0.0003  max mem: 4132
[20:18:16.725379] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4945 (0.5040)  acc1: 85.9375 (85.2402)  acc5: 100.0000 (99.1606)  time: 0.0286  data: 0.0003  max mem: 4132
[20:18:17.014898] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4917 (0.5041)  acc1: 84.3750 (85.1026)  acc5: 100.0000 (99.1770)  time: 0.0287  data: 0.0003  max mem: 4132
[20:18:17.305807] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4513 (0.5003)  acc1: 85.9375 (85.2283)  acc5: 100.0000 (99.2132)  time: 0.0288  data: 0.0006  max mem: 4132
[20:18:17.588892] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4777 (0.5000)  acc1: 84.3750 (85.2028)  acc5: 100.0000 (99.2239)  time: 0.0285  data: 0.0005  max mem: 4132
[20:18:17.743243] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5022 (0.5002)  acc1: 84.3750 (85.2000)  acc5: 100.0000 (99.2000)  time: 0.0274  data: 0.0002  max mem: 4132
[20:18:17.899828] Test: Total time: 0:00:05 (0.0344 s / it)
[20:18:17.900302] * Acc@1 85.200 Acc@5 99.200 loss 0.500
[20:18:17.900609] Accuracy of the network on the 10000 test images: 85.2%
[20:18:17.900829] Max accuracy: 85.20%
[20:18:18.119037] log_dir: ./output_dir
[20:18:19.033669] Epoch: [74]  [  0/781]  eta: 0:11:52  lr: 0.000044  training_loss: 1.1143 (1.1143)  classification_loss: 1.1139 (1.1139)  loss_mask: 0.0004 (0.0004)  time: 0.9125  data: 0.7187  max mem: 4132
[20:18:22.347584] Epoch: [74]  [ 20/781]  eta: 0:02:33  lr: 0.000044  training_loss: 1.1908 (1.1922)  classification_loss: 1.1901 (1.1918)  loss_mask: 0.0004 (0.0004)  time: 0.1656  data: 0.0002  max mem: 4132
[20:18:25.623339] Epoch: [74]  [ 40/781]  eta: 0:02:15  lr: 0.000044  training_loss: 1.2301 (1.2092)  classification_loss: 1.2295 (1.2088)  loss_mask: 0.0004 (0.0004)  time: 0.1637  data: 0.0002  max mem: 4132
[20:18:28.910367] Epoch: [74]  [ 60/781]  eta: 0:02:07  lr: 0.000044  training_loss: 1.1795 (1.2062)  classification_loss: 1.1795 (1.2058)  loss_mask: 0.0003 (0.0004)  time: 0.1643  data: 0.0003  max mem: 4132
[20:18:32.181027] Epoch: [74]  [ 80/781]  eta: 0:02:01  lr: 0.000044  training_loss: 1.2245 (1.2088)  classification_loss: 1.2242 (1.2085)  loss_mask: 0.0002 (0.0004)  time: 0.1634  data: 0.0002  max mem: 4132
[20:18:35.487709] Epoch: [74]  [100/781]  eta: 0:01:57  lr: 0.000044  training_loss: 1.1919 (1.2070)  classification_loss: 1.1916 (1.2067)  loss_mask: 0.0003 (0.0003)  time: 0.1652  data: 0.0003  max mem: 4132
[20:18:38.787285] Epoch: [74]  [120/781]  eta: 0:01:52  lr: 0.000044  training_loss: 1.1872 (1.2018)  classification_loss: 1.1868 (1.2015)  loss_mask: 0.0003 (0.0003)  time: 0.1649  data: 0.0003  max mem: 4132
[20:18:42.106151] Epoch: [74]  [140/781]  eta: 0:01:48  lr: 0.000044  training_loss: 1.1425 (1.1969)  classification_loss: 1.1424 (1.1966)  loss_mask: 0.0002 (0.0003)  time: 0.1658  data: 0.0003  max mem: 4132
[20:18:45.409065] Epoch: [74]  [160/781]  eta: 0:01:45  lr: 0.000044  training_loss: 1.1465 (1.1956)  classification_loss: 1.1463 (1.1953)  loss_mask: 0.0003 (0.0003)  time: 0.1651  data: 0.0003  max mem: 4132
[20:18:48.693768] Epoch: [74]  [180/781]  eta: 0:01:41  lr: 0.000044  training_loss: 1.2121 (1.1996)  classification_loss: 1.2119 (1.1993)  loss_mask: 0.0003 (0.0003)  time: 0.1642  data: 0.0003  max mem: 4132
[20:18:51.982458] Epoch: [74]  [200/781]  eta: 0:01:37  lr: 0.000043  training_loss: 1.1870 (1.2008)  classification_loss: 1.1868 (1.2005)  loss_mask: 0.0002 (0.0003)  time: 0.1643  data: 0.0003  max mem: 4132
[20:18:55.268517] Epoch: [74]  [220/781]  eta: 0:01:34  lr: 0.000043  training_loss: 1.1842 (1.2026)  classification_loss: 1.1840 (1.2023)  loss_mask: 0.0002 (0.0003)  time: 0.1642  data: 0.0003  max mem: 4132
[20:18:58.563444] Epoch: [74]  [240/781]  eta: 0:01:30  lr: 0.000043  training_loss: 1.1934 (1.2030)  classification_loss: 1.1931 (1.2027)  loss_mask: 0.0002 (0.0003)  time: 0.1647  data: 0.0003  max mem: 4132
[20:19:01.866666] Epoch: [74]  [260/781]  eta: 0:01:27  lr: 0.000043  training_loss: 1.2023 (1.2030)  classification_loss: 1.2021 (1.2027)  loss_mask: 0.0002 (0.0003)  time: 0.1651  data: 0.0003  max mem: 4132
[20:19:05.156337] Epoch: [74]  [280/781]  eta: 0:01:23  lr: 0.000043  training_loss: 1.2020 (1.2027)  classification_loss: 1.2017 (1.2024)  loss_mask: 0.0002 (0.0003)  time: 0.1644  data: 0.0003  max mem: 4132
[20:19:08.444992] Epoch: [74]  [300/781]  eta: 0:01:20  lr: 0.000043  training_loss: 1.2519 (1.2051)  classification_loss: 1.2516 (1.2049)  loss_mask: 0.0002 (0.0003)  time: 0.1643  data: 0.0003  max mem: 4132
[20:19:11.738008] Epoch: [74]  [320/781]  eta: 0:01:16  lr: 0.000043  training_loss: 1.2115 (1.2070)  classification_loss: 1.2112 (1.2068)  loss_mask: 0.0002 (0.0003)  time: 0.1644  data: 0.0004  max mem: 4132
[20:19:15.088854] Epoch: [74]  [340/781]  eta: 0:01:13  lr: 0.000043  training_loss: 1.1896 (1.2073)  classification_loss: 1.1892 (1.2070)  loss_mask: 0.0002 (0.0003)  time: 0.1674  data: 0.0003  max mem: 4132
[20:19:18.402401] Epoch: [74]  [360/781]  eta: 0:01:10  lr: 0.000043  training_loss: 1.1912 (1.2073)  classification_loss: 1.1911 (1.2070)  loss_mask: 0.0002 (0.0003)  time: 0.1656  data: 0.0003  max mem: 4132
[20:19:21.669554] Epoch: [74]  [380/781]  eta: 0:01:06  lr: 0.000043  training_loss: 1.2263 (1.2086)  classification_loss: 1.2261 (1.2083)  loss_mask: 0.0002 (0.0003)  time: 0.1633  data: 0.0003  max mem: 4132
[20:19:25.001561] Epoch: [74]  [400/781]  eta: 0:01:03  lr: 0.000043  training_loss: 1.1777 (1.2068)  classification_loss: 1.1775 (1.2065)  loss_mask: 0.0002 (0.0003)  time: 0.1665  data: 0.0003  max mem: 4132
[20:19:28.321647] Epoch: [74]  [420/781]  eta: 0:01:00  lr: 0.000043  training_loss: 1.2069 (1.2069)  classification_loss: 1.2067 (1.2066)  loss_mask: 0.0002 (0.0003)  time: 0.1659  data: 0.0002  max mem: 4132
[20:19:31.599955] Epoch: [74]  [440/781]  eta: 0:00:56  lr: 0.000043  training_loss: 1.1873 (1.2064)  classification_loss: 1.1870 (1.2061)  loss_mask: 0.0002 (0.0003)  time: 0.1638  data: 0.0003  max mem: 4132
[20:19:34.901554] Epoch: [74]  [460/781]  eta: 0:00:53  lr: 0.000042  training_loss: 1.1349 (1.2045)  classification_loss: 1.1346 (1.2043)  loss_mask: 0.0002 (0.0003)  time: 0.1650  data: 0.0003  max mem: 4132
[20:19:38.170866] Epoch: [74]  [480/781]  eta: 0:00:50  lr: 0.000042  training_loss: 1.2187 (1.2052)  classification_loss: 1.2186 (1.2050)  loss_mask: 0.0002 (0.0003)  time: 0.1634  data: 0.0003  max mem: 4132
[20:19:41.426214] Epoch: [74]  [500/781]  eta: 0:00:46  lr: 0.000042  training_loss: 1.1404 (1.2035)  classification_loss: 1.1402 (1.2032)  loss_mask: 0.0001 (0.0003)  time: 0.1627  data: 0.0003  max mem: 4132
[20:19:44.703262] Epoch: [74]  [520/781]  eta: 0:00:43  lr: 0.000042  training_loss: 1.1976 (1.2032)  classification_loss: 1.1974 (1.2029)  loss_mask: 0.0002 (0.0002)  time: 0.1638  data: 0.0003  max mem: 4132
[20:19:48.003442] Epoch: [74]  [540/781]  eta: 0:00:40  lr: 0.000042  training_loss: 1.2096 (1.2039)  classification_loss: 1.2095 (1.2036)  loss_mask: 0.0002 (0.0002)  time: 0.1649  data: 0.0003  max mem: 4132
[20:19:51.289354] Epoch: [74]  [560/781]  eta: 0:00:36  lr: 0.000042  training_loss: 1.1964 (1.2041)  classification_loss: 1.1962 (1.2038)  loss_mask: 0.0001 (0.0002)  time: 0.1642  data: 0.0002  max mem: 4132
[20:19:54.597618] Epoch: [74]  [580/781]  eta: 0:00:33  lr: 0.000042  training_loss: 1.1885 (1.2040)  classification_loss: 1.1884 (1.2037)  loss_mask: 0.0002 (0.0002)  time: 0.1653  data: 0.0003  max mem: 4132
[20:19:57.920706] Epoch: [74]  [600/781]  eta: 0:00:30  lr: 0.000042  training_loss: 1.1744 (1.2036)  classification_loss: 1.1742 (1.2034)  loss_mask: 0.0001 (0.0002)  time: 0.1661  data: 0.0002  max mem: 4132
[20:20:01.249207] Epoch: [74]  [620/781]  eta: 0:00:26  lr: 0.000042  training_loss: 1.2086 (1.2043)  classification_loss: 1.2083 (1.2041)  loss_mask: 0.0002 (0.0002)  time: 0.1663  data: 0.0003  max mem: 4132
[20:20:04.572273] Epoch: [74]  [640/781]  eta: 0:00:23  lr: 0.000042  training_loss: 1.2205 (1.2048)  classification_loss: 1.2204 (1.2045)  loss_mask: 0.0001 (0.0002)  time: 0.1660  data: 0.0004  max mem: 4132
[20:20:07.868010] Epoch: [74]  [660/781]  eta: 0:00:20  lr: 0.000042  training_loss: 1.1944 (1.2048)  classification_loss: 1.1942 (1.2046)  loss_mask: 0.0002 (0.0002)  time: 0.1647  data: 0.0003  max mem: 4132
[20:20:11.186563] Epoch: [74]  [680/781]  eta: 0:00:16  lr: 0.000042  training_loss: 1.2641 (1.2055)  classification_loss: 1.2639 (1.2052)  loss_mask: 0.0001 (0.0002)  time: 0.1658  data: 0.0002  max mem: 4132
[20:20:14.486579] Epoch: [74]  [700/781]  eta: 0:00:13  lr: 0.000041  training_loss: 1.1990 (1.2054)  classification_loss: 1.1988 (1.2052)  loss_mask: 0.0001 (0.0002)  time: 0.1649  data: 0.0003  max mem: 4132
[20:20:17.764690] Epoch: [74]  [720/781]  eta: 0:00:10  lr: 0.000041  training_loss: 1.1534 (1.2051)  classification_loss: 1.1532 (1.2049)  loss_mask: 0.0002 (0.0002)  time: 0.1638  data: 0.0003  max mem: 4132
[20:20:21.132183] Epoch: [74]  [740/781]  eta: 0:00:06  lr: 0.000041  training_loss: 1.1646 (1.2049)  classification_loss: 1.1645 (1.2046)  loss_mask: 0.0001 (0.0002)  time: 0.1682  data: 0.0003  max mem: 4132
[20:20:24.405452] Epoch: [74]  [760/781]  eta: 0:00:03  lr: 0.000041  training_loss: 1.1958 (1.2051)  classification_loss: 1.1956 (1.2049)  loss_mask: 0.0002 (0.0002)  time: 0.1636  data: 0.0002  max mem: 4132
[20:20:27.672172] Epoch: [74]  [780/781]  eta: 0:00:00  lr: 0.000041  training_loss: 1.2142 (1.2055)  classification_loss: 1.2141 (1.2052)  loss_mask: 0.0002 (0.0002)  time: 0.1632  data: 0.0002  max mem: 4132
[20:20:27.863342] Epoch: [74] Total time: 0:02:09 (0.1661 s / it)
[20:20:27.864886] Averaged stats: lr: 0.000041  training_loss: 1.2142 (1.2055)  classification_loss: 1.2141 (1.2052)  loss_mask: 0.0002 (0.0002)
[20:20:28.616898] Test:  [  0/157]  eta: 0:01:57  testing_loss: 0.5199 (0.5199)  acc1: 85.9375 (85.9375)  acc5: 96.8750 (96.8750)  time: 0.7475  data: 0.7180  max mem: 4132
[20:20:28.908733] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.5199 (0.5207)  acc1: 82.8125 (83.3807)  acc5: 98.4375 (99.0057)  time: 0.0942  data: 0.0655  max mem: 4132
[20:20:29.195319] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5010 (0.4903)  acc1: 82.8125 (84.8958)  acc5: 100.0000 (99.1815)  time: 0.0287  data: 0.0002  max mem: 4132
[20:20:29.481050] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4925 (0.5074)  acc1: 85.9375 (84.6774)  acc5: 100.0000 (99.0927)  time: 0.0285  data: 0.0002  max mem: 4132
[20:20:29.769850] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5100 (0.5119)  acc1: 84.3750 (84.6799)  acc5: 100.0000 (99.1235)  time: 0.0286  data: 0.0002  max mem: 4132
[20:20:30.057455] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4949 (0.5037)  acc1: 84.3750 (85.1103)  acc5: 100.0000 (99.2034)  time: 0.0287  data: 0.0002  max mem: 4132
[20:20:30.345332] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4560 (0.4999)  acc1: 84.3750 (85.1434)  acc5: 100.0000 (99.2059)  time: 0.0286  data: 0.0002  max mem: 4132
[20:20:30.631467] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5002 (0.4980)  acc1: 84.3750 (85.1452)  acc5: 100.0000 (99.2298)  time: 0.0286  data: 0.0002  max mem: 4132
[20:20:30.918249] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5077 (0.5054)  acc1: 82.8125 (84.7994)  acc5: 98.4375 (99.1898)  time: 0.0285  data: 0.0002  max mem: 4132
[20:20:31.211068] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5338 (0.5045)  acc1: 82.8125 (84.7527)  acc5: 98.4375 (99.2102)  time: 0.0288  data: 0.0002  max mem: 4132
[20:20:31.500567] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.5267 (0.5068)  acc1: 82.8125 (84.6225)  acc5: 100.0000 (99.1955)  time: 0.0289  data: 0.0002  max mem: 4132
[20:20:31.790987] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5284 (0.5078)  acc1: 84.3750 (84.6847)  acc5: 100.0000 (99.1836)  time: 0.0288  data: 0.0002  max mem: 4132
[20:20:32.080903] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5166 (0.5077)  acc1: 85.9375 (84.7624)  acc5: 100.0000 (99.1477)  time: 0.0288  data: 0.0002  max mem: 4132
[20:20:32.375765] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5166 (0.5085)  acc1: 84.3750 (84.6970)  acc5: 100.0000 (99.1531)  time: 0.0290  data: 0.0004  max mem: 4132
[20:20:32.664164] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4775 (0.5052)  acc1: 85.9375 (84.8737)  acc5: 100.0000 (99.1689)  time: 0.0290  data: 0.0004  max mem: 4132
[20:20:32.951170] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4831 (0.5057)  acc1: 85.9375 (84.8406)  acc5: 100.0000 (99.1825)  time: 0.0286  data: 0.0002  max mem: 4132
[20:20:33.106204] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4902 (0.5069)  acc1: 84.3750 (84.7300)  acc5: 100.0000 (99.1800)  time: 0.0276  data: 0.0002  max mem: 4132
[20:20:33.274962] Test: Total time: 0:00:05 (0.0344 s / it)
[20:20:33.275440] * Acc@1 84.730 Acc@5 99.180 loss 0.507
[20:20:33.275869] Accuracy of the network on the 10000 test images: 84.7%
[20:20:33.276079] Max accuracy: 85.20%
[20:20:33.403854] log_dir: ./output_dir
[20:20:34.323114] Epoch: [75]  [  0/781]  eta: 0:11:56  lr: 0.000041  training_loss: 1.1977 (1.1977)  classification_loss: 1.1976 (1.1976)  loss_mask: 0.0001 (0.0001)  time: 0.9171  data: 0.7110  max mem: 4132
[20:20:37.617090] Epoch: [75]  [ 20/781]  eta: 0:02:32  lr: 0.000041  training_loss: 1.1769 (1.1739)  classification_loss: 1.1768 (1.1738)  loss_mask: 0.0001 (0.0001)  time: 0.1645  data: 0.0003  max mem: 4132
[20:20:40.904905] Epoch: [75]  [ 40/781]  eta: 0:02:15  lr: 0.000041  training_loss: 1.2114 (1.1922)  classification_loss: 1.2112 (1.1920)  loss_mask: 0.0001 (0.0001)  time: 0.1643  data: 0.0002  max mem: 4132
[20:20:44.212873] Epoch: [75]  [ 60/781]  eta: 0:02:07  lr: 0.000041  training_loss: 1.1940 (1.1954)  classification_loss: 1.1939 (1.1953)  loss_mask: 0.0002 (0.0001)  time: 0.1653  data: 0.0003  max mem: 4132
[20:20:47.528180] Epoch: [75]  [ 80/781]  eta: 0:02:02  lr: 0.000041  training_loss: 1.2077 (1.1991)  classification_loss: 1.2076 (1.1989)  loss_mask: 0.0001 (0.0001)  time: 0.1657  data: 0.0003  max mem: 4132
[20:20:50.806405] Epoch: [75]  [100/781]  eta: 0:01:57  lr: 0.000041  training_loss: 1.1771 (1.2004)  classification_loss: 1.1771 (1.2003)  loss_mask: 0.0001 (0.0001)  time: 0.1638  data: 0.0002  max mem: 4132
[20:20:54.098308] Epoch: [75]  [120/781]  eta: 0:01:52  lr: 0.000041  training_loss: 1.2047 (1.2021)  classification_loss: 1.2046 (1.2019)  loss_mask: 0.0001 (0.0001)  time: 0.1645  data: 0.0004  max mem: 4132
[20:20:57.363846] Epoch: [75]  [140/781]  eta: 0:01:48  lr: 0.000041  training_loss: 1.1768 (1.1970)  classification_loss: 1.1767 (1.1969)  loss_mask: 0.0001 (0.0001)  time: 0.1632  data: 0.0002  max mem: 4132
[20:21:00.665171] Epoch: [75]  [160/781]  eta: 0:01:45  lr: 0.000041  training_loss: 1.1672 (1.1979)  classification_loss: 1.1671 (1.1978)  loss_mask: 0.0001 (0.0001)  time: 0.1650  data: 0.0003  max mem: 4132
[20:21:03.960236] Epoch: [75]  [180/781]  eta: 0:01:41  lr: 0.000040  training_loss: 1.2055 (1.1998)  classification_loss: 1.2054 (1.1996)  loss_mask: 0.0001 (0.0001)  time: 0.1647  data: 0.0003  max mem: 4132
[20:21:07.250810] Epoch: [75]  [200/781]  eta: 0:01:37  lr: 0.000040  training_loss: 1.1646 (1.1977)  classification_loss: 1.1645 (1.1976)  loss_mask: 0.0001 (0.0001)  time: 0.1645  data: 0.0003  max mem: 4132
[20:21:10.564343] Epoch: [75]  [220/781]  eta: 0:01:34  lr: 0.000040  training_loss: 1.2204 (1.2007)  classification_loss: 1.2203 (1.1993)  loss_mask: 0.0002 (0.0014)  time: 0.1656  data: 0.0003  max mem: 4132
[20:21:13.855147] Epoch: [75]  [240/781]  eta: 0:01:30  lr: 0.000040  training_loss: 1.2953 (1.2071)  classification_loss: 1.2137 (1.2003)  loss_mask: 0.0233 (0.0068)  time: 0.1644  data: 0.0003  max mem: 4132
[20:21:17.162105] Epoch: [75]  [260/781]  eta: 0:01:27  lr: 0.000040  training_loss: 1.2351 (1.2099)  classification_loss: 1.1868 (1.2013)  loss_mask: 0.0198 (0.0087)  time: 0.1652  data: 0.0003  max mem: 4132
[20:21:20.446387] Epoch: [75]  [280/781]  eta: 0:01:23  lr: 0.000040  training_loss: 1.1980 (1.2096)  classification_loss: 1.1976 (1.2013)  loss_mask: 0.0018 (0.0083)  time: 0.1641  data: 0.0003  max mem: 4132
[20:21:23.741324] Epoch: [75]  [300/781]  eta: 0:01:20  lr: 0.000040  training_loss: 1.1914 (1.2117)  classification_loss: 1.1910 (1.2038)  loss_mask: 0.0014 (0.0079)  time: 0.1646  data: 0.0003  max mem: 4132
[20:21:27.021819] Epoch: [75]  [320/781]  eta: 0:01:16  lr: 0.000040  training_loss: 1.1950 (1.2110)  classification_loss: 1.1943 (1.2036)  loss_mask: 0.0006 (0.0074)  time: 0.1639  data: 0.0002  max mem: 4132
[20:21:30.328149] Epoch: [75]  [340/781]  eta: 0:01:13  lr: 0.000040  training_loss: 1.1627 (1.2088)  classification_loss: 1.1620 (1.2018)  loss_mask: 0.0004 (0.0070)  time: 0.1652  data: 0.0003  max mem: 4132
[20:21:33.621181] Epoch: [75]  [360/781]  eta: 0:01:10  lr: 0.000040  training_loss: 1.1957 (1.2086)  classification_loss: 1.1954 (1.2019)  loss_mask: 0.0004 (0.0066)  time: 0.1645  data: 0.0003  max mem: 4132
[20:21:36.907141] Epoch: [75]  [380/781]  eta: 0:01:06  lr: 0.000040  training_loss: 1.1977 (1.2098)  classification_loss: 1.1975 (1.2035)  loss_mask: 0.0003 (0.0063)  time: 0.1642  data: 0.0003  max mem: 4132
[20:21:40.253685] Epoch: [75]  [400/781]  eta: 0:01:03  lr: 0.000040  training_loss: 1.2153 (1.2099)  classification_loss: 1.2148 (1.2039)  loss_mask: 0.0004 (0.0060)  time: 0.1672  data: 0.0003  max mem: 4132
[20:21:43.517727] Epoch: [75]  [420/781]  eta: 0:01:00  lr: 0.000040  training_loss: 1.2069 (1.2098)  classification_loss: 1.2067 (1.2040)  loss_mask: 0.0003 (0.0057)  time: 0.1631  data: 0.0003  max mem: 4132
[20:21:46.830418] Epoch: [75]  [440/781]  eta: 0:00:56  lr: 0.000039  training_loss: 1.1618 (1.2076)  classification_loss: 1.1616 (1.2020)  loss_mask: 0.0003 (0.0055)  time: 0.1655  data: 0.0003  max mem: 4132
[20:21:50.172487] Epoch: [75]  [460/781]  eta: 0:00:53  lr: 0.000039  training_loss: 1.1319 (1.2052)  classification_loss: 1.1316 (1.1999)  loss_mask: 0.0002 (0.0053)  time: 0.1670  data: 0.0003  max mem: 4132
[20:21:53.454146] Epoch: [75]  [480/781]  eta: 0:00:50  lr: 0.000039  training_loss: 1.1745 (1.2042)  classification_loss: 1.1740 (1.1992)  loss_mask: 0.0003 (0.0051)  time: 0.1640  data: 0.0002  max mem: 4132
[20:21:56.760208] Epoch: [75]  [500/781]  eta: 0:00:46  lr: 0.000039  training_loss: 1.2089 (1.2042)  classification_loss: 1.2087 (1.1993)  loss_mask: 0.0002 (0.0049)  time: 0.1652  data: 0.0002  max mem: 4132
[20:22:00.065284] Epoch: [75]  [520/781]  eta: 0:00:43  lr: 0.000039  training_loss: 1.1432 (1.2031)  classification_loss: 1.1431 (1.1984)  loss_mask: 0.0002 (0.0047)  time: 0.1651  data: 0.0003  max mem: 4132
[20:22:03.345063] Epoch: [75]  [540/781]  eta: 0:00:40  lr: 0.000039  training_loss: 1.1992 (1.2034)  classification_loss: 1.1990 (1.1989)  loss_mask: 0.0002 (0.0045)  time: 0.1639  data: 0.0004  max mem: 4132
[20:22:06.616559] Epoch: [75]  [560/781]  eta: 0:00:36  lr: 0.000039  training_loss: 1.2216 (1.2037)  classification_loss: 1.2215 (1.1993)  loss_mask: 0.0002 (0.0044)  time: 0.1635  data: 0.0003  max mem: 4132
[20:22:09.872814] Epoch: [75]  [580/781]  eta: 0:00:33  lr: 0.000039  training_loss: 1.1792 (1.2034)  classification_loss: 1.1791 (1.1991)  loss_mask: 0.0002 (0.0042)  time: 0.1627  data: 0.0002  max mem: 4132
[20:22:13.197513] Epoch: [75]  [600/781]  eta: 0:00:30  lr: 0.000039  training_loss: 1.2017 (1.2036)  classification_loss: 1.2014 (1.1995)  loss_mask: 0.0002 (0.0041)  time: 0.1661  data: 0.0002  max mem: 4132
[20:22:16.481070] Epoch: [75]  [620/781]  eta: 0:00:26  lr: 0.000039  training_loss: 1.2111 (1.2045)  classification_loss: 1.2108 (1.2005)  loss_mask: 0.0002 (0.0040)  time: 0.1641  data: 0.0002  max mem: 4132
[20:22:19.736147] Epoch: [75]  [640/781]  eta: 0:00:23  lr: 0.000039  training_loss: 1.1872 (1.2041)  classification_loss: 1.1870 (1.2002)  loss_mask: 0.0002 (0.0039)  time: 0.1627  data: 0.0002  max mem: 4132
[20:22:22.996182] Epoch: [75]  [660/781]  eta: 0:00:20  lr: 0.000039  training_loss: 1.1687 (1.2038)  classification_loss: 1.1685 (1.2001)  loss_mask: 0.0002 (0.0038)  time: 0.1629  data: 0.0003  max mem: 4132
[20:22:26.288894] Epoch: [75]  [680/781]  eta: 0:00:16  lr: 0.000039  training_loss: 1.2179 (1.2046)  classification_loss: 1.2178 (1.2010)  loss_mask: 0.0002 (0.0036)  time: 0.1645  data: 0.0003  max mem: 4132
[20:22:29.613704] Epoch: [75]  [700/781]  eta: 0:00:13  lr: 0.000039  training_loss: 1.1728 (1.2044)  classification_loss: 1.1726 (1.2008)  loss_mask: 0.0002 (0.0035)  time: 0.1661  data: 0.0002  max mem: 4132
[20:22:32.932499] Epoch: [75]  [720/781]  eta: 0:00:10  lr: 0.000038  training_loss: 1.1848 (1.2052)  classification_loss: 1.1847 (1.2018)  loss_mask: 0.0001 (0.0035)  time: 0.1658  data: 0.0003  max mem: 4132
[20:22:36.214253] Epoch: [75]  [740/781]  eta: 0:00:06  lr: 0.000038  training_loss: 1.1815 (1.2046)  classification_loss: 1.1813 (1.2013)  loss_mask: 0.0002 (0.0034)  time: 0.1640  data: 0.0003  max mem: 4132
[20:22:39.507010] Epoch: [75]  [760/781]  eta: 0:00:03  lr: 0.000038  training_loss: 1.2016 (1.2044)  classification_loss: 1.2014 (1.2011)  loss_mask: 0.0002 (0.0033)  time: 0.1645  data: 0.0003  max mem: 4132
[20:22:42.788898] Epoch: [75]  [780/781]  eta: 0:00:00  lr: 0.000038  training_loss: 1.1809 (1.2046)  classification_loss: 1.1807 (1.2014)  loss_mask: 0.0001 (0.0032)  time: 0.1640  data: 0.0002  max mem: 4132
[20:22:42.972193] Epoch: [75] Total time: 0:02:09 (0.1659 s / it)
[20:22:42.972681] Averaged stats: lr: 0.000038  training_loss: 1.1809 (1.2046)  classification_loss: 1.1807 (1.2014)  loss_mask: 0.0001 (0.0032)
[20:22:43.709558] Test:  [  0/157]  eta: 0:01:55  testing_loss: 0.4712 (0.4712)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (100.0000)  time: 0.7326  data: 0.7017  max mem: 4132
[20:22:43.999901] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.5080 (0.5041)  acc1: 82.8125 (83.9489)  acc5: 100.0000 (99.5739)  time: 0.0927  data: 0.0641  max mem: 4132
[20:22:44.287933] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4626 (0.4771)  acc1: 84.3750 (84.6726)  acc5: 100.0000 (99.6280)  time: 0.0287  data: 0.0003  max mem: 4132
[20:22:44.581431] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4578 (0.4986)  acc1: 84.3750 (84.3750)  acc5: 100.0000 (99.3448)  time: 0.0289  data: 0.0004  max mem: 4132
[20:22:44.872396] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4965 (0.5015)  acc1: 84.3750 (84.9085)  acc5: 98.4375 (99.1616)  time: 0.0290  data: 0.0004  max mem: 4132
[20:22:45.166733] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4904 (0.4961)  acc1: 85.9375 (85.2022)  acc5: 98.4375 (99.1728)  time: 0.0290  data: 0.0002  max mem: 4132
[20:22:45.459052] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4500 (0.4910)  acc1: 85.9375 (85.3996)  acc5: 100.0000 (99.1803)  time: 0.0291  data: 0.0003  max mem: 4132
[20:22:45.753240] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4619 (0.4875)  acc1: 85.9375 (85.5854)  acc5: 100.0000 (99.1857)  time: 0.0291  data: 0.0002  max mem: 4132
[20:22:46.042013] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4898 (0.4970)  acc1: 85.9375 (85.2816)  acc5: 98.4375 (99.1319)  time: 0.0290  data: 0.0002  max mem: 4132
[20:22:46.331347] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5078 (0.4945)  acc1: 84.3750 (85.3880)  acc5: 98.4375 (99.1587)  time: 0.0287  data: 0.0003  max mem: 4132
[20:22:46.624719] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.5078 (0.4986)  acc1: 84.3750 (85.1640)  acc5: 98.4375 (99.0873)  time: 0.0290  data: 0.0003  max mem: 4132
[20:22:46.913659] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5073 (0.4980)  acc1: 84.3750 (85.2055)  acc5: 98.4375 (99.0991)  time: 0.0289  data: 0.0003  max mem: 4132
[20:22:47.206196] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4892 (0.4966)  acc1: 85.9375 (85.2402)  acc5: 100.0000 (99.1090)  time: 0.0289  data: 0.0002  max mem: 4132
[20:22:47.493240] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4915 (0.4967)  acc1: 85.9375 (85.1861)  acc5: 100.0000 (99.1412)  time: 0.0288  data: 0.0002  max mem: 4132
[20:22:47.780671] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4783 (0.4942)  acc1: 85.9375 (85.3502)  acc5: 100.0000 (99.1578)  time: 0.0285  data: 0.0002  max mem: 4132
[20:22:48.064911] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5003 (0.4939)  acc1: 84.3750 (85.2856)  acc5: 100.0000 (99.1515)  time: 0.0284  data: 0.0002  max mem: 4132
[20:22:48.220705] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5003 (0.4950)  acc1: 84.3750 (85.2600)  acc5: 98.4375 (99.1400)  time: 0.0275  data: 0.0002  max mem: 4132
[20:22:48.392799] Test: Total time: 0:00:05 (0.0345 s / it)
[20:22:48.393741] * Acc@1 85.260 Acc@5 99.140 loss 0.495
[20:22:48.394783] Accuracy of the network on the 10000 test images: 85.3%
[20:22:48.395325] Max accuracy: 85.26%
[20:22:48.648914] log_dir: ./output_dir
[20:22:49.580960] Epoch: [76]  [  0/781]  eta: 0:12:06  lr: 0.000038  training_loss: 1.1062 (1.1062)  classification_loss: 1.1060 (1.1060)  loss_mask: 0.0002 (0.0002)  time: 0.9301  data: 0.6912  max mem: 4132
[20:22:52.864744] Epoch: [76]  [ 20/781]  eta: 0:02:32  lr: 0.000038  training_loss: 1.1406 (1.1609)  classification_loss: 1.1404 (1.1608)  loss_mask: 0.0002 (0.0002)  time: 0.1641  data: 0.0002  max mem: 4132
[20:22:56.188368] Epoch: [76]  [ 40/781]  eta: 0:02:16  lr: 0.000038  training_loss: 1.2117 (1.1823)  classification_loss: 1.2114 (1.1821)  loss_mask: 0.0002 (0.0002)  time: 0.1661  data: 0.0003  max mem: 4132
[20:22:59.498076] Epoch: [76]  [ 60/781]  eta: 0:02:08  lr: 0.000038  training_loss: 1.1774 (1.1855)  classification_loss: 1.1772 (1.1853)  loss_mask: 0.0002 (0.0002)  time: 0.1654  data: 0.0003  max mem: 4132
[20:23:02.836222] Epoch: [76]  [ 80/781]  eta: 0:02:02  lr: 0.000038  training_loss: 1.1436 (1.1807)  classification_loss: 1.1434 (1.1805)  loss_mask: 0.0002 (0.0002)  time: 0.1668  data: 0.0003  max mem: 4132
[20:23:06.158688] Epoch: [76]  [100/781]  eta: 0:01:57  lr: 0.000038  training_loss: 1.1516 (1.1836)  classification_loss: 1.1515 (1.1834)  loss_mask: 0.0001 (0.0002)  time: 0.1660  data: 0.0003  max mem: 4132
[20:23:09.474983] Epoch: [76]  [120/781]  eta: 0:01:53  lr: 0.000038  training_loss: 1.1912 (1.1847)  classification_loss: 1.1910 (1.1846)  loss_mask: 0.0001 (0.0002)  time: 0.1657  data: 0.0003  max mem: 4132
[20:23:12.787520] Epoch: [76]  [140/781]  eta: 0:01:49  lr: 0.000038  training_loss: 1.2483 (1.1896)  classification_loss: 1.2482 (1.1894)  loss_mask: 0.0001 (0.0002)  time: 0.1655  data: 0.0003  max mem: 4132
[20:23:16.069266] Epoch: [76]  [160/781]  eta: 0:01:45  lr: 0.000038  training_loss: 1.1364 (1.1868)  classification_loss: 1.1362 (1.1859)  loss_mask: 0.0002 (0.0009)  time: 0.1639  data: 0.0003  max mem: 4132
[20:23:19.376470] Epoch: [76]  [180/781]  eta: 0:01:41  lr: 0.000038  training_loss: 1.1601 (1.1891)  classification_loss: 1.1597 (1.1882)  loss_mask: 0.0003 (0.0008)  time: 0.1653  data: 0.0004  max mem: 4132
[20:23:22.671151] Epoch: [76]  [200/781]  eta: 0:01:38  lr: 0.000037  training_loss: 1.1680 (1.1878)  classification_loss: 1.1678 (1.1870)  loss_mask: 0.0002 (0.0008)  time: 0.1646  data: 0.0002  max mem: 4132
[20:23:25.963511] Epoch: [76]  [220/781]  eta: 0:01:34  lr: 0.000037  training_loss: 1.2024 (1.1883)  classification_loss: 1.2022 (1.1876)  loss_mask: 0.0002 (0.0007)  time: 0.1645  data: 0.0002  max mem: 4132
[20:23:29.258167] Epoch: [76]  [240/781]  eta: 0:01:31  lr: 0.000037  training_loss: 1.1948 (1.1895)  classification_loss: 1.1945 (1.1888)  loss_mask: 0.0002 (0.0007)  time: 0.1646  data: 0.0002  max mem: 4132
[20:23:32.520793] Epoch: [76]  [260/781]  eta: 0:01:27  lr: 0.000037  training_loss: 1.2277 (1.1914)  classification_loss: 1.2275 (1.1908)  loss_mask: 0.0002 (0.0006)  time: 0.1630  data: 0.0003  max mem: 4132
[20:23:35.846701] Epoch: [76]  [280/781]  eta: 0:01:24  lr: 0.000037  training_loss: 1.1835 (1.1907)  classification_loss: 1.1834 (1.1901)  loss_mask: 0.0001 (0.0006)  time: 0.1662  data: 0.0003  max mem: 4132
[20:23:39.183865] Epoch: [76]  [300/781]  eta: 0:01:20  lr: 0.000037  training_loss: 1.1978 (1.1911)  classification_loss: 1.1977 (1.1905)  loss_mask: 0.0002 (0.0006)  time: 0.1668  data: 0.0003  max mem: 4132
[20:23:42.461648] Epoch: [76]  [320/781]  eta: 0:01:17  lr: 0.000037  training_loss: 1.1738 (1.1912)  classification_loss: 1.1736 (1.1907)  loss_mask: 0.0001 (0.0005)  time: 0.1638  data: 0.0004  max mem: 4132
[20:23:45.779585] Epoch: [76]  [340/781]  eta: 0:01:13  lr: 0.000037  training_loss: 1.1453 (1.1889)  classification_loss: 1.1452 (1.1883)  loss_mask: 0.0001 (0.0005)  time: 0.1658  data: 0.0005  max mem: 4132
[20:23:49.065328] Epoch: [76]  [360/781]  eta: 0:01:10  lr: 0.000037  training_loss: 1.1828 (1.1896)  classification_loss: 1.1826 (1.1891)  loss_mask: 0.0002 (0.0005)  time: 0.1642  data: 0.0003  max mem: 4132
[20:23:52.394878] Epoch: [76]  [380/781]  eta: 0:01:07  lr: 0.000037  training_loss: 1.1862 (1.1903)  classification_loss: 1.1861 (1.1898)  loss_mask: 0.0001 (0.0005)  time: 0.1664  data: 0.0004  max mem: 4132
[20:23:55.721385] Epoch: [76]  [400/781]  eta: 0:01:03  lr: 0.000037  training_loss: 1.1809 (1.1906)  classification_loss: 1.1808 (1.1901)  loss_mask: 0.0001 (0.0005)  time: 0.1662  data: 0.0002  max mem: 4132
[20:23:59.027973] Epoch: [76]  [420/781]  eta: 0:01:00  lr: 0.000037  training_loss: 1.1947 (1.1907)  classification_loss: 1.1944 (1.1903)  loss_mask: 0.0001 (0.0004)  time: 0.1651  data: 0.0003  max mem: 4132
[20:24:02.314964] Epoch: [76]  [440/781]  eta: 0:00:56  lr: 0.000037  training_loss: 1.1750 (1.1895)  classification_loss: 1.1749 (1.1891)  loss_mask: 0.0001 (0.0004)  time: 0.1643  data: 0.0002  max mem: 4132
[20:24:05.599777] Epoch: [76]  [460/781]  eta: 0:00:53  lr: 0.000036  training_loss: 1.1497 (1.1880)  classification_loss: 1.1495 (1.1876)  loss_mask: 0.0001 (0.0004)  time: 0.1641  data: 0.0002  max mem: 4132
[20:24:08.898965] Epoch: [76]  [480/781]  eta: 0:00:50  lr: 0.000036  training_loss: 1.2278 (1.1904)  classification_loss: 1.2276 (1.1900)  loss_mask: 0.0001 (0.0004)  time: 0.1649  data: 0.0002  max mem: 4132
[20:24:12.197942] Epoch: [76]  [500/781]  eta: 0:00:46  lr: 0.000036  training_loss: 1.1937 (1.1902)  classification_loss: 1.1935 (1.1898)  loss_mask: 0.0001 (0.0004)  time: 0.1649  data: 0.0002  max mem: 4132
[20:24:15.553790] Epoch: [76]  [520/781]  eta: 0:00:43  lr: 0.000036  training_loss: 1.2194 (1.1915)  classification_loss: 1.2192 (1.1911)  loss_mask: 0.0001 (0.0004)  time: 0.1677  data: 0.0003  max mem: 4132
[20:24:18.837387] Epoch: [76]  [540/781]  eta: 0:00:40  lr: 0.000036  training_loss: 1.1861 (1.1919)  classification_loss: 1.1859 (1.1916)  loss_mask: 0.0001 (0.0004)  time: 0.1641  data: 0.0003  max mem: 4132
[20:24:22.128875] Epoch: [76]  [560/781]  eta: 0:00:36  lr: 0.000036  training_loss: 1.1967 (1.1918)  classification_loss: 1.1966 (1.1914)  loss_mask: 0.0001 (0.0004)  time: 0.1645  data: 0.0003  max mem: 4132
[20:24:25.409111] Epoch: [76]  [580/781]  eta: 0:00:33  lr: 0.000036  training_loss: 1.2163 (1.1918)  classification_loss: 1.2162 (1.1914)  loss_mask: 0.0001 (0.0004)  time: 0.1639  data: 0.0002  max mem: 4132
[20:24:28.673904] Epoch: [76]  [600/781]  eta: 0:00:30  lr: 0.000036  training_loss: 1.1181 (1.1907)  classification_loss: 1.1179 (1.1903)  loss_mask: 0.0001 (0.0004)  time: 0.1631  data: 0.0002  max mem: 4132
[20:24:31.936341] Epoch: [76]  [620/781]  eta: 0:00:26  lr: 0.000036  training_loss: 1.1721 (1.1904)  classification_loss: 1.1720 (1.1901)  loss_mask: 0.0001 (0.0003)  time: 0.1630  data: 0.0002  max mem: 4132
[20:24:35.200823] Epoch: [76]  [640/781]  eta: 0:00:23  lr: 0.000036  training_loss: 1.1617 (1.1903)  classification_loss: 1.1617 (1.1900)  loss_mask: 0.0001 (0.0003)  time: 0.1631  data: 0.0002  max mem: 4132
[20:24:38.464598] Epoch: [76]  [660/781]  eta: 0:00:20  lr: 0.000036  training_loss: 1.2233 (1.1912)  classification_loss: 1.2232 (1.1909)  loss_mask: 0.0001 (0.0003)  time: 0.1631  data: 0.0003  max mem: 4132
[20:24:41.732972] Epoch: [76]  [680/781]  eta: 0:00:16  lr: 0.000036  training_loss: 1.1935 (1.1918)  classification_loss: 1.1934 (1.1914)  loss_mask: 0.0001 (0.0003)  time: 0.1633  data: 0.0002  max mem: 4132
[20:24:44.995813] Epoch: [76]  [700/781]  eta: 0:00:13  lr: 0.000036  training_loss: 1.2054 (1.1915)  classification_loss: 1.2052 (1.1912)  loss_mask: 0.0001 (0.0003)  time: 0.1631  data: 0.0003  max mem: 4132
[20:24:48.300648] Epoch: [76]  [720/781]  eta: 0:00:10  lr: 0.000036  training_loss: 1.2304 (1.1927)  classification_loss: 1.2302 (1.1924)  loss_mask: 0.0001 (0.0003)  time: 0.1652  data: 0.0002  max mem: 4132
[20:24:51.554869] Epoch: [76]  [740/781]  eta: 0:00:06  lr: 0.000035  training_loss: 1.1712 (1.1922)  classification_loss: 1.1711 (1.1919)  loss_mask: 0.0001 (0.0003)  time: 0.1626  data: 0.0003  max mem: 4132
[20:24:54.836919] Epoch: [76]  [760/781]  eta: 0:00:03  lr: 0.000035  training_loss: 1.1811 (1.1922)  classification_loss: 1.1809 (1.1919)  loss_mask: 0.0001 (0.0003)  time: 0.1640  data: 0.0002  max mem: 4132
[20:24:58.130316] Epoch: [76]  [780/781]  eta: 0:00:00  lr: 0.000035  training_loss: 1.1577 (1.1917)  classification_loss: 1.1576 (1.1914)  loss_mask: 0.0001 (0.0003)  time: 0.1646  data: 0.0002  max mem: 4132
[20:24:58.294156] Epoch: [76] Total time: 0:02:09 (0.1660 s / it)
[20:24:58.294635] Averaged stats: lr: 0.000035  training_loss: 1.1577 (1.1917)  classification_loss: 1.1576 (1.1914)  loss_mask: 0.0001 (0.0003)
[20:24:58.986838] Test:  [  0/157]  eta: 0:01:47  testing_loss: 0.5125 (0.5125)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.6875  data: 0.6566  max mem: 4132
[20:24:59.280668] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.5125 (0.5078)  acc1: 82.8125 (83.9489)  acc5: 100.0000 (99.7159)  time: 0.0890  data: 0.0599  max mem: 4132
[20:24:59.569467] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4622 (0.4762)  acc1: 82.8125 (84.9702)  acc5: 100.0000 (99.7024)  time: 0.0289  data: 0.0002  max mem: 4132
[20:24:59.856113] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4622 (0.4966)  acc1: 85.9375 (84.7782)  acc5: 100.0000 (99.3952)  time: 0.0286  data: 0.0002  max mem: 4132
[20:25:00.155106] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5092 (0.5045)  acc1: 84.3750 (84.7561)  acc5: 98.4375 (99.1235)  time: 0.0291  data: 0.0002  max mem: 4132
[20:25:00.447469] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5060 (0.4999)  acc1: 84.3750 (85.1103)  acc5: 98.4375 (99.1115)  time: 0.0294  data: 0.0003  max mem: 4132
[20:25:00.740515] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4491 (0.4920)  acc1: 85.9375 (85.5020)  acc5: 100.0000 (99.1547)  time: 0.0291  data: 0.0003  max mem: 4132
[20:25:01.032497] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4654 (0.4889)  acc1: 87.5000 (85.6954)  acc5: 100.0000 (99.2077)  time: 0.0291  data: 0.0003  max mem: 4132
[20:25:01.322981] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4898 (0.4969)  acc1: 85.9375 (85.4360)  acc5: 98.4375 (99.0934)  time: 0.0289  data: 0.0002  max mem: 4132
[20:25:01.612052] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5286 (0.4954)  acc1: 84.3750 (85.4567)  acc5: 98.4375 (99.1071)  time: 0.0288  data: 0.0002  max mem: 4132
[20:25:01.901797] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.5236 (0.4981)  acc1: 84.3750 (85.2568)  acc5: 100.0000 (99.1182)  time: 0.0288  data: 0.0003  max mem: 4132
[20:25:02.189434] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5275 (0.5014)  acc1: 82.8125 (85.1351)  acc5: 100.0000 (99.0991)  time: 0.0287  data: 0.0002  max mem: 4132
[20:25:02.479407] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5275 (0.5012)  acc1: 82.8125 (85.0723)  acc5: 98.4375 (99.0832)  time: 0.0287  data: 0.0002  max mem: 4132
[20:25:02.768311] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4866 (0.5009)  acc1: 84.3750 (84.9237)  acc5: 98.4375 (99.0935)  time: 0.0287  data: 0.0002  max mem: 4132
[20:25:03.054474] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4866 (0.4979)  acc1: 84.3750 (85.0288)  acc5: 100.0000 (99.1246)  time: 0.0286  data: 0.0002  max mem: 4132
[20:25:03.339537] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4958 (0.4978)  acc1: 85.9375 (84.9545)  acc5: 100.0000 (99.1308)  time: 0.0284  data: 0.0002  max mem: 4132
[20:25:03.496589] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4987 (0.4991)  acc1: 84.3750 (84.8700)  acc5: 98.4375 (99.1200)  time: 0.0276  data: 0.0002  max mem: 4132
[20:25:03.676793] Test: Total time: 0:00:05 (0.0343 s / it)
[20:25:03.677573] * Acc@1 84.870 Acc@5 99.120 loss 0.499
[20:25:03.677966] Accuracy of the network on the 10000 test images: 84.9%
[20:25:03.678258] Max accuracy: 85.26%
[20:25:03.941613] log_dir: ./output_dir
[20:25:04.891744] Epoch: [77]  [  0/781]  eta: 0:12:20  lr: 0.000035  training_loss: 1.1115 (1.1115)  classification_loss: 1.1114 (1.1114)  loss_mask: 0.0001 (0.0001)  time: 0.9480  data: 0.7410  max mem: 4132
[20:25:08.243466] Epoch: [77]  [ 20/781]  eta: 0:02:35  lr: 0.000035  training_loss: 1.1596 (1.1587)  classification_loss: 1.1593 (1.1586)  loss_mask: 0.0001 (0.0001)  time: 0.1675  data: 0.0002  max mem: 4132
[20:25:11.531826] Epoch: [77]  [ 40/781]  eta: 0:02:17  lr: 0.000035  training_loss: 1.1942 (1.1739)  classification_loss: 1.1940 (1.1738)  loss_mask: 0.0001 (0.0001)  time: 0.1643  data: 0.0003  max mem: 4132
[20:25:14.850001] Epoch: [77]  [ 60/781]  eta: 0:02:08  lr: 0.000035  training_loss: 1.2079 (1.1874)  classification_loss: 1.2074 (1.1873)  loss_mask: 0.0001 (0.0001)  time: 0.1658  data: 0.0005  max mem: 4132
[20:25:18.147596] Epoch: [77]  [ 80/781]  eta: 0:02:02  lr: 0.000035  training_loss: 1.1946 (1.1902)  classification_loss: 1.1945 (1.1901)  loss_mask: 0.0001 (0.0001)  time: 0.1648  data: 0.0003  max mem: 4132
[20:25:21.427820] Epoch: [77]  [100/781]  eta: 0:01:57  lr: 0.000035  training_loss: 1.1933 (1.1923)  classification_loss: 1.1932 (1.1922)  loss_mask: 0.0001 (0.0001)  time: 0.1639  data: 0.0003  max mem: 4132
[20:25:24.755106] Epoch: [77]  [120/781]  eta: 0:01:53  lr: 0.000035  training_loss: 1.1731 (1.1954)  classification_loss: 1.1730 (1.1952)  loss_mask: 0.0001 (0.0001)  time: 0.1663  data: 0.0002  max mem: 4132
[20:25:28.026629] Epoch: [77]  [140/781]  eta: 0:01:49  lr: 0.000035  training_loss: 1.1859 (1.1951)  classification_loss: 1.1858 (1.1949)  loss_mask: 0.0001 (0.0001)  time: 0.1635  data: 0.0002  max mem: 4132
[20:25:31.316276] Epoch: [77]  [160/781]  eta: 0:01:45  lr: 0.000035  training_loss: 1.1797 (1.1963)  classification_loss: 1.1794 (1.1962)  loss_mask: 0.0001 (0.0001)  time: 0.1644  data: 0.0003  max mem: 4132
[20:25:34.667480] Epoch: [77]  [180/781]  eta: 0:01:41  lr: 0.000035  training_loss: 1.2009 (1.1958)  classification_loss: 1.2007 (1.1957)  loss_mask: 0.0001 (0.0001)  time: 0.1674  data: 0.0002  max mem: 4132
[20:25:37.972750] Epoch: [77]  [200/781]  eta: 0:01:38  lr: 0.000035  training_loss: 1.1450 (1.1911)  classification_loss: 1.1449 (1.1910)  loss_mask: 0.0001 (0.0001)  time: 0.1652  data: 0.0002  max mem: 4132
[20:25:41.260088] Epoch: [77]  [220/781]  eta: 0:01:34  lr: 0.000035  training_loss: 1.1950 (1.1902)  classification_loss: 1.1949 (1.1900)  loss_mask: 0.0001 (0.0001)  time: 0.1643  data: 0.0002  max mem: 4132
[20:25:44.552945] Epoch: [77]  [240/781]  eta: 0:01:31  lr: 0.000034  training_loss: 1.1373 (1.1894)  classification_loss: 1.1372 (1.1893)  loss_mask: 0.0001 (0.0001)  time: 0.1646  data: 0.0002  max mem: 4132
[20:25:47.834112] Epoch: [77]  [260/781]  eta: 0:01:27  lr: 0.000034  training_loss: 1.2464 (1.1922)  classification_loss: 1.2463 (1.1921)  loss_mask: 0.0001 (0.0001)  time: 0.1640  data: 0.0003  max mem: 4132
[20:25:51.163796] Epoch: [77]  [280/781]  eta: 0:01:24  lr: 0.000034  training_loss: 1.1537 (1.1909)  classification_loss: 1.1536 (1.1908)  loss_mask: 0.0001 (0.0001)  time: 0.1664  data: 0.0003  max mem: 4132
[20:25:54.476047] Epoch: [77]  [300/781]  eta: 0:01:20  lr: 0.000034  training_loss: 1.2312 (1.1933)  classification_loss: 1.2311 (1.1932)  loss_mask: 0.0001 (0.0001)  time: 0.1655  data: 0.0002  max mem: 4132
[20:25:57.796872] Epoch: [77]  [320/781]  eta: 0:01:17  lr: 0.000034  training_loss: 1.1496 (1.1920)  classification_loss: 1.1495 (1.1918)  loss_mask: 0.0001 (0.0001)  time: 0.1660  data: 0.0004  max mem: 4132
[20:26:01.065028] Epoch: [77]  [340/781]  eta: 0:01:13  lr: 0.000034  training_loss: 1.1571 (1.1910)  classification_loss: 1.1570 (1.1909)  loss_mask: 0.0001 (0.0001)  time: 0.1633  data: 0.0002  max mem: 4132
[20:26:04.358933] Epoch: [77]  [360/781]  eta: 0:01:10  lr: 0.000034  training_loss: 1.1736 (1.1910)  classification_loss: 1.1735 (1.1909)  loss_mask: 0.0001 (0.0001)  time: 0.1646  data: 0.0003  max mem: 4132
[20:26:07.652937] Epoch: [77]  [380/781]  eta: 0:01:07  lr: 0.000034  training_loss: 1.2365 (1.1925)  classification_loss: 1.2364 (1.1924)  loss_mask: 0.0001 (0.0001)  time: 0.1646  data: 0.0003  max mem: 4132
[20:26:10.924290] Epoch: [77]  [400/781]  eta: 0:01:03  lr: 0.000034  training_loss: 1.2174 (1.1930)  classification_loss: 1.2174 (1.1929)  loss_mask: 0.0001 (0.0001)  time: 0.1635  data: 0.0003  max mem: 4132
[20:26:14.227592] Epoch: [77]  [420/781]  eta: 0:01:00  lr: 0.000034  training_loss: 1.1874 (1.1931)  classification_loss: 1.1870 (1.1930)  loss_mask: 0.0001 (0.0001)  time: 0.1651  data: 0.0003  max mem: 4132
[20:26:17.532762] Epoch: [77]  [440/781]  eta: 0:00:56  lr: 0.000034  training_loss: 1.1715 (1.1909)  classification_loss: 1.1714 (1.1908)  loss_mask: 0.0001 (0.0001)  time: 0.1652  data: 0.0003  max mem: 4132
[20:26:20.846392] Epoch: [77]  [460/781]  eta: 0:00:53  lr: 0.000034  training_loss: 1.1770 (1.1905)  classification_loss: 1.1769 (1.1904)  loss_mask: 0.0001 (0.0001)  time: 0.1656  data: 0.0003  max mem: 4132
[20:26:24.175911] Epoch: [77]  [480/781]  eta: 0:00:50  lr: 0.000034  training_loss: 1.2061 (1.1905)  classification_loss: 1.2061 (1.1904)  loss_mask: 0.0001 (0.0001)  time: 0.1664  data: 0.0003  max mem: 4132
[20:26:27.519125] Epoch: [77]  [500/781]  eta: 0:00:46  lr: 0.000034  training_loss: 1.1948 (1.1914)  classification_loss: 1.1947 (1.1913)  loss_mask: 0.0001 (0.0001)  time: 0.1671  data: 0.0003  max mem: 4132
[20:26:30.804425] Epoch: [77]  [520/781]  eta: 0:00:43  lr: 0.000033  training_loss: 1.1558 (1.1909)  classification_loss: 1.1557 (1.1908)  loss_mask: 0.0001 (0.0001)  time: 0.1642  data: 0.0003  max mem: 4132
[20:26:34.148697] Epoch: [77]  [540/781]  eta: 0:00:40  lr: 0.000033  training_loss: 1.1991 (1.1921)  classification_loss: 1.1991 (1.1920)  loss_mask: 0.0001 (0.0001)  time: 0.1671  data: 0.0004  max mem: 4132
[20:26:37.487087] Epoch: [77]  [560/781]  eta: 0:00:36  lr: 0.000033  training_loss: 1.1849 (1.1920)  classification_loss: 1.1848 (1.1919)  loss_mask: 0.0001 (0.0001)  time: 0.1668  data: 0.0003  max mem: 4132
[20:26:40.793866] Epoch: [77]  [580/781]  eta: 0:00:33  lr: 0.000033  training_loss: 1.1814 (1.1919)  classification_loss: 1.1813 (1.1918)  loss_mask: 0.0001 (0.0001)  time: 0.1652  data: 0.0003  max mem: 4132
[20:26:44.094103] Epoch: [77]  [600/781]  eta: 0:00:30  lr: 0.000033  training_loss: 1.1705 (1.1910)  classification_loss: 1.1704 (1.1909)  loss_mask: 0.0001 (0.0001)  time: 0.1649  data: 0.0003  max mem: 4132
[20:26:47.385466] Epoch: [77]  [620/781]  eta: 0:00:26  lr: 0.000033  training_loss: 1.1546 (1.1911)  classification_loss: 1.1545 (1.1910)  loss_mask: 0.0001 (0.0001)  time: 0.1645  data: 0.0003  max mem: 4132
[20:26:50.737568] Epoch: [77]  [640/781]  eta: 0:00:23  lr: 0.000033  training_loss: 1.1638 (1.1907)  classification_loss: 1.1637 (1.1906)  loss_mask: 0.0001 (0.0001)  time: 0.1675  data: 0.0004  max mem: 4132
[20:26:54.075753] Epoch: [77]  [660/781]  eta: 0:00:20  lr: 0.000033  training_loss: 1.2180 (1.1915)  classification_loss: 1.2179 (1.1914)  loss_mask: 0.0001 (0.0001)  time: 0.1668  data: 0.0003  max mem: 4132
[20:26:57.376023] Epoch: [77]  [680/781]  eta: 0:00:16  lr: 0.000033  training_loss: 1.1984 (1.1920)  classification_loss: 1.1983 (1.1919)  loss_mask: 0.0001 (0.0001)  time: 0.1649  data: 0.0003  max mem: 4132
[20:27:00.677500] Epoch: [77]  [700/781]  eta: 0:00:13  lr: 0.000033  training_loss: 1.1915 (1.1926)  classification_loss: 1.1915 (1.1925)  loss_mask: 0.0001 (0.0001)  time: 0.1649  data: 0.0003  max mem: 4132
[20:27:04.005337] Epoch: [77]  [720/781]  eta: 0:00:10  lr: 0.000033  training_loss: 1.2257 (1.1934)  classification_loss: 1.2256 (1.1933)  loss_mask: 0.0001 (0.0001)  time: 0.1663  data: 0.0003  max mem: 4132
[20:27:07.340359] Epoch: [77]  [740/781]  eta: 0:00:06  lr: 0.000033  training_loss: 1.1720 (1.1931)  classification_loss: 1.1719 (1.1930)  loss_mask: 0.0001 (0.0001)  time: 0.1667  data: 0.0003  max mem: 4132
[20:27:10.623100] Epoch: [77]  [760/781]  eta: 0:00:03  lr: 0.000033  training_loss: 1.2047 (1.1931)  classification_loss: 1.2045 (1.1930)  loss_mask: 0.0001 (0.0001)  time: 0.1641  data: 0.0002  max mem: 4132
[20:27:13.891218] Epoch: [77]  [780/781]  eta: 0:00:00  lr: 0.000033  training_loss: 1.1636 (1.1929)  classification_loss: 1.1635 (1.1928)  loss_mask: 0.0001 (0.0001)  time: 0.1633  data: 0.0002  max mem: 4132
[20:27:14.078825] Epoch: [77] Total time: 0:02:10 (0.1666 s / it)
[20:27:14.079747] Averaged stats: lr: 0.000033  training_loss: 1.1636 (1.1929)  classification_loss: 1.1635 (1.1928)  loss_mask: 0.0001 (0.0001)
[20:27:14.771937] Test:  [  0/157]  eta: 0:01:47  testing_loss: 0.5009 (0.5009)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.6871  data: 0.6567  max mem: 4132
[20:27:15.058590] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5009 (0.5001)  acc1: 85.9375 (84.3750)  acc5: 100.0000 (99.8580)  time: 0.0882  data: 0.0599  max mem: 4132
[20:27:15.343419] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4808 (0.4767)  acc1: 84.3750 (85.1190)  acc5: 100.0000 (99.7024)  time: 0.0283  data: 0.0002  max mem: 4132
[20:27:15.626816] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4494 (0.4928)  acc1: 84.3750 (84.7278)  acc5: 100.0000 (99.3952)  time: 0.0283  data: 0.0002  max mem: 4132
[20:27:15.916873] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4964 (0.4958)  acc1: 84.3750 (84.8323)  acc5: 98.4375 (99.2378)  time: 0.0285  data: 0.0002  max mem: 4132
[20:27:16.204773] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4964 (0.4922)  acc1: 84.3750 (85.2328)  acc5: 98.4375 (99.2953)  time: 0.0288  data: 0.0002  max mem: 4132
[20:27:16.492653] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4408 (0.4877)  acc1: 87.5000 (85.4764)  acc5: 100.0000 (99.2572)  time: 0.0286  data: 0.0002  max mem: 4132
[20:27:16.776631] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4608 (0.4860)  acc1: 85.9375 (85.4754)  acc5: 100.0000 (99.2518)  time: 0.0284  data: 0.0002  max mem: 4132
[20:27:17.058877] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5006 (0.4950)  acc1: 84.3750 (85.2623)  acc5: 98.4375 (99.1319)  time: 0.0282  data: 0.0002  max mem: 4132
[20:27:17.342925] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4953 (0.4918)  acc1: 84.3750 (85.3022)  acc5: 98.4375 (99.1415)  time: 0.0282  data: 0.0002  max mem: 4132
[20:27:17.625889] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4953 (0.4942)  acc1: 84.3750 (85.1640)  acc5: 100.0000 (99.1646)  time: 0.0282  data: 0.0002  max mem: 4132
[20:27:17.909064] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5219 (0.4961)  acc1: 84.3750 (85.0788)  acc5: 100.0000 (99.1695)  time: 0.0282  data: 0.0002  max mem: 4132
[20:27:18.197187] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5110 (0.4952)  acc1: 85.9375 (85.0594)  acc5: 100.0000 (99.1736)  time: 0.0284  data: 0.0002  max mem: 4132
[20:27:18.481465] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4777 (0.4958)  acc1: 85.9375 (85.1145)  acc5: 100.0000 (99.1889)  time: 0.0285  data: 0.0002  max mem: 4132
[20:27:18.763641] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4690 (0.4925)  acc1: 85.9375 (85.2283)  acc5: 100.0000 (99.2021)  time: 0.0282  data: 0.0002  max mem: 4132
[20:27:19.045809] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4775 (0.4924)  acc1: 85.9375 (85.2235)  acc5: 100.0000 (99.2032)  time: 0.0281  data: 0.0002  max mem: 4132
[20:27:19.199150] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4978 (0.4946)  acc1: 85.9375 (85.1300)  acc5: 100.0000 (99.2000)  time: 0.0272  data: 0.0001  max mem: 4132
[20:27:19.367883] Test: Total time: 0:00:05 (0.0337 s / it)
[20:27:19.368634] * Acc@1 85.130 Acc@5 99.200 loss 0.495
[20:27:19.369052] Accuracy of the network on the 10000 test images: 85.1%
[20:27:19.369451] Max accuracy: 85.26%
[20:27:19.788478] log_dir: ./output_dir
[20:27:20.728873] Epoch: [78]  [  0/781]  eta: 0:12:12  lr: 0.000033  training_loss: 1.1686 (1.1686)  classification_loss: 1.1685 (1.1685)  loss_mask: 0.0001 (0.0001)  time: 0.9383  data: 0.7372  max mem: 4132
[20:27:23.989320] Epoch: [78]  [ 20/781]  eta: 0:02:32  lr: 0.000032  training_loss: 1.1530 (1.1771)  classification_loss: 1.1530 (1.1770)  loss_mask: 0.0001 (0.0001)  time: 0.1629  data: 0.0002  max mem: 4132
[20:27:27.268097] Epoch: [78]  [ 40/781]  eta: 0:02:15  lr: 0.000032  training_loss: 1.2422 (1.1914)  classification_loss: 1.2421 (1.1913)  loss_mask: 0.0001 (0.0001)  time: 0.1639  data: 0.0002  max mem: 4132
[20:27:30.527084] Epoch: [78]  [ 60/781]  eta: 0:02:06  lr: 0.000032  training_loss: 1.2022 (1.2018)  classification_loss: 1.2022 (1.2018)  loss_mask: 0.0001 (0.0001)  time: 0.1629  data: 0.0002  max mem: 4132
[20:27:33.848843] Epoch: [78]  [ 80/781]  eta: 0:02:01  lr: 0.000032  training_loss: 1.2318 (1.2018)  classification_loss: 1.2318 (1.2017)  loss_mask: 0.0001 (0.0001)  time: 0.1660  data: 0.0002  max mem: 4132
[20:27:37.128319] Epoch: [78]  [100/781]  eta: 0:01:56  lr: 0.000032  training_loss: 1.1755 (1.2026)  classification_loss: 1.1754 (1.2025)  loss_mask: 0.0001 (0.0001)  time: 0.1639  data: 0.0002  max mem: 4132
[20:27:40.400761] Epoch: [78]  [120/781]  eta: 0:01:52  lr: 0.000032  training_loss: 1.1517 (1.1951)  classification_loss: 1.1516 (1.1950)  loss_mask: 0.0001 (0.0001)  time: 0.1634  data: 0.0002  max mem: 4132
[20:27:43.715424] Epoch: [78]  [140/781]  eta: 0:01:48  lr: 0.000032  training_loss: 1.1048 (1.1859)  classification_loss: 1.1047 (1.1858)  loss_mask: 0.0001 (0.0001)  time: 0.1656  data: 0.0003  max mem: 4132
[20:27:47.011522] Epoch: [78]  [160/781]  eta: 0:01:44  lr: 0.000032  training_loss: 1.1468 (1.1830)  classification_loss: 1.1468 (1.1829)  loss_mask: 0.0001 (0.0001)  time: 0.1647  data: 0.0002  max mem: 4132
[20:27:50.289869] Epoch: [78]  [180/781]  eta: 0:01:41  lr: 0.000032  training_loss: 1.1656 (1.1833)  classification_loss: 1.1656 (1.1832)  loss_mask: 0.0001 (0.0001)  time: 0.1638  data: 0.0003  max mem: 4132
[20:27:53.587617] Epoch: [78]  [200/781]  eta: 0:01:37  lr: 0.000032  training_loss: 1.1678 (1.1820)  classification_loss: 1.1678 (1.1819)  loss_mask: 0.0001 (0.0001)  time: 0.1648  data: 0.0003  max mem: 4132
[20:27:56.935004] Epoch: [78]  [220/781]  eta: 0:01:34  lr: 0.000032  training_loss: 1.2022 (1.1836)  classification_loss: 1.2022 (1.1835)  loss_mask: 0.0001 (0.0001)  time: 0.1673  data: 0.0004  max mem: 4132
[20:28:00.225142] Epoch: [78]  [240/781]  eta: 0:01:30  lr: 0.000032  training_loss: 1.1702 (1.1844)  classification_loss: 1.1701 (1.1843)  loss_mask: 0.0001 (0.0001)  time: 0.1644  data: 0.0003  max mem: 4132
[20:28:03.506062] Epoch: [78]  [260/781]  eta: 0:01:27  lr: 0.000032  training_loss: 1.2113 (1.1861)  classification_loss: 1.2112 (1.1861)  loss_mask: 0.0001 (0.0001)  time: 0.1640  data: 0.0002  max mem: 4132
[20:28:06.819995] Epoch: [78]  [280/781]  eta: 0:01:23  lr: 0.000032  training_loss: 1.1379 (1.1857)  classification_loss: 1.1378 (1.1856)  loss_mask: 0.0001 (0.0001)  time: 0.1656  data: 0.0006  max mem: 4132
[20:28:10.128899] Epoch: [78]  [300/781]  eta: 0:01:20  lr: 0.000031  training_loss: 1.1618 (1.1849)  classification_loss: 1.1618 (1.1848)  loss_mask: 0.0001 (0.0001)  time: 0.1653  data: 0.0003  max mem: 4132
[20:28:13.439738] Epoch: [78]  [320/781]  eta: 0:01:17  lr: 0.000031  training_loss: 1.1748 (1.1845)  classification_loss: 1.1747 (1.1844)  loss_mask: 0.0001 (0.0001)  time: 0.1655  data: 0.0003  max mem: 4132
[20:28:16.727986] Epoch: [78]  [340/781]  eta: 0:01:13  lr: 0.000031  training_loss: 1.1301 (1.1818)  classification_loss: 1.1301 (1.1817)  loss_mask: 0.0001 (0.0001)  time: 0.1643  data: 0.0003  max mem: 4132
[20:28:20.058863] Epoch: [78]  [360/781]  eta: 0:01:10  lr: 0.000031  training_loss: 1.2042 (1.1835)  classification_loss: 1.2041 (1.1835)  loss_mask: 0.0001 (0.0001)  time: 0.1664  data: 0.0003  max mem: 4132
[20:28:23.385976] Epoch: [78]  [380/781]  eta: 0:01:06  lr: 0.000031  training_loss: 1.1635 (1.1848)  classification_loss: 1.1635 (1.1847)  loss_mask: 0.0001 (0.0001)  time: 0.1663  data: 0.0004  max mem: 4132
[20:28:26.685407] Epoch: [78]  [400/781]  eta: 0:01:03  lr: 0.000031  training_loss: 1.1619 (1.1842)  classification_loss: 1.1618 (1.1841)  loss_mask: 0.0001 (0.0001)  time: 0.1649  data: 0.0002  max mem: 4132
[20:28:29.981945] Epoch: [78]  [420/781]  eta: 0:01:00  lr: 0.000031  training_loss: 1.1653 (1.1836)  classification_loss: 1.1652 (1.1835)  loss_mask: 0.0001 (0.0001)  time: 0.1647  data: 0.0005  max mem: 4132
[20:28:33.274482] Epoch: [78]  [440/781]  eta: 0:00:56  lr: 0.000031  training_loss: 1.2151 (1.1848)  classification_loss: 1.2151 (1.1847)  loss_mask: 0.0001 (0.0001)  time: 0.1645  data: 0.0002  max mem: 4132
[20:28:36.559450] Epoch: [78]  [460/781]  eta: 0:00:53  lr: 0.000031  training_loss: 1.1614 (1.1826)  classification_loss: 1.1614 (1.1825)  loss_mask: 0.0001 (0.0001)  time: 0.1642  data: 0.0003  max mem: 4132
[20:28:39.869927] Epoch: [78]  [480/781]  eta: 0:00:50  lr: 0.000031  training_loss: 1.1350 (1.1825)  classification_loss: 1.1349 (1.1824)  loss_mask: 0.0001 (0.0001)  time: 0.1654  data: 0.0003  max mem: 4132
[20:28:43.165473] Epoch: [78]  [500/781]  eta: 0:00:46  lr: 0.000031  training_loss: 1.1972 (1.1824)  classification_loss: 1.1971 (1.1823)  loss_mask: 0.0001 (0.0001)  time: 0.1647  data: 0.0002  max mem: 4132
[20:28:46.450908] Epoch: [78]  [520/781]  eta: 0:00:43  lr: 0.000031  training_loss: 1.1566 (1.1829)  classification_loss: 1.1566 (1.1828)  loss_mask: 0.0001 (0.0001)  time: 0.1642  data: 0.0002  max mem: 4132
[20:28:49.703835] Epoch: [78]  [540/781]  eta: 0:00:40  lr: 0.000031  training_loss: 1.1772 (1.1830)  classification_loss: 1.1771 (1.1829)  loss_mask: 0.0001 (0.0001)  time: 0.1625  data: 0.0002  max mem: 4132
[20:28:53.015101] Epoch: [78]  [560/781]  eta: 0:00:36  lr: 0.000031  training_loss: 1.1720 (1.1826)  classification_loss: 1.1720 (1.1825)  loss_mask: 0.0001 (0.0001)  time: 0.1655  data: 0.0003  max mem: 4132
[20:28:56.353476] Epoch: [78]  [580/781]  eta: 0:00:33  lr: 0.000031  training_loss: 1.1553 (1.1826)  classification_loss: 1.1552 (1.1825)  loss_mask: 0.0001 (0.0001)  time: 0.1668  data: 0.0003  max mem: 4132
[20:28:59.656007] Epoch: [78]  [600/781]  eta: 0:00:30  lr: 0.000030  training_loss: 1.1737 (1.1822)  classification_loss: 1.1736 (1.1821)  loss_mask: 0.0001 (0.0001)  time: 0.1650  data: 0.0002  max mem: 4132
[20:29:02.956712] Epoch: [78]  [620/781]  eta: 0:00:26  lr: 0.000030  training_loss: 1.1335 (1.1812)  classification_loss: 1.1335 (1.1812)  loss_mask: 0.0001 (0.0001)  time: 0.1649  data: 0.0003  max mem: 4132
[20:29:06.234209] Epoch: [78]  [640/781]  eta: 0:00:23  lr: 0.000030  training_loss: 1.1942 (1.1815)  classification_loss: 1.1941 (1.1815)  loss_mask: 0.0001 (0.0001)  time: 0.1638  data: 0.0002  max mem: 4132
[20:29:09.525396] Epoch: [78]  [660/781]  eta: 0:00:20  lr: 0.000030  training_loss: 1.1876 (1.1823)  classification_loss: 1.1875 (1.1822)  loss_mask: 0.0000 (0.0001)  time: 0.1645  data: 0.0003  max mem: 4132
[20:29:12.816202] Epoch: [78]  [680/781]  eta: 0:00:16  lr: 0.000030  training_loss: 1.2564 (1.1841)  classification_loss: 1.2563 (1.1841)  loss_mask: 0.0001 (0.0001)  time: 0.1645  data: 0.0003  max mem: 4132
[20:29:16.119588] Epoch: [78]  [700/781]  eta: 0:00:13  lr: 0.000030  training_loss: 1.1989 (1.1844)  classification_loss: 1.1988 (1.1843)  loss_mask: 0.0001 (0.0001)  time: 0.1651  data: 0.0003  max mem: 4132
[20:29:19.430679] Epoch: [78]  [720/781]  eta: 0:00:10  lr: 0.000030  training_loss: 1.1831 (1.1842)  classification_loss: 1.1830 (1.1842)  loss_mask: 0.0001 (0.0001)  time: 0.1655  data: 0.0003  max mem: 4132
[20:29:22.706000] Epoch: [78]  [740/781]  eta: 0:00:06  lr: 0.000030  training_loss: 1.1082 (1.1828)  classification_loss: 1.1081 (1.1828)  loss_mask: 0.0001 (0.0001)  time: 0.1637  data: 0.0002  max mem: 4132
[20:29:25.971575] Epoch: [78]  [760/781]  eta: 0:00:03  lr: 0.000030  training_loss: 1.1605 (1.1835)  classification_loss: 1.1604 (1.1835)  loss_mask: 0.0001 (0.0001)  time: 0.1632  data: 0.0003  max mem: 4132
[20:29:29.235423] Epoch: [78]  [780/781]  eta: 0:00:00  lr: 0.000030  training_loss: 1.2055 (1.1837)  classification_loss: 1.2055 (1.1837)  loss_mask: 0.0001 (0.0001)  time: 0.1631  data: 0.0003  max mem: 4132
[20:29:29.404781] Epoch: [78] Total time: 0:02:09 (0.1660 s / it)
[20:29:29.405263] Averaged stats: lr: 0.000030  training_loss: 1.2055 (1.1837)  classification_loss: 1.2055 (1.1837)  loss_mask: 0.0001 (0.0001)
[20:29:30.142074] Test:  [  0/157]  eta: 0:01:54  testing_loss: 0.4787 (0.4787)  acc1: 82.8125 (82.8125)  acc5: 98.4375 (98.4375)  time: 0.7319  data: 0.7024  max mem: 4132
[20:29:30.429768] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.4787 (0.4998)  acc1: 81.2500 (83.2386)  acc5: 100.0000 (99.4318)  time: 0.0924  data: 0.0641  max mem: 4132
[20:29:30.724832] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4592 (0.4640)  acc1: 85.9375 (85.3423)  acc5: 100.0000 (99.6280)  time: 0.0288  data: 0.0002  max mem: 4132
[20:29:31.015100] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4592 (0.4840)  acc1: 85.9375 (84.9294)  acc5: 100.0000 (99.3952)  time: 0.0290  data: 0.0003  max mem: 4132
[20:29:31.307992] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4981 (0.4894)  acc1: 85.9375 (85.1753)  acc5: 98.4375 (99.2759)  time: 0.0289  data: 0.0003  max mem: 4132
[20:29:31.596954] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4981 (0.4884)  acc1: 85.9375 (85.5392)  acc5: 98.4375 (99.2341)  time: 0.0289  data: 0.0002  max mem: 4132
[20:29:31.887588] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4573 (0.4830)  acc1: 87.5000 (85.8350)  acc5: 100.0000 (99.2572)  time: 0.0288  data: 0.0002  max mem: 4132
[20:29:32.176851] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4639 (0.4810)  acc1: 85.9375 (85.8715)  acc5: 100.0000 (99.2518)  time: 0.0288  data: 0.0002  max mem: 4132
[20:29:32.463493] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5142 (0.4897)  acc1: 84.3750 (85.5517)  acc5: 98.4375 (99.1127)  time: 0.0286  data: 0.0002  max mem: 4132
[20:29:32.749557] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4748 (0.4860)  acc1: 84.3750 (85.5941)  acc5: 98.4375 (99.1243)  time: 0.0285  data: 0.0002  max mem: 4132
[20:29:33.037302] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.4831 (0.4874)  acc1: 84.3750 (85.4889)  acc5: 100.0000 (99.1491)  time: 0.0285  data: 0.0002  max mem: 4132
[20:29:33.330981] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4902 (0.4892)  acc1: 84.3750 (85.4730)  acc5: 100.0000 (99.1413)  time: 0.0289  data: 0.0002  max mem: 4132
[20:29:33.617814] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4891 (0.4901)  acc1: 85.9375 (85.4210)  acc5: 100.0000 (99.1736)  time: 0.0288  data: 0.0002  max mem: 4132
[20:29:33.905541] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4891 (0.4902)  acc1: 84.3750 (85.3769)  acc5: 100.0000 (99.1889)  time: 0.0284  data: 0.0002  max mem: 4132
[20:29:34.190974] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4847 (0.4871)  acc1: 85.9375 (85.4832)  acc5: 100.0000 (99.1910)  time: 0.0284  data: 0.0002  max mem: 4132
[20:29:34.476148] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4848 (0.4875)  acc1: 84.3750 (85.3891)  acc5: 100.0000 (99.2032)  time: 0.0283  data: 0.0002  max mem: 4132
[20:29:34.630006] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4847 (0.4890)  acc1: 84.3750 (85.3300)  acc5: 98.4375 (99.1900)  time: 0.0274  data: 0.0002  max mem: 4132
[20:29:34.814472] Test: Total time: 0:00:05 (0.0344 s / it)
[20:29:34.815409] * Acc@1 85.330 Acc@5 99.190 loss 0.489
[20:29:34.815845] Accuracy of the network on the 10000 test images: 85.3%
[20:29:34.816167] Max accuracy: 85.33%
[20:29:35.095603] log_dir: ./output_dir
[20:29:36.022964] Epoch: [79]  [  0/781]  eta: 0:12:02  lr: 0.000030  training_loss: 1.0400 (1.0400)  classification_loss: 1.0400 (1.0400)  loss_mask: 0.0000 (0.0000)  time: 0.9250  data: 0.6669  max mem: 4132
[20:29:39.321775] Epoch: [79]  [ 20/781]  eta: 0:02:32  lr: 0.000030  training_loss: 1.1662 (1.1526)  classification_loss: 1.1660 (1.1526)  loss_mask: 0.0001 (0.0001)  time: 0.1648  data: 0.0002  max mem: 4132
[20:29:42.619973] Epoch: [79]  [ 40/781]  eta: 0:02:15  lr: 0.000030  training_loss: 1.1458 (1.1693)  classification_loss: 1.1457 (1.1692)  loss_mask: 0.0001 (0.0001)  time: 0.1648  data: 0.0003  max mem: 4132
[20:29:45.953434] Epoch: [79]  [ 60/781]  eta: 0:02:08  lr: 0.000030  training_loss: 1.1838 (1.1689)  classification_loss: 1.1838 (1.1689)  loss_mask: 0.0001 (0.0001)  time: 0.1666  data: 0.0003  max mem: 4132
[20:29:49.233906] Epoch: [79]  [ 80/781]  eta: 0:02:02  lr: 0.000030  training_loss: 1.1360 (1.1642)  classification_loss: 1.1360 (1.1641)  loss_mask: 0.0001 (0.0001)  time: 0.1639  data: 0.0003  max mem: 4132
[20:29:52.518320] Epoch: [79]  [100/781]  eta: 0:01:57  lr: 0.000029  training_loss: 1.2214 (1.1751)  classification_loss: 1.2213 (1.1750)  loss_mask: 0.0001 (0.0001)  time: 0.1641  data: 0.0002  max mem: 4132
[20:29:55.787616] Epoch: [79]  [120/781]  eta: 0:01:52  lr: 0.000029  training_loss: 1.1807 (1.1778)  classification_loss: 1.1806 (1.1777)  loss_mask: 0.0001 (0.0001)  time: 0.1634  data: 0.0002  max mem: 4132
[20:29:59.087316] Epoch: [79]  [140/781]  eta: 0:01:49  lr: 0.000029  training_loss: 1.1391 (1.1717)  classification_loss: 1.1390 (1.1716)  loss_mask: 0.0001 (0.0001)  time: 0.1649  data: 0.0003  max mem: 4132
[20:30:02.356901] Epoch: [79]  [160/781]  eta: 0:01:45  lr: 0.000029  training_loss: 1.1417 (1.1699)  classification_loss: 1.1417 (1.1699)  loss_mask: 0.0000 (0.0001)  time: 0.1634  data: 0.0002  max mem: 4132
[20:30:05.626833] Epoch: [79]  [180/781]  eta: 0:01:41  lr: 0.000029  training_loss: 1.1756 (1.1696)  classification_loss: 1.1756 (1.1696)  loss_mask: 0.0001 (0.0001)  time: 0.1634  data: 0.0002  max mem: 4132
[20:30:08.930380] Epoch: [79]  [200/781]  eta: 0:01:37  lr: 0.000029  training_loss: 1.1939 (1.1716)  classification_loss: 1.1939 (1.1716)  loss_mask: 0.0001 (0.0001)  time: 0.1651  data: 0.0003  max mem: 4132
[20:30:12.234780] Epoch: [79]  [220/781]  eta: 0:01:34  lr: 0.000029  training_loss: 1.1674 (1.1740)  classification_loss: 1.1673 (1.1740)  loss_mask: 0.0001 (0.0001)  time: 0.1651  data: 0.0003  max mem: 4132
[20:30:15.526355] Epoch: [79]  [240/781]  eta: 0:01:30  lr: 0.000029  training_loss: 1.1625 (1.1732)  classification_loss: 1.1590 (1.1731)  loss_mask: 0.0001 (0.0001)  time: 0.1645  data: 0.0003  max mem: 4132
[20:30:18.810874] Epoch: [79]  [260/781]  eta: 0:01:27  lr: 0.000029  training_loss: 1.1586 (1.1752)  classification_loss: 1.1585 (1.1750)  loss_mask: 0.0001 (0.0001)  time: 0.1641  data: 0.0003  max mem: 4132
[20:30:22.098080] Epoch: [79]  [280/781]  eta: 0:01:23  lr: 0.000029  training_loss: 1.1595 (1.1744)  classification_loss: 1.1595 (1.1743)  loss_mask: 0.0001 (0.0001)  time: 0.1643  data: 0.0002  max mem: 4132
[20:30:25.401511] Epoch: [79]  [300/781]  eta: 0:01:20  lr: 0.000029  training_loss: 1.1920 (1.1755)  classification_loss: 1.1920 (1.1749)  loss_mask: 0.0001 (0.0007)  time: 0.1651  data: 0.0003  max mem: 4132
[20:30:28.680075] Epoch: [79]  [320/781]  eta: 0:01:16  lr: 0.000029  training_loss: 1.1964 (1.1772)  classification_loss: 1.1963 (1.1766)  loss_mask: 0.0001 (0.0006)  time: 0.1638  data: 0.0003  max mem: 4132
[20:30:32.012788] Epoch: [79]  [340/781]  eta: 0:01:13  lr: 0.000029  training_loss: 1.1936 (1.1794)  classification_loss: 1.1933 (1.1788)  loss_mask: 0.0001 (0.0006)  time: 0.1666  data: 0.0002  max mem: 4132
[20:30:35.301108] Epoch: [79]  [360/781]  eta: 0:01:10  lr: 0.000029  training_loss: 1.2097 (1.1805)  classification_loss: 1.2096 (1.1800)  loss_mask: 0.0001 (0.0006)  time: 0.1643  data: 0.0002  max mem: 4132
[20:30:38.581806] Epoch: [79]  [380/781]  eta: 0:01:06  lr: 0.000029  training_loss: 1.2066 (1.1833)  classification_loss: 1.2065 (1.1827)  loss_mask: 0.0001 (0.0006)  time: 0.1639  data: 0.0002  max mem: 4132
[20:30:41.848468] Epoch: [79]  [400/781]  eta: 0:01:03  lr: 0.000028  training_loss: 1.1666 (1.1837)  classification_loss: 1.1665 (1.1832)  loss_mask: 0.0001 (0.0005)  time: 0.1632  data: 0.0002  max mem: 4132
[20:30:45.161003] Epoch: [79]  [420/781]  eta: 0:01:00  lr: 0.000028  training_loss: 1.2137 (1.1859)  classification_loss: 1.2136 (1.1853)  loss_mask: 0.0001 (0.0005)  time: 0.1655  data: 0.0003  max mem: 4132
[20:30:48.491604] Epoch: [79]  [440/781]  eta: 0:00:56  lr: 0.000028  training_loss: 1.1940 (1.1870)  classification_loss: 1.1938 (1.1865)  loss_mask: 0.0001 (0.0005)  time: 0.1664  data: 0.0004  max mem: 4132
[20:30:51.775879] Epoch: [79]  [460/781]  eta: 0:00:53  lr: 0.000028  training_loss: 1.1120 (1.1851)  classification_loss: 1.1119 (1.1846)  loss_mask: 0.0001 (0.0005)  time: 0.1641  data: 0.0002  max mem: 4132
[20:30:55.059826] Epoch: [79]  [480/781]  eta: 0:00:50  lr: 0.000028  training_loss: 1.1836 (1.1859)  classification_loss: 1.1836 (1.1855)  loss_mask: 0.0000 (0.0005)  time: 0.1641  data: 0.0002  max mem: 4132
[20:30:58.339892] Epoch: [79]  [500/781]  eta: 0:00:46  lr: 0.000028  training_loss: 1.2346 (1.1870)  classification_loss: 1.2345 (1.1866)  loss_mask: 0.0001 (0.0004)  time: 0.1639  data: 0.0002  max mem: 4132
[20:31:01.653429] Epoch: [79]  [520/781]  eta: 0:00:43  lr: 0.000028  training_loss: 1.1379 (1.1866)  classification_loss: 1.1378 (1.1862)  loss_mask: 0.0000 (0.0004)  time: 0.1656  data: 0.0003  max mem: 4132
[20:31:04.957925] Epoch: [79]  [540/781]  eta: 0:00:40  lr: 0.000028  training_loss: 1.1910 (1.1861)  classification_loss: 1.1909 (1.1857)  loss_mask: 0.0001 (0.0004)  time: 0.1651  data: 0.0003  max mem: 4132
[20:31:08.215478] Epoch: [79]  [560/781]  eta: 0:00:36  lr: 0.000028  training_loss: 1.2218 (1.1865)  classification_loss: 1.2218 (1.1861)  loss_mask: 0.0001 (0.0004)  time: 0.1628  data: 0.0002  max mem: 4132
[20:31:11.478209] Epoch: [79]  [580/781]  eta: 0:00:33  lr: 0.000028  training_loss: 1.1596 (1.1874)  classification_loss: 1.1595 (1.1870)  loss_mask: 0.0001 (0.0004)  time: 0.1631  data: 0.0003  max mem: 4132
[20:31:14.769024] Epoch: [79]  [600/781]  eta: 0:00:30  lr: 0.000028  training_loss: 1.1369 (1.1873)  classification_loss: 1.1368 (1.1869)  loss_mask: 0.0001 (0.0004)  time: 0.1645  data: 0.0002  max mem: 4132
[20:31:18.030364] Epoch: [79]  [620/781]  eta: 0:00:26  lr: 0.000028  training_loss: 1.1714 (1.1867)  classification_loss: 1.1714 (1.1863)  loss_mask: 0.0001 (0.0004)  time: 0.1630  data: 0.0002  max mem: 4132
[20:31:21.317376] Epoch: [79]  [640/781]  eta: 0:00:23  lr: 0.000028  training_loss: 1.1581 (1.1864)  classification_loss: 1.1580 (1.1860)  loss_mask: 0.0000 (0.0004)  time: 0.1642  data: 0.0004  max mem: 4132
[20:31:24.630981] Epoch: [79]  [660/781]  eta: 0:00:20  lr: 0.000028  training_loss: 1.1381 (1.1853)  classification_loss: 1.1381 (1.1850)  loss_mask: 0.0001 (0.0004)  time: 0.1656  data: 0.0003  max mem: 4132
[20:31:27.948079] Epoch: [79]  [680/781]  eta: 0:00:16  lr: 0.000028  training_loss: 1.2196 (1.1858)  classification_loss: 1.2196 (1.1855)  loss_mask: 0.0000 (0.0003)  time: 0.1658  data: 0.0003  max mem: 4132
[20:31:31.233316] Epoch: [79]  [700/781]  eta: 0:00:13  lr: 0.000028  training_loss: 1.1972 (1.1865)  classification_loss: 1.1972 (1.1862)  loss_mask: 0.0001 (0.0003)  time: 0.1642  data: 0.0003  max mem: 4132
[20:31:34.548989] Epoch: [79]  [720/781]  eta: 0:00:10  lr: 0.000027  training_loss: 1.1375 (1.1855)  classification_loss: 1.1374 (1.1852)  loss_mask: 0.0001 (0.0003)  time: 0.1657  data: 0.0003  max mem: 4132
[20:31:37.838967] Epoch: [79]  [740/781]  eta: 0:00:06  lr: 0.000027  training_loss: 1.1534 (1.1854)  classification_loss: 1.1533 (1.1851)  loss_mask: 0.0000 (0.0003)  time: 0.1644  data: 0.0002  max mem: 4132
[20:31:41.143496] Epoch: [79]  [760/781]  eta: 0:00:03  lr: 0.000027  training_loss: 1.1922 (1.1859)  classification_loss: 1.1922 (1.1856)  loss_mask: 0.0000 (0.0003)  time: 0.1651  data: 0.0003  max mem: 4132
[20:31:44.469372] Epoch: [79]  [780/781]  eta: 0:00:00  lr: 0.000027  training_loss: 1.1616 (1.1857)  classification_loss: 1.1616 (1.1854)  loss_mask: 0.0000 (0.0003)  time: 0.1661  data: 0.0002  max mem: 4132
[20:31:44.670435] Epoch: [79] Total time: 0:02:09 (0.1659 s / it)
[20:31:44.671395] Averaged stats: lr: 0.000027  training_loss: 1.1616 (1.1857)  classification_loss: 1.1616 (1.1854)  loss_mask: 0.0000 (0.0003)
[20:31:45.417295] Test:  [  0/157]  eta: 0:01:56  testing_loss: 0.4859 (0.4859)  acc1: 84.3750 (84.3750)  acc5: 98.4375 (98.4375)  time: 0.7399  data: 0.6900  max mem: 4132
[20:31:45.709548] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.5068 (0.5009)  acc1: 82.8125 (84.3750)  acc5: 100.0000 (99.8580)  time: 0.0936  data: 0.0632  max mem: 4132
[20:31:45.998907] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4499 (0.4604)  acc1: 84.3750 (85.7887)  acc5: 100.0000 (99.8512)  time: 0.0289  data: 0.0004  max mem: 4132
[20:31:46.293144] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4229 (0.4766)  acc1: 85.9375 (85.6855)  acc5: 100.0000 (99.5968)  time: 0.0290  data: 0.0003  max mem: 4132
[20:31:46.581845] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4977 (0.4871)  acc1: 85.9375 (85.5183)  acc5: 98.4375 (99.3521)  time: 0.0290  data: 0.0003  max mem: 4132
[20:31:46.874584] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4699 (0.4837)  acc1: 84.3750 (85.7230)  acc5: 98.4375 (99.3260)  time: 0.0289  data: 0.0002  max mem: 4132
[20:31:47.165678] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4343 (0.4784)  acc1: 87.5000 (86.0400)  acc5: 100.0000 (99.3596)  time: 0.0290  data: 0.0002  max mem: 4132
[20:31:47.463630] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4478 (0.4753)  acc1: 85.9375 (86.0475)  acc5: 100.0000 (99.3838)  time: 0.0292  data: 0.0003  max mem: 4132
[20:31:47.752741] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4805 (0.4824)  acc1: 85.9375 (85.8603)  acc5: 98.4375 (99.3248)  time: 0.0291  data: 0.0003  max mem: 4132
[20:31:48.039038] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4945 (0.4795)  acc1: 85.9375 (85.9203)  acc5: 98.4375 (99.3132)  time: 0.0286  data: 0.0002  max mem: 4132
[20:31:48.327023] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.4945 (0.4811)  acc1: 84.3750 (85.7673)  acc5: 100.0000 (99.3038)  time: 0.0286  data: 0.0003  max mem: 4132
[20:31:48.614501] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4974 (0.4825)  acc1: 84.3750 (85.6700)  acc5: 100.0000 (99.3102)  time: 0.0286  data: 0.0002  max mem: 4132
[20:31:48.902197] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4923 (0.4829)  acc1: 85.9375 (85.6018)  acc5: 100.0000 (99.3156)  time: 0.0286  data: 0.0002  max mem: 4132
[20:31:49.188013] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4921 (0.4827)  acc1: 84.3750 (85.6035)  acc5: 100.0000 (99.3082)  time: 0.0285  data: 0.0002  max mem: 4132
[20:31:49.473165] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4434 (0.4789)  acc1: 85.9375 (85.8045)  acc5: 100.0000 (99.3462)  time: 0.0284  data: 0.0002  max mem: 4132
[20:31:49.758154] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4371 (0.4787)  acc1: 85.9375 (85.7616)  acc5: 100.0000 (99.3481)  time: 0.0284  data: 0.0002  max mem: 4132
[20:31:49.912867] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4572 (0.4797)  acc1: 85.9375 (85.7000)  acc5: 100.0000 (99.3200)  time: 0.0275  data: 0.0002  max mem: 4132
[20:31:50.088893] Test: Total time: 0:00:05 (0.0345 s / it)
[20:31:50.089466] * Acc@1 85.700 Acc@5 99.320 loss 0.480
[20:31:50.089806] Accuracy of the network on the 10000 test images: 85.7%
[20:31:50.090155] Max accuracy: 85.70%
[20:31:50.353481] log_dir: ./output_dir
[20:31:51.249285] Epoch: [80]  [  0/781]  eta: 0:11:38  lr: 0.000027  training_loss: 1.1306 (1.1306)  classification_loss: 1.1306 (1.1306)  loss_mask: 0.0001 (0.0001)  time: 0.8939  data: 0.6899  max mem: 4132
[20:31:54.553152] Epoch: [80]  [ 20/781]  eta: 0:02:32  lr: 0.000027  training_loss: 1.1478 (1.1524)  classification_loss: 1.1477 (1.1524)  loss_mask: 0.0000 (0.0001)  time: 0.1651  data: 0.0002  max mem: 4132
[20:31:57.847945] Epoch: [80]  [ 40/781]  eta: 0:02:15  lr: 0.000027  training_loss: 1.1516 (1.1665)  classification_loss: 1.1515 (1.1665)  loss_mask: 0.0000 (0.0001)  time: 0.1647  data: 0.0003  max mem: 4132
[20:32:01.129364] Epoch: [80]  [ 60/781]  eta: 0:02:07  lr: 0.000027  training_loss: 1.2236 (1.1840)  classification_loss: 1.2235 (1.1840)  loss_mask: 0.0001 (0.0001)  time: 0.1640  data: 0.0003  max mem: 4132
[20:32:04.440031] Epoch: [80]  [ 80/781]  eta: 0:02:01  lr: 0.000027  training_loss: 1.1431 (1.1773)  classification_loss: 1.1431 (1.1772)  loss_mask: 0.0000 (0.0001)  time: 0.1654  data: 0.0003  max mem: 4132
[20:32:07.719340] Epoch: [80]  [100/781]  eta: 0:01:57  lr: 0.000027  training_loss: 1.1782 (1.1764)  classification_loss: 1.1781 (1.1763)  loss_mask: 0.0000 (0.0001)  time: 0.1639  data: 0.0002  max mem: 4132
[20:32:10.992467] Epoch: [80]  [120/781]  eta: 0:01:52  lr: 0.000027  training_loss: 1.1864 (1.1805)  classification_loss: 1.1863 (1.1805)  loss_mask: 0.0000 (0.0001)  time: 0.1636  data: 0.0002  max mem: 4132
[20:32:14.281948] Epoch: [80]  [140/781]  eta: 0:01:48  lr: 0.000027  training_loss: 1.1318 (1.1775)  classification_loss: 1.1317 (1.1774)  loss_mask: 0.0001 (0.0001)  time: 0.1644  data: 0.0002  max mem: 4132
[20:32:17.538467] Epoch: [80]  [160/781]  eta: 0:01:44  lr: 0.000027  training_loss: 1.1733 (1.1752)  classification_loss: 1.1732 (1.1748)  loss_mask: 0.0001 (0.0004)  time: 0.1627  data: 0.0002  max mem: 4132
[20:32:20.845795] Epoch: [80]  [180/781]  eta: 0:01:41  lr: 0.000027  training_loss: 1.1612 (1.1759)  classification_loss: 1.1611 (1.1755)  loss_mask: 0.0000 (0.0004)  time: 0.1653  data: 0.0003  max mem: 4132
[20:32:24.110710] Epoch: [80]  [200/781]  eta: 0:01:37  lr: 0.000027  training_loss: 1.1121 (1.1725)  classification_loss: 1.1120 (1.1722)  loss_mask: 0.0000 (0.0004)  time: 0.1632  data: 0.0002  max mem: 4132
[20:32:27.373993] Epoch: [80]  [220/781]  eta: 0:01:33  lr: 0.000027  training_loss: 1.1351 (1.1705)  classification_loss: 1.1351 (1.1702)  loss_mask: 0.0001 (0.0003)  time: 0.1631  data: 0.0003  max mem: 4132
[20:32:30.664202] Epoch: [80]  [240/781]  eta: 0:01:30  lr: 0.000026  training_loss: 1.1347 (1.1705)  classification_loss: 1.1346 (1.1702)  loss_mask: 0.0000 (0.0003)  time: 0.1644  data: 0.0003  max mem: 4132
[20:32:33.944959] Epoch: [80]  [260/781]  eta: 0:01:26  lr: 0.000026  training_loss: 1.2012 (1.1737)  classification_loss: 1.2011 (1.1734)  loss_mask: 0.0001 (0.0003)  time: 0.1639  data: 0.0004  max mem: 4132
[20:32:37.206663] Epoch: [80]  [280/781]  eta: 0:01:23  lr: 0.000026  training_loss: 1.1496 (1.1724)  classification_loss: 1.1496 (1.1721)  loss_mask: 0.0000 (0.0003)  time: 0.1630  data: 0.0002  max mem: 4132
[20:32:40.472820] Epoch: [80]  [300/781]  eta: 0:01:20  lr: 0.000026  training_loss: 1.1949 (1.1730)  classification_loss: 1.1948 (1.1727)  loss_mask: 0.0000 (0.0003)  time: 0.1632  data: 0.0002  max mem: 4132
[20:32:43.795450] Epoch: [80]  [320/781]  eta: 0:01:16  lr: 0.000026  training_loss: 1.1674 (1.1732)  classification_loss: 1.1674 (1.1729)  loss_mask: 0.0000 (0.0002)  time: 0.1660  data: 0.0002  max mem: 4132
[20:32:47.115083] Epoch: [80]  [340/781]  eta: 0:01:13  lr: 0.000026  training_loss: 1.1497 (1.1719)  classification_loss: 1.1497 (1.1717)  loss_mask: 0.0000 (0.0002)  time: 0.1659  data: 0.0003  max mem: 4132
[20:32:50.429866] Epoch: [80]  [360/781]  eta: 0:01:10  lr: 0.000026  training_loss: 1.1491 (1.1710)  classification_loss: 1.1490 (1.1708)  loss_mask: 0.0000 (0.0002)  time: 0.1656  data: 0.0003  max mem: 4132
[20:32:53.733296] Epoch: [80]  [380/781]  eta: 0:01:06  lr: 0.000026  training_loss: 1.1432 (1.1709)  classification_loss: 1.1432 (1.1707)  loss_mask: 0.0000 (0.0002)  time: 0.1650  data: 0.0002  max mem: 4132
[20:32:57.081205] Epoch: [80]  [400/781]  eta: 0:01:03  lr: 0.000026  training_loss: 1.1439 (1.1705)  classification_loss: 1.1438 (1.1703)  loss_mask: 0.0000 (0.0002)  time: 0.1673  data: 0.0004  max mem: 4132
[20:33:00.368530] Epoch: [80]  [420/781]  eta: 0:01:00  lr: 0.000026  training_loss: 1.1719 (1.1704)  classification_loss: 1.1718 (1.1702)  loss_mask: 0.0000 (0.0002)  time: 0.1643  data: 0.0003  max mem: 4132
[20:33:03.637631] Epoch: [80]  [440/781]  eta: 0:00:56  lr: 0.000026  training_loss: 1.1351 (1.1705)  classification_loss: 1.1350 (1.1703)  loss_mask: 0.0001 (0.0002)  time: 0.1634  data: 0.0002  max mem: 4132
[20:33:06.953506] Epoch: [80]  [460/781]  eta: 0:00:53  lr: 0.000026  training_loss: 1.1517 (1.1698)  classification_loss: 1.1516 (1.1696)  loss_mask: 0.0000 (0.0002)  time: 0.1657  data: 0.0003  max mem: 4132
[20:33:10.233995] Epoch: [80]  [480/781]  eta: 0:00:49  lr: 0.000026  training_loss: 1.1519 (1.1702)  classification_loss: 1.1519 (1.1700)  loss_mask: 0.0000 (0.0002)  time: 0.1639  data: 0.0003  max mem: 4132
[20:33:13.550605] Epoch: [80]  [500/781]  eta: 0:00:46  lr: 0.000026  training_loss: 1.1778 (1.1699)  classification_loss: 1.1777 (1.1698)  loss_mask: 0.0000 (0.0002)  time: 0.1657  data: 0.0002  max mem: 4132
[20:33:16.834117] Epoch: [80]  [520/781]  eta: 0:00:43  lr: 0.000026  training_loss: 1.1852 (1.1700)  classification_loss: 1.1852 (1.1698)  loss_mask: 0.0001 (0.0002)  time: 0.1641  data: 0.0003  max mem: 4132
[20:33:20.141332] Epoch: [80]  [540/781]  eta: 0:00:39  lr: 0.000026  training_loss: 1.1903 (1.1697)  classification_loss: 1.1903 (1.1695)  loss_mask: 0.0000 (0.0002)  time: 0.1653  data: 0.0003  max mem: 4132
[20:33:23.429978] Epoch: [80]  [560/781]  eta: 0:00:36  lr: 0.000025  training_loss: 1.1363 (1.1696)  classification_loss: 1.1362 (1.1695)  loss_mask: 0.0000 (0.0002)  time: 0.1643  data: 0.0003  max mem: 4132
[20:33:26.731397] Epoch: [80]  [580/781]  eta: 0:00:33  lr: 0.000025  training_loss: 1.1688 (1.1705)  classification_loss: 1.1688 (1.1704)  loss_mask: 0.0000 (0.0002)  time: 0.1650  data: 0.0002  max mem: 4132
[20:33:30.054397] Epoch: [80]  [600/781]  eta: 0:00:30  lr: 0.000025  training_loss: 1.1549 (1.1703)  classification_loss: 1.1549 (1.1701)  loss_mask: 0.0000 (0.0002)  time: 0.1660  data: 0.0003  max mem: 4132
[20:33:33.370360] Epoch: [80]  [620/781]  eta: 0:00:26  lr: 0.000025  training_loss: 1.1875 (1.1704)  classification_loss: 1.1875 (1.1703)  loss_mask: 0.0000 (0.0001)  time: 0.1657  data: 0.0002  max mem: 4132
[20:33:36.640632] Epoch: [80]  [640/781]  eta: 0:00:23  lr: 0.000025  training_loss: 1.1403 (1.1706)  classification_loss: 1.1402 (1.1704)  loss_mask: 0.0000 (0.0001)  time: 0.1634  data: 0.0003  max mem: 4132
[20:33:39.945477] Epoch: [80]  [660/781]  eta: 0:00:20  lr: 0.000025  training_loss: 1.1869 (1.1707)  classification_loss: 1.1868 (1.1705)  loss_mask: 0.0001 (0.0002)  time: 0.1651  data: 0.0003  max mem: 4132
[20:33:43.253760] Epoch: [80]  [680/781]  eta: 0:00:16  lr: 0.000025  training_loss: 1.2136 (1.1716)  classification_loss: 1.2134 (1.1714)  loss_mask: 0.0001 (0.0002)  time: 0.1653  data: 0.0002  max mem: 4132
[20:33:46.555475] Epoch: [80]  [700/781]  eta: 0:00:13  lr: 0.000025  training_loss: 1.2107 (1.1731)  classification_loss: 1.2106 (1.1729)  loss_mask: 0.0000 (0.0002)  time: 0.1650  data: 0.0003  max mem: 4132
[20:33:49.822079] Epoch: [80]  [720/781]  eta: 0:00:10  lr: 0.000025  training_loss: 1.1688 (1.1740)  classification_loss: 1.1687 (1.1739)  loss_mask: 0.0000 (0.0001)  time: 0.1632  data: 0.0002  max mem: 4132
[20:33:53.084515] Epoch: [80]  [740/781]  eta: 0:00:06  lr: 0.000025  training_loss: 1.0976 (1.1728)  classification_loss: 1.0976 (1.1726)  loss_mask: 0.0000 (0.0001)  time: 0.1630  data: 0.0002  max mem: 4132
[20:33:56.348790] Epoch: [80]  [760/781]  eta: 0:00:03  lr: 0.000025  training_loss: 1.1970 (1.1728)  classification_loss: 1.1970 (1.1727)  loss_mask: 0.0000 (0.0001)  time: 0.1631  data: 0.0003  max mem: 4132
[20:33:59.577970] Epoch: [80]  [780/781]  eta: 0:00:00  lr: 0.000025  training_loss: 1.1335 (1.1724)  classification_loss: 1.1334 (1.1723)  loss_mask: 0.0000 (0.0001)  time: 0.1614  data: 0.0002  max mem: 4132
[20:33:59.756178] Epoch: [80] Total time: 0:02:09 (0.1657 s / it)
[20:33:59.756655] Averaged stats: lr: 0.000025  training_loss: 1.1335 (1.1724)  classification_loss: 1.1334 (1.1723)  loss_mask: 0.0000 (0.0001)
[20:34:01.616967] Test:  [  0/157]  eta: 0:01:51  testing_loss: 0.4653 (0.4653)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.7095  data: 0.6667  max mem: 4132
[20:34:01.926845] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.5034 (0.5103)  acc1: 84.3750 (84.5170)  acc5: 100.0000 (99.4318)  time: 0.0922  data: 0.0609  max mem: 4132
[20:34:02.223494] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4681 (0.4727)  acc1: 84.3750 (86.1607)  acc5: 100.0000 (99.4048)  time: 0.0299  data: 0.0003  max mem: 4132
[20:34:02.512863] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4226 (0.4853)  acc1: 87.5000 (85.7359)  acc5: 100.0000 (99.2440)  time: 0.0291  data: 0.0003  max mem: 4132
[20:34:02.800561] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4941 (0.4930)  acc1: 84.3750 (85.5945)  acc5: 98.4375 (99.0091)  time: 0.0286  data: 0.0002  max mem: 4132
[20:34:03.085195] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4795 (0.4880)  acc1: 84.3750 (85.7230)  acc5: 98.4375 (99.0502)  time: 0.0285  data: 0.0002  max mem: 4132
[20:34:03.370064] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4579 (0.4819)  acc1: 85.9375 (85.9631)  acc5: 100.0000 (99.1035)  time: 0.0283  data: 0.0002  max mem: 4132
[20:34:03.660437] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4447 (0.4782)  acc1: 85.9375 (86.1356)  acc5: 100.0000 (99.1417)  time: 0.0286  data: 0.0002  max mem: 4132
[20:34:03.950458] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4680 (0.4857)  acc1: 85.9375 (85.7639)  acc5: 98.4375 (99.0741)  time: 0.0289  data: 0.0002  max mem: 4132
[20:34:04.237426] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5094 (0.4846)  acc1: 84.3750 (85.7143)  acc5: 100.0000 (99.0900)  time: 0.0287  data: 0.0002  max mem: 4132
[20:34:04.525654] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.5090 (0.4881)  acc1: 84.3750 (85.4734)  acc5: 100.0000 (99.1027)  time: 0.0286  data: 0.0002  max mem: 4132
[20:34:04.812435] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5115 (0.4896)  acc1: 84.3750 (85.4730)  acc5: 100.0000 (99.1273)  time: 0.0286  data: 0.0002  max mem: 4132
[20:34:05.099591] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5006 (0.4905)  acc1: 85.9375 (85.4210)  acc5: 100.0000 (99.1219)  time: 0.0286  data: 0.0002  max mem: 4132
[20:34:05.386432] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4922 (0.4901)  acc1: 84.3750 (85.4365)  acc5: 100.0000 (99.1412)  time: 0.0286  data: 0.0002  max mem: 4132
[20:34:05.670629] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4774 (0.4867)  acc1: 84.3750 (85.5829)  acc5: 100.0000 (99.1578)  time: 0.0284  data: 0.0002  max mem: 4132
[20:34:05.954304] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4774 (0.4867)  acc1: 84.3750 (85.5236)  acc5: 100.0000 (99.1515)  time: 0.0282  data: 0.0002  max mem: 4132
[20:34:06.107905] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4748 (0.4878)  acc1: 84.3750 (85.4700)  acc5: 98.4375 (99.1400)  time: 0.0274  data: 0.0002  max mem: 4132
[20:34:06.277554] Test: Total time: 0:00:05 (0.0342 s / it)
[20:34:06.279425] * Acc@1 85.470 Acc@5 99.140 loss 0.488
[20:34:06.280398] Accuracy of the network on the 10000 test images: 85.5%
[20:34:06.281018] Max accuracy: 85.70%
[20:34:06.589081] log_dir: ./output_dir
[20:34:07.593248] Epoch: [81]  [  0/781]  eta: 0:13:02  lr: 0.000025  training_loss: 0.9566 (0.9566)  classification_loss: 0.9566 (0.9566)  loss_mask: 0.0000 (0.0000)  time: 1.0021  data: 0.7930  max mem: 4132
[20:34:10.887551] Epoch: [81]  [ 20/781]  eta: 0:02:35  lr: 0.000025  training_loss: 1.1279 (1.1430)  classification_loss: 1.1279 (1.1430)  loss_mask: 0.0000 (0.0000)  time: 0.1646  data: 0.0002  max mem: 4132
[20:34:14.179390] Epoch: [81]  [ 40/781]  eta: 0:02:17  lr: 0.000025  training_loss: 1.1455 (1.1446)  classification_loss: 1.1455 (1.1445)  loss_mask: 0.0000 (0.0000)  time: 0.1645  data: 0.0003  max mem: 4132
[20:34:17.456167] Epoch: [81]  [ 60/781]  eta: 0:02:08  lr: 0.000025  training_loss: 1.1710 (1.1584)  classification_loss: 1.1710 (1.1583)  loss_mask: 0.0000 (0.0000)  time: 0.1638  data: 0.0002  max mem: 4132
[20:34:20.711087] Epoch: [81]  [ 80/781]  eta: 0:02:02  lr: 0.000025  training_loss: 1.1274 (1.1542)  classification_loss: 1.1273 (1.1541)  loss_mask: 0.0000 (0.0000)  time: 0.1627  data: 0.0002  max mem: 4132
[20:34:24.004352] Epoch: [81]  [100/781]  eta: 0:01:57  lr: 0.000024  training_loss: 1.1684 (1.1621)  classification_loss: 1.1679 (1.1621)  loss_mask: 0.0000 (0.0001)  time: 0.1645  data: 0.0003  max mem: 4132
[20:34:27.298197] Epoch: [81]  [120/781]  eta: 0:01:53  lr: 0.000024  training_loss: 1.1318 (1.1584)  classification_loss: 1.1317 (1.1583)  loss_mask: 0.0000 (0.0001)  time: 0.1646  data: 0.0003  max mem: 4132
[20:34:30.597880] Epoch: [81]  [140/781]  eta: 0:01:49  lr: 0.000024  training_loss: 1.1727 (1.1607)  classification_loss: 1.1726 (1.1607)  loss_mask: 0.0000 (0.0001)  time: 0.1649  data: 0.0003  max mem: 4132
[20:34:33.884996] Epoch: [81]  [160/781]  eta: 0:01:45  lr: 0.000024  training_loss: 1.1705 (1.1605)  classification_loss: 1.1705 (1.1605)  loss_mask: 0.0000 (0.0001)  time: 0.1643  data: 0.0003  max mem: 4132
[20:34:37.175102] Epoch: [81]  [180/781]  eta: 0:01:41  lr: 0.000024  training_loss: 1.1565 (1.1613)  classification_loss: 1.1564 (1.1613)  loss_mask: 0.0000 (0.0001)  time: 0.1644  data: 0.0003  max mem: 4132
[20:34:40.457496] Epoch: [81]  [200/781]  eta: 0:01:37  lr: 0.000024  training_loss: 1.1383 (1.1604)  classification_loss: 1.1382 (1.1604)  loss_mask: 0.0000 (0.0001)  time: 0.1640  data: 0.0003  max mem: 4132
[20:34:43.745397] Epoch: [81]  [220/781]  eta: 0:01:34  lr: 0.000024  training_loss: 1.1812 (1.1607)  classification_loss: 1.1812 (1.1607)  loss_mask: 0.0000 (0.0000)  time: 0.1643  data: 0.0003  max mem: 4132
[20:34:47.054989] Epoch: [81]  [240/781]  eta: 0:01:30  lr: 0.000024  training_loss: 1.1707 (1.1619)  classification_loss: 1.1706 (1.1618)  loss_mask: 0.0000 (0.0000)  time: 0.1654  data: 0.0002  max mem: 4132
[20:34:50.347274] Epoch: [81]  [260/781]  eta: 0:01:27  lr: 0.000024  training_loss: 1.1499 (1.1618)  classification_loss: 1.1499 (1.1618)  loss_mask: 0.0000 (0.0000)  time: 0.1645  data: 0.0003  max mem: 4132
[20:34:53.639632] Epoch: [81]  [280/781]  eta: 0:01:23  lr: 0.000024  training_loss: 1.1480 (1.1612)  classification_loss: 1.1479 (1.1611)  loss_mask: 0.0000 (0.0000)  time: 0.1645  data: 0.0004  max mem: 4132
[20:34:56.902805] Epoch: [81]  [300/781]  eta: 0:01:20  lr: 0.000024  training_loss: 1.2244 (1.1669)  classification_loss: 1.2243 (1.1669)  loss_mask: 0.0000 (0.0000)  time: 0.1630  data: 0.0002  max mem: 4132
[20:35:00.154236] Epoch: [81]  [320/781]  eta: 0:01:16  lr: 0.000024  training_loss: 1.1863 (1.1687)  classification_loss: 1.1863 (1.1686)  loss_mask: 0.0000 (0.0000)  time: 0.1625  data: 0.0002  max mem: 4132
[20:35:03.404273] Epoch: [81]  [340/781]  eta: 0:01:13  lr: 0.000024  training_loss: 1.1544 (1.1692)  classification_loss: 1.1544 (1.1691)  loss_mask: 0.0000 (0.0000)  time: 0.1624  data: 0.0002  max mem: 4132
[20:35:06.653840] Epoch: [81]  [360/781]  eta: 0:01:10  lr: 0.000024  training_loss: 1.1723 (1.1701)  classification_loss: 1.1723 (1.1700)  loss_mask: 0.0000 (0.0000)  time: 0.1624  data: 0.0002  max mem: 4132
[20:35:09.935656] Epoch: [81]  [380/781]  eta: 0:01:06  lr: 0.000024  training_loss: 1.1890 (1.1726)  classification_loss: 1.1889 (1.1726)  loss_mask: 0.0000 (0.0000)  time: 0.1640  data: 0.0002  max mem: 4132
[20:35:13.192341] Epoch: [81]  [400/781]  eta: 0:01:03  lr: 0.000024  training_loss: 1.1454 (1.1722)  classification_loss: 1.1454 (1.1721)  loss_mask: 0.0000 (0.0000)  time: 0.1627  data: 0.0002  max mem: 4132
[20:35:16.460187] Epoch: [81]  [420/781]  eta: 0:00:59  lr: 0.000023  training_loss: 1.1451 (1.1726)  classification_loss: 1.1450 (1.1725)  loss_mask: 0.0000 (0.0000)  time: 0.1633  data: 0.0003  max mem: 4132
[20:35:19.744193] Epoch: [81]  [440/781]  eta: 0:00:56  lr: 0.000023  training_loss: 1.1208 (1.1710)  classification_loss: 1.1208 (1.1710)  loss_mask: 0.0000 (0.0000)  time: 0.1641  data: 0.0003  max mem: 4132
[20:35:23.033506] Epoch: [81]  [460/781]  eta: 0:00:53  lr: 0.000023  training_loss: 1.1274 (1.1690)  classification_loss: 1.1273 (1.1690)  loss_mask: 0.0000 (0.0000)  time: 0.1644  data: 0.0003  max mem: 4132
[20:35:26.318815] Epoch: [81]  [480/781]  eta: 0:00:49  lr: 0.000023  training_loss: 1.1494 (1.1696)  classification_loss: 1.1494 (1.1696)  loss_mask: 0.0000 (0.0000)  time: 0.1642  data: 0.0002  max mem: 4132
[20:35:29.637988] Epoch: [81]  [500/781]  eta: 0:00:46  lr: 0.000023  training_loss: 1.1918 (1.1712)  classification_loss: 1.1918 (1.1712)  loss_mask: 0.0000 (0.0000)  time: 0.1659  data: 0.0003  max mem: 4132
[20:35:32.937974] Epoch: [81]  [520/781]  eta: 0:00:43  lr: 0.000023  training_loss: 1.1197 (1.1710)  classification_loss: 1.1196 (1.1709)  loss_mask: 0.0000 (0.0000)  time: 0.1648  data: 0.0002  max mem: 4132
[20:35:36.212419] Epoch: [81]  [540/781]  eta: 0:00:39  lr: 0.000023  training_loss: 1.1684 (1.1715)  classification_loss: 1.1684 (1.1715)  loss_mask: 0.0000 (0.0000)  time: 0.1636  data: 0.0002  max mem: 4132
[20:35:39.510901] Epoch: [81]  [560/781]  eta: 0:00:36  lr: 0.000023  training_loss: 1.1582 (1.1717)  classification_loss: 1.1581 (1.1716)  loss_mask: 0.0000 (0.0000)  time: 0.1648  data: 0.0003  max mem: 4132
[20:35:42.812230] Epoch: [81]  [580/781]  eta: 0:00:33  lr: 0.000023  training_loss: 1.1676 (1.1712)  classification_loss: 1.1676 (1.1712)  loss_mask: 0.0000 (0.0000)  time: 0.1650  data: 0.0003  max mem: 4132
[20:35:46.113942] Epoch: [81]  [600/781]  eta: 0:00:29  lr: 0.000023  training_loss: 1.1672 (1.1709)  classification_loss: 1.1671 (1.1708)  loss_mask: 0.0000 (0.0000)  time: 0.1650  data: 0.0003  max mem: 4132
[20:35:49.416846] Epoch: [81]  [620/781]  eta: 0:00:26  lr: 0.000023  training_loss: 1.1551 (1.1711)  classification_loss: 1.1550 (1.1710)  loss_mask: 0.0000 (0.0000)  time: 0.1651  data: 0.0002  max mem: 4132
[20:35:52.727110] Epoch: [81]  [640/781]  eta: 0:00:23  lr: 0.000023  training_loss: 1.1869 (1.1715)  classification_loss: 1.1868 (1.1714)  loss_mask: 0.0000 (0.0000)  time: 0.1654  data: 0.0002  max mem: 4132
[20:35:56.018314] Epoch: [81]  [660/781]  eta: 0:00:20  lr: 0.000023  training_loss: 1.1055 (1.1706)  classification_loss: 1.1055 (1.1705)  loss_mask: 0.0000 (0.0000)  time: 0.1644  data: 0.0003  max mem: 4132
[20:35:59.333999] Epoch: [81]  [680/781]  eta: 0:00:16  lr: 0.000023  training_loss: 1.1832 (1.1714)  classification_loss: 1.1831 (1.1714)  loss_mask: 0.0000 (0.0000)  time: 0.1657  data: 0.0003  max mem: 4132
[20:36:02.596278] Epoch: [81]  [700/781]  eta: 0:00:13  lr: 0.000023  training_loss: 1.2025 (1.1719)  classification_loss: 1.2024 (1.1718)  loss_mask: 0.0000 (0.0000)  time: 0.1630  data: 0.0002  max mem: 4132
[20:36:05.883547] Epoch: [81]  [720/781]  eta: 0:00:10  lr: 0.000023  training_loss: 1.1329 (1.1713)  classification_loss: 1.1328 (1.1713)  loss_mask: 0.0000 (0.0000)  time: 0.1643  data: 0.0003  max mem: 4132
[20:36:09.168571] Epoch: [81]  [740/781]  eta: 0:00:06  lr: 0.000023  training_loss: 1.1164 (1.1703)  classification_loss: 1.1164 (1.1702)  loss_mask: 0.0000 (0.0000)  time: 0.1641  data: 0.0003  max mem: 4132
[20:36:12.486615] Epoch: [81]  [760/781]  eta: 0:00:03  lr: 0.000022  training_loss: 1.1642 (1.1699)  classification_loss: 1.1642 (1.1699)  loss_mask: 0.0000 (0.0000)  time: 0.1658  data: 0.0003  max mem: 4132
[20:36:15.729653] Epoch: [81]  [780/781]  eta: 0:00:00  lr: 0.000022  training_loss: 1.1839 (1.1702)  classification_loss: 1.1838 (1.1702)  loss_mask: 0.0000 (0.0000)  time: 0.1620  data: 0.0002  max mem: 4132
[20:36:15.910588] Epoch: [81] Total time: 0:02:09 (0.1656 s / it)
[20:36:15.911138] Averaged stats: lr: 0.000022  training_loss: 1.1839 (1.1702)  classification_loss: 1.1838 (1.1702)  loss_mask: 0.0000 (0.0000)
[20:36:16.593327] Test:  [  0/157]  eta: 0:01:46  testing_loss: 0.4855 (0.4855)  acc1: 84.3750 (84.3750)  acc5: 100.0000 (100.0000)  time: 0.6780  data: 0.6474  max mem: 4132
[20:36:16.881830] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4855 (0.4933)  acc1: 84.3750 (84.9432)  acc5: 100.0000 (99.8580)  time: 0.0876  data: 0.0590  max mem: 4132
[20:36:17.165203] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4461 (0.4628)  acc1: 85.9375 (86.0119)  acc5: 100.0000 (99.6280)  time: 0.0284  data: 0.0002  max mem: 4132
[20:36:17.449089] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4408 (0.4824)  acc1: 85.9375 (85.6855)  acc5: 100.0000 (99.3448)  time: 0.0282  data: 0.0002  max mem: 4132
[20:36:17.739890] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5060 (0.4906)  acc1: 85.9375 (85.5564)  acc5: 98.4375 (99.1235)  time: 0.0286  data: 0.0002  max mem: 4132
[20:36:18.025370] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4963 (0.4843)  acc1: 85.9375 (85.8150)  acc5: 100.0000 (99.2341)  time: 0.0287  data: 0.0002  max mem: 4132
[20:36:18.313857] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4350 (0.4770)  acc1: 89.0625 (86.3986)  acc5: 100.0000 (99.2059)  time: 0.0286  data: 0.0002  max mem: 4132
[20:36:18.602178] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4437 (0.4739)  acc1: 87.5000 (86.4657)  acc5: 100.0000 (99.2298)  time: 0.0287  data: 0.0002  max mem: 4132
[20:36:18.886833] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4757 (0.4813)  acc1: 85.9375 (86.1497)  acc5: 98.4375 (99.1898)  time: 0.0285  data: 0.0002  max mem: 4132
[20:36:19.173153] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4985 (0.4791)  acc1: 85.9375 (86.0405)  acc5: 98.4375 (99.2102)  time: 0.0284  data: 0.0002  max mem: 4132
[20:36:19.457606] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4941 (0.4818)  acc1: 85.9375 (85.9220)  acc5: 100.0000 (99.2265)  time: 0.0284  data: 0.0002  max mem: 4132
[20:36:19.743468] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4963 (0.4822)  acc1: 85.9375 (85.9375)  acc5: 100.0000 (99.1976)  time: 0.0284  data: 0.0002  max mem: 4132
[20:36:20.032759] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4633 (0.4825)  acc1: 85.9375 (85.9246)  acc5: 100.0000 (99.1994)  time: 0.0286  data: 0.0002  max mem: 4132
[20:36:20.322090] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4824 (0.4826)  acc1: 85.9375 (85.7944)  acc5: 100.0000 (99.2009)  time: 0.0288  data: 0.0002  max mem: 4132
[20:36:20.609321] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4591 (0.4801)  acc1: 84.3750 (85.8599)  acc5: 100.0000 (99.2132)  time: 0.0287  data: 0.0002  max mem: 4132
[20:36:20.892219] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4591 (0.4802)  acc1: 84.3750 (85.8030)  acc5: 100.0000 (99.2136)  time: 0.0283  data: 0.0001  max mem: 4132
[20:36:21.044558] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4894 (0.4810)  acc1: 84.3750 (85.7100)  acc5: 98.4375 (99.2000)  time: 0.0272  data: 0.0001  max mem: 4132
[20:36:21.206204] Test: Total time: 0:00:05 (0.0337 s / it)
[20:36:21.207602] * Acc@1 85.710 Acc@5 99.200 loss 0.481
[20:36:21.208466] Accuracy of the network on the 10000 test images: 85.7%
[20:36:21.209004] Max accuracy: 85.71%
[20:36:21.383788] log_dir: ./output_dir
[20:36:22.254926] Epoch: [82]  [  0/781]  eta: 0:11:18  lr: 0.000022  training_loss: 1.1229 (1.1229)  classification_loss: 1.1228 (1.1228)  loss_mask: 0.0000 (0.0000)  time: 0.8686  data: 0.6717  max mem: 4132
[20:36:25.515375] Epoch: [82]  [ 20/781]  eta: 0:02:29  lr: 0.000022  training_loss: 1.1760 (1.1678)  classification_loss: 1.1760 (1.1677)  loss_mask: 0.0000 (0.0000)  time: 0.1629  data: 0.0002  max mem: 4132
[20:36:28.795631] Epoch: [82]  [ 40/781]  eta: 0:02:13  lr: 0.000022  training_loss: 1.1881 (1.1838)  classification_loss: 1.1881 (1.1838)  loss_mask: 0.0000 (0.0000)  time: 0.1639  data: 0.0002  max mem: 4132
[20:36:32.103448] Epoch: [82]  [ 60/781]  eta: 0:02:06  lr: 0.000022  training_loss: 1.1532 (1.1806)  classification_loss: 1.1532 (1.1806)  loss_mask: 0.0000 (0.0000)  time: 0.1653  data: 0.0003  max mem: 4132
[20:36:35.412819] Epoch: [82]  [ 80/781]  eta: 0:02:01  lr: 0.000022  training_loss: 1.1590 (1.1797)  classification_loss: 1.1589 (1.1797)  loss_mask: 0.0000 (0.0000)  time: 0.1654  data: 0.0003  max mem: 4132
[20:36:38.706314] Epoch: [82]  [100/781]  eta: 0:01:56  lr: 0.000022  training_loss: 1.1673 (1.1791)  classification_loss: 1.1673 (1.1790)  loss_mask: 0.0000 (0.0000)  time: 0.1646  data: 0.0003  max mem: 4132
[20:36:41.977255] Epoch: [82]  [120/781]  eta: 0:01:52  lr: 0.000022  training_loss: 1.1745 (1.1758)  classification_loss: 1.1745 (1.1758)  loss_mask: 0.0000 (0.0000)  time: 0.1634  data: 0.0003  max mem: 4132
[20:36:45.268539] Epoch: [82]  [140/781]  eta: 0:01:48  lr: 0.000022  training_loss: 1.1254 (1.1733)  classification_loss: 1.1254 (1.1733)  loss_mask: 0.0000 (0.0000)  time: 0.1645  data: 0.0004  max mem: 4132
[20:36:48.550906] Epoch: [82]  [160/781]  eta: 0:01:44  lr: 0.000022  training_loss: 1.1582 (1.1728)  classification_loss: 1.1582 (1.1727)  loss_mask: 0.0000 (0.0000)  time: 0.1640  data: 0.0002  max mem: 4132
[20:36:51.841283] Epoch: [82]  [180/781]  eta: 0:01:41  lr: 0.000022  training_loss: 1.1580 (1.1724)  classification_loss: 1.1578 (1.1724)  loss_mask: 0.0000 (0.0000)  time: 0.1644  data: 0.0003  max mem: 4132
[20:36:55.134054] Epoch: [82]  [200/781]  eta: 0:01:37  lr: 0.000022  training_loss: 1.2096 (1.1731)  classification_loss: 1.2096 (1.1731)  loss_mask: 0.0000 (0.0000)  time: 0.1646  data: 0.0002  max mem: 4132
[20:36:58.409216] Epoch: [82]  [220/781]  eta: 0:01:33  lr: 0.000022  training_loss: 1.1765 (1.1740)  classification_loss: 1.1765 (1.1740)  loss_mask: 0.0000 (0.0000)  time: 0.1637  data: 0.0003  max mem: 4132
[20:37:01.696343] Epoch: [82]  [240/781]  eta: 0:01:30  lr: 0.000022  training_loss: 1.1110 (1.1728)  classification_loss: 1.1110 (1.1728)  loss_mask: 0.0000 (0.0000)  time: 0.1642  data: 0.0004  max mem: 4132

[20:37:05.003323] Epoch: [82]  [260/781]  eta: 0:01:27  lr: 0.000022  training_loss: 1.1255 (1.1705)  classification_loss: 1.1254 (1.1704)  loss_mask: 0.0000 (0.0000)  time: 0.1652  data: 0.0002  max mem: 4132
[20:37:08.306615] Epoch: [82]  [280/781]  eta: 0:01:23  lr: 0.000022  training_loss: 1.1054 (1.1671)  classification_loss: 1.1054 (1.1671)  loss_mask: 0.0000 (0.0000)  time: 0.1651  data: 0.0003  max mem: 4132
[20:37:11.603929] Epoch: [82]  [300/781]  eta: 0:01:20  lr: 0.000022  training_loss: 1.1557 (1.1674)  classification_loss: 1.1557 (1.1674)  loss_mask: 0.0000 (0.0000)  time: 0.1648  data: 0.0003  max mem: 4132
[20:37:14.890688] Epoch: [82]  [320/781]  eta: 0:01:16  lr: 0.000021  training_loss: 1.1534 (1.1661)  classification_loss: 1.1533 (1.1660)  loss_mask: 0.0000 (0.0000)  time: 0.1643  data: 0.0009  max mem: 4132
[20:37:18.158974] Epoch: [82]  [340/781]  eta: 0:01:13  lr: 0.000021  training_loss: 1.1573 (1.1655)  classification_loss: 1.1572 (1.1654)  loss_mask: 0.0000 (0.0000)  time: 0.1633  data: 0.0002  max mem: 4132
[20:37:21.463033] Epoch: [82]  [360/781]  eta: 0:01:10  lr: 0.000021  training_loss: 1.1931 (1.1677)  classification_loss: 1.1930 (1.1677)  loss_mask: 0.0000 (0.0000)  time: 0.1651  data: 0.0003  max mem: 4132
[20:37:24.766866] Epoch: [82]  [380/781]  eta: 0:01:06  lr: 0.000021  training_loss: 1.1631 (1.1683)  classification_loss: 1.1630 (1.1682)  loss_mask: 0.0000 (0.0000)  time: 0.1651  data: 0.0003  max mem: 4132
[20:37:28.060404] Epoch: [82]  [400/781]  eta: 0:01:03  lr: 0.000021  training_loss: 1.1968 (1.1693)  classification_loss: 1.1968 (1.1692)  loss_mask: 0.0000 (0.0000)  time: 0.1646  data: 0.0004  max mem: 4132
[20:37:31.331748] Epoch: [82]  [420/781]  eta: 0:00:59  lr: 0.000021  training_loss: 1.1637 (1.1697)  classification_loss: 1.1637 (1.1696)  loss_mask: 0.0000 (0.0000)  time: 0.1635  data: 0.0002  max mem: 4132
[20:37:34.592751] Epoch: [82]  [440/781]  eta: 0:00:56  lr: 0.000021  training_loss: 1.1676 (1.1693)  classification_loss: 1.1675 (1.1693)  loss_mask: 0.0000 (0.0000)  time: 0.1630  data: 0.0002  max mem: 4132
[20:37:37.844439] Epoch: [82]  [460/781]  eta: 0:00:53  lr: 0.000021  training_loss: 1.1223 (1.1682)  classification_loss: 1.1223 (1.1681)  loss_mask: 0.0000 (0.0000)  time: 0.1625  data: 0.0002  max mem: 4132
[20:37:41.134366] Epoch: [82]  [480/781]  eta: 0:00:49  lr: 0.000021  training_loss: 1.1907 (1.1698)  classification_loss: 1.1906 (1.1697)  loss_mask: 0.0000 (0.0000)  time: 0.1644  data: 0.0003  max mem: 4132
[20:37:44.459969] Epoch: [82]  [500/781]  eta: 0:00:46  lr: 0.000021  training_loss: 1.1886 (1.1695)  classification_loss: 1.1886 (1.1694)  loss_mask: 0.0000 (0.0000)  time: 0.1662  data: 0.0003  max mem: 4132
[20:37:47.770979] Epoch: [82]  [520/781]  eta: 0:00:43  lr: 0.000021  training_loss: 1.1849 (1.1702)  classification_loss: 1.1848 (1.1702)  loss_mask: 0.0000 (0.0000)  time: 0.1655  data: 0.0003  max mem: 4132
[20:37:51.057632] Epoch: [82]  [540/781]  eta: 0:00:39  lr: 0.000021  training_loss: 1.1691 (1.1703)  classification_loss: 1.1691 (1.1702)  loss_mask: 0.0000 (0.0000)  time: 0.1642  data: 0.0002  max mem: 4132
[20:37:54.350979] Epoch: [82]  [560/781]  eta: 0:00:36  lr: 0.000021  training_loss: 1.1598 (1.1693)  classification_loss: 1.1598 (1.1693)  loss_mask: 0.0000 (0.0000)  time: 0.1646  data: 0.0003  max mem: 4132
[20:37:57.681824] Epoch: [82]  [580/781]  eta: 0:00:33  lr: 0.000021  training_loss: 1.1663 (1.1694)  classification_loss: 1.1662 (1.1694)  loss_mask: 0.0000 (0.0000)  time: 0.1665  data: 0.0002  max mem: 4132
[20:38:00.979157] Epoch: [82]  [600/781]  eta: 0:00:29  lr: 0.000021  training_loss: 1.1695 (1.1692)  classification_loss: 1.1695 (1.1692)  loss_mask: 0.0000 (0.0000)  time: 0.1647  data: 0.0002  max mem: 4132
[20:38:04.294049] Epoch: [82]  [620/781]  eta: 0:00:26  lr: 0.000021  training_loss: 1.1348 (1.1687)  classification_loss: 1.1348 (1.1687)  loss_mask: 0.0000 (0.0000)  time: 0.1657  data: 0.0003  max mem: 4132
[20:38:07.633130] Epoch: [82]  [640/781]  eta: 0:00:23  lr: 0.000021  training_loss: 1.1466 (1.1679)  classification_loss: 1.1466 (1.1678)  loss_mask: 0.0000 (0.0000)  time: 0.1668  data: 0.0003  max mem: 4132
[20:38:10.937426] Epoch: [82]  [660/781]  eta: 0:00:20  lr: 0.000021  training_loss: 1.1465 (1.1678)  classification_loss: 1.1465 (1.1678)  loss_mask: 0.0000 (0.0000)  time: 0.1651  data: 0.0002  max mem: 4132
[20:38:14.210513] Epoch: [82]  [680/781]  eta: 0:00:16  lr: 0.000020  training_loss: 1.1994 (1.1684)  classification_loss: 1.1994 (1.1683)  loss_mask: 0.0000 (0.0000)  time: 0.1635  data: 0.0003  max mem: 4132
[20:38:17.486786] Epoch: [82]  [700/781]  eta: 0:00:13  lr: 0.000020  training_loss: 1.1739 (1.1691)  classification_loss: 1.1739 (1.1691)  loss_mask: 0.0000 (0.0000)  time: 0.1637  data: 0.0002  max mem: 4132
[20:38:20.755210] Epoch: [82]  [720/781]  eta: 0:00:10  lr: 0.000020  training_loss: 1.1695 (1.1693)  classification_loss: 1.1695 (1.1693)  loss_mask: 0.0000 (0.0000)  time: 0.1633  data: 0.0002  max mem: 4132
[20:38:24.059871] Epoch: [82]  [740/781]  eta: 0:00:06  lr: 0.000020  training_loss: 1.1863 (1.1695)  classification_loss: 1.1863 (1.1695)  loss_mask: 0.0000 (0.0000)  time: 0.1651  data: 0.0002  max mem: 4132
[20:38:27.351129] Epoch: [82]  [760/781]  eta: 0:00:03  lr: 0.000020  training_loss: 1.1864 (1.1706)  classification_loss: 1.1864 (1.1705)  loss_mask: 0.0000 (0.0000)  time: 0.1645  data: 0.0002  max mem: 4132
[20:38:30.616864] Epoch: [82]  [780/781]  eta: 0:00:00  lr: 0.000020  training_loss: 1.1660 (1.1711)  classification_loss: 1.1660 (1.1711)  loss_mask: 0.0000 (0.0000)  time: 0.1632  data: 0.0002  max mem: 4132
[20:38:30.785539] Epoch: [82] Total time: 0:02:09 (0.1657 s / it)
[20:38:30.785983] Averaged stats: lr: 0.000020  training_loss: 1.1660 (1.1711)  classification_loss: 1.1660 (1.1711)  loss_mask: 0.0000 (0.0000)
[20:38:31.500817] Test:  [  0/157]  eta: 0:01:51  testing_loss: 0.4826 (0.4826)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.7108  data: 0.6616  max mem: 4132
[20:38:31.800391] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.4725 (0.4928)  acc1: 84.3750 (84.6591)  acc5: 100.0000 (99.5739)  time: 0.0911  data: 0.0607  max mem: 4132
[20:38:32.093630] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4608 (0.4599)  acc1: 84.3750 (86.0863)  acc5: 100.0000 (99.5536)  time: 0.0292  data: 0.0004  max mem: 4132
[20:38:32.381413] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4445 (0.4765)  acc1: 87.5000 (85.8871)  acc5: 100.0000 (99.2440)  time: 0.0289  data: 0.0002  max mem: 4132
[20:38:32.671433] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4872 (0.4825)  acc1: 85.9375 (85.8994)  acc5: 98.4375 (99.1235)  time: 0.0287  data: 0.0002  max mem: 4132
[20:38:32.958266] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4872 (0.4737)  acc1: 85.9375 (86.3051)  acc5: 98.4375 (99.2034)  time: 0.0287  data: 0.0002  max mem: 4132
[20:38:33.244705] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4408 (0.4695)  acc1: 87.5000 (86.4242)  acc5: 100.0000 (99.2316)  time: 0.0285  data: 0.0002  max mem: 4132
[20:38:33.531431] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4504 (0.4663)  acc1: 87.5000 (86.4877)  acc5: 100.0000 (99.2738)  time: 0.0285  data: 0.0003  max mem: 4132
[20:38:33.820874] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4635 (0.4739)  acc1: 84.3750 (86.0340)  acc5: 98.4375 (99.1319)  time: 0.0287  data: 0.0002  max mem: 4132
[20:38:34.111453] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4993 (0.4737)  acc1: 84.3750 (86.0062)  acc5: 98.4375 (99.1415)  time: 0.0289  data: 0.0003  max mem: 4132
[20:38:34.401660] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.4915 (0.4757)  acc1: 84.3750 (85.9220)  acc5: 100.0000 (99.1646)  time: 0.0289  data: 0.0003  max mem: 4132
[20:38:34.688136] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4896 (0.4774)  acc1: 84.3750 (85.8812)  acc5: 100.0000 (99.1273)  time: 0.0287  data: 0.0002  max mem: 4132
[20:38:34.972853] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4805 (0.4783)  acc1: 85.9375 (85.8988)  acc5: 100.0000 (99.1348)  time: 0.0284  data: 0.0002  max mem: 4132
[20:38:35.262257] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4742 (0.4789)  acc1: 85.9375 (85.7824)  acc5: 100.0000 (99.1770)  time: 0.0286  data: 0.0002  max mem: 4132
[20:38:35.546373] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4470 (0.4758)  acc1: 85.9375 (85.9153)  acc5: 100.0000 (99.2021)  time: 0.0285  data: 0.0002  max mem: 4132
[20:38:35.830412] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4572 (0.4758)  acc1: 85.9375 (85.8754)  acc5: 100.0000 (99.1929)  time: 0.0283  data: 0.0002  max mem: 4132
[20:38:35.984153] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4593 (0.4762)  acc1: 85.9375 (85.8500)  acc5: 98.4375 (99.1800)  time: 0.0274  data: 0.0001  max mem: 4132
[20:38:36.162638] Test: Total time: 0:00:05 (0.0342 s / it)
[20:38:36.163115] * Acc@1 85.850 Acc@5 99.180 loss 0.476
[20:38:36.163413] Accuracy of the network on the 10000 test images: 85.8%
[20:38:36.163613] Max accuracy: 85.85%
[20:38:36.508271] log_dir: ./output_dir
[20:38:37.368306] Epoch: [83]  [  0/781]  eta: 0:11:10  lr: 0.000020  training_loss: 0.9170 (0.9170)  classification_loss: 0.9170 (0.9170)  loss_mask: 0.0000 (0.0000)  time: 0.8581  data: 0.6795  max mem: 4132
[20:38:40.644338] Epoch: [83]  [ 20/781]  eta: 0:02:29  lr: 0.000020  training_loss: 1.1502 (1.1537)  classification_loss: 1.1502 (1.1536)  loss_mask: 0.0000 (0.0000)  time: 0.1637  data: 0.0003  max mem: 4132
[20:38:43.908915] Epoch: [83]  [ 40/781]  eta: 0:02:13  lr: 0.000020  training_loss: 1.1400 (1.1618)  classification_loss: 1.1400 (1.1617)  loss_mask: 0.0000 (0.0000)  time: 0.1632  data: 0.0003  max mem: 4132
[20:38:47.208784] Epoch: [83]  [ 60/781]  eta: 0:02:06  lr: 0.000020  training_loss: 1.1629 (1.1560)  classification_loss: 1.1629 (1.1560)  loss_mask: 0.0000 (0.0000)  time: 0.1649  data: 0.0003  max mem: 4132
[20:38:50.467207] Epoch: [83]  [ 80/781]  eta: 0:02:00  lr: 0.000020  training_loss: 1.1581 (1.1586)  classification_loss: 1.1580 (1.1586)  loss_mask: 0.0000 (0.0000)  time: 0.1628  data: 0.0002  max mem: 4132
[20:38:53.735706] Epoch: [83]  [100/781]  eta: 0:01:56  lr: 0.000020  training_loss: 1.1936 (1.1625)  classification_loss: 1.1936 (1.1625)  loss_mask: 0.0000 (0.0000)  time: 0.1633  data: 0.0003  max mem: 4132
[20:38:56.995148] Epoch: [83]  [120/781]  eta: 0:01:51  lr: 0.000020  training_loss: 1.1112 (1.1581)  classification_loss: 1.1112 (1.1581)  loss_mask: 0.0000 (0.0000)  time: 0.1629  data: 0.0002  max mem: 4132
[20:39:00.302853] Epoch: [83]  [140/781]  eta: 0:01:48  lr: 0.000020  training_loss: 1.1332 (1.1557)  classification_loss: 1.1332 (1.1557)  loss_mask: 0.0000 (0.0000)  time: 0.1653  data: 0.0002  max mem: 4132
[20:39:03.580224] Epoch: [83]  [160/781]  eta: 0:01:44  lr: 0.000020  training_loss: 1.1326 (1.1539)  classification_loss: 1.1326 (1.1538)  loss_mask: 0.0000 (0.0000)  time: 0.1638  data: 0.0003  max mem: 4132
[20:39:06.859229] Epoch: [83]  [180/781]  eta: 0:01:40  lr: 0.000020  training_loss: 1.1535 (1.1545)  classification_loss: 1.1535 (1.1545)  loss_mask: 0.0000 (0.0000)  time: 0.1639  data: 0.0003  max mem: 4132
[20:39:10.153400] Epoch: [83]  [200/781]  eta: 0:01:37  lr: 0.000020  training_loss: 1.1618 (1.1577)  classification_loss: 1.1617 (1.1576)  loss_mask: 0.0000 (0.0000)  time: 0.1646  data: 0.0003  max mem: 4132
[20:39:13.447209] Epoch: [83]  [220/781]  eta: 0:01:33  lr: 0.000020  training_loss: 1.1716 (1.1593)  classification_loss: 1.1716 (1.1593)  loss_mask: 0.0000 (0.0000)  time: 0.1646  data: 0.0003  max mem: 4132
[20:39:16.753455] Epoch: [83]  [240/781]  eta: 0:01:30  lr: 0.000019  training_loss: 1.1476 (1.1598)  classification_loss: 1.1476 (1.1597)  loss_mask: 0.0000 (0.0000)  time: 0.1652  data: 0.0002  max mem: 4132
[20:39:20.092507] Epoch: [83]  [260/781]  eta: 0:01:26  lr: 0.000019  training_loss: 1.1977 (1.1608)  classification_loss: 1.1976 (1.1607)  loss_mask: 0.0000 (0.0000)  time: 0.1669  data: 0.0003  max mem: 4132
[20:39:23.373527] Epoch: [83]  [280/781]  eta: 0:01:23  lr: 0.000019  training_loss: 1.1587 (1.1598)  classification_loss: 1.1586 (1.1597)  loss_mask: 0.0000 (0.0000)  time: 0.1640  data: 0.0003  max mem: 4132
[20:39:26.696150] Epoch: [83]  [300/781]  eta: 0:01:20  lr: 0.000019  training_loss: 1.1782 (1.1613)  classification_loss: 1.1781 (1.1613)  loss_mask: 0.0000 (0.0000)  time: 0.1660  data: 0.0003  max mem: 4132
[20:39:30.006435] Epoch: [83]  [320/781]  eta: 0:01:16  lr: 0.000019  training_loss: 1.1689 (1.1610)  classification_loss: 1.1689 (1.1610)  loss_mask: 0.0000 (0.0000)  time: 0.1654  data: 0.0003  max mem: 4132
[20:39:33.304118] Epoch: [83]  [340/781]  eta: 0:01:13  lr: 0.000019  training_loss: 1.1328 (1.1596)  classification_loss: 1.1328 (1.1596)  loss_mask: 0.0000 (0.0000)  time: 0.1648  data: 0.0003  max mem: 4132
[20:39:36.605670] Epoch: [83]  [360/781]  eta: 0:01:10  lr: 0.000019  training_loss: 1.1683 (1.1604)  classification_loss: 1.1682 (1.1603)  loss_mask: 0.0000 (0.0000)  time: 0.1650  data: 0.0003  max mem: 4132
[20:39:39.882969] Epoch: [83]  [380/781]  eta: 0:01:06  lr: 0.000019  training_loss: 1.1483 (1.1611)  classification_loss: 1.1483 (1.1610)  loss_mask: 0.0000 (0.0000)  time: 0.1638  data: 0.0002  max mem: 4132
[20:39:43.174451] Epoch: [83]  [400/781]  eta: 0:01:03  lr: 0.000019  training_loss: 1.1058 (1.1604)  classification_loss: 1.1058 (1.1604)  loss_mask: 0.0000 (0.0000)  time: 0.1645  data: 0.0003  max mem: 4132
[20:39:46.436380] Epoch: [83]  [420/781]  eta: 0:00:59  lr: 0.000019  training_loss: 1.1862 (1.1614)  classification_loss: 1.1862 (1.1614)  loss_mask: 0.0000 (0.0000)  time: 0.1630  data: 0.0003  max mem: 4132
[20:39:49.698518] Epoch: [83]  [440/781]  eta: 0:00:56  lr: 0.000019  training_loss: 1.1608 (1.1609)  classification_loss: 1.1608 (1.1609)  loss_mask: 0.0000 (0.0000)  time: 0.1630  data: 0.0002  max mem: 4132
[20:39:52.975158] Epoch: [83]  [460/781]  eta: 0:00:53  lr: 0.000019  training_loss: 1.1309 (1.1605)  classification_loss: 1.1308 (1.1605)  loss_mask: 0.0000 (0.0000)  time: 0.1637  data: 0.0003  max mem: 4132
[20:39:56.291214] Epoch: [83]  [480/781]  eta: 0:00:49  lr: 0.000019  training_loss: 1.1535 (1.1613)  classification_loss: 1.1535 (1.1613)  loss_mask: 0.0000 (0.0000)  time: 0.1657  data: 0.0003  max mem: 4132
[20:39:59.584008] Epoch: [83]  [500/781]  eta: 0:00:46  lr: 0.000019  training_loss: 1.1448 (1.1620)  classification_loss: 1.1448 (1.1620)  loss_mask: 0.0000 (0.0000)  time: 0.1646  data: 0.0003  max mem: 4132
[20:40:02.848018] Epoch: [83]  [520/781]  eta: 0:00:43  lr: 0.000019  training_loss: 1.1372 (1.1619)  classification_loss: 1.1372 (1.1618)  loss_mask: 0.0000 (0.0000)  time: 0.1631  data: 0.0002  max mem: 4132
[20:40:06.132914] Epoch: [83]  [540/781]  eta: 0:00:39  lr: 0.000019  training_loss: 1.1617 (1.1619)  classification_loss: 1.1617 (1.1619)  loss_mask: 0.0000 (0.0000)  time: 0.1641  data: 0.0002  max mem: 4132
[20:40:09.401068] Epoch: [83]  [560/781]  eta: 0:00:36  lr: 0.000019  training_loss: 1.0851 (1.1613)  classification_loss: 1.0851 (1.1612)  loss_mask: 0.0000 (0.0000)  time: 0.1633  data: 0.0002  max mem: 4132
[20:40:12.686837] Epoch: [83]  [580/781]  eta: 0:00:33  lr: 0.000019  training_loss: 1.1494 (1.1611)  classification_loss: 1.1494 (1.1611)  loss_mask: 0.0000 (0.0000)  time: 0.1642  data: 0.0003  max mem: 4132
[20:40:15.981822] Epoch: [83]  [600/781]  eta: 0:00:29  lr: 0.000019  training_loss: 1.1110 (1.1607)  classification_loss: 1.1109 (1.1606)  loss_mask: 0.0000 (0.0000)  time: 0.1647  data: 0.0003  max mem: 4132
[20:40:19.251211] Epoch: [83]  [620/781]  eta: 0:00:26  lr: 0.000018  training_loss: 1.1547 (1.1607)  classification_loss: 1.1547 (1.1607)  loss_mask: 0.0000 (0.0000)  time: 0.1634  data: 0.0002  max mem: 4132
[20:40:22.535644] Epoch: [83]  [640/781]  eta: 0:00:23  lr: 0.000018  training_loss: 1.1716 (1.1604)  classification_loss: 1.1716 (1.1603)  loss_mask: 0.0000 (0.0000)  time: 0.1641  data: 0.0004  max mem: 4132
[20:40:25.828870] Epoch: [83]  [660/781]  eta: 0:00:20  lr: 0.000018  training_loss: 1.1189 (1.1599)  classification_loss: 1.1188 (1.1599)  loss_mask: 0.0000 (0.0000)  time: 0.1646  data: 0.0003  max mem: 4132
[20:40:29.136012] Epoch: [83]  [680/781]  eta: 0:00:16  lr: 0.000018  training_loss: 1.1886 (1.1610)  classification_loss: 1.1886 (1.1610)  loss_mask: 0.0000 (0.0000)  time: 0.1653  data: 0.0002  max mem: 4132
[20:40:32.446869] Epoch: [83]  [700/781]  eta: 0:00:13  lr: 0.000018  training_loss: 1.1998 (1.1621)  classification_loss: 1.1998 (1.1621)  loss_mask: 0.0000 (0.0000)  time: 0.1655  data: 0.0003  max mem: 4132
[20:40:35.722736] Epoch: [83]  [720/781]  eta: 0:00:10  lr: 0.000018  training_loss: 1.1982 (1.1633)  classification_loss: 1.1982 (1.1633)  loss_mask: 0.0000 (0.0000)  time: 0.1637  data: 0.0003  max mem: 4132
[20:40:39.008212] Epoch: [83]  [740/781]  eta: 0:00:06  lr: 0.000018  training_loss: 1.1391 (1.1633)  classification_loss: 1.1391 (1.1633)  loss_mask: 0.0000 (0.0000)  time: 0.1642  data: 0.0003  max mem: 4132
[20:40:42.301915] Epoch: [83]  [760/781]  eta: 0:00:03  lr: 0.000018  training_loss: 1.1523 (1.1636)  classification_loss: 1.1523 (1.1636)  loss_mask: 0.0000 (0.0000)  time: 0.1646  data: 0.0002  max mem: 4132
[20:40:45.636534] Epoch: [83]  [780/781]  eta: 0:00:00  lr: 0.000018  training_loss: 1.2042 (1.1649)  classification_loss: 1.2042 (1.1648)  loss_mask: 0.0000 (0.0000)  time: 0.1666  data: 0.0003  max mem: 4132
[20:40:45.797961] Epoch: [83] Total time: 0:02:09 (0.1655 s / it)
[20:40:45.799949] Averaged stats: lr: 0.000018  training_loss: 1.2042 (1.1649)  classification_loss: 1.2042 (1.1648)  loss_mask: 0.0000 (0.0000)
[20:40:46.532406] Test:  [  0/157]  eta: 0:01:54  testing_loss: 0.5067 (0.5067)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.7280  data: 0.6891  max mem: 4132
[20:40:46.837020] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.4961 (0.5000)  acc1: 85.9375 (85.2273)  acc5: 100.0000 (99.5739)  time: 0.0937  data: 0.0633  max mem: 4132
[20:40:47.126639] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4628 (0.4594)  acc1: 87.5000 (86.4583)  acc5: 100.0000 (99.5536)  time: 0.0295  data: 0.0004  max mem: 4132
[20:40:47.419561] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4391 (0.4802)  acc1: 87.5000 (85.8871)  acc5: 100.0000 (99.2944)  time: 0.0289  data: 0.0004  max mem: 4132
[20:40:47.714071] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4854 (0.4834)  acc1: 85.9375 (85.8613)  acc5: 98.4375 (99.1616)  time: 0.0292  data: 0.0005  max mem: 4132
[20:40:48.004226] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4691 (0.4775)  acc1: 84.3750 (86.1213)  acc5: 98.4375 (99.1728)  time: 0.0291  data: 0.0003  max mem: 4132
[20:40:48.300805] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4065 (0.4705)  acc1: 85.9375 (86.4242)  acc5: 100.0000 (99.2059)  time: 0.0292  data: 0.0002  max mem: 4132
[20:40:48.593355] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4136 (0.4666)  acc1: 87.5000 (86.7298)  acc5: 100.0000 (99.2298)  time: 0.0293  data: 0.0002  max mem: 4132
[20:40:48.880240] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4703 (0.4749)  acc1: 85.9375 (86.4005)  acc5: 98.4375 (99.1319)  time: 0.0288  data: 0.0002  max mem: 4132
[20:40:49.172716] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4898 (0.4728)  acc1: 84.3750 (86.3496)  acc5: 98.4375 (99.1243)  time: 0.0288  data: 0.0002  max mem: 4132
[20:40:49.459550] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.4898 (0.4754)  acc1: 85.9375 (86.2933)  acc5: 100.0000 (99.1646)  time: 0.0288  data: 0.0002  max mem: 4132
[20:40:49.750893] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5040 (0.4770)  acc1: 84.3750 (86.2050)  acc5: 100.0000 (99.1413)  time: 0.0287  data: 0.0002  max mem: 4132
[20:40:50.040053] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5025 (0.4776)  acc1: 85.9375 (86.2216)  acc5: 100.0000 (99.1477)  time: 0.0288  data: 0.0003  max mem: 4132
[20:40:50.328715] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4791 (0.4778)  acc1: 85.9375 (86.1045)  acc5: 100.0000 (99.1770)  time: 0.0287  data: 0.0003  max mem: 4132
[20:40:50.615237] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4447 (0.4741)  acc1: 85.9375 (86.2921)  acc5: 100.0000 (99.2021)  time: 0.0286  data: 0.0002  max mem: 4132
[20:40:50.899024] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4447 (0.4739)  acc1: 87.5000 (86.2065)  acc5: 100.0000 (99.2136)  time: 0.0283  data: 0.0002  max mem: 4132
[20:40:51.052485] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4447 (0.4742)  acc1: 85.9375 (86.1500)  acc5: 100.0000 (99.2100)  time: 0.0274  data: 0.0002  max mem: 4132
[20:40:51.243204] Test: Total time: 0:00:05 (0.0346 s / it)
[20:40:51.243690] * Acc@1 86.150 Acc@5 99.210 loss 0.474
[20:40:51.244001] Accuracy of the network on the 10000 test images: 86.2%
[20:40:51.244241] Max accuracy: 86.15%
[20:40:51.577631] log_dir: ./output_dir
[20:40:52.487653] Epoch: [84]  [  0/781]  eta: 0:11:49  lr: 0.000018  training_loss: 0.9730 (0.9730)  classification_loss: 0.9729 (0.9729)  loss_mask: 0.0000 (0.0000)  time: 0.9079  data: 0.7307  max mem: 4132
[20:40:55.798421] Epoch: [84]  [ 20/781]  eta: 0:02:32  lr: 0.000018  training_loss: 1.1036 (1.0887)  classification_loss: 1.1036 (1.0887)  loss_mask: 0.0000 (0.0000)  time: 0.1654  data: 0.0002  max mem: 4132
[20:40:59.083405] Epoch: [84]  [ 40/781]  eta: 0:02:15  lr: 0.000018  training_loss: 1.1560 (1.1261)  classification_loss: 1.1560 (1.1261)  loss_mask: 0.0000 (0.0000)  time: 0.1642  data: 0.0003  max mem: 4132
[20:41:02.414425] Epoch: [84]  [ 60/781]  eta: 0:02:07  lr: 0.000018  training_loss: 1.1988 (1.1392)  classification_loss: 1.1988 (1.1392)  loss_mask: 0.0000 (0.0000)  time: 0.1665  data: 0.0003  max mem: 4132
[20:41:05.695151] Epoch: [84]  [ 80/781]  eta: 0:02:02  lr: 0.000018  training_loss: 1.1471 (1.1442)  classification_loss: 1.1470 (1.1442)  loss_mask: 0.0000 (0.0000)  time: 0.1640  data: 0.0002  max mem: 4132
[20:41:08.982894] Epoch: [84]  [100/781]  eta: 0:01:57  lr: 0.000018  training_loss: 1.1570 (1.1511)  classification_loss: 1.1570 (1.1511)  loss_mask: 0.0000 (0.0000)  time: 0.1643  data: 0.0003  max mem: 4132
[20:41:12.234092] Epoch: [84]  [120/781]  eta: 0:01:52  lr: 0.000018  training_loss: 1.1852 (1.1557)  classification_loss: 1.1852 (1.1556)  loss_mask: 0.0000 (0.0000)  time: 0.1625  data: 0.0002  max mem: 4132
[20:41:15.494444] Epoch: [84]  [140/781]  eta: 0:01:48  lr: 0.000018  training_loss: 1.1622 (1.1545)  classification_loss: 1.1622 (1.1545)  loss_mask: 0.0000 (0.0000)  time: 0.1629  data: 0.0002  max mem: 4132
[20:41:18.750959] Epoch: [84]  [160/781]  eta: 0:01:44  lr: 0.000018  training_loss: 1.1518 (1.1563)  classification_loss: 1.1518 (1.1563)  loss_mask: 0.0000 (0.0000)  time: 0.1627  data: 0.0002  max mem: 4132
[20:41:22.067193] Epoch: [84]  [180/781]  eta: 0:01:41  lr: 0.000018  training_loss: 1.2105 (1.1609)  classification_loss: 1.2105 (1.1609)  loss_mask: 0.0000 (0.0000)  time: 0.1657  data: 0.0003  max mem: 4132
[20:41:25.338335] Epoch: [84]  [200/781]  eta: 0:01:37  lr: 0.000017  training_loss: 1.1338 (1.1604)  classification_loss: 1.1338 (1.1604)  loss_mask: 0.0000 (0.0000)  time: 0.1635  data: 0.0002  max mem: 4132
[20:41:28.623895] Epoch: [84]  [220/781]  eta: 0:01:33  lr: 0.000017  training_loss: 1.1588 (1.1595)  classification_loss: 1.1588 (1.1595)  loss_mask: 0.0000 (0.0000)  time: 0.1642  data: 0.0003  max mem: 4132
[20:41:31.903632] Epoch: [84]  [240/781]  eta: 0:01:30  lr: 0.000017  training_loss: 1.1575 (1.1594)  classification_loss: 1.1575 (1.1593)  loss_mask: 0.0000 (0.0000)  time: 0.1639  data: 0.0003  max mem: 4132
[20:41:35.183376] Epoch: [84]  [260/781]  eta: 0:01:26  lr: 0.000017  training_loss: 1.1319 (1.1583)  classification_loss: 1.1319 (1.1583)  loss_mask: 0.0000 (0.0000)  time: 0.1639  data: 0.0003  max mem: 4132
[20:41:38.499311] Epoch: [84]  [280/781]  eta: 0:01:23  lr: 0.000017  training_loss: 1.1557 (1.1588)  classification_loss: 1.1556 (1.1588)  loss_mask: 0.0000 (0.0000)  time: 0.1657  data: 0.0003  max mem: 4132
[20:41:41.783356] Epoch: [84]  [300/781]  eta: 0:01:20  lr: 0.000017  training_loss: 1.1195 (1.1580)  classification_loss: 1.1195 (1.1580)  loss_mask: 0.0000 (0.0000)  time: 0.1641  data: 0.0002  max mem: 4132
[20:41:45.052658] Epoch: [84]  [320/781]  eta: 0:01:16  lr: 0.000017  training_loss: 1.1452 (1.1586)  classification_loss: 1.1451 (1.1586)  loss_mask: 0.0000 (0.0000)  time: 0.1634  data: 0.0003  max mem: 4132
[20:41:48.356891] Epoch: [84]  [340/781]  eta: 0:01:13  lr: 0.000017  training_loss: 1.1342 (1.1574)  classification_loss: 1.1342 (1.1574)  loss_mask: 0.0000 (0.0000)  time: 0.1651  data: 0.0003  max mem: 4132
[20:41:51.626928] Epoch: [84]  [360/781]  eta: 0:01:09  lr: 0.000017  training_loss: 1.1967 (1.1583)  classification_loss: 1.1967 (1.1583)  loss_mask: 0.0000 (0.0000)  time: 0.1634  data: 0.0003  max mem: 4132
[20:41:54.949037] Epoch: [84]  [380/781]  eta: 0:01:06  lr: 0.000017  training_loss: 1.1830 (1.1591)  classification_loss: 1.1829 (1.1590)  loss_mask: 0.0000 (0.0000)  time: 0.1660  data: 0.0003  max mem: 4132
[20:41:58.247662] Epoch: [84]  [400/781]  eta: 0:01:03  lr: 0.000017  training_loss: 1.1277 (1.1580)  classification_loss: 1.1277 (1.1580)  loss_mask: 0.0000 (0.0000)  time: 0.1648  data: 0.0003  max mem: 4132
[20:42:01.560517] Epoch: [84]  [420/781]  eta: 0:00:59  lr: 0.000017  training_loss: 1.1198 (1.1578)  classification_loss: 1.1197 (1.1577)  loss_mask: 0.0000 (0.0000)  time: 0.1655  data: 0.0002  max mem: 4132
[20:42:04.833006] Epoch: [84]  [440/781]  eta: 0:00:56  lr: 0.000017  training_loss: 1.0887 (1.1554)  classification_loss: 1.0887 (1.1553)  loss_mask: 0.0000 (0.0000)  time: 0.1635  data: 0.0003  max mem: 4132
[20:42:08.135546] Epoch: [84]  [460/781]  eta: 0:00:53  lr: 0.000017  training_loss: 1.1510 (1.1547)  classification_loss: 1.1510 (1.1547)  loss_mask: 0.0000 (0.0000)  time: 0.1650  data: 0.0002  max mem: 4132
[20:42:11.436579] Epoch: [84]  [480/781]  eta: 0:00:49  lr: 0.000017  training_loss: 1.1778 (1.1555)  classification_loss: 1.1777 (1.1555)  loss_mask: 0.0000 (0.0000)  time: 0.1649  data: 0.0002  max mem: 4132
[20:42:14.756652] Epoch: [84]  [500/781]  eta: 0:00:46  lr: 0.000017  training_loss: 1.1539 (1.1565)  classification_loss: 1.1539 (1.1565)  loss_mask: 0.0000 (0.0000)  time: 0.1659  data: 0.0003  max mem: 4132
[20:42:18.038697] Epoch: [84]  [520/781]  eta: 0:00:43  lr: 0.000017  training_loss: 1.1402 (1.1564)  classification_loss: 1.1402 (1.1563)  loss_mask: 0.0000 (0.0000)  time: 0.1640  data: 0.0003  max mem: 4132
[20:42:21.319153] Epoch: [84]  [540/781]  eta: 0:00:39  lr: 0.000017  training_loss: 1.1443 (1.1562)  classification_loss: 1.1443 (1.1562)  loss_mask: 0.0000 (0.0000)  time: 0.1639  data: 0.0002  max mem: 4132
[20:42:24.575107] Epoch: [84]  [560/781]  eta: 0:00:36  lr: 0.000017  training_loss: 1.1503 (1.1555)  classification_loss: 1.1503 (1.1555)  loss_mask: 0.0000 (0.0000)  time: 0.1627  data: 0.0002  max mem: 4132
[20:42:27.836883] Epoch: [84]  [580/781]  eta: 0:00:33  lr: 0.000017  training_loss: 1.1163 (1.1555)  classification_loss: 1.1163 (1.1555)  loss_mask: 0.0000 (0.0000)  time: 0.1630  data: 0.0002  max mem: 4132
[20:42:31.104327] Epoch: [84]  [600/781]  eta: 0:00:29  lr: 0.000016  training_loss: 1.0840 (1.1533)  classification_loss: 1.0840 (1.1533)  loss_mask: 0.0000 (0.0000)  time: 0.1633  data: 0.0003  max mem: 4132
[20:42:34.407429] Epoch: [84]  [620/781]  eta: 0:00:26  lr: 0.000016  training_loss: 1.1043 (1.1520)  classification_loss: 1.1043 (1.1519)  loss_mask: 0.0000 (0.0000)  time: 0.1651  data: 0.0003  max mem: 4132
[20:42:37.686266] Epoch: [84]  [640/781]  eta: 0:00:23  lr: 0.000016  training_loss: 1.1446 (1.1529)  classification_loss: 1.1445 (1.1528)  loss_mask: 0.0000 (0.0000)  time: 0.1639  data: 0.0002  max mem: 4132
[20:42:40.988792] Epoch: [84]  [660/781]  eta: 0:00:20  lr: 0.000016  training_loss: 1.1651 (1.1536)  classification_loss: 1.1651 (1.1536)  loss_mask: 0.0000 (0.0000)  time: 0.1650  data: 0.0002  max mem: 4132
[20:42:44.263223] Epoch: [84]  [680/781]  eta: 0:00:16  lr: 0.000016  training_loss: 1.1483 (1.1542)  classification_loss: 1.1482 (1.1542)  loss_mask: 0.0000 (0.0000)  time: 0.1636  data: 0.0002  max mem: 4132

[20:42:47.535654] Epoch: [84]  [700/781]  eta: 0:00:13  lr: 0.000016  training_loss: 1.1143 (1.1535)  classification_loss: 1.1143 (1.1534)  loss_mask: 0.0000 (0.0000)  time: 0.1635  data: 0.0002  max mem: 4132
[20:42:50.813023] Epoch: [84]  [720/781]  eta: 0:00:10  lr: 0.000016  training_loss: 1.1904 (1.1552)  classification_loss: 1.1904 (1.1551)  loss_mask: 0.0000 (0.0000)  time: 0.1638  data: 0.0002  max mem: 4132
[20:42:54.093459] Epoch: [84]  [740/781]  eta: 0:00:06  lr: 0.000016  training_loss: 1.1290 (1.1547)  classification_loss: 1.1290 (1.1547)  loss_mask: 0.0000 (0.0000)  time: 0.1639  data: 0.0002  max mem: 4132
[20:42:57.386417] Epoch: [84]  [760/781]  eta: 0:00:03  lr: 0.000016  training_loss: 1.1160 (1.1544)  classification_loss: 1.1160 (1.1543)  loss_mask: 0.0000 (0.0000)  time: 0.1646  data: 0.0002  max mem: 4132
[20:43:00.658085] Epoch: [84]  [780/781]  eta: 0:00:00  lr: 0.000016  training_loss: 1.2027 (1.1554)  classification_loss: 1.2026 (1.1554)  loss_mask: 0.0000 (0.0000)  time: 0.1635  data: 0.0002  max mem: 4132
[20:43:00.855670] Epoch: [84] Total time: 0:02:09 (0.1655 s / it)
[20:43:00.856830] Averaged stats: lr: 0.000016  training_loss: 1.2027 (1.1554)  classification_loss: 1.2026 (1.1554)  loss_mask: 0.0000 (0.0000)
[20:43:01.563215] Test:  [  0/157]  eta: 0:01:50  testing_loss: 0.5079 (0.5079)  acc1: 84.3750 (84.3750)  acc5: 98.4375 (98.4375)  time: 0.7021  data: 0.6637  max mem: 4132
[20:43:01.857785] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.5079 (0.5024)  acc1: 85.9375 (84.6591)  acc5: 100.0000 (99.5739)  time: 0.0903  data: 0.0605  max mem: 4132
[20:43:02.149192] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4475 (0.4595)  acc1: 87.5000 (86.4583)  acc5: 100.0000 (99.5536)  time: 0.0290  data: 0.0002  max mem: 4132
[20:43:02.444552] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4339 (0.4767)  acc1: 87.5000 (85.6855)  acc5: 100.0000 (99.2944)  time: 0.0292  data: 0.0002  max mem: 4132
[20:43:02.729230] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5049 (0.4832)  acc1: 85.9375 (85.7088)  acc5: 98.4375 (99.1997)  time: 0.0288  data: 0.0002  max mem: 4132
[20:43:03.017445] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4797 (0.4774)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (99.1728)  time: 0.0285  data: 0.0002  max mem: 4132
[20:43:03.302295] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4282 (0.4717)  acc1: 85.9375 (86.1680)  acc5: 100.0000 (99.1803)  time: 0.0285  data: 0.0002  max mem: 4132
[20:43:03.591380] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4389 (0.4684)  acc1: 85.9375 (86.2676)  acc5: 100.0000 (99.1857)  time: 0.0284  data: 0.0002  max mem: 4132
[20:43:03.885296] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4792 (0.4768)  acc1: 85.9375 (85.9761)  acc5: 98.4375 (99.1319)  time: 0.0289  data: 0.0003  max mem: 4132
[20:43:04.176733] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4797 (0.4741)  acc1: 84.3750 (85.9718)  acc5: 98.4375 (99.1415)  time: 0.0291  data: 0.0005  max mem: 4132
[20:43:04.466427] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.4797 (0.4764)  acc1: 85.9375 (85.7983)  acc5: 100.0000 (99.1491)  time: 0.0289  data: 0.0005  max mem: 4132
[20:43:04.757160] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4881 (0.4779)  acc1: 85.9375 (85.7827)  acc5: 100.0000 (99.1413)  time: 0.0288  data: 0.0003  max mem: 4132
[20:43:05.042202] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4803 (0.4773)  acc1: 85.9375 (85.7955)  acc5: 100.0000 (99.1477)  time: 0.0285  data: 0.0003  max mem: 4132
[20:43:05.327955] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4803 (0.4771)  acc1: 85.9375 (85.7586)  acc5: 100.0000 (99.1531)  time: 0.0283  data: 0.0002  max mem: 4132
[20:43:05.609745] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4577 (0.4743)  acc1: 84.3750 (85.8378)  acc5: 100.0000 (99.1800)  time: 0.0282  data: 0.0002  max mem: 4132
[20:43:05.893384] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4586 (0.4751)  acc1: 84.3750 (85.7512)  acc5: 100.0000 (99.1929)  time: 0.0281  data: 0.0001  max mem: 4132
[20:43:06.049411] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4577 (0.4757)  acc1: 84.3750 (85.7200)  acc5: 100.0000 (99.1700)  time: 0.0274  data: 0.0001  max mem: 4132
[20:43:06.248089] Test: Total time: 0:00:05 (0.0343 s / it)
[20:43:06.249000] * Acc@1 85.720 Acc@5 99.170 loss 0.476
[20:43:06.249461] Accuracy of the network on the 10000 test images: 85.7%
[20:43:06.250105] Max accuracy: 86.15%
[20:43:06.642052] log_dir: ./output_dir
[20:43:07.529009] Epoch: [85]  [  0/781]  eta: 0:11:30  lr: 0.000016  training_loss: 1.0259 (1.0259)  classification_loss: 1.0257 (1.0257)  loss_mask: 0.0002 (0.0002)  time: 0.8847  data: 0.6936  max mem: 4132
[20:43:10.837129] Epoch: [85]  [ 20/781]  eta: 0:02:31  lr: 0.000016  training_loss: 1.1442 (1.1590)  classification_loss: 1.1442 (1.1590)  loss_mask: 0.0000 (0.0000)  time: 0.1653  data: 0.0002  max mem: 4132
[20:43:14.148415] Epoch: [85]  [ 40/781]  eta: 0:02:15  lr: 0.000016  training_loss: 1.1630 (1.1558)  classification_loss: 1.1629 (1.1556)  loss_mask: 0.0000 (0.0002)  time: 0.1655  data: 0.0003  max mem: 4132
[20:43:17.420751] Epoch: [85]  [ 60/781]  eta: 0:02:07  lr: 0.000016  training_loss: 1.1063 (1.1552)  classification_loss: 1.1063 (1.1551)  loss_mask: 0.0000 (0.0001)  time: 0.1634  data: 0.0003  max mem: 4132
[20:43:20.680335] Epoch: [85]  [ 80/781]  eta: 0:02:01  lr: 0.000016  training_loss: 1.1462 (1.1481)  classification_loss: 1.1462 (1.1480)  loss_mask: 0.0000 (0.0001)  time: 0.1629  data: 0.0003  max mem: 4132
[20:43:23.952043] Epoch: [85]  [100/781]  eta: 0:01:56  lr: 0.000016  training_loss: 1.1844 (1.1560)  classification_loss: 1.1843 (1.1559)  loss_mask: 0.0000 (0.0001)  time: 0.1635  data: 0.0002  max mem: 4132
[20:43:27.222647] Epoch: [85]  [120/781]  eta: 0:01:52  lr: 0.000016  training_loss: 1.1179 (1.1514)  classification_loss: 1.1179 (1.1513)  loss_mask: 0.0000 (0.0001)  time: 0.1634  data: 0.0002  max mem: 4132
[20:43:30.494888] Epoch: [85]  [140/781]  eta: 0:01:48  lr: 0.000016  training_loss: 1.1044 (1.1481)  classification_loss: 1.1044 (1.1480)  loss_mask: 0.0000 (0.0001)  time: 0.1634  data: 0.0003  max mem: 4132
[20:43:33.812054] Epoch: [85]  [160/781]  eta: 0:01:44  lr: 0.000016  training_loss: 1.1520 (1.1481)  classification_loss: 1.1520 (1.1480)  loss_mask: 0.0000 (0.0001)  time: 0.1657  data: 0.0002  max mem: 4132
[20:43:37.176837] Epoch: [85]  [180/781]  eta: 0:01:41  lr: 0.000016  training_loss: 1.1212 (1.1467)  classification_loss: 1.1212 (1.1466)  loss_mask: 0.0000 (0.0001)  time: 0.1682  data: 0.0004  max mem: 4132
[20:43:40.483215] Epoch: [85]  [200/781]  eta: 0:01:37  lr: 0.000016  training_loss: 1.1446 (1.1465)  classification_loss: 1.1446 (1.1465)  loss_mask: 0.0000 (0.0001)  time: 0.1652  data: 0.0003  max mem: 4132
[20:43:43.754886] Epoch: [85]  [220/781]  eta: 0:01:34  lr: 0.000015  training_loss: 1.1266 (1.1445)  classification_loss: 1.1266 (1.1444)  loss_mask: 0.0000 (0.0001)  time: 0.1635  data: 0.0002  max mem: 4132
[20:43:47.029914] Epoch: [85]  [240/781]  eta: 0:01:30  lr: 0.000015  training_loss: 1.1515 (1.1470)  classification_loss: 1.1514 (1.1469)  loss_mask: 0.0000 (0.0001)  time: 0.1637  data: 0.0002  max mem: 4132
[20:43:50.314747] Epoch: [85]  [260/781]  eta: 0:01:27  lr: 0.000015  training_loss: 1.1605 (1.1491)  classification_loss: 1.1605 (1.1490)  loss_mask: 0.0000 (0.0001)  time: 0.1641  data: 0.0003  max mem: 4132
[20:43:53.625445] Epoch: [85]  [280/781]  eta: 0:01:23  lr: 0.000015  training_loss: 1.1629 (1.1507)  classification_loss: 1.1628 (1.1507)  loss_mask: 0.0000 (0.0001)  time: 0.1654  data: 0.0002  max mem: 4132
[20:43:56.937318] Epoch: [85]  [300/781]  eta: 0:01:20  lr: 0.000015  training_loss: 1.1650 (1.1518)  classification_loss: 1.1649 (1.1517)  loss_mask: 0.0000 (0.0000)  time: 0.1655  data: 0.0003  max mem: 4132
[20:44:00.267952] Epoch: [85]  [320/781]  eta: 0:01:16  lr: 0.000015  training_loss: 1.0877 (1.1480)  classification_loss: 1.0876 (1.1479)  loss_mask: 0.0000 (0.0000)  time: 0.1664  data: 0.0002  max mem: 4132
[20:44:03.538608] Epoch: [85]  [340/781]  eta: 0:01:13  lr: 0.000015  training_loss: 1.1053 (1.1464)  classification_loss: 1.1052 (1.1464)  loss_mask: 0.0000 (0.0000)  time: 0.1634  data: 0.0002  max mem: 4132
[20:44:06.829188] Epoch: [85]  [360/781]  eta: 0:01:10  lr: 0.000015  training_loss: 1.1611 (1.1473)  classification_loss: 1.1611 (1.1473)  loss_mask: 0.0000 (0.0000)  time: 0.1644  data: 0.0003  max mem: 4132
[20:44:10.103453] Epoch: [85]  [380/781]  eta: 0:01:06  lr: 0.000015  training_loss: 1.1695 (1.1490)  classification_loss: 1.1695 (1.1489)  loss_mask: 0.0000 (0.0000)  time: 0.1636  data: 0.0003  max mem: 4132
[20:44:13.385772] Epoch: [85]  [400/781]  eta: 0:01:03  lr: 0.000015  training_loss: 1.1759 (1.1506)  classification_loss: 1.1759 (1.1506)  loss_mask: 0.0000 (0.0000)  time: 0.1640  data: 0.0003  max mem: 4132
[20:44:16.652545] Epoch: [85]  [420/781]  eta: 0:00:59  lr: 0.000015  training_loss: 1.1825 (1.1519)  classification_loss: 1.1825 (1.1518)  loss_mask: 0.0000 (0.0000)  time: 0.1633  data: 0.0003  max mem: 4132
[20:44:19.910022] Epoch: [85]  [440/781]  eta: 0:00:56  lr: 0.000015  training_loss: 1.1370 (1.1522)  classification_loss: 1.1370 (1.1522)  loss_mask: 0.0000 (0.0000)  time: 0.1627  data: 0.0002  max mem: 4132
[20:44:23.172353] Epoch: [85]  [460/781]  eta: 0:00:53  lr: 0.000015  training_loss: 1.1241 (1.1521)  classification_loss: 1.1241 (1.1521)  loss_mask: 0.0000 (0.0000)  time: 0.1630  data: 0.0002  max mem: 4132
[20:44:26.424500] Epoch: [85]  [480/781]  eta: 0:00:49  lr: 0.000015  training_loss: 1.1738 (1.1530)  classification_loss: 1.1738 (1.1530)  loss_mask: 0.0000 (0.0000)  time: 0.1625  data: 0.0002  max mem: 4132
[20:44:29.698558] Epoch: [85]  [500/781]  eta: 0:00:46  lr: 0.000015  training_loss: 1.1704 (1.1537)  classification_loss: 1.1703 (1.1536)  loss_mask: 0.0000 (0.0000)  time: 0.1636  data: 0.0002  max mem: 4132
[20:44:32.980225] Epoch: [85]  [520/781]  eta: 0:00:43  lr: 0.000015  training_loss: 1.1470 (1.1539)  classification_loss: 1.1470 (1.1538)  loss_mask: 0.0000 (0.0000)  time: 0.1640  data: 0.0002  max mem: 4132
[20:44:36.236329] Epoch: [85]  [540/781]  eta: 0:00:39  lr: 0.000015  training_loss: 1.1881 (1.1547)  classification_loss: 1.1881 (1.1547)  loss_mask: 0.0000 (0.0000)  time: 0.1627  data: 0.0002  max mem: 4132
[20:44:39.526303] Epoch: [85]  [560/781]  eta: 0:00:36  lr: 0.000015  training_loss: 1.1401 (1.1533)  classification_loss: 1.1401 (1.1533)  loss_mask: 0.0000 (0.0000)  time: 0.1644  data: 0.0002  max mem: 4132
[20:44:42.844968] Epoch: [85]  [580/781]  eta: 0:00:33  lr: 0.000015  training_loss: 1.1525 (1.1540)  classification_loss: 1.1525 (1.1540)  loss_mask: 0.0000 (0.0000)  time: 0.1659  data: 0.0002  max mem: 4132
[20:44:46.146414] Epoch: [85]  [600/781]  eta: 0:00:29  lr: 0.000015  training_loss: 1.1194 (1.1537)  classification_loss: 1.1194 (1.1536)  loss_mask: 0.0000 (0.0000)  time: 0.1650  data: 0.0002  max mem: 4132
[20:44:49.398691] Epoch: [85]  [620/781]  eta: 0:00:26  lr: 0.000014  training_loss: 1.1125 (1.1529)  classification_loss: 1.1125 (1.1528)  loss_mask: 0.0000 (0.0000)  time: 0.1625  data: 0.0002  max mem: 4132
[20:44:52.658431] Epoch: [85]  [640/781]  eta: 0:00:23  lr: 0.000014  training_loss: 1.1504 (1.1529)  classification_loss: 1.1503 (1.1528)  loss_mask: 0.0000 (0.0000)  time: 0.1629  data: 0.0002  max mem: 4132
[20:44:55.937558] Epoch: [85]  [660/781]  eta: 0:00:19  lr: 0.000014  training_loss: 1.1410 (1.1534)  classification_loss: 1.1409 (1.1534)  loss_mask: 0.0000 (0.0000)  time: 0.1639  data: 0.0002  max mem: 4132
[20:44:59.261128] Epoch: [85]  [680/781]  eta: 0:00:16  lr: 0.000014  training_loss: 1.1582 (1.1541)  classification_loss: 1.1581 (1.1540)  loss_mask: 0.0000 (0.0000)  time: 0.1661  data: 0.0003  max mem: 4132
[20:45:02.596163] Epoch: [85]  [700/781]  eta: 0:00:13  lr: 0.000014  training_loss: 1.1502 (1.1544)  classification_loss: 1.1501 (1.1544)  loss_mask: 0.0000 (0.0000)  time: 0.1666  data: 0.0003  max mem: 4132
[20:45:05.915825] Epoch: [85]  [720/781]  eta: 0:00:10  lr: 0.000014  training_loss: 1.1660 (1.1546)  classification_loss: 1.1660 (1.1546)  loss_mask: 0.0000 (0.0000)  time: 0.1659  data: 0.0003  max mem: 4132
[20:45:09.198923] Epoch: [85]  [740/781]  eta: 0:00:06  lr: 0.000014  training_loss: 1.1500 (1.1547)  classification_loss: 1.1500 (1.1546)  loss_mask: 0.0000 (0.0000)  time: 0.1640  data: 0.0003  max mem: 4132
[20:45:12.487216] Epoch: [85]  [760/781]  eta: 0:00:03  lr: 0.000014  training_loss: 1.1158 (1.1549)  classification_loss: 1.1158 (1.1548)  loss_mask: 0.0000 (0.0000)  time: 0.1643  data: 0.0003  max mem: 4132
[20:45:15.819438] Epoch: [85]  [780/781]  eta: 0:00:00  lr: 0.000014  training_loss: 1.1646 (1.1553)  classification_loss: 1.1646 (1.1552)  loss_mask: 0.0000 (0.0000)  time: 0.1665  data: 0.0003  max mem: 4132
[20:45:16.013235] Epoch: [85] Total time: 0:02:09 (0.1656 s / it)
[20:45:16.013841] Averaged stats: lr: 0.000014  training_loss: 1.1646 (1.1553)  classification_loss: 1.1646 (1.1552)  loss_mask: 0.0000 (0.0000)
[20:45:16.734246] Test:  [  0/157]  eta: 0:01:52  testing_loss: 0.5074 (0.5074)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.7158  data: 0.6845  max mem: 4132
[20:45:17.036639] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.5068 (0.4907)  acc1: 85.9375 (86.0795)  acc5: 100.0000 (99.7159)  time: 0.0923  data: 0.0625  max mem: 4132
[20:45:17.324982] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4404 (0.4559)  acc1: 85.9375 (86.6815)  acc5: 100.0000 (99.6280)  time: 0.0293  data: 0.0003  max mem: 4132
[20:45:17.614958] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4350 (0.4739)  acc1: 85.9375 (86.2903)  acc5: 100.0000 (99.4960)  time: 0.0287  data: 0.0002  max mem: 4132
[20:45:17.908855] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4958 (0.4792)  acc1: 85.9375 (86.1280)  acc5: 98.4375 (99.2759)  time: 0.0289  data: 0.0002  max mem: 4132
[20:45:18.208596] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4705 (0.4738)  acc1: 85.9375 (86.3358)  acc5: 98.4375 (99.2647)  time: 0.0294  data: 0.0003  max mem: 4132
[20:45:18.505045] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4204 (0.4678)  acc1: 85.9375 (86.6291)  acc5: 100.0000 (99.2316)  time: 0.0295  data: 0.0003  max mem: 4132
[20:45:18.800410] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4272 (0.4645)  acc1: 87.5000 (86.6637)  acc5: 100.0000 (99.2298)  time: 0.0293  data: 0.0003  max mem: 4132
[20:45:19.097625] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4822 (0.4735)  acc1: 84.3750 (86.2847)  acc5: 98.4375 (99.1898)  time: 0.0294  data: 0.0003  max mem: 4132
[20:45:19.393701] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4904 (0.4705)  acc1: 87.5000 (86.3496)  acc5: 98.4375 (99.2102)  time: 0.0294  data: 0.0003  max mem: 4132
[20:45:19.685350] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.4659 (0.4724)  acc1: 87.5000 (86.2160)  acc5: 100.0000 (99.1955)  time: 0.0292  data: 0.0003  max mem: 4132
[20:45:19.977873] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4648 (0.4733)  acc1: 85.9375 (86.1627)  acc5: 100.0000 (99.1413)  time: 0.0290  data: 0.0003  max mem: 4132
[20:45:20.276851] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4648 (0.4726)  acc1: 85.9375 (86.1441)  acc5: 100.0000 (99.1606)  time: 0.0294  data: 0.0003  max mem: 4132
[20:45:20.567035] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4446 (0.4722)  acc1: 85.9375 (86.0687)  acc5: 100.0000 (99.1770)  time: 0.0293  data: 0.0003  max mem: 4132
[20:45:20.860049] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4299 (0.4681)  acc1: 85.9375 (86.2145)  acc5: 100.0000 (99.1910)  time: 0.0290  data: 0.0002  max mem: 4132
[20:45:21.143338] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4299 (0.4688)  acc1: 85.9375 (86.1651)  acc5: 100.0000 (99.2239)  time: 0.0287  data: 0.0002  max mem: 4132
[20:45:21.296996] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4731 (0.4696)  acc1: 85.9375 (86.0900)  acc5: 100.0000 (99.2100)  time: 0.0274  data: 0.0002  max mem: 4132
[20:45:21.477722] Test: Total time: 0:00:05 (0.0348 s / it)
[20:45:21.478368] * Acc@1 86.090 Acc@5 99.210 loss 0.470
[20:45:21.478688] Accuracy of the network on the 10000 test images: 86.1%
[20:45:21.478882] Max accuracy: 86.15%
[20:45:21.831413] log_dir: ./output_dir
[20:45:22.755143] Epoch: [86]  [  0/781]  eta: 0:11:59  lr: 0.000014  training_loss: 1.0186 (1.0186)  classification_loss: 1.0186 (1.0186)  loss_mask: 0.0000 (0.0000)  time: 0.9216  data: 0.7209  max mem: 4132
[20:45:26.051145] Epoch: [86]  [ 20/781]  eta: 0:02:32  lr: 0.000014  training_loss: 1.1098 (1.1374)  classification_loss: 1.1097 (1.1374)  loss_mask: 0.0000 (0.0000)  time: 0.1647  data: 0.0002  max mem: 4132
[20:45:29.307842] Epoch: [86]  [ 40/781]  eta: 0:02:15  lr: 0.000014  training_loss: 1.1721 (1.1477)  classification_loss: 1.1721 (1.1477)  loss_mask: 0.0000 (0.0000)  time: 0.1628  data: 0.0002  max mem: 4132
[20:45:32.568065] Epoch: [86]  [ 60/781]  eta: 0:02:06  lr: 0.000014  training_loss: 1.1710 (1.1589)  classification_loss: 1.1709 (1.1588)  loss_mask: 0.0000 (0.0000)  time: 0.1629  data: 0.0002  max mem: 4132
[20:45:35.812811] Epoch: [86]  [ 80/781]  eta: 0:02:00  lr: 0.000014  training_loss: 1.1152 (1.1593)  classification_loss: 1.1152 (1.1593)  loss_mask: 0.0000 (0.0000)  time: 0.1622  data: 0.0002  max mem: 4132
[20:45:39.110136] Epoch: [86]  [100/781]  eta: 0:01:56  lr: 0.000014  training_loss: 1.1757 (1.1625)  classification_loss: 1.1757 (1.1625)  loss_mask: 0.0000 (0.0000)  time: 0.1648  data: 0.0003  max mem: 4132
[20:45:42.421348] Epoch: [86]  [120/781]  eta: 0:01:52  lr: 0.000014  training_loss: 1.1184 (1.1582)  classification_loss: 1.1183 (1.1582)  loss_mask: 0.0000 (0.0000)  time: 0.1655  data: 0.0004  max mem: 4132
[20:45:45.693772] Epoch: [86]  [140/781]  eta: 0:01:48  lr: 0.000014  training_loss: 1.1264 (1.1558)  classification_loss: 1.1264 (1.1558)  loss_mask: 0.0000 (0.0000)  time: 0.1635  data: 0.0002  max mem: 4132
[20:45:48.995120] Epoch: [86]  [160/781]  eta: 0:01:44  lr: 0.000014  training_loss: 1.1165 (1.1478)  classification_loss: 1.1165 (1.1478)  loss_mask: 0.0000 (0.0000)  time: 0.1650  data: 0.0003  max mem: 4132
[20:45:52.324148] Epoch: [86]  [180/781]  eta: 0:01:41  lr: 0.000014  training_loss: 1.1407 (1.1461)  classification_loss: 1.1407 (1.1460)  loss_mask: 0.0000 (0.0000)  time: 0.1663  data: 0.0003  max mem: 4132
[20:45:55.639520] Epoch: [86]  [200/781]  eta: 0:01:37  lr: 0.000014  training_loss: 1.1168 (1.1456)  classification_loss: 1.1168 (1.1455)  loss_mask: 0.0000 (0.0000)  time: 0.1657  data: 0.0003  max mem: 4132
[20:45:58.985664] Epoch: [86]  [220/781]  eta: 0:01:34  lr: 0.000014  training_loss: 1.1425 (1.1461)  classification_loss: 1.1425 (1.1461)  loss_mask: 0.0000 (0.0000)  time: 0.1672  data: 0.0004  max mem: 4132
[20:46:02.296849] Epoch: [86]  [240/781]  eta: 0:01:30  lr: 0.000014  training_loss: 1.1648 (1.1503)  classification_loss: 1.1647 (1.1503)  loss_mask: 0.0000 (0.0000)  time: 0.1655  data: 0.0003  max mem: 4132
[20:46:05.592624] Epoch: [86]  [260/781]  eta: 0:01:27  lr: 0.000014  training_loss: 1.1846 (1.1515)  classification_loss: 1.1845 (1.1514)  loss_mask: 0.0000 (0.0000)  time: 0.1647  data: 0.0003  max mem: 4132
[20:46:08.862747] Epoch: [86]  [280/781]  eta: 0:01:23  lr: 0.000013  training_loss: 1.1054 (1.1493)  classification_loss: 1.1054 (1.1493)  loss_mask: 0.0000 (0.0000)  time: 0.1634  data: 0.0002  max mem: 4132
[20:46:12.139733] Epoch: [86]  [300/781]  eta: 0:01:20  lr: 0.000013  training_loss: 1.1499 (1.1488)  classification_loss: 1.1499 (1.1487)  loss_mask: 0.0000 (0.0000)  time: 0.1637  data: 0.0003  max mem: 4132
[20:46:15.428802] Epoch: [86]  [320/781]  eta: 0:01:16  lr: 0.000013  training_loss: 1.1501 (1.1491)  classification_loss: 1.1501 (1.1491)  loss_mask: 0.0000 (0.0000)  time: 0.1643  data: 0.0002  max mem: 4132
[20:46:18.710467] Epoch: [86]  [340/781]  eta: 0:01:13  lr: 0.000013  training_loss: 1.1154 (1.1486)  classification_loss: 1.1154 (1.1486)  loss_mask: 0.0000 (0.0000)  time: 0.1640  data: 0.0002  max mem: 4132
[20:46:22.002512] Epoch: [86]  [360/781]  eta: 0:01:10  lr: 0.000013  training_loss: 1.1307 (1.1483)  classification_loss: 1.1307 (1.1483)  loss_mask: 0.0000 (0.0000)  time: 0.1645  data: 0.0003  max mem: 4132
[20:46:25.322479] Epoch: [86]  [380/781]  eta: 0:01:06  lr: 0.000013  training_loss: 1.1398 (1.1486)  classification_loss: 1.1398 (1.1486)  loss_mask: 0.0000 (0.0000)  time: 0.1659  data: 0.0004  max mem: 4132
[20:46:28.625903] Epoch: [86]  [400/781]  eta: 0:01:03  lr: 0.000013  training_loss: 1.1406 (1.1496)  classification_loss: 1.1406 (1.1495)  loss_mask: 0.0000 (0.0000)  time: 0.1651  data: 0.0003  max mem: 4132
[20:46:31.928181] Epoch: [86]  [420/781]  eta: 0:01:00  lr: 0.000013  training_loss: 1.1447 (1.1501)  classification_loss: 1.1447 (1.1501)  loss_mask: 0.0000 (0.0000)  time: 0.1650  data: 0.0003  max mem: 4132
[20:46:35.251514] Epoch: [86]  [440/781]  eta: 0:00:56  lr: 0.000013  training_loss: 1.1365 (1.1510)  classification_loss: 1.1365 (1.1510)  loss_mask: 0.0000 (0.0000)  time: 0.1661  data: 0.0003  max mem: 4132
[20:46:38.560067] Epoch: [86]  [460/781]  eta: 0:00:53  lr: 0.000013  training_loss: 1.1294 (1.1507)  classification_loss: 1.1293 (1.1506)  loss_mask: 0.0000 (0.0000)  time: 0.1653  data: 0.0003  max mem: 4132
[20:46:41.871441] Epoch: [86]  [480/781]  eta: 0:00:50  lr: 0.000013  training_loss: 1.1502 (1.1508)  classification_loss: 1.1502 (1.1508)  loss_mask: 0.0000 (0.0000)  time: 0.1655  data: 0.0003  max mem: 4132
[20:46:45.173721] Epoch: [86]  [500/781]  eta: 0:00:46  lr: 0.000013  training_loss: 1.1356 (1.1502)  classification_loss: 1.1356 (1.1502)  loss_mask: 0.0000 (0.0000)  time: 0.1650  data: 0.0003  max mem: 4132
[20:46:48.450486] Epoch: [86]  [520/781]  eta: 0:00:43  lr: 0.000013  training_loss: 1.1549 (1.1506)  classification_loss: 1.1548 (1.1506)  loss_mask: 0.0000 (0.0000)  time: 0.1637  data: 0.0002  max mem: 4132
[20:46:51.724201] Epoch: [86]  [540/781]  eta: 0:00:40  lr: 0.000013  training_loss: 1.1125 (1.1506)  classification_loss: 1.1125 (1.1506)  loss_mask: 0.0000 (0.0000)  time: 0.1636  data: 0.0003  max mem: 4132
[20:46:55.002301] Epoch: [86]  [560/781]  eta: 0:00:36  lr: 0.000013  training_loss: 1.1393 (1.1502)  classification_loss: 1.1393 (1.1502)  loss_mask: 0.0000 (0.0000)  time: 0.1638  data: 0.0003  max mem: 4132
[20:46:58.273465] Epoch: [86]  [580/781]  eta: 0:00:33  lr: 0.000013  training_loss: 1.1392 (1.1498)  classification_loss: 1.1392 (1.1498)  loss_mask: 0.0000 (0.0000)  time: 0.1635  data: 0.0002  max mem: 4132
[20:47:01.534314] Epoch: [86]  [600/781]  eta: 0:00:30  lr: 0.000013  training_loss: 1.1836 (1.1508)  classification_loss: 1.1835 (1.1507)  loss_mask: 0.0000 (0.0000)  time: 0.1630  data: 0.0003  max mem: 4132
[20:47:04.850269] Epoch: [86]  [620/781]  eta: 0:00:26  lr: 0.000013  training_loss: 1.1460 (1.1508)  classification_loss: 1.1460 (1.1507)  loss_mask: 0.0000 (0.0000)  time: 0.1657  data: 0.0003  max mem: 4132
[20:47:08.224562] Epoch: [86]  [640/781]  eta: 0:00:23  lr: 0.000013  training_loss: 1.1178 (1.1506)  classification_loss: 1.1178 (1.1505)  loss_mask: 0.0000 (0.0000)  time: 0.1686  data: 0.0007  max mem: 4132
[20:47:11.534956] Epoch: [86]  [660/781]  eta: 0:00:20  lr: 0.000013  training_loss: 1.1463 (1.1508)  classification_loss: 1.1463 (1.1508)  loss_mask: 0.0000 (0.0000)  time: 0.1654  data: 0.0003  max mem: 4132
[20:47:14.849290] Epoch: [86]  [680/781]  eta: 0:00:16  lr: 0.000013  training_loss: 1.1122 (1.1499)  classification_loss: 1.1122 (1.1499)  loss_mask: 0.0000 (0.0000)  time: 0.1656  data: 0.0003  max mem: 4132
[20:47:18.154070] Epoch: [86]  [700/781]  eta: 0:00:13  lr: 0.000013  training_loss: 1.1943 (1.1508)  classification_loss: 1.1943 (1.1508)  loss_mask: 0.0000 (0.0000)  time: 0.1651  data: 0.0003  max mem: 4132
[20:47:21.434749] Epoch: [86]  [720/781]  eta: 0:00:10  lr: 0.000012  training_loss: 1.1646 (1.1514)  classification_loss: 1.1646 (1.1514)  loss_mask: 0.0000 (0.0000)  time: 0.1639  data: 0.0002  max mem: 4132
[20:47:24.742052] Epoch: [86]  [740/781]  eta: 0:00:06  lr: 0.000012  training_loss: 1.1616 (1.1519)  classification_loss: 1.1615 (1.1519)  loss_mask: 0.0000 (0.0000)  time: 0.1653  data: 0.0003  max mem: 4132
[20:47:28.051668] Epoch: [86]  [760/781]  eta: 0:00:03  lr: 0.000012  training_loss: 1.1639 (1.1519)  classification_loss: 1.1639 (1.1519)  loss_mask: 0.0000 (0.0000)  time: 0.1654  data: 0.0003  max mem: 4132
[20:47:31.291476] Epoch: [86]  [780/781]  eta: 0:00:00  lr: 0.000012  training_loss: 1.1101 (1.1514)  classification_loss: 1.1101 (1.1514)  loss_mask: 0.0000 (0.0000)  time: 0.1619  data: 0.0002  max mem: 4132
[20:47:31.473323] Epoch: [86] Total time: 0:02:09 (0.1660 s / it)
[20:47:31.473838] Averaged stats: lr: 0.000012  training_loss: 1.1101 (1.1514)  classification_loss: 1.1101 (1.1514)  loss_mask: 0.0000 (0.0000)
[20:47:32.189335] Test:  [  0/157]  eta: 0:01:50  testing_loss: 0.4767 (0.4767)  acc1: 84.3750 (84.3750)  acc5: 98.4375 (98.4375)  time: 0.7038  data: 0.6706  max mem: 4132
[20:47:32.482086] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.4767 (0.4868)  acc1: 85.9375 (85.5114)  acc5: 100.0000 (99.5739)  time: 0.0900  data: 0.0611  max mem: 4132
[20:47:32.769194] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4363 (0.4473)  acc1: 85.9375 (86.6071)  acc5: 100.0000 (99.6280)  time: 0.0286  data: 0.0002  max mem: 4132
[20:47:33.060768] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4181 (0.4679)  acc1: 85.9375 (86.0383)  acc5: 100.0000 (99.3952)  time: 0.0287  data: 0.0002  max mem: 4132
[20:47:33.344169] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4732 (0.4742)  acc1: 84.3750 (85.9756)  acc5: 98.4375 (99.1997)  time: 0.0286  data: 0.0002  max mem: 4132
[20:47:33.627782] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4612 (0.4671)  acc1: 85.9375 (86.2132)  acc5: 98.4375 (99.2341)  time: 0.0282  data: 0.0002  max mem: 4132
[20:47:33.911792] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4241 (0.4619)  acc1: 87.5000 (86.5523)  acc5: 100.0000 (99.2572)  time: 0.0283  data: 0.0002  max mem: 4132
[20:47:34.195720] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4293 (0.4584)  acc1: 87.5000 (86.5317)  acc5: 100.0000 (99.2738)  time: 0.0283  data: 0.0002  max mem: 4132
[20:47:34.479254] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4494 (0.4674)  acc1: 85.9375 (86.2269)  acc5: 98.4375 (99.2091)  time: 0.0282  data: 0.0002  max mem: 4132
[20:47:34.762337] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4785 (0.4642)  acc1: 85.9375 (86.2637)  acc5: 98.4375 (99.2102)  time: 0.0282  data: 0.0002  max mem: 4132
[20:47:35.045644] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4664 (0.4666)  acc1: 84.3750 (86.1077)  acc5: 100.0000 (99.2110)  time: 0.0282  data: 0.0002  max mem: 4132
[20:47:35.332108] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4812 (0.4674)  acc1: 84.3750 (85.9797)  acc5: 100.0000 (99.1976)  time: 0.0284  data: 0.0002  max mem: 4132
[20:47:35.616809] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4611 (0.4671)  acc1: 85.9375 (86.0279)  acc5: 100.0000 (99.1994)  time: 0.0284  data: 0.0002  max mem: 4132
[20:47:35.902214] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4590 (0.4671)  acc1: 85.9375 (86.0210)  acc5: 100.0000 (99.2128)  time: 0.0284  data: 0.0002  max mem: 4132
[20:47:36.185027] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4448 (0.4633)  acc1: 85.9375 (86.1591)  acc5: 100.0000 (99.2465)  time: 0.0283  data: 0.0002  max mem: 4132
[20:47:36.468907] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4453 (0.4635)  acc1: 85.9375 (86.1238)  acc5: 100.0000 (99.2653)  time: 0.0282  data: 0.0002  max mem: 4132
[20:47:36.621541] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4786 (0.4642)  acc1: 85.9375 (86.0600)  acc5: 100.0000 (99.2500)  time: 0.0273  data: 0.0002  max mem: 4132
[20:47:36.818736] Test: Total time: 0:00:05 (0.0340 s / it)
[20:47:36.819403] * Acc@1 86.060 Acc@5 99.250 loss 0.464
[20:47:36.819766] Accuracy of the network on the 10000 test images: 86.1%
[20:47:36.820049] Max accuracy: 86.15%
[20:47:37.182075] log_dir: ./output_dir
[20:47:38.076932] Epoch: [87]  [  0/781]  eta: 0:11:37  lr: 0.000012  training_loss: 1.0951 (1.0951)  classification_loss: 1.0951 (1.0951)  loss_mask: 0.0000 (0.0000)  time: 0.8930  data: 0.6841  max mem: 4132
[20:47:41.322283] Epoch: [87]  [ 20/781]  eta: 0:02:29  lr: 0.000012  training_loss: 1.1123 (1.1129)  classification_loss: 1.1123 (1.1129)  loss_mask: 0.0000 (0.0000)  time: 0.1621  data: 0.0003  max mem: 4132
[20:47:44.619302] Epoch: [87]  [ 40/781]  eta: 0:02:14  lr: 0.000012  training_loss: 1.1262 (1.1400)  classification_loss: 1.1262 (1.1400)  loss_mask: 0.0000 (0.0000)  time: 0.1648  data: 0.0003  max mem: 4132
[20:47:47.955420] Epoch: [87]  [ 60/781]  eta: 0:02:07  lr: 0.000012  training_loss: 1.1297 (1.1354)  classification_loss: 1.1297 (1.1354)  loss_mask: 0.0000 (0.0000)  time: 0.1667  data: 0.0003  max mem: 4132
[20:47:51.278117] Epoch: [87]  [ 80/781]  eta: 0:02:01  lr: 0.000012  training_loss: 1.1250 (1.1355)  classification_loss: 1.1249 (1.1341)  loss_mask: 0.0000 (0.0014)  time: 0.1660  data: 0.0004  max mem: 4132
[20:47:54.565640] Epoch: [87]  [100/781]  eta: 0:01:57  lr: 0.000012  training_loss: 1.1464 (1.1438)  classification_loss: 1.1379 (1.1423)  loss_mask: 0.0002 (0.0014)  time: 0.1643  data: 0.0003  max mem: 4132
[20:47:57.836385] Epoch: [87]  [120/781]  eta: 0:01:52  lr: 0.000012  training_loss: 1.1110 (1.1430)  classification_loss: 1.1110 (1.1417)  loss_mask: 0.0001 (0.0012)  time: 0.1635  data: 0.0002  max mem: 4132
[20:48:01.164719] Epoch: [87]  [140/781]  eta: 0:01:48  lr: 0.000012  training_loss: 1.1478 (1.1448)  classification_loss: 1.1477 (1.1438)  loss_mask: 0.0000 (0.0011)  time: 0.1663  data: 0.0003  max mem: 4132
[20:48:04.467197] Epoch: [87]  [160/781]  eta: 0:01:45  lr: 0.000012  training_loss: 1.0722 (1.1396)  classification_loss: 1.0722 (1.1386)  loss_mask: 0.0000 (0.0009)  time: 0.1650  data: 0.0002  max mem: 4132
[20:48:07.761333] Epoch: [87]  [180/781]  eta: 0:01:41  lr: 0.000012  training_loss: 1.1448 (1.1413)  classification_loss: 1.1447 (1.1405)  loss_mask: 0.0000 (0.0008)  time: 0.1646  data: 0.0002  max mem: 4132
[20:48:11.044470] Epoch: [87]  [200/781]  eta: 0:01:37  lr: 0.000012  training_loss: 1.1272 (1.1419)  classification_loss: 1.1271 (1.1411)  loss_mask: 0.0000 (0.0008)  time: 0.1641  data: 0.0002  max mem: 4132
[20:48:14.344758] Epoch: [87]  [220/781]  eta: 0:01:34  lr: 0.000012  training_loss: 1.1728 (1.1442)  classification_loss: 1.1727 (1.1435)  loss_mask: 0.0000 (0.0007)  time: 0.1649  data: 0.0005  max mem: 4132
[20:48:17.642491] Epoch: [87]  [240/781]  eta: 0:01:30  lr: 0.000012  training_loss: 1.1484 (1.1463)  classification_loss: 1.1483 (1.1456)  loss_mask: 0.0000 (0.0006)  time: 0.1648  data: 0.0002  max mem: 4132
[20:48:20.906147] Epoch: [87]  [260/781]  eta: 0:01:27  lr: 0.000012  training_loss: 1.1204 (1.1467)  classification_loss: 1.1204 (1.1461)  loss_mask: 0.0000 (0.0006)  time: 0.1631  data: 0.0002  max mem: 4132
[20:48:24.154327] Epoch: [87]  [280/781]  eta: 0:01:23  lr: 0.000012  training_loss: 1.1474 (1.1471)  classification_loss: 1.1474 (1.1466)  loss_mask: 0.0000 (0.0006)  time: 0.1623  data: 0.0002  max mem: 4132
[20:48:27.468771] Epoch: [87]  [300/781]  eta: 0:01:20  lr: 0.000012  training_loss: 1.1373 (1.1464)  classification_loss: 1.1373 (1.1458)  loss_mask: 0.0001 (0.0006)  time: 0.1656  data: 0.0003  max mem: 4132
[20:48:30.778538] Epoch: [87]  [320/781]  eta: 0:01:16  lr: 0.000012  training_loss: 1.1313 (1.1472)  classification_loss: 1.1313 (1.1466)  loss_mask: 0.0001 (0.0006)  time: 0.1654  data: 0.0003  max mem: 4132

[20:48:34.078386] Epoch: [87]  [340/781]  eta: 0:01:13  lr: 0.000012  training_loss: 1.1498 (1.1495)  classification_loss: 1.1497 (1.1489)  loss_mask: 0.0000 (0.0005)  time: 0.1649  data: 0.0003  max mem: 4132
[20:48:37.367887] Epoch: [87]  [360/781]  eta: 0:01:10  lr: 0.000012  training_loss: 1.1347 (1.1503)  classification_loss: 1.1347 (1.1498)  loss_mask: 0.0000 (0.0005)  time: 0.1644  data: 0.0003  max mem: 4132
[20:48:40.661771] Epoch: [87]  [380/781]  eta: 0:01:06  lr: 0.000012  training_loss: 1.1233 (1.1502)  classification_loss: 1.1232 (1.1497)  loss_mask: 0.0000 (0.0005)  time: 0.1646  data: 0.0003  max mem: 4132
[20:48:43.997535] Epoch: [87]  [400/781]  eta: 0:01:03  lr: 0.000011  training_loss: 1.1395 (1.1504)  classification_loss: 1.1394 (1.1500)  loss_mask: 0.0000 (0.0005)  time: 0.1667  data: 0.0004  max mem: 4132
[20:48:47.302497] Epoch: [87]  [420/781]  eta: 0:01:00  lr: 0.000011  training_loss: 1.1783 (1.1500)  classification_loss: 1.1783 (1.1496)  loss_mask: 0.0000 (0.0004)  time: 0.1651  data: 0.0003  max mem: 4132
[20:48:50.598568] Epoch: [87]  [440/781]  eta: 0:00:56  lr: 0.000011  training_loss: 1.1431 (1.1498)  classification_loss: 1.1431 (1.1493)  loss_mask: 0.0000 (0.0004)  time: 0.1647  data: 0.0003  max mem: 4132
[20:48:53.874788] Epoch: [87]  [460/781]  eta: 0:00:53  lr: 0.000011  training_loss: 1.1064 (1.1491)  classification_loss: 1.1064 (1.1487)  loss_mask: 0.0000 (0.0004)  time: 0.1637  data: 0.0002  max mem: 4132
[20:48:57.124585] Epoch: [87]  [480/781]  eta: 0:00:49  lr: 0.000011  training_loss: 1.1640 (1.1497)  classification_loss: 1.1640 (1.1493)  loss_mask: 0.0000 (0.0004)  time: 0.1624  data: 0.0002  max mem: 4132
[20:49:00.369497] Epoch: [87]  [500/781]  eta: 0:00:46  lr: 0.000011  training_loss: 1.1700 (1.1498)  classification_loss: 1.1700 (1.1494)  loss_mask: 0.0000 (0.0004)  time: 0.1621  data: 0.0002  max mem: 4132
[20:49:03.626585] Epoch: [87]  [520/781]  eta: 0:00:43  lr: 0.000011  training_loss: 1.1175 (1.1495)  classification_loss: 1.1175 (1.1492)  loss_mask: 0.0000 (0.0004)  time: 0.1628  data: 0.0002  max mem: 4132
[20:49:06.938904] Epoch: [87]  [540/781]  eta: 0:00:39  lr: 0.000011  training_loss: 1.1513 (1.1498)  classification_loss: 1.1513 (1.1495)  loss_mask: 0.0000 (0.0003)  time: 0.1655  data: 0.0003  max mem: 4132
[20:49:10.284662] Epoch: [87]  [560/781]  eta: 0:00:36  lr: 0.000011  training_loss: 1.1504 (1.1500)  classification_loss: 1.1504 (1.1496)  loss_mask: 0.0000 (0.0003)  time: 0.1672  data: 0.0003  max mem: 4132
[20:49:13.600790] Epoch: [87]  [580/781]  eta: 0:00:33  lr: 0.000011  training_loss: 1.1724 (1.1509)  classification_loss: 1.1724 (1.1506)  loss_mask: 0.0000 (0.0003)  time: 0.1657  data: 0.0003  max mem: 4132
[20:49:16.902970] Epoch: [87]  [600/781]  eta: 0:00:30  lr: 0.000011  training_loss: 1.0886 (1.1499)  classification_loss: 1.0886 (1.1496)  loss_mask: 0.0000 (0.0003)  time: 0.1650  data: 0.0007  max mem: 4132
[20:49:20.189996] Epoch: [87]  [620/781]  eta: 0:00:26  lr: 0.000011  training_loss: 1.1496 (1.1504)  classification_loss: 1.1496 (1.1501)  loss_mask: 0.0000 (0.0003)  time: 0.1642  data: 0.0002  max mem: 4132
[20:49:23.534794] Epoch: [87]  [640/781]  eta: 0:00:23  lr: 0.000011  training_loss: 1.1408 (1.1501)  classification_loss: 1.1407 (1.1498)  loss_mask: 0.0000 (0.0003)  time: 0.1671  data: 0.0007  max mem: 4132
[20:49:26.844245] Epoch: [87]  [660/781]  eta: 0:00:20  lr: 0.000011  training_loss: 1.1364 (1.1493)  classification_loss: 1.1364 (1.1490)  loss_mask: 0.0000 (0.0003)  time: 0.1654  data: 0.0003  max mem: 4132
[20:49:30.123435] Epoch: [87]  [680/781]  eta: 0:00:16  lr: 0.000011  training_loss: 1.1504 (1.1499)  classification_loss: 1.1503 (1.1497)  loss_mask: 0.0000 (0.0003)  time: 0.1639  data: 0.0003  max mem: 4132
[20:49:33.413702] Epoch: [87]  [700/781]  eta: 0:00:13  lr: 0.000011  training_loss: 1.1572 (1.1502)  classification_loss: 1.1572 (1.1499)  loss_mask: 0.0000 (0.0003)  time: 0.1644  data: 0.0003  max mem: 4132
[20:49:36.675758] Epoch: [87]  [720/781]  eta: 0:00:10  lr: 0.000011  training_loss: 1.1569 (1.1504)  classification_loss: 1.1569 (1.1501)  loss_mask: 0.0000 (0.0003)  time: 0.1630  data: 0.0003  max mem: 4132
[20:49:39.928133] Epoch: [87]  [740/781]  eta: 0:00:06  lr: 0.000011  training_loss: 1.1402 (1.1509)  classification_loss: 1.1402 (1.1506)  loss_mask: 0.0000 (0.0003)  time: 0.1625  data: 0.0002  max mem: 4132
[20:49:43.198498] Epoch: [87]  [760/781]  eta: 0:00:03  lr: 0.000011  training_loss: 1.1809 (1.1516)  classification_loss: 1.1809 (1.1514)  loss_mask: 0.0000 (0.0003)  time: 0.1634  data: 0.0002  max mem: 4132
[20:49:46.452476] Epoch: [87]  [780/781]  eta: 0:00:00  lr: 0.000011  training_loss: 1.1594 (1.1520)  classification_loss: 1.1594 (1.1517)  loss_mask: 0.0000 (0.0002)  time: 0.1626  data: 0.0002  max mem: 4132
[20:49:46.630994] Epoch: [87] Total time: 0:02:09 (0.1657 s / it)
[20:49:46.631869] Averaged stats: lr: 0.000011  training_loss: 1.1594 (1.1520)  classification_loss: 1.1594 (1.1517)  loss_mask: 0.0000 (0.0002)
[20:49:47.369355] Test:  [  0/157]  eta: 0:01:54  testing_loss: 0.4819 (0.4819)  acc1: 84.3750 (84.3750)  acc5: 96.8750 (96.8750)  time: 0.7324  data: 0.7014  max mem: 4132
[20:49:47.658191] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.4819 (0.4936)  acc1: 84.3750 (85.0852)  acc5: 100.0000 (99.4318)  time: 0.0927  data: 0.0639  max mem: 4132
[20:49:47.947951] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4453 (0.4555)  acc1: 85.9375 (86.1607)  acc5: 100.0000 (99.6280)  time: 0.0288  data: 0.0003  max mem: 4132
[20:49:48.244832] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4265 (0.4720)  acc1: 87.5000 (85.9375)  acc5: 100.0000 (99.3448)  time: 0.0292  data: 0.0003  max mem: 4132
[20:49:48.538229] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4696 (0.4767)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (99.1616)  time: 0.0293  data: 0.0003  max mem: 4132
[20:49:48.833100] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4639 (0.4715)  acc1: 85.9375 (86.1520)  acc5: 98.4375 (99.1728)  time: 0.0292  data: 0.0003  max mem: 4132
[20:49:49.134524] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4259 (0.4668)  acc1: 85.9375 (86.4754)  acc5: 100.0000 (99.2059)  time: 0.0295  data: 0.0003  max mem: 4132
[20:49:49.426155] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4267 (0.4634)  acc1: 85.9375 (86.5317)  acc5: 100.0000 (99.2077)  time: 0.0294  data: 0.0003  max mem: 4132
[20:49:49.721465] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4624 (0.4716)  acc1: 85.9375 (86.1883)  acc5: 98.4375 (99.1319)  time: 0.0291  data: 0.0003  max mem: 4132
[20:49:50.018046] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4659 (0.4683)  acc1: 85.9375 (86.2637)  acc5: 98.4375 (99.1243)  time: 0.0294  data: 0.0002  max mem: 4132
[20:49:50.317733] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.4651 (0.4704)  acc1: 85.9375 (86.1541)  acc5: 100.0000 (99.1646)  time: 0.0296  data: 0.0002  max mem: 4132
[20:49:50.611306] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4727 (0.4705)  acc1: 84.3750 (86.1346)  acc5: 100.0000 (99.1554)  time: 0.0295  data: 0.0002  max mem: 4132
[20:49:50.905196] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4648 (0.4710)  acc1: 85.9375 (86.0795)  acc5: 100.0000 (99.1865)  time: 0.0292  data: 0.0002  max mem: 4132
[20:49:51.201140] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4648 (0.4710)  acc1: 84.3750 (86.0806)  acc5: 100.0000 (99.2009)  time: 0.0293  data: 0.0004  max mem: 4132
[20:49:51.488511] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4327 (0.4671)  acc1: 85.9375 (86.2367)  acc5: 100.0000 (99.2354)  time: 0.0290  data: 0.0004  max mem: 4132
[20:49:51.773290] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4327 (0.4675)  acc1: 85.9375 (86.1238)  acc5: 100.0000 (99.2550)  time: 0.0285  data: 0.0002  max mem: 4132
[20:49:51.928167] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4690 (0.4682)  acc1: 85.9375 (86.0400)  acc5: 100.0000 (99.2500)  time: 0.0275  data: 0.0002  max mem: 4132
[20:49:52.127597] Test: Total time: 0:00:05 (0.0350 s / it)
[20:49:52.129119] * Acc@1 86.040 Acc@5 99.250 loss 0.468
[20:49:52.129487] Accuracy of the network on the 10000 test images: 86.0%
[20:49:52.129726] Max accuracy: 86.15%
[20:49:52.496157] log_dir: ./output_dir
[20:49:53.505427] Epoch: [88]  [  0/781]  eta: 0:13:06  lr: 0.000011  training_loss: 1.0079 (1.0079)  classification_loss: 1.0079 (1.0079)  loss_mask: 0.0000 (0.0000)  time: 1.0070  data: 0.8132  max mem: 4132
[20:49:56.823415] Epoch: [88]  [ 20/781]  eta: 0:02:36  lr: 0.000011  training_loss: 1.1206 (1.1210)  classification_loss: 1.1206 (1.1210)  loss_mask: 0.0000 (0.0000)  time: 0.1658  data: 0.0004  max mem: 4132
[20:50:00.125616] Epoch: [88]  [ 40/781]  eta: 0:02:17  lr: 0.000011  training_loss: 1.1314 (1.1301)  classification_loss: 1.1314 (1.1301)  loss_mask: 0.0000 (0.0000)  time: 0.1650  data: 0.0003  max mem: 4132
[20:50:03.412248] Epoch: [88]  [ 60/781]  eta: 0:02:08  lr: 0.000011  training_loss: 1.1144 (1.1414)  classification_loss: 1.1144 (1.1414)  loss_mask: 0.0000 (0.0000)  time: 0.1642  data: 0.0003  max mem: 4132
[20:50:06.717805] Epoch: [88]  [ 80/781]  eta: 0:02:02  lr: 0.000011  training_loss: 1.1096 (1.1438)  classification_loss: 1.1096 (1.1438)  loss_mask: 0.0000 (0.0000)  time: 0.1652  data: 0.0003  max mem: 4132
[20:50:10.017872] Epoch: [88]  [100/781]  eta: 0:01:58  lr: 0.000010  training_loss: 1.1838 (1.1472)  classification_loss: 1.1837 (1.1472)  loss_mask: 0.0000 (0.0000)  time: 0.1649  data: 0.0003  max mem: 4132
[20:50:13.341134] Epoch: [88]  [120/781]  eta: 0:01:53  lr: 0.000010  training_loss: 1.0919 (1.1427)  classification_loss: 1.0919 (1.1427)  loss_mask: 0.0000 (0.0000)  time: 0.1661  data: 0.0003  max mem: 4132
[20:50:16.655771] Epoch: [88]  [140/781]  eta: 0:01:49  lr: 0.000010  training_loss: 1.1439 (1.1416)  classification_loss: 1.1439 (1.1416)  loss_mask: 0.0000 (0.0000)  time: 0.1656  data: 0.0005  max mem: 4132
[20:50:19.936984] Epoch: [88]  [160/781]  eta: 0:01:45  lr: 0.000010  training_loss: 1.1260 (1.1397)  classification_loss: 1.1260 (1.1397)  loss_mask: 0.0000 (0.0000)  time: 0.1640  data: 0.0002  max mem: 4132
[20:50:23.208411] Epoch: [88]  [180/781]  eta: 0:01:41  lr: 0.000010  training_loss: 1.1243 (1.1376)  classification_loss: 1.1242 (1.1376)  loss_mask: 0.0000 (0.0000)  time: 0.1635  data: 0.0002  max mem: 4132
[20:50:26.499628] Epoch: [88]  [200/781]  eta: 0:01:38  lr: 0.000010  training_loss: 1.1536 (1.1374)  classification_loss: 1.1536 (1.1373)  loss_mask: 0.0000 (0.0000)  time: 0.1645  data: 0.0003  max mem: 4132
[20:50:29.825933] Epoch: [88]  [220/781]  eta: 0:01:34  lr: 0.000010  training_loss: 1.1295 (1.1369)  classification_loss: 1.1295 (1.1368)  loss_mask: 0.0000 (0.0000)  time: 0.1662  data: 0.0003  max mem: 4132
[20:50:33.169464] Epoch: [88]  [240/781]  eta: 0:01:31  lr: 0.000010  training_loss: 1.1231 (1.1366)  classification_loss: 1.1231 (1.1366)  loss_mask: 0.0000 (0.0000)  time: 0.1671  data: 0.0003  max mem: 4132
[20:50:36.490512] Epoch: [88]  [260/781]  eta: 0:01:27  lr: 0.000010  training_loss: 1.1682 (1.1406)  classification_loss: 1.1682 (1.1406)  loss_mask: 0.0000 (0.0000)  time: 0.1660  data: 0.0004  max mem: 4132
[20:50:39.804405] Epoch: [88]  [280/781]  eta: 0:01:24  lr: 0.000010  training_loss: 1.1181 (1.1389)  classification_loss: 1.1181 (1.1389)  loss_mask: 0.0000 (0.0000)  time: 0.1656  data: 0.0002  max mem: 4132
[20:50:43.109638] Epoch: [88]  [300/781]  eta: 0:01:20  lr: 0.000010  training_loss: 1.1332 (1.1386)  classification_loss: 1.1332 (1.1385)  loss_mask: 0.0000 (0.0000)  time: 0.1651  data: 0.0003  max mem: 4132
[20:50:46.398648] Epoch: [88]  [320/781]  eta: 0:01:17  lr: 0.000010  training_loss: 1.1175 (1.1358)  classification_loss: 1.1175 (1.1358)  loss_mask: 0.0000 (0.0000)  time: 0.1643  data: 0.0003  max mem: 4132
[20:50:49.709717] Epoch: [88]  [340/781]  eta: 0:01:13  lr: 0.000010  training_loss: 1.1006 (1.1349)  classification_loss: 1.1006 (1.1349)  loss_mask: 0.0000 (0.0000)  time: 0.1655  data: 0.0004  max mem: 4132
[20:50:53.042752] Epoch: [88]  [360/781]  eta: 0:01:10  lr: 0.000010  training_loss: 1.1448 (1.1362)  classification_loss: 1.1448 (1.1362)  loss_mask: 0.0000 (0.0000)  time: 0.1665  data: 0.0003  max mem: 4132
[20:50:56.365433] Epoch: [88]  [380/781]  eta: 0:01:07  lr: 0.000010  training_loss: 1.2062 (1.1401)  classification_loss: 1.2062 (1.1400)  loss_mask: 0.0000 (0.0000)  time: 0.1660  data: 0.0003  max mem: 4132
[20:50:59.629500] Epoch: [88]  [400/781]  eta: 0:01:03  lr: 0.000010  training_loss: 1.1116 (1.1394)  classification_loss: 1.1116 (1.1394)  loss_mask: 0.0000 (0.0000)  time: 0.1631  data: 0.0002  max mem: 4132
[20:51:02.894833] Epoch: [88]  [420/781]  eta: 0:01:00  lr: 0.000010  training_loss: 1.1298 (1.1404)  classification_loss: 1.1297 (1.1404)  loss_mask: 0.0000 (0.0000)  time: 0.1632  data: 0.0002  max mem: 4132
[20:51:06.160145] Epoch: [88]  [440/781]  eta: 0:00:56  lr: 0.000010  training_loss: 1.1238 (1.1400)  classification_loss: 1.1237 (1.1400)  loss_mask: 0.0000 (0.0000)  time: 0.1632  data: 0.0002  max mem: 4132
[20:51:09.469192] Epoch: [88]  [460/781]  eta: 0:00:53  lr: 0.000010  training_loss: 1.1221 (1.1401)  classification_loss: 1.1221 (1.1401)  loss_mask: 0.0000 (0.0000)  time: 0.1653  data: 0.0003  max mem: 4132
[20:51:12.787326] Epoch: [88]  [480/781]  eta: 0:00:50  lr: 0.000010  training_loss: 1.1419 (1.1404)  classification_loss: 1.1418 (1.1403)  loss_mask: 0.0000 (0.0000)  time: 0.1658  data: 0.0003  max mem: 4132
[20:51:16.132393] Epoch: [88]  [500/781]  eta: 0:00:46  lr: 0.000010  training_loss: 1.1602 (1.1419)  classification_loss: 1.1602 (1.1419)  loss_mask: 0.0000 (0.0000)  time: 0.1672  data: 0.0004  max mem: 4132
[20:51:19.476214] Epoch: [88]  [520/781]  eta: 0:00:43  lr: 0.000010  training_loss: 1.1476 (1.1425)  classification_loss: 1.1476 (1.1424)  loss_mask: 0.0000 (0.0000)  time: 0.1671  data: 0.0003  max mem: 4132
[20:51:22.768580] Epoch: [88]  [540/781]  eta: 0:00:40  lr: 0.000010  training_loss: 1.1033 (1.1419)  classification_loss: 1.1033 (1.1418)  loss_mask: 0.0000 (0.0000)  time: 0.1645  data: 0.0003  max mem: 4132
[20:51:26.072662] Epoch: [88]  [560/781]  eta: 0:00:36  lr: 0.000010  training_loss: 1.1462 (1.1417)  classification_loss: 1.1462 (1.1416)  loss_mask: 0.0000 (0.0000)  time: 0.1651  data: 0.0005  max mem: 4132
[20:51:29.352816] Epoch: [88]  [580/781]  eta: 0:00:33  lr: 0.000010  training_loss: 1.1313 (1.1418)  classification_loss: 1.1313 (1.1418)  loss_mask: 0.0000 (0.0000)  time: 0.1639  data: 0.0003  max mem: 4132
[20:51:32.667746] Epoch: [88]  [600/781]  eta: 0:00:30  lr: 0.000009  training_loss: 1.1083 (1.1410)  classification_loss: 1.1083 (1.1410)  loss_mask: 0.0000 (0.0000)  time: 0.1656  data: 0.0003  max mem: 4132
[20:51:35.972297] Epoch: [88]  [620/781]  eta: 0:00:26  lr: 0.000009  training_loss: 1.1390 (1.1416)  classification_loss: 1.1390 (1.1415)  loss_mask: 0.0000 (0.0000)  time: 0.1651  data: 0.0002  max mem: 4132
[20:51:39.271968] Epoch: [88]  [640/781]  eta: 0:00:23  lr: 0.000009  training_loss: 1.1195 (1.1416)  classification_loss: 1.1195 (1.1416)  loss_mask: 0.0000 (0.0000)  time: 0.1649  data: 0.0003  max mem: 4132
[20:51:42.526833] Epoch: [88]  [660/781]  eta: 0:00:20  lr: 0.000009  training_loss: 1.1604 (1.1422)  classification_loss: 1.1603 (1.1422)  loss_mask: 0.0000 (0.0000)  time: 0.1627  data: 0.0002  max mem: 4132
[20:51:45.788568] Epoch: [88]  [680/781]  eta: 0:00:16  lr: 0.000009  training_loss: 1.1678 (1.1429)  classification_loss: 1.1678 (1.1429)  loss_mask: 0.0000 (0.0000)  time: 0.1630  data: 0.0002  max mem: 4132
[20:51:49.043384] Epoch: [88]  [700/781]  eta: 0:00:13  lr: 0.000009  training_loss: 1.1908 (1.1445)  classification_loss: 1.1908 (1.1445)  loss_mask: 0.0000 (0.0000)  time: 0.1627  data: 0.0002  max mem: 4132
[20:51:52.364717] Epoch: [88]  [720/781]  eta: 0:00:10  lr: 0.000009  training_loss: 1.1471 (1.1446)  classification_loss: 1.1471 (1.1446)  loss_mask: 0.0000 (0.0000)  time: 0.1660  data: 0.0002  max mem: 4132
[20:51:55.684759] Epoch: [88]  [740/781]  eta: 0:00:06  lr: 0.000009  training_loss: 1.1178 (1.1442)  classification_loss: 1.1178 (1.1442)  loss_mask: 0.0000 (0.0000)  time: 0.1659  data: 0.0002  max mem: 4132
[20:51:58.993017] Epoch: [88]  [760/781]  eta: 0:00:03  lr: 0.000009  training_loss: 1.1742 (1.1452)  classification_loss: 1.1742 (1.1451)  loss_mask: 0.0000 (0.0000)  time: 0.1653  data: 0.0003  max mem: 4132
[20:52:02.277894] Epoch: [88]  [780/781]  eta: 0:00:00  lr: 0.000009  training_loss: 1.1638 (1.1458)  classification_loss: 1.1638 (1.1458)  loss_mask: 0.0000 (0.0000)  time: 0.1641  data: 0.0003  max mem: 4132
[20:52:02.528047] Epoch: [88] Total time: 0:02:10 (0.1665 s / it)
[20:52:02.528624] Averaged stats: lr: 0.000009  training_loss: 1.1638 (1.1458)  classification_loss: 1.1638 (1.1458)  loss_mask: 0.0000 (0.0000)
[20:52:03.247812] Test:  [  0/157]  eta: 0:01:52  testing_loss: 0.4822 (0.4822)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.7147  data: 0.6807  max mem: 4132
[20:52:03.559922] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.4822 (0.4821)  acc1: 85.9375 (86.3636)  acc5: 100.0000 (99.7159)  time: 0.0931  data: 0.0621  max mem: 4132
[20:52:03.854215] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4398 (0.4505)  acc1: 85.9375 (87.1280)  acc5: 100.0000 (99.7024)  time: 0.0301  data: 0.0003  max mem: 4132
[20:52:04.150742] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4281 (0.4701)  acc1: 85.9375 (86.5423)  acc5: 100.0000 (99.3448)  time: 0.0294  data: 0.0002  max mem: 4132
[20:52:04.441275] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4882 (0.4757)  acc1: 84.3750 (86.3948)  acc5: 98.4375 (99.2378)  time: 0.0292  data: 0.0002  max mem: 4132
[20:52:04.732926] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4871 (0.4702)  acc1: 84.3750 (86.6728)  acc5: 98.4375 (99.2647)  time: 0.0289  data: 0.0002  max mem: 4132
[20:52:05.020525] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4066 (0.4642)  acc1: 89.0625 (87.0133)  acc5: 100.0000 (99.2828)  time: 0.0288  data: 0.0002  max mem: 4132
[20:52:05.311629] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4262 (0.4596)  acc1: 89.0625 (87.1479)  acc5: 100.0000 (99.2958)  time: 0.0288  data: 0.0002  max mem: 4132
[20:52:05.602209] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4523 (0.4685)  acc1: 85.9375 (86.7670)  acc5: 98.4375 (99.2091)  time: 0.0289  data: 0.0002  max mem: 4132
[20:52:05.892577] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4721 (0.4659)  acc1: 85.9375 (86.7445)  acc5: 98.4375 (99.1758)  time: 0.0289  data: 0.0002  max mem: 4132
[20:52:06.186316] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.4721 (0.4675)  acc1: 85.9375 (86.7265)  acc5: 100.0000 (99.1955)  time: 0.0290  data: 0.0002  max mem: 4132
[20:52:06.479063] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4718 (0.4675)  acc1: 85.9375 (86.7117)  acc5: 100.0000 (99.1695)  time: 0.0292  data: 0.0003  max mem: 4132
[20:52:06.772293] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4473 (0.4668)  acc1: 85.9375 (86.6477)  acc5: 100.0000 (99.1865)  time: 0.0291  data: 0.0003  max mem: 4132
[20:52:07.062034] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4449 (0.4662)  acc1: 85.9375 (86.6054)  acc5: 100.0000 (99.1889)  time: 0.0290  data: 0.0003  max mem: 4132
[20:52:07.351387] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4443 (0.4633)  acc1: 87.5000 (86.7132)  acc5: 100.0000 (99.2132)  time: 0.0287  data: 0.0002  max mem: 4132
[20:52:07.636436] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4475 (0.4632)  acc1: 85.9375 (86.6204)  acc5: 100.0000 (99.2446)  time: 0.0285  data: 0.0002  max mem: 4132
[20:52:07.791914] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4531 (0.4638)  acc1: 85.9375 (86.5400)  acc5: 100.0000 (99.2300)  time: 0.0275  data: 0.0002  max mem: 4132
[20:52:07.963002] Test: Total time: 0:00:05 (0.0346 s / it)
[20:52:07.964486] * Acc@1 86.540 Acc@5 99.230 loss 0.464
[20:52:07.964853] Accuracy of the network on the 10000 test images: 86.5%
[20:52:07.965107] Max accuracy: 86.54%
[20:52:08.227937] log_dir: ./output_dir
[20:52:09.171928] Epoch: [89]  [  0/781]  eta: 0:12:15  lr: 0.000009  training_loss: 1.0282 (1.0282)  classification_loss: 1.0282 (1.0282)  loss_mask: 0.0000 (0.0000)  time: 0.9415  data: 0.7237  max mem: 4132
[20:52:12.492292] Epoch: [89]  [ 20/781]  eta: 0:02:34  lr: 0.000009  training_loss: 1.0796 (1.1173)  classification_loss: 1.0796 (1.1173)  loss_mask: 0.0000 (0.0000)  time: 0.1658  data: 0.0003  max mem: 4132
[20:52:15.802592] Epoch: [89]  [ 40/781]  eta: 0:02:16  lr: 0.000009  training_loss: 1.1897 (1.1470)  classification_loss: 1.1897 (1.1470)  loss_mask: 0.0000 (0.0000)  time: 0.1654  data: 0.0003  max mem: 4132
[20:52:19.095246] Epoch: [89]  [ 60/781]  eta: 0:02:08  lr: 0.000009  training_loss: 1.1319 (1.1604)  classification_loss: 1.1319 (1.1604)  loss_mask: 0.0000 (0.0000)  time: 0.1646  data: 0.0002  max mem: 4132
[20:52:22.396334] Epoch: [89]  [ 80/781]  eta: 0:02:02  lr: 0.000009  training_loss: 1.1211 (1.1539)  classification_loss: 1.1211 (1.1539)  loss_mask: 0.0000 (0.0000)  time: 0.1650  data: 0.0002  max mem: 4132
[20:52:25.699333] Epoch: [89]  [100/781]  eta: 0:01:57  lr: 0.000009  training_loss: 1.1211 (1.1526)  classification_loss: 1.1211 (1.1526)  loss_mask: 0.0000 (0.0000)  time: 0.1651  data: 0.0002  max mem: 4132
[20:52:28.960394] Epoch: [89]  [120/781]  eta: 0:01:53  lr: 0.000009  training_loss: 1.0852 (1.1487)  classification_loss: 1.0852 (1.1487)  loss_mask: 0.0000 (0.0000)  time: 0.1629  data: 0.0002  max mem: 4132
[20:52:32.269959] Epoch: [89]  [140/781]  eta: 0:01:49  lr: 0.000009  training_loss: 1.1321 (1.1486)  classification_loss: 1.1320 (1.1485)  loss_mask: 0.0000 (0.0000)  time: 0.1654  data: 0.0002  max mem: 4132
[20:52:35.556704] Epoch: [89]  [160/781]  eta: 0:01:45  lr: 0.000009  training_loss: 1.1278 (1.1457)  classification_loss: 1.1278 (1.1457)  loss_mask: 0.0000 (0.0000)  time: 0.1642  data: 0.0002  max mem: 4132
[20:52:38.903641] Epoch: [89]  [180/781]  eta: 0:01:41  lr: 0.000009  training_loss: 1.1746 (1.1481)  classification_loss: 1.1746 (1.1481)  loss_mask: 0.0000 (0.0000)  time: 0.1672  data: 0.0004  max mem: 4132
[20:52:42.229369] Epoch: [89]  [200/781]  eta: 0:01:38  lr: 0.000009  training_loss: 1.1791 (1.1514)  classification_loss: 1.1791 (1.1514)  loss_mask: 0.0000 (0.0000)  time: 0.1662  data: 0.0004  max mem: 4132
[20:52:45.541259] Epoch: [89]  [220/781]  eta: 0:01:34  lr: 0.000009  training_loss: 1.1523 (1.1514)  classification_loss: 1.1522 (1.1514)  loss_mask: 0.0000 (0.0000)  time: 0.1655  data: 0.0004  max mem: 4132
[20:52:48.874435] Epoch: [89]  [240/781]  eta: 0:01:31  lr: 0.000009  training_loss: 1.1176 (1.1520)  classification_loss: 1.1176 (1.1520)  loss_mask: 0.0000 (0.0000)  time: 0.1666  data: 0.0003  max mem: 4132
[20:52:52.171590] Epoch: [89]  [260/781]  eta: 0:01:27  lr: 0.000009  training_loss: 1.1181 (1.1504)  classification_loss: 1.1181 (1.1504)  loss_mask: 0.0000 (0.0000)  time: 0.1648  data: 0.0003  max mem: 4132
[20:52:55.484235] Epoch: [89]  [280/781]  eta: 0:01:24  lr: 0.000009  training_loss: 1.1545 (1.1503)  classification_loss: 1.1545 (1.1503)  loss_mask: 0.0000 (0.0000)  time: 0.1655  data: 0.0003  max mem: 4132
[20:52:58.794478] Epoch: [89]  [300/781]  eta: 0:01:20  lr: 0.000009  training_loss: 1.1479 (1.1498)  classification_loss: 1.1479 (1.1498)  loss_mask: 0.0000 (0.0000)  time: 0.1654  data: 0.0003  max mem: 4132
[20:53:02.080464] Epoch: [89]  [320/781]  eta: 0:01:17  lr: 0.000009  training_loss: 1.1520 (1.1508)  classification_loss: 1.1520 (1.1508)  loss_mask: 0.0000 (0.0000)  time: 0.1642  data: 0.0003  max mem: 4132
[20:53:05.361633] Epoch: [89]  [340/781]  eta: 0:01:13  lr: 0.000009  training_loss: 1.1376 (1.1492)  classification_loss: 1.1376 (1.1492)  loss_mask: 0.0000 (0.0000)  time: 0.1639  data: 0.0003  max mem: 4132
[20:53:08.612072] Epoch: [89]  [360/781]  eta: 0:01:10  lr: 0.000008  training_loss: 1.1465 (1.1488)  classification_loss: 1.1465 (1.1488)  loss_mask: 0.0000 (0.0000)  time: 0.1624  data: 0.0002  max mem: 4132
[20:53:11.875165] Epoch: [89]  [380/781]  eta: 0:01:06  lr: 0.000008  training_loss: 1.1036 (1.1478)  classification_loss: 1.1036 (1.1478)  loss_mask: 0.0000 (0.0000)  time: 0.1631  data: 0.0002  max mem: 4132
[20:53:15.151063] Epoch: [89]  [400/781]  eta: 0:01:03  lr: 0.000008  training_loss: 1.1302 (1.1473)  classification_loss: 1.1301 (1.1473)  loss_mask: 0.0000 (0.0000)  time: 0.1637  data: 0.0003  max mem: 4132
[20:53:18.469506] Epoch: [89]  [420/781]  eta: 0:01:00  lr: 0.000008  training_loss: 1.1340 (1.1474)  classification_loss: 1.1339 (1.1474)  loss_mask: 0.0000 (0.0000)  time: 0.1658  data: 0.0003  max mem: 4132
[20:53:21.818888] Epoch: [89]  [440/781]  eta: 0:00:56  lr: 0.000008  training_loss: 1.1336 (1.1465)  classification_loss: 1.1335 (1.1465)  loss_mask: 0.0000 (0.0000)  time: 0.1673  data: 0.0003  max mem: 4132
[20:53:25.120753] Epoch: [89]  [460/781]  eta: 0:00:53  lr: 0.000008  training_loss: 1.1285 (1.1457)  classification_loss: 1.1285 (1.1457)  loss_mask: 0.0000 (0.0000)  time: 0.1650  data: 0.0003  max mem: 4132
[20:53:28.407213] Epoch: [89]  [480/781]  eta: 0:00:50  lr: 0.000008  training_loss: 1.1495 (1.1461)  classification_loss: 1.1495 (1.1461)  loss_mask: 0.0000 (0.0000)  time: 0.1642  data: 0.0003  max mem: 4132
[20:53:31.712353] Epoch: [89]  [500/781]  eta: 0:00:46  lr: 0.000008  training_loss: 1.1167 (1.1448)  classification_loss: 1.1167 (1.1448)  loss_mask: 0.0000 (0.0000)  time: 0.1652  data: 0.0003  max mem: 4132
[20:53:35.021858] Epoch: [89]  [520/781]  eta: 0:00:43  lr: 0.000008  training_loss: 1.1623 (1.1441)  classification_loss: 1.1623 (1.1441)  loss_mask: 0.0000 (0.0000)  time: 0.1654  data: 0.0003  max mem: 4132
[20:53:38.330246] Epoch: [89]  [540/781]  eta: 0:00:40  lr: 0.000008  training_loss: 1.1162 (1.1436)  classification_loss: 1.1162 (1.1436)  loss_mask: 0.0000 (0.0000)  time: 0.1653  data: 0.0003  max mem: 4132
[20:53:41.652221] Epoch: [89]  [560/781]  eta: 0:00:36  lr: 0.000008  training_loss: 1.0913 (1.1426)  classification_loss: 1.0913 (1.1426)  loss_mask: 0.0000 (0.0000)  time: 0.1660  data: 0.0003  max mem: 4132
[20:53:44.927889] Epoch: [89]  [580/781]  eta: 0:00:33  lr: 0.000008  training_loss: 1.1458 (1.1426)  classification_loss: 1.1457 (1.1426)  loss_mask: 0.0000 (0.0000)  time: 0.1637  data: 0.0003  max mem: 4132
[20:53:48.162714] Epoch: [89]  [600/781]  eta: 0:00:30  lr: 0.000008  training_loss: 1.1180 (1.1422)  classification_loss: 1.1179 (1.1422)  loss_mask: 0.0000 (0.0000)  time: 0.1616  data: 0.0002  max mem: 4132
[20:53:51.395030] Epoch: [89]  [620/781]  eta: 0:00:26  lr: 0.000008  training_loss: 1.0907 (1.1414)  classification_loss: 1.0907 (1.1414)  loss_mask: 0.0000 (0.0000)  time: 0.1615  data: 0.0002  max mem: 4132
[20:53:54.651140] Epoch: [89]  [640/781]  eta: 0:00:23  lr: 0.000008  training_loss: 1.1220 (1.1421)  classification_loss: 1.1220 (1.1420)  loss_mask: 0.0000 (0.0000)  time: 0.1627  data: 0.0002  max mem: 4132
[20:53:57.913823] Epoch: [89]  [660/781]  eta: 0:00:20  lr: 0.000008  training_loss: 1.1530 (1.1429)  classification_loss: 1.1529 (1.1429)  loss_mask: 0.0000 (0.0000)  time: 0.1630  data: 0.0002  max mem: 4132
[20:54:01.223033] Epoch: [89]  [680/781]  eta: 0:00:16  lr: 0.000008  training_loss: 1.1322 (1.1426)  classification_loss: 1.1322 (1.1426)  loss_mask: 0.0000 (0.0000)  time: 0.1653  data: 0.0003  max mem: 4132
[20:54:04.560964] Epoch: [89]  [700/781]  eta: 0:00:13  lr: 0.000008  training_loss: 1.1898 (1.1436)  classification_loss: 1.1898 (1.1436)  loss_mask: 0.0000 (0.0000)  time: 0.1668  data: 0.0003  max mem: 4132
[20:54:07.889931] Epoch: [89]  [720/781]  eta: 0:00:10  lr: 0.000008  training_loss: 1.1440 (1.1439)  classification_loss: 1.1440 (1.1439)  loss_mask: 0.0000 (0.0000)  time: 0.1663  data: 0.0003  max mem: 4132
[20:54:11.159416] Epoch: [89]  [740/781]  eta: 0:00:06  lr: 0.000008  training_loss: 1.0883 (1.1437)  classification_loss: 1.0883 (1.1436)  loss_mask: 0.0000 (0.0000)  time: 0.1634  data: 0.0002  max mem: 4132
[20:54:14.510788] Epoch: [89]  [760/781]  eta: 0:00:03  lr: 0.000008  training_loss: 1.1646 (1.1436)  classification_loss: 1.1645 (1.1436)  loss_mask: 0.0000 (0.0000)  time: 0.1675  data: 0.0003  max mem: 4132
[20:54:17.796545] Epoch: [89]  [780/781]  eta: 0:00:00  lr: 0.000008  training_loss: 1.1094 (1.1441)  classification_loss: 1.1094 (1.1441)  loss_mask: 0.0000 (0.0000)  time: 0.1642  data: 0.0003  max mem: 4132
[20:54:17.971317] Epoch: [89] Total time: 0:02:09 (0.1661 s / it)
[20:54:17.971986] Averaged stats: lr: 0.000008  training_loss: 1.1094 (1.1441)  classification_loss: 1.1094 (1.1441)  loss_mask: 0.0000 (0.0000)
[20:54:18.687237] Test:  [  0/157]  eta: 0:01:51  testing_loss: 0.4749 (0.4749)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.7098  data: 0.6781  max mem: 4132
[20:54:18.994016] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.4712 (0.4728)  acc1: 85.9375 (85.3693)  acc5: 100.0000 (99.5739)  time: 0.0918  data: 0.0619  max mem: 4132
[20:54:19.281571] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4206 (0.4409)  acc1: 87.5000 (86.6815)  acc5: 100.0000 (99.6280)  time: 0.0292  data: 0.0002  max mem: 4132
[20:54:19.572244] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4148 (0.4587)  acc1: 87.5000 (86.4919)  acc5: 100.0000 (99.3952)  time: 0.0287  data: 0.0002  max mem: 4132
[20:54:19.869920] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4698 (0.4634)  acc1: 87.5000 (86.5473)  acc5: 98.4375 (99.2759)  time: 0.0292  data: 0.0003  max mem: 4132
[20:54:20.165441] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4624 (0.4569)  acc1: 85.9375 (86.8260)  acc5: 98.4375 (99.2953)  time: 0.0295  data: 0.0005  max mem: 4132
[20:54:20.456140] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4121 (0.4521)  acc1: 87.5000 (87.0902)  acc5: 100.0000 (99.3084)  time: 0.0292  data: 0.0005  max mem: 4132
[20:54:20.745352] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4121 (0.4481)  acc1: 87.5000 (87.1699)  acc5: 100.0000 (99.2958)  time: 0.0288  data: 0.0003  max mem: 4132
[20:54:21.037475] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4415 (0.4566)  acc1: 87.5000 (86.8248)  acc5: 98.4375 (99.2477)  time: 0.0289  data: 0.0003  max mem: 4132
[20:54:21.332329] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4624 (0.4536)  acc1: 85.9375 (86.8475)  acc5: 100.0000 (99.2788)  time: 0.0292  data: 0.0003  max mem: 4132
[20:54:21.623131] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.4441 (0.4552)  acc1: 85.9375 (86.7420)  acc5: 100.0000 (99.2574)  time: 0.0291  data: 0.0003  max mem: 4132
[20:54:21.913572] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4416 (0.4554)  acc1: 84.3750 (86.6554)  acc5: 100.0000 (99.2539)  time: 0.0289  data: 0.0003  max mem: 4132
[20:54:22.200661] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4399 (0.4554)  acc1: 84.3750 (86.6090)  acc5: 100.0000 (99.2639)  time: 0.0287  data: 0.0003  max mem: 4132
[20:54:22.488853] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4399 (0.4551)  acc1: 85.9375 (86.6174)  acc5: 100.0000 (99.2724)  time: 0.0286  data: 0.0002  max mem: 4132
[20:54:22.775705] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4195 (0.4517)  acc1: 85.9375 (86.7575)  acc5: 100.0000 (99.2908)  time: 0.0286  data: 0.0002  max mem: 4132
[20:54:23.060866] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4303 (0.4516)  acc1: 85.9375 (86.6825)  acc5: 100.0000 (99.3171)  time: 0.0284  data: 0.0002  max mem: 4132
[20:54:23.215619] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4421 (0.4518)  acc1: 85.9375 (86.6300)  acc5: 100.0000 (99.3100)  time: 0.0275  data: 0.0002  max mem: 4132
[20:54:23.395438] Test: Total time: 0:00:05 (0.0345 s / it)
[20:54:23.395992] * Acc@1 86.630 Acc@5 99.310 loss 0.452
[20:54:23.396301] Accuracy of the network on the 10000 test images: 86.6%
[20:54:23.396508] Max accuracy: 86.63%
[20:54:23.699644] log_dir: ./output_dir
[20:54:24.619284] Epoch: [90]  [  0/781]  eta: 0:11:56  lr: 0.000008  training_loss: 1.0648 (1.0648)  classification_loss: 1.0648 (1.0648)  loss_mask: 0.0000 (0.0000)  time: 0.9178  data: 0.7262  max mem: 4132
[20:54:27.897444] Epoch: [90]  [ 20/781]  eta: 0:02:31  lr: 0.000008  training_loss: 1.1200 (1.1048)  classification_loss: 1.1200 (1.1048)  loss_mask: 0.0000 (0.0000)  time: 0.1638  data: 0.0002  max mem: 4132
[20:54:31.161258] Epoch: [90]  [ 40/781]  eta: 0:02:14  lr: 0.000008  training_loss: 1.1256 (1.1238)  classification_loss: 1.1255 (1.1238)  loss_mask: 0.0000 (0.0000)  time: 0.1631  data: 0.0003  max mem: 4132
[20:54:34.470442] Epoch: [90]  [ 60/781]  eta: 0:02:07  lr: 0.000008  training_loss: 1.1602 (1.1371)  classification_loss: 1.1602 (1.1371)  loss_mask: 0.0000 (0.0000)  time: 0.1654  data: 0.0002  max mem: 4132
[20:54:37.734378] Epoch: [90]  [ 80/781]  eta: 0:02:01  lr: 0.000008  training_loss: 1.1014 (1.1310)  classification_loss: 1.1014 (1.1310)  loss_mask: 0.0000 (0.0000)  time: 0.1631  data: 0.0003  max mem: 4132
[20:54:41.046298] Epoch: [90]  [100/781]  eta: 0:01:56  lr: 0.000008  training_loss: 1.1709 (1.1395)  classification_loss: 1.1709 (1.1395)  loss_mask: 0.0000 (0.0000)  time: 0.1655  data: 0.0003  max mem: 4132
[20:54:44.408567] Epoch: [90]  [120/781]  eta: 0:01:53  lr: 0.000008  training_loss: 1.0971 (1.1393)  classification_loss: 1.0971 (1.1393)  loss_mask: 0.0000 (0.0000)  time: 0.1680  data: 0.0003  max mem: 4132
[20:54:47.770716] Epoch: [90]  [140/781]  eta: 0:01:49  lr: 0.000008  training_loss: 1.1637 (1.1422)  classification_loss: 1.1637 (1.1422)  loss_mask: 0.0000 (0.0000)  time: 0.1680  data: 0.0004  max mem: 4132
[20:54:51.067931] Epoch: [90]  [160/781]  eta: 0:01:45  lr: 0.000007  training_loss: 1.1496 (1.1438)  classification_loss: 1.1496 (1.1438)  loss_mask: 0.0000 (0.0000)  time: 0.1648  data: 0.0003  max mem: 4132
[20:54:54.423222] Epoch: [90]  [180/781]  eta: 0:01:41  lr: 0.000007  training_loss: 1.1576 (1.1448)  classification_loss: 1.1576 (1.1448)  loss_mask: 0.0000 (0.0000)  time: 0.1677  data: 0.0003  max mem: 4132
[20:54:57.774985] Epoch: [90]  [200/781]  eta: 0:01:38  lr: 0.000007  training_loss: 1.0978 (1.1433)  classification_loss: 1.0978 (1.1433)  loss_mask: 0.0000 (0.0000)  time: 0.1675  data: 0.0004  max mem: 4132
[20:55:01.123304] Epoch: [90]  [220/781]  eta: 0:01:34  lr: 0.000007  training_loss: 1.1270 (1.1415)  classification_loss: 1.1270 (1.1415)  loss_mask: 0.0000 (0.0000)  time: 0.1673  data: 0.0003  max mem: 4132
[20:55:04.435815] Epoch: [90]  [240/781]  eta: 0:01:31  lr: 0.000007  training_loss: 1.1413 (1.1410)  classification_loss: 1.1413 (1.1410)  loss_mask: 0.0000 (0.0000)  time: 0.1655  data: 0.0003  max mem: 4132
[20:55:07.700292] Epoch: [90]  [260/781]  eta: 0:01:27  lr: 0.000007  training_loss: 1.1733 (1.1426)  classification_loss: 1.1733 (1.1426)  loss_mask: 0.0000 (0.0000)  time: 0.1631  data: 0.0002  max mem: 4132
[20:55:10.989980] Epoch: [90]  [280/781]  eta: 0:01:24  lr: 0.000007  training_loss: 1.0396 (1.1396)  classification_loss: 1.0395 (1.1396)  loss_mask: 0.0000 (0.0000)  time: 0.1644  data: 0.0002  max mem: 4132
[20:55:14.248582] Epoch: [90]  [300/781]  eta: 0:01:20  lr: 0.000007  training_loss: 1.1550 (1.1419)  classification_loss: 1.1549 (1.1418)  loss_mask: 0.0000 (0.0001)  time: 0.1628  data: 0.0002  max mem: 4132
[20:55:17.493621] Epoch: [90]  [320/781]  eta: 0:01:17  lr: 0.000007  training_loss: 1.1440 (1.1422)  classification_loss: 1.1440 (1.1421)  loss_mask: 0.0000 (0.0001)  time: 0.1622  data: 0.0003  max mem: 4132
[20:55:20.857576] Epoch: [90]  [340/781]  eta: 0:01:13  lr: 0.000007  training_loss: 1.1119 (1.1409)  classification_loss: 1.1119 (1.1408)  loss_mask: 0.0000 (0.0001)  time: 0.1681  data: 0.0003  max mem: 4132
[20:55:24.187019] Epoch: [90]  [360/781]  eta: 0:01:10  lr: 0.000007  training_loss: 1.1264 (1.1408)  classification_loss: 1.1264 (1.1407)  loss_mask: 0.0000 (0.0001)  time: 0.1664  data: 0.0003  max mem: 4132
[20:55:27.510454] Epoch: [90]  [380/781]  eta: 0:01:07  lr: 0.000007  training_loss: 1.1422 (1.1423)  classification_loss: 1.1422 (1.1422)  loss_mask: 0.0000 (0.0001)  time: 0.1661  data: 0.0003  max mem: 4132

[20:55:30.813869] Epoch: [90]  [400/781]  eta: 0:01:03  lr: 0.000007  training_loss: 1.0726 (1.1399)  classification_loss: 1.0726 (1.1398)  loss_mask: 0.0000 (0.0001)  time: 0.1651  data: 0.0003  max mem: 4132
[20:55:34.108185] Epoch: [90]  [420/781]  eta: 0:01:00  lr: 0.000007  training_loss: 1.0997 (1.1397)  classification_loss: 1.0997 (1.1396)  loss_mask: 0.0000 (0.0001)  time: 0.1646  data: 0.0003  max mem: 4132
[20:55:37.486687] Epoch: [90]  [440/781]  eta: 0:00:57  lr: 0.000007  training_loss: 1.1128 (1.1396)  classification_loss: 1.1128 (1.1396)  loss_mask: 0.0000 (0.0001)  time: 0.1688  data: 0.0003  max mem: 4132
[20:55:40.798420] Epoch: [90]  [460/781]  eta: 0:00:53  lr: 0.000007  training_loss: 1.0891 (1.1385)  classification_loss: 1.0891 (1.1385)  loss_mask: 0.0000 (0.0001)  time: 0.1655  data: 0.0003  max mem: 4132
[20:55:44.127815] Epoch: [90]  [480/781]  eta: 0:00:50  lr: 0.000007  training_loss: 1.1298 (1.1381)  classification_loss: 1.1298 (1.1380)  loss_mask: 0.0000 (0.0001)  time: 0.1664  data: 0.0003  max mem: 4132
[20:55:47.415470] Epoch: [90]  [500/781]  eta: 0:00:46  lr: 0.000007  training_loss: 1.1350 (1.1374)  classification_loss: 1.1350 (1.1374)  loss_mask: 0.0000 (0.0001)  time: 0.1643  data: 0.0002  max mem: 4132
[20:55:50.714345] Epoch: [90]  [520/781]  eta: 0:00:43  lr: 0.000007  training_loss: 1.1262 (1.1371)  classification_loss: 1.1262 (1.1371)  loss_mask: 0.0000 (0.0001)  time: 0.1649  data: 0.0002  max mem: 4132
[20:55:53.992785] Epoch: [90]  [540/781]  eta: 0:00:40  lr: 0.000007  training_loss: 1.1346 (1.1374)  classification_loss: 1.1346 (1.1374)  loss_mask: 0.0000 (0.0001)  time: 0.1638  data: 0.0003  max mem: 4132
[20:55:57.282588] Epoch: [90]  [560/781]  eta: 0:00:36  lr: 0.000007  training_loss: 1.1368 (1.1385)  classification_loss: 1.1367 (1.1385)  loss_mask: 0.0000 (0.0001)  time: 0.1643  data: 0.0003  max mem: 4132
[20:56:00.542785] Epoch: [90]  [580/781]  eta: 0:00:33  lr: 0.000007  training_loss: 1.1600 (1.1397)  classification_loss: 1.1600 (1.1396)  loss_mask: 0.0000 (0.0001)  time: 0.1629  data: 0.0002  max mem: 4132
[20:56:03.864307] Epoch: [90]  [600/781]  eta: 0:00:30  lr: 0.000007  training_loss: 1.1183 (1.1393)  classification_loss: 1.1183 (1.1393)  loss_mask: 0.0000 (0.0000)  time: 0.1659  data: 0.0002  max mem: 4132
[20:56:07.246926] Epoch: [90]  [620/781]  eta: 0:00:26  lr: 0.000007  training_loss: 1.1216 (1.1392)  classification_loss: 1.1216 (1.1392)  loss_mask: 0.0000 (0.0000)  time: 0.1690  data: 0.0003  max mem: 4132
[20:56:10.549371] Epoch: [90]  [640/781]  eta: 0:00:23  lr: 0.000007  training_loss: 1.1463 (1.1397)  classification_loss: 1.1462 (1.1396)  loss_mask: 0.0000 (0.0000)  time: 0.1650  data: 0.0003  max mem: 4132
[20:56:13.848417] Epoch: [90]  [660/781]  eta: 0:00:20  lr: 0.000007  training_loss: 1.0984 (1.1395)  classification_loss: 1.0984 (1.1394)  loss_mask: 0.0000 (0.0000)  time: 0.1648  data: 0.0002  max mem: 4132
[20:56:17.145824] Epoch: [90]  [680/781]  eta: 0:00:16  lr: 0.000007  training_loss: 1.0905 (1.1394)  classification_loss: 1.0904 (1.1394)  loss_mask: 0.0000 (0.0000)  time: 0.1648  data: 0.0002  max mem: 4132
[20:56:20.447516] Epoch: [90]  [700/781]  eta: 0:00:13  lr: 0.000007  training_loss: 1.1658 (1.1399)  classification_loss: 1.1658 (1.1399)  loss_mask: 0.0000 (0.0000)  time: 0.1650  data: 0.0003  max mem: 4132
[20:56:23.779477] Epoch: [90]  [720/781]  eta: 0:00:10  lr: 0.000007  training_loss: 1.1000 (1.1388)  classification_loss: 1.1000 (1.1387)  loss_mask: 0.0000 (0.0000)  time: 0.1665  data: 0.0004  max mem: 4132
[20:56:27.150014] Epoch: [90]  [740/781]  eta: 0:00:06  lr: 0.000007  training_loss: 1.1519 (1.1387)  classification_loss: 1.1519 (1.1387)  loss_mask: 0.0000 (0.0000)  time: 0.1684  data: 0.0003  max mem: 4132
[20:56:30.448684] Epoch: [90]  [760/781]  eta: 0:00:03  lr: 0.000007  training_loss: 1.1727 (1.1396)  classification_loss: 1.1727 (1.1396)  loss_mask: 0.0000 (0.0000)  time: 0.1648  data: 0.0003  max mem: 4132
[20:56:33.709598] Epoch: [90]  [780/781]  eta: 0:00:00  lr: 0.000006  training_loss: 1.1827 (1.1409)  classification_loss: 1.1826 (1.1409)  loss_mask: 0.0000 (0.0000)  time: 0.1629  data: 0.0002  max mem: 4132
[20:56:33.872995] Epoch: [90] Total time: 0:02:10 (0.1667 s / it)
[20:56:33.873480] Averaged stats: lr: 0.000006  training_loss: 1.1827 (1.1409)  classification_loss: 1.1826 (1.1409)  loss_mask: 0.0000 (0.0000)
[20:56:35.208175] Test:  [  0/157]  eta: 0:01:48  testing_loss: 0.4605 (0.4605)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.6882  data: 0.6585  max mem: 4132
[20:56:35.496908] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.4605 (0.4741)  acc1: 85.9375 (85.7955)  acc5: 100.0000 (99.7159)  time: 0.0887  data: 0.0600  max mem: 4132
[20:56:35.784115] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4204 (0.4392)  acc1: 87.5000 (87.3512)  acc5: 100.0000 (99.7768)  time: 0.0286  data: 0.0003  max mem: 4132
[20:56:36.068991] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4041 (0.4555)  acc1: 89.0625 (86.9960)  acc5: 100.0000 (99.4456)  time: 0.0284  data: 0.0003  max mem: 4132
[20:56:36.353163] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4788 (0.4611)  acc1: 85.9375 (87.0427)  acc5: 98.4375 (99.2378)  time: 0.0283  data: 0.0002  max mem: 4132
[20:56:36.640153] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4726 (0.4561)  acc1: 87.5000 (87.3775)  acc5: 98.4375 (99.2647)  time: 0.0284  data: 0.0002  max mem: 4132
[20:56:36.928902] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.3990 (0.4501)  acc1: 87.5000 (87.5768)  acc5: 100.0000 (99.2572)  time: 0.0287  data: 0.0002  max mem: 4132
[20:56:37.213066] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4122 (0.4463)  acc1: 87.5000 (87.6540)  acc5: 100.0000 (99.2518)  time: 0.0285  data: 0.0002  max mem: 4132
[20:56:37.496525] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4484 (0.4549)  acc1: 87.5000 (87.2685)  acc5: 98.4375 (99.1898)  time: 0.0283  data: 0.0002  max mem: 4132
[20:56:37.781946] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4527 (0.4521)  acc1: 85.9375 (87.1738)  acc5: 98.4375 (99.1930)  time: 0.0283  data: 0.0002  max mem: 4132
[20:56:38.069632] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4422 (0.4534)  acc1: 85.9375 (87.1132)  acc5: 100.0000 (99.1955)  time: 0.0285  data: 0.0002  max mem: 4132
[20:56:38.361692] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4506 (0.4545)  acc1: 85.9375 (86.9792)  acc5: 100.0000 (99.2117)  time: 0.0288  data: 0.0002  max mem: 4132
[20:56:38.646601] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4506 (0.4551)  acc1: 85.9375 (86.9447)  acc5: 100.0000 (99.2252)  time: 0.0287  data: 0.0002  max mem: 4132
[20:56:38.931280] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4458 (0.4550)  acc1: 85.9375 (86.8678)  acc5: 100.0000 (99.2486)  time: 0.0283  data: 0.0002  max mem: 4132
[20:56:39.214026] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4274 (0.4521)  acc1: 85.9375 (86.9238)  acc5: 100.0000 (99.2686)  time: 0.0282  data: 0.0002  max mem: 4132
[20:56:39.495360] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4284 (0.4521)  acc1: 85.9375 (86.8481)  acc5: 100.0000 (99.2860)  time: 0.0281  data: 0.0001  max mem: 4132
[20:56:39.645724] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4458 (0.4527)  acc1: 85.9375 (86.7900)  acc5: 100.0000 (99.2800)  time: 0.0271  data: 0.0001  max mem: 4132
[20:56:39.817703] Test: Total time: 0:00:05 (0.0338 s / it)
[20:56:39.818156] * Acc@1 86.790 Acc@5 99.280 loss 0.453
[20:56:39.818469] Accuracy of the network on the 10000 test images: 86.8%
[20:56:39.818672] Max accuracy: 86.79%
[20:56:40.006118] log_dir: ./output_dir
[20:56:40.961068] Epoch: [91]  [  0/781]  eta: 0:12:24  lr: 0.000006  training_loss: 1.0585 (1.0585)  classification_loss: 1.0585 (1.0585)  loss_mask: 0.0000 (0.0000)  time: 0.9529  data: 0.7746  max mem: 4132
[20:56:44.255066] Epoch: [91]  [ 20/781]  eta: 0:02:33  lr: 0.000006  training_loss: 1.1135 (1.1297)  classification_loss: 1.1135 (1.1297)  loss_mask: 0.0000 (0.0000)  time: 0.1646  data: 0.0002  max mem: 4132
[20:56:47.606991] Epoch: [91]  [ 40/781]  eta: 0:02:17  lr: 0.000006  training_loss: 1.1414 (1.1383)  classification_loss: 1.1414 (1.1383)  loss_mask: 0.0000 (0.0000)  time: 0.1675  data: 0.0004  max mem: 4132
[20:56:50.913736] Epoch: [91]  [ 60/781]  eta: 0:02:08  lr: 0.000006  training_loss: 1.1355 (1.1421)  classification_loss: 1.1355 (1.1420)  loss_mask: 0.0000 (0.0000)  time: 0.1652  data: 0.0003  max mem: 4132
[20:56:54.287413] Epoch: [91]  [ 80/781]  eta: 0:02:03  lr: 0.000006  training_loss: 1.1254 (1.1399)  classification_loss: 1.1253 (1.1398)  loss_mask: 0.0000 (0.0000)  time: 0.1686  data: 0.0003  max mem: 4132
[20:56:57.597968] Epoch: [91]  [100/781]  eta: 0:01:58  lr: 0.000006  training_loss: 1.1364 (1.1413)  classification_loss: 1.1364 (1.1412)  loss_mask: 0.0000 (0.0000)  time: 0.1654  data: 0.0003  max mem: 4132
[20:57:00.889166] Epoch: [91]  [120/781]  eta: 0:01:54  lr: 0.000006  training_loss: 1.1340 (1.1389)  classification_loss: 1.1340 (1.1389)  loss_mask: 0.0000 (0.0000)  time: 0.1645  data: 0.0003  max mem: 4132
[20:57:04.194987] Epoch: [91]  [140/781]  eta: 0:01:49  lr: 0.000006  training_loss: 1.1279 (1.1363)  classification_loss: 1.1278 (1.1363)  loss_mask: 0.0000 (0.0000)  time: 0.1652  data: 0.0003  max mem: 4132
[20:57:07.515496] Epoch: [91]  [160/781]  eta: 0:01:46  lr: 0.000006  training_loss: 1.0885 (1.1341)  classification_loss: 1.0885 (1.1341)  loss_mask: 0.0000 (0.0000)  time: 0.1659  data: 0.0003  max mem: 4132
[20:57:10.798669] Epoch: [91]  [180/781]  eta: 0:01:42  lr: 0.000006  training_loss: 1.1464 (1.1362)  classification_loss: 1.1462 (1.1362)  loss_mask: 0.0000 (0.0000)  time: 0.1641  data: 0.0003  max mem: 4132
[20:57:14.056538] Epoch: [91]  [200/781]  eta: 0:01:38  lr: 0.000006  training_loss: 1.0989 (1.1351)  classification_loss: 1.0989 (1.1351)  loss_mask: 0.0000 (0.0000)  time: 0.1628  data: 0.0002  max mem: 4132
[20:57:17.315250] Epoch: [91]  [220/781]  eta: 0:01:34  lr: 0.000006  training_loss: 1.1771 (1.1372)  classification_loss: 1.1771 (1.1372)  loss_mask: 0.0000 (0.0000)  time: 0.1628  data: 0.0006  max mem: 4132
[20:57:20.629588] Epoch: [91]  [240/781]  eta: 0:01:31  lr: 0.000006  training_loss: 1.1141 (1.1382)  classification_loss: 1.1141 (1.1382)  loss_mask: 0.0000 (0.0000)  time: 0.1656  data: 0.0002  max mem: 4132
[20:57:23.883158] Epoch: [91]  [260/781]  eta: 0:01:27  lr: 0.000006  training_loss: 1.0914 (1.1380)  classification_loss: 1.0914 (1.1380)  loss_mask: 0.0000 (0.0000)  time: 0.1626  data: 0.0002  max mem: 4132
[20:57:27.176006] Epoch: [91]  [280/781]  eta: 0:01:24  lr: 0.000006  training_loss: 1.1251 (1.1375)  classification_loss: 1.1250 (1.1375)  loss_mask: 0.0000 (0.0000)  time: 0.1645  data: 0.0003  max mem: 4132
[20:57:30.519877] Epoch: [91]  [300/781]  eta: 0:01:20  lr: 0.000006  training_loss: 1.1694 (1.1395)  classification_loss: 1.1694 (1.1394)  loss_mask: 0.0000 (0.0000)  time: 0.1671  data: 0.0003  max mem: 4132
[20:57:33.847817] Epoch: [91]  [320/781]  eta: 0:01:17  lr: 0.000006  training_loss: 1.1036 (1.1381)  classification_loss: 1.1036 (1.1381)  loss_mask: 0.0000 (0.0000)  time: 0.1663  data: 0.0003  max mem: 4132
[20:57:37.165519] Epoch: [91]  [340/781]  eta: 0:01:13  lr: 0.000006  training_loss: 1.1063 (1.1378)  classification_loss: 1.1063 (1.1378)  loss_mask: 0.0000 (0.0000)  time: 0.1658  data: 0.0004  max mem: 4132
[20:57:40.478454] Epoch: [91]  [360/781]  eta: 0:01:10  lr: 0.000006  training_loss: 1.1393 (1.1378)  classification_loss: 1.1393 (1.1378)  loss_mask: 0.0000 (0.0000)  time: 0.1656  data: 0.0003  max mem: 4132
[20:57:43.774289] Epoch: [91]  [380/781]  eta: 0:01:07  lr: 0.000006  training_loss: 1.1995 (1.1399)  classification_loss: 1.1995 (1.1399)  loss_mask: 0.0000 (0.0000)  time: 0.1647  data: 0.0003  max mem: 4132
[20:57:47.098935] Epoch: [91]  [400/781]  eta: 0:01:03  lr: 0.000006  training_loss: 1.1383 (1.1407)  classification_loss: 1.1383 (1.1406)  loss_mask: 0.0000 (0.0000)  time: 0.1661  data: 0.0004  max mem: 4132
[20:57:50.412855] Epoch: [91]  [420/781]  eta: 0:01:00  lr: 0.000006  training_loss: 1.1762 (1.1409)  classification_loss: 1.1761 (1.1409)  loss_mask: 0.0000 (0.0000)  time: 0.1656  data: 0.0004  max mem: 4132
[20:57:53.698306] Epoch: [91]  [440/781]  eta: 0:00:56  lr: 0.000006  training_loss: 1.1371 (1.1404)  classification_loss: 1.1371 (1.1404)  loss_mask: 0.0000 (0.0000)  time: 0.1640  data: 0.0002  max mem: 4132
[20:57:56.976260] Epoch: [91]  [460/781]  eta: 0:00:53  lr: 0.000006  training_loss: 1.1154 (1.1393)  classification_loss: 1.1154 (1.1393)  loss_mask: 0.0000 (0.0000)  time: 0.1638  data: 0.0002  max mem: 4132
[20:58:00.247250] Epoch: [91]  [480/781]  eta: 0:00:50  lr: 0.000006  training_loss: 1.1480 (1.1400)  classification_loss: 1.1480 (1.1400)  loss_mask: 0.0000 (0.0000)  time: 0.1634  data: 0.0004  max mem: 4132
[20:58:03.496024] Epoch: [91]  [500/781]  eta: 0:00:46  lr: 0.000006  training_loss: 1.1304 (1.1399)  classification_loss: 1.1303 (1.1399)  loss_mask: 0.0000 (0.0000)  time: 0.1624  data: 0.0002  max mem: 4132
[20:58:06.778288] Epoch: [91]  [520/781]  eta: 0:00:43  lr: 0.000006  training_loss: 1.1531 (1.1401)  classification_loss: 1.1530 (1.1401)  loss_mask: 0.0000 (0.0000)  time: 0.1640  data: 0.0002  max mem: 4132
[20:58:10.111724] Epoch: [91]  [540/781]  eta: 0:00:40  lr: 0.000006  training_loss: 1.1324 (1.1400)  classification_loss: 1.1324 (1.1400)  loss_mask: 0.0000 (0.0000)  time: 0.1666  data: 0.0003  max mem: 4132
[20:58:13.437631] Epoch: [91]  [560/781]  eta: 0:00:36  lr: 0.000006  training_loss: 1.1311 (1.1402)  classification_loss: 1.1311 (1.1401)  loss_mask: 0.0000 (0.0000)  time: 0.1662  data: 0.0003  max mem: 4132
[20:58:16.770527] Epoch: [91]  [580/781]  eta: 0:00:33  lr: 0.000006  training_loss: 1.0851 (1.1393)  classification_loss: 1.0851 (1.1392)  loss_mask: 0.0000 (0.0000)  time: 0.1666  data: 0.0003  max mem: 4132
[20:58:20.053824] Epoch: [91]  [600/781]  eta: 0:00:30  lr: 0.000006  training_loss: 1.0592 (1.1378)  classification_loss: 1.0592 (1.1378)  loss_mask: 0.0000 (0.0000)  time: 0.1641  data: 0.0004  max mem: 4132
[20:58:23.350922] Epoch: [91]  [620/781]  eta: 0:00:26  lr: 0.000006  training_loss: 1.1815 (1.1384)  classification_loss: 1.1815 (1.1384)  loss_mask: 0.0000 (0.0000)  time: 0.1648  data: 0.0003  max mem: 4132
[20:58:26.712354] Epoch: [91]  [640/781]  eta: 0:00:23  lr: 0.000006  training_loss: 1.1117 (1.1378)  classification_loss: 1.1117 (1.1378)  loss_mask: 0.0000 (0.0000)  time: 0.1680  data: 0.0004  max mem: 4132
[20:58:30.025965] Epoch: [91]  [660/781]  eta: 0:00:20  lr: 0.000005  training_loss: 1.1091 (1.1383)  classification_loss: 1.1091 (1.1382)  loss_mask: 0.0000 (0.0000)  time: 0.1656  data: 0.0008  max mem: 4132
[20:58:33.310797] Epoch: [91]  [680/781]  eta: 0:00:16  lr: 0.000005  training_loss: 1.1551 (1.1387)  classification_loss: 1.1551 (1.1387)  loss_mask: 0.0000 (0.0000)  time: 0.1641  data: 0.0003  max mem: 4132
[20:58:36.570941] Epoch: [91]  [700/781]  eta: 0:00:13  lr: 0.000005  training_loss: 1.0981 (1.1380)  classification_loss: 1.0980 (1.1379)  loss_mask: 0.0000 (0.0000)  time: 0.1629  data: 0.0002  max mem: 4132
[20:58:39.830047] Epoch: [91]  [720/781]  eta: 0:00:10  lr: 0.000005  training_loss: 1.1509 (1.1383)  classification_loss: 1.1508 (1.1383)  loss_mask: 0.0000 (0.0000)  time: 0.1629  data: 0.0002  max mem: 4132
[20:58:43.064784] Epoch: [91]  [740/781]  eta: 0:00:06  lr: 0.000005  training_loss: 1.1296 (1.1388)  classification_loss: 1.1295 (1.1388)  loss_mask: 0.0000 (0.0000)  time: 0.1617  data: 0.0002  max mem: 4132
[20:58:46.294066] Epoch: [91]  [760/781]  eta: 0:00:03  lr: 0.000005  training_loss: 1.1675 (1.1396)  classification_loss: 1.1675 (1.1396)  loss_mask: 0.0000 (0.0000)  time: 0.1614  data: 0.0002  max mem: 4132
[20:58:49.569545] Epoch: [91]  [780/781]  eta: 0:00:00  lr: 0.000005  training_loss: 1.1417 (1.1398)  classification_loss: 1.1417 (1.1397)  loss_mask: 0.0000 (0.0000)  time: 0.1637  data: 0.0002  max mem: 4132
[20:58:49.766078] Epoch: [91] Total time: 0:02:09 (0.1661 s / it)
[20:58:49.766758] Averaged stats: lr: 0.000005  training_loss: 1.1417 (1.1398)  classification_loss: 1.1417 (1.1397)  loss_mask: 0.0000 (0.0000)
[20:58:50.507020] Test:  [  0/157]  eta: 0:01:52  testing_loss: 0.4785 (0.4785)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.7173  data: 0.6714  max mem: 4132
[20:58:50.816580] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.4725 (0.4727)  acc1: 85.9375 (85.6534)  acc5: 100.0000 (99.4318)  time: 0.0930  data: 0.0613  max mem: 4132
[20:58:51.116046] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4194 (0.4397)  acc1: 87.5000 (86.9792)  acc5: 100.0000 (99.6280)  time: 0.0302  data: 0.0002  max mem: 4132
[20:58:51.407286] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4084 (0.4570)  acc1: 87.5000 (86.8448)  acc5: 100.0000 (99.3448)  time: 0.0294  data: 0.0002  max mem: 4132
[20:58:51.700080] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4670 (0.4629)  acc1: 87.5000 (86.7759)  acc5: 98.4375 (99.2378)  time: 0.0290  data: 0.0003  max mem: 4132
[20:58:51.996890] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4670 (0.4569)  acc1: 85.9375 (87.1630)  acc5: 98.4375 (99.2647)  time: 0.0293  data: 0.0004  max mem: 4132
[20:58:52.291307] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4199 (0.4516)  acc1: 87.5000 (87.4744)  acc5: 100.0000 (99.2828)  time: 0.0294  data: 0.0003  max mem: 4132
[20:58:52.589363] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4189 (0.4469)  acc1: 87.5000 (87.5660)  acc5: 100.0000 (99.2738)  time: 0.0293  data: 0.0003  max mem: 4132
[20:58:52.883534] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4281 (0.4555)  acc1: 87.5000 (87.1528)  acc5: 98.4375 (99.2091)  time: 0.0293  data: 0.0003  max mem: 4132
[20:58:53.184924] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4476 (0.4528)  acc1: 85.9375 (87.0536)  acc5: 98.4375 (99.2102)  time: 0.0295  data: 0.0003  max mem: 4132
[20:58:53.484718] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.4577 (0.4546)  acc1: 85.9375 (86.9431)  acc5: 100.0000 (99.2265)  time: 0.0299  data: 0.0003  max mem: 4132
[20:58:53.772897] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4664 (0.4546)  acc1: 85.9375 (86.9510)  acc5: 100.0000 (99.2258)  time: 0.0292  data: 0.0003  max mem: 4132
[20:58:54.064231] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4542 (0.4545)  acc1: 87.5000 (86.9189)  acc5: 100.0000 (99.2510)  time: 0.0288  data: 0.0003  max mem: 4132
[20:58:54.360189] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4542 (0.4548)  acc1: 85.9375 (86.8201)  acc5: 100.0000 (99.2844)  time: 0.0292  data: 0.0003  max mem: 4132
[20:58:54.661076] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4286 (0.4517)  acc1: 87.5000 (86.9348)  acc5: 100.0000 (99.3019)  time: 0.0296  data: 0.0003  max mem: 4132
[20:58:54.947826] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4321 (0.4518)  acc1: 85.9375 (86.8791)  acc5: 100.0000 (99.3067)  time: 0.0292  data: 0.0002  max mem: 4132
[20:58:55.104385] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4365 (0.4520)  acc1: 85.9375 (86.8200)  acc5: 100.0000 (99.2900)  time: 0.0277  data: 0.0002  max mem: 4132
[20:58:55.286800] Test: Total time: 0:00:05 (0.0351 s / it)
[20:58:55.287393] * Acc@1 86.820 Acc@5 99.290 loss 0.452
[20:58:55.287837] Accuracy of the network on the 10000 test images: 86.8%
[20:58:55.288094] Max accuracy: 86.82%
[20:58:55.404051] log_dir: ./output_dir
[20:58:56.355492] Epoch: [92]  [  0/781]  eta: 0:12:21  lr: 0.000005  training_loss: 0.9710 (0.9710)  classification_loss: 0.9710 (0.9710)  loss_mask: 0.0000 (0.0000)  time: 0.9495  data: 0.7393  max mem: 4132
[20:58:59.703755] Epoch: [92]  [ 20/781]  eta: 0:02:35  lr: 0.000005  training_loss: 1.0922 (1.0936)  classification_loss: 1.0922 (1.0936)  loss_mask: 0.0000 (0.0000)  time: 0.1673  data: 0.0002  max mem: 4132
[20:59:03.003515] Epoch: [92]  [ 40/781]  eta: 0:02:17  lr: 0.000005  training_loss: 1.1240 (1.1101)  classification_loss: 1.1239 (1.1100)  loss_mask: 0.0000 (0.0000)  time: 0.1649  data: 0.0002  max mem: 4132
[20:59:06.309494] Epoch: [92]  [ 60/781]  eta: 0:02:08  lr: 0.000005  training_loss: 1.1089 (1.1166)  classification_loss: 1.1089 (1.1165)  loss_mask: 0.0000 (0.0000)  time: 0.1652  data: 0.0003  max mem: 4132
[20:59:09.648103] Epoch: [92]  [ 80/781]  eta: 0:02:03  lr: 0.000005  training_loss: 1.1787 (1.1251)  classification_loss: 1.1787 (1.1251)  loss_mask: 0.0000 (0.0000)  time: 0.1668  data: 0.0004  max mem: 4132
[20:59:12.951861] Epoch: [92]  [100/781]  eta: 0:01:58  lr: 0.000005  training_loss: 1.1557 (1.1331)  classification_loss: 1.1557 (1.1331)  loss_mask: 0.0000 (0.0000)  time: 0.1651  data: 0.0003  max mem: 4132
[20:59:16.238141] Epoch: [92]  [120/781]  eta: 0:01:53  lr: 0.000005  training_loss: 1.2007 (1.1419)  classification_loss: 1.2007 (1.1419)  loss_mask: 0.0000 (0.0000)  time: 0.1642  data: 0.0002  max mem: 4132
[20:59:19.510943] Epoch: [92]  [140/781]  eta: 0:01:49  lr: 0.000005  training_loss: 1.1389 (1.1411)  classification_loss: 1.1389 (1.1411)  loss_mask: 0.0000 (0.0000)  time: 0.1636  data: 0.0002  max mem: 4132
[20:59:22.765814] Epoch: [92]  [160/781]  eta: 0:01:45  lr: 0.000005  training_loss: 1.1172 (1.1421)  classification_loss: 1.1172 (1.1421)  loss_mask: 0.0000 (0.0000)  time: 0.1627  data: 0.0002  max mem: 4132
[20:59:26.016146] Epoch: [92]  [180/781]  eta: 0:01:41  lr: 0.000005  training_loss: 1.1256 (1.1429)  classification_loss: 1.1256 (1.1428)  loss_mask: 0.0000 (0.0000)  time: 0.1624  data: 0.0003  max mem: 4132
[20:59:29.305050] Epoch: [92]  [200/781]  eta: 0:01:37  lr: 0.000005  training_loss: 1.1168 (1.1410)  classification_loss: 1.1168 (1.1410)  loss_mask: 0.0000 (0.0000)  time: 0.1643  data: 0.0003  max mem: 4132
[20:59:32.655122] Epoch: [92]  [220/781]  eta: 0:01:34  lr: 0.000005  training_loss: 1.1800 (1.1423)  classification_loss: 1.1515 (1.1415)  loss_mask: 0.0001 (0.0009)  time: 0.1674  data: 0.0003  max mem: 4132
[20:59:35.982519] Epoch: [92]  [240/781]  eta: 0:01:31  lr: 0.000005  training_loss: 1.1121 (1.1434)  classification_loss: 1.1118 (1.1426)  loss_mask: 0.0001 (0.0008)  time: 0.1663  data: 0.0003  max mem: 4132
[20:59:39.305860] Epoch: [92]  [260/781]  eta: 0:01:27  lr: 0.000005  training_loss: 1.1308 (1.1433)  classification_loss: 1.1308 (1.1426)  loss_mask: 0.0000 (0.0007)  time: 0.1661  data: 0.0003  max mem: 4132
[20:59:42.595601] Epoch: [92]  [280/781]  eta: 0:01:24  lr: 0.000005  training_loss: 1.1208 (1.1418)  classification_loss: 1.1208 (1.1411)  loss_mask: 0.0000 (0.0007)  time: 0.1644  data: 0.0003  max mem: 4132
[20:59:45.900000] Epoch: [92]  [300/781]  eta: 0:01:20  lr: 0.000005  training_loss: 1.1622 (1.1435)  classification_loss: 1.1622 (1.1428)  loss_mask: 0.0000 (0.0007)  time: 0.1651  data: 0.0003  max mem: 4132
[20:59:49.225095] Epoch: [92]  [320/781]  eta: 0:01:17  lr: 0.000005  training_loss: 1.1509 (1.1434)  classification_loss: 1.1508 (1.1428)  loss_mask: 0.0000 (0.0006)  time: 0.1661  data: 0.0002  max mem: 4132
[20:59:52.602794] Epoch: [92]  [340/781]  eta: 0:01:13  lr: 0.000005  training_loss: 1.1417 (1.1433)  classification_loss: 1.1417 (1.1427)  loss_mask: 0.0000 (0.0006)  time: 0.1688  data: 0.0003  max mem: 4132
[20:59:55.893523] Epoch: [92]  [360/781]  eta: 0:01:10  lr: 0.000005  training_loss: 1.1360 (1.1429)  classification_loss: 1.1360 (1.1423)  loss_mask: 0.0000 (0.0005)  time: 0.1644  data: 0.0002  max mem: 4132
[20:59:59.219715] Epoch: [92]  [380/781]  eta: 0:01:07  lr: 0.000005  training_loss: 1.1675 (1.1447)  classification_loss: 1.1675 (1.1442)  loss_mask: 0.0000 (0.0005)  time: 0.1662  data: 0.0003  max mem: 4132
[21:00:02.495280] Epoch: [92]  [400/781]  eta: 0:01:03  lr: 0.000005  training_loss: 1.1347 (1.1446)  classification_loss: 1.1347 (1.1441)  loss_mask: 0.0000 (0.0005)  time: 0.1637  data: 0.0002  max mem: 4132
[21:00:05.758080] Epoch: [92]  [420/781]  eta: 0:01:00  lr: 0.000005  training_loss: 1.0981 (1.1442)  classification_loss: 1.0981 (1.1437)  loss_mask: 0.0000 (0.0005)  time: 0.1631  data: 0.0002  max mem: 4132
[21:00:09.015575] Epoch: [92]  [440/781]  eta: 0:00:56  lr: 0.000005  training_loss: 1.0835 (1.1424)  classification_loss: 1.0834 (1.1420)  loss_mask: 0.0000 (0.0005)  time: 0.1628  data: 0.0002  max mem: 4132
[21:00:12.300825] Epoch: [92]  [460/781]  eta: 0:00:53  lr: 0.000005  training_loss: 1.1017 (1.1414)  classification_loss: 1.1017 (1.1410)  loss_mask: 0.0000 (0.0004)  time: 0.1642  data: 0.0002  max mem: 4132
[21:00:15.660586] Epoch: [92]  [480/781]  eta: 0:00:50  lr: 0.000005  training_loss: 1.2276 (1.1440)  classification_loss: 1.2276 (1.1436)  loss_mask: 0.0000 (0.0004)  time: 0.1679  data: 0.0003  max mem: 4132
[21:00:18.999186] Epoch: [92]  [500/781]  eta: 0:00:46  lr: 0.000005  training_loss: 1.1647 (1.1448)  classification_loss: 1.1647 (1.1444)  loss_mask: 0.0000 (0.0004)  time: 0.1668  data: 0.0004  max mem: 4132
[21:00:22.313679] Epoch: [92]  [520/781]  eta: 0:00:43  lr: 0.000005  training_loss: 1.1398 (1.1447)  classification_loss: 1.1398 (1.1443)  loss_mask: 0.0000 (0.0004)  time: 0.1656  data: 0.0004  max mem: 4132
[21:00:25.617253] Epoch: [92]  [540/781]  eta: 0:00:40  lr: 0.000005  training_loss: 1.1846 (1.1461)  classification_loss: 1.1846 (1.1457)  loss_mask: 0.0000 (0.0004)  time: 0.1651  data: 0.0003  max mem: 4132
[21:00:28.923422] Epoch: [92]  [560/781]  eta: 0:00:36  lr: 0.000005  training_loss: 1.1765 (1.1458)  classification_loss: 1.1765 (1.1454)  loss_mask: 0.0000 (0.0004)  time: 0.1652  data: 0.0003  max mem: 4132
[21:00:32.249751] Epoch: [92]  [580/781]  eta: 0:00:33  lr: 0.000005  training_loss: 1.1479 (1.1459)  classification_loss: 1.1479 (1.1455)  loss_mask: 0.0000 (0.0003)  time: 0.1662  data: 0.0003  max mem: 4132
[21:00:35.586908] Epoch: [92]  [600/781]  eta: 0:00:30  lr: 0.000005  training_loss: 1.1575 (1.1464)  classification_loss: 1.1575 (1.1460)  loss_mask: 0.0000 (0.0003)  time: 0.1668  data: 0.0003  max mem: 4132
[21:00:38.902481] Epoch: [92]  [620/781]  eta: 0:00:26  lr: 0.000005  training_loss: 1.1268 (1.1459)  classification_loss: 1.1268 (1.1456)  loss_mask: 0.0000 (0.0003)  time: 0.1657  data: 0.0003  max mem: 4132
[21:00:42.200201] Epoch: [92]  [640/781]  eta: 0:00:23  lr: 0.000004  training_loss: 1.1806 (1.1458)  classification_loss: 1.1806 (1.1455)  loss_mask: 0.0000 (0.0003)  time: 0.1648  data: 0.0002  max mem: 4132
[21:00:45.498425] Epoch: [92]  [660/781]  eta: 0:00:20  lr: 0.000004  training_loss: 1.1807 (1.1464)  classification_loss: 1.1807 (1.1461)  loss_mask: 0.0000 (0.0003)  time: 0.1648  data: 0.0002  max mem: 4132
[21:00:48.763911] Epoch: [92]  [680/781]  eta: 0:00:16  lr: 0.000004  training_loss: 1.1759 (1.1474)  classification_loss: 1.1759 (1.1471)  loss_mask: 0.0000 (0.0003)  time: 0.1632  data: 0.0004  max mem: 4132
[21:00:52.046374] Epoch: [92]  [700/781]  eta: 0:00:13  lr: 0.000004  training_loss: 1.1473 (1.1470)  classification_loss: 1.1473 (1.1467)  loss_mask: 0.0000 (0.0003)  time: 0.1640  data: 0.0002  max mem: 4132
[21:00:55.343009] Epoch: [92]  [720/781]  eta: 0:00:10  lr: 0.000004  training_loss: 1.1276 (1.1462)  classification_loss: 1.1275 (1.1459)  loss_mask: 0.0000 (0.0003)  time: 0.1647  data: 0.0003  max mem: 4132
[21:00:58.646920] Epoch: [92]  [740/781]  eta: 0:00:06  lr: 0.000004  training_loss: 1.1063 (1.1454)  classification_loss: 1.1063 (1.1451)  loss_mask: 0.0000 (0.0003)  time: 0.1651  data: 0.0003  max mem: 4132
[21:01:01.961692] Epoch: [92]  [760/781]  eta: 0:00:03  lr: 0.000004  training_loss: 1.1413 (1.1460)  classification_loss: 1.1413 (1.1457)  loss_mask: 0.0000 (0.0003)  time: 0.1656  data: 0.0004  max mem: 4132
[21:01:05.240642] Epoch: [92]  [780/781]  eta: 0:00:00  lr: 0.000004  training_loss: 1.1634 (1.1460)  classification_loss: 1.1633 (1.1457)  loss_mask: 0.0000 (0.0003)  time: 0.1638  data: 0.0002  max mem: 4132
[21:01:05.400994] Epoch: [92] Total time: 0:02:09 (0.1664 s / it)
[21:01:05.401603] Averaged stats: lr: 0.000004  training_loss: 1.1634 (1.1460)  classification_loss: 1.1633 (1.1457)  loss_mask: 0.0000 (0.0003)
[21:01:06.144572] Test:  [  0/157]  eta: 0:01:55  testing_loss: 0.4626 (0.4626)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.7353  data: 0.7014  max mem: 4132
[21:01:06.455132] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.4626 (0.4696)  acc1: 85.9375 (85.7955)  acc5: 100.0000 (99.5739)  time: 0.0949  data: 0.0640  max mem: 4132
[21:01:06.748129] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4288 (0.4402)  acc1: 87.5000 (86.8304)  acc5: 100.0000 (99.6280)  time: 0.0300  data: 0.0003  max mem: 4132
[21:01:07.044677] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4192 (0.4586)  acc1: 87.5000 (86.6935)  acc5: 100.0000 (99.2944)  time: 0.0293  data: 0.0003  max mem: 4132
[21:01:07.337950] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4736 (0.4632)  acc1: 87.5000 (86.7759)  acc5: 98.4375 (99.1616)  time: 0.0294  data: 0.0002  max mem: 4132
[21:01:07.630489] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4500 (0.4565)  acc1: 85.9375 (87.2243)  acc5: 98.4375 (99.2034)  time: 0.0291  data: 0.0003  max mem: 4132
[21:01:07.924131] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4146 (0.4516)  acc1: 89.0625 (87.3975)  acc5: 100.0000 (99.2572)  time: 0.0291  data: 0.0003  max mem: 4132
[21:01:08.218254] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4146 (0.4470)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (99.2738)  time: 0.0292  data: 0.0003  max mem: 4132
[21:01:08.512727] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4283 (0.4544)  acc1: 87.5000 (87.1721)  acc5: 98.4375 (99.1898)  time: 0.0292  data: 0.0003  max mem: 4132
[21:01:08.806512] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4366 (0.4512)  acc1: 85.9375 (87.1223)  acc5: 100.0000 (99.2273)  time: 0.0291  data: 0.0002  max mem: 4132
[21:01:09.100434] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.4390 (0.4531)  acc1: 85.9375 (87.0204)  acc5: 100.0000 (99.2110)  time: 0.0291  data: 0.0002  max mem: 4132
[21:01:09.394008] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4514 (0.4536)  acc1: 85.9375 (86.9369)  acc5: 100.0000 (99.1976)  time: 0.0291  data: 0.0002  max mem: 4132
[21:01:09.684876] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4298 (0.4537)  acc1: 85.9375 (86.8802)  acc5: 100.0000 (99.2123)  time: 0.0290  data: 0.0002  max mem: 4132
[21:01:09.977206] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4416 (0.4534)  acc1: 85.9375 (86.8440)  acc5: 100.0000 (99.2247)  time: 0.0290  data: 0.0003  max mem: 4132
[21:01:10.269011] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4204 (0.4497)  acc1: 85.9375 (86.9348)  acc5: 100.0000 (99.2797)  time: 0.0290  data: 0.0002  max mem: 4132
[21:01:10.558145] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4262 (0.4498)  acc1: 85.9375 (86.8895)  acc5: 100.0000 (99.2860)  time: 0.0288  data: 0.0002  max mem: 4132
[21:01:10.716221] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4363 (0.4502)  acc1: 85.9375 (86.8500)  acc5: 100.0000 (99.2700)  time: 0.0278  data: 0.0002  max mem: 4132
[21:01:10.933827] Test: Total time: 0:00:05 (0.0352 s / it)
[21:01:10.934626] * Acc@1 86.850 Acc@5 99.270 loss 0.450
[21:01:10.934980] Accuracy of the network on the 10000 test images: 86.8%
[21:01:10.935403] Max accuracy: 86.85%
[21:01:11.197606] log_dir: ./output_dir
[21:01:12.265808] Epoch: [93]  [  0/781]  eta: 0:13:52  lr: 0.000004  training_loss: 1.0091 (1.0091)  classification_loss: 1.0090 (1.0090)  loss_mask: 0.0000 (0.0000)  time: 1.0655  data: 0.8800  max mem: 4132
[21:01:15.610395] Epoch: [93]  [ 20/781]  eta: 0:02:39  lr: 0.000004  training_loss: 1.0616 (1.0947)  classification_loss: 1.0616 (1.0947)  loss_mask: 0.0000 (0.0000)  time: 0.1671  data: 0.0003  max mem: 4132
[21:01:18.911500] Epoch: [93]  [ 40/781]  eta: 0:02:19  lr: 0.000004  training_loss: 1.0940 (1.1169)  classification_loss: 1.0940 (1.1169)  loss_mask: 0.0000 (0.0000)  time: 0.1650  data: 0.0003  max mem: 4132
[21:01:22.180577] Epoch: [93]  [ 60/781]  eta: 0:02:09  lr: 0.000004  training_loss: 1.1207 (1.1183)  classification_loss: 1.1206 (1.1183)  loss_mask: 0.0000 (0.0000)  time: 0.1634  data: 0.0002  max mem: 4132
[21:01:25.457002] Epoch: [93]  [ 80/781]  eta: 0:02:03  lr: 0.000004  training_loss: 1.0935 (1.1157)  classification_loss: 1.0934 (1.1156)  loss_mask: 0.0000 (0.0000)  time: 0.1637  data: 0.0002  max mem: 4132
[21:01:28.751461] Epoch: [93]  [100/781]  eta: 0:01:58  lr: 0.000004  training_loss: 1.1215 (1.1197)  classification_loss: 1.1214 (1.1197)  loss_mask: 0.0000 (0.0000)  time: 0.1646  data: 0.0003  max mem: 4132
[21:01:32.034105] Epoch: [93]  [120/781]  eta: 0:01:53  lr: 0.000004  training_loss: 1.1295 (1.1210)  classification_loss: 1.1295 (1.1210)  loss_mask: 0.0000 (0.0000)  time: 0.1641  data: 0.0002  max mem: 4132
[21:01:35.321631] Epoch: [93]  [140/781]  eta: 0:01:49  lr: 0.000004  training_loss: 1.1590 (1.1234)  classification_loss: 1.1590 (1.1234)  loss_mask: 0.0000 (0.0000)  time: 0.1643  data: 0.0002  max mem: 4132
[21:01:38.666196] Epoch: [93]  [160/781]  eta: 0:01:45  lr: 0.000004  training_loss: 1.1145 (1.1220)  classification_loss: 1.1145 (1.1219)  loss_mask: 0.0000 (0.0000)  time: 0.1671  data: 0.0003  max mem: 4132
[21:01:42.016851] Epoch: [93]  [180/781]  eta: 0:01:42  lr: 0.000004  training_loss: 1.1061 (1.1220)  classification_loss: 1.1061 (1.1220)  loss_mask: 0.0000 (0.0000)  time: 0.1674  data: 0.0003  max mem: 4132
[21:01:45.326002] Epoch: [93]  [200/781]  eta: 0:01:38  lr: 0.000004  training_loss: 1.0714 (1.1209)  classification_loss: 1.0714 (1.1209)  loss_mask: 0.0000 (0.0000)  time: 0.1653  data: 0.0003  max mem: 4132
[21:01:48.658616] Epoch: [93]  [220/781]  eta: 0:01:35  lr: 0.000004  training_loss: 1.1236 (1.1210)  classification_loss: 1.1236 (1.1210)  loss_mask: 0.0000 (0.0000)  time: 0.1665  data: 0.0003  max mem: 4132
[21:01:51.987042] Epoch: [93]  [240/781]  eta: 0:01:31  lr: 0.000004  training_loss: 1.1414 (1.1227)  classification_loss: 1.1414 (1.1227)  loss_mask: 0.0000 (0.0000)  time: 0.1663  data: 0.0003  max mem: 4132
[21:01:55.303474] Epoch: [93]  [260/781]  eta: 0:01:27  lr: 0.000004  training_loss: 1.1521 (1.1253)  classification_loss: 1.1521 (1.1248)  loss_mask: 0.0000 (0.0005)  time: 0.1657  data: 0.0004  max mem: 4132
[21:01:58.637961] Epoch: [93]  [280/781]  eta: 0:01:24  lr: 0.000004  training_loss: 1.1212 (1.1262)  classification_loss: 1.1212 (1.1257)  loss_mask: 0.0000 (0.0005)  time: 0.1666  data: 0.0003  max mem: 4132
[21:02:01.940361] Epoch: [93]  [300/781]  eta: 0:01:21  lr: 0.000004  training_loss: 1.1442 (1.1276)  classification_loss: 1.1442 (1.1271)  loss_mask: 0.0000 (0.0005)  time: 0.1650  data: 0.0003  max mem: 4132
[21:02:05.228684] Epoch: [93]  [320/781]  eta: 0:01:17  lr: 0.000004  training_loss: 1.1117 (1.1298)  classification_loss: 1.1117 (1.1294)  loss_mask: 0.0000 (0.0004)  time: 0.1643  data: 0.0002  max mem: 4132
[21:02:08.511087] Epoch: [93]  [340/781]  eta: 0:01:14  lr: 0.000004  training_loss: 1.1056 (1.1281)  classification_loss: 1.1056 (1.1277)  loss_mask: 0.0000 (0.0004)  time: 0.1640  data: 0.0003  max mem: 4132
[21:02:11.772445] Epoch: [93]  [360/781]  eta: 0:01:10  lr: 0.000004  training_loss: 1.1636 (1.1298)  classification_loss: 1.1636 (1.1294)  loss_mask: 0.0000 (0.0004)  time: 0.1630  data: 0.0002  max mem: 4132
[21:02:15.096903] Epoch: [93]  [380/781]  eta: 0:01:07  lr: 0.000004  training_loss: 1.1324 (1.1292)  classification_loss: 1.1324 (1.1289)  loss_mask: 0.0000 (0.0004)  time: 0.1661  data: 0.0002  max mem: 4132
[21:02:18.398872] Epoch: [93]  [400/781]  eta: 0:01:03  lr: 0.000004  training_loss: 1.1699 (1.1302)  classification_loss: 1.1698 (1.1298)  loss_mask: 0.0000 (0.0003)  time: 0.1650  data: 0.0004  max mem: 4132
[21:02:21.704791] Epoch: [93]  [420/781]  eta: 0:01:00  lr: 0.000004  training_loss: 1.1057 (1.1297)  classification_loss: 1.1056 (1.1294)  loss_mask: 0.0000 (0.0003)  time: 0.1652  data: 0.0003  max mem: 4132
[21:02:25.025501] Epoch: [93]  [440/781]  eta: 0:00:57  lr: 0.000004  training_loss: 1.1638 (1.1319)  classification_loss: 1.1638 (1.1315)  loss_mask: 0.0000 (0.0003)  time: 0.1659  data: 0.0003  max mem: 4132
[21:02:28.329737] Epoch: [93]  [460/781]  eta: 0:00:53  lr: 0.000004  training_loss: 1.0659 (1.1300)  classification_loss: 1.0659 (1.1297)  loss_mask: 0.0000 (0.0003)  time: 0.1651  data: 0.0005  max mem: 4132
[21:02:31.657605] Epoch: [93]  [480/781]  eta: 0:00:50  lr: 0.000004  training_loss: 1.1757 (1.1321)  classification_loss: 1.1757 (1.1318)  loss_mask: 0.0000 (0.0003)  time: 0.1662  data: 0.0003  max mem: 4132
[21:02:35.015586] Epoch: [93]  [500/781]  eta: 0:00:46  lr: 0.000004  training_loss: 1.1257 (1.1324)  classification_loss: 1.1257 (1.1321)  loss_mask: 0.0000 (0.0003)  time: 0.1678  data: 0.0005  max mem: 4132
[21:02:38.366723] Epoch: [93]  [520/781]  eta: 0:00:43  lr: 0.000004  training_loss: 1.1000 (1.1323)  classification_loss: 1.1000 (1.1320)  loss_mask: 0.0000 (0.0003)  time: 0.1674  data: 0.0003  max mem: 4132
[21:02:41.645629] Epoch: [93]  [540/781]  eta: 0:00:40  lr: 0.000004  training_loss: 1.1108 (1.1331)  classification_loss: 1.1108 (1.1329)  loss_mask: 0.0000 (0.0003)  time: 0.1639  data: 0.0002  max mem: 4132
[21:02:44.926633] Epoch: [93]  [560/781]  eta: 0:00:36  lr: 0.000004  training_loss: 1.1498 (1.1337)  classification_loss: 1.1498 (1.1334)  loss_mask: 0.0000 (0.0003)  time: 0.1639  data: 0.0003  max mem: 4132
[21:02:48.205361] Epoch: [93]  [580/781]  eta: 0:00:33  lr: 0.000004  training_loss: 1.1062 (1.1328)  classification_loss: 1.1061 (1.1325)  loss_mask: 0.0000 (0.0002)  time: 0.1638  data: 0.0002  max mem: 4132
[21:02:51.480732] Epoch: [93]  [600/781]  eta: 0:00:30  lr: 0.000004  training_loss: 1.1076 (1.1325)  classification_loss: 1.1076 (1.1323)  loss_mask: 0.0000 (0.0002)  time: 0.1637  data: 0.0003  max mem: 4132
[21:02:54.750401] Epoch: [93]  [620/781]  eta: 0:00:26  lr: 0.000004  training_loss: 1.1036 (1.1330)  classification_loss: 1.1036 (1.1328)  loss_mask: 0.0000 (0.0002)  time: 0.1634  data: 0.0002  max mem: 4132
[21:02:58.040046] Epoch: [93]  [640/781]  eta: 0:00:23  lr: 0.000004  training_loss: 1.1708 (1.1340)  classification_loss: 1.1708 (1.1338)  loss_mask: 0.0000 (0.0002)  time: 0.1644  data: 0.0002  max mem: 4132
[21:03:01.358686] Epoch: [93]  [660/781]  eta: 0:00:20  lr: 0.000004  training_loss: 1.1102 (1.1340)  classification_loss: 1.1101 (1.1338)  loss_mask: 0.0000 (0.0002)  time: 0.1658  data: 0.0003  max mem: 4132
[21:03:04.684205] Epoch: [93]  [680/781]  eta: 0:00:16  lr: 0.000004  training_loss: 1.1529 (1.1349)  classification_loss: 1.1529 (1.1347)  loss_mask: 0.0000 (0.0002)  time: 0.1661  data: 0.0004  max mem: 4132
[21:03:08.006251] Epoch: [93]  [700/781]  eta: 0:00:13  lr: 0.000004  training_loss: 1.0510 (1.1347)  classification_loss: 1.0510 (1.1345)  loss_mask: 0.0000 (0.0002)  time: 0.1660  data: 0.0005  max mem: 4132
[21:03:11.309299] Epoch: [93]  [720/781]  eta: 0:00:10  lr: 0.000004  training_loss: 1.1598 (1.1356)  classification_loss: 1.1598 (1.1354)  loss_mask: 0.0000 (0.0002)  time: 0.1651  data: 0.0003  max mem: 4132
[21:03:14.604584] Epoch: [93]  [740/781]  eta: 0:00:06  lr: 0.000003  training_loss: 1.1119 (1.1353)  classification_loss: 1.1119 (1.1351)  loss_mask: 0.0000 (0.0002)  time: 0.1647  data: 0.0003  max mem: 4132
[21:03:17.920181] Epoch: [93]  [760/781]  eta: 0:00:03  lr: 0.000003  training_loss: 1.1257 (1.1358)  classification_loss: 1.1256 (1.1356)  loss_mask: 0.0000 (0.0002)  time: 0.1657  data: 0.0003  max mem: 4132
[21:03:21.202300] Epoch: [93]  [780/781]  eta: 0:00:00  lr: 0.000003  training_loss: 1.1025 (1.1355)  classification_loss: 1.1025 (1.1353)  loss_mask: 0.0000 (0.0002)  time: 0.1640  data: 0.0002  max mem: 4132
[21:03:21.406477] Epoch: [93] Total time: 0:02:10 (0.1667 s / it)
[21:03:21.407176] Averaged stats: lr: 0.000003  training_loss: 1.1025 (1.1355)  classification_loss: 1.1025 (1.1353)  loss_mask: 0.0000 (0.0002)
[21:03:22.140123] Test:  [  0/157]  eta: 0:01:54  testing_loss: 0.4811 (0.4811)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.7284  data: 0.6979  max mem: 4132
[21:03:22.427795] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.4672 (0.4743)  acc1: 85.9375 (85.5114)  acc5: 100.0000 (99.5739)  time: 0.0921  data: 0.0636  max mem: 4132
[21:03:22.712866] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4171 (0.4397)  acc1: 87.5000 (86.9048)  acc5: 100.0000 (99.6280)  time: 0.0284  data: 0.0002  max mem: 4132
[21:03:22.997673] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4101 (0.4584)  acc1: 87.5000 (86.7440)  acc5: 100.0000 (99.3952)  time: 0.0284  data: 0.0002  max mem: 4132
[21:03:23.284511] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4798 (0.4629)  acc1: 85.9375 (86.8140)  acc5: 98.4375 (99.2378)  time: 0.0284  data: 0.0002  max mem: 4132
[21:03:23.574389] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4574 (0.4565)  acc1: 85.9375 (87.1630)  acc5: 98.4375 (99.2341)  time: 0.0287  data: 0.0002  max mem: 4132
[21:03:23.865498] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4257 (0.4510)  acc1: 87.5000 (87.4232)  acc5: 100.0000 (99.2572)  time: 0.0289  data: 0.0002  max mem: 4132
[21:03:24.152847] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4216 (0.4473)  acc1: 87.5000 (87.4340)  acc5: 100.0000 (99.2518)  time: 0.0288  data: 0.0002  max mem: 4132
[21:03:24.440295] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4364 (0.4554)  acc1: 87.5000 (87.1335)  acc5: 98.4375 (99.1705)  time: 0.0286  data: 0.0002  max mem: 4132
[21:03:24.736947] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4364 (0.4519)  acc1: 85.9375 (87.0707)  acc5: 98.4375 (99.1930)  time: 0.0290  data: 0.0002  max mem: 4132
[21:03:25.024892] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.4385 (0.4535)  acc1: 85.9375 (86.9585)  acc5: 100.0000 (99.2110)  time: 0.0290  data: 0.0002  max mem: 4132
[21:03:25.318252] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4559 (0.4539)  acc1: 85.9375 (86.8525)  acc5: 100.0000 (99.2117)  time: 0.0288  data: 0.0003  max mem: 4132
[21:03:25.614642] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4392 (0.4540)  acc1: 85.9375 (86.8285)  acc5: 100.0000 (99.2123)  time: 0.0292  data: 0.0003  max mem: 4132
[21:03:25.910823] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4508 (0.4536)  acc1: 85.9375 (86.7963)  acc5: 100.0000 (99.2247)  time: 0.0294  data: 0.0002  max mem: 4132
[21:03:26.200342] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4198 (0.4503)  acc1: 85.9375 (86.8684)  acc5: 100.0000 (99.2686)  time: 0.0291  data: 0.0002  max mem: 4132
[21:03:26.485031] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4198 (0.4503)  acc1: 85.9375 (86.8067)  acc5: 100.0000 (99.2964)  time: 0.0285  data: 0.0002  max mem: 4132
[21:03:26.639385] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4279 (0.4506)  acc1: 85.9375 (86.7300)  acc5: 100.0000 (99.2800)  time: 0.0274  data: 0.0002  max mem: 4132
[21:03:26.803110] Test: Total time: 0:00:05 (0.0343 s / it)
[21:03:26.803605] * Acc@1 86.730 Acc@5 99.280 loss 0.451
[21:03:26.803939] Accuracy of the network on the 10000 test images: 86.7%
[21:03:26.804149] Max accuracy: 86.85%
[21:03:27.173788] log_dir: ./output_dir
[21:03:28.057721] Epoch: [94]  [  0/781]  eta: 0:11:28  lr: 0.000003  training_loss: 1.1524 (1.1524)  classification_loss: 1.1524 (1.1524)  loss_mask: 0.0000 (0.0000)  time: 0.8820  data: 0.6857  max mem: 4132
[21:03:31.300226] Epoch: [94]  [ 20/781]  eta: 0:02:29  lr: 0.000003  training_loss: 1.0665 (1.0961)  classification_loss: 1.0664 (1.0961)  loss_mask: 0.0000 (0.0000)  time: 0.1620  data: 0.0002  max mem: 4132
[21:03:34.567104] Epoch: [94]  [ 40/781]  eta: 0:02:13  lr: 0.000003  training_loss: 1.1205 (1.1216)  classification_loss: 1.1205 (1.1216)  loss_mask: 0.0000 (0.0000)  time: 0.1633  data: 0.0003  max mem: 4132
[21:03:37.844581] Epoch: [94]  [ 60/781]  eta: 0:02:06  lr: 0.000003  training_loss: 1.1430 (1.1303)  classification_loss: 1.1430 (1.1303)  loss_mask: 0.0000 (0.0000)  time: 0.1638  data: 0.0004  max mem: 4132
[21:03:41.127077] Epoch: [94]  [ 80/781]  eta: 0:02:00  lr: 0.000003  training_loss: 1.1031 (1.1254)  classification_loss: 1.1031 (1.1254)  loss_mask: 0.0000 (0.0000)  time: 0.1640  data: 0.0003  max mem: 4132
[21:03:44.431197] Epoch: [94]  [100/781]  eta: 0:01:56  lr: 0.000003  training_loss: 1.1356 (1.1285)  classification_loss: 1.1355 (1.1284)  loss_mask: 0.0000 (0.0000)  time: 0.1651  data: 0.0003  max mem: 4132
[21:03:47.765975] Epoch: [94]  [120/781]  eta: 0:01:52  lr: 0.000003  training_loss: 1.1192 (1.1293)  classification_loss: 1.1192 (1.1293)  loss_mask: 0.0000 (0.0000)  time: 0.1666  data: 0.0003  max mem: 4132
[21:03:51.085483] Epoch: [94]  [140/781]  eta: 0:01:48  lr: 0.000003  training_loss: 1.1497 (1.1301)  classification_loss: 1.1496 (1.1301)  loss_mask: 0.0000 (0.0000)  time: 0.1658  data: 0.0003  max mem: 4132
[21:03:54.423969] Epoch: [94]  [160/781]  eta: 0:01:45  lr: 0.000003  training_loss: 1.0702 (1.1237)  classification_loss: 1.0702 (1.1237)  loss_mask: 0.0000 (0.0000)  time: 0.1668  data: 0.0003  max mem: 4132
[21:03:57.735216] Epoch: [94]  [180/781]  eta: 0:01:41  lr: 0.000003  training_loss: 1.1315 (1.1244)  classification_loss: 1.1315 (1.1244)  loss_mask: 0.0000 (0.0000)  time: 0.1654  data: 0.0005  max mem: 4132
[21:04:01.038453] Epoch: [94]  [200/781]  eta: 0:01:37  lr: 0.000003  training_loss: 1.1113 (1.1230)  classification_loss: 1.1113 (1.1230)  loss_mask: 0.0000 (0.0000)  time: 0.1650  data: 0.0003  max mem: 4132
[21:04:04.359948] Epoch: [94]  [220/781]  eta: 0:01:34  lr: 0.000003  training_loss: 1.1234 (1.1269)  classification_loss: 1.1234 (1.1269)  loss_mask: 0.0000 (0.0000)  time: 0.1660  data: 0.0004  max mem: 4132
[21:04:07.682320] Epoch: [94]  [240/781]  eta: 0:01:30  lr: 0.000003  training_loss: 1.1348 (1.1304)  classification_loss: 1.1347 (1.1304)  loss_mask: 0.0000 (0.0000)  time: 0.1660  data: 0.0002  max mem: 4132
[21:04:10.952554] Epoch: [94]  [260/781]  eta: 0:01:27  lr: 0.000003  training_loss: 1.1268 (1.1313)  classification_loss: 1.1267 (1.1313)  loss_mask: 0.0000 (0.0000)  time: 0.1634  data: 0.0002  max mem: 4132
[21:04:14.194484] Epoch: [94]  [280/781]  eta: 0:01:23  lr: 0.000003  training_loss: 1.1206 (1.1298)  classification_loss: 1.1206 (1.1298)  loss_mask: 0.0000 (0.0000)  time: 0.1620  data: 0.0002  max mem: 4132
[21:04:17.433666] Epoch: [94]  [300/781]  eta: 0:01:20  lr: 0.000003  training_loss: 1.1333 (1.1296)  classification_loss: 1.1333 (1.1296)  loss_mask: 0.0000 (0.0000)  time: 0.1619  data: 0.0002  max mem: 4132
[21:04:20.736206] Epoch: [94]  [320/781]  eta: 0:01:16  lr: 0.000003  training_loss: 1.1032 (1.1290)  classification_loss: 1.1032 (1.1290)  loss_mask: 0.0000 (0.0000)  time: 0.1650  data: 0.0003  max mem: 4132
[21:04:24.026091] Epoch: [94]  [340/781]  eta: 0:01:13  lr: 0.000003  training_loss: 1.0472 (1.1262)  classification_loss: 1.0472 (1.1262)  loss_mask: 0.0000 (0.0000)  time: 0.1644  data: 0.0003  max mem: 4132
[21:04:27.418773] Epoch: [94]  [360/781]  eta: 0:01:10  lr: 0.000003  training_loss: 1.1299 (1.1268)  classification_loss: 1.1299 (1.1268)  loss_mask: 0.0000 (0.0000)  time: 0.1695  data: 0.0004  max mem: 4132
[21:04:30.724102] Epoch: [94]  [380/781]  eta: 0:01:06  lr: 0.000003  training_loss: 1.1442 (1.1269)  classification_loss: 1.1442 (1.1269)  loss_mask: 0.0000 (0.0000)  time: 0.1651  data: 0.0003  max mem: 4132
[21:04:34.042344] Epoch: [94]  [400/781]  eta: 0:01:03  lr: 0.000003  training_loss: 1.0896 (1.1263)  classification_loss: 1.0896 (1.1263)  loss_mask: 0.0000 (0.0000)  time: 0.1658  data: 0.0003  max mem: 4132
[21:04:37.381066] Epoch: [94]  [420/781]  eta: 0:01:00  lr: 0.000003  training_loss: 1.0862 (1.1256)  classification_loss: 1.0862 (1.1256)  loss_mask: 0.0000 (0.0000)  time: 0.1668  data: 0.0003  max mem: 4132
[21:04:40.734045] Epoch: [94]  [440/781]  eta: 0:00:56  lr: 0.000003  training_loss: 1.1612 (1.1266)  classification_loss: 1.1612 (1.1266)  loss_mask: 0.0000 (0.0000)  time: 0.1675  data: 0.0004  max mem: 4132
[21:04:44.052008] Epoch: [94]  [460/781]  eta: 0:00:53  lr: 0.000003  training_loss: 1.0896 (1.1253)  classification_loss: 1.0895 (1.1253)  loss_mask: 0.0000 (0.0000)  time: 0.1658  data: 0.0002  max mem: 4132
[21:04:47.357506] Epoch: [94]  [480/781]  eta: 0:00:50  lr: 0.000003  training_loss: 1.0970 (1.1258)  classification_loss: 1.0969 (1.1258)  loss_mask: 0.0000 (0.0000)  time: 0.1652  data: 0.0003  max mem: 4132
[21:04:50.640252] Epoch: [94]  [500/781]  eta: 0:00:46  lr: 0.000003  training_loss: 1.0947 (1.1250)  classification_loss: 1.0947 (1.1250)  loss_mask: 0.0000 (0.0000)  time: 0.1640  data: 0.0002  max mem: 4132
[21:04:53.906280] Epoch: [94]  [520/781]  eta: 0:00:43  lr: 0.000003  training_loss: 1.1493 (1.1258)  classification_loss: 1.1493 (1.1255)  loss_mask: 0.0000 (0.0003)  time: 0.1632  data: 0.0002  max mem: 4132
[21:04:57.167791] Epoch: [94]  [540/781]  eta: 0:00:40  lr: 0.000003  training_loss: 1.1407 (1.1256)  classification_loss: 1.1406 (1.1253)  loss_mask: 0.0002 (0.0003)  time: 0.1630  data: 0.0003  max mem: 4132
[21:05:00.450278] Epoch: [94]  [560/781]  eta: 0:00:36  lr: 0.000003  training_loss: 1.1535 (1.1264)  classification_loss: 1.1535 (1.1261)  loss_mask: 0.0000 (0.0003)  time: 0.1640  data: 0.0002  max mem: 4132
[21:05:03.756033] Epoch: [94]  [580/781]  eta: 0:00:33  lr: 0.000003  training_loss: 1.1431 (1.1278)  classification_loss: 1.1431 (1.1275)  loss_mask: 0.0000 (0.0003)  time: 0.1652  data: 0.0003  max mem: 4132
[21:05:07.078786] Epoch: [94]  [600/781]  eta: 0:00:30  lr: 0.000003  training_loss: 1.0990 (1.1272)  classification_loss: 1.0990 (1.1269)  loss_mask: 0.0000 (0.0003)  time: 0.1660  data: 0.0003  max mem: 4132
[21:05:10.422020] Epoch: [94]  [620/781]  eta: 0:00:26  lr: 0.000003  training_loss: 1.1130 (1.1277)  classification_loss: 1.1130 (1.1274)  loss_mask: 0.0000 (0.0003)  time: 0.1671  data: 0.0003  max mem: 4132
[21:05:13.767735] Epoch: [94]  [640/781]  eta: 0:00:23  lr: 0.000003  training_loss: 1.1475 (1.1286)  classification_loss: 1.1475 (1.1283)  loss_mask: 0.0000 (0.0003)  time: 0.1672  data: 0.0003  max mem: 4132
[21:05:17.073983] Epoch: [94]  [660/781]  eta: 0:00:20  lr: 0.000003  training_loss: 1.1931 (1.1300)  classification_loss: 1.1931 (1.1298)  loss_mask: 0.0000 (0.0002)  time: 0.1652  data: 0.0003  max mem: 4132
[21:05:20.356433] Epoch: [94]  [680/781]  eta: 0:00:16  lr: 0.000003  training_loss: 1.1421 (1.1306)  classification_loss: 1.1421 (1.1303)  loss_mask: 0.0000 (0.0002)  time: 0.1640  data: 0.0003  max mem: 4132
[21:05:23.653833] Epoch: [94]  [700/781]  eta: 0:00:13  lr: 0.000003  training_loss: 1.1809 (1.1315)  classification_loss: 1.1808 (1.1313)  loss_mask: 0.0000 (0.0002)  time: 0.1648  data: 0.0003  max mem: 4132
[21:05:26.954525] Epoch: [94]  [720/781]  eta: 0:00:10  lr: 0.000003  training_loss: 1.1127 (1.1312)  classification_loss: 1.1127 (1.1310)  loss_mask: 0.0000 (0.0002)  time: 0.1649  data: 0.0004  max mem: 4132
[21:05:30.231903] Epoch: [94]  [740/781]  eta: 0:00:06  lr: 0.000003  training_loss: 1.0774 (1.1302)  classification_loss: 1.0774 (1.1299)  loss_mask: 0.0000 (0.0002)  time: 0.1637  data: 0.0004  max mem: 4132
[21:05:33.509846] Epoch: [94]  [760/781]  eta: 0:00:03  lr: 0.000003  training_loss: 1.1815 (1.1308)  classification_loss: 1.1814 (1.1306)  loss_mask: 0.0000 (0.0002)  time: 0.1638  data: 0.0004  max mem: 4132
[21:05:36.754573] Epoch: [94]  [780/781]  eta: 0:00:00  lr: 0.000003  training_loss: 1.0886 (1.1312)  classification_loss: 1.0885 (1.1310)  loss_mask: 0.0000 (0.0002)  time: 0.1622  data: 0.0002  max mem: 4132
[21:05:36.941823] Epoch: [94] Total time: 0:02:09 (0.1662 s / it)
[21:05:36.942314] Averaged stats: lr: 0.000003  training_loss: 1.0886 (1.1312)  classification_loss: 1.0885 (1.1310)  loss_mask: 0.0000 (0.0002)
[21:05:37.648235] Test:  [  0/157]  eta: 0:01:50  testing_loss: 0.4843 (0.4843)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.7018  data: 0.6682  max mem: 4132
[21:05:37.955510] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.4749 (0.4736)  acc1: 85.9375 (85.6534)  acc5: 100.0000 (99.4318)  time: 0.0915  data: 0.0613  max mem: 4132
[21:05:38.248068] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4299 (0.4408)  acc1: 87.5000 (86.8304)  acc5: 100.0000 (99.5536)  time: 0.0297  data: 0.0004  max mem: 4132
[21:05:38.540743] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4093 (0.4592)  acc1: 87.5000 (86.5423)  acc5: 100.0000 (99.2440)  time: 0.0291  data: 0.0003  max mem: 4132
[21:05:38.830779] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4732 (0.4640)  acc1: 87.5000 (86.6616)  acc5: 98.4375 (99.0473)  time: 0.0290  data: 0.0003  max mem: 4132
[21:05:39.120712] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4611 (0.4576)  acc1: 85.9375 (87.0404)  acc5: 98.4375 (99.1115)  time: 0.0289  data: 0.0002  max mem: 4132
[21:05:39.409877] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4117 (0.4521)  acc1: 89.0625 (87.3207)  acc5: 100.0000 (99.1803)  time: 0.0288  data: 0.0002  max mem: 4132
[21:05:39.695663] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4192 (0.4475)  acc1: 87.5000 (87.3900)  acc5: 100.0000 (99.2077)  time: 0.0285  data: 0.0002  max mem: 4132
[21:05:39.979274] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4333 (0.4557)  acc1: 85.9375 (87.0370)  acc5: 98.4375 (99.1319)  time: 0.0283  data: 0.0002  max mem: 4132
[21:05:40.266501] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4436 (0.4520)  acc1: 85.9375 (87.0192)  acc5: 98.4375 (99.1587)  time: 0.0284  data: 0.0002  max mem: 4132
[21:05:40.553934] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.4436 (0.4537)  acc1: 85.9375 (86.8967)  acc5: 100.0000 (99.1646)  time: 0.0286  data: 0.0003  max mem: 4132
[21:05:40.840552] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4527 (0.4543)  acc1: 85.9375 (86.7821)  acc5: 100.0000 (99.1554)  time: 0.0286  data: 0.0003  max mem: 4132
[21:05:41.126844] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4338 (0.4541)  acc1: 85.9375 (86.7252)  acc5: 100.0000 (99.1736)  time: 0.0285  data: 0.0002  max mem: 4132
[21:05:41.411930] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4402 (0.4536)  acc1: 85.9375 (86.7247)  acc5: 100.0000 (99.1889)  time: 0.0284  data: 0.0002  max mem: 4132
[21:05:41.696761] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4229 (0.4498)  acc1: 85.9375 (86.8129)  acc5: 100.0000 (99.2354)  time: 0.0283  data: 0.0002  max mem: 4132
[21:05:41.981916] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4229 (0.4499)  acc1: 84.3750 (86.7446)  acc5: 100.0000 (99.2446)  time: 0.0283  data: 0.0002  max mem: 4132
[21:05:42.132657] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4344 (0.4504)  acc1: 84.3750 (86.6500)  acc5: 100.0000 (99.2300)  time: 0.0273  data: 0.0001  max mem: 4132
[21:05:42.298969] Test: Total time: 0:00:05 (0.0341 s / it)
[21:05:42.299503] * Acc@1 86.650 Acc@5 99.230 loss 0.450
[21:05:42.299834] Accuracy of the network on the 10000 test images: 86.7%
[21:05:42.300031] Max accuracy: 86.85%
[21:05:42.536928] log_dir: ./output_dir
[21:05:43.480167] Epoch: [95]  [  0/781]  eta: 0:12:14  lr: 0.000003  training_loss: 1.0932 (1.0932)  classification_loss: 1.0932 (1.0932)  loss_mask: 0.0000 (0.0000)  time: 0.9408  data: 0.7336  max mem: 4132
[21:05:46.817233] Epoch: [95]  [ 20/781]  eta: 0:02:34  lr: 0.000003  training_loss: 1.1142 (1.1071)  classification_loss: 1.1142 (1.1070)  loss_mask: 0.0000 (0.0000)  time: 0.1667  data: 0.0003  max mem: 4132
[21:05:50.136459] Epoch: [95]  [ 40/781]  eta: 0:02:17  lr: 0.000003  training_loss: 1.1358 (1.1313)  classification_loss: 1.1358 (1.1313)  loss_mask: 0.0000 (0.0000)  time: 0.1658  data: 0.0003  max mem: 4132
[21:05:53.431171] Epoch: [95]  [ 60/781]  eta: 0:02:08  lr: 0.000003  training_loss: 1.1313 (1.1348)  classification_loss: 1.1313 (1.1348)  loss_mask: 0.0000 (0.0000)  time: 0.1646  data: 0.0003  max mem: 4132
[21:05:56.748407] Epoch: [95]  [ 80/781]  eta: 0:02:02  lr: 0.000003  training_loss: 1.1403 (1.1379)  classification_loss: 1.1403 (1.1379)  loss_mask: 0.0000 (0.0000)  time: 0.1658  data: 0.0003  max mem: 4132
[21:06:00.046035] Epoch: [95]  [100/781]  eta: 0:01:57  lr: 0.000003  training_loss: 1.1147 (1.1343)  classification_loss: 1.1147 (1.1343)  loss_mask: 0.0000 (0.0000)  time: 0.1648  data: 0.0003  max mem: 4132
[21:06:03.390774] Epoch: [95]  [120/781]  eta: 0:01:53  lr: 0.000003  training_loss: 1.1161 (1.1307)  classification_loss: 1.1161 (1.1306)  loss_mask: 0.0000 (0.0000)  time: 0.1672  data: 0.0003  max mem: 4132
[21:06:06.719646] Epoch: [95]  [140/781]  eta: 0:01:49  lr: 0.000003  training_loss: 1.1487 (1.1313)  classification_loss: 1.1487 (1.1313)  loss_mask: 0.0000 (0.0000)  time: 0.1663  data: 0.0003  max mem: 4132
[21:06:10.036570] Epoch: [95]  [160/781]  eta: 0:01:45  lr: 0.000003  training_loss: 1.0920 (1.1281)  classification_loss: 1.0920 (1.1281)  loss_mask: 0.0000 (0.0000)  time: 0.1657  data: 0.0003  max mem: 4132
[21:06:13.326428] Epoch: [95]  [180/781]  eta: 0:01:42  lr: 0.000003  training_loss: 1.1324 (1.1279)  classification_loss: 1.1324 (1.1279)  loss_mask: 0.0000 (0.0000)  time: 0.1644  data: 0.0003  max mem: 4132
[21:06:16.613780] Epoch: [95]  [200/781]  eta: 0:01:38  lr: 0.000003  training_loss: 1.1514 (1.1286)  classification_loss: 1.1514 (1.1286)  loss_mask: 0.0000 (0.0000)  time: 0.1643  data: 0.0002  max mem: 4132
[21:06:19.872061] Epoch: [95]  [220/781]  eta: 0:01:34  lr: 0.000003  training_loss: 1.0978 (1.1268)  classification_loss: 1.0978 (1.1267)  loss_mask: 0.0000 (0.0000)  time: 0.1628  data: 0.0002  max mem: 4132
[21:06:23.167827] Epoch: [95]  [240/781]  eta: 0:01:31  lr: 0.000002  training_loss: 1.1287 (1.1300)  classification_loss: 1.1287 (1.1300)  loss_mask: 0.0000 (0.0000)  time: 0.1647  data: 0.0003  max mem: 4132
[21:06:26.472396] Epoch: [95]  [260/781]  eta: 0:01:27  lr: 0.000002  training_loss: 1.1313 (1.1300)  classification_loss: 1.1313 (1.1300)  loss_mask: 0.0000 (0.0000)  time: 0.1651  data: 0.0003  max mem: 4132
[21:06:29.800801] Epoch: [95]  [280/781]  eta: 0:01:24  lr: 0.000002  training_loss: 1.1414 (1.1310)  classification_loss: 1.1414 (1.1309)  loss_mask: 0.0000 (0.0000)  time: 0.1663  data: 0.0003  max mem: 4132
[21:06:33.118137] Epoch: [95]  [300/781]  eta: 0:01:20  lr: 0.000002  training_loss: 1.1349 (1.1317)  classification_loss: 1.1349 (1.1317)  loss_mask: 0.0000 (0.0000)  time: 0.1657  data: 0.0003  max mem: 4132
[21:06:36.415872] Epoch: [95]  [320/781]  eta: 0:01:17  lr: 0.000002  training_loss: 1.1535 (1.1333)  classification_loss: 1.1535 (1.1333)  loss_mask: 0.0000 (0.0000)  time: 0.1648  data: 0.0003  max mem: 4132
[21:06:39.719642] Epoch: [95]  [340/781]  eta: 0:01:13  lr: 0.000002  training_loss: 1.0746 (1.1311)  classification_loss: 1.0746 (1.1311)  loss_mask: 0.0000 (0.0000)  time: 0.1651  data: 0.0003  max mem: 4132
[21:06:43.064348] Epoch: [95]  [360/781]  eta: 0:01:10  lr: 0.000002  training_loss: 1.1377 (1.1328)  classification_loss: 1.1377 (1.1328)  loss_mask: 0.0000 (0.0000)  time: 0.1671  data: 0.0004  max mem: 4132
[21:06:46.388228] Epoch: [95]  [380/781]  eta: 0:01:07  lr: 0.000002  training_loss: 1.1230 (1.1320)  classification_loss: 1.1229 (1.1320)  loss_mask: 0.0000 (0.0000)  time: 0.1661  data: 0.0003  max mem: 4132
[21:06:49.707394] Epoch: [95]  [400/781]  eta: 0:01:03  lr: 0.000002  training_loss: 1.1109 (1.1312)  classification_loss: 1.1109 (1.1312)  loss_mask: 0.0000 (0.0000)  time: 0.1659  data: 0.0002  max mem: 4132
[21:06:52.978740] Epoch: [95]  [420/781]  eta: 0:01:00  lr: 0.000002  training_loss: 1.0749 (1.1299)  classification_loss: 1.0748 (1.1299)  loss_mask: 0.0000 (0.0000)  time: 0.1635  data: 0.0002  max mem: 4132
[21:06:56.250033] Epoch: [95]  [440/781]  eta: 0:00:56  lr: 0.000002  training_loss: 1.1244 (1.1289)  classification_loss: 1.1244 (1.1289)  loss_mask: 0.0000 (0.0000)  time: 0.1635  data: 0.0002  max mem: 4132
[21:06:59.513176] Epoch: [95]  [460/781]  eta: 0:00:53  lr: 0.000002  training_loss: 1.1382 (1.1291)  classification_loss: 1.1382 (1.1291)  loss_mask: 0.0000 (0.0000)  time: 0.1631  data: 0.0002  max mem: 4132
[21:07:02.802708] Epoch: [95]  [480/781]  eta: 0:00:50  lr: 0.000002  training_loss: 1.1357 (1.1294)  classification_loss: 1.1357 (1.1294)  loss_mask: 0.0000 (0.0000)  time: 0.1644  data: 0.0002  max mem: 4132
[21:07:06.097236] Epoch: [95]  [500/781]  eta: 0:00:46  lr: 0.000002  training_loss: 1.0834 (1.1288)  classification_loss: 1.0834 (1.1288)  loss_mask: 0.0000 (0.0000)  time: 0.1646  data: 0.0003  max mem: 4132
[21:07:09.421816] Epoch: [95]  [520/781]  eta: 0:00:43  lr: 0.000002  training_loss: 1.1394 (1.1303)  classification_loss: 1.1394 (1.1303)  loss_mask: 0.0000 (0.0000)  time: 0.1661  data: 0.0003  max mem: 4132
[21:07:12.745139] Epoch: [95]  [540/781]  eta: 0:00:40  lr: 0.000002  training_loss: 1.1468 (1.1307)  classification_loss: 1.1468 (1.1307)  loss_mask: 0.0000 (0.0000)  time: 0.1660  data: 0.0004  max mem: 4132
[21:07:16.069025] Epoch: [95]  [560/781]  eta: 0:00:36  lr: 0.000002  training_loss: 1.1157 (1.1308)  classification_loss: 1.1157 (1.1308)  loss_mask: 0.0000 (0.0000)  time: 0.1661  data: 0.0003  max mem: 4132
[21:07:19.365464] Epoch: [95]  [580/781]  eta: 0:00:33  lr: 0.000002  training_loss: 1.1957 (1.1317)  classification_loss: 1.1957 (1.1317)  loss_mask: 0.0000 (0.0000)  time: 0.1647  data: 0.0003  max mem: 4132
[21:07:22.700509] Epoch: [95]  [600/781]  eta: 0:00:30  lr: 0.000002  training_loss: 1.0929 (1.1308)  classification_loss: 1.0928 (1.1308)  loss_mask: 0.0000 (0.0000)  time: 0.1667  data: 0.0003  max mem: 4132
[21:07:26.069492] Epoch: [95]  [620/781]  eta: 0:00:26  lr: 0.000002  training_loss: 1.1527 (1.1311)  classification_loss: 1.1527 (1.1311)  loss_mask: 0.0000 (0.0000)  time: 0.1683  data: 0.0003  max mem: 4132
[21:07:29.383389] Epoch: [95]  [640/781]  eta: 0:00:23  lr: 0.000002  training_loss: 1.1574 (1.1315)  classification_loss: 1.1574 (1.1315)  loss_mask: 0.0000 (0.0000)  time: 0.1656  data: 0.0003  max mem: 4132
[21:07:32.684023] Epoch: [95]  [660/781]  eta: 0:00:20  lr: 0.000002  training_loss: 1.1420 (1.1320)  classification_loss: 1.1420 (1.1320)  loss_mask: 0.0000 (0.0000)  time: 0.1649  data: 0.0003  max mem: 4132
[21:07:35.959438] Epoch: [95]  [680/781]  eta: 0:00:16  lr: 0.000002  training_loss: 1.1476 (1.1326)  classification_loss: 1.1476 (1.1325)  loss_mask: 0.0000 (0.0000)  time: 0.1637  data: 0.0002  max mem: 4132
[21:07:39.272400] Epoch: [95]  [700/781]  eta: 0:00:13  lr: 0.000002  training_loss: 1.1519 (1.1330)  classification_loss: 1.1518 (1.1329)  loss_mask: 0.0000 (0.0000)  time: 0.1656  data: 0.0002  max mem: 4132
[21:07:42.533202] Epoch: [95]  [720/781]  eta: 0:00:10  lr: 0.000002  training_loss: 1.1236 (1.1326)  classification_loss: 1.1235 (1.1326)  loss_mask: 0.0000 (0.0000)  time: 0.1630  data: 0.0002  max mem: 4132
[21:07:45.843116] Epoch: [95]  [740/781]  eta: 0:00:06  lr: 0.000002  training_loss: 1.1113 (1.1318)  classification_loss: 1.1113 (1.1318)  loss_mask: 0.0000 (0.0000)  time: 0.1654  data: 0.0003  max mem: 4132
[21:07:49.156734] Epoch: [95]  [760/781]  eta: 0:00:03  lr: 0.000002  training_loss: 1.1140 (1.1325)  classification_loss: 1.1140 (1.1324)  loss_mask: 0.0000 (0.0000)  time: 0.1656  data: 0.0003  max mem: 4132
[21:07:52.443408] Epoch: [95]  [780/781]  eta: 0:00:00  lr: 0.000002  training_loss: 1.1450 (1.1329)  classification_loss: 1.1450 (1.1329)  loss_mask: 0.0000 (0.0000)  time: 0.1642  data: 0.0003  max mem: 4132
[21:07:52.648241] Epoch: [95] Total time: 0:02:10 (0.1666 s / it)
[21:07:52.649062] Averaged stats: lr: 0.000002  training_loss: 1.1450 (1.1329)  classification_loss: 1.1450 (1.1329)  loss_mask: 0.0000 (0.0000)
[21:07:53.412290] Test:  [  0/157]  eta: 0:01:58  testing_loss: 0.4797 (0.4797)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.7557  data: 0.7252  max mem: 4132
[21:07:53.714835] Test:  [ 10/157]  eta: 0:00:14  testing_loss: 0.4789 (0.4726)  acc1: 85.9375 (85.6534)  acc5: 100.0000 (99.5739)  time: 0.0960  data: 0.0662  max mem: 4132
[21:07:54.007928] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4198 (0.4379)  acc1: 87.5000 (87.0536)  acc5: 100.0000 (99.6280)  time: 0.0295  data: 0.0003  max mem: 4132
[21:07:54.301702] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.3985 (0.4567)  acc1: 87.5000 (86.9456)  acc5: 100.0000 (99.2944)  time: 0.0291  data: 0.0003  max mem: 4132
[21:07:54.592543] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4637 (0.4613)  acc1: 87.5000 (87.0808)  acc5: 98.4375 (99.0854)  time: 0.0291  data: 0.0003  max mem: 4132
[21:07:54.885659] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4560 (0.4550)  acc1: 87.5000 (87.4081)  acc5: 98.4375 (99.1422)  time: 0.0290  data: 0.0003  max mem: 4132
[21:07:55.176590] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4183 (0.4495)  acc1: 89.0625 (87.5512)  acc5: 100.0000 (99.1803)  time: 0.0290  data: 0.0003  max mem: 4132
[21:07:55.467657] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4183 (0.4455)  acc1: 89.0625 (87.6100)  acc5: 100.0000 (99.2077)  time: 0.0290  data: 0.0003  max mem: 4132
[21:07:55.758832] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4328 (0.4535)  acc1: 85.9375 (87.3071)  acc5: 98.4375 (99.1319)  time: 0.0289  data: 0.0002  max mem: 4132
[21:07:56.053533] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4421 (0.4500)  acc1: 85.9375 (87.2081)  acc5: 98.4375 (99.1587)  time: 0.0291  data: 0.0004  max mem: 4132
[21:07:56.344339] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.4472 (0.4519)  acc1: 85.9375 (87.0359)  acc5: 100.0000 (99.1646)  time: 0.0291  data: 0.0005  max mem: 4132
[21:07:56.630979] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4498 (0.4524)  acc1: 85.9375 (86.9088)  acc5: 100.0000 (99.1554)  time: 0.0287  data: 0.0004  max mem: 4132
[21:07:56.918328] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4347 (0.4519)  acc1: 87.5000 (86.8673)  acc5: 100.0000 (99.1736)  time: 0.0286  data: 0.0002  max mem: 4132
[21:07:57.207279] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4422 (0.4521)  acc1: 85.9375 (86.8321)  acc5: 100.0000 (99.2009)  time: 0.0287  data: 0.0003  max mem: 4132
[21:07:57.495158] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4205 (0.4483)  acc1: 85.9375 (86.9127)  acc5: 100.0000 (99.2354)  time: 0.0287  data: 0.0002  max mem: 4132
[21:07:57.780410] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4205 (0.4483)  acc1: 85.9375 (86.8688)  acc5: 100.0000 (99.2446)  time: 0.0285  data: 0.0002  max mem: 4132
[21:07:57.934706] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4371 (0.4487)  acc1: 85.9375 (86.8000)  acc5: 98.4375 (99.2200)  time: 0.0275  data: 0.0002  max mem: 4132
[21:07:58.097081] Test: Total time: 0:00:05 (0.0347 s / it)
[21:07:58.097780] * Acc@1 86.800 Acc@5 99.220 loss 0.449
[21:07:58.098103] Accuracy of the network on the 10000 test images: 86.8%
[21:07:58.098353] Max accuracy: 86.85%
[21:07:58.558943] log_dir: ./output_dir
[21:07:59.485410] Epoch: [96]  [  0/781]  eta: 0:12:01  lr: 0.000002  training_loss: 1.0320 (1.0320)  classification_loss: 1.0320 (1.0320)  loss_mask: 0.0000 (0.0000)  time: 0.9241  data: 0.7491  max mem: 4132
[21:08:02.817284] Epoch: [96]  [ 20/781]  eta: 0:02:34  lr: 0.000002  training_loss: 1.1302 (1.1337)  classification_loss: 1.1302 (1.1337)  loss_mask: 0.0000 (0.0000)  time: 0.1665  data: 0.0002  max mem: 4132
[21:08:06.177437] Epoch: [96]  [ 40/781]  eta: 0:02:17  lr: 0.000002  training_loss: 1.1241 (1.1384)  classification_loss: 1.1241 (1.1384)  loss_mask: 0.0000 (0.0000)  time: 0.1679  data: 0.0004  max mem: 4132
[21:08:09.472963] Epoch: [96]  [ 60/781]  eta: 0:02:08  lr: 0.000002  training_loss: 1.1306 (1.1446)  classification_loss: 1.1306 (1.1446)  loss_mask: 0.0000 (0.0000)  time: 0.1647  data: 0.0003  max mem: 4132
[21:08:12.776263] Epoch: [96]  [ 80/781]  eta: 0:02:02  lr: 0.000002  training_loss: 1.1287 (1.1449)  classification_loss: 1.1287 (1.1449)  loss_mask: 0.0000 (0.0000)  time: 0.1651  data: 0.0002  max mem: 4132
[21:08:16.086595] Epoch: [96]  [100/781]  eta: 0:01:58  lr: 0.000002  training_loss: 1.0840 (1.1381)  classification_loss: 1.0840 (1.1381)  loss_mask: 0.0000 (0.0000)  time: 0.1654  data: 0.0002  max mem: 4132
[21:08:19.365055] Epoch: [96]  [120/781]  eta: 0:01:53  lr: 0.000002  training_loss: 1.1219 (1.1329)  classification_loss: 1.1219 (1.1329)  loss_mask: 0.0000 (0.0000)  time: 0.1638  data: 0.0002  max mem: 4132
[21:08:22.673800] Epoch: [96]  [140/781]  eta: 0:01:49  lr: 0.000002  training_loss: 1.0843 (1.1290)  classification_loss: 1.0843 (1.1290)  loss_mask: 0.0000 (0.0000)  time: 0.1654  data: 0.0002  max mem: 4132
[21:08:25.935590] Epoch: [96]  [160/781]  eta: 0:01:45  lr: 0.000002  training_loss: 1.1057 (1.1315)  classification_loss: 1.1057 (1.1314)  loss_mask: 0.0000 (0.0000)  time: 0.1630  data: 0.0002  max mem: 4132
[21:08:29.240800] Epoch: [96]  [180/781]  eta: 0:01:41  lr: 0.000002  training_loss: 1.0905 (1.1295)  classification_loss: 1.0905 (1.1295)  loss_mask: 0.0000 (0.0000)  time: 0.1652  data: 0.0004  max mem: 4132
[21:08:32.581407] Epoch: [96]  [200/781]  eta: 0:01:38  lr: 0.000002  training_loss: 1.1652 (1.1327)  classification_loss: 1.1652 (1.1327)  loss_mask: 0.0000 (0.0000)  time: 0.1669  data: 0.0003  max mem: 4132
[21:08:35.905811] Epoch: [96]  [220/781]  eta: 0:01:34  lr: 0.000002  training_loss: 1.1186 (1.1325)  classification_loss: 1.1186 (1.1325)  loss_mask: 0.0000 (0.0000)  time: 0.1661  data: 0.0003  max mem: 4132
[21:08:39.259116] Epoch: [96]  [240/781]  eta: 0:01:31  lr: 0.000002  training_loss: 1.1445 (1.1343)  classification_loss: 1.1445 (1.1343)  loss_mask: 0.0000 (0.0000)  time: 0.1676  data: 0.0003  max mem: 4132
[21:08:42.557553] Epoch: [96]  [260/781]  eta: 0:01:27  lr: 0.000002  training_loss: 1.1407 (1.1356)  classification_loss: 1.1407 (1.1356)  loss_mask: 0.0000 (0.0000)  time: 0.1648  data: 0.0002  max mem: 4132
[21:08:45.864813] Epoch: [96]  [280/781]  eta: 0:01:24  lr: 0.000002  training_loss: 1.1180 (1.1358)  classification_loss: 1.1180 (1.1358)  loss_mask: 0.0000 (0.0000)  time: 0.1653  data: 0.0003  max mem: 4132
[21:08:49.185258] Epoch: [96]  [300/781]  eta: 0:01:20  lr: 0.000002  training_loss: 1.1631 (1.1372)  classification_loss: 1.1631 (1.1372)  loss_mask: 0.0000 (0.0000)  time: 0.1659  data: 0.0002  max mem: 4132
[21:08:52.527223] Epoch: [96]  [320/781]  eta: 0:01:17  lr: 0.000002  training_loss: 1.1383 (1.1375)  classification_loss: 1.1383 (1.1374)  loss_mask: 0.0000 (0.0000)  time: 0.1670  data: 0.0003  max mem: 4132
[21:08:55.846956] Epoch: [96]  [340/781]  eta: 0:01:14  lr: 0.000002  training_loss: 1.1192 (1.1368)  classification_loss: 1.1192 (1.1367)  loss_mask: 0.0000 (0.0000)  time: 0.1658  data: 0.0003  max mem: 4132
[21:08:59.153762] Epoch: [96]  [360/781]  eta: 0:01:10  lr: 0.000002  training_loss: 1.1173 (1.1363)  classification_loss: 1.1173 (1.1363)  loss_mask: 0.0000 (0.0000)  time: 0.1653  data: 0.0002  max mem: 4132
[21:09:02.466407] Epoch: [96]  [380/781]  eta: 0:01:07  lr: 0.000002  training_loss: 1.1140 (1.1362)  classification_loss: 1.1140 (1.1361)  loss_mask: 0.0000 (0.0000)  time: 0.1655  data: 0.0003  max mem: 4132
[21:09:05.779773] Epoch: [96]  [400/781]  eta: 0:01:03  lr: 0.000002  training_loss: 1.1285 (1.1355)  classification_loss: 1.1285 (1.1355)  loss_mask: 0.0000 (0.0000)  time: 0.1656  data: 0.0003  max mem: 4132
[21:09:09.083474] Epoch: [96]  [420/781]  eta: 0:01:00  lr: 0.000002  training_loss: 1.1239 (1.1355)  classification_loss: 1.1239 (1.1355)  loss_mask: 0.0000 (0.0000)  time: 0.1651  data: 0.0003  max mem: 4132
[21:09:12.412404] Epoch: [96]  [440/781]  eta: 0:00:57  lr: 0.000002  training_loss: 1.1227 (1.1349)  classification_loss: 1.1227 (1.1349)  loss_mask: 0.0000 (0.0000)  time: 0.1664  data: 0.0006  max mem: 4132
[21:09:15.788932] Epoch: [96]  [460/781]  eta: 0:00:53  lr: 0.000002  training_loss: 1.1238 (1.1344)  classification_loss: 1.1238 (1.1344)  loss_mask: 0.0000 (0.0000)  time: 0.1684  data: 0.0005  max mem: 4132
[21:09:19.108043] Epoch: [96]  [480/781]  eta: 0:00:50  lr: 0.000002  training_loss: 1.1194 (1.1344)  classification_loss: 1.1194 (1.1344)  loss_mask: 0.0000 (0.0000)  time: 0.1659  data: 0.0003  max mem: 4132
[21:09:22.440625] Epoch: [96]  [500/781]  eta: 0:00:47  lr: 0.000002  training_loss: 1.1537 (1.1349)  classification_loss: 1.1537 (1.1349)  loss_mask: 0.0000 (0.0000)  time: 0.1665  data: 0.0003  max mem: 4132
[21:09:25.766908] Epoch: [96]  [520/781]  eta: 0:00:43  lr: 0.000002  training_loss: 1.1630 (1.1366)  classification_loss: 1.1630 (1.1366)  loss_mask: 0.0000 (0.0000)  time: 0.1662  data: 0.0005  max mem: 4132
[21:09:29.077803] Epoch: [96]  [540/781]  eta: 0:00:40  lr: 0.000002  training_loss: 1.1387 (1.1363)  classification_loss: 1.1387 (1.1363)  loss_mask: 0.0000 (0.0000)  time: 0.1655  data: 0.0003  max mem: 4132
[21:09:32.437961] Epoch: [96]  [560/781]  eta: 0:00:36  lr: 0.000002  training_loss: 1.0890 (1.1350)  classification_loss: 1.0890 (1.1350)  loss_mask: 0.0000 (0.0000)  time: 0.1679  data: 0.0004  max mem: 4132
[21:09:35.747205] Epoch: [96]  [580/781]  eta: 0:00:33  lr: 0.000002  training_loss: 1.1070 (1.1341)  classification_loss: 1.1070 (1.1341)  loss_mask: 0.0000 (0.0000)  time: 0.1654  data: 0.0003  max mem: 4132
[21:09:39.052422] Epoch: [96]  [600/781]  eta: 0:00:30  lr: 0.000002  training_loss: 1.0918 (1.1332)  classification_loss: 1.0918 (1.1331)  loss_mask: 0.0000 (0.0000)  time: 0.1652  data: 0.0002  max mem: 4132
[21:09:42.353502] Epoch: [96]  [620/781]  eta: 0:00:26  lr: 0.000002  training_loss: 1.1130 (1.1337)  classification_loss: 1.1130 (1.1337)  loss_mask: 0.0000 (0.0000)  time: 0.1650  data: 0.0002  max mem: 4132
[21:09:45.639992] Epoch: [96]  [640/781]  eta: 0:00:23  lr: 0.000002  training_loss: 1.1566 (1.1345)  classification_loss: 1.1566 (1.1344)  loss_mask: 0.0000 (0.0000)  time: 0.1642  data: 0.0003  max mem: 4132
[21:09:48.899902] Epoch: [96]  [660/781]  eta: 0:00:20  lr: 0.000002  training_loss: 1.1392 (1.1351)  classification_loss: 1.1392 (1.1351)  loss_mask: 0.0000 (0.0000)  time: 0.1629  data: 0.0002  max mem: 4132
[21:09:52.268209] Epoch: [96]  [680/781]  eta: 0:00:16  lr: 0.000002  training_loss: 1.2003 (1.1367)  classification_loss: 1.2003 (1.1367)  loss_mask: 0.0000 (0.0000)  time: 0.1683  data: 0.0004  max mem: 4132
[21:09:55.597444] Epoch: [96]  [700/781]  eta: 0:00:13  lr: 0.000002  training_loss: 1.1158 (1.1365)  classification_loss: 1.1157 (1.1365)  loss_mask: 0.0000 (0.0000)  time: 0.1664  data: 0.0003  max mem: 4132
[21:09:58.931434] Epoch: [96]  [720/781]  eta: 0:00:10  lr: 0.000002  training_loss: 1.1852 (1.1374)  classification_loss: 1.1852 (1.1374)  loss_mask: 0.0000 (0.0000)  time: 0.1666  data: 0.0004  max mem: 4132
[21:10:02.250980] Epoch: [96]  [740/781]  eta: 0:00:06  lr: 0.000002  training_loss: 1.1107 (1.1366)  classification_loss: 1.1107 (1.1366)  loss_mask: 0.0000 (0.0000)  time: 0.1659  data: 0.0003  max mem: 4132
[21:10:05.538751] Epoch: [96]  [760/781]  eta: 0:00:03  lr: 0.000002  training_loss: 1.1944 (1.1373)  classification_loss: 1.1944 (1.1373)  loss_mask: 0.0000 (0.0000)  time: 0.1643  data: 0.0003  max mem: 4132
[21:10:08.834315] Epoch: [96]  [780/781]  eta: 0:00:00  lr: 0.000002  training_loss: 1.1619 (1.1374)  classification_loss: 1.1619 (1.1374)  loss_mask: 0.0000 (0.0000)  time: 0.1647  data: 0.0003  max mem: 4132
[21:10:09.022861] Epoch: [96] Total time: 0:02:10 (0.1670 s / it)
[21:10:09.023347] Averaged stats: lr: 0.000002  training_loss: 1.1619 (1.1374)  classification_loss: 1.1619 (1.1374)  loss_mask: 0.0000 (0.0000)
[21:10:09.740000] Test:  [  0/157]  eta: 0:01:51  testing_loss: 0.4758 (0.4758)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.7127  data: 0.6826  max mem: 4132
[21:10:10.031860] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.4750 (0.4737)  acc1: 87.5000 (86.2216)  acc5: 100.0000 (99.4318)  time: 0.0912  data: 0.0622  max mem: 4132
[21:10:10.320493] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4160 (0.4377)  acc1: 87.5000 (87.4256)  acc5: 100.0000 (99.6280)  time: 0.0289  data: 0.0002  max mem: 4132
[21:10:10.611885] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4104 (0.4559)  acc1: 87.5000 (87.1472)  acc5: 100.0000 (99.3448)  time: 0.0289  data: 0.0002  max mem: 4132
[21:10:10.909000] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4759 (0.4607)  acc1: 85.9375 (87.0427)  acc5: 98.4375 (99.2378)  time: 0.0293  data: 0.0004  max mem: 4132
[21:10:11.203428] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4536 (0.4543)  acc1: 87.5000 (87.3775)  acc5: 98.4375 (99.2647)  time: 0.0294  data: 0.0004  max mem: 4132
[21:10:11.499441] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4253 (0.4491)  acc1: 89.0625 (87.5512)  acc5: 100.0000 (99.2828)  time: 0.0293  data: 0.0003  max mem: 4132
[21:10:11.795037] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4226 (0.4451)  acc1: 87.5000 (87.5660)  acc5: 100.0000 (99.2958)  time: 0.0294  data: 0.0002  max mem: 4132
[21:10:12.092490] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4311 (0.4531)  acc1: 85.9375 (87.1914)  acc5: 98.4375 (99.2091)  time: 0.0294  data: 0.0002  max mem: 4132
[21:10:12.390157] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4448 (0.4498)  acc1: 85.9375 (87.1223)  acc5: 98.4375 (99.2102)  time: 0.0295  data: 0.0003  max mem: 4132
[21:10:12.682663] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.4448 (0.4518)  acc1: 85.9375 (86.9585)  acc5: 100.0000 (99.2265)  time: 0.0293  data: 0.0003  max mem: 4132
[21:10:12.973962] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4555 (0.4525)  acc1: 84.3750 (86.8525)  acc5: 100.0000 (99.2117)  time: 0.0290  data: 0.0003  max mem: 4132
[21:10:13.267203] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4452 (0.4521)  acc1: 85.9375 (86.8414)  acc5: 100.0000 (99.2252)  time: 0.0290  data: 0.0003  max mem: 4132
[21:10:13.551992] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4452 (0.4521)  acc1: 85.9375 (86.7844)  acc5: 100.0000 (99.2366)  time: 0.0287  data: 0.0002  max mem: 4132
[21:10:13.838567] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4166 (0.4486)  acc1: 85.9375 (86.9127)  acc5: 100.0000 (99.2686)  time: 0.0284  data: 0.0002  max mem: 4132
[21:10:14.124748] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4166 (0.4484)  acc1: 87.5000 (86.8688)  acc5: 100.0000 (99.2860)  time: 0.0285  data: 0.0002  max mem: 4132
[21:10:14.278763] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4339 (0.4489)  acc1: 87.5000 (86.8100)  acc5: 100.0000 (99.2600)  time: 0.0275  data: 0.0002  max mem: 4132
[21:10:14.465822] Test: Total time: 0:00:05 (0.0346 s / it)
[21:10:14.466332] * Acc@1 86.810 Acc@5 99.260 loss 0.449
[21:10:14.466716] Accuracy of the network on the 10000 test images: 86.8%
[21:10:14.466943] Max accuracy: 86.85%
[21:10:14.752644] log_dir: ./output_dir
[21:10:15.690119] Epoch: [97]  [  0/781]  eta: 0:12:10  lr: 0.000002  training_loss: 0.8734 (0.8734)  classification_loss: 0.8734 (0.8734)  loss_mask: 0.0000 (0.0000)  time: 0.9357  data: 0.7626  max mem: 4132
[21:10:18.959882] Epoch: [97]  [ 20/781]  eta: 0:02:32  lr: 0.000002  training_loss: 1.1366 (1.1005)  classification_loss: 1.1365 (1.1004)  loss_mask: 0.0000 (0.0000)  time: 0.1634  data: 0.0002  max mem: 4132
[21:10:22.211070] Epoch: [97]  [ 40/781]  eta: 0:02:14  lr: 0.000002  training_loss: 1.1608 (1.1278)  classification_loss: 1.1607 (1.1278)  loss_mask: 0.0000 (0.0000)  time: 0.1625  data: 0.0002  max mem: 4132
[21:10:25.478408] Epoch: [97]  [ 60/781]  eta: 0:02:06  lr: 0.000002  training_loss: 1.1447 (1.1329)  classification_loss: 1.1447 (1.1329)  loss_mask: 0.0000 (0.0000)  time: 0.1633  data: 0.0002  max mem: 4132
[21:10:28.754265] Epoch: [97]  [ 80/781]  eta: 0:02:01  lr: 0.000002  training_loss: 1.1475 (1.1366)  classification_loss: 1.1475 (1.1366)  loss_mask: 0.0000 (0.0000)  time: 0.1637  data: 0.0002  max mem: 4132
[21:10:32.060038] Epoch: [97]  [100/781]  eta: 0:01:56  lr: 0.000002  training_loss: 1.0634 (1.1313)  classification_loss: 1.0634 (1.1312)  loss_mask: 0.0000 (0.0000)  time: 0.1651  data: 0.0003  max mem: 4132
[21:10:35.393181] Epoch: [97]  [120/781]  eta: 0:01:52  lr: 0.000002  training_loss: 1.0849 (1.1275)  classification_loss: 1.0848 (1.1275)  loss_mask: 0.0000 (0.0000)  time: 0.1666  data: 0.0003  max mem: 4132
[21:10:38.688303] Epoch: [97]  [140/781]  eta: 0:01:48  lr: 0.000002  training_loss: 1.1032 (1.1246)  classification_loss: 1.1032 (1.1246)  loss_mask: 0.0000 (0.0000)  time: 0.1646  data: 0.0004  max mem: 4132
[21:10:41.968610] Epoch: [97]  [160/781]  eta: 0:01:44  lr: 0.000002  training_loss: 1.1296 (1.1234)  classification_loss: 1.1295 (1.1233)  loss_mask: 0.0000 (0.0000)  time: 0.1639  data: 0.0003  max mem: 4132
[21:10:45.257911] Epoch: [97]  [180/781]  eta: 0:01:41  lr: 0.000002  training_loss: 1.1322 (1.1251)  classification_loss: 1.1321 (1.1251)  loss_mask: 0.0000 (0.0000)  time: 0.1644  data: 0.0003  max mem: 4132
[21:10:48.559358] Epoch: [97]  [200/781]  eta: 0:01:37  lr: 0.000002  training_loss: 1.1074 (1.1240)  classification_loss: 1.1074 (1.1239)  loss_mask: 0.0000 (0.0000)  time: 0.1650  data: 0.0003  max mem: 4132
[21:10:51.876648] Epoch: [97]  [220/781]  eta: 0:01:34  lr: 0.000002  training_loss: 1.1752 (1.1259)  classification_loss: 1.1752 (1.1259)  loss_mask: 0.0000 (0.0000)  time: 0.1658  data: 0.0002  max mem: 4132
[21:10:55.163027] Epoch: [97]  [240/781]  eta: 0:01:30  lr: 0.000001  training_loss: 1.0975 (1.1267)  classification_loss: 1.0975 (1.1267)  loss_mask: 0.0000 (0.0000)  time: 0.1642  data: 0.0002  max mem: 4132
[21:10:58.456874] Epoch: [97]  [260/781]  eta: 0:01:27  lr: 0.000001  training_loss: 1.1316 (1.1284)  classification_loss: 1.1316 (1.1283)  loss_mask: 0.0000 (0.0000)  time: 0.1646  data: 0.0002  max mem: 4132
[21:11:01.716508] Epoch: [97]  [280/781]  eta: 0:01:23  lr: 0.000001  training_loss: 1.0945 (1.1275)  classification_loss: 1.0945 (1.1275)  loss_mask: 0.0000 (0.0000)  time: 0.1624  data: 0.0003  max mem: 4132
[21:11:04.976306] Epoch: [97]  [300/781]  eta: 0:01:20  lr: 0.000001  training_loss: 1.1190 (1.1276)  classification_loss: 1.1190 (1.1276)  loss_mask: 0.0000 (0.0000)  time: 0.1629  data: 0.0002  max mem: 4132
[21:11:08.234862] Epoch: [97]  [320/781]  eta: 0:01:16  lr: 0.000001  training_loss: 1.1264 (1.1282)  classification_loss: 1.1264 (1.1282)  loss_mask: 0.0000 (0.0000)  time: 0.1628  data: 0.0002  max mem: 4132
[21:11:11.518833] Epoch: [97]  [340/781]  eta: 0:01:13  lr: 0.000001  training_loss: 1.0763 (1.1254)  classification_loss: 1.0763 (1.1254)  loss_mask: 0.0000 (0.0000)  time: 0.1641  data: 0.0003  max mem: 4132
[21:11:14.845596] Epoch: [97]  [360/781]  eta: 0:01:10  lr: 0.000001  training_loss: 1.0628 (1.1238)  classification_loss: 1.0628 (1.1237)  loss_mask: 0.0000 (0.0000)  time: 0.1662  data: 0.0005  max mem: 4132
[21:11:18.194602] Epoch: [97]  [380/781]  eta: 0:01:06  lr: 0.000001  training_loss: 1.1214 (1.1250)  classification_loss: 1.1214 (1.1249)  loss_mask: 0.0000 (0.0000)  time: 0.1674  data: 0.0003  max mem: 4132
[21:11:21.501785] Epoch: [97]  [400/781]  eta: 0:01:03  lr: 0.000001  training_loss: 1.1086 (1.1243)  classification_loss: 1.1086 (1.1243)  loss_mask: 0.0000 (0.0000)  time: 0.1653  data: 0.0003  max mem: 4132
[21:11:24.841084] Epoch: [97]  [420/781]  eta: 0:01:00  lr: 0.000001  training_loss: 1.1483 (1.1254)  classification_loss: 1.1483 (1.1254)  loss_mask: 0.0000 (0.0000)  time: 0.1669  data: 0.0003  max mem: 4132
[21:11:28.161022] Epoch: [97]  [440/781]  eta: 0:00:56  lr: 0.000001  training_loss: 1.1342 (1.1252)  classification_loss: 1.1342 (1.1251)  loss_mask: 0.0000 (0.0000)  time: 0.1659  data: 0.0002  max mem: 4132
[21:11:31.487868] Epoch: [97]  [460/781]  eta: 0:00:53  lr: 0.000001  training_loss: 1.1129 (1.1239)  classification_loss: 1.1129 (1.1239)  loss_mask: 0.0000 (0.0000)  time: 0.1662  data: 0.0003  max mem: 4132
[21:11:34.792689] Epoch: [97]  [480/781]  eta: 0:00:50  lr: 0.000001  training_loss: 1.1277 (1.1240)  classification_loss: 1.1277 (1.1240)  loss_mask: 0.0000 (0.0000)  time: 0.1651  data: 0.0003  max mem: 4132
[21:11:38.064297] Epoch: [97]  [500/781]  eta: 0:00:46  lr: 0.000001  training_loss: 1.1597 (1.1250)  classification_loss: 1.1597 (1.1250)  loss_mask: 0.0000 (0.0000)  time: 0.1634  data: 0.0002  max mem: 4132
[21:11:41.333675] Epoch: [97]  [520/781]  eta: 0:00:43  lr: 0.000001  training_loss: 1.1000 (1.1249)  classification_loss: 1.1000 (1.1249)  loss_mask: 0.0000 (0.0000)  time: 0.1634  data: 0.0003  max mem: 4132
[21:11:44.615064] Epoch: [97]  [540/781]  eta: 0:00:40  lr: 0.000001  training_loss: 1.0986 (1.1248)  classification_loss: 1.0986 (1.1248)  loss_mask: 0.0000 (0.0000)  time: 0.1640  data: 0.0003  max mem: 4132
[21:11:47.876674] Epoch: [97]  [560/781]  eta: 0:00:36  lr: 0.000001  training_loss: 1.1535 (1.1250)  classification_loss: 1.1535 (1.1249)  loss_mask: 0.0000 (0.0000)  time: 0.1630  data: 0.0003  max mem: 4132
[21:11:51.170841] Epoch: [97]  [580/781]  eta: 0:00:33  lr: 0.000001  training_loss: 1.1003 (1.1258)  classification_loss: 1.1003 (1.1258)  loss_mask: 0.0000 (0.0000)  time: 0.1646  data: 0.0003  max mem: 4132
[21:11:54.508649] Epoch: [97]  [600/781]  eta: 0:00:30  lr: 0.000001  training_loss: 1.1199 (1.1262)  classification_loss: 1.1199 (1.1262)  loss_mask: 0.0000 (0.0000)  time: 0.1668  data: 0.0004  max mem: 4132
[21:11:57.852221] Epoch: [97]  [620/781]  eta: 0:00:26  lr: 0.000001  training_loss: 1.1114 (1.1258)  classification_loss: 1.1114 (1.1257)  loss_mask: 0.0000 (0.0000)  time: 0.1671  data: 0.0003  max mem: 4132
[21:12:01.157882] Epoch: [97]  [640/781]  eta: 0:00:23  lr: 0.000001  training_loss: 1.1047 (1.1262)  classification_loss: 1.1047 (1.1262)  loss_mask: 0.0000 (0.0000)  time: 0.1652  data: 0.0003  max mem: 4132
[21:12:04.461456] Epoch: [97]  [660/781]  eta: 0:00:20  lr: 0.000001  training_loss: 1.1299 (1.1267)  classification_loss: 1.1299 (1.1267)  loss_mask: 0.0000 (0.0000)  time: 0.1651  data: 0.0003  max mem: 4132
[21:12:07.784272] Epoch: [97]  [680/781]  eta: 0:00:16  lr: 0.000001  training_loss: 1.1517 (1.1272)  classification_loss: 1.1517 (1.1271)  loss_mask: 0.0000 (0.0000)  time: 0.1660  data: 0.0003  max mem: 4132
[21:12:11.111635] Epoch: [97]  [700/781]  eta: 0:00:13  lr: 0.000001  training_loss: 1.1548 (1.1279)  classification_loss: 1.1548 (1.1279)  loss_mask: 0.0000 (0.0000)  time: 0.1663  data: 0.0003  max mem: 4132
[21:12:14.429507] Epoch: [97]  [720/781]  eta: 0:00:10  lr: 0.000001  training_loss: 1.1405 (1.1286)  classification_loss: 1.1405 (1.1286)  loss_mask: 0.0000 (0.0000)  time: 0.1658  data: 0.0003  max mem: 4132
[21:12:17.699304] Epoch: [97]  [740/781]  eta: 0:00:06  lr: 0.000001  training_loss: 1.1045 (1.1289)  classification_loss: 1.1045 (1.1289)  loss_mask: 0.0000 (0.0000)  time: 0.1634  data: 0.0003  max mem: 4132
[21:12:20.965178] Epoch: [97]  [760/781]  eta: 0:00:03  lr: 0.000001  training_loss: 1.1254 (1.1295)  classification_loss: 1.1254 (1.1294)  loss_mask: 0.0000 (0.0000)  time: 0.1632  data: 0.0002  max mem: 4132
[21:12:24.219876] Epoch: [97]  [780/781]  eta: 0:00:00  lr: 0.000001  training_loss: 1.1733 (1.1300)  classification_loss: 1.1732 (1.1300)  loss_mask: 0.0000 (0.0000)  time: 0.1627  data: 0.0002  max mem: 4132
[21:12:24.389909] Epoch: [97] Total time: 0:02:09 (0.1660 s / it)
[21:12:24.390337] Averaged stats: lr: 0.000001  training_loss: 1.1733 (1.1300)  classification_loss: 1.1732 (1.1300)  loss_mask: 0.0000 (0.0000)
[21:12:25.062522] Test:  [  0/157]  eta: 0:01:44  testing_loss: 0.4865 (0.4865)  acc1: 85.9375 (85.9375)  acc5: 96.8750 (96.8750)  time: 0.6684  data: 0.6310  max mem: 4132
[21:12:25.360798] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4728 (0.4717)  acc1: 85.9375 (85.9375)  acc5: 100.0000 (99.4318)  time: 0.0876  data: 0.0576  max mem: 4132
[21:12:25.651292] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4174 (0.4370)  acc1: 87.5000 (87.2768)  acc5: 100.0000 (99.6280)  time: 0.0292  data: 0.0003  max mem: 4132
[21:12:25.941996] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4108 (0.4556)  acc1: 87.5000 (86.9456)  acc5: 100.0000 (99.2944)  time: 0.0289  data: 0.0002  max mem: 4132
[21:12:26.227320] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4641 (0.4603)  acc1: 85.9375 (87.0046)  acc5: 98.4375 (99.1997)  time: 0.0286  data: 0.0002  max mem: 4132
[21:12:26.519549] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4538 (0.4537)  acc1: 85.9375 (87.2855)  acc5: 98.4375 (99.2341)  time: 0.0287  data: 0.0002  max mem: 4132
[21:12:26.804694] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4214 (0.4490)  acc1: 87.5000 (87.3975)  acc5: 100.0000 (99.2828)  time: 0.0287  data: 0.0002  max mem: 4132
[21:12:27.089575] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4128 (0.4450)  acc1: 87.5000 (87.4120)  acc5: 100.0000 (99.3178)  time: 0.0284  data: 0.0002  max mem: 4132
[21:12:27.372958] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4377 (0.4528)  acc1: 85.9375 (87.0177)  acc5: 98.4375 (99.2284)  time: 0.0283  data: 0.0002  max mem: 4132
[21:12:27.655580] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4506 (0.4497)  acc1: 85.9375 (86.9677)  acc5: 98.4375 (99.2445)  time: 0.0282  data: 0.0002  max mem: 4132
[21:12:27.939516] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4467 (0.4514)  acc1: 85.9375 (86.8038)  acc5: 100.0000 (99.2574)  time: 0.0282  data: 0.0002  max mem: 4132
[21:12:28.223657] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4467 (0.4520)  acc1: 84.3750 (86.6836)  acc5: 100.0000 (99.2399)  time: 0.0283  data: 0.0002  max mem: 4132
[21:12:28.508172] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4347 (0.4520)  acc1: 87.5000 (86.6477)  acc5: 100.0000 (99.2510)  time: 0.0283  data: 0.0002  max mem: 4132
[21:12:28.792571] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4406 (0.4518)  acc1: 85.9375 (86.6174)  acc5: 100.0000 (99.2724)  time: 0.0283  data: 0.0002  max mem: 4132
[21:12:29.075738] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4204 (0.4480)  acc1: 85.9375 (86.7465)  acc5: 100.0000 (99.3129)  time: 0.0282  data: 0.0002  max mem: 4132
[21:12:29.357763] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4204 (0.4480)  acc1: 85.9375 (86.7136)  acc5: 100.0000 (99.3377)  time: 0.0281  data: 0.0001  max mem: 4132
[21:12:29.508117] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4297 (0.4486)  acc1: 85.9375 (86.6600)  acc5: 100.0000 (99.3100)  time: 0.0271  data: 0.0001  max mem: 4132
[21:12:29.680621] Test: Total time: 0:00:05 (0.0337 s / it)
[21:12:29.681493] * Acc@1 86.660 Acc@5 99.310 loss 0.449
[21:12:29.681966] Accuracy of the network on the 10000 test images: 86.7%
[21:12:29.682152] Max accuracy: 86.85%
[21:12:29.851211] log_dir: ./output_dir
[21:12:30.798553] Epoch: [98]  [  0/781]  eta: 0:12:18  lr: 0.000001  training_loss: 1.0668 (1.0668)  classification_loss: 1.0667 (1.0667)  loss_mask: 0.0000 (0.0000)  time: 0.9453  data: 0.7353  max mem: 4132
[21:12:34.107218] Epoch: [98]  [ 20/781]  eta: 0:02:34  lr: 0.000001  training_loss: 1.1069 (1.1160)  classification_loss: 1.1069 (1.1159)  loss_mask: 0.0000 (0.0000)  time: 0.1653  data: 0.0002  max mem: 4132
[21:12:37.425929] Epoch: [98]  [ 40/781]  eta: 0:02:16  lr: 0.000001  training_loss: 1.1429 (1.1247)  classification_loss: 1.1429 (1.1246)  loss_mask: 0.0000 (0.0000)  time: 0.1658  data: 0.0003  max mem: 4132
[21:12:40.757104] Epoch: [98]  [ 60/781]  eta: 0:02:08  lr: 0.000001  training_loss: 1.1112 (1.1303)  classification_loss: 1.1112 (1.1303)  loss_mask: 0.0000 (0.0000)  time: 0.1664  data: 0.0003  max mem: 4132
[21:12:44.084549] Epoch: [98]  [ 80/781]  eta: 0:02:03  lr: 0.000001  training_loss: 1.1019 (1.1256)  classification_loss: 1.1019 (1.1255)  loss_mask: 0.0000 (0.0000)  time: 0.1663  data: 0.0004  max mem: 4132
[21:12:47.428433] Epoch: [98]  [100/781]  eta: 0:01:58  lr: 0.000001  training_loss: 1.1424 (1.1289)  classification_loss: 1.1424 (1.1289)  loss_mask: 0.0000 (0.0000)  time: 0.1671  data: 0.0003  max mem: 4132
[21:12:50.757633] Epoch: [98]  [120/781]  eta: 0:01:54  lr: 0.000001  training_loss: 1.1142 (1.1266)  classification_loss: 1.1142 (1.1266)  loss_mask: 0.0000 (0.0000)  time: 0.1663  data: 0.0003  max mem: 4132
[21:12:54.106851] Epoch: [98]  [140/781]  eta: 0:01:50  lr: 0.000001  training_loss: 1.1406 (1.1280)  classification_loss: 1.1405 (1.1280)  loss_mask: 0.0000 (0.0000)  time: 0.1674  data: 0.0003  max mem: 4132
[21:12:57.431450] Epoch: [98]  [160/781]  eta: 0:01:46  lr: 0.000001  training_loss: 1.1363 (1.1280)  classification_loss: 1.1363 (1.1280)  loss_mask: 0.0000 (0.0000)  time: 0.1661  data: 0.0003  max mem: 4132
[21:13:00.699031] Epoch: [98]  [180/781]  eta: 0:01:42  lr: 0.000001  training_loss: 1.1276 (1.1296)  classification_loss: 1.1276 (1.1295)  loss_mask: 0.0000 (0.0000)  time: 0.1633  data: 0.0002  max mem: 4132
[21:13:03.978232] Epoch: [98]  [200/781]  eta: 0:01:38  lr: 0.000001  training_loss: 1.1092 (1.1308)  classification_loss: 1.1092 (1.1308)  loss_mask: 0.0000 (0.0000)  time: 0.1639  data: 0.0002  max mem: 4132
[21:13:07.248915] Epoch: [98]  [220/781]  eta: 0:01:34  lr: 0.000001  training_loss: 1.1431 (1.1306)  classification_loss: 1.1431 (1.1306)  loss_mask: 0.0000 (0.0000)  time: 0.1635  data: 0.0002  max mem: 4132
[21:13:10.502958] Epoch: [98]  [240/781]  eta: 0:01:31  lr: 0.000001  training_loss: 1.1725 (1.1331)  classification_loss: 1.1725 (1.1331)  loss_mask: 0.0000 (0.0000)  time: 0.1626  data: 0.0002  max mem: 4132
[21:13:13.812226] Epoch: [98]  [260/781]  eta: 0:01:27  lr: 0.000001  training_loss: 1.1727 (1.1345)  classification_loss: 1.1727 (1.1345)  loss_mask: 0.0000 (0.0000)  time: 0.1654  data: 0.0003  max mem: 4132
[21:13:17.144437] Epoch: [98]  [280/781]  eta: 0:01:24  lr: 0.000001  training_loss: 1.1000 (1.1345)  classification_loss: 1.1000 (1.1344)  loss_mask: 0.0000 (0.0000)  time: 0.1665  data: 0.0003  max mem: 4132
[21:13:20.452752] Epoch: [98]  [300/781]  eta: 0:01:20  lr: 0.000001  training_loss: 1.1245 (1.1344)  classification_loss: 1.1245 (1.1344)  loss_mask: 0.0000 (0.0000)  time: 0.1653  data: 0.0003  max mem: 4132
[21:13:23.808947] Epoch: [98]  [320/781]  eta: 0:01:17  lr: 0.000001  training_loss: 1.1399 (1.1342)  classification_loss: 1.1399 (1.1342)  loss_mask: 0.0000 (0.0000)  time: 0.1677  data: 0.0003  max mem: 4132
[21:13:27.123018] Epoch: [98]  [340/781]  eta: 0:01:14  lr: 0.000001  training_loss: 1.1171 (1.1337)  classification_loss: 1.1171 (1.1337)  loss_mask: 0.0000 (0.0000)  time: 0.1656  data: 0.0003  max mem: 4132
[21:13:30.422063] Epoch: [98]  [360/781]  eta: 0:01:10  lr: 0.000001  training_loss: 1.1405 (1.1347)  classification_loss: 1.1405 (1.1346)  loss_mask: 0.0000 (0.0000)  time: 0.1648  data: 0.0006  max mem: 4132
[21:13:33.745244] Epoch: [98]  [380/781]  eta: 0:01:07  lr: 0.000001  training_loss: 1.1046 (1.1345)  classification_loss: 1.1046 (1.1344)  loss_mask: 0.0000 (0.0000)  time: 0.1661  data: 0.0003  max mem: 4132
[21:13:37.074101] Epoch: [98]  [400/781]  eta: 0:01:03  lr: 0.000001  training_loss: 1.1726 (1.1366)  classification_loss: 1.1726 (1.1366)  loss_mask: 0.0000 (0.0000)  time: 0.1663  data: 0.0003  max mem: 4132
[21:13:40.347078] Epoch: [98]  [420/781]  eta: 0:01:00  lr: 0.000001  training_loss: 1.1417 (1.1370)  classification_loss: 1.1417 (1.1370)  loss_mask: 0.0000 (0.0000)  time: 0.1636  data: 0.0003  max mem: 4132
[21:13:43.622538] Epoch: [98]  [440/781]  eta: 0:00:57  lr: 0.000001  training_loss: 1.0969 (1.1356)  classification_loss: 1.0969 (1.1356)  loss_mask: 0.0000 (0.0000)  time: 0.1637  data: 0.0002  max mem: 4132
[21:13:46.902822] Epoch: [98]  [460/781]  eta: 0:00:53  lr: 0.000001  training_loss: 1.1183 (1.1352)  classification_loss: 1.1183 (1.1351)  loss_mask: 0.0000 (0.0000)  time: 0.1639  data: 0.0002  max mem: 4132
[21:13:50.192071] Epoch: [98]  [480/781]  eta: 0:00:50  lr: 0.000001  training_loss: 1.1430 (1.1361)  classification_loss: 1.1430 (1.1361)  loss_mask: 0.0000 (0.0000)  time: 0.1644  data: 0.0003  max mem: 4132
[21:13:53.455716] Epoch: [98]  [500/781]  eta: 0:00:46  lr: 0.000001  training_loss: 1.1062 (1.1357)  classification_loss: 1.1062 (1.1357)  loss_mask: 0.0000 (0.0000)  time: 0.1631  data: 0.0003  max mem: 4132
[21:13:56.759218] Epoch: [98]  [520/781]  eta: 0:00:43  lr: 0.000001  training_loss: 1.1564 (1.1354)  classification_loss: 1.1563 (1.1354)  loss_mask: 0.0000 (0.0000)  time: 0.1651  data: 0.0003  max mem: 4132
[21:14:00.058891] Epoch: [98]  [540/781]  eta: 0:00:40  lr: 0.000001  training_loss: 1.1488 (1.1364)  classification_loss: 1.1487 (1.1364)  loss_mask: 0.0000 (0.0000)  time: 0.1649  data: 0.0003  max mem: 4132
[21:14:03.387595] Epoch: [98]  [560/781]  eta: 0:00:36  lr: 0.000001  training_loss: 1.1454 (1.1365)  classification_loss: 1.1454 (1.1365)  loss_mask: 0.0000 (0.0000)  time: 0.1663  data: 0.0004  max mem: 4132
[21:14:06.723501] Epoch: [98]  [580/781]  eta: 0:00:33  lr: 0.000001  training_loss: 1.0961 (1.1365)  classification_loss: 1.0961 (1.1365)  loss_mask: 0.0000 (0.0000)  time: 0.1667  data: 0.0003  max mem: 4132
[21:14:10.025850] Epoch: [98]  [600/781]  eta: 0:00:30  lr: 0.000001  training_loss: 1.1010 (1.1354)  classification_loss: 1.1010 (1.1354)  loss_mask: 0.0000 (0.0000)  time: 0.1650  data: 0.0003  max mem: 4132
[21:14:13.361158] Epoch: [98]  [620/781]  eta: 0:00:26  lr: 0.000001  training_loss: 1.1224 (1.1351)  classification_loss: 1.1223 (1.1351)  loss_mask: 0.0000 (0.0000)  time: 0.1667  data: 0.0003  max mem: 4132
[21:14:16.675113] Epoch: [98]  [640/781]  eta: 0:00:23  lr: 0.000001  training_loss: 1.1150 (1.1351)  classification_loss: 1.1150 (1.1351)  loss_mask: 0.0000 (0.0000)  time: 0.1656  data: 0.0004  max mem: 4132
[21:14:19.972140] Epoch: [98]  [660/781]  eta: 0:00:20  lr: 0.000001  training_loss: 1.1257 (1.1349)  classification_loss: 1.1257 (1.1349)  loss_mask: 0.0000 (0.0000)  time: 0.1648  data: 0.0004  max mem: 4132
[21:14:23.240523] Epoch: [98]  [680/781]  eta: 0:00:16  lr: 0.000001  training_loss: 1.1418 (1.1355)  classification_loss: 1.1418 (1.1355)  loss_mask: 0.0000 (0.0000)  time: 0.1633  data: 0.0002  max mem: 4132
[21:14:26.517826] Epoch: [98]  [700/781]  eta: 0:00:13  lr: 0.000001  training_loss: 1.1308 (1.1354)  classification_loss: 1.1308 (1.1354)  loss_mask: 0.0000 (0.0000)  time: 0.1638  data: 0.0003  max mem: 4132
[21:14:29.817977] Epoch: [98]  [720/781]  eta: 0:00:10  lr: 0.000001  training_loss: 1.1690 (1.1362)  classification_loss: 1.1690 (1.1362)  loss_mask: 0.0000 (0.0000)  time: 0.1649  data: 0.0004  max mem: 4132
[21:14:33.075497] Epoch: [98]  [740/781]  eta: 0:00:06  lr: 0.000001  training_loss: 1.0841 (1.1355)  classification_loss: 1.0841 (1.1355)  loss_mask: 0.0000 (0.0000)  time: 0.1628  data: 0.0002  max mem: 4132
[21:14:36.378664] Epoch: [98]  [760/781]  eta: 0:00:03  lr: 0.000001  training_loss: 1.1447 (1.1363)  classification_loss: 1.1447 (1.1363)  loss_mask: 0.0000 (0.0000)  time: 0.1651  data: 0.0003  max mem: 4132
[21:14:39.678058] Epoch: [98]  [780/781]  eta: 0:00:00  lr: 0.000001  training_loss: 1.1648 (1.1363)  classification_loss: 1.1648 (1.1363)  loss_mask: 0.0000 (0.0000)  time: 0.1649  data: 0.0003  max mem: 4132
[21:14:39.883329] Epoch: [98] Total time: 0:02:10 (0.1665 s / it)
[21:14:39.883866] Averaged stats: lr: 0.000001  training_loss: 1.1648 (1.1363)  classification_loss: 1.1648 (1.1363)  loss_mask: 0.0000 (0.0000)
[21:14:40.609150] Test:  [  0/157]  eta: 0:01:53  testing_loss: 0.4869 (0.4869)  acc1: 85.9375 (85.9375)  acc5: 96.8750 (96.8750)  time: 0.7208  data: 0.6890  max mem: 4132
[21:14:40.900878] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.4744 (0.4706)  acc1: 87.5000 (86.0795)  acc5: 100.0000 (99.4318)  time: 0.0919  data: 0.0628  max mem: 4132
[21:14:41.189259] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4156 (0.4359)  acc1: 87.5000 (87.4256)  acc5: 100.0000 (99.5536)  time: 0.0288  data: 0.0002  max mem: 4132
[21:14:41.481278] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4066 (0.4548)  acc1: 87.5000 (87.1976)  acc5: 100.0000 (99.2440)  time: 0.0288  data: 0.0002  max mem: 4132
[21:14:41.779810] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4726 (0.4596)  acc1: 87.5000 (87.1570)  acc5: 98.4375 (99.1997)  time: 0.0293  data: 0.0002  max mem: 4132
[21:14:42.072293] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4604 (0.4535)  acc1: 87.5000 (87.4387)  acc5: 98.4375 (99.2341)  time: 0.0294  data: 0.0002  max mem: 4132
[21:14:42.370076] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4231 (0.4486)  acc1: 87.5000 (87.6025)  acc5: 100.0000 (99.2828)  time: 0.0293  data: 0.0003  max mem: 4132
[21:14:42.669856] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4125 (0.4446)  acc1: 87.5000 (87.6320)  acc5: 100.0000 (99.3178)  time: 0.0297  data: 0.0003  max mem: 4132
[21:14:42.959905] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4369 (0.4528)  acc1: 85.9375 (87.2106)  acc5: 98.4375 (99.2284)  time: 0.0293  data: 0.0003  max mem: 4132
[21:14:43.251128] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4457 (0.4496)  acc1: 85.9375 (87.1566)  acc5: 98.4375 (99.2273)  time: 0.0288  data: 0.0003  max mem: 4132
[21:14:43.540573] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.4425 (0.4515)  acc1: 85.9375 (86.9895)  acc5: 100.0000 (99.2420)  time: 0.0288  data: 0.0003  max mem: 4132
[21:14:43.832850] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4425 (0.4520)  acc1: 84.3750 (86.8806)  acc5: 100.0000 (99.2258)  time: 0.0289  data: 0.0003  max mem: 4132
[21:14:44.129025] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4354 (0.4516)  acc1: 87.5000 (86.8543)  acc5: 100.0000 (99.2381)  time: 0.0293  data: 0.0003  max mem: 4132
[21:14:44.420770] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4378 (0.4513)  acc1: 85.9375 (86.8082)  acc5: 100.0000 (99.2605)  time: 0.0292  data: 0.0003  max mem: 4132
[21:14:44.708564] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4118 (0.4475)  acc1: 87.5000 (86.9459)  acc5: 100.0000 (99.3019)  time: 0.0288  data: 0.0002  max mem: 4132
[21:14:44.994554] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4118 (0.4474)  acc1: 85.9375 (86.8895)  acc5: 100.0000 (99.3171)  time: 0.0285  data: 0.0002  max mem: 4132
[21:14:45.150138] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4301 (0.4477)  acc1: 85.9375 (86.8400)  acc5: 100.0000 (99.3000)  time: 0.0276  data: 0.0002  max mem: 4132
[21:14:45.317142] Test: Total time: 0:00:05 (0.0346 s / it)
[21:14:45.317731] * Acc@1 86.840 Acc@5 99.300 loss 0.448
[21:14:45.318040] Accuracy of the network on the 10000 test images: 86.8%
[21:14:45.318227] Max accuracy: 86.85%
[21:14:45.533283] log_dir: ./output_dir
[21:14:46.518590] Epoch: [99]  [  0/781]  eta: 0:12:48  lr: 0.000001  training_loss: 0.8574 (0.8574)  classification_loss: 0.8574 (0.8574)  loss_mask: 0.0000 (0.0000)  time: 0.9834  data: 0.7947  max mem: 4132
[21:14:49.818566] Epoch: [99]  [ 20/781]  eta: 0:02:35  lr: 0.000001  training_loss: 1.1013 (1.0988)  classification_loss: 1.1012 (1.0988)  loss_mask: 0.0000 (0.0000)  time: 0.1649  data: 0.0002  max mem: 4132
[21:14:53.173274] Epoch: [99]  [ 40/781]  eta: 0:02:17  lr: 0.000001  training_loss: 1.1000 (1.1101)  classification_loss: 1.1000 (1.1101)  loss_mask: 0.0000 (0.0000)  time: 0.1676  data: 0.0003  max mem: 4132
[21:14:56.506664] Epoch: [99]  [ 60/781]  eta: 0:02:09  lr: 0.000001  training_loss: 1.1102 (1.1221)  classification_loss: 1.1102 (1.1221)  loss_mask: 0.0000 (0.0000)  time: 0.1666  data: 0.0003  max mem: 4132
[21:14:59.842784] Epoch: [99]  [ 80/781]  eta: 0:02:03  lr: 0.000001  training_loss: 1.0951 (1.1155)  classification_loss: 1.0951 (1.1155)  loss_mask: 0.0000 (0.0000)  time: 0.1667  data: 0.0003  max mem: 4132
[21:15:03.124035] Epoch: [99]  [100/781]  eta: 0:01:58  lr: 0.000001  training_loss: 1.1155 (1.1226)  classification_loss: 1.1155 (1.1226)  loss_mask: 0.0000 (0.0000)  time: 0.1640  data: 0.0002  max mem: 4132
[21:15:06.375865] Epoch: [99]  [120/781]  eta: 0:01:53  lr: 0.000001  training_loss: 1.1572 (1.1290)  classification_loss: 1.1572 (1.1289)  loss_mask: 0.0000 (0.0000)  time: 0.1625  data: 0.0002  max mem: 4132
[21:15:09.634428] Epoch: [99]  [140/781]  eta: 0:01:49  lr: 0.000001  training_loss: 1.1158 (1.1298)  classification_loss: 1.1158 (1.1298)  loss_mask: 0.0000 (0.0000)  time: 0.1628  data: 0.0002  max mem: 4132
[21:15:12.876403] Epoch: [99]  [160/781]  eta: 0:01:45  lr: 0.000001  training_loss: 1.0636 (1.1259)  classification_loss: 1.0636 (1.1259)  loss_mask: 0.0000 (0.0000)  time: 0.1620  data: 0.0002  max mem: 4132
[21:15:16.197230] Epoch: [99]  [180/781]  eta: 0:01:41  lr: 0.000001  training_loss: 1.1075 (1.1274)  classification_loss: 1.1075 (1.1274)  loss_mask: 0.0000 (0.0000)  time: 0.1659  data: 0.0002  max mem: 4132
[21:15:19.549394] Epoch: [99]  [200/781]  eta: 0:01:38  lr: 0.000001  training_loss: 1.1305 (1.1277)  classification_loss: 1.1305 (1.1277)  loss_mask: 0.0000 (0.0000)  time: 0.1675  data: 0.0004  max mem: 4132
[21:15:22.889915] Epoch: [99]  [220/781]  eta: 0:01:34  lr: 0.000001  training_loss: 1.1151 (1.1270)  classification_loss: 1.1151 (1.1270)  loss_mask: 0.0000 (0.0000)  time: 0.1669  data: 0.0003  max mem: 4132
[21:15:26.207373] Epoch: [99]  [240/781]  eta: 0:01:31  lr: 0.000001  training_loss: 1.1453 (1.1316)  classification_loss: 1.1453 (1.1316)  loss_mask: 0.0000 (0.0000)  time: 0.1658  data: 0.0006  max mem: 4132
[21:15:29.531582] Epoch: [99]  [260/781]  eta: 0:01:27  lr: 0.000001  training_loss: 1.1310 (1.1304)  classification_loss: 1.1310 (1.1304)  loss_mask: 0.0000 (0.0000)  time: 0.1661  data: 0.0004  max mem: 4132
[21:15:32.828552] Epoch: [99]  [280/781]  eta: 0:01:24  lr: 0.000001  training_loss: 1.0993 (1.1288)  classification_loss: 1.0993 (1.1287)  loss_mask: 0.0000 (0.0000)  time: 0.1648  data: 0.0004  max mem: 4132
[21:15:36.133550] Epoch: [99]  [300/781]  eta: 0:01:20  lr: 0.000001  training_loss: 1.1255 (1.1273)  classification_loss: 1.1188 (1.1269)  loss_mask: 0.0000 (0.0004)  time: 0.1652  data: 0.0003  max mem: 4132
[21:15:39.431822] Epoch: [99]  [320/781]  eta: 0:01:17  lr: 0.000001  training_loss: 1.1576 (1.1302)  classification_loss: 1.1576 (1.1298)  loss_mask: 0.0000 (0.0004)  time: 0.1648  data: 0.0003  max mem: 4132
[21:15:42.712424] Epoch: [99]  [340/781]  eta: 0:01:13  lr: 0.000001  training_loss: 1.0937 (1.1282)  classification_loss: 1.0937 (1.1278)  loss_mask: 0.0000 (0.0004)  time: 0.1639  data: 0.0003  max mem: 4132
[21:15:45.976417] Epoch: [99]  [360/781]  eta: 0:01:10  lr: 0.000001  training_loss: 1.1724 (1.1303)  classification_loss: 1.1724 (1.1300)  loss_mask: 0.0000 (0.0004)  time: 0.1631  data: 0.0004  max mem: 4132
[21:15:49.238579] Epoch: [99]  [380/781]  eta: 0:01:07  lr: 0.000001  training_loss: 1.1260 (1.1315)  classification_loss: 1.1260 (1.1312)  loss_mask: 0.0000 (0.0003)  time: 0.1630  data: 0.0003  max mem: 4132
[21:15:52.509947] Epoch: [99]  [400/781]  eta: 0:01:03  lr: 0.000001  training_loss: 1.1179 (1.1315)  classification_loss: 1.1179 (1.1312)  loss_mask: 0.0000 (0.0003)  time: 0.1635  data: 0.0002  max mem: 4132
[21:15:55.832913] Epoch: [99]  [420/781]  eta: 0:01:00  lr: 0.000001  training_loss: 1.1520 (1.1320)  classification_loss: 1.1519 (1.1317)  loss_mask: 0.0000 (0.0003)  time: 0.1660  data: 0.0003  max mem: 4132
[21:15:59.152582] Epoch: [99]  [440/781]  eta: 0:00:56  lr: 0.000001  training_loss: 1.1184 (1.1312)  classification_loss: 1.1184 (1.1309)  loss_mask: 0.0000 (0.0003)  time: 0.1659  data: 0.0003  max mem: 4132
[21:16:02.461441] Epoch: [99]  [460/781]  eta: 0:00:53  lr: 0.000001  training_loss: 1.1084 (1.1304)  classification_loss: 1.1084 (1.1301)  loss_mask: 0.0000 (0.0003)  time: 0.1653  data: 0.0003  max mem: 4132
[21:16:05.772614] Epoch: [99]  [480/781]  eta: 0:00:50  lr: 0.000001  training_loss: 1.1483 (1.1311)  classification_loss: 1.1483 (1.1308)  loss_mask: 0.0000 (0.0003)  time: 0.1655  data: 0.0004  max mem: 4132
[21:16:09.111172] Epoch: [99]  [500/781]  eta: 0:00:46  lr: 0.000001  training_loss: 1.0972 (1.1305)  classification_loss: 1.0972 (1.1302)  loss_mask: 0.0000 (0.0003)  time: 0.1669  data: 0.0005  max mem: 4132
[21:16:12.417310] Epoch: [99]  [520/781]  eta: 0:00:43  lr: 0.000001  training_loss: 1.1513 (1.1315)  classification_loss: 1.1513 (1.1312)  loss_mask: 0.0000 (0.0002)  time: 0.1652  data: 0.0003  max mem: 4132
[21:16:15.717291] Epoch: [99]  [540/781]  eta: 0:00:40  lr: 0.000001  training_loss: 1.1040 (1.1306)  classification_loss: 1.1040 (1.1304)  loss_mask: 0.0000 (0.0002)  time: 0.1649  data: 0.0003  max mem: 4132
[21:16:19.066978] Epoch: [99]  [560/781]  eta: 0:00:36  lr: 0.000001  training_loss: 1.1651 (1.1311)  classification_loss: 1.1651 (1.1309)  loss_mask: 0.0000 (0.0002)  time: 0.1674  data: 0.0003  max mem: 4132
[21:16:22.354347] Epoch: [99]  [580/781]  eta: 0:00:33  lr: 0.000001  training_loss: 1.1209 (1.1310)  classification_loss: 1.1209 (1.1307)  loss_mask: 0.0000 (0.0002)  time: 0.1643  data: 0.0003  max mem: 4132
[21:16:25.660841] Epoch: [99]  [600/781]  eta: 0:00:30  lr: 0.000001  training_loss: 1.1055 (1.1301)  classification_loss: 1.1055 (1.1299)  loss_mask: 0.0000 (0.0002)  time: 0.1652  data: 0.0003  max mem: 4132
[21:16:28.947256] Epoch: [99]  [620/781]  eta: 0:00:26  lr: 0.000001  training_loss: 1.1312 (1.1300)  classification_loss: 1.1312 (1.1298)  loss_mask: 0.0000 (0.0002)  time: 0.1642  data: 0.0002  max mem: 4132
[21:16:32.249273] Epoch: [99]  [640/781]  eta: 0:00:23  lr: 0.000001  training_loss: 1.1295 (1.1307)  classification_loss: 1.1295 (1.1305)  loss_mask: 0.0000 (0.0002)  time: 0.1650  data: 0.0003  max mem: 4132
[21:16:35.517566] Epoch: [99]  [660/781]  eta: 0:00:20  lr: 0.000001  training_loss: 1.1402 (1.1307)  classification_loss: 1.1402 (1.1305)  loss_mask: 0.0000 (0.0002)  time: 0.1633  data: 0.0002  max mem: 4132
[21:16:38.816247] Epoch: [99]  [680/781]  eta: 0:00:16  lr: 0.000001  training_loss: 1.1436 (1.1311)  classification_loss: 1.1436 (1.1309)  loss_mask: 0.0000 (0.0002)  time: 0.1648  data: 0.0003  max mem: 4132
[21:16:42.123872] Epoch: [99]  [700/781]  eta: 0:00:13  lr: 0.000001  training_loss: 1.1246 (1.1316)  classification_loss: 1.1246 (1.1314)  loss_mask: 0.0000 (0.0002)  time: 0.1653  data: 0.0005  max mem: 4132
[21:16:45.418735] Epoch: [99]  [720/781]  eta: 0:00:10  lr: 0.000001  training_loss: 1.1607 (1.1330)  classification_loss: 1.1607 (1.1328)  loss_mask: 0.0000 (0.0002)  time: 0.1646  data: 0.0004  max mem: 4132
[21:16:48.750742] Epoch: [99]  [740/781]  eta: 0:00:06  lr: 0.000001  training_loss: 1.1310 (1.1335)  classification_loss: 1.1310 (1.1333)  loss_mask: 0.0000 (0.0002)  time: 0.1665  data: 0.0003  max mem: 4132
[21:16:52.018418] Epoch: [99]  [760/781]  eta: 0:00:03  lr: 0.000001  training_loss: 1.1558 (1.1340)  classification_loss: 1.1558 (1.1339)  loss_mask: 0.0000 (0.0002)  time: 0.1633  data: 0.0003  max mem: 4132
[21:16:55.338148] Epoch: [99]  [780/781]  eta: 0:00:00  lr: 0.000001  training_loss: 1.1063 (1.1343)  classification_loss: 1.1063 (1.1341)  loss_mask: 0.0000 (0.0002)  time: 0.1659  data: 0.0004  max mem: 4132
[21:16:55.510901] Epoch: [99] Total time: 0:02:09 (0.1664 s / it)
[21:16:55.511413] Averaged stats: lr: 0.000001  training_loss: 1.1063 (1.1343)  classification_loss: 1.1063 (1.1341)  loss_mask: 0.0000 (0.0002)
[21:16:56.274449] Test:  [  0/157]  eta: 0:01:58  testing_loss: 0.4838 (0.4838)  acc1: 85.9375 (85.9375)  acc5: 96.8750 (96.8750)  time: 0.7574  data: 0.7193  max mem: 4132
[21:16:56.565878] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.4726 (0.4713)  acc1: 87.5000 (86.0795)  acc5: 100.0000 (99.4318)  time: 0.0951  data: 0.0656  max mem: 4132
[21:16:56.859900] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4206 (0.4367)  acc1: 87.5000 (87.4256)  acc5: 100.0000 (99.4792)  time: 0.0291  data: 0.0003  max mem: 4132
[21:16:57.154117] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4046 (0.4556)  acc1: 87.5000 (87.1472)  acc5: 100.0000 (99.1935)  time: 0.0292  data: 0.0003  max mem: 4132
[21:16:57.443779] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4768 (0.4605)  acc1: 87.5000 (87.2332)  acc5: 98.4375 (99.1235)  time: 0.0290  data: 0.0002  max mem: 4132
[21:16:57.736305] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4602 (0.4540)  acc1: 85.9375 (87.5000)  acc5: 98.4375 (99.1728)  time: 0.0289  data: 0.0002  max mem: 4132
[21:16:58.036328] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4201 (0.4490)  acc1: 87.5000 (87.6793)  acc5: 100.0000 (99.2316)  time: 0.0294  data: 0.0003  max mem: 4132
[21:16:58.331723] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4159 (0.4450)  acc1: 87.5000 (87.6981)  acc5: 100.0000 (99.2518)  time: 0.0295  data: 0.0003  max mem: 4132
[21:16:58.622349] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4378 (0.4532)  acc1: 87.5000 (87.2685)  acc5: 98.4375 (99.1705)  time: 0.0291  data: 0.0004  max mem: 4132
[21:16:58.913302] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4515 (0.4499)  acc1: 85.9375 (87.1909)  acc5: 98.4375 (99.1930)  time: 0.0289  data: 0.0004  max mem: 4132
[21:16:59.204815] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.4446 (0.4518)  acc1: 85.9375 (86.9895)  acc5: 100.0000 (99.2110)  time: 0.0290  data: 0.0003  max mem: 4132
[21:16:59.497191] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4446 (0.4524)  acc1: 84.3750 (86.8666)  acc5: 100.0000 (99.1976)  time: 0.0291  data: 0.0002  max mem: 4132
[21:16:59.796687] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4371 (0.4521)  acc1: 85.9375 (86.8156)  acc5: 100.0000 (99.2123)  time: 0.0295  data: 0.0003  max mem: 4132
[21:17:00.092015] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4371 (0.4520)  acc1: 85.9375 (86.7844)  acc5: 100.0000 (99.2366)  time: 0.0296  data: 0.0003  max mem: 4132
[21:17:00.381955] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4141 (0.4482)  acc1: 85.9375 (86.9016)  acc5: 100.0000 (99.2686)  time: 0.0291  data: 0.0003  max mem: 4132
[21:17:00.668669] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4184 (0.4481)  acc1: 85.9375 (86.8895)  acc5: 100.0000 (99.2757)  time: 0.0287  data: 0.0002  max mem: 4132
[21:17:00.822875] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4315 (0.4486)  acc1: 87.5000 (86.8400)  acc5: 98.4375 (99.2500)  time: 0.0276  data: 0.0002  max mem: 4132
[21:17:01.010818] Test: Total time: 0:00:05 (0.0350 s / it)
[21:17:01.011254] * Acc@1 86.840 Acc@5 99.250 loss 0.449
[21:17:01.011545] Accuracy of the network on the 10000 test images: 86.8%
[21:17:01.011741] Max accuracy: 86.85%
[21:17:01.207237] Training time 3:47:06