Not using distributed mode
[17:17:02.772921] job dir: /notebooks/CVPR2023
[17:17:02.773173] Namespace(batch_size=64,
epochs=100,
accum_iter=1,
model='mae_vit_tiny',
norm_pix_loss=False,
dataset='c10',
input_size=32,
patch_size=2,
mask_ratio=0.75,
lambda_weight=0.1,
drop_path=0.1,
clip_grad=None,
weight_decay=0.05,
lr=None,
blr=0.001,
layer_decay=0.75,
min_lr=1e-06,
warmup_epochs=5,
color_jitter=None,
aa='rand-m9-mstd0.5-inc1',
smoothing=0.1,
reprob=0.25,
remode='pixel',
recount=1,
resplit=False,
mixup=0,
cutmix=0,
cutmix_minmax=None,
mixup_prob=1.0,
mixup_switch_prob=0.5,
mixup_mode='batch',
finetune='',
global_pool=True,
data_path='/datasets01/imagenet_full_size/061417/',
nb_classes=10,
output_dir='./output_dir',
log_dir='./output_dir',
device='cuda',
seed=0,
resume='',
start_epoch=0,
eval=False,
dist_eval=False,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[17:17:03.159332] Files already downloaded and verified
[17:17:03.952842] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCropAndInterpolation(size=(32, 32), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic)
               RandomHorizontalFlip(p=0.5)
               RandAugment(n=2, ops=
           	AugmentOp(name=AutoContrast, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Equalize, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Invert, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Rotate, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=PosterizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeAdd, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ColorIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ContrastIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=BrightnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SharpnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearX, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearY, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateXRel, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateYRel, p=0.5, m=9, mstd=0.5))
               ToTensor()
               Normalize(mean=tensor([0.4914, 0.4822, 0.4465]), std=tensor([0.2023, 0.1994, 0.2010]))
               RandomErasing(p=0.25, mode=pixel, count=(1, 1))
           )
/usr/local/lib/python3.9/dist-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
[17:17:04.347853] Files already downloaded and verified
[17:17:04.776531] Dataset CIFAR10
    Number of datapoints: 10000
    Root location: ./data
    Split: Test
    StandardTransform
Transform: Compose(
               Resize(size=36, interpolation=bicubic, max_size=None, antialias=None)
               CenterCrop(size=(32, 32))
               ToTensor()
               Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.201))
           )
[17:17:04.777214] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7fed41ebed60>
/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[17:17:10.468433] Model = MaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(2, 2), stride=(2, 2))
    (norm): Identity()
  )
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (7): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (8): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (9): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (10): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (11): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
  (decoder_embed): Linear(in_features=192, out_features=128, bias=True)
  (decoder_blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=128, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=128, out_features=128, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=512, out_features=128, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=128, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=128, out_features=128, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=512, out_features=128, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (decoder_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
  (decoder_pred): Linear(in_features=128, out_features=12, bias=True)
  (head): Linear(in_features=192, out_features=10, bias=True)
  (classifier_mask): Sequential(
    (0): Linear(in_features=192, out_features=5, bias=True)
    (1): LogSoftmax(dim=1)
  )
)
[17:17:10.469705] number of params (M): 5.77
[17:17:10.469918] base lr: 1.00e-03
[17:17:10.470104] actual lr: 2.50e-04
[17:17:10.470256] accumulate grad iterations: 1
[17:17:10.470616] effective batch size: 64
[17:17:10.472515] criterion = LabelSmoothingCrossEntropy()
[17:17:10.472875] Start training for 100 epochs
[17:17:10.475039] log_dir: ./output_dir
[17:17:13.450068] Epoch: [0]  [  0/781]  eta: 0:38:42  lr: 0.000000  training_loss: 2.2301 (2.2301)  classification_loss: 0.2656 (0.2656)  loss_mask: 1.9646 (1.9646)  time: 2.9734  data: 0.6070  max mem: 4070
[17:17:16.724842] Epoch: [0]  [ 20/781]  eta: 0:03:46  lr: 0.000001  training_loss: 2.1718 (2.1943)  classification_loss: 0.2622 (0.2612)  loss_mask: 1.9202 (1.9331)  time: 0.1636  data: 0.0002  max mem: 4130
[17:17:19.999598] Epoch: [0]  [ 40/781]  eta: 0:02:52  lr: 0.000003  training_loss: 2.0412 (2.1264)  classification_loss: 0.2559 (0.2589)  loss_mask: 1.7844 (1.8675)  time: 0.1636  data: 0.0004  max mem: 4130
[17:17:23.287946] Epoch: [0]  [ 60/781]  eta: 0:02:31  lr: 0.000004  training_loss: 1.9341 (2.0656)  classification_loss: 0.2524 (0.2564)  loss_mask: 1.6903 (1.8092)  time: 0.1643  data: 0.0003  max mem: 4130
[17:17:26.549589] Epoch: [0]  [ 80/781]  eta: 0:02:19  lr: 0.000005  training_loss: 1.9247 (2.0304)  classification_loss: 0.2466 (0.2542)  loss_mask: 1.6866 (1.7762)  time: 0.1630  data: 0.0003  max mem: 4130
[17:17:29.871001] Epoch: [0]  [100/781]  eta: 0:02:10  lr: 0.000006  training_loss: 1.8963 (2.0024)  classification_loss: 0.2392 (0.2515)  loss_mask: 1.6539 (1.7509)  time: 0.1660  data: 0.0004  max mem: 4130
[17:17:33.172536] Epoch: [0]  [120/781]  eta: 0:02:03  lr: 0.000008  training_loss: 1.8414 (1.9776)  classification_loss: 0.2374 (0.2490)  loss_mask: 1.6022 (1.7287)  time: 0.1650  data: 0.0003  max mem: 4130
[17:17:36.472776] Epoch: [0]  [140/781]  eta: 0:01:58  lr: 0.000009  training_loss: 1.8514 (1.9607)  classification_loss: 0.2338 (0.2468)  loss_mask: 1.6156 (1.7140)  time: 0.1649  data: 0.0003  max mem: 4130
[17:17:39.769067] Epoch: [0]  [160/781]  eta: 0:01:52  lr: 0.000010  training_loss: 1.8366 (1.9450)  classification_loss: 0.2326 (0.2450)  loss_mask: 1.6032 (1.7000)  time: 0.1647  data: 0.0003  max mem: 4130
[17:17:43.071883] Epoch: [0]  [180/781]  eta: 0:01:48  lr: 0.000012  training_loss: 1.8349 (1.9329)  classification_loss: 0.2315 (0.2435)  loss_mask: 1.6047 (1.6893)  time: 0.1651  data: 0.0003  max mem: 4130
[17:17:46.360287] Epoch: [0]  [200/781]  eta: 0:01:43  lr: 0.000013  training_loss: 1.8241 (1.9219)  classification_loss: 0.2292 (0.2422)  loss_mask: 1.5894 (1.6798)  time: 0.1643  data: 0.0003  max mem: 4130
[17:17:49.658597] Epoch: [0]  [220/781]  eta: 0:01:39  lr: 0.000014  training_loss: 1.8050 (1.9118)  classification_loss: 0.2295 (0.2410)  loss_mask: 1.5768 (1.6708)  time: 0.1648  data: 0.0003  max mem: 4130
[17:17:52.918512] Epoch: [0]  [240/781]  eta: 0:01:35  lr: 0.000015  training_loss: 1.8037 (1.9031)  classification_loss: 0.2295 (0.2401)  loss_mask: 1.5755 (1.6630)  time: 0.1629  data: 0.0003  max mem: 4130
[17:17:56.207062] Epoch: [0]  [260/781]  eta: 0:01:31  lr: 0.000017  training_loss: 1.8023 (1.8950)  classification_loss: 0.2287 (0.2392)  loss_mask: 1.5733 (1.6557)  time: 0.1643  data: 0.0004  max mem: 4130
[17:17:59.497005] Epoch: [0]  [280/781]  eta: 0:01:27  lr: 0.000018  training_loss: 1.7772 (1.8869)  classification_loss: 0.2287 (0.2385)  loss_mask: 1.5467 (1.6484)  time: 0.1644  data: 0.0003  max mem: 4130
[17:18:02.774880] Epoch: [0]  [300/781]  eta: 0:01:23  lr: 0.000019  training_loss: 1.7728 (1.8798)  classification_loss: 0.2296 (0.2379)  loss_mask: 1.5477 (1.6419)  time: 0.1638  data: 0.0003  max mem: 4130
[17:18:06.038815] Epoch: [0]  [320/781]  eta: 0:01:19  lr: 0.000020  training_loss: 1.7679 (1.8724)  classification_loss: 0.2280 (0.2373)  loss_mask: 1.5371 (1.6352)  time: 0.1631  data: 0.0003  max mem: 4130
[17:18:09.300992] Epoch: [0]  [340/781]  eta: 0:01:16  lr: 0.000022  training_loss: 1.7587 (1.8666)  classification_loss: 0.2279 (0.2367)  loss_mask: 1.5327 (1.6299)  time: 0.1630  data: 0.0002  max mem: 4130
[17:18:12.589038] Epoch: [0]  [360/781]  eta: 0:01:12  lr: 0.000023  training_loss: 1.7447 (1.8597)  classification_loss: 0.2279 (0.2362)  loss_mask: 1.5173 (1.6235)  time: 0.1642  data: 0.0004  max mem: 4130
[17:18:15.881691] Epoch: [0]  [380/781]  eta: 0:01:08  lr: 0.000024  training_loss: 1.7460 (1.8538)  classification_loss: 0.2267 (0.2357)  loss_mask: 1.5200 (1.6181)  time: 0.1645  data: 0.0003  max mem: 4130
[17:18:19.199185] Epoch: [0]  [400/781]  eta: 0:01:05  lr: 0.000026  training_loss: 1.7168 (1.8471)  classification_loss: 0.2251 (0.2352)  loss_mask: 1.4867 (1.6119)  time: 0.1658  data: 0.0003  max mem: 4130
[17:18:22.491581] Epoch: [0]  [420/781]  eta: 0:01:01  lr: 0.000027  training_loss: 1.6553 (1.8391)  classification_loss: 0.2280 (0.2349)  loss_mask: 1.4293 (1.6042)  time: 0.1644  data: 0.0003  max mem: 4130
[17:18:25.783828] Epoch: [0]  [440/781]  eta: 0:00:58  lr: 0.000028  training_loss: 1.6299 (1.8301)  classification_loss: 0.2253 (0.2345)  loss_mask: 1.4029 (1.5956)  time: 0.1645  data: 0.0004  max mem: 4130
[17:18:29.081446] Epoch: [0]  [460/781]  eta: 0:00:54  lr: 0.000029  training_loss: 1.5894 (1.8208)  classification_loss: 0.2228 (0.2340)  loss_mask: 1.3676 (1.5867)  time: 0.1648  data: 0.0003  max mem: 4130
[17:18:32.372107] Epoch: [0]  [480/781]  eta: 0:00:51  lr: 0.000031  training_loss: 1.5599 (1.8101)  classification_loss: 0.2254 (0.2337)  loss_mask: 1.3351 (1.5764)  time: 0.1644  data: 0.0004  max mem: 4130
[17:18:35.708162] Epoch: [0]  [500/781]  eta: 0:00:47  lr: 0.000032  training_loss: 1.5594 (1.8000)  classification_loss: 0.2255 (0.2334)  loss_mask: 1.3294 (1.5666)  time: 0.1667  data: 0.0003  max mem: 4130
[17:18:38.986341] Epoch: [0]  [520/781]  eta: 0:00:44  lr: 0.000033  training_loss: 1.4329 (1.7856)  classification_loss: 0.2270 (0.2331)  loss_mask: 1.2012 (1.5525)  time: 0.1638  data: 0.0002  max mem: 4130
[17:18:42.262773] Epoch: [0]  [540/781]  eta: 0:00:40  lr: 0.000035  training_loss: 1.4545 (1.7731)  classification_loss: 0.2260 (0.2329)  loss_mask: 1.2285 (1.5402)  time: 0.1637  data: 0.0003  max mem: 4130
[17:18:45.542090] Epoch: [0]  [560/781]  eta: 0:00:37  lr: 0.000036  training_loss: 1.3605 (1.7591)  classification_loss: 0.2251 (0.2327)  loss_mask: 1.1345 (1.5264)  time: 0.1639  data: 0.0003  max mem: 4130
[17:18:48.849140] Epoch: [0]  [580/781]  eta: 0:00:34  lr: 0.000037  training_loss: 1.3687 (1.7447)  classification_loss: 0.2246 (0.2324)  loss_mask: 1.1423 (1.5123)  time: 0.1653  data: 0.0004  max mem: 4130
[17:18:52.162714] Epoch: [0]  [600/781]  eta: 0:00:30  lr: 0.000038  training_loss: 1.3079 (1.7302)  classification_loss: 0.2247 (0.2321)  loss_mask: 1.0846 (1.4980)  time: 0.1656  data: 0.0004  max mem: 4130
[17:18:55.454435] Epoch: [0]  [620/781]  eta: 0:00:27  lr: 0.000040  training_loss: 1.3960 (1.7195)  classification_loss: 0.2246 (0.2319)  loss_mask: 1.1701 (1.4875)  time: 0.1645  data: 0.0003  max mem: 4130
[17:18:58.733450] Epoch: [0]  [640/781]  eta: 0:00:23  lr: 0.000041  training_loss: 1.2029 (1.7040)  classification_loss: 0.2252 (0.2317)  loss_mask: 0.9733 (1.4722)  time: 0.1638  data: 0.0003  max mem: 4130
[17:19:02.041354] Epoch: [0]  [660/781]  eta: 0:00:20  lr: 0.000042  training_loss: 1.1794 (1.6880)  classification_loss: 0.2258 (0.2316)  loss_mask: 0.9509 (1.4564)  time: 0.1653  data: 0.0003  max mem: 4130
[17:19:05.372592] Epoch: [0]  [680/781]  eta: 0:00:17  lr: 0.000044  training_loss: 1.2062 (1.6753)  classification_loss: 0.2258 (0.2314)  loss_mask: 0.9809 (1.4439)  time: 0.1664  data: 0.0003  max mem: 4130
[17:19:08.663555] Epoch: [0]  [700/781]  eta: 0:00:13  lr: 0.000045  training_loss: 1.2391 (1.6639)  classification_loss: 0.2253 (0.2312)  loss_mask: 1.0091 (1.4326)  time: 0.1645  data: 0.0003  max mem: 4130
[17:19:11.934914] Epoch: [0]  [720/781]  eta: 0:00:10  lr: 0.000046  training_loss: 1.1763 (1.6509)  classification_loss: 0.2256 (0.2311)  loss_mask: 0.9553 (1.4198)  time: 0.1635  data: 0.0003  max mem: 4130
[17:19:15.228386] Epoch: [0]  [740/781]  eta: 0:00:06  lr: 0.000047  training_loss: 1.1457 (1.6378)  classification_loss: 0.2251 (0.2310)  loss_mask: 0.9217 (1.4069)  time: 0.1645  data: 0.0003  max mem: 4130
[17:19:18.552499] Epoch: [0]  [760/781]  eta: 0:00:03  lr: 0.000049  training_loss: 1.0451 (1.6235)  classification_loss: 0.2246 (0.2308)  loss_mask: 0.8221 (1.3927)  time: 0.1661  data: 0.0003  max mem: 4130
[17:19:21.824224] Epoch: [0]  [780/781]  eta: 0:00:00  lr: 0.000050  training_loss: 1.0486 (1.6091)  classification_loss: 0.2252 (0.2307)  loss_mask: 0.8245 (1.3784)  time: 0.1634  data: 0.0003  max mem: 4130
[17:19:21.942558] Epoch: [0] Total time: 0:02:11 (0.1683 s / it)
[17:19:21.944068] Averaged stats: lr: 0.000050  training_loss: 1.0486 (1.6091)  classification_loss: 0.2252 (0.2307)  loss_mask: 0.8245 (1.3784)
[17:19:24.063932] Test:  [  0/157]  eta: 0:02:01  testing_loss: 2.1261 (2.1261)  acc1: 39.0625 (39.0625)  acc5: 84.3750 (84.3750)  time: 0.7761  data: 0.7397  max mem: 4130
[17:19:24.380775] Test:  [ 10/157]  eta: 0:00:14  testing_loss: 2.1753 (2.1699)  acc1: 26.5625 (25.9943)  acc5: 76.5625 (77.8409)  time: 0.0989  data: 0.0685  max mem: 4130
[17:19:24.671556] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 2.1768 (2.1741)  acc1: 23.4375 (25.2976)  acc5: 76.5625 (76.0417)  time: 0.0300  data: 0.0008  max mem: 4130
[17:19:24.966085] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 2.1721 (2.1685)  acc1: 23.4375 (25.6048)  acc5: 76.5625 (76.4617)  time: 0.0290  data: 0.0002  max mem: 4130
[17:19:25.260329] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 2.1596 (2.1691)  acc1: 21.8750 (24.9619)  acc5: 76.5625 (76.4482)  time: 0.0292  data: 0.0002  max mem: 4130
[17:19:25.557162] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 2.1601 (2.1686)  acc1: 21.8750 (24.7549)  acc5: 76.5625 (76.7770)  time: 0.0293  data: 0.0003  max mem: 4130
[17:19:25.849825] Test:  [ 60/157]  eta: 0:00:04  testing_loss: 2.1601 (2.1682)  acc1: 21.8750 (24.6670)  acc5: 79.6875 (77.2029)  time: 0.0293  data: 0.0003  max mem: 4130
[17:19:26.144526] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 2.1559 (2.1667)  acc1: 25.0000 (24.8239)  acc5: 79.6875 (77.5308)  time: 0.0292  data: 0.0003  max mem: 4130
[17:19:26.432571] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 2.1511 (2.1653)  acc1: 25.0000 (24.8650)  acc5: 79.6875 (78.0864)  time: 0.0290  data: 0.0002  max mem: 4130
[17:19:26.728301] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 2.1617 (2.1668)  acc1: 23.4375 (24.6223)  acc5: 78.1250 (78.0907)  time: 0.0290  data: 0.0002  max mem: 4130
[17:19:27.026803] Test:  [100/157]  eta: 0:00:02  testing_loss: 2.1687 (2.1660)  acc1: 25.0000 (24.9691)  acc5: 76.5625 (78.1250)  time: 0.0296  data: 0.0002  max mem: 4130
[17:19:27.317034] Test:  [110/157]  eta: 0:00:01  testing_loss: 2.1687 (2.1667)  acc1: 25.0000 (24.9296)  acc5: 76.5625 (78.1109)  time: 0.0292  data: 0.0002  max mem: 4130
[17:19:27.614795] Test:  [120/157]  eta: 0:00:01  testing_loss: 2.1693 (2.1655)  acc1: 25.0000 (24.9613)  acc5: 78.1250 (78.1767)  time: 0.0292  data: 0.0002  max mem: 4130
[17:19:27.907277] Test:  [130/157]  eta: 0:00:00  testing_loss: 2.1702 (2.1666)  acc1: 23.4375 (24.7734)  acc5: 78.1250 (78.1011)  time: 0.0293  data: 0.0003  max mem: 4130
[17:19:28.197844] Test:  [140/157]  eta: 0:00:00  testing_loss: 2.1697 (2.1659)  acc1: 23.4375 (24.8892)  acc5: 78.1250 (78.1804)  time: 0.0290  data: 0.0002  max mem: 4130
[17:19:28.481870] Test:  [150/157]  eta: 0:00:00  testing_loss: 2.1572 (2.1651)  acc1: 26.5625 (25.0103)  acc5: 79.6875 (78.3423)  time: 0.0286  data: 0.0002  max mem: 4130
[17:19:28.747976] Test:  [156/157]  eta: 0:00:00  testing_loss: 2.1545 (2.1649)  acc1: 26.5625 (24.9400)  acc5: 79.6875 (78.4100)  time: 0.0330  data: 0.0002  max mem: 4130
[17:19:28.914058] Test: Total time: 0:00:05 (0.0359 s / it)
[17:19:28.914825] * Acc@1 24.940 Acc@5 78.410 loss 2.165
[17:19:28.915132] Accuracy of the network on the 10000 test images: 24.9%
[17:19:28.915307] Max accuracy: 24.94%
[17:19:29.193125] log_dir: ./output_dir
[17:19:30.197944] Epoch: [1]  [  0/781]  eta: 0:13:02  lr: 0.000050  training_loss: 1.1907 (1.1907)  classification_loss: 0.2280 (0.2280)  loss_mask: 0.9627 (0.9627)  time: 1.0022  data: 0.7716  max mem: 4132
[17:19:33.496761] Epoch: [1]  [ 20/781]  eta: 0:02:35  lr: 0.000051  training_loss: 1.1238 (1.1261)  classification_loss: 0.2252 (0.2253)  loss_mask: 0.8939 (0.9008)  time: 0.1648  data: 0.0003  max mem: 4132
[17:19:36.816151] Epoch: [1]  [ 40/781]  eta: 0:02:17  lr: 0.000053  training_loss: 1.0575 (1.1088)  classification_loss: 0.2251 (0.2254)  loss_mask: 0.8322 (0.8834)  time: 0.1659  data: 0.0004  max mem: 4132
[17:19:40.133329] Epoch: [1]  [ 60/781]  eta: 0:02:09  lr: 0.000054  training_loss: 1.0592 (1.0998)  classification_loss: 0.2258 (0.2256)  loss_mask: 0.8344 (0.8742)  time: 0.1658  data: 0.0004  max mem: 4132
[17:19:43.400136] Epoch: [1]  [ 80/781]  eta: 0:02:02  lr: 0.000055  training_loss: 1.0611 (1.1067)  classification_loss: 0.2242 (0.2254)  loss_mask: 0.8364 (0.8813)  time: 0.1632  data: 0.0003  max mem: 4132
[17:19:46.669326] Epoch: [1]  [100/781]  eta: 0:01:57  lr: 0.000056  training_loss: 1.1284 (1.1153)  classification_loss: 0.2250 (0.2254)  loss_mask: 0.9025 (0.8899)  time: 0.1634  data: 0.0003  max mem: 4132
[17:19:50.005325] Epoch: [1]  [120/781]  eta: 0:01:53  lr: 0.000058  training_loss: 1.0403 (1.1097)  classification_loss: 0.2253 (0.2254)  loss_mask: 0.8165 (0.8843)  time: 0.1667  data: 0.0004  max mem: 4132
[17:19:53.328021] Epoch: [1]  [140/781]  eta: 0:01:49  lr: 0.000059  training_loss: 1.0123 (1.0955)  classification_loss: 0.2241 (0.2253)  loss_mask: 0.7847 (0.8702)  time: 0.1660  data: 0.0003  max mem: 4132
[17:19:56.654299] Epoch: [1]  [160/781]  eta: 0:01:45  lr: 0.000060  training_loss: 1.1570 (1.1017)  classification_loss: 0.2252 (0.2253)  loss_mask: 0.9329 (0.8764)  time: 0.1662  data: 0.0003  max mem: 4132
[17:19:59.973518] Epoch: [1]  [180/781]  eta: 0:01:42  lr: 0.000062  training_loss: 1.0661 (1.0991)  classification_loss: 0.2250 (0.2253)  loss_mask: 0.8399 (0.8738)  time: 0.1659  data: 0.0003  max mem: 4132
[17:20:03.246806] Epoch: [1]  [200/781]  eta: 0:01:38  lr: 0.000063  training_loss: 1.0292 (1.0914)  classification_loss: 0.2239 (0.2252)  loss_mask: 0.8069 (0.8662)  time: 0.1636  data: 0.0003  max mem: 4132
[17:20:06.562675] Epoch: [1]  [220/781]  eta: 0:01:34  lr: 0.000064  training_loss: 0.9997 (1.0845)  classification_loss: 0.2265 (0.2252)  loss_mask: 0.7725 (0.8593)  time: 0.1657  data: 0.0004  max mem: 4132
[17:20:09.860037] Epoch: [1]  [240/781]  eta: 0:01:31  lr: 0.000065  training_loss: 1.0499 (1.0836)  classification_loss: 0.2262 (0.2252)  loss_mask: 0.8210 (0.8584)  time: 0.1648  data: 0.0003  max mem: 4132
[17:20:13.149500] Epoch: [1]  [260/781]  eta: 0:01:27  lr: 0.000067  training_loss: 1.0061 (1.0796)  classification_loss: 0.2252 (0.2252)  loss_mask: 0.7807 (0.8544)  time: 0.1644  data: 0.0003  max mem: 4132
[17:20:16.410988] Epoch: [1]  [280/781]  eta: 0:01:24  lr: 0.000068  training_loss: 0.9853 (1.0751)  classification_loss: 0.2267 (0.2253)  loss_mask: 0.7611 (0.8498)  time: 0.1630  data: 0.0003  max mem: 4132
[17:20:19.708801] Epoch: [1]  [300/781]  eta: 0:01:20  lr: 0.000069  training_loss: 0.9348 (1.0667)  classification_loss: 0.2245 (0.2252)  loss_mask: 0.7118 (0.8414)  time: 0.1648  data: 0.0003  max mem: 4132
[17:20:23.046787] Epoch: [1]  [320/781]  eta: 0:01:17  lr: 0.000070  training_loss: 0.9483 (1.0605)  classification_loss: 0.2242 (0.2251)  loss_mask: 0.7237 (0.8354)  time: 0.1668  data: 0.0003  max mem: 4132
[17:20:26.353175] Epoch: [1]  [340/781]  eta: 0:01:13  lr: 0.000072  training_loss: 0.9507 (1.0557)  classification_loss: 0.2258 (0.2252)  loss_mask: 0.7214 (0.8306)  time: 0.1652  data: 0.0008  max mem: 4132
[17:20:29.721085] Epoch: [1]  [360/781]  eta: 0:01:10  lr: 0.000073  training_loss: 0.9771 (1.0534)  classification_loss: 0.2254 (0.2252)  loss_mask: 0.7521 (0.8282)  time: 0.1683  data: 0.0003  max mem: 4132
[17:20:33.070486] Epoch: [1]  [380/781]  eta: 0:01:07  lr: 0.000074  training_loss: 1.0671 (1.0536)  classification_loss: 0.2260 (0.2252)  loss_mask: 0.8394 (0.8284)  time: 0.1674  data: 0.0003  max mem: 4132
[17:20:36.367593] Epoch: [1]  [400/781]  eta: 0:01:03  lr: 0.000076  training_loss: 0.9684 (1.0497)  classification_loss: 0.2247 (0.2252)  loss_mask: 0.7443 (0.8245)  time: 0.1648  data: 0.0003  max mem: 4132
[17:20:39.667242] Epoch: [1]  [420/781]  eta: 0:01:00  lr: 0.000077  training_loss: 0.9500 (1.0461)  classification_loss: 0.2241 (0.2252)  loss_mask: 0.7250 (0.8210)  time: 0.1649  data: 0.0003  max mem: 4132
[17:20:42.976142] Epoch: [1]  [440/781]  eta: 0:00:57  lr: 0.000078  training_loss: 0.9360 (1.0415)  classification_loss: 0.2239 (0.2251)  loss_mask: 0.7122 (0.8163)  time: 0.1653  data: 0.0003  max mem: 4132
[17:20:46.249553] Epoch: [1]  [460/781]  eta: 0:00:53  lr: 0.000079  training_loss: 0.9240 (1.0367)  classification_loss: 0.2238 (0.2251)  loss_mask: 0.6994 (0.8116)  time: 0.1636  data: 0.0003  max mem: 4132
[17:20:49.552079] Epoch: [1]  [480/781]  eta: 0:00:50  lr: 0.000081  training_loss: 1.0014 (1.0373)  classification_loss: 0.2245 (0.2251)  loss_mask: 0.7740 (0.8122)  time: 0.1650  data: 0.0003  max mem: 4132
[17:20:52.852445] Epoch: [1]  [500/781]  eta: 0:00:46  lr: 0.000082  training_loss: 1.0692 (1.0387)  classification_loss: 0.2244 (0.2251)  loss_mask: 0.8489 (0.8136)  time: 0.1649  data: 0.0003  max mem: 4132
[17:20:56.153793] Epoch: [1]  [520/781]  eta: 0:00:43  lr: 0.000083  training_loss: 0.9038 (1.0342)  classification_loss: 0.2248 (0.2251)  loss_mask: 0.6807 (0.8092)  time: 0.1650  data: 0.0004  max mem: 4132
[17:20:59.479973] Epoch: [1]  [540/781]  eta: 0:00:40  lr: 0.000085  training_loss: 0.9028 (1.0307)  classification_loss: 0.2243 (0.2251)  loss_mask: 0.6834 (0.8056)  time: 0.1662  data: 0.0003  max mem: 4132
[17:21:02.862282] Epoch: [1]  [560/781]  eta: 0:00:36  lr: 0.000086  training_loss: 0.9327 (1.0272)  classification_loss: 0.2236 (0.2251)  loss_mask: 0.7093 (0.8022)  time: 0.1689  data: 0.0003  max mem: 4132
[17:21:06.147139] Epoch: [1]  [580/781]  eta: 0:00:33  lr: 0.000087  training_loss: 0.9989 (1.0267)  classification_loss: 0.2251 (0.2251)  loss_mask: 0.7737 (0.8017)  time: 0.1640  data: 0.0003  max mem: 4132
[17:21:09.479929] Epoch: [1]  [600/781]  eta: 0:00:30  lr: 0.000088  training_loss: 1.0693 (1.0280)  classification_loss: 0.2245 (0.2251)  loss_mask: 0.8444 (0.8029)  time: 0.1665  data: 0.0003  max mem: 4132
[17:21:12.817089] Epoch: [1]  [620/781]  eta: 0:00:26  lr: 0.000090  training_loss: 0.9626 (1.0265)  classification_loss: 0.2240 (0.2250)  loss_mask: 0.7384 (0.8015)  time: 0.1668  data: 0.0003  max mem: 4132
[17:21:16.113279] Epoch: [1]  [640/781]  eta: 0:00:23  lr: 0.000091  training_loss: 0.9496 (1.0242)  classification_loss: 0.2238 (0.2250)  loss_mask: 0.7276 (0.7992)  time: 0.1647  data: 0.0004  max mem: 4132
[17:21:19.380240] Epoch: [1]  [660/781]  eta: 0:00:20  lr: 0.000092  training_loss: 1.0039 (1.0235)  classification_loss: 0.2250 (0.2250)  loss_mask: 0.7838 (0.7985)  time: 0.1633  data: 0.0002  max mem: 4132
[17:21:22.652720] Epoch: [1]  [680/781]  eta: 0:00:16  lr: 0.000094  training_loss: 0.9355 (1.0213)  classification_loss: 0.2253 (0.2250)  loss_mask: 0.7147 (0.7964)  time: 0.1635  data: 0.0003  max mem: 4132
[17:21:25.965705] Epoch: [1]  [700/781]  eta: 0:00:13  lr: 0.000095  training_loss: 0.9579 (1.0199)  classification_loss: 0.2227 (0.2250)  loss_mask: 0.7366 (0.7949)  time: 0.1656  data: 0.0003  max mem: 4132
[17:21:29.281446] Epoch: [1]  [720/781]  eta: 0:00:10  lr: 0.000096  training_loss: 0.9144 (1.0175)  classification_loss: 0.2230 (0.2249)  loss_mask: 0.6868 (0.7926)  time: 0.1657  data: 0.0004  max mem: 4132
[17:21:32.584618] Epoch: [1]  [740/781]  eta: 0:00:06  lr: 0.000097  training_loss: 0.9962 (1.0177)  classification_loss: 0.2271 (0.2250)  loss_mask: 0.7705 (0.7928)  time: 0.1651  data: 0.0003  max mem: 4132
[17:21:35.876325] Epoch: [1]  [760/781]  eta: 0:00:03  lr: 0.000099  training_loss: 0.9737 (1.0167)  classification_loss: 0.2239 (0.2250)  loss_mask: 0.7519 (0.7918)  time: 0.1645  data: 0.0003  max mem: 4132
[17:21:39.155564] Epoch: [1]  [780/781]  eta: 0:00:00  lr: 0.000100  training_loss: 0.8799 (1.0139)  classification_loss: 0.2255 (0.2250)  loss_mask: 0.6544 (0.7889)  time: 0.1639  data: 0.0003  max mem: 4132
[17:21:39.326085] Epoch: [1] Total time: 0:02:10 (0.1666 s / it)
[17:21:39.327637] Averaged stats: lr: 0.000100  training_loss: 0.8799 (1.0139)  classification_loss: 0.2255 (0.2250)  loss_mask: 0.6544 (0.7889)
[17:21:40.093482] Test:  [  0/157]  eta: 0:01:59  testing_loss: 2.1143 (2.1143)  acc1: 31.2500 (31.2500)  acc5: 79.6875 (79.6875)  time: 0.7601  data: 0.7290  max mem: 4132
[17:21:40.385346] Test:  [ 10/157]  eta: 0:00:14  testing_loss: 2.1577 (2.1539)  acc1: 25.0000 (24.8580)  acc5: 76.5625 (77.2727)  time: 0.0954  data: 0.0665  max mem: 4132
[17:21:40.672661] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 2.1593 (2.1560)  acc1: 23.4375 (23.8095)  acc5: 75.0000 (75.8929)  time: 0.0288  data: 0.0003  max mem: 4132
[17:21:40.965745] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 2.1462 (2.1520)  acc1: 21.8750 (23.8911)  acc5: 75.0000 (76.2601)  time: 0.0288  data: 0.0002  max mem: 4132
[17:21:41.260559] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 2.1483 (2.1531)  acc1: 25.0000 (23.7805)  acc5: 76.5625 (76.4863)  time: 0.0292  data: 0.0002  max mem: 4132
[17:21:41.555479] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 2.1483 (2.1513)  acc1: 23.4375 (23.8358)  acc5: 76.5625 (76.5319)  time: 0.0293  data: 0.0002  max mem: 4132
[17:21:41.844488] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 2.1460 (2.1512)  acc1: 25.0000 (24.0779)  acc5: 78.1250 (76.9467)  time: 0.0290  data: 0.0002  max mem: 4132
[17:21:42.134616] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 2.1460 (2.1498)  acc1: 26.5625 (24.1857)  acc5: 79.6875 (77.1787)  time: 0.0287  data: 0.0002  max mem: 4132
[17:21:42.426241] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 2.1332 (2.1497)  acc1: 23.4375 (24.2091)  acc5: 79.6875 (77.4691)  time: 0.0289  data: 0.0003  max mem: 4132
[17:21:42.717913] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 2.1591 (2.1522)  acc1: 21.8750 (23.9526)  acc5: 75.0000 (77.0089)  time: 0.0290  data: 0.0003  max mem: 4132
[17:21:43.005426] Test:  [100/157]  eta: 0:00:02  testing_loss: 2.1627 (2.1521)  acc1: 21.8750 (24.1491)  acc5: 75.0000 (76.8719)  time: 0.0288  data: 0.0002  max mem: 4132
[17:21:43.292646] Test:  [110/157]  eta: 0:00:01  testing_loss: 2.1556 (2.1531)  acc1: 25.0000 (23.9865)  acc5: 75.0000 (76.8018)  time: 0.0286  data: 0.0002  max mem: 4132
[17:21:43.581281] Test:  [120/157]  eta: 0:00:01  testing_loss: 2.1476 (2.1520)  acc1: 23.4375 (24.1090)  acc5: 75.0000 (76.7304)  time: 0.0286  data: 0.0002  max mem: 4132
[17:21:43.869941] Test:  [130/157]  eta: 0:00:00  testing_loss: 2.1470 (2.1525)  acc1: 23.4375 (24.1174)  acc5: 75.0000 (76.6579)  time: 0.0287  data: 0.0002  max mem: 4132
[17:21:44.158695] Test:  [140/157]  eta: 0:00:00  testing_loss: 2.1563 (2.1520)  acc1: 23.4375 (24.1246)  acc5: 75.0000 (76.6068)  time: 0.0287  data: 0.0002  max mem: 4132
[17:21:44.443437] Test:  [150/157]  eta: 0:00:00  testing_loss: 2.1404 (2.1513)  acc1: 23.4375 (24.2136)  acc5: 76.5625 (76.7591)  time: 0.0285  data: 0.0002  max mem: 4132
[17:21:44.601250] Test:  [156/157]  eta: 0:00:00  testing_loss: 2.1343 (2.1506)  acc1: 23.4375 (24.1300)  acc5: 78.1250 (76.8700)  time: 0.0276  data: 0.0002  max mem: 4132
[17:21:44.795413] Test: Total time: 0:00:05 (0.0348 s / it)
[17:21:44.795960] * Acc@1 24.130 Acc@5 76.870 loss 2.151
[17:21:44.796289] Accuracy of the network on the 10000 test images: 24.1%
[17:21:44.796508] Max accuracy: 24.94%
[17:21:48.852679] log_dir: ./output_dir
[17:21:49.790812] Epoch: [2]  [  0/781]  eta: 0:12:11  lr: 0.000100  training_loss: 0.8186 (0.8186)  classification_loss: 0.2258 (0.2258)  loss_mask: 0.5928 (0.5928)  time: 0.9361  data: 0.7203  max mem: 4132
[17:21:53.103110] Epoch: [2]  [ 20/781]  eta: 0:02:33  lr: 0.000101  training_loss: 0.8702 (0.8870)  classification_loss: 0.2244 (0.2245)  loss_mask: 0.6451 (0.6625)  time: 0.1655  data: 0.0001  max mem: 4132
[17:21:56.394682] Epoch: [2]  [ 40/781]  eta: 0:02:16  lr: 0.000103  training_loss: 0.8775 (0.8900)  classification_loss: 0.2243 (0.2245)  loss_mask: 0.6480 (0.6655)  time: 0.1645  data: 0.0003  max mem: 4132
[17:21:59.688839] Epoch: [2]  [ 60/781]  eta: 0:02:07  lr: 0.000104  training_loss: 0.9409 (0.9034)  classification_loss: 0.2252 (0.2247)  loss_mask: 0.7200 (0.6787)  time: 0.1643  data: 0.0003  max mem: 4132
[17:22:02.987634] Epoch: [2]  [ 80/781]  eta: 0:02:02  lr: 0.000105  training_loss: 0.9524 (0.9146)  classification_loss: 0.2243 (0.2246)  loss_mask: 0.7251 (0.6900)  time: 0.1648  data: 0.0003  max mem: 4132
[17:22:06.283723] Epoch: [2]  [100/781]  eta: 0:01:57  lr: 0.000106  training_loss: 0.8612 (0.9120)  classification_loss: 0.2237 (0.2245)  loss_mask: 0.6397 (0.6875)  time: 0.1647  data: 0.0003  max mem: 4132
[17:22:09.559316] Epoch: [2]  [120/781]  eta: 0:01:53  lr: 0.000108  training_loss: 0.8970 (0.9187)  classification_loss: 0.2246 (0.2246)  loss_mask: 0.6748 (0.6941)  time: 0.1637  data: 0.0003  max mem: 4132
[17:22:12.877514] Epoch: [2]  [140/781]  eta: 0:01:49  lr: 0.000109  training_loss: 0.9180 (0.9191)  classification_loss: 0.2225 (0.2245)  loss_mask: 0.6901 (0.6946)  time: 0.1658  data: 0.0003  max mem: 4132
[17:22:16.181701] Epoch: [2]  [160/781]  eta: 0:01:45  lr: 0.000110  training_loss: 0.9817 (0.9252)  classification_loss: 0.2242 (0.2245)  loss_mask: 0.7554 (0.7007)  time: 0.1651  data: 0.0003  max mem: 4132
[17:22:19.474464] Epoch: [2]  [180/781]  eta: 0:01:41  lr: 0.000112  training_loss: 0.9516 (0.9288)  classification_loss: 0.2235 (0.2245)  loss_mask: 0.7252 (0.7044)  time: 0.1645  data: 0.0003  max mem: 4132
[17:22:22.729633] Epoch: [2]  [200/781]  eta: 0:01:37  lr: 0.000113  training_loss: 0.8835 (0.9257)  classification_loss: 0.2232 (0.2243)  loss_mask: 0.6585 (0.7014)  time: 0.1627  data: 0.0002  max mem: 4132
[17:22:26.032758] Epoch: [2]  [220/781]  eta: 0:01:34  lr: 0.000114  training_loss: 0.8642 (0.9218)  classification_loss: 0.2234 (0.2242)  loss_mask: 0.6398 (0.6976)  time: 0.1651  data: 0.0002  max mem: 4132
[17:22:29.328546] Epoch: [2]  [240/781]  eta: 0:01:30  lr: 0.000115  training_loss: 0.9477 (0.9222)  classification_loss: 0.2243 (0.2242)  loss_mask: 0.7269 (0.6980)  time: 0.1647  data: 0.0003  max mem: 4132
[17:22:32.690975] Epoch: [2]  [260/781]  eta: 0:01:27  lr: 0.000117  training_loss: 0.9500 (0.9242)  classification_loss: 0.2245 (0.2243)  loss_mask: 0.7200 (0.7000)  time: 0.1680  data: 0.0003  max mem: 4132
[17:22:36.023728] Epoch: [2]  [280/781]  eta: 0:01:24  lr: 0.000118  training_loss: 0.9158 (0.9237)  classification_loss: 0.2250 (0.2244)  loss_mask: 0.6868 (0.6994)  time: 0.1665  data: 0.0002  max mem: 4132
[17:22:39.334664] Epoch: [2]  [300/781]  eta: 0:01:20  lr: 0.000119  training_loss: 0.9435 (0.9264)  classification_loss: 0.2233 (0.2243)  loss_mask: 0.7164 (0.7021)  time: 0.1655  data: 0.0003  max mem: 4132
[17:22:42.665471] Epoch: [2]  [320/781]  eta: 0:01:17  lr: 0.000120  training_loss: 0.8850 (0.9250)  classification_loss: 0.2225 (0.2242)  loss_mask: 0.6594 (0.7007)  time: 0.1664  data: 0.0003  max mem: 4132
[17:22:45.984294] Epoch: [2]  [340/781]  eta: 0:01:13  lr: 0.000122  training_loss: 0.8748 (0.9217)  classification_loss: 0.2248 (0.2242)  loss_mask: 0.6536 (0.6975)  time: 0.1658  data: 0.0003  max mem: 4132
[17:22:49.302702] Epoch: [2]  [360/781]  eta: 0:01:10  lr: 0.000123  training_loss: 0.8517 (0.9184)  classification_loss: 0.2235 (0.2242)  loss_mask: 0.6235 (0.6941)  time: 0.1658  data: 0.0002  max mem: 4132
[17:22:52.579616] Epoch: [2]  [380/781]  eta: 0:01:07  lr: 0.000124  training_loss: 0.9449 (0.9201)  classification_loss: 0.2236 (0.2242)  loss_mask: 0.7205 (0.6959)  time: 0.1638  data: 0.0003  max mem: 4132
[17:22:55.863595] Epoch: [2]  [400/781]  eta: 0:01:03  lr: 0.000126  training_loss: 0.9239 (0.9201)  classification_loss: 0.2227 (0.2242)  loss_mask: 0.6991 (0.6959)  time: 0.1641  data: 0.0003  max mem: 4132
[17:22:59.174537] Epoch: [2]  [420/781]  eta: 0:01:00  lr: 0.000127  training_loss: 0.8068 (0.9158)  classification_loss: 0.2240 (0.2242)  loss_mask: 0.5814 (0.6916)  time: 0.1655  data: 0.0003  max mem: 4132
[17:23:02.501128] Epoch: [2]  [440/781]  eta: 0:00:56  lr: 0.000128  training_loss: 0.8885 (0.9172)  classification_loss: 0.2239 (0.2242)  loss_mask: 0.6661 (0.6930)  time: 0.1662  data: 0.0004  max mem: 4132
[17:23:05.782574] Epoch: [2]  [460/781]  eta: 0:00:53  lr: 0.000129  training_loss: 0.8415 (0.9139)  classification_loss: 0.2230 (0.2241)  loss_mask: 0.6137 (0.6898)  time: 0.1640  data: 0.0003  max mem: 4132
[17:23:09.067388] Epoch: [2]  [480/781]  eta: 0:00:50  lr: 0.000131  training_loss: 0.8568 (0.9130)  classification_loss: 0.2237 (0.2241)  loss_mask: 0.6337 (0.6889)  time: 0.1641  data: 0.0003  max mem: 4132
[17:23:12.368097] Epoch: [2]  [500/781]  eta: 0:00:46  lr: 0.000132  training_loss: 0.8653 (0.9122)  classification_loss: 0.2221 (0.2241)  loss_mask: 0.6448 (0.6882)  time: 0.1649  data: 0.0005  max mem: 4132
[17:23:15.656652] Epoch: [2]  [520/781]  eta: 0:00:43  lr: 0.000133  training_loss: 0.9202 (0.9133)  classification_loss: 0.2234 (0.2241)  loss_mask: 0.6967 (0.6892)  time: 0.1643  data: 0.0003  max mem: 4132
[17:23:18.950047] Epoch: [2]  [540/781]  eta: 0:00:40  lr: 0.000135  training_loss: 0.8808 (0.9121)  classification_loss: 0.2231 (0.2241)  loss_mask: 0.6562 (0.6880)  time: 0.1646  data: 0.0003  max mem: 4132
[17:23:22.237091] Epoch: [2]  [560/781]  eta: 0:00:36  lr: 0.000136  training_loss: 0.8672 (0.9105)  classification_loss: 0.2241 (0.2240)  loss_mask: 0.6469 (0.6865)  time: 0.1642  data: 0.0003  max mem: 4132
[17:23:25.524334] Epoch: [2]  [580/781]  eta: 0:00:33  lr: 0.000137  training_loss: 0.8568 (0.9086)  classification_loss: 0.2231 (0.2240)  loss_mask: 0.6298 (0.6846)  time: 0.1642  data: 0.0002  max mem: 4132
[17:23:28.832136] Epoch: [2]  [600/781]  eta: 0:00:30  lr: 0.000138  training_loss: 0.8358 (0.9070)  classification_loss: 0.2239 (0.2240)  loss_mask: 0.6148 (0.6829)  time: 0.1653  data: 0.0003  max mem: 4132
[17:23:32.155778] Epoch: [2]  [620/781]  eta: 0:00:26  lr: 0.000140  training_loss: 0.7761 (0.9042)  classification_loss: 0.2227 (0.2240)  loss_mask: 0.5477 (0.6802)  time: 0.1660  data: 0.0003  max mem: 4132
[17:23:35.480902] Epoch: [2]  [640/781]  eta: 0:00:23  lr: 0.000141  training_loss: 0.8708 (0.9037)  classification_loss: 0.2230 (0.2240)  loss_mask: 0.6465 (0.6797)  time: 0.1662  data: 0.0003  max mem: 4132
[17:23:38.789476] Epoch: [2]  [660/781]  eta: 0:00:20  lr: 0.000142  training_loss: 0.8578 (0.9022)  classification_loss: 0.2235 (0.2240)  loss_mask: 0.6381 (0.6782)  time: 0.1653  data: 0.0004  max mem: 4132
[17:23:42.098945] Epoch: [2]  [680/781]  eta: 0:00:16  lr: 0.000144  training_loss: 0.8695 (0.9016)  classification_loss: 0.2235 (0.2240)  loss_mask: 0.6439 (0.6776)  time: 0.1653  data: 0.0003  max mem: 4132
[17:23:45.433472] Epoch: [2]  [700/781]  eta: 0:00:13  lr: 0.000145  training_loss: 0.9323 (0.9032)  classification_loss: 0.2227 (0.2239)  loss_mask: 0.7087 (0.6793)  time: 0.1666  data: 0.0003  max mem: 4132
[17:23:48.766189] Epoch: [2]  [720/781]  eta: 0:00:10  lr: 0.000146  training_loss: 0.9384 (0.9046)  classification_loss: 0.2234 (0.2240)  loss_mask: 0.7122 (0.6807)  time: 0.1665  data: 0.0003  max mem: 4132
[17:23:52.111775] Epoch: [2]  [740/781]  eta: 0:00:06  lr: 0.000147  training_loss: 0.8378 (0.9035)  classification_loss: 0.2241 (0.2240)  loss_mask: 0.6133 (0.6795)  time: 0.1671  data: 0.0005  max mem: 4132
[17:23:55.397720] Epoch: [2]  [760/781]  eta: 0:00:03  lr: 0.000149  training_loss: 0.9145 (0.9037)  classification_loss: 0.2252 (0.2240)  loss_mask: 0.6872 (0.6797)  time: 0.1642  data: 0.0003  max mem: 4132
[17:23:58.694493] Epoch: [2]  [780/781]  eta: 0:00:00  lr: 0.000150  training_loss: 0.9213 (0.9039)  classification_loss: 0.2241 (0.2240)  loss_mask: 0.6971 (0.6799)  time: 0.1647  data: 0.0002  max mem: 4132
[17:23:58.866560] Epoch: [2] Total time: 0:02:10 (0.1665 s / it)
[17:23:58.868019] Averaged stats: lr: 0.000150  training_loss: 0.9213 (0.9039)  classification_loss: 0.2241 (0.2240)  loss_mask: 0.6971 (0.6799)
[17:23:59.573132] Test:  [  0/157]  eta: 0:01:49  testing_loss: 2.0769 (2.0769)  acc1: 34.3750 (34.3750)  acc5: 81.2500 (81.2500)  time: 0.6998  data: 0.6545  max mem: 4132
[17:23:59.870648] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 2.1357 (2.1298)  acc1: 21.8750 (22.3011)  acc5: 75.0000 (75.4261)  time: 0.0905  data: 0.0597  max mem: 4132
[17:24:00.165028] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 2.1372 (2.1305)  acc1: 21.8750 (23.0655)  acc5: 70.3125 (74.1815)  time: 0.0294  data: 0.0002  max mem: 4132
[17:24:00.454382] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 2.1216 (2.1231)  acc1: 23.4375 (24.0927)  acc5: 70.3125 (74.4456)  time: 0.0290  data: 0.0002  max mem: 4132
[17:24:00.745167] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 2.1082 (2.1240)  acc1: 23.4375 (23.7043)  acc5: 73.4375 (74.1235)  time: 0.0288  data: 0.0002  max mem: 4132
[17:24:01.037235] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 2.1309 (2.1249)  acc1: 21.8750 (23.8358)  acc5: 73.4375 (74.1728)  time: 0.0289  data: 0.0003  max mem: 4132
[17:24:01.325566] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 2.1218 (2.1242)  acc1: 25.0000 (24.0779)  acc5: 75.0000 (74.4109)  time: 0.0288  data: 0.0003  max mem: 4132
[17:24:01.615697] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 2.1149 (2.1228)  acc1: 25.0000 (24.0977)  acc5: 75.0000 (74.5158)  time: 0.0288  data: 0.0002  max mem: 4132
[17:24:01.909284] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 2.1128 (2.1235)  acc1: 23.4375 (23.8619)  acc5: 75.0000 (74.4020)  time: 0.0290  data: 0.0003  max mem: 4132
[17:24:02.197255] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 2.1392 (2.1262)  acc1: 20.3125 (23.4547)  acc5: 73.4375 (74.1758)  time: 0.0289  data: 0.0003  max mem: 4132
[17:24:02.491011] Test:  [100/157]  eta: 0:00:02  testing_loss: 2.1367 (2.1258)  acc1: 21.8750 (23.3756)  acc5: 75.0000 (74.2110)  time: 0.0289  data: 0.0002  max mem: 4132
[17:24:02.782699] Test:  [110/157]  eta: 0:00:01  testing_loss: 2.1297 (2.1269)  acc1: 21.8750 (23.4657)  acc5: 73.4375 (74.1695)  time: 0.0291  data: 0.0003  max mem: 4132
[17:24:03.070592] Test:  [120/157]  eta: 0:00:01  testing_loss: 2.1228 (2.1255)  acc1: 25.0000 (23.7216)  acc5: 73.4375 (74.2381)  time: 0.0288  data: 0.0003  max mem: 4132
[17:24:03.360765] Test:  [130/157]  eta: 0:00:00  testing_loss: 2.1250 (2.1261)  acc1: 25.0000 (23.5568)  acc5: 75.0000 (74.3082)  time: 0.0288  data: 0.0002  max mem: 4132
[17:24:03.649828] Test:  [140/157]  eta: 0:00:00  testing_loss: 2.1250 (2.1248)  acc1: 21.8750 (23.6924)  acc5: 76.5625 (74.5124)  time: 0.0288  data: 0.0002  max mem: 4132
[17:24:03.934543] Test:  [150/157]  eta: 0:00:00  testing_loss: 2.1119 (2.1245)  acc1: 25.0000 (23.6548)  acc5: 76.5625 (74.5344)  time: 0.0285  data: 0.0002  max mem: 4132
[17:24:04.088486] Test:  [156/157]  eta: 0:00:00  testing_loss: 2.1075 (2.1240)  acc1: 25.0000 (23.6600)  acc5: 76.5625 (74.6200)  time: 0.0274  data: 0.0002  max mem: 4132
[17:24:04.255043] Test: Total time: 0:00:05 (0.0343 s / it)
[17:24:04.256122] * Acc@1 23.660 Acc@5 74.620 loss 2.124
[17:24:04.256471] Accuracy of the network on the 10000 test images: 23.7%
[17:24:04.256696] Max accuracy: 24.94%
[17:24:04.619333] log_dir: ./output_dir
[17:24:05.540154] Epoch: [3]  [  0/781]  eta: 0:11:57  lr: 0.000150  training_loss: 0.9211 (0.9211)  classification_loss: 0.2200 (0.2200)  loss_mask: 0.7011 (0.7011)  time: 0.9185  data: 0.7308  max mem: 4132
[17:24:08.847183] Epoch: [3]  [ 20/781]  eta: 0:02:33  lr: 0.000151  training_loss: 0.8866 (0.8911)  classification_loss: 0.2224 (0.2230)  loss_mask: 0.6664 (0.6681)  time: 0.1652  data: 0.0003  max mem: 4132
[17:24:12.161213] Epoch: [3]  [ 40/781]  eta: 0:02:16  lr: 0.000153  training_loss: 0.9079 (0.8975)  classification_loss: 0.2244 (0.2235)  loss_mask: 0.6936 (0.6741)  time: 0.1656  data: 0.0003  max mem: 4132
[17:24:15.493978] Epoch: [3]  [ 60/781]  eta: 0:02:08  lr: 0.000154  training_loss: 0.8295 (0.8801)  classification_loss: 0.2233 (0.2234)  loss_mask: 0.6049 (0.6567)  time: 0.1665  data: 0.0004  max mem: 4132
[17:24:18.784982] Epoch: [3]  [ 80/781]  eta: 0:02:02  lr: 0.000155  training_loss: 0.8971 (0.8873)  classification_loss: 0.2247 (0.2236)  loss_mask: 0.6715 (0.6636)  time: 0.1645  data: 0.0003  max mem: 4132
[17:24:22.125269] Epoch: [3]  [100/781]  eta: 0:01:57  lr: 0.000156  training_loss: 0.8992 (0.8877)  classification_loss: 0.2250 (0.2238)  loss_mask: 0.6722 (0.6640)  time: 0.1669  data: 0.0003  max mem: 4132
[17:24:25.401714] Epoch: [3]  [120/781]  eta: 0:01:53  lr: 0.000158  training_loss: 0.8912 (0.8928)  classification_loss: 0.2230 (0.2237)  loss_mask: 0.6679 (0.6690)  time: 0.1637  data: 0.0003  max mem: 4132
[17:24:28.667400] Epoch: [3]  [140/781]  eta: 0:01:49  lr: 0.000159  training_loss: 0.8044 (0.8826)  classification_loss: 0.2236 (0.2236)  loss_mask: 0.5772 (0.6590)  time: 0.1632  data: 0.0002  max mem: 4132
[17:24:31.923785] Epoch: [3]  [160/781]  eta: 0:01:45  lr: 0.000160  training_loss: 0.7782 (0.8716)  classification_loss: 0.2236 (0.2236)  loss_mask: 0.5565 (0.6479)  time: 0.1627  data: 0.0003  max mem: 4132
[17:24:35.213553] Epoch: [3]  [180/781]  eta: 0:01:41  lr: 0.000162  training_loss: 0.8453 (0.8704)  classification_loss: 0.2238 (0.2237)  loss_mask: 0.6261 (0.6467)  time: 0.1644  data: 0.0003  max mem: 4132
[17:24:38.546649] Epoch: [3]  [200/781]  eta: 0:01:38  lr: 0.000163  training_loss: 0.8234 (0.8693)  classification_loss: 0.2233 (0.2237)  loss_mask: 0.6026 (0.6456)  time: 0.1666  data: 0.0004  max mem: 4132
[17:24:41.831214] Epoch: [3]  [220/781]  eta: 0:01:34  lr: 0.000164  training_loss: 0.8355 (0.8690)  classification_loss: 0.2225 (0.2236)  loss_mask: 0.6152 (0.6454)  time: 0.1641  data: 0.0007  max mem: 4132
[17:24:45.114384] Epoch: [3]  [240/781]  eta: 0:01:30  lr: 0.000165  training_loss: 0.8106 (0.8653)  classification_loss: 0.2247 (0.2237)  loss_mask: 0.5855 (0.6416)  time: 0.1641  data: 0.0004  max mem: 4132
[17:24:48.402670] Epoch: [3]  [260/781]  eta: 0:01:27  lr: 0.000167  training_loss: 0.8123 (0.8615)  classification_loss: 0.2241 (0.2237)  loss_mask: 0.5831 (0.6378)  time: 0.1643  data: 0.0004  max mem: 4132
[17:24:51.699475] Epoch: [3]  [280/781]  eta: 0:01:23  lr: 0.000168  training_loss: 0.9017 (0.8680)  classification_loss: 0.2263 (0.2239)  loss_mask: 0.6785 (0.6442)  time: 0.1648  data: 0.0004  max mem: 4132
[17:24:55.015377] Epoch: [3]  [300/781]  eta: 0:01:20  lr: 0.000169  training_loss: 0.8957 (0.8706)  classification_loss: 0.2235 (0.2239)  loss_mask: 0.6716 (0.6468)  time: 0.1657  data: 0.0003  max mem: 4132
[17:24:58.313237] Epoch: [3]  [320/781]  eta: 0:01:17  lr: 0.000170  training_loss: 0.8494 (0.8710)  classification_loss: 0.2217 (0.2238)  loss_mask: 0.6210 (0.6472)  time: 0.1648  data: 0.0002  max mem: 4132
[17:25:01.564313] Epoch: [3]  [340/781]  eta: 0:01:13  lr: 0.000172  training_loss: 0.8484 (0.8700)  classification_loss: 0.2239 (0.2238)  loss_mask: 0.6265 (0.6462)  time: 0.1625  data: 0.0003  max mem: 4132
[17:25:04.858732] Epoch: [3]  [360/781]  eta: 0:01:10  lr: 0.000173  training_loss: 0.7891 (0.8681)  classification_loss: 0.2236 (0.2238)  loss_mask: 0.5641 (0.6443)  time: 0.1646  data: 0.0003  max mem: 4132
[17:25:08.162696] Epoch: [3]  [380/781]  eta: 0:01:06  lr: 0.000174  training_loss: 0.8504 (0.8693)  classification_loss: 0.2238 (0.2239)  loss_mask: 0.6288 (0.6454)  time: 0.1651  data: 0.0004  max mem: 4132
[17:25:11.455561] Epoch: [3]  [400/781]  eta: 0:01:03  lr: 0.000176  training_loss: 0.8269 (0.8681)  classification_loss: 0.2249 (0.2239)  loss_mask: 0.5979 (0.6441)  time: 0.1646  data: 0.0003  max mem: 4132
[17:25:14.745916] Epoch: [3]  [420/781]  eta: 0:01:00  lr: 0.000177  training_loss: 0.8621 (0.8680)  classification_loss: 0.2252 (0.2240)  loss_mask: 0.6372 (0.6440)  time: 0.1644  data: 0.0003  max mem: 4132
[17:25:18.041081] Epoch: [3]  [440/781]  eta: 0:00:56  lr: 0.000178  training_loss: 0.8872 (0.8682)  classification_loss: 0.2243 (0.2240)  loss_mask: 0.6592 (0.6442)  time: 0.1647  data: 0.0003  max mem: 4132
[17:25:21.347025] Epoch: [3]  [460/781]  eta: 0:00:53  lr: 0.000179  training_loss: 0.8078 (0.8654)  classification_loss: 0.2248 (0.2240)  loss_mask: 0.5858 (0.6414)  time: 0.1652  data: 0.0003  max mem: 4132
[17:25:24.666336] Epoch: [3]  [480/781]  eta: 0:00:50  lr: 0.000181  training_loss: 0.8205 (0.8648)  classification_loss: 0.2252 (0.2240)  loss_mask: 0.5923 (0.6408)  time: 0.1659  data: 0.0003  max mem: 4132
[17:25:27.960415] Epoch: [3]  [500/781]  eta: 0:00:46  lr: 0.000182  training_loss: 0.8158 (0.8632)  classification_loss: 0.2233 (0.2240)  loss_mask: 0.5902 (0.6392)  time: 0.1646  data: 0.0004  max mem: 4132
[17:25:31.228028] Epoch: [3]  [520/781]  eta: 0:00:43  lr: 0.000183  training_loss: 0.7872 (0.8611)  classification_loss: 0.2237 (0.2240)  loss_mask: 0.5659 (0.6371)  time: 0.1633  data: 0.0003  max mem: 4132
[17:25:34.513007] Epoch: [3]  [540/781]  eta: 0:00:40  lr: 0.000185  training_loss: 0.7972 (0.8590)  classification_loss: 0.2233 (0.2240)  loss_mask: 0.5738 (0.6349)  time: 0.1642  data: 0.0003  max mem: 4132
[17:25:37.842908] Epoch: [3]  [560/781]  eta: 0:00:36  lr: 0.000186  training_loss: 0.7757 (0.8566)  classification_loss: 0.2216 (0.2240)  loss_mask: 0.5506 (0.6326)  time: 0.1664  data: 0.0004  max mem: 4132
[17:25:41.136515] Epoch: [3]  [580/781]  eta: 0:00:33  lr: 0.000187  training_loss: 0.7899 (0.8545)  classification_loss: 0.2245 (0.2240)  loss_mask: 0.5732 (0.6305)  time: 0.1646  data: 0.0004  max mem: 4132
[17:25:44.437879] Epoch: [3]  [600/781]  eta: 0:00:30  lr: 0.000188  training_loss: 0.7963 (0.8533)  classification_loss: 0.2237 (0.2240)  loss_mask: 0.5675 (0.6293)  time: 0.1650  data: 0.0003  max mem: 4132
[17:25:47.707718] Epoch: [3]  [620/781]  eta: 0:00:26  lr: 0.000190  training_loss: 0.7400 (0.8502)  classification_loss: 0.2237 (0.2240)  loss_mask: 0.5151 (0.6262)  time: 0.1634  data: 0.0003  max mem: 4132
[17:25:50.979651] Epoch: [3]  [640/781]  eta: 0:00:23  lr: 0.000191  training_loss: 0.8299 (0.8498)  classification_loss: 0.2250 (0.2240)  loss_mask: 0.6061 (0.6258)  time: 0.1635  data: 0.0004  max mem: 4132
[17:25:54.278085] Epoch: [3]  [660/781]  eta: 0:00:20  lr: 0.000192  training_loss: 0.8018 (0.8481)  classification_loss: 0.2245 (0.2240)  loss_mask: 0.5799 (0.6241)  time: 0.1648  data: 0.0003  max mem: 4132
[17:25:57.580403] Epoch: [3]  [680/781]  eta: 0:00:16  lr: 0.000194  training_loss: 0.7932 (0.8477)  classification_loss: 0.2235 (0.2240)  loss_mask: 0.5683 (0.6237)  time: 0.1650  data: 0.0003  max mem: 4132
[17:26:00.889116] Epoch: [3]  [700/781]  eta: 0:00:13  lr: 0.000195  training_loss: 0.9134 (0.8503)  classification_loss: 0.2234 (0.2240)  loss_mask: 0.6970 (0.6263)  time: 0.1653  data: 0.0003  max mem: 4132
[17:26:04.184353] Epoch: [3]  [720/781]  eta: 0:00:10  lr: 0.000196  training_loss: 0.8576 (0.8508)  classification_loss: 0.2249 (0.2240)  loss_mask: 0.6322 (0.6267)  time: 0.1646  data: 0.0004  max mem: 4132
[17:26:07.481381] Epoch: [3]  [740/781]  eta: 0:00:06  lr: 0.000197  training_loss: 0.7975 (0.8495)  classification_loss: 0.2247 (0.2240)  loss_mask: 0.5765 (0.6254)  time: 0.1648  data: 0.0004  max mem: 4132
[17:26:10.792374] Epoch: [3]  [760/781]  eta: 0:00:03  lr: 0.000199  training_loss: 0.7717 (0.8475)  classification_loss: 0.2250 (0.2240)  loss_mask: 0.5471 (0.6235)  time: 0.1654  data: 0.0003  max mem: 4132
[17:26:14.115752] Epoch: [3]  [780/781]  eta: 0:00:00  lr: 0.000200  training_loss: 0.7723 (0.8459)  classification_loss: 0.2242 (0.2240)  loss_mask: 0.5446 (0.6219)  time: 0.1661  data: 0.0003  max mem: 4132
[17:26:14.296865] Epoch: [3] Total time: 0:02:09 (0.1660 s / it)
[17:26:14.297650] Averaged stats: lr: 0.000200  training_loss: 0.7723 (0.8459)  classification_loss: 0.2242 (0.2240)  loss_mask: 0.5446 (0.6219)
[17:26:15.018825] Test:  [  0/157]  eta: 0:01:52  testing_loss: 2.0783 (2.0783)  acc1: 28.1250 (28.1250)  acc5: 79.6875 (79.6875)  time: 0.7151  data: 0.6839  max mem: 4132
[17:26:15.307025] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 2.1458 (2.1282)  acc1: 21.8750 (23.2955)  acc5: 76.5625 (76.1364)  time: 0.0910  data: 0.0624  max mem: 4132
[17:26:15.597103] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 2.1458 (2.1300)  acc1: 21.8750 (22.6190)  acc5: 73.4375 (75.1488)  time: 0.0287  data: 0.0002  max mem: 4132
[17:26:15.902434] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 2.1228 (2.1245)  acc1: 21.8750 (23.4375)  acc5: 73.4375 (75.3024)  time: 0.0295  data: 0.0002  max mem: 4132
[17:26:16.201370] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 2.1192 (2.1254)  acc1: 23.4375 (23.7043)  acc5: 76.5625 (75.1905)  time: 0.0301  data: 0.0003  max mem: 4132
[17:26:16.493062] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 2.1374 (2.1271)  acc1: 23.4375 (23.3150)  acc5: 75.0000 (75.2451)  time: 0.0294  data: 0.0003  max mem: 4132
[17:26:16.779621] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 2.1265 (2.1268)  acc1: 23.4375 (23.6680)  acc5: 75.0000 (75.4098)  time: 0.0288  data: 0.0002  max mem: 4132
[17:26:17.064883] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 2.1170 (2.1256)  acc1: 25.0000 (23.6136)  acc5: 78.1250 (75.7482)  time: 0.0285  data: 0.0002  max mem: 4132
[17:26:17.351578] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 2.1164 (2.1265)  acc1: 21.8750 (23.3603)  acc5: 76.5625 (75.6173)  time: 0.0285  data: 0.0002  max mem: 4132
[17:26:17.640380] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 2.1437 (2.1288)  acc1: 21.8750 (23.0769)  acc5: 75.0000 (75.4464)  time: 0.0286  data: 0.0002  max mem: 4132
[17:26:17.940867] Test:  [100/157]  eta: 0:00:02  testing_loss: 2.1364 (2.1291)  acc1: 20.3125 (22.8651)  acc5: 75.0000 (75.4641)  time: 0.0293  data: 0.0003  max mem: 4132
[17:26:18.242573] Test:  [110/157]  eta: 0:00:01  testing_loss: 2.1322 (2.1298)  acc1: 21.8750 (22.9307)  acc5: 75.0000 (75.4645)  time: 0.0299  data: 0.0003  max mem: 4132
[17:26:18.530875] Test:  [120/157]  eta: 0:00:01  testing_loss: 2.1198 (2.1285)  acc1: 21.8750 (22.9855)  acc5: 75.0000 (75.4132)  time: 0.0293  data: 0.0003  max mem: 4132
[17:26:18.828856] Test:  [130/157]  eta: 0:00:00  testing_loss: 2.1280 (2.1289)  acc1: 21.8750 (23.0558)  acc5: 75.0000 (75.3817)  time: 0.0292  data: 0.0003  max mem: 4132
[17:26:19.123589] Test:  [140/157]  eta: 0:00:00  testing_loss: 2.1230 (2.1278)  acc1: 23.4375 (23.1272)  acc5: 78.1250 (75.4987)  time: 0.0295  data: 0.0002  max mem: 4132
[17:26:19.409199] Test:  [150/157]  eta: 0:00:00  testing_loss: 2.1146 (2.1274)  acc1: 21.8750 (23.0857)  acc5: 76.5625 (75.5174)  time: 0.0289  data: 0.0002  max mem: 4132
[17:26:19.563000] Test:  [156/157]  eta: 0:00:00  testing_loss: 2.1169 (2.1269)  acc1: 21.8750 (23.0000)  acc5: 76.5625 (75.6000)  time: 0.0275  data: 0.0002  max mem: 4132
[17:26:19.744015] Test: Total time: 0:00:05 (0.0347 s / it)
[17:26:19.744500] * Acc@1 23.000 Acc@5 75.600 loss 2.127
[17:26:19.744834] Accuracy of the network on the 10000 test images: 23.0%
[17:26:19.745036] Max accuracy: 24.94%
[17:26:20.010801] log_dir: ./output_dir
[17:26:20.949292] Epoch: [4]  [  0/781]  eta: 0:12:11  lr: 0.000200  training_loss: 0.7457 (0.7457)  classification_loss: 0.2229 (0.2229)  loss_mask: 0.5227 (0.5227)  time: 0.9363  data: 0.7535  max mem: 4132
[17:26:24.258379] Epoch: [4]  [ 20/781]  eta: 0:02:33  lr: 0.000201  training_loss: 0.7967 (0.7761)  classification_loss: 0.2246 (0.2246)  loss_mask: 0.5722 (0.5515)  time: 0.1653  data: 0.0003  max mem: 4132
[17:26:27.570323] Epoch: [4]  [ 40/781]  eta: 0:02:16  lr: 0.000203  training_loss: 0.7918 (0.7869)  classification_loss: 0.2236 (0.2241)  loss_mask: 0.5717 (0.5628)  time: 0.1655  data: 0.0003  max mem: 4132
[17:26:30.887657] Epoch: [4]  [ 60/781]  eta: 0:02:08  lr: 0.000204  training_loss: 0.7944 (0.7947)  classification_loss: 0.2237 (0.2242)  loss_mask: 0.5679 (0.5705)  time: 0.1658  data: 0.0003  max mem: 4132
[17:26:34.175889] Epoch: [4]  [ 80/781]  eta: 0:02:02  lr: 0.000205  training_loss: 0.7121 (0.7761)  classification_loss: 0.2268 (0.2245)  loss_mask: 0.4852 (0.5516)  time: 0.1643  data: 0.0002  max mem: 4132
[17:26:37.509777] Epoch: [4]  [100/781]  eta: 0:01:57  lr: 0.000206  training_loss: 0.7590 (0.7776)  classification_loss: 0.2251 (0.2244)  loss_mask: 0.5423 (0.5532)  time: 0.1666  data: 0.0003  max mem: 4132
[17:26:40.836405] Epoch: [4]  [120/781]  eta: 0:01:53  lr: 0.000208  training_loss: 0.9313 (0.8037)  classification_loss: 0.2238 (0.2243)  loss_mask: 0.7074 (0.5793)  time: 0.1662  data: 0.0004  max mem: 4132
[17:26:44.160132] Epoch: [4]  [140/781]  eta: 0:01:49  lr: 0.000209  training_loss: 0.7998 (0.8033)  classification_loss: 0.2230 (0.2241)  loss_mask: 0.5752 (0.5792)  time: 0.1661  data: 0.0003  max mem: 4132
[17:26:47.493897] Epoch: [4]  [160/781]  eta: 0:01:45  lr: 0.000210  training_loss: 0.8424 (0.8076)  classification_loss: 0.2223 (0.2240)  loss_mask: 0.6178 (0.5836)  time: 0.1666  data: 0.0003  max mem: 4132
[17:26:50.811088] Epoch: [4]  [180/781]  eta: 0:01:42  lr: 0.000212  training_loss: 0.8943 (0.8179)  classification_loss: 0.2255 (0.2241)  loss_mask: 0.6682 (0.5938)  time: 0.1658  data: 0.0003  max mem: 4132
[17:26:54.108688] Epoch: [4]  [200/781]  eta: 0:01:38  lr: 0.000213  training_loss: 0.8820 (0.8241)  classification_loss: 0.2229 (0.2240)  loss_mask: 0.6629 (0.6002)  time: 0.1648  data: 0.0003  max mem: 4132
[17:26:57.414639] Epoch: [4]  [220/781]  eta: 0:01:34  lr: 0.000214  training_loss: 0.7652 (0.8182)  classification_loss: 0.2247 (0.2240)  loss_mask: 0.5437 (0.5942)  time: 0.1652  data: 0.0003  max mem: 4132
[17:27:00.730646] Epoch: [4]  [240/781]  eta: 0:01:31  lr: 0.000215  training_loss: 0.7560 (0.8131)  classification_loss: 0.2244 (0.2241)  loss_mask: 0.5205 (0.5890)  time: 0.1657  data: 0.0003  max mem: 4132
[17:27:03.992412] Epoch: [4]  [260/781]  eta: 0:01:27  lr: 0.000217  training_loss: 0.8846 (0.8199)  classification_loss: 0.2247 (0.2242)  loss_mask: 0.6565 (0.5957)  time: 0.1630  data: 0.0002  max mem: 4132
[17:27:07.275884] Epoch: [4]  [280/781]  eta: 0:01:24  lr: 0.000218  training_loss: 0.7618 (0.8185)  classification_loss: 0.2230 (0.2242)  loss_mask: 0.5421 (0.5942)  time: 0.1641  data: 0.0003  max mem: 4132
[17:27:10.595958] Epoch: [4]  [300/781]  eta: 0:01:20  lr: 0.000219  training_loss: 0.7209 (0.8135)  classification_loss: 0.2252 (0.2243)  loss_mask: 0.4966 (0.5892)  time: 0.1659  data: 0.0003  max mem: 4132
[17:27:13.950332] Epoch: [4]  [320/781]  eta: 0:01:17  lr: 0.000220  training_loss: 0.7263 (0.8075)  classification_loss: 0.2236 (0.2243)  loss_mask: 0.4986 (0.5832)  time: 0.1676  data: 0.0005  max mem: 4132
[17:27:17.284873] Epoch: [4]  [340/781]  eta: 0:01:14  lr: 0.000222  training_loss: 0.7293 (0.8039)  classification_loss: 0.2253 (0.2243)  loss_mask: 0.5080 (0.5796)  time: 0.1666  data: 0.0004  max mem: 4132
[17:27:20.679223] Epoch: [4]  [360/781]  eta: 0:01:10  lr: 0.000223  training_loss: 0.8379 (0.8063)  classification_loss: 0.2248 (0.2244)  loss_mask: 0.6126 (0.5819)  time: 0.1696  data: 0.0004  max mem: 4132
[17:27:23.972332] Epoch: [4]  [380/781]  eta: 0:01:07  lr: 0.000224  training_loss: 0.7636 (0.8055)  classification_loss: 0.2253 (0.2244)  loss_mask: 0.5426 (0.5811)  time: 0.1645  data: 0.0003  max mem: 4132
[17:27:27.299063] Epoch: [4]  [400/781]  eta: 0:01:03  lr: 0.000226  training_loss: 0.7389 (0.8032)  classification_loss: 0.2260 (0.2244)  loss_mask: 0.5169 (0.5787)  time: 0.1662  data: 0.0003  max mem: 4132
[17:27:30.604389] Epoch: [4]  [420/781]  eta: 0:01:00  lr: 0.000227  training_loss: 0.7607 (0.8025)  classification_loss: 0.2243 (0.2244)  loss_mask: 0.5354 (0.5781)  time: 0.1652  data: 0.0003  max mem: 4132
[17:27:33.918905] Epoch: [4]  [440/781]  eta: 0:00:57  lr: 0.000228  training_loss: 0.7293 (0.8011)  classification_loss: 0.2248 (0.2245)  loss_mask: 0.5044 (0.5767)  time: 0.1656  data: 0.0003  max mem: 4132
[17:27:37.169472] Epoch: [4]  [460/781]  eta: 0:00:53  lr: 0.000229  training_loss: 0.6873 (0.7971)  classification_loss: 0.2231 (0.2244)  loss_mask: 0.4653 (0.5727)  time: 0.1624  data: 0.0002  max mem: 4132
[17:27:40.433159] Epoch: [4]  [480/781]  eta: 0:00:50  lr: 0.000231  training_loss: 0.7725 (0.7966)  classification_loss: 0.2263 (0.2245)  loss_mask: 0.5449 (0.5722)  time: 0.1631  data: 0.0003  max mem: 4132
[17:27:43.713947] Epoch: [4]  [500/781]  eta: 0:00:46  lr: 0.000232  training_loss: 0.8818 (0.8008)  classification_loss: 0.2233 (0.2244)  loss_mask: 0.6612 (0.5764)  time: 0.1639  data: 0.0004  max mem: 4132
[17:27:47.017837] Epoch: [4]  [520/781]  eta: 0:00:43  lr: 0.000233  training_loss: 0.8078 (0.8008)  classification_loss: 0.2237 (0.2244)  loss_mask: 0.5807 (0.5764)  time: 0.1651  data: 0.0003  max mem: 4132
[17:27:50.346475] Epoch: [4]  [540/781]  eta: 0:00:40  lr: 0.000235  training_loss: 0.7453 (0.7989)  classification_loss: 0.2253 (0.2245)  loss_mask: 0.5210 (0.5744)  time: 0.1663  data: 0.0003  max mem: 4132
[17:27:53.636186] Epoch: [4]  [560/781]  eta: 0:00:36  lr: 0.000236  training_loss: 0.7269 (0.7965)  classification_loss: 0.2228 (0.2244)  loss_mask: 0.5033 (0.5721)  time: 0.1644  data: 0.0003  max mem: 4132
[17:27:56.939700] Epoch: [4]  [580/781]  eta: 0:00:33  lr: 0.000237  training_loss: 0.7514 (0.7954)  classification_loss: 0.2232 (0.2244)  loss_mask: 0.5257 (0.5710)  time: 0.1651  data: 0.0004  max mem: 4132
[17:28:00.242857] Epoch: [4]  [600/781]  eta: 0:00:30  lr: 0.000238  training_loss: 0.8030 (0.7961)  classification_loss: 0.2245 (0.2244)  loss_mask: 0.5799 (0.5717)  time: 0.1651  data: 0.0003  max mem: 4132
[17:28:03.549772] Epoch: [4]  [620/781]  eta: 0:00:26  lr: 0.000240  training_loss: 0.7935 (0.7967)  classification_loss: 0.2243 (0.2244)  loss_mask: 0.5696 (0.5724)  time: 0.1653  data: 0.0003  max mem: 4132
[17:28:06.846745] Epoch: [4]  [640/781]  eta: 0:00:23  lr: 0.000241  training_loss: 0.7822 (0.7962)  classification_loss: 0.2234 (0.2244)  loss_mask: 0.5550 (0.5718)  time: 0.1647  data: 0.0003  max mem: 4132
[17:28:10.136322] Epoch: [4]  [660/781]  eta: 0:00:20  lr: 0.000242  training_loss: 0.7057 (0.7940)  classification_loss: 0.2243 (0.2244)  loss_mask: 0.4798 (0.5697)  time: 0.1644  data: 0.0003  max mem: 4132
[17:28:13.468639] Epoch: [4]  [680/781]  eta: 0:00:16  lr: 0.000244  training_loss: 0.7627 (0.7924)  classification_loss: 0.2246 (0.2244)  loss_mask: 0.5380 (0.5681)  time: 0.1665  data: 0.0003  max mem: 4132
[17:28:16.785686] Epoch: [4]  [700/781]  eta: 0:00:13  lr: 0.000245  training_loss: 0.9066 (0.7961)  classification_loss: 0.2229 (0.2244)  loss_mask: 0.6867 (0.5717)  time: 0.1657  data: 0.0003  max mem: 4132
[17:28:20.074185] Epoch: [4]  [720/781]  eta: 0:00:10  lr: 0.000246  training_loss: 0.7453 (0.7949)  classification_loss: 0.2250 (0.2244)  loss_mask: 0.5201 (0.5705)  time: 0.1643  data: 0.0005  max mem: 4132
[17:28:23.358123] Epoch: [4]  [740/781]  eta: 0:00:06  lr: 0.000247  training_loss: 0.6986 (0.7928)  classification_loss: 0.2262 (0.2245)  loss_mask: 0.4720 (0.5683)  time: 0.1641  data: 0.0003  max mem: 4132
[17:28:26.651329] Epoch: [4]  [760/781]  eta: 0:00:03  lr: 0.000249  training_loss: 0.7104 (0.7907)  classification_loss: 0.2238 (0.2245)  loss_mask: 0.4853 (0.5663)  time: 0.1646  data: 0.0003  max mem: 4132
[17:28:29.925091] Epoch: [4]  [780/781]  eta: 0:00:00  lr: 0.000250  training_loss: 0.6858 (0.7883)  classification_loss: 0.2246 (0.2245)  loss_mask: 0.4628 (0.5638)  time: 0.1636  data: 0.0002  max mem: 4132
[17:28:30.084100] Epoch: [4] Total time: 0:02:10 (0.1665 s / it)
[17:28:30.086054] Averaged stats: lr: 0.000250  training_loss: 0.6858 (0.7883)  classification_loss: 0.2246 (0.2245)  loss_mask: 0.4628 (0.5638)
[17:28:30.797783] Test:  [  0/157]  eta: 0:01:51  testing_loss: 2.1146 (2.1146)  acc1: 21.8750 (21.8750)  acc5: 76.5625 (76.5625)  time: 0.7073  data: 0.6741  max mem: 4132
[17:28:31.094988] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 2.1592 (2.1489)  acc1: 20.3125 (18.1818)  acc5: 75.0000 (75.4261)  time: 0.0912  data: 0.0614  max mem: 4132
[17:28:31.392509] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 2.1624 (2.1491)  acc1: 17.1875 (18.6012)  acc5: 71.8750 (74.3304)  time: 0.0295  data: 0.0003  max mem: 4132
[17:28:31.690751] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 2.1436 (2.1449)  acc1: 20.3125 (19.4052)  acc5: 73.4375 (74.5464)  time: 0.0296  data: 0.0003  max mem: 4132
[17:28:31.988281] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 2.1404 (2.1440)  acc1: 20.3125 (20.1220)  acc5: 75.0000 (73.9710)  time: 0.0295  data: 0.0004  max mem: 4132
[17:28:32.290706] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 2.1493 (2.1469)  acc1: 18.7500 (19.5772)  acc5: 73.4375 (73.8358)  time: 0.0297  data: 0.0006  max mem: 4132
[17:28:32.581458] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 2.1508 (2.1467)  acc1: 17.1875 (19.4416)  acc5: 73.4375 (73.8473)  time: 0.0295  data: 0.0005  max mem: 4132
[17:28:32.873310] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 2.1404 (2.1454)  acc1: 18.7500 (19.1901)  acc5: 75.0000 (74.0317)  time: 0.0290  data: 0.0002  max mem: 4132
[17:28:33.172961] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 2.1413 (2.1475)  acc1: 17.1875 (18.8657)  acc5: 75.0000 (73.8233)  time: 0.0294  data: 0.0003  max mem: 4132
[17:28:33.471193] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 2.1687 (2.1498)  acc1: 15.6250 (18.4753)  acc5: 71.8750 (73.6607)  time: 0.0297  data: 0.0003  max mem: 4132
[17:28:33.772991] Test:  [100/157]  eta: 0:00:02  testing_loss: 2.1432 (2.1500)  acc1: 18.7500 (18.5489)  acc5: 75.0000 (73.6231)  time: 0.0298  data: 0.0002  max mem: 4132
[17:28:34.075038] Test:  [110/157]  eta: 0:00:01  testing_loss: 2.1421 (2.1500)  acc1: 18.7500 (18.6092)  acc5: 75.0000 (73.7753)  time: 0.0300  data: 0.0003  max mem: 4132
[17:28:34.368728] Test:  [120/157]  eta: 0:00:01  testing_loss: 2.1417 (2.1492)  acc1: 17.1875 (18.7371)  acc5: 75.0000 (73.8765)  time: 0.0296  data: 0.0003  max mem: 4132
[17:28:34.663602] Test:  [130/157]  eta: 0:00:00  testing_loss: 2.1520 (2.1500)  acc1: 18.7500 (18.7142)  acc5: 75.0000 (73.9146)  time: 0.0292  data: 0.0004  max mem: 4132
[17:28:34.948807] Test:  [140/157]  eta: 0:00:00  testing_loss: 2.1472 (2.1487)  acc1: 20.3125 (18.8608)  acc5: 75.0000 (73.9473)  time: 0.0288  data: 0.0003  max mem: 4132
[17:28:35.232256] Test:  [150/157]  eta: 0:00:00  testing_loss: 2.1403 (2.1486)  acc1: 20.3125 (18.9156)  acc5: 75.0000 (73.9963)  time: 0.0283  data: 0.0002  max mem: 4132
[17:28:35.385432] Test:  [156/157]  eta: 0:00:00  testing_loss: 2.1375 (2.1481)  acc1: 20.3125 (18.9200)  acc5: 75.0000 (74.1400)  time: 0.0273  data: 0.0002  max mem: 4132
[17:28:35.553302] Test: Total time: 0:00:05 (0.0348 s / it)
[17:28:35.554579] * Acc@1 18.920 Acc@5 74.140 loss 2.148
[17:28:35.555658] Accuracy of the network on the 10000 test images: 18.9%
[17:28:35.556318] Max accuracy: 24.94%
[17:28:35.828129] log_dir: ./output_dir
[17:28:36.749504] Epoch: [5]  [  0/781]  eta: 0:11:57  lr: 0.000250  training_loss: 0.6337 (0.6337)  classification_loss: 0.2255 (0.2255)  loss_mask: 0.4081 (0.4081)  time: 0.9191  data: 0.7098  max mem: 4132
[17:28:40.017373] Epoch: [5]  [ 20/781]  eta: 0:02:31  lr: 0.000250  training_loss: 0.7593 (0.7666)  classification_loss: 0.2250 (0.2247)  loss_mask: 0.5324 (0.5419)  time: 0.1633  data: 0.0002  max mem: 4132
[17:28:43.306563] Epoch: [5]  [ 40/781]  eta: 0:02:15  lr: 0.000250  training_loss: 0.9088 (0.8520)  classification_loss: 0.2253 (0.2249)  loss_mask: 0.6846 (0.6270)  time: 0.1644  data: 0.0003  max mem: 4132
[17:28:46.593274] Epoch: [5]  [ 60/781]  eta: 0:02:07  lr: 0.000250  training_loss: 0.7353 (0.8225)  classification_loss: 0.2239 (0.2247)  loss_mask: 0.5123 (0.5978)  time: 0.1642  data: 0.0003  max mem: 4132
[17:28:49.890234] Epoch: [5]  [ 80/781]  eta: 0:02:01  lr: 0.000250  training_loss: 0.6878 (0.7937)  classification_loss: 0.2241 (0.2248)  loss_mask: 0.4636 (0.5690)  time: 0.1648  data: 0.0003  max mem: 4132
[17:28:53.217123] Epoch: [5]  [100/781]  eta: 0:01:57  lr: 0.000250  training_loss: 0.7419 (0.7815)  classification_loss: 0.2230 (0.2245)  loss_mask: 0.5133 (0.5570)  time: 0.1663  data: 0.0003  max mem: 4132
[17:28:56.524693] Epoch: [5]  [120/781]  eta: 0:01:52  lr: 0.000250  training_loss: 0.6534 (0.7661)  classification_loss: 0.2234 (0.2243)  loss_mask: 0.4258 (0.5418)  time: 0.1653  data: 0.0003  max mem: 4132
[17:28:59.827948] Epoch: [5]  [140/781]  eta: 0:01:49  lr: 0.000250  training_loss: 0.6479 (0.7497)  classification_loss: 0.2225 (0.2242)  loss_mask: 0.4214 (0.5255)  time: 0.1650  data: 0.0003  max mem: 4132
[17:29:03.139343] Epoch: [5]  [160/781]  eta: 0:01:45  lr: 0.000250  training_loss: 0.6596 (0.7386)  classification_loss: 0.2236 (0.2242)  loss_mask: 0.4383 (0.5145)  time: 0.1655  data: 0.0003  max mem: 4132
[17:29:06.437936] Epoch: [5]  [180/781]  eta: 0:01:41  lr: 0.000250  training_loss: 0.7894 (0.7511)  classification_loss: 0.2256 (0.2244)  loss_mask: 0.5638 (0.5268)  time: 0.1648  data: 0.0003  max mem: 4132
[17:29:09.738536] Epoch: [5]  [200/781]  eta: 0:01:37  lr: 0.000250  training_loss: 0.8052 (0.7606)  classification_loss: 0.2249 (0.2244)  loss_mask: 0.5803 (0.5362)  time: 0.1649  data: 0.0003  max mem: 4132
[17:29:13.049804] Epoch: [5]  [220/781]  eta: 0:01:34  lr: 0.000250  training_loss: 0.6726 (0.7546)  classification_loss: 0.2251 (0.2245)  loss_mask: 0.4472 (0.5301)  time: 0.1654  data: 0.0003  max mem: 4132
[17:29:16.348237] Epoch: [5]  [240/781]  eta: 0:01:30  lr: 0.000250  training_loss: 0.6857 (0.7516)  classification_loss: 0.2240 (0.2246)  loss_mask: 0.4587 (0.5271)  time: 0.1648  data: 0.0003  max mem: 4132
[17:29:19.640151] Epoch: [5]  [260/781]  eta: 0:01:27  lr: 0.000250  training_loss: 0.6310 (0.7465)  classification_loss: 0.2238 (0.2246)  loss_mask: 0.4072 (0.5219)  time: 0.1645  data: 0.0003  max mem: 4132
[17:29:22.946635] Epoch: [5]  [280/781]  eta: 0:01:23  lr: 0.000250  training_loss: 0.6693 (0.7430)  classification_loss: 0.2253 (0.2246)  loss_mask: 0.4454 (0.5184)  time: 0.1652  data: 0.0003  max mem: 4132
[17:29:25.386123] [17:29:25.386622] [17:29:25.386798] [17:29:25.386966] [17:29:25.387142] [17:29:25.387315] [17:29:25.387502] [17:29:25.387773]
Traceback (most recent call last):
  File "/notebooks/CVPR2023/main_two_branch_new.py", line 370, in <module>
    main(args)
  File "/notebooks/CVPR2023/main_two_branch_new.py", line 322, in main
    train_stats = train_one_epoch(
  File "/notebooks/CVPR2023/engine_two_branch.py", line 73, in train_one_epoch
    loss_scaler(loss, optimizer, clip_grad=max_norm,
  File "/notebooks/CVPR2023/util/misc.py", line 258, in __call__
    self._scaler.scale(loss).backward(create_graph=create_graph)
  File "/usr/local/lib/python3.9/dist-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt