Not using distributed mode
[07:23:20.785910] job dir: /notebooks/CVPR2023
[07:23:20.786593] Namespace(batch_size=64,
epochs=100,
accum_iter=1,
model='mae_vit_tiny',
norm_pix_loss=False,
dataset='c10',
input_size=32,
patch_size=2,
mask_ratio=0.75,
lambda_weight=0.1,
drop_path=0.1,
clip_grad=None,
weight_decay=0.05,
lr=None,
blr=0.001,
layer_decay=0.75,
min_lr=1e-06,
warmup_epochs=5,
color_jitter=None,
aa='rand-m9-mstd0.5-inc1',
smoothing=0.1,
reprob=0.25,
remode='pixel',
recount=1,
resplit=False,
mixup=0,
cutmix=0,
cutmix_minmax=None,
mixup_prob=1.0,
mixup_switch_prob=0.5,
mixup_mode='batch',
finetune='',
global_pool=True,
data_path='/datasets01/imagenet_full_size/061417/',
nb_classes=10,
output_dir='./output_dir',
log_dir='./output_dir',
device='cuda',
seed=0,
resume='',
start_epoch=0,
eval=False,
dist_eval=False,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[07:23:21.630468] Files already downloaded and verified
/usr/local/lib/python3.9/dist-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
[07:23:22.409304] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCropAndInterpolation(size=(32, 32), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic)
               RandomHorizontalFlip(p=0.5)
               RandAugment(n=2, ops=
           	AugmentOp(name=AutoContrast, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Equalize, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Invert, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Rotate, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=PosterizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeAdd, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ColorIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ContrastIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=BrightnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SharpnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearX, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearY, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateXRel, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateYRel, p=0.5, m=9, mstd=0.5))
               ToTensor()
               Normalize(mean=tensor([0.4914, 0.4822, 0.4465]), std=tensor([0.2023, 0.1994, 0.2010]))
               RandomErasing(p=0.25, mode=pixel, count=(1, 1))
           )
[07:23:22.833898] Files already downloaded and verified
[07:23:23.253591] Dataset CIFAR10
    Number of datapoints: 10000
    Root location: ./data
    Split: Test
    StandardTransform
Transform: Compose(
               Resize(size=36, interpolation=bicubic, max_size=None, antialias=None)
               CenterCrop(size=(32, 32))
               ToTensor()
               Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.201))
           )
[07:23:23.254238] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7fb8e0c22700>
/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[07:23:34.692699] Model = MaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(2, 2), stride=(2, 2))
    (norm): Identity()
  )
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (7): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (8): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (9): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (10): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (11): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
  (decoder_embed): Linear(in_features=192, out_features=128, bias=True)
  (decoder_blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=128, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=128, out_features=128, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=512, out_features=128, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=128, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=128, out_features=128, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU(approximate=none)
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=512, out_features=128, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (decoder_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
  (decoder_pred): Linear(in_features=128, out_features=12, bias=True)
  (head): Linear(in_features=192, out_features=10, bias=True)
  (classifier_mask): Sequential(
    (0): Linear(in_features=192, out_features=5, bias=True)
    (1): LogSoftmax(dim=1)
  )
)
[07:23:34.695315] number of params (M): 5.77
[07:23:34.696097] base lr: 1.00e-03
[07:23:34.696426] actual lr: 2.50e-04
[07:23:34.696884] accumulate grad iterations: 1
[07:23:34.697215] effective batch size: 64
[07:23:34.699163] criterion = LabelSmoothingCrossEntropy()
[07:23:34.699477] Start training for 100 epochs
[07:23:34.701262] log_dir: ./output_dir
[07:23:40.608581] Epoch: [0]  [  0/781]  eta: 1:16:51  lr: 0.000000  training_loss: 3.6978 (3.6978)  mae_loss: 3.4323 (3.4323)  classification_loss: 0.2656 (0.2656)  time: 5.9040  data: 0.4037  max mem: 5446
[07:23:44.512416] Epoch: [0]  [ 20/781]  eta: 0:05:55  lr: 0.000001  training_loss: 3.7088 (3.7211)  mae_loss: 3.4387 (3.4594)  classification_loss: 0.2630 (0.2616)  time: 0.1951  data: 0.0002  max mem: 5508
[07:23:48.371085] Epoch: [0]  [ 40/781]  eta: 0:04:06  lr: 0.000003  training_loss: 2.9571 (3.3890)  mae_loss: 2.7011 (3.1282)  classification_loss: 0.2582 (0.2609)  time: 0.1929  data: 0.0002  max mem: 5508
[07:23:52.279768] Epoch: [0]  [ 60/781]  eta: 0:03:27  lr: 0.000004  training_loss: 2.3543 (3.0426)  mae_loss: 2.0919 (2.7818)  classification_loss: 0.2626 (0.2608)  time: 0.1954  data: 0.0002  max mem: 5508
[07:23:56.156266] Epoch: [0]  [ 80/781]  eta: 0:03:05  lr: 0.000005  training_loss: 1.9778 (2.7769)  mae_loss: 1.7072 (2.5166)  classification_loss: 0.2555 (0.2603)  time: 0.1937  data: 0.0003  max mem: 5508
[07:24:00.041442] Epoch: [0]  [100/781]  eta: 0:02:50  lr: 0.000006  training_loss: 1.7049 (2.5716)  mae_loss: 1.4526 (2.3140)  classification_loss: 0.2453 (0.2576)  time: 0.1942  data: 0.0002  max mem: 5508
[07:24:03.910775] Epoch: [0]  [120/781]  eta: 0:02:39  lr: 0.000008  training_loss: 1.6772 (2.4249)  mae_loss: 1.4401 (2.1708)  classification_loss: 0.2358 (0.2541)  time: 0.1934  data: 0.0002  max mem: 5508
[07:24:07.794852] Epoch: [0]  [140/781]  eta: 0:02:30  lr: 0.000009  training_loss: 1.5576 (2.3038)  mae_loss: 1.3304 (2.0528)  classification_loss: 0.2316 (0.2510)  time: 0.1941  data: 0.0002  max mem: 5508
[07:24:11.670648] Epoch: [0]  [160/781]  eta: 0:02:22  lr: 0.000010  training_loss: 1.5031 (2.2078)  mae_loss: 1.2746 (1.9591)  classification_loss: 0.2320 (0.2486)  time: 0.1937  data: 0.0002  max mem: 5508
[07:24:15.538431] Epoch: [0]  [180/781]  eta: 0:02:15  lr: 0.000012  training_loss: 1.5423 (2.1336)  mae_loss: 1.3123 (1.8870)  classification_loss: 0.2300 (0.2466)  time: 0.1932  data: 0.0002  max mem: 5508
[07:24:19.410392] Epoch: [0]  [200/781]  eta: 0:02:09  lr: 0.000013  training_loss: 1.4661 (2.0692)  mae_loss: 1.2359 (1.8244)  classification_loss: 0.2273 (0.2448)  time: 0.1935  data: 0.0002  max mem: 5508
[07:24:23.283709] Epoch: [0]  [220/781]  eta: 0:02:03  lr: 0.000014  training_loss: 1.4799 (2.0150)  mae_loss: 1.2485 (1.7717)  classification_loss: 0.2287 (0.2433)  time: 0.1936  data: 0.0002  max mem: 5508
[07:24:27.195281] Epoch: [0]  [240/781]  eta: 0:01:57  lr: 0.000015  training_loss: 1.4560 (1.9706)  mae_loss: 1.2265 (1.7284)  classification_loss: 0.2288 (0.2422)  time: 0.1955  data: 0.0002  max mem: 5508
[07:24:31.083009] Epoch: [0]  [260/781]  eta: 0:01:52  lr: 0.000017  training_loss: 1.4039 (1.9278)  mae_loss: 1.1731 (1.6867)  classification_loss: 0.2278 (0.2411)  time: 0.1943  data: 0.0006  max mem: 5508
[07:24:34.945664] Epoch: [0]  [280/781]  eta: 0:01:47  lr: 0.000018  training_loss: 1.4210 (1.8914)  mae_loss: 1.1907 (1.6513)  classification_loss: 0.2270 (0.2402)  time: 0.1930  data: 0.0003  max mem: 5508
[07:24:38.869048] Epoch: [0]  [300/781]  eta: 0:01:42  lr: 0.000019  training_loss: 1.3535 (1.8565)  mae_loss: 1.1247 (1.6171)  classification_loss: 0.2287 (0.2393)  time: 0.1961  data: 0.0002  max mem: 5508
[07:24:42.765055] Epoch: [0]  [320/781]  eta: 0:01:37  lr: 0.000020  training_loss: 1.3260 (1.8255)  mae_loss: 1.0964 (1.5869)  classification_loss: 0.2269 (0.2386)  time: 0.1947  data: 0.0003  max mem: 5508
[07:24:46.675118] Epoch: [0]  [340/781]  eta: 0:01:33  lr: 0.000022  training_loss: 1.3112 (1.7967)  mae_loss: 1.0841 (1.5588)  classification_loss: 0.2267 (0.2379)  time: 0.1954  data: 0.0002  max mem: 5508
[07:24:50.565575] Epoch: [0]  [360/781]  eta: 0:01:28  lr: 0.000023  training_loss: 1.2522 (1.7687)  mae_loss: 1.0215 (1.5314)  classification_loss: 0.2259 (0.2372)  time: 0.1944  data: 0.0002  max mem: 5508
[07:24:54.452820] Epoch: [0]  [380/781]  eta: 0:01:23  lr: 0.000024  training_loss: 1.2512 (1.7438)  mae_loss: 1.0204 (1.5072)  classification_loss: 0.2258 (0.2366)  time: 0.1943  data: 0.0002  max mem: 5508
[07:24:58.408099] Epoch: [0]  [400/781]  eta: 0:01:19  lr: 0.000026  training_loss: 1.2829 (1.7220)  mae_loss: 1.0558 (1.4859)  classification_loss: 0.2241 (0.2360)  time: 0.1977  data: 0.0002  max mem: 5508
[07:25:02.340081] Epoch: [0]  [420/781]  eta: 0:01:15  lr: 0.000027  training_loss: 1.2653 (1.7010)  mae_loss: 1.0419 (1.4656)  classification_loss: 0.2253 (0.2355)  time: 0.1965  data: 0.0002  max mem: 5508
[07:25:06.213634] Epoch: [0]  [440/781]  eta: 0:01:10  lr: 0.000028  training_loss: 1.3062 (1.6841)  mae_loss: 1.0819 (1.4491)  classification_loss: 0.2236 (0.2350)  time: 0.1936  data: 0.0003  max mem: 5508
[07:25:10.114171] Epoch: [0]  [460/781]  eta: 0:01:06  lr: 0.000029  training_loss: 1.2887 (1.6671)  mae_loss: 1.0670 (1.4327)  classification_loss: 0.2217 (0.2344)  time: 0.1949  data: 0.0003  max mem: 5508
[07:25:13.993070] Epoch: [0]  [480/781]  eta: 0:01:02  lr: 0.000031  training_loss: 1.2572 (1.6502)  mae_loss: 1.0314 (1.4162)  classification_loss: 0.2246 (0.2340)  time: 0.1939  data: 0.0003  max mem: 5508
[07:25:17.918518] Epoch: [0]  [500/781]  eta: 0:00:57  lr: 0.000032  training_loss: 1.2480 (1.6341)  mae_loss: 1.0203 (1.4005)  classification_loss: 0.2212 (0.2335)  time: 0.1962  data: 0.0003  max mem: 5508
[07:25:21.833574] Epoch: [0]  [520/781]  eta: 0:00:53  lr: 0.000033  training_loss: 1.2694 (1.6215)  mae_loss: 1.0458 (1.3883)  classification_loss: 0.2245 (0.2332)  time: 0.1957  data: 0.0003  max mem: 5508
[07:25:25.730772] Epoch: [0]  [540/781]  eta: 0:00:49  lr: 0.000035  training_loss: 1.2421 (1.6077)  mae_loss: 1.0246 (1.3749)  classification_loss: 0.2236 (0.2329)  time: 0.1947  data: 0.0002  max mem: 5508
[07:25:29.629897] Epoch: [0]  [560/781]  eta: 0:00:45  lr: 0.000036  training_loss: 1.2607 (1.5958)  mae_loss: 1.0396 (1.3632)  classification_loss: 0.2238 (0.2325)  time: 0.1949  data: 0.0002  max mem: 5508
[07:25:33.530773] Epoch: [0]  [580/781]  eta: 0:00:41  lr: 0.000037  training_loss: 1.2367 (1.5836)  mae_loss: 1.0187 (1.3513)  classification_loss: 0.2230 (0.2322)  time: 0.1950  data: 0.0003  max mem: 5508
[07:25:37.423270] Epoch: [0]  [600/781]  eta: 0:00:36  lr: 0.000038  training_loss: 1.2156 (1.5723)  mae_loss: 0.9894 (1.3405)  classification_loss: 0.2222 (0.2319)  time: 0.1945  data: 0.0002  max mem: 5508
[07:25:41.357803] Epoch: [0]  [620/781]  eta: 0:00:32  lr: 0.000040  training_loss: 1.2553 (1.5622)  mae_loss: 1.0449 (1.3306)  classification_loss: 0.2229 (0.2316)  time: 0.1966  data: 0.0002  max mem: 5508
[07:25:45.244412] Epoch: [0]  [640/781]  eta: 0:00:28  lr: 0.000041  training_loss: 1.2244 (1.5518)  mae_loss: 1.0024 (1.3205)  classification_loss: 0.2228 (0.2314)  time: 0.1942  data: 0.0002  max mem: 5508
[07:25:49.170149] Epoch: [0]  [660/781]  eta: 0:00:24  lr: 0.000042  training_loss: 1.2616 (1.5432)  mae_loss: 1.0416 (1.3120)  classification_loss: 0.2236 (0.2311)  time: 0.1962  data: 0.0002  max mem: 5508
[07:25:53.077891] Epoch: [0]  [680/781]  eta: 0:00:20  lr: 0.000044  training_loss: 1.2422 (1.5344)  mae_loss: 1.0199 (1.3035)  classification_loss: 0.2224 (0.2309)  time: 0.1953  data: 0.0002  max mem: 5508
[07:25:56.970352] Epoch: [0]  [700/781]  eta: 0:00:16  lr: 0.000045  training_loss: 1.1789 (1.5247)  mae_loss: 0.9605 (1.2941)  classification_loss: 0.2210 (0.2306)  time: 0.1945  data: 0.0003  max mem: 5508
[07:26:00.877115] Epoch: [0]  [720/781]  eta: 0:00:12  lr: 0.000046  training_loss: 1.2410 (1.5164)  mae_loss: 1.0175 (1.2860)  classification_loss: 0.2247 (0.2304)  time: 0.1953  data: 0.0002  max mem: 5508
[07:26:04.771888] Epoch: [0]  [740/781]  eta: 0:00:08  lr: 0.000047  training_loss: 1.1929 (1.5080)  mae_loss: 0.9615 (1.2778)  classification_loss: 0.2256 (0.2303)  time: 0.1947  data: 0.0002  max mem: 5508
[07:26:08.669415] Epoch: [0]  [760/781]  eta: 0:00:04  lr: 0.000049  training_loss: 1.1767 (1.4995)  mae_loss: 0.9545 (1.2694)  classification_loss: 0.2223 (0.2301)  time: 0.1948  data: 0.0002  max mem: 5508
[07:26:12.592480] Epoch: [0]  [780/781]  eta: 0:00:00  lr: 0.000050  training_loss: 1.2213 (1.4923)  mae_loss: 0.9868 (1.2624)  classification_loss: 0.2217 (0.2299)  time: 0.1961  data: 0.0002  max mem: 5508
[07:26:12.710021] Epoch: [0] Total time: 0:02:38 (0.2023 s / it)
[07:26:12.710527] Averaged stats: lr: 0.000050  training_loss: 1.2213 (1.4923)  mae_loss: 0.9868 (1.2624)  classification_loss: 0.2217 (0.2299)
[07:26:14.392194] Test:  [  0/157]  eta: 0:01:48  testing_loss: 1.9499 (1.9499)  acc1: 35.9375 (35.9375)  acc5: 90.6250 (90.6250)  time: 0.6932  data: 0.6630  max mem: 5508
[07:26:14.694827] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 2.0551 (2.0437)  acc1: 25.0000 (27.2727)  acc5: 78.1250 (78.9773)  time: 0.0902  data: 0.0604  max mem: 5508
[07:26:14.978673] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 2.0515 (2.0407)  acc1: 25.0000 (26.7857)  acc5: 78.1250 (79.3155)  time: 0.0290  data: 0.0002  max mem: 5508
[07:26:15.266666] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 2.0142 (2.0350)  acc1: 26.5625 (27.3690)  acc5: 79.6875 (78.8306)  time: 0.0284  data: 0.0003  max mem: 5508
[07:26:15.549655] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 2.0422 (2.0351)  acc1: 26.5625 (26.9055)  acc5: 79.6875 (78.8872)  time: 0.0284  data: 0.0003  max mem: 5508
[07:26:15.834110] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 2.0459 (2.0381)  acc1: 26.5625 (26.8076)  acc5: 79.6875 (79.1973)  time: 0.0282  data: 0.0002  max mem: 5508
[07:26:16.117631] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 2.0439 (2.0392)  acc1: 25.0000 (26.2551)  acc5: 81.2500 (79.4570)  time: 0.0281  data: 0.0002  max mem: 5508
[07:26:16.400272] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 2.0409 (2.0392)  acc1: 21.8750 (25.7482)  acc5: 82.8125 (79.7975)  time: 0.0281  data: 0.0002  max mem: 5508
[07:26:16.683206] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 2.0283 (2.0380)  acc1: 23.4375 (25.6559)  acc5: 84.3750 (80.0540)  time: 0.0281  data: 0.0002  max mem: 5508
[07:26:16.965124] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 2.0527 (2.0413)  acc1: 23.4375 (25.4121)  acc5: 79.6875 (80.0137)  time: 0.0281  data: 0.0002  max mem: 5508
[07:26:17.249149] Test:  [100/157]  eta: 0:00:01  testing_loss: 2.0560 (2.0413)  acc1: 23.4375 (25.3713)  acc5: 79.6875 (80.1671)  time: 0.0281  data: 0.0002  max mem: 5508
[07:26:17.530661] Test:  [110/157]  eta: 0:00:01  testing_loss: 2.0535 (2.0417)  acc1: 23.4375 (25.3238)  acc5: 81.2500 (80.1661)  time: 0.0281  data: 0.0002  max mem: 5508
[07:26:17.812585] Test:  [120/157]  eta: 0:00:01  testing_loss: 2.0425 (2.0397)  acc1: 25.0000 (25.3099)  acc5: 81.2500 (80.2040)  time: 0.0280  data: 0.0002  max mem: 5508
[07:26:18.095041] Test:  [130/157]  eta: 0:00:00  testing_loss: 2.0347 (2.0401)  acc1: 25.0000 (25.3578)  acc5: 81.2500 (80.1288)  time: 0.0281  data: 0.0002  max mem: 5508
[07:26:18.375956] Test:  [140/157]  eta: 0:00:00  testing_loss: 2.0266 (2.0385)  acc1: 28.1250 (25.5652)  acc5: 79.6875 (80.1308)  time: 0.0280  data: 0.0002  max mem: 5508
[07:26:18.654791] Test:  [150/157]  eta: 0:00:00  testing_loss: 2.0143 (2.0373)  acc1: 28.1250 (25.6002)  acc5: 79.6875 (80.2359)  time: 0.0279  data: 0.0001  max mem: 5508
[07:26:18.941264] Test:  [156/157]  eta: 0:00:00  testing_loss: 2.0133 (2.0365)  acc1: 26.5625 (25.6400)  acc5: 79.6875 (80.2400)  time: 0.0337  data: 0.0001  max mem: 5508
[07:26:19.092723] Test: Total time: 0:00:05 (0.0344 s / it)
[07:26:19.093504] * Acc@1 25.640 Acc@5 80.240 loss 2.036
[07:26:19.094467] Accuracy of the network on the 10000 test images: 25.6%
[07:26:19.095250] Max accuracy: 25.64%
[07:26:19.355302] log_dir: ./output_dir
[07:26:20.228768] Epoch: [1]  [  0/781]  eta: 0:11:20  lr: 0.000050  training_loss: 1.2070 (1.2070)  mae_loss: 0.9843 (0.9843)  classification_loss: 0.2227 (0.2227)  time: 0.8707  data: 0.6211  max mem: 5511
[07:26:24.145727] Epoch: [1]  [ 20/781]  eta: 0:02:53  lr: 0.000051  training_loss: 1.2263 (1.2429)  mae_loss: 0.9966 (1.0195)  classification_loss: 0.2234 (0.2234)  time: 0.1958  data: 0.0002  max mem: 5511
[07:26:28.061133] Epoch: [1]  [ 40/781]  eta: 0:02:37  lr: 0.000053  training_loss: 1.1746 (1.2290)  mae_loss: 0.9487 (1.0057)  classification_loss: 0.2217 (0.2233)  time: 0.1957  data: 0.0002  max mem: 5511
[07:26:32.008140] Epoch: [1]  [ 60/781]  eta: 0:02:29  lr: 0.000054  training_loss: 1.1988 (1.2168)  mae_loss: 0.9752 (0.9940)  classification_loss: 0.2213 (0.2228)  time: 0.1973  data: 0.0003  max mem: 5511
[07:26:35.937172] Epoch: [1]  [ 80/781]  eta: 0:02:23  lr: 0.000055  training_loss: 1.1919 (1.2156)  mae_loss: 0.9750 (0.9929)  classification_loss: 0.2212 (0.2228)  time: 0.1964  data: 0.0002  max mem: 5511
[07:26:39.842034] Epoch: [1]  [100/781]  eta: 0:02:18  lr: 0.000056  training_loss: 1.2049 (1.2131)  mae_loss: 0.9751 (0.9905)  classification_loss: 0.2205 (0.2226)  time: 0.1952  data: 0.0002  max mem: 5511
[07:26:43.789780] Epoch: [1]  [120/781]  eta: 0:02:13  lr: 0.000058  training_loss: 1.1729 (1.2058)  mae_loss: 0.9495 (0.9834)  classification_loss: 0.2214 (0.2224)  time: 0.1973  data: 0.0002  max mem: 5511
[07:26:47.684446] Epoch: [1]  [140/781]  eta: 0:02:08  lr: 0.000059  training_loss: 1.1367 (1.1993)  mae_loss: 0.9137 (0.9772)  classification_loss: 0.2183 (0.2221)  time: 0.1947  data: 0.0003  max mem: 5511
[07:26:51.591154] Epoch: [1]  [160/781]  eta: 0:02:04  lr: 0.000060  training_loss: 1.1957 (1.1984)  mae_loss: 0.9720 (0.9763)  classification_loss: 0.2233 (0.2221)  time: 0.1952  data: 0.0002  max mem: 5511
[07:26:55.502113] Epoch: [1]  [180/781]  eta: 0:01:59  lr: 0.000062  training_loss: 1.1779 (1.1977)  mae_loss: 0.9570 (0.9755)  classification_loss: 0.2197 (0.2221)  time: 0.1955  data: 0.0002  max mem: 5511
[07:26:59.405413] Epoch: [1]  [200/781]  eta: 0:01:55  lr: 0.000063  training_loss: 1.1767 (1.1947)  mae_loss: 0.9576 (0.9726)  classification_loss: 0.2229 (0.2221)  time: 0.1951  data: 0.0003  max mem: 5511
[07:27:03.333171] Epoch: [1]  [220/781]  eta: 0:01:51  lr: 0.000064  training_loss: 1.1983 (1.1945)  mae_loss: 0.9755 (0.9725)  classification_loss: 0.2193 (0.2220)  time: 0.1963  data: 0.0002  max mem: 5511
[07:27:07.262889] Epoch: [1]  [240/781]  eta: 0:01:47  lr: 0.000065  training_loss: 1.2039 (1.1947)  mae_loss: 0.9757 (0.9727)  classification_loss: 0.2202 (0.2220)  time: 0.1964  data: 0.0002  max mem: 5511
[07:27:11.165723] Epoch: [1]  [260/781]  eta: 0:01:43  lr: 0.000067  training_loss: 1.1824 (1.1937)  mae_loss: 0.9581 (0.9718)  classification_loss: 0.2205 (0.2219)  time: 0.1951  data: 0.0002  max mem: 5511
[07:27:15.099952] Epoch: [1]  [280/781]  eta: 0:01:39  lr: 0.000068  training_loss: 1.2083 (1.1938)  mae_loss: 0.9876 (0.9718)  classification_loss: 0.2221 (0.2220)  time: 0.1966  data: 0.0003  max mem: 5511
[07:27:19.028261] Epoch: [1]  [300/781]  eta: 0:01:35  lr: 0.000069  training_loss: 1.1989 (1.1934)  mae_loss: 0.9801 (0.9714)  classification_loss: 0.2212 (0.2219)  time: 0.1963  data: 0.0002  max mem: 5511
[07:27:22.931562] Epoch: [1]  [320/781]  eta: 0:01:31  lr: 0.000070  training_loss: 1.1267 (1.1897)  mae_loss: 0.9121 (0.9680)  classification_loss: 0.2188 (0.2218)  time: 0.1951  data: 0.0003  max mem: 5511
[07:27:26.845311] Epoch: [1]  [340/781]  eta: 0:01:27  lr: 0.000072  training_loss: 1.1878 (1.1891)  mae_loss: 0.9597 (0.9673)  classification_loss: 0.2242 (0.2218)  time: 0.1956  data: 0.0002  max mem: 5511
[07:27:30.762678] Epoch: [1]  [360/781]  eta: 0:01:23  lr: 0.000073  training_loss: 1.1594 (1.1873)  mae_loss: 0.9395 (0.9654)  classification_loss: 0.2238 (0.2219)  time: 0.1958  data: 0.0002  max mem: 5511
[07:27:34.677360] Epoch: [1]  [380/781]  eta: 0:01:19  lr: 0.000074  training_loss: 1.1211 (1.1848)  mae_loss: 0.9008 (0.9629)  classification_loss: 0.2206 (0.2218)  time: 0.1957  data: 0.0002  max mem: 5511
[07:27:38.566642] Epoch: [1]  [400/781]  eta: 0:01:15  lr: 0.000076  training_loss: 1.1553 (1.1838)  mae_loss: 0.9339 (0.9620)  classification_loss: 0.2212 (0.2218)  time: 0.1944  data: 0.0002  max mem: 5511
[07:27:42.517090] Epoch: [1]  [420/781]  eta: 0:01:11  lr: 0.000077  training_loss: 1.1185 (1.1816)  mae_loss: 0.8983 (0.9599)  classification_loss: 0.2210 (0.2217)  time: 0.1974  data: 0.0002  max mem: 5511
[07:27:46.428813] Epoch: [1]  [440/781]  eta: 0:01:07  lr: 0.000078  training_loss: 1.1097 (1.1792)  mae_loss: 0.8833 (0.9575)  classification_loss: 0.2197 (0.2216)  time: 0.1955  data: 0.0003  max mem: 5511
[07:27:50.325808] Epoch: [1]  [460/781]  eta: 0:01:03  lr: 0.000079  training_loss: 1.1009 (1.1759)  mae_loss: 0.8839 (0.9544)  classification_loss: 0.2170 (0.2215)  time: 0.1948  data: 0.0002  max mem: 5511
[07:27:54.264825] Epoch: [1]  [480/781]  eta: 0:00:59  lr: 0.000081  training_loss: 1.0897 (1.1723)  mae_loss: 0.8685 (0.9508)  classification_loss: 0.2211 (0.2215)  time: 0.1969  data: 0.0002  max mem: 5511
[07:27:58.182135] Epoch: [1]  [500/781]  eta: 0:00:55  lr: 0.000082  training_loss: 1.1138 (1.1704)  mae_loss: 0.8919 (0.9490)  classification_loss: 0.2165 (0.2214)  time: 0.1958  data: 0.0002  max mem: 5511
[07:28:02.089957] Epoch: [1]  [520/781]  eta: 0:00:51  lr: 0.000083  training_loss: 1.0922 (1.1675)  mae_loss: 0.8814 (0.9462)  classification_loss: 0.2175 (0.2213)  time: 0.1953  data: 0.0003  max mem: 5511
[07:28:06.039404] Epoch: [1]  [540/781]  eta: 0:00:47  lr: 0.000085  training_loss: 1.0749 (1.1642)  mae_loss: 0.8497 (0.9430)  classification_loss: 0.2198 (0.2213)  time: 0.1974  data: 0.0002  max mem: 5511
[07:28:09.954149] Epoch: [1]  [560/781]  eta: 0:00:43  lr: 0.000086  training_loss: 1.0895 (1.1615)  mae_loss: 0.8703 (0.9402)  classification_loss: 0.2193 (0.2213)  time: 0.1957  data: 0.0004  max mem: 5511
[07:28:13.849075] Epoch: [1]  [580/781]  eta: 0:00:39  lr: 0.000087  training_loss: 1.0705 (1.1592)  mae_loss: 0.8509 (0.9380)  classification_loss: 0.2188 (0.2212)  time: 0.1947  data: 0.0002  max mem: 5511
[07:28:17.743173] Epoch: [1]  [600/781]  eta: 0:00:35  lr: 0.000088  training_loss: 1.0504 (1.1564)  mae_loss: 0.8357 (0.9353)  classification_loss: 0.2182 (0.2211)  time: 0.1946  data: 0.0002  max mem: 5511
[07:28:21.630187] Epoch: [1]  [620/781]  eta: 0:00:31  lr: 0.000090  training_loss: 1.0478 (1.1532)  mae_loss: 0.8206 (0.9322)  classification_loss: 0.2196 (0.2211)  time: 0.1943  data: 0.0002  max mem: 5511
[07:28:25.535676] Epoch: [1]  [640/781]  eta: 0:00:27  lr: 0.000091  training_loss: 1.0691 (1.1511)  mae_loss: 0.8531 (0.9302)  classification_loss: 0.2181 (0.2210)  time: 0.1951  data: 0.0002  max mem: 5511
[07:28:29.427448] Epoch: [1]  [660/781]  eta: 0:00:23  lr: 0.000092  training_loss: 1.0597 (1.1483)  mae_loss: 0.8365 (0.9274)  classification_loss: 0.2176 (0.2209)  time: 0.1945  data: 0.0003  max mem: 5511
[07:28:33.325188] Epoch: [1]  [680/781]  eta: 0:00:19  lr: 0.000094  training_loss: 1.0657 (1.1460)  mae_loss: 0.8424 (0.9250)  classification_loss: 0.2214 (0.2209)  time: 0.1948  data: 0.0003  max mem: 5511
[07:28:37.232720] Epoch: [1]  [700/781]  eta: 0:00:15  lr: 0.000095  training_loss: 1.0389 (1.1428)  mae_loss: 0.8257 (0.9220)  classification_loss: 0.2182 (0.2209)  time: 0.1952  data: 0.0002  max mem: 5511
[07:28:41.164829] Epoch: [1]  [720/781]  eta: 0:00:11  lr: 0.000096  training_loss: 1.0085 (1.1393)  mae_loss: 0.7932 (0.9185)  classification_loss: 0.2178 (0.2208)  time: 0.1965  data: 0.0003  max mem: 5511
[07:28:45.073129] Epoch: [1]  [740/781]  eta: 0:00:08  lr: 0.000097  training_loss: 1.0162 (1.1363)  mae_loss: 0.7946 (0.9154)  classification_loss: 0.2217 (0.2208)  time: 0.1953  data: 0.0002  max mem: 5511
[07:28:49.005896] Epoch: [1]  [760/781]  eta: 0:00:04  lr: 0.000099  training_loss: 1.0132 (1.1335)  mae_loss: 0.8009 (0.9127)  classification_loss: 0.2191 (0.2208)  time: 0.1966  data: 0.0003  max mem: 5511
[07:28:52.898984] Epoch: [1]  [780/781]  eta: 0:00:00  lr: 0.000100  training_loss: 1.0414 (1.1311)  mae_loss: 0.8219 (0.9103)  classification_loss: 0.2204 (0.2208)  time: 0.1946  data: 0.0003  max mem: 5511
[07:28:53.080726] Epoch: [1] Total time: 0:02:33 (0.1968 s / it)
[07:28:53.081455] Averaged stats: lr: 0.000100  training_loss: 1.0414 (1.1311)  mae_loss: 0.8219 (0.9103)  classification_loss: 0.2204 (0.2208)
[07:28:53.755652] Test:  [  0/157]  eta: 0:01:45  testing_loss: 1.8622 (1.8622)  acc1: 43.7500 (43.7500)  acc5: 84.3750 (84.3750)  time: 0.6701  data: 0.6408  max mem: 5511
[07:28:54.047785] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.9710 (1.9606)  acc1: 31.2500 (32.6705)  acc5: 84.3750 (83.3807)  time: 0.0873  data: 0.0584  max mem: 5511
[07:28:54.329647] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.9710 (1.9542)  acc1: 31.2500 (32.5149)  acc5: 82.8125 (82.9613)  time: 0.0286  data: 0.0002  max mem: 5511
[07:28:54.612864] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.9421 (1.9468)  acc1: 34.3750 (33.2661)  acc5: 82.8125 (82.8629)  time: 0.0281  data: 0.0002  max mem: 5511
[07:28:54.901348] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.9460 (1.9494)  acc1: 34.3750 (33.0793)  acc5: 82.8125 (82.4695)  time: 0.0285  data: 0.0002  max mem: 5511
[07:28:55.186191] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.9679 (1.9529)  acc1: 32.8125 (32.8431)  acc5: 82.8125 (82.7206)  time: 0.0285  data: 0.0002  max mem: 5511
[07:28:55.472344] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.9686 (1.9533)  acc1: 31.2500 (32.8125)  acc5: 84.3750 (83.0430)  time: 0.0284  data: 0.0002  max mem: 5511
[07:28:55.756505] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.9544 (1.9522)  acc1: 31.2500 (32.4384)  acc5: 84.3750 (83.2526)  time: 0.0284  data: 0.0002  max mem: 5511
[07:28:56.042176] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.9544 (1.9534)  acc1: 28.1250 (32.2145)  acc5: 85.9375 (83.5069)  time: 0.0283  data: 0.0003  max mem: 5511
[07:28:56.326152] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.9793 (1.9577)  acc1: 29.6875 (31.8166)  acc5: 84.3750 (83.3963)  time: 0.0283  data: 0.0002  max mem: 5511
[07:28:56.608377] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.9793 (1.9587)  acc1: 28.1250 (31.2655)  acc5: 82.8125 (83.3230)  time: 0.0282  data: 0.0002  max mem: 5511
[07:28:56.893026] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.9604 (1.9604)  acc1: 28.1250 (31.2078)  acc5: 82.8125 (83.1222)  time: 0.0282  data: 0.0002  max mem: 5511
[07:28:57.173994] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.9482 (1.9580)  acc1: 29.6875 (31.3533)  acc5: 82.8125 (83.1999)  time: 0.0282  data: 0.0002  max mem: 5511
[07:28:57.455248] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.9536 (1.9601)  acc1: 31.2500 (31.3693)  acc5: 82.8125 (83.0272)  time: 0.0280  data: 0.0002  max mem: 5511
[07:28:57.736273] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.9732 (1.9591)  acc1: 31.2500 (31.5492)  acc5: 79.6875 (82.9455)  time: 0.0280  data: 0.0001  max mem: 5511
[07:28:58.014700] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.9430 (1.9579)  acc1: 34.3750 (31.6846)  acc5: 82.8125 (82.9988)  time: 0.0279  data: 0.0001  max mem: 5511
[07:28:58.164411] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.9284 (1.9564)  acc1: 32.8125 (31.6800)  acc5: 84.3750 (83.1100)  time: 0.0269  data: 0.0001  max mem: 5511
[07:28:58.315099] Test: Total time: 0:00:05 (0.0333 s / it)
[07:28:58.316366] * Acc@1 31.680 Acc@5 83.110 loss 1.956
[07:28:58.317788] Accuracy of the network on the 10000 test images: 31.7%
[07:28:58.318476] Max accuracy: 31.68%
[07:28:58.462155] log_dir: ./output_dir
[07:28:59.303292] Epoch: [2]  [  0/781]  eta: 0:10:55  lr: 0.000100  training_loss: 1.0739 (1.0739)  mae_loss: 0.8485 (0.8485)  classification_loss: 0.2254 (0.2254)  time: 0.8389  data: 0.6296  max mem: 5511
[07:29:03.210878] Epoch: [2]  [ 20/781]  eta: 0:02:51  lr: 0.000101  training_loss: 1.0135 (0.9980)  mae_loss: 0.7918 (0.7792)  classification_loss: 0.2195 (0.2188)  time: 0.1953  data: 0.0002  max mem: 5511
[07:29:07.118579] Epoch: [2]  [ 40/781]  eta: 0:02:36  lr: 0.000103  training_loss: 1.0192 (1.0046)  mae_loss: 0.7949 (0.7856)  classification_loss: 0.2180 (0.2190)  time: 0.1953  data: 0.0003  max mem: 5511
[07:29:11.033481] Epoch: [2]  [ 60/781]  eta: 0:02:28  lr: 0.000104  training_loss: 1.0115 (1.0097)  mae_loss: 0.7925 (0.7905)  classification_loss: 0.2186 (0.2192)  time: 0.1957  data: 0.0003  max mem: 5511
[07:29:14.938990] Epoch: [2]  [ 80/781]  eta: 0:02:22  lr: 0.000105  training_loss: 0.9679 (1.0071)  mae_loss: 0.7468 (0.7880)  classification_loss: 0.2178 (0.2192)  time: 0.1952  data: 0.0002  max mem: 5511
[07:29:18.882762] Epoch: [2]  [100/781]  eta: 0:02:17  lr: 0.000106  training_loss: 0.9564 (1.0027)  mae_loss: 0.7351 (0.7839)  classification_loss: 0.2168 (0.2188)  time: 0.1971  data: 0.0002  max mem: 5511
[07:29:22.786732] Epoch: [2]  [120/781]  eta: 0:02:12  lr: 0.000108  training_loss: 0.9696 (0.9982)  mae_loss: 0.7517 (0.7792)  classification_loss: 0.2197 (0.2191)  time: 0.1951  data: 0.0002  max mem: 5511
[07:29:26.688960] Epoch: [2]  [140/781]  eta: 0:02:08  lr: 0.000109  training_loss: 0.9430 (0.9939)  mae_loss: 0.7286 (0.7750)  classification_loss: 0.2164 (0.2189)  time: 0.1950  data: 0.0002  max mem: 5511
[07:29:30.632281] Epoch: [2]  [160/781]  eta: 0:02:04  lr: 0.000110  training_loss: 0.9628 (0.9911)  mae_loss: 0.7478 (0.7724)  classification_loss: 0.2165 (0.2187)  time: 0.1971  data: 0.0002  max mem: 5511
[07:29:34.547496] Epoch: [2]  [180/781]  eta: 0:01:59  lr: 0.000112  training_loss: 0.9701 (0.9899)  mae_loss: 0.7623 (0.7714)  classification_loss: 0.2168 (0.2185)  time: 0.1956  data: 0.0005  max mem: 5511
[07:29:38.432840] Epoch: [2]  [200/781]  eta: 0:01:55  lr: 0.000113  training_loss: 0.9938 (0.9901)  mae_loss: 0.7789 (0.7718)  classification_loss: 0.2161 (0.2183)  time: 0.1942  data: 0.0002  max mem: 5511
[07:29:42.334415] Epoch: [2]  [220/781]  eta: 0:01:51  lr: 0.000114  training_loss: 0.9427 (0.9859)  mae_loss: 0.7299 (0.7678)  classification_loss: 0.2171 (0.2181)  time: 0.1950  data: 0.0002  max mem: 5511
[07:29:46.219670] Epoch: [2]  [240/781]  eta: 0:01:47  lr: 0.000115  training_loss: 0.9524 (0.9844)  mae_loss: 0.7318 (0.7664)  classification_loss: 0.2173 (0.2180)  time: 0.1942  data: 0.0002  max mem: 5511
[07:29:50.130427] Epoch: [2]  [260/781]  eta: 0:01:43  lr: 0.000117  training_loss: 0.9471 (0.9816)  mae_loss: 0.7299 (0.7637)  classification_loss: 0.2172 (0.2179)  time: 0.1955  data: 0.0002  max mem: 5511
[07:29:54.039206] Epoch: [2]  [280/781]  eta: 0:01:39  lr: 0.000118  training_loss: 0.9711 (0.9810)  mae_loss: 0.7527 (0.7631)  classification_loss: 0.2185 (0.2180)  time: 0.1954  data: 0.0002  max mem: 5511
[07:29:57.940137] Epoch: [2]  [300/781]  eta: 0:01:35  lr: 0.000119  training_loss: 0.9209 (0.9790)  mae_loss: 0.7034 (0.7611)  classification_loss: 0.2175 (0.2180)  time: 0.1950  data: 0.0002  max mem: 5511
[07:30:01.850790] Epoch: [2]  [320/781]  eta: 0:01:30  lr: 0.000120  training_loss: 0.9256 (0.9763)  mae_loss: 0.7016 (0.7584)  classification_loss: 0.2150 (0.2179)  time: 0.1955  data: 0.0002  max mem: 5511
[07:30:05.760909] Epoch: [2]  [340/781]  eta: 0:01:26  lr: 0.000122  training_loss: 0.9264 (0.9732)  mae_loss: 0.7073 (0.7553)  classification_loss: 0.2181 (0.2179)  time: 0.1954  data: 0.0002  max mem: 5511
[07:30:09.700838] Epoch: [2]  [360/781]  eta: 0:01:23  lr: 0.000123  training_loss: 0.9385 (0.9714)  mae_loss: 0.7172 (0.7536)  classification_loss: 0.2164 (0.2178)  time: 0.1969  data: 0.0008  max mem: 5511
[07:30:13.591663] Epoch: [2]  [380/781]  eta: 0:01:19  lr: 0.000124  training_loss: 0.9169 (0.9690)  mae_loss: 0.6994 (0.7511)  classification_loss: 0.2174 (0.2178)  time: 0.1945  data: 0.0002  max mem: 5511
[07:30:17.487825] Epoch: [2]  [400/781]  eta: 0:01:15  lr: 0.000126  training_loss: 0.9068 (0.9659)  mae_loss: 0.6883 (0.7482)  classification_loss: 0.2130 (0.2177)  time: 0.1947  data: 0.0003  max mem: 5511
[07:30:21.388650] Epoch: [2]  [420/781]  eta: 0:01:11  lr: 0.000127  training_loss: 0.9390 (0.9645)  mae_loss: 0.7111 (0.7469)  classification_loss: 0.2164 (0.2177)  time: 0.1950  data: 0.0003  max mem: 5511
[07:30:25.317553] Epoch: [2]  [440/781]  eta: 0:01:07  lr: 0.000128  training_loss: 0.9367 (0.9637)  mae_loss: 0.7178 (0.7460)  classification_loss: 0.2173 (0.2176)  time: 0.1964  data: 0.0002  max mem: 5511
[07:30:29.218899] Epoch: [2]  [460/781]  eta: 0:01:03  lr: 0.000129  training_loss: 0.8889 (0.9608)  mae_loss: 0.6742 (0.7433)  classification_loss: 0.2144 (0.2175)  time: 0.1950  data: 0.0002  max mem: 5511
[07:30:33.125685] Epoch: [2]  [480/781]  eta: 0:00:59  lr: 0.000131  training_loss: 0.9228 (0.9596)  mae_loss: 0.7016 (0.7420)  classification_loss: 0.2179 (0.2175)  time: 0.1953  data: 0.0002  max mem: 5511
[07:30:37.042387] Epoch: [2]  [500/781]  eta: 0:00:55  lr: 0.000132  training_loss: 0.9001 (0.9578)  mae_loss: 0.6868 (0.7403)  classification_loss: 0.2145 (0.2174)  time: 0.1958  data: 0.0002  max mem: 5511
[07:30:40.972120] Epoch: [2]  [520/781]  eta: 0:00:51  lr: 0.000133  training_loss: 0.9057 (0.9558)  mae_loss: 0.6908 (0.7385)  classification_loss: 0.2134 (0.2174)  time: 0.1964  data: 0.0002  max mem: 5511
[07:30:44.872455] Epoch: [2]  [540/781]  eta: 0:00:47  lr: 0.000135  training_loss: 0.8803 (0.9536)  mae_loss: 0.6588 (0.7362)  classification_loss: 0.2191 (0.2174)  time: 0.1949  data: 0.0003  max mem: 5511
[07:30:48.773946] Epoch: [2]  [560/781]  eta: 0:00:43  lr: 0.000136  training_loss: 0.8798 (0.9510)  mae_loss: 0.6590 (0.7337)  classification_loss: 0.2169 (0.2173)  time: 0.1950  data: 0.0002  max mem: 5511
[07:30:52.669121] Epoch: [2]  [580/781]  eta: 0:00:39  lr: 0.000137  training_loss: 0.8599 (0.9482)  mae_loss: 0.6472 (0.7310)  classification_loss: 0.2139 (0.2172)  time: 0.1947  data: 0.0002  max mem: 5511
[07:30:56.548285] Epoch: [2]  [600/781]  eta: 0:00:35  lr: 0.000138  training_loss: 0.8934 (0.9470)  mae_loss: 0.6792 (0.7298)  classification_loss: 0.2142 (0.2172)  time: 0.1939  data: 0.0002  max mem: 5511
[07:31:00.434922] Epoch: [2]  [620/781]  eta: 0:00:31  lr: 0.000140  training_loss: 0.8686 (0.9446)  mae_loss: 0.6534 (0.7275)  classification_loss: 0.2143 (0.2171)  time: 0.1943  data: 0.0001  max mem: 5511
[07:31:04.331421] Epoch: [2]  [640/781]  eta: 0:00:27  lr: 0.000141  training_loss: 0.8752 (0.9427)  mae_loss: 0.6742 (0.7258)  classification_loss: 0.2138 (0.2170)  time: 0.1948  data: 0.0002  max mem: 5511
[07:31:08.244287] Epoch: [2]  [660/781]  eta: 0:00:23  lr: 0.000142  training_loss: 0.8477 (0.9402)  mae_loss: 0.6261 (0.7233)  classification_loss: 0.2140 (0.2169)  time: 0.1956  data: 0.0002  max mem: 5511
[07:31:12.127509] Epoch: [2]  [680/781]  eta: 0:00:19  lr: 0.000144  training_loss: 0.8776 (0.9381)  mae_loss: 0.6618 (0.7212)  classification_loss: 0.2145 (0.2168)  time: 0.1941  data: 0.0001  max mem: 5511
[07:31:16.018185] Epoch: [2]  [700/781]  eta: 0:00:15  lr: 0.000145  training_loss: 0.8488 (0.9359)  mae_loss: 0.6360 (0.7191)  classification_loss: 0.2139 (0.2168)  time: 0.1944  data: 0.0002  max mem: 5511
[07:31:19.925655] Epoch: [2]  [720/781]  eta: 0:00:11  lr: 0.000146  training_loss: 0.8572 (0.9337)  mae_loss: 0.6477 (0.7170)  classification_loss: 0.2135 (0.2167)  time: 0.1953  data: 0.0002  max mem: 5511
[07:31:23.813769] Epoch: [2]  [740/781]  eta: 0:00:08  lr: 0.000147  training_loss: 0.8495 (0.9317)  mae_loss: 0.6277 (0.7150)  classification_loss: 0.2150 (0.2167)  time: 0.1943  data: 0.0002  max mem: 5511
[07:31:27.715939] Epoch: [2]  [760/781]  eta: 0:00:04  lr: 0.000149  training_loss: 0.8422 (0.9297)  mae_loss: 0.6269 (0.7131)  classification_loss: 0.2133 (0.2166)  time: 0.1950  data: 0.0002  max mem: 5511
[07:31:31.608091] Epoch: [2]  [780/781]  eta: 0:00:00  lr: 0.000150  training_loss: 0.8666 (0.9281)  mae_loss: 0.6623 (0.7116)  classification_loss: 0.2106 (0.2165)  time: 0.1945  data: 0.0002  max mem: 5511
[07:31:31.790150] Epoch: [2] Total time: 0:02:33 (0.1963 s / it)
[07:31:31.790607] Averaged stats: lr: 0.000150  training_loss: 0.8666 (0.9281)  mae_loss: 0.6623 (0.7116)  classification_loss: 0.2106 (0.2165)
[07:31:32.482399] Test:  [  0/157]  eta: 0:01:47  testing_loss: 1.7258 (1.7258)  acc1: 45.3125 (45.3125)  acc5: 85.9375 (85.9375)  time: 0.6878  data: 0.6564  max mem: 5511
[07:31:32.766652] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.8662 (1.8473)  acc1: 37.5000 (37.2159)  acc5: 84.3750 (85.3693)  time: 0.0882  data: 0.0599  max mem: 5511
[07:31:33.047621] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.8446 (1.8365)  acc1: 35.9375 (38.0208)  acc5: 84.3750 (85.7887)  time: 0.0281  data: 0.0002  max mem: 5511
[07:31:33.328188] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.8445 (1.8304)  acc1: 37.5000 (38.1552)  acc5: 85.9375 (85.2823)  time: 0.0280  data: 0.0001  max mem: 5511
[07:31:33.608819] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.8397 (1.8341)  acc1: 35.9375 (37.9954)  acc5: 85.9375 (85.3277)  time: 0.0280  data: 0.0001  max mem: 5511
[07:31:33.890491] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.8462 (1.8364)  acc1: 37.5000 (38.0208)  acc5: 85.9375 (85.4473)  time: 0.0280  data: 0.0001  max mem: 5511
[07:31:34.175387] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.8390 (1.8313)  acc1: 39.0625 (38.5758)  acc5: 84.3750 (85.5277)  time: 0.0282  data: 0.0002  max mem: 5511
[07:31:34.462234] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.8028 (1.8299)  acc1: 40.6250 (38.5783)  acc5: 84.3750 (85.4313)  time: 0.0285  data: 0.0003  max mem: 5511
[07:31:34.749451] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.8071 (1.8297)  acc1: 37.5000 (38.4645)  acc5: 85.9375 (85.6674)  time: 0.0286  data: 0.0004  max mem: 5511
[07:31:35.031784] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.8475 (1.8340)  acc1: 37.5000 (38.2727)  acc5: 85.9375 (85.6628)  time: 0.0283  data: 0.0002  max mem: 5511
[07:31:35.314364] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.8549 (1.8361)  acc1: 37.5000 (38.1343)  acc5: 84.3750 (85.4889)  time: 0.0281  data: 0.0002  max mem: 5511
[07:31:35.602607] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.8485 (1.8380)  acc1: 35.9375 (37.9505)  acc5: 84.3750 (85.2900)  time: 0.0284  data: 0.0002  max mem: 5511
[07:31:35.886659] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.8333 (1.8367)  acc1: 35.9375 (37.7712)  acc5: 85.9375 (85.3822)  time: 0.0284  data: 0.0002  max mem: 5511
[07:31:36.181149] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.8333 (1.8388)  acc1: 35.9375 (37.7266)  acc5: 84.3750 (85.2099)  time: 0.0287  data: 0.0002  max mem: 5511
[07:31:36.465582] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.8518 (1.8377)  acc1: 37.5000 (37.7881)  acc5: 84.3750 (85.2726)  time: 0.0288  data: 0.0002  max mem: 5511
[07:31:36.745219] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.8213 (1.8364)  acc1: 37.5000 (37.7173)  acc5: 87.5000 (85.3994)  time: 0.0281  data: 0.0001  max mem: 5511
[07:31:36.895626] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.8089 (1.8358)  acc1: 35.9375 (37.5500)  acc5: 87.5000 (85.4200)  time: 0.0270  data: 0.0001  max mem: 5511
[07:31:37.052122] Test: Total time: 0:00:05 (0.0335 s / it)
[07:31:37.052590] * Acc@1 37.550 Acc@5 85.420 loss 1.836
[07:31:37.052913] Accuracy of the network on the 10000 test images: 37.5%
[07:31:37.053120] Max accuracy: 37.55%
[07:31:37.318283] log_dir: ./output_dir
[07:31:38.175677] Epoch: [3]  [  0/781]  eta: 0:11:08  lr: 0.000150  training_loss: 0.7674 (0.7674)  mae_loss: 0.5617 (0.5617)  classification_loss: 0.2057 (0.2057)  time: 0.8553  data: 0.6422  max mem: 5511
[07:31:42.097540] Epoch: [3]  [ 20/781]  eta: 0:02:53  lr: 0.000151  training_loss: 0.8422 (0.8480)  mae_loss: 0.6352 (0.6357)  classification_loss: 0.2117 (0.2123)  time: 0.1960  data: 0.0005  max mem: 5511
[07:31:46.005427] Epoch: [3]  [ 40/781]  eta: 0:02:36  lr: 0.000153  training_loss: 0.8612 (0.8547)  mae_loss: 0.6440 (0.6407)  classification_loss: 0.2165 (0.2140)  time: 0.1953  data: 0.0002  max mem: 5511
[07:31:49.909213] Epoch: [3]  [ 60/781]  eta: 0:02:28  lr: 0.000154  training_loss: 0.8406 (0.8498)  mae_loss: 0.6260 (0.6358)  classification_loss: 0.2150 (0.2140)  time: 0.1951  data: 0.0002  max mem: 5511
[07:31:53.818186] Epoch: [3]  [ 80/781]  eta: 0:02:22  lr: 0.000155  training_loss: 0.8392 (0.8486)  mae_loss: 0.6234 (0.6346)  classification_loss: 0.2130 (0.2140)  time: 0.1954  data: 0.0002  max mem: 5511
[07:31:57.753298] Epoch: [3]  [100/781]  eta: 0:02:17  lr: 0.000156  training_loss: 0.8356 (0.8450)  mae_loss: 0.6148 (0.6307)  classification_loss: 0.2144 (0.2143)  time: 0.1967  data: 0.0002  max mem: 5511
[07:32:01.657917] Epoch: [3]  [120/781]  eta: 0:02:12  lr: 0.000158  training_loss: 0.8300 (0.8428)  mae_loss: 0.6123 (0.6286)  classification_loss: 0.2143 (0.2142)  time: 0.1952  data: 0.0002  max mem: 5511
[07:32:05.561991] Epoch: [3]  [140/781]  eta: 0:02:08  lr: 0.000159  training_loss: 0.8150 (0.8406)  mae_loss: 0.6036 (0.6264)  classification_loss: 0.2116 (0.2142)  time: 0.1951  data: 0.0002  max mem: 5511
[07:32:09.462112] Epoch: [3]  [160/781]  eta: 0:02:03  lr: 0.000160  training_loss: 0.8069 (0.8369)  mae_loss: 0.5900 (0.6229)  classification_loss: 0.2117 (0.2140)  time: 0.1949  data: 0.0002  max mem: 5511
[07:32:13.379217] Epoch: [3]  [180/781]  eta: 0:01:59  lr: 0.000162  training_loss: 0.8214 (0.8366)  mae_loss: 0.6141 (0.6227)  classification_loss: 0.2140 (0.2138)  time: 0.1958  data: 0.0002  max mem: 5511
[07:32:17.341431] Epoch: [3]  [200/781]  eta: 0:01:55  lr: 0.000163  training_loss: 0.8392 (0.8373)  mae_loss: 0.6256 (0.6236)  classification_loss: 0.2129 (0.2137)  time: 0.1980  data: 0.0003  max mem: 5511
[07:32:21.235205] Epoch: [3]  [220/781]  eta: 0:01:51  lr: 0.000164  training_loss: 0.8508 (0.8391)  mae_loss: 0.6302 (0.6254)  classification_loss: 0.2138 (0.2137)  time: 0.1946  data: 0.0002  max mem: 5511
[07:32:25.142777] Epoch: [3]  [240/781]  eta: 0:01:47  lr: 0.000165  training_loss: 0.8356 (0.8393)  mae_loss: 0.6199 (0.6256)  classification_loss: 0.2138 (0.2137)  time: 0.1953  data: 0.0002  max mem: 5511
[07:32:29.033493] Epoch: [3]  [260/781]  eta: 0:01:43  lr: 0.000167  training_loss: 0.8369 (0.8389)  mae_loss: 0.6326 (0.6253)  classification_loss: 0.2130 (0.2136)  time: 0.1945  data: 0.0002  max mem: 5511
[07:32:32.934244] Epoch: [3]  [280/781]  eta: 0:01:39  lr: 0.000168  training_loss: 0.8382 (0.8390)  mae_loss: 0.6278 (0.6251)  classification_loss: 0.2133 (0.2138)  time: 0.1950  data: 0.0002  max mem: 5511
[07:32:36.854009] Epoch: [3]  [300/781]  eta: 0:01:35  lr: 0.000169  training_loss: 0.8137 (0.8380)  mae_loss: 0.5953 (0.6241)  classification_loss: 0.2130 (0.2139)  time: 0.1959  data: 0.0002  max mem: 5511
[07:32:40.768485] Epoch: [3]  [320/781]  eta: 0:01:31  lr: 0.000170  training_loss: 0.8171 (0.8364)  mae_loss: 0.6008 (0.6226)  classification_loss: 0.2112 (0.2138)  time: 0.1956  data: 0.0004  max mem: 5511
[07:32:44.671683] Epoch: [3]  [340/781]  eta: 0:01:27  lr: 0.000172  training_loss: 0.8241 (0.8365)  mae_loss: 0.6156 (0.6228)  classification_loss: 0.2108 (0.2137)  time: 0.1951  data: 0.0002  max mem: 5511
[07:32:48.571127] Epoch: [3]  [360/781]  eta: 0:01:23  lr: 0.000173  training_loss: 0.7962 (0.8350)  mae_loss: 0.5776 (0.6212)  classification_loss: 0.2130 (0.2138)  time: 0.1949  data: 0.0002  max mem: 5511
[07:32:52.497192] Epoch: [3]  [380/781]  eta: 0:01:19  lr: 0.000174  training_loss: 0.8134 (0.8350)  mae_loss: 0.5968 (0.6213)  classification_loss: 0.2120 (0.2137)  time: 0.1962  data: 0.0002  max mem: 5511
[07:32:56.397289] Epoch: [3]  [400/781]  eta: 0:01:15  lr: 0.000176  training_loss: 0.8029 (0.8339)  mae_loss: 0.5865 (0.6203)  classification_loss: 0.2112 (0.2136)  time: 0.1949  data: 0.0003  max mem: 5511
[07:33:00.343050] Epoch: [3]  [420/781]  eta: 0:01:11  lr: 0.000177  training_loss: 0.8385 (0.8335)  mae_loss: 0.6210 (0.6200)  classification_loss: 0.2119 (0.2135)  time: 0.1972  data: 0.0002  max mem: 5511
[07:33:04.245581] Epoch: [3]  [440/781]  eta: 0:01:07  lr: 0.000178  training_loss: 0.8143 (0.8331)  mae_loss: 0.6061 (0.6197)  classification_loss: 0.2092 (0.2134)  time: 0.1950  data: 0.0002  max mem: 5511
[07:33:08.169781] Epoch: [3]  [460/781]  eta: 0:01:03  lr: 0.000179  training_loss: 0.7844 (0.8312)  mae_loss: 0.5713 (0.6178)  classification_loss: 0.2113 (0.2133)  time: 0.1961  data: 0.0003  max mem: 5511
[07:33:12.078855] Epoch: [3]  [480/781]  eta: 0:00:59  lr: 0.000181  training_loss: 0.8156 (0.8307)  mae_loss: 0.5986 (0.6174)  classification_loss: 0.2106 (0.2132)  time: 0.1954  data: 0.0002  max mem: 5511
[07:33:15.984777] Epoch: [3]  [500/781]  eta: 0:00:55  lr: 0.000182  training_loss: 0.8169 (0.8301)  mae_loss: 0.6036 (0.6169)  classification_loss: 0.2113 (0.2132)  time: 0.1952  data: 0.0003  max mem: 5511
[07:33:19.866218] Epoch: [3]  [520/781]  eta: 0:00:51  lr: 0.000183  training_loss: 0.7797 (0.8290)  mae_loss: 0.5646 (0.6158)  classification_loss: 0.2127 (0.2132)  time: 0.1940  data: 0.0002  max mem: 5511
[07:33:23.753596] Epoch: [3]  [540/781]  eta: 0:00:47  lr: 0.000185  training_loss: 0.8247 (0.8285)  mae_loss: 0.6093 (0.6154)  classification_loss: 0.2124 (0.2132)  time: 0.1943  data: 0.0002  max mem: 5511
[07:33:27.661712] Epoch: [3]  [560/781]  eta: 0:00:43  lr: 0.000186  training_loss: 0.8020 (0.8279)  mae_loss: 0.5907 (0.6148)  classification_loss: 0.2087 (0.2130)  time: 0.1953  data: 0.0002  max mem: 5511
[07:33:31.629516] Epoch: [3]  [580/781]  eta: 0:00:39  lr: 0.000187  training_loss: 0.7848 (0.8264)  mae_loss: 0.5686 (0.6134)  classification_loss: 0.2136 (0.2130)  time: 0.1983  data: 0.0002  max mem: 5511
[07:33:35.530920] Epoch: [3]  [600/781]  eta: 0:00:35  lr: 0.000188  training_loss: 0.8066 (0.8260)  mae_loss: 0.5945 (0.6130)  classification_loss: 0.2128 (0.2130)  time: 0.1950  data: 0.0002  max mem: 5511
[07:33:39.427253] Epoch: [3]  [620/781]  eta: 0:00:31  lr: 0.000190  training_loss: 0.7884 (0.8248)  mae_loss: 0.5754 (0.6119)  classification_loss: 0.2101 (0.2129)  time: 0.1947  data: 0.0003  max mem: 5511
[07:33:43.363928] Epoch: [3]  [640/781]  eta: 0:00:27  lr: 0.000191  training_loss: 0.7792 (0.8241)  mae_loss: 0.5716 (0.6113)  classification_loss: 0.2123 (0.2128)  time: 0.1968  data: 0.0002  max mem: 5511
[07:33:47.276471] Epoch: [3]  [660/781]  eta: 0:00:23  lr: 0.000192  training_loss: 0.7742 (0.8229)  mae_loss: 0.5636 (0.6101)  classification_loss: 0.2101 (0.2128)  time: 0.1955  data: 0.0002  max mem: 5511
[07:33:51.172488] Epoch: [3]  [680/781]  eta: 0:00:19  lr: 0.000194  training_loss: 0.7909 (0.8219)  mae_loss: 0.5787 (0.6092)  classification_loss: 0.2107 (0.2127)  time: 0.1947  data: 0.0002  max mem: 5511
[07:33:55.133422] Epoch: [3]  [700/781]  eta: 0:00:15  lr: 0.000195  training_loss: 0.8010 (0.8213)  mae_loss: 0.5948 (0.6087)  classification_loss: 0.2095 (0.2126)  time: 0.1979  data: 0.0002  max mem: 5511
[07:33:59.029813] Epoch: [3]  [720/781]  eta: 0:00:11  lr: 0.000196  training_loss: 0.7550 (0.8200)  mae_loss: 0.5450 (0.6075)  classification_loss: 0.2071 (0.2125)  time: 0.1947  data: 0.0002  max mem: 5511
[07:34:02.945454] Epoch: [3]  [740/781]  eta: 0:00:08  lr: 0.000197  training_loss: 0.7821 (0.8191)  mae_loss: 0.5676 (0.6067)  classification_loss: 0.2128 (0.2125)  time: 0.1957  data: 0.0002  max mem: 5511
[07:34:06.838617] Epoch: [3]  [760/781]  eta: 0:00:04  lr: 0.000199  training_loss: 0.7939 (0.8184)  mae_loss: 0.5773 (0.6060)  classification_loss: 0.2123 (0.2125)  time: 0.1946  data: 0.0003  max mem: 5511
[07:34:10.732502] Epoch: [3]  [780/781]  eta: 0:00:00  lr: 0.000200  training_loss: 0.8141 (0.8179)  mae_loss: 0.5863 (0.6055)  classification_loss: 0.2077 (0.2124)  time: 0.1946  data: 0.0002  max mem: 5511
[07:34:10.889925] Epoch: [3] Total time: 0:02:33 (0.1966 s / it)
[07:34:10.890406] Averaged stats: lr: 0.000200  training_loss: 0.8141 (0.8179)  mae_loss: 0.5863 (0.6055)  classification_loss: 0.2077 (0.2124)
[07:34:11.486621] Test:  [  0/157]  eta: 0:01:32  testing_loss: 1.6257 (1.6257)  acc1: 50.0000 (50.0000)  acc5: 89.0625 (89.0625)  time: 0.5923  data: 0.5605  max mem: 5511
[07:34:11.778040] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 1.7174 (1.7325)  acc1: 39.0625 (40.7670)  acc5: 89.0625 (88.9205)  time: 0.0802  data: 0.0511  max mem: 5511
[07:34:12.064037] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.7305 (1.7245)  acc1: 40.6250 (42.0387)  acc5: 89.0625 (88.4673)  time: 0.0287  data: 0.0002  max mem: 5511
[07:34:12.350832] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 1.7268 (1.7202)  acc1: 42.1875 (41.6331)  acc5: 87.5000 (88.1552)  time: 0.0285  data: 0.0002  max mem: 5511
[07:34:12.634494] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 1.7143 (1.7226)  acc1: 40.6250 (41.4634)  acc5: 87.5000 (88.3003)  time: 0.0284  data: 0.0001  max mem: 5511
[07:34:12.920326] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.7237 (1.7242)  acc1: 40.6250 (40.9314)  acc5: 89.0625 (88.5110)  time: 0.0284  data: 0.0002  max mem: 5511
[07:34:13.205885] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.7195 (1.7204)  acc1: 40.6250 (41.1629)  acc5: 89.0625 (88.6783)  time: 0.0284  data: 0.0002  max mem: 5511
[07:34:13.491651] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.6917 (1.7193)  acc1: 40.6250 (41.2412)  acc5: 90.6250 (88.7984)  time: 0.0284  data: 0.0002  max mem: 5511
[07:34:13.778521] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.7041 (1.7205)  acc1: 40.6250 (41.2423)  acc5: 90.6250 (88.9853)  time: 0.0284  data: 0.0002  max mem: 5511
[07:34:14.061794] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.7549 (1.7234)  acc1: 40.6250 (41.0886)  acc5: 89.0625 (88.8736)  time: 0.0283  data: 0.0002  max mem: 5511
[07:34:14.345840] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.7549 (1.7265)  acc1: 39.0625 (40.6869)  acc5: 85.9375 (88.7222)  time: 0.0282  data: 0.0002  max mem: 5511
[07:34:14.628347] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.7440 (1.7288)  acc1: 37.5000 (40.6672)  acc5: 85.9375 (88.4854)  time: 0.0282  data: 0.0002  max mem: 5511
[07:34:14.912250] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.7095 (1.7270)  acc1: 37.5000 (40.5088)  acc5: 89.0625 (88.6880)  time: 0.0282  data: 0.0002  max mem: 5511
[07:34:15.197078] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.7095 (1.7289)  acc1: 39.0625 (40.4222)  acc5: 89.0625 (88.5854)  time: 0.0283  data: 0.0001  max mem: 5511
[07:34:15.477414] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.7412 (1.7289)  acc1: 39.0625 (40.3480)  acc5: 89.0625 (88.6192)  time: 0.0281  data: 0.0001  max mem: 5511
[07:34:15.757358] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.7442 (1.7283)  acc1: 39.0625 (40.3974)  acc5: 89.0625 (88.5348)  time: 0.0278  data: 0.0001  max mem: 5511
[07:34:15.909302] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.7115 (1.7279)  acc1: 39.0625 (40.3000)  acc5: 89.0625 (88.6000)  time: 0.0269  data: 0.0001  max mem: 5511
[07:34:16.069188] Test: Total time: 0:00:05 (0.0330 s / it)
[07:34:16.069658] * Acc@1 40.300 Acc@5 88.600 loss 1.728
[07:34:16.069947] Accuracy of the network on the 10000 test images: 40.3%
[07:34:16.070140] Max accuracy: 40.30%
[07:34:16.218737] log_dir: ./output_dir
[07:34:17.125056] Epoch: [4]  [  0/781]  eta: 0:11:46  lr: 0.000200  training_loss: 0.8788 (0.8788)  mae_loss: 0.6706 (0.6706)  classification_loss: 0.2082 (0.2082)  time: 0.9047  data: 0.6931  max mem: 5511
[07:34:21.017519] Epoch: [4]  [ 20/781]  eta: 0:02:53  lr: 0.000201  training_loss: 0.7541 (0.7736)  mae_loss: 0.5448 (0.5649)  classification_loss: 0.2087 (0.2087)  time: 0.1945  data: 0.0002  max mem: 5511
[07:34:24.921200] Epoch: [4]  [ 40/781]  eta: 0:02:37  lr: 0.000203  training_loss: 0.7736 (0.7770)  mae_loss: 0.5592 (0.5674)  classification_loss: 0.2106 (0.2096)  time: 0.1951  data: 0.0002  max mem: 5511
[07:34:28.829163] Epoch: [4]  [ 60/781]  eta: 0:02:28  lr: 0.000204  training_loss: 0.7578 (0.7745)  mae_loss: 0.5550 (0.5646)  classification_loss: 0.2113 (0.2099)  time: 0.1953  data: 0.0002  max mem: 5511
[07:34:32.738109] Epoch: [4]  [ 80/781]  eta: 0:02:22  lr: 0.000205  training_loss: 0.7849 (0.7785)  mae_loss: 0.5753 (0.5677)  classification_loss: 0.2136 (0.2108)  time: 0.1954  data: 0.0003  max mem: 5511
[07:34:36.643988] Epoch: [4]  [100/781]  eta: 0:02:17  lr: 0.000206  training_loss: 0.7571 (0.7754)  mae_loss: 0.5381 (0.5646)  classification_loss: 0.2120 (0.2109)  time: 0.1952  data: 0.0002  max mem: 5511
[07:34:40.544090] Epoch: [4]  [120/781]  eta: 0:02:12  lr: 0.000208  training_loss: 0.7614 (0.7748)  mae_loss: 0.5557 (0.5643)  classification_loss: 0.2084 (0.2106)  time: 0.1949  data: 0.0002  max mem: 5511
[07:34:44.456677] Epoch: [4]  [140/781]  eta: 0:02:08  lr: 0.000209  training_loss: 0.7526 (0.7722)  mae_loss: 0.5455 (0.5621)  classification_loss: 0.2082 (0.2101)  time: 0.1955  data: 0.0003  max mem: 5511
[07:34:48.368451] Epoch: [4]  [160/781]  eta: 0:02:03  lr: 0.000210  training_loss: 0.7605 (0.7711)  mae_loss: 0.5505 (0.5614)  classification_loss: 0.2044 (0.2096)  time: 0.1955  data: 0.0002  max mem: 5511
[07:34:52.267755] Epoch: [4]  [180/781]  eta: 0:01:59  lr: 0.000212  training_loss: 0.7567 (0.7706)  mae_loss: 0.5454 (0.5609)  classification_loss: 0.2113 (0.2097)  time: 0.1948  data: 0.0002  max mem: 5511
[07:34:56.172681] Epoch: [4]  [200/781]  eta: 0:01:55  lr: 0.000213  training_loss: 0.7516 (0.7699)  mae_loss: 0.5366 (0.5601)  classification_loss: 0.2098 (0.2098)  time: 0.1952  data: 0.0002  max mem: 5511
[07:35:00.084097] Epoch: [4]  [220/781]  eta: 0:01:51  lr: 0.000214  training_loss: 0.7737 (0.7699)  mae_loss: 0.5645 (0.5600)  classification_loss: 0.2079 (0.2099)  time: 0.1955  data: 0.0002  max mem: 5511
[07:35:04.001932] Epoch: [4]  [240/781]  eta: 0:01:47  lr: 0.000215  training_loss: 0.7979 (0.7718)  mae_loss: 0.5852 (0.5620)  classification_loss: 0.2090 (0.2098)  time: 0.1957  data: 0.0002  max mem: 5511
[07:35:07.993464] Epoch: [4]  [260/781]  eta: 0:01:43  lr: 0.000217  training_loss: 0.7586 (0.7713)  mae_loss: 0.5577 (0.5617)  classification_loss: 0.2074 (0.2097)  time: 0.1995  data: 0.0002  max mem: 5511
[07:35:11.908600] Epoch: [4]  [280/781]  eta: 0:01:39  lr: 0.000218  training_loss: 0.7681 (0.7717)  mae_loss: 0.5560 (0.5622)  classification_loss: 0.2062 (0.2096)  time: 0.1957  data: 0.0002  max mem: 5511
[07:35:15.822743] Epoch: [4]  [300/781]  eta: 0:01:35  lr: 0.000219  training_loss: 0.7568 (0.7718)  mae_loss: 0.5470 (0.5622)  classification_loss: 0.2101 (0.2096)  time: 0.1956  data: 0.0002  max mem: 5511
[07:35:19.727951] Epoch: [4]  [320/781]  eta: 0:01:31  lr: 0.000220  training_loss: 0.7644 (0.7717)  mae_loss: 0.5567 (0.5622)  classification_loss: 0.2056 (0.2094)  time: 0.1952  data: 0.0002  max mem: 5511
[07:35:23.633506] Epoch: [4]  [340/781]  eta: 0:01:27  lr: 0.000222  training_loss: 0.7572 (0.7711)  mae_loss: 0.5465 (0.5617)  classification_loss: 0.2107 (0.2094)  time: 0.1952  data: 0.0002  max mem: 5511
[07:35:27.556400] Epoch: [4]  [360/781]  eta: 0:01:23  lr: 0.000223  training_loss: 0.7603 (0.7708)  mae_loss: 0.5528 (0.5614)  classification_loss: 0.2089 (0.2095)  time: 0.1961  data: 0.0002  max mem: 5511
[07:35:31.460939] Epoch: [4]  [380/781]  eta: 0:01:19  lr: 0.000224  training_loss: 0.7536 (0.7707)  mae_loss: 0.5418 (0.5613)  classification_loss: 0.2078 (0.2094)  time: 0.1952  data: 0.0002  max mem: 5511
[07:35:35.364147] Epoch: [4]  [400/781]  eta: 0:01:15  lr: 0.000226  training_loss: 0.7592 (0.7700)  mae_loss: 0.5537 (0.5607)  classification_loss: 0.2052 (0.2093)  time: 0.1951  data: 0.0003  max mem: 5511
[07:35:39.271228] Epoch: [4]  [420/781]  eta: 0:01:11  lr: 0.000227  training_loss: 0.7370 (0.7685)  mae_loss: 0.5258 (0.5593)  classification_loss: 0.2065 (0.2092)  time: 0.1953  data: 0.0003  max mem: 5511
[07:35:43.163822] Epoch: [4]  [440/781]  eta: 0:01:07  lr: 0.000228  training_loss: 0.7373 (0.7672)  mae_loss: 0.5355 (0.5581)  classification_loss: 0.2064 (0.2091)  time: 0.1945  data: 0.0002  max mem: 5511
[07:35:47.099376] Epoch: [4]  [460/781]  eta: 0:01:03  lr: 0.000229  training_loss: 0.7419 (0.7662)  mae_loss: 0.5299 (0.5571)  classification_loss: 0.2093 (0.2091)  time: 0.1967  data: 0.0003  max mem: 5511
[07:35:51.042314] Epoch: [4]  [480/781]  eta: 0:00:59  lr: 0.000231  training_loss: 0.7252 (0.7650)  mae_loss: 0.5173 (0.5558)  classification_loss: 0.2091 (0.2091)  time: 0.1971  data: 0.0002  max mem: 5511
[07:35:54.957113] Epoch: [4]  [500/781]  eta: 0:00:55  lr: 0.000232  training_loss: 0.7434 (0.7642)  mae_loss: 0.5261 (0.5552)  classification_loss: 0.2058 (0.2090)  time: 0.1957  data: 0.0002  max mem: 5511
[07:35:58.863015] Epoch: [4]  [520/781]  eta: 0:00:51  lr: 0.000233  training_loss: 0.7361 (0.7636)  mae_loss: 0.5302 (0.5547)  classification_loss: 0.2051 (0.2089)  time: 0.1952  data: 0.0002  max mem: 5511
[07:36:02.751030] Epoch: [4]  [540/781]  eta: 0:00:47  lr: 0.000235  training_loss: 0.7499 (0.7634)  mae_loss: 0.5410 (0.5545)  classification_loss: 0.2085 (0.2089)  time: 0.1943  data: 0.0002  max mem: 5511
[07:36:06.682750] Epoch: [4]  [560/781]  eta: 0:00:43  lr: 0.000236  training_loss: 0.7267 (0.7626)  mae_loss: 0.5249 (0.5538)  classification_loss: 0.2031 (0.2088)  time: 0.1965  data: 0.0002  max mem: 5511
[07:36:10.574312] Epoch: [4]  [580/781]  eta: 0:00:39  lr: 0.000237  training_loss: 0.7381 (0.7619)  mae_loss: 0.5281 (0.5532)  classification_loss: 0.2077 (0.2088)  time: 0.1945  data: 0.0002  max mem: 5511
[07:36:14.478712] Epoch: [4]  [600/781]  eta: 0:00:35  lr: 0.000238  training_loss: 0.7474 (0.7613)  mae_loss: 0.5385 (0.5526)  classification_loss: 0.2045 (0.2087)  time: 0.1951  data: 0.0003  max mem: 5511
[07:36:18.425202] Epoch: [4]  [620/781]  eta: 0:00:31  lr: 0.000240  training_loss: 0.7414 (0.7609)  mae_loss: 0.5419 (0.5523)  classification_loss: 0.2040 (0.2086)  time: 0.1972  data: 0.0002  max mem: 5511
[07:36:22.332649] Epoch: [4]  [640/781]  eta: 0:00:27  lr: 0.000241  training_loss: 0.7362 (0.7602)  mae_loss: 0.5126 (0.5516)  classification_loss: 0.2058 (0.2086)  time: 0.1953  data: 0.0002  max mem: 5511
[07:36:26.258172] Epoch: [4]  [660/781]  eta: 0:00:23  lr: 0.000242  training_loss: 0.7459 (0.7596)  mae_loss: 0.5510 (0.5511)  classification_loss: 0.2071 (0.2085)  time: 0.1962  data: 0.0004  max mem: 5511
[07:36:30.183563] Epoch: [4]  [680/781]  eta: 0:00:19  lr: 0.000244  training_loss: 0.7311 (0.7589)  mae_loss: 0.5240 (0.5505)  classification_loss: 0.2082 (0.2085)  time: 0.1962  data: 0.0004  max mem: 5511
[07:36:34.086977] Epoch: [4]  [700/781]  eta: 0:00:15  lr: 0.000245  training_loss: 0.7539 (0.7590)  mae_loss: 0.5292 (0.5505)  classification_loss: 0.2049 (0.2085)  time: 0.1951  data: 0.0002  max mem: 5511
[07:36:37.972695] Epoch: [4]  [720/781]  eta: 0:00:11  lr: 0.000246  training_loss: 0.7208 (0.7583)  mae_loss: 0.5105 (0.5498)  classification_loss: 0.2074 (0.2085)  time: 0.1942  data: 0.0003  max mem: 5511
[07:36:41.923715] Epoch: [4]  [740/781]  eta: 0:00:08  lr: 0.000247  training_loss: 0.7479 (0.7578)  mae_loss: 0.5462 (0.5494)  classification_loss: 0.2067 (0.2085)  time: 0.1975  data: 0.0003  max mem: 5511
[07:36:45.842096] Epoch: [4]  [760/781]  eta: 0:00:04  lr: 0.000249  training_loss: 0.7282 (0.7572)  mae_loss: 0.5151 (0.5487)  classification_loss: 0.2071 (0.2085)  time: 0.1958  data: 0.0002  max mem: 5511
[07:36:49.777884] Epoch: [4]  [780/781]  eta: 0:00:00  lr: 0.000250  training_loss: 0.7188 (0.7565)  mae_loss: 0.5046 (0.5480)  classification_loss: 0.2067 (0.2085)  time: 0.1967  data: 0.0002  max mem: 5511
[07:36:49.932604] Epoch: [4] Total time: 0:02:33 (0.1968 s / it)
[07:36:49.933316] Averaged stats: lr: 0.000250  training_loss: 0.7188 (0.7565)  mae_loss: 0.5046 (0.5480)  classification_loss: 0.2067 (0.2085)
[07:36:50.490397] Test:  [  0/157]  eta: 0:01:26  testing_loss: 1.6439 (1.6439)  acc1: 37.5000 (37.5000)  acc5: 87.5000 (87.5000)  time: 0.5527  data: 0.5231  max mem: 5511
[07:36:50.786331] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 1.6498 (1.6717)  acc1: 40.6250 (42.3295)  acc5: 90.6250 (89.7727)  time: 0.0769  data: 0.0477  max mem: 5511
[07:36:51.072437] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.6493 (1.6528)  acc1: 43.7500 (44.0476)  acc5: 90.6250 (90.1042)  time: 0.0289  data: 0.0002  max mem: 5511
[07:36:51.355928] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 1.6469 (1.6442)  acc1: 43.7500 (44.7581)  acc5: 89.0625 (89.9698)  time: 0.0284  data: 0.0002  max mem: 5511
[07:36:51.638044] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 1.6454 (1.6519)  acc1: 43.7500 (43.9405)  acc5: 89.0625 (89.9390)  time: 0.0282  data: 0.0002  max mem: 5511
[07:36:51.919347] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.6536 (1.6537)  acc1: 43.7500 (43.8419)  acc5: 90.6250 (90.1654)  time: 0.0281  data: 0.0001  max mem: 5511
[07:36:52.200804] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.6291 (1.6494)  acc1: 43.7500 (44.1086)  acc5: 90.6250 (90.1895)  time: 0.0280  data: 0.0002  max mem: 5511
[07:36:52.483007] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.6067 (1.6481)  acc1: 45.3125 (44.2562)  acc5: 90.6250 (90.4710)  time: 0.0281  data: 0.0002  max mem: 5511
[07:36:52.765726] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.6420 (1.6489)  acc1: 45.3125 (44.3094)  acc5: 92.1875 (90.5671)  time: 0.0281  data: 0.0002  max mem: 5511
[07:36:53.050459] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.6733 (1.6522)  acc1: 43.7500 (44.1964)  acc5: 92.1875 (90.5563)  time: 0.0282  data: 0.0002  max mem: 5511
[07:36:53.333013] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.6763 (1.6561)  acc1: 42.1875 (43.8119)  acc5: 89.0625 (90.4703)  time: 0.0282  data: 0.0002  max mem: 5511
[07:36:53.615724] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.6822 (1.6590)  acc1: 42.1875 (43.8345)  acc5: 89.0625 (90.3998)  time: 0.0281  data: 0.0002  max mem: 5511
[07:36:53.903266] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.6585 (1.6568)  acc1: 42.1875 (43.6854)  acc5: 90.6250 (90.4571)  time: 0.0284  data: 0.0002  max mem: 5511
[07:36:54.185657] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.6426 (1.6577)  acc1: 43.7500 (43.7619)  acc5: 89.0625 (90.2552)  time: 0.0284  data: 0.0002  max mem: 5511
[07:36:54.466726] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.6743 (1.6567)  acc1: 45.3125 (43.9051)  acc5: 89.0625 (90.2704)  time: 0.0281  data: 0.0002  max mem: 5511
[07:36:54.745555] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.6721 (1.6557)  acc1: 43.7500 (43.9259)  acc5: 89.0625 (90.2628)  time: 0.0279  data: 0.0001  max mem: 5511
[07:36:54.895479] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.6486 (1.6564)  acc1: 42.1875 (43.8100)  acc5: 89.0625 (90.2500)  time: 0.0269  data: 0.0001  max mem: 5511
[07:36:55.049659] Test: Total time: 0:00:05 (0.0326 s / it)
[07:36:55.050099] * Acc@1 43.810 Acc@5 90.250 loss 1.656
[07:36:55.050396] Accuracy of the network on the 10000 test images: 43.8%
[07:36:55.050630] Max accuracy: 43.81%
[07:36:55.333776] log_dir: ./output_dir
[07:36:56.294112] Epoch: [5]  [  0/781]  eta: 0:12:28  lr: 0.000250  training_loss: 0.7141 (0.7141)  mae_loss: 0.5162 (0.5162)  classification_loss: 0.1979 (0.1979)  time: 0.9586  data: 0.7400  max mem: 5511
[07:37:00.244763] Epoch: [5]  [ 20/781]  eta: 0:02:57  lr: 0.000250  training_loss: 0.7229 (0.7321)  mae_loss: 0.5173 (0.5273)  classification_loss: 0.2016 (0.2048)  time: 0.1974  data: 0.0002  max mem: 5511
[07:37:04.155747] Epoch: [5]  [ 40/781]  eta: 0:02:39  lr: 0.000250  training_loss: 0.7667 (0.7482)  mae_loss: 0.5617 (0.5410)  classification_loss: 0.2099 (0.2072)  time: 0.1955  data: 0.0003  max mem: 5511
[07:37:08.064505] Epoch: [5]  [ 60/781]  eta: 0:02:30  lr: 0.000250  training_loss: 0.7413 (0.7444)  mae_loss: 0.5338 (0.5381)  classification_loss: 0.2052 (0.2062)  time: 0.1953  data: 0.0002  max mem: 5511
[07:37:11.969486] Epoch: [5]  [ 80/781]  eta: 0:02:23  lr: 0.000250  training_loss: 0.7171 (0.7401)  mae_loss: 0.5090 (0.5341)  classification_loss: 0.2047 (0.2060)  time: 0.1952  data: 0.0002  max mem: 5511
[07:37:15.874657] Epoch: [5]  [100/781]  eta: 0:02:18  lr: 0.000250  training_loss: 0.7221 (0.7375)  mae_loss: 0.5084 (0.5313)  classification_loss: 0.2049 (0.2062)  time: 0.1952  data: 0.0004  max mem: 5511
[07:37:19.795356] Epoch: [5]  [120/781]  eta: 0:02:13  lr: 0.000250  training_loss: 0.7078 (0.7327)  mae_loss: 0.5015 (0.5268)  classification_loss: 0.2046 (0.2059)  time: 0.1960  data: 0.0002  max mem: 5511
[07:37:23.684896] Epoch: [5]  [140/781]  eta: 0:02:08  lr: 0.000250  training_loss: 0.7169 (0.7310)  mae_loss: 0.5051 (0.5254)  classification_loss: 0.2021 (0.2056)  time: 0.1944  data: 0.0002  max mem: 5511
[07:37:27.594891] Epoch: [5]  [160/781]  eta: 0:02:04  lr: 0.000250  training_loss: 0.7149 (0.7297)  mae_loss: 0.5107 (0.5242)  classification_loss: 0.2058 (0.2056)  time: 0.1954  data: 0.0002  max mem: 5511
[07:37:31.496852] Epoch: [5]  [180/781]  eta: 0:02:00  lr: 0.000250  training_loss: 0.7313 (0.7296)  mae_loss: 0.5285 (0.5238)  classification_loss: 0.2061 (0.2058)  time: 0.1950  data: 0.0002  max mem: 5511
[07:37:35.452545] Epoch: [5]  [200/781]  eta: 0:01:55  lr: 0.000250  training_loss: 0.7210 (0.7298)  mae_loss: 0.5188 (0.5239)  classification_loss: 0.2040 (0.2059)  time: 0.1977  data: 0.0002  max mem: 5511
[07:37:39.361053] Epoch: [5]  [220/781]  eta: 0:01:51  lr: 0.000250  training_loss: 0.7293 (0.7300)  mae_loss: 0.5116 (0.5240)  classification_loss: 0.2068 (0.2060)  time: 0.1954  data: 0.0003  max mem: 5511
[07:37:43.275196] Epoch: [5]  [240/781]  eta: 0:01:47  lr: 0.000250  training_loss: 0.7364 (0.7304)  mae_loss: 0.5176 (0.5244)  classification_loss: 0.2051 (0.2061)  time: 0.1956  data: 0.0002  max mem: 5511
[07:37:47.264082] Epoch: [5]  [260/781]  eta: 0:01:43  lr: 0.000250  training_loss: 0.7309 (0.7303)  mae_loss: 0.5305 (0.5245)  classification_loss: 0.2009 (0.2058)  time: 0.1994  data: 0.0003  max mem: 5511
[07:37:51.152865] Epoch: [5]  [280/781]  eta: 0:01:39  lr: 0.000250  training_loss: 0.7339 (0.7315)  mae_loss: 0.5413 (0.5257)  classification_loss: 0.2035 (0.2058)  time: 0.1944  data: 0.0002  max mem: 5511
[07:37:55.125098] Epoch: [5]  [300/781]  eta: 0:01:35  lr: 0.000250  training_loss: 0.7129 (0.7310)  mae_loss: 0.5078 (0.5250)  classification_loss: 0.2102 (0.2061)  time: 0.1985  data: 0.0002  max mem: 5511
[07:37:59.044238] Epoch: [5]  [320/781]  eta: 0:01:31  lr: 0.000250  training_loss: 0.7421 (0.7316)  mae_loss: 0.5361 (0.5255)  classification_loss: 0.2063 (0.2061)  time: 0.1959  data: 0.0002  max mem: 5511
[07:38:02.947478] Epoch: [5]  [340/781]  eta: 0:01:27  lr: 0.000250  training_loss: 0.7102 (0.7305)  mae_loss: 0.5076 (0.5243)  classification_loss: 0.2044 (0.2062)  time: 0.1951  data: 0.0002  max mem: 5511
[07:38:06.861219] Epoch: [5]  [360/781]  eta: 0:01:23  lr: 0.000250  training_loss: 0.7080 (0.7299)  mae_loss: 0.5086 (0.5238)  classification_loss: 0.2063 (0.2062)  time: 0.1956  data: 0.0002  max mem: 5511
[07:38:10.766047] Epoch: [5]  [380/781]  eta: 0:01:19  lr: 0.000250  training_loss: 0.7146 (0.7294)  mae_loss: 0.5021 (0.5232)  classification_loss: 0.2063 (0.2062)  time: 0.1952  data: 0.0002  max mem: 5511
[07:38:14.649953] Epoch: [5]  [400/781]  eta: 0:01:15  lr: 0.000250  training_loss: 0.7211 (0.7283)  mae_loss: 0.5171 (0.5222)  classification_loss: 0.2060 (0.2061)  time: 0.1941  data: 0.0004  max mem: 5511
[07:38:18.555003] Epoch: [5]  [420/781]  eta: 0:01:11  lr: 0.000250  training_loss: 0.7318 (0.7282)  mae_loss: 0.5186 (0.5221)  classification_loss: 0.2064 (0.2061)  time: 0.1951  data: 0.0002  max mem: 5511
[07:38:22.451061] Epoch: [5]  [440/781]  eta: 0:01:07  lr: 0.000250  training_loss: 0.7246 (0.7276)  mae_loss: 0.5176 (0.5217)  classification_loss: 0.2020 (0.2060)  time: 0.1947  data: 0.0002  max mem: 5511
[07:38:26.371003] Epoch: [5]  [460/781]  eta: 0:01:03  lr: 0.000250  training_loss: 0.6832 (0.7262)  mae_loss: 0.4892 (0.5204)  classification_loss: 0.2030 (0.2059)  time: 0.1959  data: 0.0004  max mem: 5511
[07:38:30.266154] Epoch: [5]  [480/781]  eta: 0:00:59  lr: 0.000250  training_loss: 0.6882 (0.7252)  mae_loss: 0.4768 (0.5193)  classification_loss: 0.2077 (0.2060)  time: 0.1946  data: 0.0002  max mem: 5511
[07:38:34.161120] Epoch: [5]  [500/781]  eta: 0:00:55  lr: 0.000250  training_loss: 0.7360 (0.7253)  mae_loss: 0.5281 (0.5195)  classification_loss: 0.2021 (0.2058)  time: 0.1947  data: 0.0002  max mem: 5511
[07:38:38.059599] Epoch: [5]  [520/781]  eta: 0:00:51  lr: 0.000250  training_loss: 0.7061 (0.7245)  mae_loss: 0.4978 (0.5187)  classification_loss: 0.2015 (0.2058)  time: 0.1949  data: 0.0002  max mem: 5511
[07:38:41.961754] Epoch: [5]  [540/781]  eta: 0:00:47  lr: 0.000250  training_loss: 0.6946 (0.7238)  mae_loss: 0.4936 (0.5180)  classification_loss: 0.2058 (0.2058)  time: 0.1950  data: 0.0003  max mem: 5511
[07:38:45.866588] Epoch: [5]  [560/781]  eta: 0:00:43  lr: 0.000250  training_loss: 0.7179 (0.7231)  mae_loss: 0.5127 (0.5174)  classification_loss: 0.2020 (0.2057)  time: 0.1951  data: 0.0002  max mem: 5511
[07:38:49.780164] Epoch: [5]  [580/781]  eta: 0:00:39  lr: 0.000250  training_loss: 0.6894 (0.7222)  mae_loss: 0.4881 (0.5165)  classification_loss: 0.2056 (0.2057)  time: 0.1956  data: 0.0002  max mem: 5511
[07:38:53.701624] Epoch: [5]  [600/781]  eta: 0:00:35  lr: 0.000250  training_loss: 0.7082 (0.7219)  mae_loss: 0.4955 (0.5162)  classification_loss: 0.2062 (0.2057)  time: 0.1960  data: 0.0003  max mem: 5511
[07:38:57.658722] Epoch: [5]  [620/781]  eta: 0:00:31  lr: 0.000250  training_loss: 0.7313 (0.7216)  mae_loss: 0.5161 (0.5159)  classification_loss: 0.2018 (0.2057)  time: 0.1978  data: 0.0002  max mem: 5511
[07:39:01.568102] Epoch: [5]  [640/781]  eta: 0:00:27  lr: 0.000250  training_loss: 0.7108 (0.7214)  mae_loss: 0.5085 (0.5158)  classification_loss: 0.2058 (0.2057)  time: 0.1954  data: 0.0003  max mem: 5511
[07:39:05.480366] Epoch: [5]  [660/781]  eta: 0:00:23  lr: 0.000250  training_loss: 0.6980 (0.7207)  mae_loss: 0.4897 (0.5150)  classification_loss: 0.2048 (0.2056)  time: 0.1955  data: 0.0002  max mem: 5511
[07:39:09.379004] Epoch: [5]  [680/781]  eta: 0:00:19  lr: 0.000250  training_loss: 0.7098 (0.7203)  mae_loss: 0.5080 (0.5148)  classification_loss: 0.2028 (0.2055)  time: 0.1948  data: 0.0002  max mem: 5511
[07:39:13.351386] Epoch: [5]  [700/781]  eta: 0:00:15  lr: 0.000250  training_loss: 0.7063 (0.7197)  mae_loss: 0.5060 (0.5142)  classification_loss: 0.2043 (0.2055)  time: 0.1985  data: 0.0002  max mem: 5511
[07:39:17.252688] Epoch: [5]  [720/781]  eta: 0:00:12  lr: 0.000250  training_loss: 0.6654 (0.7185)  mae_loss: 0.4683 (0.5131)  classification_loss: 0.2048 (0.2054)  time: 0.1950  data: 0.0002  max mem: 5511
[07:39:21.159609] Epoch: [5]  [740/781]  eta: 0:00:08  lr: 0.000250  training_loss: 0.7060 (0.7178)  mae_loss: 0.5032 (0.5124)  classification_loss: 0.2044 (0.2054)  time: 0.1952  data: 0.0002  max mem: 5511
[07:39:25.072458] Epoch: [5]  [760/781]  eta: 0:00:04  lr: 0.000250  training_loss: 0.6795 (0.7171)  mae_loss: 0.4799 (0.5118)  classification_loss: 0.2018 (0.2053)  time: 0.1955  data: 0.0003  max mem: 5511
[07:39:28.954467] Epoch: [5]  [780/781]  eta: 0:00:00  lr: 0.000250  training_loss: 0.6796 (0.7162)  mae_loss: 0.4652 (0.5109)  classification_loss: 0.2066 (0.2053)  time: 0.1940  data: 0.0002  max mem: 5511
[07:39:29.126554] Epoch: [5] Total time: 0:02:33 (0.1969 s / it)
[07:39:29.127041] Averaged stats: lr: 0.000250  training_loss: 0.6796 (0.7162)  mae_loss: 0.4652 (0.5109)  classification_loss: 0.2066 (0.2053)
[07:39:29.741685] Test:  [  0/157]  eta: 0:01:35  testing_loss: 1.5951 (1.5951)  acc1: 40.6250 (40.6250)  acc5: 90.6250 (90.6250)  time: 0.6100  data: 0.5788  max mem: 5511
[07:39:30.030579] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 1.5758 (1.6078)  acc1: 43.7500 (46.3068)  acc5: 92.1875 (91.9034)  time: 0.0815  data: 0.0528  max mem: 5511
[07:39:30.313117] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.5735 (1.5953)  acc1: 46.8750 (47.6190)  acc5: 92.1875 (91.6667)  time: 0.0284  data: 0.0002  max mem: 5511
[07:39:30.593562] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 1.5514 (1.5861)  acc1: 48.4375 (48.2359)  acc5: 90.6250 (91.4819)  time: 0.0280  data: 0.0002  max mem: 5511
[07:39:30.876440] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 1.5662 (1.5926)  acc1: 48.4375 (47.7134)  acc5: 90.6250 (91.2348)  time: 0.0281  data: 0.0002  max mem: 5511
[07:39:31.161357] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.6029 (1.5923)  acc1: 48.4375 (47.6409)  acc5: 92.1875 (91.5748)  time: 0.0283  data: 0.0002  max mem: 5511
[07:39:31.442456] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.5788 (1.5879)  acc1: 48.4375 (48.2582)  acc5: 92.1875 (91.5727)  time: 0.0282  data: 0.0002  max mem: 5511
[07:39:31.728387] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.5572 (1.5874)  acc1: 46.8750 (48.0634)  acc5: 92.1875 (91.6593)  time: 0.0282  data: 0.0002  max mem: 5511
[07:39:32.013097] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.5721 (1.5876)  acc1: 45.3125 (47.8395)  acc5: 92.1875 (91.7052)  time: 0.0283  data: 0.0002  max mem: 5511
[07:39:32.294745] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.6061 (1.5923)  acc1: 42.1875 (47.3729)  acc5: 90.6250 (91.7067)  time: 0.0282  data: 0.0002  max mem: 5511
[07:39:32.579906] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.6134 (1.5961)  acc1: 40.6250 (46.8750)  acc5: 90.6250 (91.5532)  time: 0.0282  data: 0.0001  max mem: 5511
[07:39:32.863890] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.6134 (1.5999)  acc1: 43.7500 (46.8609)  acc5: 89.0625 (91.3711)  time: 0.0283  data: 0.0002  max mem: 5511
[07:39:33.147084] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.5863 (1.5997)  acc1: 42.1875 (46.5393)  acc5: 92.1875 (91.4902)  time: 0.0282  data: 0.0002  max mem: 5511
[07:39:33.434538] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.5845 (1.6013)  acc1: 43.7500 (46.3979)  acc5: 92.1875 (91.3884)  time: 0.0284  data: 0.0002  max mem: 5511
[07:39:33.718429] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.5974 (1.6019)  acc1: 43.7500 (46.3431)  acc5: 90.6250 (91.3564)  time: 0.0284  data: 0.0002  max mem: 5511
[07:39:33.998619] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.6285 (1.6014)  acc1: 45.3125 (46.3783)  acc5: 90.6250 (91.3079)  time: 0.0280  data: 0.0001  max mem: 5511
[07:39:34.149454] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.5641 (1.6007)  acc1: 46.8750 (46.3300)  acc5: 90.6250 (91.3500)  time: 0.0270  data: 0.0001  max mem: 5511
[07:39:34.307224] Test: Total time: 0:00:05 (0.0330 s / it)
[07:39:34.307714] * Acc@1 46.330 Acc@5 91.350 loss 1.601
[07:39:34.308093] Accuracy of the network on the 10000 test images: 46.3%
[07:39:34.308307] Max accuracy: 46.33%
[07:39:34.429210] log_dir: ./output_dir
[07:39:35.220588] Epoch: [6]  [  0/781]  eta: 0:10:16  lr: 0.000250  training_loss: 0.6954 (0.6954)  mae_loss: 0.4925 (0.4925)  classification_loss: 0.2029 (0.2029)  time: 0.7898  data: 0.5893  max mem: 5511
[07:39:39.138305] Epoch: [6]  [ 20/781]  eta: 0:02:50  lr: 0.000250  training_loss: 0.6971 (0.6868)  mae_loss: 0.4881 (0.4854)  classification_loss: 0.2008 (0.2014)  time: 0.1958  data: 0.0002  max mem: 5511
[07:39:43.042448] Epoch: [6]  [ 40/781]  eta: 0:02:35  lr: 0.000250  training_loss: 0.6864 (0.6878)  mae_loss: 0.4870 (0.4859)  classification_loss: 0.2012 (0.2020)  time: 0.1951  data: 0.0002  max mem: 5511
[07:39:46.957245] Epoch: [6]  [ 60/781]  eta: 0:02:27  lr: 0.000250  training_loss: 0.6802 (0.6862)  mae_loss: 0.4796 (0.4838)  classification_loss: 0.2024 (0.2024)  time: 0.1957  data: 0.0002  max mem: 5511
[07:39:50.884842] Epoch: [6]  [ 80/781]  eta: 0:02:22  lr: 0.000250  training_loss: 0.6675 (0.6823)  mae_loss: 0.4591 (0.4794)  classification_loss: 0.2035 (0.2030)  time: 0.1963  data: 0.0004  max mem: 5511
[07:39:54.786597] Epoch: [6]  [100/781]  eta: 0:02:17  lr: 0.000250  training_loss: 0.6615 (0.6813)  mae_loss: 0.4602 (0.4780)  classification_loss: 0.2036 (0.2033)  time: 0.1950  data: 0.0002  max mem: 5511
[07:39:58.690379] Epoch: [6]  [120/781]  eta: 0:02:12  lr: 0.000250  training_loss: 0.6609 (0.6779)  mae_loss: 0.4576 (0.4748)  classification_loss: 0.2024 (0.2031)  time: 0.1951  data: 0.0002  max mem: 5511
[07:40:02.605096] Epoch: [6]  [140/781]  eta: 0:02:08  lr: 0.000250  training_loss: 0.6372 (0.6728)  mae_loss: 0.4324 (0.4697)  classification_loss: 0.2035 (0.2030)  time: 0.1957  data: 0.0002  max mem: 5511
[07:40:06.521715] Epoch: [6]  [160/781]  eta: 0:02:03  lr: 0.000250  training_loss: 0.6360 (0.6689)  mae_loss: 0.4272 (0.4655)  classification_loss: 0.2059 (0.2034)  time: 0.1958  data: 0.0003  max mem: 5511
[07:40:10.431398] Epoch: [6]  [180/781]  eta: 0:01:59  lr: 0.000250  training_loss: 0.6334 (0.6655)  mae_loss: 0.4210 (0.4622)  classification_loss: 0.2017 (0.2033)  time: 0.1954  data: 0.0002  max mem: 5511
[07:40:14.332809] Epoch: [6]  [200/781]  eta: 0:01:55  lr: 0.000250  training_loss: 0.6394 (0.6641)  mae_loss: 0.4423 (0.4607)  classification_loss: 0.2053 (0.2034)  time: 0.1950  data: 0.0002  max mem: 5511
[07:40:18.234288] Epoch: [6]  [220/781]  eta: 0:01:51  lr: 0.000250  training_loss: 0.6306 (0.6618)  mae_loss: 0.4237 (0.4584)  classification_loss: 0.2039 (0.2034)  time: 0.1950  data: 0.0002  max mem: 5511
[07:40:22.163326] Epoch: [6]  [240/781]  eta: 0:01:47  lr: 0.000250  training_loss: 0.6292 (0.6599)  mae_loss: 0.4280 (0.4564)  classification_loss: 0.2048 (0.2035)  time: 0.1964  data: 0.0002  max mem: 5511
[07:40:26.057437] Epoch: [6]  [260/781]  eta: 0:01:43  lr: 0.000250  training_loss: 0.6227 (0.6575)  mae_loss: 0.4200 (0.4541)  classification_loss: 0.2024 (0.2034)  time: 0.1946  data: 0.0002  max mem: 5511
[07:40:29.953424] Epoch: [6]  [280/781]  eta: 0:01:38  lr: 0.000250  training_loss: 0.6369 (0.6569)  mae_loss: 0.4241 (0.4536)  classification_loss: 0.1987 (0.2033)  time: 0.1947  data: 0.0002  max mem: 5511
[07:40:33.867732] Epoch: [6]  [300/781]  eta: 0:01:34  lr: 0.000250  training_loss: 0.6330 (0.6552)  mae_loss: 0.4270 (0.4519)  classification_loss: 0.2040 (0.2033)  time: 0.1956  data: 0.0002  max mem: 5511
[07:40:37.801446] Epoch: [6]  [320/781]  eta: 0:01:30  lr: 0.000250  training_loss: 0.6176 (0.6527)  mae_loss: 0.4144 (0.4495)  classification_loss: 0.2007 (0.2032)  time: 0.1966  data: 0.0002  max mem: 5511
[07:40:41.696431] Epoch: [6]  [340/781]  eta: 0:01:26  lr: 0.000250  training_loss: 0.6237 (0.6508)  mae_loss: 0.4229 (0.4479)  classification_loss: 0.2001 (0.2029)  time: 0.1947  data: 0.0002  max mem: 5511
[07:40:45.585681] Epoch: [6]  [360/781]  eta: 0:01:22  lr: 0.000250  training_loss: 0.6350 (0.6502)  mae_loss: 0.4262 (0.4472)  classification_loss: 0.2048 (0.2030)  time: 0.1944  data: 0.0002  max mem: 5511
[07:40:49.483298] Epoch: [6]  [380/781]  eta: 0:01:18  lr: 0.000250  training_loss: 0.6417 (0.6500)  mae_loss: 0.4441 (0.4469)  classification_loss: 0.2016 (0.2030)  time: 0.1948  data: 0.0002  max mem: 5511
[07:40:53.387047] Epoch: [6]  [400/781]  eta: 0:01:14  lr: 0.000250  training_loss: 0.6216 (0.6484)  mae_loss: 0.4115 (0.4454)  classification_loss: 0.2019 (0.2030)  time: 0.1951  data: 0.0004  max mem: 5511
[07:40:57.318385] Epoch: [6]  [420/781]  eta: 0:01:11  lr: 0.000250  training_loss: 0.6105 (0.6469)  mae_loss: 0.4132 (0.4439)  classification_loss: 0.2023 (0.2030)  time: 0.1965  data: 0.0002  max mem: 5511
[07:41:01.232110] Epoch: [6]  [440/781]  eta: 0:01:07  lr: 0.000250  training_loss: 0.6081 (0.6451)  mae_loss: 0.4066 (0.4421)  classification_loss: 0.2020 (0.2029)  time: 0.1956  data: 0.0002  max mem: 5511
[07:41:05.137047] Epoch: [6]  [460/781]  eta: 0:01:03  lr: 0.000250  training_loss: 0.5861 (0.6427)  mae_loss: 0.3837 (0.4399)  classification_loss: 0.2007 (0.2028)  time: 0.1952  data: 0.0002  max mem: 5511
[07:41:09.089909] Epoch: [6]  [480/781]  eta: 0:00:59  lr: 0.000250  training_loss: 0.6077 (0.6416)  mae_loss: 0.4134 (0.4389)  classification_loss: 0.1998 (0.2027)  time: 0.1975  data: 0.0004  max mem: 5511
[07:41:13.016046] Epoch: [6]  [500/781]  eta: 0:00:55  lr: 0.000250  training_loss: 0.6027 (0.6404)  mae_loss: 0.3993 (0.4376)  classification_loss: 0.2035 (0.2027)  time: 0.1962  data: 0.0002  max mem: 5511
[07:41:16.910734] Epoch: [6]  [520/781]  eta: 0:00:51  lr: 0.000250  training_loss: 0.5858 (0.6388)  mae_loss: 0.3912 (0.4361)  classification_loss: 0.2006 (0.2027)  time: 0.1946  data: 0.0003  max mem: 5511
[07:41:20.837884] Epoch: [6]  [540/781]  eta: 0:00:47  lr: 0.000250  training_loss: 0.6234 (0.6379)  mae_loss: 0.4108 (0.4351)  classification_loss: 0.2041 (0.2028)  time: 0.1963  data: 0.0002  max mem: 5511
[07:41:24.768093] Epoch: [6]  [560/781]  eta: 0:00:43  lr: 0.000250  training_loss: 0.6195 (0.6371)  mae_loss: 0.4103 (0.4343)  classification_loss: 0.2017 (0.2027)  time: 0.1964  data: 0.0002  max mem: 5511
[07:41:28.662887] Epoch: [6]  [580/781]  eta: 0:00:39  lr: 0.000250  training_loss: 0.6209 (0.6365)  mae_loss: 0.4234 (0.4339)  classification_loss: 0.2007 (0.2027)  time: 0.1947  data: 0.0003  max mem: 5511
[07:41:32.624239] Epoch: [6]  [600/781]  eta: 0:00:35  lr: 0.000250  training_loss: 0.5988 (0.6356)  mae_loss: 0.4121 (0.4330)  classification_loss: 0.2019 (0.2026)  time: 0.1980  data: 0.0002  max mem: 5511
[07:41:36.618131] Epoch: [6]  [620/781]  eta: 0:00:31  lr: 0.000250  training_loss: 0.5993 (0.6343)  mae_loss: 0.3921 (0.4318)  classification_loss: 0.2004 (0.2025)  time: 0.1996  data: 0.0002  max mem: 5511
[07:41:40.536326] Epoch: [6]  [640/781]  eta: 0:00:27  lr: 0.000250  training_loss: 0.6005 (0.6333)  mae_loss: 0.3961 (0.4308)  classification_loss: 0.2051 (0.2026)  time: 0.1958  data: 0.0002  max mem: 5511
[07:41:44.437042] Epoch: [6]  [660/781]  eta: 0:00:23  lr: 0.000250  training_loss: 0.6040 (0.6321)  mae_loss: 0.3906 (0.4295)  classification_loss: 0.2001 (0.2026)  time: 0.1949  data: 0.0002  max mem: 5511
[07:41:48.366557] Epoch: [6]  [680/781]  eta: 0:00:19  lr: 0.000250  training_loss: 0.5794 (0.6306)  mae_loss: 0.3894 (0.4281)  classification_loss: 0.2000 (0.2024)  time: 0.1964  data: 0.0002  max mem: 5511
[07:41:52.261765] Epoch: [6]  [700/781]  eta: 0:00:15  lr: 0.000250  training_loss: 0.5940 (0.6296)  mae_loss: 0.3927 (0.4271)  classification_loss: 0.2014 (0.2025)  time: 0.1947  data: 0.0002  max mem: 5511
[07:41:56.173564] Epoch: [6]  [720/781]  eta: 0:00:11  lr: 0.000250  training_loss: 0.5913 (0.6285)  mae_loss: 0.3854 (0.4260)  classification_loss: 0.2033 (0.2025)  time: 0.1955  data: 0.0002  max mem: 5511
[07:42:00.114400] Epoch: [6]  [740/781]  eta: 0:00:08  lr: 0.000250  training_loss: 0.5908 (0.6274)  mae_loss: 0.3926 (0.4249)  classification_loss: 0.1989 (0.2025)  time: 0.1970  data: 0.0002  max mem: 5511
[07:42:04.011999] Epoch: [6]  [760/781]  eta: 0:00:04  lr: 0.000250  training_loss: 0.5808 (0.6263)  mae_loss: 0.3799 (0.4238)  classification_loss: 0.2017 (0.2025)  time: 0.1948  data: 0.0002  max mem: 5511
[07:42:07.901685] Epoch: [6]  [780/781]  eta: 0:00:00  lr: 0.000250  training_loss: 0.5926 (0.6255)  mae_loss: 0.3783 (0.4230)  classification_loss: 0.2030 (0.2025)  time: 0.1944  data: 0.0002  max mem: 5511
[07:42:08.049337] Epoch: [6] Total time: 0:02:33 (0.1967 s / it)
[07:42:08.049794] Averaged stats: lr: 0.000250  training_loss: 0.5926 (0.6255)  mae_loss: 0.3783 (0.4230)  classification_loss: 0.2030 (0.2025)
[07:42:08.751172] Test:  [  0/157]  eta: 0:01:49  testing_loss: 1.5602 (1.5602)  acc1: 45.3125 (45.3125)  acc5: 90.6250 (90.6250)  time: 0.6952  data: 0.6645  max mem: 5511
[07:42:09.039997] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.5602 (1.5906)  acc1: 45.3125 (44.6023)  acc5: 92.1875 (91.6193)  time: 0.0892  data: 0.0606  max mem: 5511
[07:42:09.320933] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.5398 (1.5681)  acc1: 46.8750 (46.8750)  acc5: 92.1875 (91.8899)  time: 0.0283  data: 0.0002  max mem: 5511
[07:42:09.603249] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.5350 (1.5603)  acc1: 50.0000 (47.8327)  acc5: 92.1875 (91.8851)  time: 0.0280  data: 0.0002  max mem: 5511
[07:42:09.893056] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.5415 (1.5654)  acc1: 48.4375 (47.5229)  acc5: 90.6250 (91.7302)  time: 0.0285  data: 0.0002  max mem: 5511
[07:42:10.177606] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.5636 (1.5641)  acc1: 48.4375 (47.9167)  acc5: 90.6250 (91.7279)  time: 0.0286  data: 0.0003  max mem: 5511
[07:42:10.466564] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.5568 (1.5598)  acc1: 50.0000 (48.3607)  acc5: 92.1875 (91.5984)  time: 0.0286  data: 0.0002  max mem: 5511
[07:42:10.753782] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.5531 (1.5614)  acc1: 50.0000 (48.1954)  acc5: 92.1875 (91.5273)  time: 0.0287  data: 0.0002  max mem: 5511
[07:42:11.041226] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.5663 (1.5623)  acc1: 48.4375 (48.3410)  acc5: 90.6250 (91.5509)  time: 0.0286  data: 0.0002  max mem: 5511
[07:42:11.322046] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.5663 (1.5644)  acc1: 48.4375 (48.4375)  acc5: 90.6250 (91.5522)  time: 0.0283  data: 0.0002  max mem: 5511
[07:42:11.603493] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.6161 (1.5696)  acc1: 45.3125 (47.8342)  acc5: 90.6250 (91.4913)  time: 0.0280  data: 0.0002  max mem: 5511
[07:42:11.888376] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.6161 (1.5735)  acc1: 43.7500 (47.6070)  acc5: 92.1875 (91.4555)  time: 0.0282  data: 0.0002  max mem: 5511
[07:42:12.169439] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.5875 (1.5725)  acc1: 48.4375 (47.6111)  acc5: 92.1875 (91.5418)  time: 0.0282  data: 0.0001  max mem: 5511
[07:42:12.451851] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.5686 (1.5748)  acc1: 48.4375 (47.5072)  acc5: 92.1875 (91.4719)  time: 0.0281  data: 0.0001  max mem: 5511
[07:42:12.731802] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.5714 (1.5749)  acc1: 46.8750 (47.5177)  acc5: 92.1875 (91.4783)  time: 0.0280  data: 0.0001  max mem: 5511
[07:42:13.011048] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.5766 (1.5751)  acc1: 46.8750 (47.4959)  acc5: 90.6250 (91.4114)  time: 0.0278  data: 0.0001  max mem: 5511
[07:42:13.161695] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.5766 (1.5760)  acc1: 46.8750 (47.4200)  acc5: 90.6250 (91.4000)  time: 0.0269  data: 0.0001  max mem: 5511
[07:42:13.314734] Test: Total time: 0:00:05 (0.0335 s / it)
[07:42:13.316211] * Acc@1 47.420 Acc@5 91.400 loss 1.576
[07:42:13.316512] Accuracy of the network on the 10000 test images: 47.4%
[07:42:13.316698] Max accuracy: 47.42%
[07:42:13.517169] log_dir: ./output_dir
[07:42:14.421666] Epoch: [7]  [  0/781]  eta: 0:11:44  lr: 0.000250  training_loss: 0.5878 (0.5878)  mae_loss: 0.3834 (0.3834)  classification_loss: 0.2044 (0.2044)  time: 0.9026  data: 0.6566  max mem: 5511
[07:42:18.350462] Epoch: [7]  [ 20/781]  eta: 0:02:54  lr: 0.000250  training_loss: 0.5931 (0.5843)  mae_loss: 0.3921 (0.3845)  classification_loss: 0.1980 (0.1998)  time: 0.1963  data: 0.0004  max mem: 5511
[07:42:22.250877] Epoch: [7]  [ 40/781]  eta: 0:02:37  lr: 0.000250  training_loss: 0.6040 (0.5915)  mae_loss: 0.3944 (0.3899)  classification_loss: 0.2049 (0.2016)  time: 0.1949  data: 0.0002  max mem: 5511
[07:42:26.138653] Epoch: [7]  [ 60/781]  eta: 0:02:29  lr: 0.000250  training_loss: 0.5941 (0.5932)  mae_loss: 0.3922 (0.3912)  classification_loss: 0.2016 (0.2020)  time: 0.1943  data: 0.0005  max mem: 5511
[07:42:30.032030] Epoch: [7]  [ 80/781]  eta: 0:02:22  lr: 0.000250  training_loss: 0.5767 (0.5908)  mae_loss: 0.3716 (0.3891)  classification_loss: 0.2014 (0.2018)  time: 0.1946  data: 0.0003  max mem: 5511
[07:42:33.920940] Epoch: [7]  [100/781]  eta: 0:02:17  lr: 0.000250  training_loss: 0.5625 (0.5886)  mae_loss: 0.3640 (0.3870)  classification_loss: 0.2006 (0.2015)  time: 0.1944  data: 0.0002  max mem: 5511
[07:42:37.931142] Epoch: [7]  [120/781]  eta: 0:02:13  lr: 0.000250  training_loss: 0.5825 (0.5877)  mae_loss: 0.3793 (0.3868)  classification_loss: 0.1976 (0.2010)  time: 0.2004  data: 0.0002  max mem: 5511
[07:42:41.882056] Epoch: [7]  [140/781]  eta: 0:02:08  lr: 0.000250  training_loss: 0.5714 (0.5854)  mae_loss: 0.3699 (0.3846)  classification_loss: 0.1979 (0.2007)  time: 0.1975  data: 0.0002  max mem: 5511
[07:42:45.802426] Epoch: [7]  [160/781]  eta: 0:02:04  lr: 0.000250  training_loss: 0.5863 (0.5855)  mae_loss: 0.3861 (0.3848)  classification_loss: 0.2032 (0.2007)  time: 0.1959  data: 0.0003  max mem: 5511
[07:42:49.715342] Epoch: [7]  [180/781]  eta: 0:02:00  lr: 0.000250  training_loss: 0.5812 (0.5855)  mae_loss: 0.3795 (0.3849)  classification_loss: 0.1982 (0.2006)  time: 0.1956  data: 0.0002  max mem: 5511
[07:42:53.634385] Epoch: [7]  [200/781]  eta: 0:01:55  lr: 0.000250  training_loss: 0.5807 (0.5844)  mae_loss: 0.3772 (0.3837)  classification_loss: 0.2010 (0.2007)  time: 0.1959  data: 0.0003  max mem: 5511
[07:42:57.540574] Epoch: [7]  [220/781]  eta: 0:01:51  lr: 0.000250  training_loss: 0.5789 (0.5838)  mae_loss: 0.3839 (0.3832)  classification_loss: 0.1983 (0.2006)  time: 0.1952  data: 0.0003  max mem: 5511
[07:43:01.468972] Epoch: [7]  [240/781]  eta: 0:01:47  lr: 0.000250  training_loss: 0.5956 (0.5850)  mae_loss: 0.3884 (0.3842)  classification_loss: 0.2022 (0.2008)  time: 0.1963  data: 0.0003  max mem: 5511
[07:43:05.401610] Epoch: [7]  [260/781]  eta: 0:01:43  lr: 0.000250  training_loss: 0.5835 (0.5849)  mae_loss: 0.3783 (0.3840)  classification_loss: 0.2019 (0.2009)  time: 0.1966  data: 0.0002  max mem: 5511
[07:43:09.310293] Epoch: [7]  [280/781]  eta: 0:01:39  lr: 0.000250  training_loss: 0.5820 (0.5854)  mae_loss: 0.3847 (0.3846)  classification_loss: 0.2011 (0.2009)  time: 0.1954  data: 0.0002  max mem: 5511
[07:43:13.217643] Epoch: [7]  [300/781]  eta: 0:01:35  lr: 0.000250  training_loss: 0.5769 (0.5850)  mae_loss: 0.3767 (0.3841)  classification_loss: 0.2010 (0.2009)  time: 0.1953  data: 0.0003  max mem: 5511
[07:43:17.107747] Epoch: [7]  [320/781]  eta: 0:01:31  lr: 0.000250  training_loss: 0.5796 (0.5852)  mae_loss: 0.3780 (0.3843)  classification_loss: 0.2009 (0.2009)  time: 0.1944  data: 0.0002  max mem: 5511
[07:43:21.007379] Epoch: [7]  [340/781]  eta: 0:01:27  lr: 0.000250  training_loss: 0.5803 (0.5850)  mae_loss: 0.3842 (0.3843)  classification_loss: 0.1978 (0.2008)  time: 0.1949  data: 0.0002  max mem: 5511
[07:43:24.937108] Epoch: [7]  [360/781]  eta: 0:01:23  lr: 0.000250  training_loss: 0.5824 (0.5850)  mae_loss: 0.3817 (0.3844)  classification_loss: 0.2013 (0.2007)  time: 0.1964  data: 0.0002  max mem: 5511
[07:43:28.897264] Epoch: [7]  [380/781]  eta: 0:01:19  lr: 0.000250  training_loss: 0.5937 (0.5860)  mae_loss: 0.3969 (0.3853)  classification_loss: 0.2031 (0.2007)  time: 0.1979  data: 0.0003  max mem: 5511
[07:43:32.794537] Epoch: [7]  [400/781]  eta: 0:01:15  lr: 0.000250  training_loss: 0.5624 (0.5850)  mae_loss: 0.3599 (0.3844)  classification_loss: 0.1993 (0.2006)  time: 0.1948  data: 0.0002  max mem: 5511
[07:43:36.818805] Epoch: [7]  [420/781]  eta: 0:01:11  lr: 0.000250  training_loss: 0.5866 (0.5852)  mae_loss: 0.3850 (0.3847)  classification_loss: 0.1997 (0.2006)  time: 0.2011  data: 0.0003  max mem: 5511
[07:43:40.738236] Epoch: [7]  [440/781]  eta: 0:01:07  lr: 0.000250  training_loss: 0.5749 (0.5850)  mae_loss: 0.3670 (0.3845)  classification_loss: 0.1984 (0.2006)  time: 0.1959  data: 0.0002  max mem: 5511
[07:43:44.622082] Epoch: [7]  [460/781]  eta: 0:01:03  lr: 0.000250  training_loss: 0.5727 (0.5847)  mae_loss: 0.3818 (0.3841)  classification_loss: 0.2014 (0.2006)  time: 0.1941  data: 0.0002  max mem: 5511
[07:43:48.522123] Epoch: [7]  [480/781]  eta: 0:00:59  lr: 0.000250  training_loss: 0.5591 (0.5839)  mae_loss: 0.3610 (0.3833)  classification_loss: 0.2008 (0.2006)  time: 0.1949  data: 0.0003  max mem: 5511
[07:43:52.425744] Epoch: [7]  [500/781]  eta: 0:00:55  lr: 0.000250  training_loss: 0.5623 (0.5829)  mae_loss: 0.3582 (0.3824)  classification_loss: 0.1973 (0.2005)  time: 0.1951  data: 0.0002  max mem: 5511
[07:43:56.333857] Epoch: [7]  [520/781]  eta: 0:00:51  lr: 0.000250  training_loss: 0.5711 (0.5826)  mae_loss: 0.3771 (0.3821)  classification_loss: 0.1968 (0.2004)  time: 0.1953  data: 0.0002  max mem: 5511
[07:44:00.258940] Epoch: [7]  [540/781]  eta: 0:00:47  lr: 0.000250  training_loss: 0.5613 (0.5820)  mae_loss: 0.3582 (0.3816)  classification_loss: 0.2013 (0.2004)  time: 0.1962  data: 0.0003  max mem: 5511
[07:44:04.168235] Epoch: [7]  [560/781]  eta: 0:00:43  lr: 0.000249  training_loss: 0.5719 (0.5817)  mae_loss: 0.3718 (0.3813)  classification_loss: 0.1970 (0.2004)  time: 0.1953  data: 0.0002  max mem: 5511
[07:44:08.073244] Epoch: [7]  [580/781]  eta: 0:00:39  lr: 0.000249  training_loss: 0.5548 (0.5812)  mae_loss: 0.3500 (0.3808)  classification_loss: 0.1994 (0.2004)  time: 0.1952  data: 0.0003  max mem: 5511
[07:44:11.986983] Epoch: [7]  [600/781]  eta: 0:00:35  lr: 0.000249  training_loss: 0.5617 (0.5809)  mae_loss: 0.3648 (0.3806)  classification_loss: 0.1970 (0.2004)  time: 0.1956  data: 0.0002  max mem: 5511
[07:44:15.938091] Epoch: [7]  [620/781]  eta: 0:00:31  lr: 0.000249  training_loss: 0.5747 (0.5808)  mae_loss: 0.3761 (0.3805)  classification_loss: 0.1952 (0.2003)  time: 0.1974  data: 0.0002  max mem: 5511
[07:44:19.852082] Epoch: [7]  [640/781]  eta: 0:00:27  lr: 0.000249  training_loss: 0.5886 (0.5807)  mae_loss: 0.3869 (0.3805)  classification_loss: 0.2017 (0.2003)  time: 0.1956  data: 0.0002  max mem: 5511
[07:44:23.755594] Epoch: [7]  [660/781]  eta: 0:00:23  lr: 0.000249  training_loss: 0.5627 (0.5805)  mae_loss: 0.3675 (0.3802)  classification_loss: 0.2014 (0.2003)  time: 0.1951  data: 0.0003  max mem: 5511
[07:44:27.696588] Epoch: [7]  [680/781]  eta: 0:00:19  lr: 0.000249  training_loss: 0.5805 (0.5805)  mae_loss: 0.3755 (0.3802)  classification_loss: 0.2003 (0.2003)  time: 0.1970  data: 0.0002  max mem: 5511
[07:44:31.630442] Epoch: [7]  [700/781]  eta: 0:00:15  lr: 0.000249  training_loss: 0.5696 (0.5802)  mae_loss: 0.3639 (0.3798)  classification_loss: 0.2011 (0.2004)  time: 0.1966  data: 0.0003  max mem: 5511
[07:44:35.564358] Epoch: [7]  [720/781]  eta: 0:00:12  lr: 0.000249  training_loss: 0.5637 (0.5797)  mae_loss: 0.3589 (0.3793)  classification_loss: 0.2011 (0.2005)  time: 0.1964  data: 0.0002  max mem: 5511
[07:44:39.466080] Epoch: [7]  [740/781]  eta: 0:00:08  lr: 0.000249  training_loss: 0.5664 (0.5797)  mae_loss: 0.3622 (0.3792)  classification_loss: 0.2008 (0.2005)  time: 0.1950  data: 0.0002  max mem: 5511
[07:44:43.369573] Epoch: [7]  [760/781]  eta: 0:00:04  lr: 0.000249  training_loss: 0.5667 (0.5794)  mae_loss: 0.3671 (0.3789)  classification_loss: 0.1999 (0.2005)  time: 0.1951  data: 0.0002  max mem: 5511
[07:44:47.266179] Epoch: [7]  [780/781]  eta: 0:00:00  lr: 0.000249  training_loss: 0.5613 (0.5794)  mae_loss: 0.3692 (0.3790)  classification_loss: 0.1971 (0.2004)  time: 0.1947  data: 0.0002  max mem: 5511
[07:44:47.410509] Epoch: [7] Total time: 0:02:33 (0.1970 s / it)
[07:44:47.410977] Averaged stats: lr: 0.000249  training_loss: 0.5613 (0.5794)  mae_loss: 0.3692 (0.3790)  classification_loss: 0.1971 (0.2004)
[07:44:47.998432] Test:  [  0/157]  eta: 0:01:31  testing_loss: 1.4799 (1.4799)  acc1: 46.8750 (46.8750)  acc5: 92.1875 (92.1875)  time: 0.5826  data: 0.5511  max mem: 5511
[07:44:48.283280] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 1.5137 (1.5181)  acc1: 48.4375 (48.7216)  acc5: 93.7500 (93.6080)  time: 0.0786  data: 0.0503  max mem: 5511
[07:44:48.573126] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.4935 (1.4820)  acc1: 50.0000 (51.1161)  acc5: 93.7500 (93.8988)  time: 0.0285  data: 0.0004  max mem: 5511
[07:44:48.866403] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 1.4628 (1.4815)  acc1: 53.1250 (51.2097)  acc5: 93.7500 (93.7500)  time: 0.0290  data: 0.0004  max mem: 5511
[07:44:49.147796] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 1.5015 (1.4913)  acc1: 48.4375 (49.9238)  acc5: 93.7500 (93.5213)  time: 0.0286  data: 0.0002  max mem: 5511
[07:44:49.436267] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.4982 (1.4907)  acc1: 45.3125 (50.1225)  acc5: 92.1875 (93.3824)  time: 0.0284  data: 0.0002  max mem: 5511
[07:44:49.724397] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.4937 (1.4848)  acc1: 51.5625 (50.5635)  acc5: 92.1875 (93.3402)  time: 0.0287  data: 0.0002  max mem: 5511
[07:44:50.008710] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.4524 (1.4824)  acc1: 51.5625 (50.3301)  acc5: 93.7500 (93.3979)  time: 0.0285  data: 0.0002  max mem: 5511
[07:44:50.293439] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.4573 (1.4841)  acc1: 50.0000 (50.2508)  acc5: 93.7500 (93.4606)  time: 0.0283  data: 0.0002  max mem: 5511
[07:44:50.576927] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.5208 (1.4866)  acc1: 50.0000 (50.4464)  acc5: 92.1875 (93.3723)  time: 0.0283  data: 0.0002  max mem: 5511
[07:44:50.864272] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.5347 (1.4904)  acc1: 48.4375 (50.0928)  acc5: 92.1875 (93.3787)  time: 0.0284  data: 0.0002  max mem: 5511
[07:44:51.149871] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.5336 (1.4945)  acc1: 45.3125 (49.9296)  acc5: 93.7500 (93.2995)  time: 0.0284  data: 0.0002  max mem: 5511
[07:44:51.433376] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.4905 (1.4936)  acc1: 46.8750 (49.9354)  acc5: 93.7500 (93.3497)  time: 0.0282  data: 0.0002  max mem: 5511
[07:44:51.715265] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.4722 (1.4960)  acc1: 46.8750 (49.7734)  acc5: 93.7500 (93.2371)  time: 0.0281  data: 0.0002  max mem: 5511
[07:44:51.995865] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.4983 (1.4973)  acc1: 48.4375 (49.6121)  acc5: 92.1875 (93.1516)  time: 0.0280  data: 0.0002  max mem: 5511
[07:44:52.274099] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.4983 (1.4959)  acc1: 50.0000 (49.5861)  acc5: 92.1875 (93.1084)  time: 0.0278  data: 0.0001  max mem: 5511
[07:44:52.424748] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.4931 (1.4963)  acc1: 51.5625 (49.5900)  acc5: 92.1875 (93.0800)  time: 0.0269  data: 0.0001  max mem: 5511
[07:44:52.614474] Test: Total time: 0:00:05 (0.0331 s / it)
[07:44:52.615646] * Acc@1 49.590 Acc@5 93.080 loss 1.496
[07:44:52.616177] Accuracy of the network on the 10000 test images: 49.6%
[07:44:52.616584] Max accuracy: 49.59%
[07:44:52.801283] log_dir: ./output_dir
[07:44:53.734821] Epoch: [8]  [  0/781]  eta: 0:12:07  lr: 0.000249  training_loss: 0.5811 (0.5811)  mae_loss: 0.3913 (0.3913)  classification_loss: 0.1898 (0.1898)  time: 0.9317  data: 0.7142  max mem: 5511
[07:44:57.714118] Epoch: [8]  [ 20/781]  eta: 0:02:57  lr: 0.000249  training_loss: 0.5486 (0.5634)  mae_loss: 0.3638 (0.3647)  classification_loss: 0.1990 (0.1987)  time: 0.1989  data: 0.0002  max mem: 5511
[07:45:01.625349] Epoch: [8]  [ 40/781]  eta: 0:02:39  lr: 0.000249  training_loss: 0.5654 (0.5662)  mae_loss: 0.3613 (0.3664)  classification_loss: 0.2027 (0.1998)  time: 0.1955  data: 0.0002  max mem: 5511
[07:45:05.519212] Epoch: [8]  [ 60/781]  eta: 0:02:30  lr: 0.000249  training_loss: 0.5615 (0.5646)  mae_loss: 0.3631 (0.3659)  classification_loss: 0.1928 (0.1987)  time: 0.1946  data: 0.0002  max mem: 5511
[07:45:09.405753] Epoch: [8]  [ 80/781]  eta: 0:02:23  lr: 0.000249  training_loss: 0.5565 (0.5651)  mae_loss: 0.3524 (0.3656)  classification_loss: 0.1975 (0.1995)  time: 0.1943  data: 0.0002  max mem: 5511
[07:45:13.320525] Epoch: [8]  [100/781]  eta: 0:02:18  lr: 0.000249  training_loss: 0.5549 (0.5634)  mae_loss: 0.3519 (0.3635)  classification_loss: 0.2009 (0.1999)  time: 0.1956  data: 0.0002  max mem: 5511
[07:45:17.225576] Epoch: [8]  [120/781]  eta: 0:02:13  lr: 0.000249  training_loss: 0.5580 (0.5610)  mae_loss: 0.3581 (0.3614)  classification_loss: 0.1985 (0.1996)  time: 0.1952  data: 0.0002  max mem: 5511
[07:45:21.135503] Epoch: [8]  [140/781]  eta: 0:02:08  lr: 0.000249  training_loss: 0.5497 (0.5587)  mae_loss: 0.3496 (0.3597)  classification_loss: 0.1954 (0.1990)  time: 0.1954  data: 0.0002  max mem: 5511
[07:45:25.038219] Epoch: [8]  [160/781]  eta: 0:02:04  lr: 0.000249  training_loss: 0.5565 (0.5595)  mae_loss: 0.3623 (0.3604)  classification_loss: 0.1984 (0.1992)  time: 0.1951  data: 0.0002  max mem: 5511
[07:45:28.946869] Epoch: [8]  [180/781]  eta: 0:01:59  lr: 0.000249  training_loss: 0.5664 (0.5600)  mae_loss: 0.3634 (0.3611)  classification_loss: 0.1941 (0.1990)  time: 0.1953  data: 0.0002  max mem: 5511
[07:45:32.905165] Epoch: [8]  [200/781]  eta: 0:01:55  lr: 0.000249  training_loss: 0.5595 (0.5597)  mae_loss: 0.3563 (0.3609)  classification_loss: 0.1937 (0.1988)  time: 0.1978  data: 0.0002  max mem: 5511
[07:45:36.798769] Epoch: [8]  [220/781]  eta: 0:01:51  lr: 0.000249  training_loss: 0.5434 (0.5596)  mae_loss: 0.3526 (0.3610)  classification_loss: 0.1951 (0.1986)  time: 0.1946  data: 0.0002  max mem: 5511
[07:45:40.705973] Epoch: [8]  [240/781]  eta: 0:01:47  lr: 0.000249  training_loss: 0.5548 (0.5595)  mae_loss: 0.3539 (0.3606)  classification_loss: 0.1987 (0.1988)  time: 0.1953  data: 0.0002  max mem: 5511
[07:45:44.637427] Epoch: [8]  [260/781]  eta: 0:01:43  lr: 0.000249  training_loss: 0.5410 (0.5585)  mae_loss: 0.3438 (0.3597)  classification_loss: 0.1977 (0.1988)  time: 0.1965  data: 0.0002  max mem: 5511
[07:45:48.544480] Epoch: [8]  [280/781]  eta: 0:01:39  lr: 0.000249  training_loss: 0.5961 (0.5605)  mae_loss: 0.3874 (0.3617)  classification_loss: 0.1987 (0.1988)  time: 0.1952  data: 0.0002  max mem: 5511
[07:45:52.440997] Epoch: [8]  [300/781]  eta: 0:01:35  lr: 0.000249  training_loss: 0.5489 (0.5600)  mae_loss: 0.3458 (0.3611)  classification_loss: 0.1993 (0.1989)  time: 0.1947  data: 0.0002  max mem: 5511
[07:45:56.349709] Epoch: [8]  [320/781]  eta: 0:01:31  lr: 0.000249  training_loss: 0.5602 (0.5604)  mae_loss: 0.3575 (0.3614)  classification_loss: 0.1975 (0.1989)  time: 0.1953  data: 0.0002  max mem: 5511
[07:46:00.296683] Epoch: [8]  [340/781]  eta: 0:01:27  lr: 0.000249  training_loss: 0.5610 (0.5615)  mae_loss: 0.3571 (0.3625)  classification_loss: 0.1969 (0.1989)  time: 0.1972  data: 0.0002  max mem: 5511
[07:46:04.210397] Epoch: [8]  [360/781]  eta: 0:01:23  lr: 0.000249  training_loss: 0.5707 (0.5618)  mae_loss: 0.3697 (0.3629)  classification_loss: 0.1970 (0.1989)  time: 0.1956  data: 0.0002  max mem: 5511
[07:46:08.190916] Epoch: [8]  [380/781]  eta: 0:01:19  lr: 0.000249  training_loss: 0.5677 (0.5620)  mae_loss: 0.3692 (0.3631)  classification_loss: 0.1980 (0.1989)  time: 0.1989  data: 0.0002  max mem: 5511
[07:46:12.127757] Epoch: [8]  [400/781]  eta: 0:01:15  lr: 0.000249  training_loss: 0.5448 (0.5615)  mae_loss: 0.3430 (0.3626)  classification_loss: 0.2023 (0.1990)  time: 0.1968  data: 0.0002  max mem: 5511
[07:46:16.028127] Epoch: [8]  [420/781]  eta: 0:01:11  lr: 0.000249  training_loss: 0.5689 (0.5620)  mae_loss: 0.3662 (0.3630)  classification_loss: 0.1978 (0.1989)  time: 0.1949  data: 0.0003  max mem: 5511
[07:46:19.932567] Epoch: [8]  [440/781]  eta: 0:01:07  lr: 0.000249  training_loss: 0.5524 (0.5620)  mae_loss: 0.3571 (0.3632)  classification_loss: 0.1974 (0.1988)  time: 0.1951  data: 0.0002  max mem: 5511
[07:46:23.843559] Epoch: [8]  [460/781]  eta: 0:01:03  lr: 0.000249  training_loss: 0.5440 (0.5617)  mae_loss: 0.3404 (0.3628)  classification_loss: 0.1979 (0.1988)  time: 0.1955  data: 0.0002  max mem: 5511
[07:46:27.766014] Epoch: [8]  [480/781]  eta: 0:00:59  lr: 0.000249  training_loss: 0.5522 (0.5613)  mae_loss: 0.3480 (0.3625)  classification_loss: 0.1965 (0.1988)  time: 0.1960  data: 0.0002  max mem: 5511
[07:46:31.753144] Epoch: [8]  [500/781]  eta: 0:00:55  lr: 0.000249  training_loss: 0.5408 (0.5610)  mae_loss: 0.3466 (0.3623)  classification_loss: 0.1983 (0.1987)  time: 0.1993  data: 0.0003  max mem: 5511
[07:46:35.662262] Epoch: [8]  [520/781]  eta: 0:00:51  lr: 0.000249  training_loss: 0.5631 (0.5612)  mae_loss: 0.3615 (0.3625)  classification_loss: 0.1974 (0.1987)  time: 0.1953  data: 0.0002  max mem: 5511
[07:46:39.544292] Epoch: [8]  [540/781]  eta: 0:00:47  lr: 0.000249  training_loss: 0.5523 (0.5609)  mae_loss: 0.3572 (0.3623)  classification_loss: 0.1962 (0.1986)  time: 0.1940  data: 0.0002  max mem: 5511
[07:46:43.432950] Epoch: [8]  [560/781]  eta: 0:00:43  lr: 0.000249  training_loss: 0.5482 (0.5608)  mae_loss: 0.3447 (0.3622)  classification_loss: 0.1994 (0.1986)  time: 0.1943  data: 0.0002  max mem: 5511
[07:46:47.337334] Epoch: [8]  [580/781]  eta: 0:00:39  lr: 0.000249  training_loss: 0.5614 (0.5609)  mae_loss: 0.3583 (0.3623)  classification_loss: 0.1998 (0.1987)  time: 0.1951  data: 0.0002  max mem: 5511
[07:46:51.240816] Epoch: [8]  [600/781]  eta: 0:00:35  lr: 0.000249  training_loss: 0.5518 (0.5608)  mae_loss: 0.3540 (0.3622)  classification_loss: 0.1938 (0.1986)  time: 0.1951  data: 0.0003  max mem: 5511
[07:46:55.160206] Epoch: [8]  [620/781]  eta: 0:00:31  lr: 0.000249  training_loss: 0.5445 (0.5605)  mae_loss: 0.3520 (0.3620)  classification_loss: 0.1963 (0.1985)  time: 0.1959  data: 0.0002  max mem: 5511
[07:46:59.060051] Epoch: [8]  [640/781]  eta: 0:00:27  lr: 0.000249  training_loss: 0.5502 (0.5603)  mae_loss: 0.3479 (0.3618)  classification_loss: 0.1985 (0.1985)  time: 0.1949  data: 0.0003  max mem: 5511
[07:47:02.974025] Epoch: [8]  [660/781]  eta: 0:00:23  lr: 0.000249  training_loss: 0.5653 (0.5605)  mae_loss: 0.3625 (0.3620)  classification_loss: 0.1983 (0.1985)  time: 0.1956  data: 0.0002  max mem: 5511
[07:47:06.889278] Epoch: [8]  [680/781]  eta: 0:00:19  lr: 0.000249  training_loss: 0.5473 (0.5600)  mae_loss: 0.3435 (0.3616)  classification_loss: 0.1975 (0.1984)  time: 0.1957  data: 0.0002  max mem: 5511
[07:47:10.812070] Epoch: [8]  [700/781]  eta: 0:00:15  lr: 0.000249  training_loss: 0.5267 (0.5593)  mae_loss: 0.3291 (0.3609)  classification_loss: 0.1968 (0.1984)  time: 0.1961  data: 0.0002  max mem: 5511
[07:47:14.715924] Epoch: [8]  [720/781]  eta: 0:00:12  lr: 0.000249  training_loss: 0.5400 (0.5589)  mae_loss: 0.3390 (0.3605)  classification_loss: 0.1994 (0.1984)  time: 0.1951  data: 0.0002  max mem: 5511
[07:47:18.627981] Epoch: [8]  [740/781]  eta: 0:00:08  lr: 0.000249  training_loss: 0.5271 (0.5582)  mae_loss: 0.3225 (0.3599)  classification_loss: 0.1941 (0.1983)  time: 0.1955  data: 0.0003  max mem: 5511
[07:47:22.547037] Epoch: [8]  [760/781]  eta: 0:00:04  lr: 0.000249  training_loss: 0.5627 (0.5584)  mae_loss: 0.3654 (0.3600)  classification_loss: 0.1982 (0.1984)  time: 0.1958  data: 0.0003  max mem: 5511
[07:47:26.421530] Epoch: [8]  [780/781]  eta: 0:00:00  lr: 0.000249  training_loss: 0.5380 (0.5579)  mae_loss: 0.3440 (0.3596)  classification_loss: 0.1961 (0.1983)  time: 0.1937  data: 0.0002  max mem: 5511
[07:47:26.589567] Epoch: [8] Total time: 0:02:33 (0.1969 s / it)
[07:47:26.590033] Averaged stats: lr: 0.000249  training_loss: 0.5380 (0.5579)  mae_loss: 0.3440 (0.3596)  classification_loss: 0.1961 (0.1983)
[07:47:27.280638] Test:  [  0/157]  eta: 0:01:47  testing_loss: 1.5253 (1.5253)  acc1: 48.4375 (48.4375)  acc5: 93.7500 (93.7500)  time: 0.6867  data: 0.6569  max mem: 5511
[07:47:27.564395] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.4551 (1.5033)  acc1: 48.4375 (46.4489)  acc5: 93.7500 (93.0398)  time: 0.0880  data: 0.0599  max mem: 5511
[07:47:27.847211] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.4291 (1.4653)  acc1: 50.0000 (49.7024)  acc5: 93.7500 (93.6756)  time: 0.0282  data: 0.0002  max mem: 5511
[07:47:28.131107] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.4291 (1.4619)  acc1: 50.0000 (49.9496)  acc5: 93.7500 (93.2460)  time: 0.0282  data: 0.0002  max mem: 5511
[07:47:28.418200] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.4974 (1.4724)  acc1: 48.4375 (49.0473)  acc5: 92.1875 (93.0259)  time: 0.0284  data: 0.0003  max mem: 5511
[07:47:28.702652] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.4663 (1.4684)  acc1: 48.4375 (49.3873)  acc5: 93.7500 (93.0760)  time: 0.0284  data: 0.0003  max mem: 5511
[07:47:28.986231] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.4317 (1.4617)  acc1: 50.0000 (49.8975)  acc5: 92.1875 (92.8791)  time: 0.0283  data: 0.0002  max mem: 5511
[07:47:29.271796] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.4211 (1.4586)  acc1: 51.5625 (50.1540)  acc5: 93.7500 (93.1118)  time: 0.0283  data: 0.0002  max mem: 5511
[07:47:29.553747] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.4580 (1.4602)  acc1: 51.5625 (50.2122)  acc5: 93.7500 (93.1906)  time: 0.0283  data: 0.0002  max mem: 5511
[07:47:29.837156] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.4666 (1.4622)  acc1: 51.5625 (50.3091)  acc5: 92.1875 (93.0288)  time: 0.0282  data: 0.0002  max mem: 5511
[07:47:30.118407] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.5096 (1.4668)  acc1: 46.8750 (49.9691)  acc5: 92.1875 (92.9920)  time: 0.0281  data: 0.0002  max mem: 5511
[07:47:30.400315] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.5162 (1.4707)  acc1: 45.3125 (49.7185)  acc5: 93.7500 (93.0180)  time: 0.0280  data: 0.0001  max mem: 5511
[07:47:30.682103] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.4643 (1.4701)  acc1: 46.8750 (49.6772)  acc5: 93.7500 (93.1560)  time: 0.0280  data: 0.0001  max mem: 5511
[07:47:30.966467] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.4518 (1.4722)  acc1: 48.4375 (49.5945)  acc5: 93.7500 (93.0224)  time: 0.0281  data: 0.0003  max mem: 5511
[07:47:31.252121] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.4830 (1.4729)  acc1: 50.0000 (49.5567)  acc5: 92.1875 (93.0186)  time: 0.0284  data: 0.0003  max mem: 5511
[07:47:31.531831] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.4864 (1.4733)  acc1: 50.0000 (49.5654)  acc5: 92.1875 (92.9325)  time: 0.0281  data: 0.0001  max mem: 5511
[07:47:31.682564] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.4864 (1.4742)  acc1: 50.0000 (49.5400)  acc5: 92.1875 (92.9400)  time: 0.0270  data: 0.0001  max mem: 5511
[07:47:31.824368] Test: Total time: 0:00:05 (0.0333 s / it)
[07:47:31.824827] * Acc@1 49.540 Acc@5 92.940 loss 1.474
[07:47:31.825130] Accuracy of the network on the 10000 test images: 49.5%
[07:47:31.825314] Max accuracy: 49.59%
[07:47:32.041584] log_dir: ./output_dir
[07:47:32.988141] Epoch: [9]  [  0/781]  eta: 0:12:17  lr: 0.000249  training_loss: 0.5251 (0.5251)  mae_loss: 0.3230 (0.3230)  classification_loss: 0.2021 (0.2021)  time: 0.9445  data: 0.7427  max mem: 5511
[07:47:36.902049] Epoch: [9]  [ 20/781]  eta: 0:02:55  lr: 0.000249  training_loss: 0.5528 (0.5504)  mae_loss: 0.3532 (0.3517)  classification_loss: 0.1963 (0.1987)  time: 0.1956  data: 0.0002  max mem: 5511
[07:47:40.805329] Epoch: [9]  [ 40/781]  eta: 0:02:38  lr: 0.000249  training_loss: 0.5417 (0.5494)  mae_loss: 0.3569 (0.3529)  classification_loss: 0.1938 (0.1965)  time: 0.1951  data: 0.0002  max mem: 5511
[07:47:44.704984] Epoch: [9]  [ 60/781]  eta: 0:02:29  lr: 0.000249  training_loss: 0.5554 (0.5535)  mae_loss: 0.3530 (0.3560)  classification_loss: 0.1998 (0.1976)  time: 0.1949  data: 0.0002  max mem: 5511
[07:47:48.618401] Epoch: [9]  [ 80/781]  eta: 0:02:23  lr: 0.000249  training_loss: 0.5444 (0.5528)  mae_loss: 0.3472 (0.3556)  classification_loss: 0.1933 (0.1972)  time: 0.1956  data: 0.0001  max mem: 5511
[07:47:52.523152] Epoch: [9]  [100/781]  eta: 0:02:18  lr: 0.000249  training_loss: 0.5463 (0.5512)  mae_loss: 0.3493 (0.3543)  classification_loss: 0.1955 (0.1969)  time: 0.1952  data: 0.0004  max mem: 5511
[07:47:56.465351] Epoch: [9]  [120/781]  eta: 0:02:13  lr: 0.000249  training_loss: 0.5332 (0.5497)  mae_loss: 0.3405 (0.3533)  classification_loss: 0.1923 (0.1964)  time: 0.1970  data: 0.0002  max mem: 5511
[07:48:00.390980] Epoch: [9]  [140/781]  eta: 0:02:08  lr: 0.000249  training_loss: 0.5360 (0.5478)  mae_loss: 0.3399 (0.3516)  classification_loss: 0.1921 (0.1962)  time: 0.1962  data: 0.0002  max mem: 5511
[07:48:04.376954] Epoch: [9]  [160/781]  eta: 0:02:04  lr: 0.000249  training_loss: 0.5460 (0.5473)  mae_loss: 0.3510 (0.3515)  classification_loss: 0.1924 (0.1958)  time: 0.1992  data: 0.0002  max mem: 5511
[07:48:08.345268] Epoch: [9]  [180/781]  eta: 0:02:00  lr: 0.000249  training_loss: 0.5415 (0.5464)  mae_loss: 0.3441 (0.3507)  classification_loss: 0.1963 (0.1957)  time: 0.1983  data: 0.0002  max mem: 5511
[07:48:12.257788] Epoch: [9]  [200/781]  eta: 0:01:56  lr: 0.000249  training_loss: 0.5432 (0.5462)  mae_loss: 0.3487 (0.3501)  classification_loss: 0.2007 (0.1961)  time: 0.1955  data: 0.0003  max mem: 5511
[07:48:16.159930] Epoch: [9]  [220/781]  eta: 0:01:51  lr: 0.000249  training_loss: 0.5220 (0.5447)  mae_loss: 0.3236 (0.3487)  classification_loss: 0.1937 (0.1961)  time: 0.1950  data: 0.0002  max mem: 5511
[07:48:20.066339] Epoch: [9]  [240/781]  eta: 0:01:47  lr: 0.000249  training_loss: 0.5354 (0.5443)  mae_loss: 0.3370 (0.3480)  classification_loss: 0.1960 (0.1962)  time: 0.1952  data: 0.0002  max mem: 5511
[07:48:23.998869] Epoch: [9]  [260/781]  eta: 0:01:43  lr: 0.000249  training_loss: 0.5585 (0.5449)  mae_loss: 0.3594 (0.3488)  classification_loss: 0.1942 (0.1961)  time: 0.1966  data: 0.0002  max mem: 5511
[07:48:27.900923] Epoch: [9]  [280/781]  eta: 0:01:39  lr: 0.000249  training_loss: 0.5565 (0.5455)  mae_loss: 0.3512 (0.3491)  classification_loss: 0.2006 (0.1964)  time: 0.1950  data: 0.0003  max mem: 5511
[07:48:31.827766] Epoch: [9]  [300/781]  eta: 0:01:35  lr: 0.000249  training_loss: 0.5447 (0.5452)  mae_loss: 0.3456 (0.3488)  classification_loss: 0.1928 (0.1964)  time: 0.1963  data: 0.0002  max mem: 5511
[07:48:35.721162] Epoch: [9]  [320/781]  eta: 0:01:31  lr: 0.000249  training_loss: 0.5221 (0.5442)  mae_loss: 0.3196 (0.3477)  classification_loss: 0.1984 (0.1965)  time: 0.1946  data: 0.0002  max mem: 5511
[07:48:39.680208] Epoch: [9]  [340/781]  eta: 0:01:27  lr: 0.000249  training_loss: 0.5376 (0.5436)  mae_loss: 0.3455 (0.3471)  classification_loss: 0.1955 (0.1965)  time: 0.1979  data: 0.0002  max mem: 5511
[07:48:43.651761] Epoch: [9]  [360/781]  eta: 0:01:23  lr: 0.000249  training_loss: 0.5252 (0.5427)  mae_loss: 0.3300 (0.3462)  classification_loss: 0.1967 (0.1965)  time: 0.1985  data: 0.0002  max mem: 5511
[07:48:47.555938] Epoch: [9]  [380/781]  eta: 0:01:19  lr: 0.000249  training_loss: 0.5653 (0.5436)  mae_loss: 0.3667 (0.3471)  classification_loss: 0.1956 (0.1965)  time: 0.1951  data: 0.0002  max mem: 5511
[07:48:51.461727] Epoch: [9]  [400/781]  eta: 0:01:15  lr: 0.000249  training_loss: 0.5286 (0.5431)  mae_loss: 0.3288 (0.3466)  classification_loss: 0.1982 (0.1964)  time: 0.1952  data: 0.0002  max mem: 5511
[07:48:55.383438] Epoch: [9]  [420/781]  eta: 0:01:11  lr: 0.000249  training_loss: 0.5199 (0.5424)  mae_loss: 0.3386 (0.3461)  classification_loss: 0.1923 (0.1963)  time: 0.1960  data: 0.0002  max mem: 5511
[07:48:59.285939] Epoch: [9]  [440/781]  eta: 0:01:07  lr: 0.000249  training_loss: 0.5395 (0.5423)  mae_loss: 0.3455 (0.3460)  classification_loss: 0.1943 (0.1963)  time: 0.1950  data: 0.0003  max mem: 5511
[07:49:03.220945] Epoch: [9]  [460/781]  eta: 0:01:03  lr: 0.000249  training_loss: 0.5180 (0.5416)  mae_loss: 0.3261 (0.3454)  classification_loss: 0.1934 (0.1962)  time: 0.1967  data: 0.0002  max mem: 5511
[07:49:07.118159] Epoch: [9]  [480/781]  eta: 0:00:59  lr: 0.000249  training_loss: 0.5390 (0.5413)  mae_loss: 0.3364 (0.3451)  classification_loss: 0.1916 (0.1962)  time: 0.1948  data: 0.0002  max mem: 5511
[07:49:11.032004] Epoch: [9]  [500/781]  eta: 0:00:55  lr: 0.000249  training_loss: 0.5376 (0.5408)  mae_loss: 0.3316 (0.3446)  classification_loss: 0.1930 (0.1962)  time: 0.1956  data: 0.0003  max mem: 5511
[07:49:14.929654] Epoch: [9]  [520/781]  eta: 0:00:51  lr: 0.000249  training_loss: 0.5611 (0.5412)  mae_loss: 0.3645 (0.3449)  classification_loss: 0.1979 (0.1962)  time: 0.1948  data: 0.0003  max mem: 5511
[07:49:18.827287] Epoch: [9]  [540/781]  eta: 0:00:47  lr: 0.000249  training_loss: 0.5359 (0.5413)  mae_loss: 0.3396 (0.3451)  classification_loss: 0.1956 (0.1962)  time: 0.1948  data: 0.0002  max mem: 5511
[07:49:22.758373] Epoch: [9]  [560/781]  eta: 0:00:43  lr: 0.000248  training_loss: 0.5324 (0.5409)  mae_loss: 0.3292 (0.3446)  classification_loss: 0.1959 (0.1962)  time: 0.1965  data: 0.0003  max mem: 5511
[07:49:26.693595] Epoch: [9]  [580/781]  eta: 0:00:39  lr: 0.000248  training_loss: 0.5306 (0.5407)  mae_loss: 0.3314 (0.3444)  classification_loss: 0.1991 (0.1963)  time: 0.1967  data: 0.0002  max mem: 5511
[07:49:30.589434] Epoch: [9]  [600/781]  eta: 0:00:35  lr: 0.000248  training_loss: 0.5370 (0.5406)  mae_loss: 0.3379 (0.3444)  classification_loss: 0.1925 (0.1962)  time: 0.1947  data: 0.0002  max mem: 5511
[07:49:34.486307] Epoch: [9]  [620/781]  eta: 0:00:31  lr: 0.000248  training_loss: 0.5283 (0.5405)  mae_loss: 0.3412 (0.3443)  classification_loss: 0.1943 (0.1961)  time: 0.1948  data: 0.0003  max mem: 5511
[07:49:38.409486] Epoch: [9]  [640/781]  eta: 0:00:27  lr: 0.000248  training_loss: 0.5347 (0.5402)  mae_loss: 0.3353 (0.3441)  classification_loss: 0.1953 (0.1961)  time: 0.1961  data: 0.0002  max mem: 5511
[07:49:42.378873] Epoch: [9]  [660/781]  eta: 0:00:23  lr: 0.000248  training_loss: 0.5373 (0.5400)  mae_loss: 0.3460 (0.3440)  classification_loss: 0.1920 (0.1960)  time: 0.1984  data: 0.0002  max mem: 5511
[07:49:46.331912] Epoch: [9]  [680/781]  eta: 0:00:19  lr: 0.000248  training_loss: 0.5305 (0.5401)  mae_loss: 0.3309 (0.3441)  classification_loss: 0.1929 (0.1960)  time: 0.1976  data: 0.0003  max mem: 5511
[07:49:50.248963] Epoch: [9]  [700/781]  eta: 0:00:15  lr: 0.000248  training_loss: 0.5144 (0.5398)  mae_loss: 0.3256 (0.3438)  classification_loss: 0.1943 (0.1959)  time: 0.1958  data: 0.0003  max mem: 5511
[07:49:54.175290] Epoch: [9]  [720/781]  eta: 0:00:12  lr: 0.000248  training_loss: 0.4996 (0.5388)  mae_loss: 0.3086 (0.3429)  classification_loss: 0.1923 (0.1959)  time: 0.1962  data: 0.0004  max mem: 5511
[07:49:58.085605] Epoch: [9]  [740/781]  eta: 0:00:08  lr: 0.000248  training_loss: 0.5295 (0.5384)  mae_loss: 0.3381 (0.3427)  classification_loss: 0.1881 (0.1958)  time: 0.1954  data: 0.0002  max mem: 5511
[07:50:01.990980] Epoch: [9]  [760/781]  eta: 0:00:04  lr: 0.000248  training_loss: 0.5312 (0.5382)  mae_loss: 0.3336 (0.3424)  classification_loss: 0.1967 (0.1958)  time: 0.1951  data: 0.0002  max mem: 5511
[07:50:05.882498] Epoch: [9]  [780/781]  eta: 0:00:00  lr: 0.000248  training_loss: 0.5082 (0.5377)  mae_loss: 0.3100 (0.3419)  classification_loss: 0.1984 (0.1958)  time: 0.1944  data: 0.0002  max mem: 5511
[07:50:06.030493] Epoch: [9] Total time: 0:02:33 (0.1972 s / it)
[07:50:06.030963] Averaged stats: lr: 0.000248  training_loss: 0.5082 (0.5377)  mae_loss: 0.3100 (0.3419)  classification_loss: 0.1984 (0.1958)
[07:50:06.578237] Test:  [  0/157]  eta: 0:01:25  testing_loss: 1.3879 (1.3879)  acc1: 56.2500 (56.2500)  acc5: 90.6250 (90.6250)  time: 0.5433  data: 0.5111  max mem: 5511
[07:50:06.868319] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 1.4259 (1.4533)  acc1: 53.1250 (51.7045)  acc5: 95.3125 (94.4602)  time: 0.0755  data: 0.0466  max mem: 5511
[07:50:07.157521] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.4128 (1.4243)  acc1: 53.1250 (53.3482)  acc5: 95.3125 (94.8661)  time: 0.0288  data: 0.0002  max mem: 5511
[07:50:07.443953] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 1.3943 (1.4289)  acc1: 53.1250 (52.8226)  acc5: 93.7500 (94.2036)  time: 0.0286  data: 0.0002  max mem: 5511
[07:50:07.732336] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 1.4248 (1.4389)  acc1: 50.0000 (51.6387)  acc5: 92.1875 (94.0168)  time: 0.0285  data: 0.0002  max mem: 5511
[07:50:08.021515] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.4248 (1.4341)  acc1: 51.5625 (52.2978)  acc5: 93.7500 (94.2402)  time: 0.0287  data: 0.0002  max mem: 5511
[07:50:08.310816] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.3955 (1.4305)  acc1: 53.1250 (52.6383)  acc5: 93.7500 (94.0574)  time: 0.0288  data: 0.0002  max mem: 5511
[07:50:08.593358] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.4112 (1.4311)  acc1: 51.5625 (52.6408)  acc5: 93.7500 (94.1021)  time: 0.0284  data: 0.0002  max mem: 5511
[07:50:08.880849] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.4284 (1.4330)  acc1: 53.1250 (52.6042)  acc5: 93.7500 (94.1165)  time: 0.0284  data: 0.0002  max mem: 5511
[07:50:09.164746] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.4304 (1.4348)  acc1: 53.1250 (52.6442)  acc5: 93.7500 (94.0419)  time: 0.0284  data: 0.0002  max mem: 5511
[07:50:09.448017] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.4684 (1.4392)  acc1: 50.0000 (52.1504)  acc5: 93.7500 (93.9511)  time: 0.0282  data: 0.0002  max mem: 5511
[07:50:09.735415] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.4769 (1.4427)  acc1: 46.8750 (51.9144)  acc5: 93.7500 (93.8485)  time: 0.0283  data: 0.0002  max mem: 5511
[07:50:10.017516] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.4479 (1.4429)  acc1: 51.5625 (51.7820)  acc5: 93.7500 (93.9954)  time: 0.0283  data: 0.0002  max mem: 5511
[07:50:10.298677] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.4427 (1.4449)  acc1: 51.5625 (51.7533)  acc5: 93.7500 (93.8454)  time: 0.0280  data: 0.0002  max mem: 5511
[07:50:10.579911] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.4342 (1.4439)  acc1: 53.1250 (51.8728)  acc5: 93.7500 (93.9384)  time: 0.0280  data: 0.0001  max mem: 5511
[07:50:10.858875] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.4187 (1.4436)  acc1: 53.1250 (51.8315)  acc5: 93.7500 (93.8328)  time: 0.0279  data: 0.0001  max mem: 5511
[07:50:11.008289] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.4167 (1.4441)  acc1: 51.5625 (51.7200)  acc5: 93.7500 (93.8400)  time: 0.0269  data: 0.0001  max mem: 5511
[07:50:11.175891] Test: Total time: 0:00:05 (0.0328 s / it)
[07:50:11.176569] * Acc@1 51.720 Acc@5 93.840 loss 1.444
[07:50:11.176880] Accuracy of the network on the 10000 test images: 51.7%
[07:50:11.177059] Max accuracy: 51.72%
[07:50:11.433593] log_dir: ./output_dir
[07:50:12.344732] Epoch: [10]  [  0/781]  eta: 0:11:50  lr: 0.000248  training_loss: 0.4759 (0.4759)  mae_loss: 0.2944 (0.2944)  classification_loss: 0.1815 (0.1815)  time: 0.9094  data: 0.6856  max mem: 5511
[07:50:16.285751] Epoch: [10]  [ 20/781]  eta: 0:02:55  lr: 0.000248  training_loss: 0.5078 (0.5173)  mae_loss: 0.3196 (0.3245)  classification_loss: 0.1898 (0.1928)  time: 0.1970  data: 0.0002  max mem: 5511
[07:50:20.216826] Epoch: [10]  [ 40/781]  eta: 0:02:38  lr: 0.000248  training_loss: 0.5257 (0.5221)  mae_loss: 0.3211 (0.3284)  classification_loss: 0.1944 (0.1937)  time: 0.1964  data: 0.0003  max mem: 5511
[07:50:24.138359] Epoch: [10]  [ 60/781]  eta: 0:02:30  lr: 0.000248  training_loss: 0.5236 (0.5251)  mae_loss: 0.3204 (0.3298)  classification_loss: 0.1982 (0.1953)  time: 0.1960  data: 0.0003  max mem: 5511
[07:50:28.053509] Epoch: [10]  [ 80/781]  eta: 0:02:23  lr: 0.000248  training_loss: 0.5244 (0.5244)  mae_loss: 0.3240 (0.3296)  classification_loss: 0.1923 (0.1949)  time: 0.1956  data: 0.0002  max mem: 5511
[07:50:31.958330] Epoch: [10]  [100/781]  eta: 0:02:18  lr: 0.000248  training_loss: 0.5282 (0.5233)  mae_loss: 0.3326 (0.3284)  classification_loss: 0.1952 (0.1949)  time: 0.1951  data: 0.0002  max mem: 5511
[07:50:35.870978] Epoch: [10]  [120/781]  eta: 0:02:13  lr: 0.000248  training_loss: 0.5139 (0.5228)  mae_loss: 0.3244 (0.3284)  classification_loss: 0.1909 (0.1944)  time: 0.1955  data: 0.0002  max mem: 5511
[07:50:39.779992] Epoch: [10]  [140/781]  eta: 0:02:08  lr: 0.000248  training_loss: 0.5153 (0.5225)  mae_loss: 0.3316 (0.3285)  classification_loss: 0.1924 (0.1940)  time: 0.1954  data: 0.0002  max mem: 5511
[07:50:43.760384] Epoch: [10]  [160/781]  eta: 0:02:04  lr: 0.000248  training_loss: 0.5149 (0.5235)  mae_loss: 0.3273 (0.3297)  classification_loss: 0.1928 (0.1938)  time: 0.1989  data: 0.0002  max mem: 5511
[07:50:47.733545] Epoch: [10]  [180/781]  eta: 0:02:00  lr: 0.000248  training_loss: 0.5387 (0.5246)  mae_loss: 0.3457 (0.3304)  classification_loss: 0.1930 (0.1941)  time: 0.1985  data: 0.0005  max mem: 5511
[07:50:51.705315] Epoch: [10]  [200/781]  eta: 0:01:56  lr: 0.000248  training_loss: 0.5284 (0.5244)  mae_loss: 0.3295 (0.3303)  classification_loss: 0.1940 (0.1941)  time: 0.1985  data: 0.0005  max mem: 5511
[07:50:55.610488] Epoch: [10]  [220/781]  eta: 0:01:52  lr: 0.000248  training_loss: 0.5072 (0.5240)  mae_loss: 0.3079 (0.3298)  classification_loss: 0.1924 (0.1942)  time: 0.1952  data: 0.0002  max mem: 5511
[07:50:59.519024] Epoch: [10]  [240/781]  eta: 0:01:47  lr: 0.000248  training_loss: 0.5255 (0.5242)  mae_loss: 0.3243 (0.3300)  classification_loss: 0.1926 (0.1942)  time: 0.1953  data: 0.0002  max mem: 5511
[07:51:03.411954] Epoch: [10]  [260/781]  eta: 0:01:43  lr: 0.000248  training_loss: 0.5169 (0.5236)  mae_loss: 0.3199 (0.3294)  classification_loss: 0.1923 (0.1941)  time: 0.1946  data: 0.0002  max mem: 5511
[07:51:07.345519] Epoch: [10]  [280/781]  eta: 0:01:39  lr: 0.000248  training_loss: 0.5295 (0.5236)  mae_loss: 0.3360 (0.3295)  classification_loss: 0.1931 (0.1941)  time: 0.1966  data: 0.0002  max mem: 5511
[07:51:11.309344] Epoch: [10]  [300/781]  eta: 0:01:35  lr: 0.000248  training_loss: 0.5378 (0.5246)  mae_loss: 0.3459 (0.3306)  classification_loss: 0.1926 (0.1940)  time: 0.1981  data: 0.0006  max mem: 5511
[07:51:15.232294] Epoch: [10]  [320/781]  eta: 0:01:31  lr: 0.000248  training_loss: 0.5121 (0.5241)  mae_loss: 0.3204 (0.3302)  classification_loss: 0.1914 (0.1939)  time: 0.1961  data: 0.0002  max mem: 5511
[07:51:19.139684] Epoch: [10]  [340/781]  eta: 0:01:27  lr: 0.000248  training_loss: 0.5213 (0.5242)  mae_loss: 0.3323 (0.3302)  classification_loss: 0.1932 (0.1940)  time: 0.1953  data: 0.0001  max mem: 5511
[07:51:23.041171] Epoch: [10]  [360/781]  eta: 0:01:23  lr: 0.000248  training_loss: 0.5133 (0.5239)  mae_loss: 0.3193 (0.3297)  classification_loss: 0.1967 (0.1942)  time: 0.1950  data: 0.0002  max mem: 5511
[07:51:26.952791] Epoch: [10]  [380/781]  eta: 0:01:19  lr: 0.000248  training_loss: 0.5086 (0.5233)  mae_loss: 0.3180 (0.3293)  classification_loss: 0.1926 (0.1940)  time: 0.1955  data: 0.0004  max mem: 5511
[07:51:30.857864] Epoch: [10]  [400/781]  eta: 0:01:15  lr: 0.000248  training_loss: 0.5147 (0.5230)  mae_loss: 0.3160 (0.3290)  classification_loss: 0.1898 (0.1940)  time: 0.1952  data: 0.0004  max mem: 5511
[07:51:34.750111] Epoch: [10]  [420/781]  eta: 0:01:11  lr: 0.000248  training_loss: 0.5075 (0.5223)  mae_loss: 0.3089 (0.3285)  classification_loss: 0.1930 (0.1938)  time: 0.1945  data: 0.0002  max mem: 5511
[07:51:38.652001] Epoch: [10]  [440/781]  eta: 0:01:07  lr: 0.000248  training_loss: 0.5257 (0.5224)  mae_loss: 0.3280 (0.3284)  classification_loss: 0.1955 (0.1939)  time: 0.1950  data: 0.0004  max mem: 5511
[07:51:42.540300] Epoch: [10]  [460/781]  eta: 0:01:03  lr: 0.000248  training_loss: 0.4981 (0.5219)  mae_loss: 0.3027 (0.3280)  classification_loss: 0.1923 (0.1938)  time: 0.1943  data: 0.0002  max mem: 5511
[07:51:46.429673] Epoch: [10]  [480/781]  eta: 0:00:59  lr: 0.000248  training_loss: 0.5300 (0.5224)  mae_loss: 0.3337 (0.3285)  classification_loss: 0.1934 (0.1939)  time: 0.1944  data: 0.0002  max mem: 5511
[07:51:50.435233] Epoch: [10]  [500/781]  eta: 0:00:55  lr: 0.000248  training_loss: 0.5260 (0.5228)  mae_loss: 0.3345 (0.3290)  classification_loss: 0.1929 (0.1938)  time: 0.2002  data: 0.0002  max mem: 5511
[07:51:54.338826] Epoch: [10]  [520/781]  eta: 0:00:51  lr: 0.000248  training_loss: 0.5233 (0.5230)  mae_loss: 0.3367 (0.3293)  classification_loss: 0.1903 (0.1937)  time: 0.1951  data: 0.0004  max mem: 5511
[07:51:58.268443] Epoch: [10]  [540/781]  eta: 0:00:47  lr: 0.000248  training_loss: 0.5214 (0.5229)  mae_loss: 0.3270 (0.3291)  classification_loss: 0.1939 (0.1938)  time: 0.1964  data: 0.0002  max mem: 5511
[07:52:02.175426] Epoch: [10]  [560/781]  eta: 0:00:43  lr: 0.000248  training_loss: 0.5229 (0.5231)  mae_loss: 0.3246 (0.3293)  classification_loss: 0.1943 (0.1938)  time: 0.1953  data: 0.0002  max mem: 5511
[07:52:06.076328] Epoch: [10]  [580/781]  eta: 0:00:39  lr: 0.000248  training_loss: 0.5146 (0.5229)  mae_loss: 0.3212 (0.3292)  classification_loss: 0.1935 (0.1938)  time: 0.1950  data: 0.0003  max mem: 5511
[07:52:09.990782] Epoch: [10]  [600/781]  eta: 0:00:35  lr: 0.000248  training_loss: 0.5174 (0.5228)  mae_loss: 0.3246 (0.3290)  classification_loss: 0.1912 (0.1937)  time: 0.1957  data: 0.0003  max mem: 5511
[07:52:13.887865] Epoch: [10]  [620/781]  eta: 0:00:31  lr: 0.000248  training_loss: 0.5224 (0.5229)  mae_loss: 0.3325 (0.3292)  classification_loss: 0.1890 (0.1936)  time: 0.1948  data: 0.0002  max mem: 5511
[07:52:17.781808] Epoch: [10]  [640/781]  eta: 0:00:27  lr: 0.000248  training_loss: 0.5162 (0.5227)  mae_loss: 0.3210 (0.3291)  classification_loss: 0.1931 (0.1936)  time: 0.1946  data: 0.0003  max mem: 5511
[07:52:21.688802] Epoch: [10]  [660/781]  eta: 0:00:23  lr: 0.000248  training_loss: 0.5008 (0.5224)  mae_loss: 0.3124 (0.3288)  classification_loss: 0.1936 (0.1936)  time: 0.1953  data: 0.0002  max mem: 5511
[07:52:25.592546] Epoch: [10]  [680/781]  eta: 0:00:19  lr: 0.000248  training_loss: 0.5152 (0.5222)  mae_loss: 0.3244 (0.3286)  classification_loss: 0.1907 (0.1936)  time: 0.1951  data: 0.0002  max mem: 5511
[07:52:29.513392] Epoch: [10]  [700/781]  eta: 0:00:15  lr: 0.000248  training_loss: 0.5184 (0.5223)  mae_loss: 0.3320 (0.3288)  classification_loss: 0.1902 (0.1935)  time: 0.1956  data: 0.0002  max mem: 5511
[07:52:33.432192] Epoch: [10]  [720/781]  eta: 0:00:12  lr: 0.000248  training_loss: 0.5085 (0.5218)  mae_loss: 0.3216 (0.3284)  classification_loss: 0.1871 (0.1934)  time: 0.1959  data: 0.0002  max mem: 5511
[07:52:37.328470] Epoch: [10]  [740/781]  eta: 0:00:08  lr: 0.000248  training_loss: 0.5080 (0.5216)  mae_loss: 0.3164 (0.3283)  classification_loss: 0.1911 (0.1933)  time: 0.1947  data: 0.0002  max mem: 5511
[07:52:41.213775] Epoch: [10]  [760/781]  eta: 0:00:04  lr: 0.000248  training_loss: 0.4956 (0.5210)  mae_loss: 0.2999 (0.3277)  classification_loss: 0.1910 (0.1933)  time: 0.1942  data: 0.0003  max mem: 5511
[07:52:45.135006] Epoch: [10]  [780/781]  eta: 0:00:00  lr: 0.000248  training_loss: 0.5076 (0.5206)  mae_loss: 0.3087 (0.3273)  classification_loss: 0.1915 (0.1933)  time: 0.1960  data: 0.0002  max mem: 5511
[07:52:45.290669] Epoch: [10] Total time: 0:02:33 (0.1970 s / it)
[07:52:45.291672] Averaged stats: lr: 0.000248  training_loss: 0.5076 (0.5206)  mae_loss: 0.3087 (0.3273)  classification_loss: 0.1915 (0.1933)
[07:52:47.088945] Test:  [  0/157]  eta: 0:01:43  testing_loss: 1.3497 (1.3497)  acc1: 53.1250 (53.1250)  acc5: 95.3125 (95.3125)  time: 0.6619  data: 0.6292  max mem: 5511
[07:52:47.384824] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.3497 (1.3938)  acc1: 56.2500 (52.9830)  acc5: 93.7500 (93.8920)  time: 0.0869  data: 0.0574  max mem: 5511
[07:52:47.668012] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.3292 (1.3681)  acc1: 56.2500 (54.3899)  acc5: 95.3125 (94.1220)  time: 0.0288  data: 0.0002  max mem: 5511
[07:52:47.949762] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.3582 (1.3713)  acc1: 56.2500 (54.4859)  acc5: 95.3125 (94.3044)  time: 0.0281  data: 0.0001  max mem: 5511
[07:52:48.232156] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.3694 (1.3737)  acc1: 56.2500 (54.9543)  acc5: 93.7500 (94.1311)  time: 0.0281  data: 0.0001  max mem: 5511
[07:52:48.515483] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.3694 (1.3703)  acc1: 56.2500 (55.3309)  acc5: 95.3125 (94.3321)  time: 0.0282  data: 0.0002  max mem: 5511
[07:52:48.797590] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.3470 (1.3653)  acc1: 56.2500 (55.3791)  acc5: 95.3125 (94.1855)  time: 0.0281  data: 0.0002  max mem: 5511
[07:52:49.079801] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.3264 (1.3618)  acc1: 56.2500 (55.7218)  acc5: 93.7500 (94.3002)  time: 0.0281  data: 0.0002  max mem: 5511
[07:52:49.361365] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.3459 (1.3644)  acc1: 54.6875 (55.3434)  acc5: 95.3125 (94.3094)  time: 0.0281  data: 0.0001  max mem: 5511
[07:52:49.643871] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.3705 (1.3640)  acc1: 54.6875 (55.3915)  acc5: 93.7500 (94.1793)  time: 0.0281  data: 0.0001  max mem: 5511
[07:52:49.928934] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.3888 (1.3685)  acc1: 51.5625 (54.9196)  acc5: 93.7500 (94.0903)  time: 0.0283  data: 0.0001  max mem: 5511
[07:52:50.211139] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.4365 (1.3738)  acc1: 51.5625 (54.8142)  acc5: 93.7500 (94.1019)  time: 0.0282  data: 0.0001  max mem: 5511
[07:52:50.491916] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.3903 (1.3733)  acc1: 53.1250 (54.6229)  acc5: 95.3125 (94.2278)  time: 0.0280  data: 0.0001  max mem: 5511
[07:52:50.773610] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.3829 (1.3770)  acc1: 53.1250 (54.4847)  acc5: 95.3125 (94.1436)  time: 0.0280  data: 0.0001  max mem: 5511
[07:52:51.054361] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.3959 (1.3773)  acc1: 54.6875 (54.5213)  acc5: 93.7500 (94.1600)  time: 0.0280  data: 0.0001  max mem: 5511
[07:52:51.334477] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.3951 (1.3765)  acc1: 53.1250 (54.4805)  acc5: 93.7500 (94.1225)  time: 0.0279  data: 0.0001  max mem: 5511
[07:52:51.486820] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.3678 (1.3773)  acc1: 53.1250 (54.3900)  acc5: 93.7500 (94.1100)  time: 0.0271  data: 0.0001  max mem: 5511
[07:52:51.649081] Test: Total time: 0:00:05 (0.0333 s / it)
[07:52:51.649676] * Acc@1 54.390 Acc@5 94.110 loss 1.377
[07:52:51.650004] Accuracy of the network on the 10000 test images: 54.4%
[07:52:51.650226] Max accuracy: 54.39%
[07:52:51.823658] log_dir: ./output_dir
[07:52:52.712931] Epoch: [11]  [  0/781]  eta: 0:11:33  lr: 0.000248  training_loss: 0.4953 (0.4953)  mae_loss: 0.3100 (0.3100)  classification_loss: 0.1852 (0.1852)  time: 0.8875  data: 0.6722  max mem: 5511
[07:52:56.616216] Epoch: [11]  [ 20/781]  eta: 0:02:53  lr: 0.000248  training_loss: 0.5102 (0.5141)  mae_loss: 0.3195 (0.3240)  classification_loss: 0.1904 (0.1901)  time: 0.1951  data: 0.0002  max mem: 5511
[07:53:00.526235] Epoch: [11]  [ 40/781]  eta: 0:02:37  lr: 0.000248  training_loss: 0.5060 (0.5142)  mae_loss: 0.3186 (0.3226)  classification_loss: 0.1948 (0.1916)  time: 0.1954  data: 0.0002  max mem: 5511
[07:53:04.440183] Epoch: [11]  [ 60/781]  eta: 0:02:29  lr: 0.000247  training_loss: 0.5094 (0.5138)  mae_loss: 0.3151 (0.3219)  classification_loss: 0.1927 (0.1919)  time: 0.1956  data: 0.0002  max mem: 5511
[07:53:08.363680] Epoch: [11]  [ 80/781]  eta: 0:02:23  lr: 0.000247  training_loss: 0.5090 (0.5154)  mae_loss: 0.3178 (0.3232)  classification_loss: 0.1928 (0.1922)  time: 0.1961  data: 0.0002  max mem: 5511
[07:53:12.260608] Epoch: [11]  [100/781]  eta: 0:02:17  lr: 0.000247  training_loss: 0.5079 (0.5148)  mae_loss: 0.3157 (0.3226)  classification_loss: 0.1935 (0.1922)  time: 0.1948  data: 0.0002  max mem: 5511
[07:53:16.167649] Epoch: [11]  [120/781]  eta: 0:02:12  lr: 0.000247  training_loss: 0.4933 (0.5118)  mae_loss: 0.3012 (0.3198)  classification_loss: 0.1891 (0.1921)  time: 0.1952  data: 0.0002  max mem: 5511
[07:53:20.106143] Epoch: [11]  [140/781]  eta: 0:02:08  lr: 0.000247  training_loss: 0.4942 (0.5109)  mae_loss: 0.3050 (0.3193)  classification_loss: 0.1896 (0.1916)  time: 0.1968  data: 0.0003  max mem: 5511
[07:53:24.029214] Epoch: [11]  [160/781]  eta: 0:02:04  lr: 0.000247  training_loss: 0.4955 (0.5110)  mae_loss: 0.3123 (0.3193)  classification_loss: 0.1908 (0.1917)  time: 0.1960  data: 0.0002  max mem: 5511
[07:53:27.962731] Epoch: [11]  [180/781]  eta: 0:01:59  lr: 0.000247  training_loss: 0.5087 (0.5111)  mae_loss: 0.3193 (0.3197)  classification_loss: 0.1902 (0.1914)  time: 0.1966  data: 0.0002  max mem: 5511
[07:53:31.883101] Epoch: [11]  [200/781]  eta: 0:01:55  lr: 0.000247  training_loss: 0.4982 (0.5107)  mae_loss: 0.3063 (0.3194)  classification_loss: 0.1916 (0.1913)  time: 0.1959  data: 0.0002  max mem: 5511
[07:53:35.781540] Epoch: [11]  [220/781]  eta: 0:01:51  lr: 0.000247  training_loss: 0.5061 (0.5101)  mae_loss: 0.3076 (0.3189)  classification_loss: 0.1866 (0.1913)  time: 0.1949  data: 0.0002  max mem: 5511
[07:53:39.692408] Epoch: [11]  [240/781]  eta: 0:01:47  lr: 0.000247  training_loss: 0.4970 (0.5095)  mae_loss: 0.3030 (0.3180)  classification_loss: 0.1945 (0.1915)  time: 0.1954  data: 0.0002  max mem: 5511
[07:53:43.586287] Epoch: [11]  [260/781]  eta: 0:01:43  lr: 0.000247  training_loss: 0.4994 (0.5087)  mae_loss: 0.3095 (0.3175)  classification_loss: 0.1882 (0.1912)  time: 0.1946  data: 0.0002  max mem: 5511
[07:53:47.504470] Epoch: [11]  [280/781]  eta: 0:01:39  lr: 0.000247  training_loss: 0.5118 (0.5088)  mae_loss: 0.3151 (0.3176)  classification_loss: 0.1913 (0.1913)  time: 0.1958  data: 0.0002  max mem: 5511
[07:53:51.393765] Epoch: [11]  [300/781]  eta: 0:01:35  lr: 0.000247  training_loss: 0.4962 (0.5088)  mae_loss: 0.3102 (0.3177)  classification_loss: 0.1885 (0.1911)  time: 0.1943  data: 0.0002  max mem: 5511
[07:53:55.293123] Epoch: [11]  [320/781]  eta: 0:01:31  lr: 0.000247  training_loss: 0.4840 (0.5083)  mae_loss: 0.2971 (0.3173)  classification_loss: 0.1890 (0.1910)  time: 0.1949  data: 0.0002  max mem: 5511
[07:53:59.198355] Epoch: [11]  [340/781]  eta: 0:01:27  lr: 0.000247  training_loss: 0.4953 (0.5076)  mae_loss: 0.3050 (0.3167)  classification_loss: 0.1889 (0.1909)  time: 0.1952  data: 0.0003  max mem: 5511
[07:54:03.104815] Epoch: [11]  [360/781]  eta: 0:01:23  lr: 0.000247  training_loss: 0.5062 (0.5073)  mae_loss: 0.3101 (0.3162)  classification_loss: 0.1929 (0.1911)  time: 0.1952  data: 0.0003  max mem: 5511
[07:54:07.080198] Epoch: [11]  [380/781]  eta: 0:01:19  lr: 0.000247  training_loss: 0.5230 (0.5082)  mae_loss: 0.3369 (0.3171)  classification_loss: 0.1895 (0.1911)  time: 0.1987  data: 0.0002  max mem: 5511
[07:54:10.998249] Epoch: [11]  [400/781]  eta: 0:01:15  lr: 0.000247  training_loss: 0.5118 (0.5082)  mae_loss: 0.3116 (0.3171)  classification_loss: 0.1893 (0.1911)  time: 0.1958  data: 0.0002  max mem: 5511
[07:54:14.909951] Epoch: [11]  [420/781]  eta: 0:01:11  lr: 0.000247  training_loss: 0.5154 (0.5086)  mae_loss: 0.3223 (0.3175)  classification_loss: 0.1923 (0.1911)  time: 0.1955  data: 0.0003  max mem: 5511
[07:54:18.885156] Epoch: [11]  [440/781]  eta: 0:01:07  lr: 0.000247  training_loss: 0.5043 (0.5085)  mae_loss: 0.3164 (0.3175)  classification_loss: 0.1901 (0.1910)  time: 0.1987  data: 0.0002  max mem: 5511
[07:54:22.792364] Epoch: [11]  [460/781]  eta: 0:01:03  lr: 0.000247  training_loss: 0.4931 (0.5082)  mae_loss: 0.3095 (0.3174)  classification_loss: 0.1866 (0.1909)  time: 0.1953  data: 0.0001  max mem: 5511
[07:54:26.697095] Epoch: [11]  [480/781]  eta: 0:00:59  lr: 0.000247  training_loss: 0.4958 (0.5076)  mae_loss: 0.3002 (0.3168)  classification_loss: 0.1890 (0.1908)  time: 0.1952  data: 0.0002  max mem: 5511
[07:54:30.572439] Epoch: [11]  [500/781]  eta: 0:00:55  lr: 0.000247  training_loss: 0.5001 (0.5076)  mae_loss: 0.3041 (0.3167)  classification_loss: 0.1897 (0.1909)  time: 0.1937  data: 0.0002  max mem: 5511
[07:54:34.466318] Epoch: [11]  [520/781]  eta: 0:00:51  lr: 0.000247  training_loss: 0.5011 (0.5076)  mae_loss: 0.3140 (0.3167)  classification_loss: 0.1909 (0.1909)  time: 0.1946  data: 0.0002  max mem: 5511
[07:54:38.403688] Epoch: [11]  [540/781]  eta: 0:00:47  lr: 0.000247  training_loss: 0.4981 (0.5075)  mae_loss: 0.3150 (0.3167)  classification_loss: 0.1910 (0.1908)  time: 0.1968  data: 0.0002  max mem: 5511
[07:54:42.310128] Epoch: [11]  [560/781]  eta: 0:00:43  lr: 0.000247  training_loss: 0.4867 (0.5073)  mae_loss: 0.2999 (0.3165)  classification_loss: 0.1891 (0.1908)  time: 0.1952  data: 0.0003  max mem: 5511
[07:54:46.224657] Epoch: [11]  [580/781]  eta: 0:00:39  lr: 0.000247  training_loss: 0.4919 (0.5070)  mae_loss: 0.3055 (0.3163)  classification_loss: 0.1897 (0.1908)  time: 0.1956  data: 0.0002  max mem: 5511
[07:54:50.135355] Epoch: [11]  [600/781]  eta: 0:00:35  lr: 0.000247  training_loss: 0.5006 (0.5066)  mae_loss: 0.3097 (0.3159)  classification_loss: 0.1862 (0.1907)  time: 0.1954  data: 0.0002  max mem: 5511
[07:54:54.048252] Epoch: [11]  [620/781]  eta: 0:00:31  lr: 0.000247  training_loss: 0.4975 (0.5064)  mae_loss: 0.3109 (0.3157)  classification_loss: 0.1882 (0.1906)  time: 0.1956  data: 0.0005  max mem: 5511
[07:54:57.986131] Epoch: [11]  [640/781]  eta: 0:00:27  lr: 0.000247  training_loss: 0.4878 (0.5057)  mae_loss: 0.3003 (0.3152)  classification_loss: 0.1893 (0.1906)  time: 0.1968  data: 0.0002  max mem: 5511
[07:55:01.880664] Epoch: [11]  [660/781]  eta: 0:00:23  lr: 0.000247  training_loss: 0.4950 (0.5055)  mae_loss: 0.3022 (0.3149)  classification_loss: 0.1896 (0.1906)  time: 0.1946  data: 0.0002  max mem: 5511
[07:55:05.785334] Epoch: [11]  [680/781]  eta: 0:00:19  lr: 0.000247  training_loss: 0.5202 (0.5058)  mae_loss: 0.3252 (0.3153)  classification_loss: 0.1908 (0.1905)  time: 0.1951  data: 0.0003  max mem: 5511
[07:55:09.729422] Epoch: [11]  [700/781]  eta: 0:00:15  lr: 0.000247  training_loss: 0.4985 (0.5058)  mae_loss: 0.3013 (0.3152)  classification_loss: 0.1877 (0.1906)  time: 0.1971  data: 0.0002  max mem: 5511
[07:55:13.633600] Epoch: [11]  [720/781]  eta: 0:00:11  lr: 0.000247  training_loss: 0.5010 (0.5056)  mae_loss: 0.3093 (0.3150)  classification_loss: 0.1912 (0.1906)  time: 0.1951  data: 0.0002  max mem: 5511
[07:55:17.578709] Epoch: [11]  [740/781]  eta: 0:00:08  lr: 0.000247  training_loss: 0.4906 (0.5054)  mae_loss: 0.3019 (0.3149)  classification_loss: 0.1851 (0.1905)  time: 0.1972  data: 0.0002  max mem: 5511
[07:55:21.488142] Epoch: [11]  [760/781]  eta: 0:00:04  lr: 0.000247  training_loss: 0.5101 (0.5055)  mae_loss: 0.3174 (0.3150)  classification_loss: 0.1896 (0.1905)  time: 0.1954  data: 0.0002  max mem: 5511
[07:55:25.391081] Epoch: [11]  [780/781]  eta: 0:00:00  lr: 0.000247  training_loss: 0.4938 (0.5053)  mae_loss: 0.3017 (0.3147)  classification_loss: 0.1928 (0.1905)  time: 0.1951  data: 0.0002  max mem: 5511
[07:55:25.540451] Epoch: [11] Total time: 0:02:33 (0.1968 s / it)
[07:55:25.540935] Averaged stats: lr: 0.000247  training_loss: 0.4938 (0.5053)  mae_loss: 0.3017 (0.3147)  classification_loss: 0.1928 (0.1905)
[07:55:26.126043] Test:  [  0/157]  eta: 0:01:31  testing_loss: 1.3159 (1.3159)  acc1: 54.6875 (54.6875)  acc5: 95.3125 (95.3125)  time: 0.5808  data: 0.5464  max mem: 5511
[07:55:26.419881] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 1.3679 (1.3789)  acc1: 53.1250 (54.1193)  acc5: 95.3125 (94.0341)  time: 0.0794  data: 0.0499  max mem: 5511
[07:55:26.705211] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.3044 (1.3396)  acc1: 56.2500 (55.5804)  acc5: 95.3125 (95.0149)  time: 0.0288  data: 0.0004  max mem: 5511
[07:55:26.985458] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 1.3089 (1.3399)  acc1: 56.2500 (55.6956)  acc5: 95.3125 (94.5060)  time: 0.0282  data: 0.0003  max mem: 5511
[07:55:27.267460] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 1.3387 (1.3434)  acc1: 53.1250 (54.4207)  acc5: 93.7500 (94.3979)  time: 0.0280  data: 0.0002  max mem: 5511
[07:55:27.547893] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.3227 (1.3366)  acc1: 53.1250 (54.6875)  acc5: 95.3125 (94.5772)  time: 0.0280  data: 0.0002  max mem: 5511
[07:55:27.828080] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.3052 (1.3322)  acc1: 54.6875 (55.0205)  acc5: 95.3125 (94.4416)  time: 0.0279  data: 0.0002  max mem: 5511
[07:55:28.108870] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.2917 (1.3284)  acc1: 54.6875 (55.1717)  acc5: 93.7500 (94.5423)  time: 0.0279  data: 0.0002  max mem: 5511
[07:55:28.390066] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.2962 (1.3305)  acc1: 53.1250 (54.9769)  acc5: 95.3125 (94.5023)  time: 0.0280  data: 0.0002  max mem: 5511
[07:55:28.672249] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.3473 (1.3307)  acc1: 54.6875 (54.9451)  acc5: 95.3125 (94.4196)  time: 0.0280  data: 0.0002  max mem: 5511
[07:55:28.953213] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.3473 (1.3349)  acc1: 53.1250 (54.6256)  acc5: 92.1875 (94.3998)  time: 0.0280  data: 0.0002  max mem: 5511
[07:55:29.234952] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.3897 (1.3375)  acc1: 53.1250 (54.5327)  acc5: 93.7500 (94.4257)  time: 0.0280  data: 0.0002  max mem: 5511
[07:55:29.516370] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.3488 (1.3371)  acc1: 54.6875 (54.5842)  acc5: 95.3125 (94.5764)  time: 0.0280  data: 0.0002  max mem: 5511
[07:55:29.798549] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.3417 (1.3403)  acc1: 53.1250 (54.5086)  acc5: 95.3125 (94.4656)  time: 0.0281  data: 0.0002  max mem: 5511
[07:55:30.078256] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.3434 (1.3408)  acc1: 53.1250 (54.5102)  acc5: 93.7500 (94.4038)  time: 0.0280  data: 0.0002  max mem: 5511
[07:55:30.356217] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.3434 (1.3410)  acc1: 53.1250 (54.5323)  acc5: 93.7500 (94.3502)  time: 0.0278  data: 0.0001  max mem: 5511
[07:55:30.506176] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.3320 (1.3419)  acc1: 53.1250 (54.4600)  acc5: 93.7500 (94.3600)  time: 0.0268  data: 0.0001  max mem: 5511
[07:55:30.671771] Test: Total time: 0:00:05 (0.0327 s / it)
[07:55:30.672224] * Acc@1 54.460 Acc@5 94.360 loss 1.342
[07:55:30.672515] Accuracy of the network on the 10000 test images: 54.5%
[07:55:30.672692] Max accuracy: 54.46%
[07:55:30.790834] log_dir: ./output_dir
[07:55:31.603496] Epoch: [12]  [  0/781]  eta: 0:10:33  lr: 0.000247  training_loss: 0.5089 (0.5089)  mae_loss: 0.3237 (0.3237)  classification_loss: 0.1852 (0.1852)  time: 0.8106  data: 0.6060  max mem: 5511
[07:55:35.536365] Epoch: [12]  [ 20/781]  eta: 0:02:51  lr: 0.000247  training_loss: 0.4929 (0.4950)  mae_loss: 0.3030 (0.3052)  classification_loss: 0.1881 (0.1897)  time: 0.1966  data: 0.0003  max mem: 5511
[07:55:39.472434] Epoch: [12]  [ 40/781]  eta: 0:02:36  lr: 0.000247  training_loss: 0.4909 (0.4933)  mae_loss: 0.3009 (0.3032)  classification_loss: 0.1912 (0.1901)  time: 0.1965  data: 0.0002  max mem: 5511
[07:55:43.376294] Epoch: [12]  [ 60/781]  eta: 0:02:28  lr: 0.000247  training_loss: 0.4984 (0.4966)  mae_loss: 0.3070 (0.3068)  classification_loss: 0.1891 (0.1898)  time: 0.1951  data: 0.0003  max mem: 5511
[07:55:47.301183] Epoch: [12]  [ 80/781]  eta: 0:02:22  lr: 0.000247  training_loss: 0.4957 (0.4965)  mae_loss: 0.3075 (0.3065)  classification_loss: 0.1872 (0.1899)  time: 0.1962  data: 0.0002  max mem: 5511
[07:55:51.210561] Epoch: [12]  [100/781]  eta: 0:02:17  lr: 0.000247  training_loss: 0.4779 (0.4943)  mae_loss: 0.2965 (0.3045)  classification_loss: 0.1880 (0.1898)  time: 0.1954  data: 0.0002  max mem: 5511
[07:55:55.123804] Epoch: [12]  [120/781]  eta: 0:02:12  lr: 0.000247  training_loss: 0.4778 (0.4927)  mae_loss: 0.2923 (0.3033)  classification_loss: 0.1873 (0.1894)  time: 0.1956  data: 0.0002  max mem: 5511
[07:55:59.060633] Epoch: [12]  [140/781]  eta: 0:02:08  lr: 0.000247  training_loss: 0.4854 (0.4930)  mae_loss: 0.2958 (0.3036)  classification_loss: 0.1901 (0.1895)  time: 0.1967  data: 0.0004  max mem: 5511
[07:56:02.984437] Epoch: [12]  [160/781]  eta: 0:02:04  lr: 0.000246  training_loss: 0.4869 (0.4928)  mae_loss: 0.2989 (0.3035)  classification_loss: 0.1871 (0.1893)  time: 0.1961  data: 0.0003  max mem: 5511
[07:56:06.892788] Epoch: [12]  [180/781]  eta: 0:01:59  lr: 0.000246  training_loss: 0.5090 (0.4954)  mae_loss: 0.3209 (0.3064)  classification_loss: 0.1852 (0.1890)  time: 0.1953  data: 0.0003  max mem: 5511
[07:56:10.834395] Epoch: [12]  [200/781]  eta: 0:01:55  lr: 0.000246  training_loss: 0.4923 (0.4951)  mae_loss: 0.3048 (0.3061)  classification_loss: 0.1903 (0.1890)  time: 0.1970  data: 0.0002  max mem: 5511
[07:56:14.728413] Epoch: [12]  [220/781]  eta: 0:01:51  lr: 0.000246  training_loss: 0.4910 (0.4949)  mae_loss: 0.2998 (0.3060)  classification_loss: 0.1891 (0.1889)  time: 0.1946  data: 0.0002  max mem: 5511
[07:56:18.630230] Epoch: [12]  [240/781]  eta: 0:01:47  lr: 0.000246  training_loss: 0.4890 (0.4945)  mae_loss: 0.2999 (0.3057)  classification_loss: 0.1883 (0.1888)  time: 0.1950  data: 0.0002  max mem: 5511
[07:56:22.536473] Epoch: [12]  [260/781]  eta: 0:01:43  lr: 0.000246  training_loss: 0.4772 (0.4937)  mae_loss: 0.2861 (0.3050)  classification_loss: 0.1865 (0.1887)  time: 0.1952  data: 0.0002  max mem: 5511
[07:56:26.445895] Epoch: [12]  [280/781]  eta: 0:01:39  lr: 0.000246  training_loss: 0.5049 (0.4952)  mae_loss: 0.3160 (0.3065)  classification_loss: 0.1868 (0.1887)  time: 0.1954  data: 0.0003  max mem: 5511
[07:56:30.360967] Epoch: [12]  [300/781]  eta: 0:01:35  lr: 0.000246  training_loss: 0.4897 (0.4953)  mae_loss: 0.2966 (0.3066)  classification_loss: 0.1876 (0.1887)  time: 0.1957  data: 0.0002  max mem: 5511
[07:56:34.277309] Epoch: [12]  [320/781]  eta: 0:01:31  lr: 0.000246  training_loss: 0.4958 (0.4954)  mae_loss: 0.3012 (0.3066)  classification_loss: 0.1859 (0.1888)  time: 0.1957  data: 0.0002  max mem: 5511
[07:56:38.212700] Epoch: [12]  [340/781]  eta: 0:01:27  lr: 0.000246  training_loss: 0.4870 (0.4950)  mae_loss: 0.3015 (0.3065)  classification_loss: 0.1824 (0.1885)  time: 0.1966  data: 0.0002  max mem: 5511
[07:56:42.139698] Epoch: [12]  [360/781]  eta: 0:01:23  lr: 0.000246  training_loss: 0.4806 (0.4952)  mae_loss: 0.3046 (0.3066)  classification_loss: 0.1883 (0.1886)  time: 0.1963  data: 0.0004  max mem: 5511
[07:56:46.102339] Epoch: [12]  [380/781]  eta: 0:01:19  lr: 0.000246  training_loss: 0.5088 (0.4961)  mae_loss: 0.3240 (0.3076)  classification_loss: 0.1869 (0.1885)  time: 0.1980  data: 0.0002  max mem: 5511
[07:56:50.034326] Epoch: [12]  [400/781]  eta: 0:01:15  lr: 0.000246  training_loss: 0.5076 (0.4969)  mae_loss: 0.3163 (0.3085)  classification_loss: 0.1853 (0.1884)  time: 0.1965  data: 0.0002  max mem: 5511
[07:56:53.951182] Epoch: [12]  [420/781]  eta: 0:01:11  lr: 0.000246  training_loss: 0.4919 (0.4971)  mae_loss: 0.3101 (0.3087)  classification_loss: 0.1872 (0.1884)  time: 0.1957  data: 0.0003  max mem: 5511
[07:56:57.851425] Epoch: [12]  [440/781]  eta: 0:01:07  lr: 0.000246  training_loss: 0.4947 (0.4970)  mae_loss: 0.3040 (0.3087)  classification_loss: 0.1867 (0.1884)  time: 0.1949  data: 0.0002  max mem: 5511
[07:57:01.786541] Epoch: [12]  [460/781]  eta: 0:01:03  lr: 0.000246  training_loss: 0.4726 (0.4964)  mae_loss: 0.2906 (0.3081)  classification_loss: 0.1835 (0.1882)  time: 0.1966  data: 0.0003  max mem: 5511
[07:57:05.742323] Epoch: [12]  [480/781]  eta: 0:00:59  lr: 0.000246  training_loss: 0.4792 (0.4961)  mae_loss: 0.3013 (0.3078)  classification_loss: 0.1858 (0.1882)  time: 0.1977  data: 0.0002  max mem: 5511
[07:57:09.657956] Epoch: [12]  [500/781]  eta: 0:00:55  lr: 0.000246  training_loss: 0.4781 (0.4955)  mae_loss: 0.2943 (0.3073)  classification_loss: 0.1896 (0.1882)  time: 0.1957  data: 0.0002  max mem: 5511
[07:57:13.564247] Epoch: [12]  [520/781]  eta: 0:00:51  lr: 0.000246  training_loss: 0.4927 (0.4955)  mae_loss: 0.2981 (0.3074)  classification_loss: 0.1865 (0.1881)  time: 0.1952  data: 0.0002  max mem: 5511
[07:57:17.488963] Epoch: [12]  [540/781]  eta: 0:00:47  lr: 0.000246  training_loss: 0.4897 (0.4955)  mae_loss: 0.3001 (0.3075)  classification_loss: 0.1833 (0.1880)  time: 0.1962  data: 0.0003  max mem: 5511
[07:57:21.489406] Epoch: [12]  [560/781]  eta: 0:00:43  lr: 0.000246  training_loss: 0.5029 (0.4955)  mae_loss: 0.3087 (0.3075)  classification_loss: 0.1893 (0.1880)  time: 0.1999  data: 0.0002  max mem: 5511
[07:57:25.383803] Epoch: [12]  [580/781]  eta: 0:00:39  lr: 0.000246  training_loss: 0.4905 (0.4955)  mae_loss: 0.3031 (0.3075)  classification_loss: 0.1861 (0.1880)  time: 0.1946  data: 0.0002  max mem: 5511
[07:57:29.296139] Epoch: [12]  [600/781]  eta: 0:00:35  lr: 0.000246  training_loss: 0.4903 (0.4955)  mae_loss: 0.3042 (0.3076)  classification_loss: 0.1835 (0.1879)  time: 0.1955  data: 0.0002  max mem: 5511
[07:57:33.239205] Epoch: [12]  [620/781]  eta: 0:00:31  lr: 0.000246  training_loss: 0.4794 (0.4955)  mae_loss: 0.2981 (0.3077)  classification_loss: 0.1823 (0.1878)  time: 0.1971  data: 0.0002  max mem: 5511
[07:57:37.153273] Epoch: [12]  [640/781]  eta: 0:00:27  lr: 0.000246  training_loss: 0.4801 (0.4955)  mae_loss: 0.3020 (0.3078)  classification_loss: 0.1871 (0.1877)  time: 0.1956  data: 0.0002  max mem: 5511
[07:57:41.130781] Epoch: [12]  [660/781]  eta: 0:00:23  lr: 0.000246  training_loss: 0.4868 (0.4953)  mae_loss: 0.2947 (0.3077)  classification_loss: 0.1847 (0.1877)  time: 0.1988  data: 0.0002  max mem: 5511
[07:57:45.066229] Epoch: [12]  [680/781]  eta: 0:00:19  lr: 0.000246  training_loss: 0.4983 (0.4954)  mae_loss: 0.3048 (0.3077)  classification_loss: 0.1899 (0.1877)  time: 0.1967  data: 0.0002  max mem: 5511
[07:57:48.962003] Epoch: [12]  [700/781]  eta: 0:00:15  lr: 0.000246  training_loss: 0.4849 (0.4954)  mae_loss: 0.3034 (0.3078)  classification_loss: 0.1847 (0.1876)  time: 0.1947  data: 0.0003  max mem: 5511
[07:57:52.875046] Epoch: [12]  [720/781]  eta: 0:00:12  lr: 0.000246  training_loss: 0.4999 (0.4956)  mae_loss: 0.3112 (0.3080)  classification_loss: 0.1855 (0.1877)  time: 0.1956  data: 0.0002  max mem: 5511
[07:57:56.777874] Epoch: [12]  [740/781]  eta: 0:00:08  lr: 0.000246  training_loss: 0.4882 (0.4953)  mae_loss: 0.2993 (0.3077)  classification_loss: 0.1865 (0.1876)  time: 0.1951  data: 0.0002  max mem: 5511
[07:58:00.712044] Epoch: [12]  [760/781]  eta: 0:00:04  lr: 0.000246  training_loss: 0.4850 (0.4951)  mae_loss: 0.3016 (0.3074)  classification_loss: 0.1879 (0.1876)  time: 0.1966  data: 0.0003  max mem: 5511
[07:58:04.645846] Epoch: [12]  [780/781]  eta: 0:00:00  lr: 0.000246  training_loss: 0.4826 (0.4947)  mae_loss: 0.2944 (0.3071)  classification_loss: 0.1854 (0.1876)  time: 0.1966  data: 0.0002  max mem: 5511
[07:58:04.804396] Epoch: [12] Total time: 0:02:34 (0.1972 s / it)
[07:58:04.805101] Averaged stats: lr: 0.000246  training_loss: 0.4826 (0.4947)  mae_loss: 0.2944 (0.3071)  classification_loss: 0.1854 (0.1876)
[07:58:05.413158] Test:  [  0/157]  eta: 0:01:34  testing_loss: 1.2250 (1.2250)  acc1: 65.6250 (65.6250)  acc5: 96.8750 (96.8750)  time: 0.6037  data: 0.5716  max mem: 5511
[07:58:05.699140] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 1.2462 (1.3003)  acc1: 59.3750 (58.2386)  acc5: 95.3125 (94.8864)  time: 0.0807  data: 0.0521  max mem: 5511
[07:58:05.990815] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.2455 (1.2595)  acc1: 56.2500 (58.1845)  acc5: 95.3125 (95.3125)  time: 0.0287  data: 0.0002  max mem: 5511
[07:58:06.275848] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 1.2189 (1.2664)  acc1: 57.8125 (58.3165)  acc5: 95.3125 (95.2117)  time: 0.0287  data: 0.0002  max mem: 5511
[07:58:06.566419] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 1.2894 (1.2687)  acc1: 54.6875 (57.0884)  acc5: 95.3125 (95.3506)  time: 0.0286  data: 0.0002  max mem: 5511
[07:58:06.852067] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.2451 (1.2659)  acc1: 54.6875 (57.2610)  acc5: 95.3125 (95.3431)  time: 0.0286  data: 0.0002  max mem: 5511
[07:58:07.138986] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.2347 (1.2603)  acc1: 57.8125 (57.5820)  acc5: 93.7500 (95.1588)  time: 0.0285  data: 0.0002  max mem: 5511
[07:58:07.426754] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.2221 (1.2555)  acc1: 57.8125 (57.7025)  acc5: 95.3125 (95.2685)  time: 0.0286  data: 0.0002  max mem: 5511
[07:58:07.709169] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.2392 (1.2548)  acc1: 57.8125 (57.6968)  acc5: 96.8750 (95.3125)  time: 0.0284  data: 0.0002  max mem: 5511
[07:58:07.992509] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.2462 (1.2561)  acc1: 57.8125 (57.5893)  acc5: 96.8750 (95.3640)  time: 0.0282  data: 0.0002  max mem: 5511
[07:58:08.277982] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.2903 (1.2614)  acc1: 54.6875 (57.1009)  acc5: 95.3125 (95.3589)  time: 0.0283  data: 0.0002  max mem: 5511
[07:58:08.560941] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.3327 (1.2641)  acc1: 54.6875 (57.0101)  acc5: 95.3125 (95.2984)  time: 0.0283  data: 0.0002  max mem: 5511
[07:58:08.846624] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.2785 (1.2645)  acc1: 56.2500 (57.0506)  acc5: 95.3125 (95.3642)  time: 0.0282  data: 0.0002  max mem: 5511
[07:58:09.131923] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.2785 (1.2673)  acc1: 56.2500 (56.9179)  acc5: 95.3125 (95.3006)  time: 0.0284  data: 0.0003  max mem: 5511
[07:58:09.412695] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.2714 (1.2664)  acc1: 56.2500 (56.9814)  acc5: 95.3125 (95.3125)  time: 0.0282  data: 0.0002  max mem: 5511
[07:58:09.692722] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.2633 (1.2647)  acc1: 57.8125 (57.1192)  acc5: 95.3125 (95.2815)  time: 0.0279  data: 0.0001  max mem: 5511
[07:58:09.843021] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.2542 (1.2647)  acc1: 57.8125 (57.1100)  acc5: 95.3125 (95.3400)  time: 0.0269  data: 0.0001  max mem: 5511
[07:58:10.021024] Test: Total time: 0:00:05 (0.0332 s / it)
[07:58:10.021713] * Acc@1 57.110 Acc@5 95.340 loss 1.265
[07:58:10.022012] Accuracy of the network on the 10000 test images: 57.1%
[07:58:10.022197] Max accuracy: 57.11%
[07:58:10.364298] log_dir: ./output_dir
[07:58:11.242938] Epoch: [13]  [  0/781]  eta: 0:11:24  lr: 0.000246  training_loss: 0.5248 (0.5248)  mae_loss: 0.3420 (0.3420)  classification_loss: 0.1828 (0.1828)  time: 0.8771  data: 0.6566  max mem: 5511
[07:58:15.196055] Epoch: [13]  [ 20/781]  eta: 0:02:54  lr: 0.000246  training_loss: 0.4752 (0.4837)  mae_loss: 0.2887 (0.2977)  classification_loss: 0.1852 (0.1860)  time: 0.1976  data: 0.0002  max mem: 5511
[07:58:19.116883] Epoch: [13]  [ 40/781]  eta: 0:02:38  lr: 0.000246  training_loss: 0.4964 (0.4891)  mae_loss: 0.3081 (0.3018)  classification_loss: 0.1871 (0.1873)  time: 0.1960  data: 0.0003  max mem: 5511
[07:58:23.043700] Epoch: [13]  [ 60/781]  eta: 0:02:29  lr: 0.000246  training_loss: 0.5022 (0.4936)  mae_loss: 0.3072 (0.3060)  classification_loss: 0.1877 (0.1876)  time: 0.1963  data: 0.0002  max mem: 5511
[07:58:26.937918] Epoch: [13]  [ 80/781]  eta: 0:02:23  lr: 0.000246  training_loss: 0.4813 (0.4933)  mae_loss: 0.2962 (0.3058)  classification_loss: 0.1872 (0.1875)  time: 0.1946  data: 0.0002  max mem: 5511
[07:58:30.852987] Epoch: [13]  [100/781]  eta: 0:02:18  lr: 0.000246  training_loss: 0.4881 (0.4924)  mae_loss: 0.3047 (0.3047)  classification_loss: 0.1855 (0.1877)  time: 0.1957  data: 0.0003  max mem: 5511
[07:58:34.754461] Epoch: [13]  [120/781]  eta: 0:02:13  lr: 0.000246  training_loss: 0.4737 (0.4901)  mae_loss: 0.2873 (0.3030)  classification_loss: 0.1848 (0.1870)  time: 0.1950  data: 0.0002  max mem: 5511
[07:58:38.687618] Epoch: [13]  [140/781]  eta: 0:02:08  lr: 0.000245  training_loss: 0.4888 (0.4895)  mae_loss: 0.3010 (0.3028)  classification_loss: 0.1840 (0.1868)  time: 0.1966  data: 0.0002  max mem: 5511
[07:58:42.596368] Epoch: [13]  [160/781]  eta: 0:02:04  lr: 0.000245  training_loss: 0.4694 (0.4874)  mae_loss: 0.2824 (0.3008)  classification_loss: 0.1849 (0.1866)  time: 0.1953  data: 0.0003  max mem: 5511
[07:58:46.502283] Epoch: [13]  [180/781]  eta: 0:01:59  lr: 0.000245  training_loss: 0.4808 (0.4876)  mae_loss: 0.3076 (0.3016)  classification_loss: 0.1794 (0.1860)  time: 0.1952  data: 0.0002  max mem: 5511
[07:58:50.407171] Epoch: [13]  [200/781]  eta: 0:01:55  lr: 0.000245  training_loss: 0.5019 (0.4880)  mae_loss: 0.3044 (0.3019)  classification_loss: 0.1848 (0.1861)  time: 0.1952  data: 0.0002  max mem: 5511
[07:58:54.347882] Epoch: [13]  [220/781]  eta: 0:01:51  lr: 0.000245  training_loss: 0.4670 (0.4871)  mae_loss: 0.2820 (0.3011)  classification_loss: 0.1843 (0.1859)  time: 0.1970  data: 0.0002  max mem: 5511
[07:58:58.240672] Epoch: [13]  [240/781]  eta: 0:01:47  lr: 0.000245  training_loss: 0.4744 (0.4870)  mae_loss: 0.2931 (0.3009)  classification_loss: 0.1897 (0.1861)  time: 0.1946  data: 0.0002  max mem: 5511
[07:59:02.148139] Epoch: [13]  [260/781]  eta: 0:01:43  lr: 0.000245  training_loss: 0.4806 (0.4865)  mae_loss: 0.2902 (0.3005)  classification_loss: 0.1859 (0.1861)  time: 0.1953  data: 0.0003  max mem: 5511
[07:59:06.053698] Epoch: [13]  [280/781]  eta: 0:01:39  lr: 0.000245  training_loss: 0.4954 (0.4870)  mae_loss: 0.3052 (0.3008)  classification_loss: 0.1862 (0.1861)  time: 0.1952  data: 0.0003  max mem: 5511
[07:59:09.957071] Epoch: [13]  [300/781]  eta: 0:01:35  lr: 0.000245  training_loss: 0.4876 (0.4871)  mae_loss: 0.3022 (0.3012)  classification_loss: 0.1842 (0.1859)  time: 0.1951  data: 0.0003  max mem: 5511
[07:59:13.902160] Epoch: [13]  [320/781]  eta: 0:01:31  lr: 0.000245  training_loss: 0.4730 (0.4871)  mae_loss: 0.2886 (0.3010)  classification_loss: 0.1903 (0.1861)  time: 0.1972  data: 0.0003  max mem: 5511
[07:59:17.814990] Epoch: [13]  [340/781]  eta: 0:01:27  lr: 0.000245  training_loss: 0.4699 (0.4864)  mae_loss: 0.2854 (0.3004)  classification_loss: 0.1828 (0.1860)  time: 0.1955  data: 0.0003  max mem: 5511
[07:59:21.729960] Epoch: [13]  [360/781]  eta: 0:01:23  lr: 0.000245  training_loss: 0.4799 (0.4862)  mae_loss: 0.2898 (0.3002)  classification_loss: 0.1864 (0.1860)  time: 0.1957  data: 0.0002  max mem: 5511
[07:59:25.661183] Epoch: [13]  [380/781]  eta: 0:01:19  lr: 0.000245  training_loss: 0.4775 (0.4862)  mae_loss: 0.2969 (0.3003)  classification_loss: 0.1833 (0.1859)  time: 0.1965  data: 0.0002  max mem: 5511
[07:59:29.580293] Epoch: [13]  [400/781]  eta: 0:01:15  lr: 0.000245  training_loss: 0.4818 (0.4860)  mae_loss: 0.2932 (0.3001)  classification_loss: 0.1845 (0.1859)  time: 0.1958  data: 0.0002  max mem: 5511
[07:59:33.487687] Epoch: [13]  [420/781]  eta: 0:01:11  lr: 0.000245  training_loss: 0.4761 (0.4857)  mae_loss: 0.2911 (0.2999)  classification_loss: 0.1845 (0.1858)  time: 0.1953  data: 0.0002  max mem: 5511
[07:59:37.403003] Epoch: [13]  [440/781]  eta: 0:01:07  lr: 0.000245  training_loss: 0.4813 (0.4854)  mae_loss: 0.2927 (0.2998)  classification_loss: 0.1792 (0.1856)  time: 0.1957  data: 0.0002  max mem: 5511
[07:59:41.310018] Epoch: [13]  [460/781]  eta: 0:01:03  lr: 0.000245  training_loss: 0.4541 (0.4847)  mae_loss: 0.2756 (0.2993)  classification_loss: 0.1812 (0.1854)  time: 0.1953  data: 0.0002  max mem: 5511
[07:59:45.222880] Epoch: [13]  [480/781]  eta: 0:00:59  lr: 0.000245  training_loss: 0.4800 (0.4846)  mae_loss: 0.2925 (0.2993)  classification_loss: 0.1835 (0.1854)  time: 0.1956  data: 0.0002  max mem: 5511
[07:59:49.125694] Epoch: [13]  [500/781]  eta: 0:00:55  lr: 0.000245  training_loss: 0.4715 (0.4841)  mae_loss: 0.2807 (0.2987)  classification_loss: 0.1846 (0.1854)  time: 0.1950  data: 0.0003  max mem: 5511
[07:59:53.025554] Epoch: [13]  [520/781]  eta: 0:00:51  lr: 0.000245  training_loss: 0.4759 (0.4838)  mae_loss: 0.2899 (0.2986)  classification_loss: 0.1813 (0.1852)  time: 0.1949  data: 0.0002  max mem: 5511
[07:59:56.937098] Epoch: [13]  [540/781]  eta: 0:00:47  lr: 0.000245  training_loss: 0.4967 (0.4842)  mae_loss: 0.3162 (0.2990)  classification_loss: 0.1840 (0.1852)  time: 0.1955  data: 0.0003  max mem: 5511
[08:00:00.850657] Epoch: [13]  [560/781]  eta: 0:00:43  lr: 0.000245  training_loss: 0.4809 (0.4846)  mae_loss: 0.2973 (0.2993)  classification_loss: 0.1820 (0.1852)  time: 0.1956  data: 0.0002  max mem: 5511
[08:00:04.747987] Epoch: [13]  [580/781]  eta: 0:00:39  lr: 0.000245  training_loss: 0.4733 (0.4843)  mae_loss: 0.2821 (0.2990)  classification_loss: 0.1869 (0.1853)  time: 0.1947  data: 0.0002  max mem: 5511
[08:00:08.705776] Epoch: [13]  [600/781]  eta: 0:00:35  lr: 0.000245  training_loss: 0.4774 (0.4840)  mae_loss: 0.2942 (0.2989)  classification_loss: 0.1790 (0.1851)  time: 0.1978  data: 0.0008  max mem: 5511
[08:00:12.611325] Epoch: [13]  [620/781]  eta: 0:00:31  lr: 0.000245  training_loss: 0.4686 (0.4837)  mae_loss: 0.2854 (0.2988)  classification_loss: 0.1791 (0.1849)  time: 0.1952  data: 0.0002  max mem: 5511
[08:00:16.504925] Epoch: [13]  [640/781]  eta: 0:00:27  lr: 0.000245  training_loss: 0.4915 (0.4838)  mae_loss: 0.3002 (0.2989)  classification_loss: 0.1827 (0.1849)  time: 0.1946  data: 0.0002  max mem: 5511
[08:00:20.468848] Epoch: [13]  [660/781]  eta: 0:00:23  lr: 0.000245  training_loss: 0.4659 (0.4834)  mae_loss: 0.2834 (0.2985)  classification_loss: 0.1800 (0.1849)  time: 0.1981  data: 0.0003  max mem: 5511
[08:00:24.403197] Epoch: [13]  [680/781]  eta: 0:00:19  lr: 0.000245  training_loss: 0.4864 (0.4834)  mae_loss: 0.3025 (0.2986)  classification_loss: 0.1855 (0.1849)  time: 0.1966  data: 0.0004  max mem: 5511
[08:00:28.320884] Epoch: [13]  [700/781]  eta: 0:00:15  lr: 0.000245  training_loss: 0.4890 (0.4836)  mae_loss: 0.3029 (0.2988)  classification_loss: 0.1814 (0.1848)  time: 0.1958  data: 0.0002  max mem: 5511
[08:00:32.234164] Epoch: [13]  [720/781]  eta: 0:00:11  lr: 0.000245  training_loss: 0.4634 (0.4831)  mae_loss: 0.2869 (0.2983)  classification_loss: 0.1800 (0.1848)  time: 0.1956  data: 0.0003  max mem: 5511
[08:00:36.145424] Epoch: [13]  [740/781]  eta: 0:00:08  lr: 0.000245  training_loss: 0.4604 (0.4826)  mae_loss: 0.2835 (0.2979)  classification_loss: 0.1799 (0.1847)  time: 0.1955  data: 0.0003  max mem: 5511
[08:00:40.076953] Epoch: [13]  [760/781]  eta: 0:00:04  lr: 0.000245  training_loss: 0.4839 (0.4825)  mae_loss: 0.2963 (0.2978)  classification_loss: 0.1843 (0.1847)  time: 0.1965  data: 0.0003  max mem: 5511
[08:00:43.966301] Epoch: [13]  [780/781]  eta: 0:00:00  lr: 0.000245  training_loss: 0.4652 (0.4822)  mae_loss: 0.2897 (0.2976)  classification_loss: 0.1817 (0.1846)  time: 0.1944  data: 0.0007  max mem: 5511
[08:00:44.117104] Epoch: [13] Total time: 0:02:33 (0.1969 s / it)
[08:00:44.117666] Averaged stats: lr: 0.000245  training_loss: 0.4652 (0.4822)  mae_loss: 0.2897 (0.2976)  classification_loss: 0.1817 (0.1846)
[08:00:44.731866] Test:  [  0/157]  eta: 0:01:35  testing_loss: 1.1630 (1.1630)  acc1: 62.5000 (62.5000)  acc5: 95.3125 (95.3125)  time: 0.6102  data: 0.5683  max mem: 5511
[08:00:45.026626] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.2247 (1.2487)  acc1: 60.9375 (57.9545)  acc5: 93.7500 (94.4602)  time: 0.0821  data: 0.0521  max mem: 5511
[08:00:45.314898] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.2045 (1.2081)  acc1: 60.9375 (59.9702)  acc5: 95.3125 (94.7173)  time: 0.0290  data: 0.0003  max mem: 5511
[08:00:45.609608] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.2069 (1.2147)  acc1: 59.3750 (59.2238)  acc5: 95.3125 (94.8085)  time: 0.0289  data: 0.0002  max mem: 5511
[08:00:45.895847] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.2069 (1.2091)  acc1: 57.8125 (59.1845)  acc5: 95.3125 (94.9314)  time: 0.0288  data: 0.0002  max mem: 5511
[08:00:46.177392] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.2028 (1.2085)  acc1: 59.3750 (59.4669)  acc5: 95.3125 (94.9755)  time: 0.0282  data: 0.0002  max mem: 5511
[08:00:46.460317] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.2028 (1.1974)  acc1: 59.3750 (59.8873)  acc5: 96.8750 (95.1588)  time: 0.0281  data: 0.0002  max mem: 5511
[08:00:46.744131] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.1403 (1.1910)  acc1: 60.9375 (60.2993)  acc5: 96.8750 (95.2905)  time: 0.0282  data: 0.0002  max mem: 5511
[08:00:47.031991] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.1595 (1.1913)  acc1: 60.9375 (60.1080)  acc5: 95.3125 (95.2546)  time: 0.0285  data: 0.0002  max mem: 5511
[08:00:47.315748] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.1922 (1.1924)  acc1: 60.9375 (60.1133)  acc5: 95.3125 (95.3812)  time: 0.0285  data: 0.0002  max mem: 5511
[08:00:47.604229] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.2203 (1.1976)  acc1: 56.2500 (59.7463)  acc5: 96.8750 (95.3434)  time: 0.0285  data: 0.0002  max mem: 5511
[08:00:47.888335] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.2524 (1.1999)  acc1: 54.6875 (59.5861)  acc5: 95.3125 (95.3407)  time: 0.0285  data: 0.0002  max mem: 5511
[08:00:48.175983] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.2284 (1.1999)  acc1: 57.8125 (59.4912)  acc5: 95.3125 (95.3900)  time: 0.0284  data: 0.0002  max mem: 5511
[08:00:48.464180] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.2284 (1.2031)  acc1: 59.3750 (59.4346)  acc5: 95.3125 (95.3483)  time: 0.0286  data: 0.0002  max mem: 5511
[08:00:48.745028] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.2441 (1.2040)  acc1: 59.3750 (59.5191)  acc5: 95.3125 (95.3457)  time: 0.0283  data: 0.0001  max mem: 5511
[08:00:49.024870] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.1952 (1.2031)  acc1: 60.9375 (59.5199)  acc5: 93.7500 (95.2711)  time: 0.0279  data: 0.0001  max mem: 5511
[08:00:49.175950] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.1813 (1.2029)  acc1: 60.9375 (59.4500)  acc5: 95.3125 (95.3300)  time: 0.0270  data: 0.0001  max mem: 5511
[08:00:49.325338] Test: Total time: 0:00:05 (0.0331 s / it)
[08:00:49.326048] * Acc@1 59.450 Acc@5 95.330 loss 1.203
[08:00:49.326343] Accuracy of the network on the 10000 test images: 59.5%
[08:00:49.326523] Max accuracy: 59.45%
[08:00:49.535641] log_dir: ./output_dir
[08:00:50.513352] Epoch: [14]  [  0/781]  eta: 0:12:42  lr: 0.000245  training_loss: 0.4391 (0.4391)  mae_loss: 0.2705 (0.2705)  classification_loss: 0.1686 (0.1686)  time: 0.9760  data: 0.7711  max mem: 5511
[08:00:54.458272] Epoch: [14]  [ 20/781]  eta: 0:02:58  lr: 0.000244  training_loss: 0.4532 (0.4588)  mae_loss: 0.2804 (0.2810)  classification_loss: 0.1784 (0.1778)  time: 0.1972  data: 0.0002  max mem: 5511
[08:00:58.368030] Epoch: [14]  [ 40/781]  eta: 0:02:39  lr: 0.000244  training_loss: 0.4618 (0.4650)  mae_loss: 0.2802 (0.2855)  classification_loss: 0.1814 (0.1795)  time: 0.1954  data: 0.0002  max mem: 5511
[08:01:02.260988] Epoch: [14]  [ 60/781]  eta: 0:02:30  lr: 0.000244  training_loss: 0.4817 (0.4719)  mae_loss: 0.3002 (0.2911)  classification_loss: 0.1827 (0.1808)  time: 0.1946  data: 0.0002  max mem: 5511
[08:01:06.162329] Epoch: [14]  [ 80/781]  eta: 0:02:23  lr: 0.000244  training_loss: 0.4645 (0.4728)  mae_loss: 0.2818 (0.2907)  classification_loss: 0.1839 (0.1821)  time: 0.1950  data: 0.0002  max mem: 5511
[08:01:10.075805] Epoch: [14]  [100/781]  eta: 0:02:18  lr: 0.000244  training_loss: 0.4739 (0.4725)  mae_loss: 0.2868 (0.2896)  classification_loss: 0.1842 (0.1829)  time: 0.1956  data: 0.0002  max mem: 5511
[08:01:13.987318] Epoch: [14]  [120/781]  eta: 0:02:13  lr: 0.000244  training_loss: 0.4636 (0.4722)  mae_loss: 0.2829 (0.2898)  classification_loss: 0.1772 (0.1824)  time: 0.1955  data: 0.0002  max mem: 5511
[08:01:17.898726] Epoch: [14]  [140/781]  eta: 0:02:08  lr: 0.000244  training_loss: 0.4774 (0.4721)  mae_loss: 0.2954 (0.2903)  classification_loss: 0.1766 (0.1818)  time: 0.1955  data: 0.0002  max mem: 5511
[08:01:21.792965] Epoch: [14]  [160/781]  eta: 0:02:04  lr: 0.000244  training_loss: 0.4565 (0.4704)  mae_loss: 0.2745 (0.2888)  classification_loss: 0.1784 (0.1816)  time: 0.1946  data: 0.0002  max mem: 5511

[08:01:25.699222] Epoch: [14]  [180/781]  eta: 0:02:00  lr: 0.000244  training_loss: 0.4838 (0.4718)  mae_loss: 0.3015 (0.2902)  classification_loss: 0.1817 (0.1817)  time: 0.1952  data: 0.0005  max mem: 5511
[08:01:29.593241] Epoch: [14]  [200/781]  eta: 0:01:55  lr: 0.000244  training_loss: 0.4627 (0.4713)  mae_loss: 0.2732 (0.2898)  classification_loss: 0.1765 (0.1815)  time: 0.1946  data: 0.0002  max mem: 5511
[08:01:33.524796] Epoch: [14]  [220/781]  eta: 0:01:51  lr: 0.000244  training_loss: 0.4614 (0.4709)  mae_loss: 0.2853 (0.2895)  classification_loss: 0.1833 (0.1814)  time: 0.1962  data: 0.0002  max mem: 5511
[08:01:37.438817] Epoch: [14]  [240/781]  eta: 0:01:47  lr: 0.000244  training_loss: 0.4722 (0.4712)  mae_loss: 0.2890 (0.2899)  classification_loss: 0.1777 (0.1813)  time: 0.1956  data: 0.0002  max mem: 5511
[08:01:41.358816] Epoch: [14]  [260/781]  eta: 0:01:43  lr: 0.000244  training_loss: 0.4641 (0.4708)  mae_loss: 0.2697 (0.2892)  classification_loss: 0.1826 (0.1816)  time: 0.1959  data: 0.0003  max mem: 5511
[08:01:45.267058] Epoch: [14]  [280/781]  eta: 0:01:39  lr: 0.000244  training_loss: 0.4733 (0.4713)  mae_loss: 0.2976 (0.2895)  classification_loss: 0.1838 (0.1817)  time: 0.1953  data: 0.0003  max mem: 5511
[08:01:49.198449] Epoch: [14]  [300/781]  eta: 0:01:35  lr: 0.000244  training_loss: 0.4750 (0.4713)  mae_loss: 0.2998 (0.2896)  classification_loss: 0.1815 (0.1817)  time: 0.1965  data: 0.0003  max mem: 5511
[08:01:53.130066] Epoch: [14]  [320/781]  eta: 0:01:31  lr: 0.000244  training_loss: 0.4690 (0.4709)  mae_loss: 0.2808 (0.2892)  classification_loss: 0.1819 (0.1817)  time: 0.1965  data: 0.0003  max mem: 5511
[08:01:57.088038] Epoch: [14]  [340/781]  eta: 0:01:27  lr: 0.000244  training_loss: 0.4698 (0.4713)  mae_loss: 0.2941 (0.2897)  classification_loss: 0.1794 (0.1815)  time: 0.1978  data: 0.0003  max mem: 5511
[08:02:00.986718] Epoch: [14]  [360/781]  eta: 0:01:23  lr: 0.000244  training_loss: 0.4686 (0.4717)  mae_loss: 0.2873 (0.2899)  classification_loss: 0.1848 (0.1818)  time: 0.1949  data: 0.0003  max mem: 5511
[08:02:04.889143] Epoch: [14]  [380/781]  eta: 0:01:19  lr: 0.000244  training_loss: 0.4588 (0.4717)  mae_loss: 0.2788 (0.2901)  classification_loss: 0.1810 (0.1817)  time: 0.1950  data: 0.0002  max mem: 5511
[08:02:08.781614] Epoch: [14]  [400/781]  eta: 0:01:15  lr: 0.000244  training_loss: 0.4708 (0.4718)  mae_loss: 0.2862 (0.2901)  classification_loss: 0.1803 (0.1817)  time: 0.1946  data: 0.0002  max mem: 5511
[08:02:12.693330] Epoch: [14]  [420/781]  eta: 0:01:11  lr: 0.000244  training_loss: 0.4748 (0.4721)  mae_loss: 0.3007 (0.2905)  classification_loss: 0.1794 (0.1817)  time: 0.1955  data: 0.0002  max mem: 5511
[08:02:16.610268] Epoch: [14]  [440/781]  eta: 0:01:07  lr: 0.000244  training_loss: 0.4596 (0.4718)  mae_loss: 0.2853 (0.2903)  classification_loss: 0.1776 (0.1815)  time: 0.1958  data: 0.0002  max mem: 5511
[08:02:20.546429] Epoch: [14]  [460/781]  eta: 0:01:03  lr: 0.000244  training_loss: 0.4725 (0.4717)  mae_loss: 0.2895 (0.2902)  classification_loss: 0.1826 (0.1815)  time: 0.1967  data: 0.0002  max mem: 5511
[08:02:24.455698] Epoch: [14]  [480/781]  eta: 0:00:59  lr: 0.000244  training_loss: 0.4680 (0.4721)  mae_loss: 0.2793 (0.2905)  classification_loss: 0.1843 (0.1816)  time: 0.1954  data: 0.0002  max mem: 5511
[08:02:28.383585] Epoch: [14]  [500/781]  eta: 0:00:55  lr: 0.000244  training_loss: 0.4590 (0.4716)  mae_loss: 0.2863 (0.2901)  classification_loss: 0.1792 (0.1816)  time: 0.1963  data: 0.0002  max mem: 5511
[08:02:32.301820] Epoch: [14]  [520/781]  eta: 0:00:51  lr: 0.000244  training_loss: 0.4758 (0.4723)  mae_loss: 0.3003 (0.2908)  classification_loss: 0.1768 (0.1815)  time: 0.1958  data: 0.0006  max mem: 5511
[08:02:36.233095] Epoch: [14]  [540/781]  eta: 0:00:47  lr: 0.000244  training_loss: 0.4707 (0.4727)  mae_loss: 0.2849 (0.2911)  classification_loss: 0.1795 (0.1816)  time: 0.1965  data: 0.0002  max mem: 5511
[08:02:40.179145] Epoch: [14]  [560/781]  eta: 0:00:43  lr: 0.000244  training_loss: 0.4523 (0.4720)  mae_loss: 0.2678 (0.2904)  classification_loss: 0.1814 (0.1815)  time: 0.1972  data: 0.0002  max mem: 5511
[08:02:44.079129] Epoch: [14]  [580/781]  eta: 0:00:39  lr: 0.000244  training_loss: 0.4682 (0.4720)  mae_loss: 0.2882 (0.2904)  classification_loss: 0.1829 (0.1816)  time: 0.1949  data: 0.0003  max mem: 5511
[08:02:47.980687] Epoch: [14]  [600/781]  eta: 0:00:35  lr: 0.000244  training_loss: 0.4874 (0.4723)  mae_loss: 0.3005 (0.2909)  classification_loss: 0.1753 (0.1814)  time: 0.1950  data: 0.0002  max mem: 5511
[08:02:51.885493] Epoch: [14]  [620/781]  eta: 0:00:31  lr: 0.000244  training_loss: 0.4741 (0.4725)  mae_loss: 0.2935 (0.2911)  classification_loss: 0.1789 (0.1814)  time: 0.1952  data: 0.0003  max mem: 5511
[08:02:55.794691] Epoch: [14]  [640/781]  eta: 0:00:27  lr: 0.000243  training_loss: 0.4594 (0.4720)  mae_loss: 0.2738 (0.2908)  classification_loss: 0.1757 (0.1813)  time: 0.1954  data: 0.0002  max mem: 5511
[08:02:59.701379] Epoch: [14]  [660/781]  eta: 0:00:23  lr: 0.000243  training_loss: 0.4546 (0.4718)  mae_loss: 0.2772 (0.2905)  classification_loss: 0.1830 (0.1813)  time: 0.1953  data: 0.0002  max mem: 5511
[08:03:03.630622] Epoch: [14]  [680/781]  eta: 0:00:19  lr: 0.000243  training_loss: 0.4582 (0.4715)  mae_loss: 0.2842 (0.2903)  classification_loss: 0.1755 (0.1812)  time: 0.1964  data: 0.0002  max mem: 5511
[08:03:07.541760] Epoch: [14]  [700/781]  eta: 0:00:15  lr: 0.000243  training_loss: 0.4575 (0.4713)  mae_loss: 0.2834 (0.2902)  classification_loss: 0.1776 (0.1811)  time: 0.1955  data: 0.0003  max mem: 5511
[08:03:11.457364] Epoch: [14]  [720/781]  eta: 0:00:12  lr: 0.000243  training_loss: 0.4575 (0.4711)  mae_loss: 0.2687 (0.2899)  classification_loss: 0.1843 (0.1812)  time: 0.1957  data: 0.0003  max mem: 5511
[08:03:15.365608] Epoch: [14]  [740/781]  eta: 0:00:08  lr: 0.000243  training_loss: 0.4509 (0.4708)  mae_loss: 0.2663 (0.2896)  classification_loss: 0.1805 (0.1812)  time: 0.1953  data: 0.0002  max mem: 5511
[08:03:19.280012] Epoch: [14]  [760/781]  eta: 0:00:04  lr: 0.000243  training_loss: 0.4654 (0.4708)  mae_loss: 0.2814 (0.2896)  classification_loss: 0.1823 (0.1813)  time: 0.1956  data: 0.0002  max mem: 5511
[08:03:23.181638] Epoch: [14]  [780/781]  eta: 0:00:00  lr: 0.000243  training_loss: 0.4584 (0.4706)  mae_loss: 0.2815 (0.2894)  classification_loss: 0.1761 (0.1812)  time: 0.1950  data: 0.0002  max mem: 5511
[08:03:23.334721] Epoch: [14] Total time: 0:02:33 (0.1969 s / it)
[08:03:23.335193] Averaged stats: lr: 0.000243  training_loss: 0.4584 (0.4706)  mae_loss: 0.2815 (0.2894)  classification_loss: 0.1761 (0.1812)
[08:03:23.908325] Test:  [  0/157]  eta: 0:01:29  testing_loss: 1.1145 (1.1145)  acc1: 67.1875 (67.1875)  acc5: 96.8750 (96.8750)  time: 0.5682  data: 0.5376  max mem: 5511
[08:03:24.191967] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 1.2431 (1.2430)  acc1: 57.8125 (57.9545)  acc5: 96.8750 (96.3068)  time: 0.0772  data: 0.0490  max mem: 5511
[08:03:24.478043] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.1968 (1.1984)  acc1: 57.8125 (60.0446)  acc5: 95.3125 (96.0565)  time: 0.0282  data: 0.0002  max mem: 5511
[08:03:24.761316] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 1.1929 (1.2074)  acc1: 59.3750 (59.3246)  acc5: 95.3125 (95.8165)  time: 0.0283  data: 0.0002  max mem: 5511
[08:03:25.045268] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 1.1550 (1.1982)  acc1: 59.3750 (59.3750)  acc5: 95.3125 (95.9985)  time: 0.0282  data: 0.0002  max mem: 5511
[08:03:25.331062] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.1677 (1.1960)  acc1: 59.3750 (59.9877)  acc5: 96.8750 (96.0478)  time: 0.0284  data: 0.0002  max mem: 5511
[08:03:25.618737] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.1727 (1.1910)  acc1: 60.9375 (60.0922)  acc5: 95.3125 (95.7992)  time: 0.0286  data: 0.0003  max mem: 5511
[08:03:25.903671] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.1580 (1.1870)  acc1: 59.3750 (60.0132)  acc5: 95.3125 (95.8847)  time: 0.0285  data: 0.0003  max mem: 5511
[08:03:26.187046] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.1666 (1.1872)  acc1: 57.8125 (59.7994)  acc5: 95.3125 (95.7948)  time: 0.0283  data: 0.0002  max mem: 5511
[08:03:26.471176] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.1807 (1.1868)  acc1: 60.9375 (60.0446)  acc5: 95.3125 (95.8791)  time: 0.0283  data: 0.0002  max mem: 5511
[08:03:26.757397] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.1859 (1.1881)  acc1: 60.9375 (59.9938)  acc5: 95.3125 (95.9313)  time: 0.0284  data: 0.0002  max mem: 5511
[08:03:27.048086] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.2291 (1.1895)  acc1: 59.3750 (59.9944)  acc5: 95.3125 (95.9459)  time: 0.0287  data: 0.0001  max mem: 5511
[08:03:27.332271] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.2247 (1.1895)  acc1: 60.9375 (60.0852)  acc5: 95.3125 (95.9194)  time: 0.0286  data: 0.0002  max mem: 5511
[08:03:27.615178] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.2183 (1.1927)  acc1: 60.9375 (60.1503)  acc5: 95.3125 (95.8015)  time: 0.0282  data: 0.0002  max mem: 5511
[08:03:27.910163] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.1960 (1.1915)  acc1: 60.9375 (60.3834)  acc5: 95.3125 (95.7890)  time: 0.0287  data: 0.0004  max mem: 5511
[08:03:28.192582] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.1665 (1.1911)  acc1: 60.9375 (60.4719)  acc5: 93.7500 (95.6747)  time: 0.0287  data: 0.0004  max mem: 5511
[08:03:28.343694] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.1556 (1.1919)  acc1: 60.9375 (60.4200)  acc5: 95.3125 (95.7000)  time: 0.0271  data: 0.0001  max mem: 5511
[08:03:28.494541] Test: Total time: 0:00:05 (0.0328 s / it)
[08:03:28.495117] * Acc@1 60.420 Acc@5 95.700 loss 1.192
[08:03:28.495404] Accuracy of the network on the 10000 test images: 60.4%
[08:03:28.495605] Max accuracy: 60.42%
[08:03:28.712587] log_dir: ./output_dir
[08:03:29.648220] Epoch: [15]  [  0/781]  eta: 0:12:08  lr: 0.000243  training_loss: 0.4275 (0.4275)  mae_loss: 0.2363 (0.2363)  classification_loss: 0.1912 (0.1912)  time: 0.9334  data: 0.7107  max mem: 5511
[08:03:33.551763] Epoch: [15]  [ 20/781]  eta: 0:02:55  lr: 0.000243  training_loss: 0.4733 (0.4751)  mae_loss: 0.2994 (0.2950)  classification_loss: 0.1755 (0.1801)  time: 0.1951  data: 0.0002  max mem: 5511
[08:03:37.462372] Epoch: [15]  [ 40/781]  eta: 0:02:38  lr: 0.000243  training_loss: 0.4825 (0.4770)  mae_loss: 0.2943 (0.2964)  classification_loss: 0.1785 (0.1806)  time: 0.1955  data: 0.0002  max mem: 5511
[08:03:41.362819] Epoch: [15]  [ 60/781]  eta: 0:02:29  lr: 0.000243  training_loss: 0.4612 (0.4736)  mae_loss: 0.2800 (0.2935)  classification_loss: 0.1788 (0.1801)  time: 0.1950  data: 0.0002  max mem: 5511
[08:03:45.255584] Epoch: [15]  [ 80/781]  eta: 0:02:23  lr: 0.000243  training_loss: 0.4612 (0.4720)  mae_loss: 0.2810 (0.2917)  classification_loss: 0.1806 (0.1803)  time: 0.1946  data: 0.0002  max mem: 5511
[08:03:49.137377] Epoch: [15]  [100/781]  eta: 0:02:17  lr: 0.000243  training_loss: 0.4643 (0.4704)  mae_loss: 0.2818 (0.2902)  classification_loss: 0.1795 (0.1802)  time: 0.1940  data: 0.0003  max mem: 5511
[08:03:53.038652] Epoch: [15]  [120/781]  eta: 0:02:12  lr: 0.000243  training_loss: 0.4386 (0.4668)  mae_loss: 0.2608 (0.2866)  classification_loss: 0.1810 (0.1803)  time: 0.1950  data: 0.0003  max mem: 5511
[08:03:56.936430] Epoch: [15]  [140/781]  eta: 0:02:08  lr: 0.000243  training_loss: 0.4496 (0.4656)  mae_loss: 0.2746 (0.2857)  classification_loss: 0.1759 (0.1800)  time: 0.1948  data: 0.0002  max mem: 5511
[08:04:00.873252] Epoch: [15]  [160/781]  eta: 0:02:03  lr: 0.000243  training_loss: 0.4738 (0.4668)  mae_loss: 0.2960 (0.2870)  classification_loss: 0.1765 (0.1798)  time: 0.1968  data: 0.0002  max mem: 5511
[08:04:04.784181] Epoch: [15]  [180/781]  eta: 0:01:59  lr: 0.000243  training_loss: 0.4604 (0.4662)  mae_loss: 0.2828 (0.2864)  classification_loss: 0.1781 (0.1798)  time: 0.1955  data: 0.0003  max mem: 5511
[08:04:08.677341] Epoch: [15]  [200/781]  eta: 0:01:55  lr: 0.000243  training_loss: 0.4566 (0.4657)  mae_loss: 0.2799 (0.2861)  classification_loss: 0.1783 (0.1796)  time: 0.1946  data: 0.0003  max mem: 5511
[08:04:12.587203] Epoch: [15]  [220/781]  eta: 0:01:51  lr: 0.000243  training_loss: 0.4590 (0.4652)  mae_loss: 0.2797 (0.2854)  classification_loss: 0.1826 (0.1798)  time: 0.1954  data: 0.0003  max mem: 5511
[08:04:16.532347] Epoch: [15]  [240/781]  eta: 0:01:47  lr: 0.000243  training_loss: 0.4600 (0.4656)  mae_loss: 0.2780 (0.2857)  classification_loss: 0.1789 (0.1798)  time: 0.1972  data: 0.0003  max mem: 5511
[08:04:20.427557] Epoch: [15]  [260/781]  eta: 0:01:43  lr: 0.000243  training_loss: 0.4824 (0.4664)  mae_loss: 0.3014 (0.2866)  classification_loss: 0.1769 (0.1797)  time: 0.1947  data: 0.0002  max mem: 5511
[08:04:24.340171] Epoch: [15]  [280/781]  eta: 0:01:39  lr: 0.000243  training_loss: 0.4705 (0.4667)  mae_loss: 0.2890 (0.2870)  classification_loss: 0.1820 (0.1798)  time: 0.1955  data: 0.0004  max mem: 5511
[08:04:28.237821] Epoch: [15]  [300/781]  eta: 0:01:35  lr: 0.000243  training_loss: 0.4615 (0.4668)  mae_loss: 0.2794 (0.2870)  classification_loss: 0.1791 (0.1798)  time: 0.1948  data: 0.0002  max mem: 5511
[08:04:32.143167] Epoch: [15]  [320/781]  eta: 0:01:31  lr: 0.000243  training_loss: 0.4650 (0.4664)  mae_loss: 0.2820 (0.2865)  classification_loss: 0.1826 (0.1799)  time: 0.1952  data: 0.0002  max mem: 5511
[08:04:36.037102] Epoch: [15]  [340/781]  eta: 0:01:27  lr: 0.000243  training_loss: 0.4603 (0.4663)  mae_loss: 0.2831 (0.2865)  classification_loss: 0.1793 (0.1799)  time: 0.1946  data: 0.0002  max mem: 5511

[08:04:39.966503] Epoch: [15]  [360/781]  eta: 0:01:23  lr: 0.000243  training_loss: 0.4740 (0.4665)  mae_loss: 0.2860 (0.2864)  classification_loss: 0.1834 (0.1801)  time: 0.1963  data: 0.0003  max mem: 5511
[08:04:43.895745] Epoch: [15]  [380/781]  eta: 0:01:19  lr: 0.000243  training_loss: 0.4640 (0.4669)  mae_loss: 0.2921 (0.2869)  classification_loss: 0.1781 (0.1800)  time: 0.1964  data: 0.0002  max mem: 5511
[08:04:47.841574] Epoch: [15]  [400/781]  eta: 0:01:15  lr: 0.000243  training_loss: 0.4609 (0.4666)  mae_loss: 0.2777 (0.2865)  classification_loss: 0.1806 (0.1801)  time: 0.1972  data: 0.0002  max mem: 5511
[08:04:51.738191] Epoch: [15]  [420/781]  eta: 0:01:11  lr: 0.000243  training_loss: 0.4735 (0.4666)  mae_loss: 0.2934 (0.2867)  classification_loss: 0.1765 (0.1799)  time: 0.1947  data: 0.0003  max mem: 5511
[08:04:55.654757] Epoch: [15]  [440/781]  eta: 0:01:07  lr: 0.000242  training_loss: 0.4637 (0.4668)  mae_loss: 0.2897 (0.2869)  classification_loss: 0.1774 (0.1799)  time: 0.1957  data: 0.0002  max mem: 5511
[08:04:59.571995] Epoch: [15]  [460/781]  eta: 0:01:03  lr: 0.000242  training_loss: 0.4569 (0.4663)  mae_loss: 0.2856 (0.2866)  classification_loss: 0.1760 (0.1798)  time: 0.1958  data: 0.0002  max mem: 5511
[08:05:03.497583] Epoch: [15]  [480/781]  eta: 0:00:59  lr: 0.000242  training_loss: 0.4651 (0.4661)  mae_loss: 0.2823 (0.2864)  classification_loss: 0.1824 (0.1797)  time: 0.1962  data: 0.0003  max mem: 5511
[08:05:07.452158] Epoch: [15]  [500/781]  eta: 0:00:55  lr: 0.000242  training_loss: 0.4499 (0.4658)  mae_loss: 0.2727 (0.2861)  classification_loss: 0.1796 (0.1797)  time: 0.1976  data: 0.0003  max mem: 5511
[08:05:11.379118] Epoch: [15]  [520/781]  eta: 0:00:51  lr: 0.000242  training_loss: 0.4590 (0.4654)  mae_loss: 0.2791 (0.2858)  classification_loss: 0.1759 (0.1796)  time: 0.1962  data: 0.0002  max mem: 5511
[08:05:15.331206] Epoch: [15]  [540/781]  eta: 0:00:47  lr: 0.000242  training_loss: 0.4626 (0.4653)  mae_loss: 0.2812 (0.2856)  classification_loss: 0.1799 (0.1797)  time: 0.1975  data: 0.0004  max mem: 5511
[08:05:19.228158] Epoch: [15]  [560/781]  eta: 0:00:43  lr: 0.000242  training_loss: 0.4696 (0.4653)  mae_loss: 0.2948 (0.2856)  classification_loss: 0.1802 (0.1797)  time: 0.1948  data: 0.0002  max mem: 5511
[08:05:23.131659] Epoch: [15]  [580/781]  eta: 0:00:39  lr: 0.000242  training_loss: 0.4565 (0.4650)  mae_loss: 0.2795 (0.2854)  classification_loss: 0.1754 (0.1795)  time: 0.1951  data: 0.0002  max mem: 5511
[08:05:27.026453] Epoch: [15]  [600/781]  eta: 0:00:35  lr: 0.000242  training_loss: 0.4456 (0.4645)  mae_loss: 0.2682 (0.2851)  classification_loss: 0.1776 (0.1795)  time: 0.1946  data: 0.0002  max mem: 5511
[08:05:30.940279] Epoch: [15]  [620/781]  eta: 0:00:31  lr: 0.000242  training_loss: 0.4583 (0.4644)  mae_loss: 0.2806 (0.2850)  classification_loss: 0.1757 (0.1794)  time: 0.1956  data: 0.0002  max mem: 5511
[08:05:34.844208] Epoch: [15]  [640/781]  eta: 0:00:27  lr: 0.000242  training_loss: 0.4557 (0.4645)  mae_loss: 0.2797 (0.2851)  classification_loss: 0.1794 (0.1794)  time: 0.1951  data: 0.0003  max mem: 5511
[08:05:38.759905] Epoch: [15]  [660/781]  eta: 0:00:23  lr: 0.000242  training_loss: 0.4552 (0.4643)  mae_loss: 0.2747 (0.2849)  classification_loss: 0.1800 (0.1794)  time: 0.1957  data: 0.0002  max mem: 5511
[08:05:42.680303] Epoch: [15]  [680/781]  eta: 0:00:19  lr: 0.000242  training_loss: 0.4459 (0.4641)  mae_loss: 0.2717 (0.2847)  classification_loss: 0.1796 (0.1794)  time: 0.1959  data: 0.0003  max mem: 5511
[08:05:46.600014] Epoch: [15]  [700/781]  eta: 0:00:15  lr: 0.000242  training_loss: 0.4478 (0.4637)  mae_loss: 0.2783 (0.2844)  classification_loss: 0.1759 (0.1793)  time: 0.1959  data: 0.0002  max mem: 5511
[08:05:50.580548] Epoch: [15]  [720/781]  eta: 0:00:11  lr: 0.000242  training_loss: 0.4666 (0.4636)  mae_loss: 0.2779 (0.2843)  classification_loss: 0.1768 (0.1793)  time: 0.1990  data: 0.0002  max mem: 5511
[08:05:54.498667] Epoch: [15]  [740/781]  eta: 0:00:08  lr: 0.000242  training_loss: 0.4579 (0.4636)  mae_loss: 0.2817 (0.2844)  classification_loss: 0.1752 (0.1792)  time: 0.1958  data: 0.0003  max mem: 5511
[08:05:58.405679] Epoch: [15]  [760/781]  eta: 0:00:04  lr: 0.000242  training_loss: 0.4424 (0.4632)  mae_loss: 0.2690 (0.2840)  classification_loss: 0.1756 (0.1792)  time: 0.1953  data: 0.0002  max mem: 5511
[08:06:02.333431] Epoch: [15]  [780/781]  eta: 0:00:00  lr: 0.000242  training_loss: 0.4343 (0.4629)  mae_loss: 0.2612 (0.2837)  classification_loss: 0.1799 (0.1792)  time: 0.1963  data: 0.0002  max mem: 5511
[08:06:02.507342] Epoch: [15] Total time: 0:02:33 (0.1969 s / it)
[08:06:02.507830] Averaged stats: lr: 0.000242  training_loss: 0.4343 (0.4629)  mae_loss: 0.2612 (0.2837)  classification_loss: 0.1799 (0.1792)
[08:06:03.204980] Test:  [  0/157]  eta: 0:01:48  testing_loss: 1.0193 (1.0193)  acc1: 64.0625 (64.0625)  acc5: 96.8750 (96.8750)  time: 0.6932  data: 0.6619  max mem: 5511
[08:06:03.486643] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 1.1652 (1.1885)  acc1: 62.5000 (60.2273)  acc5: 95.3125 (95.0284)  time: 0.0885  data: 0.0603  max mem: 5511
[08:06:03.769049] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.1136 (1.1452)  acc1: 62.5000 (60.7143)  acc5: 95.3125 (96.0565)  time: 0.0281  data: 0.0002  max mem: 5511
[08:06:04.050428] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.1441 (1.1563)  acc1: 60.9375 (59.7278)  acc5: 96.8750 (96.1190)  time: 0.0281  data: 0.0002  max mem: 5511
[08:06:04.332442] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.1547 (1.1512)  acc1: 60.9375 (60.1372)  acc5: 95.3125 (96.1509)  time: 0.0281  data: 0.0002  max mem: 5511
[08:06:04.613431] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.1101 (1.1499)  acc1: 60.9375 (60.3248)  acc5: 96.8750 (96.3235)  time: 0.0280  data: 0.0002  max mem: 5511
[08:06:04.893951] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.1443 (1.1486)  acc1: 59.3750 (60.0922)  acc5: 96.8750 (96.1578)  time: 0.0280  data: 0.0002  max mem: 5511
[08:06:05.174525] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.1158 (1.1444)  acc1: 60.9375 (60.4313)  acc5: 96.8750 (96.2148)  time: 0.0280  data: 0.0002  max mem: 5511
[08:06:05.455271] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.1235 (1.1428)  acc1: 62.5000 (60.4167)  acc5: 96.8750 (96.1998)  time: 0.0280  data: 0.0002  max mem: 5511
[08:06:05.737103] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.1334 (1.1433)  acc1: 60.9375 (60.3194)  acc5: 96.8750 (96.3427)  time: 0.0280  data: 0.0001  max mem: 5511
[08:06:06.018227] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.1690 (1.1476)  acc1: 56.2500 (60.0557)  acc5: 96.8750 (96.4109)  time: 0.0280  data: 0.0002  max mem: 5511
[08:06:06.299633] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.1968 (1.1495)  acc1: 56.2500 (60.0788)  acc5: 95.3125 (96.3542)  time: 0.0280  data: 0.0002  max mem: 5511
[08:06:06.581961] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.1434 (1.1481)  acc1: 59.3750 (60.1498)  acc5: 96.8750 (96.3843)  time: 0.0281  data: 0.0002  max mem: 5511
[08:06:06.864168] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.1434 (1.1504)  acc1: 60.9375 (60.0906)  acc5: 96.8750 (96.3621)  time: 0.0281  data: 0.0002  max mem: 5511
[08:06:07.151947] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.1498 (1.1489)  acc1: 60.9375 (60.3723)  acc5: 95.3125 (96.3209)  time: 0.0284  data: 0.0002  max mem: 5511
[08:06:07.430751] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.1064 (1.1483)  acc1: 60.9375 (60.3580)  acc5: 95.3125 (96.3059)  time: 0.0282  data: 0.0001  max mem: 5511
[08:06:07.581912] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.1034 (1.1500)  acc1: 59.3750 (60.2800)  acc5: 95.3125 (96.2900)  time: 0.0270  data: 0.0001  max mem: 5511
[08:06:07.733656] Test: Total time: 0:00:05 (0.0333 s / it)
[08:06:07.734312] * Acc@1 60.280 Acc@5 96.290 loss 1.150
[08:06:07.734623] Accuracy of the network on the 10000 test images: 60.3%
[08:06:07.734814] Max accuracy: 60.42%
[08:06:08.009098] log_dir: ./output_dir
[08:06:08.936317] Epoch: [16]  [  0/781]  eta: 0:12:02  lr: 0.000242  training_loss: 0.4546 (0.4546)  mae_loss: 0.2786 (0.2786)  classification_loss: 0.1760 (0.1760)  time: 0.9255  data: 0.6810  max mem: 5511
[08:06:12.847510] Epoch: [16]  [ 20/781]  eta: 0:02:55  lr: 0.000242  training_loss: 0.4595 (0.4557)  mae_loss: 0.2765 (0.2781)  classification_loss: 0.1750 (0.1776)  time: 0.1955  data: 0.0004  max mem: 5511
[08:06:16.744113] Epoch: [16]  [ 40/781]  eta: 0:02:37  lr: 0.000242  training_loss: 0.4573 (0.4565)  mae_loss: 0.2814 (0.2776)  classification_loss: 0.1762 (0.1789)  time: 0.1947  data: 0.0003  max mem: 5511
[08:06:20.652875] Epoch: [16]  [ 60/781]  eta: 0:02:29  lr: 0.000242  training_loss: 0.4514 (0.4576)  mae_loss: 0.2705 (0.2791)  classification_loss: 0.1759 (0.1786)  time: 0.1953  data: 0.0003  max mem: 5511
[08:06:24.557546] Epoch: [16]  [ 80/781]  eta: 0:02:23  lr: 0.000242  training_loss: 0.4581 (0.4573)  mae_loss: 0.2809 (0.2786)  classification_loss: 0.1796 (0.1787)  time: 0.1951  data: 0.0004  max mem: 5511
[08:06:28.465849] Epoch: [16]  [100/781]  eta: 0:02:17  lr: 0.000242  training_loss: 0.4414 (0.4544)  mae_loss: 0.2603 (0.2758)  classification_loss: 0.1782 (0.1786)  time: 0.1953  data: 0.0004  max mem: 5511
[08:06:32.372822] Epoch: [16]  [120/781]  eta: 0:02:13  lr: 0.000242  training_loss: 0.4564 (0.4546)  mae_loss: 0.2814 (0.2765)  classification_loss: 0.1745 (0.1781)  time: 0.1953  data: 0.0002  max mem: 5511
[08:06:36.273660] Epoch: [16]  [140/781]  eta: 0:02:08  lr: 0.000242  training_loss: 0.4693 (0.4570)  mae_loss: 0.2964 (0.2795)  classification_loss: 0.1748 (0.1775)  time: 0.1949  data: 0.0002  max mem: 5511
[08:06:40.175585] Epoch: [16]  [160/781]  eta: 0:02:03  lr: 0.000242  training_loss: 0.4382 (0.4555)  mae_loss: 0.2540 (0.2781)  classification_loss: 0.1759 (0.1774)  time: 0.1950  data: 0.0003  max mem: 5511
[08:06:44.100761] Epoch: [16]  [180/781]  eta: 0:01:59  lr: 0.000242  training_loss: 0.4498 (0.4562)  mae_loss: 0.2875 (0.2789)  classification_loss: 0.1762 (0.1773)  time: 0.1961  data: 0.0002  max mem: 5511
[08:06:48.039804] Epoch: [16]  [200/781]  eta: 0:01:55  lr: 0.000241  training_loss: 0.4595 (0.4572)  mae_loss: 0.2800 (0.2798)  classification_loss: 0.1761 (0.1774)  time: 0.1968  data: 0.0002  max mem: 5511
[08:06:51.990076] Epoch: [16]  [220/781]  eta: 0:01:51  lr: 0.000241  training_loss: 0.4671 (0.4576)  mae_loss: 0.2837 (0.2801)  classification_loss: 0.1790 (0.1775)  time: 0.1974  data: 0.0002  max mem: 5511
[08:06:55.905723] Epoch: [16]  [240/781]  eta: 0:01:47  lr: 0.000241  training_loss: 0.4431 (0.4569)  mae_loss: 0.2655 (0.2796)  classification_loss: 0.1771 (0.1773)  time: 0.1957  data: 0.0004  max mem: 5511
[08:06:59.826172] Epoch: [16]  [260/781]  eta: 0:01:43  lr: 0.000241  training_loss: 0.4410 (0.4559)  mae_loss: 0.2624 (0.2787)  classification_loss: 0.1751 (0.1773)  time: 0.1959  data: 0.0002  max mem: 5511
[08:07:03.727496] Epoch: [16]  [280/781]  eta: 0:01:39  lr: 0.000241  training_loss: 0.4591 (0.4566)  mae_loss: 0.2791 (0.2792)  classification_loss: 0.1777 (0.1774)  time: 0.1949  data: 0.0002  max mem: 5511
[08:07:07.653306] Epoch: [16]  [300/781]  eta: 0:01:35  lr: 0.000241  training_loss: 0.4462 (0.4562)  mae_loss: 0.2714 (0.2788)  classification_loss: 0.1746 (0.1773)  time: 0.1962  data: 0.0002  max mem: 5511
[08:07:11.564889] Epoch: [16]  [320/781]  eta: 0:01:31  lr: 0.000241  training_loss: 0.4540 (0.4557)  mae_loss: 0.2725 (0.2783)  classification_loss: 0.1787 (0.1774)  time: 0.1955  data: 0.0002  max mem: 5511
[08:07:15.464727] Epoch: [16]  [340/781]  eta: 0:01:27  lr: 0.000241  training_loss: 0.4497 (0.4561)  mae_loss: 0.2876 (0.2790)  classification_loss: 0.1682 (0.1771)  time: 0.1949  data: 0.0002  max mem: 5511
[08:07:19.360817] Epoch: [16]  [360/781]  eta: 0:01:23  lr: 0.000241  training_loss: 0.4527 (0.4559)  mae_loss: 0.2733 (0.2787)  classification_loss: 0.1786 (0.1771)  time: 0.1947  data: 0.0002  max mem: 5511
[08:07:23.300878] Epoch: [16]  [380/781]  eta: 0:01:19  lr: 0.000241  training_loss: 0.4811 (0.4567)  mae_loss: 0.2972 (0.2797)  classification_loss: 0.1759 (0.1770)  time: 0.1969  data: 0.0002  max mem: 5511
[08:07:27.199599] Epoch: [16]  [400/781]  eta: 0:01:15  lr: 0.000241  training_loss: 0.4541 (0.4566)  mae_loss: 0.2804 (0.2797)  classification_loss: 0.1730 (0.1768)  time: 0.1949  data: 0.0002  max mem: 5511
[08:07:31.105128] Epoch: [16]  [420/781]  eta: 0:01:11  lr: 0.000241  training_loss: 0.4489 (0.4566)  mae_loss: 0.2716 (0.2797)  classification_loss: 0.1761 (0.1769)  time: 0.1952  data: 0.0002  max mem: 5511
[08:07:35.048608] Epoch: [16]  [440/781]  eta: 0:01:07  lr: 0.000241  training_loss: 0.4448 (0.4562)  mae_loss: 0.2693 (0.2794)  classification_loss: 0.1743 (0.1768)  time: 0.1971  data: 0.0003  max mem: 5511
[08:07:38.964561] Epoch: [16]  [460/781]  eta: 0:01:03  lr: 0.000241  training_loss: 0.4482 (0.4556)  mae_loss: 0.2709 (0.2790)  classification_loss: 0.1752 (0.1767)  time: 0.1957  data: 0.0002  max mem: 5511
[08:07:42.936601] Epoch: [16]  [480/781]  eta: 0:00:59  lr: 0.000241  training_loss: 0.4454 (0.4556)  mae_loss: 0.2689 (0.2789)  classification_loss: 0.1777 (0.1767)  time: 0.1985  data: 0.0002  max mem: 5511
[08:07:46.879925] Epoch: [16]  [500/781]  eta: 0:00:55  lr: 0.000241  training_loss: 0.4574 (0.4562)  mae_loss: 0.2803 (0.2795)  classification_loss: 0.1756 (0.1767)  time: 0.1971  data: 0.0003  max mem: 5511
[08:07:50.821329] Epoch: [16]  [520/781]  eta: 0:00:51  lr: 0.000241  training_loss: 0.4587 (0.4566)  mae_loss: 0.2867 (0.2798)  classification_loss: 0.1770 (0.1768)  time: 0.1970  data: 0.0002  max mem: 5511
[08:07:54.743016] Epoch: [16]  [540/781]  eta: 0:00:47  lr: 0.000241  training_loss: 0.4520 (0.4564)  mae_loss: 0.2716 (0.2797)  classification_loss: 0.1742 (0.1767)  time: 0.1960  data: 0.0002  max mem: 5511
[08:07:58.649886] Epoch: [16]  [560/781]  eta: 0:00:43  lr: 0.000241  training_loss: 0.4512 (0.4564)  mae_loss: 0.2710 (0.2798)  classification_loss: 0.1727 (0.1767)  time: 0.1952  data: 0.0002  max mem: 5511
[08:08:02.586145] Epoch: [16]  [580/781]  eta: 0:00:39  lr: 0.000241  training_loss: 0.4362 (0.4559)  mae_loss: 0.2590 (0.2792)  classification_loss: 0.1767 (0.1767)  time: 0.1967  data: 0.0003  max mem: 5511
[08:08:06.486134] Epoch: [16]  [600/781]  eta: 0:00:35  lr: 0.000241  training_loss: 0.4584 (0.4559)  mae_loss: 0.2846 (0.2793)  classification_loss: 0.1758 (0.1767)  time: 0.1949  data: 0.0003  max mem: 5511
[08:08:10.388267] Epoch: [16]  [620/781]  eta: 0:00:31  lr: 0.000241  training_loss: 0.4664 (0.4560)  mae_loss: 0.2921 (0.2795)  classification_loss: 0.1743 (0.1766)  time: 0.1950  data: 0.0002  max mem: 5511
[08:08:14.287253] Epoch: [16]  [640/781]  eta: 0:00:27  lr: 0.000241  training_loss: 0.4440 (0.4557)  mae_loss: 0.2737 (0.2792)  classification_loss: 0.1728 (0.1766)  time: 0.1949  data: 0.0002  max mem: 5511
[08:08:18.186007] Epoch: [16]  [660/781]  eta: 0:00:23  lr: 0.000241  training_loss: 0.4603 (0.4559)  mae_loss: 0.2811 (0.2793)  classification_loss: 0.1782 (0.1766)  time: 0.1949  data: 0.0002  max mem: 5511
[08:08:22.087580] Epoch: [16]  [680/781]  eta: 0:00:19  lr: 0.000241  training_loss: 0.4653 (0.4560)  mae_loss: 0.2903 (0.2795)  classification_loss: 0.1743 (0.1765)  time: 0.1950  data: 0.0002  max mem: 5511
[08:08:26.011377] Epoch: [16]  [700/781]  eta: 0:00:15  lr: 0.000240  training_loss: 0.4476 (0.4560)  mae_loss: 0.2770 (0.2795)  classification_loss: 0.1717 (0.1765)  time: 0.1961  data: 0.0002  max mem: 5511
[08:08:29.921687] Epoch: [16]  [720/781]  eta: 0:00:12  lr: 0.000240  training_loss: 0.4493 (0.4559)  mae_loss: 0.2817 (0.2795)  classification_loss: 0.1727 (0.1764)  time: 0.1954  data: 0.0005  max mem: 5511
[08:08:33.857762] Epoch: [16]  [740/781]  eta: 0:00:08  lr: 0.000240  training_loss: 0.4372 (0.4557)  mae_loss: 0.2654 (0.2794)  classification_loss: 0.1722 (0.1764)  time: 0.1967  data: 0.0003  max mem: 5511
[08:08:37.764205] Epoch: [16]  [760/781]  eta: 0:00:04  lr: 0.000240  training_loss: 0.4627 (0.4559)  mae_loss: 0.2828 (0.2796)  classification_loss: 0.1768 (0.1764)  time: 0.1952  data: 0.0002  max mem: 5511
[08:08:41.689491] Epoch: [16]  [780/781]  eta: 0:00:00  lr: 0.000240  training_loss: 0.4428 (0.4558)  mae_loss: 0.2660 (0.2794)  classification_loss: 0.1757 (0.1764)  time: 0.1962  data: 0.0002  max mem: 5511
[08:08:41.841813] Epoch: [16] Total time: 0:02:33 (0.1970 s / it)
[08:08:41.842299] Averaged stats: lr: 0.000240  training_loss: 0.4428 (0.4558)  mae_loss: 0.2660 (0.2794)  classification_loss: 0.1757 (0.1764)
[08:08:42.522039] Test:  [  0/157]  eta: 0:01:46  testing_loss: 1.0411 (1.0411)  acc1: 67.1875 (67.1875)  acc5: 95.3125 (95.3125)  time: 0.6759  data: 0.6466  max mem: 5511
[08:08:42.812939] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.1174 (1.1356)  acc1: 64.0625 (62.0739)  acc5: 96.8750 (97.0170)  time: 0.0877  data: 0.0591  max mem: 5511
[08:08:43.095459] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.1006 (1.1067)  acc1: 64.0625 (62.8720)  acc5: 96.8750 (97.0982)  time: 0.0285  data: 0.0003  max mem: 5511
[08:08:43.383797] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.1155 (1.1173)  acc1: 64.0625 (62.2984)  acc5: 96.8750 (96.8750)  time: 0.0284  data: 0.0002  max mem: 5511
[08:08:43.669839] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.1165 (1.1112)  acc1: 60.9375 (62.2332)  acc5: 96.8750 (96.8369)  time: 0.0285  data: 0.0002  max mem: 5511
[08:08:43.955280] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.0859 (1.1093)  acc1: 64.0625 (62.3468)  acc5: 96.8750 (96.6299)  time: 0.0284  data: 0.0002  max mem: 5511
[08:08:44.244617] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.0859 (1.1035)  acc1: 64.0625 (62.3975)  acc5: 96.8750 (96.4908)  time: 0.0286  data: 0.0002  max mem: 5511
[08:08:44.531482] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.0432 (1.0978)  acc1: 64.0625 (62.7201)  acc5: 96.8750 (96.5669)  time: 0.0286  data: 0.0002  max mem: 5511
[08:08:44.820300] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.0746 (1.0996)  acc1: 62.5000 (62.4421)  acc5: 96.8750 (96.5085)  time: 0.0286  data: 0.0002  max mem: 5511
[08:08:45.105286] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.1039 (1.0998)  acc1: 62.5000 (62.4313)  acc5: 96.8750 (96.5488)  time: 0.0286  data: 0.0002  max mem: 5511
[08:08:45.394115] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.1145 (1.1024)  acc1: 60.9375 (62.3762)  acc5: 96.8750 (96.6120)  time: 0.0286  data: 0.0002  max mem: 5511
[08:08:45.680270] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.1400 (1.1043)  acc1: 60.9375 (62.3592)  acc5: 96.8750 (96.5090)  time: 0.0286  data: 0.0002  max mem: 5511
[08:08:45.963335] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.0935 (1.1004)  acc1: 62.5000 (62.5129)  acc5: 96.8750 (96.5651)  time: 0.0283  data: 0.0002  max mem: 5511
[08:08:46.247704] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.0961 (1.1047)  acc1: 62.5000 (62.4523)  acc5: 95.3125 (96.3621)  time: 0.0282  data: 0.0002  max mem: 5511
[08:08:46.536155] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.1141 (1.1035)  acc1: 62.5000 (62.5776)  acc5: 95.3125 (96.4207)  time: 0.0284  data: 0.0002  max mem: 5511
[08:08:46.814500] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.0946 (1.1017)  acc1: 67.1875 (62.8415)  acc5: 95.3125 (96.3473)  time: 0.0281  data: 0.0001  max mem: 5511
[08:08:46.964747] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.0939 (1.1031)  acc1: 64.0625 (62.7800)  acc5: 95.3125 (96.3600)  time: 0.0269  data: 0.0001  max mem: 5511
[08:08:47.121688] Test: Total time: 0:00:05 (0.0336 s / it)
[08:08:47.122163] * Acc@1 62.780 Acc@5 96.360 loss 1.103
[08:08:47.122485] Accuracy of the network on the 10000 test images: 62.8%
[08:08:47.122665] Max accuracy: 62.78%
[08:08:47.386603] log_dir: ./output_dir
[08:08:48.174285] Epoch: [17]  [  0/781]  eta: 0:10:13  lr: 0.000240  training_loss: 0.3951 (0.3951)  mae_loss: 0.2287 (0.2287)  classification_loss: 0.1664 (0.1664)  time: 0.7856  data: 0.5664  max mem: 5511
[08:08:52.098327] Epoch: [17]  [ 20/781]  eta: 0:02:50  lr: 0.000240  training_loss: 0.4529 (0.4475)  mae_loss: 0.2797 (0.2721)  classification_loss: 0.1751 (0.1753)  time: 0.1961  data: 0.0002  max mem: 5511
[08:08:56.032690] Epoch: [17]  [ 40/781]  eta: 0:02:36  lr: 0.000240  training_loss: 0.4587 (0.4552)  mae_loss: 0.2847 (0.2792)  classification_loss: 0.1784 (0.1761)  time: 0.1966  data: 0.0002  max mem: 5511
[08:08:59.940177] Epoch: [17]  [ 60/781]  eta: 0:02:28  lr: 0.000240  training_loss: 0.4398 (0.4539)  mae_loss: 0.2619 (0.2776)  classification_loss: 0.1773 (0.1762)  time: 0.1953  data: 0.0003  max mem: 5511
[08:09:03.833572] Epoch: [17]  [ 80/781]  eta: 0:02:22  lr: 0.000240  training_loss: 0.4364 (0.4517)  mae_loss: 0.2614 (0.2758)  classification_loss: 0.1729 (0.1759)  time: 0.1946  data: 0.0003  max mem: 5511
[08:09:07.728356] Epoch: [17]  [100/781]  eta: 0:02:17  lr: 0.000240  training_loss: 0.4371 (0.4488)  mae_loss: 0.2646 (0.2734)  classification_loss: 0.1725 (0.1754)  time: 0.1947  data: 0.0002  max mem: 5511
[08:09:11.642772] Epoch: [17]  [120/781]  eta: 0:02:12  lr: 0.000240  training_loss: 0.4395 (0.4481)  mae_loss: 0.2662 (0.2729)  classification_loss: 0.1731 (0.1752)  time: 0.1956  data: 0.0003  max mem: 5511
[08:09:15.528273] Epoch: [17]  [140/781]  eta: 0:02:07  lr: 0.000240  training_loss: 0.4332 (0.4467)  mae_loss: 0.2701 (0.2721)  classification_loss: 0.1727 (0.1746)  time: 0.1942  data: 0.0002  max mem: 5511
[08:09:19.431428] Epoch: [17]  [160/781]  eta: 0:02:03  lr: 0.000240  training_loss: 0.4416 (0.4468)  mae_loss: 0.2687 (0.2722)  classification_loss: 0.1735 (0.1746)  time: 0.1951  data: 0.0002  max mem: 5511
[08:09:23.333468] Epoch: [17]  [180/781]  eta: 0:01:59  lr: 0.000240  training_loss: 0.4430 (0.4464)  mae_loss: 0.2687 (0.2721)  classification_loss: 0.1718 (0.1743)  time: 0.1950  data: 0.0003  max mem: 5511
[08:09:27.227658] Epoch: [17]  [200/781]  eta: 0:01:55  lr: 0.000240  training_loss: 0.4545 (0.4468)  mae_loss: 0.2841 (0.2725)  classification_loss: 0.1717 (0.1742)  time: 0.1946  data: 0.0002  max mem: 5511
[08:09:31.123366] Epoch: [17]  [220/781]  eta: 0:01:50  lr: 0.000240  training_loss: 0.4474 (0.4471)  mae_loss: 0.2746 (0.2727)  classification_loss: 0.1734 (0.1743)  time: 0.1947  data: 0.0002  max mem: 5511
[08:09:35.062987] Epoch: [17]  [240/781]  eta: 0:01:46  lr: 0.000240  training_loss: 0.4525 (0.4488)  mae_loss: 0.2867 (0.2745)  classification_loss: 0.1744 (0.1743)  time: 0.1969  data: 0.0002  max mem: 5511
[08:09:38.958687] Epoch: [17]  [260/781]  eta: 0:01:42  lr: 0.000240  training_loss: 0.4502 (0.4492)  mae_loss: 0.2765 (0.2750)  classification_loss: 0.1734 (0.1742)  time: 0.1947  data: 0.0003  max mem: 5511
[08:09:42.863177] Epoch: [17]  [280/781]  eta: 0:01:38  lr: 0.000240  training_loss: 0.4530 (0.4496)  mae_loss: 0.2826 (0.2753)  classification_loss: 0.1778 (0.1743)  time: 0.1951  data: 0.0002  max mem: 5511
[08:09:46.756124] Epoch: [17]  [300/781]  eta: 0:01:34  lr: 0.000240  training_loss: 0.4284 (0.4485)  mae_loss: 0.2587 (0.2743)  classification_loss: 0.1676 (0.1742)  time: 0.1946  data: 0.0002  max mem: 5511
[08:09:50.672657] Epoch: [17]  [320/781]  eta: 0:01:30  lr: 0.000240  training_loss: 0.4488 (0.4482)  mae_loss: 0.2631 (0.2740)  classification_loss: 0.1705 (0.1742)  time: 0.1958  data: 0.0002  max mem: 5511
[08:09:54.575925] Epoch: [17]  [340/781]  eta: 0:01:26  lr: 0.000240  training_loss: 0.4544 (0.4483)  mae_loss: 0.2702 (0.2741)  classification_loss: 0.1709 (0.1742)  time: 0.1951  data: 0.0002  max mem: 5511
[08:09:58.460083] Epoch: [17]  [360/781]  eta: 0:01:22  lr: 0.000240  training_loss: 0.4395 (0.4479)  mae_loss: 0.2638 (0.2735)  classification_loss: 0.1773 (0.1744)  time: 0.1941  data: 0.0003  max mem: 5511
[08:10:02.361700] Epoch: [17]  [380/781]  eta: 0:01:18  lr: 0.000240  training_loss: 0.4534 (0.4482)  mae_loss: 0.2760 (0.2736)  classification_loss: 0.1744 (0.1745)  time: 0.1950  data: 0.0002  max mem: 5511
[08:10:06.260304] Epoch: [17]  [400/781]  eta: 0:01:14  lr: 0.000239  training_loss: 0.4425 (0.4478)  mae_loss: 0.2732 (0.2734)  classification_loss: 0.1727 (0.1744)  time: 0.1948  data: 0.0002  max mem: 5511
[08:10:10.164505] Epoch: [17]  [420/781]  eta: 0:01:10  lr: 0.000239  training_loss: 0.4564 (0.4480)  mae_loss: 0.2768 (0.2735)  classification_loss: 0.1752 (0.1745)  time: 0.1951  data: 0.0002  max mem: 5511
[08:10:14.087420] Epoch: [17]  [440/781]  eta: 0:01:07  lr: 0.000239  training_loss: 0.4247 (0.4474)  mae_loss: 0.2528 (0.2730)  classification_loss: 0.1730 (0.1744)  time: 0.1960  data: 0.0002  max mem: 5511
[08:10:17.997357] Epoch: [17]  [460/781]  eta: 0:01:03  lr: 0.000239  training_loss: 0.4420 (0.4472)  mae_loss: 0.2640 (0.2729)  classification_loss: 0.1703 (0.1743)  time: 0.1954  data: 0.0002  max mem: 5511
[08:10:21.913946] Epoch: [17]  [480/781]  eta: 0:00:59  lr: 0.000239  training_loss: 0.4369 (0.4468)  mae_loss: 0.2529 (0.2725)  classification_loss: 0.1759 (0.1743)  time: 0.1957  data: 0.0002  max mem: 5511
[08:10:25.814884] Epoch: [17]  [500/781]  eta: 0:00:55  lr: 0.000239  training_loss: 0.4400 (0.4468)  mae_loss: 0.2653 (0.2725)  classification_loss: 0.1740 (0.1743)  time: 0.1950  data: 0.0002  max mem: 5511
[08:10:29.718381] Epoch: [17]  [520/781]  eta: 0:00:51  lr: 0.000239  training_loss: 0.4368 (0.4465)  mae_loss: 0.2623 (0.2722)  classification_loss: 0.1714 (0.1743)  time: 0.1951  data: 0.0002  max mem: 5511
[08:10:33.627483] Epoch: [17]  [540/781]  eta: 0:00:47  lr: 0.000239  training_loss: 0.4394 (0.4464)  mae_loss: 0.2738 (0.2721)  classification_loss: 0.1722 (0.1743)  time: 0.1954  data: 0.0003  max mem: 5511
[08:10:37.574709] Epoch: [17]  [560/781]  eta: 0:00:43  lr: 0.000239  training_loss: 0.4440 (0.4465)  mae_loss: 0.2678 (0.2722)  classification_loss: 0.1738 (0.1743)  time: 0.1973  data: 0.0002  max mem: 5511
[08:10:41.479189] Epoch: [17]  [580/781]  eta: 0:00:39  lr: 0.000239  training_loss: 0.4262 (0.4460)  mae_loss: 0.2561 (0.2717)  classification_loss: 0.1721 (0.1743)  time: 0.1951  data: 0.0002  max mem: 5511
[08:10:45.414898] Epoch: [17]  [600/781]  eta: 0:00:35  lr: 0.000239  training_loss: 0.4386 (0.4459)  mae_loss: 0.2700 (0.2718)  classification_loss: 0.1685 (0.1741)  time: 0.1967  data: 0.0003  max mem: 5511
[08:10:49.350616] Epoch: [17]  [620/781]  eta: 0:00:31  lr: 0.000239  training_loss: 0.4384 (0.4457)  mae_loss: 0.2658 (0.2717)  classification_loss: 0.1676 (0.1739)  time: 0.1967  data: 0.0001  max mem: 5511
[08:10:53.256663] Epoch: [17]  [640/781]  eta: 0:00:27  lr: 0.000239  training_loss: 0.4389 (0.4458)  mae_loss: 0.2674 (0.2718)  classification_loss: 0.1737 (0.1739)  time: 0.1952  data: 0.0002  max mem: 5511
[08:10:57.155116] Epoch: [17]  [660/781]  eta: 0:00:23  lr: 0.000239  training_loss: 0.4537 (0.4461)  mae_loss: 0.2806 (0.2721)  classification_loss: 0.1754 (0.1740)  time: 0.1949  data: 0.0002  max mem: 5511
[08:11:01.054623] Epoch: [17]  [680/781]  eta: 0:00:19  lr: 0.000239  training_loss: 0.4487 (0.4463)  mae_loss: 0.2759 (0.2723)  classification_loss: 0.1740 (0.1740)  time: 0.1949  data: 0.0002  max mem: 5511
[08:11:04.952261] Epoch: [17]  [700/781]  eta: 0:00:15  lr: 0.000239  training_loss: 0.4273 (0.4462)  mae_loss: 0.2601 (0.2723)  classification_loss: 0.1728 (0.1739)  time: 0.1948  data: 0.0002  max mem: 5511
[08:11:08.898228] Epoch: [17]  [720/781]  eta: 0:00:11  lr: 0.000239  training_loss: 0.4369 (0.4460)  mae_loss: 0.2708 (0.2722)  classification_loss: 0.1689 (0.1739)  time: 0.1972  data: 0.0002  max mem: 5511
[08:11:12.810427] Epoch: [17]  [740/781]  eta: 0:00:08  lr: 0.000239  training_loss: 0.4294 (0.4457)  mae_loss: 0.2686 (0.2720)  classification_loss: 0.1663 (0.1737)  time: 0.1955  data: 0.0003  max mem: 5511
[08:11:16.713328] Epoch: [17]  [760/781]  eta: 0:00:04  lr: 0.000239  training_loss: 0.4428 (0.4458)  mae_loss: 0.2658 (0.2720)  classification_loss: 0.1717 (0.1737)  time: 0.1950  data: 0.0002  max mem: 5511
[08:11:20.598091] Epoch: [17]  [780/781]  eta: 0:00:00  lr: 0.000239  training_loss: 0.4359 (0.4457)  mae_loss: 0.2658 (0.2719)  classification_loss: 0.1735 (0.1737)  time: 0.1942  data: 0.0002  max mem: 5511
[08:11:20.738434] Epoch: [17] Total time: 0:02:33 (0.1964 s / it)
[08:11:20.738937] Averaged stats: lr: 0.000239  training_loss: 0.4359 (0.4457)  mae_loss: 0.2658 (0.2719)  classification_loss: 0.1735 (0.1737)
[08:11:21.289425] Test:  [  0/157]  eta: 0:01:25  testing_loss: 1.0204 (1.0204)  acc1: 71.8750 (71.8750)  acc5: 93.7500 (93.7500)  time: 0.5461  data: 0.5158  max mem: 5511
[08:11:21.575863] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 1.1063 (1.1291)  acc1: 62.5000 (61.7898)  acc5: 95.3125 (95.7386)  time: 0.0755  data: 0.0472  max mem: 5511
[08:11:21.861579] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.0770 (1.0896)  acc1: 62.5000 (63.6905)  acc5: 96.8750 (96.3542)  time: 0.0284  data: 0.0002  max mem: 5511
[08:11:22.152539] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 1.0832 (1.0969)  acc1: 65.6250 (63.5081)  acc5: 96.8750 (96.1694)  time: 0.0286  data: 0.0002  max mem: 5511
[08:11:22.440112] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 1.0832 (1.0908)  acc1: 64.0625 (63.4909)  acc5: 96.8750 (96.4939)  time: 0.0286  data: 0.0002  max mem: 5511
[08:11:22.727175] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 1.0590 (1.0903)  acc1: 64.0625 (64.0319)  acc5: 96.8750 (96.5686)  time: 0.0285  data: 0.0002  max mem: 5511
[08:11:23.012166] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.0825 (1.0892)  acc1: 65.6250 (64.1137)  acc5: 96.8750 (96.4652)  time: 0.0285  data: 0.0002  max mem: 5511
[08:11:23.297816] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 1.0249 (1.0813)  acc1: 67.1875 (64.5907)  acc5: 96.8750 (96.5889)  time: 0.0284  data: 0.0002  max mem: 5511
[08:11:23.587697] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 1.0286 (1.0847)  acc1: 67.1875 (64.4483)  acc5: 96.8750 (96.5856)  time: 0.0287  data: 0.0002  max mem: 5511
[08:11:23.876105] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.1164 (1.0870)  acc1: 64.0625 (64.4231)  acc5: 96.8750 (96.5831)  time: 0.0288  data: 0.0002  max mem: 5511
[08:11:24.159936] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.1270 (1.0907)  acc1: 62.5000 (64.2017)  acc5: 96.8750 (96.6584)  time: 0.0285  data: 0.0002  max mem: 5511
[08:11:24.447378] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.1484 (1.0938)  acc1: 60.9375 (64.0062)  acc5: 96.8750 (96.6639)  time: 0.0284  data: 0.0001  max mem: 5511
[08:11:24.731953] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.1035 (1.0926)  acc1: 64.0625 (64.0883)  acc5: 96.8750 (96.6813)  time: 0.0285  data: 0.0001  max mem: 5511
[08:11:25.020314] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.1106 (1.0949)  acc1: 64.0625 (63.9790)  acc5: 96.8750 (96.6007)  time: 0.0285  data: 0.0001  max mem: 5511
[08:11:25.301784] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.1106 (1.0938)  acc1: 65.6250 (64.1733)  acc5: 96.8750 (96.6090)  time: 0.0283  data: 0.0001  max mem: 5511
[08:11:25.581655] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.0886 (1.0918)  acc1: 65.6250 (64.3419)  acc5: 96.8750 (96.5956)  time: 0.0279  data: 0.0001  max mem: 5511
[08:11:25.731742] Test:  [156/157]  eta: 0:00:00  testing_loss: 1.0886 (1.0925)  acc1: 65.6250 (64.2600)  acc5: 96.8750 (96.6100)  time: 0.0269  data: 0.0001  max mem: 5511
[08:11:25.897732] Test: Total time: 0:00:05 (0.0328 s / it)
[08:11:25.898408] * Acc@1 64.260 Acc@5 96.610 loss 1.093
[08:11:25.898822] Accuracy of the network on the 10000 test images: 64.3%
[08:11:25.899092] Max accuracy: 64.26%
[08:11:26.163985] log_dir: ./output_dir
[08:11:27.081972] Epoch: [18]  [  0/781]  eta: 0:11:55  lr: 0.000239  training_loss: 0.3876 (0.3876)  mae_loss: 0.2172 (0.2172)  classification_loss: 0.1704 (0.1704)  time: 0.9163  data: 0.6985  max mem: 5511
[08:11:31.007219] Epoch: [18]  [ 20/781]  eta: 0:02:55  lr: 0.000239  training_loss: 0.4384 (0.4380)  mae_loss: 0.2622 (0.2676)  classification_loss: 0.1667 (0.1704)  time: 0.1962  data: 0.0002  max mem: 5511
[08:11:34.924245] Epoch: [18]  [ 40/781]  eta: 0:02:38  lr: 0.000239  training_loss: 0.4507 (0.4455)  mae_loss: 0.2788 (0.2731)  classification_loss: 0.1774 (0.1724)  time: 0.1958  data: 0.0002  max mem: 5511
[08:11:38.834131] Epoch: [18]  [ 60/781]  eta: 0:02:29  lr: 0.000239  training_loss: 0.4390 (0.4474)  mae_loss: 0.2760 (0.2751)  classification_loss: 0.1672 (0.1723)  time: 0.1954  data: 0.0002  max mem: 5511
[08:11:42.728897] Epoch: [18]  [ 80/781]  eta: 0:02:23  lr: 0.000238  training_loss: 0.4417 (0.4486)  mae_loss: 0.2711 (0.2761)  classification_loss: 0.1741 (0.1726)  time: 0.1947  data: 0.0002  max mem: 5511
[08:11:46.628073] Epoch: [18]  [100/781]  eta: 0:02:17  lr: 0.000238  training_loss: 0.4413 (0.4500)  mae_loss: 0.2673 (0.2780)  classification_loss: 0.1695 (0.1720)  time: 0.1949  data: 0.0002  max mem: 5511
[08:11:50.539337] Epoch: [18]  [120/781]  eta: 0:02:13  lr: 0.000238  training_loss: 0.4299 (0.4476)  mae_loss: 0.2660 (0.2760)  classification_loss: 0.1671 (0.1716)  time: 0.1955  data: 0.0002  max mem: 5511
[08:11:54.448347] Epoch: [18]  [140/781]  eta: 0:02:08  lr: 0.000238  training_loss: 0.4341 (0.4467)  mae_loss: 0.2694 (0.2755)  classification_loss: 0.1700 (0.1712)  time: 0.1954  data: 0.0003  max mem: 5511
[08:11:58.350796] Epoch: [18]  [160/781]  eta: 0:02:04  lr: 0.000238  training_loss: 0.4393 (0.4464)  mae_loss: 0.2688 (0.2752)  classification_loss: 0.1719 (0.1713)  time: 0.1950  data: 0.0002  max mem: 5511
[08:12:02.261793] Epoch: [18]  [180/781]  eta: 0:01:59  lr: 0.000238  training_loss: 0.4400 (0.4463)  mae_loss: 0.2728 (0.2751)  classification_loss: 0.1705 (0.1712)  time: 0.1955  data: 0.0003  max mem: 5511
[08:12:06.166830] Epoch: [18]  [200/781]  eta: 0:01:55  lr: 0.000238  training_loss: 0.4516 (0.4468)  mae_loss: 0.2792 (0.2752)  classification_loss: 0.1740 (0.1716)  time: 0.1952  data: 0.0002  max mem: 5511
[08:12:10.095359] Epoch: [18]  [220/781]  eta: 0:01:51  lr: 0.000238  training_loss: 0.4387 (0.4459)  mae_loss: 0.2596 (0.2740)  classification_loss: 0.1726 (0.1719)  time: 0.1963  data: 0.0005  max mem: 5511
[08:12:13.988712] Epoch: [18]  [240/781]  eta: 0:01:47  lr: 0.000238  training_loss: 0.4356 (0.4457)  mae_loss: 0.2587 (0.2738)  classification_loss: 0.1707 (0.1719)  time: 0.1946  data: 0.0002  max mem: 5511
[08:12:17.881998] Epoch: [18]  [260/781]  eta: 0:01:43  lr: 0.000238  training_loss: 0.4214 (0.4446)  mae_loss: 0.2558 (0.2728)  classification_loss: 0.1684 (0.1717)  time: 0.1946  data: 0.0002  max mem: 5511
[08:12:21.794507] Epoch: [18]  [280/781]  eta: 0:01:39  lr: 0.000238  training_loss: 0.4446 (0.4446)  mae_loss: 0.2715 (0.2729)  classification_loss: 0.1697 (0.1718)  time: 0.1955  data: 0.0002  max mem: 5511
[08:12:25.738313] Epoch: [18]  [300/781]  eta: 0:01:35  lr: 0.000238  training_loss: 0.4489 (0.4447)  mae_loss: 0.2755 (0.2728)  classification_loss: 0.1742 (0.1719)  time: 0.1970  data: 0.0002  max mem: 5511
[08:12:29.675935] Epoch: [18]  [320/781]  eta: 0:01:31  lr: 0.000238  training_loss: 0.4460 (0.4452)  mae_loss: 0.2691 (0.2733)  classification_loss: 0.1706 (0.1719)  time: 0.1968  data: 0.0002  max mem: 5511
[08:12:33.572378] Epoch: [18]  [340/781]  eta: 0:01:27  lr: 0.000238  training_loss: 0.4471 (0.4452)  mae_loss: 0.2776 (0.2735)  classification_loss: 0.1672 (0.1717)  time: 0.1947  data: 0.0002  max mem: 5511
[08:12:37.503121] Epoch: [18]  [360/781]  eta: 0:01:23  lr: 0.000238  training_loss: 0.4429 (0.4451)  mae_loss: 0.2647 (0.2734)  classification_loss: 0.1687 (0.1717)  time: 0.1963  data: 0.0002  max mem: 5511
[08:12:41.402129] Epoch: [18]  [380/781]  eta: 0:01:19  lr: 0.000238  training_loss: 0.4457 (0.4455)  mae_loss: 0.2677 (0.2737)  classification_loss: 0.1716 (0.1718)  time: 0.1948  data: 0.0002  max mem: 5511
[08:12:45.314969] Epoch: [18]  [400/781]  eta: 0:01:15  lr: 0.000238  training_loss: 0.4443 (0.4455)  mae_loss: 0.2764 (0.2738)  classification_loss: 0.1695 (0.1717)  time: 0.1955  data: 0.0003  max mem: 5511
[08:12:49.212698] Epoch: [18]  [420/781]  eta: 0:01:11  lr: 0.000238  training_loss: 0.4314 (0.4450)  mae_loss: 0.2607 (0.2734)  classification_loss: 0.1689 (0.1716)  time: 0.1948  data: 0.0002  max mem: 5511
[08:12:53.128213] Epoch: [18]  [440/781]  eta: 0:01:07  lr: 0.000238  training_loss: 0.4415 (0.4449)  mae_loss: 0.2743 (0.2734)  classification_loss: 0.1688 (0.1715)  time: 0.1957  data: 0.0002  max mem: 5511
[08:12:57.052296] Epoch: [18]  [460/781]  eta: 0:01:03  lr: 0.000238  training_loss: 0.4188 (0.4443)  mae_loss: 0.2535 (0.2730)  classification_loss: 0.1678 (0.1713)  time: 0.1961  data: 0.0003  max mem: 5511
[08:13:00.966664] Epoch: [18]  [480/781]  eta: 0:00:59  lr: 0.000238  training_loss: 0.4430 (0.4443)  mae_loss: 0.2710 (0.2730)  classification_loss: 0.1703 (0.1713)  time: 0.1956  data: 0.0003  max mem: 5511
[08:13:04.897451] Epoch: [18]  [500/781]  eta: 0:00:55  lr: 0.000238  training_loss: 0.4388 (0.4442)  mae_loss: 0.2641 (0.2728)  classification_loss: 0.1731 (0.1714)  time: 0.1965  data: 0.0002  max mem: 5511
[08:13:08.803498] Epoch: [18]  [520/781]  eta: 0:00:51  lr: 0.000238  training_loss: 0.4250 (0.4440)  mae_loss: 0.2663 (0.2727)  classification_loss: 0.1681 (0.1713)  time: 0.1952  data: 0.0002  max mem: 5511
[08:13:12.704784] Epoch: [18]  [540/781]  eta: 0:00:47  lr: 0.000237  training_loss: 0.4285 (0.4438)  mae_loss: 0.2616 (0.2726)  classification_loss: 0.1679 (0.1712)  time: 0.1949  data: 0.0002  max mem: 5511
[08:13:16.610119] Epoch: [18]  [560/781]  eta: 0:00:43  lr: 0.000237  training_loss: 0.4504 (0.4441)  mae_loss: 0.2765 (0.2729)  classification_loss: 0.1709 (0.1712)  time: 0.1952  data: 0.0002  max mem: 5511
[08:13:20.517679] Epoch: [18]  [580/781]  eta: 0:00:39  lr: 0.000237  training_loss: 0.4357 (0.4439)  mae_loss: 0.2597 (0.2728)  classification_loss: 0.1688 (0.1712)  time: 0.1953  data: 0.0002  max mem: 5511
[08:13:24.429167] Epoch: [18]  [600/781]  eta: 0:00:35  lr: 0.000237  training_loss: 0.4285 (0.4436)  mae_loss: 0.2622 (0.2726)  classification_loss: 0.1673 (0.1711)  time: 0.1955  data: 0.0003  max mem: 5511
[08:13:28.337529] Epoch: [18]  [620/781]  eta: 0:00:31  lr: 0.000237  training_loss: 0.4237 (0.4433)  mae_loss: 0.2638 (0.2722)  classification_loss: 0.1708 (0.1711)  time: 0.1953  data: 0.0002  max mem: 5511
[08:13:32.258266] Epoch: [18]  [640/781]  eta: 0:00:27  lr: 0.000237  training_loss: 0.4344 (0.4431)  mae_loss: 0.2593 (0.2721)  classification_loss: 0.1683 (0.1710)  time: 0.1960  data: 0.0002  max mem: 5511
[08:13:36.148433] Epoch: [18]  [660/781]  eta: 0:00:23  lr: 0.000237  training_loss: 0.4399 (0.4429)  mae_loss: 0.2685 (0.2720)  classification_loss: 0.1683 (0.1710)  time: 0.1944  data: 0.0002  max mem: 5511
[08:13:40.064286] Epoch: [18]  [680/781]  eta: 0:00:19  lr: 0.000237  training_loss: 0.4369 (0.4428)  mae_loss: 0.2663 (0.2718)  classification_loss: 0.1697 (0.1710)  time: 0.1957  data: 0.0002  max mem: 5511
[08:13:44.006349] Epoch: [18]  [700/781]  eta: 0:00:15  lr: 0.000237  training_loss: 0.4462 (0.4429)  mae_loss: 0.2729 (0.2719)  classification_loss: 0.1671 (0.1709)  time: 0.1970  data: 0.0002  max mem: 5511
[08:13:47.916276] Epoch: [18]  [720/781]  eta: 0:00:11  lr: 0.000237  training_loss: 0.4326 (0.4427)  mae_loss: 0.2633 (0.2717)  classification_loss: 0.1692 (0.1709)  time: 0.1954  data: 0.0002  max mem: 5511
[08:13:51.830671] Epoch: [18]  [740/781]  eta: 0:00:08  lr: 0.000237  training_loss: 0.4239 (0.4422)  mae_loss: 0.2503 (0.2714)  classification_loss: 0.1668 (0.1708)  time: 0.1956  data: 0.0003  max mem: 5511
[08:13:55.738944] Epoch: [18]  [760/781]  eta: 0:00:04  lr: 0.000237  training_loss: 0.4324 (0.4421)  mae_loss: 0.2614 (0.2713)  classification_loss: 0.1686 (0.1708)  time: 0.1953  data: 0.0003  max mem: 5511
[08:13:59.635945] Epoch: [18]  [780/781]  eta: 0:00:00  lr: 0.000237  training_loss: 0.4334 (0.4419)  mae_loss: 0.2578 (0.2711)  classification_loss: 0.1681 (0.1708)  time: 0.1948  data: 0.0003  max mem: 5511
[08:13:59.800974] Epoch: [18] Total time: 0:02:33 (0.1967 s / it)
[08:13:59.802170] Averaged stats: lr: 0.000237  training_loss: 0.4334 (0.4419)  mae_loss: 0.2578 (0.2711)  classification_loss: 0.1681 (0.1708)
[08:14:00.383602] Test:  [  0/157]  eta: 0:01:30  testing_loss: 0.9810 (0.9810)  acc1: 68.7500 (68.7500)  acc5: 95.3125 (95.3125)  time: 0.5765  data: 0.5460  max mem: 5511
[08:14:00.667726] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 1.0162 (1.0586)  acc1: 67.1875 (64.9148)  acc5: 96.8750 (96.3068)  time: 0.0781  data: 0.0498  max mem: 5511
[08:14:00.950986] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 1.0162 (1.0347)  acc1: 65.6250 (65.4018)  acc5: 96.8750 (96.4286)  time: 0.0282  data: 0.0001  max mem: 5511
[08:14:01.235334] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 1.0726 (1.0446)  acc1: 65.6250 (65.2722)  acc5: 95.3125 (96.2198)  time: 0.0282  data: 0.0002  max mem: 5511
[08:14:01.521571] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 1.0294 (1.0416)  acc1: 67.1875 (65.3582)  acc5: 95.3125 (96.2652)  time: 0.0284  data: 0.0002  max mem: 5511
[08:14:01.803663] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.9958 (1.0376)  acc1: 65.6250 (65.2880)  acc5: 96.8750 (96.4154)  time: 0.0283  data: 0.0002  max mem: 5511
[08:14:02.084659] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 1.0128 (1.0320)  acc1: 64.0625 (65.3945)  acc5: 96.8750 (96.5164)  time: 0.0280  data: 0.0002  max mem: 5511
[08:14:02.366089] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.9836 (1.0250)  acc1: 67.1875 (65.5370)  acc5: 96.8750 (96.6109)  time: 0.0280  data: 0.0002  max mem: 5511
[08:14:02.648401] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.9675 (1.0263)  acc1: 65.6250 (65.5093)  acc5: 96.8750 (96.5085)  time: 0.0281  data: 0.0002  max mem: 5511
[08:14:02.932025] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 1.0352 (1.0275)  acc1: 64.0625 (65.2644)  acc5: 96.8750 (96.6690)  time: 0.0282  data: 0.0002  max mem: 5511
[08:14:03.216494] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.0363 (1.0296)  acc1: 60.9375 (64.9752)  acc5: 96.8750 (96.6584)  time: 0.0283  data: 0.0002  max mem: 5511
[08:14:03.498934] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.0515 (1.0316)  acc1: 62.5000 (64.9775)  acc5: 96.8750 (96.6075)  time: 0.0282  data: 0.0002  max mem: 5511
[08:14:03.783208] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.0217 (1.0295)  acc1: 64.0625 (65.0568)  acc5: 96.8750 (96.6426)  time: 0.0282  data: 0.0002  max mem: 5511
[08:14:04.066046] Test:  [130/157]  eta: 0:00:00  testing_loss: 1.0474 (1.0341)  acc1: 64.0625 (64.9451)  acc5: 96.8750 (96.5887)  time: 0.0282  data: 0.0002  max mem: 5511
[08:14:04.346339] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.0449 (1.0314)  acc1: 67.1875 (65.2371)  acc5: 96.8750 (96.6312)  time: 0.0280  data: 0.0002  max mem: 5511
[08:14:04.624396] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.0047 (1.0298)  acc1: 68.7500 (65.3042)  acc5: 96.8750 (96.5853)  time: 0.0278  data: 0.0001  max mem: 5511
[08:14:04.775022] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.9823 (1.0294)  acc1: 68.7500 (65.3100)  acc5: 96.8750 (96.6300)  time: 0.0269  data: 0.0001  max mem: 5511
[08:14:04.900767] Test: Total time: 0:00:05 (0.0325 s / it)
[08:14:04.901220] * Acc@1 65.310 Acc@5 96.630 loss 1.029
[08:14:04.901504] Accuracy of the network on the 10000 test images: 65.3%
[08:14:04.901696] Max accuracy: 65.31%
[08:14:05.105250] log_dir: ./output_dir
[08:14:05.865045] Epoch: [19]  [  0/781]  eta: 0:09:52  lr: 0.000237  training_loss: 0.4471 (0.4471)  mae_loss: 0.2755 (0.2755)  classification_loss: 0.1716 (0.1716)  time: 0.7581  data: 0.5558  max mem: 5511
[08:14:09.777835] Epoch: [19]  [ 20/781]  eta: 0:02:49  lr: 0.000237  training_loss: 0.4290 (0.4314)  mae_loss: 0.2645 (0.2619)  classification_loss: 0.1693 (0.1695)  time: 0.1955  data: 0.0002  max mem: 5511
[08:14:13.709088] Epoch: [19]  [ 40/781]  eta: 0:02:35  lr: 0.000237  training_loss: 0.4294 (0.4317)  mae_loss: 0.2602 (0.2622)  classification_loss: 0.1685 (0.1696)  time: 0.1965  data: 0.0002  max mem: 5511
[08:14:17.624720] Epoch: [19]  [ 60/781]  eta: 0:02:27  lr: 0.000237  training_loss: 0.4178 (0.4308)  mae_loss: 0.2426 (0.2602)  classification_loss: 0.1726 (0.1706)  time: 0.1957  data: 0.0004  max mem: 5511
[08:14:21.534188] Epoch: [19]  [ 80/781]  eta: 0:02:22  lr: 0.000237  training_loss: 0.4235 (0.4293)  mae_loss: 0.2498 (0.2585)  classification_loss: 0.1727 (0.1707)  time: 0.1954  data: 0.0002  max mem: 5511
[08:14:25.440869] Epoch: [19]  [100/781]  eta: 0:02:17  lr: 0.000237  training_loss: 0.4335 (0.4313)  mae_loss: 0.2584 (0.2603)  classification_loss: 0.1690 (0.1710)  time: 0.1952  data: 0.0002  max mem: 5511
[08:14:29.353477] Epoch: [19]  [120/781]  eta: 0:02:12  lr: 0.000237  training_loss: 0.4120 (0.4291)  mae_loss: 0.2396 (0.2584)  classification_loss: 0.1694 (0.1707)  time: 0.1956  data: 0.0003  max mem: 5511
[08:14:33.283845] Epoch: [19]  [140/781]  eta: 0:02:08  lr: 0.000237  training_loss: 0.4468 (0.4304)  mae_loss: 0.2820 (0.2602)  classification_loss: 0.1677 (0.1702)  time: 0.1964  data: 0.0002  max mem: 5511
[08:14:37.178189] Epoch: [19]  [160/781]  eta: 0:02:03  lr: 0.000237  training_loss: 0.4268 (0.4307)  mae_loss: 0.2617 (0.2609)  classification_loss: 0.1663 (0.1698)  time: 0.1946  data: 0.0002  max mem: 5511
[08:14:41.063049] Epoch: [19]  [180/781]  eta: 0:01:59  lr: 0.000236  training_loss: 0.4404 (0.4318)  mae_loss: 0.2685 (0.2622)  classification_loss: 0.1708 (0.1697)  time: 0.1942  data: 0.0002  max mem: 5511
[08:14:44.963339] Epoch: [19]  [200/781]  eta: 0:01:55  lr: 0.000236  training_loss: 0.4464 (0.4333)  mae_loss: 0.2743 (0.2634)  classification_loss: 0.1715 (0.1699)  time: 0.1949  data: 0.0002  max mem: 5511
[08:14:48.951652] Epoch: [19]  [220/781]  eta: 0:01:51  lr: 0.000236  training_loss: 0.4358 (0.4348)  mae_loss: 0.2649 (0.2647)  classification_loss: 0.1706 (0.1701)  time: 0.1993  data: 0.0003  max mem: 5511
[08:14:52.870998] Epoch: [19]  [240/781]  eta: 0:01:47  lr: 0.000236  training_loss: 0.4315 (0.4348)  mae_loss: 0.2690 (0.2646)  classification_loss: 0.1710 (0.1702)  time: 0.1958  data: 0.0002  max mem: 5511
[08:14:56.761698] Epoch: [19]  [260/781]  eta: 0:01:43  lr: 0.000236  training_loss: 0.4299 (0.4345)  mae_loss: 0.2548 (0.2645)  classification_loss: 0.1694 (0.1700)  time: 0.1945  data: 0.0002  max mem: 5511
[08:15:00.696608] Epoch: [19]  [280/781]  eta: 0:01:39  lr: 0.000236  training_loss: 0.4491 (0.4353)  mae_loss: 0.2755 (0.2653)  classification_loss: 0.1708 (0.1701)  time: 0.1967  data: 0.0003  max mem: 5511
[08:15:04.627965] Epoch: [19]  [300/781]  eta: 0:01:35  lr: 0.000236  training_loss: 0.4394 (0.4363)  mae_loss: 0.2625 (0.2661)  classification_loss: 0.1704 (0.1702)  time: 0.1964  data: 0.0002  max mem: 5511
[08:15:08.597409] Epoch: [19]  [320/781]  eta: 0:01:31  lr: 0.000236  training_loss: 0.4478 (0.4370)  mae_loss: 0.2849 (0.2669)  classification_loss: 0.1680 (0.1700)  time: 0.1984  data: 0.0002  max mem: 5511
[08:15:12.552423] Epoch: [19]  [340/781]  eta: 0:01:27  lr: 0.000236  training_loss: 0.4218 (0.4365)  mae_loss: 0.2658 (0.2666)  classification_loss: 0.1660 (0.1698)  time: 0.1977  data: 0.0002  max mem: 5511
[08:15:16.473813] Epoch: [19]  [360/781]  eta: 0:01:23  lr: 0.000236  training_loss: 0.4505 (0.4373)  mae_loss: 0.2760 (0.2675)  classification_loss: 0.1688 (0.1699)  time: 0.1960  data: 0.0002  max mem: 5511
[08:15:20.394410] Epoch: [19]  [380/781]  eta: 0:01:19  lr: 0.000236  training_loss: 0.4262 (0.4371)  mae_loss: 0.2559 (0.2673)  classification_loss: 0.1676 (0.1698)  time: 0.1959  data: 0.0002  max mem: 5511
[08:15:24.325012] Epoch: [19]  [400/781]  eta: 0:01:15  lr: 0.000236  training_loss: 0.4348 (0.4371)  mae_loss: 0.2642 (0.2675)  classification_loss: 0.1681 (0.1697)  time: 0.1965  data: 0.0002  max mem: 5511
[08:15:28.223114] Epoch: [19]  [420/781]  eta: 0:01:11  lr: 0.000236  training_loss: 0.4350 (0.4373)  mae_loss: 0.2705 (0.2677)  classification_loss: 0.1682 (0.1696)  time: 0.1948  data: 0.0003  max mem: 5511
[08:15:32.124878] Epoch: [19]  [440/781]  eta: 0:01:07  lr: 0.000236  training_loss: 0.4481 (0.4377)  mae_loss: 0.2764 (0.2683)  classification_loss: 0.1658 (0.1695)  time: 0.1950  data: 0.0002  max mem: 5511
[08:15:36.017109] Epoch: [19]  [460/781]  eta: 0:01:03  lr: 0.000236  training_loss: 0.4283 (0.4376)  mae_loss: 0.2595 (0.2682)  classification_loss: 0.1666 (0.1694)  time: 0.1945  data: 0.0002  max mem: 5511
[08:15:39.941073] Epoch: [19]  [480/781]  eta: 0:00:59  lr: 0.000236  training_loss: 0.4290 (0.4373)  mae_loss: 0.2551 (0.2677)  classification_loss: 0.1725 (0.1696)  time: 0.1961  data: 0.0002  max mem: 5511
[08:15:43.843665] Epoch: [19]  [500/781]  eta: 0:00:55  lr: 0.000236  training_loss: 0.4289 (0.4375)  mae_loss: 0.2623 (0.2679)  classification_loss: 0.1671 (0.1696)  time: 0.1951  data: 0.0002  max mem: 5511
[08:15:47.767167] Epoch: [19]  [520/781]  eta: 0:00:51  lr: 0.000236  training_loss: 0.4393 (0.4378)  mae_loss: 0.2712 (0.2682)  classification_loss: 0.1697 (0.1697)  time: 0.1961  data: 0.0002  max mem: 5511
[08:15:51.673116] Epoch: [19]  [540/781]  eta: 0:00:47  lr: 0.000236  training_loss: 0.4298 (0.4378)  mae_loss: 0.2653 (0.2682)  classification_loss: 0.1645 (0.1696)  time: 0.1952  data: 0.0002  max mem: 5511
[08:15:55.567594] Epoch: [19]  [560/781]  eta: 0:00:43  lr: 0.000236  training_loss: 0.4320 (0.4378)  mae_loss: 0.2673 (0.2682)  classification_loss: 0.1647 (0.1696)  time: 0.1946  data: 0.0003  max mem: 5511
[08:15:59.468396] Epoch: [19]  [580/781]  eta: 0:00:39  lr: 0.000235  training_loss: 0.4227 (0.4371)  mae_loss: 0.2550 (0.2676)  classification_loss: 0.1676 (0.1695)  time: 0.1950  data: 0.0003  max mem: 5511
[08:16:03.382573] Epoch: [19]  [600/781]  eta: 0:00:35  lr: 0.000235  training_loss: 0.4200 (0.4367)  mae_loss: 0.2486 (0.2673)  classification_loss: 0.1657 (0.1694)  time: 0.1956  data: 0.0002  max mem: 5511
[08:16:07.278900] Epoch: [19]  [620/781]  eta: 0:00:31  lr: 0.000235  training_loss: 0.4315 (0.4366)  mae_loss: 0.2676 (0.2673)  classification_loss: 0.1639 (0.1693)  time: 0.1947  data: 0.0002  max mem: 5511
[08:16:11.172219] Epoch: [19]  [640/781]  eta: 0:00:27  lr: 0.000235  training_loss: 0.4306 (0.4365)  mae_loss: 0.2613 (0.2671)  classification_loss: 0.1715 (0.1694)  time: 0.1946  data: 0.0002  max mem: 5511
[08:16:15.076708] Epoch: [19]  [660/781]  eta: 0:00:23  lr: 0.000235  training_loss: 0.4122 (0.4359)  mae_loss: 0.2456 (0.2665)  classification_loss: 0.1707 (0.1694)  time: 0.1951  data: 0.0002  max mem: 5511
[08:16:18.985964] Epoch: [19]  [680/781]  eta: 0:00:19  lr: 0.000235  training_loss: 0.4321 (0.4359)  mae_loss: 0.2637 (0.2664)  classification_loss: 0.1699 (0.1694)  time: 0.1954  data: 0.0003  max mem: 5511
[08:16:22.902857] Epoch: [19]  [700/781]  eta: 0:00:15  lr: 0.000235  training_loss: 0.4346 (0.4360)  mae_loss: 0.2682 (0.2665)  classification_loss: 0.1682 (0.1694)  time: 0.1958  data: 0.0002  max mem: 5511
[08:16:26.809052] Epoch: [19]  [720/781]  eta: 0:00:11  lr: 0.000235  training_loss: 0.4276 (0.4358)  mae_loss: 0.2603 (0.2664)  classification_loss: 0.1673 (0.1694)  time: 0.1952  data: 0.0003  max mem: 5511
[08:16:30.728785] Epoch: [19]  [740/781]  eta: 0:00:08  lr: 0.000235  training_loss: 0.4129 (0.4354)  mae_loss: 0.2544 (0.2661)  classification_loss: 0.1656 (0.1693)  time: 0.1959  data: 0.0002  max mem: 5511
[08:16:34.725661] Epoch: [19]  [760/781]  eta: 0:00:04  lr: 0.000235  training_loss: 0.4298 (0.4354)  mae_loss: 0.2564 (0.2660)  classification_loss: 0.1751 (0.1694)  time: 0.1998  data: 0.0003  max mem: 5511
[08:16:38.682474] Epoch: [19]  [780/781]  eta: 0:00:00  lr: 0.000235  training_loss: 0.4283 (0.4352)  mae_loss: 0.2557 (0.2659)  classification_loss: 0.1656 (0.1694)  time: 0.1978  data: 0.0002  max mem: 5511
[08:16:38.813405] Epoch: [19] Total time: 0:02:33 (0.1968 s / it)
[08:16:38.813857] Averaged stats: lr: 0.000235  training_loss: 0.4283 (0.4352)  mae_loss: 0.2557 (0.2659)  classification_loss: 0.1656 (0.1694)
[08:16:39.497382] Test:  [  0/157]  eta: 0:01:46  testing_loss: 0.9878 (0.9878)  acc1: 67.1875 (67.1875)  acc5: 96.8750 (96.8750)  time: 0.6788  data: 0.6492  max mem: 5511
[08:16:39.793272] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 1.0446 (1.0312)  acc1: 64.0625 (63.7784)  acc5: 98.4375 (97.7273)  time: 0.0884  data: 0.0593  max mem: 5511
[08:16:40.076516] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 1.0355 (1.0082)  acc1: 64.0625 (65.4018)  acc5: 98.4375 (97.9167)  time: 0.0288  data: 0.0003  max mem: 5511
[08:16:40.358191] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.9936 (1.0171)  acc1: 67.1875 (65.7258)  acc5: 98.4375 (97.5806)  time: 0.0281  data: 0.0002  max mem: 5511
[08:16:40.640023] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.9755 (1.0047)  acc1: 68.7500 (66.1585)  acc5: 98.4375 (97.5229)  time: 0.0281  data: 0.0002  max mem: 5511
[08:16:40.922554] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.9453 (1.0024)  acc1: 65.6250 (66.1765)  acc5: 96.8750 (97.4571)  time: 0.0281  data: 0.0002  max mem: 5511
[08:16:41.205931] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.9964 (0.9999)  acc1: 65.6250 (66.0348)  acc5: 96.8750 (97.3873)  time: 0.0281  data: 0.0002  max mem: 5511
[08:16:41.487083] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.9680 (0.9951)  acc1: 65.6250 (66.2192)  acc5: 96.8750 (97.2711)  time: 0.0281  data: 0.0001  max mem: 5511
[08:16:41.768978] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.9738 (0.9982)  acc1: 67.1875 (66.3002)  acc5: 96.8750 (97.2029)  time: 0.0280  data: 0.0001  max mem: 5511
[08:16:42.051143] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.9896 (0.9984)  acc1: 64.0625 (66.3118)  acc5: 96.8750 (97.1497)  time: 0.0281  data: 0.0001  max mem: 5511
[08:16:42.332719] Test:  [100/157]  eta: 0:00:01  testing_loss: 1.0043 (1.0001)  acc1: 65.6250 (66.3521)  acc5: 96.8750 (97.2153)  time: 0.0281  data: 0.0001  max mem: 5511
[08:16:42.613814] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.0156 (1.0025)  acc1: 65.6250 (66.3711)  acc5: 98.4375 (97.2832)  time: 0.0280  data: 0.0001  max mem: 5511
[08:16:42.895467] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.9965 (0.9984)  acc1: 65.6250 (66.5548)  acc5: 96.8750 (97.2495)  time: 0.0280  data: 0.0001  max mem: 5511
[08:16:43.177165] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.9884 (1.0018)  acc1: 65.6250 (66.4361)  acc5: 96.8750 (97.1970)  time: 0.0281  data: 0.0001  max mem: 5511
[08:16:43.457727] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.9996 (0.9990)  acc1: 68.7500 (66.6888)  acc5: 96.8750 (97.2296)  time: 0.0280  data: 0.0001  max mem: 5511
[08:16:43.737090] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.9885 (0.9984)  acc1: 70.3125 (66.7943)  acc5: 96.8750 (97.1854)  time: 0.0279  data: 0.0001  max mem: 5511
[08:16:43.889342] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.9601 (0.9979)  acc1: 68.7500 (66.7100)  acc5: 96.8750 (97.2100)  time: 0.0270  data: 0.0001  max mem: 5511
[08:16:44.044921] Test: Total time: 0:00:05 (0.0333 s / it)
[08:16:44.045369] * Acc@1 66.710 Acc@5 97.210 loss 0.998
[08:16:44.045651] Accuracy of the network on the 10000 test images: 66.7%
[08:16:44.045864] Max accuracy: 66.71%
[08:16:44.172419] log_dir: ./output_dir
[08:16:44.960728] Epoch: [20]  [  0/781]  eta: 0:10:14  lr: 0.000235  training_loss: 0.4371 (0.4371)  mae_loss: 0.2739 (0.2739)  classification_loss: 0.1632 (0.1632)  time: 0.7867  data: 0.5535  max mem: 5511
[08:16:48.879918] Epoch: [20]  [ 20/781]  eta: 0:02:50  lr: 0.000235  training_loss: 0.4272 (0.4298)  mae_loss: 0.2632 (0.2648)  classification_loss: 0.1639 (0.1651)  time: 0.1958  data: 0.0003  max mem: 5511
[08:16:52.837288] Epoch: [20]  [ 40/781]  eta: 0:02:36  lr: 0.000235  training_loss: 0.4349 (0.4291)  mae_loss: 0.2685 (0.2632)  classification_loss: 0.1661 (0.1659)  time: 0.1978  data: 0.0003  max mem: 5511
[08:16:56.761022] Epoch: [20]  [ 60/781]  eta: 0:02:28  lr: 0.000235  training_loss: 0.4289 (0.4295)  mae_loss: 0.2591 (0.2632)  classification_loss: 0.1678 (0.1663)  time: 0.1961  data: 0.0003  max mem: 5511
[08:17:00.671352] Epoch: [20]  [ 80/781]  eta: 0:02:22  lr: 0.000235  training_loss: 0.4197 (0.4297)  mae_loss: 0.2543 (0.2630)  classification_loss: 0.1683 (0.1668)  time: 0.1954  data: 0.0003  max mem: 5511
[08:17:04.581002] Epoch: [20]  [100/781]  eta: 0:02:17  lr: 0.000235  training_loss: 0.4334 (0.4302)  mae_loss: 0.2599 (0.2632)  classification_loss: 0.1651 (0.1670)  time: 0.1954  data: 0.0002  max mem: 5511
[08:17:08.494030] Epoch: [20]  [120/781]  eta: 0:02:12  lr: 0.000235  training_loss: 0.4299 (0.4302)  mae_loss: 0.2637 (0.2634)  classification_loss: 0.1664 (0.1668)  time: 0.1956  data: 0.0002  max mem: 5511
[08:17:12.411276] Epoch: [20]  [140/781]  eta: 0:02:08  lr: 0.000235  training_loss: 0.4231 (0.4297)  mae_loss: 0.2532 (0.2628)  classification_loss: 0.1685 (0.1670)  time: 0.1958  data: 0.0003  max mem: 5511
[08:17:16.323397] Epoch: [20]  [160/781]  eta: 0:02:03  lr: 0.000235  training_loss: 0.4172 (0.4300)  mae_loss: 0.2627 (0.2629)  classification_loss: 0.1673 (0.1671)  time: 0.1955  data: 0.0003  max mem: 5511
[08:17:20.229919] Epoch: [20]  [180/781]  eta: 0:01:59  lr: 0.000235  training_loss: 0.4169 (0.4292)  mae_loss: 0.2520 (0.2621)  classification_loss: 0.1668 (0.1671)  time: 0.1952  data: 0.0002  max mem: 5511
[08:17:24.153393] Epoch: [20]  [200/781]  eta: 0:01:55  lr: 0.000234  training_loss: 0.4190 (0.4286)  mae_loss: 0.2466 (0.2612)  classification_loss: 0.1675 (0.1674)  time: 0.1961  data: 0.0003  max mem: 5511
[08:17:28.075124] Epoch: [20]  [220/781]  eta: 0:01:51  lr: 0.000234  training_loss: 0.4355 (0.4293)  mae_loss: 0.2595 (0.2615)  classification_loss: 0.1730 (0.1677)  time: 0.1960  data: 0.0002  max mem: 5511
[08:17:31.969648] Epoch: [20]  [240/781]  eta: 0:01:47  lr: 0.000234  training_loss: 0.4196 (0.4288)  mae_loss: 0.2526 (0.2609)  classification_loss: 0.1663 (0.1678)  time: 0.1947  data: 0.0003  max mem: 5511
[08:17:35.903949] Epoch: [20]  [260/781]  eta: 0:01:43  lr: 0.000234  training_loss: 0.4153 (0.4285)  mae_loss: 0.2539 (0.2610)  classification_loss: 0.1643 (0.1676)  time: 0.1966  data: 0.0002  max mem: 5511
[08:17:39.840170] Epoch: [20]  [280/781]  eta: 0:01:39  lr: 0.000234  training_loss: 0.4223 (0.4287)  mae_loss: 0.2540 (0.2609)  classification_loss: 0.1708 (0.1678)  time: 0.1967  data: 0.0002  max mem: 5511
[08:17:43.723798] Epoch: [20]  [300/781]  eta: 0:01:35  lr: 0.000234  training_loss: 0.4169 (0.4285)  mae_loss: 0.2548 (0.2606)  classification_loss: 0.1673 (0.1679)  time: 0.1941  data: 0.0002  max mem: 5511
[08:17:47.624914] Epoch: [20]  [320/781]  eta: 0:01:31  lr: 0.000234  training_loss: 0.4259 (0.4284)  mae_loss: 0.2602 (0.2605)  classification_loss: 0.1663 (0.1678)  time: 0.1950  data: 0.0002  max mem: 5511
[08:17:51.548300] Epoch: [20]  [340/781]  eta: 0:01:27  lr: 0.000234  training_loss: 0.4196 (0.4282)  mae_loss: 0.2588 (0.2606)  classification_loss: 0.1625 (0.1676)  time: 0.1961  data: 0.0002  max mem: 5511
[08:17:55.475805] Epoch: [20]  [360/781]  eta: 0:01:23  lr: 0.000234  training_loss: 0.4311 (0.4282)  mae_loss: 0.2668 (0.2605)  classification_loss: 0.1669 (0.1677)  time: 0.1963  data: 0.0003  max mem: 5511
[08:17:59.385280] Epoch: [20]  [380/781]  eta: 0:01:19  lr: 0.000234  training_loss: 0.4177 (0.4286)  mae_loss: 0.2588 (0.2610)  classification_loss: 0.1634 (0.1675)  time: 0.1954  data: 0.0002  max mem: 5511
[08:18:03.301680] Epoch: [20]  [400/781]  eta: 0:01:15  lr: 0.000234  training_loss: 0.4334 (0.4292)  mae_loss: 0.2616 (0.2615)  classification_loss: 0.1716 (0.1677)  time: 0.1958  data: 0.0002  max mem: 5511
[08:18:07.202177] Epoch: [20]  [420/781]  eta: 0:01:11  lr: 0.000234  training_loss: 0.4271 (0.4292)  mae_loss: 0.2574 (0.2616)  classification_loss: 0.1660 (0.1676)  time: 0.1949  data: 0.0003  max mem: 5511
[08:18:11.138130] Epoch: [20]  [440/781]  eta: 0:01:07  lr: 0.000234  training_loss: 0.4183 (0.4289)  mae_loss: 0.2553 (0.2613)  classification_loss: 0.1677 (0.1675)  time: 0.1967  data: 0.0002  max mem: 5511
[08:18:15.047206] Epoch: [20]  [460/781]  eta: 0:01:03  lr: 0.000234  training_loss: 0.4149 (0.4285)  mae_loss: 0.2517 (0.2611)  classification_loss: 0.1611 (0.1674)  time: 0.1954  data: 0.0002  max mem: 5511
[08:18:18.957666] Epoch: [20]  [480/781]  eta: 0:00:59  lr: 0.000234  training_loss: 0.4203 (0.4280)  mae_loss: 0.2473 (0.2606)  classification_loss: 0.1708 (0.1674)  time: 0.1954  data: 0.0003  max mem: 5511
[08:18:22.900801] Epoch: [20]  [500/781]  eta: 0:00:55  lr: 0.000234  training_loss: 0.4322 (0.4280)  mae_loss: 0.2628 (0.2606)  classification_loss: 0.1677 (0.1674)  time: 0.1970  data: 0.0003  max mem: 5511
[08:18:26.838794] Epoch: [20]  [520/781]  eta: 0:00:51  lr: 0.000234  training_loss: 0.4189 (0.4279)  mae_loss: 0.2492 (0.2605)  classification_loss: 0.1671 (0.1674)  time: 0.1968  data: 0.0002  max mem: 5511
[08:18:30.778943] Epoch: [20]  [540/781]  eta: 0:00:47  lr: 0.000234  training_loss: 0.4154 (0.4275)  mae_loss: 0.2596 (0.2603)  classification_loss: 0.1613 (0.1672)  time: 0.1969  data: 0.0002  max mem: 5511
[08:18:34.714400] Epoch: [20]  [560/781]  eta: 0:00:43  lr: 0.000234  training_loss: 0.4356 (0.4277)  mae_loss: 0.2652 (0.2606)  classification_loss: 0.1622 (0.1671)  time: 0.1967  data: 0.0002  max mem: 5511
[08:18:38.620065] Epoch: [20]  [580/781]  eta: 0:00:39  lr: 0.000234  training_loss: 0.4245 (0.4279)  mae_loss: 0.2601 (0.2607)  classification_loss: 0.1702 (0.1672)  time: 0.1952  data: 0.0003  max mem: 5511
[08:18:42.522711] Epoch: [20]  [600/781]  eta: 0:00:35  lr: 0.000233  training_loss: 0.4270 (0.4282)  mae_loss: 0.2646 (0.2611)  classification_loss: 0.1642 (0.1671)  time: 0.1950  data: 0.0002  max mem: 5511
[08:18:46.431170] Epoch: [20]  [620/781]  eta: 0:00:31  lr: 0.000233  training_loss: 0.4393 (0.4284)  mae_loss: 0.2738 (0.2613)  classification_loss: 0.1635 (0.1670)  time: 0.1953  data: 0.0002  max mem: 5511
[08:18:50.331108] Epoch: [20]  [640/781]  eta: 0:00:27  lr: 0.000233  training_loss: 0.4231 (0.4284)  mae_loss: 0.2638 (0.2613)  classification_loss: 0.1677 (0.1671)  time: 0.1949  data: 0.0002  max mem: 5511
[08:18:54.262886] Epoch: [20]  [660/781]  eta: 0:00:23  lr: 0.000233  training_loss: 0.4287 (0.4284)  mae_loss: 0.2580 (0.2614)  classification_loss: 0.1630 (0.1670)  time: 0.1965  data: 0.0003  max mem: 5511
[08:18:58.166524] Epoch: [20]  [680/781]  eta: 0:00:19  lr: 0.000233  training_loss: 0.4229 (0.4284)  mae_loss: 0.2586 (0.2614)  classification_loss: 0.1648 (0.1670)  time: 0.1951  data: 0.0002  max mem: 5511
[08:19:02.059043] Epoch: [20]  [700/781]  eta: 0:00:15  lr: 0.000233  training_loss: 0.4194 (0.4280)  mae_loss: 0.2402 (0.2609)  classification_loss: 0.1697 (0.1671)  time: 0.1946  data: 0.0002  max mem: 5511
[08:19:05.954743] Epoch: [20]  [720/781]  eta: 0:00:11  lr: 0.000233  training_loss: 0.4213 (0.4279)  mae_loss: 0.2482 (0.2608)  classification_loss: 0.1683 (0.1671)  time: 0.1947  data: 0.0002  max mem: 5511
[08:19:09.872280] Epoch: [20]  [740/781]  eta: 0:00:08  lr: 0.000233  training_loss: 0.4261 (0.4279)  mae_loss: 0.2626 (0.2609)  classification_loss: 0.1637 (0.1670)  time: 0.1957  data: 0.0003  max mem: 5511
[08:19:13.826054] Epoch: [20]  [760/781]  eta: 0:00:04  lr: 0.000233  training_loss: 0.4227 (0.4279)  mae_loss: 0.2564 (0.2609)  classification_loss: 0.1665 (0.1670)  time: 0.1976  data: 0.0002  max mem: 5511
[08:19:17.712564] Epoch: [20]  [780/781]  eta: 0:00:00  lr: 0.000233  training_loss: 0.4339 (0.4279)  mae_loss: 0.2608 (0.2609)  classification_loss: 0.1662 (0.1670)  time: 0.1942  data: 0.0002  max mem: 5511
[08:19:17.870625] Epoch: [20] Total time: 0:02:33 (0.1968 s / it)
[08:19:17.871080] Averaged stats: lr: 0.000233  training_loss: 0.4339 (0.4279)  mae_loss: 0.2608 (0.2609)  classification_loss: 0.1662 (0.1670)
[08:19:19.405323] Test:  [  0/157]  eta: 0:01:44  testing_loss: 0.9530 (0.9530)  acc1: 64.0625 (64.0625)  acc5: 100.0000 (100.0000)  time: 0.6671  data: 0.6375  max mem: 5511
[08:19:19.697222] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.9962 (1.0261)  acc1: 64.0625 (64.2045)  acc5: 98.4375 (98.5795)  time: 0.0870  data: 0.0583  max mem: 5511
[08:19:19.980381] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.9635 (0.9958)  acc1: 64.0625 (65.9970)  acc5: 98.4375 (98.2143)  time: 0.0286  data: 0.0003  max mem: 5511
[08:19:20.264723] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 1.0098 (1.0049)  acc1: 67.1875 (66.2802)  acc5: 98.4375 (97.8327)  time: 0.0282  data: 0.0002  max mem: 5511
[08:19:20.556333] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 1.0180 (1.0042)  acc1: 65.6250 (66.2348)  acc5: 96.8750 (97.7134)  time: 0.0286  data: 0.0003  max mem: 5511
[08:19:20.840844] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.9904 (1.0043)  acc1: 65.6250 (66.5441)  acc5: 96.8750 (97.6409)  time: 0.0286  data: 0.0002  max mem: 5511
[08:19:21.124733] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.9672 (1.0008)  acc1: 68.7500 (66.5984)  acc5: 96.8750 (97.5154)  time: 0.0283  data: 0.0002  max mem: 5511
[08:19:21.417037] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.9533 (0.9954)  acc1: 67.1875 (66.8574)  acc5: 96.8750 (97.5132)  time: 0.0286  data: 0.0002  max mem: 5511
[08:19:21.706865] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.9551 (0.9977)  acc1: 67.1875 (66.8596)  acc5: 96.8750 (97.4344)  time: 0.0289  data: 0.0002  max mem: 5511
[08:19:21.992691] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.9852 (0.9990)  acc1: 65.6250 (66.8269)  acc5: 98.4375 (97.5103)  time: 0.0286  data: 0.0002  max mem: 5511
[08:19:22.274916] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.9829 (1.0006)  acc1: 68.7500 (66.8936)  acc5: 98.4375 (97.4783)  time: 0.0283  data: 0.0002  max mem: 5511
[08:19:22.555054] Test:  [110/157]  eta: 0:00:01  testing_loss: 1.0194 (1.0037)  acc1: 65.6250 (66.7511)  acc5: 96.8750 (97.4803)  time: 0.0280  data: 0.0002  max mem: 5511
[08:19:22.835410] Test:  [120/157]  eta: 0:00:01  testing_loss: 1.0072 (0.9994)  acc1: 67.1875 (66.9809)  acc5: 98.4375 (97.5336)  time: 0.0279  data: 0.0002  max mem: 5511
[08:19:23.115887] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.9905 (1.0023)  acc1: 67.1875 (66.8058)  acc5: 96.8750 (97.5191)  time: 0.0279  data: 0.0002  max mem: 5511
[08:19:23.396353] Test:  [140/157]  eta: 0:00:00  testing_loss: 1.0292 (1.0016)  acc1: 65.6250 (66.9659)  acc5: 96.8750 (97.4845)  time: 0.0279  data: 0.0002  max mem: 5511
[08:19:23.674714] Test:  [150/157]  eta: 0:00:00  testing_loss: 1.0188 (0.9998)  acc1: 67.1875 (67.1254)  acc5: 96.8750 (97.4441)  time: 0.0278  data: 0.0001  max mem: 5511
[08:19:23.825287] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.9852 (1.0007)  acc1: 67.1875 (67.1000)  acc5: 96.8750 (97.4400)  time: 0.0269  data: 0.0001  max mem: 5511
[08:19:23.979630] Test: Total time: 0:00:05 (0.0334 s / it)
[08:19:23.980309] * Acc@1 67.100 Acc@5 97.440 loss 1.001
[08:19:23.980642] Accuracy of the network on the 10000 test images: 67.1%
[08:19:23.980861] Max accuracy: 67.10%
[08:19:24.079115] log_dir: ./output_dir
[08:19:25.003330] Epoch: [21]  [  0/781]  eta: 0:12:00  lr: 0.000233  training_loss: 0.4416 (0.4416)  mae_loss: 0.2871 (0.2871)  classification_loss: 0.1545 (0.1545)  time: 0.9223  data: 0.7078  max mem: 5511
[08:19:28.921651] Epoch: [21]  [ 20/781]  eta: 0:02:55  lr: 0.000233  training_loss: 0.4217 (0.4253)  mae_loss: 0.2636 (0.2654)  classification_loss: 0.1573 (0.1599)  time: 0.1958  data: 0.0004  max mem: 5511
[08:19:32.837217] Epoch: [21]  [ 40/781]  eta: 0:02:38  lr: 0.000233  training_loss: 0.4176 (0.4226)  mae_loss: 0.2469 (0.2606)  classification_loss: 0.1634 (0.1620)  time: 0.1957  data: 0.0006  max mem: 5511
[08:19:36.759201] Epoch: [21]  [ 60/781]  eta: 0:02:29  lr: 0.000233  training_loss: 0.4459 (0.4277)  mae_loss: 0.2637 (0.2632)  classification_loss: 0.1694 (0.1644)  time: 0.1960  data: 0.0002  max mem: 5511
[08:19:40.703833] Epoch: [21]  [ 80/781]  eta: 0:02:23  lr: 0.000233  training_loss: 0.4281 (0.4290)  mae_loss: 0.2682 (0.2642)  classification_loss: 0.1666 (0.1648)  time: 0.1971  data: 0.0002  max mem: 5511
[08:19:44.603468] Epoch: [21]  [100/781]  eta: 0:02:18  lr: 0.000233  training_loss: 0.4286 (0.4286)  mae_loss: 0.2585 (0.2631)  classification_loss: 0.1678 (0.1655)  time: 0.1949  data: 0.0002  max mem: 5511
[08:19:48.495095] Epoch: [21]  [120/781]  eta: 0:02:13  lr: 0.000233  training_loss: 0.4124 (0.4278)  mae_loss: 0.2513 (0.2621)  classification_loss: 0.1661 (0.1658)  time: 0.1945  data: 0.0003  max mem: 5511
[08:19:52.419629] Epoch: [21]  [140/781]  eta: 0:02:08  lr: 0.000233  training_loss: 0.4056 (0.4267)  mae_loss: 0.2463 (0.2611)  classification_loss: 0.1650 (0.1656)  time: 0.1961  data: 0.0003  max mem: 5511
[08:19:56.339919] Epoch: [21]  [160/781]  eta: 0:02:04  lr: 0.000233  training_loss: 0.4136 (0.4250)  mae_loss: 0.2449 (0.2596)  classification_loss: 0.1632 (0.1655)  time: 0.1959  data: 0.0002  max mem: 5511
[08:20:00.254076] Epoch: [21]  [180/781]  eta: 0:02:00  lr: 0.000232  training_loss: 0.4155 (0.4238)  mae_loss: 0.2560 (0.2585)  classification_loss: 0.1613 (0.1653)  time: 0.1956  data: 0.0002  max mem: 5511
[08:20:04.190866] Epoch: [21]  [200/781]  eta: 0:01:55  lr: 0.000232  training_loss: 0.4205 (0.4234)  mae_loss: 0.2546 (0.2582)  classification_loss: 0.1624 (0.1652)  time: 0.1968  data: 0.0002  max mem: 5511
[08:20:08.139037] Epoch: [21]  [220/781]  eta: 0:01:51  lr: 0.000232  training_loss: 0.4239 (0.4235)  mae_loss: 0.2585 (0.2582)  classification_loss: 0.1655 (0.1653)  time: 0.1973  data: 0.0002  max mem: 5511
[08:20:12.074350] Epoch: [21]  [240/781]  eta: 0:01:47  lr: 0.000232  training_loss: 0.4132 (0.4228)  mae_loss: 0.2453 (0.2577)  classification_loss: 0.1589 (0.1652)  time: 0.1967  data: 0.0005  max mem: 5511
[08:20:15.989318] Epoch: [21]  [260/781]  eta: 0:01:43  lr: 0.000232  training_loss: 0.4274 (0.4235)  mae_loss: 0.2653 (0.2582)  classification_loss: 0.1621 (0.1653)  time: 0.1957  data: 0.0002  max mem: 5511
[08:20:19.884997] Epoch: [21]  [280/781]  eta: 0:01:39  lr: 0.000232  training_loss: 0.4327 (0.4251)  mae_loss: 0.2704 (0.2595)  classification_loss: 0.1693 (0.1656)  time: 0.1947  data: 0.0002  max mem: 5511
[08:20:23.801718] Epoch: [21]  [300/781]  eta: 0:01:35  lr: 0.000232  training_loss: 0.4277 (0.4252)  mae_loss: 0.2623 (0.2595)  classification_loss: 0.1667 (0.1657)  time: 0.1958  data: 0.0002  max mem: 5511
[08:20:27.705319] Epoch: [21]  [320/781]  eta: 0:01:31  lr: 0.000232  training_loss: 0.4203 (0.4249)  mae_loss: 0.2561 (0.2591)  classification_loss: 0.1654 (0.1658)  time: 0.1951  data: 0.0003  max mem: 5511
[08:20:31.610365] Epoch: [21]  [340/781]  eta: 0:01:27  lr: 0.000232  training_loss: 0.4318 (0.4254)  mae_loss: 0.2657 (0.2599)  classification_loss: 0.1616 (0.1655)  time: 0.1952  data: 0.0002  max mem: 5511
[08:20:35.518848] Epoch: [21]  [360/781]  eta: 0:01:23  lr: 0.000232  training_loss: 0.4409 (0.4258)  mae_loss: 0.2607 (0.2600)  classification_loss: 0.1700 (0.1657)  time: 0.1954  data: 0.0002  max mem: 5511
[08:20:39.414621] Epoch: [21]  [380/781]  eta: 0:01:19  lr: 0.000232  training_loss: 0.4259 (0.4258)  mae_loss: 0.2618 (0.2599)  classification_loss: 0.1672 (0.1659)  time: 0.1944  data: 0.0004  max mem: 5511
[08:20:43.332071] Epoch: [21]  [400/781]  eta: 0:01:15  lr: 0.000232  training_loss: 0.4293 (0.4261)  mae_loss: 0.2690 (0.2603)  classification_loss: 0.1603 (0.1658)  time: 0.1958  data: 0.0003  max mem: 5511
[08:20:47.255814] Epoch: [21]  [420/781]  eta: 0:01:11  lr: 0.000232  training_loss: 0.4308 (0.4263)  mae_loss: 0.2575 (0.2604)  classification_loss: 0.1701 (0.1659)  time: 0.1961  data: 0.0002  max mem: 5511
[08:20:51.165198] Epoch: [21]  [440/781]  eta: 0:01:07  lr: 0.000232  training_loss: 0.4066 (0.4259)  mae_loss: 0.2486 (0.2602)  classification_loss: 0.1601 (0.1656)  time: 0.1954  data: 0.0002  max mem: 5511
[08:20:55.071839] Epoch: [21]  [460/781]  eta: 0:01:03  lr: 0.000232  training_loss: 0.4139 (0.4252)  mae_loss: 0.2536 (0.2597)  classification_loss: 0.1620 (0.1655)  time: 0.1952  data: 0.0002  max mem: 5511
[08:20:58.966349] Epoch: [21]  [480/781]  eta: 0:00:59  lr: 0.000232  training_loss: 0.4079 (0.4247)  mae_loss: 0.2473 (0.2592)  classification_loss: 0.1658 (0.1655)  time: 0.1946  data: 0.0002  max mem: 5511
[08:21:02.876446] Epoch: [21]  [500/781]  eta: 0:00:55  lr: 0.000232  training_loss: 0.4260 (0.4248)  mae_loss: 0.2620 (0.2592)  classification_loss: 0.1697 (0.1656)  time: 0.1954  data: 0.0002  max mem: 5511
[08:21:06.786929] Epoch: [21]  [520/781]  eta: 0:00:51  lr: 0.000232  training_loss: 0.4422 (0.4254)  mae_loss: 0.2739 (0.2599)  classification_loss: 0.1606 (0.1655)  time: 0.1954  data: 0.0002  max mem: 5511
[08:21:10.701003] Epoch: [21]  [540/781]  eta: 0:00:47  lr: 0.000232  training_loss: 0.4209 (0.4256)  mae_loss: 0.2585 (0.2601)  classification_loss: 0.1603 (0.1655)  time: 0.1956  data: 0.0003  max mem: 5511
[08:21:14.613785] Epoch: [21]  [560/781]  eta: 0:00:43  lr: 0.000231  training_loss: 0.4262 (0.4257)  mae_loss: 0.2640 (0.2603)  classification_loss: 0.1641 (0.1654)  time: 0.1955  data: 0.0002  max mem: 5511
[08:21:18.505433] Epoch: [21]  [580/781]  eta: 0:00:39  lr: 0.000231  training_loss: 0.4304 (0.4256)  mae_loss: 0.2564 (0.2602)  classification_loss: 0.1646 (0.1654)  time: 0.1945  data: 0.0002  max mem: 5511
[08:21:22.421759] Epoch: [21]  [600/781]  eta: 0:00:35  lr: 0.000231  training_loss: 0.4179 (0.4252)  mae_loss: 0.2559 (0.2599)  classification_loss: 0.1608 (0.1653)  time: 0.1957  data: 0.0002  max mem: 5511
[08:21:26.326985] Epoch: [21]  [620/781]  eta: 0:00:31  lr: 0.000231  training_loss: 0.4176 (0.4252)  mae_loss: 0.2602 (0.2599)  classification_loss: 0.1638 (0.1653)  time: 0.1952  data: 0.0003  max mem: 5511
[08:21:30.214767] Epoch: [21]  [640/781]  eta: 0:00:27  lr: 0.000231  training_loss: 0.4124 (0.4249)  mae_loss: 0.2499 (0.2597)  classification_loss: 0.1614 (0.1652)  time: 0.1943  data: 0.0002  max mem: 5511
[08:21:34.126945] Epoch: [21]  [660/781]  eta: 0:00:23  lr: 0.000231  training_loss: 0.4253 (0.4251)  mae_loss: 0.2632 (0.2598)  classification_loss: 0.1683 (0.1653)  time: 0.1955  data: 0.0002  max mem: 5511
[08:21:38.021932] Epoch: [21]  [680/781]  eta: 0:00:19  lr: 0.000231  training_loss: 0.4269 (0.4252)  mae_loss: 0.2581 (0.2599)  classification_loss: 0.1623 (0.1652)  time: 0.1947  data: 0.0002  max mem: 5511
[08:21:41.956820] Epoch: [21]  [700/781]  eta: 0:00:15  lr: 0.000231  training_loss: 0.4343 (0.4254)  mae_loss: 0.2621 (0.2602)  classification_loss: 0.1622 (0.1652)  time: 0.1967  data: 0.0002  max mem: 5511
[08:21:45.859147] Epoch: [21]  [720/781]  eta: 0:00:11  lr: 0.000231  training_loss: 0.4275 (0.4254)  mae_loss: 0.2647 (0.2602)  classification_loss: 0.1653 (0.1652)  time: 0.1950  data: 0.0003  max mem: 5511
[08:21:49.761810] Epoch: [21]  [740/781]  eta: 0:00:08  lr: 0.000231  training_loss: 0.4169 (0.4253)  mae_loss: 0.2551 (0.2602)  classification_loss: 0.1624 (0.1651)  time: 0.1950  data: 0.0004  max mem: 5511
[08:21:53.661386] Epoch: [21]  [760/781]  eta: 0:00:04  lr: 0.000231  training_loss: 0.4178 (0.4253)  mae_loss: 0.2516 (0.2601)  classification_loss: 0.1644 (0.1651)  time: 0.1949  data: 0.0002  max mem: 5511
[08:21:57.557276] Epoch: [21]  [780/781]  eta: 0:00:00  lr: 0.000231  training_loss: 0.4197 (0.4254)  mae_loss: 0.2525 (0.2602)  classification_loss: 0.1699 (0.1652)  time: 0.1947  data: 0.0002  max mem: 5511
[08:21:57.718991] Epoch: [21] Total time: 0:02:33 (0.1967 s / it)
[08:21:57.719557] Averaged stats: lr: 0.000231  training_loss: 0.4197 (0.4254)  mae_loss: 0.2525 (0.2602)  classification_loss: 0.1699 (0.1652)
[08:21:58.406183] Test:  [  0/157]  eta: 0:01:47  testing_loss: 0.9238 (0.9238)  acc1: 73.4375 (73.4375)  acc5: 96.8750 (96.8750)  time: 0.6827  data: 0.6535  max mem: 5511
[08:21:58.689833] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.9756 (0.9775)  acc1: 67.1875 (66.0511)  acc5: 98.4375 (98.1534)  time: 0.0877  data: 0.0596  max mem: 5511
[08:21:58.972141] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.9756 (0.9506)  acc1: 65.6250 (67.7827)  acc5: 98.4375 (97.9167)  time: 0.0281  data: 0.0002  max mem: 5511
[08:21:59.254545] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.9542 (0.9627)  acc1: 70.3125 (67.7419)  acc5: 96.8750 (97.1774)  time: 0.0281  data: 0.0002  max mem: 5511
[08:21:59.538241] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.9372 (0.9603)  acc1: 68.7500 (67.3399)  acc5: 96.8750 (97.2180)  time: 0.0282  data: 0.0002  max mem: 5511
[08:21:59.828604] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.9388 (0.9603)  acc1: 68.7500 (67.8309)  acc5: 98.4375 (97.2733)  time: 0.0286  data: 0.0002  max mem: 5511
[08:22:00.109112] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.9412 (0.9587)  acc1: 67.1875 (67.3668)  acc5: 96.8750 (97.2592)  time: 0.0284  data: 0.0002  max mem: 5511
[08:22:00.391272] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.9346 (0.9549)  acc1: 67.1875 (67.6056)  acc5: 96.8750 (97.2491)  time: 0.0280  data: 0.0002  max mem: 5511
[08:22:00.673617] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.9360 (0.9591)  acc1: 68.7500 (67.7276)  acc5: 96.8750 (97.1836)  time: 0.0281  data: 0.0002  max mem: 5511
[08:22:00.954352] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.9595 (0.9602)  acc1: 67.1875 (67.5996)  acc5: 98.4375 (97.3214)  time: 0.0280  data: 0.0002  max mem: 5511
[08:22:01.236490] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.9425 (0.9601)  acc1: 67.1875 (67.6671)  acc5: 98.4375 (97.3855)  time: 0.0280  data: 0.0002  max mem: 5511
[08:22:01.521302] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.9562 (0.9613)  acc1: 67.1875 (67.6098)  acc5: 98.4375 (97.4099)  time: 0.0282  data: 0.0002  max mem: 5511
[08:22:01.804292] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.9505 (0.9594)  acc1: 67.1875 (67.7944)  acc5: 96.8750 (97.3786)  time: 0.0283  data: 0.0002  max mem: 5511
[08:22:02.090629] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.9430 (0.9622)  acc1: 67.1875 (67.6646)  acc5: 96.8750 (97.3640)  time: 0.0283  data: 0.0002  max mem: 5511
[08:22:02.372860] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.9641 (0.9622)  acc1: 67.1875 (67.7637)  acc5: 96.8750 (97.3848)  time: 0.0283  data: 0.0002  max mem: 5511
[08:22:02.651626] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.9574 (0.9605)  acc1: 67.1875 (67.7256)  acc5: 96.8750 (97.3820)  time: 0.0279  data: 0.0001  max mem: 5511
[08:22:02.801272] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.9408 (0.9611)  acc1: 67.1875 (67.7100)  acc5: 98.4375 (97.4100)  time: 0.0269  data: 0.0001  max mem: 5511
[08:22:02.963337] Test: Total time: 0:00:05 (0.0334 s / it)
[08:22:02.964004] * Acc@1 67.710 Acc@5 97.410 loss 0.961
[08:22:02.964305] Accuracy of the network on the 10000 test images: 67.7%
[08:22:02.964484] Max accuracy: 67.71%
[08:22:03.182100] log_dir: ./output_dir
[08:22:04.012430] Epoch: [22]  [  0/781]  eta: 0:10:47  lr: 0.000231  training_loss: 0.4222 (0.4222)  mae_loss: 0.2735 (0.2735)  classification_loss: 0.1487 (0.1487)  time: 0.8287  data: 0.6228  max mem: 5511
[08:22:07.940417] Epoch: [22]  [ 20/781]  eta: 0:02:52  lr: 0.000231  training_loss: 0.4261 (0.4254)  mae_loss: 0.2626 (0.2619)  classification_loss: 0.1631 (0.1635)  time: 0.1963  data: 0.0002  max mem: 5511
[08:22:11.865484] Epoch: [22]  [ 40/781]  eta: 0:02:36  lr: 0.000231  training_loss: 0.4165 (0.4238)  mae_loss: 0.2523 (0.2602)  classification_loss: 0.1633 (0.1635)  time: 0.1962  data: 0.0002  max mem: 5511
[08:22:15.814605] Epoch: [22]  [ 60/781]  eta: 0:02:29  lr: 0.000231  training_loss: 0.4216 (0.4231)  mae_loss: 0.2590 (0.2592)  classification_loss: 0.1635 (0.1639)  time: 0.1974  data: 0.0002  max mem: 5511
[08:22:19.728039] Epoch: [22]  [ 80/781]  eta: 0:02:23  lr: 0.000231  training_loss: 0.4312 (0.4248)  mae_loss: 0.2669 (0.2611)  classification_loss: 0.1617 (0.1637)  time: 0.1956  data: 0.0002  max mem: 5511
[08:22:23.660930] Epoch: [22]  [100/781]  eta: 0:02:18  lr: 0.000231  training_loss: 0.4314 (0.4254)  mae_loss: 0.2623 (0.2614)  classification_loss: 0.1643 (0.1640)  time: 0.1966  data: 0.0002  max mem: 5511
[08:22:27.560145] Epoch: [22]  [120/781]  eta: 0:02:13  lr: 0.000231  training_loss: 0.4138 (0.4233)  mae_loss: 0.2529 (0.2597)  classification_loss: 0.1615 (0.1636)  time: 0.1949  data: 0.0002  max mem: 5511
[08:22:31.456251] Epoch: [22]  [140/781]  eta: 0:02:08  lr: 0.000230  training_loss: 0.4159 (0.4224)  mae_loss: 0.2548 (0.2592)  classification_loss: 0.1598 (0.1631)  time: 0.1947  data: 0.0004  max mem: 5511
[08:22:35.355140] Epoch: [22]  [160/781]  eta: 0:02:04  lr: 0.000230  training_loss: 0.4294 (0.4225)  mae_loss: 0.2615 (0.2592)  classification_loss: 0.1647 (0.1633)  time: 0.1949  data: 0.0002  max mem: 5511
[08:22:39.291848] Epoch: [22]  [180/781]  eta: 0:01:59  lr: 0.000230  training_loss: 0.4225 (0.4225)  mae_loss: 0.2565 (0.2592)  classification_loss: 0.1680 (0.1633)  time: 0.1967  data: 0.0002  max mem: 5511
[08:22:43.203588] Epoch: [22]  [200/781]  eta: 0:01:55  lr: 0.000230  training_loss: 0.4008 (0.4208)  mae_loss: 0.2453 (0.2579)  classification_loss: 0.1603 (0.1628)  time: 0.1955  data: 0.0002  max mem: 5511
[08:22:47.110312] Epoch: [22]  [220/781]  eta: 0:01:51  lr: 0.000230  training_loss: 0.4138 (0.4209)  mae_loss: 0.2417 (0.2578)  classification_loss: 0.1641 (0.1631)  time: 0.1953  data: 0.0002  max mem: 5511
[08:22:51.028333] Epoch: [22]  [240/781]  eta: 0:01:47  lr: 0.000230  training_loss: 0.4198 (0.4208)  mae_loss: 0.2580 (0.2578)  classification_loss: 0.1622 (0.1631)  time: 0.1958  data: 0.0002  max mem: 5511
[08:22:54.952718] Epoch: [22]  [260/781]  eta: 0:01:43  lr: 0.000230  training_loss: 0.4192 (0.4210)  mae_loss: 0.2582 (0.2578)  classification_loss: 0.1604 (0.1632)  time: 0.1961  data: 0.0002  max mem: 5511
[08:22:58.860257] Epoch: [22]  [280/781]  eta: 0:01:39  lr: 0.000230  training_loss: 0.4223 (0.4214)  mae_loss: 0.2642 (0.2582)  classification_loss: 0.1619 (0.1632)  time: 0.1953  data: 0.0002  max mem: 5511
[08:23:02.774597] Epoch: [22]  [300/781]  eta: 0:01:35  lr: 0.000230  training_loss: 0.4102 (0.4213)  mae_loss: 0.2520 (0.2580)  classification_loss: 0.1651 (0.1633)  time: 0.1956  data: 0.0003  max mem: 5511
[08:23:06.673095] Epoch: [22]  [320/781]  eta: 0:01:31  lr: 0.000230  training_loss: 0.4175 (0.4208)  mae_loss: 0.2492 (0.2574)  classification_loss: 0.1617 (0.1633)  time: 0.1948  data: 0.0002  max mem: 5511
[08:23:10.566024] Epoch: [22]  [340/781]  eta: 0:01:27  lr: 0.000230  training_loss: 0.4173 (0.4207)  mae_loss: 0.2528 (0.2574)  classification_loss: 0.1603 (0.1633)  time: 0.1946  data: 0.0003  max mem: 5511
[08:23:14.463708] Epoch: [22]  [360/781]  eta: 0:01:23  lr: 0.000230  training_loss: 0.4017 (0.4198)  mae_loss: 0.2369 (0.2565)  classification_loss: 0.1632 (0.1633)  time: 0.1948  data: 0.0003  max mem: 5511
[08:23:18.362854] Epoch: [22]  [380/781]  eta: 0:01:19  lr: 0.000230  training_loss: 0.4164 (0.4199)  mae_loss: 0.2438 (0.2566)  classification_loss: 0.1618 (0.1633)  time: 0.1949  data: 0.0002  max mem: 5511
[08:23:22.284107] Epoch: [22]  [400/781]  eta: 0:01:15  lr: 0.000230  training_loss: 0.4197 (0.4201)  mae_loss: 0.2589 (0.2570)  classification_loss: 0.1579 (0.1631)  time: 0.1960  data: 0.0002  max mem: 5511
[08:23:26.222497] Epoch: [22]  [420/781]  eta: 0:01:11  lr: 0.000230  training_loss: 0.4312 (0.4206)  mae_loss: 0.2634 (0.2573)  classification_loss: 0.1649 (0.1633)  time: 0.1968  data: 0.0002  max mem: 5511
[08:23:30.162416] Epoch: [22]  [440/781]  eta: 0:01:07  lr: 0.000230  training_loss: 0.4238 (0.4208)  mae_loss: 0.2617 (0.2576)  classification_loss: 0.1611 (0.1632)  time: 0.1969  data: 0.0002  max mem: 5511
[08:23:34.085726] Epoch: [22]  [460/781]  eta: 0:01:03  lr: 0.000230  training_loss: 0.4174 (0.4210)  mae_loss: 0.2627 (0.2579)  classification_loss: 0.1599 (0.1630)  time: 0.1961  data: 0.0002  max mem: 5511
[08:23:37.983675] Epoch: [22]  [480/781]  eta: 0:00:59  lr: 0.000229  training_loss: 0.4098 (0.4206)  mae_loss: 0.2489 (0.2575)  classification_loss: 0.1611 (0.1631)  time: 0.1948  data: 0.0002  max mem: 5511
[08:23:41.886886] Epoch: [22]  [500/781]  eta: 0:00:55  lr: 0.000229  training_loss: 0.4038 (0.4201)  mae_loss: 0.2471 (0.2571)  classification_loss: 0.1600 (0.1631)  time: 0.1951  data: 0.0002  max mem: 5511
[08:23:45.790259] Epoch: [22]  [520/781]  eta: 0:00:51  lr: 0.000229  training_loss: 0.4428 (0.4204)  mae_loss: 0.2706 (0.2573)  classification_loss: 0.1646 (0.1631)  time: 0.1951  data: 0.0003  max mem: 5511
[08:23:49.725675] Epoch: [22]  [540/781]  eta: 0:00:47  lr: 0.000229  training_loss: 0.4188 (0.4203)  mae_loss: 0.2531 (0.2572)  classification_loss: 0.1659 (0.1631)  time: 0.1967  data: 0.0002  max mem: 5511
[08:23:53.729147] Epoch: [22]  [560/781]  eta: 0:00:43  lr: 0.000229  training_loss: 0.4182 (0.4205)  mae_loss: 0.2588 (0.2575)  classification_loss: 0.1605 (0.1630)  time: 0.2001  data: 0.0002  max mem: 5511
[08:23:57.618793] Epoch: [22]  [580/781]  eta: 0:00:39  lr: 0.000229  training_loss: 0.4047 (0.4199)  mae_loss: 0.2376 (0.2570)  classification_loss: 0.1590 (0.1630)  time: 0.1944  data: 0.0002  max mem: 5511
[08:24:01.575659] Epoch: [22]  [600/781]  eta: 0:00:35  lr: 0.000229  training_loss: 0.4184 (0.4199)  mae_loss: 0.2566 (0.2570)  classification_loss: 0.1617 (0.1630)  time: 0.1977  data: 0.0002  max mem: 5511
[08:24:05.477571] Epoch: [22]  [620/781]  eta: 0:00:31  lr: 0.000229  training_loss: 0.4021 (0.4197)  mae_loss: 0.2527 (0.2569)  classification_loss: 0.1557 (0.1629)  time: 0.1950  data: 0.0002  max mem: 5511
[08:24:09.384359] Epoch: [22]  [640/781]  eta: 0:00:27  lr: 0.000229  training_loss: 0.4094 (0.4196)  mae_loss: 0.2492 (0.2568)  classification_loss: 0.1620 (0.1628)  time: 0.1953  data: 0.0002  max mem: 5511
[08:24:13.306255] Epoch: [22]  [660/781]  eta: 0:00:23  lr: 0.000229  training_loss: 0.4246 (0.4196)  mae_loss: 0.2581 (0.2568)  classification_loss: 0.1620 (0.1628)  time: 0.1960  data: 0.0002  max mem: 5511
[08:24:17.206597] Epoch: [22]  [680/781]  eta: 0:00:19  lr: 0.000229  training_loss: 0.4106 (0.4194)  mae_loss: 0.2514 (0.2566)  classification_loss: 0.1643 (0.1628)  time: 0.1949  data: 0.0002  max mem: 5511
[08:24:21.139077] Epoch: [22]  [700/781]  eta: 0:00:15  lr: 0.000229  training_loss: 0.4227 (0.4194)  mae_loss: 0.2624 (0.2567)  classification_loss: 0.1628 (0.1628)  time: 0.1965  data: 0.0002  max mem: 5511
[08:24:25.056958] Epoch: [22]  [720/781]  eta: 0:00:11  lr: 0.000229  training_loss: 0.4146 (0.4195)  mae_loss: 0.2529 (0.2568)  classification_loss: 0.1602 (0.1628)  time: 0.1958  data: 0.0002  max mem: 5511
[08:24:28.960098] Epoch: [22]  [740/781]  eta: 0:00:08  lr: 0.000229  training_loss: 0.4027 (0.4191)  mae_loss: 0.2361 (0.2564)  classification_loss: 0.1596 (0.1627)  time: 0.1951  data: 0.0002  max mem: 5511
[08:24:32.860314] Epoch: [22]  [760/781]  eta: 0:00:04  lr: 0.000229  training_loss: 0.4227 (0.4192)  mae_loss: 0.2587 (0.2564)  classification_loss: 0.1614 (0.1627)  time: 0.1949  data: 0.0002  max mem: 5511
[08:24:36.740599] Epoch: [22]  [780/781]  eta: 0:00:00  lr: 0.000229  training_loss: 0.4151 (0.4192)  mae_loss: 0.2466 (0.2565)  classification_loss: 0.1614 (0.1628)  time: 0.1939  data: 0.0002  max mem: 5511
[08:24:36.922128] Epoch: [22] Total time: 0:02:33 (0.1968 s / it)
[08:24:36.922614] Averaged stats: lr: 0.000229  training_loss: 0.4151 (0.4192)  mae_loss: 0.2466 (0.2565)  classification_loss: 0.1614 (0.1628)
[08:24:37.496789] Test:  [  0/157]  eta: 0:01:29  testing_loss: 0.9496 (0.9496)  acc1: 70.3125 (70.3125)  acc5: 93.7500 (93.7500)  time: 0.5694  data: 0.5391  max mem: 5511
[08:24:37.800529] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.9496 (0.9667)  acc1: 70.3125 (68.1818)  acc5: 96.8750 (97.4432)  time: 0.0791  data: 0.0493  max mem: 5511
[08:24:38.082704] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.8981 (0.9313)  acc1: 70.3125 (69.7173)  acc5: 96.8750 (97.8423)  time: 0.0291  data: 0.0002  max mem: 5511
[08:24:38.365551] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.9581 (0.9518)  acc1: 70.3125 (68.8508)  acc5: 96.8750 (97.6815)  time: 0.0281  data: 0.0002  max mem: 5511
[08:24:38.652394] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.9382 (0.9475)  acc1: 68.7500 (68.7881)  acc5: 98.4375 (97.7515)  time: 0.0282  data: 0.0002  max mem: 5511
[08:24:38.944137] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.9289 (0.9488)  acc1: 67.1875 (68.2598)  acc5: 98.4375 (97.8248)  time: 0.0287  data: 0.0002  max mem: 5511
[08:24:39.227816] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.9311 (0.9451)  acc1: 67.1875 (68.1609)  acc5: 96.8750 (97.7715)  time: 0.0287  data: 0.0002  max mem: 5511
[08:24:39.510399] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.8844 (0.9350)  acc1: 70.3125 (68.8820)  acc5: 98.4375 (97.8433)  time: 0.0281  data: 0.0002  max mem: 5511
[08:24:39.795060] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.8831 (0.9384)  acc1: 70.3125 (68.6728)  acc5: 98.4375 (97.6852)  time: 0.0282  data: 0.0002  max mem: 5511
[08:24:40.077479] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.9318 (0.9387)  acc1: 65.6250 (68.5440)  acc5: 98.4375 (97.7507)  time: 0.0282  data: 0.0002  max mem: 5511
[08:24:40.368118] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.9318 (0.9388)  acc1: 67.1875 (68.5489)  acc5: 98.4375 (97.8032)  time: 0.0285  data: 0.0002  max mem: 5511
[08:24:40.651901] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.9297 (0.9401)  acc1: 68.7500 (68.5670)  acc5: 98.4375 (97.8181)  time: 0.0286  data: 0.0002  max mem: 5511
[08:24:40.940792] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.9049 (0.9361)  acc1: 70.3125 (68.7113)  acc5: 98.4375 (97.8306)  time: 0.0285  data: 0.0002  max mem: 5511
[08:24:41.222202] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.9341 (0.9384)  acc1: 68.7500 (68.5711)  acc5: 98.4375 (97.8173)  time: 0.0284  data: 0.0002  max mem: 5511
[08:24:41.502499] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.9576 (0.9367)  acc1: 70.3125 (68.8276)  acc5: 98.4375 (97.8391)  time: 0.0280  data: 0.0001  max mem: 5511
[08:24:41.782570] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.9200 (0.9349)  acc1: 70.3125 (68.8845)  acc5: 98.4375 (97.8373)  time: 0.0279  data: 0.0001  max mem: 5511
[08:24:41.932866] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.9075 (0.9357)  acc1: 70.3125 (68.8100)  acc5: 98.4375 (97.8500)  time: 0.0269  data: 0.0001  max mem: 5511
[08:24:42.098660] Test: Total time: 0:00:05 (0.0329 s / it)
[08:24:42.100153] * Acc@1 68.810 Acc@5 97.850 loss 0.936
[08:24:42.100629] Accuracy of the network on the 10000 test images: 68.8%
[08:24:42.100893] Max accuracy: 68.81%
[08:24:42.278915] log_dir: ./output_dir
[08:24:43.202855] Epoch: [23]  [  0/781]  eta: 0:12:00  lr: 0.000229  training_loss: 0.4418 (0.4418)  mae_loss: 0.2797 (0.2797)  classification_loss: 0.1621 (0.1621)  time: 0.9222  data: 0.6994  max mem: 5511
[08:24:47.127683] Epoch: [23]  [ 20/781]  eta: 0:02:55  lr: 0.000229  training_loss: 0.4262 (0.4227)  mae_loss: 0.2631 (0.2600)  classification_loss: 0.1606 (0.1627)  time: 0.1961  data: 0.0002  max mem: 5511
[08:24:51.038243] Epoch: [23]  [ 40/781]  eta: 0:02:38  lr: 0.000228  training_loss: 0.4125 (0.4209)  mae_loss: 0.2519 (0.2586)  classification_loss: 0.1607 (0.1623)  time: 0.1955  data: 0.0002  max mem: 5511
[08:24:54.938475] Epoch: [23]  [ 60/781]  eta: 0:02:29  lr: 0.000228  training_loss: 0.4232 (0.4226)  mae_loss: 0.2643 (0.2598)  classification_loss: 0.1629 (0.1628)  time: 0.1949  data: 0.0003  max mem: 5511
[08:24:58.832431] Epoch: [23]  [ 80/781]  eta: 0:02:23  lr: 0.000228  training_loss: 0.4158 (0.4225)  mae_loss: 0.2576 (0.2606)  classification_loss: 0.1581 (0.1619)  time: 0.1946  data: 0.0002  max mem: 5511
[08:25:02.723429] Epoch: [23]  [100/781]  eta: 0:02:17  lr: 0.000228  training_loss: 0.4040 (0.4204)  mae_loss: 0.2472 (0.2588)  classification_loss: 0.1595 (0.1616)  time: 0.1945  data: 0.0004  max mem: 5511
[08:25:06.643021] Epoch: [23]  [120/781]  eta: 0:02:13  lr: 0.000228  training_loss: 0.3912 (0.4165)  mae_loss: 0.2284 (0.2550)  classification_loss: 0.1613 (0.1614)  time: 0.1959  data: 0.0002  max mem: 5511
[08:25:10.558597] Epoch: [23]  [140/781]  eta: 0:02:08  lr: 0.000228  training_loss: 0.4137 (0.4154)  mae_loss: 0.2549 (0.2543)  classification_loss: 0.1573 (0.1611)  time: 0.1957  data: 0.0002  max mem: 5511
[08:25:14.456965] Epoch: [23]  [160/781]  eta: 0:02:04  lr: 0.000228  training_loss: 0.4056 (0.4152)  mae_loss: 0.2468 (0.2538)  classification_loss: 0.1614 (0.1614)  time: 0.1948  data: 0.0002  max mem: 5511
[08:25:18.368725] Epoch: [23]  [180/781]  eta: 0:01:59  lr: 0.000228  training_loss: 0.4177 (0.4161)  mae_loss: 0.2598 (0.2547)  classification_loss: 0.1602 (0.1615)  time: 0.1955  data: 0.0003  max mem: 5511
[08:25:22.281989] Epoch: [23]  [200/781]  eta: 0:01:55  lr: 0.000228  training_loss: 0.4046 (0.4148)  mae_loss: 0.2400 (0.2535)  classification_loss: 0.1581 (0.1613)  time: 0.1956  data: 0.0003  max mem: 5511
[08:25:26.177920] Epoch: [23]  [220/781]  eta: 0:01:51  lr: 0.000228  training_loss: 0.4049 (0.4146)  mae_loss: 0.2502 (0.2531)  classification_loss: 0.1633 (0.1615)  time: 0.1947  data: 0.0002  max mem: 5511
[08:25:30.172301] Epoch: [23]  [240/781]  eta: 0:01:47  lr: 0.000228  training_loss: 0.4197 (0.4151)  mae_loss: 0.2622 (0.2535)  classification_loss: 0.1633 (0.1617)  time: 0.1997  data: 0.0002  max mem: 5511
[08:25:34.090236] Epoch: [23]  [260/781]  eta: 0:01:43  lr: 0.000228  training_loss: 0.4049 (0.4148)  mae_loss: 0.2470 (0.2531)  classification_loss: 0.1603 (0.1617)  time: 0.1958  data: 0.0003  max mem: 5511
[08:25:37.988662] Epoch: [23]  [280/781]  eta: 0:01:39  lr: 0.000228  training_loss: 0.4175 (0.4151)  mae_loss: 0.2511 (0.2535)  classification_loss: 0.1604 (0.1617)  time: 0.1949  data: 0.0002  max mem: 5511
[08:25:41.904835] Epoch: [23]  [300/781]  eta: 0:01:35  lr: 0.000228  training_loss: 0.4091 (0.4154)  mae_loss: 0.2478 (0.2537)  classification_loss: 0.1639 (0.1617)  time: 0.1957  data: 0.0002  max mem: 5511
[08:25:45.823664] Epoch: [23]  [320/781]  eta: 0:01:31  lr: 0.000228  training_loss: 0.4117 (0.4158)  mae_loss: 0.2493 (0.2539)  classification_loss: 0.1626 (0.1618)  time: 0.1958  data: 0.0004  max mem: 5511
[08:25:49.775217] Epoch: [23]  [340/781]  eta: 0:01:27  lr: 0.000228  training_loss: 0.4148 (0.4152)  mae_loss: 0.2468 (0.2537)  classification_loss: 0.1547 (0.1615)  time: 0.1974  data: 0.0003  max mem: 5511
[08:25:53.683477] Epoch: [23]  [360/781]  eta: 0:01:23  lr: 0.000228  training_loss: 0.4368 (0.4162)  mae_loss: 0.2695 (0.2546)  classification_loss: 0.1638 (0.1617)  time: 0.1953  data: 0.0002  max mem: 5511
[08:25:57.592336] Epoch: [23]  [380/781]  eta: 0:01:19  lr: 0.000227  training_loss: 0.4230 (0.4168)  mae_loss: 0.2690 (0.2553)  classification_loss: 0.1605 (0.1615)  time: 0.1953  data: 0.0002  max mem: 5511
[08:26:01.499987] Epoch: [23]  [400/781]  eta: 0:01:15  lr: 0.000227  training_loss: 0.4099 (0.4166)  mae_loss: 0.2539 (0.2552)  classification_loss: 0.1566 (0.1614)  time: 0.1953  data: 0.0002  max mem: 5511
[08:26:05.403265] Epoch: [23]  [420/781]  eta: 0:01:11  lr: 0.000227  training_loss: 0.4041 (0.4162)  mae_loss: 0.2394 (0.2548)  classification_loss: 0.1618 (0.1614)  time: 0.1951  data: 0.0002  max mem: 5511
[08:26:09.336081] Epoch: [23]  [440/781]  eta: 0:01:07  lr: 0.000227  training_loss: 0.4082 (0.4157)  mae_loss: 0.2445 (0.2542)  classification_loss: 0.1607 (0.1614)  time: 0.1965  data: 0.0002  max mem: 5511
[08:26:13.241190] Epoch: [23]  [460/781]  eta: 0:01:03  lr: 0.000227  training_loss: 0.4121 (0.4154)  mae_loss: 0.2531 (0.2539)  classification_loss: 0.1627 (0.1614)  time: 0.1951  data: 0.0002  max mem: 5511
[08:26:17.149618] Epoch: [23]  [480/781]  eta: 0:00:59  lr: 0.000227  training_loss: 0.4099 (0.4153)  mae_loss: 0.2521 (0.2538)  classification_loss: 0.1607 (0.1615)  time: 0.1953  data: 0.0002  max mem: 5511
[08:26:21.057493] Epoch: [23]  [500/781]  eta: 0:00:55  lr: 0.000227  training_loss: 0.4045 (0.4150)  mae_loss: 0.2389 (0.2535)  classification_loss: 0.1610 (0.1615)  time: 0.1953  data: 0.0002  max mem: 5511
[08:26:24.942493] Epoch: [23]  [520/781]  eta: 0:00:51  lr: 0.000227  training_loss: 0.4085 (0.4148)  mae_loss: 0.2503 (0.2533)  classification_loss: 0.1583 (0.1614)  time: 0.1941  data: 0.0002  max mem: 5511
[08:26:28.832409] Epoch: [23]  [540/781]  eta: 0:00:47  lr: 0.000227  training_loss: 0.4116 (0.4149)  mae_loss: 0.2550 (0.2534)  classification_loss: 0.1616 (0.1614)  time: 0.1944  data: 0.0002  max mem: 5511
[08:26:32.733187] Epoch: [23]  [560/781]  eta: 0:00:43  lr: 0.000227  training_loss: 0.4059 (0.4149)  mae_loss: 0.2425 (0.2535)  classification_loss: 0.1576 (0.1614)  time: 0.1949  data: 0.0002  max mem: 5511
[08:26:36.632385] Epoch: [23]  [580/781]  eta: 0:00:39  lr: 0.000227  training_loss: 0.4116 (0.4147)  mae_loss: 0.2512 (0.2534)  classification_loss: 0.1583 (0.1613)  time: 0.1948  data: 0.0002  max mem: 5511
[08:26:40.621265] Epoch: [23]  [600/781]  eta: 0:00:35  lr: 0.000227  training_loss: 0.4139 (0.4145)  mae_loss: 0.2532 (0.2534)  classification_loss: 0.1533 (0.1612)  time: 0.1993  data: 0.0002  max mem: 5511
[08:26:44.564185] Epoch: [23]  [620/781]  eta: 0:00:31  lr: 0.000227  training_loss: 0.4282 (0.4148)  mae_loss: 0.2682 (0.2536)  classification_loss: 0.1611 (0.1612)  time: 0.1970  data: 0.0002  max mem: 5511
[08:26:48.460141] Epoch: [23]  [640/781]  eta: 0:00:27  lr: 0.000227  training_loss: 0.4096 (0.4146)  mae_loss: 0.2504 (0.2534)  classification_loss: 0.1550 (0.1611)  time: 0.1947  data: 0.0002  max mem: 5511
[08:26:52.371143] Epoch: [23]  [660/781]  eta: 0:00:23  lr: 0.000227  training_loss: 0.4080 (0.4144)  mae_loss: 0.2481 (0.2533)  classification_loss: 0.1571 (0.1611)  time: 0.1955  data: 0.0003  max mem: 5511
[08:26:56.275350] Epoch: [23]  [680/781]  eta: 0:00:19  lr: 0.000227  training_loss: 0.4097 (0.4144)  mae_loss: 0.2565 (0.2533)  classification_loss: 0.1614 (0.1611)  time: 0.1951  data: 0.0003  max mem: 5511
[08:27:00.187520] Epoch: [23]  [700/781]  eta: 0:00:15  lr: 0.000226  training_loss: 0.4150 (0.4145)  mae_loss: 0.2561 (0.2535)  classification_loss: 0.1578 (0.1611)  time: 0.1955  data: 0.0002  max mem: 5511
[08:27:04.082165] Epoch: [23]  [720/781]  eta: 0:00:11  lr: 0.000226  training_loss: 0.4170 (0.4144)  mae_loss: 0.2533 (0.2533)  classification_loss: 0.1588 (0.1611)  time: 0.1947  data: 0.0002  max mem: 5511
[08:27:07.984012] Epoch: [23]  [740/781]  eta: 0:00:08  lr: 0.000226  training_loss: 0.4130 (0.4142)  mae_loss: 0.2457 (0.2531)  classification_loss: 0.1619 (0.1611)  time: 0.1949  data: 0.0002  max mem: 5511
[08:27:11.882305] Epoch: [23]  [760/781]  eta: 0:00:04  lr: 0.000226  training_loss: 0.4122 (0.4141)  mae_loss: 0.2548 (0.2530)  classification_loss: 0.1604 (0.1611)  time: 0.1948  data: 0.0003  max mem: 5511
[08:27:15.766304] Epoch: [23]  [780/781]  eta: 0:00:00  lr: 0.000226  training_loss: 0.4284 (0.4145)  mae_loss: 0.2672 (0.2534)  classification_loss: 0.1596 (0.1611)  time: 0.1941  data: 0.0001  max mem: 5511
[08:27:15.905548] Epoch: [23] Total time: 0:02:33 (0.1967 s / it)
[08:27:15.906105] Averaged stats: lr: 0.000226  training_loss: 0.4284 (0.4145)  mae_loss: 0.2672 (0.2534)  classification_loss: 0.1596 (0.1611)
[08:27:16.472865] Test:  [  0/157]  eta: 0:01:28  testing_loss: 0.9186 (0.9186)  acc1: 71.8750 (71.8750)  acc5: 95.3125 (95.3125)  time: 0.5624  data: 0.5295  max mem: 5511
[08:27:16.763917] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.9186 (0.9297)  acc1: 70.3125 (68.3239)  acc5: 98.4375 (97.7273)  time: 0.0774  data: 0.0483  max mem: 5511
[08:27:17.049445] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.9155 (0.9070)  acc1: 70.3125 (70.3869)  acc5: 98.4375 (97.9167)  time: 0.0287  data: 0.0002  max mem: 5511
[08:27:17.335887] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.9042 (0.9190)  acc1: 70.3125 (70.0101)  acc5: 98.4375 (97.5302)  time: 0.0285  data: 0.0001  max mem: 5511
[08:27:17.619255] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.8949 (0.9159)  acc1: 70.3125 (69.8171)  acc5: 98.4375 (97.5991)  time: 0.0283  data: 0.0001  max mem: 5511
[08:27:17.908158] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.8949 (0.9133)  acc1: 70.3125 (69.7917)  acc5: 98.4375 (97.5184)  time: 0.0285  data: 0.0001  max mem: 5511
[08:27:18.196052] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.8975 (0.9081)  acc1: 70.3125 (69.6977)  acc5: 98.4375 (97.5666)  time: 0.0287  data: 0.0002  max mem: 5511
[08:27:18.481184] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.8585 (0.8986)  acc1: 70.3125 (70.1585)  acc5: 98.4375 (97.5352)  time: 0.0285  data: 0.0002  max mem: 5511
[08:27:18.767132] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.8657 (0.9012)  acc1: 70.3125 (70.1003)  acc5: 98.4375 (97.6080)  time: 0.0284  data: 0.0002  max mem: 5511
[08:27:19.048238] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.8876 (0.9002)  acc1: 68.7500 (70.1236)  acc5: 98.4375 (97.5962)  time: 0.0282  data: 0.0002  max mem: 5511
[08:27:19.339221] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.8876 (0.9013)  acc1: 70.3125 (70.1887)  acc5: 98.4375 (97.6795)  time: 0.0285  data: 0.0001  max mem: 5511
[08:27:19.625920] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.8996 (0.9035)  acc1: 70.3125 (70.0450)  acc5: 98.4375 (97.7618)  time: 0.0288  data: 0.0002  max mem: 5511
[08:27:19.915849] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.8659 (0.8991)  acc1: 70.3125 (70.1059)  acc5: 98.4375 (97.7273)  time: 0.0287  data: 0.0002  max mem: 5511
[08:27:20.200317] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.9239 (0.9033)  acc1: 68.7500 (69.8712)  acc5: 96.8750 (97.7099)  time: 0.0286  data: 0.0002  max mem: 5511
[08:27:20.481174] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.9239 (0.9041)  acc1: 68.7500 (70.0022)  acc5: 98.4375 (97.6950)  time: 0.0282  data: 0.0001  max mem: 5511
[08:27:20.760008] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.9044 (0.9026)  acc1: 71.8750 (70.0435)  acc5: 96.8750 (97.6511)  time: 0.0279  data: 0.0001  max mem: 5511
[08:27:20.910086] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.8877 (0.9024)  acc1: 68.7500 (69.9900)  acc5: 98.4375 (97.7100)  time: 0.0269  data: 0.0001  max mem: 5511
[08:27:21.068630] Test: Total time: 0:00:05 (0.0329 s / it)
[08:27:21.069232] * Acc@1 69.990 Acc@5 97.710 loss 0.902
[08:27:21.069528] Accuracy of the network on the 10000 test images: 70.0%
[08:27:21.069850] Max accuracy: 69.99%
[08:27:21.282117] log_dir: ./output_dir
[08:27:22.119242] Epoch: [24]  [  0/781]  eta: 0:10:52  lr: 0.000226  training_loss: 0.4263 (0.4263)  mae_loss: 0.2689 (0.2689)  classification_loss: 0.1574 (0.1574)  time: 0.8353  data: 0.6247  max mem: 5511
[08:27:26.060600] Epoch: [24]  [ 20/781]  eta: 0:02:53  lr: 0.000226  training_loss: 0.4026 (0.4047)  mae_loss: 0.2394 (0.2453)  classification_loss: 0.1572 (0.1594)  time: 0.1969  data: 0.0003  max mem: 5511
[08:27:29.962822] Epoch: [24]  [ 40/781]  eta: 0:02:36  lr: 0.000226  training_loss: 0.4045 (0.4072)  mae_loss: 0.2447 (0.2483)  classification_loss: 0.1574 (0.1589)  time: 0.1950  data: 0.0002  max mem: 5511
[08:27:33.870341] Epoch: [24]  [ 60/781]  eta: 0:02:28  lr: 0.000226  training_loss: 0.3980 (0.4067)  mae_loss: 0.2378 (0.2472)  classification_loss: 0.1596 (0.1595)  time: 0.1953  data: 0.0002  max mem: 5511
[08:27:37.763554] Epoch: [24]  [ 80/781]  eta: 0:02:22  lr: 0.000226  training_loss: 0.4132 (0.4102)  mae_loss: 0.2590 (0.2505)  classification_loss: 0.1597 (0.1597)  time: 0.1946  data: 0.0003  max mem: 5511
[08:27:41.671720] Epoch: [24]  [100/781]  eta: 0:02:17  lr: 0.000226  training_loss: 0.4017 (0.4097)  mae_loss: 0.2420 (0.2496)  classification_loss: 0.1616 (0.1601)  time: 0.1953  data: 0.0003  max mem: 5511
[08:27:45.585173] Epoch: [24]  [120/781]  eta: 0:02:12  lr: 0.000226  training_loss: 0.3962 (0.4082)  mae_loss: 0.2412 (0.2485)  classification_loss: 0.1555 (0.1596)  time: 0.1956  data: 0.0003  max mem: 5511
[08:27:49.471263] Epoch: [24]  [140/781]  eta: 0:02:08  lr: 0.000226  training_loss: 0.4176 (0.4088)  mae_loss: 0.2556 (0.2495)  classification_loss: 0.1574 (0.1592)  time: 0.1942  data: 0.0002  max mem: 5511
[08:27:53.380984] Epoch: [24]  [160/781]  eta: 0:02:03  lr: 0.000226  training_loss: 0.4013 (0.4087)  mae_loss: 0.2436 (0.2496)  classification_loss: 0.1562 (0.1591)  time: 0.1954  data: 0.0002  max mem: 5511
[08:27:57.313324] Epoch: [24]  [180/781]  eta: 0:01:59  lr: 0.000226  training_loss: 0.4192 (0.4101)  mae_loss: 0.2573 (0.2510)  classification_loss: 0.1564 (0.1590)  time: 0.1965  data: 0.0002  max mem: 5511
[08:28:01.237090] Epoch: [24]  [200/781]  eta: 0:01:55  lr: 0.000226  training_loss: 0.4009 (0.4088)  mae_loss: 0.2404 (0.2499)  classification_loss: 0.1577 (0.1589)  time: 0.1961  data: 0.0004  max mem: 5511
[08:28:05.168396] Epoch: [24]  [220/781]  eta: 0:01:51  lr: 0.000226  training_loss: 0.4258 (0.4099)  mae_loss: 0.2578 (0.2505)  classification_loss: 0.1634 (0.1594)  time: 0.1965  data: 0.0004  max mem: 5511
[08:28:09.077367] Epoch: [24]  [240/781]  eta: 0:01:47  lr: 0.000225  training_loss: 0.4068 (0.4098)  mae_loss: 0.2469 (0.2502)  classification_loss: 0.1601 (0.1596)  time: 0.1953  data: 0.0002  max mem: 5511
[08:28:12.965434] Epoch: [24]  [260/781]  eta: 0:01:43  lr: 0.000225  training_loss: 0.4010 (0.4091)  mae_loss: 0.2438 (0.2494)  classification_loss: 0.1600 (0.1597)  time: 0.1943  data: 0.0002  max mem: 5511
[08:28:16.870054] Epoch: [24]  [280/781]  eta: 0:01:39  lr: 0.000225  training_loss: 0.4097 (0.4093)  mae_loss: 0.2487 (0.2498)  classification_loss: 0.1564 (0.1596)  time: 0.1952  data: 0.0002  max mem: 5511
[08:28:20.768283] Epoch: [24]  [300/781]  eta: 0:01:35  lr: 0.000225  training_loss: 0.4096 (0.4092)  mae_loss: 0.2498 (0.2498)  classification_loss: 0.1551 (0.1594)  time: 0.1948  data: 0.0002  max mem: 5511
[08:28:24.670050] Epoch: [24]  [320/781]  eta: 0:01:30  lr: 0.000225  training_loss: 0.4065 (0.4092)  mae_loss: 0.2446 (0.2495)  classification_loss: 0.1594 (0.1597)  time: 0.1950  data: 0.0002  max mem: 5511
[08:28:28.573175] Epoch: [24]  [340/781]  eta: 0:01:26  lr: 0.000225  training_loss: 0.4206 (0.4097)  mae_loss: 0.2609 (0.2500)  classification_loss: 0.1589 (0.1597)  time: 0.1951  data: 0.0002  max mem: 5511
[08:28:32.474371] Epoch: [24]  [360/781]  eta: 0:01:22  lr: 0.000225  training_loss: 0.4082 (0.4098)  mae_loss: 0.2393 (0.2500)  classification_loss: 0.1591 (0.1598)  time: 0.1950  data: 0.0003  max mem: 5511
[08:28:36.388447] Epoch: [24]  [380/781]  eta: 0:01:19  lr: 0.000225  training_loss: 0.4005 (0.4099)  mae_loss: 0.2540 (0.2501)  classification_loss: 0.1578 (0.1598)  time: 0.1956  data: 0.0003  max mem: 5511
[08:28:40.285328] Epoch: [24]  [400/781]  eta: 0:01:15  lr: 0.000225  training_loss: 0.4087 (0.4098)  mae_loss: 0.2471 (0.2500)  classification_loss: 0.1622 (0.1599)  time: 0.1948  data: 0.0002  max mem: 5511
[08:28:44.189340] Epoch: [24]  [420/781]  eta: 0:01:11  lr: 0.000225  training_loss: 0.4129 (0.4098)  mae_loss: 0.2502 (0.2501)  classification_loss: 0.1555 (0.1598)  time: 0.1951  data: 0.0002  max mem: 5511
[08:28:48.095428] Epoch: [24]  [440/781]  eta: 0:01:07  lr: 0.000225  training_loss: 0.3907 (0.4093)  mae_loss: 0.2365 (0.2497)  classification_loss: 0.1535 (0.1596)  time: 0.1952  data: 0.0004  max mem: 5511
[08:28:52.022174] Epoch: [24]  [460/781]  eta: 0:01:03  lr: 0.000225  training_loss: 0.4008 (0.4092)  mae_loss: 0.2453 (0.2496)  classification_loss: 0.1559 (0.1596)  time: 0.1963  data: 0.0002  max mem: 5511
[08:28:55.950452] Epoch: [24]  [480/781]  eta: 0:00:59  lr: 0.000225  training_loss: 0.4074 (0.4089)  mae_loss: 0.2408 (0.2491)  classification_loss: 0.1632 (0.1598)  time: 0.1963  data: 0.0002  max mem: 5511
[08:28:59.866880] Epoch: [24]  [500/781]  eta: 0:00:55  lr: 0.000225  training_loss: 0.3926 (0.4087)  mae_loss: 0.2470 (0.2491)  classification_loss: 0.1560 (0.1597)  time: 0.1957  data: 0.0003  max mem: 5511
[08:29:03.765914] Epoch: [24]  [520/781]  eta: 0:00:51  lr: 0.000225  training_loss: 0.4019 (0.4087)  mae_loss: 0.2510 (0.2491)  classification_loss: 0.1577 (0.1596)  time: 0.1949  data: 0.0002  max mem: 5511
[08:29:07.713847] Epoch: [24]  [540/781]  eta: 0:00:47  lr: 0.000225  training_loss: 0.3958 (0.4085)  mae_loss: 0.2468 (0.2489)  classification_loss: 0.1603 (0.1596)  time: 0.1973  data: 0.0002  max mem: 5511
[08:29:11.614154] Epoch: [24]  [560/781]  eta: 0:00:43  lr: 0.000224  training_loss: 0.3908 (0.4080)  mae_loss: 0.2445 (0.2485)  classification_loss: 0.1532 (0.1595)  time: 0.1949  data: 0.0002  max mem: 5511
[08:29:15.518398] Epoch: [24]  [580/781]  eta: 0:00:39  lr: 0.000224  training_loss: 0.4152 (0.4083)  mae_loss: 0.2573 (0.2488)  classification_loss: 0.1580 (0.1594)  time: 0.1951  data: 0.0002  max mem: 5511
[08:29:19.431250] Epoch: [24]  [600/781]  eta: 0:00:35  lr: 0.000224  training_loss: 0.4013 (0.4083)  mae_loss: 0.2442 (0.2489)  classification_loss: 0.1578 (0.1594)  time: 0.1956  data: 0.0002  max mem: 5511
[08:29:23.339407] Epoch: [24]  [620/781]  eta: 0:00:31  lr: 0.000224  training_loss: 0.4166 (0.4082)  mae_loss: 0.2563 (0.2488)  classification_loss: 0.1621 (0.1594)  time: 0.1953  data: 0.0002  max mem: 5511
[08:29:27.246505] Epoch: [24]  [640/781]  eta: 0:00:27  lr: 0.000224  training_loss: 0.3996 (0.4081)  mae_loss: 0.2411 (0.2488)  classification_loss: 0.1583 (0.1594)  time: 0.1953  data: 0.0004  max mem: 5511
[08:29:31.158183] Epoch: [24]  [660/781]  eta: 0:00:23  lr: 0.000224  training_loss: 0.4058 (0.4080)  mae_loss: 0.2409 (0.2485)  classification_loss: 0.1596 (0.1594)  time: 0.1955  data: 0.0003  max mem: 5511
[08:29:35.083721] Epoch: [24]  [680/781]  eta: 0:00:19  lr: 0.000224  training_loss: 0.4145 (0.4081)  mae_loss: 0.2478 (0.2487)  classification_loss: 0.1579 (0.1595)  time: 0.1962  data: 0.0002  max mem: 5511
[08:29:38.992081] Epoch: [24]  [700/781]  eta: 0:00:15  lr: 0.000224  training_loss: 0.4035 (0.4080)  mae_loss: 0.2459 (0.2486)  classification_loss: 0.1573 (0.1595)  time: 0.1953  data: 0.0002  max mem: 5511
[08:29:42.893934] Epoch: [24]  [720/781]  eta: 0:00:11  lr: 0.000224  training_loss: 0.4012 (0.4079)  mae_loss: 0.2412 (0.2484)  classification_loss: 0.1591 (0.1595)  time: 0.1950  data: 0.0002  max mem: 5511
[08:29:46.854989] Epoch: [24]  [740/781]  eta: 0:00:08  lr: 0.000224  training_loss: 0.3854 (0.4078)  mae_loss: 0.2382 (0.2484)  classification_loss: 0.1556 (0.1594)  time: 0.1980  data: 0.0002  max mem: 5511
[08:29:50.777123] Epoch: [24]  [760/781]  eta: 0:00:04  lr: 0.000224  training_loss: 0.4118 (0.4081)  mae_loss: 0.2507 (0.2487)  classification_loss: 0.1611 (0.1594)  time: 0.1960  data: 0.0002  max mem: 5511
[08:29:54.675689] Epoch: [24]  [780/781]  eta: 0:00:00  lr: 0.000224  training_loss: 0.4095 (0.4082)  mae_loss: 0.2587 (0.2488)  classification_loss: 0.1595 (0.1594)  time: 0.1948  data: 0.0002  max mem: 5511
[08:29:54.813300] Epoch: [24] Total time: 0:02:33 (0.1966 s / it)
[08:29:54.813768] Averaged stats: lr: 0.000224  training_loss: 0.4095 (0.4082)  mae_loss: 0.2587 (0.2488)  classification_loss: 0.1595 (0.1594)
[08:29:55.512789] Test:  [  0/157]  eta: 0:01:48  testing_loss: 0.8367 (0.8367)  acc1: 71.8750 (71.8750)  acc5: 93.7500 (93.7500)  time: 0.6937  data: 0.6556  max mem: 5511
[08:29:55.798122] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.8683 (0.9180)  acc1: 68.7500 (67.6136)  acc5: 98.4375 (98.0114)  time: 0.0887  data: 0.0597  max mem: 5511
[08:29:56.081435] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.8597 (0.8879)  acc1: 70.3125 (69.4940)  acc5: 98.4375 (97.8423)  time: 0.0282  data: 0.0002  max mem: 5511
[08:29:56.370232] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.8655 (0.8979)  acc1: 70.3125 (69.0524)  acc5: 96.8750 (97.5806)  time: 0.0285  data: 0.0004  max mem: 5511
[08:29:56.656556] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.8655 (0.8953)  acc1: 70.3125 (69.3598)  acc5: 98.4375 (97.6753)  time: 0.0286  data: 0.0004  max mem: 5511
[08:29:56.940087] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.8494 (0.8896)  acc1: 70.3125 (69.6078)  acc5: 98.4375 (97.7022)  time: 0.0284  data: 0.0002  max mem: 5511
[08:29:57.222769] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.8618 (0.8846)  acc1: 70.3125 (69.4160)  acc5: 98.4375 (97.6947)  time: 0.0282  data: 0.0002  max mem: 5511
[08:29:57.508716] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.8312 (0.8767)  acc1: 70.3125 (69.9164)  acc5: 98.4375 (97.7553)  time: 0.0283  data: 0.0003  max mem: 5511
[08:29:57.800108] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.8285 (0.8799)  acc1: 70.3125 (69.9460)  acc5: 98.4375 (97.7623)  time: 0.0287  data: 0.0003  max mem: 5511
[08:29:58.088169] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.8612 (0.8778)  acc1: 70.3125 (70.0549)  acc5: 98.4375 (97.7679)  time: 0.0288  data: 0.0002  max mem: 5511
[08:29:58.371332] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.8839 (0.8816)  acc1: 68.7500 (69.8020)  acc5: 98.4375 (97.8342)  time: 0.0284  data: 0.0002  max mem: 5511
[08:29:58.655880] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.9175 (0.8830)  acc1: 67.1875 (69.7917)  acc5: 98.4375 (97.8322)  time: 0.0283  data: 0.0002  max mem: 5511
[08:29:58.945424] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.8379 (0.8792)  acc1: 70.3125 (69.9380)  acc5: 98.4375 (97.8435)  time: 0.0286  data: 0.0002  max mem: 5511
[08:29:59.228334] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.8910 (0.8827)  acc1: 68.7500 (69.7758)  acc5: 98.4375 (97.8053)  time: 0.0285  data: 0.0002  max mem: 5511
[08:29:59.511877] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.9121 (0.8821)  acc1: 70.3125 (69.9136)  acc5: 98.4375 (97.7948)  time: 0.0282  data: 0.0002  max mem: 5511
[08:29:59.792849] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.8674 (0.8804)  acc1: 71.8750 (69.9814)  acc5: 96.8750 (97.7442)  time: 0.0281  data: 0.0002  max mem: 5511
[08:29:59.944865] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.8638 (0.8808)  acc1: 71.8750 (69.9300)  acc5: 96.8750 (97.7600)  time: 0.0270  data: 0.0001  max mem: 5511
[08:30:00.102672] Test: Total time: 0:00:05 (0.0337 s / it)
[08:30:00.103417] * Acc@1 69.930 Acc@5 97.760 loss 0.881
[08:30:00.103750] Accuracy of the network on the 10000 test images: 69.9%
[08:30:00.103924] Max accuracy: 69.99%
[08:30:00.224276] log_dir: ./output_dir
[08:30:01.042393] Epoch: [25]  [  0/781]  eta: 0:10:37  lr: 0.000224  training_loss: 0.4134 (0.4134)  mae_loss: 0.2654 (0.2654)  classification_loss: 0.1481 (0.1481)  time: 0.8165  data: 0.5940  max mem: 5511
[08:30:04.971902] Epoch: [25]  [ 20/781]  eta: 0:02:51  lr: 0.000224  training_loss: 0.4147 (0.4151)  mae_loss: 0.2622 (0.2589)  classification_loss: 0.1542 (0.1563)  time: 0.1964  data: 0.0002  max mem: 5511
[08:30:08.883109] Epoch: [25]  [ 40/781]  eta: 0:02:36  lr: 0.000224  training_loss: 0.3882 (0.4106)  mae_loss: 0.2310 (0.2534)  classification_loss: 0.1551 (0.1573)  time: 0.1955  data: 0.0003  max mem: 5511
[08:30:12.779483] Epoch: [25]  [ 60/781]  eta: 0:02:28  lr: 0.000224  training_loss: 0.4184 (0.4142)  mae_loss: 0.2461 (0.2552)  classification_loss: 0.1616 (0.1589)  time: 0.1947  data: 0.0002  max mem: 5511
[08:30:16.713689] Epoch: [25]  [ 80/781]  eta: 0:02:22  lr: 0.000223  training_loss: 0.4140 (0.4122)  mae_loss: 0.2437 (0.2523)  classification_loss: 0.1604 (0.1600)  time: 0.1966  data: 0.0003  max mem: 5511
[08:30:20.653633] Epoch: [25]  [100/781]  eta: 0:02:17  lr: 0.000223  training_loss: 0.4060 (0.4106)  mae_loss: 0.2382 (0.2506)  classification_loss: 0.1594 (0.1600)  time: 0.1969  data: 0.0003  max mem: 5511
[08:30:24.649596] Epoch: [25]  [120/781]  eta: 0:02:13  lr: 0.000223  training_loss: 0.3876 (0.4075)  mae_loss: 0.2316 (0.2482)  classification_loss: 0.1549 (0.1593)  time: 0.1997  data: 0.0002  max mem: 5511
[08:30:28.570738] Epoch: [25]  [140/781]  eta: 0:02:08  lr: 0.000223  training_loss: 0.4020 (0.4074)  mae_loss: 0.2457 (0.2487)  classification_loss: 0.1558 (0.1587)  time: 0.1960  data: 0.0005  max mem: 5511
[08:30:32.480231] Epoch: [25]  [160/781]  eta: 0:02:04  lr: 0.000223  training_loss: 0.3937 (0.4060)  mae_loss: 0.2406 (0.2474)  classification_loss: 0.1604 (0.1586)  time: 0.1954  data: 0.0002  max mem: 5511
[08:30:36.372108] Epoch: [25]  [180/781]  eta: 0:01:59  lr: 0.000223  training_loss: 0.4292 (0.4080)  mae_loss: 0.2704 (0.2493)  classification_loss: 0.1565 (0.1587)  time: 0.1945  data: 0.0003  max mem: 5511
[08:30:40.274719] Epoch: [25]  [200/781]  eta: 0:01:55  lr: 0.000223  training_loss: 0.4004 (0.4080)  mae_loss: 0.2496 (0.2496)  classification_loss: 0.1569 (0.1584)  time: 0.1951  data: 0.0003  max mem: 5511
[08:30:44.174126] Epoch: [25]  [220/781]  eta: 0:01:51  lr: 0.000223  training_loss: 0.4015 (0.4075)  mae_loss: 0.2479 (0.2492)  classification_loss: 0.1607 (0.1583)  time: 0.1949  data: 0.0003  max mem: 5511
[08:30:48.081276] Epoch: [25]  [240/781]  eta: 0:01:47  lr: 0.000223  training_loss: 0.4021 (0.4073)  mae_loss: 0.2487 (0.2492)  classification_loss: 0.1584 (0.1581)  time: 0.1952  data: 0.0003  max mem: 5511
[08:30:51.993853] Epoch: [25]  [260/781]  eta: 0:01:43  lr: 0.000223  training_loss: 0.3939 (0.4071)  mae_loss: 0.2340 (0.2490)  classification_loss: 0.1576 (0.1582)  time: 0.1956  data: 0.0002  max mem: 5511
[08:30:55.913453] Epoch: [25]  [280/781]  eta: 0:01:39  lr: 0.000223  training_loss: 0.4213 (0.4079)  mae_loss: 0.2688 (0.2499)  classification_loss: 0.1573 (0.1581)  time: 0.1959  data: 0.0002  max mem: 5511
[08:30:59.812649] Epoch: [25]  [300/781]  eta: 0:01:35  lr: 0.000223  training_loss: 0.4038 (0.4077)  mae_loss: 0.2421 (0.2495)  classification_loss: 0.1553 (0.1581)  time: 0.1949  data: 0.0003  max mem: 5511
[08:31:03.716989] Epoch: [25]  [320/781]  eta: 0:01:31  lr: 0.000223  training_loss: 0.4112 (0.4077)  mae_loss: 0.2562 (0.2495)  classification_loss: 0.1555 (0.1582)  time: 0.1951  data: 0.0002  max mem: 5511
[08:31:07.621490] Epoch: [25]  [340/781]  eta: 0:01:27  lr: 0.000223  training_loss: 0.4052 (0.4072)  mae_loss: 0.2408 (0.2494)  classification_loss: 0.1556 (0.1579)  time: 0.1951  data: 0.0003  max mem: 5511
[08:31:11.528554] Epoch: [25]  [360/781]  eta: 0:01:23  lr: 0.000223  training_loss: 0.4033 (0.4071)  mae_loss: 0.2377 (0.2492)  classification_loss: 0.1588 (0.1579)  time: 0.1953  data: 0.0003  max mem: 5511
[08:31:15.430017] Epoch: [25]  [380/781]  eta: 0:01:19  lr: 0.000223  training_loss: 0.4151 (0.4076)  mae_loss: 0.2550 (0.2495)  classification_loss: 0.1581 (0.1581)  time: 0.1950  data: 0.0003  max mem: 5511
[08:31:19.353200] Epoch: [25]  [400/781]  eta: 0:01:15  lr: 0.000222  training_loss: 0.4169 (0.4079)  mae_loss: 0.2563 (0.2499)  classification_loss: 0.1573 (0.1580)  time: 0.1960  data: 0.0004  max mem: 5511
[08:31:23.283592] Epoch: [25]  [420/781]  eta: 0:01:11  lr: 0.000222  training_loss: 0.4104 (0.4077)  mae_loss: 0.2450 (0.2498)  classification_loss: 0.1575 (0.1579)  time: 0.1964  data: 0.0002  max mem: 5511
[08:31:27.181974] Epoch: [25]  [440/781]  eta: 0:01:07  lr: 0.000222  training_loss: 0.3920 (0.4071)  mae_loss: 0.2337 (0.2492)  classification_loss: 0.1584 (0.1579)  time: 0.1948  data: 0.0002  max mem: 5511
[08:31:31.098831] Epoch: [25]  [460/781]  eta: 0:01:03  lr: 0.000222  training_loss: 0.4061 (0.4071)  mae_loss: 0.2463 (0.2493)  classification_loss: 0.1555 (0.1578)  time: 0.1958  data: 0.0003  max mem: 5511
[08:31:34.994060] Epoch: [25]  [480/781]  eta: 0:00:59  lr: 0.000222  training_loss: 0.4079 (0.4072)  mae_loss: 0.2455 (0.2495)  classification_loss: 0.1576 (0.1577)  time: 0.1947  data: 0.0003  max mem: 5511
[08:31:38.908289] Epoch: [25]  [500/781]  eta: 0:00:55  lr: 0.000222  training_loss: 0.3985 (0.4071)  mae_loss: 0.2415 (0.2493)  classification_loss: 0.1581 (0.1578)  time: 0.1956  data: 0.0002  max mem: 5511
[08:31:42.803110] Epoch: [25]  [520/781]  eta: 0:00:51  lr: 0.000222  training_loss: 0.4016 (0.4071)  mae_loss: 0.2439 (0.2495)  classification_loss: 0.1532 (0.1576)  time: 0.1947  data: 0.0003  max mem: 5511
[08:31:46.698800] Epoch: [25]  [540/781]  eta: 0:00:47  lr: 0.000222  training_loss: 0.3977 (0.4069)  mae_loss: 0.2425 (0.2494)  classification_loss: 0.1540 (0.1576)  time: 0.1947  data: 0.0003  max mem: 5511
[08:31:50.632010] Epoch: [25]  [560/781]  eta: 0:00:43  lr: 0.000222  training_loss: 0.3985 (0.4070)  mae_loss: 0.2460 (0.2495)  classification_loss: 0.1549 (0.1575)  time: 0.1966  data: 0.0002  max mem: 5511
[08:31:54.547515] Epoch: [25]  [580/781]  eta: 0:00:39  lr: 0.000222  training_loss: 0.3992 (0.4069)  mae_loss: 0.2495 (0.2494)  classification_loss: 0.1543 (0.1575)  time: 0.1957  data: 0.0003  max mem: 5511
[08:31:58.449360] Epoch: [25]  [600/781]  eta: 0:00:35  lr: 0.000222  training_loss: 0.3915 (0.4068)  mae_loss: 0.2405 (0.2494)  classification_loss: 0.1567 (0.1575)  time: 0.1950  data: 0.0002  max mem: 5511
[08:32:02.350578] Epoch: [25]  [620/781]  eta: 0:00:31  lr: 0.000222  training_loss: 0.4147 (0.4072)  mae_loss: 0.2600 (0.2498)  classification_loss: 0.1585 (0.1575)  time: 0.1950  data: 0.0003  max mem: 5511
[08:32:06.250773] Epoch: [25]  [640/781]  eta: 0:00:27  lr: 0.000222  training_loss: 0.3934 (0.4072)  mae_loss: 0.2398 (0.2498)  classification_loss: 0.1543 (0.1574)  time: 0.1949  data: 0.0002  max mem: 5511
[08:32:10.175974] Epoch: [25]  [660/781]  eta: 0:00:23  lr: 0.000222  training_loss: 0.3990 (0.4071)  mae_loss: 0.2437 (0.2497)  classification_loss: 0.1601 (0.1574)  time: 0.1962  data: 0.0003  max mem: 5511
[08:32:14.082098] Epoch: [25]  [680/781]  eta: 0:00:19  lr: 0.000222  training_loss: 0.4017 (0.4071)  mae_loss: 0.2437 (0.2496)  classification_loss: 0.1571 (0.1575)  time: 0.1952  data: 0.0002  max mem: 5511
[08:32:18.029879] Epoch: [25]  [700/781]  eta: 0:00:15  lr: 0.000221  training_loss: 0.4007 (0.4068)  mae_loss: 0.2375 (0.2493)  classification_loss: 0.1542 (0.1575)  time: 0.1972  data: 0.0002  max mem: 5511
[08:32:21.943789] Epoch: [25]  [720/781]  eta: 0:00:11  lr: 0.000221  training_loss: 0.4000 (0.4066)  mae_loss: 0.2352 (0.2489)  classification_loss: 0.1596 (0.1576)  time: 0.1956  data: 0.0002  max mem: 5511
[08:32:25.843752] Epoch: [25]  [740/781]  eta: 0:00:08  lr: 0.000221  training_loss: 0.3985 (0.4064)  mae_loss: 0.2516 (0.2489)  classification_loss: 0.1529 (0.1576)  time: 0.1949  data: 0.0002  max mem: 5511
[08:32:29.766101] Epoch: [25]  [760/781]  eta: 0:00:04  lr: 0.000221  training_loss: 0.3906 (0.4062)  mae_loss: 0.2463 (0.2487)  classification_loss: 0.1528 (0.1575)  time: 0.1960  data: 0.0002  max mem: 5511
[08:32:33.697838] Epoch: [25]  [780/781]  eta: 0:00:00  lr: 0.000221  training_loss: 0.3962 (0.4061)  mae_loss: 0.2378 (0.2486)  classification_loss: 0.1547 (0.1575)  time: 0.1965  data: 0.0002  max mem: 5511
[08:32:33.856866] Epoch: [25] Total time: 0:02:33 (0.1967 s / it)
[08:32:33.857539] Averaged stats: lr: 0.000221  training_loss: 0.3962 (0.4061)  mae_loss: 0.2378 (0.2486)  classification_loss: 0.1547 (0.1575)
[08:32:34.554654] Test:  [  0/157]  eta: 0:01:48  testing_loss: 0.8779 (0.8779)  acc1: 71.8750 (71.8750)  acc5: 98.4375 (98.4375)  time: 0.6924  data: 0.6554  max mem: 5511
[08:32:34.840736] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.9019 (0.8907)  acc1: 68.7500 (68.3239)  acc5: 98.4375 (99.0057)  time: 0.0888  data: 0.0598  max mem: 5511
[08:32:35.129953] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.8914 (0.8671)  acc1: 68.7500 (70.1637)  acc5: 98.4375 (98.5119)  time: 0.0286  data: 0.0002  max mem: 5511
[08:32:35.421551] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.8112 (0.8685)  acc1: 71.8750 (70.8669)  acc5: 98.4375 (98.0343)  time: 0.0289  data: 0.0001  max mem: 5511
[08:32:35.710752] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.8334 (0.8633)  acc1: 73.4375 (71.1128)  acc5: 96.8750 (98.0183)  time: 0.0289  data: 0.0002  max mem: 5511
[08:32:35.995489] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.8339 (0.8592)  acc1: 71.8750 (71.3542)  acc5: 98.4375 (97.8860)  time: 0.0285  data: 0.0002  max mem: 5511
[08:32:36.284077] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.8339 (0.8533)  acc1: 71.8750 (71.3371)  acc5: 98.4375 (97.7971)  time: 0.0285  data: 0.0002  max mem: 5511
[08:32:36.568796] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7711 (0.8458)  acc1: 71.8750 (71.7430)  acc5: 98.4375 (97.8213)  time: 0.0285  data: 0.0002  max mem: 5511
[08:32:36.859702] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.8163 (0.8490)  acc1: 71.8750 (71.7400)  acc5: 98.4375 (97.8395)  time: 0.0287  data: 0.0002  max mem: 5511
[08:32:37.148291] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7918 (0.8456)  acc1: 71.8750 (71.8235)  acc5: 98.4375 (97.9224)  time: 0.0289  data: 0.0002  max mem: 5511
[08:32:37.439063] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.8037 (0.8464)  acc1: 70.3125 (71.7822)  acc5: 98.4375 (97.9889)  time: 0.0288  data: 0.0002  max mem: 5511
[08:32:37.727874] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.8564 (0.8482)  acc1: 70.3125 (71.5935)  acc5: 98.4375 (97.9589)  time: 0.0288  data: 0.0002  max mem: 5511
[08:32:38.016401] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.8330 (0.8437)  acc1: 73.4375 (71.8621)  acc5: 98.4375 (97.9855)  time: 0.0287  data: 0.0002  max mem: 5511
[08:32:38.304264] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.8496 (0.8476)  acc1: 73.4375 (71.7438)  acc5: 98.4375 (97.9843)  time: 0.0287  data: 0.0002  max mem: 5511
[08:32:38.585364] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.8513 (0.8469)  acc1: 71.8750 (71.8418)  acc5: 98.4375 (97.9277)  time: 0.0283  data: 0.0002  max mem: 5511
[08:32:38.864356] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.8393 (0.8464)  acc1: 70.3125 (71.8543)  acc5: 96.8750 (97.8787)  time: 0.0279  data: 0.0001  max mem: 5511
[08:32:39.014772] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.8178 (0.8450)  acc1: 73.4375 (71.8400)  acc5: 96.8750 (97.9100)  time: 0.0269  data: 0.0001  max mem: 5511
[08:32:39.192729] Test: Total time: 0:00:05 (0.0340 s / it)
[08:32:39.193174] * Acc@1 71.840 Acc@5 97.910 loss 0.845
[08:32:39.193470] Accuracy of the network on the 10000 test images: 71.8%
[08:32:39.193670] Max accuracy: 71.84%
[08:32:39.364163] log_dir: ./output_dir
[08:32:40.303824] Epoch: [26]  [  0/781]  eta: 0:12:12  lr: 0.000221  training_loss: 0.4073 (0.4073)  mae_loss: 0.2617 (0.2617)  classification_loss: 0.1456 (0.1456)  time: 0.9376  data: 0.7239  max mem: 5511
[08:32:44.226864] Epoch: [26]  [ 20/781]  eta: 0:02:56  lr: 0.000221  training_loss: 0.4200 (0.4148)  mae_loss: 0.2582 (0.2619)  classification_loss: 0.1529 (0.1529)  time: 0.1961  data: 0.0003  max mem: 5511
[08:32:48.222834] Epoch: [26]  [ 40/781]  eta: 0:02:40  lr: 0.000221  training_loss: 0.3956 (0.4094)  mae_loss: 0.2379 (0.2550)  classification_loss: 0.1542 (0.1544)  time: 0.1997  data: 0.0002  max mem: 5511
[08:32:52.137394] Epoch: [26]  [ 60/781]  eta: 0:02:30  lr: 0.000221  training_loss: 0.4137 (0.4134)  mae_loss: 0.2508 (0.2557)  classification_loss: 0.1622 (0.1576)  time: 0.1956  data: 0.0002  max mem: 5511
[08:32:56.048565] Epoch: [26]  [ 80/781]  eta: 0:02:24  lr: 0.000221  training_loss: 0.3999 (0.4112)  mae_loss: 0.2508 (0.2539)  classification_loss: 0.1541 (0.1573)  time: 0.1955  data: 0.0002  max mem: 5511
[08:32:59.953220] Epoch: [26]  [100/781]  eta: 0:02:18  lr: 0.000221  training_loss: 0.3884 (0.4089)  mae_loss: 0.2344 (0.2514)  classification_loss: 0.1603 (0.1575)  time: 0.1952  data: 0.0002  max mem: 5511
[08:33:03.950964] Epoch: [26]  [120/781]  eta: 0:02:14  lr: 0.000221  training_loss: 0.3938 (0.4061)  mae_loss: 0.2395 (0.2490)  classification_loss: 0.1543 (0.1572)  time: 0.1998  data: 0.0002  max mem: 5511
[08:33:07.883129] Epoch: [26]  [140/781]  eta: 0:02:09  lr: 0.000221  training_loss: 0.3943 (0.4051)  mae_loss: 0.2418 (0.2488)  classification_loss: 0.1518 (0.1563)  time: 0.1965  data: 0.0004  max mem: 5511
[08:33:11.782029] Epoch: [26]  [160/781]  eta: 0:02:04  lr: 0.000221  training_loss: 0.3963 (0.4049)  mae_loss: 0.2457 (0.2486)  classification_loss: 0.1533 (0.1563)  time: 0.1949  data: 0.0002  max mem: 5511
[08:33:15.715236] Epoch: [26]  [180/781]  eta: 0:02:00  lr: 0.000221  training_loss: 0.4014 (0.4050)  mae_loss: 0.2471 (0.2486)  classification_loss: 0.1586 (0.1565)  time: 0.1966  data: 0.0002  max mem: 5511
[08:33:19.633594] Epoch: [26]  [200/781]  eta: 0:01:56  lr: 0.000220  training_loss: 0.3986 (0.4046)  mae_loss: 0.2452 (0.2482)  classification_loss: 0.1551 (0.1564)  time: 0.1958  data: 0.0002  max mem: 5511
[08:33:23.541194] Epoch: [26]  [220/781]  eta: 0:01:52  lr: 0.000220  training_loss: 0.3763 (0.4029)  mae_loss: 0.2244 (0.2463)  classification_loss: 0.1574 (0.1565)  time: 0.1953  data: 0.0002  max mem: 5511
[08:33:27.471324] Epoch: [26]  [240/781]  eta: 0:01:47  lr: 0.000220  training_loss: 0.3959 (0.4030)  mae_loss: 0.2392 (0.2466)  classification_loss: 0.1503 (0.1564)  time: 0.1964  data: 0.0002  max mem: 5511
[08:33:31.423335] Epoch: [26]  [260/781]  eta: 0:01:43  lr: 0.000220  training_loss: 0.4026 (0.4032)  mae_loss: 0.2485 (0.2470)  classification_loss: 0.1546 (0.1563)  time: 0.1975  data: 0.0002  max mem: 5511
[08:33:35.316700] Epoch: [26]  [280/781]  eta: 0:01:39  lr: 0.000220  training_loss: 0.3911 (0.4026)  mae_loss: 0.2327 (0.2462)  classification_loss: 0.1580 (0.1564)  time: 0.1946  data: 0.0002  max mem: 5511
[08:33:39.266878] Epoch: [26]  [300/781]  eta: 0:01:35  lr: 0.000220  training_loss: 0.4197 (0.4037)  mae_loss: 0.2657 (0.2473)  classification_loss: 0.1554 (0.1564)  time: 0.1974  data: 0.0003  max mem: 5511
[08:33:43.166327] Epoch: [26]  [320/781]  eta: 0:01:31  lr: 0.000220  training_loss: 0.3960 (0.4032)  mae_loss: 0.2356 (0.2469)  classification_loss: 0.1531 (0.1563)  time: 0.1949  data: 0.0002  max mem: 5511
[08:33:47.070857] Epoch: [26]  [340/781]  eta: 0:01:27  lr: 0.000220  training_loss: 0.3914 (0.4033)  mae_loss: 0.2423 (0.2472)  classification_loss: 0.1509 (0.1561)  time: 0.1951  data: 0.0002  max mem: 5511
[08:33:50.972998] Epoch: [26]  [360/781]  eta: 0:01:23  lr: 0.000220  training_loss: 0.4174 (0.4038)  mae_loss: 0.2488 (0.2475)  classification_loss: 0.1539 (0.1562)  time: 0.1950  data: 0.0003  max mem: 5511
[08:33:54.890437] Epoch: [26]  [380/781]  eta: 0:01:19  lr: 0.000220  training_loss: 0.4040 (0.4040)  mae_loss: 0.2430 (0.2478)  classification_loss: 0.1572 (0.1562)  time: 0.1958  data: 0.0004  max mem: 5511
[08:33:58.794816] Epoch: [26]  [400/781]  eta: 0:01:15  lr: 0.000220  training_loss: 0.3914 (0.4036)  mae_loss: 0.2389 (0.2475)  classification_loss: 0.1525 (0.1561)  time: 0.1951  data: 0.0003  max mem: 5511
[08:34:02.707326] Epoch: [26]  [420/781]  eta: 0:01:11  lr: 0.000220  training_loss: 0.3887 (0.4031)  mae_loss: 0.2317 (0.2472)  classification_loss: 0.1516 (0.1559)  time: 0.1955  data: 0.0002  max mem: 5511
[08:34:06.600902] Epoch: [26]  [440/781]  eta: 0:01:07  lr: 0.000220  training_loss: 0.3939 (0.4028)  mae_loss: 0.2394 (0.2469)  classification_loss: 0.1547 (0.1559)  time: 0.1946  data: 0.0003  max mem: 5511
[08:34:10.498547] Epoch: [26]  [460/781]  eta: 0:01:03  lr: 0.000220  training_loss: 0.4041 (0.4025)  mae_loss: 0.2352 (0.2467)  classification_loss: 0.1525 (0.1558)  time: 0.1948  data: 0.0002  max mem: 5511
[08:34:14.397428] Epoch: [26]  [480/781]  eta: 0:00:59  lr: 0.000220  training_loss: 0.3803 (0.4018)  mae_loss: 0.2197 (0.2459)  classification_loss: 0.1557 (0.1559)  time: 0.1949  data: 0.0002  max mem: 5511
[08:34:18.324314] Epoch: [26]  [500/781]  eta: 0:00:55  lr: 0.000219  training_loss: 0.3922 (0.4016)  mae_loss: 0.2377 (0.2458)  classification_loss: 0.1545 (0.1558)  time: 0.1962  data: 0.0002  max mem: 5511
[08:34:22.219782] Epoch: [26]  [520/781]  eta: 0:00:51  lr: 0.000219  training_loss: 0.3967 (0.4012)  mae_loss: 0.2322 (0.2454)  classification_loss: 0.1577 (0.1558)  time: 0.1947  data: 0.0002  max mem: 5511
[08:34:26.121095] Epoch: [26]  [540/781]  eta: 0:00:47  lr: 0.000219  training_loss: 0.3905 (0.4009)  mae_loss: 0.2352 (0.2450)  classification_loss: 0.1556 (0.1559)  time: 0.1950  data: 0.0003  max mem: 5511
[08:34:30.019412] Epoch: [26]  [560/781]  eta: 0:00:43  lr: 0.000219  training_loss: 0.3920 (0.4009)  mae_loss: 0.2369 (0.2452)  classification_loss: 0.1507 (0.1557)  time: 0.1948  data: 0.0002  max mem: 5511
[08:34:33.960756] Epoch: [26]  [580/781]  eta: 0:00:39  lr: 0.000219  training_loss: 0.4100 (0.4009)  mae_loss: 0.2467 (0.2451)  classification_loss: 0.1574 (0.1558)  time: 0.1969  data: 0.0002  max mem: 5511
[08:34:37.900985] Epoch: [26]  [600/781]  eta: 0:00:35  lr: 0.000219  training_loss: 0.3954 (0.4009)  mae_loss: 0.2386 (0.2451)  classification_loss: 0.1520 (0.1557)  time: 0.1969  data: 0.0006  max mem: 5511
[08:34:41.814525] Epoch: [26]  [620/781]  eta: 0:00:31  lr: 0.000219  training_loss: 0.3898 (0.4008)  mae_loss: 0.2391 (0.2451)  classification_loss: 0.1540 (0.1557)  time: 0.1956  data: 0.0003  max mem: 5511
[08:34:45.710706] Epoch: [26]  [640/781]  eta: 0:00:27  lr: 0.000219  training_loss: 0.3924 (0.4008)  mae_loss: 0.2397 (0.2452)  classification_loss: 0.1541 (0.1557)  time: 0.1947  data: 0.0002  max mem: 5511
[08:34:49.614749] Epoch: [26]  [660/781]  eta: 0:00:23  lr: 0.000219  training_loss: 0.4022 (0.4009)  mae_loss: 0.2475 (0.2453)  classification_loss: 0.1537 (0.1556)  time: 0.1951  data: 0.0002  max mem: 5511
[08:34:53.551629] Epoch: [26]  [680/781]  eta: 0:00:19  lr: 0.000219  training_loss: 0.4060 (0.4010)  mae_loss: 0.2525 (0.2453)  classification_loss: 0.1557 (0.1557)  time: 0.1968  data: 0.0002  max mem: 5511
[08:34:57.486809] Epoch: [26]  [700/781]  eta: 0:00:15  lr: 0.000219  training_loss: 0.3944 (0.4009)  mae_loss: 0.2384 (0.2453)  classification_loss: 0.1523 (0.1557)  time: 0.1967  data: 0.0004  max mem: 5511
[08:35:01.386187] Epoch: [26]  [720/781]  eta: 0:00:12  lr: 0.000219  training_loss: 0.3939 (0.4009)  mae_loss: 0.2465 (0.2453)  classification_loss: 0.1498 (0.1556)  time: 0.1949  data: 0.0003  max mem: 5511
[08:35:05.282954] Epoch: [26]  [740/781]  eta: 0:00:08  lr: 0.000219  training_loss: 0.4033 (0.4009)  mae_loss: 0.2376 (0.2453)  classification_loss: 0.1535 (0.1556)  time: 0.1948  data: 0.0002  max mem: 5511
[08:35:09.202367] Epoch: [26]  [760/781]  eta: 0:00:04  lr: 0.000219  training_loss: 0.4062 (0.4011)  mae_loss: 0.2463 (0.2456)  classification_loss: 0.1543 (0.1556)  time: 0.1958  data: 0.0002  max mem: 5511
[08:35:13.129802] Epoch: [26]  [780/781]  eta: 0:00:00  lr: 0.000218  training_loss: 0.4032 (0.4012)  mae_loss: 0.2392 (0.2456)  classification_loss: 0.1512 (0.1556)  time: 0.1963  data: 0.0002  max mem: 5511
[08:35:13.290838] Epoch: [26] Total time: 0:02:33 (0.1971 s / it)
[08:35:13.291324] Averaged stats: lr: 0.000218  training_loss: 0.4032 (0.4012)  mae_loss: 0.2392 (0.2456)  classification_loss: 0.1512 (0.1556)
[08:35:13.978636] Test:  [  0/157]  eta: 0:01:47  testing_loss: 0.8674 (0.8674)  acc1: 73.4375 (73.4375)  acc5: 98.4375 (98.4375)  time: 0.6831  data: 0.6497  max mem: 5511
[08:35:14.264671] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.9073 (0.9151)  acc1: 65.6250 (67.8977)  acc5: 98.4375 (98.8636)  time: 0.0878  data: 0.0592  max mem: 5511
[08:35:14.545764] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.8807 (0.8869)  acc1: 67.1875 (69.7173)  acc5: 98.4375 (98.2887)  time: 0.0282  data: 0.0002  max mem: 5511
[08:35:14.827487] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.8683 (0.8928)  acc1: 70.3125 (69.5060)  acc5: 96.8750 (97.8327)  time: 0.0280  data: 0.0001  max mem: 5511
[08:35:15.125198] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.8683 (0.8905)  acc1: 70.3125 (69.3979)  acc5: 98.4375 (97.8277)  time: 0.0288  data: 0.0001  max mem: 5511
[08:35:15.408237] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.8499 (0.8801)  acc1: 71.8750 (70.0368)  acc5: 98.4375 (97.8860)  time: 0.0289  data: 0.0001  max mem: 5511
[08:35:15.692364] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.8431 (0.8743)  acc1: 71.8750 (70.0820)  acc5: 98.4375 (97.7715)  time: 0.0282  data: 0.0002  max mem: 5511
[08:35:15.973459] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7859 (0.8615)  acc1: 71.8750 (70.6426)  acc5: 96.8750 (97.8213)  time: 0.0281  data: 0.0002  max mem: 5511
[08:35:16.254385] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7721 (0.8606)  acc1: 73.4375 (70.9298)  acc5: 98.4375 (97.7816)  time: 0.0280  data: 0.0001  max mem: 5511
[08:35:16.535011] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.8429 (0.8593)  acc1: 73.4375 (71.1538)  acc5: 98.4375 (97.7850)  time: 0.0279  data: 0.0001  max mem: 5511
[08:35:16.816241] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.8301 (0.8596)  acc1: 71.8750 (71.0551)  acc5: 98.4375 (97.8342)  time: 0.0279  data: 0.0001  max mem: 5511
[08:35:17.101303] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.8487 (0.8594)  acc1: 71.8750 (71.2556)  acc5: 98.4375 (97.8604)  time: 0.0282  data: 0.0001  max mem: 5511
[08:35:17.390837] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.8189 (0.8533)  acc1: 73.4375 (71.4747)  acc5: 98.4375 (97.8693)  time: 0.0286  data: 0.0002  max mem: 5511
[08:35:17.673147] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.8674 (0.8578)  acc1: 68.7500 (71.1951)  acc5: 98.4375 (97.8531)  time: 0.0285  data: 0.0002  max mem: 5511
[08:35:17.955260] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.8840 (0.8581)  acc1: 68.7500 (71.3098)  acc5: 98.4375 (97.8280)  time: 0.0281  data: 0.0001  max mem: 5511
[08:35:18.234898] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.8612 (0.8559)  acc1: 70.3125 (71.2955)  acc5: 98.4375 (97.8063)  time: 0.0279  data: 0.0001  max mem: 5511
[08:35:18.384963] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.8319 (0.8548)  acc1: 70.3125 (71.2400)  acc5: 98.4375 (97.8200)  time: 0.0269  data: 0.0001  max mem: 5511
[08:35:18.545878] Test: Total time: 0:00:05 (0.0334 s / it)
[08:35:18.547569] * Acc@1 71.240 Acc@5 97.820 loss 0.855
[08:35:18.548693] Accuracy of the network on the 10000 test images: 71.2%
[08:35:18.549527] Max accuracy: 71.84%
[08:35:18.902493] log_dir: ./output_dir
[08:35:19.829441] Epoch: [27]  [  0/781]  eta: 0:12:01  lr: 0.000218  training_loss: 0.4498 (0.4498)  mae_loss: 0.2852 (0.2852)  classification_loss: 0.1646 (0.1646)  time: 0.9240  data: 0.7108  max mem: 5511
[08:35:23.757122] Epoch: [27]  [ 20/781]  eta: 0:02:55  lr: 0.000218  training_loss: 0.3931 (0.3984)  mae_loss: 0.2388 (0.2456)  classification_loss: 0.1483 (0.1528)  time: 0.1963  data: 0.0003  max mem: 5511
[08:35:27.659021] Epoch: [27]  [ 40/781]  eta: 0:02:38  lr: 0.000218  training_loss: 0.3973 (0.4002)  mae_loss: 0.2392 (0.2456)  classification_loss: 0.1586 (0.1546)  time: 0.1950  data: 0.0002  max mem: 5511
[08:35:31.562922] Epoch: [27]  [ 60/781]  eta: 0:02:29  lr: 0.000218  training_loss: 0.4064 (0.4017)  mae_loss: 0.2498 (0.2469)  classification_loss: 0.1567 (0.1548)  time: 0.1951  data: 0.0002  max mem: 5511
[08:35:35.488979] Epoch: [27]  [ 80/781]  eta: 0:02:23  lr: 0.000218  training_loss: 0.3962 (0.4020)  mae_loss: 0.2448 (0.2462)  classification_loss: 0.1549 (0.1557)  time: 0.1962  data: 0.0006  max mem: 5511
[08:35:39.412060] Epoch: [27]  [100/781]  eta: 0:02:18  lr: 0.000218  training_loss: 0.3816 (0.3991)  mae_loss: 0.2364 (0.2440)  classification_loss: 0.1543 (0.1551)  time: 0.1961  data: 0.0002  max mem: 5511
[08:35:43.340489] Epoch: [27]  [120/781]  eta: 0:02:13  lr: 0.000218  training_loss: 0.3973 (0.3988)  mae_loss: 0.2493 (0.2440)  classification_loss: 0.1528 (0.1548)  time: 0.1963  data: 0.0002  max mem: 5511
[08:35:47.249777] Epoch: [27]  [140/781]  eta: 0:02:08  lr: 0.000218  training_loss: 0.3948 (0.3989)  mae_loss: 0.2422 (0.2442)  classification_loss: 0.1531 (0.1547)  time: 0.1954  data: 0.0002  max mem: 5511
[08:35:51.157925] Epoch: [27]  [160/781]  eta: 0:02:04  lr: 0.000218  training_loss: 0.4049 (0.3991)  mae_loss: 0.2590 (0.2451)  classification_loss: 0.1489 (0.1540)  time: 0.1953  data: 0.0002  max mem: 5511
[08:35:55.070681] Epoch: [27]  [180/781]  eta: 0:02:00  lr: 0.000218  training_loss: 0.3958 (0.3984)  mae_loss: 0.2459 (0.2445)  classification_loss: 0.1502 (0.1539)  time: 0.1955  data: 0.0002  max mem: 5511
[08:35:58.968990] Epoch: [27]  [200/781]  eta: 0:01:55  lr: 0.000218  training_loss: 0.3860 (0.3974)  mae_loss: 0.2317 (0.2435)  classification_loss: 0.1556 (0.1540)  time: 0.1948  data: 0.0003  max mem: 5511
[08:36:02.868629] Epoch: [27]  [220/781]  eta: 0:01:51  lr: 0.000218  training_loss: 0.3887 (0.3983)  mae_loss: 0.2408 (0.2439)  classification_loss: 0.1588 (0.1543)  time: 0.1949  data: 0.0002  max mem: 5511
[08:36:06.761309] Epoch: [27]  [240/781]  eta: 0:01:47  lr: 0.000218  training_loss: 0.4057 (0.3986)  mae_loss: 0.2471 (0.2441)  classification_loss: 0.1564 (0.1545)  time: 0.1945  data: 0.0002  max mem: 5511
[08:36:10.657177] Epoch: [27]  [260/781]  eta: 0:01:43  lr: 0.000218  training_loss: 0.3909 (0.3981)  mae_loss: 0.2334 (0.2436)  classification_loss: 0.1531 (0.1544)  time: 0.1947  data: 0.0002  max mem: 5511
[08:36:14.576107] Epoch: [27]  [280/781]  eta: 0:01:39  lr: 0.000217  training_loss: 0.3931 (0.3979)  mae_loss: 0.2420 (0.2434)  classification_loss: 0.1548 (0.1544)  time: 0.1959  data: 0.0002  max mem: 5511
[08:36:18.502211] Epoch: [27]  [300/781]  eta: 0:01:35  lr: 0.000217  training_loss: 0.4045 (0.3988)  mae_loss: 0.2437 (0.2442)  classification_loss: 0.1549 (0.1546)  time: 0.1962  data: 0.0003  max mem: 5511
[08:36:22.408288] Epoch: [27]  [320/781]  eta: 0:01:31  lr: 0.000217  training_loss: 0.3873 (0.3986)  mae_loss: 0.2343 (0.2439)  classification_loss: 0.1575 (0.1547)  time: 0.1952  data: 0.0002  max mem: 5511
[08:36:26.312526] Epoch: [27]  [340/781]  eta: 0:01:27  lr: 0.000217  training_loss: 0.3717 (0.3977)  mae_loss: 0.2232 (0.2433)  classification_loss: 0.1494 (0.1544)  time: 0.1951  data: 0.0002  max mem: 5511
[08:36:30.220688] Epoch: [27]  [360/781]  eta: 0:01:23  lr: 0.000217  training_loss: 0.3980 (0.3978)  mae_loss: 0.2387 (0.2432)  classification_loss: 0.1578 (0.1545)  time: 0.1953  data: 0.0002  max mem: 5511
[08:36:34.127231] Epoch: [27]  [380/781]  eta: 0:01:19  lr: 0.000217  training_loss: 0.3944 (0.3981)  mae_loss: 0.2418 (0.2435)  classification_loss: 0.1538 (0.1546)  time: 0.1953  data: 0.0002  max mem: 5511
[08:36:38.014903] Epoch: [27]  [400/781]  eta: 0:01:15  lr: 0.000217  training_loss: 0.4024 (0.3986)  mae_loss: 0.2526 (0.2441)  classification_loss: 0.1524 (0.1545)  time: 0.1943  data: 0.0002  max mem: 5511
[08:36:41.944056] Epoch: [27]  [420/781]  eta: 0:01:11  lr: 0.000217  training_loss: 0.4026 (0.3991)  mae_loss: 0.2502 (0.2447)  classification_loss: 0.1508 (0.1544)  time: 0.1964  data: 0.0003  max mem: 5511
[08:36:45.847765] Epoch: [27]  [440/781]  eta: 0:01:07  lr: 0.000217  training_loss: 0.3866 (0.3990)  mae_loss: 0.2346 (0.2447)  classification_loss: 0.1506 (0.1543)  time: 0.1950  data: 0.0002  max mem: 5511
[08:36:49.751668] Epoch: [27]  [460/781]  eta: 0:01:03  lr: 0.000217  training_loss: 0.3950 (0.3989)  mae_loss: 0.2430 (0.2447)  classification_loss: 0.1511 (0.1542)  time: 0.1951  data: 0.0002  max mem: 5511
[08:36:53.681598] Epoch: [27]  [480/781]  eta: 0:00:59  lr: 0.000217  training_loss: 0.4021 (0.3988)  mae_loss: 0.2456 (0.2445)  classification_loss: 0.1587 (0.1544)  time: 0.1964  data: 0.0003  max mem: 5511
[08:36:57.579607] Epoch: [27]  [500/781]  eta: 0:00:55  lr: 0.000217  training_loss: 0.4020 (0.3991)  mae_loss: 0.2532 (0.2447)  classification_loss: 0.1555 (0.1544)  time: 0.1948  data: 0.0002  max mem: 5511
[08:37:01.500029] Epoch: [27]  [520/781]  eta: 0:00:51  lr: 0.000217  training_loss: 0.4142 (0.3995)  mae_loss: 0.2559 (0.2450)  classification_loss: 0.1600 (0.1546)  time: 0.1959  data: 0.0002  max mem: 5511
[08:37:05.405141] Epoch: [27]  [540/781]  eta: 0:00:47  lr: 0.000217  training_loss: 0.3742 (0.3994)  mae_loss: 0.2340 (0.2448)  classification_loss: 0.1558 (0.1546)  time: 0.1951  data: 0.0003  max mem: 5511
[08:37:09.304805] Epoch: [27]  [560/781]  eta: 0:00:43  lr: 0.000216  training_loss: 0.3938 (0.3990)  mae_loss: 0.2396 (0.2446)  classification_loss: 0.1512 (0.1545)  time: 0.1949  data: 0.0002  max mem: 5511
[08:37:13.255576] Epoch: [27]  [580/781]  eta: 0:00:39  lr: 0.000216  training_loss: 0.3915 (0.3988)  mae_loss: 0.2382 (0.2444)  classification_loss: 0.1530 (0.1544)  time: 0.1975  data: 0.0003  max mem: 5511
[08:37:17.170412] Epoch: [27]  [600/781]  eta: 0:00:35  lr: 0.000216  training_loss: 0.3909 (0.3986)  mae_loss: 0.2262 (0.2442)  classification_loss: 0.1512 (0.1544)  time: 0.1957  data: 0.0002  max mem: 5511
[08:37:21.093631] Epoch: [27]  [620/781]  eta: 0:00:31  lr: 0.000216  training_loss: 0.3931 (0.3985)  mae_loss: 0.2416 (0.2442)  classification_loss: 0.1545 (0.1543)  time: 0.1961  data: 0.0002  max mem: 5511
[08:37:25.024811] Epoch: [27]  [640/781]  eta: 0:00:27  lr: 0.000216  training_loss: 0.3827 (0.3982)  mae_loss: 0.2386 (0.2441)  classification_loss: 0.1467 (0.1541)  time: 0.1965  data: 0.0002  max mem: 5511
[08:37:28.958580] Epoch: [27]  [660/781]  eta: 0:00:23  lr: 0.000216  training_loss: 0.4046 (0.3982)  mae_loss: 0.2530 (0.2442)  classification_loss: 0.1489 (0.1540)  time: 0.1966  data: 0.0002  max mem: 5511
[08:37:32.878872] Epoch: [27]  [680/781]  eta: 0:00:19  lr: 0.000216  training_loss: 0.4067 (0.3983)  mae_loss: 0.2454 (0.2443)  classification_loss: 0.1549 (0.1541)  time: 0.1959  data: 0.0002  max mem: 5511
[08:37:36.790216] Epoch: [27]  [700/781]  eta: 0:00:15  lr: 0.000216  training_loss: 0.3964 (0.3985)  mae_loss: 0.2414 (0.2444)  classification_loss: 0.1550 (0.1541)  time: 0.1954  data: 0.0002  max mem: 5511
[08:37:40.704703] Epoch: [27]  [720/781]  eta: 0:00:11  lr: 0.000216  training_loss: 0.3862 (0.3981)  mae_loss: 0.2348 (0.2441)  classification_loss: 0.1511 (0.1540)  time: 0.1956  data: 0.0002  max mem: 5511
[08:37:44.602756] Epoch: [27]  [740/781]  eta: 0:00:08  lr: 0.000216  training_loss: 0.4062 (0.3982)  mae_loss: 0.2449 (0.2443)  classification_loss: 0.1514 (0.1540)  time: 0.1948  data: 0.0003  max mem: 5511
[08:37:48.527631] Epoch: [27]  [760/781]  eta: 0:00:04  lr: 0.000216  training_loss: 0.3997 (0.3981)  mae_loss: 0.2332 (0.2441)  classification_loss: 0.1535 (0.1540)  time: 0.1962  data: 0.0002  max mem: 5511
[08:37:52.421266] Epoch: [27]  [780/781]  eta: 0:00:00  lr: 0.000216  training_loss: 0.3919 (0.3980)  mae_loss: 0.2331 (0.2440)  classification_loss: 0.1562 (0.1540)  time: 0.1946  data: 0.0002  max mem: 5511
[08:37:52.581907] Epoch: [27] Total time: 0:02:33 (0.1968 s / it)
[08:37:52.582392] Averaged stats: lr: 0.000216  training_loss: 0.3919 (0.3980)  mae_loss: 0.2331 (0.2440)  classification_loss: 0.1562 (0.1540)
[08:37:53.130909] Test:  [  0/157]  eta: 0:01:25  testing_loss: 0.8234 (0.8234)  acc1: 79.6875 (79.6875)  acc5: 96.8750 (96.8750)  time: 0.5443  data: 0.5147  max mem: 5511
[08:37:53.418079] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.8234 (0.8508)  acc1: 76.5625 (72.1591)  acc5: 100.0000 (99.0057)  time: 0.0754  data: 0.0470  max mem: 5511
[08:37:53.714472] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.8204 (0.8180)  acc1: 73.4375 (73.2887)  acc5: 100.0000 (99.0327)  time: 0.0290  data: 0.0002  max mem: 5511
[08:37:53.999146] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.8512 (0.8283)  acc1: 75.0000 (73.1351)  acc5: 98.4375 (98.4375)  time: 0.0289  data: 0.0002  max mem: 5511
[08:37:54.288349] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.8196 (0.8246)  acc1: 75.0000 (73.3994)  acc5: 98.4375 (98.5137)  time: 0.0286  data: 0.0002  max mem: 5511
[08:37:54.575967] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.7917 (0.8198)  acc1: 75.0000 (73.6520)  acc5: 98.4375 (98.4988)  time: 0.0287  data: 0.0002  max mem: 5511
[08:37:54.860873] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.7917 (0.8169)  acc1: 75.0000 (73.5400)  acc5: 98.4375 (98.4631)  time: 0.0285  data: 0.0002  max mem: 5511
[08:37:55.143868] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7677 (0.8095)  acc1: 76.5625 (74.0097)  acc5: 98.4375 (98.3495)  time: 0.0283  data: 0.0002  max mem: 5511
[08:37:55.436217] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7658 (0.8124)  acc1: 75.0000 (73.9005)  acc5: 98.4375 (98.2446)  time: 0.0286  data: 0.0002  max mem: 5511
[08:37:55.721425] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7837 (0.8103)  acc1: 73.4375 (73.9870)  acc5: 98.4375 (98.3173)  time: 0.0288  data: 0.0002  max mem: 5511
[08:37:56.004190] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.8105 (0.8144)  acc1: 71.8750 (73.6386)  acc5: 98.4375 (98.3292)  time: 0.0283  data: 0.0002  max mem: 5511
[08:37:56.285378] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.8314 (0.8156)  acc1: 70.3125 (73.6064)  acc5: 98.4375 (98.3671)  time: 0.0281  data: 0.0002  max mem: 5511
[08:37:56.567945] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.7568 (0.8099)  acc1: 71.8750 (73.8249)  acc5: 98.4375 (98.3858)  time: 0.0281  data: 0.0002  max mem: 5511
[08:37:56.848793] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.8042 (0.8131)  acc1: 71.8750 (73.5806)  acc5: 98.4375 (98.3182)  time: 0.0281  data: 0.0002  max mem: 5511
[08:37:57.128737] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.8377 (0.8131)  acc1: 73.4375 (73.6813)  acc5: 98.4375 (98.2934)  time: 0.0279  data: 0.0001  max mem: 5511
[08:37:57.406584] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.8177 (0.8109)  acc1: 75.0000 (73.7583)  acc5: 98.4375 (98.2616)  time: 0.0278  data: 0.0001  max mem: 5511
[08:37:57.556274] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7678 (0.8096)  acc1: 75.0000 (73.8200)  acc5: 98.4375 (98.2900)  time: 0.0268  data: 0.0001  max mem: 5511
[08:37:57.710944] Test: Total time: 0:00:05 (0.0326 s / it)
[08:37:57.711408] * Acc@1 73.820 Acc@5 98.290 loss 0.810
[08:37:57.711732] Accuracy of the network on the 10000 test images: 73.8%
[08:37:57.711906] Max accuracy: 73.82%
[08:37:57.827468] log_dir: ./output_dir
[08:37:58.630109] Epoch: [28]  [  0/781]  eta: 0:10:25  lr: 0.000216  training_loss: 0.4118 (0.4118)  mae_loss: 0.2612 (0.2612)  classification_loss: 0.1506 (0.1506)  time: 0.8010  data: 0.5863  max mem: 5511
[08:38:02.538175] Epoch: [28]  [ 20/781]  eta: 0:02:50  lr: 0.000216  training_loss: 0.3995 (0.4047)  mae_loss: 0.2554 (0.2525)  classification_loss: 0.1510 (0.1521)  time: 0.1953  data: 0.0002  max mem: 5511
[08:38:06.461562] Epoch: [28]  [ 40/781]  eta: 0:02:35  lr: 0.000216  training_loss: 0.3988 (0.4012)  mae_loss: 0.2426 (0.2477)  classification_loss: 0.1567 (0.1535)  time: 0.1961  data: 0.0002  max mem: 5511
[08:38:10.439267] Epoch: [28]  [ 60/781]  eta: 0:02:28  lr: 0.000215  training_loss: 0.3889 (0.3985)  mae_loss: 0.2368 (0.2454)  classification_loss: 0.1505 (0.1530)  time: 0.1988  data: 0.0003  max mem: 5511
[08:38:14.397262] Epoch: [28]  [ 80/781]  eta: 0:02:23  lr: 0.000215  training_loss: 0.3910 (0.3971)  mae_loss: 0.2370 (0.2447)  classification_loss: 0.1478 (0.1524)  time: 0.1978  data: 0.0002  max mem: 5511
[08:38:18.317047] Epoch: [28]  [100/781]  eta: 0:02:18  lr: 0.000215  training_loss: 0.4002 (0.3973)  mae_loss: 0.2465 (0.2445)  classification_loss: 0.1571 (0.1528)  time: 0.1959  data: 0.0002  max mem: 5511
[08:38:22.208304] Epoch: [28]  [120/781]  eta: 0:02:13  lr: 0.000215  training_loss: 0.3853 (0.3958)  mae_loss: 0.2339 (0.2432)  classification_loss: 0.1492 (0.1526)  time: 0.1945  data: 0.0002  max mem: 5511
[08:38:26.127671] Epoch: [28]  [140/781]  eta: 0:02:08  lr: 0.000215  training_loss: 0.3838 (0.3961)  mae_loss: 0.2383 (0.2431)  classification_loss: 0.1565 (0.1530)  time: 0.1959  data: 0.0002  max mem: 5511
[08:38:30.058665] Epoch: [28]  [160/781]  eta: 0:02:04  lr: 0.000215  training_loss: 0.3885 (0.3957)  mae_loss: 0.2381 (0.2427)  classification_loss: 0.1500 (0.1529)  time: 0.1965  data: 0.0003  max mem: 5511
[08:38:33.969656] Epoch: [28]  [180/781]  eta: 0:01:59  lr: 0.000215  training_loss: 0.3943 (0.3971)  mae_loss: 0.2443 (0.2440)  classification_loss: 0.1560 (0.1531)  time: 0.1955  data: 0.0002  max mem: 5511
[08:38:37.884882] Epoch: [28]  [200/781]  eta: 0:01:55  lr: 0.000215  training_loss: 0.3913 (0.3962)  mae_loss: 0.2383 (0.2433)  classification_loss: 0.1530 (0.1529)  time: 0.1957  data: 0.0002  max mem: 5511
[08:38:41.810808] Epoch: [28]  [220/781]  eta: 0:01:51  lr: 0.000215  training_loss: 0.3889 (0.3962)  mae_loss: 0.2425 (0.2433)  classification_loss: 0.1535 (0.1529)  time: 0.1962  data: 0.0003  max mem: 5511
[08:38:45.692554] Epoch: [28]  [240/781]  eta: 0:01:47  lr: 0.000215  training_loss: 0.4049 (0.3971)  mae_loss: 0.2456 (0.2438)  classification_loss: 0.1559 (0.1532)  time: 0.1940  data: 0.0003  max mem: 5511
[08:38:49.617095] Epoch: [28]  [260/781]  eta: 0:01:43  lr: 0.000215  training_loss: 0.3925 (0.3968)  mae_loss: 0.2399 (0.2437)  classification_loss: 0.1525 (0.1531)  time: 0.1962  data: 0.0002  max mem: 5511
[08:38:53.508629] Epoch: [28]  [280/781]  eta: 0:01:39  lr: 0.000215  training_loss: 0.4032 (0.3971)  mae_loss: 0.2531 (0.2440)  classification_loss: 0.1542 (0.1531)  time: 0.1944  data: 0.0003  max mem: 5511
[08:38:57.401182] Epoch: [28]  [300/781]  eta: 0:01:35  lr: 0.000215  training_loss: 0.3954 (0.3967)  mae_loss: 0.2404 (0.2436)  classification_loss: 0.1527 (0.1531)  time: 0.1945  data: 0.0002  max mem: 5511
[08:39:01.282973] Epoch: [28]  [320/781]  eta: 0:01:31  lr: 0.000215  training_loss: 0.3849 (0.3965)  mae_loss: 0.2341 (0.2435)  classification_loss: 0.1511 (0.1530)  time: 0.1940  data: 0.0002  max mem: 5511
[08:39:05.186331] Epoch: [28]  [340/781]  eta: 0:01:27  lr: 0.000214  training_loss: 0.4028 (0.3971)  mae_loss: 0.2573 (0.2442)  classification_loss: 0.1476 (0.1529)  time: 0.1951  data: 0.0002  max mem: 5511
[08:39:09.110557] Epoch: [28]  [360/781]  eta: 0:01:23  lr: 0.000214  training_loss: 0.3919 (0.3968)  mae_loss: 0.2359 (0.2438)  classification_loss: 0.1534 (0.1530)  time: 0.1961  data: 0.0002  max mem: 5511
[08:39:13.080277] Epoch: [28]  [380/781]  eta: 0:01:19  lr: 0.000214  training_loss: 0.3970 (0.3970)  mae_loss: 0.2429 (0.2439)  classification_loss: 0.1522 (0.1531)  time: 0.1979  data: 0.0002  max mem: 5511
[08:39:16.977490] Epoch: [28]  [400/781]  eta: 0:01:15  lr: 0.000214  training_loss: 0.4053 (0.3973)  mae_loss: 0.2512 (0.2444)  classification_loss: 0.1471 (0.1529)  time: 0.1948  data: 0.0003  max mem: 5511
[08:39:20.869781] Epoch: [28]  [420/781]  eta: 0:01:11  lr: 0.000214  training_loss: 0.3960 (0.3973)  mae_loss: 0.2373 (0.2444)  classification_loss: 0.1544 (0.1529)  time: 0.1945  data: 0.0003  max mem: 5511
[08:39:24.759635] Epoch: [28]  [440/781]  eta: 0:01:07  lr: 0.000214  training_loss: 0.3975 (0.3971)  mae_loss: 0.2395 (0.2441)  classification_loss: 0.1547 (0.1530)  time: 0.1944  data: 0.0003  max mem: 5511
[08:39:28.676121] Epoch: [28]  [460/781]  eta: 0:01:03  lr: 0.000214  training_loss: 0.4086 (0.3975)  mae_loss: 0.2526 (0.2445)  classification_loss: 0.1520 (0.1530)  time: 0.1957  data: 0.0002  max mem: 5511
[08:39:32.567759] Epoch: [28]  [480/781]  eta: 0:00:59  lr: 0.000214  training_loss: 0.3953 (0.3973)  mae_loss: 0.2389 (0.2443)  classification_loss: 0.1546 (0.1531)  time: 0.1945  data: 0.0002  max mem: 5511
[08:39:36.481146] Epoch: [28]  [500/781]  eta: 0:00:55  lr: 0.000214  training_loss: 0.3892 (0.3970)  mae_loss: 0.2329 (0.2439)  classification_loss: 0.1532 (0.1531)  time: 0.1956  data: 0.0002  max mem: 5511
[08:39:40.410052] Epoch: [28]  [520/781]  eta: 0:00:51  lr: 0.000214  training_loss: 0.3941 (0.3972)  mae_loss: 0.2426 (0.2441)  classification_loss: 0.1542 (0.1532)  time: 0.1964  data: 0.0002  max mem: 5511
[08:39:44.327125] Epoch: [28]  [540/781]  eta: 0:00:47  lr: 0.000214  training_loss: 0.3778 (0.3968)  mae_loss: 0.2301 (0.2436)  classification_loss: 0.1502 (0.1532)  time: 0.1958  data: 0.0006  max mem: 5511
[08:39:48.236679] Epoch: [28]  [560/781]  eta: 0:00:43  lr: 0.000214  training_loss: 0.3913 (0.3966)  mae_loss: 0.2382 (0.2435)  classification_loss: 0.1492 (0.1531)  time: 0.1954  data: 0.0003  max mem: 5511
[08:39:52.136717] Epoch: [28]  [580/781]  eta: 0:00:39  lr: 0.000214  training_loss: 0.3915 (0.3966)  mae_loss: 0.2330 (0.2435)  classification_loss: 0.1496 (0.1531)  time: 0.1949  data: 0.0002  max mem: 5511
[08:39:56.088719] Epoch: [28]  [600/781]  eta: 0:00:35  lr: 0.000213  training_loss: 0.3863 (0.3965)  mae_loss: 0.2275 (0.2433)  classification_loss: 0.1561 (0.1532)  time: 0.1975  data: 0.0003  max mem: 5511
[08:39:59.996910] Epoch: [28]  [620/781]  eta: 0:00:31  lr: 0.000213  training_loss: 0.3885 (0.3965)  mae_loss: 0.2355 (0.2434)  classification_loss: 0.1494 (0.1531)  time: 0.1953  data: 0.0003  max mem: 5511
[08:40:03.902690] Epoch: [28]  [640/781]  eta: 0:00:27  lr: 0.000213  training_loss: 0.3947 (0.3965)  mae_loss: 0.2423 (0.2435)  classification_loss: 0.1525 (0.1530)  time: 0.1952  data: 0.0002  max mem: 5511
[08:40:07.819864] Epoch: [28]  [660/781]  eta: 0:00:23  lr: 0.000213  training_loss: 0.3974 (0.3965)  mae_loss: 0.2383 (0.2434)  classification_loss: 0.1499 (0.1531)  time: 0.1958  data: 0.0002  max mem: 5511
[08:40:11.720274] Epoch: [28]  [680/781]  eta: 0:00:19  lr: 0.000213  training_loss: 0.4064 (0.3968)  mae_loss: 0.2518 (0.2438)  classification_loss: 0.1529 (0.1531)  time: 0.1949  data: 0.0002  max mem: 5511
[08:40:15.609345] Epoch: [28]  [700/781]  eta: 0:00:15  lr: 0.000213  training_loss: 0.4005 (0.3970)  mae_loss: 0.2423 (0.2438)  classification_loss: 0.1572 (0.1532)  time: 0.1944  data: 0.0002  max mem: 5511
[08:40:19.527837] Epoch: [28]  [720/781]  eta: 0:00:11  lr: 0.000213  training_loss: 0.3843 (0.3968)  mae_loss: 0.2335 (0.2435)  classification_loss: 0.1515 (0.1533)  time: 0.1958  data: 0.0002  max mem: 5511
[08:40:23.445551] Epoch: [28]  [740/781]  eta: 0:00:08  lr: 0.000213  training_loss: 0.3860 (0.3966)  mae_loss: 0.2346 (0.2433)  classification_loss: 0.1506 (0.1532)  time: 0.1958  data: 0.0002  max mem: 5511
[08:40:27.432843] Epoch: [28]  [760/781]  eta: 0:00:04  lr: 0.000213  training_loss: 0.3980 (0.3968)  mae_loss: 0.2489 (0.2435)  classification_loss: 0.1549 (0.1533)  time: 0.1993  data: 0.0004  max mem: 5511
[08:40:31.313098] Epoch: [28]  [780/781]  eta: 0:00:00  lr: 0.000213  training_loss: 0.3778 (0.3964)  mae_loss: 0.2304 (0.2432)  classification_loss: 0.1495 (0.1532)  time: 0.1939  data: 0.0002  max mem: 5511
[08:40:31.476562] Epoch: [28] Total time: 0:02:33 (0.1967 s / it)
[08:40:31.477303] Averaged stats: lr: 0.000213  training_loss: 0.3778 (0.3964)  mae_loss: 0.2304 (0.2432)  classification_loss: 0.1495 (0.1532)
[08:40:32.037182] Test:  [  0/157]  eta: 0:01:27  testing_loss: 0.8154 (0.8154)  acc1: 70.3125 (70.3125)  acc5: 98.4375 (98.4375)  time: 0.5555  data: 0.5251  max mem: 5511
[08:40:32.320020] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.8537 (0.8440)  acc1: 71.8750 (71.8750)  acc5: 98.4375 (98.2955)  time: 0.0760  data: 0.0479  max mem: 5511
[08:40:32.600078] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.7986 (0.8184)  acc1: 73.4375 (73.5863)  acc5: 98.4375 (98.2887)  time: 0.0280  data: 0.0001  max mem: 5511
[08:40:32.881186] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.7960 (0.8239)  acc1: 73.4375 (73.4375)  acc5: 98.4375 (98.0847)  time: 0.0279  data: 0.0001  max mem: 5511
[08:40:33.161998] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.7960 (0.8221)  acc1: 73.4375 (73.5137)  acc5: 98.4375 (97.9802)  time: 0.0280  data: 0.0001  max mem: 5511
[08:40:33.442895] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.7957 (0.8147)  acc1: 73.4375 (73.4375)  acc5: 98.4375 (98.0086)  time: 0.0280  data: 0.0001  max mem: 5511
[08:40:33.728512] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.7957 (0.8128)  acc1: 71.8750 (73.3094)  acc5: 98.4375 (97.9764)  time: 0.0282  data: 0.0001  max mem: 5511
[08:40:34.015334] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7569 (0.8036)  acc1: 75.0000 (73.7456)  acc5: 98.4375 (98.0194)  time: 0.0285  data: 0.0002  max mem: 5511
[08:40:34.297024] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7569 (0.8084)  acc1: 76.5625 (73.7654)  acc5: 98.4375 (97.8974)  time: 0.0283  data: 0.0002  max mem: 5511
[08:40:34.579294] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.8149 (0.8070)  acc1: 75.0000 (73.7809)  acc5: 98.4375 (97.9224)  time: 0.0281  data: 0.0001  max mem: 5511
[08:40:34.860980] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.7977 (0.8092)  acc1: 71.8750 (73.4994)  acc5: 98.4375 (97.9579)  time: 0.0281  data: 0.0001  max mem: 5511
[08:40:35.142972] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.8412 (0.8116)  acc1: 71.8750 (73.3953)  acc5: 98.4375 (97.9307)  time: 0.0281  data: 0.0001  max mem: 5511
[08:40:35.425015] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.8171 (0.8082)  acc1: 71.8750 (73.4375)  acc5: 98.4375 (97.9855)  time: 0.0280  data: 0.0002  max mem: 5511
[08:40:35.707211] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.8298 (0.8116)  acc1: 73.4375 (73.3540)  acc5: 98.4375 (97.9843)  time: 0.0281  data: 0.0002  max mem: 5511
[08:40:35.988104] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.8547 (0.8115)  acc1: 75.0000 (73.4153)  acc5: 98.4375 (97.9942)  time: 0.0280  data: 0.0001  max mem: 5511
[08:40:36.266407] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.8325 (0.8109)  acc1: 71.8750 (73.3030)  acc5: 98.4375 (97.9719)  time: 0.0278  data: 0.0001  max mem: 5511
[08:40:36.416494] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.8151 (0.8117)  acc1: 71.8750 (73.2500)  acc5: 98.4375 (97.9900)  time: 0.0269  data: 0.0001  max mem: 5511
[08:40:36.561744] Test: Total time: 0:00:05 (0.0324 s / it)
[08:40:36.562289] * Acc@1 73.250 Acc@5 97.990 loss 0.812
[08:40:36.562586] Accuracy of the network on the 10000 test images: 73.2%
[08:40:36.562771] Max accuracy: 73.82%
[08:40:36.877938] log_dir: ./output_dir
[08:40:37.675808] Epoch: [29]  [  0/781]  eta: 0:10:21  lr: 0.000213  training_loss: 0.3891 (0.3891)  mae_loss: 0.2485 (0.2485)  classification_loss: 0.1406 (0.1406)  time: 0.7963  data: 0.5751  max mem: 5511
[08:40:41.576833] Epoch: [29]  [ 20/781]  eta: 0:02:50  lr: 0.000213  training_loss: 0.3808 (0.3960)  mae_loss: 0.2416 (0.2484)  classification_loss: 0.1447 (0.1476)  time: 0.1950  data: 0.0002  max mem: 5511
[08:40:45.487793] Epoch: [29]  [ 40/781]  eta: 0:02:35  lr: 0.000213  training_loss: 0.3948 (0.3986)  mae_loss: 0.2419 (0.2479)  classification_loss: 0.1533 (0.1507)  time: 0.1955  data: 0.0002  max mem: 5511
[08:40:49.380972] Epoch: [29]  [ 60/781]  eta: 0:02:27  lr: 0.000213  training_loss: 0.3968 (0.3985)  mae_loss: 0.2436 (0.2473)  classification_loss: 0.1501 (0.1512)  time: 0.1946  data: 0.0003  max mem: 5511
[08:40:53.275960] Epoch: [29]  [ 80/781]  eta: 0:02:21  lr: 0.000213  training_loss: 0.3970 (0.3991)  mae_loss: 0.2441 (0.2478)  classification_loss: 0.1506 (0.1513)  time: 0.1947  data: 0.0002  max mem: 5511
[08:40:57.188127] Epoch: [29]  [100/781]  eta: 0:02:16  lr: 0.000212  training_loss: 0.3866 (0.3976)  mae_loss: 0.2379 (0.2461)  classification_loss: 0.1496 (0.1514)  time: 0.1955  data: 0.0003  max mem: 5511
[08:41:01.096794] Epoch: [29]  [120/781]  eta: 0:02:12  lr: 0.000212  training_loss: 0.3895 (0.3962)  mae_loss: 0.2383 (0.2444)  classification_loss: 0.1518 (0.1518)  time: 0.1954  data: 0.0002  max mem: 5511
[08:41:05.053070] Epoch: [29]  [140/781]  eta: 0:02:08  lr: 0.000212  training_loss: 0.3809 (0.3943)  mae_loss: 0.2376 (0.2433)  classification_loss: 0.1466 (0.1510)  time: 0.1977  data: 0.0002  max mem: 5511
[08:41:08.976975] Epoch: [29]  [160/781]  eta: 0:02:03  lr: 0.000212  training_loss: 0.3717 (0.3920)  mae_loss: 0.2289 (0.2413)  classification_loss: 0.1476 (0.1507)  time: 0.1961  data: 0.0002  max mem: 5511
[08:41:12.887931] Epoch: [29]  [180/781]  eta: 0:01:59  lr: 0.000212  training_loss: 0.3913 (0.3924)  mae_loss: 0.2378 (0.2419)  classification_loss: 0.1473 (0.1505)  time: 0.1955  data: 0.0005  max mem: 5511
[08:41:16.808131] Epoch: [29]  [200/781]  eta: 0:01:55  lr: 0.000212  training_loss: 0.4020 (0.3937)  mae_loss: 0.2554 (0.2432)  classification_loss: 0.1491 (0.1505)  time: 0.1959  data: 0.0003  max mem: 5511
[08:41:20.718507] Epoch: [29]  [220/781]  eta: 0:01:51  lr: 0.000212  training_loss: 0.3654 (0.3915)  mae_loss: 0.2190 (0.2412)  classification_loss: 0.1507 (0.1503)  time: 0.1954  data: 0.0002  max mem: 5511

[08:41:24.625830] Epoch: [29]  [240/781]  eta: 0:01:47  lr: 0.000212  training_loss: 0.3960 (0.3920)  mae_loss: 0.2523 (0.2419)  classification_loss: 0.1492 (0.1502)  time: 0.1953  data: 0.0004  max mem: 5511
[08:41:28.545314] Epoch: [29]  [260/781]  eta: 0:01:43  lr: 0.000212  training_loss: 0.3866 (0.3920)  mae_loss: 0.2403 (0.2419)  classification_loss: 0.1481 (0.1501)  time: 0.1958  data: 0.0002  max mem: 5511
[08:41:32.467285] Epoch: [29]  [280/781]  eta: 0:01:39  lr: 0.000212  training_loss: 0.3928 (0.3923)  mae_loss: 0.2404 (0.2420)  classification_loss: 0.1515 (0.1503)  time: 0.1960  data: 0.0003  max mem: 5511
[08:41:36.395899] Epoch: [29]  [300/781]  eta: 0:01:35  lr: 0.000212  training_loss: 0.3945 (0.3930)  mae_loss: 0.2435 (0.2426)  classification_loss: 0.1500 (0.1504)  time: 0.1964  data: 0.0002  max mem: 5511
[08:41:40.319593] Epoch: [29]  [320/781]  eta: 0:01:31  lr: 0.000212  training_loss: 0.3835 (0.3927)  mae_loss: 0.2313 (0.2423)  classification_loss: 0.1514 (0.1505)  time: 0.1961  data: 0.0002  max mem: 5511
[08:41:44.219990] Epoch: [29]  [340/781]  eta: 0:01:27  lr: 0.000212  training_loss: 0.3971 (0.3932)  mae_loss: 0.2463 (0.2428)  classification_loss: 0.1483 (0.1504)  time: 0.1949  data: 0.0002  max mem: 5511
[08:41:48.147232] Epoch: [29]  [360/781]  eta: 0:01:23  lr: 0.000211  training_loss: 0.3997 (0.3933)  mae_loss: 0.2453 (0.2427)  classification_loss: 0.1515 (0.1506)  time: 0.1963  data: 0.0002  max mem: 5511
[08:41:52.111197] Epoch: [29]  [380/781]  eta: 0:01:19  lr: 0.000211  training_loss: 0.3876 (0.3932)  mae_loss: 0.2373 (0.2424)  classification_loss: 0.1544 (0.1507)  time: 0.1981  data: 0.0003  max mem: 5511
[08:41:56.001987] Epoch: [29]  [400/781]  eta: 0:01:15  lr: 0.000211  training_loss: 0.3784 (0.3925)  mae_loss: 0.2248 (0.2418)  classification_loss: 0.1498 (0.1507)  time: 0.1945  data: 0.0002  max mem: 5511
[08:41:59.903337] Epoch: [29]  [420/781]  eta: 0:01:11  lr: 0.000211  training_loss: 0.3822 (0.3923)  mae_loss: 0.2337 (0.2416)  classification_loss: 0.1486 (0.1507)  time: 0.1950  data: 0.0003  max mem: 5511
[08:42:03.809542] Epoch: [29]  [440/781]  eta: 0:01:07  lr: 0.000211  training_loss: 0.3897 (0.3923)  mae_loss: 0.2448 (0.2418)  classification_loss: 0.1475 (0.1505)  time: 0.1952  data: 0.0002  max mem: 5511
[08:42:07.713580] Epoch: [29]  [460/781]  eta: 0:01:03  lr: 0.000211  training_loss: 0.3785 (0.3920)  mae_loss: 0.2284 (0.2417)  classification_loss: 0.1449 (0.1504)  time: 0.1951  data: 0.0004  max mem: 5511
[08:42:11.632861] Epoch: [29]  [480/781]  eta: 0:00:59  lr: 0.000211  training_loss: 0.3825 (0.3923)  mae_loss: 0.2285 (0.2417)  classification_loss: 0.1549 (0.1506)  time: 0.1958  data: 0.0003  max mem: 5511
[08:42:15.524205] Epoch: [29]  [500/781]  eta: 0:00:55  lr: 0.000211  training_loss: 0.3892 (0.3922)  mae_loss: 0.2354 (0.2416)  classification_loss: 0.1527 (0.1506)  time: 0.1945  data: 0.0003  max mem: 5511
[08:42:19.432740] Epoch: [29]  [520/781]  eta: 0:00:51  lr: 0.000211  training_loss: 0.3828 (0.3921)  mae_loss: 0.2289 (0.2414)  classification_loss: 0.1489 (0.1506)  time: 0.1953  data: 0.0002  max mem: 5511
[08:42:23.361396] Epoch: [29]  [540/781]  eta: 0:00:47  lr: 0.000211  training_loss: 0.3928 (0.3921)  mae_loss: 0.2451 (0.2415)  classification_loss: 0.1486 (0.1506)  time: 0.1964  data: 0.0003  max mem: 5511
[08:42:27.259458] Epoch: [29]  [560/781]  eta: 0:00:43  lr: 0.000211  training_loss: 0.3748 (0.3918)  mae_loss: 0.2348 (0.2414)  classification_loss: 0.1443 (0.1504)  time: 0.1948  data: 0.0002  max mem: 5511
[08:42:31.169469] Epoch: [29]  [580/781]  eta: 0:00:39  lr: 0.000211  training_loss: 0.3936 (0.3921)  mae_loss: 0.2499 (0.2417)  classification_loss: 0.1489 (0.1504)  time: 0.1951  data: 0.0002  max mem: 5511
[08:42:35.077889] Epoch: [29]  [600/781]  eta: 0:00:35  lr: 0.000211  training_loss: 0.3871 (0.3920)  mae_loss: 0.2339 (0.2416)  classification_loss: 0.1467 (0.1504)  time: 0.1953  data: 0.0004  max mem: 5511
[08:42:38.977597] Epoch: [29]  [620/781]  eta: 0:00:31  lr: 0.000210  training_loss: 0.3838 (0.3918)  mae_loss: 0.2417 (0.2415)  classification_loss: 0.1483 (0.1503)  time: 0.1949  data: 0.0002  max mem: 5511
[08:42:42.872055] Epoch: [29]  [640/781]  eta: 0:00:27  lr: 0.000210  training_loss: 0.3933 (0.3921)  mae_loss: 0.2509 (0.2418)  classification_loss: 0.1511 (0.1503)  time: 0.1947  data: 0.0003  max mem: 5511
[08:42:46.770969] Epoch: [29]  [660/781]  eta: 0:00:23  lr: 0.000210  training_loss: 0.3865 (0.3920)  mae_loss: 0.2307 (0.2417)  classification_loss: 0.1494 (0.1503)  time: 0.1949  data: 0.0002  max mem: 5511
[08:42:50.664201] Epoch: [29]  [680/781]  eta: 0:00:19  lr: 0.000210  training_loss: 0.3895 (0.3918)  mae_loss: 0.2308 (0.2414)  classification_loss: 0.1510 (0.1504)  time: 0.1946  data: 0.0002  max mem: 5511
[08:42:54.570311] Epoch: [29]  [700/781]  eta: 0:00:15  lr: 0.000210  training_loss: 0.3888 (0.3919)  mae_loss: 0.2401 (0.2414)  classification_loss: 0.1506 (0.1504)  time: 0.1952  data: 0.0003  max mem: 5511
[08:42:58.463554] Epoch: [29]  [720/781]  eta: 0:00:11  lr: 0.000210  training_loss: 0.3830 (0.3916)  mae_loss: 0.2324 (0.2412)  classification_loss: 0.1471 (0.1504)  time: 0.1946  data: 0.0003  max mem: 5511
[08:43:02.358568] Epoch: [29]  [740/781]  eta: 0:00:08  lr: 0.000210  training_loss: 0.3897 (0.3918)  mae_loss: 0.2408 (0.2414)  classification_loss: 0.1473 (0.1504)  time: 0.1947  data: 0.0003  max mem: 5511
[08:43:06.264989] Epoch: [29]  [760/781]  eta: 0:00:04  lr: 0.000210  training_loss: 0.4014 (0.3920)  mae_loss: 0.2406 (0.2415)  classification_loss: 0.1525 (0.1505)  time: 0.1952  data: 0.0002  max mem: 5511
[08:43:10.135728] Epoch: [29]  [780/781]  eta: 0:00:00  lr: 0.000210  training_loss: 0.3823 (0.3919)  mae_loss: 0.2355 (0.2414)  classification_loss: 0.1493 (0.1505)  time: 0.1935  data: 0.0002  max mem: 5511
[08:43:10.304721] Epoch: [29] Total time: 0:02:33 (0.1964 s / it)
[08:43:10.305260] Averaged stats: lr: 0.000210  training_loss: 0.3823 (0.3919)  mae_loss: 0.2355 (0.2414)  classification_loss: 0.1493 (0.1505)
[08:43:10.915300] Test:  [  0/157]  eta: 0:01:35  testing_loss: 0.8768 (0.8768)  acc1: 73.4375 (73.4375)  acc5: 95.3125 (95.3125)  time: 0.6059  data: 0.5740  max mem: 5511
[08:43:11.204356] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.8639 (0.8364)  acc1: 73.4375 (73.2955)  acc5: 98.4375 (98.2955)  time: 0.0811  data: 0.0523  max mem: 5511
[08:43:11.489302] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.7832 (0.8084)  acc1: 73.4375 (74.0327)  acc5: 98.4375 (98.3631)  time: 0.0285  data: 0.0002  max mem: 5511
[08:43:11.772635] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.8022 (0.8173)  acc1: 73.4375 (74.1935)  acc5: 98.4375 (98.0343)  time: 0.0283  data: 0.0002  max mem: 5511
[08:43:12.054324] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.8170 (0.8135)  acc1: 75.0000 (74.0854)  acc5: 98.4375 (98.0183)  time: 0.0281  data: 0.0002  max mem: 5511
[08:43:12.335851] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.7964 (0.8104)  acc1: 73.4375 (74.1728)  acc5: 98.4375 (98.0699)  time: 0.0280  data: 0.0002  max mem: 5511
[08:43:12.616233] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.7797 (0.8050)  acc1: 73.4375 (74.2828)  acc5: 98.4375 (98.0277)  time: 0.0280  data: 0.0001  max mem: 5511
[08:43:12.897494] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7217 (0.7949)  acc1: 78.1250 (74.9780)  acc5: 98.4375 (98.0634)  time: 0.0280  data: 0.0001  max mem: 5511
[08:43:13.178438] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7193 (0.7974)  acc1: 78.1250 (74.8843)  acc5: 98.4375 (97.9360)  time: 0.0280  data: 0.0001  max mem: 5511
[08:43:13.460873] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7458 (0.7926)  acc1: 78.1250 (75.1202)  acc5: 98.4375 (97.9739)  time: 0.0281  data: 0.0002  max mem: 5511
[08:43:13.742042] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.7878 (0.7942)  acc1: 73.4375 (74.9072)  acc5: 98.4375 (98.0198)  time: 0.0281  data: 0.0002  max mem: 5511
[08:43:14.022420] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.8008 (0.7953)  acc1: 70.3125 (74.6481)  acc5: 98.4375 (98.0715)  time: 0.0280  data: 0.0001  max mem: 5511
[08:43:14.302826] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.7564 (0.7900)  acc1: 73.4375 (74.7805)  acc5: 98.4375 (98.0630)  time: 0.0279  data: 0.0001  max mem: 5511
[08:43:14.583957] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.7564 (0.7913)  acc1: 75.0000 (74.7495)  acc5: 98.4375 (98.0916)  time: 0.0280  data: 0.0002  max mem: 5511
[08:43:14.863795] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7828 (0.7903)  acc1: 75.0000 (74.7784)  acc5: 98.4375 (98.1272)  time: 0.0279  data: 0.0001  max mem: 5511
[08:43:15.143361] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7646 (0.7885)  acc1: 75.0000 (74.7310)  acc5: 98.4375 (98.1064)  time: 0.0279  data: 0.0001  max mem: 5511
[08:43:15.294087] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7701 (0.7897)  acc1: 73.4375 (74.7000)  acc5: 98.4375 (98.0800)  time: 0.0269  data: 0.0001  max mem: 5511
[08:43:15.454299] Test: Total time: 0:00:05 (0.0328 s / it)
[08:43:15.454812] * Acc@1 74.700 Acc@5 98.080 loss 0.790
[08:43:15.455159] Accuracy of the network on the 10000 test images: 74.7%
[08:43:15.455389] Max accuracy: 74.70%
[08:43:15.637199] log_dir: ./output_dir
[08:43:16.550868] Epoch: [30]  [  0/781]  eta: 0:11:52  lr: 0.000210  training_loss: 0.3641 (0.3641)  mae_loss: 0.2257 (0.2257)  classification_loss: 0.1384 (0.1384)  time: 0.9118  data: 0.6727  max mem: 5511
[08:43:20.453345] Epoch: [30]  [ 20/781]  eta: 0:02:54  lr: 0.000210  training_loss: 0.3910 (0.3852)  mae_loss: 0.2380 (0.2360)  classification_loss: 0.1475 (0.1493)  time: 0.1950  data: 0.0002  max mem: 5511
[08:43:24.358482] Epoch: [30]  [ 40/781]  eta: 0:02:37  lr: 0.000210  training_loss: 0.3927 (0.3926)  mae_loss: 0.2439 (0.2414)  classification_loss: 0.1549 (0.1512)  time: 0.1952  data: 0.0002  max mem: 5511
[08:43:28.256088] Epoch: [30]  [ 60/781]  eta: 0:02:29  lr: 0.000210  training_loss: 0.3836 (0.3911)  mae_loss: 0.2379 (0.2404)  classification_loss: 0.1484 (0.1507)  time: 0.1948  data: 0.0004  max mem: 5511
[08:43:32.154755] Epoch: [30]  [ 80/781]  eta: 0:02:22  lr: 0.000210  training_loss: 0.3872 (0.3904)  mae_loss: 0.2324 (0.2402)  classification_loss: 0.1482 (0.1502)  time: 0.1949  data: 0.0002  max mem: 5511
[08:43:36.066248] Epoch: [30]  [100/781]  eta: 0:02:17  lr: 0.000209  training_loss: 0.3832 (0.3904)  mae_loss: 0.2320 (0.2399)  classification_loss: 0.1503 (0.1505)  time: 0.1954  data: 0.0002  max mem: 5511
[08:43:39.994164] Epoch: [30]  [120/781]  eta: 0:02:12  lr: 0.000209  training_loss: 0.3914 (0.3913)  mae_loss: 0.2436 (0.2412)  classification_loss: 0.1447 (0.1501)  time: 0.1963  data: 0.0003  max mem: 5511
[08:43:43.910797] Epoch: [30]  [140/781]  eta: 0:02:08  lr: 0.000209  training_loss: 0.3824 (0.3915)  mae_loss: 0.2358 (0.2421)  classification_loss: 0.1458 (0.1494)  time: 0.1958  data: 0.0002  max mem: 5511
[08:43:47.820122] Epoch: [30]  [160/781]  eta: 0:02:04  lr: 0.000209  training_loss: 0.3860 (0.3916)  mae_loss: 0.2360 (0.2422)  classification_loss: 0.1470 (0.1494)  time: 0.1954  data: 0.0004  max mem: 5511
[08:43:51.742384] Epoch: [30]  [180/781]  eta: 0:01:59  lr: 0.000209  training_loss: 0.3826 (0.3912)  mae_loss: 0.2364 (0.2418)  classification_loss: 0.1481 (0.1494)  time: 0.1960  data: 0.0002  max mem: 5511
[08:43:55.648443] Epoch: [30]  [200/781]  eta: 0:01:55  lr: 0.000209  training_loss: 0.3724 (0.3896)  mae_loss: 0.2280 (0.2403)  classification_loss: 0.1471 (0.1493)  time: 0.1952  data: 0.0002  max mem: 5511
[08:43:59.608193] Epoch: [30]  [220/781]  eta: 0:01:51  lr: 0.000209  training_loss: 0.4045 (0.3903)  mae_loss: 0.2472 (0.2408)  classification_loss: 0.1476 (0.1495)  time: 0.1979  data: 0.0002  max mem: 5511
[08:44:03.518457] Epoch: [30]  [240/781]  eta: 0:01:47  lr: 0.000209  training_loss: 0.3888 (0.3906)  mae_loss: 0.2451 (0.2411)  classification_loss: 0.1501 (0.1495)  time: 0.1954  data: 0.0002  max mem: 5511
[08:44:07.417080] Epoch: [30]  [260/781]  eta: 0:01:43  lr: 0.000209  training_loss: 0.3832 (0.3903)  mae_loss: 0.2289 (0.2407)  classification_loss: 0.1532 (0.1496)  time: 0.1949  data: 0.0002  max mem: 5511
[08:44:11.324132] Epoch: [30]  [280/781]  eta: 0:01:39  lr: 0.000209  training_loss: 0.3834 (0.3903)  mae_loss: 0.2310 (0.2407)  classification_loss: 0.1518 (0.1497)  time: 0.1953  data: 0.0002  max mem: 5511
[08:44:15.214192] Epoch: [30]  [300/781]  eta: 0:01:35  lr: 0.000209  training_loss: 0.3998 (0.3907)  mae_loss: 0.2482 (0.2409)  classification_loss: 0.1474 (0.1498)  time: 0.1944  data: 0.0002  max mem: 5511
[08:44:19.114167] Epoch: [30]  [320/781]  eta: 0:01:31  lr: 0.000209  training_loss: 0.3833 (0.3903)  mae_loss: 0.2349 (0.2405)  classification_loss: 0.1494 (0.1498)  time: 0.1949  data: 0.0002  max mem: 5511
[08:44:23.057966] Epoch: [30]  [340/781]  eta: 0:01:27  lr: 0.000208  training_loss: 0.3803 (0.3901)  mae_loss: 0.2464 (0.2404)  classification_loss: 0.1483 (0.1497)  time: 0.1971  data: 0.0004  max mem: 5511
[08:44:26.983041] Epoch: [30]  [360/781]  eta: 0:01:23  lr: 0.000208  training_loss: 0.3899 (0.3904)  mae_loss: 0.2426 (0.2406)  classification_loss: 0.1496 (0.1498)  time: 0.1962  data: 0.0002  max mem: 5511
[08:44:30.888126] Epoch: [30]  [380/781]  eta: 0:01:19  lr: 0.000208  training_loss: 0.3822 (0.3901)  mae_loss: 0.2333 (0.2402)  classification_loss: 0.1505 (0.1498)  time: 0.1952  data: 0.0002  max mem: 5511
[08:44:34.815560] Epoch: [30]  [400/781]  eta: 0:01:15  lr: 0.000208  training_loss: 0.3878 (0.3900)  mae_loss: 0.2394 (0.2402)  classification_loss: 0.1498 (0.1498)  time: 0.1963  data: 0.0003  max mem: 5511
[08:44:38.714338] Epoch: [30]  [420/781]  eta: 0:01:11  lr: 0.000208  training_loss: 0.3855 (0.3898)  mae_loss: 0.2301 (0.2399)  classification_loss: 0.1524 (0.1499)  time: 0.1948  data: 0.0002  max mem: 5511
[08:44:42.617338] Epoch: [30]  [440/781]  eta: 0:01:07  lr: 0.000208  training_loss: 0.3805 (0.3894)  mae_loss: 0.2341 (0.2395)  classification_loss: 0.1464 (0.1499)  time: 0.1951  data: 0.0002  max mem: 5511
[08:44:46.521151] Epoch: [30]  [460/781]  eta: 0:01:03  lr: 0.000208  training_loss: 0.3837 (0.3894)  mae_loss: 0.2344 (0.2398)  classification_loss: 0.1431 (0.1496)  time: 0.1951  data: 0.0003  max mem: 5511
[08:44:50.425631] Epoch: [30]  [480/781]  eta: 0:00:59  lr: 0.000208  training_loss: 0.3879 (0.3895)  mae_loss: 0.2323 (0.2398)  classification_loss: 0.1543 (0.1498)  time: 0.1951  data: 0.0002  max mem: 5511
[08:44:54.357875] Epoch: [30]  [500/781]  eta: 0:00:55  lr: 0.000208  training_loss: 0.3882 (0.3895)  mae_loss: 0.2362 (0.2396)  classification_loss: 0.1529 (0.1499)  time: 0.1965  data: 0.0002  max mem: 5511
[08:44:58.257520] Epoch: [30]  [520/781]  eta: 0:00:51  lr: 0.000208  training_loss: 0.3927 (0.3896)  mae_loss: 0.2344 (0.2396)  classification_loss: 0.1493 (0.1500)  time: 0.1949  data: 0.0001  max mem: 5511
[08:45:02.186987] Epoch: [30]  [540/781]  eta: 0:00:47  lr: 0.000208  training_loss: 0.3867 (0.3896)  mae_loss: 0.2343 (0.2396)  classification_loss: 0.1516 (0.1500)  time: 0.1964  data: 0.0002  max mem: 5511
[08:45:06.081949] Epoch: [30]  [560/781]  eta: 0:00:43  lr: 0.000208  training_loss: 0.4061 (0.3898)  mae_loss: 0.2586 (0.2399)  classification_loss: 0.1450 (0.1499)  time: 0.1947  data: 0.0002  max mem: 5511
[08:45:09.987294] Epoch: [30]  [580/781]  eta: 0:00:39  lr: 0.000208  training_loss: 0.3787 (0.3900)  mae_loss: 0.2298 (0.2401)  classification_loss: 0.1489 (0.1499)  time: 0.1952  data: 0.0002  max mem: 5511
[08:45:13.889174] Epoch: [30]  [600/781]  eta: 0:00:35  lr: 0.000207  training_loss: 0.3851 (0.3898)  mae_loss: 0.2325 (0.2399)  classification_loss: 0.1469 (0.1498)  time: 0.1950  data: 0.0002  max mem: 5511
[08:45:17.802394] Epoch: [30]  [620/781]  eta: 0:00:31  lr: 0.000207  training_loss: 0.3775 (0.3895)  mae_loss: 0.2290 (0.2397)  classification_loss: 0.1481 (0.1498)  time: 0.1956  data: 0.0003  max mem: 5511
[08:45:21.713930] Epoch: [30]  [640/781]  eta: 0:00:27  lr: 0.000207  training_loss: 0.3885 (0.3893)  mae_loss: 0.2366 (0.2396)  classification_loss: 0.1464 (0.1497)  time: 0.1955  data: 0.0003  max mem: 5511
[08:45:25.623269] Epoch: [30]  [660/781]  eta: 0:00:23  lr: 0.000207  training_loss: 0.3873 (0.3892)  mae_loss: 0.2365 (0.2395)  classification_loss: 0.1508 (0.1497)  time: 0.1954  data: 0.0002  max mem: 5511
[08:45:29.545710] Epoch: [30]  [680/781]  eta: 0:00:19  lr: 0.000207  training_loss: 0.3985 (0.3895)  mae_loss: 0.2513 (0.2399)  classification_loss: 0.1484 (0.1496)  time: 0.1960  data: 0.0002  max mem: 5511
[08:45:33.449827] Epoch: [30]  [700/781]  eta: 0:00:15  lr: 0.000207  training_loss: 0.4003 (0.3898)  mae_loss: 0.2506 (0.2401)  classification_loss: 0.1535 (0.1496)  time: 0.1951  data: 0.0003  max mem: 5511
[08:45:37.354967] Epoch: [30]  [720/781]  eta: 0:00:11  lr: 0.000207  training_loss: 0.3874 (0.3899)  mae_loss: 0.2369 (0.2402)  classification_loss: 0.1505 (0.1497)  time: 0.1952  data: 0.0002  max mem: 5511
[08:45:41.315133] Epoch: [30]  [740/781]  eta: 0:00:08  lr: 0.000207  training_loss: 0.3803 (0.3896)  mae_loss: 0.2336 (0.2400)  classification_loss: 0.1481 (0.1497)  time: 0.1979  data: 0.0007  max mem: 5511
[08:45:45.213759] Epoch: [30]  [760/781]  eta: 0:00:04  lr: 0.000207  training_loss: 0.3872 (0.3896)  mae_loss: 0.2396 (0.2399)  classification_loss: 0.1515 (0.1497)  time: 0.1949  data: 0.0002  max mem: 5511
[08:45:49.105567] Epoch: [30]  [780/781]  eta: 0:00:00  lr: 0.000207  training_loss: 0.4049 (0.3898)  mae_loss: 0.2485 (0.2401)  classification_loss: 0.1459 (0.1497)  time: 0.1945  data: 0.0003  max mem: 5511
[08:45:49.252576] Epoch: [30] Total time: 0:02:33 (0.1967 s / it)
[08:45:49.253105] Averaged stats: lr: 0.000207  training_loss: 0.4049 (0.3898)  mae_loss: 0.2485 (0.2401)  classification_loss: 0.1459 (0.1497)
[08:45:50.818302] Test:  [  0/157]  eta: 0:01:30  testing_loss: 0.7848 (0.7848)  acc1: 76.5625 (76.5625)  acc5: 96.8750 (96.8750)  time: 0.5737  data: 0.5423  max mem: 5511
[08:45:51.112211] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.7848 (0.7713)  acc1: 76.5625 (74.0057)  acc5: 98.4375 (98.5795)  time: 0.0787  data: 0.0494  max mem: 5511
[08:45:51.400293] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.7403 (0.7388)  acc1: 76.5625 (74.7768)  acc5: 98.4375 (98.5863)  time: 0.0289  data: 0.0002  max mem: 5511
[08:45:51.681953] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.7413 (0.7551)  acc1: 75.0000 (74.8488)  acc5: 98.4375 (98.4375)  time: 0.0284  data: 0.0002  max mem: 5511
[08:45:51.964902] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.7509 (0.7567)  acc1: 75.0000 (75.1524)  acc5: 98.4375 (98.3613)  time: 0.0281  data: 0.0002  max mem: 5511
[08:45:52.252260] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.7509 (0.7532)  acc1: 75.0000 (75.0919)  acc5: 98.4375 (98.4069)  time: 0.0284  data: 0.0002  max mem: 5511
[08:45:52.544241] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.7404 (0.7522)  acc1: 73.4375 (74.8975)  acc5: 98.4375 (98.3607)  time: 0.0289  data: 0.0002  max mem: 5511
[08:45:52.828386] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7096 (0.7456)  acc1: 73.4375 (75.1981)  acc5: 98.4375 (98.3935)  time: 0.0287  data: 0.0001  max mem: 5511
[08:45:53.114729] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6997 (0.7474)  acc1: 76.5625 (75.1736)  acc5: 98.4375 (98.3025)  time: 0.0284  data: 0.0002  max mem: 5511
[08:45:53.408037] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7417 (0.7459)  acc1: 75.0000 (75.3434)  acc5: 98.4375 (98.3001)  time: 0.0288  data: 0.0002  max mem: 5511
[08:45:53.702423] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.7417 (0.7441)  acc1: 75.0000 (75.3094)  acc5: 100.0000 (98.4375)  time: 0.0293  data: 0.0002  max mem: 5511
[08:45:53.986934] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.7528 (0.7457)  acc1: 73.4375 (75.2534)  acc5: 100.0000 (98.4657)  time: 0.0288  data: 0.0002  max mem: 5511
[08:45:54.272348] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.7263 (0.7410)  acc1: 75.0000 (75.4132)  acc5: 98.4375 (98.5021)  time: 0.0284  data: 0.0002  max mem: 5511
[08:45:54.556522] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.7383 (0.7447)  acc1: 73.4375 (75.2505)  acc5: 98.4375 (98.4614)  time: 0.0284  data: 0.0002  max mem: 5511
[08:45:54.840444] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7383 (0.7434)  acc1: 75.0000 (75.4654)  acc5: 98.4375 (98.4597)  time: 0.0283  data: 0.0002  max mem: 5511
[08:45:55.120117] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6940 (0.7408)  acc1: 78.1250 (75.6312)  acc5: 98.4375 (98.4168)  time: 0.0281  data: 0.0002  max mem: 5511
[08:45:55.269547] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7118 (0.7407)  acc1: 75.0000 (75.5700)  acc5: 98.4375 (98.4200)  time: 0.0269  data: 0.0001  max mem: 5511
[08:45:55.412608] Test: Total time: 0:00:05 (0.0329 s / it)
[08:45:55.413054] * Acc@1 75.570 Acc@5 98.420 loss 0.741
[08:45:55.413335] Accuracy of the network on the 10000 test images: 75.6%
[08:45:55.413531] Max accuracy: 75.57%
[08:45:55.662491] log_dir: ./output_dir
[08:45:56.461794] Epoch: [31]  [  0/781]  eta: 0:10:23  lr: 0.000207  training_loss: 0.4002 (0.4002)  mae_loss: 0.2622 (0.2622)  classification_loss: 0.1380 (0.1380)  time: 0.7977  data: 0.5620  max mem: 5511
[08:46:00.380805] Epoch: [31]  [ 20/781]  eta: 0:02:50  lr: 0.000207  training_loss: 0.3871 (0.3915)  mae_loss: 0.2410 (0.2434)  classification_loss: 0.1458 (0.1481)  time: 0.1958  data: 0.0002  max mem: 5511
[08:46:04.296649] Epoch: [31]  [ 40/781]  eta: 0:02:35  lr: 0.000207  training_loss: 0.3922 (0.3881)  mae_loss: 0.2446 (0.2390)  classification_loss: 0.1456 (0.1491)  time: 0.1957  data: 0.0003  max mem: 5511
[08:46:08.206270] Epoch: [31]  [ 60/781]  eta: 0:02:28  lr: 0.000207  training_loss: 0.3953 (0.3914)  mae_loss: 0.2435 (0.2418)  classification_loss: 0.1481 (0.1497)  time: 0.1954  data: 0.0003  max mem: 5511
[08:46:12.104511] Epoch: [31]  [ 80/781]  eta: 0:02:22  lr: 0.000206  training_loss: 0.3835 (0.3899)  mae_loss: 0.2297 (0.2401)  classification_loss: 0.1497 (0.1497)  time: 0.1948  data: 0.0002  max mem: 5511
[08:46:16.034793] Epoch: [31]  [100/781]  eta: 0:02:17  lr: 0.000206  training_loss: 0.3924 (0.3899)  mae_loss: 0.2292 (0.2398)  classification_loss: 0.1526 (0.1501)  time: 0.1964  data: 0.0002  max mem: 5511
[08:46:19.977707] Epoch: [31]  [120/781]  eta: 0:02:12  lr: 0.000206  training_loss: 0.3769 (0.3889)  mae_loss: 0.2279 (0.2394)  classification_loss: 0.1443 (0.1495)  time: 0.1971  data: 0.0003  max mem: 5511
[08:46:23.893857] Epoch: [31]  [140/781]  eta: 0:02:08  lr: 0.000206  training_loss: 0.3781 (0.3884)  mae_loss: 0.2315 (0.2393)  classification_loss: 0.1471 (0.1491)  time: 0.1957  data: 0.0002  max mem: 5511
[08:46:27.797112] Epoch: [31]  [160/781]  eta: 0:02:03  lr: 0.000206  training_loss: 0.3797 (0.3881)  mae_loss: 0.2267 (0.2389)  classification_loss: 0.1543 (0.1492)  time: 0.1951  data: 0.0002  max mem: 5511
[08:46:31.706768] Epoch: [31]  [180/781]  eta: 0:01:59  lr: 0.000206  training_loss: 0.3718 (0.3878)  mae_loss: 0.2318 (0.2389)  classification_loss: 0.1471 (0.1489)  time: 0.1954  data: 0.0006  max mem: 5511
[08:46:35.604226] Epoch: [31]  [200/781]  eta: 0:01:55  lr: 0.000206  training_loss: 0.3857 (0.3876)  mae_loss: 0.2296 (0.2386)  classification_loss: 0.1482 (0.1490)  time: 0.1948  data: 0.0003  max mem: 5511
[08:46:39.511399] Epoch: [31]  [220/781]  eta: 0:01:51  lr: 0.000206  training_loss: 0.3999 (0.3880)  mae_loss: 0.2460 (0.2387)  classification_loss: 0.1525 (0.1493)  time: 0.1952  data: 0.0002  max mem: 5511
[08:46:43.430760] Epoch: [31]  [240/781]  eta: 0:01:47  lr: 0.000206  training_loss: 0.4037 (0.3890)  mae_loss: 0.2595 (0.2400)  classification_loss: 0.1443 (0.1490)  time: 0.1959  data: 0.0003  max mem: 5511
[08:46:47.321983] Epoch: [31]  [260/781]  eta: 0:01:43  lr: 0.000206  training_loss: 0.3866 (0.3887)  mae_loss: 0.2351 (0.2398)  classification_loss: 0.1447 (0.1489)  time: 0.1945  data: 0.0002  max mem: 5511
[08:46:51.248210] Epoch: [31]  [280/781]  eta: 0:01:39  lr: 0.000206  training_loss: 0.3859 (0.3884)  mae_loss: 0.2381 (0.2397)  classification_loss: 0.1446 (0.1487)  time: 0.1962  data: 0.0003  max mem: 5511
[08:46:55.152555] Epoch: [31]  [300/781]  eta: 0:01:35  lr: 0.000206  training_loss: 0.3941 (0.3889)  mae_loss: 0.2331 (0.2401)  classification_loss: 0.1493 (0.1488)  time: 0.1951  data: 0.0003  max mem: 5511
[08:46:59.062174] Epoch: [31]  [320/781]  eta: 0:01:31  lr: 0.000205  training_loss: 0.3811 (0.3890)  mae_loss: 0.2315 (0.2403)  classification_loss: 0.1510 (0.1487)  time: 0.1954  data: 0.0002  max mem: 5511
[08:47:03.010926] Epoch: [31]  [340/781]  eta: 0:01:27  lr: 0.000205  training_loss: 0.3639 (0.3887)  mae_loss: 0.2239 (0.2401)  classification_loss: 0.1454 (0.1486)  time: 0.1974  data: 0.0003  max mem: 5511
[08:47:06.964192] Epoch: [31]  [360/781]  eta: 0:01:23  lr: 0.000205  training_loss: 0.3743 (0.3888)  mae_loss: 0.2401 (0.2401)  classification_loss: 0.1469 (0.1487)  time: 0.1976  data: 0.0004  max mem: 5511
[08:47:10.912224] Epoch: [31]  [380/781]  eta: 0:01:19  lr: 0.000205  training_loss: 0.4012 (0.3888)  mae_loss: 0.2522 (0.2402)  classification_loss: 0.1493 (0.1486)  time: 0.1973  data: 0.0003  max mem: 5511
[08:47:14.849706] Epoch: [31]  [400/781]  eta: 0:01:15  lr: 0.000205  training_loss: 0.3733 (0.3885)  mae_loss: 0.2311 (0.2401)  classification_loss: 0.1404 (0.1484)  time: 0.1968  data: 0.0003  max mem: 5511
[08:47:18.739611] Epoch: [31]  [420/781]  eta: 0:01:11  lr: 0.000205  training_loss: 0.3712 (0.3878)  mae_loss: 0.2206 (0.2394)  classification_loss: 0.1491 (0.1484)  time: 0.1943  data: 0.0003  max mem: 5511
[08:47:22.663430] Epoch: [31]  [440/781]  eta: 0:01:07  lr: 0.000205  training_loss: 0.3802 (0.3879)  mae_loss: 0.2456 (0.2398)  classification_loss: 0.1441 (0.1481)  time: 0.1961  data: 0.0003  max mem: 5511
[08:47:26.593237] Epoch: [31]  [460/781]  eta: 0:01:03  lr: 0.000205  training_loss: 0.3933 (0.3882)  mae_loss: 0.2409 (0.2402)  classification_loss: 0.1446 (0.1480)  time: 0.1964  data: 0.0002  max mem: 5511
[08:47:30.510708] Epoch: [31]  [480/781]  eta: 0:00:59  lr: 0.000205  training_loss: 0.3916 (0.3885)  mae_loss: 0.2442 (0.2404)  classification_loss: 0.1478 (0.1482)  time: 0.1958  data: 0.0002  max mem: 5511
[08:47:34.414059] Epoch: [31]  [500/781]  eta: 0:00:55  lr: 0.000205  training_loss: 0.3871 (0.3887)  mae_loss: 0.2404 (0.2405)  classification_loss: 0.1475 (0.1482)  time: 0.1951  data: 0.0002  max mem: 5511
[08:47:38.306855] Epoch: [31]  [520/781]  eta: 0:00:51  lr: 0.000205  training_loss: 0.3811 (0.3887)  mae_loss: 0.2409 (0.2404)  classification_loss: 0.1522 (0.1483)  time: 0.1945  data: 0.0002  max mem: 5511
[08:47:42.214224] Epoch: [31]  [540/781]  eta: 0:00:47  lr: 0.000205  training_loss: 0.3875 (0.3888)  mae_loss: 0.2424 (0.2405)  classification_loss: 0.1440 (0.1483)  time: 0.1953  data: 0.0002  max mem: 5511
[08:47:46.121180] Epoch: [31]  [560/781]  eta: 0:00:43  lr: 0.000204  training_loss: 0.3817 (0.3888)  mae_loss: 0.2372 (0.2406)  classification_loss: 0.1409 (0.1481)  time: 0.1953  data: 0.0002  max mem: 5511
[08:47:50.053074] Epoch: [31]  [580/781]  eta: 0:00:39  lr: 0.000204  training_loss: 0.3936 (0.3887)  mae_loss: 0.2444 (0.2406)  classification_loss: 0.1475 (0.1481)  time: 0.1965  data: 0.0004  max mem: 5511
[08:47:53.975708] Epoch: [31]  [600/781]  eta: 0:00:35  lr: 0.000204  training_loss: 0.3686 (0.3883)  mae_loss: 0.2284 (0.2403)  classification_loss: 0.1419 (0.1480)  time: 0.1961  data: 0.0005  max mem: 5511
[08:47:57.899263] Epoch: [31]  [620/781]  eta: 0:00:31  lr: 0.000204  training_loss: 0.3883 (0.3885)  mae_loss: 0.2500 (0.2405)  classification_loss: 0.1476 (0.1480)  time: 0.1961  data: 0.0002  max mem: 5511
[08:48:01.787948] Epoch: [31]  [640/781]  eta: 0:00:27  lr: 0.000204  training_loss: 0.3730 (0.3883)  mae_loss: 0.2305 (0.2405)  classification_loss: 0.1426 (0.1478)  time: 0.1943  data: 0.0002  max mem: 5511
[08:48:05.670633] Epoch: [31]  [660/781]  eta: 0:00:23  lr: 0.000204  training_loss: 0.3829 (0.3883)  mae_loss: 0.2406 (0.2405)  classification_loss: 0.1503 (0.1479)  time: 0.1941  data: 0.0002  max mem: 5511
[08:48:09.570088] Epoch: [31]  [680/781]  eta: 0:00:19  lr: 0.000204  training_loss: 0.3813 (0.3884)  mae_loss: 0.2388 (0.2405)  classification_loss: 0.1469 (0.1478)  time: 0.1949  data: 0.0002  max mem: 5511
[08:48:13.477256] Epoch: [31]  [700/781]  eta: 0:00:15  lr: 0.000204  training_loss: 0.3732 (0.3881)  mae_loss: 0.2285 (0.2404)  classification_loss: 0.1443 (0.1478)  time: 0.1953  data: 0.0006  max mem: 5511
[08:48:17.373192] Epoch: [31]  [720/781]  eta: 0:00:11  lr: 0.000204  training_loss: 0.3893 (0.3882)  mae_loss: 0.2371 (0.2404)  classification_loss: 0.1470 (0.1478)  time: 0.1947  data: 0.0002  max mem: 5511
[08:48:21.271289] Epoch: [31]  [740/781]  eta: 0:00:08  lr: 0.000204  training_loss: 0.3788 (0.3881)  mae_loss: 0.2324 (0.2403)  classification_loss: 0.1434 (0.1477)  time: 0.1948  data: 0.0003  max mem: 5511
[08:48:25.169884] Epoch: [31]  [760/781]  eta: 0:00:04  lr: 0.000204  training_loss: 0.3884 (0.3881)  mae_loss: 0.2361 (0.2403)  classification_loss: 0.1566 (0.1479)  time: 0.1948  data: 0.0002  max mem: 5511
[08:48:29.061133] Epoch: [31]  [780/781]  eta: 0:00:00  lr: 0.000204  training_loss: 0.3719 (0.3879)  mae_loss: 0.2273 (0.2400)  classification_loss: 0.1471 (0.1479)  time: 0.1945  data: 0.0002  max mem: 5511
[08:48:29.236567] Epoch: [31] Total time: 0:02:33 (0.1966 s / it)
[08:48:29.237024] Averaged stats: lr: 0.000204  training_loss: 0.3719 (0.3879)  mae_loss: 0.2273 (0.2400)  classification_loss: 0.1471 (0.1479)
[08:48:29.954601] Test:  [  0/157]  eta: 0:01:51  testing_loss: 0.7608 (0.7608)  acc1: 78.1250 (78.1250)  acc5: 96.8750 (96.8750)  time: 0.7117  data: 0.6806  max mem: 5511
[08:48:30.246349] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.7787 (0.7803)  acc1: 75.0000 (74.7159)  acc5: 98.4375 (98.7216)  time: 0.0910  data: 0.0626  max mem: 5511
[08:48:30.528745] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.7555 (0.7549)  acc1: 75.0000 (75.0744)  acc5: 100.0000 (98.9583)  time: 0.0285  data: 0.0005  max mem: 5511
[08:48:30.814036] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.7486 (0.7718)  acc1: 75.0000 (74.7984)  acc5: 98.4375 (98.3367)  time: 0.0282  data: 0.0002  max mem: 5511
[08:48:31.096598] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.7628 (0.7723)  acc1: 75.0000 (75.0381)  acc5: 98.4375 (98.2851)  time: 0.0282  data: 0.0002  max mem: 5511
[08:48:31.385922] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.7449 (0.7663)  acc1: 76.5625 (75.3983)  acc5: 98.4375 (98.3150)  time: 0.0285  data: 0.0002  max mem: 5511
[08:48:31.674300] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.7540 (0.7639)  acc1: 76.5625 (75.3586)  acc5: 98.4375 (98.2582)  time: 0.0287  data: 0.0003  max mem: 5511
[08:48:31.958932] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.7328 (0.7542)  acc1: 78.1250 (76.1224)  acc5: 98.4375 (98.3715)  time: 0.0284  data: 0.0003  max mem: 5511
[08:48:32.242039] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.7099 (0.7578)  acc1: 78.1250 (75.9645)  acc5: 98.4375 (98.2832)  time: 0.0282  data: 0.0002  max mem: 5511
[08:48:32.524171] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7110 (0.7559)  acc1: 76.5625 (76.0302)  acc5: 98.4375 (98.3516)  time: 0.0281  data: 0.0002  max mem: 5511
[08:48:32.805592] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.7202 (0.7554)  acc1: 76.5625 (76.0210)  acc5: 100.0000 (98.3756)  time: 0.0280  data: 0.0002  max mem: 5511
[08:48:33.087148] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.7936 (0.7578)  acc1: 73.4375 (75.9009)  acc5: 98.4375 (98.3812)  time: 0.0280  data: 0.0002  max mem: 5511
[08:48:33.368105] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.7166 (0.7529)  acc1: 76.5625 (76.1493)  acc5: 98.4375 (98.4117)  time: 0.0280  data: 0.0001  max mem: 5511
[08:48:33.649215] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.7166 (0.7548)  acc1: 78.1250 (76.0615)  acc5: 98.4375 (98.4256)  time: 0.0280  data: 0.0001  max mem: 5511
[08:48:33.930086] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7411 (0.7546)  acc1: 75.0000 (76.0527)  acc5: 98.4375 (98.4264)  time: 0.0280  data: 0.0001  max mem: 5511
[08:48:34.208741] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7394 (0.7535)  acc1: 75.0000 (76.0451)  acc5: 98.4375 (98.3961)  time: 0.0278  data: 0.0001  max mem: 5511
[08:48:34.359119] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7438 (0.7555)  acc1: 75.0000 (75.9900)  acc5: 98.4375 (98.4100)  time: 0.0269  data: 0.0001  max mem: 5511
[08:48:34.512619] Test: Total time: 0:00:05 (0.0336 s / it)
[08:48:34.513074] * Acc@1 75.990 Acc@5 98.410 loss 0.755
[08:48:34.513368] Accuracy of the network on the 10000 test images: 76.0%
[08:48:34.513547] Max accuracy: 75.99%
[08:48:34.811972] log_dir: ./output_dir
[08:48:35.641671] Epoch: [32]  [  0/781]  eta: 0:10:46  lr: 0.000204  training_loss: 0.3686 (0.3686)  mae_loss: 0.2399 (0.2399)  classification_loss: 0.1287 (0.1287)  time: 0.8280  data: 0.6028  max mem: 5511
[08:48:39.566168] Epoch: [32]  [ 20/781]  eta: 0:02:52  lr: 0.000204  training_loss: 0.3732 (0.3878)  mae_loss: 0.2328 (0.2428)  classification_loss: 0.1442 (0.1450)  time: 0.1961  data: 0.0001  max mem: 5511
[08:48:43.509883] Epoch: [32]  [ 40/781]  eta: 0:02:37  lr: 0.000203  training_loss: 0.3757 (0.3789)  mae_loss: 0.2293 (0.2347)  classification_loss: 0.1419 (0.1442)  time: 0.1971  data: 0.0002  max mem: 5511
[08:48:47.411843] Epoch: [32]  [ 60/781]  eta: 0:02:28  lr: 0.000203  training_loss: 0.3869 (0.3844)  mae_loss: 0.2356 (0.2384)  classification_loss: 0.1530 (0.1460)  time: 0.1950  data: 0.0002  max mem: 5511
[08:48:51.302219] Epoch: [32]  [ 80/781]  eta: 0:02:22  lr: 0.000203  training_loss: 0.3836 (0.3862)  mae_loss: 0.2365 (0.2396)  classification_loss: 0.1461 (0.1465)  time: 0.1944  data: 0.0002  max mem: 5511
[08:48:55.276129] Epoch: [32]  [100/781]  eta: 0:02:17  lr: 0.000203  training_loss: 0.3772 (0.3849)  mae_loss: 0.2229 (0.2379)  classification_loss: 0.1504 (0.1470)  time: 0.1986  data: 0.0003  max mem: 5511
[08:48:59.178013] Epoch: [32]  [120/781]  eta: 0:02:13  lr: 0.000203  training_loss: 0.3746 (0.3839)  mae_loss: 0.2322 (0.2372)  classification_loss: 0.1427 (0.1467)  time: 0.1950  data: 0.0002  max mem: 5511
[08:49:03.096837] Epoch: [32]  [140/781]  eta: 0:02:08  lr: 0.000203  training_loss: 0.3817 (0.3839)  mae_loss: 0.2284 (0.2370)  classification_loss: 0.1427 (0.1468)  time: 0.1959  data: 0.0002  max mem: 5511
[08:49:07.004221] Epoch: [32]  [160/781]  eta: 0:02:04  lr: 0.000203  training_loss: 0.3759 (0.3824)  mae_loss: 0.2289 (0.2358)  classification_loss: 0.1461 (0.1466)  time: 0.1952  data: 0.0003  max mem: 5511
[08:49:10.912798] Epoch: [32]  [180/781]  eta: 0:01:59  lr: 0.000203  training_loss: 0.3836 (0.3829)  mae_loss: 0.2296 (0.2361)  classification_loss: 0.1440 (0.1467)  time: 0.1954  data: 0.0002  max mem: 5511
[08:49:14.833926] Epoch: [32]  [200/781]  eta: 0:01:55  lr: 0.000203  training_loss: 0.3827 (0.3831)  mae_loss: 0.2381 (0.2367)  classification_loss: 0.1459 (0.1464)  time: 0.1960  data: 0.0002  max mem: 5511
[08:49:18.736432] Epoch: [32]  [220/781]  eta: 0:01:51  lr: 0.000203  training_loss: 0.3726 (0.3823)  mae_loss: 0.2265 (0.2357)  classification_loss: 0.1451 (0.1465)  time: 0.1950  data: 0.0002  max mem: 5511
[08:49:22.695331] Epoch: [32]  [240/781]  eta: 0:01:47  lr: 0.000203  training_loss: 0.3779 (0.3820)  mae_loss: 0.2272 (0.2354)  classification_loss: 0.1489 (0.1467)  time: 0.1978  data: 0.0003  max mem: 5511
[08:49:26.608826] Epoch: [32]  [260/781]  eta: 0:01:43  lr: 0.000203  training_loss: 0.3794 (0.3819)  mae_loss: 0.2308 (0.2352)  classification_loss: 0.1469 (0.1468)  time: 0.1956  data: 0.0002  max mem: 5511
[08:49:30.507032] Epoch: [32]  [280/781]  eta: 0:01:39  lr: 0.000202  training_loss: 0.3953 (0.3827)  mae_loss: 0.2404 (0.2356)  classification_loss: 0.1500 (0.1471)  time: 0.1948  data: 0.0002  max mem: 5511
[08:49:34.426093] Epoch: [32]  [300/781]  eta: 0:01:35  lr: 0.000202  training_loss: 0.3909 (0.3836)  mae_loss: 0.2476 (0.2365)  classification_loss: 0.1438 (0.1471)  time: 0.1959  data: 0.0003  max mem: 5511
[08:49:38.322755] Epoch: [32]  [320/781]  eta: 0:01:31  lr: 0.000202  training_loss: 0.3767 (0.3833)  mae_loss: 0.2333 (0.2365)  classification_loss: 0.1424 (0.1468)  time: 0.1948  data: 0.0002  max mem: 5511
[08:49:42.281884] Epoch: [32]  [340/781]  eta: 0:01:27  lr: 0.000202  training_loss: 0.3803 (0.3835)  mae_loss: 0.2326 (0.2365)  classification_loss: 0.1421 (0.1469)  time: 0.1979  data: 0.0002  max mem: 5511
[08:49:46.218878] Epoch: [32]  [360/781]  eta: 0:01:23  lr: 0.000202  training_loss: 0.3691 (0.3827)  mae_loss: 0.2299 (0.2360)  classification_loss: 0.1436 (0.1467)  time: 0.1968  data: 0.0002  max mem: 5511
[08:49:50.147380] Epoch: [32]  [380/781]  eta: 0:01:19  lr: 0.000202  training_loss: 0.3732 (0.3827)  mae_loss: 0.2299 (0.2360)  classification_loss: 0.1454 (0.1467)  time: 0.1963  data: 0.0003  max mem: 5511
[08:49:54.051888] Epoch: [32]  [400/781]  eta: 0:01:15  lr: 0.000202  training_loss: 0.3672 (0.3824)  mae_loss: 0.2126 (0.2357)  classification_loss: 0.1443 (0.1467)  time: 0.1951  data: 0.0002  max mem: 5511
[08:49:57.949591] Epoch: [32]  [420/781]  eta: 0:01:11  lr: 0.000202  training_loss: 0.3913 (0.3828)  mae_loss: 0.2483 (0.2361)  classification_loss: 0.1476 (0.1467)  time: 0.1948  data: 0.0003  max mem: 5511
[08:50:01.843209] Epoch: [32]  [440/781]  eta: 0:01:07  lr: 0.000202  training_loss: 0.4027 (0.3832)  mae_loss: 0.2477 (0.2365)  classification_loss: 0.1459 (0.1467)  time: 0.1946  data: 0.0002  max mem: 5511
[08:50:05.729667] Epoch: [32]  [460/781]  eta: 0:01:03  lr: 0.000202  training_loss: 0.3677 (0.3825)  mae_loss: 0.2263 (0.2359)  classification_loss: 0.1440 (0.1466)  time: 0.1942  data: 0.0002  max mem: 5511
[08:50:09.623720] Epoch: [32]  [480/781]  eta: 0:00:59  lr: 0.000202  training_loss: 0.3867 (0.3824)  mae_loss: 0.2315 (0.2358)  classification_loss: 0.1463 (0.1467)  time: 0.1946  data: 0.0002  max mem: 5511
[08:50:13.497713] Epoch: [32]  [500/781]  eta: 0:00:55  lr: 0.000202  training_loss: 0.3617 (0.3819)  mae_loss: 0.2198 (0.2352)  classification_loss: 0.1496 (0.1467)  time: 0.1936  data: 0.0002  max mem: 5511
[08:50:17.412351] Epoch: [32]  [520/781]  eta: 0:00:51  lr: 0.000201  training_loss: 0.3913 (0.3823)  mae_loss: 0.2426 (0.2354)  classification_loss: 0.1494 (0.1468)  time: 0.1956  data: 0.0002  max mem: 5511
[08:50:21.319162] Epoch: [32]  [540/781]  eta: 0:00:47  lr: 0.000201  training_loss: 0.3839 (0.3825)  mae_loss: 0.2364 (0.2357)  classification_loss: 0.1451 (0.1468)  time: 0.1953  data: 0.0002  max mem: 5511
[08:50:25.228163] Epoch: [32]  [560/781]  eta: 0:00:43  lr: 0.000201  training_loss: 0.3778 (0.3823)  mae_loss: 0.2321 (0.2355)  classification_loss: 0.1410 (0.1467)  time: 0.1954  data: 0.0002  max mem: 5511
[08:50:29.133964] Epoch: [32]  [580/781]  eta: 0:00:39  lr: 0.000201  training_loss: 0.3746 (0.3822)  mae_loss: 0.2279 (0.2355)  classification_loss: 0.1436 (0.1467)  time: 0.1952  data: 0.0002  max mem: 5511
[08:50:33.072807] Epoch: [32]  [600/781]  eta: 0:00:35  lr: 0.000201  training_loss: 0.3810 (0.3823)  mae_loss: 0.2358 (0.2356)  classification_loss: 0.1509 (0.1467)  time: 0.1968  data: 0.0003  max mem: 5511
[08:50:36.964652] Epoch: [32]  [620/781]  eta: 0:00:31  lr: 0.000201  training_loss: 0.3963 (0.3825)  mae_loss: 0.2439 (0.2359)  classification_loss: 0.1432 (0.1467)  time: 0.1945  data: 0.0002  max mem: 5511
[08:50:40.859437] Epoch: [32]  [640/781]  eta: 0:00:27  lr: 0.000201  training_loss: 0.3718 (0.3822)  mae_loss: 0.2237 (0.2356)  classification_loss: 0.1460 (0.1466)  time: 0.1946  data: 0.0002  max mem: 5511
[08:50:44.816204] Epoch: [32]  [660/781]  eta: 0:00:23  lr: 0.000201  training_loss: 0.3885 (0.3825)  mae_loss: 0.2383 (0.2358)  classification_loss: 0.1480 (0.1467)  time: 0.1978  data: 0.0002  max mem: 5511
[08:50:48.700746] Epoch: [32]  [680/781]  eta: 0:00:19  lr: 0.000201  training_loss: 0.3894 (0.3826)  mae_loss: 0.2392 (0.2359)  classification_loss: 0.1430 (0.1466)  time: 0.1941  data: 0.0002  max mem: 5511
[08:50:52.613257] Epoch: [32]  [700/781]  eta: 0:00:15  lr: 0.000201  training_loss: 0.3816 (0.3827)  mae_loss: 0.2389 (0.2360)  classification_loss: 0.1498 (0.1467)  time: 0.1956  data: 0.0002  max mem: 5511
[08:50:56.528135] Epoch: [32]  [720/781]  eta: 0:00:11  lr: 0.000201  training_loss: 0.3756 (0.3827)  mae_loss: 0.2340 (0.2360)  classification_loss: 0.1468 (0.1467)  time: 0.1957  data: 0.0007  max mem: 5511
[08:51:00.427403] Epoch: [32]  [740/781]  eta: 0:00:08  lr: 0.000201  training_loss: 0.3793 (0.3825)  mae_loss: 0.2355 (0.2359)  classification_loss: 0.1423 (0.1466)  time: 0.1949  data: 0.0002  max mem: 5511
[08:51:04.335045] Epoch: [32]  [760/781]  eta: 0:00:04  lr: 0.000200  training_loss: 0.3771 (0.3824)  mae_loss: 0.2227 (0.2358)  classification_loss: 0.1490 (0.1467)  time: 0.1953  data: 0.0003  max mem: 5511
[08:51:08.221913] Epoch: [32]  [780/781]  eta: 0:00:00  lr: 0.000200  training_loss: 0.3751 (0.3823)  mae_loss: 0.2279 (0.2357)  classification_loss: 0.1434 (0.1466)  time: 0.1943  data: 0.0002  max mem: 5511
[08:51:08.396093] Epoch: [32] Total time: 0:02:33 (0.1966 s / it)
[08:51:08.397190] Averaged stats: lr: 0.000200  training_loss: 0.3751 (0.3823)  mae_loss: 0.2279 (0.2357)  classification_loss: 0.1434 (0.1466)
[08:51:09.065187] Test:  [  0/157]  eta: 0:01:44  testing_loss: 0.7423 (0.7423)  acc1: 79.6875 (79.6875)  acc5: 95.3125 (95.3125)  time: 0.6639  data: 0.6339  max mem: 5511
[08:51:09.348782] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.7429 (0.7542)  acc1: 76.5625 (75.2841)  acc5: 98.4375 (98.8636)  time: 0.0860  data: 0.0578  max mem: 5511
[08:51:09.637404] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.6901 (0.7276)  acc1: 76.5625 (75.9673)  acc5: 100.0000 (99.0327)  time: 0.0285  data: 0.0002  max mem: 5511
[08:51:09.927292] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.7563 (0.7532)  acc1: 76.5625 (75.4536)  acc5: 98.4375 (98.6391)  time: 0.0288  data: 0.0002  max mem: 5511
[08:51:10.209572] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.7643 (0.7534)  acc1: 76.5625 (75.6479)  acc5: 98.4375 (98.6280)  time: 0.0285  data: 0.0002  max mem: 5511
[08:51:10.491905] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.7412 (0.7503)  acc1: 76.5625 (75.6740)  acc5: 98.4375 (98.6826)  time: 0.0281  data: 0.0002  max mem: 5511
[08:51:10.773360] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.7310 (0.7508)  acc1: 75.0000 (75.3074)  acc5: 98.4375 (98.6680)  time: 0.0281  data: 0.0002  max mem: 5511
[08:51:11.056029] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6837 (0.7391)  acc1: 76.5625 (75.9683)  acc5: 98.4375 (98.7236)  time: 0.0281  data: 0.0001  max mem: 5511
[08:51:11.337370] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6992 (0.7417)  acc1: 76.5625 (75.6944)  acc5: 98.4375 (98.6111)  time: 0.0281  data: 0.0002  max mem: 5511
[08:51:11.628285] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7044 (0.7404)  acc1: 76.5625 (75.7898)  acc5: 98.4375 (98.5920)  time: 0.0285  data: 0.0002  max mem: 5511
[08:51:11.910218] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.7273 (0.7407)  acc1: 76.5625 (75.7426)  acc5: 98.4375 (98.6077)  time: 0.0285  data: 0.0001  max mem: 5511
[08:51:12.192304] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.7448 (0.7413)  acc1: 73.4375 (75.5771)  acc5: 98.4375 (98.5501)  time: 0.0280  data: 0.0002  max mem: 5511
[08:51:12.476632] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.7395 (0.7367)  acc1: 75.0000 (75.7619)  acc5: 98.4375 (98.5666)  time: 0.0282  data: 0.0002  max mem: 5511
[08:51:12.761629] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.7234 (0.7385)  acc1: 76.5625 (75.5606)  acc5: 98.4375 (98.5329)  time: 0.0283  data: 0.0002  max mem: 5511
[08:51:13.042607] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7441 (0.7384)  acc1: 76.5625 (75.7092)  acc5: 98.4375 (98.5040)  time: 0.0282  data: 0.0002  max mem: 5511
[08:51:13.320429] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7611 (0.7364)  acc1: 76.5625 (75.8485)  acc5: 98.4375 (98.4478)  time: 0.0278  data: 0.0001  max mem: 5511
[08:51:13.470213] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7604 (0.7360)  acc1: 76.5625 (75.7500)  acc5: 98.4375 (98.4800)  time: 0.0268  data: 0.0001  max mem: 5511
[08:51:13.639233] Test: Total time: 0:00:05 (0.0334 s / it)
[08:51:13.639975] * Acc@1 75.750 Acc@5 98.480 loss 0.736
[08:51:13.640273] Accuracy of the network on the 10000 test images: 75.8%
[08:51:13.640446] Max accuracy: 75.99%
[08:51:13.870121] log_dir: ./output_dir
[08:51:14.785430] Epoch: [33]  [  0/781]  eta: 0:11:53  lr: 0.000200  training_loss: 0.3635 (0.3635)  mae_loss: 0.2255 (0.2255)  classification_loss: 0.1379 (0.1379)  time: 0.9132  data: 0.6821  max mem: 5511
[08:51:18.685872] Epoch: [33]  [ 20/781]  eta: 0:02:54  lr: 0.000200  training_loss: 0.3715 (0.3693)  mae_loss: 0.2220 (0.2263)  classification_loss: 0.1424 (0.1430)  time: 0.1949  data: 0.0002  max mem: 5511
[08:51:22.587694] Epoch: [33]  [ 40/781]  eta: 0:02:37  lr: 0.000200  training_loss: 0.3784 (0.3750)  mae_loss: 0.2341 (0.2306)  classification_loss: 0.1460 (0.1444)  time: 0.1950  data: 0.0002  max mem: 5511
[08:51:26.494362] Epoch: [33]  [ 60/781]  eta: 0:02:29  lr: 0.000200  training_loss: 0.3800 (0.3764)  mae_loss: 0.2370 (0.2307)  classification_loss: 0.1479 (0.1457)  time: 0.1952  data: 0.0002  max mem: 5511
[08:51:30.444928] Epoch: [33]  [ 80/781]  eta: 0:02:23  lr: 0.000200  training_loss: 0.3672 (0.3747)  mae_loss: 0.2163 (0.2286)  classification_loss: 0.1464 (0.1461)  time: 0.1975  data: 0.0003  max mem: 5511
[08:51:34.350822] Epoch: [33]  [100/781]  eta: 0:02:18  lr: 0.000200  training_loss: 0.3515 (0.3727)  mae_loss: 0.2129 (0.2265)  classification_loss: 0.1455 (0.1462)  time: 0.1952  data: 0.0002  max mem: 5511
[08:51:38.236074] Epoch: [33]  [120/781]  eta: 0:02:13  lr: 0.000200  training_loss: 0.3536 (0.3720)  mae_loss: 0.2166 (0.2263)  classification_loss: 0.1422 (0.1457)  time: 0.1942  data: 0.0002  max mem: 5511
[08:51:42.150326] Epoch: [33]  [140/781]  eta: 0:02:08  lr: 0.000200  training_loss: 0.3763 (0.3728)  mae_loss: 0.2325 (0.2274)  classification_loss: 0.1431 (0.1455)  time: 0.1956  data: 0.0002  max mem: 5511
[08:51:46.094013] Epoch: [33]  [160/781]  eta: 0:02:04  lr: 0.000200  training_loss: 0.3673 (0.3724)  mae_loss: 0.2192 (0.2270)  classification_loss: 0.1425 (0.1454)  time: 0.1971  data: 0.0003  max mem: 5511
[08:51:50.003968] Epoch: [33]  [180/781]  eta: 0:01:59  lr: 0.000200  training_loss: 0.3757 (0.3726)  mae_loss: 0.2277 (0.2276)  classification_loss: 0.1440 (0.1450)  time: 0.1954  data: 0.0003  max mem: 5511
[08:51:53.915600] Epoch: [33]  [200/781]  eta: 0:01:55  lr: 0.000199  training_loss: 0.3630 (0.3732)  mae_loss: 0.2198 (0.2283)  classification_loss: 0.1436 (0.1449)  time: 0.1955  data: 0.0003  max mem: 5511
[08:51:57.799982] Epoch: [33]  [220/781]  eta: 0:01:51  lr: 0.000199  training_loss: 0.3653 (0.3728)  mae_loss: 0.2131 (0.2279)  classification_loss: 0.1478 (0.1450)  time: 0.1941  data: 0.0003  max mem: 5511
[08:52:01.782554] Epoch: [33]  [240/781]  eta: 0:01:47  lr: 0.000199  training_loss: 0.3812 (0.3737)  mae_loss: 0.2348 (0.2287)  classification_loss: 0.1448 (0.1450)  time: 0.1990  data: 0.0002  max mem: 5511
[08:52:05.692397] Epoch: [33]  [260/781]  eta: 0:01:43  lr: 0.000199  training_loss: 0.3852 (0.3751)  mae_loss: 0.2415 (0.2299)  classification_loss: 0.1468 (0.1452)  time: 0.1954  data: 0.0002  max mem: 5511
[08:52:09.607936] Epoch: [33]  [280/781]  eta: 0:01:39  lr: 0.000199  training_loss: 0.3847 (0.3760)  mae_loss: 0.2374 (0.2306)  classification_loss: 0.1463 (0.1454)  time: 0.1957  data: 0.0002  max mem: 5511
[08:52:13.504967] Epoch: [33]  [300/781]  eta: 0:01:35  lr: 0.000199  training_loss: 0.3786 (0.3763)  mae_loss: 0.2360 (0.2309)  classification_loss: 0.1450 (0.1454)  time: 0.1948  data: 0.0002  max mem: 5511
[08:52:17.414439] Epoch: [33]  [320/781]  eta: 0:01:31  lr: 0.000199  training_loss: 0.3712 (0.3764)  mae_loss: 0.2293 (0.2310)  classification_loss: 0.1446 (0.1454)  time: 0.1954  data: 0.0002  max mem: 5511
[08:52:21.331611] Epoch: [33]  [340/781]  eta: 0:01:27  lr: 0.000199  training_loss: 0.3712 (0.3761)  mae_loss: 0.2246 (0.2308)  classification_loss: 0.1417 (0.1452)  time: 0.1958  data: 0.0002  max mem: 5511
[08:52:25.268728] Epoch: [33]  [360/781]  eta: 0:01:23  lr: 0.000199  training_loss: 0.3814 (0.3767)  mae_loss: 0.2342 (0.2312)  classification_loss: 0.1493 (0.1454)  time: 0.1968  data: 0.0002  max mem: 5511
[08:52:29.203440] Epoch: [33]  [380/781]  eta: 0:01:19  lr: 0.000199  training_loss: 0.3678 (0.3763)  mae_loss: 0.2187 (0.2308)  classification_loss: 0.1439 (0.1455)  time: 0.1967  data: 0.0003  max mem: 5511
[08:52:33.146773] Epoch: [33]  [400/781]  eta: 0:01:15  lr: 0.000199  training_loss: 0.3861 (0.3768)  mae_loss: 0.2401 (0.2315)  classification_loss: 0.1442 (0.1454)  time: 0.1970  data: 0.0003  max mem: 5511
[08:52:37.053121] Epoch: [33]  [420/781]  eta: 0:01:11  lr: 0.000199  training_loss: 0.3870 (0.3771)  mae_loss: 0.2478 (0.2318)  classification_loss: 0.1433 (0.1453)  time: 0.1952  data: 0.0003  max mem: 5511
[08:52:40.950745] Epoch: [33]  [440/781]  eta: 0:01:07  lr: 0.000198  training_loss: 0.3829 (0.3773)  mae_loss: 0.2356 (0.2319)  classification_loss: 0.1490 (0.1453)  time: 0.1948  data: 0.0003  max mem: 5511
[08:52:44.870914] Epoch: [33]  [460/781]  eta: 0:01:03  lr: 0.000198  training_loss: 0.3656 (0.3769)  mae_loss: 0.2282 (0.2317)  classification_loss: 0.1408 (0.1452)  time: 0.1959  data: 0.0002  max mem: 5511
[08:52:48.792551] Epoch: [33]  [480/781]  eta: 0:00:59  lr: 0.000198  training_loss: 0.3692 (0.3768)  mae_loss: 0.2223 (0.2314)  classification_loss: 0.1504 (0.1454)  time: 0.1960  data: 0.0005  max mem: 5511
[08:52:52.722320] Epoch: [33]  [500/781]  eta: 0:00:55  lr: 0.000198  training_loss: 0.3735 (0.3769)  mae_loss: 0.2338 (0.2315)  classification_loss: 0.1411 (0.1453)  time: 0.1964  data: 0.0003  max mem: 5511
[08:52:56.657785] Epoch: [33]  [520/781]  eta: 0:00:51  lr: 0.000198  training_loss: 0.3810 (0.3772)  mae_loss: 0.2377 (0.2318)  classification_loss: 0.1459 (0.1454)  time: 0.1967  data: 0.0002  max mem: 5511
[08:53:00.608682] Epoch: [33]  [540/781]  eta: 0:00:47  lr: 0.000198  training_loss: 0.3916 (0.3777)  mae_loss: 0.2394 (0.2322)  classification_loss: 0.1497 (0.1455)  time: 0.1975  data: 0.0002  max mem: 5511
[08:53:04.512371] Epoch: [33]  [560/781]  eta: 0:00:43  lr: 0.000198  training_loss: 0.3621 (0.3775)  mae_loss: 0.2213 (0.2321)  classification_loss: 0.1434 (0.1454)  time: 0.1951  data: 0.0002  max mem: 5511
[08:53:08.413380] Epoch: [33]  [580/781]  eta: 0:00:39  lr: 0.000198  training_loss: 0.3723 (0.3775)  mae_loss: 0.2332 (0.2321)  classification_loss: 0.1426 (0.1454)  time: 0.1950  data: 0.0003  max mem: 5511
[08:53:12.355322] Epoch: [33]  [600/781]  eta: 0:00:35  lr: 0.000198  training_loss: 0.3852 (0.3777)  mae_loss: 0.2343 (0.2324)  classification_loss: 0.1407 (0.1453)  time: 0.1970  data: 0.0002  max mem: 5511
[08:53:16.262869] Epoch: [33]  [620/781]  eta: 0:00:31  lr: 0.000198  training_loss: 0.3890 (0.3780)  mae_loss: 0.2400 (0.2327)  classification_loss: 0.1447 (0.1454)  time: 0.1953  data: 0.0002  max mem: 5511
[08:53:20.224150] Epoch: [33]  [640/781]  eta: 0:00:27  lr: 0.000198  training_loss: 0.3519 (0.3776)  mae_loss: 0.2187 (0.2324)  classification_loss: 0.1399 (0.1452)  time: 0.1980  data: 0.0002  max mem: 5511
[08:53:24.166053] Epoch: [33]  [660/781]  eta: 0:00:23  lr: 0.000198  training_loss: 0.3791 (0.3778)  mae_loss: 0.2397 (0.2326)  classification_loss: 0.1437 (0.1452)  time: 0.1970  data: 0.0002  max mem: 5511
[08:53:28.074297] Epoch: [33]  [680/781]  eta: 0:00:19  lr: 0.000197  training_loss: 0.3806 (0.3780)  mae_loss: 0.2323 (0.2327)  classification_loss: 0.1451 (0.1453)  time: 0.1953  data: 0.0002  max mem: 5511
[08:53:32.022423] Epoch: [33]  [700/781]  eta: 0:00:15  lr: 0.000197  training_loss: 0.3873 (0.3783)  mae_loss: 0.2282 (0.2329)  classification_loss: 0.1459 (0.1454)  time: 0.1973  data: 0.0002  max mem: 5511
[08:53:35.930772] Epoch: [33]  [720/781]  eta: 0:00:12  lr: 0.000197  training_loss: 0.3919 (0.3787)  mae_loss: 0.2418 (0.2332)  classification_loss: 0.1450 (0.1454)  time: 0.1953  data: 0.0002  max mem: 5511
[08:53:39.829200] Epoch: [33]  [740/781]  eta: 0:00:08  lr: 0.000197  training_loss: 0.3664 (0.3785)  mae_loss: 0.2283 (0.2332)  classification_loss: 0.1413 (0.1454)  time: 0.1948  data: 0.0002  max mem: 5511
[08:53:43.751623] Epoch: [33]  [760/781]  eta: 0:00:04  lr: 0.000197  training_loss: 0.3839 (0.3787)  mae_loss: 0.2450 (0.2333)  classification_loss: 0.1467 (0.1454)  time: 0.1960  data: 0.0003  max mem: 5511
[08:53:47.646096] Epoch: [33]  [780/781]  eta: 0:00:00  lr: 0.000197  training_loss: 0.3774 (0.3787)  mae_loss: 0.2355 (0.2333)  classification_loss: 0.1414 (0.1454)  time: 0.1946  data: 0.0002  max mem: 5511
[08:53:47.802917] Epoch: [33] Total time: 0:02:33 (0.1971 s / it)
[08:53:47.803417] Averaged stats: lr: 0.000197  training_loss: 0.3774 (0.3787)  mae_loss: 0.2355 (0.2333)  classification_loss: 0.1414 (0.1454)
[08:53:48.352600] Test:  [  0/157]  eta: 0:01:25  testing_loss: 0.6546 (0.6546)  acc1: 79.6875 (79.6875)  acc5: 95.3125 (95.3125)  time: 0.5451  data: 0.5154  max mem: 5511
[08:53:48.638803] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.7023 (0.7471)  acc1: 76.5625 (75.9943)  acc5: 100.0000 (99.1477)  time: 0.0754  data: 0.0472  max mem: 5511
[08:53:48.933880] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.6967 (0.7111)  acc1: 78.1250 (77.5298)  acc5: 100.0000 (99.0327)  time: 0.0289  data: 0.0003  max mem: 5511
[08:53:49.218069] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.6908 (0.7179)  acc1: 78.1250 (77.4698)  acc5: 98.4375 (98.7903)  time: 0.0288  data: 0.0002  max mem: 5511
[08:53:49.500465] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.7047 (0.7250)  acc1: 78.1250 (77.1341)  acc5: 98.4375 (98.6662)  time: 0.0282  data: 0.0002  max mem: 5511
[08:53:49.784285] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.7071 (0.7233)  acc1: 76.5625 (77.1752)  acc5: 98.4375 (98.5294)  time: 0.0282  data: 0.0002  max mem: 5511
[08:53:50.076117] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6971 (0.7194)  acc1: 76.5625 (77.2797)  acc5: 98.4375 (98.4887)  time: 0.0286  data: 0.0002  max mem: 5511
[08:53:50.372750] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6570 (0.7085)  acc1: 79.6875 (77.8169)  acc5: 98.4375 (98.5475)  time: 0.0292  data: 0.0002  max mem: 5511
[08:53:50.665595] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6570 (0.7103)  acc1: 79.6875 (77.7006)  acc5: 98.4375 (98.3796)  time: 0.0293  data: 0.0002  max mem: 5511
[08:53:50.953268] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.7076 (0.7074)  acc1: 78.1250 (77.9018)  acc5: 98.4375 (98.4890)  time: 0.0288  data: 0.0002  max mem: 5511
[08:53:51.239380] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.7076 (0.7104)  acc1: 78.1250 (77.6300)  acc5: 100.0000 (98.5303)  time: 0.0284  data: 0.0003  max mem: 5511
[08:53:51.523025] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.7011 (0.7105)  acc1: 76.5625 (77.5479)  acc5: 98.4375 (98.5501)  time: 0.0283  data: 0.0002  max mem: 5511
[08:53:51.806443] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6839 (0.7037)  acc1: 78.1250 (77.6860)  acc5: 98.4375 (98.6183)  time: 0.0282  data: 0.0002  max mem: 5511
[08:53:52.090040] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6309 (0.7057)  acc1: 78.1250 (77.5763)  acc5: 98.4375 (98.6283)  time: 0.0282  data: 0.0002  max mem: 5511
[08:53:52.371753] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7215 (0.7058)  acc1: 76.5625 (77.5377)  acc5: 98.4375 (98.6480)  time: 0.0281  data: 0.0002  max mem: 5511
[08:53:52.650671] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6965 (0.7054)  acc1: 76.5625 (77.4524)  acc5: 98.4375 (98.6341)  time: 0.0279  data: 0.0001  max mem: 5511
[08:53:52.800767] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6853 (0.7048)  acc1: 78.1250 (77.4400)  acc5: 98.4375 (98.6600)  time: 0.0269  data: 0.0001  max mem: 5511
[08:53:52.966766] Test: Total time: 0:00:05 (0.0329 s / it)
[08:53:52.967341] * Acc@1 77.440 Acc@5 98.660 loss 0.705
[08:53:52.967656] Accuracy of the network on the 10000 test images: 77.4%
[08:53:52.967838] Max accuracy: 77.44%
[08:53:53.185497] log_dir: ./output_dir
[08:53:54.064624] Epoch: [34]  [  0/781]  eta: 0:11:25  lr: 0.000197  training_loss: 0.3578 (0.3578)  mae_loss: 0.2125 (0.2125)  classification_loss: 0.1452 (0.1452)  time: 0.8774  data: 0.6708  max mem: 5511
[08:53:57.983081] Epoch: [34]  [ 20/781]  eta: 0:02:53  lr: 0.000197  training_loss: 0.3629 (0.3710)  mae_loss: 0.2272 (0.2297)  classification_loss: 0.1411 (0.1413)  time: 0.1958  data: 0.0003  max mem: 5511
[08:54:01.893437] Epoch: [34]  [ 40/781]  eta: 0:02:37  lr: 0.000197  training_loss: 0.3840 (0.3782)  mae_loss: 0.2420 (0.2357)  classification_loss: 0.1428 (0.1425)  time: 0.1954  data: 0.0002  max mem: 5511
[08:54:05.811193] Epoch: [34]  [ 60/781]  eta: 0:02:29  lr: 0.000197  training_loss: 0.3744 (0.3789)  mae_loss: 0.2341 (0.2357)  classification_loss: 0.1417 (0.1432)  time: 0.1958  data: 0.0002  max mem: 5511
[08:54:09.715963] Epoch: [34]  [ 80/781]  eta: 0:02:22  lr: 0.000197  training_loss: 0.3715 (0.3779)  mae_loss: 0.2285 (0.2346)  classification_loss: 0.1430 (0.1433)  time: 0.1952  data: 0.0002  max mem: 5511
[08:54:13.607437] Epoch: [34]  [100/781]  eta: 0:02:17  lr: 0.000197  training_loss: 0.3767 (0.3786)  mae_loss: 0.2260 (0.2354)  classification_loss: 0.1423 (0.1432)  time: 0.1945  data: 0.0002  max mem: 5511
[08:54:17.492500] Epoch: [34]  [120/781]  eta: 0:02:12  lr: 0.000196  training_loss: 0.3636 (0.3776)  mae_loss: 0.2274 (0.2345)  classification_loss: 0.1440 (0.1432)  time: 0.1942  data: 0.0002  max mem: 5511
[08:54:21.399721] Epoch: [34]  [140/781]  eta: 0:02:08  lr: 0.000196  training_loss: 0.3833 (0.3780)  mae_loss: 0.2374 (0.2350)  classification_loss: 0.1415 (0.1429)  time: 0.1953  data: 0.0002  max mem: 5511
[08:54:25.294168] Epoch: [34]  [160/781]  eta: 0:02:03  lr: 0.000196  training_loss: 0.3746 (0.3779)  mae_loss: 0.2311 (0.2349)  classification_loss: 0.1428 (0.1430)  time: 0.1946  data: 0.0002  max mem: 5511
[08:54:29.214886] Epoch: [34]  [180/781]  eta: 0:01:59  lr: 0.000196  training_loss: 0.3758 (0.3785)  mae_loss: 0.2353 (0.2358)  classification_loss: 0.1439 (0.1427)  time: 0.1959  data: 0.0002  max mem: 5511
[08:54:33.122069] Epoch: [34]  [200/781]  eta: 0:01:55  lr: 0.000196  training_loss: 0.3700 (0.3783)  mae_loss: 0.2263 (0.2353)  classification_loss: 0.1480 (0.1430)  time: 0.1953  data: 0.0002  max mem: 5511
[08:54:37.027147] Epoch: [34]  [220/781]  eta: 0:01:51  lr: 0.000196  training_loss: 0.3698 (0.3777)  mae_loss: 0.2334 (0.2344)  classification_loss: 0.1461 (0.1433)  time: 0.1952  data: 0.0002  max mem: 5511
[08:54:40.967656] Epoch: [34]  [240/781]  eta: 0:01:47  lr: 0.000196  training_loss: 0.3791 (0.3778)  mae_loss: 0.2339 (0.2344)  classification_loss: 0.1455 (0.1434)  time: 0.1970  data: 0.0002  max mem: 5511
[08:54:44.886030] Epoch: [34]  [260/781]  eta: 0:01:43  lr: 0.000196  training_loss: 0.3801 (0.3776)  mae_loss: 0.2310 (0.2343)  classification_loss: 0.1428 (0.1432)  time: 0.1958  data: 0.0002  max mem: 5511
[08:54:48.807666] Epoch: [34]  [280/781]  eta: 0:01:39  lr: 0.000196  training_loss: 0.3782 (0.3781)  mae_loss: 0.2428 (0.2349)  classification_loss: 0.1427 (0.1433)  time: 0.1960  data: 0.0002  max mem: 5511
[08:54:52.725796] Epoch: [34]  [300/781]  eta: 0:01:35  lr: 0.000196  training_loss: 0.3712 (0.3778)  mae_loss: 0.2318 (0.2345)  classification_loss: 0.1433 (0.1433)  time: 0.1958  data: 0.0002  max mem: 5511
[08:54:56.622208] Epoch: [34]  [320/781]  eta: 0:01:31  lr: 0.000196  training_loss: 0.3727 (0.3775)  mae_loss: 0.2237 (0.2341)  classification_loss: 0.1439 (0.1434)  time: 0.1947  data: 0.0002  max mem: 5511
[08:55:00.552850] Epoch: [34]  [340/781]  eta: 0:01:27  lr: 0.000196  training_loss: 0.3672 (0.3771)  mae_loss: 0.2215 (0.2339)  classification_loss: 0.1394 (0.1431)  time: 0.1965  data: 0.0002  max mem: 5511
[08:55:04.448645] Epoch: [34]  [360/781]  eta: 0:01:23  lr: 0.000195  training_loss: 0.3689 (0.3771)  mae_loss: 0.2208 (0.2338)  classification_loss: 0.1431 (0.1433)  time: 0.1947  data: 0.0002  max mem: 5511
[08:55:08.343643] Epoch: [34]  [380/781]  eta: 0:01:19  lr: 0.000195  training_loss: 0.3644 (0.3770)  mae_loss: 0.2208 (0.2337)  classification_loss: 0.1439 (0.1433)  time: 0.1947  data: 0.0002  max mem: 5511
[08:55:12.254522] Epoch: [34]  [400/781]  eta: 0:01:15  lr: 0.000195  training_loss: 0.3806 (0.3772)  mae_loss: 0.2329 (0.2338)  classification_loss: 0.1471 (0.1434)  time: 0.1954  data: 0.0003  max mem: 5511
[08:55:16.181583] Epoch: [34]  [420/781]  eta: 0:01:11  lr: 0.000195  training_loss: 0.3679 (0.3768)  mae_loss: 0.2238 (0.2336)  classification_loss: 0.1441 (0.1432)  time: 0.1963  data: 0.0002  max mem: 5511
[08:55:20.096721] Epoch: [34]  [440/781]  eta: 0:01:07  lr: 0.000195  training_loss: 0.3699 (0.3766)  mae_loss: 0.2278 (0.2335)  classification_loss: 0.1413 (0.1431)  time: 0.1957  data: 0.0003  max mem: 5511
[08:55:24.078320] Epoch: [34]  [460/781]  eta: 0:01:03  lr: 0.000195  training_loss: 0.3608 (0.3761)  mae_loss: 0.2262 (0.2330)  classification_loss: 0.1397 (0.1430)  time: 0.1990  data: 0.0002  max mem: 5511
[08:55:27.992231] Epoch: [34]  [480/781]  eta: 0:00:59  lr: 0.000195  training_loss: 0.3727 (0.3761)  mae_loss: 0.2329 (0.2330)  classification_loss: 0.1456 (0.1431)  time: 0.1956  data: 0.0002  max mem: 5511
[08:55:31.921726] Epoch: [34]  [500/781]  eta: 0:00:55  lr: 0.000195  training_loss: 0.3856 (0.3765)  mae_loss: 0.2305 (0.2334)  classification_loss: 0.1437 (0.1431)  time: 0.1964  data: 0.0002  max mem: 5511
[08:55:35.835436] Epoch: [34]  [520/781]  eta: 0:00:51  lr: 0.000195  training_loss: 0.3653 (0.3763)  mae_loss: 0.2256 (0.2329)  classification_loss: 0.1463 (0.1433)  time: 0.1956  data: 0.0002  max mem: 5511
[08:55:39.743511] Epoch: [34]  [540/781]  eta: 0:00:47  lr: 0.000195  training_loss: 0.3744 (0.3764)  mae_loss: 0.2281 (0.2330)  classification_loss: 0.1431 (0.1433)  time: 0.1953  data: 0.0004  max mem: 5511
[08:55:43.668737] Epoch: [34]  [560/781]  eta: 0:00:43  lr: 0.000195  training_loss: 0.3864 (0.3766)  mae_loss: 0.2344 (0.2331)  classification_loss: 0.1454 (0.1435)  time: 0.1962  data: 0.0002  max mem: 5511
[08:55:47.588762] Epoch: [34]  [580/781]  eta: 0:00:39  lr: 0.000194  training_loss: 0.3659 (0.3762)  mae_loss: 0.2243 (0.2328)  classification_loss: 0.1389 (0.1434)  time: 0.1959  data: 0.0002  max mem: 5511
[08:55:51.500162] Epoch: [34]  [600/781]  eta: 0:00:35  lr: 0.000194  training_loss: 0.3822 (0.3766)  mae_loss: 0.2361 (0.2332)  classification_loss: 0.1398 (0.1433)  time: 0.1955  data: 0.0001  max mem: 5511
[08:55:55.417352] Epoch: [34]  [620/781]  eta: 0:00:31  lr: 0.000194  training_loss: 0.3722 (0.3764)  mae_loss: 0.2297 (0.2330)  classification_loss: 0.1431 (0.1433)  time: 0.1958  data: 0.0002  max mem: 5511
[08:55:59.310758] Epoch: [34]  [640/781]  eta: 0:00:27  lr: 0.000194  training_loss: 0.3861 (0.3768)  mae_loss: 0.2434 (0.2334)  classification_loss: 0.1444 (0.1434)  time: 0.1946  data: 0.0002  max mem: 5511
[08:56:03.205822] Epoch: [34]  [660/781]  eta: 0:00:23  lr: 0.000194  training_loss: 0.3772 (0.3769)  mae_loss: 0.2363 (0.2334)  classification_loss: 0.1430 (0.1435)  time: 0.1946  data: 0.0003  max mem: 5511
[08:56:07.099786] Epoch: [34]  [680/781]  eta: 0:00:19  lr: 0.000194  training_loss: 0.3705 (0.3768)  mae_loss: 0.2306 (0.2333)  classification_loss: 0.1441 (0.1435)  time: 0.1946  data: 0.0002  max mem: 5511
[08:56:10.982314] Epoch: [34]  [700/781]  eta: 0:00:15  lr: 0.000194  training_loss: 0.3785 (0.3768)  mae_loss: 0.2289 (0.2332)  classification_loss: 0.1466 (0.1436)  time: 0.1941  data: 0.0002  max mem: 5511
[08:56:14.879009] Epoch: [34]  [720/781]  eta: 0:00:11  lr: 0.000194  training_loss: 0.3695 (0.3766)  mae_loss: 0.2235 (0.2330)  classification_loss: 0.1450 (0.1436)  time: 0.1948  data: 0.0003  max mem: 5511
[08:56:18.763557] Epoch: [34]  [740/781]  eta: 0:00:08  lr: 0.000194  training_loss: 0.3611 (0.3767)  mae_loss: 0.2307 (0.2331)  classification_loss: 0.1418 (0.1435)  time: 0.1941  data: 0.0003  max mem: 5511
[08:56:22.677443] Epoch: [34]  [760/781]  eta: 0:00:04  lr: 0.000194  training_loss: 0.3827 (0.3769)  mae_loss: 0.2353 (0.2333)  classification_loss: 0.1418 (0.1436)  time: 0.1956  data: 0.0002  max mem: 5511
[08:56:26.595436] Epoch: [34]  [780/781]  eta: 0:00:00  lr: 0.000194  training_loss: 0.3679 (0.3767)  mae_loss: 0.2291 (0.2331)  classification_loss: 0.1433 (0.1436)  time: 0.1958  data: 0.0002  max mem: 5511
[08:56:26.774756] Epoch: [34] Total time: 0:02:33 (0.1967 s / it)
[08:56:26.775185] Averaged stats: lr: 0.000194  training_loss: 0.3679 (0.3767)  mae_loss: 0.2291 (0.2331)  classification_loss: 0.1433 (0.1436)
[08:56:27.450243] Test:  [  0/157]  eta: 0:01:45  testing_loss: 0.6774 (0.6774)  acc1: 82.8125 (82.8125)  acc5: 96.8750 (96.8750)  time: 0.6710  data: 0.6415  max mem: 5511
[08:56:27.739487] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.6921 (0.7345)  acc1: 78.1250 (77.4148)  acc5: 98.4375 (98.4375)  time: 0.0871  data: 0.0585  max mem: 5511
[08:56:28.023251] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.6784 (0.7031)  acc1: 79.6875 (79.0923)  acc5: 98.4375 (98.8839)  time: 0.0285  data: 0.0002  max mem: 5511
[08:56:28.314025] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6876 (0.7157)  acc1: 79.6875 (78.6794)  acc5: 98.4375 (98.6391)  time: 0.0286  data: 0.0001  max mem: 5511
[08:56:28.602496] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.7347 (0.7232)  acc1: 76.5625 (78.0488)  acc5: 98.4375 (98.6280)  time: 0.0288  data: 0.0002  max mem: 5511
[08:56:28.885100] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.7347 (0.7211)  acc1: 76.5625 (77.7880)  acc5: 98.4375 (98.5600)  time: 0.0284  data: 0.0002  max mem: 5511
[08:56:29.166492] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.7096 (0.7182)  acc1: 76.5625 (77.6383)  acc5: 98.4375 (98.5400)  time: 0.0281  data: 0.0001  max mem: 5511
[08:56:29.448649] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6678 (0.7089)  acc1: 78.1250 (78.1250)  acc5: 98.4375 (98.5915)  time: 0.0281  data: 0.0002  max mem: 5511
[08:56:29.730770] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6678 (0.7149)  acc1: 79.6875 (77.9321)  acc5: 98.4375 (98.4182)  time: 0.0281  data: 0.0002  max mem: 5511
[08:56:30.012038] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6748 (0.7122)  acc1: 78.1250 (78.1937)  acc5: 98.4375 (98.4203)  time: 0.0281  data: 0.0002  max mem: 5511
[08:56:30.292451] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.6883 (0.7134)  acc1: 76.5625 (78.0167)  acc5: 98.4375 (98.4066)  time: 0.0280  data: 0.0002  max mem: 5511
[08:56:30.575860] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6963 (0.7138)  acc1: 78.1250 (77.9842)  acc5: 98.4375 (98.3671)  time: 0.0281  data: 0.0002  max mem: 5511
[08:56:30.857548] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6818 (0.7092)  acc1: 78.1250 (78.2154)  acc5: 98.4375 (98.3988)  time: 0.0281  data: 0.0002  max mem: 5511
[08:56:31.146995] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6824 (0.7134)  acc1: 78.1250 (78.0057)  acc5: 100.0000 (98.4256)  time: 0.0284  data: 0.0002  max mem: 5511
[08:56:31.432643] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7324 (0.7125)  acc1: 78.1250 (77.9920)  acc5: 100.0000 (98.4707)  time: 0.0286  data: 0.0002  max mem: 5511
[08:56:31.714427] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.7263 (0.7125)  acc1: 76.5625 (77.8974)  acc5: 98.4375 (98.4272)  time: 0.0282  data: 0.0001  max mem: 5511
[08:56:31.865119] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.7171 (0.7134)  acc1: 76.5625 (77.8200)  acc5: 98.4375 (98.4500)  time: 0.0271  data: 0.0001  max mem: 5511
[08:56:32.021172] Test: Total time: 0:00:05 (0.0334 s / it)
[08:56:32.021677] * Acc@1 77.820 Acc@5 98.450 loss 0.713
[08:56:32.022129] Accuracy of the network on the 10000 test images: 77.8%
[08:56:32.022325] Max accuracy: 77.82%
[08:56:32.216588] log_dir: ./output_dir
[08:56:33.140096] Epoch: [35]  [  0/781]  eta: 0:11:59  lr: 0.000194  training_loss: 0.3927 (0.3927)  mae_loss: 0.2551 (0.2551)  classification_loss: 0.1376 (0.1376)  time: 0.9215  data: 0.6900  max mem: 5511
[08:56:37.050090] Epoch: [35]  [ 20/781]  eta: 0:02:55  lr: 0.000194  training_loss: 0.3596 (0.3695)  mae_loss: 0.2214 (0.2280)  classification_loss: 0.1388 (0.1415)  time: 0.1954  data: 0.0002  max mem: 5511
[08:56:40.963219] Epoch: [35]  [ 40/781]  eta: 0:02:37  lr: 0.000193  training_loss: 0.3563 (0.3642)  mae_loss: 0.2199 (0.2229)  classification_loss: 0.1414 (0.1413)  time: 0.1956  data: 0.0003  max mem: 5511
[08:56:44.873409] Epoch: [35]  [ 60/781]  eta: 0:02:29  lr: 0.000193  training_loss: 0.3657 (0.3678)  mae_loss: 0.2271 (0.2248)  classification_loss: 0.1442 (0.1430)  time: 0.1954  data: 0.0003  max mem: 5511
[08:56:48.782481] Epoch: [35]  [ 80/781]  eta: 0:02:23  lr: 0.000193  training_loss: 0.3831 (0.3717)  mae_loss: 0.2361 (0.2285)  classification_loss: 0.1446 (0.1432)  time: 0.1954  data: 0.0003  max mem: 5511
[08:56:52.694266] Epoch: [35]  [100/781]  eta: 0:02:18  lr: 0.000193  training_loss: 0.3615 (0.3706)  mae_loss: 0.2218 (0.2273)  classification_loss: 0.1396 (0.1433)  time: 0.1955  data: 0.0003  max mem: 5511
[08:56:56.598569] Epoch: [35]  [120/781]  eta: 0:02:13  lr: 0.000193  training_loss: 0.3531 (0.3703)  mae_loss: 0.2143 (0.2273)  classification_loss: 0.1412 (0.1430)  time: 0.1951  data: 0.0002  max mem: 5511
[08:57:00.488590] Epoch: [35]  [140/781]  eta: 0:02:08  lr: 0.000193  training_loss: 0.3564 (0.3694)  mae_loss: 0.2257 (0.2271)  classification_loss: 0.1383 (0.1423)  time: 0.1944  data: 0.0001  max mem: 5511
[08:57:04.413349] Epoch: [35]  [160/781]  eta: 0:02:04  lr: 0.000193  training_loss: 0.3541 (0.3677)  mae_loss: 0.2113 (0.2258)  classification_loss: 0.1399 (0.1419)  time: 0.1962  data: 0.0002  max mem: 5511
[08:57:08.382333] Epoch: [35]  [180/781]  eta: 0:02:00  lr: 0.000193  training_loss: 0.3684 (0.3674)  mae_loss: 0.2265 (0.2259)  classification_loss: 0.1379 (0.1415)  time: 0.1984  data: 0.0003  max mem: 5511
[08:57:12.317124] Epoch: [35]  [200/781]  eta: 0:01:55  lr: 0.000193  training_loss: 0.3810 (0.3681)  mae_loss: 0.2325 (0.2263)  classification_loss: 0.1462 (0.1418)  time: 0.1966  data: 0.0002  max mem: 5511
[08:57:16.224973] Epoch: [35]  [220/781]  eta: 0:01:51  lr: 0.000193  training_loss: 0.3668 (0.3678)  mae_loss: 0.2237 (0.2259)  classification_loss: 0.1387 (0.1419)  time: 0.1953  data: 0.0003  max mem: 5511
[08:57:20.121507] Epoch: [35]  [240/781]  eta: 0:01:47  lr: 0.000193  training_loss: 0.3703 (0.3675)  mae_loss: 0.2161 (0.2256)  classification_loss: 0.1378 (0.1419)  time: 0.1948  data: 0.0005  max mem: 5511
[08:57:24.014459] Epoch: [35]  [260/781]  eta: 0:01:43  lr: 0.000192  training_loss: 0.3759 (0.3679)  mae_loss: 0.2282 (0.2260)  classification_loss: 0.1426 (0.1419)  time: 0.1946  data: 0.0002  max mem: 5511
[08:57:27.941754] Epoch: [35]  [280/781]  eta: 0:01:39  lr: 0.000192  training_loss: 0.3828 (0.3692)  mae_loss: 0.2443 (0.2274)  classification_loss: 0.1385 (0.1419)  time: 0.1963  data: 0.0002  max mem: 5511
[08:57:31.853496] Epoch: [35]  [300/781]  eta: 0:01:35  lr: 0.000192  training_loss: 0.3603 (0.3689)  mae_loss: 0.2171 (0.2271)  classification_loss: 0.1431 (0.1418)  time: 0.1955  data: 0.0002  max mem: 5511
[08:57:35.794952] Epoch: [35]  [320/781]  eta: 0:01:31  lr: 0.000192  training_loss: 0.3618 (0.3693)  mae_loss: 0.2179 (0.2274)  classification_loss: 0.1432 (0.1419)  time: 0.1970  data: 0.0002  max mem: 5511
[08:57:39.709622] Epoch: [35]  [340/781]  eta: 0:01:27  lr: 0.000192  training_loss: 0.3696 (0.3696)  mae_loss: 0.2363 (0.2278)  classification_loss: 0.1388 (0.1418)  time: 0.1956  data: 0.0002  max mem: 5511
[08:57:43.622859] Epoch: [35]  [360/781]  eta: 0:01:23  lr: 0.000192  training_loss: 0.3828 (0.3702)  mae_loss: 0.2333 (0.2282)  classification_loss: 0.1447 (0.1420)  time: 0.1956  data: 0.0002  max mem: 5511
[08:57:47.600492] Epoch: [35]  [380/781]  eta: 0:01:19  lr: 0.000192  training_loss: 0.3781 (0.3705)  mae_loss: 0.2299 (0.2283)  classification_loss: 0.1440 (0.1421)  time: 0.1988  data: 0.0001  max mem: 5511
[08:57:51.507820] Epoch: [35]  [400/781]  eta: 0:01:15  lr: 0.000192  training_loss: 0.3539 (0.3703)  mae_loss: 0.2132 (0.2282)  classification_loss: 0.1413 (0.1421)  time: 0.1953  data: 0.0002  max mem: 5511
[08:57:55.412065] Epoch: [35]  [420/781]  eta: 0:01:11  lr: 0.000192  training_loss: 0.3664 (0.3704)  mae_loss: 0.2258 (0.2282)  classification_loss: 0.1408 (0.1421)  time: 0.1951  data: 0.0003  max mem: 5511
[08:57:59.412520] Epoch: [35]  [440/781]  eta: 0:01:07  lr: 0.000192  training_loss: 0.3652 (0.3702)  mae_loss: 0.2275 (0.2283)  classification_loss: 0.1375 (0.1420)  time: 0.2000  data: 0.0002  max mem: 5511
[08:58:03.319944] Epoch: [35]  [460/781]  eta: 0:01:03  lr: 0.000192  training_loss: 0.3491 (0.3698)  mae_loss: 0.2050 (0.2279)  classification_loss: 0.1400 (0.1419)  time: 0.1953  data: 0.0002  max mem: 5511
[08:58:07.229776] Epoch: [35]  [480/781]  eta: 0:00:59  lr: 0.000191  training_loss: 0.3649 (0.3698)  mae_loss: 0.2268 (0.2279)  classification_loss: 0.1417 (0.1419)  time: 0.1954  data: 0.0003  max mem: 5511
[08:58:11.138583] Epoch: [35]  [500/781]  eta: 0:00:55  lr: 0.000191  training_loss: 0.3786 (0.3704)  mae_loss: 0.2295 (0.2283)  classification_loss: 0.1471 (0.1421)  time: 0.1954  data: 0.0002  max mem: 5511
[08:58:15.040204] Epoch: [35]  [520/781]  eta: 0:00:51  lr: 0.000191  training_loss: 0.3918 (0.3709)  mae_loss: 0.2441 (0.2288)  classification_loss: 0.1419 (0.1421)  time: 0.1950  data: 0.0002  max mem: 5511
[08:58:18.938467] Epoch: [35]  [540/781]  eta: 0:00:47  lr: 0.000191  training_loss: 0.3803 (0.3714)  mae_loss: 0.2353 (0.2292)  classification_loss: 0.1425 (0.1422)  time: 0.1948  data: 0.0002  max mem: 5511
[08:58:22.860363] Epoch: [35]  [560/781]  eta: 0:00:43  lr: 0.000191  training_loss: 0.3641 (0.3713)  mae_loss: 0.2206 (0.2291)  classification_loss: 0.1442 (0.1422)  time: 0.1960  data: 0.0002  max mem: 5511
[08:58:26.774801] Epoch: [35]  [580/781]  eta: 0:00:39  lr: 0.000191  training_loss: 0.3704 (0.3716)  mae_loss: 0.2298 (0.2293)  classification_loss: 0.1414 (0.1423)  time: 0.1956  data: 0.0002  max mem: 5511
[08:58:30.719960] Epoch: [35]  [600/781]  eta: 0:00:35  lr: 0.000191  training_loss: 0.3802 (0.3719)  mae_loss: 0.2303 (0.2296)  classification_loss: 0.1421 (0.1423)  time: 0.1972  data: 0.0002  max mem: 5511
[08:58:34.638480] Epoch: [35]  [620/781]  eta: 0:00:31  lr: 0.000191  training_loss: 0.3711 (0.3719)  mae_loss: 0.2331 (0.2297)  classification_loss: 0.1403 (0.1422)  time: 0.1959  data: 0.0003  max mem: 5511
[08:58:38.546834] Epoch: [35]  [640/781]  eta: 0:00:27  lr: 0.000191  training_loss: 0.3745 (0.3719)  mae_loss: 0.2255 (0.2297)  classification_loss: 0.1408 (0.1422)  time: 0.1953  data: 0.0002  max mem: 5511
[08:58:42.473455] Epoch: [35]  [660/781]  eta: 0:00:23  lr: 0.000191  training_loss: 0.3679 (0.3719)  mae_loss: 0.2232 (0.2297)  classification_loss: 0.1405 (0.1422)  time: 0.1963  data: 0.0003  max mem: 5511
[08:58:46.378113] Epoch: [35]  [680/781]  eta: 0:00:19  lr: 0.000191  training_loss: 0.3922 (0.3722)  mae_loss: 0.2358 (0.2299)  classification_loss: 0.1418 (0.1423)  time: 0.1952  data: 0.0003  max mem: 5511
[08:58:50.289228] Epoch: [35]  [700/781]  eta: 0:00:15  lr: 0.000190  training_loss: 0.3714 (0.3722)  mae_loss: 0.2264 (0.2298)  classification_loss: 0.1419 (0.1424)  time: 0.1955  data: 0.0004  max mem: 5511
[08:58:54.218191] Epoch: [35]  [720/781]  eta: 0:00:12  lr: 0.000190  training_loss: 0.3675 (0.3723)  mae_loss: 0.2248 (0.2299)  classification_loss: 0.1425 (0.1424)  time: 0.1964  data: 0.0005  max mem: 5511
[08:58:58.122197] Epoch: [35]  [740/781]  eta: 0:00:08  lr: 0.000190  training_loss: 0.3655 (0.3721)  mae_loss: 0.2233 (0.2298)  classification_loss: 0.1394 (0.1423)  time: 0.1951  data: 0.0002  max mem: 5511
[08:59:02.013016] Epoch: [35]  [760/781]  eta: 0:00:04  lr: 0.000190  training_loss: 0.3663 (0.3720)  mae_loss: 0.2231 (0.2296)  classification_loss: 0.1411 (0.1423)  time: 0.1944  data: 0.0003  max mem: 5511
[08:59:05.901953] Epoch: [35]  [780/781]  eta: 0:00:00  lr: 0.000190  training_loss: 0.3651 (0.3719)  mae_loss: 0.2240 (0.2295)  classification_loss: 0.1402 (0.1423)  time: 0.1943  data: 0.0002  max mem: 5511
[08:59:06.056601] Epoch: [35] Total time: 0:02:33 (0.1970 s / it)
[08:59:06.057368] Averaged stats: lr: 0.000190  training_loss: 0.3651 (0.3719)  mae_loss: 0.2240 (0.2295)  classification_loss: 0.1402 (0.1423)
[08:59:06.741289] Test:  [  0/157]  eta: 0:01:46  testing_loss: 0.6471 (0.6471)  acc1: 84.3750 (84.3750)  acc5: 96.8750 (96.8750)  time: 0.6793  data: 0.6419  max mem: 5511
[08:59:07.028377] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.6471 (0.6918)  acc1: 76.5625 (77.9830)  acc5: 100.0000 (99.1477)  time: 0.0876  data: 0.0585  max mem: 5511
[08:59:07.312500] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.6311 (0.6712)  acc1: 78.1250 (79.3155)  acc5: 98.4375 (98.9583)  time: 0.0284  data: 0.0002  max mem: 5511
[08:59:07.595751] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6543 (0.6837)  acc1: 78.1250 (78.4778)  acc5: 98.4375 (98.8407)  time: 0.0282  data: 0.0002  max mem: 5511
[08:59:07.890492] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6772 (0.6884)  acc1: 79.6875 (78.5061)  acc5: 98.4375 (98.8567)  time: 0.0288  data: 0.0002  max mem: 5511
[08:59:08.176853] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6772 (0.6843)  acc1: 79.6875 (78.7684)  acc5: 98.4375 (98.8358)  time: 0.0289  data: 0.0002  max mem: 5511
[08:59:08.461756] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6637 (0.6818)  acc1: 79.6875 (78.5861)  acc5: 98.4375 (98.7961)  time: 0.0284  data: 0.0002  max mem: 5511
[08:59:08.742884] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6325 (0.6736)  acc1: 79.6875 (78.8072)  acc5: 98.4375 (98.7676)  time: 0.0282  data: 0.0002  max mem: 5511
[08:59:09.023982] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6407 (0.6753)  acc1: 79.6875 (78.7423)  acc5: 98.4375 (98.6111)  time: 0.0280  data: 0.0001  max mem: 5511
[08:59:09.304169] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6637 (0.6737)  acc1: 79.6875 (78.9320)  acc5: 98.4375 (98.5920)  time: 0.0280  data: 0.0001  max mem: 5511
[08:59:09.585890] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.6546 (0.6748)  acc1: 78.1250 (78.6819)  acc5: 98.4375 (98.6696)  time: 0.0280  data: 0.0001  max mem: 5511
[08:59:09.869642] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6527 (0.6749)  acc1: 78.1250 (78.7725)  acc5: 98.4375 (98.6627)  time: 0.0281  data: 0.0002  max mem: 5511
[08:59:10.153541] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6362 (0.6711)  acc1: 81.2500 (78.8869)  acc5: 98.4375 (98.6958)  time: 0.0283  data: 0.0003  max mem: 5511
[08:59:10.435494] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6522 (0.6736)  acc1: 79.6875 (78.7691)  acc5: 100.0000 (98.6999)  time: 0.0282  data: 0.0002  max mem: 5511
[08:59:10.716567] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6522 (0.6719)  acc1: 78.1250 (78.7677)  acc5: 98.4375 (98.7256)  time: 0.0280  data: 0.0001  max mem: 5511
[08:59:10.995636] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6569 (0.6701)  acc1: 78.1250 (78.7666)  acc5: 98.4375 (98.6962)  time: 0.0279  data: 0.0001  max mem: 5511
[08:59:11.146078] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6660 (0.6707)  acc1: 79.6875 (78.7400)  acc5: 98.4375 (98.7100)  time: 0.0269  data: 0.0001  max mem: 5511
[08:59:11.316873] Test: Total time: 0:00:05 (0.0335 s / it)
[08:59:11.317323] * Acc@1 78.740 Acc@5 98.710 loss 0.671
[08:59:11.317639] Accuracy of the network on the 10000 test images: 78.7%
[08:59:11.317815] Max accuracy: 78.74%
[08:59:11.430041] log_dir: ./output_dir
[08:59:12.366296] Epoch: [36]  [  0/781]  eta: 0:12:09  lr: 0.000190  training_loss: 0.3578 (0.3578)  mae_loss: 0.2058 (0.2058)  classification_loss: 0.1519 (0.1519)  time: 0.9342  data: 0.7298  max mem: 5511
[08:59:16.279683] Epoch: [36]  [ 20/781]  eta: 0:02:55  lr: 0.000190  training_loss: 0.3789 (0.3752)  mae_loss: 0.2273 (0.2330)  classification_loss: 0.1427 (0.1422)  time: 0.1956  data: 0.0003  max mem: 5511
[08:59:20.179700] Epoch: [36]  [ 40/781]  eta: 0:02:38  lr: 0.000190  training_loss: 0.3824 (0.3775)  mae_loss: 0.2342 (0.2339)  classification_loss: 0.1470 (0.1436)  time: 0.1949  data: 0.0002  max mem: 5511
[08:59:24.085546] Epoch: [36]  [ 60/781]  eta: 0:02:29  lr: 0.000190  training_loss: 0.3843 (0.3790)  mae_loss: 0.2388 (0.2362)  classification_loss: 0.1403 (0.1428)  time: 0.1952  data: 0.0002  max mem: 5511
[08:59:27.999626] Epoch: [36]  [ 80/781]  eta: 0:02:23  lr: 0.000190  training_loss: 0.3712 (0.3778)  mae_loss: 0.2311 (0.2350)  classification_loss: 0.1424 (0.1428)  time: 0.1956  data: 0.0002  max mem: 5511
[08:59:31.902406] Epoch: [36]  [100/781]  eta: 0:02:17  lr: 0.000190  training_loss: 0.3694 (0.3769)  mae_loss: 0.2358 (0.2346)  classification_loss: 0.1422 (0.1423)  time: 0.1950  data: 0.0002  max mem: 5511
[08:59:35.792549] Epoch: [36]  [120/781]  eta: 0:02:13  lr: 0.000190  training_loss: 0.3682 (0.3760)  mae_loss: 0.2212 (0.2339)  classification_loss: 0.1393 (0.1421)  time: 0.1944  data: 0.0002  max mem: 5511
[08:59:39.708795] Epoch: [36]  [140/781]  eta: 0:02:08  lr: 0.000189  training_loss: 0.3694 (0.3746)  mae_loss: 0.2299 (0.2329)  classification_loss: 0.1403 (0.1417)  time: 0.1957  data: 0.0002  max mem: 5511
[08:59:43.599836] Epoch: [36]  [160/781]  eta: 0:02:04  lr: 0.000189  training_loss: 0.3645 (0.3731)  mae_loss: 0.2230 (0.2318)  classification_loss: 0.1364 (0.1413)  time: 0.1945  data: 0.0002  max mem: 5511
[08:59:47.494445] Epoch: [36]  [180/781]  eta: 0:01:59  lr: 0.000189  training_loss: 0.3612 (0.3730)  mae_loss: 0.2279 (0.2318)  classification_loss: 0.1384 (0.1412)  time: 0.1946  data: 0.0002  max mem: 5511
[08:59:51.409816] Epoch: [36]  [200/781]  eta: 0:01:55  lr: 0.000189  training_loss: 0.3684 (0.3727)  mae_loss: 0.2344 (0.2315)  classification_loss: 0.1423 (0.1412)  time: 0.1957  data: 0.0002  max mem: 5511
[08:59:55.330664] Epoch: [36]  [220/781]  eta: 0:01:51  lr: 0.000189  training_loss: 0.3808 (0.3736)  mae_loss: 0.2400 (0.2322)  classification_loss: 0.1440 (0.1413)  time: 0.1960  data: 0.0002  max mem: 5511
[08:59:59.258756] Epoch: [36]  [240/781]  eta: 0:01:47  lr: 0.000189  training_loss: 0.3678 (0.3730)  mae_loss: 0.2342 (0.2322)  classification_loss: 0.1336 (0.1408)  time: 0.1963  data: 0.0002  max mem: 5511
[09:00:03.166103] Epoch: [36]  [260/781]  eta: 0:01:43  lr: 0.000189  training_loss: 0.3664 (0.3729)  mae_loss: 0.2238 (0.2318)  classification_loss: 0.1439 (0.1411)  time: 0.1953  data: 0.0002  max mem: 5511
[09:00:07.098739] Epoch: [36]  [280/781]  eta: 0:01:39  lr: 0.000189  training_loss: 0.3553 (0.3717)  mae_loss: 0.2213 (0.2307)  classification_loss: 0.1360 (0.1409)  time: 0.1966  data: 0.0003  max mem: 5511
[09:00:11.014038] Epoch: [36]  [300/781]  eta: 0:01:35  lr: 0.000189  training_loss: 0.3648 (0.3716)  mae_loss: 0.2261 (0.2303)  classification_loss: 0.1472 (0.1413)  time: 0.1957  data: 0.0002  max mem: 5511
[09:00:14.957935] Epoch: [36]  [320/781]  eta: 0:01:31  lr: 0.000189  training_loss: 0.3561 (0.3713)  mae_loss: 0.2196 (0.2301)  classification_loss: 0.1392 (0.1412)  time: 0.1971  data: 0.0002  max mem: 5511
[09:00:18.851477] Epoch: [36]  [340/781]  eta: 0:01:27  lr: 0.000189  training_loss: 0.3801 (0.3717)  mae_loss: 0.2381 (0.2304)  classification_loss: 0.1439 (0.1412)  time: 0.1946  data: 0.0002  max mem: 5511
[09:00:22.752341] Epoch: [36]  [360/781]  eta: 0:01:23  lr: 0.000188  training_loss: 0.3503 (0.3715)  mae_loss: 0.2250 (0.2301)  classification_loss: 0.1439 (0.1414)  time: 0.1950  data: 0.0003  max mem: 5511
[09:00:26.750224] Epoch: [36]  [380/781]  eta: 0:01:19  lr: 0.000188  training_loss: 0.3748 (0.3720)  mae_loss: 0.2310 (0.2306)  classification_loss: 0.1388 (0.1414)  time: 0.1998  data: 0.0004  max mem: 5511
[09:00:30.705433] Epoch: [36]  [400/781]  eta: 0:01:15  lr: 0.000188  training_loss: 0.3662 (0.3718)  mae_loss: 0.2265 (0.2305)  classification_loss: 0.1369 (0.1413)  time: 0.1976  data: 0.0002  max mem: 5511
[09:00:34.650164] Epoch: [36]  [420/781]  eta: 0:01:11  lr: 0.000188  training_loss: 0.3784 (0.3722)  mae_loss: 0.2377 (0.2309)  classification_loss: 0.1381 (0.1413)  time: 0.1972  data: 0.0003  max mem: 5511
[09:00:38.547695] Epoch: [36]  [440/781]  eta: 0:01:07  lr: 0.000188  training_loss: 0.3575 (0.3718)  mae_loss: 0.2247 (0.2305)  classification_loss: 0.1413 (0.1413)  time: 0.1947  data: 0.0002  max mem: 5511
[09:00:42.467266] Epoch: [36]  [460/781]  eta: 0:01:03  lr: 0.000188  training_loss: 0.3442 (0.3711)  mae_loss: 0.2097 (0.2300)  classification_loss: 0.1350 (0.1411)  time: 0.1959  data: 0.0002  max mem: 5511
[09:00:46.393452] Epoch: [36]  [480/781]  eta: 0:00:59  lr: 0.000188  training_loss: 0.3737 (0.3715)  mae_loss: 0.2301 (0.2303)  classification_loss: 0.1472 (0.1412)  time: 0.1962  data: 0.0002  max mem: 5511
[09:00:50.316517] Epoch: [36]  [500/781]  eta: 0:00:55  lr: 0.000188  training_loss: 0.3629 (0.3714)  mae_loss: 0.2157 (0.2300)  classification_loss: 0.1417 (0.1414)  time: 0.1961  data: 0.0002  max mem: 5511
[09:00:54.239926] Epoch: [36]  [520/781]  eta: 0:00:51  lr: 0.000188  training_loss: 0.3757 (0.3715)  mae_loss: 0.2331 (0.2301)  classification_loss: 0.1403 (0.1414)  time: 0.1961  data: 0.0002  max mem: 5511
[09:00:58.148858] Epoch: [36]  [540/781]  eta: 0:00:47  lr: 0.000188  training_loss: 0.3638 (0.3714)  mae_loss: 0.2257 (0.2300)  classification_loss: 0.1413 (0.1414)  time: 0.1953  data: 0.0002  max mem: 5511
[09:01:02.057654] Epoch: [36]  [560/781]  eta: 0:00:43  lr: 0.000188  training_loss: 0.3719 (0.3716)  mae_loss: 0.2291 (0.2302)  classification_loss: 0.1408 (0.1414)  time: 0.1954  data: 0.0002  max mem: 5511
[09:01:05.985674] Epoch: [36]  [580/781]  eta: 0:00:39  lr: 0.000187  training_loss: 0.3763 (0.3717)  mae_loss: 0.2359 (0.2303)  classification_loss: 0.1404 (0.1413)  time: 0.1963  data: 0.0002  max mem: 5511
[09:01:09.942744] Epoch: [36]  [600/781]  eta: 0:00:35  lr: 0.000187  training_loss: 0.3723 (0.3718)  mae_loss: 0.2339 (0.2305)  classification_loss: 0.1394 (0.1413)  time: 0.1978  data: 0.0003  max mem: 5511
[09:01:13.870791] Epoch: [36]  [620/781]  eta: 0:00:31  lr: 0.000187  training_loss: 0.3617 (0.3717)  mae_loss: 0.2243 (0.2303)  classification_loss: 0.1393 (0.1414)  time: 0.1963  data: 0.0002  max mem: 5511
[09:01:17.801124] Epoch: [36]  [640/781]  eta: 0:00:27  lr: 0.000187  training_loss: 0.3566 (0.3714)  mae_loss: 0.2181 (0.2300)  classification_loss: 0.1385 (0.1413)  time: 0.1964  data: 0.0002  max mem: 5511
[09:01:21.744614] Epoch: [36]  [660/781]  eta: 0:00:23  lr: 0.000187  training_loss: 0.3777 (0.3717)  mae_loss: 0.2405 (0.2304)  classification_loss: 0.1429 (0.1413)  time: 0.1971  data: 0.0002  max mem: 5511
[09:01:25.638143] Epoch: [36]  [680/781]  eta: 0:00:19  lr: 0.000187  training_loss: 0.3825 (0.3719)  mae_loss: 0.2350 (0.2305)  classification_loss: 0.1406 (0.1413)  time: 0.1946  data: 0.0002  max mem: 5511
[09:01:29.530302] Epoch: [36]  [700/781]  eta: 0:00:15  lr: 0.000187  training_loss: 0.3555 (0.3720)  mae_loss: 0.2206 (0.2306)  classification_loss: 0.1434 (0.1414)  time: 0.1945  data: 0.0002  max mem: 5511
[09:01:33.438748] Epoch: [36]  [720/781]  eta: 0:00:12  lr: 0.000187  training_loss: 0.3703 (0.3720)  mae_loss: 0.2258 (0.2306)  classification_loss: 0.1442 (0.1414)  time: 0.1954  data: 0.0003  max mem: 5511
[09:01:37.357402] Epoch: [36]  [740/781]  eta: 0:00:08  lr: 0.000187  training_loss: 0.3631 (0.3720)  mae_loss: 0.2274 (0.2307)  classification_loss: 0.1371 (0.1413)  time: 0.1958  data: 0.0002  max mem: 5511
[09:01:41.307712] Epoch: [36]  [760/781]  eta: 0:00:04  lr: 0.000187  training_loss: 0.3702 (0.3718)  mae_loss: 0.2211 (0.2305)  classification_loss: 0.1401 (0.1414)  time: 0.1974  data: 0.0002  max mem: 5511
[09:01:45.215403] Epoch: [36]  [780/781]  eta: 0:00:00  lr: 0.000187  training_loss: 0.3676 (0.3718)  mae_loss: 0.2209 (0.2304)  classification_loss: 0.1367 (0.1414)  time: 0.1953  data: 0.0002  max mem: 5511
[09:01:45.375119] Epoch: [36] Total time: 0:02:33 (0.1971 s / it)
[09:01:45.375706] Averaged stats: lr: 0.000187  training_loss: 0.3676 (0.3718)  mae_loss: 0.2209 (0.2304)  classification_loss: 0.1367 (0.1414)
[09:01:45.974314] Test:  [  0/157]  eta: 0:01:33  testing_loss: 0.6321 (0.6321)  acc1: 81.2500 (81.2500)  acc5: 95.3125 (95.3125)  time: 0.5930  data: 0.5637  max mem: 5511
[09:01:46.270193] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.7073 (0.7252)  acc1: 78.1250 (76.2784)  acc5: 100.0000 (99.0057)  time: 0.0805  data: 0.0515  max mem: 5511
[09:01:46.553791] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.7022 (0.6954)  acc1: 76.5625 (77.2321)  acc5: 100.0000 (99.1071)  time: 0.0288  data: 0.0002  max mem: 5511
[09:01:46.838859] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.6952 (0.7043)  acc1: 76.5625 (77.2681)  acc5: 100.0000 (98.8407)  time: 0.0283  data: 0.0002  max mem: 5511
[09:01:47.123482] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.6956 (0.7091)  acc1: 76.5625 (77.0198)  acc5: 98.4375 (98.7424)  time: 0.0284  data: 0.0002  max mem: 5511
[09:01:47.408450] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6839 (0.7052)  acc1: 78.1250 (77.4510)  acc5: 98.4375 (98.7132)  time: 0.0284  data: 0.0002  max mem: 5511
[09:01:47.706658] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6770 (0.6993)  acc1: 78.1250 (77.4334)  acc5: 98.4375 (98.6424)  time: 0.0290  data: 0.0002  max mem: 5511
[09:01:47.988949] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6389 (0.6894)  acc1: 79.6875 (77.9269)  acc5: 98.4375 (98.6796)  time: 0.0289  data: 0.0002  max mem: 5511
[09:01:48.270680] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6420 (0.6919)  acc1: 79.6875 (77.8935)  acc5: 98.4375 (98.5918)  time: 0.0281  data: 0.0002  max mem: 5511
[09:01:48.552899] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6528 (0.6866)  acc1: 78.1250 (78.1937)  acc5: 98.4375 (98.6092)  time: 0.0281  data: 0.0002  max mem: 5511
[09:01:48.835330] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.6738 (0.6878)  acc1: 78.1250 (77.9394)  acc5: 100.0000 (98.6696)  time: 0.0281  data: 0.0002  max mem: 5511
[09:01:49.118064] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6725 (0.6876)  acc1: 76.5625 (77.9420)  acc5: 100.0000 (98.6768)  time: 0.0281  data: 0.0002  max mem: 5511
[09:01:49.400152] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6350 (0.6822)  acc1: 79.6875 (78.1250)  acc5: 98.4375 (98.7087)  time: 0.0281  data: 0.0002  max mem: 5511
[09:01:49.684349] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6333 (0.6845)  acc1: 78.1250 (78.0534)  acc5: 98.4375 (98.6760)  time: 0.0282  data: 0.0002  max mem: 5511
[09:01:49.965478] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.7045 (0.6850)  acc1: 76.5625 (77.9588)  acc5: 98.4375 (98.7145)  time: 0.0281  data: 0.0002  max mem: 5511
[09:01:50.244161] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6740 (0.6844)  acc1: 78.1250 (77.9077)  acc5: 98.4375 (98.6651)  time: 0.0279  data: 0.0001  max mem: 5511
[09:01:50.394171] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6729 (0.6849)  acc1: 78.1250 (77.8600)  acc5: 98.4375 (98.6800)  time: 0.0269  data: 0.0001  max mem: 5511
[09:01:50.538192] Test: Total time: 0:00:05 (0.0329 s / it)
[09:01:50.538705] * Acc@1 77.860 Acc@5 98.680 loss 0.685
[09:01:50.539052] Accuracy of the network on the 10000 test images: 77.9%
[09:01:50.539248] Max accuracy: 78.74%
[09:01:50.681603] log_dir: ./output_dir
[09:01:51.545544] Epoch: [37]  [  0/781]  eta: 0:11:13  lr: 0.000187  training_loss: 0.3650 (0.3650)  mae_loss: 0.2323 (0.2323)  classification_loss: 0.1327 (0.1327)  time: 0.8622  data: 0.6337  max mem: 5511
[09:01:55.475265] Epoch: [37]  [ 20/781]  eta: 0:02:53  lr: 0.000186  training_loss: 0.3593 (0.3570)  mae_loss: 0.2298 (0.2211)  classification_loss: 0.1345 (0.1359)  time: 0.1964  data: 0.0003  max mem: 5511
[09:01:59.392395] Epoch: [37]  [ 40/781]  eta: 0:02:37  lr: 0.000186  training_loss: 0.3699 (0.3663)  mae_loss: 0.2306 (0.2279)  classification_loss: 0.1396 (0.1384)  time: 0.1957  data: 0.0003  max mem: 5511
[09:02:03.284510] Epoch: [37]  [ 60/781]  eta: 0:02:28  lr: 0.000186  training_loss: 0.3620 (0.3677)  mae_loss: 0.2190 (0.2281)  classification_loss: 0.1440 (0.1396)  time: 0.1945  data: 0.0003  max mem: 5511
[09:02:07.192754] Epoch: [37]  [ 80/781]  eta: 0:02:22  lr: 0.000186  training_loss: 0.3697 (0.3681)  mae_loss: 0.2303 (0.2288)  classification_loss: 0.1376 (0.1393)  time: 0.1953  data: 0.0003  max mem: 5511
[09:02:11.156567] Epoch: [37]  [100/781]  eta: 0:02:17  lr: 0.000186  training_loss: 0.3764 (0.3697)  mae_loss: 0.2260 (0.2295)  classification_loss: 0.1426 (0.1402)  time: 0.1981  data: 0.0002  max mem: 5511
[09:02:15.060762] Epoch: [37]  [120/781]  eta: 0:02:13  lr: 0.000186  training_loss: 0.3565 (0.3675)  mae_loss: 0.2170 (0.2277)  classification_loss: 0.1357 (0.1397)  time: 0.1951  data: 0.0002  max mem: 5511
[09:02:18.980739] Epoch: [37]  [140/781]  eta: 0:02:08  lr: 0.000186  training_loss: 0.3732 (0.3685)  mae_loss: 0.2327 (0.2286)  classification_loss: 0.1387 (0.1398)  time: 0.1959  data: 0.0002  max mem: 5511
[09:02:22.880449] Epoch: [37]  [160/781]  eta: 0:02:04  lr: 0.000186  training_loss: 0.3764 (0.3690)  mae_loss: 0.2368 (0.2292)  classification_loss: 0.1398 (0.1398)  time: 0.1949  data: 0.0002  max mem: 5511
[09:02:26.775689] Epoch: [37]  [180/781]  eta: 0:01:59  lr: 0.000186  training_loss: 0.3577 (0.3686)  mae_loss: 0.2330 (0.2290)  classification_loss: 0.1349 (0.1396)  time: 0.1947  data: 0.0002  max mem: 5511
[09:02:30.703002] Epoch: [37]  [200/781]  eta: 0:01:55  lr: 0.000186  training_loss: 0.3635 (0.3691)  mae_loss: 0.2182 (0.2293)  classification_loss: 0.1380 (0.1398)  time: 0.1963  data: 0.0002  max mem: 5511
[09:02:34.623604] Epoch: [37]  [220/781]  eta: 0:01:51  lr: 0.000186  training_loss: 0.3791 (0.3705)  mae_loss: 0.2379 (0.2305)  classification_loss: 0.1455 (0.1400)  time: 0.1959  data: 0.0002  max mem: 5511
[09:02:38.529730] Epoch: [37]  [240/781]  eta: 0:01:47  lr: 0.000185  training_loss: 0.3721 (0.3709)  mae_loss: 0.2318 (0.2308)  classification_loss: 0.1428 (0.1401)  time: 0.1952  data: 0.0002  max mem: 5511
[09:02:42.421794] Epoch: [37]  [260/781]  eta: 0:01:43  lr: 0.000185  training_loss: 0.3558 (0.3698)  mae_loss: 0.2120 (0.2296)  classification_loss: 0.1415 (0.1402)  time: 0.1945  data: 0.0002  max mem: 5511
[09:02:46.303983] Epoch: [37]  [280/781]  eta: 0:01:39  lr: 0.000185  training_loss: 0.3654 (0.3694)  mae_loss: 0.2169 (0.2290)  classification_loss: 0.1427 (0.1404)  time: 0.1940  data: 0.0002  max mem: 5511
[09:02:50.289075] Epoch: [37]  [300/781]  eta: 0:01:35  lr: 0.000185  training_loss: 0.3661 (0.3698)  mae_loss: 0.2269 (0.2293)  classification_loss: 0.1404 (0.1406)  time: 0.1992  data: 0.0002  max mem: 5511
[09:02:54.190063] Epoch: [37]  [320/781]  eta: 0:01:31  lr: 0.000185  training_loss: 0.3713 (0.3700)  mae_loss: 0.2336 (0.2294)  classification_loss: 0.1373 (0.1406)  time: 0.1950  data: 0.0002  max mem: 5511
[09:02:58.094553] Epoch: [37]  [340/781]  eta: 0:01:27  lr: 0.000185  training_loss: 0.3615 (0.3696)  mae_loss: 0.2317 (0.2292)  classification_loss: 0.1343 (0.1404)  time: 0.1952  data: 0.0002  max mem: 5511
[09:03:01.992091] Epoch: [37]  [360/781]  eta: 0:01:23  lr: 0.000185  training_loss: 0.3769 (0.3705)  mae_loss: 0.2373 (0.2300)  classification_loss: 0.1431 (0.1405)  time: 0.1948  data: 0.0002  max mem: 5511
[09:03:05.898821] Epoch: [37]  [380/781]  eta: 0:01:19  lr: 0.000185  training_loss: 0.3860 (0.3712)  mae_loss: 0.2435 (0.2306)  classification_loss: 0.1402 (0.1406)  time: 0.1953  data: 0.0003  max mem: 5511
[09:03:09.785080] Epoch: [37]  [400/781]  eta: 0:01:15  lr: 0.000185  training_loss: 0.3651 (0.3714)  mae_loss: 0.2313 (0.2307)  classification_loss: 0.1427 (0.1407)  time: 0.1942  data: 0.0002  max mem: 5511
[09:03:13.686453] Epoch: [37]  [420/781]  eta: 0:01:11  lr: 0.000185  training_loss: 0.3673 (0.3713)  mae_loss: 0.2201 (0.2306)  classification_loss: 0.1359 (0.1407)  time: 0.1950  data: 0.0003  max mem: 5511
[09:03:17.603834] Epoch: [37]  [440/781]  eta: 0:01:07  lr: 0.000185  training_loss: 0.3671 (0.3708)  mae_loss: 0.2244 (0.2302)  classification_loss: 0.1375 (0.1406)  time: 0.1958  data: 0.0003  max mem: 5511
[09:03:21.497920] Epoch: [37]  [460/781]  eta: 0:01:03  lr: 0.000184  training_loss: 0.3557 (0.3705)  mae_loss: 0.2181 (0.2300)  classification_loss: 0.1381 (0.1405)  time: 0.1946  data: 0.0004  max mem: 5511
[09:03:25.505980] Epoch: [37]  [480/781]  eta: 0:00:59  lr: 0.000184  training_loss: 0.3570 (0.3704)  mae_loss: 0.2341 (0.2300)  classification_loss: 0.1412 (0.1404)  time: 0.2003  data: 0.0003  max mem: 5511
[09:03:29.466441] Epoch: [37]  [500/781]  eta: 0:00:55  lr: 0.000184  training_loss: 0.3527 (0.3703)  mae_loss: 0.2199 (0.2298)  classification_loss: 0.1429 (0.1405)  time: 0.1979  data: 0.0002  max mem: 5511
[09:03:33.366965] Epoch: [37]  [520/781]  eta: 0:00:51  lr: 0.000184  training_loss: 0.3742 (0.3706)  mae_loss: 0.2379 (0.2301)  classification_loss: 0.1372 (0.1405)  time: 0.1949  data: 0.0002  max mem: 5511
[09:03:37.296729] Epoch: [37]  [540/781]  eta: 0:00:47  lr: 0.000184  training_loss: 0.3591 (0.3705)  mae_loss: 0.2208 (0.2300)  classification_loss: 0.1392 (0.1405)  time: 0.1964  data: 0.0002  max mem: 5511
[09:03:41.192438] Epoch: [37]  [560/781]  eta: 0:00:43  lr: 0.000184  training_loss: 0.3680 (0.3705)  mae_loss: 0.2349 (0.2302)  classification_loss: 0.1338 (0.1403)  time: 0.1947  data: 0.0002  max mem: 5511
[09:03:45.118987] Epoch: [37]  [580/781]  eta: 0:00:39  lr: 0.000184  training_loss: 0.3644 (0.3702)  mae_loss: 0.2213 (0.2298)  classification_loss: 0.1393 (0.1404)  time: 0.1962  data: 0.0003  max mem: 5511
[09:03:49.027641] Epoch: [37]  [600/781]  eta: 0:00:35  lr: 0.000184  training_loss: 0.3580 (0.3699)  mae_loss: 0.2216 (0.2296)  classification_loss: 0.1385 (0.1403)  time: 0.1953  data: 0.0003  max mem: 5511
[09:03:52.970874] Epoch: [37]  [620/781]  eta: 0:00:31  lr: 0.000184  training_loss: 0.3656 (0.3700)  mae_loss: 0.2231 (0.2297)  classification_loss: 0.1361 (0.1402)  time: 0.1971  data: 0.0003  max mem: 5511
[09:03:56.878334] Epoch: [37]  [640/781]  eta: 0:00:27  lr: 0.000184  training_loss: 0.3768 (0.3703)  mae_loss: 0.2358 (0.2300)  classification_loss: 0.1423 (0.1403)  time: 0.1953  data: 0.0004  max mem: 5511
[09:04:00.794331] Epoch: [37]  [660/781]  eta: 0:00:23  lr: 0.000184  training_loss: 0.3779 (0.3705)  mae_loss: 0.2342 (0.2303)  classification_loss: 0.1415 (0.1403)  time: 0.1957  data: 0.0003  max mem: 5511
[09:04:04.737567] Epoch: [37]  [680/781]  eta: 0:00:19  lr: 0.000183  training_loss: 0.3596 (0.3705)  mae_loss: 0.2199 (0.2301)  classification_loss: 0.1479 (0.1404)  time: 0.1971  data: 0.0002  max mem: 5511
[09:04:08.647795] Epoch: [37]  [700/781]  eta: 0:00:15  lr: 0.000183  training_loss: 0.3724 (0.3706)  mae_loss: 0.2265 (0.2300)  classification_loss: 0.1430 (0.1405)  time: 0.1954  data: 0.0002  max mem: 5511
[09:04:12.570867] Epoch: [37]  [720/781]  eta: 0:00:11  lr: 0.000183  training_loss: 0.3671 (0.3704)  mae_loss: 0.2267 (0.2299)  classification_loss: 0.1398 (0.1406)  time: 0.1961  data: 0.0002  max mem: 5511
[09:04:16.518307] Epoch: [37]  [740/781]  eta: 0:00:08  lr: 0.000183  training_loss: 0.3577 (0.3703)  mae_loss: 0.2192 (0.2297)  classification_loss: 0.1419 (0.1405)  time: 0.1973  data: 0.0002  max mem: 5511
[09:04:20.424011] Epoch: [37]  [760/781]  eta: 0:00:04  lr: 0.000183  training_loss: 0.3677 (0.3701)  mae_loss: 0.2227 (0.2295)  classification_loss: 0.1394 (0.1406)  time: 0.1952  data: 0.0002  max mem: 5511
[09:04:24.311703] Epoch: [37]  [780/781]  eta: 0:00:00  lr: 0.000183  training_loss: 0.3533 (0.3697)  mae_loss: 0.2174 (0.2293)  classification_loss: 0.1356 (0.1405)  time: 0.1943  data: 0.0002  max mem: 5511
[09:04:24.471167] Epoch: [37] Total time: 0:02:33 (0.1969 s / it)
[09:04:24.471948] Averaged stats: lr: 0.000183  training_loss: 0.3533 (0.3697)  mae_loss: 0.2174 (0.2293)  classification_loss: 0.1356 (0.1405)
[09:04:25.032002] Test:  [  0/157]  eta: 0:01:27  testing_loss: 0.5952 (0.5952)  acc1: 84.3750 (84.3750)  acc5: 98.4375 (98.4375)  time: 0.5560  data: 0.5240  max mem: 5511
[09:04:25.314505] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.6404 (0.6630)  acc1: 79.6875 (79.8295)  acc5: 100.0000 (99.5739)  time: 0.0761  data: 0.0478  max mem: 5511
[09:04:25.602784] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.6192 (0.6423)  acc1: 79.6875 (80.3571)  acc5: 100.0000 (99.4048)  time: 0.0284  data: 0.0002  max mem: 5511
[09:04:25.892611] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.6119 (0.6536)  acc1: 78.1250 (79.4859)  acc5: 100.0000 (99.1935)  time: 0.0287  data: 0.0002  max mem: 5511
[09:04:26.180226] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.6462 (0.6584)  acc1: 78.1250 (79.2683)  acc5: 98.4375 (99.1997)  time: 0.0287  data: 0.0002  max mem: 5511
[09:04:26.462428] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6392 (0.6537)  acc1: 79.6875 (79.5650)  acc5: 100.0000 (99.0809)  time: 0.0283  data: 0.0002  max mem: 5511
[09:04:26.748015] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6302 (0.6497)  acc1: 79.6875 (79.4057)  acc5: 100.0000 (99.0523)  time: 0.0282  data: 0.0002  max mem: 5511
[09:04:27.036336] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6058 (0.6450)  acc1: 79.6875 (79.5775)  acc5: 100.0000 (99.0537)  time: 0.0286  data: 0.0002  max mem: 5511
[09:04:27.318526] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6449 (0.6504)  acc1: 79.6875 (79.3789)  acc5: 98.4375 (98.9390)  time: 0.0284  data: 0.0002  max mem: 5511
[09:04:27.602353] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6433 (0.6462)  acc1: 79.6875 (79.5330)  acc5: 98.4375 (98.9183)  time: 0.0281  data: 0.0002  max mem: 5511
[09:04:27.884798] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.6235 (0.6482)  acc1: 79.6875 (79.3626)  acc5: 98.4375 (98.9325)  time: 0.0282  data: 0.0002  max mem: 5511
[09:04:28.168220] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6225 (0.6500)  acc1: 79.6875 (79.2934)  acc5: 98.4375 (98.8880)  time: 0.0282  data: 0.0002  max mem: 5511
[09:04:28.449854] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6166 (0.6445)  acc1: 81.2500 (79.5196)  acc5: 98.4375 (98.8895)  time: 0.0281  data: 0.0002  max mem: 5511
[09:04:28.730435] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6166 (0.6457)  acc1: 79.6875 (79.5444)  acc5: 98.4375 (98.9027)  time: 0.0280  data: 0.0001  max mem: 5511
[09:04:29.012514] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6503 (0.6462)  acc1: 78.1250 (79.4548)  acc5: 98.4375 (98.9029)  time: 0.0280  data: 0.0001  max mem: 5511
[09:04:29.292406] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6447 (0.6460)  acc1: 78.1250 (79.4185)  acc5: 98.4375 (98.8721)  time: 0.0280  data: 0.0001  max mem: 5511
[09:04:29.444919] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6250 (0.6462)  acc1: 79.6875 (79.3500)  acc5: 98.4375 (98.9000)  time: 0.0271  data: 0.0001  max mem: 5511
[09:04:29.608554] Test: Total time: 0:00:05 (0.0327 s / it)
[09:04:29.609029] * Acc@1 79.350 Acc@5 98.900 loss 0.646
[09:04:29.609312] Accuracy of the network on the 10000 test images: 79.3%
[09:04:29.609534] Max accuracy: 79.35%
[09:04:29.810353] log_dir: ./output_dir
[09:04:30.555699] Epoch: [38]  [  0/781]  eta: 0:09:40  lr: 0.000183  training_loss: 0.2930 (0.2930)  mae_loss: 0.1868 (0.1868)  classification_loss: 0.1062 (0.1062)  time: 0.7433  data: 0.5428  max mem: 5511
[09:04:34.540952] Epoch: [38]  [ 20/781]  eta: 0:02:51  lr: 0.000183  training_loss: 0.3743 (0.3755)  mae_loss: 0.2358 (0.2380)  classification_loss: 0.1381 (0.1375)  time: 0.1992  data: 0.0002  max mem: 5511
[09:04:38.462928] Epoch: [38]  [ 40/781]  eta: 0:02:36  lr: 0.000183  training_loss: 0.3611 (0.3670)  mae_loss: 0.2192 (0.2290)  classification_loss: 0.1402 (0.1380)  time: 0.1960  data: 0.0003  max mem: 5511
[09:04:42.359452] Epoch: [38]  [ 60/781]  eta: 0:02:28  lr: 0.000183  training_loss: 0.3740 (0.3706)  mae_loss: 0.2361 (0.2317)  classification_loss: 0.1376 (0.1389)  time: 0.1948  data: 0.0003  max mem: 5511
[09:04:46.255096] Epoch: [38]  [ 80/781]  eta: 0:02:22  lr: 0.000183  training_loss: 0.3645 (0.3700)  mae_loss: 0.2199 (0.2304)  classification_loss: 0.1402 (0.1396)  time: 0.1947  data: 0.0003  max mem: 5511
[09:04:50.152372] Epoch: [38]  [100/781]  eta: 0:02:17  lr: 0.000182  training_loss: 0.3586 (0.3698)  mae_loss: 0.2212 (0.2296)  classification_loss: 0.1395 (0.1401)  time: 0.1948  data: 0.0002  max mem: 5511
[09:04:54.043374] Epoch: [38]  [120/781]  eta: 0:02:12  lr: 0.000182  training_loss: 0.3518 (0.3670)  mae_loss: 0.2149 (0.2280)  classification_loss: 0.1317 (0.1390)  time: 0.1945  data: 0.0003  max mem: 5511
[09:04:57.952273] Epoch: [38]  [140/781]  eta: 0:02:07  lr: 0.000182  training_loss: 0.3486 (0.3658)  mae_loss: 0.2169 (0.2271)  classification_loss: 0.1361 (0.1387)  time: 0.1954  data: 0.0002  max mem: 5511
[09:05:01.859570] Epoch: [38]  [160/781]  eta: 0:02:03  lr: 0.000182  training_loss: 0.3650 (0.3655)  mae_loss: 0.2266 (0.2270)  classification_loss: 0.1365 (0.1385)  time: 0.1953  data: 0.0002  max mem: 5511
[09:05:05.755878] Epoch: [38]  [180/781]  eta: 0:01:59  lr: 0.000182  training_loss: 0.3516 (0.3654)  mae_loss: 0.2200 (0.2271)  classification_loss: 0.1360 (0.1383)  time: 0.1947  data: 0.0002  max mem: 5511
[09:05:09.651116] Epoch: [38]  [200/781]  eta: 0:01:55  lr: 0.000182  training_loss: 0.3750 (0.3656)  mae_loss: 0.2264 (0.2270)  classification_loss: 0.1393 (0.1386)  time: 0.1947  data: 0.0002  max mem: 5511
[09:05:13.552128] Epoch: [38]  [220/781]  eta: 0:01:50  lr: 0.000182  training_loss: 0.3590 (0.3649)  mae_loss: 0.2206 (0.2264)  classification_loss: 0.1372 (0.1385)  time: 0.1950  data: 0.0003  max mem: 5511
[09:05:17.481916] Epoch: [38]  [240/781]  eta: 0:01:46  lr: 0.000182  training_loss: 0.3679 (0.3648)  mae_loss: 0.2156 (0.2260)  classification_loss: 0.1403 (0.1388)  time: 0.1963  data: 0.0002  max mem: 5511
[09:05:21.374956] Epoch: [38]  [260/781]  eta: 0:01:42  lr: 0.000182  training_loss: 0.3628 (0.3647)  mae_loss: 0.2232 (0.2260)  classification_loss: 0.1382 (0.1387)  time: 0.1946  data: 0.0002  max mem: 5511
[09:05:25.313131] Epoch: [38]  [280/781]  eta: 0:01:38  lr: 0.000182  training_loss: 0.3583 (0.3644)  mae_loss: 0.2138 (0.2254)  classification_loss: 0.1430 (0.1390)  time: 0.1968  data: 0.0002  max mem: 5511
[09:05:29.218752] Epoch: [38]  [300/781]  eta: 0:01:34  lr: 0.000182  training_loss: 0.3631 (0.3645)  mae_loss: 0.2206 (0.2253)  classification_loss: 0.1422 (0.1392)  time: 0.1952  data: 0.0002  max mem: 5511
[09:05:33.117722] Epoch: [38]  [320/781]  eta: 0:01:30  lr: 0.000181  training_loss: 0.3667 (0.3650)  mae_loss: 0.2275 (0.2258)  classification_loss: 0.1371 (0.1392)  time: 0.1949  data: 0.0002  max mem: 5511
[09:05:37.003567] Epoch: [38]  [340/781]  eta: 0:01:26  lr: 0.000181  training_loss: 0.3692 (0.3655)  mae_loss: 0.2353 (0.2263)  classification_loss: 0.1372 (0.1392)  time: 0.1942  data: 0.0002  max mem: 5511
[09:05:40.896111] Epoch: [38]  [360/781]  eta: 0:01:22  lr: 0.000181  training_loss: 0.3573 (0.3654)  mae_loss: 0.2132 (0.2262)  classification_loss: 0.1402 (0.1392)  time: 0.1945  data: 0.0002  max mem: 5511
[09:05:44.787606] Epoch: [38]  [380/781]  eta: 0:01:18  lr: 0.000181  training_loss: 0.3695 (0.3655)  mae_loss: 0.2256 (0.2264)  classification_loss: 0.1372 (0.1391)  time: 0.1945  data: 0.0002  max mem: 5511
[09:05:48.720022] Epoch: [38]  [400/781]  eta: 0:01:14  lr: 0.000181  training_loss: 0.3620 (0.3656)  mae_loss: 0.2265 (0.2267)  classification_loss: 0.1343 (0.1389)  time: 0.1965  data: 0.0002  max mem: 5511
[09:05:52.677335] Epoch: [38]  [420/781]  eta: 0:01:11  lr: 0.000181  training_loss: 0.3519 (0.3656)  mae_loss: 0.2277 (0.2268)  classification_loss: 0.1364 (0.1388)  time: 0.1977  data: 0.0002  max mem: 5511
[09:05:56.590802] Epoch: [38]  [440/781]  eta: 0:01:07  lr: 0.000181  training_loss: 0.3597 (0.3657)  mae_loss: 0.2222 (0.2268)  classification_loss: 0.1392 (0.1389)  time: 0.1956  data: 0.0002  max mem: 5511
[09:06:00.490085] Epoch: [38]  [460/781]  eta: 0:01:03  lr: 0.000181  training_loss: 0.3553 (0.3653)  mae_loss: 0.2154 (0.2264)  classification_loss: 0.1379 (0.1389)  time: 0.1949  data: 0.0003  max mem: 5511
[09:06:04.418896] Epoch: [38]  [480/781]  eta: 0:00:59  lr: 0.000181  training_loss: 0.3618 (0.3655)  mae_loss: 0.2218 (0.2265)  classification_loss: 0.1447 (0.1390)  time: 0.1963  data: 0.0002  max mem: 5511
[09:06:08.324428] Epoch: [38]  [500/781]  eta: 0:00:55  lr: 0.000181  training_loss: 0.3629 (0.3654)  mae_loss: 0.2241 (0.2265)  classification_loss: 0.1347 (0.1389)  time: 0.1952  data: 0.0003  max mem: 5511
[09:06:12.206484] Epoch: [38]  [520/781]  eta: 0:00:51  lr: 0.000180  training_loss: 0.3688 (0.3656)  mae_loss: 0.2259 (0.2268)  classification_loss: 0.1360 (0.1389)  time: 0.1940  data: 0.0002  max mem: 5511
[09:06:16.100706] Epoch: [38]  [540/781]  eta: 0:00:47  lr: 0.000180  training_loss: 0.3643 (0.3657)  mae_loss: 0.2186 (0.2267)  classification_loss: 0.1423 (0.1390)  time: 0.1946  data: 0.0002  max mem: 5511
[09:06:20.018987] Epoch: [38]  [560/781]  eta: 0:00:43  lr: 0.000180  training_loss: 0.3604 (0.3658)  mae_loss: 0.2231 (0.2268)  classification_loss: 0.1373 (0.1390)  time: 0.1958  data: 0.0003  max mem: 5511
[09:06:23.912032] Epoch: [38]  [580/781]  eta: 0:00:39  lr: 0.000180  training_loss: 0.3804 (0.3664)  mae_loss: 0.2372 (0.2274)  classification_loss: 0.1389 (0.1390)  time: 0.1946  data: 0.0002  max mem: 5511
[09:06:27.882051] Epoch: [38]  [600/781]  eta: 0:00:35  lr: 0.000180  training_loss: 0.3555 (0.3661)  mae_loss: 0.2184 (0.2272)  classification_loss: 0.1386 (0.1390)  time: 0.1984  data: 0.0003  max mem: 5511
[09:06:31.808715] Epoch: [38]  [620/781]  eta: 0:00:31  lr: 0.000180  training_loss: 0.3653 (0.3664)  mae_loss: 0.2364 (0.2275)  classification_loss: 0.1322 (0.1389)  time: 0.1962  data: 0.0002  max mem: 5511
[09:06:35.708519] Epoch: [38]  [640/781]  eta: 0:00:27  lr: 0.000180  training_loss: 0.3441 (0.3662)  mae_loss: 0.2089 (0.2273)  classification_loss: 0.1366 (0.1389)  time: 0.1949  data: 0.0002  max mem: 5511
[09:06:39.671761] Epoch: [38]  [660/781]  eta: 0:00:23  lr: 0.000180  training_loss: 0.3590 (0.3662)  mae_loss: 0.2208 (0.2272)  classification_loss: 0.1387 (0.1390)  time: 0.1981  data: 0.0002  max mem: 5511
[09:06:43.585185] Epoch: [38]  [680/781]  eta: 0:00:19  lr: 0.000180  training_loss: 0.3694 (0.3665)  mae_loss: 0.2331 (0.2275)  classification_loss: 0.1383 (0.1390)  time: 0.1956  data: 0.0002  max mem: 5511
[09:06:47.475311] Epoch: [38]  [700/781]  eta: 0:00:15  lr: 0.000180  training_loss: 0.3694 (0.3668)  mae_loss: 0.2376 (0.2278)  classification_loss: 0.1406 (0.1390)  time: 0.1944  data: 0.0002  max mem: 5511
[09:06:51.383417] Epoch: [38]  [720/781]  eta: 0:00:11  lr: 0.000180  training_loss: 0.3605 (0.3668)  mae_loss: 0.2191 (0.2277)  classification_loss: 0.1427 (0.1391)  time: 0.1953  data: 0.0002  max mem: 5511
[09:06:55.312079] Epoch: [38]  [740/781]  eta: 0:00:08  lr: 0.000179  training_loss: 0.3503 (0.3666)  mae_loss: 0.2131 (0.2276)  classification_loss: 0.1343 (0.1391)  time: 0.1963  data: 0.0002  max mem: 5511
[09:06:59.301272] Epoch: [38]  [760/781]  eta: 0:00:04  lr: 0.000179  training_loss: 0.3548 (0.3668)  mae_loss: 0.2229 (0.2276)  classification_loss: 0.1398 (0.1391)  time: 0.1994  data: 0.0002  max mem: 5511
[09:07:03.197957] Epoch: [38]  [780/781]  eta: 0:00:00  lr: 0.000179  training_loss: 0.3717 (0.3665)  mae_loss: 0.2308 (0.2275)  classification_loss: 0.1376 (0.1391)  time: 0.1948  data: 0.0002  max mem: 5511
[09:07:03.355176] Epoch: [38] Total time: 0:02:33 (0.1966 s / it)
[09:07:03.355641] Averaged stats: lr: 0.000179  training_loss: 0.3717 (0.3665)  mae_loss: 0.2308 (0.2275)  classification_loss: 0.1376 (0.1391)
[09:07:04.034672] Test:  [  0/157]  eta: 0:01:45  testing_loss: 0.6716 (0.6716)  acc1: 84.3750 (84.3750)  acc5: 96.8750 (96.8750)  time: 0.6751  data: 0.6457  max mem: 5511
[09:07:04.340177] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.6716 (0.7039)  acc1: 81.2500 (79.1193)  acc5: 98.4375 (98.7216)  time: 0.0890  data: 0.0592  max mem: 5511
[09:07:04.627893] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.6216 (0.6620)  acc1: 81.2500 (80.2827)  acc5: 98.4375 (98.8095)  time: 0.0295  data: 0.0003  max mem: 5511
[09:07:04.912197] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6342 (0.6662)  acc1: 79.6875 (79.7883)  acc5: 100.0000 (98.7903)  time: 0.0285  data: 0.0001  max mem: 5511
[09:07:05.194353] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6772 (0.6712)  acc1: 78.1250 (79.4588)  acc5: 100.0000 (98.6280)  time: 0.0282  data: 0.0002  max mem: 5511
[09:07:05.475136] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6512 (0.6638)  acc1: 78.1250 (79.5650)  acc5: 98.4375 (98.7132)  time: 0.0280  data: 0.0002  max mem: 5511
[09:07:05.757755] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6347 (0.6603)  acc1: 79.6875 (79.5338)  acc5: 100.0000 (98.7961)  time: 0.0281  data: 0.0002  max mem: 5511
[09:07:06.040374] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6015 (0.6519)  acc1: 81.2500 (79.9516)  acc5: 98.4375 (98.8116)  time: 0.0281  data: 0.0002  max mem: 5511
[09:07:06.322889] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6025 (0.6528)  acc1: 82.8125 (79.9190)  acc5: 98.4375 (98.7076)  time: 0.0281  data: 0.0002  max mem: 5511
[09:07:06.603991] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6330 (0.6486)  acc1: 82.8125 (80.2026)  acc5: 98.4375 (98.7294)  time: 0.0281  data: 0.0002  max mem: 5511
[09:07:06.885324] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.6362 (0.6509)  acc1: 81.2500 (80.0588)  acc5: 98.4375 (98.7314)  time: 0.0280  data: 0.0002  max mem: 5511
[09:07:07.165887] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6582 (0.6515)  acc1: 78.1250 (80.0113)  acc5: 98.4375 (98.7331)  time: 0.0280  data: 0.0002  max mem: 5511
[09:07:07.447466] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6138 (0.6457)  acc1: 82.8125 (80.1911)  acc5: 100.0000 (98.7991)  time: 0.0280  data: 0.0002  max mem: 5511
[09:07:07.729735] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5755 (0.6481)  acc1: 79.6875 (80.1646)  acc5: 100.0000 (98.8192)  time: 0.0281  data: 0.0002  max mem: 5511
[09:07:08.009460] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6735 (0.6481)  acc1: 79.6875 (80.2527)  acc5: 100.0000 (98.8364)  time: 0.0280  data: 0.0002  max mem: 5511
[09:07:08.288398] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6694 (0.6476)  acc1: 79.6875 (80.2773)  acc5: 98.4375 (98.8100)  time: 0.0278  data: 0.0001  max mem: 5511
[09:07:08.437935] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6260 (0.6471)  acc1: 81.2500 (80.2300)  acc5: 98.4375 (98.8300)  time: 0.0269  data: 0.0001  max mem: 5511
[09:07:08.614980] Test: Total time: 0:00:05 (0.0335 s / it)
[09:07:08.615444] * Acc@1 80.230 Acc@5 98.830 loss 0.647
[09:07:08.615779] Accuracy of the network on the 10000 test images: 80.2%
[09:07:08.616015] Max accuracy: 80.23%
[09:07:08.792377] log_dir: ./output_dir
[09:07:09.569962] Epoch: [39]  [  0/781]  eta: 0:10:05  lr: 0.000179  training_loss: 0.3148 (0.3148)  mae_loss: 0.2008 (0.2008)  classification_loss: 0.1140 (0.1140)  time: 0.7752  data: 0.5618  max mem: 5511
[09:07:13.469080] Epoch: [39]  [ 20/781]  eta: 0:02:49  lr: 0.000179  training_loss: 0.3540 (0.3620)  mae_loss: 0.2203 (0.2278)  classification_loss: 0.1322 (0.1342)  time: 0.1949  data: 0.0002  max mem: 5511
[09:07:17.376388] Epoch: [39]  [ 40/781]  eta: 0:02:35  lr: 0.000179  training_loss: 0.3627 (0.3620)  mae_loss: 0.2216 (0.2250)  classification_loss: 0.1390 (0.1370)  time: 0.1952  data: 0.0002  max mem: 5511
[09:07:21.285128] Epoch: [39]  [ 60/781]  eta: 0:02:27  lr: 0.000179  training_loss: 0.3637 (0.3614)  mae_loss: 0.2146 (0.2238)  classification_loss: 0.1353 (0.1376)  time: 0.1954  data: 0.0002  max mem: 5511
[09:07:25.180858] Epoch: [39]  [ 80/781]  eta: 0:02:21  lr: 0.000179  training_loss: 0.3617 (0.3596)  mae_loss: 0.2193 (0.2220)  classification_loss: 0.1354 (0.1376)  time: 0.1947  data: 0.0002  max mem: 5511
[09:07:29.071799] Epoch: [39]  [100/781]  eta: 0:02:16  lr: 0.000179  training_loss: 0.3646 (0.3604)  mae_loss: 0.2246 (0.2228)  classification_loss: 0.1355 (0.1376)  time: 0.1945  data: 0.0003  max mem: 5511
[09:07:32.983238] Epoch: [39]  [120/781]  eta: 0:02:12  lr: 0.000179  training_loss: 0.3508 (0.3597)  mae_loss: 0.2120 (0.2228)  classification_loss: 0.1353 (0.1370)  time: 0.1955  data: 0.0002  max mem: 5511
[09:07:36.897519] Epoch: [39]  [140/781]  eta: 0:02:07  lr: 0.000179  training_loss: 0.3550 (0.3587)  mae_loss: 0.2175 (0.2221)  classification_loss: 0.1329 (0.1366)  time: 0.1956  data: 0.0002  max mem: 5511
[09:07:40.830982] Epoch: [39]  [160/781]  eta: 0:02:03  lr: 0.000178  training_loss: 0.3459 (0.3570)  mae_loss: 0.2085 (0.2207)  classification_loss: 0.1325 (0.1363)  time: 0.1966  data: 0.0004  max mem: 5511
[09:07:44.759902] Epoch: [39]  [180/781]  eta: 0:01:59  lr: 0.000178  training_loss: 0.3586 (0.3576)  mae_loss: 0.2234 (0.2213)  classification_loss: 0.1364 (0.1363)  time: 0.1963  data: 0.0004  max mem: 5511
[09:07:48.723063] Epoch: [39]  [200/781]  eta: 0:01:55  lr: 0.000178  training_loss: 0.3476 (0.3571)  mae_loss: 0.2068 (0.2206)  classification_loss: 0.1364 (0.1365)  time: 0.1981  data: 0.0002  max mem: 5511
[09:07:52.626043] Epoch: [39]  [220/781]  eta: 0:01:51  lr: 0.000178  training_loss: 0.3584 (0.3581)  mae_loss: 0.2200 (0.2212)  classification_loss: 0.1389 (0.1368)  time: 0.1951  data: 0.0002  max mem: 5511
[09:07:56.527076] Epoch: [39]  [240/781]  eta: 0:01:47  lr: 0.000178  training_loss: 0.3691 (0.3593)  mae_loss: 0.2261 (0.2218)  classification_loss: 0.1419 (0.1374)  time: 0.1949  data: 0.0002  max mem: 5511
[09:08:00.436260] Epoch: [39]  [260/781]  eta: 0:01:43  lr: 0.000178  training_loss: 0.3545 (0.3595)  mae_loss: 0.2144 (0.2219)  classification_loss: 0.1372 (0.1375)  time: 0.1953  data: 0.0002  max mem: 5511
[09:08:04.354749] Epoch: [39]  [280/781]  eta: 0:01:39  lr: 0.000178  training_loss: 0.3672 (0.3599)  mae_loss: 0.2241 (0.2221)  classification_loss: 0.1421 (0.1378)  time: 0.1958  data: 0.0002  max mem: 5511
[09:08:08.255020] Epoch: [39]  [300/781]  eta: 0:01:34  lr: 0.000178  training_loss: 0.3495 (0.3595)  mae_loss: 0.2187 (0.2218)  classification_loss: 0.1362 (0.1377)  time: 0.1949  data: 0.0002  max mem: 5511
[09:08:12.161176] Epoch: [39]  [320/781]  eta: 0:01:30  lr: 0.000178  training_loss: 0.3599 (0.3594)  mae_loss: 0.2190 (0.2217)  classification_loss: 0.1374 (0.1377)  time: 0.1952  data: 0.0002  max mem: 5511
[09:08:16.100931] Epoch: [39]  [340/781]  eta: 0:01:27  lr: 0.000178  training_loss: 0.3601 (0.3600)  mae_loss: 0.2245 (0.2222)  classification_loss: 0.1343 (0.1377)  time: 0.1969  data: 0.0004  max mem: 5511
[09:08:19.991970] Epoch: [39]  [360/781]  eta: 0:01:22  lr: 0.000178  training_loss: 0.3409 (0.3593)  mae_loss: 0.2061 (0.2215)  classification_loss: 0.1357 (0.1377)  time: 0.1945  data: 0.0003  max mem: 5511
[09:08:23.964453] Epoch: [39]  [380/781]  eta: 0:01:19  lr: 0.000177  training_loss: 0.3670 (0.3595)  mae_loss: 0.2273 (0.2217)  classification_loss: 0.1380 (0.1378)  time: 0.1985  data: 0.0002  max mem: 5511
[09:08:27.859981] Epoch: [39]  [400/781]  eta: 0:01:15  lr: 0.000177  training_loss: 0.3734 (0.3601)  mae_loss: 0.2271 (0.2221)  classification_loss: 0.1383 (0.1380)  time: 0.1947  data: 0.0002  max mem: 5511
[09:08:31.767506] Epoch: [39]  [420/781]  eta: 0:01:11  lr: 0.000177  training_loss: 0.3598 (0.3601)  mae_loss: 0.2141 (0.2221)  classification_loss: 0.1335 (0.1379)  time: 0.1953  data: 0.0002  max mem: 5511
[09:08:35.663582] Epoch: [39]  [440/781]  eta: 0:01:07  lr: 0.000177  training_loss: 0.3580 (0.3603)  mae_loss: 0.2185 (0.2224)  classification_loss: 0.1363 (0.1379)  time: 0.1947  data: 0.0002  max mem: 5511
[09:08:39.565562] Epoch: [39]  [460/781]  eta: 0:01:03  lr: 0.000177  training_loss: 0.3600 (0.3604)  mae_loss: 0.2222 (0.2224)  classification_loss: 0.1380 (0.1379)  time: 0.1950  data: 0.0002  max mem: 5511
[09:08:43.453528] Epoch: [39]  [480/781]  eta: 0:00:59  lr: 0.000177  training_loss: 0.3552 (0.3606)  mae_loss: 0.2185 (0.2225)  classification_loss: 0.1415 (0.1381)  time: 0.1943  data: 0.0003  max mem: 5511
[09:08:47.336650] Epoch: [39]  [500/781]  eta: 0:00:55  lr: 0.000177  training_loss: 0.3745 (0.3609)  mae_loss: 0.2364 (0.2229)  classification_loss: 0.1328 (0.1380)  time: 0.1941  data: 0.0003  max mem: 5511
[09:08:51.231226] Epoch: [39]  [520/781]  eta: 0:00:51  lr: 0.000177  training_loss: 0.3772 (0.3614)  mae_loss: 0.2284 (0.2233)  classification_loss: 0.1402 (0.1381)  time: 0.1947  data: 0.0002  max mem: 5511
[09:08:55.152447] Epoch: [39]  [540/781]  eta: 0:00:47  lr: 0.000177  training_loss: 0.3631 (0.3615)  mae_loss: 0.2185 (0.2233)  classification_loss: 0.1383 (0.1382)  time: 0.1960  data: 0.0003  max mem: 5511
[09:08:59.073580] Epoch: [39]  [560/781]  eta: 0:00:43  lr: 0.000177  training_loss: 0.3598 (0.3614)  mae_loss: 0.2215 (0.2233)  classification_loss: 0.1373 (0.1381)  time: 0.1960  data: 0.0003  max mem: 5511
[09:09:02.979934] Epoch: [39]  [580/781]  eta: 0:00:39  lr: 0.000176  training_loss: 0.3556 (0.3614)  mae_loss: 0.2192 (0.2233)  classification_loss: 0.1367 (0.1381)  time: 0.1952  data: 0.0002  max mem: 5511
[09:09:06.902554] Epoch: [39]  [600/781]  eta: 0:00:35  lr: 0.000176  training_loss: 0.3567 (0.3615)  mae_loss: 0.2281 (0.2235)  classification_loss: 0.1340 (0.1380)  time: 0.1961  data: 0.0002  max mem: 5511
[09:09:10.809933] Epoch: [39]  [620/781]  eta: 0:00:31  lr: 0.000176  training_loss: 0.3720 (0.3619)  mae_loss: 0.2273 (0.2240)  classification_loss: 0.1371 (0.1380)  time: 0.1953  data: 0.0002  max mem: 5511
[09:09:14.719255] Epoch: [39]  [640/781]  eta: 0:00:27  lr: 0.000176  training_loss: 0.3570 (0.3620)  mae_loss: 0.2227 (0.2241)  classification_loss: 0.1357 (0.1379)  time: 0.1954  data: 0.0002  max mem: 5511
[09:09:18.626795] Epoch: [39]  [660/781]  eta: 0:00:23  lr: 0.000176  training_loss: 0.3729 (0.3625)  mae_loss: 0.2329 (0.2245)  classification_loss: 0.1378 (0.1380)  time: 0.1953  data: 0.0002  max mem: 5511
[09:09:22.580727] Epoch: [39]  [680/781]  eta: 0:00:19  lr: 0.000176  training_loss: 0.3700 (0.3627)  mae_loss: 0.2283 (0.2246)  classification_loss: 0.1408 (0.1381)  time: 0.1976  data: 0.0002  max mem: 5511
[09:09:26.461279] Epoch: [39]  [700/781]  eta: 0:00:15  lr: 0.000176  training_loss: 0.3668 (0.3629)  mae_loss: 0.2324 (0.2248)  classification_loss: 0.1400 (0.1381)  time: 0.1940  data: 0.0002  max mem: 5511
[09:09:30.358889] Epoch: [39]  [720/781]  eta: 0:00:11  lr: 0.000176  training_loss: 0.3662 (0.3629)  mae_loss: 0.2260 (0.2248)  classification_loss: 0.1403 (0.1381)  time: 0.1948  data: 0.0002  max mem: 5511
[09:09:34.299309] Epoch: [39]  [740/781]  eta: 0:00:08  lr: 0.000176  training_loss: 0.3452 (0.3626)  mae_loss: 0.2116 (0.2246)  classification_loss: 0.1312 (0.1380)  time: 0.1969  data: 0.0002  max mem: 5511
[09:09:38.234708] Epoch: [39]  [760/781]  eta: 0:00:04  lr: 0.000176  training_loss: 0.3698 (0.3629)  mae_loss: 0.2304 (0.2248)  classification_loss: 0.1432 (0.1381)  time: 0.1966  data: 0.0002  max mem: 5511
[09:09:42.169794] Epoch: [39]  [780/781]  eta: 0:00:00  lr: 0.000176  training_loss: 0.3680 (0.3630)  mae_loss: 0.2228 (0.2248)  classification_loss: 0.1406 (0.1382)  time: 0.1967  data: 0.0003  max mem: 5511
[09:09:42.326735] Epoch: [39] Total time: 0:02:33 (0.1966 s / it)
[09:09:42.327190] Averaged stats: lr: 0.000176  training_loss: 0.3680 (0.3630)  mae_loss: 0.2228 (0.2248)  classification_loss: 0.1406 (0.1382)
[09:09:43.015699] Test:  [  0/157]  eta: 0:01:47  testing_loss: 0.5561 (0.5561)  acc1: 87.5000 (87.5000)  acc5: 95.3125 (95.3125)  time: 0.6840  data: 0.6524  max mem: 5511
[09:09:43.303849] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.6970 (0.6712)  acc1: 76.5625 (77.2727)  acc5: 98.4375 (98.7216)  time: 0.0882  data: 0.0595  max mem: 5511
[09:09:43.594539] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.6395 (0.6431)  acc1: 78.1250 (78.8690)  acc5: 100.0000 (99.0327)  time: 0.0288  data: 0.0001  max mem: 5511
[09:09:43.877341] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6414 (0.6568)  acc1: 79.6875 (79.0323)  acc5: 100.0000 (98.8407)  time: 0.0286  data: 0.0002  max mem: 5511
[09:09:44.167237] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6569 (0.6615)  acc1: 79.6875 (78.7729)  acc5: 100.0000 (98.9329)  time: 0.0285  data: 0.0002  max mem: 5511
[09:09:44.456002] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6228 (0.6541)  acc1: 79.6875 (78.8909)  acc5: 100.0000 (98.8971)  time: 0.0288  data: 0.0002  max mem: 5511
[09:09:44.743305] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6117 (0.6478)  acc1: 79.6875 (79.0215)  acc5: 98.4375 (98.8473)  time: 0.0287  data: 0.0002  max mem: 5511
[09:09:45.024961] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5789 (0.6401)  acc1: 81.2500 (79.3354)  acc5: 100.0000 (98.9217)  time: 0.0283  data: 0.0002  max mem: 5511
[09:09:45.307295] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5887 (0.6441)  acc1: 79.6875 (79.2438)  acc5: 98.4375 (98.7847)  time: 0.0281  data: 0.0002  max mem: 5511
[09:09:45.589813] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6567 (0.6414)  acc1: 79.6875 (79.3956)  acc5: 98.4375 (98.7637)  time: 0.0281  data: 0.0002  max mem: 5511
[09:09:45.872164] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.6529 (0.6446)  acc1: 79.6875 (79.2234)  acc5: 100.0000 (98.7933)  time: 0.0281  data: 0.0002  max mem: 5511
[09:09:46.154125] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6195 (0.6441)  acc1: 78.1250 (79.1667)  acc5: 100.0000 (98.7613)  time: 0.0281  data: 0.0001  max mem: 5511
[09:09:46.436516] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6084 (0.6399)  acc1: 79.6875 (79.3388)  acc5: 98.4375 (98.7732)  time: 0.0281  data: 0.0001  max mem: 5511
[09:09:46.718840] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6084 (0.6422)  acc1: 81.2500 (79.2223)  acc5: 100.0000 (98.7715)  time: 0.0281  data: 0.0002  max mem: 5511
[09:09:46.998291] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6164 (0.6411)  acc1: 81.2500 (79.2996)  acc5: 100.0000 (98.7921)  time: 0.0280  data: 0.0001  max mem: 5511
[09:09:47.277321] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6164 (0.6382)  acc1: 81.2500 (79.4599)  acc5: 98.4375 (98.8100)  time: 0.0278  data: 0.0001  max mem: 5511
[09:09:47.428568] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6106 (0.6374)  acc1: 79.6875 (79.4100)  acc5: 100.0000 (98.8400)  time: 0.0269  data: 0.0001  max mem: 5511
[09:09:47.610068] Test: Total time: 0:00:05 (0.0336 s / it)
[09:09:47.610515] * Acc@1 79.410 Acc@5 98.840 loss 0.637
[09:09:47.610840] Accuracy of the network on the 10000 test images: 79.4%
[09:09:47.611038] Max accuracy: 80.23%
[09:09:47.768963] log_dir: ./output_dir
[09:09:48.686500] Epoch: [40]  [  0/781]  eta: 0:11:55  lr: 0.000176  training_loss: 0.3069 (0.3069)  mae_loss: 0.1795 (0.1795)  classification_loss: 0.1274 (0.1274)  time: 0.9160  data: 0.6997  max mem: 5511
[09:09:52.587691] Epoch: [40]  [ 20/781]  eta: 0:02:54  lr: 0.000175  training_loss: 0.3697 (0.3693)  mae_loss: 0.2442 (0.2378)  classification_loss: 0.1274 (0.1315)  time: 0.1949  data: 0.0002  max mem: 5511
[09:09:56.515581] Epoch: [40]  [ 40/781]  eta: 0:02:37  lr: 0.000175  training_loss: 0.3609 (0.3672)  mae_loss: 0.2202 (0.2318)  classification_loss: 0.1407 (0.1354)  time: 0.1963  data: 0.0003  max mem: 5511
[09:10:00.486374] Epoch: [40]  [ 60/781]  eta: 0:02:30  lr: 0.000175  training_loss: 0.3458 (0.3632)  mae_loss: 0.2179 (0.2276)  classification_loss: 0.1326 (0.1356)  time: 0.1985  data: 0.0002  max mem: 5511
[09:10:04.379435] Epoch: [40]  [ 80/781]  eta: 0:02:23  lr: 0.000175  training_loss: 0.3426 (0.3609)  mae_loss: 0.2016 (0.2256)  classification_loss: 0.1315 (0.1353)  time: 0.1946  data: 0.0002  max mem: 5511
[09:10:08.283800] Epoch: [40]  [100/781]  eta: 0:02:18  lr: 0.000175  training_loss: 0.3544 (0.3622)  mae_loss: 0.2195 (0.2254)  classification_loss: 0.1399 (0.1368)  time: 0.1951  data: 0.0003  max mem: 5511
[09:10:12.197366] Epoch: [40]  [120/781]  eta: 0:02:13  lr: 0.000175  training_loss: 0.3539 (0.3615)  mae_loss: 0.2194 (0.2249)  classification_loss: 0.1337 (0.1366)  time: 0.1956  data: 0.0002  max mem: 5511
[09:10:16.089165] Epoch: [40]  [140/781]  eta: 0:02:08  lr: 0.000175  training_loss: 0.3611 (0.3626)  mae_loss: 0.2220 (0.2262)  classification_loss: 0.1361 (0.1365)  time: 0.1945  data: 0.0001  max mem: 5511
[09:10:20.010521] Epoch: [40]  [160/781]  eta: 0:02:04  lr: 0.000175  training_loss: 0.3498 (0.3620)  mae_loss: 0.2155 (0.2260)  classification_loss: 0.1294 (0.1360)  time: 0.1960  data: 0.0003  max mem: 5511
[09:10:23.920477] Epoch: [40]  [180/781]  eta: 0:01:59  lr: 0.000175  training_loss: 0.3429 (0.3614)  mae_loss: 0.2171 (0.2255)  classification_loss: 0.1331 (0.1360)  time: 0.1954  data: 0.0003  max mem: 5511
[09:10:27.849993] Epoch: [40]  [200/781]  eta: 0:01:55  lr: 0.000175  training_loss: 0.3555 (0.3611)  mae_loss: 0.2176 (0.2251)  classification_loss: 0.1323 (0.1359)  time: 0.1964  data: 0.0002  max mem: 5511
[09:10:31.781424] Epoch: [40]  [220/781]  eta: 0:01:51  lr: 0.000174  training_loss: 0.3661 (0.3617)  mae_loss: 0.2325 (0.2257)  classification_loss: 0.1389 (0.1360)  time: 0.1965  data: 0.0002  max mem: 5511
[09:10:35.693273] Epoch: [40]  [240/781]  eta: 0:01:47  lr: 0.000174  training_loss: 0.3591 (0.3613)  mae_loss: 0.2191 (0.2252)  classification_loss: 0.1350 (0.1361)  time: 0.1955  data: 0.0002  max mem: 5511
[09:10:39.596966] Epoch: [40]  [260/781]  eta: 0:01:43  lr: 0.000174  training_loss: 0.3708 (0.3625)  mae_loss: 0.2322 (0.2261)  classification_loss: 0.1399 (0.1364)  time: 0.1951  data: 0.0002  max mem: 5511
[09:10:43.505162] Epoch: [40]  [280/781]  eta: 0:01:39  lr: 0.000174  training_loss: 0.3669 (0.3633)  mae_loss: 0.2335 (0.2269)  classification_loss: 0.1363 (0.1364)  time: 0.1953  data: 0.0002  max mem: 5511
[09:10:47.428729] Epoch: [40]  [300/781]  eta: 0:01:35  lr: 0.000174  training_loss: 0.3520 (0.3631)  mae_loss: 0.2184 (0.2264)  classification_loss: 0.1383 (0.1367)  time: 0.1961  data: 0.0002  max mem: 5511
[09:10:51.342380] Epoch: [40]  [320/781]  eta: 0:01:31  lr: 0.000174  training_loss: 0.3714 (0.3638)  mae_loss: 0.2410 (0.2273)  classification_loss: 0.1321 (0.1366)  time: 0.1956  data: 0.0002  max mem: 5511
[09:10:55.243446] Epoch: [40]  [340/781]  eta: 0:01:27  lr: 0.000174  training_loss: 0.3721 (0.3642)  mae_loss: 0.2400 (0.2279)  classification_loss: 0.1328 (0.1363)  time: 0.1950  data: 0.0002  max mem: 5511
[09:10:59.195152] Epoch: [40]  [360/781]  eta: 0:01:23  lr: 0.000174  training_loss: 0.3542 (0.3640)  mae_loss: 0.2180 (0.2277)  classification_loss: 0.1380 (0.1363)  time: 0.1975  data: 0.0004  max mem: 5511
[09:11:03.113911] Epoch: [40]  [380/781]  eta: 0:01:19  lr: 0.000174  training_loss: 0.3656 (0.3641)  mae_loss: 0.2217 (0.2278)  classification_loss: 0.1358 (0.1363)  time: 0.1959  data: 0.0003  max mem: 5511
[09:11:07.072589] Epoch: [40]  [400/781]  eta: 0:01:15  lr: 0.000174  training_loss: 0.3551 (0.3635)  mae_loss: 0.2131 (0.2271)  classification_loss: 0.1363 (0.1364)  time: 0.1979  data: 0.0003  max mem: 5511
[09:11:10.974061] Epoch: [40]  [420/781]  eta: 0:01:11  lr: 0.000173  training_loss: 0.3509 (0.3635)  mae_loss: 0.2205 (0.2271)  classification_loss: 0.1320 (0.1364)  time: 0.1950  data: 0.0002  max mem: 5511
[09:11:14.866130] Epoch: [40]  [440/781]  eta: 0:01:07  lr: 0.000173  training_loss: 0.3568 (0.3634)  mae_loss: 0.2181 (0.2270)  classification_loss: 0.1354 (0.1364)  time: 0.1945  data: 0.0002  max mem: 5511
[09:11:18.755030] Epoch: [40]  [460/781]  eta: 0:01:03  lr: 0.000173  training_loss: 0.3571 (0.3631)  mae_loss: 0.2209 (0.2269)  classification_loss: 0.1316 (0.1363)  time: 0.1944  data: 0.0002  max mem: 5511
[09:11:22.672394] Epoch: [40]  [480/781]  eta: 0:00:59  lr: 0.000173  training_loss: 0.3488 (0.3628)  mae_loss: 0.2096 (0.2265)  classification_loss: 0.1395 (0.1363)  time: 0.1958  data: 0.0003  max mem: 5511
[09:11:26.570404] Epoch: [40]  [500/781]  eta: 0:00:55  lr: 0.000173  training_loss: 0.3704 (0.3631)  mae_loss: 0.2280 (0.2267)  classification_loss: 0.1339 (0.1364)  time: 0.1948  data: 0.0002  max mem: 5511
[09:11:30.469396] Epoch: [40]  [520/781]  eta: 0:00:51  lr: 0.000173  training_loss: 0.3748 (0.3635)  mae_loss: 0.2368 (0.2271)  classification_loss: 0.1381 (0.1364)  time: 0.1949  data: 0.0002  max mem: 5511
[09:11:34.404538] Epoch: [40]  [540/781]  eta: 0:00:47  lr: 0.000173  training_loss: 0.3559 (0.3634)  mae_loss: 0.2215 (0.2269)  classification_loss: 0.1363 (0.1364)  time: 0.1967  data: 0.0002  max mem: 5511
[09:11:38.317088] Epoch: [40]  [560/781]  eta: 0:00:43  lr: 0.000173  training_loss: 0.3570 (0.3632)  mae_loss: 0.2310 (0.2268)  classification_loss: 0.1353 (0.1364)  time: 0.1956  data: 0.0002  max mem: 5511
[09:11:42.212161] Epoch: [40]  [580/781]  eta: 0:00:39  lr: 0.000173  training_loss: 0.3444 (0.3626)  mae_loss: 0.2121 (0.2264)  classification_loss: 0.1299 (0.1362)  time: 0.1946  data: 0.0004  max mem: 5511
[09:11:46.122669] Epoch: [40]  [600/781]  eta: 0:00:35  lr: 0.000173  training_loss: 0.3495 (0.3626)  mae_loss: 0.2158 (0.2264)  classification_loss: 0.1354 (0.1362)  time: 0.1954  data: 0.0002  max mem: 5511
[09:11:50.052530] Epoch: [40]  [620/781]  eta: 0:00:31  lr: 0.000173  training_loss: 0.3654 (0.3626)  mae_loss: 0.2277 (0.2263)  classification_loss: 0.1383 (0.1363)  time: 0.1964  data: 0.0003  max mem: 5511
[09:11:53.989742] Epoch: [40]  [640/781]  eta: 0:00:27  lr: 0.000172  training_loss: 0.3587 (0.3627)  mae_loss: 0.2279 (0.2264)  classification_loss: 0.1373 (0.1363)  time: 0.1967  data: 0.0002  max mem: 5511
[09:11:57.882423] Epoch: [40]  [660/781]  eta: 0:00:23  lr: 0.000172  training_loss: 0.3524 (0.3625)  mae_loss: 0.2163 (0.2262)  classification_loss: 0.1347 (0.1363)  time: 0.1945  data: 0.0002  max mem: 5511
[09:12:01.776064] Epoch: [40]  [680/781]  eta: 0:00:19  lr: 0.000172  training_loss: 0.3534 (0.3625)  mae_loss: 0.2125 (0.2261)  classification_loss: 0.1386 (0.1364)  time: 0.1946  data: 0.0002  max mem: 5511
[09:12:05.681591] Epoch: [40]  [700/781]  eta: 0:00:15  lr: 0.000172  training_loss: 0.3569 (0.3624)  mae_loss: 0.2180 (0.2259)  classification_loss: 0.1389 (0.1365)  time: 0.1952  data: 0.0002  max mem: 5511
[09:12:09.579147] Epoch: [40]  [720/781]  eta: 0:00:11  lr: 0.000172  training_loss: 0.3534 (0.3625)  mae_loss: 0.2188 (0.2260)  classification_loss: 0.1351 (0.1365)  time: 0.1948  data: 0.0002  max mem: 5511
[09:12:13.488329] Epoch: [40]  [740/781]  eta: 0:00:08  lr: 0.000172  training_loss: 0.3408 (0.3622)  mae_loss: 0.2155 (0.2258)  classification_loss: 0.1309 (0.1364)  time: 0.1954  data: 0.0002  max mem: 5511
[09:12:17.434764] Epoch: [40]  [760/781]  eta: 0:00:04  lr: 0.000172  training_loss: 0.3567 (0.3621)  mae_loss: 0.2262 (0.2256)  classification_loss: 0.1368 (0.1365)  time: 0.1972  data: 0.0002  max mem: 5511
[09:12:21.318505] Epoch: [40]  [780/781]  eta: 0:00:00  lr: 0.000172  training_loss: 0.3616 (0.3619)  mae_loss: 0.2255 (0.2254)  classification_loss: 0.1354 (0.1365)  time: 0.1941  data: 0.0002  max mem: 5511
[09:12:21.490419] Epoch: [40] Total time: 0:02:33 (0.1968 s / it)
[09:12:21.490860] Averaged stats: lr: 0.000172  training_loss: 0.3616 (0.3619)  mae_loss: 0.2255 (0.2254)  classification_loss: 0.1354 (0.1365)
[09:12:23.486028] Test:  [  0/157]  eta: 0:01:48  testing_loss: 0.5761 (0.5761)  acc1: 84.3750 (84.3750)  acc5: 98.4375 (98.4375)  time: 0.6941  data: 0.6611  max mem: 5511
[09:12:23.791409] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.6696 (0.6843)  acc1: 76.5625 (78.1250)  acc5: 98.4375 (98.7216)  time: 0.0907  data: 0.0602  max mem: 5511
[09:12:24.072753] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5971 (0.6461)  acc1: 78.1250 (79.8363)  acc5: 100.0000 (98.9583)  time: 0.0292  data: 0.0002  max mem: 5511
[09:12:24.355757] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6317 (0.6601)  acc1: 81.2500 (79.4859)  acc5: 100.0000 (98.8407)  time: 0.0281  data: 0.0002  max mem: 5511
[09:12:24.637750] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6675 (0.6604)  acc1: 78.1250 (79.3445)  acc5: 98.4375 (98.7805)  time: 0.0281  data: 0.0002  max mem: 5511
[09:12:24.920431] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6378 (0.6536)  acc1: 79.6875 (79.7488)  acc5: 98.4375 (98.8051)  time: 0.0281  data: 0.0002  max mem: 5511
[09:12:25.201885] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6118 (0.6509)  acc1: 79.6875 (79.6875)  acc5: 98.4375 (98.6936)  time: 0.0281  data: 0.0001  max mem: 5511
[09:12:25.483822] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.6070 (0.6423)  acc1: 81.2500 (79.9296)  acc5: 100.0000 (98.8116)  time: 0.0281  data: 0.0001  max mem: 5511
[09:12:25.767584] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.6219 (0.6441)  acc1: 79.6875 (79.8225)  acc5: 100.0000 (98.7847)  time: 0.0282  data: 0.0002  max mem: 5511
[09:12:26.054751] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6601 (0.6419)  acc1: 79.6875 (80.0652)  acc5: 98.4375 (98.7637)  time: 0.0284  data: 0.0002  max mem: 5511
[09:12:26.340811] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.6490 (0.6455)  acc1: 79.6875 (79.8267)  acc5: 98.4375 (98.7778)  time: 0.0285  data: 0.0002  max mem: 5511
[09:12:26.629420] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6402 (0.6456)  acc1: 79.6875 (79.8142)  acc5: 98.4375 (98.7894)  time: 0.0286  data: 0.0002  max mem: 5511
[09:12:26.916640] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6244 (0.6407)  acc1: 79.6875 (79.9845)  acc5: 98.4375 (98.7991)  time: 0.0287  data: 0.0002  max mem: 5511
[09:12:27.203169] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6201 (0.6433)  acc1: 79.6875 (79.9260)  acc5: 100.0000 (98.8192)  time: 0.0286  data: 0.0002  max mem: 5511
[09:12:27.484936] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6347 (0.6420)  acc1: 79.6875 (79.9202)  acc5: 100.0000 (98.8475)  time: 0.0283  data: 0.0002  max mem: 5511
[09:12:27.766048] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6347 (0.6413)  acc1: 79.6875 (79.9772)  acc5: 98.4375 (98.8100)  time: 0.0280  data: 0.0001  max mem: 5511
[09:12:27.916597] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6346 (0.6406)  acc1: 79.6875 (79.9900)  acc5: 98.4375 (98.8100)  time: 0.0271  data: 0.0001  max mem: 5511
[09:12:28.087665] Test: Total time: 0:00:05 (0.0337 s / it)
[09:12:28.088127] * Acc@1 79.990 Acc@5 98.810 loss 0.641
[09:12:28.088420] Accuracy of the network on the 10000 test images: 80.0%
[09:12:28.088605] Max accuracy: 80.23%
[09:12:28.302930] log_dir: ./output_dir
[09:12:29.058573] Epoch: [41]  [  0/781]  eta: 0:09:48  lr: 0.000172  training_loss: 0.3173 (0.3173)  mae_loss: 0.1924 (0.1924)  classification_loss: 0.1249 (0.1249)  time: 0.7540  data: 0.5530  max mem: 5511
[09:12:32.966509] Epoch: [41]  [ 20/781]  eta: 0:02:48  lr: 0.000172  training_loss: 0.3523 (0.3539)  mae_loss: 0.2211 (0.2205)  classification_loss: 0.1321 (0.1334)  time: 0.1953  data: 0.0002  max mem: 5511
[09:12:36.875608] Epoch: [41]  [ 40/781]  eta: 0:02:34  lr: 0.000172  training_loss: 0.3591 (0.3562)  mae_loss: 0.2187 (0.2210)  classification_loss: 0.1369 (0.1352)  time: 0.1954  data: 0.0002  max mem: 5511
[09:12:40.784677] Epoch: [41]  [ 60/781]  eta: 0:02:27  lr: 0.000171  training_loss: 0.3517 (0.3554)  mae_loss: 0.2150 (0.2197)  classification_loss: 0.1340 (0.1357)  time: 0.1954  data: 0.0003  max mem: 5511
[09:12:44.717454] Epoch: [41]  [ 80/781]  eta: 0:02:21  lr: 0.000171  training_loss: 0.3608 (0.3589)  mae_loss: 0.2269 (0.2231)  classification_loss: 0.1362 (0.1358)  time: 0.1966  data: 0.0005  max mem: 5511
[09:12:48.649618] Epoch: [41]  [100/781]  eta: 0:02:17  lr: 0.000171  training_loss: 0.3533 (0.3612)  mae_loss: 0.2210 (0.2252)  classification_loss: 0.1362 (0.1360)  time: 0.1965  data: 0.0004  max mem: 5511
[09:12:52.555098] Epoch: [41]  [120/781]  eta: 0:02:12  lr: 0.000171  training_loss: 0.3540 (0.3607)  mae_loss: 0.2157 (0.2247)  classification_loss: 0.1335 (0.1360)  time: 0.1952  data: 0.0002  max mem: 5511
[09:12:56.470294] Epoch: [41]  [140/781]  eta: 0:02:07  lr: 0.000171  training_loss: 0.3712 (0.3611)  mae_loss: 0.2329 (0.2255)  classification_loss: 0.1335 (0.1356)  time: 0.1957  data: 0.0002  max mem: 5511
[09:13:00.415517] Epoch: [41]  [160/781]  eta: 0:02:03  lr: 0.000171  training_loss: 0.3510 (0.3597)  mae_loss: 0.2166 (0.2243)  classification_loss: 0.1335 (0.1354)  time: 0.1972  data: 0.0003  max mem: 5511
[09:13:04.326329] Epoch: [41]  [180/781]  eta: 0:01:59  lr: 0.000171  training_loss: 0.3432 (0.3582)  mae_loss: 0.2109 (0.2235)  classification_loss: 0.1275 (0.1346)  time: 0.1955  data: 0.0002  max mem: 5511
[09:13:08.222821] Epoch: [41]  [200/781]  eta: 0:01:55  lr: 0.000171  training_loss: 0.3604 (0.3586)  mae_loss: 0.2179 (0.2236)  classification_loss: 0.1365 (0.1350)  time: 0.1948  data: 0.0002  max mem: 5511
[09:13:12.128460] Epoch: [41]  [220/781]  eta: 0:01:51  lr: 0.000171  training_loss: 0.3726 (0.3593)  mae_loss: 0.2289 (0.2241)  classification_loss: 0.1403 (0.1352)  time: 0.1952  data: 0.0002  max mem: 5511
[09:13:16.131574] Epoch: [41]  [240/781]  eta: 0:01:47  lr: 0.000171  training_loss: 0.3398 (0.3586)  mae_loss: 0.2059 (0.2234)  classification_loss: 0.1335 (0.1352)  time: 0.2001  data: 0.0002  max mem: 5511
[09:13:20.062335] Epoch: [41]  [260/781]  eta: 0:01:43  lr: 0.000170  training_loss: 0.3560 (0.3583)  mae_loss: 0.2209 (0.2231)  classification_loss: 0.1346 (0.1352)  time: 0.1964  data: 0.0003  max mem: 5511
[09:13:24.005292] Epoch: [41]  [280/781]  eta: 0:01:39  lr: 0.000170  training_loss: 0.3631 (0.3584)  mae_loss: 0.2242 (0.2232)  classification_loss: 0.1338 (0.1352)  time: 0.1971  data: 0.0002  max mem: 5511
[09:13:27.961981] Epoch: [41]  [300/781]  eta: 0:01:35  lr: 0.000170  training_loss: 0.3467 (0.3580)  mae_loss: 0.2076 (0.2227)  classification_loss: 0.1350 (0.1353)  time: 0.1977  data: 0.0002  max mem: 5511
[09:13:31.902599] Epoch: [41]  [320/781]  eta: 0:01:31  lr: 0.000170  training_loss: 0.3517 (0.3579)  mae_loss: 0.2112 (0.2224)  classification_loss: 0.1354 (0.1355)  time: 0.1969  data: 0.0002  max mem: 5511
[09:13:35.795140] Epoch: [41]  [340/781]  eta: 0:01:27  lr: 0.000170  training_loss: 0.3583 (0.3583)  mae_loss: 0.2197 (0.2227)  classification_loss: 0.1356 (0.1356)  time: 0.1945  data: 0.0002  max mem: 5511
[09:13:39.700380] Epoch: [41]  [360/781]  eta: 0:01:23  lr: 0.000170  training_loss: 0.3616 (0.3584)  mae_loss: 0.2206 (0.2226)  classification_loss: 0.1360 (0.1358)  time: 0.1952  data: 0.0002  max mem: 5511
[09:13:43.607138] Epoch: [41]  [380/781]  eta: 0:01:19  lr: 0.000170  training_loss: 0.3566 (0.3585)  mae_loss: 0.2222 (0.2227)  classification_loss: 0.1367 (0.1357)  time: 0.1952  data: 0.0003  max mem: 5511
[09:13:47.524251] Epoch: [41]  [400/781]  eta: 0:01:15  lr: 0.000170  training_loss: 0.3488 (0.3582)  mae_loss: 0.2133 (0.2225)  classification_loss: 0.1317 (0.1357)  time: 0.1958  data: 0.0002  max mem: 5511
[09:13:51.445436] Epoch: [41]  [420/781]  eta: 0:01:11  lr: 0.000170  training_loss: 0.3568 (0.3583)  mae_loss: 0.2224 (0.2227)  classification_loss: 0.1352 (0.1356)  time: 0.1960  data: 0.0002  max mem: 5511
[09:13:55.366135] Epoch: [41]  [440/781]  eta: 0:01:07  lr: 0.000170  training_loss: 0.3565 (0.3583)  mae_loss: 0.2194 (0.2226)  classification_loss: 0.1360 (0.1357)  time: 0.1960  data: 0.0002  max mem: 5511
[09:13:59.279350] Epoch: [41]  [460/781]  eta: 0:01:03  lr: 0.000169  training_loss: 0.3579 (0.3582)  mae_loss: 0.2275 (0.2227)  classification_loss: 0.1295 (0.1355)  time: 0.1956  data: 0.0002  max mem: 5511
[09:14:03.198217] Epoch: [41]  [480/781]  eta: 0:00:59  lr: 0.000169  training_loss: 0.3494 (0.3578)  mae_loss: 0.2052 (0.2223)  classification_loss: 0.1368 (0.1355)  time: 0.1959  data: 0.0003  max mem: 5511
[09:14:07.144921] Epoch: [41]  [500/781]  eta: 0:00:55  lr: 0.000169  training_loss: 0.3635 (0.3580)  mae_loss: 0.2220 (0.2224)  classification_loss: 0.1361 (0.1356)  time: 0.1973  data: 0.0002  max mem: 5511
[09:14:11.072122] Epoch: [41]  [520/781]  eta: 0:00:51  lr: 0.000169  training_loss: 0.3667 (0.3583)  mae_loss: 0.2301 (0.2226)  classification_loss: 0.1329 (0.1356)  time: 0.1963  data: 0.0002  max mem: 5511
[09:14:14.971071] Epoch: [41]  [540/781]  eta: 0:00:47  lr: 0.000169  training_loss: 0.3562 (0.3585)  mae_loss: 0.2205 (0.2228)  classification_loss: 0.1359 (0.1357)  time: 0.1949  data: 0.0003  max mem: 5511
[09:14:18.902689] Epoch: [41]  [560/781]  eta: 0:00:43  lr: 0.000169  training_loss: 0.3716 (0.3590)  mae_loss: 0.2332 (0.2233)  classification_loss: 0.1372 (0.1357)  time: 0.1965  data: 0.0002  max mem: 5511
[09:14:22.804590] Epoch: [41]  [580/781]  eta: 0:00:39  lr: 0.000169  training_loss: 0.3630 (0.3592)  mae_loss: 0.2291 (0.2235)  classification_loss: 0.1343 (0.1357)  time: 0.1950  data: 0.0002  max mem: 5511
[09:14:26.717215] Epoch: [41]  [600/781]  eta: 0:00:35  lr: 0.000169  training_loss: 0.3467 (0.3593)  mae_loss: 0.2118 (0.2235)  classification_loss: 0.1377 (0.1357)  time: 0.1956  data: 0.0002  max mem: 5511
[09:14:30.596405] Epoch: [41]  [620/781]  eta: 0:00:31  lr: 0.000169  training_loss: 0.3579 (0.3592)  mae_loss: 0.2276 (0.2235)  classification_loss: 0.1359 (0.1357)  time: 0.1939  data: 0.0002  max mem: 5511
[09:14:34.491512] Epoch: [41]  [640/781]  eta: 0:00:27  lr: 0.000169  training_loss: 0.3466 (0.3592)  mae_loss: 0.2116 (0.2236)  classification_loss: 0.1299 (0.1356)  time: 0.1947  data: 0.0002  max mem: 5511
[09:14:38.395550] Epoch: [41]  [660/781]  eta: 0:00:23  lr: 0.000168  training_loss: 0.3677 (0.3595)  mae_loss: 0.2241 (0.2238)  classification_loss: 0.1384 (0.1357)  time: 0.1951  data: 0.0002  max mem: 5511
[09:14:42.309223] Epoch: [41]  [680/781]  eta: 0:00:19  lr: 0.000168  training_loss: 0.3483 (0.3593)  mae_loss: 0.2162 (0.2236)  classification_loss: 0.1340 (0.1357)  time: 0.1956  data: 0.0002  max mem: 5511
[09:14:46.232307] Epoch: [41]  [700/781]  eta: 0:00:15  lr: 0.000168  training_loss: 0.3566 (0.3594)  mae_loss: 0.2231 (0.2237)  classification_loss: 0.1335 (0.1357)  time: 0.1961  data: 0.0003  max mem: 5511
[09:14:50.138914] Epoch: [41]  [720/781]  eta: 0:00:11  lr: 0.000168  training_loss: 0.3744 (0.3595)  mae_loss: 0.2342 (0.2237)  classification_loss: 0.1344 (0.1358)  time: 0.1952  data: 0.0002  max mem: 5511
[09:14:54.069599] Epoch: [41]  [740/781]  eta: 0:00:08  lr: 0.000168  training_loss: 0.3632 (0.3596)  mae_loss: 0.2309 (0.2238)  classification_loss: 0.1337 (0.1357)  time: 0.1965  data: 0.0002  max mem: 5511
[09:14:58.061860] Epoch: [41]  [760/781]  eta: 0:00:04  lr: 0.000168  training_loss: 0.3559 (0.3597)  mae_loss: 0.2187 (0.2239)  classification_loss: 0.1374 (0.1358)  time: 0.1995  data: 0.0002  max mem: 5511
[09:15:01.956520] Epoch: [41]  [780/781]  eta: 0:00:00  lr: 0.000168  training_loss: 0.3741 (0.3600)  mae_loss: 0.2313 (0.2242)  classification_loss: 0.1357 (0.1358)  time: 0.1947  data: 0.0002  max mem: 5511
[09:15:02.108821] Epoch: [41] Total time: 0:02:33 (0.1969 s / it)
[09:15:02.109580] Averaged stats: lr: 0.000168  training_loss: 0.3741 (0.3600)  mae_loss: 0.2313 (0.2242)  classification_loss: 0.1357 (0.1358)
[09:15:02.788078] Test:  [  0/157]  eta: 0:01:45  testing_loss: 0.6156 (0.6156)  acc1: 82.8125 (82.8125)  acc5: 96.8750 (96.8750)  time: 0.6741  data: 0.6447  max mem: 5511
[09:15:03.079548] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.6255 (0.6470)  acc1: 81.2500 (79.9716)  acc5: 100.0000 (99.0057)  time: 0.0875  data: 0.0588  max mem: 5511
[09:15:03.366010] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.6071 (0.6199)  acc1: 81.2500 (80.8036)  acc5: 100.0000 (98.8839)  time: 0.0287  data: 0.0002  max mem: 5511
[09:15:03.655314] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.6272 (0.6393)  acc1: 79.6875 (80.2419)  acc5: 98.4375 (98.8911)  time: 0.0286  data: 0.0002  max mem: 5511
[09:15:03.938060] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.6470 (0.6395)  acc1: 79.6875 (80.1067)  acc5: 100.0000 (98.8948)  time: 0.0285  data: 0.0002  max mem: 5511
[09:15:04.223987] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5937 (0.6346)  acc1: 79.6875 (80.2083)  acc5: 98.4375 (98.8664)  time: 0.0283  data: 0.0002  max mem: 5511
[09:15:04.507086] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5814 (0.6301)  acc1: 79.6875 (80.3023)  acc5: 98.4375 (98.7961)  time: 0.0283  data: 0.0002  max mem: 5511
[09:15:04.791992] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5490 (0.6191)  acc1: 82.8125 (80.7218)  acc5: 100.0000 (98.8776)  time: 0.0282  data: 0.0002  max mem: 5511
[09:15:05.084066] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5539 (0.6238)  acc1: 81.2500 (80.5170)  acc5: 98.4375 (98.8426)  time: 0.0287  data: 0.0002  max mem: 5511
[09:15:05.370586] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.6260 (0.6204)  acc1: 79.6875 (80.7177)  acc5: 100.0000 (98.9354)  time: 0.0288  data: 0.0002  max mem: 5511
[09:15:05.654473] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.6260 (0.6221)  acc1: 81.2500 (80.7085)  acc5: 100.0000 (98.9790)  time: 0.0284  data: 0.0002  max mem: 5511
[09:15:05.940349] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6447 (0.6238)  acc1: 79.6875 (80.6729)  acc5: 98.4375 (98.9302)  time: 0.0284  data: 0.0002  max mem: 5511
[09:15:06.225346] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.6235 (0.6186)  acc1: 81.2500 (80.8239)  acc5: 98.4375 (98.9540)  time: 0.0284  data: 0.0002  max mem: 5511
[09:15:06.510454] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.6221 (0.6201)  acc1: 81.2500 (80.6656)  acc5: 98.4375 (98.9385)  time: 0.0283  data: 0.0002  max mem: 5511
[09:15:06.792830] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6873 (0.6227)  acc1: 79.6875 (80.5297)  acc5: 98.4375 (98.9583)  time: 0.0282  data: 0.0001  max mem: 5511
[09:15:07.072062] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6229 (0.6209)  acc1: 78.1250 (80.4946)  acc5: 100.0000 (98.9549)  time: 0.0279  data: 0.0001  max mem: 5511
[09:15:07.222558] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6132 (0.6217)  acc1: 78.1250 (80.3700)  acc5: 98.4375 (98.9600)  time: 0.0269  data: 0.0001  max mem: 5511
[09:15:07.392526] Test: Total time: 0:00:05 (0.0336 s / it)
[09:15:07.392990] * Acc@1 80.370 Acc@5 98.960 loss 0.622
[09:15:07.393288] Accuracy of the network on the 10000 test images: 80.4%
[09:15:07.393491] Max accuracy: 80.37%
[09:15:07.524670] log_dir: ./output_dir
[09:15:08.329574] Epoch: [42]  [  0/781]  eta: 0:10:26  lr: 0.000168  training_loss: 0.3511 (0.3511)  mae_loss: 0.2225 (0.2225)  classification_loss: 0.1286 (0.1286)  time: 0.8028  data: 0.5814  max mem: 5511
[09:15:12.249847] Epoch: [42]  [ 20/781]  eta: 0:02:51  lr: 0.000168  training_loss: 0.3740 (0.3689)  mae_loss: 0.2310 (0.2330)  classification_loss: 0.1330 (0.1358)  time: 0.1959  data: 0.0002  max mem: 5511
[09:15:16.149946] Epoch: [42]  [ 40/781]  eta: 0:02:35  lr: 0.000168  training_loss: 0.3538 (0.3642)  mae_loss: 0.2193 (0.2276)  classification_loss: 0.1377 (0.1366)  time: 0.1949  data: 0.0003  max mem: 5511
[09:15:20.080881] Epoch: [42]  [ 60/781]  eta: 0:02:28  lr: 0.000168  training_loss: 0.3530 (0.3639)  mae_loss: 0.2221 (0.2271)  classification_loss: 0.1355 (0.1368)  time: 0.1965  data: 0.0003  max mem: 5511
[09:15:24.016207] Epoch: [42]  [ 80/781]  eta: 0:02:22  lr: 0.000167  training_loss: 0.3618 (0.3641)  mae_loss: 0.2267 (0.2280)  classification_loss: 0.1302 (0.1361)  time: 0.1967  data: 0.0002  max mem: 5511
[09:15:27.923633] Epoch: [42]  [100/781]  eta: 0:02:17  lr: 0.000167  training_loss: 0.3810 (0.3669)  mae_loss: 0.2314 (0.2292)  classification_loss: 0.1416 (0.1377)  time: 0.1953  data: 0.0002  max mem: 5511
[09:15:31.824855] Epoch: [42]  [120/781]  eta: 0:02:12  lr: 0.000167  training_loss: 0.3635 (0.3654)  mae_loss: 0.2303 (0.2287)  classification_loss: 0.1336 (0.1367)  time: 0.1950  data: 0.0003  max mem: 5511
[09:15:35.721792] Epoch: [42]  [140/781]  eta: 0:02:08  lr: 0.000167  training_loss: 0.3548 (0.3643)  mae_loss: 0.2211 (0.2279)  classification_loss: 0.1333 (0.1364)  time: 0.1948  data: 0.0002  max mem: 5511
[09:15:39.638242] Epoch: [42]  [160/781]  eta: 0:02:03  lr: 0.000167  training_loss: 0.3527 (0.3623)  mae_loss: 0.2154 (0.2264)  classification_loss: 0.1313 (0.1359)  time: 0.1957  data: 0.0002  max mem: 5511
[09:15:43.563412] Epoch: [42]  [180/781]  eta: 0:01:59  lr: 0.000167  training_loss: 0.3477 (0.3622)  mae_loss: 0.2170 (0.2265)  classification_loss: 0.1308 (0.1357)  time: 0.1962  data: 0.0003  max mem: 5511
[09:15:47.498159] Epoch: [42]  [200/781]  eta: 0:01:55  lr: 0.000167  training_loss: 0.3638 (0.3617)  mae_loss: 0.2276 (0.2264)  classification_loss: 0.1297 (0.1353)  time: 0.1967  data: 0.0002  max mem: 5511
[09:15:51.498693] Epoch: [42]  [220/781]  eta: 0:01:51  lr: 0.000167  training_loss: 0.3545 (0.3612)  mae_loss: 0.2127 (0.2259)  classification_loss: 0.1362 (0.1353)  time: 0.1999  data: 0.0004  max mem: 5511
[09:15:55.412437] Epoch: [42]  [240/781]  eta: 0:01:47  lr: 0.000167  training_loss: 0.3538 (0.3609)  mae_loss: 0.2134 (0.2257)  classification_loss: 0.1325 (0.1352)  time: 0.1954  data: 0.0002  max mem: 5511
[09:15:59.321888] Epoch: [42]  [260/781]  eta: 0:01:43  lr: 0.000167  training_loss: 0.3401 (0.3599)  mae_loss: 0.2063 (0.2248)  classification_loss: 0.1308 (0.1350)  time: 0.1954  data: 0.0002  max mem: 5511
[09:16:03.215027] Epoch: [42]  [280/781]  eta: 0:01:39  lr: 0.000166  training_loss: 0.3598 (0.3596)  mae_loss: 0.2324 (0.2248)  classification_loss: 0.1310 (0.1348)  time: 0.1946  data: 0.0003  max mem: 5511
[09:16:07.097061] Epoch: [42]  [300/781]  eta: 0:01:35  lr: 0.000166  training_loss: 0.3531 (0.3597)  mae_loss: 0.2171 (0.2248)  classification_loss: 0.1357 (0.1350)  time: 0.1940  data: 0.0002  max mem: 5511
[09:16:11.003348] Epoch: [42]  [320/781]  eta: 0:01:31  lr: 0.000166  training_loss: 0.3478 (0.3591)  mae_loss: 0.2090 (0.2241)  classification_loss: 0.1363 (0.1350)  time: 0.1952  data: 0.0002  max mem: 5511
[09:16:14.908501] Epoch: [42]  [340/781]  eta: 0:01:27  lr: 0.000166  training_loss: 0.3548 (0.3591)  mae_loss: 0.2239 (0.2241)  classification_loss: 0.1324 (0.1350)  time: 0.1952  data: 0.0002  max mem: 5511
[09:16:18.815058] Epoch: [42]  [360/781]  eta: 0:01:23  lr: 0.000166  training_loss: 0.3589 (0.3589)  mae_loss: 0.2216 (0.2239)  classification_loss: 0.1344 (0.1350)  time: 0.1953  data: 0.0002  max mem: 5511
[09:16:22.704511] Epoch: [42]  [380/781]  eta: 0:01:19  lr: 0.000166  training_loss: 0.3655 (0.3595)  mae_loss: 0.2230 (0.2243)  classification_loss: 0.1370 (0.1352)  time: 0.1944  data: 0.0003  max mem: 5511
[09:16:26.629298] Epoch: [42]  [400/781]  eta: 0:01:15  lr: 0.000166  training_loss: 0.3424 (0.3592)  mae_loss: 0.2110 (0.2241)  classification_loss: 0.1335 (0.1351)  time: 0.1962  data: 0.0003  max mem: 5511
[09:16:30.559298] Epoch: [42]  [420/781]  eta: 0:01:11  lr: 0.000166  training_loss: 0.3726 (0.3598)  mae_loss: 0.2280 (0.2248)  classification_loss: 0.1313 (0.1350)  time: 0.1964  data: 0.0002  max mem: 5511
[09:16:34.468717] Epoch: [42]  [440/781]  eta: 0:01:07  lr: 0.000166  training_loss: 0.3633 (0.3601)  mae_loss: 0.2225 (0.2250)  classification_loss: 0.1393 (0.1352)  time: 0.1954  data: 0.0002  max mem: 5511
[09:16:38.391161] Epoch: [42]  [460/781]  eta: 0:01:03  lr: 0.000166  training_loss: 0.3457 (0.3597)  mae_loss: 0.2128 (0.2246)  classification_loss: 0.1313 (0.1351)  time: 0.1961  data: 0.0002  max mem: 5511
[09:16:42.301303] Epoch: [42]  [480/781]  eta: 0:00:59  lr: 0.000165  training_loss: 0.3531 (0.3594)  mae_loss: 0.2118 (0.2243)  classification_loss: 0.1389 (0.1351)  time: 0.1954  data: 0.0003  max mem: 5511
[09:16:46.201633] Epoch: [42]  [500/781]  eta: 0:00:55  lr: 0.000165  training_loss: 0.3669 (0.3596)  mae_loss: 0.2350 (0.2245)  classification_loss: 0.1352 (0.1352)  time: 0.1949  data: 0.0002  max mem: 5511
[09:16:50.128381] Epoch: [42]  [520/781]  eta: 0:00:51  lr: 0.000165  training_loss: 0.3609 (0.3598)  mae_loss: 0.2270 (0.2247)  classification_loss: 0.1337 (0.1352)  time: 0.1963  data: 0.0002  max mem: 5511
[09:16:54.025982] Epoch: [42]  [540/781]  eta: 0:00:47  lr: 0.000165  training_loss: 0.3640 (0.3601)  mae_loss: 0.2300 (0.2249)  classification_loss: 0.1356 (0.1352)  time: 0.1948  data: 0.0002  max mem: 5511
[09:16:57.949951] Epoch: [42]  [560/781]  eta: 0:00:43  lr: 0.000165  training_loss: 0.3616 (0.3602)  mae_loss: 0.2262 (0.2251)  classification_loss: 0.1350 (0.1351)  time: 0.1961  data: 0.0002  max mem: 5511
[09:17:01.865941] Epoch: [42]  [580/781]  eta: 0:00:39  lr: 0.000165  training_loss: 0.3495 (0.3600)  mae_loss: 0.2143 (0.2249)  classification_loss: 0.1356 (0.1351)  time: 0.1957  data: 0.0002  max mem: 5511
[09:17:05.785714] Epoch: [42]  [600/781]  eta: 0:00:35  lr: 0.000165  training_loss: 0.3572 (0.3601)  mae_loss: 0.2275 (0.2250)  classification_loss: 0.1327 (0.1350)  time: 0.1959  data: 0.0003  max mem: 5511
[09:17:09.718735] Epoch: [42]  [620/781]  eta: 0:00:31  lr: 0.000165  training_loss: 0.3758 (0.3605)  mae_loss: 0.2359 (0.2254)  classification_loss: 0.1361 (0.1351)  time: 0.1965  data: 0.0003  max mem: 5511
[09:17:13.645521] Epoch: [42]  [640/781]  eta: 0:00:27  lr: 0.000165  training_loss: 0.3542 (0.3604)  mae_loss: 0.2218 (0.2253)  classification_loss: 0.1395 (0.1351)  time: 0.1962  data: 0.0002  max mem: 5511
[09:17:17.544677] Epoch: [42]  [660/781]  eta: 0:00:23  lr: 0.000165  training_loss: 0.3476 (0.3601)  mae_loss: 0.2153 (0.2250)  classification_loss: 0.1339 (0.1351)  time: 0.1947  data: 0.0002  max mem: 5511
[09:17:21.455059] Epoch: [42]  [680/781]  eta: 0:00:19  lr: 0.000164  training_loss: 0.3510 (0.3601)  mae_loss: 0.2248 (0.2250)  classification_loss: 0.1329 (0.1350)  time: 0.1954  data: 0.0002  max mem: 5511
[09:17:25.360699] Epoch: [42]  [700/781]  eta: 0:00:15  lr: 0.000164  training_loss: 0.3705 (0.3603)  mae_loss: 0.2357 (0.2252)  classification_loss: 0.1371 (0.1351)  time: 0.1952  data: 0.0003  max mem: 5511
[09:17:29.267649] Epoch: [42]  [720/781]  eta: 0:00:11  lr: 0.000164  training_loss: 0.3485 (0.3602)  mae_loss: 0.2082 (0.2250)  classification_loss: 0.1382 (0.1352)  time: 0.1953  data: 0.0003  max mem: 5511
[09:17:33.184598] Epoch: [42]  [740/781]  eta: 0:00:08  lr: 0.000164  training_loss: 0.3412 (0.3599)  mae_loss: 0.2129 (0.2248)  classification_loss: 0.1328 (0.1351)  time: 0.1958  data: 0.0002  max mem: 5511
[09:17:37.093839] Epoch: [42]  [760/781]  eta: 0:00:04  lr: 0.000164  training_loss: 0.3560 (0.3600)  mae_loss: 0.2248 (0.2249)  classification_loss: 0.1385 (0.1351)  time: 0.1954  data: 0.0002  max mem: 5511
[09:17:40.998416] Epoch: [42]  [780/781]  eta: 0:00:00  lr: 0.000164  training_loss: 0.3537 (0.3599)  mae_loss: 0.2233 (0.2248)  classification_loss: 0.1327 (0.1351)  time: 0.1952  data: 0.0001  max mem: 5511
[09:17:41.141516] Epoch: [42] Total time: 0:02:33 (0.1967 s / it)
[09:17:41.142417] Averaged stats: lr: 0.000164  training_loss: 0.3537 (0.3599)  mae_loss: 0.2233 (0.2248)  classification_loss: 0.1327 (0.1351)
[09:17:41.699054] Test:  [  0/157]  eta: 0:01:26  testing_loss: 0.6329 (0.6329)  acc1: 81.2500 (81.2500)  acc5: 96.8750 (96.8750)  time: 0.5522  data: 0.5201  max mem: 5511
[09:17:41.988074] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.6335 (0.6425)  acc1: 79.6875 (79.6875)  acc5: 100.0000 (99.2898)  time: 0.0763  data: 0.0475  max mem: 5511
[09:17:42.276225] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.6074 (0.6130)  acc1: 79.6875 (80.5060)  acc5: 100.0000 (99.1071)  time: 0.0287  data: 0.0002  max mem: 5511
[09:17:42.558000] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.6035 (0.6253)  acc1: 79.6875 (80.2419)  acc5: 100.0000 (98.8407)  time: 0.0284  data: 0.0002  max mem: 5511
[09:17:42.840041] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.6221 (0.6309)  acc1: 78.1250 (79.8018)  acc5: 98.4375 (98.8186)  time: 0.0280  data: 0.0002  max mem: 5511
[09:17:43.121855] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6154 (0.6279)  acc1: 79.6875 (80.2083)  acc5: 98.4375 (98.7132)  time: 0.0281  data: 0.0001  max mem: 5511
[09:17:43.404177] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.6154 (0.6220)  acc1: 79.6875 (80.3791)  acc5: 98.4375 (98.7193)  time: 0.0281  data: 0.0002  max mem: 5511
[09:17:43.685684] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5552 (0.6119)  acc1: 81.2500 (80.6998)  acc5: 100.0000 (98.7896)  time: 0.0280  data: 0.0002  max mem: 5511
[09:17:43.968714] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5552 (0.6166)  acc1: 81.2500 (80.5941)  acc5: 100.0000 (98.6883)  time: 0.0281  data: 0.0002  max mem: 5511
[09:17:44.254863] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5847 (0.6131)  acc1: 81.2500 (80.8894)  acc5: 98.4375 (98.7122)  time: 0.0283  data: 0.0002  max mem: 5511
[09:17:44.539851] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5847 (0.6150)  acc1: 81.2500 (80.8787)  acc5: 98.4375 (98.7314)  time: 0.0284  data: 0.0002  max mem: 5511
[09:17:44.821318] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6062 (0.6148)  acc1: 81.2500 (80.8840)  acc5: 100.0000 (98.7894)  time: 0.0282  data: 0.0002  max mem: 5511
[09:17:45.108131] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5952 (0.6104)  acc1: 81.2500 (80.9788)  acc5: 100.0000 (98.7862)  time: 0.0283  data: 0.0002  max mem: 5511
[09:17:45.389807] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5969 (0.6138)  acc1: 79.6875 (80.9041)  acc5: 98.4375 (98.7834)  time: 0.0283  data: 0.0002  max mem: 5511
[09:17:45.671298] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6388 (0.6152)  acc1: 78.1250 (80.7513)  acc5: 100.0000 (98.8254)  time: 0.0280  data: 0.0001  max mem: 5511
[09:17:45.950681] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6085 (0.6129)  acc1: 78.1250 (80.7430)  acc5: 100.0000 (98.8721)  time: 0.0279  data: 0.0001  max mem: 5511
[09:17:46.100853] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5771 (0.6125)  acc1: 81.2500 (80.7100)  acc5: 100.0000 (98.8700)  time: 0.0270  data: 0.0001  max mem: 5511
[09:17:46.238426] Test: Total time: 0:00:05 (0.0324 s / it)
[09:17:46.238893] * Acc@1 80.710 Acc@5 98.870 loss 0.612
[09:17:46.239199] Accuracy of the network on the 10000 test images: 80.7%
[09:17:46.239386] Max accuracy: 80.71%
[09:17:46.544318] log_dir: ./output_dir
[09:17:47.330076] Epoch: [43]  [  0/781]  eta: 0:10:12  lr: 0.000164  training_loss: 0.3659 (0.3659)  mae_loss: 0.2419 (0.2419)  classification_loss: 0.1240 (0.1240)  time: 0.7839  data: 0.5372  max mem: 5511
[09:17:51.230517] Epoch: [43]  [ 20/781]  eta: 0:02:49  lr: 0.000164  training_loss: 0.3614 (0.3597)  mae_loss: 0.2275 (0.2261)  classification_loss: 0.1343 (0.1336)  time: 0.1949  data: 0.0002  max mem: 5511
[09:17:55.138558] Epoch: [43]  [ 40/781]  eta: 0:02:35  lr: 0.000164  training_loss: 0.3701 (0.3618)  mae_loss: 0.2371 (0.2280)  classification_loss: 0.1338 (0.1338)  time: 0.1953  data: 0.0002  max mem: 5511
[09:17:59.097920] Epoch: [43]  [ 60/781]  eta: 0:02:28  lr: 0.000164  training_loss: 0.3698 (0.3644)  mae_loss: 0.2359 (0.2292)  classification_loss: 0.1341 (0.1352)  time: 0.1979  data: 0.0003  max mem: 5511
[09:18:03.000211] Epoch: [43]  [ 80/781]  eta: 0:02:22  lr: 0.000164  training_loss: 0.3549 (0.3622)  mae_loss: 0.2210 (0.2276)  classification_loss: 0.1328 (0.1346)  time: 0.1950  data: 0.0002  max mem: 5511
[09:18:06.912299] Epoch: [43]  [100/781]  eta: 0:02:17  lr: 0.000163  training_loss: 0.3451 (0.3606)  mae_loss: 0.2073 (0.2252)  classification_loss: 0.1393 (0.1354)  time: 0.1955  data: 0.0002  max mem: 5511
[09:18:10.859094] Epoch: [43]  [120/781]  eta: 0:02:12  lr: 0.000163  training_loss: 0.3571 (0.3601)  mae_loss: 0.2254 (0.2249)  classification_loss: 0.1311 (0.1352)  time: 0.1973  data: 0.0002  max mem: 5511
[09:18:14.782354] Epoch: [43]  [140/781]  eta: 0:02:08  lr: 0.000163  training_loss: 0.3612 (0.3599)  mae_loss: 0.2129 (0.2250)  classification_loss: 0.1332 (0.1349)  time: 0.1961  data: 0.0002  max mem: 5511
[09:18:18.702009] Epoch: [43]  [160/781]  eta: 0:02:03  lr: 0.000163  training_loss: 0.3612 (0.3598)  mae_loss: 0.2321 (0.2255)  classification_loss: 0.1301 (0.1343)  time: 0.1959  data: 0.0002  max mem: 5511
[09:18:22.648549] Epoch: [43]  [180/781]  eta: 0:01:59  lr: 0.000163  training_loss: 0.3615 (0.3593)  mae_loss: 0.2189 (0.2253)  classification_loss: 0.1283 (0.1340)  time: 0.1973  data: 0.0003  max mem: 5511
[09:18:26.585709] Epoch: [43]  [200/781]  eta: 0:01:55  lr: 0.000163  training_loss: 0.3550 (0.3588)  mae_loss: 0.2168 (0.2248)  classification_loss: 0.1354 (0.1340)  time: 0.1968  data: 0.0003  max mem: 5511
[09:18:30.482005] Epoch: [43]  [220/781]  eta: 0:01:51  lr: 0.000163  training_loss: 0.3565 (0.3588)  mae_loss: 0.2107 (0.2246)  classification_loss: 0.1398 (0.1342)  time: 0.1947  data: 0.0003  max mem: 5511
[09:18:34.382369] Epoch: [43]  [240/781]  eta: 0:01:47  lr: 0.000163  training_loss: 0.3622 (0.3591)  mae_loss: 0.2291 (0.2249)  classification_loss: 0.1317 (0.1342)  time: 0.1949  data: 0.0003  max mem: 5511
[09:18:38.327787] Epoch: [43]  [260/781]  eta: 0:01:43  lr: 0.000163  training_loss: 0.3523 (0.3591)  mae_loss: 0.2218 (0.2248)  classification_loss: 0.1339 (0.1343)  time: 0.1972  data: 0.0002  max mem: 5511
[09:18:42.281798] Epoch: [43]  [280/781]  eta: 0:01:39  lr: 0.000163  training_loss: 0.3454 (0.3583)  mae_loss: 0.2053 (0.2239)  classification_loss: 0.1320 (0.1343)  time: 0.1976  data: 0.0003  max mem: 5511
[09:18:46.230961] Epoch: [43]  [300/781]  eta: 0:01:35  lr: 0.000162  training_loss: 0.3580 (0.3581)  mae_loss: 0.2091 (0.2236)  classification_loss: 0.1345 (0.1345)  time: 0.1974  data: 0.0003  max mem: 5511
[09:18:50.161904] Epoch: [43]  [320/781]  eta: 0:01:31  lr: 0.000162  training_loss: 0.3487 (0.3577)  mae_loss: 0.2095 (0.2232)  classification_loss: 0.1345 (0.1345)  time: 0.1965  data: 0.0002  max mem: 5511
[09:18:54.082289] Epoch: [43]  [340/781]  eta: 0:01:27  lr: 0.000162  training_loss: 0.3573 (0.3577)  mae_loss: 0.2199 (0.2233)  classification_loss: 0.1330 (0.1344)  time: 0.1959  data: 0.0003  max mem: 5511
[09:18:57.988644] Epoch: [43]  [360/781]  eta: 0:01:23  lr: 0.000162  training_loss: 0.3506 (0.3574)  mae_loss: 0.2172 (0.2230)  classification_loss: 0.1332 (0.1344)  time: 0.1952  data: 0.0002  max mem: 5511
[09:19:01.915777] Epoch: [43]  [380/781]  eta: 0:01:19  lr: 0.000162  training_loss: 0.3516 (0.3578)  mae_loss: 0.2178 (0.2235)  classification_loss: 0.1294 (0.1344)  time: 0.1963  data: 0.0002  max mem: 5511
[09:19:05.821038] Epoch: [43]  [400/781]  eta: 0:01:15  lr: 0.000162  training_loss: 0.3651 (0.3581)  mae_loss: 0.2312 (0.2237)  classification_loss: 0.1347 (0.1344)  time: 0.1952  data: 0.0003  max mem: 5511
[09:19:09.714925] Epoch: [43]  [420/781]  eta: 0:01:11  lr: 0.000162  training_loss: 0.3568 (0.3579)  mae_loss: 0.2226 (0.2236)  classification_loss: 0.1342 (0.1343)  time: 0.1946  data: 0.0002  max mem: 5511
[09:19:13.649474] Epoch: [43]  [440/781]  eta: 0:01:07  lr: 0.000162  training_loss: 0.3556 (0.3578)  mae_loss: 0.2314 (0.2236)  classification_loss: 0.1293 (0.1341)  time: 0.1966  data: 0.0002  max mem: 5511
[09:19:17.542746] Epoch: [43]  [460/781]  eta: 0:01:03  lr: 0.000162  training_loss: 0.3381 (0.3568)  mae_loss: 0.2062 (0.2230)  classification_loss: 0.1257 (0.1338)  time: 0.1946  data: 0.0001  max mem: 5511
[09:19:21.469653] Epoch: [43]  [480/781]  eta: 0:00:59  lr: 0.000162  training_loss: 0.3659 (0.3572)  mae_loss: 0.2298 (0.2233)  classification_loss: 0.1374 (0.1339)  time: 0.1963  data: 0.0002  max mem: 5511
[09:19:25.373539] Epoch: [43]  [500/781]  eta: 0:00:55  lr: 0.000161  training_loss: 0.3513 (0.3571)  mae_loss: 0.2135 (0.2231)  classification_loss: 0.1376 (0.1341)  time: 0.1951  data: 0.0002  max mem: 5511
[09:19:29.285629] Epoch: [43]  [520/781]  eta: 0:00:51  lr: 0.000161  training_loss: 0.3458 (0.3570)  mae_loss: 0.2102 (0.2228)  classification_loss: 0.1393 (0.1342)  time: 0.1955  data: 0.0002  max mem: 5511
[09:19:33.258636] Epoch: [43]  [540/781]  eta: 0:00:47  lr: 0.000161  training_loss: 0.3422 (0.3566)  mae_loss: 0.2092 (0.2224)  classification_loss: 0.1335 (0.1342)  time: 0.1986  data: 0.0002  max mem: 5511
[09:19:37.160396] Epoch: [43]  [560/781]  eta: 0:00:43  lr: 0.000161  training_loss: 0.3467 (0.3563)  mae_loss: 0.2129 (0.2222)  classification_loss: 0.1323 (0.1341)  time: 0.1950  data: 0.0002  max mem: 5511
[09:19:41.060126] Epoch: [43]  [580/781]  eta: 0:00:39  lr: 0.000161  training_loss: 0.3524 (0.3563)  mae_loss: 0.2205 (0.2221)  classification_loss: 0.1319 (0.1342)  time: 0.1949  data: 0.0007  max mem: 5511
[09:19:45.014262] Epoch: [43]  [600/781]  eta: 0:00:35  lr: 0.000161  training_loss: 0.3541 (0.3562)  mae_loss: 0.2169 (0.2219)  classification_loss: 0.1375 (0.1343)  time: 0.1976  data: 0.0002  max mem: 5511
[09:19:48.918005] Epoch: [43]  [620/781]  eta: 0:00:31  lr: 0.000161  training_loss: 0.3564 (0.3562)  mae_loss: 0.2252 (0.2220)  classification_loss: 0.1339 (0.1343)  time: 0.1951  data: 0.0002  max mem: 5511
[09:19:52.817346] Epoch: [43]  [640/781]  eta: 0:00:27  lr: 0.000161  training_loss: 0.3412 (0.3559)  mae_loss: 0.2091 (0.2217)  classification_loss: 0.1305 (0.1342)  time: 0.1949  data: 0.0002  max mem: 5511
[09:19:56.732090] Epoch: [43]  [660/781]  eta: 0:00:23  lr: 0.000161  training_loss: 0.3512 (0.3559)  mae_loss: 0.2167 (0.2217)  classification_loss: 0.1318 (0.1342)  time: 0.1957  data: 0.0003  max mem: 5511
[09:20:00.636678] Epoch: [43]  [680/781]  eta: 0:00:19  lr: 0.000161  training_loss: 0.3468 (0.3557)  mae_loss: 0.2137 (0.2216)  classification_loss: 0.1326 (0.1341)  time: 0.1952  data: 0.0002  max mem: 5511
[09:20:04.547469] Epoch: [43]  [700/781]  eta: 0:00:15  lr: 0.000160  training_loss: 0.3523 (0.3558)  mae_loss: 0.2237 (0.2217)  classification_loss: 0.1333 (0.1341)  time: 0.1955  data: 0.0002  max mem: 5511
[09:20:08.523473] Epoch: [43]  [720/781]  eta: 0:00:12  lr: 0.000160  training_loss: 0.3399 (0.3554)  mae_loss: 0.2096 (0.2214)  classification_loss: 0.1300 (0.1340)  time: 0.1987  data: 0.0002  max mem: 5511
[09:20:12.435606] Epoch: [43]  [740/781]  eta: 0:00:08  lr: 0.000160  training_loss: 0.3496 (0.3554)  mae_loss: 0.2152 (0.2214)  classification_loss: 0.1311 (0.1340)  time: 0.1955  data: 0.0002  max mem: 5511
[09:20:16.333734] Epoch: [43]  [760/781]  eta: 0:00:04  lr: 0.000160  training_loss: 0.3477 (0.3554)  mae_loss: 0.2073 (0.2213)  classification_loss: 0.1381 (0.1341)  time: 0.1948  data: 0.0002  max mem: 5511
[09:20:20.226569] Epoch: [43]  [780/781]  eta: 0:00:00  lr: 0.000160  training_loss: 0.3401 (0.3552)  mae_loss: 0.2099 (0.2212)  classification_loss: 0.1274 (0.1340)  time: 0.1945  data: 0.0002  max mem: 5511
[09:20:20.400143] Epoch: [43] Total time: 0:02:33 (0.1970 s / it)
[09:20:20.400710] Averaged stats: lr: 0.000160  training_loss: 0.3401 (0.3552)  mae_loss: 0.2099 (0.2212)  classification_loss: 0.1274 (0.1340)
[09:20:20.983879] Test:  [  0/157]  eta: 0:01:30  testing_loss: 0.5725 (0.5725)  acc1: 84.3750 (84.3750)  acc5: 100.0000 (100.0000)  time: 0.5782  data: 0.5365  max mem: 5511
[09:20:21.268344] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.6349 (0.6314)  acc1: 79.6875 (79.8295)  acc5: 98.4375 (99.1477)  time: 0.0783  data: 0.0489  max mem: 5511
[09:20:21.548777] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.6237 (0.6025)  acc1: 79.6875 (81.3244)  acc5: 98.4375 (99.1071)  time: 0.0281  data: 0.0001  max mem: 5511
[09:20:21.830350] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.5578 (0.6080)  acc1: 82.8125 (81.3004)  acc5: 98.4375 (98.8911)  time: 0.0280  data: 0.0001  max mem: 5511
[09:20:22.112506] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.5801 (0.6156)  acc1: 81.2500 (80.5640)  acc5: 98.4375 (98.8948)  time: 0.0281  data: 0.0002  max mem: 5511
[09:20:22.394374] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5952 (0.6092)  acc1: 79.6875 (81.0049)  acc5: 98.4375 (98.8664)  time: 0.0281  data: 0.0002  max mem: 5511
[09:20:22.675968] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5957 (0.6092)  acc1: 81.2500 (80.7633)  acc5: 98.4375 (98.7961)  time: 0.0281  data: 0.0001  max mem: 5511
[09:20:22.958244] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5415 (0.6000)  acc1: 81.2500 (81.2720)  acc5: 100.0000 (98.8776)  time: 0.0281  data: 0.0002  max mem: 5511
[09:20:23.239688] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5415 (0.6036)  acc1: 82.8125 (81.0378)  acc5: 100.0000 (98.7847)  time: 0.0281  data: 0.0002  max mem: 5511
[09:20:23.520932] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5977 (0.5997)  acc1: 81.2500 (81.3874)  acc5: 98.4375 (98.7294)  time: 0.0280  data: 0.0001  max mem: 5511
[09:20:23.802132] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5961 (0.6001)  acc1: 82.8125 (81.2655)  acc5: 98.4375 (98.7624)  time: 0.0280  data: 0.0001  max mem: 5511
[09:20:24.083788] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.6020 (0.6022)  acc1: 81.2500 (81.1655)  acc5: 100.0000 (98.7894)  time: 0.0280  data: 0.0001  max mem: 5511
[09:20:24.365823] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5621 (0.5966)  acc1: 81.2500 (81.2500)  acc5: 100.0000 (98.7991)  time: 0.0281  data: 0.0001  max mem: 5511
[09:20:24.646132] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5553 (0.6002)  acc1: 81.2500 (81.0353)  acc5: 98.4375 (98.7715)  time: 0.0280  data: 0.0001  max mem: 5511
[09:20:24.927002] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.6405 (0.6018)  acc1: 79.6875 (81.0505)  acc5: 98.4375 (98.7589)  time: 0.0279  data: 0.0001  max mem: 5511
[09:20:25.207095] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.6221 (0.6001)  acc1: 79.6875 (81.0637)  acc5: 98.4375 (98.7686)  time: 0.0279  data: 0.0001  max mem: 5511
[09:20:25.358525] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.6179 (0.6005)  acc1: 79.6875 (80.9000)  acc5: 100.0000 (98.7800)  time: 0.0270  data: 0.0001  max mem: 5511
[09:20:25.512633] Test: Total time: 0:00:05 (0.0325 s / it)
[09:20:25.513124] * Acc@1 80.900 Acc@5 98.780 loss 0.600
[09:20:25.513453] Accuracy of the network on the 10000 test images: 80.9%
[09:20:25.513682] Max accuracy: 80.90%
[09:20:25.886171] log_dir: ./output_dir
[09:20:26.670009] Epoch: [44]  [  0/781]  eta: 0:10:10  lr: 0.000160  training_loss: 0.3501 (0.3501)  mae_loss: 0.2416 (0.2416)  classification_loss: 0.1085 (0.1085)  time: 0.7819  data: 0.5812  max mem: 5511
[09:20:30.562929] Epoch: [44]  [ 20/781]  eta: 0:02:49  lr: 0.000160  training_loss: 0.3524 (0.3543)  mae_loss: 0.2185 (0.2247)  classification_loss: 0.1285 (0.1296)  time: 0.1946  data: 0.0002  max mem: 5511
[09:20:34.455356] Epoch: [44]  [ 40/781]  eta: 0:02:34  lr: 0.000160  training_loss: 0.3443 (0.3517)  mae_loss: 0.2102 (0.2206)  classification_loss: 0.1332 (0.1311)  time: 0.1946  data: 0.0002  max mem: 5511
[09:20:38.357640] Epoch: [44]  [ 60/781]  eta: 0:02:27  lr: 0.000160  training_loss: 0.3533 (0.3539)  mae_loss: 0.2192 (0.2212)  classification_loss: 0.1359 (0.1326)  time: 0.1950  data: 0.0002  max mem: 5511
[09:20:42.300342] Epoch: [44]  [ 80/781]  eta: 0:02:21  lr: 0.000160  training_loss: 0.3578 (0.3560)  mae_loss: 0.2199 (0.2231)  classification_loss: 0.1344 (0.1329)  time: 0.1971  data: 0.0002  max mem: 5511
[09:20:46.199026] Epoch: [44]  [100/781]  eta: 0:02:16  lr: 0.000160  training_loss: 0.3561 (0.3561)  mae_loss: 0.2229 (0.2234)  classification_loss: 0.1317 (0.1327)  time: 0.1949  data: 0.0002  max mem: 5511
[09:20:50.220260] Epoch: [44]  [120/781]  eta: 0:02:12  lr: 0.000159  training_loss: 0.3400 (0.3548)  mae_loss: 0.2125 (0.2221)  classification_loss: 0.1315 (0.1327)  time: 0.2010  data: 0.0002  max mem: 5511
[09:20:54.169939] Epoch: [44]  [140/781]  eta: 0:02:08  lr: 0.000159  training_loss: 0.3557 (0.3553)  mae_loss: 0.2203 (0.2229)  classification_loss: 0.1312 (0.1324)  time: 0.1974  data: 0.0002  max mem: 5511
[09:20:58.066905] Epoch: [44]  [160/781]  eta: 0:02:04  lr: 0.000159  training_loss: 0.3382 (0.3542)  mae_loss: 0.2056 (0.2217)  classification_loss: 0.1345 (0.1325)  time: 0.1948  data: 0.0002  max mem: 5511
[09:21:02.006908] Epoch: [44]  [180/781]  eta: 0:01:59  lr: 0.000159  training_loss: 0.3419 (0.3530)  mae_loss: 0.2096 (0.2209)  classification_loss: 0.1281 (0.1321)  time: 0.1969  data: 0.0002  max mem: 5511
[09:21:05.904357] Epoch: [44]  [200/781]  eta: 0:01:55  lr: 0.000159  training_loss: 0.3475 (0.3528)  mae_loss: 0.2151 (0.2209)  classification_loss: 0.1301 (0.1319)  time: 0.1948  data: 0.0001  max mem: 5511
[09:21:09.798788] Epoch: [44]  [220/781]  eta: 0:01:51  lr: 0.000159  training_loss: 0.3500 (0.3523)  mae_loss: 0.2054 (0.2202)  classification_loss: 0.1321 (0.1320)  time: 0.1946  data: 0.0002  max mem: 5511
[09:21:13.694046] Epoch: [44]  [240/781]  eta: 0:01:47  lr: 0.000159  training_loss: 0.3600 (0.3521)  mae_loss: 0.2126 (0.2202)  classification_loss: 0.1273 (0.1320)  time: 0.1947  data: 0.0002  max mem: 5511
[09:21:17.592688] Epoch: [44]  [260/781]  eta: 0:01:43  lr: 0.000159  training_loss: 0.3418 (0.3515)  mae_loss: 0.2157 (0.2194)  classification_loss: 0.1351 (0.1321)  time: 0.1949  data: 0.0002  max mem: 5511
[09:21:21.505401] Epoch: [44]  [280/781]  eta: 0:01:39  lr: 0.000159  training_loss: 0.3480 (0.3516)  mae_loss: 0.2124 (0.2196)  classification_loss: 0.1286 (0.1320)  time: 0.1956  data: 0.0003  max mem: 5511
[09:21:25.407833] Epoch: [44]  [300/781]  eta: 0:01:35  lr: 0.000159  training_loss: 0.3309 (0.3507)  mae_loss: 0.1984 (0.2186)  classification_loss: 0.1333 (0.1321)  time: 0.1950  data: 0.0004  max mem: 5511
[09:21:29.327661] Epoch: [44]  [320/781]  eta: 0:01:31  lr: 0.000158  training_loss: 0.3669 (0.3516)  mae_loss: 0.2206 (0.2191)  classification_loss: 0.1364 (0.1325)  time: 0.1959  data: 0.0001  max mem: 5511
[09:21:33.239881] Epoch: [44]  [340/781]  eta: 0:01:27  lr: 0.000158  training_loss: 0.3575 (0.3521)  mae_loss: 0.2233 (0.2197)  classification_loss: 0.1290 (0.1323)  time: 0.1955  data: 0.0002  max mem: 5511
[09:21:37.116944] Epoch: [44]  [360/781]  eta: 0:01:23  lr: 0.000158  training_loss: 0.3647 (0.3523)  mae_loss: 0.2221 (0.2196)  classification_loss: 0.1345 (0.1326)  time: 0.1938  data: 0.0002  max mem: 5511
[09:21:41.004467] Epoch: [44]  [380/781]  eta: 0:01:19  lr: 0.000158  training_loss: 0.3586 (0.3530)  mae_loss: 0.2225 (0.2201)  classification_loss: 0.1342 (0.1328)  time: 0.1943  data: 0.0002  max mem: 5511
[09:21:44.903330] Epoch: [44]  [400/781]  eta: 0:01:15  lr: 0.000158  training_loss: 0.3582 (0.3531)  mae_loss: 0.2173 (0.2201)  classification_loss: 0.1376 (0.1331)  time: 0.1948  data: 0.0002  max mem: 5511
[09:21:48.889338] Epoch: [44]  [420/781]  eta: 0:01:11  lr: 0.000158  training_loss: 0.3582 (0.3530)  mae_loss: 0.2246 (0.2200)  classification_loss: 0.1289 (0.1330)  time: 0.1992  data: 0.0002  max mem: 5511
[09:21:52.814504] Epoch: [44]  [440/781]  eta: 0:01:07  lr: 0.000158  training_loss: 0.3511 (0.3529)  mae_loss: 0.2248 (0.2201)  classification_loss: 0.1295 (0.1328)  time: 0.1962  data: 0.0002  max mem: 5511
[09:21:56.737063] Epoch: [44]  [460/781]  eta: 0:01:03  lr: 0.000158  training_loss: 0.3485 (0.3527)  mae_loss: 0.2205 (0.2200)  classification_loss: 0.1271 (0.1327)  time: 0.1960  data: 0.0002  max mem: 5511
[09:22:00.625994] Epoch: [44]  [480/781]  eta: 0:00:59  lr: 0.000158  training_loss: 0.3547 (0.3526)  mae_loss: 0.2218 (0.2199)  classification_loss: 0.1307 (0.1327)  time: 0.1944  data: 0.0002  max mem: 5511
[09:22:04.531010] Epoch: [44]  [500/781]  eta: 0:00:55  lr: 0.000157  training_loss: 0.3471 (0.3524)  mae_loss: 0.2060 (0.2196)  classification_loss: 0.1330 (0.1328)  time: 0.1952  data: 0.0002  max mem: 5511
[09:22:08.441585] Epoch: [44]  [520/781]  eta: 0:00:51  lr: 0.000157  training_loss: 0.3508 (0.3525)  mae_loss: 0.2277 (0.2197)  classification_loss: 0.1311 (0.1329)  time: 0.1954  data: 0.0002  max mem: 5511
[09:22:12.348340] Epoch: [44]  [540/781]  eta: 0:00:47  lr: 0.000157  training_loss: 0.3565 (0.3527)  mae_loss: 0.2165 (0.2198)  classification_loss: 0.1335 (0.1329)  time: 0.1953  data: 0.0002  max mem: 5511

[09:22:16.271453] Epoch: [44]  [560/781]  eta: 0:00:43  lr: 0.000157  training_loss: 0.3529 (0.3527)  mae_loss: 0.2152 (0.2196)  classification_loss: 0.1358 (0.1331)  time: 0.1961  data: 0.0002  max mem: 5511
[09:22:20.211879] Epoch: [44]  [580/781]  eta: 0:00:39  lr: 0.000157  training_loss: 0.3586 (0.3529)  mae_loss: 0.2313 (0.2199)  classification_loss: 0.1315 (0.1330)  time: 0.1969  data: 0.0003  max mem: 5511
[09:22:24.142470] Epoch: [44]  [600/781]  eta: 0:00:35  lr: 0.000157  training_loss: 0.3557 (0.3531)  mae_loss: 0.2287 (0.2202)  classification_loss: 0.1234 (0.1329)  time: 0.1965  data: 0.0003  max mem: 5511
[09:22:28.067558] Epoch: [44]  [620/781]  eta: 0:00:31  lr: 0.000157  training_loss: 0.3596 (0.3534)  mae_loss: 0.2304 (0.2205)  classification_loss: 0.1304 (0.1328)  time: 0.1962  data: 0.0004  max mem: 5511
[09:22:32.002195] Epoch: [44]  [640/781]  eta: 0:00:27  lr: 0.000157  training_loss: 0.3406 (0.3532)  mae_loss: 0.2106 (0.2203)  classification_loss: 0.1302 (0.1329)  time: 0.1966  data: 0.0002  max mem: 5511
[09:22:35.906451] Epoch: [44]  [660/781]  eta: 0:00:23  lr: 0.000157  training_loss: 0.3478 (0.3532)  mae_loss: 0.2184 (0.2204)  classification_loss: 0.1301 (0.1328)  time: 0.1951  data: 0.0005  max mem: 5511
[09:22:39.818497] Epoch: [44]  [680/781]  eta: 0:00:19  lr: 0.000157  training_loss: 0.3559 (0.3534)  mae_loss: 0.2282 (0.2206)  classification_loss: 0.1327 (0.1328)  time: 0.1955  data: 0.0002  max mem: 5511
[09:22:43.717191] Epoch: [44]  [700/781]  eta: 0:00:15  lr: 0.000156  training_loss: 0.3593 (0.3535)  mae_loss: 0.2161 (0.2207)  classification_loss: 0.1332 (0.1328)  time: 0.1949  data: 0.0002  max mem: 5511
[09:22:47.620435] Epoch: [44]  [720/781]  eta: 0:00:11  lr: 0.000156  training_loss: 0.3394 (0.3533)  mae_loss: 0.2100 (0.2205)  classification_loss: 0.1307 (0.1328)  time: 0.1951  data: 0.0002  max mem: 5511
[09:22:51.532123] Epoch: [44]  [740/781]  eta: 0:00:08  lr: 0.000156  training_loss: 0.3563 (0.3533)  mae_loss: 0.2232 (0.2206)  classification_loss: 0.1267 (0.1328)  time: 0.1955  data: 0.0002  max mem: 5511
[09:22:55.417674] Epoch: [44]  [760/781]  eta: 0:00:04  lr: 0.000156  training_loss: 0.3483 (0.3535)  mae_loss: 0.2130 (0.2206)  classification_loss: 0.1382 (0.1328)  time: 0.1942  data: 0.0002  max mem: 5511
[09:22:59.296582] Epoch: [44]  [780/781]  eta: 0:00:00  lr: 0.000156  training_loss: 0.3657 (0.3537)  mae_loss: 0.2250 (0.2208)  classification_loss: 0.1333 (0.1329)  time: 0.1938  data: 0.0002  max mem: 5511
[09:22:59.425993] Epoch: [44] Total time: 0:02:33 (0.1966 s / it)
[09:22:59.426469] Averaged stats: lr: 0.000156  training_loss: 0.3657 (0.3537)  mae_loss: 0.2250 (0.2208)  classification_loss: 0.1333 (0.1329)
[09:23:00.019193] Test:  [  0/157]  eta: 0:01:32  testing_loss: 0.5762 (0.5762)  acc1: 84.3750 (84.3750)  acc5: 95.3125 (95.3125)  time: 0.5882  data: 0.5590  max mem: 5511
[09:23:00.304561] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.5957 (0.6464)  acc1: 82.8125 (79.5455)  acc5: 100.0000 (99.2898)  time: 0.0792  data: 0.0510  max mem: 5511
[09:23:00.586065] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.5765 (0.6168)  acc1: 82.8125 (80.8036)  acc5: 100.0000 (99.4048)  time: 0.0282  data: 0.0002  max mem: 5511
[09:23:00.867803] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.5778 (0.6190)  acc1: 82.8125 (80.7964)  acc5: 100.0000 (99.2440)  time: 0.0280  data: 0.0002  max mem: 5511
[09:23:01.149438] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.5898 (0.6230)  acc1: 81.2500 (80.3354)  acc5: 98.4375 (99.1616)  time: 0.0281  data: 0.0002  max mem: 5511
[09:23:01.431120] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.6207 (0.6158)  acc1: 79.6875 (80.6066)  acc5: 98.4375 (99.1115)  time: 0.0280  data: 0.0002  max mem: 5511
[09:23:01.716792] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5657 (0.6133)  acc1: 81.2500 (80.6865)  acc5: 98.4375 (99.0523)  time: 0.0283  data: 0.0002  max mem: 5511
[09:23:02.003858] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5369 (0.6027)  acc1: 84.3750 (81.2720)  acc5: 100.0000 (99.0757)  time: 0.0285  data: 0.0002  max mem: 5511
[09:23:02.292263] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5497 (0.6054)  acc1: 84.3750 (81.2500)  acc5: 98.4375 (99.0355)  time: 0.0286  data: 0.0002  max mem: 5511
[09:23:02.574487] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5892 (0.6017)  acc1: 81.2500 (81.3530)  acc5: 100.0000 (99.0556)  time: 0.0284  data: 0.0002  max mem: 5511
[09:23:02.859713] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5892 (0.6043)  acc1: 81.2500 (81.2500)  acc5: 100.0000 (99.0873)  time: 0.0282  data: 0.0002  max mem: 5511
[09:23:03.142719] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5974 (0.6037)  acc1: 79.6875 (81.1655)  acc5: 98.4375 (99.0569)  time: 0.0282  data: 0.0002  max mem: 5511
[09:23:03.432015] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5543 (0.5975)  acc1: 81.2500 (81.4050)  acc5: 98.4375 (99.0702)  time: 0.0285  data: 0.0002  max mem: 5511
[09:23:03.721143] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5443 (0.5988)  acc1: 81.2500 (81.3454)  acc5: 100.0000 (99.1174)  time: 0.0288  data: 0.0002  max mem: 5511
[09:23:04.004515] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5650 (0.5987)  acc1: 81.2500 (81.4162)  acc5: 100.0000 (99.1467)  time: 0.0285  data: 0.0002  max mem: 5511
[09:23:04.284642] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5756 (0.5961)  acc1: 82.8125 (81.5397)  acc5: 100.0000 (99.1308)  time: 0.0281  data: 0.0001  max mem: 5511
[09:23:04.435343] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5565 (0.5944)  acc1: 82.8125 (81.5100)  acc5: 100.0000 (99.1400)  time: 0.0270  data: 0.0001  max mem: 5511
[09:23:04.599483] Test: Total time: 0:00:05 (0.0329 s / it)
[09:23:04.600137] * Acc@1 81.510 Acc@5 99.140 loss 0.594
[09:23:04.600610] Accuracy of the network on the 10000 test images: 81.5%
[09:23:04.600921] Max accuracy: 81.51%
[09:23:04.845062] log_dir: ./output_dir
[09:23:05.784571] Epoch: [45]  [  0/781]  eta: 0:12:12  lr: 0.000156  training_loss: 0.3082 (0.3082)  mae_loss: 0.1826 (0.1826)  classification_loss: 0.1256 (0.1256)  time: 0.9379  data: 0.7284  max mem: 5511
[09:23:09.713459] Epoch: [45]  [ 20/781]  eta: 0:02:56  lr: 0.000156  training_loss: 0.3601 (0.3602)  mae_loss: 0.2258 (0.2272)  classification_loss: 0.1345 (0.1330)  time: 0.1964  data: 0.0002  max mem: 5511
[09:23:13.629222] Epoch: [45]  [ 40/781]  eta: 0:02:38  lr: 0.000156  training_loss: 0.3501 (0.3588)  mae_loss: 0.2213 (0.2269)  classification_loss: 0.1268 (0.1319)  time: 0.1957  data: 0.0002  max mem: 5511
[09:23:17.530604] Epoch: [45]  [ 60/781]  eta: 0:02:29  lr: 0.000156  training_loss: 0.3447 (0.3575)  mae_loss: 0.2059 (0.2248)  classification_loss: 0.1344 (0.1328)  time: 0.1950  data: 0.0003  max mem: 5511
[09:23:21.427848] Epoch: [45]  [ 80/781]  eta: 0:02:23  lr: 0.000156  training_loss: 0.3618 (0.3577)  mae_loss: 0.2303 (0.2254)  classification_loss: 0.1318 (0.1322)  time: 0.1948  data: 0.0002  max mem: 5511
[09:23:25.321005] Epoch: [45]  [100/781]  eta: 0:02:17  lr: 0.000156  training_loss: 0.3478 (0.3567)  mae_loss: 0.2114 (0.2239)  classification_loss: 0.1363 (0.1328)  time: 0.1946  data: 0.0003  max mem: 5511
[09:23:29.227834] Epoch: [45]  [120/781]  eta: 0:02:13  lr: 0.000155  training_loss: 0.3369 (0.3548)  mae_loss: 0.2134 (0.2230)  classification_loss: 0.1255 (0.1318)  time: 0.1953  data: 0.0003  max mem: 5511
[09:23:33.161753] Epoch: [45]  [140/781]  eta: 0:02:08  lr: 0.000155  training_loss: 0.3475 (0.3550)  mae_loss: 0.2225 (0.2234)  classification_loss: 0.1300 (0.1316)  time: 0.1966  data: 0.0002  max mem: 5511
[09:23:37.099714] Epoch: [45]  [160/781]  eta: 0:02:04  lr: 0.000155  training_loss: 0.3513 (0.3547)  mae_loss: 0.2173 (0.2228)  classification_loss: 0.1339 (0.1319)  time: 0.1968  data: 0.0002  max mem: 5511
[09:23:41.035692] Epoch: [45]  [180/781]  eta: 0:02:00  lr: 0.000155  training_loss: 0.3563 (0.3549)  mae_loss: 0.2297 (0.2233)  classification_loss: 0.1288 (0.1316)  time: 0.1967  data: 0.0002  max mem: 5511
[09:23:44.949899] Epoch: [45]  [200/781]  eta: 0:01:55  lr: 0.000155  training_loss: 0.3519 (0.3547)  mae_loss: 0.2260 (0.2235)  classification_loss: 0.1267 (0.1312)  time: 0.1956  data: 0.0002  max mem: 5511
[09:23:48.911294] Epoch: [45]  [220/781]  eta: 0:01:51  lr: 0.000155  training_loss: 0.3457 (0.3543)  mae_loss: 0.2098 (0.2230)  classification_loss: 0.1284 (0.1313)  time: 0.1979  data: 0.0002  max mem: 5511
[09:23:52.852841] Epoch: [45]  [240/781]  eta: 0:01:47  lr: 0.000155  training_loss: 0.3460 (0.3541)  mae_loss: 0.2148 (0.2227)  classification_loss: 0.1270 (0.1314)  time: 0.1970  data: 0.0002  max mem: 5511
[09:23:56.772467] Epoch: [45]  [260/781]  eta: 0:01:43  lr: 0.000155  training_loss: 0.3449 (0.3538)  mae_loss: 0.2147 (0.2223)  classification_loss: 0.1300 (0.1315)  time: 0.1958  data: 0.0002  max mem: 5511
[09:24:00.683620] Epoch: [45]  [280/781]  eta: 0:01:39  lr: 0.000155  training_loss: 0.3483 (0.3536)  mae_loss: 0.2232 (0.2224)  classification_loss: 0.1249 (0.1312)  time: 0.1955  data: 0.0003  max mem: 5511
[09:24:04.626361] Epoch: [45]  [300/781]  eta: 0:01:35  lr: 0.000155  training_loss: 0.3628 (0.3539)  mae_loss: 0.2282 (0.2225)  classification_loss: 0.1328 (0.1314)  time: 0.1971  data: 0.0002  max mem: 5511
[09:24:08.539238] Epoch: [45]  [320/781]  eta: 0:01:31  lr: 0.000154  training_loss: 0.3536 (0.3542)  mae_loss: 0.2293 (0.2227)  classification_loss: 0.1288 (0.1315)  time: 0.1956  data: 0.0002  max mem: 5511
[09:24:12.434273] Epoch: [45]  [340/781]  eta: 0:01:27  lr: 0.000154  training_loss: 0.3496 (0.3544)  mae_loss: 0.2231 (0.2231)  classification_loss: 0.1283 (0.1313)  time: 0.1946  data: 0.0002  max mem: 5511
[09:24:16.390195] Epoch: [45]  [360/781]  eta: 0:01:23  lr: 0.000154  training_loss: 0.3575 (0.3550)  mae_loss: 0.2293 (0.2237)  classification_loss: 0.1302 (0.1313)  time: 0.1977  data: 0.0002  max mem: 5511
[09:24:20.300354] Epoch: [45]  [380/781]  eta: 0:01:19  lr: 0.000154  training_loss: 0.3529 (0.3551)  mae_loss: 0.2216 (0.2238)  classification_loss: 0.1309 (0.1313)  time: 0.1954  data: 0.0002  max mem: 5511
[09:24:24.197869] Epoch: [45]  [400/781]  eta: 0:01:15  lr: 0.000154  training_loss: 0.3543 (0.3550)  mae_loss: 0.2245 (0.2239)  classification_loss: 0.1304 (0.1312)  time: 0.1948  data: 0.0002  max mem: 5511
[09:24:28.106775] Epoch: [45]  [420/781]  eta: 0:01:11  lr: 0.000154  training_loss: 0.3647 (0.3553)  mae_loss: 0.2329 (0.2241)  classification_loss: 0.1304 (0.1311)  time: 0.1954  data: 0.0002  max mem: 5511
[09:24:32.009880] Epoch: [45]  [440/781]  eta: 0:01:07  lr: 0.000154  training_loss: 0.3473 (0.3553)  mae_loss: 0.2155 (0.2241)  classification_loss: 0.1327 (0.1312)  time: 0.1951  data: 0.0003  max mem: 5511
[09:24:35.961136] Epoch: [45]  [460/781]  eta: 0:01:03  lr: 0.000154  training_loss: 0.3398 (0.3548)  mae_loss: 0.2112 (0.2236)  classification_loss: 0.1279 (0.1312)  time: 0.1975  data: 0.0002  max mem: 5511
[09:24:39.899770] Epoch: [45]  [480/781]  eta: 0:00:59  lr: 0.000154  training_loss: 0.3431 (0.3544)  mae_loss: 0.2178 (0.2234)  classification_loss: 0.1251 (0.1310)  time: 0.1968  data: 0.0002  max mem: 5511
[09:24:43.807817] Epoch: [45]  [500/781]  eta: 0:00:55  lr: 0.000154  training_loss: 0.3580 (0.3544)  mae_loss: 0.2253 (0.2234)  classification_loss: 0.1330 (0.1311)  time: 0.1953  data: 0.0002  max mem: 5511
[09:24:47.718501] Epoch: [45]  [520/781]  eta: 0:00:51  lr: 0.000153  training_loss: 0.3595 (0.3547)  mae_loss: 0.2304 (0.2237)  classification_loss: 0.1265 (0.1310)  time: 0.1955  data: 0.0002  max mem: 5511
[09:24:51.636076] Epoch: [45]  [540/781]  eta: 0:00:47  lr: 0.000153  training_loss: 0.3511 (0.3547)  mae_loss: 0.2196 (0.2236)  classification_loss: 0.1300 (0.1311)  time: 0.1958  data: 0.0002  max mem: 5511
[09:24:55.545613] Epoch: [45]  [560/781]  eta: 0:00:43  lr: 0.000153  training_loss: 0.3465 (0.3544)  mae_loss: 0.2134 (0.2234)  classification_loss: 0.1284 (0.1310)  time: 0.1954  data: 0.0002  max mem: 5511
[09:24:59.462931] Epoch: [45]  [580/781]  eta: 0:00:39  lr: 0.000153  training_loss: 0.3297 (0.3542)  mae_loss: 0.2181 (0.2232)  classification_loss: 0.1335 (0.1310)  time: 0.1958  data: 0.0002  max mem: 5511
[09:25:03.382432] Epoch: [45]  [600/781]  eta: 0:00:35  lr: 0.000153  training_loss: 0.3446 (0.3540)  mae_loss: 0.2171 (0.2230)  classification_loss: 0.1310 (0.1310)  time: 0.1959  data: 0.0003  max mem: 5511
[09:25:07.273369] Epoch: [45]  [620/781]  eta: 0:00:31  lr: 0.000153  training_loss: 0.3747 (0.3546)  mae_loss: 0.2412 (0.2235)  classification_loss: 0.1315 (0.1311)  time: 0.1944  data: 0.0003  max mem: 5511
[09:25:11.178123] Epoch: [45]  [640/781]  eta: 0:00:27  lr: 0.000153  training_loss: 0.3434 (0.3543)  mae_loss: 0.2135 (0.2232)  classification_loss: 0.1302 (0.1311)  time: 0.1952  data: 0.0003  max mem: 5511
[09:25:15.087085] Epoch: [45]  [660/781]  eta: 0:00:23  lr: 0.000153  training_loss: 0.3522 (0.3545)  mae_loss: 0.2186 (0.2233)  classification_loss: 0.1351 (0.1312)  time: 0.1954  data: 0.0002  max mem: 5511
[09:25:18.980764] Epoch: [45]  [680/781]  eta: 0:00:19  lr: 0.000153  training_loss: 0.3587 (0.3548)  mae_loss: 0.2233 (0.2235)  classification_loss: 0.1343 (0.1313)  time: 0.1946  data: 0.0002  max mem: 5511
[09:25:22.886589] Epoch: [45]  [700/781]  eta: 0:00:15  lr: 0.000152  training_loss: 0.3480 (0.3547)  mae_loss: 0.2138 (0.2233)  classification_loss: 0.1346 (0.1314)  time: 0.1952  data: 0.0002  max mem: 5511
[09:25:26.869630] Epoch: [45]  [720/781]  eta: 0:00:12  lr: 0.000152  training_loss: 0.3533 (0.3548)  mae_loss: 0.2223 (0.2234)  classification_loss: 0.1297 (0.1314)  time: 0.1991  data: 0.0002  max mem: 5511
[09:25:30.771344] Epoch: [45]  [740/781]  eta: 0:00:08  lr: 0.000152  training_loss: 0.3329 (0.3544)  mae_loss: 0.2063 (0.2230)  classification_loss: 0.1313 (0.1314)  time: 0.1950  data: 0.0002  max mem: 5511
[09:25:34.677383] Epoch: [45]  [760/781]  eta: 0:00:04  lr: 0.000152  training_loss: 0.3442 (0.3543)  mae_loss: 0.2076 (0.2229)  classification_loss: 0.1321 (0.1314)  time: 0.1952  data: 0.0003  max mem: 5511
[09:25:38.583971] Epoch: [45]  [780/781]  eta: 0:00:00  lr: 0.000152  training_loss: 0.3399 (0.3540)  mae_loss: 0.2053 (0.2226)  classification_loss: 0.1321 (0.1314)  time: 0.1953  data: 0.0002  max mem: 5511
[09:25:38.731095] Epoch: [45] Total time: 0:02:33 (0.1970 s / it)
[09:25:38.731511] Averaged stats: lr: 0.000152  training_loss: 0.3399 (0.3540)  mae_loss: 0.2053 (0.2226)  classification_loss: 0.1321 (0.1314)
[09:25:39.412667] Test:  [  0/157]  eta: 0:01:46  testing_loss: 0.5318 (0.5318)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.6772  data: 0.6476  max mem: 5511
[09:25:39.705729] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.6278 (0.6113)  acc1: 81.2500 (81.9602)  acc5: 100.0000 (99.2898)  time: 0.0880  data: 0.0591  max mem: 5511
[09:25:39.986484] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.6066 (0.5790)  acc1: 81.2500 (82.2173)  acc5: 100.0000 (99.2560)  time: 0.0285  data: 0.0002  max mem: 5511
[09:25:40.267705] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5861 (0.5871)  acc1: 81.2500 (82.1069)  acc5: 100.0000 (98.9415)  time: 0.0280  data: 0.0002  max mem: 5511
[09:25:40.548813] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5861 (0.5933)  acc1: 79.6875 (81.4787)  acc5: 98.4375 (98.8948)  time: 0.0280  data: 0.0002  max mem: 5511
[09:25:40.830122] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5605 (0.5837)  acc1: 81.2500 (82.2304)  acc5: 100.0000 (98.9583)  time: 0.0280  data: 0.0002  max mem: 5511
[09:25:41.110979] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5328 (0.5822)  acc1: 84.3750 (82.1721)  acc5: 100.0000 (98.8986)  time: 0.0280  data: 0.0002  max mem: 5511
[09:25:41.391678] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5145 (0.5736)  acc1: 82.8125 (82.4164)  acc5: 98.4375 (98.9217)  time: 0.0280  data: 0.0001  max mem: 5511
[09:25:41.674164] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5157 (0.5774)  acc1: 82.8125 (82.3302)  acc5: 98.4375 (98.9005)  time: 0.0280  data: 0.0002  max mem: 5511
[09:25:41.956513] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5437 (0.5731)  acc1: 82.8125 (82.4519)  acc5: 100.0000 (98.9870)  time: 0.0281  data: 0.0002  max mem: 5511
[09:25:42.238194] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5735 (0.5786)  acc1: 81.2500 (82.1937)  acc5: 100.0000 (99.0408)  time: 0.0281  data: 0.0002  max mem: 5511
[09:25:42.520295] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5785 (0.5772)  acc1: 81.2500 (82.2354)  acc5: 100.0000 (99.0428)  time: 0.0281  data: 0.0002  max mem: 5511
[09:25:42.801783] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5417 (0.5718)  acc1: 82.8125 (82.4251)  acc5: 98.4375 (99.0315)  time: 0.0281  data: 0.0002  max mem: 5511
[09:25:43.082377] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5265 (0.5724)  acc1: 82.8125 (82.3235)  acc5: 100.0000 (99.0339)  time: 0.0280  data: 0.0002  max mem: 5511
[09:25:43.363585] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5647 (0.5737)  acc1: 79.6875 (82.3027)  acc5: 100.0000 (99.0137)  time: 0.0280  data: 0.0001  max mem: 5511
[09:25:43.643794] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5429 (0.5727)  acc1: 84.3750 (82.3262)  acc5: 100.0000 (99.0377)  time: 0.0280  data: 0.0001  max mem: 5511
[09:25:43.794036] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5368 (0.5727)  acc1: 82.8125 (82.2400)  acc5: 100.0000 (99.0700)  time: 0.0270  data: 0.0001  max mem: 5511
[09:25:43.953681] Test: Total time: 0:00:05 (0.0332 s / it)
[09:25:43.954955] * Acc@1 82.240 Acc@5 99.070 loss 0.573
[09:25:43.956775] Accuracy of the network on the 10000 test images: 82.2%
[09:25:43.957037] Max accuracy: 82.24%
[09:25:44.320032] log_dir: ./output_dir
[09:25:45.099944] Epoch: [46]  [  0/781]  eta: 0:10:07  lr: 0.000152  training_loss: 0.3076 (0.3076)  mae_loss: 0.1894 (0.1894)  classification_loss: 0.1181 (0.1181)  time: 0.7778  data: 0.5631  max mem: 5511
[09:25:49.008527] Epoch: [46]  [ 20/781]  eta: 0:02:49  lr: 0.000152  training_loss: 0.3506 (0.3493)  mae_loss: 0.2185 (0.2198)  classification_loss: 0.1286 (0.1294)  time: 0.1953  data: 0.0002  max mem: 5511
[09:25:52.955657] Epoch: [46]  [ 40/781]  eta: 0:02:35  lr: 0.000152  training_loss: 0.3578 (0.3509)  mae_loss: 0.2277 (0.2215)  classification_loss: 0.1274 (0.1294)  time: 0.1973  data: 0.0002  max mem: 5511
[09:25:56.858733] Epoch: [46]  [ 60/781]  eta: 0:02:28  lr: 0.000152  training_loss: 0.3495 (0.3500)  mae_loss: 0.2143 (0.2194)  classification_loss: 0.1353 (0.1307)  time: 0.1951  data: 0.0002  max mem: 5511
[09:26:00.754334] Epoch: [46]  [ 80/781]  eta: 0:02:22  lr: 0.000152  training_loss: 0.3530 (0.3494)  mae_loss: 0.2211 (0.2191)  classification_loss: 0.1263 (0.1302)  time: 0.1947  data: 0.0002  max mem: 5511
[09:26:04.658431] Epoch: [46]  [100/781]  eta: 0:02:17  lr: 0.000152  training_loss: 0.3516 (0.3493)  mae_loss: 0.2216 (0.2188)  classification_loss: 0.1318 (0.1304)  time: 0.1951  data: 0.0002  max mem: 5511
[09:26:08.567796] Epoch: [46]  [120/781]  eta: 0:02:12  lr: 0.000151  training_loss: 0.3502 (0.3499)  mae_loss: 0.2201 (0.2195)  classification_loss: 0.1305 (0.1305)  time: 0.1954  data: 0.0004  max mem: 5511
[09:26:12.485760] Epoch: [46]  [140/781]  eta: 0:02:07  lr: 0.000151  training_loss: 0.3546 (0.3512)  mae_loss: 0.2216 (0.2205)  classification_loss: 0.1332 (0.1307)  time: 0.1958  data: 0.0002  max mem: 5511
[09:26:16.376046] Epoch: [46]  [160/781]  eta: 0:02:03  lr: 0.000151  training_loss: 0.3416 (0.3504)  mae_loss: 0.2131 (0.2199)  classification_loss: 0.1286 (0.1304)  time: 0.1944  data: 0.0002  max mem: 5511
[09:26:20.289179] Epoch: [46]  [180/781]  eta: 0:01:59  lr: 0.000151  training_loss: 0.3538 (0.3510)  mae_loss: 0.2178 (0.2205)  classification_loss: 0.1280 (0.1305)  time: 0.1956  data: 0.0002  max mem: 5511
[09:26:24.210315] Epoch: [46]  [200/781]  eta: 0:01:55  lr: 0.000151  training_loss: 0.3448 (0.3505)  mae_loss: 0.2178 (0.2198)  classification_loss: 0.1300 (0.1306)  time: 0.1959  data: 0.0002  max mem: 5511
[09:26:28.141648] Epoch: [46]  [220/781]  eta: 0:01:51  lr: 0.000151  training_loss: 0.3423 (0.3499)  mae_loss: 0.2130 (0.2193)  classification_loss: 0.1274 (0.1307)  time: 0.1964  data: 0.0002  max mem: 5511
[09:26:32.064336] Epoch: [46]  [240/781]  eta: 0:01:47  lr: 0.000151  training_loss: 0.3550 (0.3505)  mae_loss: 0.2249 (0.2197)  classification_loss: 0.1337 (0.1309)  time: 0.1960  data: 0.0002  max mem: 5511
[09:26:35.966075] Epoch: [46]  [260/781]  eta: 0:01:43  lr: 0.000151  training_loss: 0.3481 (0.3507)  mae_loss: 0.2201 (0.2198)  classification_loss: 0.1317 (0.1309)  time: 0.1950  data: 0.0002  max mem: 5511
[09:26:39.865393] Epoch: [46]  [280/781]  eta: 0:01:38  lr: 0.000151  training_loss: 0.3458 (0.3508)  mae_loss: 0.2182 (0.2200)  classification_loss: 0.1297 (0.1308)  time: 0.1949  data: 0.0002  max mem: 5511
[09:26:43.761964] Epoch: [46]  [300/781]  eta: 0:01:34  lr: 0.000151  training_loss: 0.3569 (0.3515)  mae_loss: 0.2273 (0.2208)  classification_loss: 0.1300 (0.1307)  time: 0.1948  data: 0.0002  max mem: 5511
[09:26:47.675579] Epoch: [46]  [320/781]  eta: 0:01:30  lr: 0.000150  training_loss: 0.3566 (0.3517)  mae_loss: 0.2270 (0.2209)  classification_loss: 0.1305 (0.1308)  time: 0.1956  data: 0.0002  max mem: 5511
[09:26:51.578414] Epoch: [46]  [340/781]  eta: 0:01:26  lr: 0.000150  training_loss: 0.3545 (0.3521)  mae_loss: 0.2244 (0.2214)  classification_loss: 0.1270 (0.1307)  time: 0.1951  data: 0.0002  max mem: 5511
[09:26:55.505401] Epoch: [46]  [360/781]  eta: 0:01:22  lr: 0.000150  training_loss: 0.3475 (0.3521)  mae_loss: 0.2163 (0.2213)  classification_loss: 0.1288 (0.1308)  time: 0.1962  data: 0.0002  max mem: 5511
[09:26:59.475789] Epoch: [46]  [380/781]  eta: 0:01:19  lr: 0.000150  training_loss: 0.3470 (0.3524)  mae_loss: 0.2116 (0.2215)  classification_loss: 0.1304 (0.1309)  time: 0.1984  data: 0.0002  max mem: 5511
[09:27:03.409490] Epoch: [46]  [400/781]  eta: 0:01:15  lr: 0.000150  training_loss: 0.3553 (0.3524)  mae_loss: 0.2250 (0.2215)  classification_loss: 0.1263 (0.1309)  time: 0.1966  data: 0.0002  max mem: 5511
[09:27:07.327114] Epoch: [46]  [420/781]  eta: 0:01:11  lr: 0.000150  training_loss: 0.3523 (0.3523)  mae_loss: 0.2185 (0.2216)  classification_loss: 0.1288 (0.1307)  time: 0.1957  data: 0.0002  max mem: 5511
[09:27:11.299770] Epoch: [46]  [440/781]  eta: 0:01:07  lr: 0.000150  training_loss: 0.3562 (0.3525)  mae_loss: 0.2232 (0.2218)  classification_loss: 0.1314 (0.1306)  time: 0.1986  data: 0.0002  max mem: 5511
[09:27:15.214142] Epoch: [46]  [460/781]  eta: 0:01:03  lr: 0.000150  training_loss: 0.3433 (0.3520)  mae_loss: 0.2127 (0.2215)  classification_loss: 0.1289 (0.1305)  time: 0.1956  data: 0.0002  max mem: 5511
[09:27:19.126042] Epoch: [46]  [480/781]  eta: 0:00:59  lr: 0.000150  training_loss: 0.3552 (0.3520)  mae_loss: 0.2167 (0.2213)  classification_loss: 0.1333 (0.1307)  time: 0.1955  data: 0.0002  max mem: 5511
[09:27:23.050633] Epoch: [46]  [500/781]  eta: 0:00:55  lr: 0.000149  training_loss: 0.3491 (0.3517)  mae_loss: 0.2145 (0.2209)  classification_loss: 0.1302 (0.1308)  time: 0.1962  data: 0.0003  max mem: 5511
[09:27:26.963334] Epoch: [46]  [520/781]  eta: 0:00:51  lr: 0.000149  training_loss: 0.3538 (0.3519)  mae_loss: 0.2291 (0.2212)  classification_loss: 0.1262 (0.1307)  time: 0.1956  data: 0.0002  max mem: 5511
[09:27:30.868154] Epoch: [46]  [540/781]  eta: 0:00:47  lr: 0.000149  training_loss: 0.3560 (0.3521)  mae_loss: 0.2164 (0.2213)  classification_loss: 0.1341 (0.1308)  time: 0.1951  data: 0.0002  max mem: 5511
[09:27:34.774041] Epoch: [46]  [560/781]  eta: 0:00:43  lr: 0.000149  training_loss: 0.3396 (0.3517)  mae_loss: 0.2122 (0.2211)  classification_loss: 0.1265 (0.1306)  time: 0.1952  data: 0.0002  max mem: 5511
[09:27:38.676696] Epoch: [46]  [580/781]  eta: 0:00:39  lr: 0.000149  training_loss: 0.3566 (0.3519)  mae_loss: 0.2226 (0.2212)  classification_loss: 0.1328 (0.1307)  time: 0.1951  data: 0.0002  max mem: 5511
[09:27:42.617168] Epoch: [46]  [600/781]  eta: 0:00:35  lr: 0.000149  training_loss: 0.3363 (0.3517)  mae_loss: 0.2141 (0.2210)  classification_loss: 0.1281 (0.1307)  time: 0.1969  data: 0.0003  max mem: 5511
[09:27:46.532393] Epoch: [46]  [620/781]  eta: 0:00:31  lr: 0.000149  training_loss: 0.3484 (0.3518)  mae_loss: 0.2059 (0.2210)  classification_loss: 0.1316 (0.1307)  time: 0.1957  data: 0.0003  max mem: 5511
[09:27:50.433706] Epoch: [46]  [640/781]  eta: 0:00:27  lr: 0.000149  training_loss: 0.3408 (0.3517)  mae_loss: 0.2124 (0.2208)  classification_loss: 0.1313 (0.1308)  time: 0.1950  data: 0.0006  max mem: 5511
[09:27:54.377947] Epoch: [46]  [660/781]  eta: 0:00:23  lr: 0.000149  training_loss: 0.3458 (0.3518)  mae_loss: 0.2173 (0.2209)  classification_loss: 0.1314 (0.1309)  time: 0.1971  data: 0.0003  max mem: 5511
[09:27:58.277272] Epoch: [46]  [680/781]  eta: 0:00:19  lr: 0.000149  training_loss: 0.3409 (0.3519)  mae_loss: 0.2091 (0.2208)  classification_loss: 0.1317 (0.1310)  time: 0.1949  data: 0.0002  max mem: 5511
[09:28:02.209006] Epoch: [46]  [700/781]  eta: 0:00:15  lr: 0.000148  training_loss: 0.3550 (0.3519)  mae_loss: 0.2147 (0.2208)  classification_loss: 0.1304 (0.1311)  time: 0.1965  data: 0.0002  max mem: 5511
[09:28:06.235290] Epoch: [46]  [720/781]  eta: 0:00:12  lr: 0.000148  training_loss: 0.3601 (0.3521)  mae_loss: 0.2159 (0.2209)  classification_loss: 0.1361 (0.1312)  time: 0.2012  data: 0.0003  max mem: 5511
[09:28:10.160496] Epoch: [46]  [740/781]  eta: 0:00:08  lr: 0.000148  training_loss: 0.3444 (0.3520)  mae_loss: 0.2139 (0.2208)  classification_loss: 0.1340 (0.1312)  time: 0.1961  data: 0.0002  max mem: 5511
[09:28:14.056837] Epoch: [46]  [760/781]  eta: 0:00:04  lr: 0.000148  training_loss: 0.3335 (0.3519)  mae_loss: 0.2092 (0.2206)  classification_loss: 0.1309 (0.1312)  time: 0.1947  data: 0.0002  max mem: 5511
[09:28:17.949602] Epoch: [46]  [780/781]  eta: 0:00:00  lr: 0.000148  training_loss: 0.3507 (0.3518)  mae_loss: 0.2150 (0.2205)  classification_loss: 0.1324 (0.1313)  time: 0.1946  data: 0.0002  max mem: 5511
[09:28:18.081925] Epoch: [46] Total time: 0:02:33 (0.1969 s / it)
[09:28:18.082507] Averaged stats: lr: 0.000148  training_loss: 0.3507 (0.3518)  mae_loss: 0.2150 (0.2205)  classification_loss: 0.1324 (0.1313)
[09:28:18.661472] Test:  [  0/157]  eta: 0:01:30  testing_loss: 0.5660 (0.5660)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.5744  data: 0.5450  max mem: 5511
[09:28:18.951998] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.6327 (0.6251)  acc1: 79.6875 (79.1193)  acc5: 100.0000 (99.7159)  time: 0.0784  data: 0.0497  max mem: 5511
[09:28:19.239986] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.6031 (0.5880)  acc1: 81.2500 (81.7708)  acc5: 100.0000 (99.5536)  time: 0.0287  data: 0.0002  max mem: 5511
[09:28:19.525514] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.5600 (0.5871)  acc1: 82.8125 (82.0565)  acc5: 98.4375 (99.2944)  time: 0.0286  data: 0.0002  max mem: 5511
[09:28:19.811224] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.5795 (0.5938)  acc1: 81.2500 (81.5930)  acc5: 98.4375 (99.2378)  time: 0.0284  data: 0.0002  max mem: 5511
[09:28:20.096104] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5831 (0.5865)  acc1: 82.8125 (82.0159)  acc5: 98.4375 (99.1422)  time: 0.0284  data: 0.0002  max mem: 5511
[09:28:20.379974] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5639 (0.5854)  acc1: 82.8125 (82.0697)  acc5: 98.4375 (99.0266)  time: 0.0283  data: 0.0002  max mem: 5511
[09:28:20.664434] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5226 (0.5768)  acc1: 82.8125 (82.5044)  acc5: 100.0000 (99.0757)  time: 0.0283  data: 0.0002  max mem: 5511
[09:28:20.955617] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5289 (0.5815)  acc1: 82.8125 (82.3688)  acc5: 98.4375 (99.0162)  time: 0.0286  data: 0.0002  max mem: 5511
[09:28:21.238271] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5613 (0.5757)  acc1: 82.8125 (82.6065)  acc5: 98.4375 (99.0728)  time: 0.0285  data: 0.0002  max mem: 5511
[09:28:21.520091] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5613 (0.5779)  acc1: 81.2500 (82.5031)  acc5: 100.0000 (99.0563)  time: 0.0280  data: 0.0002  max mem: 5511
[09:28:21.810399] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5925 (0.5783)  acc1: 81.2500 (82.5450)  acc5: 100.0000 (99.0709)  time: 0.0285  data: 0.0002  max mem: 5511
[09:28:22.096268] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5584 (0.5733)  acc1: 84.3750 (82.7996)  acc5: 100.0000 (99.1090)  time: 0.0287  data: 0.0002  max mem: 5511
[09:28:22.393022] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5236 (0.5744)  acc1: 84.3750 (82.6932)  acc5: 100.0000 (99.1174)  time: 0.0290  data: 0.0002  max mem: 5511
[09:28:22.674068] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5886 (0.5744)  acc1: 82.8125 (82.6684)  acc5: 100.0000 (99.1024)  time: 0.0287  data: 0.0001  max mem: 5511
[09:28:22.954257] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5697 (0.5738)  acc1: 82.8125 (82.6469)  acc5: 100.0000 (99.1204)  time: 0.0279  data: 0.0001  max mem: 5511
[09:28:23.104292] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5674 (0.5727)  acc1: 82.8125 (82.6800)  acc5: 100.0000 (99.1200)  time: 0.0270  data: 0.0001  max mem: 5511
[09:28:23.243334] Test: Total time: 0:00:05 (0.0329 s / it)
[09:28:23.243801] * Acc@1 82.680 Acc@5 99.120 loss 0.573
[09:28:23.244096] Accuracy of the network on the 10000 test images: 82.7%
[09:28:23.244273] Max accuracy: 82.68%
[09:28:23.554238] log_dir: ./output_dir
[09:28:24.371355] Epoch: [47]  [  0/781]  eta: 0:10:36  lr: 0.000148  training_loss: 0.3604 (0.3604)  mae_loss: 0.2270 (0.2270)  classification_loss: 0.1334 (0.1334)  time: 0.8152  data: 0.5568  max mem: 5511
[09:28:28.274875] Epoch: [47]  [ 20/781]  eta: 0:02:50  lr: 0.000148  training_loss: 0.3490 (0.3486)  mae_loss: 0.2141 (0.2157)  classification_loss: 0.1311 (0.1328)  time: 0.1951  data: 0.0002  max mem: 5511
[09:28:32.190234] Epoch: [47]  [ 40/781]  eta: 0:02:35  lr: 0.000148  training_loss: 0.3469 (0.3488)  mae_loss: 0.2144 (0.2164)  classification_loss: 0.1273 (0.1324)  time: 0.1957  data: 0.0002  max mem: 5511
[09:28:36.135547] Epoch: [47]  [ 60/781]  eta: 0:02:28  lr: 0.000148  training_loss: 0.3659 (0.3531)  mae_loss: 0.2274 (0.2200)  classification_loss: 0.1374 (0.1331)  time: 0.1972  data: 0.0002  max mem: 5511
[09:28:40.049076] Epoch: [47]  [ 80/781]  eta: 0:02:22  lr: 0.000148  training_loss: 0.3401 (0.3499)  mae_loss: 0.2116 (0.2184)  classification_loss: 0.1251 (0.1315)  time: 0.1956  data: 0.0003  max mem: 5511
[09:28:43.953326] Epoch: [47]  [100/781]  eta: 0:02:17  lr: 0.000148  training_loss: 0.3426 (0.3506)  mae_loss: 0.2098 (0.2187)  classification_loss: 0.1341 (0.1319)  time: 0.1951  data: 0.0002  max mem: 5511
[09:28:47.897374] Epoch: [47]  [120/781]  eta: 0:02:12  lr: 0.000147  training_loss: 0.3399 (0.3497)  mae_loss: 0.2090 (0.2186)  classification_loss: 0.1275 (0.1311)  time: 0.1971  data: 0.0002  max mem: 5511
[09:28:51.804062] Epoch: [47]  [140/781]  eta: 0:02:08  lr: 0.000147  training_loss: 0.3428 (0.3497)  mae_loss: 0.2216 (0.2192)  classification_loss: 0.1254 (0.1305)  time: 0.1953  data: 0.0002  max mem: 5511
[09:28:55.732368] Epoch: [47]  [160/781]  eta: 0:02:04  lr: 0.000147  training_loss: 0.3473 (0.3488)  mae_loss: 0.2069 (0.2182)  classification_loss: 0.1301 (0.1305)  time: 0.1963  data: 0.0008  max mem: 5511
[09:28:59.641486] Epoch: [47]  [180/781]  eta: 0:01:59  lr: 0.000147  training_loss: 0.3545 (0.3490)  mae_loss: 0.2193 (0.2182)  classification_loss: 0.1283 (0.1308)  time: 0.1954  data: 0.0002  max mem: 5511
[09:29:03.539026] Epoch: [47]  [200/781]  eta: 0:01:55  lr: 0.000147  training_loss: 0.3401 (0.3486)  mae_loss: 0.2123 (0.2179)  classification_loss: 0.1284 (0.1307)  time: 0.1948  data: 0.0002  max mem: 5511
[09:29:07.443249] Epoch: [47]  [220/781]  eta: 0:01:51  lr: 0.000147  training_loss: 0.3420 (0.3488)  mae_loss: 0.2086 (0.2179)  classification_loss: 0.1346 (0.1309)  time: 0.1951  data: 0.0002  max mem: 5511
[09:29:11.340181] Epoch: [47]  [240/781]  eta: 0:01:47  lr: 0.000147  training_loss: 0.3481 (0.3491)  mae_loss: 0.2235 (0.2179)  classification_loss: 0.1292 (0.1312)  time: 0.1946  data: 0.0002  max mem: 5511
[09:29:15.273608] Epoch: [47]  [260/781]  eta: 0:01:43  lr: 0.000147  training_loss: 0.3527 (0.3495)  mae_loss: 0.2257 (0.2184)  classification_loss: 0.1282 (0.1311)  time: 0.1966  data: 0.0002  max mem: 5511
[09:29:19.163758] Epoch: [47]  [280/781]  eta: 0:01:39  lr: 0.000147  training_loss: 0.3587 (0.3501)  mae_loss: 0.2228 (0.2190)  classification_loss: 0.1282 (0.1311)  time: 0.1944  data: 0.0002  max mem: 5511
[09:29:23.046251] Epoch: [47]  [300/781]  eta: 0:01:35  lr: 0.000146  training_loss: 0.3570 (0.3505)  mae_loss: 0.2247 (0.2193)  classification_loss: 0.1330 (0.1312)  time: 0.1940  data: 0.0002  max mem: 5511
[09:29:26.963616] Epoch: [47]  [320/781]  eta: 0:01:31  lr: 0.000146  training_loss: 0.3446 (0.3506)  mae_loss: 0.2253 (0.2198)  classification_loss: 0.1231 (0.1308)  time: 0.1958  data: 0.0002  max mem: 5511
[09:29:30.875398] Epoch: [47]  [340/781]  eta: 0:01:27  lr: 0.000146  training_loss: 0.3464 (0.3504)  mae_loss: 0.2136 (0.2196)  classification_loss: 0.1315 (0.1308)  time: 0.1955  data: 0.0003  max mem: 5511
[09:29:34.784692] Epoch: [47]  [360/781]  eta: 0:01:23  lr: 0.000146  training_loss: 0.3502 (0.3503)  mae_loss: 0.2189 (0.2193)  classification_loss: 0.1331 (0.1309)  time: 0.1954  data: 0.0003  max mem: 5511
[09:29:38.689432] Epoch: [47]  [380/781]  eta: 0:01:19  lr: 0.000146  training_loss: 0.3505 (0.3507)  mae_loss: 0.2221 (0.2198)  classification_loss: 0.1298 (0.1308)  time: 0.1951  data: 0.0002  max mem: 5511
[09:29:42.582933] Epoch: [47]  [400/781]  eta: 0:01:15  lr: 0.000146  training_loss: 0.3451 (0.3506)  mae_loss: 0.2211 (0.2199)  classification_loss: 0.1268 (0.1307)  time: 0.1946  data: 0.0003  max mem: 5511
[09:29:46.474101] Epoch: [47]  [420/781]  eta: 0:01:11  lr: 0.000146  training_loss: 0.3446 (0.3508)  mae_loss: 0.2139 (0.2201)  classification_loss: 0.1315 (0.1307)  time: 0.1945  data: 0.0002  max mem: 5511
[09:29:50.409270] Epoch: [47]  [440/781]  eta: 0:01:07  lr: 0.000146  training_loss: 0.3378 (0.3503)  mae_loss: 0.2141 (0.2198)  classification_loss: 0.1251 (0.1305)  time: 0.1967  data: 0.0002  max mem: 5511
[09:29:54.330316] Epoch: [47]  [460/781]  eta: 0:01:03  lr: 0.000146  training_loss: 0.3415 (0.3502)  mae_loss: 0.2070 (0.2199)  classification_loss: 0.1268 (0.1304)  time: 0.1960  data: 0.0002  max mem: 5511
[09:29:58.259040] Epoch: [47]  [480/781]  eta: 0:00:59  lr: 0.000146  training_loss: 0.3467 (0.3503)  mae_loss: 0.2144 (0.2197)  classification_loss: 0.1350 (0.1305)  time: 0.1964  data: 0.0002  max mem: 5511
[09:30:02.185376] Epoch: [47]  [500/781]  eta: 0:00:55  lr: 0.000145  training_loss: 0.3493 (0.3504)  mae_loss: 0.2143 (0.2198)  classification_loss: 0.1327 (0.1306)  time: 0.1962  data: 0.0002  max mem: 5511
[09:30:06.103690] Epoch: [47]  [520/781]  eta: 0:00:51  lr: 0.000145  training_loss: 0.3476 (0.3505)  mae_loss: 0.2130 (0.2198)  classification_loss: 0.1332 (0.1308)  time: 0.1958  data: 0.0002  max mem: 5511
[09:30:10.010777] Epoch: [47]  [540/781]  eta: 0:00:47  lr: 0.000145  training_loss: 0.3618 (0.3508)  mae_loss: 0.2229 (0.2201)  classification_loss: 0.1278 (0.1308)  time: 0.1953  data: 0.0002  max mem: 5511
[09:30:13.916506] Epoch: [47]  [560/781]  eta: 0:00:43  lr: 0.000145  training_loss: 0.3343 (0.3506)  mae_loss: 0.2067 (0.2198)  classification_loss: 0.1300 (0.1308)  time: 0.1952  data: 0.0002  max mem: 5511
[09:30:17.833221] Epoch: [47]  [580/781]  eta: 0:00:39  lr: 0.000145  training_loss: 0.3488 (0.3506)  mae_loss: 0.2178 (0.2198)  classification_loss: 0.1322 (0.1308)  time: 0.1958  data: 0.0002  max mem: 5511
[09:30:21.745786] Epoch: [47]  [600/781]  eta: 0:00:35  lr: 0.000145  training_loss: 0.3488 (0.3506)  mae_loss: 0.2149 (0.2199)  classification_loss: 0.1275 (0.1307)  time: 0.1956  data: 0.0003  max mem: 5511
[09:30:25.669962] Epoch: [47]  [620/781]  eta: 0:00:31  lr: 0.000145  training_loss: 0.3642 (0.3509)  mae_loss: 0.2314 (0.2203)  classification_loss: 0.1267 (0.1306)  time: 0.1961  data: 0.0003  max mem: 5511
[09:30:29.563760] Epoch: [47]  [640/781]  eta: 0:00:27  lr: 0.000145  training_loss: 0.3340 (0.3506)  mae_loss: 0.2054 (0.2201)  classification_loss: 0.1311 (0.1306)  time: 0.1946  data: 0.0002  max mem: 5511
[09:30:33.496760] Epoch: [47]  [660/781]  eta: 0:00:23  lr: 0.000145  training_loss: 0.3449 (0.3505)  mae_loss: 0.2190 (0.2200)  classification_loss: 0.1284 (0.1305)  time: 0.1966  data: 0.0002  max mem: 5511
[09:30:37.379622] Epoch: [47]  [680/781]  eta: 0:00:19  lr: 0.000144  training_loss: 0.3447 (0.3506)  mae_loss: 0.2174 (0.2200)  classification_loss: 0.1289 (0.1306)  time: 0.1941  data: 0.0002  max mem: 5511
[09:30:41.266910] Epoch: [47]  [700/781]  eta: 0:00:15  lr: 0.000144  training_loss: 0.3553 (0.3508)  mae_loss: 0.2264 (0.2201)  classification_loss: 0.1337 (0.1307)  time: 0.1943  data: 0.0002  max mem: 5511
[09:30:45.161350] Epoch: [47]  [720/781]  eta: 0:00:11  lr: 0.000144  training_loss: 0.3574 (0.3512)  mae_loss: 0.2346 (0.2206)  classification_loss: 0.1254 (0.1306)  time: 0.1946  data: 0.0003  max mem: 5511
[09:30:49.071150] Epoch: [47]  [740/781]  eta: 0:00:08  lr: 0.000144  training_loss: 0.3571 (0.3511)  mae_loss: 0.2172 (0.2206)  classification_loss: 0.1289 (0.1306)  time: 0.1954  data: 0.0002  max mem: 5511
[09:30:52.987810] Epoch: [47]  [760/781]  eta: 0:00:04  lr: 0.000144  training_loss: 0.3379 (0.3509)  mae_loss: 0.2100 (0.2203)  classification_loss: 0.1305 (0.1306)  time: 0.1957  data: 0.0003  max mem: 5511
[09:30:56.937190] Epoch: [47]  [780/781]  eta: 0:00:00  lr: 0.000144  training_loss: 0.3440 (0.3508)  mae_loss: 0.2139 (0.2202)  classification_loss: 0.1294 (0.1306)  time: 0.1974  data: 0.0002  max mem: 5511
[09:30:57.095732] Epoch: [47] Total time: 0:02:33 (0.1966 s / it)
[09:30:57.096358] Averaged stats: lr: 0.000144  training_loss: 0.3440 (0.3508)  mae_loss: 0.2139 (0.2202)  classification_loss: 0.1294 (0.1306)
[09:30:57.780614] Test:  [  0/157]  eta: 0:01:46  testing_loss: 0.5549 (0.5549)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.6792  data: 0.6498  max mem: 5511
[09:30:58.073918] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5919 (0.5994)  acc1: 84.3750 (81.6761)  acc5: 100.0000 (99.0057)  time: 0.0880  data: 0.0594  max mem: 5511
[09:30:58.357245] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5721 (0.5812)  acc1: 81.2500 (82.2173)  acc5: 100.0000 (99.0327)  time: 0.0285  data: 0.0003  max mem: 5511
[09:30:58.638540] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5558 (0.5826)  acc1: 81.2500 (82.0060)  acc5: 98.4375 (98.7903)  time: 0.0281  data: 0.0002  max mem: 5511
[09:30:58.920323] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5654 (0.5847)  acc1: 82.8125 (81.8598)  acc5: 98.4375 (98.7043)  time: 0.0280  data: 0.0002  max mem: 5511
[09:30:59.201310] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5456 (0.5705)  acc1: 82.8125 (82.3529)  acc5: 98.4375 (98.7132)  time: 0.0280  data: 0.0002  max mem: 5511
[09:30:59.482343] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5264 (0.5683)  acc1: 82.8125 (82.4283)  acc5: 100.0000 (98.7449)  time: 0.0280  data: 0.0002  max mem: 5511
[09:30:59.771688] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5246 (0.5608)  acc1: 84.3750 (82.7685)  acc5: 98.4375 (98.7236)  time: 0.0284  data: 0.0002  max mem: 5511
[09:31:00.053885] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5259 (0.5645)  acc1: 82.8125 (82.5810)  acc5: 98.4375 (98.6883)  time: 0.0284  data: 0.0002  max mem: 5511
[09:31:00.336612] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5419 (0.5600)  acc1: 81.2500 (82.8297)  acc5: 100.0000 (98.7637)  time: 0.0281  data: 0.0002  max mem: 5511
[09:31:00.620432] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5528 (0.5628)  acc1: 82.8125 (82.7351)  acc5: 100.0000 (98.8243)  time: 0.0282  data: 0.0002  max mem: 5511
[09:31:00.909333] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5668 (0.5631)  acc1: 82.8125 (82.6858)  acc5: 100.0000 (98.8598)  time: 0.0285  data: 0.0002  max mem: 5511
[09:31:01.191581] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5191 (0.5586)  acc1: 82.8125 (82.7350)  acc5: 100.0000 (98.9153)  time: 0.0284  data: 0.0001  max mem: 5511
[09:31:01.473864] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5025 (0.5600)  acc1: 82.8125 (82.6574)  acc5: 100.0000 (98.9265)  time: 0.0281  data: 0.0002  max mem: 5511
[09:31:01.756229] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5856 (0.5613)  acc1: 81.2500 (82.6241)  acc5: 100.0000 (98.9473)  time: 0.0281  data: 0.0002  max mem: 5511
[09:31:02.034496] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5711 (0.5606)  acc1: 81.2500 (82.6055)  acc5: 100.0000 (98.9859)  time: 0.0279  data: 0.0002  max mem: 5511
[09:31:02.185212] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5544 (0.5593)  acc1: 82.8125 (82.6200)  acc5: 100.0000 (99.0200)  time: 0.0269  data: 0.0001  max mem: 5511
[09:31:02.337706] Test: Total time: 0:00:05 (0.0334 s / it)
[09:31:02.338164] * Acc@1 82.620 Acc@5 99.020 loss 0.559
[09:31:02.338471] Accuracy of the network on the 10000 test images: 82.6%
[09:31:02.338677] Max accuracy: 82.68%
[09:31:02.820436] log_dir: ./output_dir
[09:31:03.587041] Epoch: [48]  [  0/781]  eta: 0:09:57  lr: 0.000144  training_loss: 0.3315 (0.3315)  mae_loss: 0.2067 (0.2067)  classification_loss: 0.1247 (0.1247)  time: 0.7647  data: 0.5345  max mem: 5511
[09:31:07.515141] Epoch: [48]  [ 20/781]  eta: 0:02:49  lr: 0.000144  training_loss: 0.3530 (0.3590)  mae_loss: 0.2341 (0.2306)  classification_loss: 0.1295 (0.1284)  time: 0.1963  data: 0.0003  max mem: 5511
[09:31:11.423838] Epoch: [48]  [ 40/781]  eta: 0:02:35  lr: 0.000144  training_loss: 0.3435 (0.3515)  mae_loss: 0.2102 (0.2218)  classification_loss: 0.1326 (0.1298)  time: 0.1954  data: 0.0002  max mem: 5511
[09:31:15.331105] Epoch: [48]  [ 60/781]  eta: 0:02:27  lr: 0.000144  training_loss: 0.3421 (0.3512)  mae_loss: 0.2151 (0.2206)  classification_loss: 0.1297 (0.1306)  time: 0.1953  data: 0.0002  max mem: 5511
[09:31:19.265145] Epoch: [48]  [ 80/781]  eta: 0:02:22  lr: 0.000144  training_loss: 0.3432 (0.3490)  mae_loss: 0.2079 (0.2185)  classification_loss: 0.1294 (0.1306)  time: 0.1966  data: 0.0002  max mem: 5511
[09:31:23.173667] Epoch: [48]  [100/781]  eta: 0:02:17  lr: 0.000143  training_loss: 0.3426 (0.3497)  mae_loss: 0.2178 (0.2194)  classification_loss: 0.1280 (0.1303)  time: 0.1953  data: 0.0002  max mem: 5511
[09:31:27.082621] Epoch: [48]  [120/781]  eta: 0:02:12  lr: 0.000143  training_loss: 0.3423 (0.3491)  mae_loss: 0.2248 (0.2196)  classification_loss: 0.1235 (0.1294)  time: 0.1953  data: 0.0002  max mem: 5511
[09:31:31.030220] Epoch: [48]  [140/781]  eta: 0:02:08  lr: 0.000143  training_loss: 0.3375 (0.3487)  mae_loss: 0.2143 (0.2197)  classification_loss: 0.1237 (0.1290)  time: 0.1973  data: 0.0002  max mem: 5511
[09:31:34.936716] Epoch: [48]  [160/781]  eta: 0:02:03  lr: 0.000143  training_loss: 0.3364 (0.3483)  mae_loss: 0.2068 (0.2195)  classification_loss: 0.1266 (0.1288)  time: 0.1952  data: 0.0002  max mem: 5511
[09:31:38.872573] Epoch: [48]  [180/781]  eta: 0:01:59  lr: 0.000143  training_loss: 0.3447 (0.3479)  mae_loss: 0.2161 (0.2189)  classification_loss: 0.1298 (0.1290)  time: 0.1967  data: 0.0002  max mem: 5511
[09:31:42.803333] Epoch: [48]  [200/781]  eta: 0:01:55  lr: 0.000143  training_loss: 0.3437 (0.3481)  mae_loss: 0.2132 (0.2192)  classification_loss: 0.1237 (0.1289)  time: 0.1965  data: 0.0002  max mem: 5511
[09:31:46.697531] Epoch: [48]  [220/781]  eta: 0:01:51  lr: 0.000143  training_loss: 0.3436 (0.3471)  mae_loss: 0.1996 (0.2180)  classification_loss: 0.1312 (0.1291)  time: 0.1946  data: 0.0003  max mem: 5511
[09:31:50.594511] Epoch: [48]  [240/781]  eta: 0:01:47  lr: 0.000143  training_loss: 0.3265 (0.3466)  mae_loss: 0.2126 (0.2175)  classification_loss: 0.1246 (0.1290)  time: 0.1948  data: 0.0002  max mem: 5511
[09:31:54.531461] Epoch: [48]  [260/781]  eta: 0:01:43  lr: 0.000143  training_loss: 0.3364 (0.3463)  mae_loss: 0.2085 (0.2173)  classification_loss: 0.1301 (0.1290)  time: 0.1967  data: 0.0002  max mem: 5511
[09:31:58.453763] Epoch: [48]  [280/781]  eta: 0:01:39  lr: 0.000142  training_loss: 0.3616 (0.3470)  mae_loss: 0.2318 (0.2181)  classification_loss: 0.1284 (0.1289)  time: 0.1960  data: 0.0002  max mem: 5511
[09:32:02.362263] Epoch: [48]  [300/781]  eta: 0:01:35  lr: 0.000142  training_loss: 0.3393 (0.3472)  mae_loss: 0.2084 (0.2182)  classification_loss: 0.1298 (0.1290)  time: 0.1953  data: 0.0002  max mem: 5511
[09:32:06.295962] Epoch: [48]  [320/781]  eta: 0:01:31  lr: 0.000142  training_loss: 0.3375 (0.3468)  mae_loss: 0.2101 (0.2177)  classification_loss: 0.1304 (0.1290)  time: 0.1966  data: 0.0002  max mem: 5511
[09:32:10.197419] Epoch: [48]  [340/781]  eta: 0:01:27  lr: 0.000142  training_loss: 0.3511 (0.3470)  mae_loss: 0.2136 (0.2178)  classification_loss: 0.1293 (0.1291)  time: 0.1950  data: 0.0002  max mem: 5511
[09:32:14.136036] Epoch: [48]  [360/781]  eta: 0:01:23  lr: 0.000142  training_loss: 0.3409 (0.3470)  mae_loss: 0.2065 (0.2179)  classification_loss: 0.1287 (0.1292)  time: 0.1968  data: 0.0002  max mem: 5511
[09:32:18.028442] Epoch: [48]  [380/781]  eta: 0:01:19  lr: 0.000142  training_loss: 0.3377 (0.3472)  mae_loss: 0.2151 (0.2181)  classification_loss: 0.1287 (0.1291)  time: 0.1945  data: 0.0003  max mem: 5511
[09:32:21.939579] Epoch: [48]  [400/781]  eta: 0:01:15  lr: 0.000142  training_loss: 0.3415 (0.3471)  mae_loss: 0.2215 (0.2182)  classification_loss: 0.1217 (0.1289)  time: 0.1955  data: 0.0003  max mem: 5511
[09:32:25.846682] Epoch: [48]  [420/781]  eta: 0:01:11  lr: 0.000142  training_loss: 0.3386 (0.3471)  mae_loss: 0.2085 (0.2180)  classification_loss: 0.1345 (0.1290)  time: 0.1951  data: 0.0003  max mem: 5511
[09:32:29.773934] Epoch: [48]  [440/781]  eta: 0:01:07  lr: 0.000142  training_loss: 0.3395 (0.3470)  mae_loss: 0.2198 (0.2180)  classification_loss: 0.1266 (0.1290)  time: 0.1963  data: 0.0003  max mem: 5511
[09:32:33.705383] Epoch: [48]  [460/781]  eta: 0:01:03  lr: 0.000142  training_loss: 0.3465 (0.3469)  mae_loss: 0.2231 (0.2181)  classification_loss: 0.1241 (0.1288)  time: 0.1965  data: 0.0002  max mem: 5511
[09:32:37.610806] Epoch: [48]  [480/781]  eta: 0:00:59  lr: 0.000141  training_loss: 0.3425 (0.3467)  mae_loss: 0.2097 (0.2178)  classification_loss: 0.1305 (0.1289)  time: 0.1952  data: 0.0003  max mem: 5511
[09:32:41.495506] Epoch: [48]  [500/781]  eta: 0:00:55  lr: 0.000141  training_loss: 0.3463 (0.3468)  mae_loss: 0.2151 (0.2179)  classification_loss: 0.1293 (0.1290)  time: 0.1942  data: 0.0002  max mem: 5511
[09:32:45.376511] Epoch: [48]  [520/781]  eta: 0:00:51  lr: 0.000141  training_loss: 0.3585 (0.3474)  mae_loss: 0.2319 (0.2184)  classification_loss: 0.1281 (0.1290)  time: 0.1940  data: 0.0003  max mem: 5511
[09:32:49.269342] Epoch: [48]  [540/781]  eta: 0:00:47  lr: 0.000141  training_loss: 0.3588 (0.3482)  mae_loss: 0.2317 (0.2191)  classification_loss: 0.1263 (0.1290)  time: 0.1946  data: 0.0002  max mem: 5511
[09:32:53.181321] Epoch: [48]  [560/781]  eta: 0:00:43  lr: 0.000141  training_loss: 0.3438 (0.3482)  mae_loss: 0.2158 (0.2193)  classification_loss: 0.1280 (0.1290)  time: 0.1955  data: 0.0003  max mem: 5511
[09:32:57.083251] Epoch: [48]  [580/781]  eta: 0:00:39  lr: 0.000141  training_loss: 0.3630 (0.3483)  mae_loss: 0.2250 (0.2194)  classification_loss: 0.1269 (0.1289)  time: 0.1950  data: 0.0003  max mem: 5511
[09:33:01.038278] Epoch: [48]  [600/781]  eta: 0:00:35  lr: 0.000141  training_loss: 0.3281 (0.3477)  mae_loss: 0.1991 (0.2189)  classification_loss: 0.1243 (0.1289)  time: 0.1976  data: 0.0002  max mem: 5511
[09:33:04.945829] Epoch: [48]  [620/781]  eta: 0:00:31  lr: 0.000141  training_loss: 0.3437 (0.3479)  mae_loss: 0.2159 (0.2190)  classification_loss: 0.1320 (0.1290)  time: 0.1953  data: 0.0002  max mem: 5511
[09:33:08.839958] Epoch: [48]  [640/781]  eta: 0:00:27  lr: 0.000141  training_loss: 0.3373 (0.3478)  mae_loss: 0.2090 (0.2189)  classification_loss: 0.1272 (0.1289)  time: 0.1946  data: 0.0002  max mem: 5511
[09:33:12.739108] Epoch: [48]  [660/781]  eta: 0:00:23  lr: 0.000141  training_loss: 0.3405 (0.3475)  mae_loss: 0.2098 (0.2186)  classification_loss: 0.1301 (0.1289)  time: 0.1949  data: 0.0002  max mem: 5511
[09:33:16.657116] Epoch: [48]  [680/781]  eta: 0:00:19  lr: 0.000140  training_loss: 0.3507 (0.3474)  mae_loss: 0.2200 (0.2185)  classification_loss: 0.1267 (0.1289)  time: 0.1958  data: 0.0002  max mem: 5511
[09:33:20.634884] Epoch: [48]  [700/781]  eta: 0:00:15  lr: 0.000140  training_loss: 0.3375 (0.3474)  mae_loss: 0.2135 (0.2185)  classification_loss: 0.1291 (0.1289)  time: 0.1988  data: 0.0003  max mem: 5511
[09:33:24.575829] Epoch: [48]  [720/781]  eta: 0:00:11  lr: 0.000140  training_loss: 0.3414 (0.3476)  mae_loss: 0.2187 (0.2186)  classification_loss: 0.1336 (0.1290)  time: 0.1969  data: 0.0002  max mem: 5511
[09:33:28.477690] Epoch: [48]  [740/781]  eta: 0:00:08  lr: 0.000140  training_loss: 0.3410 (0.3475)  mae_loss: 0.2144 (0.2185)  classification_loss: 0.1260 (0.1290)  time: 0.1950  data: 0.0002  max mem: 5511
[09:33:32.373276] Epoch: [48]  [760/781]  eta: 0:00:04  lr: 0.000140  training_loss: 0.3360 (0.3473)  mae_loss: 0.2057 (0.2183)  classification_loss: 0.1325 (0.1291)  time: 0.1947  data: 0.0003  max mem: 5511
[09:33:36.253704] Epoch: [48]  [780/781]  eta: 0:00:00  lr: 0.000140  training_loss: 0.3379 (0.3472)  mae_loss: 0.2125 (0.2181)  classification_loss: 0.1345 (0.1292)  time: 0.1939  data: 0.0002  max mem: 5511
[09:33:36.403797] Epoch: [48] Total time: 0:02:33 (0.1966 s / it)
[09:33:36.404287] Averaged stats: lr: 0.000140  training_loss: 0.3379 (0.3472)  mae_loss: 0.2125 (0.2181)  classification_loss: 0.1345 (0.1292)
[09:33:37.124049] Test:  [  0/157]  eta: 0:01:52  testing_loss: 0.5545 (0.5545)  acc1: 85.9375 (85.9375)  acc5: 100.0000 (100.0000)  time: 0.7157  data: 0.6839  max mem: 5511
[09:33:37.412208] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.5892 (0.5910)  acc1: 81.2500 (81.2500)  acc5: 100.0000 (99.4318)  time: 0.0911  data: 0.0623  max mem: 5511
[09:33:37.695239] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5733 (0.5550)  acc1: 82.8125 (82.5893)  acc5: 100.0000 (99.5536)  time: 0.0284  data: 0.0002  max mem: 5511
[09:33:37.978367] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5494 (0.5627)  acc1: 82.8125 (82.3589)  acc5: 100.0000 (99.3448)  time: 0.0282  data: 0.0002  max mem: 5511
[09:33:38.261619] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5406 (0.5636)  acc1: 82.8125 (82.1646)  acc5: 100.0000 (99.3140)  time: 0.0282  data: 0.0002  max mem: 5511
[09:33:38.543060] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5238 (0.5533)  acc1: 82.8125 (82.8431)  acc5: 100.0000 (99.2647)  time: 0.0281  data: 0.0002  max mem: 5511
[09:33:38.825022] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5120 (0.5485)  acc1: 84.3750 (83.1199)  acc5: 100.0000 (99.2059)  time: 0.0281  data: 0.0002  max mem: 5511
[09:33:39.108363] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4773 (0.5407)  acc1: 85.9375 (83.3847)  acc5: 100.0000 (99.1857)  time: 0.0281  data: 0.0001  max mem: 5511
[09:33:39.393327] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4974 (0.5442)  acc1: 84.3750 (83.2562)  acc5: 100.0000 (99.1898)  time: 0.0283  data: 0.0002  max mem: 5511
[09:33:39.690431] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5008 (0.5394)  acc1: 82.8125 (83.4135)  acc5: 100.0000 (99.1930)  time: 0.0290  data: 0.0006  max mem: 5511
[09:33:39.980494] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.5073 (0.5417)  acc1: 82.8125 (83.1993)  acc5: 100.0000 (99.2110)  time: 0.0292  data: 0.0006  max mem: 5511
[09:33:40.264222] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5267 (0.5401)  acc1: 82.8125 (83.2770)  acc5: 100.0000 (99.2258)  time: 0.0285  data: 0.0002  max mem: 5511
[09:33:40.546984] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5004 (0.5360)  acc1: 84.3750 (83.4065)  acc5: 100.0000 (99.2381)  time: 0.0282  data: 0.0002  max mem: 5511
[09:33:40.831699] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5004 (0.5377)  acc1: 84.3750 (83.3850)  acc5: 100.0000 (99.2366)  time: 0.0282  data: 0.0002  max mem: 5511
[09:33:41.116108] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5455 (0.5386)  acc1: 84.3750 (83.4331)  acc5: 100.0000 (99.2575)  time: 0.0283  data: 0.0002  max mem: 5511
[09:33:41.395891] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5455 (0.5379)  acc1: 82.8125 (83.4437)  acc5: 100.0000 (99.2860)  time: 0.0281  data: 0.0001  max mem: 5511
[09:33:41.545997] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5348 (0.5370)  acc1: 84.3750 (83.5100)  acc5: 100.0000 (99.2900)  time: 0.0269  data: 0.0001  max mem: 5511
[09:33:41.713016] Test: Total time: 0:00:05 (0.0338 s / it)
[09:33:41.713465] * Acc@1 83.510 Acc@5 99.290 loss 0.537
[09:33:41.713797] Accuracy of the network on the 10000 test images: 83.5%
[09:33:41.714026] Max accuracy: 83.51%
[09:33:42.038889] log_dir: ./output_dir
[09:33:42.950499] Epoch: [49]  [  0/781]  eta: 0:11:50  lr: 0.000140  training_loss: 0.3340 (0.3340)  mae_loss: 0.2140 (0.2140)  classification_loss: 0.1200 (0.1200)  time: 0.9099  data: 0.6897  max mem: 5511
[09:33:46.882762] Epoch: [49]  [ 20/781]  eta: 0:02:55  lr: 0.000140  training_loss: 0.3390 (0.3473)  mae_loss: 0.2069 (0.2191)  classification_loss: 0.1300 (0.1282)  time: 0.1965  data: 0.0002  max mem: 5511
[09:33:50.780476] Epoch: [49]  [ 40/781]  eta: 0:02:37  lr: 0.000140  training_loss: 0.3480 (0.3501)  mae_loss: 0.2181 (0.2207)  classification_loss: 0.1315 (0.1295)  time: 0.1948  data: 0.0002  max mem: 5511
[09:33:54.700933] Epoch: [49]  [ 60/781]  eta: 0:02:29  lr: 0.000140  training_loss: 0.3653 (0.3547)  mae_loss: 0.2281 (0.2252)  classification_loss: 0.1289 (0.1294)  time: 0.1959  data: 0.0002  max mem: 5511
[09:33:58.603275] Epoch: [49]  [ 80/781]  eta: 0:02:23  lr: 0.000139  training_loss: 0.3577 (0.3570)  mae_loss: 0.2349 (0.2277)  classification_loss: 0.1275 (0.1293)  time: 0.1950  data: 0.0002  max mem: 5511
[09:34:02.529360] Epoch: [49]  [100/781]  eta: 0:02:18  lr: 0.000139  training_loss: 0.3450 (0.3550)  mae_loss: 0.2159 (0.2263)  classification_loss: 0.1264 (0.1287)  time: 0.1962  data: 0.0002  max mem: 5511
[09:34:06.425313] Epoch: [49]  [120/781]  eta: 0:02:13  lr: 0.000139  training_loss: 0.3456 (0.3541)  mae_loss: 0.2186 (0.2256)  classification_loss: 0.1263 (0.1285)  time: 0.1947  data: 0.0002  max mem: 5511
[09:34:10.335214] Epoch: [49]  [140/781]  eta: 0:02:08  lr: 0.000139  training_loss: 0.3370 (0.3528)  mae_loss: 0.2139 (0.2243)  classification_loss: 0.1277 (0.1285)  time: 0.1954  data: 0.0002  max mem: 5511
[09:34:14.257976] Epoch: [49]  [160/781]  eta: 0:02:04  lr: 0.000139  training_loss: 0.3358 (0.3513)  mae_loss: 0.2129 (0.2230)  classification_loss: 0.1266 (0.1282)  time: 0.1960  data: 0.0002  max mem: 5511
[09:34:18.167005] Epoch: [49]  [180/781]  eta: 0:01:59  lr: 0.000139  training_loss: 0.3427 (0.3510)  mae_loss: 0.2159 (0.2229)  classification_loss: 0.1277 (0.1281)  time: 0.1954  data: 0.0003  max mem: 5511
[09:34:22.115465] Epoch: [49]  [200/781]  eta: 0:01:55  lr: 0.000139  training_loss: 0.3341 (0.3499)  mae_loss: 0.2087 (0.2216)  classification_loss: 0.1262 (0.1283)  time: 0.1973  data: 0.0002  max mem: 5511
[09:34:26.043277] Epoch: [49]  [220/781]  eta: 0:01:51  lr: 0.000139  training_loss: 0.3357 (0.3496)  mae_loss: 0.2110 (0.2211)  classification_loss: 0.1230 (0.1285)  time: 0.1963  data: 0.0002  max mem: 5511
[09:34:29.971178] Epoch: [49]  [240/781]  eta: 0:01:47  lr: 0.000139  training_loss: 0.3513 (0.3497)  mae_loss: 0.2215 (0.2210)  classification_loss: 0.1295 (0.1287)  time: 0.1963  data: 0.0002  max mem: 5511
[09:34:33.877194] Epoch: [49]  [260/781]  eta: 0:01:43  lr: 0.000139  training_loss: 0.3420 (0.3491)  mae_loss: 0.2130 (0.2204)  classification_loss: 0.1261 (0.1288)  time: 0.1952  data: 0.0002  max mem: 5511
[09:34:37.800557] Epoch: [49]  [280/781]  eta: 0:01:39  lr: 0.000138  training_loss: 0.3436 (0.3491)  mae_loss: 0.2111 (0.2201)  classification_loss: 0.1265 (0.1289)  time: 0.1961  data: 0.0002  max mem: 5511
[09:34:41.713518] Epoch: [49]  [300/781]  eta: 0:01:35  lr: 0.000138  training_loss: 0.3546 (0.3496)  mae_loss: 0.2221 (0.2205)  classification_loss: 0.1256 (0.1290)  time: 0.1956  data: 0.0003  max mem: 5511
[09:34:45.621460] Epoch: [49]  [320/781]  eta: 0:01:31  lr: 0.000138  training_loss: 0.3371 (0.3490)  mae_loss: 0.2142 (0.2201)  classification_loss: 0.1267 (0.1289)  time: 0.1953  data: 0.0001  max mem: 5511
[09:34:49.523035] Epoch: [49]  [340/781]  eta: 0:01:27  lr: 0.000138  training_loss: 0.3373 (0.3485)  mae_loss: 0.2087 (0.2199)  classification_loss: 0.1237 (0.1286)  time: 0.1950  data: 0.0002  max mem: 5511
[09:34:53.430471] Epoch: [49]  [360/781]  eta: 0:01:23  lr: 0.000138  training_loss: 0.3336 (0.3482)  mae_loss: 0.2088 (0.2196)  classification_loss: 0.1273 (0.1286)  time: 0.1952  data: 0.0002  max mem: 5511
[09:34:57.330402] Epoch: [49]  [380/781]  eta: 0:01:19  lr: 0.000138  training_loss: 0.3506 (0.3484)  mae_loss: 0.2184 (0.2199)  classification_loss: 0.1251 (0.1285)  time: 0.1949  data: 0.0004  max mem: 5511
[09:35:01.228498] Epoch: [49]  [400/781]  eta: 0:01:15  lr: 0.000138  training_loss: 0.3415 (0.3481)  mae_loss: 0.2251 (0.2197)  classification_loss: 0.1282 (0.1284)  time: 0.1948  data: 0.0002  max mem: 5511
[09:35:05.129930] Epoch: [49]  [420/781]  eta: 0:01:11  lr: 0.000138  training_loss: 0.3468 (0.3481)  mae_loss: 0.2237 (0.2198)  classification_loss: 0.1257 (0.1283)  time: 0.1950  data: 0.0002  max mem: 5511
[09:35:09.049541] Epoch: [49]  [440/781]  eta: 0:01:07  lr: 0.000138  training_loss: 0.3345 (0.3477)  mae_loss: 0.2105 (0.2194)  classification_loss: 0.1236 (0.1283)  time: 0.1959  data: 0.0003  max mem: 5511
[09:35:12.947449] Epoch: [49]  [460/781]  eta: 0:01:03  lr: 0.000137  training_loss: 0.3387 (0.3476)  mae_loss: 0.2154 (0.2195)  classification_loss: 0.1203 (0.1281)  time: 0.1948  data: 0.0002  max mem: 5511
[09:35:16.879263] Epoch: [49]  [480/781]  eta: 0:00:59  lr: 0.000137  training_loss: 0.3427 (0.3474)  mae_loss: 0.2147 (0.2192)  classification_loss: 0.1296 (0.1282)  time: 0.1965  data: 0.0003  max mem: 5511
[09:35:20.778694] Epoch: [49]  [500/781]  eta: 0:00:55  lr: 0.000137  training_loss: 0.3471 (0.3476)  mae_loss: 0.2171 (0.2194)  classification_loss: 0.1230 (0.1282)  time: 0.1949  data: 0.0002  max mem: 5511
[09:35:24.683021] Epoch: [49]  [520/781]  eta: 0:00:51  lr: 0.000137  training_loss: 0.3193 (0.3469)  mae_loss: 0.1928 (0.2187)  classification_loss: 0.1266 (0.1282)  time: 0.1951  data: 0.0002  max mem: 5511
[09:35:28.662372] Epoch: [49]  [540/781]  eta: 0:00:47  lr: 0.000137  training_loss: 0.3447 (0.3466)  mae_loss: 0.2072 (0.2183)  classification_loss: 0.1279 (0.1283)  time: 0.1989  data: 0.0003  max mem: 5511
[09:35:32.565251] Epoch: [49]  [560/781]  eta: 0:00:43  lr: 0.000137  training_loss: 0.3430 (0.3468)  mae_loss: 0.2208 (0.2185)  classification_loss: 0.1248 (0.1283)  time: 0.1950  data: 0.0005  max mem: 5511
[09:35:36.460464] Epoch: [49]  [580/781]  eta: 0:00:39  lr: 0.000137  training_loss: 0.3402 (0.3466)  mae_loss: 0.2141 (0.2184)  classification_loss: 0.1248 (0.1282)  time: 0.1947  data: 0.0003  max mem: 5511
[09:35:40.395853] Epoch: [49]  [600/781]  eta: 0:00:35  lr: 0.000137  training_loss: 0.3464 (0.3467)  mae_loss: 0.2129 (0.2185)  classification_loss: 0.1257 (0.1281)  time: 0.1967  data: 0.0002  max mem: 5511
[09:35:44.302032] Epoch: [49]  [620/781]  eta: 0:00:31  lr: 0.000137  training_loss: 0.3551 (0.3469)  mae_loss: 0.2231 (0.2189)  classification_loss: 0.1215 (0.1280)  time: 0.1952  data: 0.0002  max mem: 5511
[09:35:48.201865] Epoch: [49]  [640/781]  eta: 0:00:27  lr: 0.000137  training_loss: 0.3473 (0.3471)  mae_loss: 0.2187 (0.2190)  classification_loss: 0.1292 (0.1281)  time: 0.1949  data: 0.0003  max mem: 5511
[09:35:52.114373] Epoch: [49]  [660/781]  eta: 0:00:23  lr: 0.000136  training_loss: 0.3485 (0.3473)  mae_loss: 0.2094 (0.2191)  classification_loss: 0.1293 (0.1281)  time: 0.1955  data: 0.0002  max mem: 5511
[09:35:56.015912] Epoch: [49]  [680/781]  eta: 0:00:19  lr: 0.000136  training_loss: 0.3426 (0.3473)  mae_loss: 0.2153 (0.2192)  classification_loss: 0.1259 (0.1281)  time: 0.1950  data: 0.0002  max mem: 5511
[09:35:59.943280] Epoch: [49]  [700/781]  eta: 0:00:15  lr: 0.000136  training_loss: 0.3421 (0.3472)  mae_loss: 0.2081 (0.2190)  classification_loss: 0.1344 (0.1282)  time: 0.1963  data: 0.0003  max mem: 5511
[09:36:03.847556] Epoch: [49]  [720/781]  eta: 0:00:11  lr: 0.000136  training_loss: 0.3485 (0.3471)  mae_loss: 0.2096 (0.2188)  classification_loss: 0.1290 (0.1283)  time: 0.1951  data: 0.0002  max mem: 5511
[09:36:07.832015] Epoch: [49]  [740/781]  eta: 0:00:08  lr: 0.000136  training_loss: 0.3390 (0.3469)  mae_loss: 0.2049 (0.2186)  classification_loss: 0.1294 (0.1283)  time: 0.1991  data: 0.0002  max mem: 5511
[09:36:11.736714] Epoch: [49]  [760/781]  eta: 0:00:04  lr: 0.000136  training_loss: 0.3520 (0.3471)  mae_loss: 0.2227 (0.2188)  classification_loss: 0.1307 (0.1283)  time: 0.1951  data: 0.0003  max mem: 5511
[09:36:15.621514] Epoch: [49]  [780/781]  eta: 0:00:00  lr: 0.000136  training_loss: 0.3492 (0.3471)  mae_loss: 0.2144 (0.2188)  classification_loss: 0.1288 (0.1284)  time: 0.1942  data: 0.0002  max mem: 5511
[09:36:15.801973] Epoch: [49] Total time: 0:02:33 (0.1969 s / it)
[09:36:15.802482] Averaged stats: lr: 0.000136  training_loss: 0.3492 (0.3471)  mae_loss: 0.2144 (0.2188)  classification_loss: 0.1288 (0.1284)
[09:36:16.468304] Test:  [  0/157]  eta: 0:01:43  testing_loss: 0.5187 (0.5187)  acc1: 84.3750 (84.3750)  acc5: 100.0000 (100.0000)  time: 0.6619  data: 0.6272  max mem: 5511
[09:36:16.753999] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.6420 (0.6170)  acc1: 79.6875 (79.8295)  acc5: 100.0000 (99.5739)  time: 0.0860  data: 0.0572  max mem: 5511
[09:36:17.036094] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5946 (0.5829)  acc1: 81.2500 (81.6220)  acc5: 100.0000 (99.5536)  time: 0.0282  data: 0.0002  max mem: 5511
[09:36:17.318243] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5195 (0.5842)  acc1: 82.8125 (82.2077)  acc5: 100.0000 (99.1431)  time: 0.0281  data: 0.0002  max mem: 5511
[09:36:17.601304] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5711 (0.5879)  acc1: 81.2500 (81.7073)  acc5: 100.0000 (99.1616)  time: 0.0281  data: 0.0002  max mem: 5511
[09:36:17.882813] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5711 (0.5807)  acc1: 81.2500 (82.0466)  acc5: 100.0000 (99.1115)  time: 0.0281  data: 0.0002  max mem: 5511
[09:36:18.164661] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5380 (0.5721)  acc1: 84.3750 (82.3770)  acc5: 98.4375 (99.0266)  time: 0.0280  data: 0.0002  max mem: 5511
[09:36:18.447596] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.5262 (0.5646)  acc1: 82.8125 (82.3724)  acc5: 100.0000 (99.0757)  time: 0.0281  data: 0.0002  max mem: 5511
[09:36:18.729715] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.5379 (0.5708)  acc1: 81.2500 (82.1952)  acc5: 98.4375 (98.9390)  time: 0.0281  data: 0.0002  max mem: 5511
[09:36:19.012232] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5408 (0.5671)  acc1: 82.8125 (82.3832)  acc5: 98.4375 (98.9870)  time: 0.0281  data: 0.0002  max mem: 5511
[09:36:19.293752] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5416 (0.5695)  acc1: 82.8125 (82.3793)  acc5: 100.0000 (99.0254)  time: 0.0281  data: 0.0002  max mem: 5511
[09:36:19.575388] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5585 (0.5698)  acc1: 82.8125 (82.4465)  acc5: 100.0000 (99.0428)  time: 0.0280  data: 0.0001  max mem: 5511
[09:36:19.861479] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5397 (0.5659)  acc1: 82.8125 (82.5026)  acc5: 100.0000 (99.0444)  time: 0.0283  data: 0.0002  max mem: 5511
[09:36:20.144286] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5417 (0.5678)  acc1: 82.8125 (82.4189)  acc5: 98.4375 (99.0219)  time: 0.0283  data: 0.0002  max mem: 5511
[09:36:20.427153] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5734 (0.5685)  acc1: 82.8125 (82.4801)  acc5: 98.4375 (99.0359)  time: 0.0282  data: 0.0003  max mem: 5511
[09:36:20.705779] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5734 (0.5691)  acc1: 82.8125 (82.4503)  acc5: 100.0000 (99.0273)  time: 0.0280  data: 0.0003  max mem: 5511
[09:36:20.855674] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5599 (0.5694)  acc1: 81.2500 (82.3800)  acc5: 100.0000 (99.0500)  time: 0.0269  data: 0.0001  max mem: 5511
[09:36:21.004014] Test: Total time: 0:00:05 (0.0331 s / it)
[09:36:21.004469] * Acc@1 82.380 Acc@5 99.050 loss 0.569
[09:36:21.004821] Accuracy of the network on the 10000 test images: 82.4%
[09:36:21.004999] Max accuracy: 83.51%
[09:36:21.404256] log_dir: ./output_dir
[09:36:22.220886] Epoch: [50]  [  0/781]  eta: 0:10:36  lr: 0.000136  training_loss: 0.3619 (0.3619)  mae_loss: 0.2474 (0.2474)  classification_loss: 0.1146 (0.1146)  time: 0.8149  data: 0.5950  max mem: 5511
[09:36:26.141352] Epoch: [50]  [ 20/781]  eta: 0:02:51  lr: 0.000136  training_loss: 0.3390 (0.3448)  mae_loss: 0.2098 (0.2207)  classification_loss: 0.1250 (0.1241)  time: 0.1959  data: 0.0004  max mem: 5511
[09:36:30.115802] Epoch: [50]  [ 40/781]  eta: 0:02:37  lr: 0.000136  training_loss: 0.3346 (0.3428)  mae_loss: 0.2097 (0.2160)  classification_loss: 0.1264 (0.1269)  time: 0.1986  data: 0.0002  max mem: 5511
[09:36:34.038994] Epoch: [50]  [ 60/781]  eta: 0:02:29  lr: 0.000135  training_loss: 0.3384 (0.3420)  mae_loss: 0.2130 (0.2149)  classification_loss: 0.1265 (0.1271)  time: 0.1961  data: 0.0001  max mem: 5511
[09:36:37.929880] Epoch: [50]  [ 80/781]  eta: 0:02:22  lr: 0.000135  training_loss: 0.3593 (0.3457)  mae_loss: 0.2275 (0.2184)  classification_loss: 0.1272 (0.1273)  time: 0.1945  data: 0.0002  max mem: 5511
[09:36:41.854460] Epoch: [50]  [100/781]  eta: 0:02:17  lr: 0.000135  training_loss: 0.3490 (0.3466)  mae_loss: 0.2100 (0.2182)  classification_loss: 0.1292 (0.1284)  time: 0.1962  data: 0.0002  max mem: 5511
[09:36:45.764190] Epoch: [50]  [120/781]  eta: 0:02:13  lr: 0.000135  training_loss: 0.3403 (0.3470)  mae_loss: 0.2160 (0.2182)  classification_loss: 0.1297 (0.1287)  time: 0.1954  data: 0.0002  max mem: 5511
[09:36:49.661889] Epoch: [50]  [140/781]  eta: 0:02:08  lr: 0.000135  training_loss: 0.3342 (0.3451)  mae_loss: 0.2020 (0.2166)  classification_loss: 0.1254 (0.1284)  time: 0.1948  data: 0.0002  max mem: 5511
[09:36:53.549862] Epoch: [50]  [160/781]  eta: 0:02:03  lr: 0.000135  training_loss: 0.3412 (0.3463)  mae_loss: 0.2190 (0.2180)  classification_loss: 0.1266 (0.1284)  time: 0.1943  data: 0.0002  max mem: 5511
[09:36:57.454251] Epoch: [50]  [180/781]  eta: 0:01:59  lr: 0.000135  training_loss: 0.3506 (0.3467)  mae_loss: 0.2314 (0.2186)  classification_loss: 0.1248 (0.1281)  time: 0.1951  data: 0.0002  max mem: 5511
[09:37:01.357574] Epoch: [50]  [200/781]  eta: 0:01:55  lr: 0.000135  training_loss: 0.3390 (0.3460)  mae_loss: 0.2071 (0.2179)  classification_loss: 0.1253 (0.1280)  time: 0.1951  data: 0.0001  max mem: 5511
[09:37:05.259963] Epoch: [50]  [220/781]  eta: 0:01:51  lr: 0.000135  training_loss: 0.3400 (0.3457)  mae_loss: 0.2086 (0.2178)  classification_loss: 0.1270 (0.1279)  time: 0.1950  data: 0.0002  max mem: 5511
[09:37:09.176365] Epoch: [50]  [240/781]  eta: 0:01:47  lr: 0.000135  training_loss: 0.3359 (0.3452)  mae_loss: 0.2066 (0.2174)  classification_loss: 0.1271 (0.1278)  time: 0.1957  data: 0.0002  max mem: 5511
[09:37:13.078210] Epoch: [50]  [260/781]  eta: 0:01:43  lr: 0.000134  training_loss: 0.3183 (0.3441)  mae_loss: 0.1910 (0.2163)  classification_loss: 0.1270 (0.1278)  time: 0.1950  data: 0.0002  max mem: 5511
[09:37:16.976373] Epoch: [50]  [280/781]  eta: 0:01:39  lr: 0.000134  training_loss: 0.3389 (0.3444)  mae_loss: 0.2135 (0.2166)  classification_loss: 0.1231 (0.1278)  time: 0.1948  data: 0.0002  max mem: 5511
[09:37:20.889293] Epoch: [50]  [300/781]  eta: 0:01:35  lr: 0.000134  training_loss: 0.3543 (0.3444)  mae_loss: 0.2122 (0.2165)  classification_loss: 0.1308 (0.1279)  time: 0.1956  data: 0.0002  max mem: 5511
[09:37:24.804744] Epoch: [50]  [320/781]  eta: 0:01:31  lr: 0.000134  training_loss: 0.3548 (0.3448)  mae_loss: 0.2182 (0.2168)  classification_loss: 0.1263 (0.1280)  time: 0.1957  data: 0.0003  max mem: 5511
[09:37:28.723779] Epoch: [50]  [340/781]  eta: 0:01:27  lr: 0.000134  training_loss: 0.3465 (0.3448)  mae_loss: 0.2207 (0.2171)  classification_loss: 0.1241 (0.1278)  time: 0.1959  data: 0.0002  max mem: 5511
[09:37:32.639303] Epoch: [50]  [360/781]  eta: 0:01:23  lr: 0.000134  training_loss: 0.3356 (0.3447)  mae_loss: 0.2142 (0.2170)  classification_loss: 0.1255 (0.1277)  time: 0.1957  data: 0.0002  max mem: 5511
[09:37:36.550154] Epoch: [50]  [380/781]  eta: 0:01:19  lr: 0.000134  training_loss: 0.3457 (0.3454)  mae_loss: 0.2249 (0.2177)  classification_loss: 0.1266 (0.1277)  time: 0.1954  data: 0.0003  max mem: 5511
[09:37:40.510763] Epoch: [50]  [400/781]  eta: 0:01:15  lr: 0.000134  training_loss: 0.3465 (0.3453)  mae_loss: 0.2125 (0.2177)  classification_loss: 0.1298 (0.1277)  time: 0.1980  data: 0.0002  max mem: 5511
[09:37:44.465155] Epoch: [50]  [420/781]  eta: 0:01:11  lr: 0.000134  training_loss: 0.3358 (0.3451)  mae_loss: 0.2170 (0.2176)  classification_loss: 0.1245 (0.1275)  time: 0.1976  data: 0.0003  max mem: 5511
[09:37:48.373555] Epoch: [50]  [440/781]  eta: 0:01:07  lr: 0.000133  training_loss: 0.3452 (0.3450)  mae_loss: 0.2135 (0.2177)  classification_loss: 0.1230 (0.1274)  time: 0.1953  data: 0.0003  max mem: 5511
[09:37:52.281981] Epoch: [50]  [460/781]  eta: 0:01:03  lr: 0.000133  training_loss: 0.3455 (0.3449)  mae_loss: 0.2160 (0.2177)  classification_loss: 0.1248 (0.1273)  time: 0.1953  data: 0.0002  max mem: 5511
[09:37:56.175999] Epoch: [50]  [480/781]  eta: 0:00:59  lr: 0.000133  training_loss: 0.3409 (0.3448)  mae_loss: 0.2184 (0.2176)  classification_loss: 0.1267 (0.1272)  time: 0.1946  data: 0.0002  max mem: 5511
[09:38:00.093601] Epoch: [50]  [500/781]  eta: 0:00:55  lr: 0.000133  training_loss: 0.3510 (0.3452)  mae_loss: 0.2252 (0.2178)  classification_loss: 0.1304 (0.1274)  time: 0.1958  data: 0.0002  max mem: 5511
[09:38:04.008739] Epoch: [50]  [520/781]  eta: 0:00:51  lr: 0.000133  training_loss: 0.3422 (0.3450)  mae_loss: 0.2147 (0.2177)  classification_loss: 0.1267 (0.1273)  time: 0.1957  data: 0.0002  max mem: 5511
[09:38:07.999177] Epoch: [50]  [540/781]  eta: 0:00:47  lr: 0.000133  training_loss: 0.3517 (0.3451)  mae_loss: 0.2260 (0.2177)  classification_loss: 0.1302 (0.1274)  time: 0.1994  data: 0.0002  max mem: 5511
[09:38:11.973291] Epoch: [50]  [560/781]  eta: 0:00:43  lr: 0.000133  training_loss: 0.3428 (0.3450)  mae_loss: 0.2104 (0.2175)  classification_loss: 0.1262 (0.1275)  time: 0.1986  data: 0.0002  max mem: 5511
[09:38:15.913584] Epoch: [50]  [580/781]  eta: 0:00:39  lr: 0.000133  training_loss: 0.3330 (0.3449)  mae_loss: 0.2096 (0.2174)  classification_loss: 0.1272 (0.1275)  time: 0.1969  data: 0.0002  max mem: 5511
[09:38:19.807371] Epoch: [50]  [600/781]  eta: 0:00:35  lr: 0.000133  training_loss: 0.3353 (0.3445)  mae_loss: 0.2153 (0.2172)  classification_loss: 0.1201 (0.1273)  time: 0.1946  data: 0.0002  max mem: 5511
[09:38:23.710028] Epoch: [50]  [620/781]  eta: 0:00:31  lr: 0.000133  training_loss: 0.3641 (0.3453)  mae_loss: 0.2398 (0.2181)  classification_loss: 0.1230 (0.1272)  time: 0.1950  data: 0.0003  max mem: 5511
[09:38:27.606991] Epoch: [50]  [640/781]  eta: 0:00:27  lr: 0.000132  training_loss: 0.3306 (0.3450)  mae_loss: 0.2054 (0.2178)  classification_loss: 0.1236 (0.1272)  time: 0.1948  data: 0.0002  max mem: 5511
[09:38:31.513105] Epoch: [50]  [660/781]  eta: 0:00:23  lr: 0.000132  training_loss: 0.3582 (0.3454)  mae_loss: 0.2232 (0.2182)  classification_loss: 0.1285 (0.1272)  time: 0.1952  data: 0.0002  max mem: 5511
[09:38:35.397755] Epoch: [50]  [680/781]  eta: 0:00:19  lr: 0.000132  training_loss: 0.3477 (0.3453)  mae_loss: 0.2143 (0.2180)  classification_loss: 0.1293 (0.1273)  time: 0.1942  data: 0.0002  max mem: 5511
[09:38:39.303595] Epoch: [50]  [700/781]  eta: 0:00:15  lr: 0.000132  training_loss: 0.3412 (0.3456)  mae_loss: 0.2197 (0.2183)  classification_loss: 0.1281 (0.1273)  time: 0.1952  data: 0.0002  max mem: 5511
[09:38:43.213785] Epoch: [50]  [720/781]  eta: 0:00:11  lr: 0.000132  training_loss: 0.3520 (0.3459)  mae_loss: 0.2238 (0.2185)  classification_loss: 0.1273 (0.1273)  time: 0.1954  data: 0.0002  max mem: 5511
[09:38:47.144391] Epoch: [50]  [740/781]  eta: 0:00:08  lr: 0.000132  training_loss: 0.3516 (0.3461)  mae_loss: 0.2256 (0.2188)  classification_loss: 0.1263 (0.1274)  time: 0.1964  data: 0.0002  max mem: 5511
[09:38:51.070069] Epoch: [50]  [760/781]  eta: 0:00:04  lr: 0.000132  training_loss: 0.3510 (0.3463)  mae_loss: 0.2185 (0.2189)  classification_loss: 0.1313 (0.1274)  time: 0.1962  data: 0.0003  max mem: 5511
[09:38:54.971638] Epoch: [50]  [780/781]  eta: 0:00:00  lr: 0.000132  training_loss: 0.3562 (0.3465)  mae_loss: 0.2239 (0.2190)  classification_loss: 0.1314 (0.1275)  time: 0.1950  data: 0.0002  max mem: 5511
[09:38:55.123032] Epoch: [50] Total time: 0:02:33 (0.1968 s / it)
[09:38:55.123615] Averaged stats: lr: 0.000132  training_loss: 0.3562 (0.3465)  mae_loss: 0.2239 (0.2190)  classification_loss: 0.1314 (0.1275)
[09:38:57.151084] Test:  [  0/157]  eta: 0:01:48  testing_loss: 0.5269 (0.5269)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (100.0000)  time: 0.6893  data: 0.6459  max mem: 5511
[09:38:57.446511] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.5888 (0.5852)  acc1: 81.2500 (81.2500)  acc5: 100.0000 (99.7159)  time: 0.0893  data: 0.0589  max mem: 5511
[09:38:57.728214] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5633 (0.5623)  acc1: 82.8125 (82.3661)  acc5: 100.0000 (99.6280)  time: 0.0287  data: 0.0001  max mem: 5511
[09:38:58.008906] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5621 (0.5738)  acc1: 82.8125 (81.8548)  acc5: 100.0000 (99.1431)  time: 0.0280  data: 0.0001  max mem: 5511
[09:38:58.289734] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5682 (0.5760)  acc1: 81.2500 (81.9741)  acc5: 98.4375 (99.1235)  time: 0.0280  data: 0.0001  max mem: 5511
[09:38:58.570432] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5533 (0.5669)  acc1: 82.8125 (82.3529)  acc5: 98.4375 (99.0809)  time: 0.0280  data: 0.0001  max mem: 5511
[09:38:58.851828] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5350 (0.5618)  acc1: 82.8125 (82.7613)  acc5: 98.4375 (99.0266)  time: 0.0280  data: 0.0001  max mem: 5511
[09:38:59.132813] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4810 (0.5524)  acc1: 85.9375 (83.1426)  acc5: 100.0000 (99.1197)  time: 0.0280  data: 0.0001  max mem: 5511
[09:38:59.416179] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4865 (0.5556)  acc1: 84.3750 (83.1404)  acc5: 100.0000 (99.0934)  time: 0.0281  data: 0.0001  max mem: 5511
[09:38:59.702931] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4963 (0.5508)  acc1: 85.9375 (83.4306)  acc5: 100.0000 (99.1415)  time: 0.0284  data: 0.0002  max mem: 5511
[09:38:59.989184] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5034 (0.5521)  acc1: 85.9375 (83.3230)  acc5: 100.0000 (99.1337)  time: 0.0285  data: 0.0002  max mem: 5511
[09:39:00.271211] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5628 (0.5519)  acc1: 82.8125 (83.3756)  acc5: 100.0000 (99.1273)  time: 0.0283  data: 0.0002  max mem: 5511
[09:39:00.552616] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.5163 (0.5488)  acc1: 84.3750 (83.4840)  acc5: 100.0000 (99.1477)  time: 0.0280  data: 0.0002  max mem: 5511
[09:39:00.835224] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5143 (0.5506)  acc1: 84.3750 (83.2896)  acc5: 100.0000 (99.1531)  time: 0.0281  data: 0.0002  max mem: 5511
[09:39:01.115343] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5588 (0.5506)  acc1: 82.8125 (83.3223)  acc5: 100.0000 (99.1689)  time: 0.0280  data: 0.0001  max mem: 5511
[09:39:01.393791] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5444 (0.5506)  acc1: 82.8125 (83.2988)  acc5: 100.0000 (99.1618)  time: 0.0278  data: 0.0001  max mem: 5511
[09:39:01.544436] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5417 (0.5501)  acc1: 82.8125 (83.2500)  acc5: 100.0000 (99.1800)  time: 0.0269  data: 0.0001  max mem: 5511
[09:39:01.698781] Test: Total time: 0:00:05 (0.0334 s / it)
[09:39:01.699354] * Acc@1 83.250 Acc@5 99.180 loss 0.550
[09:39:01.699675] Accuracy of the network on the 10000 test images: 83.2%
[09:39:01.699857] Max accuracy: 83.51%
[09:39:02.090367] log_dir: ./output_dir
[09:39:02.860333] Epoch: [51]  [  0/781]  eta: 0:10:00  lr: 0.000132  training_loss: 0.3371 (0.3371)  mae_loss: 0.2163 (0.2163)  classification_loss: 0.1208 (0.1208)  time: 0.7683  data: 0.5644  max mem: 5511
[09:39:06.811028] Epoch: [51]  [ 20/781]  eta: 0:02:50  lr: 0.000132  training_loss: 0.3475 (0.3483)  mae_loss: 0.2232 (0.2254)  classification_loss: 0.1214 (0.1229)  time: 0.1974  data: 0.0001  max mem: 5511
[09:39:10.707893] Epoch: [51]  [ 40/781]  eta: 0:02:35  lr: 0.000131  training_loss: 0.3590 (0.3523)  mae_loss: 0.2337 (0.2261)  classification_loss: 0.1289 (0.1262)  time: 0.1948  data: 0.0002  max mem: 5511
[09:39:14.607073] Epoch: [51]  [ 60/781]  eta: 0:02:27  lr: 0.000131  training_loss: 0.3424 (0.3477)  mae_loss: 0.2113 (0.2217)  classification_loss: 0.1253 (0.1260)  time: 0.1949  data: 0.0002  max mem: 5511
[09:39:18.517884] Epoch: [51]  [ 80/781]  eta: 0:02:22  lr: 0.000131  training_loss: 0.3292 (0.3447)  mae_loss: 0.2033 (0.2189)  classification_loss: 0.1230 (0.1258)  time: 0.1955  data: 0.0002  max mem: 5511
[09:39:22.457675] Epoch: [51]  [100/781]  eta: 0:02:17  lr: 0.000131  training_loss: 0.3339 (0.3452)  mae_loss: 0.2164 (0.2194)  classification_loss: 0.1232 (0.1258)  time: 0.1969  data: 0.0003  max mem: 5511
[09:39:26.339924] Epoch: [51]  [120/781]  eta: 0:02:12  lr: 0.000131  training_loss: 0.3377 (0.3452)  mae_loss: 0.2213 (0.2194)  classification_loss: 0.1222 (0.1258)  time: 0.1940  data: 0.0002  max mem: 5511
[09:39:30.229150] Epoch: [51]  [140/781]  eta: 0:02:07  lr: 0.000131  training_loss: 0.3380 (0.3447)  mae_loss: 0.2094 (0.2189)  classification_loss: 0.1273 (0.1259)  time: 0.1944  data: 0.0003  max mem: 5511
[09:39:34.129236] Epoch: [51]  [160/781]  eta: 0:02:03  lr: 0.000131  training_loss: 0.3255 (0.3427)  mae_loss: 0.2014 (0.2169)  classification_loss: 0.1210 (0.1258)  time: 0.1949  data: 0.0003  max mem: 5511
[09:39:38.036554] Epoch: [51]  [180/781]  eta: 0:01:59  lr: 0.000131  training_loss: 0.3355 (0.3431)  mae_loss: 0.2102 (0.2172)  classification_loss: 0.1267 (0.1260)  time: 0.1953  data: 0.0004  max mem: 5511
[09:39:41.969198] Epoch: [51]  [200/781]  eta: 0:01:55  lr: 0.000131  training_loss: 0.3419 (0.3438)  mae_loss: 0.2259 (0.2178)  classification_loss: 0.1219 (0.1260)  time: 0.1965  data: 0.0002  max mem: 5511
[09:39:45.874160] Epoch: [51]  [220/781]  eta: 0:01:51  lr: 0.000131  training_loss: 0.3401 (0.3437)  mae_loss: 0.2086 (0.2176)  classification_loss: 0.1275 (0.1261)  time: 0.1952  data: 0.0002  max mem: 5511
[09:39:49.783422] Epoch: [51]  [240/781]  eta: 0:01:47  lr: 0.000130  training_loss: 0.3480 (0.3441)  mae_loss: 0.2170 (0.2177)  classification_loss: 0.1276 (0.1264)  time: 0.1954  data: 0.0002  max mem: 5511
[09:39:53.711441] Epoch: [51]  [260/781]  eta: 0:01:42  lr: 0.000130  training_loss: 0.3437 (0.3442)  mae_loss: 0.2169 (0.2178)  classification_loss: 0.1268 (0.1264)  time: 0.1963  data: 0.0002  max mem: 5511
[09:39:57.605005] Epoch: [51]  [280/781]  eta: 0:01:38  lr: 0.000130  training_loss: 0.3303 (0.3436)  mae_loss: 0.2090 (0.2174)  classification_loss: 0.1241 (0.1262)  time: 0.1946  data: 0.0003  max mem: 5511
[09:40:01.493137] Epoch: [51]  [300/781]  eta: 0:01:34  lr: 0.000130  training_loss: 0.3299 (0.3433)  mae_loss: 0.2143 (0.2172)  classification_loss: 0.1247 (0.1261)  time: 0.1943  data: 0.0002  max mem: 5511
[09:40:05.396013] Epoch: [51]  [320/781]  eta: 0:01:30  lr: 0.000130  training_loss: 0.3330 (0.3428)  mae_loss: 0.2058 (0.2168)  classification_loss: 0.1209 (0.1260)  time: 0.1951  data: 0.0002  max mem: 5511
[09:40:09.317335] Epoch: [51]  [340/781]  eta: 0:01:26  lr: 0.000130  training_loss: 0.3496 (0.3429)  mae_loss: 0.2210 (0.2170)  classification_loss: 0.1247 (0.1259)  time: 0.1960  data: 0.0003  max mem: 5511
[09:40:13.228063] Epoch: [51]  [360/781]  eta: 0:01:22  lr: 0.000130  training_loss: 0.3390 (0.3431)  mae_loss: 0.2165 (0.2173)  classification_loss: 0.1245 (0.1259)  time: 0.1955  data: 0.0002  max mem: 5511
[09:40:17.185125] Epoch: [51]  [380/781]  eta: 0:01:19  lr: 0.000130  training_loss: 0.3576 (0.3436)  mae_loss: 0.2227 (0.2176)  classification_loss: 0.1292 (0.1260)  time: 0.1978  data: 0.0002  max mem: 5511
[09:40:21.106507] Epoch: [51]  [400/781]  eta: 0:01:15  lr: 0.000130  training_loss: 0.3419 (0.3438)  mae_loss: 0.2156 (0.2178)  classification_loss: 0.1264 (0.1260)  time: 0.1960  data: 0.0002  max mem: 5511
[09:40:25.009678] Epoch: [51]  [420/781]  eta: 0:01:11  lr: 0.000129  training_loss: 0.3342 (0.3435)  mae_loss: 0.2101 (0.2176)  classification_loss: 0.1217 (0.1259)  time: 0.1951  data: 0.0002  max mem: 5511
[09:40:28.905853] Epoch: [51]  [440/781]  eta: 0:01:07  lr: 0.000129  training_loss: 0.3318 (0.3433)  mae_loss: 0.2149 (0.2174)  classification_loss: 0.1254 (0.1259)  time: 0.1947  data: 0.0002  max mem: 5511
[09:40:32.796409] Epoch: [51]  [460/781]  eta: 0:01:03  lr: 0.000129  training_loss: 0.3378 (0.3431)  mae_loss: 0.2130 (0.2173)  classification_loss: 0.1243 (0.1258)  time: 0.1945  data: 0.0002  max mem: 5511
[09:40:36.688320] Epoch: [51]  [480/781]  eta: 0:00:59  lr: 0.000129  training_loss: 0.3382 (0.3431)  mae_loss: 0.2138 (0.2172)  classification_loss: 0.1297 (0.1259)  time: 0.1945  data: 0.0003  max mem: 5511
[09:40:40.592351] Epoch: [51]  [500/781]  eta: 0:00:55  lr: 0.000129  training_loss: 0.3379 (0.3431)  mae_loss: 0.2076 (0.2171)  classification_loss: 0.1296 (0.1260)  time: 0.1951  data: 0.0002  max mem: 5511
[09:40:44.502068] Epoch: [51]  [520/781]  eta: 0:00:51  lr: 0.000129  training_loss: 0.3442 (0.3432)  mae_loss: 0.2165 (0.2171)  classification_loss: 0.1245 (0.1261)  time: 0.1954  data: 0.0002  max mem: 5511
[09:40:48.412555] Epoch: [51]  [540/781]  eta: 0:00:47  lr: 0.000129  training_loss: 0.3406 (0.3431)  mae_loss: 0.2104 (0.2169)  classification_loss: 0.1278 (0.1262)  time: 0.1955  data: 0.0003  max mem: 5511
[09:40:52.312938] Epoch: [51]  [560/781]  eta: 0:00:43  lr: 0.000129  training_loss: 0.3594 (0.3435)  mae_loss: 0.2408 (0.2174)  classification_loss: 0.1243 (0.1261)  time: 0.1949  data: 0.0002  max mem: 5511
[09:40:56.210648] Epoch: [51]  [580/781]  eta: 0:00:39  lr: 0.000129  training_loss: 0.3453 (0.3435)  mae_loss: 0.2132 (0.2174)  classification_loss: 0.1241 (0.1261)  time: 0.1948  data: 0.0002  max mem: 5511
[09:41:00.096937] Epoch: [51]  [600/781]  eta: 0:00:35  lr: 0.000129  training_loss: 0.3484 (0.3437)  mae_loss: 0.2191 (0.2177)  classification_loss: 0.1210 (0.1261)  time: 0.1942  data: 0.0003  max mem: 5511
[09:41:04.003808] Epoch: [51]  [620/781]  eta: 0:00:31  lr: 0.000128  training_loss: 0.3381 (0.3437)  mae_loss: 0.2079 (0.2176)  classification_loss: 0.1242 (0.1261)  time: 0.1953  data: 0.0004  max mem: 5511
[09:41:07.921017] Epoch: [51]  [640/781]  eta: 0:00:27  lr: 0.000128  training_loss: 0.3405 (0.3437)  mae_loss: 0.2101 (0.2176)  classification_loss: 0.1290 (0.1261)  time: 0.1958  data: 0.0002  max mem: 5511
[09:41:11.808168] Epoch: [51]  [660/781]  eta: 0:00:23  lr: 0.000128  training_loss: 0.3465 (0.3437)  mae_loss: 0.2171 (0.2175)  classification_loss: 0.1247 (0.1261)  time: 0.1943  data: 0.0002  max mem: 5511
[09:41:15.717658] Epoch: [51]  [680/781]  eta: 0:00:19  lr: 0.000128  training_loss: 0.3477 (0.3438)  mae_loss: 0.2159 (0.2177)  classification_loss: 0.1262 (0.1262)  time: 0.1954  data: 0.0002  max mem: 5511
[09:41:19.618785] Epoch: [51]  [700/781]  eta: 0:00:15  lr: 0.000128  training_loss: 0.3439 (0.3439)  mae_loss: 0.2134 (0.2176)  classification_loss: 0.1290 (0.1263)  time: 0.1949  data: 0.0003  max mem: 5511
[09:41:23.526320] Epoch: [51]  [720/781]  eta: 0:00:11  lr: 0.000128  training_loss: 0.3322 (0.3439)  mae_loss: 0.2054 (0.2176)  classification_loss: 0.1296 (0.1263)  time: 0.1953  data: 0.0002  max mem: 5511
[09:41:27.449739] Epoch: [51]  [740/781]  eta: 0:00:08  lr: 0.000128  training_loss: 0.3508 (0.3441)  mae_loss: 0.2248 (0.2178)  classification_loss: 0.1229 (0.1263)  time: 0.1961  data: 0.0002  max mem: 5511
[09:41:31.360672] Epoch: [51]  [760/781]  eta: 0:00:04  lr: 0.000128  training_loss: 0.3466 (0.3441)  mae_loss: 0.2224 (0.2177)  classification_loss: 0.1274 (0.1263)  time: 0.1955  data: 0.0002  max mem: 5511
[09:41:35.343494] Epoch: [51]  [780/781]  eta: 0:00:00  lr: 0.000128  training_loss: 0.3466 (0.3442)  mae_loss: 0.2179 (0.2178)  classification_loss: 0.1250 (0.1263)  time: 0.1991  data: 0.0002  max mem: 5511
[09:41:35.522637] Epoch: [51] Total time: 0:02:33 (0.1965 s / it)
[09:41:35.523080] Averaged stats: lr: 0.000128  training_loss: 0.3466 (0.3442)  mae_loss: 0.2179 (0.2178)  classification_loss: 0.1250 (0.1263)
[09:41:36.184697] Test:  [  0/157]  eta: 0:01:43  testing_loss: 0.5022 (0.5022)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.6576  data: 0.6228  max mem: 5511
[09:41:36.475946] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5648 (0.5361)  acc1: 82.8125 (82.2443)  acc5: 100.0000 (99.2898)  time: 0.0861  data: 0.0568  max mem: 5511
[09:41:36.758548] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5326 (0.5231)  acc1: 82.8125 (83.1101)  acc5: 100.0000 (99.4048)  time: 0.0285  data: 0.0002  max mem: 5511
[09:41:37.043849] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4854 (0.5258)  acc1: 84.3750 (83.3669)  acc5: 100.0000 (99.2440)  time: 0.0282  data: 0.0002  max mem: 5511
[09:41:37.326733] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5204 (0.5326)  acc1: 84.3750 (83.4223)  acc5: 100.0000 (99.1616)  time: 0.0283  data: 0.0002  max mem: 5511
[09:41:37.620552] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5007 (0.5254)  acc1: 84.3750 (83.6091)  acc5: 100.0000 (99.2341)  time: 0.0287  data: 0.0002  max mem: 5511
[09:41:37.907793] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4976 (0.5241)  acc1: 82.8125 (83.5297)  acc5: 100.0000 (99.1803)  time: 0.0289  data: 0.0003  max mem: 5511
[09:41:38.191393] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4596 (0.5147)  acc1: 84.3750 (83.9789)  acc5: 100.0000 (99.2738)  time: 0.0284  data: 0.0003  max mem: 5511
[09:41:38.480429] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4765 (0.5199)  acc1: 84.3750 (83.8349)  acc5: 100.0000 (99.3056)  time: 0.0285  data: 0.0002  max mem: 5511
[09:41:38.767986] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4997 (0.5135)  acc1: 84.3750 (84.0831)  acc5: 100.0000 (99.3304)  time: 0.0287  data: 0.0005  max mem: 5511
[09:41:39.052618] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5074 (0.5172)  acc1: 84.3750 (83.9728)  acc5: 100.0000 (99.3657)  time: 0.0285  data: 0.0004  max mem: 5511
[09:41:39.333506] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5129 (0.5157)  acc1: 84.3750 (84.0653)  acc5: 100.0000 (99.3243)  time: 0.0282  data: 0.0002  max mem: 5511
[09:41:39.616794] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4912 (0.5121)  acc1: 85.9375 (84.1296)  acc5: 98.4375 (99.3414)  time: 0.0281  data: 0.0002  max mem: 5511
[09:41:39.899283] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4641 (0.5118)  acc1: 85.9375 (84.1365)  acc5: 100.0000 (99.3798)  time: 0.0282  data: 0.0002  max mem: 5511
[09:41:40.182814] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4750 (0.5112)  acc1: 84.3750 (84.2088)  acc5: 100.0000 (99.4016)  time: 0.0282  data: 0.0002  max mem: 5511
[09:41:40.464348] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5183 (0.5104)  acc1: 84.3750 (84.1267)  acc5: 100.0000 (99.4205)  time: 0.0281  data: 0.0002  max mem: 5511
[09:41:40.615568] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5183 (0.5113)  acc1: 82.8125 (84.0200)  acc5: 100.0000 (99.4300)  time: 0.0271  data: 0.0001  max mem: 5511
[09:41:40.802143] Test: Total time: 0:00:05 (0.0336 s / it)
[09:41:40.802893] * Acc@1 84.020 Acc@5 99.430 loss 0.511
[09:41:40.803191] Accuracy of the network on the 10000 test images: 84.0%
[09:41:40.803363] Max accuracy: 84.02%
[09:41:41.073723] log_dir: ./output_dir
[09:41:41.926909] Epoch: [52]  [  0/781]  eta: 0:11:04  lr: 0.000128  training_loss: 0.3189 (0.3189)  mae_loss: 0.2074 (0.2074)  classification_loss: 0.1115 (0.1115)  time: 0.8514  data: 0.6311  max mem: 5511
[09:41:45.844440] Epoch: [52]  [ 20/781]  eta: 0:02:52  lr: 0.000127  training_loss: 0.3382 (0.3432)  mae_loss: 0.2186 (0.2177)  classification_loss: 0.1235 (0.1255)  time: 0.1958  data: 0.0003  max mem: 5511
[09:41:49.741176] Epoch: [52]  [ 40/781]  eta: 0:02:36  lr: 0.000127  training_loss: 0.3232 (0.3409)  mae_loss: 0.2055 (0.2155)  classification_loss: 0.1207 (0.1254)  time: 0.1947  data: 0.0002  max mem: 5511
[09:41:53.691766] Epoch: [52]  [ 60/781]  eta: 0:02:29  lr: 0.000127  training_loss: 0.3428 (0.3401)  mae_loss: 0.2136 (0.2151)  classification_loss: 0.1250 (0.1250)  time: 0.1974  data: 0.0002  max mem: 5511
[09:41:57.612898] Epoch: [52]  [ 80/781]  eta: 0:02:23  lr: 0.000127  training_loss: 0.3431 (0.3407)  mae_loss: 0.2180 (0.2155)  classification_loss: 0.1225 (0.1251)  time: 0.1960  data: 0.0002  max mem: 5511

[09:42:01.518052] Epoch: [52]  [100/781]  eta: 0:02:17  lr: 0.000127  training_loss: 0.3495 (0.3412)  mae_loss: 0.2207 (0.2158)  classification_loss: 0.1263 (0.1254)  time: 0.1952  data: 0.0002  max mem: 5511
[09:42:05.411646] Epoch: [52]  [120/781]  eta: 0:02:12  lr: 0.000127  training_loss: 0.3292 (0.3397)  mae_loss: 0.2056 (0.2142)  classification_loss: 0.1262 (0.1254)  time: 0.1946  data: 0.0003  max mem: 5511
[09:42:09.321216] Epoch: [52]  [140/781]  eta: 0:02:08  lr: 0.000127  training_loss: 0.3309 (0.3387)  mae_loss: 0.2080 (0.2137)  classification_loss: 0.1209 (0.1250)  time: 0.1954  data: 0.0002  max mem: 5511
[09:42:13.229935] Epoch: [52]  [160/781]  eta: 0:02:03  lr: 0.000127  training_loss: 0.3429 (0.3386)  mae_loss: 0.2117 (0.2137)  classification_loss: 0.1239 (0.1249)  time: 0.1954  data: 0.0002  max mem: 5511
[09:42:17.182221] Epoch: [52]  [180/781]  eta: 0:01:59  lr: 0.000127  training_loss: 0.3285 (0.3388)  mae_loss: 0.2045 (0.2135)  classification_loss: 0.1277 (0.1253)  time: 0.1975  data: 0.0003  max mem: 5511
[09:42:21.086614] Epoch: [52]  [200/781]  eta: 0:01:55  lr: 0.000127  training_loss: 0.3356 (0.3386)  mae_loss: 0.2094 (0.2131)  classification_loss: 0.1244 (0.1255)  time: 0.1951  data: 0.0002  max mem: 5511
[09:42:25.038541] Epoch: [52]  [220/781]  eta: 0:01:51  lr: 0.000126  training_loss: 0.3454 (0.3385)  mae_loss: 0.2175 (0.2132)  classification_loss: 0.1255 (0.1253)  time: 0.1975  data: 0.0002  max mem: 5511
[09:42:28.949002] Epoch: [52]  [240/781]  eta: 0:01:47  lr: 0.000126  training_loss: 0.3340 (0.3386)  mae_loss: 0.2062 (0.2132)  classification_loss: 0.1243 (0.1254)  time: 0.1954  data: 0.0002  max mem: 5511
[09:42:32.860144] Epoch: [52]  [260/781]  eta: 0:01:43  lr: 0.000126  training_loss: 0.3291 (0.3386)  mae_loss: 0.2042 (0.2132)  classification_loss: 0.1242 (0.1254)  time: 0.1955  data: 0.0004  max mem: 5511
[09:42:36.752293] Epoch: [52]  [280/781]  eta: 0:01:39  lr: 0.000126  training_loss: 0.3493 (0.3392)  mae_loss: 0.2206 (0.2138)  classification_loss: 0.1264 (0.1254)  time: 0.1945  data: 0.0002  max mem: 5511
[09:42:40.653558] Epoch: [52]  [300/781]  eta: 0:01:35  lr: 0.000126  training_loss: 0.3397 (0.3393)  mae_loss: 0.2178 (0.2140)  classification_loss: 0.1223 (0.1253)  time: 0.1950  data: 0.0002  max mem: 5511
[09:42:44.583318] Epoch: [52]  [320/781]  eta: 0:01:31  lr: 0.000126  training_loss: 0.3426 (0.3397)  mae_loss: 0.2143 (0.2144)  classification_loss: 0.1249 (0.1253)  time: 0.1964  data: 0.0002  max mem: 5511
[09:42:48.523607] Epoch: [52]  [340/781]  eta: 0:01:27  lr: 0.000126  training_loss: 0.3505 (0.3407)  mae_loss: 0.2194 (0.2152)  classification_loss: 0.1303 (0.1256)  time: 0.1969  data: 0.0005  max mem: 5511
[09:42:52.438590] Epoch: [52]  [360/781]  eta: 0:01:23  lr: 0.000126  training_loss: 0.3432 (0.3411)  mae_loss: 0.2049 (0.2154)  classification_loss: 0.1283 (0.1257)  time: 0.1957  data: 0.0003  max mem: 5511
[09:42:56.345461] Epoch: [52]  [380/781]  eta: 0:01:19  lr: 0.000126  training_loss: 0.3454 (0.3411)  mae_loss: 0.2198 (0.2155)  classification_loss: 0.1217 (0.1257)  time: 0.1952  data: 0.0002  max mem: 5511
[09:43:00.250962] Epoch: [52]  [400/781]  eta: 0:01:15  lr: 0.000125  training_loss: 0.3281 (0.3410)  mae_loss: 0.2054 (0.2153)  classification_loss: 0.1254 (0.1258)  time: 0.1952  data: 0.0002  max mem: 5511
[09:43:04.158070] Epoch: [52]  [420/781]  eta: 0:01:11  lr: 0.000125  training_loss: 0.3295 (0.3407)  mae_loss: 0.2024 (0.2149)  classification_loss: 0.1225 (0.1257)  time: 0.1952  data: 0.0002  max mem: 5511
[09:43:08.064024] Epoch: [52]  [440/781]  eta: 0:01:07  lr: 0.000125  training_loss: 0.3252 (0.3402)  mae_loss: 0.2065 (0.2146)  classification_loss: 0.1192 (0.1256)  time: 0.1952  data: 0.0003  max mem: 5511
[09:43:11.998882] Epoch: [52]  [460/781]  eta: 0:01:03  lr: 0.000125  training_loss: 0.3330 (0.3401)  mae_loss: 0.2101 (0.2145)  classification_loss: 0.1253 (0.1256)  time: 0.1967  data: 0.0002  max mem: 5511
[09:43:15.919126] Epoch: [52]  [480/781]  eta: 0:00:59  lr: 0.000125  training_loss: 0.3171 (0.3395)  mae_loss: 0.1860 (0.2139)  classification_loss: 0.1259 (0.1257)  time: 0.1959  data: 0.0002  max mem: 5511
[09:43:19.834593] Epoch: [52]  [500/781]  eta: 0:00:55  lr: 0.000125  training_loss: 0.3472 (0.3397)  mae_loss: 0.2175 (0.2141)  classification_loss: 0.1209 (0.1256)  time: 0.1957  data: 0.0002  max mem: 5511
[09:43:23.748192] Epoch: [52]  [520/781]  eta: 0:00:51  lr: 0.000125  training_loss: 0.3273 (0.3395)  mae_loss: 0.2080 (0.2138)  classification_loss: 0.1296 (0.1257)  time: 0.1956  data: 0.0003  max mem: 5511
[09:43:27.669121] Epoch: [52]  [540/781]  eta: 0:00:47  lr: 0.000125  training_loss: 0.3446 (0.3399)  mae_loss: 0.2190 (0.2142)  classification_loss: 0.1262 (0.1257)  time: 0.1959  data: 0.0002  max mem: 5511
[09:43:31.593306] Epoch: [52]  [560/781]  eta: 0:00:43  lr: 0.000125  training_loss: 0.3378 (0.3403)  mae_loss: 0.2173 (0.2146)  classification_loss: 0.1223 (0.1257)  time: 0.1961  data: 0.0002  max mem: 5511
[09:43:35.522723] Epoch: [52]  [580/781]  eta: 0:00:39  lr: 0.000125  training_loss: 0.3452 (0.3406)  mae_loss: 0.2162 (0.2150)  classification_loss: 0.1277 (0.1257)  time: 0.1964  data: 0.0002  max mem: 5511
[09:43:39.444510] Epoch: [52]  [600/781]  eta: 0:00:35  lr: 0.000124  training_loss: 0.3373 (0.3406)  mae_loss: 0.2098 (0.2149)  classification_loss: 0.1215 (0.1257)  time: 0.1960  data: 0.0002  max mem: 5511
[09:43:43.366911] Epoch: [52]  [620/781]  eta: 0:00:31  lr: 0.000124  training_loss: 0.3373 (0.3406)  mae_loss: 0.2161 (0.2149)  classification_loss: 0.1243 (0.1257)  time: 0.1960  data: 0.0002  max mem: 5511
[09:43:47.299368] Epoch: [52]  [640/781]  eta: 0:00:27  lr: 0.000124  training_loss: 0.3317 (0.3405)  mae_loss: 0.2131 (0.2150)  classification_loss: 0.1233 (0.1256)  time: 0.1965  data: 0.0004  max mem: 5511
[09:43:51.199956] Epoch: [52]  [660/781]  eta: 0:00:23  lr: 0.000124  training_loss: 0.3394 (0.3409)  mae_loss: 0.2165 (0.2152)  classification_loss: 0.1292 (0.1257)  time: 0.1949  data: 0.0002  max mem: 5511
[09:43:55.112919] Epoch: [52]  [680/781]  eta: 0:00:19  lr: 0.000124  training_loss: 0.3288 (0.3407)  mae_loss: 0.1988 (0.2150)  classification_loss: 0.1249 (0.1257)  time: 0.1955  data: 0.0002  max mem: 5511
[09:43:59.014656] Epoch: [52]  [700/781]  eta: 0:00:15  lr: 0.000124  training_loss: 0.3350 (0.3407)  mae_loss: 0.2121 (0.2149)  classification_loss: 0.1270 (0.1258)  time: 0.1950  data: 0.0002  max mem: 5511
[09:44:02.934086] Epoch: [52]  [720/781]  eta: 0:00:11  lr: 0.000124  training_loss: 0.3463 (0.3409)  mae_loss: 0.2246 (0.2151)  classification_loss: 0.1292 (0.1258)  time: 0.1959  data: 0.0002  max mem: 5511
[09:44:06.869157] Epoch: [52]  [740/781]  eta: 0:00:08  lr: 0.000124  training_loss: 0.3363 (0.3407)  mae_loss: 0.2074 (0.2150)  classification_loss: 0.1240 (0.1257)  time: 0.1966  data: 0.0002  max mem: 5511
[09:44:10.813953] Epoch: [52]  [760/781]  eta: 0:00:04  lr: 0.000124  training_loss: 0.3419 (0.3408)  mae_loss: 0.2162 (0.2151)  classification_loss: 0.1255 (0.1258)  time: 0.1971  data: 0.0002  max mem: 5511
[09:44:14.725162] Epoch: [52]  [780/781]  eta: 0:00:00  lr: 0.000123  training_loss: 0.3300 (0.3407)  mae_loss: 0.2029 (0.2150)  classification_loss: 0.1189 (0.1258)  time: 0.1955  data: 0.0002  max mem: 5511
[09:44:14.880256] Epoch: [52] Total time: 0:02:33 (0.1969 s / it)
[09:44:14.881330] Averaged stats: lr: 0.000123  training_loss: 0.3300 (0.3407)  mae_loss: 0.2029 (0.2150)  classification_loss: 0.1189 (0.1258)
[09:44:15.575480] Test:  [  0/157]  eta: 0:01:48  testing_loss: 0.5817 (0.5817)  acc1: 81.2500 (81.2500)  acc5: 98.4375 (98.4375)  time: 0.6899  data: 0.6599  max mem: 5511
[09:44:15.860352] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5817 (0.5482)  acc1: 82.8125 (82.6705)  acc5: 100.0000 (99.5739)  time: 0.0884  data: 0.0601  max mem: 5511
[09:44:16.144877] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.5128 (0.5343)  acc1: 84.3750 (83.7798)  acc5: 100.0000 (99.4792)  time: 0.0283  data: 0.0002  max mem: 5511
[09:44:16.430765] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.5122 (0.5438)  acc1: 82.8125 (83.6694)  acc5: 100.0000 (99.3448)  time: 0.0284  data: 0.0002  max mem: 5511
[09:44:16.715993] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5122 (0.5466)  acc1: 82.8125 (83.3841)  acc5: 100.0000 (99.3140)  time: 0.0283  data: 0.0002  max mem: 5511
[09:44:17.001160] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5126 (0.5404)  acc1: 82.8125 (83.7623)  acc5: 100.0000 (99.3260)  time: 0.0283  data: 0.0002  max mem: 5511
[09:44:17.287353] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5069 (0.5349)  acc1: 84.3750 (83.9395)  acc5: 100.0000 (99.2572)  time: 0.0284  data: 0.0002  max mem: 5511
[09:44:17.570854] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4695 (0.5270)  acc1: 85.9375 (84.2430)  acc5: 98.4375 (99.2298)  time: 0.0284  data: 0.0002  max mem: 5511
[09:44:17.856280] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4900 (0.5315)  acc1: 84.3750 (83.9892)  acc5: 98.4375 (99.1705)  time: 0.0283  data: 0.0003  max mem: 5511
[09:44:18.145360] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.5141 (0.5280)  acc1: 84.3750 (84.2205)  acc5: 100.0000 (99.2273)  time: 0.0286  data: 0.0004  max mem: 5511
[09:44:18.429353] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5141 (0.5300)  acc1: 84.3750 (84.1120)  acc5: 100.0000 (99.2729)  time: 0.0284  data: 0.0002  max mem: 5511
[09:44:18.713484] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5193 (0.5289)  acc1: 84.3750 (84.2202)  acc5: 100.0000 (99.2821)  time: 0.0282  data: 0.0002  max mem: 5511
[09:44:18.994738] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4892 (0.5239)  acc1: 84.3750 (84.2588)  acc5: 100.0000 (99.3285)  time: 0.0282  data: 0.0001  max mem: 5511
[09:44:19.281278] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.5177 (0.5281)  acc1: 82.8125 (84.0052)  acc5: 100.0000 (99.3440)  time: 0.0282  data: 0.0001  max mem: 5511
[09:44:19.562050] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5549 (0.5287)  acc1: 81.2500 (83.8763)  acc5: 100.0000 (99.3573)  time: 0.0282  data: 0.0001  max mem: 5511
[09:44:19.841493] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5474 (0.5280)  acc1: 81.2500 (83.8783)  acc5: 100.0000 (99.3481)  time: 0.0279  data: 0.0001  max mem: 5511
[09:44:19.991613] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5328 (0.5277)  acc1: 84.3750 (83.9200)  acc5: 100.0000 (99.3700)  time: 0.0269  data: 0.0001  max mem: 5511
[09:44:20.146717] Test: Total time: 0:00:05 (0.0335 s / it)
[09:44:20.147159] * Acc@1 83.920 Acc@5 99.370 loss 0.528
[09:44:20.147492] Accuracy of the network on the 10000 test images: 83.9%
[09:44:20.147891] Max accuracy: 84.02%
[09:44:20.537316] log_dir: ./output_dir
[09:44:21.459088] Epoch: [53]  [  0/781]  eta: 0:11:58  lr: 0.000123  training_loss: 0.3330 (0.3330)  mae_loss: 0.2192 (0.2192)  classification_loss: 0.1138 (0.1138)  time: 0.9201  data: 0.6845  max mem: 5511
[09:44:25.356029] Epoch: [53]  [ 20/781]  eta: 0:02:54  lr: 0.000123  training_loss: 0.3350 (0.3410)  mae_loss: 0.2055 (0.2160)  classification_loss: 0.1239 (0.1250)  time: 0.1948  data: 0.0002  max mem: 5511
[09:44:29.248257] Epoch: [53]  [ 40/781]  eta: 0:02:37  lr: 0.000123  training_loss: 0.3338 (0.3389)  mae_loss: 0.2006 (0.2130)  classification_loss: 0.1249 (0.1259)  time: 0.1945  data: 0.0003  max mem: 5511
[09:44:33.155735] Epoch: [53]  [ 60/781]  eta: 0:02:29  lr: 0.000123  training_loss: 0.3341 (0.3417)  mae_loss: 0.2153 (0.2154)  classification_loss: 0.1269 (0.1264)  time: 0.1953  data: 0.0002  max mem: 5511
[09:44:37.093437] Epoch: [53]  [ 80/781]  eta: 0:02:23  lr: 0.000123  training_loss: 0.3508 (0.3414)  mae_loss: 0.2123 (0.2150)  classification_loss: 0.1252 (0.1264)  time: 0.1968  data: 0.0002  max mem: 5511
[09:44:41.003198] Epoch: [53]  [100/781]  eta: 0:02:17  lr: 0.000123  training_loss: 0.3427 (0.3421)  mae_loss: 0.2116 (0.2157)  classification_loss: 0.1273 (0.1264)  time: 0.1954  data: 0.0002  max mem: 5511
[09:44:44.978616] Epoch: [53]  [120/781]  eta: 0:02:13  lr: 0.000123  training_loss: 0.3249 (0.3392)  mae_loss: 0.2019 (0.2135)  classification_loss: 0.1216 (0.1257)  time: 0.1987  data: 0.0002  max mem: 5511
[09:44:48.936748] Epoch: [53]  [140/781]  eta: 0:02:09  lr: 0.000123  training_loss: 0.3331 (0.3386)  mae_loss: 0.2224 (0.2138)  classification_loss: 0.1182 (0.1248)  time: 0.1978  data: 0.0002  max mem: 5511
[09:44:52.833479] Epoch: [53]  [160/781]  eta: 0:02:04  lr: 0.000123  training_loss: 0.3323 (0.3387)  mae_loss: 0.2069 (0.2141)  classification_loss: 0.1233 (0.1246)  time: 0.1947  data: 0.0002  max mem: 5511
[09:44:56.765119] Epoch: [53]  [180/781]  eta: 0:02:00  lr: 0.000122  training_loss: 0.3324 (0.3386)  mae_loss: 0.2120 (0.2140)  classification_loss: 0.1228 (0.1245)  time: 0.1965  data: 0.0002  max mem: 5511
[09:45:00.674373] Epoch: [53]  [200/781]  eta: 0:01:55  lr: 0.000122  training_loss: 0.3355 (0.3388)  mae_loss: 0.2128 (0.2144)  classification_loss: 0.1187 (0.1244)  time: 0.1954  data: 0.0002  max mem: 5511
[09:45:04.589303] Epoch: [53]  [220/781]  eta: 0:01:51  lr: 0.000122  training_loss: 0.3305 (0.3382)  mae_loss: 0.2043 (0.2141)  classification_loss: 0.1220 (0.1241)  time: 0.1956  data: 0.0003  max mem: 5511
[09:45:08.476511] Epoch: [53]  [240/781]  eta: 0:01:47  lr: 0.000122  training_loss: 0.3299 (0.3380)  mae_loss: 0.2090 (0.2138)  classification_loss: 0.1242 (0.1241)  time: 0.1943  data: 0.0002  max mem: 5511
[09:45:12.380387] Epoch: [53]  [260/781]  eta: 0:01:43  lr: 0.000122  training_loss: 0.3384 (0.3383)  mae_loss: 0.2160 (0.2142)  classification_loss: 0.1242 (0.1241)  time: 0.1951  data: 0.0002  max mem: 5511
[09:45:16.288050] Epoch: [53]  [280/781]  eta: 0:01:39  lr: 0.000122  training_loss: 0.3401 (0.3386)  mae_loss: 0.2239 (0.2146)  classification_loss: 0.1207 (0.1239)  time: 0.1953  data: 0.0003  max mem: 5511
[09:45:20.198525] Epoch: [53]  [300/781]  eta: 0:01:35  lr: 0.000122  training_loss: 0.3562 (0.3393)  mae_loss: 0.2236 (0.2152)  classification_loss: 0.1247 (0.1241)  time: 0.1954  data: 0.0003  max mem: 5511
[09:45:24.102490] Epoch: [53]  [320/781]  eta: 0:01:31  lr: 0.000122  training_loss: 0.3229 (0.3389)  mae_loss: 0.2024 (0.2148)  classification_loss: 0.1205 (0.1240)  time: 0.1951  data: 0.0003  max mem: 5511
[09:45:28.006748] Epoch: [53]  [340/781]  eta: 0:01:27  lr: 0.000122  training_loss: 0.3334 (0.3387)  mae_loss: 0.2152 (0.2148)  classification_loss: 0.1220 (0.1239)  time: 0.1951  data: 0.0002  max mem: 5511
[09:45:31.993336] Epoch: [53]  [360/781]  eta: 0:01:23  lr: 0.000122  training_loss: 0.3370 (0.3388)  mae_loss: 0.2095 (0.2148)  classification_loss: 0.1291 (0.1240)  time: 0.1992  data: 0.0002  max mem: 5511
[09:45:35.927896] Epoch: [53]  [380/781]  eta: 0:01:19  lr: 0.000121  training_loss: 0.3224 (0.3385)  mae_loss: 0.2090 (0.2144)  classification_loss: 0.1213 (0.1242)  time: 0.1966  data: 0.0002  max mem: 5511

[09:45:39.825213] Epoch: [53]  [400/781]  eta: 0:01:15  lr: 0.000121  training_loss: 0.3273 (0.3384)  mae_loss: 0.2104 (0.2142)  classification_loss: 0.1239 (0.1242)  time: 0.1948  data: 0.0003  max mem: 5511
[09:45:43.716524] Epoch: [53]  [420/781]  eta: 0:01:11  lr: 0.000121  training_loss: 0.3299 (0.3381)  mae_loss: 0.2122 (0.2140)  classification_loss: 0.1210 (0.1241)  time: 0.1945  data: 0.0002  max mem: 5511
[09:45:47.622776] Epoch: [53]  [440/781]  eta: 0:01:07  lr: 0.000121  training_loss: 0.3328 (0.3382)  mae_loss: 0.2153 (0.2142)  classification_loss: 0.1173 (0.1240)  time: 0.1952  data: 0.0002  max mem: 5511
[09:45:51.536381] Epoch: [53]  [460/781]  eta: 0:01:03  lr: 0.000121  training_loss: 0.3316 (0.3381)  mae_loss: 0.2139 (0.2142)  classification_loss: 0.1190 (0.1240)  time: 0.1956  data: 0.0002  max mem: 5511
[09:45:55.466773] Epoch: [53]  [480/781]  eta: 0:00:59  lr: 0.000121  training_loss: 0.3396 (0.3383)  mae_loss: 0.2152 (0.2143)  classification_loss: 0.1223 (0.1240)  time: 0.1964  data: 0.0002  max mem: 5511
[09:45:59.371122] Epoch: [53]  [500/781]  eta: 0:00:55  lr: 0.000121  training_loss: 0.3374 (0.3386)  mae_loss: 0.2138 (0.2144)  classification_loss: 0.1247 (0.1241)  time: 0.1951  data: 0.0002  max mem: 5511
[09:46:03.311215] Epoch: [53]  [520/781]  eta: 0:00:51  lr: 0.000121  training_loss: 0.3471 (0.3388)  mae_loss: 0.2199 (0.2146)  classification_loss: 0.1242 (0.1242)  time: 0.1969  data: 0.0003  max mem: 5511
[09:46:07.243801] Epoch: [53]  [540/781]  eta: 0:00:47  lr: 0.000121  training_loss: 0.3288 (0.3389)  mae_loss: 0.2106 (0.2146)  classification_loss: 0.1227 (0.1242)  time: 0.1965  data: 0.0003  max mem: 5511
[09:46:11.146116] Epoch: [53]  [560/781]  eta: 0:00:43  lr: 0.000120  training_loss: 0.3241 (0.3388)  mae_loss: 0.2128 (0.2146)  classification_loss: 0.1206 (0.1241)  time: 0.1950  data: 0.0003  max mem: 5511
[09:46:15.083254] Epoch: [53]  [580/781]  eta: 0:00:39  lr: 0.000120  training_loss: 0.3510 (0.3392)  mae_loss: 0.2307 (0.2152)  classification_loss: 0.1184 (0.1241)  time: 0.1968  data: 0.0004  max mem: 5511
[09:46:18.969965] Epoch: [53]  [600/781]  eta: 0:00:35  lr: 0.000120  training_loss: 0.3434 (0.3394)  mae_loss: 0.2171 (0.2153)  classification_loss: 0.1263 (0.1241)  time: 0.1943  data: 0.0002  max mem: 5511
[09:46:22.903961] Epoch: [53]  [620/781]  eta: 0:00:31  lr: 0.000120  training_loss: 0.3372 (0.3395)  mae_loss: 0.2135 (0.2154)  classification_loss: 0.1223 (0.1241)  time: 0.1966  data: 0.0002  max mem: 5511
[09:46:26.821056] Epoch: [53]  [640/781]  eta: 0:00:27  lr: 0.000120  training_loss: 0.3261 (0.3392)  mae_loss: 0.2078 (0.2152)  classification_loss: 0.1207 (0.1241)  time: 0.1958  data: 0.0002  max mem: 5511
[09:46:30.763483] Epoch: [53]  [660/781]  eta: 0:00:23  lr: 0.000120  training_loss: 0.3286 (0.3389)  mae_loss: 0.2072 (0.2149)  classification_loss: 0.1229 (0.1241)  time: 0.1970  data: 0.0002  max mem: 5511
[09:46:34.671891] Epoch: [53]  [680/781]  eta: 0:00:19  lr: 0.000120  training_loss: 0.3324 (0.3390)  mae_loss: 0.2153 (0.2150)  classification_loss: 0.1239 (0.1241)  time: 0.1953  data: 0.0002  max mem: 5511
[09:46:38.576625] Epoch: [53]  [700/781]  eta: 0:00:15  lr: 0.000120  training_loss: 0.3475 (0.3393)  mae_loss: 0.2152 (0.2151)  classification_loss: 0.1259 (0.1242)  time: 0.1952  data: 0.0003  max mem: 5511
[09:46:42.495082] Epoch: [53]  [720/781]  eta: 0:00:12  lr: 0.000120  training_loss: 0.3496 (0.3395)  mae_loss: 0.2160 (0.2152)  classification_loss: 0.1304 (0.1243)  time: 0.1958  data: 0.0003  max mem: 5511
[09:46:46.394832] Epoch: [53]  [740/781]  eta: 0:00:08  lr: 0.000120  training_loss: 0.3317 (0.3395)  mae_loss: 0.2117 (0.2152)  classification_loss: 0.1216 (0.1243)  time: 0.1949  data: 0.0003  max mem: 5511
[09:46:50.342398] Epoch: [53]  [760/781]  eta: 0:00:04  lr: 0.000119  training_loss: 0.3269 (0.3392)  mae_loss: 0.2018 (0.2150)  classification_loss: 0.1248 (0.1243)  time: 0.1973  data: 0.0002  max mem: 5511
[09:46:54.227967] Epoch: [53]  [780/781]  eta: 0:00:00  lr: 0.000119  training_loss: 0.3389 (0.3393)  mae_loss: 0.2123 (0.2150)  classification_loss: 0.1248 (0.1243)  time: 0.1942  data: 0.0002  max mem: 5511
[09:46:54.390497] Epoch: [53] Total time: 0:02:33 (0.1970 s / it)
[09:46:54.390942] Averaged stats: lr: 0.000119  training_loss: 0.3389 (0.3393)  mae_loss: 0.2123 (0.2150)  classification_loss: 0.1248 (0.1243)
[09:46:54.975272] Test:  [  0/157]  eta: 0:01:31  testing_loss: 0.5733 (0.5733)  acc1: 84.3750 (84.3750)  acc5: 98.4375 (98.4375)  time: 0.5798  data: 0.5390  max mem: 5511
[09:46:55.264232] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.5733 (0.5656)  acc1: 81.2500 (81.3920)  acc5: 100.0000 (99.4318)  time: 0.0788  data: 0.0492  max mem: 5511
[09:46:55.551059] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.5048 (0.5327)  acc1: 82.8125 (82.8869)  acc5: 100.0000 (99.5536)  time: 0.0286  data: 0.0002  max mem: 5511
[09:46:55.835916] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.4772 (0.5337)  acc1: 84.3750 (83.1149)  acc5: 100.0000 (99.2944)  time: 0.0284  data: 0.0002  max mem: 5511
[09:46:56.117969] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.5220 (0.5411)  acc1: 84.3750 (83.2317)  acc5: 98.4375 (99.1997)  time: 0.0282  data: 0.0002  max mem: 5511
[09:46:56.404458] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5200 (0.5343)  acc1: 84.3750 (83.6091)  acc5: 98.4375 (99.2341)  time: 0.0283  data: 0.0002  max mem: 5511
[09:46:56.689014] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.5004 (0.5299)  acc1: 84.3750 (83.7859)  acc5: 100.0000 (99.2316)  time: 0.0284  data: 0.0002  max mem: 5511
[09:46:56.972753] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4754 (0.5217)  acc1: 84.3750 (84.0229)  acc5: 100.0000 (99.2298)  time: 0.0283  data: 0.0002  max mem: 5511
[09:46:57.253770] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4918 (0.5250)  acc1: 82.8125 (83.7384)  acc5: 100.0000 (99.2284)  time: 0.0281  data: 0.0002  max mem: 5511
[09:46:57.535150] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4987 (0.5203)  acc1: 82.8125 (83.9114)  acc5: 100.0000 (99.2617)  time: 0.0280  data: 0.0002  max mem: 5511
[09:46:57.817701] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5077 (0.5250)  acc1: 84.3750 (83.7562)  acc5: 100.0000 (99.2884)  time: 0.0281  data: 0.0002  max mem: 5511
[09:46:58.099823] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5187 (0.5250)  acc1: 82.8125 (83.7275)  acc5: 100.0000 (99.2539)  time: 0.0281  data: 0.0002  max mem: 5511
[09:46:58.382633] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4654 (0.5211)  acc1: 85.9375 (83.8714)  acc5: 100.0000 (99.2769)  time: 0.0281  data: 0.0002  max mem: 5511
[09:46:58.664314] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4656 (0.5234)  acc1: 84.3750 (83.7548)  acc5: 100.0000 (99.3082)  time: 0.0281  data: 0.0002  max mem: 5511
[09:46:58.943954] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5031 (0.5227)  acc1: 84.3750 (83.8209)  acc5: 100.0000 (99.3240)  time: 0.0280  data: 0.0001  max mem: 5511
[09:46:59.222278] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5189 (0.5222)  acc1: 84.3750 (83.8266)  acc5: 100.0000 (99.3377)  time: 0.0278  data: 0.0001  max mem: 5511
[09:46:59.371326] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5009 (0.5212)  acc1: 82.8125 (83.7500)  acc5: 100.0000 (99.3400)  time: 0.0268  data: 0.0001  max mem: 5511
[09:46:59.536627] Test: Total time: 0:00:05 (0.0328 s / it)
[09:46:59.537215] * Acc@1 83.750 Acc@5 99.340 loss 0.521
[09:46:59.537585] Accuracy of the network on the 10000 test images: 83.8%
[09:46:59.537834] Max accuracy: 84.02%
[09:46:59.862758] log_dir: ./output_dir
[09:47:00.652517] Epoch: [54]  [  0/781]  eta: 0:10:15  lr: 0.000119  training_loss: 0.3582 (0.3582)  mae_loss: 0.2369 (0.2369)  classification_loss: 0.1213 (0.1213)  time: 0.7879  data: 0.5641  max mem: 5511
[09:47:04.597402] Epoch: [54]  [ 20/781]  eta: 0:02:51  lr: 0.000119  training_loss: 0.3232 (0.3241)  mae_loss: 0.2022 (0.2028)  classification_loss: 0.1191 (0.1213)  time: 0.1971  data: 0.0002  max mem: 5511
[09:47:08.494213] Epoch: [54]  [ 40/781]  eta: 0:02:35  lr: 0.000119  training_loss: 0.3424 (0.3362)  mae_loss: 0.2151 (0.2125)  classification_loss: 0.1229 (0.1238)  time: 0.1948  data: 0.0002  max mem: 5511
[09:47:12.394848] Epoch: [54]  [ 60/781]  eta: 0:02:28  lr: 0.000119  training_loss: 0.3450 (0.3407)  mae_loss: 0.2140 (0.2163)  classification_loss: 0.1248 (0.1244)  time: 0.1950  data: 0.0003  max mem: 5511
[09:47:16.285856] Epoch: [54]  [ 80/781]  eta: 0:02:22  lr: 0.000119  training_loss: 0.3326 (0.3401)  mae_loss: 0.2141 (0.2163)  classification_loss: 0.1227 (0.1237)  time: 0.1945  data: 0.0002  max mem: 5511
[09:47:20.209130] Epoch: [54]  [100/781]  eta: 0:02:17  lr: 0.000119  training_loss: 0.3253 (0.3413)  mae_loss: 0.2075 (0.2170)  classification_loss: 0.1239 (0.1242)  time: 0.1958  data: 0.0003  max mem: 5511
[09:47:24.112467] Epoch: [54]  [120/781]  eta: 0:02:12  lr: 0.000119  training_loss: 0.3239 (0.3395)  mae_loss: 0.1953 (0.2155)  classification_loss: 0.1213 (0.1240)  time: 0.1951  data: 0.0002  max mem: 5511
[09:47:28.007881] Epoch: [54]  [140/781]  eta: 0:02:07  lr: 0.000119  training_loss: 0.3338 (0.3384)  mae_loss: 0.2084 (0.2147)  classification_loss: 0.1206 (0.1237)  time: 0.1947  data: 0.0002  max mem: 5511
[09:47:31.920954] Epoch: [54]  [160/781]  eta: 0:02:03  lr: 0.000118  training_loss: 0.3446 (0.3388)  mae_loss: 0.2175 (0.2152)  classification_loss: 0.1233 (0.1236)  time: 0.1956  data: 0.0002  max mem: 5511
[09:47:35.840230] Epoch: [54]  [180/781]  eta: 0:01:59  lr: 0.000118  training_loss: 0.3439 (0.3400)  mae_loss: 0.2196 (0.2163)  classification_loss: 0.1242 (0.1236)  time: 0.1959  data: 0.0002  max mem: 5511
[09:47:39.816664] Epoch: [54]  [200/781]  eta: 0:01:55  lr: 0.000118  training_loss: 0.3289 (0.3390)  mae_loss: 0.1964 (0.2153)  classification_loss: 0.1249 (0.1238)  time: 0.1987  data: 0.0002  max mem: 5511
[09:47:43.759592] Epoch: [54]  [220/781]  eta: 0:01:51  lr: 0.000118  training_loss: 0.3480 (0.3401)  mae_loss: 0.2094 (0.2156)  classification_loss: 0.1318 (0.1245)  time: 0.1970  data: 0.0002  max mem: 5511
[09:47:47.652312] Epoch: [54]  [240/781]  eta: 0:01:47  lr: 0.000118  training_loss: 0.3411 (0.3395)  mae_loss: 0.2040 (0.2150)  classification_loss: 0.1248 (0.1244)  time: 0.1945  data: 0.0002  max mem: 5511
[09:47:51.562779] Epoch: [54]  [260/781]  eta: 0:01:43  lr: 0.000118  training_loss: 0.3403 (0.3402)  mae_loss: 0.2129 (0.2155)  classification_loss: 0.1269 (0.1246)  time: 0.1954  data: 0.0004  max mem: 5511
[09:47:55.488156] Epoch: [54]  [280/781]  eta: 0:01:39  lr: 0.000118  training_loss: 0.3309 (0.3405)  mae_loss: 0.2135 (0.2161)  classification_loss: 0.1225 (0.1244)  time: 0.1962  data: 0.0002  max mem: 5511
[09:47:59.384411] Epoch: [54]  [300/781]  eta: 0:01:35  lr: 0.000118  training_loss: 0.3421 (0.3413)  mae_loss: 0.2169 (0.2168)  classification_loss: 0.1240 (0.1244)  time: 0.1947  data: 0.0002  max mem: 5511
[09:48:03.301228] Epoch: [54]  [320/781]  eta: 0:01:31  lr: 0.000118  training_loss: 0.3348 (0.3409)  mae_loss: 0.2082 (0.2164)  classification_loss: 0.1265 (0.1245)  time: 0.1957  data: 0.0002  max mem: 5511
[09:48:07.235553] Epoch: [54]  [340/781]  eta: 0:01:27  lr: 0.000118  training_loss: 0.3534 (0.3418)  mae_loss: 0.2338 (0.2174)  classification_loss: 0.1223 (0.1244)  time: 0.1966  data: 0.0003  max mem: 5511
[09:48:11.148574] Epoch: [54]  [360/781]  eta: 0:01:23  lr: 0.000117  training_loss: 0.3428 (0.3416)  mae_loss: 0.2209 (0.2173)  classification_loss: 0.1239 (0.1243)  time: 0.1956  data: 0.0002  max mem: 5511
[09:48:15.070455] Epoch: [54]  [380/781]  eta: 0:01:19  lr: 0.000117  training_loss: 0.3411 (0.3417)  mae_loss: 0.2158 (0.2174)  classification_loss: 0.1231 (0.1243)  time: 0.1960  data: 0.0005  max mem: 5511
[09:48:18.965748] Epoch: [54]  [400/781]  eta: 0:01:15  lr: 0.000117  training_loss: 0.3195 (0.3406)  mae_loss: 0.2017 (0.2166)  classification_loss: 0.1163 (0.1240)  time: 0.1947  data: 0.0002  max mem: 5511
[09:48:22.910036] Epoch: [54]  [420/781]  eta: 0:01:11  lr: 0.000117  training_loss: 0.3383 (0.3407)  mae_loss: 0.2117 (0.2165)  classification_loss: 0.1272 (0.1242)  time: 0.1971  data: 0.0003  max mem: 5511
[09:48:26.841281] Epoch: [54]  [440/781]  eta: 0:01:07  lr: 0.000117  training_loss: 0.3485 (0.3408)  mae_loss: 0.2307 (0.2168)  classification_loss: 0.1195 (0.1240)  time: 0.1965  data: 0.0002  max mem: 5511
[09:48:30.735896] Epoch: [54]  [460/781]  eta: 0:01:03  lr: 0.000117  training_loss: 0.3282 (0.3401)  mae_loss: 0.2092 (0.2164)  classification_loss: 0.1173 (0.1237)  time: 0.1946  data: 0.0002  max mem: 5511
[09:48:34.639134] Epoch: [54]  [480/781]  eta: 0:00:59  lr: 0.000117  training_loss: 0.3214 (0.3399)  mae_loss: 0.2037 (0.2161)  classification_loss: 0.1234 (0.1238)  time: 0.1951  data: 0.0004  max mem: 5511
[09:48:38.562354] Epoch: [54]  [500/781]  eta: 0:00:55  lr: 0.000117  training_loss: 0.3407 (0.3402)  mae_loss: 0.2188 (0.2165)  classification_loss: 0.1237 (0.1238)  time: 0.1961  data: 0.0002  max mem: 5511
[09:48:42.494557] Epoch: [54]  [520/781]  eta: 0:00:51  lr: 0.000117  training_loss: 0.3383 (0.3403)  mae_loss: 0.2160 (0.2165)  classification_loss: 0.1220 (0.1238)  time: 0.1965  data: 0.0002  max mem: 5511
[09:48:46.387396] Epoch: [54]  [540/781]  eta: 0:00:47  lr: 0.000116  training_loss: 0.3491 (0.3406)  mae_loss: 0.2251 (0.2168)  classification_loss: 0.1215 (0.1238)  time: 0.1946  data: 0.0002  max mem: 5511
[09:48:50.294982] Epoch: [54]  [560/781]  eta: 0:00:43  lr: 0.000116  training_loss: 0.3293 (0.3403)  mae_loss: 0.2100 (0.2166)  classification_loss: 0.1223 (0.1237)  time: 0.1953  data: 0.0002  max mem: 5511
[09:48:54.218683] Epoch: [54]  [580/781]  eta: 0:00:39  lr: 0.000116  training_loss: 0.3362 (0.3403)  mae_loss: 0.2152 (0.2165)  classification_loss: 0.1211 (0.1237)  time: 0.1961  data: 0.0003  max mem: 5511
[09:48:58.112105] Epoch: [54]  [600/781]  eta: 0:00:35  lr: 0.000116  training_loss: 0.3417 (0.3401)  mae_loss: 0.2186 (0.2164)  classification_loss: 0.1231 (0.1237)  time: 0.1946  data: 0.0002  max mem: 5511
[09:49:02.024887] Epoch: [54]  [620/781]  eta: 0:00:31  lr: 0.000116  training_loss: 0.3294 (0.3399)  mae_loss: 0.2053 (0.2163)  classification_loss: 0.1203 (0.1236)  time: 0.1955  data: 0.0002  max mem: 5511
[09:49:05.944179] Epoch: [54]  [640/781]  eta: 0:00:27  lr: 0.000116  training_loss: 0.3433 (0.3401)  mae_loss: 0.2151 (0.2164)  classification_loss: 0.1242 (0.1237)  time: 0.1959  data: 0.0003  max mem: 5511
[09:49:09.846112] Epoch: [54]  [660/781]  eta: 0:00:23  lr: 0.000116  training_loss: 0.3468 (0.3404)  mae_loss: 0.2157 (0.2166)  classification_loss: 0.1248 (0.1238)  time: 0.1950  data: 0.0002  max mem: 5511
[09:49:13.746590] Epoch: [54]  [680/781]  eta: 0:00:19  lr: 0.000116  training_loss: 0.3372 (0.3406)  mae_loss: 0.2052 (0.2168)  classification_loss: 0.1262 (0.1239)  time: 0.1949  data: 0.0002  max mem: 5511
[09:49:17.648819] Epoch: [54]  [700/781]  eta: 0:00:15  lr: 0.000116  training_loss: 0.3380 (0.3404)  mae_loss: 0.2156 (0.2166)  classification_loss: 0.1204 (0.1238)  time: 0.1950  data: 0.0002  max mem: 5511
[09:49:21.548149] Epoch: [54]  [720/781]  eta: 0:00:11  lr: 0.000116  training_loss: 0.3370 (0.3404)  mae_loss: 0.2128 (0.2166)  classification_loss: 0.1262 (0.1239)  time: 0.1949  data: 0.0002  max mem: 5511
[09:49:25.448049] Epoch: [54]  [740/781]  eta: 0:00:08  lr: 0.000115  training_loss: 0.3450 (0.3406)  mae_loss: 0.2183 (0.2166)  classification_loss: 0.1250 (0.1239)  time: 0.1949  data: 0.0002  max mem: 5511
[09:49:29.339083] Epoch: [54]  [760/781]  eta: 0:00:04  lr: 0.000115  training_loss: 0.3312 (0.3404)  mae_loss: 0.2067 (0.2164)  classification_loss: 0.1262 (0.1240)  time: 0.1945  data: 0.0003  max mem: 5511
[09:49:33.214866] Epoch: [54]  [780/781]  eta: 0:00:00  lr: 0.000115  training_loss: 0.3416 (0.3406)  mae_loss: 0.2164 (0.2165)  classification_loss: 0.1270 (0.1241)  time: 0.1937  data: 0.0002  max mem: 5511
[09:49:33.385940] Epoch: [54] Total time: 0:02:33 (0.1966 s / it)
[09:49:33.386538] Averaged stats: lr: 0.000115  training_loss: 0.3416 (0.3406)  mae_loss: 0.2164 (0.2165)  classification_loss: 0.1270 (0.1241)
[09:49:33.969661] Test:  [  0/157]  eta: 0:01:30  testing_loss: 0.5405 (0.5405)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.5783  data: 0.5436  max mem: 5511
[09:49:34.255933] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.5433 (0.5485)  acc1: 82.8125 (83.6648)  acc5: 100.0000 (99.5739)  time: 0.0784  data: 0.0496  max mem: 5511
[09:49:34.546999] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.5134 (0.5159)  acc1: 84.3750 (85.1935)  acc5: 100.0000 (99.4792)  time: 0.0287  data: 0.0002  max mem: 5511
[09:49:34.829100] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.5133 (0.5274)  acc1: 85.9375 (85.2319)  acc5: 100.0000 (99.2440)  time: 0.0285  data: 0.0001  max mem: 5511
[09:49:35.111361] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.5267 (0.5364)  acc1: 84.3750 (84.4131)  acc5: 100.0000 (99.2378)  time: 0.0281  data: 0.0001  max mem: 5511
[09:49:35.394651] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5088 (0.5281)  acc1: 84.3750 (84.5588)  acc5: 100.0000 (99.2341)  time: 0.0281  data: 0.0002  max mem: 5511
[09:49:35.677329] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4739 (0.5254)  acc1: 85.9375 (84.6824)  acc5: 100.0000 (99.1803)  time: 0.0281  data: 0.0002  max mem: 5511
[09:49:35.959892] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4643 (0.5178)  acc1: 85.9375 (84.9032)  acc5: 100.0000 (99.2077)  time: 0.0281  data: 0.0002  max mem: 5511
[09:49:36.242481] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4933 (0.5229)  acc1: 85.9375 (84.6451)  acc5: 100.0000 (99.1705)  time: 0.0281  data: 0.0001  max mem: 5511
[09:49:36.530992] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4964 (0.5194)  acc1: 85.9375 (84.8043)  acc5: 100.0000 (99.1930)  time: 0.0281  data: 0.0001  max mem: 5511
[09:49:36.814862] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.5037 (0.5229)  acc1: 85.9375 (84.5297)  acc5: 100.0000 (99.2265)  time: 0.0282  data: 0.0002  max mem: 5511
[09:49:37.096397] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5041 (0.5221)  acc1: 85.9375 (84.6143)  acc5: 100.0000 (99.1976)  time: 0.0282  data: 0.0002  max mem: 5511
[09:49:37.377645] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4683 (0.5167)  acc1: 85.9375 (84.7882)  acc5: 100.0000 (99.2252)  time: 0.0280  data: 0.0001  max mem: 5511
[09:49:37.659003] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4784 (0.5201)  acc1: 84.3750 (84.6255)  acc5: 100.0000 (99.2486)  time: 0.0280  data: 0.0001  max mem: 5511
[09:49:37.939185] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5371 (0.5208)  acc1: 82.8125 (84.5080)  acc5: 100.0000 (99.2575)  time: 0.0280  data: 0.0001  max mem: 5511
[09:49:38.218177] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5389 (0.5212)  acc1: 82.8125 (84.4474)  acc5: 100.0000 (99.2653)  time: 0.0278  data: 0.0001  max mem: 5511
[09:49:38.367776] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.5154 (0.5206)  acc1: 82.8125 (84.5100)  acc5: 100.0000 (99.2600)  time: 0.0269  data: 0.0001  max mem: 5511
[09:49:38.521986] Test: Total time: 0:00:05 (0.0327 s / it)
[09:49:38.522779] * Acc@1 84.510 Acc@5 99.260 loss 0.521
[09:49:38.523117] Accuracy of the network on the 10000 test images: 84.5%
[09:49:38.523365] Max accuracy: 84.51%
[09:49:38.928971] log_dir: ./output_dir
[09:49:39.817171] Epoch: [55]  [  0/781]  eta: 0:11:32  lr: 0.000115  training_loss: 0.3215 (0.3215)  mae_loss: 0.1942 (0.1942)  classification_loss: 0.1274 (0.1274)  time: 0.8864  data: 0.6696  max mem: 5511
[09:49:43.744082] Epoch: [55]  [ 20/781]  eta: 0:02:54  lr: 0.000115  training_loss: 0.3528 (0.3491)  mae_loss: 0.2253 (0.2254)  classification_loss: 0.1217 (0.1237)  time: 0.1962  data: 0.0002  max mem: 5511
[09:49:47.667128] Epoch: [55]  [ 40/781]  eta: 0:02:37  lr: 0.000115  training_loss: 0.3372 (0.3449)  mae_loss: 0.2159 (0.2210)  classification_loss: 0.1230 (0.1240)  time: 0.1961  data: 0.0002  max mem: 5511
[09:49:51.631984] Epoch: [55]  [ 60/781]  eta: 0:02:30  lr: 0.000115  training_loss: 0.3370 (0.3429)  mae_loss: 0.2118 (0.2192)  classification_loss: 0.1234 (0.1236)  time: 0.1982  data: 0.0003  max mem: 5511
[09:49:55.546824] Epoch: [55]  [ 80/781]  eta: 0:02:23  lr: 0.000115  training_loss: 0.3276 (0.3414)  mae_loss: 0.2086 (0.2180)  classification_loss: 0.1194 (0.1234)  time: 0.1956  data: 0.0002  max mem: 5511
[09:49:59.452753] Epoch: [55]  [100/781]  eta: 0:02:18  lr: 0.000115  training_loss: 0.3423 (0.3410)  mae_loss: 0.2130 (0.2174)  classification_loss: 0.1216 (0.1237)  time: 0.1952  data: 0.0003  max mem: 5511
[09:50:03.365669] Epoch: [55]  [120/781]  eta: 0:02:13  lr: 0.000115  training_loss: 0.3354 (0.3406)  mae_loss: 0.2078 (0.2165)  classification_loss: 0.1248 (0.1240)  time: 0.1955  data: 0.0002  max mem: 5511
[09:50:07.272815] Epoch: [55]  [140/781]  eta: 0:02:08  lr: 0.000114  training_loss: 0.3323 (0.3398)  mae_loss: 0.2103 (0.2162)  classification_loss: 0.1226 (0.1236)  time: 0.1953  data: 0.0003  max mem: 5511
[09:50:11.172109] Epoch: [55]  [160/781]  eta: 0:02:04  lr: 0.000114  training_loss: 0.3279 (0.3385)  mae_loss: 0.2113 (0.2153)  classification_loss: 0.1190 (0.1232)  time: 0.1949  data: 0.0002  max mem: 5511
[09:50:15.066611] Epoch: [55]  [180/781]  eta: 0:01:59  lr: 0.000114  training_loss: 0.3435 (0.3389)  mae_loss: 0.2217 (0.2160)  classification_loss: 0.1194 (0.1229)  time: 0.1946  data: 0.0002  max mem: 5511
[09:50:18.969369] Epoch: [55]  [200/781]  eta: 0:01:55  lr: 0.000114  training_loss: 0.3434 (0.3390)  mae_loss: 0.2146 (0.2159)  classification_loss: 0.1224 (0.1230)  time: 0.1951  data: 0.0003  max mem: 5511
[09:50:22.875202] Epoch: [55]  [220/781]  eta: 0:01:51  lr: 0.000114  training_loss: 0.3399 (0.3390)  mae_loss: 0.2148 (0.2158)  classification_loss: 0.1240 (0.1232)  time: 0.1952  data: 0.0002  max mem: 5511
[09:50:26.804495] Epoch: [55]  [240/781]  eta: 0:01:47  lr: 0.000114  training_loss: 0.3433 (0.3390)  mae_loss: 0.2238 (0.2161)  classification_loss: 0.1192 (0.1229)  time: 0.1964  data: 0.0002  max mem: 5511
[09:50:30.712799] Epoch: [55]  [260/781]  eta: 0:01:43  lr: 0.000114  training_loss: 0.3342 (0.3389)  mae_loss: 0.2090 (0.2158)  classification_loss: 0.1225 (0.1230)  time: 0.1953  data: 0.0002  max mem: 5511
[09:50:34.618452] Epoch: [55]  [280/781]  eta: 0:01:39  lr: 0.000114  training_loss: 0.3411 (0.3392)  mae_loss: 0.2165 (0.2163)  classification_loss: 0.1199 (0.1229)  time: 0.1952  data: 0.0002  max mem: 5511
[09:50:38.544084] Epoch: [55]  [300/781]  eta: 0:01:35  lr: 0.000114  training_loss: 0.3309 (0.3390)  mae_loss: 0.2060 (0.2159)  classification_loss: 0.1260 (0.1231)  time: 0.1961  data: 0.0002  max mem: 5511
[09:50:42.453925] Epoch: [55]  [320/781]  eta: 0:01:31  lr: 0.000114  training_loss: 0.3615 (0.3400)  mae_loss: 0.2243 (0.2168)  classification_loss: 0.1252 (0.1232)  time: 0.1954  data: 0.0003  max mem: 5511
[09:50:46.388828] Epoch: [55]  [340/781]  eta: 0:01:27  lr: 0.000113  training_loss: 0.3364 (0.3399)  mae_loss: 0.2111 (0.2167)  classification_loss: 0.1218 (0.1232)  time: 0.1967  data: 0.0005  max mem: 5511
[09:50:50.311293] Epoch: [55]  [360/781]  eta: 0:01:23  lr: 0.000113  training_loss: 0.3338 (0.3398)  mae_loss: 0.2087 (0.2165)  classification_loss: 0.1262 (0.1233)  time: 0.1960  data: 0.0002  max mem: 5511
[09:50:54.215246] Epoch: [55]  [380/781]  eta: 0:01:19  lr: 0.000113  training_loss: 0.3284 (0.3399)  mae_loss: 0.2081 (0.2165)  classification_loss: 0.1214 (0.1234)  time: 0.1951  data: 0.0004  max mem: 5511
[09:50:58.112352] Epoch: [55]  [400/781]  eta: 0:01:15  lr: 0.000113  training_loss: 0.3323 (0.3398)  mae_loss: 0.2112 (0.2166)  classification_loss: 0.1213 (0.1232)  time: 0.1948  data: 0.0003  max mem: 5511
[09:51:02.032503] Epoch: [55]  [420/781]  eta: 0:01:11  lr: 0.000113  training_loss: 0.3417 (0.3399)  mae_loss: 0.2078 (0.2165)  classification_loss: 0.1225 (0.1234)  time: 0.1959  data: 0.0002  max mem: 5511
[09:51:05.949357] Epoch: [55]  [440/781]  eta: 0:01:07  lr: 0.000113  training_loss: 0.3383 (0.3397)  mae_loss: 0.2096 (0.2163)  classification_loss: 0.1255 (0.1233)  time: 0.1958  data: 0.0008  max mem: 5511
[09:51:09.839270] Epoch: [55]  [460/781]  eta: 0:01:03  lr: 0.000113  training_loss: 0.3239 (0.3394)  mae_loss: 0.2058 (0.2162)  classification_loss: 0.1215 (0.1232)  time: 0.1944  data: 0.0002  max mem: 5511
[09:51:13.720210] Epoch: [55]  [480/781]  eta: 0:00:59  lr: 0.000113  training_loss: 0.3302 (0.3395)  mae_loss: 0.2058 (0.2161)  classification_loss: 0.1240 (0.1234)  time: 0.1939  data: 0.0003  max mem: 5511
[09:51:17.661832] Epoch: [55]  [500/781]  eta: 0:00:55  lr: 0.000113  training_loss: 0.3376 (0.3394)  mae_loss: 0.2134 (0.2161)  classification_loss: 0.1208 (0.1233)  time: 0.1970  data: 0.0002  max mem: 5511
[09:51:21.568191] Epoch: [55]  [520/781]  eta: 0:00:51  lr: 0.000112  training_loss: 0.3312 (0.3394)  mae_loss: 0.2111 (0.2161)  classification_loss: 0.1190 (0.1233)  time: 0.1952  data: 0.0002  max mem: 5511
[09:51:25.461327] Epoch: [55]  [540/781]  eta: 0:00:47  lr: 0.000112  training_loss: 0.3417 (0.3399)  mae_loss: 0.2178 (0.2164)  classification_loss: 0.1295 (0.1234)  time: 0.1946  data: 0.0002  max mem: 5511
[09:51:29.354125] Epoch: [55]  [560/781]  eta: 0:00:43  lr: 0.000112  training_loss: 0.3394 (0.3401)  mae_loss: 0.2182 (0.2167)  classification_loss: 0.1197 (0.1233)  time: 0.1946  data: 0.0002  max mem: 5511
[09:51:33.301835] Epoch: [55]  [580/781]  eta: 0:00:39  lr: 0.000112  training_loss: 0.3452 (0.3400)  mae_loss: 0.2119 (0.2167)  classification_loss: 0.1199 (0.1233)  time: 0.1973  data: 0.0002  max mem: 5511
[09:51:37.198377] Epoch: [55]  [600/781]  eta: 0:00:35  lr: 0.000112  training_loss: 0.3395 (0.3398)  mae_loss: 0.2190 (0.2168)  classification_loss: 0.1167 (0.1231)  time: 0.1947  data: 0.0003  max mem: 5511
[09:51:41.100470] Epoch: [55]  [620/781]  eta: 0:00:31  lr: 0.000112  training_loss: 0.3235 (0.3395)  mae_loss: 0.2010 (0.2165)  classification_loss: 0.1233 (0.1231)  time: 0.1950  data: 0.0002  max mem: 5511
[09:51:44.998783] Epoch: [55]  [640/781]  eta: 0:00:27  lr: 0.000112  training_loss: 0.3441 (0.3395)  mae_loss: 0.2143 (0.2164)  classification_loss: 0.1241 (0.1231)  time: 0.1948  data: 0.0005  max mem: 5511
[09:51:48.888498] Epoch: [55]  [660/781]  eta: 0:00:23  lr: 0.000112  training_loss: 0.3369 (0.3393)  mae_loss: 0.2109 (0.2161)  classification_loss: 0.1265 (0.1232)  time: 0.1944  data: 0.0002  max mem: 5511
[09:51:52.853538] Epoch: [55]  [680/781]  eta: 0:00:19  lr: 0.000112  training_loss: 0.3525 (0.3398)  mae_loss: 0.2365 (0.2166)  classification_loss: 0.1258 (0.1232)  time: 0.1982  data: 0.0002  max mem: 5511
[09:51:56.774807] Epoch: [55]  [700/781]  eta: 0:00:15  lr: 0.000112  training_loss: 0.3323 (0.3398)  mae_loss: 0.2107 (0.2166)  classification_loss: 0.1222 (0.1232)  time: 0.1960  data: 0.0002  max mem: 5511
[09:52:00.672966] Epoch: [55]  [720/781]  eta: 0:00:11  lr: 0.000111  training_loss: 0.3491 (0.3401)  mae_loss: 0.2205 (0.2168)  classification_loss: 0.1221 (0.1233)  time: 0.1948  data: 0.0003  max mem: 5511
[09:52:04.587915] Epoch: [55]  [740/781]  eta: 0:00:08  lr: 0.000111  training_loss: 0.3267 (0.3398)  mae_loss: 0.2024 (0.2166)  classification_loss: 0.1195 (0.1232)  time: 0.1957  data: 0.0002  max mem: 5511
[09:52:08.503928] Epoch: [55]  [760/781]  eta: 0:00:04  lr: 0.000111  training_loss: 0.3210 (0.3395)  mae_loss: 0.2020 (0.2164)  classification_loss: 0.1186 (0.1232)  time: 0.1957  data: 0.0002  max mem: 5511
[09:52:12.392305] Epoch: [55]  [780/781]  eta: 0:00:00  lr: 0.000111  training_loss: 0.3253 (0.3393)  mae_loss: 0.2091 (0.2162)  classification_loss: 0.1245 (0.1232)  time: 0.1943  data: 0.0002  max mem: 5511
[09:52:12.540143] Epoch: [55] Total time: 0:02:33 (0.1967 s / it)
[09:52:12.540762] Averaged stats: lr: 0.000111  training_loss: 0.3253 (0.3393)  mae_loss: 0.2091 (0.2162)  classification_loss: 0.1245 (0.1232)
[09:52:13.084569] Test:  [  0/157]  eta: 0:01:24  testing_loss: 0.5613 (0.5613)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.5391  data: 0.5014  max mem: 5511
[09:52:13.372192] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.5613 (0.5279)  acc1: 82.8125 (83.2386)  acc5: 100.0000 (99.4318)  time: 0.0750  data: 0.0457  max mem: 5511
[09:52:13.656532] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4944 (0.4979)  acc1: 82.8125 (84.3006)  acc5: 100.0000 (99.5536)  time: 0.0284  data: 0.0002  max mem: 5511
[09:52:13.941439] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.4944 (0.5134)  acc1: 84.3750 (83.9718)  acc5: 100.0000 (99.2944)  time: 0.0283  data: 0.0002  max mem: 5511
[09:52:14.227991] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.5088 (0.5166)  acc1: 84.3750 (83.9558)  acc5: 98.4375 (99.2378)  time: 0.0285  data: 0.0003  max mem: 5511
[09:52:14.511636] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4944 (0.5085)  acc1: 84.3750 (84.1912)  acc5: 100.0000 (99.2341)  time: 0.0284  data: 0.0002  max mem: 5511
[09:52:14.796299] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4772 (0.5071)  acc1: 84.3750 (84.2469)  acc5: 100.0000 (99.2316)  time: 0.0283  data: 0.0002  max mem: 5511
[09:52:15.085480] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4557 (0.4981)  acc1: 87.5000 (84.7711)  acc5: 100.0000 (99.2298)  time: 0.0285  data: 0.0002  max mem: 5511
[09:52:15.369552] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4702 (0.5021)  acc1: 87.5000 (84.7415)  acc5: 100.0000 (99.2091)  time: 0.0285  data: 0.0002  max mem: 5511
[09:52:15.660777] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4900 (0.4967)  acc1: 85.9375 (84.9588)  acc5: 98.4375 (99.2102)  time: 0.0286  data: 0.0003  max mem: 5511
[09:52:15.942429] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4950 (0.5013)  acc1: 84.3750 (84.9010)  acc5: 100.0000 (99.2265)  time: 0.0285  data: 0.0003  max mem: 5511
[09:52:16.226458] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4950 (0.4987)  acc1: 85.9375 (85.0507)  acc5: 100.0000 (99.2539)  time: 0.0281  data: 0.0002  max mem: 5511
[09:52:16.516110] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4590 (0.4944)  acc1: 85.9375 (85.2273)  acc5: 100.0000 (99.2510)  time: 0.0285  data: 0.0005  max mem: 5511
[09:52:16.803003] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4715 (0.4956)  acc1: 85.9375 (85.1503)  acc5: 98.4375 (99.2366)  time: 0.0287  data: 0.0005  max mem: 5511
[09:52:17.088889] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5004 (0.4954)  acc1: 84.3750 (85.1396)  acc5: 100.0000 (99.2686)  time: 0.0284  data: 0.0002  max mem: 5511
[09:52:17.367353] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4930 (0.4947)  acc1: 84.3750 (85.1821)  acc5: 100.0000 (99.2757)  time: 0.0280  data: 0.0002  max mem: 5511
[09:52:17.519861] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4623 (0.4943)  acc1: 84.3750 (85.1400)  acc5: 100.0000 (99.2700)  time: 0.0270  data: 0.0001  max mem: 5511
[09:52:17.681995] Test: Total time: 0:00:05 (0.0327 s / it)
[09:52:17.682452] * Acc@1 85.140 Acc@5 99.270 loss 0.494
[09:52:17.682754] Accuracy of the network on the 10000 test images: 85.1%
[09:52:17.682934] Max accuracy: 85.14%
[09:52:17.869698] log_dir: ./output_dir
[09:52:18.637578] Epoch: [56]  [  0/781]  eta: 0:09:58  lr: 0.000111  training_loss: 0.3611 (0.3611)  mae_loss: 0.2576 (0.2576)  classification_loss: 0.1035 (0.1035)  time: 0.7662  data: 0.5496  max mem: 5511
[09:52:22.541603] Epoch: [56]  [ 20/781]  eta: 0:02:49  lr: 0.000111  training_loss: 0.3358 (0.3327)  mae_loss: 0.2100 (0.2154)  classification_loss: 0.1159 (0.1174)  time: 0.1951  data: 0.0002  max mem: 5511
[09:52:26.463579] Epoch: [56]  [ 40/781]  eta: 0:02:35  lr: 0.000111  training_loss: 0.3170 (0.3291)  mae_loss: 0.1949 (0.2095)  classification_loss: 0.1226 (0.1196)  time: 0.1960  data: 0.0002  max mem: 5511
[09:52:30.363001] Epoch: [56]  [ 60/781]  eta: 0:02:27  lr: 0.000111  training_loss: 0.3262 (0.3280)  mae_loss: 0.2039 (0.2076)  classification_loss: 0.1220 (0.1204)  time: 0.1949  data: 0.0002  max mem: 5511
[09:52:34.309901] Epoch: [56]  [ 80/781]  eta: 0:02:22  lr: 0.000111  training_loss: 0.3336 (0.3314)  mae_loss: 0.2167 (0.2105)  classification_loss: 0.1227 (0.1209)  time: 0.1973  data: 0.0002  max mem: 5511
[09:52:38.205811] Epoch: [56]  [100/781]  eta: 0:02:17  lr: 0.000111  training_loss: 0.3414 (0.3338)  mae_loss: 0.2203 (0.2125)  classification_loss: 0.1215 (0.1213)  time: 0.1947  data: 0.0002  max mem: 5511
[09:52:42.133080] Epoch: [56]  [120/781]  eta: 0:02:12  lr: 0.000110  training_loss: 0.3235 (0.3319)  mae_loss: 0.1946 (0.2107)  classification_loss: 0.1207 (0.1212)  time: 0.1963  data: 0.0003  max mem: 5511
[09:52:46.022204] Epoch: [56]  [140/781]  eta: 0:02:07  lr: 0.000110  training_loss: 0.3384 (0.3321)  mae_loss: 0.2184 (0.2111)  classification_loss: 0.1207 (0.1210)  time: 0.1944  data: 0.0002  max mem: 5511
[09:52:49.988078] Epoch: [56]  [160/781]  eta: 0:02:03  lr: 0.000110  training_loss: 0.3383 (0.3324)  mae_loss: 0.2182 (0.2112)  classification_loss: 0.1188 (0.1211)  time: 0.1982  data: 0.0004  max mem: 5511
[09:52:53.921931] Epoch: [56]  [180/781]  eta: 0:01:59  lr: 0.000110  training_loss: 0.3305 (0.3322)  mae_loss: 0.2118 (0.2112)  classification_loss: 0.1187 (0.1211)  time: 0.1966  data: 0.0004  max mem: 5511
[09:52:57.820723] Epoch: [56]  [200/781]  eta: 0:01:55  lr: 0.000110  training_loss: 0.3253 (0.3324)  mae_loss: 0.2048 (0.2109)  classification_loss: 0.1258 (0.1215)  time: 0.1949  data: 0.0002  max mem: 5511
[09:53:01.739333] Epoch: [56]  [220/781]  eta: 0:01:51  lr: 0.000110  training_loss: 0.3292 (0.3324)  mae_loss: 0.2057 (0.2108)  classification_loss: 0.1231 (0.1216)  time: 0.1959  data: 0.0002  max mem: 5511
[09:53:05.641703] Epoch: [56]  [240/781]  eta: 0:01:47  lr: 0.000110  training_loss: 0.3242 (0.3320)  mae_loss: 0.1980 (0.2104)  classification_loss: 0.1214 (0.1217)  time: 0.1950  data: 0.0003  max mem: 5511
[09:53:09.614759] Epoch: [56]  [260/781]  eta: 0:01:43  lr: 0.000110  training_loss: 0.3432 (0.3325)  mae_loss: 0.2183 (0.2109)  classification_loss: 0.1190 (0.1215)  time: 0.1986  data: 0.0002  max mem: 5511
[09:53:13.517694] Epoch: [56]  [280/781]  eta: 0:01:39  lr: 0.000110  training_loss: 0.3387 (0.3334)  mae_loss: 0.2226 (0.2117)  classification_loss: 0.1181 (0.1217)  time: 0.1950  data: 0.0003  max mem: 5511
[09:53:17.420351] Epoch: [56]  [300/781]  eta: 0:01:35  lr: 0.000110  training_loss: 0.3258 (0.3328)  mae_loss: 0.2086 (0.2114)  classification_loss: 0.1158 (0.1214)  time: 0.1951  data: 0.0002  max mem: 5511
[09:53:21.330318] Epoch: [56]  [320/781]  eta: 0:01:31  lr: 0.000109  training_loss: 0.3298 (0.3330)  mae_loss: 0.2096 (0.2116)  classification_loss: 0.1188 (0.1214)  time: 0.1954  data: 0.0004  max mem: 5511
[09:53:25.233565] Epoch: [56]  [340/781]  eta: 0:01:27  lr: 0.000109  training_loss: 0.3360 (0.3334)  mae_loss: 0.2204 (0.2122)  classification_loss: 0.1140 (0.1212)  time: 0.1951  data: 0.0003  max mem: 5511
[09:53:29.164280] Epoch: [56]  [360/781]  eta: 0:01:23  lr: 0.000109  training_loss: 0.3394 (0.3338)  mae_loss: 0.2095 (0.2124)  classification_loss: 0.1217 (0.1214)  time: 0.1964  data: 0.0003  max mem: 5511
[09:53:33.094927] Epoch: [56]  [380/781]  eta: 0:01:19  lr: 0.000109  training_loss: 0.3316 (0.3342)  mae_loss: 0.2197 (0.2128)  classification_loss: 0.1173 (0.1213)  time: 0.1965  data: 0.0002  max mem: 5511
[09:53:37.006712] Epoch: [56]  [400/781]  eta: 0:01:15  lr: 0.000109  training_loss: 0.3263 (0.3340)  mae_loss: 0.2120 (0.2127)  classification_loss: 0.1221 (0.1213)  time: 0.1955  data: 0.0003  max mem: 5511
[09:53:40.900654] Epoch: [56]  [420/781]  eta: 0:01:11  lr: 0.000109  training_loss: 0.3260 (0.3337)  mae_loss: 0.2020 (0.2123)  classification_loss: 0.1197 (0.1214)  time: 0.1946  data: 0.0002  max mem: 5511
[09:53:44.820084] Epoch: [56]  [440/781]  eta: 0:01:07  lr: 0.000109  training_loss: 0.3301 (0.3338)  mae_loss: 0.2095 (0.2124)  classification_loss: 0.1253 (0.1215)  time: 0.1959  data: 0.0002  max mem: 5511
[09:53:48.736480] Epoch: [56]  [460/781]  eta: 0:01:03  lr: 0.000109  training_loss: 0.3235 (0.3337)  mae_loss: 0.2105 (0.2123)  classification_loss: 0.1197 (0.1214)  time: 0.1957  data: 0.0004  max mem: 5511
[09:53:52.634966] Epoch: [56]  [480/781]  eta: 0:00:59  lr: 0.000109  training_loss: 0.3383 (0.3340)  mae_loss: 0.2107 (0.2125)  classification_loss: 0.1202 (0.1215)  time: 0.1948  data: 0.0002  max mem: 5511
[09:53:56.604746] Epoch: [56]  [500/781]  eta: 0:00:55  lr: 0.000109  training_loss: 0.3430 (0.3342)  mae_loss: 0.2099 (0.2126)  classification_loss: 0.1288 (0.1217)  time: 0.1984  data: 0.0003  max mem: 5511
[09:54:00.492408] Epoch: [56]  [520/781]  eta: 0:00:51  lr: 0.000108  training_loss: 0.3378 (0.3344)  mae_loss: 0.2145 (0.2126)  classification_loss: 0.1233 (0.1218)  time: 0.1943  data: 0.0002  max mem: 5511
[09:54:04.409632] Epoch: [56]  [540/781]  eta: 0:00:47  lr: 0.000108  training_loss: 0.3430 (0.3349)  mae_loss: 0.2160 (0.2131)  classification_loss: 0.1197 (0.1219)  time: 0.1958  data: 0.0002  max mem: 5511
[09:54:08.311498] Epoch: [56]  [560/781]  eta: 0:00:43  lr: 0.000108  training_loss: 0.3314 (0.3350)  mae_loss: 0.2134 (0.2133)  classification_loss: 0.1195 (0.1217)  time: 0.1950  data: 0.0002  max mem: 5511
[09:54:12.210555] Epoch: [56]  [580/781]  eta: 0:00:39  lr: 0.000108  training_loss: 0.3332 (0.3350)  mae_loss: 0.2115 (0.2133)  classification_loss: 0.1171 (0.1217)  time: 0.1948  data: 0.0003  max mem: 5511
[09:54:16.106499] Epoch: [56]  [600/781]  eta: 0:00:35  lr: 0.000108  training_loss: 0.3303 (0.3351)  mae_loss: 0.2189 (0.2133)  classification_loss: 0.1234 (0.1218)  time: 0.1947  data: 0.0004  max mem: 5511
[09:54:20.005763] Epoch: [56]  [620/781]  eta: 0:00:31  lr: 0.000108  training_loss: 0.3357 (0.3352)  mae_loss: 0.2196 (0.2134)  classification_loss: 0.1192 (0.1218)  time: 0.1949  data: 0.0002  max mem: 5511
[09:54:23.925217] Epoch: [56]  [640/781]  eta: 0:00:27  lr: 0.000108  training_loss: 0.3128 (0.3348)  mae_loss: 0.1991 (0.2131)  classification_loss: 0.1204 (0.1217)  time: 0.1959  data: 0.0003  max mem: 5511
[09:54:27.819094] Epoch: [56]  [660/781]  eta: 0:00:23  lr: 0.000108  training_loss: 0.3461 (0.3348)  mae_loss: 0.2144 (0.2132)  classification_loss: 0.1178 (0.1216)  time: 0.1946  data: 0.0004  max mem: 5511
[09:54:31.740888] Epoch: [56]  [680/781]  eta: 0:00:19  lr: 0.000108  training_loss: 0.3338 (0.3348)  mae_loss: 0.2053 (0.2131)  classification_loss: 0.1243 (0.1217)  time: 0.1960  data: 0.0004  max mem: 5511
[09:54:35.626306] Epoch: [56]  [700/781]  eta: 0:00:15  lr: 0.000107  training_loss: 0.3460 (0.3350)  mae_loss: 0.2121 (0.2134)  classification_loss: 0.1223 (0.1217)  time: 0.1942  data: 0.0002  max mem: 5511
[09:54:39.552228] Epoch: [56]  [720/781]  eta: 0:00:11  lr: 0.000107  training_loss: 0.3397 (0.3353)  mae_loss: 0.2109 (0.2135)  classification_loss: 0.1241 (0.1218)  time: 0.1962  data: 0.0002  max mem: 5511
[09:54:43.453066] Epoch: [56]  [740/781]  eta: 0:00:08  lr: 0.000107  training_loss: 0.3314 (0.3351)  mae_loss: 0.2106 (0.2133)  classification_loss: 0.1222 (0.1218)  time: 0.1949  data: 0.0002  max mem: 5511
[09:54:47.411672] Epoch: [56]  [760/781]  eta: 0:00:04  lr: 0.000107  training_loss: 0.3328 (0.3351)  mae_loss: 0.2085 (0.2132)  classification_loss: 0.1268 (0.1219)  time: 0.1978  data: 0.0003  max mem: 5511
[09:54:51.293572] Epoch: [56]  [780/781]  eta: 0:00:00  lr: 0.000107  training_loss: 0.3350 (0.3352)  mae_loss: 0.2095 (0.2132)  classification_loss: 0.1231 (0.1220)  time: 0.1940  data: 0.0002  max mem: 5511
[09:54:51.447871] Epoch: [56] Total time: 0:02:33 (0.1966 s / it)
[09:54:51.448332] Averaged stats: lr: 0.000107  training_loss: 0.3350 (0.3352)  mae_loss: 0.2095 (0.2132)  classification_loss: 0.1231 (0.1220)
[09:54:52.087061] Test:  [  0/157]  eta: 0:01:39  testing_loss: 0.6189 (0.6189)  acc1: 81.2500 (81.2500)  acc5: 100.0000 (100.0000)  time: 0.6349  data: 0.6051  max mem: 5511
[09:54:52.373361] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.5190 (0.5376)  acc1: 81.2500 (82.1023)  acc5: 100.0000 (99.4318)  time: 0.0836  data: 0.0552  max mem: 5511
[09:54:52.663979] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.5068 (0.5097)  acc1: 82.8125 (83.4821)  acc5: 100.0000 (99.4792)  time: 0.0285  data: 0.0002  max mem: 5511
[09:54:52.947422] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4851 (0.5217)  acc1: 84.3750 (83.6694)  acc5: 100.0000 (99.1431)  time: 0.0284  data: 0.0002  max mem: 5511
[09:54:53.234677] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.5032 (0.5252)  acc1: 84.3750 (83.7652)  acc5: 100.0000 (99.1235)  time: 0.0283  data: 0.0003  max mem: 5511
[09:54:53.515931] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4883 (0.5098)  acc1: 85.9375 (84.4669)  acc5: 100.0000 (99.2341)  time: 0.0283  data: 0.0003  max mem: 5511
[09:54:53.802255] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4340 (0.5069)  acc1: 85.9375 (84.6055)  acc5: 100.0000 (99.1803)  time: 0.0283  data: 0.0002  max mem: 5511
[09:54:54.085431] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4267 (0.4966)  acc1: 85.9375 (84.8151)  acc5: 100.0000 (99.2738)  time: 0.0283  data: 0.0002  max mem: 5511
[09:54:54.368060] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4584 (0.5017)  acc1: 84.3750 (84.5486)  acc5: 100.0000 (99.3248)  time: 0.0281  data: 0.0002  max mem: 5511
[09:54:54.651842] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4928 (0.4980)  acc1: 84.3750 (84.7871)  acc5: 100.0000 (99.3819)  time: 0.0282  data: 0.0002  max mem: 5511
[09:54:54.940044] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4799 (0.5011)  acc1: 85.9375 (84.5916)  acc5: 100.0000 (99.3812)  time: 0.0285  data: 0.0002  max mem: 5511
[09:54:55.221401] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.5377 (0.5036)  acc1: 84.3750 (84.5861)  acc5: 100.0000 (99.3666)  time: 0.0284  data: 0.0002  max mem: 5511
[09:54:55.504915] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4918 (0.4994)  acc1: 84.3750 (84.7237)  acc5: 100.0000 (99.3931)  time: 0.0281  data: 0.0002  max mem: 5511
[09:54:55.792259] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4803 (0.5012)  acc1: 85.9375 (84.6851)  acc5: 100.0000 (99.3559)  time: 0.0284  data: 0.0002  max mem: 5511
[09:54:56.072575] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.5191 (0.5012)  acc1: 84.3750 (84.6520)  acc5: 100.0000 (99.3794)  time: 0.0283  data: 0.0002  max mem: 5511
[09:54:56.351650] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5085 (0.5000)  acc1: 84.3750 (84.6544)  acc5: 100.0000 (99.3791)  time: 0.0279  data: 0.0001  max mem: 5511
[09:54:56.502815] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4561 (0.4994)  acc1: 84.3750 (84.6100)  acc5: 100.0000 (99.3800)  time: 0.0270  data: 0.0001  max mem: 5511
[09:54:56.647821] Test: Total time: 0:00:05 (0.0331 s / it)
[09:54:56.648272] * Acc@1 84.610 Acc@5 99.380 loss 0.499
[09:54:56.648588] Accuracy of the network on the 10000 test images: 84.6%
[09:54:56.648768] Max accuracy: 85.14%
[09:54:57.014362] log_dir: ./output_dir
[09:54:57.921134] Epoch: [57]  [  0/781]  eta: 0:11:46  lr: 0.000107  training_loss: 0.3318 (0.3318)  mae_loss: 0.2010 (0.2010)  classification_loss: 0.1308 (0.1308)  time: 0.9049  data: 0.6793  max mem: 5511
[09:55:01.838622] Epoch: [57]  [ 20/781]  eta: 0:02:54  lr: 0.000107  training_loss: 0.3352 (0.3347)  mae_loss: 0.2149 (0.2128)  classification_loss: 0.1226 (0.1219)  time: 0.1958  data: 0.0002  max mem: 5511
[09:55:05.757712] Epoch: [57]  [ 40/781]  eta: 0:02:37  lr: 0.000107  training_loss: 0.3366 (0.3356)  mae_loss: 0.2081 (0.2119)  classification_loss: 0.1244 (0.1237)  time: 0.1959  data: 0.0003  max mem: 5511
[09:55:09.683470] Epoch: [57]  [ 60/781]  eta: 0:02:29  lr: 0.000107  training_loss: 0.3274 (0.3358)  mae_loss: 0.2033 (0.2110)  classification_loss: 0.1273 (0.1248)  time: 0.1962  data: 0.0002  max mem: 5511
[09:55:13.613053] Epoch: [57]  [ 80/781]  eta: 0:02:23  lr: 0.000107  training_loss: 0.3153 (0.3345)  mae_loss: 0.1993 (0.2111)  classification_loss: 0.1199 (0.1234)  time: 0.1964  data: 0.0002  max mem: 5511
[09:55:17.523976] Epoch: [57]  [100/781]  eta: 0:02:18  lr: 0.000107  training_loss: 0.3495 (0.3364)  mae_loss: 0.2196 (0.2129)  classification_loss: 0.1240 (0.1235)  time: 0.1955  data: 0.0003  max mem: 5511
[09:55:21.460915] Epoch: [57]  [120/781]  eta: 0:02:13  lr: 0.000106  training_loss: 0.3228 (0.3349)  mae_loss: 0.1980 (0.2119)  classification_loss: 0.1204 (0.1230)  time: 0.1968  data: 0.0002  max mem: 5511
[09:55:25.390973] Epoch: [57]  [140/781]  eta: 0:02:08  lr: 0.000106  training_loss: 0.3286 (0.3345)  mae_loss: 0.2045 (0.2120)  classification_loss: 0.1166 (0.1225)  time: 0.1964  data: 0.0003  max mem: 5511
[09:55:29.322635] Epoch: [57]  [160/781]  eta: 0:02:04  lr: 0.000106  training_loss: 0.3198 (0.3340)  mae_loss: 0.2005 (0.2114)  classification_loss: 0.1237 (0.1226)  time: 0.1965  data: 0.0002  max mem: 5511
[09:55:33.223188] Epoch: [57]  [180/781]  eta: 0:02:00  lr: 0.000106  training_loss: 0.3349 (0.3349)  mae_loss: 0.2209 (0.2123)  classification_loss: 0.1202 (0.1226)  time: 0.1949  data: 0.0003  max mem: 5511
[09:55:37.189866] Epoch: [57]  [200/781]  eta: 0:01:56  lr: 0.000106  training_loss: 0.3341 (0.3351)  mae_loss: 0.2084 (0.2125)  classification_loss: 0.1212 (0.1225)  time: 0.1982  data: 0.0002  max mem: 5511
[09:55:41.129751] Epoch: [57]  [220/781]  eta: 0:01:51  lr: 0.000106  training_loss: 0.3296 (0.3350)  mae_loss: 0.2116 (0.2127)  classification_loss: 0.1201 (0.1223)  time: 0.1969  data: 0.0002  max mem: 5511
[09:55:45.037779] Epoch: [57]  [240/781]  eta: 0:01:47  lr: 0.000106  training_loss: 0.3376 (0.3353)  mae_loss: 0.2155 (0.2130)  classification_loss: 0.1217 (0.1223)  time: 0.1953  data: 0.0003  max mem: 5511
[09:55:48.964281] Epoch: [57]  [260/781]  eta: 0:01:43  lr: 0.000106  training_loss: 0.3395 (0.3358)  mae_loss: 0.2194 (0.2136)  classification_loss: 0.1202 (0.1221)  time: 0.1963  data: 0.0002  max mem: 5511
[09:55:52.882378] Epoch: [57]  [280/781]  eta: 0:01:39  lr: 0.000106  training_loss: 0.3267 (0.3349)  mae_loss: 0.2023 (0.2128)  classification_loss: 0.1186 (0.1221)  time: 0.1958  data: 0.0002  max mem: 5511
[09:55:56.793447] Epoch: [57]  [300/781]  eta: 0:01:35  lr: 0.000105  training_loss: 0.3369 (0.3356)  mae_loss: 0.2162 (0.2134)  classification_loss: 0.1253 (0.1223)  time: 0.1955  data: 0.0002  max mem: 5511
[09:56:00.730926] Epoch: [57]  [320/781]  eta: 0:01:31  lr: 0.000105  training_loss: 0.3289 (0.3351)  mae_loss: 0.2044 (0.2129)  classification_loss: 0.1202 (0.1222)  time: 0.1968  data: 0.0002  max mem: 5511
[09:56:04.620665] Epoch: [57]  [340/781]  eta: 0:01:27  lr: 0.000105  training_loss: 0.3436 (0.3351)  mae_loss: 0.2081 (0.2129)  classification_loss: 0.1215 (0.1222)  time: 0.1944  data: 0.0003  max mem: 5511
[09:56:08.526635] Epoch: [57]  [360/781]  eta: 0:01:23  lr: 0.000105  training_loss: 0.3291 (0.3352)  mae_loss: 0.2068 (0.2128)  classification_loss: 0.1249 (0.1223)  time: 0.1952  data: 0.0002  max mem: 5511
[09:56:12.451350] Epoch: [57]  [380/781]  eta: 0:01:19  lr: 0.000105  training_loss: 0.3422 (0.3356)  mae_loss: 0.2218 (0.2132)  classification_loss: 0.1237 (0.1224)  time: 0.1962  data: 0.0002  max mem: 5511
[09:56:16.345687] Epoch: [57]  [400/781]  eta: 0:01:15  lr: 0.000105  training_loss: 0.3250 (0.3354)  mae_loss: 0.2050 (0.2130)  classification_loss: 0.1197 (0.1224)  time: 0.1946  data: 0.0003  max mem: 5511
[09:56:20.244032] Epoch: [57]  [420/781]  eta: 0:01:11  lr: 0.000105  training_loss: 0.3303 (0.3353)  mae_loss: 0.2046 (0.2131)  classification_loss: 0.1184 (0.1223)  time: 0.1948  data: 0.0002  max mem: 5511
[09:56:24.156894] Epoch: [57]  [440/781]  eta: 0:01:07  lr: 0.000105  training_loss: 0.3299 (0.3356)  mae_loss: 0.2127 (0.2133)  classification_loss: 0.1189 (0.1223)  time: 0.1956  data: 0.0003  max mem: 5511
[09:56:28.048763] Epoch: [57]  [460/781]  eta: 0:01:03  lr: 0.000105  training_loss: 0.3252 (0.3353)  mae_loss: 0.2082 (0.2131)  classification_loss: 0.1170 (0.1222)  time: 0.1945  data: 0.0002  max mem: 5511
[09:56:31.956640] Epoch: [57]  [480/781]  eta: 0:00:59  lr: 0.000105  training_loss: 0.3239 (0.3350)  mae_loss: 0.2075 (0.2130)  classification_loss: 0.1174 (0.1221)  time: 0.1953  data: 0.0002  max mem: 5511
[09:56:35.861808] Epoch: [57]  [500/781]  eta: 0:00:55  lr: 0.000104  training_loss: 0.3421 (0.3355)  mae_loss: 0.2128 (0.2134)  classification_loss: 0.1184 (0.1221)  time: 0.1952  data: 0.0002  max mem: 5511
[09:56:39.795787] Epoch: [57]  [520/781]  eta: 0:00:51  lr: 0.000104  training_loss: 0.3445 (0.3359)  mae_loss: 0.2219 (0.2137)  classification_loss: 0.1258 (0.1222)  time: 0.1966  data: 0.0003  max mem: 5511
[09:56:43.691985] Epoch: [57]  [540/781]  eta: 0:00:47  lr: 0.000104  training_loss: 0.3218 (0.3356)  mae_loss: 0.2058 (0.2133)  classification_loss: 0.1250 (0.1223)  time: 0.1947  data: 0.0003  max mem: 5511
[09:56:47.594561] Epoch: [57]  [560/781]  eta: 0:00:43  lr: 0.000104  training_loss: 0.3267 (0.3354)  mae_loss: 0.2064 (0.2133)  classification_loss: 0.1185 (0.1222)  time: 0.1950  data: 0.0004  max mem: 5511
[09:56:51.494930] Epoch: [57]  [580/781]  eta: 0:00:39  lr: 0.000104  training_loss: 0.3204 (0.3349)  mae_loss: 0.1948 (0.2128)  classification_loss: 0.1164 (0.1221)  time: 0.1949  data: 0.0003  max mem: 5511
[09:56:55.408967] Epoch: [57]  [600/781]  eta: 0:00:35  lr: 0.000104  training_loss: 0.3263 (0.3348)  mae_loss: 0.2102 (0.2128)  classification_loss: 0.1170 (0.1220)  time: 0.1956  data: 0.0002  max mem: 5511
[09:56:59.310854] Epoch: [57]  [620/781]  eta: 0:00:31  lr: 0.000104  training_loss: 0.3388 (0.3350)  mae_loss: 0.2248 (0.2131)  classification_loss: 0.1194 (0.1219)  time: 0.1950  data: 0.0002  max mem: 5511
[09:57:03.256072] Epoch: [57]  [640/781]  eta: 0:00:27  lr: 0.000104  training_loss: 0.3427 (0.3352)  mae_loss: 0.2174 (0.2133)  classification_loss: 0.1192 (0.1219)  time: 0.1972  data: 0.0002  max mem: 5511
[09:57:07.163989] Epoch: [57]  [660/781]  eta: 0:00:23  lr: 0.000104  training_loss: 0.3337 (0.3351)  mae_loss: 0.2049 (0.2131)  classification_loss: 0.1219 (0.1220)  time: 0.1953  data: 0.0002  max mem: 5511
[09:57:11.104346] Epoch: [57]  [680/781]  eta: 0:00:19  lr: 0.000104  training_loss: 0.3375 (0.3352)  mae_loss: 0.2136 (0.2131)  classification_loss: 0.1259 (0.1221)  time: 0.1969  data: 0.0002  max mem: 5511
[09:57:15.013132] Epoch: [57]  [700/781]  eta: 0:00:15  lr: 0.000103  training_loss: 0.3288 (0.3351)  mae_loss: 0.2077 (0.2131)  classification_loss: 0.1211 (0.1221)  time: 0.1953  data: 0.0003  max mem: 5511
[09:57:18.922215] Epoch: [57]  [720/781]  eta: 0:00:12  lr: 0.000103  training_loss: 0.3372 (0.3353)  mae_loss: 0.2184 (0.2133)  classification_loss: 0.1191 (0.1220)  time: 0.1954  data: 0.0002  max mem: 5511
[09:57:22.838465] Epoch: [57]  [740/781]  eta: 0:00:08  lr: 0.000103  training_loss: 0.3292 (0.3354)  mae_loss: 0.2136 (0.2135)  classification_loss: 0.1179 (0.1219)  time: 0.1957  data: 0.0002  max mem: 5511
[09:57:26.747855] Epoch: [57]  [760/781]  eta: 0:00:04  lr: 0.000103  training_loss: 0.3292 (0.3354)  mae_loss: 0.2027 (0.2134)  classification_loss: 0.1272 (0.1220)  time: 0.1954  data: 0.0003  max mem: 5511
[09:57:30.639284] Epoch: [57]  [780/781]  eta: 0:00:00  lr: 0.000103  training_loss: 0.3332 (0.3357)  mae_loss: 0.2158 (0.2137)  classification_loss: 0.1174 (0.1220)  time: 0.1945  data: 0.0002  max mem: 5511
[09:57:30.792709] Epoch: [57] Total time: 0:02:33 (0.1969 s / it)
[09:57:30.793169] Averaged stats: lr: 0.000103  training_loss: 0.3332 (0.3357)  mae_loss: 0.2158 (0.2137)  classification_loss: 0.1174 (0.1220)
[09:57:31.343048] Test:  [  0/157]  eta: 0:01:25  testing_loss: 0.4773 (0.4773)  acc1: 89.0625 (89.0625)  acc5: 100.0000 (100.0000)  time: 0.5454  data: 0.5152  max mem: 5511
[09:57:31.633333] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.5141 (0.5093)  acc1: 84.3750 (84.0909)  acc5: 100.0000 (99.5739)  time: 0.0758  data: 0.0470  max mem: 5511
[09:57:31.916485] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4691 (0.4772)  acc1: 84.3750 (85.4911)  acc5: 100.0000 (99.7024)  time: 0.0285  data: 0.0002  max mem: 5511
[09:57:32.203998] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.4649 (0.4903)  acc1: 85.9375 (85.4839)  acc5: 100.0000 (99.3952)  time: 0.0284  data: 0.0002  max mem: 5511
[09:57:32.486449] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.5201 (0.5021)  acc1: 84.3750 (84.8704)  acc5: 100.0000 (99.2759)  time: 0.0284  data: 0.0002  max mem: 5511
[09:57:32.772650] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4840 (0.4936)  acc1: 85.9375 (85.3554)  acc5: 100.0000 (99.2953)  time: 0.0283  data: 0.0002  max mem: 5511
[09:57:33.056721] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4491 (0.4892)  acc1: 85.9375 (85.3740)  acc5: 100.0000 (99.2316)  time: 0.0283  data: 0.0002  max mem: 5511
[09:57:33.340615] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4280 (0.4790)  acc1: 87.5000 (85.8275)  acc5: 100.0000 (99.2958)  time: 0.0282  data: 0.0002  max mem: 5511
[09:57:33.626977] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4627 (0.4861)  acc1: 85.9375 (85.5710)  acc5: 100.0000 (99.2670)  time: 0.0284  data: 0.0002  max mem: 5511
[09:57:33.917639] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4832 (0.4824)  acc1: 84.3750 (85.7486)  acc5: 100.0000 (99.2617)  time: 0.0287  data: 0.0002  max mem: 5511
[09:57:34.201159] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4832 (0.4857)  acc1: 84.3750 (85.5043)  acc5: 100.0000 (99.3038)  time: 0.0285  data: 0.0002  max mem: 5511
[09:57:34.488353] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4618 (0.4847)  acc1: 85.9375 (85.7123)  acc5: 100.0000 (99.3525)  time: 0.0284  data: 0.0002  max mem: 5511
[09:57:34.770515] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4422 (0.4809)  acc1: 85.9375 (85.7696)  acc5: 100.0000 (99.3156)  time: 0.0283  data: 0.0002  max mem: 5511
[09:57:35.051429] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4275 (0.4825)  acc1: 84.3750 (85.6632)  acc5: 100.0000 (99.3321)  time: 0.0280  data: 0.0002  max mem: 5511
[09:57:35.333397] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4755 (0.4828)  acc1: 82.8125 (85.5718)  acc5: 100.0000 (99.3573)  time: 0.0280  data: 0.0002  max mem: 5511
[09:57:35.612144] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.5000 (0.4825)  acc1: 84.3750 (85.5753)  acc5: 100.0000 (99.3584)  time: 0.0279  data: 0.0001  max mem: 5511
[09:57:35.762821] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4574 (0.4815)  acc1: 85.9375 (85.6400)  acc5: 100.0000 (99.3500)  time: 0.0269  data: 0.0001  max mem: 5511
[09:57:35.921109] Test: Total time: 0:00:05 (0.0326 s / it)
[09:57:35.921792] * Acc@1 85.640 Acc@5 99.350 loss 0.481
[09:57:35.922138] Accuracy of the network on the 10000 test images: 85.6%
[09:57:35.922364] Max accuracy: 85.64%
[09:57:36.289428] log_dir: ./output_dir
[09:57:37.089764] Epoch: [58]  [  0/781]  eta: 0:10:23  lr: 0.000103  training_loss: 0.3415 (0.3415)  mae_loss: 0.2305 (0.2305)  classification_loss: 0.1110 (0.1110)  time: 0.7982  data: 0.5634  max mem: 5511
[09:57:40.992277] Epoch: [58]  [ 20/781]  eta: 0:02:50  lr: 0.000103  training_loss: 0.3189 (0.3272)  mae_loss: 0.2022 (0.2082)  classification_loss: 0.1167 (0.1190)  time: 0.1950  data: 0.0002  max mem: 5511
[09:57:44.896899] Epoch: [58]  [ 40/781]  eta: 0:02:35  lr: 0.000103  training_loss: 0.3229 (0.3255)  mae_loss: 0.1951 (0.2052)  classification_loss: 0.1205 (0.1202)  time: 0.1952  data: 0.0002  max mem: 5511
[09:57:48.813553] Epoch: [58]  [ 60/781]  eta: 0:02:27  lr: 0.000103  training_loss: 0.3386 (0.3314)  mae_loss: 0.2144 (0.2101)  classification_loss: 0.1209 (0.1213)  time: 0.1958  data: 0.0002  max mem: 5511
[09:57:52.735795] Epoch: [58]  [ 80/781]  eta: 0:02:22  lr: 0.000103  training_loss: 0.3243 (0.3316)  mae_loss: 0.2044 (0.2104)  classification_loss: 0.1214 (0.1212)  time: 0.1960  data: 0.0002  max mem: 5511
[09:57:56.657785] Epoch: [58]  [100/781]  eta: 0:02:17  lr: 0.000102  training_loss: 0.3172 (0.3309)  mae_loss: 0.1963 (0.2100)  classification_loss: 0.1212 (0.1209)  time: 0.1960  data: 0.0003  max mem: 5511
[09:58:00.567871] Epoch: [58]  [120/781]  eta: 0:02:12  lr: 0.000102  training_loss: 0.3215 (0.3306)  mae_loss: 0.1929 (0.2096)  classification_loss: 0.1206 (0.1210)  time: 0.1954  data: 0.0003  max mem: 5511
[09:58:04.475752] Epoch: [58]  [140/781]  eta: 0:02:08  lr: 0.000102  training_loss: 0.3306 (0.3313)  mae_loss: 0.2070 (0.2102)  classification_loss: 0.1163 (0.1211)  time: 0.1953  data: 0.0002  max mem: 5511
[09:58:08.374694] Epoch: [58]  [160/781]  eta: 0:02:03  lr: 0.000102  training_loss: 0.3288 (0.3307)  mae_loss: 0.2036 (0.2098)  classification_loss: 0.1193 (0.1209)  time: 0.1949  data: 0.0003  max mem: 5511
[09:58:12.332862] Epoch: [58]  [180/781]  eta: 0:01:59  lr: 0.000102  training_loss: 0.3391 (0.3319)  mae_loss: 0.2177 (0.2108)  classification_loss: 0.1220 (0.1211)  time: 0.1978  data: 0.0002  max mem: 5511
[09:58:16.234322] Epoch: [58]  [200/781]  eta: 0:01:55  lr: 0.000102  training_loss: 0.3325 (0.3325)  mae_loss: 0.2099 (0.2114)  classification_loss: 0.1195 (0.1211)  time: 0.1950  data: 0.0002  max mem: 5511
[09:58:20.134066] Epoch: [58]  [220/781]  eta: 0:01:51  lr: 0.000102  training_loss: 0.3245 (0.3326)  mae_loss: 0.1990 (0.2112)  classification_loss: 0.1239 (0.1214)  time: 0.1949  data: 0.0002  max mem: 5511
[09:58:24.063110] Epoch: [58]  [240/781]  eta: 0:01:47  lr: 0.000102  training_loss: 0.3330 (0.3329)  mae_loss: 0.2159 (0.2114)  classification_loss: 0.1161 (0.1215)  time: 0.1964  data: 0.0003  max mem: 5511
[09:58:27.966926] Epoch: [58]  [260/781]  eta: 0:01:43  lr: 0.000102  training_loss: 0.3435 (0.3338)  mae_loss: 0.2218 (0.2123)  classification_loss: 0.1221 (0.1215)  time: 0.1951  data: 0.0002  max mem: 5511
[09:58:31.903764] Epoch: [58]  [280/781]  eta: 0:01:39  lr: 0.000102  training_loss: 0.3236 (0.3336)  mae_loss: 0.2131 (0.2122)  classification_loss: 0.1194 (0.1214)  time: 0.1968  data: 0.0003  max mem: 5511
[09:58:35.915704] Epoch: [58]  [300/781]  eta: 0:01:35  lr: 0.000101  training_loss: 0.3247 (0.3333)  mae_loss: 0.2027 (0.2120)  classification_loss: 0.1221 (0.1212)  time: 0.2005  data: 0.0006  max mem: 5511
[09:58:39.844028] Epoch: [58]  [320/781]  eta: 0:01:31  lr: 0.000101  training_loss: 0.3308 (0.3332)  mae_loss: 0.2063 (0.2118)  classification_loss: 0.1242 (0.1213)  time: 0.1963  data: 0.0002  max mem: 5511
[09:58:43.737363] Epoch: [58]  [340/781]  eta: 0:01:27  lr: 0.000101  training_loss: 0.3275 (0.3339)  mae_loss: 0.2085 (0.2126)  classification_loss: 0.1190 (0.1213)  time: 0.1946  data: 0.0002  max mem: 5511
[09:58:47.625246] Epoch: [58]  [360/781]  eta: 0:01:23  lr: 0.000101  training_loss: 0.3317 (0.3343)  mae_loss: 0.2119 (0.2128)  classification_loss: 0.1247 (0.1214)  time: 0.1943  data: 0.0003  max mem: 5511
[09:58:51.528481] Epoch: [58]  [380/781]  eta: 0:01:19  lr: 0.000101  training_loss: 0.3265 (0.3342)  mae_loss: 0.2103 (0.2128)  classification_loss: 0.1172 (0.1214)  time: 0.1951  data: 0.0003  max mem: 5511
[09:58:55.454931] Epoch: [58]  [400/781]  eta: 0:01:15  lr: 0.000101  training_loss: 0.3267 (0.3338)  mae_loss: 0.2045 (0.2125)  classification_loss: 0.1195 (0.1213)  time: 0.1962  data: 0.0003  max mem: 5511
[09:58:59.355618] Epoch: [58]  [420/781]  eta: 0:01:11  lr: 0.000101  training_loss: 0.3291 (0.3339)  mae_loss: 0.2096 (0.2126)  classification_loss: 0.1213 (0.1213)  time: 0.1950  data: 0.0002  max mem: 5511
[09:59:03.258801] Epoch: [58]  [440/781]  eta: 0:01:07  lr: 0.000101  training_loss: 0.3254 (0.3336)  mae_loss: 0.2057 (0.2124)  classification_loss: 0.1217 (0.1212)  time: 0.1951  data: 0.0002  max mem: 5511
[09:59:07.166247] Epoch: [58]  [460/781]  eta: 0:01:03  lr: 0.000101  training_loss: 0.3390 (0.3338)  mae_loss: 0.2133 (0.2124)  classification_loss: 0.1265 (0.1214)  time: 0.1953  data: 0.0004  max mem: 5511
[09:59:11.064414] Epoch: [58]  [480/781]  eta: 0:00:59  lr: 0.000100  training_loss: 0.3435 (0.3344)  mae_loss: 0.2219 (0.2131)  classification_loss: 0.1217 (0.1214)  time: 0.1948  data: 0.0002  max mem: 5511

[09:59:14.959702] Epoch: [58]  [500/781]  eta: 0:00:55  lr: 0.000100  training_loss: 0.3436 (0.3346)  mae_loss: 0.2187 (0.2132)  classification_loss: 0.1233 (0.1214)  time: 0.1947  data: 0.0002  max mem: 5511
[09:59:18.871392] Epoch: [58]  [520/781]  eta: 0:00:51  lr: 0.000100  training_loss: 0.3290 (0.3345)  mae_loss: 0.1985 (0.2130)  classification_loss: 0.1262 (0.1215)  time: 0.1955  data: 0.0002  max mem: 5511
[09:59:22.782047] Epoch: [58]  [540/781]  eta: 0:00:47  lr: 0.000100  training_loss: 0.3320 (0.3344)  mae_loss: 0.2034 (0.2130)  classification_loss: 0.1230 (0.1215)  time: 0.1954  data: 0.0002  max mem: 5511
[09:59:26.686093] Epoch: [58]  [560/781]  eta: 0:00:43  lr: 0.000100  training_loss: 0.3313 (0.3345)  mae_loss: 0.2102 (0.2131)  classification_loss: 0.1163 (0.1214)  time: 0.1951  data: 0.0003  max mem: 5511
[09:59:30.599571] Epoch: [58]  [580/781]  eta: 0:00:39  lr: 0.000100  training_loss: 0.3276 (0.3346)  mae_loss: 0.2176 (0.2133)  classification_loss: 0.1206 (0.1213)  time: 0.1956  data: 0.0004  max mem: 5511
[09:59:34.506644] Epoch: [58]  [600/781]  eta: 0:00:35  lr: 0.000100  training_loss: 0.3229 (0.3343)  mae_loss: 0.2076 (0.2132)  classification_loss: 0.1142 (0.1212)  time: 0.1953  data: 0.0002  max mem: 5511
[09:59:38.422561] Epoch: [58]  [620/781]  eta: 0:00:31  lr: 0.000100  training_loss: 0.3213 (0.3340)  mae_loss: 0.1978 (0.2129)  classification_loss: 0.1193 (0.1210)  time: 0.1957  data: 0.0003  max mem: 5511
[09:59:42.329856] Epoch: [58]  [640/781]  eta: 0:00:27  lr: 0.000100  training_loss: 0.3322 (0.3340)  mae_loss: 0.2183 (0.2131)  classification_loss: 0.1192 (0.1210)  time: 0.1953  data: 0.0002  max mem: 5511
[09:59:46.284379] Epoch: [58]  [660/781]  eta: 0:00:23  lr: 0.000100  training_loss: 0.3216 (0.3337)  mae_loss: 0.2003 (0.2127)  classification_loss: 0.1219 (0.1210)  time: 0.1973  data: 0.0002  max mem: 5511
[09:59:50.179283] Epoch: [58]  [680/781]  eta: 0:00:19  lr: 0.000099  training_loss: 0.3448 (0.3340)  mae_loss: 0.2200 (0.2129)  classification_loss: 0.1248 (0.1211)  time: 0.1947  data: 0.0002  max mem: 5511
[09:59:54.074573] Epoch: [58]  [700/781]  eta: 0:00:15  lr: 0.000099  training_loss: 0.3424 (0.3342)  mae_loss: 0.2143 (0.2131)  classification_loss: 0.1242 (0.1212)  time: 0.1947  data: 0.0002  max mem: 5511
[09:59:57.986500] Epoch: [58]  [720/781]  eta: 0:00:11  lr: 0.000099  training_loss: 0.3357 (0.3341)  mae_loss: 0.2063 (0.2129)  classification_loss: 0.1221 (0.1213)  time: 0.1955  data: 0.0003  max mem: 5511
[10:00:01.885933] Epoch: [58]  [740/781]  eta: 0:00:08  lr: 0.000099  training_loss: 0.3190 (0.3338)  mae_loss: 0.2052 (0.2127)  classification_loss: 0.1194 (0.1212)  time: 0.1949  data: 0.0002  max mem: 5511
[10:00:05.776717] Epoch: [58]  [760/781]  eta: 0:00:04  lr: 0.000099  training_loss: 0.3186 (0.3339)  mae_loss: 0.1994 (0.2127)  classification_loss: 0.1206 (0.1212)  time: 0.1945  data: 0.0003  max mem: 5511
[10:00:09.675277] Epoch: [58]  [780/781]  eta: 0:00:00  lr: 0.000099  training_loss: 0.3302 (0.3338)  mae_loss: 0.2081 (0.2126)  classification_loss: 0.1227 (0.1213)  time: 0.1949  data: 0.0002  max mem: 5511
[10:00:09.821414] Epoch: [58] Total time: 0:02:33 (0.1966 s / it)
[10:00:09.821899] Averaged stats: lr: 0.000099  training_loss: 0.3302 (0.3338)  mae_loss: 0.2081 (0.2126)  classification_loss: 0.1227 (0.1213)
[10:00:10.397761] Test:  [  0/157]  eta: 0:01:29  testing_loss: 0.5737 (0.5737)  acc1: 81.2500 (81.2500)  acc5: 100.0000 (100.0000)  time: 0.5707  data: 0.5379  max mem: 5511
[10:00:10.680376] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.5448 (0.5523)  acc1: 81.2500 (82.1023)  acc5: 100.0000 (99.5739)  time: 0.0774  data: 0.0491  max mem: 5511
[10:00:10.962072] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4964 (0.4986)  acc1: 84.3750 (84.9702)  acc5: 100.0000 (99.5536)  time: 0.0280  data: 0.0002  max mem: 5511
[10:00:11.244987] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.4738 (0.5064)  acc1: 85.9375 (84.7782)  acc5: 100.0000 (99.3448)  time: 0.0281  data: 0.0001  max mem: 5511
[10:00:11.532666] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.5225 (0.5159)  acc1: 84.3750 (84.5655)  acc5: 100.0000 (99.2759)  time: 0.0284  data: 0.0002  max mem: 5511
[10:00:11.824019] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.5196 (0.5058)  acc1: 84.3750 (84.8958)  acc5: 100.0000 (99.2647)  time: 0.0287  data: 0.0002  max mem: 5511
[10:00:12.107132] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4567 (0.5023)  acc1: 85.9375 (85.0410)  acc5: 100.0000 (99.2316)  time: 0.0285  data: 0.0002  max mem: 5511
[10:00:12.392326] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4156 (0.4919)  acc1: 85.9375 (85.3213)  acc5: 100.0000 (99.3398)  time: 0.0283  data: 0.0003  max mem: 5511
[10:00:12.677123] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4616 (0.4999)  acc1: 85.9375 (84.8573)  acc5: 100.0000 (99.2670)  time: 0.0284  data: 0.0003  max mem: 5511
[10:00:12.960517] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4796 (0.4933)  acc1: 82.8125 (85.1477)  acc5: 100.0000 (99.3475)  time: 0.0283  data: 0.0002  max mem: 5511
[10:00:13.251866] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4678 (0.4968)  acc1: 85.9375 (85.0866)  acc5: 100.0000 (99.3657)  time: 0.0286  data: 0.0002  max mem: 5511
[10:00:13.537977] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4933 (0.4969)  acc1: 84.3750 (85.0929)  acc5: 100.0000 (99.3806)  time: 0.0287  data: 0.0002  max mem: 5511
[10:00:13.820915] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4574 (0.4924)  acc1: 85.9375 (85.2789)  acc5: 100.0000 (99.3414)  time: 0.0283  data: 0.0002  max mem: 5511
[10:00:14.106676] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4516 (0.4944)  acc1: 85.9375 (85.1861)  acc5: 100.0000 (99.3440)  time: 0.0283  data: 0.0002  max mem: 5511
[10:00:14.391490] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4708 (0.4930)  acc1: 84.3750 (85.1396)  acc5: 100.0000 (99.3794)  time: 0.0284  data: 0.0002  max mem: 5511
[10:00:14.670472] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4664 (0.4911)  acc1: 85.9375 (85.1718)  acc5: 100.0000 (99.3998)  time: 0.0281  data: 0.0001  max mem: 5511
[10:00:14.819957] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4708 (0.4917)  acc1: 85.9375 (85.1100)  acc5: 100.0000 (99.4000)  time: 0.0269  data: 0.0001  max mem: 5511
[10:00:14.991477] Test: Total time: 0:00:05 (0.0329 s / it)
[10:00:14.991953] * Acc@1 85.110 Acc@5 99.400 loss 0.492
[10:00:14.992253] Accuracy of the network on the 10000 test images: 85.1%
[10:00:14.992455] Max accuracy: 85.64%
[10:00:15.382508] log_dir: ./output_dir
[10:00:16.150862] Epoch: [59]  [  0/781]  eta: 0:09:58  lr: 0.000099  training_loss: 0.2870 (0.2870)  mae_loss: 0.1881 (0.1881)  classification_loss: 0.0989 (0.0989)  time: 0.7668  data: 0.5494  max mem: 5511
[10:00:20.034335] Epoch: [59]  [ 20/781]  eta: 0:02:48  lr: 0.000099  training_loss: 0.3358 (0.3403)  mae_loss: 0.2256 (0.2229)  classification_loss: 0.1158 (0.1174)  time: 0.1941  data: 0.0003  max mem: 5511
[10:00:23.914264] Epoch: [59]  [ 40/781]  eta: 0:02:34  lr: 0.000099  training_loss: 0.3269 (0.3372)  mae_loss: 0.2181 (0.2193)  classification_loss: 0.1146 (0.1180)  time: 0.1939  data: 0.0003  max mem: 5511
[10:00:27.829486] Epoch: [59]  [ 60/781]  eta: 0:02:27  lr: 0.000099  training_loss: 0.3397 (0.3384)  mae_loss: 0.2173 (0.2190)  classification_loss: 0.1231 (0.1194)  time: 0.1957  data: 0.0002  max mem: 5511
[10:00:31.728455] Epoch: [59]  [ 80/781]  eta: 0:02:21  lr: 0.000099  training_loss: 0.3329 (0.3384)  mae_loss: 0.2091 (0.2190)  classification_loss: 0.1188 (0.1194)  time: 0.1949  data: 0.0002  max mem: 5511
[10:00:35.649395] Epoch: [59]  [100/781]  eta: 0:02:16  lr: 0.000098  training_loss: 0.3412 (0.3381)  mae_loss: 0.2155 (0.2181)  classification_loss: 0.1218 (0.1199)  time: 0.1960  data: 0.0002  max mem: 5511
[10:00:39.554908] Epoch: [59]  [120/781]  eta: 0:02:11  lr: 0.000098  training_loss: 0.3218 (0.3358)  mae_loss: 0.2019 (0.2160)  classification_loss: 0.1198 (0.1198)  time: 0.1952  data: 0.0002  max mem: 5511
[10:00:43.451764] Epoch: [59]  [140/781]  eta: 0:02:07  lr: 0.000098  training_loss: 0.3358 (0.3353)  mae_loss: 0.2180 (0.2156)  classification_loss: 0.1170 (0.1198)  time: 0.1948  data: 0.0003  max mem: 5511
[10:00:47.367578] Epoch: [59]  [160/781]  eta: 0:02:03  lr: 0.000098  training_loss: 0.3276 (0.3342)  mae_loss: 0.2072 (0.2147)  classification_loss: 0.1171 (0.1195)  time: 0.1957  data: 0.0002  max mem: 5511
[10:00:51.284712] Epoch: [59]  [180/781]  eta: 0:01:59  lr: 0.000098  training_loss: 0.3338 (0.3343)  mae_loss: 0.2160 (0.2144)  classification_loss: 0.1188 (0.1199)  time: 0.1958  data: 0.0004  max mem: 5511
[10:00:55.206875] Epoch: [59]  [200/781]  eta: 0:01:55  lr: 0.000098  training_loss: 0.3283 (0.3344)  mae_loss: 0.2089 (0.2145)  classification_loss: 0.1194 (0.1199)  time: 0.1960  data: 0.0003  max mem: 5511
[10:00:59.174692] Epoch: [59]  [220/781]  eta: 0:01:51  lr: 0.000098  training_loss: 0.3322 (0.3347)  mae_loss: 0.2097 (0.2144)  classification_loss: 0.1211 (0.1203)  time: 0.1983  data: 0.0002  max mem: 5511
[10:01:03.091360] Epoch: [59]  [240/781]  eta: 0:01:47  lr: 0.000098  training_loss: 0.3221 (0.3337)  mae_loss: 0.1953 (0.2135)  classification_loss: 0.1177 (0.1202)  time: 0.1958  data: 0.0003  max mem: 5511
[10:01:06.975110] Epoch: [59]  [260/781]  eta: 0:01:42  lr: 0.000098  training_loss: 0.3301 (0.3336)  mae_loss: 0.2166 (0.2135)  classification_loss: 0.1169 (0.1202)  time: 0.1941  data: 0.0003  max mem: 5511
[10:01:10.868768] Epoch: [59]  [280/781]  eta: 0:01:38  lr: 0.000098  training_loss: 0.3326 (0.3338)  mae_loss: 0.2089 (0.2136)  classification_loss: 0.1184 (0.1202)  time: 0.1946  data: 0.0002  max mem: 5511
[10:01:14.777032] Epoch: [59]  [300/781]  eta: 0:01:34  lr: 0.000097  training_loss: 0.3239 (0.3337)  mae_loss: 0.2132 (0.2133)  classification_loss: 0.1230 (0.1204)  time: 0.1953  data: 0.0002  max mem: 5511
[10:01:18.681194] Epoch: [59]  [320/781]  eta: 0:01:30  lr: 0.000097  training_loss: 0.3137 (0.3333)  mae_loss: 0.2000 (0.2127)  classification_loss: 0.1247 (0.1206)  time: 0.1951  data: 0.0004  max mem: 5511
[10:01:22.618290] Epoch: [59]  [340/781]  eta: 0:01:26  lr: 0.000097  training_loss: 0.3299 (0.3331)  mae_loss: 0.2113 (0.2128)  classification_loss: 0.1174 (0.1203)  time: 0.1968  data: 0.0003  max mem: 5511
[10:01:26.644978] Epoch: [59]  [360/781]  eta: 0:01:23  lr: 0.000097  training_loss: 0.3344 (0.3332)  mae_loss: 0.2092 (0.2128)  classification_loss: 0.1216 (0.1204)  time: 0.2013  data: 0.0002  max mem: 5511
[10:01:30.529574] Epoch: [59]  [380/781]  eta: 0:01:19  lr: 0.000097  training_loss: 0.3378 (0.3337)  mae_loss: 0.2169 (0.2131)  classification_loss: 0.1235 (0.1206)  time: 0.1942  data: 0.0003  max mem: 5511
[10:01:34.432822] Epoch: [59]  [400/781]  eta: 0:01:15  lr: 0.000097  training_loss: 0.3226 (0.3334)  mae_loss: 0.2066 (0.2128)  classification_loss: 0.1162 (0.1206)  time: 0.1951  data: 0.0002  max mem: 5511
[10:01:38.376457] Epoch: [59]  [420/781]  eta: 0:01:11  lr: 0.000097  training_loss: 0.3158 (0.3329)  mae_loss: 0.1956 (0.2124)  classification_loss: 0.1187 (0.1205)  time: 0.1971  data: 0.0002  max mem: 5511
[10:01:42.334091] Epoch: [59]  [440/781]  eta: 0:01:07  lr: 0.000097  training_loss: 0.3290 (0.3326)  mae_loss: 0.2077 (0.2123)  classification_loss: 0.1178 (0.1203)  time: 0.1978  data: 0.0002  max mem: 5511
[10:01:46.254384] Epoch: [59]  [460/781]  eta: 0:01:03  lr: 0.000097  training_loss: 0.3237 (0.3324)  mae_loss: 0.2103 (0.2122)  classification_loss: 0.1167 (0.1202)  time: 0.1959  data: 0.0002  max mem: 5511
[10:01:50.200138] Epoch: [59]  [480/781]  eta: 0:00:59  lr: 0.000096  training_loss: 0.3271 (0.3321)  mae_loss: 0.2077 (0.2120)  classification_loss: 0.1173 (0.1201)  time: 0.1972  data: 0.0002  max mem: 5511
[10:01:54.157547] Epoch: [59]  [500/781]  eta: 0:00:55  lr: 0.000096  training_loss: 0.3345 (0.3322)  mae_loss: 0.2050 (0.2120)  classification_loss: 0.1237 (0.1202)  time: 0.1978  data: 0.0002  max mem: 5511
[10:01:58.065378] Epoch: [59]  [520/781]  eta: 0:00:51  lr: 0.000096  training_loss: 0.3370 (0.3325)  mae_loss: 0.2155 (0.2123)  classification_loss: 0.1190 (0.1202)  time: 0.1953  data: 0.0002  max mem: 5511
[10:02:01.992931] Epoch: [59]  [540/781]  eta: 0:00:47  lr: 0.000096  training_loss: 0.3461 (0.3328)  mae_loss: 0.2198 (0.2124)  classification_loss: 0.1252 (0.1204)  time: 0.1963  data: 0.0002  max mem: 5511
[10:02:05.907385] Epoch: [59]  [560/781]  eta: 0:00:43  lr: 0.000096  training_loss: 0.3306 (0.3328)  mae_loss: 0.2081 (0.2124)  classification_loss: 0.1189 (0.1204)  time: 0.1956  data: 0.0002  max mem: 5511
[10:02:09.817944] Epoch: [59]  [580/781]  eta: 0:00:39  lr: 0.000096  training_loss: 0.3273 (0.3327)  mae_loss: 0.2041 (0.2124)  classification_loss: 0.1171 (0.1203)  time: 0.1955  data: 0.0002  max mem: 5511
[10:02:13.722726] Epoch: [59]  [600/781]  eta: 0:00:35  lr: 0.000096  training_loss: 0.3284 (0.3328)  mae_loss: 0.2042 (0.2125)  classification_loss: 0.1188 (0.1203)  time: 0.1952  data: 0.0003  max mem: 5511
[10:02:17.628134] Epoch: [59]  [620/781]  eta: 0:00:31  lr: 0.000096  training_loss: 0.3353 (0.3329)  mae_loss: 0.2177 (0.2127)  classification_loss: 0.1185 (0.1202)  time: 0.1952  data: 0.0002  max mem: 5511
[10:02:21.534808] Epoch: [59]  [640/781]  eta: 0:00:27  lr: 0.000096  training_loss: 0.3371 (0.3330)  mae_loss: 0.2080 (0.2127)  classification_loss: 0.1287 (0.1204)  time: 0.1952  data: 0.0003  max mem: 5511
[10:02:25.447350] Epoch: [59]  [660/781]  eta: 0:00:23  lr: 0.000096  training_loss: 0.3140 (0.3327)  mae_loss: 0.1972 (0.2123)  classification_loss: 0.1165 (0.1203)  time: 0.1956  data: 0.0002  max mem: 5511
[10:02:29.349817] Epoch: [59]  [680/781]  eta: 0:00:19  lr: 0.000095  training_loss: 0.3379 (0.3329)  mae_loss: 0.2214 (0.2126)  classification_loss: 0.1160 (0.1203)  time: 0.1950  data: 0.0002  max mem: 5511

[10:02:33.252125] Epoch: [59]  [700/781]  eta: 0:00:15  lr: 0.000095  training_loss: 0.3388 (0.3333)  mae_loss: 0.2175 (0.2130)  classification_loss: 0.1207 (0.1203)  time: 0.1950  data: 0.0002  max mem: 5511
[10:02:37.151038] Epoch: [59]  [720/781]  eta: 0:00:11  lr: 0.000095  training_loss: 0.3225 (0.3331)  mae_loss: 0.1961 (0.2128)  classification_loss: 0.1236 (0.1203)  time: 0.1949  data: 0.0002  max mem: 5511
[10:02:41.054285] Epoch: [59]  [740/781]  eta: 0:00:08  lr: 0.000095  training_loss: 0.3285 (0.3330)  mae_loss: 0.2084 (0.2128)  classification_loss: 0.1186 (0.1202)  time: 0.1951  data: 0.0002  max mem: 5511
[10:02:45.056650] Epoch: [59]  [760/781]  eta: 0:00:04  lr: 0.000095  training_loss: 0.3359 (0.3332)  mae_loss: 0.2217 (0.2129)  classification_loss: 0.1209 (0.1203)  time: 0.2000  data: 0.0002  max mem: 5511
[10:02:48.939312] Epoch: [59]  [780/781]  eta: 0:00:00  lr: 0.000095  training_loss: 0.3317 (0.3331)  mae_loss: 0.2130 (0.2130)  classification_loss: 0.1148 (0.1202)  time: 0.1941  data: 0.0002  max mem: 5511
[10:02:49.092519] Epoch: [59] Total time: 0:02:33 (0.1968 s / it)
[10:02:49.092963] Averaged stats: lr: 0.000095  training_loss: 0.3317 (0.3331)  mae_loss: 0.2130 (0.2130)  classification_loss: 0.1148 (0.1202)
[10:02:49.691967] Test:  [  0/157]  eta: 0:01:33  testing_loss: 0.4800 (0.4800)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.5949  data: 0.5656  max mem: 5511
[10:02:49.979101] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.5154 (0.4936)  acc1: 84.3750 (84.2330)  acc5: 100.0000 (99.5739)  time: 0.0800  data: 0.0516  max mem: 5511
[10:02:50.267056] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4684 (0.4661)  acc1: 85.9375 (85.8631)  acc5: 100.0000 (99.4792)  time: 0.0286  data: 0.0002  max mem: 5511
[10:02:50.557503] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.4420 (0.4767)  acc1: 87.5000 (85.7359)  acc5: 100.0000 (99.2944)  time: 0.0288  data: 0.0003  max mem: 5511
[10:02:50.847291] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.4716 (0.4842)  acc1: 84.3750 (85.5183)  acc5: 100.0000 (99.1997)  time: 0.0289  data: 0.0004  max mem: 5511
[10:02:51.132091] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4429 (0.4728)  acc1: 85.9375 (85.7537)  acc5: 100.0000 (99.2647)  time: 0.0286  data: 0.0003  max mem: 5511
[10:02:51.419733] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4313 (0.4677)  acc1: 85.9375 (85.7326)  acc5: 100.0000 (99.2572)  time: 0.0285  data: 0.0002  max mem: 5511
[10:02:51.700961] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.3721 (0.4560)  acc1: 87.5000 (86.1136)  acc5: 100.0000 (99.3178)  time: 0.0283  data: 0.0002  max mem: 5511
[10:02:51.987064] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4078 (0.4644)  acc1: 85.9375 (85.8603)  acc5: 100.0000 (99.2670)  time: 0.0282  data: 0.0003  max mem: 5511
[10:02:52.276220] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4549 (0.4614)  acc1: 85.9375 (86.0920)  acc5: 100.0000 (99.3304)  time: 0.0286  data: 0.0003  max mem: 5511
[10:02:52.566934] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4584 (0.4652)  acc1: 85.9375 (85.9839)  acc5: 100.0000 (99.3657)  time: 0.0288  data: 0.0003  max mem: 5511
[10:02:52.856343] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4341 (0.4635)  acc1: 87.5000 (86.1064)  acc5: 100.0000 (99.3947)  time: 0.0288  data: 0.0005  max mem: 5511
[10:02:53.144307] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4199 (0.4587)  acc1: 89.0625 (86.4282)  acc5: 100.0000 (99.3802)  time: 0.0287  data: 0.0003  max mem: 5511
[10:02:53.436044] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4333 (0.4603)  acc1: 87.5000 (86.3550)  acc5: 100.0000 (99.3678)  time: 0.0288  data: 0.0004  max mem: 5511
[10:02:53.719279] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4505 (0.4596)  acc1: 85.9375 (86.3586)  acc5: 100.0000 (99.4016)  time: 0.0286  data: 0.0004  max mem: 5511
[10:02:53.998760] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4598 (0.4592)  acc1: 85.9375 (86.3514)  acc5: 100.0000 (99.4205)  time: 0.0280  data: 0.0001  max mem: 5511
[10:02:54.149460] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4553 (0.4589)  acc1: 85.9375 (86.3200)  acc5: 100.0000 (99.4300)  time: 0.0270  data: 0.0001  max mem: 5511
[10:02:54.297005] Test: Total time: 0:00:05 (0.0331 s / it)
[10:02:54.297474] * Acc@1 86.320 Acc@5 99.430 loss 0.459
[10:02:54.297760] Accuracy of the network on the 10000 test images: 86.3%
[10:02:54.297967] Max accuracy: 86.32%
[10:02:54.591082] log_dir: ./output_dir
[10:02:55.393408] Epoch: [60]  [  0/781]  eta: 0:10:25  lr: 0.000095  training_loss: 0.2889 (0.2889)  mae_loss: 0.1739 (0.1739)  classification_loss: 0.1151 (0.1151)  time: 0.8006  data: 0.5925  max mem: 5511

[10:02:59.312985] Epoch: [60]  [ 20/781]  eta: 0:02:50  lr: 0.000095  training_loss: 0.3271 (0.3238)  mae_loss: 0.2011 (0.2061)  classification_loss: 0.1171 (0.1177)  time: 0.1959  data: 0.0002  max mem: 5511
[10:03:03.313107] Epoch: [60]  [ 40/781]  eta: 0:02:37  lr: 0.000095  training_loss: 0.3315 (0.3281)  mae_loss: 0.2118 (0.2088)  classification_loss: 0.1189 (0.1193)  time: 0.1999  data: 0.0003  max mem: 5511
[10:03:07.223013] Epoch: [60]  [ 60/781]  eta: 0:02:29  lr: 0.000095  training_loss: 0.3316 (0.3302)  mae_loss: 0.2094 (0.2102)  classification_loss: 0.1222 (0.1200)  time: 0.1954  data: 0.0004  max mem: 5511
[10:03:11.146269] Epoch: [60]  [ 80/781]  eta: 0:02:23  lr: 0.000095  training_loss: 0.3250 (0.3303)  mae_loss: 0.2070 (0.2110)  classification_loss: 0.1176 (0.1193)  time: 0.1961  data: 0.0002  max mem: 5511
[10:03:15.078487] Epoch: [60]  [100/781]  eta: 0:02:18  lr: 0.000094  training_loss: 0.3419 (0.3319)  mae_loss: 0.2236 (0.2120)  classification_loss: 0.1217 (0.1198)  time: 0.1965  data: 0.0002  max mem: 5511
[10:03:18.983328] Epoch: [60]  [120/781]  eta: 0:02:13  lr: 0.000094  training_loss: 0.3250 (0.3320)  mae_loss: 0.2040 (0.2120)  classification_loss: 0.1214 (0.1200)  time: 0.1952  data: 0.0003  max mem: 5511
[10:03:22.898350] Epoch: [60]  [140/781]  eta: 0:02:08  lr: 0.000094  training_loss: 0.3261 (0.3316)  mae_loss: 0.2127 (0.2118)  classification_loss: 0.1177 (0.1199)  time: 0.1957  data: 0.0002  max mem: 5511
[10:03:26.831033] Epoch: [60]  [160/781]  eta: 0:02:04  lr: 0.000094  training_loss: 0.3259 (0.3316)  mae_loss: 0.2155 (0.2120)  classification_loss: 0.1126 (0.1196)  time: 0.1966  data: 0.0003  max mem: 5511
[10:03:30.767409] Epoch: [60]  [180/781]  eta: 0:02:00  lr: 0.000094  training_loss: 0.3315 (0.3309)  mae_loss: 0.2057 (0.2115)  classification_loss: 0.1160 (0.1195)  time: 0.1967  data: 0.0002  max mem: 5511
[10:03:34.679039] Epoch: [60]  [200/781]  eta: 0:01:55  lr: 0.000094  training_loss: 0.3219 (0.3303)  mae_loss: 0.2046 (0.2107)  classification_loss: 0.1208 (0.1196)  time: 0.1955  data: 0.0002  max mem: 5511
[10:03:38.590589] Epoch: [60]  [220/781]  eta: 0:01:51  lr: 0.000094  training_loss: 0.3228 (0.3298)  mae_loss: 0.2036 (0.2102)  classification_loss: 0.1164 (0.1196)  time: 0.1954  data: 0.0002  max mem: 5511
[10:03:42.520150] Epoch: [60]  [240/781]  eta: 0:01:47  lr: 0.000094  training_loss: 0.3397 (0.3311)  mae_loss: 0.2211 (0.2113)  classification_loss: 0.1202 (0.1198)  time: 0.1964  data: 0.0002  max mem: 5511
[10:03:46.438569] Epoch: [60]  [260/781]  eta: 0:01:43  lr: 0.000094  training_loss: 0.3292 (0.3312)  mae_loss: 0.2089 (0.2114)  classification_loss: 0.1202 (0.1198)  time: 0.1958  data: 0.0002  max mem: 5511
[10:03:50.343007] Epoch: [60]  [280/781]  eta: 0:01:39  lr: 0.000094  training_loss: 0.3385 (0.3318)  mae_loss: 0.2206 (0.2120)  classification_loss: 0.1157 (0.1198)  time: 0.1951  data: 0.0002  max mem: 5511
[10:03:54.246584] Epoch: [60]  [300/781]  eta: 0:01:35  lr: 0.000093  training_loss: 0.3268 (0.3313)  mae_loss: 0.2052 (0.2115)  classification_loss: 0.1220 (0.1199)  time: 0.1951  data: 0.0002  max mem: 5511
[10:03:58.146913] Epoch: [60]  [320/781]  eta: 0:01:31  lr: 0.000093  training_loss: 0.3398 (0.3321)  mae_loss: 0.2239 (0.2122)  classification_loss: 0.1177 (0.1199)  time: 0.1949  data: 0.0002  max mem: 5511
[10:04:02.061894] Epoch: [60]  [340/781]  eta: 0:01:27  lr: 0.000093  training_loss: 0.3252 (0.3318)  mae_loss: 0.2138 (0.2121)  classification_loss: 0.1140 (0.1197)  time: 0.1957  data: 0.0002  max mem: 5511
[10:04:05.973630] Epoch: [60]  [360/781]  eta: 0:01:23  lr: 0.000093  training_loss: 0.3329 (0.3316)  mae_loss: 0.2055 (0.2119)  classification_loss: 0.1183 (0.1197)  time: 0.1955  data: 0.0004  max mem: 5511
[10:04:09.875395] Epoch: [60]  [380/781]  eta: 0:01:19  lr: 0.000093  training_loss: 0.3367 (0.3320)  mae_loss: 0.2152 (0.2123)  classification_loss: 0.1212 (0.1197)  time: 0.1950  data: 0.0003  max mem: 5511
[10:04:13.781117] Epoch: [60]  [400/781]  eta: 0:01:15  lr: 0.000093  training_loss: 0.3303 (0.3322)  mae_loss: 0.2128 (0.2125)  classification_loss: 0.1171 (0.1197)  time: 0.1952  data: 0.0002  max mem: 5511
[10:04:17.714891] Epoch: [60]  [420/781]  eta: 0:01:11  lr: 0.000093  training_loss: 0.3333 (0.3321)  mae_loss: 0.2103 (0.2124)  classification_loss: 0.1188 (0.1197)  time: 0.1966  data: 0.0002  max mem: 5511
[10:04:21.619007] Epoch: [60]  [440/781]  eta: 0:01:07  lr: 0.000093  training_loss: 0.3315 (0.3323)  mae_loss: 0.2149 (0.2127)  classification_loss: 0.1166 (0.1196)  time: 0.1951  data: 0.0002  max mem: 5511
[10:04:25.514050] Epoch: [60]  [460/781]  eta: 0:01:03  lr: 0.000093  training_loss: 0.3361 (0.3326)  mae_loss: 0.2158 (0.2130)  classification_loss: 0.1163 (0.1196)  time: 0.1947  data: 0.0003  max mem: 5511

[10:04:29.415436] Epoch: [60]  [480/781]  eta: 0:00:59  lr: 0.000092  training_loss: 0.3129 (0.3320)  mae_loss: 0.1866 (0.2123)  classification_loss: 0.1235 (0.1197)  time: 0.1950  data: 0.0002  max mem: 5511
[10:04:33.308432] Epoch: [60]  [500/781]  eta: 0:00:55  lr: 0.000092  training_loss: 0.3323 (0.3322)  mae_loss: 0.2170 (0.2125)  classification_loss: 0.1185 (0.1197)  time: 0.1946  data: 0.0002  max mem: 5511
[10:04:37.213576] Epoch: [60]  [520/781]  eta: 0:00:51  lr: 0.000092  training_loss: 0.3245 (0.3320)  mae_loss: 0.2037 (0.2125)  classification_loss: 0.1182 (0.1196)  time: 0.1952  data: 0.0002  max mem: 5511
[10:04:41.114314] Epoch: [60]  [540/781]  eta: 0:00:47  lr: 0.000092  training_loss: 0.3243 (0.3319)  mae_loss: 0.2036 (0.2123)  classification_loss: 0.1160 (0.1195)  time: 0.1949  data: 0.0002  max mem: 5511
[10:04:45.064888] Epoch: [60]  [560/781]  eta: 0:00:43  lr: 0.000092  training_loss: 0.3254 (0.3318)  mae_loss: 0.2159 (0.2125)  classification_loss: 0.1106 (0.1193)  time: 0.1974  data: 0.0002  max mem: 5511
[10:04:48.960572] Epoch: [60]  [580/781]  eta: 0:00:39  lr: 0.000092  training_loss: 0.3457 (0.3322)  mae_loss: 0.2162 (0.2128)  classification_loss: 0.1191 (0.1195)  time: 0.1947  data: 0.0002  max mem: 5511
[10:04:52.862762] Epoch: [60]  [600/781]  eta: 0:00:35  lr: 0.000092  training_loss: 0.3275 (0.3323)  mae_loss: 0.2118 (0.2129)  classification_loss: 0.1196 (0.1194)  time: 0.1950  data: 0.0002  max mem: 5511
[10:04:56.782873] Epoch: [60]  [620/781]  eta: 0:00:31  lr: 0.000092  training_loss: 0.3278 (0.3321)  mae_loss: 0.2051 (0.2126)  classification_loss: 0.1165 (0.1194)  time: 0.1959  data: 0.0002  max mem: 5511
[10:05:00.678066] Epoch: [60]  [640/781]  eta: 0:00:27  lr: 0.000092  training_loss: 0.3320 (0.3320)  mae_loss: 0.2133 (0.2126)  classification_loss: 0.1199 (0.1194)  time: 0.1946  data: 0.0003  max mem: 5511
[10:05:04.576553] Epoch: [60]  [660/781]  eta: 0:00:23  lr: 0.000092  training_loss: 0.3324 (0.3321)  mae_loss: 0.2147 (0.2127)  classification_loss: 0.1143 (0.1193)  time: 0.1949  data: 0.0002  max mem: 5511
[10:05:08.487465] Epoch: [60]  [680/781]  eta: 0:00:19  lr: 0.000091  training_loss: 0.3191 (0.3320)  mae_loss: 0.2059 (0.2127)  classification_loss: 0.1186 (0.1193)  time: 0.1955  data: 0.0002  max mem: 5511
[10:05:12.382185] Epoch: [60]  [700/781]  eta: 0:00:15  lr: 0.000091  training_loss: 0.3362 (0.3320)  mae_loss: 0.2061 (0.2127)  classification_loss: 0.1183 (0.1193)  time: 0.1946  data: 0.0002  max mem: 5511
[10:05:16.283301] Epoch: [60]  [720/781]  eta: 0:00:11  lr: 0.000091  training_loss: 0.3210 (0.3319)  mae_loss: 0.2024 (0.2125)  classification_loss: 0.1228 (0.1194)  time: 0.1949  data: 0.0002  max mem: 5511
[10:05:20.240064] Epoch: [60]  [740/781]  eta: 0:00:08  lr: 0.000091  training_loss: 0.3233 (0.3319)  mae_loss: 0.2042 (0.2125)  classification_loss: 0.1180 (0.1194)  time: 0.1978  data: 0.0003  max mem: 5511
[10:05:24.135185] Epoch: [60]  [760/781]  eta: 0:00:04  lr: 0.000091  training_loss: 0.3330 (0.3319)  mae_loss: 0.2059 (0.2124)  classification_loss: 0.1210 (0.1195)  time: 0.1947  data: 0.0003  max mem: 5511
[10:05:28.111655] Epoch: [60]  [780/781]  eta: 0:00:00  lr: 0.000091  training_loss: 0.3256 (0.3319)  mae_loss: 0.2030 (0.2124)  classification_loss: 0.1190 (0.1195)  time: 0.1987  data: 0.0002  max mem: 5511
[10:05:28.283072] Epoch: [60] Total time: 0:02:33 (0.1968 s / it)
[10:05:28.283597] Averaged stats: lr: 0.000091  training_loss: 0.3256 (0.3319)  mae_loss: 0.2030 (0.2124)  classification_loss: 0.1190 (0.1195)
[10:05:30.690005] Test:  [  0/157]  eta: 0:01:51  testing_loss: 0.5029 (0.5029)  acc1: 84.3750 (84.3750)  acc5: 100.0000 (100.0000)  time: 0.7112  data: 0.6757  max mem: 5511
[10:05:30.982589] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.5034 (0.4977)  acc1: 84.3750 (84.6591)  acc5: 100.0000 (99.2898)  time: 0.0910  data: 0.0616  max mem: 5511
[10:05:31.266802] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4437 (0.4560)  acc1: 85.9375 (86.2351)  acc5: 98.4375 (99.2560)  time: 0.0287  data: 0.0002  max mem: 5511
[10:05:31.565727] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4295 (0.4682)  acc1: 85.9375 (85.9375)  acc5: 100.0000 (99.0927)  time: 0.0290  data: 0.0002  max mem: 5511
[10:05:31.852602] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4591 (0.4782)  acc1: 85.9375 (85.4802)  acc5: 100.0000 (99.0854)  time: 0.0291  data: 0.0002  max mem: 5511
[10:05:32.138241] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4479 (0.4675)  acc1: 87.5000 (86.0907)  acc5: 100.0000 (99.0809)  time: 0.0285  data: 0.0002  max mem: 5511
[10:05:32.424782] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4212 (0.4662)  acc1: 87.5000 (86.0912)  acc5: 100.0000 (99.0010)  time: 0.0285  data: 0.0003  max mem: 5511
[10:05:32.712160] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.3945 (0.4563)  acc1: 85.9375 (86.3776)  acc5: 100.0000 (99.1417)  time: 0.0286  data: 0.0002  max mem: 5511
[10:05:33.000171] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4231 (0.4643)  acc1: 85.9375 (86.0725)  acc5: 100.0000 (99.1319)  time: 0.0286  data: 0.0003  max mem: 5511
[10:05:33.285520] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4492 (0.4595)  acc1: 85.9375 (86.3496)  acc5: 98.4375 (99.1587)  time: 0.0285  data: 0.0003  max mem: 5511
[10:05:33.568478] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.4711 (0.4651)  acc1: 84.3750 (86.1077)  acc5: 100.0000 (99.2420)  time: 0.0283  data: 0.0002  max mem: 5511
[10:05:33.853071] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4807 (0.4646)  acc1: 84.3750 (86.2190)  acc5: 100.0000 (99.2258)  time: 0.0282  data: 0.0001  max mem: 5511
[10:05:34.136018] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4329 (0.4600)  acc1: 87.5000 (86.3765)  acc5: 100.0000 (99.2639)  time: 0.0282  data: 0.0002  max mem: 5511
[10:05:34.420681] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4329 (0.4624)  acc1: 85.9375 (86.2476)  acc5: 100.0000 (99.2844)  time: 0.0282  data: 0.0002  max mem: 5511
[10:05:34.703053] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4684 (0.4607)  acc1: 84.3750 (86.3032)  acc5: 100.0000 (99.3240)  time: 0.0281  data: 0.0002  max mem: 5511
[10:05:34.982506] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4371 (0.4594)  acc1: 87.5000 (86.3618)  acc5: 100.0000 (99.3171)  time: 0.0279  data: 0.0001  max mem: 5511
[10:05:35.133342] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4320 (0.4594)  acc1: 85.9375 (86.3100)  acc5: 100.0000 (99.3300)  time: 0.0270  data: 0.0001  max mem: 5511
[10:05:35.290861] Test: Total time: 0:00:05 (0.0338 s / it)
[10:05:35.291332] * Acc@1 86.310 Acc@5 99.330 loss 0.459
[10:05:35.291669] Accuracy of the network on the 10000 test images: 86.3%
[10:05:35.291870] Max accuracy: 86.32%
[10:05:35.553904] log_dir: ./output_dir
[10:05:36.365133] Epoch: [61]  [  0/781]  eta: 0:10:32  lr: 0.000091  training_loss: 0.3306 (0.3306)  mae_loss: 0.2206 (0.2206)  classification_loss: 0.1100 (0.1100)  time: 0.8092  data: 0.5999  max mem: 5511
[10:05:40.266674] Epoch: [61]  [ 20/781]  eta: 0:02:50  lr: 0.000091  training_loss: 0.3418 (0.3468)  mae_loss: 0.2257 (0.2252)  classification_loss: 0.1197 (0.1216)  time: 0.1949  data: 0.0002  max mem: 5511
[10:05:44.162444] Epoch: [61]  [ 40/781]  eta: 0:02:35  lr: 0.000091  training_loss: 0.3365 (0.3427)  mae_loss: 0.2201 (0.2216)  classification_loss: 0.1222 (0.1212)  time: 0.1947  data: 0.0002  max mem: 5511
[10:05:48.065212] Epoch: [61]  [ 60/781]  eta: 0:02:27  lr: 0.000091  training_loss: 0.3407 (0.3423)  mae_loss: 0.2226 (0.2211)  classification_loss: 0.1172 (0.1211)  time: 0.1950  data: 0.0002  max mem: 5511
[10:05:51.960819] Epoch: [61]  [ 80/781]  eta: 0:02:21  lr: 0.000091  training_loss: 0.3269 (0.3387)  mae_loss: 0.2120 (0.2184)  classification_loss: 0.1162 (0.1202)  time: 0.1947  data: 0.0002  max mem: 5511
[10:05:55.860108] Epoch: [61]  [100/781]  eta: 0:02:16  lr: 0.000090  training_loss: 0.3267 (0.3382)  mae_loss: 0.2072 (0.2175)  classification_loss: 0.1240 (0.1207)  time: 0.1949  data: 0.0002  max mem: 5511
[10:05:59.766858] Epoch: [61]  [120/781]  eta: 0:02:12  lr: 0.000090  training_loss: 0.3278 (0.3365)  mae_loss: 0.2072 (0.2164)  classification_loss: 0.1163 (0.1200)  time: 0.1953  data: 0.0002  max mem: 5511
[10:06:03.717888] Epoch: [61]  [140/781]  eta: 0:02:07  lr: 0.000090  training_loss: 0.3389 (0.3367)  mae_loss: 0.2165 (0.2168)  classification_loss: 0.1208 (0.1199)  time: 0.1975  data: 0.0002  max mem: 5511
[10:06:07.619873] Epoch: [61]  [160/781]  eta: 0:02:03  lr: 0.000090  training_loss: 0.3174 (0.3350)  mae_loss: 0.1996 (0.2156)  classification_loss: 0.1122 (0.1194)  time: 0.1950  data: 0.0002  max mem: 5511
[10:06:11.539439] Epoch: [61]  [180/781]  eta: 0:01:59  lr: 0.000090  training_loss: 0.3337 (0.3352)  mae_loss: 0.2131 (0.2160)  classification_loss: 0.1159 (0.1191)  time: 0.1959  data: 0.0003  max mem: 5511
[10:06:15.436109] Epoch: [61]  [200/781]  eta: 0:01:55  lr: 0.000090  training_loss: 0.3265 (0.3341)  mae_loss: 0.2019 (0.2149)  classification_loss: 0.1182 (0.1192)  time: 0.1948  data: 0.0002  max mem: 5511
[10:06:19.325067] Epoch: [61]  [220/781]  eta: 0:01:51  lr: 0.000090  training_loss: 0.3170 (0.3334)  mae_loss: 0.1993 (0.2144)  classification_loss: 0.1190 (0.1191)  time: 0.1944  data: 0.0002  max mem: 5511
[10:06:23.233631] Epoch: [61]  [240/781]  eta: 0:01:46  lr: 0.000090  training_loss: 0.3220 (0.3327)  mae_loss: 0.2072 (0.2139)  classification_loss: 0.1148 (0.1188)  time: 0.1954  data: 0.0003  max mem: 5511
[10:06:27.126176] Epoch: [61]  [260/781]  eta: 0:01:42  lr: 0.000090  training_loss: 0.3338 (0.3323)  mae_loss: 0.2099 (0.2137)  classification_loss: 0.1154 (0.1186)  time: 0.1946  data: 0.0002  max mem: 5511
[10:06:31.033022] Epoch: [61]  [280/781]  eta: 0:01:38  lr: 0.000090  training_loss: 0.3251 (0.3322)  mae_loss: 0.2130 (0.2140)  classification_loss: 0.1113 (0.1182)  time: 0.1953  data: 0.0002  max mem: 5511
[10:06:34.936478] Epoch: [61]  [300/781]  eta: 0:01:34  lr: 0.000089  training_loss: 0.3330 (0.3322)  mae_loss: 0.2133 (0.2139)  classification_loss: 0.1194 (0.1183)  time: 0.1951  data: 0.0002  max mem: 5511
[10:06:38.926882] Epoch: [61]  [320/781]  eta: 0:01:30  lr: 0.000089  training_loss: 0.3269 (0.3323)  mae_loss: 0.2042 (0.2139)  classification_loss: 0.1184 (0.1184)  time: 0.1994  data: 0.0002  max mem: 5511
[10:06:42.833567] Epoch: [61]  [340/781]  eta: 0:01:26  lr: 0.000089  training_loss: 0.3426 (0.3329)  mae_loss: 0.2224 (0.2146)  classification_loss: 0.1168 (0.1183)  time: 0.1953  data: 0.0002  max mem: 5511
[10:06:46.734829] Epoch: [61]  [360/781]  eta: 0:01:22  lr: 0.000089  training_loss: 0.3159 (0.3322)  mae_loss: 0.1999 (0.2139)  classification_loss: 0.1192 (0.1183)  time: 0.1950  data: 0.0002  max mem: 5511
[10:06:50.644991] Epoch: [61]  [380/781]  eta: 0:01:18  lr: 0.000089  training_loss: 0.3110 (0.3320)  mae_loss: 0.1971 (0.2135)  classification_loss: 0.1176 (0.1184)  time: 0.1954  data: 0.0003  max mem: 5511
[10:06:54.567702] Epoch: [61]  [400/781]  eta: 0:01:15  lr: 0.000089  training_loss: 0.3404 (0.3321)  mae_loss: 0.2156 (0.2136)  classification_loss: 0.1213 (0.1186)  time: 0.1960  data: 0.0002  max mem: 5511
[10:06:58.462965] Epoch: [61]  [420/781]  eta: 0:01:11  lr: 0.000089  training_loss: 0.3261 (0.3318)  mae_loss: 0.2065 (0.2131)  classification_loss: 0.1199 (0.1186)  time: 0.1947  data: 0.0002  max mem: 5511
[10:07:02.398053] Epoch: [61]  [440/781]  eta: 0:01:07  lr: 0.000089  training_loss: 0.3267 (0.3320)  mae_loss: 0.2188 (0.2134)  classification_loss: 0.1160 (0.1186)  time: 0.1967  data: 0.0002  max mem: 5511
[10:07:06.310470] Epoch: [61]  [460/781]  eta: 0:01:03  lr: 0.000089  training_loss: 0.3338 (0.3321)  mae_loss: 0.2110 (0.2136)  classification_loss: 0.1182 (0.1185)  time: 0.1955  data: 0.0003  max mem: 5511
[10:07:10.225390] Epoch: [61]  [480/781]  eta: 0:00:59  lr: 0.000089  training_loss: 0.3299 (0.3320)  mae_loss: 0.2172 (0.2134)  classification_loss: 0.1189 (0.1186)  time: 0.1957  data: 0.0002  max mem: 5511
[10:07:14.147706] Epoch: [61]  [500/781]  eta: 0:00:55  lr: 0.000088  training_loss: 0.3186 (0.3320)  mae_loss: 0.1991 (0.2134)  classification_loss: 0.1169 (0.1186)  time: 0.1960  data: 0.0002  max mem: 5511
[10:07:18.052689] Epoch: [61]  [520/781]  eta: 0:00:51  lr: 0.000088  training_loss: 0.3254 (0.3318)  mae_loss: 0.2113 (0.2133)  classification_loss: 0.1180 (0.1185)  time: 0.1952  data: 0.0004  max mem: 5511
[10:07:21.953347] Epoch: [61]  [540/781]  eta: 0:00:47  lr: 0.000088  training_loss: 0.3256 (0.3317)  mae_loss: 0.2041 (0.2132)  classification_loss: 0.1174 (0.1184)  time: 0.1949  data: 0.0002  max mem: 5511
[10:07:25.914831] Epoch: [61]  [560/781]  eta: 0:00:43  lr: 0.000088  training_loss: 0.3096 (0.3311)  mae_loss: 0.1954 (0.2128)  classification_loss: 0.1163 (0.1184)  time: 0.1980  data: 0.0002  max mem: 5511
[10:07:29.813790] Epoch: [61]  [580/781]  eta: 0:00:39  lr: 0.000088  training_loss: 0.3318 (0.3311)  mae_loss: 0.2157 (0.2128)  classification_loss: 0.1165 (0.1183)  time: 0.1948  data: 0.0002  max mem: 5511
[10:07:33.755675] Epoch: [61]  [600/781]  eta: 0:00:35  lr: 0.000088  training_loss: 0.3262 (0.3309)  mae_loss: 0.2105 (0.2127)  classification_loss: 0.1137 (0.1181)  time: 0.1970  data: 0.0002  max mem: 5511
[10:07:37.715940] Epoch: [61]  [620/781]  eta: 0:00:31  lr: 0.000088  training_loss: 0.3199 (0.3307)  mae_loss: 0.1993 (0.2124)  classification_loss: 0.1215 (0.1183)  time: 0.1979  data: 0.0002  max mem: 5511
[10:07:41.619080] Epoch: [61]  [640/781]  eta: 0:00:27  lr: 0.000088  training_loss: 0.3446 (0.3310)  mae_loss: 0.2298 (0.2128)  classification_loss: 0.1170 (0.1182)  time: 0.1951  data: 0.0002  max mem: 5511
[10:07:45.525299] Epoch: [61]  [660/781]  eta: 0:00:23  lr: 0.000088  training_loss: 0.3254 (0.3309)  mae_loss: 0.2075 (0.2127)  classification_loss: 0.1179 (0.1182)  time: 0.1952  data: 0.0004  max mem: 5511
[10:07:49.408876] Epoch: [61]  [680/781]  eta: 0:00:19  lr: 0.000088  training_loss: 0.3219 (0.3306)  mae_loss: 0.1979 (0.2124)  classification_loss: 0.1177 (0.1182)  time: 0.1941  data: 0.0001  max mem: 5511
[10:07:53.299833] Epoch: [61]  [700/781]  eta: 0:00:15  lr: 0.000087  training_loss: 0.3061 (0.3301)  mae_loss: 0.1941 (0.2119)  classification_loss: 0.1176 (0.1182)  time: 0.1944  data: 0.0002  max mem: 5511
[10:07:57.191392] Epoch: [61]  [720/781]  eta: 0:00:11  lr: 0.000087  training_loss: 0.3462 (0.3305)  mae_loss: 0.2194 (0.2121)  classification_loss: 0.1226 (0.1184)  time: 0.1945  data: 0.0002  max mem: 5511
[10:08:01.088484] Epoch: [61]  [740/781]  eta: 0:00:08  lr: 0.000087  training_loss: 0.3241 (0.3304)  mae_loss: 0.2035 (0.2120)  classification_loss: 0.1166 (0.1184)  time: 0.1948  data: 0.0003  max mem: 5511
[10:08:04.983835] Epoch: [61]  [760/781]  eta: 0:00:04  lr: 0.000087  training_loss: 0.3293 (0.3305)  mae_loss: 0.2036 (0.2121)  classification_loss: 0.1226 (0.1184)  time: 0.1947  data: 0.0003  max mem: 5511
[10:08:08.872437] Epoch: [61]  [780/781]  eta: 0:00:00  lr: 0.000087  training_loss: 0.3264 (0.3305)  mae_loss: 0.2062 (0.2121)  classification_loss: 0.1178 (0.1184)  time: 0.1943  data: 0.0003  max mem: 5511
[10:08:09.011770] Epoch: [61] Total time: 0:02:33 (0.1965 s / it)
[10:08:09.012249] Averaged stats: lr: 0.000087  training_loss: 0.3264 (0.3305)  mae_loss: 0.2062 (0.2121)  classification_loss: 0.1178 (0.1184)
[10:08:09.574522] Test:  [  0/157]  eta: 0:01:27  testing_loss: 0.5968 (0.5968)  acc1: 82.8125 (82.8125)  acc5: 98.4375 (98.4375)  time: 0.5576  data: 0.5281  max mem: 5511
[10:08:09.860048] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.4993 (0.5062)  acc1: 82.8125 (83.6648)  acc5: 100.0000 (99.5739)  time: 0.0765  data: 0.0482  max mem: 5511
[10:08:10.145819] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4801 (0.4708)  acc1: 85.9375 (85.7143)  acc5: 100.0000 (99.4048)  time: 0.0284  data: 0.0002  max mem: 5511
[10:08:10.432165] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.4718 (0.4840)  acc1: 85.9375 (85.5847)  acc5: 100.0000 (99.2440)  time: 0.0285  data: 0.0002  max mem: 5511
[10:08:10.720030] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.4725 (0.4905)  acc1: 85.9375 (85.2896)  acc5: 100.0000 (99.1997)  time: 0.0286  data: 0.0002  max mem: 5511
[10:08:11.005027] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4665 (0.4780)  acc1: 85.9375 (85.7537)  acc5: 100.0000 (99.1728)  time: 0.0285  data: 0.0002  max mem: 5511
[10:08:11.290773] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4124 (0.4709)  acc1: 87.5000 (86.0400)  acc5: 100.0000 (99.1547)  time: 0.0284  data: 0.0002  max mem: 5511
[10:08:11.573698] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4190 (0.4645)  acc1: 87.5000 (86.2236)  acc5: 100.0000 (99.1857)  time: 0.0283  data: 0.0002  max mem: 5511
[10:08:11.856445] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4509 (0.4707)  acc1: 85.9375 (85.9954)  acc5: 100.0000 (99.2091)  time: 0.0282  data: 0.0002  max mem: 5511
[10:08:12.140382] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4575 (0.4695)  acc1: 85.9375 (86.1264)  acc5: 100.0000 (99.2102)  time: 0.0282  data: 0.0002  max mem: 5511
[10:08:12.427481] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4692 (0.4735)  acc1: 85.9375 (86.0613)  acc5: 100.0000 (99.2574)  time: 0.0284  data: 0.0001  max mem: 5511
[10:08:12.712262] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4445 (0.4730)  acc1: 85.9375 (86.1627)  acc5: 100.0000 (99.2680)  time: 0.0285  data: 0.0002  max mem: 5511
[10:08:12.999217] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4182 (0.4686)  acc1: 85.9375 (86.2603)  acc5: 100.0000 (99.2639)  time: 0.0284  data: 0.0002  max mem: 5511
[10:08:13.283044] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4737 (0.4716)  acc1: 85.9375 (86.1045)  acc5: 100.0000 (99.2486)  time: 0.0284  data: 0.0002  max mem: 5511
[10:08:13.564487] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4792 (0.4711)  acc1: 84.3750 (86.1148)  acc5: 100.0000 (99.2908)  time: 0.0281  data: 0.0002  max mem: 5511
[10:08:13.843618] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4599 (0.4695)  acc1: 85.9375 (86.2169)  acc5: 100.0000 (99.3067)  time: 0.0279  data: 0.0001  max mem: 5511
[10:08:13.992892] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4672 (0.4712)  acc1: 85.9375 (86.1200)  acc5: 100.0000 (99.3100)  time: 0.0269  data: 0.0001  max mem: 5511
[10:08:14.146821] Test: Total time: 0:00:05 (0.0327 s / it)
[10:08:14.147438] * Acc@1 86.120 Acc@5 99.310 loss 0.471
[10:08:14.147788] Accuracy of the network on the 10000 test images: 86.1%
[10:08:14.147971] Max accuracy: 86.32%
[10:08:14.556754] log_dir: ./output_dir
[10:08:15.452710] Epoch: [62]  [  0/781]  eta: 0:11:38  lr: 0.000087  training_loss: 0.2921 (0.2921)  mae_loss: 0.1767 (0.1767)  classification_loss: 0.1154 (0.1154)  time: 0.8944  data: 0.6618  max mem: 5511
[10:08:19.354151] Epoch: [62]  [ 20/781]  eta: 0:02:53  lr: 0.000087  training_loss: 0.3302 (0.3272)  mae_loss: 0.2093 (0.2121)  classification_loss: 0.1153 (0.1151)  time: 0.1950  data: 0.0002  max mem: 5511
[10:08:23.274550] Epoch: [62]  [ 40/781]  eta: 0:02:37  lr: 0.000087  training_loss: 0.3326 (0.3289)  mae_loss: 0.2113 (0.2131)  classification_loss: 0.1148 (0.1158)  time: 0.1959  data: 0.0002  max mem: 5511
[10:08:27.198581] Epoch: [62]  [ 60/781]  eta: 0:02:29  lr: 0.000087  training_loss: 0.3312 (0.3299)  mae_loss: 0.2097 (0.2133)  classification_loss: 0.1215 (0.1166)  time: 0.1961  data: 0.0002  max mem: 5511
[10:08:31.089266] Epoch: [62]  [ 80/781]  eta: 0:02:23  lr: 0.000087  training_loss: 0.3220 (0.3310)  mae_loss: 0.2191 (0.2145)  classification_loss: 0.1141 (0.1165)  time: 0.1945  data: 0.0002  max mem: 5511
[10:08:35.019047] Epoch: [62]  [100/781]  eta: 0:02:17  lr: 0.000087  training_loss: 0.3386 (0.3338)  mae_loss: 0.2204 (0.2165)  classification_loss: 0.1195 (0.1173)  time: 0.1964  data: 0.0002  max mem: 5511
[10:08:38.925829] Epoch: [62]  [120/781]  eta: 0:02:13  lr: 0.000086  training_loss: 0.3154 (0.3325)  mae_loss: 0.2011 (0.2148)  classification_loss: 0.1157 (0.1177)  time: 0.1952  data: 0.0005  max mem: 5511
[10:08:42.832121] Epoch: [62]  [140/781]  eta: 0:02:08  lr: 0.000086  training_loss: 0.3391 (0.3342)  mae_loss: 0.2271 (0.2170)  classification_loss: 0.1135 (0.1172)  time: 0.1952  data: 0.0002  max mem: 5511
[10:08:46.735356] Epoch: [62]  [160/781]  eta: 0:02:04  lr: 0.000086  training_loss: 0.3287 (0.3334)  mae_loss: 0.2085 (0.2158)  classification_loss: 0.1219 (0.1177)  time: 0.1951  data: 0.0002  max mem: 5511
[10:08:50.637095] Epoch: [62]  [180/781]  eta: 0:01:59  lr: 0.000086  training_loss: 0.3307 (0.3341)  mae_loss: 0.2091 (0.2162)  classification_loss: 0.1171 (0.1178)  time: 0.1950  data: 0.0004  max mem: 5511
[10:08:54.547852] Epoch: [62]  [200/781]  eta: 0:01:55  lr: 0.000086  training_loss: 0.3439 (0.3349)  mae_loss: 0.2211 (0.2168)  classification_loss: 0.1225 (0.1182)  time: 0.1955  data: 0.0002  max mem: 5511
[10:08:58.454508] Epoch: [62]  [220/781]  eta: 0:01:51  lr: 0.000086  training_loss: 0.3267 (0.3341)  mae_loss: 0.2096 (0.2159)  classification_loss: 0.1188 (0.1182)  time: 0.1952  data: 0.0002  max mem: 5511
[10:09:02.367335] Epoch: [62]  [240/781]  eta: 0:01:47  lr: 0.000086  training_loss: 0.3353 (0.3344)  mae_loss: 0.2088 (0.2159)  classification_loss: 0.1189 (0.1185)  time: 0.1955  data: 0.0002  max mem: 5511
[10:09:06.265349] Epoch: [62]  [260/781]  eta: 0:01:43  lr: 0.000086  training_loss: 0.3369 (0.3343)  mae_loss: 0.2069 (0.2159)  classification_loss: 0.1194 (0.1185)  time: 0.1948  data: 0.0002  max mem: 5511
[10:09:10.181521] Epoch: [62]  [280/781]  eta: 0:01:39  lr: 0.000086  training_loss: 0.3388 (0.3345)  mae_loss: 0.2212 (0.2164)  classification_loss: 0.1160 (0.1182)  time: 0.1957  data: 0.0002  max mem: 5511
[10:09:14.127917] Epoch: [62]  [300/781]  eta: 0:01:35  lr: 0.000086  training_loss: 0.3274 (0.3339)  mae_loss: 0.2012 (0.2154)  classification_loss: 0.1240 (0.1185)  time: 0.1972  data: 0.0002  max mem: 5511
[10:09:18.056732] Epoch: [62]  [320/781]  eta: 0:01:31  lr: 0.000085  training_loss: 0.3160 (0.3332)  mae_loss: 0.2011 (0.2147)  classification_loss: 0.1150 (0.1185)  time: 0.1964  data: 0.0004  max mem: 5511
[10:09:21.996516] Epoch: [62]  [340/781]  eta: 0:01:27  lr: 0.000085  training_loss: 0.3302 (0.3333)  mae_loss: 0.2166 (0.2151)  classification_loss: 0.1114 (0.1182)  time: 0.1969  data: 0.0002  max mem: 5511
[10:09:25.923508] Epoch: [62]  [360/781]  eta: 0:01:23  lr: 0.000085  training_loss: 0.3308 (0.3329)  mae_loss: 0.2133 (0.2147)  classification_loss: 0.1133 (0.1182)  time: 0.1963  data: 0.0002  max mem: 5511
[10:09:29.852794] Epoch: [62]  [380/781]  eta: 0:01:19  lr: 0.000085  training_loss: 0.3293 (0.3329)  mae_loss: 0.2076 (0.2146)  classification_loss: 0.1220 (0.1184)  time: 0.1964  data: 0.0002  max mem: 5511
[10:09:33.769525] Epoch: [62]  [400/781]  eta: 0:01:15  lr: 0.000085  training_loss: 0.3251 (0.3329)  mae_loss: 0.2071 (0.2144)  classification_loss: 0.1193 (0.1185)  time: 0.1958  data: 0.0002  max mem: 5511
[10:09:37.698655] Epoch: [62]  [420/781]  eta: 0:01:11  lr: 0.000085  training_loss: 0.3299 (0.3330)  mae_loss: 0.2134 (0.2147)  classification_loss: 0.1137 (0.1184)  time: 0.1964  data: 0.0002  max mem: 5511
[10:09:41.614863] Epoch: [62]  [440/781]  eta: 0:01:07  lr: 0.000085  training_loss: 0.3261 (0.3326)  mae_loss: 0.2031 (0.2143)  classification_loss: 0.1113 (0.1183)  time: 0.1957  data: 0.0003  max mem: 5511
[10:09:45.509349] Epoch: [62]  [460/781]  eta: 0:01:03  lr: 0.000085  training_loss: 0.3144 (0.3324)  mae_loss: 0.2037 (0.2143)  classification_loss: 0.1130 (0.1181)  time: 0.1946  data: 0.0002  max mem: 5511
[10:09:49.440042] Epoch: [62]  [480/781]  eta: 0:00:59  lr: 0.000085  training_loss: 0.3354 (0.3324)  mae_loss: 0.2125 (0.2143)  classification_loss: 0.1200 (0.1182)  time: 0.1965  data: 0.0002  max mem: 5511
[10:09:53.345878] Epoch: [62]  [500/781]  eta: 0:00:55  lr: 0.000085  training_loss: 0.3253 (0.3322)  mae_loss: 0.2091 (0.2141)  classification_loss: 0.1157 (0.1181)  time: 0.1952  data: 0.0002  max mem: 5511
[10:09:57.244528] Epoch: [62]  [520/781]  eta: 0:00:51  lr: 0.000084  training_loss: 0.3261 (0.3321)  mae_loss: 0.2079 (0.2140)  classification_loss: 0.1161 (0.1181)  time: 0.1948  data: 0.0002  max mem: 5511
[10:10:01.151221] Epoch: [62]  [540/781]  eta: 0:00:47  lr: 0.000084  training_loss: 0.3220 (0.3319)  mae_loss: 0.1991 (0.2139)  classification_loss: 0.1173 (0.1180)  time: 0.1953  data: 0.0002  max mem: 5511
[10:10:05.058920] Epoch: [62]  [560/781]  eta: 0:00:43  lr: 0.000084  training_loss: 0.3289 (0.3319)  mae_loss: 0.2139 (0.2139)  classification_loss: 0.1149 (0.1180)  time: 0.1953  data: 0.0002  max mem: 5511
[10:10:08.981787] Epoch: [62]  [580/781]  eta: 0:00:39  lr: 0.000084  training_loss: 0.3198 (0.3318)  mae_loss: 0.2071 (0.2137)  classification_loss: 0.1165 (0.1181)  time: 0.1961  data: 0.0002  max mem: 5511
[10:10:12.892519] Epoch: [62]  [600/781]  eta: 0:00:35  lr: 0.000084  training_loss: 0.3175 (0.3315)  mae_loss: 0.2064 (0.2135)  classification_loss: 0.1177 (0.1180)  time: 0.1954  data: 0.0002  max mem: 5511
[10:10:16.847824] Epoch: [62]  [620/781]  eta: 0:00:31  lr: 0.000084  training_loss: 0.3295 (0.3319)  mae_loss: 0.2153 (0.2139)  classification_loss: 0.1155 (0.1181)  time: 0.1977  data: 0.0002  max mem: 5511
[10:10:20.797023] Epoch: [62]  [640/781]  eta: 0:00:27  lr: 0.000084  training_loss: 0.3392 (0.3319)  mae_loss: 0.2159 (0.2139)  classification_loss: 0.1135 (0.1181)  time: 0.1974  data: 0.0002  max mem: 5511
[10:10:24.687672] Epoch: [62]  [660/781]  eta: 0:00:23  lr: 0.000084  training_loss: 0.3364 (0.3323)  mae_loss: 0.2210 (0.2142)  classification_loss: 0.1166 (0.1181)  time: 0.1945  data: 0.0002  max mem: 5511
[10:10:28.586360] Epoch: [62]  [680/781]  eta: 0:00:19  lr: 0.000084  training_loss: 0.3241 (0.3320)  mae_loss: 0.2053 (0.2139)  classification_loss: 0.1169 (0.1181)  time: 0.1949  data: 0.0003  max mem: 5511
[10:10:32.496143] Epoch: [62]  [700/781]  eta: 0:00:15  lr: 0.000084  training_loss: 0.3432 (0.3323)  mae_loss: 0.2205 (0.2142)  classification_loss: 0.1183 (0.1181)  time: 0.1954  data: 0.0002  max mem: 5511
[10:10:36.383712] Epoch: [62]  [720/781]  eta: 0:00:11  lr: 0.000083  training_loss: 0.3196 (0.3322)  mae_loss: 0.2045 (0.2139)  classification_loss: 0.1204 (0.1182)  time: 0.1943  data: 0.0003  max mem: 5511
[10:10:40.306147] Epoch: [62]  [740/781]  eta: 0:00:08  lr: 0.000083  training_loss: 0.3315 (0.3322)  mae_loss: 0.2091 (0.2140)  classification_loss: 0.1187 (0.1182)  time: 0.1961  data: 0.0002  max mem: 5511
[10:10:44.273148] Epoch: [62]  [760/781]  eta: 0:00:04  lr: 0.000083  training_loss: 0.3236 (0.3321)  mae_loss: 0.2066 (0.2139)  classification_loss: 0.1180 (0.1182)  time: 0.1983  data: 0.0007  max mem: 5511
[10:10:48.188916] Epoch: [62]  [780/781]  eta: 0:00:00  lr: 0.000083  training_loss: 0.3391 (0.3323)  mae_loss: 0.2219 (0.2142)  classification_loss: 0.1179 (0.1181)  time: 0.1957  data: 0.0002  max mem: 5511
[10:10:48.344634] Epoch: [62] Total time: 0:02:33 (0.1969 s / it)
[10:10:48.345110] Averaged stats: lr: 0.000083  training_loss: 0.3391 (0.3323)  mae_loss: 0.2219 (0.2142)  classification_loss: 0.1179 (0.1181)
[10:10:48.936270] Test:  [  0/157]  eta: 0:01:32  testing_loss: 0.5323 (0.5323)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.5864  data: 0.5548  max mem: 5511
[10:10:49.226625] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.4775 (0.4818)  acc1: 84.3750 (85.0852)  acc5: 100.0000 (99.7159)  time: 0.0795  data: 0.0507  max mem: 5511
[10:10:49.510754] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4396 (0.4674)  acc1: 84.3750 (85.7143)  acc5: 100.0000 (99.7768)  time: 0.0286  data: 0.0002  max mem: 5511
[10:10:49.801479] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.4830 (0.4849)  acc1: 84.3750 (85.2319)  acc5: 100.0000 (99.4960)  time: 0.0286  data: 0.0003  max mem: 5511
[10:10:50.084720] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.4875 (0.4922)  acc1: 84.3750 (84.9085)  acc5: 100.0000 (99.4665)  time: 0.0286  data: 0.0003  max mem: 5511
[10:10:50.368106] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4472 (0.4751)  acc1: 85.9375 (85.6311)  acc5: 100.0000 (99.5098)  time: 0.0282  data: 0.0002  max mem: 5511
[10:10:50.653037] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.3884 (0.4680)  acc1: 87.5000 (86.0143)  acc5: 100.0000 (99.4877)  time: 0.0283  data: 0.0003  max mem: 5511
[10:10:50.937672] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4053 (0.4605)  acc1: 87.5000 (86.3336)  acc5: 100.0000 (99.5158)  time: 0.0284  data: 0.0002  max mem: 5511
[10:10:51.222933] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4250 (0.4691)  acc1: 85.9375 (86.2269)  acc5: 100.0000 (99.4599)  time: 0.0284  data: 0.0002  max mem: 5511
[10:10:51.504659] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4659 (0.4668)  acc1: 85.9375 (86.3496)  acc5: 100.0000 (99.4162)  time: 0.0282  data: 0.0002  max mem: 5511
[10:10:51.787133] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4842 (0.4709)  acc1: 85.9375 (86.1386)  acc5: 100.0000 (99.4585)  time: 0.0281  data: 0.0002  max mem: 5511
[10:10:52.070000] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4705 (0.4695)  acc1: 87.5000 (86.2753)  acc5: 100.0000 (99.4510)  time: 0.0281  data: 0.0002  max mem: 5511
[10:10:52.353375] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4532 (0.4666)  acc1: 87.5000 (86.3120)  acc5: 100.0000 (99.4706)  time: 0.0281  data: 0.0002  max mem: 5511
[10:10:52.636303] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4460 (0.4693)  acc1: 84.3750 (86.1999)  acc5: 100.0000 (99.4752)  time: 0.0281  data: 0.0002  max mem: 5511
[10:10:52.916128] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4745 (0.4690)  acc1: 84.3750 (86.1148)  acc5: 100.0000 (99.5013)  time: 0.0280  data: 0.0002  max mem: 5511
[10:10:53.194857] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4698 (0.4687)  acc1: 84.3750 (86.0306)  acc5: 100.0000 (99.4826)  time: 0.0278  data: 0.0001  max mem: 5511
[10:10:53.344848] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4610 (0.4695)  acc1: 85.9375 (85.9700)  acc5: 100.0000 (99.4800)  time: 0.0269  data: 0.0001  max mem: 5511
[10:10:53.511633] Test: Total time: 0:00:05 (0.0329 s / it)
[10:10:53.512312] * Acc@1 85.970 Acc@5 99.480 loss 0.470
[10:10:53.512634] Accuracy of the network on the 10000 test images: 86.0%
[10:10:53.512851] Max accuracy: 86.32%
[10:10:53.809674] log_dir: ./output_dir
[10:10:54.746781] Epoch: [63]  [  0/781]  eta: 0:12:10  lr: 0.000083  training_loss: 0.2902 (0.2902)  mae_loss: 0.1795 (0.1795)  classification_loss: 0.1107 (0.1107)  time: 0.9351  data: 0.7043  max mem: 5511
[10:10:58.654840] Epoch: [63]  [ 20/781]  eta: 0:02:55  lr: 0.000083  training_loss: 0.3160 (0.3137)  mae_loss: 0.1990 (0.1999)  classification_loss: 0.1112 (0.1138)  time: 0.1953  data: 0.0002  max mem: 5511
[10:11:02.553532] Epoch: [63]  [ 40/781]  eta: 0:02:37  lr: 0.000083  training_loss: 0.3315 (0.3202)  mae_loss: 0.2022 (0.2031)  classification_loss: 0.1176 (0.1170)  time: 0.1948  data: 0.0002  max mem: 5511
[10:11:06.455109] Epoch: [63]  [ 60/781]  eta: 0:02:29  lr: 0.000083  training_loss: 0.3297 (0.3246)  mae_loss: 0.2123 (0.2068)  classification_loss: 0.1183 (0.1177)  time: 0.1950  data: 0.0002  max mem: 5511
[10:11:10.353415] Epoch: [63]  [ 80/781]  eta: 0:02:23  lr: 0.000083  training_loss: 0.3311 (0.3259)  mae_loss: 0.2117 (0.2081)  classification_loss: 0.1155 (0.1178)  time: 0.1948  data: 0.0003  max mem: 5511
[10:11:14.269680] Epoch: [63]  [100/781]  eta: 0:02:17  lr: 0.000083  training_loss: 0.3285 (0.3266)  mae_loss: 0.2016 (0.2082)  classification_loss: 0.1200 (0.1184)  time: 0.1957  data: 0.0002  max mem: 5511
[10:11:18.185025] Epoch: [63]  [120/781]  eta: 0:02:13  lr: 0.000083  training_loss: 0.3242 (0.3266)  mae_loss: 0.2082 (0.2084)  classification_loss: 0.1159 (0.1181)  time: 0.1957  data: 0.0003  max mem: 5511
[10:11:22.114990] Epoch: [63]  [140/781]  eta: 0:02:08  lr: 0.000082  training_loss: 0.3258 (0.3268)  mae_loss: 0.2098 (0.2087)  classification_loss: 0.1166 (0.1181)  time: 0.1964  data: 0.0003  max mem: 5511
[10:11:26.001348] Epoch: [63]  [160/781]  eta: 0:02:04  lr: 0.000082  training_loss: 0.3179 (0.3264)  mae_loss: 0.2053 (0.2087)  classification_loss: 0.1123 (0.1178)  time: 0.1942  data: 0.0002  max mem: 5511
[10:11:29.940970] Epoch: [63]  [180/781]  eta: 0:01:59  lr: 0.000082  training_loss: 0.3219 (0.3259)  mae_loss: 0.1928 (0.2080)  classification_loss: 0.1160 (0.1179)  time: 0.1969  data: 0.0002  max mem: 5511
[10:11:33.836145] Epoch: [63]  [200/781]  eta: 0:01:55  lr: 0.000082  training_loss: 0.3240 (0.3257)  mae_loss: 0.2037 (0.2079)  classification_loss: 0.1171 (0.1178)  time: 0.1947  data: 0.0002  max mem: 5511
[10:11:37.780116] Epoch: [63]  [220/781]  eta: 0:01:51  lr: 0.000082  training_loss: 0.3219 (0.3253)  mae_loss: 0.2021 (0.2075)  classification_loss: 0.1153 (0.1178)  time: 0.1971  data: 0.0002  max mem: 5511
[10:11:41.682413] Epoch: [63]  [240/781]  eta: 0:01:47  lr: 0.000082  training_loss: 0.3177 (0.3261)  mae_loss: 0.2037 (0.2084)  classification_loss: 0.1163 (0.1177)  time: 0.1950  data: 0.0002  max mem: 5511
[10:11:45.593767] Epoch: [63]  [260/781]  eta: 0:01:43  lr: 0.000082  training_loss: 0.3234 (0.3262)  mae_loss: 0.2060 (0.2086)  classification_loss: 0.1162 (0.1176)  time: 0.1955  data: 0.0004  max mem: 5511
[10:11:49.504134] Epoch: [63]  [280/781]  eta: 0:01:39  lr: 0.000082  training_loss: 0.3251 (0.3265)  mae_loss: 0.2065 (0.2088)  classification_loss: 0.1165 (0.1177)  time: 0.1954  data: 0.0002  max mem: 5511
[10:11:53.389789] Epoch: [63]  [300/781]  eta: 0:01:35  lr: 0.000082  training_loss: 0.3406 (0.3269)  mae_loss: 0.2147 (0.2091)  classification_loss: 0.1187 (0.1178)  time: 0.1942  data: 0.0002  max mem: 5511
[10:11:57.301856] Epoch: [63]  [320/781]  eta: 0:01:31  lr: 0.000082  training_loss: 0.3177 (0.3266)  mae_loss: 0.1948 (0.2088)  classification_loss: 0.1185 (0.1178)  time: 0.1955  data: 0.0003  max mem: 5511
[10:12:01.272534] Epoch: [63]  [340/781]  eta: 0:01:27  lr: 0.000081  training_loss: 0.3243 (0.3267)  mae_loss: 0.2082 (0.2092)  classification_loss: 0.1153 (0.1176)  time: 0.1984  data: 0.0002  max mem: 5511
[10:12:05.166264] Epoch: [63]  [360/781]  eta: 0:01:23  lr: 0.000081  training_loss: 0.3314 (0.3268)  mae_loss: 0.2132 (0.2094)  classification_loss: 0.1162 (0.1174)  time: 0.1946  data: 0.0003  max mem: 5511
[10:12:09.087829] Epoch: [63]  [380/781]  eta: 0:01:19  lr: 0.000081  training_loss: 0.3460 (0.3275)  mae_loss: 0.2213 (0.2100)  classification_loss: 0.1205 (0.1175)  time: 0.1960  data: 0.0002  max mem: 5511
[10:12:12.987562] Epoch: [63]  [400/781]  eta: 0:01:15  lr: 0.000081  training_loss: 0.3229 (0.3273)  mae_loss: 0.1998 (0.2099)  classification_loss: 0.1160 (0.1174)  time: 0.1949  data: 0.0002  max mem: 5511
[10:12:16.886341] Epoch: [63]  [420/781]  eta: 0:01:11  lr: 0.000081  training_loss: 0.3262 (0.3274)  mae_loss: 0.2046 (0.2099)  classification_loss: 0.1174 (0.1174)  time: 0.1949  data: 0.0004  max mem: 5511
[10:12:20.786733] Epoch: [63]  [440/781]  eta: 0:01:07  lr: 0.000081  training_loss: 0.3257 (0.3274)  mae_loss: 0.2073 (0.2099)  classification_loss: 0.1164 (0.1175)  time: 0.1950  data: 0.0003  max mem: 5511
[10:12:24.684269] Epoch: [63]  [460/781]  eta: 0:01:03  lr: 0.000081  training_loss: 0.3218 (0.3273)  mae_loss: 0.2035 (0.2099)  classification_loss: 0.1188 (0.1175)  time: 0.1948  data: 0.0003  max mem: 5511
[10:12:28.602978] Epoch: [63]  [480/781]  eta: 0:00:59  lr: 0.000081  training_loss: 0.3176 (0.3274)  mae_loss: 0.2020 (0.2098)  classification_loss: 0.1190 (0.1176)  time: 0.1958  data: 0.0002  max mem: 5511
[10:12:32.525435] Epoch: [63]  [500/781]  eta: 0:00:55  lr: 0.000081  training_loss: 0.3244 (0.3274)  mae_loss: 0.1998 (0.2097)  classification_loss: 0.1195 (0.1176)  time: 0.1960  data: 0.0002  max mem: 5511
[10:12:36.447802] Epoch: [63]  [520/781]  eta: 0:00:51  lr: 0.000081  training_loss: 0.3258 (0.3273)  mae_loss: 0.2049 (0.2096)  classification_loss: 0.1187 (0.1176)  time: 0.1960  data: 0.0002  max mem: 5511
[10:12:40.371644] Epoch: [63]  [540/781]  eta: 0:00:47  lr: 0.000080  training_loss: 0.3269 (0.3277)  mae_loss: 0.2131 (0.2100)  classification_loss: 0.1157 (0.1177)  time: 0.1961  data: 0.0002  max mem: 5511
[10:12:44.297717] Epoch: [63]  [560/781]  eta: 0:00:43  lr: 0.000080  training_loss: 0.3244 (0.3276)  mae_loss: 0.2005 (0.2099)  classification_loss: 0.1194 (0.1177)  time: 0.1962  data: 0.0002  max mem: 5511
[10:12:48.262516] Epoch: [63]  [580/781]  eta: 0:00:39  lr: 0.000080  training_loss: 0.3429 (0.3283)  mae_loss: 0.2255 (0.2106)  classification_loss: 0.1187 (0.1177)  time: 0.1982  data: 0.0002  max mem: 5511
[10:12:52.183451] Epoch: [63]  [600/781]  eta: 0:00:35  lr: 0.000080  training_loss: 0.3415 (0.3286)  mae_loss: 0.2176 (0.2109)  classification_loss: 0.1182 (0.1177)  time: 0.1960  data: 0.0002  max mem: 5511
[10:12:56.125908] Epoch: [63]  [620/781]  eta: 0:00:31  lr: 0.000080  training_loss: 0.3210 (0.3286)  mae_loss: 0.2081 (0.2110)  classification_loss: 0.1150 (0.1176)  time: 0.1970  data: 0.0003  max mem: 5511
[10:13:00.029097] Epoch: [63]  [640/781]  eta: 0:00:27  lr: 0.000080  training_loss: 0.3329 (0.3287)  mae_loss: 0.2105 (0.2111)  classification_loss: 0.1169 (0.1176)  time: 0.1951  data: 0.0002  max mem: 5511
[10:13:03.929970] Epoch: [63]  [660/781]  eta: 0:00:23  lr: 0.000080  training_loss: 0.3293 (0.3287)  mae_loss: 0.2159 (0.2112)  classification_loss: 0.1124 (0.1176)  time: 0.1949  data: 0.0002  max mem: 5511
[10:13:07.849059] Epoch: [63]  [680/781]  eta: 0:00:19  lr: 0.000080  training_loss: 0.3218 (0.3285)  mae_loss: 0.2038 (0.2109)  classification_loss: 0.1132 (0.1175)  time: 0.1959  data: 0.0002  max mem: 5511
[10:13:11.770595] Epoch: [63]  [700/781]  eta: 0:00:15  lr: 0.000080  training_loss: 0.3262 (0.3286)  mae_loss: 0.2091 (0.2110)  classification_loss: 0.1213 (0.1176)  time: 0.1960  data: 0.0003  max mem: 5511
[10:13:15.679849] Epoch: [63]  [720/781]  eta: 0:00:11  lr: 0.000080  training_loss: 0.3322 (0.3288)  mae_loss: 0.2118 (0.2110)  classification_loss: 0.1191 (0.1177)  time: 0.1954  data: 0.0002  max mem: 5511
[10:13:19.594683] Epoch: [63]  [740/781]  eta: 0:00:08  lr: 0.000079  training_loss: 0.3290 (0.3288)  mae_loss: 0.2148 (0.2112)  classification_loss: 0.1133 (0.1176)  time: 0.1957  data: 0.0003  max mem: 5511
[10:13:23.491855] Epoch: [63]  [760/781]  eta: 0:00:04  lr: 0.000079  training_loss: 0.3209 (0.3288)  mae_loss: 0.1988 (0.2111)  classification_loss: 0.1172 (0.1177)  time: 0.1948  data: 0.0001  max mem: 5511
[10:13:27.376904] Epoch: [63]  [780/781]  eta: 0:00:00  lr: 0.000079  training_loss: 0.3272 (0.3287)  mae_loss: 0.2065 (0.2111)  classification_loss: 0.1161 (0.1176)  time: 0.1942  data: 0.0002  max mem: 5511
[10:13:27.522638] Epoch: [63] Total time: 0:02:33 (0.1968 s / it)
[10:13:27.523377] Averaged stats: lr: 0.000079  training_loss: 0.3272 (0.3287)  mae_loss: 0.2065 (0.2111)  classification_loss: 0.1161 (0.1176)
[10:13:28.070377] Test:  [  0/157]  eta: 0:01:25  testing_loss: 0.4697 (0.4697)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.5419  data: 0.5124  max mem: 5511
[10:13:28.374315] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.4617 (0.4711)  acc1: 85.9375 (84.9432)  acc5: 100.0000 (99.5739)  time: 0.0764  data: 0.0474  max mem: 5511
[10:13:28.657158] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4467 (0.4475)  acc1: 85.9375 (86.3095)  acc5: 100.0000 (99.4048)  time: 0.0290  data: 0.0006  max mem: 5511
[10:13:28.939760] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.4170 (0.4597)  acc1: 87.5000 (86.2399)  acc5: 98.4375 (99.1431)  time: 0.0282  data: 0.0003  max mem: 5511
[10:13:29.221963] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.4558 (0.4713)  acc1: 85.9375 (85.7851)  acc5: 98.4375 (99.0473)  time: 0.0281  data: 0.0002  max mem: 5511
[10:13:29.512297] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4437 (0.4607)  acc1: 85.9375 (86.1520)  acc5: 100.0000 (99.1115)  time: 0.0285  data: 0.0002  max mem: 5511
[10:13:29.800351] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4217 (0.4588)  acc1: 87.5000 (86.3217)  acc5: 100.0000 (99.1291)  time: 0.0288  data: 0.0002  max mem: 5511
[10:13:30.084324] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4080 (0.4513)  acc1: 89.0625 (86.6857)  acc5: 100.0000 (99.1637)  time: 0.0284  data: 0.0002  max mem: 5511
[10:13:30.376426] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4296 (0.4591)  acc1: 87.5000 (86.4583)  acc5: 98.4375 (99.1319)  time: 0.0286  data: 0.0002  max mem: 5511
[10:13:30.660427] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4707 (0.4557)  acc1: 85.9375 (86.6243)  acc5: 100.0000 (99.2102)  time: 0.0286  data: 0.0001  max mem: 5511
[10:13:30.946262] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4586 (0.4599)  acc1: 85.9375 (86.4171)  acc5: 100.0000 (99.2574)  time: 0.0283  data: 0.0002  max mem: 5511
[10:13:31.232309] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4586 (0.4602)  acc1: 85.9375 (86.4865)  acc5: 100.0000 (99.2680)  time: 0.0284  data: 0.0002  max mem: 5511
[10:13:31.516228] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4364 (0.4566)  acc1: 87.5000 (86.6736)  acc5: 100.0000 (99.3027)  time: 0.0283  data: 0.0002  max mem: 5511
[10:13:31.804616] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4247 (0.4570)  acc1: 87.5000 (86.6651)  acc5: 100.0000 (99.3201)  time: 0.0285  data: 0.0002  max mem: 5511
[10:13:32.087329] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3966 (0.4549)  acc1: 87.5000 (86.6910)  acc5: 100.0000 (99.3462)  time: 0.0284  data: 0.0002  max mem: 5511
[10:13:32.366075] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4423 (0.4548)  acc1: 87.5000 (86.6825)  acc5: 100.0000 (99.3377)  time: 0.0280  data: 0.0001  max mem: 5511
[10:13:32.516321] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4377 (0.4545)  acc1: 85.9375 (86.6300)  acc5: 100.0000 (99.3400)  time: 0.0269  data: 0.0001  max mem: 5511
[10:13:32.677275] Test: Total time: 0:00:05 (0.0328 s / it)
[10:13:32.678029] * Acc@1 86.630 Acc@5 99.340 loss 0.455
[10:13:32.678435] Accuracy of the network on the 10000 test images: 86.6%
[10:13:32.679201] Max accuracy: 86.63%
[10:13:33.067177] log_dir: ./output_dir
[10:13:34.014541] Epoch: [64]  [  0/781]  eta: 0:12:18  lr: 0.000079  training_loss: 0.3189 (0.3189)  mae_loss: 0.2083 (0.2083)  classification_loss: 0.1106 (0.1106)  time: 0.9457  data: 0.7426  max mem: 5511
[10:13:37.923160] Epoch: [64]  [ 20/781]  eta: 0:02:55  lr: 0.000079  training_loss: 0.3265 (0.3302)  mae_loss: 0.2130 (0.2169)  classification_loss: 0.1149 (0.1133)  time: 0.1953  data: 0.0002  max mem: 5511
[10:13:41.846149] Epoch: [64]  [ 40/781]  eta: 0:02:38  lr: 0.000079  training_loss: 0.3212 (0.3286)  mae_loss: 0.2026 (0.2139)  classification_loss: 0.1103 (0.1147)  time: 0.1961  data: 0.0003  max mem: 5511
[10:13:45.769846] Epoch: [64]  [ 60/781]  eta: 0:02:30  lr: 0.000079  training_loss: 0.3306 (0.3322)  mae_loss: 0.2087 (0.2162)  classification_loss: 0.1174 (0.1160)  time: 0.1961  data: 0.0002  max mem: 5511
[10:13:49.677642] Epoch: [64]  [ 80/781]  eta: 0:02:23  lr: 0.000079  training_loss: 0.3085 (0.3268)  mae_loss: 0.1969 (0.2114)  classification_loss: 0.1139 (0.1154)  time: 0.1953  data: 0.0002  max mem: 5511
[10:13:53.569058] Epoch: [64]  [100/781]  eta: 0:02:18  lr: 0.000079  training_loss: 0.3152 (0.3244)  mae_loss: 0.1979 (0.2088)  classification_loss: 0.1212 (0.1156)  time: 0.1945  data: 0.0002  max mem: 5511
[10:13:57.478336] Epoch: [64]  [120/781]  eta: 0:02:13  lr: 0.000079  training_loss: 0.3280 (0.3256)  mae_loss: 0.2199 (0.2105)  classification_loss: 0.1089 (0.1151)  time: 0.1954  data: 0.0002  max mem: 5511
[10:14:01.430590] Epoch: [64]  [140/781]  eta: 0:02:08  lr: 0.000079  training_loss: 0.3192 (0.3247)  mae_loss: 0.1993 (0.2095)  classification_loss: 0.1148 (0.1152)  time: 0.1975  data: 0.0002  max mem: 5511
[10:14:05.346108] Epoch: [64]  [160/781]  eta: 0:02:04  lr: 0.000079  training_loss: 0.3137 (0.3246)  mae_loss: 0.2030 (0.2092)  classification_loss: 0.1165 (0.1154)  time: 0.1957  data: 0.0002  max mem: 5511
[10:14:09.243394] Epoch: [64]  [180/781]  eta: 0:02:00  lr: 0.000078  training_loss: 0.3179 (0.3241)  mae_loss: 0.2052 (0.2089)  classification_loss: 0.1106 (0.1152)  time: 0.1948  data: 0.0002  max mem: 5511
[10:14:13.164125] Epoch: [64]  [200/781]  eta: 0:01:55  lr: 0.000078  training_loss: 0.3320 (0.3248)  mae_loss: 0.2153 (0.2094)  classification_loss: 0.1188 (0.1154)  time: 0.1959  data: 0.0002  max mem: 5511
[10:14:17.077447] Epoch: [64]  [220/781]  eta: 0:01:51  lr: 0.000078  training_loss: 0.3360 (0.3256)  mae_loss: 0.2114 (0.2099)  classification_loss: 0.1173 (0.1157)  time: 0.1956  data: 0.0002  max mem: 5511
[10:14:20.980030] Epoch: [64]  [240/781]  eta: 0:01:47  lr: 0.000078  training_loss: 0.3156 (0.3255)  mae_loss: 0.1972 (0.2096)  classification_loss: 0.1184 (0.1159)  time: 0.1950  data: 0.0001  max mem: 5511
[10:14:24.924553] Epoch: [64]  [260/781]  eta: 0:01:43  lr: 0.000078  training_loss: 0.3196 (0.3250)  mae_loss: 0.2026 (0.2091)  classification_loss: 0.1154 (0.1159)  time: 0.1971  data: 0.0003  max mem: 5511
[10:14:28.861505] Epoch: [64]  [280/781]  eta: 0:01:39  lr: 0.000078  training_loss: 0.3286 (0.3255)  mae_loss: 0.2155 (0.2098)  classification_loss: 0.1121 (0.1157)  time: 0.1967  data: 0.0002  max mem: 5511
[10:14:32.781943] Epoch: [64]  [300/781]  eta: 0:01:35  lr: 0.000078  training_loss: 0.3296 (0.3263)  mae_loss: 0.2069 (0.2103)  classification_loss: 0.1181 (0.1159)  time: 0.1959  data: 0.0002  max mem: 5511
[10:14:36.696584] Epoch: [64]  [320/781]  eta: 0:01:31  lr: 0.000078  training_loss: 0.3140 (0.3260)  mae_loss: 0.2046 (0.2099)  classification_loss: 0.1196 (0.1161)  time: 0.1957  data: 0.0002  max mem: 5511
[10:14:40.642655] Epoch: [64]  [340/781]  eta: 0:01:27  lr: 0.000078  training_loss: 0.3198 (0.3261)  mae_loss: 0.2181 (0.2101)  classification_loss: 0.1117 (0.1160)  time: 0.1972  data: 0.0003  max mem: 5511
[10:14:44.571103] Epoch: [64]  [360/781]  eta: 0:01:23  lr: 0.000078  training_loss: 0.3228 (0.3261)  mae_loss: 0.2001 (0.2100)  classification_loss: 0.1172 (0.1161)  time: 0.1963  data: 0.0003  max mem: 5511
[10:14:48.502530] Epoch: [64]  [380/781]  eta: 0:01:19  lr: 0.000077  training_loss: 0.3460 (0.3273)  mae_loss: 0.2257 (0.2109)  classification_loss: 0.1240 (0.1164)  time: 0.1965  data: 0.0002  max mem: 5511
[10:14:52.405848] Epoch: [64]  [400/781]  eta: 0:01:15  lr: 0.000077  training_loss: 0.3273 (0.3273)  mae_loss: 0.2116 (0.2111)  classification_loss: 0.1124 (0.1162)  time: 0.1951  data: 0.0002  max mem: 5511
[10:14:56.332774] Epoch: [64]  [420/781]  eta: 0:01:11  lr: 0.000077  training_loss: 0.3238 (0.3274)  mae_loss: 0.2085 (0.2110)  classification_loss: 0.1191 (0.1164)  time: 0.1963  data: 0.0002  max mem: 5511
[10:15:00.263435] Epoch: [64]  [440/781]  eta: 0:01:07  lr: 0.000077  training_loss: 0.3219 (0.3271)  mae_loss: 0.2048 (0.2107)  classification_loss: 0.1167 (0.1164)  time: 0.1964  data: 0.0002  max mem: 5511
[10:15:04.166344] Epoch: [64]  [460/781]  eta: 0:01:03  lr: 0.000077  training_loss: 0.3106 (0.3267)  mae_loss: 0.1988 (0.2105)  classification_loss: 0.1122 (0.1162)  time: 0.1950  data: 0.0001  max mem: 5511
[10:15:08.141236] Epoch: [64]  [480/781]  eta: 0:00:59  lr: 0.000077  training_loss: 0.3238 (0.3268)  mae_loss: 0.2145 (0.2107)  classification_loss: 0.1119 (0.1161)  time: 0.1987  data: 0.0002  max mem: 5511
[10:15:12.063775] Epoch: [64]  [500/781]  eta: 0:00:55  lr: 0.000077  training_loss: 0.3365 (0.3271)  mae_loss: 0.2181 (0.2109)  classification_loss: 0.1184 (0.1162)  time: 0.1960  data: 0.0002  max mem: 5511
[10:15:15.957483] Epoch: [64]  [520/781]  eta: 0:00:51  lr: 0.000077  training_loss: 0.3306 (0.3272)  mae_loss: 0.2087 (0.2110)  classification_loss: 0.1178 (0.1162)  time: 0.1946  data: 0.0003  max mem: 5511
[10:15:19.861303] Epoch: [64]  [540/781]  eta: 0:00:47  lr: 0.000077  training_loss: 0.3273 (0.3274)  mae_loss: 0.2143 (0.2111)  classification_loss: 0.1172 (0.1163)  time: 0.1951  data: 0.0002  max mem: 5511
[10:15:23.769025] Epoch: [64]  [560/781]  eta: 0:00:43  lr: 0.000077  training_loss: 0.3180 (0.3273)  mae_loss: 0.2038 (0.2111)  classification_loss: 0.1135 (0.1163)  time: 0.1953  data: 0.0002  max mem: 5511
[10:15:27.674880] Epoch: [64]  [580/781]  eta: 0:00:39  lr: 0.000076  training_loss: 0.3113 (0.3271)  mae_loss: 0.1984 (0.2108)  classification_loss: 0.1164 (0.1163)  time: 0.1952  data: 0.0002  max mem: 5511
[10:15:31.609794] Epoch: [64]  [600/781]  eta: 0:00:35  lr: 0.000076  training_loss: 0.3248 (0.3271)  mae_loss: 0.2103 (0.2109)  classification_loss: 0.1162 (0.1162)  time: 0.1966  data: 0.0003  max mem: 5511
[10:15:35.530739] Epoch: [64]  [620/781]  eta: 0:00:31  lr: 0.000076  training_loss: 0.3393 (0.3274)  mae_loss: 0.2183 (0.2113)  classification_loss: 0.1155 (0.1162)  time: 0.1960  data: 0.0002  max mem: 5511
[10:15:39.433719] Epoch: [64]  [640/781]  eta: 0:00:27  lr: 0.000076  training_loss: 0.3136 (0.3271)  mae_loss: 0.1985 (0.2110)  classification_loss: 0.1157 (0.1161)  time: 0.1950  data: 0.0003  max mem: 5511
[10:15:43.351520] Epoch: [64]  [660/781]  eta: 0:00:23  lr: 0.000076  training_loss: 0.3328 (0.3274)  mae_loss: 0.2175 (0.2113)  classification_loss: 0.1147 (0.1161)  time: 0.1958  data: 0.0002  max mem: 5511
[10:15:47.273624] Epoch: [64]  [680/781]  eta: 0:00:19  lr: 0.000076  training_loss: 0.3291 (0.3276)  mae_loss: 0.2113 (0.2114)  classification_loss: 0.1160 (0.1161)  time: 0.1960  data: 0.0005  max mem: 5511
[10:15:51.184674] Epoch: [64]  [700/781]  eta: 0:00:15  lr: 0.000076  training_loss: 0.3317 (0.3278)  mae_loss: 0.2168 (0.2116)  classification_loss: 0.1160 (0.1162)  time: 0.1955  data: 0.0003  max mem: 5511
[10:15:55.096598] Epoch: [64]  [720/781]  eta: 0:00:12  lr: 0.000076  training_loss: 0.3282 (0.3279)  mae_loss: 0.2004 (0.2115)  classification_loss: 0.1188 (0.1164)  time: 0.1955  data: 0.0002  max mem: 5511
[10:15:58.993378] Epoch: [64]  [740/781]  eta: 0:00:08  lr: 0.000076  training_loss: 0.3151 (0.3276)  mae_loss: 0.2021 (0.2113)  classification_loss: 0.1114 (0.1163)  time: 0.1947  data: 0.0002  max mem: 5511
[10:16:02.902725] Epoch: [64]  [760/781]  eta: 0:00:04  lr: 0.000076  training_loss: 0.3310 (0.3277)  mae_loss: 0.2099 (0.2113)  classification_loss: 0.1181 (0.1163)  time: 0.1954  data: 0.0002  max mem: 5511
[10:16:06.846660] Epoch: [64]  [780/781]  eta: 0:00:00  lr: 0.000075  training_loss: 0.3188 (0.3274)  mae_loss: 0.2066 (0.2111)  classification_loss: 0.1151 (0.1163)  time: 0.1971  data: 0.0002  max mem: 5511
[10:16:07.009573] Epoch: [64] Total time: 0:02:33 (0.1971 s / it)
[10:16:07.010222] Averaged stats: lr: 0.000075  training_loss: 0.3188 (0.3274)  mae_loss: 0.2066 (0.2111)  classification_loss: 0.1151 (0.1163)
[10:16:07.567816] Test:  [  0/157]  eta: 0:01:26  testing_loss: 0.4676 (0.4676)  acc1: 85.9375 (85.9375)  acc5: 100.0000 (100.0000)  time: 0.5527  data: 0.5199  max mem: 5511
[10:16:07.859669] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.4853 (0.4842)  acc1: 85.9375 (84.8011)  acc5: 100.0000 (99.5739)  time: 0.0766  data: 0.0475  max mem: 5511
[10:16:08.143153] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4431 (0.4548)  acc1: 85.9375 (85.5655)  acc5: 100.0000 (99.5536)  time: 0.0286  data: 0.0002  max mem: 5511
[10:16:08.426182] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.4431 (0.4663)  acc1: 85.9375 (85.6855)  acc5: 100.0000 (99.3448)  time: 0.0282  data: 0.0001  max mem: 5511
[10:16:08.713290] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.4705 (0.4770)  acc1: 85.9375 (85.3277)  acc5: 100.0000 (99.2759)  time: 0.0284  data: 0.0003  max mem: 5511
[10:16:08.996686] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4557 (0.4625)  acc1: 85.9375 (85.9375)  acc5: 100.0000 (99.2953)  time: 0.0284  data: 0.0003  max mem: 5511
[10:16:09.279756] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4101 (0.4577)  acc1: 87.5000 (86.0143)  acc5: 100.0000 (99.2828)  time: 0.0282  data: 0.0002  max mem: 5511
[10:16:09.563718] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.3935 (0.4486)  acc1: 89.0625 (86.4877)  acc5: 100.0000 (99.3618)  time: 0.0282  data: 0.0002  max mem: 5511
[10:16:09.853352] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4210 (0.4550)  acc1: 87.5000 (86.3426)  acc5: 100.0000 (99.3441)  time: 0.0285  data: 0.0002  max mem: 5511
[10:16:10.137717] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4441 (0.4512)  acc1: 85.9375 (86.5213)  acc5: 100.0000 (99.3819)  time: 0.0285  data: 0.0002  max mem: 5511
[10:16:10.419495] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4405 (0.4561)  acc1: 85.9375 (86.3397)  acc5: 100.0000 (99.3967)  time: 0.0281  data: 0.0002  max mem: 5511
[10:16:10.700651] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4405 (0.4565)  acc1: 85.9375 (86.3457)  acc5: 100.0000 (99.4088)  time: 0.0280  data: 0.0001  max mem: 5511
[10:16:10.982096] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4385 (0.4528)  acc1: 87.5000 (86.4282)  acc5: 100.0000 (99.4189)  time: 0.0280  data: 0.0001  max mem: 5511
[10:16:11.263875] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4495 (0.4545)  acc1: 87.5000 (86.3907)  acc5: 100.0000 (99.4513)  time: 0.0280  data: 0.0002  max mem: 5511
[10:16:11.548384] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4375 (0.4527)  acc1: 87.5000 (86.3808)  acc5: 100.0000 (99.4792)  time: 0.0282  data: 0.0001  max mem: 5511
[10:16:11.828753] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4375 (0.4520)  acc1: 87.5000 (86.4238)  acc5: 100.0000 (99.4619)  time: 0.0281  data: 0.0001  max mem: 5511
[10:16:11.980487] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4469 (0.4529)  acc1: 85.9375 (86.3600)  acc5: 100.0000 (99.4300)  time: 0.0272  data: 0.0001  max mem: 5511
[10:16:12.125596] Test: Total time: 0:00:05 (0.0326 s / it)
[10:16:12.126409] * Acc@1 86.360 Acc@5 99.430 loss 0.453
[10:16:12.126799] Accuracy of the network on the 10000 test images: 86.4%
[10:16:12.127232] Max accuracy: 86.63%
[10:16:12.363713] log_dir: ./output_dir
[10:16:13.164165] Epoch: [65]  [  0/781]  eta: 0:10:23  lr: 0.000075  training_loss: 0.3324 (0.3324)  mae_loss: 0.2249 (0.2249)  classification_loss: 0.1075 (0.1075)  time: 0.7988  data: 0.5968  max mem: 5511
[10:16:17.112903] Epoch: [65]  [ 20/781]  eta: 0:02:51  lr: 0.000075  training_loss: 0.3376 (0.3385)  mae_loss: 0.2234 (0.2235)  classification_loss: 0.1147 (0.1149)  time: 0.1973  data: 0.0002  max mem: 5511
[10:16:21.035030] Epoch: [65]  [ 40/781]  eta: 0:02:36  lr: 0.000075  training_loss: 0.3334 (0.3352)  mae_loss: 0.2139 (0.2193)  classification_loss: 0.1165 (0.1160)  time: 0.1960  data: 0.0003  max mem: 5511
[10:16:24.938256] Epoch: [65]  [ 60/781]  eta: 0:02:28  lr: 0.000075  training_loss: 0.3184 (0.3303)  mae_loss: 0.1980 (0.2138)  classification_loss: 0.1163 (0.1165)  time: 0.1951  data: 0.0002  max mem: 5511
[10:16:28.843572] Epoch: [65]  [ 80/781]  eta: 0:02:22  lr: 0.000075  training_loss: 0.3206 (0.3284)  mae_loss: 0.2019 (0.2113)  classification_loss: 0.1179 (0.1171)  time: 0.1952  data: 0.0002  max mem: 5511
[10:16:32.752540] Epoch: [65]  [100/781]  eta: 0:02:17  lr: 0.000075  training_loss: 0.3338 (0.3300)  mae_loss: 0.2187 (0.2127)  classification_loss: 0.1187 (0.1174)  time: 0.1954  data: 0.0003  max mem: 5511
[10:16:36.658875] Epoch: [65]  [120/781]  eta: 0:02:12  lr: 0.000075  training_loss: 0.3034 (0.3265)  mae_loss: 0.1873 (0.2094)  classification_loss: 0.1169 (0.1170)  time: 0.1952  data: 0.0002  max mem: 5511
[10:16:40.648949] Epoch: [65]  [140/781]  eta: 0:02:08  lr: 0.000075  training_loss: 0.3200 (0.3255)  mae_loss: 0.2081 (0.2090)  classification_loss: 0.1136 (0.1165)  time: 0.1994  data: 0.0003  max mem: 5511
[10:16:44.559965] Epoch: [65]  [160/781]  eta: 0:02:04  lr: 0.000075  training_loss: 0.3258 (0.3260)  mae_loss: 0.2134 (0.2098)  classification_loss: 0.1132 (0.1163)  time: 0.1955  data: 0.0002  max mem: 5511
[10:16:48.468567] Epoch: [65]  [180/781]  eta: 0:01:59  lr: 0.000075  training_loss: 0.3234 (0.3263)  mae_loss: 0.2080 (0.2103)  classification_loss: 0.1154 (0.1160)  time: 0.1954  data: 0.0003  max mem: 5511
[10:16:52.371167] Epoch: [65]  [200/781]  eta: 0:01:55  lr: 0.000075  training_loss: 0.3217 (0.3262)  mae_loss: 0.2090 (0.2106)  classification_loss: 0.1096 (0.1157)  time: 0.1951  data: 0.0002  max mem: 5511
[10:16:56.280053] Epoch: [65]  [220/781]  eta: 0:01:51  lr: 0.000074  training_loss: 0.3225 (0.3266)  mae_loss: 0.2072 (0.2107)  classification_loss: 0.1171 (0.1159)  time: 0.1954  data: 0.0002  max mem: 5511
[10:17:00.180772] Epoch: [65]  [240/781]  eta: 0:01:47  lr: 0.000074  training_loss: 0.3308 (0.3268)  mae_loss: 0.2093 (0.2110)  classification_loss: 0.1101 (0.1158)  time: 0.1949  data: 0.0002  max mem: 5511
[10:17:04.084885] Epoch: [65]  [260/781]  eta: 0:01:43  lr: 0.000074  training_loss: 0.3212 (0.3263)  mae_loss: 0.1943 (0.2103)  classification_loss: 0.1178 (0.1160)  time: 0.1951  data: 0.0006  max mem: 5511
[10:17:07.994647] Epoch: [65]  [280/781]  eta: 0:01:39  lr: 0.000074  training_loss: 0.3064 (0.3259)  mae_loss: 0.1915 (0.2099)  classification_loss: 0.1153 (0.1160)  time: 0.1954  data: 0.0003  max mem: 5511
[10:17:11.930948] Epoch: [65]  [300/781]  eta: 0:01:35  lr: 0.000074  training_loss: 0.3287 (0.3264)  mae_loss: 0.2105 (0.2102)  classification_loss: 0.1156 (0.1162)  time: 0.1967  data: 0.0003  max mem: 5511
[10:17:15.858750] Epoch: [65]  [320/781]  eta: 0:01:31  lr: 0.000074  training_loss: 0.3226 (0.3264)  mae_loss: 0.2057 (0.2103)  classification_loss: 0.1151 (0.1162)  time: 0.1963  data: 0.0002  max mem: 5511
[10:17:19.747991] Epoch: [65]  [340/781]  eta: 0:01:27  lr: 0.000074  training_loss: 0.3165 (0.3259)  mae_loss: 0.1972 (0.2098)  classification_loss: 0.1164 (0.1161)  time: 0.1944  data: 0.0002  max mem: 5511
[10:17:23.648566] Epoch: [65]  [360/781]  eta: 0:01:23  lr: 0.000074  training_loss: 0.3145 (0.3255)  mae_loss: 0.1957 (0.2094)  classification_loss: 0.1183 (0.1162)  time: 0.1949  data: 0.0003  max mem: 5511
[10:17:27.556851] Epoch: [65]  [380/781]  eta: 0:01:19  lr: 0.000074  training_loss: 0.3374 (0.3261)  mae_loss: 0.2184 (0.2100)  classification_loss: 0.1182 (0.1161)  time: 0.1953  data: 0.0002  max mem: 5511
[10:17:31.537471] Epoch: [65]  [400/781]  eta: 0:01:15  lr: 0.000074  training_loss: 0.3320 (0.3265)  mae_loss: 0.2194 (0.2105)  classification_loss: 0.1133 (0.1160)  time: 0.1989  data: 0.0004  max mem: 5511
[10:17:35.468733] Epoch: [65]  [420/781]  eta: 0:01:11  lr: 0.000073  training_loss: 0.3327 (0.3270)  mae_loss: 0.2164 (0.2110)  classification_loss: 0.1181 (0.1161)  time: 0.1965  data: 0.0003  max mem: 5511
[10:17:39.391440] Epoch: [65]  [440/781]  eta: 0:01:07  lr: 0.000073  training_loss: 0.3222 (0.3268)  mae_loss: 0.2026 (0.2107)  classification_loss: 0.1174 (0.1162)  time: 0.1960  data: 0.0003  max mem: 5511
[10:17:43.299184] Epoch: [65]  [460/781]  eta: 0:01:03  lr: 0.000073  training_loss: 0.3222 (0.3266)  mae_loss: 0.2124 (0.2106)  classification_loss: 0.1100 (0.1159)  time: 0.1953  data: 0.0003  max mem: 5511
[10:17:47.240086] Epoch: [65]  [480/781]  eta: 0:00:59  lr: 0.000073  training_loss: 0.3124 (0.3264)  mae_loss: 0.1961 (0.2104)  classification_loss: 0.1149 (0.1160)  time: 0.1970  data: 0.0003  max mem: 5511
[10:17:51.137847] Epoch: [65]  [500/781]  eta: 0:00:55  lr: 0.000073  training_loss: 0.3143 (0.3261)  mae_loss: 0.1957 (0.2101)  classification_loss: 0.1143 (0.1160)  time: 0.1948  data: 0.0002  max mem: 5511
[10:17:55.043468] Epoch: [65]  [520/781]  eta: 0:00:51  lr: 0.000073  training_loss: 0.3251 (0.3259)  mae_loss: 0.2046 (0.2099)  classification_loss: 0.1161 (0.1160)  time: 0.1952  data: 0.0002  max mem: 5511
[10:17:58.942661] Epoch: [65]  [540/781]  eta: 0:00:47  lr: 0.000073  training_loss: 0.3148 (0.3256)  mae_loss: 0.1983 (0.2096)  classification_loss: 0.1156 (0.1160)  time: 0.1949  data: 0.0002  max mem: 5511
[10:18:02.883892] Epoch: [65]  [560/781]  eta: 0:00:43  lr: 0.000073  training_loss: 0.3258 (0.3255)  mae_loss: 0.2075 (0.2096)  classification_loss: 0.1111 (0.1159)  time: 0.1969  data: 0.0003  max mem: 5511
[10:18:06.784404] Epoch: [65]  [580/781]  eta: 0:00:39  lr: 0.000073  training_loss: 0.3277 (0.3256)  mae_loss: 0.2130 (0.2096)  classification_loss: 0.1154 (0.1160)  time: 0.1949  data: 0.0003  max mem: 5511
[10:18:10.677705] Epoch: [65]  [600/781]  eta: 0:00:35  lr: 0.000073  training_loss: 0.3208 (0.3258)  mae_loss: 0.2001 (0.2098)  classification_loss: 0.1158 (0.1160)  time: 0.1946  data: 0.0002  max mem: 5511
[10:18:14.621445] Epoch: [65]  [620/781]  eta: 0:00:31  lr: 0.000073  training_loss: 0.3444 (0.3262)  mae_loss: 0.2316 (0.2103)  classification_loss: 0.1171 (0.1159)  time: 0.1971  data: 0.0007  max mem: 5511
[10:18:18.528474] Epoch: [65]  [640/781]  eta: 0:00:27  lr: 0.000072  training_loss: 0.3510 (0.3266)  mae_loss: 0.2251 (0.2107)  classification_loss: 0.1173 (0.1159)  time: 0.1953  data: 0.0002  max mem: 5511
[10:18:22.451171] Epoch: [65]  [660/781]  eta: 0:00:23  lr: 0.000072  training_loss: 0.3320 (0.3267)  mae_loss: 0.2168 (0.2107)  classification_loss: 0.1172 (0.1160)  time: 0.1961  data: 0.0003  max mem: 5511
[10:18:26.346270] Epoch: [65]  [680/781]  eta: 0:00:19  lr: 0.000072  training_loss: 0.3251 (0.3267)  mae_loss: 0.2041 (0.2106)  classification_loss: 0.1184 (0.1161)  time: 0.1947  data: 0.0002  max mem: 5511
[10:18:30.371681] Epoch: [65]  [700/781]  eta: 0:00:15  lr: 0.000072  training_loss: 0.3365 (0.3270)  mae_loss: 0.2139 (0.2108)  classification_loss: 0.1175 (0.1162)  time: 0.2012  data: 0.0003  max mem: 5511
[10:18:34.324214] Epoch: [65]  [720/781]  eta: 0:00:12  lr: 0.000072  training_loss: 0.3230 (0.3270)  mae_loss: 0.2098 (0.2108)  classification_loss: 0.1192 (0.1162)  time: 0.1976  data: 0.0002  max mem: 5511
[10:18:38.221680] Epoch: [65]  [740/781]  eta: 0:00:08  lr: 0.000072  training_loss: 0.3139 (0.3269)  mae_loss: 0.1906 (0.2107)  classification_loss: 0.1147 (0.1162)  time: 0.1948  data: 0.0002  max mem: 5511
[10:18:42.122325] Epoch: [65]  [760/781]  eta: 0:00:04  lr: 0.000072  training_loss: 0.3219 (0.3271)  mae_loss: 0.2098 (0.2108)  classification_loss: 0.1157 (0.1163)  time: 0.1950  data: 0.0002  max mem: 5511
[10:18:46.002605] Epoch: [65]  [780/781]  eta: 0:00:00  lr: 0.000072  training_loss: 0.3192 (0.3268)  mae_loss: 0.2016 (0.2106)  classification_loss: 0.1174 (0.1163)  time: 0.1939  data: 0.0001  max mem: 5511
[10:18:46.155814] Epoch: [65] Total time: 0:02:33 (0.1969 s / it)
[10:18:46.156267] Averaged stats: lr: 0.000072  training_loss: 0.3192 (0.3268)  mae_loss: 0.2016 (0.2106)  classification_loss: 0.1174 (0.1163)
[10:18:46.707369] Test:  [  0/157]  eta: 0:01:25  testing_loss: 0.3972 (0.3972)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (100.0000)  time: 0.5465  data: 0.5102  max mem: 5511
[10:18:46.995073] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.4770 (0.4643)  acc1: 85.9375 (85.2273)  acc5: 100.0000 (99.5739)  time: 0.0756  data: 0.0467  max mem: 5511
[10:18:47.277312] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4499 (0.4439)  acc1: 84.3750 (85.9375)  acc5: 100.0000 (99.4048)  time: 0.0283  data: 0.0002  max mem: 5511
[10:18:47.561102] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.4216 (0.4512)  acc1: 85.9375 (86.3407)  acc5: 100.0000 (99.1935)  time: 0.0282  data: 0.0002  max mem: 5511
[10:18:47.847456] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.4430 (0.4640)  acc1: 85.9375 (85.8994)  acc5: 100.0000 (99.1235)  time: 0.0284  data: 0.0002  max mem: 5511
[10:18:48.132248] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4391 (0.4481)  acc1: 87.5000 (86.6115)  acc5: 98.4375 (99.1422)  time: 0.0284  data: 0.0002  max mem: 5511
[10:18:48.416351] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.3724 (0.4437)  acc1: 90.6250 (86.8596)  acc5: 100.0000 (99.1547)  time: 0.0283  data: 0.0002  max mem: 5511
[10:18:48.699132] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.4025 (0.4355)  acc1: 89.0625 (87.1479)  acc5: 100.0000 (99.2738)  time: 0.0281  data: 0.0002  max mem: 5511
[10:18:48.983800] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4114 (0.4444)  acc1: 87.5000 (86.7670)  acc5: 100.0000 (99.2091)  time: 0.0281  data: 0.0001  max mem: 5511
[10:18:49.264396] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4423 (0.4407)  acc1: 85.9375 (86.9849)  acc5: 98.4375 (99.2273)  time: 0.0281  data: 0.0001  max mem: 5511
[10:18:49.554789] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4279 (0.4443)  acc1: 87.5000 (86.8502)  acc5: 100.0000 (99.2729)  time: 0.0284  data: 0.0001  max mem: 5511
[10:18:49.838667] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4181 (0.4431)  acc1: 87.5000 (86.9651)  acc5: 100.0000 (99.3243)  time: 0.0285  data: 0.0002  max mem: 5511
[10:18:50.124388] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.3971 (0.4395)  acc1: 89.0625 (87.1772)  acc5: 100.0000 (99.3543)  time: 0.0282  data: 0.0002  max mem: 5511
[10:18:50.412751] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.3971 (0.4404)  acc1: 87.5000 (87.1064)  acc5: 100.0000 (99.3559)  time: 0.0284  data: 0.0001  max mem: 5511
[10:18:50.694037] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4117 (0.4389)  acc1: 85.9375 (87.0567)  acc5: 100.0000 (99.3794)  time: 0.0283  data: 0.0001  max mem: 5511
[10:18:50.976160] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4143 (0.4384)  acc1: 85.9375 (87.0550)  acc5: 100.0000 (99.3791)  time: 0.0280  data: 0.0001  max mem: 5511
[10:18:51.128544] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4092 (0.4383)  acc1: 87.5000 (87.0400)  acc5: 100.0000 (99.3900)  time: 0.0271  data: 0.0001  max mem: 5511
[10:18:51.275296] Test: Total time: 0:00:05 (0.0326 s / it)
[10:18:51.276796] * Acc@1 87.040 Acc@5 99.390 loss 0.438
[10:18:51.277914] Accuracy of the network on the 10000 test images: 87.0%
[10:18:51.278727] Max accuracy: 87.04%
[10:18:51.590584] log_dir: ./output_dir
[10:18:52.379243] Epoch: [66]  [  0/781]  eta: 0:10:13  lr: 0.000072  training_loss: 0.3408 (0.3408)  mae_loss: 0.2337 (0.2337)  classification_loss: 0.1071 (0.1071)  time: 0.7858  data: 0.5626  max mem: 5511
[10:18:56.305388] Epoch: [66]  [ 20/781]  eta: 0:02:50  lr: 0.000072  training_loss: 0.3267 (0.3256)  mae_loss: 0.2118 (0.2134)  classification_loss: 0.1115 (0.1122)  time: 0.1962  data: 0.0003  max mem: 5511
[10:19:00.199284] Epoch: [66]  [ 40/781]  eta: 0:02:35  lr: 0.000072  training_loss: 0.3102 (0.3197)  mae_loss: 0.1942 (0.2061)  classification_loss: 0.1164 (0.1136)  time: 0.1946  data: 0.0002  max mem: 5511
[10:19:04.107793] Epoch: [66]  [ 60/781]  eta: 0:02:27  lr: 0.000071  training_loss: 0.3286 (0.3237)  mae_loss: 0.2080 (0.2088)  classification_loss: 0.1179 (0.1149)  time: 0.1953  data: 0.0002  max mem: 5511
[10:19:08.004058] Epoch: [66]  [ 80/781]  eta: 0:02:21  lr: 0.000071  training_loss: 0.3095 (0.3224)  mae_loss: 0.2013 (0.2073)  classification_loss: 0.1133 (0.1151)  time: 0.1947  data: 0.0002  max mem: 5511
[10:19:11.918403] Epoch: [66]  [100/781]  eta: 0:02:16  lr: 0.000071  training_loss: 0.3141 (0.3211)  mae_loss: 0.2020 (0.2064)  classification_loss: 0.1133 (0.1148)  time: 0.1956  data: 0.0002  max mem: 5511
[10:19:15.811040] Epoch: [66]  [120/781]  eta: 0:02:12  lr: 0.000071  training_loss: 0.3202 (0.3211)  mae_loss: 0.2122 (0.2069)  classification_loss: 0.1088 (0.1142)  time: 0.1945  data: 0.0003  max mem: 5511
[10:19:19.718021] Epoch: [66]  [140/781]  eta: 0:02:07  lr: 0.000071  training_loss: 0.3059 (0.3199)  mae_loss: 0.1888 (0.2058)  classification_loss: 0.1127 (0.1141)  time: 0.1953  data: 0.0003  max mem: 5511
[10:19:23.629247] Epoch: [66]  [160/781]  eta: 0:02:03  lr: 0.000071  training_loss: 0.3369 (0.3214)  mae_loss: 0.2178 (0.2068)  classification_loss: 0.1165 (0.1146)  time: 0.1955  data: 0.0002  max mem: 5511
[10:19:27.604724] Epoch: [66]  [180/781]  eta: 0:01:59  lr: 0.000071  training_loss: 0.3247 (0.3224)  mae_loss: 0.2168 (0.2080)  classification_loss: 0.1099 (0.1144)  time: 0.1987  data: 0.0002  max mem: 5511
[10:19:31.500898] Epoch: [66]  [200/781]  eta: 0:01:55  lr: 0.000071  training_loss: 0.3229 (0.3226)  mae_loss: 0.2046 (0.2082)  classification_loss: 0.1125 (0.1144)  time: 0.1947  data: 0.0002  max mem: 5511
[10:19:35.395704] Epoch: [66]  [220/781]  eta: 0:01:51  lr: 0.000071  training_loss: 0.3272 (0.3227)  mae_loss: 0.2086 (0.2081)  classification_loss: 0.1158 (0.1146)  time: 0.1946  data: 0.0002  max mem: 5511
[10:19:39.323088] Epoch: [66]  [240/781]  eta: 0:01:47  lr: 0.000071  training_loss: 0.3174 (0.3222)  mae_loss: 0.2034 (0.2074)  classification_loss: 0.1181 (0.1148)  time: 0.1963  data: 0.0003  max mem: 5511
[10:19:43.320318] Epoch: [66]  [260/781]  eta: 0:01:43  lr: 0.000071  training_loss: 0.3137 (0.3220)  mae_loss: 0.2025 (0.2074)  classification_loss: 0.1106 (0.1147)  time: 0.1998  data: 0.0002  max mem: 5511
[10:19:47.248313] Epoch: [66]  [280/781]  eta: 0:01:39  lr: 0.000070  training_loss: 0.3229 (0.3225)  mae_loss: 0.2174 (0.2078)  classification_loss: 0.1149 (0.1147)  time: 0.1963  data: 0.0002  max mem: 5511
[10:19:51.195646] Epoch: [66]  [300/781]  eta: 0:01:35  lr: 0.000070  training_loss: 0.3224 (0.3226)  mae_loss: 0.2051 (0.2076)  classification_loss: 0.1197 (0.1151)  time: 0.1973  data: 0.0002  max mem: 5511
[10:19:55.119321] Epoch: [66]  [320/781]  eta: 0:01:31  lr: 0.000070  training_loss: 0.3238 (0.3230)  mae_loss: 0.2105 (0.2079)  classification_loss: 0.1160 (0.1151)  time: 0.1961  data: 0.0002  max mem: 5511
[10:19:59.055413] Epoch: [66]  [340/781]  eta: 0:01:27  lr: 0.000070  training_loss: 0.3114 (0.3228)  mae_loss: 0.1988 (0.2078)  classification_loss: 0.1152 (0.1150)  time: 0.1967  data: 0.0002  max mem: 5511
[10:20:02.956243] Epoch: [66]  [360/781]  eta: 0:01:23  lr: 0.000070  training_loss: 0.3228 (0.3232)  mae_loss: 0.2118 (0.2083)  classification_loss: 0.1137 (0.1149)  time: 0.1949  data: 0.0002  max mem: 5511
[10:20:06.843059] Epoch: [66]  [380/781]  eta: 0:01:19  lr: 0.000070  training_loss: 0.3202 (0.3232)  mae_loss: 0.2133 (0.2085)  classification_loss: 0.1137 (0.1147)  time: 0.1942  data: 0.0002  max mem: 5511
[10:20:10.730853] Epoch: [66]  [400/781]  eta: 0:01:15  lr: 0.000070  training_loss: 0.3153 (0.3230)  mae_loss: 0.2016 (0.2082)  classification_loss: 0.1157 (0.1149)  time: 0.1943  data: 0.0003  max mem: 5511
[10:20:14.628091] Epoch: [66]  [420/781]  eta: 0:01:11  lr: 0.000070  training_loss: 0.3357 (0.3236)  mae_loss: 0.2189 (0.2088)  classification_loss: 0.1146 (0.1148)  time: 0.1948  data: 0.0003  max mem: 5511
[10:20:18.523578] Epoch: [66]  [440/781]  eta: 0:01:07  lr: 0.000070  training_loss: 0.3141 (0.3234)  mae_loss: 0.2053 (0.2087)  classification_loss: 0.1106 (0.1147)  time: 0.1947  data: 0.0002  max mem: 5511
[10:20:22.427869] Epoch: [66]  [460/781]  eta: 0:01:03  lr: 0.000070  training_loss: 0.3093 (0.3235)  mae_loss: 0.2042 (0.2087)  classification_loss: 0.1123 (0.1147)  time: 0.1951  data: 0.0003  max mem: 5511
[10:20:26.345560] Epoch: [66]  [480/781]  eta: 0:00:59  lr: 0.000069  training_loss: 0.3141 (0.3235)  mae_loss: 0.1996 (0.2087)  classification_loss: 0.1135 (0.1148)  time: 0.1958  data: 0.0002  max mem: 5511
[10:20:30.243895] Epoch: [66]  [500/781]  eta: 0:00:55  lr: 0.000069  training_loss: 0.3202 (0.3234)  mae_loss: 0.1974 (0.2084)  classification_loss: 0.1166 (0.1149)  time: 0.1948  data: 0.0003  max mem: 5511
[10:20:34.156888] Epoch: [66]  [520/781]  eta: 0:00:51  lr: 0.000069  training_loss: 0.3286 (0.3237)  mae_loss: 0.2103 (0.2086)  classification_loss: 0.1171 (0.1151)  time: 0.1955  data: 0.0002  max mem: 5511
[10:20:38.066831] Epoch: [66]  [540/781]  eta: 0:00:47  lr: 0.000069  training_loss: 0.3246 (0.3237)  mae_loss: 0.2089 (0.2085)  classification_loss: 0.1168 (0.1152)  time: 0.1954  data: 0.0002  max mem: 5511
[10:20:41.985988] Epoch: [66]  [560/781]  eta: 0:00:43  lr: 0.000069  training_loss: 0.3287 (0.3240)  mae_loss: 0.2065 (0.2088)  classification_loss: 0.1144 (0.1152)  time: 0.1959  data: 0.0002  max mem: 5511
[10:20:45.896153] Epoch: [66]  [580/781]  eta: 0:00:39  lr: 0.000069  training_loss: 0.3256 (0.3243)  mae_loss: 0.2128 (0.2091)  classification_loss: 0.1141 (0.1152)  time: 0.1954  data: 0.0006  max mem: 5511
[10:20:49.778936] Epoch: [66]  [600/781]  eta: 0:00:35  lr: 0.000069  training_loss: 0.3267 (0.3245)  mae_loss: 0.2128 (0.2092)  classification_loss: 0.1160 (0.1153)  time: 0.1941  data: 0.0002  max mem: 5511
[10:20:53.676985] Epoch: [66]  [620/781]  eta: 0:00:31  lr: 0.000069  training_loss: 0.3217 (0.3246)  mae_loss: 0.2030 (0.2094)  classification_loss: 0.1095 (0.1152)  time: 0.1948  data: 0.0002  max mem: 5511
[10:20:57.592630] Epoch: [66]  [640/781]  eta: 0:00:27  lr: 0.000069  training_loss: 0.3143 (0.3244)  mae_loss: 0.2064 (0.2093)  classification_loss: 0.1120 (0.1152)  time: 0.1957  data: 0.0003  max mem: 5511
[10:21:01.486934] Epoch: [66]  [660/781]  eta: 0:00:23  lr: 0.000069  training_loss: 0.3141 (0.3244)  mae_loss: 0.2009 (0.2092)  classification_loss: 0.1150 (0.1152)  time: 0.1946  data: 0.0003  max mem: 5511
[10:21:05.422008] Epoch: [66]  [680/781]  eta: 0:00:19  lr: 0.000069  training_loss: 0.3257 (0.3245)  mae_loss: 0.2183 (0.2094)  classification_loss: 0.1132 (0.1151)  time: 0.1967  data: 0.0002  max mem: 5511
[10:21:09.354167] Epoch: [66]  [700/781]  eta: 0:00:15  lr: 0.000068  training_loss: 0.3274 (0.3248)  mae_loss: 0.2150 (0.2095)  classification_loss: 0.1185 (0.1152)  time: 0.1965  data: 0.0002  max mem: 5511
[10:21:13.262167] Epoch: [66]  [720/781]  eta: 0:00:11  lr: 0.000068  training_loss: 0.3261 (0.3249)  mae_loss: 0.2107 (0.2096)  classification_loss: 0.1171 (0.1153)  time: 0.1953  data: 0.0002  max mem: 5511
[10:21:17.158373] Epoch: [66]  [740/781]  eta: 0:00:08  lr: 0.000068  training_loss: 0.3115 (0.3247)  mae_loss: 0.1988 (0.2095)  classification_loss: 0.1128 (0.1152)  time: 0.1947  data: 0.0002  max mem: 5511
[10:21:21.075566] Epoch: [66]  [760/781]  eta: 0:00:04  lr: 0.000068  training_loss: 0.3128 (0.3246)  mae_loss: 0.1992 (0.2092)  classification_loss: 0.1194 (0.1154)  time: 0.1958  data: 0.0002  max mem: 5511
[10:21:24.963460] Epoch: [66]  [780/781]  eta: 0:00:00  lr: 0.000068  training_loss: 0.3299 (0.3247)  mae_loss: 0.2126 (0.2093)  classification_loss: 0.1143 (0.1154)  time: 0.1943  data: 0.0002  max mem: 5511
[10:21:25.115441] Epoch: [66] Total time: 0:02:33 (0.1966 s / it)
[10:21:25.116232] Averaged stats: lr: 0.000068  training_loss: 0.3299 (0.3247)  mae_loss: 0.2126 (0.2093)  classification_loss: 0.1143 (0.1154)
[10:21:25.724899] Test:  [  0/157]  eta: 0:01:34  testing_loss: 0.5102 (0.5102)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.5996  data: 0.5689  max mem: 5511
[10:21:26.016274] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.4842 (0.4657)  acc1: 85.9375 (85.0852)  acc5: 100.0000 (99.4318)  time: 0.0808  data: 0.0519  max mem: 5511
[10:21:26.296539] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4292 (0.4399)  acc1: 85.9375 (86.8304)  acc5: 100.0000 (99.5536)  time: 0.0284  data: 0.0001  max mem: 5511
[10:21:26.577081] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.4141 (0.4436)  acc1: 87.5000 (86.9960)  acc5: 100.0000 (99.4456)  time: 0.0279  data: 0.0001  max mem: 5511
[10:21:26.858328] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.4526 (0.4529)  acc1: 85.9375 (86.8902)  acc5: 100.0000 (99.2759)  time: 0.0280  data: 0.0001  max mem: 5511
[10:21:27.139903] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4526 (0.4445)  acc1: 87.5000 (87.1017)  acc5: 100.0000 (99.2953)  time: 0.0280  data: 0.0001  max mem: 5511
[10:21:27.423206] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.4082 (0.4406)  acc1: 89.0625 (87.4488)  acc5: 100.0000 (99.2828)  time: 0.0281  data: 0.0001  max mem: 5511
[10:21:27.704704] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.3555 (0.4297)  acc1: 89.0625 (87.7641)  acc5: 100.0000 (99.3398)  time: 0.0281  data: 0.0001  max mem: 5511
[10:21:27.986279] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.3948 (0.4383)  acc1: 87.5000 (87.4228)  acc5: 100.0000 (99.2863)  time: 0.0280  data: 0.0001  max mem: 5511
[10:21:28.268137] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4288 (0.4343)  acc1: 87.5000 (87.5172)  acc5: 100.0000 (99.2960)  time: 0.0281  data: 0.0001  max mem: 5511
[10:21:28.549091] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4444 (0.4387)  acc1: 87.5000 (87.3762)  acc5: 100.0000 (99.3348)  time: 0.0280  data: 0.0001  max mem: 5511
[10:21:28.830044] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4350 (0.4380)  acc1: 85.9375 (87.3170)  acc5: 100.0000 (99.3525)  time: 0.0280  data: 0.0001  max mem: 5511
[10:21:29.111406] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4118 (0.4336)  acc1: 87.5000 (87.4613)  acc5: 100.0000 (99.3802)  time: 0.0280  data: 0.0001  max mem: 5511
[10:21:29.394364] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4084 (0.4353)  acc1: 87.5000 (87.3927)  acc5: 100.0000 (99.3440)  time: 0.0281  data: 0.0002  max mem: 5511
[10:21:29.677216] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3982 (0.4337)  acc1: 87.5000 (87.3116)  acc5: 100.0000 (99.3905)  time: 0.0282  data: 0.0002  max mem: 5511
[10:21:29.955896] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4074 (0.4336)  acc1: 87.5000 (87.3965)  acc5: 100.0000 (99.3895)  time: 0.0280  data: 0.0001  max mem: 5511
[10:21:30.105138] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4056 (0.4324)  acc1: 89.0625 (87.4100)  acc5: 100.0000 (99.3800)  time: 0.0269  data: 0.0001  max mem: 5511
[10:21:30.258020] Test: Total time: 0:00:05 (0.0327 s / it)
[10:21:30.258747] * Acc@1 87.410 Acc@5 99.380 loss 0.432
[10:21:30.259066] Accuracy of the network on the 10000 test images: 87.4%
[10:21:30.259253] Max accuracy: 87.41%
[10:21:30.674119] log_dir: ./output_dir
[10:21:31.477997] Epoch: [67]  [  0/781]  eta: 0:10:26  lr: 0.000068  training_loss: 0.3129 (0.3129)  mae_loss: 0.2035 (0.2035)  classification_loss: 0.1093 (0.1093)  time: 0.8022  data: 0.6007  max mem: 5511
[10:21:35.403601] Epoch: [67]  [ 20/781]  eta: 0:02:51  lr: 0.000068  training_loss: 0.3239 (0.3266)  mae_loss: 0.2128 (0.2132)  classification_loss: 0.1096 (0.1134)  time: 0.1962  data: 0.0003  max mem: 5511
[10:21:39.348828] Epoch: [67]  [ 40/781]  eta: 0:02:36  lr: 0.000068  training_loss: 0.3232 (0.3259)  mae_loss: 0.2049 (0.2119)  classification_loss: 0.1159 (0.1141)  time: 0.1972  data: 0.0002  max mem: 5511
[10:21:43.249711] Epoch: [67]  [ 60/781]  eta: 0:02:28  lr: 0.000068  training_loss: 0.3115 (0.3230)  mae_loss: 0.1925 (0.2079)  classification_loss: 0.1178 (0.1151)  time: 0.1949  data: 0.0002  max mem: 5511
[10:21:47.156154] Epoch: [67]  [ 80/781]  eta: 0:02:22  lr: 0.000068  training_loss: 0.3068 (0.3219)  mae_loss: 0.1981 (0.2068)  classification_loss: 0.1115 (0.1150)  time: 0.1952  data: 0.0002  max mem: 5511
[10:21:51.168807] Epoch: [67]  [100/781]  eta: 0:02:18  lr: 0.000068  training_loss: 0.3090 (0.3207)  mae_loss: 0.2013 (0.2062)  classification_loss: 0.1106 (0.1145)  time: 0.2006  data: 0.0002  max mem: 5511
[10:21:55.107704] Epoch: [67]  [120/781]  eta: 0:02:13  lr: 0.000068  training_loss: 0.3051 (0.3187)  mae_loss: 0.1887 (0.2045)  classification_loss: 0.1107 (0.1142)  time: 0.1969  data: 0.0003  max mem: 5511
[10:21:59.027793] Epoch: [67]  [140/781]  eta: 0:02:08  lr: 0.000067  training_loss: 0.3160 (0.3195)  mae_loss: 0.2044 (0.2051)  classification_loss: 0.1153 (0.1144)  time: 0.1959  data: 0.0002  max mem: 5511
[10:22:02.933480] Epoch: [67]  [160/781]  eta: 0:02:04  lr: 0.000067  training_loss: 0.3136 (0.3187)  mae_loss: 0.1988 (0.2045)  classification_loss: 0.1127 (0.1142)  time: 0.1952  data: 0.0002  max mem: 5511
[10:22:06.829408] Epoch: [67]  [180/781]  eta: 0:01:59  lr: 0.000067  training_loss: 0.3091 (0.3188)  mae_loss: 0.1938 (0.2044)  classification_loss: 0.1154 (0.1143)  time: 0.1947  data: 0.0002  max mem: 5511
[10:22:10.782390] Epoch: [67]  [200/781]  eta: 0:01:55  lr: 0.000067  training_loss: 0.3195 (0.3198)  mae_loss: 0.2112 (0.2056)  classification_loss: 0.1123 (0.1142)  time: 0.1976  data: 0.0002  max mem: 5511
[10:22:14.701858] Epoch: [67]  [220/781]  eta: 0:01:51  lr: 0.000067  training_loss: 0.3220 (0.3204)  mae_loss: 0.2095 (0.2060)  classification_loss: 0.1164 (0.1144)  time: 0.1958  data: 0.0004  max mem: 5511
[10:22:18.607001] Epoch: [67]  [240/781]  eta: 0:01:47  lr: 0.000067  training_loss: 0.3235 (0.3209)  mae_loss: 0.2070 (0.2064)  classification_loss: 0.1149 (0.1144)  time: 0.1952  data: 0.0002  max mem: 5511
[10:22:22.537613] Epoch: [67]  [260/781]  eta: 0:01:43  lr: 0.000067  training_loss: 0.3159 (0.3204)  mae_loss: 0.2007 (0.2060)  classification_loss: 0.1153 (0.1144)  time: 0.1964  data: 0.0002  max mem: 5511
[10:22:26.468975] Epoch: [67]  [280/781]  eta: 0:01:39  lr: 0.000067  training_loss: 0.3269 (0.3211)  mae_loss: 0.2125 (0.2067)  classification_loss: 0.1142 (0.1144)  time: 0.1965  data: 0.0002  max mem: 5511
[10:22:30.374469] Epoch: [67]  [300/781]  eta: 0:01:35  lr: 0.000067  training_loss: 0.3194 (0.3213)  mae_loss: 0.1964 (0.2068)  classification_loss: 0.1136 (0.1144)  time: 0.1952  data: 0.0002  max mem: 5511
[10:22:34.298949] Epoch: [67]  [320/781]  eta: 0:01:31  lr: 0.000067  training_loss: 0.3255 (0.3215)  mae_loss: 0.2098 (0.2071)  classification_loss: 0.1142 (0.1145)  time: 0.1961  data: 0.0003  max mem: 5511
[10:22:38.221497] Epoch: [67]  [340/781]  eta: 0:01:27  lr: 0.000066  training_loss: 0.3365 (0.3223)  mae_loss: 0.2161 (0.2077)  classification_loss: 0.1131 (0.1146)  time: 0.1961  data: 0.0002  max mem: 5511
[10:22:42.111296] Epoch: [67]  [360/781]  eta: 0:01:23  lr: 0.000066  training_loss: 0.3115 (0.3219)  mae_loss: 0.2011 (0.2076)  classification_loss: 0.1075 (0.1143)  time: 0.1944  data: 0.0002  max mem: 5511
[10:22:46.013956] Epoch: [67]  [380/781]  eta: 0:01:19  lr: 0.000066  training_loss: 0.3191 (0.3220)  mae_loss: 0.2076 (0.2078)  classification_loss: 0.1101 (0.1142)  time: 0.1951  data: 0.0002  max mem: 5511
[10:22:49.936432] Epoch: [67]  [400/781]  eta: 0:01:15  lr: 0.000066  training_loss: 0.3126 (0.3222)  mae_loss: 0.1998 (0.2079)  classification_loss: 0.1139 (0.1143)  time: 0.1960  data: 0.0002  max mem: 5511
[10:22:53.869081] Epoch: [67]  [420/781]  eta: 0:01:11  lr: 0.000066  training_loss: 0.3275 (0.3224)  mae_loss: 0.2067 (0.2081)  classification_loss: 0.1143 (0.1143)  time: 0.1966  data: 0.0003  max mem: 5511
[10:22:57.764601] Epoch: [67]  [440/781]  eta: 0:01:07  lr: 0.000066  training_loss: 0.3181 (0.3224)  mae_loss: 0.2054 (0.2082)  classification_loss: 0.1121 (0.1142)  time: 0.1947  data: 0.0002  max mem: 5511
[10:23:01.693969] Epoch: [67]  [460/781]  eta: 0:01:03  lr: 0.000066  training_loss: 0.3052 (0.3216)  mae_loss: 0.1942 (0.2074)  classification_loss: 0.1116 (0.1142)  time: 0.1964  data: 0.0002  max mem: 5511
[10:23:05.604818] Epoch: [67]  [480/781]  eta: 0:00:59  lr: 0.000066  training_loss: 0.3174 (0.3216)  mae_loss: 0.1994 (0.2074)  classification_loss: 0.1148 (0.1142)  time: 0.1955  data: 0.0002  max mem: 5511
[10:23:09.572437] Epoch: [67]  [500/781]  eta: 0:00:55  lr: 0.000066  training_loss: 0.3259 (0.3218)  mae_loss: 0.2088 (0.2076)  classification_loss: 0.1108 (0.1142)  time: 0.1983  data: 0.0002  max mem: 5511
[10:23:13.473443] Epoch: [67]  [520/781]  eta: 0:00:51  lr: 0.000066  training_loss: 0.3264 (0.3220)  mae_loss: 0.2109 (0.2078)  classification_loss: 0.1155 (0.1142)  time: 0.1949  data: 0.0003  max mem: 5511
[10:23:17.363861] Epoch: [67]  [540/781]  eta: 0:00:47  lr: 0.000066  training_loss: 0.3109 (0.3218)  mae_loss: 0.1963 (0.2075)  classification_loss: 0.1146 (0.1143)  time: 0.1944  data: 0.0002  max mem: 5511
[10:23:21.281603] Epoch: [67]  [560/781]  eta: 0:00:43  lr: 0.000065  training_loss: 0.3216 (0.3218)  mae_loss: 0.2080 (0.2077)  classification_loss: 0.1107 (0.1142)  time: 0.1958  data: 0.0002  max mem: 5511
[10:23:25.181745] Epoch: [67]  [580/781]  eta: 0:00:39  lr: 0.000065  training_loss: 0.3233 (0.3221)  mae_loss: 0.2173 (0.2080)  classification_loss: 0.1111 (0.1141)  time: 0.1949  data: 0.0002  max mem: 5511
[10:23:29.083522] Epoch: [67]  [600/781]  eta: 0:00:35  lr: 0.000065  training_loss: 0.3125 (0.3221)  mae_loss: 0.2060 (0.2082)  classification_loss: 0.1068 (0.1139)  time: 0.1950  data: 0.0002  max mem: 5511
[10:23:32.986817] Epoch: [67]  [620/781]  eta: 0:00:31  lr: 0.000065  training_loss: 0.3267 (0.3224)  mae_loss: 0.2144 (0.2084)  classification_loss: 0.1151 (0.1140)  time: 0.1951  data: 0.0002  max mem: 5511
[10:23:36.884662] Epoch: [67]  [640/781]  eta: 0:00:27  lr: 0.000065  training_loss: 0.3322 (0.3227)  mae_loss: 0.2104 (0.2086)  classification_loss: 0.1154 (0.1141)  time: 0.1948  data: 0.0002  max mem: 5511
[10:23:40.788669] Epoch: [67]  [660/781]  eta: 0:00:23  lr: 0.000065  training_loss: 0.3248 (0.3229)  mae_loss: 0.2084 (0.2088)  classification_loss: 0.1111 (0.1141)  time: 0.1951  data: 0.0002  max mem: 5511
[10:23:44.733606] Epoch: [67]  [680/781]  eta: 0:00:19  lr: 0.000065  training_loss: 0.3108 (0.3227)  mae_loss: 0.1973 (0.2086)  classification_loss: 0.1146 (0.1141)  time: 0.1972  data: 0.0003  max mem: 5511
[10:23:48.691746] Epoch: [67]  [700/781]  eta: 0:00:15  lr: 0.000065  training_loss: 0.3275 (0.3229)  mae_loss: 0.2094 (0.2087)  classification_loss: 0.1171 (0.1142)  time: 0.1978  data: 0.0003  max mem: 5511
[10:23:52.589241] Epoch: [67]  [720/781]  eta: 0:00:12  lr: 0.000065  training_loss: 0.3244 (0.3231)  mae_loss: 0.2090 (0.2088)  classification_loss: 0.1160 (0.1143)  time: 0.1948  data: 0.0002  max mem: 5511
[10:23:56.503577] Epoch: [67]  [740/781]  eta: 0:00:08  lr: 0.000065  training_loss: 0.3055 (0.3228)  mae_loss: 0.1849 (0.2085)  classification_loss: 0.1116 (0.1143)  time: 0.1956  data: 0.0002  max mem: 5511
[10:24:00.416917] Epoch: [67]  [760/781]  eta: 0:00:04  lr: 0.000065  training_loss: 0.3170 (0.3227)  mae_loss: 0.1975 (0.2083)  classification_loss: 0.1173 (0.1144)  time: 0.1956  data: 0.0002  max mem: 5511
[10:24:04.312287] Epoch: [67]  [780/781]  eta: 0:00:00  lr: 0.000064  training_loss: 0.3267 (0.3231)  mae_loss: 0.2102 (0.2086)  classification_loss: 0.1158 (0.1145)  time: 0.1947  data: 0.0002  max mem: 5511
[10:24:04.476404] Epoch: [67] Total time: 0:02:33 (0.1969 s / it)
[10:24:04.476889] Averaged stats: lr: 0.000064  training_loss: 0.3267 (0.3231)  mae_loss: 0.2102 (0.2086)  classification_loss: 0.1158 (0.1145)
[10:24:05.038999] Test:  [  0/157]  eta: 0:01:27  testing_loss: 0.4517 (0.4517)  acc1: 90.6250 (90.6250)  acc5: 100.0000 (100.0000)  time: 0.5569  data: 0.5236  max mem: 5511
[10:24:05.325516] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.4673 (0.4545)  acc1: 87.5000 (86.5057)  acc5: 100.0000 (99.8580)  time: 0.0765  data: 0.0478  max mem: 5511
[10:24:05.619975] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4389 (0.4319)  acc1: 87.5000 (87.4256)  acc5: 100.0000 (99.7768)  time: 0.0289  data: 0.0002  max mem: 5511
[10:24:05.902267] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.4111 (0.4384)  acc1: 87.5000 (87.3992)  acc5: 100.0000 (99.5968)  time: 0.0287  data: 0.0002  max mem: 5511
[10:24:06.184071] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.4524 (0.4489)  acc1: 87.5000 (86.8902)  acc5: 100.0000 (99.4665)  time: 0.0280  data: 0.0002  max mem: 5511
[10:24:06.465366] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4123 (0.4380)  acc1: 87.5000 (87.4387)  acc5: 100.0000 (99.4792)  time: 0.0280  data: 0.0002  max mem: 5511
[10:24:06.746852] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.3801 (0.4333)  acc1: 89.0625 (87.6537)  acc5: 100.0000 (99.4109)  time: 0.0280  data: 0.0002  max mem: 5511
[10:24:07.028197] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.3792 (0.4250)  acc1: 89.0625 (87.8521)  acc5: 100.0000 (99.4938)  time: 0.0280  data: 0.0002  max mem: 5511
[10:24:07.309377] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4034 (0.4320)  acc1: 89.0625 (87.6350)  acc5: 100.0000 (99.4599)  time: 0.0280  data: 0.0002  max mem: 5511
[10:24:07.591669] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4312 (0.4274)  acc1: 89.0625 (87.8091)  acc5: 100.0000 (99.4849)  time: 0.0280  data: 0.0001  max mem: 5511
[10:24:07.880027] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4312 (0.4334)  acc1: 87.5000 (87.5928)  acc5: 100.0000 (99.4895)  time: 0.0284  data: 0.0002  max mem: 5511
[10:24:08.166470] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4097 (0.4314)  acc1: 85.9375 (87.5985)  acc5: 100.0000 (99.5214)  time: 0.0286  data: 0.0002  max mem: 5511
[10:24:08.451619] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.3891 (0.4272)  acc1: 87.5000 (87.6808)  acc5: 100.0000 (99.5222)  time: 0.0285  data: 0.0003  max mem: 5511
[10:24:08.732670] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.3972 (0.4282)  acc1: 87.5000 (87.5835)  acc5: 100.0000 (99.5110)  time: 0.0282  data: 0.0002  max mem: 5511
[10:24:09.012984] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4128 (0.4276)  acc1: 87.5000 (87.5332)  acc5: 100.0000 (99.5346)  time: 0.0280  data: 0.0001  max mem: 5511
[10:24:09.292839] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4468 (0.4274)  acc1: 87.5000 (87.5724)  acc5: 100.0000 (99.5240)  time: 0.0279  data: 0.0001  max mem: 5511
[10:24:09.442543] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4386 (0.4273)  acc1: 87.5000 (87.5200)  acc5: 100.0000 (99.5100)  time: 0.0269  data: 0.0001  max mem: 5511
[10:24:09.623949] Test: Total time: 0:00:05 (0.0328 s / it)
[10:24:09.624396] * Acc@1 87.520 Acc@5 99.510 loss 0.427
[10:24:09.624717] Accuracy of the network on the 10000 test images: 87.5%
[10:24:09.625133] Max accuracy: 87.52%
[10:24:09.810131] log_dir: ./output_dir
[10:24:10.675163] Epoch: [68]  [  0/781]  eta: 0:11:14  lr: 0.000064  training_loss: 0.2969 (0.2969)  mae_loss: 0.1993 (0.1993)  classification_loss: 0.0976 (0.0976)  time: 0.8634  data: 0.6517  max mem: 5511
[10:24:14.607589] Epoch: [68]  [ 20/781]  eta: 0:02:53  lr: 0.000064  training_loss: 0.3245 (0.3222)  mae_loss: 0.2078 (0.2064)  classification_loss: 0.1135 (0.1158)  time: 0.1965  data: 0.0002  max mem: 5511
[10:24:18.509904] Epoch: [68]  [ 40/781]  eta: 0:02:37  lr: 0.000064  training_loss: 0.3272 (0.3234)  mae_loss: 0.2187 (0.2085)  classification_loss: 0.1135 (0.1149)  time: 0.1950  data: 0.0002  max mem: 5511
[10:24:22.420420] Epoch: [68]  [ 60/781]  eta: 0:02:28  lr: 0.000064  training_loss: 0.3228 (0.3247)  mae_loss: 0.2013 (0.2091)  classification_loss: 0.1162 (0.1156)  time: 0.1954  data: 0.0002  max mem: 5511
[10:24:26.362211] Epoch: [68]  [ 80/781]  eta: 0:02:23  lr: 0.000064  training_loss: 0.3288 (0.3259)  mae_loss: 0.2193 (0.2110)  classification_loss: 0.1114 (0.1148)  time: 0.1970  data: 0.0002  max mem: 5511
[10:24:30.256686] Epoch: [68]  [100/781]  eta: 0:02:17  lr: 0.000064  training_loss: 0.3263 (0.3265)  mae_loss: 0.2113 (0.2117)  classification_loss: 0.1118 (0.1148)  time: 0.1946  data: 0.0002  max mem: 5511
[10:24:34.199244] Epoch: [68]  [120/781]  eta: 0:02:13  lr: 0.000064  training_loss: 0.3135 (0.3253)  mae_loss: 0.1994 (0.2102)  classification_loss: 0.1158 (0.1151)  time: 0.1971  data: 0.0002  max mem: 5511
[10:24:38.134126] Epoch: [68]  [140/781]  eta: 0:02:08  lr: 0.000064  training_loss: 0.3249 (0.3256)  mae_loss: 0.2140 (0.2106)  classification_loss: 0.1129 (0.1150)  time: 0.1967  data: 0.0002  max mem: 5511
[10:24:42.031815] Epoch: [68]  [160/781]  eta: 0:02:04  lr: 0.000064  training_loss: 0.3163 (0.3252)  mae_loss: 0.2002 (0.2100)  classification_loss: 0.1172 (0.1152)  time: 0.1948  data: 0.0002  max mem: 5511
[10:24:45.962841] Epoch: [68]  [180/781]  eta: 0:01:59  lr: 0.000064  training_loss: 0.3306 (0.3261)  mae_loss: 0.2091 (0.2110)  classification_loss: 0.1151 (0.1151)  time: 0.1964  data: 0.0002  max mem: 5511
[10:24:49.916262] Epoch: [68]  [200/781]  eta: 0:01:55  lr: 0.000064  training_loss: 0.3101 (0.3249)  mae_loss: 0.1966 (0.2099)  classification_loss: 0.1135 (0.1151)  time: 0.1976  data: 0.0002  max mem: 5511
[10:24:53.820843] Epoch: [68]  [220/781]  eta: 0:01:51  lr: 0.000063  training_loss: 0.3050 (0.3241)  mae_loss: 0.1945 (0.2093)  classification_loss: 0.1146 (0.1149)  time: 0.1952  data: 0.0003  max mem: 5511
[10:24:57.722049] Epoch: [68]  [240/781]  eta: 0:01:47  lr: 0.000063  training_loss: 0.3228 (0.3239)  mae_loss: 0.2089 (0.2091)  classification_loss: 0.1094 (0.1147)  time: 0.1950  data: 0.0002  max mem: 5511
[10:25:01.619429] Epoch: [68]  [260/781]  eta: 0:01:43  lr: 0.000063  training_loss: 0.3207 (0.3238)  mae_loss: 0.2072 (0.2093)  classification_loss: 0.1086 (0.1145)  time: 0.1948  data: 0.0002  max mem: 5511
[10:25:05.559582] Epoch: [68]  [280/781]  eta: 0:01:39  lr: 0.000063  training_loss: 0.3239 (0.3239)  mae_loss: 0.2066 (0.2094)  classification_loss: 0.1163 (0.1146)  time: 0.1969  data: 0.0002  max mem: 5511
[10:25:09.465197] Epoch: [68]  [300/781]  eta: 0:01:35  lr: 0.000063  training_loss: 0.3294 (0.3247)  mae_loss: 0.2183 (0.2102)  classification_loss: 0.1151 (0.1145)  time: 0.1952  data: 0.0002  max mem: 5511
[10:25:13.399263] Epoch: [68]  [320/781]  eta: 0:01:31  lr: 0.000063  training_loss: 0.3138 (0.3244)  mae_loss: 0.2042 (0.2098)  classification_loss: 0.1117 (0.1146)  time: 0.1966  data: 0.0002  max mem: 5511
[10:25:17.327831] Epoch: [68]  [340/781]  eta: 0:01:27  lr: 0.000063  training_loss: 0.3172 (0.3239)  mae_loss: 0.2077 (0.2096)  classification_loss: 0.1100 (0.1143)  time: 0.1964  data: 0.0002  max mem: 5511
[10:25:21.235366] Epoch: [68]  [360/781]  eta: 0:01:23  lr: 0.000063  training_loss: 0.3134 (0.3236)  mae_loss: 0.2009 (0.2091)  classification_loss: 0.1155 (0.1145)  time: 0.1953  data: 0.0003  max mem: 5511
[10:25:25.159623] Epoch: [68]  [380/781]  eta: 0:01:19  lr: 0.000063  training_loss: 0.3302 (0.3241)  mae_loss: 0.2156 (0.2097)  classification_loss: 0.1121 (0.1145)  time: 0.1961  data: 0.0003  max mem: 5511
[10:25:29.057619] Epoch: [68]  [400/781]  eta: 0:01:15  lr: 0.000063  training_loss: 0.3279 (0.3246)  mae_loss: 0.2164 (0.2101)  classification_loss: 0.1155 (0.1145)  time: 0.1948  data: 0.0003  max mem: 5511
[10:25:32.982938] Epoch: [68]  [420/781]  eta: 0:01:11  lr: 0.000063  training_loss: 0.3298 (0.3251)  mae_loss: 0.2161 (0.2105)  classification_loss: 0.1145 (0.1146)  time: 0.1962  data: 0.0002  max mem: 5511
[10:25:36.893832] Epoch: [68]  [440/781]  eta: 0:01:07  lr: 0.000062  training_loss: 0.3307 (0.3251)  mae_loss: 0.2062 (0.2104)  classification_loss: 0.1175 (0.1147)  time: 0.1954  data: 0.0002  max mem: 5511
[10:25:40.794146] Epoch: [68]  [460/781]  eta: 0:01:03  lr: 0.000062  training_loss: 0.3141 (0.3247)  mae_loss: 0.2053 (0.2101)  classification_loss: 0.1119 (0.1146)  time: 0.1949  data: 0.0002  max mem: 5511
[10:25:44.716724] Epoch: [68]  [480/781]  eta: 0:00:59  lr: 0.000062  training_loss: 0.3126 (0.3242)  mae_loss: 0.1944 (0.2097)  classification_loss: 0.1142 (0.1145)  time: 0.1960  data: 0.0002  max mem: 5511
[10:25:48.626775] Epoch: [68]  [500/781]  eta: 0:00:55  lr: 0.000062  training_loss: 0.3311 (0.3243)  mae_loss: 0.2101 (0.2098)  classification_loss: 0.1139 (0.1145)  time: 0.1954  data: 0.0002  max mem: 5511
[10:25:52.603632] Epoch: [68]  [520/781]  eta: 0:00:51  lr: 0.000062  training_loss: 0.3217 (0.3245)  mae_loss: 0.2128 (0.2100)  classification_loss: 0.1133 (0.1144)  time: 0.1988  data: 0.0003  max mem: 5511
[10:25:56.544211] Epoch: [68]  [540/781]  eta: 0:00:47  lr: 0.000062  training_loss: 0.3248 (0.3246)  mae_loss: 0.2088 (0.2101)  classification_loss: 0.1142 (0.1145)  time: 0.1970  data: 0.0003  max mem: 5511
[10:26:00.458456] Epoch: [68]  [560/781]  eta: 0:00:43  lr: 0.000062  training_loss: 0.3288 (0.3247)  mae_loss: 0.2088 (0.2102)  classification_loss: 0.1127 (0.1145)  time: 0.1956  data: 0.0002  max mem: 5511
[10:26:04.372313] Epoch: [68]  [580/781]  eta: 0:00:39  lr: 0.000062  training_loss: 0.3272 (0.3248)  mae_loss: 0.2173 (0.2102)  classification_loss: 0.1171 (0.1145)  time: 0.1956  data: 0.0002  max mem: 5511
[10:26:08.254953] Epoch: [68]  [600/781]  eta: 0:00:35  lr: 0.000062  training_loss: 0.3272 (0.3249)  mae_loss: 0.2066 (0.2104)  classification_loss: 0.1115 (0.1145)  time: 0.1941  data: 0.0002  max mem: 5511
[10:26:12.138766] Epoch: [68]  [620/781]  eta: 0:00:31  lr: 0.000062  training_loss: 0.3237 (0.3248)  mae_loss: 0.2081 (0.2104)  classification_loss: 0.1074 (0.1144)  time: 0.1941  data: 0.0002  max mem: 5511
[10:26:16.045729] Epoch: [68]  [640/781]  eta: 0:00:27  lr: 0.000062  training_loss: 0.3057 (0.3244)  mae_loss: 0.1938 (0.2101)  classification_loss: 0.1141 (0.1144)  time: 0.1953  data: 0.0003  max mem: 5511
[10:26:19.940245] Epoch: [68]  [660/781]  eta: 0:00:23  lr: 0.000061  training_loss: 0.3233 (0.3246)  mae_loss: 0.2132 (0.2102)  classification_loss: 0.1146 (0.1144)  time: 0.1946  data: 0.0003  max mem: 5511
[10:26:23.850624] Epoch: [68]  [680/781]  eta: 0:00:19  lr: 0.000061  training_loss: 0.3163 (0.3245)  mae_loss: 0.2035 (0.2103)  classification_loss: 0.1107 (0.1143)  time: 0.1954  data: 0.0003  max mem: 5511
[10:26:27.778433] Epoch: [68]  [700/781]  eta: 0:00:15  lr: 0.000061  training_loss: 0.3297 (0.3247)  mae_loss: 0.2124 (0.2104)  classification_loss: 0.1134 (0.1143)  time: 0.1963  data: 0.0002  max mem: 5511
[10:26:31.679458] Epoch: [68]  [720/781]  eta: 0:00:11  lr: 0.000061  training_loss: 0.3205 (0.3247)  mae_loss: 0.2117 (0.2104)  classification_loss: 0.1127 (0.1143)  time: 0.1950  data: 0.0002  max mem: 5511
[10:26:35.617995] Epoch: [68]  [740/781]  eta: 0:00:08  lr: 0.000061  training_loss: 0.3159 (0.3245)  mae_loss: 0.2036 (0.2103)  classification_loss: 0.1137 (0.1142)  time: 0.1968  data: 0.0003  max mem: 5511
[10:26:39.514690] Epoch: [68]  [760/781]  eta: 0:00:04  lr: 0.000061  training_loss: 0.3241 (0.3246)  mae_loss: 0.2130 (0.2103)  classification_loss: 0.1176 (0.1143)  time: 0.1948  data: 0.0002  max mem: 5511
[10:26:43.428473] Epoch: [68]  [780/781]  eta: 0:00:00  lr: 0.000061  training_loss: 0.3098 (0.3246)  mae_loss: 0.1989 (0.2103)  classification_loss: 0.1093 (0.1143)  time: 0.1956  data: 0.0002  max mem: 5511
[10:26:43.568571] Epoch: [68] Total time: 0:02:33 (0.1969 s / it)
[10:26:43.569077] Averaged stats: lr: 0.000061  training_loss: 0.3098 (0.3246)  mae_loss: 0.1989 (0.2103)  classification_loss: 0.1093 (0.1143)
[10:26:44.121808] Test:  [  0/157]  eta: 0:01:26  testing_loss: 0.4898 (0.4898)  acc1: 84.3750 (84.3750)  acc5: 98.4375 (98.4375)  time: 0.5481  data: 0.5173  max mem: 5511
[10:26:44.405609] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.4694 (0.4561)  acc1: 84.3750 (85.9375)  acc5: 100.0000 (99.2898)  time: 0.0754  data: 0.0472  max mem: 5511
[10:26:44.688692] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4675 (0.4401)  acc1: 85.9375 (86.9048)  acc5: 100.0000 (99.4792)  time: 0.0282  data: 0.0002  max mem: 5511
[10:26:44.971237] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.4156 (0.4415)  acc1: 89.0625 (86.9960)  acc5: 100.0000 (99.3952)  time: 0.0281  data: 0.0002  max mem: 5511
[10:26:45.251968] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.4580 (0.4529)  acc1: 85.9375 (86.6997)  acc5: 100.0000 (99.3140)  time: 0.0280  data: 0.0002  max mem: 5511
[10:26:45.532634] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4341 (0.4437)  acc1: 89.0625 (87.1017)  acc5: 100.0000 (99.2953)  time: 0.0280  data: 0.0002  max mem: 5511
[10:26:45.813798] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.3601 (0.4407)  acc1: 89.0625 (87.1414)  acc5: 100.0000 (99.2828)  time: 0.0280  data: 0.0002  max mem: 5511
[10:26:46.094677] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.3535 (0.4305)  acc1: 89.0625 (87.5440)  acc5: 100.0000 (99.3398)  time: 0.0280  data: 0.0002  max mem: 5511
[10:26:46.376461] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4028 (0.4372)  acc1: 89.0625 (87.3457)  acc5: 100.0000 (99.2863)  time: 0.0280  data: 0.0002  max mem: 5511
[10:26:46.658470] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4361 (0.4316)  acc1: 85.9375 (87.5515)  acc5: 100.0000 (99.2788)  time: 0.0281  data: 0.0001  max mem: 5511
[10:26:46.939036] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4288 (0.4348)  acc1: 87.5000 (87.4226)  acc5: 100.0000 (99.3193)  time: 0.0280  data: 0.0001  max mem: 5511
[10:26:47.220112] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4131 (0.4342)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (99.3243)  time: 0.0280  data: 0.0002  max mem: 5511
[10:26:47.501243] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4114 (0.4309)  acc1: 87.5000 (87.5517)  acc5: 100.0000 (99.3414)  time: 0.0280  data: 0.0002  max mem: 5511
[10:26:47.782585] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4301 (0.4326)  acc1: 85.9375 (87.4046)  acc5: 100.0000 (99.3082)  time: 0.0280  data: 0.0002  max mem: 5511
[10:26:48.067797] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.4084 (0.4315)  acc1: 87.5000 (87.4778)  acc5: 100.0000 (99.3573)  time: 0.0282  data: 0.0002  max mem: 5511
[10:26:48.348247] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4084 (0.4309)  acc1: 89.0625 (87.4897)  acc5: 100.0000 (99.3584)  time: 0.0281  data: 0.0001  max mem: 5511
[10:26:48.499953] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4225 (0.4315)  acc1: 87.5000 (87.4000)  acc5: 100.0000 (99.3500)  time: 0.0270  data: 0.0001  max mem: 5511
[10:26:48.644084] Test: Total time: 0:00:05 (0.0323 s / it)
[10:26:48.644537] * Acc@1 87.400 Acc@5 99.350 loss 0.432
[10:26:48.644828] Accuracy of the network on the 10000 test images: 87.4%
[10:26:48.645043] Max accuracy: 87.52%
[10:26:48.899200] log_dir: ./output_dir
[10:26:49.790199] Epoch: [69]  [  0/781]  eta: 0:11:34  lr: 0.000061  training_loss: 0.2973 (0.2973)  mae_loss: 0.1912 (0.1912)  classification_loss: 0.1061 (0.1061)  time: 0.8893  data: 0.6628  max mem: 5511
[10:26:53.696454] Epoch: [69]  [ 20/781]  eta: 0:02:53  lr: 0.000061  training_loss: 0.3306 (0.3225)  mae_loss: 0.2140 (0.2099)  classification_loss: 0.1116 (0.1126)  time: 0.1952  data: 0.0004  max mem: 5511
[10:26:57.595117] Epoch: [69]  [ 40/781]  eta: 0:02:37  lr: 0.000061  training_loss: 0.3123 (0.3187)  mae_loss: 0.1942 (0.2060)  classification_loss: 0.1130 (0.1127)  time: 0.1949  data: 0.0002  max mem: 5511
[10:27:01.514861] Epoch: [69]  [ 60/781]  eta: 0:02:29  lr: 0.000061  training_loss: 0.3145 (0.3207)  mae_loss: 0.2063 (0.2068)  classification_loss: 0.1157 (0.1139)  time: 0.1959  data: 0.0003  max mem: 5511
[10:27:05.439236] Epoch: [69]  [ 80/781]  eta: 0:02:23  lr: 0.000061  training_loss: 0.3223 (0.3221)  mae_loss: 0.2099 (0.2084)  classification_loss: 0.1104 (0.1137)  time: 0.1961  data: 0.0003  max mem: 5511
[10:27:09.334859] Epoch: [69]  [100/781]  eta: 0:02:17  lr: 0.000060  training_loss: 0.3205 (0.3226)  mae_loss: 0.2056 (0.2086)  classification_loss: 0.1162 (0.1141)  time: 0.1947  data: 0.0002  max mem: 5511
[10:27:13.268879] Epoch: [69]  [120/781]  eta: 0:02:13  lr: 0.000060  training_loss: 0.3236 (0.3229)  mae_loss: 0.2105 (0.2087)  classification_loss: 0.1151 (0.1142)  time: 0.1966  data: 0.0004  max mem: 5511
[10:27:17.229946] Epoch: [69]  [140/781]  eta: 0:02:08  lr: 0.000060  training_loss: 0.3147 (0.3220)  mae_loss: 0.2043 (0.2078)  classification_loss: 0.1133 (0.1142)  time: 0.1979  data: 0.0003  max mem: 5511
[10:27:21.140399] Epoch: [69]  [160/781]  eta: 0:02:04  lr: 0.000060  training_loss: 0.3064 (0.3210)  mae_loss: 0.1935 (0.2071)  classification_loss: 0.1106 (0.1139)  time: 0.1954  data: 0.0002  max mem: 5511
[10:27:25.034697] Epoch: [69]  [180/781]  eta: 0:01:59  lr: 0.000060  training_loss: 0.3097 (0.3211)  mae_loss: 0.1994 (0.2073)  classification_loss: 0.1151 (0.1138)  time: 0.1946  data: 0.0002  max mem: 5511
[10:27:28.948208] Epoch: [69]  [200/781]  eta: 0:01:55  lr: 0.000060  training_loss: 0.3088 (0.3204)  mae_loss: 0.1992 (0.2066)  classification_loss: 0.1154 (0.1138)  time: 0.1956  data: 0.0002  max mem: 5511
[10:27:32.844613] Epoch: [69]  [220/781]  eta: 0:01:51  lr: 0.000060  training_loss: 0.3111 (0.3195)  mae_loss: 0.1920 (0.2059)  classification_loss: 0.1130 (0.1137)  time: 0.1947  data: 0.0003  max mem: 5511
[10:27:36.766779] Epoch: [69]  [240/781]  eta: 0:01:47  lr: 0.000060  training_loss: 0.3213 (0.3199)  mae_loss: 0.2112 (0.2064)  classification_loss: 0.1126 (0.1135)  time: 0.1960  data: 0.0003  max mem: 5511
[10:27:40.710464] Epoch: [69]  [260/781]  eta: 0:01:43  lr: 0.000060  training_loss: 0.3211 (0.3201)  mae_loss: 0.1959 (0.2065)  classification_loss: 0.1128 (0.1136)  time: 0.1971  data: 0.0006  max mem: 5511
[10:27:44.635480] Epoch: [69]  [280/781]  eta: 0:01:39  lr: 0.000060  training_loss: 0.3273 (0.3206)  mae_loss: 0.2155 (0.2070)  classification_loss: 0.1147 (0.1136)  time: 0.1962  data: 0.0002  max mem: 5511
[10:27:48.542429] Epoch: [69]  [300/781]  eta: 0:01:35  lr: 0.000060  training_loss: 0.3206 (0.3207)  mae_loss: 0.2038 (0.2072)  classification_loss: 0.1119 (0.1136)  time: 0.1953  data: 0.0002  max mem: 5511
[10:27:52.451586] Epoch: [69]  [320/781]  eta: 0:01:31  lr: 0.000059  training_loss: 0.3217 (0.3207)  mae_loss: 0.2062 (0.2072)  classification_loss: 0.1142 (0.1135)  time: 0.1954  data: 0.0002  max mem: 5511
[10:27:56.374786] Epoch: [69]  [340/781]  eta: 0:01:27  lr: 0.000059  training_loss: 0.3143 (0.3206)  mae_loss: 0.2077 (0.2072)  classification_loss: 0.1066 (0.1134)  time: 0.1961  data: 0.0002  max mem: 5511
[10:28:00.292097] Epoch: [69]  [360/781]  eta: 0:01:23  lr: 0.000059  training_loss: 0.3189 (0.3203)  mae_loss: 0.2040 (0.2067)  classification_loss: 0.1138 (0.1136)  time: 0.1958  data: 0.0002  max mem: 5511
[10:28:04.187338] Epoch: [69]  [380/781]  eta: 0:01:19  lr: 0.000059  training_loss: 0.3268 (0.3209)  mae_loss: 0.2097 (0.2070)  classification_loss: 0.1159 (0.1139)  time: 0.1947  data: 0.0002  max mem: 5511
[10:28:08.091034] Epoch: [69]  [400/781]  eta: 0:01:15  lr: 0.000059  training_loss: 0.3304 (0.3213)  mae_loss: 0.2064 (0.2073)  classification_loss: 0.1179 (0.1140)  time: 0.1951  data: 0.0002  max mem: 5511
[10:28:12.002160] Epoch: [69]  [420/781]  eta: 0:01:11  lr: 0.000059  training_loss: 0.3106 (0.3213)  mae_loss: 0.2027 (0.2074)  classification_loss: 0.1106 (0.1139)  time: 0.1955  data: 0.0003  max mem: 5511
[10:28:15.901761] Epoch: [69]  [440/781]  eta: 0:01:07  lr: 0.000059  training_loss: 0.3148 (0.3212)  mae_loss: 0.1975 (0.2075)  classification_loss: 0.1093 (0.1138)  time: 0.1949  data: 0.0003  max mem: 5511
[10:28:19.814140] Epoch: [69]  [460/781]  eta: 0:01:03  lr: 0.000059  training_loss: 0.3178 (0.3214)  mae_loss: 0.2099 (0.2077)  classification_loss: 0.1131 (0.1138)  time: 0.1955  data: 0.0002  max mem: 5511
[10:28:23.710720] Epoch: [69]  [480/781]  eta: 0:00:59  lr: 0.000059  training_loss: 0.3135 (0.3213)  mae_loss: 0.2042 (0.2075)  classification_loss: 0.1129 (0.1138)  time: 0.1948  data: 0.0002  max mem: 5511
[10:28:27.624246] Epoch: [69]  [500/781]  eta: 0:00:55  lr: 0.000059  training_loss: 0.3232 (0.3214)  mae_loss: 0.2097 (0.2076)  classification_loss: 0.1131 (0.1138)  time: 0.1956  data: 0.0002  max mem: 5511
[10:28:31.542832] Epoch: [69]  [520/781]  eta: 0:00:51  lr: 0.000059  training_loss: 0.3069 (0.3212)  mae_loss: 0.1970 (0.2073)  classification_loss: 0.1148 (0.1139)  time: 0.1958  data: 0.0002  max mem: 5511
[10:28:35.435994] Epoch: [69]  [540/781]  eta: 0:00:47  lr: 0.000058  training_loss: 0.3195 (0.3212)  mae_loss: 0.2041 (0.2074)  classification_loss: 0.1126 (0.1138)  time: 0.1945  data: 0.0002  max mem: 5511
[10:28:39.370484] Epoch: [69]  [560/781]  eta: 0:00:43  lr: 0.000058  training_loss: 0.3114 (0.3210)  mae_loss: 0.2051 (0.2073)  classification_loss: 0.1111 (0.1137)  time: 0.1967  data: 0.0002  max mem: 5511
[10:28:43.272867] Epoch: [69]  [580/781]  eta: 0:00:39  lr: 0.000058  training_loss: 0.3192 (0.3212)  mae_loss: 0.2094 (0.2073)  classification_loss: 0.1161 (0.1138)  time: 0.1950  data: 0.0002  max mem: 5511
[10:28:47.174351] Epoch: [69]  [600/781]  eta: 0:00:35  lr: 0.000058  training_loss: 0.3258 (0.3214)  mae_loss: 0.2145 (0.2075)  classification_loss: 0.1138 (0.1139)  time: 0.1950  data: 0.0002  max mem: 5511
[10:28:51.167859] Epoch: [69]  [620/781]  eta: 0:00:31  lr: 0.000058  training_loss: 0.3164 (0.3217)  mae_loss: 0.2098 (0.2078)  classification_loss: 0.1096 (0.1139)  time: 0.1996  data: 0.0001  max mem: 5511
[10:28:55.107929] Epoch: [69]  [640/781]  eta: 0:00:27  lr: 0.000058  training_loss: 0.3201 (0.3217)  mae_loss: 0.2074 (0.2078)  classification_loss: 0.1126 (0.1139)  time: 0.1969  data: 0.0002  max mem: 5511
[10:28:59.024486] Epoch: [69]  [660/781]  eta: 0:00:23  lr: 0.000058  training_loss: 0.3228 (0.3217)  mae_loss: 0.2024 (0.2077)  classification_loss: 0.1172 (0.1140)  time: 0.1957  data: 0.0003  max mem: 5511
[10:29:02.929396] Epoch: [69]  [680/781]  eta: 0:00:19  lr: 0.000058  training_loss: 0.3197 (0.3217)  mae_loss: 0.2002 (0.2077)  classification_loss: 0.1124 (0.1139)  time: 0.1952  data: 0.0002  max mem: 5511
[10:29:06.836251] Epoch: [69]  [700/781]  eta: 0:00:15  lr: 0.000058  training_loss: 0.3302 (0.3219)  mae_loss: 0.2112 (0.2079)  classification_loss: 0.1144 (0.1140)  time: 0.1953  data: 0.0002  max mem: 5511
[10:29:10.738764] Epoch: [69]  [720/781]  eta: 0:00:11  lr: 0.000058  training_loss: 0.3351 (0.3220)  mae_loss: 0.2151 (0.2079)  classification_loss: 0.1144 (0.1141)  time: 0.1950  data: 0.0003  max mem: 5511
[10:29:14.632097] Epoch: [69]  [740/781]  eta: 0:00:08  lr: 0.000058  training_loss: 0.3246 (0.3220)  mae_loss: 0.2103 (0.2079)  classification_loss: 0.1128 (0.1141)  time: 0.1946  data: 0.0002  max mem: 5511
[10:29:18.530553] Epoch: [69]  [760/781]  eta: 0:00:04  lr: 0.000057  training_loss: 0.3259 (0.3221)  mae_loss: 0.2069 (0.2080)  classification_loss: 0.1187 (0.1142)  time: 0.1948  data: 0.0002  max mem: 5511
[10:29:22.449926] Epoch: [69]  [780/781]  eta: 0:00:00  lr: 0.000057  training_loss: 0.3293 (0.3223)  mae_loss: 0.2092 (0.2080)  classification_loss: 0.1170 (0.1143)  time: 0.1959  data: 0.0002  max mem: 5511
[10:29:22.619513] Epoch: [69] Total time: 0:02:33 (0.1968 s / it)
[10:29:22.620167] Averaged stats: lr: 0.000057  training_loss: 0.3293 (0.3223)  mae_loss: 0.2092 (0.2080)  classification_loss: 0.1170 (0.1143)
[10:29:23.304472] Test:  [  0/157]  eta: 0:01:46  testing_loss: 0.5016 (0.5016)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (100.0000)  time: 0.6803  data: 0.6461  max mem: 5511
[10:29:23.586382] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4771 (0.4538)  acc1: 85.9375 (86.3636)  acc5: 100.0000 (99.5739)  time: 0.0873  data: 0.0589  max mem: 5511
[10:29:23.868032] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4353 (0.4308)  acc1: 85.9375 (87.4256)  acc5: 100.0000 (99.5536)  time: 0.0280  data: 0.0002  max mem: 5511
[10:29:24.152072] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.3893 (0.4380)  acc1: 89.0625 (87.1976)  acc5: 100.0000 (99.5464)  time: 0.0282  data: 0.0002  max mem: 5511
[10:29:24.437001] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4545 (0.4493)  acc1: 87.5000 (86.7378)  acc5: 100.0000 (99.3521)  time: 0.0283  data: 0.0002  max mem: 5511
[10:29:24.721209] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4384 (0.4415)  acc1: 87.5000 (86.9485)  acc5: 100.0000 (99.3566)  time: 0.0283  data: 0.0002  max mem: 5511
[10:29:25.003377] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.3911 (0.4367)  acc1: 87.5000 (87.4232)  acc5: 100.0000 (99.3084)  time: 0.0282  data: 0.0002  max mem: 5511
[10:29:25.289170] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.3630 (0.4267)  acc1: 89.0625 (87.6981)  acc5: 100.0000 (99.3838)  time: 0.0282  data: 0.0003  max mem: 5511
[10:29:25.573083] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4035 (0.4335)  acc1: 89.0625 (87.5000)  acc5: 100.0000 (99.3441)  time: 0.0283  data: 0.0003  max mem: 5511
[10:29:25.868668] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4201 (0.4282)  acc1: 87.5000 (87.6889)  acc5: 100.0000 (99.3819)  time: 0.0288  data: 0.0002  max mem: 5511
[10:29:26.150668] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4201 (0.4311)  acc1: 87.5000 (87.5464)  acc5: 100.0000 (99.4276)  time: 0.0288  data: 0.0002  max mem: 5511
[10:29:26.436140] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4172 (0.4305)  acc1: 87.5000 (87.7252)  acc5: 100.0000 (99.4088)  time: 0.0282  data: 0.0002  max mem: 5511
[10:29:26.721956] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.4117 (0.4277)  acc1: 89.0625 (87.8616)  acc5: 100.0000 (99.3931)  time: 0.0284  data: 0.0002  max mem: 5511
[10:29:27.007320] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4032 (0.4293)  acc1: 87.5000 (87.6789)  acc5: 100.0000 (99.4036)  time: 0.0284  data: 0.0002  max mem: 5511
[10:29:27.289640] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3818 (0.4265)  acc1: 87.5000 (87.7327)  acc5: 100.0000 (99.4238)  time: 0.0283  data: 0.0002  max mem: 5511
[10:29:27.568423] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.3819 (0.4254)  acc1: 89.0625 (87.8104)  acc5: 100.0000 (99.4412)  time: 0.0279  data: 0.0001  max mem: 5511
[10:29:27.718300] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4117 (0.4248)  acc1: 87.5000 (87.7600)  acc5: 100.0000 (99.4400)  time: 0.0269  data: 0.0001  max mem: 5511
[10:29:27.872402] Test: Total time: 0:00:05 (0.0334 s / it)
[10:29:27.873372] * Acc@1 87.760 Acc@5 99.440 loss 0.425
[10:29:27.874230] Accuracy of the network on the 10000 test images: 87.8%
[10:29:27.874559] Max accuracy: 87.76%
[10:29:28.114625] log_dir: ./output_dir
[10:29:29.062314] Epoch: [70]  [  0/781]  eta: 0:12:18  lr: 0.000057  training_loss: 0.2630 (0.2630)  mae_loss: 0.1709 (0.1709)  classification_loss: 0.0921 (0.0921)  time: 0.9460  data: 0.7393  max mem: 5511
[10:29:32.978359] Epoch: [70]  [ 20/781]  eta: 0:02:56  lr: 0.000057  training_loss: 0.3142 (0.3140)  mae_loss: 0.2028 (0.2028)  classification_loss: 0.1089 (0.1111)  time: 0.1957  data: 0.0002  max mem: 5511
[10:29:36.874264] Epoch: [70]  [ 40/781]  eta: 0:02:38  lr: 0.000057  training_loss: 0.3137 (0.3151)  mae_loss: 0.2032 (0.2038)  classification_loss: 0.1104 (0.1113)  time: 0.1947  data: 0.0002  max mem: 5511
[10:29:40.791748] Epoch: [70]  [ 60/781]  eta: 0:02:29  lr: 0.000057  training_loss: 0.3123 (0.3179)  mae_loss: 0.2086 (0.2057)  classification_loss: 0.1143 (0.1122)  time: 0.1958  data: 0.0002  max mem: 5511
[10:29:44.696940] Epoch: [70]  [ 80/781]  eta: 0:02:23  lr: 0.000057  training_loss: 0.3023 (0.3169)  mae_loss: 0.1917 (0.2050)  classification_loss: 0.1078 (0.1120)  time: 0.1952  data: 0.0002  max mem: 5511
[10:29:48.608199] Epoch: [70]  [100/781]  eta: 0:02:18  lr: 0.000057  training_loss: 0.3037 (0.3146)  mae_loss: 0.1900 (0.2027)  classification_loss: 0.1124 (0.1119)  time: 0.1955  data: 0.0002  max mem: 5511
[10:29:52.522227] Epoch: [70]  [120/781]  eta: 0:02:13  lr: 0.000057  training_loss: 0.3057 (0.3152)  mae_loss: 0.1981 (0.2028)  classification_loss: 0.1124 (0.1125)  time: 0.1956  data: 0.0002  max mem: 5511
[10:29:56.435434] Epoch: [70]  [140/781]  eta: 0:02:08  lr: 0.000057  training_loss: 0.3059 (0.3149)  mae_loss: 0.1998 (0.2031)  classification_loss: 0.1051 (0.1118)  time: 0.1955  data: 0.0002  max mem: 5511
[10:30:00.345663] Epoch: [70]  [160/781]  eta: 0:02:04  lr: 0.000057  training_loss: 0.3257 (0.3165)  mae_loss: 0.2036 (0.2043)  classification_loss: 0.1135 (0.1122)  time: 0.1954  data: 0.0002  max mem: 5511
[10:30:04.259854] Epoch: [70]  [180/781]  eta: 0:01:59  lr: 0.000057  training_loss: 0.3230 (0.3179)  mae_loss: 0.2086 (0.2054)  classification_loss: 0.1103 (0.1124)  time: 0.1956  data: 0.0002  max mem: 5511
[10:30:08.195114] Epoch: [70]  [200/781]  eta: 0:01:55  lr: 0.000057  training_loss: 0.3251 (0.3185)  mae_loss: 0.2120 (0.2062)  classification_loss: 0.1111 (0.1123)  time: 0.1966  data: 0.0002  max mem: 5511
[10:30:12.161229] Epoch: [70]  [220/781]  eta: 0:01:51  lr: 0.000056  training_loss: 0.3173 (0.3182)  mae_loss: 0.1984 (0.2059)  classification_loss: 0.1120 (0.1123)  time: 0.1982  data: 0.0002  max mem: 5511
[10:30:16.077577] Epoch: [70]  [240/781]  eta: 0:01:47  lr: 0.000056  training_loss: 0.3158 (0.3187)  mae_loss: 0.2075 (0.2064)  classification_loss: 0.1091 (0.1123)  time: 0.1957  data: 0.0006  max mem: 5511
[10:30:19.981230] Epoch: [70]  [260/781]  eta: 0:01:43  lr: 0.000056  training_loss: 0.3339 (0.3191)  mae_loss: 0.2097 (0.2069)  classification_loss: 0.1086 (0.1123)  time: 0.1951  data: 0.0002  max mem: 5511
[10:30:23.896062] Epoch: [70]  [280/781]  eta: 0:01:39  lr: 0.000056  training_loss: 0.3075 (0.3188)  mae_loss: 0.2037 (0.2067)  classification_loss: 0.1079 (0.1121)  time: 0.1957  data: 0.0002  max mem: 5511
[10:30:27.797607] Epoch: [70]  [300/781]  eta: 0:01:35  lr: 0.000056  training_loss: 0.3201 (0.3189)  mae_loss: 0.2089 (0.2067)  classification_loss: 0.1125 (0.1121)  time: 0.1950  data: 0.0002  max mem: 5511
[10:30:31.740687] Epoch: [70]  [320/781]  eta: 0:01:31  lr: 0.000056  training_loss: 0.3261 (0.3191)  mae_loss: 0.2107 (0.2068)  classification_loss: 0.1173 (0.1124)  time: 0.1971  data: 0.0003  max mem: 5511
[10:30:35.668519] Epoch: [70]  [340/781]  eta: 0:01:27  lr: 0.000056  training_loss: 0.3166 (0.3191)  mae_loss: 0.2060 (0.2068)  classification_loss: 0.1090 (0.1123)  time: 0.1963  data: 0.0002  max mem: 5511
[10:30:39.581040] Epoch: [70]  [360/781]  eta: 0:01:23  lr: 0.000056  training_loss: 0.3168 (0.3188)  mae_loss: 0.2005 (0.2063)  classification_loss: 0.1158 (0.1125)  time: 0.1955  data: 0.0002  max mem: 5511
[10:30:43.509588] Epoch: [70]  [380/781]  eta: 0:01:19  lr: 0.000056  training_loss: 0.3240 (0.3195)  mae_loss: 0.2072 (0.2068)  classification_loss: 0.1165 (0.1127)  time: 0.1963  data: 0.0002  max mem: 5511
[10:30:47.422413] Epoch: [70]  [400/781]  eta: 0:01:15  lr: 0.000056  training_loss: 0.3172 (0.3196)  mae_loss: 0.2052 (0.2070)  classification_loss: 0.1069 (0.1126)  time: 0.1956  data: 0.0002  max mem: 5511
[10:30:51.320719] Epoch: [70]  [420/781]  eta: 0:01:11  lr: 0.000056  training_loss: 0.3290 (0.3203)  mae_loss: 0.2169 (0.2077)  classification_loss: 0.1137 (0.1126)  time: 0.1948  data: 0.0002  max mem: 5511
[10:30:55.271651] Epoch: [70]  [440/781]  eta: 0:01:07  lr: 0.000055  training_loss: 0.3227 (0.3204)  mae_loss: 0.2036 (0.2077)  classification_loss: 0.1136 (0.1127)  time: 0.1975  data: 0.0002  max mem: 5511
[10:30:59.186463] Epoch: [70]  [460/781]  eta: 0:01:03  lr: 0.000055  training_loss: 0.3035 (0.3201)  mae_loss: 0.1954 (0.2075)  classification_loss: 0.1099 (0.1126)  time: 0.1957  data: 0.0002  max mem: 5511
[10:31:03.094990] Epoch: [70]  [480/781]  eta: 0:00:59  lr: 0.000055  training_loss: 0.3232 (0.3203)  mae_loss: 0.2036 (0.2076)  classification_loss: 0.1156 (0.1127)  time: 0.1953  data: 0.0002  max mem: 5511
[10:31:07.026314] Epoch: [70]  [500/781]  eta: 0:00:55  lr: 0.000055  training_loss: 0.3233 (0.3206)  mae_loss: 0.2030 (0.2078)  classification_loss: 0.1135 (0.1128)  time: 0.1965  data: 0.0004  max mem: 5511
[10:31:10.914027] Epoch: [70]  [520/781]  eta: 0:00:51  lr: 0.000055  training_loss: 0.3067 (0.3205)  mae_loss: 0.2076 (0.2077)  classification_loss: 0.1117 (0.1128)  time: 0.1943  data: 0.0002  max mem: 5511
[10:31:14.820725] Epoch: [70]  [540/781]  eta: 0:00:47  lr: 0.000055  training_loss: 0.3151 (0.3203)  mae_loss: 0.2016 (0.2075)  classification_loss: 0.1078 (0.1128)  time: 0.1952  data: 0.0003  max mem: 5511
[10:31:18.721535] Epoch: [70]  [560/781]  eta: 0:00:43  lr: 0.000055  training_loss: 0.3161 (0.3204)  mae_loss: 0.2044 (0.2077)  classification_loss: 0.1081 (0.1127)  time: 0.1949  data: 0.0002  max mem: 5511
[10:31:22.647769] Epoch: [70]  [580/781]  eta: 0:00:39  lr: 0.000055  training_loss: 0.3267 (0.3206)  mae_loss: 0.2125 (0.2080)  classification_loss: 0.1073 (0.1126)  time: 0.1962  data: 0.0002  max mem: 5511
[10:31:26.625080] Epoch: [70]  [600/781]  eta: 0:00:35  lr: 0.000055  training_loss: 0.3435 (0.3210)  mae_loss: 0.2310 (0.2084)  classification_loss: 0.1103 (0.1126)  time: 0.1987  data: 0.0002  max mem: 5511
[10:31:30.548979] Epoch: [70]  [620/781]  eta: 0:00:31  lr: 0.000055  training_loss: 0.3366 (0.3212)  mae_loss: 0.2183 (0.2086)  classification_loss: 0.1082 (0.1126)  time: 0.1960  data: 0.0002  max mem: 5511
[10:31:34.466243] Epoch: [70]  [640/781]  eta: 0:00:27  lr: 0.000055  training_loss: 0.3030 (0.3210)  mae_loss: 0.1983 (0.2083)  classification_loss: 0.1138 (0.1126)  time: 0.1958  data: 0.0003  max mem: 5511
[10:31:38.373558] Epoch: [70]  [660/781]  eta: 0:00:23  lr: 0.000055  training_loss: 0.3114 (0.3207)  mae_loss: 0.2007 (0.2081)  classification_loss: 0.1102 (0.1126)  time: 0.1953  data: 0.0002  max mem: 5511
[10:31:42.311785] Epoch: [70]  [680/781]  eta: 0:00:19  lr: 0.000054  training_loss: 0.3287 (0.3209)  mae_loss: 0.2178 (0.2084)  classification_loss: 0.1091 (0.1125)  time: 0.1968  data: 0.0003  max mem: 5511
[10:31:46.229268] Epoch: [70]  [700/781]  eta: 0:00:15  lr: 0.000054  training_loss: 0.3153 (0.3208)  mae_loss: 0.1963 (0.2082)  classification_loss: 0.1167 (0.1126)  time: 0.1958  data: 0.0003  max mem: 5511
[10:31:50.141249] Epoch: [70]  [720/781]  eta: 0:00:12  lr: 0.000054  training_loss: 0.3212 (0.3210)  mae_loss: 0.2090 (0.2083)  classification_loss: 0.1138 (0.1127)  time: 0.1955  data: 0.0002  max mem: 5511
[10:31:54.109969] Epoch: [70]  [740/781]  eta: 0:00:08  lr: 0.000054  training_loss: 0.3283 (0.3213)  mae_loss: 0.2130 (0.2086)  classification_loss: 0.1089 (0.1127)  time: 0.1983  data: 0.0002  max mem: 5511
[10:31:58.079899] Epoch: [70]  [760/781]  eta: 0:00:04  lr: 0.000054  training_loss: 0.3387 (0.3217)  mae_loss: 0.2166 (0.2089)  classification_loss: 0.1157 (0.1128)  time: 0.1984  data: 0.0002  max mem: 5511
[10:32:01.972628] Epoch: [70]  [780/781]  eta: 0:00:00  lr: 0.000054  training_loss: 0.3155 (0.3217)  mae_loss: 0.2077 (0.2089)  classification_loss: 0.1101 (0.1128)  time: 0.1945  data: 0.0002  max mem: 5511
[10:32:02.099076] Epoch: [70] Total time: 0:02:33 (0.1972 s / it)
[10:32:02.099908] Averaged stats: lr: 0.000054  training_loss: 0.3155 (0.3217)  mae_loss: 0.2077 (0.2089)  classification_loss: 0.1101 (0.1128)
[10:32:04.345135] Test:  [  0/157]  eta: 0:01:46  testing_loss: 0.5316 (0.5316)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.6797  data: 0.6479  max mem: 5511
[10:32:04.640965] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4278 (0.4276)  acc1: 85.9375 (87.3580)  acc5: 100.0000 (99.7159)  time: 0.0880  data: 0.0591  max mem: 5511
[10:32:04.923209] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.3934 (0.4122)  acc1: 87.5000 (87.8720)  acc5: 100.0000 (99.7024)  time: 0.0284  data: 0.0002  max mem: 5511
[10:32:05.206202] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.3804 (0.4228)  acc1: 89.0625 (87.7016)  acc5: 100.0000 (99.4960)  time: 0.0281  data: 0.0002  max mem: 5511
[10:32:05.491292] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4316 (0.4356)  acc1: 87.5000 (87.4238)  acc5: 100.0000 (99.3902)  time: 0.0283  data: 0.0002  max mem: 5511
[10:32:05.776976] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4243 (0.4289)  acc1: 87.5000 (87.6532)  acc5: 100.0000 (99.3873)  time: 0.0284  data: 0.0002  max mem: 5511
[10:32:06.060287] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.3691 (0.4273)  acc1: 89.0625 (87.9098)  acc5: 100.0000 (99.3852)  time: 0.0283  data: 0.0002  max mem: 5511
[10:32:06.346541] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.3705 (0.4207)  acc1: 89.0625 (88.0502)  acc5: 100.0000 (99.4278)  time: 0.0284  data: 0.0001  max mem: 5511
[10:32:06.628985] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.4050 (0.4284)  acc1: 87.5000 (87.7315)  acc5: 100.0000 (99.4020)  time: 0.0283  data: 0.0002  max mem: 5511
[10:32:06.917691] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4139 (0.4240)  acc1: 87.5000 (87.8262)  acc5: 100.0000 (99.4162)  time: 0.0284  data: 0.0002  max mem: 5511
[10:32:07.199952] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4206 (0.4282)  acc1: 85.9375 (87.5619)  acc5: 100.0000 (99.4431)  time: 0.0284  data: 0.0002  max mem: 5511
[10:32:07.485660] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4348 (0.4278)  acc1: 85.9375 (87.6267)  acc5: 100.0000 (99.4229)  time: 0.0283  data: 0.0002  max mem: 5511
[10:32:07.773069] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.3994 (0.4249)  acc1: 89.0625 (87.7324)  acc5: 100.0000 (99.4189)  time: 0.0285  data: 0.0002  max mem: 5511
[10:32:08.060169] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.3994 (0.4259)  acc1: 87.5000 (87.6551)  acc5: 100.0000 (99.4156)  time: 0.0286  data: 0.0002  max mem: 5511
[10:32:08.345289] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3920 (0.4230)  acc1: 87.5000 (87.7327)  acc5: 100.0000 (99.4459)  time: 0.0284  data: 0.0002  max mem: 5511
[10:32:08.623550] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.3920 (0.4222)  acc1: 87.5000 (87.6966)  acc5: 100.0000 (99.4723)  time: 0.0280  data: 0.0001  max mem: 5511
[10:32:08.774019] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.3977 (0.4225)  acc1: 85.9375 (87.6600)  acc5: 100.0000 (99.4600)  time: 0.0269  data: 0.0001  max mem: 5511
[10:32:08.919101] Test: Total time: 0:00:05 (0.0335 s / it)
[10:32:08.920289] * Acc@1 87.660 Acc@5 99.460 loss 0.423
[10:32:08.920576] Accuracy of the network on the 10000 test images: 87.7%
[10:32:08.920764] Max accuracy: 87.76%
[10:32:09.165293] log_dir: ./output_dir
[10:32:09.989940] Epoch: [71]  [  0/781]  eta: 0:10:42  lr: 0.000054  training_loss: 0.3344 (0.3344)  mae_loss: 0.2275 (0.2275)  classification_loss: 0.1069 (0.1069)  time: 0.8230  data: 0.5973  max mem: 5511

[10:32:13.921058] Epoch: [71]  [ 20/781]  eta: 0:02:52  lr: 0.000054  training_loss: 0.3171 (0.3225)  mae_loss: 0.2077 (0.2118)  classification_loss: 0.1084 (0.1107)  time: 0.1964  data: 0.0002  max mem: 5511
[10:32:17.845531] Epoch: [71]  [ 40/781]  eta: 0:02:36  lr: 0.000054  training_loss: 0.3225 (0.3245)  mae_loss: 0.2099 (0.2114)  classification_loss: 0.1126 (0.1130)  time: 0.1961  data: 0.0002  max mem: 5511
[10:32:21.811014] Epoch: [71]  [ 60/781]  eta: 0:02:29  lr: 0.000054  training_loss: 0.3305 (0.3257)  mae_loss: 0.2157 (0.2124)  classification_loss: 0.1134 (0.1133)  time: 0.1981  data: 0.0002  max mem: 5511
[10:32:25.716864] Epoch: [71]  [ 80/781]  eta: 0:02:23  lr: 0.000054  training_loss: 0.3130 (0.3240)  mae_loss: 0.2031 (0.2111)  classification_loss: 0.1099 (0.1129)  time: 0.1952  data: 0.0002  max mem: 5511
[10:32:29.609944] Epoch: [71]  [100/781]  eta: 0:02:17  lr: 0.000054  training_loss: 0.3111 (0.3228)  mae_loss: 0.2037 (0.2106)  classification_loss: 0.1096 (0.1122)  time: 0.1946  data: 0.0003  max mem: 5511
[10:32:33.525176] Epoch: [71]  [120/781]  eta: 0:02:12  lr: 0.000053  training_loss: 0.3163 (0.3213)  mae_loss: 0.1934 (0.2088)  classification_loss: 0.1137 (0.1125)  time: 0.1957  data: 0.0002  max mem: 5511
[10:32:37.427163] Epoch: [71]  [140/781]  eta: 0:02:08  lr: 0.000053  training_loss: 0.3185 (0.3213)  mae_loss: 0.2168 (0.2091)  classification_loss: 0.1087 (0.1122)  time: 0.1950  data: 0.0002  max mem: 5511
[10:32:41.349754] Epoch: [71]  [160/781]  eta: 0:02:04  lr: 0.000053  training_loss: 0.3088 (0.3205)  mae_loss: 0.2004 (0.2084)  classification_loss: 0.1090 (0.1121)  time: 0.1961  data: 0.0002  max mem: 5511
[10:32:45.248943] Epoch: [71]  [180/781]  eta: 0:01:59  lr: 0.000053  training_loss: 0.2997 (0.3191)  mae_loss: 0.1887 (0.2072)  classification_loss: 0.1091 (0.1119)  time: 0.1949  data: 0.0002  max mem: 5511
[10:32:49.195159] Epoch: [71]  [200/781]  eta: 0:01:55  lr: 0.000053  training_loss: 0.3142 (0.3188)  mae_loss: 0.2022 (0.2070)  classification_loss: 0.1092 (0.1117)  time: 0.1972  data: 0.0003  max mem: 5511
[10:32:53.093278] Epoch: [71]  [220/781]  eta: 0:01:51  lr: 0.000053  training_loss: 0.3140 (0.3195)  mae_loss: 0.2070 (0.2077)  classification_loss: 0.1079 (0.1118)  time: 0.1948  data: 0.0003  max mem: 5511
[10:32:56.987312] Epoch: [71]  [240/781]  eta: 0:01:47  lr: 0.000053  training_loss: 0.3263 (0.3204)  mae_loss: 0.2164 (0.2087)  classification_loss: 0.1098 (0.1117)  time: 0.1946  data: 0.0003  max mem: 5511
[10:33:00.874270] Epoch: [71]  [260/781]  eta: 0:01:43  lr: 0.000053  training_loss: 0.3059 (0.3197)  mae_loss: 0.1957 (0.2081)  classification_loss: 0.1107 (0.1116)  time: 0.1942  data: 0.0002  max mem: 5511
[10:33:04.771469] Epoch: [71]  [280/781]  eta: 0:01:39  lr: 0.000053  training_loss: 0.3176 (0.3200)  mae_loss: 0.2109 (0.2082)  classification_loss: 0.1121 (0.1118)  time: 0.1948  data: 0.0002  max mem: 5511
[10:33:08.661607] Epoch: [71]  [300/781]  eta: 0:01:35  lr: 0.000053  training_loss: 0.3294 (0.3205)  mae_loss: 0.2112 (0.2086)  classification_loss: 0.1178 (0.1119)  time: 0.1944  data: 0.0002  max mem: 5511
[10:33:12.570526] Epoch: [71]  [320/781]  eta: 0:01:31  lr: 0.000053  training_loss: 0.3066 (0.3198)  mae_loss: 0.1943 (0.2078)  classification_loss: 0.1111 (0.1120)  time: 0.1954  data: 0.0004  max mem: 5511
[10:33:16.538718] Epoch: [71]  [340/781]  eta: 0:01:27  lr: 0.000053  training_loss: 0.3106 (0.3194)  mae_loss: 0.2039 (0.2077)  classification_loss: 0.1059 (0.1117)  time: 0.1983  data: 0.0002  max mem: 5511
[10:33:20.458188] Epoch: [71]  [360/781]  eta: 0:01:23  lr: 0.000052  training_loss: 0.3216 (0.3193)  mae_loss: 0.1985 (0.2074)  classification_loss: 0.1145 (0.1119)  time: 0.1959  data: 0.0003  max mem: 5511
[10:33:24.377711] Epoch: [71]  [380/781]  eta: 0:01:19  lr: 0.000052  training_loss: 0.3093 (0.3191)  mae_loss: 0.2079 (0.2073)  classification_loss: 0.1107 (0.1118)  time: 0.1959  data: 0.0002  max mem: 5511
[10:33:28.275315] Epoch: [71]  [400/781]  eta: 0:01:15  lr: 0.000052  training_loss: 0.3084 (0.3192)  mae_loss: 0.1979 (0.2073)  classification_loss: 0.1109 (0.1118)  time: 0.1948  data: 0.0003  max mem: 5511
[10:33:32.183185] Epoch: [71]  [420/781]  eta: 0:01:11  lr: 0.000052  training_loss: 0.3236 (0.3193)  mae_loss: 0.2131 (0.2076)  classification_loss: 0.1104 (0.1118)  time: 0.1953  data: 0.0002  max mem: 5511
[10:33:36.077441] Epoch: [71]  [440/781]  eta: 0:01:07  lr: 0.000052  training_loss: 0.3305 (0.3198)  mae_loss: 0.2115 (0.2079)  classification_loss: 0.1141 (0.1119)  time: 0.1946  data: 0.0002  max mem: 5511

[10:33:40.011178] Epoch: [71]  [460/781]  eta: 0:01:03  lr: 0.000052  training_loss: 0.3260 (0.3199)  mae_loss: 0.2133 (0.2080)  classification_loss: 0.1093 (0.1118)  time: 0.1966  data: 0.0002  max mem: 5511
[10:33:43.902559] Epoch: [71]  [480/781]  eta: 0:00:59  lr: 0.000052  training_loss: 0.3236 (0.3199)  mae_loss: 0.2055 (0.2080)  classification_loss: 0.1120 (0.1119)  time: 0.1945  data: 0.0002  max mem: 5511
[10:33:47.853747] Epoch: [71]  [500/781]  eta: 0:00:55  lr: 0.000052  training_loss: 0.3238 (0.3202)  mae_loss: 0.2079 (0.2081)  classification_loss: 0.1150 (0.1120)  time: 0.1974  data: 0.0002  max mem: 5511
[10:33:51.790062] Epoch: [71]  [520/781]  eta: 0:00:51  lr: 0.000052  training_loss: 0.3339 (0.3207)  mae_loss: 0.2141 (0.2085)  classification_loss: 0.1166 (0.1122)  time: 0.1967  data: 0.0002  max mem: 5511
[10:33:55.708606] Epoch: [71]  [540/781]  eta: 0:00:47  lr: 0.000052  training_loss: 0.3213 (0.3208)  mae_loss: 0.2066 (0.2086)  classification_loss: 0.1119 (0.1122)  time: 0.1958  data: 0.0004  max mem: 5511
[10:33:59.650405] Epoch: [71]  [560/781]  eta: 0:00:43  lr: 0.000052  training_loss: 0.3208 (0.3209)  mae_loss: 0.2060 (0.2088)  classification_loss: 0.1094 (0.1122)  time: 0.1970  data: 0.0002  max mem: 5511
[10:34:03.546056] Epoch: [71]  [580/781]  eta: 0:00:39  lr: 0.000052  training_loss: 0.3303 (0.3212)  mae_loss: 0.2175 (0.2090)  classification_loss: 0.1097 (0.1121)  time: 0.1947  data: 0.0002  max mem: 5511
[10:34:07.460878] Epoch: [71]  [600/781]  eta: 0:00:35  lr: 0.000051  training_loss: 0.3124 (0.3210)  mae_loss: 0.2017 (0.2089)  classification_loss: 0.1119 (0.1121)  time: 0.1957  data: 0.0002  max mem: 5511
[10:34:11.358043] Epoch: [71]  [620/781]  eta: 0:00:31  lr: 0.000051  training_loss: 0.3131 (0.3210)  mae_loss: 0.2045 (0.2089)  classification_loss: 0.1148 (0.1121)  time: 0.1948  data: 0.0002  max mem: 5511
[10:34:15.312782] Epoch: [71]  [640/781]  eta: 0:00:27  lr: 0.000051  training_loss: 0.3082 (0.3209)  mae_loss: 0.2027 (0.2087)  classification_loss: 0.1096 (0.1121)  time: 0.1976  data: 0.0002  max mem: 5511
[10:34:19.297959] Epoch: [71]  [660/781]  eta: 0:00:23  lr: 0.000051  training_loss: 0.3144 (0.3207)  mae_loss: 0.2036 (0.2086)  classification_loss: 0.1106 (0.1121)  time: 0.1992  data: 0.0002  max mem: 5511
[10:34:23.194910] Epoch: [71]  [680/781]  eta: 0:00:19  lr: 0.000051  training_loss: 0.3171 (0.3207)  mae_loss: 0.2108 (0.2086)  classification_loss: 0.1095 (0.1121)  time: 0.1948  data: 0.0002  max mem: 5511
[10:34:27.104744] Epoch: [71]  [700/781]  eta: 0:00:15  lr: 0.000051  training_loss: 0.3123 (0.3207)  mae_loss: 0.1992 (0.2085)  classification_loss: 0.1113 (0.1121)  time: 0.1954  data: 0.0005  max mem: 5511
[10:34:31.017756] Epoch: [71]  [720/781]  eta: 0:00:11  lr: 0.000051  training_loss: 0.3137 (0.3206)  mae_loss: 0.2040 (0.2084)  classification_loss: 0.1137 (0.1122)  time: 0.1956  data: 0.0002  max mem: 5511
[10:34:34.902991] Epoch: [71]  [740/781]  eta: 0:00:08  lr: 0.000051  training_loss: 0.3090 (0.3205)  mae_loss: 0.1977 (0.2083)  classification_loss: 0.1108 (0.1121)  time: 0.1942  data: 0.0003  max mem: 5511
[10:34:38.821566] Epoch: [71]  [760/781]  eta: 0:00:04  lr: 0.000051  training_loss: 0.3235 (0.3207)  mae_loss: 0.2129 (0.2084)  classification_loss: 0.1179 (0.1122)  time: 0.1958  data: 0.0005  max mem: 5511
[10:34:42.724414] Epoch: [71]  [780/781]  eta: 0:00:00  lr: 0.000051  training_loss: 0.3148 (0.3205)  mae_loss: 0.2017 (0.2083)  classification_loss: 0.1145 (0.1123)  time: 0.1951  data: 0.0002  max mem: 5511
[10:34:42.872565] Epoch: [71] Total time: 0:02:33 (0.1968 s / it)
[10:34:42.873020] Averaged stats: lr: 0.000051  training_loss: 0.3148 (0.3205)  mae_loss: 0.2017 (0.2083)  classification_loss: 0.1145 (0.1123)
[10:34:43.591966] Test:  [  0/157]  eta: 0:01:52  testing_loss: 0.6040 (0.6040)  acc1: 81.2500 (81.2500)  acc5: 98.4375 (98.4375)  time: 0.7140  data: 0.6692  max mem: 5511
[10:34:43.876169] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.4580 (0.4597)  acc1: 85.9375 (85.7955)  acc5: 100.0000 (99.5739)  time: 0.0905  data: 0.0610  max mem: 5511
[10:34:44.158444] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.4411 (0.4306)  acc1: 85.9375 (87.2768)  acc5: 100.0000 (99.6280)  time: 0.0281  data: 0.0002  max mem: 5511
[10:34:44.444574] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.4077 (0.4338)  acc1: 87.5000 (87.1976)  acc5: 100.0000 (99.4456)  time: 0.0283  data: 0.0002  max mem: 5511
[10:34:44.727326] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4368 (0.4426)  acc1: 85.9375 (86.6997)  acc5: 100.0000 (99.3521)  time: 0.0283  data: 0.0002  max mem: 5511
[10:34:45.012532] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4246 (0.4302)  acc1: 87.5000 (87.1630)  acc5: 100.0000 (99.3873)  time: 0.0282  data: 0.0002  max mem: 5511
[10:34:45.297109] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.3860 (0.4289)  acc1: 87.5000 (87.3207)  acc5: 100.0000 (99.3084)  time: 0.0283  data: 0.0002  max mem: 5511
[10:34:45.585267] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.3735 (0.4178)  acc1: 89.0625 (87.7421)  acc5: 100.0000 (99.3838)  time: 0.0285  data: 0.0003  max mem: 5511
[10:34:45.866576] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.3818 (0.4252)  acc1: 89.0625 (87.5000)  acc5: 100.0000 (99.3634)  time: 0.0284  data: 0.0003  max mem: 5511
[10:34:46.147847] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4283 (0.4221)  acc1: 87.5000 (87.6889)  acc5: 100.0000 (99.4162)  time: 0.0280  data: 0.0002  max mem: 5511
[10:34:46.430176] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4627 (0.4271)  acc1: 87.5000 (87.4381)  acc5: 100.0000 (99.4431)  time: 0.0281  data: 0.0002  max mem: 5511
[10:34:46.712744] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4627 (0.4266)  acc1: 87.5000 (87.5141)  acc5: 100.0000 (99.4651)  time: 0.0281  data: 0.0002  max mem: 5511
[10:34:46.993222] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.3983 (0.4229)  acc1: 89.0625 (87.6550)  acc5: 100.0000 (99.4576)  time: 0.0280  data: 0.0002  max mem: 5511
[10:34:47.274307] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.3972 (0.4245)  acc1: 87.5000 (87.5239)  acc5: 100.0000 (99.4633)  time: 0.0280  data: 0.0002  max mem: 5511
[10:34:47.555116] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3811 (0.4214)  acc1: 87.5000 (87.5887)  acc5: 100.0000 (99.4902)  time: 0.0280  data: 0.0001  max mem: 5511
[10:34:47.834352] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4064 (0.4221)  acc1: 87.5000 (87.5103)  acc5: 100.0000 (99.4723)  time: 0.0279  data: 0.0001  max mem: 5511
[10:34:47.984307] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4207 (0.4226)  acc1: 85.9375 (87.5100)  acc5: 100.0000 (99.4600)  time: 0.0269  data: 0.0001  max mem: 5511
[10:34:48.123286] Test: Total time: 0:00:05 (0.0334 s / it)
[10:34:48.123964] * Acc@1 87.510 Acc@5 99.460 loss 0.423
[10:34:48.124326] Accuracy of the network on the 10000 test images: 87.5%
[10:34:48.124518] Max accuracy: 87.76%
[10:34:48.399129] log_dir: ./output_dir
[10:34:49.318106] Epoch: [72]  [  0/781]  eta: 0:11:55  lr: 0.000051  training_loss: 0.3491 (0.3491)  mae_loss: 0.2530 (0.2530)  classification_loss: 0.0961 (0.0961)  time: 0.9165  data: 0.6822  max mem: 5511
[10:34:53.224876] Epoch: [72]  [ 20/781]  eta: 0:02:54  lr: 0.000051  training_loss: 0.3315 (0.3263)  mae_loss: 0.2211 (0.2199)  classification_loss: 0.1070 (0.1064)  time: 0.1952  data: 0.0002  max mem: 5511
[10:34:57.175826] Epoch: [72]  [ 40/781]  eta: 0:02:38  lr: 0.000050  training_loss: 0.3294 (0.3226)  mae_loss: 0.2128 (0.2131)  classification_loss: 0.1117 (0.1095)  time: 0.1975  data: 0.0003  max mem: 5511
[10:35:01.076185] Epoch: [72]  [ 60/781]  eta: 0:02:29  lr: 0.000050  training_loss: 0.3097 (0.3198)  mae_loss: 0.2026 (0.2090)  classification_loss: 0.1106 (0.1108)  time: 0.1949  data: 0.0002  max mem: 5511
[10:35:04.998171] Epoch: [72]  [ 80/781]  eta: 0:02:23  lr: 0.000050  training_loss: 0.3136 (0.3205)  mae_loss: 0.1956 (0.2087)  classification_loss: 0.1139 (0.1118)  time: 0.1960  data: 0.0002  max mem: 5511
[10:35:08.926157] Epoch: [72]  [100/781]  eta: 0:02:18  lr: 0.000050  training_loss: 0.3198 (0.3202)  mae_loss: 0.2062 (0.2079)  classification_loss: 0.1117 (0.1123)  time: 0.1963  data: 0.0004  max mem: 5511
[10:35:12.837560] Epoch: [72]  [120/781]  eta: 0:02:13  lr: 0.000050  training_loss: 0.2993 (0.3179)  mae_loss: 0.1878 (0.2056)  classification_loss: 0.1112 (0.1124)  time: 0.1955  data: 0.0003  max mem: 5511
[10:35:16.738969] Epoch: [72]  [140/781]  eta: 0:02:08  lr: 0.000050  training_loss: 0.3178 (0.3186)  mae_loss: 0.2051 (0.2060)  classification_loss: 0.1155 (0.1126)  time: 0.1950  data: 0.0002  max mem: 5511
[10:35:20.653028] Epoch: [72]  [160/781]  eta: 0:02:04  lr: 0.000050  training_loss: 0.3076 (0.3175)  mae_loss: 0.1986 (0.2053)  classification_loss: 0.1092 (0.1122)  time: 0.1956  data: 0.0002  max mem: 5511
[10:35:24.559124] Epoch: [72]  [180/781]  eta: 0:02:00  lr: 0.000050  training_loss: 0.3090 (0.3167)  mae_loss: 0.1968 (0.2046)  classification_loss: 0.1136 (0.1121)  time: 0.1952  data: 0.0002  max mem: 5511
[10:35:28.463522] Epoch: [72]  [200/781]  eta: 0:01:55  lr: 0.000050  training_loss: 0.3206 (0.3178)  mae_loss: 0.2032 (0.2055)  classification_loss: 0.1138 (0.1123)  time: 0.1951  data: 0.0004  max mem: 5511
[10:35:32.409170] Epoch: [72]  [220/781]  eta: 0:01:51  lr: 0.000050  training_loss: 0.3172 (0.3176)  mae_loss: 0.2024 (0.2051)  classification_loss: 0.1164 (0.1125)  time: 0.1972  data: 0.0003  max mem: 5511
[10:35:36.356253] Epoch: [72]  [240/781]  eta: 0:01:47  lr: 0.000050  training_loss: 0.3092 (0.3177)  mae_loss: 0.2038 (0.2057)  classification_loss: 0.1062 (0.1121)  time: 0.1973  data: 0.0003  max mem: 5511
[10:35:40.282411] Epoch: [72]  [260/781]  eta: 0:01:43  lr: 0.000050  training_loss: 0.3243 (0.3180)  mae_loss: 0.2070 (0.2059)  classification_loss: 0.1144 (0.1121)  time: 0.1962  data: 0.0002  max mem: 5511
[10:35:44.187431] Epoch: [72]  [280/781]  eta: 0:01:39  lr: 0.000049  training_loss: 0.3211 (0.3182)  mae_loss: 0.2103 (0.2062)  classification_loss: 0.1121 (0.1120)  time: 0.1951  data: 0.0002  max mem: 5511
[10:35:48.091482] Epoch: [72]  [300/781]  eta: 0:01:35  lr: 0.000049  training_loss: 0.3122 (0.3185)  mae_loss: 0.2044 (0.2065)  classification_loss: 0.1114 (0.1121)  time: 0.1950  data: 0.0002  max mem: 5511
[10:35:52.007013] Epoch: [72]  [320/781]  eta: 0:01:31  lr: 0.000049  training_loss: 0.3168 (0.3184)  mae_loss: 0.1990 (0.2062)  classification_loss: 0.1122 (0.1122)  time: 0.1957  data: 0.0002  max mem: 5511
[10:35:55.938344] Epoch: [72]  [340/781]  eta: 0:01:27  lr: 0.000049  training_loss: 0.3147 (0.3182)  mae_loss: 0.2093 (0.2063)  classification_loss: 0.1061 (0.1119)  time: 0.1965  data: 0.0002  max mem: 5511
[10:35:59.852208] Epoch: [72]  [360/781]  eta: 0:01:23  lr: 0.000049  training_loss: 0.3088 (0.3176)  mae_loss: 0.1925 (0.2056)  classification_loss: 0.1121 (0.1120)  time: 0.1956  data: 0.0002  max mem: 5511
[10:36:03.754055] Epoch: [72]  [380/781]  eta: 0:01:19  lr: 0.000049  training_loss: 0.3184 (0.3179)  mae_loss: 0.2093 (0.2058)  classification_loss: 0.1134 (0.1121)  time: 0.1950  data: 0.0002  max mem: 5511
[10:36:07.663003] Epoch: [72]  [400/781]  eta: 0:01:15  lr: 0.000049  training_loss: 0.3133 (0.3177)  mae_loss: 0.1991 (0.2057)  classification_loss: 0.1068 (0.1120)  time: 0.1954  data: 0.0002  max mem: 5511
[10:36:11.553447] Epoch: [72]  [420/781]  eta: 0:01:11  lr: 0.000049  training_loss: 0.3307 (0.3182)  mae_loss: 0.2180 (0.2061)  classification_loss: 0.1119 (0.1121)  time: 0.1944  data: 0.0002  max mem: 5511
[10:36:15.462163] Epoch: [72]  [440/781]  eta: 0:01:07  lr: 0.000049  training_loss: 0.3013 (0.3178)  mae_loss: 0.1998 (0.2059)  classification_loss: 0.1089 (0.1119)  time: 0.1954  data: 0.0002  max mem: 5511
[10:36:19.369655] Epoch: [72]  [460/781]  eta: 0:01:03  lr: 0.000049  training_loss: 0.3189 (0.3179)  mae_loss: 0.2116 (0.2062)  classification_loss: 0.1060 (0.1117)  time: 0.1953  data: 0.0002  max mem: 5511
[10:36:23.271250] Epoch: [72]  [480/781]  eta: 0:00:59  lr: 0.000049  training_loss: 0.3216 (0.3181)  mae_loss: 0.2123 (0.2063)  classification_loss: 0.1122 (0.1118)  time: 0.1950  data: 0.0003  max mem: 5511
[10:36:27.171194] Epoch: [72]  [500/781]  eta: 0:00:55  lr: 0.000049  training_loss: 0.3106 (0.3179)  mae_loss: 0.2020 (0.2062)  classification_loss: 0.1068 (0.1117)  time: 0.1949  data: 0.0003  max mem: 5511
[10:36:31.062927] Epoch: [72]  [520/781]  eta: 0:00:51  lr: 0.000048  training_loss: 0.3242 (0.3182)  mae_loss: 0.2080 (0.2064)  classification_loss: 0.1093 (0.1118)  time: 0.1945  data: 0.0002  max mem: 5511
[10:36:34.955024] Epoch: [72]  [540/781]  eta: 0:00:47  lr: 0.000048  training_loss: 0.3201 (0.3182)  mae_loss: 0.2081 (0.2065)  classification_loss: 0.1122 (0.1117)  time: 0.1945  data: 0.0002  max mem: 5511
[10:36:38.866449] Epoch: [72]  [560/781]  eta: 0:00:43  lr: 0.000048  training_loss: 0.3107 (0.3182)  mae_loss: 0.2020 (0.2066)  classification_loss: 0.1128 (0.1116)  time: 0.1955  data: 0.0002  max mem: 5511
[10:36:42.769601] Epoch: [72]  [580/781]  eta: 0:00:39  lr: 0.000048  training_loss: 0.3160 (0.3181)  mae_loss: 0.1971 (0.2064)  classification_loss: 0.1130 (0.1117)  time: 0.1950  data: 0.0002  max mem: 5511
[10:36:46.679477] Epoch: [72]  [600/781]  eta: 0:00:35  lr: 0.000048  training_loss: 0.3266 (0.3181)  mae_loss: 0.2078 (0.2063)  classification_loss: 0.1137 (0.1118)  time: 0.1954  data: 0.0002  max mem: 5511
[10:36:50.569209] Epoch: [72]  [620/781]  eta: 0:00:31  lr: 0.000048  training_loss: 0.3047 (0.3179)  mae_loss: 0.2048 (0.2062)  classification_loss: 0.1059 (0.1117)  time: 0.1944  data: 0.0003  max mem: 5511
[10:36:54.458622] Epoch: [72]  [640/781]  eta: 0:00:27  lr: 0.000048  training_loss: 0.3234 (0.3179)  mae_loss: 0.2064 (0.2063)  classification_loss: 0.1071 (0.1116)  time: 0.1944  data: 0.0003  max mem: 5511
[10:36:58.350572] Epoch: [72]  [660/781]  eta: 0:00:23  lr: 0.000048  training_loss: 0.3028 (0.3177)  mae_loss: 0.1904 (0.2059)  classification_loss: 0.1168 (0.1118)  time: 0.1945  data: 0.0003  max mem: 5511
[10:37:02.282723] Epoch: [72]  [680/781]  eta: 0:00:19  lr: 0.000048  training_loss: 0.3113 (0.3174)  mae_loss: 0.1935 (0.2056)  classification_loss: 0.1109 (0.1118)  time: 0.1965  data: 0.0002  max mem: 5511
[10:37:06.158415] Epoch: [72]  [700/781]  eta: 0:00:15  lr: 0.000048  training_loss: 0.3163 (0.3173)  mae_loss: 0.2015 (0.2055)  classification_loss: 0.1111 (0.1118)  time: 0.1937  data: 0.0003  max mem: 5511
[10:37:10.063195] Epoch: [72]  [720/781]  eta: 0:00:11  lr: 0.000048  training_loss: 0.3227 (0.3175)  mae_loss: 0.2111 (0.2057)  classification_loss: 0.1094 (0.1118)  time: 0.1952  data: 0.0002  max mem: 5511
[10:37:13.970872] Epoch: [72]  [740/781]  eta: 0:00:08  lr: 0.000048  training_loss: 0.3103 (0.3175)  mae_loss: 0.2034 (0.2057)  classification_loss: 0.1075 (0.1117)  time: 0.1953  data: 0.0002  max mem: 5511
[10:37:17.914733] Epoch: [72]  [760/781]  eta: 0:00:04  lr: 0.000048  training_loss: 0.3146 (0.3174)  mae_loss: 0.2012 (0.2057)  classification_loss: 0.1090 (0.1117)  time: 0.1971  data: 0.0002  max mem: 5511
[10:37:21.798392] Epoch: [72]  [780/781]  eta: 0:00:00  lr: 0.000047  training_loss: 0.3189 (0.3178)  mae_loss: 0.2089 (0.2061)  classification_loss: 0.1116 (0.1117)  time: 0.1941  data: 0.0002  max mem: 5511
[10:37:21.952814] Epoch: [72] Total time: 0:02:33 (0.1966 s / it)
[10:37:21.953387] Averaged stats: lr: 0.000047  training_loss: 0.3189 (0.3178)  mae_loss: 0.2089 (0.2061)  classification_loss: 0.1116 (0.1117)
[10:37:22.514487] Test:  [  0/157]  eta: 0:01:27  testing_loss: 0.5164 (0.5164)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (100.0000)  time: 0.5570  data: 0.5259  max mem: 5511
[10:37:22.806426] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.4125 (0.4231)  acc1: 87.5000 (87.3580)  acc5: 100.0000 (99.8580)  time: 0.0770  data: 0.0482  max mem: 5511
[10:37:23.092818] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4122 (0.4120)  acc1: 87.5000 (87.6488)  acc5: 100.0000 (99.7768)  time: 0.0288  data: 0.0003  max mem: 5511
[10:37:23.375808] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.3936 (0.4183)  acc1: 87.5000 (87.5504)  acc5: 100.0000 (99.4456)  time: 0.0283  data: 0.0002  max mem: 5511
[10:37:23.658440] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.4223 (0.4266)  acc1: 87.5000 (87.3095)  acc5: 100.0000 (99.3521)  time: 0.0281  data: 0.0002  max mem: 5511
[10:37:23.940800] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.3874 (0.4166)  acc1: 89.0625 (87.6838)  acc5: 100.0000 (99.3873)  time: 0.0281  data: 0.0002  max mem: 5511
[10:37:24.222765] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.3462 (0.4129)  acc1: 89.0625 (87.8842)  acc5: 100.0000 (99.4109)  time: 0.0281  data: 0.0002  max mem: 5511
[10:37:24.505700] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.3480 (0.4044)  acc1: 89.0625 (88.0942)  acc5: 100.0000 (99.4718)  time: 0.0281  data: 0.0002  max mem: 5511
[10:37:24.792289] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.3873 (0.4122)  acc1: 87.5000 (87.8472)  acc5: 100.0000 (99.4599)  time: 0.0284  data: 0.0002  max mem: 5511
[10:37:25.080525] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4347 (0.4098)  acc1: 87.5000 (87.9979)  acc5: 100.0000 (99.4677)  time: 0.0286  data: 0.0002  max mem: 5511
[10:37:25.364150] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4347 (0.4151)  acc1: 87.5000 (87.9486)  acc5: 100.0000 (99.4740)  time: 0.0285  data: 0.0002  max mem: 5511
[10:37:25.650117] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4206 (0.4140)  acc1: 87.5000 (87.9786)  acc5: 100.0000 (99.4651)  time: 0.0284  data: 0.0002  max mem: 5511
[10:37:25.936261] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.3564 (0.4113)  acc1: 87.5000 (88.0682)  acc5: 100.0000 (99.4835)  time: 0.0285  data: 0.0002  max mem: 5511
[10:37:26.224013] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.3918 (0.4120)  acc1: 89.0625 (88.0248)  acc5: 100.0000 (99.4871)  time: 0.0286  data: 0.0002  max mem: 5511
[10:37:26.507259] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3636 (0.4094)  acc1: 89.0625 (88.1316)  acc5: 100.0000 (99.4902)  time: 0.0284  data: 0.0003  max mem: 5511
[10:37:26.785827] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.3636 (0.4090)  acc1: 89.0625 (88.1105)  acc5: 100.0000 (99.5137)  time: 0.0280  data: 0.0002  max mem: 5511
[10:37:26.935715] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.3657 (0.4085)  acc1: 89.0625 (88.1200)  acc5: 100.0000 (99.5200)  time: 0.0269  data: 0.0001  max mem: 5511
[10:37:27.096973] Test: Total time: 0:00:05 (0.0327 s / it)
[10:37:27.097432] * Acc@1 88.120 Acc@5 99.520 loss 0.409
[10:37:27.097856] Accuracy of the network on the 10000 test images: 88.1%
[10:37:27.098151] Max accuracy: 88.12%
[10:37:27.366118] log_dir: ./output_dir
[10:37:28.131216] Epoch: [73]  [  0/781]  eta: 0:09:56  lr: 0.000047  training_loss: 0.3603 (0.3603)  mae_loss: 0.2496 (0.2496)  classification_loss: 0.1107 (0.1107)  time: 0.7633  data: 0.5217  max mem: 5511
[10:37:32.044327] Epoch: [73]  [ 20/781]  eta: 0:02:49  lr: 0.000047  training_loss: 0.3189 (0.3240)  mae_loss: 0.2098 (0.2155)  classification_loss: 0.1098 (0.1085)  time: 0.1956  data: 0.0004  max mem: 5511
[10:37:35.960817] Epoch: [73]  [ 40/781]  eta: 0:02:35  lr: 0.000047  training_loss: 0.3113 (0.3240)  mae_loss: 0.2069 (0.2135)  classification_loss: 0.1110 (0.1105)  time: 0.1957  data: 0.0003  max mem: 5511

[10:37:39.866973] Epoch: [73]  [ 60/781]  eta: 0:02:27  lr: 0.000047  training_loss: 0.3126 (0.3212)  mae_loss: 0.2033 (0.2109)  classification_loss: 0.1101 (0.1103)  time: 0.1952  data: 0.0002  max mem: 5511
[10:37:43.770759] Epoch: [73]  [ 80/781]  eta: 0:02:21  lr: 0.000047  training_loss: 0.3008 (0.3166)  mae_loss: 0.1934 (0.2063)  classification_loss: 0.1086 (0.1102)  time: 0.1951  data: 0.0003  max mem: 5511
[10:37:47.671562] Epoch: [73]  [100/781]  eta: 0:02:16  lr: 0.000047  training_loss: 0.3079 (0.3148)  mae_loss: 0.2003 (0.2046)  classification_loss: 0.1083 (0.1102)  time: 0.1950  data: 0.0002  max mem: 5511
[10:37:51.587737] Epoch: [73]  [120/781]  eta: 0:02:12  lr: 0.000047  training_loss: 0.3073 (0.3136)  mae_loss: 0.1990 (0.2040)  classification_loss: 0.1087 (0.1097)  time: 0.1957  data: 0.0003  max mem: 5511
[10:37:55.476800] Epoch: [73]  [140/781]  eta: 0:02:07  lr: 0.000047  training_loss: 0.3084 (0.3134)  mae_loss: 0.1934 (0.2034)  classification_loss: 0.1111 (0.1101)  time: 0.1943  data: 0.0002  max mem: 5511
[10:37:59.383069] Epoch: [73]  [160/781]  eta: 0:02:03  lr: 0.000047  training_loss: 0.3252 (0.3145)  mae_loss: 0.2070 (0.2043)  classification_loss: 0.1077 (0.1102)  time: 0.1952  data: 0.0003  max mem: 5511
[10:38:03.288290] Epoch: [73]  [180/781]  eta: 0:01:59  lr: 0.000047  training_loss: 0.3177 (0.3154)  mae_loss: 0.1963 (0.2047)  classification_loss: 0.1112 (0.1108)  time: 0.1951  data: 0.0002  max mem: 5511
[10:38:07.194522] Epoch: [73]  [200/781]  eta: 0:01:55  lr: 0.000047  training_loss: 0.3216 (0.3164)  mae_loss: 0.2091 (0.2056)  classification_loss: 0.1087 (0.1108)  time: 0.1952  data: 0.0002  max mem: 5511
[10:38:11.113425] Epoch: [73]  [220/781]  eta: 0:01:50  lr: 0.000047  training_loss: 0.3230 (0.3171)  mae_loss: 0.2001 (0.2058)  classification_loss: 0.1134 (0.1113)  time: 0.1959  data: 0.0003  max mem: 5511
[10:38:15.060074] Epoch: [73]  [240/781]  eta: 0:01:47  lr: 0.000046  training_loss: 0.3178 (0.3175)  mae_loss: 0.2075 (0.2062)  classification_loss: 0.1083 (0.1113)  time: 0.1973  data: 0.0005  max mem: 5511
[10:38:18.961684] Epoch: [73]  [260/781]  eta: 0:01:42  lr: 0.000046  training_loss: 0.2958 (0.3166)  mae_loss: 0.1895 (0.2054)  classification_loss: 0.1100 (0.1112)  time: 0.1950  data: 0.0002  max mem: 5511
[10:38:22.872951] Epoch: [73]  [280/781]  eta: 0:01:38  lr: 0.000046  training_loss: 0.3057 (0.3164)  mae_loss: 0.2020 (0.2054)  classification_loss: 0.1058 (0.1110)  time: 0.1954  data: 0.0003  max mem: 5511
[10:38:26.823412] Epoch: [73]  [300/781]  eta: 0:01:34  lr: 0.000046  training_loss: 0.3228 (0.3174)  mae_loss: 0.2146 (0.2062)  classification_loss: 0.1140 (0.1112)  time: 0.1975  data: 0.0004  max mem: 5511
[10:38:30.713639] Epoch: [73]  [320/781]  eta: 0:01:30  lr: 0.000046  training_loss: 0.3179 (0.3175)  mae_loss: 0.2000 (0.2061)  classification_loss: 0.1117 (0.1114)  time: 0.1944  data: 0.0002  max mem: 5511
[10:38:34.622980] Epoch: [73]  [340/781]  eta: 0:01:26  lr: 0.000046  training_loss: 0.3105 (0.3172)  mae_loss: 0.2049 (0.2062)  classification_loss: 0.1045 (0.1111)  time: 0.1954  data: 0.0003  max mem: 5511
[10:38:38.562037] Epoch: [73]  [360/781]  eta: 0:01:22  lr: 0.000046  training_loss: 0.3274 (0.3177)  mae_loss: 0.2167 (0.2066)  classification_loss: 0.1109 (0.1112)  time: 0.1969  data: 0.0003  max mem: 5511
[10:38:42.490432] Epoch: [73]  [380/781]  eta: 0:01:19  lr: 0.000046  training_loss: 0.3158 (0.3181)  mae_loss: 0.2073 (0.2068)  classification_loss: 0.1116 (0.1112)  time: 0.1963  data: 0.0002  max mem: 5511
[10:38:46.383320] Epoch: [73]  [400/781]  eta: 0:01:15  lr: 0.000046  training_loss: 0.3283 (0.3186)  mae_loss: 0.2172 (0.2072)  classification_loss: 0.1111 (0.1114)  time: 0.1945  data: 0.0003  max mem: 5511
[10:38:50.287440] Epoch: [73]  [420/781]  eta: 0:01:11  lr: 0.000046  training_loss: 0.3148 (0.3187)  mae_loss: 0.2119 (0.2074)  classification_loss: 0.1122 (0.1114)  time: 0.1951  data: 0.0002  max mem: 5511
[10:38:54.201542] Epoch: [73]  [440/781]  eta: 0:01:07  lr: 0.000046  training_loss: 0.3166 (0.3187)  mae_loss: 0.2012 (0.2073)  classification_loss: 0.1129 (0.1114)  time: 0.1956  data: 0.0002  max mem: 5511
[10:38:58.119142] Epoch: [73]  [460/781]  eta: 0:01:03  lr: 0.000046  training_loss: 0.2957 (0.3181)  mae_loss: 0.1919 (0.2069)  classification_loss: 0.1063 (0.1112)  time: 0.1958  data: 0.0002  max mem: 5511
[10:39:02.026787] Epoch: [73]  [480/781]  eta: 0:00:59  lr: 0.000045  training_loss: 0.3227 (0.3182)  mae_loss: 0.2102 (0.2070)  classification_loss: 0.1120 (0.1112)  time: 0.1953  data: 0.0002  max mem: 5511
[10:39:05.930344] Epoch: [73]  [500/781]  eta: 0:00:55  lr: 0.000045  training_loss: 0.3317 (0.3187)  mae_loss: 0.2200 (0.2075)  classification_loss: 0.1162 (0.1113)  time: 0.1951  data: 0.0002  max mem: 5511
[10:39:09.844820] Epoch: [73]  [520/781]  eta: 0:00:51  lr: 0.000045  training_loss: 0.3195 (0.3189)  mae_loss: 0.2110 (0.2076)  classification_loss: 0.1083 (0.1113)  time: 0.1957  data: 0.0003  max mem: 5511
[10:39:13.745821] Epoch: [73]  [540/781]  eta: 0:00:47  lr: 0.000045  training_loss: 0.3127 (0.3187)  mae_loss: 0.2036 (0.2075)  classification_loss: 0.1078 (0.1112)  time: 0.1950  data: 0.0003  max mem: 5511
[10:39:17.668982] Epoch: [73]  [560/781]  eta: 0:00:43  lr: 0.000045  training_loss: 0.3155 (0.3184)  mae_loss: 0.2052 (0.2073)  classification_loss: 0.1077 (0.1111)  time: 0.1961  data: 0.0005  max mem: 5511
[10:39:21.590804] Epoch: [73]  [580/781]  eta: 0:00:39  lr: 0.000045  training_loss: 0.3147 (0.3186)  mae_loss: 0.2080 (0.2075)  classification_loss: 0.1068 (0.1112)  time: 0.1960  data: 0.0002  max mem: 5511
[10:39:25.498783] Epoch: [73]  [600/781]  eta: 0:00:35  lr: 0.000045  training_loss: 0.3032 (0.3184)  mae_loss: 0.2009 (0.2073)  classification_loss: 0.1114 (0.1111)  time: 0.1953  data: 0.0003  max mem: 5511
[10:39:29.417837] Epoch: [73]  [620/781]  eta: 0:00:31  lr: 0.000045  training_loss: 0.3302 (0.3184)  mae_loss: 0.2092 (0.2074)  classification_loss: 0.1046 (0.1110)  time: 0.1958  data: 0.0002  max mem: 5511
[10:39:33.318078] Epoch: [73]  [640/781]  eta: 0:00:27  lr: 0.000045  training_loss: 0.3282 (0.3188)  mae_loss: 0.2128 (0.2077)  classification_loss: 0.1068 (0.1110)  time: 0.1949  data: 0.0002  max mem: 5511
[10:39:37.225293] Epoch: [73]  [660/781]  eta: 0:00:23  lr: 0.000045  training_loss: 0.3130 (0.3187)  mae_loss: 0.2088 (0.2077)  classification_loss: 0.1067 (0.1109)  time: 0.1953  data: 0.0002  max mem: 5511
[10:39:41.125130] Epoch: [73]  [680/781]  eta: 0:00:19  lr: 0.000045  training_loss: 0.3115 (0.3185)  mae_loss: 0.2013 (0.2075)  classification_loss: 0.1114 (0.1110)  time: 0.1949  data: 0.0002  max mem: 5511
[10:39:45.022531] Epoch: [73]  [700/781]  eta: 0:00:15  lr: 0.000045  training_loss: 0.3226 (0.3186)  mae_loss: 0.2045 (0.2077)  classification_loss: 0.1119 (0.1110)  time: 0.1948  data: 0.0002  max mem: 5511
[10:39:48.916352] Epoch: [73]  [720/781]  eta: 0:00:11  lr: 0.000044  training_loss: 0.3197 (0.3188)  mae_loss: 0.2071 (0.2077)  classification_loss: 0.1141 (0.1111)  time: 0.1946  data: 0.0002  max mem: 5511
[10:39:52.816255] Epoch: [73]  [740/781]  eta: 0:00:08  lr: 0.000044  training_loss: 0.3180 (0.3191)  mae_loss: 0.2126 (0.2080)  classification_loss: 0.1088 (0.1111)  time: 0.1949  data: 0.0003  max mem: 5511
[10:39:56.727072] Epoch: [73]  [760/781]  eta: 0:00:04  lr: 0.000044  training_loss: 0.3336 (0.3193)  mae_loss: 0.2173 (0.2082)  classification_loss: 0.1144 (0.1112)  time: 0.1955  data: 0.0002  max mem: 5511
[10:40:00.664197] Epoch: [73]  [780/781]  eta: 0:00:00  lr: 0.000044  training_loss: 0.3215 (0.3193)  mae_loss: 0.2166 (0.2082)  classification_loss: 0.1077 (0.1111)  time: 0.1968  data: 0.0002  max mem: 5511
[10:40:00.818031] Epoch: [73] Total time: 0:02:33 (0.1965 s / it)
[10:40:00.818474] Averaged stats: lr: 0.000044  training_loss: 0.3215 (0.3193)  mae_loss: 0.2166 (0.2082)  classification_loss: 0.1077 (0.1111)
[10:40:01.511378] Test:  [  0/157]  eta: 0:01:48  testing_loss: 0.5246 (0.5246)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.6886  data: 0.6586  max mem: 5511
[10:40:01.812552] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.4277 (0.4230)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (99.7159)  time: 0.0898  data: 0.0601  max mem: 5511
[10:40:02.096335] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.3805 (0.4073)  acc1: 87.5000 (88.1696)  acc5: 100.0000 (99.7768)  time: 0.0291  data: 0.0002  max mem: 5511
[10:40:02.378109] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.3810 (0.4163)  acc1: 89.0625 (88.0544)  acc5: 100.0000 (99.5464)  time: 0.0282  data: 0.0002  max mem: 5511
[10:40:02.659701] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4014 (0.4256)  acc1: 87.5000 (87.5762)  acc5: 100.0000 (99.4284)  time: 0.0280  data: 0.0002  max mem: 5511
[10:40:02.940472] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.3858 (0.4163)  acc1: 87.5000 (87.9289)  acc5: 100.0000 (99.3566)  time: 0.0280  data: 0.0002  max mem: 5511
[10:40:03.221230] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.3776 (0.4144)  acc1: 89.0625 (88.1916)  acc5: 100.0000 (99.3852)  time: 0.0280  data: 0.0001  max mem: 5511
[10:40:03.503606] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.3851 (0.4081)  acc1: 89.0625 (88.3363)  acc5: 100.0000 (99.4278)  time: 0.0280  data: 0.0002  max mem: 5511
[10:40:03.787949] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.3940 (0.4173)  acc1: 87.5000 (88.0980)  acc5: 100.0000 (99.4213)  time: 0.0282  data: 0.0002  max mem: 5511
[10:40:04.073857] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4106 (0.4143)  acc1: 87.5000 (88.1525)  acc5: 100.0000 (99.4677)  time: 0.0284  data: 0.0002  max mem: 5511
[10:40:04.360558] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4106 (0.4176)  acc1: 87.5000 (88.0260)  acc5: 100.0000 (99.5050)  time: 0.0285  data: 0.0004  max mem: 5511
[10:40:04.644872] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4072 (0.4167)  acc1: 89.0625 (88.1475)  acc5: 100.0000 (99.4932)  time: 0.0284  data: 0.0004  max mem: 5511
[10:40:04.926949] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.3977 (0.4145)  acc1: 89.0625 (88.1844)  acc5: 100.0000 (99.5093)  time: 0.0282  data: 0.0002  max mem: 5511
[10:40:05.208142] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4019 (0.4157)  acc1: 87.5000 (88.1083)  acc5: 100.0000 (99.4871)  time: 0.0280  data: 0.0002  max mem: 5511
[10:40:05.488423] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3758 (0.4128)  acc1: 89.0625 (88.1760)  acc5: 100.0000 (99.5124)  time: 0.0280  data: 0.0001  max mem: 5511
[10:40:05.767694] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.3774 (0.4121)  acc1: 87.5000 (88.1623)  acc5: 100.0000 (99.5033)  time: 0.0279  data: 0.0001  max mem: 5511
[10:40:05.917911] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.3897 (0.4127)  acc1: 89.0625 (88.1500)  acc5: 100.0000 (99.5200)  time: 0.0269  data: 0.0001  max mem: 5511
[10:40:06.082816] Test: Total time: 0:00:05 (0.0335 s / it)
[10:40:06.083368] * Acc@1 88.150 Acc@5 99.520 loss 0.413
[10:40:06.083711] Accuracy of the network on the 10000 test images: 88.2%
[10:40:06.083903] Max accuracy: 88.15%
[10:40:06.359008] log_dir: ./output_dir
[10:40:07.172320] Epoch: [74]  [  0/781]  eta: 0:10:33  lr: 0.000044  training_loss: 0.3186 (0.3186)  mae_loss: 0.2155 (0.2155)  classification_loss: 0.1031 (0.1031)  time: 0.8115  data: 0.6041  max mem: 5511
[10:40:11.076894] Epoch: [74]  [ 20/781]  eta: 0:02:50  lr: 0.000044  training_loss: 0.3146 (0.3186)  mae_loss: 0.2082 (0.2086)  classification_loss: 0.1105 (0.1101)  time: 0.1951  data: 0.0002  max mem: 5511
[10:40:15.012668] Epoch: [74]  [ 40/781]  eta: 0:02:36  lr: 0.000044  training_loss: 0.3240 (0.3202)  mae_loss: 0.2076 (0.2088)  classification_loss: 0.1139 (0.1114)  time: 0.1967  data: 0.0002  max mem: 5511
[10:40:18.907584] Epoch: [74]  [ 60/781]  eta: 0:02:28  lr: 0.000044  training_loss: 0.3058 (0.3193)  mae_loss: 0.1981 (0.2079)  classification_loss: 0.1076 (0.1113)  time: 0.1947  data: 0.0002  max mem: 5511
[10:40:22.819491] Epoch: [74]  [ 80/781]  eta: 0:02:22  lr: 0.000044  training_loss: 0.3197 (0.3208)  mae_loss: 0.2065 (0.2097)  classification_loss: 0.1105 (0.1111)  time: 0.1955  data: 0.0004  max mem: 5511
[10:40:26.731098] Epoch: [74]  [100/781]  eta: 0:02:17  lr: 0.000044  training_loss: 0.3242 (0.3206)  mae_loss: 0.2120 (0.2094)  classification_loss: 0.1112 (0.1112)  time: 0.1955  data: 0.0002  max mem: 5511
[10:40:30.632723] Epoch: [74]  [120/781]  eta: 0:02:12  lr: 0.000044  training_loss: 0.3062 (0.3191)  mae_loss: 0.2007 (0.2087)  classification_loss: 0.1055 (0.1104)  time: 0.1950  data: 0.0003  max mem: 5511
[10:40:34.534259] Epoch: [74]  [140/781]  eta: 0:02:08  lr: 0.000044  training_loss: 0.3164 (0.3202)  mae_loss: 0.2016 (0.2099)  classification_loss: 0.1073 (0.1103)  time: 0.1950  data: 0.0003  max mem: 5511
[10:40:38.495328] Epoch: [74]  [160/781]  eta: 0:02:03  lr: 0.000044  training_loss: 0.2982 (0.3188)  mae_loss: 0.2023 (0.2087)  classification_loss: 0.1099 (0.1101)  time: 0.1980  data: 0.0002  max mem: 5511
[10:40:42.401219] Epoch: [74]  [180/781]  eta: 0:01:59  lr: 0.000044  training_loss: 0.3145 (0.3197)  mae_loss: 0.2079 (0.2093)  classification_loss: 0.1145 (0.1103)  time: 0.1952  data: 0.0002  max mem: 5511
[10:40:46.308238] Epoch: [74]  [200/781]  eta: 0:01:55  lr: 0.000043  training_loss: 0.3170 (0.3193)  mae_loss: 0.2026 (0.2089)  classification_loss: 0.1134 (0.1105)  time: 0.1953  data: 0.0003  max mem: 5511
[10:40:50.188367] Epoch: [74]  [220/781]  eta: 0:01:51  lr: 0.000043  training_loss: 0.3169 (0.3192)  mae_loss: 0.2061 (0.2084)  classification_loss: 0.1139 (0.1108)  time: 0.1939  data: 0.0002  max mem: 5511
[10:40:54.075996] Epoch: [74]  [240/781]  eta: 0:01:47  lr: 0.000043  training_loss: 0.3329 (0.3200)  mae_loss: 0.2237 (0.2093)  classification_loss: 0.1079 (0.1107)  time: 0.1943  data: 0.0002  max mem: 5511
[10:40:57.976077] Epoch: [74]  [260/781]  eta: 0:01:42  lr: 0.000043  training_loss: 0.2999 (0.3191)  mae_loss: 0.1891 (0.2084)  classification_loss: 0.1076 (0.1107)  time: 0.1949  data: 0.0002  max mem: 5511
[10:41:01.892270] Epoch: [74]  [280/781]  eta: 0:01:38  lr: 0.000043  training_loss: 0.3217 (0.3191)  mae_loss: 0.2157 (0.2087)  classification_loss: 0.1082 (0.1105)  time: 0.1957  data: 0.0002  max mem: 5511
[10:41:05.822900] Epoch: [74]  [300/781]  eta: 0:01:34  lr: 0.000043  training_loss: 0.3099 (0.3189)  mae_loss: 0.1982 (0.2082)  classification_loss: 0.1126 (0.1107)  time: 0.1964  data: 0.0003  max mem: 5511
[10:41:09.742698] Epoch: [74]  [320/781]  eta: 0:01:30  lr: 0.000043  training_loss: 0.3141 (0.3187)  mae_loss: 0.2058 (0.2079)  classification_loss: 0.1063 (0.1108)  time: 0.1959  data: 0.0002  max mem: 5511
[10:41:13.657837] Epoch: [74]  [340/781]  eta: 0:01:26  lr: 0.000043  training_loss: 0.3135 (0.3187)  mae_loss: 0.2022 (0.2080)  classification_loss: 0.1049 (0.1107)  time: 0.1957  data: 0.0003  max mem: 5511
[10:41:17.546820] Epoch: [74]  [360/781]  eta: 0:01:22  lr: 0.000043  training_loss: 0.3112 (0.3185)  mae_loss: 0.1964 (0.2077)  classification_loss: 0.1104 (0.1108)  time: 0.1944  data: 0.0002  max mem: 5511
[10:41:21.440837] Epoch: [74]  [380/781]  eta: 0:01:18  lr: 0.000043  training_loss: 0.3151 (0.3187)  mae_loss: 0.2089 (0.2077)  classification_loss: 0.1134 (0.1110)  time: 0.1946  data: 0.0002  max mem: 5511
[10:41:25.393760] Epoch: [74]  [400/781]  eta: 0:01:15  lr: 0.000043  training_loss: 0.3156 (0.3188)  mae_loss: 0.2127 (0.2080)  classification_loss: 0.1030 (0.1108)  time: 0.1976  data: 0.0003  max mem: 5511
[10:41:29.293489] Epoch: [74]  [420/781]  eta: 0:01:11  lr: 0.000043  training_loss: 0.3215 (0.3189)  mae_loss: 0.2077 (0.2081)  classification_loss: 0.1129 (0.1109)  time: 0.1949  data: 0.0003  max mem: 5511
[10:41:33.198115] Epoch: [74]  [440/781]  eta: 0:01:07  lr: 0.000043  training_loss: 0.3109 (0.3188)  mae_loss: 0.2038 (0.2080)  classification_loss: 0.1095 (0.1108)  time: 0.1952  data: 0.0002  max mem: 5511
[10:41:37.105275] Epoch: [74]  [460/781]  eta: 0:01:03  lr: 0.000042  training_loss: 0.2956 (0.3181)  mae_loss: 0.1955 (0.2074)  classification_loss: 0.1053 (0.1107)  time: 0.1953  data: 0.0002  max mem: 5511
[10:41:41.000460] Epoch: [74]  [480/781]  eta: 0:00:59  lr: 0.000042  training_loss: 0.2935 (0.3175)  mae_loss: 0.1912 (0.2068)  classification_loss: 0.1127 (0.1107)  time: 0.1947  data: 0.0002  max mem: 5511
[10:41:44.904199] Epoch: [74]  [500/781]  eta: 0:00:55  lr: 0.000042  training_loss: 0.3135 (0.3173)  mae_loss: 0.2045 (0.2068)  classification_loss: 0.1064 (0.1106)  time: 0.1951  data: 0.0002  max mem: 5511
[10:41:48.826667] Epoch: [74]  [520/781]  eta: 0:00:51  lr: 0.000042  training_loss: 0.3050 (0.3171)  mae_loss: 0.1954 (0.2065)  classification_loss: 0.1092 (0.1106)  time: 0.1960  data: 0.0002  max mem: 5511
[10:41:52.737034] Epoch: [74]  [540/781]  eta: 0:00:47  lr: 0.000042  training_loss: 0.3284 (0.3173)  mae_loss: 0.2147 (0.2067)  classification_loss: 0.1118 (0.1106)  time: 0.1954  data: 0.0002  max mem: 5511
[10:41:56.678205] Epoch: [74]  [560/781]  eta: 0:00:43  lr: 0.000042  training_loss: 0.3153 (0.3173)  mae_loss: 0.2072 (0.2067)  classification_loss: 0.1106 (0.1106)  time: 0.1970  data: 0.0002  max mem: 5511
[10:42:00.585003] Epoch: [74]  [580/781]  eta: 0:00:39  lr: 0.000042  training_loss: 0.3233 (0.3178)  mae_loss: 0.2144 (0.2071)  classification_loss: 0.1098 (0.1106)  time: 0.1952  data: 0.0002  max mem: 5511
[10:42:04.493717] Epoch: [74]  [600/781]  eta: 0:00:35  lr: 0.000042  training_loss: 0.3133 (0.3176)  mae_loss: 0.1953 (0.2070)  classification_loss: 0.1091 (0.1106)  time: 0.1954  data: 0.0002  max mem: 5511
[10:42:08.402218] Epoch: [74]  [620/781]  eta: 0:00:31  lr: 0.000042  training_loss: 0.3244 (0.3178)  mae_loss: 0.2142 (0.2071)  classification_loss: 0.1133 (0.1107)  time: 0.1953  data: 0.0002  max mem: 5511
[10:42:12.317876] Epoch: [74]  [640/781]  eta: 0:00:27  lr: 0.000042  training_loss: 0.3163 (0.3177)  mae_loss: 0.2061 (0.2071)  classification_loss: 0.1085 (0.1106)  time: 0.1956  data: 0.0002  max mem: 5511
[10:42:16.230419] Epoch: [74]  [660/781]  eta: 0:00:23  lr: 0.000042  training_loss: 0.3110 (0.3179)  mae_loss: 0.2069 (0.2073)  classification_loss: 0.1085 (0.1106)  time: 0.1955  data: 0.0002  max mem: 5511
[10:42:20.137795] Epoch: [74]  [680/781]  eta: 0:00:19  lr: 0.000042  training_loss: 0.3183 (0.3179)  mae_loss: 0.2049 (0.2073)  classification_loss: 0.1105 (0.1107)  time: 0.1953  data: 0.0002  max mem: 5511
[10:42:24.046918] Epoch: [74]  [700/781]  eta: 0:00:15  lr: 0.000041  training_loss: 0.3140 (0.3180)  mae_loss: 0.2026 (0.2073)  classification_loss: 0.1068 (0.1106)  time: 0.1954  data: 0.0004  max mem: 5511
[10:42:27.945048] Epoch: [74]  [720/781]  eta: 0:00:11  lr: 0.000041  training_loss: 0.3031 (0.3176)  mae_loss: 0.1891 (0.2070)  classification_loss: 0.1096 (0.1106)  time: 0.1948  data: 0.0003  max mem: 5511
[10:42:31.841825] Epoch: [74]  [740/781]  eta: 0:00:08  lr: 0.000041  training_loss: 0.3202 (0.3177)  mae_loss: 0.2097 (0.2071)  classification_loss: 0.1091 (0.1106)  time: 0.1948  data: 0.0002  max mem: 5511
[10:42:35.735346] Epoch: [74]  [760/781]  eta: 0:00:04  lr: 0.000041  training_loss: 0.3171 (0.3176)  mae_loss: 0.2071 (0.2071)  classification_loss: 0.1081 (0.1106)  time: 0.1946  data: 0.0002  max mem: 5511
[10:42:39.626551] Epoch: [74]  [780/781]  eta: 0:00:00  lr: 0.000041  training_loss: 0.3264 (0.3177)  mae_loss: 0.2121 (0.2071)  classification_loss: 0.1128 (0.1107)  time: 0.1945  data: 0.0002  max mem: 5511
[10:42:39.799733] Epoch: [74] Total time: 0:02:33 (0.1965 s / it)
[10:42:39.800207] Averaged stats: lr: 0.000041  training_loss: 0.3264 (0.3177)  mae_loss: 0.2121 (0.2071)  classification_loss: 0.1128 (0.1107)
[10:42:40.334729] Test:  [  0/157]  eta: 0:01:23  testing_loss: 0.4936 (0.4936)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.5306  data: 0.5012  max mem: 5511
[10:42:40.619203] Test:  [ 10/157]  eta: 0:00:10  testing_loss: 0.4267 (0.4216)  acc1: 87.5000 (87.6420)  acc5: 100.0000 (99.7159)  time: 0.0738  data: 0.0457  max mem: 5511
[10:42:40.900635] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4207 (0.4070)  acc1: 89.0625 (88.2440)  acc5: 100.0000 (99.7768)  time: 0.0281  data: 0.0001  max mem: 5511
[10:42:41.182837] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.3826 (0.4174)  acc1: 89.0625 (87.8024)  acc5: 100.0000 (99.5464)  time: 0.0280  data: 0.0002  max mem: 5511
[10:42:41.466370] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.4349 (0.4298)  acc1: 89.0625 (87.5762)  acc5: 100.0000 (99.4665)  time: 0.0281  data: 0.0002  max mem: 5511
[10:42:41.749298] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.3984 (0.4198)  acc1: 89.0625 (87.8370)  acc5: 100.0000 (99.4485)  time: 0.0281  data: 0.0002  max mem: 5511
[10:42:42.031281] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.3596 (0.4159)  acc1: 89.0625 (87.9355)  acc5: 100.0000 (99.4109)  time: 0.0281  data: 0.0002  max mem: 5511
[10:42:42.320377] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.3817 (0.4091)  acc1: 89.0625 (88.2702)  acc5: 100.0000 (99.4498)  time: 0.0284  data: 0.0002  max mem: 5511
[10:42:42.608956] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.3934 (0.4162)  acc1: 89.0625 (88.0787)  acc5: 100.0000 (99.4599)  time: 0.0287  data: 0.0002  max mem: 5511
[10:42:42.893335] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4075 (0.4133)  acc1: 87.5000 (88.1181)  acc5: 100.0000 (99.5021)  time: 0.0285  data: 0.0002  max mem: 5511
[10:42:43.177769] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4285 (0.4177)  acc1: 87.5000 (87.9796)  acc5: 100.0000 (99.5204)  time: 0.0283  data: 0.0002  max mem: 5511
[10:42:43.460240] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.3956 (0.4166)  acc1: 89.0625 (88.1334)  acc5: 100.0000 (99.5073)  time: 0.0282  data: 0.0002  max mem: 5511
[10:42:43.742517] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.3762 (0.4140)  acc1: 90.6250 (88.2361)  acc5: 100.0000 (99.4706)  time: 0.0281  data: 0.0002  max mem: 5511
[10:42:44.024571] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4116 (0.4169)  acc1: 87.5000 (88.0844)  acc5: 100.0000 (99.4513)  time: 0.0281  data: 0.0002  max mem: 5511
[10:42:44.304644] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3591 (0.4140)  acc1: 87.5000 (88.1427)  acc5: 100.0000 (99.4792)  time: 0.0280  data: 0.0001  max mem: 5511
[10:42:44.584123] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.3679 (0.4138)  acc1: 87.5000 (88.1209)  acc5: 100.0000 (99.4619)  time: 0.0278  data: 0.0001  max mem: 5511
[10:42:44.737880] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.3704 (0.4137)  acc1: 87.5000 (88.0900)  acc5: 100.0000 (99.4500)  time: 0.0271  data: 0.0001  max mem: 5511
[10:42:44.923978] Test: Total time: 0:00:05 (0.0326 s / it)
[10:42:44.924434] * Acc@1 88.090 Acc@5 99.450 loss 0.414
[10:42:44.924721] Accuracy of the network on the 10000 test images: 88.1%
[10:42:44.924906] Max accuracy: 88.15%
[10:42:45.325423] log_dir: ./output_dir
[10:42:46.099245] Epoch: [75]  [  0/781]  eta: 0:10:02  lr: 0.000041  training_loss: 0.3268 (0.3268)  mae_loss: 0.2176 (0.2176)  classification_loss: 0.1091 (0.1091)  time: 0.7720  data: 0.5393  max mem: 5511
[10:42:49.989211] Epoch: [75]  [ 20/781]  eta: 0:02:48  lr: 0.000041  training_loss: 0.3078 (0.3169)  mae_loss: 0.2043 (0.2084)  classification_loss: 0.1073 (0.1085)  time: 0.1944  data: 0.0002  max mem: 5511
[10:42:53.887081] Epoch: [75]  [ 40/781]  eta: 0:02:34  lr: 0.000041  training_loss: 0.3037 (0.3141)  mae_loss: 0.1954 (0.2041)  classification_loss: 0.1117 (0.1100)  time: 0.1948  data: 0.0002  max mem: 5511
[10:42:57.794852] Epoch: [75]  [ 60/781]  eta: 0:02:27  lr: 0.000041  training_loss: 0.3027 (0.3123)  mae_loss: 0.1980 (0.2023)  classification_loss: 0.1100 (0.1100)  time: 0.1953  data: 0.0002  max mem: 5511
[10:43:01.692398] Epoch: [75]  [ 80/781]  eta: 0:02:21  lr: 0.000041  training_loss: 0.3156 (0.3126)  mae_loss: 0.2010 (0.2024)  classification_loss: 0.1112 (0.1102)  time: 0.1948  data: 0.0002  max mem: 5511
[10:43:05.668704] Epoch: [75]  [100/781]  eta: 0:02:17  lr: 0.000041  training_loss: 0.3177 (0.3145)  mae_loss: 0.2120 (0.2045)  classification_loss: 0.1049 (0.1100)  time: 0.1987  data: 0.0002  max mem: 5511
[10:43:09.567842] Epoch: [75]  [120/781]  eta: 0:02:12  lr: 0.000041  training_loss: 0.3177 (0.3155)  mae_loss: 0.2059 (0.2055)  classification_loss: 0.1114 (0.1100)  time: 0.1949  data: 0.0003  max mem: 5511
[10:43:13.489119] Epoch: [75]  [140/781]  eta: 0:02:07  lr: 0.000041  training_loss: 0.3130 (0.3164)  mae_loss: 0.2040 (0.2064)  classification_loss: 0.1092 (0.1100)  time: 0.1960  data: 0.0002  max mem: 5511
[10:43:17.396475] Epoch: [75]  [160/781]  eta: 0:02:03  lr: 0.000041  training_loss: 0.3214 (0.3165)  mae_loss: 0.2098 (0.2062)  classification_loss: 0.1121 (0.1103)  time: 0.1953  data: 0.0002  max mem: 5511
[10:43:21.293980] Epoch: [75]  [180/781]  eta: 0:01:59  lr: 0.000040  training_loss: 0.3028 (0.3165)  mae_loss: 0.2030 (0.2064)  classification_loss: 0.1064 (0.1101)  time: 0.1948  data: 0.0003  max mem: 5511
[10:43:25.190833] Epoch: [75]  [200/781]  eta: 0:01:55  lr: 0.000040  training_loss: 0.3124 (0.3166)  mae_loss: 0.2045 (0.2065)  classification_loss: 0.1100 (0.1100)  time: 0.1948  data: 0.0002  max mem: 5511
[10:43:29.094363] Epoch: [75]  [220/781]  eta: 0:01:51  lr: 0.000040  training_loss: 0.3224 (0.3169)  mae_loss: 0.2049 (0.2064)  classification_loss: 0.1122 (0.1104)  time: 0.1951  data: 0.0002  max mem: 5511
[10:43:33.004863] Epoch: [75]  [240/781]  eta: 0:01:46  lr: 0.000040  training_loss: 0.3158 (0.3170)  mae_loss: 0.2052 (0.2066)  classification_loss: 0.1112 (0.1105)  time: 0.1955  data: 0.0005  max mem: 5511
[10:43:36.914075] Epoch: [75]  [260/781]  eta: 0:01:42  lr: 0.000040  training_loss: 0.3088 (0.3170)  mae_loss: 0.2010 (0.2066)  classification_loss: 0.1078 (0.1104)  time: 0.1954  data: 0.0002  max mem: 5511
[10:43:40.828367] Epoch: [75]  [280/781]  eta: 0:01:38  lr: 0.000040  training_loss: 0.3182 (0.3170)  mae_loss: 0.2064 (0.2067)  classification_loss: 0.1094 (0.1103)  time: 0.1956  data: 0.0003  max mem: 5511
[10:43:44.751570] Epoch: [75]  [300/781]  eta: 0:01:34  lr: 0.000040  training_loss: 0.3188 (0.3172)  mae_loss: 0.2126 (0.2070)  classification_loss: 0.1085 (0.1103)  time: 0.1961  data: 0.0003  max mem: 5511
[10:43:48.664827] Epoch: [75]  [320/781]  eta: 0:01:30  lr: 0.000040  training_loss: 0.2979 (0.3169)  mae_loss: 0.1893 (0.2065)  classification_loss: 0.1072 (0.1104)  time: 0.1956  data: 0.0004  max mem: 5511
[10:43:52.575103] Epoch: [75]  [340/781]  eta: 0:01:26  lr: 0.000040  training_loss: 0.3110 (0.3168)  mae_loss: 0.2067 (0.2066)  classification_loss: 0.1045 (0.1102)  time: 0.1954  data: 0.0003  max mem: 5511
[10:43:56.490160] Epoch: [75]  [360/781]  eta: 0:01:22  lr: 0.000040  training_loss: 0.3291 (0.3172)  mae_loss: 0.2139 (0.2069)  classification_loss: 0.1097 (0.1103)  time: 0.1957  data: 0.0002  max mem: 5511

[10:44:00.394008] Epoch: [75]  [380/781]  eta: 0:01:18  lr: 0.000040  training_loss: 0.3205 (0.3175)  mae_loss: 0.2034 (0.2070)  classification_loss: 0.1149 (0.1105)  time: 0.1951  data: 0.0004  max mem: 5511
[10:44:04.298391] Epoch: [75]  [400/781]  eta: 0:01:15  lr: 0.000040  training_loss: 0.3167 (0.3179)  mae_loss: 0.2103 (0.2073)  classification_loss: 0.1118 (0.1106)  time: 0.1951  data: 0.0002  max mem: 5511
[10:44:08.246844] Epoch: [75]  [420/781]  eta: 0:01:11  lr: 0.000040  training_loss: 0.3138 (0.3182)  mae_loss: 0.2017 (0.2076)  classification_loss: 0.1072 (0.1106)  time: 0.1973  data: 0.0002  max mem: 5511
[10:44:12.212686] Epoch: [75]  [440/781]  eta: 0:01:07  lr: 0.000039  training_loss: 0.3118 (0.3179)  mae_loss: 0.1998 (0.2074)  classification_loss: 0.1093 (0.1105)  time: 0.1982  data: 0.0002  max mem: 5511
[10:44:16.130286] Epoch: [75]  [460/781]  eta: 0:01:03  lr: 0.000039  training_loss: 0.3037 (0.3173)  mae_loss: 0.2008 (0.2070)  classification_loss: 0.1037 (0.1103)  time: 0.1958  data: 0.0006  max mem: 5511
[10:44:20.033595] Epoch: [75]  [480/781]  eta: 0:00:59  lr: 0.000039  training_loss: 0.3023 (0.3170)  mae_loss: 0.1939 (0.2067)  classification_loss: 0.1097 (0.1103)  time: 0.1951  data: 0.0002  max mem: 5511
[10:44:23.980071] Epoch: [75]  [500/781]  eta: 0:00:55  lr: 0.000039  training_loss: 0.3152 (0.3172)  mae_loss: 0.2078 (0.2069)  classification_loss: 0.1079 (0.1103)  time: 0.1972  data: 0.0002  max mem: 5511
[10:44:27.908330] Epoch: [75]  [520/781]  eta: 0:00:51  lr: 0.000039  training_loss: 0.3101 (0.3172)  mae_loss: 0.1955 (0.2069)  classification_loss: 0.1078 (0.1103)  time: 0.1963  data: 0.0003  max mem: 5511
[10:44:31.801391] Epoch: [75]  [540/781]  eta: 0:00:47  lr: 0.000039  training_loss: 0.3181 (0.3173)  mae_loss: 0.2072 (0.2071)  classification_loss: 0.1068 (0.1103)  time: 0.1946  data: 0.0002  max mem: 5511
[10:44:35.786841] Epoch: [75]  [560/781]  eta: 0:00:43  lr: 0.000039  training_loss: 0.3289 (0.3176)  mae_loss: 0.2025 (0.2073)  classification_loss: 0.1095 (0.1103)  time: 0.1992  data: 0.0002  max mem: 5511
[10:44:39.733080] Epoch: [75]  [580/781]  eta: 0:00:39  lr: 0.000039  training_loss: 0.3092 (0.3173)  mae_loss: 0.1923 (0.2071)  classification_loss: 0.1065 (0.1102)  time: 0.1972  data: 0.0002  max mem: 5511
[10:44:43.623238] Epoch: [75]  [600/781]  eta: 0:00:35  lr: 0.000039  training_loss: 0.3086 (0.3170)  mae_loss: 0.1997 (0.2068)  classification_loss: 0.1076 (0.1102)  time: 0.1944  data: 0.0002  max mem: 5511
[10:44:47.537091] Epoch: [75]  [620/781]  eta: 0:00:31  lr: 0.000039  training_loss: 0.3194 (0.3172)  mae_loss: 0.2077 (0.2070)  classification_loss: 0.1101 (0.1102)  time: 0.1956  data: 0.0002  max mem: 5511
[10:44:51.460278] Epoch: [75]  [640/781]  eta: 0:00:27  lr: 0.000039  training_loss: 0.3215 (0.3171)  mae_loss: 0.2189 (0.2070)  classification_loss: 0.1077 (0.1101)  time: 0.1961  data: 0.0003  max mem: 5511
[10:44:55.367593] Epoch: [75]  [660/781]  eta: 0:00:23  lr: 0.000039  training_loss: 0.3070 (0.3170)  mae_loss: 0.2021 (0.2070)  classification_loss: 0.1069 (0.1100)  time: 0.1953  data: 0.0002  max mem: 5511
[10:44:59.293366] Epoch: [75]  [680/781]  eta: 0:00:19  lr: 0.000039  training_loss: 0.3257 (0.3175)  mae_loss: 0.2139 (0.2074)  classification_loss: 0.1123 (0.1101)  time: 0.1962  data: 0.0002  max mem: 5511
[10:45:03.224920] Epoch: [75]  [700/781]  eta: 0:00:15  lr: 0.000039  training_loss: 0.3220 (0.3178)  mae_loss: 0.2098 (0.2077)  classification_loss: 0.1085 (0.1101)  time: 0.1965  data: 0.0003  max mem: 5511
[10:45:07.146219] Epoch: [75]  [720/781]  eta: 0:00:11  lr: 0.000038  training_loss: 0.3260 (0.3183)  mae_loss: 0.2179 (0.2080)  classification_loss: 0.1132 (0.1103)  time: 0.1960  data: 0.0002  max mem: 5511
[10:45:11.079189] Epoch: [75]  [740/781]  eta: 0:00:08  lr: 0.000038  training_loss: 0.3174 (0.3183)  mae_loss: 0.2056 (0.2080)  classification_loss: 0.1096 (0.1102)  time: 0.1965  data: 0.0002  max mem: 5511
[10:45:14.964111] Epoch: [75]  [760/781]  eta: 0:00:04  lr: 0.000038  training_loss: 0.3097 (0.3180)  mae_loss: 0.1979 (0.2078)  classification_loss: 0.1131 (0.1103)  time: 0.1942  data: 0.0003  max mem: 5511
[10:45:18.885123] Epoch: [75]  [780/781]  eta: 0:00:00  lr: 0.000038  training_loss: 0.3249 (0.3181)  mae_loss: 0.2130 (0.2079)  classification_loss: 0.1087 (0.1103)  time: 0.1960  data: 0.0002  max mem: 5511
[10:45:19.044113] Epoch: [75] Total time: 0:02:33 (0.1968 s / it)
[10:45:19.044727] Averaged stats: lr: 0.000038  training_loss: 0.3249 (0.3181)  mae_loss: 0.2130 (0.2079)  classification_loss: 0.1087 (0.1103)
[10:45:19.686533] Test:  [  0/157]  eta: 0:01:39  testing_loss: 0.5334 (0.5334)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.6364  data: 0.6049  max mem: 5511
[10:45:19.975713] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4291 (0.4321)  acc1: 85.9375 (87.7841)  acc5: 100.0000 (99.5739)  time: 0.0838  data: 0.0551  max mem: 5511
[10:45:20.262609] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.4154 (0.4084)  acc1: 85.9375 (88.2440)  acc5: 100.0000 (99.6280)  time: 0.0285  data: 0.0002  max mem: 5511
[10:45:20.545497] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.3635 (0.4124)  acc1: 89.0625 (88.0544)  acc5: 100.0000 (99.4456)  time: 0.0282  data: 0.0002  max mem: 5511
[10:45:20.828872] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4226 (0.4229)  acc1: 87.5000 (87.6905)  acc5: 100.0000 (99.3902)  time: 0.0281  data: 0.0002  max mem: 5511
[10:45:21.112906] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4083 (0.4126)  acc1: 87.5000 (88.0821)  acc5: 100.0000 (99.3873)  time: 0.0281  data: 0.0002  max mem: 5511
[10:45:21.397428] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.3494 (0.4091)  acc1: 89.0625 (88.3197)  acc5: 100.0000 (99.3596)  time: 0.0282  data: 0.0002  max mem: 5511
[10:45:21.679596] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.3499 (0.4020)  acc1: 89.0625 (88.5783)  acc5: 100.0000 (99.3618)  time: 0.0282  data: 0.0002  max mem: 5511
[10:45:21.963843] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.3756 (0.4093)  acc1: 89.0625 (88.4066)  acc5: 100.0000 (99.3441)  time: 0.0282  data: 0.0002  max mem: 5511
[10:45:22.246317] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4022 (0.4054)  acc1: 87.5000 (88.4615)  acc5: 100.0000 (99.3819)  time: 0.0282  data: 0.0001  max mem: 5511
[10:45:22.530477] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4022 (0.4109)  acc1: 89.0625 (88.3045)  acc5: 100.0000 (99.3812)  time: 0.0282  data: 0.0002  max mem: 5511
[10:45:22.812497] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.3952 (0.4093)  acc1: 89.0625 (88.3305)  acc5: 100.0000 (99.3806)  time: 0.0282  data: 0.0002  max mem: 5511
[10:45:23.094466] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.3615 (0.4063)  acc1: 87.5000 (88.4427)  acc5: 100.0000 (99.3931)  time: 0.0281  data: 0.0002  max mem: 5511
[10:45:23.376424] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.4189 (0.4081)  acc1: 87.5000 (88.3707)  acc5: 100.0000 (99.3798)  time: 0.0281  data: 0.0002  max mem: 5511
[10:45:23.658091] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3789 (0.4065)  acc1: 87.5000 (88.3644)  acc5: 100.0000 (99.4016)  time: 0.0281  data: 0.0002  max mem: 5511
[10:45:23.936875] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.4075 (0.4078)  acc1: 87.5000 (88.3278)  acc5: 100.0000 (99.4102)  time: 0.0279  data: 0.0001  max mem: 5511
[10:45:24.086324] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.4060 (0.4072)  acc1: 87.5000 (88.2600)  acc5: 100.0000 (99.4100)  time: 0.0269  data: 0.0001  max mem: 5511
[10:45:24.241583] Test: Total time: 0:00:05 (0.0331 s / it)
[10:45:24.242002] * Acc@1 88.260 Acc@5 99.410 loss 0.407
[10:45:24.242286] Accuracy of the network on the 10000 test images: 88.3%
[10:45:24.242457] Max accuracy: 88.26%
[10:45:24.428319] log_dir: ./output_dir
[10:45:25.375327] Epoch: [76]  [  0/781]  eta: 0:12:18  lr: 0.000038  training_loss: 0.3317 (0.3317)  mae_loss: 0.2152 (0.2152)  classification_loss: 0.1165 (0.1165)  time: 0.9453  data: 0.7222  max mem: 5511
[10:45:29.310452] Epoch: [76]  [ 20/781]  eta: 0:02:56  lr: 0.000038  training_loss: 0.3173 (0.3211)  mae_loss: 0.2071 (0.2123)  classification_loss: 0.1068 (0.1088)  time: 0.1966  data: 0.0003  max mem: 5511
[10:45:33.247489] Epoch: [76]  [ 40/781]  eta: 0:02:39  lr: 0.000038  training_loss: 0.3060 (0.3149)  mae_loss: 0.1915 (0.2046)  classification_loss: 0.1117 (0.1103)  time: 0.1968  data: 0.0002  max mem: 5511
[10:45:37.184404] Epoch: [76]  [ 60/781]  eta: 0:02:30  lr: 0.000038  training_loss: 0.3172 (0.3167)  mae_loss: 0.2073 (0.2066)  classification_loss: 0.1078 (0.1101)  time: 0.1968  data: 0.0003  max mem: 5511
[10:45:41.130377] Epoch: [76]  [ 80/781]  eta: 0:02:24  lr: 0.000038  training_loss: 0.3035 (0.3163)  mae_loss: 0.2058 (0.2068)  classification_loss: 0.1079 (0.1094)  time: 0.1972  data: 0.0003  max mem: 5511
[10:45:45.080480] Epoch: [76]  [100/781]  eta: 0:02:19  lr: 0.000038  training_loss: 0.3142 (0.3181)  mae_loss: 0.2106 (0.2083)  classification_loss: 0.1096 (0.1097)  time: 0.1974  data: 0.0003  max mem: 5511
[10:45:48.990243] Epoch: [76]  [120/781]  eta: 0:02:14  lr: 0.000038  training_loss: 0.2983 (0.3157)  mae_loss: 0.1848 (0.2059)  classification_loss: 0.1099 (0.1098)  time: 0.1954  data: 0.0002  max mem: 5511
[10:45:52.927876] Epoch: [76]  [140/781]  eta: 0:02:09  lr: 0.000038  training_loss: 0.3154 (0.3165)  mae_loss: 0.2040 (0.2067)  classification_loss: 0.1104 (0.1098)  time: 0.1968  data: 0.0003  max mem: 5511
[10:45:56.858242] Epoch: [76]  [160/781]  eta: 0:02:05  lr: 0.000038  training_loss: 0.3155 (0.3161)  mae_loss: 0.2036 (0.2064)  classification_loss: 0.1104 (0.1097)  time: 0.1965  data: 0.0002  max mem: 5511
[10:46:00.817642] Epoch: [76]  [180/781]  eta: 0:02:00  lr: 0.000038  training_loss: 0.3108 (0.3161)  mae_loss: 0.1969 (0.2062)  classification_loss: 0.1082 (0.1099)  time: 0.1979  data: 0.0004  max mem: 5511
[10:46:04.713520] Epoch: [76]  [200/781]  eta: 0:01:56  lr: 0.000037  training_loss: 0.3232 (0.3170)  mae_loss: 0.2115 (0.2069)  classification_loss: 0.1113 (0.1101)  time: 0.1947  data: 0.0002  max mem: 5511
[10:46:08.598942] Epoch: [76]  [220/781]  eta: 0:01:52  lr: 0.000037  training_loss: 0.3158 (0.3170)  mae_loss: 0.2001 (0.2069)  classification_loss: 0.1102 (0.1102)  time: 0.1942  data: 0.0002  max mem: 5511
[10:46:12.520118] Epoch: [76]  [240/781]  eta: 0:01:47  lr: 0.000037  training_loss: 0.3169 (0.3174)  mae_loss: 0.2087 (0.2074)  classification_loss: 0.1081 (0.1100)  time: 0.1960  data: 0.0002  max mem: 5511
[10:46:16.425320] Epoch: [76]  [260/781]  eta: 0:01:43  lr: 0.000037  training_loss: 0.3200 (0.3173)  mae_loss: 0.2068 (0.2072)  classification_loss: 0.1111 (0.1101)  time: 0.1952  data: 0.0002  max mem: 5511
[10:46:20.331788] Epoch: [76]  [280/781]  eta: 0:01:39  lr: 0.000037  training_loss: 0.3114 (0.3176)  mae_loss: 0.2073 (0.2075)  classification_loss: 0.1089 (0.1100)  time: 0.1953  data: 0.0005  max mem: 5511
[10:46:24.225567] Epoch: [76]  [300/781]  eta: 0:01:35  lr: 0.000037  training_loss: 0.3079 (0.3176)  mae_loss: 0.2058 (0.2077)  classification_loss: 0.1079 (0.1099)  time: 0.1946  data: 0.0002  max mem: 5511
[10:46:28.147505] Epoch: [76]  [320/781]  eta: 0:01:31  lr: 0.000037  training_loss: 0.3089 (0.3173)  mae_loss: 0.1975 (0.2073)  classification_loss: 0.1129 (0.1100)  time: 0.1960  data: 0.0002  max mem: 5511
[10:46:32.123212] Epoch: [76]  [340/781]  eta: 0:01:27  lr: 0.000037  training_loss: 0.3006 (0.3165)  mae_loss: 0.2008 (0.2066)  classification_loss: 0.1065 (0.1099)  time: 0.1987  data: 0.0002  max mem: 5511
[10:46:36.035989] Epoch: [76]  [360/781]  eta: 0:01:23  lr: 0.000037  training_loss: 0.3102 (0.3162)  mae_loss: 0.2013 (0.2062)  classification_loss: 0.1104 (0.1100)  time: 0.1956  data: 0.0002  max mem: 5511
[10:46:39.933434] Epoch: [76]  [380/781]  eta: 0:01:19  lr: 0.000037  training_loss: 0.3151 (0.3165)  mae_loss: 0.2021 (0.2065)  classification_loss: 0.1080 (0.1100)  time: 0.1948  data: 0.0003  max mem: 5511
[10:46:43.937103] Epoch: [76]  [400/781]  eta: 0:01:15  lr: 0.000037  training_loss: 0.3212 (0.3166)  mae_loss: 0.2095 (0.2066)  classification_loss: 0.1111 (0.1100)  time: 0.2001  data: 0.0002  max mem: 5511
[10:46:47.880968] Epoch: [76]  [420/781]  eta: 0:01:11  lr: 0.000037  training_loss: 0.3136 (0.3166)  mae_loss: 0.2057 (0.2066)  classification_loss: 0.1093 (0.1100)  time: 0.1971  data: 0.0003  max mem: 5511
[10:46:51.786001] Epoch: [76]  [440/781]  eta: 0:01:07  lr: 0.000037  training_loss: 0.3212 (0.3169)  mae_loss: 0.2111 (0.2070)  classification_loss: 0.1096 (0.1099)  time: 0.1952  data: 0.0002  max mem: 5511
[10:46:55.704097] Epoch: [76]  [460/781]  eta: 0:01:03  lr: 0.000036  training_loss: 0.3040 (0.3164)  mae_loss: 0.1946 (0.2067)  classification_loss: 0.1041 (0.1097)  time: 0.1958  data: 0.0002  max mem: 5511
[10:46:59.620773] Epoch: [76]  [480/781]  eta: 0:00:59  lr: 0.000036  training_loss: 0.3230 (0.3166)  mae_loss: 0.2075 (0.2066)  classification_loss: 0.1141 (0.1100)  time: 0.1958  data: 0.0003  max mem: 5511
[10:47:03.569577] Epoch: [76]  [500/781]  eta: 0:00:55  lr: 0.000036  training_loss: 0.3148 (0.3164)  mae_loss: 0.2024 (0.2064)  classification_loss: 0.1117 (0.1100)  time: 0.1974  data: 0.0003  max mem: 5511
[10:47:07.473903] Epoch: [76]  [520/781]  eta: 0:00:51  lr: 0.000036  training_loss: 0.3135 (0.3165)  mae_loss: 0.2046 (0.2065)  classification_loss: 0.1068 (0.1100)  time: 0.1951  data: 0.0002  max mem: 5511
[10:47:11.379901] Epoch: [76]  [540/781]  eta: 0:00:47  lr: 0.000036  training_loss: 0.3163 (0.3166)  mae_loss: 0.2076 (0.2066)  classification_loss: 0.1088 (0.1100)  time: 0.1952  data: 0.0003  max mem: 5511
[10:47:15.331541] Epoch: [76]  [560/781]  eta: 0:00:43  lr: 0.000036  training_loss: 0.3062 (0.3164)  mae_loss: 0.1973 (0.2064)  classification_loss: 0.1102 (0.1100)  time: 0.1975  data: 0.0003  max mem: 5511
[10:47:19.267614] Epoch: [76]  [580/781]  eta: 0:00:39  lr: 0.000036  training_loss: 0.3155 (0.3166)  mae_loss: 0.2206 (0.2067)  classification_loss: 0.1076 (0.1099)  time: 0.1967  data: 0.0002  max mem: 5511
[10:47:23.168139] Epoch: [76]  [600/781]  eta: 0:00:35  lr: 0.000036  training_loss: 0.3261 (0.3169)  mae_loss: 0.2178 (0.2071)  classification_loss: 0.1060 (0.1098)  time: 0.1950  data: 0.0002  max mem: 5511
[10:47:27.086355] Epoch: [76]  [620/781]  eta: 0:00:31  lr: 0.000036  training_loss: 0.3201 (0.3171)  mae_loss: 0.2034 (0.2071)  classification_loss: 0.1116 (0.1099)  time: 0.1958  data: 0.0002  max mem: 5511
[10:47:30.987702] Epoch: [76]  [640/781]  eta: 0:00:27  lr: 0.000036  training_loss: 0.3088 (0.3170)  mae_loss: 0.1990 (0.2071)  classification_loss: 0.1089 (0.1099)  time: 0.1950  data: 0.0002  max mem: 5511
[10:47:34.890710] Epoch: [76]  [660/781]  eta: 0:00:23  lr: 0.000036  training_loss: 0.3193 (0.3169)  mae_loss: 0.2092 (0.2070)  classification_loss: 0.1078 (0.1099)  time: 0.1951  data: 0.0002  max mem: 5511
[10:47:38.801690] Epoch: [76]  [680/781]  eta: 0:00:19  lr: 0.000036  training_loss: 0.2982 (0.3167)  mae_loss: 0.1876 (0.2068)  classification_loss: 0.1092 (0.1099)  time: 0.1955  data: 0.0002  max mem: 5511
[10:47:42.696553] Epoch: [76]  [700/781]  eta: 0:00:15  lr: 0.000036  training_loss: 0.3186 (0.3168)  mae_loss: 0.2061 (0.2070)  classification_loss: 0.1079 (0.1099)  time: 0.1947  data: 0.0002  max mem: 5511
[10:47:46.616851] Epoch: [76]  [720/781]  eta: 0:00:12  lr: 0.000036  training_loss: 0.3143 (0.3167)  mae_loss: 0.2121 (0.2068)  classification_loss: 0.1107 (0.1099)  time: 0.1959  data: 0.0002  max mem: 5511
[10:47:50.523233] Epoch: [76]  [740/781]  eta: 0:00:08  lr: 0.000035  training_loss: 0.3062 (0.3165)  mae_loss: 0.1953 (0.2066)  classification_loss: 0.1109 (0.1099)  time: 0.1952  data: 0.0002  max mem: 5511
[10:47:54.416078] Epoch: [76]  [760/781]  eta: 0:00:04  lr: 0.000035  training_loss: 0.3045 (0.3163)  mae_loss: 0.1962 (0.2064)  classification_loss: 0.1074 (0.1099)  time: 0.1945  data: 0.0003  max mem: 5511
[10:47:58.320554] Epoch: [76]  [780/781]  eta: 0:00:00  lr: 0.000035  training_loss: 0.3161 (0.3164)  mae_loss: 0.1980 (0.2065)  classification_loss: 0.1078 (0.1099)  time: 0.1951  data: 0.0002  max mem: 5511
[10:47:58.476279] Epoch: [76] Total time: 0:02:34 (0.1972 s / it)
[10:47:58.476744] Averaged stats: lr: 0.000035  training_loss: 0.3161 (0.3164)  mae_loss: 0.1980 (0.2065)  classification_loss: 0.1078 (0.1099)
[10:47:59.156954] Test:  [  0/157]  eta: 0:01:46  testing_loss: 0.4687 (0.4687)  acc1: 89.0625 (89.0625)  acc5: 98.4375 (98.4375)  time: 0.6762  data: 0.6452  max mem: 5511
[10:47:59.439944] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.3931 (0.4024)  acc1: 87.5000 (88.4943)  acc5: 100.0000 (99.7159)  time: 0.0870  data: 0.0588  max mem: 5511
[10:47:59.722161] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.3725 (0.3897)  acc1: 87.5000 (89.0625)  acc5: 100.0000 (99.7024)  time: 0.0281  data: 0.0002  max mem: 5511
[10:48:00.008598] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.3630 (0.3989)  acc1: 87.5000 (88.9113)  acc5: 100.0000 (99.3952)  time: 0.0283  data: 0.0002  max mem: 5511
[10:48:00.294935] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4336 (0.4130)  acc1: 87.5000 (88.4909)  acc5: 100.0000 (99.3140)  time: 0.0285  data: 0.0002  max mem: 5511
[10:48:00.579958] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.3948 (0.4002)  acc1: 89.0625 (88.8480)  acc5: 100.0000 (99.3566)  time: 0.0285  data: 0.0003  max mem: 5511
[10:48:00.865101] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.3340 (0.3984)  acc1: 90.6250 (88.8832)  acc5: 100.0000 (99.3852)  time: 0.0284  data: 0.0003  max mem: 5511
[10:48:01.149099] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.3582 (0.3920)  acc1: 90.6250 (89.1945)  acc5: 100.0000 (99.4498)  time: 0.0283  data: 0.0002  max mem: 5511
[10:48:01.437670] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.3722 (0.3970)  acc1: 89.0625 (89.0046)  acc5: 100.0000 (99.4599)  time: 0.0285  data: 0.0002  max mem: 5511
[10:48:01.720324] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4098 (0.3946)  acc1: 89.0625 (89.1312)  acc5: 100.0000 (99.4677)  time: 0.0284  data: 0.0002  max mem: 5511
[10:48:02.007662] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4243 (0.4002)  acc1: 87.5000 (88.8304)  acc5: 100.0000 (99.4740)  time: 0.0283  data: 0.0002  max mem: 5511
[10:48:02.292490] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4243 (0.3999)  acc1: 87.5000 (88.9077)  acc5: 100.0000 (99.4932)  time: 0.0284  data: 0.0002  max mem: 5511
[10:48:02.574436] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.3563 (0.3973)  acc1: 89.0625 (88.9463)  acc5: 100.0000 (99.4964)  time: 0.0282  data: 0.0002  max mem: 5511
[10:48:02.857260] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.3813 (0.3988)  acc1: 89.0625 (88.8120)  acc5: 100.0000 (99.4752)  time: 0.0280  data: 0.0002  max mem: 5511
[10:48:03.139570] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3396 (0.3959)  acc1: 89.0625 (88.8852)  acc5: 100.0000 (99.5013)  time: 0.0280  data: 0.0001  max mem: 5511
[10:48:03.418197] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.3526 (0.3961)  acc1: 89.0625 (88.8555)  acc5: 100.0000 (99.4826)  time: 0.0279  data: 0.0001  max mem: 5511
[10:48:03.573963] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.3713 (0.3953)  acc1: 89.0625 (88.8500)  acc5: 100.0000 (99.4900)  time: 0.0271  data: 0.0001  max mem: 5511
[10:48:03.718770] Test: Total time: 0:00:05 (0.0334 s / it)
[10:48:03.719210] * Acc@1 88.850 Acc@5 99.490 loss 0.395
[10:48:03.719636] Accuracy of the network on the 10000 test images: 88.8%
[10:48:03.719836] Max accuracy: 88.85%
[10:48:04.051312] log_dir: ./output_dir
[10:48:04.951370] Epoch: [77]  [  0/781]  eta: 0:11:41  lr: 0.000035  training_loss: 0.2942 (0.2942)  mae_loss: 0.1970 (0.1970)  classification_loss: 0.0972 (0.0972)  time: 0.8980  data: 0.6699  max mem: 5511
[10:48:08.875701] Epoch: [77]  [ 20/781]  eta: 0:02:54  lr: 0.000035  training_loss: 0.3062 (0.3119)  mae_loss: 0.2031 (0.2041)  classification_loss: 0.1071 (0.1078)  time: 0.1961  data: 0.0002  max mem: 5511
[10:48:12.772099] Epoch: [77]  [ 40/781]  eta: 0:02:37  lr: 0.000035  training_loss: 0.3200 (0.3155)  mae_loss: 0.2116 (0.2064)  classification_loss: 0.1110 (0.1091)  time: 0.1947  data: 0.0002  max mem: 5511
[10:48:16.684899] Epoch: [77]  [ 60/781]  eta: 0:02:29  lr: 0.000035  training_loss: 0.3199 (0.3175)  mae_loss: 0.2133 (0.2080)  classification_loss: 0.1101 (0.1095)  time: 0.1956  data: 0.0002  max mem: 5511
[10:48:20.608531] Epoch: [77]  [ 80/781]  eta: 0:02:23  lr: 0.000035  training_loss: 0.3208 (0.3183)  mae_loss: 0.2190 (0.2087)  classification_loss: 0.1084 (0.1095)  time: 0.1961  data: 0.0002  max mem: 5511
[10:48:24.505794] Epoch: [77]  [100/781]  eta: 0:02:17  lr: 0.000035  training_loss: 0.3036 (0.3165)  mae_loss: 0.1942 (0.2071)  classification_loss: 0.1073 (0.1095)  time: 0.1947  data: 0.0003  max mem: 5511
[10:48:28.415876] Epoch: [77]  [120/781]  eta: 0:02:13  lr: 0.000035  training_loss: 0.3017 (0.3164)  mae_loss: 0.1966 (0.2067)  classification_loss: 0.1063 (0.1098)  time: 0.1954  data: 0.0002  max mem: 5511
[10:48:32.350049] Epoch: [77]  [140/781]  eta: 0:02:08  lr: 0.000035  training_loss: 0.3089 (0.3157)  mae_loss: 0.2004 (0.2062)  classification_loss: 0.1082 (0.1095)  time: 0.1966  data: 0.0002  max mem: 5511
[10:48:36.329906] Epoch: [77]  [160/781]  eta: 0:02:04  lr: 0.000035  training_loss: 0.3089 (0.3154)  mae_loss: 0.1991 (0.2055)  classification_loss: 0.1104 (0.1098)  time: 0.1989  data: 0.0007  max mem: 5511
[10:48:40.246275] Epoch: [77]  [180/781]  eta: 0:02:00  lr: 0.000035  training_loss: 0.3135 (0.3155)  mae_loss: 0.2032 (0.2056)  classification_loss: 0.1123 (0.1099)  time: 0.1957  data: 0.0002  max mem: 5511
[10:48:44.174290] Epoch: [77]  [200/781]  eta: 0:01:55  lr: 0.000035  training_loss: 0.3201 (0.3157)  mae_loss: 0.2134 (0.2061)  classification_loss: 0.1049 (0.1096)  time: 0.1963  data: 0.0002  max mem: 5511
[10:48:48.084859] Epoch: [77]  [220/781]  eta: 0:01:51  lr: 0.000035  training_loss: 0.3018 (0.3150)  mae_loss: 0.1906 (0.2055)  classification_loss: 0.1092 (0.1095)  time: 0.1955  data: 0.0002  max mem: 5511
[10:48:52.014513] Epoch: [77]  [240/781]  eta: 0:01:47  lr: 0.000034  training_loss: 0.3061 (0.3148)  mae_loss: 0.1953 (0.2054)  classification_loss: 0.1081 (0.1094)  time: 0.1964  data: 0.0002  max mem: 5511
[10:48:55.934861] Epoch: [77]  [260/781]  eta: 0:01:43  lr: 0.000034  training_loss: 0.3221 (0.3153)  mae_loss: 0.2074 (0.2056)  classification_loss: 0.1110 (0.1096)  time: 0.1959  data: 0.0002  max mem: 5511
[10:48:59.852353] Epoch: [77]  [280/781]  eta: 0:01:39  lr: 0.000034  training_loss: 0.3144 (0.3156)  mae_loss: 0.2074 (0.2062)  classification_loss: 0.1051 (0.1094)  time: 0.1958  data: 0.0002  max mem: 5511
[10:49:03.775647] Epoch: [77]  [300/781]  eta: 0:01:35  lr: 0.000034  training_loss: 0.3144 (0.3162)  mae_loss: 0.2037 (0.2065)  classification_loss: 0.1108 (0.1097)  time: 0.1961  data: 0.0003  max mem: 5511
[10:49:07.728348] Epoch: [77]  [320/781]  eta: 0:01:31  lr: 0.000034  training_loss: 0.3063 (0.3157)  mae_loss: 0.1985 (0.2062)  classification_loss: 0.1062 (0.1095)  time: 0.1976  data: 0.0002  max mem: 5511
[10:49:11.647566] Epoch: [77]  [340/781]  eta: 0:01:27  lr: 0.000034  training_loss: 0.3281 (0.3164)  mae_loss: 0.2134 (0.2070)  classification_loss: 0.1059 (0.1094)  time: 0.1959  data: 0.0002  max mem: 5511
[10:49:15.574633] Epoch: [77]  [360/781]  eta: 0:01:23  lr: 0.000034  training_loss: 0.3132 (0.3163)  mae_loss: 0.2050 (0.2068)  classification_loss: 0.1104 (0.1095)  time: 0.1963  data: 0.0003  max mem: 5511
[10:49:19.468312] Epoch: [77]  [380/781]  eta: 0:01:19  lr: 0.000034  training_loss: 0.3146 (0.3164)  mae_loss: 0.1990 (0.2068)  classification_loss: 0.1120 (0.1096)  time: 0.1946  data: 0.0002  max mem: 5511
[10:49:23.383331] Epoch: [77]  [400/781]  eta: 0:01:15  lr: 0.000034  training_loss: 0.3184 (0.3163)  mae_loss: 0.2016 (0.2067)  classification_loss: 0.1089 (0.1096)  time: 0.1957  data: 0.0003  max mem: 5511
[10:49:27.296157] Epoch: [77]  [420/781]  eta: 0:01:11  lr: 0.000034  training_loss: 0.3024 (0.3161)  mae_loss: 0.1958 (0.2065)  classification_loss: 0.1074 (0.1096)  time: 0.1956  data: 0.0002  max mem: 5511
[10:49:31.204376] Epoch: [77]  [440/781]  eta: 0:01:07  lr: 0.000034  training_loss: 0.3194 (0.3161)  mae_loss: 0.2132 (0.2068)  classification_loss: 0.1034 (0.1093)  time: 0.1953  data: 0.0003  max mem: 5511
[10:49:35.098623] Epoch: [77]  [460/781]  eta: 0:01:03  lr: 0.000034  training_loss: 0.3198 (0.3161)  mae_loss: 0.2039 (0.2068)  classification_loss: 0.1067 (0.1093)  time: 0.1946  data: 0.0002  max mem: 5511
[10:49:39.001078] Epoch: [77]  [480/781]  eta: 0:00:59  lr: 0.000034  training_loss: 0.3150 (0.3162)  mae_loss: 0.2104 (0.2070)  classification_loss: 0.1074 (0.1092)  time: 0.1950  data: 0.0004  max mem: 5511

[10:49:42.931466] Epoch: [77]  [500/781]  eta: 0:00:55  lr: 0.000034  training_loss: 0.3333 (0.3167)  mae_loss: 0.2107 (0.2073)  classification_loss: 0.1131 (0.1094)  time: 0.1965  data: 0.0002  max mem: 5511
[10:49:46.923413] Epoch: [77]  [520/781]  eta: 0:00:51  lr: 0.000033  training_loss: 0.3156 (0.3167)  mae_loss: 0.2054 (0.2073)  classification_loss: 0.1108 (0.1094)  time: 0.1995  data: 0.0003  max mem: 5511
[10:49:50.827488] Epoch: [77]  [540/781]  eta: 0:00:47  lr: 0.000033  training_loss: 0.3140 (0.3168)  mae_loss: 0.2042 (0.2073)  classification_loss: 0.1115 (0.1095)  time: 0.1951  data: 0.0002  max mem: 5511
[10:49:54.739692] Epoch: [77]  [560/781]  eta: 0:00:43  lr: 0.000033  training_loss: 0.3053 (0.3166)  mae_loss: 0.2079 (0.2072)  classification_loss: 0.1065 (0.1094)  time: 0.1955  data: 0.0002  max mem: 5511
[10:49:58.633592] Epoch: [77]  [580/781]  eta: 0:00:39  lr: 0.000033  training_loss: 0.3185 (0.3167)  mae_loss: 0.2013 (0.2073)  classification_loss: 0.1087 (0.1094)  time: 0.1946  data: 0.0002  max mem: 5511
[10:50:02.524882] Epoch: [77]  [600/781]  eta: 0:00:35  lr: 0.000033  training_loss: 0.3035 (0.3164)  mae_loss: 0.1994 (0.2071)  classification_loss: 0.1042 (0.1093)  time: 0.1945  data: 0.0002  max mem: 5511
[10:50:06.488490] Epoch: [77]  [620/781]  eta: 0:00:31  lr: 0.000033  training_loss: 0.3192 (0.3166)  mae_loss: 0.2076 (0.2073)  classification_loss: 0.1075 (0.1093)  time: 0.1981  data: 0.0003  max mem: 5511
[10:50:10.388367] Epoch: [77]  [640/781]  eta: 0:00:27  lr: 0.000033  training_loss: 0.3044 (0.3165)  mae_loss: 0.2043 (0.2073)  classification_loss: 0.1047 (0.1091)  time: 0.1949  data: 0.0002  max mem: 5511
[10:50:14.307940] Epoch: [77]  [660/781]  eta: 0:00:23  lr: 0.000033  training_loss: 0.2983 (0.3162)  mae_loss: 0.1850 (0.2070)  classification_loss: 0.1105 (0.1093)  time: 0.1958  data: 0.0003  max mem: 5511
[10:50:18.229984] Epoch: [77]  [680/781]  eta: 0:00:19  lr: 0.000033  training_loss: 0.3072 (0.3161)  mae_loss: 0.1966 (0.2068)  classification_loss: 0.1085 (0.1093)  time: 0.1960  data: 0.0002  max mem: 5511
[10:50:22.138419] Epoch: [77]  [700/781]  eta: 0:00:15  lr: 0.000033  training_loss: 0.3168 (0.3163)  mae_loss: 0.2044 (0.2070)  classification_loss: 0.1092 (0.1093)  time: 0.1953  data: 0.0002  max mem: 5511
[10:50:26.038913] Epoch: [77]  [720/781]  eta: 0:00:12  lr: 0.000033  training_loss: 0.3050 (0.3163)  mae_loss: 0.1865 (0.2069)  classification_loss: 0.1121 (0.1094)  time: 0.1949  data: 0.0002  max mem: 5511
[10:50:29.949323] Epoch: [77]  [740/781]  eta: 0:00:08  lr: 0.000033  training_loss: 0.3210 (0.3166)  mae_loss: 0.2174 (0.2072)  classification_loss: 0.1084 (0.1094)  time: 0.1954  data: 0.0002  max mem: 5511
[10:50:33.904841] Epoch: [77]  [760/781]  eta: 0:00:04  lr: 0.000033  training_loss: 0.3198 (0.3168)  mae_loss: 0.2062 (0.2074)  classification_loss: 0.1092 (0.1094)  time: 0.1976  data: 0.0003  max mem: 5511
[10:50:37.790889] Epoch: [77]  [780/781]  eta: 0:00:00  lr: 0.000033  training_loss: 0.3040 (0.3165)  mae_loss: 0.1986 (0.2072)  classification_loss: 0.1056 (0.1093)  time: 0.1942  data: 0.0002  max mem: 5511
[10:50:37.958607] Epoch: [77] Total time: 0:02:33 (0.1971 s / it)
[10:50:37.959038] Averaged stats: lr: 0.000033  training_loss: 0.3040 (0.3165)  mae_loss: 0.1986 (0.2072)  classification_loss: 0.1056 (0.1093)
[10:50:38.666375] Test:  [  0/157]  eta: 0:01:50  testing_loss: 0.5112 (0.5112)  acc1: 84.3750 (84.3750)  acc5: 98.4375 (98.4375)  time: 0.7029  data: 0.6719  max mem: 5511
[10:50:38.951936] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.3938 (0.4054)  acc1: 89.0625 (88.3523)  acc5: 100.0000 (99.7159)  time: 0.0897  data: 0.0613  max mem: 5511
[10:50:39.236588] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.3729 (0.3978)  acc1: 89.0625 (88.9137)  acc5: 100.0000 (99.6280)  time: 0.0283  data: 0.0002  max mem: 5511
[10:50:39.519623] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.3610 (0.4047)  acc1: 89.0625 (88.5585)  acc5: 100.0000 (99.4456)  time: 0.0282  data: 0.0002  max mem: 5511
[10:50:39.809620] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4260 (0.4163)  acc1: 87.5000 (88.2622)  acc5: 100.0000 (99.3521)  time: 0.0285  data: 0.0002  max mem: 5511
[10:50:40.093505] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.3924 (0.4038)  acc1: 89.0625 (88.6029)  acc5: 100.0000 (99.3873)  time: 0.0285  data: 0.0002  max mem: 5511
[10:50:40.376803] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.3272 (0.4015)  acc1: 90.6250 (88.7039)  acc5: 100.0000 (99.4365)  time: 0.0282  data: 0.0002  max mem: 5511
[10:50:40.663877] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.3738 (0.3950)  acc1: 90.6250 (89.0185)  acc5: 100.0000 (99.4938)  time: 0.0284  data: 0.0002  max mem: 5511
[10:50:40.948326] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.3791 (0.4011)  acc1: 89.0625 (88.7346)  acc5: 100.0000 (99.4599)  time: 0.0285  data: 0.0002  max mem: 5511
[10:50:41.233165] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4171 (0.3980)  acc1: 87.5000 (88.9251)  acc5: 100.0000 (99.5021)  time: 0.0283  data: 0.0002  max mem: 5511
[10:50:41.516394] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4240 (0.4033)  acc1: 89.0625 (88.6757)  acc5: 100.0000 (99.5050)  time: 0.0283  data: 0.0002  max mem: 5511
[10:50:41.800506] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4202 (0.4024)  acc1: 87.5000 (88.7247)  acc5: 100.0000 (99.5214)  time: 0.0282  data: 0.0002  max mem: 5511
[10:50:42.088713] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.3673 (0.4002)  acc1: 89.0625 (88.8171)  acc5: 100.0000 (99.5351)  time: 0.0285  data: 0.0002  max mem: 5511
[10:50:42.372802] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.3754 (0.4021)  acc1: 89.0625 (88.6927)  acc5: 100.0000 (99.5229)  time: 0.0285  data: 0.0002  max mem: 5511
[10:50:42.654121] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3388 (0.3988)  acc1: 89.0625 (88.7411)  acc5: 100.0000 (99.5457)  time: 0.0281  data: 0.0002  max mem: 5511
[10:50:42.932346] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.3661 (0.3985)  acc1: 87.5000 (88.6900)  acc5: 100.0000 (99.5240)  time: 0.0279  data: 0.0001  max mem: 5511
[10:50:43.081601] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.3661 (0.3980)  acc1: 87.5000 (88.6300)  acc5: 100.0000 (99.5400)  time: 0.0268  data: 0.0001  max mem: 5511
[10:50:43.223188] Test: Total time: 0:00:05 (0.0335 s / it)
[10:50:43.224497] * Acc@1 88.630 Acc@5 99.540 loss 0.398
[10:50:43.224837] Accuracy of the network on the 10000 test images: 88.6%
[10:50:43.225026] Max accuracy: 88.85%
[10:50:43.519332] log_dir: ./output_dir
[10:50:44.434613] Epoch: [78]  [  0/781]  eta: 0:11:53  lr: 0.000033  training_loss: 0.3201 (0.3201)  mae_loss: 0.2107 (0.2107)  classification_loss: 0.1094 (0.1094)  time: 0.9134  data: 0.6820  max mem: 5511
[10:50:48.344197] Epoch: [78]  [ 20/781]  eta: 0:02:54  lr: 0.000032  training_loss: 0.3197 (0.3172)  mae_loss: 0.2099 (0.2101)  classification_loss: 0.1040 (0.1071)  time: 0.1953  data: 0.0003  max mem: 5511
[10:50:52.253638] Epoch: [78]  [ 40/781]  eta: 0:02:37  lr: 0.000032  training_loss: 0.3173 (0.3177)  mae_loss: 0.2110 (0.2104)  classification_loss: 0.1065 (0.1073)  time: 0.1954  data: 0.0002  max mem: 5511
[10:50:56.151327] Epoch: [78]  [ 60/781]  eta: 0:02:29  lr: 0.000032  training_loss: 0.3262 (0.3211)  mae_loss: 0.2160 (0.2129)  classification_loss: 0.1079 (0.1082)  time: 0.1948  data: 0.0002  max mem: 5511
[10:51:00.056134] Epoch: [78]  [ 80/781]  eta: 0:02:23  lr: 0.000032  training_loss: 0.3092 (0.3196)  mae_loss: 0.1991 (0.2110)  classification_loss: 0.1113 (0.1086)  time: 0.1952  data: 0.0004  max mem: 5511
[10:51:03.966354] Epoch: [78]  [100/781]  eta: 0:02:17  lr: 0.000032  training_loss: 0.3101 (0.3179)  mae_loss: 0.1987 (0.2089)  classification_loss: 0.1077 (0.1091)  time: 0.1954  data: 0.0003  max mem: 5511
[10:51:07.977378] Epoch: [78]  [120/781]  eta: 0:02:13  lr: 0.000032  training_loss: 0.3124 (0.3163)  mae_loss: 0.1985 (0.2078)  classification_loss: 0.1019 (0.1085)  time: 0.2005  data: 0.0003  max mem: 5511
[10:51:11.889300] Epoch: [78]  [140/781]  eta: 0:02:08  lr: 0.000032  training_loss: 0.3067 (0.3167)  mae_loss: 0.2092 (0.2088)  classification_loss: 0.1054 (0.1079)  time: 0.1955  data: 0.0002  max mem: 5511
[10:51:15.803826] Epoch: [78]  [160/781]  eta: 0:02:04  lr: 0.000032  training_loss: 0.3138 (0.3165)  mae_loss: 0.2111 (0.2087)  classification_loss: 0.1056 (0.1078)  time: 0.1956  data: 0.0003  max mem: 5511
[10:51:19.769515] Epoch: [78]  [180/781]  eta: 0:02:00  lr: 0.000032  training_loss: 0.3037 (0.3163)  mae_loss: 0.2001 (0.2084)  classification_loss: 0.1057 (0.1079)  time: 0.1982  data: 0.0002  max mem: 5511
[10:51:23.738694] Epoch: [78]  [200/781]  eta: 0:01:56  lr: 0.000032  training_loss: 0.3137 (0.3162)  mae_loss: 0.2033 (0.2081)  classification_loss: 0.1082 (0.1081)  time: 0.1984  data: 0.0002  max mem: 5511
[10:51:27.631627] Epoch: [78]  [220/781]  eta: 0:01:51  lr: 0.000032  training_loss: 0.3041 (0.3155)  mae_loss: 0.2005 (0.2072)  classification_loss: 0.1082 (0.1082)  time: 0.1945  data: 0.0002  max mem: 5511
[10:51:31.539731] Epoch: [78]  [240/781]  eta: 0:01:47  lr: 0.000032  training_loss: 0.3234 (0.3161)  mae_loss: 0.2190 (0.2078)  classification_loss: 0.1063 (0.1083)  time: 0.1953  data: 0.0003  max mem: 5511
[10:51:35.444670] Epoch: [78]  [260/781]  eta: 0:01:43  lr: 0.000032  training_loss: 0.3198 (0.3162)  mae_loss: 0.2131 (0.2078)  classification_loss: 0.1065 (0.1084)  time: 0.1952  data: 0.0002  max mem: 5511
[10:51:39.346966] Epoch: [78]  [280/781]  eta: 0:01:39  lr: 0.000032  training_loss: 0.3272 (0.3169)  mae_loss: 0.2150 (0.2085)  classification_loss: 0.1048 (0.1085)  time: 0.1950  data: 0.0002  max mem: 5511
[10:51:43.253157] Epoch: [78]  [300/781]  eta: 0:01:35  lr: 0.000031  training_loss: 0.3101 (0.3167)  mae_loss: 0.2031 (0.2082)  classification_loss: 0.1072 (0.1085)  time: 0.1952  data: 0.0003  max mem: 5511
[10:51:47.185762] Epoch: [78]  [320/781]  eta: 0:01:31  lr: 0.000031  training_loss: 0.3178 (0.3169)  mae_loss: 0.2034 (0.2083)  classification_loss: 0.1079 (0.1086)  time: 0.1966  data: 0.0003  max mem: 5511
[10:51:51.083521] Epoch: [78]  [340/781]  eta: 0:01:27  lr: 0.000031  training_loss: 0.2991 (0.3164)  mae_loss: 0.1942 (0.2079)  classification_loss: 0.1027 (0.1084)  time: 0.1948  data: 0.0002  max mem: 5511
[10:51:54.998168] Epoch: [78]  [360/781]  eta: 0:01:23  lr: 0.000031  training_loss: 0.3270 (0.3163)  mae_loss: 0.2068 (0.2077)  classification_loss: 0.1137 (0.1086)  time: 0.1957  data: 0.0002  max mem: 5511
[10:51:58.929049] Epoch: [78]  [380/781]  eta: 0:01:19  lr: 0.000031  training_loss: 0.3197 (0.3168)  mae_loss: 0.2038 (0.2081)  classification_loss: 0.1089 (0.1087)  time: 0.1964  data: 0.0002  max mem: 5511
[10:52:02.833001] Epoch: [78]  [400/781]  eta: 0:01:15  lr: 0.000031  training_loss: 0.3280 (0.3167)  mae_loss: 0.2105 (0.2081)  classification_loss: 0.1057 (0.1087)  time: 0.1951  data: 0.0002  max mem: 5511
[10:52:06.721358] Epoch: [78]  [420/781]  eta: 0:01:11  lr: 0.000031  training_loss: 0.3067 (0.3166)  mae_loss: 0.2047 (0.2080)  classification_loss: 0.1076 (0.1086)  time: 0.1943  data: 0.0002  max mem: 5511
[10:52:10.614520] Epoch: [78]  [440/781]  eta: 0:01:07  lr: 0.000031  training_loss: 0.3217 (0.3167)  mae_loss: 0.2018 (0.2080)  classification_loss: 0.1117 (0.1088)  time: 0.1946  data: 0.0002  max mem: 5511
[10:52:14.496566] Epoch: [78]  [460/781]  eta: 0:01:03  lr: 0.000031  training_loss: 0.2986 (0.3162)  mae_loss: 0.1995 (0.2075)  classification_loss: 0.1065 (0.1087)  time: 0.1940  data: 0.0002  max mem: 5511
[10:52:18.416497] Epoch: [78]  [480/781]  eta: 0:00:59  lr: 0.000031  training_loss: 0.3114 (0.3162)  mae_loss: 0.1979 (0.2075)  classification_loss: 0.1101 (0.1088)  time: 0.1959  data: 0.0002  max mem: 5511
[10:52:22.320796] Epoch: [78]  [500/781]  eta: 0:00:55  lr: 0.000031  training_loss: 0.3126 (0.3163)  mae_loss: 0.2036 (0.2075)  classification_loss: 0.1093 (0.1088)  time: 0.1951  data: 0.0002  max mem: 5511
[10:52:26.249946] Epoch: [78]  [520/781]  eta: 0:00:51  lr: 0.000031  training_loss: 0.3280 (0.3165)  mae_loss: 0.2169 (0.2078)  classification_loss: 0.1072 (0.1088)  time: 0.1964  data: 0.0002  max mem: 5511
[10:52:30.192136] Epoch: [78]  [540/781]  eta: 0:00:47  lr: 0.000031  training_loss: 0.3060 (0.3163)  mae_loss: 0.1917 (0.2075)  classification_loss: 0.1092 (0.1088)  time: 0.1970  data: 0.0002  max mem: 5511
[10:52:34.097308] Epoch: [78]  [560/781]  eta: 0:00:43  lr: 0.000031  training_loss: 0.3002 (0.3159)  mae_loss: 0.1934 (0.2072)  classification_loss: 0.1080 (0.1087)  time: 0.1952  data: 0.0002  max mem: 5511
[10:52:38.043669] Epoch: [78]  [580/781]  eta: 0:00:39  lr: 0.000031  training_loss: 0.3261 (0.3161)  mae_loss: 0.2044 (0.2073)  classification_loss: 0.1096 (0.1088)  time: 0.1972  data: 0.0002  max mem: 5511
[10:52:41.946456] Epoch: [78]  [600/781]  eta: 0:00:35  lr: 0.000030  training_loss: 0.3010 (0.3159)  mae_loss: 0.2034 (0.2072)  classification_loss: 0.1044 (0.1088)  time: 0.1951  data: 0.0002  max mem: 5511
[10:52:45.888635] Epoch: [78]  [620/781]  eta: 0:00:31  lr: 0.000030  training_loss: 0.3146 (0.3162)  mae_loss: 0.2063 (0.2075)  classification_loss: 0.1069 (0.1087)  time: 0.1970  data: 0.0002  max mem: 5511
[10:52:49.791211] Epoch: [78]  [640/781]  eta: 0:00:27  lr: 0.000030  training_loss: 0.3119 (0.3161)  mae_loss: 0.2006 (0.2074)  classification_loss: 0.1092 (0.1087)  time: 0.1951  data: 0.0002  max mem: 5511
[10:52:53.724735] Epoch: [78]  [660/781]  eta: 0:00:23  lr: 0.000030  training_loss: 0.3089 (0.3160)  mae_loss: 0.1996 (0.2073)  classification_loss: 0.1048 (0.1087)  time: 0.1966  data: 0.0002  max mem: 5511
[10:52:57.685034] Epoch: [78]  [680/781]  eta: 0:00:19  lr: 0.000030  training_loss: 0.3146 (0.3162)  mae_loss: 0.2045 (0.2074)  classification_loss: 0.1128 (0.1088)  time: 0.1979  data: 0.0002  max mem: 5511
[10:53:01.634736] Epoch: [78]  [700/781]  eta: 0:00:15  lr: 0.000030  training_loss: 0.3155 (0.3163)  mae_loss: 0.2099 (0.2075)  classification_loss: 0.1065 (0.1088)  time: 0.1974  data: 0.0002  max mem: 5511
[10:53:05.530061] Epoch: [78]  [720/781]  eta: 0:00:12  lr: 0.000030  training_loss: 0.3011 (0.3159)  mae_loss: 0.1966 (0.2071)  classification_loss: 0.1062 (0.1088)  time: 0.1947  data: 0.0002  max mem: 5511
[10:53:09.431198] Epoch: [78]  [740/781]  eta: 0:00:08  lr: 0.000030  training_loss: 0.3052 (0.3158)  mae_loss: 0.2036 (0.2071)  classification_loss: 0.1039 (0.1087)  time: 0.1950  data: 0.0003  max mem: 5511
[10:53:13.325738] Epoch: [78]  [760/781]  eta: 0:00:04  lr: 0.000030  training_loss: 0.3032 (0.3157)  mae_loss: 0.1919 (0.2069)  classification_loss: 0.1111 (0.1087)  time: 0.1946  data: 0.0002  max mem: 5511
[10:53:17.217547] Epoch: [78]  [780/781]  eta: 0:00:00  lr: 0.000030  training_loss: 0.3026 (0.3155)  mae_loss: 0.1948 (0.2067)  classification_loss: 0.1063 (0.1087)  time: 0.1945  data: 0.0003  max mem: 5511
[10:53:17.394140] Epoch: [78] Total time: 0:02:33 (0.1970 s / it)
[10:53:17.395314] Averaged stats: lr: 0.000030  training_loss: 0.3026 (0.3155)  mae_loss: 0.1948 (0.2067)  classification_loss: 0.1063 (0.1087)
[10:53:17.960087] Test:  [  0/157]  eta: 0:01:27  testing_loss: 0.4974 (0.4974)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.5559  data: 0.5259  max mem: 5511
[10:53:18.246934] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.3950 (0.4084)  acc1: 87.5000 (88.2102)  acc5: 100.0000 (99.4318)  time: 0.0765  data: 0.0480  max mem: 5511
[10:53:18.536218] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.3570 (0.3911)  acc1: 89.0625 (89.1369)  acc5: 100.0000 (99.5536)  time: 0.0287  data: 0.0002  max mem: 5511
[10:53:18.822119] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.3449 (0.3971)  acc1: 89.0625 (88.8609)  acc5: 100.0000 (99.4456)  time: 0.0286  data: 0.0002  max mem: 5511
[10:53:19.112561] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.4027 (0.4106)  acc1: 89.0625 (88.4527)  acc5: 100.0000 (99.3140)  time: 0.0287  data: 0.0003  max mem: 5511
[10:53:19.402021] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.3789 (0.4001)  acc1: 89.0625 (88.8174)  acc5: 100.0000 (99.2953)  time: 0.0288  data: 0.0003  max mem: 5511
[10:53:19.688638] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.3414 (0.3994)  acc1: 89.0625 (88.8320)  acc5: 100.0000 (99.3084)  time: 0.0286  data: 0.0002  max mem: 5511
[10:53:19.973458] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.3757 (0.3944)  acc1: 89.0625 (89.0185)  acc5: 100.0000 (99.4058)  time: 0.0284  data: 0.0002  max mem: 5511
[10:53:20.256726] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.3943 (0.4015)  acc1: 89.0625 (88.7346)  acc5: 100.0000 (99.3827)  time: 0.0282  data: 0.0003  max mem: 5511
[10:53:20.544423] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4191 (0.3987)  acc1: 87.5000 (88.8736)  acc5: 100.0000 (99.3990)  time: 0.0284  data: 0.0002  max mem: 5511
[10:53:20.828622] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4227 (0.4052)  acc1: 87.5000 (88.5675)  acc5: 100.0000 (99.4276)  time: 0.0285  data: 0.0002  max mem: 5511
[10:53:21.112886] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4198 (0.4047)  acc1: 89.0625 (88.6684)  acc5: 100.0000 (99.4088)  time: 0.0283  data: 0.0002  max mem: 5511
[10:53:21.398449] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.3736 (0.4020)  acc1: 89.0625 (88.7913)  acc5: 100.0000 (99.4060)  time: 0.0283  data: 0.0002  max mem: 5511
[10:53:21.687288] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.3598 (0.4035)  acc1: 89.0625 (88.7166)  acc5: 100.0000 (99.4036)  time: 0.0286  data: 0.0003  max mem: 5511
[10:53:21.968926] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3519 (0.3993)  acc1: 89.0625 (88.8520)  acc5: 100.0000 (99.4348)  time: 0.0284  data: 0.0002  max mem: 5511
[10:53:22.250533] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.3633 (0.3990)  acc1: 89.0625 (88.8142)  acc5: 100.0000 (99.4309)  time: 0.0280  data: 0.0001  max mem: 5511
[10:53:22.401933] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.3779 (0.3980)  acc1: 89.0625 (88.8000)  acc5: 100.0000 (99.4400)  time: 0.0271  data: 0.0001  max mem: 5511
[10:53:22.544595] Test: Total time: 0:00:05 (0.0328 s / it)
[10:53:22.545066] * Acc@1 88.800 Acc@5 99.440 loss 0.398
[10:53:22.545482] Accuracy of the network on the 10000 test images: 88.8%
[10:53:22.545690] Max accuracy: 88.85%
[10:53:23.090036] log_dir: ./output_dir
[10:53:23.953187] Epoch: [79]  [  0/781]  eta: 0:11:12  lr: 0.000030  training_loss: 0.2947 (0.2947)  mae_loss: 0.2019 (0.2019)  classification_loss: 0.0928 (0.0928)  time: 0.8605  data: 0.6389  max mem: 5511
[10:53:27.861866] Epoch: [79]  [ 20/781]  eta: 0:02:52  lr: 0.000030  training_loss: 0.3030 (0.3113)  mae_loss: 0.2081 (0.2089)  classification_loss: 0.0994 (0.1025)  time: 0.1953  data: 0.0003  max mem: 5511
[10:53:31.773279] Epoch: [79]  [ 40/781]  eta: 0:02:36  lr: 0.000030  training_loss: 0.3164 (0.3145)  mae_loss: 0.2085 (0.2095)  classification_loss: 0.1064 (0.1049)  time: 0.1955  data: 0.0004  max mem: 5511
[10:53:35.689972] Epoch: [79]  [ 60/781]  eta: 0:02:28  lr: 0.000030  training_loss: 0.2967 (0.3127)  mae_loss: 0.1908 (0.2066)  classification_loss: 0.1108 (0.1061)  time: 0.1957  data: 0.0002  max mem: 5511
[10:53:39.678787] Epoch: [79]  [ 80/781]  eta: 0:02:23  lr: 0.000030  training_loss: 0.3165 (0.3135)  mae_loss: 0.2131 (0.2077)  classification_loss: 0.1030 (0.1058)  time: 0.1994  data: 0.0002  max mem: 5511
[10:53:43.598084] Epoch: [79]  [100/781]  eta: 0:02:18  lr: 0.000029  training_loss: 0.3165 (0.3141)  mae_loss: 0.2036 (0.2075)  classification_loss: 0.1124 (0.1067)  time: 0.1959  data: 0.0002  max mem: 5511
[10:53:47.533345] Epoch: [79]  [120/781]  eta: 0:02:13  lr: 0.000029  training_loss: 0.3056 (0.3124)  mae_loss: 0.1972 (0.2055)  classification_loss: 0.1065 (0.1069)  time: 0.1967  data: 0.0004  max mem: 5511
[10:53:51.440697] Epoch: [79]  [140/781]  eta: 0:02:08  lr: 0.000029  training_loss: 0.3198 (0.3130)  mae_loss: 0.2164 (0.2062)  classification_loss: 0.1039 (0.1068)  time: 0.1952  data: 0.0002  max mem: 5511
[10:53:55.349879] Epoch: [79]  [160/781]  eta: 0:02:04  lr: 0.000029  training_loss: 0.3105 (0.3123)  mae_loss: 0.2051 (0.2053)  classification_loss: 0.1055 (0.1070)  time: 0.1954  data: 0.0002  max mem: 5511
[10:53:59.275959] Epoch: [79]  [180/781]  eta: 0:02:00  lr: 0.000029  training_loss: 0.3130 (0.3126)  mae_loss: 0.2074 (0.2058)  classification_loss: 0.1057 (0.1068)  time: 0.1962  data: 0.0002  max mem: 5511
[10:54:03.178390] Epoch: [79]  [200/781]  eta: 0:01:55  lr: 0.000029  training_loss: 0.3102 (0.3126)  mae_loss: 0.2042 (0.2056)  classification_loss: 0.1099 (0.1070)  time: 0.1950  data: 0.0002  max mem: 5511
[10:54:07.070117] Epoch: [79]  [220/781]  eta: 0:01:51  lr: 0.000029  training_loss: 0.3146 (0.3128)  mae_loss: 0.1998 (0.2054)  classification_loss: 0.1083 (0.1074)  time: 0.1945  data: 0.0003  max mem: 5511
[10:54:10.989575] Epoch: [79]  [240/781]  eta: 0:01:47  lr: 0.000029  training_loss: 0.3071 (0.3130)  mae_loss: 0.2041 (0.2056)  classification_loss: 0.1044 (0.1075)  time: 0.1959  data: 0.0002  max mem: 5511
[10:54:14.918728] Epoch: [79]  [260/781]  eta: 0:01:43  lr: 0.000029  training_loss: 0.2994 (0.3131)  mae_loss: 0.1909 (0.2055)  classification_loss: 0.1076 (0.1076)  time: 0.1964  data: 0.0003  max mem: 5511
[10:54:18.840782] Epoch: [79]  [280/781]  eta: 0:01:39  lr: 0.000029  training_loss: 0.3028 (0.3130)  mae_loss: 0.2056 (0.2054)  classification_loss: 0.1051 (0.1076)  time: 0.1960  data: 0.0002  max mem: 5511
[10:54:22.734931] Epoch: [79]  [300/781]  eta: 0:01:35  lr: 0.000029  training_loss: 0.2995 (0.3124)  mae_loss: 0.1856 (0.2047)  classification_loss: 0.1082 (0.1077)  time: 0.1946  data: 0.0002  max mem: 5511
[10:54:26.653271] Epoch: [79]  [320/781]  eta: 0:01:31  lr: 0.000029  training_loss: 0.3270 (0.3135)  mae_loss: 0.2156 (0.2055)  classification_loss: 0.1097 (0.1079)  time: 0.1958  data: 0.0002  max mem: 5511
[10:54:30.568506] Epoch: [79]  [340/781]  eta: 0:01:27  lr: 0.000029  training_loss: 0.3095 (0.3135)  mae_loss: 0.2021 (0.2054)  classification_loss: 0.1083 (0.1081)  time: 0.1956  data: 0.0003  max mem: 5511
[10:54:34.478511] Epoch: [79]  [360/781]  eta: 0:01:23  lr: 0.000029  training_loss: 0.3137 (0.3137)  mae_loss: 0.2054 (0.2056)  classification_loss: 0.1072 (0.1081)  time: 0.1954  data: 0.0002  max mem: 5511
[10:54:38.377970] Epoch: [79]  [380/781]  eta: 0:01:19  lr: 0.000029  training_loss: 0.3161 (0.3138)  mae_loss: 0.2006 (0.2054)  classification_loss: 0.1120 (0.1084)  time: 0.1949  data: 0.0002  max mem: 5511
[10:54:42.276971] Epoch: [79]  [400/781]  eta: 0:01:15  lr: 0.000028  training_loss: 0.3173 (0.3142)  mae_loss: 0.2002 (0.2057)  classification_loss: 0.1114 (0.1085)  time: 0.1949  data: 0.0002  max mem: 5511
[10:54:46.209139] Epoch: [79]  [420/781]  eta: 0:01:11  lr: 0.000028  training_loss: 0.3184 (0.3144)  mae_loss: 0.2096 (0.2057)  classification_loss: 0.1110 (0.1087)  time: 0.1965  data: 0.0003  max mem: 5511
[10:54:50.115088] Epoch: [79]  [440/781]  eta: 0:01:07  lr: 0.000028  training_loss: 0.3220 (0.3149)  mae_loss: 0.2110 (0.2061)  classification_loss: 0.1088 (0.1088)  time: 0.1952  data: 0.0004  max mem: 5511
[10:54:54.070599] Epoch: [79]  [460/781]  eta: 0:01:03  lr: 0.000028  training_loss: 0.3071 (0.3145)  mae_loss: 0.1989 (0.2058)  classification_loss: 0.1035 (0.1086)  time: 0.1977  data: 0.0002  max mem: 5511
[10:54:58.042407] Epoch: [79]  [480/781]  eta: 0:00:59  lr: 0.000028  training_loss: 0.3107 (0.3146)  mae_loss: 0.1969 (0.2058)  classification_loss: 0.1110 (0.1088)  time: 0.1985  data: 0.0002  max mem: 5511
[10:55:01.963430] Epoch: [79]  [500/781]  eta: 0:00:55  lr: 0.000028  training_loss: 0.3210 (0.3149)  mae_loss: 0.2065 (0.2060)  classification_loss: 0.1122 (0.1089)  time: 0.1960  data: 0.0002  max mem: 5511
[10:55:05.858253] Epoch: [79]  [520/781]  eta: 0:00:51  lr: 0.000028  training_loss: 0.3120 (0.3147)  mae_loss: 0.1910 (0.2057)  classification_loss: 0.1106 (0.1090)  time: 0.1947  data: 0.0002  max mem: 5511
[10:55:09.757215] Epoch: [79]  [540/781]  eta: 0:00:47  lr: 0.000028  training_loss: 0.3109 (0.3147)  mae_loss: 0.1987 (0.2057)  classification_loss: 0.1057 (0.1089)  time: 0.1949  data: 0.0003  max mem: 5511
[10:55:13.665487] Epoch: [79]  [560/781]  eta: 0:00:43  lr: 0.000028  training_loss: 0.3220 (0.3149)  mae_loss: 0.2116 (0.2059)  classification_loss: 0.1060 (0.1089)  time: 0.1953  data: 0.0002  max mem: 5511
[10:55:17.608517] Epoch: [79]  [580/781]  eta: 0:00:39  lr: 0.000028  training_loss: 0.3070 (0.3149)  mae_loss: 0.1948 (0.2059)  classification_loss: 0.1075 (0.1089)  time: 0.1971  data: 0.0002  max mem: 5511
[10:55:21.513493] Epoch: [79]  [600/781]  eta: 0:00:35  lr: 0.000028  training_loss: 0.3100 (0.3147)  mae_loss: 0.2009 (0.2058)  classification_loss: 0.1074 (0.1089)  time: 0.1951  data: 0.0002  max mem: 5511
[10:55:25.420735] Epoch: [79]  [620/781]  eta: 0:00:31  lr: 0.000028  training_loss: 0.3031 (0.3147)  mae_loss: 0.1981 (0.2059)  classification_loss: 0.1027 (0.1087)  time: 0.1953  data: 0.0002  max mem: 5511
[10:55:29.317644] Epoch: [79]  [640/781]  eta: 0:00:27  lr: 0.000028  training_loss: 0.3179 (0.3147)  mae_loss: 0.2064 (0.2059)  classification_loss: 0.1082 (0.1087)  time: 0.1948  data: 0.0003  max mem: 5511
[10:55:33.232559] Epoch: [79]  [660/781]  eta: 0:00:23  lr: 0.000028  training_loss: 0.2970 (0.3146)  mae_loss: 0.2011 (0.2059)  classification_loss: 0.1033 (0.1086)  time: 0.1957  data: 0.0003  max mem: 5511
[10:55:37.152193] Epoch: [79]  [680/781]  eta: 0:00:19  lr: 0.000028  training_loss: 0.3127 (0.3146)  mae_loss: 0.2024 (0.2059)  classification_loss: 0.1118 (0.1087)  time: 0.1959  data: 0.0003  max mem: 5511
[10:55:41.133322] Epoch: [79]  [700/781]  eta: 0:00:15  lr: 0.000028  training_loss: 0.3033 (0.3145)  mae_loss: 0.2003 (0.2057)  classification_loss: 0.1076 (0.1088)  time: 0.1990  data: 0.0002  max mem: 5511
[10:55:45.058573] Epoch: [79]  [720/781]  eta: 0:00:12  lr: 0.000027  training_loss: 0.3106 (0.3144)  mae_loss: 0.1920 (0.2056)  classification_loss: 0.1064 (0.1088)  time: 0.1962  data: 0.0002  max mem: 5511
[10:55:48.969114] Epoch: [79]  [740/781]  eta: 0:00:08  lr: 0.000027  training_loss: 0.3182 (0.3145)  mae_loss: 0.2046 (0.2057)  classification_loss: 0.1090 (0.1088)  time: 0.1954  data: 0.0002  max mem: 5511
[10:55:52.878614] Epoch: [79]  [760/781]  eta: 0:00:04  lr: 0.000027  training_loss: 0.3005 (0.3143)  mae_loss: 0.1993 (0.2055)  classification_loss: 0.1064 (0.1088)  time: 0.1954  data: 0.0002  max mem: 5511
[10:55:56.765875] Epoch: [79]  [780/781]  eta: 0:00:00  lr: 0.000027  training_loss: 0.3094 (0.3143)  mae_loss: 0.2017 (0.2056)  classification_loss: 0.1055 (0.1087)  time: 0.1943  data: 0.0002  max mem: 5511
[10:55:56.912163] Epoch: [79] Total time: 0:02:33 (0.1970 s / it)
[10:55:56.913007] Averaged stats: lr: 0.000027  training_loss: 0.3094 (0.3143)  mae_loss: 0.2017 (0.2056)  classification_loss: 0.1055 (0.1087)
[10:55:57.602560] Test:  [  0/157]  eta: 0:01:47  testing_loss: 0.4722 (0.4722)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.6845  data: 0.6544  max mem: 5511
[10:55:57.889952] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.3823 (0.4036)  acc1: 87.5000 (87.6420)  acc5: 100.0000 (99.5739)  time: 0.0882  data: 0.0600  max mem: 5511
[10:55:58.171692] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.3762 (0.3879)  acc1: 87.5000 (88.6161)  acc5: 100.0000 (99.5536)  time: 0.0283  data: 0.0004  max mem: 5511
[10:55:58.453831] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.3397 (0.3944)  acc1: 90.6250 (88.6089)  acc5: 100.0000 (99.4456)  time: 0.0281  data: 0.0002  max mem: 5511
[10:55:58.738073] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4103 (0.4086)  acc1: 89.0625 (88.2241)  acc5: 100.0000 (99.2759)  time: 0.0282  data: 0.0002  max mem: 5511
[10:55:59.021908] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.3761 (0.3966)  acc1: 89.0625 (88.6949)  acc5: 100.0000 (99.2647)  time: 0.0283  data: 0.0002  max mem: 5511
[10:55:59.304186] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.3410 (0.3940)  acc1: 90.6250 (88.9344)  acc5: 100.0000 (99.2828)  time: 0.0282  data: 0.0002  max mem: 5511
[10:55:59.596541] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.3668 (0.3880)  acc1: 90.6250 (89.1505)  acc5: 100.0000 (99.3398)  time: 0.0286  data: 0.0002  max mem: 5511
[10:55:59.881506] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.3745 (0.3964)  acc1: 87.5000 (88.8117)  acc5: 100.0000 (99.3441)  time: 0.0287  data: 0.0002  max mem: 5511
[10:56:00.163421] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4209 (0.3933)  acc1: 87.5000 (88.9423)  acc5: 100.0000 (99.3647)  time: 0.0282  data: 0.0002  max mem: 5511
[10:56:00.453578] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4139 (0.3985)  acc1: 87.5000 (88.7222)  acc5: 100.0000 (99.3657)  time: 0.0284  data: 0.0002  max mem: 5511
[10:56:00.740125] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.3905 (0.3971)  acc1: 87.5000 (88.7810)  acc5: 100.0000 (99.3947)  time: 0.0287  data: 0.0002  max mem: 5511
[10:56:01.032057] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.3612 (0.3951)  acc1: 89.0625 (88.8430)  acc5: 100.0000 (99.3931)  time: 0.0288  data: 0.0003  max mem: 5511
[10:56:01.314994] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.3748 (0.3957)  acc1: 89.0625 (88.7643)  acc5: 100.0000 (99.3798)  time: 0.0286  data: 0.0002  max mem: 5511
[10:56:01.597019] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3590 (0.3919)  acc1: 89.0625 (88.8520)  acc5: 100.0000 (99.4127)  time: 0.0281  data: 0.0002  max mem: 5511
[10:56:01.876757] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.3600 (0.3916)  acc1: 89.0625 (88.8452)  acc5: 100.0000 (99.3998)  time: 0.0280  data: 0.0002  max mem: 5511
[10:56:02.026562] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.3681 (0.3913)  acc1: 89.0625 (88.7700)  acc5: 100.0000 (99.4100)  time: 0.0269  data: 0.0001  max mem: 5511
[10:56:02.191211] Test: Total time: 0:00:05 (0.0336 s / it)
[10:56:02.191971] * Acc@1 88.770 Acc@5 99.410 loss 0.391
[10:56:02.192259] Accuracy of the network on the 10000 test images: 88.8%
[10:56:02.192456] Max accuracy: 88.85%
[10:56:02.505735] log_dir: ./output_dir
[10:56:03.453111] Epoch: [80]  [  0/781]  eta: 0:12:18  lr: 0.000027  training_loss: 0.3064 (0.3064)  mae_loss: 0.2070 (0.2070)  classification_loss: 0.0994 (0.0994)  time: 0.9461  data: 0.7074  max mem: 5511
[10:56:07.364809] Epoch: [80]  [ 20/781]  eta: 0:02:55  lr: 0.000027  training_loss: 0.3003 (0.3109)  mae_loss: 0.1979 (0.2027)  classification_loss: 0.1069 (0.1082)  time: 0.1955  data: 0.0002  max mem: 5511
[10:56:11.264789] Epoch: [80]  [ 40/781]  eta: 0:02:38  lr: 0.000027  training_loss: 0.3073 (0.3091)  mae_loss: 0.1991 (0.2004)  classification_loss: 0.1045 (0.1087)  time: 0.1949  data: 0.0002  max mem: 5511
[10:56:15.184386] Epoch: [80]  [ 60/781]  eta: 0:02:29  lr: 0.000027  training_loss: 0.3092 (0.3139)  mae_loss: 0.2054 (0.2034)  classification_loss: 0.1116 (0.1105)  time: 0.1958  data: 0.0002  max mem: 5511
[10:56:19.086891] Epoch: [80]  [ 80/781]  eta: 0:02:23  lr: 0.000027  training_loss: 0.3084 (0.3132)  mae_loss: 0.2048 (0.2033)  classification_loss: 0.1072 (0.1099)  time: 0.1950  data: 0.0002  max mem: 5511
[10:56:22.988119] Epoch: [80]  [100/781]  eta: 0:02:18  lr: 0.000027  training_loss: 0.3192 (0.3125)  mae_loss: 0.2061 (0.2033)  classification_loss: 0.1073 (0.1092)  time: 0.1950  data: 0.0002  max mem: 5511
[10:56:26.889056] Epoch: [80]  [120/781]  eta: 0:02:13  lr: 0.000027  training_loss: 0.3125 (0.3136)  mae_loss: 0.2055 (0.2039)  classification_loss: 0.1129 (0.1096)  time: 0.1949  data: 0.0002  max mem: 5511
[10:56:30.846937] Epoch: [80]  [140/781]  eta: 0:02:08  lr: 0.000027  training_loss: 0.3103 (0.3137)  mae_loss: 0.2007 (0.2044)  classification_loss: 0.1085 (0.1093)  time: 0.1978  data: 0.0002  max mem: 5511
[10:56:34.788379] Epoch: [80]  [160/781]  eta: 0:02:04  lr: 0.000027  training_loss: 0.3147 (0.3139)  mae_loss: 0.2084 (0.2050)  classification_loss: 0.1069 (0.1089)  time: 0.1970  data: 0.0002  max mem: 5511
[10:56:38.689062] Epoch: [80]  [180/781]  eta: 0:02:00  lr: 0.000027  training_loss: 0.3066 (0.3127)  mae_loss: 0.1976 (0.2039)  classification_loss: 0.1070 (0.1088)  time: 0.1949  data: 0.0003  max mem: 5511
[10:56:42.604179] Epoch: [80]  [200/781]  eta: 0:01:55  lr: 0.000027  training_loss: 0.3044 (0.3118)  mae_loss: 0.1996 (0.2035)  classification_loss: 0.1019 (0.1083)  time: 0.1957  data: 0.0002  max mem: 5511
[10:56:46.505892] Epoch: [80]  [220/781]  eta: 0:01:51  lr: 0.000027  training_loss: 0.3004 (0.3112)  mae_loss: 0.1975 (0.2031)  classification_loss: 0.1039 (0.1081)  time: 0.1949  data: 0.0002  max mem: 5511
[10:56:50.433787] Epoch: [80]  [240/781]  eta: 0:01:47  lr: 0.000026  training_loss: 0.3048 (0.3114)  mae_loss: 0.2060 (0.2035)  classification_loss: 0.1043 (0.1079)  time: 0.1963  data: 0.0002  max mem: 5511
[10:56:54.342614] Epoch: [80]  [260/781]  eta: 0:01:43  lr: 0.000026  training_loss: 0.3068 (0.3114)  mae_loss: 0.1971 (0.2034)  classification_loss: 0.1084 (0.1080)  time: 0.1954  data: 0.0002  max mem: 5511
[10:56:58.251999] Epoch: [80]  [280/781]  eta: 0:01:39  lr: 0.000026  training_loss: 0.3050 (0.3114)  mae_loss: 0.1993 (0.2035)  classification_loss: 0.1062 (0.1079)  time: 0.1953  data: 0.0002  max mem: 5511
[10:57:02.179948] Epoch: [80]  [300/781]  eta: 0:01:35  lr: 0.000026  training_loss: 0.3042 (0.3112)  mae_loss: 0.1964 (0.2033)  classification_loss: 0.1077 (0.1079)  time: 0.1963  data: 0.0002  max mem: 5511
[10:57:06.085823] Epoch: [80]  [320/781]  eta: 0:01:31  lr: 0.000026  training_loss: 0.3170 (0.3116)  mae_loss: 0.2095 (0.2034)  classification_loss: 0.1096 (0.1081)  time: 0.1952  data: 0.0002  max mem: 5511
[10:57:10.053349] Epoch: [80]  [340/781]  eta: 0:01:27  lr: 0.000026  training_loss: 0.3014 (0.3117)  mae_loss: 0.1992 (0.2039)  classification_loss: 0.1038 (0.1079)  time: 0.1983  data: 0.0002  max mem: 5511
[10:57:13.942897] Epoch: [80]  [360/781]  eta: 0:01:23  lr: 0.000026  training_loss: 0.2979 (0.3116)  mae_loss: 0.2016 (0.2038)  classification_loss: 0.1055 (0.1078)  time: 0.1944  data: 0.0002  max mem: 5511
[10:57:17.855301] Epoch: [80]  [380/781]  eta: 0:01:19  lr: 0.000026  training_loss: 0.3029 (0.3113)  mae_loss: 0.1907 (0.2037)  classification_loss: 0.1035 (0.1077)  time: 0.1955  data: 0.0003  max mem: 5511
[10:57:21.773967] Epoch: [80]  [400/781]  eta: 0:01:15  lr: 0.000026  training_loss: 0.3181 (0.3114)  mae_loss: 0.2075 (0.2039)  classification_loss: 0.1038 (0.1076)  time: 0.1958  data: 0.0003  max mem: 5511
[10:57:25.670420] Epoch: [80]  [420/781]  eta: 0:01:11  lr: 0.000026  training_loss: 0.3267 (0.3120)  mae_loss: 0.2118 (0.2043)  classification_loss: 0.1097 (0.1077)  time: 0.1947  data: 0.0002  max mem: 5511
[10:57:29.582876] Epoch: [80]  [440/781]  eta: 0:01:07  lr: 0.000026  training_loss: 0.3123 (0.3122)  mae_loss: 0.2044 (0.2045)  classification_loss: 0.1036 (0.1077)  time: 0.1955  data: 0.0002  max mem: 5511
[10:57:33.488769] Epoch: [80]  [460/781]  eta: 0:01:03  lr: 0.000026  training_loss: 0.3133 (0.3123)  mae_loss: 0.2057 (0.2046)  classification_loss: 0.1059 (0.1077)  time: 0.1952  data: 0.0002  max mem: 5511
[10:57:37.405270] Epoch: [80]  [480/781]  eta: 0:00:59  lr: 0.000026  training_loss: 0.3099 (0.3123)  mae_loss: 0.2094 (0.2047)  classification_loss: 0.1046 (0.1076)  time: 0.1958  data: 0.0002  max mem: 5511
[10:57:41.340426] Epoch: [80]  [500/781]  eta: 0:00:55  lr: 0.000026  training_loss: 0.3019 (0.3122)  mae_loss: 0.1982 (0.2047)  classification_loss: 0.1071 (0.1076)  time: 0.1967  data: 0.0002  max mem: 5511
[10:57:45.277079] Epoch: [80]  [520/781]  eta: 0:00:51  lr: 0.000026  training_loss: 0.3164 (0.3125)  mae_loss: 0.2089 (0.2050)  classification_loss: 0.1073 (0.1075)  time: 0.1967  data: 0.0002  max mem: 5511
[10:57:49.186654] Epoch: [80]  [540/781]  eta: 0:00:47  lr: 0.000026  training_loss: 0.3173 (0.3126)  mae_loss: 0.2070 (0.2051)  classification_loss: 0.1102 (0.1075)  time: 0.1954  data: 0.0002  max mem: 5511
[10:57:53.083721] Epoch: [80]  [560/781]  eta: 0:00:43  lr: 0.000025  training_loss: 0.3198 (0.3130)  mae_loss: 0.2111 (0.2054)  classification_loss: 0.1050 (0.1075)  time: 0.1948  data: 0.0002  max mem: 5511
[10:57:57.031610] Epoch: [80]  [580/781]  eta: 0:00:39  lr: 0.000025  training_loss: 0.3114 (0.3130)  mae_loss: 0.2016 (0.2054)  classification_loss: 0.1095 (0.1077)  time: 0.1973  data: 0.0002  max mem: 5511
[10:58:00.929688] Epoch: [80]  [600/781]  eta: 0:00:35  lr: 0.000025  training_loss: 0.2999 (0.3126)  mae_loss: 0.1907 (0.2050)  classification_loss: 0.1066 (0.1076)  time: 0.1948  data: 0.0003  max mem: 5511
[10:58:04.831866] Epoch: [80]  [620/781]  eta: 0:00:31  lr: 0.000025  training_loss: 0.3074 (0.3128)  mae_loss: 0.2028 (0.2052)  classification_loss: 0.1063 (0.1076)  time: 0.1950  data: 0.0003  max mem: 5511
[10:58:08.727288] Epoch: [80]  [640/781]  eta: 0:00:27  lr: 0.000025  training_loss: 0.3081 (0.3124)  mae_loss: 0.1920 (0.2049)  classification_loss: 0.1046 (0.1076)  time: 0.1947  data: 0.0003  max mem: 5511
[10:58:12.640432] Epoch: [80]  [660/781]  eta: 0:00:23  lr: 0.000025  training_loss: 0.3252 (0.3127)  mae_loss: 0.2080 (0.2051)  classification_loss: 0.1081 (0.1076)  time: 0.1956  data: 0.0002  max mem: 5511
[10:58:16.579704] Epoch: [80]  [680/781]  eta: 0:00:19  lr: 0.000025  training_loss: 0.3091 (0.3129)  mae_loss: 0.2010 (0.2052)  classification_loss: 0.1100 (0.1077)  time: 0.1969  data: 0.0003  max mem: 5511
[10:58:20.489262] Epoch: [80]  [700/781]  eta: 0:00:15  lr: 0.000025  training_loss: 0.3253 (0.3131)  mae_loss: 0.2066 (0.2052)  classification_loss: 0.1134 (0.1079)  time: 0.1954  data: 0.0003  max mem: 5511
[10:58:24.377561] Epoch: [80]  [720/781]  eta: 0:00:11  lr: 0.000025  training_loss: 0.3106 (0.3131)  mae_loss: 0.1996 (0.2052)  classification_loss: 0.1030 (0.1079)  time: 0.1943  data: 0.0003  max mem: 5511
[10:58:28.286940] Epoch: [80]  [740/781]  eta: 0:00:08  lr: 0.000025  training_loss: 0.3184 (0.3132)  mae_loss: 0.2172 (0.2054)  classification_loss: 0.1049 (0.1078)  time: 0.1954  data: 0.0003  max mem: 5511
[10:58:32.207170] Epoch: [80]  [760/781]  eta: 0:00:04  lr: 0.000025  training_loss: 0.3085 (0.3133)  mae_loss: 0.2038 (0.2055)  classification_loss: 0.1079 (0.1078)  time: 0.1959  data: 0.0003  max mem: 5511
[10:58:36.095095] Epoch: [80]  [780/781]  eta: 0:00:00  lr: 0.000025  training_loss: 0.3078 (0.3133)  mae_loss: 0.2060 (0.2056)  classification_loss: 0.1091 (0.1077)  time: 0.1943  data: 0.0002  max mem: 5511
[10:58:36.235419] Epoch: [80] Total time: 0:02:33 (0.1968 s / it)
[10:58:36.235910] Averaged stats: lr: 0.000025  training_loss: 0.3078 (0.3133)  mae_loss: 0.2060 (0.2056)  classification_loss: 0.1091 (0.1077)
[10:58:38.346820] Test:  [  0/157]  eta: 0:01:43  testing_loss: 0.5182 (0.5182)  acc1: 84.3750 (84.3750)  acc5: 98.4375 (98.4375)  time: 0.6597  data: 0.6281  max mem: 5511
[10:58:38.644186] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4333 (0.4161)  acc1: 87.5000 (87.0739)  acc5: 100.0000 (99.2898)  time: 0.0868  data: 0.0575  max mem: 5511
[10:58:38.925662] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.3745 (0.3915)  acc1: 89.0625 (88.3929)  acc5: 100.0000 (99.4792)  time: 0.0288  data: 0.0003  max mem: 5511
[10:58:39.208204] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.3417 (0.3957)  acc1: 90.6250 (88.4577)  acc5: 100.0000 (99.3448)  time: 0.0281  data: 0.0002  max mem: 5511
[10:58:39.493031] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4138 (0.4112)  acc1: 87.5000 (87.9192)  acc5: 100.0000 (99.2378)  time: 0.0283  data: 0.0002  max mem: 5511
[10:58:39.776085] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.4062 (0.4003)  acc1: 87.5000 (88.4804)  acc5: 100.0000 (99.2034)  time: 0.0283  data: 0.0002  max mem: 5511
[10:58:40.059044] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.3261 (0.3980)  acc1: 90.6250 (88.6783)  acc5: 100.0000 (99.2059)  time: 0.0282  data: 0.0002  max mem: 5511
[10:58:40.341114] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.3599 (0.3934)  acc1: 89.0625 (88.8644)  acc5: 100.0000 (99.2738)  time: 0.0281  data: 0.0002  max mem: 5511
[10:58:40.622660] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.3946 (0.4010)  acc1: 87.5000 (88.5417)  acc5: 100.0000 (99.2670)  time: 0.0281  data: 0.0002  max mem: 5511
[10:58:40.904330] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4177 (0.3980)  acc1: 87.5000 (88.6676)  acc5: 100.0000 (99.2960)  time: 0.0280  data: 0.0002  max mem: 5511
[10:58:41.186092] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4121 (0.4036)  acc1: 89.0625 (88.4127)  acc5: 100.0000 (99.3193)  time: 0.0281  data: 0.0002  max mem: 5511
[10:58:41.468632] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4120 (0.4034)  acc1: 87.5000 (88.4150)  acc5: 100.0000 (99.3243)  time: 0.0281  data: 0.0002  max mem: 5511
[10:58:41.754116] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.3563 (0.4021)  acc1: 89.0625 (88.4814)  acc5: 100.0000 (99.3156)  time: 0.0283  data: 0.0002  max mem: 5511
[10:58:42.039282] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.3620 (0.4032)  acc1: 89.0625 (88.4423)  acc5: 100.0000 (99.3082)  time: 0.0284  data: 0.0002  max mem: 5511
[10:58:42.321308] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3461 (0.3994)  acc1: 90.6250 (88.5527)  acc5: 100.0000 (99.3462)  time: 0.0282  data: 0.0002  max mem: 5511
[10:58:42.599588] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.3484 (0.3990)  acc1: 89.0625 (88.5762)  acc5: 100.0000 (99.3584)  time: 0.0279  data: 0.0001  max mem: 5511
[10:58:42.749860] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.3774 (0.3993)  acc1: 89.0625 (88.5500)  acc5: 100.0000 (99.3600)  time: 0.0269  data: 0.0001  max mem: 5511
[10:58:42.915449] Test: Total time: 0:00:05 (0.0333 s / it)
[10:58:42.916036] * Acc@1 88.550 Acc@5 99.360 loss 0.399
[10:58:42.916390] Accuracy of the network on the 10000 test images: 88.5%
[10:58:42.916600] Max accuracy: 88.85%
[10:58:43.277044] log_dir: ./output_dir
[10:58:44.138293] Epoch: [81]  [  0/781]  eta: 0:11:11  lr: 0.000025  training_loss: 0.2943 (0.2943)  mae_loss: 0.2013 (0.2013)  classification_loss: 0.0931 (0.0931)  time: 0.8595  data: 0.6312  max mem: 5511
[10:58:48.044993] Epoch: [81]  [ 20/781]  eta: 0:02:52  lr: 0.000025  training_loss: 0.3150 (0.3145)  mae_loss: 0.2032 (0.2090)  classification_loss: 0.1053 (0.1055)  time: 0.1952  data: 0.0002  max mem: 5511
[10:58:51.955690] Epoch: [81]  [ 40/781]  eta: 0:02:36  lr: 0.000025  training_loss: 0.2971 (0.3101)  mae_loss: 0.1925 (0.2042)  classification_loss: 0.1075 (0.1059)  time: 0.1955  data: 0.0002  max mem: 5511
[10:58:55.878256] Epoch: [81]  [ 60/781]  eta: 0:02:28  lr: 0.000025  training_loss: 0.2969 (0.3080)  mae_loss: 0.1883 (0.2012)  classification_loss: 0.1086 (0.1068)  time: 0.1961  data: 0.0002  max mem: 5511
[10:58:59.784537] Epoch: [81]  [ 80/781]  eta: 0:02:22  lr: 0.000025  training_loss: 0.3012 (0.3070)  mae_loss: 0.1982 (0.2009)  classification_loss: 0.1012 (0.1061)  time: 0.1952  data: 0.0003  max mem: 5511
[10:59:03.707134] Epoch: [81]  [100/781]  eta: 0:02:17  lr: 0.000024  training_loss: 0.3158 (0.3103)  mae_loss: 0.2094 (0.2038)  classification_loss: 0.1090 (0.1065)  time: 0.1960  data: 0.0003  max mem: 5511
[10:59:07.624467] Epoch: [81]  [120/781]  eta: 0:02:12  lr: 0.000024  training_loss: 0.3044 (0.3092)  mae_loss: 0.2041 (0.2030)  classification_loss: 0.1042 (0.1063)  time: 0.1958  data: 0.0002  max mem: 5511
[10:59:11.553137] Epoch: [81]  [140/781]  eta: 0:02:08  lr: 0.000024  training_loss: 0.3110 (0.3099)  mae_loss: 0.2067 (0.2034)  classification_loss: 0.1040 (0.1065)  time: 0.1964  data: 0.0002  max mem: 5511
[10:59:15.494114] Epoch: [81]  [160/781]  eta: 0:02:04  lr: 0.000024  training_loss: 0.3095 (0.3102)  mae_loss: 0.2019 (0.2035)  classification_loss: 0.1083 (0.1067)  time: 0.1970  data: 0.0002  max mem: 5511
[10:59:19.397816] Epoch: [81]  [180/781]  eta: 0:01:59  lr: 0.000024  training_loss: 0.3156 (0.3115)  mae_loss: 0.2117 (0.2046)  classification_loss: 0.1075 (0.1069)  time: 0.1951  data: 0.0003  max mem: 5511
[10:59:23.323675] Epoch: [81]  [200/781]  eta: 0:01:55  lr: 0.000024  training_loss: 0.3012 (0.3113)  mae_loss: 0.2033 (0.2046)  classification_loss: 0.0997 (0.1067)  time: 0.1962  data: 0.0002  max mem: 5511
[10:59:27.280706] Epoch: [81]  [220/781]  eta: 0:01:51  lr: 0.000024  training_loss: 0.3213 (0.3117)  mae_loss: 0.2154 (0.2050)  classification_loss: 0.1049 (0.1067)  time: 0.1978  data: 0.0002  max mem: 5511
[10:59:31.226485] Epoch: [81]  [240/781]  eta: 0:01:47  lr: 0.000024  training_loss: 0.3105 (0.3122)  mae_loss: 0.2058 (0.2053)  classification_loss: 0.1089 (0.1069)  time: 0.1972  data: 0.0002  max mem: 5511
[10:59:35.117574] Epoch: [81]  [260/781]  eta: 0:01:43  lr: 0.000024  training_loss: 0.2989 (0.3116)  mae_loss: 0.1922 (0.2048)  classification_loss: 0.1027 (0.1067)  time: 0.1945  data: 0.0002  max mem: 5511
[10:59:39.026156] Epoch: [81]  [280/781]  eta: 0:01:39  lr: 0.000024  training_loss: 0.3102 (0.3119)  mae_loss: 0.2091 (0.2053)  classification_loss: 0.1061 (0.1067)  time: 0.1953  data: 0.0002  max mem: 5511
[10:59:42.966369] Epoch: [81]  [300/781]  eta: 0:01:35  lr: 0.000024  training_loss: 0.3133 (0.3126)  mae_loss: 0.2029 (0.2055)  classification_loss: 0.1134 (0.1071)  time: 0.1969  data: 0.0002  max mem: 5511
[10:59:46.877288] Epoch: [81]  [320/781]  eta: 0:01:31  lr: 0.000024  training_loss: 0.3120 (0.3123)  mae_loss: 0.2058 (0.2050)  classification_loss: 0.1082 (0.1073)  time: 0.1954  data: 0.0003  max mem: 5511
[10:59:50.787626] Epoch: [81]  [340/781]  eta: 0:01:27  lr: 0.000024  training_loss: 0.3179 (0.3122)  mae_loss: 0.1994 (0.2048)  classification_loss: 0.1051 (0.1073)  time: 0.1954  data: 0.0004  max mem: 5511
[10:59:54.685593] Epoch: [81]  [360/781]  eta: 0:01:23  lr: 0.000024  training_loss: 0.2991 (0.3119)  mae_loss: 0.1917 (0.2045)  classification_loss: 0.1052 (0.1074)  time: 0.1948  data: 0.0003  max mem: 5511
[10:59:58.589026] Epoch: [81]  [380/781]  eta: 0:01:19  lr: 0.000024  training_loss: 0.3137 (0.3124)  mae_loss: 0.2018 (0.2049)  classification_loss: 0.1085 (0.1075)  time: 0.1951  data: 0.0002  max mem: 5511
[11:00:02.499721] Epoch: [81]  [400/781]  eta: 0:01:15  lr: 0.000024  training_loss: 0.3015 (0.3121)  mae_loss: 0.1945 (0.2047)  classification_loss: 0.1073 (0.1074)  time: 0.1954  data: 0.0002  max mem: 5511
[11:00:06.380458] Epoch: [81]  [420/781]  eta: 0:01:11  lr: 0.000023  training_loss: 0.2990 (0.3117)  mae_loss: 0.1943 (0.2044)  classification_loss: 0.1036 (0.1073)  time: 0.1940  data: 0.0002  max mem: 5511
[11:00:10.321817] Epoch: [81]  [440/781]  eta: 0:01:07  lr: 0.000023  training_loss: 0.3056 (0.3113)  mae_loss: 0.2013 (0.2041)  classification_loss: 0.1052 (0.1072)  time: 0.1970  data: 0.0002  max mem: 5511
[11:00:14.314670] Epoch: [81]  [460/781]  eta: 0:01:03  lr: 0.000023  training_loss: 0.3035 (0.3111)  mae_loss: 0.2030 (0.2041)  classification_loss: 0.1026 (0.1070)  time: 0.1996  data: 0.0002  max mem: 5511
[11:00:18.248013] Epoch: [81]  [480/781]  eta: 0:00:59  lr: 0.000023  training_loss: 0.3070 (0.3110)  mae_loss: 0.1993 (0.2039)  classification_loss: 0.1058 (0.1071)  time: 0.1966  data: 0.0003  max mem: 5511
[11:00:22.179037] Epoch: [81]  [500/781]  eta: 0:00:55  lr: 0.000023  training_loss: 0.3161 (0.3115)  mae_loss: 0.2001 (0.2042)  classification_loss: 0.1105 (0.1073)  time: 0.1965  data: 0.0002  max mem: 5511
[11:00:26.082543] Epoch: [81]  [520/781]  eta: 0:00:51  lr: 0.000023  training_loss: 0.3065 (0.3116)  mae_loss: 0.2035 (0.2043)  classification_loss: 0.1032 (0.1073)  time: 0.1951  data: 0.0003  max mem: 5511
[11:00:29.996715] Epoch: [81]  [540/781]  eta: 0:00:47  lr: 0.000023  training_loss: 0.3175 (0.3121)  mae_loss: 0.2052 (0.2047)  classification_loss: 0.1078 (0.1074)  time: 0.1956  data: 0.0002  max mem: 5511
[11:00:33.904597] Epoch: [81]  [560/781]  eta: 0:00:43  lr: 0.000023  training_loss: 0.3087 (0.3123)  mae_loss: 0.2127 (0.2050)  classification_loss: 0.1050 (0.1073)  time: 0.1953  data: 0.0003  max mem: 5511
[11:00:37.856686] Epoch: [81]  [580/781]  eta: 0:00:39  lr: 0.000023  training_loss: 0.3234 (0.3125)  mae_loss: 0.2199 (0.2052)  classification_loss: 0.1029 (0.1072)  time: 0.1975  data: 0.0002  max mem: 5511
[11:00:41.764465] Epoch: [81]  [600/781]  eta: 0:00:35  lr: 0.000023  training_loss: 0.2984 (0.3122)  mae_loss: 0.1930 (0.2050)  classification_loss: 0.1032 (0.1072)  time: 0.1953  data: 0.0002  max mem: 5511
[11:00:45.671187] Epoch: [81]  [620/781]  eta: 0:00:31  lr: 0.000023  training_loss: 0.3137 (0.3121)  mae_loss: 0.2028 (0.2049)  classification_loss: 0.1086 (0.1072)  time: 0.1953  data: 0.0002  max mem: 5511
[11:00:49.588376] Epoch: [81]  [640/781]  eta: 0:00:27  lr: 0.000023  training_loss: 0.3079 (0.3121)  mae_loss: 0.2010 (0.2048)  classification_loss: 0.1073 (0.1072)  time: 0.1958  data: 0.0002  max mem: 5511
[11:00:53.511781] Epoch: [81]  [660/781]  eta: 0:00:23  lr: 0.000023  training_loss: 0.2979 (0.3116)  mae_loss: 0.1943 (0.2045)  classification_loss: 0.1033 (0.1071)  time: 0.1960  data: 0.0003  max mem: 5511
[11:00:57.428525] Epoch: [81]  [680/781]  eta: 0:00:19  lr: 0.000023  training_loss: 0.3221 (0.3118)  mae_loss: 0.2148 (0.2047)  classification_loss: 0.1073 (0.1072)  time: 0.1957  data: 0.0002  max mem: 5511
[11:01:01.337805] Epoch: [81]  [700/781]  eta: 0:00:15  lr: 0.000023  training_loss: 0.3091 (0.3118)  mae_loss: 0.1916 (0.2046)  classification_loss: 0.1076 (0.1072)  time: 0.1954  data: 0.0002  max mem: 5511
[11:01:05.235681] Epoch: [81]  [720/781]  eta: 0:00:12  lr: 0.000023  training_loss: 0.3046 (0.3117)  mae_loss: 0.1969 (0.2046)  classification_loss: 0.1022 (0.1071)  time: 0.1948  data: 0.0005  max mem: 5511
[11:01:09.151558] Epoch: [81]  [740/781]  eta: 0:00:08  lr: 0.000023  training_loss: 0.3132 (0.3115)  mae_loss: 0.1973 (0.2044)  classification_loss: 0.1052 (0.1071)  time: 0.1957  data: 0.0002  max mem: 5511
[11:01:13.058559] Epoch: [81]  [760/781]  eta: 0:00:04  lr: 0.000022  training_loss: 0.3092 (0.3115)  mae_loss: 0.2053 (0.2045)  classification_loss: 0.1044 (0.1070)  time: 0.1953  data: 0.0003  max mem: 5511
[11:01:16.943131] Epoch: [81]  [780/781]  eta: 0:00:00  lr: 0.000022  training_loss: 0.3078 (0.3116)  mae_loss: 0.1987 (0.2045)  classification_loss: 0.1082 (0.1070)  time: 0.1941  data: 0.0002  max mem: 5511
[11:01:17.110523] Epoch: [81] Total time: 0:02:33 (0.1970 s / it)
[11:01:17.110978] Averaged stats: lr: 0.000022  training_loss: 0.3078 (0.3116)  mae_loss: 0.1987 (0.2045)  classification_loss: 0.1082 (0.1070)
[11:01:17.793395] Test:  [  0/157]  eta: 0:01:46  testing_loss: 0.4819 (0.4819)  acc1: 89.0625 (89.0625)  acc5: 98.4375 (98.4375)  time: 0.6786  data: 0.6460  max mem: 5511
[11:01:18.082868] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.3903 (0.3927)  acc1: 89.0625 (88.0682)  acc5: 100.0000 (99.5739)  time: 0.0879  data: 0.0590  max mem: 5511
[11:01:18.372282] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.3425 (0.3770)  acc1: 89.0625 (88.8393)  acc5: 100.0000 (99.7024)  time: 0.0288  data: 0.0002  max mem: 5511
[11:01:18.663159] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.3370 (0.3856)  acc1: 90.6250 (88.7601)  acc5: 100.0000 (99.5464)  time: 0.0289  data: 0.0002  max mem: 5511
[11:01:18.948481] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.3955 (0.3986)  acc1: 89.0625 (88.4527)  acc5: 100.0000 (99.5046)  time: 0.0287  data: 0.0002  max mem: 5511
[11:01:19.236059] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.3791 (0.3914)  acc1: 89.0625 (88.6029)  acc5: 100.0000 (99.4179)  time: 0.0285  data: 0.0002  max mem: 5511
[11:01:19.532626] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.3307 (0.3867)  acc1: 90.6250 (89.0113)  acc5: 100.0000 (99.3852)  time: 0.0291  data: 0.0002  max mem: 5511
[11:01:19.820033] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.3564 (0.3811)  acc1: 90.6250 (89.1725)  acc5: 100.0000 (99.4498)  time: 0.0291  data: 0.0002  max mem: 5511
[11:01:20.103748] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.3726 (0.3885)  acc1: 89.0625 (88.9082)  acc5: 100.0000 (99.4792)  time: 0.0284  data: 0.0002  max mem: 5511
[11:01:20.387768] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.3930 (0.3850)  acc1: 89.0625 (88.9766)  acc5: 100.0000 (99.5021)  time: 0.0283  data: 0.0002  max mem: 5511
[11:01:20.669042] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.3930 (0.3899)  acc1: 89.0625 (88.8304)  acc5: 100.0000 (99.5204)  time: 0.0281  data: 0.0002  max mem: 5511
[11:01:20.952529] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.3734 (0.3887)  acc1: 89.0625 (88.9358)  acc5: 100.0000 (99.5355)  time: 0.0281  data: 0.0003  max mem: 5511
[11:01:21.237523] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.3600 (0.3863)  acc1: 89.0625 (88.9979)  acc5: 100.0000 (99.5222)  time: 0.0283  data: 0.0003  max mem: 5511
[11:01:21.520849] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.3824 (0.3880)  acc1: 89.0625 (88.9194)  acc5: 100.0000 (99.4990)  time: 0.0283  data: 0.0002  max mem: 5511
[11:01:21.807797] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3414 (0.3843)  acc1: 90.6250 (89.0403)  acc5: 100.0000 (99.5346)  time: 0.0283  data: 0.0002  max mem: 5511
[11:01:22.089139] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.3507 (0.3844)  acc1: 90.6250 (89.0108)  acc5: 100.0000 (99.5447)  time: 0.0283  data: 0.0002  max mem: 5511
[11:01:22.242383] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.3700 (0.3844)  acc1: 87.5000 (88.9600)  acc5: 100.0000 (99.5300)  time: 0.0272  data: 0.0001  max mem: 5511
[11:01:22.411233] Test: Total time: 0:00:05 (0.0337 s / it)
[11:01:22.411698] * Acc@1 88.960 Acc@5 99.530 loss 0.384
[11:01:22.412009] Accuracy of the network on the 10000 test images: 89.0%
[11:01:22.412200] Max accuracy: 88.96%
[11:01:22.641710] log_dir: ./output_dir
[11:01:23.548661] Epoch: [82]  [  0/781]  eta: 0:11:46  lr: 0.000022  training_loss: 0.2962 (0.2962)  mae_loss: 0.2030 (0.2030)  classification_loss: 0.0933 (0.0933)  time: 0.9052  data: 0.6639  max mem: 5511
[11:01:27.474564] Epoch: [82]  [ 20/781]  eta: 0:02:54  lr: 0.000022  training_loss: 0.3092 (0.3117)  mae_loss: 0.2093 (0.2048)  classification_loss: 0.1073 (0.1069)  time: 0.1962  data: 0.0002  max mem: 5511
[11:01:31.374442] Epoch: [82]  [ 40/781]  eta: 0:02:37  lr: 0.000022  training_loss: 0.3154 (0.3125)  mae_loss: 0.1974 (0.2040)  classification_loss: 0.1104 (0.1086)  time: 0.1949  data: 0.0002  max mem: 5511
[11:01:35.375998] Epoch: [82]  [ 60/781]  eta: 0:02:30  lr: 0.000022  training_loss: 0.3076 (0.3106)  mae_loss: 0.2006 (0.2026)  classification_loss: 0.1058 (0.1079)  time: 0.2000  data: 0.0002  max mem: 5511
[11:01:39.309846] Epoch: [82]  [ 80/781]  eta: 0:02:24  lr: 0.000022  training_loss: 0.3065 (0.3117)  mae_loss: 0.1990 (0.2043)  classification_loss: 0.1037 (0.1074)  time: 0.1966  data: 0.0002  max mem: 5511
[11:01:43.204953] Epoch: [82]  [100/781]  eta: 0:02:18  lr: 0.000022  training_loss: 0.3133 (0.3119)  mae_loss: 0.2033 (0.2038)  classification_loss: 0.1085 (0.1080)  time: 0.1947  data: 0.0002  max mem: 5511
[11:01:47.099723] Epoch: [82]  [120/781]  eta: 0:02:13  lr: 0.000022  training_loss: 0.3192 (0.3135)  mae_loss: 0.2106 (0.2055)  classification_loss: 0.1084 (0.1080)  time: 0.1946  data: 0.0002  max mem: 5511
[11:01:51.043015] Epoch: [82]  [140/781]  eta: 0:02:09  lr: 0.000022  training_loss: 0.3218 (0.3143)  mae_loss: 0.2133 (0.2064)  classification_loss: 0.1076 (0.1078)  time: 0.1971  data: 0.0002  max mem: 5511
[11:01:54.968429] Epoch: [82]  [160/781]  eta: 0:02:04  lr: 0.000022  training_loss: 0.3028 (0.3134)  mae_loss: 0.1975 (0.2057)  classification_loss: 0.1054 (0.1078)  time: 0.1962  data: 0.0003  max mem: 5511
[11:01:58.899660] Epoch: [82]  [180/781]  eta: 0:02:00  lr: 0.000022  training_loss: 0.3159 (0.3140)  mae_loss: 0.2052 (0.2061)  classification_loss: 0.1057 (0.1079)  time: 0.1965  data: 0.0002  max mem: 5511
[11:02:02.826032] Epoch: [82]  [200/781]  eta: 0:01:56  lr: 0.000022  training_loss: 0.3065 (0.3134)  mae_loss: 0.1960 (0.2053)  classification_loss: 0.1107 (0.1081)  time: 0.1962  data: 0.0003  max mem: 5511
[11:02:06.743725] Epoch: [82]  [220/781]  eta: 0:01:51  lr: 0.000022  training_loss: 0.3113 (0.3130)  mae_loss: 0.2004 (0.2051)  classification_loss: 0.1061 (0.1079)  time: 0.1958  data: 0.0002  max mem: 5511
[11:02:10.647242] Epoch: [82]  [240/781]  eta: 0:01:47  lr: 0.000022  training_loss: 0.3119 (0.3127)  mae_loss: 0.1963 (0.2049)  classification_loss: 0.1041 (0.1079)  time: 0.1951  data: 0.0003  max mem: 5511
[11:02:14.638771] Epoch: [82]  [260/781]  eta: 0:01:43  lr: 0.000022  training_loss: 0.3054 (0.3123)  mae_loss: 0.1986 (0.2047)  classification_loss: 0.1036 (0.1076)  time: 0.1995  data: 0.0002  max mem: 5511
[11:02:18.545890] Epoch: [82]  [280/781]  eta: 0:01:39  lr: 0.000022  training_loss: 0.3025 (0.3121)  mae_loss: 0.1985 (0.2046)  classification_loss: 0.1056 (0.1075)  time: 0.1953  data: 0.0002  max mem: 5511
[11:02:22.431233] Epoch: [82]  [300/781]  eta: 0:01:35  lr: 0.000022  training_loss: 0.3015 (0.3118)  mae_loss: 0.1927 (0.2042)  classification_loss: 0.1074 (0.1075)  time: 0.1942  data: 0.0003  max mem: 5511
[11:02:26.335303] Epoch: [82]  [320/781]  eta: 0:01:31  lr: 0.000021  training_loss: 0.3060 (0.3116)  mae_loss: 0.1984 (0.2043)  classification_loss: 0.1051 (0.1073)  time: 0.1951  data: 0.0002  max mem: 5511
[11:02:30.259313] Epoch: [82]  [340/781]  eta: 0:01:27  lr: 0.000021  training_loss: 0.3107 (0.3119)  mae_loss: 0.2104 (0.2046)  classification_loss: 0.1063 (0.1073)  time: 0.1961  data: 0.0003  max mem: 5511
[11:02:34.153549] Epoch: [82]  [360/781]  eta: 0:01:23  lr: 0.000021  training_loss: 0.3133 (0.3122)  mae_loss: 0.2035 (0.2047)  classification_loss: 0.1111 (0.1074)  time: 0.1946  data: 0.0002  max mem: 5511
[11:02:38.051505] Epoch: [82]  [380/781]  eta: 0:01:19  lr: 0.000021  training_loss: 0.3068 (0.3119)  mae_loss: 0.1929 (0.2044)  classification_loss: 0.1084 (0.1075)  time: 0.1948  data: 0.0002  max mem: 5511
[11:02:41.947719] Epoch: [82]  [400/781]  eta: 0:01:15  lr: 0.000021  training_loss: 0.3155 (0.3126)  mae_loss: 0.2061 (0.2050)  classification_loss: 0.1059 (0.1076)  time: 0.1947  data: 0.0003  max mem: 5511
[11:02:45.859987] Epoch: [82]  [420/781]  eta: 0:01:11  lr: 0.000021  training_loss: 0.3197 (0.3132)  mae_loss: 0.1984 (0.2056)  classification_loss: 0.1054 (0.1076)  time: 0.1955  data: 0.0002  max mem: 5511
[11:02:49.770516] Epoch: [82]  [440/781]  eta: 0:01:07  lr: 0.000021  training_loss: 0.3108 (0.3131)  mae_loss: 0.2028 (0.2056)  classification_loss: 0.1081 (0.1076)  time: 0.1954  data: 0.0003  max mem: 5511
[11:02:53.699672] Epoch: [82]  [460/781]  eta: 0:01:03  lr: 0.000021  training_loss: 0.2946 (0.3124)  mae_loss: 0.1854 (0.2049)  classification_loss: 0.1027 (0.1075)  time: 0.1964  data: 0.0003  max mem: 5511
[11:02:57.605898] Epoch: [82]  [480/781]  eta: 0:00:59  lr: 0.000021  training_loss: 0.3047 (0.3125)  mae_loss: 0.2015 (0.2050)  classification_loss: 0.1080 (0.1075)  time: 0.1952  data: 0.0003  max mem: 5511
[11:03:01.540141] Epoch: [82]  [500/781]  eta: 0:00:55  lr: 0.000021  training_loss: 0.3076 (0.3125)  mae_loss: 0.1982 (0.2050)  classification_loss: 0.1067 (0.1075)  time: 0.1966  data: 0.0003  max mem: 5511
[11:03:05.443200] Epoch: [82]  [520/781]  eta: 0:00:51  lr: 0.000021  training_loss: 0.3185 (0.3128)  mae_loss: 0.2171 (0.2053)  classification_loss: 0.1047 (0.1075)  time: 0.1951  data: 0.0002  max mem: 5511
[11:03:09.355275] Epoch: [82]  [540/781]  eta: 0:00:47  lr: 0.000021  training_loss: 0.3090 (0.3129)  mae_loss: 0.2076 (0.2056)  classification_loss: 0.1008 (0.1074)  time: 0.1955  data: 0.0002  max mem: 5511
[11:03:13.272253] Epoch: [82]  [560/781]  eta: 0:00:43  lr: 0.000021  training_loss: 0.3070 (0.3129)  mae_loss: 0.1994 (0.2055)  classification_loss: 0.1070 (0.1073)  time: 0.1957  data: 0.0003  max mem: 5511
[11:03:17.161862] Epoch: [82]  [580/781]  eta: 0:00:39  lr: 0.000021  training_loss: 0.3163 (0.3130)  mae_loss: 0.2059 (0.2056)  classification_loss: 0.1050 (0.1074)  time: 0.1944  data: 0.0002  max mem: 5511
[11:03:21.104401] Epoch: [82]  [600/781]  eta: 0:00:35  lr: 0.000021  training_loss: 0.3052 (0.3127)  mae_loss: 0.2006 (0.2054)  classification_loss: 0.1045 (0.1073)  time: 0.1970  data: 0.0002  max mem: 5511
[11:03:25.028051] Epoch: [82]  [620/781]  eta: 0:00:31  lr: 0.000021  training_loss: 0.3054 (0.3126)  mae_loss: 0.2016 (0.2054)  classification_loss: 0.1013 (0.1072)  time: 0.1961  data: 0.0002  max mem: 5511
[11:03:28.940263] Epoch: [82]  [640/781]  eta: 0:00:27  lr: 0.000021  training_loss: 0.2995 (0.3125)  mae_loss: 0.1986 (0.2053)  classification_loss: 0.1021 (0.1071)  time: 0.1955  data: 0.0002  max mem: 5511
[11:03:32.846282] Epoch: [82]  [660/781]  eta: 0:00:23  lr: 0.000021  training_loss: 0.3066 (0.3124)  mae_loss: 0.1968 (0.2052)  classification_loss: 0.1112 (0.1072)  time: 0.1952  data: 0.0002  max mem: 5511
[11:03:36.758656] Epoch: [82]  [680/781]  eta: 0:00:19  lr: 0.000020  training_loss: 0.3153 (0.3125)  mae_loss: 0.2016 (0.2053)  classification_loss: 0.1069 (0.1072)  time: 0.1955  data: 0.0002  max mem: 5511
[11:03:40.682737] Epoch: [82]  [700/781]  eta: 0:00:15  lr: 0.000020  training_loss: 0.2997 (0.3122)  mae_loss: 0.1937 (0.2049)  classification_loss: 0.1077 (0.1073)  time: 0.1961  data: 0.0002  max mem: 5511
[11:03:44.625212] Epoch: [82]  [720/781]  eta: 0:00:12  lr: 0.000020  training_loss: 0.3113 (0.3121)  mae_loss: 0.1937 (0.2048)  classification_loss: 0.1099 (0.1074)  time: 0.1970  data: 0.0003  max mem: 5511
[11:03:48.549839] Epoch: [82]  [740/781]  eta: 0:00:08  lr: 0.000020  training_loss: 0.3053 (0.3121)  mae_loss: 0.1969 (0.2048)  classification_loss: 0.1070 (0.1074)  time: 0.1961  data: 0.0003  max mem: 5511
[11:03:52.444638] Epoch: [82]  [760/781]  eta: 0:00:04  lr: 0.000020  training_loss: 0.3120 (0.3123)  mae_loss: 0.1997 (0.2048)  classification_loss: 0.1123 (0.1075)  time: 0.1947  data: 0.0002  max mem: 5511
[11:03:56.355709] Epoch: [82]  [780/781]  eta: 0:00:00  lr: 0.000020  training_loss: 0.3026 (0.3123)  mae_loss: 0.1965 (0.2047)  classification_loss: 0.1081 (0.1076)  time: 0.1955  data: 0.0003  max mem: 5511
[11:03:56.502521] Epoch: [82] Total time: 0:02:33 (0.1970 s / it)
[11:03:56.502980] Averaged stats: lr: 0.000020  training_loss: 0.3026 (0.3123)  mae_loss: 0.1965 (0.2047)  classification_loss: 0.1081 (0.1076)
[11:03:57.054909] Test:  [  0/157]  eta: 0:01:25  testing_loss: 0.4763 (0.4763)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.5470  data: 0.5178  max mem: 5511
[11:03:57.343765] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.3939 (0.3974)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (99.4318)  time: 0.0756  data: 0.0473  max mem: 5511
[11:03:57.632336] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.3753 (0.3823)  acc1: 87.5000 (88.6161)  acc5: 100.0000 (99.5536)  time: 0.0285  data: 0.0003  max mem: 5511
[11:03:57.920791] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.3423 (0.3908)  acc1: 90.6250 (88.5585)  acc5: 100.0000 (99.4456)  time: 0.0286  data: 0.0003  max mem: 5511
[11:03:58.206060] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.4148 (0.4043)  acc1: 87.5000 (88.1479)  acc5: 100.0000 (99.3902)  time: 0.0285  data: 0.0003  max mem: 5511
[11:03:58.496965] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.3928 (0.3952)  acc1: 87.5000 (88.4191)  acc5: 100.0000 (99.4179)  time: 0.0286  data: 0.0003  max mem: 5511
[11:03:58.781286] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.3150 (0.3926)  acc1: 90.6250 (88.7551)  acc5: 100.0000 (99.3852)  time: 0.0286  data: 0.0002  max mem: 5511
[11:03:59.063166] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.3746 (0.3885)  acc1: 90.6250 (88.9745)  acc5: 100.0000 (99.4498)  time: 0.0282  data: 0.0002  max mem: 5511
[11:03:59.345142] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.3832 (0.3952)  acc1: 87.5000 (88.6767)  acc5: 100.0000 (99.4599)  time: 0.0280  data: 0.0002  max mem: 5511
[11:03:59.628496] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4130 (0.3920)  acc1: 85.9375 (88.7534)  acc5: 100.0000 (99.4849)  time: 0.0281  data: 0.0002  max mem: 5511
[11:03:59.913137] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4059 (0.3964)  acc1: 89.0625 (88.6293)  acc5: 100.0000 (99.4895)  time: 0.0282  data: 0.0002  max mem: 5511
[11:04:00.195297] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4006 (0.3951)  acc1: 89.0625 (88.7669)  acc5: 100.0000 (99.5073)  time: 0.0282  data: 0.0002  max mem: 5511
[11:04:00.480133] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.3584 (0.3938)  acc1: 89.0625 (88.7784)  acc5: 100.0000 (99.5093)  time: 0.0282  data: 0.0002  max mem: 5511
[11:04:00.762054] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.3744 (0.3957)  acc1: 87.5000 (88.7047)  acc5: 100.0000 (99.4990)  time: 0.0282  data: 0.0002  max mem: 5511
[11:04:01.043962] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3601 (0.3926)  acc1: 89.0625 (88.7633)  acc5: 100.0000 (99.5124)  time: 0.0280  data: 0.0001  max mem: 5511
[11:04:01.322071] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.3600 (0.3920)  acc1: 89.0625 (88.7935)  acc5: 100.0000 (99.5240)  time: 0.0279  data: 0.0001  max mem: 5511
[11:04:01.474394] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.3705 (0.3919)  acc1: 89.0625 (88.7700)  acc5: 100.0000 (99.5300)  time: 0.0270  data: 0.0001  max mem: 5511
[11:04:01.637100] Test: Total time: 0:00:05 (0.0327 s / it)
[11:04:01.637548] * Acc@1 88.770 Acc@5 99.530 loss 0.392
[11:04:01.637841] Accuracy of the network on the 10000 test images: 88.8%
[11:04:01.638048] Max accuracy: 88.96%
[11:04:02.032033] log_dir: ./output_dir
[11:04:02.788467] Epoch: [83]  [  0/781]  eta: 0:09:49  lr: 0.000020  training_loss: 0.2567 (0.2567)  mae_loss: 0.1680 (0.1680)  classification_loss: 0.0887 (0.0887)  time: 0.7546  data: 0.5377  max mem: 5511
[11:04:06.686883] Epoch: [83]  [ 20/781]  eta: 0:02:48  lr: 0.000020  training_loss: 0.3072 (0.3162)  mae_loss: 0.2072 (0.2104)  classification_loss: 0.1033 (0.1057)  time: 0.1948  data: 0.0002  max mem: 5511
[11:04:10.589784] Epoch: [83]  [ 40/781]  eta: 0:02:34  lr: 0.000020  training_loss: 0.3073 (0.3117)  mae_loss: 0.1994 (0.2044)  classification_loss: 0.1061 (0.1073)  time: 0.1951  data: 0.0002  max mem: 5511
[11:04:14.518045] Epoch: [83]  [ 60/781]  eta: 0:02:27  lr: 0.000020  training_loss: 0.3166 (0.3155)  mae_loss: 0.2102 (0.2085)  classification_loss: 0.1064 (0.1071)  time: 0.1963  data: 0.0002  max mem: 5511
[11:04:18.449813] Epoch: [83]  [ 80/781]  eta: 0:02:22  lr: 0.000020  training_loss: 0.2977 (0.3132)  mae_loss: 0.1926 (0.2063)  classification_loss: 0.1087 (0.1069)  time: 0.1965  data: 0.0004  max mem: 5511
[11:04:22.345812] Epoch: [83]  [100/781]  eta: 0:02:16  lr: 0.000020  training_loss: 0.3201 (0.3129)  mae_loss: 0.2096 (0.2056)  classification_loss: 0.1101 (0.1073)  time: 0.1947  data: 0.0002  max mem: 5511
[11:04:26.260031] Epoch: [83]  [120/781]  eta: 0:02:12  lr: 0.000020  training_loss: 0.3006 (0.3113)  mae_loss: 0.1937 (0.2042)  classification_loss: 0.1025 (0.1070)  time: 0.1956  data: 0.0002  max mem: 5511
[11:04:30.177280] Epoch: [83]  [140/781]  eta: 0:02:07  lr: 0.000020  training_loss: 0.3089 (0.3114)  mae_loss: 0.2077 (0.2046)  classification_loss: 0.1076 (0.1068)  time: 0.1958  data: 0.0002  max mem: 5511
[11:04:34.117785] Epoch: [83]  [160/781]  eta: 0:02:03  lr: 0.000020  training_loss: 0.3061 (0.3114)  mae_loss: 0.2036 (0.2047)  classification_loss: 0.1020 (0.1067)  time: 0.1969  data: 0.0002  max mem: 5511
[11:04:38.134153] Epoch: [83]  [180/781]  eta: 0:01:59  lr: 0.000020  training_loss: 0.3066 (0.3113)  mae_loss: 0.2081 (0.2048)  classification_loss: 0.1019 (0.1065)  time: 0.2007  data: 0.0002  max mem: 5511
[11:04:42.027457] Epoch: [83]  [200/781]  eta: 0:01:55  lr: 0.000020  training_loss: 0.3065 (0.3118)  mae_loss: 0.2094 (0.2051)  classification_loss: 0.1054 (0.1067)  time: 0.1946  data: 0.0003  max mem: 5511
[11:04:45.932728] Epoch: [83]  [220/781]  eta: 0:01:51  lr: 0.000020  training_loss: 0.3141 (0.3115)  mae_loss: 0.1946 (0.2045)  classification_loss: 0.1103 (0.1070)  time: 0.1952  data: 0.0002  max mem: 5511
[11:04:49.832145] Epoch: [83]  [240/781]  eta: 0:01:47  lr: 0.000019  training_loss: 0.3135 (0.3121)  mae_loss: 0.2052 (0.2050)  classification_loss: 0.1063 (0.1071)  time: 0.1949  data: 0.0002  max mem: 5511
[11:04:53.791810] Epoch: [83]  [260/781]  eta: 0:01:43  lr: 0.000019  training_loss: 0.2996 (0.3116)  mae_loss: 0.1881 (0.2045)  classification_loss: 0.1101 (0.1071)  time: 0.1979  data: 0.0003  max mem: 5511
[11:04:57.703960] Epoch: [83]  [280/781]  eta: 0:01:39  lr: 0.000019  training_loss: 0.3052 (0.3117)  mae_loss: 0.1999 (0.2048)  classification_loss: 0.1037 (0.1069)  time: 0.1955  data: 0.0002  max mem: 5511
[11:05:01.625452] Epoch: [83]  [300/781]  eta: 0:01:35  lr: 0.000019  training_loss: 0.3035 (0.3118)  mae_loss: 0.1910 (0.2048)  classification_loss: 0.1066 (0.1071)  time: 0.1960  data: 0.0002  max mem: 5511
[11:05:05.519833] Epoch: [83]  [320/781]  eta: 0:01:31  lr: 0.000019  training_loss: 0.3120 (0.3120)  mae_loss: 0.1955 (0.2051)  classification_loss: 0.1039 (0.1069)  time: 0.1946  data: 0.0003  max mem: 5511
[11:05:09.418983] Epoch: [83]  [340/781]  eta: 0:01:27  lr: 0.000019  training_loss: 0.3208 (0.3123)  mae_loss: 0.2149 (0.2055)  classification_loss: 0.1080 (0.1069)  time: 0.1949  data: 0.0002  max mem: 5511
[11:05:13.325167] Epoch: [83]  [360/781]  eta: 0:01:23  lr: 0.000019  training_loss: 0.3103 (0.3125)  mae_loss: 0.2118 (0.2056)  classification_loss: 0.1060 (0.1069)  time: 0.1952  data: 0.0003  max mem: 5511
[11:05:17.235850] Epoch: [83]  [380/781]  eta: 0:01:19  lr: 0.000019  training_loss: 0.3018 (0.3124)  mae_loss: 0.2021 (0.2056)  classification_loss: 0.1036 (0.1069)  time: 0.1954  data: 0.0002  max mem: 5511
[11:05:21.140423] Epoch: [83]  [400/781]  eta: 0:01:15  lr: 0.000019  training_loss: 0.3110 (0.3123)  mae_loss: 0.2031 (0.2055)  classification_loss: 0.1032 (0.1068)  time: 0.1951  data: 0.0003  max mem: 5511
[11:05:25.036854] Epoch: [83]  [420/781]  eta: 0:01:11  lr: 0.000019  training_loss: 0.3066 (0.3121)  mae_loss: 0.1964 (0.2052)  classification_loss: 0.1082 (0.1069)  time: 0.1947  data: 0.0002  max mem: 5511
[11:05:28.964629] Epoch: [83]  [440/781]  eta: 0:01:07  lr: 0.000019  training_loss: 0.3077 (0.3118)  mae_loss: 0.2001 (0.2051)  classification_loss: 0.1052 (0.1068)  time: 0.1963  data: 0.0004  max mem: 5511
[11:05:32.884137] Epoch: [83]  [460/781]  eta: 0:01:03  lr: 0.000019  training_loss: 0.3027 (0.3115)  mae_loss: 0.1971 (0.2048)  classification_loss: 0.1075 (0.1067)  time: 0.1959  data: 0.0002  max mem: 5511
[11:05:36.786829] Epoch: [83]  [480/781]  eta: 0:00:59  lr: 0.000019  training_loss: 0.3048 (0.3115)  mae_loss: 0.1955 (0.2047)  classification_loss: 0.1054 (0.1068)  time: 0.1950  data: 0.0003  max mem: 5511
[11:05:40.693141] Epoch: [83]  [500/781]  eta: 0:00:55  lr: 0.000019  training_loss: 0.3051 (0.3113)  mae_loss: 0.1954 (0.2044)  classification_loss: 0.1097 (0.1069)  time: 0.1952  data: 0.0002  max mem: 5511
[11:05:44.624534] Epoch: [83]  [520/781]  eta: 0:00:51  lr: 0.000019  training_loss: 0.3192 (0.3115)  mae_loss: 0.2057 (0.2046)  classification_loss: 0.1076 (0.1069)  time: 0.1965  data: 0.0002  max mem: 5511
[11:05:48.532007] Epoch: [83]  [540/781]  eta: 0:00:47  lr: 0.000019  training_loss: 0.3070 (0.3116)  mae_loss: 0.2002 (0.2047)  classification_loss: 0.1044 (0.1069)  time: 0.1953  data: 0.0003  max mem: 5511
[11:05:52.482018] Epoch: [83]  [560/781]  eta: 0:00:43  lr: 0.000019  training_loss: 0.3033 (0.3113)  mae_loss: 0.2020 (0.2046)  classification_loss: 0.1033 (0.1068)  time: 0.1974  data: 0.0003  max mem: 5511
[11:05:56.400476] Epoch: [83]  [580/781]  eta: 0:00:39  lr: 0.000019  training_loss: 0.3143 (0.3113)  mae_loss: 0.2087 (0.2046)  classification_loss: 0.1044 (0.1067)  time: 0.1958  data: 0.0003  max mem: 5511
[11:06:00.293696] Epoch: [83]  [600/781]  eta: 0:00:35  lr: 0.000019  training_loss: 0.3109 (0.3114)  mae_loss: 0.1956 (0.2046)  classification_loss: 0.1078 (0.1068)  time: 0.1946  data: 0.0003  max mem: 5511
[11:06:04.195862] Epoch: [83]  [620/781]  eta: 0:00:31  lr: 0.000018  training_loss: 0.3331 (0.3118)  mae_loss: 0.2172 (0.2050)  classification_loss: 0.1092 (0.1068)  time: 0.1950  data: 0.0002  max mem: 5511
[11:06:08.119059] Epoch: [83]  [640/781]  eta: 0:00:27  lr: 0.000018  training_loss: 0.3084 (0.3118)  mae_loss: 0.2018 (0.2051)  classification_loss: 0.1054 (0.1067)  time: 0.1961  data: 0.0003  max mem: 5511
[11:06:12.059994] Epoch: [83]  [660/781]  eta: 0:00:23  lr: 0.000018  training_loss: 0.2983 (0.3115)  mae_loss: 0.1855 (0.2048)  classification_loss: 0.1049 (0.1067)  time: 0.1970  data: 0.0002  max mem: 5511
[11:06:15.967186] Epoch: [83]  [680/781]  eta: 0:00:19  lr: 0.000018  training_loss: 0.3124 (0.3116)  mae_loss: 0.2022 (0.2049)  classification_loss: 0.1085 (0.1068)  time: 0.1953  data: 0.0002  max mem: 5511
[11:06:19.884865] Epoch: [83]  [700/781]  eta: 0:00:15  lr: 0.000018  training_loss: 0.3236 (0.3120)  mae_loss: 0.2072 (0.2050)  classification_loss: 0.1122 (0.1069)  time: 0.1958  data: 0.0002  max mem: 5511
[11:06:23.792552] Epoch: [83]  [720/781]  eta: 0:00:11  lr: 0.000018  training_loss: 0.3183 (0.3120)  mae_loss: 0.2032 (0.2050)  classification_loss: 0.1128 (0.1070)  time: 0.1953  data: 0.0002  max mem: 5511
[11:06:27.702451] Epoch: [83]  [740/781]  eta: 0:00:08  lr: 0.000018  training_loss: 0.3022 (0.3120)  mae_loss: 0.1984 (0.2050)  classification_loss: 0.1049 (0.1070)  time: 0.1954  data: 0.0002  max mem: 5511
[11:06:31.611428] Epoch: [83]  [760/781]  eta: 0:00:04  lr: 0.000018  training_loss: 0.3198 (0.3122)  mae_loss: 0.2096 (0.2051)  classification_loss: 0.1077 (0.1071)  time: 0.1954  data: 0.0004  max mem: 5511
[11:06:35.535435] Epoch: [83]  [780/781]  eta: 0:00:00  lr: 0.000018  training_loss: 0.3076 (0.3121)  mae_loss: 0.2005 (0.2050)  classification_loss: 0.1076 (0.1071)  time: 0.1961  data: 0.0002  max mem: 5511
[11:06:35.706831] Epoch: [83] Total time: 0:02:33 (0.1968 s / it)
[11:06:35.707296] Averaged stats: lr: 0.000018  training_loss: 0.3076 (0.3121)  mae_loss: 0.2005 (0.2050)  classification_loss: 0.1076 (0.1071)
[11:06:36.389468] Test:  [  0/157]  eta: 0:01:46  testing_loss: 0.4806 (0.4806)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.6778  data: 0.6466  max mem: 5511
[11:06:36.672098] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.3885 (0.3905)  acc1: 87.5000 (89.3466)  acc5: 100.0000 (99.5739)  time: 0.0871  data: 0.0589  max mem: 5511
[11:06:36.952612] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.3481 (0.3795)  acc1: 89.0625 (89.7321)  acc5: 100.0000 (99.6280)  time: 0.0280  data: 0.0001  max mem: 5511
[11:06:37.232966] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.3481 (0.3858)  acc1: 90.6250 (89.3145)  acc5: 100.0000 (99.4456)  time: 0.0279  data: 0.0001  max mem: 5511
[11:06:37.514099] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.3998 (0.4018)  acc1: 87.5000 (88.6814)  acc5: 100.0000 (99.3521)  time: 0.0280  data: 0.0001  max mem: 5511
[11:06:37.796786] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.3823 (0.3913)  acc1: 89.0625 (89.0012)  acc5: 100.0000 (99.3873)  time: 0.0281  data: 0.0002  max mem: 5511
[11:06:38.077363] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.3219 (0.3894)  acc1: 90.6250 (88.9857)  acc5: 100.0000 (99.3852)  time: 0.0281  data: 0.0002  max mem: 5511
[11:06:38.359379] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.3542 (0.3842)  acc1: 89.0625 (89.1285)  acc5: 100.0000 (99.4498)  time: 0.0280  data: 0.0002  max mem: 5511
[11:06:38.640300] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.3838 (0.3913)  acc1: 87.5000 (88.8310)  acc5: 100.0000 (99.4599)  time: 0.0280  data: 0.0002  max mem: 5511
[11:06:38.922220] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4157 (0.3881)  acc1: 87.5000 (88.9080)  acc5: 100.0000 (99.4677)  time: 0.0280  data: 0.0002  max mem: 5511
[11:06:39.202735] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4157 (0.3916)  acc1: 89.0625 (88.8614)  acc5: 100.0000 (99.4585)  time: 0.0280  data: 0.0002  max mem: 5511
[11:06:39.484460] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.3946 (0.3901)  acc1: 89.0625 (88.9077)  acc5: 100.0000 (99.4792)  time: 0.0280  data: 0.0002  max mem: 5511
[11:06:39.766853] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.3579 (0.3878)  acc1: 89.0625 (88.9721)  acc5: 100.0000 (99.4706)  time: 0.0281  data: 0.0002  max mem: 5511
[11:06:40.048484] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.3591 (0.3889)  acc1: 89.0625 (88.9552)  acc5: 100.0000 (99.4752)  time: 0.0281  data: 0.0002  max mem: 5511
[11:06:40.329023] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3482 (0.3858)  acc1: 90.6250 (89.0847)  acc5: 100.0000 (99.4902)  time: 0.0280  data: 0.0001  max mem: 5511
[11:06:40.607445] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.3482 (0.3853)  acc1: 90.6250 (89.0832)  acc5: 100.0000 (99.5033)  time: 0.0278  data: 0.0001  max mem: 5511
[11:06:40.758567] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.3600 (0.3856)  acc1: 89.0625 (89.0000)  acc5: 100.0000 (99.5100)  time: 0.0270  data: 0.0001  max mem: 5511
[11:06:40.942851] Test: Total time: 0:00:05 (0.0333 s / it)
[11:06:40.943316] * Acc@1 89.000 Acc@5 99.510 loss 0.386
[11:06:40.943633] Accuracy of the network on the 10000 test images: 89.0%
[11:06:40.943843] Max accuracy: 89.00%
[11:06:41.316040] log_dir: ./output_dir
[11:06:42.173627] Epoch: [84]  [  0/781]  eta: 0:11:08  lr: 0.000018  training_loss: 0.3273 (0.3273)  mae_loss: 0.2294 (0.2294)  classification_loss: 0.0979 (0.0979)  time: 0.8558  data: 0.6444  max mem: 5511
[11:06:46.107808] Epoch: [84]  [ 20/781]  eta: 0:02:53  lr: 0.000018  training_loss: 0.3072 (0.3102)  mae_loss: 0.2032 (0.2074)  classification_loss: 0.1033 (0.1027)  time: 0.1966  data: 0.0003  max mem: 5511
[11:06:50.032221] Epoch: [84]  [ 40/781]  eta: 0:02:37  lr: 0.000018  training_loss: 0.3060 (0.3075)  mae_loss: 0.1928 (0.2031)  classification_loss: 0.1059 (0.1044)  time: 0.1961  data: 0.0002  max mem: 5511
[11:06:53.940055] Epoch: [84]  [ 60/781]  eta: 0:02:29  lr: 0.000018  training_loss: 0.3002 (0.3065)  mae_loss: 0.1967 (0.2013)  classification_loss: 0.1086 (0.1052)  time: 0.1953  data: 0.0002  max mem: 5511
[11:06:57.854340] Epoch: [84]  [ 80/781]  eta: 0:02:23  lr: 0.000018  training_loss: 0.3188 (0.3093)  mae_loss: 0.2129 (0.2029)  classification_loss: 0.1088 (0.1064)  time: 0.1956  data: 0.0003  max mem: 5511
[11:07:01.784338] Epoch: [84]  [100/781]  eta: 0:02:17  lr: 0.000018  training_loss: 0.3174 (0.3103)  mae_loss: 0.2031 (0.2031)  classification_loss: 0.1078 (0.1072)  time: 0.1964  data: 0.0002  max mem: 5511
[11:07:05.719081] Epoch: [84]  [120/781]  eta: 0:02:13  lr: 0.000018  training_loss: 0.3037 (0.3097)  mae_loss: 0.1922 (0.2019)  classification_loss: 0.1094 (0.1078)  time: 0.1967  data: 0.0002  max mem: 5511
[11:07:09.633764] Epoch: [84]  [140/781]  eta: 0:02:08  lr: 0.000018  training_loss: 0.3087 (0.3100)  mae_loss: 0.1992 (0.2024)  classification_loss: 0.1048 (0.1076)  time: 0.1956  data: 0.0003  max mem: 5511
[11:07:13.552540] Epoch: [84]  [160/781]  eta: 0:02:04  lr: 0.000018  training_loss: 0.3065 (0.3105)  mae_loss: 0.2075 (0.2031)  classification_loss: 0.1024 (0.1074)  time: 0.1958  data: 0.0002  max mem: 5511
[11:07:17.444858] Epoch: [84]  [180/781]  eta: 0:01:59  lr: 0.000018  training_loss: 0.3085 (0.3112)  mae_loss: 0.2002 (0.2036)  classification_loss: 0.1061 (0.1076)  time: 0.1945  data: 0.0002  max mem: 5511
[11:07:21.337299] Epoch: [84]  [200/781]  eta: 0:01:55  lr: 0.000017  training_loss: 0.3062 (0.3108)  mae_loss: 0.1946 (0.2032)  classification_loss: 0.1072 (0.1076)  time: 0.1945  data: 0.0003  max mem: 5511
[11:07:25.245987] Epoch: [84]  [220/781]  eta: 0:01:51  lr: 0.000017  training_loss: 0.3115 (0.3106)  mae_loss: 0.2037 (0.2030)  classification_loss: 0.1067 (0.1075)  time: 0.1953  data: 0.0003  max mem: 5511
[11:07:29.185999] Epoch: [84]  [240/781]  eta: 0:01:47  lr: 0.000017  training_loss: 0.3048 (0.3103)  mae_loss: 0.1958 (0.2028)  classification_loss: 0.1066 (0.1075)  time: 0.1969  data: 0.0002  max mem: 5511
[11:07:33.113979] Epoch: [84]  [260/781]  eta: 0:01:43  lr: 0.000017  training_loss: 0.2995 (0.3100)  mae_loss: 0.1917 (0.2026)  classification_loss: 0.1076 (0.1074)  time: 0.1963  data: 0.0002  max mem: 5511
[11:07:37.053492] Epoch: [84]  [280/781]  eta: 0:01:39  lr: 0.000017  training_loss: 0.2915 (0.3091)  mae_loss: 0.1860 (0.2017)  classification_loss: 0.1062 (0.1074)  time: 0.1969  data: 0.0002  max mem: 5511
[11:07:40.961851] Epoch: [84]  [300/781]  eta: 0:01:35  lr: 0.000017  training_loss: 0.3169 (0.3098)  mae_loss: 0.2076 (0.2024)  classification_loss: 0.1055 (0.1074)  time: 0.1953  data: 0.0002  max mem: 5511
[11:07:44.877014] Epoch: [84]  [320/781]  eta: 0:01:31  lr: 0.000017  training_loss: 0.3211 (0.3108)  mae_loss: 0.2103 (0.2034)  classification_loss: 0.1052 (0.1074)  time: 0.1957  data: 0.0002  max mem: 5511
[11:07:48.788022] Epoch: [84]  [340/781]  eta: 0:01:27  lr: 0.000017  training_loss: 0.2992 (0.3106)  mae_loss: 0.1943 (0.2033)  classification_loss: 0.1041 (0.1073)  time: 0.1955  data: 0.0002  max mem: 5511
[11:07:52.671285] Epoch: [84]  [360/781]  eta: 0:01:23  lr: 0.000017  training_loss: 0.3051 (0.3101)  mae_loss: 0.1934 (0.2028)  classification_loss: 0.1075 (0.1073)  time: 0.1941  data: 0.0002  max mem: 5511
[11:07:56.567189] Epoch: [84]  [380/781]  eta: 0:01:19  lr: 0.000017  training_loss: 0.3140 (0.3108)  mae_loss: 0.2070 (0.2035)  classification_loss: 0.1069 (0.1073)  time: 0.1947  data: 0.0002  max mem: 5511
[11:08:00.475365] Epoch: [84]  [400/781]  eta: 0:01:15  lr: 0.000017  training_loss: 0.3091 (0.3107)  mae_loss: 0.1993 (0.2035)  classification_loss: 0.1031 (0.1072)  time: 0.1953  data: 0.0002  max mem: 5511
[11:08:04.364371] Epoch: [84]  [420/781]  eta: 0:01:11  lr: 0.000017  training_loss: 0.3124 (0.3110)  mae_loss: 0.2081 (0.2038)  classification_loss: 0.1038 (0.1072)  time: 0.1944  data: 0.0002  max mem: 5511
[11:08:08.290387] Epoch: [84]  [440/781]  eta: 0:01:07  lr: 0.000017  training_loss: 0.3034 (0.3105)  mae_loss: 0.1960 (0.2034)  classification_loss: 0.1050 (0.1070)  time: 0.1962  data: 0.0002  max mem: 5511
[11:08:12.232296] Epoch: [84]  [460/781]  eta: 0:01:03  lr: 0.000017  training_loss: 0.3035 (0.3106)  mae_loss: 0.1967 (0.2036)  classification_loss: 0.1034 (0.1069)  time: 0.1970  data: 0.0002  max mem: 5511
[11:08:16.159081] Epoch: [84]  [480/781]  eta: 0:00:59  lr: 0.000017  training_loss: 0.2977 (0.3102)  mae_loss: 0.1908 (0.2032)  classification_loss: 0.1074 (0.1070)  time: 0.1963  data: 0.0002  max mem: 5511
[11:08:20.086516] Epoch: [84]  [500/781]  eta: 0:00:55  lr: 0.000017  training_loss: 0.3061 (0.3105)  mae_loss: 0.2033 (0.2035)  classification_loss: 0.1072 (0.1071)  time: 0.1963  data: 0.0003  max mem: 5511
[11:08:23.984014] Epoch: [84]  [520/781]  eta: 0:00:51  lr: 0.000017  training_loss: 0.3025 (0.3104)  mae_loss: 0.1956 (0.2033)  classification_loss: 0.1054 (0.1070)  time: 0.1948  data: 0.0002  max mem: 5511
[11:08:27.899400] Epoch: [84]  [540/781]  eta: 0:00:47  lr: 0.000017  training_loss: 0.3039 (0.3102)  mae_loss: 0.2021 (0.2031)  classification_loss: 0.1054 (0.1070)  time: 0.1957  data: 0.0002  max mem: 5511
[11:08:31.853389] Epoch: [84]  [560/781]  eta: 0:00:43  lr: 0.000017  training_loss: 0.2979 (0.3100)  mae_loss: 0.1983 (0.2031)  classification_loss: 0.1031 (0.1069)  time: 0.1976  data: 0.0002  max mem: 5511
[11:08:35.777766] Epoch: [84]  [580/781]  eta: 0:00:39  lr: 0.000017  training_loss: 0.3046 (0.3096)  mae_loss: 0.1878 (0.2028)  classification_loss: 0.1011 (0.1068)  time: 0.1961  data: 0.0003  max mem: 5511
[11:08:39.706596] Epoch: [84]  [600/781]  eta: 0:00:35  lr: 0.000016  training_loss: 0.3054 (0.3095)  mae_loss: 0.2008 (0.2029)  classification_loss: 0.0983 (0.1066)  time: 0.1964  data: 0.0002  max mem: 5511
[11:08:43.626887] Epoch: [84]  [620/781]  eta: 0:00:31  lr: 0.000016  training_loss: 0.3078 (0.3094)  mae_loss: 0.2082 (0.2030)  classification_loss: 0.1000 (0.1064)  time: 0.1959  data: 0.0002  max mem: 5511
[11:08:47.545510] Epoch: [84]  [640/781]  eta: 0:00:27  lr: 0.000016  training_loss: 0.3066 (0.3095)  mae_loss: 0.1937 (0.2030)  classification_loss: 0.1081 (0.1064)  time: 0.1958  data: 0.0002  max mem: 5511
[11:08:51.442056] Epoch: [84]  [660/781]  eta: 0:00:23  lr: 0.000016  training_loss: 0.3030 (0.3094)  mae_loss: 0.1928 (0.2029)  classification_loss: 0.1092 (0.1065)  time: 0.1948  data: 0.0002  max mem: 5511
[11:08:55.347437] Epoch: [84]  [680/781]  eta: 0:00:19  lr: 0.000016  training_loss: 0.3147 (0.3093)  mae_loss: 0.2017 (0.2028)  classification_loss: 0.1066 (0.1065)  time: 0.1952  data: 0.0002  max mem: 5511
[11:08:59.285586] Epoch: [84]  [700/781]  eta: 0:00:15  lr: 0.000016  training_loss: 0.3164 (0.3094)  mae_loss: 0.2034 (0.2029)  classification_loss: 0.1054 (0.1065)  time: 0.1968  data: 0.0002  max mem: 5511
[11:09:03.210657] Epoch: [84]  [720/781]  eta: 0:00:11  lr: 0.000016  training_loss: 0.3200 (0.3095)  mae_loss: 0.2036 (0.2029)  classification_loss: 0.1132 (0.1067)  time: 0.1961  data: 0.0003  max mem: 5511
[11:09:07.192673] Epoch: [84]  [740/781]  eta: 0:00:08  lr: 0.000016  training_loss: 0.3110 (0.3095)  mae_loss: 0.1993 (0.2027)  classification_loss: 0.1075 (0.1067)  time: 0.1990  data: 0.0002  max mem: 5511
[11:09:11.103396] Epoch: [84]  [760/781]  eta: 0:00:04  lr: 0.000016  training_loss: 0.3021 (0.3096)  mae_loss: 0.1983 (0.2029)  classification_loss: 0.1038 (0.1067)  time: 0.1955  data: 0.0002  max mem: 5511
[11:09:14.995244] Epoch: [84]  [780/781]  eta: 0:00:00  lr: 0.000016  training_loss: 0.3073 (0.3097)  mae_loss: 0.2039 (0.2030)  classification_loss: 0.1072 (0.1068)  time: 0.1945  data: 0.0002  max mem: 5511
[11:09:15.161008] Epoch: [84] Total time: 0:02:33 (0.1970 s / it)
[11:09:15.161531] Averaged stats: lr: 0.000016  training_loss: 0.3073 (0.3097)  mae_loss: 0.2039 (0.2030)  classification_loss: 0.1072 (0.1068)
[11:09:15.830010] Test:  [  0/157]  eta: 0:01:44  testing_loss: 0.4640 (0.4640)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.6637  data: 0.6330  max mem: 5511
[11:09:16.113614] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.3945 (0.3949)  acc1: 87.5000 (88.2102)  acc5: 100.0000 (99.5739)  time: 0.0859  data: 0.0577  max mem: 5511
[11:09:16.393934] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.3603 (0.3841)  acc1: 87.5000 (88.8393)  acc5: 100.0000 (99.5536)  time: 0.0280  data: 0.0002  max mem: 5511
[11:09:16.674261] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.3559 (0.3884)  acc1: 90.6250 (88.9113)  acc5: 100.0000 (99.3952)  time: 0.0279  data: 0.0001  max mem: 5511
[11:09:16.955195] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4039 (0.4015)  acc1: 89.0625 (88.5671)  acc5: 100.0000 (99.3140)  time: 0.0279  data: 0.0002  max mem: 5511
[11:09:17.235879] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.3876 (0.3907)  acc1: 89.0625 (88.9706)  acc5: 100.0000 (99.3566)  time: 0.0280  data: 0.0002  max mem: 5511
[11:09:17.516554] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.3074 (0.3883)  acc1: 90.6250 (89.1650)  acc5: 100.0000 (99.3340)  time: 0.0279  data: 0.0002  max mem: 5511
[11:09:17.799229] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.3763 (0.3846)  acc1: 90.6250 (89.2165)  acc5: 100.0000 (99.3618)  time: 0.0281  data: 0.0002  max mem: 5511
[11:09:18.081496] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.3895 (0.3921)  acc1: 89.0625 (88.9853)  acc5: 100.0000 (99.3441)  time: 0.0281  data: 0.0002  max mem: 5511
[11:09:18.362546] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4075 (0.3888)  acc1: 87.5000 (89.0797)  acc5: 100.0000 (99.3647)  time: 0.0280  data: 0.0002  max mem: 5511
[11:09:18.645262] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4075 (0.3926)  acc1: 89.0625 (88.9078)  acc5: 100.0000 (99.3657)  time: 0.0281  data: 0.0002  max mem: 5511
[11:09:18.933062] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.3850 (0.3915)  acc1: 89.0625 (88.9780)  acc5: 100.0000 (99.3666)  time: 0.0284  data: 0.0002  max mem: 5511
[11:09:19.218863] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.3652 (0.3901)  acc1: 89.0625 (88.9334)  acc5: 100.0000 (99.3802)  time: 0.0286  data: 0.0002  max mem: 5511
[11:09:19.504048] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.3723 (0.3906)  acc1: 89.0625 (88.9074)  acc5: 100.0000 (99.4036)  time: 0.0284  data: 0.0002  max mem: 5511
[11:09:19.792877] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3434 (0.3878)  acc1: 90.6250 (89.0071)  acc5: 100.0000 (99.4348)  time: 0.0285  data: 0.0002  max mem: 5511
[11:09:20.071708] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.3478 (0.3870)  acc1: 90.6250 (89.0625)  acc5: 100.0000 (99.4619)  time: 0.0281  data: 0.0001  max mem: 5511
[11:09:20.224675] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.3526 (0.3865)  acc1: 90.6250 (89.0100)  acc5: 100.0000 (99.4600)  time: 0.0270  data: 0.0001  max mem: 5511
[11:09:20.385733] Test: Total time: 0:00:05 (0.0333 s / it)
[11:09:20.386212] * Acc@1 89.010 Acc@5 99.460 loss 0.386
[11:09:20.386515] Accuracy of the network on the 10000 test images: 89.0%
[11:09:20.386701] Max accuracy: 89.01%
[11:09:20.751630] log_dir: ./output_dir
[11:09:21.543846] Epoch: [85]  [  0/781]  eta: 0:10:17  lr: 0.000016  training_loss: 0.2840 (0.2840)  mae_loss: 0.1897 (0.1897)  classification_loss: 0.0944 (0.0944)  time: 0.7904  data: 0.5729  max mem: 5511
[11:09:25.433220] Epoch: [85]  [ 20/781]  eta: 0:02:49  lr: 0.000016  training_loss: 0.3072 (0.3101)  mae_loss: 0.2031 (0.2055)  classification_loss: 0.1012 (0.1046)  time: 0.1944  data: 0.0002  max mem: 5511
[11:09:29.327034] Epoch: [85]  [ 40/781]  eta: 0:02:34  lr: 0.000016  training_loss: 0.3159 (0.3122)  mae_loss: 0.2101 (0.2068)  classification_loss: 0.1075 (0.1054)  time: 0.1946  data: 0.0003  max mem: 5511
[11:09:33.230212] Epoch: [85]  [ 60/781]  eta: 0:02:27  lr: 0.000016  training_loss: 0.3196 (0.3153)  mae_loss: 0.2062 (0.2091)  classification_loss: 0.1091 (0.1062)  time: 0.1951  data: 0.0002  max mem: 5511
[11:09:37.138924] Epoch: [85]  [ 80/781]  eta: 0:02:21  lr: 0.000016  training_loss: 0.2975 (0.3134)  mae_loss: 0.2019 (0.2074)  classification_loss: 0.1084 (0.1061)  time: 0.1954  data: 0.0002  max mem: 5511
[11:09:41.023934] Epoch: [85]  [100/781]  eta: 0:02:16  lr: 0.000016  training_loss: 0.3089 (0.3131)  mae_loss: 0.2095 (0.2065)  classification_loss: 0.1078 (0.1066)  time: 0.1942  data: 0.0003  max mem: 5511
[11:09:44.929385] Epoch: [85]  [120/781]  eta: 0:02:12  lr: 0.000016  training_loss: 0.2947 (0.3106)  mae_loss: 0.1907 (0.2042)  classification_loss: 0.1040 (0.1063)  time: 0.1952  data: 0.0003  max mem: 5511
[11:09:48.913709] Epoch: [85]  [140/781]  eta: 0:02:07  lr: 0.000016  training_loss: 0.2884 (0.3087)  mae_loss: 0.1953 (0.2027)  classification_loss: 0.1014 (0.1060)  time: 0.1991  data: 0.0003  max mem: 5511
[11:09:52.812707] Epoch: [85]  [160/781]  eta: 0:02:03  lr: 0.000016  training_loss: 0.3065 (0.3088)  mae_loss: 0.2062 (0.2030)  classification_loss: 0.1036 (0.1058)  time: 0.1949  data: 0.0002  max mem: 5511
[11:09:56.765500] Epoch: [85]  [180/781]  eta: 0:01:59  lr: 0.000016  training_loss: 0.3209 (0.3099)  mae_loss: 0.2141 (0.2043)  classification_loss: 0.1044 (0.1056)  time: 0.1976  data: 0.0002  max mem: 5511
[11:10:00.692583] Epoch: [85]  [200/781]  eta: 0:01:55  lr: 0.000016  training_loss: 0.3017 (0.3098)  mae_loss: 0.1985 (0.2040)  classification_loss: 0.1041 (0.1058)  time: 0.1963  data: 0.0002  max mem: 5511
[11:10:04.627495] Epoch: [85]  [220/781]  eta: 0:01:51  lr: 0.000015  training_loss: 0.3026 (0.3092)  mae_loss: 0.1998 (0.2037)  classification_loss: 0.1035 (0.1055)  time: 0.1966  data: 0.0002  max mem: 5511
[11:10:08.534689] Epoch: [85]  [240/781]  eta: 0:01:47  lr: 0.000015  training_loss: 0.3095 (0.3093)  mae_loss: 0.1988 (0.2035)  classification_loss: 0.1107 (0.1058)  time: 0.1953  data: 0.0002  max mem: 5511
[11:10:12.435921] Epoch: [85]  [260/781]  eta: 0:01:43  lr: 0.000015  training_loss: 0.3057 (0.3095)  mae_loss: 0.2033 (0.2033)  classification_loss: 0.1095 (0.1062)  time: 0.1950  data: 0.0002  max mem: 5511
[11:10:16.374152] Epoch: [85]  [280/781]  eta: 0:01:39  lr: 0.000015  training_loss: 0.3217 (0.3101)  mae_loss: 0.2149 (0.2037)  classification_loss: 0.1107 (0.1064)  time: 0.1968  data: 0.0004  max mem: 5511
[11:10:20.307375] Epoch: [85]  [300/781]  eta: 0:01:35  lr: 0.000015  training_loss: 0.3024 (0.3100)  mae_loss: 0.1949 (0.2037)  classification_loss: 0.1036 (0.1063)  time: 0.1966  data: 0.0002  max mem: 5511
[11:10:24.230486] Epoch: [85]  [320/781]  eta: 0:01:31  lr: 0.000015  training_loss: 0.3060 (0.3100)  mae_loss: 0.2040 (0.2040)  classification_loss: 0.1007 (0.1060)  time: 0.1960  data: 0.0003  max mem: 5511
[11:10:28.129303] Epoch: [85]  [340/781]  eta: 0:01:27  lr: 0.000015  training_loss: 0.3134 (0.3105)  mae_loss: 0.2123 (0.2046)  classification_loss: 0.1000 (0.1058)  time: 0.1948  data: 0.0002  max mem: 5511
[11:10:32.030337] Epoch: [85]  [360/781]  eta: 0:01:23  lr: 0.000015  training_loss: 0.3200 (0.3110)  mae_loss: 0.2101 (0.2052)  classification_loss: 0.1038 (0.1058)  time: 0.1950  data: 0.0002  max mem: 5511
[11:10:35.937716] Epoch: [85]  [380/781]  eta: 0:01:19  lr: 0.000015  training_loss: 0.3302 (0.3117)  mae_loss: 0.2117 (0.2058)  classification_loss: 0.1053 (0.1059)  time: 0.1953  data: 0.0002  max mem: 5511
[11:10:39.884291] Epoch: [85]  [400/781]  eta: 0:01:15  lr: 0.000015  training_loss: 0.3255 (0.3122)  mae_loss: 0.2149 (0.2061)  classification_loss: 0.1091 (0.1061)  time: 0.1972  data: 0.0003  max mem: 5511
[11:10:43.784998] Epoch: [85]  [420/781]  eta: 0:01:11  lr: 0.000015  training_loss: 0.3199 (0.3124)  mae_loss: 0.2055 (0.2063)  classification_loss: 0.1073 (0.1062)  time: 0.1950  data: 0.0002  max mem: 5511
[11:10:47.730651] Epoch: [85]  [440/781]  eta: 0:01:07  lr: 0.000015  training_loss: 0.2987 (0.3119)  mae_loss: 0.1860 (0.2057)  classification_loss: 0.1066 (0.1062)  time: 0.1972  data: 0.0003  max mem: 5511
[11:10:51.682835] Epoch: [85]  [460/781]  eta: 0:01:03  lr: 0.000015  training_loss: 0.3155 (0.3118)  mae_loss: 0.1981 (0.2056)  classification_loss: 0.1023 (0.1062)  time: 0.1975  data: 0.0002  max mem: 5511
[11:10:55.670305] Epoch: [85]  [480/781]  eta: 0:00:59  lr: 0.000015  training_loss: 0.3151 (0.3121)  mae_loss: 0.2083 (0.2058)  classification_loss: 0.1064 (0.1063)  time: 0.1993  data: 0.0003  max mem: 5511
[11:10:59.567172] Epoch: [85]  [500/781]  eta: 0:00:55  lr: 0.000015  training_loss: 0.3101 (0.3122)  mae_loss: 0.2053 (0.2058)  classification_loss: 0.1105 (0.1064)  time: 0.1947  data: 0.0003  max mem: 5511
[11:11:03.490780] Epoch: [85]  [520/781]  eta: 0:00:51  lr: 0.000015  training_loss: 0.3104 (0.3124)  mae_loss: 0.2109 (0.2061)  classification_loss: 0.1017 (0.1063)  time: 0.1961  data: 0.0003  max mem: 5511
[11:11:07.412726] Epoch: [85]  [540/781]  eta: 0:00:47  lr: 0.000015  training_loss: 0.3080 (0.3127)  mae_loss: 0.2056 (0.2063)  classification_loss: 0.1067 (0.1064)  time: 0.1960  data: 0.0003  max mem: 5511
[11:11:11.331988] Epoch: [85]  [560/781]  eta: 0:00:43  lr: 0.000015  training_loss: 0.3021 (0.3125)  mae_loss: 0.1997 (0.2063)  classification_loss: 0.1052 (0.1063)  time: 0.1958  data: 0.0002  max mem: 5511
[11:11:15.228025] Epoch: [85]  [580/781]  eta: 0:00:39  lr: 0.000015  training_loss: 0.3002 (0.3124)  mae_loss: 0.1963 (0.2061)  classification_loss: 0.1051 (0.1063)  time: 0.1947  data: 0.0002  max mem: 5511
[11:11:19.126129] Epoch: [85]  [600/781]  eta: 0:00:35  lr: 0.000015  training_loss: 0.3127 (0.3125)  mae_loss: 0.2084 (0.2062)  classification_loss: 0.1050 (0.1063)  time: 0.1948  data: 0.0002  max mem: 5511
[11:11:23.056597] Epoch: [85]  [620/781]  eta: 0:00:31  lr: 0.000014  training_loss: 0.3098 (0.3126)  mae_loss: 0.2041 (0.2063)  classification_loss: 0.1033 (0.1063)  time: 0.1965  data: 0.0002  max mem: 5511
[11:11:26.950846] Epoch: [85]  [640/781]  eta: 0:00:27  lr: 0.000014  training_loss: 0.3061 (0.3124)  mae_loss: 0.1973 (0.2061)  classification_loss: 0.1034 (0.1062)  time: 0.1946  data: 0.0002  max mem: 5511
[11:11:30.842942] Epoch: [85]  [660/781]  eta: 0:00:23  lr: 0.000014  training_loss: 0.3061 (0.3123)  mae_loss: 0.1982 (0.2061)  classification_loss: 0.1070 (0.1062)  time: 0.1945  data: 0.0003  max mem: 5511
[11:11:34.736239] Epoch: [85]  [680/781]  eta: 0:00:19  lr: 0.000014  training_loss: 0.3177 (0.3124)  mae_loss: 0.2091 (0.2061)  classification_loss: 0.1085 (0.1063)  time: 0.1946  data: 0.0002  max mem: 5511
[11:11:38.665836] Epoch: [85]  [700/781]  eta: 0:00:15  lr: 0.000014  training_loss: 0.3164 (0.3125)  mae_loss: 0.1965 (0.2062)  classification_loss: 0.1049 (0.1063)  time: 0.1964  data: 0.0003  max mem: 5511
[11:11:42.563258] Epoch: [85]  [720/781]  eta: 0:00:11  lr: 0.000014  training_loss: 0.3057 (0.3124)  mae_loss: 0.2066 (0.2062)  classification_loss: 0.1020 (0.1062)  time: 0.1948  data: 0.0002  max mem: 5511
[11:11:46.473663] Epoch: [85]  [740/781]  eta: 0:00:08  lr: 0.000014  training_loss: 0.3098 (0.3123)  mae_loss: 0.2060 (0.2062)  classification_loss: 0.1035 (0.1061)  time: 0.1954  data: 0.0002  max mem: 5511
[11:11:50.388758] Epoch: [85]  [760/781]  eta: 0:00:04  lr: 0.000014  training_loss: 0.3202 (0.3125)  mae_loss: 0.2042 (0.2063)  classification_loss: 0.1025 (0.1062)  time: 0.1957  data: 0.0002  max mem: 5511
[11:11:54.270864] Epoch: [85]  [780/781]  eta: 0:00:00  lr: 0.000014  training_loss: 0.3049 (0.3124)  mae_loss: 0.1991 (0.2062)  classification_loss: 0.1081 (0.1061)  time: 0.1940  data: 0.0001  max mem: 5511
[11:11:54.430738] Epoch: [85] Total time: 0:02:33 (0.1968 s / it)
[11:11:54.431276] Averaged stats: lr: 0.000014  training_loss: 0.3049 (0.3124)  mae_loss: 0.1991 (0.2062)  classification_loss: 0.1081 (0.1061)
[11:11:55.051751] Test:  [  0/157]  eta: 0:01:36  testing_loss: 0.4765 (0.4765)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.6159  data: 0.5864  max mem: 5511
[11:11:55.352131] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.4005 (0.3946)  acc1: 87.5000 (87.7841)  acc5: 100.0000 (99.4318)  time: 0.0831  data: 0.0535  max mem: 5511
[11:11:55.637052] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.3833 (0.3812)  acc1: 87.5000 (88.6905)  acc5: 100.0000 (99.5536)  time: 0.0291  data: 0.0002  max mem: 5511
[11:11:55.920464] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.3456 (0.3866)  acc1: 90.6250 (88.4073)  acc5: 100.0000 (99.4456)  time: 0.0283  data: 0.0002  max mem: 5511
[11:11:56.204377] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.4065 (0.4004)  acc1: 89.0625 (88.1098)  acc5: 100.0000 (99.3902)  time: 0.0282  data: 0.0002  max mem: 5511
[11:11:56.485722] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.3763 (0.3898)  acc1: 89.0625 (88.5110)  acc5: 100.0000 (99.3260)  time: 0.0281  data: 0.0002  max mem: 5511
[11:11:56.771893] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.3198 (0.3876)  acc1: 90.6250 (88.7807)  acc5: 100.0000 (99.3596)  time: 0.0283  data: 0.0002  max mem: 5511
[11:11:57.055760] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.3451 (0.3823)  acc1: 90.6250 (88.9525)  acc5: 100.0000 (99.4058)  time: 0.0284  data: 0.0002  max mem: 5511
[11:11:57.339257] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.3795 (0.3891)  acc1: 89.0625 (88.7924)  acc5: 100.0000 (99.4020)  time: 0.0282  data: 0.0001  max mem: 5511
[11:11:57.627580] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4024 (0.3844)  acc1: 89.0625 (89.0110)  acc5: 100.0000 (99.4162)  time: 0.0284  data: 0.0001  max mem: 5511
[11:11:57.908326] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4010 (0.3880)  acc1: 90.6250 (88.8923)  acc5: 100.0000 (99.4276)  time: 0.0283  data: 0.0002  max mem: 5511
[11:11:58.189060] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.3903 (0.3864)  acc1: 89.0625 (89.0062)  acc5: 100.0000 (99.4088)  time: 0.0279  data: 0.0002  max mem: 5511
[11:11:58.469843] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.3555 (0.3850)  acc1: 89.0625 (88.9592)  acc5: 100.0000 (99.4318)  time: 0.0280  data: 0.0002  max mem: 5511
[11:11:58.751725] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.3555 (0.3858)  acc1: 89.0625 (88.8836)  acc5: 100.0000 (99.4036)  time: 0.0280  data: 0.0002  max mem: 5511
[11:11:59.034133] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3438 (0.3826)  acc1: 89.0625 (89.0293)  acc5: 100.0000 (99.4348)  time: 0.0281  data: 0.0002  max mem: 5511
[11:11:59.313412] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.3494 (0.3824)  acc1: 92.1875 (89.0625)  acc5: 100.0000 (99.4516)  time: 0.0280  data: 0.0001  max mem: 5511
[11:11:59.466085] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.3575 (0.3819)  acc1: 90.6250 (89.0300)  acc5: 100.0000 (99.4400)  time: 0.0270  data: 0.0001  max mem: 5511
[11:11:59.613856] Test: Total time: 0:00:05 (0.0330 s / it)
[11:11:59.615178] * Acc@1 89.030 Acc@5 99.440 loss 0.382
[11:11:59.615559] Accuracy of the network on the 10000 test images: 89.0%
[11:11:59.615753] Max accuracy: 89.03%
[11:12:00.183396] log_dir: ./output_dir
[11:12:00.984050] Epoch: [86]  [  0/781]  eta: 0:10:23  lr: 0.000014  training_loss: 0.3307 (0.3307)  mae_loss: 0.2284 (0.2284)  classification_loss: 0.1023 (0.1023)  time: 0.7983  data: 0.5848  max mem: 5511
[11:12:04.889029] Epoch: [86]  [ 20/781]  eta: 0:02:50  lr: 0.000014  training_loss: 0.2956 (0.3093)  mae_loss: 0.2026 (0.2041)  classification_loss: 0.1027 (0.1052)  time: 0.1952  data: 0.0002  max mem: 5511
[11:12:08.876567] Epoch: [86]  [ 40/781]  eta: 0:02:37  lr: 0.000014  training_loss: 0.3020 (0.3080)  mae_loss: 0.2022 (0.2025)  classification_loss: 0.1059 (0.1055)  time: 0.1993  data: 0.0003  max mem: 5511
[11:12:12.782543] Epoch: [86]  [ 60/781]  eta: 0:02:28  lr: 0.000014  training_loss: 0.3132 (0.3116)  mae_loss: 0.2017 (0.2055)  classification_loss: 0.1074 (0.1061)  time: 0.1952  data: 0.0002  max mem: 5511
[11:12:16.703302] Epoch: [86]  [ 80/781]  eta: 0:02:22  lr: 0.000014  training_loss: 0.3038 (0.3107)  mae_loss: 0.2041 (0.2049)  classification_loss: 0.1029 (0.1058)  time: 0.1960  data: 0.0002  max mem: 5511
[11:12:20.605034] Epoch: [86]  [100/781]  eta: 0:02:17  lr: 0.000014  training_loss: 0.3120 (0.3107)  mae_loss: 0.1923 (0.2041)  classification_loss: 0.1089 (0.1066)  time: 0.1949  data: 0.0002  max mem: 5511
[11:12:24.504180] Epoch: [86]  [120/781]  eta: 0:02:12  lr: 0.000014  training_loss: 0.2933 (0.3097)  mae_loss: 0.1867 (0.2033)  classification_loss: 0.1044 (0.1064)  time: 0.1949  data: 0.0001  max mem: 5511
[11:12:28.415743] Epoch: [86]  [140/781]  eta: 0:02:08  lr: 0.000014  training_loss: 0.3039 (0.3091)  mae_loss: 0.2001 (0.2034)  classification_loss: 0.1031 (0.1057)  time: 0.1955  data: 0.0003  max mem: 5511
[11:12:32.474961] Epoch: [86]  [160/781]  eta: 0:02:04  lr: 0.000014  training_loss: 0.3018 (0.3090)  mae_loss: 0.2024 (0.2038)  classification_loss: 0.1014 (0.1053)  time: 0.2028  data: 0.0003  max mem: 5511
[11:12:36.397196] Epoch: [86]  [180/781]  eta: 0:02:00  lr: 0.000014  training_loss: 0.3015 (0.3085)  mae_loss: 0.1997 (0.2034)  classification_loss: 0.1057 (0.1051)  time: 0.1960  data: 0.0002  max mem: 5511
[11:12:40.301389] Epoch: [86]  [200/781]  eta: 0:01:55  lr: 0.000014  training_loss: 0.3068 (0.3085)  mae_loss: 0.1988 (0.2035)  classification_loss: 0.1041 (0.1050)  time: 0.1951  data: 0.0004  max mem: 5511
[11:12:44.233865] Epoch: [86]  [220/781]  eta: 0:01:51  lr: 0.000014  training_loss: 0.2990 (0.3077)  mae_loss: 0.1911 (0.2026)  classification_loss: 0.1058 (0.1051)  time: 0.1966  data: 0.0002  max mem: 5511
[11:12:48.142442] Epoch: [86]  [240/781]  eta: 0:01:47  lr: 0.000014  training_loss: 0.2980 (0.3080)  mae_loss: 0.2007 (0.2027)  classification_loss: 0.1039 (0.1052)  time: 0.1953  data: 0.0002  max mem: 5511
[11:12:52.045637] Epoch: [86]  [260/781]  eta: 0:01:43  lr: 0.000014  training_loss: 0.3181 (0.3086)  mae_loss: 0.2075 (0.2032)  classification_loss: 0.1069 (0.1054)  time: 0.1951  data: 0.0002  max mem: 5511
[11:12:55.950169] Epoch: [86]  [280/781]  eta: 0:01:39  lr: 0.000013  training_loss: 0.3034 (0.3082)  mae_loss: 0.1925 (0.2028)  classification_loss: 0.1065 (0.1054)  time: 0.1951  data: 0.0002  max mem: 5511
[11:12:59.863149] Epoch: [86]  [300/781]  eta: 0:01:35  lr: 0.000013  training_loss: 0.3060 (0.3082)  mae_loss: 0.1973 (0.2030)  classification_loss: 0.1046 (0.1052)  time: 0.1956  data: 0.0002  max mem: 5511
[11:13:03.769140] Epoch: [86]  [320/781]  eta: 0:01:31  lr: 0.000013  training_loss: 0.3096 (0.3084)  mae_loss: 0.2081 (0.2033)  classification_loss: 0.1022 (0.1051)  time: 0.1952  data: 0.0001  max mem: 5511
[11:13:07.709967] Epoch: [86]  [340/781]  eta: 0:01:27  lr: 0.000013  training_loss: 0.3133 (0.3087)  mae_loss: 0.2073 (0.2035)  classification_loss: 0.1034 (0.1053)  time: 0.1970  data: 0.0003  max mem: 5511
[11:13:11.619013] Epoch: [86]  [360/781]  eta: 0:01:23  lr: 0.000013  training_loss: 0.3059 (0.3090)  mae_loss: 0.1977 (0.2037)  classification_loss: 0.1079 (0.1053)  time: 0.1953  data: 0.0003  max mem: 5511
[11:13:15.523176] Epoch: [86]  [380/781]  eta: 0:01:19  lr: 0.000013  training_loss: 0.3083 (0.3091)  mae_loss: 0.2008 (0.2038)  classification_loss: 0.1029 (0.1053)  time: 0.1951  data: 0.0002  max mem: 5511
[11:13:19.449911] Epoch: [86]  [400/781]  eta: 0:01:15  lr: 0.000013  training_loss: 0.3086 (0.3092)  mae_loss: 0.2051 (0.2038)  classification_loss: 0.1078 (0.1054)  time: 0.1963  data: 0.0002  max mem: 5511
[11:13:23.363357] Epoch: [86]  [420/781]  eta: 0:01:11  lr: 0.000013  training_loss: 0.3174 (0.3095)  mae_loss: 0.2100 (0.2040)  classification_loss: 0.1043 (0.1055)  time: 0.1956  data: 0.0003  max mem: 5511
[11:13:27.307057] Epoch: [86]  [440/781]  eta: 0:01:07  lr: 0.000013  training_loss: 0.3195 (0.3099)  mae_loss: 0.2112 (0.2044)  classification_loss: 0.1025 (0.1055)  time: 0.1971  data: 0.0003  max mem: 5511
[11:13:31.201119] Epoch: [86]  [460/781]  eta: 0:01:03  lr: 0.000013  training_loss: 0.3193 (0.3100)  mae_loss: 0.2146 (0.2045)  classification_loss: 0.1032 (0.1055)  time: 0.1946  data: 0.0002  max mem: 5511
[11:13:35.123649] Epoch: [86]  [480/781]  eta: 0:00:59  lr: 0.000013  training_loss: 0.3060 (0.3100)  mae_loss: 0.2041 (0.2046)  classification_loss: 0.1009 (0.1055)  time: 0.1960  data: 0.0002  max mem: 5511
[11:13:39.117763] Epoch: [86]  [500/781]  eta: 0:00:55  lr: 0.000013  training_loss: 0.3012 (0.3098)  mae_loss: 0.1865 (0.2044)  classification_loss: 0.1033 (0.1054)  time: 0.1996  data: 0.0002  max mem: 5511
[11:13:43.027792] Epoch: [86]  [520/781]  eta: 0:00:51  lr: 0.000013  training_loss: 0.3134 (0.3101)  mae_loss: 0.2044 (0.2046)  classification_loss: 0.1054 (0.1055)  time: 0.1954  data: 0.0004  max mem: 5511
[11:13:46.992565] Epoch: [86]  [540/781]  eta: 0:00:47  lr: 0.000013  training_loss: 0.3082 (0.3102)  mae_loss: 0.2037 (0.2048)  classification_loss: 0.1045 (0.1055)  time: 0.1982  data: 0.0004  max mem: 5511
[11:13:50.897952] Epoch: [86]  [560/781]  eta: 0:00:43  lr: 0.000013  training_loss: 0.3066 (0.3103)  mae_loss: 0.2030 (0.2048)  classification_loss: 0.1056 (0.1055)  time: 0.1952  data: 0.0002  max mem: 5511
[11:13:54.805246] Epoch: [86]  [580/781]  eta: 0:00:39  lr: 0.000013  training_loss: 0.3071 (0.3101)  mae_loss: 0.1986 (0.2046)  classification_loss: 0.1044 (0.1055)  time: 0.1953  data: 0.0002  max mem: 5511
[11:13:58.701944] Epoch: [86]  [600/781]  eta: 0:00:35  lr: 0.000013  training_loss: 0.3203 (0.3101)  mae_loss: 0.2072 (0.2046)  classification_loss: 0.1045 (0.1056)  time: 0.1948  data: 0.0005  max mem: 5511
[11:14:02.603252] Epoch: [86]  [620/781]  eta: 0:00:31  lr: 0.000013  training_loss: 0.3150 (0.3103)  mae_loss: 0.2101 (0.2047)  classification_loss: 0.1017 (0.1056)  time: 0.1950  data: 0.0002  max mem: 5511
[11:14:06.513639] Epoch: [86]  [640/781]  eta: 0:00:27  lr: 0.000013  training_loss: 0.3056 (0.3102)  mae_loss: 0.1983 (0.2047)  classification_loss: 0.1052 (0.1055)  time: 0.1954  data: 0.0002  max mem: 5511
[11:14:10.424195] Epoch: [86]  [660/781]  eta: 0:00:23  lr: 0.000013  training_loss: 0.2982 (0.3101)  mae_loss: 0.1815 (0.2046)  classification_loss: 0.1030 (0.1055)  time: 0.1954  data: 0.0002  max mem: 5511
[11:14:14.346863] Epoch: [86]  [680/781]  eta: 0:00:19  lr: 0.000013  training_loss: 0.3053 (0.3099)  mae_loss: 0.1994 (0.2045)  classification_loss: 0.1054 (0.1055)  time: 0.1961  data: 0.0002  max mem: 5511
[11:14:18.295745] Epoch: [86]  [700/781]  eta: 0:00:15  lr: 0.000013  training_loss: 0.3148 (0.3103)  mae_loss: 0.2127 (0.2047)  classification_loss: 0.1051 (0.1056)  time: 0.1973  data: 0.0003  max mem: 5511
[11:14:22.218011] Epoch: [86]  [720/781]  eta: 0:00:12  lr: 0.000012  training_loss: 0.2967 (0.3102)  mae_loss: 0.1938 (0.2046)  classification_loss: 0.1037 (0.1056)  time: 0.1960  data: 0.0004  max mem: 5511
[11:14:26.129759] Epoch: [86]  [740/781]  eta: 0:00:08  lr: 0.000012  training_loss: 0.3143 (0.3102)  mae_loss: 0.2067 (0.2045)  classification_loss: 0.1076 (0.1057)  time: 0.1955  data: 0.0002  max mem: 5511
[11:14:30.056427] Epoch: [86]  [760/781]  eta: 0:00:04  lr: 0.000012  training_loss: 0.2986 (0.3102)  mae_loss: 0.2014 (0.2044)  classification_loss: 0.1088 (0.1057)  time: 0.1963  data: 0.0003  max mem: 5511
[11:14:34.015246] Epoch: [86]  [780/781]  eta: 0:00:00  lr: 0.000012  training_loss: 0.3019 (0.3100)  mae_loss: 0.1963 (0.2043)  classification_loss: 0.1033 (0.1057)  time: 0.1979  data: 0.0002  max mem: 5511
[11:14:34.177220] Epoch: [86] Total time: 0:02:33 (0.1972 s / it)
[11:14:34.177997] Averaged stats: lr: 0.000012  training_loss: 0.3019 (0.3100)  mae_loss: 0.1963 (0.2043)  classification_loss: 0.1033 (0.1057)
[11:14:34.745368] Test:  [  0/157]  eta: 0:01:28  testing_loss: 0.4498 (0.4498)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.5633  data: 0.5320  max mem: 5511
[11:14:35.032633] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.3980 (0.3931)  acc1: 87.5000 (88.0682)  acc5: 100.0000 (99.5739)  time: 0.0767  data: 0.0485  max mem: 5511
[11:14:35.315126] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.3529 (0.3787)  acc1: 89.0625 (88.9137)  acc5: 100.0000 (99.6280)  time: 0.0281  data: 0.0002  max mem: 5511
[11:14:35.602806] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.3458 (0.3828)  acc1: 90.6250 (88.7097)  acc5: 100.0000 (99.4960)  time: 0.0283  data: 0.0002  max mem: 5511
[11:14:35.894779] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.3988 (0.3996)  acc1: 89.0625 (88.3003)  acc5: 100.0000 (99.3140)  time: 0.0288  data: 0.0005  max mem: 5511
[11:14:36.181486] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.3898 (0.3890)  acc1: 89.0625 (88.7561)  acc5: 98.4375 (99.2953)  time: 0.0288  data: 0.0007  max mem: 5511
[11:14:36.462807] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.3162 (0.3869)  acc1: 90.6250 (88.9088)  acc5: 100.0000 (99.3084)  time: 0.0283  data: 0.0003  max mem: 5511
[11:14:36.752025] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.3518 (0.3822)  acc1: 90.6250 (89.0625)  acc5: 100.0000 (99.3838)  time: 0.0284  data: 0.0002  max mem: 5511
[11:14:37.036394] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.3905 (0.3894)  acc1: 89.0625 (88.8696)  acc5: 100.0000 (99.3827)  time: 0.0285  data: 0.0003  max mem: 5511
[11:14:37.317278] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4103 (0.3865)  acc1: 89.0625 (89.0110)  acc5: 100.0000 (99.3990)  time: 0.0282  data: 0.0003  max mem: 5511
[11:14:37.605717] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4043 (0.3901)  acc1: 89.0625 (88.8304)  acc5: 100.0000 (99.4276)  time: 0.0283  data: 0.0002  max mem: 5511
[11:14:37.888142] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.3805 (0.3888)  acc1: 90.6250 (88.9921)  acc5: 100.0000 (99.4369)  time: 0.0284  data: 0.0002  max mem: 5511
[11:14:38.176357] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.3669 (0.3873)  acc1: 90.6250 (88.9979)  acc5: 100.0000 (99.4318)  time: 0.0284  data: 0.0002  max mem: 5511
[11:14:38.464072] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.3668 (0.3887)  acc1: 89.0625 (88.8955)  acc5: 100.0000 (99.4156)  time: 0.0286  data: 0.0002  max mem: 5511
[11:14:38.751519] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3475 (0.3854)  acc1: 89.0625 (88.9628)  acc5: 100.0000 (99.4459)  time: 0.0286  data: 0.0002  max mem: 5511
[11:14:39.031283] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.3439 (0.3849)  acc1: 90.6250 (89.0211)  acc5: 100.0000 (99.4619)  time: 0.0282  data: 0.0001  max mem: 5511
[11:14:39.184669] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.3502 (0.3853)  acc1: 90.6250 (88.9700)  acc5: 100.0000 (99.4600)  time: 0.0271  data: 0.0001  max mem: 5511
[11:14:39.348310] Test: Total time: 0:00:05 (0.0329 s / it)
[11:14:39.348771] * Acc@1 88.970 Acc@5 99.460 loss 0.385
[11:14:39.349085] Accuracy of the network on the 10000 test images: 89.0%
[11:14:39.349295] Max accuracy: 89.03%
[11:14:39.601094] log_dir: ./output_dir
[11:14:40.544155] Epoch: [87]  [  0/781]  eta: 0:12:15  lr: 0.000012  training_loss: 0.2969 (0.2969)  mae_loss: 0.1969 (0.1969)  classification_loss: 0.0999 (0.0999)  time: 0.9413  data: 0.7165  max mem: 5511
[11:14:44.453864] Epoch: [87]  [ 20/781]  eta: 0:02:55  lr: 0.000012  training_loss: 0.3019 (0.3102)  mae_loss: 0.1984 (0.2067)  classification_loss: 0.1040 (0.1034)  time: 0.1954  data: 0.0002  max mem: 5511
[11:14:48.356135] Epoch: [87]  [ 40/781]  eta: 0:02:38  lr: 0.000012  training_loss: 0.3109 (0.3098)  mae_loss: 0.2008 (0.2042)  classification_loss: 0.1053 (0.1056)  time: 0.1950  data: 0.0002  max mem: 5511
[11:14:52.294341] Epoch: [87]  [ 60/781]  eta: 0:02:29  lr: 0.000012  training_loss: 0.3124 (0.3099)  mae_loss: 0.2044 (0.2046)  classification_loss: 0.1066 (0.1053)  time: 0.1968  data: 0.0007  max mem: 5511
[11:14:56.288840] Epoch: [87]  [ 80/781]  eta: 0:02:24  lr: 0.000012  training_loss: 0.3109 (0.3093)  mae_loss: 0.2075 (0.2049)  classification_loss: 0.1006 (0.1044)  time: 0.1997  data: 0.0003  max mem: 5511
[11:15:00.189543] Epoch: [87]  [100/781]  eta: 0:02:18  lr: 0.000012  training_loss: 0.3030 (0.3085)  mae_loss: 0.1982 (0.2032)  classification_loss: 0.1050 (0.1053)  time: 0.1950  data: 0.0002  max mem: 5511
[11:15:04.101474] Epoch: [87]  [120/781]  eta: 0:02:13  lr: 0.000012  training_loss: 0.3112 (0.3095)  mae_loss: 0.2007 (0.2041)  classification_loss: 0.1083 (0.1054)  time: 0.1955  data: 0.0002  max mem: 5511
[11:15:08.001271] Epoch: [87]  [140/781]  eta: 0:02:09  lr: 0.000012  training_loss: 0.3056 (0.3092)  mae_loss: 0.2012 (0.2039)  classification_loss: 0.1044 (0.1052)  time: 0.1949  data: 0.0002  max mem: 5511
[11:15:11.911092] Epoch: [87]  [160/781]  eta: 0:02:04  lr: 0.000012  training_loss: 0.3074 (0.3094)  mae_loss: 0.2038 (0.2045)  classification_loss: 0.1003 (0.1049)  time: 0.1954  data: 0.0001  max mem: 5511
[11:15:15.812475] Epoch: [87]  [180/781]  eta: 0:02:00  lr: 0.000012  training_loss: 0.3087 (0.3091)  mae_loss: 0.2066 (0.2043)  classification_loss: 0.1042 (0.1048)  time: 0.1950  data: 0.0002  max mem: 5511
[11:15:19.732961] Epoch: [87]  [200/781]  eta: 0:01:55  lr: 0.000012  training_loss: 0.2910 (0.3085)  mae_loss: 0.1901 (0.2036)  classification_loss: 0.1070 (0.1049)  time: 0.1959  data: 0.0002  max mem: 5511
[11:15:23.653324] Epoch: [87]  [220/781]  eta: 0:01:51  lr: 0.000012  training_loss: 0.3033 (0.3083)  mae_loss: 0.1920 (0.2031)  classification_loss: 0.1060 (0.1051)  time: 0.1959  data: 0.0003  max mem: 5511
[11:15:27.558454] Epoch: [87]  [240/781]  eta: 0:01:47  lr: 0.000012  training_loss: 0.3122 (0.3092)  mae_loss: 0.2071 (0.2038)  classification_loss: 0.1077 (0.1054)  time: 0.1952  data: 0.0004  max mem: 5511
[11:15:31.507145] Epoch: [87]  [260/781]  eta: 0:01:43  lr: 0.000012  training_loss: 0.3199 (0.3098)  mae_loss: 0.2170 (0.2044)  classification_loss: 0.1067 (0.1054)  time: 0.1974  data: 0.0002  max mem: 5511
[11:15:35.430728] Epoch: [87]  [280/781]  eta: 0:01:39  lr: 0.000012  training_loss: 0.3022 (0.3096)  mae_loss: 0.1944 (0.2040)  classification_loss: 0.1078 (0.1056)  time: 0.1961  data: 0.0002  max mem: 5511
[11:15:39.330626] Epoch: [87]  [300/781]  eta: 0:01:35  lr: 0.000012  training_loss: 0.3065 (0.3096)  mae_loss: 0.2025 (0.2041)  classification_loss: 0.1040 (0.1055)  time: 0.1949  data: 0.0003  max mem: 5511
[11:15:43.312777] Epoch: [87]  [320/781]  eta: 0:01:31  lr: 0.000012  training_loss: 0.3111 (0.3096)  mae_loss: 0.2059 (0.2040)  classification_loss: 0.1030 (0.1056)  time: 0.1990  data: 0.0002  max mem: 5511
[11:15:47.296713] Epoch: [87]  [340/781]  eta: 0:01:27  lr: 0.000012  training_loss: 0.3113 (0.3097)  mae_loss: 0.2090 (0.2042)  classification_loss: 0.1036 (0.1055)  time: 0.1991  data: 0.0002  max mem: 5511
[11:15:51.217336] Epoch: [87]  [360/781]  eta: 0:01:23  lr: 0.000012  training_loss: 0.3048 (0.3099)  mae_loss: 0.1986 (0.2042)  classification_loss: 0.1016 (0.1056)  time: 0.1959  data: 0.0003  max mem: 5511
[11:15:55.127037] Epoch: [87]  [380/781]  eta: 0:01:19  lr: 0.000012  training_loss: 0.3038 (0.3098)  mae_loss: 0.1958 (0.2042)  classification_loss: 0.1008 (0.1056)  time: 0.1954  data: 0.0002  max mem: 5511
[11:15:59.027489] Epoch: [87]  [400/781]  eta: 0:01:15  lr: 0.000011  training_loss: 0.2959 (0.3098)  mae_loss: 0.1970 (0.2041)  classification_loss: 0.1068 (0.1056)  time: 0.1949  data: 0.0002  max mem: 5511
[11:16:02.930274] Epoch: [87]  [420/781]  eta: 0:01:11  lr: 0.000011  training_loss: 0.3046 (0.3099)  mae_loss: 0.2009 (0.2043)  classification_loss: 0.1055 (0.1056)  time: 0.1950  data: 0.0003  max mem: 5511
[11:16:06.841733] Epoch: [87]  [440/781]  eta: 0:01:07  lr: 0.000011  training_loss: 0.3024 (0.3096)  mae_loss: 0.1947 (0.2041)  classification_loss: 0.1027 (0.1055)  time: 0.1954  data: 0.0002  max mem: 5511
[11:16:10.762044] Epoch: [87]  [460/781]  eta: 0:01:03  lr: 0.000011  training_loss: 0.2978 (0.3098)  mae_loss: 0.1918 (0.2043)  classification_loss: 0.1051 (0.1054)  time: 0.1959  data: 0.0002  max mem: 5511
[11:16:14.653979] Epoch: [87]  [480/781]  eta: 0:00:59  lr: 0.000011  training_loss: 0.3021 (0.3098)  mae_loss: 0.1996 (0.2043)  classification_loss: 0.1060 (0.1055)  time: 0.1945  data: 0.0003  max mem: 5511
[11:16:18.584489] Epoch: [87]  [500/781]  eta: 0:00:55  lr: 0.000011  training_loss: 0.3157 (0.3101)  mae_loss: 0.2132 (0.2046)  classification_loss: 0.1065 (0.1055)  time: 0.1964  data: 0.0002  max mem: 5511
[11:16:22.500902] Epoch: [87]  [520/781]  eta: 0:00:51  lr: 0.000011  training_loss: 0.3065 (0.3101)  mae_loss: 0.2036 (0.2046)  classification_loss: 0.1044 (0.1055)  time: 0.1957  data: 0.0004  max mem: 5511
[11:16:26.432307] Epoch: [87]  [540/781]  eta: 0:00:47  lr: 0.000011  training_loss: 0.3136 (0.3102)  mae_loss: 0.2122 (0.2048)  classification_loss: 0.1032 (0.1054)  time: 0.1964  data: 0.0002  max mem: 5511
[11:16:30.326173] Epoch: [87]  [560/781]  eta: 0:00:43  lr: 0.000011  training_loss: 0.3007 (0.3101)  mae_loss: 0.2021 (0.2047)  classification_loss: 0.1045 (0.1054)  time: 0.1946  data: 0.0003  max mem: 5511
[11:16:34.245285] Epoch: [87]  [580/781]  eta: 0:00:39  lr: 0.000011  training_loss: 0.3023 (0.3101)  mae_loss: 0.1990 (0.2046)  classification_loss: 0.1067 (0.1055)  time: 0.1959  data: 0.0002  max mem: 5511
[11:16:38.205474] Epoch: [87]  [600/781]  eta: 0:00:35  lr: 0.000011  training_loss: 0.3069 (0.3100)  mae_loss: 0.2011 (0.2046)  classification_loss: 0.1024 (0.1054)  time: 0.1979  data: 0.0003  max mem: 5511
[11:16:42.120107] Epoch: [87]  [620/781]  eta: 0:00:31  lr: 0.000011  training_loss: 0.3145 (0.3101)  mae_loss: 0.2064 (0.2047)  classification_loss: 0.1018 (0.1054)  time: 0.1956  data: 0.0003  max mem: 5511
[11:16:46.018797] Epoch: [87]  [640/781]  eta: 0:00:27  lr: 0.000011  training_loss: 0.2994 (0.3097)  mae_loss: 0.1936 (0.2044)  classification_loss: 0.1036 (0.1053)  time: 0.1949  data: 0.0002  max mem: 5511
[11:16:49.928461] Epoch: [87]  [660/781]  eta: 0:00:23  lr: 0.000011  training_loss: 0.3076 (0.3098)  mae_loss: 0.2018 (0.2046)  classification_loss: 0.1003 (0.1052)  time: 0.1954  data: 0.0002  max mem: 5511
[11:16:53.880738] Epoch: [87]  [680/781]  eta: 0:00:19  lr: 0.000011  training_loss: 0.3070 (0.3098)  mae_loss: 0.1924 (0.2045)  classification_loss: 0.1069 (0.1053)  time: 0.1975  data: 0.0002  max mem: 5511
[11:16:57.807650] Epoch: [87]  [700/781]  eta: 0:00:15  lr: 0.000011  training_loss: 0.2889 (0.3093)  mae_loss: 0.1880 (0.2039)  classification_loss: 0.1066 (0.1054)  time: 0.1962  data: 0.0002  max mem: 5511
[11:17:01.706946] Epoch: [87]  [720/781]  eta: 0:00:12  lr: 0.000011  training_loss: 0.3055 (0.3094)  mae_loss: 0.1958 (0.2040)  classification_loss: 0.1058 (0.1054)  time: 0.1949  data: 0.0002  max mem: 5511
[11:17:05.601978] Epoch: [87]  [740/781]  eta: 0:00:08  lr: 0.000011  training_loss: 0.3151 (0.3095)  mae_loss: 0.2060 (0.2040)  classification_loss: 0.1091 (0.1055)  time: 0.1947  data: 0.0002  max mem: 5511
[11:17:09.506281] Epoch: [87]  [760/781]  eta: 0:00:04  lr: 0.000011  training_loss: 0.3024 (0.3093)  mae_loss: 0.1857 (0.2037)  classification_loss: 0.1084 (0.1056)  time: 0.1951  data: 0.0002  max mem: 5511
[11:17:13.383242] Epoch: [87]  [780/781]  eta: 0:00:00  lr: 0.000011  training_loss: 0.3042 (0.3092)  mae_loss: 0.1951 (0.2036)  classification_loss: 0.1066 (0.1056)  time: 0.1938  data: 0.0002  max mem: 5511
[11:17:13.551056] Epoch: [87] Total time: 0:02:33 (0.1971 s / it)
[11:17:13.551888] Averaged stats: lr: 0.000011  training_loss: 0.3042 (0.3092)  mae_loss: 0.1951 (0.2036)  classification_loss: 0.1066 (0.1056)
[11:17:14.230172] Test:  [  0/157]  eta: 0:01:45  testing_loss: 0.4475 (0.4475)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.6733  data: 0.6438  max mem: 5511
[11:17:14.520160] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.3716 (0.3846)  acc1: 87.5000 (88.0682)  acc5: 100.0000 (99.7159)  time: 0.0874  data: 0.0587  max mem: 5511
[11:17:14.801293] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.3482 (0.3725)  acc1: 89.0625 (88.8393)  acc5: 100.0000 (99.7024)  time: 0.0284  data: 0.0002  max mem: 5511
[11:17:15.083808] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.3420 (0.3772)  acc1: 90.6250 (89.2641)  acc5: 100.0000 (99.4960)  time: 0.0280  data: 0.0002  max mem: 5511
[11:17:15.365218] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.3988 (0.3899)  acc1: 89.0625 (88.9101)  acc5: 100.0000 (99.2759)  time: 0.0281  data: 0.0002  max mem: 5511
[11:17:15.649861] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.3853 (0.3776)  acc1: 89.0625 (89.2770)  acc5: 98.4375 (99.2953)  time: 0.0282  data: 0.0002  max mem: 5511
[11:17:15.930974] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.3163 (0.3757)  acc1: 90.6250 (89.3443)  acc5: 100.0000 (99.3084)  time: 0.0282  data: 0.0002  max mem: 5511
[11:17:16.212778] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.3357 (0.3709)  acc1: 90.6250 (89.4586)  acc5: 100.0000 (99.3838)  time: 0.0280  data: 0.0002  max mem: 5511
[11:17:16.494108] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.3753 (0.3791)  acc1: 89.0625 (89.1204)  acc5: 100.0000 (99.3634)  time: 0.0280  data: 0.0002  max mem: 5511
[11:17:16.781289] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4003 (0.3771)  acc1: 89.0625 (89.3029)  acc5: 100.0000 (99.3990)  time: 0.0283  data: 0.0002  max mem: 5511
[11:17:17.067796] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.3994 (0.3803)  acc1: 89.0625 (89.1863)  acc5: 100.0000 (99.4431)  time: 0.0286  data: 0.0002  max mem: 5511
[11:17:17.355056] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.3679 (0.3789)  acc1: 90.6250 (89.2877)  acc5: 100.0000 (99.4651)  time: 0.0286  data: 0.0002  max mem: 5511
[11:17:17.640746] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.3404 (0.3773)  acc1: 90.6250 (89.3466)  acc5: 100.0000 (99.4835)  time: 0.0285  data: 0.0002  max mem: 5511
[11:17:17.921594] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.3598 (0.3786)  acc1: 90.6250 (89.3010)  acc5: 100.0000 (99.4752)  time: 0.0282  data: 0.0002  max mem: 5511
[11:17:18.202672] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3350 (0.3750)  acc1: 89.0625 (89.3839)  acc5: 100.0000 (99.5013)  time: 0.0280  data: 0.0001  max mem: 5511
[11:17:18.480859] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.3272 (0.3745)  acc1: 90.6250 (89.4661)  acc5: 100.0000 (99.5033)  time: 0.0279  data: 0.0001  max mem: 5511
[11:17:18.632207] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.3272 (0.3739)  acc1: 90.6250 (89.4300)  acc5: 100.0000 (99.4900)  time: 0.0269  data: 0.0001  max mem: 5511
[11:17:18.807014] Test: Total time: 0:00:05 (0.0334 s / it)
[11:17:18.807469] * Acc@1 89.430 Acc@5 99.490 loss 0.374
[11:17:18.807789] Accuracy of the network on the 10000 test images: 89.4%
[11:17:18.807975] Max accuracy: 89.43%
[11:17:19.180985] log_dir: ./output_dir
[11:17:20.145538] Epoch: [88]  [  0/781]  eta: 0:12:31  lr: 0.000011  training_loss: 0.2871 (0.2871)  mae_loss: 0.1948 (0.1948)  classification_loss: 0.0923 (0.0923)  time: 0.9628  data: 0.7295  max mem: 5511
[11:17:24.105191] Epoch: [88]  [ 20/781]  eta: 0:02:58  lr: 0.000011  training_loss: 0.3081 (0.3114)  mae_loss: 0.2084 (0.2088)  classification_loss: 0.1030 (0.1026)  time: 0.1979  data: 0.0002  max mem: 5511
[11:17:28.016063] Epoch: [88]  [ 40/781]  eta: 0:02:39  lr: 0.000011  training_loss: 0.3097 (0.3124)  mae_loss: 0.2048 (0.2085)  classification_loss: 0.1050 (0.1039)  time: 0.1954  data: 0.0002  max mem: 5511
[11:17:31.932413] Epoch: [88]  [ 60/781]  eta: 0:02:30  lr: 0.000011  training_loss: 0.3130 (0.3126)  mae_loss: 0.2053 (0.2075)  classification_loss: 0.1048 (0.1051)  time: 0.1957  data: 0.0002  max mem: 5511
[11:17:35.832254] Epoch: [88]  [ 80/781]  eta: 0:02:24  lr: 0.000011  training_loss: 0.3047 (0.3103)  mae_loss: 0.1967 (0.2055)  classification_loss: 0.1029 (0.1048)  time: 0.1949  data: 0.0002  max mem: 5511
[11:17:39.789382] Epoch: [88]  [100/781]  eta: 0:02:18  lr: 0.000010  training_loss: 0.3131 (0.3118)  mae_loss: 0.2005 (0.2065)  classification_loss: 0.1051 (0.1053)  time: 0.1977  data: 0.0002  max mem: 5511
[11:17:43.721014] Epoch: [88]  [120/781]  eta: 0:02:13  lr: 0.000010  training_loss: 0.3074 (0.3116)  mae_loss: 0.2026 (0.2066)  classification_loss: 0.1007 (0.1049)  time: 0.1965  data: 0.0002  max mem: 5511
[11:17:47.636299] Epoch: [88]  [140/781]  eta: 0:02:09  lr: 0.000010  training_loss: 0.3046 (0.3118)  mae_loss: 0.2007 (0.2073)  classification_loss: 0.1012 (0.1044)  time: 0.1955  data: 0.0002  max mem: 5511
[11:17:51.615921] Epoch: [88]  [160/781]  eta: 0:02:05  lr: 0.000010  training_loss: 0.3029 (0.3110)  mae_loss: 0.1968 (0.2066)  classification_loss: 0.1025 (0.1044)  time: 0.1989  data: 0.0003  max mem: 5511
[11:17:55.531591] Epoch: [88]  [180/781]  eta: 0:02:00  lr: 0.000010  training_loss: 0.3007 (0.3113)  mae_loss: 0.1958 (0.2067)  classification_loss: 0.1035 (0.1046)  time: 0.1957  data: 0.0003  max mem: 5511
[11:17:59.440539] Epoch: [88]  [200/781]  eta: 0:01:56  lr: 0.000010  training_loss: 0.3081 (0.3112)  mae_loss: 0.1946 (0.2065)  classification_loss: 0.1041 (0.1047)  time: 0.1954  data: 0.0002  max mem: 5511
[11:18:03.363065] Epoch: [88]  [220/781]  eta: 0:01:52  lr: 0.000010  training_loss: 0.3084 (0.3115)  mae_loss: 0.2017 (0.2067)  classification_loss: 0.1022 (0.1048)  time: 0.1960  data: 0.0002  max mem: 5511
[11:18:07.269671] Epoch: [88]  [240/781]  eta: 0:01:47  lr: 0.000010  training_loss: 0.3030 (0.3108)  mae_loss: 0.2037 (0.2061)  classification_loss: 0.1027 (0.1047)  time: 0.1953  data: 0.0003  max mem: 5511
[11:18:11.181266] Epoch: [88]  [260/781]  eta: 0:01:43  lr: 0.000010  training_loss: 0.3080 (0.3106)  mae_loss: 0.1996 (0.2057)  classification_loss: 0.1075 (0.1049)  time: 0.1955  data: 0.0003  max mem: 5511
[11:18:15.068244] Epoch: [88]  [280/781]  eta: 0:01:39  lr: 0.000010  training_loss: 0.3158 (0.3110)  mae_loss: 0.2066 (0.2062)  classification_loss: 0.1076 (0.1048)  time: 0.1942  data: 0.0002  max mem: 5511
[11:18:18.968211] Epoch: [88]  [300/781]  eta: 0:01:35  lr: 0.000010  training_loss: 0.3113 (0.3113)  mae_loss: 0.1978 (0.2065)  classification_loss: 0.1030 (0.1048)  time: 0.1949  data: 0.0002  max mem: 5511
[11:18:22.881928] Epoch: [88]  [320/781]  eta: 0:01:31  lr: 0.000010  training_loss: 0.2964 (0.3106)  mae_loss: 0.1949 (0.2060)  classification_loss: 0.1014 (0.1046)  time: 0.1956  data: 0.0002  max mem: 5511
[11:18:26.794785] Epoch: [88]  [340/781]  eta: 0:01:27  lr: 0.000010  training_loss: 0.2919 (0.3097)  mae_loss: 0.1917 (0.2053)  classification_loss: 0.1003 (0.1044)  time: 0.1955  data: 0.0002  max mem: 5511
[11:18:30.701495] Epoch: [88]  [360/781]  eta: 0:01:23  lr: 0.000010  training_loss: 0.3008 (0.3097)  mae_loss: 0.2023 (0.2052)  classification_loss: 0.1047 (0.1044)  time: 0.1953  data: 0.0003  max mem: 5511
[11:18:34.588149] Epoch: [88]  [380/781]  eta: 0:01:19  lr: 0.000010  training_loss: 0.3078 (0.3100)  mae_loss: 0.1990 (0.2052)  classification_loss: 0.1096 (0.1048)  time: 0.1943  data: 0.0002  max mem: 5511
[11:18:38.479366] Epoch: [88]  [400/781]  eta: 0:01:15  lr: 0.000010  training_loss: 0.3012 (0.3099)  mae_loss: 0.1937 (0.2052)  classification_loss: 0.1019 (0.1047)  time: 0.1945  data: 0.0002  max mem: 5511
[11:18:42.375022] Epoch: [88]  [420/781]  eta: 0:01:11  lr: 0.000010  training_loss: 0.3101 (0.3099)  mae_loss: 0.2022 (0.2052)  classification_loss: 0.1056 (0.1048)  time: 0.1947  data: 0.0002  max mem: 5511
[11:18:46.292301] Epoch: [88]  [440/781]  eta: 0:01:07  lr: 0.000010  training_loss: 0.2990 (0.3096)  mae_loss: 0.1985 (0.2049)  classification_loss: 0.1011 (0.1047)  time: 0.1958  data: 0.0003  max mem: 5511
[11:18:50.187160] Epoch: [88]  [460/781]  eta: 0:01:03  lr: 0.000010  training_loss: 0.3165 (0.3100)  mae_loss: 0.2115 (0.2053)  classification_loss: 0.1048 (0.1047)  time: 0.1947  data: 0.0002  max mem: 5511
[11:18:54.115848] Epoch: [88]  [480/781]  eta: 0:00:59  lr: 0.000010  training_loss: 0.3143 (0.3103)  mae_loss: 0.2096 (0.2054)  classification_loss: 0.1058 (0.1048)  time: 0.1963  data: 0.0002  max mem: 5511
[11:18:58.040274] Epoch: [88]  [500/781]  eta: 0:00:55  lr: 0.000010  training_loss: 0.2974 (0.3100)  mae_loss: 0.1959 (0.2051)  classification_loss: 0.1091 (0.1049)  time: 0.1961  data: 0.0002  max mem: 5511
[11:19:01.976931] Epoch: [88]  [520/781]  eta: 0:00:51  lr: 0.000010  training_loss: 0.3034 (0.3098)  mae_loss: 0.1998 (0.2048)  classification_loss: 0.1061 (0.1050)  time: 0.1968  data: 0.0002  max mem: 5511
[11:19:05.886653] Epoch: [88]  [540/781]  eta: 0:00:47  lr: 0.000010  training_loss: 0.2964 (0.3095)  mae_loss: 0.1916 (0.2046)  classification_loss: 0.1045 (0.1050)  time: 0.1954  data: 0.0002  max mem: 5511
[11:19:09.809555] Epoch: [88]  [560/781]  eta: 0:00:43  lr: 0.000010  training_loss: 0.2935 (0.3092)  mae_loss: 0.1985 (0.2043)  classification_loss: 0.1022 (0.1048)  time: 0.1961  data: 0.0002  max mem: 5511
[11:19:13.721249] Epoch: [88]  [580/781]  eta: 0:00:39  lr: 0.000010  training_loss: 0.3117 (0.3092)  mae_loss: 0.2070 (0.2044)  classification_loss: 0.1017 (0.1048)  time: 0.1955  data: 0.0006  max mem: 5511
[11:19:17.632888] Epoch: [88]  [600/781]  eta: 0:00:35  lr: 0.000009  training_loss: 0.2990 (0.3091)  mae_loss: 0.1927 (0.2043)  classification_loss: 0.0996 (0.1047)  time: 0.1955  data: 0.0002  max mem: 5511
[11:19:21.547907] Epoch: [88]  [620/781]  eta: 0:00:31  lr: 0.000009  training_loss: 0.3171 (0.3093)  mae_loss: 0.2133 (0.2045)  classification_loss: 0.1055 (0.1048)  time: 0.1957  data: 0.0002  max mem: 5511
[11:19:25.455010] Epoch: [88]  [640/781]  eta: 0:00:27  lr: 0.000009  training_loss: 0.2997 (0.3091)  mae_loss: 0.1957 (0.2043)  classification_loss: 0.1050 (0.1048)  time: 0.1953  data: 0.0002  max mem: 5511
[11:19:29.372309] Epoch: [88]  [660/781]  eta: 0:00:23  lr: 0.000009  training_loss: 0.3078 (0.3091)  mae_loss: 0.1980 (0.2043)  classification_loss: 0.1039 (0.1048)  time: 0.1958  data: 0.0002  max mem: 5511
[11:19:33.263806] Epoch: [88]  [680/781]  eta: 0:00:19  lr: 0.000009  training_loss: 0.3133 (0.3091)  mae_loss: 0.2048 (0.2043)  classification_loss: 0.1059 (0.1049)  time: 0.1945  data: 0.0002  max mem: 5511
[11:19:37.158273] Epoch: [88]  [700/781]  eta: 0:00:15  lr: 0.000009  training_loss: 0.2993 (0.3093)  mae_loss: 0.1946 (0.2042)  classification_loss: 0.1084 (0.1050)  time: 0.1946  data: 0.0002  max mem: 5511
[11:19:41.056826] Epoch: [88]  [720/781]  eta: 0:00:11  lr: 0.000009  training_loss: 0.2977 (0.3093)  mae_loss: 0.1958 (0.2042)  classification_loss: 0.1023 (0.1051)  time: 0.1948  data: 0.0002  max mem: 5511
[11:19:44.949706] Epoch: [88]  [740/781]  eta: 0:00:08  lr: 0.000009  training_loss: 0.3117 (0.3092)  mae_loss: 0.1997 (0.2042)  classification_loss: 0.1022 (0.1050)  time: 0.1946  data: 0.0004  max mem: 5511
[11:19:48.863869] Epoch: [88]  [760/781]  eta: 0:00:04  lr: 0.000009  training_loss: 0.3131 (0.3095)  mae_loss: 0.2086 (0.2044)  classification_loss: 0.1047 (0.1051)  time: 0.1956  data: 0.0003  max mem: 5511
[11:19:52.758033] Epoch: [88]  [780/781]  eta: 0:00:00  lr: 0.000009  training_loss: 0.2945 (0.3094)  mae_loss: 0.1956 (0.2043)  classification_loss: 0.1075 (0.1051)  time: 0.1946  data: 0.0002  max mem: 5511
[11:19:52.917793] Epoch: [88] Total time: 0:02:33 (0.1968 s / it)
[11:19:52.918707] Averaged stats: lr: 0.000009  training_loss: 0.2945 (0.3094)  mae_loss: 0.1956 (0.2043)  classification_loss: 0.1075 (0.1051)
[11:19:53.512571] Test:  [  0/157]  eta: 0:01:32  testing_loss: 0.4653 (0.4653)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.5898  data: 0.5593  max mem: 5511
[11:19:53.800102] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.3780 (0.3875)  acc1: 87.5000 (89.0625)  acc5: 100.0000 (99.5739)  time: 0.0796  data: 0.0511  max mem: 5511
[11:19:54.085712] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.3569 (0.3749)  acc1: 90.6250 (89.4345)  acc5: 100.0000 (99.5536)  time: 0.0285  data: 0.0003  max mem: 5511
[11:19:54.369439] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.3404 (0.3811)  acc1: 90.6250 (89.1129)  acc5: 100.0000 (99.3952)  time: 0.0283  data: 0.0002  max mem: 5511
[11:19:54.662654] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.3951 (0.3939)  acc1: 89.0625 (88.7957)  acc5: 100.0000 (99.3140)  time: 0.0287  data: 0.0002  max mem: 5511
[11:19:54.949282] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.3751 (0.3814)  acc1: 90.6250 (89.2157)  acc5: 100.0000 (99.3260)  time: 0.0289  data: 0.0002  max mem: 5511
[11:19:55.231726] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.3092 (0.3793)  acc1: 92.1875 (89.4211)  acc5: 100.0000 (99.3084)  time: 0.0283  data: 0.0002  max mem: 5511
[11:19:55.513631] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.3377 (0.3738)  acc1: 90.6250 (89.5467)  acc5: 100.0000 (99.3618)  time: 0.0281  data: 0.0002  max mem: 5511
[11:19:55.796636] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.3778 (0.3824)  acc1: 87.5000 (89.1975)  acc5: 100.0000 (99.3441)  time: 0.0281  data: 0.0002  max mem: 5511
[11:19:56.080404] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4189 (0.3805)  acc1: 87.5000 (89.3201)  acc5: 100.0000 (99.3647)  time: 0.0282  data: 0.0002  max mem: 5511
[11:19:56.367722] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4159 (0.3849)  acc1: 89.0625 (89.2172)  acc5: 100.0000 (99.3657)  time: 0.0284  data: 0.0002  max mem: 5511
[11:19:56.651446] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.3889 (0.3841)  acc1: 89.0625 (89.3018)  acc5: 100.0000 (99.3947)  time: 0.0284  data: 0.0002  max mem: 5511
[11:19:56.932357] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.3711 (0.3826)  acc1: 90.6250 (89.3466)  acc5: 100.0000 (99.4318)  time: 0.0281  data: 0.0002  max mem: 5511
[11:19:57.213186] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.3701 (0.3834)  acc1: 89.0625 (89.2414)  acc5: 100.0000 (99.4275)  time: 0.0280  data: 0.0002  max mem: 5511
[11:19:57.494154] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3194 (0.3795)  acc1: 90.6250 (89.3617)  acc5: 100.0000 (99.4681)  time: 0.0280  data: 0.0002  max mem: 5511
[11:19:57.774257] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.3299 (0.3789)  acc1: 90.6250 (89.3729)  acc5: 100.0000 (99.4826)  time: 0.0279  data: 0.0001  max mem: 5511
[11:19:57.925620] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.3351 (0.3778)  acc1: 90.6250 (89.3900)  acc5: 100.0000 (99.4700)  time: 0.0270  data: 0.0001  max mem: 5511
[11:19:58.072013] Test: Total time: 0:00:05 (0.0328 s / it)
[11:19:58.072471] * Acc@1 89.390 Acc@5 99.470 loss 0.378
[11:19:58.072782] Accuracy of the network on the 10000 test images: 89.4%
[11:19:58.073032] Max accuracy: 89.43%
[11:19:58.241969] log_dir: ./output_dir
[11:19:59.056535] Epoch: [89]  [  0/781]  eta: 0:10:34  lr: 0.000009  training_loss: 0.2851 (0.2851)  mae_loss: 0.1874 (0.1874)  classification_loss: 0.0977 (0.0977)  time: 0.8127  data: 0.5871  max mem: 5511
[11:20:02.984081] Epoch: [89]  [ 20/781]  eta: 0:02:51  lr: 0.000009  training_loss: 0.3032 (0.3086)  mae_loss: 0.2000 (0.2057)  classification_loss: 0.0994 (0.1030)  time: 0.1962  data: 0.0002  max mem: 5511
[11:20:06.911521] Epoch: [89]  [ 40/781]  eta: 0:02:36  lr: 0.000009  training_loss: 0.2957 (0.3054)  mae_loss: 0.1893 (0.2008)  classification_loss: 0.1041 (0.1046)  time: 0.1963  data: 0.0003  max mem: 5511
[11:20:10.818845] Epoch: [89]  [ 60/781]  eta: 0:02:28  lr: 0.000009  training_loss: 0.3082 (0.3086)  mae_loss: 0.2010 (0.2017)  classification_loss: 0.1104 (0.1069)  time: 0.1952  data: 0.0002  max mem: 5511
[11:20:14.721807] Epoch: [89]  [ 80/781]  eta: 0:02:22  lr: 0.000009  training_loss: 0.3036 (0.3086)  mae_loss: 0.1958 (0.2023)  classification_loss: 0.1050 (0.1062)  time: 0.1951  data: 0.0002  max mem: 5511
[11:20:18.659908] Epoch: [89]  [100/781]  eta: 0:02:17  lr: 0.000009  training_loss: 0.3038 (0.3091)  mae_loss: 0.2000 (0.2032)  classification_loss: 0.1031 (0.1059)  time: 0.1968  data: 0.0004  max mem: 5511
[11:20:22.550016] Epoch: [89]  [120/781]  eta: 0:02:12  lr: 0.000009  training_loss: 0.3053 (0.3080)  mae_loss: 0.1978 (0.2024)  classification_loss: 0.1031 (0.1056)  time: 0.1944  data: 0.0003  max mem: 5511
[11:20:26.441057] Epoch: [89]  [140/781]  eta: 0:02:08  lr: 0.000009  training_loss: 0.3131 (0.3092)  mae_loss: 0.2062 (0.2034)  classification_loss: 0.1068 (0.1058)  time: 0.1945  data: 0.0003  max mem: 5511
[11:20:30.328604] Epoch: [89]  [160/781]  eta: 0:02:03  lr: 0.000009  training_loss: 0.3143 (0.3093)  mae_loss: 0.1997 (0.2034)  classification_loss: 0.1044 (0.1059)  time: 0.1943  data: 0.0003  max mem: 5511
[11:20:34.281087] Epoch: [89]  [180/781]  eta: 0:01:59  lr: 0.000009  training_loss: 0.3127 (0.3097)  mae_loss: 0.2004 (0.2035)  classification_loss: 0.1107 (0.1062)  time: 0.1976  data: 0.0002  max mem: 5511
[11:20:38.190236] Epoch: [89]  [200/781]  eta: 0:01:55  lr: 0.000009  training_loss: 0.3161 (0.3106)  mae_loss: 0.2004 (0.2041)  classification_loss: 0.1057 (0.1066)  time: 0.1954  data: 0.0003  max mem: 5511
[11:20:42.153858] Epoch: [89]  [220/781]  eta: 0:01:51  lr: 0.000009  training_loss: 0.2973 (0.3098)  mae_loss: 0.1903 (0.2032)  classification_loss: 0.1070 (0.1065)  time: 0.1981  data: 0.0002  max mem: 5511
[11:20:46.067922] Epoch: [89]  [240/781]  eta: 0:01:47  lr: 0.000009  training_loss: 0.3154 (0.3108)  mae_loss: 0.2087 (0.2041)  classification_loss: 0.1053 (0.1066)  time: 0.1955  data: 0.0002  max mem: 5511
[11:20:49.963820] Epoch: [89]  [260/781]  eta: 0:01:43  lr: 0.000009  training_loss: 0.3048 (0.3102)  mae_loss: 0.2038 (0.2037)  classification_loss: 0.1030 (0.1065)  time: 0.1947  data: 0.0003  max mem: 5511
[11:20:53.875869] Epoch: [89]  [280/781]  eta: 0:01:39  lr: 0.000009  training_loss: 0.3087 (0.3103)  mae_loss: 0.2010 (0.2038)  classification_loss: 0.1057 (0.1065)  time: 0.1955  data: 0.0003  max mem: 5511
[11:20:57.805432] Epoch: [89]  [300/781]  eta: 0:01:35  lr: 0.000009  training_loss: 0.3062 (0.3101)  mae_loss: 0.2029 (0.2037)  classification_loss: 0.1047 (0.1064)  time: 0.1964  data: 0.0002  max mem: 5511
[11:21:01.708511] Epoch: [89]  [320/781]  eta: 0:01:31  lr: 0.000009  training_loss: 0.3083 (0.3101)  mae_loss: 0.1977 (0.2037)  classification_loss: 0.1092 (0.1064)  time: 0.1951  data: 0.0002  max mem: 5511
[11:21:05.613485] Epoch: [89]  [340/781]  eta: 0:01:27  lr: 0.000009  training_loss: 0.3234 (0.3109)  mae_loss: 0.2159 (0.2047)  classification_loss: 0.1049 (0.1062)  time: 0.1952  data: 0.0002  max mem: 5511
[11:21:09.537467] Epoch: [89]  [360/781]  eta: 0:01:23  lr: 0.000008  training_loss: 0.3004 (0.3105)  mae_loss: 0.1874 (0.2042)  classification_loss: 0.1057 (0.1063)  time: 0.1961  data: 0.0002  max mem: 5511
[11:21:13.477813] Epoch: [89]  [380/781]  eta: 0:01:19  lr: 0.000008  training_loss: 0.3121 (0.3108)  mae_loss: 0.2115 (0.2048)  classification_loss: 0.1021 (0.1060)  time: 0.1969  data: 0.0002  max mem: 5511
[11:21:17.363701] Epoch: [89]  [400/781]  eta: 0:01:15  lr: 0.000008  training_loss: 0.2999 (0.3106)  mae_loss: 0.1964 (0.2046)  classification_loss: 0.1009 (0.1059)  time: 0.1942  data: 0.0002  max mem: 5511
[11:21:21.272354] Epoch: [89]  [420/781]  eta: 0:01:11  lr: 0.000008  training_loss: 0.3216 (0.3107)  mae_loss: 0.2009 (0.2048)  classification_loss: 0.1034 (0.1058)  time: 0.1954  data: 0.0002  max mem: 5511
[11:21:25.177520] Epoch: [89]  [440/781]  eta: 0:01:07  lr: 0.000008  training_loss: 0.2988 (0.3103)  mae_loss: 0.1937 (0.2046)  classification_loss: 0.1031 (0.1057)  time: 0.1952  data: 0.0002  max mem: 5511
[11:21:29.085495] Epoch: [89]  [460/781]  eta: 0:01:03  lr: 0.000008  training_loss: 0.3091 (0.3102)  mae_loss: 0.2020 (0.2046)  classification_loss: 0.1044 (0.1057)  time: 0.1953  data: 0.0002  max mem: 5511
[11:21:33.035174] Epoch: [89]  [480/781]  eta: 0:00:59  lr: 0.000008  training_loss: 0.3042 (0.3098)  mae_loss: 0.1904 (0.2041)  classification_loss: 0.1039 (0.1057)  time: 0.1974  data: 0.0003  max mem: 5511
[11:21:36.931421] Epoch: [89]  [500/781]  eta: 0:00:55  lr: 0.000008  training_loss: 0.3016 (0.3095)  mae_loss: 0.1995 (0.2040)  classification_loss: 0.1029 (0.1056)  time: 0.1947  data: 0.0003  max mem: 5511
[11:21:40.844189] Epoch: [89]  [520/781]  eta: 0:00:51  lr: 0.000008  training_loss: 0.3064 (0.3095)  mae_loss: 0.2017 (0.2038)  classification_loss: 0.1061 (0.1057)  time: 0.1955  data: 0.0002  max mem: 5511
[11:21:44.755850] Epoch: [89]  [540/781]  eta: 0:00:47  lr: 0.000008  training_loss: 0.3120 (0.3097)  mae_loss: 0.2042 (0.2041)  classification_loss: 0.1006 (0.1056)  time: 0.1955  data: 0.0003  max mem: 5511
[11:21:48.693414] Epoch: [89]  [560/781]  eta: 0:00:43  lr: 0.000008  training_loss: 0.3188 (0.3099)  mae_loss: 0.2071 (0.2044)  classification_loss: 0.0985 (0.1055)  time: 0.1968  data: 0.0002  max mem: 5511
[11:21:52.611019] Epoch: [89]  [580/781]  eta: 0:00:39  lr: 0.000008  training_loss: 0.3156 (0.3101)  mae_loss: 0.2059 (0.2046)  classification_loss: 0.1052 (0.1055)  time: 0.1958  data: 0.0002  max mem: 5511
[11:21:56.523287] Epoch: [89]  [600/781]  eta: 0:00:35  lr: 0.000008  training_loss: 0.3061 (0.3098)  mae_loss: 0.1982 (0.2044)  classification_loss: 0.1022 (0.1054)  time: 0.1955  data: 0.0003  max mem: 5511
[11:22:00.476341] Epoch: [89]  [620/781]  eta: 0:00:31  lr: 0.000008  training_loss: 0.2970 (0.3097)  mae_loss: 0.1911 (0.2043)  classification_loss: 0.1058 (0.1054)  time: 0.1975  data: 0.0002  max mem: 5511
[11:22:04.370234] Epoch: [89]  [640/781]  eta: 0:00:27  lr: 0.000008  training_loss: 0.2955 (0.3096)  mae_loss: 0.1902 (0.2042)  classification_loss: 0.1067 (0.1054)  time: 0.1946  data: 0.0003  max mem: 5511
[11:22:08.269109] Epoch: [89]  [660/781]  eta: 0:00:23  lr: 0.000008  training_loss: 0.3071 (0.3097)  mae_loss: 0.1992 (0.2042)  classification_loss: 0.1074 (0.1055)  time: 0.1949  data: 0.0002  max mem: 5511
[11:22:12.164328] Epoch: [89]  [680/781]  eta: 0:00:19  lr: 0.000008  training_loss: 0.3061 (0.3096)  mae_loss: 0.1953 (0.2041)  classification_loss: 0.1074 (0.1055)  time: 0.1947  data: 0.0002  max mem: 5511
[11:22:16.075238] Epoch: [89]  [700/781]  eta: 0:00:15  lr: 0.000008  training_loss: 0.3150 (0.3098)  mae_loss: 0.2090 (0.2043)  classification_loss: 0.1069 (0.1055)  time: 0.1955  data: 0.0003  max mem: 5511
[11:22:19.971882] Epoch: [89]  [720/781]  eta: 0:00:11  lr: 0.000008  training_loss: 0.3128 (0.3099)  mae_loss: 0.2000 (0.2044)  classification_loss: 0.1026 (0.1055)  time: 0.1948  data: 0.0002  max mem: 5511
[11:22:23.889212] Epoch: [89]  [740/781]  eta: 0:00:08  lr: 0.000008  training_loss: 0.3014 (0.3098)  mae_loss: 0.1999 (0.2043)  classification_loss: 0.1020 (0.1055)  time: 0.1958  data: 0.0002  max mem: 5511
[11:22:27.787723] Epoch: [89]  [760/781]  eta: 0:00:04  lr: 0.000008  training_loss: 0.3041 (0.3100)  mae_loss: 0.2034 (0.2045)  classification_loss: 0.1047 (0.1055)  time: 0.1948  data: 0.0002  max mem: 5511
[11:22:31.677211] Epoch: [89]  [780/781]  eta: 0:00:00  lr: 0.000008  training_loss: 0.3122 (0.3104)  mae_loss: 0.2102 (0.2048)  classification_loss: 0.1042 (0.1055)  time: 0.1944  data: 0.0002  max mem: 5511
[11:22:31.830972] Epoch: [89] Total time: 0:02:33 (0.1967 s / it)
[11:22:31.831559] Averaged stats: lr: 0.000008  training_loss: 0.3122 (0.3104)  mae_loss: 0.2102 (0.2048)  classification_loss: 0.1042 (0.1055)
[11:22:32.385741] Test:  [  0/157]  eta: 0:01:26  testing_loss: 0.4558 (0.4558)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.5499  data: 0.5205  max mem: 5511
[11:22:32.668804] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.3761 (0.3857)  acc1: 90.6250 (89.6307)  acc5: 100.0000 (99.7159)  time: 0.0756  data: 0.0475  max mem: 5511
[11:22:32.950455] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.3579 (0.3734)  acc1: 90.6250 (89.8065)  acc5: 100.0000 (99.7024)  time: 0.0281  data: 0.0002  max mem: 5511
[11:22:33.233068] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.3277 (0.3787)  acc1: 90.6250 (89.7681)  acc5: 100.0000 (99.4456)  time: 0.0280  data: 0.0002  max mem: 5511
[11:22:33.515039] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.3989 (0.3932)  acc1: 89.0625 (89.2149)  acc5: 100.0000 (99.2759)  time: 0.0281  data: 0.0002  max mem: 5511
[11:22:33.798483] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.3825 (0.3813)  acc1: 89.0625 (89.5527)  acc5: 98.4375 (99.2953)  time: 0.0281  data: 0.0002  max mem: 5511
[11:22:34.081422] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.3075 (0.3795)  acc1: 90.6250 (89.6004)  acc5: 100.0000 (99.3084)  time: 0.0282  data: 0.0002  max mem: 5511
[11:22:34.363717] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.3343 (0.3733)  acc1: 90.6250 (89.7447)  acc5: 100.0000 (99.3398)  time: 0.0281  data: 0.0002  max mem: 5511
[11:22:34.647790] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.3744 (0.3820)  acc1: 89.0625 (89.3326)  acc5: 100.0000 (99.3827)  time: 0.0282  data: 0.0002  max mem: 5511
[11:22:34.929327] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4213 (0.3800)  acc1: 87.5000 (89.4402)  acc5: 100.0000 (99.4162)  time: 0.0282  data: 0.0002  max mem: 5511
[11:22:35.211392] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4121 (0.3844)  acc1: 89.0625 (89.2791)  acc5: 100.0000 (99.4585)  time: 0.0281  data: 0.0002  max mem: 5511
[11:22:35.493095] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.3910 (0.3826)  acc1: 89.0625 (89.3722)  acc5: 100.0000 (99.4510)  time: 0.0281  data: 0.0002  max mem: 5511
[11:22:35.774623] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.3446 (0.3810)  acc1: 89.0625 (89.3982)  acc5: 100.0000 (99.4706)  time: 0.0280  data: 0.0002  max mem: 5511
[11:22:36.056830] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.3546 (0.3823)  acc1: 87.5000 (89.3368)  acc5: 100.0000 (99.4633)  time: 0.0280  data: 0.0002  max mem: 5511
[11:22:36.336598] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3217 (0.3778)  acc1: 89.0625 (89.3949)  acc5: 100.0000 (99.5013)  time: 0.0280  data: 0.0001  max mem: 5511
[11:22:36.614818] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.3156 (0.3772)  acc1: 90.6250 (89.4247)  acc5: 100.0000 (99.5240)  time: 0.0278  data: 0.0001  max mem: 5511
[11:22:36.764939] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.3260 (0.3763)  acc1: 89.0625 (89.3800)  acc5: 100.0000 (99.5200)  time: 0.0268  data: 0.0001  max mem: 5511
[11:22:36.918132] Test: Total time: 0:00:05 (0.0324 s / it)
[11:22:36.919773] * Acc@1 89.380 Acc@5 99.520 loss 0.376
[11:22:36.920768] Accuracy of the network on the 10000 test images: 89.4%
[11:22:36.921689] Max accuracy: 89.43%
[11:22:37.351914] log_dir: ./output_dir
[11:22:38.174665] Epoch: [90]  [  0/781]  eta: 0:10:40  lr: 0.000008  training_loss: 0.2985 (0.2985)  mae_loss: 0.1942 (0.1942)  classification_loss: 0.1044 (0.1044)  time: 0.8206  data: 0.5950  max mem: 5511
[11:22:42.091737] Epoch: [90]  [ 20/781]  eta: 0:02:51  lr: 0.000008  training_loss: 0.3049 (0.3059)  mae_loss: 0.2022 (0.2027)  classification_loss: 0.1013 (0.1032)  time: 0.1957  data: 0.0002  max mem: 5511
[11:22:45.998333] Epoch: [90]  [ 40/781]  eta: 0:02:36  lr: 0.000008  training_loss: 0.3089 (0.3073)  mae_loss: 0.2036 (0.2027)  classification_loss: 0.1027 (0.1046)  time: 0.1953  data: 0.0002  max mem: 5511
[11:22:49.936408] Epoch: [90]  [ 60/781]  eta: 0:02:28  lr: 0.000008  training_loss: 0.3105 (0.3087)  mae_loss: 0.2036 (0.2029)  classification_loss: 0.1074 (0.1057)  time: 0.1968  data: 0.0002  max mem: 5511
[11:22:53.833224] Epoch: [90]  [ 80/781]  eta: 0:02:22  lr: 0.000008  training_loss: 0.3043 (0.3062)  mae_loss: 0.2019 (0.2015)  classification_loss: 0.0999 (0.1048)  time: 0.1948  data: 0.0002  max mem: 5511
[11:22:57.748109] Epoch: [90]  [100/781]  eta: 0:02:17  lr: 0.000008  training_loss: 0.3073 (0.3077)  mae_loss: 0.2004 (0.2019)  classification_loss: 0.1074 (0.1058)  time: 0.1957  data: 0.0002  max mem: 5511
[11:23:01.671448] Epoch: [90]  [120/781]  eta: 0:02:12  lr: 0.000008  training_loss: 0.3062 (0.3078)  mae_loss: 0.2060 (0.2021)  classification_loss: 0.1019 (0.1056)  time: 0.1961  data: 0.0002  max mem: 5511
[11:23:05.579916] Epoch: [90]  [140/781]  eta: 0:02:08  lr: 0.000008  training_loss: 0.2953 (0.3075)  mae_loss: 0.1894 (0.2016)  classification_loss: 0.1083 (0.1059)  time: 0.1953  data: 0.0002  max mem: 5511
[11:23:09.484430] Epoch: [90]  [160/781]  eta: 0:02:03  lr: 0.000007  training_loss: 0.3106 (0.3089)  mae_loss: 0.2043 (0.2032)  classification_loss: 0.1030 (0.1057)  time: 0.1951  data: 0.0003  max mem: 5511
[11:23:13.404371] Epoch: [90]  [180/781]  eta: 0:01:59  lr: 0.000007  training_loss: 0.3121 (0.3085)  mae_loss: 0.2011 (0.2025)  classification_loss: 0.1064 (0.1060)  time: 0.1959  data: 0.0002  max mem: 5511
[11:23:17.431364] Epoch: [90]  [200/781]  eta: 0:01:55  lr: 0.000007  training_loss: 0.3029 (0.3081)  mae_loss: 0.2046 (0.2026)  classification_loss: 0.1016 (0.1055)  time: 0.2013  data: 0.0002  max mem: 5511
[11:23:21.319295] Epoch: [90]  [220/781]  eta: 0:01:51  lr: 0.000007  training_loss: 0.3029 (0.3074)  mae_loss: 0.1986 (0.2021)  classification_loss: 0.1035 (0.1053)  time: 0.1943  data: 0.0002  max mem: 5511
[11:23:25.207491] Epoch: [90]  [240/781]  eta: 0:01:47  lr: 0.000007  training_loss: 0.3178 (0.3081)  mae_loss: 0.2020 (0.2030)  classification_loss: 0.1020 (0.1051)  time: 0.1943  data: 0.0003  max mem: 5511
[11:23:29.095231] Epoch: [90]  [260/781]  eta: 0:01:43  lr: 0.000007  training_loss: 0.3040 (0.3078)  mae_loss: 0.1995 (0.2028)  classification_loss: 0.1050 (0.1051)  time: 0.1943  data: 0.0003  max mem: 5511
[11:23:33.035465] Epoch: [90]  [280/781]  eta: 0:01:39  lr: 0.000007  training_loss: 0.3039 (0.3078)  mae_loss: 0.2018 (0.2029)  classification_loss: 0.0988 (0.1049)  time: 0.1969  data: 0.0002  max mem: 5511
[11:23:36.951923] Epoch: [90]  [300/781]  eta: 0:01:35  lr: 0.000007  training_loss: 0.2977 (0.3077)  mae_loss: 0.1893 (0.2026)  classification_loss: 0.1092 (0.1051)  time: 0.1957  data: 0.0002  max mem: 5511
[11:23:40.848242] Epoch: [90]  [320/781]  eta: 0:01:31  lr: 0.000007  training_loss: 0.2956 (0.3076)  mae_loss: 0.1927 (0.2024)  classification_loss: 0.1051 (0.1052)  time: 0.1947  data: 0.0002  max mem: 5511
[11:23:44.765666] Epoch: [90]  [340/781]  eta: 0:01:27  lr: 0.000007  training_loss: 0.3191 (0.3082)  mae_loss: 0.2129 (0.2031)  classification_loss: 0.1043 (0.1051)  time: 0.1958  data: 0.0002  max mem: 5511
[11:23:48.664104] Epoch: [90]  [360/781]  eta: 0:01:23  lr: 0.000007  training_loss: 0.2962 (0.3074)  mae_loss: 0.1910 (0.2023)  classification_loss: 0.1020 (0.1051)  time: 0.1948  data: 0.0003  max mem: 5511
[11:23:52.560987] Epoch: [90]  [380/781]  eta: 0:01:19  lr: 0.000007  training_loss: 0.3099 (0.3078)  mae_loss: 0.2063 (0.2026)  classification_loss: 0.1076 (0.1052)  time: 0.1947  data: 0.0002  max mem: 5511
[11:23:56.471061] Epoch: [90]  [400/781]  eta: 0:01:15  lr: 0.000007  training_loss: 0.2957 (0.3070)  mae_loss: 0.1953 (0.2019)  classification_loss: 0.1017 (0.1051)  time: 0.1954  data: 0.0002  max mem: 5511
[11:24:00.387041] Epoch: [90]  [420/781]  eta: 0:01:11  lr: 0.000007  training_loss: 0.3150 (0.3073)  mae_loss: 0.2063 (0.2023)  classification_loss: 0.1045 (0.1050)  time: 0.1957  data: 0.0004  max mem: 5511
[11:24:04.327173] Epoch: [90]  [440/781]  eta: 0:01:07  lr: 0.000007  training_loss: 0.3149 (0.3075)  mae_loss: 0.2063 (0.2025)  classification_loss: 0.1048 (0.1051)  time: 0.1969  data: 0.0003  max mem: 5511
[11:24:08.225688] Epoch: [90]  [460/781]  eta: 0:01:03  lr: 0.000007  training_loss: 0.2993 (0.3073)  mae_loss: 0.1955 (0.2022)  classification_loss: 0.1003 (0.1050)  time: 0.1948  data: 0.0002  max mem: 5511
[11:24:12.135264] Epoch: [90]  [480/781]  eta: 0:00:59  lr: 0.000007  training_loss: 0.3063 (0.3072)  mae_loss: 0.1959 (0.2022)  classification_loss: 0.1059 (0.1051)  time: 0.1954  data: 0.0002  max mem: 5511
[11:24:16.099374] Epoch: [90]  [500/781]  eta: 0:00:55  lr: 0.000007  training_loss: 0.3031 (0.3075)  mae_loss: 0.2039 (0.2025)  classification_loss: 0.1021 (0.1050)  time: 0.1981  data: 0.0003  max mem: 5511
[11:24:20.024895] Epoch: [90]  [520/781]  eta: 0:00:51  lr: 0.000007  training_loss: 0.3029 (0.3074)  mae_loss: 0.2065 (0.2024)  classification_loss: 0.1078 (0.1050)  time: 0.1962  data: 0.0002  max mem: 5511
[11:24:23.922956] Epoch: [90]  [540/781]  eta: 0:00:47  lr: 0.000007  training_loss: 0.2993 (0.3076)  mae_loss: 0.1984 (0.2027)  classification_loss: 0.1041 (0.1049)  time: 0.1948  data: 0.0002  max mem: 5511
[11:24:27.843358] Epoch: [90]  [560/781]  eta: 0:00:43  lr: 0.000007  training_loss: 0.2957 (0.3075)  mae_loss: 0.1902 (0.2026)  classification_loss: 0.1049 (0.1049)  time: 0.1959  data: 0.0002  max mem: 5511
[11:24:31.747475] Epoch: [90]  [580/781]  eta: 0:00:39  lr: 0.000007  training_loss: 0.3063 (0.3075)  mae_loss: 0.2013 (0.2026)  classification_loss: 0.1031 (0.1049)  time: 0.1951  data: 0.0003  max mem: 5511
[11:24:35.665067] Epoch: [90]  [600/781]  eta: 0:00:35  lr: 0.000007  training_loss: 0.3049 (0.3075)  mae_loss: 0.2014 (0.2026)  classification_loss: 0.1041 (0.1049)  time: 0.1958  data: 0.0002  max mem: 5511
[11:24:39.563635] Epoch: [90]  [620/781]  eta: 0:00:31  lr: 0.000007  training_loss: 0.3118 (0.3076)  mae_loss: 0.2041 (0.2028)  classification_loss: 0.1069 (0.1049)  time: 0.1948  data: 0.0002  max mem: 5511
[11:24:43.507587] Epoch: [90]  [640/781]  eta: 0:00:27  lr: 0.000007  training_loss: 0.3034 (0.3075)  mae_loss: 0.1892 (0.2026)  classification_loss: 0.1078 (0.1049)  time: 0.1971  data: 0.0002  max mem: 5511
[11:24:47.417506] Epoch: [90]  [660/781]  eta: 0:00:23  lr: 0.000007  training_loss: 0.3111 (0.3078)  mae_loss: 0.2137 (0.2029)  classification_loss: 0.1002 (0.1049)  time: 0.1954  data: 0.0002  max mem: 5511
[11:24:51.308395] Epoch: [90]  [680/781]  eta: 0:00:19  lr: 0.000007  training_loss: 0.3120 (0.3080)  mae_loss: 0.2029 (0.2031)  classification_loss: 0.1055 (0.1049)  time: 0.1945  data: 0.0002  max mem: 5511
[11:24:55.243458] Epoch: [90]  [700/781]  eta: 0:00:15  lr: 0.000007  training_loss: 0.2966 (0.3078)  mae_loss: 0.1908 (0.2029)  classification_loss: 0.1032 (0.1049)  time: 0.1967  data: 0.0002  max mem: 5511
[11:24:59.203870] Epoch: [90]  [720/781]  eta: 0:00:11  lr: 0.000007  training_loss: 0.3004 (0.3078)  mae_loss: 0.1981 (0.2030)  classification_loss: 0.0991 (0.1048)  time: 0.1979  data: 0.0003  max mem: 5511
[11:25:03.114254] Epoch: [90]  [740/781]  eta: 0:00:08  lr: 0.000007  training_loss: 0.3000 (0.3076)  mae_loss: 0.2024 (0.2028)  classification_loss: 0.1056 (0.1048)  time: 0.1954  data: 0.0003  max mem: 5511
[11:25:07.055907] Epoch: [90]  [760/781]  eta: 0:00:04  lr: 0.000007  training_loss: 0.3054 (0.3075)  mae_loss: 0.1989 (0.2026)  classification_loss: 0.1057 (0.1049)  time: 0.1970  data: 0.0004  max mem: 5511
[11:25:10.946028] Epoch: [90]  [780/781]  eta: 0:00:00  lr: 0.000006  training_loss: 0.3049 (0.3075)  mae_loss: 0.1909 (0.2025)  classification_loss: 0.1063 (0.1050)  time: 0.1944  data: 0.0002  max mem: 5511
[11:25:11.114127] Epoch: [90] Total time: 0:02:33 (0.1969 s / it)
[11:25:11.114594] Averaged stats: lr: 0.000006  training_loss: 0.3049 (0.3075)  mae_loss: 0.1909 (0.2025)  classification_loss: 0.1063 (0.1050)
[11:25:13.657176] Test:  [  0/157]  eta: 0:01:51  testing_loss: 0.4859 (0.4859)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.7120  data: 0.6745  max mem: 5511
[11:25:13.949150] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.3746 (0.3889)  acc1: 87.5000 (88.9205)  acc5: 100.0000 (99.5739)  time: 0.0911  data: 0.0615  max mem: 5511
[11:25:14.230846] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.3663 (0.3735)  acc1: 90.6250 (89.2113)  acc5: 100.0000 (99.6280)  time: 0.0285  data: 0.0001  max mem: 5511
[11:25:14.512425] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.3314 (0.3801)  acc1: 90.6250 (89.2137)  acc5: 100.0000 (99.4456)  time: 0.0280  data: 0.0001  max mem: 5511
[11:25:14.794257] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.3996 (0.3931)  acc1: 90.6250 (89.0625)  acc5: 100.0000 (99.3902)  time: 0.0280  data: 0.0001  max mem: 5511
[11:25:15.080228] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.3714 (0.3809)  acc1: 89.0625 (89.3995)  acc5: 100.0000 (99.3873)  time: 0.0282  data: 0.0002  max mem: 5511
[11:25:15.363978] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.3038 (0.3778)  acc1: 90.6250 (89.5492)  acc5: 100.0000 (99.4109)  time: 0.0283  data: 0.0002  max mem: 5511
[11:25:15.647789] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.3423 (0.3731)  acc1: 90.6250 (89.6567)  acc5: 100.0000 (99.4498)  time: 0.0283  data: 0.0002  max mem: 5511
[11:25:15.941445] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.3785 (0.3811)  acc1: 89.0625 (89.4290)  acc5: 100.0000 (99.4599)  time: 0.0288  data: 0.0002  max mem: 5511
[11:25:16.224223] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4183 (0.3788)  acc1: 89.0625 (89.5433)  acc5: 100.0000 (99.4849)  time: 0.0287  data: 0.0002  max mem: 5511
[11:25:16.505678] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.4183 (0.3834)  acc1: 89.0625 (89.4338)  acc5: 100.0000 (99.5050)  time: 0.0281  data: 0.0002  max mem: 5511
[11:25:16.794602] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.3906 (0.3821)  acc1: 89.0625 (89.5130)  acc5: 100.0000 (99.4932)  time: 0.0284  data: 0.0002  max mem: 5511
[11:25:17.077653] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.3465 (0.3801)  acc1: 90.6250 (89.5790)  acc5: 100.0000 (99.5222)  time: 0.0284  data: 0.0002  max mem: 5511
[11:25:17.361009] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.3683 (0.3817)  acc1: 89.0625 (89.4919)  acc5: 100.0000 (99.4990)  time: 0.0281  data: 0.0002  max mem: 5511
[11:25:17.642727] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3338 (0.3784)  acc1: 90.6250 (89.5612)  acc5: 100.0000 (99.5346)  time: 0.0280  data: 0.0002  max mem: 5511
[11:25:17.921430] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.3326 (0.3779)  acc1: 90.6250 (89.5695)  acc5: 100.0000 (99.5447)  time: 0.0279  data: 0.0001  max mem: 5511
[11:25:18.071835] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.3340 (0.3772)  acc1: 90.6250 (89.5300)  acc5: 100.0000 (99.5300)  time: 0.0269  data: 0.0001  max mem: 5511
[11:25:18.225087] Test: Total time: 0:00:05 (0.0336 s / it)
[11:25:18.225530] * Acc@1 89.530 Acc@5 99.530 loss 0.377
[11:25:18.225830] Accuracy of the network on the 10000 test images: 89.5%
[11:25:18.226029] Max accuracy: 89.53%
[11:25:18.604014] log_dir: ./output_dir
[11:25:19.398826] Epoch: [91]  [  0/781]  eta: 0:10:19  lr: 0.000006  training_loss: 0.2801 (0.2801)  mae_loss: 0.1934 (0.1934)  classification_loss: 0.0867 (0.0867)  time: 0.7933  data: 0.5789  max mem: 5511
[11:25:23.313684] Epoch: [91]  [ 20/781]  eta: 0:02:50  lr: 0.000006  training_loss: 0.2983 (0.3016)  mae_loss: 0.2018 (0.1999)  classification_loss: 0.1011 (0.1016)  time: 0.1956  data: 0.0005  max mem: 5511
[11:25:27.215904] Epoch: [91]  [ 40/781]  eta: 0:02:35  lr: 0.000006  training_loss: 0.3151 (0.3105)  mae_loss: 0.2203 (0.2070)  classification_loss: 0.1042 (0.1035)  time: 0.1950  data: 0.0002  max mem: 5511
[11:25:31.139800] Epoch: [91]  [ 60/781]  eta: 0:02:28  lr: 0.000006  training_loss: 0.3063 (0.3100)  mae_loss: 0.1978 (0.2049)  classification_loss: 0.1079 (0.1051)  time: 0.1959  data: 0.0002  max mem: 5511
[11:25:35.033652] Epoch: [91]  [ 80/781]  eta: 0:02:22  lr: 0.000006  training_loss: 0.3016 (0.3098)  mae_loss: 0.1982 (0.2051)  classification_loss: 0.1022 (0.1047)  time: 0.1946  data: 0.0003  max mem: 5511
[11:25:38.950342] Epoch: [91]  [100/781]  eta: 0:02:17  lr: 0.000006  training_loss: 0.3042 (0.3085)  mae_loss: 0.1965 (0.2040)  classification_loss: 0.1023 (0.1045)  time: 0.1958  data: 0.0002  max mem: 5511
[11:25:42.866005] Epoch: [91]  [120/781]  eta: 0:02:12  lr: 0.000006  training_loss: 0.3008 (0.3078)  mae_loss: 0.1955 (0.2037)  classification_loss: 0.1022 (0.1041)  time: 0.1957  data: 0.0002  max mem: 5511
[11:25:46.775881] Epoch: [91]  [140/781]  eta: 0:02:07  lr: 0.000006  training_loss: 0.3270 (0.3095)  mae_loss: 0.2132 (0.2054)  classification_loss: 0.1021 (0.1041)  time: 0.1954  data: 0.0002  max mem: 5511
[11:25:50.685506] Epoch: [91]  [160/781]  eta: 0:02:03  lr: 0.000006  training_loss: 0.3058 (0.3090)  mae_loss: 0.2019 (0.2050)  classification_loss: 0.0994 (0.1040)  time: 0.1954  data: 0.0002  max mem: 5511
[11:25:54.644733] Epoch: [91]  [180/781]  eta: 0:01:59  lr: 0.000006  training_loss: 0.3044 (0.3088)  mae_loss: 0.1982 (0.2046)  classification_loss: 0.1055 (0.1042)  time: 0.1978  data: 0.0002  max mem: 5511
[11:25:58.560098] Epoch: [91]  [200/781]  eta: 0:01:55  lr: 0.000006  training_loss: 0.2952 (0.3072)  mae_loss: 0.1958 (0.2031)  classification_loss: 0.1009 (0.1042)  time: 0.1957  data: 0.0002  max mem: 5511
[11:26:02.504091] Epoch: [91]  [220/781]  eta: 0:01:51  lr: 0.000006  training_loss: 0.3107 (0.3085)  mae_loss: 0.2093 (0.2040)  classification_loss: 0.1071 (0.1045)  time: 0.1971  data: 0.0005  max mem: 5511
[11:26:06.384508] Epoch: [91]  [240/781]  eta: 0:01:47  lr: 0.000006  training_loss: 0.3101 (0.3082)  mae_loss: 0.2015 (0.2038)  classification_loss: 0.0993 (0.1044)  time: 0.1940  data: 0.0002  max mem: 5511
[11:26:10.310883] Epoch: [91]  [260/781]  eta: 0:01:43  lr: 0.000006  training_loss: 0.2929 (0.3075)  mae_loss: 0.1901 (0.2031)  classification_loss: 0.1034 (0.1044)  time: 0.1962  data: 0.0002  max mem: 5511
[11:26:14.230080] Epoch: [91]  [280/781]  eta: 0:01:39  lr: 0.000006  training_loss: 0.3093 (0.3083)  mae_loss: 0.2076 (0.2038)  classification_loss: 0.1047 (0.1044)  time: 0.1959  data: 0.0003  max mem: 5511
[11:26:18.138538] Epoch: [91]  [300/781]  eta: 0:01:35  lr: 0.000006  training_loss: 0.3067 (0.3088)  mae_loss: 0.1979 (0.2040)  classification_loss: 0.1084 (0.1048)  time: 0.1953  data: 0.0002  max mem: 5511
[11:26:22.052269] Epoch: [91]  [320/781]  eta: 0:01:31  lr: 0.000006  training_loss: 0.3210 (0.3095)  mae_loss: 0.2130 (0.2047)  classification_loss: 0.1021 (0.1048)  time: 0.1956  data: 0.0002  max mem: 5511
[11:26:25.962566] Epoch: [91]  [340/781]  eta: 0:01:27  lr: 0.000006  training_loss: 0.3162 (0.3098)  mae_loss: 0.2089 (0.2050)  classification_loss: 0.1015 (0.1048)  time: 0.1954  data: 0.0002  max mem: 5511
[11:26:29.872888] Epoch: [91]  [360/781]  eta: 0:01:23  lr: 0.000006  training_loss: 0.3079 (0.3099)  mae_loss: 0.2071 (0.2052)  classification_loss: 0.1021 (0.1047)  time: 0.1954  data: 0.0003  max mem: 5511
[11:26:33.775456] Epoch: [91]  [380/781]  eta: 0:01:19  lr: 0.000006  training_loss: 0.3176 (0.3105)  mae_loss: 0.2111 (0.2055)  classification_loss: 0.1107 (0.1050)  time: 0.1950  data: 0.0002  max mem: 5511
[11:26:37.672606] Epoch: [91]  [400/781]  eta: 0:01:15  lr: 0.000006  training_loss: 0.3132 (0.3107)  mae_loss: 0.2116 (0.2057)  classification_loss: 0.1051 (0.1050)  time: 0.1948  data: 0.0002  max mem: 5511
[11:26:41.566927] Epoch: [91]  [420/781]  eta: 0:01:11  lr: 0.000006  training_loss: 0.3092 (0.3106)  mae_loss: 0.2022 (0.2056)  classification_loss: 0.1059 (0.1051)  time: 0.1946  data: 0.0002  max mem: 5511
[11:26:45.466473] Epoch: [91]  [440/781]  eta: 0:01:07  lr: 0.000006  training_loss: 0.3068 (0.3104)  mae_loss: 0.1985 (0.2054)  classification_loss: 0.1041 (0.1051)  time: 0.1949  data: 0.0002  max mem: 5511
[11:26:49.374678] Epoch: [91]  [460/781]  eta: 0:01:03  lr: 0.000006  training_loss: 0.2961 (0.3099)  mae_loss: 0.1889 (0.2050)  classification_loss: 0.1029 (0.1049)  time: 0.1953  data: 0.0001  max mem: 5511
[11:26:53.285460] Epoch: [91]  [480/781]  eta: 0:00:59  lr: 0.000006  training_loss: 0.3085 (0.3102)  mae_loss: 0.2020 (0.2051)  classification_loss: 0.1080 (0.1051)  time: 0.1954  data: 0.0002  max mem: 5511
[11:26:57.178006] Epoch: [91]  [500/781]  eta: 0:00:55  lr: 0.000006  training_loss: 0.2960 (0.3097)  mae_loss: 0.1984 (0.2047)  classification_loss: 0.1034 (0.1050)  time: 0.1946  data: 0.0002  max mem: 5511
[11:27:01.070278] Epoch: [91]  [520/781]  eta: 0:00:51  lr: 0.000006  training_loss: 0.3033 (0.3096)  mae_loss: 0.1965 (0.2046)  classification_loss: 0.1063 (0.1050)  time: 0.1945  data: 0.0003  max mem: 5511
[11:27:05.002910] Epoch: [91]  [540/781]  eta: 0:00:47  lr: 0.000006  training_loss: 0.2884 (0.3093)  mae_loss: 0.1892 (0.2044)  classification_loss: 0.0996 (0.1049)  time: 0.1966  data: 0.0002  max mem: 5511
[11:27:08.919736] Epoch: [91]  [560/781]  eta: 0:00:43  lr: 0.000006  training_loss: 0.3100 (0.3096)  mae_loss: 0.2008 (0.2048)  classification_loss: 0.1038 (0.1049)  time: 0.1957  data: 0.0002  max mem: 5511
[11:27:12.820027] Epoch: [91]  [580/781]  eta: 0:00:39  lr: 0.000006  training_loss: 0.3069 (0.3098)  mae_loss: 0.2085 (0.2049)  classification_loss: 0.1019 (0.1049)  time: 0.1949  data: 0.0002  max mem: 5511
[11:27:16.715233] Epoch: [91]  [600/781]  eta: 0:00:35  lr: 0.000006  training_loss: 0.3047 (0.3096)  mae_loss: 0.1989 (0.2049)  classification_loss: 0.0986 (0.1047)  time: 0.1947  data: 0.0002  max mem: 5511
[11:27:20.641953] Epoch: [91]  [620/781]  eta: 0:00:31  lr: 0.000006  training_loss: 0.3046 (0.3097)  mae_loss: 0.2008 (0.2049)  classification_loss: 0.1043 (0.1047)  time: 0.1963  data: 0.0002  max mem: 5511
[11:27:24.622254] Epoch: [91]  [640/781]  eta: 0:00:27  lr: 0.000006  training_loss: 0.3228 (0.3100)  mae_loss: 0.2148 (0.2052)  classification_loss: 0.1077 (0.1048)  time: 0.1989  data: 0.0002  max mem: 5511
[11:27:28.537424] Epoch: [91]  [660/781]  eta: 0:00:23  lr: 0.000005  training_loss: 0.3094 (0.3098)  mae_loss: 0.1990 (0.2050)  classification_loss: 0.0998 (0.1048)  time: 0.1957  data: 0.0002  max mem: 5511
[11:27:32.459832] Epoch: [91]  [680/781]  eta: 0:00:19  lr: 0.000005  training_loss: 0.3079 (0.3098)  mae_loss: 0.2046 (0.2050)  classification_loss: 0.1051 (0.1048)  time: 0.1960  data: 0.0002  max mem: 5511
[11:27:36.364855] Epoch: [91]  [700/781]  eta: 0:00:15  lr: 0.000005  training_loss: 0.3028 (0.3098)  mae_loss: 0.2025 (0.2051)  classification_loss: 0.1044 (0.1047)  time: 0.1952  data: 0.0002  max mem: 5511
[11:27:40.271238] Epoch: [91]  [720/781]  eta: 0:00:11  lr: 0.000005  training_loss: 0.3022 (0.3098)  mae_loss: 0.2049 (0.2050)  classification_loss: 0.1022 (0.1047)  time: 0.1952  data: 0.0002  max mem: 5511
[11:27:44.167089] Epoch: [91]  [740/781]  eta: 0:00:08  lr: 0.000005  training_loss: 0.3121 (0.3100)  mae_loss: 0.2067 (0.2053)  classification_loss: 0.1050 (0.1048)  time: 0.1947  data: 0.0003  max mem: 5511
[11:27:48.136317] Epoch: [91]  [760/781]  eta: 0:00:04  lr: 0.000005  training_loss: 0.3215 (0.3103)  mae_loss: 0.2059 (0.2054)  classification_loss: 0.1096 (0.1049)  time: 0.1984  data: 0.0002  max mem: 5511
[11:27:52.035893] Epoch: [91]  [780/781]  eta: 0:00:00  lr: 0.000005  training_loss: 0.3150 (0.3104)  mae_loss: 0.2056 (0.2055)  classification_loss: 0.1011 (0.1049)  time: 0.1949  data: 0.0002  max mem: 5511
[11:27:52.206362] Epoch: [91] Total time: 0:02:33 (0.1967 s / it)
[11:27:52.206831] Averaged stats: lr: 0.000005  training_loss: 0.3150 (0.3104)  mae_loss: 0.2056 (0.2055)  classification_loss: 0.1011 (0.1049)
[11:27:52.899690] Test:  [  0/157]  eta: 0:01:48  testing_loss: 0.4847 (0.4847)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.6882  data: 0.6579  max mem: 5511
[11:27:53.192646] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.3833 (0.3882)  acc1: 89.0625 (88.9205)  acc5: 100.0000 (99.5739)  time: 0.0890  data: 0.0604  max mem: 5511
[11:27:53.475940] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.3494 (0.3735)  acc1: 89.0625 (89.4345)  acc5: 100.0000 (99.6280)  time: 0.0286  data: 0.0004  max mem: 5511
[11:27:53.762636] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.3446 (0.3798)  acc1: 90.6250 (89.2641)  acc5: 100.0000 (99.4960)  time: 0.0284  data: 0.0002  max mem: 5511
[11:27:54.045337] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.3966 (0.3939)  acc1: 89.0625 (88.9482)  acc5: 100.0000 (99.3902)  time: 0.0283  data: 0.0002  max mem: 5511
[11:27:54.327200] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.3762 (0.3811)  acc1: 90.6250 (89.3995)  acc5: 98.4375 (99.3873)  time: 0.0281  data: 0.0002  max mem: 5511
[11:27:54.610464] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.3204 (0.3788)  acc1: 90.6250 (89.4723)  acc5: 100.0000 (99.3852)  time: 0.0281  data: 0.0002  max mem: 5511
[11:27:54.894876] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.3455 (0.3734)  acc1: 90.6250 (89.5907)  acc5: 100.0000 (99.4498)  time: 0.0283  data: 0.0002  max mem: 5511
[11:27:55.179005] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.3749 (0.3814)  acc1: 89.0625 (89.3904)  acc5: 100.0000 (99.4599)  time: 0.0283  data: 0.0002  max mem: 5511
[11:27:55.460886] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4073 (0.3788)  acc1: 89.0625 (89.5089)  acc5: 100.0000 (99.4849)  time: 0.0282  data: 0.0002  max mem: 5511
[11:27:55.742478] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4073 (0.3837)  acc1: 90.6250 (89.3255)  acc5: 100.0000 (99.5050)  time: 0.0281  data: 0.0002  max mem: 5511
[11:27:56.023342] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.3899 (0.3822)  acc1: 90.6250 (89.4144)  acc5: 100.0000 (99.5073)  time: 0.0280  data: 0.0001  max mem: 5511
[11:27:56.304022] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.3485 (0.3804)  acc1: 90.6250 (89.4886)  acc5: 100.0000 (99.5222)  time: 0.0280  data: 0.0001  max mem: 5511
[11:27:56.586364] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.3730 (0.3819)  acc1: 89.0625 (89.3368)  acc5: 100.0000 (99.4990)  time: 0.0280  data: 0.0001  max mem: 5511
[11:27:56.871546] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3259 (0.3783)  acc1: 89.0625 (89.3506)  acc5: 100.0000 (99.5346)  time: 0.0283  data: 0.0001  max mem: 5511
[11:27:57.150510] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.3259 (0.3777)  acc1: 89.0625 (89.3729)  acc5: 100.0000 (99.5344)  time: 0.0281  data: 0.0001  max mem: 5511
[11:27:57.300244] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.3319 (0.3771)  acc1: 89.0625 (89.3500)  acc5: 100.0000 (99.5300)  time: 0.0269  data: 0.0001  max mem: 5511
[11:27:57.456869] Test: Total time: 0:00:05 (0.0334 s / it)
[11:27:57.457619] * Acc@1 89.350 Acc@5 99.530 loss 0.377
[11:27:57.457969] Accuracy of the network on the 10000 test images: 89.3%
[11:27:57.458161] Max accuracy: 89.53%
[11:27:57.859661] log_dir: ./output_dir
[11:27:58.788235] Epoch: [92]  [  0/781]  eta: 0:12:03  lr: 0.000005  training_loss: 0.3018 (0.3018)  mae_loss: 0.2105 (0.2105)  classification_loss: 0.0913 (0.0913)  time: 0.9265  data: 0.7163  max mem: 5511
[11:28:02.685194] Epoch: [92]  [ 20/781]  eta: 0:02:54  lr: 0.000005  training_loss: 0.3003 (0.3089)  mae_loss: 0.2005 (0.2071)  classification_loss: 0.0998 (0.1018)  time: 0.1948  data: 0.0002  max mem: 5511
[11:28:06.598386] Epoch: [92]  [ 40/781]  eta: 0:02:37  lr: 0.000005  training_loss: 0.3011 (0.3082)  mae_loss: 0.1949 (0.2044)  classification_loss: 0.1041 (0.1038)  time: 0.1956  data: 0.0002  max mem: 5511
[11:28:10.529105] Epoch: [92]  [ 60/781]  eta: 0:02:29  lr: 0.000005  training_loss: 0.3045 (0.3083)  mae_loss: 0.1962 (0.2038)  classification_loss: 0.1057 (0.1045)  time: 0.1964  data: 0.0007  max mem: 5511
[11:28:14.458050] Epoch: [92]  [ 80/781]  eta: 0:02:23  lr: 0.000005  training_loss: 0.3018 (0.3074)  mae_loss: 0.2033 (0.2028)  classification_loss: 0.1054 (0.1046)  time: 0.1964  data: 0.0003  max mem: 5511
[11:28:18.366320] Epoch: [92]  [100/781]  eta: 0:02:18  lr: 0.000005  training_loss: 0.3059 (0.3087)  mae_loss: 0.1993 (0.2035)  classification_loss: 0.1074 (0.1052)  time: 0.1953  data: 0.0004  max mem: 5511
[11:28:22.309283] Epoch: [92]  [120/781]  eta: 0:02:13  lr: 0.000005  training_loss: 0.3199 (0.3103)  mae_loss: 0.2072 (0.2045)  classification_loss: 0.1091 (0.1058)  time: 0.1971  data: 0.0002  max mem: 5511
[11:28:26.258520] Epoch: [92]  [140/781]  eta: 0:02:09  lr: 0.000005  training_loss: 0.2995 (0.3093)  mae_loss: 0.1963 (0.2038)  classification_loss: 0.1079 (0.1055)  time: 0.1974  data: 0.0002  max mem: 5511
[11:28:30.153412] Epoch: [92]  [160/781]  eta: 0:02:04  lr: 0.000005  training_loss: 0.3203 (0.3106)  mae_loss: 0.2157 (0.2051)  classification_loss: 0.1048 (0.1055)  time: 0.1947  data: 0.0002  max mem: 5511
[11:28:34.075549] Epoch: [92]  [180/781]  eta: 0:02:00  lr: 0.000005  training_loss: 0.3074 (0.3106)  mae_loss: 0.2051 (0.2051)  classification_loss: 0.1034 (0.1054)  time: 0.1960  data: 0.0002  max mem: 5511
[11:28:37.975753] Epoch: [92]  [200/781]  eta: 0:01:55  lr: 0.000005  training_loss: 0.3075 (0.3105)  mae_loss: 0.2040 (0.2051)  classification_loss: 0.1023 (0.1054)  time: 0.1949  data: 0.0002  max mem: 5511
[11:28:41.871946] Epoch: [92]  [220/781]  eta: 0:01:51  lr: 0.000005  training_loss: 0.3081 (0.3105)  mae_loss: 0.2071 (0.2052)  classification_loss: 0.1013 (0.1053)  time: 0.1947  data: 0.0004  max mem: 5511
[11:28:45.772458] Epoch: [92]  [240/781]  eta: 0:01:47  lr: 0.000005  training_loss: 0.3011 (0.3103)  mae_loss: 0.2013 (0.2051)  classification_loss: 0.1041 (0.1052)  time: 0.1949  data: 0.0002  max mem: 5511
[11:28:49.686905] Epoch: [92]  [260/781]  eta: 0:01:43  lr: 0.000005  training_loss: 0.3036 (0.3100)  mae_loss: 0.1990 (0.2049)  classification_loss: 0.1051 (0.1051)  time: 0.1956  data: 0.0002  max mem: 5511
[11:28:53.643415] Epoch: [92]  [280/781]  eta: 0:01:39  lr: 0.000005  training_loss: 0.3079 (0.3100)  mae_loss: 0.2031 (0.2048)  classification_loss: 0.1053 (0.1052)  time: 0.1977  data: 0.0002  max mem: 5511
[11:28:57.554186] Epoch: [92]  [300/781]  eta: 0:01:35  lr: 0.000005  training_loss: 0.3152 (0.3101)  mae_loss: 0.2047 (0.2048)  classification_loss: 0.1078 (0.1053)  time: 0.1954  data: 0.0002  max mem: 5511
[11:29:01.483054] Epoch: [92]  [320/781]  eta: 0:01:31  lr: 0.000005  training_loss: 0.3098 (0.3100)  mae_loss: 0.2049 (0.2047)  classification_loss: 0.1046 (0.1053)  time: 0.1963  data: 0.0002  max mem: 5511
[11:29:05.396710] Epoch: [92]  [340/781]  eta: 0:01:27  lr: 0.000005  training_loss: 0.3050 (0.3100)  mae_loss: 0.2056 (0.2048)  classification_loss: 0.1026 (0.1052)  time: 0.1956  data: 0.0003  max mem: 5511
[11:29:09.322609] Epoch: [92]  [360/781]  eta: 0:01:23  lr: 0.000005  training_loss: 0.3035 (0.3100)  mae_loss: 0.1989 (0.2048)  classification_loss: 0.1028 (0.1051)  time: 0.1962  data: 0.0003  max mem: 5511
[11:29:13.262994] Epoch: [92]  [380/781]  eta: 0:01:19  lr: 0.000005  training_loss: 0.3009 (0.3102)  mae_loss: 0.2032 (0.2049)  classification_loss: 0.1071 (0.1052)  time: 0.1969  data: 0.0003  max mem: 5511
[11:29:17.158456] Epoch: [92]  [400/781]  eta: 0:01:15  lr: 0.000005  training_loss: 0.3063 (0.3099)  mae_loss: 0.1990 (0.2048)  classification_loss: 0.1015 (0.1051)  time: 0.1947  data: 0.0004  max mem: 5511
[11:29:21.065796] Epoch: [92]  [420/781]  eta: 0:01:11  lr: 0.000005  training_loss: 0.3200 (0.3105)  mae_loss: 0.2140 (0.2054)  classification_loss: 0.1050 (0.1051)  time: 0.1953  data: 0.0002  max mem: 5511
[11:29:24.973792] Epoch: [92]  [440/781]  eta: 0:01:07  lr: 0.000005  training_loss: 0.3042 (0.3104)  mae_loss: 0.2025 (0.2055)  classification_loss: 0.0992 (0.1049)  time: 0.1953  data: 0.0003  max mem: 5511
[11:29:28.895373] Epoch: [92]  [460/781]  eta: 0:01:03  lr: 0.000005  training_loss: 0.2908 (0.3097)  mae_loss: 0.1914 (0.2049)  classification_loss: 0.0989 (0.1048)  time: 0.1960  data: 0.0002  max mem: 5511
[11:29:32.816677] Epoch: [92]  [480/781]  eta: 0:00:59  lr: 0.000005  training_loss: 0.3057 (0.3095)  mae_loss: 0.1961 (0.2046)  classification_loss: 0.1104 (0.1049)  time: 0.1959  data: 0.0003  max mem: 5511
[11:29:36.704717] Epoch: [92]  [500/781]  eta: 0:00:55  lr: 0.000005  training_loss: 0.3147 (0.3100)  mae_loss: 0.2042 (0.2050)  classification_loss: 0.1070 (0.1050)  time: 0.1943  data: 0.0002  max mem: 5511
[11:29:40.602868] Epoch: [92]  [520/781]  eta: 0:00:51  lr: 0.000005  training_loss: 0.3127 (0.3103)  mae_loss: 0.2058 (0.2052)  classification_loss: 0.1089 (0.1051)  time: 0.1948  data: 0.0002  max mem: 5511
[11:29:44.521654] Epoch: [92]  [540/781]  eta: 0:00:47  lr: 0.000005  training_loss: 0.3130 (0.3106)  mae_loss: 0.2022 (0.2054)  classification_loss: 0.1108 (0.1052)  time: 0.1959  data: 0.0003  max mem: 5511
[11:29:48.429536] Epoch: [92]  [560/781]  eta: 0:00:43  lr: 0.000005  training_loss: 0.3069 (0.3106)  mae_loss: 0.2066 (0.2054)  classification_loss: 0.1059 (0.1051)  time: 0.1953  data: 0.0002  max mem: 5511
[11:29:52.343203] Epoch: [92]  [580/781]  eta: 0:00:39  lr: 0.000005  training_loss: 0.3151 (0.3107)  mae_loss: 0.1984 (0.2055)  classification_loss: 0.1063 (0.1052)  time: 0.1956  data: 0.0002  max mem: 5511
[11:29:56.258653] Epoch: [92]  [600/781]  eta: 0:00:35  lr: 0.000005  training_loss: 0.3068 (0.3104)  mae_loss: 0.1904 (0.2051)  classification_loss: 0.1025 (0.1052)  time: 0.1957  data: 0.0004  max mem: 5511
[11:30:00.172958] Epoch: [92]  [620/781]  eta: 0:00:31  lr: 0.000005  training_loss: 0.3110 (0.3105)  mae_loss: 0.2096 (0.2053)  classification_loss: 0.1029 (0.1052)  time: 0.1956  data: 0.0002  max mem: 5511
[11:30:04.081815] Epoch: [92]  [640/781]  eta: 0:00:27  lr: 0.000004  training_loss: 0.3213 (0.3107)  mae_loss: 0.2080 (0.2055)  classification_loss: 0.1062 (0.1052)  time: 0.1954  data: 0.0002  max mem: 5511
[11:30:08.005668] Epoch: [92]  [660/781]  eta: 0:00:23  lr: 0.000004  training_loss: 0.3065 (0.3106)  mae_loss: 0.1983 (0.2053)  classification_loss: 0.1111 (0.1053)  time: 0.1961  data: 0.0001  max mem: 5511
[11:30:11.914997] Epoch: [92]  [680/781]  eta: 0:00:19  lr: 0.000004  training_loss: 0.3085 (0.3105)  mae_loss: 0.2023 (0.2052)  classification_loss: 0.1057 (0.1053)  time: 0.1954  data: 0.0002  max mem: 5511
[11:30:15.808977] Epoch: [92]  [700/781]  eta: 0:00:15  lr: 0.000004  training_loss: 0.3118 (0.3107)  mae_loss: 0.2082 (0.2055)  classification_loss: 0.1024 (0.1052)  time: 0.1946  data: 0.0003  max mem: 5511
[11:30:19.711168] Epoch: [92]  [720/781]  eta: 0:00:11  lr: 0.000004  training_loss: 0.3021 (0.3107)  mae_loss: 0.2033 (0.2055)  classification_loss: 0.1058 (0.1052)  time: 0.1950  data: 0.0002  max mem: 5511
[11:30:23.611248] Epoch: [92]  [740/781]  eta: 0:00:08  lr: 0.000004  training_loss: 0.3039 (0.3105)  mae_loss: 0.2017 (0.2054)  classification_loss: 0.1020 (0.1052)  time: 0.1949  data: 0.0002  max mem: 5511
[11:30:27.520739] Epoch: [92]  [760/781]  eta: 0:00:04  lr: 0.000004  training_loss: 0.3099 (0.3107)  mae_loss: 0.2048 (0.2054)  classification_loss: 0.1077 (0.1053)  time: 0.1954  data: 0.0002  max mem: 5511
[11:30:31.418274] Epoch: [92]  [780/781]  eta: 0:00:00  lr: 0.000004  training_loss: 0.3114 (0.3106)  mae_loss: 0.2007 (0.2053)  classification_loss: 0.1064 (0.1053)  time: 0.1948  data: 0.0002  max mem: 5511
[11:30:31.596005] Epoch: [92] Total time: 0:02:33 (0.1968 s / it)
[11:30:31.596474] Averaged stats: lr: 0.000004  training_loss: 0.3114 (0.3106)  mae_loss: 0.2007 (0.2053)  classification_loss: 0.1064 (0.1053)
[11:30:32.294272] Test:  [  0/157]  eta: 0:01:48  testing_loss: 0.4696 (0.4696)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.6938  data: 0.6641  max mem: 5511
[11:30:32.581628] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.3689 (0.3840)  acc1: 89.0625 (89.4886)  acc5: 100.0000 (99.7159)  time: 0.0889  data: 0.0606  max mem: 5511
[11:30:32.865024] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.3566 (0.3707)  acc1: 90.6250 (89.6577)  acc5: 100.0000 (99.7024)  time: 0.0283  data: 0.0002  max mem: 5511
[11:30:33.149272] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.3322 (0.3754)  acc1: 90.6250 (89.3649)  acc5: 100.0000 (99.5464)  time: 0.0282  data: 0.0002  max mem: 5511
[11:30:33.430346] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.3905 (0.3899)  acc1: 89.0625 (88.9482)  acc5: 100.0000 (99.4665)  time: 0.0281  data: 0.0002  max mem: 5511
[11:30:33.713291] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.3694 (0.3777)  acc1: 89.0625 (89.2770)  acc5: 100.0000 (99.4485)  time: 0.0281  data: 0.0002  max mem: 5511
[11:30:33.994767] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.3090 (0.3758)  acc1: 90.6250 (89.3186)  acc5: 100.0000 (99.4621)  time: 0.0281  data: 0.0002  max mem: 5511
[11:30:34.278476] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.3411 (0.3705)  acc1: 90.6250 (89.4586)  acc5: 100.0000 (99.5379)  time: 0.0281  data: 0.0002  max mem: 5511
[11:30:34.560882] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.3773 (0.3783)  acc1: 89.0625 (89.2747)  acc5: 100.0000 (99.5370)  time: 0.0281  data: 0.0002  max mem: 5511
[11:30:34.842915] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4088 (0.3763)  acc1: 89.0625 (89.4059)  acc5: 100.0000 (99.5536)  time: 0.0281  data: 0.0002  max mem: 5511
[11:30:35.126186] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4100 (0.3806)  acc1: 89.0625 (89.2946)  acc5: 100.0000 (99.5668)  time: 0.0281  data: 0.0002  max mem: 5511
[11:30:35.416365] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.3996 (0.3794)  acc1: 89.0625 (89.4285)  acc5: 100.0000 (99.5495)  time: 0.0285  data: 0.0002  max mem: 5511
[11:30:35.700119] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.3604 (0.3775)  acc1: 90.6250 (89.5403)  acc5: 100.0000 (99.5739)  time: 0.0285  data: 0.0002  max mem: 5511
[11:30:35.984039] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.3624 (0.3787)  acc1: 89.0625 (89.4442)  acc5: 100.0000 (99.5587)  time: 0.0282  data: 0.0002  max mem: 5511
[11:30:36.269656] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3249 (0.3752)  acc1: 89.0625 (89.5058)  acc5: 100.0000 (99.5789)  time: 0.0283  data: 0.0002  max mem: 5511
[11:30:36.548845] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.3231 (0.3746)  acc1: 90.6250 (89.5075)  acc5: 100.0000 (99.5757)  time: 0.0281  data: 0.0001  max mem: 5511
[11:30:36.699038] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.3231 (0.3737)  acc1: 90.6250 (89.5000)  acc5: 100.0000 (99.5700)  time: 0.0269  data: 0.0001  max mem: 5511
[11:30:36.846730] Test: Total time: 0:00:05 (0.0334 s / it)
[11:30:36.847202] * Acc@1 89.500 Acc@5 99.570 loss 0.374
[11:30:36.847490] Accuracy of the network on the 10000 test images: 89.5%
[11:30:36.847983] Max accuracy: 89.53%
[11:30:37.160081] log_dir: ./output_dir
[11:30:37.963691] Epoch: [93]  [  0/781]  eta: 0:10:26  lr: 0.000004  training_loss: 0.2731 (0.2731)  mae_loss: 0.1733 (0.1733)  classification_loss: 0.0998 (0.0998)  time: 0.8019  data: 0.5973  max mem: 5511
[11:30:41.855262] Epoch: [93]  [ 20/781]  eta: 0:02:50  lr: 0.000004  training_loss: 0.3057 (0.3092)  mae_loss: 0.2120 (0.2080)  classification_loss: 0.1003 (0.1013)  time: 0.1945  data: 0.0002  max mem: 5511
[11:30:45.772244] Epoch: [93]  [ 40/781]  eta: 0:02:35  lr: 0.000004  training_loss: 0.2917 (0.3033)  mae_loss: 0.1829 (0.2001)  classification_loss: 0.1044 (0.1031)  time: 0.1957  data: 0.0003  max mem: 5511
[11:30:49.689058] Epoch: [93]  [ 60/781]  eta: 0:02:27  lr: 0.000004  training_loss: 0.3161 (0.3083)  mae_loss: 0.2171 (0.2052)  classification_loss: 0.1024 (0.1032)  time: 0.1958  data: 0.0002  max mem: 5511
[11:30:53.591161] Epoch: [93]  [ 80/781]  eta: 0:02:22  lr: 0.000004  training_loss: 0.3013 (0.3084)  mae_loss: 0.1956 (0.2055)  classification_loss: 0.1016 (0.1029)  time: 0.1950  data: 0.0003  max mem: 5511
[11:30:57.515981] Epoch: [93]  [100/781]  eta: 0:02:17  lr: 0.000004  training_loss: 0.2965 (0.3079)  mae_loss: 0.1910 (0.2045)  classification_loss: 0.1048 (0.1035)  time: 0.1961  data: 0.0003  max mem: 5511
[11:31:01.420506] Epoch: [93]  [120/781]  eta: 0:02:12  lr: 0.000004  training_loss: 0.3033 (0.3076)  mae_loss: 0.2050 (0.2044)  classification_loss: 0.1006 (0.1032)  time: 0.1951  data: 0.0002  max mem: 5511
[11:31:05.333578] Epoch: [93]  [140/781]  eta: 0:02:08  lr: 0.000004  training_loss: 0.2975 (0.3070)  mae_loss: 0.1947 (0.2036)  classification_loss: 0.1035 (0.1033)  time: 0.1956  data: 0.0003  max mem: 5511
[11:31:09.214422] Epoch: [93]  [160/781]  eta: 0:02:03  lr: 0.000004  training_loss: 0.3008 (0.3070)  mae_loss: 0.1975 (0.2036)  classification_loss: 0.1033 (0.1033)  time: 0.1940  data: 0.0002  max mem: 5511
[11:31:13.165507] Epoch: [93]  [180/781]  eta: 0:01:59  lr: 0.000004  training_loss: 0.3005 (0.3067)  mae_loss: 0.2045 (0.2036)  classification_loss: 0.1033 (0.1032)  time: 0.1974  data: 0.0003  max mem: 5511
[11:31:17.081169] Epoch: [93]  [200/781]  eta: 0:01:55  lr: 0.000004  training_loss: 0.3040 (0.3062)  mae_loss: 0.1932 (0.2033)  classification_loss: 0.0976 (0.1030)  time: 0.1957  data: 0.0002  max mem: 5511
[11:31:20.977498] Epoch: [93]  [220/781]  eta: 0:01:51  lr: 0.000004  training_loss: 0.2872 (0.3053)  mae_loss: 0.1891 (0.2024)  classification_loss: 0.1005 (0.1029)  time: 0.1947  data: 0.0002  max mem: 5511
[11:31:24.883604] Epoch: [93]  [240/781]  eta: 0:01:47  lr: 0.000004  training_loss: 0.2880 (0.3051)  mae_loss: 0.1868 (0.2023)  classification_loss: 0.1004 (0.1029)  time: 0.1952  data: 0.0003  max mem: 5511
[11:31:28.781807] Epoch: [93]  [260/781]  eta: 0:01:42  lr: 0.000004  training_loss: 0.3098 (0.3059)  mae_loss: 0.2032 (0.2028)  classification_loss: 0.1035 (0.1031)  time: 0.1948  data: 0.0002  max mem: 5511
[11:31:32.688996] Epoch: [93]  [280/781]  eta: 0:01:38  lr: 0.000004  training_loss: 0.3113 (0.3062)  mae_loss: 0.2071 (0.2028)  classification_loss: 0.1035 (0.1033)  time: 0.1953  data: 0.0002  max mem: 5511
[11:31:36.606416] Epoch: [93]  [300/781]  eta: 0:01:34  lr: 0.000004  training_loss: 0.3099 (0.3067)  mae_loss: 0.2073 (0.2032)  classification_loss: 0.1065 (0.1035)  time: 0.1958  data: 0.0002  max mem: 5511
[11:31:40.505246] Epoch: [93]  [320/781]  eta: 0:01:30  lr: 0.000004  training_loss: 0.2944 (0.3064)  mae_loss: 0.1925 (0.2027)  classification_loss: 0.1043 (0.1036)  time: 0.1949  data: 0.0002  max mem: 5511
[11:31:44.413364] Epoch: [93]  [340/781]  eta: 0:01:26  lr: 0.000004  training_loss: 0.3054 (0.3066)  mae_loss: 0.2040 (0.2032)  classification_loss: 0.1001 (0.1035)  time: 0.1953  data: 0.0004  max mem: 5511
[11:31:48.329916] Epoch: [93]  [360/781]  eta: 0:01:22  lr: 0.000004  training_loss: 0.3143 (0.3073)  mae_loss: 0.2047 (0.2036)  classification_loss: 0.1082 (0.1037)  time: 0.1958  data: 0.0002  max mem: 5511
[11:31:52.234858] Epoch: [93]  [380/781]  eta: 0:01:18  lr: 0.000004  training_loss: 0.3045 (0.3071)  mae_loss: 0.2001 (0.2034)  classification_loss: 0.1032 (0.1036)  time: 0.1952  data: 0.0004  max mem: 5511
[11:31:56.143050] Epoch: [93]  [400/781]  eta: 0:01:15  lr: 0.000004  training_loss: 0.3165 (0.3077)  mae_loss: 0.2073 (0.2040)  classification_loss: 0.1054 (0.1037)  time: 0.1953  data: 0.0002  max mem: 5511
[11:32:00.046378] Epoch: [93]  [420/781]  eta: 0:01:11  lr: 0.000004  training_loss: 0.2938 (0.3073)  mae_loss: 0.1893 (0.2036)  classification_loss: 0.1043 (0.1037)  time: 0.1951  data: 0.0002  max mem: 5511
[11:32:03.979426] Epoch: [93]  [440/781]  eta: 0:01:07  lr: 0.000004  training_loss: 0.3072 (0.3074)  mae_loss: 0.1997 (0.2034)  classification_loss: 0.1075 (0.1039)  time: 0.1966  data: 0.0002  max mem: 5511
[11:32:07.896772] Epoch: [93]  [460/781]  eta: 0:01:03  lr: 0.000004  training_loss: 0.3004 (0.3072)  mae_loss: 0.1985 (0.2033)  classification_loss: 0.1025 (0.1039)  time: 0.1957  data: 0.0002  max mem: 5511
[11:32:11.821566] Epoch: [93]  [480/781]  eta: 0:00:59  lr: 0.000004  training_loss: 0.3132 (0.3074)  mae_loss: 0.2027 (0.2034)  classification_loss: 0.1053 (0.1040)  time: 0.1962  data: 0.0002  max mem: 5511
[11:32:15.775763] Epoch: [93]  [500/781]  eta: 0:00:55  lr: 0.000004  training_loss: 0.3047 (0.3073)  mae_loss: 0.1916 (0.2031)  classification_loss: 0.1065 (0.1042)  time: 0.1976  data: 0.0002  max mem: 5511
[11:32:19.679249] Epoch: [93]  [520/781]  eta: 0:00:51  lr: 0.000004  training_loss: 0.3086 (0.3075)  mae_loss: 0.2009 (0.2033)  classification_loss: 0.1049 (0.1042)  time: 0.1951  data: 0.0002  max mem: 5511
[11:32:23.601545] Epoch: [93]  [540/781]  eta: 0:00:47  lr: 0.000004  training_loss: 0.2977 (0.3074)  mae_loss: 0.1882 (0.2031)  classification_loss: 0.1034 (0.1042)  time: 0.1960  data: 0.0004  max mem: 5511
[11:32:27.565444] Epoch: [93]  [560/781]  eta: 0:00:43  lr: 0.000004  training_loss: 0.3015 (0.3073)  mae_loss: 0.1981 (0.2030)  classification_loss: 0.1049 (0.1043)  time: 0.1981  data: 0.0002  max mem: 5511
[11:32:31.481555] Epoch: [93]  [580/781]  eta: 0:00:39  lr: 0.000004  training_loss: 0.2992 (0.3072)  mae_loss: 0.1979 (0.2029)  classification_loss: 0.1013 (0.1042)  time: 0.1957  data: 0.0002  max mem: 5511
[11:32:35.429327] Epoch: [93]  [600/781]  eta: 0:00:35  lr: 0.000004  training_loss: 0.3079 (0.3073)  mae_loss: 0.2106 (0.2032)  classification_loss: 0.0997 (0.1041)  time: 0.1973  data: 0.0002  max mem: 5511
[11:32:39.349639] Epoch: [93]  [620/781]  eta: 0:00:31  lr: 0.000004  training_loss: 0.3236 (0.3077)  mae_loss: 0.2165 (0.2035)  classification_loss: 0.1043 (0.1042)  time: 0.1959  data: 0.0001  max mem: 5511
[11:32:43.262834] Epoch: [93]  [640/781]  eta: 0:00:27  lr: 0.000004  training_loss: 0.3226 (0.3082)  mae_loss: 0.2133 (0.2040)  classification_loss: 0.1054 (0.1042)  time: 0.1956  data: 0.0002  max mem: 5511
[11:32:47.159279] Epoch: [93]  [660/781]  eta: 0:00:23  lr: 0.000004  training_loss: 0.3050 (0.3082)  mae_loss: 0.1924 (0.2040)  classification_loss: 0.1016 (0.1042)  time: 0.1948  data: 0.0002  max mem: 5511
[11:32:51.096629] Epoch: [93]  [680/781]  eta: 0:00:19  lr: 0.000004  training_loss: 0.3219 (0.3084)  mae_loss: 0.2119 (0.2041)  classification_loss: 0.1074 (0.1043)  time: 0.1968  data: 0.0002  max mem: 5511
[11:32:55.027947] Epoch: [93]  [700/781]  eta: 0:00:15  lr: 0.000004  training_loss: 0.3054 (0.3085)  mae_loss: 0.2089 (0.2042)  classification_loss: 0.1035 (0.1043)  time: 0.1964  data: 0.0002  max mem: 5511
[11:32:58.941633] Epoch: [93]  [720/781]  eta: 0:00:11  lr: 0.000004  training_loss: 0.2969 (0.3084)  mae_loss: 0.1898 (0.2040)  classification_loss: 0.1090 (0.1044)  time: 0.1956  data: 0.0002  max mem: 5511
[11:33:02.853469] Epoch: [93]  [740/781]  eta: 0:00:08  lr: 0.000003  training_loss: 0.2862 (0.3081)  mae_loss: 0.1850 (0.2038)  classification_loss: 0.1036 (0.1044)  time: 0.1955  data: 0.0002  max mem: 5511
[11:33:06.820700] Epoch: [93]  [760/781]  eta: 0:00:04  lr: 0.000003  training_loss: 0.3142 (0.3083)  mae_loss: 0.2047 (0.2040)  classification_loss: 0.1042 (0.1044)  time: 0.1983  data: 0.0002  max mem: 5511
[11:33:10.734328] Epoch: [93]  [780/781]  eta: 0:00:00  lr: 0.000003  training_loss: 0.2966 (0.3082)  mae_loss: 0.1919 (0.2038)  classification_loss: 0.1030 (0.1043)  time: 0.1956  data: 0.0002  max mem: 5511
[11:33:10.916379] Epoch: [93] Total time: 0:02:33 (0.1969 s / it)
[11:33:10.916835] Averaged stats: lr: 0.000003  training_loss: 0.2966 (0.3082)  mae_loss: 0.1919 (0.2038)  classification_loss: 0.1030 (0.1043)
[11:33:11.627781] Test:  [  0/157]  eta: 0:01:50  testing_loss: 0.4694 (0.4694)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.7062  data: 0.6771  max mem: 5511
[11:33:11.915425] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.3704 (0.3799)  acc1: 89.0625 (89.3466)  acc5: 100.0000 (99.7159)  time: 0.0902  data: 0.0617  max mem: 5511
[11:33:12.200402] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.3397 (0.3695)  acc1: 90.6250 (89.8810)  acc5: 100.0000 (99.7024)  time: 0.0285  data: 0.0002  max mem: 5511
[11:33:12.488225] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.3331 (0.3745)  acc1: 90.6250 (89.8690)  acc5: 100.0000 (99.5464)  time: 0.0285  data: 0.0002  max mem: 5511
[11:33:12.773173] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.3826 (0.3882)  acc1: 89.0625 (89.3293)  acc5: 100.0000 (99.4665)  time: 0.0285  data: 0.0002  max mem: 5511
[11:33:13.055406] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.3636 (0.3754)  acc1: 89.0625 (89.7059)  acc5: 100.0000 (99.4485)  time: 0.0282  data: 0.0002  max mem: 5511
[11:33:13.339071] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.3113 (0.3733)  acc1: 90.6250 (89.8053)  acc5: 100.0000 (99.4621)  time: 0.0282  data: 0.0002  max mem: 5511
[11:33:13.624840] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.3363 (0.3683)  acc1: 90.6250 (89.9208)  acc5: 100.0000 (99.5379)  time: 0.0283  data: 0.0002  max mem: 5511
[11:33:13.910304] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.3656 (0.3762)  acc1: 89.0625 (89.6412)  acc5: 100.0000 (99.4985)  time: 0.0284  data: 0.0002  max mem: 5511
[11:33:14.197445] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.3985 (0.3742)  acc1: 89.0625 (89.7321)  acc5: 100.0000 (99.5192)  time: 0.0285  data: 0.0002  max mem: 5511
[11:33:14.480492] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.4029 (0.3781)  acc1: 89.0625 (89.6349)  acc5: 100.0000 (99.5514)  time: 0.0284  data: 0.0001  max mem: 5511
[11:33:14.764555] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.4025 (0.3772)  acc1: 89.0625 (89.7100)  acc5: 100.0000 (99.5355)  time: 0.0282  data: 0.0002  max mem: 5511
[11:33:15.052087] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.3596 (0.3757)  acc1: 89.0625 (89.7211)  acc5: 100.0000 (99.5610)  time: 0.0285  data: 0.0002  max mem: 5511
[11:33:15.336391] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.3605 (0.3767)  acc1: 89.0625 (89.6231)  acc5: 100.0000 (99.5468)  time: 0.0285  data: 0.0002  max mem: 5511
[11:33:15.617856] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3158 (0.3733)  acc1: 90.6250 (89.7052)  acc5: 100.0000 (99.5789)  time: 0.0282  data: 0.0001  max mem: 5511
[11:33:15.898261] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.3117 (0.3729)  acc1: 90.6250 (89.7144)  acc5: 100.0000 (99.5964)  time: 0.0280  data: 0.0001  max mem: 5511
[11:33:16.049437] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.3117 (0.3718)  acc1: 90.6250 (89.6700)  acc5: 100.0000 (99.5800)  time: 0.0270  data: 0.0001  max mem: 5511
[11:33:16.188416] Test: Total time: 0:00:05 (0.0336 s / it)
[11:33:16.189208] * Acc@1 89.670 Acc@5 99.580 loss 0.372
[11:33:16.189533] Accuracy of the network on the 10000 test images: 89.7%
[11:33:16.189712] Max accuracy: 89.67%
[11:33:16.611245] log_dir: ./output_dir
[11:33:17.386799] Epoch: [94]  [  0/781]  eta: 0:10:04  lr: 0.000003  training_loss: 0.3261 (0.3261)  mae_loss: 0.2218 (0.2218)  classification_loss: 0.1043 (0.1043)  time: 0.7743  data: 0.5415  max mem: 5511
[11:33:21.318781] Epoch: [94]  [ 20/781]  eta: 0:02:50  lr: 0.000003  training_loss: 0.3098 (0.3140)  mae_loss: 0.2087 (0.2131)  classification_loss: 0.0957 (0.1009)  time: 0.1965  data: 0.0002  max mem: 5511
[11:33:25.217465] Epoch: [94]  [ 40/781]  eta: 0:02:35  lr: 0.000003  training_loss: 0.3072 (0.3107)  mae_loss: 0.1967 (0.2068)  classification_loss: 0.1049 (0.1040)  time: 0.1948  data: 0.0002  max mem: 5511
[11:33:29.113076] Epoch: [94]  [ 60/781]  eta: 0:02:27  lr: 0.000003  training_loss: 0.3092 (0.3088)  mae_loss: 0.2034 (0.2048)  classification_loss: 0.1025 (0.1040)  time: 0.1947  data: 0.0002  max mem: 5511
[11:33:33.022172] Epoch: [94]  [ 80/781]  eta: 0:02:21  lr: 0.000003  training_loss: 0.3133 (0.3105)  mae_loss: 0.2113 (0.2070)  classification_loss: 0.1023 (0.1035)  time: 0.1954  data: 0.0002  max mem: 5511
[11:33:36.977698] Epoch: [94]  [100/781]  eta: 0:02:17  lr: 0.000003  training_loss: 0.3027 (0.3087)  mae_loss: 0.1964 (0.2048)  classification_loss: 0.1064 (0.1039)  time: 0.1977  data: 0.0002  max mem: 5511
[11:33:40.903482] Epoch: [94]  [120/781]  eta: 0:02:12  lr: 0.000003  training_loss: 0.2941 (0.3083)  mae_loss: 0.1947 (0.2041)  classification_loss: 0.1045 (0.1042)  time: 0.1962  data: 0.0002  max mem: 5511
[11:33:44.857715] Epoch: [94]  [140/781]  eta: 0:02:08  lr: 0.000003  training_loss: 0.3032 (0.3083)  mae_loss: 0.2068 (0.2040)  classification_loss: 0.1027 (0.1043)  time: 0.1976  data: 0.0002  max mem: 5511
[11:33:48.776702] Epoch: [94]  [160/781]  eta: 0:02:04  lr: 0.000003  training_loss: 0.2911 (0.3069)  mae_loss: 0.1917 (0.2033)  classification_loss: 0.0981 (0.1036)  time: 0.1959  data: 0.0002  max mem: 5511
[11:33:52.691054] Epoch: [94]  [180/781]  eta: 0:01:59  lr: 0.000003  training_loss: 0.2960 (0.3065)  mae_loss: 0.1929 (0.2027)  classification_loss: 0.1032 (0.1037)  time: 0.1956  data: 0.0002  max mem: 5511
[11:33:56.585962] Epoch: [94]  [200/781]  eta: 0:01:55  lr: 0.000003  training_loss: 0.2953 (0.3061)  mae_loss: 0.1950 (0.2026)  classification_loss: 0.1025 (0.1036)  time: 0.1947  data: 0.0002  max mem: 5511
[11:34:00.513986] Epoch: [94]  [220/781]  eta: 0:01:51  lr: 0.000003  training_loss: 0.3255 (0.3077)  mae_loss: 0.2019 (0.2036)  classification_loss: 0.1057 (0.1041)  time: 0.1963  data: 0.0002  max mem: 5511
[11:34:04.416349] Epoch: [94]  [240/781]  eta: 0:01:47  lr: 0.000003  training_loss: 0.2993 (0.3077)  mae_loss: 0.1965 (0.2034)  classification_loss: 0.1012 (0.1043)  time: 0.1950  data: 0.0003  max mem: 5511
[11:34:08.319359] Epoch: [94]  [260/781]  eta: 0:01:43  lr: 0.000003  training_loss: 0.3027 (0.3079)  mae_loss: 0.1966 (0.2035)  classification_loss: 0.1022 (0.1044)  time: 0.1951  data: 0.0002  max mem: 5511
[11:34:12.290194] Epoch: [94]  [280/781]  eta: 0:01:39  lr: 0.000003  training_loss: 0.3148 (0.3084)  mae_loss: 0.2112 (0.2042)  classification_loss: 0.1012 (0.1042)  time: 0.1984  data: 0.0002  max mem: 5511
[11:34:16.214134] Epoch: [94]  [300/781]  eta: 0:01:35  lr: 0.000003  training_loss: 0.2954 (0.3080)  mae_loss: 0.1981 (0.2039)  classification_loss: 0.1026 (0.1041)  time: 0.1961  data: 0.0002  max mem: 5511
[11:34:20.121021] Epoch: [94]  [320/781]  eta: 0:01:31  lr: 0.000003  training_loss: 0.3168 (0.3086)  mae_loss: 0.2060 (0.2043)  classification_loss: 0.1022 (0.1042)  time: 0.1952  data: 0.0002  max mem: 5511
[11:34:24.061385] Epoch: [94]  [340/781]  eta: 0:01:27  lr: 0.000003  training_loss: 0.2970 (0.3081)  mae_loss: 0.2080 (0.2044)  classification_loss: 0.0926 (0.1037)  time: 0.1969  data: 0.0002  max mem: 5511
[11:34:27.988406] Epoch: [94]  [360/781]  eta: 0:01:23  lr: 0.000003  training_loss: 0.3046 (0.3081)  mae_loss: 0.2020 (0.2044)  classification_loss: 0.1048 (0.1038)  time: 0.1962  data: 0.0002  max mem: 5511
[11:34:31.948580] Epoch: [94]  [380/781]  eta: 0:01:19  lr: 0.000003  training_loss: 0.2943 (0.3078)  mae_loss: 0.1939 (0.2040)  classification_loss: 0.1038 (0.1038)  time: 0.1979  data: 0.0003  max mem: 5511
[11:34:35.843640] Epoch: [94]  [400/781]  eta: 0:01:15  lr: 0.000003  training_loss: 0.3044 (0.3081)  mae_loss: 0.2065 (0.2043)  classification_loss: 0.1014 (0.1038)  time: 0.1947  data: 0.0002  max mem: 5511
[11:34:39.741066] Epoch: [94]  [420/781]  eta: 0:01:11  lr: 0.000003  training_loss: 0.3021 (0.3080)  mae_loss: 0.1958 (0.2042)  classification_loss: 0.1016 (0.1038)  time: 0.1948  data: 0.0002  max mem: 5511
[11:34:43.667349] Epoch: [94]  [440/781]  eta: 0:01:07  lr: 0.000003  training_loss: 0.2932 (0.3078)  mae_loss: 0.1955 (0.2041)  classification_loss: 0.1010 (0.1037)  time: 0.1962  data: 0.0003  max mem: 5511
[11:34:47.637588] Epoch: [94]  [460/781]  eta: 0:01:03  lr: 0.000003  training_loss: 0.2915 (0.3077)  mae_loss: 0.1927 (0.2040)  classification_loss: 0.1008 (0.1037)  time: 0.1984  data: 0.0002  max mem: 5511
[11:34:51.543567] Epoch: [94]  [480/781]  eta: 0:00:59  lr: 0.000003  training_loss: 0.3036 (0.3075)  mae_loss: 0.1994 (0.2039)  classification_loss: 0.1004 (0.1036)  time: 0.1952  data: 0.0002  max mem: 5511
[11:34:55.456293] Epoch: [94]  [500/781]  eta: 0:00:55  lr: 0.000003  training_loss: 0.3095 (0.3075)  mae_loss: 0.2004 (0.2040)  classification_loss: 0.1023 (0.1036)  time: 0.1956  data: 0.0002  max mem: 5511
[11:34:59.393364] Epoch: [94]  [520/781]  eta: 0:00:51  lr: 0.000003  training_loss: 0.3077 (0.3078)  mae_loss: 0.2100 (0.2041)  classification_loss: 0.1056 (0.1037)  time: 0.1968  data: 0.0002  max mem: 5511
[11:35:03.279103] Epoch: [94]  [540/781]  eta: 0:00:47  lr: 0.000003  training_loss: 0.3112 (0.3079)  mae_loss: 0.1947 (0.2043)  classification_loss: 0.1000 (0.1037)  time: 0.1942  data: 0.0002  max mem: 5511
[11:35:07.164017] Epoch: [94]  [560/781]  eta: 0:00:43  lr: 0.000003  training_loss: 0.2970 (0.3078)  mae_loss: 0.1908 (0.2040)  classification_loss: 0.1042 (0.1038)  time: 0.1942  data: 0.0002  max mem: 5511
[11:35:11.058905] Epoch: [94]  [580/781]  eta: 0:00:39  lr: 0.000003  training_loss: 0.2972 (0.3076)  mae_loss: 0.1963 (0.2038)  classification_loss: 0.1035 (0.1038)  time: 0.1947  data: 0.0002  max mem: 5511

[11:35:14.949490] Epoch: [94]  [600/781]  eta: 0:00:35  lr: 0.000003  training_loss: 0.3007 (0.3077)  mae_loss: 0.1978 (0.2039)  classification_loss: 0.1003 (0.1038)  time: 0.1944  data: 0.0002  max mem: 5511
[11:35:18.858929] Epoch: [94]  [620/781]  eta: 0:00:31  lr: 0.000003  training_loss: 0.2965 (0.3076)  mae_loss: 0.1965 (0.2039)  classification_loss: 0.0982 (0.1038)  time: 0.1954  data: 0.0002  max mem: 5511
[11:35:22.782673] Epoch: [94]  [640/781]  eta: 0:00:27  lr: 0.000003  training_loss: 0.3051 (0.3078)  mae_loss: 0.2043 (0.2039)  classification_loss: 0.1068 (0.1038)  time: 0.1961  data: 0.0003  max mem: 5511
[11:35:26.689228] Epoch: [94]  [660/781]  eta: 0:00:23  lr: 0.000003  training_loss: 0.3127 (0.3078)  mae_loss: 0.2036 (0.2039)  classification_loss: 0.1070 (0.1039)  time: 0.1952  data: 0.0002  max mem: 5511
[11:35:30.600205] Epoch: [94]  [680/781]  eta: 0:00:19  lr: 0.000003  training_loss: 0.3108 (0.3080)  mae_loss: 0.2064 (0.2040)  classification_loss: 0.1032 (0.1040)  time: 0.1955  data: 0.0002  max mem: 5511
[11:35:34.488690] Epoch: [94]  [700/781]  eta: 0:00:15  lr: 0.000003  training_loss: 0.3105 (0.3080)  mae_loss: 0.2001 (0.2039)  classification_loss: 0.1052 (0.1041)  time: 0.1943  data: 0.0002  max mem: 5511
[11:35:38.385804] Epoch: [94]  [720/781]  eta: 0:00:11  lr: 0.000003  training_loss: 0.2997 (0.3077)  mae_loss: 0.1965 (0.2037)  classification_loss: 0.1004 (0.1041)  time: 0.1948  data: 0.0002  max mem: 5511
[11:35:42.277756] Epoch: [94]  [740/781]  eta: 0:00:08  lr: 0.000003  training_loss: 0.2941 (0.3076)  mae_loss: 0.1938 (0.2036)  classification_loss: 0.0996 (0.1040)  time: 0.1945  data: 0.0002  max mem: 5511
[11:35:46.174287] Epoch: [94]  [760/781]  eta: 0:00:04  lr: 0.000003  training_loss: 0.3164 (0.3078)  mae_loss: 0.2192 (0.2039)  classification_loss: 0.1021 (0.1039)  time: 0.1947  data: 0.0002  max mem: 5511
[11:35:50.067216] Epoch: [94]  [780/781]  eta: 0:00:00  lr: 0.000003  training_loss: 0.3072 (0.3080)  mae_loss: 0.2065 (0.2040)  classification_loss: 0.1017 (0.1040)  time: 0.1946  data: 0.0002  max mem: 5511
[11:35:50.228989] Epoch: [94] Total time: 0:02:33 (0.1967 s / it)
[11:35:50.229505] Averaged stats: lr: 0.000003  training_loss: 0.3072 (0.3080)  mae_loss: 0.2065 (0.2040)  classification_loss: 0.1017 (0.1040)
[11:35:50.893209] Test:  [  0/157]  eta: 0:01:43  testing_loss: 0.4836 (0.4836)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.6597  data: 0.6303  max mem: 5511
[11:35:51.197609] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.3591 (0.3829)  acc1: 90.6250 (89.3466)  acc5: 100.0000 (99.5739)  time: 0.0874  data: 0.0591  max mem: 5511
[11:35:51.494129] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.3481 (0.3693)  acc1: 90.6250 (89.7321)  acc5: 100.0000 (99.6280)  time: 0.0299  data: 0.0013  max mem: 5511
[11:35:51.785619] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.3225 (0.3739)  acc1: 90.6250 (89.6169)  acc5: 100.0000 (99.4960)  time: 0.0292  data: 0.0004  max mem: 5511
[11:35:52.072222] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.3899 (0.3873)  acc1: 89.0625 (89.1768)  acc5: 100.0000 (99.5046)  time: 0.0287  data: 0.0002  max mem: 5511
[11:35:52.356408] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.3686 (0.3753)  acc1: 89.0625 (89.5527)  acc5: 100.0000 (99.4792)  time: 0.0284  data: 0.0002  max mem: 5511
[11:35:52.646102] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.2969 (0.3724)  acc1: 90.6250 (89.6516)  acc5: 100.0000 (99.4877)  time: 0.0285  data: 0.0002  max mem: 5511
[11:35:52.929940] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.3355 (0.3669)  acc1: 90.6250 (89.7887)  acc5: 100.0000 (99.5379)  time: 0.0285  data: 0.0002  max mem: 5511
[11:35:53.214727] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.3657 (0.3745)  acc1: 89.0625 (89.6026)  acc5: 100.0000 (99.5563)  time: 0.0282  data: 0.0002  max mem: 5511
[11:35:53.499888] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.3932 (0.3725)  acc1: 89.0625 (89.6806)  acc5: 100.0000 (99.5707)  time: 0.0283  data: 0.0002  max mem: 5511
[11:35:53.792122] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4116 (0.3768)  acc1: 90.6250 (89.5730)  acc5: 100.0000 (99.5823)  time: 0.0287  data: 0.0003  max mem: 5511
[11:35:54.076406] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.3939 (0.3757)  acc1: 89.0625 (89.6537)  acc5: 100.0000 (99.5777)  time: 0.0287  data: 0.0002  max mem: 5511
[11:35:54.366211] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.3534 (0.3742)  acc1: 90.6250 (89.7340)  acc5: 100.0000 (99.5868)  time: 0.0286  data: 0.0002  max mem: 5511
[11:35:54.656151] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.3661 (0.3754)  acc1: 89.0625 (89.6350)  acc5: 100.0000 (99.5587)  time: 0.0289  data: 0.0002  max mem: 5511
[11:35:54.938343] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3147 (0.3716)  acc1: 89.0625 (89.7274)  acc5: 100.0000 (99.5900)  time: 0.0285  data: 0.0003  max mem: 5511
[11:35:55.217261] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.3147 (0.3712)  acc1: 92.1875 (89.7765)  acc5: 100.0000 (99.5964)  time: 0.0279  data: 0.0002  max mem: 5511
[11:35:55.369236] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.3186 (0.3703)  acc1: 92.1875 (89.7500)  acc5: 100.0000 (99.6000)  time: 0.0270  data: 0.0001  max mem: 5511
[11:35:55.531728] Test: Total time: 0:00:05 (0.0338 s / it)
[11:35:55.532167] * Acc@1 89.750 Acc@5 99.600 loss 0.370
[11:35:55.532461] Accuracy of the network on the 10000 test images: 89.8%
[11:35:55.532634] Max accuracy: 89.75%
[11:35:55.798742] log_dir: ./output_dir
[11:35:56.749080] Epoch: [95]  [  0/781]  eta: 0:12:20  lr: 0.000003  training_loss: 0.2789 (0.2789)  mae_loss: 0.1753 (0.1753)  classification_loss: 0.1036 (0.1036)  time: 0.9485  data: 0.7301  max mem: 5511
[11:36:00.691926] Epoch: [95]  [ 20/781]  eta: 0:02:57  lr: 0.000003  training_loss: 0.2887 (0.2988)  mae_loss: 0.1886 (0.1950)  classification_loss: 0.1000 (0.1038)  time: 0.1970  data: 0.0003  max mem: 5511
[11:36:04.594809] Epoch: [95]  [ 40/781]  eta: 0:02:38  lr: 0.000003  training_loss: 0.2976 (0.3017)  mae_loss: 0.1951 (0.1975)  classification_loss: 0.1015 (0.1042)  time: 0.1950  data: 0.0002  max mem: 5511
[11:36:08.486709] Epoch: [95]  [ 60/781]  eta: 0:02:29  lr: 0.000003  training_loss: 0.3065 (0.3055)  mae_loss: 0.2108 (0.2015)  classification_loss: 0.1027 (0.1040)  time: 0.1945  data: 0.0003  max mem: 5511
[11:36:12.392842] Epoch: [95]  [ 80/781]  eta: 0:02:23  lr: 0.000003  training_loss: 0.2968 (0.3048)  mae_loss: 0.1952 (0.2007)  classification_loss: 0.1025 (0.1042)  time: 0.1952  data: 0.0003  max mem: 5511
[11:36:16.297140] Epoch: [95]  [100/781]  eta: 0:02:18  lr: 0.000003  training_loss: 0.3190 (0.3087)  mae_loss: 0.2145 (0.2047)  classification_loss: 0.1030 (0.1040)  time: 0.1951  data: 0.0003  max mem: 5511
[11:36:20.199369] Epoch: [95]  [120/781]  eta: 0:02:13  lr: 0.000003  training_loss: 0.3035 (0.3086)  mae_loss: 0.2057 (0.2046)  classification_loss: 0.1022 (0.1039)  time: 0.1950  data: 0.0003  max mem: 5511
[11:36:24.145778] Epoch: [95]  [140/781]  eta: 0:02:08  lr: 0.000003  training_loss: 0.3089 (0.3092)  mae_loss: 0.2006 (0.2047)  classification_loss: 0.1073 (0.1044)  time: 0.1972  data: 0.0002  max mem: 5511
[11:36:28.056997] Epoch: [95]  [160/781]  eta: 0:02:04  lr: 0.000003  training_loss: 0.3062 (0.3093)  mae_loss: 0.2061 (0.2049)  classification_loss: 0.1031 (0.1045)  time: 0.1955  data: 0.0003  max mem: 5511
[11:36:32.026913] Epoch: [95]  [180/781]  eta: 0:02:00  lr: 0.000003  training_loss: 0.3092 (0.3094)  mae_loss: 0.2058 (0.2051)  classification_loss: 0.1030 (0.1043)  time: 0.1984  data: 0.0002  max mem: 5511
[11:36:35.957068] Epoch: [95]  [200/781]  eta: 0:01:56  lr: 0.000003  training_loss: 0.3057 (0.3095)  mae_loss: 0.1996 (0.2051)  classification_loss: 0.1066 (0.1044)  time: 0.1964  data: 0.0002  max mem: 5511
[11:36:39.866006] Epoch: [95]  [220/781]  eta: 0:01:51  lr: 0.000003  training_loss: 0.2948 (0.3083)  mae_loss: 0.1942 (0.2039)  classification_loss: 0.1000 (0.1043)  time: 0.1954  data: 0.0002  max mem: 5511
[11:36:43.790635] Epoch: [95]  [240/781]  eta: 0:01:47  lr: 0.000002  training_loss: 0.2984 (0.3084)  mae_loss: 0.1948 (0.2038)  classification_loss: 0.1066 (0.1046)  time: 0.1961  data: 0.0002  max mem: 5511
[11:36:47.708429] Epoch: [95]  [260/781]  eta: 0:01:43  lr: 0.000002  training_loss: 0.3056 (0.3082)  mae_loss: 0.1952 (0.2037)  classification_loss: 0.1056 (0.1045)  time: 0.1958  data: 0.0002  max mem: 5511
[11:36:51.605168] Epoch: [95]  [280/781]  eta: 0:01:39  lr: 0.000002  training_loss: 0.2949 (0.3084)  mae_loss: 0.1899 (0.2038)  classification_loss: 0.1051 (0.1046)  time: 0.1947  data: 0.0002  max mem: 5511
[11:36:55.516498] Epoch: [95]  [300/781]  eta: 0:01:35  lr: 0.000002  training_loss: 0.3091 (0.3085)  mae_loss: 0.2032 (0.2038)  classification_loss: 0.1057 (0.1047)  time: 0.1955  data: 0.0002  max mem: 5511
[11:36:59.420014] Epoch: [95]  [320/781]  eta: 0:01:31  lr: 0.000002  training_loss: 0.3057 (0.3085)  mae_loss: 0.1995 (0.2037)  classification_loss: 0.1072 (0.1047)  time: 0.1951  data: 0.0002  max mem: 5511
[11:37:03.328329] Epoch: [95]  [340/781]  eta: 0:01:27  lr: 0.000002  training_loss: 0.2985 (0.3084)  mae_loss: 0.2075 (0.2038)  classification_loss: 0.1036 (0.1046)  time: 0.1953  data: 0.0002  max mem: 5511
[11:37:07.238876] Epoch: [95]  [360/781]  eta: 0:01:23  lr: 0.000002  training_loss: 0.3116 (0.3087)  mae_loss: 0.2087 (0.2040)  classification_loss: 0.1055 (0.1047)  time: 0.1954  data: 0.0003  max mem: 5511
[11:37:11.161167] Epoch: [95]  [380/781]  eta: 0:01:19  lr: 0.000002  training_loss: 0.2978 (0.3084)  mae_loss: 0.1958 (0.2036)  classification_loss: 0.1035 (0.1048)  time: 0.1960  data: 0.0002  max mem: 5511

[11:37:15.128917] Epoch: [95]  [400/781]  eta: 0:01:15  lr: 0.000002  training_loss: 0.2976 (0.3083)  mae_loss: 0.2028 (0.2037)  classification_loss: 0.1016 (0.1046)  time: 0.1983  data: 0.0003  max mem: 5511
[11:37:19.041459] Epoch: [95]  [420/781]  eta: 0:01:11  lr: 0.000002  training_loss: 0.3265 (0.3088)  mae_loss: 0.2143 (0.2043)  classification_loss: 0.1005 (0.1046)  time: 0.1955  data: 0.0002  max mem: 5511
[11:37:22.943814] Epoch: [95]  [440/781]  eta: 0:01:07  lr: 0.000002  training_loss: 0.3013 (0.3085)  mae_loss: 0.1970 (0.2041)  classification_loss: 0.0998 (0.1044)  time: 0.1950  data: 0.0002  max mem: 5511
[11:37:26.945525] Epoch: [95]  [460/781]  eta: 0:01:03  lr: 0.000002  training_loss: 0.2918 (0.3083)  mae_loss: 0.1916 (0.2039)  classification_loss: 0.1075 (0.1044)  time: 0.2000  data: 0.0002  max mem: 5511
[11:37:30.844549] Epoch: [95]  [480/781]  eta: 0:00:59  lr: 0.000002  training_loss: 0.3167 (0.3087)  mae_loss: 0.2149 (0.2043)  classification_loss: 0.1052 (0.1044)  time: 0.1949  data: 0.0002  max mem: 5511
[11:37:34.735374] Epoch: [95]  [500/781]  eta: 0:00:55  lr: 0.000002  training_loss: 0.3140 (0.3089)  mae_loss: 0.2049 (0.2045)  classification_loss: 0.1023 (0.1043)  time: 0.1945  data: 0.0002  max mem: 5511
[11:37:38.627250] Epoch: [95]  [520/781]  eta: 0:00:51  lr: 0.000002  training_loss: 0.3027 (0.3086)  mae_loss: 0.2000 (0.2042)  classification_loss: 0.1027 (0.1044)  time: 0.1945  data: 0.0002  max mem: 5511
[11:37:42.539448] Epoch: [95]  [540/781]  eta: 0:00:47  lr: 0.000002  training_loss: 0.3131 (0.3088)  mae_loss: 0.2067 (0.2045)  classification_loss: 0.1072 (0.1044)  time: 0.1955  data: 0.0003  max mem: 5511
[11:37:46.446688] Epoch: [95]  [560/781]  eta: 0:00:43  lr: 0.000002  training_loss: 0.3051 (0.3088)  mae_loss: 0.2026 (0.2044)  classification_loss: 0.1035 (0.1044)  time: 0.1953  data: 0.0002  max mem: 5511
[11:37:50.373925] Epoch: [95]  [580/781]  eta: 0:00:39  lr: 0.000002  training_loss: 0.3045 (0.3089)  mae_loss: 0.2043 (0.2044)  classification_loss: 0.1059 (0.1044)  time: 0.1962  data: 0.0002  max mem: 5511
[11:37:54.298612] Epoch: [95]  [600/781]  eta: 0:00:35  lr: 0.000002  training_loss: 0.2974 (0.3087)  mae_loss: 0.2004 (0.2044)  classification_loss: 0.1000 (0.1043)  time: 0.1962  data: 0.0003  max mem: 5511
[11:37:58.219102] Epoch: [95]  [620/781]  eta: 0:00:31  lr: 0.000002  training_loss: 0.2997 (0.3088)  mae_loss: 0.1991 (0.2045)  classification_loss: 0.0997 (0.1042)  time: 0.1960  data: 0.0002  max mem: 5511
[11:38:02.119709] Epoch: [95]  [640/781]  eta: 0:00:27  lr: 0.000002  training_loss: 0.3053 (0.3088)  mae_loss: 0.2060 (0.2046)  classification_loss: 0.1038 (0.1042)  time: 0.1949  data: 0.0002  max mem: 5511
[11:38:06.044103] Epoch: [95]  [660/781]  eta: 0:00:23  lr: 0.000002  training_loss: 0.3003 (0.3089)  mae_loss: 0.2024 (0.2047)  classification_loss: 0.1026 (0.1042)  time: 0.1961  data: 0.0003  max mem: 5511
[11:38:10.026096] Epoch: [95]  [680/781]  eta: 0:00:19  lr: 0.000002  training_loss: 0.3129 (0.3090)  mae_loss: 0.2039 (0.2047)  classification_loss: 0.1036 (0.1043)  time: 0.1989  data: 0.0003  max mem: 5511
[11:38:13.929688] Epoch: [95]  [700/781]  eta: 0:00:15  lr: 0.000002  training_loss: 0.3053 (0.3090)  mae_loss: 0.2041 (0.2047)  classification_loss: 0.1033 (0.1043)  time: 0.1951  data: 0.0003  max mem: 5511
[11:38:17.826584] Epoch: [95]  [720/781]  eta: 0:00:12  lr: 0.000002  training_loss: 0.3010 (0.3089)  mae_loss: 0.1958 (0.2046)  classification_loss: 0.1003 (0.1042)  time: 0.1947  data: 0.0002  max mem: 5511
[11:38:21.747086] Epoch: [95]  [740/781]  eta: 0:00:08  lr: 0.000002  training_loss: 0.3075 (0.3088)  mae_loss: 0.2001 (0.2045)  classification_loss: 0.1032 (0.1043)  time: 0.1959  data: 0.0002  max mem: 5511
[11:38:25.666762] Epoch: [95]  [760/781]  eta: 0:00:04  lr: 0.000002  training_loss: 0.3012 (0.3085)  mae_loss: 0.1937 (0.2043)  classification_loss: 0.1033 (0.1043)  time: 0.1959  data: 0.0002  max mem: 5511
[11:38:29.568632] Epoch: [95]  [780/781]  eta: 0:00:00  lr: 0.000002  training_loss: 0.3127 (0.3086)  mae_loss: 0.2028 (0.2042)  classification_loss: 0.1064 (0.1044)  time: 0.1950  data: 0.0002  max mem: 5511
[11:38:29.719684] Epoch: [95] Total time: 0:02:33 (0.1971 s / it)
[11:38:29.720145] Averaged stats: lr: 0.000002  training_loss: 0.3127 (0.3086)  mae_loss: 0.2028 (0.2042)  classification_loss: 0.1064 (0.1044)
[11:38:30.300566] Test:  [  0/157]  eta: 0:01:30  testing_loss: 0.4773 (0.4773)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.5749  data: 0.5420  max mem: 5511
[11:38:30.584629] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.3743 (0.3830)  acc1: 89.0625 (88.9205)  acc5: 100.0000 (99.7159)  time: 0.0779  data: 0.0495  max mem: 5511
[11:38:30.865922] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.3381 (0.3688)  acc1: 90.6250 (89.6577)  acc5: 100.0000 (99.7024)  time: 0.0281  data: 0.0002  max mem: 5511
[11:38:31.147882] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.3212 (0.3724)  acc1: 90.6250 (89.6673)  acc5: 100.0000 (99.5464)  time: 0.0280  data: 0.0002  max mem: 5511
[11:38:31.428636] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.3901 (0.3865)  acc1: 89.0625 (89.1768)  acc5: 100.0000 (99.5046)  time: 0.0280  data: 0.0002  max mem: 5511
[11:38:31.710966] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.3723 (0.3742)  acc1: 89.0625 (89.4914)  acc5: 100.0000 (99.4792)  time: 0.0280  data: 0.0002  max mem: 5511
[11:38:31.991940] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.2926 (0.3714)  acc1: 90.6250 (89.6004)  acc5: 100.0000 (99.4877)  time: 0.0280  data: 0.0002  max mem: 5511
[11:38:32.273795] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.3304 (0.3656)  acc1: 90.6250 (89.7667)  acc5: 100.0000 (99.5599)  time: 0.0280  data: 0.0002  max mem: 5511
[11:38:32.558447] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.3703 (0.3730)  acc1: 89.0625 (89.5640)  acc5: 100.0000 (99.5563)  time: 0.0282  data: 0.0002  max mem: 5511
[11:38:32.840915] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4010 (0.3710)  acc1: 89.0625 (89.6635)  acc5: 100.0000 (99.5707)  time: 0.0282  data: 0.0002  max mem: 5511
[11:38:33.123012] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4099 (0.3754)  acc1: 89.0625 (89.5111)  acc5: 100.0000 (99.5978)  time: 0.0281  data: 0.0002  max mem: 5511
[11:38:33.405185] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.3938 (0.3747)  acc1: 89.0625 (89.5974)  acc5: 100.0000 (99.5918)  time: 0.0281  data: 0.0002  max mem: 5511
[11:38:33.686596] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.3552 (0.3733)  acc1: 90.6250 (89.6565)  acc5: 100.0000 (99.5997)  time: 0.0280  data: 0.0002  max mem: 5511
[11:38:33.968101] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.3552 (0.3746)  acc1: 90.6250 (89.5277)  acc5: 100.0000 (99.5706)  time: 0.0280  data: 0.0002  max mem: 5511
[11:38:34.248853] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3223 (0.3710)  acc1: 90.6250 (89.5944)  acc5: 100.0000 (99.6011)  time: 0.0280  data: 0.0002  max mem: 5511
[11:38:34.526930] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.3174 (0.3706)  acc1: 90.6250 (89.6213)  acc5: 100.0000 (99.6068)  time: 0.0278  data: 0.0001  max mem: 5511
[11:38:34.677129] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.3223 (0.3697)  acc1: 90.6250 (89.6000)  acc5: 100.0000 (99.5900)  time: 0.0269  data: 0.0001  max mem: 5511
[11:38:34.818196] Test: Total time: 0:00:05 (0.0325 s / it)
[11:38:34.818660] * Acc@1 89.600 Acc@5 99.590 loss 0.370
[11:38:34.818979] Accuracy of the network on the 10000 test images: 89.6%
[11:38:34.819149] Max accuracy: 89.75%
[11:38:35.240692] log_dir: ./output_dir
[11:38:36.045552] Epoch: [96]  [  0/781]  eta: 0:10:27  lr: 0.000002  training_loss: 0.3039 (0.3039)  mae_loss: 0.2131 (0.2131)  classification_loss: 0.0908 (0.0908)  time: 0.8033  data: 0.5732  max mem: 5511
[11:38:39.935240] Epoch: [96]  [ 20/781]  eta: 0:02:50  lr: 0.000002  training_loss: 0.2883 (0.3023)  mae_loss: 0.1912 (0.2000)  classification_loss: 0.1029 (0.1023)  time: 0.1944  data: 0.0002  max mem: 5511
[11:38:43.858338] Epoch: [96]  [ 40/781]  eta: 0:02:35  lr: 0.000002  training_loss: 0.3028 (0.3059)  mae_loss: 0.2021 (0.2029)  classification_loss: 0.1036 (0.1030)  time: 0.1960  data: 0.0002  max mem: 5511
[11:38:47.825766] Epoch: [96]  [ 60/781]  eta: 0:02:28  lr: 0.000002  training_loss: 0.3009 (0.3065)  mae_loss: 0.1930 (0.2021)  classification_loss: 0.1057 (0.1044)  time: 0.1983  data: 0.0003  max mem: 5511
[11:38:51.764089] Epoch: [96]  [ 80/781]  eta: 0:02:22  lr: 0.000002  training_loss: 0.3043 (0.3072)  mae_loss: 0.1987 (0.2027)  classification_loss: 0.1034 (0.1045)  time: 0.1968  data: 0.0007  max mem: 5511
[11:38:55.680803] Epoch: [96]  [100/781]  eta: 0:02:17  lr: 0.000002  training_loss: 0.3071 (0.3073)  mae_loss: 0.2122 (0.2034)  classification_loss: 0.1039 (0.1039)  time: 0.1958  data: 0.0002  max mem: 5511
[11:38:59.588420] Epoch: [96]  [120/781]  eta: 0:02:12  lr: 0.000002  training_loss: 0.3048 (0.3062)  mae_loss: 0.2049 (0.2025)  classification_loss: 0.1002 (0.1036)  time: 0.1953  data: 0.0002  max mem: 5511
[11:39:03.478540] Epoch: [96]  [140/781]  eta: 0:02:08  lr: 0.000002  training_loss: 0.2932 (0.3059)  mae_loss: 0.1917 (0.2026)  classification_loss: 0.0984 (0.1033)  time: 0.1944  data: 0.0002  max mem: 5511
[11:39:07.397839] Epoch: [96]  [160/781]  eta: 0:02:03  lr: 0.000002  training_loss: 0.3087 (0.3068)  mae_loss: 0.2019 (0.2031)  classification_loss: 0.1056 (0.1037)  time: 0.1959  data: 0.0003  max mem: 5511
[11:39:11.334727] Epoch: [96]  [180/781]  eta: 0:01:59  lr: 0.000002  training_loss: 0.3049 (0.3067)  mae_loss: 0.1946 (0.2031)  classification_loss: 0.1049 (0.1036)  time: 0.1968  data: 0.0002  max mem: 5511
[11:39:15.262122] Epoch: [96]  [200/781]  eta: 0:01:55  lr: 0.000002  training_loss: 0.3146 (0.3077)  mae_loss: 0.2075 (0.2040)  classification_loss: 0.1052 (0.1038)  time: 0.1963  data: 0.0002  max mem: 5511
[11:39:19.159802] Epoch: [96]  [220/781]  eta: 0:01:51  lr: 0.000002  training_loss: 0.3012 (0.3075)  mae_loss: 0.1977 (0.2036)  classification_loss: 0.1040 (0.1040)  time: 0.1948  data: 0.0002  max mem: 5511
[11:39:23.063426] Epoch: [96]  [240/781]  eta: 0:01:47  lr: 0.000002  training_loss: 0.3089 (0.3081)  mae_loss: 0.2049 (0.2039)  classification_loss: 0.1066 (0.1042)  time: 0.1951  data: 0.0004  max mem: 5511
[11:39:26.966203] Epoch: [96]  [260/781]  eta: 0:01:43  lr: 0.000002  training_loss: 0.3079 (0.3084)  mae_loss: 0.2009 (0.2042)  classification_loss: 0.1054 (0.1042)  time: 0.1950  data: 0.0002  max mem: 5511
[11:39:30.859994] Epoch: [96]  [280/781]  eta: 0:01:39  lr: 0.000002  training_loss: 0.3044 (0.3086)  mae_loss: 0.2019 (0.2043)  classification_loss: 0.1060 (0.1043)  time: 0.1946  data: 0.0002  max mem: 5511
[11:39:34.770429] Epoch: [96]  [300/781]  eta: 0:01:35  lr: 0.000002  training_loss: 0.3047 (0.3083)  mae_loss: 0.1940 (0.2038)  classification_loss: 0.1055 (0.1045)  time: 0.1954  data: 0.0002  max mem: 5511
[11:39:38.688882] Epoch: [96]  [320/781]  eta: 0:01:31  lr: 0.000002  training_loss: 0.3030 (0.3086)  mae_loss: 0.1999 (0.2040)  classification_loss: 0.1067 (0.1046)  time: 0.1958  data: 0.0003  max mem: 5511
[11:39:42.609368] Epoch: [96]  [340/781]  eta: 0:01:27  lr: 0.000002  training_loss: 0.2987 (0.3086)  mae_loss: 0.1930 (0.2041)  classification_loss: 0.0993 (0.1045)  time: 0.1959  data: 0.0002  max mem: 5511
[11:39:46.529388] Epoch: [96]  [360/781]  eta: 0:01:23  lr: 0.000002  training_loss: 0.3094 (0.3089)  mae_loss: 0.2090 (0.2045)  classification_loss: 0.1028 (0.1044)  time: 0.1959  data: 0.0003  max mem: 5511
[11:39:50.441816] Epoch: [96]  [380/781]  eta: 0:01:19  lr: 0.000002  training_loss: 0.3178 (0.3092)  mae_loss: 0.2100 (0.2048)  classification_loss: 0.1051 (0.1045)  time: 0.1955  data: 0.0002  max mem: 5511
[11:39:54.363788] Epoch: [96]  [400/781]  eta: 0:01:15  lr: 0.000002  training_loss: 0.3107 (0.3095)  mae_loss: 0.2099 (0.2051)  classification_loss: 0.1042 (0.1044)  time: 0.1960  data: 0.0002  max mem: 5511
[11:39:58.257312] Epoch: [96]  [420/781]  eta: 0:01:11  lr: 0.000002  training_loss: 0.3021 (0.3092)  mae_loss: 0.1926 (0.2048)  classification_loss: 0.1015 (0.1043)  time: 0.1946  data: 0.0002  max mem: 5511
[11:40:02.157226] Epoch: [96]  [440/781]  eta: 0:01:07  lr: 0.000002  training_loss: 0.3022 (0.3090)  mae_loss: 0.1956 (0.2047)  classification_loss: 0.1008 (0.1043)  time: 0.1949  data: 0.0002  max mem: 5511
[11:40:06.092954] Epoch: [96]  [460/781]  eta: 0:01:03  lr: 0.000002  training_loss: 0.3093 (0.3089)  mae_loss: 0.1984 (0.2046)  classification_loss: 0.1035 (0.1043)  time: 0.1967  data: 0.0002  max mem: 5511
[11:40:10.025317] Epoch: [96]  [480/781]  eta: 0:00:59  lr: 0.000002  training_loss: 0.3035 (0.3088)  mae_loss: 0.2036 (0.2046)  classification_loss: 0.1032 (0.1042)  time: 0.1965  data: 0.0002  max mem: 5511
[11:40:13.916568] Epoch: [96]  [500/781]  eta: 0:00:55  lr: 0.000002  training_loss: 0.3060 (0.3089)  mae_loss: 0.2014 (0.2047)  classification_loss: 0.1031 (0.1042)  time: 0.1945  data: 0.0002  max mem: 5511
[11:40:17.821509] Epoch: [96]  [520/781]  eta: 0:00:51  lr: 0.000002  training_loss: 0.3113 (0.3089)  mae_loss: 0.2006 (0.2046)  classification_loss: 0.1028 (0.1043)  time: 0.1952  data: 0.0003  max mem: 5511
[11:40:21.720478] Epoch: [96]  [540/781]  eta: 0:00:47  lr: 0.000002  training_loss: 0.3008 (0.3087)  mae_loss: 0.1968 (0.2044)  classification_loss: 0.1060 (0.1043)  time: 0.1948  data: 0.0002  max mem: 5511
[11:40:25.618090] Epoch: [96]  [560/781]  eta: 0:00:43  lr: 0.000002  training_loss: 0.2983 (0.3084)  mae_loss: 0.1983 (0.2042)  classification_loss: 0.1025 (0.1042)  time: 0.1948  data: 0.0002  max mem: 5511
[11:40:29.509448] Epoch: [96]  [580/781]  eta: 0:00:39  lr: 0.000002  training_loss: 0.3049 (0.3082)  mae_loss: 0.1916 (0.2040)  classification_loss: 0.1004 (0.1042)  time: 0.1945  data: 0.0002  max mem: 5511
[11:40:33.487658] Epoch: [96]  [600/781]  eta: 0:00:35  lr: 0.000002  training_loss: 0.2949 (0.3081)  mae_loss: 0.2001 (0.2040)  classification_loss: 0.0962 (0.1040)  time: 0.1988  data: 0.0002  max mem: 5511
[11:40:37.387357] Epoch: [96]  [620/781]  eta: 0:00:31  lr: 0.000002  training_loss: 0.3013 (0.3080)  mae_loss: 0.2011 (0.2040)  classification_loss: 0.1027 (0.1040)  time: 0.1949  data: 0.0003  max mem: 5511
[11:40:41.319324] Epoch: [96]  [640/781]  eta: 0:00:27  lr: 0.000002  training_loss: 0.2987 (0.3080)  mae_loss: 0.1943 (0.2039)  classification_loss: 0.1050 (0.1041)  time: 0.1965  data: 0.0002  max mem: 5511
[11:40:45.255981] Epoch: [96]  [660/781]  eta: 0:00:23  lr: 0.000002  training_loss: 0.2961 (0.3080)  mae_loss: 0.1964 (0.2039)  classification_loss: 0.1072 (0.1041)  time: 0.1967  data: 0.0002  max mem: 5511
[11:40:49.149421] Epoch: [96]  [680/781]  eta: 0:00:19  lr: 0.000002  training_loss: 0.3019 (0.3080)  mae_loss: 0.1938 (0.2038)  classification_loss: 0.1060 (0.1042)  time: 0.1946  data: 0.0003  max mem: 5511
[11:40:53.059249] Epoch: [96]  [700/781]  eta: 0:00:15  lr: 0.000002  training_loss: 0.3045 (0.3080)  mae_loss: 0.2000 (0.2038)  classification_loss: 0.1031 (0.1042)  time: 0.1954  data: 0.0003  max mem: 5511
[11:40:56.958299] Epoch: [96]  [720/781]  eta: 0:00:11  lr: 0.000002  training_loss: 0.3089 (0.3080)  mae_loss: 0.2046 (0.2037)  classification_loss: 0.1060 (0.1043)  time: 0.1949  data: 0.0002  max mem: 5511
[11:41:00.887513] Epoch: [96]  [740/781]  eta: 0:00:08  lr: 0.000002  training_loss: 0.3066 (0.3080)  mae_loss: 0.2000 (0.2037)  classification_loss: 0.1049 (0.1043)  time: 0.1964  data: 0.0002  max mem: 5511
[11:41:04.827743] Epoch: [96]  [760/781]  eta: 0:00:04  lr: 0.000002  training_loss: 0.3157 (0.3081)  mae_loss: 0.2058 (0.2037)  classification_loss: 0.1109 (0.1044)  time: 0.1969  data: 0.0002  max mem: 5511
[11:41:08.713370] Epoch: [96]  [780/781]  eta: 0:00:00  lr: 0.000002  training_loss: 0.3023 (0.3080)  mae_loss: 0.1986 (0.2036)  classification_loss: 0.1038 (0.1044)  time: 0.1942  data: 0.0002  max mem: 5511
[11:41:08.867498] Epoch: [96] Total time: 0:02:33 (0.1967 s / it)
[11:41:08.868726] Averaged stats: lr: 0.000002  training_loss: 0.3023 (0.3080)  mae_loss: 0.1986 (0.2036)  classification_loss: 0.1038 (0.1044)
[11:41:09.589865] Test:  [  0/157]  eta: 0:01:52  testing_loss: 0.4718 (0.4718)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.7166  data: 0.6805  max mem: 5511
[11:41:09.872938] Test:  [ 10/157]  eta: 0:00:13  testing_loss: 0.3740 (0.3839)  acc1: 89.0625 (88.9205)  acc5: 100.0000 (99.7159)  time: 0.0907  data: 0.0621  max mem: 5511
[11:41:10.153990] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.3437 (0.3704)  acc1: 90.6250 (89.7321)  acc5: 100.0000 (99.7024)  time: 0.0281  data: 0.0002  max mem: 5511
[11:41:10.435938] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.3282 (0.3742)  acc1: 90.6250 (89.6169)  acc5: 100.0000 (99.5464)  time: 0.0280  data: 0.0002  max mem: 5511
[11:41:10.720067] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.3925 (0.3880)  acc1: 89.0625 (89.1387)  acc5: 100.0000 (99.4665)  time: 0.0282  data: 0.0002  max mem: 5511
[11:41:11.008368] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.3710 (0.3754)  acc1: 89.0625 (89.5833)  acc5: 100.0000 (99.4485)  time: 0.0285  data: 0.0003  max mem: 5511
[11:41:11.291172] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.2990 (0.3727)  acc1: 90.6250 (89.6773)  acc5: 100.0000 (99.4621)  time: 0.0284  data: 0.0003  max mem: 5511
[11:41:11.583618] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.3279 (0.3670)  acc1: 89.0625 (89.7667)  acc5: 100.0000 (99.5379)  time: 0.0286  data: 0.0001  max mem: 5511
[11:41:11.868465] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.3604 (0.3744)  acc1: 89.0625 (89.6026)  acc5: 100.0000 (99.5370)  time: 0.0287  data: 0.0002  max mem: 5511
[11:41:12.149971] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4070 (0.3724)  acc1: 89.0625 (89.7321)  acc5: 100.0000 (99.5536)  time: 0.0282  data: 0.0002  max mem: 5511
[11:41:12.430851] Test:  [100/157]  eta: 0:00:02  testing_loss: 0.4070 (0.3766)  acc1: 90.6250 (89.6040)  acc5: 100.0000 (99.5668)  time: 0.0280  data: 0.0002  max mem: 5511
[11:41:12.717810] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.3935 (0.3757)  acc1: 89.0625 (89.6819)  acc5: 100.0000 (99.5495)  time: 0.0283  data: 0.0002  max mem: 5511
[11:41:12.999275] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.3592 (0.3743)  acc1: 89.0625 (89.7211)  acc5: 100.0000 (99.5610)  time: 0.0283  data: 0.0002  max mem: 5511
[11:41:13.281758] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.3601 (0.3756)  acc1: 89.0625 (89.5992)  acc5: 100.0000 (99.5348)  time: 0.0281  data: 0.0002  max mem: 5511
[11:41:13.563810] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3321 (0.3722)  acc1: 89.0625 (89.6387)  acc5: 100.0000 (99.5678)  time: 0.0281  data: 0.0002  max mem: 5511
[11:41:13.843030] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.3191 (0.3719)  acc1: 90.6250 (89.6834)  acc5: 100.0000 (99.5757)  time: 0.0279  data: 0.0001  max mem: 5511
[11:41:13.993454] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.3191 (0.3709)  acc1: 90.6250 (89.6500)  acc5: 100.0000 (99.5700)  time: 0.0270  data: 0.0001  max mem: 5511
[11:41:14.159052] Test: Total time: 0:00:05 (0.0337 s / it)
[11:41:14.159545] * Acc@1 89.650 Acc@5 99.570 loss 0.371
[11:41:14.159896] Accuracy of the network on the 10000 test images: 89.7%
[11:41:14.160145] Max accuracy: 89.75%
[11:41:14.378051] log_dir: ./output_dir
[11:41:15.162430] Epoch: [97]  [  0/781]  eta: 0:10:11  lr: 0.000002  training_loss: 0.2634 (0.2634)  mae_loss: 0.1807 (0.1807)  classification_loss: 0.0827 (0.0827)  time: 0.7826  data: 0.5609  max mem: 5511
[11:41:19.082222] Epoch: [97]  [ 20/781]  eta: 0:02:50  lr: 0.000002  training_loss: 0.3051 (0.3063)  mae_loss: 0.2019 (0.2037)  classification_loss: 0.1057 (0.1026)  time: 0.1959  data: 0.0002  max mem: 5511
[11:41:22.998989] Epoch: [97]  [ 40/781]  eta: 0:02:35  lr: 0.000002  training_loss: 0.3041 (0.3048)  mae_loss: 0.1908 (0.2001)  classification_loss: 0.1073 (0.1048)  time: 0.1958  data: 0.0003  max mem: 5511
[11:41:26.886415] Epoch: [97]  [ 60/781]  eta: 0:02:27  lr: 0.000002  training_loss: 0.3161 (0.3095)  mae_loss: 0.2151 (0.2047)  classification_loss: 0.1042 (0.1048)  time: 0.1943  data: 0.0002  max mem: 5511
[11:41:30.799989] Epoch: [97]  [ 80/781]  eta: 0:02:22  lr: 0.000002  training_loss: 0.3012 (0.3077)  mae_loss: 0.2002 (0.2033)  classification_loss: 0.1005 (0.1044)  time: 0.1956  data: 0.0002  max mem: 5511
[11:41:34.712566] Epoch: [97]  [100/781]  eta: 0:02:17  lr: 0.000002  training_loss: 0.2919 (0.3065)  mae_loss: 0.1942 (0.2026)  classification_loss: 0.0983 (0.1038)  time: 0.1955  data: 0.0002  max mem: 5511
[11:41:38.624049] Epoch: [97]  [120/781]  eta: 0:02:12  lr: 0.000002  training_loss: 0.3072 (0.3068)  mae_loss: 0.1983 (0.2030)  classification_loss: 0.1009 (0.1038)  time: 0.1955  data: 0.0003  max mem: 5511
[11:41:42.560840] Epoch: [97]  [140/781]  eta: 0:02:08  lr: 0.000002  training_loss: 0.2956 (0.3051)  mae_loss: 0.1912 (0.2017)  classification_loss: 0.0996 (0.1034)  time: 0.1968  data: 0.0002  max mem: 5511
[11:41:46.520872] Epoch: [97]  [160/781]  eta: 0:02:03  lr: 0.000002  training_loss: 0.3034 (0.3059)  mae_loss: 0.2069 (0.2026)  classification_loss: 0.1023 (0.1033)  time: 0.1979  data: 0.0002  max mem: 5511
[11:41:50.452491] Epoch: [97]  [180/781]  eta: 0:01:59  lr: 0.000002  training_loss: 0.3023 (0.3069)  mae_loss: 0.2039 (0.2035)  classification_loss: 0.1033 (0.1034)  time: 0.1965  data: 0.0001  max mem: 5511
[11:41:54.380400] Epoch: [97]  [200/781]  eta: 0:01:55  lr: 0.000002  training_loss: 0.2948 (0.3059)  mae_loss: 0.1959 (0.2026)  classification_loss: 0.1034 (0.1033)  time: 0.1963  data: 0.0003  max mem: 5511
[11:41:58.316965] Epoch: [97]  [220/781]  eta: 0:01:51  lr: 0.000002  training_loss: 0.3196 (0.3071)  mae_loss: 0.2115 (0.2035)  classification_loss: 0.1066 (0.1036)  time: 0.1968  data: 0.0002  max mem: 5511
[11:42:02.210638] Epoch: [97]  [240/781]  eta: 0:01:47  lr: 0.000001  training_loss: 0.3068 (0.3072)  mae_loss: 0.2018 (0.2037)  classification_loss: 0.0998 (0.1036)  time: 0.1946  data: 0.0002  max mem: 5511
[11:42:06.121815] Epoch: [97]  [260/781]  eta: 0:01:43  lr: 0.000001  training_loss: 0.3044 (0.3072)  mae_loss: 0.2042 (0.2034)  classification_loss: 0.1051 (0.1038)  time: 0.1954  data: 0.0002  max mem: 5511
[11:42:10.003036] Epoch: [97]  [280/781]  eta: 0:01:39  lr: 0.000001  training_loss: 0.3053 (0.3071)  mae_loss: 0.2009 (0.2034)  classification_loss: 0.1051 (0.1038)  time: 0.1940  data: 0.0002  max mem: 5511
[11:42:13.876783] Epoch: [97]  [300/781]  eta: 0:01:35  lr: 0.000001  training_loss: 0.3004 (0.3071)  mae_loss: 0.1994 (0.2034)  classification_loss: 0.1029 (0.1037)  time: 0.1936  data: 0.0002  max mem: 5511
[11:42:17.771672] Epoch: [97]  [320/781]  eta: 0:01:30  lr: 0.000001  training_loss: 0.3257 (0.3077)  mae_loss: 0.2175 (0.2040)  classification_loss: 0.1024 (0.1037)  time: 0.1947  data: 0.0002  max mem: 5511
[11:42:21.668013] Epoch: [97]  [340/781]  eta: 0:01:26  lr: 0.000001  training_loss: 0.3002 (0.3078)  mae_loss: 0.2012 (0.2042)  classification_loss: 0.0976 (0.1036)  time: 0.1947  data: 0.0002  max mem: 5511
[11:42:25.581024] Epoch: [97]  [360/781]  eta: 0:01:22  lr: 0.000001  training_loss: 0.2930 (0.3073)  mae_loss: 0.1917 (0.2038)  classification_loss: 0.1020 (0.1035)  time: 0.1956  data: 0.0002  max mem: 5511
[11:42:29.497353] Epoch: [97]  [380/781]  eta: 0:01:19  lr: 0.000001  training_loss: 0.2864 (0.3069)  mae_loss: 0.1807 (0.2034)  classification_loss: 0.1000 (0.1035)  time: 0.1957  data: 0.0002  max mem: 5511
[11:42:33.410528] Epoch: [97]  [400/781]  eta: 0:01:15  lr: 0.000001  training_loss: 0.2980 (0.3065)  mae_loss: 0.1929 (0.2031)  classification_loss: 0.1018 (0.1034)  time: 0.1956  data: 0.0002  max mem: 5511
[11:42:37.330009] Epoch: [97]  [420/781]  eta: 0:01:11  lr: 0.000001  training_loss: 0.3002 (0.3065)  mae_loss: 0.1953 (0.2029)  classification_loss: 0.1051 (0.1036)  time: 0.1959  data: 0.0003  max mem: 5511
[11:42:41.237597] Epoch: [97]  [440/781]  eta: 0:01:07  lr: 0.000001  training_loss: 0.3074 (0.3071)  mae_loss: 0.2043 (0.2035)  classification_loss: 0.1044 (0.1036)  time: 0.1953  data: 0.0002  max mem: 5511
[11:42:45.138174] Epoch: [97]  [460/781]  eta: 0:01:03  lr: 0.000001  training_loss: 0.2960 (0.3068)  mae_loss: 0.1908 (0.2032)  classification_loss: 0.1056 (0.1036)  time: 0.1950  data: 0.0002  max mem: 5511
[11:42:49.080480] Epoch: [97]  [480/781]  eta: 0:00:59  lr: 0.000001  training_loss: 0.2960 (0.3067)  mae_loss: 0.1987 (0.2031)  classification_loss: 0.1013 (0.1036)  time: 0.1970  data: 0.0002  max mem: 5511
[11:42:53.030412] Epoch: [97]  [500/781]  eta: 0:00:55  lr: 0.000001  training_loss: 0.2989 (0.3065)  mae_loss: 0.1958 (0.2028)  classification_loss: 0.1092 (0.1038)  time: 0.1974  data: 0.0003  max mem: 5511
[11:42:57.000425] Epoch: [97]  [520/781]  eta: 0:00:51  lr: 0.000001  training_loss: 0.3135 (0.3067)  mae_loss: 0.2087 (0.2031)  classification_loss: 0.1000 (0.1037)  time: 0.1984  data: 0.0003  max mem: 5511
[11:43:00.953596] Epoch: [97]  [540/781]  eta: 0:00:47  lr: 0.000001  training_loss: 0.3086 (0.3067)  mae_loss: 0.2000 (0.2030)  classification_loss: 0.1060 (0.1037)  time: 0.1976  data: 0.0002  max mem: 5511
[11:43:04.848809] Epoch: [97]  [560/781]  eta: 0:00:43  lr: 0.000001  training_loss: 0.3015 (0.3067)  mae_loss: 0.1954 (0.2030)  classification_loss: 0.1048 (0.1038)  time: 0.1947  data: 0.0002  max mem: 5511
[11:43:08.775611] Epoch: [97]  [580/781]  eta: 0:00:39  lr: 0.000001  training_loss: 0.3031 (0.3066)  mae_loss: 0.1954 (0.2028)  classification_loss: 0.1024 (0.1038)  time: 0.1962  data: 0.0002  max mem: 5511
[11:43:12.675069] Epoch: [97]  [600/781]  eta: 0:00:35  lr: 0.000001  training_loss: 0.3144 (0.3067)  mae_loss: 0.2069 (0.2028)  classification_loss: 0.1028 (0.1038)  time: 0.1949  data: 0.0002  max mem: 5511
[11:43:16.573905] Epoch: [97]  [620/781]  eta: 0:00:31  lr: 0.000001  training_loss: 0.3061 (0.3068)  mae_loss: 0.2050 (0.2030)  classification_loss: 0.0985 (0.1038)  time: 0.1949  data: 0.0002  max mem: 5511
[11:43:20.464120] Epoch: [97]  [640/781]  eta: 0:00:27  lr: 0.000001  training_loss: 0.3012 (0.3069)  mae_loss: 0.2033 (0.2031)  classification_loss: 0.1028 (0.1038)  time: 0.1944  data: 0.0002  max mem: 5511
[11:43:24.367130] Epoch: [97]  [660/781]  eta: 0:00:23  lr: 0.000001  training_loss: 0.3066 (0.3069)  mae_loss: 0.1984 (0.2031)  classification_loss: 0.1033 (0.1038)  time: 0.1951  data: 0.0002  max mem: 5511
[11:43:28.281309] Epoch: [97]  [680/781]  eta: 0:00:19  lr: 0.000001  training_loss: 0.3175 (0.3072)  mae_loss: 0.2099 (0.2034)  classification_loss: 0.1035 (0.1038)  time: 0.1956  data: 0.0003  max mem: 5511
[11:43:32.209207] Epoch: [97]  [700/781]  eta: 0:00:15  lr: 0.000001  training_loss: 0.2930 (0.3070)  mae_loss: 0.1964 (0.2032)  classification_loss: 0.1030 (0.1039)  time: 0.1963  data: 0.0004  max mem: 5511
[11:43:36.129323] Epoch: [97]  [720/781]  eta: 0:00:11  lr: 0.000001  training_loss: 0.3068 (0.3071)  mae_loss: 0.1952 (0.2032)  classification_loss: 0.1027 (0.1039)  time: 0.1959  data: 0.0002  max mem: 5511
[11:43:40.046744] Epoch: [97]  [740/781]  eta: 0:00:08  lr: 0.000001  training_loss: 0.3012 (0.3073)  mae_loss: 0.2064 (0.2034)  classification_loss: 0.1014 (0.1039)  time: 0.1958  data: 0.0002  max mem: 5511
[11:43:43.938642] Epoch: [97]  [760/781]  eta: 0:00:04  lr: 0.000001  training_loss: 0.3143 (0.3077)  mae_loss: 0.2078 (0.2037)  classification_loss: 0.1034 (0.1039)  time: 0.1945  data: 0.0002  max mem: 5511
[11:43:47.812108] Epoch: [97]  [780/781]  eta: 0:00:00  lr: 0.000001  training_loss: 0.3081 (0.3078)  mae_loss: 0.2036 (0.2038)  classification_loss: 0.1053 (0.1040)  time: 0.1936  data: 0.0002  max mem: 5511
[11:43:47.987898] Epoch: [97] Total time: 0:02:33 (0.1967 s / it)
[11:43:47.988685] Averaged stats: lr: 0.000001  training_loss: 0.3081 (0.3078)  mae_loss: 0.2036 (0.2038)  classification_loss: 0.1053 (0.1040)
[11:43:48.552157] Test:  [  0/157]  eta: 0:01:27  testing_loss: 0.4796 (0.4796)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.5593  data: 0.5166  max mem: 5511
[11:43:48.836769] Test:  [ 10/157]  eta: 0:00:11  testing_loss: 0.3672 (0.3832)  acc1: 90.6250 (89.3466)  acc5: 100.0000 (99.7159)  time: 0.0764  data: 0.0471  max mem: 5511
[11:43:49.118613] Test:  [ 20/157]  eta: 0:00:07  testing_loss: 0.3404 (0.3692)  acc1: 90.6250 (89.8810)  acc5: 100.0000 (99.7024)  time: 0.0281  data: 0.0002  max mem: 5511
[11:43:49.400223] Test:  [ 30/157]  eta: 0:00:05  testing_loss: 0.3302 (0.3732)  acc1: 90.6250 (89.8185)  acc5: 100.0000 (99.5464)  time: 0.0280  data: 0.0002  max mem: 5511
[11:43:49.685088] Test:  [ 40/157]  eta: 0:00:04  testing_loss: 0.3816 (0.3868)  acc1: 89.0625 (89.2912)  acc5: 100.0000 (99.4665)  time: 0.0282  data: 0.0002  max mem: 5511
[11:43:49.972624] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.3705 (0.3745)  acc1: 89.0625 (89.6446)  acc5: 100.0000 (99.4485)  time: 0.0285  data: 0.0002  max mem: 5511
[11:43:50.261942] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.2930 (0.3720)  acc1: 90.6250 (89.7797)  acc5: 100.0000 (99.4621)  time: 0.0286  data: 0.0002  max mem: 5511
[11:43:50.547206] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.3344 (0.3666)  acc1: 89.0625 (89.8548)  acc5: 100.0000 (99.5379)  time: 0.0285  data: 0.0002  max mem: 5511
[11:43:50.836016] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.3613 (0.3737)  acc1: 89.0625 (89.6605)  acc5: 100.0000 (99.5370)  time: 0.0286  data: 0.0002  max mem: 5511
[11:43:51.118898] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.3966 (0.3715)  acc1: 89.0625 (89.7321)  acc5: 100.0000 (99.5536)  time: 0.0284  data: 0.0002  max mem: 5511
[11:43:51.401392] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.3995 (0.3758)  acc1: 89.0625 (89.6040)  acc5: 100.0000 (99.5823)  time: 0.0281  data: 0.0002  max mem: 5511
[11:43:51.688009] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.3942 (0.3749)  acc1: 89.0625 (89.6819)  acc5: 100.0000 (99.5636)  time: 0.0283  data: 0.0002  max mem: 5511
[11:43:51.976119] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.3539 (0.3735)  acc1: 90.6250 (89.6823)  acc5: 100.0000 (99.5868)  time: 0.0286  data: 0.0002  max mem: 5511
[11:43:52.264113] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.3558 (0.3747)  acc1: 89.0625 (89.5635)  acc5: 100.0000 (99.5468)  time: 0.0286  data: 0.0003  max mem: 5511
[11:43:52.545236] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3225 (0.3713)  acc1: 90.6250 (89.6498)  acc5: 100.0000 (99.5789)  time: 0.0283  data: 0.0003  max mem: 5511
[11:43:52.825659] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.3179 (0.3711)  acc1: 90.6250 (89.7041)  acc5: 100.0000 (99.5964)  time: 0.0280  data: 0.0001  max mem: 5511
[11:43:52.976825] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.3225 (0.3701)  acc1: 90.6250 (89.6800)  acc5: 100.0000 (99.5900)  time: 0.0270  data: 0.0001  max mem: 5511
[11:43:53.126311] Test: Total time: 0:00:05 (0.0327 s / it)
[11:43:53.128300] * Acc@1 89.680 Acc@5 99.590 loss 0.370
[11:43:53.129283] Accuracy of the network on the 10000 test images: 89.7%
[11:43:53.130126] Max accuracy: 89.75%
[11:43:53.403154] log_dir: ./output_dir
[11:43:54.317686] Epoch: [98]  [  0/781]  eta: 0:11:52  lr: 0.000001  training_loss: 0.2805 (0.2805)  mae_loss: 0.1748 (0.1748)  classification_loss: 0.1057 (0.1057)  time: 0.9118  data: 0.6979  max mem: 5511
[11:43:58.264099] Epoch: [98]  [ 20/781]  eta: 0:02:55  lr: 0.000001  training_loss: 0.3170 (0.3078)  mae_loss: 0.1997 (0.2040)  classification_loss: 0.1007 (0.1038)  time: 0.1972  data: 0.0003  max mem: 5511
[11:44:02.175139] Epoch: [98]  [ 40/781]  eta: 0:02:38  lr: 0.000001  training_loss: 0.3033 (0.3056)  mae_loss: 0.2027 (0.2023)  classification_loss: 0.1024 (0.1032)  time: 0.1955  data: 0.0002  max mem: 5511
[11:44:06.070011] Epoch: [98]  [ 60/781]  eta: 0:02:29  lr: 0.000001  training_loss: 0.3259 (0.3100)  mae_loss: 0.2136 (0.2068)  classification_loss: 0.1023 (0.1033)  time: 0.1946  data: 0.0003  max mem: 5511
[11:44:09.972555] Epoch: [98]  [ 80/781]  eta: 0:02:23  lr: 0.000001  training_loss: 0.2940 (0.3065)  mae_loss: 0.1921 (0.2036)  classification_loss: 0.0998 (0.1029)  time: 0.1950  data: 0.0003  max mem: 5511
[11:44:13.924401] Epoch: [98]  [100/781]  eta: 0:02:18  lr: 0.000001  training_loss: 0.3047 (0.3066)  mae_loss: 0.1979 (0.2033)  classification_loss: 0.1039 (0.1033)  time: 0.1975  data: 0.0002  max mem: 5511
[11:44:17.823418] Epoch: [98]  [120/781]  eta: 0:02:13  lr: 0.000001  training_loss: 0.2938 (0.3052)  mae_loss: 0.1971 (0.2019)  classification_loss: 0.1019 (0.1033)  time: 0.1949  data: 0.0002  max mem: 5511
[11:44:21.743868] Epoch: [98]  [140/781]  eta: 0:02:08  lr: 0.000001  training_loss: 0.3011 (0.3050)  mae_loss: 0.1929 (0.2016)  classification_loss: 0.1017 (0.1034)  time: 0.1959  data: 0.0002  max mem: 5511
[11:44:25.660565] Epoch: [98]  [160/781]  eta: 0:02:04  lr: 0.000001  training_loss: 0.3027 (0.3046)  mae_loss: 0.2027 (0.2014)  classification_loss: 0.0992 (0.1032)  time: 0.1958  data: 0.0002  max mem: 5511
[11:44:29.554701] Epoch: [98]  [180/781]  eta: 0:01:59  lr: 0.000001  training_loss: 0.3019 (0.3057)  mae_loss: 0.1981 (0.2024)  classification_loss: 0.1054 (0.1032)  time: 0.1946  data: 0.0003  max mem: 5511
[11:44:33.460313] Epoch: [98]  [200/781]  eta: 0:01:55  lr: 0.000001  training_loss: 0.2978 (0.3055)  mae_loss: 0.1952 (0.2021)  classification_loss: 0.1033 (0.1034)  time: 0.1952  data: 0.0004  max mem: 5511
[11:44:37.371053] Epoch: [98]  [220/781]  eta: 0:01:51  lr: 0.000001  training_loss: 0.2896 (0.3048)  mae_loss: 0.1827 (0.2014)  classification_loss: 0.1021 (0.1034)  time: 0.1955  data: 0.0002  max mem: 5511
[11:44:41.333427] Epoch: [98]  [240/781]  eta: 0:01:47  lr: 0.000001  training_loss: 0.3039 (0.3049)  mae_loss: 0.1936 (0.2013)  classification_loss: 0.1074 (0.1036)  time: 0.1980  data: 0.0002  max mem: 5511
[11:44:45.319626] Epoch: [98]  [260/781]  eta: 0:01:43  lr: 0.000001  training_loss: 0.3006 (0.3049)  mae_loss: 0.1913 (0.2013)  classification_loss: 0.1047 (0.1036)  time: 0.1992  data: 0.0003  max mem: 5511
[11:44:49.258817] Epoch: [98]  [280/781]  eta: 0:01:39  lr: 0.000001  training_loss: 0.3022 (0.3048)  mae_loss: 0.1972 (0.2014)  classification_loss: 0.1014 (0.1035)  time: 0.1969  data: 0.0003  max mem: 5511
[11:44:53.164147] Epoch: [98]  [300/781]  eta: 0:01:35  lr: 0.000001  training_loss: 0.3014 (0.3048)  mae_loss: 0.1881 (0.2013)  classification_loss: 0.1043 (0.1035)  time: 0.1952  data: 0.0002  max mem: 5511
[11:44:57.097575] Epoch: [98]  [320/781]  eta: 0:01:31  lr: 0.000001  training_loss: 0.3058 (0.3050)  mae_loss: 0.2019 (0.2015)  classification_loss: 0.1055 (0.1035)  time: 0.1966  data: 0.0003  max mem: 5511
[11:45:01.033997] Epoch: [98]  [340/781]  eta: 0:01:27  lr: 0.000001  training_loss: 0.3244 (0.3056)  mae_loss: 0.2112 (0.2020)  classification_loss: 0.1046 (0.1035)  time: 0.1967  data: 0.0002  max mem: 5511
[11:45:04.967951] Epoch: [98]  [360/781]  eta: 0:01:23  lr: 0.000001  training_loss: 0.3056 (0.3056)  mae_loss: 0.1939 (0.2020)  classification_loss: 0.1032 (0.1036)  time: 0.1966  data: 0.0002  max mem: 5511
[11:45:08.880122] Epoch: [98]  [380/781]  eta: 0:01:19  lr: 0.000001  training_loss: 0.3092 (0.3060)  mae_loss: 0.1994 (0.2023)  classification_loss: 0.1078 (0.1037)  time: 0.1955  data: 0.0003  max mem: 5511
[11:45:12.868806] Epoch: [98]  [400/781]  eta: 0:01:15  lr: 0.000001  training_loss: 0.3028 (0.3059)  mae_loss: 0.1978 (0.2020)  classification_loss: 0.1049 (0.1038)  time: 0.1993  data: 0.0003  max mem: 5511
[11:45:16.775027] Epoch: [98]  [420/781]  eta: 0:01:11  lr: 0.000001  training_loss: 0.3199 (0.3065)  mae_loss: 0.2154 (0.2027)  classification_loss: 0.1028 (0.1038)  time: 0.1952  data: 0.0002  max mem: 5511
[11:45:20.684706] Epoch: [98]  [440/781]  eta: 0:01:07  lr: 0.000001  training_loss: 0.3086 (0.3069)  mae_loss: 0.2140 (0.2032)  classification_loss: 0.1017 (0.1037)  time: 0.1954  data: 0.0003  max mem: 5511
[11:45:24.587924] Epoch: [98]  [460/781]  eta: 0:01:03  lr: 0.000001  training_loss: 0.3126 (0.3069)  mae_loss: 0.2057 (0.2032)  classification_loss: 0.1049 (0.1037)  time: 0.1951  data: 0.0002  max mem: 5511
[11:45:28.506807] Epoch: [98]  [480/781]  eta: 0:00:59  lr: 0.000001  training_loss: 0.3026 (0.3069)  mae_loss: 0.2004 (0.2031)  classification_loss: 0.1034 (0.1038)  time: 0.1959  data: 0.0005  max mem: 5511
[11:45:32.436871] Epoch: [98]  [500/781]  eta: 0:00:55  lr: 0.000001  training_loss: 0.2921 (0.3066)  mae_loss: 0.1891 (0.2029)  classification_loss: 0.1021 (0.1038)  time: 0.1964  data: 0.0002  max mem: 5511
[11:45:36.344925] Epoch: [98]  [520/781]  eta: 0:00:51  lr: 0.000001  training_loss: 0.3006 (0.3064)  mae_loss: 0.1946 (0.2027)  classification_loss: 0.1011 (0.1037)  time: 0.1953  data: 0.0003  max mem: 5511
[11:45:40.277611] Epoch: [98]  [540/781]  eta: 0:00:47  lr: 0.000001  training_loss: 0.2924 (0.3061)  mae_loss: 0.1917 (0.2023)  classification_loss: 0.1089 (0.1039)  time: 0.1966  data: 0.0002  max mem: 5511
[11:45:44.178726] Epoch: [98]  [560/781]  eta: 0:00:43  lr: 0.000001  training_loss: 0.3054 (0.3063)  mae_loss: 0.2059 (0.2024)  classification_loss: 0.1002 (0.1039)  time: 0.1950  data: 0.0002  max mem: 5511
[11:45:48.087405] Epoch: [98]  [580/781]  eta: 0:00:39  lr: 0.000001  training_loss: 0.3115 (0.3065)  mae_loss: 0.1976 (0.2026)  classification_loss: 0.1036 (0.1040)  time: 0.1953  data: 0.0003  max mem: 5511
[11:45:51.991264] Epoch: [98]  [600/781]  eta: 0:00:35  lr: 0.000001  training_loss: 0.3034 (0.3065)  mae_loss: 0.2019 (0.2025)  classification_loss: 0.1037 (0.1040)  time: 0.1951  data: 0.0002  max mem: 5511
[11:45:55.888094] Epoch: [98]  [620/781]  eta: 0:00:31  lr: 0.000001  training_loss: 0.3067 (0.3064)  mae_loss: 0.1897 (0.2024)  classification_loss: 0.1061 (0.1040)  time: 0.1948  data: 0.0002  max mem: 5511
[11:45:59.789803] Epoch: [98]  [640/781]  eta: 0:00:27  lr: 0.000001  training_loss: 0.2987 (0.3063)  mae_loss: 0.1983 (0.2022)  classification_loss: 0.1044 (0.1041)  time: 0.1950  data: 0.0003  max mem: 5511
[11:46:03.676552] Epoch: [98]  [660/781]  eta: 0:00:23  lr: 0.000001  training_loss: 0.3127 (0.3065)  mae_loss: 0.2090 (0.2024)  classification_loss: 0.1029 (0.1041)  time: 0.1942  data: 0.0002  max mem: 5511
[11:46:07.570950] Epoch: [98]  [680/781]  eta: 0:00:19  lr: 0.000001  training_loss: 0.3215 (0.3069)  mae_loss: 0.2202 (0.2028)  classification_loss: 0.1033 (0.1041)  time: 0.1946  data: 0.0002  max mem: 5511
[11:46:11.510531] Epoch: [98]  [700/781]  eta: 0:00:15  lr: 0.000001  training_loss: 0.2980 (0.3069)  mae_loss: 0.1956 (0.2027)  classification_loss: 0.1038 (0.1041)  time: 0.1969  data: 0.0002  max mem: 5511
[11:46:15.422120] Epoch: [98]  [720/781]  eta: 0:00:12  lr: 0.000001  training_loss: 0.2977 (0.3069)  mae_loss: 0.1946 (0.2027)  classification_loss: 0.1027 (0.1042)  time: 0.1955  data: 0.0002  max mem: 5511
[11:46:19.311973] Epoch: [98]  [740/781]  eta: 0:00:08  lr: 0.000001  training_loss: 0.3133 (0.3070)  mae_loss: 0.2158 (0.2030)  classification_loss: 0.0973 (0.1040)  time: 0.1944  data: 0.0002  max mem: 5511
[11:46:23.230770] Epoch: [98]  [760/781]  eta: 0:00:04  lr: 0.000001  training_loss: 0.3060 (0.3070)  mae_loss: 0.1998 (0.2029)  classification_loss: 0.1071 (0.1041)  time: 0.1959  data: 0.0002  max mem: 5511
[11:46:27.170360] Epoch: [98]  [780/781]  eta: 0:00:00  lr: 0.000001  training_loss: 0.3213 (0.3072)  mae_loss: 0.2129 (0.2031)  classification_loss: 0.1046 (0.1041)  time: 0.1969  data: 0.0002  max mem: 5511
[11:46:27.339969] Epoch: [98] Total time: 0:02:33 (0.1971 s / it)
[11:46:27.340447] Averaged stats: lr: 0.000001  training_loss: 0.3213 (0.3072)  mae_loss: 0.2129 (0.2031)  classification_loss: 0.1046 (0.1041)
[11:46:27.990050] Test:  [  0/157]  eta: 0:01:41  testing_loss: 0.4764 (0.4764)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.6454  data: 0.6161  max mem: 5511
[11:46:28.286629] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.3681 (0.3820)  acc1: 89.0625 (89.3466)  acc5: 100.0000 (99.7159)  time: 0.0855  data: 0.0571  max mem: 5511
[11:46:28.576177] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.3406 (0.3693)  acc1: 90.6250 (89.8065)  acc5: 100.0000 (99.7024)  time: 0.0292  data: 0.0007  max mem: 5511
[11:46:28.864259] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.3294 (0.3727)  acc1: 90.6250 (89.7681)  acc5: 100.0000 (99.5464)  time: 0.0288  data: 0.0002  max mem: 5511
[11:46:29.150956] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.3793 (0.3861)  acc1: 89.0625 (89.2530)  acc5: 100.0000 (99.4284)  time: 0.0286  data: 0.0002  max mem: 5511
[11:46:29.436252] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.3663 (0.3737)  acc1: 89.0625 (89.6140)  acc5: 98.4375 (99.4179)  time: 0.0285  data: 0.0002  max mem: 5511
[11:46:29.721494] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.2915 (0.3712)  acc1: 90.6250 (89.6516)  acc5: 100.0000 (99.4365)  time: 0.0284  data: 0.0002  max mem: 5511
[11:46:30.010717] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.3402 (0.3663)  acc1: 90.6250 (89.7447)  acc5: 100.0000 (99.4938)  time: 0.0285  data: 0.0004  max mem: 5511
[11:46:30.304025] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.3611 (0.3736)  acc1: 89.0625 (89.5640)  acc5: 100.0000 (99.4985)  time: 0.0290  data: 0.0006  max mem: 5511
[11:46:30.592029] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.3942 (0.3714)  acc1: 89.0625 (89.6635)  acc5: 100.0000 (99.5192)  time: 0.0289  data: 0.0004  max mem: 5511
[11:46:30.883954] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.3995 (0.3756)  acc1: 90.6250 (89.5421)  acc5: 100.0000 (99.5514)  time: 0.0288  data: 0.0002  max mem: 5511
[11:46:31.165733] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.3948 (0.3747)  acc1: 89.0625 (89.6396)  acc5: 100.0000 (99.5495)  time: 0.0285  data: 0.0002  max mem: 5511
[11:46:31.455886] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.3525 (0.3730)  acc1: 90.6250 (89.6694)  acc5: 100.0000 (99.5739)  time: 0.0284  data: 0.0002  max mem: 5511
[11:46:31.743562] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.3525 (0.3739)  acc1: 89.0625 (89.5515)  acc5: 100.0000 (99.5468)  time: 0.0288  data: 0.0002  max mem: 5511
[11:46:32.025973] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3255 (0.3707)  acc1: 89.0625 (89.6055)  acc5: 100.0000 (99.5789)  time: 0.0284  data: 0.0001  max mem: 5511
[11:46:32.303825] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.3215 (0.3704)  acc1: 90.6250 (89.6627)  acc5: 100.0000 (99.5861)  time: 0.0279  data: 0.0001  max mem: 5511
[11:46:32.453275] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.3179 (0.3693)  acc1: 90.6250 (89.6400)  acc5: 100.0000 (99.5800)  time: 0.0268  data: 0.0001  max mem: 5511
[11:46:32.612549] Test: Total time: 0:00:05 (0.0336 s / it)
[11:46:32.613183] * Acc@1 89.640 Acc@5 99.580 loss 0.369
[11:46:32.613550] Accuracy of the network on the 10000 test images: 89.6%
[11:46:32.613825] Max accuracy: 89.75%
[11:46:32.882654] log_dir: ./output_dir
[11:46:33.813822] Epoch: [99]  [  0/781]  eta: 0:12:05  lr: 0.000001  training_loss: 0.2554 (0.2554)  mae_loss: 0.1717 (0.1717)  classification_loss: 0.0837 (0.0837)  time: 0.9294  data: 0.7127  max mem: 5511
[11:46:37.725924] Epoch: [99]  [ 20/781]  eta: 0:02:55  lr: 0.000001  training_loss: 0.3102 (0.3117)  mae_loss: 0.1995 (0.2067)  classification_loss: 0.1040 (0.1050)  time: 0.1955  data: 0.0003  max mem: 5511
[11:46:41.623371] Epoch: [99]  [ 40/781]  eta: 0:02:37  lr: 0.000001  training_loss: 0.2991 (0.3067)  mae_loss: 0.1995 (0.2026)  classification_loss: 0.1008 (0.1041)  time: 0.1948  data: 0.0002  max mem: 5511
[11:46:45.550091] Epoch: [99]  [ 60/781]  eta: 0:02:29  lr: 0.000001  training_loss: 0.2996 (0.3060)  mae_loss: 0.1895 (0.2008)  classification_loss: 0.1069 (0.1052)  time: 0.1963  data: 0.0002  max mem: 5511

[11:46:49.457391] Epoch: [99]  [ 80/781]  eta: 0:02:23  lr: 0.000001  training_loss: 0.3002 (0.3050)  mae_loss: 0.1949 (0.2008)  classification_loss: 0.0981 (0.1043)  time: 0.1953  data: 0.0002  max mem: 5511
[11:46:53.375745] Epoch: [99]  [100/781]  eta: 0:02:18  lr: 0.000001  training_loss: 0.3184 (0.3061)  mae_loss: 0.2041 (0.2013)  classification_loss: 0.1023 (0.1048)  time: 0.1958  data: 0.0002  max mem: 5511
[11:46:57.287149] Epoch: [99]  [120/781]  eta: 0:02:13  lr: 0.000001  training_loss: 0.3059 (0.3066)  mae_loss: 0.2021 (0.2014)  classification_loss: 0.1078 (0.1053)  time: 0.1955  data: 0.0002  max mem: 5511
[11:47:01.186507] Epoch: [99]  [140/781]  eta: 0:02:08  lr: 0.000001  training_loss: 0.2979 (0.3058)  mae_loss: 0.1995 (0.2007)  classification_loss: 0.1005 (0.1051)  time: 0.1949  data: 0.0002  max mem: 5511
[11:47:05.096438] Epoch: [99]  [160/781]  eta: 0:02:04  lr: 0.000001  training_loss: 0.2927 (0.3046)  mae_loss: 0.1885 (0.1999)  classification_loss: 0.1019 (0.1047)  time: 0.1954  data: 0.0002  max mem: 5511
[11:47:08.992318] Epoch: [99]  [180/781]  eta: 0:01:59  lr: 0.000001  training_loss: 0.3173 (0.3061)  mae_loss: 0.2121 (0.2015)  classification_loss: 0.1019 (0.1045)  time: 0.1947  data: 0.0003  max mem: 5511
[11:47:12.877887] Epoch: [99]  [200/781]  eta: 0:01:55  lr: 0.000001  training_loss: 0.3128 (0.3068)  mae_loss: 0.2046 (0.2024)  classification_loss: 0.1050 (0.1044)  time: 0.1942  data: 0.0003  max mem: 5511
[11:47:16.793917] Epoch: [99]  [220/781]  eta: 0:01:51  lr: 0.000001  training_loss: 0.3308 (0.3080)  mae_loss: 0.2151 (0.2036)  classification_loss: 0.1038 (0.1043)  time: 0.1957  data: 0.0003  max mem: 5511
[11:47:20.685641] Epoch: [99]  [240/781]  eta: 0:01:47  lr: 0.000001  training_loss: 0.3095 (0.3078)  mae_loss: 0.1942 (0.2031)  classification_loss: 0.1080 (0.1047)  time: 0.1945  data: 0.0003  max mem: 5511
[11:47:24.592570] Epoch: [99]  [260/781]  eta: 0:01:43  lr: 0.000001  training_loss: 0.3039 (0.3081)  mae_loss: 0.2036 (0.2034)  classification_loss: 0.1051 (0.1048)  time: 0.1953  data: 0.0003  max mem: 5511
[11:47:28.502599] Epoch: [99]  [280/781]  eta: 0:01:39  lr: 0.000001  training_loss: 0.2997 (0.3078)  mae_loss: 0.2042 (0.2033)  classification_loss: 0.1001 (0.1045)  time: 0.1954  data: 0.0002  max mem: 5511
[11:47:32.511299] Epoch: [99]  [300/781]  eta: 0:01:35  lr: 0.000001  training_loss: 0.2888 (0.3071)  mae_loss: 0.1885 (0.2027)  classification_loss: 0.0996 (0.1044)  time: 0.2004  data: 0.0002  max mem: 5511
[11:47:36.437753] Epoch: [99]  [320/781]  eta: 0:01:31  lr: 0.000001  training_loss: 0.3140 (0.3074)  mae_loss: 0.2058 (0.2029)  classification_loss: 0.1064 (0.1045)  time: 0.1962  data: 0.0003  max mem: 5511
[11:47:40.334981] Epoch: [99]  [340/781]  eta: 0:01:27  lr: 0.000001  training_loss: 0.3097 (0.3076)  mae_loss: 0.2039 (0.2032)  classification_loss: 0.0982 (0.1044)  time: 0.1948  data: 0.0002  max mem: 5511
[11:47:44.241484] Epoch: [99]  [360/781]  eta: 0:01:23  lr: 0.000001  training_loss: 0.3106 (0.3079)  mae_loss: 0.2063 (0.2034)  classification_loss: 0.1051 (0.1045)  time: 0.1952  data: 0.0002  max mem: 5511
[11:47:48.143051] Epoch: [99]  [380/781]  eta: 0:01:19  lr: 0.000001  training_loss: 0.3160 (0.3084)  mae_loss: 0.2094 (0.2039)  classification_loss: 0.1085 (0.1046)  time: 0.1950  data: 0.0002  max mem: 5511
[11:47:52.072446] Epoch: [99]  [400/781]  eta: 0:01:15  lr: 0.000001  training_loss: 0.3115 (0.3085)  mae_loss: 0.2046 (0.2041)  classification_loss: 0.1004 (0.1044)  time: 0.1964  data: 0.0002  max mem: 5511
[11:47:55.981212] Epoch: [99]  [420/781]  eta: 0:01:11  lr: 0.000001  training_loss: 0.3087 (0.3087)  mae_loss: 0.2073 (0.2043)  classification_loss: 0.1028 (0.1044)  time: 0.1953  data: 0.0003  max mem: 5511
[11:47:59.879617] Epoch: [99]  [440/781]  eta: 0:01:07  lr: 0.000001  training_loss: 0.3070 (0.3089)  mae_loss: 0.2053 (0.2046)  classification_loss: 0.1031 (0.1044)  time: 0.1948  data: 0.0005  max mem: 5511
[11:48:03.821072] Epoch: [99]  [460/781]  eta: 0:01:03  lr: 0.000001  training_loss: 0.3110 (0.3089)  mae_loss: 0.1958 (0.2046)  classification_loss: 0.1018 (0.1043)  time: 0.1970  data: 0.0002  max mem: 5511
[11:48:07.732609] Epoch: [99]  [480/781]  eta: 0:00:59  lr: 0.000001  training_loss: 0.3114 (0.3090)  mae_loss: 0.2112 (0.2046)  classification_loss: 0.1045 (0.1043)  time: 0.1955  data: 0.0002  max mem: 5511
[11:48:11.671449] Epoch: [99]  [500/781]  eta: 0:00:55  lr: 0.000001  training_loss: 0.3064 (0.3090)  mae_loss: 0.1956 (0.2046)  classification_loss: 0.1063 (0.1044)  time: 0.1969  data: 0.0005  max mem: 5511
[11:48:15.565269] Epoch: [99]  [520/781]  eta: 0:00:51  lr: 0.000001  training_loss: 0.3066 (0.3092)  mae_loss: 0.2000 (0.2048)  classification_loss: 0.1023 (0.1044)  time: 0.1946  data: 0.0002  max mem: 5511
[11:48:19.465337] Epoch: [99]  [540/781]  eta: 0:00:47  lr: 0.000001  training_loss: 0.3074 (0.3091)  mae_loss: 0.2033 (0.2049)  classification_loss: 0.0989 (0.1042)  time: 0.1949  data: 0.0004  max mem: 5511
[11:48:23.443090] Epoch: [99]  [560/781]  eta: 0:00:43  lr: 0.000001  training_loss: 0.3003 (0.3089)  mae_loss: 0.2002 (0.2047)  classification_loss: 0.1056 (0.1042)  time: 0.1988  data: 0.0002  max mem: 5511
[11:48:27.359085] Epoch: [99]  [580/781]  eta: 0:00:39  lr: 0.000001  training_loss: 0.2986 (0.3088)  mae_loss: 0.2022 (0.2047)  classification_loss: 0.0983 (0.1042)  time: 0.1957  data: 0.0002  max mem: 5511
[11:48:31.256435] Epoch: [99]  [600/781]  eta: 0:00:35  lr: 0.000001  training_loss: 0.2995 (0.3087)  mae_loss: 0.2016 (0.2046)  classification_loss: 0.1001 (0.1041)  time: 0.1948  data: 0.0002  max mem: 5511
[11:48:35.181832] Epoch: [99]  [620/781]  eta: 0:00:31  lr: 0.000001  training_loss: 0.3116 (0.3088)  mae_loss: 0.2104 (0.2048)  classification_loss: 0.1024 (0.1041)  time: 0.1962  data: 0.0003  max mem: 5511
[11:48:39.104721] Epoch: [99]  [640/781]  eta: 0:00:27  lr: 0.000001  training_loss: 0.3102 (0.3089)  mae_loss: 0.2018 (0.2049)  classification_loss: 0.1036 (0.1041)  time: 0.1960  data: 0.0002  max mem: 5511
[11:48:43.013163] Epoch: [99]  [660/781]  eta: 0:00:23  lr: 0.000001  training_loss: 0.3043 (0.3089)  mae_loss: 0.1991 (0.2049)  classification_loss: 0.1040 (0.1040)  time: 0.1953  data: 0.0003  max mem: 5511
[11:48:46.890588] Epoch: [99]  [680/781]  eta: 0:00:19  lr: 0.000001  training_loss: 0.3091 (0.3091)  mae_loss: 0.2018 (0.2050)  classification_loss: 0.1053 (0.1041)  time: 0.1938  data: 0.0002  max mem: 5511
[11:48:50.780594] Epoch: [99]  [700/781]  eta: 0:00:15  lr: 0.000001  training_loss: 0.3067 (0.3091)  mae_loss: 0.2005 (0.2049)  classification_loss: 0.1074 (0.1041)  time: 0.1944  data: 0.0002  max mem: 5511
[11:48:54.773141] Epoch: [99]  [720/781]  eta: 0:00:11  lr: 0.000001  training_loss: 0.3055 (0.3091)  mae_loss: 0.1965 (0.2048)  classification_loss: 0.1091 (0.1043)  time: 0.1995  data: 0.0002  max mem: 5511
[11:48:58.675868] Epoch: [99]  [740/781]  eta: 0:00:08  lr: 0.000001  training_loss: 0.2961 (0.3090)  mae_loss: 0.1881 (0.2047)  classification_loss: 0.1063 (0.1043)  time: 0.1950  data: 0.0002  max mem: 5511
[11:49:02.635114] Epoch: [99]  [760/781]  eta: 0:00:04  lr: 0.000001  training_loss: 0.3092 (0.3090)  mae_loss: 0.1987 (0.2047)  classification_loss: 0.1031 (0.1043)  time: 0.1978  data: 0.0002  max mem: 5511
[11:49:06.534278] Epoch: [99]  [780/781]  eta: 0:00:00  lr: 0.000001  training_loss: 0.3106 (0.3090)  mae_loss: 0.2082 (0.2046)  classification_loss: 0.1035 (0.1044)  time: 0.1949  data: 0.0002  max mem: 5511
[11:49:06.703077] Epoch: [99] Total time: 0:02:33 (0.1970 s / it)
[11:49:06.703580] Averaged stats: lr: 0.000001  training_loss: 0.3106 (0.3090)  mae_loss: 0.2082 (0.2046)  classification_loss: 0.1035 (0.1044)
[11:49:07.375442] Test:  [  0/157]  eta: 0:01:44  testing_loss: 0.4716 (0.4716)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.6681  data: 0.6341  max mem: 5511
[11:49:07.670291] Test:  [ 10/157]  eta: 0:00:12  testing_loss: 0.3732 (0.3835)  acc1: 89.0625 (88.9205)  acc5: 100.0000 (99.7159)  time: 0.0874  data: 0.0579  max mem: 5511
[11:49:07.956489] Test:  [ 20/157]  eta: 0:00:08  testing_loss: 0.3400 (0.3701)  acc1: 90.6250 (89.6577)  acc5: 100.0000 (99.7024)  time: 0.0289  data: 0.0003  max mem: 5511
[11:49:08.241078] Test:  [ 30/157]  eta: 0:00:06  testing_loss: 0.3274 (0.3737)  acc1: 90.6250 (89.6169)  acc5: 100.0000 (99.5464)  time: 0.0284  data: 0.0003  max mem: 5511
[11:49:08.525698] Test:  [ 40/157]  eta: 0:00:05  testing_loss: 0.3843 (0.3873)  acc1: 89.0625 (89.1768)  acc5: 100.0000 (99.4665)  time: 0.0283  data: 0.0002  max mem: 5511
[11:49:08.808309] Test:  [ 50/157]  eta: 0:00:04  testing_loss: 0.3691 (0.3745)  acc1: 89.0625 (89.5221)  acc5: 100.0000 (99.4485)  time: 0.0282  data: 0.0001  max mem: 5511
[11:49:09.100250] Test:  [ 60/157]  eta: 0:00:03  testing_loss: 0.2963 (0.3719)  acc1: 90.6250 (89.6260)  acc5: 100.0000 (99.4621)  time: 0.0286  data: 0.0002  max mem: 5511
[11:49:09.384427] Test:  [ 70/157]  eta: 0:00:03  testing_loss: 0.3390 (0.3667)  acc1: 90.6250 (89.7667)  acc5: 100.0000 (99.5158)  time: 0.0287  data: 0.0002  max mem: 5511
[11:49:09.666182] Test:  [ 80/157]  eta: 0:00:02  testing_loss: 0.3660 (0.3741)  acc1: 89.0625 (89.5833)  acc5: 100.0000 (99.4985)  time: 0.0282  data: 0.0002  max mem: 5511
[11:49:09.949019] Test:  [ 90/157]  eta: 0:00:02  testing_loss: 0.4050 (0.3720)  acc1: 89.0625 (89.7150)  acc5: 100.0000 (99.5192)  time: 0.0281  data: 0.0001  max mem: 5511
[11:49:10.234856] Test:  [100/157]  eta: 0:00:01  testing_loss: 0.4050 (0.3762)  acc1: 90.6250 (89.5730)  acc5: 100.0000 (99.5204)  time: 0.0283  data: 0.0002  max mem: 5511
[11:49:10.515931] Test:  [110/157]  eta: 0:00:01  testing_loss: 0.3966 (0.3753)  acc1: 89.0625 (89.6537)  acc5: 100.0000 (99.5073)  time: 0.0282  data: 0.0001  max mem: 5511
[11:49:10.803866] Test:  [120/157]  eta: 0:00:01  testing_loss: 0.3450 (0.3737)  acc1: 89.0625 (89.6952)  acc5: 100.0000 (99.5222)  time: 0.0283  data: 0.0002  max mem: 5511
[11:49:11.092685] Test:  [130/157]  eta: 0:00:00  testing_loss: 0.3531 (0.3747)  acc1: 89.0625 (89.5754)  acc5: 100.0000 (99.4871)  time: 0.0287  data: 0.0002  max mem: 5511
[11:49:11.373101] Test:  [140/157]  eta: 0:00:00  testing_loss: 0.3280 (0.3713)  acc1: 89.0625 (89.6498)  acc5: 100.0000 (99.5235)  time: 0.0283  data: 0.0002  max mem: 5511
[11:49:11.651997] Test:  [150/157]  eta: 0:00:00  testing_loss: 0.3263 (0.3711)  acc1: 90.6250 (89.6834)  acc5: 100.0000 (99.5344)  time: 0.0278  data: 0.0001  max mem: 5511
[11:49:11.802962] Test:  [156/157]  eta: 0:00:00  testing_loss: 0.3203 (0.3700)  acc1: 90.6250 (89.6600)  acc5: 100.0000 (99.5300)  time: 0.0269  data: 0.0001  max mem: 5511
[11:49:11.970922] Test: Total time: 0:00:05 (0.0335 s / it)
[11:49:11.971474] * Acc@1 89.660 Acc@5 99.530 loss 0.370
[11:49:11.971819] Accuracy of the network on the 10000 test images: 89.7%
[11:49:11.971998] Max accuracy: 89.75%
[11:49:12.170650] Training time 4:25:37